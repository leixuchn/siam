INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "253"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-13 08:51:21.676582: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:51:21.676621: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:51:21.676627: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:51:21.676631: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:51:21.676635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:51:22.430544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-13 08:51:22.430579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-13 08:51:22.430586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-13 08:51:22.430593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-13 08:51:26.994565: step 0, loss = 2.28, batch loss = 2.23 (2.5 examples/sec; 3.159 sec/batch; 291h:48m:24s remains)
2017-12-13 08:51:27.491651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3785596 -4.3784137 -4.3782973 -4.3781614 -4.37801 -4.3778467 -4.3776965 -4.3775859 -4.3775449 -4.3775945 -4.3777337 -4.3779564 -4.378212 -4.3784595 -4.3786778][-4.3783693 -4.378181 -4.3780131 -4.3778114 -4.3775764 -4.3773174 -4.37708 -4.3769164 -4.376874 -4.3769712 -4.3771982 -4.3775325 -4.377893 -4.3782277 -4.3785152][-4.3782315 -4.3780036 -4.3777857 -4.3775105 -4.3771825 -4.3768191 -4.3764896 -4.376276 -4.3762403 -4.3764029 -4.3767285 -4.3771768 -4.3776426 -4.3780627 -4.3784142][-4.3780951 -4.3778367 -4.3775744 -4.3772287 -4.3768139 -4.3763533 -4.375947 -4.3756933 -4.3756785 -4.3759203 -4.3763456 -4.3769 -4.3774538 -4.3779397 -4.3783374][-4.3779659 -4.3776803 -4.3773789 -4.376976 -4.3764944 -4.3759689 -4.3755155 -4.3752408 -4.3752689 -4.3755994 -4.3761139 -4.3767438 -4.3773513 -4.3778744 -4.3782945][-4.3778381 -4.3775306 -4.3772011 -4.3767629 -4.3762527 -4.3757119 -4.3752594 -4.3749981 -4.3751025 -4.3755274 -4.3761024 -4.3767529 -4.3773589 -4.3778787 -4.3782916][-4.3777137 -4.377388 -4.3770466 -4.3766074 -4.3761196 -4.3756285 -4.3752451 -4.3750582 -4.3752732 -4.375762 -4.3763328 -4.3769293 -4.3774743 -4.3779469 -4.3783245][-4.377605 -4.3772755 -4.376945 -4.3765464 -4.3761382 -4.3757672 -4.3755288 -4.3754816 -4.3757749 -4.3762383 -4.3767219 -4.3772035 -4.3776469 -4.3780475 -4.378376][-4.3775148 -4.3772006 -4.3769116 -4.3765893 -4.3762956 -4.3760757 -4.3759971 -4.3760676 -4.376368 -4.3767476 -4.3771133 -4.37747 -4.3778143 -4.3781457 -4.3784289][-4.37747 -4.3771877 -4.3769593 -4.3767304 -4.3765554 -4.3764682 -4.3764973 -4.3766212 -4.3768792 -4.3771605 -4.3774161 -4.3776717 -4.377943 -4.3782258 -4.3784757][-4.3774838 -4.3772488 -4.3770914 -4.376956 -4.3768783 -4.3768768 -4.376955 -4.3770761 -4.3772645 -4.3774524 -4.3776217 -4.3778133 -4.37804 -4.3782907 -4.3785152][-4.3775716 -4.3773994 -4.377315 -4.37726 -4.3772469 -4.3772855 -4.3773589 -4.3774347 -4.3775363 -4.3776369 -4.3777404 -4.3778958 -4.378099 -4.3783331 -4.3785419][-4.37771 -4.377605 -4.377584 -4.377584 -4.3776011 -4.3776364 -4.377667 -4.3776741 -4.3776879 -4.3777156 -4.3777747 -4.3779125 -4.3781118 -4.3783441 -4.3785505][-4.3778286 -4.3777719 -4.3777919 -4.3778191 -4.3778377 -4.377841 -4.3778086 -4.3777418 -4.3776855 -4.3776674 -4.3777108 -4.3778582 -4.3780761 -4.3783245 -4.37854][-4.3778539 -4.3778214 -4.3778648 -4.3779016 -4.3779058 -4.3778644 -4.37777 -4.3776422 -4.3775434 -4.3775139 -4.3775749 -4.3777575 -4.3780112 -4.3782864 -4.3785195]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 08:51:30.143691: step 10, loss = 1.78, batch loss = 1.69 (35.2 examples/sec; 0.228 sec/batch; 21h:01m:07s remains)
INFO - root - 2017-12-13 08:51:32.343079: step 20, loss = 0.90, batch loss = 0.80 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:56s remains)
INFO - root - 2017-12-13 08:51:34.566663: step 30, loss = 1.05, batch loss = 0.95 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:27s remains)
INFO - root - 2017-12-13 08:51:36.841889: step 40, loss = 0.84, batch loss = 0.73 (35.3 examples/sec; 0.227 sec/batch; 20h:57m:10s remains)
INFO - root - 2017-12-13 08:51:39.035567: step 50, loss = 0.79, batch loss = 0.67 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-13 08:51:41.250043: step 60, loss = 0.95, batch loss = 0.81 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-13 08:51:43.465176: step 70, loss = 1.35, batch loss = 1.20 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:01s remains)
INFO - root - 2017-12-13 08:51:45.708344: step 80, loss = 0.93, batch loss = 0.76 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:16s remains)
INFO - root - 2017-12-13 08:51:47.927773: step 90, loss = 0.89, batch loss = 0.71 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:20s remains)
INFO - root - 2017-12-13 08:51:50.178967: step 100, loss = 1.03, batch loss = 0.85 (34.2 examples/sec; 0.234 sec/batch; 21h:34m:08s remains)
2017-12-13 08:51:50.514456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6432331 -1.842766 -2.0557146 -2.2747779 -2.4821956 -2.6464577 -2.6909275 -2.7171431 -2.7369609 -2.7442133 -2.6894751 -2.66338 -2.6690545 -2.6633396 -2.6545463][-1.4816509 -1.6775309 -1.9115951 -2.1512878 -2.3875489 -2.5821819 -2.6377625 -2.6711407 -2.6991053 -2.7179766 -2.6654649 -2.6415818 -2.6502109 -2.6507671 -2.6487463][-1.4164867 -1.6119181 -1.8427094 -2.0851135 -2.3353989 -2.539397 -2.5982435 -2.6277966 -2.6539774 -2.6782303 -2.6379869 -2.6194882 -2.6415706 -2.65766 -2.6679316][-1.3348758 -1.5306135 -1.7592194 -2.0149012 -2.2830203 -2.5087402 -2.5763655 -2.6030037 -2.6246305 -2.6471212 -2.6386733 -2.6356688 -2.6662245 -2.695801 -2.7228065][-1.2984879 -1.4929836 -1.7206874 -1.9753575 -2.2481391 -2.4932659 -2.5678158 -2.5697098 -2.5868614 -2.5714478 -2.5717418 -2.5904708 -2.6199946 -2.6790142 -2.7207716][-1.2847213 -1.4808372 -1.7054352 -1.9567636 -2.220583 -2.4638605 -2.5493309 -2.5455775 -2.5583863 -2.5114484 -2.511631 -2.493716 -2.4875469 -2.5053725 -2.5384729][-1.3193055 -1.5017205 -1.7262065 -1.970637 -2.2078652 -2.4209237 -2.4900205 -2.5074952 -2.5132775 -2.4668865 -2.4735684 -2.4395306 -2.4433529 -2.4577842 -2.4483612][-1.3764483 -1.5473692 -1.7652147 -1.9938439 -2.1711893 -2.3243861 -2.4109268 -2.4625561 -2.4853773 -2.4290078 -2.4370427 -2.4121413 -2.4243176 -2.4299154 -2.4283097][-1.4816325 -1.6635947 -1.8907379 -2.1121323 -2.2354579 -2.3372004 -2.4097667 -2.455188 -2.4797819 -2.4167008 -2.4226143 -2.3919284 -2.402185 -2.4152732 -2.4153218][-1.6103909 -1.7542986 -1.995797 -2.2199042 -2.3157198 -2.3720489 -2.4238424 -2.4665709 -2.4919646 -2.4374495 -2.4477296 -2.4106386 -2.4167857 -2.4133797 -2.4122729][-1.85882 -1.949654 -2.1433794 -2.3251395 -2.4120288 -2.4299164 -2.4605215 -2.5139182 -2.5324101 -2.5146372 -2.5294368 -2.49304 -2.4960835 -2.4857974 -2.4752488][-2.1306653 -2.1832335 -2.3269947 -2.4596047 -2.4882331 -2.47542 -2.4907713 -2.5153449 -2.5167527 -2.523385 -2.573087 -2.5669262 -2.5923438 -2.6054983 -2.5937979][-2.3472817 -2.3922672 -2.5017915 -2.594564 -2.6033719 -2.5888481 -2.5863922 -2.5777273 -2.5633259 -2.5548642 -2.5934277 -2.5974391 -2.6094739 -2.6169379 -2.6325552][-2.5197983 -2.5510247 -2.637043 -2.6999564 -2.7031534 -2.6887217 -2.6671922 -2.6374834 -2.6107614 -2.593456 -2.6166553 -2.603369 -2.5991833 -2.6074829 -2.6170743][-2.6370792 -2.6431086 -2.6902032 -2.7241132 -2.720541 -2.7129261 -2.6949887 -2.6661711 -2.6385033 -2.6170087 -2.6311166 -2.6087811 -2.5973909 -2.5966709 -2.5985477]]...]
INFO - root - 2017-12-13 08:51:52.721066: step 110, loss = 1.26, batch loss = 1.06 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:21s remains)
INFO - root - 2017-12-13 08:51:54.925162: step 120, loss = 1.33, batch loss = 1.12 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:14s remains)
INFO - root - 2017-12-13 08:51:57.148172: step 130, loss = 1.36, batch loss = 1.14 (36.0 examples/sec; 0.222 sec/batch; 20h:32m:24s remains)
INFO - root - 2017-12-13 08:51:59.342502: step 140, loss = 1.25, batch loss = 1.02 (38.4 examples/sec; 0.208 sec/batch; 19h:14m:55s remains)
INFO - root - 2017-12-13 08:52:01.518550: step 150, loss = 1.34, batch loss = 1.09 (35.5 examples/sec; 0.225 sec/batch; 20h:48m:20s remains)
INFO - root - 2017-12-13 08:52:03.720992: step 160, loss = 1.27, batch loss = 1.02 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:15s remains)
INFO - root - 2017-12-13 08:52:05.896357: step 170, loss = 1.21, batch loss = 0.97 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:42s remains)
INFO - root - 2017-12-13 08:52:08.094655: step 180, loss = 1.12, batch loss = 0.87 (36.1 examples/sec; 0.222 sec/batch; 20h:28m:06s remains)
INFO - root - 2017-12-13 08:52:10.294653: step 190, loss = 1.06, batch loss = 0.81 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:22s remains)
INFO - root - 2017-12-13 08:52:12.499497: step 200, loss = 1.05, batch loss = 0.79 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:12s remains)
2017-12-13 08:52:12.863288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007][-1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007 -1.112007]]...]
INFO - root - 2017-12-13 08:52:15.006303: step 210, loss = 1.04, batch loss = 0.78 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:10s remains)
INFO - root - 2017-12-13 08:52:17.185184: step 220, loss = 1.04, batch loss = 0.77 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:13s remains)
INFO - root - 2017-12-13 08:52:19.332921: step 230, loss = 1.02, batch loss = 0.75 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:58s remains)
INFO - root - 2017-12-13 08:52:21.523285: step 240, loss = 0.99, batch loss = 0.71 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:33s remains)
INFO - root - 2017-12-13 08:52:23.710431: step 250, loss = 1.00, batch loss = 0.72 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:51s remains)
INFO - root - 2017-12-13 08:52:25.921587: step 260, loss = 1.01, batch loss = 0.71 (36.9 examples/sec; 0.217 sec/batch; 20h:02m:04s remains)
INFO - root - 2017-12-13 08:52:28.094771: step 270, loss = 1.03, batch loss = 0.73 (36.0 examples/sec; 0.222 sec/batch; 20h:31m:18s remains)
INFO - root - 2017-12-13 08:52:30.338307: step 280, loss = 0.99, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 21h:05m:04s remains)
INFO - root - 2017-12-13 08:52:32.534617: step 290, loss = 1.01, batch loss = 0.71 (34.2 examples/sec; 0.234 sec/batch; 21h:36m:39s remains)
INFO - root - 2017-12-13 08:52:34.701675: step 300, loss = 1.02, batch loss = 0.72 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:55s remains)
2017-12-13 08:52:34.988432: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.083656281 -0.044712305 -0.19777131 -0.12848067 -0.11269945 -0.052530736 -0.19786663 -0.1977541 -0.0906558 -0.089319319 0.049124211 0.037031084 -0.085003972 -0.12391314 -0.071425229][-0.16887641 -0.14428699 -0.18913336 -0.17341909 -0.24054818 -0.26921815 -0.37762293 -0.27720529 -0.35169768 -0.32794517 -0.26911765 -0.12744111 -0.04756695 0.030332953 -0.18289047][-0.40321246 -0.25307977 -0.20305158 0.09046486 -0.053103715 -0.096632272 -0.24613017 -0.30920497 -0.30580163 -0.15997338 -0.10287952 -0.22031645 -0.26086622 -0.22976421 -0.11443129][-0.062568188 0.077321976 -0.16816121 -0.0221515 -0.071098506 -0.039465785 -0.01924634 -0.052293837 -0.25180778 -0.15842158 -0.17559382 -0.18707895 -0.122123 -0.19313566 -0.2595602][-0.28129095 -0.16440198 -0.17707682 -0.14777017 -0.30827868 -0.10258967 -0.037037104 -0.25482553 -0.043848276 0.085477561 -0.072169393 -0.097154796 -0.14490601 -0.10437843 -0.011720031][-0.078749448 0.0031317174 -0.089957774 -0.089766562 -0.29435804 -0.30455363 -0.32509646 -0.17396662 0.03466785 -0.069675952 0.048386127 -0.06889078 -0.028190166 0.0599052 0.075527579][-0.28975448 -0.2733317 -0.20077369 -0.059521139 -0.07131359 0.095483512 0.12600413 0.090517551 0.037917107 -0.032027036 0.010316223 0.043004632 0.19345668 0.17820343 0.086508423][-0.24169075 -0.10198328 -0.069064289 -0.11355272 -0.17297378 -0.19398034 -0.11177331 -0.020903885 -0.023465395 -0.035129964 -0.028567195 -0.077012837 -0.083906919 -0.095405579 -0.064206123][-0.20209435 -0.22162123 -0.43355173 -0.34220654 -0.20787521 -0.20376304 0.056050181 -0.0081380606 -0.16877839 -0.23419183 -0.25444311 -0.31057253 -0.2807188 -0.28149033 -0.37475517][-0.28233963 -0.28557515 -0.28469712 -0.39585885 -0.43642622 -0.19998838 -0.0814518 -0.050717473 0.010049284 0.12063023 -0.054443389 -0.03265366 -0.27533919 -0.26236075 -0.12875783][0.049033642 -0.01083985 -0.19954631 -0.26552692 -0.28706735 -0.21342787 -0.10937518 -0.15080607 0.033870667 0.14642349 0.11859033 0.15701851 0.24574605 0.21015379 -0.051364869][0.22134545 0.24006411 0.13091776 0.012849212 -0.22902428 -0.24144822 -0.27911884 -0.20729999 0.027940184 0.0093702376 0.17410788 0.092346817 0.16378108 0.14018741 0.16175047][0.16035888 0.11226025 0.013938963 0.081905276 0.13592473 0.086093277 -0.11561066 -0.06528303 -0.055321842 -0.064314693 0.084160656 0.10618362 0.1205956 0.11877617 -0.019149542][0.11854967 0.10318968 0.15409824 -0.001750052 -0.10269916 -0.047169209 0.2026045 0.0032922924 0.093754858 0.11423025 -0.022516251 -0.026006103 0.12826911 -0.057675719 -0.20861796][0.13658622 0.1338248 0.17079136 0.16638204 0.182201 0.079782695 -0.19045168 -0.21863286 -0.14279076 0.0099101961 0.15762231 0.18299463 0.097793847 -0.043493629 0.0098102987]]...]
INFO - root - 2017-12-13 08:52:37.166087: step 310, loss = 1.00, batch loss = 0.70 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:50s remains)
INFO - root - 2017-12-13 08:52:39.400245: step 320, loss = 1.01, batch loss = 0.71 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:37s remains)
INFO - root - 2017-12-13 08:52:41.574339: step 330, loss = 1.01, batch loss = 0.71 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:11s remains)
INFO - root - 2017-12-13 08:52:43.729188: step 340, loss = 1.00, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:17s remains)
INFO - root - 2017-12-13 08:52:45.919390: step 350, loss = 1.05, batch loss = 0.73 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:22s remains)
INFO - root - 2017-12-13 08:52:48.071758: step 360, loss = 1.03, batch loss = 0.72 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:28s remains)
INFO - root - 2017-12-13 08:52:50.303293: step 370, loss = 1.02, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:24s remains)
INFO - root - 2017-12-13 08:52:52.506013: step 380, loss = 1.03, batch loss = 0.70 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:46s remains)
INFO - root - 2017-12-13 08:52:54.725952: step 390, loss = 1.02, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:19s remains)
INFO - root - 2017-12-13 08:52:56.979567: step 400, loss = 1.03, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:50s remains)
2017-12-13 08:52:57.293478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21116175 -0.21116176 -0.21116178 -0.21116178 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116178 -0.21116176 -0.21116175 -0.21116173 -0.21116173 -0.21116175 -0.21116176][-0.21116176 -0.21116176 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116178 -0.21116176 -0.21116173 -0.21116175 -0.21116175][-0.21116176 -0.21116176 -0.21116176 -0.21116178 -0.21116176 -0.21116176 -0.21116176 -0.21116178 -0.21116176 -0.21116176 -0.21116176 -0.21116178 -0.12309311 -0.13118851 -0.21116176][-0.21116175 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116178 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.12323026][-0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116178 -0.21116178 -0.10921229 -0.10749444 -0.21116175][-0.21116178 -0.21116176 -0.21116176 -0.21116175 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116175 -0.21116176 -0.11063437][-0.21116178 -0.21116176 -0.21116178 -0.21116178 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116178 -0.21116178 -0.21116176 -0.21116176 -0.21116176 -0.21116176][-0.21116175 -0.21116173 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116178 -0.21116178 -0.054287136 0.10788402][-0.24500288 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116175][-0.11085851 -0.10933556 -0.21116176 -0.21116175 -0.21116175 -0.21116176 -0.21116178 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116175][-0.21116175 -0.21116173 -0.21116175 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116178 -0.21116178 -0.21116176][-0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116178 -0.21116176 -0.21116175 -0.21116175 -0.21116175][-0.21116175 -0.21116173 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176 -0.21116176][-0.21116176 -0.055067822 0.10658196 -0.049513236 -0.21116175 -0.21116175 -0.21116175 -0.21116176 -0.21116176 -0.21116178 -0.21116178 -0.21116178 -0.21116176 -0.21116176 -0.21116176][-0.21116176 -0.21116176 -0.055150852 0.10654047 0.10663727 -0.04946965 -0.21116176 -0.21116176 -0.21116175 -0.21116176 -0.21116178 -0.21116179 -0.21116178 -0.21116176 -0.21116176]]...]
INFO - root - 2017-12-13 08:52:59.458310: step 410, loss = 1.04, batch loss = 0.70 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-13 08:53:01.664614: step 420, loss = 1.05, batch loss = 0.70 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:33s remains)
INFO - root - 2017-12-13 08:53:03.930359: step 430, loss = 1.03, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:36s remains)
INFO - root - 2017-12-13 08:53:06.161180: step 440, loss = 1.05, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:16s remains)
INFO - root - 2017-12-13 08:53:08.375048: step 450, loss = 1.07, batch loss = 0.70 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:06s remains)
INFO - root - 2017-12-13 08:53:10.664244: step 460, loss = 1.07, batch loss = 0.68 (34.6 examples/sec; 0.231 sec/batch; 21h:19m:11s remains)
INFO - root - 2017-12-13 08:53:12.887339: step 470, loss = 1.10, batch loss = 0.70 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:21s remains)
INFO - root - 2017-12-13 08:53:15.098018: step 480, loss = 1.09, batch loss = 0.70 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:33s remains)
INFO - root - 2017-12-13 08:53:17.302246: step 490, loss = 1.09, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:46s remains)
INFO - root - 2017-12-13 08:53:19.513825: step 500, loss = 1.08, batch loss = 0.68 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:56s remains)
2017-12-13 08:53:19.804508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10298787 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.084093332 -0.08668939 -0.080545813 -0.11591161 -0.10711712 -0.086252682 -0.095287651 -0.087372683][-0.087487713 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.10298786 -0.10298786 -0.08104042 -0.061370563 -0.060208004 -0.06058481 -0.11230379 -0.094464093 -0.054689825][-0.072061628 -0.088998474 -0.082764052 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298787 -0.10298787 -0.10298787 -0.077470981 -0.075861834 -0.075462185 -0.077243641 -0.094375432][-0.082466081 -0.088865481 -0.059333663 -0.086623825 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298787 -0.10298787 -0.10298787 -0.10298787 -0.10298787][-0.10298785 -0.10298785 -0.059807763 -0.060738862 -0.058466468 -0.056956239 -0.077692769 -0.077919923 -0.094401151 -0.085254058 -0.084658973 -0.081354819 -0.086195871 -0.086975813 -0.086710162][-0.10298785 -0.10298783 -0.10298785 -0.10298784 -0.059509512 -0.059210479 -0.068478495 -0.074210763 -0.07333269 -0.0781578 -0.089307956 -0.055089612 -0.060079049 -0.059719793 -0.056942642][-0.10298785 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.059086028 -0.055318657 -0.088891894 -0.094879016 -0.078091875 -0.08379332 -0.080887035 -0.055401228 -0.057961736][-0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.075963795 -0.077967532 -0.10298785 -0.10298785 -0.051371977 -0.054992318 -0.058495782][-0.10298786 -0.10298786 -0.10298785 -0.10298785 -0.10298785 -0.10298786 -0.10298786 -0.10298785 -0.10298786 -0.11750405 -0.10298785 -0.10298785 -0.10298786 -0.10298786 -0.076783054][-0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.10089538 -0.10462938 -0.10298785 -0.11775995 -0.10298784 -0.10298786 -0.10298786][-0.10298786 -0.10298785 -0.10298785 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10813725 -0.10298785 -0.10298785 -0.10028891 -0.10445318 -0.10298785 -0.10298784 -0.10298785][-0.10298785 -0.10298785 -0.10298785 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.0192236 -0.10298785 -0.10298784 -0.10298783 -0.10298785 -0.10298786][-0.10298785 -0.10298784 -0.10298784 -0.10298784 -0.10298785 -0.10298786 -0.10298786 -0.10298785 -0.10298785 -0.10298785 -0.10298785 -0.10298785 -0.10298785 -0.10298784 -0.10298783][-0.10298786 -0.10298786 -0.10298786 -0.10298784 -0.10298785 -0.10298785 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298786 -0.10298785 -0.10298786 -0.10298785 -0.10298785][-0.10298786 -0.10298787 -0.10298786 -0.10298786 -0.10298785 -0.10298785 -0.10298786 -0.10298785 -0.10298785 -0.10298786 -0.10939374 -0.10298786 -0.10298786 -0.10298786 -0.10298785]]...]
INFO - root - 2017-12-13 08:53:22.064644: step 510, loss = 1.07, batch loss = 0.66 (35.5 examples/sec; 0.226 sec/batch; 20h:48m:09s remains)
INFO - root - 2017-12-13 08:53:24.266214: step 520, loss = 1.10, batch loss = 0.68 (36.3 examples/sec; 0.221 sec/batch; 20h:20m:35s remains)
INFO - root - 2017-12-13 08:53:26.502054: step 530, loss = 1.16, batch loss = 0.74 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:21s remains)
INFO - root - 2017-12-13 08:53:28.702703: step 540, loss = 1.11, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:42s remains)
INFO - root - 2017-12-13 08:53:30.925142: step 550, loss = 1.11, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 21h:30m:40s remains)
INFO - root - 2017-12-13 08:53:33.141704: step 560, loss = 1.12, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:36s remains)
INFO - root - 2017-12-13 08:53:35.370024: step 570, loss = 1.13, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:36s remains)
INFO - root - 2017-12-13 08:53:37.598846: step 580, loss = 1.16, batch loss = 0.70 (35.3 examples/sec; 0.226 sec/batch; 20h:52m:40s remains)
INFO - root - 2017-12-13 08:53:39.829004: step 590, loss = 1.16, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:17s remains)
INFO - root - 2017-12-13 08:53:42.065395: step 600, loss = 1.16, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:40s remains)
2017-12-13 08:53:42.422185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373][-0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373 -0.049667373]]...]
INFO - root - 2017-12-13 08:53:44.646158: step 610, loss = 1.16, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:19s remains)
INFO - root - 2017-12-13 08:53:46.876503: step 620, loss = 1.15, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:36s remains)
INFO - root - 2017-12-13 08:53:49.081651: step 630, loss = 1.16, batch loss = 0.70 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:16s remains)
INFO - root - 2017-12-13 08:53:51.290919: step 640, loss = 1.17, batch loss = 0.70 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:03s remains)
INFO - root - 2017-12-13 08:53:53.517715: step 650, loss = 1.18, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:34s remains)
INFO - root - 2017-12-13 08:53:55.697684: step 660, loss = 1.18, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:07s remains)
INFO - root - 2017-12-13 08:53:57.927012: step 670, loss = 1.18, batch loss = 0.69 (37.1 examples/sec; 0.215 sec/batch; 19h:51m:29s remains)
INFO - root - 2017-12-13 08:54:00.136904: step 680, loss = 1.18, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:07s remains)
INFO - root - 2017-12-13 08:54:02.339999: step 690, loss = 1.18, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:32s remains)
INFO - root - 2017-12-13 08:54:04.602238: step 700, loss = 1.19, batch loss = 0.69 (34.1 examples/sec; 0.234 sec/batch; 21h:36m:36s remains)
2017-12-13 08:54:04.929804: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12320699 0.15887266 0.12676167 0.08193253 0.071889371 0.059931856 0.087543085 0.095653519 0.095371991 0.11078794 0.13322957 0.13962036 0.12626155 0.13079312 0.14134598][0.19543642 0.21070129 0.18819134 0.1821157 0.16791032 0.18544422 0.2272792 0.25245884 0.26345819 0.24912724 0.24300146 0.20774221 0.18998726 0.19070575 0.18966641][0.12351944 0.13056077 0.15153071 0.18246315 0.2314471 0.2867668 0.29531437 0.29360932 0.28169298 0.27167872 0.2559469 0.20961064 0.22746882 0.24208516 0.24688953][0.14748342 0.12572792 0.14368773 0.18117945 0.21645994 0.27279839 0.32014868 0.34387606 0.35569298 0.38210866 0.34859976 0.33956829 0.33378771 0.30562231 0.34480512][0.1473254 0.23674715 0.22886838 0.2681458 0.31973818 0.36737785 0.43750566 0.43384188 0.44330165 0.44537824 0.4280999 0.41544962 0.38465765 0.38382092 0.34326881][0.3067936 0.31596196 0.29444873 0.33032748 0.34899813 0.40968445 0.46580568 0.54819822 0.54985064 0.48023805 0.45465273 0.46381977 0.45986006 0.44741911 0.40548345][0.31676996 0.44724536 0.51661164 0.51213956 0.50979435 0.53409553 0.5500142 0.58027422 0.548522 0.59144956 0.56413043 0.48568243 0.46541974 0.45308721 0.39949039][0.51100349 0.52389705 0.55836678 0.60733396 0.585924 0.54143608 0.57797867 0.59751546 0.65088522 0.65086335 0.58189195 0.56687397 0.45919392 0.38451821 0.38569039][0.52063113 0.53509694 0.56103843 0.57679135 0.57223344 0.56578857 0.58545423 0.55361 0.55737007 0.58984441 0.66771078 0.64151633 0.5270142 0.45812768 0.35461682][0.59020627 0.61398095 0.6369561 0.64481384 0.65732962 0.60929877 0.58702505 0.5309189 0.51762712 0.505703 0.51799667 0.54314137 0.54973888 0.49065959 0.38728398][0.53808725 0.57872319 0.65531564 0.63411254 0.63793027 0.61585295 0.58110428 0.51956791 0.50053591 0.46208522 0.48315018 0.44282231 0.43301407 0.45388722 0.43972906][0.5451954 0.54840887 0.5677343 0.54282504 0.55325383 0.537623 0.52881914 0.49563384 0.4263975 0.36664844 0.39536211 0.33727568 0.37417144 0.4116846 0.46510959][0.52550876 0.59040135 0.59501153 0.51734269 0.48502141 0.44205487 0.40503913 0.39832082 0.39421093 0.35683706 0.34443951 0.28149641 0.29971698 0.33129337 0.44965598][0.38747185 0.53367645 0.51213282 0.53191215 0.45958623 0.40511504 0.33974212 0.26893464 0.2243962 0.19920978 0.22152062 0.28078011 0.34984291 0.39808282 0.48034692][0.26910627 0.36174157 0.47720256 0.54903626 0.47919685 0.44198808 0.33993345 0.27456287 0.19328386 0.10458103 0.13594612 0.22713058 0.35233414 0.46706551 0.5462265]]...]
INFO - root - 2017-12-13 08:54:07.118786: step 710, loss = 1.20, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:07s remains)
INFO - root - 2017-12-13 08:54:09.286054: step 720, loss = 1.23, batch loss = 0.72 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:36s remains)
INFO - root - 2017-12-13 08:54:11.507195: step 730, loss = 1.20, batch loss = 0.69 (33.6 examples/sec; 0.238 sec/batch; 21h:55m:26s remains)
INFO - root - 2017-12-13 08:54:13.737716: step 740, loss = 1.21, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-13 08:54:15.944548: step 750, loss = 1.22, batch loss = 0.70 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:23s remains)
INFO - root - 2017-12-13 08:54:18.144863: step 760, loss = 1.27, batch loss = 0.74 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:05s remains)
INFO - root - 2017-12-13 08:54:20.357413: step 770, loss = 1.25, batch loss = 0.70 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:16s remains)
INFO - root - 2017-12-13 08:54:22.572225: step 780, loss = 1.25, batch loss = 0.70 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:06s remains)
INFO - root - 2017-12-13 08:54:24.759501: step 790, loss = 1.25, batch loss = 0.69 (36.3 examples/sec; 0.221 sec/batch; 20h:19m:36s remains)
INFO - root - 2017-12-13 08:54:27.018244: step 800, loss = 1.26, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:14s remains)
2017-12-13 08:54:27.334836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024527671 -0.024527641 -0.024527613 -0.024527626 -0.016175244 -0.061255626 -0.10427189 -0.15670773 -0.18259089 -0.17545684 -0.32424018 -0.070466347 -0.024527645 -0.010807398 0.19113788][0.071606509 -0.024527656 -0.18839692 -0.17459522 -0.28105128 -0.27885905 -0.26647046 -0.26616225 -0.28674102 -0.27854002 -0.0007510446 -0.17159282 -0.169429 -0.034306515 0.025565723][0.025412226 -0.024527667 -0.024527675 -0.024527667 -0.32390696 -0.3237758 -0.50262809 -0.49510643 -0.48629239 -0.48250455 -0.23549218 -0.1842747 -0.049929395 -0.16531336 -0.10964938][-0.024527641 -0.024527652 -0.024527663 -0.02452768 -0.02452765 -0.024527656 -0.16043431 -0.17182942 -0.24639733 -0.21412773 -0.33317131 -0.33056736 -0.024527628 -0.02452765 -0.065883018][-0.066609114 -0.024527648 -0.024527656 -0.024527676 -0.024527669 -0.024527671 -0.024527658 -0.024527656 -0.024527652 -0.024527656 -0.02452763 0.0056431126 -0.10301095 -0.10084902 -0.1203491][-0.024527648 -0.024527648 -0.024527656 -0.024527678 -0.024527671 -0.024527676 -0.024527684 -0.024527654 -0.024527609 -0.024527626 -0.024527639 -0.024527648 -0.02452763 -0.024527628 -0.024527641][-0.024527626 -0.024527626 -0.024527622 -0.024527648 -0.024527673 -0.0245277 -0.024527697 -0.024527697 -0.024527654 -0.024527613 -0.024527621 -0.024527624 -0.024527637 -0.26202253 -0.11170932][-0.024527626 -0.024527634 -0.024527634 -0.024527634 -0.024527663 -0.024527686 -0.024527662 -0.024527689 -0.024527663 -0.024527663 -0.024527682 -0.024527645 -0.024527656 -0.024527641 -0.25346744][-0.024527656 -0.024527641 -0.024527634 -0.024527634 -0.024527652 -0.024527665 -0.024527639 -0.024527632 -0.024527645 -0.02452763 -0.024527663 -0.024527675 -0.024527699 -0.024527676 -0.024527637][-0.024527667 -0.024527656 -0.024527648 -0.024527626 -0.024527673 -0.024527643 -0.024527667 -0.024527656 -0.02452766 -0.024527645 -0.024527662 -0.024527654 -0.024527675 -0.024527665 -0.024527673][-0.024527641 -0.024527648 -0.024527654 -0.02452766 -0.024527656 -0.024527675 -0.024527693 -0.024527665 -0.024527663 -0.024527641 -0.024527635 -0.024527663 -0.024527678 -0.02452765 0.016472595][-0.02452763 -0.024527626 -0.024527626 -0.024527656 -0.024527686 -0.024527676 -0.024527671 -0.024527673 -0.024527691 -0.024527667 -0.024527667 -0.024527639 -0.024527604 -0.024527639 -0.024527662][-0.024527628 -0.024527624 -0.024527615 -0.024527622 -0.024527637 -0.02452765 -0.024527669 -0.02452766 0.023295211 0.050106615 -0.024527675 -0.024527656 -0.024527632 -0.024527632 -0.024527639][-0.024527643 -0.024527641 -0.024527641 -0.024527641 -0.024527665 -0.024527648 -0.024527656 -0.024527663 -0.02452766 0.061044723 0.096758954 0.04844071 -0.02452766 -0.02452765 -0.024527643][-0.024527671 -0.024527652 -0.024527628 -0.02452763 -0.024527648 -0.024527676 -0.024527673 -0.079217292 -0.024527652 -0.024527648 0.013035184 0.012173163 -0.024527654 -0.024527637 -0.024527637]]...]
INFO - root - 2017-12-13 08:54:29.537358: step 810, loss = 1.27, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:42s remains)
INFO - root - 2017-12-13 08:54:31.716907: step 820, loss = 1.26, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:51s remains)
INFO - root - 2017-12-13 08:54:33.907500: step 830, loss = 1.27, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:31s remains)
INFO - root - 2017-12-13 08:54:36.096538: step 840, loss = 1.27, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:20s remains)
INFO - root - 2017-12-13 08:54:38.308613: step 850, loss = 1.27, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:23s remains)
INFO - root - 2017-12-13 08:54:40.517314: step 860, loss = 1.28, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 21h:01m:55s remains)
INFO - root - 2017-12-13 08:54:42.694120: step 870, loss = 1.28, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:04s remains)
INFO - root - 2017-12-13 08:54:44.839330: step 880, loss = 1.29, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-13 08:54:47.028246: step 890, loss = 1.29, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:26s remains)
INFO - root - 2017-12-13 08:54:49.214273: step 900, loss = 1.29, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:18s remains)
2017-12-13 08:54:49.571584: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483][0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483 0.0003915483]]...]
INFO - root - 2017-12-13 08:54:51.726616: step 910, loss = 1.30, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:35s remains)
INFO - root - 2017-12-13 08:54:53.901889: step 920, loss = 1.31, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:57s remains)
INFO - root - 2017-12-13 08:54:56.068438: step 930, loss = 1.32, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:53s remains)
INFO - root - 2017-12-13 08:54:58.219773: step 940, loss = 1.33, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:03s remains)
INFO - root - 2017-12-13 08:55:00.366130: step 950, loss = 1.33, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-13 08:55:02.576967: step 960, loss = 1.35, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:46s remains)
INFO - root - 2017-12-13 08:55:04.757173: step 970, loss = 1.35, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:11s remains)
INFO - root - 2017-12-13 08:55:06.959053: step 980, loss = 1.37, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:06s remains)
INFO - root - 2017-12-13 08:55:09.174867: step 990, loss = 1.37, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:23s remains)
INFO - root - 2017-12-13 08:55:11.363200: step 1000, loss = 1.36, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:17s remains)
2017-12-13 08:55:11.676483: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851][0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851 0.0027875851]]...]
INFO - root - 2017-12-13 08:55:13.908968: step 1010, loss = 1.37, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:51s remains)
INFO - root - 2017-12-13 08:55:16.065574: step 1020, loss = 1.37, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:31s remains)
INFO - root - 2017-12-13 08:55:18.234345: step 1030, loss = 1.37, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 08:55:20.410976: step 1040, loss = 1.39, batch loss = 0.70 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:36s remains)
INFO - root - 2017-12-13 08:55:22.582893: step 1050, loss = 1.40, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:54s remains)
INFO - root - 2017-12-13 08:55:24.795990: step 1060, loss = 1.40, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:03s remains)
INFO - root - 2017-12-13 08:55:26.998349: step 1070, loss = 1.40, batch loss = 0.69 (33.7 examples/sec; 0.237 sec/batch; 21h:50m:01s remains)
INFO - root - 2017-12-13 08:55:29.151413: step 1080, loss = 1.41, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:29s remains)
INFO - root - 2017-12-13 08:55:31.333578: step 1090, loss = 1.41, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:21s remains)
INFO - root - 2017-12-13 08:55:33.555312: step 1100, loss = 1.42, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:19s remains)
2017-12-13 08:55:33.859586: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933][0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933 0.0056532933]]...]
INFO - root - 2017-12-13 08:55:35.973720: step 1110, loss = 1.41, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:10s remains)
INFO - root - 2017-12-13 08:55:38.136002: step 1120, loss = 1.42, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:48s remains)
INFO - root - 2017-12-13 08:55:40.293185: step 1130, loss = 1.41, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:35s remains)
INFO - root - 2017-12-13 08:55:42.522979: step 1140, loss = 1.41, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:07s remains)
INFO - root - 2017-12-13 08:55:44.665736: step 1150, loss = 1.42, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:24s remains)
INFO - root - 2017-12-13 08:55:46.822706: step 1160, loss = 1.42, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:20s remains)
INFO - root - 2017-12-13 08:55:48.982221: step 1170, loss = 1.42, batch loss = 0.70 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-13 08:55:51.144887: step 1180, loss = 1.42, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-13 08:55:53.306961: step 1190, loss = 1.43, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-13 08:55:55.502483: step 1200, loss = 1.43, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:11s remains)
2017-12-13 08:55:55.833707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607][-0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607 -0.0002027607]]...]
INFO - root - 2017-12-13 08:55:57.943476: step 1210, loss = 1.43, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:27m:02s remains)
INFO - root - 2017-12-13 08:56:00.095464: step 1220, loss = 1.43, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:50s remains)
INFO - root - 2017-12-13 08:56:02.206309: step 1230, loss = 1.44, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:12s remains)
INFO - root - 2017-12-13 08:56:04.417419: step 1240, loss = 1.45, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:20s remains)
INFO - root - 2017-12-13 08:56:06.565558: step 1250, loss = 1.45, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:49s remains)
INFO - root - 2017-12-13 08:56:08.673394: step 1260, loss = 1.46, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:09m:24s remains)
INFO - root - 2017-12-13 08:56:10.815818: step 1270, loss = 1.46, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:10s remains)
INFO - root - 2017-12-13 08:56:12.974277: step 1280, loss = 1.46, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:27s remains)
INFO - root - 2017-12-13 08:56:15.096964: step 1290, loss = 1.46, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:44s remains)
INFO - root - 2017-12-13 08:56:17.213812: step 1300, loss = 1.46, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:48s remains)
2017-12-13 08:56:17.519004: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06][8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06 8.88488e-06]]...]
INFO - root - 2017-12-13 08:56:19.599291: step 1310, loss = 1.46, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-13 08:56:21.779774: step 1320, loss = 1.46, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:42s remains)
INFO - root - 2017-12-13 08:56:23.928802: step 1330, loss = 1.47, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:16s remains)
INFO - root - 2017-12-13 08:56:26.055741: step 1340, loss = 1.47, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:38s remains)
INFO - root - 2017-12-13 08:56:28.199918: step 1350, loss = 1.48, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:01s remains)
INFO - root - 2017-12-13 08:56:30.368800: step 1360, loss = 1.49, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:22s remains)
INFO - root - 2017-12-13 08:56:32.498478: step 1370, loss = 1.49, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-13 08:56:34.639051: step 1380, loss = 1.49, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-13 08:56:36.772466: step 1390, loss = 1.49, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:13s remains)
INFO - root - 2017-12-13 08:56:38.913573: step 1400, loss = 1.49, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:11s remains)
2017-12-13 08:56:39.270276: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938][0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938 0.00037361938]]...]
INFO - root - 2017-12-13 08:56:41.390607: step 1410, loss = 1.51, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:30s remains)
INFO - root - 2017-12-13 08:56:43.572438: step 1420, loss = 1.52, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:33s remains)
INFO - root - 2017-12-13 08:56:45.729119: step 1430, loss = 1.53, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:22s remains)
INFO - root - 2017-12-13 08:56:47.917065: step 1440, loss = 1.54, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:34s remains)
INFO - root - 2017-12-13 08:56:50.086349: step 1450, loss = 1.55, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:18m:32s remains)
INFO - root - 2017-12-13 08:56:52.218632: step 1460, loss = 1.56, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:35m:15s remains)
INFO - root - 2017-12-13 08:56:54.355538: step 1470, loss = 1.57, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:37s remains)
INFO - root - 2017-12-13 08:56:56.451451: step 1480, loss = 1.57, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:36s remains)
INFO - root - 2017-12-13 08:56:58.583207: step 1490, loss = 1.59, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:10m:39s remains)
INFO - root - 2017-12-13 08:57:00.717322: step 1500, loss = 1.59, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:02s remains)
2017-12-13 08:57:01.084672: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316][0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316 0.00011336316]]...]
INFO - root - 2017-12-13 08:57:03.192461: step 1510, loss = 1.59, batch loss = 0.69 (37.1 examples/sec; 0.215 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-13 08:57:05.355361: step 1520, loss = 1.61, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:56s remains)
INFO - root - 2017-12-13 08:57:07.443981: step 1530, loss = 1.62, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:32s remains)
INFO - root - 2017-12-13 08:57:09.619407: step 1540, loss = 1.63, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:04m:49s remains)
INFO - root - 2017-12-13 08:57:11.722082: step 1550, loss = 1.64, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:43m:17s remains)
INFO - root - 2017-12-13 08:57:13.852425: step 1560, loss = 1.66, batch loss = 0.69 (34.0 examples/sec; 0.235 sec/batch; 21h:38m:14s remains)
INFO - root - 2017-12-13 08:57:15.975784: step 1570, loss = 1.67, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:29s remains)
INFO - root - 2017-12-13 08:57:18.146194: step 1580, loss = 1.67, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-13 08:57:20.297246: step 1590, loss = 1.69, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:52s remains)
INFO - root - 2017-12-13 08:57:22.391418: step 1600, loss = 1.69, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:23s remains)
2017-12-13 08:57:22.741998: I tensorflow/core/kernels/logging_ops.cc:79] [[[4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05][4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05 4.8881531e-05]]...]
INFO - root - 2017-12-13 08:57:24.854963: step 1610, loss = 1.71, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:23s remains)
INFO - root - 2017-12-13 08:57:26.991061: step 1620, loss = 1.72, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:11m:46s remains)
INFO - root - 2017-12-13 08:57:29.078457: step 1630, loss = 1.73, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:13m:35s remains)
INFO - root - 2017-12-13 08:57:31.223463: step 1640, loss = 1.74, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:38s remains)
INFO - root - 2017-12-13 08:57:33.361776: step 1650, loss = 1.75, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:07m:01s remains)
INFO - root - 2017-12-13 08:57:35.501086: step 1660, loss = 1.77, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-13 08:57:37.623479: step 1670, loss = 1.79, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:28s remains)
INFO - root - 2017-12-13 08:57:39.808843: step 1680, loss = 1.81, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:14s remains)
INFO - root - 2017-12-13 08:57:41.957189: step 1690, loss = 1.82, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:59m:46s remains)
INFO - root - 2017-12-13 08:57:44.064984: step 1700, loss = 1.83, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:25m:20s remains)
2017-12-13 08:57:44.396537: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05][1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05 1.8235854e-05]]...]
INFO - root - 2017-12-13 08:57:46.540083: step 1710, loss = 1.84, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:52m:28s remains)
INFO - root - 2017-12-13 08:57:48.657447: step 1720, loss = 1.85, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:37m:18s remains)
INFO - root - 2017-12-13 08:57:50.785454: step 1730, loss = 1.86, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:14s remains)
INFO - root - 2017-12-13 08:57:52.903134: step 1740, loss = 1.88, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:59m:15s remains)
INFO - root - 2017-12-13 08:57:55.064390: step 1750, loss = 1.90, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:03m:29s remains)
INFO - root - 2017-12-13 08:57:57.178833: step 1760, loss = 1.94, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:42m:33s remains)
INFO - root - 2017-12-13 08:57:59.283029: step 1770, loss = 1.96, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:09m:38s remains)
INFO - root - 2017-12-13 08:58:01.411709: step 1780, loss = 1.99, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:11m:24s remains)
INFO - root - 2017-12-13 08:58:03.532878: step 1790, loss = 2.01, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:06s remains)
INFO - root - 2017-12-13 08:58:05.627298: step 1800, loss = 2.02, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:17m:47s remains)
2017-12-13 08:58:05.988289: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915][0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915 0.00014710915]]...]
INFO - root - 2017-12-13 08:58:08.114871: step 1810, loss = 2.04, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:21s remains)
INFO - root - 2017-12-13 08:58:10.271488: step 1820, loss = 2.06, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:08m:27s remains)
INFO - root - 2017-12-13 08:58:12.419358: step 1830, loss = 2.08, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:08m:39s remains)
INFO - root - 2017-12-13 08:58:14.500207: step 1840, loss = 2.10, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:41s remains)
INFO - root - 2017-12-13 08:58:16.670136: step 1850, loss = 2.12, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:03s remains)
INFO - root - 2017-12-13 08:58:18.770136: step 1860, loss = 2.15, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:04m:31s remains)
INFO - root - 2017-12-13 08:58:20.875978: step 1870, loss = 2.16, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:24s remains)
INFO - root - 2017-12-13 08:58:23.011925: step 1880, loss = 2.18, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:14s remains)
INFO - root - 2017-12-13 08:58:25.139867: step 1890, loss = 2.21, batch loss = 0.69 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:53s remains)
INFO - root - 2017-12-13 08:58:27.255666: step 1900, loss = 2.25, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:19s remains)
2017-12-13 08:58:27.579114: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05][6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05 6.1622915e-05]]...]
INFO - root - 2017-12-13 08:58:29.741481: step 1910, loss = 2.28, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:56s remains)
INFO - root - 2017-12-13 08:58:31.865223: step 1920, loss = 2.31, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:08m:29s remains)
INFO - root - 2017-12-13 08:58:34.056046: step 1930, loss = 2.33, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:39s remains)
INFO - root - 2017-12-13 08:58:36.159326: step 1940, loss = 2.35, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:17m:12s remains)
INFO - root - 2017-12-13 08:58:38.280636: step 1950, loss = 2.36, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:25m:18s remains)
INFO - root - 2017-12-13 08:58:40.371165: step 1960, loss = 2.38, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:24m:32s remains)
INFO - root - 2017-12-13 08:58:42.502366: step 1970, loss = 2.41, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:24s remains)
INFO - root - 2017-12-13 08:58:44.586434: step 1980, loss = 2.42, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:50m:51s remains)
INFO - root - 2017-12-13 08:58:46.685408: step 1990, loss = 2.44, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:11s remains)
INFO - root - 2017-12-13 08:58:48.812662: step 2000, loss = 2.46, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:26s remains)
2017-12-13 08:58:49.120162: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05][2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05 2.5430969e-05]]...]
INFO - root - 2017-12-13 08:58:51.268823: step 2010, loss = 2.49, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-13 08:58:53.369079: step 2020, loss = 2.50, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:40m:33s remains)
INFO - root - 2017-12-13 08:58:55.482949: step 2030, loss = 2.52, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:59s remains)
INFO - root - 2017-12-13 08:58:57.612736: step 2040, loss = 2.56, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 08:58:59.716756: step 2050, loss = 2.58, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:19s remains)
INFO - root - 2017-12-13 08:59:01.806562: step 2060, loss = 2.61, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:37s remains)
INFO - root - 2017-12-13 08:59:03.925958: step 2070, loss = 2.63, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-13 08:59:06.034907: step 2080, loss = 2.66, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:12m:13s remains)
INFO - root - 2017-12-13 08:59:08.164307: step 2090, loss = 2.69, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:41m:07s remains)
INFO - root - 2017-12-13 08:59:10.343278: step 2100, loss = 2.71, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:33m:23s remains)
2017-12-13 08:59:10.711009: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05][1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05 1.1914076e-05]]...]
INFO - root - 2017-12-13 08:59:12.847341: step 2110, loss = 2.74, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:02s remains)
INFO - root - 2017-12-13 08:59:14.936086: step 2120, loss = 2.76, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:47s remains)
INFO - root - 2017-12-13 08:59:17.024963: step 2130, loss = 2.77, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:20s remains)
INFO - root - 2017-12-13 08:59:19.158531: step 2140, loss = 2.80, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:40s remains)
INFO - root - 2017-12-13 08:59:21.287408: step 2150, loss = 2.82, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:37m:42s remains)
INFO - root - 2017-12-13 08:59:23.362058: step 2160, loss = 2.84, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:11s remains)
INFO - root - 2017-12-13 08:59:25.509325: step 2170, loss = 2.86, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:39s remains)
INFO - root - 2017-12-13 08:59:27.590329: step 2180, loss = 2.89, batch loss = 0.69 (39.9 examples/sec; 0.200 sec/batch; 18h:22m:48s remains)
INFO - root - 2017-12-13 08:59:29.676968: step 2190, loss = 2.92, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:30s remains)
INFO - root - 2017-12-13 08:59:31.779341: step 2200, loss = 2.97, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:55s remains)
2017-12-13 08:59:32.111922: I tensorflow/core/kernels/logging_ops.cc:79] [[[5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06][5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06 5.0651784e-06]]...]
INFO - root - 2017-12-13 08:59:34.174507: step 2210, loss = 2.98, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:11s remains)
INFO - root - 2017-12-13 08:59:36.273540: step 2220, loss = 3.00, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:40s remains)
INFO - root - 2017-12-13 08:59:38.380232: step 2230, loss = 3.03, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:12s remains)
INFO - root - 2017-12-13 08:59:40.454282: step 2240, loss = 3.05, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:00m:00s remains)
INFO - root - 2017-12-13 08:59:42.527171: step 2250, loss = 3.07, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:08m:21s remains)
INFO - root - 2017-12-13 08:59:44.663408: step 2260, loss = 3.08, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:28s remains)
INFO - root - 2017-12-13 08:59:46.744142: step 2270, loss = 3.11, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:18s remains)
INFO - root - 2017-12-13 08:59:48.836965: step 2280, loss = 3.12, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-13 08:59:50.941350: step 2290, loss = 3.14, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:51m:03s remains)
INFO - root - 2017-12-13 08:59:53.012075: step 2300, loss = 3.16, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:45s remains)
2017-12-13 08:59:53.382028: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06][1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06 1.5533006e-06]]...]
INFO - root - 2017-12-13 08:59:55.448493: step 2310, loss = 3.18, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 18h:03m:24s remains)
INFO - root - 2017-12-13 08:59:57.545737: step 2320, loss = 3.21, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:57m:16s remains)
INFO - root - 2017-12-13 08:59:59.628952: step 2330, loss = 3.22, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:02m:54s remains)
INFO - root - 2017-12-13 09:00:01.697648: step 2340, loss = 3.24, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:09m:45s remains)
INFO - root - 2017-12-13 09:00:03.783612: step 2350, loss = 3.26, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:04s remains)
INFO - root - 2017-12-13 09:00:05.839090: step 2360, loss = 3.28, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-13 09:00:07.915562: step 2370, loss = 3.30, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:25m:47s remains)
INFO - root - 2017-12-13 09:00:09.973206: step 2380, loss = 3.31, batch loss = 0.69 (39.7 examples/sec; 0.201 sec/batch; 18h:28m:14s remains)
INFO - root - 2017-12-13 09:00:12.043619: step 2390, loss = 3.33, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:08m:20s remains)
INFO - root - 2017-12-13 09:00:14.109651: step 2400, loss = 3.35, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:39s remains)
2017-12-13 09:00:14.492337: I tensorflow/core/kernels/logging_ops.cc:79] [[[6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07][6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07 6.8476044e-07]]...]
INFO - root - 2017-12-13 09:00:16.573816: step 2410, loss = 3.37, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:16s remains)
INFO - root - 2017-12-13 09:00:18.683235: step 2420, loss = 3.40, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:57m:07s remains)
INFO - root - 2017-12-13 09:00:20.809142: step 2430, loss = 3.41, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:44m:38s remains)
INFO - root - 2017-12-13 09:00:22.920365: step 2440, loss = 3.43, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:18s remains)
INFO - root - 2017-12-13 09:00:25.020112: step 2450, loss = 3.46, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:50s remains)
INFO - root - 2017-12-13 09:00:27.129632: step 2460, loss = 3.48, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:47m:05s remains)
INFO - root - 2017-12-13 09:00:29.206885: step 2470, loss = 3.50, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:01m:07s remains)
INFO - root - 2017-12-13 09:00:31.307610: step 2480, loss = 3.51, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-13 09:00:33.399437: step 2490, loss = 3.52, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:37s remains)
INFO - root - 2017-12-13 09:00:35.478247: step 2500, loss = 3.54, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:19s remains)
2017-12-13 09:00:35.772359: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07][2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07 2.9388434e-07]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-13 09:00:38.225738: step 2510, loss = 3.57, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:21s remains)
INFO - root - 2017-12-13 09:00:40.339337: step 2520, loss = 3.58, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:12m:30s remains)
INFO - root - 2017-12-13 09:00:42.433718: step 2530, loss = 3.60, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:18s remains)
INFO - root - 2017-12-13 09:00:44.560754: step 2540, loss = 3.62, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:45s remains)
INFO - root - 2017-12-13 09:00:46.639299: step 2550, loss = 3.62, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:00m:12s remains)
INFO - root - 2017-12-13 09:00:48.722414: step 2560, loss = 3.64, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:18m:27s remains)
INFO - root - 2017-12-13 09:00:50.769201: step 2570, loss = 3.66, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:29m:53s remains)
INFO - root - 2017-12-13 09:00:52.826957: step 2580, loss = 3.68, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:43s remains)
INFO - root - 2017-12-13 09:00:54.916066: step 2590, loss = 3.70, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:11m:44s remains)
INFO - root - 2017-12-13 09:00:57.013384: step 2600, loss = 3.72, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:56s remains)
2017-12-13 09:00:57.325666: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07][1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07 1.6159312e-07]]...]
INFO - root - 2017-12-13 09:00:59.421609: step 2610, loss = 3.73, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:25s remains)
INFO - root - 2017-12-13 09:01:01.518645: step 2620, loss = 3.75, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:08s remains)
INFO - root - 2017-12-13 09:01:03.630838: step 2630, loss = 3.78, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:51s remains)
INFO - root - 2017-12-13 09:01:05.700731: step 2640, loss = 3.79, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:12m:50s remains)
INFO - root - 2017-12-13 09:01:07.769851: step 2650, loss = 3.82, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:43m:24s remains)
INFO - root - 2017-12-13 09:01:09.892293: step 2660, loss = 3.84, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:42s remains)
INFO - root - 2017-12-13 09:01:11.956071: step 2670, loss = 3.86, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:19s remains)
INFO - root - 2017-12-13 09:01:14.027481: step 2680, loss = 3.88, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:38m:09s remains)
INFO - root - 2017-12-13 09:01:16.077462: step 2690, loss = 3.89, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:43m:16s remains)
INFO - root - 2017-12-13 09:01:18.153392: step 2700, loss = 3.91, batch loss = 0.69 (38.9 examples/sec; 0.205 sec/batch; 18h:49m:20s remains)
2017-12-13 09:01:18.440949: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07][1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07 1.6190754e-07]]...]
INFO - root - 2017-12-13 09:01:20.506642: step 2710, loss = 3.94, batch loss = 0.69 (36.3 examples/sec; 0.221 sec/batch; 20h:12m:40s remains)
INFO - root - 2017-12-13 09:01:22.584987: step 2720, loss = 3.97, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:12s remains)
INFO - root - 2017-12-13 09:01:24.670894: step 2730, loss = 3.98, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:15m:47s remains)
INFO - root - 2017-12-13 09:01:26.760808: step 2740, loss = 4.00, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:18m:18s remains)
INFO - root - 2017-12-13 09:01:28.820943: step 2750, loss = 4.02, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:43m:26s remains)
INFO - root - 2017-12-13 09:01:30.894673: step 2760, loss = 4.04, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:06s remains)
INFO - root - 2017-12-13 09:01:33.028142: step 2770, loss = 4.07, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:47m:36s remains)
INFO - root - 2017-12-13 09:01:35.079176: step 2780, loss = 4.10, batch loss = 0.69 (40.1 examples/sec; 0.199 sec/batch; 18h:15m:13s remains)
INFO - root - 2017-12-13 09:01:37.136283: step 2790, loss = 4.12, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:17m:37s remains)
INFO - root - 2017-12-13 09:01:39.226017: step 2800, loss = 4.13, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:33m:51s remains)
2017-12-13 09:01:39.536895: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07][1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07 1.6240818e-07]]...]
INFO - root - 2017-12-13 09:01:41.614440: step 2810, loss = 4.16, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:40s remains)
INFO - root - 2017-12-13 09:01:43.698524: step 2820, loss = 4.17, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-13 09:01:45.762194: step 2830, loss = 4.18, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:31s remains)
INFO - root - 2017-12-13 09:01:47.818616: step 2840, loss = 4.19, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:12m:40s remains)
INFO - root - 2017-12-13 09:01:49.888991: step 2850, loss = 4.23, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:35m:44s remains)
INFO - root - 2017-12-13 09:01:51.945722: step 2860, loss = 4.26, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:57s remains)
INFO - root - 2017-12-13 09:01:54.004307: step 2870, loss = 4.28, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:53m:41s remains)
INFO - root - 2017-12-13 09:01:56.065311: step 2880, loss = 4.29, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:50s remains)
INFO - root - 2017-12-13 09:01:58.161811: step 2890, loss = 4.32, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:32s remains)
INFO - root - 2017-12-13 09:02:00.198290: step 2900, loss = 4.35, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:07m:26s remains)
2017-12-13 09:02:00.589069: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07][1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07 1.6680872e-07]]...]
INFO - root - 2017-12-13 09:02:02.686388: step 2910, loss = 4.38, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:49m:06s remains)
INFO - root - 2017-12-13 09:02:04.754479: step 2920, loss = 4.41, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:43m:22s remains)
INFO - root - 2017-12-13 09:02:06.819480: step 2930, loss = 4.42, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:07m:11s remains)
INFO - root - 2017-12-13 09:02:08.851045: step 2940, loss = 4.43, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:47s remains)
INFO - root - 2017-12-13 09:02:10.947044: step 2950, loss = 4.44, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:28m:22s remains)
INFO - root - 2017-12-13 09:02:13.028617: step 2960, loss = 4.46, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:59s remains)
INFO - root - 2017-12-13 09:02:15.122731: step 2970, loss = 4.48, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:35m:03s remains)
INFO - root - 2017-12-13 09:02:17.195271: step 2980, loss = 4.49, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:45m:49s remains)
INFO - root - 2017-12-13 09:02:19.256765: step 2990, loss = 4.50, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:03s remains)
INFO - root - 2017-12-13 09:02:21.301111: step 3000, loss = 4.52, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:03s remains)
2017-12-13 09:02:21.581406: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07][1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07 1.6428257e-07]]...]
INFO - root - 2017-12-13 09:02:23.710395: step 3010, loss = 4.54, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:21s remains)
INFO - root - 2017-12-13 09:02:25.785697: step 3020, loss = 4.58, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:34m:52s remains)
INFO - root - 2017-12-13 09:02:27.805470: step 3030, loss = 4.60, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:09m:07s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 09:02:29.886171: step 3040, loss = 4.62, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:36s remains)
INFO - root - 2017-12-13 09:02:31.943824: step 3050, loss = 4.63, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:39s remains)
INFO - root - 2017-12-13 09:02:34.023271: step 3060, loss = 4.65, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:22s remains)
INFO - root - 2017-12-13 09:02:36.133803: step 3070, loss = 4.67, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:12s remains)
INFO - root - 2017-12-13 09:02:38.220478: step 3080, loss = 4.68, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:01s remains)
INFO - root - 2017-12-13 09:02:40.320284: step 3090, loss = 4.69, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:42m:37s remains)
INFO - root - 2017-12-13 09:02:42.354439: step 3100, loss = 4.71, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:09m:09s remains)
2017-12-13 09:02:42.746887: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07][1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07 1.6484137e-07]]...]
INFO - root - 2017-12-13 09:02:44.789399: step 3110, loss = 4.72, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:09m:59s remains)
INFO - root - 2017-12-13 09:02:46.838667: step 3120, loss = 4.74, batch loss = 0.69 (41.2 examples/sec; 0.194 sec/batch; 17h:46m:37s remains)
INFO - root - 2017-12-13 09:02:48.863474: step 3130, loss = 4.76, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:54m:18s remains)
INFO - root - 2017-12-13 09:02:50.887312: step 3140, loss = 4.77, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:36m:56s remains)
INFO - root - 2017-12-13 09:02:52.949382: step 3150, loss = 4.78, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:27s remains)
INFO - root - 2017-12-13 09:02:55.023096: step 3160, loss = 4.79, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:26s remains)
INFO - root - 2017-12-13 09:02:57.084177: step 3170, loss = 4.80, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:51s remains)
INFO - root - 2017-12-13 09:02:59.131614: step 3180, loss = 4.81, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:09m:27s remains)
INFO - root - 2017-12-13 09:03:01.169348: step 3190, loss = 4.82, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:48s remains)
INFO - root - 2017-12-13 09:03:03.226480: step 3200, loss = 4.83, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:32m:48s remains)
2017-12-13 09:03:03.523949: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07][1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07 1.6699509e-07]]...]
INFO - root - 2017-12-13 09:03:05.563048: step 3210, loss = 4.84, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:21m:43s remains)
INFO - root - 2017-12-13 09:03:07.635540: step 3220, loss = 4.85, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:03s remains)
INFO - root - 2017-12-13 09:03:09.713075: step 3230, loss = 4.86, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:10s remains)
INFO - root - 2017-12-13 09:03:11.813519: step 3240, loss = 4.87, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:10m:20s remains)
INFO - root - 2017-12-13 09:03:13.864063: step 3250, loss = 4.89, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:38s remains)
INFO - root - 2017-12-13 09:03:15.931797: step 3260, loss = 4.90, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:02m:10s remains)
INFO - root - 2017-12-13 09:03:17.999848: step 3270, loss = 4.92, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:47m:53s remains)
INFO - root - 2017-12-13 09:03:20.051606: step 3280, loss = 4.93, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:04m:22s remains)
INFO - root - 2017-12-13 09:03:22.124712: step 3290, loss = 4.94, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:24s remains)
INFO - root - 2017-12-13 09:03:24.174529: step 3300, loss = 4.95, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:39m:42s remains)
2017-12-13 09:03:24.490557: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07][1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07 1.6446886e-07]]...]
INFO - root - 2017-12-13 09:03:26.561920: step 3310, loss = 4.96, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-13 09:03:28.596381: step 3320, loss = 4.97, batch loss = 0.69 (40.4 examples/sec; 0.198 sec/batch; 18h:07m:31s remains)
INFO - root - 2017-12-13 09:03:30.652692: step 3330, loss = 4.97, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:18m:15s remains)
INFO - root - 2017-12-13 09:03:32.686188: step 3340, loss = 4.98, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:37m:37s remains)
INFO - root - 2017-12-13 09:03:34.747614: step 3350, loss = 5.00, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:33s remains)
INFO - root - 2017-12-13 09:03:36.814160: step 3360, loss = 5.00, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:29m:05s remains)
INFO - root - 2017-12-13 09:03:38.850065: step 3370, loss = 5.01, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:52s remains)
INFO - root - 2017-12-13 09:03:40.911910: step 3380, loss = 5.02, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-13 09:03:42.965813: step 3390, loss = 5.04, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:23m:16s remains)
INFO - root - 2017-12-13 09:03:45.035683: step 3400, loss = 5.04, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:40s remains)
2017-12-13 09:03:45.347558: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07][1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07 1.6547004e-07]]...]
INFO - root - 2017-12-13 09:03:47.412350: step 3410, loss = 5.04, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:40m:50s remains)
INFO - root - 2017-12-13 09:03:49.453489: step 3420, loss = 5.05, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:44m:48s remains)
INFO - root - 2017-12-13 09:03:51.482679: step 3430, loss = 5.05, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:35m:30s remains)
INFO - root - 2017-12-13 09:03:53.507947: step 3440, loss = 5.06, batch loss = 0.69 (39.7 examples/sec; 0.201 sec/batch; 18h:24m:35s remains)
INFO - root - 2017-12-13 09:03:55.579018: step 3450, loss = 5.07, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:26m:11s remains)
INFO - root - 2017-12-13 09:03:57.628691: step 3460, loss = 5.07, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:47m:54s remains)
INFO - root - 2017-12-13 09:03:59.676731: step 3470, loss = 5.08, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:41s remains)
INFO - root - 2017-12-13 09:04:01.725743: step 3480, loss = 5.08, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:27s remains)
INFO - root - 2017-12-13 09:04:03.783943: step 3490, loss = 5.08, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:54m:55s remains)
INFO - root - 2017-12-13 09:04:05.839407: step 3500, loss = 5.08, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:22m:00s remains)
2017-12-13 09:04:06.165112: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07][1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07 1.6601717e-07]]...]
INFO - root - 2017-12-13 09:04:08.205677: step 3510, loss = 5.09, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:31m:57s remains)
INFO - root - 2017-12-13 09:04:10.300151: step 3520, loss = 5.10, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:08m:05s remains)
INFO - root - 2017-12-13 09:04:12.364573: step 3530, loss = 5.10, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:22m:13s remains)
INFO - root - 2017-12-13 09:04:14.400979: step 3540, loss = 5.10, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:51s remains)
INFO - root - 2017-12-13 09:04:16.454022: step 3550, loss = 5.11, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:29m:12s remains)
INFO - root - 2017-12-13 09:04:18.470856: step 3560, loss = 5.11, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:34m:48s remains)
INFO - root - 2017-12-13 09:04:20.524459: step 3570, loss = 5.12, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:34m:41s remains)
INFO - root - 2017-12-13 09:04:22.584742: step 3580, loss = 5.13, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:36m:25s remains)
INFO - root - 2017-12-13 09:04:24.689554: step 3590, loss = 5.13, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:16m:53s remains)
INFO - root - 2017-12-13 09:04:26.762898: step 3600, loss = 5.15, batch loss = 0.69 (40.4 examples/sec; 0.198 sec/batch; 18h:04m:19s remains)
2017-12-13 09:04:27.080245: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07][1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07 1.6530703e-07]]...]
INFO - root - 2017-12-13 09:04:29.146962: step 3610, loss = 5.15, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:28m:03s remains)
INFO - root - 2017-12-13 09:04:31.235481: step 3620, loss = 5.16, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:54m:20s remains)
INFO - root - 2017-12-13 09:04:33.315546: step 3630, loss = 5.16, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:15m:25s remains)
INFO - root - 2017-12-13 09:04:35.390464: step 3640, loss = 5.17, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:16m:02s remains)
INFO - root - 2017-12-13 09:04:37.444037: step 3650, loss = 5.18, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:57m:07s remains)
INFO - root - 2017-12-13 09:04:39.479587: step 3660, loss = 5.19, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:47m:05s remains)
INFO - root - 2017-12-13 09:04:41.530439: step 3670, loss = 5.19, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:20m:41s remains)
INFO - root - 2017-12-13 09:04:43.588291: step 3680, loss = 5.21, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:33m:04s remains)
INFO - root - 2017-12-13 09:04:45.643681: step 3690, loss = 5.21, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:19m:16s remains)
INFO - root - 2017-12-13 09:04:47.695174: step 3700, loss = 5.23, batch loss = 0.69 (40.1 examples/sec; 0.199 sec/batch; 18h:12m:40s remains)
2017-12-13 09:04:48.001461: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07][1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07 1.6507458e-07]]...]
INFO - root - 2017-12-13 09:04:50.068906: step 3710, loss = 5.24, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:30m:41s remains)
INFO - root - 2017-12-13 09:04:52.134195: step 3720, loss = 5.25, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 19h:58m:01s remains)
INFO - root - 2017-12-13 09:04:54.232811: step 3730, loss = 5.27, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:37s remains)
INFO - root - 2017-12-13 09:04:56.297681: step 3740, loss = 5.27, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:22m:33s remains)
INFO - root - 2017-12-13 09:04:58.330726: step 3750, loss = 5.29, batch loss = 0.69 (40.3 examples/sec; 0.198 sec/batch; 18h:07m:34s remains)
INFO - root - 2017-12-13 09:05:00.415958: step 3760, loss = 5.29, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:30m:18s remains)
INFO - root - 2017-12-13 09:05:02.442296: step 3770, loss = 5.30, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:20m:26s remains)
INFO - root - 2017-12-13 09:05:04.483048: step 3780, loss = 5.30, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:46m:47s remains)
INFO - root - 2017-12-13 09:05:06.568543: step 3790, loss = 5.31, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:54m:19s remains)
INFO - root - 2017-12-13 09:05:08.599103: step 3800, loss = 5.31, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:56m:00s remains)
2017-12-13 09:05:08.994389: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07][1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07 1.6570351e-07]]...]
INFO - root - 2017-12-13 09:05:11.057399: step 3810, loss = 5.33, batch loss = 0.69 (38.9 examples/sec; 0.205 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-13 09:05:13.114328: step 3820, loss = 5.34, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:54m:22s remains)
INFO - root - 2017-12-13 09:05:15.173419: step 3830, loss = 5.34, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:20s remains)
INFO - root - 2017-12-13 09:05:17.215636: step 3840, loss = 5.36, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:34m:43s remains)
INFO - root - 2017-12-13 09:05:19.264489: step 3850, loss = 5.37, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:40s remains)
INFO - root - 2017-12-13 09:05:21.349230: step 3860, loss = 5.37, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:08m:18s remains)
INFO - root - 2017-12-13 09:05:23.459486: step 3870, loss = 5.40, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:54m:31s remains)
INFO - root - 2017-12-13 09:05:25.472596: step 3880, loss = 5.41, batch loss = 0.69 (40.5 examples/sec; 0.197 sec/batch; 18h:01m:33s remains)
INFO - root - 2017-12-13 09:05:27.535923: step 3890, loss = 5.43, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:01m:49s remains)
INFO - root - 2017-12-13 09:05:29.566033: step 3900, loss = 5.45, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:47s remains)
2017-12-13 09:05:29.940177: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07][1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07 1.6204808e-07]]...]
INFO - root - 2017-12-13 09:05:31.973699: step 3910, loss = 5.47, batch loss = 0.69 (40.1 examples/sec; 0.199 sec/batch; 18h:11m:24s remains)
INFO - root - 2017-12-13 09:05:34.035823: step 3920, loss = 5.48, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:19m:09s remains)
INFO - root - 2017-12-13 09:05:36.066104: step 3930, loss = 5.49, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:46s remains)
INFO - root - 2017-12-13 09:05:38.129964: step 3940, loss = 5.50, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:26s remains)
INFO - root - 2017-12-13 09:05:40.183464: step 3950, loss = 5.52, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:18m:36s remains)
INFO - root - 2017-12-13 09:05:42.231510: step 3960, loss = 5.54, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:17m:52s remains)
INFO - root - 2017-12-13 09:05:44.278378: step 3970, loss = 5.56, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:51m:56s remains)
INFO - root - 2017-12-13 09:05:46.339528: step 3980, loss = 5.56, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:33m:43s remains)
INFO - root - 2017-12-13 09:05:48.387611: step 3990, loss = 5.57, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:20s remains)
INFO - root - 2017-12-13 09:05:50.418616: step 4000, loss = 5.57, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:39m:34s remains)
2017-12-13 09:05:50.712395: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07][1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07 1.6621571e-07]]...]
INFO - root - 2017-12-13 09:05:52.840788: step 4010, loss = 5.59, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:55s remains)
INFO - root - 2017-12-13 09:05:54.897282: step 4020, loss = 5.60, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:31s remains)
INFO - root - 2017-12-13 09:05:56.953161: step 4030, loss = 5.62, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:47m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 09:05:59.012466: step 4040, loss = 5.63, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:39m:45s remains)
INFO - root - 2017-12-13 09:06:01.073704: step 4050, loss = 5.63, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:11s remains)
INFO - root - 2017-12-13 09:06:03.134088: step 4060, loss = 5.63, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:06m:13s remains)
INFO - root - 2017-12-13 09:06:05.176528: step 4070, loss = 5.65, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:14m:31s remains)
INFO - root - 2017-12-13 09:06:07.269216: step 4080, loss = 5.65, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:47s remains)
INFO - root - 2017-12-13 09:06:09.344070: step 4090, loss = 5.66, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:49m:09s remains)
INFO - root - 2017-12-13 09:06:11.411693: step 4100, loss = 5.68, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:29m:46s remains)
2017-12-13 09:06:11.756591: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07][1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07 1.6376754e-07]]...]
INFO - root - 2017-12-13 09:06:13.814602: step 4110, loss = 5.70, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:32s remains)
INFO - root - 2017-12-13 09:06:15.880547: step 4120, loss = 5.71, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:34m:55s remains)
INFO - root - 2017-12-13 09:06:17.926106: step 4130, loss = 5.72, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:43m:35s remains)
INFO - root - 2017-12-13 09:06:19.986925: step 4140, loss = 5.73, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:05s remains)
INFO - root - 2017-12-13 09:06:22.060910: step 4150, loss = 5.73, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:35m:32s remains)
INFO - root - 2017-12-13 09:06:24.125598: step 4160, loss = 5.74, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:44m:54s remains)
INFO - root - 2017-12-13 09:06:26.183170: step 4170, loss = 5.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:12m:02s remains)
INFO - root - 2017-12-13 09:06:28.256629: step 4180, loss = 5.78, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:51m:02s remains)
INFO - root - 2017-12-13 09:06:30.292738: step 4190, loss = 5.80, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:40m:41s remains)
INFO - root - 2017-12-13 09:06:32.381154: step 4200, loss = 5.81, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:00m:27s remains)
2017-12-13 09:06:32.680581: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07][1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07 1.6581255e-07]]...]
INFO - root - 2017-12-13 09:06:34.746126: step 4210, loss = 5.83, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:54s remains)
INFO - root - 2017-12-13 09:06:36.802593: step 4220, loss = 5.85, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:33m:51s remains)
INFO - root - 2017-12-13 09:06:38.835407: step 4230, loss = 5.85, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:38m:23s remains)
INFO - root - 2017-12-13 09:06:40.899817: step 4240, loss = 5.86, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:27m:52s remains)
INFO - root - 2017-12-13 09:06:42.966071: step 4250, loss = 5.88, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:57m:44s remains)
INFO - root - 2017-12-13 09:06:45.019526: step 4260, loss = 5.89, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:58m:12s remains)
INFO - root - 2017-12-13 09:06:47.094045: step 4270, loss = 5.91, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:52m:47s remains)
INFO - root - 2017-12-13 09:06:49.166622: step 4280, loss = 5.93, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:15m:28s remains)
INFO - root - 2017-12-13 09:06:51.227387: step 4290, loss = 5.95, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:06s remains)
INFO - root - 2017-12-13 09:06:53.276113: step 4300, loss = 5.97, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:22s remains)
2017-12-13 09:06:53.592634: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07][1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07 1.6668568e-07]]...]
INFO - root - 2017-12-13 09:06:55.687520: step 4310, loss = 5.98, batch loss = 0.69 (40.7 examples/sec; 0.197 sec/batch; 17h:54m:49s remains)
INFO - root - 2017-12-13 09:06:57.752187: step 4320, loss = 5.99, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:40m:42s remains)
INFO - root - 2017-12-13 09:06:59.791783: step 4330, loss = 6.01, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:40m:17s remains)
INFO - root - 2017-12-13 09:07:01.839824: step 4340, loss = 6.02, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:05m:44s remains)
INFO - root - 2017-12-13 09:07:03.900176: step 4350, loss = 6.05, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:07m:00s remains)
INFO - root - 2017-12-13 09:07:06.002056: step 4360, loss = 6.07, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:17m:56s remains)
INFO - root - 2017-12-13 09:07:08.007943: step 4370, loss = 6.07, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:20m:24s remains)
INFO - root - 2017-12-13 09:07:10.098629: step 4380, loss = 6.09, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:46m:21s remains)
INFO - root - 2017-12-13 09:07:12.155737: step 4390, loss = 6.10, batch loss = 0.69 (40.7 examples/sec; 0.196 sec/batch; 17h:54m:06s remains)
INFO - root - 2017-12-13 09:07:14.200877: step 4400, loss = 6.11, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:12m:29s remains)
2017-12-13 09:07:14.501227: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07][1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07 1.6271593e-07]]...]
INFO - root - 2017-12-13 09:07:16.584642: step 4410, loss = 6.12, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:53m:44s remains)
INFO - root - 2017-12-13 09:07:18.621818: step 4420, loss = 6.13, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:26m:19s remains)
INFO - root - 2017-12-13 09:07:20.659885: step 4430, loss = 6.14, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:00m:17s remains)
INFO - root - 2017-12-13 09:07:22.701430: step 4440, loss = 6.15, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 18h:58m:53s remains)
INFO - root - 2017-12-13 09:07:24.788353: step 4450, loss = 6.17, batch loss = 0.69 (39.8 examples/sec; 0.201 sec/batch; 18h:19m:20s remains)
INFO - root - 2017-12-13 09:07:26.837164: step 4460, loss = 6.19, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:14m:30s remains)
INFO - root - 2017-12-13 09:07:28.898432: step 4470, loss = 6.19, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:10m:54s remains)
INFO - root - 2017-12-13 09:07:30.968503: step 4480, loss = 6.21, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:26m:48s remains)
INFO - root - 2017-12-13 09:07:33.033395: step 4490, loss = 6.22, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 18h:59m:44s remains)
INFO - root - 2017-12-13 09:07:35.086787: step 4500, loss = 6.24, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:34m:06s remains)
2017-12-13 09:07:35.413097: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07][1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07 1.6641795e-07]]...]
INFO - root - 2017-12-13 09:07:37.438897: step 4510, loss = 6.25, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:28m:55s remains)
INFO - root - 2017-12-13 09:07:39.462690: step 4520, loss = 6.26, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 17h:57m:59s remains)
INFO - root - 2017-12-13 09:07:41.536025: step 4530, loss = 6.27, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:09s remains)
INFO - root - 2017-12-13 09:07:43.603672: step 4540, loss = 6.31, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:08s remains)
INFO - root - 2017-12-13 09:07:45.689159: step 4550, loss = 6.32, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:55s remains)
INFO - root - 2017-12-13 09:07:47.751854: step 4560, loss = 6.33, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:13m:15s remains)
INFO - root - 2017-12-13 09:07:49.771489: step 4570, loss = 6.34, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:41m:00s remains)
INFO - root - 2017-12-13 09:07:51.847641: step 4580, loss = 6.35, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:30m:50s remains)
INFO - root - 2017-12-13 09:07:53.902679: step 4590, loss = 6.38, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:40m:06s remains)
INFO - root - 2017-12-13 09:07:55.950483: step 4600, loss = 6.40, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:27m:43s remains)
2017-12-13 09:07:56.252138: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07][1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07 1.6266939e-07]]...]
INFO - root - 2017-12-13 09:07:58.310776: step 4610, loss = 6.41, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:38m:32s remains)
INFO - root - 2017-12-13 09:08:00.371110: step 4620, loss = 6.43, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:44m:43s remains)
INFO - root - 2017-12-13 09:08:02.413129: step 4630, loss = 6.44, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:26s remains)
INFO - root - 2017-12-13 09:08:04.474698: step 4640, loss = 6.45, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:29m:55s remains)
INFO - root - 2017-12-13 09:08:06.546212: step 4650, loss = 6.46, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:52m:21s remains)
INFO - root - 2017-12-13 09:08:08.638685: step 4660, loss = 6.48, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:03m:08s remains)
INFO - root - 2017-12-13 09:08:10.698211: step 4670, loss = 6.49, batch loss = 0.69 (40.5 examples/sec; 0.198 sec/batch; 17h:59m:48s remains)
INFO - root - 2017-12-13 09:08:12.769675: step 4680, loss = 6.51, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:13m:46s remains)
INFO - root - 2017-12-13 09:08:14.848466: step 4690, loss = 6.53, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:33m:05s remains)
INFO - root - 2017-12-13 09:08:16.922483: step 4700, loss = 6.57, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:22m:07s remains)
2017-12-13 09:08:17.214823: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07][1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07 1.660222e-07]]...]
INFO - root - 2017-12-13 09:08:19.257633: step 4710, loss = 6.60, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:31m:06s remains)
INFO - root - 2017-12-13 09:08:21.291393: step 4720, loss = 6.63, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 17h:57m:08s remains)
INFO - root - 2017-12-13 09:08:23.354020: step 4730, loss = 6.65, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:32m:10s remains)
INFO - root - 2017-12-13 09:08:25.441603: step 4740, loss = 6.67, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:33s remains)
INFO - root - 2017-12-13 09:08:27.510687: step 4750, loss = 6.69, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:38m:08s remains)
INFO - root - 2017-12-13 09:08:29.586972: step 4760, loss = 6.70, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:31m:30s remains)
INFO - root - 2017-12-13 09:08:31.643434: step 4770, loss = 6.71, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:39m:52s remains)
INFO - root - 2017-12-13 09:08:33.694076: step 4780, loss = 6.73, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:02s remains)
INFO - root - 2017-12-13 09:08:35.762458: step 4790, loss = 6.74, batch loss = 0.69 (39.1 examples/sec; 0.205 sec/batch; 18h:38m:37s remains)
INFO - root - 2017-12-13 09:08:37.816966: step 4800, loss = 6.76, batch loss = 0.69 (40.4 examples/sec; 0.198 sec/batch; 18h:02m:28s remains)
2017-12-13 09:08:38.151436: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07][1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07 1.6615031e-07]]...]
INFO - root - 2017-12-13 09:08:40.243976: step 4810, loss = 6.79, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:09s remains)
INFO - root - 2017-12-13 09:08:42.308016: step 4820, loss = 6.80, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:46m:44s remains)
INFO - root - 2017-12-13 09:08:44.417672: step 4830, loss = 6.81, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 18h:58m:57s remains)
INFO - root - 2017-12-13 09:08:46.495973: step 4840, loss = 6.83, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 18h:56m:48s remains)
INFO - root - 2017-12-13 09:08:48.529770: step 4850, loss = 6.84, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:40m:24s remains)
INFO - root - 2017-12-13 09:08:50.557721: step 4860, loss = 6.85, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:46m:45s remains)
INFO - root - 2017-12-13 09:08:52.618445: step 4870, loss = 6.87, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:03m:15s remains)
INFO - root - 2017-12-13 09:08:54.680884: step 4880, loss = 6.90, batch loss = 0.69 (38.9 examples/sec; 0.205 sec/batch; 18h:41m:48s remains)
INFO - root - 2017-12-13 09:08:56.746560: step 4890, loss = 6.92, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:30m:06s remains)
INFO - root - 2017-12-13 09:08:58.844763: step 4900, loss = 6.93, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:49m:41s remains)
2017-12-13 09:08:59.184974: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07][1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07 1.6661605e-07]]...]
INFO - root - 2017-12-13 09:09:01.252285: step 4910, loss = 6.94, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:51m:44s remains)
INFO - root - 2017-12-13 09:09:03.341432: step 4920, loss = 6.96, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:58s remains)
INFO - root - 2017-12-13 09:09:05.417185: step 4930, loss = 6.98, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:32m:03s remains)
INFO - root - 2017-12-13 09:09:07.519693: step 4940, loss = 6.99, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:55m:13s remains)
INFO - root - 2017-12-13 09:09:09.582778: step 4950, loss = 6.99, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:46s remains)
INFO - root - 2017-12-13 09:09:11.634055: step 4960, loss = 7.01, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-13 09:09:13.700731: step 4970, loss = 7.03, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:15m:18s remains)
INFO - root - 2017-12-13 09:09:15.782422: step 4980, loss = 7.04, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:29s remains)
INFO - root - 2017-12-13 09:09:17.864547: step 4990, loss = 7.06, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:48m:54s remains)
INFO - root - 2017-12-13 09:09:19.908830: step 5000, loss = 7.07, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:52m:05s remains)
2017-12-13 09:09:20.249911: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07][1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07 1.6259978e-07]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-13 09:09:23.058876: step 5010, loss = 7.07, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:14m:55s remains)
INFO - root - 2017-12-13 09:09:25.140064: step 5020, loss = 7.08, batch loss = 0.69 (38.8 examples/sec; 0.206 sec/batch; 18h:44m:25s remains)
INFO - root - 2017-12-13 09:09:27.208819: step 5030, loss = 7.09, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 17h:55m:03s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4
INFO - root - 2017-12-13 09:09:29.282888: step 5040, loss = 7.11, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 18h:55m:35s remains)
INFO - root - 2017-12-13 09:09:31.345717: step 5050, loss = 7.12, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:25m:19s remains)
INFO - root - 2017-12-13 09:09:33.399788: step 5060, loss = 7.12, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:34m:03s remains)
INFO - root - 2017-12-13 09:09:35.464032: step 5070, loss = 7.12, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 18h:58m:18s remains)
INFO - root - 2017-12-13 09:09:37.512381: step 5080, loss = 7.13, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:45s remains)
INFO - root - 2017-12-13 09:09:39.592532: step 5090, loss = 7.13, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 19h:44m:44s remains)
INFO - root - 2017-12-13 09:09:41.624455: step 5100, loss = 7.13, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:09m:21s remains)
2017-12-13 09:09:41.923625: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07][1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07 1.6633675e-07]]...]
INFO - root - 2017-12-13 09:09:43.973996: step 5110, loss = 7.14, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 18h:58m:31s remains)
INFO - root - 2017-12-13 09:09:46.026038: step 5120, loss = 7.16, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:43m:24s remains)
INFO - root - 2017-12-13 09:09:48.054736: step 5130, loss = 7.17, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:00m:44s remains)
INFO - root - 2017-12-13 09:09:50.116367: step 5140, loss = 7.17, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:18m:50s remains)
INFO - root - 2017-12-13 09:09:52.190543: step 5150, loss = 7.17, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:32m:18s remains)
INFO - root - 2017-12-13 09:09:54.279787: step 5160, loss = 7.19, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:27m:54s remains)
INFO - root - 2017-12-13 09:09:56.353649: step 5170, loss = 7.19, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:03s remains)
INFO - root - 2017-12-13 09:09:58.398895: step 5180, loss = 7.20, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 18h:59m:34s remains)
INFO - root - 2017-12-13 09:10:00.467578: step 5190, loss = 7.20, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:27m:30s remains)
INFO - root - 2017-12-13 09:10:02.519736: step 5200, loss = 7.21, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:35m:07s remains)
2017-12-13 09:10:02.850777: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07][1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07 1.6400843e-07]]...]
INFO - root - 2017-12-13 09:10:04.897796: step 5210, loss = 7.23, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:51m:21s remains)
INFO - root - 2017-12-13 09:10:06.970882: step 5220, loss = 7.24, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:23m:30s remains)
INFO - root - 2017-12-13 09:10:09.047080: step 5230, loss = 7.25, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 18h:59m:27s remains)
INFO - root - 2017-12-13 09:10:11.087093: step 5240, loss = 7.25, batch loss = 0.69 (40.1 examples/sec; 0.200 sec/batch; 18h:09m:20s remains)
INFO - root - 2017-12-13 09:10:13.147993: step 5250, loss = 7.26, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:29m:27s remains)
INFO - root - 2017-12-13 09:10:15.174642: step 5260, loss = 7.26, batch loss = 0.69 (38.7 examples/sec; 0.207 sec/batch; 18h:47m:02s remains)
INFO - root - 2017-12-13 09:10:17.216001: step 5270, loss = 7.26, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:26m:25s remains)
INFO - root - 2017-12-13 09:10:19.277079: step 5280, loss = 7.26, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:32m:20s remains)
INFO - root - 2017-12-13 09:10:21.341502: step 5290, loss = 7.26, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:03m:31s remains)
INFO - root - 2017-12-13 09:10:23.389287: step 5300, loss = 7.26, batch loss = 0.69 (39.5 examples/sec; 0.202 sec/batch; 18h:23m:23s remains)
2017-12-13 09:10:23.730408: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07][1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07 1.6239029e-07]]...]
INFO - root - 2017-12-13 09:10:25.827483: step 5310, loss = 7.27, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:34m:20s remains)
INFO - root - 2017-12-13 09:10:27.864222: step 5320, loss = 7.28, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-13 09:10:29.928854: step 5330, loss = 7.29, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:22s remains)
INFO - root - 2017-12-13 09:10:31.955496: step 5340, loss = 7.30, batch loss = 0.69 (39.2 examples/sec; 0.204 sec/batch; 18h:32m:53s remains)
INFO - root - 2017-12-13 09:10:34.047321: step 5350, loss = 7.31, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:37m:23s remains)
INFO - root - 2017-12-13 09:10:36.125564: step 5360, loss = 7.31, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:18m:27s remains)
INFO - root - 2017-12-13 09:10:38.148742: step 5370, loss = 7.31, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:25m:59s remains)
INFO - root - 2017-12-13 09:10:40.217429: step 5380, loss = 7.32, batch loss = 0.69 (39.3 examples/sec; 0.204 sec/batch; 18h:29m:52s remains)
INFO - root - 2017-12-13 09:10:42.268747: step 5390, loss = 7.33, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 17h:54m:01s remains)
INFO - root - 2017-12-13 09:10:44.319331: step 5400, loss = 7.33, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:05m:48s remains)
2017-12-13 09:10:44.642430: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07][1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07 1.6374075e-07]]...]
INFO - root - 2017-12-13 09:10:46.668120: step 5410, loss = 7.34, batch loss = 0.69 (39.5 examples/sec; 0.203 sec/batch; 18h:25m:17s remains)
