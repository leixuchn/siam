INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "85"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 05:39:49.395556: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:39:49.395883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:39:49.395951: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:39:49.395981: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 05:39:49.396033: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-06 05:39:55.375575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.03GiB
2017-12-06 05:39:55.375622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 05:39:55.375629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 05:39:55.375638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 05:40:21.773969: step 0, loss = 2.03, batch loss = 1.97 (0.4 examples/sec; 18.731 sec/batch; 1730h:02m:55s remains)
2017-12-06 05:40:24.561448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3313365 -4.3292065 -4.3313661 -4.3303323 -4.3208809 -4.3026381 -4.2795134 -4.2552328 -4.2422671 -4.2435293 -4.2559457 -4.2766809 -4.3016157 -4.3214602 -4.3312507][-4.3308315 -4.3292856 -4.3305426 -4.3245063 -4.3039169 -4.270957 -4.2317157 -4.1971354 -4.1851616 -4.1945477 -4.2189922 -4.2510691 -4.2845011 -4.3098378 -4.3242755][-4.324441 -4.3236508 -4.3216887 -4.304626 -4.2649155 -4.2105546 -4.148066 -4.1011314 -4.0933428 -4.1191611 -4.1648645 -4.2152987 -4.2620096 -4.2959766 -4.3163433][-4.3106747 -4.312511 -4.3085108 -4.2786045 -4.2209244 -4.1452851 -4.0562811 -3.9900358 -3.9871547 -4.0348053 -4.103519 -4.1761088 -4.2399106 -4.2833843 -4.3105741][-4.2945876 -4.2998476 -4.292232 -4.2507644 -4.1773047 -4.0778961 -3.9567387 -3.8652549 -3.873714 -3.9520335 -4.0468664 -4.1417446 -4.2210183 -4.2751751 -4.3074231][-4.2764421 -4.2826648 -4.2696028 -4.217412 -4.1285248 -4.0037642 -3.8476562 -3.7300298 -3.7642522 -3.8838408 -4.0058932 -4.1192555 -4.20732 -4.2696347 -4.3054256][-4.260469 -4.2621589 -4.2412109 -4.1798544 -4.0790572 -3.9348688 -3.7508049 -3.6226087 -3.702996 -3.8635778 -4.001905 -4.1183052 -4.2066836 -4.2709866 -4.3064389][-4.2477674 -4.2394481 -4.2104087 -4.1487546 -4.05466 -3.91952 -3.7500477 -3.6612916 -3.7760692 -3.9349518 -4.0529971 -4.149096 -4.2248969 -4.2811141 -4.3109951][-4.2359924 -4.2181535 -4.1867685 -4.1387739 -4.0716815 -3.9746165 -3.8643136 -3.8335171 -3.9331913 -4.045392 -4.1211581 -4.1871028 -4.246439 -4.2928629 -4.3168368][-4.2259116 -4.2036643 -4.1755705 -4.14516 -4.10707 -4.0503626 -3.9912746 -3.9879589 -4.0560174 -4.1243978 -4.1659389 -4.2087021 -4.2586193 -4.3010159 -4.3217597][-4.2169518 -4.1940851 -4.1725473 -4.1561942 -4.1389027 -4.110147 -4.0798573 -4.0845151 -4.1275916 -4.16796 -4.1907821 -4.2235 -4.2700877 -4.3094587 -4.3267784][-4.21879 -4.2022839 -4.1903672 -4.1860795 -4.1829462 -4.1698418 -4.15271 -4.1560779 -4.1778688 -4.1968837 -4.2091556 -4.2384906 -4.2822995 -4.3166666 -4.3305497][-4.2331209 -4.2277741 -4.2282853 -4.2320895 -4.2321868 -4.2213798 -4.2033291 -4.1981506 -4.2031732 -4.2096381 -4.2167392 -4.2451549 -4.2867455 -4.3186693 -4.3310404][-4.2400246 -4.24393 -4.252831 -4.2593279 -4.2580104 -4.2443829 -4.2194014 -4.2037783 -4.1989169 -4.20102 -4.2081943 -4.239058 -4.2824564 -4.3148394 -4.328568][-4.2318268 -4.239871 -4.2516561 -4.25898 -4.256865 -4.2399597 -4.2071719 -4.1811213 -4.1721358 -4.1763339 -4.1879597 -4.2226305 -4.2694268 -4.3049326 -4.32234]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:40:34.660376: step 10, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 78h:27m:19s remains)
INFO - root - 2017-12-06 05:40:43.473998: step 20, loss = 2.05, batch loss = 1.99 (7.9 examples/sec; 1.014 sec/batch; 93h:38m:03s remains)
INFO - root - 2017-12-06 05:40:52.294476: step 30, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.956 sec/batch; 88h:18m:59s remains)
INFO - root - 2017-12-06 05:41:01.113014: step 40, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 77h:49m:04s remains)
INFO - root - 2017-12-06 05:41:09.527997: step 50, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 81h:10m:39s remains)
INFO - root - 2017-12-06 05:41:17.972975: step 60, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 77h:00m:29s remains)
INFO - root - 2017-12-06 05:41:26.430832: step 70, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 78h:08m:29s remains)
INFO - root - 2017-12-06 05:41:34.647749: step 80, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 77h:34m:25s remains)
INFO - root - 2017-12-06 05:41:43.056369: step 90, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 77h:29m:31s remains)
INFO - root - 2017-12-06 05:41:51.537083: step 100, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 81h:41m:01s remains)
2017-12-06 05:41:52.379440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2534208 -4.2246161 -4.1944733 -4.1647463 -4.1522884 -4.1619658 -4.1783366 -4.1894822 -4.2088304 -4.2258248 -4.226748 -4.2220235 -4.2169809 -4.1981592 -4.1771235][-4.2654343 -4.2333426 -4.1929851 -4.1487422 -4.120223 -4.1239519 -4.1442418 -4.1588879 -4.1771312 -4.1937323 -4.1977153 -4.1994176 -4.2015758 -4.1869245 -4.1701851][-4.2751718 -4.2398276 -4.1901331 -4.132822 -4.0896053 -4.0833297 -4.10165 -4.1173482 -4.1344795 -4.1534395 -4.1634884 -4.170404 -4.1770349 -4.166532 -4.1531744][-4.2795539 -4.241734 -4.1877131 -4.1270781 -4.0811744 -4.0704875 -4.084516 -4.0962586 -4.1109624 -4.13416 -4.1502924 -4.1580172 -4.1632943 -4.1535316 -4.1384296][-4.2800303 -4.2405019 -4.1861315 -4.1276417 -4.0844312 -4.0704346 -4.080193 -4.0880814 -4.0993409 -4.1247177 -4.145483 -4.1525345 -4.1548271 -4.1466703 -4.1326404][-4.2778554 -4.2380452 -4.1859341 -4.1321621 -4.0886583 -4.0667691 -4.0681715 -4.0679278 -4.0723348 -4.0984631 -4.1249671 -4.1355805 -4.1388216 -4.1344209 -4.1245627][-4.2747211 -4.2353325 -4.1859412 -4.1348629 -4.0874281 -4.055819 -4.046382 -4.0323052 -4.0238161 -4.0477581 -4.0840616 -4.1030693 -4.1122589 -4.1133232 -4.1082854][-4.2712479 -4.23184 -4.1817722 -4.1271753 -4.0742745 -4.0374937 -4.0214238 -3.9982171 -3.9801641 -4.0003695 -4.0430565 -4.0680361 -4.0817642 -4.0892334 -4.0895891][-4.2710018 -4.2325978 -4.1827555 -4.1255684 -4.0715261 -4.0362844 -4.0201411 -3.9985697 -3.9813104 -3.9989951 -4.0384727 -4.0604978 -4.0719771 -4.0807471 -4.0808811][-4.2724547 -4.2341928 -4.1848793 -4.1260743 -4.0729074 -4.0410185 -4.0266609 -4.012517 -4.0052309 -4.0265074 -4.0586705 -4.0711446 -4.072896 -4.0755424 -4.0728083][-4.277698 -4.2389736 -4.1899505 -4.1310372 -4.0797687 -4.051096 -4.0372844 -4.027483 -4.0285735 -4.0539894 -4.0815125 -4.0860543 -4.0789223 -4.0753589 -4.0721254][-4.2873158 -4.2505574 -4.20448 -4.1510153 -4.106411 -4.0838332 -4.0741396 -4.0685673 -4.0729332 -4.0948296 -4.1157942 -4.1169367 -4.1057673 -4.0984769 -4.0946889][-4.2970519 -4.2634907 -4.222507 -4.1771479 -4.1416183 -4.1264238 -4.1252022 -4.128593 -4.1366682 -4.1519084 -4.1644096 -4.1637893 -4.1514111 -4.142086 -4.1389523][-4.3094254 -4.282506 -4.2500424 -4.2148681 -4.1868892 -4.1768112 -4.1822224 -4.193594 -4.206069 -4.2166696 -4.2226167 -4.2195258 -4.2075233 -4.1983728 -4.1973009][-4.3236952 -4.3069167 -4.2869463 -4.2652225 -4.245749 -4.2377057 -4.2424212 -4.2532763 -4.264349 -4.271646 -4.2748814 -4.2717757 -4.2628441 -4.2568192 -4.257689]]...]
INFO - root - 2017-12-06 05:42:00.904011: step 110, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 75h:04m:10s remains)
INFO - root - 2017-12-06 05:42:09.273251: step 120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 79h:33m:04s remains)
INFO - root - 2017-12-06 05:42:17.962883: step 130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 80h:09m:51s remains)
INFO - root - 2017-12-06 05:42:26.479992: step 140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:42m:49s remains)
INFO - root - 2017-12-06 05:42:34.882639: step 150, loss = 2.10, batch loss = 2.05 (12.1 examples/sec; 0.663 sec/batch; 61h:11m:17s remains)
INFO - root - 2017-12-06 05:42:43.382133: step 160, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:54m:55s remains)
INFO - root - 2017-12-06 05:42:51.932043: step 170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 79h:32m:20s remains)
INFO - root - 2017-12-06 05:43:00.325727: step 180, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 61h:24m:54s remains)
INFO - root - 2017-12-06 05:43:07.453853: step 190, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 78h:18m:53s remains)
INFO - root - 2017-12-06 05:43:15.973878: step 200, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 81h:23m:47s remains)
2017-12-06 05:43:19.113729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2684765 -4.2516613 -4.2351346 -4.2303238 -4.229486 -4.2296104 -4.230967 -4.2297134 -4.2247148 -4.2259622 -4.2320967 -4.2380033 -4.23346 -4.2291784 -4.2350287][-4.2795882 -4.256937 -4.2370381 -4.2276077 -4.2213731 -4.2191982 -4.21878 -4.2151723 -4.2076497 -4.2094288 -4.2186089 -4.2236609 -4.2186775 -4.2138333 -4.2199659][-4.2679877 -4.2476368 -4.2386422 -4.2288561 -4.2177458 -4.213294 -4.2116256 -4.2087851 -4.2033548 -4.2082839 -4.2182283 -4.2171736 -4.2071624 -4.1997924 -4.2030444][-4.233212 -4.2230558 -4.2333059 -4.2306895 -4.2176652 -4.2121739 -4.2037673 -4.1957355 -4.1985445 -4.215941 -4.2277422 -4.2173958 -4.1979094 -4.1833177 -4.1791377][-4.2050309 -4.199862 -4.21847 -4.2247434 -4.2155085 -4.2079306 -4.1884041 -4.1701345 -4.1808729 -4.2126441 -4.2295327 -4.2135811 -4.1855431 -4.1615844 -4.148818][-4.1842089 -4.1714034 -4.1887336 -4.2068715 -4.2054896 -4.1894989 -4.1509271 -4.112155 -4.1322746 -4.1863155 -4.2121558 -4.1995916 -4.1693468 -4.1416965 -4.1276741][-4.1789575 -4.1497092 -4.157773 -4.1783819 -4.1798038 -4.1492972 -4.0712 -3.9934452 -4.0297332 -4.1213441 -4.1706314 -4.1733847 -4.1489768 -4.1244464 -4.1224971][-4.1687326 -4.1269135 -4.1301694 -4.1574883 -4.1608481 -4.1096778 -3.9773402 -3.84532 -3.9044271 -4.0482669 -4.1305404 -4.1474304 -4.12095 -4.0940566 -4.1061969][-4.1532393 -4.1133103 -4.1204066 -4.1597667 -4.1720357 -4.12392 -3.9863756 -3.8501813 -3.9066811 -4.0526648 -4.1407542 -4.162909 -4.14043 -4.109756 -4.1201062][-4.1595192 -4.1331205 -4.1457491 -4.189445 -4.2085514 -4.179708 -4.0913868 -4.0072479 -4.0330658 -4.1276493 -4.1953197 -4.2154093 -4.2009506 -4.1691389 -4.1662164][-4.1988287 -4.1819835 -4.1926188 -4.2244968 -4.2409124 -4.2248955 -4.1701379 -4.120172 -4.1294055 -4.1857448 -4.2361526 -4.253232 -4.2427917 -4.2143412 -4.2026987][-4.2327394 -4.2245069 -4.2316828 -4.2506804 -4.2613468 -4.2504339 -4.2088156 -4.1723304 -4.1713409 -4.2061844 -4.2474375 -4.2645764 -4.2561712 -4.2323184 -4.2180748][-4.2518382 -4.24908 -4.2545009 -4.2642317 -4.2712746 -4.2675695 -4.2381954 -4.2107358 -4.2058291 -4.2238584 -4.2540927 -4.2699518 -4.2663593 -4.2506638 -4.240077][-4.2689095 -4.2723751 -4.2809944 -4.2839308 -4.2850919 -4.2837462 -4.2627506 -4.2416553 -4.2393279 -4.25088 -4.27396 -4.2863345 -4.2818546 -4.272213 -4.2648916][-4.2800016 -4.2868819 -4.296639 -4.2992139 -4.2998362 -4.2995396 -4.282948 -4.2663608 -4.2634482 -4.2694526 -4.2844224 -4.2945542 -4.2911897 -4.2820983 -4.2712212]]...]
INFO - root - 2017-12-06 05:43:27.607860: step 210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 79h:25m:20s remains)
INFO - root - 2017-12-06 05:43:36.023202: step 220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 77h:59m:56s remains)
INFO - root - 2017-12-06 05:43:44.526662: step 230, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.807 sec/batch; 74h:31m:26s remains)
INFO - root - 2017-12-06 05:43:53.062290: step 240, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 76h:37m:34s remains)
INFO - root - 2017-12-06 05:44:01.527744: step 250, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 80h:20m:02s remains)
INFO - root - 2017-12-06 05:44:10.074729: step 260, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 80h:22m:02s remains)
INFO - root - 2017-12-06 05:44:18.687030: step 270, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 77h:26m:48s remains)
INFO - root - 2017-12-06 05:44:27.277762: step 280, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:52m:09s remains)
INFO - root - 2017-12-06 05:44:35.715263: step 290, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 79h:03m:52s remains)
INFO - root - 2017-12-06 05:44:44.247868: step 300, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:55m:31s remains)
2017-12-06 05:44:45.209160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1722445 -4.1569395 -4.1276755 -4.1051068 -4.1018267 -4.1136994 -4.1324844 -4.1445236 -4.1541495 -4.1659522 -4.1863995 -4.2018118 -4.1990438 -4.1717882 -4.1377497][-4.157928 -4.1474013 -4.1326056 -4.1163673 -4.1043906 -4.1065674 -4.1140909 -4.118175 -4.1329761 -4.15756 -4.1836543 -4.19941 -4.1952543 -4.1710162 -4.1451664][-4.1459379 -4.1428018 -4.146183 -4.1421566 -4.1292186 -4.1224856 -4.11521 -4.1102791 -4.1249237 -4.1496243 -4.1699262 -4.1834908 -4.1786017 -4.1595216 -4.1472311][-4.124764 -4.1285133 -4.148232 -4.1580963 -4.1512589 -4.1376395 -4.1193929 -4.1072812 -4.1119637 -4.1249833 -4.1331692 -4.1424403 -4.1443214 -4.1417332 -4.1479955][-4.1155934 -4.120482 -4.14636 -4.1677475 -4.1669769 -4.1493731 -4.1210766 -4.0996766 -4.0868311 -4.0797234 -4.0732436 -4.0815787 -4.1008883 -4.1247683 -4.1521797][-4.1318808 -4.1346259 -4.1583662 -4.1809435 -4.1791053 -4.1558094 -4.1168895 -4.0787392 -4.0455337 -4.0203929 -4.0025835 -4.0124421 -4.0500212 -4.0976219 -4.1419282][-4.1560125 -4.156477 -4.1739759 -4.1937103 -4.1902032 -4.1632996 -4.1152749 -4.0605431 -4.0124965 -3.9744093 -3.9459822 -3.9500725 -3.99267 -4.0579548 -4.1211991][-4.1771049 -4.17759 -4.191062 -4.2082686 -4.2086253 -4.1839385 -4.1356492 -4.0787792 -4.0290494 -3.9847159 -3.9461277 -3.9369245 -3.9675412 -4.0354719 -4.1077762][-4.2068706 -4.2050438 -4.2114778 -4.2220526 -4.2231021 -4.2073684 -4.1708388 -4.1256166 -4.0849547 -4.0431061 -4.002605 -3.980783 -3.9899993 -4.0451903 -4.1109738][-4.2506719 -4.2476568 -4.2474241 -4.2495813 -4.2482271 -4.2392769 -4.2160468 -4.18628 -4.1577964 -4.1194096 -4.0789514 -4.05096 -4.0440726 -4.0757213 -4.1212425][-4.2849059 -4.2874541 -4.2885447 -4.2877059 -4.2853155 -4.2820992 -4.2689281 -4.2493882 -4.228652 -4.1926179 -4.1519909 -4.1193347 -4.0977058 -4.1028948 -4.1244788][-4.2903519 -4.3007545 -4.3079424 -4.3100262 -4.309814 -4.3117862 -4.3055906 -4.289875 -4.270649 -4.2401514 -4.2043157 -4.1713066 -4.1423278 -4.1298008 -4.1317577][-4.2682667 -4.2849355 -4.297626 -4.30615 -4.3136683 -4.3225489 -4.3220263 -4.3115158 -4.2936454 -4.2670717 -4.238627 -4.2106729 -4.1830893 -4.162055 -4.14648][-4.2509494 -4.2698474 -4.284584 -4.2944841 -4.3052731 -4.3175349 -4.3223348 -4.3180737 -4.304152 -4.2840462 -4.2663198 -4.2475729 -4.22418 -4.1979928 -4.169383][-4.2543058 -4.2721119 -4.2850266 -4.290657 -4.296937 -4.3042035 -4.3090386 -4.3100805 -4.2997231 -4.2878456 -4.2820473 -4.2739444 -4.2582836 -4.2341495 -4.1975822]]...]
INFO - root - 2017-12-06 05:44:53.661837: step 310, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 78h:18m:23s remains)
INFO - root - 2017-12-06 05:45:02.155427: step 320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 78h:05m:42s remains)
INFO - root - 2017-12-06 05:45:10.792042: step 330, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 78h:14m:06s remains)
INFO - root - 2017-12-06 05:45:19.368680: step 340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 78h:17m:53s remains)
INFO - root - 2017-12-06 05:45:27.946769: step 350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 79h:12m:44s remains)
INFO - root - 2017-12-06 05:45:36.539928: step 360, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 81h:18m:45s remains)
INFO - root - 2017-12-06 05:45:45.129633: step 370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 79h:07m:01s remains)
INFO - root - 2017-12-06 05:45:53.628718: step 380, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 77h:19m:17s remains)
INFO - root - 2017-12-06 05:46:01.182244: step 390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 79h:00m:03s remains)
INFO - root - 2017-12-06 05:46:09.732742: step 400, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 76h:44m:02s remains)
2017-12-06 05:46:10.531371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651725 -4.2581735 -4.2540617 -4.24612 -4.2361536 -4.2313056 -4.2331595 -4.2406511 -4.2454405 -4.2464461 -4.2460628 -4.239439 -4.228569 -4.2215824 -4.2171469][-4.27727 -4.2755766 -4.2720675 -4.2620769 -4.2515879 -4.2494531 -4.2588692 -4.275589 -4.2874908 -4.2927766 -4.2933974 -4.282393 -4.2575684 -4.2322183 -4.2106256][-4.2705531 -4.2713184 -4.2648792 -4.2515225 -4.2398195 -4.23596 -4.2452469 -4.2682114 -4.2901211 -4.3072453 -4.3177996 -4.3116746 -4.2838306 -4.2448521 -4.20319][-4.2556834 -4.2519131 -4.2371383 -4.219089 -4.2073994 -4.2001753 -4.2033167 -4.2247543 -4.2540565 -4.283422 -4.3084412 -4.3146367 -4.2935958 -4.2515836 -4.1981635][-4.2495689 -4.2344108 -4.20745 -4.1799588 -4.1606588 -4.1432633 -4.1351781 -4.1503038 -4.1840596 -4.2268195 -4.2678542 -4.2898583 -4.2824945 -4.2468691 -4.1879439][-4.2467442 -4.2223454 -4.1826768 -4.1436782 -4.1126771 -4.0815282 -4.0573349 -4.0627384 -4.101521 -4.1575837 -4.2131848 -4.2500858 -4.259738 -4.2384729 -4.1834664][-4.2461829 -4.2183161 -4.1710262 -4.117969 -4.0696111 -4.0173588 -3.9690886 -3.9581277 -4.002552 -4.077239 -4.1493521 -4.2014837 -4.2309589 -4.2331529 -4.1985388][-4.249403 -4.2229896 -4.1767759 -4.1165285 -4.0520644 -3.9756958 -3.8927124 -3.8511734 -3.8947189 -3.9896109 -4.0821161 -4.1518087 -4.2000208 -4.2277379 -4.2237639][-4.2580576 -4.2378259 -4.2034793 -4.152669 -4.0911469 -4.0129943 -3.9179316 -3.8518662 -3.87363 -3.9542925 -4.0418963 -4.11597 -4.1743298 -4.2207241 -4.2451148][-4.2734 -4.2610803 -4.2436876 -4.2133632 -4.1727114 -4.1174407 -4.0424523 -3.9806509 -3.9750111 -4.0117035 -4.0612803 -4.1142154 -4.1658278 -4.2171311 -4.2587829][-4.2915897 -4.2853808 -4.2801609 -4.268456 -4.2505903 -4.2253866 -4.1837492 -4.1415038 -4.1251178 -4.1288147 -4.1438055 -4.1708474 -4.2045269 -4.2444563 -4.280674][-4.3073797 -4.3058457 -4.3045959 -4.30283 -4.3005834 -4.2973843 -4.2839341 -4.2630491 -4.2490053 -4.2406926 -4.2395897 -4.2503314 -4.2661963 -4.2867908 -4.3086352][-4.3190975 -4.3224111 -4.321382 -4.3219361 -4.3249669 -4.331305 -4.3326864 -4.3240137 -4.315383 -4.3075757 -4.3041687 -4.3066139 -4.3107166 -4.317759 -4.326035][-4.3266482 -4.330987 -4.3302469 -4.3294573 -4.3322344 -4.3383117 -4.3423443 -4.34022 -4.3364706 -4.3327942 -4.3307371 -4.3302212 -4.3311248 -4.3329506 -4.3348312][-4.3313169 -4.3336139 -4.3329339 -4.3313971 -4.3327503 -4.3369489 -4.3400555 -4.3401709 -4.3388119 -4.3374658 -4.336678 -4.3367248 -4.3382382 -4.3395877 -4.338501]]...]
INFO - root - 2017-12-06 05:46:19.099624: step 410, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 79h:00m:32s remains)
INFO - root - 2017-12-06 05:46:27.570997: step 420, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:33m:58s remains)
INFO - root - 2017-12-06 05:46:36.047514: step 430, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 79h:54m:03s remains)
INFO - root - 2017-12-06 05:46:44.603472: step 440, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 75h:34m:05s remains)
INFO - root - 2017-12-06 05:46:53.177233: step 450, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 80h:02m:00s remains)
INFO - root - 2017-12-06 05:47:01.830476: step 460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 81h:44m:26s remains)
INFO - root - 2017-12-06 05:47:10.367127: step 470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 77h:32m:12s remains)
INFO - root - 2017-12-06 05:47:19.079568: step 480, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 81h:49m:53s remains)
INFO - root - 2017-12-06 05:47:27.492760: step 490, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.775 sec/batch; 71h:30m:57s remains)
INFO - root - 2017-12-06 05:47:36.040336: step 500, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 80h:22m:11s remains)
2017-12-06 05:47:36.798549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1429539 -4.1792459 -4.2248931 -4.2661772 -4.2982383 -4.296257 -4.2576389 -4.2179155 -4.195056 -4.1869869 -4.1991057 -4.2210164 -4.23875 -4.250114 -4.2441821][-4.1271753 -4.1794233 -4.2366052 -4.2819939 -4.3090477 -4.2997756 -4.2529268 -4.2054143 -4.1759877 -4.1668644 -4.1833591 -4.2127328 -4.2381406 -4.2561593 -4.2581673][-4.1357536 -4.2001824 -4.2600784 -4.2973642 -4.3137035 -4.2974572 -4.2442517 -4.1888666 -4.150537 -4.1411819 -4.1664462 -4.2091827 -4.2454748 -4.2709713 -4.2830567][-4.1716986 -4.2342291 -4.2797713 -4.2997065 -4.3032074 -4.2796936 -4.2222495 -4.1590219 -4.1148348 -4.1113672 -4.1502709 -4.2090688 -4.2535319 -4.2829123 -4.3030133][-4.221128 -4.261004 -4.2831326 -4.2866926 -4.2731972 -4.2397079 -4.1739187 -4.0968151 -4.0517769 -4.0684538 -4.1301703 -4.2043862 -4.2529154 -4.2807732 -4.3030448][-4.2577291 -4.2667375 -4.2682586 -4.2592793 -4.2304616 -4.1813722 -4.096437 -3.9963183 -3.961776 -4.0192418 -4.1133547 -4.2015662 -4.2515049 -4.2741475 -4.2909427][-4.2689195 -4.255013 -4.2443967 -4.2308908 -4.1918597 -4.1242681 -4.0141487 -3.8870165 -3.8736796 -3.9816427 -4.10185 -4.1952467 -4.244164 -4.2625389 -4.2733727][-4.2431526 -4.2162824 -4.2060471 -4.2005877 -4.1622953 -4.0881667 -3.9728043 -3.852345 -3.8758333 -4.0049329 -4.1218696 -4.2032084 -4.2420177 -4.2513413 -4.2539992][-4.2043333 -4.1755486 -4.1758184 -4.1808162 -4.148509 -4.0855069 -3.9941332 -3.9208961 -3.9708209 -4.0791855 -4.1687031 -4.2264738 -4.2480044 -4.2467709 -4.2415466][-4.180984 -4.1592717 -4.168776 -4.1801977 -4.1525497 -4.10697 -4.0509863 -4.0191655 -4.0720458 -4.1499176 -4.2107029 -4.2461176 -4.2555852 -4.2511578 -4.2455478][-4.1660261 -4.1538587 -4.1690779 -4.18457 -4.1682277 -4.1406403 -4.1129684 -4.1000667 -4.1404667 -4.1904426 -4.2298303 -4.2518592 -4.2592049 -4.2575846 -4.2572079][-4.1600952 -4.1533632 -4.1676974 -4.1851788 -4.1829462 -4.1746049 -4.1618223 -4.1531649 -4.1766067 -4.2047973 -4.22987 -4.2467003 -4.2580876 -4.2633328 -4.26906][-4.1734576 -4.1642957 -4.1719027 -4.1878014 -4.197979 -4.2015834 -4.1964488 -4.1891575 -4.1997066 -4.2170415 -4.2354884 -4.2510757 -4.2655106 -4.2742658 -4.2810349][-4.220902 -4.2080169 -4.2089005 -4.2223125 -4.2353296 -4.2411366 -4.2358069 -4.2286263 -4.2339535 -4.2457809 -4.2607908 -4.2754111 -4.2882419 -4.294435 -4.2990065][-4.274332 -4.2616768 -4.261096 -4.2688627 -4.2771692 -4.2773581 -4.2699413 -4.2649355 -4.2709117 -4.2795219 -4.2904282 -4.3017254 -4.3093071 -4.310585 -4.3117375]]...]
INFO - root - 2017-12-06 05:47:45.170514: step 510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:36m:08s remains)
INFO - root - 2017-12-06 05:47:53.654697: step 520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:33m:33s remains)
INFO - root - 2017-12-06 05:48:02.330771: step 530, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 82h:16m:13s remains)
INFO - root - 2017-12-06 05:48:10.900522: step 540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 78h:28m:22s remains)
INFO - root - 2017-12-06 05:48:19.421328: step 550, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 75h:09m:46s remains)
INFO - root - 2017-12-06 05:48:27.944878: step 560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 78h:55m:40s remains)
INFO - root - 2017-12-06 05:48:36.400154: step 570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 80h:15m:33s remains)
INFO - root - 2017-12-06 05:48:44.961255: step 580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 79h:47m:57s remains)
INFO - root - 2017-12-06 05:48:52.796858: step 590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:43m:41s remains)
INFO - root - 2017-12-06 05:49:01.170012: step 600, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 78h:14m:26s remains)
2017-12-06 05:49:01.969367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.217268 -4.189743 -4.17496 -4.1680312 -4.1681614 -4.1721487 -4.1763387 -4.1938615 -4.2277737 -4.2558689 -4.2737975 -4.2797971 -4.2605224 -4.2237134 -4.2001095][-4.2107825 -4.19423 -4.1875496 -4.1849031 -4.1836853 -4.1837592 -4.1852126 -4.200933 -4.2345266 -4.2610607 -4.2763457 -4.2798729 -4.2663741 -4.2397738 -4.2274909][-4.1937551 -4.1811814 -4.1779327 -4.1779947 -4.1841145 -4.1880245 -4.1899819 -4.2071085 -4.2367315 -4.256794 -4.2649655 -4.261858 -4.2489686 -4.2307439 -4.2298169][-4.1768937 -4.164371 -4.160295 -4.1574354 -4.1654792 -4.1690879 -4.1655617 -4.1762452 -4.1970496 -4.210609 -4.21542 -4.2158103 -4.2105465 -4.1998868 -4.2063684][-4.1566868 -4.1457725 -4.1416931 -4.1395445 -4.148603 -4.1456795 -4.1304135 -4.131484 -4.1453643 -4.158555 -4.1679978 -4.1763067 -4.1761146 -4.1675382 -4.1704845][-4.1233621 -4.1134849 -4.111886 -4.1104822 -4.1106486 -4.0915265 -4.0618119 -4.0583377 -4.0759592 -4.1053176 -4.1330838 -4.1481009 -4.1415887 -4.1251774 -4.1183333][-4.107821 -4.1025462 -4.0987673 -4.0865216 -4.0630164 -4.0164852 -3.9656632 -3.9558444 -3.9881935 -4.0436206 -4.087245 -4.1018806 -4.0840945 -4.0552344 -4.0404749][-4.1154451 -4.1094775 -4.102366 -4.0816121 -4.0396185 -3.9710786 -3.904367 -3.8904259 -3.9292238 -3.9906406 -4.0306363 -4.0351372 -4.0096245 -3.97928 -3.9700935][-4.139339 -4.1307511 -4.1190934 -4.096858 -4.0534225 -3.9832723 -3.9195716 -3.904825 -3.932641 -3.9739318 -3.9921894 -3.9799771 -3.9488661 -3.9273832 -3.9390671][-4.1713119 -4.1639881 -4.1562085 -4.143383 -4.11276 -4.0599785 -4.0104189 -3.9950509 -4.0053887 -4.0223885 -4.0222468 -3.9989064 -3.9652743 -3.9509633 -3.9748325][-4.2025509 -4.1981916 -4.1939616 -4.1900444 -4.1754856 -4.1461248 -4.1166334 -4.1074305 -4.10877 -4.1120596 -4.1028862 -4.077147 -4.0474124 -4.0383363 -4.0601997][-4.2121253 -4.2128639 -4.206852 -4.2046986 -4.202805 -4.1944146 -4.1878619 -4.191473 -4.1939287 -4.1935968 -4.1844106 -4.1651759 -4.1420097 -4.1343856 -4.1467042][-4.2042456 -4.2093215 -4.2027612 -4.1993 -4.2058258 -4.2127891 -4.2241392 -4.2371612 -4.2425971 -4.2431383 -4.2371173 -4.2242355 -4.2069478 -4.201097 -4.2075992][-4.1984639 -4.2071915 -4.2033086 -4.1978135 -4.2028737 -4.215713 -4.23474 -4.2509246 -4.2571316 -4.2578669 -4.2530508 -4.2437196 -4.2299294 -4.226439 -4.2314882][-4.2142034 -4.228056 -4.2215238 -4.211236 -4.212317 -4.2229776 -4.2383528 -4.2504063 -4.2538223 -4.25223 -4.2459316 -4.23695 -4.2240877 -4.2215333 -4.2274384]]...]
INFO - root - 2017-12-06 05:49:10.400938: step 610, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.828 sec/batch; 76h:20m:25s remains)
INFO - root - 2017-12-06 05:49:18.900209: step 620, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 80h:41m:03s remains)
INFO - root - 2017-12-06 05:49:27.495385: step 630, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:54m:56s remains)
INFO - root - 2017-12-06 05:49:36.030946: step 640, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 77h:12m:08s remains)
INFO - root - 2017-12-06 05:49:44.555195: step 650, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 80h:05m:48s remains)
INFO - root - 2017-12-06 05:49:53.135943: step 660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 78h:56m:23s remains)
INFO - root - 2017-12-06 05:50:01.625180: step 670, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 77h:46m:51s remains)
INFO - root - 2017-12-06 05:50:10.061740: step 680, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 79h:10m:36s remains)
INFO - root - 2017-12-06 05:50:18.622714: step 690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:40m:18s remains)
INFO - root - 2017-12-06 05:50:26.583470: step 700, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 77h:32m:34s remains)
2017-12-06 05:50:27.376175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3003993 -4.2834506 -4.2559772 -4.2350712 -4.2331343 -4.2476549 -4.2671614 -4.2830386 -4.2925787 -4.2952375 -4.2934022 -4.2923231 -4.2945709 -4.2980161 -4.2995405][-4.2929063 -4.2627726 -4.21953 -4.186007 -4.1810703 -4.2022343 -4.2331433 -4.2608333 -4.2804508 -4.2904167 -4.2923965 -4.2926655 -4.2945251 -4.2973781 -4.2990437][-4.2798767 -4.2321472 -4.1664596 -4.1141939 -4.103281 -4.1324358 -4.1770611 -4.2168503 -4.2489119 -4.2722149 -4.2839503 -4.2903013 -4.2941027 -4.297389 -4.2993097][-4.2656574 -4.1970029 -4.1053176 -4.0310321 -4.0104146 -4.04609 -4.1043191 -4.1573429 -4.2029214 -4.242106 -4.2689419 -4.2863336 -4.2953849 -4.3003721 -4.3018003][-4.2574162 -4.1737423 -4.0609512 -3.965394 -3.9307902 -3.9612541 -4.0239964 -4.0870585 -4.1472516 -4.2021508 -4.2455149 -4.276938 -4.2947726 -4.3030133 -4.3034887][-4.26245 -4.1755838 -4.0538583 -3.9430256 -3.8903849 -3.8998358 -3.9456916 -4.0063968 -4.0768681 -4.1508193 -4.2134991 -4.2593236 -4.2872915 -4.2998586 -4.3000326][-4.2777529 -4.202714 -4.0884738 -3.976084 -3.9073508 -3.8857114 -3.8916855 -3.9296443 -4.000895 -4.09191 -4.173058 -4.2335038 -4.272706 -4.291841 -4.2933421][-4.2972693 -4.2436 -4.1516304 -4.0507274 -3.978879 -3.9365716 -3.9004369 -3.8940463 -3.9449124 -4.040751 -4.1357675 -4.2100163 -4.2588792 -4.2839661 -4.2860155][-4.3141346 -4.286046 -4.2248268 -4.1464057 -4.082479 -4.0348125 -3.9757013 -3.9313698 -3.9451065 -4.0268383 -4.1213522 -4.1989803 -4.2519188 -4.2778888 -4.2783952][-4.3200288 -4.3112316 -4.2790542 -4.2268667 -4.1755781 -4.1303816 -4.0692616 -4.0091543 -3.9951777 -4.0504985 -4.1297326 -4.19977 -4.249126 -4.2726674 -4.2727919][-4.31692 -4.3180389 -4.3075991 -4.2802563 -4.2460947 -4.2082186 -4.1508522 -4.090435 -4.0641842 -4.0968208 -4.1545587 -4.2094073 -4.2494926 -4.2703509 -4.2722192][-4.3113766 -4.3158121 -4.3171167 -4.3085194 -4.2915649 -4.2643881 -4.2154675 -4.1600571 -4.12944 -4.1465516 -4.1863618 -4.2263246 -4.2568378 -4.2740555 -4.277494][-4.3056059 -4.3094835 -4.3152809 -4.3184409 -4.3162551 -4.30178 -4.2660036 -4.2192059 -4.1900711 -4.1980071 -4.2212896 -4.2456045 -4.2666497 -4.2793255 -4.2831917][-4.3009253 -4.3020883 -4.3064847 -4.3135376 -4.3217831 -4.3215694 -4.3026514 -4.2695818 -4.2457204 -4.2441807 -4.2512231 -4.2609272 -4.2726731 -4.2807941 -4.285007][-4.2984982 -4.2958145 -4.294076 -4.2972579 -4.3084745 -4.3213377 -4.3209181 -4.3046842 -4.2873249 -4.2768745 -4.2701116 -4.2683287 -4.2723589 -4.2773309 -4.2815056]]...]
INFO - root - 2017-12-06 05:50:35.907061: step 710, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 78h:18m:33s remains)
INFO - root - 2017-12-06 05:50:44.477447: step 720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 79h:28m:07s remains)
INFO - root - 2017-12-06 05:50:53.095418: step 730, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 76h:28m:04s remains)
INFO - root - 2017-12-06 05:51:01.628339: step 740, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:53m:04s remains)
INFO - root - 2017-12-06 05:51:10.300689: step 750, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 81h:28m:08s remains)
INFO - root - 2017-12-06 05:51:18.802777: step 760, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.838 sec/batch; 77h:11m:13s remains)
INFO - root - 2017-12-06 05:51:27.305710: step 770, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 77h:13m:37s remains)
INFO - root - 2017-12-06 05:51:35.668713: step 780, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.754 sec/batch; 69h:27m:18s remains)
INFO - root - 2017-12-06 05:51:44.151628: step 790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:35m:10s remains)
INFO - root - 2017-12-06 05:51:52.490810: step 800, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 78h:09m:05s remains)
2017-12-06 05:51:53.275616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2795839 -4.2664866 -4.2597384 -4.2567835 -4.2426734 -4.2137837 -4.1864324 -4.1705031 -4.1688094 -4.1760573 -4.1878843 -4.197566 -4.1989 -4.1878014 -4.1759791][-4.2890797 -4.2768254 -4.269278 -4.2648091 -4.2499 -4.2210817 -4.1902375 -4.1677222 -4.158617 -4.1604176 -4.1689696 -4.177218 -4.1769867 -4.1659088 -4.155591][-4.2786779 -4.2674704 -4.2602763 -4.2560554 -4.2411842 -4.2131538 -4.1807613 -4.1532192 -4.1383281 -4.1356015 -4.1426768 -4.1545663 -4.1581836 -4.1522627 -4.1484861][-4.2484813 -4.2355247 -4.2261262 -4.219749 -4.2045684 -4.1787128 -4.150033 -4.1252818 -4.1109161 -4.1083174 -4.1171365 -4.1325293 -4.1388721 -4.1393857 -4.1423473][-4.2082567 -4.1924996 -4.1819916 -4.1745806 -4.1598854 -4.1376123 -4.1126981 -4.0910077 -4.0793333 -4.0811529 -4.0964966 -4.1145768 -4.1193261 -4.1240396 -4.1353288][-4.1689768 -4.1520233 -4.1390829 -4.1263866 -4.10765 -4.0843058 -4.0578871 -4.0333104 -4.0246181 -4.0404353 -4.074625 -4.1028857 -4.1114483 -4.1206064 -4.1350422][-4.1418281 -4.1293535 -4.1156936 -4.0956039 -4.0684319 -4.0353351 -3.9942167 -3.952903 -3.9399304 -3.9735153 -4.0312357 -4.0738568 -4.0896597 -4.0976343 -4.1050472][-4.1348052 -4.1306658 -4.1192875 -4.0946231 -4.05835 -4.015337 -3.962163 -3.9078574 -3.8892961 -3.9291844 -3.9965062 -4.0451617 -4.0641041 -4.0674925 -4.0655031][-4.148385 -4.1508861 -4.1414852 -4.1151619 -4.0799365 -4.0445633 -4.0057898 -3.9653833 -3.9479005 -3.9754009 -4.0261316 -4.0631051 -4.0745878 -4.0719075 -4.06447][-4.1838913 -4.1882567 -4.1798744 -4.1543117 -4.124733 -4.0994883 -4.07413 -4.0456495 -4.0300612 -4.0442753 -4.076077 -4.0993519 -4.1036792 -4.0947876 -4.0857143][-4.2296462 -4.2314243 -4.2240939 -4.2033882 -4.1810374 -4.1622915 -4.1429558 -4.1183829 -4.1025691 -4.1084032 -4.12659 -4.1408157 -4.1415143 -4.12772 -4.1162505][-4.2745633 -4.2734671 -4.2674589 -4.25233 -4.2360182 -4.2211361 -4.2043409 -4.1827607 -4.1695433 -4.1715951 -4.1821818 -4.1909785 -4.1903348 -4.1764984 -4.1643434][-4.2995543 -4.2977834 -4.2939405 -4.28554 -4.2756143 -4.2648349 -4.2528419 -4.2389092 -4.2320509 -4.2345181 -4.2400188 -4.24324 -4.2410135 -4.2300315 -4.2208447][-4.3060651 -4.3054562 -4.3037891 -4.2993464 -4.2935848 -4.2869411 -4.2804084 -4.2732143 -4.2707963 -4.2730656 -4.274344 -4.2730341 -4.2697759 -4.2629304 -4.257165][-4.3075805 -4.3075562 -4.3067503 -4.3039827 -4.30027 -4.2966013 -4.2937737 -4.2910948 -4.2904639 -4.2911558 -4.2897272 -4.2850838 -4.2792168 -4.2729783 -4.2666049]]...]
INFO - root - 2017-12-06 05:52:01.761899: step 810, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 79h:58m:45s remains)
INFO - root - 2017-12-06 05:52:10.252665: step 820, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.840 sec/batch; 77h:23m:37s remains)
INFO - root - 2017-12-06 05:52:18.732555: step 830, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 78h:23m:35s remains)
INFO - root - 2017-12-06 05:52:27.394522: step 840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 78h:17m:11s remains)
INFO - root - 2017-12-06 05:52:35.861121: step 850, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 77h:07m:33s remains)
INFO - root - 2017-12-06 05:52:44.367538: step 860, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 77h:02m:56s remains)
INFO - root - 2017-12-06 05:52:52.817248: step 870, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 79h:47m:15s remains)
INFO - root - 2017-12-06 05:53:01.437155: step 880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 79h:08m:45s remains)
INFO - root - 2017-12-06 05:53:09.760651: step 890, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 79h:07m:42s remains)
INFO - root - 2017-12-06 05:53:18.267220: step 900, loss = 2.04, batch loss = 1.98 (10.1 examples/sec; 0.788 sec/batch; 72h:36m:21s remains)
2017-12-06 05:53:19.014794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1001563 -4.0942822 -4.099905 -4.1021585 -4.0908833 -4.0959229 -4.109561 -4.1210232 -4.12992 -4.1255245 -4.1136117 -4.1143694 -4.1005554 -4.0803103 -4.0898972][-4.0655546 -4.0931892 -4.1187124 -4.12098 -4.0997462 -4.0894675 -4.0993209 -4.1175895 -4.1316276 -4.1322403 -4.1214757 -4.1152735 -4.0957475 -4.0752926 -4.08621][-4.0237088 -4.0913467 -4.1399155 -4.1429038 -4.1166739 -4.0920525 -4.0936518 -4.1078825 -4.1194906 -4.1284032 -4.1285539 -4.1280713 -4.1125145 -4.0908504 -4.0970206][-4.03568 -4.112329 -4.167922 -4.168231 -4.133399 -4.0849853 -4.0575681 -4.0550866 -4.0708418 -4.0962577 -4.1143174 -4.1279025 -4.1268244 -4.1093593 -4.1076632][-4.1107206 -4.1598129 -4.1974483 -4.1868329 -4.1312003 -4.0644059 -4.0075927 -3.983927 -4.0108995 -4.056797 -4.0933123 -4.1193094 -4.1288214 -4.1200714 -4.1104937][-4.1806111 -4.1958818 -4.2049327 -4.1739097 -4.1022215 -4.0224891 -3.9392962 -3.90651 -3.9574323 -4.0216866 -4.0780621 -4.1085877 -4.1126747 -4.10001 -4.0917749][-4.2120247 -4.1989484 -4.1812878 -4.1304178 -4.0368295 -3.9393733 -3.8352544 -3.8086853 -3.9008393 -3.9990988 -4.0702639 -4.0942669 -4.0808835 -4.0583911 -4.0486007][-4.2226253 -4.1932707 -4.1564794 -4.090044 -3.9861746 -3.8923707 -3.8068626 -3.8003991 -3.9203012 -4.0401516 -4.1088529 -4.1256971 -4.1019683 -4.0624785 -4.0418482][-4.2255368 -4.1988039 -4.1641369 -4.1033459 -4.0224853 -3.9635425 -3.9234779 -3.9381511 -4.0345635 -4.1274343 -4.1710167 -4.1799989 -4.1561637 -4.1068616 -4.0795565][-4.2304239 -4.2229548 -4.2029171 -4.1606703 -4.1047273 -4.071733 -4.0607452 -4.0767555 -4.1348906 -4.1895261 -4.202467 -4.2027121 -4.187726 -4.1450744 -4.12136][-4.2276478 -4.236887 -4.231442 -4.2105026 -4.1768951 -4.1624742 -4.1638031 -4.1753259 -4.2027636 -4.22492 -4.2184668 -4.2125936 -4.202868 -4.1689782 -4.1429482][-4.2232733 -4.2366385 -4.2414 -4.23388 -4.2146811 -4.2094536 -4.21526 -4.22047 -4.2278881 -4.2328768 -4.2162189 -4.2058792 -4.1980824 -4.1659026 -4.1355457][-4.228538 -4.2341456 -4.2389274 -4.236856 -4.224968 -4.2203178 -4.2263694 -4.2317972 -4.2365384 -4.23751 -4.2242312 -4.2176585 -4.2104588 -4.1773992 -4.1396794][-4.2376966 -4.2368937 -4.2385941 -4.2384944 -4.234942 -4.2327495 -4.2391725 -4.2439575 -4.2461367 -4.2448797 -4.2397838 -4.2412782 -4.2340918 -4.2005391 -4.1613936][-4.2396941 -4.2374916 -4.24038 -4.2460561 -4.2507277 -4.2535553 -4.2602453 -4.263586 -4.2639012 -4.2594385 -4.2525663 -4.2532496 -4.2450323 -4.2174573 -4.1874533]]...]
INFO - root - 2017-12-06 05:53:27.571112: step 910, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 80h:49m:30s remains)
INFO - root - 2017-12-06 05:53:36.135758: step 920, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.880 sec/batch; 81h:02m:36s remains)
INFO - root - 2017-12-06 05:53:44.726720: step 930, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 79h:58m:35s remains)
INFO - root - 2017-12-06 05:53:53.367147: step 940, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 77h:24m:13s remains)
INFO - root - 2017-12-06 05:54:01.905103: step 950, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.886 sec/batch; 81h:36m:12s remains)
INFO - root - 2017-12-06 05:54:10.570352: step 960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 77h:46m:24s remains)
INFO - root - 2017-12-06 05:54:19.081575: step 970, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 78h:35m:36s remains)
INFO - root - 2017-12-06 05:54:27.724871: step 980, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 80h:27m:26s remains)
INFO - root - 2017-12-06 05:54:36.000860: step 990, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.759 sec/batch; 69h:51m:56s remains)
INFO - root - 2017-12-06 05:54:44.434040: step 1000, loss = 2.03, batch loss = 1.97 (10.1 examples/sec; 0.791 sec/batch; 72h:51m:26s remains)
2017-12-06 05:54:45.149767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1129484 -4.1276073 -4.151989 -4.176723 -4.1842885 -4.1772685 -4.1556745 -4.140749 -4.138639 -4.1418982 -4.1574173 -4.1746144 -4.2031093 -4.2353458 -4.2676029][-4.1146188 -4.1285567 -4.1523042 -4.1763182 -4.1842723 -4.1741147 -4.1486583 -4.1384177 -4.1471944 -4.1576166 -4.1765146 -4.1949568 -4.2206182 -4.2474427 -4.2751193][-4.1044064 -4.1212711 -4.1439366 -4.1665006 -4.173058 -4.1596832 -4.13165 -4.1229315 -4.1381631 -4.1552558 -4.1780467 -4.197753 -4.2221231 -4.2474413 -4.2744422][-4.0937562 -4.1091256 -4.1287222 -4.1495051 -4.1552768 -4.1392221 -4.1093421 -4.1011653 -4.1193714 -4.1423321 -4.167695 -4.1887293 -4.2137489 -4.2415156 -4.2716565][-4.1024003 -4.1090422 -4.1187353 -4.1281972 -4.1248994 -4.1020141 -4.069962 -4.0668106 -4.0914378 -4.1220889 -4.1501288 -4.1756124 -4.2055779 -4.2375755 -4.2717457][-4.1266766 -4.1193871 -4.1147528 -4.1066203 -4.0872197 -4.0480313 -4.0068774 -4.0107327 -4.0536823 -4.099978 -4.1362495 -4.169569 -4.204752 -4.2385092 -4.2739482][-4.1500163 -4.1295838 -4.1112962 -4.0845628 -4.0443888 -3.9840031 -3.9294369 -3.9421811 -4.0109916 -4.0805264 -4.1298838 -4.1700253 -4.2073889 -4.2425385 -4.2772112][-4.1610622 -4.1344638 -4.1103129 -4.0743036 -4.0241718 -3.9523363 -3.8874972 -3.9087696 -3.9968357 -4.0823941 -4.14047 -4.1829119 -4.2177005 -4.2519736 -4.2843456][-4.1686811 -4.1443806 -4.12462 -4.0950532 -4.0560064 -3.9982123 -3.9438791 -3.963443 -4.0375776 -4.1099553 -4.1596937 -4.1975269 -4.228013 -4.2604942 -4.2910604][-4.1648417 -4.1451225 -4.1352611 -4.121892 -4.1045866 -4.0715227 -4.0346293 -4.043087 -4.0879025 -4.1347752 -4.1693373 -4.1995955 -4.2275686 -4.2603602 -4.2925725][-4.1600757 -4.1450148 -4.1437216 -4.1434293 -4.1430163 -4.1290984 -4.1036325 -4.09972 -4.1203032 -4.1472187 -4.1702271 -4.1952009 -4.2241988 -4.2584457 -4.2925496][-4.1613374 -4.1479092 -4.1502919 -4.1562133 -4.1666174 -4.1678834 -4.1516776 -4.1419563 -4.1495233 -4.1637473 -4.1793065 -4.2022548 -4.2318258 -4.2648005 -4.2964191][-4.1731644 -4.1607308 -4.164289 -4.173934 -4.1900425 -4.19896 -4.1875167 -4.1758351 -4.1777835 -4.1858144 -4.1988635 -4.2221313 -4.251513 -4.2807336 -4.3063936][-4.201683 -4.1909904 -4.1948457 -4.2043738 -4.2180843 -4.2245817 -4.2122664 -4.2005463 -4.2024837 -4.2113285 -4.2262511 -4.2510004 -4.2787824 -4.3026361 -4.3213205][-4.2370615 -4.2285166 -4.2302327 -4.2359776 -4.2439108 -4.2462544 -4.2344894 -4.225574 -4.2300968 -4.2423277 -4.2595825 -4.2818928 -4.3047428 -4.322371 -4.3344412]]...]
INFO - root - 2017-12-06 05:54:53.675963: step 1010, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 80h:33m:09s remains)
INFO - root - 2017-12-06 05:55:01.948402: step 1020, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 76h:49m:46s remains)
INFO - root - 2017-12-06 05:55:10.330186: step 1030, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.809 sec/batch; 74h:27m:19s remains)
INFO - root - 2017-12-06 05:55:18.723666: step 1040, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:58m:00s remains)
INFO - root - 2017-12-06 05:55:27.123900: step 1050, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 77h:11m:23s remains)
INFO - root - 2017-12-06 05:55:35.585311: step 1060, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 76h:31m:55s remains)
INFO - root - 2017-12-06 05:55:44.073114: step 1070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:59m:42s remains)
INFO - root - 2017-12-06 05:55:52.676803: step 1080, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 76h:39m:15s remains)
INFO - root - 2017-12-06 05:56:01.065878: step 1090, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 68h:22m:23s remains)
INFO - root - 2017-12-06 05:56:09.691892: step 1100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:58m:27s remains)
2017-12-06 05:56:10.533999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1765466 -4.1327829 -4.0928831 -4.0972142 -4.1217074 -4.1502953 -4.1852007 -4.2091856 -4.2280359 -4.2429171 -4.2538371 -4.2632885 -4.2738347 -4.2791333 -4.2827139][-4.1845493 -4.1460304 -4.0985675 -4.0706921 -4.0581183 -4.0737028 -4.1286435 -4.1785717 -4.2095742 -4.2334909 -4.2490368 -4.2594204 -4.2730846 -4.2848306 -4.2920842][-4.18549 -4.1437507 -4.0861192 -4.0320268 -3.9849851 -3.9912663 -4.0670033 -4.1372428 -4.1790533 -4.2107816 -4.2329693 -4.2486906 -4.2651157 -4.2823138 -4.294075][-4.1978469 -4.1605349 -4.1083198 -4.0493402 -3.984797 -3.9717259 -4.03709 -4.1086512 -4.1523561 -4.1862907 -4.2121377 -4.2336807 -4.2566118 -4.2801151 -4.2936735][-4.2044897 -4.1738958 -4.1347613 -4.0913076 -4.0346179 -4.011447 -4.0485706 -4.100153 -4.1387186 -4.16895 -4.195015 -4.2217088 -4.2487745 -4.2758617 -4.2922707][-4.1891861 -4.1617723 -4.1265116 -4.0924439 -4.0517292 -4.0283833 -4.037138 -4.064034 -4.0986924 -4.1345253 -4.1690826 -4.2009907 -4.2334785 -4.2663703 -4.2878857][-4.1619105 -4.1406808 -4.1118255 -4.0814996 -4.0444536 -4.0127945 -3.9947925 -3.9939356 -4.0249095 -4.0758052 -4.1252093 -4.1662264 -4.2074533 -4.2491059 -4.2804422][-4.1556168 -4.133779 -4.1082659 -4.0839562 -4.0501742 -4.0109663 -3.975817 -3.9512796 -3.975687 -4.0332541 -4.0902147 -4.1381841 -4.1849709 -4.2346129 -4.273612][-4.183712 -4.1593833 -4.1321239 -4.110796 -4.0839233 -4.0484867 -4.0145645 -3.9839194 -3.9911675 -4.03552 -4.0852871 -4.1325011 -4.1828675 -4.2360673 -4.2779512][-4.2112126 -4.1900229 -4.1635451 -4.1387863 -4.1109834 -4.0808516 -4.0565467 -4.0328207 -4.0330677 -4.0625272 -4.1046371 -4.1469831 -4.1956158 -4.24696 -4.287571][-4.2111297 -4.1990366 -4.181098 -4.15815 -4.1294494 -4.0980453 -4.0760255 -4.0574679 -4.0544271 -4.0749469 -4.1141138 -4.1570377 -4.2042918 -4.2541065 -4.2930427][-4.200779 -4.2009373 -4.1958542 -4.1828351 -4.1600032 -4.1280246 -4.1023855 -4.0809021 -4.0706177 -4.0822668 -4.1172533 -4.1590309 -4.2047048 -4.2528458 -4.2901521][-4.2066331 -4.2147307 -4.2197337 -4.2173524 -4.2041469 -4.1790433 -4.1529231 -4.1251955 -4.10366 -4.1029968 -4.1294293 -4.1641078 -4.2045197 -4.2486796 -4.2830386][-4.2253542 -4.2336245 -4.2407069 -4.244379 -4.2425113 -4.2307081 -4.2110105 -4.1797147 -4.1488595 -4.1363463 -4.1506143 -4.1756754 -4.2085943 -4.2468791 -4.2774768][-4.236136 -4.2412205 -4.2470565 -4.2525668 -4.2566295 -4.2536235 -4.2401619 -4.2088976 -4.1726837 -4.1522317 -4.1592617 -4.1787834 -4.2063146 -4.2393475 -4.2681985]]...]
INFO - root - 2017-12-06 05:56:18.896540: step 1110, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 78h:58m:42s remains)
INFO - root - 2017-12-06 05:56:27.534169: step 1120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 78h:01m:08s remains)
INFO - root - 2017-12-06 05:56:36.082008: step 1130, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 80h:56m:06s remains)
INFO - root - 2017-12-06 05:56:44.479134: step 1140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 77h:43m:59s remains)
INFO - root - 2017-12-06 05:56:52.914607: step 1150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:40m:11s remains)
INFO - root - 2017-12-06 05:57:01.425470: step 1160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:38m:17s remains)
INFO - root - 2017-12-06 05:57:10.004850: step 1170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:57m:52s remains)
INFO - root - 2017-12-06 05:57:18.605170: step 1180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 78h:17m:03s remains)
INFO - root - 2017-12-06 05:57:27.124731: step 1190, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 67h:35m:10s remains)
INFO - root - 2017-12-06 05:57:35.592978: step 1200, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 78h:30m:33s remains)
2017-12-06 05:57:36.322858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726917 -4.2694745 -4.2769566 -4.2866812 -4.2896271 -4.2866921 -4.2842441 -4.2864423 -4.2932868 -4.296308 -4.2957082 -4.2902832 -4.2817144 -4.2700276 -4.2617779][-4.262991 -4.2503076 -4.2561655 -4.275445 -4.2898245 -4.2940845 -4.2982192 -4.3040819 -4.3166103 -4.3222842 -4.3177342 -4.3012896 -4.2823997 -4.2648811 -4.2508421][-4.2215171 -4.2050338 -4.2104816 -4.24316 -4.2711563 -4.2822704 -4.2916064 -4.3018823 -4.3212357 -4.3327737 -4.3299265 -4.3079486 -4.2806215 -4.2566028 -4.2351766][-4.1850266 -4.1703296 -4.1796837 -4.2214646 -4.252851 -4.2579069 -4.2629175 -4.2764 -4.3004179 -4.3181229 -4.3218508 -4.3070722 -4.2843127 -4.2589283 -4.2306881][-4.1680627 -4.1670651 -4.1888685 -4.2314663 -4.2515635 -4.2375255 -4.2201929 -4.22608 -4.2544508 -4.2793283 -4.2954316 -4.297833 -4.2900052 -4.2697973 -4.2392635][-4.1597595 -4.173131 -4.2016392 -4.2331328 -4.2323213 -4.1917763 -4.1410875 -4.1278906 -4.1662426 -4.2131653 -4.251389 -4.27481 -4.2859411 -4.277504 -4.2512836][-4.1474762 -4.1722651 -4.1999264 -4.2125149 -4.1863384 -4.1191206 -4.0234919 -3.9663928 -4.023149 -4.11742 -4.194097 -4.2430463 -4.270834 -4.2748585 -4.2555804][-4.14154 -4.1677651 -4.1917696 -4.1917539 -4.1462231 -4.0564532 -3.9257915 -3.8237782 -3.9034085 -4.0470843 -4.1552081 -4.2236671 -4.2613912 -4.2728224 -4.2606697][-4.1496334 -4.1682496 -4.1902671 -4.1875052 -4.144453 -4.0648694 -3.9593081 -3.8755708 -3.9405303 -4.0688963 -4.1676307 -4.2313256 -4.2679873 -4.2830658 -4.2780066][-4.176199 -4.18737 -4.206346 -4.2095251 -4.1868696 -4.139967 -4.0849037 -4.0428767 -4.0755873 -4.1515403 -4.2124209 -4.2520337 -4.2784967 -4.2919922 -4.2940478][-4.216218 -4.2192645 -4.2324252 -4.2427874 -4.2373018 -4.2135987 -4.1925111 -4.1819634 -4.1989627 -4.2323742 -4.2564721 -4.2691889 -4.27949 -4.2854104 -4.2915716][-4.2610874 -4.2589068 -4.2638597 -4.2701807 -4.2673135 -4.2567682 -4.2558851 -4.2612371 -4.2697306 -4.2758808 -4.2765026 -4.2731829 -4.26998 -4.2675853 -4.2735362][-4.2937059 -4.28771 -4.2845845 -4.282866 -4.2768526 -4.2701373 -4.2783303 -4.2873473 -4.2844057 -4.27162 -4.26023 -4.2523994 -4.2484665 -4.2466874 -4.2517591][-4.3005919 -4.2924438 -4.2866335 -4.2830606 -4.2775803 -4.2729011 -4.2813854 -4.2872868 -4.275157 -4.2531114 -4.2386236 -4.2330718 -4.2322292 -4.2369332 -4.24544][-4.2925477 -4.284358 -4.2806506 -4.2790275 -4.2762408 -4.2765641 -4.2834592 -4.2839403 -4.2700267 -4.2482395 -4.2352881 -4.2331524 -4.2339988 -4.2451763 -4.2613816]]...]
INFO - root - 2017-12-06 05:57:44.865928: step 1210, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:48m:20s remains)
INFO - root - 2017-12-06 05:57:53.399659: step 1220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 80h:04m:07s remains)
INFO - root - 2017-12-06 05:58:01.855388: step 1230, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:24m:11s remains)
INFO - root - 2017-12-06 05:58:10.324090: step 1240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 77h:00m:35s remains)
INFO - root - 2017-12-06 05:58:18.986223: step 1250, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 79h:23m:03s remains)
INFO - root - 2017-12-06 05:58:27.670560: step 1260, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 79h:01m:32s remains)
INFO - root - 2017-12-06 05:58:36.272859: step 1270, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 80h:09m:25s remains)
INFO - root - 2017-12-06 05:58:44.881611: step 1280, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 80h:47m:36s remains)
INFO - root - 2017-12-06 05:58:53.421029: step 1290, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.755 sec/batch; 69h:26m:56s remains)
INFO - root - 2017-12-06 05:59:01.706009: step 1300, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 77h:11m:30s remains)
2017-12-06 05:59:02.445367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1778359 -4.1923442 -4.2044678 -4.21156 -4.21092 -4.2057152 -4.1867046 -4.1292353 -4.0445819 -4.030375 -4.0714784 -4.1116476 -4.1642385 -4.2209969 -4.2638364][-4.139998 -4.151711 -4.1649828 -4.1758037 -4.180778 -4.1864204 -4.1758804 -4.1207929 -4.0347781 -4.0177236 -4.0576777 -4.0960555 -4.1491828 -4.2095394 -4.2569389][-4.1023912 -4.1094322 -4.1236982 -4.1392307 -4.1519127 -4.170238 -4.1662774 -4.1114955 -4.0240507 -4.0074434 -4.0510516 -4.0907383 -4.145946 -4.2077837 -4.2558656][-4.0686107 -4.0712628 -4.0860915 -4.1045632 -4.1231804 -4.1518097 -4.1511445 -4.09307 -4.002564 -3.9912009 -4.0446372 -4.0917344 -4.1512933 -4.2136087 -4.25993][-4.0379825 -4.0350771 -4.0488892 -4.0661216 -4.0842514 -4.1170921 -4.1170936 -4.053771 -3.96017 -3.9604003 -4.0298824 -4.0911579 -4.1592174 -4.2232971 -4.2671833][-4.0345583 -4.0231872 -4.0278206 -4.0340319 -4.0393291 -4.0649943 -4.0607033 -3.9903696 -3.8976803 -3.917562 -4.006916 -4.0843768 -4.1627131 -4.2300596 -4.273005][-4.0663939 -4.0476947 -4.0409088 -4.031991 -4.0189533 -4.0273886 -4.0102868 -3.9330435 -3.8431396 -3.8818796 -3.9866087 -4.0738769 -4.1595192 -4.2307119 -4.2747874][-4.1060882 -4.08635 -4.0746379 -4.056005 -4.0332222 -4.0270948 -3.9974616 -3.9200683 -3.8396034 -3.8875313 -3.9919689 -4.0746355 -4.1574688 -4.2289672 -4.2739248][-4.1463656 -4.1276364 -4.1135793 -4.0928063 -4.0714464 -4.0592909 -4.0254884 -3.9586964 -3.8950148 -3.9437089 -4.0337057 -4.0996461 -4.1685877 -4.2318759 -4.2734537][-4.1797304 -4.1624403 -4.1472507 -4.1295218 -4.1152048 -4.1037288 -4.0723152 -4.0195236 -3.9701922 -4.0150461 -4.0892549 -4.1384735 -4.1898642 -4.2401428 -4.2749538][-4.203907 -4.1861124 -4.1699543 -4.1577396 -4.1510181 -4.1415634 -4.11585 -4.0773849 -4.0375724 -4.0740314 -4.1350679 -4.1735678 -4.2124257 -4.2513089 -4.2789197][-4.2240891 -4.2082877 -4.1933651 -4.1849103 -4.1829863 -4.1752019 -4.1559863 -4.1292071 -4.0958133 -4.1229844 -4.17283 -4.2035222 -4.2330265 -4.2639022 -4.2850385][-4.2455754 -4.2350154 -4.2237434 -4.2183857 -4.2181754 -4.211987 -4.1979694 -4.1783738 -4.151166 -4.170198 -4.2085552 -4.23204 -4.254878 -4.2795506 -4.294838][-4.2691236 -4.2624106 -4.2545829 -4.2522092 -4.2527614 -4.2493353 -4.241087 -4.2272797 -4.2059069 -4.2183666 -4.2452993 -4.2624946 -4.2809339 -4.2996564 -4.3088517][-4.2888989 -4.2840362 -4.2795081 -4.2785807 -4.2782559 -4.277801 -4.275526 -4.2666059 -4.2508941 -4.2591524 -4.2777319 -4.2910476 -4.3063731 -4.3198204 -4.3233361]]...]
INFO - root - 2017-12-06 05:59:10.740114: step 1310, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.829 sec/batch; 76h:15m:09s remains)
INFO - root - 2017-12-06 05:59:19.245420: step 1320, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:37m:21s remains)
INFO - root - 2017-12-06 05:59:27.755702: step 1330, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 78h:14m:46s remains)
INFO - root - 2017-12-06 05:59:36.405508: step 1340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 78h:37m:12s remains)
INFO - root - 2017-12-06 05:59:44.951106: step 1350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 80h:02m:01s remains)
INFO - root - 2017-12-06 05:59:53.450064: step 1360, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 80h:43m:15s remains)
INFO - root - 2017-12-06 06:00:01.936859: step 1370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:32m:09s remains)
INFO - root - 2017-12-06 06:00:10.454268: step 1380, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 76h:24m:53s remains)
INFO - root - 2017-12-06 06:00:18.958644: step 1390, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.874 sec/batch; 80h:25m:25s remains)
INFO - root - 2017-12-06 06:00:27.589537: step 1400, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:56m:26s remains)
2017-12-06 06:00:28.474476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1042976 -4.0892515 -4.0906377 -4.1059761 -4.1189442 -4.1261868 -4.1293745 -4.1127615 -4.1122336 -4.1428933 -4.171164 -4.1703606 -4.1535125 -4.1387453 -4.1294909][-4.111887 -4.1012411 -4.1015644 -4.1160941 -4.12575 -4.1253843 -4.1155872 -4.087811 -4.0936575 -4.146287 -4.1920686 -4.2013736 -4.1969357 -4.1929417 -4.1860166][-4.1326571 -4.1295028 -4.1215267 -4.1286106 -4.1351156 -4.1269855 -4.1016545 -4.0636826 -4.0774136 -4.1453137 -4.2016635 -4.2243514 -4.2396183 -4.250237 -4.2436166][-4.1476197 -4.1450462 -4.1270719 -4.1242266 -4.1240544 -4.1035318 -4.0590043 -4.01772 -4.0542588 -4.1375179 -4.2028036 -4.2392416 -4.2737722 -4.2953854 -4.284996][-4.147305 -4.1449952 -4.124155 -4.1115766 -4.0964642 -4.0575957 -3.9895191 -3.9462943 -4.0179868 -4.1223783 -4.1969538 -4.2454681 -4.295341 -4.3229833 -4.3080592][-4.1501212 -4.1593742 -4.1431723 -4.1186814 -4.0828137 -4.0202451 -3.9239161 -3.874146 -3.9767907 -4.1026855 -4.18552 -4.2465291 -4.3055506 -4.3335266 -4.3093815][-4.1621261 -4.1902428 -4.1772547 -4.1407604 -4.0844274 -4.0006351 -3.8832922 -3.8207653 -3.9334903 -4.0696554 -4.1634626 -4.2404571 -4.30511 -4.3284969 -4.2935638][-4.1825523 -4.2209735 -4.2098289 -4.1649103 -4.0969391 -4.0043569 -3.8847082 -3.8173149 -3.9078774 -4.0337219 -4.1342387 -4.2225013 -4.29177 -4.3102145 -4.26743][-4.213275 -4.2508459 -4.2422309 -4.1960855 -4.1338544 -4.0540991 -3.9471037 -3.8826966 -3.9349113 -4.0327616 -4.1238155 -4.2101555 -4.2761593 -4.289454 -4.2442026][-4.2439151 -4.2752337 -4.2705379 -4.2330465 -4.186039 -4.1281772 -4.0382075 -3.9793866 -4.0015993 -4.0691991 -4.1417851 -4.2144952 -4.2691064 -4.2795057 -4.2373762][-4.2628784 -4.2856703 -4.2826796 -4.2549338 -4.2229323 -4.1865854 -4.1183667 -4.0660677 -4.0715876 -4.1177311 -4.1732359 -4.226438 -4.2667465 -4.2765436 -4.2419877][-4.2802906 -4.2973709 -4.2943749 -4.2702203 -4.2441816 -4.2209125 -4.1760764 -4.1346755 -4.1345854 -4.169075 -4.2123137 -4.2474704 -4.2750411 -4.2864008 -4.2598362][-4.2976875 -4.309104 -4.3053493 -4.2835746 -4.2608328 -4.2445831 -4.21777 -4.19046 -4.1898456 -4.21546 -4.2480345 -4.2703328 -4.2902236 -4.3018627 -4.2822628][-4.3124676 -4.3160076 -4.3109503 -4.294805 -4.2760906 -4.2639737 -4.2495737 -4.2362294 -4.2367764 -4.2552562 -4.27823 -4.29072 -4.3020983 -4.3107576 -4.2966647][-4.3181958 -4.3164773 -4.3115997 -4.3018842 -4.2890778 -4.2792253 -4.2713761 -4.2684741 -4.2723365 -4.2852807 -4.2998219 -4.30659 -4.3094082 -4.3121676 -4.3021173]]...]
INFO - root - 2017-12-06 06:00:36.854461: step 1410, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.778 sec/batch; 71h:31m:52s remains)
INFO - root - 2017-12-06 06:00:45.446045: step 1420, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:26m:24s remains)
INFO - root - 2017-12-06 06:00:53.937349: step 1430, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 79h:57m:48s remains)
INFO - root - 2017-12-06 06:01:02.558511: step 1440, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 83h:04m:31s remains)
INFO - root - 2017-12-06 06:01:11.029852: step 1450, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 77h:01m:35s remains)
INFO - root - 2017-12-06 06:01:19.579452: step 1460, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 78h:44m:01s remains)
INFO - root - 2017-12-06 06:01:28.223333: step 1470, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 79h:43m:21s remains)
INFO - root - 2017-12-06 06:01:36.723573: step 1480, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 76h:42m:00s remains)
INFO - root - 2017-12-06 06:01:45.375256: step 1490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 78h:44m:24s remains)
INFO - root - 2017-12-06 06:01:53.831468: step 1500, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 76h:01m:51s remains)
2017-12-06 06:01:54.584663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.224854 -4.2367048 -4.25234 -4.2530127 -4.2296972 -4.2221565 -4.2223773 -4.2292943 -4.2362585 -4.2228928 -4.2118144 -4.2273703 -4.2507019 -4.2602596 -4.2599559][-4.157865 -4.183145 -4.2217245 -4.2355113 -4.2102427 -4.19907 -4.2012334 -4.2117186 -4.2139988 -4.198482 -4.1875978 -4.2054315 -4.2346153 -4.2490826 -4.2498617][-4.1090841 -4.1434293 -4.1955013 -4.2133555 -4.1857867 -4.1677103 -4.1692185 -4.1815929 -4.1829309 -4.1734672 -4.165513 -4.1839237 -4.2148428 -4.2336068 -4.2383337][-4.0958848 -4.1292129 -4.1816897 -4.1973939 -4.16917 -4.145772 -4.1455555 -4.1572227 -4.1616921 -4.1608267 -4.1578903 -4.1765337 -4.2075214 -4.2270236 -4.2359467][-4.1005955 -4.1277103 -4.1745257 -4.1846333 -4.1502056 -4.1240506 -4.1293612 -4.1436467 -4.1523781 -4.1557012 -4.156714 -4.1776447 -4.2086992 -4.2288551 -4.2402449][-4.1177912 -4.1391349 -4.1745734 -4.1743 -4.1294665 -4.09846 -4.1104307 -4.1352739 -4.1510911 -4.15335 -4.1536803 -4.17413 -4.2057953 -4.2276993 -4.2425838][-4.1395955 -4.1554394 -4.1726589 -4.1578588 -4.1015472 -4.0588489 -4.065021 -4.098783 -4.126411 -4.131731 -4.1290617 -4.1478324 -4.1822753 -4.2114129 -4.2344351][-4.1363826 -4.1462107 -4.1441307 -4.11754 -4.05797 -4.0037112 -3.9918146 -4.0248594 -4.065186 -4.0787416 -4.0726242 -4.08879 -4.13149 -4.1766963 -4.2125731][-4.103138 -4.1112375 -4.1040258 -4.0781198 -4.0217547 -3.9570875 -3.9210849 -3.94272 -3.9886005 -4.0052276 -3.9926274 -4.001761 -4.0511332 -4.1149211 -4.1652236][-4.0743194 -4.0832582 -4.0795841 -4.0634775 -4.0197244 -3.9557316 -3.9046881 -3.9086802 -3.9473677 -3.964247 -3.9476075 -3.9501679 -3.9984632 -4.0674553 -4.1237097][-4.0607915 -4.0739021 -4.0750804 -4.0709324 -4.04812 -4.0038004 -3.9554224 -3.9430866 -3.96543 -3.9792755 -3.9638262 -3.9637814 -4.0031567 -4.0610375 -4.1112142][-4.0713096 -4.0898752 -4.0970435 -4.1022167 -4.09811 -4.0757346 -4.0364537 -4.0129147 -4.017611 -4.0268245 -4.0166831 -4.0173235 -4.0468135 -4.0921893 -4.1310639][-4.1212955 -4.1436286 -4.1531858 -4.160049 -4.1634903 -4.1555657 -4.1306748 -4.1066613 -4.0981669 -4.1023064 -4.0976357 -4.0981736 -4.1163683 -4.1476421 -4.1781836][-4.1876445 -4.2089767 -4.218936 -4.2247257 -4.2279363 -4.2264204 -4.2151718 -4.1984448 -4.1864142 -4.1851225 -4.1830516 -4.1835585 -4.1917329 -4.2105422 -4.2348685][-4.2512794 -4.2653723 -4.2732987 -4.2780695 -4.2807307 -4.2812176 -4.2784696 -4.2715917 -4.264627 -4.2618933 -4.2595792 -4.2598023 -4.26329 -4.2740092 -4.289299]]...]
INFO - root - 2017-12-06 06:02:03.158615: step 1510, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 81h:00m:59s remains)
INFO - root - 2017-12-06 06:02:11.605350: step 1520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 79h:53m:48s remains)
INFO - root - 2017-12-06 06:02:20.270112: step 1530, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 79h:05m:50s remains)
INFO - root - 2017-12-06 06:02:28.896597: step 1540, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 77h:39m:08s remains)
INFO - root - 2017-12-06 06:02:37.417365: step 1550, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 79h:07m:16s remains)
INFO - root - 2017-12-06 06:02:45.768078: step 1560, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 76h:11m:58s remains)
INFO - root - 2017-12-06 06:02:54.196979: step 1570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 77h:10m:29s remains)
INFO - root - 2017-12-06 06:03:02.646467: step 1580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 79h:33m:51s remains)
INFO - root - 2017-12-06 06:03:11.145267: step 1590, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 76h:39m:10s remains)
INFO - root - 2017-12-06 06:03:19.473777: step 1600, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 76h:47m:25s remains)
2017-12-06 06:03:20.203700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3304548 -4.3337317 -4.33425 -4.3336148 -4.3293672 -4.3222046 -4.3148026 -4.3062186 -4.3059688 -4.3118572 -4.3187971 -4.3294697 -4.3404408 -4.3473687 -4.3493028][-4.314445 -4.3194208 -4.3235106 -4.3264208 -4.3239255 -4.3177633 -4.3107524 -4.3017845 -4.3022275 -4.3106203 -4.3204265 -4.3333778 -4.3426719 -4.3437662 -4.3386669][-4.2947321 -4.2982969 -4.3045931 -4.3093195 -4.3061528 -4.2982874 -4.2909021 -4.28175 -4.2833319 -4.2948365 -4.3086867 -4.3238125 -4.3325291 -4.3278093 -4.31538][-4.2777042 -4.2798653 -4.2877703 -4.2898021 -4.2835011 -4.273632 -4.2651434 -4.25864 -4.2654767 -4.2826371 -4.3019996 -4.3198581 -4.327806 -4.319159 -4.3012438][-4.2645245 -4.263236 -4.2674341 -4.2612739 -4.2483115 -4.2324915 -4.2190018 -4.2101927 -4.2218924 -4.2526274 -4.283813 -4.3098903 -4.320961 -4.3152084 -4.2979121][-4.2565308 -4.2484956 -4.2400589 -4.2206097 -4.1996374 -4.1769471 -4.1541848 -4.1336861 -4.1463742 -4.195529 -4.2439241 -4.2813978 -4.3004642 -4.302762 -4.2921104][-4.2561049 -4.2408109 -4.2183781 -4.1848311 -4.1489172 -4.1057744 -4.0576696 -4.0124536 -4.0274854 -4.1070027 -4.1789389 -4.2332449 -4.2687244 -4.2815323 -4.2794747][-4.25966 -4.2404451 -4.2089953 -4.1654525 -4.1116343 -4.0391712 -3.9485486 -3.8635213 -3.8817136 -3.9940438 -4.0950217 -4.175776 -4.2333608 -4.2594881 -4.2675376][-4.2638669 -4.250186 -4.2270751 -4.1965857 -4.1543508 -4.0874805 -3.9948783 -3.9054275 -3.9138963 -4.0086226 -4.0992708 -4.1802459 -4.2379742 -4.2634754 -4.2705951][-4.268774 -4.2660856 -4.2595792 -4.2497458 -4.2254615 -4.1744866 -4.1040773 -4.036952 -4.0328493 -4.0860581 -4.1424327 -4.2030764 -4.2469106 -4.2668042 -4.2729125][-4.2534838 -4.2531004 -4.2559094 -4.2577138 -4.2413139 -4.1995258 -4.1483707 -4.10321 -4.09894 -4.130774 -4.1666188 -4.2093024 -4.2427468 -4.2597861 -4.2664537][-4.2347412 -4.2329788 -4.2393918 -4.2473526 -4.2384906 -4.2083087 -4.1769104 -4.1493087 -4.1495643 -4.1750326 -4.2019911 -4.2297149 -4.2527018 -4.2657571 -4.270628][-4.2240744 -4.2158589 -4.2184095 -4.223146 -4.2176352 -4.1984792 -4.1815767 -4.1646657 -4.1682267 -4.1915646 -4.2134027 -4.2368331 -4.2592664 -4.2745161 -4.2794533][-4.2219348 -4.2056456 -4.2009521 -4.1994481 -4.1949468 -4.18621 -4.18074 -4.1699028 -4.1774197 -4.2020483 -4.2238369 -4.2463145 -4.2687325 -4.2846308 -4.2878032][-4.232728 -4.2122636 -4.2034817 -4.1990767 -4.1955166 -4.1919484 -4.1907206 -4.1838479 -4.1918688 -4.2149844 -4.2349005 -4.2559948 -4.2780337 -4.2938294 -4.2952943]]...]
INFO - root - 2017-12-06 06:03:28.713053: step 1610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 78h:21m:58s remains)
INFO - root - 2017-12-06 06:03:37.234023: step 1620, loss = 2.04, batch loss = 1.99 (10.7 examples/sec; 0.749 sec/batch; 68h:51m:02s remains)
INFO - root - 2017-12-06 06:03:45.810789: step 1630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 79h:29m:24s remains)
INFO - root - 2017-12-06 06:03:54.381409: step 1640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:19m:18s remains)
INFO - root - 2017-12-06 06:04:02.960227: step 1650, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:19m:32s remains)
INFO - root - 2017-12-06 06:04:11.540556: step 1660, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 77h:10m:01s remains)
INFO - root - 2017-12-06 06:04:20.002588: step 1670, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 75h:54m:07s remains)
INFO - root - 2017-12-06 06:04:28.485653: step 1680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 77h:04m:07s remains)
INFO - root - 2017-12-06 06:04:36.908069: step 1690, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:49m:30s remains)
INFO - root - 2017-12-06 06:04:45.346212: step 1700, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 75h:06m:09s remains)
2017-12-06 06:04:46.138367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1566787 -4.1323647 -4.1516342 -4.2037358 -4.2605495 -4.3034911 -4.3234863 -4.325007 -4.3168964 -4.3045678 -4.3077736 -4.3166785 -4.3141341 -4.2982235 -4.2826972][-4.1064758 -4.0660892 -4.0962734 -4.1727333 -4.2453933 -4.2906384 -4.3065639 -4.3024487 -4.2977271 -4.2962971 -4.3071856 -4.3191342 -4.315928 -4.2951722 -4.2776766][-4.0552893 -4.0028725 -4.0450068 -4.1448665 -4.2295694 -4.2739935 -4.2813077 -4.2723784 -4.2752967 -4.286643 -4.3053613 -4.3211465 -4.3164124 -4.2913642 -4.272439][-4.0567813 -4.0127826 -4.0593319 -4.1582289 -4.2384219 -4.2686181 -4.2574215 -4.2373834 -4.2437687 -4.2656975 -4.2979345 -4.3242664 -4.3177896 -4.2870131 -4.2632694][-4.1137419 -4.1016083 -4.1459651 -4.2157717 -4.2686915 -4.2693968 -4.2263169 -4.1911931 -4.1993637 -4.2339616 -4.281559 -4.3196006 -4.314064 -4.2813096 -4.2562881][-4.1744924 -4.1905231 -4.2258863 -4.2578716 -4.2717056 -4.2298369 -4.1475658 -4.096035 -4.1205854 -4.1858063 -4.2571721 -4.302958 -4.2982097 -4.2680883 -4.2494173][-4.2216444 -4.2467842 -4.265533 -4.2617197 -4.230083 -4.1306744 -3.9919882 -3.9285791 -3.9971912 -4.1150765 -4.2172055 -4.2695022 -4.2677126 -4.2429914 -4.2333093][-4.2552524 -4.2787614 -4.281395 -4.2487497 -4.1794696 -4.0333567 -3.8513961 -3.7951627 -3.9175804 -4.0739083 -4.1897755 -4.2418218 -4.2416692 -4.2194023 -4.2165384][-4.2848272 -4.2980652 -4.2873812 -4.2452393 -4.1718802 -4.0378108 -3.8936582 -3.871671 -3.9862068 -4.1177578 -4.209619 -4.242794 -4.2319713 -4.2057576 -4.2073288][-4.2981038 -4.3017921 -4.2866859 -4.2529325 -4.2004132 -4.1140242 -4.0412121 -4.0496464 -4.1293912 -4.2141671 -4.2653961 -4.2680025 -4.2397447 -4.2087255 -4.2107859][-4.3012519 -4.30267 -4.2906318 -4.2672486 -4.2369628 -4.189703 -4.1611834 -4.1807661 -4.2341433 -4.2864308 -4.3096762 -4.2926254 -4.2550111 -4.2225986 -4.2233639][-4.3027639 -4.3062525 -4.2994609 -4.2877793 -4.2735863 -4.2464433 -4.2316995 -4.24798 -4.2835851 -4.3140917 -4.3226318 -4.3025837 -4.2727752 -4.2475333 -4.2450008][-4.3073125 -4.3079762 -4.3031974 -4.2989488 -4.2944903 -4.2796087 -4.2702155 -4.2795806 -4.2987618 -4.3124957 -4.3137321 -4.2990375 -4.2822785 -4.2664328 -4.2629995][-4.3146691 -4.3124061 -4.3075242 -4.3058567 -4.3064723 -4.3019829 -4.2998319 -4.3039908 -4.31038 -4.3123827 -4.3093586 -4.2995238 -4.2906809 -4.2805443 -4.2787418][-4.3217793 -4.3183131 -4.3142061 -4.3138976 -4.3169971 -4.3191381 -4.3217068 -4.3232789 -4.3226347 -4.3184371 -4.3129988 -4.30655 -4.3024654 -4.2980824 -4.2982163]]...]
INFO - root - 2017-12-06 06:04:54.647454: step 1710, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.865 sec/batch; 79h:27m:49s remains)
INFO - root - 2017-12-06 06:05:03.405730: step 1720, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:40m:24s remains)
INFO - root - 2017-12-06 06:05:12.056879: step 1730, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 82h:08m:13s remains)
INFO - root - 2017-12-06 06:05:20.660636: step 1740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 78h:31m:38s remains)
INFO - root - 2017-12-06 06:05:29.204491: step 1750, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 80h:12m:49s remains)
INFO - root - 2017-12-06 06:05:37.764341: step 1760, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 76h:47m:21s remains)
INFO - root - 2017-12-06 06:05:46.396943: step 1770, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 78h:37m:17s remains)
INFO - root - 2017-12-06 06:05:54.946267: step 1780, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 81h:16m:30s remains)
INFO - root - 2017-12-06 06:06:03.512753: step 1790, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 79h:19m:45s remains)
INFO - root - 2017-12-06 06:06:11.898388: step 1800, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 80h:31m:43s remains)
2017-12-06 06:06:12.696205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229815 -4.2122426 -4.2017913 -4.191041 -4.1902456 -4.2004828 -4.2151308 -4.2268238 -4.23681 -4.2448506 -4.2481761 -4.2499466 -4.2492847 -4.2452936 -4.2379751][-4.1803412 -4.1614251 -4.152307 -4.149972 -4.1583862 -4.177669 -4.1971064 -4.209383 -4.2187247 -4.2240644 -4.2230453 -4.2204771 -4.2173967 -4.2144403 -4.2109275][-4.131609 -4.0986533 -4.0858059 -4.089426 -4.1083589 -4.1386824 -4.1677928 -4.1878123 -4.2005658 -4.2048464 -4.2013178 -4.1980028 -4.1963272 -4.1971369 -4.1989031][-4.0984526 -4.0573635 -4.0393925 -4.0400386 -4.058598 -4.0944314 -4.1351385 -4.1676788 -4.1867685 -4.1890907 -4.1837244 -4.178309 -4.1766586 -4.1827068 -4.1903687][-4.1144972 -4.0826197 -4.0633788 -4.0484171 -4.0505247 -4.0734577 -4.1166677 -4.1631069 -4.1917605 -4.1915169 -4.1810627 -4.1689768 -4.1609063 -4.1643705 -4.175848][-4.151473 -4.1349115 -4.1187911 -4.0878444 -4.0585642 -4.0480709 -4.0779119 -4.134738 -4.179513 -4.1888876 -4.1786184 -4.1593895 -4.1377764 -4.1266713 -4.1346307][-4.1636906 -4.1653919 -4.1596441 -4.1210837 -4.0597692 -4.0097575 -4.0146594 -4.0739937 -4.1446686 -4.178144 -4.1742821 -4.1492729 -4.1106429 -4.0715084 -4.0683804][-4.1429873 -4.1639223 -4.1700954 -4.141645 -4.0641975 -3.9748161 -3.9446213 -3.99992 -4.0937376 -4.154108 -4.1587725 -4.1315804 -4.0719342 -3.9980454 -3.980381][-4.1072941 -4.1380229 -4.156971 -4.14561 -4.0612979 -3.9303916 -3.8571131 -3.9076738 -4.026166 -4.1085825 -4.132865 -4.11599 -4.0479264 -3.9523046 -3.9188967][-4.0811882 -4.1145787 -4.1390138 -4.1379061 -4.0628924 -3.9152842 -3.8072066 -3.8448842 -3.9763806 -4.074224 -4.11868 -4.1189871 -4.0684257 -3.9866347 -3.9510565][-4.0694957 -4.096271 -4.1241922 -4.1322775 -4.08866 -3.9848471 -3.8862889 -3.8914049 -3.9871461 -4.0744352 -4.125617 -4.1438646 -4.1240969 -4.072772 -4.0449939][-4.0880218 -4.1018386 -4.1304536 -4.1453266 -4.1361651 -4.0991096 -4.0494361 -4.0325651 -4.0701804 -4.1238251 -4.1629567 -4.1853118 -4.1863322 -4.158772 -4.1400986][-4.153048 -4.1494861 -4.16527 -4.1780195 -4.185925 -4.1955323 -4.1872993 -4.1731296 -4.1761103 -4.196908 -4.2164407 -4.2288594 -4.2332287 -4.2208924 -4.2091575][-4.2290211 -4.217082 -4.2186766 -4.2214661 -4.22868 -4.2503753 -4.2589459 -4.2510996 -4.2472539 -4.2565341 -4.2657375 -4.2678161 -4.2687035 -4.2669907 -4.2596846][-4.2873917 -4.2772574 -4.271235 -4.2665057 -4.2699885 -4.2887278 -4.3003407 -4.3000212 -4.2980762 -4.3043637 -4.3113251 -4.309958 -4.3089819 -4.3093815 -4.3054223]]...]
INFO - root - 2017-12-06 06:06:21.296016: step 1810, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.790 sec/batch; 72h:33m:47s remains)
INFO - root - 2017-12-06 06:06:29.960037: step 1820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:52m:18s remains)
INFO - root - 2017-12-06 06:06:38.414984: step 1830, loss = 2.03, batch loss = 1.98 (10.7 examples/sec; 0.750 sec/batch; 68h:54m:32s remains)
INFO - root - 2017-12-06 06:06:46.978245: step 1840, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 79h:32m:36s remains)
INFO - root - 2017-12-06 06:06:55.570600: step 1850, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 76h:10m:55s remains)
INFO - root - 2017-12-06 06:07:04.161356: step 1860, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.857 sec/batch; 78h:40m:02s remains)
INFO - root - 2017-12-06 06:07:12.654630: step 1870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:59m:09s remains)
INFO - root - 2017-12-06 06:07:21.284952: step 1880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 79h:02m:37s remains)
INFO - root - 2017-12-06 06:07:29.771004: step 1890, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.807 sec/batch; 74h:08m:52s remains)
INFO - root - 2017-12-06 06:07:38.277655: step 1900, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:14m:13s remains)
2017-12-06 06:07:39.043242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2004042 -4.1922097 -4.192533 -4.203609 -4.224741 -4.2472944 -4.2641993 -4.2779903 -4.2862954 -4.2860103 -4.2784886 -4.2688975 -4.2568803 -4.2383265 -4.2139893][-4.2066126 -4.2014771 -4.2051373 -4.2219553 -4.2449884 -4.2678838 -4.2838621 -4.2956343 -4.3021641 -4.3024554 -4.298327 -4.2937117 -4.2869759 -4.271009 -4.2503052][-4.22976 -4.22543 -4.230227 -4.2475104 -4.2671022 -4.2887774 -4.3068724 -4.3184786 -4.3240347 -4.326015 -4.3282218 -4.3316689 -4.3315444 -4.3206911 -4.3057747][-4.2549114 -4.2453985 -4.2438254 -4.2530146 -4.2634149 -4.278883 -4.2927427 -4.3018312 -4.3095126 -4.3171306 -4.3252773 -4.3342042 -4.3371325 -4.3324676 -4.324492][-4.2739105 -4.25694 -4.2430553 -4.2372036 -4.2307343 -4.229238 -4.2277427 -4.2321467 -4.2474728 -4.2685037 -4.2879276 -4.302268 -4.3070498 -4.3078976 -4.3076634][-4.2721272 -4.2499166 -4.221426 -4.1907825 -4.1558747 -4.1265054 -4.1023488 -4.1036639 -4.1338415 -4.1725044 -4.2047234 -4.2269063 -4.2376771 -4.245904 -4.2557154][-4.2403235 -4.2082763 -4.1624403 -4.1023173 -4.0361419 -3.9747648 -3.9246874 -3.9258106 -3.9779482 -4.0424585 -4.0950718 -4.1286 -4.149456 -4.1676269 -4.18717][-4.2015047 -4.1619248 -4.1042233 -4.0297675 -3.9533598 -3.8867269 -3.8366723 -3.852283 -3.9265754 -4.0119081 -4.0818224 -4.1255374 -4.1484623 -4.1609526 -4.1711869][-4.1978116 -4.1686306 -4.12685 -4.075676 -4.03229 -4.0013914 -3.9824204 -4.0074587 -4.0671906 -4.1297054 -4.1804214 -4.20938 -4.2172184 -4.2141018 -4.206717][-4.2080874 -4.1934886 -4.1727672 -4.1518583 -4.1418819 -4.138917 -4.1359339 -4.1534362 -4.1887856 -4.2245169 -4.2541866 -4.2689652 -4.2675519 -4.2562094 -4.2398648][-4.221755 -4.2175069 -4.2141104 -4.2165179 -4.22437 -4.2299304 -4.2300029 -4.238687 -4.2582064 -4.2766485 -4.2889919 -4.2904496 -4.2822971 -4.2685924 -4.2514915][-4.2498808 -4.2504325 -4.2578316 -4.2707148 -4.283267 -4.2870708 -4.2855344 -4.2903004 -4.3002462 -4.3052797 -4.3031683 -4.2960515 -4.2891083 -4.2788692 -4.2642875][-4.2684183 -4.2720008 -4.2831984 -4.2961106 -4.3038478 -4.3035073 -4.3000717 -4.3000603 -4.3032994 -4.3014688 -4.2934256 -4.2870288 -4.28563 -4.2776661 -4.2644129][-4.2668028 -4.2708492 -4.2802439 -4.2882118 -4.2920036 -4.2917876 -4.2898564 -4.2913675 -4.2943735 -4.2922034 -4.2848592 -4.2808075 -4.2813182 -4.2731466 -4.2589369][-4.2358141 -4.2380276 -4.2443829 -4.2463913 -4.2453094 -4.2480865 -4.2540607 -4.2635708 -4.2698717 -4.2696738 -4.2671428 -4.2691932 -4.2744513 -4.2714076 -4.260098]]...]
INFO - root - 2017-12-06 06:07:47.584423: step 1910, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 77h:06m:58s remains)
INFO - root - 2017-12-06 06:07:56.004296: step 1920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 77h:05m:18s remains)
INFO - root - 2017-12-06 06:08:04.652257: step 1930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 78h:59m:21s remains)
INFO - root - 2017-12-06 06:08:13.124520: step 1940, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 77h:00m:34s remains)
INFO - root - 2017-12-06 06:08:21.674090: step 1950, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 79h:04m:51s remains)
INFO - root - 2017-12-06 06:08:30.213386: step 1960, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 76h:11m:10s remains)
INFO - root - 2017-12-06 06:08:38.783470: step 1970, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 79h:33m:04s remains)
INFO - root - 2017-12-06 06:08:47.346544: step 1980, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 76h:11m:28s remains)
INFO - root - 2017-12-06 06:08:55.898987: step 1990, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 79h:31m:51s remains)
INFO - root - 2017-12-06 06:09:04.423988: step 2000, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:36m:32s remains)
2017-12-06 06:09:05.209250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.233922 -4.2325745 -4.2412252 -4.2474637 -4.2460155 -4.2440491 -4.2467332 -4.2449389 -4.241509 -4.2368793 -4.2344227 -4.2414522 -4.2497692 -4.2527475 -4.2592435][-4.2351556 -4.2325096 -4.2430325 -4.2503548 -4.2435336 -4.2357292 -4.2399635 -4.2410707 -4.2345424 -4.2210779 -4.2094502 -4.208787 -4.2134118 -4.2177939 -4.2292151][-4.2429175 -4.2411203 -4.2505136 -4.255805 -4.2406278 -4.2236204 -4.2258391 -4.2337027 -4.2329454 -4.2183309 -4.1984162 -4.1842675 -4.1765137 -4.1759048 -4.1842704][-4.2330194 -4.2316847 -4.2379327 -4.2401967 -4.2196231 -4.1948128 -4.1914754 -4.2085443 -4.2205105 -4.20993 -4.1855812 -4.1626482 -4.1440015 -4.1387377 -4.14493][-4.20137 -4.1958542 -4.1977711 -4.1999974 -4.1775379 -4.140945 -4.1262121 -4.1521945 -4.1832 -4.1868124 -4.1693206 -4.1480684 -4.1269279 -4.1194458 -4.1277695][-4.1855078 -4.1663976 -4.1593866 -4.1600137 -4.1305923 -4.0686779 -4.0251212 -4.0558724 -4.1212339 -4.1588635 -4.1623983 -4.1514468 -4.1341977 -4.12843 -4.1406035][-4.1936207 -4.1569853 -4.1369095 -4.1316419 -4.0933113 -4.0019846 -3.9192038 -3.9472818 -4.0453749 -4.1194286 -4.1513314 -4.1563234 -4.1454077 -4.1437573 -4.1617961][-4.2035575 -4.1593819 -4.1315012 -4.125052 -4.0887589 -3.9962127 -3.9034579 -3.9165525 -4.0103288 -4.0938597 -4.1403546 -4.1529465 -4.1422267 -4.1404543 -4.1614389][-4.212853 -4.1768475 -4.1560416 -4.1611037 -4.1459222 -4.085103 -4.0136337 -4.0031023 -4.0501094 -4.1053433 -4.1421638 -4.1510634 -4.1381793 -4.1342163 -4.1557989][-4.1987143 -4.1848912 -4.1851082 -4.2073517 -4.2134471 -4.1817908 -4.1341305 -4.1073718 -4.1130757 -4.1359334 -4.1577892 -4.161633 -4.1495142 -4.1415572 -4.159183][-4.1694679 -4.1735024 -4.1919332 -4.2268434 -4.2483473 -4.2367854 -4.2092662 -4.1808352 -4.1678586 -4.1742845 -4.1886215 -4.1902723 -4.1800356 -4.1675744 -4.1761007][-4.1558437 -4.1634483 -4.1861868 -4.2207222 -4.248456 -4.2527237 -4.2405524 -4.2196469 -4.2058992 -4.2071114 -4.2166371 -4.2195458 -4.212018 -4.1995511 -4.2018056][-4.1645164 -4.1661148 -4.1812382 -4.2073383 -4.2359896 -4.2514567 -4.2491713 -4.2345734 -4.2250357 -4.2251377 -4.2306147 -4.2357364 -4.23381 -4.2282004 -4.22888][-4.1834383 -4.1763096 -4.1810431 -4.1962442 -4.2197628 -4.2390852 -4.2429805 -4.2341695 -4.2272892 -4.2272639 -4.2309046 -4.2370954 -4.2395854 -4.2399893 -4.2431321][-4.21149 -4.2022691 -4.1990547 -4.2043896 -4.2201519 -4.23692 -4.2440867 -4.2418451 -4.2392817 -4.2391372 -4.2418876 -4.2491927 -4.2526746 -4.2521291 -4.25382]]...]
INFO - root - 2017-12-06 06:09:13.837941: step 2010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 78h:22m:20s remains)
INFO - root - 2017-12-06 06:09:22.342076: step 2020, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 79h:30m:36s remains)
INFO - root - 2017-12-06 06:09:30.917108: step 2030, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:45m:04s remains)
INFO - root - 2017-12-06 06:09:39.356372: step 2040, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.775 sec/batch; 71h:08m:41s remains)
INFO - root - 2017-12-06 06:09:47.890489: step 2050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:34m:26s remains)
INFO - root - 2017-12-06 06:09:56.385771: step 2060, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 76h:27m:40s remains)
INFO - root - 2017-12-06 06:10:04.887192: step 2070, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 79h:12m:27s remains)
INFO - root - 2017-12-06 06:10:13.636031: step 2080, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.939 sec/batch; 86h:09m:19s remains)
INFO - root - 2017-12-06 06:10:22.286742: step 2090, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 78h:05m:27s remains)
INFO - root - 2017-12-06 06:10:30.744194: step 2100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:11m:23s remains)
2017-12-06 06:10:31.494860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.133626 -4.1325049 -4.1426611 -4.1701479 -4.195817 -4.2015543 -4.2090983 -4.2213573 -4.2289371 -4.2421312 -4.271503 -4.2989306 -4.3153486 -4.3149986 -4.3013849][-4.1531653 -4.1576843 -4.172688 -4.1970892 -4.2176228 -4.2224112 -4.2319093 -4.2435303 -4.2588429 -4.27987 -4.3097749 -4.3288283 -4.3372297 -4.3332582 -4.31639][-4.175004 -4.1725221 -4.1860271 -4.2067604 -4.2224588 -4.22751 -4.2365217 -4.2462587 -4.2626581 -4.2883005 -4.3207393 -4.3372917 -4.3438387 -4.33843 -4.3191519][-4.1911726 -4.1789026 -4.19162 -4.2147207 -4.2269025 -4.2221613 -4.2195797 -4.2214947 -4.2348495 -4.2696838 -4.3097391 -4.3272214 -4.3366261 -4.3350234 -4.3170514][-4.20431 -4.1896758 -4.2017183 -4.2227912 -4.2245317 -4.1989312 -4.1737742 -4.1647844 -4.1774549 -4.2266359 -4.2793474 -4.3076954 -4.3242168 -4.3273668 -4.3115821][-4.2162657 -4.2079177 -4.2153025 -4.2262297 -4.2125568 -4.1584716 -4.1007404 -4.0766344 -4.0968504 -4.1638789 -4.2335625 -4.2775874 -4.3059444 -4.3201151 -4.3132195][-4.2276521 -4.2303286 -4.238183 -4.2385926 -4.2124243 -4.1359849 -4.040266 -3.9877663 -4.00999 -4.0968437 -4.1870027 -4.2486515 -4.2896042 -4.3126473 -4.3137131][-4.2419238 -4.2558384 -4.2635069 -4.2568707 -4.2252512 -4.1462631 -4.0389609 -3.9619632 -3.9725058 -4.0628657 -4.1597886 -4.2313428 -4.2804441 -4.3019276 -4.3017807][-4.2398038 -4.2616563 -4.2680745 -4.2566934 -4.2265754 -4.1610708 -4.0656152 -3.9875305 -3.9843056 -4.0572405 -4.1473236 -4.2207174 -4.2755408 -4.2959447 -4.2902732][-4.2258563 -4.2495837 -4.2551455 -4.243433 -4.2193961 -4.1682782 -4.0898237 -4.0244246 -4.0218582 -4.0778084 -4.1523314 -4.2174039 -4.2704105 -4.2908635 -4.2818069][-4.2236714 -4.2433968 -4.2467556 -4.2364135 -4.2190418 -4.1797371 -4.1135621 -4.0600395 -4.0649686 -4.1153674 -4.1777272 -4.2296805 -4.267735 -4.2829857 -4.2759504][-4.237793 -4.2483826 -4.2419195 -4.2305989 -4.2186508 -4.1925478 -4.1438479 -4.1080489 -4.1209769 -4.1682835 -4.2167335 -4.25243 -4.2716031 -4.2748203 -4.2695713][-4.2640676 -4.2700815 -4.2572875 -4.24252 -4.2294807 -4.21041 -4.1790514 -4.1601882 -4.1773667 -4.2186384 -4.2506671 -4.2647038 -4.2574434 -4.2448931 -4.2405095][-4.2772422 -4.2864361 -4.2791576 -4.2662663 -4.2537894 -4.2432809 -4.2260909 -4.21889 -4.233984 -4.2636428 -4.2791514 -4.2738371 -4.24465 -4.21314 -4.205544][-4.2752743 -4.2860851 -4.2856402 -4.2822785 -4.2765951 -4.2737956 -4.2681036 -4.2697597 -4.2827854 -4.3001728 -4.3006678 -4.2791381 -4.2349391 -4.1909294 -4.1781154]]...]
INFO - root - 2017-12-06 06:10:40.125047: step 2110, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 78h:38m:27s remains)
INFO - root - 2017-12-06 06:10:48.677741: step 2120, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 80h:49m:56s remains)
INFO - root - 2017-12-06 06:10:57.356603: step 2130, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 79h:07m:43s remains)
INFO - root - 2017-12-06 06:11:05.865328: step 2140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 80h:17m:40s remains)
INFO - root - 2017-12-06 06:11:14.232848: step 2150, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 77h:53m:23s remains)
INFO - root - 2017-12-06 06:11:22.704951: step 2160, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 79h:11m:50s remains)
INFO - root - 2017-12-06 06:11:31.201225: step 2170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 78h:45m:43s remains)
INFO - root - 2017-12-06 06:11:39.719576: step 2180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 77h:39m:38s remains)
INFO - root - 2017-12-06 06:11:48.165234: step 2190, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 75h:15m:27s remains)
INFO - root - 2017-12-06 06:11:56.567886: step 2200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 79h:11m:13s remains)
2017-12-06 06:11:57.296613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0395966 -4.038825 -4.1120815 -4.1776261 -4.2227378 -4.2504272 -4.260026 -4.2544537 -4.2588406 -4.2772722 -4.2964153 -4.3129063 -4.3200521 -4.31753 -4.3086853][-3.9749467 -3.9756169 -4.072938 -4.1546936 -4.2028503 -4.2244925 -4.2248259 -4.21548 -4.2269073 -4.2556677 -4.2853866 -4.309938 -4.3224845 -4.3188205 -4.3064647][-3.9782128 -3.98489 -4.0903463 -4.1696696 -4.2017579 -4.201745 -4.1790962 -4.161222 -4.1812344 -4.2226734 -4.2645359 -4.2997971 -4.3193617 -4.3173623 -4.3045893][-4.074193 -4.089201 -4.170095 -4.2205544 -4.220767 -4.1848745 -4.1273274 -4.09828 -4.1315894 -4.1877117 -4.2377329 -4.2813368 -4.3087859 -4.3115978 -4.3026266][-4.1518917 -4.1710844 -4.2244449 -4.2460485 -4.2204261 -4.147624 -4.0505347 -4.014936 -4.0760307 -4.1536012 -4.2121797 -4.2613845 -4.2941475 -4.3007255 -4.2982154][-4.197104 -4.214766 -4.246747 -4.2449894 -4.201684 -4.0968218 -3.9578154 -3.9156189 -4.0118723 -4.1152043 -4.18791 -4.2433114 -4.27697 -4.2869425 -4.291121][-4.2157545 -4.2380066 -4.2627444 -4.2469134 -4.1927247 -4.0751705 -3.9187737 -3.8734598 -3.9879987 -4.1031237 -4.1766663 -4.2292519 -4.2597528 -4.2715845 -4.281014][-4.1962705 -4.2286491 -4.2575254 -4.2446804 -4.1992607 -4.1114163 -3.9968846 -3.9643016 -4.0508928 -4.1405807 -4.194828 -4.2323213 -4.2513757 -4.2599149 -4.2702174][-4.1390691 -4.1860819 -4.2335658 -4.2376685 -4.2126369 -4.1626368 -4.0987382 -4.0804229 -4.1320167 -4.189065 -4.2228909 -4.2440205 -4.2503839 -4.2529545 -4.2623205][-4.0624971 -4.1226439 -4.1955037 -4.2234716 -4.2205825 -4.1969562 -4.1601338 -4.1516247 -4.1837859 -4.2186623 -4.2398276 -4.2517295 -4.2503166 -4.2487745 -4.2588696][-4.0046825 -4.0750852 -4.1737771 -4.2204065 -4.22947 -4.21738 -4.1934171 -4.1944876 -4.2182088 -4.2371254 -4.2496142 -4.2573233 -4.2547364 -4.252336 -4.2620482][-4.0412273 -4.107831 -4.2031879 -4.2450843 -4.24574 -4.2210765 -4.190114 -4.193119 -4.2175527 -4.2347007 -4.2464733 -4.2562776 -4.2611957 -4.2633877 -4.2723055][-4.1208735 -4.1731977 -4.2433167 -4.2663007 -4.247117 -4.197135 -4.1432047 -4.1409359 -4.1797705 -4.21477 -4.237607 -4.2542973 -4.2659416 -4.2694817 -4.2771592][-4.1776342 -4.2109909 -4.2542715 -4.2597117 -4.2230082 -4.1443315 -4.0532227 -4.0404949 -4.1108036 -4.1804037 -4.224237 -4.2485867 -4.2603931 -4.259769 -4.2651262][-4.2167125 -4.2389746 -4.2644243 -4.2526665 -4.2015748 -4.097054 -3.9696198 -3.945354 -4.0509834 -4.1538329 -4.2129774 -4.2380624 -4.2446547 -4.2381754 -4.2405457]]...]
INFO - root - 2017-12-06 06:12:05.985632: step 2210, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 77h:48m:30s remains)
INFO - root - 2017-12-06 06:12:14.612676: step 2220, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 76h:35m:35s remains)
INFO - root - 2017-12-06 06:12:23.205583: step 2230, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 77h:10m:28s remains)
INFO - root - 2017-12-06 06:12:31.761865: step 2240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:50m:14s remains)
INFO - root - 2017-12-06 06:12:40.328560: step 2250, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 80h:05m:26s remains)
INFO - root - 2017-12-06 06:12:48.659683: step 2260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:28m:43s remains)
INFO - root - 2017-12-06 06:12:57.274642: step 2270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:18m:24s remains)
INFO - root - 2017-12-06 06:13:05.877005: step 2280, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:19m:17s remains)
INFO - root - 2017-12-06 06:13:14.412951: step 2290, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 80h:34m:16s remains)
INFO - root - 2017-12-06 06:13:22.834511: step 2300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 78h:00m:44s remains)
2017-12-06 06:13:23.638322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537308 -4.2497697 -4.2386527 -4.2132955 -4.1837387 -4.1625419 -4.1381083 -4.1260781 -4.1126308 -4.0950642 -4.0793233 -4.0732932 -4.0775261 -4.1132636 -4.1652575][-4.257051 -4.2376871 -4.2111778 -4.1779819 -4.1458321 -4.1213951 -4.0982547 -4.0804658 -4.0512271 -4.0286527 -4.0411372 -4.0769653 -4.1124887 -4.1597342 -4.2128682][-4.2442045 -4.2115021 -4.173521 -4.1384845 -4.1154203 -4.0987706 -4.0771208 -4.05248 -4.0149164 -3.9991748 -4.0323749 -4.0862317 -4.1363935 -4.1888018 -4.2418222][-4.2335596 -4.2003403 -4.1606445 -4.1247263 -4.1026497 -4.0888166 -4.0655055 -4.0347576 -4.0127735 -4.0214376 -4.0588651 -4.1101561 -4.1616058 -4.2112069 -4.2569728][-4.23534 -4.2064948 -4.1699853 -4.1278019 -4.0949421 -4.0671449 -4.0249772 -3.9870212 -4.0021238 -4.051065 -4.0983071 -4.1520414 -4.2011137 -4.2421904 -4.2723274][-4.2227545 -4.207253 -4.1782064 -4.12763 -4.07155 -4.005815 -3.9293094 -3.8849108 -3.9510617 -4.0502667 -4.1196074 -4.1834979 -4.234755 -4.2685757 -4.284636][-4.19612 -4.1851463 -4.16084 -4.10439 -4.0289612 -3.9378648 -3.8417358 -3.8132412 -3.9238243 -4.0536642 -4.1391153 -4.2055058 -4.2552381 -4.2836795 -4.2918043][-4.1492209 -4.1421075 -4.1292458 -4.0894656 -4.0356021 -3.9728038 -3.917851 -3.9195735 -4.0101128 -4.1096439 -4.1758127 -4.2265058 -4.2639122 -4.285687 -4.2904377][-4.0886903 -4.1030812 -4.1183891 -4.1111469 -4.0914478 -4.0717559 -4.0605354 -4.0816369 -4.1368766 -4.1934671 -4.2292981 -4.2558427 -4.2744837 -4.2857466 -4.2914081][-4.0773048 -4.1111293 -4.1398263 -4.1458969 -4.1415682 -4.1392045 -4.1448417 -4.1693482 -4.211205 -4.24818 -4.2699523 -4.2843952 -4.2893291 -4.2915573 -4.2981706][-4.1080565 -4.1405163 -4.1620364 -4.1687913 -4.1713982 -4.1749258 -4.1848836 -4.2054033 -4.2370815 -4.263206 -4.2838387 -4.2989144 -4.3026228 -4.3044105 -4.3107238][-4.1532407 -4.1742949 -4.1868916 -4.1941848 -4.1953273 -4.1937876 -4.1999559 -4.2179637 -4.2433195 -4.2651305 -4.2871909 -4.3051586 -4.3114095 -4.3148336 -4.3195205][-4.1789074 -4.1937051 -4.20088 -4.2069206 -4.21008 -4.2075605 -4.2078977 -4.2224369 -4.2466512 -4.2682362 -4.2921844 -4.3108392 -4.3177676 -4.3212385 -4.3228817][-4.1841493 -4.1968703 -4.2010112 -4.2033191 -4.2115722 -4.2223449 -4.2303152 -4.243896 -4.2667975 -4.28649 -4.3063726 -4.319994 -4.3232074 -4.3238673 -4.3241296][-4.1976976 -4.2128067 -4.2168417 -4.216187 -4.228241 -4.2494974 -4.2637835 -4.278595 -4.2956734 -4.308321 -4.3202915 -4.3261318 -4.326098 -4.325038 -4.3239889]]...]
INFO - root - 2017-12-06 06:13:32.323505: step 2310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 78h:29m:43s remains)
INFO - root - 2017-12-06 06:13:40.816998: step 2320, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 79h:47m:50s remains)
INFO - root - 2017-12-06 06:13:49.367063: step 2330, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 79h:06m:17s remains)
INFO - root - 2017-12-06 06:13:57.980470: step 2340, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 79h:38m:15s remains)
INFO - root - 2017-12-06 06:14:06.596149: step 2350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 78h:46m:52s remains)
INFO - root - 2017-12-06 06:14:15.098248: step 2360, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 77h:08m:38s remains)
INFO - root - 2017-12-06 06:14:23.747942: step 2370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 78h:06m:48s remains)
INFO - root - 2017-12-06 06:14:32.296642: step 2380, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 80h:43m:02s remains)
INFO - root - 2017-12-06 06:14:40.842987: step 2390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 78h:10m:33s remains)
INFO - root - 2017-12-06 06:14:49.247077: step 2400, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.811 sec/batch; 74h:24m:35s remains)
2017-12-06 06:14:49.965479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2399292 -4.2143855 -4.1903634 -4.16963 -4.15339 -4.1499019 -4.1678495 -4.2032924 -4.2263732 -4.2482953 -4.27423 -4.2791409 -4.2797332 -4.2803807 -4.2663889][-4.2225409 -4.1948748 -4.1690273 -4.1412959 -4.1167994 -4.1092653 -4.13316 -4.1771164 -4.2050147 -4.2321157 -4.2640228 -4.2729187 -4.2695522 -4.2600079 -4.2363844][-4.2066832 -4.1746583 -4.15071 -4.1197619 -4.0843968 -4.0695982 -4.0971985 -4.144949 -4.1756411 -4.2022667 -4.2354989 -4.2461877 -4.24365 -4.2306037 -4.2030206][-4.2040443 -4.1695104 -4.1449 -4.1131673 -4.0638857 -4.0404606 -4.0703421 -4.123322 -4.1583409 -4.1818 -4.2097526 -4.2158523 -4.2141418 -4.2072954 -4.1828876][-4.2038693 -4.16853 -4.1409888 -4.1043572 -4.0469956 -4.017015 -4.0439978 -4.09788 -4.1375141 -4.1639805 -4.1914535 -4.1963696 -4.1983647 -4.1997533 -4.180511][-4.1892743 -4.1533737 -4.1234288 -4.0808434 -4.020421 -3.990015 -4.0103216 -4.0577908 -4.0974712 -4.1296849 -4.1608481 -4.1751547 -4.1834517 -4.1936946 -4.1839247][-4.177865 -4.136343 -4.0987077 -4.0463181 -3.987309 -3.9672728 -3.9867585 -4.0234165 -4.0586066 -4.09173 -4.1281018 -4.1585007 -4.1772676 -4.1937709 -4.1966257][-4.1873636 -4.1451159 -4.1017528 -4.0434203 -3.9903603 -3.9851623 -4.0101528 -4.0395451 -4.0677562 -4.0951395 -4.1250434 -4.1582704 -4.1810894 -4.1953559 -4.20198][-4.20757 -4.1737223 -4.1291323 -4.0710583 -4.0273108 -4.0336475 -4.0678921 -4.0964966 -4.1201057 -4.1389337 -4.1563349 -4.1821513 -4.2049594 -4.2133565 -4.2151651][-4.2324677 -4.2143388 -4.177958 -4.1251831 -4.0907459 -4.099143 -4.1337171 -4.165308 -4.1887817 -4.2002459 -4.2078791 -4.2233858 -4.2437658 -4.2494764 -4.2440062][-4.2545228 -4.25107 -4.2269826 -4.1868553 -4.1611376 -4.1655045 -4.1896944 -4.2190118 -4.2441468 -4.2546968 -4.2563581 -4.2597346 -4.2726431 -4.2763157 -4.2691145][-4.2696452 -4.2738166 -4.2619376 -4.2357712 -4.2183309 -4.2196765 -4.2319012 -4.2513175 -4.2728348 -4.2815056 -4.2819562 -4.2796173 -4.2863584 -4.2905684 -4.284873][-4.2839632 -4.2905941 -4.2868743 -4.2729821 -4.2632089 -4.2625127 -4.2666793 -4.2763996 -4.2896404 -4.2955918 -4.2952356 -4.2893686 -4.2898617 -4.2918444 -4.2873769][-4.2900009 -4.2940903 -4.2934923 -4.2873912 -4.2840304 -4.2827916 -4.2834644 -4.2890444 -4.2983451 -4.3028297 -4.3026862 -4.2977495 -4.29512 -4.29292 -4.2889624][-4.2940254 -4.29439 -4.2945943 -4.2930136 -4.2930365 -4.2932963 -4.2933197 -4.2957826 -4.3016734 -4.3055539 -4.3050776 -4.3016472 -4.2986312 -4.2954822 -4.2923675]]...]
INFO - root - 2017-12-06 06:14:58.522274: step 2410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:20m:40s remains)
INFO - root - 2017-12-06 06:15:06.752324: step 2420, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 77h:04m:01s remains)
INFO - root - 2017-12-06 06:15:15.333112: step 2430, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 78h:35m:08s remains)
INFO - root - 2017-12-06 06:15:23.777388: step 2440, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 78h:24m:38s remains)
INFO - root - 2017-12-06 06:15:32.367129: step 2450, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.853 sec/batch; 78h:11m:25s remains)
INFO - root - 2017-12-06 06:15:40.955761: step 2460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:18m:19s remains)
INFO - root - 2017-12-06 06:15:49.345213: step 2470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 77h:05m:43s remains)
INFO - root - 2017-12-06 06:15:57.817200: step 2480, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 76h:22m:40s remains)
INFO - root - 2017-12-06 06:16:06.410473: step 2490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 77h:49m:31s remains)
INFO - root - 2017-12-06 06:16:14.877278: step 2500, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 76h:02m:41s remains)
2017-12-06 06:16:15.699228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2924509 -4.2916079 -4.2883377 -4.2857013 -4.2803249 -4.275589 -4.2742887 -4.2827373 -4.2955794 -4.3030925 -4.3037457 -4.2959027 -4.2898488 -4.2890186 -4.2928653][-4.2895713 -4.2881212 -4.2812762 -4.2739153 -4.2638912 -4.2525897 -4.2435217 -4.2518477 -4.2731738 -4.2878819 -4.2920642 -4.2840958 -4.2758088 -4.2705431 -4.2691369][-4.2798576 -4.27541 -4.26145 -4.2478676 -4.2334986 -4.2157826 -4.2028513 -4.2151408 -4.2458 -4.2683635 -4.2789397 -4.273766 -4.2642775 -4.2541361 -4.2479253][-4.25374 -4.2452965 -4.2231936 -4.2079444 -4.193397 -4.1717768 -4.1588373 -4.1731987 -4.210001 -4.2378392 -4.252387 -4.2527142 -4.2482648 -4.23745 -4.2279377][-4.23456 -4.2185011 -4.19019 -4.1781282 -4.1631746 -4.1366177 -4.1185408 -4.1287417 -4.1727595 -4.2045851 -4.2212892 -4.22821 -4.2276769 -4.2170186 -4.20639][-4.2198553 -4.1987691 -4.164156 -4.1471634 -4.1267376 -4.0926733 -4.0648904 -4.0690255 -4.1181693 -4.1591706 -4.1827254 -4.1917362 -4.1958942 -4.1946573 -4.192214][-4.2109528 -4.1884284 -4.1516123 -4.1190262 -4.0834074 -4.0374222 -3.9920373 -3.9729207 -4.0223465 -4.086731 -4.1336293 -4.1547041 -4.16919 -4.1783619 -4.1873369][-4.2134371 -4.1901307 -4.1554809 -4.1068797 -4.0490828 -3.9846733 -3.9113708 -3.8609536 -3.9045715 -3.9969957 -4.070487 -4.1118436 -4.1385479 -4.1542244 -4.1714859][-4.2161317 -4.1903043 -4.1542325 -4.0949631 -4.0245638 -3.9625585 -3.8875997 -3.8330588 -3.8768008 -3.9706287 -4.0381613 -4.0752516 -4.1065321 -4.1279726 -4.1485596][-4.2157063 -4.1946268 -4.1604824 -4.098402 -4.0249734 -3.9702942 -3.9108944 -3.8734479 -3.9157834 -3.9854527 -4.0304775 -4.0499425 -4.0711722 -4.0901222 -4.1089149][-4.1991377 -4.1860442 -4.1628108 -4.1157179 -4.0546784 -4.0061541 -3.9578969 -3.929074 -3.9585559 -4.0029097 -4.0278816 -4.0386696 -4.0514359 -4.06453 -4.0802908][-4.1705151 -4.1621161 -4.1543307 -4.1318631 -4.0951238 -4.058465 -4.0191526 -3.9971249 -4.0101652 -4.0291915 -4.0334921 -4.0389767 -4.04999 -4.0655975 -4.0818753][-4.1715927 -4.1683497 -4.1711736 -4.1608109 -4.1404486 -4.1165519 -4.0881138 -4.072731 -4.0829878 -4.0898185 -4.0786328 -4.0745115 -4.0873141 -4.1055107 -4.1205192][-4.1936693 -4.1942935 -4.2006783 -4.1952128 -4.1845489 -4.1697521 -4.1518888 -4.1426563 -4.1524358 -4.1541958 -4.1387429 -4.1314707 -4.1450024 -4.163064 -4.1767][-4.22836 -4.2320113 -4.2397871 -4.23803 -4.2323074 -4.2268119 -4.2180023 -4.2126746 -4.2229705 -4.2262282 -4.214704 -4.20965 -4.2207909 -4.2332549 -4.2408996]]...]
INFO - root - 2017-12-06 06:16:24.153906: step 2510, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 78h:15m:14s remains)
INFO - root - 2017-12-06 06:16:32.626834: step 2520, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.778 sec/batch; 71h:16m:17s remains)
INFO - root - 2017-12-06 06:16:41.161946: step 2530, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 80h:10m:10s remains)
INFO - root - 2017-12-06 06:16:49.763068: step 2540, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 77h:02m:32s remains)
INFO - root - 2017-12-06 06:16:58.282881: step 2550, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 75h:50m:22s remains)
INFO - root - 2017-12-06 06:17:06.775923: step 2560, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 77h:27m:04s remains)
INFO - root - 2017-12-06 06:17:15.154676: step 2570, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 77h:47m:49s remains)
INFO - root - 2017-12-06 06:17:23.704002: step 2580, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 75h:41m:56s remains)
INFO - root - 2017-12-06 06:17:32.268522: step 2590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 78h:35m:46s remains)
INFO - root - 2017-12-06 06:17:40.802628: step 2600, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 79h:03m:31s remains)
2017-12-06 06:17:41.599592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1934185 -4.2054234 -4.2173948 -4.2221341 -4.2216105 -4.2132325 -4.1994672 -4.1832209 -4.1742263 -4.1651821 -4.1555448 -4.1480622 -4.146368 -4.1644807 -4.19085][-4.179832 -4.1862822 -4.200068 -4.209095 -4.2148829 -4.2159662 -4.2118621 -4.1976728 -4.1779628 -4.1531506 -4.1291685 -4.1167631 -4.1153016 -4.1344733 -4.1717997][-4.16739 -4.1729717 -4.1919427 -4.2071424 -4.2185493 -4.2250085 -4.2236462 -4.2083273 -4.1777921 -4.1396065 -4.1054897 -4.0900087 -4.0869288 -4.1027918 -4.1480093][-4.1667805 -4.1767654 -4.1996665 -4.2175145 -4.2248874 -4.2233453 -4.2159233 -4.2012486 -4.1680636 -4.1227255 -4.0852604 -4.0653496 -4.0556154 -4.0651894 -4.1130977][-4.1759233 -4.187521 -4.2076955 -4.22028 -4.2210641 -4.2097716 -4.19458 -4.1808791 -4.1534839 -4.1136012 -4.0756917 -4.0484138 -4.0308518 -4.0336013 -4.0807986][-4.1863284 -4.19713 -4.2119675 -4.2195086 -4.2120409 -4.1903749 -4.1654081 -4.1461139 -4.1268916 -4.1008477 -4.0695276 -4.0371614 -4.0160022 -4.01607 -4.0626345][-4.1866879 -4.1947556 -4.2053719 -4.2115097 -4.1995077 -4.1685662 -4.1299572 -4.0996051 -4.0868354 -4.0779762 -4.059381 -4.035234 -4.0199933 -4.0254979 -4.0698156][-4.1787057 -4.1887565 -4.1996241 -4.2064614 -4.1937275 -4.1555409 -4.1044865 -4.0651197 -4.0582786 -4.0627408 -4.0588436 -4.046999 -4.0396118 -4.0479751 -4.0837536][-4.1758471 -4.1949997 -4.2092075 -4.2162237 -4.2003875 -4.1598415 -4.1088386 -4.0734782 -4.0687013 -4.0771761 -4.084095 -4.0821724 -4.079792 -4.0858517 -4.1084342][-4.1724286 -4.2005725 -4.2180667 -4.2262735 -4.2139454 -4.1771064 -4.1332607 -4.1065679 -4.1040683 -4.1143064 -4.1268516 -4.1314535 -4.1297398 -4.1324916 -4.1439085][-4.1703348 -4.202642 -4.2248292 -4.2338572 -4.2239146 -4.1918268 -4.1577692 -4.1424532 -4.1481037 -4.16219 -4.1732769 -4.1726856 -4.1670904 -4.1687193 -4.1767325][-4.1785951 -4.2113175 -4.2353716 -4.244607 -4.2352228 -4.206161 -4.1801476 -4.1733422 -4.1862044 -4.2010932 -4.20601 -4.1990752 -4.192256 -4.1953468 -4.2058878][-4.1906443 -4.2211723 -4.244647 -4.2552505 -4.2502851 -4.22842 -4.2103143 -4.2097645 -4.2238874 -4.2356048 -4.2342587 -4.2236271 -4.2170296 -4.2199059 -4.2292604][-4.2126842 -4.2341838 -4.2550178 -4.2674403 -4.2685013 -4.2570672 -4.2479272 -4.24963 -4.2592387 -4.2653165 -4.2602592 -4.2489567 -4.2450585 -4.248364 -4.2558074][-4.2481394 -4.257926 -4.27149 -4.2822652 -4.286356 -4.2826519 -4.2788787 -4.2809076 -4.2870502 -4.2899389 -4.2855926 -4.2781677 -4.2772288 -4.2807741 -4.2859325]]...]
INFO - root - 2017-12-06 06:17:50.244590: step 2610, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:35m:13s remains)
INFO - root - 2017-12-06 06:17:58.812992: step 2620, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.815 sec/batch; 74h:43m:29s remains)
INFO - root - 2017-12-06 06:18:07.465683: step 2630, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 81h:22m:20s remains)
INFO - root - 2017-12-06 06:18:16.065043: step 2640, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 75h:58m:22s remains)
INFO - root - 2017-12-06 06:18:22.644329: step 2650, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:26m:45s remains)
INFO - root - 2017-12-06 06:18:29.042005: step 2660, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:07m:22s remains)
INFO - root - 2017-12-06 06:18:35.597768: step 2670, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 60h:10m:46s remains)
INFO - root - 2017-12-06 06:18:41.856490: step 2680, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 59h:11m:16s remains)
INFO - root - 2017-12-06 06:18:48.374194: step 2690, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:42m:19s remains)
INFO - root - 2017-12-06 06:18:54.878230: step 2700, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 58h:11m:04s remains)
2017-12-06 06:18:55.508119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2235818 -4.2411728 -4.2738581 -4.3041186 -4.3166771 -4.3173614 -4.3173194 -4.3198757 -4.3191633 -4.3151374 -4.3109059 -4.3078842 -4.306571 -4.3059788 -4.3057618][-4.2401657 -4.2533112 -4.2768121 -4.297205 -4.3041511 -4.3015823 -4.3018084 -4.30519 -4.3062911 -4.3045697 -4.3027964 -4.3025036 -4.3026819 -4.3022494 -4.3023043][-4.2737875 -4.27802 -4.2879767 -4.2928128 -4.2892718 -4.2800946 -4.2784786 -4.283442 -4.2883887 -4.2919769 -4.2957039 -4.3005381 -4.3040218 -4.30418 -4.3019958][-4.2849288 -4.2832608 -4.2818294 -4.2754993 -4.2644572 -4.2504711 -4.2460475 -4.2526393 -4.2623596 -4.2725329 -4.2822037 -4.2915068 -4.2981691 -4.2974758 -4.2897668][-4.2521 -4.2457356 -4.2374725 -4.2211671 -4.2048025 -4.1893716 -4.1851621 -4.1975079 -4.2164 -4.2351065 -4.2511559 -4.266304 -4.279 -4.2781124 -4.2636046][-4.1969962 -4.1875734 -4.1732121 -4.1528158 -4.1363959 -4.1221132 -4.1171546 -4.1313939 -4.1603556 -4.1921244 -4.2171545 -4.2373447 -4.2528138 -4.250617 -4.2292223][-4.1294265 -4.1140428 -4.089849 -4.06587 -4.0494666 -4.0357842 -4.0262761 -4.0381165 -4.0785155 -4.126687 -4.1607018 -4.183804 -4.2013388 -4.1976991 -4.1745305][-4.0764365 -4.0537024 -4.0209165 -3.9937952 -3.9769094 -3.959043 -3.9405141 -3.9437432 -3.9887269 -4.0490155 -4.0925565 -4.1209021 -4.1406093 -4.1378193 -4.11798][-4.0857234 -4.06453 -4.0402212 -4.0263782 -4.0205669 -4.0077281 -3.9882138 -3.985754 -4.0230713 -4.0750818 -4.1108909 -4.1309748 -4.1406016 -4.1292496 -4.1024709][-4.1433835 -4.1291275 -4.1178126 -4.1179867 -4.1217756 -4.1180353 -4.1068959 -4.1055965 -4.1299987 -4.1646352 -4.1867723 -4.1964407 -4.1971459 -4.1825457 -4.1534424][-4.2096019 -4.1966624 -4.1856904 -4.1852694 -4.1894965 -4.1910558 -4.1896281 -4.192615 -4.2075129 -4.2282829 -4.2374992 -4.2375083 -4.2340932 -4.2234564 -4.2005973][-4.2351947 -4.2226129 -4.20672 -4.1978612 -4.1980138 -4.2037945 -4.2084136 -4.2086787 -4.2152534 -4.2273908 -4.2258177 -4.2170472 -4.2130814 -4.2111812 -4.201117][-4.2186022 -4.2026114 -4.1799178 -4.1621408 -4.158392 -4.1671352 -4.1766248 -4.17567 -4.1792183 -4.1888614 -4.1852245 -4.1784453 -4.1819181 -4.192389 -4.1971569][-4.2023492 -4.1829433 -4.1563306 -4.1346097 -4.1302662 -4.1431084 -4.1557817 -4.1541839 -4.1575389 -4.1694 -4.1704111 -4.1699681 -4.1806221 -4.2002664 -4.2119122][-4.1988983 -4.1807966 -4.1579089 -4.139761 -4.1386185 -4.1528368 -4.1651978 -4.1614389 -4.1628881 -4.174325 -4.1787133 -4.1817565 -4.1954756 -4.2168016 -4.2312808]]...]
INFO - root - 2017-12-06 06:19:01.916076: step 2710, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.630 sec/batch; 57h:43m:25s remains)
INFO - root - 2017-12-06 06:19:08.306079: step 2720, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 58h:28m:17s remains)
INFO - root - 2017-12-06 06:19:14.596302: step 2730, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 58h:34m:18s remains)
INFO - root - 2017-12-06 06:19:21.039279: step 2740, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 59h:10m:57s remains)
INFO - root - 2017-12-06 06:19:27.492737: step 2750, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:59m:30s remains)
INFO - root - 2017-12-06 06:19:33.969104: step 2760, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 59h:47m:35s remains)
INFO - root - 2017-12-06 06:19:40.509875: step 2770, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:17m:11s remains)
INFO - root - 2017-12-06 06:19:46.741607: step 2780, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 57h:44m:57s remains)
INFO - root - 2017-12-06 06:19:53.156561: step 2790, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:08m:13s remains)
INFO - root - 2017-12-06 06:19:59.628785: step 2800, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:15m:47s remains)
2017-12-06 06:20:00.255398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1738992 -4.1751394 -4.1743832 -4.1768708 -4.1823368 -4.1719012 -4.1254611 -4.0749803 -4.0721846 -4.1163187 -4.1794958 -4.2376442 -4.2799692 -4.3124638 -4.3311305][-4.1305962 -4.1203852 -4.1150765 -4.1203213 -4.1315584 -4.1263518 -4.0821733 -4.03403 -4.0364652 -4.0904608 -4.1638818 -4.2280574 -4.272922 -4.3047185 -4.3230042][-4.1272087 -4.0995378 -4.0824494 -4.08285 -4.0957165 -4.0986977 -4.0670667 -4.0272365 -4.0310984 -4.0902209 -4.1647353 -4.2258511 -4.267705 -4.2979574 -4.3157754][-4.16371 -4.1227584 -4.0902886 -4.0746503 -4.0766253 -4.0825458 -4.0660295 -4.0404572 -4.0478373 -4.1121221 -4.1852393 -4.2373776 -4.2744417 -4.3025384 -4.3181129][-4.2029948 -4.1572 -4.1148844 -4.08401 -4.0648794 -4.0596685 -4.0490375 -4.030714 -4.0418868 -4.1159735 -4.1969857 -4.245822 -4.2798319 -4.3079882 -4.3232679][-4.22923 -4.18148 -4.1303005 -4.08226 -4.0376678 -4.0129118 -3.9958105 -3.9800255 -3.9999514 -4.0857062 -4.1805129 -4.234653 -4.2707148 -4.3027973 -4.3222561][-4.2388673 -4.1917353 -4.13241 -4.0603786 -3.9881115 -3.9542477 -3.9327881 -3.9154007 -3.9435229 -4.0466943 -4.1550035 -4.2180543 -4.2583694 -4.2955513 -4.31994][-4.23809 -4.1943774 -4.1307688 -4.0403647 -3.94402 -3.9060957 -3.8865461 -3.8716247 -3.9071796 -4.02002 -4.135498 -4.2063885 -4.2525172 -4.2903261 -4.3182144][-4.2286997 -4.1889825 -4.1308217 -4.0471439 -3.951355 -3.9190617 -3.8990843 -3.8845239 -3.9158545 -4.0209484 -4.1336479 -4.2073579 -4.2552261 -4.292284 -4.319613][-4.2185326 -4.182313 -4.1395583 -4.0948 -4.0404315 -4.0135078 -3.9839344 -3.9573026 -3.9704442 -4.0490093 -4.1460381 -4.2141552 -4.256959 -4.2911177 -4.3175993][-4.197515 -4.1636925 -4.1352682 -4.1246796 -4.1092262 -4.0932832 -4.0648484 -4.0351896 -4.0369892 -4.0917439 -4.170114 -4.2276855 -4.2625728 -4.2919 -4.3159704][-4.1670103 -4.1311007 -4.1110573 -4.1177945 -4.1217289 -4.1138206 -4.08949 -4.0702953 -4.0799031 -4.1254005 -4.189374 -4.2366867 -4.2678747 -4.2972603 -4.3188291][-4.1565681 -4.120429 -4.1065974 -4.1224318 -4.1284761 -4.1173968 -4.0945034 -4.0869408 -4.1032958 -4.1433105 -4.1985965 -4.240263 -4.2717795 -4.30292 -4.3239093][-4.1625924 -4.1309071 -4.1276746 -4.1481009 -4.1504049 -4.1328034 -4.1075835 -4.1037874 -4.1236391 -4.1588497 -4.2071486 -4.2432513 -4.2728724 -4.3074603 -4.32814][-4.1671071 -4.1287589 -4.1239433 -4.1451154 -4.1529031 -4.1416779 -4.11999 -4.1203823 -4.144 -4.1789355 -4.2180605 -4.2469206 -4.2715259 -4.3069096 -4.3291054]]...]
INFO - root - 2017-12-06 06:20:06.702728: step 2810, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:39m:27s remains)
INFO - root - 2017-12-06 06:20:13.191831: step 2820, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.635 sec/batch; 58h:07m:47s remains)
INFO - root - 2017-12-06 06:20:19.595336: step 2830, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 59h:45m:23s remains)
INFO - root - 2017-12-06 06:20:26.149659: step 2840, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 59h:20m:14s remains)
INFO - root - 2017-12-06 06:20:32.635674: step 2850, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 59h:24m:27s remains)
INFO - root - 2017-12-06 06:20:39.145695: step 2860, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 57h:17m:06s remains)
INFO - root - 2017-12-06 06:20:45.600720: step 2870, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 57h:20m:08s remains)
INFO - root - 2017-12-06 06:20:52.002297: step 2880, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:14m:06s remains)
INFO - root - 2017-12-06 06:20:58.258475: step 2890, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 57h:35m:25s remains)
INFO - root - 2017-12-06 06:21:04.677998: step 2900, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:25m:56s remains)
2017-12-06 06:21:05.297096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.326086 -4.3307071 -4.3337097 -4.3361487 -4.3382931 -4.3388596 -4.3352475 -4.3285427 -4.324472 -4.3268051 -4.3334312 -4.3403811 -4.34411 -4.3446045 -4.3418889][-4.3284774 -4.3331084 -4.3363013 -4.3386955 -4.3405151 -4.3403215 -4.3369503 -4.3330388 -4.3327255 -4.3381491 -4.3457694 -4.3514075 -4.3518562 -4.349371 -4.344203][-4.330853 -4.3350716 -4.3371348 -4.3365736 -4.3320279 -4.3215523 -4.3074312 -4.295126 -4.2914438 -4.3008957 -4.3176231 -4.3339286 -4.343226 -4.3465552 -4.3443923][-4.3314948 -4.3352752 -4.3351316 -4.3285308 -4.3116293 -4.2830873 -4.2480507 -4.2175503 -4.204946 -4.219563 -4.2526689 -4.2906027 -4.3200083 -4.3379674 -4.3435802][-4.329145 -4.3320804 -4.3289518 -4.3134117 -4.2778192 -4.22208 -4.1552172 -4.0969949 -4.070034 -4.0927858 -4.1515641 -4.2218127 -4.2816668 -4.3208585 -4.3376451][-4.3255048 -4.3275547 -4.3209605 -4.2942691 -4.2346072 -4.1441703 -4.0369987 -3.9393764 -3.8905795 -3.9272733 -4.0237088 -4.1352558 -4.2304482 -4.2940025 -4.3239908][-4.3240585 -4.3249345 -4.3145862 -4.2780352 -4.2000532 -4.0813928 -3.9366477 -3.7966261 -3.72128 -3.7747016 -3.9137213 -4.065372 -4.1891928 -4.2707963 -4.3104157][-4.3278222 -4.328196 -4.3174357 -4.280282 -4.2010469 -4.07715 -3.9224279 -3.766788 -3.6788044 -3.7356653 -3.8861625 -4.0475512 -4.1771779 -4.2618232 -4.3038926][-4.33448 -4.3339262 -4.3268347 -4.30011 -4.2418447 -4.1495686 -4.03524 -3.9202898 -3.8558812 -3.8880646 -3.9893274 -4.1075149 -4.2096715 -4.2780671 -4.3107667][-4.3334208 -4.3266673 -4.3196678 -4.3072429 -4.2814908 -4.2387137 -4.183311 -4.1233182 -4.0872426 -4.0961056 -4.142158 -4.2055111 -4.2675924 -4.3106446 -4.3276367][-4.3102484 -4.2888379 -4.274157 -4.2705851 -4.2747469 -4.2769542 -4.27264 -4.2579575 -4.2459617 -4.2466216 -4.260859 -4.2854691 -4.3161836 -4.3380327 -4.3430705][-4.2631459 -4.217947 -4.1863103 -4.18321 -4.2113132 -4.2532549 -4.2931242 -4.3155193 -4.3231907 -4.3230872 -4.3225207 -4.3274941 -4.3411036 -4.3507395 -4.3492832][-4.1857305 -4.1037741 -4.0423837 -4.032805 -4.0863619 -4.1701245 -4.2532697 -4.3102961 -4.33905 -4.3459692 -4.3422303 -4.3401546 -4.3458872 -4.3497233 -4.3464265][-4.0948095 -3.9670618 -3.8621116 -3.8394151 -3.9242866 -4.0546679 -4.1798234 -4.2697773 -4.3193855 -4.3378034 -4.3389525 -4.3373108 -4.3412466 -4.3434834 -4.3405728][-4.02863 -3.8671479 -3.7267773 -3.6903253 -3.8000007 -3.9662595 -4.1206717 -4.2315159 -4.2945027 -4.3213263 -4.3287945 -4.3297234 -4.3338776 -4.3360186 -4.3338947]]...]
INFO - root - 2017-12-06 06:21:11.775018: step 2910, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 59h:41m:54s remains)
INFO - root - 2017-12-06 06:21:18.282287: step 2920, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 59h:06m:18s remains)
INFO - root - 2017-12-06 06:21:24.597867: step 2930, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.652 sec/batch; 59h:41m:46s remains)
INFO - root - 2017-12-06 06:21:31.120294: step 2940, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 60h:00m:27s remains)
INFO - root - 2017-12-06 06:21:37.511598: step 2950, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.634 sec/batch; 58h:00m:56s remains)
INFO - root - 2017-12-06 06:21:43.962431: step 2960, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.640 sec/batch; 58h:37m:06s remains)
INFO - root - 2017-12-06 06:21:50.457131: step 2970, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.766 sec/batch; 70h:04m:24s remains)
INFO - root - 2017-12-06 06:21:56.905807: step 2980, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:10m:13s remains)
INFO - root - 2017-12-06 06:22:03.161012: step 2990, loss = 2.07, batch loss = 2.01 (15.8 examples/sec; 0.505 sec/batch; 46h:15m:59s remains)
INFO - root - 2017-12-06 06:22:09.515818: step 3000, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 59h:14m:22s remains)
2017-12-06 06:22:10.186546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1690946 -4.1891618 -4.2300916 -4.2649245 -4.2709613 -4.2365136 -4.1816392 -4.1401415 -4.13821 -4.1567163 -4.1743321 -4.1855092 -4.2034993 -4.2033777 -4.1955051][-4.1582413 -4.1742063 -4.214705 -4.252646 -4.2634134 -4.230185 -4.1738462 -4.1314783 -4.1316938 -4.1548548 -4.1710606 -4.1759291 -4.1876631 -4.1856294 -4.1789541][-4.14089 -4.1533141 -4.18933 -4.2297387 -4.2491317 -4.2273951 -4.1873674 -4.1609893 -4.1733351 -4.198698 -4.209475 -4.2086415 -4.2118912 -4.2042923 -4.1964226][-4.1287479 -4.1356316 -4.1563597 -4.1867695 -4.207695 -4.1955276 -4.1743546 -4.1669779 -4.19368 -4.2277532 -4.2447615 -4.2475662 -4.248662 -4.2434492 -4.2372651][-4.117435 -4.1142769 -4.1126151 -4.1267252 -4.1463952 -4.1372743 -4.1153994 -4.1082683 -4.1439571 -4.1890712 -4.2177386 -4.2291794 -4.234077 -4.2372684 -4.2418222][-4.1007767 -4.0827479 -4.0524731 -4.0432496 -4.0563545 -4.0433345 -4.0024214 -3.978277 -4.0245652 -4.0986867 -4.1500793 -4.1724014 -4.1813726 -4.1929874 -4.2095461][-4.0997872 -4.0555315 -3.9882932 -3.9445033 -3.9392202 -3.9104083 -3.8343925 -3.7776773 -3.8469317 -3.9730566 -4.0588326 -4.0884461 -4.0964627 -4.1177068 -4.1479597][-4.1399059 -4.0811319 -3.9944522 -3.9287925 -3.9067304 -3.866132 -3.7635195 -3.6756139 -3.7609622 -3.9194345 -4.0199013 -4.0440979 -4.0423522 -4.0616055 -4.0928469][-4.207233 -4.1582794 -4.0875216 -4.0290642 -4.0074539 -3.9791307 -3.9111998 -3.856885 -3.9143279 -4.0282216 -4.1015806 -4.10672 -4.0800505 -4.0768704 -4.0928783][-4.2682185 -4.238389 -4.1952562 -4.1548195 -4.1383924 -4.1206045 -4.0872016 -4.0587378 -4.0867453 -4.1477518 -4.1842895 -4.1720953 -4.1350985 -4.1182766 -4.1205549][-4.2971177 -4.2817731 -4.2612491 -4.2404251 -4.2309704 -4.22019 -4.199667 -4.1797996 -4.1841803 -4.2036228 -4.2035222 -4.1710052 -4.1286244 -4.1031122 -4.0908422][-4.309288 -4.301476 -4.2923226 -4.2830305 -4.2779908 -4.26963 -4.2526293 -4.2338176 -4.2252822 -4.21855 -4.1889691 -4.1350079 -4.0845146 -4.0488873 -4.0227633][-4.3191781 -4.3135357 -4.308475 -4.3045378 -4.3018646 -4.2956958 -4.2810516 -4.2616053 -4.2444563 -4.2229004 -4.1764636 -4.1092153 -4.0502577 -4.0102458 -3.9804308][-4.3274064 -4.3219485 -4.31723 -4.3143077 -4.3133154 -4.3099904 -4.3000059 -4.2831445 -4.262042 -4.2351742 -4.1906719 -4.1301532 -4.0756369 -4.0412951 -4.015409][-4.338213 -4.3343778 -4.3304782 -4.328 -4.3263278 -4.3231068 -4.3158484 -4.3027167 -4.2839885 -4.2586274 -4.2256341 -4.1861129 -4.1471415 -4.1233258 -4.1060996]]...]
INFO - root - 2017-12-06 06:22:16.618963: step 3010, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:43m:07s remains)
INFO - root - 2017-12-06 06:22:23.032861: step 3020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:24m:18s remains)
INFO - root - 2017-12-06 06:22:29.403002: step 3030, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 58h:28m:00s remains)
INFO - root - 2017-12-06 06:22:35.812455: step 3040, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 60h:24m:57s remains)
INFO - root - 2017-12-06 06:22:42.293238: step 3050, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 57h:55m:16s remains)
INFO - root - 2017-12-06 06:22:48.677438: step 3060, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 61h:27m:50s remains)
INFO - root - 2017-12-06 06:22:55.110984: step 3070, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 58h:30m:12s remains)
INFO - root - 2017-12-06 06:23:01.542462: step 3080, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:50m:57s remains)
INFO - root - 2017-12-06 06:23:07.910803: step 3090, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 57h:19m:26s remains)
INFO - root - 2017-12-06 06:23:14.271000: step 3100, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 58h:04m:27s remains)
2017-12-06 06:23:14.936480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3155055 -4.3177547 -4.3188992 -4.3163276 -4.3127985 -4.3085332 -4.3032351 -4.3016539 -4.299005 -4.2925563 -4.2853889 -4.2791152 -4.2741189 -4.272964 -4.2753134][-4.3427305 -4.3465514 -4.3466954 -4.3410625 -4.3344088 -4.3248658 -4.3134575 -4.3067689 -4.2977314 -4.2856483 -4.2744021 -4.2659988 -4.2555919 -4.2493382 -4.2453737][-4.3601332 -4.3651042 -4.3615956 -4.3496323 -4.3360286 -4.3199253 -4.3023567 -4.2901378 -4.279521 -4.269362 -4.2590714 -4.2470746 -4.2304215 -4.2176251 -4.2036805][-4.3553929 -4.3597059 -4.3525681 -4.3346024 -4.3151774 -4.2936821 -4.2705269 -4.2505293 -4.2387991 -4.2355638 -4.2284222 -4.2091203 -4.1831622 -4.16405 -4.1393247][-4.3271222 -4.3287678 -4.3180542 -4.295548 -4.2721009 -4.2437534 -4.2115254 -4.1785579 -4.1643004 -4.1716633 -4.1700716 -4.1465087 -4.1155562 -4.0961442 -4.0741992][-4.2936025 -4.288238 -4.2685204 -4.2366438 -4.2045417 -4.1646323 -4.1144085 -4.06393 -4.0505667 -4.0742216 -4.0828238 -4.0630264 -4.0405092 -4.0387096 -4.038249][-4.2737284 -4.2575631 -4.2215514 -4.1718483 -4.1251392 -4.0726595 -4.0003443 -3.9211054 -3.9099815 -3.9635835 -3.9978764 -3.9987004 -4.0046492 -4.0338759 -4.0637641][-4.2723541 -4.2450213 -4.1914134 -4.1221423 -4.060493 -4.0025735 -3.9168684 -3.8118265 -3.8057551 -3.8917019 -3.9543447 -3.9836903 -4.0254235 -4.0847421 -4.1348028][-4.2761331 -4.2465348 -4.1898875 -4.1168594 -4.0533319 -4.0019579 -3.9313271 -3.84323 -3.8375921 -3.916348 -3.9805126 -4.021533 -4.0765181 -4.1447406 -4.199368][-4.2941074 -4.2696843 -4.2243638 -4.1652727 -4.1090784 -4.0659423 -4.0188951 -3.9657962 -3.957119 -4.0027161 -4.0478029 -4.0841737 -4.1328673 -4.1939125 -4.2437468][-4.3139186 -4.2993193 -4.2698741 -4.2294707 -4.1868653 -4.1550765 -4.12691 -4.0975208 -4.0870514 -4.1084151 -4.1369648 -4.1627789 -4.1952338 -4.2360072 -4.2711658][-4.325397 -4.319469 -4.3034968 -4.2804747 -4.2543325 -4.2352366 -4.22042 -4.2053885 -4.1974277 -4.2061453 -4.2226281 -4.2362871 -4.2504354 -4.2701216 -4.2898169][-4.3279591 -4.3279724 -4.3206925 -4.30917 -4.2967553 -4.2889414 -4.2837834 -4.278327 -4.2761459 -4.2808442 -4.2884483 -4.2905827 -4.2894545 -4.293817 -4.3041096][-4.3256664 -4.3279147 -4.324739 -4.320365 -4.3160439 -4.3146706 -4.3163643 -4.3176403 -4.3204613 -4.3243718 -4.3271985 -4.3236675 -4.3161516 -4.3128643 -4.31786][-4.3174362 -4.3208814 -4.3204389 -4.3204212 -4.3199492 -4.3222218 -4.3272061 -4.3314447 -4.3357978 -4.3402314 -4.3426585 -4.3387628 -4.3311729 -4.3277574 -4.3313484]]...]
INFO - root - 2017-12-06 06:23:21.343817: step 3110, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 60h:16m:10s remains)
INFO - root - 2017-12-06 06:23:27.854979: step 3120, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 61h:50m:54s remains)
INFO - root - 2017-12-06 06:23:34.301002: step 3130, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:39m:53s remains)
INFO - root - 2017-12-06 06:23:40.509413: step 3140, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 57h:38m:05s remains)
INFO - root - 2017-12-06 06:23:46.982945: step 3150, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.661 sec/batch; 60h:30m:57s remains)
INFO - root - 2017-12-06 06:23:53.342962: step 3160, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 58h:30m:23s remains)
INFO - root - 2017-12-06 06:23:59.841130: step 3170, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 58h:33m:31s remains)
INFO - root - 2017-12-06 06:24:06.195388: step 3180, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 61h:03m:49s remains)
INFO - root - 2017-12-06 06:24:12.650542: step 3190, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-06 06:24:19.117788: step 3200, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:20m:40s remains)
2017-12-06 06:24:19.767808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227649 -4.3215785 -4.3115845 -4.2971697 -4.2880082 -4.2824817 -4.2774405 -4.2703919 -4.2642479 -4.2656884 -4.2739043 -4.2858648 -4.2979569 -4.3039532 -4.3025885][-4.3100305 -4.3056335 -4.2889681 -4.2688189 -4.2559905 -4.2487259 -4.2433367 -4.2370105 -4.2323108 -4.2376232 -4.251081 -4.2668767 -4.27776 -4.2780652 -4.26553][-4.2921977 -4.283287 -4.2621284 -4.236824 -4.218636 -4.2068405 -4.1981163 -4.1961813 -4.1960626 -4.204443 -4.2197003 -4.2338495 -4.2431183 -4.2386026 -4.2172289][-4.2702136 -4.2529526 -4.2209144 -4.1865611 -4.1611705 -4.1437492 -4.1389985 -4.1466188 -4.1523752 -4.1595607 -4.173347 -4.1896272 -4.2044077 -4.2023444 -4.1766968][-4.249814 -4.2224684 -4.1818914 -4.1362724 -4.0944734 -4.06276 -4.0588584 -4.0776825 -4.0904722 -4.0956688 -4.1105318 -4.1371708 -4.1699409 -4.18618 -4.1700597][-4.2390389 -4.2048311 -4.1583095 -4.0963392 -4.0257659 -3.9649262 -3.9530878 -3.9834394 -4.0083652 -4.0251946 -4.0548711 -4.1019611 -4.1538167 -4.1905255 -4.1902528][-4.2349873 -4.19566 -4.1447668 -4.0701094 -3.9715261 -3.8776107 -3.8526623 -3.8920248 -3.9348824 -3.9760885 -4.0281672 -4.0940323 -4.1575561 -4.2011046 -4.2128415][-4.2305493 -4.1899586 -4.1460562 -4.0796595 -3.9723213 -3.8628817 -3.8392091 -3.891875 -3.9478183 -3.9958367 -4.0473838 -4.10624 -4.1602888 -4.1925564 -4.2053819][-4.2229695 -4.1846981 -4.1515946 -4.0984716 -4.0010443 -3.9042377 -3.892591 -3.956378 -4.0197482 -4.0567017 -4.0894346 -4.1269064 -4.1580167 -4.173265 -4.1833014][-4.2235184 -4.1892748 -4.1663189 -4.1276169 -4.0527663 -3.9770246 -3.9660828 -4.0213752 -4.0720768 -4.0919394 -4.1106167 -4.1359015 -4.1543865 -4.1568375 -4.1604819][-4.22646 -4.20327 -4.187314 -4.1619778 -4.1081476 -4.0459743 -4.027441 -4.067802 -4.1047 -4.1111155 -4.1204557 -4.1391921 -4.1481943 -4.141068 -4.1390681][-4.2263522 -4.2140422 -4.2062149 -4.1897659 -4.1491137 -4.09848 -4.0748382 -4.1020236 -4.1271014 -4.1248946 -4.1254191 -4.1374054 -4.1405859 -4.1310706 -4.1273007][-4.2280383 -4.223022 -4.2225924 -4.2146354 -4.1864872 -4.1514421 -4.1309185 -4.1455722 -4.1598835 -4.1518135 -4.1445775 -4.1491718 -4.1475782 -4.1386557 -4.1386929][-4.242734 -4.2387838 -4.24232 -4.2412586 -4.2274909 -4.20831 -4.1931796 -4.1983051 -4.2040453 -4.1955829 -4.1884527 -4.189065 -4.1840529 -4.1767797 -4.1820865][-4.2661357 -4.2629061 -4.2696972 -4.2734132 -4.2669697 -4.2546535 -4.2413807 -4.2406273 -4.2417088 -4.2356648 -4.2344804 -4.2393856 -4.2403522 -4.2387323 -4.2482347]]...]
INFO - root - 2017-12-06 06:24:26.005133: step 3210, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 56h:50m:13s remains)
INFO - root - 2017-12-06 06:24:32.405263: step 3220, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:53m:52s remains)
INFO - root - 2017-12-06 06:24:38.758932: step 3230, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 58h:13m:39s remains)
INFO - root - 2017-12-06 06:24:45.097406: step 3240, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:07m:42s remains)
INFO - root - 2017-12-06 06:24:51.663407: step 3250, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 57h:47m:06s remains)
INFO - root - 2017-12-06 06:24:58.067006: step 3260, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 56h:31m:10s remains)
INFO - root - 2017-12-06 06:25:04.454810: step 3270, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 61h:44m:19s remains)
INFO - root - 2017-12-06 06:25:10.812863: step 3280, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 57h:15m:41s remains)
INFO - root - 2017-12-06 06:25:17.192048: step 3290, loss = 2.04, batch loss = 1.99 (13.1 examples/sec; 0.612 sec/batch; 55h:57m:00s remains)
INFO - root - 2017-12-06 06:25:23.585594: step 3300, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:33m:01s remains)
2017-12-06 06:25:24.227138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.177434 -4.1916966 -4.2330894 -4.2711759 -4.2997031 -4.3213925 -4.329145 -4.3265281 -4.3133373 -4.2945108 -4.278255 -4.2516279 -4.2255745 -4.228106 -4.242557][-4.1676021 -4.1941853 -4.2378478 -4.2755671 -4.3010588 -4.3211012 -4.3340869 -4.3389988 -4.3374653 -4.3315458 -4.325954 -4.310225 -4.2904634 -4.2788124 -4.2662582][-4.1555109 -4.1920867 -4.2351723 -4.2683239 -4.2867632 -4.2998362 -4.309361 -4.3131356 -4.3163519 -4.3216109 -4.32894 -4.32956 -4.3233995 -4.3083463 -4.280385][-4.1744046 -4.205883 -4.2362442 -4.2546158 -4.258852 -4.2603064 -4.2614183 -4.2605529 -4.265327 -4.2799926 -4.302011 -4.3212976 -4.33109 -4.3252149 -4.2982974][-4.209733 -4.2239952 -4.235332 -4.2342777 -4.2189484 -4.2013764 -4.1856318 -4.1718349 -4.1743646 -4.198462 -4.2361059 -4.2768364 -4.3100519 -4.3254752 -4.3133969][-4.2357016 -4.2352133 -4.2292819 -4.2103486 -4.1766911 -4.1378322 -4.1002502 -4.0676723 -4.0674462 -4.1050496 -4.1589527 -4.2175789 -4.2712617 -4.3045516 -4.3051066][-4.2498965 -4.2457676 -4.2280579 -4.1922412 -4.13932 -4.0769887 -4.0129552 -3.956697 -3.9572051 -4.0156536 -4.0886755 -4.1627903 -4.2305655 -4.2724934 -4.2774677][-4.267262 -4.2656465 -4.2420869 -4.1926107 -4.1252708 -4.0431042 -3.9498863 -3.8658493 -3.8662972 -3.9442728 -4.0339918 -4.1218433 -4.1977 -4.2418628 -4.2502785][-4.2922993 -4.2939038 -4.2693343 -4.2153835 -4.1437564 -4.0560117 -3.9545991 -3.8642442 -3.8676491 -3.9488361 -4.0390635 -4.1264353 -4.1992593 -4.2413135 -4.253912][-4.3189697 -4.3208957 -4.2957368 -4.2440796 -4.1805735 -4.1080852 -4.0318208 -3.9700823 -3.9801602 -4.0406036 -4.1043024 -4.1693707 -4.2270875 -4.2656889 -4.2826557][-4.3300309 -4.3281155 -4.3013554 -4.2557416 -4.2083516 -4.1619964 -4.1218042 -4.0965848 -4.1112113 -4.1450319 -4.1756043 -4.2122245 -4.2526 -4.2880898 -4.3118391][-4.3178806 -4.3088355 -4.2810245 -4.2469416 -4.2224126 -4.204731 -4.1964393 -4.1973934 -4.2099156 -4.2183251 -4.2181187 -4.2268906 -4.248301 -4.2780046 -4.3090682][-4.2973156 -4.2804284 -4.25251 -4.2313895 -4.227726 -4.2321482 -4.2443123 -4.2606945 -4.2698445 -4.25987 -4.2364497 -4.220716 -4.222795 -4.2445745 -4.2811046][-4.279922 -4.257103 -4.2255 -4.2105579 -4.2200251 -4.2384043 -4.2616973 -4.286098 -4.295033 -4.2772632 -4.240262 -4.2073417 -4.1919889 -4.2051158 -4.2447128][-4.2651076 -4.244267 -4.2155395 -4.2064042 -4.2226315 -4.2454934 -4.2696934 -4.2936654 -4.3029585 -4.2874255 -4.2503643 -4.210907 -4.1835203 -4.1870375 -4.2229276]]...]
INFO - root - 2017-12-06 06:25:30.467789: step 3310, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 57h:24m:56s remains)
INFO - root - 2017-12-06 06:25:37.043524: step 3320, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 58h:54m:50s remains)
INFO - root - 2017-12-06 06:25:43.417708: step 3330, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:48m:16s remains)
INFO - root - 2017-12-06 06:25:49.664401: step 3340, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:57m:48s remains)
INFO - root - 2017-12-06 06:25:56.082509: step 3350, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 59h:24m:29s remains)
INFO - root - 2017-12-06 06:26:02.461490: step 3360, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:27m:39s remains)
INFO - root - 2017-12-06 06:26:08.872977: step 3370, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 57h:34m:12s remains)
INFO - root - 2017-12-06 06:26:15.235740: step 3380, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 56h:57m:40s remains)
INFO - root - 2017-12-06 06:26:21.595482: step 3390, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 57h:05m:30s remains)
INFO - root - 2017-12-06 06:26:28.032227: step 3400, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 58h:49m:28s remains)
2017-12-06 06:26:28.737730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3407512 -4.341063 -4.3436761 -4.3473291 -4.3513646 -4.3512564 -4.3480682 -4.3431149 -4.3372512 -4.3323069 -4.3292623 -4.3264914 -4.3265095 -4.3290253 -4.3334403][-4.340724 -4.341814 -4.344872 -4.3486233 -4.352334 -4.3496356 -4.3416572 -4.3305793 -4.3191485 -4.3133135 -4.3136992 -4.3160529 -4.3205805 -4.3255196 -4.3316479][-4.3433208 -4.3447919 -4.3467894 -4.3476105 -4.3470492 -4.3376188 -4.32151 -4.3010035 -4.2819524 -4.27486 -4.2826195 -4.2966371 -4.3119454 -4.3216009 -4.3296227][-4.3471775 -4.3490067 -4.3494372 -4.3433142 -4.331584 -4.3109431 -4.28483 -4.2520304 -4.2234182 -4.2180924 -4.23775 -4.2675743 -4.2972078 -4.3146682 -4.326273][-4.3479123 -4.3491697 -4.3470359 -4.3325562 -4.3088727 -4.2746067 -4.2313547 -4.1800141 -4.1396842 -4.1392112 -4.1744757 -4.2221203 -4.2673659 -4.2974839 -4.3174853][-4.3458953 -4.3452616 -4.3383784 -4.3170981 -4.2844372 -4.2373681 -4.1755424 -4.1057291 -4.0532155 -4.0603857 -4.1127868 -4.1753736 -4.2325435 -4.2746243 -4.3055844][-4.3402677 -4.3360429 -4.3239107 -4.29439 -4.2510672 -4.189959 -4.1096263 -4.0212774 -3.9563801 -3.9753129 -4.0506482 -4.129704 -4.197557 -4.25074 -4.2924528][-4.335279 -4.3265195 -4.3080521 -4.2713242 -4.222753 -4.1573377 -4.0705123 -3.9670851 -3.8940573 -3.9237309 -4.014441 -4.1040654 -4.1804996 -4.2396088 -4.2837644][-4.3379946 -4.3265066 -4.3047113 -4.2669034 -4.2221012 -4.1658797 -4.0937748 -4.00125 -3.9406972 -3.9685647 -4.0473938 -4.1261387 -4.1971507 -4.2517729 -4.2889428][-4.343698 -4.3321853 -4.3118191 -4.281364 -4.24585 -4.2044311 -4.1509142 -4.0837331 -4.0501842 -4.0749569 -4.1277332 -4.1822019 -4.2355275 -4.2779741 -4.3036022][-4.3431382 -4.3321123 -4.315125 -4.2938623 -4.2678661 -4.2363739 -4.1960049 -4.1531763 -4.142303 -4.1661377 -4.2034822 -4.2406521 -4.2771935 -4.3060427 -4.319756][-4.3377271 -4.3262825 -4.3127551 -4.2944026 -4.269361 -4.2401166 -4.2088251 -4.1873865 -4.1933961 -4.2212505 -4.2543807 -4.2836561 -4.3095279 -4.3282022 -4.3347731][-4.3299007 -4.3186135 -4.3068652 -4.2849483 -4.2565479 -4.22361 -4.199141 -4.1979938 -4.2247295 -4.2606683 -4.2926974 -4.3159847 -4.3333182 -4.3429966 -4.3432784][-4.3219056 -4.3109722 -4.2959094 -4.2650938 -4.2304392 -4.194407 -4.1793232 -4.1982188 -4.2404156 -4.2825742 -4.312017 -4.3300824 -4.3406239 -4.3438759 -4.3412161][-4.3120947 -4.2974763 -4.2744966 -4.2354717 -4.1942496 -4.16099 -4.16216 -4.1992521 -4.249486 -4.2916741 -4.31778 -4.3319082 -4.3369427 -4.33678 -4.3346305]]...]
INFO - root - 2017-12-06 06:26:35.128148: step 3410, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 57h:22m:13s remains)
INFO - root - 2017-12-06 06:26:41.395842: step 3420, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 57h:11m:52s remains)
INFO - root - 2017-12-06 06:26:47.759780: step 3430, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:57m:49s remains)
INFO - root - 2017-12-06 06:26:54.080319: step 3440, loss = 2.04, batch loss = 1.98 (14.0 examples/sec; 0.572 sec/batch; 52h:19m:16s remains)
INFO - root - 2017-12-06 06:27:00.591900: step 3450, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 58h:40m:31s remains)
INFO - root - 2017-12-06 06:27:07.090719: step 3460, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:51m:40s remains)
INFO - root - 2017-12-06 06:27:13.519978: step 3470, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:28m:09s remains)
INFO - root - 2017-12-06 06:27:19.925773: step 3480, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 58h:05m:22s remains)
INFO - root - 2017-12-06 06:27:26.370879: step 3490, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.605 sec/batch; 55h:19m:07s remains)
INFO - root - 2017-12-06 06:27:32.784448: step 3500, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 57h:16m:04s remains)
2017-12-06 06:27:33.430983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2629108 -4.2648296 -4.2704992 -4.2693281 -4.2613673 -4.2542357 -4.2489519 -4.2435994 -4.2478046 -4.2617722 -4.2787 -4.2956309 -4.3083363 -4.318934 -4.325644][-4.2264404 -4.2276745 -4.2314587 -4.2248964 -4.2113767 -4.2026978 -4.2001615 -4.1988497 -4.2055221 -4.2228861 -4.2475204 -4.2728391 -4.2889838 -4.3008533 -4.3073616][-4.184145 -4.1842136 -4.1863155 -4.175139 -4.1572971 -4.146153 -4.1427445 -4.1448026 -4.1539679 -4.1753836 -4.20858 -4.2446928 -4.2653313 -4.2783036 -4.2860212][-4.14779 -4.1457214 -4.1477251 -4.1349545 -4.113585 -4.0979333 -4.091033 -4.0967417 -4.1108813 -4.1395555 -4.1810875 -4.2248693 -4.2465634 -4.2565527 -4.2627172][-4.1173439 -4.111299 -4.1152043 -4.109858 -4.0864954 -4.0561323 -4.034555 -4.0404706 -4.0686936 -4.114563 -4.1655178 -4.2147217 -4.239511 -4.247149 -4.2494154][-4.0587959 -4.0527148 -4.0622096 -4.068459 -4.0455303 -3.9921844 -3.9387014 -3.9345315 -3.9911451 -4.0736532 -4.140204 -4.1946907 -4.2254763 -4.2346239 -4.2365313][-3.9689522 -3.9683275 -3.9850152 -4.0021071 -3.9816971 -3.90739 -3.8070486 -3.7684669 -3.8568795 -3.9948468 -4.0939431 -4.1596475 -4.1992116 -4.2141223 -4.2192364][-3.9240282 -3.9290802 -3.9473367 -3.964462 -3.9462717 -3.8675451 -3.7413101 -3.6622708 -3.7563124 -3.9273965 -4.05372 -4.1292467 -4.1735711 -4.1921034 -4.201231][-3.9807692 -3.9855633 -3.9985874 -4.0104623 -3.9984624 -3.9413984 -3.8447165 -3.7671237 -3.8124321 -3.9446342 -4.0574303 -4.1286273 -4.1675982 -4.1820183 -4.1924572][-4.0867252 -4.0849128 -4.0907135 -4.0974507 -4.090292 -4.0573826 -4.0031171 -3.9470215 -3.9495721 -4.0222654 -4.1014791 -4.158114 -4.1870527 -4.1954913 -4.2029738][-4.1884842 -4.1813955 -4.1801119 -4.1815991 -4.1761885 -4.1559196 -4.124907 -4.0887542 -4.0758834 -4.1086903 -4.1594305 -4.2005572 -4.2214146 -4.2255945 -4.2297635][-4.2619052 -4.2511787 -4.2451792 -4.24513 -4.2426786 -4.2320337 -4.2141213 -4.1929827 -4.1818748 -4.1973181 -4.228478 -4.256269 -4.2678566 -4.2666268 -4.265491][-4.2961688 -4.2876072 -4.2809229 -4.28129 -4.281229 -4.2748632 -4.2636452 -4.2518926 -4.2471614 -4.2576747 -4.2800207 -4.2994142 -4.3056893 -4.3013072 -4.2959723][-4.3035741 -4.298749 -4.2930541 -4.2915196 -4.29005 -4.2845697 -4.2768488 -4.2698121 -4.2691045 -4.2783093 -4.2951064 -4.3104467 -4.3171587 -4.3161325 -4.313201][-4.3063536 -4.304801 -4.3010039 -4.2993517 -4.297225 -4.2926054 -4.2865653 -4.281642 -4.2810616 -4.2865863 -4.2969041 -4.3087645 -4.3173709 -4.3212881 -4.3231606]]...]
INFO - root - 2017-12-06 06:27:39.835724: step 3510, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:10m:10s remains)
INFO - root - 2017-12-06 06:27:46.100002: step 3520, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 0.515 sec/batch; 47h:04m:17s remains)
INFO - root - 2017-12-06 06:27:52.547866: step 3530, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 57h:50m:24s remains)
INFO - root - 2017-12-06 06:27:58.974913: step 3540, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:26m:28s remains)
INFO - root - 2017-12-06 06:28:05.185368: step 3550, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 57h:25m:01s remains)
INFO - root - 2017-12-06 06:28:11.617322: step 3560, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 59h:03m:21s remains)
INFO - root - 2017-12-06 06:28:17.933703: step 3570, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:07m:16s remains)
INFO - root - 2017-12-06 06:28:24.343598: step 3580, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 58h:46m:03s remains)
INFO - root - 2017-12-06 06:28:30.721007: step 3590, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:21m:35s remains)
INFO - root - 2017-12-06 06:28:37.138881: step 3600, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:08m:13s remains)
2017-12-06 06:28:37.811803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2740359 -4.257431 -4.2350149 -4.2043986 -4.1804471 -4.1741691 -4.1804438 -4.186223 -4.192028 -4.2066889 -4.22125 -4.2228594 -4.20455 -4.1756 -4.1535358][-4.2595687 -4.2438593 -4.2302289 -4.2033334 -4.16955 -4.1491027 -4.1537733 -4.1698213 -4.1862717 -4.203558 -4.2215638 -4.2225032 -4.1949997 -4.1526303 -4.1215167][-4.2420478 -4.2336836 -4.2323451 -4.2159886 -4.1776505 -4.1413503 -4.1376128 -4.1610065 -4.1897893 -4.2115517 -4.2295027 -4.230907 -4.2041283 -4.1615653 -4.1284385][-4.2291665 -4.2312355 -4.2409282 -4.2390809 -4.2055745 -4.1595073 -4.1455812 -4.1718512 -4.2105618 -4.2364645 -4.2542863 -4.2573752 -4.236485 -4.2009916 -4.1698904][-4.2168026 -4.2267833 -4.2412744 -4.2486987 -4.2231183 -4.174027 -4.1537757 -4.1794505 -4.2223344 -4.2530832 -4.2734838 -4.2806487 -4.2673979 -4.2403169 -4.2124457][-4.1972389 -4.2123213 -4.2264051 -4.2350159 -4.2157993 -4.1700253 -4.1479969 -4.1728377 -4.2174935 -4.2521772 -4.276628 -4.2897429 -4.2826147 -4.2610264 -4.234251][-4.1720176 -4.186296 -4.1971459 -4.2040858 -4.1906548 -4.1536069 -4.1368332 -4.165175 -4.2101359 -4.242444 -4.2644019 -4.2789545 -4.2771444 -4.2591615 -4.2338305][-4.1478491 -4.1533175 -4.1635442 -4.1720457 -4.1647353 -4.1377134 -4.1287937 -4.158216 -4.2003236 -4.2272329 -4.2394772 -4.249578 -4.2507315 -4.2377992 -4.2185869][-4.1379275 -4.1300225 -4.1423588 -4.1569023 -4.1569824 -4.13859 -4.1361551 -4.1628141 -4.1953092 -4.2107363 -4.2098603 -4.2125955 -4.2150869 -4.2098565 -4.2033005][-4.1572156 -4.1395354 -4.1482897 -4.166532 -4.1745286 -4.1662493 -4.1674876 -4.1885104 -4.2079616 -4.2090425 -4.1956162 -4.1902385 -4.1916575 -4.1929984 -4.2002244][-4.1888967 -4.168736 -4.1682491 -4.1826587 -4.1940246 -4.19273 -4.1950135 -4.2104011 -4.2188735 -4.2117457 -4.1943936 -4.18639 -4.1854057 -4.1900349 -4.2041407][-4.2125773 -4.1952996 -4.1885138 -4.1948638 -4.204061 -4.2057753 -4.2098327 -4.2194238 -4.2193594 -4.2090383 -4.1959038 -4.1924052 -4.1918817 -4.1956024 -4.2085805][-4.2269306 -4.2128096 -4.2032433 -4.204226 -4.2105293 -4.2115536 -4.2132478 -4.2163377 -4.2108283 -4.2003937 -4.1935081 -4.1953392 -4.197866 -4.2014475 -4.2123346][-4.234695 -4.2239666 -4.2146931 -4.2138786 -4.2191963 -4.2202964 -4.2192283 -4.2164593 -4.208322 -4.1985278 -4.1948891 -4.1993709 -4.202992 -4.2068429 -4.216157][-4.2463932 -4.2405882 -4.23457 -4.2349448 -4.239574 -4.2401891 -4.2375488 -4.2310796 -4.2203431 -4.2103639 -4.2070937 -4.2124653 -4.2175641 -4.2214141 -4.2280459]]...]
INFO - root - 2017-12-06 06:28:44.289433: step 3610, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:36m:07s remains)
INFO - root - 2017-12-06 06:28:50.624784: step 3620, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 57h:05m:23s remains)
INFO - root - 2017-12-06 06:28:56.872424: step 3630, loss = 2.04, batch loss = 1.99 (12.9 examples/sec; 0.621 sec/batch; 56h:46m:05s remains)
INFO - root - 2017-12-06 06:29:03.257881: step 3640, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:57m:01s remains)
INFO - root - 2017-12-06 06:29:09.529004: step 3650, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 58h:44m:29s remains)
INFO - root - 2017-12-06 06:29:15.984420: step 3660, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 59h:00m:55s remains)
INFO - root - 2017-12-06 06:29:22.518190: step 3670, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:46m:25s remains)
INFO - root - 2017-12-06 06:29:28.873413: step 3680, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 58h:07m:33s remains)
INFO - root - 2017-12-06 06:29:35.316928: step 3690, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:14m:17s remains)
INFO - root - 2017-12-06 06:29:41.805694: step 3700, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:35m:43s remains)
2017-12-06 06:29:42.506388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.260283 -4.2541528 -4.23111 -4.2063446 -4.1888385 -4.1838694 -4.184165 -4.1783028 -4.1638427 -4.1595473 -4.1705489 -4.1827459 -4.186523 -4.1891322 -4.1978974][-4.2366381 -4.2309957 -4.2118568 -4.19089 -4.1763258 -4.1769404 -4.1766033 -4.1602855 -4.1348367 -4.1258092 -4.1389136 -4.1614914 -4.1766934 -4.1805515 -4.1850357][-4.1987615 -4.1950393 -4.1780472 -4.163094 -4.1569118 -4.1651535 -4.164988 -4.1465755 -4.1199903 -4.1110716 -4.1306486 -4.1622629 -4.1828632 -4.1838269 -4.17911][-4.1474161 -4.1437211 -4.1297956 -4.1280675 -4.1359677 -4.1542196 -4.1589417 -4.1456208 -4.1302824 -4.1344104 -4.1599369 -4.1897879 -4.2016168 -4.1895566 -4.1702266][-4.1018295 -4.0933471 -4.0856042 -4.0923581 -4.1101117 -4.1293826 -4.1362128 -4.1325293 -4.135694 -4.1551275 -4.1816578 -4.204752 -4.2048349 -4.1768832 -4.1452742][-4.0747638 -4.0602808 -4.0478077 -4.047739 -4.0581484 -4.0594711 -4.0592446 -4.0682111 -4.0893021 -4.1218538 -4.1520681 -4.1722908 -4.170866 -4.1434851 -4.1153212][-4.01952 -3.9921665 -3.9659786 -3.9527416 -3.9487529 -3.932189 -3.9270718 -3.9486871 -3.9896641 -4.0400348 -4.0784779 -4.0985827 -4.1050878 -4.0955052 -4.0867071][-3.9422626 -3.8859572 -3.829776 -3.8010836 -3.7871614 -3.7606721 -3.7604206 -3.8048091 -3.8702505 -3.9402432 -3.9895983 -4.0167913 -4.0359645 -4.0523643 -4.0673184][-3.9553332 -3.8906665 -3.8296685 -3.8001893 -3.7914803 -3.7787037 -3.7912827 -3.8377142 -3.8981907 -3.9562159 -3.9948013 -4.0125332 -4.0249796 -4.04651 -4.0678325][-4.0704885 -4.02679 -3.989399 -3.975183 -3.9753618 -3.9793923 -3.9944527 -4.0187812 -4.0482779 -4.0727878 -4.0850334 -4.0846529 -4.0836296 -4.0931735 -4.1066313][-4.181891 -4.1588879 -4.1413856 -4.1368351 -4.1405625 -4.1475472 -4.1559567 -4.1629629 -4.1718192 -4.1772642 -4.1762695 -4.169107 -4.1626959 -4.1628933 -4.1678305][-4.2553134 -4.2409358 -4.2278953 -4.2213159 -4.2211885 -4.2235651 -4.2255607 -4.2264934 -4.2293911 -4.2316432 -4.2295694 -4.2252479 -4.2235651 -4.2265878 -4.2313685][-4.2958446 -4.2837629 -4.2710137 -4.2616363 -4.2578163 -4.2569757 -4.2579846 -4.25986 -4.2628541 -4.2650738 -4.2649455 -4.2656093 -4.2694955 -4.2760739 -4.2802458][-4.31196 -4.3035212 -4.2941475 -4.2867479 -4.2835884 -4.2834387 -4.2842484 -4.285305 -4.2864294 -4.287539 -4.2888179 -4.2915754 -4.2957821 -4.301291 -4.3031774][-4.3176308 -4.3107934 -4.304131 -4.2988615 -4.2970448 -4.2981558 -4.3006487 -4.3014493 -4.3000994 -4.2982187 -4.2979937 -4.2996807 -4.301477 -4.3029733 -4.3015976]]...]
INFO - root - 2017-12-06 06:29:49.003174: step 3710, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 59h:06m:40s remains)
INFO - root - 2017-12-06 06:29:55.386678: step 3720, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 62h:20m:53s remains)
INFO - root - 2017-12-06 06:30:01.780678: step 3730, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.618 sec/batch; 56h:25m:42s remains)
INFO - root - 2017-12-06 06:30:08.032191: step 3740, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 60h:38m:25s remains)
INFO - root - 2017-12-06 06:30:14.361004: step 3750, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 60h:56m:48s remains)
INFO - root - 2017-12-06 06:30:20.869031: step 3760, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 58h:55m:11s remains)
INFO - root - 2017-12-06 06:30:27.308547: step 3770, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.607 sec/batch; 55h:24m:13s remains)
INFO - root - 2017-12-06 06:30:33.645781: step 3780, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 58h:23m:02s remains)
INFO - root - 2017-12-06 06:30:40.000094: step 3790, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:24m:16s remains)
INFO - root - 2017-12-06 06:30:46.402956: step 3800, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 57h:26m:15s remains)
2017-12-06 06:30:47.018198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2454515 -4.237205 -4.2199388 -4.2059116 -4.2128491 -4.2392774 -4.2629352 -4.2659764 -4.2459497 -4.21424 -4.1893835 -4.19038 -4.2061858 -4.2190166 -4.2223058][-4.2267904 -4.2240038 -4.20718 -4.1904683 -4.194181 -4.2176194 -4.2413354 -4.2452807 -4.2269974 -4.1982636 -4.1750069 -4.1746268 -4.1873136 -4.2006288 -4.2050748][-4.20949 -4.2176151 -4.2042246 -4.1835475 -4.1794915 -4.1962767 -4.2168322 -4.2174945 -4.200964 -4.1778965 -4.1581717 -4.1541581 -4.1622696 -4.1791511 -4.1916537][-4.2092128 -4.22528 -4.2131596 -4.182888 -4.1662345 -4.1743183 -4.1890554 -4.1867437 -4.1750736 -4.1648817 -4.1539683 -4.1457129 -4.1439071 -4.1599083 -4.1803432][-4.2176676 -4.230515 -4.2104816 -4.1671386 -4.1406116 -4.1468649 -4.1646485 -4.1649542 -4.1592226 -4.1626348 -4.1657939 -4.16028 -4.1514816 -4.1618619 -4.1813607][-4.2302237 -4.2323384 -4.1974063 -4.1401391 -4.109519 -4.1213531 -4.1515574 -4.1599417 -4.160851 -4.1732726 -4.1931744 -4.2029319 -4.1981192 -4.19967 -4.2068615][-4.2436843 -4.2348833 -4.187181 -4.1204042 -4.0869613 -4.101862 -4.1405683 -4.1589708 -4.1674414 -4.1908035 -4.2279029 -4.2573709 -4.264143 -4.25917 -4.2494922][-4.241796 -4.229835 -4.1842203 -4.1218944 -4.08642 -4.0961466 -4.1294751 -4.1477423 -4.16269 -4.1971602 -4.2487931 -4.2913804 -4.3087635 -4.3050356 -4.2879567][-4.2344193 -4.2261672 -4.1938133 -4.1474781 -4.1145682 -4.1099973 -4.1235533 -4.1274252 -4.1401358 -4.1828527 -4.2471042 -4.298377 -4.3255811 -4.3286548 -4.3148518][-4.238193 -4.2375402 -4.219048 -4.1866031 -4.1583581 -4.1407361 -4.1277866 -4.1103153 -4.1110058 -4.1514554 -4.2225494 -4.2812605 -4.3179946 -4.3327131 -4.3291645][-4.2436957 -4.2518196 -4.2458081 -4.2246923 -4.2005067 -4.175786 -4.1425061 -4.1081076 -4.0921669 -4.1183839 -4.1839156 -4.246079 -4.29207 -4.3179817 -4.3262076][-4.2440948 -4.2589393 -4.2643323 -4.2557459 -4.2395 -4.2170029 -4.1785369 -4.1377411 -4.10894 -4.1149077 -4.1629252 -4.2184086 -4.2663312 -4.2935643 -4.3040996][-4.2317014 -4.2516456 -4.2672162 -4.2695613 -4.2641106 -4.2521572 -4.2249618 -4.1927032 -4.1643248 -4.1546011 -4.1791058 -4.2178788 -4.253468 -4.2690525 -4.2709694][-4.1919127 -4.220602 -4.2474055 -4.2617397 -4.2674136 -4.2670255 -4.2562022 -4.2407007 -4.2244349 -4.2139859 -4.2197437 -4.2365913 -4.2537093 -4.2535028 -4.2430158][-4.1445932 -4.1837764 -4.2203746 -4.2435851 -4.2566361 -4.2620792 -4.2609525 -4.2600965 -4.2583995 -4.2565141 -4.2568536 -4.2592969 -4.2606678 -4.2473164 -4.228559]]...]
INFO - root - 2017-12-06 06:30:53.335302: step 3810, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 57h:38m:51s remains)
INFO - root - 2017-12-06 06:30:59.760552: step 3820, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:26m:53s remains)
INFO - root - 2017-12-06 06:31:06.167399: step 3830, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:38m:59s remains)
INFO - root - 2017-12-06 06:31:12.424105: step 3840, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.613 sec/batch; 55h:59m:47s remains)
INFO - root - 2017-12-06 06:31:18.592924: step 3850, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 0.531 sec/batch; 48h:29m:00s remains)
INFO - root - 2017-12-06 06:31:24.986934: step 3860, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 58h:25m:56s remains)
INFO - root - 2017-12-06 06:31:31.498569: step 3870, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 60h:20m:42s remains)
INFO - root - 2017-12-06 06:31:37.793112: step 3880, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 57h:32m:39s remains)
INFO - root - 2017-12-06 06:31:44.216951: step 3890, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 55h:43m:17s remains)
INFO - root - 2017-12-06 06:31:50.561956: step 3900, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 59h:48m:01s remains)
2017-12-06 06:31:51.221073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2419114 -4.177166 -4.1178303 -4.0807195 -4.0874944 -4.1287689 -4.1681356 -4.1796246 -4.1835337 -4.1801715 -4.16999 -4.1716871 -4.1764512 -4.1737523 -4.1641645][-4.2435937 -4.17195 -4.0970497 -4.0512638 -4.0682292 -4.1258411 -4.17063 -4.1780467 -4.182333 -4.1750436 -4.1635284 -4.1655331 -4.173243 -4.1697989 -4.1569033][-4.2470584 -4.1699815 -4.0822864 -4.0315514 -4.062685 -4.1348009 -4.1815438 -4.1847572 -4.184217 -4.1732941 -4.1601486 -4.1613903 -4.1669469 -4.1626573 -4.15312][-4.2517567 -4.1704555 -4.0740314 -4.0165496 -4.0566554 -4.13997 -4.1909256 -4.1940117 -4.1891403 -4.1742773 -4.1587715 -4.1572371 -4.1602187 -4.1640697 -4.165246][-4.2663269 -4.1869121 -4.0874796 -4.0225616 -4.0585332 -4.1420045 -4.192544 -4.1918097 -4.1848321 -4.1720033 -4.1558685 -4.1535378 -4.1579895 -4.1684351 -4.1790452][-4.2795792 -4.2092657 -4.1155577 -4.0479794 -4.0714436 -4.1435246 -4.1860971 -4.1810861 -4.1738935 -4.1681747 -4.1520147 -4.14832 -4.1510558 -4.1625805 -4.1787167][-4.2837949 -4.2219467 -4.1384907 -4.0773559 -4.0967674 -4.15816 -4.196362 -4.1923485 -4.1831412 -4.1744318 -4.1505809 -4.1362243 -4.1337891 -4.1477265 -4.1718926][-4.2891083 -4.234283 -4.1635518 -4.1149044 -4.1311965 -4.1815243 -4.2209506 -4.2237611 -4.2113519 -4.1960578 -4.1696362 -4.1480112 -4.1330881 -4.1411009 -4.1636028][-4.293539 -4.2433004 -4.1829858 -4.1457195 -4.1600337 -4.202836 -4.243947 -4.2503395 -4.23599 -4.2203312 -4.199512 -4.1788983 -4.1578608 -4.15139 -4.1615334][-4.2874165 -4.2389488 -4.1870947 -4.1599579 -4.1734614 -4.2110515 -4.2555609 -4.2675624 -4.2535219 -4.2402906 -4.2249279 -4.2058716 -4.1843233 -4.1715732 -4.1737509][-4.2799187 -4.2335181 -4.1906013 -4.1742115 -4.188076 -4.2209449 -4.265512 -4.2797337 -4.267118 -4.2555265 -4.2457976 -4.2307067 -4.2091622 -4.194344 -4.193594][-4.2763653 -4.2350059 -4.2008476 -4.1917586 -4.2044306 -4.2287903 -4.2672939 -4.2813449 -4.2733788 -4.2671895 -4.2629719 -4.2492409 -4.2265353 -4.2101631 -4.2099576][-4.2698 -4.2358365 -4.2110267 -4.207108 -4.2176394 -4.2346878 -4.2646379 -4.2761226 -4.2715554 -4.2691774 -4.2688003 -4.2582426 -4.2385397 -4.223578 -4.2250581][-4.2676849 -4.2431407 -4.226913 -4.2259917 -4.2346706 -4.2471361 -4.2683916 -4.2751055 -4.2696815 -4.2665358 -4.2668996 -4.2617397 -4.2507143 -4.2425141 -4.2464113][-4.2782397 -4.2609963 -4.2513952 -4.2517295 -4.2588291 -4.2675166 -4.2807183 -4.283042 -4.2767515 -4.2719622 -4.2684312 -4.2653704 -4.2615113 -4.2616057 -4.2685041]]...]
INFO - root - 2017-12-06 06:31:57.629401: step 3910, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 60h:28m:44s remains)
INFO - root - 2017-12-06 06:32:04.034698: step 3920, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 56h:30m:14s remains)
INFO - root - 2017-12-06 06:32:10.426672: step 3930, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.624 sec/batch; 56h:59m:33s remains)
INFO - root - 2017-12-06 06:32:16.774454: step 3940, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-06 06:32:23.092837: step 3950, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 58h:55m:09s remains)
INFO - root - 2017-12-06 06:32:29.380702: step 3960, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 59h:33m:58s remains)
INFO - root - 2017-12-06 06:32:35.815183: step 3970, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 58h:01m:30s remains)
INFO - root - 2017-12-06 06:32:42.236712: step 3980, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 58h:53m:51s remains)
INFO - root - 2017-12-06 06:32:48.734107: step 3990, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:44m:50s remains)
INFO - root - 2017-12-06 06:32:55.033812: step 4000, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 57h:31m:55s remains)
2017-12-06 06:32:55.635484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33445 -4.3347344 -4.3301163 -4.3243632 -4.3204713 -4.3159 -4.3073912 -4.2954674 -4.2924175 -4.3002586 -4.3080382 -4.3121133 -4.3192215 -4.326385 -4.3311167][-4.3130083 -4.3186164 -4.3170538 -4.3072824 -4.2924466 -4.2743106 -4.25028 -4.2283983 -4.2271919 -4.243072 -4.2595415 -4.2674928 -4.2789416 -4.2961736 -4.3116159][-4.287385 -4.2987452 -4.2977209 -4.2830939 -4.2524576 -4.2129335 -4.167491 -4.127809 -4.12658 -4.1533766 -4.1820889 -4.201777 -4.2221475 -4.25176 -4.2779021][-4.2601733 -4.2744045 -4.274992 -4.2522807 -4.2020555 -4.1305695 -4.0556536 -3.999059 -4.0090737 -4.0660348 -4.1164594 -4.1505227 -4.1818681 -4.219471 -4.2507782][-4.2250137 -4.2410736 -4.2473016 -4.2198758 -4.1462259 -4.032547 -3.9096656 -3.83756 -3.8846087 -3.9975266 -4.0872478 -4.1372986 -4.1781707 -4.2199674 -4.2473969][-4.1623268 -4.1797652 -4.1994767 -4.1791611 -4.088201 -3.9234591 -3.725132 -3.6291234 -3.7484951 -3.942389 -4.0799689 -4.1527538 -4.1994138 -4.2387919 -4.254303][-4.0884538 -4.1052432 -4.1396813 -4.1363392 -4.04983 -3.8647087 -3.60654 -3.4756083 -3.66469 -3.9241779 -4.0962758 -4.1876044 -4.2375031 -4.2658014 -4.2653904][-4.0358491 -4.0464969 -4.0896096 -4.1122751 -4.0549736 -3.9092922 -3.688426 -3.5653486 -3.7242754 -3.9593115 -4.1207638 -4.2094965 -4.2556386 -4.2699518 -4.2541795][-4.0256553 -4.0249348 -4.0688152 -4.1157775 -4.1005087 -4.020205 -3.894254 -3.8237429 -3.9009893 -4.0435467 -4.1532249 -4.21647 -4.244801 -4.2468882 -4.22303][-4.0502563 -4.0438042 -4.0922041 -4.1503172 -4.1659269 -4.1337843 -4.074326 -4.0390029 -4.0610256 -4.1222692 -4.178503 -4.2151713 -4.2234855 -4.2152143 -4.1921][-4.0835667 -4.0794296 -4.1286798 -4.184279 -4.2049556 -4.1893158 -4.1580458 -4.138763 -4.1316361 -4.141449 -4.1679029 -4.1959171 -4.2074466 -4.2064247 -4.1906886][-4.1059265 -4.1065326 -4.1515627 -4.1925993 -4.2061825 -4.1943493 -4.178329 -4.1615133 -4.1403127 -4.1254039 -4.140615 -4.1744719 -4.20466 -4.221261 -4.2145944][-4.1293564 -4.1341076 -4.1719894 -4.1997485 -4.2017717 -4.1915154 -4.1824412 -4.1661353 -4.144361 -4.1318526 -4.1473742 -4.1802311 -4.2201514 -4.2505951 -4.2528787][-4.1678166 -4.1744709 -4.2028885 -4.2165666 -4.2046127 -4.1899295 -4.1824818 -4.17377 -4.159935 -4.1602879 -4.1791272 -4.2071991 -4.2488227 -4.285708 -4.2897029][-4.2141671 -4.2208438 -4.2406993 -4.2418675 -4.2177119 -4.1949024 -4.19041 -4.1923809 -4.1853871 -4.1883912 -4.2076063 -4.2350044 -4.2750921 -4.3114243 -4.3134704]]...]
INFO - root - 2017-12-06 06:33:02.004806: step 4010, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 56h:45m:47s remains)
INFO - root - 2017-12-06 06:33:08.439793: step 4020, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 59h:46m:35s remains)
INFO - root - 2017-12-06 06:33:14.852930: step 4030, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 60h:22m:44s remains)
INFO - root - 2017-12-06 06:33:21.206116: step 4040, loss = 2.05, batch loss = 1.99 (13.4 examples/sec; 0.595 sec/batch; 54h:19m:51s remains)
INFO - root - 2017-12-06 06:33:27.587669: step 4050, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:27m:15s remains)
INFO - root - 2017-12-06 06:33:33.637459: step 4060, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 58h:19m:09s remains)
INFO - root - 2017-12-06 06:33:40.006515: step 4070, loss = 2.05, batch loss = 1.99 (13.4 examples/sec; 0.599 sec/batch; 54h:37m:17s remains)
INFO - root - 2017-12-06 06:33:46.407635: step 4080, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:59m:42s remains)
INFO - root - 2017-12-06 06:33:52.779616: step 4090, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 58h:15m:47s remains)
INFO - root - 2017-12-06 06:33:59.106951: step 4100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 61h:28m:18s remains)
2017-12-06 06:33:59.720720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3183236 -4.3196349 -4.3177581 -4.3163509 -4.3199167 -4.3228035 -4.323163 -4.3224716 -4.3275805 -4.3318968 -4.3266687 -4.3161173 -4.3071465 -4.3043928 -4.3051724][-4.2892323 -4.283546 -4.2779484 -4.2808347 -4.2926044 -4.2983327 -4.3017864 -4.3060942 -4.3150434 -4.3184719 -4.3068919 -4.2892079 -4.2753263 -4.2734971 -4.2775011][-4.2540379 -4.24611 -4.2415662 -4.2507319 -4.2680807 -4.2748389 -4.277082 -4.2831855 -4.2926097 -4.2928209 -4.2761397 -4.2569833 -4.2463646 -4.2496758 -4.2591047][-4.218585 -4.2137384 -4.2148323 -4.229342 -4.2481747 -4.2516966 -4.2453032 -4.2425485 -4.246366 -4.2448778 -4.2323704 -4.2242985 -4.2253036 -4.2348 -4.2439837][-4.2010784 -4.2018437 -4.2064838 -4.2190561 -4.2337694 -4.2255216 -4.197793 -4.173934 -4.173594 -4.1825404 -4.1885166 -4.2020617 -4.2180476 -4.2286777 -4.2298427][-4.2058873 -4.2045288 -4.2030654 -4.2061014 -4.2084885 -4.1772528 -4.1212835 -4.0733595 -4.0811315 -4.1210241 -4.1631942 -4.2007685 -4.2243085 -4.2281451 -4.2195849][-4.2155085 -4.2048068 -4.1894612 -4.1771908 -4.1613474 -4.1057191 -4.018117 -3.9474032 -3.9848228 -4.0767121 -4.1553411 -4.2023597 -4.2171574 -4.2099743 -4.191566][-4.2195377 -4.2014656 -4.1789851 -4.1628485 -4.1390648 -4.0748291 -3.9877291 -3.9302959 -3.9913538 -4.0995975 -4.178865 -4.2143817 -4.2128696 -4.1899 -4.163147][-4.2269707 -4.20985 -4.1911907 -4.1775012 -4.1564407 -4.1115742 -4.0664463 -4.0538425 -4.1081557 -4.1792879 -4.221149 -4.2280083 -4.2041063 -4.1678119 -4.1456103][-4.2202277 -4.211709 -4.1982403 -4.18727 -4.1735282 -4.1516328 -4.1402826 -4.1515927 -4.1904964 -4.2252955 -4.2320232 -4.2122993 -4.1699109 -4.1342087 -4.1309056][-4.2091484 -4.2130957 -4.2073679 -4.2007294 -4.191009 -4.178916 -4.1796751 -4.1975737 -4.2251668 -4.2353883 -4.2196403 -4.1852469 -4.1401734 -4.1241236 -4.1455936][-4.1965876 -4.2088218 -4.2090793 -4.2066178 -4.1960192 -4.1847577 -4.1910686 -4.2131939 -4.234973 -4.2358689 -4.2097659 -4.17017 -4.1356969 -4.1386414 -4.175199][-4.1703749 -4.183517 -4.1861715 -4.1879735 -4.1805496 -4.1745753 -4.184268 -4.2059345 -4.2218518 -4.2205057 -4.1965904 -4.1655688 -4.1458454 -4.15281 -4.1857896][-4.1318669 -4.1328268 -4.1324115 -4.1384273 -4.13953 -4.141017 -4.1545534 -4.1755519 -4.1864634 -4.1882315 -4.1766024 -4.1626668 -4.1514192 -4.1522546 -4.1759448][-4.1082125 -4.0969844 -4.0915275 -4.0973921 -4.1018181 -4.1042876 -4.1182442 -4.1384368 -4.1507907 -4.1642976 -4.1658883 -4.1591845 -4.1492257 -4.1440978 -4.1618309]]...]
INFO - root - 2017-12-06 06:34:06.114689: step 4110, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 57h:19m:49s remains)
INFO - root - 2017-12-06 06:34:12.521641: step 4120, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 56h:57m:58s remains)
INFO - root - 2017-12-06 06:34:18.798923: step 4130, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.612 sec/batch; 55h:49m:45s remains)
INFO - root - 2017-12-06 06:34:25.098477: step 4140, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.630 sec/batch; 57h:29m:16s remains)
INFO - root - 2017-12-06 06:34:31.431598: step 4150, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.630 sec/batch; 57h:29m:57s remains)
INFO - root - 2017-12-06 06:34:37.355500: step 4160, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 57h:33m:39s remains)
INFO - root - 2017-12-06 06:34:43.731917: step 4170, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 58h:40m:38s remains)
INFO - root - 2017-12-06 06:34:50.132291: step 4180, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:42m:00s remains)
INFO - root - 2017-12-06 06:34:56.496267: step 4190, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:56m:57s remains)
INFO - root - 2017-12-06 06:35:02.867850: step 4200, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.613 sec/batch; 55h:56m:31s remains)
2017-12-06 06:35:03.509348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2050052 -4.2133174 -4.2296896 -4.2431588 -4.2367067 -4.204422 -4.1369185 -4.053802 -3.9763825 -3.9150846 -3.894629 -3.9179862 -3.9668579 -4.0121565 -4.0527859][-4.2222733 -4.2356462 -4.2585588 -4.2770729 -4.2729316 -4.2394061 -4.17608 -4.0931149 -4.0043015 -3.9287724 -3.8977776 -3.9185646 -3.9710381 -4.0204263 -4.0608635][-4.240962 -4.2614126 -4.2879667 -4.3042045 -4.2995739 -4.2690272 -4.2145324 -4.1384215 -4.0461903 -3.9560099 -3.9053068 -3.9162381 -3.969574 -4.028194 -4.0750113][-4.2453337 -4.2709064 -4.2963071 -4.3084335 -4.3017359 -4.2748928 -4.2281513 -4.1591716 -4.0751977 -3.9788072 -3.9089561 -3.9029779 -3.9516737 -4.0197339 -4.0777292][-4.2395725 -4.2685103 -4.2891955 -4.296689 -4.2893753 -4.2672739 -4.2254477 -4.1639595 -4.0917091 -3.9971268 -3.9185181 -3.8980923 -3.9346476 -3.9998379 -4.0676][-4.2356029 -4.2619925 -4.2795868 -4.2822952 -4.2708178 -4.2498908 -4.2116814 -4.1587653 -4.092958 -4.0019007 -3.925632 -3.9001215 -3.920809 -3.9751363 -4.0474024][-4.2387009 -4.2638297 -4.280138 -4.2791171 -4.2650843 -4.2443852 -4.2115097 -4.1654696 -4.1014862 -4.0131159 -3.9439342 -3.9150143 -3.9168572 -3.9560115 -4.0271535][-4.2626281 -4.2843847 -4.298419 -4.2954168 -4.2819233 -4.2621717 -4.2341871 -4.1915884 -4.1296368 -4.04743 -3.9843485 -3.9465971 -3.9313438 -3.9558754 -4.0237064][-4.2904921 -4.3078108 -4.3162365 -4.3118148 -4.3024225 -4.2862711 -4.2647762 -4.2287779 -4.1729956 -4.1008458 -4.0445776 -4.000206 -3.9780951 -3.994122 -4.0514607][-4.3070216 -4.3208485 -4.3248844 -4.3165278 -4.3094096 -4.3005533 -4.28654 -4.2589564 -4.211225 -4.1520576 -4.1050763 -4.0639229 -4.040915 -4.0540147 -4.0989571][-4.323534 -4.332377 -4.3321991 -4.3230519 -4.3186064 -4.3154659 -4.3061767 -4.2849445 -4.247086 -4.2021532 -4.1644807 -4.131278 -4.1115007 -4.1194777 -4.1507735][-4.3409328 -4.3459268 -4.3429971 -4.3349118 -4.3293896 -4.3255167 -4.3192244 -4.3050218 -4.2789178 -4.2470441 -4.21917 -4.1952858 -4.1797595 -4.1820197 -4.2022357][-4.3451681 -4.3461728 -4.3415742 -4.3335285 -4.32551 -4.319943 -4.3156929 -4.3067784 -4.290463 -4.2712169 -4.252996 -4.237782 -4.2282577 -4.2310009 -4.2473555][-4.3266068 -4.3224258 -4.31609 -4.3086448 -4.3014817 -4.2969213 -4.2938061 -4.2905974 -4.2842379 -4.2768488 -4.2703128 -4.2656183 -4.2652111 -4.2728281 -4.2883434][-4.3098593 -4.3045368 -4.2991853 -4.295073 -4.2922597 -4.2905583 -4.290031 -4.2915359 -4.2922988 -4.2929311 -4.2947297 -4.2979355 -4.3037977 -4.3136029 -4.3257585]]...]
INFO - root - 2017-12-06 06:35:09.954182: step 4210, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 59h:02m:27s remains)
INFO - root - 2017-12-06 06:35:16.256435: step 4220, loss = 2.10, batch loss = 2.04 (12.9 examples/sec; 0.621 sec/batch; 56h:39m:21s remains)
INFO - root - 2017-12-06 06:35:22.784828: step 4230, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:28m:54s remains)
INFO - root - 2017-12-06 06:35:29.168198: step 4240, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 58h:21m:21s remains)
INFO - root - 2017-12-06 06:35:35.647746: step 4250, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.632 sec/batch; 57h:36m:56s remains)
INFO - root - 2017-12-06 06:35:42.036551: step 4260, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 57h:24m:15s remains)
INFO - root - 2017-12-06 06:35:48.191692: step 4270, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:59m:10s remains)
INFO - root - 2017-12-06 06:35:54.493412: step 4280, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 56h:52m:38s remains)
INFO - root - 2017-12-06 06:36:00.869608: step 4290, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 57h:23m:17s remains)
INFO - root - 2017-12-06 06:36:07.255417: step 4300, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 58h:07m:28s remains)
2017-12-06 06:36:07.923720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1754146 -4.1743402 -4.1719542 -4.1632867 -4.1477227 -4.1269383 -4.1088247 -4.1073189 -4.1240168 -4.1445141 -4.1617346 -4.1730289 -4.1757407 -4.1706061 -4.1609735][-4.1628566 -4.1651 -4.1635704 -4.1505246 -4.1250191 -4.092227 -4.0679069 -4.0704055 -4.0969687 -4.1244073 -4.1480975 -4.1671424 -4.1747236 -4.1697922 -4.1577387][-4.1518636 -4.1555848 -4.1518073 -4.1298895 -4.0906949 -4.0462565 -4.0200477 -4.0315 -4.0695581 -4.1053591 -4.1364779 -4.1615739 -4.1716409 -4.1654024 -4.1496916][-4.1464796 -4.1492691 -4.1404896 -4.1079817 -4.0548458 -4.00105 -3.9744613 -3.9959707 -4.0471067 -4.0940351 -4.1337972 -4.1613488 -4.1694231 -4.1596494 -4.1377382][-4.1467161 -4.1446619 -4.1282587 -4.0865126 -4.0231247 -3.9644947 -3.938297 -3.9704745 -4.0356359 -4.0930676 -4.13993 -4.1682663 -4.1738014 -4.1587076 -4.1258135][-4.1535907 -4.1419187 -4.1160049 -4.0661278 -3.995095 -3.9350092 -3.9093509 -3.9542449 -4.0352554 -4.1027646 -4.1550326 -4.1837158 -4.1864018 -4.1624794 -4.1132922][-4.1601562 -4.1391921 -4.1054416 -4.0481248 -3.9707787 -3.9075496 -3.8809497 -3.9401999 -4.0361366 -4.11218 -4.167819 -4.1982226 -4.2006073 -4.1689253 -4.1040387][-4.1602068 -4.1334143 -4.0977836 -4.0386395 -3.9579313 -3.8884621 -3.859026 -3.9300025 -4.0371528 -4.1210895 -4.1788287 -4.21154 -4.2152729 -4.18002 -4.107214][-4.1475782 -4.119225 -4.0910439 -4.0454922 -3.9773097 -3.9097912 -3.8805983 -3.9493852 -4.05468 -4.1382256 -4.1922269 -4.2251873 -4.2311854 -4.1980753 -4.1272721][-4.1289597 -4.1004286 -4.0835958 -4.059587 -4.0172 -3.9664242 -3.9474792 -4.005908 -4.0956984 -4.1665998 -4.2078362 -4.2344036 -4.2393293 -4.2124815 -4.1535397][-4.1119971 -4.086915 -4.082984 -4.0819683 -4.0675254 -4.0357995 -4.0251074 -4.0706739 -4.140522 -4.19448 -4.2220731 -4.2399912 -4.2417464 -4.2228718 -4.176672][-4.0967894 -4.0815687 -4.0932403 -4.1126385 -4.1187177 -4.1005583 -4.0924835 -4.1242027 -4.1752071 -4.2152157 -4.233377 -4.2438121 -4.2425723 -4.227664 -4.1869407][-4.0846748 -4.0830278 -4.1094322 -4.14165 -4.1586738 -4.15097 -4.1464038 -4.1669736 -4.2022934 -4.2323971 -4.2469053 -4.2529755 -4.2470784 -4.2274942 -4.1828108][-4.076951 -4.0860944 -4.1191721 -4.15525 -4.1758409 -4.1756349 -4.176075 -4.1909661 -4.2173386 -4.242424 -4.2568 -4.2616425 -4.2515764 -4.2257524 -4.1788096][-4.0730047 -4.0920515 -4.126297 -4.1597819 -4.1770182 -4.179657 -4.1822734 -4.1903777 -4.2096462 -4.2340336 -4.251164 -4.2584591 -4.2470613 -4.217886 -4.1742735]]...]
INFO - root - 2017-12-06 06:36:14.427692: step 4310, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 58h:21m:01s remains)
INFO - root - 2017-12-06 06:36:20.814248: step 4320, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:37m:02s remains)
INFO - root - 2017-12-06 06:36:27.131426: step 4330, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 56h:52m:41s remains)
INFO - root - 2017-12-06 06:36:33.674903: step 4340, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 58h:05m:26s remains)
INFO - root - 2017-12-06 06:36:39.996040: step 4350, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 57h:10m:51s remains)
INFO - root - 2017-12-06 06:36:46.452379: step 4360, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-06 06:36:52.494168: step 4370, loss = 2.08, batch loss = 2.02 (16.1 examples/sec; 0.496 sec/batch; 45h:14m:14s remains)
INFO - root - 2017-12-06 06:36:58.869799: step 4380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:55m:29s remains)
INFO - root - 2017-12-06 06:37:05.287914: step 4390, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 58h:38m:35s remains)
INFO - root - 2017-12-06 06:37:11.614944: step 4400, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 57h:57m:45s remains)
2017-12-06 06:37:12.287412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1783009 -4.1896787 -4.1898479 -4.1759572 -4.1724129 -4.1820617 -4.1949315 -4.1875119 -4.1586409 -4.1480989 -4.1592164 -4.1846437 -4.2042484 -4.1951628 -4.174643][-4.1697006 -4.1798544 -4.1740322 -4.1598473 -4.1630392 -4.1749783 -4.1869345 -4.1843796 -4.1662369 -4.1600947 -4.1674924 -4.1802516 -4.1797557 -4.1609678 -4.1480503][-4.1280689 -4.1371889 -4.1307683 -4.1161089 -4.1146088 -4.1227846 -4.1349773 -4.1443691 -4.1427412 -4.1451464 -4.154264 -4.1522365 -4.1379228 -4.12014 -4.1179156][-4.0498285 -4.0585518 -4.059453 -4.0459824 -4.042964 -4.0521441 -4.0658655 -4.0833373 -4.0920758 -4.0962782 -4.1059747 -4.0961714 -4.076128 -4.0587945 -4.0528336][-3.9790461 -3.9899361 -3.9952731 -3.9876618 -3.9888866 -4.0007606 -4.0088787 -4.017035 -4.0232272 -4.0259967 -4.0344133 -4.0268807 -4.0131392 -4.0055113 -3.994936][-3.9611144 -3.9685953 -3.9659026 -3.9507759 -3.9469552 -3.9455957 -3.9311886 -3.928683 -3.9498844 -3.9638693 -3.9685471 -3.9652617 -3.9593465 -3.9614797 -3.9553378][-3.9561481 -3.9529076 -3.9329858 -3.903127 -3.877665 -3.8382926 -3.78037 -3.7675352 -3.827776 -3.8715522 -3.8904128 -3.9044192 -3.9101925 -3.9262376 -3.9433711][-3.9796185 -3.9582911 -3.9241819 -3.8824 -3.8355768 -3.7667127 -3.6838942 -3.6739173 -3.7665491 -3.834614 -3.8666327 -3.9009573 -3.9288111 -3.9637396 -4.0053282][-4.0409131 -4.0099669 -3.9759119 -3.9402454 -3.9000721 -3.8430762 -3.7833738 -3.7838459 -3.8606112 -3.9177244 -3.9441113 -3.9788864 -4.016645 -4.054883 -4.1015167][-4.1044436 -4.0764117 -4.04842 -4.0246916 -4.0021434 -3.9725418 -3.9446816 -3.9489462 -3.9957161 -4.0309963 -4.0472531 -4.0713758 -4.1019664 -4.1317763 -4.1708727][-4.159297 -4.1411972 -4.1227961 -4.1089296 -4.0976806 -4.0856905 -4.0755534 -4.0770116 -4.0999713 -4.1193066 -4.12769 -4.1408534 -4.1589236 -4.1745753 -4.1994634][-4.2038388 -4.1918745 -4.181613 -4.1749115 -4.1709046 -4.1683745 -4.1639562 -4.1607785 -4.1687055 -4.1784487 -4.181097 -4.1842165 -4.185792 -4.1840858 -4.1922197][-4.2441854 -4.2336154 -4.2254033 -4.2216692 -4.2212272 -4.2231565 -4.2205014 -4.213294 -4.210731 -4.2112174 -4.2061505 -4.1974339 -4.1809239 -4.1628332 -4.1578593][-4.280611 -4.2728567 -4.2666316 -4.2646155 -4.2641068 -4.2640738 -4.2617278 -4.2564058 -4.2522092 -4.2482519 -4.2377272 -4.220367 -4.1904149 -4.1565485 -4.1328993][-4.313345 -4.3095746 -4.30376 -4.2998209 -4.29646 -4.2932377 -4.2908845 -4.2882142 -4.2855105 -4.2822309 -4.272831 -4.2550893 -4.2202864 -4.1748948 -4.1375494]]...]
INFO - root - 2017-12-06 06:37:18.717270: step 4410, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 56h:30m:03s remains)
INFO - root - 2017-12-06 06:37:25.106945: step 4420, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:53m:12s remains)
INFO - root - 2017-12-06 06:37:31.517053: step 4430, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 56h:50m:41s remains)
INFO - root - 2017-12-06 06:37:37.909276: step 4440, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:36m:06s remains)
INFO - root - 2017-12-06 06:37:44.377412: step 4450, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:43m:11s remains)
INFO - root - 2017-12-06 06:37:50.674037: step 4460, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 58h:10m:44s remains)
INFO - root - 2017-12-06 06:37:56.922049: step 4470, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:57m:46s remains)
INFO - root - 2017-12-06 06:38:03.146741: step 4480, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:51m:21s remains)
INFO - root - 2017-12-06 06:38:09.583534: step 4490, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 61h:02m:31s remains)
INFO - root - 2017-12-06 06:38:15.928939: step 4500, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 56h:50m:04s remains)
2017-12-06 06:38:16.627558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2100797 -4.210362 -4.2155437 -4.2125964 -4.2181854 -4.2237206 -4.2102346 -4.1782274 -4.157392 -4.1451769 -4.147028 -4.1687593 -4.2081923 -4.2517252 -4.2892752][-4.1936355 -4.1919255 -4.1951075 -4.1905727 -4.191124 -4.1940546 -4.1809607 -4.1485415 -4.123385 -4.1026316 -4.1026821 -4.1293259 -4.1752634 -4.2294326 -4.2777977][-4.1793656 -4.1742892 -4.1715288 -4.164578 -4.1656861 -4.170671 -4.1620331 -4.1335549 -4.10545 -4.0768781 -4.0754642 -4.1058168 -4.155725 -4.217093 -4.2728004][-4.1671348 -4.1546936 -4.1406884 -4.1234765 -4.1253839 -4.1377158 -4.1409721 -4.1245637 -4.0953174 -4.0613956 -4.0589585 -4.091156 -4.1443062 -4.2114415 -4.2723718][-4.1642137 -4.1532016 -4.1308489 -4.099319 -4.0902781 -4.1012421 -4.1105452 -4.1067433 -4.0807533 -4.0495152 -4.0531635 -4.0877166 -4.1400609 -4.2098203 -4.2736759][-4.1658144 -4.1599751 -4.1401095 -4.1003747 -4.069509 -4.0594997 -4.061717 -4.0647764 -4.045486 -4.0227556 -4.0376749 -4.0786638 -4.1343379 -4.2058988 -4.2722125][-4.1655965 -4.1680412 -4.1524348 -4.1109638 -4.0597825 -4.020648 -4.0038161 -4.0077 -3.9946651 -3.9839337 -4.0101323 -4.0566983 -4.1172996 -4.1930637 -4.2631621][-4.162643 -4.1721392 -4.1644545 -4.1322012 -4.0780559 -4.0253477 -3.9896612 -3.978704 -3.9617863 -3.9540157 -3.9837742 -4.03206 -4.0951657 -4.1740909 -4.2487698][-4.1562161 -4.1778908 -4.1826706 -4.1653538 -4.1207228 -4.0685706 -4.0237913 -3.9954705 -3.9623191 -3.9408908 -3.9629533 -4.0109439 -4.0783224 -4.1608009 -4.237668][-4.1431355 -4.1712275 -4.1916094 -4.1908689 -4.1645327 -4.1250663 -4.0854688 -4.0481243 -4.0044155 -3.9649105 -3.9658537 -4.004281 -4.0726018 -4.1572185 -4.2344537][-4.1535735 -4.1777167 -4.2024751 -4.2091341 -4.1965461 -4.1734562 -4.1422973 -4.1056828 -4.0608568 -4.0159392 -4.0033579 -4.0323091 -4.0941296 -4.1714954 -4.2418127][-4.1757264 -4.1867714 -4.20576 -4.2131872 -4.2101717 -4.2011566 -4.1813064 -4.1531115 -4.1139178 -4.0749345 -4.0624251 -4.0873957 -4.1375623 -4.1995087 -4.2580633][-4.2014537 -4.1985445 -4.2038264 -4.2083511 -4.214684 -4.2164364 -4.2092619 -4.1911216 -4.1590552 -4.1279626 -4.1221972 -4.1459641 -4.18542 -4.2321687 -4.2778578][-4.2211671 -4.2142229 -4.2105503 -4.20881 -4.2159333 -4.2221479 -4.2211747 -4.2112312 -4.1885719 -4.1657987 -4.1655803 -4.18877 -4.2231617 -4.2600851 -4.2939835][-4.2393937 -4.2370272 -4.2359509 -4.2341223 -4.2365484 -4.2384214 -4.2383442 -4.2353344 -4.2244473 -4.2106738 -4.2126656 -4.23044 -4.2564759 -4.2830563 -4.306366]]...]
INFO - root - 2017-12-06 06:38:23.037097: step 4510, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 58h:54m:03s remains)
INFO - root - 2017-12-06 06:38:29.456499: step 4520, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:50m:02s remains)
INFO - root - 2017-12-06 06:38:35.817352: step 4530, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 57h:53m:19s remains)
INFO - root - 2017-12-06 06:38:42.151491: step 4540, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 56h:37m:54s remains)
INFO - root - 2017-12-06 06:38:48.516533: step 4550, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 59h:25m:19s remains)
INFO - root - 2017-12-06 06:38:54.866541: step 4560, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.617 sec/batch; 56h:12m:19s remains)
INFO - root - 2017-12-06 06:39:01.034238: step 4570, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 57h:08m:26s remains)
INFO - root - 2017-12-06 06:39:07.440057: step 4580, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:57m:41s remains)
INFO - root - 2017-12-06 06:39:13.595575: step 4590, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.606 sec/batch; 55h:12m:58s remains)
INFO - root - 2017-12-06 06:39:20.075608: step 4600, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 56h:49m:31s remains)
2017-12-06 06:39:20.717285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2938461 -4.2920403 -4.2932935 -4.2946291 -4.2962928 -4.2975464 -4.2980304 -4.2977624 -4.2971191 -4.2921653 -4.2869291 -4.2838578 -4.27997 -4.2747087 -4.2693114][-4.2998796 -4.295733 -4.2952642 -4.2953396 -4.2965469 -4.2980747 -4.2997456 -4.3015924 -4.3030667 -4.297327 -4.2888017 -4.2813988 -4.2748995 -4.2678661 -4.2610445][-4.2834015 -4.2765026 -4.2750797 -4.2738962 -4.2749467 -4.27696 -4.2805653 -4.2863545 -4.2925725 -4.2907267 -4.2850986 -4.2779875 -4.2724552 -4.2661738 -4.261167][-4.2581844 -4.2463765 -4.2412429 -4.2365007 -4.2355046 -4.2376132 -4.2425723 -4.2505407 -4.2613273 -4.2663374 -4.2685533 -4.2669783 -4.2668886 -4.2656574 -4.2652984][-4.2368336 -4.2173076 -4.2056513 -4.1964583 -4.1920929 -4.1919708 -4.1972795 -4.2079945 -4.2247806 -4.23826 -4.2495332 -4.2564325 -4.2628341 -4.2652321 -4.2674341][-4.1921535 -4.1658769 -4.1470146 -4.1328993 -4.1243916 -4.1171169 -4.1171007 -4.1272793 -4.1496434 -4.1741385 -4.1989427 -4.2188792 -4.2357459 -4.2457952 -4.2530451][-4.1334085 -4.1019211 -4.0766692 -4.054687 -4.0361667 -4.0149221 -4.0017004 -4.007246 -4.0341897 -4.0713787 -4.1128697 -4.1486325 -4.1764326 -4.1962838 -4.2123647][-4.1258116 -4.0932617 -4.0663557 -4.0393887 -4.012393 -3.9786177 -3.9475861 -3.9379978 -3.9557881 -3.9918828 -4.0384221 -4.0847421 -4.1229272 -4.1508589 -4.173408][-4.1623216 -4.138906 -4.1216917 -4.1039357 -4.08464 -4.0572386 -4.0298581 -4.0125284 -4.0125661 -4.0311956 -4.0624447 -4.0984936 -4.1311007 -4.1558661 -4.1756525][-4.2162352 -4.1996446 -4.1884875 -4.178215 -4.1690388 -4.1544557 -4.1402073 -4.1263485 -4.1197662 -4.1285987 -4.1492219 -4.1740923 -4.1941104 -4.2077651 -4.2171636][-4.2610545 -4.2526894 -4.2459855 -4.2404537 -4.2385092 -4.2316108 -4.2251511 -4.2190557 -4.2149835 -4.2198944 -4.2321339 -4.24631 -4.2563162 -4.260818 -4.2613006][-4.2846794 -4.2798815 -4.2749777 -4.2725277 -4.2768826 -4.2791557 -4.2798324 -4.2804193 -4.2801189 -4.2819705 -4.2851639 -4.2877393 -4.2878609 -4.2841039 -4.277885][-4.2849617 -4.2838678 -4.2838449 -4.2861066 -4.2950053 -4.3022261 -4.3074532 -4.3109226 -4.3096118 -4.3045435 -4.2982988 -4.291811 -4.2846847 -4.2773719 -4.26972][-4.26208 -4.2619572 -4.263926 -4.2682247 -4.2780023 -4.286274 -4.2922068 -4.2973337 -4.2971382 -4.2860866 -4.2740326 -4.2638235 -4.2558217 -4.2494464 -4.2441278][-4.2339363 -4.2327085 -4.2341843 -4.2373757 -4.2443423 -4.2513657 -4.2557483 -4.2608471 -4.2607074 -4.2455063 -4.229825 -4.21834 -4.2138052 -4.2136154 -4.2149863]]...]
INFO - root - 2017-12-06 06:39:27.232342: step 4610, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 58h:01m:31s remains)
INFO - root - 2017-12-06 06:39:33.560279: step 4620, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 57h:37m:38s remains)
INFO - root - 2017-12-06 06:39:39.930941: step 4630, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:30m:34s remains)
INFO - root - 2017-12-06 06:39:46.309696: step 4640, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 57h:32m:46s remains)
INFO - root - 2017-12-06 06:39:52.731331: step 4650, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 60h:35m:39s remains)
INFO - root - 2017-12-06 06:39:59.125238: step 4660, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 58h:35m:27s remains)
INFO - root - 2017-12-06 06:40:05.402790: step 4670, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.625 sec/batch; 56h:55m:33s remains)
INFO - root - 2017-12-06 06:40:11.551784: step 4680, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 56h:58m:43s remains)
INFO - root - 2017-12-06 06:40:17.794030: step 4690, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:24m:41s remains)
INFO - root - 2017-12-06 06:40:24.141216: step 4700, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 57h:31m:19s remains)
2017-12-06 06:40:24.787388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2552824 -4.2562509 -4.2573209 -4.2574668 -4.2548175 -4.2520161 -4.2507434 -4.25045 -4.2510867 -4.25287 -4.2591486 -4.2675862 -4.2774887 -4.2826719 -4.2882061][-4.2407193 -4.24704 -4.2524529 -4.2527056 -4.2461133 -4.2398052 -4.237556 -4.2392898 -4.2416534 -4.2462544 -4.2534523 -4.2610893 -4.2709641 -4.2761745 -4.2819281][-4.2335982 -4.238143 -4.2437372 -4.2464371 -4.2416406 -4.23393 -4.2302046 -4.2343822 -4.2413325 -4.2487917 -4.2536583 -4.2569466 -4.2628975 -4.2637911 -4.2643361][-4.2330337 -4.23146 -4.2288041 -4.227788 -4.2205062 -4.2098837 -4.2019629 -4.2101507 -4.2289443 -4.2458682 -4.2508378 -4.2474804 -4.2437506 -4.236752 -4.2265005][-4.2384863 -4.2363653 -4.2260876 -4.2140174 -4.1971493 -4.1803865 -4.166409 -4.173317 -4.2039433 -4.2298241 -4.2358279 -4.2268195 -4.2147737 -4.2005715 -4.1797347][-4.2348671 -4.23123 -4.2142081 -4.1888351 -4.1596756 -4.1325083 -4.1064863 -4.1045189 -4.1475472 -4.1805291 -4.1886907 -4.1835666 -4.1713896 -4.1553569 -4.1321311][-4.2076235 -4.19193 -4.1655288 -4.131628 -4.0925579 -4.04622 -3.9947076 -3.9706328 -4.0289044 -4.0859389 -4.1103673 -4.1235271 -4.124898 -4.1232133 -4.113802][-4.1889071 -4.1587248 -4.1182251 -4.0690904 -4.0175595 -3.9520419 -3.8699551 -3.8216846 -3.9058347 -3.992991 -4.029645 -4.0604796 -4.0835381 -4.1078715 -4.1221313][-4.1994328 -4.1615973 -4.1120934 -4.0541239 -3.9993539 -3.9383032 -3.8663566 -3.8313708 -3.9137073 -3.99364 -4.0134029 -4.0389357 -4.0756187 -4.1193261 -4.1486349][-4.2316766 -4.2027903 -4.1671185 -4.1291642 -4.094357 -4.0593529 -4.01957 -4.004312 -4.0506177 -4.0917745 -4.0846362 -4.0917139 -4.1235619 -4.1640186 -4.1923437][-4.2711272 -4.255579 -4.2414985 -4.2288351 -4.2181144 -4.2034044 -4.1848769 -4.1749892 -4.1861124 -4.197391 -4.1810541 -4.1727762 -4.1933827 -4.2224007 -4.2410579][-4.296237 -4.2875423 -4.2867279 -4.2876358 -4.2901006 -4.2834606 -4.270803 -4.2621617 -4.2565808 -4.2509856 -4.2311392 -4.2156096 -4.2280378 -4.25022 -4.26203][-4.2868652 -4.27595 -4.2783904 -4.2877169 -4.293766 -4.2927279 -4.2863226 -4.2824264 -4.2771 -4.2651448 -4.2459021 -4.2257614 -4.228313 -4.2442179 -4.2511082][-4.2450366 -4.2318969 -4.233367 -4.2465587 -4.2592931 -4.2668056 -4.2663493 -4.2671351 -4.269877 -4.2650313 -4.2524033 -4.2341533 -4.2261806 -4.2272582 -4.2221532][-4.1982045 -4.1850991 -4.1866221 -4.1998792 -4.2117233 -4.2243476 -4.2346816 -4.2443385 -4.2584848 -4.2626925 -4.2542191 -4.2387466 -4.2241611 -4.2065563 -4.1822963]]...]
INFO - root - 2017-12-06 06:40:31.145162: step 4710, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 58h:07m:07s remains)
INFO - root - 2017-12-06 06:40:37.505324: step 4720, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.638 sec/batch; 58h:06m:23s remains)
INFO - root - 2017-12-06 06:40:43.953254: step 4730, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:16m:21s remains)
INFO - root - 2017-12-06 06:40:50.271938: step 4740, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 58h:15m:45s remains)
INFO - root - 2017-12-06 06:40:56.699624: step 4750, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 59h:30m:12s remains)
INFO - root - 2017-12-06 06:41:03.148455: step 4760, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:10m:57s remains)
INFO - root - 2017-12-06 06:41:09.557073: step 4770, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 57h:44m:32s remains)
INFO - root - 2017-12-06 06:41:15.812804: step 4780, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:19m:54s remains)
INFO - root - 2017-12-06 06:41:22.128118: step 4790, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:45m:59s remains)
INFO - root - 2017-12-06 06:41:28.456291: step 4800, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.628 sec/batch; 57h:09m:38s remains)
2017-12-06 06:41:29.083431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1961346 -4.1959071 -4.2027435 -4.2100468 -4.2161956 -4.2176604 -4.2132764 -4.2123804 -4.2197952 -4.2273321 -4.2290244 -4.2243176 -4.2271943 -4.2403722 -4.2602596][-4.2112455 -4.2087879 -4.2110705 -4.2150316 -4.2218394 -4.2276478 -4.2293172 -4.23209 -4.2396679 -4.2455425 -4.24472 -4.2403116 -4.245903 -4.2583432 -4.2731605][-4.2311568 -4.226377 -4.2260203 -4.2284369 -4.2360015 -4.2424269 -4.2442608 -4.2447119 -4.2487507 -4.2512283 -4.2454672 -4.2369981 -4.2404981 -4.2513285 -4.2625065][-4.2462463 -4.2393823 -4.2381315 -4.2403555 -4.2469168 -4.2505856 -4.2455382 -4.2369051 -4.2342033 -4.2330112 -4.2236962 -4.2112226 -4.2115736 -4.2220063 -4.2317176][-4.2424989 -4.2345243 -4.2309942 -4.2317343 -4.2374034 -4.2349329 -4.2145214 -4.1891665 -4.1798253 -4.1795368 -4.1733131 -4.161171 -4.1604085 -4.1721206 -4.1845646][-4.2153611 -4.2060547 -4.1968226 -4.1943145 -4.2000275 -4.1907525 -4.1484995 -4.1037421 -4.0924368 -4.1073027 -4.1169548 -4.11177 -4.114872 -4.1286416 -4.142807][-4.1719069 -4.1634622 -4.1498656 -4.1440144 -4.1498966 -4.1299596 -4.0603476 -3.9905651 -3.9851608 -4.0331745 -4.07173 -4.0837841 -4.0971894 -4.1127896 -4.126915][-4.1287613 -4.1238117 -4.1104069 -4.1011562 -4.1072459 -4.0824652 -3.9956331 -3.9100525 -3.9189377 -3.9989824 -4.0604038 -4.0850391 -4.1033931 -4.1182303 -4.1331644][-4.0969572 -4.0945363 -4.0830946 -4.0734029 -4.0887637 -4.0813355 -4.01554 -3.9511228 -3.9716597 -4.0508165 -4.1096916 -4.131814 -4.1432843 -4.1505647 -4.162189][-4.0810947 -4.0755486 -4.0636196 -4.0581031 -4.0879607 -4.1061554 -4.0798507 -4.0507216 -4.0756187 -4.135036 -4.1810579 -4.1984725 -4.2015395 -4.1997495 -4.204545][-4.080389 -4.0679088 -4.0542068 -4.0586772 -4.1009355 -4.1361523 -4.1386247 -4.1342964 -4.1557879 -4.1970668 -4.232893 -4.24978 -4.2532806 -4.25078 -4.2509227][-4.0888166 -4.0699992 -4.0593233 -4.0776286 -4.125617 -4.1645093 -4.18042 -4.1884341 -4.204937 -4.2322569 -4.2595377 -4.2763834 -4.284349 -4.2855911 -4.28355][-4.1251526 -4.10578 -4.1006632 -4.1227779 -4.1612725 -4.1924267 -4.2100034 -4.2223759 -4.2348552 -4.2520318 -4.2713447 -4.2861767 -4.2953191 -4.2968516 -4.2919593][-4.1656055 -4.1518884 -4.1502986 -4.1676521 -4.1923947 -4.2129107 -4.2288308 -4.2433405 -4.2524395 -4.2587457 -4.266921 -4.2766271 -4.2850266 -4.284565 -4.2767582][-4.1879039 -4.181253 -4.1809349 -4.1929374 -4.2081237 -4.2224989 -4.238668 -4.2520103 -4.2519917 -4.2418108 -4.2359967 -4.2437449 -4.2551079 -4.2537322 -4.242528]]...]
INFO - root - 2017-12-06 06:41:35.586269: step 4810, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 58h:17m:05s remains)
INFO - root - 2017-12-06 06:41:42.036741: step 4820, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:54m:54s remains)
INFO - root - 2017-12-06 06:41:48.438652: step 4830, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:45m:50s remains)
INFO - root - 2017-12-06 06:41:54.959151: step 4840, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 55h:39m:27s remains)
INFO - root - 2017-12-06 06:42:01.494410: step 4850, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:51m:10s remains)
INFO - root - 2017-12-06 06:42:07.919145: step 4860, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 61h:04m:16s remains)
INFO - root - 2017-12-06 06:42:14.432050: step 4870, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 59h:03m:32s remains)
INFO - root - 2017-12-06 06:42:20.626199: step 4880, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 56h:39m:30s remains)
INFO - root - 2017-12-06 06:42:27.046769: step 4890, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:42m:25s remains)
INFO - root - 2017-12-06 06:42:33.317170: step 4900, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 57h:23m:48s remains)
2017-12-06 06:42:33.943993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2249579 -4.2178879 -4.1981926 -4.1801119 -4.1777725 -4.1859212 -4.1981153 -4.2149806 -4.236228 -4.2593803 -4.2757325 -4.2741942 -4.262393 -4.2489362 -4.2385678][-4.246305 -4.2464671 -4.2299657 -4.2113194 -4.2081265 -4.2151747 -4.221725 -4.2317243 -4.2520537 -4.2797117 -4.3014517 -4.3037119 -4.2944789 -4.2795134 -4.2632351][-4.2539315 -4.2550135 -4.2374887 -4.2164431 -4.209507 -4.2113581 -4.2091064 -4.2093687 -4.2269096 -4.2567515 -4.2811222 -4.28727 -4.284513 -4.2740369 -4.2577806][-4.2367139 -4.2337713 -4.2116485 -4.1883483 -4.1772404 -4.1745057 -4.1638722 -4.1532965 -4.16554 -4.1930175 -4.216413 -4.2261443 -4.2327614 -4.2339554 -4.2288332][-4.20247 -4.19635 -4.1736126 -4.1511869 -4.1408434 -4.1371374 -4.1230803 -4.1053176 -4.1130137 -4.1321011 -4.1469259 -4.1570048 -4.170598 -4.1843777 -4.1968594][-4.1747141 -4.16539 -4.1444063 -4.12484 -4.1176114 -4.1154451 -4.0999131 -4.0810461 -4.0865188 -4.0951118 -4.0941439 -4.0980616 -4.1172223 -4.1426373 -4.1707225][-4.1613836 -4.1473775 -4.1271515 -4.1097884 -4.1041684 -4.1007833 -4.0802512 -4.0588813 -4.0647025 -4.0637312 -4.0456004 -4.0428786 -4.0714192 -4.1095376 -4.1471438][-4.16924 -4.1510134 -4.1304336 -4.11295 -4.10464 -4.0961285 -4.0720229 -4.0508947 -4.0564928 -4.0526628 -4.0279708 -4.021966 -4.0545573 -4.0957193 -4.1353889][-4.1974187 -4.1782551 -4.1578794 -4.1400824 -4.1297054 -4.1238074 -4.1105304 -4.0994639 -4.1033244 -4.0972695 -4.0728259 -4.0613461 -4.0827541 -4.1138382 -4.146049][-4.2160692 -4.2007003 -4.1845884 -4.1671309 -4.1570735 -4.15987 -4.166234 -4.1735058 -4.1822772 -4.1788568 -4.155457 -4.1361332 -4.1434021 -4.1601567 -4.178072][-4.2148862 -4.1944776 -4.1777062 -4.1611719 -4.1540189 -4.1639938 -4.1861944 -4.2128186 -4.2364697 -4.2447791 -4.2276287 -4.2072349 -4.2046795 -4.2085204 -4.2125883][-4.2106729 -4.1803622 -4.1515884 -4.1260028 -4.11661 -4.1297188 -4.1626773 -4.2053628 -4.2443604 -4.2672276 -4.2637339 -4.2515907 -4.2462835 -4.242929 -4.2385068][-4.2222362 -4.1875553 -4.1458473 -4.1011357 -4.0728331 -4.0769529 -4.110055 -4.15784 -4.2031894 -4.2360678 -4.245935 -4.2461033 -4.2483687 -4.2472568 -4.2407122][-4.2425165 -4.2078409 -4.1601205 -4.09856 -4.04354 -4.0280704 -4.0487008 -4.0866671 -4.1285157 -4.1637039 -4.1805797 -4.1933188 -4.2109962 -4.2199478 -4.2195292][-4.2682724 -4.2380075 -4.193193 -4.130487 -4.0648627 -4.0321474 -4.027729 -4.0409207 -4.0690069 -4.097465 -4.113637 -4.13281 -4.1649976 -4.18694 -4.1980925]]...]
INFO - root - 2017-12-06 06:42:40.255169: step 4910, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 56h:45m:48s remains)
INFO - root - 2017-12-06 06:42:46.680805: step 4920, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 57h:21m:47s remains)
INFO - root - 2017-12-06 06:42:53.137697: step 4930, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 57h:48m:02s remains)
INFO - root - 2017-12-06 06:42:59.472407: step 4940, loss = 2.06, batch loss = 2.01 (13.2 examples/sec; 0.606 sec/batch; 55h:07m:11s remains)
INFO - root - 2017-12-06 06:43:05.877597: step 4950, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:16m:51s remains)
INFO - root - 2017-12-06 06:43:12.299796: step 4960, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 58h:37m:22s remains)
INFO - root - 2017-12-06 06:43:18.746332: step 4970, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 56h:16m:53s remains)
INFO - root - 2017-12-06 06:43:25.128620: step 4980, loss = 2.10, batch loss = 2.04 (14.6 examples/sec; 0.546 sec/batch; 49h:42m:06s remains)
INFO - root - 2017-12-06 06:43:31.547134: step 4990, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 58h:37m:20s remains)
INFO - root - 2017-12-06 06:43:38.010750: step 5000, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 59h:23m:28s remains)
2017-12-06 06:43:38.626366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9790812 -4.0271063 -4.0771537 -4.1249752 -4.1655846 -4.1909566 -4.1924448 -4.1675215 -4.1240025 -4.075048 -4.0496712 -4.0440178 -4.0575724 -4.0709081 -4.0598125][-4.0846429 -4.1308537 -4.1721826 -4.2077951 -4.2338557 -4.2427845 -4.2296925 -4.19348 -4.1403527 -4.0875707 -4.0679026 -4.0724034 -4.0931883 -4.110548 -4.1007071][-4.195086 -4.2343559 -4.2698269 -4.2968836 -4.3099747 -4.3017187 -4.2729592 -4.2270713 -4.1728778 -4.12909 -4.11732 -4.1210985 -4.1338859 -4.1437812 -4.1314411][-4.2854161 -4.3202267 -4.3474064 -4.3577967 -4.3465309 -4.3119392 -4.2630205 -4.2095222 -4.1636949 -4.1383886 -4.1371536 -4.1422167 -4.149159 -4.1545677 -4.1479592][-4.3438292 -4.3639402 -4.3688703 -4.34704 -4.3017993 -4.2419829 -4.1771092 -4.1202807 -4.0891228 -4.0845146 -4.0957646 -4.1049247 -4.1134324 -4.13083 -4.1514721][-4.3502841 -4.3474088 -4.3229184 -4.2638588 -4.1817207 -4.096117 -4.0129704 -3.9557321 -3.9528179 -3.9845197 -4.0260668 -4.055603 -4.0825229 -4.1253872 -4.1760826][-4.3062959 -4.2838426 -4.2336307 -4.1439314 -4.0341582 -3.936435 -3.8577855 -3.8284407 -3.88061 -3.968513 -4.0507684 -4.1034331 -4.1404924 -4.1847348 -4.2311659][-4.252872 -4.2205238 -4.1656156 -4.0788159 -3.9859533 -3.9180737 -3.8803778 -3.8927832 -3.9752057 -4.0750303 -4.1537914 -4.1960907 -4.2148623 -4.2344875 -4.2557521][-4.2407603 -4.2192578 -4.1867285 -4.1369824 -4.0856819 -4.0504169 -4.0382433 -4.0603709 -4.1243567 -4.1916456 -4.237093 -4.2567267 -4.2557564 -4.250905 -4.2494197][-4.2531013 -4.24619 -4.2371173 -4.2194252 -4.1986575 -4.1816249 -4.1767006 -4.1906676 -4.2240376 -4.2591138 -4.2816205 -4.2894011 -4.2763886 -4.2525625 -4.2335062][-4.2531176 -4.2607508 -4.2750716 -4.2812033 -4.2771111 -4.2671289 -4.2617006 -4.2658257 -4.2740607 -4.2833195 -4.2883539 -4.2842674 -4.2593265 -4.2283134 -4.2038059][-4.2271371 -4.2506409 -4.2845521 -4.3060884 -4.3072505 -4.2959113 -4.2824392 -4.2765837 -4.2665129 -4.2548051 -4.2469239 -4.2368288 -4.2101374 -4.1870761 -4.1744728][-4.1981993 -4.2327714 -4.2729573 -4.2959113 -4.2913837 -4.2685504 -4.2402234 -4.2249527 -4.2070212 -4.188446 -4.1831465 -4.18202 -4.1629353 -4.1515107 -4.1551828][-4.1960378 -4.2261 -4.2562308 -4.2691283 -4.2550206 -4.2237687 -4.1905494 -4.1755757 -4.1627011 -4.1510148 -4.15798 -4.166358 -4.1513081 -4.1416688 -4.1503482][-4.2272944 -4.2463093 -4.262414 -4.2659278 -4.2497611 -4.2234344 -4.1978388 -4.1897721 -4.1838531 -4.1782184 -4.187026 -4.1943274 -4.1810222 -4.1723418 -4.1783395]]...]
INFO - root - 2017-12-06 06:43:44.939200: step 5010, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 56h:28m:52s remains)
INFO - root - 2017-12-06 06:43:51.328146: step 5020, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 60h:37m:36s remains)
INFO - root - 2017-12-06 06:43:57.791507: step 5030, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:16m:57s remains)
INFO - root - 2017-12-06 06:44:04.164684: step 5040, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:16m:57s remains)
INFO - root - 2017-12-06 06:44:10.707318: step 5050, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:57m:18s remains)
INFO - root - 2017-12-06 06:44:17.135195: step 5060, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 57h:01m:55s remains)
INFO - root - 2017-12-06 06:44:23.490664: step 5070, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 57h:48m:20s remains)
INFO - root - 2017-12-06 06:44:30.121232: step 5080, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:58m:29s remains)
INFO - root - 2017-12-06 06:44:36.423531: step 5090, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:40m:01s remains)
INFO - root - 2017-12-06 06:44:42.874222: step 5100, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:35m:49s remains)
2017-12-06 06:44:43.100607: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:44:43.100745: E tensorflow/core/util/events_writer.cc:131] Failed to flush 2 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:44:43.530762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1900129 -4.2053618 -4.2194471 -4.2137156 -4.2102013 -4.2311678 -4.2562594 -4.2589955 -4.2512875 -4.2450461 -4.2496834 -4.2661505 -4.2748303 -4.2570419 -4.2232509][-4.1424618 -4.1618671 -4.1811237 -4.1822734 -4.1826315 -4.2064142 -4.2371073 -4.246213 -4.2373857 -4.2288265 -4.2313905 -4.2449312 -4.2576752 -4.2434855 -4.2074642][-4.15256 -4.1717305 -4.1887183 -4.1911902 -4.1898389 -4.2016211 -4.2157969 -4.2210503 -4.21232 -4.2093158 -4.21834 -4.2325068 -4.2486062 -4.2397413 -4.2101016][-4.2054939 -4.2180595 -4.2264719 -4.2208791 -4.2120037 -4.2088046 -4.2033491 -4.1970658 -4.1916018 -4.2004972 -4.2168684 -4.2315369 -4.2459474 -4.2410097 -4.2217088][-4.2412028 -4.2468109 -4.2467442 -4.2339592 -4.219 -4.2032671 -4.1822748 -4.1665874 -4.1676192 -4.1868157 -4.2101035 -4.22497 -4.2347217 -4.22945 -4.22035][-4.2425604 -4.2438421 -4.2388635 -4.2209792 -4.1961722 -4.1635075 -4.12438 -4.0973287 -4.1116529 -4.1460404 -4.1740351 -4.1931767 -4.2060819 -4.2069006 -4.2096906][-4.2243848 -4.2212696 -4.2077084 -4.1783447 -4.1345916 -4.0758843 -4.0100174 -3.9695451 -4.0025163 -4.0658088 -4.1100683 -4.1386995 -4.157989 -4.1726103 -4.1912837][-4.1758194 -4.1685719 -4.1490068 -4.1111469 -4.0518918 -3.9701655 -3.8768768 -3.822819 -3.8815742 -3.9902692 -4.0686936 -4.1149406 -4.1407037 -4.1626353 -4.1902566][-4.1564908 -4.1450586 -4.1252389 -4.0906706 -4.0384383 -3.9661064 -3.8837388 -3.8351998 -3.8956413 -4.0099931 -4.0920115 -4.138597 -4.1627374 -4.17848 -4.1964464][-4.1887546 -4.1722937 -4.155405 -4.130043 -4.0967169 -4.0510187 -4.002327 -3.9761462 -4.0177188 -4.098392 -4.1570005 -4.1888733 -4.1993432 -4.1957121 -4.1959138][-4.22156 -4.2068715 -4.19621 -4.1790123 -4.1596975 -4.1331115 -4.105341 -4.0963631 -4.1222858 -4.1718554 -4.2113118 -4.2319779 -4.2316589 -4.2149625 -4.2006412][-4.2267776 -4.2196584 -4.2181959 -4.2084918 -4.1964397 -4.1841745 -4.1747108 -4.1804857 -4.1995149 -4.2270474 -4.2475519 -4.2561445 -4.2508678 -4.230433 -4.2112288][-4.2285762 -4.2307978 -4.2361736 -4.2366176 -4.2341595 -4.2310419 -4.2289371 -4.2354746 -4.2463083 -4.2578111 -4.2645583 -4.2623014 -4.2520838 -4.2322459 -4.2112813][-4.2573061 -4.2633348 -4.2682037 -4.2721477 -4.2728395 -4.271565 -4.2705879 -4.2721081 -4.27352 -4.2732482 -4.2714405 -4.2645955 -4.2538137 -4.2376442 -4.2174325][-4.3011909 -4.3065095 -4.3085361 -4.3097234 -4.30851 -4.3043995 -4.3004251 -4.2981315 -4.2959361 -4.2925339 -4.2883911 -4.2830157 -4.2750111 -4.2614427 -4.2445536]]...]
INFO - root - 2017-12-06 06:44:49.807521: step 5110, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 0.525 sec/batch; 47h:42m:31s remains)
INFO - root - 2017-12-06 06:44:56.312414: step 5120, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:34m:46s remains)
INFO - root - 2017-12-06 06:45:02.816224: step 5130, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:44m:14s remains)
INFO - root - 2017-12-06 06:45:09.310236: step 5140, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:54m:36s remains)
INFO - root - 2017-12-06 06:45:15.719689: step 5150, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:45m:05s remains)
INFO - root - 2017-12-06 06:45:22.086294: step 5160, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 58h:44m:50s remains)
INFO - root - 2017-12-06 06:45:28.592455: step 5170, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:14m:32s remains)
INFO - root - 2017-12-06 06:45:34.966123: step 5180, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 57h:11m:29s remains)
INFO - root - 2017-12-06 06:45:41.272066: step 5190, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:35m:21s remains)
INFO - root - 2017-12-06 06:45:47.657857: step 5200, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:46m:54s remains)
2017-12-06 06:45:48.358642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2879391 -4.2814879 -4.2747722 -4.2700272 -4.2686677 -4.270997 -4.2734861 -4.2741127 -4.2732821 -4.272398 -4.2721171 -4.2734084 -4.2746158 -4.2733159 -4.2698178][-4.2835751 -4.2827187 -4.277216 -4.2684183 -4.2609787 -4.2590361 -4.2621293 -4.2679429 -4.2721214 -4.273787 -4.2728872 -4.27207 -4.2716069 -4.2701 -4.2674727][-4.2624345 -4.27267 -4.2705383 -4.2561011 -4.2387891 -4.2298808 -4.23355 -4.247685 -4.2629905 -4.2735906 -4.2769055 -4.2755818 -4.2731962 -4.2705026 -4.2677927][-4.2177467 -4.2457323 -4.2495885 -4.22615 -4.1941566 -4.1749406 -4.179565 -4.2049541 -4.2373004 -4.2650342 -4.2801943 -4.2832017 -4.280551 -4.2759218 -4.2714195][-4.1509428 -4.2040768 -4.218183 -4.1852322 -4.1304531 -4.0897837 -4.0915837 -4.131731 -4.1873512 -4.2397208 -4.2742739 -4.2880778 -4.2883744 -4.2838225 -4.2770653][-4.0841904 -4.1658025 -4.1927915 -4.1533685 -4.0706205 -3.9938126 -3.9749112 -4.0241446 -4.1068358 -4.19005 -4.2519813 -4.284132 -4.2919464 -4.2884679 -4.2801833][-4.0495729 -4.15002 -4.1898932 -4.1564755 -4.0600109 -3.9418833 -3.8779602 -3.9133253 -4.0133739 -4.1245728 -4.2133269 -4.2677283 -4.2889991 -4.2885056 -4.2789292][-4.0790424 -4.1674128 -4.2104678 -4.1886358 -4.1021562 -3.9767454 -3.8756928 -3.8663018 -3.9521868 -4.0683575 -4.169971 -4.2405291 -4.2768435 -4.2835493 -4.2743139][-4.1575713 -4.2126093 -4.2406454 -4.2260628 -4.1626458 -4.0672541 -3.9799018 -3.9440632 -3.9801593 -4.0635552 -4.1504359 -4.2207484 -4.2656407 -4.2799997 -4.2723][-4.232482 -4.2594633 -4.2663503 -4.2477541 -4.2031364 -4.1464086 -4.0947146 -4.0680437 -4.0798383 -4.1211505 -4.1729875 -4.2261949 -4.2683339 -4.2866168 -4.2802887][-4.2766356 -4.2869163 -4.2762957 -4.2508574 -4.2187247 -4.1911368 -4.16854 -4.15718 -4.1646886 -4.1815643 -4.2060733 -4.2414584 -4.2766371 -4.2964463 -4.2931237][-4.2927094 -4.2890267 -4.2653213 -4.2352123 -4.210979 -4.2004404 -4.1950073 -4.1901264 -4.1904569 -4.1928668 -4.2047758 -4.2352161 -4.2719107 -4.2967429 -4.3003249][-4.2929244 -4.27488 -4.2404089 -4.2076321 -4.1900592 -4.1879315 -4.1872129 -4.1777315 -4.1603594 -4.1436591 -4.1504745 -4.1917391 -4.243988 -4.2836337 -4.2997212][-4.2761455 -4.2479358 -4.2094803 -4.1804323 -4.172338 -4.1784458 -4.1766248 -4.1531696 -4.1061835 -4.0580468 -4.0527129 -4.1059275 -4.180284 -4.242662 -4.2806911][-4.2483811 -4.2170572 -4.1859145 -4.1706786 -4.1771984 -4.192276 -4.1893415 -4.1514006 -4.0765963 -3.9881847 -3.9506631 -4.0035124 -4.0955296 -4.1798868 -4.2421556]]...]
INFO - root - 2017-12-06 06:45:54.978747: step 5210, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 61h:13m:10s remains)
INFO - root - 2017-12-06 06:46:01.392915: step 5220, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:57m:20s remains)
INFO - root - 2017-12-06 06:46:07.758381: step 5230, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 57h:55m:09s remains)
INFO - root - 2017-12-06 06:46:14.101575: step 5240, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 57h:57m:57s remains)
INFO - root - 2017-12-06 06:46:20.438822: step 5250, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:34m:11s remains)
INFO - root - 2017-12-06 06:46:26.876647: step 5260, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 57h:23m:05s remains)
INFO - root - 2017-12-06 06:46:33.308417: step 5270, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:43m:09s remains)
INFO - root - 2017-12-06 06:46:39.776970: step 5280, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 59h:01m:35s remains)
INFO - root - 2017-12-06 06:46:46.067493: step 5290, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:35m:24s remains)
INFO - root - 2017-12-06 06:46:52.541053: step 5300, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 57h:51m:10s remains)
2017-12-06 06:46:52.758014: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:46:52.758054: E tensorflow/core/util/events_writer.cc:131] Failed to flush 4 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:46:53.197391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3172398 -4.3080397 -4.3014131 -4.2921343 -4.28132 -4.2748322 -4.2798367 -4.2843523 -4.2874589 -4.2969627 -4.3068533 -4.3077445 -4.3052883 -4.3025842 -4.2963719][-4.298491 -4.2820945 -4.2716222 -4.2593036 -4.2475734 -4.2429485 -4.2545223 -4.2619095 -4.2673779 -4.2785339 -4.2906189 -4.292717 -4.2894678 -4.2841983 -4.2767577][-4.266572 -4.2436547 -4.2273326 -4.2121677 -4.2001057 -4.2013235 -4.2191415 -4.23073 -4.2405739 -4.25513 -4.27045 -4.2769175 -4.2757668 -4.2703524 -4.2622709][-4.2261209 -4.1995287 -4.1804934 -4.1634431 -4.1514316 -4.1577559 -4.1809893 -4.2003865 -4.2177024 -4.2360387 -4.2529044 -4.2619462 -4.2614074 -4.2575865 -4.2506595][-4.1659565 -4.1416254 -4.1168656 -4.0913267 -4.0778446 -4.0845571 -4.104548 -4.1281662 -4.1596375 -4.1940136 -4.2185774 -4.2287273 -4.2302332 -4.2337651 -4.2343988][-4.0956159 -4.07349 -4.0378189 -3.9961472 -3.9786935 -3.9861786 -3.9981263 -4.026237 -4.0745726 -4.1271682 -4.165556 -4.1849589 -4.1975403 -4.2083063 -4.2156663][-4.0340548 -4.0054154 -3.9503813 -3.8875306 -3.8637071 -3.8657515 -3.8681595 -3.9032605 -3.9687893 -4.0353994 -4.09146 -4.1292734 -4.1579795 -4.1774268 -4.190136][-4.0063357 -3.9738615 -3.9077127 -3.8395948 -3.8132391 -3.7994523 -3.7778552 -3.8031845 -3.8711631 -3.9473674 -4.0211382 -4.0776381 -4.1233573 -4.152411 -4.1713448][-4.0234365 -3.998847 -3.9453549 -3.8976705 -3.8806121 -3.8603795 -3.8259313 -3.8277354 -3.8681817 -3.9290657 -4.0025415 -4.0656238 -4.1164403 -4.1459818 -4.1669426][-4.0903482 -4.0744166 -4.039619 -4.0114779 -3.9993455 -3.9793067 -3.9495108 -3.9388535 -3.9560385 -3.9964862 -4.0537472 -4.1040945 -4.144382 -4.1676083 -4.1860313][-4.1658883 -4.1584916 -4.1404104 -4.1270423 -4.1164742 -4.0985112 -4.0781465 -4.0660954 -4.0717826 -4.0977325 -4.1372943 -4.1726928 -4.1998663 -4.2128186 -4.2225142][-4.226655 -4.2252221 -4.2171283 -4.2110472 -4.2022014 -4.1906519 -4.181438 -4.1743422 -4.1749945 -4.1901269 -4.2146816 -4.2389736 -4.2583895 -4.2640939 -4.2638168][-4.2618923 -4.2599373 -4.2566409 -4.2529812 -4.2452679 -4.2394652 -4.2380714 -4.2364645 -4.2379541 -4.2460737 -4.2596507 -4.2744746 -4.2892156 -4.293076 -4.2907434][-4.2618694 -4.2573118 -4.2551579 -4.2535739 -4.2486253 -4.2455392 -4.2462425 -4.249126 -4.2527623 -4.2597075 -4.269567 -4.2812519 -4.294034 -4.2992463 -4.3003573][-4.2503695 -4.2448239 -4.2440438 -4.2443061 -4.242002 -4.2403626 -4.2405634 -4.2432132 -4.2470789 -4.2531042 -4.2620125 -4.2735362 -4.2870717 -4.2957549 -4.3015871]]...]
INFO - root - 2017-12-06 06:46:59.764234: step 5310, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.644 sec/batch; 58h:34m:01s remains)
INFO - root - 2017-12-06 06:47:06.258489: step 5320, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 62h:58m:36s remains)
INFO - root - 2017-12-06 06:47:12.492165: step 5330, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 56h:36m:10s remains)
INFO - root - 2017-12-06 06:47:18.936505: step 5340, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 57h:07m:28s remains)
INFO - root - 2017-12-06 06:47:25.341178: step 5350, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:53m:04s remains)
INFO - root - 2017-12-06 06:47:31.763966: step 5360, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 56h:35m:50s remains)
INFO - root - 2017-12-06 06:47:38.204577: step 5370, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 57h:06m:16s remains)
INFO - root - 2017-12-06 06:47:44.601946: step 5380, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 57h:24m:54s remains)
INFO - root - 2017-12-06 06:47:50.884364: step 5390, loss = 2.08, batch loss = 2.02 (14.8 examples/sec; 0.539 sec/batch; 48h:58m:04s remains)
INFO - root - 2017-12-06 06:47:57.240081: step 5400, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:02m:46s remains)
2017-12-06 06:47:57.890314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.315136 -4.3145075 -4.3151617 -4.314816 -4.3095412 -4.3059297 -4.306747 -4.3045282 -4.3014159 -4.2971516 -4.2891126 -4.27962 -4.2710118 -4.2696748 -4.2744927][-4.3126154 -4.3147788 -4.3230519 -4.32772 -4.3232536 -4.3187485 -4.3196945 -4.3170509 -4.3139977 -4.3091812 -4.2973428 -4.2826247 -4.2675543 -4.2598863 -4.26079][-4.2951331 -4.2946577 -4.3058934 -4.3127718 -4.3065419 -4.2993088 -4.2992835 -4.2988286 -4.3001523 -4.3012357 -4.2912059 -4.2732644 -4.2544265 -4.2447505 -4.2451324][-4.2784953 -4.2738333 -4.2838116 -4.2886958 -4.2778392 -4.2626038 -4.2554483 -4.2538877 -4.2638669 -4.2792573 -4.2784996 -4.264535 -4.2475224 -4.2393017 -4.23941][-4.2665873 -4.2550931 -4.2554388 -4.2501683 -4.2315264 -4.205389 -4.1825271 -4.1732864 -4.1947913 -4.2318511 -4.2509904 -4.2516856 -4.2424912 -4.240624 -4.2423244][-4.2607036 -4.2437525 -4.232193 -4.2148547 -4.186204 -4.1461616 -4.0981426 -4.0664053 -4.0965986 -4.1631589 -4.2087936 -4.2311811 -4.2357969 -4.2416043 -4.248456][-4.2613263 -4.2428246 -4.2251954 -4.1962209 -4.1506681 -4.0864654 -4.005125 -3.9426491 -3.9698234 -4.0595474 -4.1343918 -4.1834984 -4.2143612 -4.234787 -4.2517848][-4.2573438 -4.239037 -4.2200246 -4.1829624 -4.1196194 -4.0326924 -3.9315858 -3.8463216 -3.8555908 -3.9507577 -4.0469007 -4.1219087 -4.1812654 -4.2164121 -4.2390995][-4.2588096 -4.2469192 -4.2384748 -4.2106938 -4.1558542 -4.0808053 -3.9975145 -3.9213314 -3.9056966 -3.9636512 -4.0339708 -4.1002588 -4.1580286 -4.1917043 -4.2110882][-4.26437 -4.2612572 -4.2652035 -4.2530365 -4.22244 -4.1829743 -4.1339936 -4.076448 -4.0408726 -4.0509672 -4.0764093 -4.1115561 -4.1469927 -4.1684427 -4.1817331][-4.2567611 -4.2554893 -4.2635865 -4.2594533 -4.2438984 -4.2303882 -4.21093 -4.1741214 -4.1384821 -4.1216736 -4.1159792 -4.1255255 -4.1410403 -4.1514773 -4.1629648][-4.2436066 -4.2368865 -4.2448587 -4.2477369 -4.2407832 -4.2398667 -4.2389812 -4.2235947 -4.2011709 -4.1810122 -4.1630497 -4.1582193 -4.1605892 -4.1639867 -4.1742005][-4.2399368 -4.2310095 -4.2355008 -4.2393484 -4.2357817 -4.2421508 -4.2537403 -4.2558603 -4.2480311 -4.2364764 -4.2195182 -4.2083435 -4.2040548 -4.20322 -4.2106476][-4.2538571 -4.2441621 -4.2445107 -4.2460709 -4.2446585 -4.2543874 -4.2708197 -4.2790604 -4.2791781 -4.2781448 -4.2699957 -4.2591577 -4.2494149 -4.2444787 -4.2466946][-4.2695155 -4.2604284 -4.2587771 -4.2608933 -4.2618227 -4.2697949 -4.2814503 -4.2862821 -4.28762 -4.2919369 -4.2921076 -4.2846036 -4.2742033 -4.2701831 -4.2729721]]...]
INFO - root - 2017-12-06 06:48:04.301681: step 5410, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:07m:49s remains)
INFO - root - 2017-12-06 06:48:10.762406: step 5420, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:53m:33s remains)
INFO - root - 2017-12-06 06:48:17.058127: step 5430, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 56h:39m:49s remains)
INFO - root - 2017-12-06 06:48:23.480406: step 5440, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 59h:18m:16s remains)
INFO - root - 2017-12-06 06:48:29.964880: step 5450, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:07m:33s remains)
INFO - root - 2017-12-06 06:48:36.431701: step 5460, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 61h:08m:31s remains)
INFO - root - 2017-12-06 06:48:42.701252: step 5470, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 56h:28m:56s remains)
INFO - root - 2017-12-06 06:48:49.121134: step 5480, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 57h:24m:51s remains)
INFO - root - 2017-12-06 06:48:55.649075: step 5490, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:33m:49s remains)
INFO - root - 2017-12-06 06:49:01.975915: step 5500, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:27m:46s remains)
2017-12-06 06:49:02.183439: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:49:02.183471: E tensorflow/core/util/events_writer.cc:131] Failed to flush 6 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:49:02.606553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33228 -4.31621 -4.2979555 -4.2855206 -4.2783709 -4.2728353 -4.2775197 -4.2886791 -4.2940173 -4.2966566 -4.299232 -4.3055048 -4.3151116 -4.3233395 -4.328259][-4.3191786 -4.2960372 -4.2732468 -4.2584043 -4.2518044 -4.2430019 -4.2443509 -4.2589927 -4.2602906 -4.25775 -4.262362 -4.2758713 -4.2936931 -4.3102164 -4.3233738][-4.3080831 -4.2777448 -4.2473197 -4.2262425 -4.2143836 -4.1981816 -4.1952939 -4.2172184 -4.2177081 -4.206522 -4.2120452 -4.2360773 -4.26448 -4.2900348 -4.3109922][-4.2976465 -4.2614031 -4.2187204 -4.1847248 -4.1635537 -4.131031 -4.1192217 -4.1506019 -4.1612988 -4.1503849 -4.1598768 -4.1975107 -4.2385 -4.2735662 -4.3000641][-4.2837191 -4.2391477 -4.1881475 -4.13701 -4.0980816 -4.0410061 -4.0027227 -4.0366406 -4.0665207 -4.0728736 -4.0991607 -4.1559196 -4.21498 -4.2624364 -4.2939553][-4.2662635 -4.2095633 -4.144577 -4.0699134 -4.0094757 -3.925205 -3.8429976 -3.8678677 -3.9343665 -3.9754491 -4.0288534 -4.10285 -4.1776538 -4.2397313 -4.2822404][-4.2552118 -4.1883192 -4.1107149 -4.0133667 -3.9217739 -3.7952738 -3.6393466 -3.6330583 -3.7462482 -3.8522432 -3.9474385 -4.0441713 -4.1334157 -4.2095222 -4.2646389][-4.2566266 -4.189784 -4.1103954 -4.0054126 -3.884743 -3.7121117 -3.4851904 -3.4454782 -3.5956907 -3.764801 -3.8951612 -4.0087876 -4.1119604 -4.1958652 -4.2573543][-4.2594471 -4.202827 -4.1417079 -4.0552092 -3.9469295 -3.781312 -3.569272 -3.5357523 -3.6627851 -3.8138926 -3.9293168 -4.0316281 -4.1297045 -4.2083592 -4.2663159][-4.270154 -4.226047 -4.185822 -4.1289935 -4.0598097 -3.944521 -3.799551 -3.7902424 -3.8696117 -3.9613686 -4.0320568 -4.1062031 -4.181252 -4.2436128 -4.2875242][-4.2821856 -4.2501369 -4.2239261 -4.1959291 -4.1668334 -4.103694 -4.0235262 -4.0272384 -4.0731711 -4.1233277 -4.1601887 -4.2026858 -4.2469997 -4.2852168 -4.3104234][-4.2890844 -4.2698503 -4.2545805 -4.242137 -4.2358356 -4.2088985 -4.17044 -4.1783252 -4.2016525 -4.2308431 -4.2498312 -4.2717538 -4.29638 -4.316473 -4.3298106][-4.2958207 -4.284368 -4.2740855 -4.268033 -4.2742252 -4.2699637 -4.2521505 -4.2615805 -4.2764506 -4.2944541 -4.3033524 -4.3113737 -4.3221607 -4.3314877 -4.3394694][-4.310102 -4.3009353 -4.2934637 -4.2902932 -4.2985721 -4.3022494 -4.29482 -4.3051791 -4.315526 -4.3236127 -4.325 -4.3251748 -4.3282609 -4.332375 -4.3369408][-4.3222213 -4.3122091 -4.3047748 -4.3001842 -4.3057747 -4.3089175 -4.3043575 -4.3117037 -4.3194618 -4.3233762 -4.3254871 -4.3279977 -4.3301086 -4.3299847 -4.3290963]]...]
INFO - root - 2017-12-06 06:49:09.032921: step 5510, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:03m:37s remains)
INFO - root - 2017-12-06 06:49:15.493687: step 5520, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:30m:48s remains)
INFO - root - 2017-12-06 06:49:21.839754: step 5530, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:59m:37s remains)
INFO - root - 2017-12-06 06:49:28.126052: step 5540, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:31m:35s remains)
INFO - root - 2017-12-06 06:49:34.555052: step 5550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:52m:30s remains)
INFO - root - 2017-12-06 06:49:40.906923: step 5560, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:04m:46s remains)
INFO - root - 2017-12-06 06:49:47.411841: step 5570, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 58h:36m:05s remains)
INFO - root - 2017-12-06 06:49:53.983192: step 5580, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 60h:38m:45s remains)
INFO - root - 2017-12-06 06:50:00.353471: step 5590, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:45m:32s remains)
INFO - root - 2017-12-06 06:50:06.663469: step 5600, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 57h:51m:39s remains)
2017-12-06 06:50:07.440483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1530495 -4.1868157 -4.2046442 -4.21573 -4.2279506 -4.2283993 -4.2000442 -4.1629996 -4.1500831 -4.1630254 -4.1947646 -4.2281022 -4.2620997 -4.2907076 -4.3085146][-4.181613 -4.2168651 -4.2316136 -4.2432971 -4.2575283 -4.2571115 -4.2261677 -4.1829486 -4.1716247 -4.1961317 -4.2341619 -4.2677431 -4.2972636 -4.3199911 -4.3321476][-4.1934996 -4.2288728 -4.24411 -4.256134 -4.2702403 -4.2626872 -4.2178283 -4.1632762 -4.1576223 -4.1986189 -4.2510047 -4.2910986 -4.322001 -4.3436165 -4.3541331][-4.1908317 -4.2205963 -4.2365727 -4.2462845 -4.2581377 -4.2370353 -4.1727352 -4.1084051 -4.1154132 -4.1755953 -4.2422538 -4.2935491 -4.3317008 -4.3516717 -4.3589077][-4.1875687 -4.2054958 -4.2184873 -4.2244911 -4.2339048 -4.2024045 -4.113389 -4.0354891 -4.0547919 -4.1331768 -4.2142329 -4.276957 -4.3222547 -4.3434153 -4.3500504][-4.1813741 -4.190115 -4.1973181 -4.2007084 -4.2058496 -4.1646762 -4.0546312 -3.9603052 -3.9890451 -4.0877666 -4.1841669 -4.2593327 -4.3105145 -4.3349295 -4.3425393][-4.1843729 -4.1866007 -4.1859097 -4.1825113 -4.1728 -4.1181784 -3.9912477 -3.88216 -3.9212179 -4.0397792 -4.1486998 -4.2352347 -4.2922735 -4.3221092 -4.3328772][-4.196218 -4.1973295 -4.1916332 -4.1806369 -4.1552176 -4.0874968 -3.9551115 -3.8464389 -3.8914006 -4.015378 -4.1234722 -4.2084537 -4.2655349 -4.2988873 -4.3131833][-4.2071438 -4.2118211 -4.2067356 -4.1955271 -4.1627531 -4.091989 -3.97172 -3.8799572 -3.9246149 -4.0388808 -4.1338868 -4.2078133 -4.2590919 -4.2924528 -4.3054423][-4.2233272 -4.2326608 -4.2318435 -4.2255225 -4.1938438 -4.1268282 -4.0200152 -3.9413748 -3.9722731 -4.0705352 -4.1571932 -4.2239652 -4.2687893 -4.2981009 -4.304245][-4.243896 -4.2546406 -4.2581291 -4.2564769 -4.2247663 -4.1595588 -4.0607214 -3.9834971 -3.9989042 -4.0868778 -4.1724329 -4.2364469 -4.2757654 -4.2980251 -4.2989588][-4.25784 -4.2653003 -4.2718387 -4.2701564 -4.2360187 -4.1721163 -4.0821052 -4.0040336 -4.0086107 -4.0909219 -4.1777034 -4.2431107 -4.27616 -4.2928238 -4.2896485][-4.2632985 -4.2677193 -4.2752371 -4.2737184 -4.2413077 -4.1818986 -4.1016512 -4.0262003 -4.0228877 -4.0945916 -4.1757922 -4.2386918 -4.2669559 -4.2819586 -4.2771673][-4.2665839 -4.26815 -4.2770162 -4.2781482 -4.2533507 -4.2005992 -4.128202 -4.0563512 -4.0472703 -4.105968 -4.1735463 -4.2257538 -4.2490363 -4.2635608 -4.2603297][-4.2669659 -4.2654958 -4.2738953 -4.2776966 -4.2638507 -4.2162595 -4.1497955 -4.0837145 -4.07078 -4.1204238 -4.1703238 -4.2057657 -4.2235065 -4.2375832 -4.2393527]]...]
INFO - root - 2017-12-06 06:50:13.908119: step 5610, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 55h:44m:33s remains)
INFO - root - 2017-12-06 06:50:20.313775: step 5620, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:58m:41s remains)
INFO - root - 2017-12-06 06:50:26.735458: step 5630, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.635 sec/batch; 57h:40m:46s remains)
INFO - root - 2017-12-06 06:50:33.065416: step 5640, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:40m:56s remains)
INFO - root - 2017-12-06 06:50:39.510708: step 5650, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:19m:23s remains)
INFO - root - 2017-12-06 06:50:45.856461: step 5660, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:44m:15s remains)
INFO - root - 2017-12-06 06:50:52.316618: step 5670, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 60h:03m:18s remains)
INFO - root - 2017-12-06 06:50:58.728397: step 5680, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:30m:41s remains)
INFO - root - 2017-12-06 06:51:05.180466: step 5690, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 57h:21m:47s remains)
INFO - root - 2017-12-06 06:51:11.452077: step 5700, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 59h:13m:27s remains)
2017-12-06 06:51:11.652768: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:51:11.652801: E tensorflow/core/util/events_writer.cc:131] Failed to flush 8 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:51:12.154235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2348604 -4.2044988 -4.1744394 -4.1500874 -4.1383786 -4.1385121 -4.1252809 -4.1138325 -4.1058683 -4.1016889 -4.094625 -4.0928292 -4.0780063 -4.0547667 -4.0503163][-4.2317581 -4.1935959 -4.1491408 -4.1096525 -4.0860538 -4.082448 -4.0678468 -4.0533571 -4.03816 -4.0303984 -4.0295944 -4.0331049 -4.0184841 -3.9943678 -3.9933019][-4.2350154 -4.1915817 -4.1375322 -4.088891 -4.0590878 -4.0545816 -4.0432386 -4.0285029 -4.0108962 -4.0014529 -4.0041256 -4.0097904 -3.9954772 -3.9744356 -3.9735589][-4.2407131 -4.1994505 -4.1484108 -4.1029172 -4.0761375 -4.0736375 -4.0680647 -4.0552144 -4.0400057 -4.0337725 -4.0348969 -4.0327644 -4.0153561 -3.9950609 -3.9900422][-4.2464142 -4.2145624 -4.1784029 -4.1482787 -4.1325607 -4.1343794 -4.136127 -4.1287103 -4.1175919 -4.1134567 -4.1106315 -4.099679 -4.07814 -4.0563984 -4.0472531][-4.25019 -4.2301159 -4.2072582 -4.1906924 -4.1846962 -4.19071 -4.1985455 -4.1973953 -4.1916666 -4.1868291 -4.1810579 -4.1692791 -4.1499619 -4.1295805 -4.1213045][-4.2494574 -4.2363963 -4.2187071 -4.2069974 -4.2023463 -4.2076 -4.217351 -4.2221885 -4.2228894 -4.2200904 -4.2168856 -4.2138886 -4.2050266 -4.1943583 -4.1915245][-4.2438045 -4.2309256 -4.211154 -4.1974506 -4.1905255 -4.1929431 -4.2017689 -4.2085195 -4.2120171 -4.2119684 -4.2154021 -4.2249489 -4.2299757 -4.2329059 -4.2374372][-4.2340584 -4.2160673 -4.19142 -4.1734667 -4.1626105 -4.1631155 -4.1700068 -4.173358 -4.1745272 -4.1763721 -4.1870861 -4.2084432 -4.2272072 -4.241981 -4.2529917][-4.2241669 -4.2015028 -4.1733441 -4.1519213 -4.1384273 -4.1377997 -4.14058 -4.1378717 -4.136867 -4.1412287 -4.1576424 -4.1862578 -4.2138114 -4.2358484 -4.2516079][-4.2184348 -4.1946335 -4.1675644 -4.1443043 -4.1283927 -4.1264539 -4.1269879 -4.1210346 -4.120398 -4.1282635 -4.1477647 -4.1789374 -4.2090015 -4.2329483 -4.2495785][-4.217772 -4.196991 -4.1743979 -4.151454 -4.1343684 -4.1319323 -4.134944 -4.1314893 -4.133636 -4.1432123 -4.1615882 -4.1902828 -4.2177916 -4.2385883 -4.2519121][-4.2226906 -4.2057705 -4.1885195 -4.1689053 -4.1545119 -4.1536937 -4.1616449 -4.1632719 -4.1677852 -4.1767392 -4.19195 -4.2143383 -4.2348051 -4.2488222 -4.2559977][-4.2328682 -4.2186351 -4.2059407 -4.1919088 -4.1828151 -4.1850386 -4.1986637 -4.2051444 -4.2114205 -4.218833 -4.22873 -4.2428131 -4.2548137 -4.2611938 -4.2624149][-4.2458444 -4.2347422 -4.2246261 -4.2164359 -4.2131066 -4.2190332 -4.2359161 -4.2454443 -4.2524338 -4.2572069 -4.2619681 -4.2687225 -4.273756 -4.2739739 -4.271297]]...]
INFO - root - 2017-12-06 06:51:18.522901: step 5710, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:00m:43s remains)
INFO - root - 2017-12-06 06:51:24.956800: step 5720, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 59h:26m:02s remains)
INFO - root - 2017-12-06 06:51:31.374108: step 5730, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:43m:48s remains)
INFO - root - 2017-12-06 06:51:37.813394: step 5740, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 58h:05m:09s remains)
INFO - root - 2017-12-06 06:51:44.032874: step 5750, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 59h:09m:44s remains)
INFO - root - 2017-12-06 06:51:50.550938: step 5760, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.639 sec/batch; 58h:00m:51s remains)
INFO - root - 2017-12-06 06:51:56.855023: step 5770, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:24m:46s remains)
INFO - root - 2017-12-06 06:52:03.291926: step 5780, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 58h:14m:54s remains)
INFO - root - 2017-12-06 06:52:09.771180: step 5790, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:44m:02s remains)
INFO - root - 2017-12-06 06:52:16.005012: step 5800, loss = 2.07, batch loss = 2.01 (13.4 examples/sec; 0.599 sec/batch; 54h:21m:53s remains)
2017-12-06 06:52:16.648452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1897984 -4.1897578 -4.1888304 -4.1950212 -4.2149968 -4.2379432 -4.2567625 -4.2645807 -4.2558079 -4.2358246 -4.1976776 -4.1592965 -4.1534462 -4.1709828 -4.1861615][-4.160852 -4.1800451 -4.1982121 -4.2143111 -4.2360539 -4.2531595 -4.2636666 -4.2622528 -4.2464643 -4.2271786 -4.1999369 -4.1701207 -4.1649895 -4.180562 -4.195827][-4.1500554 -4.1818037 -4.2180281 -4.2410932 -4.2547522 -4.2538009 -4.2497625 -4.2392631 -4.224462 -4.2139482 -4.202179 -4.1861267 -4.1790872 -4.1885786 -4.2027731][-4.1506972 -4.1842608 -4.2252688 -4.2440381 -4.2434926 -4.2270174 -4.2172608 -4.2110791 -4.2016072 -4.1977773 -4.193099 -4.1862988 -4.1793356 -4.18469 -4.1931338][-4.136951 -4.1673784 -4.2007289 -4.2068372 -4.1872673 -4.1548223 -4.1443033 -4.1504955 -4.1521878 -4.1535411 -4.1550488 -4.1626024 -4.1616187 -4.1611881 -4.1570754][-4.1033983 -4.1240611 -4.141871 -4.1292033 -4.0917726 -4.04405 -4.0298066 -4.052608 -4.07096 -4.08289 -4.0962715 -4.1220732 -4.1372108 -4.1373367 -4.1225605][-4.087904 -4.0836129 -4.0756869 -4.0394444 -3.9808674 -3.9087965 -3.8724339 -3.9116669 -3.9674811 -4.0057082 -4.0377941 -4.0838704 -4.1219959 -4.1364632 -4.1219831][-4.1008034 -4.0699372 -4.0319791 -3.9730737 -3.8969471 -3.8029685 -3.7210641 -3.7509937 -3.8521183 -3.9286497 -3.9827762 -4.0484586 -4.1067724 -4.1336594 -4.1317968][-4.138176 -4.0995979 -4.0507007 -3.9876189 -3.9225502 -3.8470788 -3.7598724 -3.7584708 -3.8515542 -3.9365904 -3.9927003 -4.0529146 -4.1086617 -4.141336 -4.1518288][-4.1890378 -4.1567183 -4.1187449 -4.0715547 -4.0281572 -3.9872646 -3.9330018 -3.9184318 -3.9724352 -4.034821 -4.0766959 -4.1194439 -4.1541138 -4.1748905 -4.1809011][-4.2296104 -4.2077093 -4.1864519 -4.1580286 -4.1277866 -4.1078572 -4.0845294 -4.0768018 -4.10854 -4.1543832 -4.1844144 -4.2127509 -4.2296648 -4.2371526 -4.2306504][-4.2599483 -4.2442479 -4.2344875 -4.2239089 -4.2101941 -4.20599 -4.2025771 -4.2001595 -4.2154212 -4.2449851 -4.26818 -4.2857995 -4.2936525 -4.2922435 -4.2780738][-4.2818661 -4.2709389 -4.2666106 -4.2663026 -4.2651935 -4.2697806 -4.2752857 -4.2771254 -4.2833738 -4.29716 -4.3113003 -4.3211589 -4.3234596 -4.3207479 -4.3087463][-4.2967563 -4.290832 -4.2873497 -4.287497 -4.2898078 -4.295742 -4.30417 -4.3085885 -4.31121 -4.3164039 -4.3216467 -4.3244486 -4.323278 -4.32296 -4.3182755][-4.3032608 -4.3001022 -4.2985725 -4.2988563 -4.3012919 -4.3060017 -4.3124857 -4.3184056 -4.3210359 -4.3220906 -4.3220243 -4.3204551 -4.3172703 -4.3160362 -4.3145308]]...]
INFO - root - 2017-12-06 06:52:23.127931: step 5810, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 57h:19m:09s remains)
INFO - root - 2017-12-06 06:52:29.389254: step 5820, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.613 sec/batch; 55h:36m:07s remains)
INFO - root - 2017-12-06 06:52:35.925197: step 5830, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:33m:22s remains)
INFO - root - 2017-12-06 06:52:42.304355: step 5840, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:31m:21s remains)
INFO - root - 2017-12-06 06:52:48.646101: step 5850, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.605 sec/batch; 54h:55m:20s remains)
INFO - root - 2017-12-06 06:52:54.824116: step 5860, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 56h:30m:27s remains)
INFO - root - 2017-12-06 06:53:01.229754: step 5870, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 58h:44m:57s remains)
INFO - root - 2017-12-06 06:53:07.637080: step 5880, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 57h:30m:00s remains)
INFO - root - 2017-12-06 06:53:14.061585: step 5890, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:10m:02s remains)
INFO - root - 2017-12-06 06:53:20.493470: step 5900, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:08m:04s remains)
2017-12-06 06:53:20.707882: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:53:20.708008: E tensorflow/core/util/events_writer.cc:131] Failed to flush 10 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:53:21.142619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2514534 -4.2295403 -4.1967988 -4.1514139 -4.1053958 -4.0779848 -4.0538321 -4.0118318 -3.9559793 -3.9145038 -3.8914099 -3.9232607 -4.0073209 -4.0959768 -4.1800957][-4.2250819 -4.2057648 -4.1703134 -4.1254182 -4.0831389 -4.0616622 -4.0454473 -4.0155349 -3.9786267 -3.9482975 -3.928242 -3.9613004 -4.0428104 -4.1288977 -4.2102165][-4.203022 -4.1952267 -4.167 -4.1283612 -4.0943661 -4.0806007 -4.0715251 -4.0521684 -4.0313816 -4.0137515 -3.999258 -4.0263634 -4.0962548 -4.1727428 -4.2427635][-4.18088 -4.1873293 -4.1695809 -4.14012 -4.1162181 -4.1103725 -4.1079135 -4.0944133 -4.0892653 -4.0859289 -4.0771008 -4.0962949 -4.1511488 -4.2132187 -4.2695904][-4.1640968 -4.1851358 -4.181006 -4.161231 -4.1431756 -4.1324081 -4.1238718 -4.1149464 -4.1271157 -4.1365204 -4.1341152 -4.1464581 -4.18662 -4.2370162 -4.2850847][-4.168334 -4.1943326 -4.2015786 -4.1879063 -4.1620526 -4.1322246 -4.1113653 -4.1116567 -4.1411071 -4.1606169 -4.1653614 -4.1751618 -4.2039914 -4.24581 -4.2901487][-4.1803064 -4.204587 -4.219943 -4.2064424 -4.1637697 -4.1099653 -4.0782537 -4.094882 -4.1460104 -4.1770029 -4.1848435 -4.1885791 -4.2064652 -4.2441449 -4.2893414][-4.1882749 -4.2096643 -4.2278504 -4.2125788 -4.1546607 -4.0840611 -4.0550165 -4.0970988 -4.1671352 -4.204299 -4.2068949 -4.1984086 -4.2082982 -4.2439008 -4.2901015][-4.1873794 -4.2105637 -4.2285705 -4.2081385 -4.1397939 -4.0657172 -4.0537777 -4.1198893 -4.1987376 -4.2337251 -4.2300925 -4.2142596 -4.2202992 -4.2537708 -4.2967577][-4.1812782 -4.2096906 -4.2281504 -4.2030315 -4.1298518 -4.0683513 -4.0790081 -4.1546965 -4.2280297 -4.2566128 -4.247941 -4.2303185 -4.2366037 -4.2676105 -4.30514][-4.1802578 -4.2147384 -4.2326536 -4.2017765 -4.1338367 -4.0940933 -4.1211371 -4.1919971 -4.2539492 -4.2758079 -4.2643728 -4.2472005 -4.2529492 -4.2796435 -4.3118529][-4.1771097 -4.2209048 -4.24093 -4.2096105 -4.1515064 -4.1338272 -4.1665783 -4.2230225 -4.2725773 -4.2875857 -4.2754021 -4.2587075 -4.2634606 -4.28629 -4.314693][-4.1819348 -4.231461 -4.2526379 -4.2229252 -4.1777472 -4.1754107 -4.2028375 -4.2435346 -4.2832222 -4.2962632 -4.2831335 -4.2638354 -4.2647839 -4.2841158 -4.3106837][-4.1879845 -4.2411532 -4.2627521 -4.237803 -4.2045946 -4.2057223 -4.2217574 -4.2512016 -4.2861667 -4.2976532 -4.2804737 -4.2566271 -4.2543449 -4.2725749 -4.2997785][-4.188612 -4.2477374 -4.2732687 -4.2566786 -4.2306828 -4.2245669 -4.2316155 -4.2551751 -4.2866678 -4.2933478 -4.2692466 -4.2408257 -4.236393 -4.2552772 -4.2848182]]...]
INFO - root - 2017-12-06 06:53:27.439656: step 5910, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:33m:17s remains)
INFO - root - 2017-12-06 06:53:33.834617: step 5920, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:40m:52s remains)
INFO - root - 2017-12-06 06:53:40.207397: step 5930, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 57h:02m:53s remains)
INFO - root - 2017-12-06 06:53:46.611711: step 5940, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 60h:42m:32s remains)
INFO - root - 2017-12-06 06:53:53.127402: step 5950, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:04m:43s remains)
INFO - root - 2017-12-06 06:53:59.308235: step 5960, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 56h:45m:43s remains)
INFO - root - 2017-12-06 06:54:05.602474: step 5970, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:07m:17s remains)
INFO - root - 2017-12-06 06:54:12.134410: step 5980, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 60h:12m:16s remains)
INFO - root - 2017-12-06 06:54:18.470963: step 5990, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:45m:37s remains)
INFO - root - 2017-12-06 06:54:24.955709: step 6000, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 59h:15m:38s remains)
2017-12-06 06:54:25.605607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1862254 -4.187602 -4.1836805 -4.1692195 -4.1494193 -4.1339865 -4.1337595 -4.1430321 -4.1601667 -4.1745543 -4.1760592 -4.1763048 -4.1774569 -4.1870961 -4.2114878][-4.1771793 -4.1719155 -4.1676364 -4.1569357 -4.1413054 -4.1288161 -4.1280432 -4.1398778 -4.157865 -4.1713543 -4.1722879 -4.1698532 -4.1679373 -4.1751833 -4.2017589][-4.159761 -4.1514521 -4.1409888 -4.13522 -4.1290269 -4.1186438 -4.1185684 -4.1332364 -4.1551728 -4.1786423 -4.18848 -4.1905432 -4.1848416 -4.1890068 -4.2111382][-4.1603785 -4.14519 -4.1318169 -4.1258397 -4.1198878 -4.1065416 -4.1035528 -4.1108513 -4.1351895 -4.1738629 -4.1992354 -4.20856 -4.2054915 -4.2061787 -4.2166924][-4.1701517 -4.1453695 -4.1287351 -4.1121082 -4.0926552 -4.0710154 -4.0535383 -4.0405579 -4.0598683 -4.1140985 -4.1606541 -4.184195 -4.1945357 -4.2013197 -4.2079434][-4.1919312 -4.1592083 -4.1274014 -4.0894237 -4.0465946 -4.0013952 -3.9465003 -3.8891506 -3.8925252 -3.9758527 -4.0622535 -4.1180182 -4.154479 -4.17977 -4.1962438][-4.197475 -4.1646442 -4.1214428 -4.0637369 -3.9936521 -3.9145377 -3.811343 -3.6855025 -3.6623416 -3.7980618 -3.9451151 -4.0432367 -4.1148982 -4.16269 -4.1924219][-4.1819954 -4.1546741 -4.1093583 -4.047307 -3.9598932 -3.8564894 -3.7155223 -3.5296311 -3.4784646 -3.6620498 -3.8593764 -3.991014 -4.0909767 -4.1596823 -4.2020092][-4.1677132 -4.1521606 -4.1181788 -4.0738411 -3.9996905 -3.9065542 -3.7843513 -3.6209924 -3.5551164 -3.696594 -3.8782134 -4.0041037 -4.1025329 -4.173213 -4.2142553][-4.1752954 -4.1730223 -4.1511321 -4.1301284 -4.0886321 -4.0268846 -3.951828 -3.8432705 -3.7765369 -3.8428943 -3.9648869 -4.0586882 -4.1338525 -4.1867509 -4.2149096][-4.1777606 -4.1874733 -4.1730447 -4.1639419 -4.15159 -4.1172738 -4.0761967 -4.0075078 -3.9486313 -3.9638042 -4.0312376 -4.0947084 -4.1463203 -4.1856308 -4.2058372][-4.1490335 -4.1603661 -4.1517444 -4.1515489 -4.1615028 -4.1504722 -4.1298213 -4.0849648 -4.0396323 -4.0308046 -4.0643497 -4.1039543 -4.1389132 -4.1688123 -4.1853485][-4.1028247 -4.1116691 -4.1062922 -4.1192679 -4.1424327 -4.151731 -4.1419444 -4.1082454 -4.0818253 -4.071672 -4.0844336 -4.1056414 -4.1268969 -4.1495023 -4.1591172][-4.0709181 -4.0773883 -4.0793 -4.105032 -4.13632 -4.14915 -4.1431737 -4.1189928 -4.1027079 -4.0915384 -4.0897517 -4.0979576 -4.1063452 -4.1113777 -4.110435][-4.0819445 -4.0780082 -4.0799613 -4.1008558 -4.1200976 -4.1289907 -4.1234684 -4.1048512 -4.0922437 -4.0783687 -4.0667815 -4.0730677 -4.0771809 -4.0696011 -4.068253]]...]
INFO - root - 2017-12-06 06:54:31.854775: step 6010, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 59h:20m:07s remains)
INFO - root - 2017-12-06 06:54:38.273768: step 6020, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 58h:00m:15s remains)
INFO - root - 2017-12-06 06:54:44.693566: step 6030, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 56h:59m:56s remains)
INFO - root - 2017-12-06 06:54:51.045055: step 6040, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 59h:08m:33s remains)
INFO - root - 2017-12-06 06:54:57.514061: step 6050, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:02m:06s remains)
INFO - root - 2017-12-06 06:55:03.985455: step 6060, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 58h:37m:15s remains)
INFO - root - 2017-12-06 06:55:10.226717: step 6070, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.610 sec/batch; 55h:21m:08s remains)
INFO - root - 2017-12-06 06:55:16.613261: step 6080, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 58h:40m:19s remains)
INFO - root - 2017-12-06 06:55:23.016655: step 6090, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:45m:21s remains)
INFO - root - 2017-12-06 06:55:29.427119: step 6100, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:49m:11s remains)
2017-12-06 06:55:29.646001: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:55:29.646027: E tensorflow/core/util/events_writer.cc:131] Failed to flush 12 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:55:30.070004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2484884 -4.2485704 -4.249671 -4.2504759 -4.2493844 -4.2463417 -4.2425513 -4.2399693 -4.2401514 -4.2429118 -4.248445 -4.2547007 -4.2584357 -4.259737 -4.260211][-4.2531772 -4.2516513 -4.2500072 -4.2475834 -4.2430458 -4.2364297 -4.2299066 -4.2260041 -4.2270465 -4.2325821 -4.2409635 -4.2498784 -4.2565069 -4.2601991 -4.2618117][-4.2542725 -4.2491369 -4.2430038 -4.2356296 -4.2254796 -4.2141371 -4.2044735 -4.2006497 -4.2040558 -4.2120671 -4.2239361 -4.2363281 -4.2468028 -4.2556505 -4.26069][-4.254312 -4.2448497 -4.2330356 -4.2178507 -4.1972542 -4.1761551 -4.1609645 -4.1572595 -4.1632752 -4.1765213 -4.1931734 -4.2086749 -4.2229848 -4.2374558 -4.2497454][-4.2499118 -4.2336373 -4.2125373 -4.1844249 -4.1491027 -4.1142359 -4.0902619 -4.0842943 -4.0974817 -4.1241183 -4.151114 -4.172195 -4.1920056 -4.2125945 -4.2310643][-4.2380872 -4.2116761 -4.1766291 -4.1332245 -4.0821233 -4.0321717 -3.9980817 -3.9871268 -4.0090995 -4.0546041 -4.099453 -4.1330671 -4.1624427 -4.1890321 -4.2106261][-4.2191172 -4.1820507 -4.1314573 -4.0717349 -4.0052638 -3.9396932 -3.8928719 -3.8729463 -3.8994682 -3.9629993 -4.0311189 -4.0832148 -4.1264935 -4.15978 -4.1849451][-4.1924438 -4.1463838 -4.0844092 -4.0140076 -3.9388573 -3.8666179 -3.8165467 -3.8009217 -3.8374102 -3.917474 -4.0015306 -4.0652905 -4.1151004 -4.1481104 -4.1699691][-4.168829 -4.1246119 -4.068491 -4.0046406 -3.9385972 -3.8754776 -3.8372955 -3.8400869 -3.8883233 -3.9696026 -4.0461969 -4.1017694 -4.1424837 -4.1642323 -4.174902][-4.1520314 -4.1195512 -4.081645 -4.0389624 -3.9962239 -3.9553466 -3.935771 -3.9529562 -4.0020547 -4.0677085 -4.1234522 -4.1609955 -4.1847482 -4.1912389 -4.1888528][-4.1412697 -4.1225839 -4.1072774 -4.0896854 -4.0705738 -4.0501661 -4.0443506 -4.0665522 -4.1080604 -4.1533551 -4.1865535 -4.21072 -4.2238178 -4.2206507 -4.2095008][-4.1382804 -4.1312346 -4.132071 -4.1336684 -4.1334305 -4.1271629 -4.1298246 -4.1520691 -4.1812811 -4.2079396 -4.2226067 -4.2329855 -4.2370377 -4.2291021 -4.2161617][-4.111115 -4.1122308 -4.12509 -4.1397071 -4.1498127 -4.1519818 -4.1611319 -4.1791949 -4.1971045 -4.2091055 -4.2123556 -4.2129192 -4.211256 -4.2050381 -4.195684][-4.0462394 -4.0535254 -4.0776577 -4.1018925 -4.1175032 -4.1240716 -4.1376877 -4.1545658 -4.1626987 -4.1618953 -4.156332 -4.1529493 -4.1544237 -4.1548147 -4.1518154][-3.9611387 -3.9707444 -4.0006442 -4.0293489 -4.0459919 -4.0505576 -4.0658445 -4.0883 -4.0946503 -4.0875254 -4.0807476 -4.0819869 -4.0923958 -4.1030331 -4.1094146]]...]
INFO - root - 2017-12-06 06:55:36.417551: step 6110, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:48m:21s remains)
INFO - root - 2017-12-06 06:55:42.894805: step 6120, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.635 sec/batch; 57h:32m:05s remains)
INFO - root - 2017-12-06 06:55:49.257108: step 6130, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:51m:58s remains)
INFO - root - 2017-12-06 06:55:55.615962: step 6140, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 58h:03m:12s remains)
INFO - root - 2017-12-06 06:56:02.072751: step 6150, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 57h:06m:26s remains)
INFO - root - 2017-12-06 06:56:08.498870: step 6160, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 56h:22m:16s remains)
INFO - root - 2017-12-06 06:56:14.751133: step 6170, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 0.524 sec/batch; 47h:32m:16s remains)
INFO - root - 2017-12-06 06:56:21.066825: step 6180, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 58h:25m:00s remains)
INFO - root - 2017-12-06 06:56:27.528035: step 6190, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 57h:33m:33s remains)
INFO - root - 2017-12-06 06:56:33.872152: step 6200, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 56h:58m:59s remains)
2017-12-06 06:56:34.467042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21364 -4.2042756 -4.2122231 -4.2304058 -4.2367272 -4.2202296 -4.185688 -4.1431532 -4.1058025 -4.0988755 -4.1308765 -4.167954 -4.1979208 -4.2278709 -4.2701716][-4.2238607 -4.2194648 -4.2315116 -4.247673 -4.2475524 -4.2230644 -4.1798029 -4.1338248 -4.1035271 -4.1019206 -4.1319675 -4.1741276 -4.215539 -4.2585564 -4.3030005][-4.2362461 -4.2413082 -4.257885 -4.2718368 -4.2633467 -4.227457 -4.1739755 -4.120173 -4.0912633 -4.0977292 -4.1324658 -4.1822991 -4.2308874 -4.278873 -4.3188257][-4.2531896 -4.2659307 -4.2848783 -4.2935987 -4.2732258 -4.2253227 -4.1573281 -4.0883455 -4.0576763 -4.0764527 -4.1251945 -4.188518 -4.2472582 -4.2954488 -4.3258715][-4.27416 -4.2914238 -4.3073506 -4.304687 -4.2699471 -4.2082152 -4.118917 -4.0278482 -4.0017247 -4.048749 -4.1229839 -4.2030296 -4.2698107 -4.3138633 -4.3287635][-4.293232 -4.3073835 -4.3160448 -4.2998567 -4.2505069 -4.1688461 -4.05713 -3.9572141 -3.9551742 -4.0423355 -4.1424203 -4.2301307 -4.2927361 -4.32463 -4.3209615][-4.2936244 -4.3032141 -4.3049331 -4.27595 -4.2105041 -4.108748 -3.9891727 -3.90917 -3.9471416 -4.0634851 -4.1691456 -4.2512012 -4.2998548 -4.3154025 -4.2960453][-4.2712708 -4.2784815 -4.2761445 -4.2380691 -4.1623764 -4.0582619 -3.9636979 -3.9366283 -4.0033779 -4.1111903 -4.2007747 -4.2697425 -4.3029122 -4.3015437 -4.2688389][-4.23921 -4.2524 -4.2523808 -4.2116437 -4.13231 -4.038115 -3.9787729 -3.994559 -4.0684571 -4.1571193 -4.2322679 -4.2870321 -4.3029552 -4.2839832 -4.2417245][-4.2106709 -4.2316036 -4.2332692 -4.18902 -4.1111832 -4.0373335 -4.0116434 -4.0536084 -4.1249657 -4.1958766 -4.257668 -4.2986994 -4.2997088 -4.2698503 -4.2199435][-4.1892333 -4.2111349 -4.2062497 -4.1604691 -4.0938754 -4.0469661 -4.0507793 -4.1089458 -4.1748576 -4.2276998 -4.2764297 -4.3050861 -4.2978649 -4.262517 -4.2055807][-4.1790891 -4.1980653 -4.1858163 -4.1433644 -4.09549 -4.0721545 -4.0915833 -4.1497493 -4.2064996 -4.24546 -4.2847576 -4.3044944 -4.2887568 -4.2442775 -4.1823134][-4.18715 -4.1959691 -4.1755672 -4.1398082 -4.1110892 -4.1047711 -4.1287479 -4.1792541 -4.2268891 -4.2614465 -4.2936764 -4.3045321 -4.2808185 -4.2272286 -4.1589918][-4.1968102 -4.2001438 -4.1795774 -4.15256 -4.1397767 -4.1411533 -4.1626568 -4.2032337 -4.2399011 -4.2677817 -4.2897677 -4.2934546 -4.2640195 -4.2061057 -4.1361141][-4.2122564 -4.2200475 -4.2061362 -4.185863 -4.1771379 -4.1768231 -4.1921239 -4.2200356 -4.2443881 -4.26277 -4.2742496 -4.2721715 -4.2410626 -4.1831555 -4.1202583]]...]
INFO - root - 2017-12-06 06:56:40.852022: step 6210, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 66h:18m:25s remains)
INFO - root - 2017-12-06 06:56:47.230907: step 6220, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:40m:22s remains)
INFO - root - 2017-12-06 06:56:53.601177: step 6230, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.604 sec/batch; 54h:45m:05s remains)
INFO - root - 2017-12-06 06:56:59.983592: step 6240, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:38m:23s remains)
INFO - root - 2017-12-06 06:57:06.374531: step 6250, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:14m:22s remains)
INFO - root - 2017-12-06 06:57:12.854187: step 6260, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 59h:13m:12s remains)
INFO - root - 2017-12-06 06:57:19.212699: step 6270, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.611 sec/batch; 55h:19m:57s remains)
INFO - root - 2017-12-06 06:57:25.483226: step 6280, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 60h:30m:17s remains)
INFO - root - 2017-12-06 06:57:31.931605: step 6290, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:51m:34s remains)
INFO - root - 2017-12-06 06:57:38.287154: step 6300, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 57h:43m:29s remains)
2017-12-06 06:57:38.492362: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:57:38.492400: E tensorflow/core/util/events_writer.cc:131] Failed to flush 14 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:57:38.943256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3268409 -4.2893 -4.23768 -4.1724939 -4.0987859 -4.060277 -4.0846114 -4.1288996 -4.147418 -4.1536126 -4.1598873 -4.1675386 -4.1754441 -4.1893954 -4.2076535][-4.3303251 -4.2925053 -4.2387114 -4.16614 -4.0802321 -4.0208406 -4.0384364 -4.1027279 -4.1397653 -4.1519527 -4.1565924 -4.1620016 -4.172195 -4.1918216 -4.2159495][-4.3352547 -4.3008647 -4.2491093 -4.1717114 -4.073534 -3.9903002 -3.9895887 -4.0692434 -4.1264648 -4.1468506 -4.1503825 -4.1534905 -4.1643581 -4.1898756 -4.2203817][-4.3385291 -4.3091254 -4.2619338 -4.1856756 -4.0843878 -3.9827201 -3.9566374 -4.0392008 -4.111968 -4.1404238 -4.1414304 -4.1383977 -4.1479712 -4.1782508 -4.2143211][-4.3380146 -4.3140483 -4.2732511 -4.2054191 -4.1094155 -3.9982679 -3.9463282 -4.0191307 -4.0991268 -4.1350045 -4.1377316 -4.1309562 -4.1376591 -4.1669536 -4.2023706][-4.3356972 -4.3170466 -4.2860546 -4.2306128 -4.1450505 -4.0321341 -3.9610729 -4.0107303 -4.0884881 -4.1303768 -4.1383762 -4.13064 -4.1331315 -4.1572657 -4.1858115][-4.3333712 -4.3190923 -4.2964368 -4.2511206 -4.1753421 -4.0633411 -3.9797592 -4.003603 -4.0723658 -4.1247239 -4.1427269 -4.1406269 -4.1375704 -4.1496091 -4.1648335][-4.3319392 -4.3199077 -4.3030205 -4.2672982 -4.2057886 -4.1046166 -4.0199656 -4.0221786 -4.072185 -4.1293192 -4.1603985 -4.1681294 -4.1627522 -4.1604953 -4.1589737][-4.3316588 -4.3197584 -4.3052235 -4.2789469 -4.2348843 -4.1523323 -4.0763111 -4.0636382 -4.0903382 -4.1387491 -4.17893 -4.1998715 -4.2004056 -4.1906037 -4.1764045][-4.3330207 -4.3212814 -4.3094168 -4.2898273 -4.2612796 -4.1993613 -4.1374693 -4.1199169 -4.1273661 -4.1601758 -4.2015357 -4.2336464 -4.243978 -4.2340579 -4.2126641][-4.33532 -4.3247972 -4.3146052 -4.2988234 -4.2815318 -4.2407289 -4.1924396 -4.1723294 -4.1660028 -4.1845613 -4.2248435 -4.2678294 -4.2897587 -4.2844429 -4.2605882][-4.338172 -4.3279362 -4.316977 -4.3028154 -4.293447 -4.2738218 -4.23977 -4.2185278 -4.2029214 -4.2119751 -4.2508497 -4.3014264 -4.3310981 -4.3283887 -4.3056469][-4.3423324 -4.3330054 -4.322969 -4.31098 -4.3055158 -4.2972765 -4.274632 -4.2534809 -4.235898 -4.2429352 -4.27799 -4.3278522 -4.3610177 -4.3609905 -4.3417335][-4.346293 -4.337605 -4.32784 -4.3176417 -4.3138518 -4.3120642 -4.299686 -4.2820911 -4.2652411 -4.27152 -4.29838 -4.3401461 -4.3703856 -4.3724203 -4.35883][-4.3465734 -4.3375363 -4.3285866 -4.3222737 -4.3226266 -4.3253551 -4.3220563 -4.308239 -4.2908521 -4.290657 -4.3055506 -4.3364029 -4.3625646 -4.3676071 -4.3605032]]...]
INFO - root - 2017-12-06 06:57:45.399560: step 6310, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:52m:30s remains)
INFO - root - 2017-12-06 06:57:51.691306: step 6320, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 59h:20m:42s remains)
INFO - root - 2017-12-06 06:57:58.234806: step 6330, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 58h:54m:58s remains)
INFO - root - 2017-12-06 06:58:04.580409: step 6340, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 57h:16m:21s remains)
INFO - root - 2017-12-06 06:58:11.093282: step 6350, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:27m:30s remains)
INFO - root - 2017-12-06 06:58:17.380684: step 6360, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 58h:19m:55s remains)
INFO - root - 2017-12-06 06:58:23.788475: step 6370, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:34m:21s remains)
INFO - root - 2017-12-06 06:58:30.249387: step 6380, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 58h:57m:10s remains)
INFO - root - 2017-12-06 06:58:36.505682: step 6390, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 57h:10m:22s remains)
INFO - root - 2017-12-06 06:58:42.926354: step 6400, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 58h:23m:27s remains)
2017-12-06 06:58:43.553633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1904368 -4.1843386 -4.1762524 -4.162993 -4.1598806 -4.1344161 -4.0971031 -4.0982776 -4.1399469 -4.1863213 -4.2165556 -4.2345481 -4.2472048 -4.2521696 -4.2601752][-4.157104 -4.1574979 -4.1577749 -4.1581473 -4.164793 -4.145709 -4.1048126 -4.0944109 -4.1321449 -4.1774344 -4.2117906 -4.2328334 -4.2464976 -4.2491679 -4.2553687][-4.1352139 -4.1472621 -4.1637659 -4.1720576 -4.1814346 -4.1620221 -4.1115 -4.0800819 -4.103363 -4.1456504 -4.1873889 -4.2145591 -4.233521 -4.2390146 -4.2477508][-4.1475487 -4.1650233 -4.1906939 -4.2008691 -4.2024732 -4.1747503 -4.1046844 -4.0523567 -4.067493 -4.1129413 -4.1639366 -4.1988645 -4.2232509 -4.2315135 -4.2413387][-4.1721292 -4.1920657 -4.2188377 -4.2261844 -4.2144141 -4.1723628 -4.0760121 -4.0072517 -4.0339279 -4.088594 -4.1472316 -4.1898341 -4.2191849 -4.2288756 -4.239213][-4.1858182 -4.2039676 -4.226438 -4.2285919 -4.2028761 -4.1432447 -4.0205379 -3.9395356 -3.9951758 -4.069418 -4.1381874 -4.1863122 -4.2172389 -4.2275124 -4.2382884][-4.1909637 -4.2033682 -4.2159443 -4.204524 -4.161891 -4.081183 -3.9300807 -3.843364 -3.9433429 -4.0473027 -4.1303768 -4.1864805 -4.2197871 -4.2292428 -4.2387238][-4.200757 -4.20619 -4.2040052 -4.1767406 -4.1184158 -4.0214763 -3.8650396 -3.7970796 -3.9364386 -4.0592227 -4.1437778 -4.2010312 -4.2345057 -4.2403779 -4.2440743][-4.2001266 -4.2085495 -4.2036347 -4.1758332 -4.1224389 -4.0447197 -3.9355907 -3.9006243 -4.0202541 -4.1187816 -4.1833081 -4.2293859 -4.2527728 -4.2510786 -4.2489471][-4.176455 -4.1934533 -4.2013774 -4.1893554 -4.153439 -4.0981188 -4.026093 -4.0098147 -4.1000919 -4.1694322 -4.2120261 -4.2443438 -4.2564774 -4.24932 -4.245441][-4.1458712 -4.1634932 -4.1858764 -4.1931677 -4.1804709 -4.1406789 -4.0830421 -4.0738811 -4.1422 -4.1913452 -4.2216396 -4.2456803 -4.2481632 -4.23607 -4.2339244][-4.1222682 -4.139019 -4.1713128 -4.1931334 -4.2017426 -4.1722245 -4.1179938 -4.1105456 -4.1678457 -4.2056293 -4.2282381 -4.2448359 -4.2416964 -4.2279649 -4.2264314][-4.1008205 -4.1150675 -4.1513214 -4.1811633 -4.2001023 -4.1756258 -4.1172042 -4.1124573 -4.1720667 -4.2069349 -4.2294259 -4.243412 -4.23928 -4.2281561 -4.2283945][-4.0978403 -4.1090465 -4.1372232 -4.158289 -4.1754303 -4.1546383 -4.095274 -4.0995159 -4.1683531 -4.2111664 -4.2353287 -4.2437 -4.2361803 -4.2271743 -4.2311482][-4.1265435 -4.134594 -4.1488218 -4.1556969 -4.166008 -4.150166 -4.0997925 -4.1140418 -4.1874371 -4.2338781 -4.2545042 -4.257206 -4.2456274 -4.2363939 -4.2415829]]...]
INFO - root - 2017-12-06 06:58:49.911325: step 6410, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 59h:18m:29s remains)
INFO - root - 2017-12-06 06:58:56.253390: step 6420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:30m:49s remains)
INFO - root - 2017-12-06 06:59:02.667585: step 6430, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.618 sec/batch; 55h:58m:28s remains)
INFO - root - 2017-12-06 06:59:09.074509: step 6440, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:03m:36s remains)
INFO - root - 2017-12-06 06:59:15.408229: step 6450, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 58h:14m:39s remains)
INFO - root - 2017-12-06 06:59:21.814944: step 6460, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:37m:58s remains)
INFO - root - 2017-12-06 06:59:28.260791: step 6470, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:53m:39s remains)
INFO - root - 2017-12-06 06:59:34.655779: step 6480, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 57h:52m:04s remains)
INFO - root - 2017-12-06 06:59:40.956879: step 6490, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:53m:14s remains)
INFO - root - 2017-12-06 06:59:47.333354: step 6500, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 58h:00m:37s remains)
2017-12-06 06:59:47.539912: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 06:59:47.539951: E tensorflow/core/util/events_writer.cc:131] Failed to flush 16 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 06:59:47.917523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3414435 -4.3619304 -4.3712149 -4.37269 -4.3521667 -4.3118205 -4.2631559 -4.216507 -4.1763887 -4.13351 -4.1117 -4.1286426 -4.1750512 -4.2257891 -4.2679243][-4.3486733 -4.3749647 -4.3832841 -4.3786249 -4.3462768 -4.2936239 -4.2358518 -4.1822848 -4.142067 -4.1132903 -4.1016979 -4.1189318 -4.1639681 -4.2204123 -4.2660065][-4.351665 -4.3788548 -4.3833165 -4.3703585 -4.32482 -4.263814 -4.1965322 -4.1338587 -4.0970063 -4.0819635 -4.0852284 -4.1058035 -4.1508093 -4.2150354 -4.2636552][-4.3540678 -4.3788142 -4.3798761 -4.35909 -4.300519 -4.228529 -4.149889 -4.0879431 -4.05055 -4.040688 -4.0542407 -4.0789351 -4.1353536 -4.2094173 -4.2629032][-4.3590174 -4.3838725 -4.3796153 -4.3468385 -4.2718258 -4.1846323 -4.1001272 -4.0357614 -3.995254 -3.9954586 -4.02384 -4.0626698 -4.1373014 -4.2176723 -4.2702918][-4.3693843 -4.3927093 -4.3800149 -4.3297672 -4.234416 -4.1318684 -4.0397153 -3.9649599 -3.9318867 -3.9557385 -4.0067468 -4.0693316 -4.1590023 -4.2384577 -4.2875614][-4.3769054 -4.39255 -4.3701091 -4.3043919 -4.1904769 -4.0638356 -3.946744 -3.8716202 -3.8738718 -3.93078 -4.0055952 -4.0918503 -4.1890292 -4.2617159 -4.30363][-4.3782654 -4.3795218 -4.3473539 -4.2705665 -4.1407294 -3.9803178 -3.8307514 -3.7697768 -3.8201909 -3.912729 -4.010118 -4.1164331 -4.2156653 -4.2769394 -4.3061604][-4.3733568 -4.3601789 -4.3162518 -4.2295408 -4.0957022 -3.9181383 -3.7555981 -3.7140257 -3.7939267 -3.9016311 -4.0099459 -4.1301751 -4.2247143 -4.2755003 -4.2897549][-4.3648295 -4.3381481 -4.2823324 -4.1887627 -4.0674782 -3.9116294 -3.7800837 -3.760499 -3.8395247 -3.9369388 -4.0366049 -4.1500874 -4.2315612 -4.2714338 -4.2788153][-4.3526306 -4.3176661 -4.2539268 -4.1648607 -4.0700531 -3.9625683 -3.8817077 -3.8775923 -3.9361119 -4.0073867 -4.0870061 -4.1788311 -4.2408342 -4.2673225 -4.2693076][-4.3370185 -4.3022523 -4.240376 -4.165576 -4.1005349 -4.0346112 -3.9914887 -3.9959881 -4.0383806 -4.0887861 -4.1446548 -4.2103662 -4.252264 -4.268744 -4.2691264][-4.325994 -4.2968321 -4.2463856 -4.1870127 -4.1427484 -4.0988808 -4.07307 -4.0836053 -4.1185155 -4.1558938 -4.1967154 -4.2431388 -4.2697434 -4.279973 -4.2794852][-4.3272634 -4.308857 -4.2746243 -4.2317696 -4.20071 -4.1695509 -4.1533241 -4.1631312 -4.1887975 -4.2169328 -4.2465739 -4.2784691 -4.2935486 -4.2984519 -4.2981753][-4.3365378 -4.3284082 -4.3099093 -4.282495 -4.261579 -4.243187 -4.2333918 -4.2391124 -4.2559414 -4.2736897 -4.2918158 -4.3093824 -4.3158422 -4.3170328 -4.316947]]...]
INFO - root - 2017-12-06 06:59:54.396974: step 6510, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 57h:39m:53s remains)
INFO - root - 2017-12-06 07:00:00.596695: step 6520, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 56h:29m:06s remains)
INFO - root - 2017-12-06 07:00:06.956790: step 6530, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:29m:49s remains)
INFO - root - 2017-12-06 07:00:13.417982: step 6540, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.629 sec/batch; 56h:57m:13s remains)
INFO - root - 2017-12-06 07:00:19.823262: step 6550, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 58h:14m:58s remains)
INFO - root - 2017-12-06 07:00:26.299602: step 6560, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:00m:05s remains)
INFO - root - 2017-12-06 07:00:32.742487: step 6570, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 58h:26m:01s remains)
INFO - root - 2017-12-06 07:00:39.271776: step 6580, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 58h:30m:55s remains)
INFO - root - 2017-12-06 07:00:45.689416: step 6590, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:01m:06s remains)
INFO - root - 2017-12-06 07:00:51.679613: step 6600, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:03m:31s remains)
2017-12-06 07:00:52.291628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3321939 -4.3168864 -4.2880983 -4.2516913 -4.2139235 -4.1834054 -4.1599159 -4.1317687 -4.1027822 -4.0899534 -4.0874038 -4.09985 -4.1226764 -4.1526561 -4.16916][-4.3378396 -4.32555 -4.298398 -4.26294 -4.223042 -4.1892939 -4.1629357 -4.1378908 -4.114471 -4.10421 -4.103435 -4.11436 -4.14372 -4.1737642 -4.185883][-4.33873 -4.3264709 -4.298862 -4.2642789 -4.2243276 -4.1917415 -4.1642747 -4.1387496 -4.1256928 -4.1229706 -4.1237841 -4.1303396 -4.1568236 -4.1796255 -4.1866231][-4.3361826 -4.3206625 -4.2889724 -4.2494755 -4.2098775 -4.182775 -4.1544862 -4.1254077 -4.1151514 -4.1198759 -4.1269188 -4.1315885 -4.1485395 -4.1668935 -4.1793141][-4.3306131 -4.3108234 -4.2692285 -4.2223988 -4.1849332 -4.1613059 -4.1299105 -4.090539 -4.0777364 -4.0897059 -4.1082206 -4.1162167 -4.1269736 -4.1439195 -4.1626678][-4.3208151 -4.2938538 -4.2397823 -4.1824665 -4.1413169 -4.1136723 -4.0702605 -4.0195746 -4.0112405 -4.0390697 -4.0718122 -4.091136 -4.1048322 -4.1230192 -4.1463356][-4.3125257 -4.2780356 -4.2151713 -4.1485047 -4.0967088 -4.0567327 -4.0049548 -3.9491596 -3.946764 -3.9875298 -4.0339551 -4.0663996 -4.0856366 -4.1055918 -4.1284122][-4.3069487 -4.2687345 -4.2103004 -4.1525478 -4.1025395 -4.0610309 -4.007123 -3.9482696 -3.9410794 -3.9780893 -4.0277138 -4.0672865 -4.0875621 -4.0961914 -4.105444][-4.305491 -4.2673192 -4.2216706 -4.1792741 -4.1388736 -4.1062341 -4.0610318 -4.0054932 -3.9907639 -4.019249 -4.0621433 -4.0947032 -4.1043911 -4.0934591 -4.0827641][-4.3051763 -4.2687869 -4.2330465 -4.2005076 -4.1688309 -4.1452761 -4.1121955 -4.0688605 -4.0536904 -4.0753407 -4.1085305 -4.13014 -4.1247678 -4.0987077 -4.0773387][-4.3031611 -4.2692356 -4.2389212 -4.2148256 -4.19311 -4.1776886 -4.1563578 -4.1316142 -4.1196308 -4.1293969 -4.1435604 -4.14951 -4.1352754 -4.1028528 -4.0809565][-4.3036237 -4.2731013 -4.2447886 -4.221365 -4.2042332 -4.1962447 -4.1846108 -4.1694064 -4.1614017 -4.1642294 -4.1651425 -4.1583543 -4.1411309 -4.1118174 -4.0956726][-4.3128772 -4.2877049 -4.2617478 -4.2375326 -4.2236257 -4.2199893 -4.2119608 -4.1997547 -4.1935959 -4.1941528 -4.1894922 -4.1788964 -4.1673679 -4.1451154 -4.1309586][-4.325593 -4.307529 -4.286 -4.2657366 -4.2563515 -4.2593975 -4.2581415 -4.2496057 -4.2427893 -4.2397957 -4.23136 -4.2207661 -4.2128711 -4.1966305 -4.1828976][-4.3378038 -4.3254151 -4.3100333 -4.2975531 -4.2937169 -4.2999516 -4.3005476 -4.2915044 -4.2830367 -4.2793512 -4.2713161 -4.262845 -4.2596545 -4.2519584 -4.2447596]]...]
INFO - root - 2017-12-06 07:00:58.662230: step 6610, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.613 sec/batch; 55h:32m:03s remains)
INFO - root - 2017-12-06 07:01:04.885680: step 6620, loss = 2.05, batch loss = 1.99 (14.3 examples/sec; 0.559 sec/batch; 50h:36m:20s remains)
INFO - root - 2017-12-06 07:01:11.219335: step 6630, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 58h:13m:21s remains)
INFO - root - 2017-12-06 07:01:17.636802: step 6640, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:28m:24s remains)
INFO - root - 2017-12-06 07:01:23.925470: step 6650, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 56h:14m:03s remains)
INFO - root - 2017-12-06 07:01:30.322462: step 6660, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:16m:36s remains)
INFO - root - 2017-12-06 07:01:36.741122: step 6670, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 59h:36m:54s remains)
INFO - root - 2017-12-06 07:01:43.185869: step 6680, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 56h:04m:04s remains)
INFO - root - 2017-12-06 07:01:49.545530: step 6690, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.626 sec/batch; 56h:40m:41s remains)
INFO - root - 2017-12-06 07:01:55.873082: step 6700, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 57h:06m:17s remains)
2017-12-06 07:01:56.081154: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:01:56.081193: E tensorflow/core/util/events_writer.cc:131] Failed to flush 18 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:01:56.498734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3145108 -4.307035 -4.2823577 -4.2689071 -4.2666097 -4.26861 -4.273119 -4.2688384 -4.2461219 -4.2162981 -4.1911716 -4.1798396 -4.1903749 -4.21705 -4.2412639][-4.3194528 -4.3159924 -4.2960258 -4.2842431 -4.2804947 -4.2807665 -4.2791047 -4.2603045 -4.2292972 -4.197185 -4.1709304 -4.161458 -4.1753683 -4.2070389 -4.2334824][-4.3283033 -4.3272562 -4.30619 -4.2883463 -4.2734571 -4.26274 -4.2473235 -4.2116475 -4.182426 -4.1636958 -4.1516542 -4.15154 -4.1678367 -4.199842 -4.2262783][-4.3365474 -4.3360767 -4.310523 -4.2808614 -4.2491579 -4.2200089 -4.1815739 -4.1265683 -4.1021123 -4.1056046 -4.1186957 -4.1351938 -4.1576571 -4.1922145 -4.2227554][-4.3401771 -4.3347235 -4.2989464 -4.2518759 -4.2035456 -4.1531487 -4.0827579 -4.003232 -3.9911661 -4.0309525 -4.0750537 -4.1155949 -4.1521797 -4.1959872 -4.2280025][-4.3362174 -4.3222361 -4.2736788 -4.208951 -4.1462231 -4.0720782 -3.964288 -3.8623972 -3.877702 -3.963876 -4.0382934 -4.1000404 -4.1510329 -4.2008009 -4.2299981][-4.3262472 -4.3033595 -4.2441354 -4.1657081 -4.0859647 -3.9811649 -3.8378139 -3.7299905 -3.787488 -3.9194434 -4.0191107 -4.09304 -4.1519136 -4.2020059 -4.2286172][-4.3184357 -4.287498 -4.2192221 -4.1327252 -4.0415635 -3.9121721 -3.7582688 -3.6778541 -3.7707767 -3.9225767 -4.0262403 -4.1016273 -4.1609182 -4.2019076 -4.2237144][-4.3118572 -4.2766566 -4.2069135 -4.1272 -4.04545 -3.9246726 -3.8026097 -3.7577367 -3.8463717 -3.9790041 -4.0662465 -4.1287184 -4.1784754 -4.2064757 -4.222044][-4.3090296 -4.2793326 -4.22178 -4.1636496 -4.1046929 -4.0148439 -3.9349873 -3.9129052 -3.9740934 -4.0653863 -4.126174 -4.1716781 -4.2088313 -4.2243443 -4.2351055][-4.3081131 -4.2876239 -4.2477088 -4.2122068 -4.1766758 -4.1171217 -4.0701938 -4.0609579 -4.0966945 -4.1550951 -4.1958079 -4.2270894 -4.2491827 -4.2546973 -4.2592649][-4.3113232 -4.2969103 -4.2707543 -4.2508979 -4.2328711 -4.2006464 -4.1776819 -4.1759892 -4.1981721 -4.2352705 -4.2612553 -4.2797017 -4.287467 -4.2846842 -4.282835][-4.3186421 -4.3103662 -4.2918038 -4.2773328 -4.2688618 -4.2580004 -4.2524037 -4.2568512 -4.2726197 -4.2962737 -4.3095827 -4.3154483 -4.3115716 -4.3038759 -4.2984109][-4.3237624 -4.3187528 -4.3004007 -4.2860842 -4.2821546 -4.2827778 -4.2896481 -4.3004241 -4.31483 -4.3301058 -4.3357825 -4.3352613 -4.32619 -4.3165169 -4.3088317][-4.323667 -4.3178339 -4.2939553 -4.2781553 -4.2775173 -4.2820692 -4.2954021 -4.3108535 -4.3253503 -4.3380485 -4.3427033 -4.3411369 -4.3321643 -4.322731 -4.3147774]]...]
INFO - root - 2017-12-06 07:02:02.884463: step 6710, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:06m:21s remains)
INFO - root - 2017-12-06 07:02:09.317478: step 6720, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 59h:26m:00s remains)
INFO - root - 2017-12-06 07:02:15.637781: step 6730, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:59m:04s remains)
INFO - root - 2017-12-06 07:02:21.947613: step 6740, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 57h:49m:43s remains)
INFO - root - 2017-12-06 07:02:28.206510: step 6750, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:49m:31s remains)
INFO - root - 2017-12-06 07:02:34.710641: step 6760, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 58h:46m:08s remains)
INFO - root - 2017-12-06 07:02:41.152149: step 6770, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:13m:14s remains)
INFO - root - 2017-12-06 07:02:47.671542: step 6780, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:46m:24s remains)
INFO - root - 2017-12-06 07:02:54.166768: step 6790, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.616 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-06 07:03:00.520438: step 6800, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 57h:03m:05s remains)
2017-12-06 07:03:01.131251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846165 -4.2475028 -4.2067375 -4.1812429 -4.1767311 -4.20045 -4.2203689 -4.2215462 -4.2225709 -4.2338085 -4.2429729 -4.2384405 -4.2298245 -4.2309408 -4.2203722][-4.28519 -4.2456522 -4.1974616 -4.1645937 -4.1534305 -4.16862 -4.1806221 -4.1752739 -4.1697974 -4.1851211 -4.2110324 -4.2141271 -4.20427 -4.1997561 -4.1871748][-4.28755 -4.2470069 -4.1980534 -4.1617613 -4.1402965 -4.1404634 -4.1429691 -4.1344523 -4.1263614 -4.1458049 -4.1835985 -4.1900167 -4.1750283 -4.1645627 -4.1464329][-4.2911606 -4.2523613 -4.2063279 -4.1691332 -4.1389265 -4.1263695 -4.1232624 -4.1147618 -4.10586 -4.1250362 -4.1643167 -4.1712031 -4.157362 -4.1487823 -4.12939][-4.293756 -4.2582827 -4.2164397 -4.183763 -4.1552558 -4.1422353 -4.1381335 -4.1304669 -4.1201973 -4.1342835 -4.1668987 -4.1714592 -4.1615186 -4.1590328 -4.1415997][-4.2909389 -4.2555909 -4.2131739 -4.1834135 -4.1605392 -4.1501856 -4.1445985 -4.1371469 -4.1310906 -4.1416626 -4.1657562 -4.1758056 -4.1744351 -4.1754661 -4.1655684][-4.2809291 -4.240294 -4.193121 -4.16241 -4.1443553 -4.1348171 -4.1243181 -4.1170034 -4.1152611 -4.1263614 -4.1484542 -4.1668329 -4.1736727 -4.1788907 -4.1770043][-4.2677689 -4.2180638 -4.1643891 -4.1317811 -4.1168613 -4.1082153 -4.0873127 -4.0722136 -4.063406 -4.074861 -4.0995274 -4.1285882 -4.1483788 -4.1610708 -4.1664677][-4.25438 -4.1960363 -4.1342473 -4.0960503 -4.0802078 -4.0739388 -4.05325 -4.0329056 -4.018003 -4.0295711 -4.0566182 -4.0908141 -4.1217093 -4.1428838 -4.1554942][-4.2500973 -4.1923251 -4.1305871 -4.090239 -4.07231 -4.0691791 -4.0568824 -4.0428004 -4.0294518 -4.038991 -4.0616288 -4.0921078 -4.1218042 -4.1470127 -4.1662679][-4.2581444 -4.2096953 -4.1574292 -4.121912 -4.1052446 -4.1028438 -4.0964947 -4.0923948 -4.0860238 -4.0908957 -4.1081862 -4.132009 -4.1565056 -4.18084 -4.2013059][-4.2798219 -4.2452989 -4.2087259 -4.1844878 -4.1753359 -4.1755047 -4.1718583 -4.1724176 -4.1672721 -4.1645751 -4.174438 -4.1886215 -4.2034707 -4.2215858 -4.2394981][-4.3051596 -4.2820892 -4.258604 -4.2458448 -4.243166 -4.2454009 -4.2438149 -4.2452464 -4.23893 -4.2297711 -4.2323513 -4.2385139 -4.2463117 -4.2566328 -4.2688384][-4.3204751 -4.3048596 -4.28771 -4.2805996 -4.2820616 -4.286087 -4.287128 -4.2890625 -4.2826667 -4.2734666 -4.2725692 -4.2740383 -4.2773924 -4.2842603 -4.2942958][-4.3245344 -4.3144579 -4.3038177 -4.3006821 -4.3046455 -4.3089681 -4.3112173 -4.31334 -4.3085666 -4.3011155 -4.2980409 -4.2971311 -4.2991438 -4.3045626 -4.3129921]]...]
INFO - root - 2017-12-06 07:03:07.489940: step 6810, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 59h:57m:59s remains)
INFO - root - 2017-12-06 07:03:13.992045: step 6820, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 56h:57m:21s remains)
INFO - root - 2017-12-06 07:03:20.053583: step 6830, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:46m:32s remains)
INFO - root - 2017-12-06 07:03:26.377808: step 6840, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 56h:47m:55s remains)
INFO - root - 2017-12-06 07:03:32.860372: step 6850, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:15m:18s remains)
INFO - root - 2017-12-06 07:03:39.242468: step 6860, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:49m:07s remains)
INFO - root - 2017-12-06 07:03:45.580774: step 6870, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:33m:44s remains)
INFO - root - 2017-12-06 07:03:52.000173: step 6880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 59h:04m:12s remains)
INFO - root - 2017-12-06 07:03:58.393812: step 6890, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:46m:04s remains)
INFO - root - 2017-12-06 07:04:04.891646: step 6900, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 56h:12m:22s remains)
2017-12-06 07:04:05.096726: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:04:05.096758: E tensorflow/core/util/events_writer.cc:131] Failed to flush 20 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:04:05.634411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.225966 -4.200388 -4.1857653 -4.1757627 -4.1626387 -4.1508088 -4.1526117 -4.1779604 -4.2121482 -4.2296786 -4.2399445 -4.2376113 -4.2236924 -4.1892295 -4.1541677][-4.2188706 -4.1851606 -4.17115 -4.1648564 -4.1495781 -4.130126 -4.1271791 -4.1607985 -4.206068 -4.2287226 -4.2379236 -4.2349315 -4.2199612 -4.1774883 -4.1303864][-4.2110229 -4.1733408 -4.1584253 -4.1536274 -4.1407723 -4.1182632 -4.1098213 -4.1458755 -4.1983442 -4.2254863 -4.2339993 -4.2302575 -4.2179909 -4.1740885 -4.1179581][-4.2100673 -4.1718454 -4.151701 -4.1400976 -4.1254773 -4.0929317 -4.0686884 -4.1063848 -4.1734076 -4.2086921 -4.2212415 -4.2215447 -4.2118816 -4.1728063 -4.1205091][-4.2168288 -4.1761332 -4.1427083 -4.114244 -4.0849075 -4.0261097 -3.9740319 -4.0245337 -4.1276441 -4.1828551 -4.2004304 -4.2060604 -4.2035332 -4.1758046 -4.1314044][-4.2307329 -4.1863627 -4.1363239 -4.0781422 -4.0150275 -3.9021156 -3.8082743 -3.8956957 -4.0559926 -4.1463938 -4.1874938 -4.211998 -4.2259641 -4.2146273 -4.17913][-4.2439423 -4.2014461 -4.1471062 -4.0698795 -3.9628136 -3.7821927 -3.650321 -3.7911282 -3.9981411 -4.1174359 -4.184124 -4.2295647 -4.2533345 -4.2516627 -4.2245808][-4.2484756 -4.2119107 -4.1727562 -4.1093984 -4.001308 -3.8317111 -3.7366343 -3.8620868 -4.0296373 -4.1317053 -4.1973381 -4.2438135 -4.2654462 -4.2596049 -4.2354422][-4.2457118 -4.2132273 -4.1868711 -4.1441741 -4.0694246 -3.9703643 -3.932941 -4.0116129 -4.112206 -4.1780763 -4.2165589 -4.2392154 -4.2487426 -4.2375908 -4.2155652][-4.2523608 -4.222652 -4.195466 -4.1549988 -4.099071 -4.0394135 -4.030652 -4.0944309 -4.1656384 -4.2036052 -4.222497 -4.2296247 -4.2233415 -4.207365 -4.1861367][-4.2705941 -4.2459917 -4.2182107 -4.1766009 -4.127028 -4.0793896 -4.0710564 -4.1261849 -4.1840358 -4.2084022 -4.2206197 -4.224669 -4.214673 -4.19765 -4.1728349][-4.2868004 -4.2675738 -4.2431045 -4.2069135 -4.1718016 -4.1301818 -4.1185513 -4.1714015 -4.2162738 -4.2308517 -4.2390866 -4.237524 -4.2253785 -4.2099342 -4.1847491][-4.2994051 -4.2845659 -4.2620091 -4.2359409 -4.2137051 -4.1790538 -4.1715631 -4.2248197 -4.2595282 -4.2643695 -4.26528 -4.2598796 -4.2491207 -4.2363677 -4.2131987][-4.3097386 -4.3007483 -4.2840495 -4.2630167 -4.2490263 -4.2214341 -4.2156072 -4.2632146 -4.2929125 -4.2940416 -4.2935405 -4.2902355 -4.2849574 -4.2766037 -4.2577972][-4.3212886 -4.3196235 -4.3115268 -4.2943006 -4.2857575 -4.268259 -4.2622027 -4.2920008 -4.3113532 -4.3141403 -4.3196077 -4.3238049 -4.3276558 -4.3265467 -4.3148112]]...]
INFO - root - 2017-12-06 07:04:12.155052: step 6910, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.613 sec/batch; 55h:25m:23s remains)
INFO - root - 2017-12-06 07:04:18.393852: step 6920, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:50m:20s remains)
INFO - root - 2017-12-06 07:04:24.561947: step 6930, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 56h:05m:34s remains)
INFO - root - 2017-12-06 07:04:31.029994: step 6940, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:27m:22s remains)
INFO - root - 2017-12-06 07:04:37.368589: step 6950, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 57h:25m:28s remains)
INFO - root - 2017-12-06 07:04:43.815685: step 6960, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 55h:31m:54s remains)
INFO - root - 2017-12-06 07:04:50.105584: step 6970, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.624 sec/batch; 56h:26m:39s remains)
INFO - root - 2017-12-06 07:04:56.515917: step 6980, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 58h:10m:35s remains)
INFO - root - 2017-12-06 07:05:02.999208: step 6990, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 57h:22m:25s remains)
INFO - root - 2017-12-06 07:05:09.453676: step 7000, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 59h:17m:16s remains)
2017-12-06 07:05:10.077997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2735066 -4.2621136 -4.2564716 -4.2502151 -4.2458076 -4.2506771 -4.2569637 -4.259335 -4.2564173 -4.2508183 -4.2477479 -4.260602 -4.2806692 -4.2992182 -4.315527][-4.2431564 -4.2227149 -4.2147746 -4.2070165 -4.2014532 -4.2077661 -4.2154417 -4.2174492 -4.2114472 -4.204175 -4.2008209 -4.2225924 -4.252583 -4.2767353 -4.2977767][-4.212677 -4.1836014 -4.1734433 -4.1672115 -4.1627707 -4.1703982 -4.1801376 -4.1808877 -4.1740985 -4.1696687 -4.1699595 -4.1982903 -4.2332325 -4.25803 -4.2800236][-4.1695471 -4.1378026 -4.1292143 -4.1272616 -4.1258121 -4.1355228 -4.14707 -4.1480508 -4.1431336 -4.1437092 -4.1515021 -4.1834955 -4.2179441 -4.2397537 -4.2629094][-4.1273913 -4.0963812 -4.0893354 -4.0855389 -4.0821538 -4.0892477 -4.098321 -4.09896 -4.1000485 -4.1110721 -4.1338868 -4.1710505 -4.2042732 -4.2257376 -4.2521725][-4.0980735 -4.0628872 -4.0546284 -4.0461583 -4.0333424 -4.0270724 -4.0239539 -4.0157266 -4.02394 -4.0544825 -4.0977926 -4.1456275 -4.1814594 -4.2082143 -4.2411118][-4.0619764 -4.0198836 -4.007278 -3.99664 -3.9743416 -3.9506803 -3.9218488 -3.8942652 -3.9172568 -3.9786813 -4.0446754 -4.1069927 -4.1512117 -4.1859179 -4.2278056][-4.01455 -3.9692402 -3.9518306 -3.9330153 -3.8995352 -3.8556201 -3.7933466 -3.7430844 -3.7915132 -3.8890216 -3.9755256 -4.0539465 -4.1124172 -4.1596417 -4.2105603][-4.008307 -3.9720087 -3.954345 -3.9319081 -3.8962703 -3.8488259 -3.7822855 -3.7336116 -3.7912054 -3.8889456 -3.9662521 -4.038404 -4.1005344 -4.1525769 -4.201889][-4.0524583 -4.0341249 -4.0260873 -4.0130367 -3.9897568 -3.95718 -3.910625 -3.8792038 -3.92083 -3.9850426 -4.0312071 -4.0818076 -4.1333361 -4.1761966 -4.2148528][-4.1243553 -4.1223989 -4.1270065 -4.1241994 -4.1147323 -4.0973105 -4.0704594 -4.0525451 -4.0762267 -4.11067 -4.1317215 -4.1631074 -4.200182 -4.2282066 -4.2515607][-4.1970263 -4.20583 -4.2178717 -4.22274 -4.2234387 -4.2175632 -4.2018576 -4.1919 -4.203558 -4.2204466 -4.2282224 -4.2450404 -4.2686591 -4.2827387 -4.293108][-4.2540083 -4.2628803 -4.2721314 -4.2770386 -4.2800908 -4.2793703 -4.2703176 -4.2652082 -4.2715592 -4.2813907 -4.2847538 -4.2944369 -4.3105083 -4.3195882 -4.3237176][-4.27553 -4.2797523 -4.2835741 -4.2864995 -4.2891164 -4.29058 -4.2860665 -4.2835674 -4.287497 -4.294322 -4.2983031 -4.30693 -4.3208971 -4.3307171 -4.3347106][-4.2806764 -4.2833948 -4.28507 -4.287138 -4.288362 -4.2889075 -4.285821 -4.2844176 -4.2865644 -4.2908497 -4.2945371 -4.3030658 -4.317008 -4.328723 -4.3358135]]...]
INFO - root - 2017-12-06 07:05:16.610491: step 7010, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 57h:40m:26s remains)
INFO - root - 2017-12-06 07:05:22.821529: step 7020, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:19m:29s remains)
INFO - root - 2017-12-06 07:05:29.109673: step 7030, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 57h:33m:03s remains)
INFO - root - 2017-12-06 07:05:35.558340: step 7040, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 57h:18m:37s remains)
INFO - root - 2017-12-06 07:05:41.865356: step 7050, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:44m:26s remains)
INFO - root - 2017-12-06 07:05:48.268226: step 7060, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 56h:24m:39s remains)
INFO - root - 2017-12-06 07:05:54.696323: step 7070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:30m:56s remains)
INFO - root - 2017-12-06 07:06:01.069914: step 7080, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 56h:05m:57s remains)
INFO - root - 2017-12-06 07:06:07.427890: step 7090, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 56h:58m:23s remains)
INFO - root - 2017-12-06 07:06:13.819636: step 7100, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 58h:19m:24s remains)
2017-12-06 07:06:14.025356: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:06:14.025394: E tensorflow/core/util/events_writer.cc:131] Failed to flush 22 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:06:14.428018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3075027 -4.3140454 -4.313251 -4.3016548 -4.2901897 -4.2872524 -4.2945123 -4.3005219 -4.3020225 -4.2949977 -4.2872639 -4.2856116 -4.2940745 -4.3062639 -4.3070083][-4.3147421 -4.3109694 -4.2990756 -4.2808328 -4.2648067 -4.2625637 -4.282217 -4.3039489 -4.3183894 -4.3206878 -4.3125219 -4.3032613 -4.300765 -4.3031034 -4.2973018][-4.3140111 -4.3004503 -4.2768569 -4.251687 -4.2284594 -4.2228537 -4.2519274 -4.2868185 -4.308744 -4.3169713 -4.3073139 -4.2913685 -4.278688 -4.2712893 -4.2606363][-4.3034921 -4.2836232 -4.2492967 -4.2151818 -4.1811075 -4.1688437 -4.2054992 -4.252172 -4.280746 -4.2934 -4.2867508 -4.2702188 -4.2549806 -4.2425971 -4.2280507][-4.2974572 -4.2753944 -4.2305503 -4.1810632 -4.1305866 -4.1077523 -4.1524568 -4.2162538 -4.2568612 -4.2758594 -4.2804031 -4.2721338 -4.2616162 -4.2495127 -4.23387][-4.30034 -4.2803245 -4.2284737 -4.159884 -4.0848832 -4.0476208 -4.0991769 -4.1842074 -4.2394838 -4.2654796 -4.2801766 -4.2837896 -4.2819333 -4.2756772 -4.2645946][-4.2947421 -4.2792253 -4.2258048 -4.1463904 -4.0507545 -3.9981217 -4.0512772 -4.1506839 -4.2161803 -4.2460594 -4.2643666 -4.2765865 -4.284514 -4.2907705 -4.2893996][-4.2722507 -4.2610464 -4.21598 -4.139924 -4.0418911 -3.9836984 -4.0312924 -4.1300097 -4.19472 -4.2234197 -4.2389 -4.2519574 -4.265758 -4.282671 -4.2906857][-4.2493048 -4.2399073 -4.2086024 -4.1483493 -4.0642881 -4.0092621 -4.048893 -4.1351628 -4.1908593 -4.2129068 -4.2235265 -4.23566 -4.252666 -4.2736931 -4.285141][-4.2467737 -4.2343717 -4.213295 -4.1678329 -4.102561 -4.0550442 -4.0852079 -4.1590772 -4.2082653 -4.2246547 -4.2300334 -4.2393923 -4.2554908 -4.2733235 -4.2829895][-4.2652736 -4.2486262 -4.2343774 -4.2023721 -4.1562095 -4.1187963 -4.1381807 -4.2012186 -4.2428579 -4.2531409 -4.2554111 -4.2606759 -4.2691741 -4.2794952 -4.2862191][-4.2777209 -4.2638316 -4.2563224 -4.2383342 -4.2091913 -4.1833797 -4.193162 -4.2391295 -4.2713861 -4.2795897 -4.2821293 -4.2852015 -4.2870507 -4.2897758 -4.2917709][-4.276473 -4.269206 -4.2694745 -4.2629323 -4.2472911 -4.2280135 -4.2277479 -4.2545853 -4.276319 -4.285851 -4.2951212 -4.3027549 -4.3036389 -4.3028173 -4.3003922][-4.2648706 -4.2639151 -4.271327 -4.2734556 -4.266263 -4.2499242 -4.2377768 -4.243937 -4.2544165 -4.2661643 -4.2845774 -4.3002276 -4.3044147 -4.3033447 -4.3001518][-4.2480288 -4.2525535 -4.26522 -4.2743468 -4.2725296 -4.2552724 -4.2301946 -4.2148418 -4.2101116 -4.2210369 -4.2470403 -4.270885 -4.2826519 -4.28762 -4.2886224]]...]
INFO - root - 2017-12-06 07:06:20.806059: step 7110, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 57h:57m:51s remains)
INFO - root - 2017-12-06 07:06:27.223211: step 7120, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 57h:46m:02s remains)
INFO - root - 2017-12-06 07:06:33.523513: step 7130, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 57h:00m:24s remains)
INFO - root - 2017-12-06 07:06:39.740006: step 7140, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 58h:56m:27s remains)
INFO - root - 2017-12-06 07:06:46.109122: step 7150, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 55h:45m:07s remains)
INFO - root - 2017-12-06 07:06:52.404651: step 7160, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.610 sec/batch; 55h:08m:23s remains)
INFO - root - 2017-12-06 07:06:58.686523: step 7170, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 57h:44m:10s remains)
INFO - root - 2017-12-06 07:07:05.127452: step 7180, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:02m:16s remains)
INFO - root - 2017-12-06 07:07:11.475438: step 7190, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 58h:36m:28s remains)
INFO - root - 2017-12-06 07:07:17.807722: step 7200, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 57h:39m:47s remains)
2017-12-06 07:07:18.439593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2081757 -4.2190695 -4.2373128 -4.2485614 -4.2581272 -4.2633643 -4.2520132 -4.2446532 -4.250217 -4.2553978 -4.2571745 -4.2568283 -4.257288 -4.2634687 -4.2705646][-4.2583036 -4.2646255 -4.2795057 -4.2866287 -4.29078 -4.2915654 -4.2764463 -4.264277 -4.2648191 -4.2649069 -4.2628055 -4.2614622 -4.2620397 -4.2691374 -4.2726545][-4.2940679 -4.2958207 -4.3015223 -4.30047 -4.2958117 -4.2916884 -4.2728553 -4.2545466 -4.2505541 -4.2506781 -4.2485147 -4.2449713 -4.2434726 -4.2469544 -4.242744][-4.3078423 -4.3117347 -4.3101492 -4.3021674 -4.29014 -4.2778711 -4.2532663 -4.2304544 -4.2257237 -4.2329345 -4.2399087 -4.237329 -4.2311077 -4.2263775 -4.2203264][-4.2984433 -4.309195 -4.3070011 -4.2901292 -4.2672768 -4.2392254 -4.1983743 -4.1713171 -4.1800246 -4.2164164 -4.24842 -4.2570224 -4.2491961 -4.239378 -4.2315564][-4.2810326 -4.2988553 -4.2929111 -4.2591782 -4.2133603 -4.1573505 -4.0875916 -4.05026 -4.0823789 -4.1622477 -4.2308555 -4.266355 -4.2757807 -4.276257 -4.27149][-4.27153 -4.2894144 -4.2746859 -4.2169228 -4.1399155 -4.0478296 -3.9356246 -3.8744354 -3.9329777 -4.0652709 -4.1762257 -4.2470574 -4.2864614 -4.3036084 -4.3040929][-4.2690849 -4.2892237 -4.2692852 -4.1977496 -4.1037564 -3.9861047 -3.8380091 -3.7423344 -3.8109379 -3.9833288 -4.1201625 -4.2102256 -4.2717166 -4.299921 -4.2965765][-4.2564726 -4.2899218 -4.2791443 -4.2150016 -4.1297216 -4.0233593 -3.8839259 -3.7784681 -3.8230367 -3.9767263 -4.1041579 -4.1910167 -4.25347 -4.2804132 -4.2704811][-4.243022 -4.2910113 -4.2972846 -4.2552776 -4.1912904 -4.1157889 -4.019701 -3.9390206 -3.9516222 -4.0486579 -4.1380625 -4.1981144 -4.2434831 -4.2592793 -4.2422671][-4.253231 -4.3038111 -4.3198938 -4.2965693 -4.2518845 -4.2038374 -4.1516008 -4.1015606 -4.0973096 -4.146585 -4.1975956 -4.230155 -4.2512455 -4.2508907 -4.2273035][-4.2837315 -4.3198676 -4.3289452 -4.3100405 -4.2765951 -4.2489376 -4.2271733 -4.2027268 -4.1999636 -4.2293196 -4.2592354 -4.2737255 -4.2753696 -4.2607222 -4.2330928][-4.3019123 -4.3218641 -4.3218145 -4.30714 -4.2822776 -4.264607 -4.2601318 -4.2552261 -4.2593603 -4.2792435 -4.29783 -4.3014889 -4.2908664 -4.272316 -4.2473145][-4.3049512 -4.3184357 -4.3128057 -4.30176 -4.2902527 -4.2818069 -4.2813492 -4.2811103 -4.2862196 -4.2983437 -4.3109818 -4.3127866 -4.3021216 -4.2860265 -4.2669029][-4.3083215 -4.3163466 -4.3085952 -4.3002105 -4.2944889 -4.2900305 -4.2879648 -4.2862182 -4.2894807 -4.2980366 -4.3081174 -4.3121896 -4.3064094 -4.2953782 -4.2818179]]...]
INFO - root - 2017-12-06 07:07:24.794647: step 7210, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.614 sec/batch; 55h:27m:05s remains)
INFO - root - 2017-12-06 07:07:31.128694: step 7220, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.620 sec/batch; 56h:01m:56s remains)
INFO - root - 2017-12-06 07:07:37.527795: step 7230, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 57h:46m:50s remains)
INFO - root - 2017-12-06 07:07:43.602047: step 7240, loss = 2.02, batch loss = 1.97 (12.1 examples/sec; 0.662 sec/batch; 59h:51m:03s remains)
INFO - root - 2017-12-06 07:07:49.961261: step 7250, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 58h:42m:14s remains)
INFO - root - 2017-12-06 07:07:56.457230: step 7260, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.612 sec/batch; 55h:15m:05s remains)
INFO - root - 2017-12-06 07:08:02.843560: step 7270, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:01m:44s remains)
INFO - root - 2017-12-06 07:08:09.274927: step 7280, loss = 2.08, batch loss = 2.02 (13.3 examples/sec; 0.601 sec/batch; 54h:18m:53s remains)
INFO - root - 2017-12-06 07:08:15.706116: step 7290, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 61h:53m:54s remains)
INFO - root - 2017-12-06 07:08:22.131754: step 7300, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 56h:48m:33s remains)
2017-12-06 07:08:22.333841: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:08:22.333872: E tensorflow/core/util/events_writer.cc:131] Failed to flush 24 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:08:22.827307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2736263 -4.2658472 -4.2758832 -4.2910008 -4.3007393 -4.2978363 -4.2880874 -4.2888646 -4.2940025 -4.2896729 -4.272954 -4.2569795 -4.2519789 -4.2571249 -4.27384][-4.2224989 -4.202755 -4.21029 -4.2320514 -4.2481952 -4.2478738 -4.2384133 -4.2376804 -4.2409434 -4.2348185 -4.215785 -4.2026887 -4.2041416 -4.21707 -4.2382312][-4.1862574 -4.1573977 -4.1602764 -4.1881762 -4.209569 -4.2113481 -4.2018156 -4.2013173 -4.203021 -4.1967983 -4.1812615 -4.1774697 -4.1899047 -4.2096906 -4.2305775][-4.1739426 -4.1462011 -4.1475129 -4.1753688 -4.1977496 -4.1989188 -4.1894093 -4.1877685 -4.1945696 -4.194819 -4.1899433 -4.1952648 -4.2096405 -4.2228169 -4.2357469][-4.1805096 -4.1589265 -4.1631804 -4.1839352 -4.1948233 -4.1853247 -4.1608763 -4.152607 -4.1683831 -4.1896629 -4.2031918 -4.2134995 -4.2212195 -4.2219286 -4.2252913][-4.19671 -4.1809368 -4.18781 -4.2003484 -4.1951342 -4.1631985 -4.1085987 -4.0789733 -4.1096587 -4.1699834 -4.20604 -4.219481 -4.2223525 -4.2176523 -4.2151608][-4.2159843 -4.2054968 -4.2097769 -4.2163682 -4.1997132 -4.1454191 -4.0460172 -3.9712217 -4.0225372 -4.1364975 -4.2009687 -4.2206187 -4.2274308 -4.2273593 -4.2237082][-4.2274952 -4.2191906 -4.2228322 -4.2290111 -4.2131639 -4.1565762 -4.0426779 -3.9444151 -3.9925051 -4.1217332 -4.1962795 -4.219243 -4.2311525 -4.2395935 -4.2382245][-4.2313428 -4.2279406 -4.2376709 -4.2521858 -4.2491069 -4.2102904 -4.1277313 -4.0573521 -4.0823965 -4.1634049 -4.2120857 -4.2240806 -4.2318339 -4.2400055 -4.2382312][-4.233851 -4.2359476 -4.2505674 -4.2712121 -4.2804179 -4.2607732 -4.2158289 -4.1773458 -4.1860042 -4.22444 -4.2416129 -4.236402 -4.2329965 -4.2325211 -4.2276845][-4.2386866 -4.2406158 -4.2566361 -4.2782593 -4.2894258 -4.2816334 -4.2621255 -4.2466326 -4.2497616 -4.2668738 -4.27007 -4.2642708 -4.2593637 -4.2528348 -4.241941][-4.2454462 -4.2461162 -4.2592297 -4.279336 -4.2880325 -4.2839808 -4.277565 -4.2741084 -4.2794609 -4.2920637 -4.2967219 -4.3008504 -4.3033323 -4.2972832 -4.2807918][-4.2676563 -4.2708845 -4.2824726 -4.2950597 -4.2990665 -4.2959385 -4.2919559 -4.2927608 -4.3029127 -4.3168979 -4.3222656 -4.3316135 -4.3356438 -4.3308415 -4.3125806][-4.2904081 -4.2937274 -4.3019385 -4.3070617 -4.3083324 -4.304677 -4.3028841 -4.3027806 -4.3127785 -4.3265505 -4.3331976 -4.3398342 -4.3432913 -4.3392549 -4.3227024][-4.3080721 -4.3096366 -4.3144321 -4.3170576 -4.3181529 -4.3181071 -4.3150272 -4.3130879 -4.3213477 -4.333818 -4.3401742 -4.34298 -4.3453832 -4.3446522 -4.33353]]...]
INFO - root - 2017-12-06 07:08:29.255600: step 7310, loss = 2.09, batch loss = 2.03 (13.2 examples/sec; 0.607 sec/batch; 54h:48m:19s remains)
INFO - root - 2017-12-06 07:08:35.645222: step 7320, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 56h:26m:15s remains)
INFO - root - 2017-12-06 07:08:41.902728: step 7330, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 59h:18m:44s remains)
INFO - root - 2017-12-06 07:08:47.840233: step 7340, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 58h:05m:09s remains)
INFO - root - 2017-12-06 07:08:54.278199: step 7350, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 58h:01m:01s remains)
INFO - root - 2017-12-06 07:09:00.704041: step 7360, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 57h:57m:22s remains)
INFO - root - 2017-12-06 07:09:07.031474: step 7370, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 57h:58m:30s remains)
INFO - root - 2017-12-06 07:09:13.377494: step 7380, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:04m:43s remains)
INFO - root - 2017-12-06 07:09:19.708292: step 7390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:32m:24s remains)
INFO - root - 2017-12-06 07:09:26.128924: step 7400, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 56h:04m:34s remains)
2017-12-06 07:09:26.841341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581377 -4.2926135 -4.3199019 -4.321784 -4.301652 -4.2619152 -4.2142248 -4.1659131 -4.1079092 -4.078721 -4.109437 -4.1598787 -4.2073817 -4.2280731 -4.237174][-4.2830877 -4.311327 -4.3259635 -4.3161016 -4.2926 -4.2536721 -4.205337 -4.1512566 -4.093574 -4.0692983 -4.105154 -4.1533484 -4.1946144 -4.2200141 -4.2383475][-4.2871566 -4.314712 -4.3229547 -4.3050237 -4.2832503 -4.2565956 -4.2196617 -4.1674552 -4.111424 -4.0834947 -4.1129346 -4.1550632 -4.1910052 -4.2246294 -4.2523251][-4.2885222 -4.3100157 -4.3134518 -4.29524 -4.2797747 -4.2714949 -4.248013 -4.1995125 -4.1424222 -4.1030383 -4.1162834 -4.15462 -4.1927147 -4.2323093 -4.2662172][-4.3050036 -4.3170247 -4.3158088 -4.2981186 -4.281426 -4.2765751 -4.2534246 -4.1961832 -4.1289263 -4.0897098 -4.1030178 -4.1426749 -4.189826 -4.2380257 -4.2782936][-4.3250227 -4.3295236 -4.3235173 -4.3040695 -4.2788143 -4.255712 -4.2122517 -4.1272707 -4.0455184 -4.0233397 -4.0647264 -4.1196923 -4.1772413 -4.23398 -4.2798409][-4.3330903 -4.3343487 -4.3279495 -4.3058343 -4.26704 -4.2137408 -4.12914 -4.0008044 -3.8948011 -3.9159541 -4.0170774 -4.0993876 -4.1672969 -4.2327566 -4.2832108][-4.3290668 -4.332428 -4.3295531 -4.3062038 -4.2557321 -4.1724882 -4.0414882 -3.8660035 -3.7339938 -3.8067236 -3.9677513 -4.0797834 -4.1593704 -4.2345586 -4.2901697][-4.3243556 -4.3282738 -4.325603 -4.3025489 -4.2486362 -4.1582437 -4.0205956 -3.8470778 -3.7097862 -3.7748511 -3.9389367 -4.0624204 -4.1523833 -4.2341323 -4.292666][-4.3215938 -4.3238993 -4.3204632 -4.3001537 -4.2507172 -4.1766224 -4.0784059 -3.9565649 -3.849524 -3.8666015 -3.9723902 -4.0735683 -4.1592207 -4.239615 -4.2949715][-4.3185587 -4.3186622 -4.3134608 -4.2962275 -4.2579727 -4.2051735 -4.1483731 -4.0783067 -4.0050955 -3.9949682 -4.0470061 -4.1150503 -4.1875658 -4.2577248 -4.3055639][-4.3198085 -4.3186774 -4.3120513 -4.2970157 -4.2673421 -4.2298217 -4.1986456 -4.1615796 -4.1136141 -4.1022863 -4.1322322 -4.1768932 -4.233758 -4.2872667 -4.3223567][-4.3213234 -4.3204865 -4.3139949 -4.3003778 -4.2771988 -4.252018 -4.2344103 -4.2101927 -4.1769347 -4.1706676 -4.1943011 -4.2302008 -4.275928 -4.3148837 -4.3360176][-4.3210373 -4.3217087 -4.3189974 -4.311161 -4.292758 -4.2717547 -4.2565117 -4.2360277 -4.2106552 -4.2075043 -4.2285938 -4.2637534 -4.3022604 -4.3294411 -4.3402109][-4.3124247 -4.3169007 -4.318378 -4.3169322 -4.3040633 -4.2854667 -4.2701693 -4.2522192 -4.2347307 -4.2346435 -4.2538691 -4.2843003 -4.313345 -4.3299122 -4.3342538]]...]
INFO - root - 2017-12-06 07:09:33.210806: step 7410, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 56h:27m:22s remains)
INFO - root - 2017-12-06 07:09:39.633576: step 7420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:17m:25s remains)
INFO - root - 2017-12-06 07:09:45.991056: step 7430, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:24m:19s remains)
INFO - root - 2017-12-06 07:09:52.279741: step 7440, loss = 2.07, batch loss = 2.01 (13.6 examples/sec; 0.587 sec/batch; 53h:01m:06s remains)
INFO - root - 2017-12-06 07:09:58.461563: step 7450, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:13m:50s remains)
INFO - root - 2017-12-06 07:10:04.922428: step 7460, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:23m:32s remains)
INFO - root - 2017-12-06 07:10:11.375986: step 7470, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 58h:01m:49s remains)
INFO - root - 2017-12-06 07:10:17.667903: step 7480, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 57h:09m:35s remains)
INFO - root - 2017-12-06 07:10:24.039098: step 7490, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 56h:46m:39s remains)
INFO - root - 2017-12-06 07:10:30.436059: step 7500, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 58h:59m:07s remains)
2017-12-06 07:10:30.642302: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:10:30.642339: E tensorflow/core/util/events_writer.cc:131] Failed to flush 26 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:10:31.124056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1727815 -4.1676874 -4.1581011 -4.1289239 -4.1006494 -4.0964885 -4.1272526 -4.1938448 -4.2468877 -4.2743392 -4.2817221 -4.2732058 -4.2546334 -4.2212582 -4.1872182][-4.1367221 -4.1415105 -4.1383853 -4.1063275 -4.0712786 -4.0619431 -4.08779 -4.1622882 -4.2252507 -4.2544403 -4.2601275 -4.2553105 -4.2498946 -4.22842 -4.1999245][-4.1225405 -4.1342239 -4.1397257 -4.113718 -4.0759516 -4.057272 -4.0638123 -4.1292768 -4.1922579 -4.219584 -4.2263746 -4.2268882 -4.2359257 -4.2247014 -4.1962824][-4.1406546 -4.1542463 -4.1632748 -4.1374807 -4.0936375 -4.0615749 -4.0403881 -4.0870972 -4.1450644 -4.174202 -4.1906977 -4.199677 -4.2167807 -4.2162418 -4.1947093][-4.1653404 -4.1697197 -4.17605 -4.1477013 -4.0946865 -4.0426192 -3.9987211 -4.0320992 -4.0944657 -4.1332045 -4.1604123 -4.1792712 -4.2029572 -4.2084537 -4.1976957][-4.1678581 -4.1605926 -4.163383 -4.1314163 -4.0704908 -3.9956918 -3.9320722 -3.9647398 -4.0503855 -4.1128964 -4.1490421 -4.1741052 -4.196578 -4.2003264 -4.1935453][-4.138752 -4.1266661 -4.1348944 -4.1053686 -4.0397415 -3.9449203 -3.8716936 -3.9196901 -4.0319848 -4.1158862 -4.15666 -4.1810226 -4.1948671 -4.1922903 -4.1840124][-4.1132278 -4.1050291 -4.1325974 -4.1193924 -4.0575976 -3.9639952 -3.9039011 -3.956213 -4.0678954 -4.1510458 -4.1882977 -4.2045283 -4.2054119 -4.1927009 -4.17886][-4.126368 -4.1261053 -4.1663642 -4.1651011 -4.1118135 -4.0379233 -3.9992297 -4.0386095 -4.1215892 -4.1856222 -4.2114253 -4.2162013 -4.2119684 -4.1986523 -4.18874][-4.1589231 -4.1653595 -4.1983171 -4.1964116 -4.1582847 -4.116117 -4.095962 -4.1181555 -4.1668258 -4.2012935 -4.2123694 -4.2130184 -4.2118311 -4.2057719 -4.2068048][-4.2079563 -4.2132268 -4.2316632 -4.2249689 -4.1979437 -4.1729226 -4.1618643 -4.1733913 -4.2058387 -4.218678 -4.2182245 -4.2210813 -4.2215524 -4.2199249 -4.2231011][-4.26081 -4.2632442 -4.2743664 -4.2635889 -4.2413163 -4.2172647 -4.2097497 -4.2163029 -4.2440867 -4.2544026 -4.2527275 -4.252892 -4.248529 -4.2458286 -4.2432985][-4.290349 -4.2914867 -4.3013139 -4.28837 -4.2692213 -4.2519355 -4.2418394 -4.244205 -4.2712531 -4.2901549 -4.289979 -4.2826228 -4.2788577 -4.2801819 -4.2761459][-4.3029208 -4.2991629 -4.3029981 -4.2911716 -4.2781482 -4.2691813 -4.2591949 -4.2577176 -4.281302 -4.3025479 -4.2996225 -4.289639 -4.2882504 -4.2929134 -4.2917924][-4.3159108 -4.3080678 -4.30578 -4.2981009 -4.2908964 -4.2846909 -4.2756228 -4.2701979 -4.2844286 -4.2999053 -4.2957883 -4.2910962 -4.2949839 -4.3004 -4.3025169]]...]
INFO - root - 2017-12-06 07:10:37.593764: step 7510, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:10m:49s remains)
INFO - root - 2017-12-06 07:10:44.031367: step 7520, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:26m:12s remains)
INFO - root - 2017-12-06 07:10:50.422717: step 7530, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:11m:28s remains)
INFO - root - 2017-12-06 07:10:56.841986: step 7540, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:43m:49s remains)
INFO - root - 2017-12-06 07:11:02.885967: step 7550, loss = 2.06, batch loss = 2.00 (15.8 examples/sec; 0.508 sec/batch; 45h:48m:32s remains)
INFO - root - 2017-12-06 07:11:09.309687: step 7560, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 58h:31m:19s remains)
INFO - root - 2017-12-06 07:11:15.672281: step 7570, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:34m:40s remains)
INFO - root - 2017-12-06 07:11:22.088249: step 7580, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:26m:53s remains)
INFO - root - 2017-12-06 07:11:28.484328: step 7590, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 58h:19m:32s remains)
INFO - root - 2017-12-06 07:11:34.974476: step 7600, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 57h:58m:27s remains)
2017-12-06 07:11:35.626418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.204864 -4.209548 -4.2174149 -4.2211275 -4.2292337 -4.24404 -4.2410307 -4.225997 -4.2333198 -4.2554369 -4.2631564 -4.2669997 -4.2671461 -4.2654834 -4.2573762][-4.1856003 -4.1969666 -4.2039132 -4.2040915 -4.2127914 -4.2327261 -4.230855 -4.2135172 -4.2209644 -4.242723 -4.2486429 -4.2538109 -4.2597537 -4.257432 -4.2496424][-4.177609 -4.2055688 -4.2195792 -4.2212076 -4.227622 -4.2427411 -4.2307725 -4.2040372 -4.2084174 -4.2275114 -4.2316356 -4.2386622 -4.2515631 -4.2517929 -4.2440348][-4.1963735 -4.2318621 -4.2463675 -4.2423277 -4.235939 -4.2344012 -4.2100673 -4.1799273 -4.1886153 -4.2098904 -4.2177815 -4.2279363 -4.2455273 -4.2511506 -4.2436986][-4.2281089 -4.2647157 -4.2738233 -4.2576256 -4.2350073 -4.2131858 -4.1767488 -4.1471653 -4.1590981 -4.1828604 -4.1957226 -4.2104683 -4.2347794 -4.249176 -4.2470646][-4.2324071 -4.26761 -4.2769079 -4.2545414 -4.2186246 -4.17202 -4.1151862 -4.0843878 -4.1059308 -4.1354609 -4.1546187 -4.175992 -4.209003 -4.2352247 -4.244154][-4.2089868 -4.2365 -4.2425823 -4.2119346 -4.1611958 -4.0885653 -4.0080433 -3.9807625 -4.0218163 -4.0670586 -4.0964837 -4.1275582 -4.1713128 -4.2136903 -4.2370543][-4.1633649 -4.1815648 -4.1766219 -4.1336966 -4.0676355 -3.9736955 -3.8822229 -3.8722718 -3.9416137 -4.0098939 -4.0523567 -4.0898 -4.1430392 -4.1969867 -4.2309775][-4.1197124 -4.1244078 -4.1098776 -4.0584841 -3.9873548 -3.8947945 -3.8205786 -3.8372812 -3.9224658 -4.0007606 -4.04767 -4.0842767 -4.1375308 -4.194108 -4.2331944][-4.1155486 -4.1121259 -4.0905986 -4.0414553 -3.9806421 -3.9212408 -3.8903377 -3.9169791 -3.98556 -4.0476346 -4.0847826 -4.1129122 -4.1569977 -4.207942 -4.2460132][-4.1437902 -4.1385145 -4.1157084 -4.0711212 -4.0252385 -4.0021033 -4.0007553 -4.0216794 -4.0644636 -4.1039824 -4.1289315 -4.1513591 -4.1877809 -4.2298832 -4.2636051][-4.17995 -4.1722078 -4.1488256 -4.1122637 -4.0831232 -4.0809546 -4.0912628 -4.105854 -4.1297975 -4.1524239 -4.1703777 -4.1892614 -4.2181182 -4.2518191 -4.2810235][-4.2275786 -4.220386 -4.2017617 -4.1778073 -4.1642222 -4.1685534 -4.1763406 -4.184072 -4.1961145 -4.2091475 -4.2221704 -4.2372971 -4.2590775 -4.2828112 -4.3036604][-4.2826371 -4.2778721 -4.2660303 -4.2524366 -4.2454185 -4.2487617 -4.2529736 -4.2561369 -4.261816 -4.2701268 -4.2797122 -4.2907181 -4.3046327 -4.3162923 -4.3259516][-4.3238544 -4.3215785 -4.3162928 -4.3105688 -4.3071527 -4.3083816 -4.3105397 -4.3122306 -4.3149695 -4.31897 -4.3236752 -4.3294797 -4.3354635 -4.337882 -4.3399272]]...]
INFO - root - 2017-12-06 07:11:42.131971: step 7610, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 57h:11m:00s remains)
INFO - root - 2017-12-06 07:11:48.477355: step 7620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:37m:44s remains)
INFO - root - 2017-12-06 07:11:54.876836: step 7630, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 55h:53m:46s remains)
INFO - root - 2017-12-06 07:12:01.310104: step 7640, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.643 sec/batch; 57h:58m:56s remains)
INFO - root - 2017-12-06 07:12:07.665337: step 7650, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 56h:51m:40s remains)
INFO - root - 2017-12-06 07:12:13.981470: step 7660, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:17m:59s remains)
INFO - root - 2017-12-06 07:12:20.358048: step 7670, loss = 2.04, batch loss = 1.99 (12.9 examples/sec; 0.621 sec/batch; 56h:04m:00s remains)
INFO - root - 2017-12-06 07:12:26.789986: step 7680, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 58h:00m:24s remains)
INFO - root - 2017-12-06 07:12:33.220256: step 7690, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 60h:16m:24s remains)
INFO - root - 2017-12-06 07:12:39.631907: step 7700, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.647 sec/batch; 58h:20m:51s remains)
2017-12-06 07:12:39.859261: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:12:39.859399: E tensorflow/core/util/events_writer.cc:131] Failed to flush 28 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:12:40.281579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2479076 -4.2611437 -4.2631006 -4.2530227 -4.2361493 -4.2272768 -4.2406836 -4.2688336 -4.2958393 -4.3162231 -4.3333139 -4.3386736 -4.3225608 -4.2953224 -4.2761569][-4.1990166 -4.2278986 -4.2334 -4.2235155 -4.2064013 -4.1972647 -4.2114515 -4.2429061 -4.2724524 -4.2889972 -4.3007607 -4.3026414 -4.2841887 -4.2525415 -4.2298064][-4.1403742 -4.1846938 -4.1977382 -4.1877084 -4.1753159 -4.1702814 -4.1839418 -4.2144508 -4.2411075 -4.2530274 -4.2599468 -4.2618937 -4.2471752 -4.2199745 -4.2025352][-4.11223 -4.16136 -4.1781645 -4.1694989 -4.16357 -4.1606336 -4.1685 -4.1888661 -4.2059164 -4.2133031 -4.216948 -4.2217135 -4.2164431 -4.202723 -4.1984735][-4.1554003 -4.1891832 -4.1974888 -4.1904812 -4.1833715 -4.1701336 -4.1606507 -4.1659265 -4.1793456 -4.1844716 -4.1856451 -4.1925869 -4.19858 -4.1979365 -4.2058272][-4.2202358 -4.225934 -4.2160974 -4.2006707 -4.1824837 -4.1496315 -4.1169629 -4.1104903 -4.1255627 -4.1324539 -4.1349597 -4.1456575 -4.16166 -4.171176 -4.1947861][-4.2479835 -4.2327876 -4.203084 -4.1703091 -4.1306558 -4.0745373 -4.0292068 -4.0228481 -4.0458884 -4.0574341 -4.0654655 -4.08536 -4.1106691 -4.1331582 -4.1728024][-4.2510118 -4.2281203 -4.1816263 -4.1271372 -4.060535 -3.9902027 -3.9488044 -3.9546194 -3.9875414 -4.0080547 -4.0233741 -4.046699 -4.0759306 -4.1129947 -4.1658688][-4.2536902 -4.2306023 -4.1779 -4.1133118 -4.0359836 -3.9688025 -3.9380343 -3.9488091 -3.9855509 -4.0189371 -4.0441365 -4.0665889 -4.0948238 -4.1405478 -4.19753][-4.2532191 -4.2365131 -4.1922512 -4.1404486 -4.0802755 -4.0307927 -4.01089 -4.0227461 -4.05645 -4.0990353 -4.13245 -4.1531472 -4.17954 -4.2197852 -4.2601204][-4.2480955 -4.2386351 -4.2082853 -4.181263 -4.1515317 -4.1279545 -4.1225014 -4.1388164 -4.1708293 -4.2091966 -4.2352757 -4.2500396 -4.2665067 -4.2884336 -4.308147][-4.2313967 -4.2305112 -4.2206211 -4.2231283 -4.2249713 -4.2248659 -4.2329559 -4.2498736 -4.2740536 -4.2968616 -4.3065162 -4.3101916 -4.3148646 -4.3255258 -4.333992][-4.1903706 -4.2012362 -4.218039 -4.248106 -4.2755103 -4.292161 -4.3028131 -4.3113933 -4.3236609 -4.334219 -4.3353209 -4.3353019 -4.3381023 -4.3442969 -4.3488679][-4.1341119 -4.1549463 -4.1940231 -4.2459116 -4.2889667 -4.312161 -4.3199272 -4.3216095 -4.3281417 -4.3348489 -4.335639 -4.336432 -4.3400025 -4.3449926 -4.348033][-4.0910053 -4.1158633 -4.1625948 -4.2222509 -4.2720323 -4.2983732 -4.3080797 -4.3126264 -4.3185205 -4.3247747 -4.3273911 -4.3295789 -4.3326879 -4.3351293 -4.3365712]]...]
INFO - root - 2017-12-06 07:12:46.654063: step 7710, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 56h:38m:57s remains)
INFO - root - 2017-12-06 07:12:53.017005: step 7720, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:23m:59s remains)
INFO - root - 2017-12-06 07:12:59.483932: step 7730, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 55h:48m:10s remains)
INFO - root - 2017-12-06 07:13:05.832747: step 7740, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:03m:59s remains)
INFO - root - 2017-12-06 07:13:12.180814: step 7750, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 57h:42m:14s remains)
INFO - root - 2017-12-06 07:13:18.568453: step 7760, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 59h:14m:36s remains)
INFO - root - 2017-12-06 07:13:24.957330: step 7770, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:00m:13s remains)
INFO - root - 2017-12-06 07:13:31.532708: step 7780, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 59h:31m:54s remains)
INFO - root - 2017-12-06 07:13:37.942853: step 7790, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 56h:10m:16s remains)
INFO - root - 2017-12-06 07:13:44.354594: step 7800, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:14m:21s remains)
2017-12-06 07:13:45.027351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520657 -4.2404594 -4.2382097 -4.2311168 -4.2226229 -4.2233963 -4.2230792 -4.2174935 -4.2136559 -4.2129211 -4.2148333 -4.2261977 -4.2484345 -4.26307 -4.2661848][-4.2169504 -4.2069144 -4.2161131 -4.2178736 -4.2152386 -4.2214661 -4.2219238 -4.2133803 -4.2058544 -4.203217 -4.2048974 -4.2109671 -4.2299495 -4.2439985 -4.248765][-4.1876869 -4.176671 -4.1952696 -4.2077169 -4.2064781 -4.2073355 -4.1983142 -4.1832767 -4.1796565 -4.1853433 -4.1948771 -4.201376 -4.2169867 -4.2284126 -4.2353687][-4.1822991 -4.1704011 -4.1867094 -4.1992331 -4.1909623 -4.179358 -4.1560793 -4.1354756 -4.1439776 -4.1668768 -4.1877222 -4.1988463 -4.2111406 -4.2209458 -4.2325578][-4.1915445 -4.1824031 -4.1908889 -4.1999135 -4.18637 -4.1616507 -4.1234751 -4.0971875 -4.1223063 -4.1651616 -4.1945424 -4.2097321 -4.2183738 -4.2242961 -4.2381663][-4.1943846 -4.1862106 -4.189528 -4.1926737 -4.1699529 -4.1270146 -4.0628257 -4.0157318 -4.055963 -4.1289482 -4.1756535 -4.2028103 -4.2173457 -4.2269416 -4.2453589][-4.1904049 -4.1836395 -4.1797972 -4.1681175 -4.1245308 -4.0515995 -3.9436774 -3.8540149 -3.9141958 -4.034935 -4.1153736 -4.1615777 -4.1877756 -4.202229 -4.2235589][-4.1979575 -4.1948195 -4.1850834 -4.1572185 -4.0948772 -3.9979026 -3.8564563 -3.7209561 -3.7899213 -3.951587 -4.0555668 -4.1139264 -4.1479912 -4.1681004 -4.1873274][-4.2242317 -4.2271452 -4.2179089 -4.1907005 -4.1363983 -4.0587649 -3.9541652 -3.8545234 -3.8987236 -4.0233784 -4.1066957 -4.1491122 -4.1738591 -4.183979 -4.1890783][-4.2519517 -4.2617378 -4.2571239 -4.2367392 -4.2008991 -4.152277 -4.0893254 -4.0284224 -4.0468907 -4.1284223 -4.19001 -4.2191377 -4.2354016 -4.2387033 -4.2349467][-4.2623186 -4.2772079 -4.2804642 -4.2701774 -4.2507224 -4.2218237 -4.1803284 -4.1385374 -4.1384692 -4.1891127 -4.2349958 -4.2581234 -4.2719874 -4.2752523 -4.2715569][-4.2726769 -4.2929535 -4.3021522 -4.2964611 -4.2828193 -4.2592592 -4.2264767 -4.1979995 -4.1932373 -4.2231951 -4.2550526 -4.2699561 -4.2796803 -4.2824311 -4.2794461][-4.2784042 -4.3020234 -4.3161922 -4.314847 -4.3044744 -4.2863965 -4.2662196 -4.2540493 -4.2526436 -4.26748 -4.2837782 -4.29041 -4.2929831 -4.2917409 -4.286622][-4.27434 -4.2969751 -4.3147669 -4.3177609 -4.310452 -4.2987213 -4.28933 -4.2877359 -4.28886 -4.2937584 -4.3008714 -4.3052168 -4.3067174 -4.3050508 -4.3008356][-4.2688146 -4.2902427 -4.30786 -4.3134847 -4.3076158 -4.3011456 -4.2997694 -4.3033228 -4.304996 -4.3055358 -4.3074951 -4.310822 -4.3128929 -4.313024 -4.3117237]]...]
INFO - root - 2017-12-06 07:13:51.409779: step 7810, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:19m:28s remains)
INFO - root - 2017-12-06 07:13:57.895837: step 7820, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:46m:43s remains)
INFO - root - 2017-12-06 07:14:04.192759: step 7830, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:56m:15s remains)
INFO - root - 2017-12-06 07:14:10.592985: step 7840, loss = 2.03, batch loss = 1.97 (12.9 examples/sec; 0.619 sec/batch; 55h:47m:48s remains)
INFO - root - 2017-12-06 07:14:16.897439: step 7850, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 55h:04m:32s remains)
INFO - root - 2017-12-06 07:14:23.193635: step 7860, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 56h:58m:48s remains)
INFO - root - 2017-12-06 07:14:29.445227: step 7870, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.617 sec/batch; 55h:39m:42s remains)
INFO - root - 2017-12-06 07:14:35.834112: step 7880, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 57h:35m:45s remains)
INFO - root - 2017-12-06 07:14:42.241443: step 7890, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 56h:51m:15s remains)
INFO - root - 2017-12-06 07:14:48.676302: step 7900, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 61h:06m:26s remains)
2017-12-06 07:14:48.885292: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:14:48.885329: E tensorflow/core/util/events_writer.cc:131] Failed to flush 30 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:14:49.302441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2728181 -4.2637253 -4.263031 -4.2662435 -4.2697196 -4.2722635 -4.2721162 -4.2710614 -4.2743177 -4.2823496 -4.2884321 -4.2897782 -4.2848997 -4.2812653 -4.2800169][-4.2275209 -4.2161541 -4.2191744 -4.2261915 -4.2304134 -4.2322679 -4.2328157 -4.2350006 -4.24243 -4.2562561 -4.2675481 -4.2710757 -4.264751 -4.2581367 -4.252202][-4.1866632 -4.1771379 -4.1865735 -4.1962628 -4.1997023 -4.1984477 -4.1983709 -4.202961 -4.2138286 -4.2317634 -4.2466044 -4.2510362 -4.2424579 -4.2318153 -4.2213936][-4.1675239 -4.1648369 -4.1835475 -4.1951051 -4.1938682 -4.1853366 -4.1794696 -4.1835942 -4.1971064 -4.2196088 -4.2389722 -4.2449255 -4.2349606 -4.2214665 -4.2073994][-4.1694403 -4.1734447 -4.195796 -4.207068 -4.1999516 -4.1818271 -4.1680317 -4.1661787 -4.1778016 -4.20359 -4.2286668 -4.2379022 -4.2301965 -4.2184691 -4.2056718][-4.1736412 -4.1814289 -4.2007036 -4.2098289 -4.199182 -4.1770568 -4.158637 -4.1515422 -4.1578155 -4.1831846 -4.209249 -4.2202077 -4.2162347 -4.212738 -4.2091036][-4.1853905 -4.1948066 -4.2072177 -4.2098188 -4.1951108 -4.1726675 -4.1540546 -4.1449022 -4.1465416 -4.1664753 -4.1875234 -4.1983223 -4.1998115 -4.2063031 -4.2131081][-4.1910934 -4.1984634 -4.2044287 -4.202755 -4.1876612 -4.1700516 -4.155654 -4.1473579 -4.1444688 -4.1553621 -4.1695828 -4.1783857 -4.1843605 -4.1960006 -4.2065387][-4.1824784 -4.1885419 -4.1920805 -4.1920505 -4.1836629 -4.1745992 -4.1679177 -4.1593671 -4.1482129 -4.1507616 -4.1579232 -4.1621675 -4.1691785 -4.181365 -4.1898742][-4.1633329 -4.1709523 -4.17766 -4.185256 -4.1873307 -4.1867375 -4.1849284 -4.1729059 -4.1527572 -4.1496525 -4.1508636 -4.14789 -4.1513677 -4.1620803 -4.1660256][-4.141685 -4.1541724 -4.1685719 -4.1852937 -4.1946807 -4.1968908 -4.1942091 -4.179348 -4.1536632 -4.1435151 -4.1387086 -4.126935 -4.1231642 -4.1319976 -4.13682][-4.1263218 -4.1413093 -4.1613626 -4.1841269 -4.1963291 -4.1964746 -4.1909409 -4.1755419 -4.1486125 -4.1351309 -4.1293497 -4.1131797 -4.1041131 -4.1137815 -4.1237593][-4.11454 -4.1294684 -4.1530662 -4.1783342 -4.1917477 -4.1890593 -4.1805429 -4.1628008 -4.1383576 -4.1249814 -4.119925 -4.1080074 -4.1022854 -4.114799 -4.1295033][-4.1150408 -4.1275492 -4.1508613 -4.1769814 -4.1919246 -4.1904368 -4.18049 -4.1606874 -4.1416512 -4.1323271 -4.1320572 -4.1289635 -4.1294174 -4.1420903 -4.1560698][-4.1417527 -4.1508951 -4.1681123 -4.1892176 -4.2033491 -4.2028203 -4.1904783 -4.1697645 -4.156116 -4.1526155 -4.1568689 -4.1585751 -4.1613479 -4.1727915 -4.1858258]]...]
INFO - root - 2017-12-06 07:14:55.811441: step 7910, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:18m:04s remains)
INFO - root - 2017-12-06 07:15:02.124697: step 7920, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.612 sec/batch; 55h:11m:41s remains)
INFO - root - 2017-12-06 07:15:08.519861: step 7930, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:01m:01s remains)
INFO - root - 2017-12-06 07:15:14.859539: step 7940, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.616 sec/batch; 55h:34m:13s remains)
INFO - root - 2017-12-06 07:15:21.219243: step 7950, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 56h:20m:29s remains)
INFO - root - 2017-12-06 07:15:27.358591: step 7960, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 56h:35m:14s remains)
INFO - root - 2017-12-06 07:15:33.763528: step 7970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 58h:59m:29s remains)
INFO - root - 2017-12-06 07:15:39.937562: step 7980, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:34m:50s remains)
INFO - root - 2017-12-06 07:15:46.234604: step 7990, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 57h:14m:16s remains)
INFO - root - 2017-12-06 07:15:52.542107: step 8000, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 56h:31m:39s remains)
2017-12-06 07:15:53.190812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.231071 -4.2218614 -4.2295141 -4.2351904 -4.2249455 -4.20979 -4.2050681 -4.2111421 -4.2176976 -4.2282724 -4.2492375 -4.26515 -4.26453 -4.2552972 -4.2487755][-4.2181873 -4.2085452 -4.2134571 -4.2115531 -4.1904616 -4.1653285 -4.1514764 -4.1557631 -4.1719232 -4.1967034 -4.230197 -4.2547078 -4.2578807 -4.2527571 -4.25033][-4.2053256 -4.1959805 -4.1959338 -4.1871552 -4.1594667 -4.1235666 -4.0962629 -4.0935597 -4.1204095 -4.164175 -4.2087626 -4.2387815 -4.2464294 -4.247087 -4.2501516][-4.198308 -4.1904087 -4.18636 -4.1724596 -4.140873 -4.0957174 -4.0513716 -4.0339446 -4.0655065 -4.1273818 -4.1846037 -4.219625 -4.2327085 -4.2389035 -4.2462645][-4.1928115 -4.1850863 -4.1774559 -4.1598558 -4.1263371 -4.0749364 -4.0124917 -3.9756556 -4.0066814 -4.0824394 -4.1515722 -4.1930466 -4.2140231 -4.2273707 -4.2381477][-4.1905575 -4.1804261 -4.1643209 -4.1384897 -4.0983987 -4.0380597 -3.9568872 -3.8970385 -3.9231169 -4.01415 -4.1002779 -4.1555948 -4.1915894 -4.2163763 -4.2322903][-4.192111 -4.1790214 -4.1548281 -4.1187067 -4.0712228 -4.0061703 -3.9182191 -3.8431854 -3.8588674 -3.9522693 -4.0457535 -4.1123972 -4.1644044 -4.2045069 -4.2283683][-4.1920128 -4.1786475 -4.1534648 -4.1158767 -4.0720711 -4.0215783 -3.9562888 -3.8974991 -3.8952727 -3.9552948 -4.0283275 -4.088037 -4.1404552 -4.1876912 -4.2181282][-4.1863861 -4.1708927 -4.1484094 -4.1153045 -4.0809178 -4.0521688 -4.0199642 -3.986062 -3.9742072 -3.9978213 -4.0385571 -4.0787516 -4.1169639 -4.1562624 -4.1879787][-4.1785979 -4.1586566 -4.1348267 -4.10312 -4.075911 -4.0654593 -4.0617266 -4.0476274 -4.0339813 -4.0384641 -4.0575237 -4.0820274 -4.103385 -4.1310372 -4.1617551][-4.175529 -4.147851 -4.1180577 -4.0849204 -4.0653772 -4.0724664 -4.090878 -4.0894146 -4.07557 -4.0715289 -4.0829992 -4.098588 -4.108037 -4.1284657 -4.157999][-4.1801424 -4.1477776 -4.1131091 -4.0803876 -4.0701337 -4.0927172 -4.1239285 -4.1257691 -4.1086149 -4.0987272 -4.1089659 -4.1187925 -4.1235533 -4.1371288 -4.1569896][-4.1945562 -4.1657705 -4.1320815 -4.1034327 -4.1012316 -4.1340342 -4.1658869 -4.1612415 -4.1374717 -4.1214719 -4.1280403 -4.1347079 -4.1417832 -4.1547551 -4.163434][-4.2195263 -4.1994658 -4.1699448 -4.1454291 -4.1497135 -4.1841164 -4.2085156 -4.194427 -4.1620817 -4.1388659 -4.1407 -4.1456075 -4.1587243 -4.1741939 -4.1757331][-4.2436209 -4.233604 -4.2118211 -4.194221 -4.2035203 -4.2339411 -4.2479506 -4.2232032 -4.1830254 -4.1582079 -4.16104 -4.1683154 -4.1851978 -4.2034292 -4.2030559]]...]
INFO - root - 2017-12-06 07:15:59.508379: step 8010, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:25m:14s remains)
INFO - root - 2017-12-06 07:16:05.877481: step 8020, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 56h:43m:09s remains)
INFO - root - 2017-12-06 07:16:12.282474: step 8030, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 56h:39m:36s remains)
INFO - root - 2017-12-06 07:16:18.567355: step 8040, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 58h:21m:18s remains)
INFO - root - 2017-12-06 07:16:24.937280: step 8050, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 56h:47m:42s remains)
INFO - root - 2017-12-06 07:16:31.164431: step 8060, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:13m:21s remains)
INFO - root - 2017-12-06 07:16:37.510416: step 8070, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 57h:24m:10s remains)
INFO - root - 2017-12-06 07:16:43.929160: step 8080, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 57h:27m:53s remains)
INFO - root - 2017-12-06 07:16:50.173895: step 8090, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:16m:40s remains)
INFO - root - 2017-12-06 07:16:56.519095: step 8100, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 55h:19m:41s remains)
2017-12-06 07:16:56.728975: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:16:56.729014: E tensorflow/core/util/events_writer.cc:131] Failed to flush 32 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:16:57.151905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2044411 -4.1642833 -4.1253428 -4.10691 -4.1148052 -4.12875 -4.1301394 -4.11545 -4.1116266 -4.117682 -4.1194558 -4.1265621 -4.141902 -4.1665583 -4.1960559][-4.20047 -4.16317 -4.1243496 -4.0993638 -4.0998034 -4.1122489 -4.1110487 -4.0929275 -4.0938873 -4.1109958 -4.1220517 -4.1345057 -4.1464205 -4.1628432 -4.18754][-4.1890054 -4.1620779 -4.1323333 -4.1071863 -4.0978403 -4.1040034 -4.0982237 -4.0805187 -4.0900559 -4.1162267 -4.1347232 -4.1539726 -4.1638441 -4.1701455 -4.1865439][-4.172451 -4.1608486 -4.1437678 -4.1214123 -4.1028233 -4.0944433 -4.0774322 -4.0578203 -4.07594 -4.111794 -4.1381874 -4.1672707 -4.1799593 -4.1799636 -4.1891747][-4.1501479 -4.1519608 -4.1480722 -4.1311269 -4.1064258 -4.0804539 -4.0435934 -4.0132074 -4.0366297 -4.0887213 -4.1280723 -4.167407 -4.1863351 -4.1863227 -4.193222][-4.1309571 -4.1375041 -4.1397777 -4.1276684 -4.1019669 -4.0626183 -4.0019197 -3.9484825 -3.9702685 -4.0444121 -4.1039557 -4.1552329 -4.1825027 -4.1868343 -4.1983113][-4.1326542 -4.1368818 -4.13491 -4.1213403 -4.094986 -4.0487232 -3.9711952 -3.8932223 -3.908545 -4.0008979 -4.0798459 -4.140727 -4.1736383 -4.1827989 -4.2032309][-4.1606584 -4.1542349 -4.1382718 -4.1164279 -4.0892329 -4.048902 -3.9766836 -3.8983564 -3.905313 -3.994416 -4.0736322 -4.1337347 -4.1684361 -4.1833677 -4.211659][-4.195478 -4.1778193 -4.1483865 -4.1172009 -4.0923433 -4.0692492 -4.023694 -3.9709625 -3.97723 -4.0415616 -4.0998445 -4.1466551 -4.1780548 -4.1963782 -4.2277842][-4.2213273 -4.1981349 -4.1635 -4.1292295 -4.1088028 -4.1020856 -4.0852571 -4.0616021 -4.0733232 -4.1132789 -4.1464615 -4.1763091 -4.2010441 -4.2193184 -4.2478833][-4.2306032 -4.2061539 -4.171351 -4.137815 -4.1229353 -4.1305017 -4.1377 -4.1369658 -4.151886 -4.1726313 -4.1868963 -4.202251 -4.2189522 -4.2331414 -4.2576246][-4.2312226 -4.2069736 -4.1747146 -4.1441746 -4.1334753 -4.1509924 -4.1728206 -4.1840534 -4.197576 -4.2070651 -4.2100658 -4.2163272 -4.22632 -4.2364688 -4.2573185][-4.2314777 -4.2090955 -4.1828594 -4.1585288 -4.1498094 -4.1688023 -4.1951118 -4.2092028 -4.2183242 -4.2223945 -4.2207065 -4.2226534 -4.22808 -4.2360191 -4.2527614][-4.2382855 -4.2215524 -4.2052813 -4.1896663 -4.1841235 -4.1984243 -4.2210879 -4.2321606 -4.2354774 -4.2337041 -4.2273836 -4.2260919 -4.2284594 -4.2351284 -4.2470746][-4.2499413 -4.2403312 -4.2362542 -4.22971 -4.2252235 -4.2334805 -4.2504363 -4.258831 -4.2571311 -4.2492914 -4.2372484 -4.2322764 -4.2344513 -4.2382474 -4.2432523]]...]
INFO - root - 2017-12-06 07:17:03.579612: step 8110, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 59h:03m:19s remains)
INFO - root - 2017-12-06 07:17:09.934617: step 8120, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:49m:32s remains)
INFO - root - 2017-12-06 07:17:16.471357: step 8130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:18m:31s remains)
INFO - root - 2017-12-06 07:17:22.820503: step 8140, loss = 2.02, batch loss = 1.96 (12.6 examples/sec; 0.635 sec/batch; 57h:11m:24s remains)
INFO - root - 2017-12-06 07:17:29.117024: step 8150, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 56h:46m:44s remains)
INFO - root - 2017-12-06 07:17:35.379320: step 8160, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:22m:07s remains)
INFO - root - 2017-12-06 07:17:41.764512: step 8170, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:13m:30s remains)
INFO - root - 2017-12-06 07:17:48.088572: step 8180, loss = 2.08, batch loss = 2.02 (15.2 examples/sec; 0.526 sec/batch; 47h:25m:06s remains)
INFO - root - 2017-12-06 07:17:52.140205: step 8190, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.422 sec/batch; 38h:01m:23s remains)
INFO - root - 2017-12-06 07:17:56.548153: step 8200, loss = 2.07, batch loss = 2.02 (17.6 examples/sec; 0.456 sec/batch; 41h:02m:11s remains)
2017-12-06 07:17:57.038500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2186704 -4.2165413 -4.2300024 -4.2440925 -4.2497087 -4.2420874 -4.2358589 -4.240881 -4.24892 -4.2501888 -4.2451763 -4.2399535 -4.2397346 -4.24325 -4.2445073][-4.1825314 -4.1792531 -4.2004271 -4.2232819 -4.2282605 -4.2158084 -4.1998162 -4.192512 -4.1998897 -4.2052932 -4.1961069 -4.1889482 -4.1951542 -4.20364 -4.2040539][-4.1788974 -4.167902 -4.1839709 -4.2028408 -4.2016721 -4.1837025 -4.1579094 -4.1444058 -4.15885 -4.172893 -4.1639471 -4.1548748 -4.1609945 -4.1677885 -4.1644592][-4.1937656 -4.1776347 -4.1812048 -4.1855807 -4.1762581 -4.1548615 -4.1258006 -4.116044 -4.1384773 -4.1604528 -4.1491747 -4.1418376 -4.1450343 -4.1448059 -4.1372442][-4.1888819 -4.1729836 -4.17266 -4.1673436 -4.1524811 -4.1306806 -4.1033998 -4.0965924 -4.1237512 -4.1483955 -4.142962 -4.1442661 -4.1461306 -4.14043 -4.1333609][-4.1628513 -4.1438584 -4.14499 -4.1399755 -4.11586 -4.0759573 -4.0415349 -4.0452738 -4.077446 -4.1084375 -4.1223946 -4.1430368 -4.1580033 -4.1567626 -4.1536326][-4.1088572 -4.0885291 -4.094439 -4.0973091 -4.0635438 -3.9898427 -3.9311171 -3.9501128 -4.0011158 -4.0462875 -4.0852323 -4.1281924 -4.1575365 -4.1644411 -4.1655731][-4.0401707 -4.0232859 -4.0471835 -4.0678816 -4.0365081 -3.9533198 -3.880157 -3.8959982 -3.9655766 -4.0258074 -4.0826921 -4.1297579 -4.1533251 -4.158576 -4.1614704][-3.9943633 -3.9906566 -4.0416155 -4.0805821 -4.067461 -4.0111079 -3.9461629 -3.9422088 -3.9993496 -4.0618978 -4.119247 -4.1515183 -4.1594729 -4.1522388 -4.15198][-4.03318 -4.0397549 -4.0995331 -4.1417179 -4.1378655 -4.0997958 -4.0485573 -4.03186 -4.0641532 -4.1142626 -4.16071 -4.1846476 -4.184989 -4.171011 -4.1664586][-4.1236649 -4.1301742 -4.1785231 -4.212523 -4.2120862 -4.1818094 -4.1421552 -4.122683 -4.1345153 -4.1638651 -4.1945004 -4.2169824 -4.2238336 -4.2136211 -4.2078872][-4.2010775 -4.2049227 -4.2378755 -4.2609458 -4.2589669 -4.2355084 -4.2121167 -4.2019448 -4.2078981 -4.2195864 -4.2309079 -4.2436466 -4.2527924 -4.24833 -4.2433963][-4.2362194 -4.2453208 -4.2656436 -4.2785344 -4.2748618 -4.2601995 -4.2477555 -4.2458496 -4.2492757 -4.251421 -4.25487 -4.2608962 -4.2662659 -4.2649012 -4.2638121][-4.2649364 -4.276041 -4.2897339 -4.2949128 -4.2857895 -4.275497 -4.2714977 -4.2721529 -4.2737203 -4.2710681 -4.2711139 -4.2765169 -4.2838116 -4.2873669 -4.2877154][-4.2957258 -4.30289 -4.3065934 -4.3055329 -4.2971191 -4.2910495 -4.2914047 -4.2950244 -4.2968674 -4.293488 -4.291543 -4.2963696 -4.3043237 -4.3079195 -4.3066111]]...]
INFO - root - 2017-12-06 07:18:01.362244: step 8210, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:31m:27s remains)
INFO - root - 2017-12-06 07:18:05.643864: step 8220, loss = 2.06, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 38h:20m:20s remains)
INFO - root - 2017-12-06 07:18:09.993444: step 8230, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.445 sec/batch; 40h:04m:27s remains)
INFO - root - 2017-12-06 07:18:14.355004: step 8240, loss = 2.06, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 38h:32m:33s remains)
INFO - root - 2017-12-06 07:18:18.674166: step 8250, loss = 2.05, batch loss = 2.00 (18.3 examples/sec; 0.436 sec/batch; 39h:17m:46s remains)
INFO - root - 2017-12-06 07:18:22.994987: step 8260, loss = 2.05, batch loss = 1.99 (18.5 examples/sec; 0.433 sec/batch; 39h:01m:30s remains)
INFO - root - 2017-12-06 07:18:27.343724: step 8270, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 39h:57m:03s remains)
INFO - root - 2017-12-06 07:18:31.669106: step 8280, loss = 2.06, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 39h:56m:35s remains)
INFO - root - 2017-12-06 07:18:35.990348: step 8290, loss = 2.08, batch loss = 2.02 (17.6 examples/sec; 0.456 sec/batch; 41h:01m:56s remains)
INFO - root - 2017-12-06 07:18:39.986675: step 8300, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:35m:56s remains)
2017-12-06 07:18:40.497059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1970391 -4.1712584 -4.1716495 -4.1822524 -4.1755872 -4.1682177 -4.1897235 -4.2170954 -4.2348275 -4.2429404 -4.2419863 -4.2367654 -4.2328043 -4.2195158 -4.2072754][-4.19106 -4.163723 -4.1650295 -4.176394 -4.1666527 -4.1528492 -4.168592 -4.1963363 -4.2158117 -4.225811 -4.2269316 -4.2210803 -4.2135644 -4.1905923 -4.1690278][-4.183845 -4.1516695 -4.1514153 -4.1626329 -4.1541886 -4.1383276 -4.1486354 -4.1731687 -4.1973343 -4.2148051 -4.2228661 -4.2200046 -4.2078633 -4.1704288 -4.1363773][-4.1849928 -4.1458569 -4.1357088 -4.1403913 -4.1301665 -4.1138625 -4.1196384 -4.1402535 -4.1656227 -4.1893 -4.2039814 -4.2048874 -4.1908212 -4.1447582 -4.1040859][-4.1911139 -4.146574 -4.1201439 -4.11073 -4.0940418 -4.0726452 -4.0682707 -4.0771985 -4.0974336 -4.12939 -4.1564841 -4.1676183 -4.1570864 -4.1150527 -4.0812678][-4.193152 -4.1443834 -4.1031437 -4.0793409 -4.0566487 -4.0295811 -4.0093646 -3.9965582 -4.0077939 -4.0497994 -4.0961046 -4.1203637 -4.1190553 -4.0909405 -4.0749497][-4.1949563 -4.148911 -4.1022134 -4.0709462 -4.0474944 -4.02167 -3.9890723 -3.9549606 -3.9532146 -4.0027676 -4.0664134 -4.0973029 -4.105238 -4.0915108 -4.0938706][-4.1993303 -4.1676111 -4.1317048 -4.1049557 -4.0878263 -4.0697618 -4.03549 -3.9903524 -3.9728768 -4.0170155 -4.0806131 -4.1031094 -4.1073871 -4.0969148 -4.1084847][-4.2060051 -4.1861038 -4.1669874 -4.1545439 -4.1452451 -4.1354995 -4.1036115 -4.0563936 -4.0243516 -4.055141 -4.1071434 -4.1183944 -4.1150503 -4.0999651 -4.10441][-4.2128696 -4.197319 -4.1874685 -4.1830649 -4.1794939 -4.1708741 -4.1438551 -4.0994225 -4.0635815 -4.0779986 -4.1125226 -4.11767 -4.1107831 -4.0918856 -4.0866389][-4.2219033 -4.2060027 -4.1970925 -4.19426 -4.189887 -4.1780663 -4.149435 -4.1075668 -4.0777626 -4.0854435 -4.1068525 -4.1103616 -4.1095729 -4.0911098 -4.0746169][-4.2236633 -4.2065988 -4.2019529 -4.2025995 -4.1969037 -4.1827211 -4.1511788 -4.1125536 -4.0955372 -4.10306 -4.1131992 -4.1122289 -4.1174564 -4.1007404 -4.079505][-4.215775 -4.1987929 -4.19757 -4.2052078 -4.2055955 -4.1919193 -4.1566763 -4.120378 -4.112184 -4.122992 -4.1256914 -4.1184978 -4.1273408 -4.1164184 -4.09485][-4.2129483 -4.19303 -4.1884785 -4.1985021 -4.2051425 -4.1939206 -4.1612215 -4.1290827 -4.1297121 -4.1486073 -4.1529322 -4.1441808 -4.1507807 -4.1448693 -4.1242185][-4.2152042 -4.1899381 -4.1775608 -4.1860752 -4.1969814 -4.1909456 -4.1656108 -4.1416793 -4.1523042 -4.1791835 -4.1895027 -4.1812897 -4.1799493 -4.1658325 -4.1395478]]...]
INFO - root - 2017-12-06 07:18:44.760079: step 8310, loss = 2.07, batch loss = 2.01 (19.7 examples/sec; 0.407 sec/batch; 36h:38m:21s remains)
INFO - root - 2017-12-06 07:18:49.003158: step 8320, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.422 sec/batch; 38h:01m:43s remains)
INFO - root - 2017-12-06 07:18:53.245941: step 8330, loss = 2.08, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 37h:41m:57s remains)
INFO - root - 2017-12-06 07:18:57.574971: step 8340, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.440 sec/batch; 39h:35m:05s remains)
INFO - root - 2017-12-06 07:19:01.958229: step 8350, loss = 2.10, batch loss = 2.04 (18.7 examples/sec; 0.428 sec/batch; 38h:33m:36s remains)
INFO - root - 2017-12-06 07:19:06.188161: step 8360, loss = 2.07, batch loss = 2.02 (18.3 examples/sec; 0.438 sec/batch; 39h:27m:38s remains)
INFO - root - 2017-12-06 07:19:10.494584: step 8370, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.424 sec/batch; 38h:13m:01s remains)
INFO - root - 2017-12-06 07:19:14.874842: step 8380, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.438 sec/batch; 39h:28m:28s remains)
INFO - root - 2017-12-06 07:19:19.254977: step 8390, loss = 2.07, batch loss = 2.01 (19.4 examples/sec; 0.412 sec/batch; 37h:06m:45s remains)
INFO - root - 2017-12-06 07:19:23.295571: step 8400, loss = 2.07, batch loss = 2.01 (19.5 examples/sec; 0.410 sec/batch; 36h:52m:56s remains)
2017-12-06 07:19:23.446994: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:19:23.447031: E tensorflow/core/util/events_writer.cc:131] Failed to flush 35 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:19:23.783172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3516688 -4.3614292 -4.3649912 -4.3424644 -4.2779827 -4.1890292 -4.1172137 -4.0959396 -4.1303606 -4.1948748 -4.2530804 -4.2906828 -4.3050356 -4.304666 -4.2967715][-4.3564615 -4.3677425 -4.3761964 -4.363802 -4.3116198 -4.2303753 -4.1575217 -4.1243067 -4.1371713 -4.1794739 -4.22514 -4.2640033 -4.2878146 -4.2978091 -4.29638][-4.3663764 -4.3802934 -4.3933535 -4.3885522 -4.3491578 -4.2787042 -4.2055359 -4.1563764 -4.1425495 -4.1603417 -4.1951742 -4.2360973 -4.2721863 -4.2953978 -4.3038592][-4.3762989 -4.392715 -4.4077177 -4.4045944 -4.3714113 -4.306953 -4.2314572 -4.1675282 -4.1308513 -4.1293836 -4.1595311 -4.2062364 -4.2551985 -4.2947073 -4.3172092][-4.3825922 -4.3998756 -4.4147496 -4.4124718 -4.3796754 -4.315475 -4.2329626 -4.1553125 -4.1000814 -4.0854 -4.1175761 -4.1703777 -4.2318168 -4.28844 -4.3271337][-4.3857121 -4.4021029 -4.4126287 -4.4070549 -4.370749 -4.301589 -4.2115722 -4.1196246 -4.0450926 -4.0187864 -4.0578661 -4.122365 -4.195271 -4.2667074 -4.3198652][-4.3859096 -4.4002895 -4.40402 -4.3886304 -4.3420911 -4.2622313 -4.1622453 -4.0519238 -3.9527361 -3.9177892 -3.9754686 -4.0602551 -4.1479673 -4.23378 -4.3012948][-4.382051 -4.3926482 -4.3880892 -4.3591762 -4.2956243 -4.2008195 -4.0906792 -3.9667988 -3.8478649 -3.8139753 -3.8957107 -4.0019078 -4.1029892 -4.1989212 -4.2776875][-4.3760452 -4.3802719 -4.3646903 -4.3224487 -4.2442513 -4.1413503 -4.0335021 -3.9159818 -3.8028674 -3.7835929 -3.8756714 -3.9851823 -4.087245 -4.1851487 -4.2668996][-4.3697658 -4.3656559 -4.34145 -4.2910433 -4.2065372 -4.1069369 -4.0154848 -3.9243245 -3.8460875 -3.8497941 -3.9315052 -4.0255551 -4.1171002 -4.2066741 -4.2803373][-4.3640332 -4.3545928 -4.3276072 -4.2781076 -4.2005763 -4.1140828 -4.0453668 -3.9888103 -3.9522519 -3.9725535 -4.0354843 -4.1076474 -4.1826468 -4.255929 -4.3121505][-4.3607516 -4.3516607 -4.3300838 -4.2919741 -4.2322307 -4.1665959 -4.1192365 -4.0920763 -4.0853715 -4.1096029 -4.1516929 -4.2011366 -4.2548518 -4.3068733 -4.3416028][-4.3588567 -4.3528166 -4.3401804 -4.3176556 -4.2801495 -4.2372971 -4.2087178 -4.2006068 -4.2086625 -4.2296667 -4.2556033 -4.2852516 -4.3169909 -4.3446321 -4.3549604][-4.3574042 -4.3549461 -4.3502078 -4.3413582 -4.324059 -4.3023887 -4.2893529 -4.2902083 -4.3005371 -4.3153014 -4.3298268 -4.3449292 -4.3571811 -4.3620677 -4.3530188][-4.3558908 -4.3556581 -4.3548555 -4.3536177 -4.3493729 -4.3434758 -4.3414545 -4.3459234 -4.3535037 -4.36132 -4.3671846 -4.3703766 -4.3668814 -4.3567209 -4.3356438]]...]
INFO - root - 2017-12-06 07:19:28.175350: step 8410, loss = 2.05, batch loss = 2.00 (17.9 examples/sec; 0.447 sec/batch; 40h:13m:16s remains)
INFO - root - 2017-12-06 07:19:32.434049: step 8420, loss = 2.04, batch loss = 1.98 (18.2 examples/sec; 0.439 sec/batch; 39h:29m:54s remains)
INFO - root - 2017-12-06 07:19:36.746129: step 8430, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.426 sec/batch; 38h:18m:59s remains)
INFO - root - 2017-12-06 07:19:41.013793: step 8440, loss = 2.07, batch loss = 2.01 (18.8 examples/sec; 0.425 sec/batch; 38h:14m:58s remains)
INFO - root - 2017-12-06 07:19:45.376213: step 8450, loss = 2.03, batch loss = 1.97 (18.2 examples/sec; 0.439 sec/batch; 39h:28m:16s remains)
INFO - root - 2017-12-06 07:19:49.709024: step 8460, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 39h:19m:35s remains)
INFO - root - 2017-12-06 07:19:54.097536: step 8470, loss = 2.06, batch loss = 2.00 (18.4 examples/sec; 0.435 sec/batch; 39h:09m:08s remains)
INFO - root - 2017-12-06 07:19:58.458536: step 8480, loss = 2.08, batch loss = 2.02 (18.6 examples/sec; 0.430 sec/batch; 38h:43m:00s remains)
INFO - root - 2017-12-06 07:20:02.739475: step 8490, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 38h:04m:51s remains)
INFO - root - 2017-12-06 07:20:07.108022: step 8500, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 39h:44m:33s remains)
2017-12-06 07:20:07.624833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2349305 -4.2209349 -4.2155666 -4.2168603 -4.2041688 -4.1863871 -4.1820126 -4.1846647 -4.2032156 -4.2339773 -4.2580957 -4.2662368 -4.2626066 -4.2470479 -4.2204237][-4.21506 -4.2023797 -4.2030983 -4.2101183 -4.20119 -4.18647 -4.185298 -4.1908746 -4.2047353 -4.2271361 -4.2394476 -4.2367454 -4.2247062 -4.2075477 -4.1867023][-4.2070146 -4.1961861 -4.1988945 -4.205986 -4.2013822 -4.189961 -4.1885 -4.1907954 -4.2036023 -4.2253246 -4.230794 -4.2168841 -4.1946807 -4.1752834 -4.1595273][-4.2177377 -4.2067695 -4.2035685 -4.2014036 -4.1884007 -4.1695638 -4.1607971 -4.1627607 -4.1812077 -4.2073278 -4.2150116 -4.1964931 -4.1678505 -4.148159 -4.1360922][-4.2289248 -4.2172904 -4.2021747 -4.1829567 -4.1520958 -4.1137633 -4.0926991 -4.0979319 -4.1278963 -4.1649981 -4.1827917 -4.17323 -4.1484342 -4.1296854 -4.1205177][-4.231173 -4.2183905 -4.1897259 -4.1530523 -4.1014285 -4.0394306 -4.0008492 -4.0054312 -4.0447216 -4.0970349 -4.133009 -4.1430688 -4.1351748 -4.1234407 -4.1166434][-4.2296429 -4.2126322 -4.17051 -4.1164131 -4.0491142 -3.9724443 -3.9260263 -3.9263034 -3.9710486 -4.0408564 -4.0952759 -4.123642 -4.1326313 -4.1303573 -4.1275816][-4.2273912 -4.2020392 -4.1485572 -4.0864534 -4.0182757 -3.9493587 -3.9119034 -3.91302 -3.9546413 -4.0265102 -4.084434 -4.1178589 -4.1317058 -4.1372766 -4.1381435][-4.2366056 -4.2046695 -4.1476574 -4.089798 -4.0347209 -3.9887578 -3.9718959 -3.9803371 -4.0153303 -4.0744553 -4.1205792 -4.1432152 -4.1463494 -4.1465139 -4.14842][-4.2533665 -4.2205653 -4.1711955 -4.1272826 -4.0883265 -4.0681129 -4.0694518 -4.0796523 -4.1059818 -4.1453261 -4.168417 -4.1657948 -4.146678 -4.1355495 -4.138257][-4.2675915 -4.2383604 -4.2019582 -4.1728935 -4.1467614 -4.1418257 -4.1520009 -4.160737 -4.1753149 -4.1957607 -4.1956215 -4.1602635 -4.1148624 -4.0913372 -4.0961027][-4.27274 -4.2494526 -4.2248831 -4.2047977 -4.1851025 -4.1852822 -4.1948695 -4.1989 -4.2039957 -4.2084489 -4.1877766 -4.1291156 -4.0707045 -4.0476389 -4.0602736][-4.2731247 -4.2546353 -4.2334161 -4.2126274 -4.1919861 -4.1873455 -4.1910253 -4.1923265 -4.1917896 -4.1837816 -4.150816 -4.0871139 -4.0347047 -4.0217748 -4.0467548][-4.2757959 -4.2619057 -4.24235 -4.2170033 -4.1903028 -4.1764641 -4.1699686 -4.1653185 -4.1640344 -4.15479 -4.1241956 -4.0716476 -4.0340533 -4.0370364 -4.0689168][-4.2758036 -4.268795 -4.2541418 -4.2277508 -4.1950841 -4.1695929 -4.1506634 -4.1389794 -4.1410031 -4.1402435 -4.1211586 -4.084187 -4.0624709 -4.0788374 -4.1153717]]...]
INFO - root - 2017-12-06 07:20:11.700832: step 8510, loss = 2.10, batch loss = 2.04 (18.6 examples/sec; 0.429 sec/batch; 38h:37m:23s remains)
INFO - root - 2017-12-06 07:20:15.950470: step 8520, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:47m:13s remains)
INFO - root - 2017-12-06 07:20:20.344912: step 8530, loss = 2.06, batch loss = 2.00 (18.2 examples/sec; 0.439 sec/batch; 39h:32m:02s remains)
INFO - root - 2017-12-06 07:20:24.692233: step 8540, loss = 2.07, batch loss = 2.01 (17.8 examples/sec; 0.449 sec/batch; 40h:25m:45s remains)
INFO - root - 2017-12-06 07:20:29.203757: step 8550, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 39h:23m:38s remains)
INFO - root - 2017-12-06 07:20:33.438228: step 8560, loss = 2.06, batch loss = 2.01 (18.8 examples/sec; 0.424 sec/batch; 38h:11m:43s remains)
INFO - root - 2017-12-06 07:20:37.867141: step 8570, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.428 sec/batch; 38h:28m:02s remains)
INFO - root - 2017-12-06 07:20:42.166586: step 8580, loss = 2.07, batch loss = 2.02 (19.1 examples/sec; 0.419 sec/batch; 37h:43m:45s remains)
INFO - root - 2017-12-06 07:20:46.460824: step 8590, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.425 sec/batch; 38h:15m:20s remains)
INFO - root - 2017-12-06 07:20:50.742548: step 8600, loss = 2.03, batch loss = 1.97 (18.9 examples/sec; 0.423 sec/batch; 38h:02m:23s remains)
2017-12-06 07:20:51.293267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3350377 -4.3391495 -4.3379817 -4.331048 -4.3233037 -4.3186502 -4.3182979 -4.3234482 -4.3307047 -4.3355494 -4.3368554 -4.3367348 -4.3341694 -4.3307047 -4.3259864][-4.3359718 -4.3420243 -4.3438787 -4.338202 -4.3306193 -4.32516 -4.3246741 -4.3291435 -4.3353629 -4.3397202 -4.3409443 -4.34042 -4.3379192 -4.33459 -4.3302379][-4.3204355 -4.3267074 -4.3312335 -4.3273077 -4.3197432 -4.3132062 -4.3120904 -4.3150134 -4.3186855 -4.3223667 -4.32666 -4.3297415 -4.3296018 -4.3266568 -4.3222332][-4.2792468 -4.2847657 -4.2919035 -4.2918453 -4.2846088 -4.2772136 -4.2728505 -4.2711167 -4.2697439 -4.2706442 -4.2796717 -4.289062 -4.2942667 -4.2940845 -4.2917666][-4.2211089 -4.2250576 -4.23379 -4.2364373 -4.2281432 -4.2160144 -4.202105 -4.1909733 -4.1827126 -4.1796031 -4.192708 -4.2102413 -4.222086 -4.2250609 -4.2246861][-4.1487451 -4.152813 -4.1595545 -4.1632338 -4.1555958 -4.1324234 -4.0998363 -4.0771041 -4.0699081 -4.0695086 -4.0888424 -4.1174259 -4.1326866 -4.1312556 -4.1275516][-4.0689096 -4.0758619 -4.083869 -4.0906372 -4.0853071 -4.0470347 -3.9813631 -3.9413667 -3.9496834 -3.9701321 -4.0010839 -4.0362988 -4.0502372 -4.0410008 -4.030652][-4.0110321 -4.0228357 -4.0310369 -4.0340509 -4.0258946 -3.9702129 -3.8739893 -3.824872 -3.8672118 -3.9248297 -3.9682074 -3.9974318 -4.0047588 -3.9889479 -3.9729407][-3.996002 -4.011332 -4.0166693 -4.0114875 -4.0002356 -3.9567142 -3.8902373 -3.8702998 -3.9240503 -3.9807367 -4.00714 -4.0121503 -4.005373 -3.9886065 -3.9738176][-4.04308 -4.0564938 -4.0583582 -4.0507402 -4.0487885 -4.0391359 -4.0194912 -4.0213175 -4.0553164 -4.0799294 -4.0740471 -4.0509191 -4.0313153 -4.0235443 -4.0189219][-4.12174 -4.1210613 -4.1132026 -4.1090736 -4.1171384 -4.1275868 -4.1305542 -4.1394596 -4.1543412 -4.1552329 -4.1351748 -4.10819 -4.0913472 -4.0912828 -4.0902596][-4.1639466 -4.1491308 -4.137856 -4.14258 -4.163362 -4.1867838 -4.1956043 -4.1998949 -4.2010765 -4.1909966 -4.1737452 -4.1625166 -4.1569023 -4.1598353 -4.1577921][-4.1482968 -4.1343379 -4.1353154 -4.1526175 -4.1845269 -4.2123208 -4.2188973 -4.2148781 -4.2063189 -4.1955895 -4.1877789 -4.1910162 -4.1985865 -4.207 -4.2079015][-4.1267576 -4.1189008 -4.1307988 -4.1520672 -4.1809297 -4.2044363 -4.2066965 -4.1993651 -4.1933327 -4.1883965 -4.1895595 -4.1992259 -4.2136288 -4.2269287 -4.2328596][-4.1394238 -4.1314774 -4.1391907 -4.1540804 -4.170918 -4.1853004 -4.1844058 -4.1758165 -4.1748853 -4.178926 -4.1898441 -4.2039433 -4.2214837 -4.2365155 -4.2432308]]...]
INFO - root - 2017-12-06 07:20:55.561396: step 8610, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 39h:58m:01s remains)
INFO - root - 2017-12-06 07:21:00.000078: step 8620, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:28m:17s remains)
INFO - root - 2017-12-06 07:21:04.299907: step 8630, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.421 sec/batch; 37h:51m:58s remains)
INFO - root - 2017-12-06 07:21:08.652273: step 8640, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.455 sec/batch; 40h:58m:30s remains)
INFO - root - 2017-12-06 07:21:12.905784: step 8650, loss = 2.04, batch loss = 1.98 (18.6 examples/sec; 0.429 sec/batch; 38h:37m:37s remains)
INFO - root - 2017-12-06 07:21:17.187905: step 8660, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:02m:52s remains)
INFO - root - 2017-12-06 07:21:21.572718: step 8670, loss = 2.08, batch loss = 2.02 (17.4 examples/sec; 0.461 sec/batch; 41h:28m:22s remains)
INFO - root - 2017-12-06 07:21:25.922603: step 8680, loss = 2.06, batch loss = 2.00 (17.4 examples/sec; 0.459 sec/batch; 41h:16m:25s remains)
INFO - root - 2017-12-06 07:21:30.190829: step 8690, loss = 2.08, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:13m:53s remains)
INFO - root - 2017-12-06 07:21:34.522096: step 8700, loss = 2.03, batch loss = 1.97 (18.9 examples/sec; 0.423 sec/batch; 38h:03m:59s remains)
2017-12-06 07:21:34.691003: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:21:34.691061: E tensorflow/core/util/events_writer.cc:131] Failed to flush 38 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:21:35.144371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2361474 -4.2358389 -4.23427 -4.2327461 -4.2295418 -4.2282209 -4.228651 -4.2307692 -4.2355504 -4.23879 -4.238945 -4.2413044 -4.245122 -4.2495742 -4.2537012][-4.2142453 -4.2116041 -4.208704 -4.2043052 -4.1996441 -4.1992903 -4.2010036 -4.2046528 -4.2114973 -4.2162142 -4.2170653 -4.2218862 -4.2302394 -4.2395148 -4.246582][-4.1919417 -4.1847596 -4.1766129 -4.1664929 -4.1584458 -4.156446 -4.1581154 -4.162971 -4.174376 -4.183938 -4.1880741 -4.1991186 -4.2148819 -4.2294021 -4.2388163][-4.1736879 -4.1603575 -4.1456428 -4.1248717 -4.10643 -4.0932856 -4.0865974 -4.0906944 -4.1080365 -4.1292162 -4.144979 -4.1682663 -4.1963906 -4.2194319 -4.2330346][-4.1622605 -4.1385393 -4.113306 -4.0797529 -4.0454612 -4.011456 -3.9885838 -3.9925795 -4.01751 -4.0500875 -4.07882 -4.1168118 -4.1601138 -4.1959763 -4.2194295][-4.1540122 -4.1181717 -4.0800376 -4.0359583 -3.9857411 -3.9250312 -3.8801687 -3.8884931 -3.9308856 -3.9763896 -4.0135484 -4.0600019 -4.1091413 -4.1528125 -4.1897931][-4.1498017 -4.1058879 -4.0597014 -4.0122137 -3.9552388 -3.8744862 -3.8084459 -3.8213875 -3.880182 -3.9333112 -3.9726923 -4.0155497 -4.0599313 -4.1068473 -4.1524024][-4.1551132 -4.1110139 -4.0690632 -4.0324931 -3.9900839 -3.9241862 -3.8699532 -3.875139 -3.9110353 -3.9424469 -3.9675062 -3.9953642 -4.0278583 -4.0727277 -4.1265745][-4.1670542 -4.1328812 -4.1067572 -4.0865378 -4.0635462 -4.02173 -3.9916644 -3.990238 -3.993973 -3.9951665 -3.9993055 -4.0059953 -4.02222 -4.0570297 -4.1092582][-4.1903329 -4.1697636 -4.1568179 -4.1436963 -4.1286235 -4.1025848 -4.0919309 -4.0909433 -4.0787282 -4.05834 -4.0391297 -4.0263987 -4.0337019 -4.0617342 -4.1057639][-4.2202358 -4.2084541 -4.2004409 -4.1861768 -4.1698885 -4.1501527 -4.1457319 -4.1460195 -4.1324482 -4.1031737 -4.066946 -4.044476 -4.0538187 -4.078485 -4.1116953][-4.2429843 -4.2335243 -4.2231059 -4.2048192 -4.1868424 -4.1709342 -4.1666274 -4.16566 -4.1582885 -4.13522 -4.1001391 -4.0769925 -4.0817533 -4.0975609 -4.1193085][-4.2465019 -4.2355814 -4.2210269 -4.1988139 -4.1812477 -4.169116 -4.1630273 -4.15879 -4.1559567 -4.1444678 -4.1218281 -4.105021 -4.1000519 -4.1044273 -4.1154532][-4.2298689 -4.2153625 -4.199264 -4.1807995 -4.167541 -4.15715 -4.1451159 -4.1323471 -4.1258383 -4.1187773 -4.1050863 -4.0956135 -4.094552 -4.0983462 -4.098237][-4.2065978 -4.194592 -4.1800828 -4.1670794 -4.1583457 -4.1492014 -4.1363153 -4.1227474 -4.1086278 -4.1001692 -4.0951924 -4.0886488 -4.0920753 -4.1024446 -4.0885406]]...]
INFO - root - 2017-12-06 07:21:39.444540: step 8710, loss = 2.07, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:13m:46s remains)
INFO - root - 2017-12-06 07:21:43.521863: step 8720, loss = 2.06, batch loss = 2.01 (19.2 examples/sec; 0.417 sec/batch; 37h:32m:03s remains)
INFO - root - 2017-12-06 07:21:47.918694: step 8730, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:04m:03s remains)
INFO - root - 2017-12-06 07:21:52.249283: step 8740, loss = 2.05, batch loss = 1.99 (18.1 examples/sec; 0.443 sec/batch; 39h:48m:00s remains)
INFO - root - 2017-12-06 07:21:56.562208: step 8750, loss = 2.08, batch loss = 2.02 (18.8 examples/sec; 0.426 sec/batch; 38h:18m:35s remains)
INFO - root - 2017-12-06 07:22:00.906976: step 8760, loss = 2.07, batch loss = 2.02 (18.5 examples/sec; 0.431 sec/batch; 38h:48m:01s remains)
INFO - root - 2017-12-06 07:22:05.257066: step 8770, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.434 sec/batch; 39h:01m:21s remains)
INFO - root - 2017-12-06 07:22:09.610777: step 8780, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.426 sec/batch; 38h:19m:52s remains)
INFO - root - 2017-12-06 07:22:13.837008: step 8790, loss = 2.03, batch loss = 1.97 (18.7 examples/sec; 0.427 sec/batch; 38h:22m:56s remains)
INFO - root - 2017-12-06 07:22:18.144616: step 8800, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.422 sec/batch; 37h:54m:20s remains)
2017-12-06 07:22:18.734449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2945414 -4.2665234 -4.240294 -4.22794 -4.2271714 -4.2360039 -4.2436481 -4.2494555 -4.2519975 -4.2414222 -4.2236624 -4.2099414 -4.2077079 -4.2043486 -4.2062902][-4.2812252 -4.249958 -4.2219009 -4.2115407 -4.2171392 -4.2359982 -4.2500844 -4.2553005 -4.2563095 -4.2452559 -4.2245617 -4.2085404 -4.2086368 -4.2118244 -4.2236762][-4.2626715 -4.2245393 -4.1923108 -4.1841989 -4.1959329 -4.2265053 -4.2472248 -4.2506218 -4.2486033 -4.2370758 -4.2148986 -4.1941247 -4.1951938 -4.2024164 -4.2247419][-4.2408714 -4.1935921 -4.1543765 -4.1471629 -4.1635022 -4.1992307 -4.2216935 -4.220437 -4.2168517 -4.2064538 -4.1904573 -4.1679564 -4.1599073 -4.1604142 -4.1850238][-4.2222815 -4.1702738 -4.128612 -4.1219912 -4.1399927 -4.1722374 -4.1857762 -4.1760259 -4.1754251 -4.1761441 -4.1761832 -4.1599016 -4.143446 -4.1354547 -4.1564927][-4.2095828 -4.1567664 -4.1161785 -4.106853 -4.1145878 -4.1284509 -4.1243649 -4.10529 -4.1131897 -4.1408858 -4.1682267 -4.1653361 -4.1502032 -4.14213 -4.1541252][-4.2022858 -4.1494484 -4.1059656 -4.0861344 -4.0748248 -4.0635381 -4.0363407 -4.0038939 -4.0207052 -4.0827551 -4.1393771 -4.155221 -4.1504993 -4.1458344 -4.152051][-4.197772 -4.1415095 -4.0882788 -4.0529895 -4.0192547 -3.9791155 -3.9273815 -3.877528 -3.9000702 -3.9922578 -4.0741253 -4.112082 -4.1257634 -4.1275606 -4.1392951][-4.20383 -4.1471224 -4.0849218 -4.037147 -3.9886212 -3.9290113 -3.8626971 -3.8052642 -3.8291767 -3.9364333 -4.0295272 -4.0794206 -4.1082344 -4.1137609 -4.1292195][-4.2246075 -4.1744642 -4.1131659 -4.0637007 -4.016046 -3.9618516 -3.9133449 -3.8788452 -3.9069598 -3.9950953 -4.0675535 -4.1070843 -4.13221 -4.1322885 -4.1442037][-4.2591105 -4.2211967 -4.1715331 -4.1291065 -4.0919533 -4.0555544 -4.0308089 -4.0214791 -4.0526042 -4.1143575 -4.1584663 -4.1809034 -4.1938343 -4.1863136 -4.1892433][-4.2930884 -4.2700243 -4.2385845 -4.2116609 -4.187706 -4.1680918 -4.1573367 -4.1604137 -4.186902 -4.2237878 -4.2462034 -4.2554674 -4.2568016 -4.2474318 -4.2471762][-4.3140483 -4.3021951 -4.2848859 -4.2694836 -4.2564158 -4.2492876 -4.247972 -4.2534204 -4.2684264 -4.2857594 -4.2935166 -4.2952929 -4.2921572 -4.2869735 -4.2898993][-4.32504 -4.3179946 -4.3095746 -4.3018718 -4.2945743 -4.2920232 -4.2924623 -4.295229 -4.3018751 -4.3092813 -4.3120165 -4.3122869 -4.31067 -4.311573 -4.3178239][-4.3313913 -4.3264141 -4.3228273 -4.3210444 -4.3194036 -4.3189454 -4.3190465 -4.3193841 -4.3211479 -4.3233337 -4.3251624 -4.3262691 -4.3272586 -4.3313718 -4.3387041]]...]
INFO - root - 2017-12-06 07:22:23.026758: step 8810, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.429 sec/batch; 38h:36m:14s remains)
INFO - root - 2017-12-06 07:22:27.215569: step 8820, loss = 2.07, batch loss = 2.01 (18.5 examples/sec; 0.433 sec/batch; 38h:53m:49s remains)
INFO - root - 2017-12-06 07:22:31.501853: step 8830, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:11m:01s remains)
INFO - root - 2017-12-06 07:22:35.855342: step 8840, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.436 sec/batch; 39h:11m:36s remains)
INFO - root - 2017-12-06 07:22:40.262832: step 8850, loss = 2.09, batch loss = 2.04 (17.7 examples/sec; 0.452 sec/batch; 40h:39m:43s remains)
INFO - root - 2017-12-06 07:22:44.642005: step 8860, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 39h:54m:36s remains)
INFO - root - 2017-12-06 07:22:48.919387: step 8870, loss = 2.06, batch loss = 2.00 (18.9 examples/sec; 0.423 sec/batch; 38h:00m:28s remains)
INFO - root - 2017-12-06 07:22:53.295593: step 8880, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:42m:34s remains)
INFO - root - 2017-12-06 07:22:57.580660: step 8890, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 39h:07m:42s remains)
INFO - root - 2017-12-06 07:23:01.885956: step 8900, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.421 sec/batch; 37h:52m:59s remains)
2017-12-06 07:23:02.343679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1796823 -4.1761141 -4.1857796 -4.20496 -4.2167325 -4.2164435 -4.2216549 -4.2232361 -4.2285562 -4.2379212 -4.2386436 -4.2199311 -4.1890941 -4.1574421 -4.1432347][-4.1174879 -4.1327562 -4.1575394 -4.1934824 -4.2251849 -4.2348838 -4.2384529 -4.23344 -4.2338328 -4.2426414 -4.2408338 -4.208775 -4.1583304 -4.1134462 -4.0908537][-4.049643 -4.0907254 -4.1421247 -4.1971259 -4.2369413 -4.2432733 -4.234386 -4.2173219 -4.2076807 -4.206861 -4.1989512 -4.161314 -4.0989351 -4.0480294 -4.0251174][-3.9891622 -4.0607524 -4.14211 -4.2074361 -4.2405767 -4.2281756 -4.1988049 -4.1690722 -4.15003 -4.1442246 -4.1386437 -4.1099176 -4.0609941 -4.0269928 -4.0155997][-4.0143771 -4.100378 -4.1819425 -4.2260556 -4.2314959 -4.1931405 -4.1376009 -4.0925417 -4.0731511 -4.0742273 -4.0820951 -4.0797672 -4.0662928 -4.0604067 -4.06193][-4.1210351 -4.1857557 -4.2335243 -4.2361579 -4.2068944 -4.143249 -4.0566034 -3.9974601 -3.995553 -4.02266 -4.0571527 -4.08486 -4.1054955 -4.1166916 -4.119616][-4.1955514 -4.22939 -4.2368164 -4.2011409 -4.1397543 -4.0544634 -3.9359725 -3.865267 -3.9071765 -3.9860723 -4.05162 -4.1054649 -4.1459579 -4.1578856 -4.1475973][-4.2121682 -4.2240486 -4.1999636 -4.1329422 -4.0424623 -3.9374082 -3.7988713 -3.7286072 -3.8281965 -3.952764 -4.0378885 -4.1084104 -4.156867 -4.1598926 -4.13276][-4.2195482 -4.2078261 -4.1583986 -4.0712614 -3.9742491 -3.8825622 -3.7834294 -3.7531259 -3.8670764 -3.9915841 -4.063314 -4.1223278 -4.1662607 -4.1585445 -4.1236048][-4.2099109 -4.1872344 -4.1313291 -4.0486751 -3.9711266 -3.9152384 -3.8745019 -3.877614 -3.9621768 -4.0516367 -4.0964789 -4.13738 -4.1723437 -4.163353 -4.13486][-4.198535 -4.1696343 -4.1131415 -4.0440378 -3.9916871 -3.969218 -3.9682412 -3.9876709 -4.0425258 -4.0984926 -4.127007 -4.1534638 -4.176928 -4.1703968 -4.1511827][-4.2117558 -4.1806874 -4.1328988 -4.08722 -4.059556 -4.0573626 -4.0754929 -4.0984497 -4.1299095 -4.1581974 -4.1743956 -4.1905956 -4.2035728 -4.2007828 -4.1903086][-4.249527 -4.2260952 -4.196713 -4.1761951 -4.1629133 -4.1647739 -4.1815329 -4.1969852 -4.2083564 -4.2182841 -4.227448 -4.2379618 -4.2458043 -4.2479024 -4.2444229][-4.288991 -4.2755594 -4.2612019 -4.25673 -4.2514329 -4.2521572 -4.2642422 -4.2746105 -4.2770452 -4.2792683 -4.2851734 -4.2904239 -4.2937989 -4.2960992 -4.2947812][-4.3140841 -4.3075261 -4.3021021 -4.3031583 -4.3020306 -4.3023715 -4.3097005 -4.3160028 -4.3162308 -4.3160725 -4.3201027 -4.3221154 -4.3229389 -4.3234935 -4.3218808]]...]
INFO - root - 2017-12-06 07:23:06.692665: step 8910, loss = 2.04, batch loss = 1.98 (19.1 examples/sec; 0.419 sec/batch; 37h:38m:28s remains)
INFO - root - 2017-12-06 07:23:10.958530: step 8920, loss = 2.04, batch loss = 1.98 (19.2 examples/sec; 0.416 sec/batch; 37h:21m:59s remains)
INFO - root - 2017-12-06 07:23:15.090435: step 8930, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 39h:57m:41s remains)
INFO - root - 2017-12-06 07:23:19.394166: step 8940, loss = 2.07, batch loss = 2.01 (19.3 examples/sec; 0.415 sec/batch; 37h:20m:31s remains)
INFO - root - 2017-12-06 07:23:23.848851: step 8950, loss = 2.07, batch loss = 2.01 (17.7 examples/sec; 0.452 sec/batch; 40h:35m:13s remains)
INFO - root - 2017-12-06 07:23:28.140471: step 8960, loss = 2.07, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 38h:00m:51s remains)
INFO - root - 2017-12-06 07:23:32.475952: step 8970, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.395 sec/batch; 35h:31m:02s remains)
INFO - root - 2017-12-06 07:23:36.792545: step 8980, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 39h:39m:05s remains)
INFO - root - 2017-12-06 07:23:41.136527: step 8990, loss = 2.06, batch loss = 2.00 (18.3 examples/sec; 0.438 sec/batch; 39h:21m:13s remains)
INFO - root - 2017-12-06 07:23:45.413215: step 9000, loss = 2.06, batch loss = 2.01 (17.9 examples/sec; 0.447 sec/batch; 40h:09m:24s remains)
2017-12-06 07:23:45.558682: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:23:45.558716: E tensorflow/core/util/events_writer.cc:131] Failed to flush 41 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:23:45.956997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0798435 -4.0930471 -4.0929942 -4.0682807 -4.0504627 -4.0699024 -4.1124625 -4.1550384 -4.1789279 -4.1791673 -4.1555629 -4.1456971 -4.159893 -4.1774697 -4.1754179][-4.115633 -4.1273856 -4.1238132 -4.1032243 -4.08978 -4.1052418 -4.1433487 -4.1825724 -4.2007904 -4.1911793 -4.15926 -4.1392775 -4.1451344 -4.1586819 -4.1582127][-4.1390381 -4.1461334 -4.141366 -4.1281281 -4.1204195 -4.1312585 -4.1622081 -4.197978 -4.2120008 -4.2009263 -4.1665964 -4.1389751 -4.1354089 -4.1450295 -4.1505179][-4.1687274 -4.1619554 -4.1512661 -4.1401772 -4.1364975 -4.1440225 -4.1671605 -4.1990361 -4.2127185 -4.201684 -4.161643 -4.1250887 -4.114821 -4.1270752 -4.1441674][-4.17811 -4.1624236 -4.14442 -4.1332703 -4.1311078 -4.1340261 -4.1477995 -4.175426 -4.1968284 -4.1911411 -4.1498485 -4.1036873 -4.0867538 -4.105238 -4.1372786][-4.1665688 -4.1508875 -4.1279554 -4.1173124 -4.1148133 -4.1042733 -4.0992556 -4.1212044 -4.16025 -4.1733503 -4.1431766 -4.0962768 -4.0753279 -4.0950737 -4.1338892][-4.1618986 -4.1426082 -4.1145811 -4.100378 -4.093987 -4.0674009 -4.0302458 -4.0380931 -4.098762 -4.139267 -4.1284509 -4.08859 -4.0653467 -4.0786428 -4.1185837][-4.1562819 -4.1319437 -4.1025667 -4.0858135 -4.0752177 -4.0304151 -3.9542232 -3.9310789 -4.0089631 -4.0834475 -4.1006637 -4.0800867 -4.0608506 -4.0705104 -4.1082568][-4.160162 -4.1358552 -4.1069775 -4.0838251 -4.0697365 -4.0267892 -3.9400849 -3.8953533 -3.9662323 -4.0551839 -4.095849 -4.0930414 -4.0817075 -4.0876393 -4.1200161][-4.182322 -4.1600471 -4.132679 -4.1057558 -4.0947962 -4.0758691 -4.0169153 -3.9736257 -4.0057836 -4.0742674 -4.1182685 -4.1212692 -4.1125078 -4.1153674 -4.1406007][-4.2121797 -4.1972275 -4.1749349 -4.1473732 -4.1385851 -4.1386795 -4.1050477 -4.0629745 -4.0665216 -4.1117587 -4.1505742 -4.1593103 -4.1541076 -4.1547155 -4.1716328][-4.2421379 -4.2356253 -4.2208457 -4.1976247 -4.1898675 -4.1975107 -4.1795082 -4.1406384 -4.1264753 -4.151679 -4.1874595 -4.2028341 -4.1976213 -4.1908789 -4.1968937][-4.269773 -4.2680707 -4.2590337 -4.24394 -4.2397108 -4.2512 -4.2437377 -4.2050872 -4.1791968 -4.1903009 -4.2179842 -4.2328181 -4.2229638 -4.2081261 -4.2054963][-4.2793312 -4.2843351 -4.2838964 -4.2772 -4.2757268 -4.2883158 -4.28516 -4.2495074 -4.21846 -4.2195454 -4.2377038 -4.2488275 -4.23763 -4.2206469 -4.2127118][-4.2724733 -4.2836223 -4.2938466 -4.2960014 -4.2969522 -4.3074193 -4.3047023 -4.2746887 -4.2457128 -4.2412543 -4.2566137 -4.2676439 -4.2581444 -4.2366056 -4.2202539]]...]
INFO - root - 2017-12-06 07:23:50.239173: step 9010, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.423 sec/batch; 38h:01m:29s remains)
INFO - root - 2017-12-06 07:23:54.582305: step 9020, loss = 2.06, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:25m:09s remains)
INFO - root - 2017-12-06 07:23:58.714252: step 9030, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 39h:53m:40s remains)
INFO - root - 2017-12-06 07:24:02.956328: step 9040, loss = 2.10, batch loss = 2.04 (18.8 examples/sec; 0.424 sec/batch; 38h:08m:15s remains)
INFO - root - 2017-12-06 07:24:07.277719: step 9050, loss = 2.05, batch loss = 1.99 (19.1 examples/sec; 0.419 sec/batch; 37h:38m:17s remains)
INFO - root - 2017-12-06 07:24:11.632306: step 9060, loss = 2.09, batch loss = 2.04 (18.5 examples/sec; 0.432 sec/batch; 38h:47m:11s remains)
INFO - root - 2017-12-06 07:24:16.051138: step 9070, loss = 2.06, batch loss = 2.01 (18.3 examples/sec; 0.437 sec/batch; 39h:14m:56s remains)
INFO - root - 2017-12-06 07:24:20.311119: step 9080, loss = 2.06, batch loss = 2.00 (20.1 examples/sec; 0.397 sec/batch; 35h:41m:21s remains)
INFO - root - 2017-12-06 07:24:24.749058: step 9090, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.443 sec/batch; 39h:49m:23s remains)
INFO - root - 2017-12-06 07:24:29.117958: step 9100, loss = 2.08, batch loss = 2.02 (17.3 examples/sec; 0.463 sec/batch; 41h:37m:55s remains)
2017-12-06 07:24:29.611328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3302174 -4.3134022 -4.2744126 -4.2256007 -4.1925993 -4.1833549 -4.1864223 -4.1872377 -4.1933093 -4.2031212 -4.2155242 -4.2303276 -4.2496948 -4.259088 -4.2454114][-4.3309116 -4.31124 -4.270823 -4.2229404 -4.1936989 -4.1839828 -4.1893816 -4.193202 -4.198236 -4.2037973 -4.2106218 -4.2246084 -4.2483897 -4.2603636 -4.2488723][-4.3326473 -4.3098826 -4.2681956 -4.2202778 -4.1920366 -4.1833706 -4.1905413 -4.2003365 -4.210031 -4.2152719 -4.2147493 -4.2201419 -4.2383142 -4.2514534 -4.2459965][-4.33941 -4.3159451 -4.2732997 -4.2216573 -4.18672 -4.1745729 -4.181531 -4.1966367 -4.2160096 -4.225122 -4.2210045 -4.21386 -4.2212296 -4.2301822 -4.2321734][-4.3477263 -4.32555 -4.2812581 -4.2235746 -4.1752329 -4.1485877 -4.1456347 -4.1587877 -4.1894431 -4.2153668 -4.2196665 -4.2092934 -4.2059512 -4.2078447 -4.2134018][-4.35511 -4.3351436 -4.2910051 -4.2263875 -4.1601524 -4.105577 -4.0777392 -4.0799112 -4.1271791 -4.18573 -4.2142668 -4.2151537 -4.2074966 -4.2005458 -4.2017565][-4.3585439 -4.3424087 -4.3030624 -4.2367849 -4.1562595 -4.0721741 -4.004777 -3.9763393 -4.03927 -4.1377883 -4.200213 -4.2242417 -4.2242556 -4.2133641 -4.2080126][-4.3591328 -4.3476915 -4.3151326 -4.254283 -4.1708889 -4.0691009 -3.9638579 -3.8896782 -3.953232 -4.0801387 -4.1726646 -4.2242479 -4.2416167 -4.2373457 -4.2287188][-4.3599005 -4.3511419 -4.3245907 -4.2737017 -4.2012887 -4.1123915 -4.0131307 -3.937413 -3.9721835 -4.0683351 -4.151834 -4.2126231 -4.2443333 -4.2510061 -4.2440767][-4.3607183 -4.353961 -4.3319974 -4.2908783 -4.2334075 -4.1715736 -4.1045189 -4.055027 -4.0633841 -4.1046362 -4.1528821 -4.2042136 -4.2405815 -4.2547932 -4.2511358][-4.3612323 -4.3538852 -4.3329511 -4.2937064 -4.2427926 -4.2009373 -4.1633472 -4.1414104 -4.1448545 -4.1545763 -4.1722908 -4.2069983 -4.2408476 -4.255372 -4.25197][-4.3598409 -4.3515081 -4.3279085 -4.285152 -4.235177 -4.2033792 -4.1824574 -4.1721997 -4.1779809 -4.1873465 -4.2003336 -4.2210264 -4.2428641 -4.2491665 -4.2429366][-4.35713 -4.3478012 -4.3204994 -4.2734919 -4.2229729 -4.1914668 -4.1734824 -4.1678972 -4.1802526 -4.2016468 -4.2230911 -4.2391472 -4.2478619 -4.2418633 -4.228374][-4.3560762 -4.3454852 -4.3154826 -4.2664804 -4.2170711 -4.1850262 -4.1676712 -4.1682463 -4.1901035 -4.2159724 -4.2371068 -4.249352 -4.2509251 -4.236516 -4.2158451][-4.3550272 -4.3436561 -4.3145342 -4.2711668 -4.2309222 -4.2055731 -4.1913919 -4.1967025 -4.2206955 -4.2447371 -4.2574177 -4.2609935 -4.2552118 -4.2391114 -4.2183075]]...]
INFO - root - 2017-12-06 07:24:33.907416: step 9110, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.425 sec/batch; 38h:09m:06s remains)
INFO - root - 2017-12-06 07:24:38.178890: step 9120, loss = 2.04, batch loss = 1.98 (18.5 examples/sec; 0.432 sec/batch; 38h:50m:26s remains)
INFO - root - 2017-12-06 07:24:42.479350: step 9130, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.428 sec/batch; 38h:24m:19s remains)
INFO - root - 2017-12-06 07:24:46.611817: step 9140, loss = 2.08, batch loss = 2.02 (18.5 examples/sec; 0.433 sec/batch; 38h:51m:12s remains)
INFO - root - 2017-12-06 07:24:50.991863: step 9150, loss = 2.04, batch loss = 1.99 (18.8 examples/sec; 0.427 sec/batch; 38h:19m:19s remains)
INFO - root - 2017-12-06 07:24:55.308570: step 9160, loss = 2.03, batch loss = 1.98 (17.8 examples/sec; 0.450 sec/batch; 40h:24m:22s remains)
INFO - root - 2017-12-06 07:24:59.649936: step 9170, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.444 sec/batch; 39h:51m:03s remains)
INFO - root - 2017-12-06 07:25:03.946380: step 9180, loss = 2.10, batch loss = 2.04 (18.2 examples/sec; 0.439 sec/batch; 39h:23m:12s remains)
INFO - root - 2017-12-06 07:25:08.247057: step 9190, loss = 2.09, batch loss = 2.03 (19.3 examples/sec; 0.415 sec/batch; 37h:18m:14s remains)
INFO - root - 2017-12-06 07:25:12.607806: step 9200, loss = 2.07, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 38h:28m:08s remains)
2017-12-06 07:25:13.091182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3265004 -4.317863 -4.3061256 -4.2891321 -4.2705579 -4.2610216 -4.2650232 -4.2739735 -4.2778091 -4.2808175 -4.2885413 -4.2964411 -4.2989039 -4.301929 -4.3034205][-4.3252234 -4.3114357 -4.2869034 -4.2548594 -4.2229052 -4.2086158 -4.2153544 -4.2317538 -4.2405457 -4.2492223 -4.261405 -4.2703137 -4.2737989 -4.2755055 -4.2744169][-4.326458 -4.304132 -4.2613096 -4.2083907 -4.1631312 -4.1401482 -4.1414514 -4.1685605 -4.1927233 -4.2109089 -4.2283111 -4.2363076 -4.2428069 -4.2455382 -4.24201][-4.3297844 -4.2982125 -4.2387033 -4.1753058 -4.1237364 -4.0868449 -4.0731354 -4.1048307 -4.1441832 -4.1773214 -4.2015004 -4.2079535 -4.212749 -4.21451 -4.2108736][-4.3348794 -4.2975492 -4.2302842 -4.1592221 -4.099195 -4.0362592 -3.9937227 -4.0283861 -4.0891423 -4.1396308 -4.1701674 -4.1754289 -4.1756935 -4.1758213 -4.1756992][-4.3370028 -4.2995977 -4.2287817 -4.1532345 -4.0818663 -3.9872649 -3.9060936 -3.9347055 -4.0179935 -4.0860171 -4.1299572 -4.1400685 -4.1389508 -4.1392918 -4.1464047][-4.3329487 -4.2937951 -4.2156358 -4.1291552 -4.0426421 -3.9170814 -3.788008 -3.7962086 -3.9092355 -4.0130148 -4.0811605 -4.105113 -4.1088758 -4.1137743 -4.1283026][-4.3245316 -4.283886 -4.200686 -4.106236 -3.9996185 -3.8518116 -3.6941714 -3.6851044 -3.8259735 -3.96533 -4.0475965 -4.0821609 -4.0901766 -4.1010947 -4.120029][-4.3109918 -4.26566 -4.1821408 -4.0866027 -3.9768932 -3.8408649 -3.7229593 -3.7426193 -3.8701606 -3.9858246 -4.0515447 -4.0800281 -4.0837488 -4.0947442 -4.1146016][-4.2981319 -4.2559137 -4.1855803 -4.1076765 -4.0176368 -3.9160054 -3.855551 -3.897285 -3.9821706 -4.0474019 -4.0820651 -4.0983281 -4.0966678 -4.1018238 -4.1208816][-4.2935681 -4.2588549 -4.2058439 -4.1518264 -4.086657 -4.01082 -3.9805491 -4.020359 -4.068943 -4.0948 -4.1088033 -4.1215177 -4.1249123 -4.1283188 -4.1431818][-4.2976723 -4.2727027 -4.2371588 -4.1987286 -4.1487856 -4.0933275 -4.0806255 -4.1133275 -4.1348257 -4.1379476 -4.1399674 -4.150332 -4.1634064 -4.1693225 -4.1785517][-4.3076315 -4.290906 -4.2643547 -4.2337413 -4.1983538 -4.1645789 -4.1660624 -4.189961 -4.1962643 -4.1865854 -4.1748357 -4.1834021 -4.2046471 -4.2128973 -4.2171068][-4.3236771 -4.3130326 -4.295001 -4.2768373 -4.2565293 -4.2407322 -4.2469473 -4.258853 -4.256959 -4.2454882 -4.2297826 -4.2320704 -4.2536597 -4.2598605 -4.2598934][-4.3360906 -4.3307047 -4.3215928 -4.3108692 -4.2981229 -4.2931185 -4.3031187 -4.3123064 -4.3113503 -4.3028188 -4.2883925 -4.2846894 -4.2998371 -4.3026156 -4.2980137]]...]
INFO - root - 2017-12-06 07:25:17.439651: step 9210, loss = 2.04, batch loss = 1.98 (18.7 examples/sec; 0.428 sec/batch; 38h:27m:39s remains)
INFO - root - 2017-12-06 07:25:21.817909: step 9220, loss = 2.05, batch loss = 1.99 (18.7 examples/sec; 0.427 sec/batch; 38h:21m:10s remains)
INFO - root - 2017-12-06 07:25:26.174515: step 9230, loss = 2.09, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 41h:07m:02s remains)
INFO - root - 2017-12-06 07:25:30.197690: step 9240, loss = 2.10, batch loss = 2.04 (27.3 examples/sec; 0.293 sec/batch; 26h:18m:47s remains)
INFO - root - 2017-12-06 07:25:34.504520: step 9250, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 38h:39m:16s remains)
INFO - root - 2017-12-06 07:25:38.850997: step 9260, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.451 sec/batch; 40h:27m:05s remains)
INFO - root - 2017-12-06 07:25:43.320004: step 9270, loss = 2.05, batch loss = 2.00 (18.7 examples/sec; 0.429 sec/batch; 38h:29m:21s remains)
INFO - root - 2017-12-06 07:25:47.600728: step 9280, loss = 2.08, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 39h:14m:55s remains)
INFO - root - 2017-12-06 07:25:51.895826: step 9290, loss = 2.07, batch loss = 2.01 (18.4 examples/sec; 0.435 sec/batch; 39h:02m:19s remains)
INFO - root - 2017-12-06 07:25:56.335742: step 9300, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.441 sec/batch; 39h:37m:47s remains)
2017-12-06 07:25:56.476793: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:25:56.476830: E tensorflow/core/util/events_writer.cc:131] Failed to flush 44 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:25:56.828629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2832031 -4.2811427 -4.2801971 -4.2770252 -4.2738032 -4.2728286 -4.2743979 -4.2792811 -4.2866726 -4.2894807 -4.2883143 -4.2782626 -4.2634883 -4.2528768 -4.2543373][-4.2391791 -4.2321038 -4.2301474 -4.2296534 -4.2301507 -4.233366 -4.2383771 -4.2480326 -4.2637768 -4.2707653 -4.2689528 -4.2541614 -4.2320738 -4.2144246 -4.2101326][-4.1987405 -4.1946492 -4.1968079 -4.2010059 -4.20572 -4.2134581 -4.2214513 -4.2355142 -4.2572908 -4.2675333 -4.2608104 -4.2362971 -4.2057748 -4.1813784 -4.1702728][-4.1588464 -4.1596665 -4.1685452 -4.177444 -4.1845403 -4.1936231 -4.2018008 -4.2195621 -4.2465148 -4.2624593 -4.2582 -4.2254505 -4.1824312 -4.1455965 -4.1246762][-4.1078763 -4.1074543 -4.122416 -4.1385875 -4.150187 -4.1604786 -4.1682048 -4.1866198 -4.2206755 -4.2480631 -4.2498636 -4.2133937 -4.161562 -4.1184421 -4.0911479][-4.0484028 -4.0432653 -4.0646596 -4.0883512 -4.1021986 -4.1105518 -4.1142116 -4.1307073 -4.1715851 -4.2102523 -4.2163053 -4.1808763 -4.1291218 -4.0888166 -4.0674257][-3.9984241 -3.9845908 -4.0065131 -4.03139 -4.0391717 -4.04171 -4.0446367 -4.058548 -4.1018472 -4.1493998 -4.165308 -4.1390557 -4.0936613 -4.0589314 -4.0478196][-3.9877052 -3.9704542 -3.9886644 -4.00346 -4.0025134 -4.00135 -4.0026646 -4.0142555 -4.0568051 -4.108542 -4.1316 -4.1133194 -4.0739512 -4.0436754 -4.0421505][-4.0070205 -3.9986153 -4.0119987 -4.012156 -3.9960444 -3.9859126 -3.9857037 -4.000742 -4.0485406 -4.1064024 -4.1357741 -4.1260867 -4.0925136 -4.06389 -4.0644674][-4.0234318 -4.0199442 -4.0281096 -4.0136151 -3.9799812 -3.95375 -3.9490917 -3.9751272 -4.0379906 -4.1039329 -4.1384215 -4.1414342 -4.1205759 -4.1032376 -4.111093][-4.0550914 -4.0467391 -4.0462723 -4.0270629 -3.9872286 -3.9550014 -3.9483068 -3.9817729 -4.0479469 -4.1051116 -4.1331062 -4.138875 -4.1316481 -4.1298122 -4.1453562][-4.1295877 -4.1139917 -4.1069479 -4.088963 -4.0571251 -4.03287 -4.0274959 -4.0556173 -4.1031404 -4.1397696 -4.1557083 -4.1569161 -4.1515732 -4.1529255 -4.1635385][-4.2118464 -4.1967654 -4.189683 -4.1771083 -4.1610308 -4.150002 -4.1450119 -4.1559868 -4.1815128 -4.2027512 -4.2099075 -4.2052622 -4.196444 -4.1910172 -4.1900826][-4.2751932 -4.2676892 -4.2650685 -4.2580886 -4.2489619 -4.2452755 -4.2424674 -4.2459264 -4.2581296 -4.2689486 -4.2714829 -4.2643065 -4.2543187 -4.2478967 -4.24347][-4.3169904 -4.3162513 -4.3166809 -4.3130593 -4.3084235 -4.3079433 -4.3075066 -4.3098159 -4.3154593 -4.3193884 -4.3185019 -4.3123474 -4.3061686 -4.3033624 -4.3020811]]...]
INFO - root - 2017-12-06 07:26:01.209768: step 9310, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.430 sec/batch; 38h:35m:07s remains)
INFO - root - 2017-12-06 07:26:05.546578: step 9320, loss = 2.05, batch loss = 2.00 (18.7 examples/sec; 0.428 sec/batch; 38h:25m:59s remains)
INFO - root - 2017-12-06 07:26:09.923199: step 9330, loss = 2.08, batch loss = 2.03 (17.2 examples/sec; 0.466 sec/batch; 41h:51m:55s remains)
INFO - root - 2017-12-06 07:26:14.258402: step 9340, loss = 2.08, batch loss = 2.03 (18.2 examples/sec; 0.439 sec/batch; 39h:24m:54s remains)
INFO - root - 2017-12-06 07:26:18.354608: step 9350, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:29m:40s remains)
INFO - root - 2017-12-06 07:26:22.764936: step 9360, loss = 2.05, batch loss = 1.99 (17.7 examples/sec; 0.453 sec/batch; 40h:39m:37s remains)
INFO - root - 2017-12-06 07:26:27.071792: step 9370, loss = 2.09, batch loss = 2.04 (18.7 examples/sec; 0.429 sec/batch; 38h:28m:50s remains)
INFO - root - 2017-12-06 07:26:31.405324: step 9380, loss = 2.04, batch loss = 1.98 (17.9 examples/sec; 0.447 sec/batch; 40h:08m:56s remains)
INFO - root - 2017-12-06 07:26:35.692569: step 9390, loss = 2.05, batch loss = 1.99 (18.9 examples/sec; 0.424 sec/batch; 38h:02m:36s remains)
INFO - root - 2017-12-06 07:26:40.093231: step 9400, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 39h:04m:01s remains)
2017-12-06 07:26:40.571199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.184166 -4.1792078 -4.187705 -4.2034221 -4.2133141 -4.2181005 -4.2287259 -4.2382231 -4.2260356 -4.1792293 -4.1220007 -4.119173 -4.1532855 -4.1774449 -4.1926847][-4.1806731 -4.1887937 -4.2137008 -4.2336283 -4.2356172 -4.2317739 -4.2398682 -4.255589 -4.2487192 -4.1968269 -4.1328459 -4.1239195 -4.1547952 -4.177701 -4.1891589][-4.1920581 -4.2060032 -4.236166 -4.2495379 -4.2419209 -4.2342186 -4.2421193 -4.2611918 -4.2572231 -4.2004194 -4.1292448 -4.1163878 -4.1469231 -4.1695719 -4.1808133][-4.2266088 -4.2371883 -4.2546988 -4.2533441 -4.241703 -4.2364874 -4.2409945 -4.2556138 -4.2500095 -4.1971617 -4.1304474 -4.1174779 -4.1482458 -4.16733 -4.1761055][-4.244504 -4.2483435 -4.2436504 -4.2254086 -4.2167597 -4.2216959 -4.2284379 -4.2381783 -4.2317567 -4.1878543 -4.1292562 -4.1220136 -4.15459 -4.1702118 -4.1795864][-4.2441707 -4.2379842 -4.2108707 -4.1700191 -4.1565709 -4.1697679 -4.1829128 -4.1904054 -4.1828642 -4.150691 -4.1043415 -4.1028843 -4.1365452 -4.148551 -4.1605062][-4.2127256 -4.1895981 -4.1343741 -4.0607743 -4.0309386 -4.0465717 -4.0627561 -4.057981 -4.0441561 -4.0304942 -4.0119772 -4.0304184 -4.0773306 -4.101222 -4.1232958][-4.1530838 -4.114223 -4.0354171 -3.9369183 -3.8868306 -3.8981924 -3.9179273 -3.8925056 -3.8651061 -3.8759017 -3.8965783 -3.9421194 -4.0094428 -4.0510831 -4.0865984][-4.102931 -4.0683584 -3.9892974 -3.890408 -3.8426976 -3.8586323 -3.8843389 -3.849081 -3.8141932 -3.849124 -3.8909521 -3.9340992 -3.9968295 -4.0427475 -4.0821834][-4.0713868 -4.0552649 -3.9959867 -3.9154658 -3.8803639 -3.8996224 -3.9283719 -3.8918266 -3.8546951 -3.9028933 -3.9597452 -4.0028172 -4.0585756 -4.100503 -4.1363983][-4.0799818 -4.0761909 -4.0423994 -3.9921415 -3.9679775 -3.9792318 -3.9985013 -3.9649248 -3.9347007 -3.9834561 -4.04567 -4.092659 -4.14251 -4.1782975 -4.2083554][-4.1295314 -4.1303821 -4.1186523 -4.0975633 -4.0811715 -4.0812316 -4.0888944 -4.0644627 -4.04358 -4.076664 -4.1237121 -4.1683588 -4.2120347 -4.2421126 -4.266747][-4.1892734 -4.1946793 -4.1953688 -4.1938071 -4.1880374 -4.1835084 -4.1822562 -4.1681194 -4.1549315 -4.1671267 -4.1877756 -4.220345 -4.2555389 -4.2784123 -4.2957096][-4.2379079 -4.2438478 -4.2507248 -4.2551365 -4.2548537 -4.252142 -4.2514067 -4.2473068 -4.237565 -4.2318497 -4.2270803 -4.2428846 -4.2683125 -4.2821097 -4.2954903][-4.2739539 -4.2745624 -4.2778835 -4.2812414 -4.2816949 -4.2805734 -4.2798662 -4.2781043 -4.2697191 -4.2515759 -4.2269545 -4.2326331 -4.2530761 -4.2628307 -4.27904]]...]
INFO - root - 2017-12-06 07:26:44.873258: step 9410, loss = 2.07, batch loss = 2.02 (17.7 examples/sec; 0.452 sec/batch; 40h:32m:50s remains)
INFO - root - 2017-12-06 07:26:49.246204: step 9420, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:28m:01s remains)
INFO - root - 2017-12-06 07:26:53.535243: step 9430, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.441 sec/batch; 39h:35m:26s remains)
INFO - root - 2017-12-06 07:26:57.879373: step 9440, loss = 2.07, batch loss = 2.02 (18.6 examples/sec; 0.431 sec/batch; 38h:39m:54s remains)
INFO - root - 2017-12-06 07:27:01.924686: step 9450, loss = 2.07, batch loss = 2.01 (24.2 examples/sec; 0.330 sec/batch; 29h:37m:00s remains)
INFO - root - 2017-12-06 07:27:06.363445: step 9460, loss = 2.07, batch loss = 2.02 (18.3 examples/sec; 0.437 sec/batch; 39h:15m:13s remains)
INFO - root - 2017-12-06 07:27:10.721944: step 9470, loss = 2.05, batch loss = 1.99 (18.0 examples/sec; 0.446 sec/batch; 39h:58m:31s remains)
INFO - root - 2017-12-06 07:27:15.088343: step 9480, loss = 2.08, batch loss = 2.02 (18.1 examples/sec; 0.443 sec/batch; 39h:45m:28s remains)
INFO - root - 2017-12-06 07:27:19.425836: step 9490, loss = 2.05, batch loss = 1.99 (18.9 examples/sec; 0.423 sec/batch; 37h:56m:59s remains)
INFO - root - 2017-12-06 07:27:23.719724: step 9500, loss = 2.06, batch loss = 2.00 (17.9 examples/sec; 0.448 sec/batch; 40h:10m:33s remains)
2017-12-06 07:27:24.250829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2249832 -4.2595258 -4.2816625 -4.2530651 -4.16583 -4.0662389 -4.0019441 -4.0129042 -4.1031775 -4.2052217 -4.2577329 -4.279902 -4.2908368 -4.2957878 -4.2904029][-4.244493 -4.2731371 -4.2915344 -4.2580328 -4.1701584 -4.0712218 -4.0087018 -4.0156007 -4.0966868 -4.1959014 -4.2513742 -4.2795649 -4.2969017 -4.306406 -4.3041992][-4.2610607 -4.27669 -4.2860894 -4.2515554 -4.1701369 -4.0844426 -4.0309434 -4.0227442 -4.079267 -4.1660023 -4.2235341 -4.2602482 -4.2893324 -4.3064494 -4.3116961][-4.2694893 -4.2689185 -4.2653289 -4.2273097 -4.1528459 -4.0860682 -4.0465117 -4.02967 -4.0642657 -4.130425 -4.1828127 -4.2248287 -4.266293 -4.2940068 -4.3074126][-4.2601013 -4.2495494 -4.2316937 -4.1838584 -4.1156588 -4.0693121 -4.0419903 -4.0287786 -4.054255 -4.1020241 -4.1447563 -4.1901851 -4.2371783 -4.2718611 -4.2910209][-4.228776 -4.2127619 -4.1828756 -4.1190906 -4.0486646 -4.0202994 -4.0114379 -4.0076776 -4.0323386 -4.068749 -4.10288 -4.1516109 -4.2011781 -4.2374349 -4.2621408][-4.1957498 -4.1759677 -4.1283545 -4.0511374 -3.9834423 -3.9666679 -3.9752088 -3.9782932 -3.9953756 -4.0256643 -4.0608282 -4.1154404 -4.1659527 -4.2033772 -4.2328115][-4.179719 -4.1617169 -4.118566 -4.0478559 -3.9870539 -3.9808023 -3.9965405 -3.9984565 -4.0008583 -4.0168118 -4.0556989 -4.118844 -4.1705513 -4.2048311 -4.2316413][-4.1950784 -4.1877065 -4.1628804 -4.1064892 -4.0502148 -4.0436645 -4.0554013 -4.0547972 -4.0523882 -4.0638833 -4.1060519 -4.1692085 -4.2154546 -4.2373776 -4.252408][-4.2307987 -4.2323909 -4.2255173 -4.1885686 -4.1422067 -4.1339016 -4.1448007 -4.1453018 -4.1406331 -4.146822 -4.1776338 -4.2293077 -4.2661386 -4.2803063 -4.2859397][-4.2630248 -4.27053 -4.2734575 -4.2504067 -4.2159214 -4.2136254 -4.2297072 -4.2313838 -4.2214155 -4.2226052 -4.2376957 -4.2703075 -4.2951479 -4.3041205 -4.3075333][-4.2769632 -4.2878828 -4.2926121 -4.275743 -4.2503419 -4.2511826 -4.2714171 -4.2807407 -4.2713871 -4.2677412 -4.2745981 -4.2916145 -4.3044763 -4.309278 -4.3108473][-4.2760329 -4.28511 -4.2909284 -4.2814512 -4.2652311 -4.2691474 -4.2862134 -4.2964725 -4.2884941 -4.284142 -4.289784 -4.2976313 -4.3038921 -4.3081541 -4.310122][-4.2799134 -4.2830977 -4.2884541 -4.280973 -4.2673383 -4.26915 -4.2800808 -4.2875967 -4.2855697 -4.2833529 -4.2888942 -4.2963719 -4.3024468 -4.3083739 -4.3116593][-4.2928977 -4.2929635 -4.2948017 -4.2888455 -4.2790089 -4.2771816 -4.2815375 -4.2861814 -4.2881956 -4.2912159 -4.2958593 -4.3011661 -4.305594 -4.3110981 -4.314435]]...]
INFO - root - 2017-12-06 07:27:28.646875: step 9510, loss = 2.04, batch loss = 1.98 (18.2 examples/sec; 0.440 sec/batch; 39h:30m:16s remains)
INFO - root - 2017-12-06 07:27:32.968062: step 9520, loss = 2.05, batch loss = 2.00 (18.0 examples/sec; 0.444 sec/batch; 39h:48m:27s remains)
INFO - root - 2017-12-06 07:27:37.241614: step 9530, loss = 2.07, batch loss = 2.02 (18.8 examples/sec; 0.425 sec/batch; 38h:08m:07s remains)
INFO - root - 2017-12-06 07:27:41.618075: step 9540, loss = 2.08, batch loss = 2.02 (17.8 examples/sec; 0.450 sec/batch; 40h:21m:31s remains)
INFO - root - 2017-12-06 07:27:45.901631: step 9550, loss = 2.06, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:20m:54s remains)
INFO - root - 2017-12-06 07:27:50.002351: step 9560, loss = 2.09, batch loss = 2.03 (18.6 examples/sec; 0.430 sec/batch; 38h:32m:05s remains)
INFO - root - 2017-12-06 07:27:54.374506: step 9570, loss = 2.05, batch loss = 1.99 (18.4 examples/sec; 0.436 sec/batch; 39h:05m:48s remains)
INFO - root - 2017-12-06 07:27:58.745684: step 9580, loss = 2.05, batch loss = 1.99 (18.8 examples/sec; 0.426 sec/batch; 38h:13m:15s remains)
INFO - root - 2017-12-06 07:28:03.029035: step 9590, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 39h:21m:44s remains)
INFO - root - 2017-12-06 07:28:07.386127: step 9600, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.427 sec/batch; 38h:17m:33s remains)
2017-12-06 07:28:07.535843: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:28:07.535874: E tensorflow/core/util/events_writer.cc:131] Failed to flush 47 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:28:07.922812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.315907 -4.3205781 -4.3250084 -4.3267055 -4.3261075 -4.3249 -4.3235769 -4.3218126 -4.3211784 -4.3229041 -4.3265591 -4.3315058 -4.3368535 -4.3414087 -4.344501][-4.276804 -4.2843556 -4.2920761 -4.2940054 -4.2918744 -4.289886 -4.2881289 -4.2848091 -4.282135 -4.2828636 -4.2867422 -4.2931023 -4.3018651 -4.3104763 -4.3171191][-4.2417979 -4.2522 -4.2621107 -4.263525 -4.2596369 -4.2572675 -4.2544742 -4.248292 -4.2425461 -4.2416339 -4.2448726 -4.2517314 -4.2638264 -4.2774382 -4.28893][-4.2203455 -4.2315254 -4.2418065 -4.2416987 -4.2352848 -4.2336555 -4.2309551 -4.2237415 -4.2159486 -4.2131529 -4.2146606 -4.2208824 -4.2344418 -4.2519836 -4.2672291][-4.2142305 -4.224021 -4.2302461 -4.2262721 -4.2173963 -4.2179751 -4.2204647 -4.2177424 -4.2125859 -4.2096748 -4.2079878 -4.2115245 -4.223825 -4.243679 -4.2614322][-4.2177696 -4.2222762 -4.2194958 -4.2090898 -4.1992531 -4.2049675 -4.216217 -4.2219357 -4.2235665 -4.223207 -4.218833 -4.2180152 -4.2269468 -4.2454467 -4.2612691][-4.2252321 -4.2205186 -4.2041078 -4.1874824 -4.1766429 -4.1843133 -4.2040524 -4.2200956 -4.2300291 -4.2368622 -4.2350993 -4.23182 -4.2352729 -4.2458272 -4.2568054][-4.2367654 -4.2232194 -4.19729 -4.1758661 -4.1618872 -4.1660705 -4.1880822 -4.211586 -4.22799 -4.2429218 -4.24586 -4.2414041 -4.2373977 -4.2369618 -4.23982][-4.243691 -4.2240906 -4.1965919 -4.1738667 -4.1556172 -4.1550126 -4.1746492 -4.2000074 -4.2210565 -4.2435064 -4.2498422 -4.2436132 -4.2320843 -4.2221169 -4.2179885][-4.2391777 -4.2172418 -4.1936278 -4.1738067 -4.1565533 -4.1550703 -4.1721954 -4.195756 -4.2171912 -4.2407694 -4.2468123 -4.2381668 -4.2215581 -4.2076573 -4.2021427][-4.2217221 -4.1994724 -4.1823249 -4.1721225 -4.1655574 -4.1686296 -4.1824193 -4.2008381 -4.2205215 -4.2405405 -4.2433505 -4.230762 -4.21032 -4.1961875 -4.1944208][-4.2026572 -4.183135 -4.1730852 -4.1750622 -4.183845 -4.1938376 -4.2018642 -4.2119694 -4.2278838 -4.244699 -4.2465725 -4.2319775 -4.2106647 -4.1955419 -4.1941142][-4.1872215 -4.1736751 -4.1719074 -4.1857781 -4.2080517 -4.2236595 -4.2279372 -4.2313938 -4.2409935 -4.2520542 -4.2511039 -4.2368712 -4.216115 -4.1979923 -4.1924105][-4.1885734 -4.1806192 -4.1869125 -4.2108483 -4.2384768 -4.2544332 -4.2552624 -4.2540026 -4.25696 -4.2595253 -4.2515182 -4.2341447 -4.2117167 -4.1892495 -4.17811][-4.1988711 -4.1939383 -4.2057114 -4.2314215 -4.2565489 -4.2679596 -4.2686057 -4.267786 -4.2677736 -4.2619128 -4.2456455 -4.2230639 -4.1975789 -4.1723413 -4.1574645]]...]
INFO - root - 2017-12-06 07:28:12.334977: step 9610, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 39h:55m:47s remains)
INFO - root - 2017-12-06 07:28:16.666931: step 9620, loss = 2.06, batch loss = 2.00 (19.3 examples/sec; 0.414 sec/batch; 37h:10m:00s remains)
INFO - root - 2017-12-06 07:28:20.904972: step 9630, loss = 2.06, batch loss = 2.00 (18.8 examples/sec; 0.424 sec/batch; 38h:04m:11s remains)
INFO - root - 2017-12-06 07:28:25.266295: step 9640, loss = 2.07, batch loss = 2.01 (18.3 examples/sec; 0.436 sec/batch; 39h:06m:07s remains)
INFO - root - 2017-12-06 07:28:29.670677: step 9650, loss = 2.08, batch loss = 2.02 (18.0 examples/sec; 0.444 sec/batch; 39h:51m:32s remains)
INFO - root - 2017-12-06 07:28:33.734312: step 9660, loss = 2.07, batch loss = 2.02 (19.9 examples/sec; 0.403 sec/batch; 36h:08m:24s remains)
INFO - root - 2017-12-06 07:28:38.019792: step 9670, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.427 sec/batch; 38h:15m:52s remains)
INFO - root - 2017-12-06 07:28:42.358917: step 9680, loss = 2.07, batch loss = 2.01 (18.9 examples/sec; 0.422 sec/batch; 37h:52m:58s remains)
INFO - root - 2017-12-06 07:28:46.689252: step 9690, loss = 2.07, batch loss = 2.01 (19.4 examples/sec; 0.412 sec/batch; 36h:58m:10s remains)
INFO - root - 2017-12-06 07:28:50.962639: step 9700, loss = 2.07, batch loss = 2.01 (18.7 examples/sec; 0.429 sec/batch; 38h:26m:17s remains)
2017-12-06 07:28:51.479782: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2375355 -4.2315936 -4.2304931 -4.237843 -4.2485976 -4.2564893 -4.2588148 -4.2556815 -4.2472906 -4.2394285 -4.2395887 -4.2403736 -4.2346487 -4.2199774 -4.199801][-4.2282219 -4.2178144 -4.2166648 -4.2282543 -4.2432356 -4.2504406 -4.2467508 -4.2373366 -4.2225394 -4.2106938 -4.2092032 -4.2097645 -4.2059369 -4.1931581 -4.1768656][-4.2118721 -4.2008076 -4.2056084 -4.225492 -4.2454133 -4.2528057 -4.2447319 -4.2269859 -4.2028856 -4.1857681 -4.1806569 -4.1810246 -4.1830106 -4.1746764 -4.1597257][-4.1954803 -4.1920943 -4.2072206 -4.228704 -4.2414575 -4.23886 -4.2193627 -4.1888881 -4.1574106 -4.1416235 -4.1415396 -4.1507506 -4.1652284 -4.1670423 -4.1541486][-4.1871719 -4.196033 -4.2168279 -4.2291989 -4.2227902 -4.1998105 -4.1609979 -4.1149931 -4.0855961 -4.0855856 -4.1032987 -4.1303759 -4.1577482 -4.1638932 -4.1506348][-4.1854424 -4.1991539 -4.2176018 -4.212482 -4.1830745 -4.1338143 -4.0702858 -4.01645 -4.0117884 -4.0447969 -4.0884757 -4.1324053 -4.1644158 -4.1715112 -4.15906][-4.1888542 -4.1991591 -4.2085462 -4.1891122 -4.1436791 -4.0771284 -4.0003858 -3.9646733 -4.0002174 -4.0625687 -4.1198516 -4.1663523 -4.1918254 -4.1969819 -4.1880379][-4.1964326 -4.2028794 -4.2072592 -4.1857057 -4.1432781 -4.08932 -4.0356407 -4.0283737 -4.0760641 -4.13189 -4.1748524 -4.2070603 -4.2240105 -4.2293119 -4.2223682][-4.2164083 -4.2193103 -4.2211418 -4.2028027 -4.1761646 -4.1478086 -4.1235876 -4.1322274 -4.1698561 -4.2056832 -4.2316871 -4.2517109 -4.2624311 -4.265306 -4.2549381][-4.2501664 -4.2541542 -4.2583919 -4.2453837 -4.2293324 -4.2134204 -4.2029104 -4.2144518 -4.2386436 -4.2578826 -4.2728209 -4.2840872 -4.2902265 -4.2863512 -4.2697115][-4.272316 -4.2782512 -4.28547 -4.2782865 -4.2683473 -4.26062 -4.2573137 -4.2682524 -4.2842417 -4.2954025 -4.3031607 -4.3049145 -4.3025718 -4.2905674 -4.2693467][-4.2817626 -4.2875972 -4.2946162 -4.2904291 -4.2831755 -4.2806969 -4.2825675 -4.2925882 -4.3063474 -4.3145542 -4.3172135 -4.3118014 -4.3021388 -4.2861466 -4.26449][-4.2929382 -4.2972636 -4.3013558 -4.2988605 -4.2936616 -4.293787 -4.2978377 -4.30556 -4.314219 -4.317987 -4.3161926 -4.3075442 -4.2954769 -4.2798924 -4.2606988][-4.302319 -4.3029718 -4.3029289 -4.300209 -4.2970409 -4.2980051 -4.3006835 -4.3043566 -4.3077674 -4.3072858 -4.3045096 -4.2999477 -4.2943268 -4.2853184 -4.2722607][-4.308454 -4.30572 -4.3030171 -4.2997341 -4.2970037 -4.2972512 -4.2991219 -4.3016305 -4.3042679 -4.3054328 -4.3068757 -4.3082461 -4.3085876 -4.3048253 -4.2957573]]...]
INFO - root - 2017-12-06 07:28:55.727038: step 9710, loss = 2.05, batch loss = 1.99 (19.8 examples/sec; 0.405 sec/batch; 36h:19m:09s remains)
INFO - root - 2017-12-06 07:29:00.032829: step 9720, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:24m:05s remains)
INFO - root - 2017-12-06 07:29:04.320631: step 9730, loss = 2.08, batch loss = 2.02 (19.0 examples/sec; 0.420 sec/batch; 37h:39m:15s remains)
INFO - root - 2017-12-06 07:29:08.663818: step 9740, loss = 2.06, batch loss = 2.00 (17.8 examples/sec; 0.449 sec/batch; 40h:16m:25s remains)
INFO - root - 2017-12-06 07:29:13.028261: step 9750, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.434 sec/batch; 38h:54m:07s remains)
INFO - root - 2017-12-06 07:29:17.334850: step 9760, loss = 2.03, batch loss = 1.97 (18.4 examples/sec; 0.434 sec/batch; 38h:55m:03s remains)
INFO - root - 2017-12-06 07:29:21.409381: step 9770, loss = 2.09, batch loss = 2.03 (18.8 examples/sec; 0.425 sec/batch; 38h:05m:36s remains)
INFO - root - 2017-12-06 07:29:25.697623: step 9780, loss = 2.04, batch loss = 1.98 (18.9 examples/sec; 0.422 sec/batch; 37h:52m:12s remains)
INFO - root - 2017-12-06 07:29:30.059849: step 9790, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.481 sec/batch; 43h:06m:25s remains)
INFO - root - 2017-12-06 07:29:34.310299: step 9800, loss = 2.08, batch loss = 2.03 (19.0 examples/sec; 0.422 sec/batch; 37h:49m:48s remains)
2017-12-06 07:29:34.805965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3030968 -4.30042 -4.2983541 -4.2959795 -4.2946925 -4.2957487 -4.2986226 -4.30185 -4.305244 -4.3043084 -4.2964559 -4.2881279 -4.2842436 -4.2792397 -4.269443][-4.2883224 -4.2827044 -4.2765102 -4.2693706 -4.2647738 -4.2664919 -4.2740426 -4.2834449 -4.2930703 -4.2959414 -4.290391 -4.2827888 -4.2766724 -4.2644877 -4.2478261][-4.2804112 -4.2711334 -4.2603955 -4.2478452 -4.2394629 -4.2417927 -4.2544308 -4.2703171 -4.2860489 -4.2930279 -4.2905679 -4.2857261 -4.2784438 -4.2601361 -4.2377229][-4.2855773 -4.2770057 -4.2639112 -4.2452054 -4.2305784 -4.2306461 -4.2441406 -4.2628989 -4.2828193 -4.2936983 -4.2957268 -4.2953672 -4.2907381 -4.2699647 -4.2436638][-4.2732182 -4.2640309 -4.2473984 -4.2215075 -4.1985559 -4.193532 -4.2047982 -4.2228212 -4.2428803 -4.25598 -4.2664933 -4.2762251 -4.27954 -4.2644229 -4.2421608][-4.2374568 -4.2264791 -4.2060871 -4.1736417 -4.1419559 -4.1285119 -4.1331139 -4.1455212 -4.1584582 -4.1694331 -4.1922855 -4.218647 -4.2365232 -4.2343 -4.2237906][-4.2108097 -4.2032042 -4.1853547 -4.152739 -4.1156368 -4.0917163 -4.0814877 -4.0790229 -4.0781126 -4.0832634 -4.1175156 -4.15763 -4.1868768 -4.1933689 -4.195868][-4.2300963 -4.2264681 -4.2157164 -4.1926003 -4.16338 -4.140975 -4.1222839 -4.1049581 -4.0889754 -4.081645 -4.108593 -4.1451 -4.1723003 -4.1762838 -4.1811495][-4.2679591 -4.2652617 -4.2606139 -4.2493711 -4.23506 -4.2253132 -4.2122774 -4.1930165 -4.1706572 -4.149158 -4.1551485 -4.1752706 -4.19301 -4.1879816 -4.1853209][-4.273571 -4.2728362 -4.2721772 -4.268158 -4.2622671 -4.2599888 -4.2552075 -4.2410831 -4.2212715 -4.1953239 -4.1886187 -4.1977634 -4.2079291 -4.1973715 -4.1895089][-4.27303 -4.272047 -4.2708535 -4.2658973 -4.2602291 -4.2586708 -4.25933 -4.2522297 -4.2397003 -4.2183981 -4.2122355 -4.2179346 -4.2224278 -4.2091775 -4.198319][-4.2884607 -4.284924 -4.2800412 -4.270421 -4.2590079 -4.2516232 -4.2515583 -4.2493935 -4.2432947 -4.2277546 -4.22594 -4.2314434 -4.2302589 -4.2134805 -4.2029037][-4.3088412 -4.3044996 -4.2971272 -4.2834191 -4.2658424 -4.2505679 -4.2445741 -4.2427053 -4.2395897 -4.2276173 -4.2280054 -4.2319307 -4.2267284 -4.2091403 -4.2016668][-4.3248696 -4.322176 -4.3162436 -4.3033342 -4.2839937 -4.2634196 -4.2510734 -4.2447782 -4.2398777 -4.2282681 -4.2280846 -4.2289405 -4.2222586 -4.2048583 -4.1992178][-4.3370647 -4.3374662 -4.3356371 -4.3270607 -4.3097544 -4.2880921 -4.2717581 -4.2597823 -4.24972 -4.23634 -4.2349257 -4.23504 -4.2303548 -4.2142763 -4.2090645]]...]
INFO - root - 2017-12-06 07:29:39.136480: step 9810, loss = 2.09, batch loss = 2.03 (18.3 examples/sec; 0.436 sec/batch; 39h:05m:48s remains)
INFO - root - 2017-12-06 07:29:43.535980: step 9820, loss = 2.04, batch loss = 1.98 (18.3 examples/sec; 0.438 sec/batch; 39h:14m:55s remains)
INFO - root - 2017-12-06 07:29:48.010092: step 9830, loss = 2.05, batch loss = 1.99 (17.9 examples/sec; 0.446 sec/batch; 39h:58m:10s remains)
INFO - root - 2017-12-06 07:29:52.338734: step 9840, loss = 2.07, batch loss = 2.01 (18.6 examples/sec; 0.431 sec/batch; 38h:38m:57s remains)
INFO - root - 2017-12-06 07:29:56.664791: step 9850, loss = 2.04, batch loss = 1.98 (18.6 examples/sec; 0.431 sec/batch; 38h:35m:32s remains)
INFO - root - 2017-12-06 07:30:00.913258: step 9860, loss = 2.05, batch loss = 1.99 (18.6 examples/sec; 0.430 sec/batch; 38h:33m:43s remains)
INFO - root - 2017-12-06 07:30:05.128858: step 9870, loss = 2.11, batch loss = 2.05 (22.4 examples/sec; 0.357 sec/batch; 31h:58m:00s remains)
INFO - root - 2017-12-06 07:30:09.550709: step 9880, loss = 2.06, batch loss = 2.00 (18.0 examples/sec; 0.445 sec/batch; 39h:53m:39s remains)
INFO - root - 2017-12-06 07:30:13.887317: step 9890, loss = 2.04, batch loss = 1.98 (17.6 examples/sec; 0.455 sec/batch; 40h:45m:54s remains)
INFO - root - 2017-12-06 07:30:18.278678: step 9900, loss = 2.07, batch loss = 2.01 (18.2 examples/sec; 0.439 sec/batch; 39h:18m:36s remains)
2017-12-06 07:30:18.413631: E tensorflow/core/util/events_writer.cc:162] The events file /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116 has disappeared.
2017-12-06 07:30:18.413684: E tensorflow/core/util/events_writer.cc:131] Failed to flush 50 events to /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6-sgd-clip5-fixqian-lr0.01/events.out.tfevents.1512538788.GCRAZGDL116
2017-12-06 07:30:18.726013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2561913 -4.2583847 -4.2698116 -4.2763834 -4.2826762 -4.28661 -4.2877932 -4.2848334 -4.2813787 -4.2806191 -4.2802329 -4.2713594 -4.2595544 -4.2452435 -4.2299609][-4.2522273 -4.25727 -4.2702436 -4.2726068 -4.2741075 -4.2746677 -4.2716212 -4.2655687 -4.2612205 -4.2576733 -4.2566204 -4.2467661 -4.2345657 -4.2177486 -4.19644][-4.2433672 -4.2542152 -4.2647953 -4.2624865 -4.2573528 -4.249712 -4.2423859 -4.2335267 -4.2286572 -4.2254586 -4.2245688 -4.2162309 -4.2064342 -4.1864576 -4.1584697][-4.2374816 -4.2499142 -4.2529173 -4.2432756 -4.2314568 -4.2200451 -4.2092934 -4.1950307 -4.1897688 -4.1946745 -4.2015328 -4.1988378 -4.1925235 -4.1717644 -4.13983][-4.2457895 -4.2517648 -4.240921 -4.2205586 -4.2042131 -4.1910825 -4.1753087 -4.1548309 -4.1483297 -4.1634846 -4.1852283 -4.1924214 -4.1918674 -4.1724834 -4.1420455][-4.2525105 -4.2535682 -4.2310753 -4.2018166 -4.1805053 -4.1629167 -4.142972 -4.1169744 -4.1078987 -4.1317897 -4.1641073 -4.1821589 -4.1931262 -4.1818824 -4.1548939][-4.2514391 -4.249898 -4.2223186 -4.1870418 -4.1593847 -4.1344562 -4.110054 -4.0824432 -4.0762615 -4.1042914 -4.1389761 -4.1635542 -4.1869912 -4.1853242 -4.163271][-4.2293954 -4.2273293 -4.2004838 -4.1661358 -4.1305408 -4.0987883 -4.0729775 -4.0561414 -4.06311 -4.093761 -4.1252465 -4.1484628 -4.1730013 -4.1731277 -4.1609311][-4.1934295 -4.1923394 -4.1686 -4.1375208 -4.099966 -4.0641727 -4.0373697 -4.0343361 -4.0545731 -4.0887694 -4.1202559 -4.1410437 -4.1565919 -4.1531558 -4.1490669][-4.1636372 -4.1567955 -4.1281557 -4.0968704 -4.0664296 -4.032238 -4.0054846 -4.0096912 -4.0339513 -4.072227 -4.1092796 -4.1295619 -4.1389961 -4.1329546 -4.1263027][-4.1514325 -4.1329994 -4.0944057 -4.061511 -4.0390482 -4.0152149 -3.9963853 -4.0055223 -4.027967 -4.0657558 -4.1021261 -4.1215978 -4.1294308 -4.1241107 -4.1119986][-4.1495733 -4.1245675 -4.0786543 -4.0409584 -4.0236125 -4.01568 -4.0144281 -4.0320349 -4.0520492 -4.0765305 -4.1007915 -4.1210871 -4.1341834 -4.1300411 -4.1144567][-4.1516905 -4.1311994 -4.0879807 -4.051609 -4.04026 -4.0448971 -4.0607734 -4.0820093 -4.0934706 -4.1023474 -4.1153312 -4.137691 -4.1564527 -4.15392 -4.1349277][-4.1554117 -4.1418557 -4.1059551 -4.0770121 -4.0736318 -4.0871248 -4.1094217 -4.1295495 -4.1382017 -4.1450295 -4.1558547 -4.1746731 -4.1881466 -4.1849403 -4.1646934][-4.1817722 -4.1696835 -4.1429296 -4.122458 -4.1234303 -4.1424017 -4.1640716 -4.181365 -4.1912165 -4.2044158 -4.2166409 -4.2257905 -4.2296348 -4.2247949 -4.2083526]]...]
INFO - root - 2017-12-06 07:30:23.016275: step 9910, loss = 2.08, batch loss = 2.02 (18.9 examples/sec; 0.423 sec/batch; 37h:54m:45s remains)
INFO - root - 2017-12-06 07:30:27.279012: step 9920, loss = 2.08, batch loss = 2.03 (19.4 examples/sec; 0.411 sec/batch; 36h:51m:51s remains)
INFO - root - 2017-12-06 07:30:31.748022: step 9930, loss = 2.08, batch loss = 2.02 (18.7 examples/sec; 0.428 sec/batch; 38h:22m:43s remains)
INFO - root - 2017-12-06 07:30:36.071437: step 9940, loss = 2.06, batch loss = 2.00 (19.7 examples/sec; 0.407 sec/batch; 36h:26m:23s remains)
INFO - root - 2017-12-06 07:30:40.426785: step 9950, loss = 2.08, batch loss = 2.03 (19.3 examples/sec; 0.415 sec/batch; 37h:08m:17s remains)
INFO - root - 2017-12-06 07:30:44.747583: step 9960, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.442 sec/batch; 39h:36m:34s remains)
INFO - root - 2017-12-06 07:30:49.173173: step 9970, loss = 2.08, batch loss = 2.02 (17.5 examples/sec; 0.457 sec/batch; 40h:56m:44s remains)
INFO - root - 2017-12-06 07:30:53.252392: step 9980, loss = 2.04, batch loss = 1.98 (18.0 examples/sec; 0.444 sec/batch; 39h:46m:07s remains)
