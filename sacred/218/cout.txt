INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "218"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 10:57:05.535566: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:57:05.535632: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:57:05.535640: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:57:05.535645: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:57:05.535649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:57:06.556535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-11 10:57:06.556571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 10:57:06.556579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 10:57:06.556587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-11 10:57:11.150277: step 0, loss = 2.28, batch loss = 2.23 (2.4 examples/sec; 3.279 sec/batch; 302h:53m:35s remains)
2017-12-11 10:57:11.586175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3786516 -4.3785357 -4.378449 -4.3783388 -4.3782125 -4.3780751 -4.37793 -4.3777943 -4.377707 -4.377697 -4.3777738 -4.3779383 -4.3781624 -4.378396 -4.3786268][-4.3784728 -4.3783207 -4.3781877 -4.3780189 -4.3778219 -4.3776021 -4.3773651 -4.3771577 -4.37704 -4.377049 -4.3771868 -4.3774471 -4.3777795 -4.37811 -4.3784223][-4.3783326 -4.3781433 -4.3779626 -4.3777289 -4.377449 -4.37713 -4.3767886 -4.3765016 -4.3763547 -4.3763981 -4.3766203 -4.3769941 -4.3774462 -4.3778815 -4.3782759][-4.3781848 -4.3779669 -4.3777504 -4.3774562 -4.3770943 -4.3766761 -4.3762302 -4.3758674 -4.3756995 -4.3757958 -4.3761139 -4.3766012 -4.3771639 -4.3776889 -4.3781495][-4.3780375 -4.3777976 -4.3775496 -4.3772044 -4.3767748 -4.3762789 -4.375752 -4.3753328 -4.3751707 -4.3753467 -4.3757639 -4.3763461 -4.3769865 -4.3775673 -4.3780646][-4.3778944 -4.3776317 -4.3773584 -4.3769789 -4.3765116 -4.3759809 -4.3754292 -4.3750081 -4.374908 -4.3751931 -4.3756881 -4.3763094 -4.37696 -4.377543 -4.378037][-4.3777704 -4.3774862 -4.3771963 -4.3768106 -4.3763518 -4.375855 -4.37537 -4.3750377 -4.3750596 -4.3754292 -4.3759365 -4.376513 -4.3770943 -4.3776217 -4.3780732][-4.3776755 -4.3773861 -4.3771024 -4.3767457 -4.3763471 -4.3759542 -4.3756213 -4.3754554 -4.3755889 -4.3759584 -4.3764 -4.3768625 -4.377326 -4.3777642 -4.3781519][-4.37761 -4.3773327 -4.3770814 -4.3767805 -4.3764706 -4.3762107 -4.3760471 -4.3760285 -4.3762164 -4.376533 -4.3768735 -4.3772092 -4.3775568 -4.3779087 -4.3782358][-4.3775921 -4.3773375 -4.3771324 -4.3769021 -4.3766885 -4.3765526 -4.3765211 -4.3765888 -4.3767748 -4.3770161 -4.3772559 -4.3774843 -4.3777428 -4.3780303 -4.3783112][-4.3776274 -4.3774118 -4.3772616 -4.3771033 -4.3769789 -4.3769326 -4.3769732 -4.3770638 -4.3772054 -4.3773651 -4.3775196 -4.3776741 -4.3778806 -4.3781266 -4.3783746][-4.3777108 -4.3775516 -4.3774633 -4.3773756 -4.3773189 -4.3773189 -4.3773732 -4.3774347 -4.3775077 -4.3775845 -4.37767 -4.37778 -4.3779597 -4.3781862 -4.3784161][-4.3778105 -4.3777132 -4.3776879 -4.3776593 -4.3776445 -4.3776517 -4.3776784 -4.3776813 -4.377677 -4.3776784 -4.3777118 -4.3778 -4.3779764 -4.3782024 -4.3784289][-4.3778806 -4.3778286 -4.3778491 -4.3778591 -4.3778648 -4.3778553 -4.3778319 -4.3777676 -4.3776917 -4.37764 -4.3776484 -4.3777409 -4.3779345 -4.3781772 -4.3784151][-4.3778687 -4.3778405 -4.3778887 -4.3779206 -4.3779297 -4.377893 -4.3778162 -4.3776956 -4.377573 -4.3774962 -4.37751 -4.37763 -4.3778563 -4.3781252 -4.3783827]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50
INFO - root - 2017-12-11 10:57:14.401095: step 10, loss = 2.22, batch loss = 2.13 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:57s remains)
INFO - root - 2017-12-11 10:57:16.528286: step 20, loss = 2.01, batch loss = 1.87 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:07s remains)
INFO - root - 2017-12-11 10:57:18.672377: step 30, loss = 1.99, batch loss = 1.82 (37.7 examples/sec; 0.212 sec/batch; 19h:36m:06s remains)
INFO - root - 2017-12-11 10:57:20.826362: step 40, loss = 1.91, batch loss = 1.72 (37.4 examples/sec; 0.214 sec/batch; 19h:46m:21s remains)
INFO - root - 2017-12-11 10:57:22.960544: step 50, loss = 1.74, batch loss = 1.52 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:57s remains)
INFO - root - 2017-12-11 10:57:25.090845: step 60, loss = 1.37, batch loss = 1.12 (37.0 examples/sec; 0.216 sec/batch; 19h:59m:21s remains)
INFO - root - 2017-12-11 10:57:27.231240: step 70, loss = 2.21, batch loss = 1.92 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:50s remains)
INFO - root - 2017-12-11 10:57:29.381600: step 80, loss = 2.28, batch loss = 1.97 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:27s remains)
INFO - root - 2017-12-11 10:57:31.484179: step 90, loss = 1.27, batch loss = 0.94 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:57s remains)
INFO - root - 2017-12-11 10:57:33.591064: step 100, loss = 1.69, batch loss = 1.33 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:04s remains)
2017-12-11 10:57:33.954442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3257148 -1.3462029 -1.3834455 -1.5019164 -1.6171765 -1.7573285 -2.0215919 -1.9419117 -2.0903916 -2.9036012 -3.5347311 -3.6680841 -3.859844 -3.7535167 -3.6483755][-1.3591912 -1.3280025 -1.2961574 -1.2994661 -1.2737389 -1.3816702 -1.4891405 -1.7147958 -2.1320949 -2.2875867 -2.5529976 -3.0928442 -3.7596796 -3.8930438 -3.9637783][-1.4099622 -1.2795432 -1.1487815 -1.2093873 -1.2694676 -1.2809913 -1.2467275 -1.4123778 -1.5779841 -1.8783362 -2.3715467 -2.4324489 -2.7252741 -3.3911612 -3.9699249][-0.98152423 -1.1956215 -1.2899766 -1.0244329 -0.87713456 -0.97821355 -1.0623143 -1.0393622 -1.0165253 -1.2280397 -1.7186682 -2.1663675 -2.7806005 -3.1032739 -3.4194486][-0.96571374 -0.79328132 -0.714175 -0.82918406 -0.77329421 -0.62731576 -0.48339581 -0.46164751 -0.73102474 -1.1832538 -1.6348112 -2.5317261 -2.9506054 -3.4477966 -3.7635252][-0.48412275 -0.4742527 -0.3785646 -0.35143065 -0.26714826 -0.27282143 -0.27824807 -0.56605792 -0.9449172 -1.5457923 -2.5532336 -2.9908838 -3.4539165 -3.7294116 -3.9447827][-0.34419751 -0.36551595 -0.38603544 -0.32376385 -0.24939251 -0.17495394 -0.049108028 -0.54008031 -1.0461755 -1.6186829 -2.7589004 -3.2861619 -3.8160083 -4.0181046 -3.9425359][-0.346869 -0.32130361 -0.2931304 -0.26645589 -0.19134259 -0.11081529 0.011770248 -0.50715351 -1.0461805 -1.6958666 -2.860337 -3.3707509 -4.0174417 -4.0174527 -4.06647][-0.5775125 -0.57015705 -0.57634354 -0.53109646 -0.45966458 -0.40408707 -0.32671547 -0.81693673 -1.2969263 -1.8415415 -2.9080443 -3.3873503 -4.06647 -4.06647 -4.0664697][-0.93602419 -1.1742053 -1.1747732 -1.1639555 -1.1455774 -1.0661874 -0.97248912 -1.4308212 -1.8513107 -2.3300424 -3.1709962 -3.5146227 -4.06647 -4.06647 -4.06647][-0.94144535 -1.0613182 -1.180357 -1.2956684 -1.2928021 -1.2751067 -1.2464108 -1.5953581 -1.9363658 -2.4564588 -3.2802505 -3.5720625 -4.06647 -4.06647 -4.0664697][-1.3258357 -1.4063377 -1.4075003 -1.4067512 -1.402492 -1.3877928 -1.3715522 -1.747895 -2.1078877 -2.5573871 -3.3063686 -3.6123266 -4.0664697 -3.9551892 -3.8435264][-1.1156824 -1.1470823 -1.1447253 -1.1922536 -1.1881356 -1.176595 -1.1625354 -1.5352337 -1.9170063 -2.0628576 -2.5193553 -3.0106025 -3.6333411 -3.7458377 -3.7473836][-0.67493796 -0.6754837 -0.67455912 -0.67171884 -0.66377592 -0.66110778 -0.65531921 -1.0185332 -1.3852806 -1.8417993 -2.3071759 -2.2663269 -2.6680441 -3.0749702 -3.4877002][-1.4625716 -1.1676986 -1.167486 -1.2715757 -1.3738348 -1.6707435 -1.9661181 -1.9607708 -2.2549019 -2.7029524 -3.1566169 -3.2330866 -3.4363823 -3.5395868 -3.6432674]]...]
INFO - root - 2017-12-11 10:57:36.099100: step 110, loss = 2.32, batch loss = 1.94 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:47s remains)
INFO - root - 2017-12-11 10:57:38.235861: step 120, loss = 2.40, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:57s remains)
INFO - root - 2017-12-11 10:57:40.335762: step 130, loss = 2.32, batch loss = 1.90 (37.7 examples/sec; 0.212 sec/batch; 19h:37m:00s remains)
INFO - root - 2017-12-11 10:57:42.430650: step 140, loss = 2.06, batch loss = 1.62 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:32s remains)
INFO - root - 2017-12-11 10:57:44.536283: step 150, loss = 2.24, batch loss = 1.78 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-11 10:57:46.608784: step 160, loss = 2.26, batch loss = 1.78 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:57s remains)
INFO - root - 2017-12-11 10:57:48.696671: step 170, loss = 2.19, batch loss = 1.69 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:35s remains)
INFO - root - 2017-12-11 10:57:50.739386: step 180, loss = 2.10, batch loss = 1.58 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:48s remains)
INFO - root - 2017-12-11 10:57:52.794790: step 190, loss = 2.05, batch loss = 1.52 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:39s remains)
INFO - root - 2017-12-11 10:57:54.843632: step 200, loss = 2.04, batch loss = 1.50 (39.4 examples/sec; 0.203 sec/batch; 18h:44m:39s remains)
2017-12-11 10:57:55.243308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392][-2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392 -2.901392]]...]
INFO - root - 2017-12-11 10:57:57.275512: step 210, loss = 1.99, batch loss = 1.44 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:49s remains)
INFO - root - 2017-12-11 10:57:59.285815: step 220, loss = 1.87, batch loss = 1.32 (40.5 examples/sec; 0.197 sec/batch; 18h:13m:01s remains)
INFO - root - 2017-12-11 10:58:01.344207: step 230, loss = 1.86, batch loss = 1.28 (38.4 examples/sec; 0.209 sec/batch; 19h:15m:07s remains)
INFO - root - 2017-12-11 10:58:03.430495: step 240, loss = 1.78, batch loss = 1.20 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:21s remains)
INFO - root - 2017-12-11 10:58:05.506699: step 250, loss = 1.75, batch loss = 1.15 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-11 10:58:07.564484: step 260, loss = 1.71, batch loss = 1.09 (39.7 examples/sec; 0.201 sec/batch; 18h:35m:15s remains)
INFO - root - 2017-12-11 10:58:09.584198: step 270, loss = 1.65, batch loss = 1.02 (40.8 examples/sec; 0.196 sec/batch; 18h:07m:02s remains)
INFO - root - 2017-12-11 10:58:11.684481: step 280, loss = 1.68, batch loss = 1.02 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:20s remains)
INFO - root - 2017-12-11 10:58:13.771227: step 290, loss = 1.68, batch loss = 0.99 (38.4 examples/sec; 0.208 sec/batch; 19h:14m:06s remains)
INFO - root - 2017-12-11 10:58:15.788154: step 300, loss = 1.68, batch loss = 0.96 (40.7 examples/sec; 0.197 sec/batch; 18h:09m:03s remains)
2017-12-11 10:58:16.152787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694][-1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694 -1.4936694]]...]
INFO - root - 2017-12-11 10:58:18.199455: step 310, loss = 1.67, batch loss = 0.93 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:56s remains)
INFO - root - 2017-12-11 10:58:20.240231: step 320, loss = 1.66, batch loss = 0.90 (38.8 examples/sec; 0.206 sec/batch; 19h:02m:11s remains)
INFO - root - 2017-12-11 10:58:22.325595: step 330, loss = 1.64, batch loss = 0.86 (38.4 examples/sec; 0.208 sec/batch; 19h:12m:06s remains)
