INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "266"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-14 10:40:46.393495: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 10:40:46.393536: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 10:40:46.393542: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 10:40:46.393547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-14 10:40:46.393551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-14 10:40:51.177815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-14 10:40:51.177853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-14 10:40:51.177859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-14 10:40:51.177885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-14 10:41:13.037476: step 0, loss = 2.28, batch loss = 2.23 (0.6 examples/sec; 12.423 sec/batch; 1147h:22m:14s remains)
2017-12-14 10:41:13.768976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143][-4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143 -4.329143]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip--10-10-adam/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip--10-10-adam/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip--10-10-adam
INFO - root - 2017-12-14 10:41:20.493985: step 10, loss = 41.79, batch loss = 1.75 (18.6 examples/sec; 0.430 sec/batch; 39h:40m:56s remains)
INFO - root - 2017-12-14 10:41:24.988287: step 20, loss = 36.57, batch loss = 1.30 (18.6 examples/sec; 0.430 sec/batch; 39h:44m:50s remains)
INFO - root - 2017-12-14 10:41:29.407698: step 30, loss = 20.46, batch loss = 0.95 (17.2 examples/sec; 0.466 sec/batch; 43h:02m:34s remains)
INFO - root - 2017-12-14 10:41:33.857653: step 40, loss = 10.20, batch loss = 0.75 (17.7 examples/sec; 0.452 sec/batch; 41h:42m:41s remains)
INFO - root - 2017-12-14 10:41:37.954853: step 50, loss = 5.14, batch loss = 0.69 (19.0 examples/sec; 0.422 sec/batch; 38h:58m:55s remains)
INFO - root - 2017-12-14 10:41:42.233778: step 60, loss = 2.81, batch loss = 0.70 (19.0 examples/sec; 0.421 sec/batch; 38h:52m:52s remains)
INFO - root - 2017-12-14 10:41:46.515343: step 70, loss = 1.73, batch loss = 0.70 (18.6 examples/sec; 0.430 sec/batch; 39h:41m:26s remains)
INFO - root - 2017-12-14 10:41:50.913704: step 80, loss = 1.22, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 40h:12m:19s remains)
INFO - root - 2017-12-14 10:41:55.344231: step 90, loss = 0.97, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:23m:43s remains)
INFO - root - 2017-12-14 10:41:59.739543: step 100, loss = 0.84, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:50m:34s remains)
2017-12-14 10:42:00.265021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211][-0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211 -0.028354211]]...]
INFO - root - 2017-12-14 10:42:04.758986: step 110, loss = 0.78, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 40h:12m:16s remains)
INFO - root - 2017-12-14 10:42:09.126855: step 120, loss = 0.74, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:37m:22s remains)
INFO - root - 2017-12-14 10:42:13.529142: step 130, loss = 0.72, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:55m:01s remains)
INFO - root - 2017-12-14 10:42:17.867779: step 140, loss = 0.71, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:22m:29s remains)
INFO - root - 2017-12-14 10:42:21.912034: step 150, loss = 0.71, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 40h:08m:32s remains)
INFO - root - 2017-12-14 10:42:26.317145: step 160, loss = 0.70, batch loss = 0.69 (18.2 examples/sec; 0.441 sec/batch; 40h:41m:08s remains)
INFO - root - 2017-12-14 10:42:30.654178: step 170, loss = 0.70, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 41h:16m:35s remains)
INFO - root - 2017-12-14 10:42:35.020501: step 180, loss = 0.70, batch loss = 0.69 (18.8 examples/sec; 0.426 sec/batch; 39h:21m:54s remains)
INFO - root - 2017-12-14 10:42:39.421972: step 190, loss = 0.70, batch loss = 0.69 (17.5 examples/sec; 0.458 sec/batch; 42h:17m:37s remains)
INFO - root - 2017-12-14 10:42:43.808764: step 200, loss = 0.70, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 41h:04m:50s remains)
2017-12-14 10:42:44.330153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965][0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965 0.00020746965]]...]
INFO - root - 2017-12-14 10:42:48.731219: step 210, loss = 0.69, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 41h:13m:08s remains)
INFO - root - 2017-12-14 10:42:53.167737: step 220, loss = 0.69, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:40m:46s remains)
INFO - root - 2017-12-14 10:42:57.524623: step 230, loss = 0.69, batch loss = 0.69 (18.7 examples/sec; 0.429 sec/batch; 39h:34m:28s remains)
INFO - root - 2017-12-14 10:43:01.851114: step 240, loss = 0.69, batch loss = 0.69 (19.0 examples/sec; 0.422 sec/batch; 38h:56m:23s remains)
INFO - root - 2017-12-14 10:43:05.986017: step 250, loss = 0.69, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:25m:40s remains)
INFO - root - 2017-12-14 10:43:10.300183: step 260, loss = 0.69, batch loss = 0.69 (18.9 examples/sec; 0.424 sec/batch; 39h:09m:05s remains)
INFO - root - 2017-12-14 10:43:14.604595: step 270, loss = 0.69, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:55m:15s remains)
INFO - root - 2017-12-14 10:43:18.835075: step 280, loss = 0.69, batch loss = 0.69 (18.9 examples/sec; 0.424 sec/batch; 39h:08m:40s remains)
INFO - root - 2017-12-14 10:43:23.151116: step 290, loss = 0.69, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 39h:15m:40s remains)
INFO - root - 2017-12-14 10:43:27.434972: step 300, loss = 0.69, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:17m:35s remains)
2017-12-14 10:43:27.977925: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06][1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06 1.1199915e-06]]...]
INFO - root - 2017-12-14 10:43:32.372507: step 310, loss = 0.69, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:32m:06s remains)
INFO - root - 2017-12-14 10:43:36.664813: step 320, loss = 0.69, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 39h:28m:49s remains)
