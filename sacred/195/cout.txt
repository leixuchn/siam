INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "195"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4-baias-relu
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-10 07:26:58.296987: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:26:58.297023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:26:58.297029: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:26:58.297033: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:26:58.297038: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 07:26:58.658433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 07:26:58.658470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 07:26:58.658476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 07:26:58.658484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>]
kkkkkkkkkkkkkkkkkkkkkkk [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'OptimiINFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 07:27:01.611812: step 0, loss = 0.75, batch loss = 0.69 (3.6 examples/sec; 2.228 sec/batch; 205h:44m:08s remains)
2017-12-10 07:27:02.012488: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00019978415 0.00021486402 0.0002284537 0.00024345053 0.0002569473 0.0002668628 0.00027801309 0.00029283672 0.00030377496 0.00030412595 0.00029437291 0.00027378474 0.0002465522 0.00021844378 0.00019306797][0.00021422109 0.00023232061 0.00025113081 0.00027352976 0.00029734295 0.0003199405 0.00034157257 0.00036246071 0.00037530641 0.00037302083 0.00035491138 0.00031993818 0.00027915507 0.00024089933 0.0002077323][0.00022601269 0.000244892 0.00026790306 0.00030002007 0.00033811419 0.00037198953 0.00040365642 0.00043412036 0.00045043437 0.00044322727 0.00041288935 0.00036229962 0.00030786518 0.00025953012 0.00021936958][0.00023490207 0.00025243059 0.00027656797 0.00031654994 0.00036684988 0.00041162764 0.00045646774 0.00050471077 0.00052937085 0.00051609217 0.00047133854 0.00040389816 0.0003350917 0.00027607623 0.00022917891][0.00024216171 0.00025868896 0.00028333024 0.00032948976 0.00039224583 0.0004514324 0.00051580841 0.00058809621 0.00061921234 0.00059411605 0.00053166971 0.00044637563 0.00036132309 0.00029116226 0.0002374997][0.00025266237 0.00027217853 0.00029922178 0.00035087118 0.00041986586 0.00048922136 0.00057520706 0.00067772291 0.00071521389 0.00066629372 0.00057800795 0.00047465891 0.00037772011 0.00029904567 0.00024162626][0.0002689641 0.0002933366 0.00032371873 0.00037563639 0.0004429913 0.00052086927 0.00063083787 0.00075853348 0.00078360381 0.00069105276 0.00058062235 0.00047380483 0.00037698488 0.00029745104 0.00024076412][0.0002869883 0.00031602435 0.00034847227 0.00039633614 0.00045601319 0.00053011207 0.0006366829 0.0007455537 0.00073243544 0.00062547351 0.00052652997 0.00043938993 0.00035674291 0.00028611414 0.00023556252][0.00029841374 0.00032973851 0.00036144842 0.00040153068 0.00044545418 0.00049986178 0.00056967244 0.00062679691 0.00059220655 0.00051781657 0.00045747653 0.00039537536 0.00032934439 0.00027252463 0.00022998328][0.00030112505 0.0003331117 0.00036299819 0.00039658946 0.00042621163 0.00045808969 0.00049368181 0.00051465019 0.00048130171 0.00043786154 0.00040548126 0.000361811 0.00030978423 0.00026292045 0.0002255202][0.00029889832 0.00032960795 0.00035796833 0.00038345598 0.0004000222 0.00041439681 0.00043032703 0.00043251424 0.00040378165 0.00037837611 0.00036255055 0.00033012524 0.00028857825 0.00025076937 0.00021938646][0.00029011368 0.00031559158 0.00033833017 0.00035319303 0.0003593713 0.00036451861 0.00037385093 0.00037085079 0.00035102485 0.0003385692 0.00033256045 0.00030580859 0.00027071845 0.00023998939 0.00021361158][0.000277733 0.00029292153 0.00030650402 0.00031272252 0.00031430827 0.00031838525 0.00032974681 0.00033190876 0.00032491211 0.00032277495 0.00032218586 0.00029692904 0.000263264 0.00023473252 0.0002104864][0.000265299 0.00027363098 0.00028197019 0.0002858497 0.000289507 0.00029663526 0.00031096782 0.00032023666 0.00032384857 0.00032956089 0.00032978252 0.00030205547 0.00026575566 0.00023492763 0.00021058517][0.00026388108 0.00027126513 0.00027729 0.00028241304 0.00028925858 0.00029884189 0.00031523811 0.00032957911 0.00033920573 0.00034717665 0.00034401249 0.00031161215 0.00027143635 0.00023715183 0.00021193206]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0005-clip50-initconv1-4-baias-relu/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 07:27:04.600118: step 10, loss = 0.75, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:12s remains)
INFO - root - 2017-12-10 07:27:06.713372: step 20, loss = 0.75, batch loss = 0.69 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:10s remains)
zeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset1/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/offset2/weights/Momentum:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b1/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/weights/Momentum:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/siamese_fc/conv5/def/b2/biases/Momentum:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'OptimizeLoss/detection/biases/Momentum:0' shape=(1,) dtype=float32_ref>]
INFO - root - 2017-12-10 07:27:08.854961: step 30, loss = 0.75, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:16m:09s remains)
INFO - root - 2017-12-10 07:27:10.998070: step 40, loss = 0.75, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:24m:33s remains)
INFO - root - 2017-12-10 07:27:13.123209: step 50, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:07s remains)
INFO - root - 2017-12-10 07:27:15.406744: step 60, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:40m:05s remains)
INFO - root - 2017-12-10 07:27:17.528456: step 70, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:34s remains)
INFO - root - 2017-12-10 07:27:19.678402: step 80, loss = 0.75, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-10 07:27:21.826277: step 90, loss = 0.75, batch loss = 0.69 (35.3 examples/sec; 0.226 sec/batch; 20h:54m:43s remains)
INFO - root - 2017-12-10 07:27:23.947361: step 100, loss = 0.75, batch loss = 0.69 (39.1 examples/sec; 0.204 sec/batch; 18h:52m:50s remains)
2017-12-10 07:27:24.266178: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00037683017 0.00043050028 0.00047958738 0.00052212912 0.00055118586 0.000559207 0.00055158563 0.00053099892 0.00050505606 0.00048009044 0.00046021509 0.00045525873 0.00045950367 0.00047527009 0.00049329619][0.00036637881 0.00041467629 0.00046011133 0.00049740029 0.00052390143 0.00053523877 0.0005331068 0.00051944959 0.00049893191 0.00048028669 0.00046440816 0.00046071302 0.00046467214 0.00048110087 0.00050097716][0.00035498748 0.00039823831 0.00043993845 0.00047539806 0.00050295581 0.00051983324 0.00052579807 0.00052131584 0.00050964108 0.00049751432 0.00048640056 0.000484393 0.00048746605 0.0005011 0.000518117][0.00035054129 0.00038806532 0.00042472265 0.00045828335 0.00048534208 0.00050572096 0.00051838771 0.00052297354 0.00052043761 0.00051679224 0.00051385094 0.00051596377 0.00052187958 0.00053452264 0.00054900703][0.00035124825 0.00038566813 0.00041932872 0.00045105693 0.00047683748 0.00049721036 0.00051190186 0.00052149955 0.00052555208 0.00053055311 0.00053775823 0.00054640212 0.00055746507 0.00057209138 0.00058582018][0.00035102628 0.00038420843 0.00041637677 0.00044717223 0.00047204655 0.0004936133 0.00051125407 0.00052635564 0.00053833035 0.00055142952 0.000566446 0.00058305025 0.00059995224 0.00061573222 0.00062808435][0.00034983357 0.00038349678 0.00041720632 0.00044791627 0.0004717198 0.00049366482 0.00051231706 0.00052925386 0.00054550933 0.00056399382 0.00058534712 0.00060922734 0.00063268904 0.00065401697 0.000670586][0.00034737252 0.00038238073 0.00041752626 0.00044856124 0.00047356728 0.0004965082 0.00051567535 0.00053384015 0.00055240683 0.00057354692 0.00059800781 0.00062455429 0.00065117853 0.00067527551 0.00069477275][0.00034421121 0.00037977486 0.00041594706 0.00044834372 0.00047531279 0.00049794739 0.00051543384 0.0005314913 0.00054839253 0.00056869368 0.00059297378 0.00062215066 0.0006519694 0.00068029593 0.00070364337][0.00034311926 0.00037995368 0.00041732879 0.00045160032 0.00047927734 0.00050066708 0.00051523116 0.00052696595 0.00053698308 0.00055082765 0.00057060551 0.000599783 0.00063311664 0.00066592562 0.00069436344][0.00034141444 0.00037857151 0.00041641595 0.00045228307 0.00048195591 0.00050444884 0.0005179398 0.00052553974 0.0005292031 0.00053656625 0.00055040268 0.0005753974 0.00060739811 0.00064088404 0.00067219872][0.00033663434 0.00037191727 0.00040849473 0.00044415129 0.00047512527 0.00049884093 0.0005125996 0.00051794446 0.00051732955 0.00051944819 0.00052701606 0.00054555328 0.00057211681 0.00060286809 0.00063354708][0.00033100275 0.00036285006 0.00039569038 0.0004285546 0.00045805203 0.00048134197 0.00049481326 0.00050036021 0.00050022913 0.000501172 0.00050539995 0.00051962171 0.00054029428 0.00056597282 0.00059327605][0.00032269652 0.00035120419 0.00038084073 0.00040990609 0.00043620254 0.00045684804 0.00046923396 0.00047529396 0.0004764403 0.00047847591 0.00048285339 0.00049436209 0.0005112834 0.00053156674 0.00055232248][0.00031271196 0.00033608094 0.0003608774 0.00038489181 0.00040718992 0.00042636355 0.00043949365 0.00044819337 0.0004514227 0.00045473396 0.00045900614 0.00046773459 0.00048034784 0.00049455068 0.0005098462]]...]
INFO - root - 2017-12-10 07:27:26.390176: step 110, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:37s remains)
INFO - root - 2017-12-10 07:27:28.477371: step 120, loss = 0.75, batch loss = 0.69 (39.4 examples/sec; 0.203 sec/batch; 18h:46m:10s remains)
INFO - root - 2017-12-10 07:27:30.592464: step 130, loss = 0.75, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-10 07:27:32.724562: step 140, loss = 0.75, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:32m:28s remains)
INFO - root - 2017-12-10 07:27:34.851966: step 150, loss = 0.75, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:37s remains)
INFO - root - 2017-12-10 07:27:36.977961: step 160, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:11s remains)
INFO - root - 2017-12-10 07:27:39.102569: step 170, loss = 0.75, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:37m:39s remains)
INFO - root - 2017-12-10 07:27:41.232628: step 180, loss = 0.75, batch loss = 0.69 (39.3 examples/sec; 0.203 sec/batch; 18h:46m:59s remains)
INFO - root - 2017-12-10 07:27:43.382991: step 190, loss = 0.75, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:16s remains)
INFO - root - 2017-12-10 07:27:45.507319: step 200, loss = 0.75, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:08s remains)
2017-12-10 07:27:45.812138: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0069097625 0.0071398979 0.0072551835 0.007403838 0.0076070991 0.0078476109 0.008099705 0.008322834 0.0084850676 0.0085609816 0.008546982 0.008452436 0.0082857423 0.008067877 0.0078405123][0.007070038 0.0073391893 0.0075368504 0.0078038052 0.008141309 0.0085244831 0.0089070108 0.0092318021 0.0094548 0.0095395874 0.0094949687 0.00932724 0.0090580722 0.0087229777 0.0083762649][0.0071018338 0.0074243043 0.0077267806 0.0081258435 0.0086046709 0.0091243293 0.0096251033 0.010040415 0.010312853 0.010400383 0.01031929 0.010086023 0.0097185327 0.0092487391 0.0087683843][0.0071954783 0.0075897807 0.0079906126 0.0085038953 0.0091002425 0.0097295595 0.010321565 0.010800827 0.011104199 0.011189507 0.011074128 0.010787339 0.010337072 0.0097734509 0.0091957282][0.0073958263 0.0078654224 0.0083502382 0.0089450255 0.0096169654 0.010310708 0.010947047 0.011445584 0.011753771 0.011824404 0.011676464 0.011334562 0.010812416 0.010178408 0.0095253447][0.0077009974 0.0082299821 0.0087643368 0.0093947016 0.010079892 0.010778267 0.011403025 0.01188715 0.01217624 0.012230163 0.012068192 0.011703822 0.01115146 0.010478551 0.0097885672][0.0080654072 0.0086359633 0.0091739856 0.0097793695 0.010420293 0.011059153 0.011619354 0.012057728 0.012312788 0.012355494 0.01219889 0.011852224 0.011317597 0.010644312 0.0099596176][0.0083989855 0.0089748288 0.0094707878 0.010007454 0.010563145 0.011110793 0.011589726 0.011960175 0.012178296 0.012221609 0.012081108 0.011764675 0.011269816 0.010655363 0.010024504][0.008621105 0.009177791 0.0096143968 0.010067988 0.010514927 0.010950726 0.011334898 0.011637965 0.011818804 0.011861715 0.011757047 0.011477874 0.011041299 0.010516365 0.0099724457][0.00871392 0.0092291748 0.0095910374 0.0099537745 0.010290424 0.010622214 0.010910431 0.011142441 0.011296321 0.011366513 0.011321652 0.011099934 0.010745457 0.010304577 0.0098495865][0.008660851 0.0091109378 0.0094090132 0.009681059 0.0099111488 0.010154813 0.01036766 0.010537825 0.010664785 0.010768133 0.010791926 0.010657995 0.010421949 0.010099806 0.0097545432][0.0084879026 0.0088673169 0.0091041159 0.009293084 0.0094547616 0.0096166143 0.0097571779 0.0098725092 0.0099797118 0.010115039 0.010190886 0.010166297 0.010071419 0.0098901214 0.0096712355][0.00823188 0.0085413624 0.0087077823 0.0088257361 0.0089340061 0.0090569388 0.0091682915 0.0092494292 0.0093711587 0.0095510688 0.0096738059 0.0097315013 0.0097508729 0.0097155068 0.0096228728][0.0079507958 0.0082003 0.0083115222 0.008371437 0.008425747 0.0085140681 0.0086057447 0.0086883949 0.0088318372 0.0090552317 0.0092459489 0.0093874782 0.0095152855 0.0095915115 0.009607303][0.0076906639 0.0079000629 0.0079760877 0.0079960907 0.0080273021 0.0080977827 0.0081762923 0.0082655177 0.0084294425 0.00868044 0.0089031328 0.0091119371 0.0093257753 0.0095103914 0.0096316682]]...]
INFO - root - 2017-12-10 07:27:47.929821: step 210, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:39m:17s remains)
INFO - root - 2017-12-10 07:27:50.046601: step 220, loss = 0.75, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-10 07:27:52.167218: step 230, loss = 0.75, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:47s remains)
INFO - root - 2017-12-10 07:27:54.314747: step 240, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:31s remains)
INFO - root - 2017-12-10 07:27:56.433987: step 250, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:30s remains)
INFO - root - 2017-12-10 07:27:58.594544: step 260, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-10 07:28:00.723708: step 270, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:29s remains)
INFO - root - 2017-12-10 07:28:02.894683: step 280, loss = 0.75, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:48s remains)
INFO - root - 2017-12-10 07:28:05.062214: step 290, loss = 0.75, batch loss = 0.69 (36.1 examples/sec; 0.221 sec/batch; 20h:25m:59s remains)
INFO - root - 2017-12-10 07:28:07.213369: step 300, loss = 0.75, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:42s remains)
2017-12-10 07:28:07.582331: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.5308345e-05 0.00010148941 0.00043630679 0.0011443915 0.0021938204 0.0033972056 0.0045676394 0.0054652421 0.0057595675 0.005322211 0.0044091674 0.0033255359 0.0022858204 0.0014112419 0.00082030933][5.1586485e-05 0.00016003393 0.00054303295 0.0013183702 0.002463269 0.003799129 0.0051129311 0.0061517931 0.0065499265 0.0061463807 0.0051908926 0.0039957678 0.0028146191 0.0017848727 0.0010566033][0.00017708109 0.00036249805 0.00079650176 0.0015814699 0.0026908542 0.0039768759 0.0052381833 0.0062578321 0.0066910256 0.0063752 0.0055152532 0.0043806126 0.0032243778 0.0021782506 0.001399418][0.00051073392 0.0008624625 0.0014065645 0.0021946942 0.0031986507 0.00432659 0.0053956131 0.0062613366 0.0066589471 0.006457794 0.0057908669 0.0048478367 0.003849468 0.0029040945 0.0021479791][0.0011120308 0.0017353006 0.0024782575 0.0033411342 0.0042777304 0.0052341227 0.0060626236 0.0067176088 0.0070301923 0.0069190362 0.006461943 0.0057580913 0.0049721566 0.0041768136 0.0034751152][0.0018598492 0.002812607 0.0038271921 0.0048527261 0.0058084023 0.0066468394 0.0072721257 0.0077369008 0.0079611614 0.00792451 0.0076480862 0.0071681342 0.0065830168 0.0059300056 0.0052816146][0.0025237279 0.0037711747 0.0050426777 0.0062381634 0.0072546918 0.0080497591 0.0085692639 0.0089233834 0.0091021648 0.0091348477 0.008972791 0.008649067 0.0082229078 0.0077055423 0.0071383324][0.0028517845 0.0042386837 0.0056387763 0.0069292774 0.0079940762 0.0087870453 0.0093032075 0.009668787 0.0098802065 0.0099884607 0.0099088345 0.0096880505 0.0093741845 0.0089818062 0.0085314838][0.0026685381 0.0039614779 0.0052729151 0.0065015079 0.0075507588 0.0083833681 0.0089678746 0.0093959915 0.0096805217 0.0098830434 0.0098929787 0.0097774705 0.0095948242 0.0093683684 0.0090918187][0.0020767809 0.00308996 0.0041398732 0.0051691295 0.0061067385 0.0069122394 0.0075408104 0.0080398275 0.0083978549 0.0086678909 0.0087758508 0.0087966127 0.0087849218 0.0087602818 0.0086981719][0.001328332 0.0020100018 0.0027391172 0.003490763 0.0042203981 0.004902612 0.0054814373 0.0059579005 0.0063385768 0.0066561541 0.0068601039 0.0070120497 0.0071764817 0.0073609338 0.0075142407][0.00069113739 0.0010860285 0.0015259649 0.0020058271 0.0025002353 0.0029938242 0.0034396492 0.003822712 0.0041572987 0.0044524902 0.0046968097 0.0049298243 0.0052057095 0.0055278321 0.0058286451][0.00029821062 0.00049305637 0.0007178646 0.00098366989 0.0012674109 0.0015631571 0.0018505745 0.0021054533 0.00234973 0.0025720631 0.0028005145 0.0030465315 0.0033477203 0.0037008931 0.004046937][0.00011591766 0.00020755087 0.00030757504 0.00043949444 0.00058288657 0.00074645918 0.00091565953 0.0010581359 0.0012083013 0.0013468199 0.0015064623 0.0016899612 0.0019255765 0.0021952286 0.0024807283][5.8745e-05 0.00010723672 0.00015769331 0.00022824119 0.00029979227 0.00039492809 0.00051454967 0.00061860448 0.00071436225 0.00078203011 0.00086450181 0.00095539016 0.0010829675 0.0012320675 0.0014230143]]...]
INFO - root - 2017-12-10 07:28:09.749868: step 310, loss = 0.75, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:29s remains)
INFO - root - 2017-12-10 07:28:11.927620: step 320, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-10 07:28:14.126966: step 330, loss = 0.75, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:33s remains)
INFO - root - 2017-12-10 07:28:16.291670: step 340, loss = 0.75, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:48s remains)
INFO - root - 2017-12-10 07:28:18.502067: step 350, loss = 0.75, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:37m:04s remains)
INFO - root - 2017-12-10 07:28:20.697841: step 360, loss = 0.75, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-10 07:28:22.936644: step 370, loss = 0.75, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:02s remains)
INFO - root - 2017-12-10 07:28:25.159971: step 380, loss = 0.75, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:03s remains)
INFO - root - 2017-12-10 07:28:27.358492: step 390, loss = 0.75, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:03s remains)
INFO - root - 2017-12-10 07:28:29.592042: step 400, loss = 0.75, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:09s remains)
2017-12-10 07:28:29.936499: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.044394087 0.043125708 0.041513886 0.040495086 0.040237956 0.040871922 0.042431373 0.044565566 0.046895012 0.049148098 0.051157445 0.05272169 0.053574096 0.053928003 0.053934161][0.042429216 0.040533971 0.038452424 0.037217509 0.037019685 0.038048327 0.040169574 0.042897224 0.04582794 0.048634797 0.051110569 0.052982166 0.054003727 0.054453418 0.054489288][0.041529566 0.039208729 0.036892328 0.035539169 0.035375703 0.036636263 0.039116777 0.042247128 0.045583379 0.048723828 0.051489867 0.053510342 0.054581083 0.055000842 0.05495913][0.042310588 0.040018238 0.037786715 0.03649975 0.036383186 0.037635677 0.040096376 0.043228313 0.0465521 0.049696956 0.052474063 0.054536119 0.055635776 0.055958126 0.0557967][0.044655003 0.042774737 0.040918756 0.039923128 0.039920628 0.041062683 0.043333314 0.046176333 0.049180444 0.051999193 0.054495633 0.056336094 0.057219386 0.057409655 0.057090092][0.047894664 0.046736032 0.045581006 0.045207188 0.045558896 0.0467189 0.048709333 0.0509965 0.053314235 0.055385295 0.057215054 0.058510669 0.05900348 0.058981013 0.058509681][0.051292069 0.050914153 0.05050493 0.050710376 0.051353138 0.0525183 0.054196566 0.055941198 0.057616528 0.058907121 0.060013447 0.060705107 0.060782742 0.06047089 0.0597882][0.05426218 0.054616693 0.054939061 0.055653539 0.056551334 0.057674114 0.058976244 0.060236476 0.06124654 0.061911862 0.062438436 0.062654346 0.062413912 0.061825223 0.060944136][0.05665594 0.057650831 0.058512531 0.059577163 0.060646173 0.061676327 0.0626324 0.063474543 0.063958436 0.064142682 0.06419386 0.064051472 0.063608676 0.062875986 0.061913859][0.05834135 0.059721909 0.060915578 0.062163718 0.063239053 0.064192072 0.064978436 0.065589555 0.065795496 0.06568417 0.065426812 0.065019011 0.06438382 0.063544638 0.062529065][0.059347678 0.060948089 0.062239278 0.063536085 0.064527079 0.065359309 0.066017017 0.066474959 0.066554472 0.066319145 0.065927319 0.065365434 0.064700156 0.063805021 0.062759236][0.060127158 0.061748762 0.062939793 0.06403368 0.0647388 0.065333962 0.065782741 0.066094175 0.066136323 0.065986618 0.065746337 0.065221243 0.0646379 0.063789979 0.062808856][0.0601514 0.061701536 0.062741376 0.063630924 0.064143121 0.064613782 0.064976409 0.065269604 0.065379664 0.065361813 0.065255217 0.06485872 0.064396426 0.063642986 0.062776424][0.059133023 0.060531255 0.061464962 0.062166218 0.062527373 0.062855326 0.063191719 0.063427694 0.063599765 0.063746445 0.06381318 0.06360209 0.063264094 0.062716551 0.062081277][0.057632711 0.058763515 0.059512328 0.060118914 0.060447969 0.060791034 0.061255153 0.06163362 0.061999325 0.062306408 0.062547207 0.0624782 0.062229373 0.061876334 0.061455503]]...]
INFO - root - 2017-12-10 07:28:32.163065: step 410, loss = 0.76, batch loss = 0.69 (34.0 examples/sec; 0.235 sec/batch; 21h:42m:09s remains)
INFO - root - 2017-12-10 07:28:34.360307: step 420, loss = 0.75, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:55s remains)
INFO - root - 2017-12-10 07:28:36.556214: step 430, loss = 0.75, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:46s remains)
INFO - root - 2017-12-10 07:28:38.755075: step 440, loss = 0.75, batch loss = 0.69 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:44s remains)
INFO - root - 2017-12-10 07:28:40.928580: step 450, loss = 0.75, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:37s remains)
INFO - root - 2017-12-10 07:28:43.141882: step 460, loss = 0.75, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:13s remains)
INFO - root - 2017-12-10 07:28:45.333125: step 470, loss = 0.75, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-10 07:28:47.491382: step 480, loss = 0.75, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:42s remains)
INFO - root - 2017-12-10 07:28:49.702574: step 490, loss = 0.75, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:05s remains)
INFO - root - 2017-12-10 07:28:51.878440: step 500, loss = 0.75, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:57s remains)
2017-12-10 07:28:52.235343: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00040689146 0.00030630361 0.00019300425 0.00028374541 0.00036082615 0.000745381 0.0023616587 0.0061371736 0.012468184 0.021122679 0.031679448 0.043480922 0.055086754 0.065570526 0.073490366][0.0015949523 0.001426089 0.001210138 0.0011144945 0.00095342414 0.0011645056 0.0025789964 0.0061383476 0.012324025 0.021082511 0.031875674 0.043998718 0.055990633 0.066867687 0.0749353][0.0046143122 0.0044622202 0.0040413933 0.0035279796 0.0028478056 0.0024655445 0.0033190935 0.0063494369 0.011939609 0.020316537 0.030793738 0.042693522 0.054588292 0.065553129 0.073873147][0.011411447 0.011409595 0.010440458 0.0087706586 0.006754613 0.0050531905 0.004736389 0.0067267516 0.011634396 0.019786054 0.030177036 0.041822374 0.053400595 0.064159617 0.072488293][0.02309577 0.02301373 0.021025673 0.017779402 0.013989649 0.01045477 0.008469074 0.00925793 0.013617538 0.021769991 0.032209344 0.0435746 0.054546606 0.064427562 0.072103493][0.0372284 0.037023243 0.034334749 0.030040236 0.02499369 0.020252811 0.016818279 0.016370164 0.019971887 0.027659303 0.03765586 0.048222575 0.05812965 0.066521645 0.072917625][0.050733067 0.050691482 0.048039239 0.043721072 0.038573913 0.033799857 0.030123523 0.029238356 0.032165658 0.038946725 0.047764663 0.056664605 0.06475006 0.0709941 0.0755953][0.061886087 0.062118813 0.059902985 0.056407176 0.052236695 0.048534825 0.045727182 0.044960648 0.047062479 0.052141357 0.058829516 0.065264374 0.071101665 0.0754305 0.0786216][0.07095848 0.071272381 0.069526464 0.066872485 0.063732892 0.061295182 0.0595864 0.059261568 0.060778804 0.064076513 0.0685195 0.0727088 0.076582864 0.079346769 0.081669793][0.077174231 0.077299327 0.075650856 0.0735614 0.071207941 0.069771782 0.068976618 0.069422722 0.070911445 0.073221132 0.076178856 0.07880988 0.081326216 0.082983546 0.084376916][0.0798316 0.079681039 0.078088321 0.076305367 0.074402615 0.073494315 0.0732561 0.074211389 0.075765319 0.077732638 0.080015391 0.08188203 0.083751522 0.084829934 0.085618362][0.07834392 0.077970855 0.076320581 0.07470303 0.073159404 0.072443828 0.0724409 0.073470011 0.074811369 0.076700829 0.078772135 0.080450036 0.08206819 0.082869 0.08336015][0.07119447 0.070582151 0.069180548 0.068056069 0.066975355 0.066517927 0.066587307 0.0672326 0.06801784 0.069566421 0.071280956 0.073049158 0.074620813 0.075484045 0.075981922][0.059400536 0.058705088 0.057625514 0.056770679 0.056025371 0.055833735 0.056137092 0.056742389 0.057420928 0.058413193 0.059553262 0.061146948 0.06235934 0.06324178 0.063836083][0.043810867 0.043523632 0.042848129 0.042295728 0.042001579 0.042062353 0.042441893 0.043064095 0.04372533 0.044561222 0.045379803 0.046290156 0.046875417 0.047503795 0.04780281]]...]
INFO - root - 2017-12-10 07:28:54.459396: step 510, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:02s remains)
INFO - root - 2017-12-10 07:28:56.683845: step 520, loss = 0.74, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:29s remains)
INFO - root - 2017-12-10 07:28:58.910262: step 530, loss = 0.75, batch loss = 0.68 (34.4 examples/sec; 0.232 sec/batch; 21h:25m:45s remains)
INFO - root - 2017-12-10 07:29:01.151189: step 540, loss = 0.75, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:37s remains)
INFO - root - 2017-12-10 07:29:03.367078: step 550, loss = 0.75, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:05s remains)
INFO - root - 2017-12-10 07:29:05.598382: step 560, loss = 0.75, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:46s remains)
INFO - root - 2017-12-10 07:29:07.854235: step 570, loss = 0.75, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:12s remains)
INFO - root - 2017-12-10 07:29:10.079174: step 580, loss = 0.75, batch loss = 0.68 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-10 07:29:12.282025: step 590, loss = 0.75, batch loss = 0.68 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:38s remains)
INFO - root - 2017-12-10 07:29:14.463284: step 600, loss = 0.75, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:00s remains)
2017-12-10 07:29:14.806656: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13416225 0.13540931 0.13500768 0.13312404 0.13039611 0.12557127 0.11751909 0.1069462 0.093379311 0.079000257 0.064288467 0.052434128 0.045997575 0.044564795 0.046459448][0.13850424 0.13961552 0.13995278 0.13909096 0.13748929 0.13319494 0.12609114 0.11621232 0.10304093 0.088977955 0.074371614 0.062466472 0.055471748 0.052797712 0.0525089][0.14045823 0.14228085 0.14336967 0.14350554 0.14332983 0.14024319 0.13467045 0.1263873 0.11492924 0.10259287 0.089516118 0.07843364 0.070762254 0.066135757 0.06240391][0.14426155 0.14762615 0.14994593 0.15181625 0.15320562 0.15169784 0.14831096 0.14204176 0.13279316 0.12223943 0.11089617 0.10047359 0.091489069 0.084124513 0.076055788][0.1485251 0.15396811 0.15751076 0.16112205 0.16416609 0.1645816 0.1636491 0.1599251 0.15346389 0.14467214 0.13492402 0.12496527 0.1143353 0.10398094 0.0914337][0.15158369 0.1588266 0.16302118 0.16778164 0.17216778 0.17446071 0.17575844 0.17450354 0.17125525 0.16456383 0.1565243 0.14691174 0.13527285 0.12261041 0.10648983][0.15370329 0.16171172 0.16613393 0.17133614 0.17589489 0.17875737 0.18109231 0.18177868 0.18114942 0.17675658 0.17078559 0.16210921 0.15056615 0.13664816 0.1186129][0.15442419 0.16265909 0.16667709 0.1714171 0.17523211 0.17743473 0.17945889 0.18065327 0.1811385 0.1787242 0.17476207 0.16787137 0.1577148 0.14385915 0.12524308][0.15403792 0.16189408 0.16446055 0.16759075 0.16967724 0.17037539 0.17138895 0.17204592 0.17246227 0.17117441 0.16870189 0.16356115 0.15490913 0.14231403 0.12476435][0.15453668 0.16118936 0.16131276 0.16212852 0.16147871 0.15987188 0.15855907 0.15764703 0.15777278 0.15713423 0.15521957 0.15085603 0.14311773 0.1316656 0.11534756][0.15465537 0.15976629 0.15731174 0.1552072 0.15106747 0.1468752 0.14292903 0.14009643 0.13914225 0.13847379 0.13666478 0.13237315 0.12492809 0.11396874 0.098328516][0.15290636 0.15668963 0.15145184 0.14638738 0.13856475 0.13108407 0.12422062 0.11948722 0.11763263 0.11647955 0.11448853 0.11003963 0.10298177 0.092430666 0.077657089][0.14892329 0.15053706 0.14237916 0.13378523 0.12273835 0.11218823 0.10249943 0.095811158 0.092452168 0.090574354 0.088540018 0.084360108 0.07829082 0.06902805 0.056493223][0.14408438 0.14269195 0.13122451 0.11908685 0.10493144 0.09146785 0.079510495 0.071129426 0.066663526 0.06437213 0.062556341 0.058992188 0.054577086 0.047883492 0.038845304][0.14021328 0.13569264 0.1211822 0.10575386 0.088835731 0.072946317 0.059163813 0.049717464 0.044308066 0.0416234 0.040428367 0.038328536 0.035998091 0.031639874 0.025990563]]...]
INFO - root - 2017-12-10 07:29:17.035415: step 610, loss = 0.75, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:40s remains)
INFO - root - 2017-12-10 07:29:19.260088: step 620, loss = 0.75, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:55s remains)
INFO - root - 2017-12-10 07:29:21.457166: step 630, loss = 0.75, batch loss = 0.68 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-10 07:29:23.687617: step 640, loss = 0.75, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:46s remains)
INFO - root - 2017-12-10 07:29:25.901406: step 650, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:41s remains)
INFO - root - 2017-12-10 07:29:28.079506: step 660, loss = 0.75, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:59s remains)
INFO - root - 2017-12-10 07:29:30.308045: step 670, loss = 0.75, batch loss = 0.68 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:11s remains)
INFO - root - 2017-12-10 07:29:32.521303: step 680, loss = 0.76, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:10s remains)
INFO - root - 2017-12-10 07:29:34.701043: step 690, loss = 0.75, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:55s remains)
INFO - root - 2017-12-10 07:29:36.914159: step 700, loss = 0.75, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:56s remains)
2017-12-10 07:29:37.303977: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4352164 0.43663931 0.43823689 0.4397482 0.44186768 0.44331717 0.44408345 0.44464287 0.44500053 0.44597235 0.44734111 0.45068079 0.45255607 0.45173368 0.44846305][0.44977438 0.45138314 0.45211077 0.45240271 0.45395011 0.45320711 0.45176238 0.45081702 0.44979674 0.44965616 0.45005608 0.45319673 0.45527428 0.45566291 0.45350283][0.4528029 0.45635521 0.45789787 0.45765078 0.45788 0.45627078 0.45425761 0.45277432 0.45145285 0.45046633 0.45064783 0.45233998 0.45354125 0.45264307 0.44961327][0.45391414 0.45933384 0.46120307 0.46108794 0.46119621 0.45878085 0.45606133 0.4544228 0.45293307 0.45155242 0.45140049 0.45184642 0.45217928 0.44958615 0.44516683][0.45188016 0.45808887 0.45896319 0.45884117 0.45915946 0.45734537 0.45556894 0.45397428 0.45233113 0.45110637 0.4508931 0.45078465 0.45055473 0.44778475 0.44328067][0.446329 0.45468068 0.45587119 0.45630461 0.45760167 0.45837545 0.45926663 0.45904267 0.45861539 0.45735466 0.45687822 0.45527053 0.45336956 0.4497835 0.44476584][0.43985304 0.45127213 0.45390332 0.45606863 0.45910546 0.46167856 0.46433425 0.46477261 0.46482781 0.46456864 0.46499088 0.46313336 0.46154398 0.45827171 0.45341071][0.43189067 0.44730228 0.45222813 0.45679381 0.46118852 0.46554476 0.4697324 0.47080308 0.4714486 0.4715417 0.47179341 0.47013471 0.46859068 0.46538687 0.46065709][0.42648241 0.44440597 0.45013285 0.45627764 0.46219075 0.46718466 0.4715991 0.47234946 0.47288233 0.4740108 0.47480631 0.47380474 0.47262418 0.47002831 0.46577016][0.42086342 0.44068834 0.44710258 0.45384118 0.46032739 0.4666169 0.47209385 0.47262055 0.47299516 0.47407785 0.47480994 0.47469473 0.47420922 0.47215226 0.468302][0.41378665 0.43568882 0.44379842 0.450681 0.45719007 0.46354648 0.46904239 0.47018114 0.4718681 0.47254792 0.47275457 0.47154766 0.46992716 0.4675518 0.46409306][0.41365659 0.43532771 0.44334465 0.44982466 0.45466202 0.45885029 0.46252766 0.46332905 0.46490386 0.46612933 0.46678004 0.46589872 0.46436882 0.4613165 0.45729107][0.41300318 0.43374664 0.44120038 0.44721249 0.45155007 0.45365629 0.45550841 0.45565805 0.45658213 0.45654571 0.45612115 0.45616329 0.45557848 0.45342612 0.45052746][0.41358307 0.43283206 0.4395431 0.44513944 0.44941056 0.45176473 0.45387295 0.45339128 0.45360136 0.45284367 0.45171288 0.45102367 0.4499405 0.44867936 0.44689533][0.42068222 0.43893242 0.44512936 0.4496052 0.45201626 0.45355541 0.45497802 0.4544501 0.45394278 0.45311844 0.45186749 0.45041233 0.44872081 0.4471769 0.44600931]]...]
INFO - root - 2017-12-10 07:29:39.539179: step 710, loss = 0.75, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:05s remains)
INFO - root - 2017-12-10 07:29:41.787722: step 720, loss = 0.76, batch loss = 0.70 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:54s remains)
INFO - root - 2017-12-10 07:29:44.033592: step 730, loss = 0.76, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:40s remains)
INFO - root - 2017-12-10 07:29:46.281589: step 740, loss = 0.75, batch loss = 0.68 (36.3 examples/sec; 0.221 sec/batch; 20h:19m:16s remains)
INFO - root - 2017-12-10 07:29:48.471041: step 750, loss = 0.76, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 21h:00m:43s remains)
INFO - root - 2017-12-10 07:29:50.675446: step 760, loss = 0.75, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:15s remains)
INFO - root - 2017-12-10 07:29:52.886065: step 770, loss = 0.76, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:44m:52s remains)
INFO - root - 2017-12-10 07:29:55.097481: step 780, loss = 0.76, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:15s remains)
INFO - root - 2017-12-10 07:29:57.288137: step 790, loss = 0.76, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:35m:32s remains)
INFO - root - 2017-12-10 07:29:59.515121: step 800, loss = 0.76, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:09s remains)
2017-12-10 07:29:59.887423: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071938492 0.088639438 0.10479211 0.11968323 0.13264655 0.14278309 0.15284847 0.16125438 0.16662675 0.16956381 0.17108154 0.17326008 0.17481238 0.17541763 0.17369224][0.086369485 0.10407148 0.12036142 0.13415097 0.14512402 0.15204602 0.15776844 0.16155097 0.16251518 0.16235131 0.16223758 0.16432367 0.1669285 0.16898814 0.16894816][0.10181116 0.1203104 0.13655867 0.14946753 0.15869695 0.16246876 0.16375327 0.16422676 0.16214044 0.15967685 0.15839425 0.16041282 0.16415313 0.16658981 0.16727874][0.11742414 0.13737097 0.15380909 0.1659504 0.17380033 0.17587265 0.17498283 0.17264426 0.16772962 0.16439769 0.1624762 0.16389571 0.16748685 0.16960648 0.17100585][0.12856315 0.1494114 0.16578574 0.17675932 0.18296419 0.18454316 0.1832104 0.18067136 0.17610908 0.17352645 0.17138185 0.17243133 0.17528482 0.17700094 0.1787685][0.13408354 0.15582398 0.17242692 0.18323694 0.18877096 0.19013456 0.18886648 0.18733546 0.18426579 0.18300611 0.18193319 0.18336134 0.18529181 0.18662465 0.18803908][0.13376215 0.15660094 0.17428449 0.18614602 0.19288227 0.19637927 0.19692466 0.19721243 0.1957204 0.19630675 0.1964747 0.19800666 0.1998217 0.20034257 0.20175441][0.12898472 0.15333483 0.17274943 0.18695839 0.19595453 0.20199232 0.20522247 0.20783323 0.20840462 0.2101713 0.21086276 0.21283777 0.21461926 0.2145692 0.2153462][0.12245686 0.14924331 0.17119747 0.1882717 0.19928114 0.2073812 0.21281001 0.21675839 0.2186206 0.22089063 0.22214897 0.22481678 0.22601691 0.22663386 0.22704038][0.11542385 0.14350355 0.16800676 0.1874548 0.20018351 0.20989585 0.21641578 0.22200543 0.2260748 0.22950986 0.23194 0.23497178 0.23647547 0.2383932 0.23925525][0.1104596 0.13976279 0.16662481 0.18825452 0.20305353 0.21343201 0.22024521 0.2268223 0.23196922 0.23566355 0.23877828 0.24210274 0.24391459 0.24505387 0.24518801][0.11315293 0.14281093 0.16963317 0.19165412 0.20653504 0.2164202 0.22281887 0.22920316 0.23486426 0.23936839 0.24399988 0.24823445 0.25155124 0.25335035 0.25338176][0.12133151 0.14976519 0.1748924 0.19479007 0.20792042 0.21711241 0.22328007 0.22963373 0.23576878 0.24105424 0.2459213 0.250606 0.25414005 0.25645235 0.25685865][0.1341238 0.15998551 0.18186846 0.19955586 0.21130498 0.21960345 0.22551957 0.23195001 0.23832737 0.24353407 0.24851444 0.25231272 0.25508645 0.25730464 0.25737584][0.1493742 0.17195167 0.18985325 0.20557627 0.21661271 0.22484495 0.23130386 0.2382655 0.24442594 0.24844417 0.25198889 0.25420824 0.2552363 0.25561404 0.25536966]]...]
INFO - root - 2017-12-10 07:30:02.150487: step 810, loss = 0.76, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:20s remains)
INFO - root - 2017-12-10 07:30:04.380724: step 820, loss = 0.76, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:02s remains)
INFO - root - 2017-12-10 07:30:06.607229: step 830, loss = 0.76, batch loss = 0.70 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:14s remains)
INFO - root - 2017-12-10 07:30:08.833636: step 840, loss = 0.76, batch loss = 0.69 (34.0 examples/sec; 0.235 sec/batch; 21h:39m:02s remains)
INFO - root - 2017-12-10 07:30:11.069901: step 850, loss = 0.76, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:07s remains)
INFO - root - 2017-12-10 07:30:13.283019: step 860, loss = 0.76, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:53s remains)
INFO - root - 2017-12-10 07:30:15.517292: step 870, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:05s remains)
INFO - root - 2017-12-10 07:30:17.734652: step 880, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:38s remains)
INFO - root - 2017-12-10 07:30:19.960336: step 890, loss = 0.76, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:21s remains)
INFO - root - 2017-12-10 07:30:22.216753: step 900, loss = 0.76, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:51s remains)
2017-12-10 07:30:22.696474: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011047421 0.0095097907 0.0085116457 0.0083806515 0.0081995856 0.0085388459 0.0083788857 0.0078929458 0.0068094884 0.0057814023 0.0052382476 0.0055881054 0.0066234986 0.0080934372 0.0095685069][0.01605631 0.012988502 0.010448901 0.0096584093 0.0089731757 0.0091908909 0.0090908352 0.0088613331 0.0078316443 0.0065675867 0.0056623281 0.0060099647 0.0072778757 0.0091538206 0.011124531][0.018673709 0.015460197 0.012711082 0.011502499 0.010653764 0.010833649 0.010641523 0.010294639 0.009071067 0.0078708325 0.0068221339 0.006981621 0.0081675984 0.0099422932 0.011922717][0.019749783 0.016668225 0.013993807 0.012660049 0.012086749 0.012272436 0.011883808 0.011360848 0.009945523 0.0086270925 0.0069749975 0.006728821 0.0076786629 0.0095214862 0.011680439][0.021408033 0.018070521 0.015230259 0.013995366 0.013553498 0.013608942 0.012941124 0.012096606 0.010343134 0.0086813457 0.0067337039 0.0062500476 0.0069750971 0.008632753 0.010698659][0.022387125 0.019333728 0.01670431 0.015573332 0.015222738 0.015168825 0.01447041 0.013418248 0.01145828 0.0095050577 0.0072568073 0.0063599558 0.0066453065 0.0080250911 0.0099082328][0.022661444 0.021278828 0.019823223 0.019299673 0.019277183 0.019234022 0.018432518 0.017220197 0.015316268 0.013396554 0.011184217 0.010097872 0.010168163 0.011286411 0.012968171][0.021817312 0.022439938 0.022850355 0.023538465 0.024315903 0.024287794 0.02327152 0.021556353 0.019235486 0.017299829 0.015319059 0.014502261 0.014918858 0.016453829 0.018471321][0.02534041 0.02701127 0.028575124 0.030077385 0.031261042 0.030586122 0.028948041 0.02643547 0.02339391 0.02117206 0.019304274 0.018841214 0.019649522 0.02180508 0.024503108][0.032261733 0.035261821 0.037611477 0.039691426 0.041028645 0.03921435 0.036225546 0.032272182 0.028586341 0.025629995 0.023268407 0.022844153 0.023709642 0.026331436 0.029697539][0.039845061 0.04446784 0.047111843 0.049611576 0.051373698 0.049198825 0.045847729 0.040812522 0.035920735 0.031400915 0.027593503 0.02644206 0.02637578 0.028701777 0.031474598][0.044804681 0.050057631 0.053117644 0.056139234 0.058447447 0.057116065 0.054437779 0.049580161 0.044244464 0.038696963 0.033373415 0.030658705 0.028823966 0.029921146 0.031485911][0.043179683 0.047924951 0.051250562 0.054928318 0.058166143 0.057936043 0.055581905 0.051122364 0.045713961 0.039972402 0.034738913 0.032263637 0.030339347 0.030914532 0.031601734][0.039857298 0.04342423 0.045856524 0.048624542 0.05130494 0.05174524 0.050342105 0.047395259 0.043076303 0.038374186 0.033790052 0.031708188 0.029958421 0.030306706 0.030523909][0.035454266 0.037921656 0.039411776 0.041543797 0.043880336 0.045144167 0.045077749 0.044064593 0.041179277 0.0375453 0.033755194 0.031702012 0.029777454 0.02939357 0.029483216]]...]
INFO - root - 2017-12-10 07:30:24.894881: step 910, loss = 0.76, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:56s remains)
INFO - root - 2017-12-10 07:30:27.094020: step 920, loss = 0.75, batch loss = 0.68 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:28s remains)
INFO - root - 2017-12-10 07:30:29.317375: step 930, loss = 0.76, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:40s remains)
INFO - root - 2017-12-10 07:30:31.507635: step 940, loss = 0.75, batch loss = 0.68 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:53s remains)
INFO - root - 2017-12-10 07:30:33.708531: step 950, loss = 0.75, batch loss = 0.68 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:27s remains)
INFO - root - 2017-12-10 07:30:35.887342: step 960, loss = 0.76, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:50s remains)
INFO - root - 2017-12-10 07:30:38.117590: step 970, loss = 0.77, batch loss = 0.70 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:45s remains)
INFO - root - 2017-12-10 07:30:40.341430: step 980, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:18s remains)
INFO - root - 2017-12-10 07:30:42.535998: step 990, loss = 0.76, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:07s remains)
INFO - root - 2017-12-10 07:30:44.737578: step 1000, loss = 0.76, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:44s remains)
2017-12-10 07:30:45.086792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0036136881 -0.0036128722 -0.0036125844 -0.0036125728 -0.0036125933 -0.0036125905 -0.00361259 -0.0036125546 -0.0036125123 -0.0036124855 -0.0036124871 -0.0036125204 -0.0036125393 -0.003612604 -0.0036126808][-0.0036135837 -0.003612777 -0.0036125903 -0.0036126976 -0.003612783 -0.0036127942 -0.0036127064 -0.0036125313 -0.0036123365 -0.0036121449 -0.0036119991 -0.0036119244 -0.0036118957 -0.0036119136 -0.0036119837][-0.0036137139 -0.0036130468 -0.0036130543 -0.0036133244 -0.003613519 -0.0036135779 -0.0036134792 -0.0036132205 -0.0036128664 -0.0036124869 -0.0036121625 -0.0036119251 -0.0036117672 -0.003611682 -0.0036116841][-0.0036140184 -0.0036135281 -0.0036137628 -0.0036142294 -0.0036145912 -0.0036147654 -0.0036147053 -0.0036144084 -0.0036139209 -0.0036133435 -0.0036127889 -0.0036123344 -0.0036119877 -0.0036117348 -0.0036116161][-0.0036143893 -0.0036140108 -0.0036144673 -0.0036151377 -0.0036156874 -0.00361602 -0.0036160764 -0.0036158247 -0.0036152839 -0.0036145528 -0.0036137633 -0.0036130536 -0.0036124641 -0.003612001 -0.0036117239][-0.0036146911 -0.0036144624 -0.0036150515 -0.0036158978 -0.0036166473 -0.0036171782 -0.0036174085 -0.0036172657 -0.0036167377 -0.0036158913 -0.0036148729 -0.0036138832 -0.0036130198 -0.0036123272 -0.0036118811][-0.0036147565 -0.0036146292 -0.0036153232 -0.00361631 -0.0036172532 -0.0036179975 -0.0036184283 -0.0036184357 -0.0036179619 -0.0036170422 -0.0036158399 -0.0036146005 -0.0036134876 -0.003612597 -0.0036120163][-0.0036146387 -0.0036144487 -0.0036151025 -0.0036161542 -0.0036172415 -0.0036181747 -0.0036188043 -0.0036189833 -0.0036186012 -0.0036176769 -0.0036163847 -0.0036149973 -0.0036137321 -0.0036127288 -0.0036120818][-0.0036144343 -0.0036140771 -0.0036145325 -0.0036155283 -0.0036166555 -0.0036177018 -0.0036184827 -0.0036188113 -0.0036185472 -0.0036176941 -0.0036164229 -0.0036150236 -0.0036137351 -0.0036127241 -0.0036120873][-0.0036141148 -0.0036136003 -0.0036138822 -0.0036147365 -0.0036157845 -0.0036168157 -0.0036176345 -0.0036180383 -0.0036178743 -0.0036171346 -0.0036159917 -0.0036147169 -0.0036135339 -0.0036126184 -0.0036120559][-0.0036138792 -0.0036132112 -0.0036133125 -0.0036139765 -0.00361486 -0.0036157619 -0.0036164955 -0.0036168753 -0.0036167654 -0.0036161619 -0.0036152205 -0.0036141672 -0.0036131963 -0.0036124594 -0.0036120217][-0.0036137823 -0.0036129321 -0.0036128492 -0.0036133102 -0.0036139581 -0.003614635 -0.0036151914 -0.0036154885 -0.0036154143 -0.0036149703 -0.0036142801 -0.0036135118 -0.0036128145 -0.0036122967 -0.0036120086][-0.0036138615 -0.003612885 -0.0036126438 -0.0036128776 -0.0036132613 -0.00361368 -0.0036140373 -0.0036142427 -0.0036142133 -0.0036139388 -0.003613499 -0.0036130056 -0.0036125584 -0.0036122312 -0.0036120617][-0.0036140278 -0.0036130273 -0.0036126378 -0.003612722 -0.0036129076 -0.0036131225 -0.0036133185 -0.0036134431 -0.0036134387 -0.0036132892 -0.0036130352 -0.0036127432 -0.003612475 -0.0036122797 -0.0036121837][-0.003614139 -0.0036131267 -0.0036127146 -0.0036127036 -0.0036127733 -0.003612862 -0.0036129495 -0.0036130105 -0.0036130128 -0.0036129402 -0.0036128089 -0.003612651 -0.0036125027 -0.0036123914 -0.0036123348]]...]
INFO - root - 2017-12-10 07:30:47.285472: step 1010, loss = 0.76, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:25s remains)
INFO - root - 2017-12-10 07:30:49.481119: step 1020, loss = 0.76, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:14s remains)
INFO - root - 2017-12-10 07:30:51.711741: step 1030, loss = 0.76, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:14s remains)
INFO - root - 2017-12-10 07:30:53.919157: step 1040, loss = 0.76, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:21s remains)
INFO - root - 2017-12-10 07:30:56.120564: step 1050, loss = 0.76, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-10 07:30:58.328088: step 1060, loss = 0.76, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:35s remains)
INFO - root - 2017-12-10 07:31:00.528796: step 1070, loss = 0.76, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:29s remains)
INFO - root - 2017-12-10 07:31:02.724114: step 1080, loss = 0.76, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:52s remains)
INFO - root - 2017-12-10 07:31:04.952565: step 1090, loss = 0.76, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 21h:06m:54s remains)
INFO - root - 2017-12-10 07:31:07.129318: step 1100, loss = 0.76, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:38s remains)
2017-12-10 07:31:07.472839: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0029718901 0.0095565328 0.015949164 0.020211754 0.020894978 0.018146893 0.01374482 0.0094589414 0.0058462378 0.0026700266 4.4346787e-05 -0.001681996 -0.0026479843 -0.0030885381 -0.0028126473][0.00041761878 0.0060810437 0.012795942 0.018331105 0.020636471 0.019158445 0.014810629 0.009673845 0.005377301 0.0019556938 -0.00036364421 -0.0019778581 -0.0029793922 -0.0035078391 -0.0034973905][-0.0011060191 0.0032505195 0.0090241032 0.014717679 0.018314574 0.01855813 0.015070161 0.0096455924 0.0046436768 0.00084993965 -0.0011290649 -0.0022325194 -0.0028457493 -0.0032599028 -0.0035521081][-0.0019524591 0.0012752481 0.0059758369 0.011177036 0.015499068 0.017131474 0.014835604 0.0099733947 0.0049804039 0.00090945349 -0.001015007 -0.0019333627 -0.0023181769 -0.002693519 -0.0031543095][-0.0023870759 -0.00012489525 0.0035873998 0.00850582 0.013654822 0.016621059 0.015525529 0.011520578 0.0068279505 0.0024603817 0.00021103513 -0.00094812084 -0.001377299 -0.0018105414 -0.0025195819][-0.0030110367 -0.0015001609 0.0014784343 0.0060567772 0.011601809 0.015528573 0.015582899 0.012600873 0.0082634548 0.0037755643 0.00094488985 -0.00048826053 -0.00084484671 -0.001117293 -0.0018773819][-0.0035163451 -0.0026884573 -0.00023372681 0.0040003341 0.009358895 0.013544388 0.014473051 0.012577431 0.0091656782 0.0050351853 0.0018774795 1.6569393e-05 -0.00056976383 -0.00081707607 -0.0015415961][-0.0036305846 -0.003147332 -0.0014100743 0.0015825818 0.0055516753 0.0090356283 0.010358593 0.0095130736 0.0072005922 0.0039570872 0.001092708 -0.00063897623 -0.00128816 -0.0015516032 -0.0020839241][-0.0036500203 -0.0033743454 -0.0021709907 -0.00060690893 0.0011998352 0.0030059123 0.0039850026 0.0039057822 0.0029221766 0.001211039 -0.00045722676 -0.0015720439 -0.0022542817 -0.0025726915 -0.0029377311][-0.0037047025 -0.0035169472 -0.0024338139 -0.0015121598 -0.0011241634 -0.00076038972 -0.00074121635 -0.0012181359 -0.0017971722 -0.0022644964 -0.0026047742 -0.0026240451 -0.0028666356 -0.0031198317 -0.0034235651][-0.0036553151 -0.00348361 -0.0025214516 -0.0017050898 -0.001700297 -0.0016594336 -0.0019415475 -0.0024867989 -0.0029747142 -0.0032072111 -0.0032535968 -0.0029607196 -0.0029940763 -0.0030546994 -0.0032930458][-0.0036086969 -0.0035806566 -0.0030266845 -0.0023906901 -0.0023419997 -0.0019953777 -0.0020412058 -0.0024798764 -0.0029619727 -0.003124964 -0.0031470966 -0.0031152528 -0.0032406375 -0.0032474408 -0.0034011109][-0.0036643948 -0.0036629718 -0.0036984063 -0.0034821527 -0.0034144544 -0.0028443076 -0.0025302127 -0.0026829347 -0.0030621118 -0.0031101177 -0.0032480131 -0.0032066971 -0.0033239208 -0.0032727008 -0.0033914016][-0.003764119 -0.0037444949 -0.003742751 -0.0037568994 -0.0037766814 -0.0034028469 -0.0031750945 -0.0031867502 -0.0033597862 -0.0032355543 -0.0032684929 -0.0031074043 -0.0032115141 -0.0032149148 -0.0033854388][-0.0037151752 -0.00366542 -0.0036503039 -0.0036776555 -0.0037175394 -0.0037571562 -0.0037851536 -0.0037986552 -0.0038032234 -0.0038053647 -0.0037689619 -0.0035036679 -0.003451484 -0.0032263976 -0.003291334]]...]
INFO - root - 2017-12-10 07:31:09.682309: step 1110, loss = 0.76, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-10 07:31:11.873990: step 1120, loss = 0.77, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:58s remains)
INFO - root - 2017-12-10 07:31:14.101707: step 1130, loss = 0.76, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:15s remains)
INFO - root - 2017-12-10 07:31:16.280251: step 1140, loss = 0.76, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:59s remains)
INFO - root - 2017-12-10 07:31:18.509480: step 1150, loss = 0.76, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:48s remains)
INFO - root - 2017-12-10 07:31:20.738366: step 1160, loss = 0.76, batch loss = 0.69 (33.0 examples/sec; 0.242 sec/batch; 22h:17m:09s remains)
INFO - root - 2017-12-10 07:31:22.919321: step 1170, loss = 0.76, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:03s remains)
INFO - root - 2017-12-10 07:31:25.118162: step 1180, loss = 0.77, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:33m:49s remains)
INFO - root - 2017-12-10 07:31:27.344135: step 1190, loss = 0.76, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:08s remains)
INFO - root - 2017-12-10 07:31:29.524919: step 1200, loss = 0.76, batch loss = 0.68 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:21s remains)
2017-12-10 07:31:29.841665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025458217 -0.0018459281 -0.00083077792 -0.0004476537 -0.00016294234 -0.00021224516 -0.00049072411 -0.0011252877 -0.0020370111 -0.002925213 -0.0035808531 -0.0039200117 -0.0040589659 -0.004135306 -0.0041892235][-0.0021208681 -0.0016164377 -0.000533971 0.00026229303 0.00079771783 0.0004919176 -0.00010951841 -0.00066738436 -0.0016471951 -0.0026375661 -0.0032572672 -0.0034533944 -0.0033988408 -0.0033627863 -0.0034988641][-5.4195989e-05 0.00018208381 0.000879867 0.001249955 0.0012804344 0.0010013189 0.00036969455 -0.00044420897 -0.0017402938 -0.0027076567 -0.003257018 -0.0032802098 -0.0029353569 -0.002567356 -0.0025455384][0.0038933596 0.0038470151 0.0043948106 0.0047698421 0.0045698322 0.0037774118 0.00251449 0.0017749011 0.00041324925 -0.00087155472 -0.0014715651 -0.0014308211 -0.0011001504 -0.0011271958 -0.0013676947][0.0091815516 0.0096258447 0.010138434 0.010295909 0.0099699907 0.0094679855 0.0085183764 0.0077953935 0.0053802589 0.0036333362 0.0025561065 0.0021440047 0.0018869755 0.0011481005 -1.6187318e-05][0.014683753 0.016809946 0.017885366 0.018430036 0.018121418 0.017478956 0.016278353 0.015244933 0.012673626 0.0098461043 0.007381754 0.0056908065 0.00420586 0.0027429066 0.0011737547][0.017795412 0.022552809 0.02558421 0.02819355 0.029165464 0.028318191 0.026497671 0.023874614 0.019805145 0.015161596 0.010711584 0.0073306132 0.0051020226 0.0037791487 0.0028476506][0.020181088 0.026677351 0.03121152 0.03524626 0.03756167 0.037526809 0.035488918 0.031741105 0.026453074 0.020033294 0.013950327 0.0091068959 0.0065099411 0.0062570954 0.0069388822][0.017230842 0.025226995 0.031450637 0.03702087 0.040520057 0.041626744 0.040044658 0.035947241 0.030351607 0.023604507 0.017672665 0.012449438 0.0097544855 0.00950544 0.01072479][0.012612268 0.020139528 0.02636447 0.032377914 0.0370944 0.039643586 0.038711868 0.034970313 0.029789513 0.023633791 0.018954996 0.015427329 0.014214396 0.013962811 0.0149746][0.0081454925 0.014356624 0.019581959 0.024861086 0.029452307 0.032657824 0.032992132 0.030562093 0.026162611 0.020841906 0.017054198 0.015116006 0.015620528 0.016693171 0.018292272][0.0051175356 0.010334856 0.014057619 0.017648578 0.020886848 0.023469003 0.024266537 0.023229623 0.020497793 0.016698344 0.013723101 0.012730222 0.014089895 0.015958518 0.017737037][0.0035516308 0.0081944531 0.010597346 0.012702288 0.014627257 0.015960947 0.016218031 0.015329506 0.013489837 0.011056727 0.0091236383 0.0086405165 0.0097060874 0.011225894 0.012444511][0.0018978412 0.0058834339 0.0076697869 0.0092852516 0.010461771 0.010835447 0.010251994 0.0091126356 0.0077036098 0.0059128245 0.0045296149 0.0044366093 0.0055343537 0.0068681706 0.0076028695][-0.0009750363 0.0015785224 0.0028647971 0.0043759253 0.0059177475 0.007097696 0.0069428906 0.0055770054 0.0040053884 0.0025342447 0.0014294339 0.0012340066 0.002030924 0.003301878 0.0041525615]]...]
INFO - root - 2017-12-10 07:31:32.038140: step 1210, loss = 0.76, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:27m:21s remains)
INFO - root - 2017-12-10 07:31:34.242005: step 1220, loss = 0.76, batch loss = 0.69 (34.1 examples/sec; 0.234 sec/batch; 21h:33m:55s remains)
INFO - root - 2017-12-10 07:31:36.430512: step 1230, loss = 0.77, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:38s remains)
INFO - root - 2017-12-10 07:31:38.639065: step 1240, loss = 0.76, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:09s remains)
INFO - root - 2017-12-10 07:31:40.866812: step 1250, loss = 0.76, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:30s remains)
INFO - root - 2017-12-10 07:31:43.073661: step 1260, loss = 0.76, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:08s remains)
INFO - root - 2017-12-10 07:31:45.260171: step 1270, loss = 0.76, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:27s remains)
INFO - root - 2017-12-10 07:31:47.453260: step 1280, loss = 0.76, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:45s remains)
