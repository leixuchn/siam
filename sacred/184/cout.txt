INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "184"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-lastlr0.5-clip50
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
2017-12-10 04:36:02.148384: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:36:02.148426: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:36:02.148433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:36:02.148437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:36:02.148441: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 04:36:02.626259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-10 04:36:02.626296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-10 04:36:02.626303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-10 04:36:02.626310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-10 04:36:05.824128: step 0, loss = 2.28, batch loss = 2.23 (3.6 examples/sec; 2.247 sec/batch; 207h:34m:51s remains)
2017-12-10 04:36:06.225682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.429028 -4.4290185 -4.4290133 -4.429 -4.4289794 -4.4289541 -4.4289236 -4.4288859 -4.4288592 -4.4288554 -4.4288731 -4.4289045 -4.42894 -4.4289794 -4.4290104][-4.4290237 -4.4290113 -4.4289961 -4.4289694 -4.4289312 -4.4288878 -4.4288359 -4.4287786 -4.4287453 -4.4287524 -4.4287825 -4.4288335 -4.4288893 -4.4289451 -4.42899][-4.4290161 -4.4290018 -4.4289794 -4.4289374 -4.4288764 -4.4288054 -4.4287229 -4.4286346 -4.4285822 -4.4285975 -4.4286504 -4.4287357 -4.4288239 -4.4289012 -4.4289622][-4.4289989 -4.4289894 -4.4289689 -4.4289155 -4.4288273 -4.4287224 -4.4286017 -4.4284658 -4.428381 -4.4284024 -4.4284887 -4.4286242 -4.4287558 -4.4288597 -4.4289365][-4.4289727 -4.4289713 -4.4289579 -4.4288912 -4.4287715 -4.4286222 -4.4284477 -4.4282436 -4.4281206 -4.4281707 -4.4283171 -4.4285212 -4.4287028 -4.4288344 -4.4289241][-4.4289403 -4.4289422 -4.4289308 -4.4288473 -4.4286976 -4.4285011 -4.4282584 -4.4279685 -4.4278221 -4.427948 -4.4281893 -4.4284563 -4.4286742 -4.4288254 -4.4289217][-4.4289069 -4.4289036 -4.4288855 -4.4287891 -4.4286237 -4.4283948 -4.4280896 -4.4277368 -4.4276223 -4.4278631 -4.4281821 -4.42847 -4.4286861 -4.4288344 -4.4289274][-4.428874 -4.428853 -4.4288192 -4.4287176 -4.4285655 -4.42835 -4.4280567 -4.4277511 -4.4277377 -4.4280257 -4.428328 -4.4285717 -4.4287419 -4.428864 -4.4289436][-4.4288473 -4.4288058 -4.4287648 -4.4286761 -4.4285607 -4.4284015 -4.4281921 -4.4280133 -4.4280791 -4.4283133 -4.4285264 -4.4286914 -4.4288082 -4.4289 -4.4289622][-4.4288259 -4.42877 -4.4287291 -4.4286609 -4.4285817 -4.4284854 -4.4283714 -4.4282985 -4.4283867 -4.4285412 -4.4286695 -4.4287705 -4.4288459 -4.4289184 -4.4289722][-4.4288092 -4.428751 -4.4287128 -4.4286666 -4.4286156 -4.428566 -4.4285183 -4.4285054 -4.4285851 -4.428679 -4.428751 -4.4288144 -4.42887 -4.4289346 -4.4289818][-4.4288068 -4.4287605 -4.428731 -4.4287062 -4.4286819 -4.4286671 -4.428659 -4.4286709 -4.4287267 -4.4287705 -4.4288015 -4.4288478 -4.428896 -4.428956 -4.4289942][-4.4288254 -4.4287972 -4.4287815 -4.4287772 -4.4287715 -4.4287724 -4.4287739 -4.4287758 -4.4287968 -4.428803 -4.4288139 -4.4288564 -4.4289074 -4.4289665 -4.4289989][-4.4288478 -4.4288383 -4.4288383 -4.4288464 -4.428844 -4.4288368 -4.4288216 -4.4287992 -4.428793 -4.4287829 -4.42879 -4.4288383 -4.4288988 -4.4289608 -4.4289927][-4.4288497 -4.4288483 -4.4288573 -4.4288731 -4.4288735 -4.4288516 -4.428812 -4.4287629 -4.4287443 -4.4287343 -4.4287486 -4.4288087 -4.42888 -4.428946 -4.4289827]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-lastlr0.5-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-lastlr0.5-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 04:36:09.099371: step 10, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:30m:25s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-10 04:36:11.584720: step 20, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 22h:07m:17s remains)
INFO - root - 2017-12-10 04:36:14.039693: step 30, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:58m:32s remains)
INFO - root - 2017-12-10 04:36:16.480712: step 40, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:10m:11s remains)
INFO - root - 2017-12-10 04:36:18.972320: step 50, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.250 sec/batch; 23h:07m:42s remains)
INFO - root - 2017-12-10 04:36:21.446318: step 60, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:32m:36s remains)
INFO - root - 2017-12-10 04:36:23.911993: step 70, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:29m:45s remains)
INFO - root - 2017-12-10 04:36:26.377754: step 80, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:56m:36s remains)
INFO - root - 2017-12-10 04:36:28.852083: step 90, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:08m:26s remains)
INFO - root - 2017-12-10 04:36:31.295336: step 100, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:44m:57s remains)
2017-12-10 04:36:31.574948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42884 -4.4287949 -4.428751 -4.42873 -4.4287405 -4.42877 -4.4288039 -4.4288583 -4.4289069 -4.428916 -4.4289002 -4.428884 -4.4288521 -4.4288063 -4.4287729][-4.4288445 -4.4287848 -4.4287181 -4.4286757 -4.4286819 -4.4287181 -4.4287572 -4.4288125 -4.4288588 -4.4288712 -4.4288692 -4.4288659 -4.428844 -4.428802 -4.4287729][-4.4288464 -4.4287748 -4.4286923 -4.4286342 -4.4286337 -4.428669 -4.4287086 -4.4287567 -4.4287977 -4.4288139 -4.4288225 -4.4288297 -4.4288206 -4.4287896 -4.4287705][-4.4288435 -4.4287658 -4.4286804 -4.4286194 -4.4286132 -4.4286427 -4.428678 -4.4287195 -4.4287615 -4.4287858 -4.4287992 -4.42881 -4.4288092 -4.4287839 -4.4287658][-4.4288387 -4.4287596 -4.4286771 -4.4286194 -4.4286065 -4.4286256 -4.4286551 -4.4286923 -4.4287405 -4.4287753 -4.4287939 -4.4288025 -4.428802 -4.4287791 -4.4287577][-4.4288306 -4.4287519 -4.4286757 -4.4286208 -4.4286 -4.428607 -4.4286246 -4.4286523 -4.4287009 -4.4287462 -4.4287715 -4.4287815 -4.4287829 -4.4287686 -4.428751][-4.4288254 -4.4287505 -4.428678 -4.4286184 -4.4285827 -4.4285755 -4.4285784 -4.4285879 -4.4286256 -4.4286814 -4.4287195 -4.4287343 -4.4287424 -4.4287424 -4.4287376][-4.4288182 -4.4287448 -4.4286709 -4.4286051 -4.4285564 -4.4285398 -4.4285312 -4.4285207 -4.4285426 -4.4286079 -4.42866 -4.4286866 -4.4287057 -4.4287205 -4.4287281][-4.4288163 -4.4287438 -4.4286704 -4.4286041 -4.4285579 -4.4285431 -4.4285307 -4.4285111 -4.4285207 -4.428587 -4.4286394 -4.4286647 -4.4286861 -4.4287071 -4.4287205][-4.4288173 -4.4287462 -4.4286747 -4.4286122 -4.4285746 -4.4285684 -4.4285631 -4.4285479 -4.4285603 -4.4286175 -4.4286561 -4.4286666 -4.428678 -4.4286957 -4.4287076][-4.4288216 -4.4287515 -4.42868 -4.4286203 -4.42859 -4.4285865 -4.4285879 -4.4285831 -4.4286046 -4.4286556 -4.4286795 -4.4286757 -4.4286761 -4.4286861 -4.4286947][-4.4288349 -4.4287672 -4.4286981 -4.4286437 -4.4286194 -4.4286141 -4.42862 -4.4286237 -4.4286494 -4.4286909 -4.4287076 -4.4286985 -4.4286962 -4.4287004 -4.4287062][-4.4288583 -4.4287949 -4.4287338 -4.428689 -4.42867 -4.428668 -4.428678 -4.42869 -4.4287138 -4.428741 -4.4287505 -4.4287429 -4.4287419 -4.4287448 -4.4287457][-4.4288898 -4.4288359 -4.4287853 -4.4287477 -4.4287314 -4.4287357 -4.4287543 -4.4287753 -4.4287968 -4.428812 -4.4288125 -4.4288039 -4.4287996 -4.4287992 -4.4287972][-4.4289322 -4.428894 -4.4288578 -4.4288282 -4.4288139 -4.4288211 -4.428843 -4.4288664 -4.4288831 -4.4288893 -4.4288836 -4.4288726 -4.4288645 -4.4288616 -4.4288616]]...]
INFO - root - 2017-12-10 04:36:34.069585: step 110, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:27m:58s remains)
INFO - root - 2017-12-10 04:36:36.557458: step 120, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:21m:09s remains)
INFO - root - 2017-12-10 04:36:39.007279: step 130, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:20m:12s remains)
INFO - root - 2017-12-10 04:36:41.456226: step 140, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:39m:49s remains)
INFO - root - 2017-12-10 04:36:43.920867: step 150, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:38m:05s remains)
INFO - root - 2017-12-10 04:36:46.373319: step 160, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:03m:01s remains)
INFO - root - 2017-12-10 04:36:48.858375: step 170, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:29m:09s remains)
INFO - root - 2017-12-10 04:36:51.308880: step 180, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.250 sec/batch; 23h:07m:04s remains)
INFO - root - 2017-12-10 04:36:53.803513: step 190, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:42m:26s remains)
INFO - root - 2017-12-10 04:36:56.265437: step 200, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:11m:05s remains)
2017-12-10 04:36:56.576246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288116 -4.4287891 -4.4287829 -4.4287896 -4.4287729 -4.4287624 -4.4287634 -4.4287639 -4.4287624 -4.4287667 -4.4287734 -4.4287872 -4.4287977 -4.4288082 -4.4288106][-4.4288092 -4.4287724 -4.4287591 -4.4287515 -4.4287357 -4.42873 -4.4287343 -4.4287276 -4.4287124 -4.428719 -4.4287372 -4.4287586 -4.4287672 -4.4287705 -4.4287739][-4.4287982 -4.4287605 -4.4287519 -4.4287381 -4.4287238 -4.4287205 -4.4287243 -4.4287181 -4.4287047 -4.4287157 -4.4287386 -4.428762 -4.4287686 -4.4287586 -4.4287539][-4.4287705 -4.4287386 -4.4287424 -4.4287381 -4.4287257 -4.4287128 -4.4287047 -4.4287052 -4.4287133 -4.4287381 -4.4287596 -4.4287739 -4.4287682 -4.42874 -4.4287229][-4.4287691 -4.4287324 -4.4287319 -4.42873 -4.4287176 -4.4286885 -4.4286566 -4.4286561 -4.4286971 -4.42875 -4.428781 -4.4287872 -4.428762 -4.4287276 -4.428709][-4.4287472 -4.4287033 -4.4287066 -4.4287162 -4.4286981 -4.4286418 -4.4285626 -4.4285445 -4.4286366 -4.4287324 -4.4287791 -4.428782 -4.428751 -4.42872 -4.4287133][-4.4286933 -4.4286447 -4.4286566 -4.4286809 -4.4286523 -4.4285531 -4.4283943 -4.4283295 -4.4285007 -4.4286666 -4.4287415 -4.428741 -4.4287028 -4.4286757 -4.4286904][-4.428648 -4.4285851 -4.4285917 -4.4286318 -4.4286022 -4.4284558 -4.428215 -4.4280853 -4.4283376 -4.4285851 -4.4286942 -4.42869 -4.42864 -4.4286189 -4.4286523][-4.4286337 -4.4285803 -4.4285927 -4.42864 -4.4286203 -4.4285026 -4.4283118 -4.4281979 -4.4283957 -4.4286218 -4.4287333 -4.4287419 -4.4286919 -4.4286594 -4.4286733][-4.4286838 -4.4286423 -4.4286466 -4.4286876 -4.4286785 -4.4286146 -4.4285135 -4.4284477 -4.4285526 -4.4287 -4.4287863 -4.4288025 -4.4287682 -4.4287362 -4.4287286][-4.4287505 -4.4287086 -4.4287014 -4.4287314 -4.4287295 -4.4286985 -4.4286456 -4.4286065 -4.4286575 -4.4287448 -4.428803 -4.4288197 -4.428791 -4.4287629 -4.4287524][-4.4287653 -4.4287338 -4.42874 -4.4287682 -4.4287734 -4.42876 -4.4287195 -4.428688 -4.4287109 -4.4287658 -4.4288054 -4.4288168 -4.4288 -4.4287777 -4.4287696][-4.4287705 -4.4287505 -4.4287667 -4.4287863 -4.4287925 -4.4287877 -4.4287577 -4.4287376 -4.4287457 -4.4287777 -4.4288096 -4.4288192 -4.4288182 -4.4288125 -4.4288125][-4.42878 -4.4287691 -4.4287887 -4.4288025 -4.4288068 -4.4288077 -4.4287906 -4.4287825 -4.4287858 -4.4287968 -4.4288154 -4.4288273 -4.4288387 -4.4288468 -4.4288521][-4.4288039 -4.4287996 -4.4288173 -4.4288311 -4.4288363 -4.4288416 -4.428834 -4.4288268 -4.4288268 -4.4288268 -4.4288421 -4.4288583 -4.4288707 -4.4288774 -4.4288769]]...]
INFO - root - 2017-12-10 04:36:59.043562: step 210, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:56m:53s remains)
INFO - root - 2017-12-10 04:37:01.533959: step 220, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:48s remains)
INFO - root - 2017-12-10 04:37:04.036156: step 230, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:17m:31s remains)
INFO - root - 2017-12-10 04:37:06.491471: step 240, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:59m:36s remains)
INFO - root - 2017-12-10 04:37:08.983082: step 250, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 23h:11m:36s remains)
INFO - root - 2017-12-10 04:37:11.468982: step 260, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:14m:44s remains)
INFO - root - 2017-12-10 04:37:13.943590: step 270, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:48m:16s remains)
INFO - root - 2017-12-10 04:37:16.513298: step 280, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:16m:25s remains)
INFO - root - 2017-12-10 04:37:19.068753: step 290, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 24h:16m:27s remains)
INFO - root - 2017-12-10 04:37:21.574682: step 300, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 23h:00m:21s remains)
2017-12-10 04:37:21.894655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285831 -4.4285645 -4.4285889 -4.4286504 -4.4287319 -4.4288011 -4.4288325 -4.4288316 -4.4287963 -4.4287329 -4.4286757 -4.4286604 -4.4286771 -4.4286995 -4.4287043][-4.4287033 -4.4286804 -4.42867 -4.4286823 -4.42873 -4.4287915 -4.4288321 -4.4288411 -4.4288177 -4.4287739 -4.428741 -4.4287467 -4.4287562 -4.4287539 -4.4287353][-4.4288044 -4.4287887 -4.4287524 -4.4287138 -4.4287124 -4.4287515 -4.428802 -4.4288349 -4.4288411 -4.428823 -4.4288106 -4.4288225 -4.4288211 -4.428792 -4.4287543][-4.428863 -4.4288511 -4.428793 -4.4287162 -4.4286709 -4.4286819 -4.4287343 -4.428791 -4.42883 -4.4288392 -4.4288387 -4.4288449 -4.4288297 -4.428782 -4.4287338][-4.4288826 -4.4288669 -4.4287958 -4.4286942 -4.4286165 -4.4286022 -4.4286432 -4.4287095 -4.4287682 -4.4287972 -4.4288025 -4.4287949 -4.4287624 -4.4287081 -4.4286709][-4.4288688 -4.42885 -4.4287829 -4.4286857 -4.4286027 -4.428575 -4.4285955 -4.4286585 -4.428721 -4.4287529 -4.4287572 -4.4287333 -4.4286861 -4.4286289 -4.4286008][-4.4288526 -4.428834 -4.4287748 -4.4286985 -4.4286342 -4.4286075 -4.42861 -4.4286609 -4.4287152 -4.4287314 -4.4287171 -4.4286785 -4.4286242 -4.4285612 -4.4285388][-4.4288335 -4.4288106 -4.4287653 -4.4287109 -4.4286666 -4.4286427 -4.4286318 -4.4286671 -4.4287114 -4.4287162 -4.4286842 -4.428638 -4.42858 -4.4285035 -4.4284782][-4.4288154 -4.4287882 -4.4287572 -4.4287181 -4.4286904 -4.4286709 -4.428648 -4.428669 -4.4287028 -4.4287081 -4.42868 -4.4286418 -4.4285836 -4.4285049 -4.4284768][-4.4288106 -4.4287906 -4.4287791 -4.4287591 -4.4287424 -4.4287276 -4.4287019 -4.4287076 -4.4287224 -4.4287248 -4.4287052 -4.428679 -4.4286318 -4.4285641 -4.4285407][-4.428793 -4.4287868 -4.4287944 -4.428792 -4.4287863 -4.4287806 -4.4287567 -4.4287472 -4.4287491 -4.4287467 -4.4287314 -4.4287171 -4.42869 -4.4286413 -4.4286251][-4.4287586 -4.4287663 -4.4287939 -4.4288082 -4.4288125 -4.4288087 -4.4287853 -4.4287686 -4.4287639 -4.4287524 -4.4287324 -4.42873 -4.4287324 -4.4287157 -4.4287095][-4.4287109 -4.4287262 -4.4287658 -4.4287963 -4.4288158 -4.4288187 -4.4288044 -4.4287896 -4.4287796 -4.4287581 -4.428731 -4.4287305 -4.4287457 -4.4287443 -4.4287457][-4.4286733 -4.4286852 -4.4287162 -4.4287519 -4.4287844 -4.4288025 -4.4288087 -4.4288082 -4.4287968 -4.4287686 -4.4287367 -4.4287295 -4.4287481 -4.4287524 -4.4287562][-4.428689 -4.4286933 -4.4287114 -4.4287395 -4.4287734 -4.4288034 -4.4288297 -4.42884 -4.4288225 -4.4287858 -4.4287448 -4.42873 -4.4287462 -4.4287572 -4.4287667]]...]
INFO - root - 2017-12-10 04:37:24.393058: step 310, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 24h:02m:51s remains)
INFO - root - 2017-12-10 04:37:26.846551: step 320, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 22h:23m:19s remains)
INFO - root - 2017-12-10 04:37:29.333552: step 330, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:58m:19s remains)
INFO - root - 2017-12-10 04:37:31.788775: step 340, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:35m:07s remains)
INFO - root - 2017-12-10 04:37:34.275682: step 350, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:16m:55s remains)
INFO - root - 2017-12-10 04:37:36.724731: step 360, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:20m:20s remains)
INFO - root - 2017-12-10 04:37:39.218936: step 370, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:13m:58s remains)
INFO - root - 2017-12-10 04:37:41.682826: step 380, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:33m:16s remains)
INFO - root - 2017-12-10 04:37:44.172437: step 390, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:29m:48s remains)
INFO - root - 2017-12-10 04:37:46.617124: step 400, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:38s remains)
2017-12-10 04:37:46.903694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288421 -4.428822 -4.4288182 -4.4288335 -4.4288764 -4.4289021 -4.4289 -4.4288774 -4.4288616 -4.4288611 -4.4288554 -4.4288454 -4.4288397 -4.4288511 -4.4288497][-4.4288406 -4.4288197 -4.4288154 -4.4288368 -4.4288859 -4.4289141 -4.4289055 -4.428874 -4.4288511 -4.4288468 -4.4288311 -4.4288077 -4.4287925 -4.4287972 -4.4287963][-4.4288483 -4.42882 -4.4288087 -4.4288249 -4.4288707 -4.428895 -4.4288764 -4.4288383 -4.428812 -4.42881 -4.4287906 -4.42876 -4.428741 -4.4287434 -4.4287481][-4.4288349 -4.4287915 -4.4287643 -4.428772 -4.4288135 -4.428834 -4.4288135 -4.4287729 -4.4287457 -4.4287491 -4.4287372 -4.4287128 -4.4286962 -4.4287024 -4.4287186][-4.4288154 -4.4287472 -4.4286928 -4.4286847 -4.4287167 -4.4287362 -4.4287229 -4.4286938 -4.4286766 -4.4286885 -4.4286838 -4.4286714 -4.4286642 -4.4286809 -4.4287052][-4.428823 -4.4287462 -4.4286613 -4.4286165 -4.4286203 -4.4286294 -4.4286218 -4.42861 -4.4286122 -4.428638 -4.4286437 -4.4286408 -4.4286423 -4.4286704 -4.4287019][-4.4288445 -4.4287839 -4.4286923 -4.4286218 -4.4286027 -4.4285965 -4.4285727 -4.4285526 -4.4285574 -4.4285884 -4.4285994 -4.4286065 -4.4286213 -4.4286604 -4.4286995][-4.4288287 -4.4287834 -4.4287114 -4.4286585 -4.4286671 -4.4286842 -4.4286466 -4.4285936 -4.4285645 -4.4285669 -4.4285641 -4.4285703 -4.4285975 -4.4286489 -4.4286976][-4.4287386 -4.4286814 -4.4286232 -4.4286094 -4.4286766 -4.4287539 -4.4287434 -4.4286876 -4.4286323 -4.4285989 -4.4285693 -4.4285626 -4.4285913 -4.428647 -4.4287014][-4.4286346 -4.428566 -4.4285188 -4.42853 -4.4286351 -4.4287548 -4.4287834 -4.4287624 -4.4287186 -4.4286737 -4.4286308 -4.4286118 -4.4286323 -4.4286814 -4.4287314][-4.4286013 -4.4285288 -4.4284859 -4.4285054 -4.428618 -4.4287457 -4.4287896 -4.42879 -4.4287667 -4.4287376 -4.4287076 -4.4286904 -4.4287081 -4.4287443 -4.4287806][-4.4286509 -4.4285812 -4.4285364 -4.4285502 -4.4286528 -4.4287667 -4.4288111 -4.4288163 -4.428802 -4.4287863 -4.4287739 -4.4287705 -4.428791 -4.4288187 -4.42884][-4.4287596 -4.4287066 -4.4286666 -4.4286647 -4.42873 -4.4288082 -4.4288425 -4.4288516 -4.4288445 -4.4288363 -4.4288321 -4.4288373 -4.4288588 -4.4288807 -4.428894][-4.4288592 -4.4288287 -4.4288015 -4.4287906 -4.4288197 -4.42886 -4.42888 -4.4288874 -4.4288864 -4.4288831 -4.4288797 -4.4288836 -4.4288988 -4.4289165 -4.4289289][-4.4289212 -4.4289041 -4.4288855 -4.4288759 -4.4288883 -4.4289069 -4.4289155 -4.4289207 -4.4289227 -4.4289236 -4.4289207 -4.4289188 -4.428925 -4.428936 -4.4289474]]...]
INFO - root - 2017-12-10 04:37:49.376807: step 410, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:31m:00s remains)
INFO - root - 2017-12-10 04:37:51.836435: step 420, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:47m:15s remains)
INFO - root - 2017-12-10 04:37:54.350627: step 430, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:44m:27s remains)
INFO - root - 2017-12-10 04:37:56.841122: step 440, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:22m:51s remains)
INFO - root - 2017-12-10 04:37:59.318820: step 450, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:59m:03s remains)
INFO - root - 2017-12-10 04:38:01.771519: step 460, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:55m:18s remains)
INFO - root - 2017-12-10 04:38:04.286394: step 470, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 22h:07m:06s remains)
INFO - root - 2017-12-10 04:38:06.839960: step 480, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:30m:57s remains)
INFO - root - 2017-12-10 04:38:09.351331: step 490, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:09m:24s remains)
INFO - root - 2017-12-10 04:38:11.816020: step 500, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 22h:22m:19s remains)
2017-12-10 04:38:12.113178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287224 -4.4287829 -4.4288282 -4.4288769 -4.4289136 -4.4289093 -4.4288874 -4.4288621 -4.42883 -4.4288116 -4.4288235 -4.4288363 -4.4288435 -4.4288473 -4.4288187][-4.4286785 -4.4287672 -4.42884 -4.4289093 -4.4289522 -4.4289417 -4.4289002 -4.4288521 -4.4288077 -4.4287896 -4.4288068 -4.4288244 -4.4288335 -4.428843 -4.4288206][-4.428668 -4.4287782 -4.4288726 -4.4289441 -4.4289813 -4.4289632 -4.4288993 -4.4288263 -4.4287696 -4.4287524 -4.4287825 -4.4288158 -4.428833 -4.4288545 -4.42885][-4.4286952 -4.4288049 -4.4288955 -4.4289584 -4.4289851 -4.4289551 -4.428865 -4.4287686 -4.4287033 -4.4286981 -4.4287515 -4.4288092 -4.4288425 -4.4288812 -4.4288988][-4.4287548 -4.4288354 -4.4289012 -4.4289484 -4.4289618 -4.4289088 -4.428793 -4.4286742 -4.428617 -4.4286432 -4.4287295 -4.4288087 -4.4288583 -4.4289117 -4.4289384][-4.4288182 -4.4288616 -4.4288979 -4.4289184 -4.4289074 -4.4288163 -4.4286642 -4.4285336 -4.428525 -4.4286013 -4.42872 -4.4288058 -4.4288554 -4.42891 -4.4289374][-4.4288645 -4.4288816 -4.4288921 -4.4288807 -4.4288254 -4.42868 -4.428463 -4.4283328 -4.4284072 -4.4285512 -4.4287014 -4.4287915 -4.42884 -4.4288855 -4.4289036][-4.4288635 -4.4288683 -4.4288654 -4.4288225 -4.4287248 -4.4285259 -4.4282513 -4.4281516 -4.4283233 -4.4285259 -4.428688 -4.4287806 -4.4288311 -4.4288664 -4.4288697][-4.4288206 -4.4288254 -4.4288216 -4.428761 -4.4286451 -4.42844 -4.4281931 -4.4281645 -4.4283652 -4.428565 -4.4287047 -4.4287882 -4.4288349 -4.4288611 -4.4288545][-4.42877 -4.4287748 -4.4287758 -4.4287176 -4.42861 -4.4284577 -4.42831 -4.4283361 -4.4285026 -4.4286594 -4.4287639 -4.428823 -4.42885 -4.4288678 -4.4288564][-4.4287348 -4.4287338 -4.4287362 -4.4286852 -4.4286017 -4.4285154 -4.4284596 -4.4285116 -4.4286351 -4.4287448 -4.4288173 -4.4288554 -4.4288707 -4.4288836 -4.4288726][-4.4287057 -4.4287028 -4.4287086 -4.4286728 -4.4286265 -4.4285965 -4.4285893 -4.4286408 -4.4287252 -4.4287953 -4.4288435 -4.42887 -4.4288845 -4.4288974 -4.4288931][-4.428709 -4.4287047 -4.4287124 -4.4287009 -4.4286885 -4.4286909 -4.4286985 -4.4287348 -4.4287858 -4.4288306 -4.4288607 -4.4288812 -4.428896 -4.4289093 -4.4289131][-4.42876 -4.4287529 -4.4287634 -4.4287691 -4.4287763 -4.4287891 -4.4287982 -4.4288182 -4.42885 -4.4288793 -4.4288964 -4.4289064 -4.4289165 -4.4289279 -4.4289355][-4.4288406 -4.4288359 -4.4288464 -4.4288549 -4.4288607 -4.4288712 -4.4288754 -4.4288898 -4.4289117 -4.4289303 -4.4289403 -4.4289436 -4.4289465 -4.4289522 -4.4289594]]...]
INFO - root - 2017-12-10 04:38:14.577229: step 510, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:26m:31s remains)
INFO - root - 2017-12-10 04:38:17.022722: step 520, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:57m:03s remains)
INFO - root - 2017-12-10 04:38:19.539758: step 530, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:28m:44s remains)
INFO - root - 2017-12-10 04:38:22.054064: step 540, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 24h:02m:41s remains)
INFO - root - 2017-12-10 04:38:24.532630: step 550, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:24m:36s remains)
INFO - root - 2017-12-10 04:38:26.975572: step 560, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:30m:34s remains)
INFO - root - 2017-12-10 04:38:29.462161: step 570, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:31m:30s remains)
INFO - root - 2017-12-10 04:38:31.967624: step 580, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:02m:15s remains)
INFO - root - 2017-12-10 04:38:34.433690: step 590, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:31m:58s remains)
INFO - root - 2017-12-10 04:38:36.894486: step 600, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:33m:56s remains)
2017-12-10 04:38:37.175328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288573 -4.4288316 -4.4288106 -4.4287939 -4.4287944 -4.428791 -4.4288077 -4.4288621 -4.4289174 -4.4289384 -4.4289427 -4.4289246 -4.4288726 -4.4288354 -4.4288464][-4.4288311 -4.428812 -4.4288015 -4.428792 -4.4287877 -4.4287782 -4.4287891 -4.4288368 -4.4288917 -4.4289112 -4.428905 -4.4288869 -4.4288554 -4.4288468 -4.4288807][-4.428793 -4.4287715 -4.4287682 -4.4287705 -4.4287682 -4.4287543 -4.428762 -4.4288063 -4.4288545 -4.4288669 -4.4288549 -4.4288344 -4.428812 -4.4288177 -4.4288669][-4.4287381 -4.4287195 -4.4287243 -4.4287357 -4.428731 -4.4287019 -4.4287019 -4.4287372 -4.4287758 -4.4287915 -4.428793 -4.4287815 -4.428762 -4.4287653 -4.4288116][-4.4286833 -4.4286785 -4.4286947 -4.4287014 -4.4286723 -4.42862 -4.4286075 -4.4286375 -4.4286828 -4.4287205 -4.42874 -4.4287391 -4.4287114 -4.4286985 -4.4287267][-4.4286432 -4.4286571 -4.4286747 -4.4286571 -4.4285922 -4.4285111 -4.4284854 -4.4285197 -4.4285865 -4.428647 -4.4286771 -4.4286761 -4.42864 -4.4286041 -4.4286113][-4.4286337 -4.4286494 -4.4286561 -4.42861 -4.4285145 -4.4284077 -4.4283638 -4.4284029 -4.4284797 -4.4285421 -4.4285727 -4.4285684 -4.428524 -4.428484 -4.4284959][-4.4286475 -4.4286509 -4.428647 -4.4285908 -4.4284916 -4.4283867 -4.4283433 -4.4283791 -4.4284396 -4.4284739 -4.4284782 -4.4284582 -4.4284077 -4.4283867 -4.4284358][-4.4287028 -4.4286928 -4.4286804 -4.4286318 -4.4285536 -4.4284725 -4.4284339 -4.42845 -4.4284735 -4.4284735 -4.4284544 -4.4284196 -4.4283729 -4.4283757 -4.428453][-4.4287705 -4.4287577 -4.4287496 -4.4287205 -4.4286728 -4.4286227 -4.4285975 -4.4285984 -4.4285984 -4.4285784 -4.428545 -4.4284987 -4.4284554 -4.4284692 -4.428546][-4.4288225 -4.4288139 -4.4288092 -4.4287987 -4.42878 -4.4287577 -4.4287534 -4.4287529 -4.4287481 -4.4287291 -4.4286962 -4.4286475 -4.428607 -4.4286189 -4.4286785][-4.4288378 -4.428834 -4.4288278 -4.4288239 -4.4288249 -4.4288292 -4.4288459 -4.4288549 -4.4288535 -4.4288425 -4.4288211 -4.4287868 -4.4287567 -4.4287643 -4.428802][-4.4288349 -4.42883 -4.4288225 -4.4288149 -4.4288225 -4.4288449 -4.4288735 -4.4288874 -4.4288893 -4.428884 -4.428874 -4.4288521 -4.4288297 -4.4288359 -4.4288611][-4.4288287 -4.4288278 -4.4288206 -4.4288139 -4.4288268 -4.428853 -4.4288783 -4.4288864 -4.4288826 -4.428874 -4.4288626 -4.4288435 -4.4288216 -4.4288278 -4.42885][-4.4288425 -4.42885 -4.4288454 -4.428843 -4.4288559 -4.4288721 -4.4288812 -4.4288769 -4.4288626 -4.4288421 -4.4288225 -4.4288015 -4.4287815 -4.4287953 -4.4288225]]...]
INFO - root - 2017-12-10 04:38:39.644235: step 610, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:33m:19s remains)
INFO - root - 2017-12-10 04:38:42.096598: step 620, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:26m:46s remains)
INFO - root - 2017-12-10 04:38:44.597096: step 630, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:38m:55s remains)
INFO - root - 2017-12-10 04:38:47.075358: step 640, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:20m:44s remains)
INFO - root - 2017-12-10 04:38:49.583021: step 650, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:57m:26s remains)
INFO - root - 2017-12-10 04:38:52.044644: step 660, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:06m:22s remains)
INFO - root - 2017-12-10 04:38:54.563468: step 670, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:04m:11s remains)
INFO - root - 2017-12-10 04:38:57.072267: step 680, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:23m:16s remains)
INFO - root - 2017-12-10 04:38:59.544167: step 690, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:37m:41s remains)
INFO - root - 2017-12-10 04:39:02.043906: step 700, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:58m:26s remains)
2017-12-10 04:39:02.363293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287872 -4.4287729 -4.4287705 -4.4287724 -4.4287763 -4.4287777 -4.4287724 -4.4287643 -4.4287586 -4.4287558 -4.4287539 -4.4287515 -4.4287424 -4.4287238 -4.4287057][-4.4288206 -4.4288058 -4.428803 -4.4288044 -4.4288077 -4.4288049 -4.4287944 -4.4287786 -4.4287677 -4.4287605 -4.4287539 -4.4287496 -4.4287453 -4.4287324 -4.4287195][-4.4288559 -4.4288425 -4.428843 -4.4288487 -4.4288559 -4.4288535 -4.4288406 -4.4288211 -4.4288054 -4.4287939 -4.4287839 -4.4287777 -4.4287782 -4.4287729 -4.4287658][-4.4288511 -4.4288325 -4.4288306 -4.4288406 -4.4288559 -4.42886 -4.4288535 -4.4288363 -4.4288254 -4.4288211 -4.4288158 -4.4288111 -4.4288116 -4.4288068 -4.4288006][-4.428812 -4.42878 -4.4287677 -4.4287786 -4.428802 -4.4288139 -4.4288087 -4.4287944 -4.4287963 -4.4288111 -4.4288249 -4.428834 -4.4288397 -4.4288354 -4.4288254][-4.4287663 -4.4287195 -4.4286976 -4.4287071 -4.4287295 -4.4287348 -4.4287157 -4.4286938 -4.4287038 -4.4287415 -4.4287825 -4.4288163 -4.4288406 -4.4288468 -4.4288392][-4.4287558 -4.4287076 -4.42868 -4.4286838 -4.4286933 -4.4286795 -4.4286356 -4.4285955 -4.4286027 -4.4286528 -4.4287143 -4.4287724 -4.4288192 -4.42884 -4.4288416][-4.4287672 -4.4287333 -4.4287143 -4.4287181 -4.4287219 -4.4286976 -4.4286432 -4.42859 -4.4285817 -4.42862 -4.4286766 -4.4287362 -4.4287896 -4.4288182 -4.4288278][-4.42878 -4.4287643 -4.4287591 -4.4287667 -4.4287724 -4.4287562 -4.4287148 -4.4286723 -4.4286566 -4.4286728 -4.4287052 -4.4287448 -4.4287825 -4.428802 -4.42881][-4.4288082 -4.4287996 -4.4287977 -4.4288011 -4.4288015 -4.4287906 -4.4287653 -4.4287424 -4.4287353 -4.4287472 -4.4287677 -4.4287896 -4.4288068 -4.4288087 -4.4288054][-4.4288464 -4.4288321 -4.4288206 -4.4288116 -4.428803 -4.4287891 -4.42877 -4.4287605 -4.4287639 -4.4287796 -4.4287996 -4.4288158 -4.4288244 -4.4288144 -4.4288011][-4.4288669 -4.428844 -4.4288239 -4.42881 -4.4287996 -4.4287868 -4.428771 -4.4287639 -4.4287677 -4.428781 -4.4288006 -4.4288154 -4.4288211 -4.4288058 -4.4287882][-4.4288568 -4.4288325 -4.4288149 -4.4288106 -4.4288092 -4.4288044 -4.4287949 -4.4287872 -4.4287844 -4.4287877 -4.4287987 -4.4288087 -4.4288106 -4.4287934 -4.4287744][-4.4288373 -4.4288149 -4.4288015 -4.4288054 -4.4288139 -4.4288192 -4.4288197 -4.4288163 -4.4288092 -4.4288025 -4.4288039 -4.4288082 -4.4288063 -4.4287882 -4.4287677][-4.4288116 -4.428792 -4.4287806 -4.4287863 -4.4287977 -4.4288073 -4.428813 -4.4288139 -4.4288082 -4.4287996 -4.4287963 -4.4287987 -4.4287953 -4.4287796 -4.4287605]]...]
INFO - root - 2017-12-10 04:39:04.826814: step 710, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:36m:32s remains)
INFO - root - 2017-12-10 04:39:07.316980: step 720, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:07m:05s remains)
INFO - root - 2017-12-10 04:39:09.789938: step 730, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:08m:22s remains)
INFO - root - 2017-12-10 04:39:12.268705: step 740, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:18s remains)
INFO - root - 2017-12-10 04:39:14.738423: step 750, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:50m:04s remains)
INFO - root - 2017-12-10 04:39:17.282372: step 760, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:52m:48s remains)
INFO - root - 2017-12-10 04:39:19.793943: step 770, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:30m:52s remains)
INFO - root - 2017-12-10 04:39:22.294468: step 780, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:15m:52s remains)
INFO - root - 2017-12-10 04:39:24.784789: step 790, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:57m:26s remains)
INFO - root - 2017-12-10 04:39:27.231176: step 800, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:50m:32s remains)
2017-12-10 04:39:27.498649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287252 -4.4287052 -4.4286757 -4.4286485 -4.428658 -4.4287276 -4.428782 -4.4287877 -4.4287524 -4.4287033 -4.4286637 -4.4286489 -4.4286547 -4.4286718 -4.4286723][-4.4287934 -4.4287786 -4.428762 -4.4287453 -4.4287448 -4.4287686 -4.428772 -4.4287324 -4.4286675 -4.4286103 -4.4285727 -4.428575 -4.4285851 -4.4285984 -4.4285979][-4.4287968 -4.4287958 -4.4287934 -4.4287848 -4.4287705 -4.4287586 -4.4287333 -4.42868 -4.4286175 -4.4285684 -4.42854 -4.4285512 -4.4285603 -4.4285641 -4.4285655][-4.4287686 -4.4287834 -4.4287872 -4.4287815 -4.4287591 -4.4287295 -4.4286947 -4.4286528 -4.4286108 -4.4285755 -4.4285603 -4.4285765 -4.428575 -4.428565 -4.4285603][-4.4287448 -4.4287586 -4.4287543 -4.4287453 -4.428721 -4.4286838 -4.4286532 -4.4286222 -4.4285874 -4.4285617 -4.4285522 -4.4285669 -4.4285707 -4.4285626 -4.4285536][-4.4287229 -4.4287333 -4.4287214 -4.4287143 -4.428699 -4.42867 -4.4286394 -4.4286041 -4.4285645 -4.4285626 -4.428586 -4.4286151 -4.4286275 -4.4286256 -4.428616][-4.4287119 -4.4287257 -4.428721 -4.4287195 -4.4287176 -4.4286942 -4.4286408 -4.4285669 -4.4285107 -4.4285474 -4.4286294 -4.4286819 -4.428699 -4.4286904 -4.4286733][-4.4287086 -4.4287262 -4.4287295 -4.4287257 -4.4287171 -4.4286671 -4.4285493 -4.4283972 -4.4283175 -4.4284158 -4.4285707 -4.4286633 -4.4286933 -4.4286823 -4.4286666][-4.4286985 -4.4287238 -4.4287229 -4.4286928 -4.4286494 -4.4285555 -4.4283657 -4.4281516 -4.4280863 -4.42827 -4.4284868 -4.4286094 -4.4286475 -4.4286442 -4.4286408][-4.4287229 -4.4287467 -4.4287462 -4.4287071 -4.4286523 -4.4285622 -4.4284039 -4.4282484 -4.428236 -4.4284029 -4.4285684 -4.4286466 -4.4286642 -4.4286566 -4.428658][-4.4287724 -4.428793 -4.4287968 -4.4287705 -4.4287357 -4.4286876 -4.4285994 -4.4285221 -4.42853 -4.4286289 -4.4287105 -4.4287357 -4.4287319 -4.4287243 -4.4287295][-4.4288073 -4.4288177 -4.4288206 -4.4288073 -4.4287963 -4.4287882 -4.4287577 -4.4287233 -4.4287281 -4.4287753 -4.4288006 -4.4288015 -4.428793 -4.4287915 -4.4288068][-4.4287958 -4.4287906 -4.4287906 -4.428792 -4.4288096 -4.4288378 -4.4288449 -4.4288344 -4.4288359 -4.4288526 -4.4288521 -4.4288511 -4.4288454 -4.4288483 -4.4288716][-4.4287419 -4.42873 -4.42873 -4.4287438 -4.4287834 -4.4288387 -4.4288669 -4.4288721 -4.4288731 -4.4288793 -4.428874 -4.4288774 -4.4288783 -4.4288807 -4.428895][-4.4286633 -4.4286532 -4.4286566 -4.428688 -4.4287481 -4.4288225 -4.4288716 -4.4288859 -4.4288836 -4.428885 -4.428884 -4.4288917 -4.4288945 -4.4288907 -4.4288921]]...]
INFO - root - 2017-12-10 04:39:29.986990: step 810, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:22m:23s remains)
INFO - root - 2017-12-10 04:39:32.456690: step 820, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:45m:34s remains)
INFO - root - 2017-12-10 04:39:34.946899: step 830, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:17m:29s remains)
INFO - root - 2017-12-10 04:39:37.415829: step 840, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:42m:18s remains)
INFO - root - 2017-12-10 04:39:39.891400: step 850, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:02m:44s remains)
INFO - root - 2017-12-10 04:39:42.427235: step 860, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:52m:01s remains)
INFO - root - 2017-12-10 04:39:44.884265: step 870, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:45m:15s remains)
INFO - root - 2017-12-10 04:39:47.373706: step 880, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:19m:47s remains)
INFO - root - 2017-12-10 04:39:49.826269: step 890, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:06m:17s remains)
INFO - root - 2017-12-10 04:39:52.295497: step 900, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:55m:01s remains)
2017-12-10 04:39:52.607312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428967 -4.4289522 -4.4289412 -4.4289389 -4.4289436 -4.4289522 -4.4289618 -4.4289718 -4.4289789 -4.4289818 -4.4289837 -4.428987 -4.4289885 -4.4289875 -4.4289851][-4.4289484 -4.4289222 -4.4289012 -4.4288931 -4.4288993 -4.4289145 -4.4289331 -4.4289541 -4.4289684 -4.4289751 -4.428977 -4.4289827 -4.428988 -4.4289885 -4.4289851][-4.428926 -4.428885 -4.4288459 -4.4288244 -4.4288278 -4.4288497 -4.4288778 -4.42891 -4.4289384 -4.428956 -4.4289637 -4.4289737 -4.4289837 -4.4289861 -4.4289837][-4.4288964 -4.4288359 -4.4287734 -4.4287343 -4.4287314 -4.4287558 -4.428792 -4.4288354 -4.4288783 -4.4289122 -4.4289308 -4.4289517 -4.4289718 -4.4289789 -4.4289775][-4.4288692 -4.428792 -4.4287024 -4.4286351 -4.4286146 -4.4286337 -4.4286761 -4.4287329 -4.4287939 -4.428843 -4.4288721 -4.4289036 -4.428936 -4.4289556 -4.4289618][-4.4288516 -4.4287667 -4.4286575 -4.42856 -4.428504 -4.4284921 -4.4285274 -4.4285979 -4.428689 -4.428762 -4.4288025 -4.4288406 -4.4288816 -4.4289112 -4.4289312][-4.42884 -4.4287567 -4.428637 -4.4285116 -4.4284134 -4.4283457 -4.4283404 -4.4284105 -4.42854 -4.428658 -4.4287252 -4.4287758 -4.4288249 -4.4288616 -4.428895][-4.4288325 -4.4287596 -4.4286466 -4.4285126 -4.4283834 -4.4282565 -4.4281731 -4.4282079 -4.42837 -4.4285345 -4.4286346 -4.4287033 -4.4287643 -4.4288154 -4.428863][-4.4288349 -4.4287744 -4.4286823 -4.4285688 -4.4284396 -4.4282823 -4.4281263 -4.4280825 -4.4282279 -4.4284072 -4.4285359 -4.428627 -4.4287062 -4.4287763 -4.4288392][-4.4288507 -4.4287996 -4.4287338 -4.4286542 -4.4285517 -4.42842 -4.4282675 -4.4281664 -4.4282241 -4.428349 -4.4284663 -4.4285736 -4.4286718 -4.4287577 -4.42883][-4.4288774 -4.4288349 -4.4287915 -4.4287424 -4.4286709 -4.4285793 -4.428472 -4.4283843 -4.4283743 -4.4284182 -4.4284849 -4.4285793 -4.4286814 -4.4287705 -4.428844][-4.4289055 -4.4288692 -4.4288368 -4.4288054 -4.428762 -4.42871 -4.4286513 -4.4285984 -4.4285769 -4.4285812 -4.4286027 -4.428658 -4.4287381 -4.428812 -4.4288735][-4.4289303 -4.4288993 -4.4288726 -4.4288526 -4.4288287 -4.4288039 -4.4287829 -4.4287624 -4.4287515 -4.4287505 -4.4287548 -4.4287772 -4.42882 -4.428863 -4.428905][-4.428957 -4.4289336 -4.4289131 -4.4289 -4.4288869 -4.4288783 -4.4288764 -4.4288735 -4.4288721 -4.428875 -4.4288793 -4.4288888 -4.4289031 -4.4289188 -4.4289389][-4.4289784 -4.4289656 -4.4289541 -4.4289465 -4.4289393 -4.4289379 -4.4289389 -4.4289389 -4.4289389 -4.4289441 -4.4289513 -4.428957 -4.4289589 -4.4289618 -4.4289694]]...]
INFO - root - 2017-12-10 04:39:55.146983: step 910, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.250 sec/batch; 22h:59m:07s remains)
INFO - root - 2017-12-10 04:39:57.658245: step 920, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:24m:34s remains)
INFO - root - 2017-12-10 04:40:00.145972: step 930, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:51m:23s remains)
INFO - root - 2017-12-10 04:40:02.595005: step 940, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:22m:39s remains)
INFO - root - 2017-12-10 04:40:05.065178: step 950, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:15m:19s remains)
INFO - root - 2017-12-10 04:40:07.535237: step 960, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:00m:25s remains)
INFO - root - 2017-12-10 04:40:10.039561: step 970, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:19m:32s remains)
INFO - root - 2017-12-10 04:40:12.507113: step 980, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:02m:29s remains)
INFO - root - 2017-12-10 04:40:14.999206: step 990, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:16s remains)
INFO - root - 2017-12-10 04:40:17.444712: step 1000, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:17m:14s remains)
2017-12-10 04:40:17.744018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288096 -4.4288049 -4.4287992 -4.4288096 -4.42883 -4.428844 -4.4288464 -4.428844 -4.4288387 -4.4288335 -4.4288411 -4.4288507 -4.4288416 -4.4288244 -4.4288182][-4.42877 -4.4287572 -4.4287429 -4.4287448 -4.4287591 -4.4287772 -4.4287934 -4.4288096 -4.4288244 -4.4288378 -4.4288588 -4.4288697 -4.428853 -4.4288249 -4.4288154][-4.4287357 -4.428721 -4.4287071 -4.4287 -4.428699 -4.428709 -4.4287343 -4.4287691 -4.4288063 -4.4288397 -4.4288716 -4.4288855 -4.4288607 -4.4288144 -4.4287944][-4.428719 -4.4287148 -4.4287124 -4.4286966 -4.4286737 -4.4286585 -4.4286671 -4.4287004 -4.4287524 -4.4288077 -4.4288559 -4.4288759 -4.4288507 -4.4287939 -4.4287572][-4.4287224 -4.4287386 -4.4287515 -4.4287305 -4.428679 -4.4286184 -4.4285822 -4.428596 -4.4286566 -4.4287372 -4.42881 -4.4288464 -4.428834 -4.4287863 -4.4287472][-4.428731 -4.4287653 -4.4287887 -4.42876 -4.4286776 -4.42857 -4.4284835 -4.4284716 -4.4285364 -4.4286442 -4.4287567 -4.4288192 -4.4288287 -4.4288044 -4.4287829][-4.4287162 -4.4287462 -4.42877 -4.4287405 -4.4286437 -4.4285135 -4.4284024 -4.4283738 -4.4284377 -4.4285569 -4.4286919 -4.4287729 -4.4287949 -4.4287868 -4.4287882][-4.4286885 -4.4286976 -4.4287152 -4.4286933 -4.428606 -4.4284859 -4.4283853 -4.4283638 -4.4284286 -4.4285407 -4.4286623 -4.4287367 -4.4287596 -4.4287596 -4.4287806][-4.4287057 -4.4286952 -4.4286971 -4.4286771 -4.428607 -4.4285164 -4.4284563 -4.4284644 -4.4285283 -4.428618 -4.4287014 -4.4287434 -4.4287505 -4.4287448 -4.4287691][-4.4287891 -4.4287691 -4.4287534 -4.4287229 -4.4286613 -4.4285994 -4.4285769 -4.4286108 -4.4286766 -4.4287429 -4.4287858 -4.4287934 -4.4287791 -4.42876 -4.4287696][-4.4288745 -4.4288578 -4.428823 -4.4287734 -4.4287105 -4.4286647 -4.4286628 -4.4287176 -4.428793 -4.4288487 -4.4288664 -4.4288511 -4.428822 -4.4287891 -4.428782][-4.4289141 -4.4289064 -4.4288621 -4.4287996 -4.4287372 -4.4287028 -4.4287128 -4.4287782 -4.428854 -4.4288979 -4.428906 -4.4288836 -4.4288454 -4.4288039 -4.4287844][-4.4288993 -4.4289112 -4.4288692 -4.4288006 -4.4287376 -4.4287071 -4.4287133 -4.4287691 -4.4288344 -4.428874 -4.4288869 -4.428874 -4.4288416 -4.4287982 -4.42877][-4.4288611 -4.4288869 -4.4288507 -4.42878 -4.4287176 -4.428688 -4.4286814 -4.4287138 -4.42876 -4.4287972 -4.4288239 -4.4288297 -4.4288073 -4.4287648 -4.4287353][-4.42885 -4.4288774 -4.4288449 -4.4287791 -4.428719 -4.4286857 -4.4286661 -4.4286718 -4.4286947 -4.4287262 -4.4287629 -4.4287882 -4.428782 -4.4287448 -4.4287138]]...]
INFO - root - 2017-12-10 04:40:20.216178: step 1010, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:37m:08s remains)
INFO - root - 2017-12-10 04:40:22.744969: step 1020, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:43m:38s remains)
INFO - root - 2017-12-10 04:40:25.237483: step 1030, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:06m:12s remains)
INFO - root - 2017-12-10 04:40:27.731018: step 1040, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:30m:06s remains)
INFO - root - 2017-12-10 04:40:30.191190: step 1050, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:56m:54s remains)
INFO - root - 2017-12-10 04:40:32.713840: step 1060, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:58m:29s remains)
INFO - root - 2017-12-10 04:40:35.181526: step 1070, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:03m:01s remains)
INFO - root - 2017-12-10 04:40:37.681302: step 1080, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.254 sec/batch; 23h:20m:34s remains)
INFO - root - 2017-12-10 04:40:40.147061: step 1090, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:47m:29s remains)
INFO - root - 2017-12-10 04:40:42.612455: step 1100, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:44m:25s remains)
2017-12-10 04:40:42.899688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286671 -4.4285784 -4.428525 -4.4285269 -4.4285526 -4.4285989 -4.4286656 -4.4287286 -4.428772 -4.4287939 -4.4288158 -4.4288373 -4.4288654 -4.4288893 -4.4289036][-4.428699 -4.4286165 -4.4285388 -4.4284768 -4.4284406 -4.4284668 -4.42855 -4.4286485 -4.4287243 -4.4287753 -4.4288116 -4.4288311 -4.4288592 -4.4288855 -4.428905][-4.4287186 -4.4286418 -4.4285464 -4.4284329 -4.4283419 -4.4283514 -4.428453 -4.4285822 -4.428689 -4.4287663 -4.4288173 -4.4288368 -4.4288545 -4.4288754 -4.4288988][-4.4287362 -4.428669 -4.4285831 -4.4284697 -4.4283657 -4.42835 -4.4284363 -4.4285641 -4.4286828 -4.4287672 -4.4288225 -4.4288425 -4.4288521 -4.4288683 -4.4288907][-4.4287448 -4.4286933 -4.4286222 -4.4285307 -4.4284458 -4.4284177 -4.4284711 -4.4285655 -4.4286685 -4.4287453 -4.4288025 -4.4288316 -4.4288473 -4.428865 -4.4288883][-4.4287248 -4.4286795 -4.4286122 -4.4285374 -4.4284668 -4.4284282 -4.4284458 -4.4285092 -4.4286 -4.4286742 -4.4287386 -4.4287939 -4.4288311 -4.4288607 -4.4288912][-4.4287095 -4.4286647 -4.4285946 -4.4285212 -4.4284534 -4.4283981 -4.4283757 -4.4284053 -4.4284849 -4.428565 -4.428648 -4.4287357 -4.428802 -4.4288554 -4.428896][-4.428741 -4.4286971 -4.4286289 -4.4285531 -4.428483 -4.42842 -4.4283748 -4.4283733 -4.4284358 -4.4285169 -4.4286027 -4.4287 -4.4287834 -4.4288554 -4.428906][-4.4288006 -4.4287624 -4.4287038 -4.4286308 -4.4285579 -4.4284925 -4.4284463 -4.4284377 -4.4284859 -4.4285588 -4.4286351 -4.4287214 -4.4288034 -4.4288793 -4.4289303][-4.428833 -4.4288058 -4.4287596 -4.4286914 -4.4286132 -4.4285474 -4.4285116 -4.4285078 -4.4285455 -4.4286079 -4.4286804 -4.428761 -4.4288373 -4.428906 -4.4289503][-4.4288244 -4.4288149 -4.4287891 -4.4287324 -4.4286566 -4.4285908 -4.428555 -4.4285469 -4.4285679 -4.4286175 -4.4286904 -4.4287739 -4.4288487 -4.4289122 -4.4289517][-4.4288092 -4.4288154 -4.4288111 -4.4287744 -4.4287138 -4.4286566 -4.428618 -4.4285951 -4.4285893 -4.4286203 -4.4286852 -4.4287667 -4.4288411 -4.428905 -4.4289417][-4.4288297 -4.4288373 -4.4288425 -4.4288263 -4.4287887 -4.4287472 -4.4287114 -4.428678 -4.4286513 -4.4286609 -4.4287071 -4.428772 -4.4288383 -4.4288974 -4.4289322][-4.4288607 -4.4288578 -4.4288573 -4.4288521 -4.4288378 -4.4288177 -4.4287915 -4.4287596 -4.4287224 -4.42871 -4.4287367 -4.4287853 -4.4288425 -4.4288912 -4.4289222][-4.4288726 -4.4288578 -4.4288492 -4.4288468 -4.4288473 -4.4288445 -4.4288292 -4.428802 -4.428762 -4.4287386 -4.4287519 -4.428791 -4.4288397 -4.4288769 -4.4289031]]...]
INFO - root - 2017-12-10 04:40:45.378471: step 1110, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:46m:16s remains)
INFO - root - 2017-12-10 04:40:47.864488: step 1120, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:39m:20s remains)
INFO - root - 2017-12-10 04:40:50.346863: step 1130, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:28m:34s remains)
INFO - root - 2017-12-10 04:40:52.859658: step 1140, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 23h:00m:43s remains)
INFO - root - 2017-12-10 04:40:55.303967: step 1150, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:23m:02s remains)
INFO - root - 2017-12-10 04:40:57.770890: step 1160, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:59m:40s remains)
INFO - root - 2017-12-10 04:41:00.218320: step 1170, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:21m:57s remains)
INFO - root - 2017-12-10 04:41:02.684892: step 1180, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:56m:18s remains)
INFO - root - 2017-12-10 04:41:05.141497: step 1190, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:37m:29s remains)
INFO - root - 2017-12-10 04:41:07.614277: step 1200, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:43m:50s remains)
2017-12-10 04:41:07.943963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288626 -4.428863 -4.4288812 -4.4288821 -4.4288764 -4.4288778 -4.428885 -4.4289103 -4.4289351 -4.428947 -4.4289546 -4.428956 -4.428966 -4.4289641 -4.4289594][-4.4287882 -4.4287896 -4.4288282 -4.4288545 -4.428863 -4.4288707 -4.4288831 -4.4289165 -4.4289484 -4.428957 -4.428956 -4.428947 -4.4289494 -4.4289384 -4.428926][-4.4287181 -4.4287238 -4.4287815 -4.4288387 -4.4288635 -4.4288731 -4.42888 -4.42891 -4.4289422 -4.4289503 -4.4289451 -4.4289336 -4.4289341 -4.4289179 -4.4289][-4.4286757 -4.4286914 -4.4287567 -4.428822 -4.4288435 -4.428843 -4.4288368 -4.428863 -4.4289012 -4.428925 -4.4289355 -4.4289346 -4.4289379 -4.4289169 -4.4288926][-4.4286728 -4.42869 -4.4287486 -4.4287972 -4.4288025 -4.4287777 -4.4287472 -4.4287686 -4.4288263 -4.4288764 -4.428916 -4.4289327 -4.4289427 -4.428926 -4.4289017][-4.428689 -4.4286962 -4.4287386 -4.4287596 -4.4287333 -4.4286695 -4.4285951 -4.4286013 -4.4286933 -4.4287868 -4.4288645 -4.4289012 -4.4289255 -4.4289255 -4.4289112][-4.4287157 -4.4287157 -4.4287333 -4.4287157 -4.4286447 -4.4285235 -4.4283819 -4.4283519 -4.4284911 -4.4286509 -4.4287791 -4.4288507 -4.4288936 -4.4289145 -4.4289112][-4.4287376 -4.4287262 -4.4287224 -4.4286704 -4.4285569 -4.4283829 -4.4281735 -4.4280992 -4.4282966 -4.4285278 -4.4287057 -4.4288096 -4.4288659 -4.42889 -4.4288955][-4.4287491 -4.4287271 -4.4287062 -4.4286437 -4.4285259 -4.4283638 -4.42818 -4.4281211 -4.4283061 -4.4285293 -4.4287019 -4.4288106 -4.4288697 -4.4288917 -4.4288931][-4.4287505 -4.4287372 -4.4287152 -4.4286652 -4.4285846 -4.428494 -4.4284072 -4.4283886 -4.4284916 -4.428627 -4.42875 -4.4288397 -4.4288907 -4.428906 -4.428906][-4.4287605 -4.4287658 -4.4287572 -4.4287281 -4.4286904 -4.4286637 -4.4286509 -4.4286594 -4.4287014 -4.4287605 -4.4288297 -4.4288883 -4.4289188 -4.428926 -4.4289279][-4.4288087 -4.428822 -4.4288182 -4.4288 -4.4287839 -4.4287925 -4.4288163 -4.4288392 -4.4288578 -4.4288759 -4.42891 -4.4289393 -4.4289465 -4.4289389 -4.4289417][-4.4288797 -4.4288869 -4.4288783 -4.4288573 -4.4288445 -4.4288626 -4.428895 -4.4289179 -4.428925 -4.428926 -4.4289393 -4.42895 -4.42894 -4.4289203 -4.4289184][-4.428915 -4.4289088 -4.4289017 -4.428884 -4.4288664 -4.428874 -4.428894 -4.4289021 -4.428896 -4.4288864 -4.4288926 -4.4288945 -4.4288836 -4.42887 -4.4288697][-4.4289126 -4.428896 -4.428894 -4.4288855 -4.4288683 -4.4288673 -4.4288731 -4.42887 -4.4288564 -4.42884 -4.4288411 -4.4288392 -4.4288321 -4.42883 -4.4288373]]...]
INFO - root - 2017-12-10 04:41:10.401057: step 1210, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:35m:10s remains)
INFO - root - 2017-12-10 04:41:12.864964: step 1220, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:20m:35s remains)
INFO - root - 2017-12-10 04:41:15.315650: step 1230, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:32m:15s remains)
INFO - root - 2017-12-10 04:41:17.777031: step 1240, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:41m:49s remains)
INFO - root - 2017-12-10 04:41:20.249168: step 1250, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:26m:02s remains)
INFO - root - 2017-12-10 04:41:22.736167: step 1260, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:50s remains)
INFO - root - 2017-12-10 04:41:25.190006: step 1270, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:23m:13s remains)
INFO - root - 2017-12-10 04:41:27.715119: step 1280, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:22m:39s remains)
INFO - root - 2017-12-10 04:41:30.206506: step 1290, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:35m:09s remains)
INFO - root - 2017-12-10 04:41:32.675679: step 1300, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:45m:43s remains)
2017-12-10 04:41:32.973418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428741 -4.4287543 -4.4287734 -4.4287853 -4.4287877 -4.4287772 -4.4287233 -4.4286103 -4.428535 -4.4285717 -4.4286413 -4.428709 -4.4287763 -4.4288387 -4.4288726][-4.4286838 -4.4286928 -4.42871 -4.4287224 -4.4287353 -4.4287438 -4.4286995 -4.4285889 -4.4285164 -4.4285555 -4.4286218 -4.4286885 -4.42876 -4.4288282 -4.4288645][-4.4286346 -4.4286356 -4.4286475 -4.42866 -4.4286871 -4.4287186 -4.4286847 -4.4285803 -4.428515 -4.4285626 -4.4286294 -4.4286928 -4.4287615 -4.4288278 -4.4288621][-4.4286022 -4.4285913 -4.428596 -4.428606 -4.4286456 -4.4286952 -4.428669 -4.4285669 -4.4285092 -4.4285717 -4.428647 -4.4287095 -4.4287748 -4.4288359 -4.4288659][-4.4285822 -4.4285607 -4.4285583 -4.4285626 -4.4286041 -4.4286609 -4.4286342 -4.4285259 -4.4284759 -4.4285612 -4.4286537 -4.428721 -4.4287887 -4.4288492 -4.4288754][-4.4285975 -4.4285612 -4.4285445 -4.4285307 -4.4285541 -4.4286013 -4.4285612 -4.4284391 -4.4283996 -4.4285173 -4.4286375 -4.428719 -4.4287972 -4.428863 -4.4288883][-4.4286666 -4.4286194 -4.4285855 -4.4285469 -4.4285383 -4.4285593 -4.4284921 -4.4283452 -4.4283113 -4.4284596 -4.4286056 -4.4287028 -4.4287953 -4.4288707 -4.4288993][-4.4287515 -4.4287086 -4.4286675 -4.4286156 -4.428586 -4.4285808 -4.4284859 -4.4283185 -4.4282856 -4.428443 -4.4285917 -4.42869 -4.4287891 -4.4288716 -4.4289036][-4.4288278 -4.4287939 -4.4287491 -4.4286947 -4.4286618 -4.4286451 -4.4285445 -4.4283867 -4.4283605 -4.4285021 -4.4286284 -4.428709 -4.4287972 -4.4288745 -4.4289045][-4.4288726 -4.4288483 -4.4288063 -4.4287586 -4.4287329 -4.4287157 -4.4286251 -4.428494 -4.4284782 -4.4285979 -4.4286985 -4.42876 -4.4288278 -4.4288888 -4.42891][-4.4288936 -4.4288788 -4.428843 -4.4288049 -4.4287896 -4.4287772 -4.4287024 -4.4285975 -4.4285903 -4.4286919 -4.4287739 -4.428823 -4.4288716 -4.4289145 -4.4289227][-4.4289041 -4.428895 -4.4288645 -4.4288354 -4.4288297 -4.428822 -4.4287624 -4.4286809 -4.4286819 -4.4287686 -4.4288368 -4.4288764 -4.4289117 -4.4289389 -4.428937][-4.4289117 -4.4289036 -4.42888 -4.4288607 -4.4288588 -4.4288516 -4.4288049 -4.4287467 -4.4287572 -4.4288321 -4.42889 -4.4289236 -4.4289474 -4.4289613 -4.4289513][-4.4289131 -4.4289055 -4.42889 -4.42888 -4.4288788 -4.4288707 -4.4288383 -4.4288011 -4.4288163 -4.4288793 -4.4289303 -4.4289589 -4.4289761 -4.4289794 -4.4289641][-4.4289203 -4.4289122 -4.4289021 -4.4288988 -4.4288974 -4.4288936 -4.428875 -4.4288511 -4.4288635 -4.4289131 -4.4289541 -4.428978 -4.4289918 -4.42899 -4.4289713]]...]
INFO - root - 2017-12-10 04:41:35.486043: step 1310, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:13m:25s remains)
INFO - root - 2017-12-10 04:41:37.960740: step 1320, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:36m:18s remains)
INFO - root - 2017-12-10 04:41:40.430555: step 1330, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:36m:33s remains)
INFO - root - 2017-12-10 04:41:42.916698: step 1340, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:58m:01s remains)
INFO - root - 2017-12-10 04:41:45.400424: step 1350, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 22h:19m:04s remains)
INFO - root - 2017-12-10 04:41:47.868215: step 1360, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:29m:47s remains)
INFO - root - 2017-12-10 04:41:50.316763: step 1370, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:22m:55s remains)
INFO - root - 2017-12-10 04:41:52.770672: step 1380, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:05m:22s remains)
INFO - root - 2017-12-10 04:41:55.225533: step 1390, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:31m:28s remains)
INFO - root - 2017-12-10 04:41:57.723337: step 1400, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.250 sec/batch; 23h:01m:53s remains)
2017-12-10 04:41:58.027588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288321 -4.4288359 -4.4288421 -4.4288378 -4.428833 -4.4288359 -4.4288435 -4.4288378 -4.4288154 -4.4287891 -4.428772 -4.4287624 -4.428762 -4.4287672 -4.4287896][-4.4288111 -4.4288163 -4.4288259 -4.4288235 -4.4288192 -4.4288216 -4.4288316 -4.4288359 -4.4288278 -4.4288139 -4.4288054 -4.4287944 -4.4287868 -4.4287834 -4.4287987][-4.4288015 -4.428803 -4.4288082 -4.4288034 -4.4287982 -4.4287977 -4.4288096 -4.4288225 -4.42883 -4.4288321 -4.42883 -4.4288177 -4.4287982 -4.428782 -4.4287906][-4.4287825 -4.4287772 -4.4287758 -4.4287653 -4.4287505 -4.4287372 -4.4287405 -4.4287586 -4.4287834 -4.4288111 -4.4288321 -4.42883 -4.4288054 -4.4287782 -4.4287758][-4.4287548 -4.4287367 -4.4287229 -4.4287052 -4.4286766 -4.4286442 -4.4286313 -4.4286447 -4.4286795 -4.4287391 -4.4287939 -4.4288158 -4.428803 -4.4287786 -4.4287696][-4.4287438 -4.4287081 -4.4286728 -4.4286356 -4.4285784 -4.4285111 -4.4284759 -4.4284859 -4.4285374 -4.4286351 -4.4287276 -4.4287796 -4.4287877 -4.4287763 -4.4287682][-4.4287839 -4.4287271 -4.4286661 -4.4286027 -4.428514 -4.428401 -4.4283314 -4.4283347 -4.4284029 -4.42854 -4.4286714 -4.4287586 -4.4287953 -4.428803 -4.4287996][-4.4288244 -4.4287553 -4.4286833 -4.4286118 -4.428515 -4.428381 -4.4282904 -4.428288 -4.4283667 -4.4285278 -4.4286819 -4.4287863 -4.4288392 -4.42886 -4.428864][-4.4288597 -4.428793 -4.4287262 -4.4286642 -4.4285817 -4.4284678 -4.4283872 -4.4283919 -4.4284639 -4.4286079 -4.4287319 -4.4288111 -4.4288545 -4.4288797 -4.4288921][-4.4289021 -4.4288516 -4.4287944 -4.42874 -4.4286733 -4.42859 -4.428534 -4.428545 -4.4285994 -4.4286971 -4.4287705 -4.4288087 -4.4288297 -4.4288473 -4.428864][-4.428925 -4.4288983 -4.4288578 -4.4288111 -4.4287596 -4.4287019 -4.42866 -4.4286609 -4.4286852 -4.4287343 -4.4287634 -4.4287763 -4.4287863 -4.4288039 -4.4288282][-4.4289222 -4.4289184 -4.4288993 -4.4288692 -4.4288354 -4.4287944 -4.4287548 -4.4287362 -4.4287291 -4.4287376 -4.4287376 -4.4287395 -4.4287529 -4.4287829 -4.4288187][-4.4288921 -4.4289103 -4.4289126 -4.4289012 -4.4288859 -4.4288616 -4.4288244 -4.42879 -4.4287643 -4.4287477 -4.42873 -4.428731 -4.4287539 -4.4287963 -4.428843][-4.4288421 -4.4288745 -4.4288955 -4.4289031 -4.4289055 -4.428895 -4.4288673 -4.4288373 -4.4288116 -4.4287868 -4.4287648 -4.4287677 -4.4287896 -4.4288311 -4.4288783][-4.4287934 -4.4288292 -4.4288583 -4.428875 -4.4288859 -4.4288869 -4.4288726 -4.4288573 -4.4288406 -4.4288182 -4.4287977 -4.4288034 -4.4288211 -4.4288526 -4.4288898]]...]
INFO - root - 2017-12-10 04:42:00.512819: step 1410, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:34m:48s remains)
INFO - root - 2017-12-10 04:42:03.008989: step 1420, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:12m:43s remains)
INFO - root - 2017-12-10 04:42:05.497593: step 1430, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:42m:18s remains)
INFO - root - 2017-12-10 04:42:07.983639: step 1440, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:44m:32s remains)
INFO - root - 2017-12-10 04:42:10.488322: step 1450, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:42m:05s remains)
INFO - root - 2017-12-10 04:42:12.940598: step 1460, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:04m:59s remains)
INFO - root - 2017-12-10 04:42:15.438737: step 1470, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:16m:03s remains)
INFO - root - 2017-12-10 04:42:17.927499: step 1480, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:51m:43s remains)
INFO - root - 2017-12-10 04:42:20.445031: step 1490, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:56s remains)
INFO - root - 2017-12-10 04:42:22.916303: step 1500, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:30m:41s remains)
2017-12-10 04:42:23.211495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288173 -4.4287739 -4.4287477 -4.4287505 -4.4287744 -4.4287887 -4.428823 -4.4288721 -4.4288936 -4.4288812 -4.4288473 -4.4288192 -4.4287891 -4.4287992 -4.4288568][-4.4288034 -4.4287429 -4.4287033 -4.4286995 -4.4287372 -4.428762 -4.4287934 -4.428843 -4.42887 -4.4288568 -4.4288092 -4.4287724 -4.4287333 -4.4287448 -4.4288154][-4.4287939 -4.42872 -4.4286637 -4.4286494 -4.4287 -4.42874 -4.428771 -4.4288259 -4.4288635 -4.428853 -4.4287996 -4.4287577 -4.4287138 -4.4287262 -4.4287996][-4.4287786 -4.4286966 -4.428617 -4.4285855 -4.4286475 -4.4287128 -4.4287434 -4.4288015 -4.4288435 -4.4288278 -4.4287758 -4.4287329 -4.4286981 -4.4287181 -4.4287939][-4.4287753 -4.4286795 -4.42858 -4.4285326 -4.428597 -4.428679 -4.4287119 -4.4287682 -4.4288025 -4.4287739 -4.4287171 -4.4286819 -4.4286633 -4.4286952 -4.4287777][-4.4287567 -4.4286513 -4.4285383 -4.4284768 -4.4285388 -4.4286346 -4.4286757 -4.4287257 -4.4287539 -4.4287124 -4.42865 -4.4286284 -4.428637 -4.4286833 -4.4287715][-4.4287324 -4.428616 -4.4284863 -4.4283953 -4.4284458 -4.4285612 -4.4286227 -4.4286847 -4.4287171 -4.42867 -4.4286084 -4.4285989 -4.4286284 -4.4286866 -4.4287777][-4.4287066 -4.4285774 -4.4284225 -4.428299 -4.4283452 -4.4284878 -4.4285808 -4.4286647 -4.4287109 -4.4286737 -4.4286194 -4.4286132 -4.4286451 -4.4287038 -4.4287906][-4.4287109 -4.4285913 -4.4284387 -4.4283118 -4.4283581 -4.4285035 -4.4286084 -4.4286947 -4.4287415 -4.4287066 -4.428648 -4.4286332 -4.4286547 -4.4287062 -4.4287882][-4.4287472 -4.4286423 -4.42851 -4.4284005 -4.428442 -4.428566 -4.4286547 -4.4287119 -4.4287376 -4.4286971 -4.4286418 -4.4286327 -4.4286556 -4.4287052 -4.4287887][-4.428791 -4.4287033 -4.4286041 -4.428524 -4.42855 -4.4286356 -4.4287004 -4.4287372 -4.4287467 -4.4287081 -4.4286752 -4.4286776 -4.4287038 -4.4287539 -4.4288311][-4.42883 -4.4287591 -4.4286952 -4.4286518 -4.4286671 -4.4287152 -4.4287615 -4.428792 -4.4287934 -4.428761 -4.42874 -4.428751 -4.428781 -4.4288297 -4.42889][-4.4288411 -4.4287906 -4.4287596 -4.4287496 -4.4287663 -4.4287915 -4.4288192 -4.4288425 -4.4288406 -4.4288144 -4.4287992 -4.4288073 -4.4288359 -4.4288836 -4.4289341][-4.4288445 -4.4288087 -4.4287958 -4.428802 -4.4288192 -4.4288335 -4.4288483 -4.4288678 -4.4288688 -4.4288516 -4.4288435 -4.428854 -4.4288764 -4.4289136 -4.4289546][-4.4288764 -4.4288526 -4.4288406 -4.4288425 -4.4288526 -4.4288592 -4.4288678 -4.4288859 -4.4288931 -4.4288864 -4.428884 -4.4288936 -4.4289088 -4.4289336 -4.4289656]]...]
INFO - root - 2017-12-10 04:42:25.710457: step 1510, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:21m:07s remains)
INFO - root - 2017-12-10 04:42:28.177591: step 1520, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:02m:36s remains)
INFO - root - 2017-12-10 04:42:30.680836: step 1530, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:21m:12s remains)
INFO - root - 2017-12-10 04:42:33.168814: step 1540, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:38m:04s remains)
INFO - root - 2017-12-10 04:42:35.631310: step 1550, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:10m:52s remains)
INFO - root - 2017-12-10 04:42:38.099058: step 1560, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:21m:50s remains)
INFO - root - 2017-12-10 04:42:40.562476: step 1570, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:28m:50s remains)
INFO - root - 2017-12-10 04:42:43.033626: step 1580, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:13m:04s remains)
INFO - root - 2017-12-10 04:42:45.494251: step 1590, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:01m:55s remains)
INFO - root - 2017-12-10 04:42:47.950500: step 1600, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:51m:48s remains)
2017-12-10 04:42:48.233295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290013 -4.429019 -4.4290323 -4.4290404 -4.4290361 -4.4290233 -4.4290009 -4.4289861 -4.4289851 -4.4289794 -4.4289727 -4.4289665 -4.4289584 -4.4289522 -4.4289513][-4.428967 -4.4289861 -4.4290056 -4.4290218 -4.4290252 -4.4290161 -4.4289913 -4.4289727 -4.4289708 -4.4289627 -4.428956 -4.4289489 -4.4289351 -4.428925 -4.4289207][-4.4289336 -4.4289484 -4.4289694 -4.428988 -4.4289961 -4.4289937 -4.4289732 -4.42896 -4.4289651 -4.4289646 -4.4289622 -4.4289536 -4.4289317 -4.4289145 -4.4289012][-4.4289069 -4.4289069 -4.4289155 -4.4289227 -4.4289236 -4.4289227 -4.4289064 -4.4289017 -4.4289236 -4.4289432 -4.4289556 -4.4289551 -4.4289341 -4.4289136 -4.4288931][-4.4288859 -4.4288669 -4.4288545 -4.428843 -4.4288244 -4.4288087 -4.4287839 -4.4287825 -4.4288335 -4.4288864 -4.4289203 -4.4289308 -4.4289131 -4.428896 -4.4288764][-4.4288788 -4.4288445 -4.4288144 -4.4287791 -4.4287291 -4.4286852 -4.428628 -4.4286103 -4.4286985 -4.4287992 -4.4288659 -4.4288926 -4.4288831 -4.4288726 -4.4288588][-4.4288864 -4.4288435 -4.4287944 -4.4287267 -4.42863 -4.4285312 -4.4284015 -4.4283376 -4.4284668 -4.4286318 -4.4287477 -4.428813 -4.4288287 -4.4288344 -4.4288311][-4.4289079 -4.428875 -4.4288311 -4.42876 -4.4286504 -4.4285178 -4.4283285 -4.4282179 -4.4283476 -4.4285269 -4.4286666 -4.4287562 -4.4287872 -4.4288054 -4.4288163][-4.4289274 -4.42892 -4.4289107 -4.4288764 -4.4288025 -4.428699 -4.4285412 -4.4284363 -4.4285035 -4.4286075 -4.4287024 -4.4287691 -4.4287863 -4.4288006 -4.4288149][-4.428906 -4.4289069 -4.4289203 -4.4289136 -4.4288645 -4.428792 -4.4286838 -4.428616 -4.4286528 -4.4286962 -4.4287434 -4.4287744 -4.4287758 -4.4287858 -4.4288015][-4.4288707 -4.4288673 -4.428885 -4.42889 -4.4288526 -4.4288077 -4.4287434 -4.4287124 -4.4287415 -4.428761 -4.428781 -4.4287882 -4.4287825 -4.4287887 -4.4288][-4.4288397 -4.4288259 -4.4288387 -4.4288464 -4.4288244 -4.4288096 -4.428782 -4.4287763 -4.4288063 -4.4288144 -4.4288192 -4.4288139 -4.4288077 -4.428812 -4.4288177][-4.4288249 -4.4287996 -4.4287996 -4.4287968 -4.4287806 -4.4287839 -4.4287767 -4.4287848 -4.4288225 -4.4288349 -4.4288373 -4.4288278 -4.4288211 -4.4288235 -4.4288263][-4.4288306 -4.4287958 -4.4287858 -4.4287777 -4.4287663 -4.4287782 -4.4287777 -4.4287887 -4.4288282 -4.428843 -4.4288487 -4.4288421 -4.4288392 -4.4288397 -4.4288368][-4.4288478 -4.4288077 -4.4287872 -4.4287734 -4.4287634 -4.4287744 -4.4287724 -4.4287815 -4.4288139 -4.4288282 -4.4288368 -4.4288406 -4.4288492 -4.4288564 -4.4288578]]...]
INFO - root - 2017-12-10 04:42:50.673495: step 1610, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:23m:10s remains)
INFO - root - 2017-12-10 04:42:53.199347: step 1620, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:30m:04s remains)
INFO - root - 2017-12-10 04:42:55.667529: step 1630, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:08m:54s remains)
INFO - root - 2017-12-10 04:42:58.121372: step 1640, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:21s remains)
INFO - root - 2017-12-10 04:43:00.571409: step 1650, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:24m:38s remains)
INFO - root - 2017-12-10 04:43:03.106982: step 1660, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:47m:01s remains)
INFO - root - 2017-12-10 04:43:05.568415: step 1670, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:19m:31s remains)
INFO - root - 2017-12-10 04:43:08.034855: step 1680, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:36s remains)
INFO - root - 2017-12-10 04:43:10.520731: step 1690, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:51m:01s remains)
INFO - root - 2017-12-10 04:43:12.991915: step 1700, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:19m:05s remains)
2017-12-10 04:43:13.262615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285979 -4.4284515 -4.4283991 -4.4284906 -4.4286466 -4.4287763 -4.428853 -4.4288735 -4.4288673 -4.4288478 -4.428843 -4.4288549 -4.428863 -4.4288406 -4.4287939][-4.4285073 -4.4283252 -4.4282818 -4.4284182 -4.4286151 -4.4287596 -4.4288354 -4.4288507 -4.4288344 -4.4288168 -4.4288173 -4.4288335 -4.42885 -4.4288235 -4.4287643][-4.428493 -4.428298 -4.4282656 -4.4284244 -4.4286294 -4.4287586 -4.4288039 -4.4287949 -4.4287653 -4.428761 -4.4287815 -4.42882 -4.4288492 -4.4288225 -4.4287562][-4.4285574 -4.4284024 -4.4283981 -4.4285464 -4.4287157 -4.4287977 -4.4287782 -4.4287081 -4.4286551 -4.4286704 -4.4287238 -4.4287944 -4.42884 -4.4288187 -4.4287529][-4.4286575 -4.4285808 -4.4286051 -4.4287047 -4.4288011 -4.4288106 -4.4287062 -4.4285512 -4.4284697 -4.4285207 -4.4286246 -4.4287386 -4.4288092 -4.4287915 -4.4287286][-4.4287481 -4.4287267 -4.4287505 -4.4287825 -4.428791 -4.4287124 -4.4285073 -4.42826 -4.4281693 -4.4283 -4.4284835 -4.4286447 -4.4287424 -4.4287348 -4.4286814][-4.4288077 -4.4288039 -4.4288039 -4.4287863 -4.4287291 -4.4285674 -4.4282551 -4.42792 -4.4278512 -4.4280882 -4.4283562 -4.4285588 -4.4286747 -4.4286823 -4.4286485][-4.4288545 -4.42885 -4.4288254 -4.4287796 -4.4286919 -4.4285 -4.4281707 -4.4278502 -4.4278378 -4.428091 -4.4283528 -4.4285436 -4.4286518 -4.4286652 -4.42864][-4.4288855 -4.4288726 -4.4288354 -4.4287844 -4.4287014 -4.4285445 -4.4283071 -4.4281125 -4.4281244 -4.4282956 -4.4284787 -4.4286137 -4.4286842 -4.4286804 -4.4286566][-4.428895 -4.4288797 -4.4288425 -4.4287992 -4.4287348 -4.4286275 -4.4284887 -4.4283919 -4.4284134 -4.4285216 -4.42864 -4.4287195 -4.4287462 -4.4287219 -4.4286976][-4.4288945 -4.428885 -4.4288578 -4.4288259 -4.4287887 -4.4287271 -4.4286509 -4.4286041 -4.4286294 -4.4286995 -4.4287705 -4.4288096 -4.4288144 -4.4287858 -4.4287634][-4.4289002 -4.428895 -4.4288754 -4.4288554 -4.4288421 -4.4288182 -4.4287825 -4.4287629 -4.4287825 -4.4288206 -4.4288564 -4.4288716 -4.4288669 -4.4288383 -4.4288163][-4.4289083 -4.4289002 -4.4288936 -4.42889 -4.4288921 -4.4288893 -4.4288759 -4.42887 -4.4288831 -4.4288974 -4.4289045 -4.4289021 -4.428894 -4.4288721 -4.4288521][-4.4289317 -4.4289174 -4.428915 -4.4289207 -4.4289317 -4.4289379 -4.4289379 -4.4289351 -4.4289346 -4.4289331 -4.4289269 -4.4289174 -4.4289112 -4.4288983 -4.428885][-4.4289455 -4.4289322 -4.4289327 -4.4289403 -4.4289503 -4.4289556 -4.4289556 -4.4289517 -4.428946 -4.4289393 -4.4289308 -4.4289241 -4.4289231 -4.42892 -4.428915]]...]
INFO - root - 2017-12-10 04:43:15.726106: step 1710, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:20m:48s remains)
INFO - root - 2017-12-10 04:43:18.174439: step 1720, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:23m:21s remains)
INFO - root - 2017-12-10 04:43:20.638892: step 1730, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 24h:01m:53s remains)
INFO - root - 2017-12-10 04:43:23.142185: step 1740, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:07m:05s remains)
INFO - root - 2017-12-10 04:43:25.625231: step 1750, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:23m:19s remains)
INFO - root - 2017-12-10 04:43:28.095071: step 1760, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:29m:54s remains)
INFO - root - 2017-12-10 04:43:30.554024: step 1770, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.250 sec/batch; 22h:55m:27s remains)
INFO - root - 2017-12-10 04:43:33.012134: step 1780, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:48m:30s remains)
INFO - root - 2017-12-10 04:43:35.453397: step 1790, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:09m:34s remains)
INFO - root - 2017-12-10 04:43:37.961978: step 1800, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:14m:19s remains)
2017-12-10 04:43:38.246445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288192 -4.4287796 -4.4287314 -4.4287014 -4.4286942 -4.4287071 -4.4287229 -4.4287524 -4.4287877 -4.4288139 -4.42883 -4.4288344 -4.4288325 -4.4288197 -4.4287968][-4.4287877 -4.4287376 -4.4286847 -4.4286633 -4.4286714 -4.42869 -4.4287038 -4.4287286 -4.4287615 -4.4287829 -4.4287915 -4.4287896 -4.4287863 -4.4287744 -4.4287567][-4.4287667 -4.4287119 -4.428658 -4.4286394 -4.4286542 -4.428678 -4.4286933 -4.428719 -4.4287553 -4.4287705 -4.4287672 -4.428761 -4.4287567 -4.4287519 -4.4287457][-4.4287586 -4.42872 -4.4286709 -4.428637 -4.4286294 -4.428647 -4.4286709 -4.4287109 -4.428762 -4.4287748 -4.4287581 -4.4287453 -4.42874 -4.4287419 -4.4287462][-4.4288006 -4.4287949 -4.4287615 -4.4287028 -4.4286442 -4.4286218 -4.4286375 -4.428689 -4.4287643 -4.4287839 -4.4287558 -4.4287353 -4.4287252 -4.4287181 -4.4287214][-4.4288244 -4.4288416 -4.4288278 -4.428762 -4.4286571 -4.4285769 -4.4285579 -4.428607 -4.4287081 -4.428751 -4.428731 -4.4287148 -4.4286933 -4.4286532 -4.4286366][-4.4288111 -4.428844 -4.4288416 -4.4287782 -4.428648 -4.428515 -4.4284472 -4.428472 -4.428587 -4.4286728 -4.4286895 -4.42868 -4.42864 -4.4285555 -4.4284992][-4.428771 -4.4288197 -4.4288316 -4.4287815 -4.4286466 -4.4284687 -4.4283361 -4.4283214 -4.4284463 -4.4285927 -4.4286575 -4.42865 -4.428586 -4.4284554 -4.4283547][-4.4287047 -4.4287634 -4.4287968 -4.4287643 -4.4286385 -4.4284329 -4.4282284 -4.4281721 -4.4283142 -4.4285159 -4.4286356 -4.4286442 -4.4285736 -4.4284244 -4.4283056][-4.4286308 -4.4286938 -4.4287486 -4.4287362 -4.428637 -4.4284534 -4.4282365 -4.4281607 -4.4282951 -4.4284987 -4.4286327 -4.4286623 -4.4286184 -4.42851 -4.428422][-4.4285827 -4.428637 -4.4287 -4.4287119 -4.4286604 -4.4285517 -4.4284153 -4.4283543 -4.4284329 -4.4285712 -4.4286637 -4.4286952 -4.4286947 -4.4286456 -4.4285903][-4.4286103 -4.4286337 -4.4286866 -4.4287133 -4.4287052 -4.428679 -4.4286356 -4.4286032 -4.4286289 -4.4286828 -4.4287176 -4.4287333 -4.4287534 -4.4287453 -4.4287157][-4.4287076 -4.4286995 -4.4287271 -4.4287539 -4.4287605 -4.42877 -4.4287724 -4.4287629 -4.4287648 -4.4287677 -4.4287682 -4.4287806 -4.428813 -4.4288268 -4.4288111][-4.428823 -4.4288063 -4.428813 -4.4288287 -4.4288349 -4.4288492 -4.4288607 -4.4288673 -4.4288688 -4.4288588 -4.4288511 -4.4288659 -4.4288993 -4.4289184 -4.4289045][-4.4289131 -4.4289 -4.428896 -4.4288993 -4.4289041 -4.428915 -4.4289227 -4.4289284 -4.4289317 -4.428926 -4.4289246 -4.4289403 -4.4289613 -4.4289727 -4.4289622]]...]
INFO - root - 2017-12-10 04:43:40.721083: step 1810, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:56m:56s remains)
INFO - root - 2017-12-10 04:43:43.174159: step 1820, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:50m:15s remains)
INFO - root - 2017-12-10 04:43:45.671789: step 1830, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 21h:58m:55s remains)
INFO - root - 2017-12-10 04:43:48.157003: step 1840, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:56m:55s remains)
INFO - root - 2017-12-10 04:43:50.638705: step 1850, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:37m:06s remains)
INFO - root - 2017-12-10 04:43:53.122880: step 1860, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:50m:19s remains)
INFO - root - 2017-12-10 04:43:55.639338: step 1870, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:12m:01s remains)
INFO - root - 2017-12-10 04:43:58.095734: step 1880, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:08m:42s remains)
INFO - root - 2017-12-10 04:44:00.576947: step 1890, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:03m:01s remains)
INFO - root - 2017-12-10 04:44:03.046208: step 1900, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:57m:01s remains)
2017-12-10 04:44:03.339041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287286 -4.4287 -4.4287229 -4.4287596 -4.4287953 -4.4288168 -4.4288192 -4.4288344 -4.4288487 -4.4288425 -4.4288321 -4.4288044 -4.4287715 -4.4287839 -4.4288082][-4.4287024 -4.4286346 -4.4286346 -4.4286838 -4.4287515 -4.4287777 -4.428772 -4.4287877 -4.4288149 -4.428812 -4.4288063 -4.4287848 -4.4287543 -4.4287586 -4.4287729][-4.4286628 -4.4285617 -4.4285398 -4.4285989 -4.4286923 -4.4287219 -4.4287119 -4.4287243 -4.4287577 -4.428762 -4.428771 -4.4287724 -4.4287519 -4.4287448 -4.4287415][-4.4286342 -4.4285312 -4.4284935 -4.4285412 -4.4286313 -4.4286675 -4.4286647 -4.4286742 -4.4287071 -4.42871 -4.4287353 -4.4287615 -4.4287519 -4.4287372 -4.4287229][-4.4286237 -4.4285412 -4.4285045 -4.4285274 -4.4285951 -4.4286289 -4.42864 -4.428658 -4.4286933 -4.4286981 -4.4287333 -4.4287724 -4.4287682 -4.4287477 -4.4287219][-4.4286404 -4.4285812 -4.4285412 -4.4285312 -4.4285645 -4.4285784 -4.4286003 -4.4286432 -4.4286933 -4.4286976 -4.4287267 -4.4287591 -4.4287457 -4.4287238 -4.4286985][-4.4286489 -4.4286075 -4.4285626 -4.4285212 -4.4285116 -4.4284992 -4.4285235 -4.4285827 -4.4286485 -4.4286518 -4.4286695 -4.4286928 -4.4286852 -4.4286728 -4.4286551][-4.4286342 -4.4286003 -4.4285564 -4.4285026 -4.428462 -4.4284239 -4.4284325 -4.4284844 -4.428556 -4.4285722 -4.4285917 -4.4286218 -4.4286337 -4.4286389 -4.4286313][-4.4286065 -4.4285712 -4.4285312 -4.4284859 -4.4284391 -4.42839 -4.4283791 -4.4284067 -4.4284658 -4.4284911 -4.4285259 -4.4285736 -4.4286008 -4.4286137 -4.4286118][-4.4285741 -4.4285383 -4.4285026 -4.4284687 -4.4284344 -4.4283915 -4.428371 -4.4283805 -4.4284277 -4.4284611 -4.4285083 -4.4285693 -4.428606 -4.4286203 -4.4286132][-4.4285994 -4.4285674 -4.4285398 -4.4285135 -4.4284854 -4.4284515 -4.4284306 -4.4284306 -4.4284587 -4.42848 -4.4285245 -4.4285927 -4.4286408 -4.428659 -4.4286461][-4.428688 -4.4286656 -4.4286442 -4.4286213 -4.4285975 -4.4285746 -4.428556 -4.4285474 -4.428556 -4.4285522 -4.42857 -4.4286237 -4.4286718 -4.4286923 -4.428678][-4.4288058 -4.4287949 -4.4287806 -4.4287634 -4.4287462 -4.4287319 -4.4287181 -4.4287062 -4.4287004 -4.42868 -4.4286714 -4.4286976 -4.4287295 -4.4287457 -4.4287343][-4.4289093 -4.4289021 -4.4288936 -4.4288831 -4.4288716 -4.42886 -4.42885 -4.42884 -4.4288316 -4.4288116 -4.4287915 -4.4287953 -4.4288116 -4.4288235 -4.4288206][-4.4289761 -4.4289675 -4.4289618 -4.4289551 -4.4289484 -4.4289403 -4.4289322 -4.4289255 -4.42892 -4.4289055 -4.4288883 -4.428885 -4.4288912 -4.4288983 -4.4289021]]...]
INFO - root - 2017-12-10 04:44:05.802783: step 1910, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:30m:19s remains)
INFO - root - 2017-12-10 04:44:08.267824: step 1920, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:47m:02s remains)
INFO - root - 2017-12-10 04:44:10.760314: step 1930, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:46m:49s remains)
INFO - root - 2017-12-10 04:44:13.222331: step 1940, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.238 sec/batch; 21h:53m:46s remains)
INFO - root - 2017-12-10 04:44:15.724353: step 1950, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:21m:13s remains)
INFO - root - 2017-12-10 04:44:18.235913: step 1960, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:09m:26s remains)
INFO - root - 2017-12-10 04:44:20.746811: step 1970, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.272 sec/batch; 24h:56m:23s remains)
INFO - root - 2017-12-10 04:44:23.270339: step 1980, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:17m:29s remains)
INFO - root - 2017-12-10 04:44:25.735340: step 1990, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:14m:03s remains)
INFO - root - 2017-12-10 04:44:28.212798: step 2000, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:32m:27s remains)
2017-12-10 04:44:28.501768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42901 -4.4290051 -4.4290028 -4.4290018 -4.4290018 -4.4290028 -4.4290056 -4.4290056 -4.4290042 -4.4290023 -4.4290009 -4.4290032 -4.429008 -4.42901 -4.429008][-4.4290137 -4.4290056 -4.4289985 -4.4289918 -4.4289875 -4.4289861 -4.4289889 -4.428987 -4.4289813 -4.4289775 -4.4289789 -4.428988 -4.4289994 -4.4290071 -4.4290066][-4.4290075 -4.4289889 -4.4289703 -4.4289536 -4.4289432 -4.4289389 -4.4289432 -4.4289389 -4.4289303 -4.428926 -4.4289293 -4.4289427 -4.4289656 -4.4289889 -4.4289985][-4.4289889 -4.4289546 -4.4289222 -4.428895 -4.4288788 -4.4288712 -4.4288735 -4.4288588 -4.4288416 -4.4288287 -4.4288211 -4.42883 -4.4288678 -4.4289227 -4.4289589][-4.4289551 -4.4289055 -4.4288635 -4.4288249 -4.4288025 -4.4287958 -4.4287982 -4.4287686 -4.4287271 -4.4286909 -4.4286623 -4.4286695 -4.4287305 -4.4288244 -4.4288955][-4.4289193 -4.4288559 -4.4288044 -4.428751 -4.428719 -4.4287138 -4.4287214 -4.4286757 -4.4286027 -4.4285388 -4.4284916 -4.4285107 -4.4286032 -4.4287295 -4.4288316][-4.428875 -4.4288015 -4.4287357 -4.4286618 -4.4286189 -4.4286132 -4.4286251 -4.4285674 -4.4284811 -4.4284158 -4.4283772 -4.4284225 -4.4285393 -4.4286809 -4.4287963][-4.4288235 -4.4287343 -4.4286394 -4.4285212 -4.4284616 -4.4284692 -4.4284954 -4.428442 -4.4283762 -4.4283614 -4.428371 -4.4284353 -4.4285545 -4.4286909 -4.4288006][-4.4287653 -4.4286509 -4.428514 -4.4283319 -4.4282656 -4.4283085 -4.4283762 -4.428349 -4.4283247 -4.4283657 -4.4284182 -4.4284916 -4.4286022 -4.4287276 -4.4288259][-4.4287133 -4.42858 -4.4284143 -4.4281893 -4.4281378 -4.4282317 -4.4283514 -4.4283724 -4.4283872 -4.4284506 -4.4285173 -4.4285865 -4.4286785 -4.4287825 -4.4288597][-4.4287152 -4.4285917 -4.42846 -4.4282808 -4.4282565 -4.4283605 -4.4284811 -4.4285259 -4.42856 -4.4286146 -4.4286671 -4.4287171 -4.4287796 -4.4288497 -4.4288983][-4.428793 -4.4287047 -4.428637 -4.428545 -4.4285374 -4.428606 -4.4286919 -4.4287324 -4.4287615 -4.4287958 -4.4288235 -4.4288464 -4.4288764 -4.4289088 -4.4289322][-4.4288983 -4.428843 -4.4288144 -4.4287806 -4.4287791 -4.4288135 -4.4288673 -4.4288931 -4.4289088 -4.4289217 -4.4289293 -4.4289336 -4.4289412 -4.4289517 -4.4289589][-4.4289775 -4.4289517 -4.4289427 -4.4289303 -4.4289269 -4.4289346 -4.4289594 -4.4289727 -4.42898 -4.4289823 -4.4289823 -4.4289804 -4.4289794 -4.4289808 -4.42898][-4.4290104 -4.4290047 -4.4290066 -4.4290023 -4.4289951 -4.4289894 -4.4289927 -4.4289966 -4.4289985 -4.428998 -4.4289975 -4.4289961 -4.4289942 -4.4289923 -4.42899]]...]
INFO - root - 2017-12-10 04:44:30.959923: step 2010, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:30m:47s remains)
INFO - root - 2017-12-10 04:44:33.425916: step 2020, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:30m:13s remains)
INFO - root - 2017-12-10 04:44:35.896265: step 2030, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:26m:46s remains)
INFO - root - 2017-12-10 04:44:38.394581: step 2040, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:22m:27s remains)
INFO - root - 2017-12-10 04:44:40.880362: step 2050, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 23h:02m:14s remains)
INFO - root - 2017-12-10 04:44:43.335238: step 2060, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:38m:41s remains)
INFO - root - 2017-12-10 04:44:45.814269: step 2070, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:51m:58s remains)
INFO - root - 2017-12-10 04:44:48.286941: step 2080, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:21s remains)
INFO - root - 2017-12-10 04:44:50.751336: step 2090, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:38m:17s remains)
INFO - root - 2017-12-10 04:44:53.255155: step 2100, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:45m:14s remains)
2017-12-10 04:44:53.575961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287128 -4.4286976 -4.4287138 -4.4287477 -4.4287548 -4.4287424 -4.4287467 -4.428761 -4.4287753 -4.428793 -4.428823 -4.4288487 -4.4288616 -4.4288683 -4.428853][-4.4287243 -4.4287219 -4.4287453 -4.4287772 -4.4287844 -4.4287696 -4.4287734 -4.4287982 -4.4288321 -4.4288578 -4.4288726 -4.4288754 -4.4288759 -4.4288812 -4.4288678][-4.4287505 -4.4287443 -4.4287639 -4.4287863 -4.4287858 -4.4287686 -4.4287705 -4.4287953 -4.4288354 -4.42886 -4.42887 -4.42887 -4.4288673 -4.4288673 -4.4288573][-4.4287748 -4.4287548 -4.4287629 -4.42877 -4.4287524 -4.4287276 -4.4287291 -4.4287534 -4.4288077 -4.4288449 -4.4288583 -4.4288669 -4.4288664 -4.4288621 -4.4288459][-4.4287806 -4.4287539 -4.4287496 -4.4287376 -4.4286876 -4.4286375 -4.428628 -4.4286556 -4.4287319 -4.4287968 -4.4288325 -4.4288568 -4.4288678 -4.4288669 -4.4288507][-4.4287753 -4.4287553 -4.4287457 -4.4287133 -4.4286313 -4.4285312 -4.4284792 -4.4285011 -4.4286041 -4.4287109 -4.4287906 -4.42885 -4.428884 -4.4288921 -4.4288797][-4.4287996 -4.4287906 -4.4287806 -4.4287415 -4.4286418 -4.4284935 -4.4283743 -4.4283671 -4.4284887 -4.4286318 -4.4287505 -4.4288454 -4.4289031 -4.4289184 -4.428906][-4.4288216 -4.4288225 -4.4288096 -4.4287729 -4.4286833 -4.4285297 -4.4283829 -4.4283476 -4.42846 -4.4286056 -4.4287291 -4.4288359 -4.4289007 -4.4289126 -4.428896][-4.4288049 -4.4288116 -4.4287939 -4.4287658 -4.4287047 -4.4285746 -4.4284348 -4.4283891 -4.42848 -4.4286113 -4.4287224 -4.4288197 -4.4288783 -4.4288869 -4.4288597][-4.428781 -4.4287853 -4.4287658 -4.4287391 -4.4286976 -4.4285941 -4.4284782 -4.4284472 -4.4285288 -4.4286361 -4.4287243 -4.4287972 -4.4288492 -4.4288597 -4.4288278][-4.4287758 -4.428771 -4.4287438 -4.4287224 -4.4287024 -4.4286284 -4.4285421 -4.428525 -4.4285994 -4.4286838 -4.4287477 -4.4287858 -4.4288244 -4.42884 -4.4288149][-4.4287934 -4.428781 -4.4287534 -4.4287367 -4.4287338 -4.4286923 -4.4286375 -4.4286304 -4.428689 -4.4287505 -4.4287844 -4.4287834 -4.4287868 -4.4287987 -4.4287963][-4.4288197 -4.428813 -4.4287944 -4.4287796 -4.4287806 -4.42876 -4.4287281 -4.4287267 -4.4287691 -4.4288106 -4.4288182 -4.4287777 -4.42873 -4.4287243 -4.4287443][-4.4288192 -4.428813 -4.4288082 -4.4288073 -4.4288135 -4.4288077 -4.4287934 -4.4287963 -4.4288263 -4.4288521 -4.4288411 -4.4287772 -4.4286928 -4.4286652 -4.4286947][-4.4288058 -4.4287982 -4.4288025 -4.4288135 -4.4288225 -4.4288249 -4.4288268 -4.4288425 -4.4288688 -4.4288764 -4.4288483 -4.4287729 -4.4286771 -4.4286356 -4.4286647]]...]
INFO - root - 2017-12-10 04:44:56.015975: step 2110, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:14m:23s remains)
INFO - root - 2017-12-10 04:44:58.530545: step 2120, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:50m:03s remains)
INFO - root - 2017-12-10 04:45:01.021291: step 2130, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:52m:53s remains)
INFO - root - 2017-12-10 04:45:03.500535: step 2140, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:23m:52s remains)
INFO - root - 2017-12-10 04:45:06.012401: step 2150, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:19m:10s remains)
INFO - root - 2017-12-10 04:45:08.552592: step 2160, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:26m:40s remains)
INFO - root - 2017-12-10 04:45:11.007041: step 2170, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:37m:24s remains)
INFO - root - 2017-12-10 04:45:13.455439: step 2180, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:07m:10s remains)
INFO - root - 2017-12-10 04:45:15.921008: step 2190, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:45m:29s remains)
INFO - root - 2017-12-10 04:45:18.419261: step 2200, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:31m:04s remains)
2017-12-10 04:45:18.691542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287171 -4.428721 -4.4287887 -4.4288497 -4.4289012 -4.4289322 -4.4289432 -4.4289379 -4.4289408 -4.4289556 -4.4289722 -4.4289889 -4.4289994 -4.4290037 -4.4290023][-4.4286022 -4.4286075 -4.4287062 -4.4287949 -4.4288573 -4.4288926 -4.4289017 -4.428895 -4.4289045 -4.4289269 -4.4289536 -4.4289784 -4.4289918 -4.4289956 -4.4289927][-4.4284673 -4.4284673 -4.4286132 -4.4287434 -4.4288177 -4.428853 -4.4288564 -4.4288459 -4.4288645 -4.4288993 -4.4289384 -4.4289708 -4.4289865 -4.4289885 -4.4289837][-4.4283638 -4.4283633 -4.4285564 -4.4287224 -4.4287992 -4.4288182 -4.4288034 -4.4287872 -4.4288216 -4.4288754 -4.4289274 -4.4289656 -4.4289846 -4.4289856 -4.4289789][-4.4284134 -4.4284225 -4.4286017 -4.428751 -4.4288044 -4.4287872 -4.4287405 -4.4287176 -4.428771 -4.4288425 -4.4289093 -4.4289536 -4.4289784 -4.4289827 -4.4289775][-4.4285684 -4.4285831 -4.42869 -4.4287744 -4.4287887 -4.4287252 -4.4286332 -4.4286008 -4.4286876 -4.4287882 -4.4288764 -4.4289336 -4.4289656 -4.4289765 -4.4289775][-4.4286957 -4.4287033 -4.4287524 -4.4287791 -4.4287577 -4.4286418 -4.4284868 -4.4284449 -4.4285793 -4.4287257 -4.4288378 -4.4289117 -4.4289527 -4.4289689 -4.4289765][-4.428781 -4.4287891 -4.4288058 -4.4287925 -4.4287348 -4.4285636 -4.4283404 -4.4282827 -4.4284692 -4.428668 -4.4288063 -4.4288926 -4.42894 -4.4289575 -4.4289713][-4.4287949 -4.4288168 -4.4288311 -4.4288063 -4.4287353 -4.4285536 -4.4283252 -4.4282665 -4.4284592 -4.4286633 -4.4288011 -4.4288826 -4.4289246 -4.4289408 -4.4289603][-4.4287224 -4.4287682 -4.4288082 -4.4288011 -4.4287543 -4.428628 -4.4284763 -4.4284334 -4.4285626 -4.4287124 -4.428823 -4.4288874 -4.4289165 -4.4289279 -4.4289503][-4.4286056 -4.4286771 -4.4287605 -4.4287872 -4.4287734 -4.4287119 -4.4286256 -4.4285984 -4.4286776 -4.4287744 -4.4288564 -4.4289041 -4.4289193 -4.4289207 -4.4289403][-4.4284678 -4.4285631 -4.4287043 -4.42878 -4.4288049 -4.4287868 -4.4287386 -4.4287257 -4.4287767 -4.4288349 -4.42889 -4.42892 -4.4289222 -4.4289155 -4.4289289][-4.4284158 -4.4285169 -4.4286838 -4.4287863 -4.4288273 -4.4288173 -4.4287825 -4.4287829 -4.42883 -4.4288688 -4.4289126 -4.4289312 -4.4289293 -4.4289207 -4.4289269][-4.4285297 -4.4285989 -4.4287171 -4.4287906 -4.4288173 -4.4287777 -4.4287219 -4.4287238 -4.4287949 -4.4288535 -4.4289064 -4.42893 -4.4289312 -4.4289255 -4.4289284][-4.4286594 -4.4286947 -4.4287596 -4.4287944 -4.4287858 -4.4286914 -4.4285812 -4.4285736 -4.4286914 -4.428802 -4.4288754 -4.4289107 -4.4289165 -4.4289112 -4.428915]]...]
INFO - root - 2017-12-10 04:45:21.188928: step 2210, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:24m:35s remains)
INFO - root - 2017-12-10 04:45:23.679422: step 2220, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.250 sec/batch; 22h:53m:29s remains)
INFO - root - 2017-12-10 04:45:26.212346: step 2230, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:29m:40s remains)
INFO - root - 2017-12-10 04:45:28.716199: step 2240, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:15m:52s remains)
INFO - root - 2017-12-10 04:45:31.162500: step 2250, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:12s remains)
INFO - root - 2017-12-10 04:45:33.626871: step 2260, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:18m:42s remains)
INFO - root - 2017-12-10 04:45:36.098044: step 2270, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:21m:09s remains)
INFO - root - 2017-12-10 04:45:38.609203: step 2280, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:16m:41s remains)
INFO - root - 2017-12-10 04:45:41.103984: step 2290, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:39m:12s remains)
INFO - root - 2017-12-10 04:45:43.567640: step 2300, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:28m:05s remains)
2017-12-10 04:45:43.844212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288869 -4.4288688 -4.4288545 -4.4288292 -4.4287891 -4.4287539 -4.42872 -4.4286861 -4.428678 -4.4286695 -4.4286432 -4.4285946 -4.4285693 -4.4286075 -4.4286957][-4.4288912 -4.4288745 -4.4288497 -4.4288163 -4.4287715 -4.4287262 -4.4286733 -4.4286251 -4.4286041 -4.4285741 -4.4285541 -4.4285545 -4.4285827 -4.4286509 -4.4287481][-4.4288869 -4.4288583 -4.428813 -4.4287639 -4.4287186 -4.4286728 -4.4286118 -4.4285588 -4.4285197 -4.4284749 -4.42848 -4.4285383 -4.428616 -4.4287086 -4.4288087][-4.4288712 -4.4288306 -4.4287696 -4.4287105 -4.4286685 -4.428628 -4.428566 -4.4285097 -4.4284687 -4.4284415 -4.4284863 -4.4285789 -4.4286747 -4.4287629 -4.4288468][-4.4288492 -4.4288073 -4.4287491 -4.4286904 -4.4286413 -4.4285975 -4.4285364 -4.428472 -4.4284477 -4.428472 -4.4285541 -4.4286561 -4.4287477 -4.4288163 -4.42887][-4.4288144 -4.4287806 -4.4287395 -4.4286857 -4.4286237 -4.4285707 -4.4284887 -4.428381 -4.4283705 -4.4284821 -4.4286151 -4.4287229 -4.4288073 -4.4288659 -4.428895][-4.4287739 -4.4287558 -4.4287386 -4.4286909 -4.42861 -4.4285173 -4.4283772 -4.4282193 -4.4282537 -4.4284644 -4.4286404 -4.4287577 -4.4288449 -4.428896 -4.4289126][-4.42874 -4.4287238 -4.4287124 -4.4286628 -4.4285703 -4.4284573 -4.4283161 -4.4282041 -4.4282942 -4.4285254 -4.4286928 -4.4287887 -4.4288597 -4.4289026 -4.4289122][-4.4286809 -4.4286585 -4.4286556 -4.4286323 -4.4285789 -4.4285192 -4.4284582 -4.4284396 -4.428534 -4.4286814 -4.4287729 -4.4288263 -4.42887 -4.428895 -4.4289012][-4.4286 -4.4286108 -4.4286466 -4.4286594 -4.428647 -4.428637 -4.4286289 -4.4286509 -4.42872 -4.4287896 -4.4288292 -4.4288573 -4.4288797 -4.4288869 -4.428895][-4.428586 -4.4286337 -4.4286919 -4.428719 -4.4287214 -4.428721 -4.4287133 -4.4287357 -4.4287939 -4.4288359 -4.4288573 -4.4288797 -4.428895 -4.4288979 -4.42891][-4.428628 -4.4286847 -4.4287443 -4.4287677 -4.4287672 -4.4287519 -4.42873 -4.4287515 -4.4288044 -4.4288416 -4.4288621 -4.4288864 -4.4289021 -4.4289107 -4.4289284][-4.4286909 -4.4287319 -4.4287663 -4.428772 -4.4287691 -4.4287572 -4.4287462 -4.4287696 -4.4288106 -4.4288449 -4.4288654 -4.4288888 -4.42891 -4.4289293 -4.4289484][-4.428721 -4.4287362 -4.4287472 -4.4287429 -4.4287486 -4.4287605 -4.4287753 -4.4287987 -4.4288278 -4.4288516 -4.4288697 -4.428894 -4.4289217 -4.4289455 -4.4289632][-4.4287157 -4.4287305 -4.4287376 -4.4287376 -4.4287558 -4.4287863 -4.4288163 -4.4288368 -4.4288549 -4.4288735 -4.4288945 -4.4289179 -4.4289389 -4.4289584 -4.4289703]]...]
INFO - root - 2017-12-10 04:45:46.345938: step 2310, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:33m:53s remains)
INFO - root - 2017-12-10 04:45:48.813831: step 2320, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:26m:11s remains)
INFO - root - 2017-12-10 04:45:51.284409: step 2330, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:47m:37s remains)
INFO - root - 2017-12-10 04:45:53.799534: step 2340, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:21m:04s remains)
INFO - root - 2017-12-10 04:45:56.271298: step 2350, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:04m:29s remains)
INFO - root - 2017-12-10 04:45:58.778746: step 2360, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:14m:02s remains)
INFO - root - 2017-12-10 04:46:01.225542: step 2370, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:59m:23s remains)
INFO - root - 2017-12-10 04:46:03.693856: step 2380, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:51m:22s remains)
INFO - root - 2017-12-10 04:46:06.161889: step 2390, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:10m:16s remains)
INFO - root - 2017-12-10 04:46:08.628949: step 2400, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:21m:16s remains)
2017-12-10 04:46:08.920931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287143 -4.4287224 -4.4287214 -4.4287252 -4.4287677 -4.428822 -4.4288363 -4.4288249 -4.4288359 -4.4288564 -4.428865 -4.4288807 -4.4288945 -4.4289064 -4.4289193][-4.4287024 -4.4287224 -4.4287271 -4.4287353 -4.4287848 -4.42884 -4.4288464 -4.4288249 -4.4288287 -4.4288392 -4.428843 -4.428864 -4.4288907 -4.42891 -4.4289188][-4.4286709 -4.4287105 -4.4287348 -4.4287553 -4.4288015 -4.4288335 -4.4288239 -4.4287953 -4.4287963 -4.4287987 -4.4287982 -4.4288254 -4.4288645 -4.4288921 -4.4289031][-4.4286785 -4.428719 -4.4287524 -4.4287748 -4.4287992 -4.4288054 -4.4287829 -4.4287567 -4.4287591 -4.428762 -4.428762 -4.4287887 -4.4288311 -4.4288645 -4.4288826][-4.4287391 -4.4287682 -4.4287877 -4.4287877 -4.4287844 -4.428761 -4.4287205 -4.4286876 -4.4286933 -4.4287057 -4.4287214 -4.4287543 -4.4287949 -4.4288316 -4.4288616][-4.4287753 -4.428793 -4.4287958 -4.4287686 -4.428731 -4.4286752 -4.4286108 -4.428565 -4.4285812 -4.4286232 -4.4286652 -4.4287128 -4.4287581 -4.4288011 -4.42884][-4.428751 -4.4287629 -4.42875 -4.4287043 -4.428638 -4.4285469 -4.4284372 -4.4283566 -4.4283824 -4.42847 -4.4285483 -4.4286246 -4.4286942 -4.4287543 -4.4288077][-4.4286866 -4.42869 -4.4286671 -4.428617 -4.4285469 -4.4284372 -4.4282904 -4.428164 -4.428184 -4.4282942 -4.4283981 -4.4285092 -4.4286146 -4.4287024 -4.4287748][-4.4286623 -4.4286633 -4.4286494 -4.4286189 -4.4285779 -4.4284992 -4.4283648 -4.4282231 -4.4281988 -4.4282665 -4.4283519 -4.4284663 -4.4285865 -4.4286857 -4.4287667][-4.4287081 -4.4287057 -4.4287071 -4.4287047 -4.428699 -4.4286652 -4.4285769 -4.428462 -4.4284115 -4.42842 -4.4284573 -4.4285383 -4.4286375 -4.4287224 -4.4287915][-4.4288082 -4.428803 -4.4288063 -4.4288154 -4.428822 -4.4288154 -4.4287682 -4.4286919 -4.4286423 -4.4286251 -4.4286327 -4.4286814 -4.4287457 -4.4288039 -4.4288516][-4.4288898 -4.4288859 -4.4288907 -4.4288988 -4.4289055 -4.4289103 -4.4288945 -4.4288588 -4.4288311 -4.4288173 -4.4288139 -4.428834 -4.4288611 -4.4288893 -4.4289145][-4.4289308 -4.42893 -4.4289351 -4.4289408 -4.4289489 -4.42896 -4.4289622 -4.4289541 -4.428946 -4.4289374 -4.4289308 -4.4289351 -4.4289389 -4.4289432 -4.4289532][-4.4289618 -4.4289618 -4.4289637 -4.4289675 -4.4289765 -4.4289885 -4.4289961 -4.428997 -4.4289951 -4.4289885 -4.4289827 -4.4289832 -4.4289808 -4.4289742 -4.4289722][-4.4289856 -4.4289846 -4.4289846 -4.428987 -4.4289951 -4.4290032 -4.429009 -4.4290113 -4.4290109 -4.429008 -4.4290051 -4.4290061 -4.4290032 -4.4289947 -4.428987]]...]
INFO - root - 2017-12-10 04:46:11.357625: step 2410, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:38m:33s remains)
INFO - root - 2017-12-10 04:46:13.805105: step 2420, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:38m:08s remains)
INFO - root - 2017-12-10 04:46:16.275269: step 2430, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:35m:17s remains)
INFO - root - 2017-12-10 04:46:18.747342: step 2440, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:15m:27s remains)
INFO - root - 2017-12-10 04:46:21.292266: step 2450, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:55m:54s remains)
INFO - root - 2017-12-10 04:46:23.769417: step 2460, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 22h:02m:21s remains)
INFO - root - 2017-12-10 04:46:26.228398: step 2470, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:40m:56s remains)
INFO - root - 2017-12-10 04:46:28.686015: step 2480, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:28m:03s remains)
INFO - root - 2017-12-10 04:46:31.151867: step 2490, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:23m:36s remains)
INFO - root - 2017-12-10 04:46:33.672895: step 2500, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:48m:10s remains)
2017-12-10 04:46:33.955327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289274 -4.4289093 -4.4288931 -4.4288836 -4.4288821 -4.4288826 -4.4288754 -4.4288712 -4.4288783 -4.4288826 -4.4288821 -4.428875 -4.4288735 -4.42888 -4.4288936][-4.4289169 -4.4288859 -4.4288621 -4.4288478 -4.4288445 -4.4288383 -4.4288239 -4.4288249 -4.4288459 -4.4288611 -4.4288616 -4.4288473 -4.4288359 -4.4288359 -4.4288492][-4.4288898 -4.4288406 -4.4288044 -4.4287815 -4.4287744 -4.428762 -4.4287395 -4.4287462 -4.4287844 -4.4288154 -4.428823 -4.4288077 -4.4287891 -4.4287825 -4.4287939][-4.4288492 -4.4287834 -4.4287367 -4.4287066 -4.4286928 -4.4286637 -4.4286261 -4.4286432 -4.4287062 -4.4287515 -4.4287648 -4.4287534 -4.4287329 -4.428721 -4.4287319][-4.4288135 -4.4287419 -4.4286828 -4.4286427 -4.4286141 -4.4285636 -4.428503 -4.4285126 -4.4285975 -4.4286656 -4.4286928 -4.4286952 -4.4286871 -4.4286785 -4.42869][-4.4287877 -4.4287219 -4.4286509 -4.4285846 -4.42853 -4.4284577 -4.4283566 -4.4283376 -4.4284382 -4.4285455 -4.428607 -4.4286408 -4.4286566 -4.4286675 -4.4286857][-4.4287772 -4.4287267 -4.4286575 -4.4285626 -4.4284706 -4.4283667 -4.4282165 -4.4281354 -4.4282441 -4.4284134 -4.4285192 -4.4285851 -4.42862 -4.4286523 -4.4286833][-4.4287863 -4.4287496 -4.428689 -4.42859 -4.428483 -4.4283705 -4.4282069 -4.4280987 -4.428184 -4.4283528 -4.4284625 -4.4285278 -4.428565 -4.4286141 -4.4286666][-4.4288077 -4.42878 -4.4287305 -4.4286356 -4.4285359 -4.4284339 -4.4283013 -4.4282193 -4.4282756 -4.4283791 -4.4284434 -4.4284821 -4.4285111 -4.4285631 -4.4286342][-4.4288068 -4.4287777 -4.4287419 -4.4286723 -4.4285879 -4.428503 -4.4284158 -4.4283557 -4.4283781 -4.4284182 -4.4284363 -4.4284539 -4.4284725 -4.4285178 -4.4285965][-4.428791 -4.4287477 -4.4287167 -4.4286785 -4.4286222 -4.4285603 -4.4285026 -4.4284663 -4.4284697 -4.428462 -4.4284449 -4.4284515 -4.4284778 -4.4285212 -4.4285913][-4.4288168 -4.4287734 -4.4287386 -4.4287171 -4.4286795 -4.4286423 -4.4286122 -4.4286 -4.4286032 -4.4285827 -4.4285522 -4.4285483 -4.4285774 -4.4286137 -4.4286618][-4.42887 -4.4288378 -4.4288054 -4.4287848 -4.4287572 -4.42874 -4.4287333 -4.4287348 -4.4287448 -4.428731 -4.4286995 -4.4286962 -4.428721 -4.4287481 -4.4287772][-4.4289165 -4.4289 -4.4288769 -4.42886 -4.4288425 -4.4288421 -4.4288473 -4.4288573 -4.4288712 -4.4288669 -4.428844 -4.4288406 -4.4288554 -4.42887 -4.4288812][-4.4289355 -4.4289327 -4.4289217 -4.42891 -4.4289031 -4.4289069 -4.4289126 -4.4289265 -4.4289403 -4.4289417 -4.4289293 -4.428925 -4.4289317 -4.4289384 -4.4289436]]...]
INFO - root - 2017-12-10 04:46:36.410928: step 2510, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:17m:39s remains)
INFO - root - 2017-12-10 04:46:38.857017: step 2520, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:29m:36s remains)
INFO - root - 2017-12-10 04:46:41.307219: step 2530, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:25m:35s remains)
INFO - root - 2017-12-10 04:46:43.776153: step 2540, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:10m:50s remains)
INFO - root - 2017-12-10 04:46:46.227289: step 2550, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:43m:07s remains)
INFO - root - 2017-12-10 04:46:48.697213: step 2560, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:56m:16s remains)
INFO - root - 2017-12-10 04:46:51.169152: step 2570, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:27m:20s remains)
INFO - root - 2017-12-10 04:46:53.631404: step 2580, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 22h:07m:44s remains)
INFO - root - 2017-12-10 04:46:56.143933: step 2590, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:12m:12s remains)
INFO - root - 2017-12-10 04:46:58.633393: step 2600, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:25m:16s remains)
2017-12-10 04:46:58.914875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287834 -4.4287944 -4.4288092 -4.4288192 -4.4288282 -4.4288216 -4.4287934 -4.428751 -4.42871 -4.4286871 -4.4286709 -4.42866 -4.4286628 -4.4286737 -4.4286981][-4.4287333 -4.4287429 -4.4287639 -4.4287872 -4.4288111 -4.4288168 -4.4287934 -4.4287415 -4.42868 -4.4286351 -4.4286003 -4.4285893 -4.4286022 -4.4286337 -4.4286866][-4.428699 -4.4287171 -4.4287524 -4.4287782 -4.4287972 -4.4287925 -4.42876 -4.4287028 -4.4286337 -4.42858 -4.4285378 -4.4285254 -4.4285455 -4.4285941 -4.4286723][-4.4286885 -4.4287138 -4.4287586 -4.428782 -4.4287834 -4.4287558 -4.4287076 -4.4286504 -4.42859 -4.4285426 -4.4285016 -4.4284821 -4.428494 -4.428544 -4.4286342][-4.4286985 -4.4287224 -4.4287634 -4.4287815 -4.4287691 -4.4287238 -4.4286551 -4.4285941 -4.4285507 -4.428525 -4.428494 -4.4284692 -4.4284706 -4.4285107 -4.4286013][-4.4287224 -4.4287415 -4.4287748 -4.428781 -4.4287491 -4.4286823 -4.4285932 -4.42852 -4.4284925 -4.4284992 -4.4284868 -4.4284658 -4.4284673 -4.428504 -4.4285975][-4.4287395 -4.4287515 -4.4287753 -4.4287705 -4.4287238 -4.4286385 -4.4285278 -4.428441 -4.4284244 -4.428463 -4.4284825 -4.4284844 -4.428503 -4.4285493 -4.428638][-4.428751 -4.4287643 -4.4287772 -4.4287653 -4.4287176 -4.4286337 -4.4285197 -4.4284339 -4.4284272 -4.4284835 -4.4285312 -4.4285526 -4.4285769 -4.4286132 -4.4286761][-4.4287734 -4.4287953 -4.428793 -4.4287686 -4.4287248 -4.4286585 -4.4285688 -4.4285054 -4.4285054 -4.42856 -4.4286184 -4.428648 -4.4286652 -4.4286809 -4.4287105][-4.4287782 -4.4288087 -4.4288015 -4.4287748 -4.4287405 -4.4286952 -4.428638 -4.4286032 -4.4286146 -4.4286609 -4.4287086 -4.4287271 -4.4287319 -4.4287348 -4.4287443][-4.4287658 -4.4287953 -4.4287953 -4.4287858 -4.4287686 -4.4287395 -4.4287038 -4.4286933 -4.4287171 -4.4287519 -4.4287744 -4.4287739 -4.4287677 -4.4287643 -4.4287739][-4.4287672 -4.4287972 -4.4288135 -4.4288292 -4.4288282 -4.4288058 -4.4287796 -4.4287744 -4.4287963 -4.4288187 -4.42882 -4.4288058 -4.428793 -4.4287891 -4.428802][-4.4287896 -4.4288206 -4.4288507 -4.4288788 -4.4288898 -4.4288774 -4.4288597 -4.4288545 -4.4288664 -4.428874 -4.4288626 -4.4288421 -4.4288263 -4.428822 -4.4288344][-4.4288235 -4.4288468 -4.4288759 -4.4289012 -4.4289145 -4.4289126 -4.4289041 -4.4289012 -4.4289069 -4.4289083 -4.4288964 -4.4288788 -4.4288664 -4.428864 -4.4288764][-4.4288793 -4.4288878 -4.4289079 -4.428926 -4.4289379 -4.4289389 -4.4289355 -4.428936 -4.4289403 -4.4289432 -4.4289374 -4.4289279 -4.4289184 -4.4289155 -4.428925]]...]
INFO - root - 2017-12-10 04:47:01.368845: step 2610, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:35m:29s remains)
INFO - root - 2017-12-10 04:47:03.858340: step 2620, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:28m:39s remains)
INFO - root - 2017-12-10 04:47:06.323391: step 2630, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.238 sec/batch; 21h:51m:03s remains)
INFO - root - 2017-12-10 04:47:08.843222: step 2640, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:50m:18s remains)
INFO - root - 2017-12-10 04:47:11.326322: step 2650, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:12m:18s remains)
INFO - root - 2017-12-10 04:47:13.798534: step 2660, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:03m:43s remains)
INFO - root - 2017-12-10 04:47:16.256779: step 2670, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:29m:16s remains)
INFO - root - 2017-12-10 04:47:18.715398: step 2680, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:18m:16s remains)
INFO - root - 2017-12-10 04:47:21.177926: step 2690, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:19m:15s remains)
INFO - root - 2017-12-10 04:47:23.687550: step 2700, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:54m:19s remains)
2017-12-10 04:47:23.959202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.4287891 -4.4288135 -4.4288416 -4.4288597 -4.4288654 -4.4288731 -4.42888 -4.42888 -4.4288745 -4.4288688 -4.4288678 -4.4288697 -4.4288726 -4.428874][-4.4287958 -4.4288015 -4.4288206 -4.4288478 -4.4288654 -4.4288707 -4.428875 -4.4288788 -4.428874 -4.4288654 -4.4288583 -4.4288545 -4.4288545 -4.4288573 -4.4288592][-4.428854 -4.4288516 -4.4288559 -4.4288683 -4.428874 -4.4288692 -4.4288645 -4.4288669 -4.4288678 -4.4288654 -4.4288621 -4.4288621 -4.4288664 -4.42887 -4.4288731][-4.4288859 -4.4288807 -4.4288759 -4.428875 -4.428865 -4.4288473 -4.4288387 -4.4288487 -4.4288607 -4.4288664 -4.4288659 -4.4288669 -4.4288731 -4.4288764 -4.4288774][-4.4288917 -4.4288883 -4.428875 -4.4288621 -4.4288425 -4.4288206 -4.4288125 -4.42883 -4.428854 -4.428865 -4.4288626 -4.4288616 -4.4288659 -4.4288616 -4.4288511][-4.4288626 -4.4288526 -4.428833 -4.42881 -4.4287786 -4.4287477 -4.4287386 -4.4287667 -4.4288054 -4.4288316 -4.428843 -4.4288545 -4.4288692 -4.4288611 -4.4288354][-4.4287944 -4.4287739 -4.4287491 -4.4287167 -4.4286757 -4.428637 -4.4286246 -4.4286575 -4.4287143 -4.4287658 -4.4288 -4.4288316 -4.4288611 -4.4288597 -4.4288263][-4.4286819 -4.4286489 -4.4286141 -4.4285741 -4.4285288 -4.4284797 -4.428452 -4.428483 -4.4285574 -4.4286385 -4.4286995 -4.4287477 -4.4287829 -4.4287829 -4.4287534][-4.4286041 -4.4285693 -4.4285345 -4.4284945 -4.4284425 -4.4283781 -4.428328 -4.4283433 -4.4284205 -4.4285178 -4.4286013 -4.4286637 -4.4286985 -4.4286942 -4.42867][-4.42866 -4.4286342 -4.428617 -4.428597 -4.4285622 -4.4285131 -4.4284668 -4.428473 -4.4285321 -4.4286103 -4.4286852 -4.4287376 -4.4287591 -4.4287472 -4.4287248][-4.4287696 -4.4287524 -4.4287467 -4.428741 -4.4287271 -4.4287014 -4.4286771 -4.4286852 -4.428719 -4.4287648 -4.4288096 -4.4288421 -4.428854 -4.4288459 -4.4288378][-4.4288244 -4.428813 -4.42881 -4.428802 -4.4287925 -4.4287815 -4.4287739 -4.4287844 -4.4288054 -4.42883 -4.4288507 -4.4288659 -4.428875 -4.4288797 -4.428884][-4.4288225 -4.42881 -4.4287992 -4.4287791 -4.4287663 -4.4287643 -4.4287667 -4.4287786 -4.4287944 -4.428812 -4.4288182 -4.4288211 -4.4288259 -4.4288359 -4.4288406][-4.4287987 -4.4287806 -4.4287572 -4.4287267 -4.4287133 -4.428719 -4.4287257 -4.4287324 -4.4287405 -4.4287505 -4.4287505 -4.4287534 -4.428771 -4.4287949 -4.428812][-4.4288106 -4.4287868 -4.4287577 -4.4287281 -4.4287195 -4.4287267 -4.4287319 -4.4287357 -4.4287405 -4.4287467 -4.4287457 -4.4287505 -4.4287758 -4.4288092 -4.4288397]]...]
INFO - root - 2017-12-10 04:47:26.449962: step 2710, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:38m:54s remains)
INFO - root - 2017-12-10 04:47:28.923990: step 2720, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:23m:02s remains)
INFO - root - 2017-12-10 04:47:31.369101: step 2730, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:16m:01s remains)
INFO - root - 2017-12-10 04:47:33.858284: step 2740, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:12m:26s remains)
INFO - root - 2017-12-10 04:47:36.330152: step 2750, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:46m:55s remains)
INFO - root - 2017-12-10 04:47:38.801911: step 2760, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:16m:32s remains)
INFO - root - 2017-12-10 04:47:41.245299: step 2770, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:19m:42s remains)
INFO - root - 2017-12-10 04:47:43.711898: step 2780, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:26m:30s remains)
INFO - root - 2017-12-10 04:47:46.169416: step 2790, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 22h:13m:45s remains)
INFO - root - 2017-12-10 04:47:48.638549: step 2800, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:28m:33s remains)
2017-12-10 04:47:48.928725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289217 -4.4289031 -4.428895 -4.4288979 -4.4288979 -4.428905 -4.4289012 -4.428874 -4.4288349 -4.428813 -4.4288049 -4.4288092 -4.428823 -4.428843 -4.428865][-4.4289412 -4.4289222 -4.4289103 -4.4289055 -4.4288969 -4.428894 -4.4288869 -4.4288616 -4.4288268 -4.4288049 -4.4287992 -4.4288025 -4.4288135 -4.428834 -4.4288592][-4.42891 -4.4288945 -4.4288888 -4.4288869 -4.4288869 -4.4288869 -4.4288783 -4.428853 -4.4288259 -4.4288106 -4.4288015 -4.4287972 -4.4288077 -4.4288368 -4.4288697][-4.4288611 -4.4288521 -4.4288578 -4.4288712 -4.4288855 -4.4288859 -4.4288683 -4.4288416 -4.4288263 -4.4288244 -4.4288211 -4.4288187 -4.4288254 -4.4288459 -4.4288716][-4.4288297 -4.4288254 -4.4288392 -4.4288588 -4.4288769 -4.428875 -4.4288468 -4.4288177 -4.4288092 -4.4288158 -4.4288259 -4.4288321 -4.4288411 -4.4288521 -4.428865][-4.4287987 -4.4287868 -4.42879 -4.4288034 -4.4288197 -4.4288158 -4.4287786 -4.4287376 -4.4287271 -4.4287477 -4.4287696 -4.4287806 -4.4287977 -4.4288144 -4.428823][-4.4287319 -4.428709 -4.4286976 -4.4287057 -4.4287214 -4.4287167 -4.428668 -4.4285979 -4.4285789 -4.4286175 -4.4286566 -4.4286804 -4.4287133 -4.4287481 -4.4287648][-4.4286742 -4.4286556 -4.4286423 -4.4286518 -4.4286723 -4.42867 -4.428606 -4.4285069 -4.4284692 -4.4285159 -4.4285731 -4.42862 -4.4286714 -4.4287186 -4.4287462][-4.4286804 -4.4286728 -4.4286675 -4.4286823 -4.4287095 -4.4287181 -4.4286718 -4.4285927 -4.4285507 -4.4285755 -4.4286251 -4.4286785 -4.4287333 -4.4287767 -4.4288049][-4.4287372 -4.4287357 -4.4287391 -4.4287667 -4.4287958 -4.4288058 -4.4287772 -4.4287333 -4.4287004 -4.4287033 -4.4287324 -4.4287772 -4.4288235 -4.4288545 -4.4288683][-4.4287992 -4.4287891 -4.4287906 -4.4288154 -4.4288335 -4.4288321 -4.4288054 -4.4287748 -4.4287515 -4.4287558 -4.4287839 -4.4288282 -4.4288707 -4.4288964 -4.4289041][-4.4288406 -4.4288206 -4.4288135 -4.4288273 -4.428834 -4.4288192 -4.4287839 -4.4287586 -4.42874 -4.4287477 -4.4287767 -4.4288187 -4.4288588 -4.4288836 -4.4288931][-4.4288425 -4.4288163 -4.4288025 -4.4288106 -4.428822 -4.4288082 -4.4287734 -4.4287529 -4.4287386 -4.4287419 -4.4287591 -4.4287887 -4.4288173 -4.4288325 -4.4288392][-4.4288187 -4.4287982 -4.42879 -4.4288077 -4.4288306 -4.4288244 -4.4287882 -4.4287615 -4.4287448 -4.4287376 -4.4287434 -4.4287648 -4.4287834 -4.4287887 -4.4288][-4.4287877 -4.4287848 -4.4287882 -4.4288077 -4.4288359 -4.4288387 -4.4288049 -4.4287806 -4.4287686 -4.4287591 -4.4287567 -4.4287686 -4.4287786 -4.4287758 -4.4287887]]...]
INFO - root - 2017-12-10 04:47:51.408918: step 2810, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:29m:42s remains)
INFO - root - 2017-12-10 04:47:53.898209: step 2820, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:02m:09s remains)
INFO - root - 2017-12-10 04:47:56.411238: step 2830, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:25m:47s remains)
INFO - root - 2017-12-10 04:47:58.898602: step 2840, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:36m:54s remains)
INFO - root - 2017-12-10 04:48:01.361264: step 2850, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:16m:16s remains)
INFO - root - 2017-12-10 04:48:03.813090: step 2860, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:09m:40s remains)
INFO - root - 2017-12-10 04:48:06.262539: step 2870, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:45m:24s remains)
INFO - root - 2017-12-10 04:48:08.728989: step 2880, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:15m:48s remains)
INFO - root - 2017-12-10 04:48:11.214163: step 2890, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:50m:50s remains)
INFO - root - 2017-12-10 04:48:13.716114: step 2900, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 23h:01m:00s remains)
2017-12-10 04:48:14.001298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288764 -4.4288993 -4.428915 -4.4289231 -4.4289064 -4.4288659 -4.42882 -4.4287858 -4.4287653 -4.4287772 -4.4288182 -4.4288659 -4.428895 -4.4288888 -4.4288435][-4.4288635 -4.4288669 -4.428864 -4.4288635 -4.4288578 -4.4288406 -4.4288125 -4.4287844 -4.4287572 -4.4287667 -4.4288154 -4.4288707 -4.428906 -4.4289074 -4.4288707][-4.4288583 -4.4288387 -4.4288163 -4.4288177 -4.4288378 -4.4288545 -4.4288473 -4.4288249 -4.4287868 -4.4287786 -4.4288163 -4.4288683 -4.4289074 -4.4289155 -4.42889][-4.4288573 -4.4288263 -4.4287987 -4.4288082 -4.4288478 -4.4288836 -4.4288921 -4.4288707 -4.4288211 -4.4287944 -4.4288168 -4.4288578 -4.4288921 -4.4289017 -4.4288855][-4.4288521 -4.4288273 -4.4288082 -4.4288268 -4.4288716 -4.4289093 -4.4289184 -4.4288931 -4.4288387 -4.428802 -4.4288106 -4.4288359 -4.4288573 -4.4288635 -4.4288511][-4.4288497 -4.4288406 -4.4288387 -4.4288635 -4.4289026 -4.4289322 -4.428936 -4.4289107 -4.4288611 -4.4288254 -4.4288216 -4.4288287 -4.4288297 -4.4288216 -4.4288025][-4.4288516 -4.4288621 -4.4288774 -4.4289036 -4.4289322 -4.4289474 -4.4289436 -4.4289227 -4.4288874 -4.4288683 -4.428865 -4.42886 -4.4288411 -4.4288163 -4.4287891][-4.4288526 -4.4288821 -4.4289079 -4.4289279 -4.4289389 -4.4289374 -4.4289269 -4.4289145 -4.4289002 -4.4289021 -4.4289088 -4.4289017 -4.4288769 -4.4288492 -4.4288244][-4.4288497 -4.428896 -4.4289231 -4.4289284 -4.4289179 -4.4289026 -4.4288898 -4.428884 -4.428885 -4.4289002 -4.4289174 -4.4289155 -4.428896 -4.4288774 -4.4288673][-4.4288359 -4.4288931 -4.4289155 -4.4289036 -4.4288745 -4.428853 -4.4288406 -4.4288387 -4.4288473 -4.4288712 -4.4288945 -4.4288983 -4.4288869 -4.42888 -4.4288888][-4.4288206 -4.4288831 -4.4289002 -4.4288778 -4.4288416 -4.4288177 -4.4288077 -4.42881 -4.4288173 -4.4288344 -4.4288511 -4.4288464 -4.4288335 -4.4288363 -4.428865][-4.4288263 -4.428885 -4.4288979 -4.4288731 -4.4288368 -4.4288116 -4.428803 -4.4288054 -4.4288087 -4.4288135 -4.4288144 -4.4287949 -4.428771 -4.4287739 -4.4288135][-4.4288583 -4.428906 -4.4289131 -4.4288912 -4.4288578 -4.4288316 -4.4288235 -4.4288263 -4.4288249 -4.4288225 -4.4288096 -4.4287744 -4.4287348 -4.4287305 -4.4287658][-4.4288964 -4.4289265 -4.4289269 -4.4289064 -4.4288783 -4.4288535 -4.4288468 -4.4288497 -4.4288521 -4.4288564 -4.4288392 -4.4287896 -4.4287386 -4.4287257 -4.4287448][-4.4289203 -4.4289303 -4.4289274 -4.4289136 -4.4288921 -4.4288712 -4.4288607 -4.4288607 -4.4288692 -4.4288888 -4.4288788 -4.428823 -4.42877 -4.428751 -4.4287486]]...]
INFO - root - 2017-12-10 04:48:16.461202: step 2910, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 22h:06m:17s remains)
INFO - root - 2017-12-10 04:48:18.910190: step 2920, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:34m:39s remains)
INFO - root - 2017-12-10 04:48:21.402759: step 2930, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:06m:03s remains)
INFO - root - 2017-12-10 04:48:23.869540: step 2940, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:49m:48s remains)
INFO - root - 2017-12-10 04:48:26.319848: step 2950, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:23m:12s remains)
INFO - root - 2017-12-10 04:48:28.801603: step 2960, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:19m:15s remains)
INFO - root - 2017-12-10 04:48:31.269731: step 2970, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:57m:52s remains)
INFO - root - 2017-12-10 04:48:33.741613: step 2980, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:43m:09s remains)
INFO - root - 2017-12-10 04:48:36.224526: step 2990, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:41m:39s remains)
INFO - root - 2017-12-10 04:48:38.714044: step 3000, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:15m:25s remains)
2017-12-10 04:48:38.989860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288135 -4.4287767 -4.4287543 -4.4287591 -4.4287834 -4.4288077 -4.4288287 -4.4288359 -4.4288182 -4.4287958 -4.4287863 -4.4287939 -4.4288063 -4.42883 -4.4288607][-4.4287629 -4.4287157 -4.4286876 -4.4286928 -4.4287233 -4.4287553 -4.4287887 -4.4288 -4.4287863 -4.4287577 -4.4287353 -4.4287262 -4.4287329 -4.4287539 -4.4287782][-4.428721 -4.4286685 -4.4286413 -4.4286504 -4.4286885 -4.4287248 -4.4287558 -4.4287639 -4.4287519 -4.4287171 -4.4286733 -4.4286475 -4.4286542 -4.4286785 -4.4287076][-4.4286819 -4.428627 -4.4286089 -4.4286265 -4.4286714 -4.4287066 -4.4287252 -4.4287214 -4.428709 -4.4286628 -4.428596 -4.4285622 -4.4285793 -4.4286141 -4.42865][-4.4286475 -4.4285851 -4.4285626 -4.4285765 -4.42861 -4.4286346 -4.4286423 -4.42864 -4.4286356 -4.4285803 -4.4284968 -4.4284649 -4.428504 -4.4285607 -4.4286132][-4.428607 -4.4285316 -4.4284954 -4.4284897 -4.4284973 -4.428503 -4.4285045 -4.4285164 -4.4285336 -4.4284821 -4.4283957 -4.4283705 -4.4284253 -4.428503 -4.4285703][-4.4285612 -4.4284658 -4.4284086 -4.4283772 -4.4283557 -4.4283333 -4.4283333 -4.4283681 -4.4284115 -4.4283886 -4.4283218 -4.4283104 -4.4283705 -4.4284577 -4.4285297][-4.4285212 -4.42841 -4.42833 -4.4282737 -4.428225 -4.4281859 -4.4281874 -4.4282432 -4.4283113 -4.4283228 -4.4282861 -4.4282789 -4.4283242 -4.4284043 -4.4284806][-4.4284887 -4.4283714 -4.4282784 -4.4282064 -4.4281521 -4.4281158 -4.4281321 -4.4282007 -4.4282761 -4.4283051 -4.4282904 -4.4282846 -4.4283161 -4.428381 -4.4284563][-4.4285111 -4.4284024 -4.4283185 -4.4282517 -4.4282017 -4.4281754 -4.4282002 -4.4282594 -4.4283137 -4.4283342 -4.4283342 -4.4283428 -4.4283814 -4.428443 -4.4285197][-4.4286137 -4.4285288 -4.4284635 -4.4284077 -4.4283648 -4.428349 -4.4283748 -4.42842 -4.4284549 -4.4284663 -4.4284687 -4.4284854 -4.4285278 -4.4285836 -4.4286504][-4.4287496 -4.4286957 -4.4286561 -4.4286165 -4.4285846 -4.4285731 -4.4285917 -4.4286237 -4.4286485 -4.428658 -4.4286618 -4.4286733 -4.4287086 -4.4287524 -4.4288015][-4.4288769 -4.4288521 -4.428834 -4.4288125 -4.4287958 -4.428791 -4.4287996 -4.4288168 -4.4288311 -4.4288373 -4.4288406 -4.4288464 -4.4288669 -4.428895 -4.4289255][-4.42897 -4.4289646 -4.4289622 -4.4289556 -4.4289484 -4.4289465 -4.4289479 -4.4289517 -4.428956 -4.4289589 -4.42896 -4.4289613 -4.428968 -4.4289808 -4.4289966][-4.4290214 -4.4290233 -4.4290233 -4.4290204 -4.4290166 -4.4290147 -4.4290152 -4.4290161 -4.429018 -4.4290185 -4.4290175 -4.4290166 -4.4290171 -4.4290214 -4.4290271]]...]
INFO - root - 2017-12-10 04:48:41.437107: step 3010, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:20m:47s remains)
INFO - root - 2017-12-10 04:48:43.929641: step 3020, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:25m:05s remains)
INFO - root - 2017-12-10 04:48:46.405853: step 3030, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.254 sec/batch; 23h:12m:04s remains)
INFO - root - 2017-12-10 04:48:48.868492: step 3040, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:07m:52s remains)
INFO - root - 2017-12-10 04:48:51.339192: step 3050, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:19m:36s remains)
INFO - root - 2017-12-10 04:48:53.802223: step 3060, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:07m:37s remains)
INFO - root - 2017-12-10 04:48:56.277193: step 3070, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:58m:55s remains)
INFO - root - 2017-12-10 04:48:58.718268: step 3080, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:53m:31s remains)
INFO - root - 2017-12-10 04:49:01.195685: step 3090, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:45m:34s remains)
INFO - root - 2017-12-10 04:49:03.702818: step 3100, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:33m:52s remains)
2017-12-10 04:49:04.007188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289532 -4.4289513 -4.4289479 -4.4289412 -4.4289374 -4.428936 -4.4289374 -4.4289374 -4.4289384 -4.4289451 -4.428947 -4.4289446 -4.4289432 -4.4289427 -4.4289393][-4.4289565 -4.4289565 -4.4289513 -4.4289412 -4.4289346 -4.4289284 -4.4289236 -4.4289217 -4.4289231 -4.4289336 -4.428937 -4.4289351 -4.4289293 -4.4289269 -4.4289212][-4.4289479 -4.4289494 -4.4289403 -4.428926 -4.4289141 -4.4288993 -4.4288878 -4.428885 -4.4288869 -4.4289031 -4.4289083 -4.4289007 -4.4288874 -4.4288769 -4.4288654][-4.4289293 -4.42893 -4.4289141 -4.4288907 -4.4288683 -4.42884 -4.4288154 -4.4288073 -4.4288163 -4.4288406 -4.4288383 -4.4288158 -4.4287953 -4.4287715 -4.4287477][-4.4288974 -4.4288979 -4.4288783 -4.4288445 -4.4288077 -4.4287586 -4.4287066 -4.4286809 -4.4287019 -4.4287372 -4.4287219 -4.4286828 -4.4286566 -4.4286256 -4.4285913][-4.4288435 -4.428844 -4.4288125 -4.42876 -4.4287028 -4.4286227 -4.4285259 -4.4284706 -4.4285212 -4.4285879 -4.4285803 -4.4285522 -4.428545 -4.4285331 -4.4285083][-4.4288 -4.4287906 -4.4287343 -4.4286509 -4.4285588 -4.4284348 -4.4282742 -4.4281697 -4.4282594 -4.4283924 -4.4284329 -4.4284477 -4.4284906 -4.4285359 -4.4285488][-4.4288044 -4.4287834 -4.4287038 -4.4285913 -4.4284716 -4.4283228 -4.4281216 -4.4279723 -4.4280953 -4.42829 -4.4283843 -4.42844 -4.4285212 -4.4286146 -4.428668][-4.4288507 -4.4288273 -4.4287443 -4.4286237 -4.4285007 -4.4283676 -4.4282036 -4.4280815 -4.4281826 -4.4283581 -4.4284635 -4.4285283 -4.4286108 -4.4287105 -4.4287739][-4.4288955 -4.4288797 -4.4288139 -4.4287076 -4.4285984 -4.4284959 -4.4283962 -4.42833 -4.4283872 -4.4285007 -4.4285836 -4.42864 -4.4287047 -4.4287858 -4.4288435][-4.4289441 -4.4289351 -4.4288917 -4.4288144 -4.4287314 -4.428659 -4.4286075 -4.4285822 -4.4286051 -4.4286628 -4.4287143 -4.4287515 -4.4287953 -4.4288516 -4.4288955][-4.4289737 -4.4289708 -4.428947 -4.4289017 -4.428854 -4.428813 -4.428792 -4.4287872 -4.4287949 -4.4288168 -4.4288397 -4.4288516 -4.4288688 -4.4288964 -4.4289231][-4.4289756 -4.4289794 -4.42897 -4.4289479 -4.4289255 -4.42891 -4.4289074 -4.428915 -4.4289227 -4.4289317 -4.4289379 -4.4289341 -4.4289303 -4.4289346 -4.4289474][-4.428967 -4.4289727 -4.4289708 -4.4289632 -4.4289579 -4.4289589 -4.4289665 -4.428978 -4.4289894 -4.4289947 -4.4289937 -4.4289851 -4.4289742 -4.428967 -4.4289718][-4.4289579 -4.4289622 -4.4289637 -4.4289641 -4.42897 -4.4289794 -4.4289885 -4.4289985 -4.429008 -4.4290113 -4.4290094 -4.4290028 -4.4289942 -4.4289894 -4.4289927]]...]
INFO - root - 2017-12-10 04:49:06.485833: step 3110, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.241 sec/batch; 22h:00m:31s remains)
INFO - root - 2017-12-10 04:49:09.017742: step 3120, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:18m:56s remains)
INFO - root - 2017-12-10 04:49:11.482347: step 3130, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:28m:46s remains)
INFO - root - 2017-12-10 04:49:13.954027: step 3140, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 22h:12m:29s remains)
INFO - root - 2017-12-10 04:49:16.426175: step 3150, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:52m:56s remains)
INFO - root - 2017-12-10 04:49:18.910339: step 3160, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:35m:21s remains)
INFO - root - 2017-12-10 04:49:21.392174: step 3170, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:30m:36s remains)
INFO - root - 2017-12-10 04:49:23.930792: step 3180, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:52m:41s remains)
INFO - root - 2017-12-10 04:49:26.414321: step 3190, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:57m:48s remains)
INFO - root - 2017-12-10 04:49:28.886565: step 3200, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:45m:10s remains)
2017-12-10 04:49:29.167076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289832 -4.428967 -4.4289613 -4.4289632 -4.4289746 -4.4289775 -4.4289708 -4.4289618 -4.4289579 -4.4289575 -4.4289575 -4.4289575 -4.4289622 -4.4289694 -4.4289756][-4.4289503 -4.4289212 -4.4289064 -4.4288983 -4.4289136 -4.4289217 -4.4289107 -4.4288993 -4.4288955 -4.4288993 -4.4289074 -4.4289141 -4.4289293 -4.428947 -4.4289551][-4.4288893 -4.4288483 -4.4288287 -4.4288116 -4.4288225 -4.4288273 -4.4288087 -4.4288 -4.4288006 -4.4288149 -4.4288359 -4.4288507 -4.4288797 -4.4289126 -4.4289289][-4.4288411 -4.4287872 -4.4287548 -4.4287276 -4.4287271 -4.4287233 -4.4286952 -4.4286876 -4.4286933 -4.4287124 -4.4287472 -4.4287758 -4.4288239 -4.4288769 -4.4289036][-4.4287996 -4.4287295 -4.4286747 -4.4286318 -4.4286156 -4.4286041 -4.4285688 -4.4285645 -4.4285665 -4.4285774 -4.4286246 -4.4286828 -4.4287686 -4.4288473 -4.4288855][-4.4287462 -4.4286504 -4.4285703 -4.4285049 -4.4284673 -4.4284472 -4.4284034 -4.4283981 -4.4283857 -4.4283781 -4.4284344 -4.4285293 -4.4286714 -4.4287925 -4.4288559][-4.4286871 -4.428566 -4.4284654 -4.4283895 -4.4283466 -4.4283133 -4.4282484 -4.4282227 -4.4281759 -4.4281406 -4.428195 -4.4283319 -4.4285421 -4.4287181 -4.4288139][-4.4286022 -4.4284806 -4.4283824 -4.4283147 -4.4282789 -4.4282336 -4.428153 -4.4281111 -4.4280567 -4.4280186 -4.4280815 -4.4282465 -4.4284878 -4.4286828 -4.4287891][-4.4285603 -4.4284596 -4.4283881 -4.428339 -4.4283113 -4.4282651 -4.4281869 -4.4281554 -4.4281278 -4.4281139 -4.4281869 -4.4283381 -4.4285469 -4.4287128 -4.4288058][-4.428607 -4.428534 -4.4284878 -4.428453 -4.4284334 -4.4283996 -4.4283509 -4.42834 -4.4283309 -4.4283309 -4.4283934 -4.4285064 -4.4286604 -4.4287791 -4.4288487][-4.4286957 -4.4286447 -4.4286194 -4.4286008 -4.4285951 -4.428586 -4.4285722 -4.4285774 -4.42858 -4.4285851 -4.4286251 -4.4286952 -4.4287944 -4.4288688 -4.4289112][-4.4288 -4.4287672 -4.4287505 -4.4287434 -4.4287515 -4.4287615 -4.4287667 -4.4287763 -4.4287839 -4.428792 -4.4288187 -4.4288611 -4.4289174 -4.428956 -4.4289732][-4.4288764 -4.4288554 -4.428844 -4.4288435 -4.4288521 -4.4288616 -4.4288726 -4.428884 -4.4288936 -4.4289012 -4.4289207 -4.4289494 -4.4289823 -4.4290013 -4.4290071][-4.4289336 -4.4289212 -4.4289141 -4.4289131 -4.4289179 -4.4289246 -4.428936 -4.4289465 -4.4289532 -4.4289579 -4.4289694 -4.4289875 -4.4290051 -4.4290137 -4.4290175][-4.4289775 -4.42897 -4.4289675 -4.4289675 -4.4289684 -4.4289713 -4.4289789 -4.4289856 -4.4289889 -4.4289918 -4.4289989 -4.429009 -4.4290171 -4.4290209 -4.4290242]]...]
INFO - root - 2017-12-10 04:49:31.656645: step 3210, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:10m:18s remains)
INFO - root - 2017-12-10 04:49:34.104146: step 3220, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:58m:31s remains)
INFO - root - 2017-12-10 04:49:36.610078: step 3230, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:09m:51s remains)
INFO - root - 2017-12-10 04:49:39.099006: step 3240, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:00m:55s remains)
INFO - root - 2017-12-10 04:49:41.571092: step 3250, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:15m:22s remains)
INFO - root - 2017-12-10 04:49:44.041454: step 3260, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:28m:30s remains)
INFO - root - 2017-12-10 04:49:46.523364: step 3270, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:59m:06s remains)
INFO - root - 2017-12-10 04:49:49.009116: step 3280, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:27m:44s remains)
INFO - root - 2017-12-10 04:49:51.482673: step 3290, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:23m:22s remains)
INFO - root - 2017-12-10 04:49:53.988804: step 3300, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:50m:28s remains)
2017-12-10 04:49:54.278064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288621 -4.4288888 -4.4289331 -4.4289689 -4.428998 -4.4290175 -4.4290209 -4.4290056 -4.4289756 -4.4289374 -4.428905 -4.4288683 -4.42883 -4.4288297 -4.4288654][-4.4288087 -4.4288559 -4.4289131 -4.4289579 -4.4289885 -4.4290104 -4.4290252 -4.4290247 -4.4290123 -4.4289956 -4.4289832 -4.4289632 -4.4289312 -4.4289117 -4.4289083][-4.4287567 -4.4288116 -4.4288707 -4.428915 -4.4289403 -4.4289589 -4.4289761 -4.4289832 -4.4289832 -4.4289861 -4.4289956 -4.4289951 -4.428977 -4.4289579 -4.4289384][-4.4287624 -4.428803 -4.42884 -4.428864 -4.4288716 -4.4288778 -4.4288845 -4.4288859 -4.4288893 -4.4289083 -4.4289379 -4.4289608 -4.4289684 -4.4289708 -4.4289632][-4.42881 -4.4288259 -4.42883 -4.428822 -4.4287996 -4.4287739 -4.4287486 -4.4287219 -4.4287152 -4.4287491 -4.4288068 -4.4288664 -4.428916 -4.4289565 -4.428978][-4.428864 -4.428863 -4.4288435 -4.4288044 -4.4287429 -4.42867 -4.4285903 -4.4285159 -4.4284911 -4.428545 -4.4286389 -4.4287429 -4.4288383 -4.4289141 -4.42896][-4.4289217 -4.428915 -4.4288816 -4.4288168 -4.428719 -4.4285955 -4.4284534 -4.4283218 -4.4282823 -4.428369 -4.4285064 -4.4286518 -4.4287796 -4.4288712 -4.42892][-4.4289842 -4.4289827 -4.4289427 -4.4288635 -4.4287434 -4.4285827 -4.4283881 -4.4282074 -4.4281621 -4.42828 -4.4284492 -4.4286175 -4.4287558 -4.4288435 -4.4288826][-4.4290304 -4.4290352 -4.4289975 -4.4289203 -4.428803 -4.4286447 -4.4284468 -4.4282646 -4.4282279 -4.4283457 -4.4285035 -4.4286556 -4.4287763 -4.4288449 -4.42887][-4.4290628 -4.4290671 -4.4290333 -4.4289694 -4.4288754 -4.4287539 -4.4286046 -4.4284739 -4.4284568 -4.4285445 -4.4286518 -4.4287581 -4.4288449 -4.4288926 -4.4289079][-4.4290657 -4.4290633 -4.4290342 -4.4289846 -4.4289212 -4.4288497 -4.4287653 -4.4286995 -4.4287033 -4.4287534 -4.4288025 -4.4288578 -4.4289103 -4.4289441 -4.4289584][-4.4290423 -4.4290333 -4.4290094 -4.4289765 -4.428946 -4.4289193 -4.4288907 -4.428874 -4.4288859 -4.4289031 -4.4289064 -4.4289188 -4.4289408 -4.4289637 -4.4289842][-4.4290028 -4.4289856 -4.4289632 -4.4289451 -4.4289451 -4.4289551 -4.4289646 -4.428978 -4.4289885 -4.42898 -4.4289517 -4.4289303 -4.4289246 -4.4289374 -4.428968][-4.4289651 -4.4289412 -4.4289131 -4.4289007 -4.4289212 -4.4289556 -4.428988 -4.429018 -4.4290285 -4.4290056 -4.4289575 -4.42891 -4.4288807 -4.4288816 -4.4289203][-4.4289379 -4.428916 -4.4288869 -4.4288759 -4.4289026 -4.4289422 -4.4289784 -4.4290104 -4.4290218 -4.4290009 -4.4289517 -4.4288936 -4.4288483 -4.4288383 -4.4288769]]...]
INFO - root - 2017-12-10 04:49:56.768392: step 3310, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:14m:30s remains)
INFO - root - 2017-12-10 04:49:59.243411: step 3320, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:23m:11s remains)
INFO - root - 2017-12-10 04:50:01.705275: step 3330, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:20m:36s remains)
INFO - root - 2017-12-10 04:50:04.154087: step 3340, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:01m:26s remains)
INFO - root - 2017-12-10 04:50:06.605923: step 3350, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:26m:36s remains)
INFO - root - 2017-12-10 04:50:09.122583: step 3360, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:01m:06s remains)
INFO - root - 2017-12-10 04:50:11.583930: step 3370, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:17m:55s remains)
INFO - root - 2017-12-10 04:50:14.068003: step 3380, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:17m:50s remains)
INFO - root - 2017-12-10 04:50:16.554592: step 3390, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:14m:51s remains)
INFO - root - 2017-12-10 04:50:19.018343: step 3400, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:58m:48s remains)
2017-12-10 04:50:19.321815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287605 -4.4287596 -4.4287996 -4.4288359 -4.4288592 -4.4288635 -4.4288535 -4.428834 -4.4288096 -4.4287934 -4.4287987 -4.4288063 -4.4287858 -4.42873 -4.4286618][-4.4287119 -4.4287095 -4.4287543 -4.4287977 -4.4288273 -4.4288306 -4.4288096 -4.4287729 -4.4287357 -4.4287186 -4.4287338 -4.4287481 -4.4287195 -4.4286375 -4.428545][-4.4286385 -4.428628 -4.4286871 -4.4287486 -4.4287834 -4.42878 -4.4287472 -4.4286957 -4.4286518 -4.4286418 -4.4286742 -4.4287 -4.4286766 -4.4285884 -4.4284973][-4.428544 -4.42852 -4.4286013 -4.428689 -4.4287348 -4.428719 -4.4286637 -4.4285951 -4.428556 -4.4285727 -4.4286294 -4.4286671 -4.42866 -4.4285908 -4.42853][-4.4284663 -4.4284272 -4.4285235 -4.4286318 -4.4286852 -4.42864 -4.4285455 -4.4284625 -4.4284511 -4.4285121 -4.4285932 -4.4286337 -4.4286427 -4.4286141 -4.4285989][-4.428452 -4.4284182 -4.4285183 -4.428616 -4.4286451 -4.428556 -4.4284053 -4.4283109 -4.4283476 -4.4284596 -4.4285641 -4.428607 -4.4286289 -4.4286413 -4.4286661][-4.4285216 -4.4285045 -4.4285841 -4.4286442 -4.428628 -4.4284878 -4.4282808 -4.4281764 -4.4282694 -4.428422 -4.4285388 -4.4285851 -4.4286141 -4.428648 -4.4286923][-4.4286146 -4.4286089 -4.4286528 -4.4286656 -4.4286065 -4.4284406 -4.42822 -4.4281325 -4.4282651 -4.4284377 -4.4285483 -4.42859 -4.4286246 -4.4286642 -4.4287043][-4.4286928 -4.4286847 -4.4287062 -4.4286942 -4.4286175 -4.428473 -4.4283109 -4.4282732 -4.4283934 -4.4285326 -4.4286165 -4.4286509 -4.4286871 -4.42872 -4.4287415][-4.4287658 -4.4287467 -4.428751 -4.4287357 -4.4286828 -4.4285946 -4.4285164 -4.428514 -4.4285941 -4.4286814 -4.4287348 -4.4287581 -4.428781 -4.4287891 -4.4287848][-4.4288464 -4.42882 -4.4288106 -4.4287949 -4.4287667 -4.4287243 -4.4286981 -4.4287119 -4.42876 -4.4288149 -4.4288526 -4.4288673 -4.4288754 -4.4288583 -4.4288349][-4.428925 -4.428896 -4.4288716 -4.4288535 -4.4288368 -4.4288192 -4.4288187 -4.4288378 -4.4288735 -4.4289136 -4.4289417 -4.4289479 -4.4289436 -4.4289141 -4.4288855][-4.4289622 -4.4289379 -4.4289169 -4.4289041 -4.4288964 -4.4288931 -4.4289031 -4.4289203 -4.428947 -4.4289761 -4.4289966 -4.4289956 -4.4289823 -4.4289527 -4.4289303][-4.4289651 -4.4289508 -4.4289417 -4.4289384 -4.4289365 -4.4289384 -4.42895 -4.4289646 -4.4289846 -4.4290037 -4.4290147 -4.4290066 -4.4289894 -4.4289641 -4.42895][-4.4289708 -4.4289618 -4.4289603 -4.4289627 -4.4289641 -4.428966 -4.4289751 -4.4289889 -4.4290061 -4.4290133 -4.4290113 -4.4289994 -4.4289842 -4.428967 -4.4289584]]...]
INFO - root - 2017-12-10 04:50:21.757796: step 3410, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:36m:21s remains)
INFO - root - 2017-12-10 04:50:24.242053: step 3420, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:56m:28s remains)
INFO - root - 2017-12-10 04:50:26.715814: step 3430, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 21h:59m:47s remains)
INFO - root - 2017-12-10 04:50:29.196126: step 3440, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 23h:01m:45s remains)
INFO - root - 2017-12-10 04:50:31.648198: step 3450, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:06m:00s remains)
INFO - root - 2017-12-10 04:50:34.126095: step 3460, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:12m:02s remains)
INFO - root - 2017-12-10 04:50:36.584764: step 3470, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:06m:11s remains)
INFO - root - 2017-12-10 04:50:39.072021: step 3480, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:28m:17s remains)
INFO - root - 2017-12-10 04:50:41.523361: step 3490, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:16m:34s remains)
INFO - root - 2017-12-10 04:50:44.024658: step 3500, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:40m:29s remains)
2017-12-10 04:50:44.305463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286609 -4.4286585 -4.4287028 -4.4287286 -4.4287524 -4.4287634 -4.4287639 -4.4287763 -4.4287868 -4.4287887 -4.4287858 -4.428771 -4.4287553 -4.4287419 -4.4287248][-4.4286461 -4.4286461 -4.4286819 -4.4286981 -4.4287205 -4.428731 -4.4287286 -4.428741 -4.4287524 -4.4287591 -4.4287648 -4.4287467 -4.4287162 -4.428688 -4.4286623][-4.4286079 -4.4286118 -4.4286523 -4.4286637 -4.4286895 -4.4286995 -4.428689 -4.4287 -4.4287167 -4.4287295 -4.4287467 -4.4287329 -4.4286971 -4.4286551 -4.4286165][-4.4285398 -4.42854 -4.4285889 -4.4286051 -4.4286342 -4.4286437 -4.42862 -4.4286232 -4.4286456 -4.4286633 -4.4286904 -4.4286861 -4.428647 -4.4285903 -4.4285369][-4.4285359 -4.4285297 -4.4285769 -4.4285913 -4.4286137 -4.4286137 -4.4285741 -4.4285679 -4.4285955 -4.4286165 -4.4286513 -4.4286532 -4.4286137 -4.4285483 -4.4284863][-4.4285994 -4.4285755 -4.4286046 -4.4286151 -4.4286284 -4.4286151 -4.4285622 -4.4285493 -4.4285808 -4.4286065 -4.4286485 -4.4286594 -4.4286313 -4.4285722 -4.4285083][-4.4286389 -4.4285984 -4.4286146 -4.428618 -4.4286189 -4.4285879 -4.4285207 -4.4285059 -4.4285398 -4.4285703 -4.42862 -4.4286542 -4.4286571 -4.4286127 -4.428545][-4.4286566 -4.4286127 -4.4286218 -4.4286175 -4.4286046 -4.4285536 -4.4284778 -4.4284611 -4.4284921 -4.4285197 -4.4285603 -4.4286051 -4.428628 -4.4285903 -4.4285297][-4.4286666 -4.4286294 -4.4286408 -4.4286447 -4.4286432 -4.4285994 -4.4285359 -4.4285197 -4.4285364 -4.4285464 -4.4285617 -4.428597 -4.428618 -4.4285779 -4.428525][-4.428688 -4.42866 -4.4286876 -4.42871 -4.4287291 -4.4287028 -4.4286585 -4.4286466 -4.4286513 -4.4286447 -4.4286413 -4.4286652 -4.4286804 -4.4286475 -4.4286032][-4.4287057 -4.4286857 -4.428731 -4.4287639 -4.4287906 -4.4287791 -4.4287543 -4.428751 -4.4287562 -4.4287505 -4.4287429 -4.4287539 -4.4287629 -4.4287419 -4.428719][-4.4287128 -4.4286971 -4.4287534 -4.4288006 -4.4288421 -4.428853 -4.428853 -4.4288626 -4.4288716 -4.4288692 -4.4288621 -4.42886 -4.4288568 -4.4288373 -4.4288182][-4.4287271 -4.4287186 -4.4287834 -4.4288383 -4.428884 -4.4289074 -4.4289222 -4.4289336 -4.4289436 -4.4289484 -4.4289417 -4.4289355 -4.4289284 -4.4289088 -4.4288859][-4.4287467 -4.4287424 -4.4287982 -4.4288478 -4.4288907 -4.428916 -4.4289317 -4.4289374 -4.4289403 -4.4289432 -4.4289422 -4.4289432 -4.42894 -4.4289265 -4.4289107][-4.4287529 -4.4287486 -4.4287972 -4.4288449 -4.4288883 -4.4289103 -4.4289236 -4.4289255 -4.4289241 -4.4289289 -4.428936 -4.428946 -4.4289489 -4.4289446 -4.4289384]]...]
INFO - root - 2017-12-10 04:50:46.782060: step 3510, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:54m:10s remains)
INFO - root - 2017-12-10 04:50:49.290634: step 3520, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:11m:37s remains)
INFO - root - 2017-12-10 04:50:51.736317: step 3530, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:17m:15s remains)
INFO - root - 2017-12-10 04:50:54.225642: step 3540, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:31m:49s remains)
INFO - root - 2017-12-10 04:50:56.676695: step 3550, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 22h:18m:44s remains)
INFO - root - 2017-12-10 04:50:59.129937: step 3560, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 22h:26m:19s remains)
INFO - root - 2017-12-10 04:51:01.573246: step 3570, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.250 sec/batch; 22h:48m:00s remains)
INFO - root - 2017-12-10 04:51:04.065050: step 3580, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 22h:13m:48s remains)
INFO - root - 2017-12-10 04:51:06.556262: step 3590, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:36m:13s remains)
INFO - root - 2017-12-10 04:51:09.041491: step 3600, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:22m:43s remains)
2017-12-10 04:51:09.333258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428772 -4.4288855 -4.4289703 -4.4290109 -4.4290133 -4.4290085 -4.4290104 -4.4290071 -4.4289923 -4.4289627 -4.4289179 -4.428853 -4.4287829 -4.4287572 -4.4287758][-4.4286537 -4.428793 -4.4289126 -4.4289789 -4.4289932 -4.4289913 -4.4289927 -4.4289937 -4.4289842 -4.4289641 -4.4289341 -4.4288797 -4.4288216 -4.4287858 -4.4287853][-4.4285793 -4.4287214 -4.4288597 -4.4289441 -4.4289675 -4.428967 -4.4289618 -4.4289613 -4.4289627 -4.4289579 -4.4289408 -4.4288993 -4.4288583 -4.4288211 -4.4288092][-4.4285932 -4.4287109 -4.42884 -4.4289274 -4.4289517 -4.4289441 -4.4289274 -4.4289212 -4.4289274 -4.4289389 -4.4289379 -4.42892 -4.4289079 -4.42888 -4.4288626][-4.4286785 -4.4287496 -4.4288483 -4.4289231 -4.4289408 -4.4289274 -4.4288979 -4.4288778 -4.4288845 -4.4289179 -4.4289451 -4.4289613 -4.4289708 -4.4289489 -4.4289265][-4.4287806 -4.4288096 -4.4288726 -4.428925 -4.428926 -4.4288931 -4.4288373 -4.428791 -4.4287949 -4.4288635 -4.4289346 -4.4289823 -4.4290056 -4.4289865 -4.4289589][-4.4288411 -4.4288545 -4.4288955 -4.4289274 -4.4289079 -4.4288359 -4.4287338 -4.4286404 -4.4286351 -4.4287524 -4.4288836 -4.4289632 -4.428997 -4.4289846 -4.4289494][-4.4288521 -4.4288669 -4.4288964 -4.4289179 -4.428884 -4.4287829 -4.4286361 -4.4284878 -4.4284539 -4.4286103 -4.4287996 -4.4289093 -4.4289503 -4.4289446 -4.4289055][-4.4288788 -4.4288926 -4.4289155 -4.4289327 -4.4288988 -4.428802 -4.428659 -4.4285049 -4.4284444 -4.4285631 -4.4287395 -4.4288487 -4.4288855 -4.4288778 -4.4288349][-4.4289279 -4.4289374 -4.4289522 -4.428957 -4.4289255 -4.4288511 -4.4287453 -4.4286356 -4.4285841 -4.4286456 -4.4287596 -4.4288349 -4.4288549 -4.428834 -4.4287829][-4.4289632 -4.4289665 -4.4289751 -4.4289727 -4.4289465 -4.4288974 -4.4288316 -4.4287648 -4.4287276 -4.4287505 -4.4288106 -4.428853 -4.4288597 -4.4288359 -4.4287915][-4.4289651 -4.4289613 -4.4289627 -4.4289603 -4.4289446 -4.4289193 -4.4288907 -4.4288635 -4.4288435 -4.4288459 -4.4288688 -4.4288831 -4.4288797 -4.4288535 -4.4288173][-4.4289384 -4.4289265 -4.4289193 -4.4289136 -4.4289074 -4.4289026 -4.4289055 -4.428906 -4.4289007 -4.428894 -4.4288964 -4.428894 -4.42888 -4.4288564 -4.4288244][-4.4289079 -4.4288907 -4.4288759 -4.4288635 -4.4288583 -4.4288635 -4.4288797 -4.4288912 -4.4288945 -4.4288888 -4.4288831 -4.4288764 -4.428863 -4.4288459 -4.4288235][-4.4289145 -4.4288945 -4.428875 -4.42886 -4.428853 -4.4288568 -4.4288659 -4.4288697 -4.4288678 -4.4288588 -4.4288454 -4.4288354 -4.4288292 -4.4288244 -4.42882]]...]
INFO - root - 2017-12-10 04:51:11.808053: step 3610, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:57m:58s remains)
INFO - root - 2017-12-10 04:51:14.281303: step 3620, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 22h:09m:07s remains)
INFO - root - 2017-12-10 04:51:16.766554: step 3630, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:58m:30s remains)
INFO - root - 2017-12-10 04:51:19.234355: step 3640, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:25m:17s remains)
INFO - root - 2017-12-10 04:51:21.750414: step 3650, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:48m:55s remains)
INFO - root - 2017-12-10 04:51:24.217178: step 3660, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:32m:59s remains)
INFO - root - 2017-12-10 04:51:26.684085: step 3670, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:57m:29s remains)
INFO - root - 2017-12-10 04:51:29.174351: step 3680, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:35m:36s remains)
INFO - root - 2017-12-10 04:51:31.677615: step 3690, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:00m:01s remains)
INFO - root - 2017-12-10 04:51:34.187095: step 3700, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:37m:59s remains)
2017-12-10 04:51:34.523666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287667 -4.4287791 -4.4287729 -4.4287391 -4.4287095 -4.4286981 -4.4287004 -4.4286995 -4.428678 -4.4286366 -4.4286261 -4.4286509 -4.4287033 -4.4287424 -4.4287386][-4.4287295 -4.4287353 -4.4287329 -4.4287062 -4.4286885 -4.4286895 -4.4287028 -4.4286914 -4.4286523 -4.4285979 -4.4285879 -4.4286141 -4.4286695 -4.4287148 -4.428719][-4.4286718 -4.4286695 -4.4286656 -4.4286456 -4.4286475 -4.4286666 -4.4286981 -4.4286971 -4.4286652 -4.4286275 -4.4286284 -4.4286585 -4.4287038 -4.4287262 -4.4287114][-4.4286289 -4.4286237 -4.4286189 -4.4286013 -4.42862 -4.4286556 -4.42869 -4.428699 -4.4286842 -4.4286685 -4.4286823 -4.4287128 -4.4287405 -4.4287372 -4.4287009][-4.428606 -4.4285913 -4.4285779 -4.4285579 -4.4285669 -4.4285917 -4.4286079 -4.4286151 -4.4286242 -4.4286408 -4.4286695 -4.4287004 -4.4287205 -4.4287105 -4.4286742][-4.4285717 -4.4285359 -4.4285016 -4.4284635 -4.428443 -4.4284272 -4.4284 -4.4283967 -4.4284382 -4.4284983 -4.4285588 -4.4286075 -4.4286351 -4.4286427 -4.4286342][-4.4284954 -4.4284258 -4.4283533 -4.4282813 -4.4282274 -4.4281707 -4.4281058 -4.4281087 -4.4281974 -4.4283071 -4.4284086 -4.4284844 -4.4285288 -4.4285712 -4.4286051][-4.4284663 -4.4283791 -4.4282861 -4.428195 -4.4281368 -4.4280844 -4.42803 -4.4280391 -4.4281349 -4.4282513 -4.4283619 -4.4284482 -4.4285011 -4.4285536 -4.4286056][-4.4285917 -4.4285274 -4.4284549 -4.4283876 -4.4283571 -4.4283404 -4.42832 -4.4283237 -4.4283724 -4.4284396 -4.4285097 -4.4285717 -4.4286 -4.4286218 -4.4286532][-4.4287219 -4.4286804 -4.4286256 -4.4285769 -4.4285588 -4.42856 -4.4285607 -4.4285674 -4.4285932 -4.42863 -4.4286728 -4.42871 -4.4287205 -4.4287162 -4.4287224][-4.4288044 -4.4287763 -4.4287367 -4.4287014 -4.4286876 -4.4286914 -4.4287014 -4.4287128 -4.4287305 -4.4287505 -4.4287753 -4.4287963 -4.4287963 -4.4287868 -4.4287844][-4.428865 -4.4288378 -4.4288054 -4.4287777 -4.4287653 -4.4287643 -4.428772 -4.4287839 -4.4287982 -4.4288125 -4.4288292 -4.4288454 -4.4288487 -4.4288483 -4.4288511][-4.4289041 -4.4288783 -4.4288535 -4.4288311 -4.4288173 -4.4288125 -4.4288182 -4.4288292 -4.4288378 -4.4288459 -4.4288554 -4.4288707 -4.4288807 -4.4288878 -4.4288936][-4.4289441 -4.4289222 -4.4289055 -4.4288917 -4.4288807 -4.4288774 -4.4288812 -4.428885 -4.4288855 -4.4288831 -4.4288836 -4.4288898 -4.4288974 -4.4289041 -4.4289083][-4.4289708 -4.428956 -4.4289494 -4.428946 -4.4289441 -4.4289465 -4.4289527 -4.4289541 -4.4289479 -4.4289389 -4.4289308 -4.4289274 -4.4289265 -4.428926 -4.428925]]...]
INFO - root - 2017-12-10 04:51:37.046157: step 3710, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 22h:35m:20s remains)
INFO - root - 2017-12-10 04:51:39.572071: step 3720, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:11m:50s remains)
INFO - root - 2017-12-10 04:51:42.122792: step 3730, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:32m:48s remains)
INFO - root - 2017-12-10 04:51:44.696607: step 3740, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:10m:07s remains)
INFO - root - 2017-12-10 04:51:47.275194: step 3750, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:26m:33s remains)
INFO - root - 2017-12-10 04:51:49.793147: step 3760, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:48m:10s remains)
INFO - root - 2017-12-10 04:51:52.343878: step 3770, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:58m:56s remains)
INFO - root - 2017-12-10 04:51:54.962528: step 3780, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:50m:07s remains)
INFO - root - 2017-12-10 04:51:57.569579: step 3790, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:21m:06s remains)
INFO - root - 2017-12-10 04:52:00.082601: step 3800, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:12s remains)
2017-12-10 04:52:00.415386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289479 -4.4289341 -4.4289203 -4.4289074 -4.4288983 -4.4288907 -4.4288936 -4.4289021 -4.4289002 -4.4288898 -4.4288864 -4.4288979 -4.4289145 -4.4289246 -4.4289255][-4.4289322 -4.4289055 -4.4288759 -4.4288535 -4.428844 -4.4288406 -4.4288473 -4.4288578 -4.4288588 -4.4288516 -4.4288526 -4.4288645 -4.4288869 -4.4289041 -4.42891][-4.4289007 -4.4288492 -4.4287949 -4.4287534 -4.4287391 -4.4287481 -4.4287724 -4.428793 -4.428802 -4.4288063 -4.4288173 -4.4288321 -4.4288583 -4.4288812 -4.4288878][-4.4288716 -4.4287944 -4.4287081 -4.4286332 -4.428597 -4.4286003 -4.4286318 -4.4286675 -4.4287047 -4.4287405 -4.4287691 -4.4287896 -4.4288263 -4.4288554 -4.4288645][-4.4288659 -4.428772 -4.4286628 -4.4285507 -4.4284678 -4.4284286 -4.42843 -4.4284654 -4.4285436 -4.4286327 -4.4286938 -4.4287333 -4.4287896 -4.42883 -4.4288468][-4.4288807 -4.4287877 -4.4286728 -4.4285364 -4.4284019 -4.428287 -4.428195 -4.428174 -4.42829 -4.4284444 -4.4285541 -4.4286327 -4.4287186 -4.4287791 -4.4288116][-4.4289036 -4.4288287 -4.4287276 -4.4285955 -4.4284458 -4.4282761 -4.4280739 -4.4279327 -4.4280291 -4.4282255 -4.4283857 -4.4285188 -4.4286437 -4.4287319 -4.42878][-4.4289246 -4.428875 -4.4288034 -4.4287081 -4.4285984 -4.4284573 -4.4282684 -4.4281058 -4.4281092 -4.4282336 -4.4283776 -4.4285188 -4.4286442 -4.428731 -4.4287806][-4.4289365 -4.4289069 -4.4288645 -4.4288087 -4.4287415 -4.4286509 -4.4285321 -4.4284306 -4.4283986 -4.4284396 -4.428525 -4.4286222 -4.4287071 -4.4287639 -4.4287992][-4.428937 -4.4289241 -4.4289002 -4.4288697 -4.4288363 -4.4287872 -4.4287267 -4.428678 -4.428647 -4.4286485 -4.4286904 -4.4287462 -4.428792 -4.4288197 -4.4288383][-4.4289351 -4.4289293 -4.42891 -4.4288883 -4.428874 -4.4288526 -4.4288306 -4.4288254 -4.4288111 -4.4288044 -4.4288278 -4.4288564 -4.4288764 -4.4288836 -4.4288888][-4.428937 -4.428937 -4.4289231 -4.4289045 -4.4288993 -4.4288988 -4.4288993 -4.428916 -4.4289136 -4.4289083 -4.4289241 -4.4289389 -4.4289422 -4.4289341 -4.428925][-4.4289432 -4.4289422 -4.4289331 -4.4289193 -4.4289184 -4.4289303 -4.4289393 -4.428956 -4.4289565 -4.4289536 -4.4289632 -4.4289718 -4.4289665 -4.4289513 -4.4289379][-4.4289675 -4.4289646 -4.428957 -4.428946 -4.4289436 -4.4289522 -4.4289589 -4.4289665 -4.4289675 -4.4289665 -4.4289737 -4.42898 -4.4289737 -4.4289613 -4.4289522][-4.4289904 -4.4289875 -4.4289837 -4.4289765 -4.4289718 -4.4289732 -4.4289756 -4.4289765 -4.4289784 -4.4289818 -4.428987 -4.4289885 -4.4289818 -4.4289732 -4.4289694]]...]
INFO - root - 2017-12-10 04:52:02.983953: step 3810, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 23h:02m:02s remains)
INFO - root - 2017-12-10 04:52:05.556123: step 3820, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:56m:58s remains)
INFO - root - 2017-12-10 04:52:08.128223: step 3830, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:14m:05s remains)
INFO - root - 2017-12-10 04:52:10.696087: step 3840, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:26m:03s remains)
INFO - root - 2017-12-10 04:52:13.303925: step 3850, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:14m:59s remains)
INFO - root - 2017-12-10 04:52:15.851632: step 3860, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 23h:03m:28s remains)
INFO - root - 2017-12-10 04:52:18.400660: step 3870, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:52m:50s remains)
INFO - root - 2017-12-10 04:52:21.003011: step 3880, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:37m:20s remains)
INFO - root - 2017-12-10 04:52:23.539291: step 3890, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:56m:31s remains)
INFO - root - 2017-12-10 04:52:26.115511: step 3900, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:10m:45s remains)
2017-12-10 04:52:26.460615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288125 -4.4286537 -4.4285026 -4.4284086 -4.4284091 -4.4284873 -4.4286013 -4.4286838 -4.4287262 -4.4287148 -4.4286742 -4.4286551 -4.4286408 -4.4286227 -4.4286222][-4.4288006 -4.4286151 -4.428431 -4.4283261 -4.4283442 -4.4284492 -4.4285774 -4.4286704 -4.4287195 -4.4287024 -4.4286613 -4.4286475 -4.4286394 -4.4286246 -4.428616][-4.4287953 -4.4285946 -4.4283924 -4.4282866 -4.4283228 -4.428453 -4.4285879 -4.42868 -4.4287286 -4.4287086 -4.4286637 -4.4286513 -4.4286509 -4.428637 -4.4286194][-4.4287944 -4.4285822 -4.4283643 -4.42826 -4.4283152 -4.4284639 -4.4286051 -4.4286976 -4.4287405 -4.4287181 -4.4286704 -4.4286537 -4.4286532 -4.42864 -4.4286237][-4.4288049 -4.42859 -4.4283586 -4.4282465 -4.4283123 -4.428472 -4.42862 -4.4287138 -4.4287558 -4.4287367 -4.4286909 -4.4286618 -4.4286556 -4.4286466 -4.4286456][-4.4288187 -4.4286151 -4.4283814 -4.4282613 -4.4283276 -4.4284863 -4.4286346 -4.4287205 -4.4287581 -4.4287462 -4.428709 -4.4286785 -4.4286695 -4.4286661 -4.4286804][-4.4288297 -4.4286427 -4.4284172 -4.428299 -4.4283595 -4.4285054 -4.4286447 -4.4287219 -4.42876 -4.42876 -4.4287267 -4.4286942 -4.4286823 -4.428688 -4.4287105][-4.4288368 -4.4286652 -4.4284587 -4.4283562 -4.428412 -4.4285398 -4.4286733 -4.4287438 -4.4287815 -4.4287939 -4.4287653 -4.4287186 -4.4286985 -4.42871 -4.4287367][-4.4288478 -4.4286914 -4.428503 -4.4284148 -4.4284706 -4.4285822 -4.4287095 -4.4287715 -4.4288044 -4.4288273 -4.4288068 -4.4287572 -4.4287276 -4.4287338 -4.4287524][-4.4288626 -4.4287195 -4.428546 -4.4284668 -4.4285178 -4.428616 -4.4287343 -4.4287915 -4.4288216 -4.4288526 -4.4288363 -4.4287877 -4.4287572 -4.4287629 -4.4287753][-4.4288664 -4.4287353 -4.4285755 -4.4285011 -4.4285431 -4.4286294 -4.4287434 -4.4288049 -4.4288344 -4.4288669 -4.4288535 -4.42881 -4.4287796 -4.4287853 -4.4288044][-4.4288721 -4.4287548 -4.4286122 -4.4285417 -4.4285741 -4.4286485 -4.4287529 -4.4288163 -4.428843 -4.428874 -4.4288654 -4.4288278 -4.4287963 -4.4288044 -4.428834][-4.4288845 -4.4287863 -4.4286656 -4.4286013 -4.4286227 -4.4286814 -4.4287682 -4.4288259 -4.4288492 -4.4288769 -4.4288712 -4.4288378 -4.4288082 -4.4288192 -4.4288559][-4.4288974 -4.4288177 -4.42872 -4.4286642 -4.4286757 -4.4287186 -4.4287853 -4.4288321 -4.4288487 -4.428864 -4.4288549 -4.4288254 -4.4288015 -4.4288163 -4.4288545][-4.4289193 -4.4288578 -4.4287844 -4.4287391 -4.4287448 -4.4287772 -4.4288268 -4.4288592 -4.428863 -4.4288621 -4.4288464 -4.4288168 -4.4288015 -4.4288197 -4.428854]]...]
INFO - root - 2017-12-10 04:52:29.029296: step 3910, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 22h:03m:48s remains)
INFO - root - 2017-12-10 04:52:31.541036: step 3920, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:52m:29s remains)
INFO - root - 2017-12-10 04:52:34.094204: step 3930, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:17m:28s remains)
INFO - root - 2017-12-10 04:52:36.619798: step 3940, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:10m:50s remains)
INFO - root - 2017-12-10 04:52:39.160834: step 3950, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:17m:57s remains)
INFO - root - 2017-12-10 04:52:41.724812: step 3960, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:54m:51s remains)
INFO - root - 2017-12-10 04:52:44.310418: step 3970, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:19m:34s remains)
INFO - root - 2017-12-10 04:52:46.901807: step 3980, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:17m:27s remains)
INFO - root - 2017-12-10 04:52:49.449879: step 3990, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:24m:31s remains)
INFO - root - 2017-12-10 04:52:52.016412: step 4000, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:24m:14s remains)
2017-12-10 04:52:52.395089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288 -4.4288182 -4.4288297 -4.4288268 -4.4288392 -4.4288626 -4.428885 -4.4289002 -4.4289131 -4.4289136 -4.4289017 -4.4288931 -4.428884 -4.4288788 -4.4288979][-4.4287786 -4.4287863 -4.4287891 -4.428782 -4.4287872 -4.4288087 -4.4288392 -4.4288688 -4.4288974 -4.4289088 -4.4289017 -4.4288917 -4.4288788 -4.4288607 -4.4288688][-4.4287314 -4.4287257 -4.4287286 -4.4287291 -4.4287357 -4.4287519 -4.4287853 -4.4288292 -4.4288726 -4.4288983 -4.4288993 -4.4288893 -4.4288683 -4.4288363 -4.4288268][-4.4286938 -4.4286804 -4.428689 -4.4286971 -4.4287019 -4.42871 -4.4287395 -4.4287925 -4.4288411 -4.4288721 -4.4288793 -4.4288712 -4.4288492 -4.4288116 -4.4287863][-4.4287066 -4.4286866 -4.4286909 -4.4286914 -4.4286776 -4.4286628 -4.428679 -4.4287333 -4.4287882 -4.4288292 -4.4288497 -4.4288564 -4.4288445 -4.4288068 -4.4287715][-4.4287572 -4.4287343 -4.4287291 -4.4287038 -4.4286542 -4.4285965 -4.4285746 -4.4286289 -4.4287138 -4.4287839 -4.42883 -4.4288592 -4.4288626 -4.428833 -4.4287944][-4.4288144 -4.4287925 -4.4287772 -4.4287276 -4.4286456 -4.4285398 -4.4284639 -4.4285145 -4.4286394 -4.4287496 -4.4288225 -4.4288688 -4.428885 -4.4288697 -4.4288349][-4.4288588 -4.4288416 -4.42882 -4.4287629 -4.4286766 -4.428555 -4.4284477 -4.4284759 -4.4286051 -4.4287257 -4.4288111 -4.4288654 -4.42889 -4.42889 -4.4288697][-4.4288898 -4.4288788 -4.4288554 -4.4288082 -4.4287357 -4.4286304 -4.4285312 -4.4285288 -4.428616 -4.4287138 -4.428793 -4.4288483 -4.4288821 -4.4288988 -4.4288969][-4.4289007 -4.4288974 -4.42888 -4.4288483 -4.428793 -4.4287066 -4.4286218 -4.4286017 -4.4286518 -4.4287219 -4.4287844 -4.4288387 -4.4288807 -4.4289112 -4.4289231][-4.4289174 -4.4289169 -4.4289031 -4.4288812 -4.4288392 -4.428771 -4.4287033 -4.4286776 -4.4287086 -4.4287596 -4.4288058 -4.4288559 -4.4289 -4.4289308 -4.4289465][-4.4289393 -4.4289379 -4.4289241 -4.4289064 -4.428874 -4.4288273 -4.428781 -4.4287577 -4.4287763 -4.4288163 -4.4288478 -4.428884 -4.4289169 -4.4289403 -4.4289541][-4.4289551 -4.4289508 -4.4289284 -4.4289041 -4.4288764 -4.4288497 -4.4288278 -4.4288149 -4.4288278 -4.4288588 -4.4288812 -4.4288988 -4.4289179 -4.4289355 -4.4289489][-4.4289503 -4.4289474 -4.4289165 -4.4288769 -4.4288445 -4.4288273 -4.4288192 -4.428813 -4.4288263 -4.4288621 -4.4288907 -4.4289083 -4.4289222 -4.4289374 -4.4289489][-4.4289374 -4.4289341 -4.4289 -4.4288497 -4.4288135 -4.4287972 -4.4287896 -4.4287782 -4.428791 -4.4288387 -4.4288874 -4.4289241 -4.4289474 -4.4289584 -4.4289603]]...]
INFO - root - 2017-12-10 04:52:54.943335: step 4010, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:33m:40s remains)
INFO - root - 2017-12-10 04:52:57.529996: step 4020, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:25s remains)
INFO - root - 2017-12-10 04:53:00.086303: step 4030, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:38m:11s remains)
INFO - root - 2017-12-10 04:53:02.668707: step 4040, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:59m:03s remains)
INFO - root - 2017-12-10 04:53:05.209715: step 4050, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:14m:27s remains)
INFO - root - 2017-12-10 04:53:07.780542: step 4060, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:56m:00s remains)
INFO - root - 2017-12-10 04:53:10.389814: step 4070, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:19s remains)
INFO - root - 2017-12-10 04:53:12.971719: step 4080, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:37m:06s remains)
INFO - root - 2017-12-10 04:53:15.586968: step 4090, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:12m:43s remains)
INFO - root - 2017-12-10 04:53:18.139001: step 4100, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:13m:32s remains)
2017-12-10 04:53:18.512615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289503 -4.4289474 -4.4289408 -4.4289432 -4.428946 -4.428946 -4.4289427 -4.4289365 -4.4289379 -4.4289403 -4.4289408 -4.4289408 -4.42894 -4.4289379 -4.4289379][-4.4289312 -4.4289227 -4.4289126 -4.4289188 -4.4289274 -4.4289317 -4.4289346 -4.4289308 -4.4289341 -4.428937 -4.4289308 -4.4289227 -4.4289179 -4.4289184 -4.4289246][-4.4288969 -4.4288812 -4.4288721 -4.4288859 -4.4289026 -4.4289165 -4.4289289 -4.42893 -4.4289312 -4.4289303 -4.4289141 -4.4288945 -4.4288912 -4.4288983 -4.4289131][-4.4288268 -4.428813 -4.4288154 -4.4288445 -4.4288726 -4.4288921 -4.4289012 -4.4288931 -4.4288816 -4.4288688 -4.428844 -4.4288306 -4.4288406 -4.4288578 -4.4288774][-4.4287543 -4.4287519 -4.428771 -4.4288139 -4.4288449 -4.4288588 -4.4288511 -4.4288135 -4.4287748 -4.4287581 -4.4287524 -4.4287753 -4.4288116 -4.4288349 -4.4288459][-4.428731 -4.4287357 -4.428751 -4.4287825 -4.4288034 -4.4287992 -4.4287577 -4.42867 -4.4286094 -4.4286304 -4.42869 -4.4287686 -4.428822 -4.4288378 -4.4288268][-4.4287362 -4.4287362 -4.4287286 -4.428731 -4.4287291 -4.4286876 -4.4285865 -4.4284434 -4.4283996 -4.4285207 -4.4286714 -4.4287877 -4.4288292 -4.4288182 -4.42879][-4.4287567 -4.4287415 -4.4287114 -4.4287 -4.4286852 -4.428616 -4.4284792 -4.4283247 -4.4283319 -4.42853 -4.4287109 -4.428813 -4.4288206 -4.4287825 -4.4287376][-4.4287968 -4.4287672 -4.428731 -4.4287219 -4.4287066 -4.428647 -4.4285488 -4.4284725 -4.4285259 -4.4286842 -4.4288006 -4.4288406 -4.4288111 -4.42876 -4.4287152][-4.4288254 -4.4287949 -4.4287648 -4.4287624 -4.4287486 -4.4287114 -4.4286652 -4.4286504 -4.4287081 -4.4287963 -4.4288373 -4.4288282 -4.4287796 -4.4287262 -4.4286957][-4.4288292 -4.4288211 -4.42881 -4.428813 -4.4288 -4.4287777 -4.4287586 -4.4287682 -4.4288039 -4.4288344 -4.4288278 -4.428791 -4.4287415 -4.4287004 -4.4287014][-4.428813 -4.428834 -4.4288392 -4.4288445 -4.4288311 -4.4288244 -4.4288263 -4.4288425 -4.4288545 -4.4288483 -4.4288144 -4.4287634 -4.4287186 -4.4287071 -4.4287391][-4.4287682 -4.4287987 -4.42881 -4.4288149 -4.4288025 -4.4288096 -4.4288278 -4.4288464 -4.42885 -4.4288282 -4.428781 -4.42873 -4.4287052 -4.4287219 -4.4287648][-4.4287128 -4.4287257 -4.4287262 -4.428731 -4.4287305 -4.4287486 -4.428772 -4.4287934 -4.4287977 -4.4287724 -4.4287281 -4.4286966 -4.4286966 -4.4287195 -4.4287586][-4.4286718 -4.428668 -4.4286618 -4.4286671 -4.4286728 -4.4286866 -4.4286981 -4.4287219 -4.4287372 -4.4287319 -4.4287152 -4.4287019 -4.4286981 -4.4287081 -4.4287395]]...]
INFO - root - 2017-12-10 04:53:21.075118: step 4110, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:04m:56s remains)
INFO - root - 2017-12-10 04:53:23.696448: step 4120, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:05m:12s remains)
INFO - root - 2017-12-10 04:53:26.310877: step 4130, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:57m:24s remains)
INFO - root - 2017-12-10 04:53:28.987505: step 4140, loss = 2.28, batch loss = 2.23 (27.8 examples/sec; 0.288 sec/batch; 26h:14m:21s remains)
INFO - root - 2017-12-10 04:53:31.567288: step 4150, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:08m:24s remains)
INFO - root - 2017-12-10 04:53:34.139124: step 4160, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:42m:02s remains)
INFO - root - 2017-12-10 04:53:36.730526: step 4170, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:51m:09s remains)
INFO - root - 2017-12-10 04:53:39.332036: step 4180, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:56m:24s remains)
INFO - root - 2017-12-10 04:53:41.889536: step 4190, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:54m:12s remains)
INFO - root - 2017-12-10 04:53:44.434128: step 4200, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:15m:19s remains)
2017-12-10 04:53:44.811542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287543 -4.4287653 -4.4287887 -4.4288116 -4.4288206 -4.4287963 -4.4287343 -4.4286504 -4.4286079 -4.4286408 -4.4286747 -4.4286652 -4.4286647 -4.4287133 -4.4287753][-4.4287353 -4.4287548 -4.4287877 -4.42881 -4.428812 -4.428792 -4.4287457 -4.42868 -4.4286413 -4.4286656 -4.4286733 -4.4286323 -4.4286051 -4.4286442 -4.428719][-4.428751 -4.4287796 -4.4288149 -4.4288335 -4.4288249 -4.4287982 -4.4287534 -4.4287043 -4.42868 -4.4286933 -4.428678 -4.428617 -4.4285731 -4.4285927 -4.428659][-4.428792 -4.428823 -4.4288545 -4.4288616 -4.4288368 -4.4287853 -4.4287267 -4.428688 -4.4286819 -4.4287009 -4.4286866 -4.4286375 -4.4286008 -4.4286118 -4.4286551][-4.4288373 -4.4288616 -4.4288816 -4.428874 -4.4288354 -4.4287567 -4.4286661 -4.4286151 -4.4286232 -4.4286661 -4.4286795 -4.4286613 -4.4286437 -4.4286551 -4.4286838][-4.42887 -4.4288883 -4.4289012 -4.4288855 -4.4288325 -4.4287252 -4.4285903 -4.4285026 -4.4285073 -4.4285817 -4.4286432 -4.4286704 -4.4286842 -4.4287047 -4.4287248][-4.4288969 -4.42891 -4.4289203 -4.4289026 -4.4288335 -4.4287004 -4.4285297 -4.4284 -4.4283814 -4.4284654 -4.4285684 -4.4286466 -4.4287024 -4.428741 -4.4287605][-4.4289126 -4.4289265 -4.4289379 -4.4289193 -4.4288478 -4.4287128 -4.4285398 -4.4283843 -4.4283237 -4.4283757 -4.4284859 -4.4286022 -4.4286942 -4.4287548 -4.4287868][-4.4289179 -4.4289322 -4.4289412 -4.4289179 -4.4288526 -4.4287424 -4.4286027 -4.4284577 -4.4283662 -4.4283652 -4.4284444 -4.4285707 -4.4286876 -4.4287686 -4.428813][-4.4289236 -4.4289379 -4.428947 -4.4289289 -4.4288769 -4.42879 -4.428679 -4.4285512 -4.428452 -4.4284134 -4.4284577 -4.4285679 -4.4286866 -4.4287763 -4.4288292][-4.4289222 -4.4289322 -4.4289412 -4.4289346 -4.4289021 -4.428843 -4.4287543 -4.4286461 -4.428555 -4.428504 -4.4285188 -4.4286036 -4.4287133 -4.4288049 -4.4288626][-4.4289293 -4.4289346 -4.4289403 -4.4289427 -4.42893 -4.4288974 -4.4288378 -4.4287562 -4.4286809 -4.42863 -4.4286242 -4.428678 -4.4287682 -4.428853 -4.4289074][-4.4289503 -4.4289494 -4.4289532 -4.4289618 -4.4289627 -4.428947 -4.42891 -4.428854 -4.4287968 -4.4287548 -4.4287429 -4.4287739 -4.4288378 -4.4289031 -4.4289489][-4.4289722 -4.4289694 -4.4289708 -4.4289789 -4.4289861 -4.4289827 -4.4289622 -4.428926 -4.4288859 -4.4288559 -4.4288464 -4.4288616 -4.4289012 -4.4289441 -4.428977][-4.4289761 -4.4289737 -4.4289737 -4.4289818 -4.4289913 -4.4289947 -4.4289861 -4.4289641 -4.42894 -4.42892 -4.4289126 -4.4289212 -4.4289422 -4.4289651 -4.4289856]]...]
INFO - root - 2017-12-10 04:53:47.388680: step 4210, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:25m:22s remains)
INFO - root - 2017-12-10 04:53:49.970859: step 4220, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:58m:07s remains)
INFO - root - 2017-12-10 04:53:52.543734: step 4230, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:18s remains)
INFO - root - 2017-12-10 04:53:55.114830: step 4240, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:49m:12s remains)
INFO - root - 2017-12-10 04:53:57.732079: step 4250, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:39s remains)
INFO - root - 2017-12-10 04:54:00.320158: step 4260, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 22h:17m:17s remains)
INFO - root - 2017-12-10 04:54:02.969335: step 4270, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:47m:08s remains)
INFO - root - 2017-12-10 04:54:05.586053: step 4280, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:23m:34s remains)
INFO - root - 2017-12-10 04:54:08.218164: step 4290, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:31m:00s remains)
INFO - root - 2017-12-10 04:54:10.844334: step 4300, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:39m:53s remains)
2017-12-10 04:54:11.206209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287839 -4.4288 -4.4288077 -4.4288063 -4.42879 -4.4287572 -4.4287186 -4.4286909 -4.4286914 -4.4287176 -4.4287491 -4.4287734 -4.42879 -4.4287944 -4.4287906][-4.4287467 -4.4287772 -4.4287939 -4.4287934 -4.4287677 -4.4287143 -4.4286513 -4.42861 -4.4286141 -4.4286551 -4.4287028 -4.4287438 -4.4287739 -4.4287834 -4.4287786][-4.4287224 -4.4287634 -4.4287848 -4.428781 -4.4287391 -4.4286618 -4.4285736 -4.4285226 -4.4285359 -4.4285932 -4.4286561 -4.4287133 -4.4287567 -4.4287715 -4.4287658][-4.4287233 -4.4287653 -4.4287844 -4.4287672 -4.4287047 -4.428606 -4.4285035 -4.4284515 -4.4284768 -4.4285464 -4.4286194 -4.428688 -4.4287395 -4.4287577 -4.4287491][-4.4287515 -4.4287853 -4.4287906 -4.4287548 -4.42867 -4.4285555 -4.4284492 -4.4284048 -4.4284463 -4.4285283 -4.42861 -4.4286885 -4.4287443 -4.4287605 -4.4287429][-4.4287987 -4.4288149 -4.4288 -4.4287419 -4.4286394 -4.4285154 -4.4284105 -4.4283705 -4.4284267 -4.428524 -4.4286156 -4.4287043 -4.4287624 -4.4287724 -4.4287381][-4.42884 -4.42884 -4.4288077 -4.4287333 -4.42862 -4.4284854 -4.42837 -4.4283204 -4.4283891 -4.4285083 -4.4286175 -4.4287186 -4.4287772 -4.4287791 -4.4287291][-4.4288583 -4.4288473 -4.4288073 -4.4287314 -4.4286203 -4.4284792 -4.4283447 -4.4282794 -4.4283562 -4.4284983 -4.4286251 -4.4287324 -4.4287906 -4.4287906 -4.4287343][-4.4288478 -4.4288282 -4.4287934 -4.4287353 -4.42865 -4.4285212 -4.4283829 -4.428309 -4.4283829 -4.4285326 -4.4286618 -4.428761 -4.4288135 -4.4288168 -4.4287682][-4.4288206 -4.428793 -4.428771 -4.4287429 -4.4286962 -4.4286 -4.4284854 -4.4284248 -4.4284897 -4.4286213 -4.4287262 -4.4288011 -4.4288416 -4.4288454 -4.428812][-4.42879 -4.4287534 -4.4287419 -4.4287438 -4.4287376 -4.4286814 -4.4286 -4.4285579 -4.4286118 -4.4287152 -4.4287887 -4.4288359 -4.4288611 -4.4288659 -4.4288464][-4.4287472 -4.4287052 -4.4287081 -4.4287362 -4.4287643 -4.4287467 -4.4286957 -4.428669 -4.4287062 -4.4287772 -4.4288244 -4.4288535 -4.42887 -4.4288759 -4.4288654][-4.4286938 -4.4286571 -4.4286785 -4.4287305 -4.4287825 -4.4287958 -4.428772 -4.4287586 -4.42878 -4.4288225 -4.4288521 -4.4288697 -4.4288793 -4.4288812 -4.4288669][-4.428647 -4.4286261 -4.4286628 -4.4287252 -4.4287868 -4.4288154 -4.4288106 -4.4288077 -4.4288197 -4.4288435 -4.4288659 -4.4288812 -4.4288878 -4.4288836 -4.4288626][-4.4286189 -4.4286213 -4.4286671 -4.4287286 -4.4287872 -4.4288125 -4.4288082 -4.4288025 -4.4288015 -4.428812 -4.4288392 -4.428865 -4.4288797 -4.428875 -4.4288516]]...]
INFO - root - 2017-12-10 04:54:13.839337: step 4310, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:24m:49s remains)
INFO - root - 2017-12-10 04:54:16.498089: step 4320, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:52m:18s remains)
INFO - root - 2017-12-10 04:54:19.096511: step 4330, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:53m:13s remains)
INFO - root - 2017-12-10 04:54:21.730017: step 4340, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:44m:02s remains)
INFO - root - 2017-12-10 04:54:24.354486: step 4350, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:46m:54s remains)
INFO - root - 2017-12-10 04:54:27.020840: step 4360, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:37m:23s remains)
INFO - root - 2017-12-10 04:54:29.605606: step 4370, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:56m:18s remains)
INFO - root - 2017-12-10 04:54:32.184736: step 4380, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:15m:31s remains)
INFO - root - 2017-12-10 04:54:34.745330: step 4390, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:51m:03s remains)
INFO - root - 2017-12-10 04:54:37.347818: step 4400, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:37m:07s remains)
2017-12-10 04:54:37.745040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288397 -4.4288173 -4.42881 -4.4288297 -4.4288449 -4.4288454 -4.4288411 -4.428844 -4.4288454 -4.4288368 -4.4288273 -4.42882 -4.4288087 -4.4287834 -4.4287596][-4.4287848 -4.4287548 -4.4287448 -4.4287734 -4.4287987 -4.4287992 -4.4287925 -4.4287996 -4.42881 -4.428812 -4.4288063 -4.428802 -4.4287958 -4.4287696 -4.4287372][-4.42874 -4.4287004 -4.42869 -4.4287233 -4.4287558 -4.4287605 -4.4287543 -4.4287615 -4.4287786 -4.4287877 -4.4287853 -4.4287896 -4.428792 -4.4287691 -4.4287357][-4.42872 -4.428668 -4.4286523 -4.4286776 -4.4287095 -4.4287157 -4.4287057 -4.42871 -4.4287319 -4.4287477 -4.4287453 -4.4287519 -4.4287586 -4.4287438 -4.4287176][-4.4287219 -4.4286537 -4.4286294 -4.4286461 -4.4286685 -4.4286666 -4.4286442 -4.4286447 -4.428678 -4.4287019 -4.4286995 -4.4287019 -4.4287081 -4.4287066 -4.4287033][-4.4287558 -4.4286842 -4.4286542 -4.4286642 -4.4286733 -4.4286475 -4.4285994 -4.4285793 -4.4286122 -4.4286513 -4.4286647 -4.4286709 -4.4286828 -4.4287009 -4.4287243][-4.4287777 -4.4287057 -4.4286628 -4.4286551 -4.4286432 -4.4285994 -4.428534 -4.4284878 -4.4285073 -4.4285707 -4.4286203 -4.4286437 -4.42866 -4.4286866 -4.4287281][-4.4287591 -4.4286833 -4.4286237 -4.4285889 -4.4285622 -4.4285278 -4.42847 -4.4284039 -4.4283938 -4.42847 -4.4285536 -4.4285994 -4.42862 -4.4286461 -4.4286942][-4.4287262 -4.4286442 -4.4285712 -4.4285259 -4.4285059 -4.4284954 -4.4284587 -4.4283853 -4.4283395 -4.4283881 -4.4284773 -4.4285479 -4.4285851 -4.4286137 -4.4286537][-4.4287348 -4.4286642 -4.4286017 -4.4285684 -4.4285607 -4.42857 -4.4285617 -4.428515 -4.4284649 -4.428472 -4.4285264 -4.4285879 -4.4286251 -4.428647 -4.4286718][-4.4287677 -4.4287152 -4.4286661 -4.4286447 -4.4286432 -4.4286652 -4.4286828 -4.4286647 -4.4286218 -4.4286056 -4.4286227 -4.4286618 -4.4286952 -4.4287119 -4.428719][-4.4287815 -4.4287343 -4.4286957 -4.428689 -4.4286976 -4.4287276 -4.4287529 -4.4287448 -4.4287057 -4.4286776 -4.428668 -4.4286895 -4.4287248 -4.4287472 -4.4287481][-4.4288025 -4.4287481 -4.4287105 -4.4287109 -4.4287257 -4.42876 -4.4287848 -4.4287767 -4.4287457 -4.4287162 -4.4286957 -4.4287057 -4.4287429 -4.4287777 -4.4287896][-4.4288073 -4.4287424 -4.4287014 -4.4287043 -4.4287229 -4.4287577 -4.4287839 -4.4287782 -4.428761 -4.4287438 -4.4287267 -4.4287338 -4.4287686 -4.4288073 -4.4288287][-4.4288197 -4.4287524 -4.4287124 -4.4287124 -4.4287291 -4.4287548 -4.4287763 -4.4287729 -4.4287672 -4.428762 -4.4287539 -4.4287629 -4.4287925 -4.4288297 -4.4288564]]...]
INFO - root - 2017-12-10 04:54:40.377400: step 4410, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:34m:56s remains)
INFO - root - 2017-12-10 04:54:42.989164: step 4420, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:09m:24s remains)
INFO - root - 2017-12-10 04:54:45.609897: step 4430, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:19m:13s remains)
INFO - root - 2017-12-10 04:54:48.203242: step 4440, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:30m:02s remains)
INFO - root - 2017-12-10 04:54:50.813910: step 4450, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:39m:17s remains)
INFO - root - 2017-12-10 04:54:53.443259: step 4460, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:30m:49s remains)
INFO - root - 2017-12-10 04:54:56.071123: step 4470, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:03m:08s remains)
INFO - root - 2017-12-10 04:54:58.714582: step 4480, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:16m:23s remains)
INFO - root - 2017-12-10 04:55:01.319969: step 4490, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:45m:25s remains)
INFO - root - 2017-12-10 04:55:03.900493: step 4500, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:01s remains)
2017-12-10 04:55:04.244948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289522 -4.4289427 -4.4289289 -4.4289212 -4.4289103 -4.4288979 -4.4288859 -4.4288645 -4.4288468 -4.428864 -4.42889 -4.4289122 -4.4289269 -4.4289446 -4.4289637][-4.4289327 -4.4289269 -4.4289041 -4.4288888 -4.4288712 -4.4288564 -4.4288392 -4.4288 -4.4287686 -4.4287944 -4.4288363 -4.4288769 -4.4289007 -4.4289222 -4.4289465][-4.4289222 -4.4289207 -4.4288883 -4.428863 -4.4288344 -4.428812 -4.4287777 -4.4287119 -4.428678 -4.4287271 -4.4287963 -4.4288573 -4.4288926 -4.4289231 -4.4289484][-4.4289236 -4.428925 -4.4288869 -4.4288473 -4.4288058 -4.4287758 -4.4287152 -4.4286089 -4.4285741 -4.428658 -4.4287567 -4.4288321 -4.4288769 -4.4289184 -4.4289513][-4.4289236 -4.4289188 -4.4288697 -4.4288168 -4.4287639 -4.4287186 -4.4286284 -4.4284892 -4.428462 -4.428575 -4.4287 -4.4287958 -4.4288568 -4.4289083 -4.4289465][-4.4289188 -4.4289055 -4.4288435 -4.4287777 -4.4287128 -4.4286418 -4.42851 -4.4283428 -4.4283485 -4.4285045 -4.4286547 -4.4287705 -4.4288473 -4.4289064 -4.4289479][-4.428915 -4.4288936 -4.4288259 -4.4287438 -4.4286618 -4.4285536 -4.4283514 -4.4281449 -4.4282136 -4.4284387 -4.4286251 -4.4287605 -4.4288483 -4.4289122 -4.4289522][-4.4289107 -4.428894 -4.4288392 -4.4287543 -4.4286571 -4.4285164 -4.4282479 -4.4280005 -4.4281249 -4.4284148 -4.4286251 -4.4287634 -4.4288564 -4.4289188 -4.4289513][-4.4289031 -4.4289031 -4.428874 -4.4288049 -4.4287171 -4.4285879 -4.42834 -4.4281034 -4.4282012 -4.4284692 -4.4286633 -4.4287825 -4.4288645 -4.4289169 -4.4289403][-4.4288936 -4.4289079 -4.42889 -4.428844 -4.4287829 -4.4286866 -4.4285049 -4.4283419 -4.4283948 -4.4285774 -4.4287248 -4.4288135 -4.4288735 -4.4289093 -4.4289241][-4.4288797 -4.4289007 -4.4288874 -4.4288597 -4.4288144 -4.4287415 -4.4286127 -4.4285192 -4.4285612 -4.4286823 -4.4287844 -4.42884 -4.42887 -4.4288926 -4.428906][-4.4288673 -4.4288907 -4.4288788 -4.4288621 -4.4288259 -4.4287677 -4.4286828 -4.4286366 -4.4286776 -4.428762 -4.4288297 -4.4288607 -4.4288673 -4.428874 -4.4288821][-4.42885 -4.4288707 -4.4288678 -4.4288621 -4.4288354 -4.428793 -4.4287443 -4.4287243 -4.4287581 -4.4288077 -4.4288507 -4.428864 -4.4288554 -4.4288492 -4.4288497][-4.4288464 -4.4288616 -4.4288688 -4.4288678 -4.4288464 -4.4288168 -4.4287968 -4.4287939 -4.4288139 -4.4288373 -4.4288578 -4.4288507 -4.4288273 -4.4288125 -4.4288077][-4.4288688 -4.4288831 -4.428895 -4.4288955 -4.4288774 -4.4288549 -4.428843 -4.42884 -4.4288473 -4.4288526 -4.4288592 -4.428843 -4.428812 -4.4287853 -4.4287734]]...]
INFO - root - 2017-12-10 04:55:06.806147: step 4510, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:50m:46s remains)
INFO - root - 2017-12-10 04:55:09.411092: step 4520, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:06m:54s remains)
INFO - root - 2017-12-10 04:55:12.026561: step 4530, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:25m:30s remains)
INFO - root - 2017-12-10 04:55:14.649923: step 4540, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:13m:34s remains)
INFO - root - 2017-12-10 04:55:17.260757: step 4550, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:03m:01s remains)
INFO - root - 2017-12-10 04:55:19.860934: step 4560, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:31m:17s remains)
INFO - root - 2017-12-10 04:55:22.532198: step 4570, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.279 sec/batch; 25h:26m:34s remains)
INFO - root - 2017-12-10 04:55:25.183076: step 4580, loss = 2.28, batch loss = 2.23 (28.8 examples/sec; 0.278 sec/batch; 25h:20m:14s remains)
INFO - root - 2017-12-10 04:55:27.785952: step 4590, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:37m:37s remains)
INFO - root - 2017-12-10 04:55:30.363925: step 4600, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:01m:29s remains)
2017-12-10 04:55:30.741222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288797 -4.4288769 -4.4288797 -4.4288826 -4.4288864 -4.4288864 -4.4288831 -4.4288783 -4.4288745 -4.4288592 -4.4288368 -4.4288158 -4.4288034 -4.4287992 -4.4287977][-4.4288864 -4.4288831 -4.428885 -4.4288874 -4.4288912 -4.4288917 -4.4288864 -4.4288812 -4.4288783 -4.428865 -4.4288421 -4.4288135 -4.4287958 -4.428791 -4.4287949][-4.4288812 -4.4288716 -4.4288688 -4.4288678 -4.4288697 -4.4288707 -4.4288673 -4.4288654 -4.4288688 -4.4288664 -4.42885 -4.428823 -4.4288073 -4.4288077 -4.4288182][-4.4288731 -4.4288549 -4.4288421 -4.4288306 -4.428823 -4.4288158 -4.4288087 -4.4288082 -4.4288216 -4.4288387 -4.42884 -4.4288259 -4.4288182 -4.4288249 -4.4288378][-4.4288383 -4.428813 -4.42879 -4.4287653 -4.4287457 -4.4287219 -4.4286966 -4.4286852 -4.4287033 -4.4287376 -4.428761 -4.4287715 -4.4287868 -4.4288082 -4.4288292][-4.4287515 -4.4287157 -4.4286823 -4.4286427 -4.4286075 -4.4285612 -4.4285069 -4.4284725 -4.4284844 -4.4285297 -4.4285784 -4.4286261 -4.4286742 -4.4287162 -4.4287548][-4.4287119 -4.4286733 -4.4286332 -4.4285808 -4.4285264 -4.4284544 -4.4283657 -4.4282961 -4.4282818 -4.428319 -4.4283838 -4.4284663 -4.4285455 -4.4286089 -4.4286628][-4.4287457 -4.4287157 -4.4286847 -4.4286437 -4.428597 -4.428534 -4.4284511 -4.4283738 -4.4283285 -4.4283252 -4.4283628 -4.4284353 -4.428514 -4.4285769 -4.4286327][-4.4288149 -4.4287906 -4.4287667 -4.4287367 -4.4287076 -4.4286728 -4.4286256 -4.4285722 -4.4285283 -4.4285092 -4.428525 -4.4285703 -4.4286232 -4.4286613 -4.4286952][-4.4288621 -4.4288483 -4.428834 -4.4288158 -4.428803 -4.428792 -4.4287734 -4.4287457 -4.4287157 -4.4287009 -4.42871 -4.4287362 -4.4287639 -4.4287786 -4.4287877][-4.4288936 -4.4288855 -4.4288778 -4.4288731 -4.4288735 -4.4288778 -4.4288783 -4.4288683 -4.428853 -4.4288435 -4.4288487 -4.4288588 -4.4288669 -4.4288673 -4.4288621][-4.4289045 -4.4288979 -4.4288964 -4.4289012 -4.4289107 -4.4289241 -4.428936 -4.4289379 -4.4289336 -4.4289222 -4.4289141 -4.4289041 -4.4288964 -4.42889 -4.4288816][-4.4288778 -4.4288731 -4.4288788 -4.4288912 -4.428905 -4.4289193 -4.4289341 -4.4289427 -4.4289432 -4.428926 -4.4289002 -4.42887 -4.4288492 -4.4288435 -4.4288416][-4.4288378 -4.4288335 -4.4288421 -4.4288578 -4.4288726 -4.428884 -4.4288979 -4.4289088 -4.4289079 -4.4288797 -4.4288383 -4.428792 -4.4287653 -4.4287691 -4.4287839][-4.428823 -4.4288158 -4.4288216 -4.428833 -4.4288435 -4.42885 -4.4288626 -4.428874 -4.4288707 -4.4288373 -4.4287863 -4.4287286 -4.4286962 -4.4287105 -4.4287477]]...]
INFO - root - 2017-12-10 04:55:33.382461: step 4610, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:43m:55s remains)
INFO - root - 2017-12-10 04:55:35.989658: step 4620, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:14m:32s remains)
INFO - root - 2017-12-10 04:55:38.611315: step 4630, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:09m:10s remains)
INFO - root - 2017-12-10 04:55:41.221281: step 4640, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:25m:14s remains)
INFO - root - 2017-12-10 04:55:43.868088: step 4650, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:19m:37s remains)
INFO - root - 2017-12-10 04:55:46.447671: step 4660, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:09m:18s remains)
INFO - root - 2017-12-10 04:55:49.076086: step 4670, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:48m:06s remains)
INFO - root - 2017-12-10 04:55:51.640553: step 4680, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:16m:29s remains)
INFO - root - 2017-12-10 04:55:54.269145: step 4690, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:23s remains)
INFO - root - 2017-12-10 04:55:56.870122: step 4700, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.254 sec/batch; 23h:05m:13s remains)
2017-12-10 04:55:57.201112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288116 -4.4287844 -4.4287868 -4.428803 -4.4288173 -4.4288073 -4.4287744 -4.42875 -4.4287586 -4.4287724 -4.4287491 -4.4287076 -4.4287028 -4.4287481 -4.4287953][-4.4287839 -4.4287658 -4.4287853 -4.4288206 -4.4288411 -4.4288268 -4.4287858 -4.4287543 -4.4287577 -4.4287763 -4.4287744 -4.4287438 -4.4287348 -4.4287777 -4.4288282][-4.4287424 -4.4287453 -4.4287839 -4.4288325 -4.4288511 -4.4288235 -4.4287763 -4.4287457 -4.4287472 -4.4287691 -4.4287782 -4.428751 -4.4287357 -4.4287767 -4.4288354][-4.4287004 -4.4287229 -4.4287744 -4.4288278 -4.4288411 -4.4287925 -4.4287329 -4.4287162 -4.4287348 -4.4287653 -4.4287753 -4.4287443 -4.4287257 -4.4287663 -4.4288306][-4.42869 -4.4287081 -4.4287629 -4.4288158 -4.4288168 -4.4287314 -4.4286427 -4.4286451 -4.4287081 -4.42876 -4.4287658 -4.4287329 -4.42872 -4.4287682 -4.4288378][-4.4287043 -4.4287062 -4.4287534 -4.4287963 -4.428762 -4.4286165 -4.4284768 -4.4285016 -4.428628 -4.4287157 -4.4287233 -4.4286876 -4.428688 -4.4287581 -4.4288473][-4.4287252 -4.4287176 -4.4287529 -4.4287658 -4.4286804 -4.4284668 -4.428277 -4.4283228 -4.42851 -4.4286318 -4.4286375 -4.4285941 -4.4286141 -4.4287143 -4.4288321][-4.4287677 -4.4287605 -4.4287834 -4.4287663 -4.4286413 -4.4284 -4.428196 -4.428246 -4.4284573 -4.4285893 -4.4285793 -4.428524 -4.4285603 -4.4286885 -4.4288273][-4.4288149 -4.4288149 -4.4288311 -4.4288034 -4.4286804 -4.4284692 -4.4282913 -4.4283295 -4.4285111 -4.4286318 -4.4286103 -4.4285479 -4.428586 -4.4287171 -4.4288559][-4.4288425 -4.42885 -4.428864 -4.4288416 -4.4287477 -4.4285874 -4.4284449 -4.4284587 -4.4285936 -4.4286952 -4.4286838 -4.4286275 -4.428659 -4.4287753 -4.428895][-4.4288621 -4.4288683 -4.4288797 -4.4288683 -4.4288092 -4.4287014 -4.4285893 -4.4285827 -4.4286766 -4.4287562 -4.4287591 -4.4287162 -4.4287348 -4.42883 -4.4289231][-4.4288683 -4.4288754 -4.4288869 -4.4288893 -4.42886 -4.4287958 -4.4287162 -4.4287014 -4.4287648 -4.4288268 -4.4288387 -4.4288044 -4.4288082 -4.4288769 -4.4289432][-4.4288692 -4.4288826 -4.4288988 -4.4289083 -4.4288945 -4.4288607 -4.4288135 -4.4288063 -4.4288497 -4.4288974 -4.4289074 -4.4288769 -4.428875 -4.4289207 -4.4289622][-4.4288797 -4.4289 -4.4289169 -4.4289269 -4.4289207 -4.4289007 -4.4288783 -4.4288836 -4.4289193 -4.4289532 -4.4289556 -4.4289303 -4.4289269 -4.4289541 -4.4289722][-4.4288898 -4.42891 -4.4289231 -4.4289341 -4.4289346 -4.4289231 -4.4289117 -4.4289217 -4.4289527 -4.4289756 -4.4289746 -4.428956 -4.4289503 -4.4289618 -4.4289622]]...]
INFO - root - 2017-12-10 04:55:59.798492: step 4710, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:35m:44s remains)
INFO - root - 2017-12-10 04:56:02.390340: step 4720, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:04m:32s remains)
INFO - root - 2017-12-10 04:56:05.007486: step 4730, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:58m:06s remains)
INFO - root - 2017-12-10 04:56:07.632333: step 4740, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:18m:04s remains)
INFO - root - 2017-12-10 04:56:10.216416: step 4750, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:04m:22s remains)
INFO - root - 2017-12-10 04:56:12.806888: step 4760, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:43m:41s remains)
INFO - root - 2017-12-10 04:56:15.388379: step 4770, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:14m:44s remains)
INFO - root - 2017-12-10 04:56:17.982582: step 4780, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:29m:19s remains)
INFO - root - 2017-12-10 04:56:20.616783: step 4790, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 25h:00m:29s remains)
INFO - root - 2017-12-10 04:56:23.267420: step 4800, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:29m:32s remains)
2017-12-10 04:56:23.655952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289117 -4.4289026 -4.4288921 -4.4288855 -4.4288874 -4.4288974 -4.4289145 -4.4289327 -4.4289494 -4.42896 -4.4289556 -4.4289403 -4.428926 -4.4289293 -4.4289474][-4.4288468 -4.4288459 -4.42884 -4.4288354 -4.42884 -4.4288535 -4.4288735 -4.4288955 -4.4289222 -4.4289432 -4.4289474 -4.4289365 -4.428925 -4.4289293 -4.4289503][-4.4287696 -4.4287815 -4.4287891 -4.4287896 -4.428791 -4.4287972 -4.42881 -4.428833 -4.4288688 -4.4289026 -4.4289203 -4.4289179 -4.4289112 -4.42892 -4.4289432][-4.4287033 -4.42873 -4.42875 -4.4287548 -4.4287467 -4.4287381 -4.4287376 -4.4287548 -4.4287968 -4.4288416 -4.428874 -4.4288859 -4.4288874 -4.4289017 -4.4289284][-4.4286642 -4.4287014 -4.4287267 -4.428731 -4.4287109 -4.4286809 -4.42866 -4.4286661 -4.4287152 -4.4287724 -4.4288182 -4.4288468 -4.4288597 -4.4288774 -4.4289045][-4.4286628 -4.4286981 -4.4287119 -4.4286995 -4.4286604 -4.4286013 -4.42855 -4.4285407 -4.4286079 -4.4286909 -4.4287534 -4.4287958 -4.4288197 -4.42884 -4.4288659][-4.4287009 -4.4287205 -4.4287086 -4.4286652 -4.4285965 -4.4285 -4.4284 -4.4283681 -4.4284682 -4.4286017 -4.4286923 -4.4287457 -4.4287734 -4.4287915 -4.428813][-4.428762 -4.4287677 -4.4287333 -4.4286633 -4.428565 -4.4284344 -4.4282794 -4.428205 -4.42832 -4.4285035 -4.4286304 -4.4286957 -4.4287238 -4.42874 -4.428761][-4.4288263 -4.4288278 -4.4287868 -4.4287081 -4.428596 -4.4284511 -4.428267 -4.428153 -4.4282403 -4.4284277 -4.4285703 -4.4286442 -4.428678 -4.4287009 -4.4287271][-4.4288774 -4.4288793 -4.4288387 -4.4287682 -4.4286714 -4.428546 -4.4283819 -4.4282637 -4.4282994 -4.4284344 -4.4285526 -4.4286194 -4.4286547 -4.4286838 -4.4287148][-4.4289117 -4.428915 -4.4288855 -4.4288306 -4.42876 -4.428669 -4.4285522 -4.4284616 -4.4284592 -4.4285326 -4.4286065 -4.4286494 -4.4286718 -4.4286919 -4.4287124][-4.4289269 -4.4289351 -4.4289207 -4.4288821 -4.42883 -4.428771 -4.4287009 -4.4286485 -4.42864 -4.4286747 -4.4287171 -4.4287405 -4.4287467 -4.4287472 -4.4287424][-4.4289184 -4.4289312 -4.4289289 -4.4289064 -4.428874 -4.4288406 -4.4288068 -4.42879 -4.4287934 -4.4288092 -4.42883 -4.4288383 -4.4288373 -4.4288311 -4.428812][-4.4288836 -4.428896 -4.4289 -4.4288898 -4.4288754 -4.4288659 -4.4288645 -4.4288774 -4.4288926 -4.4289031 -4.4289093 -4.4289031 -4.4289002 -4.4288988 -4.4288845][-4.4288464 -4.4288492 -4.4288568 -4.4288564 -4.4288549 -4.4288607 -4.4288812 -4.4289126 -4.4289355 -4.4289465 -4.428946 -4.4289346 -4.4289331 -4.428937 -4.4289312]]...]
INFO - root - 2017-12-10 04:56:26.241265: step 4810, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:09m:32s remains)
INFO - root - 2017-12-10 04:56:28.875828: step 4820, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:32s remains)
INFO - root - 2017-12-10 04:56:31.481651: step 4830, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:34m:51s remains)
INFO - root - 2017-12-10 04:56:34.083999: step 4840, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:31s remains)
INFO - root - 2017-12-10 04:56:36.743893: step 4850, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:39m:51s remains)
INFO - root - 2017-12-10 04:56:39.347415: step 4860, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:48s remains)
INFO - root - 2017-12-10 04:56:41.946453: step 4870, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:47m:44s remains)
INFO - root - 2017-12-10 04:56:44.546819: step 4880, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:23m:06s remains)
INFO - root - 2017-12-10 04:56:47.195461: step 4890, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.271 sec/batch; 24h:37m:58s remains)
INFO - root - 2017-12-10 04:56:49.828319: step 4900, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:33m:40s remains)
2017-12-10 04:56:50.187041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288616 -4.4288292 -4.4288092 -4.4287953 -4.4287953 -4.4288182 -4.4288421 -4.4288611 -4.4288859 -4.4288988 -4.4288912 -4.4288783 -4.428865 -4.4288554 -4.4288545][-4.4288845 -4.4288545 -4.4288349 -4.4288273 -4.4288316 -4.4288507 -4.4288745 -4.4288988 -4.4289255 -4.4289412 -4.4289374 -4.4289241 -4.4289041 -4.4288836 -4.4288692][-4.4288864 -4.428853 -4.4288273 -4.4288149 -4.4288158 -4.428823 -4.428843 -4.4288731 -4.428906 -4.4289293 -4.4289322 -4.4289236 -4.4289041 -4.4288778 -4.428854][-4.4288635 -4.4288206 -4.4287844 -4.4287658 -4.4287539 -4.4287443 -4.4287515 -4.4287796 -4.4288125 -4.4288406 -4.4288526 -4.4288564 -4.4288516 -4.4288335 -4.4288116][-4.4288144 -4.4287558 -4.4287062 -4.4286857 -4.4286647 -4.4286432 -4.4286466 -4.4286675 -4.428689 -4.4287105 -4.4287291 -4.4287491 -4.4287729 -4.4287858 -4.4287882][-4.4287481 -4.428678 -4.4286246 -4.4286118 -4.4285951 -4.428576 -4.4285889 -4.4286051 -4.4286032 -4.4286056 -4.4286246 -4.4286551 -4.4287024 -4.4287467 -4.4287782][-4.4287047 -4.4286261 -4.4285736 -4.4285636 -4.4285488 -4.4285336 -4.4285569 -4.4285712 -4.4285469 -4.4285278 -4.4285526 -4.4285941 -4.4286494 -4.4287119 -4.4287658][-4.4287329 -4.4286413 -4.4285812 -4.4285588 -4.4285316 -4.4285169 -4.4285488 -4.42857 -4.4285355 -4.4285073 -4.4285326 -4.4285717 -4.42862 -4.4286895 -4.4287591][-4.4288154 -4.4287281 -4.4286642 -4.4286261 -4.4285975 -4.4285936 -4.4286337 -4.4286618 -4.4286389 -4.4286141 -4.4286213 -4.4286323 -4.4286518 -4.4287052 -4.42877][-4.4288559 -4.4287853 -4.4287262 -4.4286838 -4.4286637 -4.4286766 -4.4287281 -4.4287724 -4.4287691 -4.4287486 -4.4287415 -4.4287305 -4.4287224 -4.4287505 -4.428791][-4.4288607 -4.4287972 -4.4287324 -4.428679 -4.428659 -4.4286857 -4.4287491 -4.4288111 -4.4288321 -4.4288268 -4.428822 -4.4288011 -4.4287767 -4.4287872 -4.4288058][-4.4288692 -4.4287968 -4.4287124 -4.4286356 -4.4286051 -4.4286275 -4.4286976 -4.4287786 -4.4288268 -4.4288464 -4.4288564 -4.4288397 -4.4288154 -4.4288192 -4.4288182][-4.4288778 -4.4288044 -4.4287024 -4.4286003 -4.4285488 -4.4285502 -4.428606 -4.4286847 -4.4287429 -4.4287815 -4.4288158 -4.4288158 -4.4288087 -4.428823 -4.42881][-4.428894 -4.428833 -4.4287357 -4.4286222 -4.4285431 -4.4285045 -4.4285173 -4.4285617 -4.4286003 -4.4286432 -4.4287047 -4.428731 -4.4287519 -4.4287944 -4.4287887][-4.4289193 -4.428884 -4.4288154 -4.4287214 -4.42863 -4.42855 -4.4285121 -4.4285088 -4.4285097 -4.4285436 -4.4286232 -4.4286737 -4.4287167 -4.4287806 -4.428793]]...]
INFO - root - 2017-12-10 04:56:52.848460: step 4910, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:48m:48s remains)
INFO - root - 2017-12-10 04:56:55.424950: step 4920, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:57m:55s remains)
INFO - root - 2017-12-10 04:56:58.028899: step 4930, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:18m:38s remains)
INFO - root - 2017-12-10 04:57:00.587350: step 4940, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:18m:51s remains)
INFO - root - 2017-12-10 04:57:03.189432: step 4950, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:45m:02s remains)
INFO - root - 2017-12-10 04:57:05.741551: step 4960, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:52m:41s remains)
INFO - root - 2017-12-10 04:57:08.358697: step 4970, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:49m:40s remains)
INFO - root - 2017-12-10 04:57:10.940976: step 4980, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:31m:47s remains)
INFO - root - 2017-12-10 04:57:13.574606: step 4990, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:18m:01s remains)
INFO - root - 2017-12-10 04:57:16.196989: step 5000, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:56m:15s remains)
2017-12-10 04:57:16.515634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287467 -4.4287724 -4.4287839 -4.4287405 -4.4286542 -4.4285994 -4.4285941 -4.4286304 -4.4286704 -4.42871 -4.4287586 -4.4287996 -4.4288363 -4.4288812 -4.4289012][-4.4287381 -4.4287777 -4.4287877 -4.4287405 -4.428659 -4.4286041 -4.428597 -4.4286232 -4.4286604 -4.4286976 -4.4287419 -4.4287815 -4.4288158 -4.4288597 -4.4288826][-4.4287362 -4.4287796 -4.4287887 -4.4287405 -4.4286666 -4.4286218 -4.4286184 -4.4286337 -4.4286642 -4.4286981 -4.4287319 -4.4287605 -4.4287896 -4.42883 -4.4288588][-4.4287 -4.4287429 -4.4287596 -4.4287281 -4.428668 -4.4286304 -4.4286251 -4.4286289 -4.4286594 -4.4286971 -4.4287286 -4.4287438 -4.4287653 -4.4288039 -4.4288397][-4.4286547 -4.4287 -4.428731 -4.4287224 -4.4286847 -4.4286518 -4.4286208 -4.428597 -4.4286394 -4.4286928 -4.428719 -4.4287143 -4.4287219 -4.4287581 -4.4288015][-4.4286394 -4.4287 -4.4287434 -4.4287438 -4.4287095 -4.4286633 -4.4285846 -4.4285226 -4.4285874 -4.4286618 -4.4286795 -4.4286628 -4.4286695 -4.428709 -4.4287567][-4.4286604 -4.4287162 -4.4287553 -4.4287453 -4.4286876 -4.4285975 -4.4284577 -4.4283648 -4.4284797 -4.4285965 -4.4286275 -4.428617 -4.4286342 -4.4286847 -4.4287319][-4.4286757 -4.4287171 -4.4287443 -4.4287114 -4.4286113 -4.428442 -4.428195 -4.4280834 -4.4283047 -4.4285088 -4.4285817 -4.4285851 -4.4286051 -4.4286532 -4.4286947][-4.4286866 -4.4287019 -4.4287071 -4.4286537 -4.4285331 -4.4283204 -4.4280114 -4.4279118 -4.4282279 -4.4284873 -4.4285717 -4.4285607 -4.4285517 -4.4285822 -4.4286294][-4.4287329 -4.4287271 -4.428719 -4.42866 -4.4285583 -4.4283953 -4.4281874 -4.4281387 -4.4283829 -4.4285736 -4.4286122 -4.428565 -4.4285159 -4.4285226 -4.4285593][-4.4288073 -4.4287887 -4.4287772 -4.4287276 -4.4286537 -4.4285522 -4.4284477 -4.4284248 -4.4285612 -4.4286637 -4.428659 -4.4285903 -4.4285288 -4.4285226 -4.4285407][-4.4288564 -4.42883 -4.428823 -4.4287853 -4.4287257 -4.42866 -4.4286127 -4.4285975 -4.4286623 -4.4287109 -4.4286814 -4.4286013 -4.4285431 -4.4285445 -4.428556][-4.4288735 -4.4288568 -4.4288583 -4.4288359 -4.4287896 -4.4287457 -4.4287243 -4.4287071 -4.4287257 -4.4287424 -4.4287071 -4.4286332 -4.4285793 -4.428576 -4.4285817][-4.4288988 -4.4288883 -4.42889 -4.4288769 -4.4288464 -4.4288192 -4.4288111 -4.4287939 -4.4287915 -4.4287987 -4.4287753 -4.4287276 -4.4286833 -4.42866 -4.4286456][-4.4289222 -4.4289131 -4.4289107 -4.428894 -4.4288716 -4.4288611 -4.42886 -4.4288421 -4.4288363 -4.4288468 -4.4288354 -4.4288106 -4.4287877 -4.4287567 -4.4287319]]...]
INFO - root - 2017-12-10 04:57:19.141523: step 5010, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 23h:03m:17s remains)
INFO - root - 2017-12-10 04:57:21.732697: step 5020, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:32m:02s remains)
INFO - root - 2017-12-10 04:57:24.372281: step 5030, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:25m:49s remains)
INFO - root - 2017-12-10 04:57:26.982585: step 5040, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:56m:08s remains)
INFO - root - 2017-12-10 04:57:29.566287: step 5050, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:11m:42s remains)
INFO - root - 2017-12-10 04:57:32.148400: step 5060, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:55m:52s remains)
INFO - root - 2017-12-10 04:57:34.794539: step 5070, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:09m:04s remains)
INFO - root - 2017-12-10 04:57:37.394074: step 5080, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:46m:09s remains)
INFO - root - 2017-12-10 04:57:40.045139: step 5090, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:08m:24s remains)
INFO - root - 2017-12-10 04:57:42.610955: step 5100, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:26m:17s remains)
2017-12-10 04:57:42.964423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289012 -4.4289122 -4.4289131 -4.4289069 -4.428895 -4.4288807 -4.4288759 -4.4288883 -4.428896 -4.4288917 -4.4288735 -4.4288468 -4.42882 -4.4288034 -4.4288063][-4.4288654 -4.428874 -4.4288726 -4.4288616 -4.4288483 -4.4288378 -4.4288406 -4.4288626 -4.4288807 -4.4288769 -4.4288492 -4.4288135 -4.4287868 -4.4287777 -4.4287839][-4.4288564 -4.428863 -4.4288626 -4.4288487 -4.4288292 -4.4288144 -4.4288182 -4.4288435 -4.4288707 -4.428875 -4.4288492 -4.4288106 -4.4287796 -4.4287672 -4.428771][-4.4288721 -4.4288764 -4.4288764 -4.4288597 -4.4288292 -4.4288006 -4.4287939 -4.4288154 -4.4288483 -4.4288673 -4.4288597 -4.4288349 -4.4288068 -4.4287891 -4.4287844][-4.4288988 -4.4289002 -4.4288955 -4.4288712 -4.4288268 -4.4287806 -4.4287577 -4.4287686 -4.4288006 -4.4288349 -4.4288521 -4.4288507 -4.4288378 -4.4288273 -4.4288239][-4.4289236 -4.4289193 -4.4289036 -4.4288669 -4.42881 -4.4287491 -4.4287119 -4.4287167 -4.4287505 -4.4287944 -4.4288268 -4.4288459 -4.4288521 -4.4288592 -4.4288697][-4.428936 -4.4289236 -4.428895 -4.4288425 -4.4287758 -4.4287071 -4.428668 -4.42868 -4.4287205 -4.4287672 -4.4288044 -4.4288316 -4.4288526 -4.4288797 -4.4289088][-4.4289308 -4.4289069 -4.4288607 -4.428792 -4.4287205 -4.4286571 -4.4286304 -4.4286618 -4.4287171 -4.4287663 -4.4287987 -4.42882 -4.4288454 -4.4288859 -4.4289289][-4.42891 -4.4288716 -4.4288058 -4.428721 -4.4286423 -4.4285855 -4.42858 -4.4286423 -4.428721 -4.4287753 -4.4288068 -4.428823 -4.4288478 -4.4288926 -4.4289403][-4.4288912 -4.428843 -4.4287643 -4.4286652 -4.4285784 -4.4285259 -4.4285383 -4.4286284 -4.4287276 -4.4287887 -4.42882 -4.428834 -4.4288583 -4.4289007 -4.4289422][-4.4288888 -4.4288425 -4.4287667 -4.4286709 -4.4285865 -4.4285383 -4.4285579 -4.4286547 -4.4287586 -4.4288206 -4.4288507 -4.42886 -4.4288788 -4.4289112 -4.4289408][-4.4289036 -4.4288692 -4.4288116 -4.4287391 -4.4286742 -4.4286375 -4.4286509 -4.4287243 -4.428812 -4.4288654 -4.4288893 -4.4288936 -4.428905 -4.4289265 -4.4289432][-4.4289255 -4.4289069 -4.428875 -4.4288359 -4.4288006 -4.4287786 -4.4287839 -4.4288263 -4.42888 -4.4289126 -4.4289255 -4.4289269 -4.4289336 -4.4289446 -4.4289494][-4.4289489 -4.4289408 -4.4289289 -4.4289174 -4.4289074 -4.4289021 -4.4289045 -4.4289236 -4.4289479 -4.4289565 -4.4289551 -4.4289508 -4.4289532 -4.428957 -4.4289536][-4.4289665 -4.428966 -4.428966 -4.428968 -4.4289708 -4.4289718 -4.4289718 -4.428977 -4.4289827 -4.4289765 -4.4289665 -4.4289603 -4.4289618 -4.4289622 -4.4289556]]...]
INFO - root - 2017-12-10 04:57:45.589207: step 5110, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:27m:14s remains)
INFO - root - 2017-12-10 04:57:48.177397: step 5120, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:19m:48s remains)
INFO - root - 2017-12-10 04:57:50.803661: step 5130, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:55m:22s remains)
INFO - root - 2017-12-10 04:57:53.368255: step 5140, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:27m:22s remains)
INFO - root - 2017-12-10 04:57:55.940542: step 5150, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:28m:50s remains)
INFO - root - 2017-12-10 04:57:58.513719: step 5160, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 23h:08m:03s remains)
INFO - root - 2017-12-10 04:58:01.053416: step 5170, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:22m:36s remains)
INFO - root - 2017-12-10 04:58:03.666844: step 5180, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:38m:35s remains)
INFO - root - 2017-12-10 04:58:06.358286: step 5190, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:13m:43s remains)
INFO - root - 2017-12-10 04:58:08.962144: step 5200, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:24m:17s remains)
2017-12-10 04:58:09.302597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289794 -4.4289794 -4.4289808 -4.4289804 -4.4289784 -4.4289784 -4.4289784 -4.4289818 -4.4289856 -4.4289894 -4.4289913 -4.4289932 -4.4289956 -4.428998 -4.4290023][-4.4289494 -4.4289522 -4.428956 -4.428956 -4.4289532 -4.4289508 -4.4289484 -4.428956 -4.4289641 -4.428966 -4.4289646 -4.4289632 -4.4289641 -4.428968 -4.4289765][-4.4289093 -4.428915 -4.4289203 -4.4289203 -4.428916 -4.4289083 -4.4288993 -4.4289122 -4.4289255 -4.4289212 -4.42891 -4.4289026 -4.4289 -4.4289055 -4.4289236][-4.4288692 -4.4288764 -4.4288793 -4.4288683 -4.4288545 -4.4288354 -4.4288116 -4.4288325 -4.4288597 -4.428853 -4.428833 -4.4288182 -4.4288116 -4.4288254 -4.4288578][-4.4288259 -4.4288297 -4.42883 -4.4288125 -4.4287882 -4.4287462 -4.4286966 -4.4287291 -4.428781 -4.4287772 -4.4287496 -4.4287281 -4.4287195 -4.4287472 -4.4287963][-4.4287853 -4.4287829 -4.428772 -4.4287395 -4.4286947 -4.4286132 -4.4285126 -4.4285541 -4.4286485 -4.4286709 -4.4286594 -4.4286485 -4.4286461 -4.4286923 -4.4287624][-4.4287453 -4.428731 -4.4287014 -4.4286456 -4.4285769 -4.4284472 -4.4282894 -4.4283543 -4.4285188 -4.4285908 -4.4286175 -4.428637 -4.4286485 -4.4287019 -4.4287772][-4.4287148 -4.4286952 -4.4286637 -4.428606 -4.4285455 -4.4284396 -4.4283142 -4.4284015 -4.4285688 -4.4286485 -4.42869 -4.428719 -4.4287286 -4.4287696 -4.4288278][-4.4287376 -4.4287343 -4.4287305 -4.4287033 -4.428678 -4.4286251 -4.4285469 -4.4286017 -4.4287057 -4.42876 -4.4287939 -4.428813 -4.4288106 -4.428834 -4.4288707][-4.4287858 -4.4287944 -4.4288068 -4.4288 -4.4287953 -4.42877 -4.4287186 -4.4287467 -4.4288087 -4.4288464 -4.4288707 -4.4288826 -4.4288754 -4.4288836 -4.4288993][-4.4288363 -4.4288449 -4.4288578 -4.4288549 -4.4288559 -4.4288445 -4.4288116 -4.4288311 -4.428874 -4.4289002 -4.4289203 -4.4289317 -4.4289284 -4.4289284 -4.42893][-4.4288878 -4.4288926 -4.4289012 -4.4288974 -4.428895 -4.4288859 -4.4288626 -4.4288745 -4.4289007 -4.428915 -4.4289336 -4.4289503 -4.4289522 -4.4289489 -4.4289479][-4.4289331 -4.4289331 -4.4289336 -4.4289265 -4.4289203 -4.4289122 -4.428896 -4.4289017 -4.428915 -4.4289231 -4.4289384 -4.4289575 -4.4289608 -4.4289565 -4.428957][-4.4289641 -4.4289632 -4.4289622 -4.4289565 -4.4289527 -4.4289479 -4.4289389 -4.4289412 -4.4289465 -4.42895 -4.4289603 -4.4289732 -4.4289751 -4.4289713 -4.4289708][-4.428988 -4.428987 -4.4289856 -4.4289818 -4.4289794 -4.4289765 -4.4289727 -4.4289737 -4.4289761 -4.4289784 -4.4289842 -4.4289918 -4.4289932 -4.42899 -4.428988]]...]
INFO - root - 2017-12-10 04:58:11.915149: step 5210, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:47m:16s remains)
INFO - root - 2017-12-10 04:58:14.532622: step 5220, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:22m:04s remains)
INFO - root - 2017-12-10 04:58:17.196659: step 5230, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.273 sec/batch; 24h:46m:32s remains)
INFO - root - 2017-12-10 04:58:19.784726: step 5240, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:07m:16s remains)
INFO - root - 2017-12-10 04:58:22.381497: step 5250, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:11m:06s remains)
INFO - root - 2017-12-10 04:58:25.004410: step 5260, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:53m:52s remains)
INFO - root - 2017-12-10 04:58:27.602172: step 5270, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:25m:05s remains)
INFO - root - 2017-12-10 04:58:30.242349: step 5280, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:26m:33s remains)
INFO - root - 2017-12-10 04:58:32.888138: step 5290, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:18m:50s remains)
INFO - root - 2017-12-10 04:58:35.487361: step 5300, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:58m:55s remains)
2017-12-10 04:58:35.831085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289775 -4.4289508 -4.4288988 -4.428812 -4.4287014 -4.428607 -4.4285541 -4.428576 -4.4286375 -4.4286752 -4.4287014 -4.4287109 -4.4287229 -4.4287457 -4.4287882][-4.4289742 -4.4289484 -4.4288969 -4.4287987 -4.4286709 -4.428556 -4.4284697 -4.4284706 -4.428544 -4.4286094 -4.4286551 -4.4286685 -4.4286666 -4.4286828 -4.4287353][-4.4289746 -4.4289503 -4.428895 -4.4287858 -4.4286413 -4.4285054 -4.428391 -4.4283814 -4.4284663 -4.42855 -4.4285955 -4.4285955 -4.4285655 -4.4285622 -4.4286222][-4.4289761 -4.4289575 -4.4289041 -4.4287982 -4.4286509 -4.4285064 -4.4283848 -4.4283824 -4.4284687 -4.42854 -4.4285631 -4.4285421 -4.4284744 -4.4284325 -4.4284964][-4.4289756 -4.4289579 -4.428905 -4.428803 -4.4286628 -4.4285226 -4.4284153 -4.4284248 -4.4285078 -4.428565 -4.42857 -4.4285312 -4.4284339 -4.4283495 -4.4284024][-4.4289742 -4.4289484 -4.4288869 -4.4287829 -4.4286513 -4.4285178 -4.428411 -4.4284215 -4.4285088 -4.4285741 -4.4285889 -4.4285583 -4.4284558 -4.4283543 -4.4283853][-4.4289703 -4.4289341 -4.4288573 -4.4287457 -4.428617 -4.428483 -4.4283657 -4.42837 -4.4284668 -4.4285569 -4.428607 -4.4286022 -4.428514 -4.428411 -4.4284158][-4.4289708 -4.4289289 -4.4288435 -4.4287238 -4.428597 -4.4284525 -4.4283142 -4.428298 -4.4284058 -4.4285274 -4.4286146 -4.4286427 -4.428586 -4.4284973 -4.4284806][-4.4289732 -4.4289355 -4.4288545 -4.4287443 -4.4286366 -4.4284973 -4.4283471 -4.4283195 -4.4284286 -4.4285555 -4.4286523 -4.428699 -4.4286809 -4.4286261 -4.4286][-4.428968 -4.428936 -4.4288635 -4.4287729 -4.4286923 -4.4285822 -4.4284577 -4.4284425 -4.4285412 -4.4286447 -4.4287186 -4.4287653 -4.4287734 -4.4287505 -4.4287276][-4.4289589 -4.428926 -4.4288597 -4.4287825 -4.4287252 -4.4286537 -4.4285655 -4.4285688 -4.42865 -4.4287214 -4.4287677 -4.428802 -4.4288177 -4.4288116 -4.4288034][-4.4289489 -4.4289103 -4.4288445 -4.4287734 -4.4287295 -4.42869 -4.4286389 -4.4286609 -4.4287333 -4.4287891 -4.4288173 -4.4288321 -4.4288373 -4.4288354 -4.4288378][-4.4289412 -4.4288969 -4.4288306 -4.428762 -4.4287229 -4.4286904 -4.4286532 -4.4286866 -4.4287653 -4.428823 -4.4288507 -4.4288626 -4.4288635 -4.4288578 -4.4288588][-4.4289403 -4.4288974 -4.42883 -4.4287629 -4.4287224 -4.4286785 -4.4286261 -4.428659 -4.4287438 -4.4288025 -4.4288344 -4.4288507 -4.4288568 -4.4288549 -4.4288535][-4.4289432 -4.4289069 -4.4288449 -4.42878 -4.4287329 -4.4286723 -4.4285994 -4.4286222 -4.42871 -4.428771 -4.4288049 -4.4288254 -4.4288354 -4.4288349 -4.428833]]...]
INFO - root - 2017-12-10 04:58:38.462516: step 5310, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:47m:49s remains)
INFO - root - 2017-12-10 04:58:41.109747: step 5320, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:26m:54s remains)
INFO - root - 2017-12-10 04:58:43.737498: step 5330, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:31m:13s remains)
INFO - root - 2017-12-10 04:58:46.322095: step 5340, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:50m:16s remains)
INFO - root - 2017-12-10 04:58:48.880756: step 5350, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:21m:00s remains)
INFO - root - 2017-12-10 04:58:51.508297: step 5360, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:57m:59s remains)
INFO - root - 2017-12-10 04:58:54.147828: step 5370, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:05m:55s remains)
INFO - root - 2017-12-10 04:58:56.692117: step 5380, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:39m:23s remains)
INFO - root - 2017-12-10 04:58:59.305691: step 5390, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:56m:32s remains)
INFO - root - 2017-12-10 04:59:01.865698: step 5400, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:58m:02s remains)
2017-12-10 04:59:02.242858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289494 -4.4289403 -4.4289331 -4.428925 -4.42892 -4.4289179 -4.4289112 -4.4289041 -4.4288893 -4.4288731 -4.4288592 -4.4288583 -4.4288745 -4.4288936 -4.4289112][-4.4289737 -4.4289722 -4.4289689 -4.4289589 -4.42895 -4.428946 -4.428937 -4.4289317 -4.4289145 -4.4288898 -4.4288683 -4.4288588 -4.4288659 -4.4288778 -4.4288917][-4.4289756 -4.4289742 -4.42897 -4.4289565 -4.4289436 -4.42894 -4.4289355 -4.42894 -4.4289322 -4.4289126 -4.4288936 -4.428875 -4.4288669 -4.4288654 -4.42887][-4.42895 -4.4289455 -4.4289403 -4.4289236 -4.4289088 -4.428906 -4.4289107 -4.4289284 -4.4289336 -4.4289265 -4.4289088 -4.4288778 -4.4288549 -4.4288473 -4.4288468][-4.4289231 -4.4289093 -4.4288979 -4.4288712 -4.4288397 -4.4288211 -4.4288282 -4.4288697 -4.4288983 -4.42891 -4.4288974 -4.428865 -4.4288387 -4.4288316 -4.428834][-4.4288945 -4.4288645 -4.4288344 -4.4287834 -4.4287243 -4.4286728 -4.4286675 -4.4287319 -4.4287934 -4.428834 -4.4288478 -4.42884 -4.4288344 -4.4288354 -4.4288397][-4.4288726 -4.4288316 -4.4287753 -4.4286923 -4.4285941 -4.428493 -4.428452 -4.4285259 -4.4286237 -4.4287047 -4.4287634 -4.4288 -4.4288292 -4.4288521 -4.428863][-4.4288573 -4.4288096 -4.4287319 -4.4286122 -4.4284644 -4.4283023 -4.4281979 -4.4282575 -4.4283867 -4.4285197 -4.428638 -4.4287324 -4.428803 -4.4288521 -4.4288759][-4.4288664 -4.4288306 -4.4287524 -4.4286275 -4.428472 -4.4282994 -4.4281659 -4.4281855 -4.4283032 -4.4284453 -4.4285803 -4.4286919 -4.4287767 -4.428833 -4.4288616][-4.4289122 -4.4289017 -4.4288578 -4.4287796 -4.428678 -4.4285641 -4.4284573 -4.4284306 -4.4284749 -4.428546 -4.428628 -4.4287019 -4.4287639 -4.4288068 -4.4288311][-4.4289222 -4.4289293 -4.4289131 -4.4288778 -4.4288325 -4.4287724 -4.4286971 -4.4286456 -4.4286237 -4.4286251 -4.4286542 -4.4286833 -4.428721 -4.4287577 -4.4287863][-4.4288988 -4.428906 -4.4288983 -4.4288807 -4.4288678 -4.4288487 -4.428813 -4.4287744 -4.4287333 -4.4287062 -4.4286985 -4.4286919 -4.4286995 -4.4287281 -4.4287634][-4.4288831 -4.4288797 -4.4288712 -4.4288654 -4.4288716 -4.4288778 -4.4288731 -4.4288573 -4.4288249 -4.428791 -4.42876 -4.4287338 -4.4287271 -4.4287467 -4.4287815][-4.4288864 -4.4288692 -4.4288526 -4.4288449 -4.42886 -4.4288807 -4.4288931 -4.428895 -4.4288788 -4.4288573 -4.4288278 -4.4287968 -4.4287782 -4.4287815 -4.4288044][-4.4288831 -4.428863 -4.4288497 -4.4288416 -4.4288573 -4.4288807 -4.4288955 -4.4289036 -4.4289007 -4.4288974 -4.4288831 -4.4288573 -4.4288349 -4.4288268 -4.4288349]]...]
INFO - root - 2017-12-10 04:59:04.885257: step 5410, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:38m:18s remains)
INFO - root - 2017-12-10 04:59:07.449136: step 5420, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:19m:26s remains)
INFO - root - 2017-12-10 04:59:10.042266: step 5430, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:23m:18s remains)
INFO - root - 2017-12-10 04:59:12.656301: step 5440, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:52m:30s remains)
INFO - root - 2017-12-10 04:59:15.226297: step 5450, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 24h:00m:10s remains)
INFO - root - 2017-12-10 04:59:17.872803: step 5460, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:31m:58s remains)
INFO - root - 2017-12-10 04:59:20.469618: step 5470, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:47m:47s remains)
INFO - root - 2017-12-10 04:59:23.084672: step 5480, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:39m:08s remains)
INFO - root - 2017-12-10 04:59:25.704781: step 5490, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:21m:59s remains)
INFO - root - 2017-12-10 04:59:28.324267: step 5500, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:30m:44s remains)
2017-12-10 04:59:28.725162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287267 -4.4287338 -4.4287424 -4.428719 -4.4287066 -4.4287148 -4.4287176 -4.4287291 -4.4287524 -4.4287868 -4.4288063 -4.428791 -4.4287772 -4.42876 -4.4287152][-4.4286737 -4.4286819 -4.4286971 -4.4286761 -4.4286733 -4.4286952 -4.4287 -4.4287128 -4.4287338 -4.4287629 -4.4287782 -4.4287515 -4.4287362 -4.4287329 -4.4286976][-4.428638 -4.4286451 -4.4286613 -4.4286528 -4.4286528 -4.4286785 -4.428689 -4.4287071 -4.428731 -4.4287519 -4.4287543 -4.428709 -4.4286914 -4.4287066 -4.4286914][-4.4286165 -4.4286261 -4.42864 -4.4286361 -4.4286389 -4.4286447 -4.4286475 -4.4286618 -4.4286833 -4.4287014 -4.4286981 -4.4286475 -4.4286265 -4.4286551 -4.4286566][-4.4285665 -4.4285645 -4.4285731 -4.4285717 -4.4285774 -4.4285641 -4.4285445 -4.428546 -4.4285803 -4.4286132 -4.4286304 -4.428586 -4.4285612 -4.4285874 -4.4285932][-4.4285145 -4.4284878 -4.4284744 -4.4284725 -4.4284735 -4.4284139 -4.4283166 -4.4283018 -4.4284067 -4.4285073 -4.42857 -4.4285455 -4.4285097 -4.4285336 -4.4285393][-4.4284725 -4.4284172 -4.42837 -4.4283409 -4.4283209 -4.4281883 -4.42795 -4.4278755 -4.428092 -4.4283042 -4.4284339 -4.4284573 -4.4284396 -4.4284844 -4.4285083][-4.4284625 -4.4283957 -4.4283338 -4.428299 -4.4282885 -4.428153 -4.4278584 -4.4277 -4.4279361 -4.4281907 -4.4283476 -4.4283986 -4.4284048 -4.4284773 -4.4285231][-4.428462 -4.4284158 -4.4283957 -4.4283905 -4.4284072 -4.4283433 -4.4281659 -4.4280295 -4.4281459 -4.4283123 -4.4284167 -4.4284582 -4.42846 -4.428514 -4.4285541][-4.4284592 -4.4284482 -4.428473 -4.4284973 -4.428525 -4.4285049 -4.4284253 -4.4283314 -4.4283686 -4.4284477 -4.4284997 -4.4285169 -4.4285064 -4.42855 -4.4285841][-4.428503 -4.4285154 -4.4285641 -4.4285975 -4.428617 -4.4286246 -4.428606 -4.4285479 -4.4285431 -4.4285684 -4.42858 -4.4285593 -4.4285326 -4.4285774 -4.4286189][-4.4285636 -4.428586 -4.4286485 -4.4286876 -4.4287095 -4.4287262 -4.4287238 -4.4286857 -4.4286561 -4.4286418 -4.428618 -4.428546 -4.4284925 -4.4285474 -4.4286213][-4.4285994 -4.4286294 -4.4286985 -4.4287376 -4.4287686 -4.428792 -4.4287944 -4.4287643 -4.4287248 -4.4286904 -4.4286394 -4.4285426 -4.4284787 -4.4285307 -4.4286113][-4.4286175 -4.428648 -4.4287038 -4.4287295 -4.4287581 -4.4287896 -4.4288 -4.4287806 -4.4287481 -4.4287171 -4.4286718 -4.4286 -4.4285626 -4.4286132 -4.4286723][-4.428689 -4.428709 -4.4287372 -4.4287462 -4.4287658 -4.4287896 -4.428792 -4.4287748 -4.4287515 -4.4287319 -4.4287105 -4.4286771 -4.4286761 -4.4287257 -4.4287653]]...]
INFO - root - 2017-12-10 04:59:31.383204: step 5510, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:25m:13s remains)
INFO - root - 2017-12-10 04:59:34.040828: step 5520, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:59m:37s remains)
INFO - root - 2017-12-10 04:59:36.649671: step 5530, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:31m:27s remains)
INFO - root - 2017-12-10 04:59:39.296954: step 5540, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:29m:02s remains)
INFO - root - 2017-12-10 04:59:41.948246: step 5550, loss = 2.28, batch loss = 2.23 (28.9 examples/sec; 0.277 sec/batch; 25h:08m:32s remains)
INFO - root - 2017-12-10 04:59:44.526610: step 5560, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:23m:45s remains)
INFO - root - 2017-12-10 04:59:47.152021: step 5570, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:53m:31s remains)
INFO - root - 2017-12-10 04:59:49.766703: step 5580, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.264 sec/batch; 24h:01m:08s remains)
INFO - root - 2017-12-10 04:59:52.358487: step 5590, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:59s remains)
INFO - root - 2017-12-10 04:59:54.922473: step 5600, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:20m:22s remains)
2017-12-10 04:59:55.340270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287882 -4.42875 -4.4287472 -4.4287596 -4.4287868 -4.4288063 -4.4287982 -4.4287634 -4.42871 -4.4286609 -4.4286432 -4.42866 -4.4286866 -4.4286909 -4.4286714][-4.4287286 -4.4286842 -4.4286776 -4.4286919 -4.42873 -4.4287658 -4.4287767 -4.4287567 -4.4287071 -4.42866 -4.428638 -4.428647 -4.4286695 -4.428678 -4.4286718][-4.4287024 -4.4286613 -4.4286609 -4.4286766 -4.4287167 -4.4287558 -4.4287829 -4.4287753 -4.4287314 -4.4286876 -4.4286566 -4.4286451 -4.4286551 -4.428669 -4.4286757][-4.4286942 -4.4286528 -4.4286504 -4.4286661 -4.4287043 -4.4287515 -4.4287882 -4.4287944 -4.4287677 -4.4287353 -4.4286962 -4.4286623 -4.4286532 -4.4286656 -4.4286795][-4.4286752 -4.4286237 -4.4286103 -4.4286246 -4.4286675 -4.4287219 -4.4287567 -4.4287663 -4.4287505 -4.4287262 -4.4286842 -4.428637 -4.428616 -4.428628 -4.4286404][-4.4286346 -4.428565 -4.4285417 -4.4285526 -4.4285941 -4.4286432 -4.4286666 -4.4286647 -4.4286461 -4.4286246 -4.4285865 -4.42854 -4.4285169 -4.428524 -4.4285374][-4.4285927 -4.4285064 -4.4284749 -4.4284797 -4.4285069 -4.4285278 -4.428524 -4.4285059 -4.4284868 -4.4284735 -4.4284568 -4.4284348 -4.428421 -4.4284244 -4.4284396][-4.4285936 -4.428514 -4.4284816 -4.4284673 -4.4284611 -4.4284344 -4.4283881 -4.4283533 -4.4283457 -4.4283628 -4.4283862 -4.4284005 -4.4284029 -4.4284062 -4.428422][-4.428669 -4.4286065 -4.4285655 -4.4285231 -4.4284749 -4.4284062 -4.4283342 -4.4282985 -4.4283156 -4.4283695 -4.4284229 -4.4284568 -4.4284739 -4.4284816 -4.4285045][-4.4287791 -4.4287329 -4.4286861 -4.4286304 -4.4285727 -4.4285083 -4.428452 -4.4284363 -4.4284649 -4.4285178 -4.428565 -4.4285941 -4.4286079 -4.42862 -4.4286509][-4.428853 -4.4288254 -4.42879 -4.4287477 -4.4287109 -4.4286723 -4.4286385 -4.428628 -4.4286437 -4.4286752 -4.4287071 -4.42873 -4.4287457 -4.4287629 -4.428792][-4.4288845 -4.4288726 -4.4288588 -4.4288397 -4.4288244 -4.4288049 -4.4287825 -4.4287705 -4.4287729 -4.4287858 -4.4288034 -4.4288225 -4.4288435 -4.4288626 -4.4288826][-4.4289093 -4.4289064 -4.428905 -4.4289002 -4.4288979 -4.4288931 -4.4288826 -4.428874 -4.4288678 -4.4288659 -4.4288716 -4.4288845 -4.4289017 -4.428915 -4.428925][-4.4289389 -4.4289389 -4.42894 -4.4289393 -4.4289417 -4.428946 -4.4289484 -4.4289465 -4.4289389 -4.4289303 -4.4289265 -4.4289293 -4.4289346 -4.4289379 -4.4289408][-4.42897 -4.428968 -4.428966 -4.4289646 -4.4289675 -4.4289737 -4.4289794 -4.4289808 -4.4289775 -4.4289713 -4.4289665 -4.4289641 -4.4289622 -4.4289608 -4.4289613]]...]
INFO - root - 2017-12-10 04:59:57.975918: step 5610, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:13m:07s remains)
INFO - root - 2017-12-10 05:00:00.580562: step 5620, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:16m:57s remains)
INFO - root - 2017-12-10 05:00:03.203572: step 5630, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:59m:50s remains)
INFO - root - 2017-12-10 05:00:05.849350: step 5640, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:02m:45s remains)
INFO - root - 2017-12-10 05:00:08.509176: step 5650, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:35m:20s remains)
INFO - root - 2017-12-10 05:00:11.106967: step 5660, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:20m:34s remains)
INFO - root - 2017-12-10 05:00:13.669900: step 5670, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:50m:59s remains)
INFO - root - 2017-12-10 05:00:16.271260: step 5680, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:35m:00s remains)
INFO - root - 2017-12-10 05:00:18.877865: step 5690, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:52s remains)
INFO - root - 2017-12-10 05:00:21.518786: step 5700, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:37m:17s remains)
2017-12-10 05:00:21.966779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289765 -4.4289746 -4.4289703 -4.4289684 -4.4289727 -4.4289823 -4.4289932 -4.428998 -4.4289932 -4.4289846 -4.4289818 -4.4289861 -4.428997 -4.4290075 -4.42901][-4.4289217 -4.4289131 -4.4289 -4.4288936 -4.4288983 -4.4289126 -4.4289293 -4.4289389 -4.4289379 -4.4289322 -4.4289327 -4.4289441 -4.4289651 -4.4289818 -4.4289856][-4.4288816 -4.4288645 -4.4288416 -4.4288287 -4.4288297 -4.42884 -4.4288578 -4.42887 -4.4288735 -4.4288697 -4.4288716 -4.4288883 -4.428915 -4.4289331 -4.4289355][-4.42883 -4.4288106 -4.4287848 -4.4287663 -4.4287605 -4.4287663 -4.4287868 -4.4288106 -4.4288259 -4.4288259 -4.4288268 -4.4288435 -4.4288683 -4.4288812 -4.4288774][-4.428771 -4.4287539 -4.4287343 -4.4287157 -4.4287033 -4.4286971 -4.4287162 -4.4287567 -4.4287958 -4.4288063 -4.4288077 -4.4288225 -4.4288421 -4.4288449 -4.4288287][-4.4287324 -4.4287243 -4.4287124 -4.4286904 -4.4286652 -4.428637 -4.4286408 -4.4286947 -4.4287667 -4.4288054 -4.4288158 -4.4288282 -4.4288392 -4.4288263 -4.428792][-4.42874 -4.4287343 -4.4287176 -4.4286795 -4.4286222 -4.4285555 -4.4285264 -4.4285827 -4.4286947 -4.4287839 -4.4288225 -4.4288473 -4.4288564 -4.4288244 -4.4287615][-4.428792 -4.4287744 -4.4287391 -4.4286628 -4.4285512 -4.4284306 -4.4283504 -4.4283895 -4.428544 -4.4287095 -4.4288039 -4.4288578 -4.4288764 -4.4288244 -4.4287181][-4.4288526 -4.4288197 -4.4287639 -4.4286475 -4.4284792 -4.4283075 -4.428175 -4.4281936 -4.42839 -4.42863 -4.4287777 -4.42885 -4.4288688 -4.4287977 -4.4286456][-4.428906 -4.4288621 -4.428791 -4.4286542 -4.4284568 -4.4282484 -4.4280834 -4.4280963 -4.4283204 -4.4285927 -4.428762 -4.4288378 -4.4288468 -4.4287572 -4.4285693][-4.4289474 -4.428906 -4.4288311 -4.4286904 -4.4284959 -4.428287 -4.4281425 -4.4281816 -4.4283977 -4.4286389 -4.42878 -4.428833 -4.4288273 -4.4287319 -4.4285369][-4.4289784 -4.4289513 -4.428885 -4.4287682 -4.428607 -4.4284434 -4.4283605 -4.42842 -4.4285836 -4.4287496 -4.4288311 -4.4288435 -4.4288225 -4.4287353 -4.42857][-4.4289865 -4.4289861 -4.4289465 -4.4288664 -4.4287605 -4.4286675 -4.42864 -4.4286923 -4.4287796 -4.4288597 -4.4288831 -4.4288597 -4.4288239 -4.4287519 -4.4286375][-4.4289584 -4.4289904 -4.4289842 -4.4289432 -4.4288869 -4.4288445 -4.4288421 -4.4288692 -4.4288993 -4.4289212 -4.4289093 -4.4288731 -4.4288359 -4.4287896 -4.42873][-4.4289026 -4.4289603 -4.4289837 -4.4289708 -4.4289436 -4.42893 -4.4289336 -4.4289403 -4.4289432 -4.4289403 -4.4289188 -4.4288859 -4.4288626 -4.4288445 -4.4288282]]...]
INFO - root - 2017-12-10 05:00:24.596834: step 5710, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:39m:52s remains)
INFO - root - 2017-12-10 05:00:27.246763: step 5720, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:47m:16s remains)
INFO - root - 2017-12-10 05:00:29.858066: step 5730, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:56m:58s remains)
INFO - root - 2017-12-10 05:00:32.505046: step 5740, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:29m:47s remains)
INFO - root - 2017-12-10 05:00:35.117245: step 5750, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:46m:47s remains)
INFO - root - 2017-12-10 05:00:37.703066: step 5760, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:50m:28s remains)
INFO - root - 2017-12-10 05:00:40.333206: step 5770, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:06m:55s remains)
INFO - root - 2017-12-10 05:00:42.963452: step 5780, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:36s remains)
INFO - root - 2017-12-10 05:00:45.544907: step 5790, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:40m:25s remains)
INFO - root - 2017-12-10 05:00:48.136230: step 5800, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:19m:15s remains)
2017-12-10 05:00:48.474838: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287992 -4.4287853 -4.4287548 -4.4287319 -4.4287314 -4.4287519 -4.4287844 -4.4288235 -4.428843 -4.428822 -4.4287658 -4.4287271 -4.4287257 -4.4287357 -4.4287367][-4.4287319 -4.4287438 -4.4287429 -4.4287376 -4.4287381 -4.4287434 -4.4287543 -4.4287744 -4.4287872 -4.428771 -4.42873 -4.4286952 -4.4286947 -4.4287138 -4.4287257][-4.42871 -4.428741 -4.4287634 -4.4287643 -4.42876 -4.4287472 -4.4287424 -4.4287462 -4.428751 -4.4287424 -4.4287248 -4.4287024 -4.4286995 -4.4287152 -4.4287252][-4.4287181 -4.4287477 -4.4287686 -4.4287529 -4.4287214 -4.4286923 -4.4286876 -4.4287014 -4.4287167 -4.42872 -4.4287224 -4.4287176 -4.4287148 -4.4287214 -4.4287157][-4.4287009 -4.4287157 -4.428721 -4.4286814 -4.4286141 -4.4285569 -4.4285593 -4.4286022 -4.4286408 -4.4286652 -4.4286919 -4.4287128 -4.4287167 -4.4287181 -4.4287024][-4.4286714 -4.4286575 -4.4286423 -4.4285736 -4.4284568 -4.4283504 -4.4283476 -4.4284377 -4.4285207 -4.4285688 -4.4286294 -4.4286871 -4.4287095 -4.428719 -4.4287014][-4.4286895 -4.4286313 -4.42857 -4.4284511 -4.4282742 -4.4280815 -4.4280243 -4.4281712 -4.4283342 -4.4284396 -4.4285445 -4.4286528 -4.4287081 -4.4287362 -4.4287295][-4.4287271 -4.4286513 -4.4285555 -4.4284143 -4.4282317 -4.4280066 -4.4278803 -4.42801 -4.4282112 -4.4283533 -4.428484 -4.4286146 -4.4286847 -4.4287233 -4.4287333][-4.4288034 -4.4287415 -4.4286609 -4.4285545 -4.428441 -4.4282947 -4.4281788 -4.428226 -4.4283524 -4.4284506 -4.4285436 -4.4286408 -4.4286919 -4.4287124 -4.4287248][-4.4288626 -4.428822 -4.4287734 -4.4287124 -4.42865 -4.4285765 -4.4285107 -4.4285188 -4.4285774 -4.42863 -4.428678 -4.4287295 -4.4287548 -4.4287562 -4.4287567][-4.4288821 -4.4288521 -4.4288321 -4.4288087 -4.4287806 -4.4287577 -4.4287286 -4.4287295 -4.4287553 -4.42878 -4.428802 -4.4288268 -4.4288416 -4.4288397 -4.4288297][-4.428895 -4.4288735 -4.4288669 -4.4288621 -4.4288554 -4.4288588 -4.428853 -4.4288464 -4.4288492 -4.4288559 -4.4288726 -4.4288898 -4.4289026 -4.428906 -4.4288921][-4.4289174 -4.4289002 -4.428896 -4.4288983 -4.4289012 -4.4289083 -4.4289131 -4.4289064 -4.4289007 -4.4289 -4.4289131 -4.4289212 -4.4289308 -4.4289379 -4.4289293][-4.4289317 -4.4289155 -4.4289117 -4.4289165 -4.4289227 -4.4289303 -4.4289355 -4.4289336 -4.4289289 -4.4289269 -4.4289331 -4.4289308 -4.4289346 -4.4289432 -4.4289422][-4.4289393 -4.428925 -4.4289203 -4.4289212 -4.428925 -4.4289308 -4.4289384 -4.4289412 -4.4289389 -4.4289346 -4.4289336 -4.4289284 -4.4289303 -4.4289393 -4.4289432]]...]
INFO - root - 2017-12-10 05:00:51.108449: step 5810, loss = 2.28, batch loss = 2.23 (26.1 examples/sec; 0.306 sec/batch; 27h:47m:31s remains)
INFO - root - 2017-12-10 05:00:53.724626: step 5820, loss = 2.28, batch loss = 2.23 (28.7 examples/sec; 0.279 sec/batch; 25h:17m:45s remains)
INFO - root - 2017-12-10 05:00:56.314832: step 5830, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:13m:49s remains)
INFO - root - 2017-12-10 05:00:58.907402: step 5840, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:09m:50s remains)
INFO - root - 2017-12-10 05:01:01.468920: step 5850, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:58m:12s remains)
INFO - root - 2017-12-10 05:01:04.024310: step 5860, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:37m:48s remains)
INFO - root - 2017-12-10 05:01:06.676785: step 5870, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:19m:08s remains)
INFO - root - 2017-12-10 05:01:09.287516: step 5880, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:52s remains)
INFO - root - 2017-12-10 05:01:11.927090: step 5890, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:37m:07s remains)
INFO - root - 2017-12-10 05:01:14.522123: step 5900, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:16m:08s remains)
2017-12-10 05:01:14.880674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287214 -4.4287028 -4.4286928 -4.4287028 -4.4287243 -4.4287434 -4.4287543 -4.4287658 -4.4287748 -4.4287786 -4.428781 -4.4287896 -4.4288025 -4.428813 -4.4288235][-4.4286904 -4.428669 -4.4286532 -4.42866 -4.4286785 -4.4286942 -4.4287038 -4.4287205 -4.42874 -4.4287534 -4.428762 -4.4287763 -4.428792 -4.4288025 -4.428813][-4.4286861 -4.4286566 -4.4286389 -4.4286461 -4.4286594 -4.42867 -4.4286742 -4.4286962 -4.4287262 -4.4287491 -4.4287648 -4.4287844 -4.4288025 -4.42881 -4.4288158][-4.4286938 -4.4286613 -4.4286513 -4.4286633 -4.4286695 -4.4286661 -4.4286571 -4.4286728 -4.4287066 -4.4287362 -4.428762 -4.428792 -4.4288149 -4.4288197 -4.4288197][-4.4286847 -4.4286532 -4.4286604 -4.4286847 -4.428689 -4.4286747 -4.4286509 -4.4286547 -4.4286828 -4.4287162 -4.4287524 -4.4287949 -4.4288254 -4.4288297 -4.428822][-4.4286547 -4.428628 -4.428659 -4.4286981 -4.4287062 -4.4286876 -4.4286594 -4.4286556 -4.4286771 -4.4287119 -4.4287539 -4.428803 -4.4288359 -4.4288344 -4.4288149][-4.4286246 -4.428607 -4.42866 -4.4287109 -4.428721 -4.4287014 -4.4286757 -4.4286714 -4.4286895 -4.4287238 -4.4287677 -4.4288192 -4.4288549 -4.42885 -4.428822][-4.428606 -4.4286022 -4.4286733 -4.42873 -4.4287386 -4.4287124 -4.4286866 -4.4286814 -4.4286966 -4.42873 -4.4287758 -4.4288311 -4.4288721 -4.4288697 -4.4288397][-4.4285936 -4.4286056 -4.4286876 -4.4287434 -4.4287457 -4.4287128 -4.4286852 -4.4286795 -4.4286952 -4.4287333 -4.4287844 -4.428844 -4.4288883 -4.42889 -4.428863][-4.4285951 -4.4286184 -4.4287028 -4.4287553 -4.4287534 -4.428721 -4.4286995 -4.4287024 -4.4287267 -4.4287696 -4.4288211 -4.4288769 -4.4289169 -4.4289188 -4.4288931][-4.4286308 -4.4286566 -4.4287333 -4.4287815 -4.4287777 -4.4287462 -4.4287357 -4.428751 -4.4287863 -4.4288316 -4.4288759 -4.4289188 -4.4289484 -4.4289465 -4.42892][-4.4286852 -4.4287066 -4.42877 -4.4288111 -4.4288049 -4.4287753 -4.428771 -4.4287944 -4.4288349 -4.4288788 -4.4289136 -4.4289417 -4.42896 -4.4289556 -4.4289317][-4.4287314 -4.4287448 -4.428793 -4.4288282 -4.4288254 -4.428803 -4.4288039 -4.4288268 -4.4288616 -4.4288983 -4.4289246 -4.4289427 -4.4289532 -4.4289508 -4.4289351][-4.4287848 -4.4287882 -4.4288216 -4.42885 -4.4288507 -4.4288411 -4.4288511 -4.4288716 -4.4288964 -4.4289231 -4.4289389 -4.428946 -4.4289489 -4.4289484 -4.4289408][-4.4288468 -4.4288435 -4.428865 -4.4288869 -4.42889 -4.4288893 -4.4289026 -4.4289193 -4.4289355 -4.4289522 -4.4289608 -4.4289613 -4.4289589 -4.428957 -4.4289536]]...]
INFO - root - 2017-12-10 05:01:17.477807: step 5910, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:55m:51s remains)
INFO - root - 2017-12-10 05:01:20.068170: step 5920, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 23h:03m:27s remains)
INFO - root - 2017-12-10 05:01:22.703603: step 5930, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:25m:20s remains)
INFO - root - 2017-12-10 05:01:25.338086: step 5940, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:48m:02s remains)
INFO - root - 2017-12-10 05:01:27.992304: step 5950, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:58m:56s remains)
INFO - root - 2017-12-10 05:01:30.553166: step 5960, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:19m:47s remains)
INFO - root - 2017-12-10 05:01:33.148784: step 5970, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:46m:27s remains)
INFO - root - 2017-12-10 05:01:35.738997: step 5980, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:09m:53s remains)
INFO - root - 2017-12-10 05:01:38.384660: step 5990, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:45m:01s remains)
INFO - root - 2017-12-10 05:01:41.003831: step 6000, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:36m:15s remains)
2017-12-10 05:01:41.355001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42871 -4.4287181 -4.4287419 -4.4287734 -4.4288 -4.4288125 -4.4288068 -4.4288044 -4.428823 -4.4288392 -4.4288454 -4.4288244 -4.428782 -4.4287357 -4.4286904][-4.4286404 -4.4286442 -4.4286771 -4.4287257 -4.4287658 -4.4287896 -4.4287953 -4.4287972 -4.4288187 -4.4288335 -4.4288349 -4.428802 -4.4287424 -4.428679 -4.4286318][-4.4286218 -4.4286184 -4.428658 -4.4287152 -4.4287548 -4.4287791 -4.4287882 -4.4287958 -4.4288139 -4.4288254 -4.4288216 -4.4287844 -4.4287195 -4.4286509 -4.4286013][-4.4286513 -4.4286418 -4.4286747 -4.4287186 -4.4287443 -4.4287572 -4.4287615 -4.42877 -4.4287949 -4.42881 -4.4288063 -4.4287724 -4.42871 -4.4286451 -4.4285979][-4.4286947 -4.428689 -4.4287071 -4.4287291 -4.4287362 -4.4287324 -4.4287267 -4.4287333 -4.428761 -4.4287815 -4.4287829 -4.4287605 -4.42871 -4.4286575 -4.4286208][-4.4287329 -4.4287353 -4.4287415 -4.4287395 -4.4287267 -4.4287052 -4.4286909 -4.4287 -4.4287181 -4.4287305 -4.4287343 -4.4287329 -4.428709 -4.4286771 -4.4286566][-4.4287605 -4.4287567 -4.4287505 -4.4287353 -4.4287152 -4.4286785 -4.4286547 -4.4286613 -4.42867 -4.4286642 -4.4286618 -4.4286861 -4.4287043 -4.4287081 -4.4287109][-4.4287539 -4.4287429 -4.4287353 -4.4287314 -4.42872 -4.4286766 -4.4286394 -4.4286342 -4.4286265 -4.4285908 -4.428566 -4.4286056 -4.4286714 -4.4287267 -4.4287682][-4.4287105 -4.4287062 -4.4287114 -4.4287319 -4.4287448 -4.4287176 -4.4286776 -4.4286489 -4.4286041 -4.4285221 -4.4284511 -4.4284897 -4.4285984 -4.4287033 -4.4287868][-4.4286551 -4.4286704 -4.4286976 -4.4287457 -4.4287868 -4.4287872 -4.4287539 -4.4286933 -4.4286036 -4.4284611 -4.4283352 -4.4283648 -4.4285054 -4.4286494 -4.4287643][-4.4286118 -4.4286585 -4.4287186 -4.4287915 -4.4288483 -4.4288616 -4.4288273 -4.4287348 -4.4286046 -4.4284163 -4.4282522 -4.4282727 -4.4284329 -4.4286041 -4.4287372][-4.428576 -4.428659 -4.4287486 -4.4288321 -4.4288898 -4.42891 -4.4288654 -4.4287515 -4.4286036 -4.4284086 -4.428247 -4.428268 -4.4284277 -4.4285975 -4.4287248][-4.4285345 -4.4286489 -4.4287486 -4.4288278 -4.4288783 -4.4288917 -4.42884 -4.4287286 -4.4286008 -4.4284458 -4.4283171 -4.4283381 -4.4284716 -4.4286132 -4.428719][-4.4284811 -4.428606 -4.4286904 -4.4287524 -4.4287968 -4.4288092 -4.4287643 -4.4286757 -4.4285846 -4.4284911 -4.4284158 -4.4284377 -4.4285293 -4.4286289 -4.4287086][-4.4284463 -4.428544 -4.4285908 -4.4286318 -4.4286795 -4.4286981 -4.4286742 -4.4286208 -4.4285717 -4.4285321 -4.4285092 -4.4285393 -4.4285941 -4.4286513 -4.4287019]]...]
INFO - root - 2017-12-10 05:01:44.017154: step 6010, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:19m:12s remains)
INFO - root - 2017-12-10 05:01:46.629639: step 6020, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 23h:05m:01s remains)
INFO - root - 2017-12-10 05:01:49.197289: step 6030, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:40m:01s remains)
INFO - root - 2017-12-10 05:01:51.823552: step 6040, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:46m:35s remains)
INFO - root - 2017-12-10 05:01:54.444699: step 6050, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:28m:52s remains)
INFO - root - 2017-12-10 05:01:57.061284: step 6060, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:08m:30s remains)
INFO - root - 2017-12-10 05:01:59.683155: step 6070, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:24m:44s remains)
INFO - root - 2017-12-10 05:02:02.288854: step 6080, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:19m:28s remains)
INFO - root - 2017-12-10 05:02:04.922505: step 6090, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:08m:06s remains)
INFO - root - 2017-12-10 05:02:07.537817: step 6100, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:43m:18s remains)
2017-12-10 05:02:07.937513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289536 -4.4289441 -4.428936 -4.4289217 -4.4288993 -4.4288764 -4.4288721 -4.42888 -4.4288936 -4.4289155 -4.4289327 -4.4289455 -4.4289374 -4.42892 -4.4289207][-4.4289451 -4.4289322 -4.4289212 -4.4288993 -4.4288621 -4.4288278 -4.42882 -4.4288292 -4.4288406 -4.4288683 -4.4288983 -4.4289179 -4.4289093 -4.4288907 -4.4288979][-4.42893 -4.4289122 -4.4288993 -4.4288712 -4.4288177 -4.4287667 -4.4287515 -4.4287586 -4.4287663 -4.4287963 -4.4288464 -4.4288831 -4.4288797 -4.4288597 -4.4288707][-4.4289083 -4.4288859 -4.4288697 -4.4288363 -4.4287748 -4.4287133 -4.4286923 -4.4287028 -4.4287133 -4.4287429 -4.4288011 -4.4288497 -4.42885 -4.4288244 -4.4288316][-4.4288874 -4.4288621 -4.4288464 -4.4288139 -4.4287572 -4.4286928 -4.428668 -4.4286871 -4.4287105 -4.4287376 -4.4287882 -4.4288344 -4.4288311 -4.4287968 -4.4287949][-4.4288688 -4.4288421 -4.4288259 -4.4288034 -4.4287586 -4.4286976 -4.4286675 -4.4286952 -4.4287362 -4.4287682 -4.4288054 -4.428833 -4.4288068 -4.4287524 -4.4287348][-4.4288607 -4.4288244 -4.4287987 -4.4287763 -4.4287281 -4.4286637 -4.428628 -4.4286647 -4.4287305 -4.4287834 -4.4288263 -4.4288406 -4.428793 -4.4287186 -4.4286819][-4.4288616 -4.4288187 -4.4287844 -4.4287519 -4.4286947 -4.4286208 -4.4285789 -4.4286127 -4.4286942 -4.4287734 -4.428833 -4.428853 -4.42881 -4.4287429 -4.4287028][-4.4288712 -4.4288292 -4.4287925 -4.4287505 -4.4286857 -4.4286036 -4.428556 -4.4285812 -4.4286671 -4.4287643 -4.4288359 -4.4288588 -4.4288287 -4.4287834 -4.4287539][-4.4288917 -4.4288568 -4.4288239 -4.4287772 -4.4287043 -4.4286146 -4.428555 -4.428565 -4.4286451 -4.4287438 -4.428813 -4.4288378 -4.4288216 -4.4287972 -4.4287767][-4.4289074 -4.4288797 -4.4288526 -4.4288068 -4.4287357 -4.4286442 -4.4285741 -4.4285693 -4.4286375 -4.4287243 -4.4287829 -4.4288073 -4.4288058 -4.4287987 -4.4287882][-4.4289222 -4.4288974 -4.4288716 -4.4288287 -4.4287643 -4.4286857 -4.4286237 -4.42862 -4.4286761 -4.4287429 -4.4287858 -4.428803 -4.4288058 -4.4288054 -4.4288006][-4.4289384 -4.428915 -4.4288898 -4.428853 -4.4288082 -4.4287624 -4.4287271 -4.42873 -4.42877 -4.4288135 -4.4288363 -4.4288425 -4.4288421 -4.4288425 -4.428844][-4.4289517 -4.4289341 -4.4289145 -4.4288898 -4.4288683 -4.4288521 -4.42884 -4.4288464 -4.4288712 -4.428896 -4.428906 -4.4289064 -4.4289041 -4.4289041 -4.428906][-4.428957 -4.4289479 -4.4289384 -4.428926 -4.4289184 -4.4289184 -4.4289207 -4.4289289 -4.4289408 -4.4289508 -4.4289532 -4.4289503 -4.428947 -4.428946 -4.428947]]...]
INFO - root - 2017-12-10 05:02:10.571129: step 6110, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:12m:39s remains)
INFO - root - 2017-12-10 05:02:13.212341: step 6120, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:16m:37s remains)
INFO - root - 2017-12-10 05:02:15.879033: step 6130, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:12m:54s remains)
INFO - root - 2017-12-10 05:02:18.463685: step 6140, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:45s remains)
INFO - root - 2017-12-10 05:02:21.115956: step 6150, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:10m:27s remains)
INFO - root - 2017-12-10 05:02:23.745962: step 6160, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:50m:02s remains)
INFO - root - 2017-12-10 05:02:26.401198: step 6170, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:33m:32s remains)
INFO - root - 2017-12-10 05:02:28.975762: step 6180, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:40m:23s remains)
INFO - root - 2017-12-10 05:02:31.593224: step 6190, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:09m:03s remains)
INFO - root - 2017-12-10 05:02:34.238417: step 6200, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:55m:29s remains)
2017-12-10 05:02:34.578996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287157 -4.4287171 -4.4286985 -4.4286733 -4.428659 -4.4286432 -4.428637 -4.4286356 -4.4286451 -4.4286551 -4.4286637 -4.4286704 -4.4286814 -4.4286885 -4.4286852][-4.4286914 -4.4286885 -4.4286604 -4.4286342 -4.4286208 -4.4286036 -4.4285879 -4.4285784 -4.4285893 -4.4286122 -4.4286036 -4.428596 -4.4286141 -4.4286494 -4.4286761][-4.428699 -4.4286704 -4.428618 -4.4285889 -4.4285812 -4.4285674 -4.4285355 -4.4285221 -4.4285526 -4.4285855 -4.4285488 -4.4285107 -4.4285312 -4.4285917 -4.4286423][-4.4287186 -4.4286642 -4.4285893 -4.4285641 -4.4285583 -4.4285431 -4.4285145 -4.42851 -4.4285493 -4.4285779 -4.428514 -4.4284377 -4.4284425 -4.4285173 -4.4285936][-4.4287343 -4.4286709 -4.4285851 -4.4285488 -4.4285369 -4.4285269 -4.4285235 -4.4285455 -4.4285817 -4.4285908 -4.4285159 -4.4284434 -4.4284372 -4.4284921 -4.4285712][-4.4287515 -4.4286914 -4.4285994 -4.4285331 -4.4284863 -4.4284577 -4.4284739 -4.4285283 -4.4285703 -4.4285631 -4.4285164 -4.428504 -4.4285131 -4.4285421 -4.4285831][-4.4287558 -4.4287186 -4.4286265 -4.4285345 -4.428441 -4.4283977 -4.4284225 -4.4284863 -4.42851 -4.4284768 -4.4284625 -4.4285173 -4.4285669 -4.4285893 -4.4285941][-4.4287577 -4.4287429 -4.4286604 -4.4285731 -4.4284682 -4.4284248 -4.4284534 -4.4285 -4.4284863 -4.42844 -4.4284582 -4.4285488 -4.428616 -4.4286289 -4.4286008][-4.42876 -4.4287558 -4.4286804 -4.4286036 -4.4285212 -4.4285059 -4.4285417 -4.4285717 -4.4285407 -4.4285064 -4.4285512 -4.4286394 -4.4286838 -4.4286737 -4.4286256][-4.4287658 -4.4287734 -4.4287224 -4.4286728 -4.4286289 -4.4286342 -4.4286489 -4.4286504 -4.428618 -4.4286051 -4.428659 -4.428721 -4.4287271 -4.4286962 -4.4286494][-4.4287944 -4.4288197 -4.4288006 -4.4287782 -4.4287543 -4.4287539 -4.4287453 -4.4287276 -4.4287052 -4.4287086 -4.428751 -4.4287715 -4.4287391 -4.4287028 -4.4286761][-4.4288292 -4.4288459 -4.4288425 -4.4288311 -4.4288149 -4.428812 -4.4288077 -4.428802 -4.4287925 -4.4287944 -4.4288096 -4.4288006 -4.4287553 -4.428731 -4.4287338][-4.4288454 -4.4288507 -4.4288559 -4.4288549 -4.4288511 -4.4288511 -4.4288492 -4.4288464 -4.428844 -4.4288411 -4.4288363 -4.428812 -4.4287806 -4.42878 -4.4288073][-4.4288473 -4.4288454 -4.428854 -4.42886 -4.428865 -4.4288669 -4.4288659 -4.4288626 -4.4288564 -4.4288478 -4.4288373 -4.4288173 -4.4288039 -4.4288163 -4.4288492][-4.4288635 -4.4288583 -4.4288621 -4.4288669 -4.428874 -4.4288788 -4.4288783 -4.428875 -4.4288673 -4.4288592 -4.428853 -4.4288487 -4.4288487 -4.4288592 -4.4288783]]...]
INFO - root - 2017-12-10 05:02:37.154885: step 6210, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:20m:53s remains)
INFO - root - 2017-12-10 05:02:39.720202: step 6220, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:50m:55s remains)
INFO - root - 2017-12-10 05:02:42.293452: step 6230, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:18m:36s remains)
INFO - root - 2017-12-10 05:02:44.960514: step 6240, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:43m:06s remains)
INFO - root - 2017-12-10 05:02:47.595952: step 6250, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:38m:12s remains)
INFO - root - 2017-12-10 05:02:50.171635: step 6260, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:30m:45s remains)
INFO - root - 2017-12-10 05:02:52.744816: step 6270, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:16m:15s remains)
INFO - root - 2017-12-10 05:02:55.364293: step 6280, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:38m:08s remains)
INFO - root - 2017-12-10 05:02:57.946477: step 6290, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:09m:07s remains)
INFO - root - 2017-12-10 05:03:00.587262: step 6300, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:31m:17s remains)
2017-12-10 05:03:00.942170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286742 -4.42869 -4.4287052 -4.4287043 -4.4287171 -4.4287443 -4.4287791 -4.4288011 -4.4288073 -4.4287906 -4.428772 -4.4287696 -4.4287648 -4.4287429 -4.4287224][-4.4287214 -4.42875 -4.4287772 -4.4287806 -4.4287844 -4.4287958 -4.4288163 -4.4288297 -4.4288311 -4.4288058 -4.4287758 -4.4287643 -4.4287567 -4.4287343 -4.4287214][-4.4287963 -4.4288268 -4.4288526 -4.428854 -4.4288507 -4.4288516 -4.4288583 -4.4288568 -4.4288516 -4.42883 -4.4288011 -4.4287767 -4.4287548 -4.4287286 -4.4287252][-4.4288487 -4.4288735 -4.4288931 -4.4288917 -4.428885 -4.4288783 -4.4288692 -4.4288473 -4.4288354 -4.42882 -4.4287987 -4.4287672 -4.4287333 -4.4287076 -4.4287148][-4.4288483 -4.42887 -4.4288855 -4.4288836 -4.4288731 -4.4288473 -4.4287996 -4.4287391 -4.4287238 -4.4287286 -4.4287281 -4.4287024 -4.4286647 -4.4286485 -4.4286742][-4.4288168 -4.4288335 -4.42884 -4.428834 -4.4288192 -4.4287667 -4.4286609 -4.4285464 -4.4285259 -4.4285746 -4.4286184 -4.42861 -4.4285836 -4.4285874 -4.4286327][-4.4287848 -4.4287872 -4.4287777 -4.4287558 -4.4287295 -4.4286442 -4.4284711 -4.4282832 -4.4282641 -4.4283929 -4.4284973 -4.4285159 -4.42851 -4.4285393 -4.4286027][-4.42876 -4.4287362 -4.4286981 -4.4286523 -4.4286156 -4.4285126 -4.4282956 -4.4280519 -4.4280505 -4.4282665 -4.4284253 -4.4284649 -4.4284725 -4.4285121 -4.4285808][-4.4287467 -4.4287043 -4.4286509 -4.4285951 -4.4285574 -4.4284692 -4.4282827 -4.4280696 -4.4280777 -4.4282918 -4.4284458 -4.4284854 -4.4284916 -4.4285245 -4.4285846][-4.4287653 -4.4287229 -4.4286709 -4.42862 -4.4285941 -4.4285388 -4.4284234 -4.428288 -4.4282856 -4.42842 -4.4285254 -4.4285583 -4.4285645 -4.4285855 -4.4286327][-4.42881 -4.42878 -4.42874 -4.4286985 -4.4286795 -4.4286528 -4.4285994 -4.4285245 -4.4285073 -4.4285655 -4.4286242 -4.4286547 -4.4286656 -4.428679 -4.42871][-4.4288597 -4.4288464 -4.4288206 -4.4287863 -4.4287634 -4.4287472 -4.4287276 -4.4286861 -4.4286613 -4.4286737 -4.4287028 -4.4287319 -4.4287486 -4.4287572 -4.4287772][-4.4288831 -4.4288807 -4.4288659 -4.4288373 -4.4288139 -4.4288063 -4.4288063 -4.4287872 -4.4287591 -4.4287457 -4.4287567 -4.428782 -4.4287992 -4.428802 -4.42881][-4.4288964 -4.4289012 -4.4288969 -4.4288788 -4.42886 -4.4288578 -4.4288669 -4.4288597 -4.4288335 -4.4288125 -4.4288139 -4.4288292 -4.4288354 -4.4288244 -4.4288173][-4.4289141 -4.4289246 -4.4289269 -4.4289174 -4.4289031 -4.4289002 -4.42891 -4.4289064 -4.4288859 -4.4288664 -4.4288616 -4.4288621 -4.4288526 -4.4288282 -4.4288063]]...]
INFO - root - 2017-12-10 05:03:03.561686: step 6310, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:27m:35s remains)
INFO - root - 2017-12-10 05:03:06.224835: step 6320, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:22m:51s remains)
INFO - root - 2017-12-10 05:03:08.873275: step 6330, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 24h:02m:02s remains)
INFO - root - 2017-12-10 05:03:11.477853: step 6340, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.250 sec/batch; 22h:36m:30s remains)
INFO - root - 2017-12-10 05:03:14.100739: step 6350, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:25m:02s remains)
INFO - root - 2017-12-10 05:03:16.713810: step 6360, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:38m:33s remains)
INFO - root - 2017-12-10 05:03:19.400850: step 6370, loss = 2.28, batch loss = 2.23 (28.6 examples/sec; 0.280 sec/batch; 25h:20m:27s remains)
INFO - root - 2017-12-10 05:03:22.107205: step 6380, loss = 2.28, batch loss = 2.23 (26.8 examples/sec; 0.299 sec/batch; 27h:03m:28s remains)
INFO - root - 2017-12-10 05:03:24.784339: step 6390, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:40m:50s remains)
INFO - root - 2017-12-10 05:03:27.372301: step 6400, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:42m:59s remains)
2017-12-10 05:03:27.737176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288197 -4.4288507 -4.4288769 -4.4288931 -4.4288936 -4.42888 -4.4288635 -4.4288306 -4.4288011 -4.4287829 -4.428762 -4.4287381 -4.4287071 -4.42867 -4.4286532][-4.4288335 -4.4288573 -4.4288726 -4.4288816 -4.4288754 -4.4288597 -4.4288406 -4.4288111 -4.4287963 -4.4287887 -4.42877 -4.4287467 -4.4287152 -4.4286833 -4.428668][-4.4288478 -4.4288559 -4.4288597 -4.4288549 -4.4288378 -4.4288139 -4.4287868 -4.4287643 -4.4287724 -4.4287877 -4.4287729 -4.4287481 -4.4287148 -4.4286871 -4.4286723][-4.4288511 -4.4288511 -4.4288526 -4.4288359 -4.4287992 -4.4287448 -4.4286976 -4.42869 -4.4287314 -4.4287691 -4.4287553 -4.4287248 -4.4286976 -4.428679 -4.4286656][-4.4288473 -4.42885 -4.42886 -4.4288325 -4.4287663 -4.4286714 -4.4286008 -4.4286041 -4.4286737 -4.4287295 -4.4287162 -4.4286833 -4.4286637 -4.4286604 -4.4286537][-4.428844 -4.4288573 -4.4288707 -4.4288311 -4.4287443 -4.4286251 -4.4285393 -4.4285469 -4.4286275 -4.4286895 -4.4286776 -4.4286466 -4.4286342 -4.4286432 -4.4286542][-4.4288349 -4.4288445 -4.4288497 -4.4288096 -4.4287305 -4.428617 -4.4285259 -4.428524 -4.4286 -4.42866 -4.4286594 -4.42865 -4.4286609 -4.4286866 -4.4287095][-4.4288116 -4.4288116 -4.4288096 -4.4287863 -4.4287367 -4.4286408 -4.4285436 -4.428525 -4.4285917 -4.4286532 -4.4286704 -4.4286919 -4.4287372 -4.4287825 -4.4288][-4.4288044 -4.4287934 -4.4287891 -4.4287796 -4.4287596 -4.428689 -4.4285984 -4.4285641 -4.4286084 -4.4286623 -4.4286861 -4.4287333 -4.4288092 -4.428863 -4.428874][-4.428813 -4.4287934 -4.4287872 -4.42879 -4.4287915 -4.4287529 -4.4286842 -4.4286413 -4.4286561 -4.4286947 -4.4287243 -4.4287796 -4.4288588 -4.4289055 -4.4289041][-4.4288225 -4.4287987 -4.4287872 -4.4287939 -4.4288139 -4.4288111 -4.4287753 -4.4287381 -4.4287395 -4.4287658 -4.4287915 -4.4288316 -4.428884 -4.4289079 -4.4288883][-4.4288392 -4.4288173 -4.4287968 -4.4287953 -4.428823 -4.4288464 -4.4288411 -4.4288187 -4.4288244 -4.4288464 -4.4288588 -4.4288683 -4.4288797 -4.4288764 -4.4288435][-4.4288669 -4.4288454 -4.4288173 -4.4288068 -4.42883 -4.4288621 -4.4288735 -4.4288697 -4.4288864 -4.4289117 -4.4289093 -4.428884 -4.4288578 -4.428833 -4.4287949][-4.4288764 -4.4288549 -4.4288297 -4.4288173 -4.4288325 -4.4288626 -4.4288812 -4.4288883 -4.4289064 -4.4289303 -4.4289246 -4.4288845 -4.428844 -4.4288173 -4.428782][-4.428874 -4.4288535 -4.4288349 -4.4288259 -4.428834 -4.4288583 -4.4288745 -4.4288826 -4.4288931 -4.42891 -4.4289055 -4.4288731 -4.428844 -4.4288254 -4.4287968]]...]
INFO - root - 2017-12-10 05:03:30.331466: step 6410, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:13m:07s remains)
INFO - root - 2017-12-10 05:03:32.990862: step 6420, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:29m:37s remains)
INFO - root - 2017-12-10 05:03:35.656753: step 6430, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:06m:11s remains)
INFO - root - 2017-12-10 05:03:38.302135: step 6440, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:34m:26s remains)
INFO - root - 2017-12-10 05:03:40.934925: step 6450, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:15m:21s remains)
INFO - root - 2017-12-10 05:03:43.507735: step 6460, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:39m:20s remains)
INFO - root - 2017-12-10 05:03:46.082238: step 6470, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:31m:32s remains)
INFO - root - 2017-12-10 05:03:48.746152: step 6480, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:26m:57s remains)
INFO - root - 2017-12-10 05:03:51.370805: step 6490, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:21m:10s remains)
INFO - root - 2017-12-10 05:03:54.005298: step 6500, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:25m:38s remains)
2017-12-10 05:03:54.394274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289379 -4.4288979 -4.4288344 -4.428771 -4.4287062 -4.4286494 -4.4286466 -4.4286876 -4.4287395 -4.4287939 -4.4288235 -4.4288139 -4.4287839 -4.42875 -4.4287233][-4.4289179 -4.4288635 -4.4287825 -4.4287062 -4.4286337 -4.4285827 -4.4285965 -4.4286513 -4.4287138 -4.4287734 -4.4287915 -4.4287648 -4.4287262 -4.428689 -4.4286571][-4.4288979 -4.4288254 -4.4287243 -4.4286323 -4.4285488 -4.4285035 -4.4285421 -4.4286151 -4.4286838 -4.4287419 -4.4287496 -4.4287176 -4.4286761 -4.4286366 -4.4286036][-4.4288774 -4.4287863 -4.4286695 -4.4285703 -4.4284835 -4.4284468 -4.4285064 -4.4285908 -4.4286671 -4.4287186 -4.4287157 -4.4286795 -4.4286332 -4.4285927 -4.4285612][-4.4288597 -4.4287548 -4.4286346 -4.4285388 -4.4284587 -4.4284372 -4.4285054 -4.4285889 -4.4286613 -4.4286957 -4.4286885 -4.4286518 -4.4286013 -4.4285569 -4.4285316][-4.4288454 -4.4287343 -4.4286132 -4.4285154 -4.4284425 -4.4284263 -4.4284816 -4.4285431 -4.4286027 -4.4286308 -4.4286356 -4.4286113 -4.42857 -4.4285297 -4.4285092][-4.4288354 -4.4287214 -4.4285927 -4.4284792 -4.428401 -4.4283795 -4.4284096 -4.4284482 -4.4285054 -4.428544 -4.4285774 -4.428576 -4.4285564 -4.4285293 -4.4285064][-4.428823 -4.428699 -4.4285588 -4.4284329 -4.4283419 -4.4283075 -4.4283247 -4.4283624 -4.4284296 -4.4284768 -4.4285254 -4.4285436 -4.4285407 -4.4285226 -4.4284897][-4.4287982 -4.428659 -4.4285092 -4.4283838 -4.4282889 -4.4282446 -4.4282575 -4.4283075 -4.4283929 -4.4284482 -4.4285007 -4.4285316 -4.428534 -4.428515 -4.4284754][-4.4287825 -4.4286404 -4.4285016 -4.4283991 -4.4283156 -4.428267 -4.428278 -4.4283371 -4.4284282 -4.4284868 -4.4285417 -4.4285779 -4.4285779 -4.42855 -4.4285111][-4.4287863 -4.4286613 -4.4285517 -4.4284768 -4.428412 -4.4283686 -4.4283838 -4.4284482 -4.4285336 -4.4285941 -4.4286489 -4.42868 -4.4286666 -4.4286275 -4.4285903][-4.4288106 -4.4287152 -4.4286351 -4.4285808 -4.4285278 -4.4284954 -4.42852 -4.4285855 -4.4286542 -4.4287043 -4.4287519 -4.4287744 -4.4287543 -4.4287143 -4.4286842][-4.428843 -4.4287782 -4.4287257 -4.4286914 -4.4286523 -4.42863 -4.4286585 -4.42871 -4.4287548 -4.4287858 -4.4288168 -4.4288278 -4.4288082 -4.4287782 -4.4287572][-4.4288826 -4.4288411 -4.428812 -4.428793 -4.4287677 -4.4287515 -4.4287724 -4.4288054 -4.4288297 -4.428843 -4.4288588 -4.4288626 -4.4288487 -4.4288316 -4.4288192][-4.4289265 -4.4289017 -4.4288836 -4.4288697 -4.4288516 -4.4288406 -4.4288545 -4.4288731 -4.4288864 -4.4288921 -4.4288964 -4.4288955 -4.4288864 -4.4288774 -4.4288712]]...]
INFO - root - 2017-12-10 05:03:56.972782: step 6510, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 24h:02m:19s remains)
INFO - root - 2017-12-10 05:03:59.567632: step 6520, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:15m:22s remains)
INFO - root - 2017-12-10 05:04:02.149578: step 6530, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:55m:42s remains)
INFO - root - 2017-12-10 05:04:04.789603: step 6540, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:32m:24s remains)
INFO - root - 2017-12-10 05:04:07.441567: step 6550, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:36m:55s remains)
INFO - root - 2017-12-10 05:04:10.127245: step 6560, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:50m:06s remains)
INFO - root - 2017-12-10 05:04:12.714243: step 6570, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:06m:51s remains)
INFO - root - 2017-12-10 05:04:15.346771: step 6580, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:10m:58s remains)
INFO - root - 2017-12-10 05:04:17.892772: step 6590, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:34m:41s remains)
INFO - root - 2017-12-10 05:04:20.490969: step 6600, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:34m:49s remains)
2017-12-10 05:04:20.834784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289227 -4.4288712 -4.428812 -4.4287682 -4.428741 -4.4287181 -4.4287 -4.4286723 -4.4286518 -4.428637 -4.4286036 -4.4285836 -4.4285865 -4.42859 -4.4285641][-4.4289265 -4.4288654 -4.4287953 -4.4287467 -4.4287219 -4.4287081 -4.428699 -4.4286804 -4.4286566 -4.4286351 -4.428607 -4.4286008 -4.4286146 -4.4286137 -4.4285808][-4.4289374 -4.4288759 -4.4288006 -4.4287448 -4.42871 -4.42869 -4.4286809 -4.4286776 -4.4286709 -4.4286609 -4.4286485 -4.4286513 -4.4286609 -4.428647 -4.4286089][-4.4289441 -4.4288845 -4.4288058 -4.4287424 -4.4286966 -4.4286666 -4.4286504 -4.4286523 -4.4286609 -4.42868 -4.4286847 -4.4286833 -4.4286771 -4.4286604 -4.4286337][-4.428936 -4.4288759 -4.4288006 -4.4287348 -4.4286895 -4.4286532 -4.4286113 -4.4285989 -4.4286184 -4.4286547 -4.4286761 -4.4286714 -4.4286637 -4.428659 -4.4286551][-4.4289255 -4.4288592 -4.4287844 -4.4287219 -4.4286728 -4.42862 -4.4285464 -4.4285135 -4.428545 -4.4286013 -4.4286366 -4.42863 -4.4286156 -4.4286208 -4.4286404][-4.42892 -4.4288406 -4.4287558 -4.4286866 -4.4286237 -4.4285421 -4.4284363 -4.4283938 -4.4284368 -4.4285021 -4.4285488 -4.4285483 -4.428534 -4.4285417 -4.4285684][-4.4289184 -4.4288354 -4.4287434 -4.4286671 -4.4285913 -4.4284921 -4.4283662 -4.4283147 -4.4283528 -4.4284124 -4.428463 -4.4284754 -4.4284744 -4.4284816 -4.4285069][-4.428926 -4.4288559 -4.4287786 -4.4287086 -4.428637 -4.42854 -4.4284167 -4.428359 -4.4283776 -4.4284196 -4.4284639 -4.4284816 -4.4284778 -4.4284635 -4.4284668][-4.4289346 -4.4288816 -4.4288225 -4.4287672 -4.4287043 -4.4286184 -4.4285069 -4.4284482 -4.4284515 -4.4284854 -4.4285293 -4.4285445 -4.4285221 -4.4284782 -4.4284587][-4.4289331 -4.4288855 -4.428834 -4.4287858 -4.4287205 -4.4286413 -4.42856 -4.42851 -4.4285026 -4.4285369 -4.42858 -4.4285984 -4.4285703 -4.4285178 -4.4284925][-4.42893 -4.4288845 -4.4288363 -4.4287891 -4.4287243 -4.4286585 -4.4286051 -4.42857 -4.4285593 -4.4285874 -4.4286184 -4.4286337 -4.428607 -4.428556 -4.42853][-4.4289317 -4.4288898 -4.4288445 -4.4287953 -4.4287405 -4.4286957 -4.4286609 -4.4286366 -4.4286246 -4.4286394 -4.4286494 -4.4286537 -4.4286332 -4.4285908 -4.4285674][-4.4289427 -4.4289031 -4.4288597 -4.428813 -4.4287667 -4.4287348 -4.4287157 -4.4287019 -4.4286933 -4.4287009 -4.4286995 -4.4287 -4.4286857 -4.4286489 -4.4286251][-4.4289675 -4.4289327 -4.428896 -4.4288568 -4.4288244 -4.4288087 -4.4288044 -4.4287982 -4.4287944 -4.4287944 -4.4287887 -4.4287896 -4.4287806 -4.4287496 -4.4287214]]...]
INFO - root - 2017-12-10 05:04:23.551136: step 6610, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:09m:49s remains)
INFO - root - 2017-12-10 05:04:26.191222: step 6620, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:08m:50s remains)
INFO - root - 2017-12-10 05:04:28.793317: step 6630, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:21m:59s remains)
INFO - root - 2017-12-10 05:04:31.397373: step 6640, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:30m:25s remains)
INFO - root - 2017-12-10 05:04:34.033837: step 6650, loss = 2.28, batch loss = 2.23 (29.2 examples/sec; 0.274 sec/batch; 24h:46m:11s remains)
INFO - root - 2017-12-10 05:04:36.670446: step 6660, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:43m:34s remains)
INFO - root - 2017-12-10 05:04:39.339543: step 6670, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:15m:49s remains)
INFO - root - 2017-12-10 05:04:41.936738: step 6680, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:04m:46s remains)
INFO - root - 2017-12-10 05:04:44.548853: step 6690, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:11m:30s remains)
INFO - root - 2017-12-10 05:04:47.199434: step 6700, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:22m:06s remains)
2017-12-10 05:04:47.610333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287825 -4.4287953 -4.428803 -4.4287882 -4.4287457 -4.4286895 -4.4286022 -4.4285259 -4.4285231 -4.4286017 -4.4287076 -4.4287863 -4.4288077 -4.4288054 -4.428791][-4.4287786 -4.4287853 -4.4287882 -4.4287763 -4.42875 -4.4287033 -4.4286256 -4.4285531 -4.4285369 -4.428587 -4.4286623 -4.4287252 -4.4287462 -4.4287553 -4.4287686][-4.4287767 -4.428791 -4.4288049 -4.4287992 -4.4287806 -4.4287438 -4.428678 -4.4286103 -4.42858 -4.4286003 -4.4286366 -4.4286642 -4.4286718 -4.4286914 -4.4287348][-4.4287481 -4.4287877 -4.4288268 -4.4288306 -4.4288125 -4.4287739 -4.4287124 -4.4286556 -4.42863 -4.4286356 -4.4286389 -4.4286284 -4.4286222 -4.4286542 -4.4287119][-4.4287019 -4.4287443 -4.4287972 -4.4288187 -4.428813 -4.4287705 -4.4287009 -4.4286432 -4.4286218 -4.4286332 -4.4286451 -4.4286304 -4.4286242 -4.4286594 -4.42871][-4.4286604 -4.4286752 -4.4287286 -4.4287734 -4.4287829 -4.4287343 -4.4286418 -4.4285583 -4.4285245 -4.4285583 -4.4286189 -4.4286475 -4.428668 -4.4287047 -4.4287424][-4.4286175 -4.4286065 -4.42866 -4.428719 -4.4287357 -4.4286842 -4.4285755 -4.4284587 -4.4284062 -4.4284596 -4.4285688 -4.4286504 -4.4287052 -4.4287543 -4.4287872][-4.428596 -4.4285712 -4.4286046 -4.4286551 -4.4286714 -4.4286218 -4.4285235 -4.42842 -4.4283752 -4.4284296 -4.4285383 -4.4286332 -4.4287024 -4.4287567 -4.428793][-4.428616 -4.4285817 -4.4285927 -4.42863 -4.4286427 -4.4286108 -4.4285483 -4.4284873 -4.4284697 -4.4285154 -4.428587 -4.4286571 -4.4287095 -4.4287486 -4.4287763][-4.4286385 -4.4286132 -4.4286342 -4.42867 -4.4286685 -4.4286418 -4.4286032 -4.4285755 -4.4285822 -4.4286284 -4.4286776 -4.4287205 -4.4287472 -4.428762 -4.4287739][-4.4286327 -4.428637 -4.4286871 -4.4287271 -4.4287152 -4.428689 -4.4286561 -4.4286489 -4.4286795 -4.4287324 -4.4287715 -4.4287987 -4.4288082 -4.4288092 -4.4288054][-4.4286623 -4.4286885 -4.4287415 -4.4287767 -4.4287677 -4.4287486 -4.4287271 -4.4287305 -4.4287705 -4.4288192 -4.4288516 -4.428875 -4.4288807 -4.428875 -4.42886][-4.4287505 -4.4287715 -4.4288011 -4.4288259 -4.4288297 -4.4288239 -4.4288144 -4.4288192 -4.4288497 -4.428885 -4.4289112 -4.42893 -4.4289351 -4.4289284 -4.4289155][-4.4288459 -4.4288549 -4.4288697 -4.4288898 -4.4289045 -4.42891 -4.4289036 -4.4288983 -4.4289112 -4.4289322 -4.4289527 -4.428967 -4.4289708 -4.428968 -4.4289594][-4.4289169 -4.4289165 -4.428925 -4.42894 -4.4289536 -4.4289589 -4.4289551 -4.4289474 -4.4289455 -4.4289541 -4.4289684 -4.42898 -4.4289851 -4.4289846 -4.4289804]]...]
INFO - root - 2017-12-10 05:04:50.237543: step 6710, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:45m:41s remains)
INFO - root - 2017-12-10 05:04:52.811020: step 6720, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:11m:39s remains)
INFO - root - 2017-12-10 05:04:55.407767: step 6730, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 22h:25m:52s remains)
INFO - root - 2017-12-10 05:04:58.050895: step 6740, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:45m:28s remains)
INFO - root - 2017-12-10 05:05:00.636046: step 6750, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:36m:01s remains)
INFO - root - 2017-12-10 05:05:03.249832: step 6760, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:16m:20s remains)
INFO - root - 2017-12-10 05:05:05.890271: step 6770, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:22m:55s remains)
INFO - root - 2017-12-10 05:05:08.552933: step 6780, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:05m:18s remains)
INFO - root - 2017-12-10 05:05:11.197077: step 6790, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:40m:11s remains)
INFO - root - 2017-12-10 05:05:13.825654: step 6800, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:56m:43s remains)
2017-12-10 05:05:14.206531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288788 -4.4287996 -4.4287376 -4.4287057 -4.4287024 -4.4287405 -4.4287748 -4.4287939 -4.4287953 -4.428802 -4.4287972 -4.428792 -4.4287953 -4.4287987 -4.42878][-4.4288692 -4.428782 -4.4287186 -4.4286757 -4.4286585 -4.4286842 -4.4287038 -4.4287148 -4.4287186 -4.4287367 -4.4287457 -4.4287415 -4.4287415 -4.4287462 -4.4287286][-4.4288645 -4.4287772 -4.4287105 -4.4286642 -4.4286375 -4.4286489 -4.4286542 -4.4286542 -4.4286513 -4.4286737 -4.4286966 -4.4286876 -4.42868 -4.4286814 -4.428669][-4.4288654 -4.4287853 -4.4287176 -4.428668 -4.4286327 -4.4286261 -4.4286189 -4.4286056 -4.4285889 -4.428607 -4.4286394 -4.4286294 -4.4286108 -4.4286084 -4.4285975][-4.4288659 -4.4287906 -4.4287262 -4.4286709 -4.4286194 -4.4285955 -4.4285865 -4.428575 -4.4285536 -4.4285693 -4.4286075 -4.4286051 -4.4285851 -4.4285841 -4.4285707][-4.4288626 -4.4287872 -4.4287186 -4.4286542 -4.4285922 -4.4285622 -4.428565 -4.4285712 -4.4285612 -4.4285808 -4.4286165 -4.4286189 -4.4286146 -4.4286227 -4.4286103][-4.4288621 -4.4287786 -4.4286981 -4.4286246 -4.4285579 -4.4285283 -4.4285393 -4.4285665 -4.4285746 -4.4286008 -4.4286351 -4.4286437 -4.4286561 -4.42867 -4.4286575][-4.4288478 -4.4287429 -4.4286447 -4.4285622 -4.4284973 -4.4284739 -4.4284797 -4.4285116 -4.4285259 -4.4285522 -4.4285975 -4.4286218 -4.4286489 -4.4286685 -4.4286628][-4.428822 -4.4286966 -4.428576 -4.4284792 -4.4284148 -4.428391 -4.4283767 -4.4283915 -4.4283924 -4.4284153 -4.4284797 -4.428525 -4.4285684 -4.4286027 -4.4286218][-4.4287958 -4.4286571 -4.4285164 -4.4284019 -4.4283323 -4.4283037 -4.4282808 -4.428277 -4.4282618 -4.4282775 -4.4283466 -4.428412 -4.4284778 -4.4285369 -4.4285884][-4.42879 -4.4286523 -4.428514 -4.4283934 -4.428318 -4.4282937 -4.4282894 -4.4282913 -4.428277 -4.42829 -4.4283466 -4.4284086 -4.4284806 -4.4285522 -4.4286203][-4.428822 -4.4287057 -4.4285903 -4.4284892 -4.42842 -4.4284019 -4.4284124 -4.42843 -4.4284315 -4.4284468 -4.4284859 -4.428534 -4.428597 -4.4286685 -4.4287314][-4.4288845 -4.4287992 -4.4287186 -4.4286509 -4.4286041 -4.4285889 -4.4286003 -4.4286246 -4.428638 -4.4286513 -4.4286766 -4.428709 -4.42875 -4.4287977 -4.4288397][-4.428947 -4.4288917 -4.4288387 -4.4287987 -4.428771 -4.4287586 -4.4287672 -4.428791 -4.4288058 -4.4288135 -4.4288263 -4.4288435 -4.428863 -4.4288864 -4.4289083][-4.428987 -4.4289536 -4.4289184 -4.4288955 -4.4288807 -4.4288726 -4.4288769 -4.4288912 -4.4289 -4.4289036 -4.4289074 -4.4289126 -4.42892 -4.4289303 -4.4289427]]...]
INFO - root - 2017-12-10 05:05:16.788132: step 6810, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:49m:35s remains)
INFO - root - 2017-12-10 05:05:19.412574: step 6820, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:49m:50s remains)
INFO - root - 2017-12-10 05:05:22.002703: step 6830, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:34m:35s remains)
INFO - root - 2017-12-10 05:05:24.571038: step 6840, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:33m:01s remains)
INFO - root - 2017-12-10 05:05:27.137566: step 6850, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:03m:40s remains)
INFO - root - 2017-12-10 05:05:29.786771: step 6860, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:37m:27s remains)
INFO - root - 2017-12-10 05:05:32.360632: step 6870, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:08m:27s remains)
INFO - root - 2017-12-10 05:05:34.949583: step 6880, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:57m:49s remains)
INFO - root - 2017-12-10 05:05:37.575918: step 6890, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:06m:06s remains)
INFO - root - 2017-12-10 05:05:40.214974: step 6900, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:52m:03s remains)
2017-12-10 05:05:40.565895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287639 -4.4287343 -4.4287438 -4.4287586 -4.4287686 -4.42875 -4.4287205 -4.4286981 -4.4287043 -4.4287357 -4.4287858 -4.4288344 -4.4288511 -4.4288449 -4.4288368][-4.4287724 -4.4287572 -4.4287629 -4.4287777 -4.4287939 -4.4287863 -4.4287653 -4.4287477 -4.4287448 -4.4287639 -4.4288092 -4.4288616 -4.4288769 -4.4288669 -4.42885][-4.4287686 -4.4287577 -4.4287567 -4.4287767 -4.4288049 -4.42881 -4.4287891 -4.4287643 -4.4287515 -4.4287691 -4.4288158 -4.4288669 -4.4288793 -4.4288678 -4.4288449][-4.4287386 -4.4287314 -4.4287305 -4.4287548 -4.4287834 -4.428781 -4.4287472 -4.4287205 -4.4287133 -4.4287367 -4.4287915 -4.4288464 -4.4288635 -4.4288511 -4.4288149][-4.4287124 -4.4287109 -4.4287171 -4.4287386 -4.4287434 -4.4287043 -4.4286366 -4.4285984 -4.4286084 -4.4286547 -4.428731 -4.428793 -4.428822 -4.4288168 -4.4287724][-4.428688 -4.4286866 -4.4286895 -4.4286942 -4.4286637 -4.4285679 -4.4284315 -4.4283791 -4.4284449 -4.4285593 -4.4286809 -4.4287562 -4.4287939 -4.4287896 -4.4287376][-4.4286819 -4.4286652 -4.4286437 -4.4286208 -4.4285464 -4.4283695 -4.4281421 -4.4281087 -4.4282851 -4.4284854 -4.4286456 -4.4287314 -4.4287734 -4.4287663 -4.4287109][-4.4286895 -4.4286466 -4.4286013 -4.4285407 -4.4284143 -4.4281721 -4.4279027 -4.4279504 -4.4282327 -4.4284778 -4.4286361 -4.4287186 -4.4287605 -4.4287562 -4.4287038][-4.4287014 -4.4286366 -4.4285746 -4.4284992 -4.4283633 -4.4281559 -4.4279981 -4.428113 -4.4283624 -4.4285641 -4.4286857 -4.4287519 -4.4287906 -4.4287825 -4.4287333][-4.4287186 -4.4286437 -4.42857 -4.4284859 -4.4283805 -4.42829 -4.4282794 -4.4284015 -4.4285603 -4.4286776 -4.4287448 -4.4287882 -4.4288173 -4.4288135 -4.4287682][-4.4287138 -4.428637 -4.4285593 -4.4284849 -4.4284415 -4.4284592 -4.4285192 -4.4286141 -4.4286928 -4.4287367 -4.4287596 -4.4287848 -4.4288096 -4.4288154 -4.4287858][-4.4286938 -4.4286222 -4.42856 -4.4285169 -4.4285288 -4.4286 -4.428669 -4.428719 -4.4287415 -4.4287395 -4.4287419 -4.4287648 -4.4287939 -4.4288125 -4.4287972][-4.4286819 -4.4286242 -4.428576 -4.4285612 -4.4286041 -4.42869 -4.4287524 -4.4287763 -4.4287629 -4.4287348 -4.4287238 -4.42875 -4.4287887 -4.4288139 -4.4288011][-4.4286995 -4.4286551 -4.4286151 -4.4286127 -4.4286637 -4.4287448 -4.4287996 -4.4288058 -4.4287744 -4.4287353 -4.4287086 -4.4287319 -4.4287739 -4.4288006 -4.4287906][-4.4287305 -4.4286895 -4.4286432 -4.4286332 -4.4286737 -4.4287462 -4.428791 -4.428791 -4.428762 -4.4287233 -4.4286985 -4.4287195 -4.42875 -4.4287658 -4.4287629]]...]
INFO - root - 2017-12-10 05:05:43.218371: step 6910, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:26m:46s remains)
INFO - root - 2017-12-10 05:05:45.869806: step 6920, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:19m:28s remains)
INFO - root - 2017-12-10 05:05:48.520511: step 6930, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:26m:50s remains)
INFO - root - 2017-12-10 05:05:51.170855: step 6940, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.267 sec/batch; 24h:10m:13s remains)
INFO - root - 2017-12-10 05:05:53.777242: step 6950, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:31m:08s remains)
INFO - root - 2017-12-10 05:05:56.348633: step 6960, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:08m:37s remains)
INFO - root - 2017-12-10 05:05:58.943396: step 6970, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:41m:31s remains)
INFO - root - 2017-12-10 05:06:01.526491: step 6980, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:39m:32s remains)
INFO - root - 2017-12-10 05:06:04.155524: step 6990, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:08m:01s remains)
INFO - root - 2017-12-10 05:06:06.810767: step 7000, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:20m:46s remains)
2017-12-10 05:06:07.206403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289103 -4.4289002 -4.4288945 -4.4288878 -4.4288788 -4.4288735 -4.4288673 -4.4288521 -4.4288182 -4.4287834 -4.4287696 -4.4287891 -4.4288144 -4.4288387 -4.4288721][-4.428874 -4.4288564 -4.4288507 -4.4288445 -4.4288359 -4.4288316 -4.4288225 -4.4287968 -4.4287472 -4.4287081 -4.4286995 -4.4287271 -4.4287562 -4.4287872 -4.42883][-4.4288216 -4.4287987 -4.4287934 -4.4287906 -4.4287863 -4.4287858 -4.4287796 -4.4287534 -4.4287043 -4.4286709 -4.4286704 -4.4286962 -4.4287195 -4.4287496 -4.4287977][-4.4287539 -4.4287267 -4.4287243 -4.4287267 -4.4287257 -4.4287219 -4.4287086 -4.4286819 -4.428648 -4.4286346 -4.4286456 -4.4286695 -4.4286933 -4.4287276 -4.4287848][-4.4286895 -4.4286489 -4.42865 -4.4286561 -4.4286518 -4.428638 -4.4286041 -4.4285684 -4.4285636 -4.4285855 -4.4286141 -4.4286461 -4.42868 -4.4287248 -4.4287882][-4.4286361 -4.4285765 -4.4285665 -4.4285631 -4.4285445 -4.4285116 -4.4284425 -4.4283853 -4.4284215 -4.428494 -4.4285526 -4.42861 -4.4286618 -4.4287224 -4.4287934][-4.428586 -4.4285121 -4.4284816 -4.4284563 -4.4284182 -4.4283595 -4.4282494 -4.4281654 -4.4282489 -4.4283757 -4.4284687 -4.4285564 -4.4286323 -4.428709 -4.4287896][-4.4286013 -4.4285388 -4.4284964 -4.4284625 -4.4284225 -4.4283566 -4.4282417 -4.4281659 -4.4282584 -4.4283824 -4.4284682 -4.4285588 -4.4286389 -4.4287157 -4.4287939][-4.4286752 -4.428638 -4.4286079 -4.4285932 -4.4285722 -4.4285188 -4.4284329 -4.4283829 -4.4284358 -4.4285097 -4.4285574 -4.4286213 -4.4286861 -4.4287462 -4.428812][-4.428762 -4.4287543 -4.4287519 -4.4287639 -4.428762 -4.4287233 -4.4286661 -4.4286346 -4.4286504 -4.4286761 -4.4286933 -4.4287286 -4.4287729 -4.4288125 -4.4288545][-4.4288545 -4.4288712 -4.4288888 -4.4289112 -4.4289193 -4.428894 -4.4288564 -4.4288368 -4.4288349 -4.4288363 -4.4288368 -4.4288545 -4.4288797 -4.4288955 -4.4289093][-4.4289308 -4.4289503 -4.4289684 -4.4289865 -4.4289961 -4.4289827 -4.4289603 -4.4289508 -4.428947 -4.4289417 -4.4289351 -4.4289408 -4.4289551 -4.428957 -4.4289532][-4.4289446 -4.4289556 -4.4289651 -4.4289751 -4.4289837 -4.4289794 -4.4289656 -4.4289618 -4.4289637 -4.4289627 -4.4289603 -4.4289637 -4.4289737 -4.4289746 -4.4289694][-4.428926 -4.4289293 -4.4289303 -4.4289336 -4.42894 -4.4289403 -4.4289341 -4.4289322 -4.4289379 -4.4289417 -4.4289432 -4.428947 -4.4289575 -4.4289632 -4.4289641][-4.428926 -4.4289246 -4.4289236 -4.428925 -4.4289293 -4.4289317 -4.4289293 -4.4289274 -4.4289303 -4.4289331 -4.4289346 -4.4289384 -4.4289474 -4.4289546 -4.4289608]]...]
INFO - root - 2017-12-10 05:06:09.811509: step 7010, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:42m:06s remains)
INFO - root - 2017-12-10 05:06:12.448633: step 7020, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:35m:26s remains)
INFO - root - 2017-12-10 05:06:15.057825: step 7030, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:01s remains)
INFO - root - 2017-12-10 05:06:17.657796: step 7040, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:06m:23s remains)
INFO - root - 2017-12-10 05:06:20.239919: step 7050, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 22h:25m:40s remains)
INFO - root - 2017-12-10 05:06:22.947011: step 7060, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:31m:06s remains)
INFO - root - 2017-12-10 05:06:25.576273: step 7070, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.253 sec/batch; 22h:49m:53s remains)
INFO - root - 2017-12-10 05:06:28.187098: step 7080, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:06m:11s remains)
INFO - root - 2017-12-10 05:06:30.796919: step 7090, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:24s remains)
INFO - root - 2017-12-10 05:06:33.410768: step 7100, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:13m:43s remains)
2017-12-10 05:06:33.799293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288096 -4.428772 -4.4287281 -4.4286795 -4.42866 -4.4286666 -4.4286766 -4.4286871 -4.4286752 -4.4286675 -4.4286995 -4.4287539 -4.4287748 -4.42875 -4.4287405][-4.4287944 -4.4287605 -4.4287238 -4.4286809 -4.4286556 -4.4286542 -4.4286737 -4.4287066 -4.4287071 -4.42869 -4.4287119 -4.4287629 -4.4287791 -4.4287429 -4.4287267][-4.4287615 -4.4287395 -4.4287176 -4.4286847 -4.4286618 -4.4286537 -4.4286847 -4.4287453 -4.428771 -4.4287567 -4.4287586 -4.4287806 -4.4287815 -4.4287405 -4.4287195][-4.4287062 -4.4286995 -4.4286919 -4.4286671 -4.4286447 -4.4286385 -4.4286828 -4.4287672 -4.4288154 -4.4288158 -4.4288116 -4.4288058 -4.4287891 -4.4287505 -4.4287276][-4.428659 -4.4286652 -4.4286718 -4.4286532 -4.4286304 -4.4286165 -4.4286509 -4.4287271 -4.4287796 -4.4288087 -4.4288268 -4.4288197 -4.4287968 -4.4287648 -4.4287386][-4.4286623 -4.428669 -4.4286823 -4.4286714 -4.4286423 -4.4286041 -4.4285865 -4.4286032 -4.4286342 -4.4286985 -4.4287667 -4.4287925 -4.428782 -4.4287624 -4.4287419][-4.4287229 -4.4287267 -4.4287381 -4.4287281 -4.4286838 -4.4286017 -4.4285035 -4.4284167 -4.4283905 -4.4284897 -4.4286342 -4.4287195 -4.4287391 -4.4287443 -4.428751][-4.42879 -4.4288058 -4.4288144 -4.4288 -4.4287472 -4.4286342 -4.42847 -4.4282842 -4.4281764 -4.4282722 -4.4284787 -4.4286208 -4.4286747 -4.4287181 -4.4287639][-4.4288187 -4.4288507 -4.4288616 -4.4288492 -4.4288025 -4.4286981 -4.4285283 -4.4283266 -4.4281983 -4.4282527 -4.4284182 -4.4285469 -4.428606 -4.4286733 -4.4287553][-4.4287963 -4.4288383 -4.4288511 -4.4288459 -4.4288249 -4.4287648 -4.4286475 -4.4285069 -4.4284182 -4.4284291 -4.4284921 -4.4285331 -4.4285502 -4.4286079 -4.4286957][-4.42873 -4.428772 -4.4287863 -4.428793 -4.4288034 -4.4287906 -4.4287372 -4.4286671 -4.4286222 -4.4286194 -4.4286189 -4.428587 -4.4285517 -4.4285684 -4.4286304][-4.42866 -4.4286909 -4.4287138 -4.4287367 -4.4287567 -4.4287648 -4.4287519 -4.4287367 -4.4287314 -4.4287434 -4.4287324 -4.4286776 -4.4286132 -4.4285922 -4.4286218][-4.4286323 -4.4286442 -4.4286795 -4.4287186 -4.4287324 -4.4287281 -4.4287186 -4.4287229 -4.4287438 -4.4287796 -4.42879 -4.4287486 -4.42869 -4.4286585 -4.4286652][-4.4286623 -4.4286561 -4.4287004 -4.4287505 -4.4287548 -4.4287333 -4.4287028 -4.4286904 -4.4287109 -4.4287539 -4.4287796 -4.4287658 -4.4287314 -4.4287114 -4.4287152][-4.4287243 -4.4287186 -4.4287596 -4.4288039 -4.4288034 -4.4287806 -4.4287367 -4.4287 -4.4286938 -4.4287119 -4.4287305 -4.4287376 -4.4287319 -4.4287324 -4.4287429]]...]
INFO - root - 2017-12-10 05:06:36.418268: step 7110, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:12m:02s remains)
INFO - root - 2017-12-10 05:06:39.039533: step 7120, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 23h:34m:29s remains)
INFO - root - 2017-12-10 05:06:41.628755: step 7130, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:20m:45s remains)
INFO - root - 2017-12-10 05:06:44.212094: step 7140, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:57m:00s remains)
INFO - root - 2017-12-10 05:06:46.825887: step 7150, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:40m:05s remains)
INFO - root - 2017-12-10 05:06:49.423159: step 7160, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:35m:25s remains)
INFO - root - 2017-12-10 05:06:52.095070: step 7170, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:08m:01s remains)
INFO - root - 2017-12-10 05:06:54.741688: step 7180, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:14m:31s remains)
INFO - root - 2017-12-10 05:06:57.359615: step 7190, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:05m:58s remains)
INFO - root - 2017-12-10 05:06:59.974290: step 7200, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:52m:12s remains)
2017-12-10 05:07:00.370494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289026 -4.4288421 -4.4287853 -4.4287524 -4.4287591 -4.4287834 -4.4287806 -4.4287457 -4.4287267 -4.4287252 -4.4287324 -4.4287624 -4.4288049 -4.4288158 -4.4287724][-4.4289203 -4.4288831 -4.4288454 -4.428813 -4.4288054 -4.428812 -4.4288011 -4.4287715 -4.4287515 -4.4287386 -4.4287343 -4.4287572 -4.4287806 -4.4287763 -4.4287348][-4.4289427 -4.4289289 -4.4289017 -4.4288611 -4.4288311 -4.4288197 -4.4288154 -4.428812 -4.4288092 -4.4287977 -4.4287863 -4.4288058 -4.4288144 -4.4287925 -4.4287481][-4.4289694 -4.4289732 -4.4289503 -4.4288907 -4.4288192 -4.4287744 -4.4287686 -4.4287982 -4.4288335 -4.4288445 -4.4288425 -4.4288673 -4.4288783 -4.4288521 -4.4288044][-4.4289875 -4.4290047 -4.428978 -4.4288945 -4.4287667 -4.428659 -4.4286246 -4.4286723 -4.4287663 -4.4288349 -4.4288611 -4.4288955 -4.4289141 -4.4288921 -4.4288507][-4.4289885 -4.4290185 -4.4289923 -4.4288907 -4.4287119 -4.4285264 -4.4284172 -4.4284296 -4.4285755 -4.428721 -4.428803 -4.4288597 -4.4288921 -4.4288864 -4.4288621][-4.4289756 -4.42902 -4.4290037 -4.4288979 -4.4286895 -4.4284353 -4.4282174 -4.4281354 -4.4283085 -4.4285278 -4.4286628 -4.4287577 -4.4288244 -4.4288554 -4.4288626][-4.4289694 -4.4290209 -4.4290118 -4.4289184 -4.4287128 -4.4284406 -4.4281607 -4.427989 -4.42814 -4.4283705 -4.428515 -4.4286327 -4.4287376 -4.4288068 -4.428844][-4.42898 -4.4290304 -4.4290247 -4.4289455 -4.4287753 -4.4285479 -4.4283066 -4.4281507 -4.4282293 -4.428371 -4.4284616 -4.4285564 -4.4286618 -4.4287529 -4.4288096][-4.428997 -4.4290433 -4.4290414 -4.4289837 -4.4288669 -4.4287143 -4.42856 -4.428462 -4.4284759 -4.4285107 -4.4285245 -4.4285645 -4.4286351 -4.4287195 -4.4287767][-4.4290113 -4.4290438 -4.4290395 -4.4290066 -4.4289446 -4.4288659 -4.4287896 -4.4287357 -4.4287143 -4.4286904 -4.4286594 -4.4286537 -4.4286833 -4.4287381 -4.4287758][-4.4290147 -4.429029 -4.4290209 -4.4290042 -4.4289837 -4.4289584 -4.42893 -4.428905 -4.4288769 -4.4288416 -4.4288058 -4.4287887 -4.4287996 -4.4288192 -4.428822][-4.4290018 -4.4290004 -4.4289923 -4.4289908 -4.4289937 -4.4289975 -4.4290009 -4.428997 -4.4289727 -4.4289422 -4.4289207 -4.4289141 -4.4289231 -4.4289274 -4.4289017][-4.4289651 -4.4289536 -4.4289541 -4.4289689 -4.4289818 -4.428997 -4.4290209 -4.4290347 -4.4290223 -4.4290032 -4.4289942 -4.4289956 -4.4290047 -4.4290004 -4.4289584][-4.4289112 -4.4288988 -4.4289112 -4.4289384 -4.428957 -4.4289784 -4.4290137 -4.4290414 -4.4290419 -4.4290318 -4.4290252 -4.429029 -4.4290323 -4.4290204 -4.4289675]]...]
INFO - root - 2017-12-10 05:07:03.061020: step 7210, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:46m:12s remains)
INFO - root - 2017-12-10 05:07:05.699890: step 7220, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 22h:43m:00s remains)
INFO - root - 2017-12-10 05:07:08.336781: step 7230, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:42m:07s remains)
INFO - root - 2017-12-10 05:07:10.977588: step 7240, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:46m:11s remains)
INFO - root - 2017-12-10 05:07:13.559341: step 7250, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:29m:33s remains)
INFO - root - 2017-12-10 05:07:16.201609: step 7260, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:21m:53s remains)
INFO - root - 2017-12-10 05:07:18.816474: step 7270, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:10m:28s remains)
INFO - root - 2017-12-10 05:07:21.431450: step 7280, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:10m:44s remains)
INFO - root - 2017-12-10 05:07:24.068972: step 7290, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:16m:02s remains)
INFO - root - 2017-12-10 05:07:26.677684: step 7300, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:06m:50s remains)
2017-12-10 05:07:27.028222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288473 -4.4288321 -4.4288311 -4.4288163 -4.428812 -4.4288025 -4.4287539 -4.4286909 -4.4286838 -4.4286861 -4.4286919 -4.42873 -4.4287553 -4.4287643 -4.4287605][-4.428853 -4.4288282 -4.4288216 -4.4287968 -4.4287834 -4.4287667 -4.428709 -4.4286385 -4.428638 -4.42866 -4.4286695 -4.4286928 -4.4286976 -4.4286923 -4.4286909][-4.4288635 -4.428833 -4.428822 -4.4287968 -4.4287815 -4.4287634 -4.4286995 -4.4286156 -4.4286065 -4.4286442 -4.4286575 -4.4286714 -4.4286666 -4.4286637 -4.4286685][-4.4288559 -4.4288278 -4.4288187 -4.4287977 -4.4287844 -4.4287748 -4.4287109 -4.4286141 -4.4285903 -4.428637 -4.4286575 -4.4286537 -4.4286432 -4.4286504 -4.4286628][-4.42884 -4.4288197 -4.4288192 -4.4288082 -4.4287996 -4.4287868 -4.4287114 -4.4285874 -4.4285393 -4.4285984 -4.4286366 -4.4286275 -4.4286089 -4.4286175 -4.4286313][-4.4288111 -4.4288025 -4.4288149 -4.4288168 -4.4288058 -4.4287815 -4.4286852 -4.4285188 -4.42844 -4.42853 -4.4286156 -4.4286332 -4.4286208 -4.4286261 -4.428637][-4.4287891 -4.4288011 -4.4288259 -4.4288306 -4.4288087 -4.4287634 -4.4286265 -4.4283924 -4.4282727 -4.4284167 -4.4285741 -4.428638 -4.4286594 -4.4286728 -4.42868][-4.4287987 -4.42882 -4.42884 -4.4288344 -4.428792 -4.4287176 -4.4285336 -4.4282293 -4.4280624 -4.4282665 -4.4284835 -4.42859 -4.428647 -4.4286909 -4.4287152][-4.4288206 -4.4288397 -4.4288445 -4.4288287 -4.4287839 -4.4287157 -4.428555 -4.4282851 -4.4281268 -4.4282928 -4.4284768 -4.4285722 -4.4286385 -4.4287014 -4.428741][-4.4288449 -4.4288559 -4.4288473 -4.4288249 -4.4287944 -4.4287553 -4.4286561 -4.4284749 -4.4283552 -4.4284439 -4.428556 -4.4286127 -4.4286528 -4.4287071 -4.428751][-4.4288554 -4.4288445 -4.4288244 -4.4288039 -4.428791 -4.4287758 -4.4287219 -4.4286189 -4.4285321 -4.4285669 -4.4286342 -4.428669 -4.428688 -4.4287343 -4.4287848][-4.4288468 -4.4288173 -4.4287949 -4.4287844 -4.4287925 -4.4287972 -4.4287715 -4.4287219 -4.4286642 -4.4286652 -4.428699 -4.4287205 -4.4287238 -4.428762 -4.428812][-4.42883 -4.4287853 -4.4287667 -4.4287724 -4.4287968 -4.4288177 -4.4288211 -4.4288087 -4.4287758 -4.4287648 -4.4287777 -4.4287796 -4.4287634 -4.4287825 -4.4288173][-4.4288177 -4.4287696 -4.4287596 -4.4287758 -4.428802 -4.4288249 -4.4288354 -4.4288378 -4.4288235 -4.4288158 -4.4288225 -4.4288154 -4.4287844 -4.4287796 -4.4287972][-4.4288335 -4.4287939 -4.4287896 -4.4288039 -4.4288206 -4.4288378 -4.4288425 -4.4288406 -4.428833 -4.4288297 -4.4288354 -4.4288287 -4.4287992 -4.4287829 -4.4287858]]...]
INFO - root - 2017-12-10 05:07:29.675375: step 7310, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:24m:33s remains)
INFO - root - 2017-12-10 05:07:32.309823: step 7320, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:00m:59s remains)
INFO - root - 2017-12-10 05:07:34.904148: step 7330, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 24h:03m:20s remains)
INFO - root - 2017-12-10 05:07:37.515473: step 7340, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:23m:03s remains)
INFO - root - 2017-12-10 05:07:40.106259: step 7350, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:00m:55s remains)
INFO - root - 2017-12-10 05:07:42.741787: step 7360, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.259 sec/batch; 23h:25m:18s remains)
INFO - root - 2017-12-10 05:07:45.343621: step 7370, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:04m:07s remains)
INFO - root - 2017-12-10 05:07:47.938866: step 7380, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:07m:36s remains)
INFO - root - 2017-12-10 05:07:50.561112: step 7390, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:05m:36s remains)
INFO - root - 2017-12-10 05:07:53.188104: step 7400, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:51m:18s remains)
2017-12-10 05:07:53.514451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289966 -4.4289823 -4.4289427 -4.428896 -4.4288607 -4.4288335 -4.4287896 -4.4287329 -4.4287395 -4.4287734 -4.4287796 -4.4287953 -4.4288311 -4.4288588 -4.4288621][-4.4289956 -4.4289804 -4.4289393 -4.4288917 -4.4288559 -4.428823 -4.4287663 -4.4287066 -4.4287171 -4.4287553 -4.4287648 -4.4287887 -4.4288378 -4.4288764 -4.4288797][-4.4289951 -4.4289808 -4.4289393 -4.4288893 -4.4288545 -4.4288225 -4.4287663 -4.4287109 -4.4287181 -4.4287515 -4.428762 -4.4287806 -4.4288192 -4.4288516 -4.4288511][-4.4289966 -4.4289823 -4.4289374 -4.428885 -4.4288487 -4.4288192 -4.42877 -4.4287248 -4.4287257 -4.4287434 -4.4287505 -4.4287634 -4.4287915 -4.4288144 -4.4288158][-4.4289989 -4.4289851 -4.428936 -4.4288731 -4.4288273 -4.4287996 -4.4287648 -4.4287276 -4.4287143 -4.4287124 -4.4287138 -4.4287276 -4.4287572 -4.428782 -4.42879][-4.4290013 -4.4289885 -4.4289327 -4.42885 -4.4287825 -4.4287496 -4.4287219 -4.4286771 -4.4286456 -4.4286289 -4.42863 -4.4286618 -4.4287167 -4.4287581 -4.4287734][-4.4290042 -4.4289932 -4.4289308 -4.4288249 -4.4287291 -4.4286819 -4.4286442 -4.4285774 -4.428515 -4.4284782 -4.42848 -4.428545 -4.4286456 -4.4287219 -4.4287553][-4.4290032 -4.4289927 -4.4289241 -4.4288 -4.4286838 -4.428618 -4.4285655 -4.4284816 -4.4284072 -4.4283581 -4.4283681 -4.4284611 -4.4285913 -4.4286928 -4.4287314][-4.4289994 -4.42899 -4.4289222 -4.4287987 -4.428678 -4.4285989 -4.4285398 -4.4284649 -4.42842 -4.4283962 -4.4284167 -4.4285078 -4.4286351 -4.4287376 -4.42877][-4.4289908 -4.4289846 -4.428926 -4.4288182 -4.4287128 -4.4286332 -4.4285808 -4.4285312 -4.4285331 -4.428546 -4.4285636 -4.4286289 -4.42873 -4.4288173 -4.4288454][-4.4289813 -4.4289794 -4.4289331 -4.4288549 -4.4287806 -4.428719 -4.428679 -4.428648 -4.428679 -4.4287076 -4.4287143 -4.4287543 -4.4288244 -4.4288888 -4.4289036][-4.4289746 -4.4289761 -4.4289408 -4.4288931 -4.4288526 -4.4288111 -4.4287772 -4.4287524 -4.4287949 -4.4288278 -4.4288263 -4.4288411 -4.428884 -4.4289293 -4.4289308][-4.4289703 -4.4289713 -4.4289427 -4.4289107 -4.4288917 -4.428865 -4.4288406 -4.4288235 -4.4288664 -4.4288969 -4.4288912 -4.4288893 -4.42891 -4.4289365 -4.4289312][-4.4289656 -4.4289632 -4.428937 -4.4289112 -4.4289002 -4.4288845 -4.4288697 -4.4288588 -4.4288945 -4.4289207 -4.428915 -4.4289083 -4.4289141 -4.4289241 -4.4289217][-4.4289608 -4.4289579 -4.428937 -4.4289122 -4.4288988 -4.4288816 -4.4288645 -4.4288478 -4.4288712 -4.4289031 -4.4289117 -4.4289103 -4.4289103 -4.42891 -4.4289112]]...]
INFO - root - 2017-12-10 05:07:56.136899: step 7410, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:13m:14s remains)
INFO - root - 2017-12-10 05:07:58.761370: step 7420, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:34m:14s remains)
INFO - root - 2017-12-10 05:08:01.321284: step 7430, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:52m:54s remains)
INFO - root - 2017-12-10 05:08:03.906057: step 7440, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 22h:46m:16s remains)
INFO - root - 2017-12-10 05:08:06.463603: step 7450, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:43m:03s remains)
INFO - root - 2017-12-10 05:08:09.101449: step 7460, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.257 sec/batch; 23h:11m:17s remains)
INFO - root - 2017-12-10 05:08:11.741859: step 7470, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:35m:21s remains)
INFO - root - 2017-12-10 05:08:14.358000: step 7480, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:21m:09s remains)
INFO - root - 2017-12-10 05:08:16.935485: step 7490, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:06m:56s remains)
INFO - root - 2017-12-10 05:08:19.595806: step 7500, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:12m:24s remains)
2017-12-10 05:08:19.923809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289718 -4.4289618 -4.4289575 -4.4289532 -4.4289355 -4.4289083 -4.4288859 -4.4288721 -4.4288697 -4.4288735 -4.4288764 -4.4288788 -4.4288864 -4.42889 -4.4288931][-4.4289627 -4.4289508 -4.4289432 -4.4289341 -4.4289112 -4.4288721 -4.4288325 -4.428812 -4.4288096 -4.4288149 -4.4288235 -4.4288373 -4.42885 -4.4288497 -4.428843][-4.4289641 -4.4289508 -4.4289374 -4.4289227 -4.4288893 -4.4288316 -4.428772 -4.4287472 -4.42875 -4.42877 -4.4287944 -4.4288173 -4.42883 -4.4288158 -4.4287996][-4.4289713 -4.428957 -4.4289427 -4.42892 -4.4288726 -4.4287944 -4.4287186 -4.4286952 -4.4287181 -4.4287596 -4.4287944 -4.428822 -4.4288206 -4.4287863 -4.4287562][-4.4289832 -4.4289713 -4.4289532 -4.4289203 -4.428854 -4.4287534 -4.4286613 -4.4286461 -4.4287004 -4.4287667 -4.4288054 -4.4288268 -4.4288073 -4.4287543 -4.4287148][-4.4289894 -4.4289732 -4.42894 -4.4288893 -4.4288025 -4.4286823 -4.4285712 -4.4285555 -4.4286404 -4.4287386 -4.4287872 -4.4288044 -4.4287777 -4.4287119 -4.4286752][-4.4289732 -4.4289451 -4.4288921 -4.4288197 -4.4287176 -4.4285817 -4.4284406 -4.4284215 -4.4285469 -4.4286866 -4.4287572 -4.4287834 -4.4287577 -4.4286895 -4.4286661][-4.4289241 -4.4288793 -4.4288111 -4.4287233 -4.4286084 -4.4284472 -4.4282575 -4.4282265 -4.42841 -4.4286027 -4.4286985 -4.4287362 -4.4287233 -4.4286666 -4.4286585][-4.4288573 -4.4287968 -4.4287195 -4.4286313 -4.4285288 -4.4283652 -4.4281478 -4.4280858 -4.4282928 -4.4285111 -4.4286189 -4.4286628 -4.42866 -4.4286242 -4.4286361][-4.428792 -4.4287262 -4.428659 -4.4285941 -4.4285417 -4.4284468 -4.4283061 -4.42826 -4.428391 -4.4285426 -4.4286218 -4.4286566 -4.4286537 -4.4286184 -4.4286289][-4.4287515 -4.4286909 -4.4286461 -4.4286156 -4.4286137 -4.4285851 -4.4285221 -4.4284959 -4.42856 -4.428638 -4.428689 -4.4287181 -4.4287028 -4.4286561 -4.4286604][-4.4287443 -4.4286957 -4.4286728 -4.4286685 -4.4286876 -4.428688 -4.4286718 -4.4286695 -4.4287081 -4.4287486 -4.4287782 -4.4287934 -4.4287691 -4.428721 -4.4287229][-4.4287667 -4.4287324 -4.4287224 -4.4287281 -4.4287529 -4.4287782 -4.428793 -4.428813 -4.428844 -4.4288645 -4.4288793 -4.4288921 -4.428874 -4.4288349 -4.4288297][-4.4288063 -4.4287834 -4.4287844 -4.4287963 -4.428823 -4.428853 -4.4288778 -4.4289021 -4.4289265 -4.42894 -4.4289517 -4.4289675 -4.4289641 -4.428937 -4.4289217][-4.4288616 -4.4288454 -4.4288492 -4.4288592 -4.4288769 -4.4288988 -4.4289203 -4.42894 -4.4289551 -4.428966 -4.4289765 -4.428988 -4.4289918 -4.4289808 -4.4289656]]...]
INFO - root - 2017-12-10 05:08:22.546272: step 7510, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:52m:39s remains)
INFO - root - 2017-12-10 05:08:25.205268: step 7520, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:13m:01s remains)
INFO - root - 2017-12-10 05:08:27.835192: step 7530, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:29m:49s remains)
INFO - root - 2017-12-10 05:08:30.464249: step 7540, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:50m:45s remains)
INFO - root - 2017-12-10 05:08:33.125773: step 7550, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:06m:40s remains)
INFO - root - 2017-12-10 05:08:35.712240: step 7560, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:40m:03s remains)
INFO - root - 2017-12-10 05:08:38.376509: step 7570, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:53m:54s remains)
INFO - root - 2017-12-10 05:08:41.000090: step 7580, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:10m:11s remains)
INFO - root - 2017-12-10 05:08:43.673514: step 7590, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:27m:51s remains)
INFO - root - 2017-12-10 05:08:46.254409: step 7600, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:25m:59s remains)
2017-12-10 05:08:46.653439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289756 -4.4289513 -4.4289317 -4.42892 -4.4289131 -4.4289074 -4.4289064 -4.428915 -4.4289255 -4.4289279 -4.4289284 -4.4289341 -4.4289503 -4.4289622 -4.4289651][-4.4289322 -4.4288921 -4.4288578 -4.4288349 -4.4288173 -4.428802 -4.4287963 -4.4288106 -4.4288344 -4.4288521 -4.4288626 -4.4288712 -4.4288917 -4.4289074 -4.4289107][-4.4288931 -4.4288411 -4.4287915 -4.4287519 -4.4287186 -4.4286876 -4.4286723 -4.4286942 -4.4287319 -4.4287672 -4.4287934 -4.4288087 -4.42883 -4.4288445 -4.4288497][-4.428865 -4.4288044 -4.4287424 -4.4286861 -4.4286394 -4.4285932 -4.428576 -4.4286084 -4.4286485 -4.4286852 -4.428719 -4.4287395 -4.4287658 -4.4287829 -4.428791][-4.4288459 -4.42878 -4.4287038 -4.4286237 -4.4285564 -4.4284987 -4.4284887 -4.42854 -4.4285803 -4.4286065 -4.4286289 -4.4286418 -4.428669 -4.4286957 -4.4287181][-4.4288373 -4.4287667 -4.4286728 -4.4285617 -4.428463 -4.4283881 -4.4283895 -4.4284768 -4.4285359 -4.4285579 -4.428555 -4.4285426 -4.4285679 -4.4286118 -4.4286542][-4.42884 -4.4287682 -4.4286652 -4.4285293 -4.428381 -4.4282556 -4.4282722 -4.4284215 -4.4285259 -4.4285593 -4.4285336 -4.428494 -4.4285207 -4.4285855 -4.428648][-4.4288449 -4.428781 -4.4286866 -4.4285512 -4.4283757 -4.4281955 -4.4282007 -4.4283915 -4.428535 -4.4285846 -4.4285493 -4.4285026 -4.4285312 -4.4286017 -4.4286656][-4.4288568 -4.4288106 -4.4287424 -4.4286427 -4.4285073 -4.4283462 -4.4283118 -4.4284506 -4.428576 -4.4286304 -4.4286156 -4.4285908 -4.4286075 -4.4286451 -4.4286819][-4.4288726 -4.42884 -4.4287958 -4.4287362 -4.4286575 -4.4285517 -4.4285054 -4.4285707 -4.4286394 -4.4286785 -4.4286904 -4.4286985 -4.4287086 -4.4287066 -4.4287028][-4.4288793 -4.428854 -4.428823 -4.4287887 -4.4287477 -4.4286866 -4.428647 -4.4286737 -4.4287028 -4.4287219 -4.4287477 -4.4287724 -4.4287834 -4.4287643 -4.4287376][-4.4288764 -4.4288497 -4.4288235 -4.4288011 -4.4287791 -4.4287515 -4.428731 -4.4287486 -4.4287615 -4.42877 -4.4287982 -4.4288259 -4.4288354 -4.4288173 -4.4287906][-4.4288731 -4.4288397 -4.4288087 -4.428791 -4.428781 -4.4287777 -4.4287796 -4.4288049 -4.4288144 -4.4288177 -4.4288383 -4.4288588 -4.4288688 -4.4288578 -4.4288378][-4.4288764 -4.4288306 -4.4287858 -4.4287586 -4.4287477 -4.4287577 -4.4287777 -4.4288216 -4.4288449 -4.4288516 -4.4288583 -4.4288673 -4.4288759 -4.4288673 -4.4288511][-4.428885 -4.4288325 -4.4287772 -4.4287353 -4.4287095 -4.428719 -4.4287496 -4.4288077 -4.4288483 -4.4288611 -4.4288592 -4.4288621 -4.4288721 -4.4288697 -4.4288626]]...]
INFO - root - 2017-12-10 05:08:49.257961: step 7610, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:55m:45s remains)
INFO - root - 2017-12-10 05:08:51.848379: step 7620, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:38m:05s remains)
INFO - root - 2017-12-10 05:08:54.438305: step 7630, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:45m:29s remains)
INFO - root - 2017-12-10 05:08:57.071900: step 7640, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.269 sec/batch; 24h:15m:44s remains)
INFO - root - 2017-12-10 05:08:59.710242: step 7650, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:45m:38s remains)
INFO - root - 2017-12-10 05:09:02.394652: step 7660, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:53m:17s remains)
INFO - root - 2017-12-10 05:09:05.029485: step 7670, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:23m:15s remains)
INFO - root - 2017-12-10 05:09:07.607452: step 7680, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:18m:09s remains)
INFO - root - 2017-12-10 05:09:10.174879: step 7690, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:04m:06s remains)
INFO - root - 2017-12-10 05:09:12.814904: step 7700, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:38m:24s remains)
2017-12-10 05:09:13.233743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286895 -4.4287271 -4.4287505 -4.4287434 -4.4287305 -4.4287362 -4.4287634 -4.42881 -4.4288683 -4.4289064 -4.4289193 -4.4289088 -4.4288774 -4.4288349 -4.4288039][-4.4285336 -4.4286027 -4.4286475 -4.4286537 -4.4286518 -4.4286628 -4.4286957 -4.4287443 -4.4288044 -4.4288397 -4.4288487 -4.4288344 -4.4287992 -4.4287558 -4.4287271][-4.428442 -4.4285288 -4.4285951 -4.4286146 -4.4286218 -4.4286318 -4.4286571 -4.4286933 -4.4287424 -4.4287744 -4.4287858 -4.4287786 -4.4287591 -4.4287329 -4.4287162][-4.4285021 -4.4285722 -4.4286304 -4.4286513 -4.4286537 -4.4286475 -4.4286447 -4.4286494 -4.4286766 -4.4287038 -4.428721 -4.4287286 -4.4287391 -4.4287424 -4.4287443][-4.4286852 -4.4287138 -4.4287329 -4.42873 -4.4287004 -4.4286542 -4.4286027 -4.4285865 -4.428607 -4.4286275 -4.4286432 -4.4286666 -4.4287052 -4.4287319 -4.4287586][-4.4288135 -4.428802 -4.4287724 -4.4287286 -4.4286561 -4.4285679 -4.4284782 -4.4284639 -4.4285 -4.4285269 -4.4285474 -4.4285822 -4.42863 -4.4286647 -4.4287171][-4.428863 -4.4288292 -4.4287562 -4.4286647 -4.4285488 -4.4284406 -4.4283476 -4.428349 -4.4284067 -4.4284444 -4.4284821 -4.4285245 -4.4285722 -4.4286089 -4.4286761][-4.4288721 -4.4288368 -4.4287486 -4.4286342 -4.4285054 -4.4284134 -4.4283495 -4.4283619 -4.4284225 -4.4284739 -4.4285231 -4.4285545 -4.4286036 -4.4286518 -4.4287176][-4.428875 -4.4288526 -4.42878 -4.4286823 -4.4285765 -4.4285107 -4.4284739 -4.4284825 -4.4285321 -4.4285951 -4.4286585 -4.428679 -4.4287171 -4.428761 -4.4288092][-4.4288683 -4.4288635 -4.4288211 -4.4287615 -4.4286947 -4.4286532 -4.4286361 -4.4286485 -4.4286923 -4.4287472 -4.4288025 -4.4288206 -4.4288492 -4.4288764 -4.4289031][-4.428833 -4.4288406 -4.4288325 -4.4288206 -4.4288025 -4.428792 -4.4287972 -4.4288154 -4.42885 -4.4288888 -4.4289212 -4.4289317 -4.4289422 -4.4289546 -4.428968][-4.4287868 -4.4288034 -4.4288244 -4.428853 -4.4288797 -4.4289012 -4.4289217 -4.4289379 -4.4289613 -4.428988 -4.4290061 -4.4290085 -4.4290071 -4.4290075 -4.4290104][-4.4287248 -4.4287491 -4.428793 -4.4288573 -4.4289155 -4.4289589 -4.4289813 -4.4289913 -4.4290047 -4.4290161 -4.4290223 -4.4290209 -4.4290185 -4.4290185 -4.429018][-4.4286814 -4.4287071 -4.4287577 -4.4288387 -4.4289155 -4.4289708 -4.4289932 -4.4289975 -4.4290004 -4.4290028 -4.4290032 -4.4290042 -4.4290066 -4.4290085 -4.4290066][-4.4286804 -4.4286985 -4.4287434 -4.4288187 -4.428895 -4.4289517 -4.4289775 -4.4289851 -4.4289894 -4.4289918 -4.4289942 -4.4289966 -4.4289994 -4.429 -4.4289956]]...]
INFO - root - 2017-12-10 05:09:15.876611: step 7710, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:19m:01s remains)
INFO - root - 2017-12-10 05:09:18.548751: step 7720, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:43m:25s remains)
INFO - root - 2017-12-10 05:09:21.160765: step 7730, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:29m:38s remains)
INFO - root - 2017-12-10 05:09:23.758152: step 7740, loss = 2.28, batch loss = 2.23 (29.6 examples/sec; 0.270 sec/batch; 24h:21m:28s remains)
INFO - root - 2017-12-10 05:09:26.414826: step 7750, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:17m:40s remains)
INFO - root - 2017-12-10 05:09:29.064558: step 7760, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.255 sec/batch; 23h:01m:31s remains)
INFO - root - 2017-12-10 05:09:31.722340: step 7770, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:07m:59s remains)
INFO - root - 2017-12-10 05:09:34.338409: step 7780, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:27m:11s remains)
INFO - root - 2017-12-10 05:09:37.027101: step 7790, loss = 2.28, batch loss = 2.23 (27.6 examples/sec; 0.290 sec/batch; 26h:11m:18s remains)
INFO - root - 2017-12-10 05:09:39.702622: step 7800, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 24h:00m:28s remains)
2017-12-10 05:09:40.119708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287853 -4.4288068 -4.4288335 -4.4288559 -4.4288778 -4.4288878 -4.4288831 -4.4288759 -4.4288616 -4.4288378 -4.4288244 -4.4288335 -4.4288406 -4.42885 -4.4288669][-4.4287596 -4.4287844 -4.4288144 -4.428843 -4.4288688 -4.4288778 -4.4288764 -4.4288735 -4.4288564 -4.42883 -4.4288173 -4.4288254 -4.4288249 -4.4288273 -4.4288406][-4.4287357 -4.4287481 -4.4287682 -4.4287982 -4.4288335 -4.428854 -4.4288673 -4.42888 -4.4288635 -4.4288378 -4.4288273 -4.4288363 -4.4288311 -4.4288282 -4.4288378][-4.4287 -4.4286957 -4.4287057 -4.4287353 -4.4287791 -4.4288111 -4.4288306 -4.42884 -4.4288158 -4.4288011 -4.4288054 -4.428822 -4.4288254 -4.4288287 -4.428844][-4.4286575 -4.4286332 -4.4286251 -4.428647 -4.4286938 -4.4287224 -4.4287314 -4.4287224 -4.4286814 -4.4286847 -4.4287238 -4.4287658 -4.4287906 -4.428813 -4.428844][-4.4285808 -4.4285321 -4.4285035 -4.4285145 -4.42855 -4.4285631 -4.4285541 -4.4285231 -4.4284663 -4.4284945 -4.4285893 -4.428679 -4.4287405 -4.4287934 -4.4288445][-4.428544 -4.428483 -4.4284449 -4.428431 -4.4284358 -4.4284148 -4.4283662 -4.4282956 -4.428206 -4.4282427 -4.4283957 -4.428546 -4.4286571 -4.428751 -4.4288278][-4.4286432 -4.4285994 -4.4285645 -4.4285316 -4.4285088 -4.4284539 -4.4283705 -4.42827 -4.4281573 -4.4281683 -4.4283209 -4.4284906 -4.4286237 -4.4287395 -4.4288273][-4.4287682 -4.4287434 -4.4287219 -4.4286909 -4.4286647 -4.4286156 -4.4285474 -4.4284654 -4.4283743 -4.428369 -4.428473 -4.4286032 -4.4287066 -4.4287987 -4.428864][-4.4288888 -4.428875 -4.4288669 -4.4288521 -4.4288316 -4.4288006 -4.4287577 -4.4287047 -4.4286404 -4.4286222 -4.4286695 -4.4287448 -4.4288044 -4.4288583 -4.4288964][-4.4289732 -4.4289622 -4.428957 -4.4289517 -4.4289432 -4.4289317 -4.428915 -4.4288855 -4.4288473 -4.4288297 -4.4288468 -4.42888 -4.428905 -4.4289193 -4.4289265][-4.4289794 -4.4289742 -4.4289742 -4.4289746 -4.4289794 -4.4289851 -4.4289818 -4.4289651 -4.4289412 -4.4289341 -4.4289489 -4.428966 -4.4289708 -4.4289632 -4.4289494][-4.4289579 -4.4289508 -4.4289508 -4.4289551 -4.4289632 -4.4289742 -4.428977 -4.4289637 -4.4289417 -4.4289393 -4.42896 -4.4289789 -4.4289827 -4.4289751 -4.4289594][-4.4289541 -4.428946 -4.4289422 -4.4289451 -4.4289522 -4.4289594 -4.4289608 -4.4289474 -4.4289274 -4.4289293 -4.4289551 -4.428978 -4.4289846 -4.4289784 -4.4289641][-4.4289322 -4.4289241 -4.4289207 -4.42892 -4.4289231 -4.428926 -4.4289265 -4.4289193 -4.4289083 -4.4289136 -4.4289417 -4.42897 -4.4289808 -4.4289746 -4.4289627]]...]
INFO - root - 2017-12-10 05:09:42.727883: step 7810, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:19m:05s remains)
INFO - root - 2017-12-10 05:09:45.339064: step 7820, loss = 2.28, batch loss = 2.23 (30.9 examples/sec; 0.259 sec/batch; 23h:21m:53s remains)
INFO - root - 2017-12-10 05:09:47.976569: step 7830, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:49m:19s remains)
INFO - root - 2017-12-10 05:09:50.645212: step 7840, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:25m:57s remains)
INFO - root - 2017-12-10 05:09:53.322869: step 7850, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:26m:39s remains)
INFO - root - 2017-12-10 05:09:55.952334: step 7860, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:14m:32s remains)
INFO - root - 2017-12-10 05:09:58.626278: step 7870, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.270 sec/batch; 24h:19m:33s remains)
INFO - root - 2017-12-10 05:10:01.271532: step 7880, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:14m:07s remains)
INFO - root - 2017-12-10 05:10:03.908065: step 7890, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:41m:21s remains)
INFO - root - 2017-12-10 05:10:06.538854: step 7900, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:34m:43s remains)
2017-12-10 05:10:06.984428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288774 -4.4288635 -4.4288683 -4.4288759 -4.428885 -4.4288893 -4.4288869 -4.42888 -4.4288793 -4.4288878 -4.4289017 -4.4289136 -4.4289188 -4.4289222 -4.4289308][-4.4288044 -4.4287848 -4.4287992 -4.4288182 -4.4288311 -4.4288354 -4.4288297 -4.4288216 -4.4288187 -4.42883 -4.4288507 -4.4288692 -4.4288793 -4.4288864 -4.4288964][-4.4287376 -4.4287133 -4.4287381 -4.4287691 -4.4287863 -4.4287882 -4.4287772 -4.4287643 -4.4287581 -4.4287748 -4.4288039 -4.42883 -4.4288435 -4.4288526 -4.4288621][-4.4286914 -4.4286675 -4.4287009 -4.4287438 -4.4287643 -4.4287643 -4.4287496 -4.4287295 -4.42872 -4.428741 -4.4287782 -4.42881 -4.4288278 -4.4288383 -4.4288478][-4.42868 -4.4286613 -4.4286976 -4.4287405 -4.4287591 -4.4287543 -4.4287348 -4.4287095 -4.4286957 -4.428719 -4.428761 -4.4287972 -4.4288182 -4.4288316 -4.4288416][-4.4286895 -4.4286823 -4.4287205 -4.4287553 -4.4287653 -4.4287486 -4.428721 -4.4286895 -4.4286737 -4.4286933 -4.4287357 -4.4287763 -4.4288025 -4.4288216 -4.4288383][-4.4287109 -4.4287124 -4.4287462 -4.4287658 -4.4287634 -4.4287405 -4.4287071 -4.4286737 -4.4286618 -4.4286804 -4.4287233 -4.4287667 -4.4287953 -4.4288173 -4.4288363][-4.4287219 -4.4287243 -4.4287481 -4.4287548 -4.4287462 -4.4287238 -4.4286928 -4.4286675 -4.4286656 -4.4286871 -4.4287319 -4.4287729 -4.4287963 -4.4288149 -4.4288316][-4.4287133 -4.4287128 -4.4287353 -4.4287405 -4.4287357 -4.4287243 -4.4287057 -4.4286928 -4.42869 -4.4287038 -4.4287391 -4.4287682 -4.4287729 -4.428781 -4.4287939][-4.4287024 -4.4286985 -4.4287243 -4.4287376 -4.4287467 -4.4287539 -4.4287477 -4.4287391 -4.4287286 -4.4287291 -4.4287486 -4.4287562 -4.4287353 -4.4287295 -4.4287415][-4.4286966 -4.4286895 -4.4287195 -4.4287467 -4.4287663 -4.4287858 -4.4287863 -4.4287796 -4.4287643 -4.4287486 -4.4287443 -4.42873 -4.428689 -4.4286752 -4.4286966][-4.4286876 -4.4286814 -4.4287162 -4.4287553 -4.4287853 -4.4288135 -4.4288154 -4.4288054 -4.4287806 -4.4287496 -4.4287281 -4.4287057 -4.4286604 -4.4286451 -4.4286795][-4.4286761 -4.4286752 -4.4287148 -4.4287629 -4.4288034 -4.4288349 -4.4288335 -4.4288111 -4.4287753 -4.4287357 -4.4287095 -4.4286971 -4.4286695 -4.4286637 -4.4287052][-4.4286718 -4.4286761 -4.4287167 -4.4287691 -4.4288173 -4.42885 -4.4288464 -4.4288163 -4.4287767 -4.42874 -4.4287257 -4.4287305 -4.4287243 -4.4287248 -4.4287615][-4.428699 -4.4287128 -4.4287515 -4.4287963 -4.4288387 -4.428865 -4.4288592 -4.4288306 -4.4287944 -4.4287691 -4.4287663 -4.4287791 -4.4287839 -4.4287891 -4.4288182]]...]
INFO - root - 2017-12-10 05:10:09.700073: step 7910, loss = 2.28, batch loss = 2.23 (29.5 examples/sec; 0.271 sec/batch; 24h:28m:39s remains)
INFO - root - 2017-12-10 05:10:12.368775: step 7920, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:47m:58s remains)
INFO - root - 2017-12-10 05:10:15.014093: step 7930, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:58m:19s remains)
INFO - root - 2017-12-10 05:10:17.642428: step 7940, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:34m:34s remains)
INFO - root - 2017-12-10 05:10:20.281142: step 7950, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:35m:20s remains)
INFO - root - 2017-12-10 05:10:22.962969: step 7960, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:46m:44s remains)
INFO - root - 2017-12-10 05:10:25.629930: step 7970, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:15m:15s remains)
INFO - root - 2017-12-10 05:10:28.271845: step 7980, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:26m:51s remains)
INFO - root - 2017-12-10 05:10:30.864561: step 7990, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:28m:59s remains)
INFO - root - 2017-12-10 05:10:33.476746: step 8000, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:56m:40s remains)
2017-12-10 05:10:33.833152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287715 -4.4287724 -4.428792 -4.4288125 -4.4288249 -4.4288363 -4.4288473 -4.4288383 -4.4288225 -4.4288297 -4.428844 -4.4288435 -4.428833 -4.4288235 -4.4288125][-4.4287953 -4.4287992 -4.4288187 -4.4288354 -4.428844 -4.4288521 -4.4288564 -4.4288387 -4.4288135 -4.4288125 -4.4288249 -4.4288239 -4.4288054 -4.4287786 -4.4287548][-4.4288459 -4.4288526 -4.4288645 -4.4288712 -4.4288697 -4.4288645 -4.4288568 -4.4288354 -4.4288044 -4.42879 -4.4287972 -4.4287949 -4.4287672 -4.4287171 -4.4286752][-4.4288807 -4.4288807 -4.42888 -4.4288726 -4.42886 -4.4288349 -4.4288039 -4.4287744 -4.4287376 -4.4287171 -4.4287214 -4.4287176 -4.428679 -4.4286089 -4.4285541][-4.428874 -4.4288611 -4.4288397 -4.4288116 -4.4287815 -4.4287286 -4.4286661 -4.4286261 -4.4285975 -4.4285965 -4.428627 -4.4286389 -4.4286003 -4.4285297 -4.428484][-4.4288125 -4.4287872 -4.4287457 -4.4286942 -4.4286432 -4.4285707 -4.4284949 -4.428462 -4.4284592 -4.4284987 -4.4285703 -4.4286065 -4.4285851 -4.4285374 -4.4285164][-4.4287295 -4.4287167 -4.4286757 -4.428617 -4.4285622 -4.4284945 -4.428431 -4.4284163 -4.4284306 -4.4284811 -4.42856 -4.4286022 -4.4285913 -4.4285564 -4.4285522][-4.4286876 -4.4287024 -4.42869 -4.4286451 -4.4285984 -4.4285464 -4.4285007 -4.4284945 -4.4285026 -4.4285278 -4.428576 -4.4286056 -4.4285989 -4.428575 -4.4285836][-4.4287 -4.4287233 -4.4287305 -4.4287047 -4.428669 -4.4286318 -4.4285913 -4.4285803 -4.4285784 -4.4285831 -4.4286089 -4.4286308 -4.4286342 -4.4286261 -4.4286432][-4.4287362 -4.4287572 -4.4287734 -4.4287667 -4.42875 -4.4287233 -4.4286866 -4.4286647 -4.4286466 -4.4286408 -4.4286547 -4.4286718 -4.4286833 -4.4286871 -4.4287024][-4.4287839 -4.4288006 -4.4288216 -4.42883 -4.4288282 -4.42881 -4.4287767 -4.4287405 -4.4287066 -4.4286895 -4.4286909 -4.4287 -4.4287124 -4.4287176 -4.4287238][-4.4288459 -4.4288573 -4.4288731 -4.4288783 -4.4288783 -4.428863 -4.4288273 -4.428782 -4.4287372 -4.4287124 -4.4287057 -4.4287043 -4.4287133 -4.4287229 -4.4287324][-4.4289002 -4.4288979 -4.4288983 -4.4288936 -4.4288869 -4.4288716 -4.42884 -4.4287977 -4.4287548 -4.4287319 -4.4287262 -4.4287229 -4.4287286 -4.4287391 -4.4287496][-4.4289289 -4.4289112 -4.428895 -4.4288812 -4.4288726 -4.4288616 -4.4288416 -4.4288168 -4.4287915 -4.4287782 -4.4287767 -4.4287739 -4.4287686 -4.4287653 -4.4287615][-4.428937 -4.4289145 -4.42889 -4.4288712 -4.428863 -4.4288597 -4.4288526 -4.4288449 -4.4288383 -4.4288344 -4.4288344 -4.4288259 -4.4288096 -4.4287891 -4.4287653]]...]
INFO - root - 2017-12-10 05:10:36.442914: step 8010, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:47m:54s remains)
INFO - root - 2017-12-10 05:10:39.059041: step 8020, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:02m:31s remains)
INFO - root - 2017-12-10 05:10:41.758649: step 8030, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:56m:24s remains)
INFO - root - 2017-12-10 05:10:44.405617: step 8040, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.257 sec/batch; 23h:10m:49s remains)
INFO - root - 2017-12-10 05:10:47.049988: step 8050, loss = 2.28, batch loss = 2.23 (29.4 examples/sec; 0.272 sec/batch; 24h:32m:00s remains)
INFO - root - 2017-12-10 05:10:49.699795: step 8060, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.266 sec/batch; 23h:59m:53s remains)
INFO - root - 2017-12-10 05:10:52.349332: step 8070, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:28s remains)
INFO - root - 2017-12-10 05:10:54.976145: step 8080, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:25m:23s remains)
INFO - root - 2017-12-10 05:10:57.546501: step 8090, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:56m:54s remains)
INFO - root - 2017-12-10 05:11:00.188250: step 8100, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:36m:18s remains)
2017-12-10 05:11:00.530210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428771 -4.4287338 -4.4287286 -4.4287596 -4.4288 -4.4288244 -4.4288168 -4.428802 -4.4287972 -4.428791 -4.4287972 -4.4288235 -4.4288578 -4.4288797 -4.428895][-4.4287505 -4.4287114 -4.4287033 -4.4287329 -4.4287715 -4.4287834 -4.4287691 -4.4287624 -4.4287715 -4.4287815 -4.4287953 -4.4288182 -4.428844 -4.4288588 -4.4288707][-4.4287415 -4.4287086 -4.4286962 -4.4287176 -4.42875 -4.4287486 -4.4287243 -4.42873 -4.428761 -4.4287906 -4.4288092 -4.4288216 -4.4288325 -4.4288392 -4.4288511][-4.4287372 -4.4287152 -4.428699 -4.4287047 -4.4287271 -4.4287205 -4.4286981 -4.4287224 -4.428771 -4.4288144 -4.4288387 -4.428844 -4.4288363 -4.4288316 -4.42884][-4.4287395 -4.4287286 -4.4287076 -4.4286923 -4.4286995 -4.4286861 -4.4286685 -4.4287109 -4.4287734 -4.4288278 -4.428864 -4.4288673 -4.4288468 -4.4288316 -4.4288383][-4.4287362 -4.4287305 -4.428709 -4.4286785 -4.4286695 -4.4286466 -4.4286261 -4.4286828 -4.4287615 -4.4288249 -4.42887 -4.4288769 -4.4288526 -4.42883 -4.4288387][-4.4287248 -4.4287214 -4.4287052 -4.4286737 -4.4286494 -4.4286008 -4.4285574 -4.4286175 -4.428721 -4.4288025 -4.428854 -4.4288626 -4.4288421 -4.4288249 -4.42884][-4.4287314 -4.42872 -4.4287043 -4.4286752 -4.4286318 -4.4285483 -4.4284649 -4.4285164 -4.4286509 -4.4287586 -4.4288197 -4.4288316 -4.4288211 -4.4288187 -4.4288459][-4.428762 -4.4287295 -4.4287033 -4.4286761 -4.4286237 -4.428524 -4.4284167 -4.4284587 -4.42861 -4.4287319 -4.4287925 -4.4288058 -4.4288077 -4.4288273 -4.428864][-4.4288058 -4.4287486 -4.4287057 -4.4286861 -4.4286513 -4.4285789 -4.4284945 -4.4285216 -4.4286456 -4.4287481 -4.4287939 -4.4288034 -4.4288149 -4.4288483 -4.4288888][-4.4288521 -4.4287825 -4.428731 -4.4287171 -4.4287086 -4.4286776 -4.4286337 -4.4286561 -4.4287357 -4.4287987 -4.428823 -4.4288287 -4.428844 -4.4288774 -4.4289122][-4.4288764 -4.428812 -4.4287624 -4.4287524 -4.4287653 -4.4287705 -4.4287629 -4.428782 -4.4288211 -4.4288473 -4.4288554 -4.4288588 -4.42887 -4.4288979 -4.4289255][-4.428885 -4.4288335 -4.428793 -4.4287853 -4.4288116 -4.4288387 -4.4288492 -4.428864 -4.4288764 -4.4288774 -4.4288745 -4.4288759 -4.4288826 -4.4289041 -4.4289274][-4.428885 -4.4288507 -4.4288216 -4.4288135 -4.4288406 -4.4288759 -4.4288921 -4.4289026 -4.4289026 -4.428894 -4.4288907 -4.428894 -4.4289012 -4.4289188 -4.4289331][-4.4288917 -4.428875 -4.4288568 -4.4288425 -4.4288588 -4.428894 -4.4289093 -4.428915 -4.4289136 -4.4289045 -4.4289036 -4.4289122 -4.4289236 -4.4289389 -4.4289427]]...]
INFO - root - 2017-12-10 05:11:03.183774: step 8110, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:42m:07s remains)
INFO - root - 2017-12-10 05:11:05.812415: step 8120, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:52m:44s remains)
INFO - root - 2017-12-10 05:11:08.469440: step 8130, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:41m:49s remains)
INFO - root - 2017-12-10 05:11:11.072244: step 8140, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:55m:25s remains)
INFO - root - 2017-12-10 05:11:13.670677: step 8150, loss = 2.28, batch loss = 2.23 (30.4 examples/sec; 0.263 sec/batch; 23h:42m:21s remains)
INFO - root - 2017-12-10 05:11:16.291252: step 8160, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.262 sec/batch; 23h:38m:22s remains)
INFO - root - 2017-12-10 05:11:18.927256: step 8170, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:53m:53s remains)
INFO - root - 2017-12-10 05:11:21.618229: step 8180, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.262 sec/batch; 23h:34m:11s remains)
INFO - root - 2017-12-10 05:11:24.257208: step 8190, loss = 2.28, batch loss = 2.23 (29.9 examples/sec; 0.268 sec/batch; 24h:06m:35s remains)
INFO - root - 2017-12-10 05:11:26.882855: step 8200, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 23h:01m:29s remains)
2017-12-10 05:11:27.269042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290466 -4.4290233 -4.4289632 -4.428853 -4.4287386 -4.4286366 -4.4285541 -4.4285641 -4.4286342 -4.4286585 -4.4286451 -4.4286523 -4.4286971 -4.4287591 -4.4288082][-4.4290586 -4.4290352 -4.4289708 -4.4288559 -4.4287348 -4.4286232 -4.4285364 -4.4285569 -4.428638 -4.4286747 -4.4286742 -4.4286857 -4.4287238 -4.4287739 -4.4288111][-4.4290576 -4.42903 -4.4289579 -4.4288421 -4.4287176 -4.4286051 -4.4285293 -4.4285727 -4.4286709 -4.4287271 -4.4287477 -4.4287639 -4.4287944 -4.4288263 -4.4288454][-4.4290481 -4.4290142 -4.428937 -4.4288211 -4.4286995 -4.4285903 -4.4285226 -4.4285817 -4.4286962 -4.428772 -4.42881 -4.4288306 -4.4288545 -4.4288754 -4.428884][-4.4290423 -4.42901 -4.4289303 -4.4288116 -4.4286852 -4.4285665 -4.4284825 -4.4285336 -4.4286609 -4.4287572 -4.4288144 -4.4288473 -4.42888 -4.428906 -4.4289155][-4.4290442 -4.429018 -4.4289412 -4.4288173 -4.4286771 -4.4285283 -4.4283915 -4.4284019 -4.4285359 -4.4286628 -4.4287519 -4.4288111 -4.4288654 -4.4289064 -4.4289246][-4.42905 -4.4290314 -4.4289622 -4.4288325 -4.42867 -4.4284711 -4.428246 -4.4281926 -4.4283252 -4.4284892 -4.4286284 -4.4287248 -4.4288044 -4.4288611 -4.4288936][-4.4290509 -4.4290357 -4.4289713 -4.4288387 -4.4286585 -4.4284105 -4.4281063 -4.4280109 -4.4281564 -4.4283524 -4.4285226 -4.4286447 -4.4287405 -4.4288039 -4.4288459][-4.4290466 -4.4290318 -4.4289713 -4.4288449 -4.4286733 -4.4284248 -4.4281192 -4.4280391 -4.428195 -4.4283814 -4.4285336 -4.4286447 -4.4287291 -4.4287796 -4.4288158][-4.42904 -4.4290237 -4.428966 -4.4288559 -4.4287229 -4.4285283 -4.4282832 -4.4282408 -4.4283838 -4.4285278 -4.4286337 -4.4287081 -4.4287653 -4.4287944 -4.4288187][-4.4290304 -4.4290128 -4.4289575 -4.4288635 -4.4287677 -4.4286342 -4.4284582 -4.428442 -4.4285645 -4.4286671 -4.428731 -4.4287715 -4.4288044 -4.4288206 -4.4288383][-4.4290204 -4.4290032 -4.4289575 -4.4288764 -4.4288025 -4.4287086 -4.4285879 -4.4285884 -4.42869 -4.4287615 -4.4287987 -4.4288168 -4.4288321 -4.42884 -4.4288554][-4.4290137 -4.4289932 -4.4289546 -4.4288831 -4.4288177 -4.4287496 -4.4286714 -4.428689 -4.428782 -4.4288368 -4.4288559 -4.4288564 -4.4288578 -4.4288597 -4.4288712][-4.4290147 -4.4289942 -4.4289594 -4.4288888 -4.4288239 -4.42876 -4.4287024 -4.4287286 -4.4288235 -4.4288807 -4.4289 -4.4288964 -4.4288917 -4.4288874 -4.4288912][-4.4290237 -4.4290061 -4.4289722 -4.4288974 -4.4288216 -4.4287443 -4.4286776 -4.4286985 -4.4288034 -4.4288778 -4.4289126 -4.42892 -4.4289193 -4.4289165 -4.4289145]]...]
INFO - root - 2017-12-10 05:11:29.884814: step 8210, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:46m:26s remains)
INFO - root - 2017-12-10 05:11:32.482985: step 8220, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:55m:32s remains)
INFO - root - 2017-12-10 05:11:35.124641: step 8230, loss = 2.28, batch loss = 2.23 (30.3 examples/sec; 0.264 sec/batch; 23h:45m:29s remains)
INFO - root - 2017-12-10 05:11:37.840032: step 8240, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:33m:04s remains)
INFO - root - 2017-12-10 05:11:40.468715: step 8250, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:14m:10s remains)
INFO - root - 2017-12-10 05:11:43.095084: step 8260, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.255 sec/batch; 22h:55m:40s remains)
INFO - root - 2017-12-10 05:11:45.762106: step 8270, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:15m:45s remains)
INFO - root - 2017-12-10 05:11:48.466084: step 8280, loss = 2.28, batch loss = 2.23 (29.7 examples/sec; 0.269 sec/batch; 24h:14m:09s remains)
INFO - root - 2017-12-10 05:11:51.067815: step 8290, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 23h:27m:32s remains)
INFO - root - 2017-12-10 05:11:53.714730: step 8300, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 23h:23m:43s remains)
2017-12-10 05:11:54.100596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290271 -4.4290223 -4.4290161 -4.4290104 -4.429009 -4.4290109 -4.4290118 -4.4290109 -4.4290094 -4.4290075 -4.4290071 -4.4290071 -4.4290061 -4.4290051 -4.429008][-4.429028 -4.4290242 -4.4290171 -4.429009 -4.4290042 -4.4290032 -4.4290013 -4.4290009 -4.4290013 -4.4290028 -4.4290023 -4.4289985 -4.4289918 -4.4289846 -4.4289832][-4.4290295 -4.4290247 -4.4290137 -4.4290018 -4.4289923 -4.4289827 -4.4289742 -4.4289713 -4.4289732 -4.428978 -4.4289789 -4.4289713 -4.4289584 -4.4289446 -4.4289393][-4.4290214 -4.4290094 -4.4289918 -4.4289751 -4.428956 -4.4289308 -4.4289074 -4.4288931 -4.4288926 -4.4289041 -4.4289107 -4.4289007 -4.4288836 -4.4288678 -4.4288659][-4.4289875 -4.4289632 -4.428937 -4.4289126 -4.4288793 -4.4288354 -4.4287863 -4.4287477 -4.428741 -4.4287682 -4.4287887 -4.4287844 -4.4287715 -4.4287639 -4.4287758][-4.4289322 -4.4288893 -4.428853 -4.4288235 -4.428782 -4.4287205 -4.428638 -4.4285617 -4.4285545 -4.4286208 -4.4286709 -4.428679 -4.4286737 -4.4286752 -4.4287004][-4.4288836 -4.4288216 -4.4287796 -4.42875 -4.4286976 -4.4286103 -4.4284887 -4.4283662 -4.4283686 -4.4284992 -4.4285927 -4.4286146 -4.4286141 -4.4286227 -4.42866][-4.4288621 -4.428791 -4.4287453 -4.428709 -4.4286385 -4.4285207 -4.4283652 -4.4282112 -4.4282355 -4.4284239 -4.4285579 -4.4285979 -4.4286008 -4.4286118 -4.4286542][-4.4288845 -4.42882 -4.4287724 -4.4287257 -4.4286413 -4.4285169 -4.4283752 -4.4282532 -4.4282937 -4.4284763 -4.4286127 -4.42866 -4.4286661 -4.4286752 -4.4287028][-4.4289126 -4.4288597 -4.4288087 -4.4287539 -4.4286709 -4.4285684 -4.4284668 -4.4283905 -4.4284253 -4.4285636 -4.4286809 -4.4287381 -4.4287558 -4.4287615 -4.4287677][-4.4289217 -4.4288797 -4.4288325 -4.4287825 -4.4287205 -4.4286532 -4.4285903 -4.4285431 -4.4285531 -4.4286351 -4.4287262 -4.428791 -4.4288211 -4.428823 -4.4288163][-4.4289336 -4.4289026 -4.4288721 -4.4288411 -4.4288044 -4.4287658 -4.42873 -4.428699 -4.428688 -4.4287262 -4.4287868 -4.4288487 -4.428885 -4.4288888 -4.428875][-4.4289527 -4.428936 -4.4289217 -4.4289107 -4.4288983 -4.4288812 -4.4288607 -4.4288406 -4.4288211 -4.4288278 -4.4288568 -4.4289002 -4.4289322 -4.4289408 -4.4289303][-4.4289727 -4.4289646 -4.4289594 -4.4289613 -4.428967 -4.4289646 -4.4289556 -4.4289446 -4.4289265 -4.4289155 -4.4289193 -4.4289408 -4.4289622 -4.4289732 -4.42897][-4.4289942 -4.4289865 -4.4289818 -4.4289861 -4.4289947 -4.428997 -4.4289956 -4.4289927 -4.4289832 -4.4289713 -4.428967 -4.4289756 -4.4289875 -4.4289961 -4.4289961]]...]
INFO - root - 2017-12-10 05:11:56.709202: step 8310, loss = 2.28, batch loss = 2.23 (30.0 examples/sec; 0.267 sec/batch; 24h:00m:40s remains)
INFO - root - 2017-12-10 05:11:59.363027: step 8320, loss = 2.28, batch loss = 2.23 (31.2 examples/sec; 0.256 sec/batch; 23h:03m:35s remains)
INFO - root - 2017-12-10 05:12:02.006558: step 8330, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:30m:48s remains)
INFO - root - 2017-12-10 05:12:04.648165: step 8340, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:39m:10s remains)
INFO - root - 2017-12-10 05:12:07.242862: step 8350, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.265 sec/batch; 23h:53m:37s remains)
INFO - root - 2017-12-10 05:12:09.852827: step 8360, loss = 2.28, batch loss = 2.23 (29.0 examples/sec; 0.276 sec/batch; 24h:50m:13s remains)
INFO - root - 2017-12-10 05:12:12.455535: step 8370, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:31m:27s remains)
INFO - root - 2017-12-10 05:12:15.115405: step 8380, loss = 2.28, batch loss = 2.23 (30.1 examples/sec; 0.266 sec/batch; 23h:57m:46s remains)
INFO - root - 2017-12-10 05:12:17.757125: step 8390, loss = 2.28, batch loss = 2.23 (30.5 examples/sec; 0.263 sec/batch; 23h:38m:16s remains)
INFO - root - 2017-12-10 05:12:20.364850: step 8400, loss = 2.28, batch loss = 2.23 (31.1 examples/sec; 0.258 sec/batch; 23h:11m:14s remains)
2017-12-10 05:12:20.761639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.429028 -4.4290214 -4.4289923 -4.4289184 -4.4288135 -4.4287267 -4.4286923 -4.4287167 -4.4287848 -4.4288645 -4.4289346 -4.4289713 -4.4289722 -4.4289508 -4.4289317][-4.4290409 -4.4290428 -4.4290252 -4.4289675 -4.4288831 -4.4288034 -4.4287524 -4.4287381 -4.4287643 -4.4288216 -4.4288907 -4.42894 -4.4289589 -4.4289546 -4.4289479][-4.4290514 -4.4290538 -4.4290395 -4.4289913 -4.4289174 -4.4288335 -4.4287643 -4.42872 -4.4287167 -4.4287572 -4.4288325 -4.4289026 -4.4289489 -4.42897 -4.4289794][-4.4290576 -4.4290576 -4.4290428 -4.428998 -4.4289246 -4.4288297 -4.4287448 -4.428678 -4.428647 -4.4286757 -4.4287581 -4.4288507 -4.42893 -4.4289832 -4.4290156][-4.4290624 -4.4290595 -4.4290438 -4.4289985 -4.4289222 -4.4288197 -4.4287133 -4.4286137 -4.4285464 -4.4285641 -4.4286575 -4.4287724 -4.4288845 -4.4289703 -4.4290276][-4.4290662 -4.4290581 -4.4290338 -4.4289784 -4.4288883 -4.428772 -4.4286337 -4.4284835 -4.4283767 -4.4284029 -4.4285288 -4.4286752 -4.4288197 -4.4289351 -4.4290156][-4.4290662 -4.4290504 -4.4290075 -4.4289289 -4.4288116 -4.4286714 -4.4285026 -4.4283094 -4.4281769 -4.4282365 -4.4284067 -4.4285846 -4.428751 -4.4288869 -4.4289861][-4.4290576 -4.42903 -4.428968 -4.428865 -4.4287252 -4.4285741 -4.4283972 -4.4281926 -4.4280658 -4.4281549 -4.4283428 -4.42853 -4.4287028 -4.4288492 -4.4289594][-4.4290404 -4.4290028 -4.4289269 -4.4288077 -4.4286594 -4.4285159 -4.4283624 -4.4281993 -4.4281235 -4.4282155 -4.4283743 -4.4285378 -4.4286957 -4.4288368 -4.428947][-4.4290237 -4.4289808 -4.4289 -4.4287786 -4.4286385 -4.4285131 -4.4283967 -4.4282956 -4.4282765 -4.4283633 -4.4284849 -4.4286151 -4.4287448 -4.428865 -4.42896][-4.4290137 -4.4289737 -4.4289021 -4.4287949 -4.4286747 -4.4285731 -4.4284992 -4.4284592 -4.4284825 -4.4285574 -4.4286442 -4.42874 -4.4288368 -4.428925 -4.4289885][-4.4290109 -4.42898 -4.42893 -4.4288545 -4.4287691 -4.4287 -4.4286633 -4.4286647 -4.4287043 -4.4287577 -4.4288135 -4.4288778 -4.42894 -4.4289904 -4.4290161][-4.4290128 -4.4289947 -4.42897 -4.4289308 -4.4288831 -4.4288468 -4.4288368 -4.4288526 -4.4288855 -4.4289174 -4.42895 -4.4289865 -4.4290156 -4.4290304 -4.4290252][-4.4290161 -4.4290094 -4.4290042 -4.4289932 -4.4289756 -4.4289632 -4.428966 -4.4289808 -4.4290009 -4.4290175 -4.4290314 -4.4290414 -4.4290404 -4.4290314 -4.4290128][-4.4290204 -4.4290214 -4.4290271 -4.4290304 -4.4290295 -4.4290271 -4.42903 -4.429038 -4.4290476 -4.4290528 -4.4290514 -4.4290423 -4.4290257 -4.429008 -4.4289842]]...]
INFO - root - 2017-12-10 05:12:23.338031: step 8410, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 22h:46m:57s remains)
INFO - root - 2017-12-10 05:12:25.973417: step 8420, loss = 2.28, batch loss = 2.23 (29.1 examples/sec; 0.275 sec/batch; 24h:45m:50s remains)
INFO - root - 2017-12-10 05:12:28.611041: step 8430, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:15m:21s remains)
INFO - root - 2017-12-10 05:12:31.233936: step 8440, loss = 2.28, batch loss = 2.23 (30.2 examples/sec; 0.265 sec/batch; 23h:49m:54s remains)
INFO - root - 2017-12-10 05:12:33.873081: step 8450, loss = 2.28, batch loss = 2.23 (30.6 examples/sec; 0.261 sec/batch; 23h:30m:41s remains)
INFO - root - 2017-12-10 05:12:36.453944: step 8460, loss = 2.28, batch loss = 2.23 (31.0 examples/sec; 0.258 sec/batch; 23h:13m:35s remains)
INFO - root - 2017-12-10 05:12:39.095842: step 8470, loss = 2.28, batch loss = 2.23 (29.8 examples/sec; 0.268 sec/batch; 24h:09m:54s remains)
INFO - root - 2017-12-10 05:12:41.715335: step 8480, loss = 2.28, batch loss = 2.23 (29.3 examples/sec; 0.273 sec/batch; 24h:36m:08s remains)
