INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "206"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 03:25:30.022934: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 03:25:30.022996: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 03:25:30.023022: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 03:25:30.023043: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 03:25:30.023063: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc/conv4/b1/Relu:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv4/b1/Relu:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 384, 18) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref>]
2017-12-11 03:25:38.863796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-11 03:25:38.863850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 03:25:38.863859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 03:25:38.863869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-11 03:25:55.414395: step 0, loss = 0.75, batch loss = 0.69 (0.7 examples/sec; 12.242 sec/batch; 1130h:42m:35s remains)
2017-12-11 03:25:56.232166: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012263989 0.00011773103 0.0001118873 0.00010711735 0.00010887202 0.00012237922 0.00014652454 0.00017196192 0.00018451254 0.00018097772 0.0001623042 0.00013555965 0.00011637341 0.00010850259 0.00010428236][0.00012508083 0.00012036376 0.00011658175 0.00011648147 0.00012809463 0.00015714511 0.00019858385 0.00023553031 0.00024714292 0.00023177449 0.00019536309 0.00015294173 0.00012506211 0.00011384413 0.00010711624][0.00012971286 0.000126751 0.00012586842 0.00013374357 0.00015960418 0.00020859964 0.00027292472 0.00032652338 0.00033922697 0.00030670606 0.00024731806 0.00018485612 0.0001427697 0.00012201558 0.00011064403][0.00013812221 0.00013464819 0.00013501239 0.00015102574 0.00019191827 0.00026276358 0.00035316937 0.00043123538 0.00044986149 0.00039594673 0.00031233934 0.00022483042 0.00016405043 0.0001322594 0.00011587899][0.00014685746 0.00014258682 0.00014577636 0.00017218647 0.00022924847 0.00032413445 0.00044911826 0.00056467403 0.00058554026 0.00049971027 0.00038360807 0.00026698975 0.00018617626 0.0001428603 0.00012221643][0.00015948087 0.00015651449 0.00016515944 0.00020366363 0.00027680481 0.00039656993 0.00055800937 0.00071228627 0.00071633008 0.00058129511 0.00043173143 0.00029704234 0.00020467461 0.00015289322 0.00012843381][0.00017389559 0.0001757115 0.00019072006 0.00023567487 0.00031578681 0.00044558928 0.00061752589 0.00077217084 0.00073089963 0.00056171871 0.00040956869 0.00028576615 0.00020137119 0.00015279815 0.00013003571][0.00018461933 0.00019190584 0.00020989389 0.00025083742 0.00031989749 0.00042594541 0.00055499823 0.00064944482 0.000577972 0.00043770697 0.00032824959 0.00024362169 0.00018064311 0.00014316659 0.00012709494][0.0001916358 0.00020144491 0.00021619756 0.00024616224 0.00029521025 0.00036173739 0.00043484822 0.00047551308 0.00041076291 0.00031993294 0.00025929118 0.00021048296 0.00016357281 0.00013323087 0.00012241509][0.00019534011 0.00020556981 0.00021654437 0.00023590805 0.00026490047 0.00029623249 0.0003314708 0.00034527702 0.00030073005 0.00024686 0.00021726347 0.00018876659 0.00015097573 0.00012487327 0.00011721167][0.0001931369 0.00020052904 0.00020987181 0.00022204264 0.00023567094 0.00024427945 0.00025880794 0.00026328664 0.00023753203 0.00020546291 0.00019080041 0.00016869587 0.00013551116 0.00011435249 0.00011108474][0.00018003443 0.00017981401 0.00018568322 0.0001915905 0.00019567234 0.00019200445 0.00019927224 0.00020693219 0.00019768359 0.00018220041 0.00017427142 0.00015333459 0.00012310795 0.00010608338 0.00010659887][0.00015496371 0.00014756089 0.0001496202 0.00015322582 0.00015599809 0.0001534375 0.00016502042 0.00018140049 0.00018602006 0.00018241185 0.00017658011 0.0001531477 0.00012089466 0.00010420591 0.00010584783][0.00012934562 0.00011910082 0.00011996735 0.00012289877 0.00012574668 0.00012981519 0.00015207908 0.00017979849 0.00019422009 0.00019564916 0.0001883572 0.0001598411 0.00012367165 0.00010664263 0.00010779043][0.00012387117 0.00011390237 0.00011397916 0.0001156094 0.00011720819 0.00012956272 0.0001615209 0.00019531832 0.00021202043 0.00021121062 0.00019940952 0.00016575106 0.00012747917 0.00010974838 0.00010945582]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:26:03.387358: step 10, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.561 sec/batch; 51h:49m:57s remains)
INFO - root - 2017-12-11 03:26:08.757998: step 20, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.514 sec/batch; 47h:29m:18s remains)
INFO - root - 2017-12-11 03:26:14.067451: step 30, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.512 sec/batch; 47h:17m:58s remains)
INFO - root - 2017-12-11 03:26:19.229595: step 40, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 50h:15m:04s remains)
INFO - root - 2017-12-11 03:26:24.611818: step 50, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 48h:06m:17s remains)
INFO - root - 2017-12-11 03:26:29.948112: step 60, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:43m:34s remains)
INFO - root - 2017-12-11 03:26:35.245782: step 70, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.523 sec/batch; 48h:19m:41s remains)
INFO - root - 2017-12-11 03:26:40.592402: step 80, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.555 sec/batch; 51h:14m:04s remains)
INFO - root - 2017-12-11 03:26:45.966561: step 90, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.558 sec/batch; 51h:31m:26s remains)
INFO - root - 2017-12-11 03:26:51.216009: step 100, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 50h:16m:20s remains)
2017-12-11 03:26:51.807851: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0001323842 0.00014681836 0.00016033929 0.00017090217 0.00016331441 0.00015310691 0.0001384866 0.00012208248 0.00011121164 0.00011498472 0.00011588627 0.00011647255 0.00013239673 0.00015363829 0.00016432593][0.00013321993 0.00015426768 0.00017709925 0.00019560484 0.00018983733 0.00017433973 0.00015564376 0.00014055881 0.0001280062 0.00012838314 0.00012383879 0.00012001034 0.00013168872 0.00014841813 0.00015493498][0.00013233977 0.00015947003 0.00018918372 0.0002101522 0.00020459939 0.00018875411 0.00017300474 0.00015929244 0.0001496889 0.0001469355 0.00013883067 0.00013231626 0.00013987802 0.00014995187 0.00015301371][0.00012857438 0.0001579135 0.00018945591 0.00021030377 0.00020896253 0.0001983041 0.00018799628 0.00017928644 0.00016933995 0.00016361907 0.00015632424 0.00015139914 0.00015850257 0.00016614984 0.00016645031][0.00012750123 0.00015534827 0.00018508411 0.00020473808 0.00020837865 0.00020379934 0.00019950778 0.00019433215 0.00018369433 0.00017402161 0.00016664276 0.00016305734 0.00016879845 0.00017615051 0.00017911587][0.00012665123 0.00015218249 0.00018044762 0.00020158401 0.00021097279 0.00021289919 0.00021484817 0.00021442023 0.00020313288 0.00018883962 0.00017974564 0.00017433339 0.00017595493 0.00017978293 0.00018432076][0.00012600415 0.00015098837 0.00017806598 0.00020254185 0.00021960097 0.00022800265 0.00023722599 0.00024467605 0.00023606222 0.000217326 0.00020193083 0.00019400242 0.00019064058 0.00018975655 0.00019285789][0.0001300093 0.00015680744 0.00018604817 0.00021277343 0.00023401069 0.00024660912 0.00026113566 0.00027340019 0.00026571215 0.00024317895 0.00022369325 0.00021246879 0.0002048247 0.00020185397 0.00020297198][0.00013162098 0.00016067509 0.00019093078 0.00021744409 0.00023892288 0.00025268807 0.00026835053 0.00028061427 0.00027235795 0.00024936453 0.00023158902 0.00022287293 0.00021697051 0.00021451716 0.00021609908][0.00013422882 0.00016275216 0.00019327948 0.00021977499 0.00023891081 0.00025153617 0.00026412698 0.00027084522 0.00026011531 0.00023976459 0.00022813521 0.00022453653 0.00022177251 0.00021994281 0.00022309025][0.00013721167 0.00016503737 0.00019511554 0.00022109323 0.00024004036 0.00025185838 0.0002605374 0.00026013632 0.00024673156 0.00023082916 0.00022578535 0.00022923095 0.00023056151 0.00022779482 0.00022764725][0.00013818014 0.00016292628 0.00019068898 0.00021581454 0.0002340761 0.000244738 0.00024887748 0.00024477061 0.00023198665 0.00021968639 0.00021758174 0.00022484688 0.00022962701 0.00022529181 0.0002211759][0.00013935479 0.00016087275 0.00018482954 0.00020704696 0.00022192206 0.00022856066 0.00022747731 0.00022013531 0.0002086719 0.00019822283 0.00019714712 0.00020194284 0.00020446561 0.00020088478 0.00019489342][0.00013660362 0.00015392486 0.00017289483 0.00019149587 0.000201301 0.00020183827 0.00019425343 0.00018432226 0.00017524863 0.00016867451 0.00016841269 0.0001721094 0.00017204242 0.00016725855 0.00016019227][0.00012660003 0.00013810617 0.00015107013 0.00016632034 0.00017341824 0.0001706894 0.00016054189 0.00015160238 0.00014438201 0.00013882549 0.00013821504 0.00014007145 0.00013790381 0.00013255978 0.00012584651]]...]
INFO - root - 2017-12-11 03:26:57.122942: step 110, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.555 sec/batch; 51h:17m:20s remains)
INFO - root - 2017-12-11 03:27:02.459256: step 120, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:59m:17s remains)
INFO - root - 2017-12-11 03:27:07.932825: step 130, loss = 0.75, batch loss = 0.69 (14.2 examples/sec; 0.563 sec/batch; 51h:55m:58s remains)
INFO - root - 2017-12-11 03:27:13.347335: step 140, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:39m:44s remains)
INFO - root - 2017-12-11 03:27:18.404009: step 150, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:17m:50s remains)
INFO - root - 2017-12-11 03:27:23.689551: step 160, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.514 sec/batch; 47h:24m:28s remains)
INFO - root - 2017-12-11 03:27:28.961966: step 170, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:55m:46s remains)
INFO - root - 2017-12-11 03:27:34.285209: step 180, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:59m:53s remains)
INFO - root - 2017-12-11 03:27:39.622396: step 190, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:29m:46s remains)
INFO - root - 2017-12-11 03:27:44.920399: step 200, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:34m:18s remains)
2017-12-11 03:27:45.558291: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0001030974 0.00011726622 0.0001411579 0.00016052744 0.00016638685 0.00016399549 0.00015559414 0.00014395312 0.00013670285 0.00014085817 0.00014492613 0.00014632597 0.00014958552 0.00015399714 0.00017249865][0.00010818474 0.00011791602 0.00013447789 0.00014651696 0.00014998979 0.00014821033 0.00014077556 0.00013500688 0.0001331473 0.00014108729 0.00014864109 0.0001530375 0.00016059245 0.00016925306 0.00018880168][0.00012250552 0.00012724681 0.00013191042 0.00013674542 0.00014202239 0.00014395571 0.0001421842 0.00014243882 0.00014134217 0.00014469374 0.00014652553 0.00014809509 0.00015608482 0.00016621102 0.00018402965][0.00013497283 0.00013964843 0.00014005796 0.00014296049 0.00015523324 0.00016379288 0.00016827752 0.00016845096 0.00016001049 0.00015286145 0.00014519814 0.00014070224 0.00014552312 0.00015247548 0.00016566692][0.00014169805 0.00014889777 0.00015150005 0.00015829502 0.00018241645 0.00020575411 0.00021930037 0.0002118183 0.00018633563 0.00016631771 0.00014961691 0.00013898713 0.00013694206 0.00014064595 0.00015177023][0.0001411322 0.00015047043 0.00015908199 0.00017551394 0.00021899462 0.00027463652 0.00031309528 0.00029479249 0.00023963787 0.0001968921 0.00016427902 0.00014546742 0.00013483531 0.00013363326 0.00014465915][0.00013291085 0.0001446735 0.00016197241 0.00019153215 0.00025818424 0.00035279198 0.00042764284 0.00039516063 0.00029541866 0.00022217166 0.00017306391 0.00014473528 0.00012586225 0.00012344636 0.00013824964][0.00012252781 0.00013338697 0.00015802948 0.00019679428 0.00026909597 0.00037201686 0.00045564576 0.00042188115 0.00031220462 0.00022778333 0.00017261427 0.00014078665 0.00011942026 0.0001241345 0.00014625197][0.00011417879 0.00012265134 0.00015085575 0.00018542696 0.00023822901 0.00030470587 0.00035240306 0.00033573853 0.00026525772 0.00020464398 0.00016465777 0.00013711053 0.00012165352 0.00013544003 0.00016550232][0.00010903538 0.00011521522 0.00013984782 0.00016617341 0.00019789142 0.00022731535 0.00024259415 0.0002342033 0.00019812022 0.00016818292 0.0001486563 0.00013204328 0.0001255689 0.00014623886 0.00018060995][0.00010894529 0.00011253975 0.00012547716 0.0001422528 0.00015948783 0.00016743108 0.00016575998 0.00015559405 0.00013710037 0.00012771416 0.00012309982 0.00011973537 0.0001262914 0.00015582745 0.00019619746][0.0001096623 0.00011172542 0.00011451681 0.00012069006 0.00013015018 0.00013131658 0.0001223101 0.00010715032 9.3623021e-05 9.1609931e-05 9.468483e-05 0.00010105385 0.00011649869 0.0001503026 0.00019055072][0.00010856085 0.00011300831 0.00011091702 0.00010642936 0.00010548311 0.00010070129 9.03002e-05 7.7373275e-05 6.7890054e-05 6.8597728e-05 7.3781805e-05 8.607989e-05 0.00010581958 0.00013705298 0.00016935384][0.00010752952 0.00011405969 0.00011296535 0.00010461851 9.3768867e-05 8.3541323e-05 7.3922209e-05 6.5361412e-05 6.0943243e-05 6.4912951e-05 7.1627321e-05 8.2367129e-05 0.00010016378 0.00012690783 0.00015087811][0.00010100037 0.00010787393 0.00010946118 0.00010332169 9.1313785e-05 8.0225866e-05 7.1092589e-05 6.5760541e-05 6.392572e-05 6.7573208e-05 7.2942756e-05 8.150909e-05 9.3717419e-05 0.00011345848 0.00012972479]]...]
INFO - root - 2017-12-11 03:27:50.862870: step 210, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.522 sec/batch; 48h:13m:39s remains)
INFO - root - 2017-12-11 03:27:56.225983: step 220, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.545 sec/batch; 50h:19m:52s remains)
INFO - root - 2017-12-11 03:28:01.591361: step 230, loss = 0.75, batch loss = 0.69 (13.9 examples/sec; 0.578 sec/batch; 53h:18m:37s remains)
INFO - root - 2017-12-11 03:28:06.979250: step 240, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.545 sec/batch; 50h:20m:46s remains)
INFO - root - 2017-12-11 03:28:12.011992: step 250, loss = 0.75, batch loss = 0.69 (15.7 examples/sec; 0.509 sec/batch; 46h:57m:07s remains)
INFO - root - 2017-12-11 03:28:17.321128: step 260, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.528 sec/batch; 48h:40m:59s remains)
INFO - root - 2017-12-11 03:28:22.660795: step 270, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:36m:24s remains)
INFO - root - 2017-12-11 03:28:27.965747: step 280, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 48h:04m:59s remains)
INFO - root - 2017-12-11 03:28:33.326973: step 290, loss = 0.75, batch loss = 0.69 (14.2 examples/sec; 0.562 sec/batch; 51h:50m:11s remains)
INFO - root - 2017-12-11 03:28:38.647684: step 300, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:31m:10s remains)
2017-12-11 03:28:39.271515: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00016306652 0.00018886034 0.00021931277 0.00025185448 0.00028002911 0.00029926549 0.00031140065 0.00031297473 0.00030133111 0.00027923335 0.00025211996 0.00023010345 0.00022454459 0.00022871183 0.00024096126][0.0001710559 0.00019817374 0.00022985142 0.00026332325 0.00029244288 0.00031487533 0.00033103605 0.0003341487 0.000320881 0.00029533045 0.00026378306 0.00023544536 0.00022194073 0.00022091604 0.00022895477][0.00017909537 0.00020183498 0.00022696536 0.00025451259 0.00027904467 0.00029572775 0.00030420866 0.00030246557 0.00028966382 0.00027168906 0.00024865856 0.00022419191 0.00020956162 0.0002096229 0.00021703457][0.00019079873 0.00020906502 0.00022680675 0.00024797718 0.0002656125 0.00027381 0.00027201546 0.0002642333 0.00025266706 0.00024242916 0.00023075323 0.00021666387 0.00021115587 0.00021629933 0.00022403018][0.00021693151 0.00023292769 0.00024544189 0.00025884164 0.00027028259 0.00027270781 0.00026175965 0.00024557923 0.00023012368 0.00022254689 0.00021662041 0.00020996059 0.00021126152 0.00021958043 0.00022669724][0.000265055 0.0002830131 0.00029303241 0.00030303266 0.00031340553 0.00031467417 0.00030099723 0.00028075089 0.00026287019 0.00025428296 0.00024718163 0.00024081174 0.0002402182 0.00024482736 0.0002473541][0.00031412687 0.00033428968 0.00034530438 0.000357632 0.0003726873 0.00037844293 0.00036643984 0.0003453526 0.00032659396 0.00031843927 0.00031203122 0.00030235667 0.00029781085 0.00029648311 0.00029194885][0.00034897108 0.0003759136 0.00039223494 0.00041268559 0.00044098543 0.00045509663 0.00044680285 0.00042783917 0.00040824019 0.00039771033 0.00038947808 0.00037591089 0.00036352212 0.00035412732 0.00034054808][0.00034003271 0.00036782789 0.00038553934 0.00040883073 0.00044296859 0.00046575238 0.0004699048 0.00046215972 0.00045042348 0.00044367867 0.00043708441 0.00042486473 0.00041305236 0.00039966687 0.00037865929][0.00031277342 0.00033126088 0.00033799055 0.0003490503 0.00036990817 0.00038669503 0.0003955517 0.00039778397 0.00039609976 0.00039729747 0.00039840559 0.00039316074 0.00038781198 0.00037963723 0.00036602686][0.00028285375 0.00028848916 0.00028282317 0.00028087466 0.00028903803 0.0002986324 0.00030843174 0.00031652453 0.00032113705 0.00032663913 0.0003311444 0.00032854278 0.00032432412 0.00031871386 0.00031318961][0.00023327934 0.00023161639 0.0002211055 0.00021290826 0.00021265296 0.00021752957 0.00022615053 0.00023558992 0.00024237548 0.00024717033 0.00025136364 0.00025026084 0.00024865373 0.00024710194 0.00024935819][0.00018304605 0.000179893 0.00017115771 0.00016489721 0.00016343039 0.00016693377 0.00017381439 0.00018085947 0.00018316295 0.00018378979 0.000183982 0.00018154424 0.00018151454 0.00018430375 0.00019293102][0.00015281985 0.00015119642 0.00014495653 0.00014073971 0.00013850963 0.00013935157 0.00014180464 0.00014304624 0.00014066609 0.00013800102 0.00013660533 0.00013525593 0.00013761323 0.00014434363 0.00015742975][0.00014798205 0.000147764 0.00014302814 0.00013953542 0.00013648529 0.00013408343 0.00013248992 0.00013018046 0.00012617816 0.00012193335 0.00011904657 0.00011861385 0.00012171564 0.00012954551 0.00014203771]]...]
INFO - root - 2017-12-11 03:28:44.587251: step 310, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.540 sec/batch; 49h:47m:13s remains)
INFO - root - 2017-12-11 03:28:49.899103: step 320, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.523 sec/batch; 48h:16m:01s remains)
INFO - root - 2017-12-11 03:28:55.172153: step 330, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:54m:21s remains)
INFO - root - 2017-12-11 03:29:00.509788: step 340, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:26m:42s remains)
INFO - root - 2017-12-11 03:29:05.493979: step 350, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:54m:04s remains)
INFO - root - 2017-12-11 03:29:10.765851: step 360, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.521 sec/batch; 48h:05m:41s remains)
INFO - root - 2017-12-11 03:29:16.057005: step 370, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:55m:52s remains)
INFO - root - 2017-12-11 03:29:21.447090: step 380, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.513 sec/batch; 47h:20m:37s remains)
INFO - root - 2017-12-11 03:29:26.796363: step 390, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:43m:36s remains)
INFO - root - 2017-12-11 03:29:32.110189: step 400, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:34m:00s remains)
2017-12-11 03:29:32.716394: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0002434643 0.00023738136 0.00023016677 0.00022924576 0.0002446867 0.00025595588 0.00024869555 0.00023787163 0.00021886558 0.00019135205 0.00016510356 0.000152046 0.00015115587 0.00015554536 0.0001632772][0.00026389459 0.00024388389 0.0002300545 0.00023168998 0.00025118524 0.00026856977 0.00026721437 0.00025522828 0.00022924769 0.0001961213 0.00016644438 0.00015007981 0.00014818565 0.00015293024 0.00016277457][0.00027231945 0.00024438137 0.00022803 0.00023221127 0.00025740531 0.00028265049 0.00029212571 0.00028163774 0.00024554282 0.00020423323 0.00017085524 0.00015133516 0.00015033087 0.00015466263 0.00016618067][0.0002441283 0.00022125116 0.00021674545 0.00023536352 0.00027182148 0.00030754131 0.00033462074 0.00032533231 0.00027628531 0.00022744478 0.00019234073 0.00016885684 0.00016536182 0.00016826521 0.00017954721][0.00019193039 0.00018940534 0.00021288161 0.00025593996 0.00030334218 0.00035352094 0.00040456461 0.00039153112 0.00032119997 0.00026275069 0.00022908421 0.00020503887 0.0001991379 0.00020283193 0.00021504177][0.00015721515 0.00017901546 0.00023011885 0.00029438763 0.00035411821 0.00042358512 0.00050122407 0.00047758175 0.00037695718 0.00030377312 0.00026749185 0.00024228905 0.00023294998 0.00023904024 0.00025675783][0.0001512918 0.00018674492 0.00025121492 0.0003248486 0.00039235209 0.00047654941 0.00057238177 0.00054212322 0.00042554428 0.00034289513 0.00030117907 0.00027472366 0.00026502865 0.0002751686 0.00029783754][0.00015259351 0.00018797119 0.00024810369 0.0003134842 0.00037056793 0.00044239813 0.00052211771 0.000501377 0.00041140689 0.0003442663 0.00030771625 0.00028122871 0.00027138085 0.00028502848 0.00031293585][0.00013878981 0.00016641043 0.00021685778 0.00026752637 0.00030445721 0.00034698716 0.00039296152 0.00037990982 0.00033113034 0.00029188007 0.00026467975 0.00023842744 0.00022963059 0.00024662589 0.0002782013][0.00011212649 0.00012977743 0.00016755918 0.00020411823 0.00022694893 0.00024863728 0.00026894224 0.00025893253 0.00023756192 0.00021970559 0.00019923849 0.0001745565 0.00016528266 0.00018091648 0.00021062559][8.3619248e-05 9.3810253e-05 0.00011880138 0.00014378854 0.00015911848 0.00016934508 0.00017549118 0.00016929535 0.0001617511 0.00015317586 0.00013793311 0.00011697226 0.00010494021 0.00011298225 0.00013291834][6.8294794e-05 7.1954564e-05 8.6136592e-05 0.0001027792 0.00011595761 0.00012265825 0.00012476271 0.00012311219 0.00011970445 0.00011304642 0.00010145702 8.4972293e-05 7.0995695e-05 6.8994188e-05 7.8642945e-05][7.5275362e-05 7.4086296e-05 7.9245867e-05 9.0513306e-05 0.00010586191 0.00011362798 0.00011773351 0.00012044986 0.00012124992 0.00011837237 0.00010998582 9.5174626e-05 7.8923724e-05 7.0599024e-05 7.36062e-05][0.00010427655 9.8780947e-05 9.5500727e-05 0.00010171587 0.00011691255 0.00012775902 0.00013569185 0.00014230999 0.00014893923 0.00015519984 0.00015235035 0.00013704682 0.00012061884 0.00011252298 0.00011503433][0.00014176083 0.00013220441 0.00012262123 0.00012192642 0.00013145943 0.00014108396 0.00015008054 0.00015783194 0.00016915445 0.00018414094 0.00018804528 0.00017650629 0.00016591228 0.00016570096 0.00017280022]]...]
INFO - root - 2017-12-11 03:29:38.101393: step 410, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.525 sec/batch; 48h:23m:27s remains)
INFO - root - 2017-12-11 03:29:43.419940: step 420, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.522 sec/batch; 48h:09m:07s remains)
INFO - root - 2017-12-11 03:29:48.729360: step 430, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:34m:47s remains)
INFO - root - 2017-12-11 03:29:54.048441: step 440, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 48h:03m:37s remains)
INFO - root - 2017-12-11 03:29:59.311490: step 450, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.546 sec/batch; 50h:19m:58s remains)
INFO - root - 2017-12-11 03:30:04.343212: step 460, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:20m:34s remains)
INFO - root - 2017-12-11 03:30:09.686181: step 470, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:33m:35s remains)
INFO - root - 2017-12-11 03:30:14.915183: step 480, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.517 sec/batch; 47h:43m:11s remains)
INFO - root - 2017-12-11 03:30:20.197864: step 490, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:15m:12s remains)
INFO - root - 2017-12-11 03:30:25.478521: step 500, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 49h:11m:49s remains)
2017-12-11 03:30:26.053760: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00022199047 0.00023458312 0.00024229122 0.000245782 0.00024541747 0.00023934417 0.00023535205 0.00023619995 0.00024235851 0.000255485 0.00028270495 0.00031941343 0.00035328884 0.00037598779 0.00039180089][0.00026053051 0.00027732438 0.0002898775 0.00029587076 0.0002946686 0.00028545191 0.00027567841 0.00027101458 0.00027720025 0.00029207737 0.00032206473 0.000360429 0.00039479107 0.00041899431 0.0004309791][0.00028379686 0.0003074811 0.00032689009 0.00033651682 0.00033586379 0.00032441082 0.00030893495 0.00029907029 0.00030217686 0.00031735373 0.0003473502 0.00038406081 0.00041516442 0.00043277856 0.00043370429][0.00030593146 0.0003362635 0.00036218113 0.00037719309 0.00038038733 0.00037082261 0.00035475628 0.00033853142 0.00033584912 0.0003516679 0.00038097717 0.00040824822 0.00042293686 0.00042336941 0.00040626718][0.00033570844 0.00036784739 0.00039574498 0.00041603163 0.00042553904 0.00042434983 0.00041855287 0.0004045539 0.00039499556 0.00040388311 0.0004226522 0.00042869194 0.00042000171 0.00040427921 0.00037754406][0.0003550922 0.00038441626 0.00040909494 0.00043273444 0.00045557672 0.000477746 0.00049534015 0.00048686756 0.00046318377 0.00045386574 0.00045343555 0.00043843474 0.00041089655 0.00038427717 0.00035709125][0.00037001621 0.00039301804 0.0004108646 0.00043272474 0.00046396139 0.00050943578 0.00055572874 0.00055435434 0.00051156682 0.00048198606 0.00046121434 0.00043177852 0.00039741918 0.000371467 0.00034782931][0.00039063589 0.0004103808 0.0004203904 0.00043044231 0.00045381679 0.00049729744 0.00054597645 0.0005488522 0.000504939 0.00047340157 0.00044607613 0.00041385522 0.00038310958 0.00036170651 0.00033975992][0.00040427156 0.00042137422 0.00041844908 0.00040871854 0.00041412839 0.0004406707 0.0004735761 0.00047624536 0.00045200787 0.00044007559 0.00042422063 0.00039512193 0.0003667507 0.00034577225 0.00032449685][0.0003965958 0.00040136214 0.00038181181 0.00035711049 0.00034918109 0.00036299744 0.00038587561 0.00039413868 0.00039246259 0.0004026664 0.00040155544 0.00037798664 0.00035074211 0.00033225887 0.0003167752][0.00036394459 0.00035659276 0.00033181437 0.00030794891 0.00029915836 0.00030647477 0.00032454732 0.00033813895 0.00034901837 0.0003735116 0.00038437048 0.00036979248 0.00034766988 0.00033235189 0.00031895345][0.00031964417 0.00031513764 0.00029731757 0.00028227246 0.00027908379 0.00028443406 0.00030093334 0.00031713303 0.000332573 0.00035889086 0.00037118865 0.0003594702 0.00034056467 0.00032247364 0.000305933][0.00028240896 0.00028443808 0.00027505917 0.00026928325 0.00027060616 0.00027648159 0.00029022043 0.00030354669 0.00031404564 0.00032995467 0.00033399762 0.00032323421 0.00030826946 0.00029038105 0.00027323264][0.00025356497 0.00025698406 0.00025272541 0.00025237832 0.00025515907 0.00026165263 0.00027370072 0.00028071512 0.00028111722 0.00028574 0.00028437265 0.00027794213 0.00027197073 0.00026028289 0.00024943295][0.00023176263 0.00023392923 0.00022994549 0.00023083152 0.0002336516 0.00024020203 0.00025177366 0.00025392932 0.00024749295 0.00024442788 0.00024230096 0.00024304468 0.00024436184 0.00024070041 0.00023795254]]...]
INFO - root - 2017-12-11 03:30:31.537031: step 510, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.555 sec/batch; 51h:09m:01s remains)
INFO - root - 2017-12-11 03:30:36.785513: step 520, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.516 sec/batch; 47h:34m:00s remains)
INFO - root - 2017-12-11 03:30:42.097607: step 530, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:21m:01s remains)
INFO - root - 2017-12-11 03:30:47.526139: step 540, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:29m:03s remains)
INFO - root - 2017-12-11 03:30:52.843584: step 550, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:23m:39s remains)
INFO - root - 2017-12-11 03:30:57.787358: step 560, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:32m:09s remains)
INFO - root - 2017-12-11 03:31:03.137540: step 570, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.551 sec/batch; 50h:47m:45s remains)
INFO - root - 2017-12-11 03:31:08.495890: step 580, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:51m:56s remains)
INFO - root - 2017-12-11 03:31:13.843563: step 590, loss = 0.75, batch loss = 0.69 (15.8 examples/sec; 0.506 sec/batch; 46h:37m:04s remains)
INFO - root - 2017-12-11 03:31:19.172687: step 600, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:17m:41s remains)
2017-12-11 03:31:19.743824: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00043477779 0.00046225439 0.00043815296 0.00039164181 0.00035686663 0.00035070223 0.00037406076 0.00040328474 0.00040378261 0.00037397907 0.00032365258 0.00027174523 0.00022229884 0.00017819356 0.00014450944][0.0004431612 0.00046651694 0.00043594494 0.00038346442 0.0003412847 0.00033169138 0.00035215885 0.00038269206 0.00038894833 0.00036341729 0.0003138383 0.00026486587 0.00022026028 0.00018245105 0.00015570274][0.00038505695 0.000395484 0.00036811686 0.0003268368 0.00029379374 0.00028387844 0.00029716641 0.0003189639 0.00032464456 0.000307835 0.00027489121 0.00024489721 0.00021721727 0.00019229219 0.00017549524][0.00031130208 0.00031396863 0.00029726417 0.00027771163 0.00026306845 0.00025695743 0.00026214088 0.00026922626 0.00026930423 0.00025960256 0.00024664533 0.00023733461 0.0002242624 0.00020882244 0.00019857264][0.00026690413 0.00027936584 0.00027941127 0.0002792693 0.00027628528 0.00027304678 0.00026816846 0.00026051109 0.00025267561 0.00024176712 0.00023607482 0.00023536346 0.00022957126 0.00021885235 0.00020921136][0.00027772589 0.00031291094 0.00033187348 0.0003410957 0.000340464 0.00033568533 0.00032270048 0.00030059053 0.00027714224 0.00025240745 0.00023732148 0.00023010696 0.00022206966 0.00021129072 0.00019987553][0.00031558925 0.00037516135 0.00041256967 0.00042676949 0.00042108609 0.00040801312 0.00038415749 0.00034908505 0.00030908774 0.00027034976 0.00024049469 0.00022194002 0.00020684526 0.00019355158 0.00018173347][0.00031822812 0.0003858096 0.00042967603 0.00044345221 0.00043743182 0.00042389924 0.00039788158 0.00035820628 0.00031359756 0.00027081734 0.00023484585 0.00020744718 0.00018536113 0.0001709826 0.00016131406][0.00027631424 0.00032974663 0.00036438656 0.00037682257 0.000374226 0.00036666225 0.00034892035 0.00031555255 0.00027950204 0.00024413851 0.00021209753 0.000185456 0.00016290318 0.00014728469 0.00013730083][0.00022776351 0.00026063193 0.00027973155 0.00028547444 0.00028110077 0.00027446498 0.00026349546 0.00024166275 0.00021998954 0.00019822898 0.00017700145 0.0001566594 0.00013771794 0.00012410418 0.0001151058][0.0002065156 0.00022575862 0.00023283059 0.00022790916 0.00021335272 0.00019874924 0.00018723498 0.0001744225 0.00016513634 0.00015483241 0.00014325655 0.00013069468 0.00011826515 0.00010908768 0.0001027862][0.00020114252 0.00021321399 0.0002133174 0.00020075603 0.00017996914 0.00016153199 0.00014803706 0.00013908243 0.00013443753 0.00013043878 0.0001262577 0.0001186497 0.00010906698 0.00010191036 9.8043005e-05][0.00018581269 0.00019654907 0.00019588752 0.00018428604 0.00016633181 0.00015168171 0.00014215893 0.00013658416 0.0001343461 0.00013278566 0.00012775508 0.00011834464 0.00010781122 0.00010010863 9.4819043e-05][0.00016251858 0.00017442934 0.00017610422 0.0001696004 0.00016056007 0.00015454576 0.00015067041 0.00014815414 0.00014627876 0.0001435801 0.00013420705 0.00011958808 0.0001067648 9.71663e-05 9.0791989e-05][0.00013786998 0.00014799855 0.00014866531 0.00014526318 0.0001413366 0.00014112116 0.00014128063 0.00014057249 0.00013969334 0.0001374578 0.00012734372 0.00011202657 9.906217e-05 9.0486006e-05 8.465525e-05]]...]
INFO - root - 2017-12-11 03:31:25.104421: step 610, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:26m:41s remains)
INFO - root - 2017-12-11 03:31:30.439990: step 620, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 50h:01m:31s remains)
INFO - root - 2017-12-11 03:31:35.807833: step 630, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:54m:36s remains)
INFO - root - 2017-12-11 03:31:41.157092: step 640, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:55m:56s remains)
INFO - root - 2017-12-11 03:31:46.526844: step 650, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.528 sec/batch; 48h:38m:32s remains)
INFO - root - 2017-12-11 03:31:51.491741: step 660, loss = 0.75, batch loss = 0.69 (28.8 examples/sec; 0.278 sec/batch; 25h:35m:24s remains)
INFO - root - 2017-12-11 03:31:56.779945: step 670, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 49h:00m:23s remains)
INFO - root - 2017-12-11 03:32:02.095376: step 680, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:54m:50s remains)
INFO - root - 2017-12-11 03:32:07.423669: step 690, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:22m:18s remains)
INFO - root - 2017-12-11 03:32:12.732897: step 700, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.521 sec/batch; 48h:03m:37s remains)
2017-12-11 03:32:13.365189: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0001919526 0.00019123466 0.00018987022 0.00018980086 0.00019260947 0.00019964435 0.00020920829 0.00021776585 0.00022327747 0.00022549929 0.000226967 0.00023104955 0.00023935837 0.0002542432 0.00026925615][0.0001730022 0.00017214278 0.00017194543 0.00017436821 0.00018117209 0.00019432 0.00021082824 0.00022631613 0.00023703458 0.00024211277 0.00024390408 0.00024556371 0.00024865507 0.00025595064 0.000264707][0.00015781169 0.00015726597 0.00015688708 0.00015981837 0.00016876297 0.00018541554 0.00020573041 0.00022458725 0.00023684825 0.00024214173 0.00024314078 0.0002420813 0.00023939776 0.00023952419 0.0002436466][0.00016962264 0.00017275465 0.00017309292 0.00017552974 0.00018382857 0.00020039229 0.00022018702 0.0002366047 0.00024408358 0.00024395723 0.00024104374 0.00023656836 0.00023060547 0.00022708702 0.00022885103][0.00020254958 0.00021411774 0.00021831867 0.00022137249 0.00022919575 0.00024349435 0.00025875683 0.00026767291 0.000265635 0.00025565413 0.000244191 0.00023318148 0.00022415671 0.00021983022 0.0002222613][0.00023749235 0.00025757149 0.00026559425 0.00027031172 0.00028003979 0.00029671087 0.00031221553 0.00031582595 0.00030423523 0.00028232593 0.00025829597 0.00023621286 0.0002208315 0.000214324 0.00021706845][0.00025502164 0.00027890544 0.0002897435 0.000297284 0.00031142894 0.00033372833 0.00035288226 0.0003547658 0.00033746261 0.00030715376 0.00027192969 0.00023847862 0.00021570305 0.00020674549 0.00020957543][0.00025183431 0.00027242192 0.00028222249 0.000291141 0.00030808421 0.00033408339 0.00035649052 0.00036021727 0.00034337048 0.00031262258 0.00027413547 0.00023544443 0.00020856438 0.00019907118 0.00020242755][0.00024136777 0.00025502295 0.00026141302 0.00026926943 0.00028411404 0.00030571036 0.00032390584 0.00032707825 0.00031460109 0.00029066819 0.00025769311 0.00022406303 0.00020128035 0.00019549677 0.00020066564][0.00023079128 0.00024066553 0.0002468171 0.00025560742 0.0002690141 0.00028526192 0.00029608121 0.00029395617 0.00028024128 0.00025944141 0.00023199801 0.0002059879 0.0001913165 0.00019289086 0.00020263212][0.00022301011 0.00023508971 0.00024498036 0.00025797536 0.00027375229 0.00028800804 0.00029316504 0.00028538794 0.0002662327 0.0002424854 0.00021613025 0.00019448505 0.00018642211 0.00019462568 0.00020988335][0.00022494838 0.00024130488 0.00025427932 0.0002696416 0.00028690256 0.00030003936 0.00030197637 0.00029173435 0.0002703674 0.00024469709 0.00021857345 0.00019919522 0.00019576434 0.00020849299 0.00022543133][0.00023638735 0.00025280993 0.00026411712 0.00027665604 0.00029010972 0.00029836214 0.00029671387 0.00028719159 0.00026931625 0.00024750785 0.00022580005 0.00021129988 0.00021271947 0.0002277397 0.00024267586][0.00024031897 0.00025464996 0.00026190639 0.00026855196 0.0002755133 0.00027823795 0.00027461207 0.00026782101 0.00025657751 0.00024199863 0.00022743407 0.00021914611 0.00022422489 0.00023860211 0.00024915315][0.00022518293 0.00023837308 0.00024337889 0.00024708081 0.00025014215 0.00025003526 0.00024601049 0.0002412975 0.00023515373 0.00022682743 0.00021821163 0.00021392867 0.00021999673 0.00023118463 0.00023612456]]...]
INFO - root - 2017-12-11 03:32:18.726512: step 710, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:25m:22s remains)
INFO - root - 2017-12-11 03:32:24.116607: step 720, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:42m:31s remains)
INFO - root - 2017-12-11 03:32:29.329546: step 730, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.517 sec/batch; 47h:36m:24s remains)
INFO - root - 2017-12-11 03:32:34.631895: step 740, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:16m:53s remains)
INFO - root - 2017-12-11 03:32:39.900997: step 750, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:51m:43s remains)
INFO - root - 2017-12-11 03:32:45.123123: step 760, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:51m:41s remains)
INFO - root - 2017-12-11 03:32:50.123571: step 770, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:52m:32s remains)
INFO - root - 2017-12-11 03:32:55.387375: step 780, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:23m:17s remains)
INFO - root - 2017-12-11 03:33:00.636178: step 790, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:30m:56s remains)
INFO - root - 2017-12-11 03:33:05.989316: step 800, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.516 sec/batch; 47h:35m:02s remains)
2017-12-11 03:33:06.562859: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00016415902 0.0001663197 0.00017029549 0.00018613835 0.00021815703 0.0002614983 0.00030123992 0.00032970108 0.00034632586 0.000354087 0.00036284342 0.00037576433 0.00040035218 0.00043655542 0.00047374368][0.00016392858 0.00016859853 0.0001765949 0.00019795694 0.00023389749 0.00027900274 0.00032053865 0.00035145297 0.00037162323 0.00038339032 0.00039496162 0.00041182645 0.000441379 0.00047932751 0.000515366][0.00017900381 0.00018735205 0.00020074523 0.0002262682 0.00026347651 0.00030790825 0.00035095168 0.00038301983 0.00040187364 0.00041266193 0.00042073883 0.00043284768 0.00045796562 0.00049039506 0.00051876507][0.00022048061 0.00023747439 0.00025973757 0.00029161241 0.00033037429 0.00037259198 0.00041104548 0.00043790272 0.00044995954 0.00045164998 0.00045105329 0.00045443862 0.00046896341 0.00048942451 0.00050573767][0.00028371561 0.00030919851 0.00033889164 0.00037555504 0.00041397169 0.00045214512 0.00048503248 0.00050626742 0.00050946232 0.00049795269 0.00048374155 0.00047567632 0.00047504768 0.00047822675 0.00047845716][0.00034888223 0.00038278571 0.00042024872 0.00046297922 0.00050178985 0.00053711887 0.0005664989 0.00058284105 0.00057310658 0.00054423779 0.00051333546 0.00048992061 0.00047469142 0.00046439207 0.00045467785][0.00038818223 0.00042401787 0.00046670189 0.0005166636 0.000561748 0.00060428091 0.0006430698 0.0006639419 0.00064402522 0.0005973854 0.00054969295 0.00051262049 0.00048761009 0.00047278829 0.00046228143][0.00038669334 0.00041772579 0.00045924625 0.00051236758 0.00056361069 0.00061528577 0.00066516525 0.00069181935 0.00067001686 0.00061656919 0.00056208135 0.00052197376 0.00049852562 0.00049053039 0.000490762][0.00034665142 0.00037036039 0.000405041 0.00045074191 0.00049510086 0.00053968094 0.00058437826 0.00061048637 0.00059767428 0.00055430533 0.000509038 0.00047934116 0.00046811666 0.00047456415 0.0004917417][0.00028419521 0.0002998533 0.00032576922 0.00036070988 0.00039493226 0.00043029568 0.00046790007 0.00049226527 0.00048730144 0.00045464907 0.0004195208 0.00040105206 0.00040099226 0.00041879283 0.00044990145][0.00020826064 0.00021832372 0.00023457549 0.0002595454 0.00028652448 0.00031575569 0.00034780454 0.00037030471 0.00037030291 0.00034728271 0.00032314184 0.00031393406 0.00032247248 0.00034681216 0.00038334806][0.00014445082 0.00014943178 0.00015872136 0.00017539134 0.00019587632 0.00021897464 0.00024420879 0.00026216573 0.00026375873 0.00024865667 0.00023379854 0.00023142487 0.00024495969 0.00027155675 0.00030775295][0.00010530049 0.00010828914 0.0001139534 0.00012366308 0.00013608263 0.0001501946 0.00016448656 0.00017333402 0.00017390848 0.00016742319 0.0001617602 0.00016470558 0.00017930602 0.00020205027 0.00023300467][8.8106281e-05 9.1295471e-05 9.5371004e-05 0.00010058609 0.0001071619 0.00011422297 0.00011957854 0.00012136813 0.00012083668 0.00011912419 0.00011880047 0.00012330941 0.00013540035 0.00015341205 0.00017667159][8.5549938e-05 8.9501838e-05 9.2389928e-05 9.5505631e-05 9.891521e-05 0.0001014526 0.00010168402 0.00010044428 9.9874145e-05 0.00010061947 0.00010396116 0.00011059696 0.00012130416 0.00013512066 0.00015311297]]...]
INFO - root - 2017-12-11 03:33:11.860221: step 810, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.529 sec/batch; 48h:43m:40s remains)
INFO - root - 2017-12-11 03:33:17.099663: step 820, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.517 sec/batch; 47h:39m:48s remains)
INFO - root - 2017-12-11 03:33:22.399478: step 830, loss = 0.75, batch loss = 0.69 (15.8 examples/sec; 0.506 sec/batch; 46h:38m:36s remains)
INFO - root - 2017-12-11 03:33:27.751061: step 840, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.550 sec/batch; 50h:42m:04s remains)
INFO - root - 2017-12-11 03:33:33.070409: step 850, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:22m:35s remains)
INFO - root - 2017-12-11 03:33:38.342316: step 860, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 47h:57m:56s remains)
INFO - root - 2017-12-11 03:33:43.327832: step 870, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.517 sec/batch; 47h:37m:02s remains)
INFO - root - 2017-12-11 03:33:48.599961: step 880, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.528 sec/batch; 48h:36m:40s remains)
INFO - root - 2017-12-11 03:33:53.923556: step 890, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:58m:52s remains)
INFO - root - 2017-12-11 03:33:59.246026: step 900, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:22m:10s remains)
2017-12-11 03:33:59.776938: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00057525391 0.00063900993 0.00067719974 0.00069662475 0.0006940235 0.000667842 0.00062226859 0.00058666151 0.0005680889 0.0005691601 0.00057798746 0.00058034604 0.00059876952 0.00062485092 0.000634047][0.00066157908 0.00070129277 0.00072281592 0.000737029 0.00072623079 0.00068500178 0.00062237366 0.00057354418 0.00055194966 0.00055602333 0.00057319645 0.00058894459 0.00061603286 0.00064724224 0.00065900205][0.00073293177 0.00072264817 0.00070347585 0.00069801882 0.00068439048 0.00064776343 0.00059088517 0.00054546323 0.00052950374 0.00053646835 0.00055381469 0.00057579408 0.00060646195 0.00063531945 0.00064115919][0.00076924707 0.00071070163 0.00064892217 0.00062505307 0.00061754446 0.00060615531 0.00057906739 0.00055372767 0.0005467239 0.00055564835 0.00056919164 0.00058470311 0.00060649932 0.00062559097 0.00062085438][0.00070786942 0.00063849357 0.00056488771 0.00053997536 0.00055548991 0.00058053259 0.00059592 0.00059825997 0.000599435 0.00060605886 0.00061419385 0.00061928667 0.00062695215 0.00063829531 0.00062389945][0.00058453431 0.00054174673 0.00049775053 0.00049518078 0.0005392998 0.00060372881 0.00066525553 0.00069822016 0.0006965709 0.00068495621 0.00067678635 0.00067117397 0.00066649052 0.00066869246 0.00064804859][0.00047885819 0.00047488781 0.00047630502 0.00050886325 0.0005837286 0.00068244344 0.000785463 0.00083678955 0.000809634 0.00076391757 0.00072856457 0.00070164946 0.00067811989 0.00067017192 0.00064872211][0.00040379033 0.00043556339 0.00048061935 0.00054773944 0.00064968551 0.00077145133 0.00090063509 0.000957534 0.00089326658 0.0008080944 0.000740649 0.00069320394 0.00065553206 0.00065118133 0.00063739554][0.00034515176 0.00040173944 0.00047844541 0.00057020236 0.00068101089 0.00079264469 0.0009012224 0.00093380694 0.00084854692 0.00074840669 0.00067100377 0.00061914319 0.00058764778 0.000595583 0.00059742597][0.00030160043 0.00036332107 0.00044933349 0.00054648606 0.0006479185 0.000727152 0.0007912846 0.00079445168 0.0007122145 0.00062409765 0.00055896147 0.000518616 0.000497822 0.00051501731 0.00053370063][0.00028867988 0.00033645923 0.00040564485 0.00048531962 0.00056297367 0.00060941593 0.00063470582 0.00061996619 0.00055436569 0.00049005746 0.00044745061 0.00042825623 0.00042318806 0.00045093818 0.00048501173][0.00028993876 0.00031395527 0.00035313232 0.00040272245 0.0004505305 0.00047373632 0.00047829151 0.00046123512 0.00041513427 0.00036968346 0.00034574434 0.00034720055 0.00036418764 0.00041013566 0.00046169289][0.00029862666 0.00030752167 0.00032079642 0.00034018996 0.00036116672 0.000369472 0.00036369622 0.00034504209 0.00031158247 0.00028273315 0.00027406705 0.00028890843 0.00032349565 0.00038252582 0.00044910915][0.00031304767 0.00031792952 0.00031650529 0.0003163298 0.00031880583 0.00031682331 0.00030533702 0.00028567322 0.00025885183 0.00023786227 0.00023576412 0.00025453788 0.00029566206 0.0003593491 0.00043287381][0.00033962197 0.00034178488 0.0003323307 0.0003217913 0.0003096663 0.0002980025 0.00028426564 0.00026776985 0.00025013179 0.00023660024 0.00023625138 0.00025126478 0.00028963372 0.00034970604 0.0004195247]]...]
INFO - root - 2017-12-11 03:34:05.102875: step 910, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.550 sec/batch; 50h:39m:30s remains)
INFO - root - 2017-12-11 03:34:10.401710: step 920, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.516 sec/batch; 47h:33m:26s remains)
INFO - root - 2017-12-11 03:34:15.701142: step 930, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.518 sec/batch; 47h:40m:20s remains)
INFO - root - 2017-12-11 03:34:21.035085: step 940, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:31m:40s remains)
INFO - root - 2017-12-11 03:34:26.395466: step 950, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:24m:18s remains)
INFO - root - 2017-12-11 03:34:31.627833: step 960, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.518 sec/batch; 47h:40m:44s remains)
INFO - root - 2017-12-11 03:34:36.875986: step 970, loss = 0.75, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:30m:27s remains)
INFO - root - 2017-12-11 03:34:41.991450: step 980, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:48m:54s remains)
INFO - root - 2017-12-11 03:34:47.322325: step 990, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.546 sec/batch; 50h:17m:59s remains)
INFO - root - 2017-12-11 03:34:52.637506: step 1000, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:30m:46s remains)
2017-12-11 03:34:53.149065: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00064697315 0.00063781918 0.00060054095 0.000559107 0.00053818105 0.00053247908 0.00053578295 0.00053397409 0.00052651227 0.00050425279 0.00046355659 0.00041133011 0.00035357513 0.00029290107 0.00023361741][0.00071984966 0.00070840254 0.0006663237 0.0006216492 0.00060053135 0.00059818052 0.000604615 0.00060011662 0.00058163825 0.00054448092 0.00048786702 0.00042087806 0.0003534049 0.0002890444 0.00022917129][0.00071435992 0.00070340937 0.00066990627 0.00063974585 0.00063415465 0.00064567424 0.00066067913 0.000652481 0.00061959092 0.00056642632 0.00049267733 0.00041416768 0.00033868846 0.00027052857 0.00021248151][0.000671722 0.00066284469 0.00064455229 0.00063614169 0.00065412966 0.00068875944 0.00071686861 0.00070927793 0.00066516036 0.0005965649 0.00050814665 0.00041770079 0.00033289596 0.00025743034 0.0001975553][0.00060079055 0.00059941894 0.00060155086 0.00062028051 0.00066486862 0.00072390668 0.00076502014 0.000756536 0.00070036977 0.00061814405 0.00051886262 0.00042072209 0.0003288234 0.00024784417 0.00018511988][0.00050496508 0.00052155962 0.00055017881 0.00060095557 0.00067429367 0.00075545092 0.00080753659 0.00079379336 0.00072029245 0.00062234071 0.00051508419 0.00041354829 0.0003188001 0.00023568216 0.00017248371][0.00040872642 0.00044466145 0.00049818953 0.00057613436 0.00067217229 0.00076779415 0.00082821696 0.0008079441 0.00071723416 0.00060584879 0.000493853 0.00039329767 0.00030150588 0.00022035542 0.00015966991][0.00037407078 0.00041973364 0.00048153187 0.00056524988 0.0006631717 0.00075807597 0.0008180273 0.00079123682 0.00069111586 0.00057480542 0.00046511943 0.0003712578 0.00028659622 0.00020995394 0.00015227798][0.00037973234 0.00042650665 0.00048019076 0.00054773368 0.00062364829 0.000696777 0.00074303249 0.00071593729 0.00062751537 0.00052555383 0.00043063381 0.00034974754 0.00027481036 0.00020420919 0.00014912566][0.000394597 0.00043569907 0.000471434 0.00051076815 0.00055275147 0.00059521559 0.00062462117 0.00060627604 0.00054608111 0.00047251466 0.00039915892 0.00033226641 0.00026556457 0.00020023897 0.0001475936][0.00040445066 0.00043578443 0.00045263095 0.00046506096 0.00047703178 0.00049483724 0.00051298563 0.00050523441 0.0004706902 0.00042252973 0.0003680611 0.00031202586 0.00025261022 0.00019258044 0.00014330355][0.0003969176 0.00041697969 0.0004175652 0.00041033974 0.00040243371 0.00040446193 0.0004154972 0.00041498669 0.00039705736 0.00036603658 0.000325502 0.00027873565 0.00022781284 0.00017547337 0.00013318764][0.00035912523 0.00036965279 0.00036060673 0.00034330069 0.00032649949 0.00032092718 0.00032838684 0.00033074207 0.00032042232 0.00029857268 0.00026770349 0.00023048226 0.00019046213 0.00014968727 0.00011829232][0.00029017861 0.00029595732 0.00028468337 0.00026673777 0.00025007606 0.00024357805 0.00024929387 0.00025137307 0.00024423111 0.00022797816 0.00020460616 0.00017744412 0.00014984939 0.00012195694 0.00010262381][0.00021638992 0.00022059468 0.00021161769 0.00019847529 0.00018714421 0.00018297177 0.00018627013 0.00018518494 0.00017791563 0.00016529445 0.00014861312 0.00013144461 0.00011582034 9.9700694e-05 9.0497349e-05]]...]
INFO - root - 2017-12-11 03:34:58.450903: step 1010, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:46m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:35:03.731967: step 1020, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:29m:44s remains)
INFO - root - 2017-12-11 03:35:09.144873: step 1030, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:16m:21s remains)
INFO - root - 2017-12-11 03:35:14.445597: step 1040, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:24m:28s remains)
INFO - root - 2017-12-11 03:35:19.746521: step 1050, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:54m:53s remains)
INFO - root - 2017-12-11 03:35:25.074565: step 1060, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:53m:25s remains)
INFO - root - 2017-12-11 03:35:30.356833: step 1070, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 47h:58m:19s remains)
INFO - root - 2017-12-11 03:35:35.315643: step 1080, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:28m:52s remains)
INFO - root - 2017-12-11 03:35:40.639042: step 1090, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:11m:21s remains)
INFO - root - 2017-12-11 03:35:46.002949: step 1100, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 50h:19m:50s remains)
2017-12-11 03:35:46.522431: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00069824076 0.00082697178 0.0008867585 0.00088749011 0.00084446796 0.00076039246 0.00065400428 0.00056303944 0.0004967624 0.00044566038 0.00040806903 0.00037380721 0.00034133514 0.00030773421 0.00026817989][0.00065023865 0.00079488161 0.00089742977 0.00095664506 0.00095953292 0.00089131691 0.00077343849 0.00066414895 0.00057974446 0.00050987845 0.00045592157 0.00040906222 0.00036605264 0.00032105562 0.00027202073][0.00058199407 0.00072832283 0.00085887936 0.00096720987 0.0010174831 0.00097255735 0.00085668627 0.00073668233 0.00063990534 0.00055871654 0.00049239845 0.00043698435 0.00038739303 0.00033388557 0.000275754][0.00052902603 0.00066470279 0.0007988608 0.00092892192 0.0010118876 0.00099811 0.0009060392 0.0007925817 0.00069236383 0.000606004 0.00052937481 0.00046458925 0.00040663549 0.00034566323 0.00028076669][0.00050552405 0.00061924412 0.00073577417 0.00085870962 0.000951369 0.00096742826 0.00091333094 0.00082586973 0.00073443569 0.00064748421 0.00056389411 0.00048905378 0.0004225774 0.00035478183 0.00028422184][0.00052430609 0.00060813385 0.00069836725 0.0007986491 0.0008840207 0.00092086819 0.00090574636 0.00085236161 0.00077768083 0.00069633895 0.00060885935 0.00052373082 0.00044423176 0.00036578017 0.00028774841][0.0005563692 0.00061965385 0.000685316 0.0007610076 0.00083545939 0.00088371919 0.000897783 0.0008737006 0.00081624475 0.00074082194 0.00065053551 0.00055428914 0.00045983147 0.00037064592 0.00028734232][0.00056953845 0.000626124 0.00067687 0.00073275284 0.00079340377 0.00084407127 0.00087548752 0.00087546534 0.00083442667 0.00076812605 0.00067840581 0.00057427416 0.00046565535 0.00036609871 0.00028095464][0.000549291 0.00060499803 0.00064946467 0.00069206959 0.00073929527 0.00078006944 0.00081325881 0.00082892936 0.00080771011 0.00075628009 0.00067430321 0.000570536 0.00045407689 0.00034632857 0.00026305616][0.00050347834 0.0005603091 0.00060461881 0.00064175494 0.00067756849 0.00070541119 0.00072969391 0.00074935012 0.00074156822 0.00070519891 0.00063377444 0.0005377137 0.00042411478 0.00031710725 0.00023911308][0.000450238 0.00050296774 0.00054604595 0.00058257749 0.00061466807 0.00063744321 0.00065572566 0.00067499391 0.00067606155 0.00064805138 0.00058418635 0.00049740128 0.000392001 0.00029288061 0.00022036808][0.00040364519 0.00044649819 0.000482027 0.00051723258 0.00055090158 0.00057650392 0.00059598 0.00061526633 0.00061916414 0.0005943962 0.00053551281 0.00045834258 0.00036437542 0.0002758599 0.00021092843][0.00037088676 0.00040067485 0.00042411164 0.00045330182 0.00048711392 0.00051882723 0.00054584147 0.00057032437 0.00057647383 0.00055407651 0.00050279527 0.00043502261 0.00035290528 0.00027461577 0.00021486901][0.00035409012 0.000374106 0.00038575169 0.0004063833 0.0004368572 0.00047035309 0.00050336315 0.00053376885 0.00054492359 0.00052872731 0.00048568303 0.00042623348 0.00035396067 0.00028355003 0.00022671724][0.00035812162 0.00037606846 0.0003798921 0.00039019415 0.00041221344 0.00044109809 0.00047415117 0.00050909509 0.00052847184 0.00052155607 0.00048763031 0.00043515392 0.00036983515 0.00030370723 0.00024635656]]...]
INFO - root - 2017-12-11 03:35:51.840810: step 1110, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:14m:04s remains)
INFO - root - 2017-12-11 03:35:57.172281: step 1120, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:53m:40s remains)
INFO - root - 2017-12-11 03:36:02.462404: step 1130, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:45m:45s remains)
INFO - root - 2017-12-11 03:36:07.839946: step 1140, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:53m:22s remains)
INFO - root - 2017-12-11 03:36:13.162089: step 1150, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:12m:30s remains)
INFO - root - 2017-12-11 03:36:18.464758: step 1160, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.540 sec/batch; 49h:43m:54s remains)
INFO - root - 2017-12-11 03:36:23.860799: step 1170, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:36m:14s remains)
INFO - root - 2017-12-11 03:36:28.891730: step 1180, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:27m:25s remains)
INFO - root - 2017-12-11 03:36:34.253147: step 1190, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:47m:47s remains)
INFO - root - 2017-12-11 03:36:39.672213: step 1200, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.560 sec/batch; 51h:29m:59s remains)
2017-12-11 03:36:40.222428: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0005386874 0.00054563273 0.00053249649 0.00051105861 0.00049208646 0.00047944221 0.0004787161 0.00048433131 0.00048561022 0.00047368562 0.00044722555 0.00042568633 0.00041866137 0.00042811717 0.000450977][0.00060728029 0.00062020478 0.00059785956 0.00055962842 0.0005226949 0.00049691438 0.0004875912 0.00048983534 0.00048964511 0.00047943706 0.00045758762 0.00044339735 0.00044313361 0.00045215286 0.00046876038][0.00070104591 0.00071468286 0.00067426224 0.00061234011 0.00055479852 0.00051695085 0.00050252263 0.00049873092 0.00049533491 0.000489026 0.00047778644 0.0004754788 0.00047984376 0.00048423116 0.00048803794][0.00082403305 0.000832338 0.00077224651 0.00068661512 0.00061472924 0.00057233224 0.00056008709 0.00055561721 0.00054838334 0.00054289587 0.000536844 0.00053817348 0.00053883676 0.00053637614 0.00053116895][0.00091513386 0.00091021077 0.00084301678 0.00075064 0.0006798759 0.00064558885 0.0006432113 0.00064561161 0.00063488865 0.00062137545 0.000605387 0.00059706572 0.00059045659 0.00057985116 0.00056775403][0.00092887349 0.0009223533 0.0008660474 0.00079615228 0.00075151725 0.00074086821 0.00075600186 0.0007630394 0.00073923945 0.00070366875 0.00066476013 0.00063426839 0.00061521784 0.00059952645 0.00058340578][0.0008895935 0.00088899652 0.00085818977 0.00083121139 0.00083034835 0.00085885217 0.00090028555 0.00090516062 0.00085057796 0.00077706348 0.000708933 0.00065427547 0.00062027754 0.00060253136 0.00058636739][0.00083734567 0.00084123 0.0008375441 0.00086015457 0.00090840686 0.00097681338 0.0010460737 0.0010457466 0.00094376248 0.00082194444 0.00072424673 0.00065466441 0.00061356713 0.00059624837 0.00058581424][0.00078660337 0.00079982355 0.00082031213 0.00087916874 0.00095937151 0.0010431745 0.0011181926 0.0011071959 0.00096212619 0.00080908387 0.00070012873 0.00063192897 0.00059717003 0.00058617769 0.00058294105][0.00072173378 0.00074559666 0.00078371807 0.00085965754 0.0009420173 0.0010040803 0.001035508 0.00099411968 0.00085491437 0.0007203315 0.00063100772 0.00058065535 0.00056566135 0.00056600233 0.00056698668][0.000625994 0.0006555384 0.00069904845 0.000772791 0.00083752722 0.00086367235 0.0008465678 0.00077991118 0.00067175546 0.00058092759 0.00052707212 0.00050574896 0.00051229371 0.00052573159 0.00053049269][0.00051551667 0.00054655888 0.0005861547 0.00064439839 0.00068482105 0.00068183691 0.00063811196 0.00056746678 0.00049300579 0.000444235 0.00042589733 0.00043480843 0.00046084361 0.00048319454 0.00049051532][0.00041977497 0.0004488526 0.0004835971 0.00052473892 0.00054566294 0.00052846194 0.00048059138 0.00042276987 0.00037915996 0.00036310835 0.00037166566 0.00039874655 0.00043706279 0.00046483145 0.00047416074][0.00035438986 0.00038457132 0.00041457775 0.00044208029 0.00045293089 0.000439075 0.00040490332 0.00036907007 0.00034879809 0.00035177835 0.00037030477 0.00039950467 0.00043873276 0.0004685345 0.00048035872][0.00032120218 0.00035441326 0.00037882981 0.00039658693 0.00040577713 0.00040082258 0.00038146169 0.00036222232 0.00035651756 0.0003659853 0.00038486114 0.00041012146 0.00044235427 0.00046504411 0.00047389773]]...]
INFO - root - 2017-12-11 03:36:45.623213: step 1210, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.551 sec/batch; 50h:41m:16s remains)
INFO - root - 2017-12-11 03:36:50.966735: step 1220, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:11m:12s remains)
INFO - root - 2017-12-11 03:36:56.229147: step 1230, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:25m:03s remains)
INFO - root - 2017-12-11 03:37:01.608768: step 1240, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:43m:38s remains)
INFO - root - 2017-12-11 03:37:07.000152: step 1250, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:30m:31s remains)
INFO - root - 2017-12-11 03:37:12.270112: step 1260, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:53m:11s remains)
INFO - root - 2017-12-11 03:37:17.683391: step 1270, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:46m:05s remains)
INFO - root - 2017-12-11 03:37:22.911646: step 1280, loss = 0.75, batch loss = 0.69 (19.4 examples/sec; 0.413 sec/batch; 38h:00m:12s remains)
INFO - root - 2017-12-11 03:37:28.061365: step 1290, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:56m:13s remains)
INFO - root - 2017-12-11 03:37:33.472760: step 1300, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:57m:58s remains)
2017-12-11 03:37:34.037818: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00032566098 0.00030960611 0.0002956627 0.0003058346 0.00034585391 0.00041908343 0.00052947394 0.000644146 0.00068801583 0.000657526 0.0005766442 0.00046143687 0.00033950724 0.00023953599 0.0001717313][0.00042284746 0.00040255251 0.00038341308 0.0003914941 0.00043165952 0.00050885306 0.000626547 0.00074749545 0.0007916161 0.0007517616 0.00065382046 0.00051916664 0.00037876624 0.0002642255 0.00018605987][0.00052661356 0.00050381926 0.00047962277 0.00048290327 0.00051902432 0.00059501891 0.00071291247 0.00083162385 0.000869289 0.0008177362 0.00070386223 0.00055361725 0.00040143396 0.00027872354 0.00019479459][0.00063261879 0.00061092124 0.00058206415 0.00057945767 0.00060818827 0.00067867315 0.00079202262 0.00090361148 0.00092968548 0.00086428213 0.000735745 0.00057359855 0.00041402108 0.00028753802 0.00020099855][0.00072595157 0.0007095107 0.00068034569 0.00067384093 0.000694937 0.00075825222 0.00086536614 0.00096661481 0.000976596 0.00089592405 0.00075466733 0.00058353558 0.00041890715 0.00029106546 0.00020424167][0.00079649256 0.00079134345 0.00076870684 0.00076451828 0.00078179437 0.00083913555 0.00093870284 0.0010262709 0.0010146433 0.00091628736 0.0007632146 0.00058470346 0.00041707524 0.00028937781 0.00020403419][0.0008306097 0.00084466068 0.0008374473 0.000843949 0.00086226262 0.00091358129 0.0010035448 0.0010737526 0.0010369578 0.00092077832 0.00076008216 0.00057827117 0.00040987876 0.00028388907 0.00020133589][0.00082788657 0.00085833209 0.00086903863 0.00088822917 0.000909656 0.00095582 0.0010350167 0.0010883027 0.0010314902 0.00090524548 0.00074530568 0.0005666361 0.00040113719 0.00027785017 0.00019801235][0.00079186127 0.0008337652 0.00086169463 0.00089408865 0.00092060975 0.00096228643 0.0010275787 0.0010641709 0.0009973105 0.0008717014 0.00072061969 0.00055150239 0.0003927775 0.00027275694 0.00019481566][0.00072067149 0.00076772459 0.00081046141 0.00085458817 0.00088805275 0.000928511 0.00098057336 0.0010020254 0.0009333521 0.00081529358 0.00067671947 0.0005223513 0.00037705261 0.00026525647 0.00019120886][0.00062471215 0.00067319546 0.00072505441 0.00077792624 0.00081760844 0.00085698417 0.0008982963 0.00090965343 0.00084443262 0.00073774689 0.00061393756 0.00047821237 0.00035209104 0.00025438637 0.00018717469][0.00052246742 0.00056760653 0.00062077324 0.00067621772 0.00071880064 0.00075586559 0.00078885729 0.000795397 0.00073820836 0.00064645318 0.00054004724 0.00042566319 0.00032116563 0.00023999208 0.0001816031][0.00041805007 0.00045547166 0.00050193042 0.00055398687 0.00059706892 0.00063149323 0.00065822533 0.0006626164 0.00061748008 0.00054428424 0.00045758465 0.00036553241 0.00028321822 0.00021977526 0.00017206295][0.00031093915 0.00033915992 0.00037460096 0.0004189747 0.00045965286 0.000491479 0.000513091 0.00051587366 0.00048318948 0.00042981812 0.00036470772 0.00029570062 0.00023596289 0.00019147385 0.00015718174][0.00022248919 0.00024354389 0.00026820245 0.00030066518 0.00033232506 0.00035656523 0.00037261055 0.00037536022 0.00035374178 0.00031797652 0.00027256465 0.0002248762 0.0001862988 0.00016031715 0.00014005834]]...]
INFO - root - 2017-12-11 03:37:39.394127: step 1310, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:46m:58s remains)
INFO - root - 2017-12-11 03:37:44.763703: step 1320, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 47h:56m:41s remains)
INFO - root - 2017-12-11 03:37:50.210463: step 1330, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.523 sec/batch; 48h:09m:01s remains)
INFO - root - 2017-12-11 03:37:55.514596: step 1340, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.518 sec/batch; 47h:40m:13s remains)
INFO - root - 2017-12-11 03:38:00.948269: step 1350, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:37m:14s remains)
INFO - root - 2017-12-11 03:38:06.362075: step 1360, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.552 sec/batch; 50h:46m:46s remains)
INFO - root - 2017-12-11 03:38:11.707178: step 1370, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:12m:35s remains)
INFO - root - 2017-12-11 03:38:16.979581: step 1380, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.523 sec/batch; 48h:06m:03s remains)
INFO - root - 2017-12-11 03:38:22.041725: step 1390, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:17m:39s remains)
INFO - root - 2017-12-11 03:38:27.381502: step 1400, loss = 0.75, batch loss = 0.69 (16.0 examples/sec; 0.501 sec/batch; 46h:03m:30s remains)
2017-12-11 03:38:27.915560: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00076033693 0.00074673153 0.00071697094 0.00070158846 0.00069958728 0.000718652 0.00076064916 0.00082253205 0.00089561217 0.00096648245 0.0010219216 0.0010487448 0.0010411306 0.0010016613 0.00094441813][0.00077080779 0.00076527271 0.0007367169 0.00072164467 0.00071966782 0.00073714176 0.00077492016 0.00083085633 0.00090058928 0.00097306212 0.0010334397 0.0010676597 0.0010696485 0.0010378372 0.00098536233][0.0007267078 0.00073797838 0.00072765048 0.00072716246 0.00073530572 0.00075590389 0.00078800682 0.00082948303 0.00088119938 0.00093658356 0.00098444789 0.0010131401 0.0010166637 0.00099045888 0.00094504619][0.0006845069 0.00072255934 0.00074311416 0.00076829636 0.00079509034 0.00082173623 0.00084795133 0.00087047776 0.00089133007 0.00090999674 0.0009290427 0.00093795027 0.00093017885 0.00090248563 0.0008637823][0.00066627562 0.000743491 0.00080685836 0.00087165681 0.00093055813 0.00097512075 0.0010044449 0.0010118475 0.00099287438 0.00095432083 0.00091924291 0.00088697666 0.00084784132 0.00080381735 0.00076331478][0.00065499882 0.0007735687 0.00089498988 0.0010196429 0.0011338084 0.001215618 0.0012606968 0.0012571074 0.0011932321 0.0010835776 0.00097528828 0.00088488375 0.00079942285 0.00072532007 0.0006723142][0.00062315649 0.00077949726 0.00096188957 0.0011534004 0.0013358811 0.0014731849 0.0015483737 0.0015368855 0.0014281016 0.001246961 0.0010639949 0.00091378309 0.00078477361 0.00068057515 0.00061119749][0.00058999739 0.00076458883 0.00097984436 0.001212534 0.0014397922 0.0016128805 0.0017088568 0.0016965324 0.0015657365 0.0013443686 0.001120678 0.00093913346 0.00078715594 0.00066580332 0.00058255339][0.00054355746 0.00070792489 0.00091734604 0.0011515419 0.0013837103 0.0015603639 0.0016580296 0.0016512841 0.0015288492 0.0013189636 0.0011051398 0.00093562703 0.00079311425 0.00067502668 0.00058979896][0.00046779824 0.00059943495 0.0007749864 0.00098165055 0.0011912645 0.0013515464 0.0014436028 0.0014486752 0.0013578642 0.0011970713 0.001030572 0.00090127351 0.00079419522 0.00070018932 0.000624299][0.00039384043 0.00047513709 0.00059708831 0.00075648143 0.00092907966 0.0010701874 0.0011648072 0.0011961275 0.0011568465 0.0010637399 0.00095695374 0.00087163871 0.00079676247 0.0007185599 0.00064639287][0.00037589518 0.00040058623 0.00045729359 0.000557833 0.00068454054 0.00080550078 0.00091186445 0.00098464231 0.0010093614 0.00098639028 0.00093388429 0.00087807636 0.00080916053 0.00072274462 0.00063945766][0.00043608609 0.00040616927 0.00039566989 0.0004319314 0.00050754717 0.00060534372 0.00072051946 0.00083255576 0.00091506011 0.00095289858 0.00094150181 0.00089827419 0.0008189067 0.00071249629 0.00061275758][0.00056791259 0.00049447687 0.00042519285 0.00040264716 0.00042918348 0.00049942156 0.00061052648 0.00073971035 0.00085282081 0.00092725054 0.00094279257 0.00090542232 0.00081775489 0.0006996406 0.00059290469][0.00075423945 0.00065572816 0.0005429181 0.00047364467 0.00045596724 0.00049352081 0.00058501819 0.00070777594 0.00082562043 0.00091104943 0.0009392667 0.00090674974 0.00082029897 0.00070309767 0.00060047104]]...]
INFO - root - 2017-12-11 03:38:33.226790: step 1410, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:09m:12s remains)
INFO - root - 2017-12-11 03:38:38.565248: step 1420, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:51m:18s remains)
INFO - root - 2017-12-11 03:38:43.844332: step 1430, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:42m:29s remains)
INFO - root - 2017-12-11 03:38:49.173380: step 1440, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:24m:37s remains)
INFO - root - 2017-12-11 03:38:54.583556: step 1450, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:03m:35s remains)
INFO - root - 2017-12-11 03:38:59.990530: step 1460, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:10m:47s remains)
INFO - root - 2017-12-11 03:39:05.272257: step 1470, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:41m:52s remains)
INFO - root - 2017-12-11 03:39:10.622897: step 1480, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:48m:52s remains)
INFO - root - 2017-12-11 03:39:15.769906: step 1490, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:30m:42s remains)
INFO - root - 2017-12-11 03:39:21.115065: step 1500, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.521 sec/batch; 47h:56m:56s remains)
2017-12-11 03:39:21.727144: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0011740613 0.0010663152 0.00099004887 0.001015098 0.0011123362 0.0012185419 0.001265185 0.0012448557 0.0011697877 0.0010827408 0.0010029826 0.00093640987 0.00090872793 0.00090419478 0.00088772352][0.0013229605 0.0012053642 0.0011238316 0.0011740833 0.001317705 0.0014605698 0.0015211315 0.0014890234 0.001383713 0.0012535694 0.0011293949 0.0010262838 0.00097839546 0.00096765044 0.00094961183][0.0013271583 0.0012243715 0.0011656964 0.0012514306 0.0014345518 0.0016061451 0.0016802344 0.0016417161 0.001508373 0.0013363849 0.0011720287 0.0010403192 0.00097612268 0.00095625967 0.00093335722][0.0012432891 0.0011784919 0.0011599293 0.0012874452 0.001501184 0.0016934269 0.0017793984 0.0017407869 0.0015876897 0.0013843171 0.0011893659 0.0010402802 0.00096323842 0.00093016762 0.00089692621][0.0011330553 0.0011079726 0.0011356877 0.001298673 0.0015324433 0.0017338354 0.001826645 0.0017907884 0.0016347904 0.0014231548 0.0012178862 0.0010633391 0.00097873819 0.00093309511 0.00088421756][0.0010182268 0.0010316283 0.0011000799 0.0012874022 0.0015316325 0.0017387079 0.0018391267 0.0018125736 0.0016680664 0.0014651169 0.0012653284 0.0011128774 0.0010239354 0.00096824934 0.00090462313][0.00094316225 0.00099211512 0.0010908421 0.0012908145 0.0015368748 0.0017492555 0.0018653697 0.0018565406 0.0017313878 0.0015437171 0.0013565352 0.0012033934 0.0011021423 0.0010303168 0.00095147215][0.00094651146 0.0010091946 0.0011141689 0.0013114652 0.001555288 0.0017768601 0.0019149104 0.0019345349 0.0018414007 0.0016806351 0.0015100911 0.0013529707 0.0012316779 0.0011376331 0.001035933][0.0010057847 0.0010623065 0.0011419859 0.0013070698 0.0015284778 0.0017476828 0.001902234 0.0019536656 0.0019036507 0.0017839684 0.0016410135 0.0014870433 0.0013527308 0.0012426679 0.0011227453][0.0010774229 0.0011155878 0.0011508124 0.0012584029 0.0014316869 0.0016216153 0.0017703503 0.0018415847 0.0018371681 0.0017663229 0.0016563118 0.0015149926 0.0013865083 0.0012772925 0.0011545878][0.0011331537 0.0011429033 0.0011243192 0.0011631298 0.0012709707 0.0014092686 0.001531233 0.0016043049 0.0016317925 0.0016066101 0.0015385259 0.0014300507 0.0013244677 0.0012292376 0.0011151867][0.0011317995 0.0011160193 0.0010536833 0.0010334764 0.0010755876 0.0011552402 0.0012369435 0.001293862 0.0013286908 0.0013348405 0.0013080201 0.0012461576 0.0011785193 0.0011056083 0.0010052712][0.001068613 0.001035569 0.00094688608 0.00088424282 0.00087070203 0.00089403894 0.0009308497 0.00096527964 0.00099553482 0.0010196655 0.0010252903 0.0010053681 0.00097516493 0.00092639972 0.0008446626][0.00090991554 0.00087021152 0.00077472453 0.00069246214 0.0006446839 0.00062965218 0.00063442148 0.00064988423 0.00067374494 0.00070347812 0.0007245267 0.00072786526 0.00072262291 0.00069844915 0.00064354908][0.00066473655 0.00063051673 0.00055010215 0.00047705864 0.00042721929 0.00040110023 0.00039222083 0.00039485467 0.00040738657 0.00042989393 0.00044922935 0.00045772037 0.0004611992 0.00045671308 0.00043223824]]...]
INFO - root - 2017-12-11 03:39:27.012160: step 1510, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:42m:40s remains)
INFO - root - 2017-12-11 03:39:32.288092: step 1520, loss = 0.75, batch loss = 0.69 (15.7 examples/sec; 0.511 sec/batch; 46h:57m:26s remains)
INFO - root - 2017-12-11 03:39:37.593057: step 1530, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:11m:53s remains)
INFO - root - 2017-12-11 03:39:43.026505: step 1540, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:09m:46s remains)
INFO - root - 2017-12-11 03:39:48.464506: step 1550, loss = 0.75, batch loss = 0.69 (14.1 examples/sec; 0.568 sec/batch; 52h:15m:08s remains)
INFO - root - 2017-12-11 03:39:53.743716: step 1560, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:54m:27s remains)
INFO - root - 2017-12-11 03:39:59.125399: step 1570, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.532 sec/batch; 48h:51m:41s remains)
INFO - root - 2017-12-11 03:40:04.502013: step 1580, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:49m:37s remains)
INFO - root - 2017-12-11 03:40:09.645169: step 1590, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.539 sec/batch; 49h:30m:14s remains)
INFO - root - 2017-12-11 03:40:14.987106: step 1600, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:44m:39s remains)
2017-12-11 03:40:15.508865: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0011949816 0.0011490136 0.0010399921 0.00093780376 0.00085161906 0.00078120589 0.0007391806 0.00073107082 0.00076009566 0.00082702306 0.00094138569 0.0011037794 0.0012901506 0.001475475 0.0016313677][0.001147629 0.0010612549 0.00092442194 0.00081871753 0.00075358606 0.00071875867 0.00071815419 0.00075467478 0.00082730845 0.00093583314 0.0010809237 0.0012612047 0.0014507321 0.0016286409 0.0017736759][0.0010445641 0.00095595437 0.00082676951 0.00074277888 0.00071541028 0.00073184026 0.00078201236 0.00085996452 0.00096188235 0.0010827816 0.0012247334 0.0013868936 0.0015406414 0.0016745663 0.0017824941][0.0010016415 0.00094539125 0.00085427472 0.00080796145 0.00082324242 0.000891115 0.00098759716 0.0010896896 0.0011922059 0.001295501 0.0014077575 0.0015291057 0.0016327456 0.001715926 0.0017845946][0.0010374008 0.0010330278 0.00099636544 0.00099626009 0.0010522819 0.0011631455 0.0012891751 0.0013886541 0.0014616462 0.0015259002 0.0015937753 0.0016646016 0.001720213 0.0017637271 0.0018040778][0.0011235238 0.0011810459 0.0012099524 0.0012657648 0.0013655864 0.0015073551 0.0016387998 0.001703999 0.0017205277 0.0017305291 0.0017522573 0.0017755855 0.0017860536 0.0017956998 0.0018153393][0.0012135005 0.0013382077 0.0014353213 0.0015455063 0.001684037 0.0018468193 0.0019645595 0.0019721303 0.0019181087 0.001870822 0.0018513389 0.0018398352 0.0018197049 0.0018088632 0.0018147739][0.001277093 0.0014621358 0.0016194924 0.0017717209 0.001931032 0.0020935389 0.0021861596 0.0021338228 0.0020125769 0.0019130893 0.0018605749 0.0018284711 0.0017971188 0.0017816671 0.0017872129][0.001274279 0.0015009914 0.0016998827 0.0018742755 0.00202456 0.0021467826 0.0021923275 0.0021091376 0.0019622974 0.0018399182 0.0017737222 0.0017362318 0.0017118733 0.001709776 0.0017311977][0.0011898144 0.0014281872 0.0016396475 0.0018163333 0.0019470566 0.00202154 0.0020200433 0.0019280177 0.0017939652 0.0016835722 0.0016281808 0.001603968 0.0015966769 0.0016120032 0.0016483411][0.001056291 0.0012766174 0.0014721268 0.0016325896 0.0017406085 0.0017790674 0.0017452956 0.0016527672 0.0015448747 0.001468904 0.0014443069 0.0014498585 0.001468118 0.0014972376 0.0015326723][0.00092797633 0.0011038594 0.0012559124 0.0013785115 0.001454738 0.0014678929 0.0014184633 0.0013336132 0.0012556933 0.0012199259 0.0012346774 0.0012766846 0.0013246909 0.001364093 0.0013909157][0.00084644003 0.00095870486 0.0010463025 0.0011118319 0.0011451469 0.0011366503 0.0010862463 0.001019387 0.00097313873 0.000972261 0.0010167138 0.0010832616 0.0011498826 0.0011980431 0.0012241689][0.00081801281 0.0008620178 0.00088608934 0.00089792942 0.00089417008 0.000869586 0.0008242242 0.00077574159 0.00075248448 0.00077192276 0.00082877377 0.00090170163 0.00097320531 0.0010284694 0.0010629978][0.00086285861 0.00085868838 0.00083693833 0.00081188517 0.00078264321 0.00074721134 0.00070448965 0.00066622859 0.0006533775 0.000680511 0.00073943712 0.00081006088 0.00088012713 0.00094031444 0.00098765036]]...]
INFO - root - 2017-12-11 03:40:20.864596: step 1610, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:10m:56s remains)
INFO - root - 2017-12-11 03:40:26.203199: step 1620, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:21m:59s remains)
INFO - root - 2017-12-11 03:40:31.592991: step 1630, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:15m:01s remains)
INFO - root - 2017-12-11 03:40:36.885091: step 1640, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 49h:11m:02s remains)
INFO - root - 2017-12-11 03:40:42.262707: step 1650, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.529 sec/batch; 48h:34m:29s remains)
INFO - root - 2017-12-11 03:40:47.580540: step 1660, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:46m:54s remains)
INFO - root - 2017-12-11 03:40:52.869563: step 1670, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.529 sec/batch; 48h:39m:18s remains)
INFO - root - 2017-12-11 03:40:58.175498: step 1680, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:33m:51s remains)
INFO - root - 2017-12-11 03:41:03.502471: step 1690, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:26m:13s remains)
INFO - root - 2017-12-11 03:41:08.577691: step 1700, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:33m:00s remains)
2017-12-11 03:41:09.151721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0016252149 0.0016637633 0.0016323293 0.0015766817 0.0015046214 0.0014306004 0.0013542112 0.0012713333 0.0012068469 0.0011727227 0.0011874152 0.0012262393 0.0012330645 0.0012004222 0.0011542826][0.0016319227 0.0016587009 0.0016169966 0.0015534053 0.0014708785 0.0013861838 0.0013048945 0.0012167549 0.0011446232 0.0011172624 0.0011523319 0.0012194706 0.0012534338 0.0012434253 0.0012053485][0.0015343055 0.0015592499 0.0015309948 0.0014875765 0.0014251661 0.0013593817 0.0012997941 0.0012320015 0.0011699029 0.0011428457 0.0011728487 0.0012331443 0.0012640334 0.0012556373 0.0012138974][0.0014303481 0.0014763998 0.0014950694 0.0015096442 0.0015074185 0.0014974992 0.0014895761 0.0014649878 0.0014217959 0.0013767672 0.0013647072 0.0013776276 0.0013676463 0.0013260935 0.0012626369][0.001315893 0.0014081671 0.0015096077 0.0016246904 0.0017253011 0.0018062076 0.0018744546 0.0019033038 0.0018728391 0.0017817371 0.0016883686 0.0016173635 0.0015374435 0.0014406258 0.0013426018][0.0011760544 0.0013348706 0.0015431077 0.0017818382 0.0019995871 0.0021785165 0.0023261302 0.0024147711 0.002398317 0.0022530633 0.0020687808 0.0019150728 0.0017706596 0.0016182886 0.0014762273][0.0010280362 0.0012449145 0.0015459044 0.001889848 0.0022057823 0.0024657121 0.0026785904 0.0028194983 0.0028226033 0.0026374708 0.0023914261 0.0022003613 0.0020354739 0.0018511012 0.001664054][0.0008682193 0.0011238395 0.0014757067 0.0018778115 0.0022506374 0.0025567662 0.0028010565 0.0029640805 0.00297882 0.0027914641 0.0025550772 0.002399036 0.0022724452 0.0020917766 0.0018741967][0.00072299963 0.00097447506 0.0013191128 0.0017136459 0.0020851332 0.0023953898 0.0026378264 0.0027962956 0.0028200382 0.002688108 0.0025375402 0.0024635906 0.0023989021 0.002240852 0.0020091925][0.00063284481 0.00083888037 0.001119576 0.0014450975 0.0017597402 0.0020314436 0.0022508414 0.0024070456 0.0024638092 0.0024143448 0.00236571 0.0023646581 0.0023409724 0.002200552 0.001969588][0.00061541726 0.00075088179 0.0009387593 0.001160051 0.0013815499 0.0015818842 0.0017573424 0.0019039317 0.0019923185 0.0020126859 0.0020403585 0.0020851863 0.0020846778 0.0019666455 0.001760371][0.00063589 0.00071092718 0.00081267883 0.00093621359 0.0010676568 0.0011977594 0.0013262461 0.0014498024 0.0015438343 0.0015975458 0.0016536997 0.0017017542 0.0017020691 0.0016035719 0.0014378756][0.00066233677 0.00069769734 0.0007411085 0.000797389 0.00086461619 0.0009405617 0.0010248652 0.0011155746 0.0011932701 0.0012466399 0.0012931519 0.0013156362 0.0012958999 0.0012080777 0.0010818199][0.00068993337 0.00070290291 0.00071544159 0.000733869 0.000757696 0.00078942493 0.00083177147 0.00088366109 0.00093353452 0.00096708507 0.000988807 0.00098412868 0.00094723422 0.00086878589 0.0007761232][0.00072834879 0.00073310122 0.00073099596 0.00072649895 0.00071786949 0.00071139581 0.00071401434 0.0007279244 0.00074875262 0.00076061627 0.00076085742 0.00074097113 0.00070183945 0.00063892076 0.00057524041]]...]
INFO - root - 2017-12-11 03:41:14.448047: step 1710, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:25m:57s remains)
INFO - root - 2017-12-11 03:41:19.786255: step 1720, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:16m:06s remains)
INFO - root - 2017-12-11 03:41:25.130657: step 1730, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:08m:22s remains)
INFO - root - 2017-12-11 03:41:30.422968: step 1740, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:01m:46s remains)
INFO - root - 2017-12-11 03:41:35.818821: step 1750, loss = 0.75, batch loss = 0.69 (14.1 examples/sec; 0.568 sec/batch; 52h:08m:50s remains)
INFO - root - 2017-12-11 03:41:41.187927: step 1760, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 49h:57m:09s remains)
INFO - root - 2017-12-11 03:41:46.560322: step 1770, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:05m:19s remains)
INFO - root - 2017-12-11 03:41:51.913898: step 1780, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.560 sec/batch; 51h:28m:52s remains)
INFO - root - 2017-12-11 03:41:57.224465: step 1790, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 49h:04m:16s remains)
INFO - root - 2017-12-11 03:42:02.263436: step 1800, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:17m:22s remains)
2017-12-11 03:42:02.810576: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0019563753 0.0019218128 0.0018501823 0.0018121086 0.0017908212 0.0018130892 0.0018494092 0.0018858422 0.0018908877 0.0018594378 0.0018428236 0.0018274038 0.0018016878 0.0018002666 0.0017915966][0.001890099 0.0018547588 0.0017794669 0.0017395454 0.0017178622 0.00174758 0.0018025396 0.0018599641 0.00187629 0.0018379465 0.0018110336 0.0017773389 0.0017376776 0.0017226011 0.0017141324][0.0017567133 0.0017360351 0.0016682985 0.0016387916 0.0016269044 0.0016636027 0.0017314046 0.0018009183 0.0018209831 0.0017685026 0.0017198558 0.0016561989 0.0015879548 0.0015546607 0.001541927][0.0017514101 0.0017559425 0.0017081607 0.0016917742 0.0016841416 0.0017159478 0.0017903582 0.0018711768 0.001894276 0.0018312057 0.0017635381 0.0016653076 0.0015575173 0.0014896159 0.0014537739][0.0018397977 0.0018788676 0.001860028 0.0018533622 0.0018398954 0.0018575009 0.0019372894 0.0020365978 0.002062137 0.0019919253 0.0019050664 0.0017698029 0.0016154089 0.0014969581 0.0014225934][0.0019671919 0.002034608 0.0020369356 0.0020308832 0.0020086605 0.0020154305 0.0021053352 0.00222698 0.0022510309 0.0021653343 0.0020518056 0.0018826203 0.0016876304 0.00152198 0.0014150958][0.0020547055 0.0021297152 0.0021329357 0.0021219084 0.0020968949 0.0021075932 0.0022142953 0.0023584138 0.0023729608 0.0022623555 0.0021218453 0.0019301706 0.0017146824 0.0015320806 0.0014195276][0.0020140086 0.0020838161 0.0020780959 0.0020684542 0.0020532892 0.0020825763 0.0022078902 0.0023615805 0.0023589253 0.0022239066 0.0020662032 0.0018698265 0.0016624342 0.0014985395 0.0014083737][0.0018091218 0.0018684079 0.0018576876 0.0018616008 0.0018755074 0.0019331838 0.0020635305 0.0021951851 0.0021749567 0.002036561 0.0018876842 0.0017153431 0.0015481832 0.0014323432 0.0013818144][0.0015353826 0.0015600862 0.0015319467 0.001543226 0.0015889128 0.0016798511 0.0018110111 0.0019192208 0.0019120427 0.0018137058 0.0017090036 0.0015812948 0.0014679463 0.0014077734 0.0013878482][0.0013198047 0.0012928231 0.0012321088 0.0012345266 0.0013015486 0.0014267078 0.001574414 0.00169096 0.0017294724 0.0016952571 0.0016462918 0.0015656663 0.0015007136 0.0014809752 0.0014706614][0.0012275863 0.0011712569 0.0010866852 0.0010707553 0.0011418212 0.0012855225 0.0014540543 0.0016034999 0.0017002393 0.0017323853 0.0017341725 0.0016893976 0.0016512899 0.0016435742 0.0016184635][0.0012740411 0.0012251727 0.0011401514 0.0011096556 0.0011750174 0.001322299 0.0015058148 0.0016862811 0.0018278321 0.0019005147 0.0019293027 0.0018993756 0.0018655339 0.0018472136 0.0017982535][0.0014233439 0.0014112282 0.0013428844 0.0013073623 0.001363503 0.0014997524 0.0016714211 0.0018489434 0.0019999077 0.0020764726 0.0021002123 0.002060533 0.0020171604 0.0019901688 0.0019326251][0.0015572145 0.001599203 0.001561882 0.0015363949 0.0015853156 0.0016952609 0.0018270629 0.0019579744 0.0020713068 0.0021120741 0.0021034721 0.0020429702 0.0019938017 0.0019746264 0.0019396165]]...]
INFO - root - 2017-12-11 03:42:08.113374: step 1810, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.540 sec/batch; 49h:37m:25s remains)
INFO - root - 2017-12-11 03:42:13.488188: step 1820, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:52m:45s remains)
INFO - root - 2017-12-11 03:42:18.851840: step 1830, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.550 sec/batch; 50h:32m:09s remains)
INFO - root - 2017-12-11 03:42:24.133406: step 1840, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:29m:07s remains)
INFO - root - 2017-12-11 03:42:29.501762: step 1850, loss = 0.75, batch loss = 0.69 (14.2 examples/sec; 0.562 sec/batch; 51h:38m:35s remains)
INFO - root - 2017-12-11 03:42:34.826570: step 1860, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:19m:35s remains)
INFO - root - 2017-12-11 03:42:40.125313: step 1870, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.555 sec/batch; 50h:55m:36s remains)
INFO - root - 2017-12-11 03:42:45.516157: step 1880, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:24m:57s remains)
INFO - root - 2017-12-11 03:42:50.871997: step 1890, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 49h:54m:16s remains)
INFO - root - 2017-12-11 03:42:55.962102: step 1900, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:47m:50s remains)
2017-12-11 03:42:56.516381: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0016034124 0.0017559861 0.001808952 0.0017953737 0.0017094546 0.0015549308 0.0013729569 0.0012203129 0.0011251437 0.0010748849 0.0010723328 0.0011236618 0.0012128955 0.0013163296 0.0014123791][0.0017554148 0.0019202308 0.0019786807 0.0019595439 0.0018568557 0.0016881233 0.001498498 0.0013418015 0.0012443569 0.0011949504 0.0011893981 0.0012289673 0.0012982083 0.0013771789 0.0014488333][0.0016954627 0.0018526404 0.0019161935 0.0019064542 0.0018169542 0.0016729246 0.0015151196 0.001385275 0.001302745 0.0012576669 0.0012446182 0.0012606569 0.0012939733 0.0013294988 0.0013563512][0.0015240281 0.0016749196 0.0017607979 0.0017946274 0.0017633589 0.0016856209 0.0015945436 0.0015169467 0.001466008 0.001433547 0.0014163961 0.0014108646 0.0014028293 0.0013771589 0.0013352236][0.001363367 0.0015240287 0.0016504645 0.001760482 0.0018256729 0.0018507644 0.0018607725 0.0018661674 0.0018646866 0.0018492661 0.0018228501 0.0017868464 0.0017239198 0.0016176187 0.0014832285][0.0012780225 0.0014721428 0.0016664247 0.0018773928 0.0020607153 0.0022053332 0.0023264876 0.0024129769 0.0024465995 0.0024207751 0.0023546307 0.0022644331 0.0021325226 0.001943077 0.0017240375][0.0012967368 0.0015425113 0.0018048388 0.0021016854 0.0023796007 0.0026130483 0.0028084696 0.0029338929 0.0029628084 0.0028971417 0.002772114 0.0026216828 0.0024314474 0.0021869498 0.001920622][0.0014011086 0.0016864914 0.0019868212 0.0023148777 0.0026147827 0.0028534466 0.0030321528 0.0031205222 0.0031027014 0.0029888039 0.0028179719 0.0026319027 0.0024286248 0.0021924609 0.0019478063][0.0014843303 0.001773678 0.0020615638 0.0023593847 0.0026153971 0.0027983887 0.0029088908 0.0029301562 0.002864094 0.0027214596 0.0025339168 0.0023440206 0.0021621482 0.0019775275 0.001799698][0.0014805843 0.0017331553 0.0019603965 0.0021803943 0.0023536931 0.0024577128 0.0024984609 0.0024712498 0.0023794086 0.0022303774 0.0020509008 0.0018849259 0.0017495085 0.0016348473 0.0015384372][0.0013667279 0.0015500368 0.0016898558 0.001810511 0.0018916064 0.0019222334 0.0019110052 0.0018577009 0.0017617035 0.0016327227 0.0014924236 0.0013800142 0.0013087409 0.0012696432 0.0012493839][0.0011936531 0.0013127052 0.0013731702 0.0014091483 0.0014202811 0.0014044278 0.0013655708 0.0013058818 0.0012265558 0.0011398057 0.0010584048 0.0010093766 0.00099592248 0.0010105927 0.0010340536][0.00106053 0.0011231959 0.0011313289 0.0011215874 0.0011059792 0.0010800522 0.0010367794 0.00098156685 0.00092060043 0.0008704979 0.00083292538 0.00082391972 0.00084078842 0.00087738567 0.00091559556][0.0010276873 0.0010583408 0.0010408389 0.0010175634 0.0010008038 0.00098052563 0.00094184966 0.00089200621 0.00084076542 0.00080672541 0.00078306027 0.00077710056 0.00078843726 0.00081376935 0.00084206642][0.0010730212 0.0011026995 0.0010885366 0.0010772229 0.0010718543 0.001061764 0.0010289035 0.00098064472 0.00092861574 0.00089082978 0.00085811358 0.00082984497 0.0008077044 0.00079734647 0.00080130377]]...]
INFO - root - 2017-12-11 03:43:01.814573: step 1910, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:47m:46s remains)
INFO - root - 2017-12-11 03:43:07.080945: step 1920, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:16m:03s remains)
INFO - root - 2017-12-11 03:43:12.379945: step 1930, loss = 0.75, batch loss = 0.69 (15.8 examples/sec; 0.506 sec/batch; 46h:26m:05s remains)
INFO - root - 2017-12-11 03:43:17.728300: step 1940, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.554 sec/batch; 50h:53m:42s remains)
INFO - root - 2017-12-11 03:43:23.070878: step 1950, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:46m:13s remains)
INFO - root - 2017-12-11 03:43:28.408733: step 1960, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 50h:14m:30s remains)
INFO - root - 2017-12-11 03:43:33.711275: step 1970, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:30m:38s remains)
INFO - root - 2017-12-11 03:43:39.071521: step 1980, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 49h:08m:14s remains)
INFO - root - 2017-12-11 03:43:44.429696: step 1990, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:13m:25s remains)
INFO - root - 2017-12-11 03:43:49.738117: step 2000, loss = 0.75, batch loss = 0.69 (17.0 examples/sec; 0.470 sec/batch; 43h:06m:14s remains)
2017-12-11 03:43:50.185826: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0012169614 0.0012937775 0.0013486112 0.0014103006 0.0014702627 0.0015224031 0.0015736052 0.0016421159 0.0017544406 0.0018940738 0.0020581828 0.0022165724 0.0023110125 0.0023018927 0.0021979706][0.0013934737 0.0015360514 0.0016594031 0.001789141 0.0019155665 0.002031056 0.0021218464 0.002195782 0.0022924414 0.0024207979 0.0025825042 0.0027377096 0.0028230755 0.0027996344 0.0026769282][0.00159338 0.0018237563 0.0020442368 0.0022616913 0.0024629484 0.0026406732 0.0027573251 0.002802853 0.0028350542 0.0028986051 0.0030038306 0.0031124859 0.0031681724 0.0031399343 0.0030240177][0.0018751395 0.0022098096 0.0025409833 0.0028508208 0.0031096996 0.0033132609 0.0034134642 0.0033928975 0.0033254474 0.00329028 0.003308363 0.0033504101 0.003375585 0.0033582437 0.0032716277][0.0021403038 0.0025658766 0.0029936666 0.0033772937 0.0036648596 0.0038592806 0.0039208219 0.0038291791 0.0036702997 0.0035565083 0.0035073159 0.0034954129 0.0034927083 0.0034829162 0.003423098][0.0023322292 0.0028128934 0.0032921364 0.0037071467 0.0039893826 0.0041581243 0.004194316 0.0040681716 0.0038648779 0.0037229673 0.0036518641 0.003611661 0.0035767953 0.0035483469 0.0034840282][0.0024404568 0.0029209605 0.003401177 0.0038064772 0.0040565329 0.004189529 0.0042172461 0.0040971939 0.0038987177 0.0037723284 0.003722406 0.0036884265 0.0036282479 0.0035610076 0.0034619239][0.0024595095 0.0028939641 0.0033294952 0.0036922691 0.003900788 0.0039952579 0.0040135728 0.0039094132 0.0037403794 0.0036521633 0.0036493824 0.0036492283 0.0035868427 0.0034863751 0.0033465317][0.0023869607 0.0027500167 0.0031098162 0.0034120076 0.0035786887 0.0036352661 0.0036370507 0.0035466098 0.0034186451 0.0033850796 0.0034358585 0.0034727103 0.0034129564 0.0032820483 0.0031075084][0.0023111627 0.0025977851 0.0028662391 0.0030966247 0.0032214737 0.0032468901 0.0032308763 0.0031541225 0.0030614228 0.003064645 0.0031444337 0.0031999631 0.0031338586 0.0029698536 0.0027661787][0.0022348221 0.0024665147 0.0026587318 0.0028336656 0.0029322766 0.0029397346 0.0029048126 0.0028275291 0.0027414896 0.002729848 0.0027822305 0.0028167549 0.0027348662 0.0025561904 0.0023497208][0.0020880969 0.0022821268 0.002435013 0.0025949012 0.0027044707 0.0027229676 0.0026804965 0.0025921871 0.0024835304 0.0024127781 0.0023909307 0.0023704621 0.0022713372 0.0021081893 0.0019380604][0.0018495616 0.0020130381 0.0021477388 0.0023139168 0.0024531218 0.0025031252 0.0024755613 0.0023963978 0.0022785545 0.002159982 0.0020644746 0.0019806027 0.0018608304 0.0017174664 0.0015902191][0.0015954578 0.0017137895 0.0018141269 0.0019641691 0.002107667 0.0021766864 0.0021735916 0.0021275932 0.0020384493 0.0019230467 0.0018049527 0.0016940335 0.001571877 0.0014485563 0.0013555919][0.0013667435 0.0014305867 0.001482147 0.0015906147 0.0017096213 0.0017787645 0.001794333 0.0017809359 0.0017315227 0.0016466895 0.0015496903 0.0014531531 0.0013545499 0.0012624826 0.0012011091]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:43:55.656521: step 2010, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:56m:39s remains)
INFO - root - 2017-12-11 03:44:01.025261: step 2020, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.529 sec/batch; 48h:31m:55s remains)
INFO - root - 2017-12-11 03:44:06.315534: step 2030, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:48m:15s remains)
INFO - root - 2017-12-11 03:44:11.727037: step 2040, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 49h:50m:34s remains)
INFO - root - 2017-12-11 03:44:17.111772: step 2050, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:37m:11s remains)
INFO - root - 2017-12-11 03:44:22.517184: step 2060, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.540 sec/batch; 49h:33m:08s remains)
INFO - root - 2017-12-11 03:44:27.927836: step 2070, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.556 sec/batch; 51h:03m:16s remains)
INFO - root - 2017-12-11 03:44:33.307975: step 2080, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.545 sec/batch; 50h:00m:02s remains)
INFO - root - 2017-12-11 03:44:38.557855: step 2090, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:37m:36s remains)
INFO - root - 2017-12-11 03:44:43.855018: step 2100, loss = 0.75, batch loss = 0.69 (15.7 examples/sec; 0.510 sec/batch; 46h:47m:46s remains)
2017-12-11 03:44:44.376925: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028829826 0.003300654 0.0036556965 0.0039389525 0.0041164155 0.0042196675 0.004231873 0.0041149878 0.0039576963 0.0037506572 0.0035309433 0.0032713469 0.0030036285 0.0027627475 0.0025768541][0.002955205 0.0033663113 0.0036984454 0.0039625349 0.0041170525 0.0041763335 0.0041315891 0.0039687147 0.0037730529 0.0035710284 0.0033718178 0.0031540871 0.0029229508 0.0027108756 0.0025693907][0.0028550145 0.0032333224 0.0035186198 0.0037379605 0.0038616166 0.0038875972 0.0038081796 0.0036338144 0.0034455229 0.0032753947 0.0031179425 0.0029494506 0.0027637708 0.0025860821 0.0024798252][0.0028911491 0.0032062593 0.0034228258 0.0035769884 0.0036663625 0.0036755039 0.0035887433 0.0034269192 0.0032628842 0.0031220971 0.002991881 0.00285021 0.0026807019 0.0025037355 0.0023976306][0.0030912105 0.0033119083 0.0034307558 0.0035024178 0.0035467118 0.003544542 0.0034668227 0.0033269019 0.0031822224 0.0030542656 0.0029286414 0.0027900611 0.0026147014 0.0024160396 0.0022821487][0.0033238132 0.0034342601 0.0034405673 0.0034323528 0.0034401342 0.003448518 0.0034148095 0.0033239867 0.0032040544 0.0030776439 0.0029329774 0.002772318 0.0025730361 0.0023397096 0.0021652179][0.0034079554 0.0034113498 0.0033383935 0.0032991539 0.0033170721 0.0033721845 0.0034123617 0.003395143 0.0033060189 0.0031678001 0.0029853599 0.0027953158 0.0025802217 0.002328997 0.0021336675][0.0033688541 0.0032994957 0.0031901151 0.0031691098 0.0032332912 0.0033505976 0.0034602135 0.0035014944 0.0034344795 0.0032784806 0.0030631288 0.0028564471 0.002650273 0.0024178773 0.002244283][0.0033322107 0.0032170964 0.0030902026 0.003101941 0.0032195584 0.0033927485 0.0035521891 0.0036254902 0.0035688861 0.0033972035 0.0031698067 0.0029698394 0.0027931302 0.0026076389 0.0024918804][0.0032951047 0.0031541835 0.003012 0.0030550209 0.0032283389 0.0034605581 0.0036669676 0.0037672552 0.0037169941 0.0035324031 0.003302787 0.0031187879 0.002971204 0.0028293978 0.0027623416][0.0032080365 0.0030691682 0.0029365055 0.0030174535 0.0032404065 0.0035163136 0.0037580631 0.0038800871 0.0038368208 0.003643967 0.0034156439 0.0032561643 0.0031474344 0.0030486891 0.0030109205][0.0030749976 0.0029644251 0.0028668882 0.0029862842 0.00323908 0.0035265323 0.0037658457 0.003883193 0.00383796 0.0036416734 0.0034259642 0.00331363 0.0032772894 0.0032522709 0.0032534075][0.0029961204 0.0029007324 0.0028090561 0.0029273883 0.0031773211 0.0034585786 0.0036850364 0.0037928859 0.0037485315 0.0035592406 0.0033695651 0.0033152741 0.0033683844 0.0034377859 0.0034857732][0.0030630676 0.0029298805 0.0027798908 0.00282854 0.0030273343 0.0032867114 0.0035108784 0.0036302942 0.0036050454 0.003436693 0.0032722107 0.0032583356 0.0033768388 0.0035247775 0.0036113746][0.0031501844 0.0029912642 0.0027842033 0.0027457711 0.0028644281 0.0030729184 0.0032803447 0.0034138595 0.0034189087 0.0032915238 0.0031625419 0.0031777578 0.0033319506 0.0035242874 0.0036358386]]...]
INFO - root - 2017-12-11 03:44:49.470752: step 2110, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:36m:17s remains)
INFO - root - 2017-12-11 03:44:54.928053: step 2120, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.528 sec/batch; 48h:24m:40s remains)
INFO - root - 2017-12-11 03:45:00.205533: step 2130, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.519 sec/batch; 47h:37m:01s remains)
INFO - root - 2017-12-11 03:45:05.509077: step 2140, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.522 sec/batch; 47h:55m:28s remains)
INFO - root - 2017-12-11 03:45:10.908607: step 2150, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:57m:36s remains)
INFO - root - 2017-12-11 03:45:16.283110: step 2160, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.522 sec/batch; 47h:53m:50s remains)
INFO - root - 2017-12-11 03:45:21.527978: step 2170, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 50h:10m:02s remains)
INFO - root - 2017-12-11 03:45:26.899019: step 2180, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:06m:26s remains)
INFO - root - 2017-12-11 03:45:32.283205: step 2190, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.539 sec/batch; 49h:27m:22s remains)
INFO - root - 2017-12-11 03:45:37.564327: step 2200, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:18m:13s remains)
2017-12-11 03:45:38.102276: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028801614 0.0029381816 0.0030522149 0.0033462024 0.0038285824 0.0044246162 0.004953396 0.0052595027 0.0052451226 0.0049563828 0.0045141364 0.0040544053 0.0036833766 0.0033967244 0.0032230576][0.0028881524 0.0029129516 0.0030166542 0.0033433549 0.0039106361 0.0046246075 0.005261804 0.0056031393 0.0055458797 0.0051369811 0.0045386208 0.0039411746 0.0034935842 0.0032212348 0.0031296746][0.0026962529 0.0026670925 0.0027460915 0.003085241 0.0036992668 0.0044773878 0.0051686917 0.0055183871 0.0054160245 0.0049327109 0.0042494088 0.0035774638 0.0031063796 0.0028924157 0.0029207033][0.0025830972 0.0024948011 0.0025489309 0.0028927648 0.0035276671 0.0043200781 0.0050067273 0.0053198752 0.0051486208 0.0045944215 0.003860058 0.0031726405 0.0027335417 0.0026128816 0.0027871269][0.0024921042 0.0023659104 0.0024233914 0.002795218 0.0034603751 0.0042608017 0.0049238917 0.0051748659 0.0049131559 0.0042789695 0.0035049431 0.0028358402 0.0024636756 0.0024536408 0.0027691338][0.0023543215 0.0022346503 0.0023301437 0.0027548373 0.0034606406 0.0042686719 0.0049045621 0.0050894716 0.0047356607 0.0040253992 0.0032210122 0.0025772951 0.0022745314 0.0023673447 0.0028093765][0.0021699977 0.0020889142 0.0022391663 0.0027195413 0.0034547683 0.0042565642 0.0048606964 0.0049918564 0.0045731817 0.0038170493 0.0030027831 0.002388221 0.0021477065 0.0023267213 0.002877359][0.001973605 0.0019371477 0.0021393164 0.0026548307 0.0033929746 0.0041746716 0.0047589964 0.0048820986 0.0044622975 0.0037109421 0.0029144853 0.0023248335 0.0021152613 0.0023348064 0.0029386696][0.0017665351 0.0017865323 0.002035134 0.0025626041 0.0032721006 0.004015089 0.0045807632 0.0047294875 0.0043659019 0.0036698089 0.002924331 0.0023693754 0.0021799665 0.0024048882 0.0030028573][0.0015578863 0.0016402688 0.001921003 0.0024341708 0.0030852531 0.0037612536 0.0042911619 0.0044691507 0.0041981842 0.0036108028 0.0029599448 0.0024626683 0.0022920286 0.0024921612 0.0030273858][0.0013732455 0.0014943817 0.0017749671 0.0022411861 0.0028095485 0.0033987882 0.0038766027 0.0040841685 0.0039360537 0.0035073659 0.0029976962 0.0025888931 0.0024437974 0.0025963439 0.0030208128][0.0012416282 0.0013767523 0.0016246369 0.0020151809 0.0024831777 0.0029714738 0.0033830383 0.0036100165 0.0035846396 0.0033299676 0.0029865466 0.0026904729 0.0025844728 0.0026893478 0.0029886968][0.001182526 0.0013140435 0.001515658 0.0018169128 0.0021701481 0.0025408093 0.0028642523 0.0030723282 0.0031186566 0.0029972731 0.0028029387 0.0026264358 0.0025770785 0.0026646932 0.0028730766][0.0011653893 0.001287976 0.0014457093 0.0016632759 0.0019086899 0.0021661215 0.0023932124 0.002547117 0.0026061807 0.002562759 0.0024811598 0.0024140044 0.002428262 0.0025242849 0.002685226][0.0011499164 0.0012586968 0.0013804724 0.0015335629 0.0016990915 0.0018690643 0.0020168941 0.0021192678 0.0021721604 0.0021747155 0.0021677224 0.0021786005 0.0022383067 0.0023385654 0.0024639189]]...]
INFO - root - 2017-12-11 03:45:43.194540: step 2210, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:17m:50s remains)
INFO - root - 2017-12-11 03:45:48.470457: step 2220, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.517 sec/batch; 47h:26m:09s remains)
INFO - root - 2017-12-11 03:45:53.779289: step 2230, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:08m:09s remains)
INFO - root - 2017-12-11 03:45:59.175787: step 2240, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:40m:04s remains)
INFO - root - 2017-12-11 03:46:04.542040: step 2250, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:59m:27s remains)
INFO - root - 2017-12-11 03:46:09.896982: step 2260, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:04m:49s remains)
INFO - root - 2017-12-11 03:46:15.191247: step 2270, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.514 sec/batch; 47h:09m:35s remains)
INFO - root - 2017-12-11 03:46:20.528465: step 2280, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 49h:02m:10s remains)
INFO - root - 2017-12-11 03:46:25.918863: step 2290, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:35m:40s remains)
INFO - root - 2017-12-11 03:46:31.248511: step 2300, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:27m:37s remains)
2017-12-11 03:46:31.827133: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0037539497 0.0039939811 0.0042797043 0.0046698828 0.0051545231 0.00573284 0.0063558221 0.006844006 0.0071344236 0.0072152535 0.0071165659 0.0068756538 0.0065045417 0.0060793036 0.0056556668][0.0040437765 0.004360877 0.0047325119 0.0052011576 0.0057464172 0.0063641923 0.0069990191 0.0074787913 0.0077107381 0.0076741697 0.0074121775 0.0069941059 0.0064356946 0.005856229 0.0053385897][0.0040607019 0.0044828597 0.00494896 0.0054922751 0.0060643023 0.0066670203 0.0072481139 0.0076451097 0.0077698808 0.0075996812 0.0071941908 0.0066422774 0.0059728408 0.0053158607 0.0047596651][0.00403201 0.00454088 0.0050743236 0.0056610405 0.006221632 0.0067589176 0.0072355275 0.0075151743 0.0075258082 0.0072642989 0.0068077794 0.006235301 0.005560264 0.0048920633 0.0043223947][0.00414466 0.0046921195 0.0052377386 0.00582024 0.0063239327 0.0067431857 0.0070668045 0.0072067827 0.0071060448 0.0067946054 0.006358848 0.0058451286 0.0052389423 0.0046034483 0.00403607][0.0045919991 0.0051084887 0.005590837 0.0060953917 0.0064919475 0.006778412 0.006964175 0.0069882385 0.006797635 0.0064573325 0.0060521392 0.0055936282 0.0050416468 0.004429033 0.0038518962][0.0052672885 0.0056673568 0.0059848703 0.0063250558 0.0065837493 0.0067723915 0.0068986551 0.0068738386 0.0066311858 0.0062786564 0.0058870218 0.0054319557 0.0048776618 0.0042534452 0.0036559012][0.0059287045 0.0061570792 0.0062423269 0.0063555264 0.006442036 0.0065404917 0.0066289208 0.006588262 0.00635633 0.0060457289 0.0056854682 0.0052230116 0.0046529784 0.0040166928 0.0034161469][0.0063499054 0.0064152051 0.0062801652 0.0061643738 0.0060598929 0.0060298387 0.0060372516 0.00598551 0.0058100969 0.0055779917 0.0052743885 0.004834854 0.0042890017 0.0036914144 0.003132612][0.0064280778 0.0063813743 0.0060998127 0.0058284942 0.0055923476 0.005474112 0.0054416209 0.0053930506 0.005246527 0.0050467029 0.004757416 0.0043195137 0.0038006022 0.0032661501 0.0027794363][0.0061956905 0.0061102654 0.0057822536 0.0054655639 0.00520226 0.0050781635 0.0050482983 0.0049959049 0.0048312447 0.0045968876 0.0042536426 0.0037752278 0.0032574034 0.0027770472 0.0023813534][0.0057652867 0.0057131895 0.0054245135 0.0051577096 0.004946168 0.0048528234 0.0048152171 0.0047243605 0.0045077661 0.00420525 0.0037927725 0.0032735956 0.0027571707 0.0023298818 0.0020292043][0.0053888713 0.0054092915 0.005197078 0.0050029634 0.0048344987 0.0047150156 0.0045941393 0.0044056303 0.0041126581 0.003750568 0.003313943 0.0028137863 0.0023490381 0.0020016027 0.001795389][0.0051655835 0.0052454141 0.0050857319 0.0049230643 0.0047373278 0.0045233388 0.0042725718 0.0039730631 0.003615079 0.0032296854 0.0028192205 0.00239787 0.0020362861 0.0017907025 0.0016704145][0.0050021964 0.0050844545 0.0049392949 0.0047612227 0.004515274 0.0041968711 0.0038385864 0.0034664543 0.0030837944 0.0027176517 0.0023685494 0.0020514992 0.0018036973 0.0016549049 0.0016048616]]...]
INFO - root - 2017-12-11 03:46:37.004177: step 2310, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 49h:55m:23s remains)
INFO - root - 2017-12-11 03:46:42.462723: step 2320, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:48m:35s remains)
INFO - root - 2017-12-11 03:46:47.864007: step 2330, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:07m:46s remains)
INFO - root - 2017-12-11 03:46:53.167681: step 2340, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:06m:45s remains)
INFO - root - 2017-12-11 03:46:58.499014: step 2350, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 49h:05m:24s remains)
INFO - root - 2017-12-11 03:47:03.816223: step 2360, loss = 0.75, batch loss = 0.69 (14.5 examples/sec; 0.550 sec/batch; 50h:28m:47s remains)
INFO - root - 2017-12-11 03:47:09.135952: step 2370, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:27m:19s remains)
INFO - root - 2017-12-11 03:47:14.552695: step 2380, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:13m:37s remains)
INFO - root - 2017-12-11 03:47:19.862551: step 2390, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 49h:06m:08s remains)
INFO - root - 2017-12-11 03:47:25.256984: step 2400, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:58m:33s remains)
2017-12-11 03:47:25.820958: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0042544864 0.0048051318 0.0053008245 0.005716214 0.0059794313 0.00609864 0.0060749259 0.0058936528 0.0056516752 0.0054499251 0.0053213849 0.0051739006 0.0049937037 0.0049415077 0.0050640167][0.0044672396 0.0051966086 0.0057995757 0.0062519019 0.0064792493 0.0065089851 0.0063404725 0.0060003428 0.0056481222 0.0054320414 0.0053712018 0.0053453315 0.0053154482 0.005361075 0.0054978854][0.0044013835 0.0052109607 0.0058399732 0.0062795249 0.0064344532 0.0063557061 0.0060839583 0.0056623328 0.0052793189 0.0050915196 0.0051227994 0.0052447543 0.0053736605 0.0055213063 0.0056447964][0.0044242069 0.0051493607 0.0057220748 0.0061075655 0.0061909314 0.0060309479 0.0056967279 0.0052470677 0.0048766052 0.0047208234 0.0048069255 0.0050345934 0.0052941414 0.0055346643 0.0056545031][0.0043653143 0.0049677468 0.0054527 0.0057892893 0.0058564362 0.0056979908 0.0053836689 0.0049795504 0.0046658777 0.0045433515 0.0046398621 0.0049062418 0.0052386248 0.0055440408 0.0056775734][0.0042126253 0.0046846322 0.0050731585 0.0053780712 0.0054829135 0.0054118838 0.0052166111 0.0049513159 0.0047485251 0.0046735848 0.0047511687 0.0049843686 0.0053083855 0.0056218486 0.0057734838][0.0040594442 0.0044158129 0.0047229496 0.0050099227 0.0051738485 0.0052224961 0.0051944773 0.0051192059 0.0050615296 0.0050533288 0.0051046694 0.0052567222 0.0054978696 0.0057510794 0.0058922819][0.0040297662 0.0043344465 0.0046052444 0.004888555 0.0050924094 0.0052231667 0.0053180754 0.0053789578 0.0054209158 0.0054557319 0.0054731048 0.0055269049 0.0056463359 0.00579865 0.0059022927][0.0041508465 0.004460156 0.0047258283 0.005005978 0.0052155391 0.0053622834 0.00549035 0.0055944077 0.0056693256 0.0057149022 0.0056996732 0.0056635956 0.0056512649 0.0056759943 0.0057102689][0.0043422631 0.0046468643 0.0049101459 0.0051849447 0.0053801243 0.0055035315 0.00560791 0.0056874314 0.0057301773 0.0057450584 0.005692794 0.0055862754 0.00546296 0.00536949 0.0053263991][0.0045133452 0.0048046955 0.0050548161 0.0053114216 0.005470485 0.00554196 0.0055773938 0.0055703023 0.0055252146 0.0054682423 0.0053720288 0.0052271374 0.0050530448 0.0049038311 0.00481601][0.004526787 0.0048039393 0.005025343 0.0052527366 0.0053684153 0.0053732074 0.0053135422 0.0051871319 0.0050336337 0.0048962673 0.0047683814 0.0046337503 0.004491739 0.0043787789 0.0043091872][0.0043415483 0.0045397659 0.004702447 0.0048909867 0.0049796677 0.0049473811 0.0048233094 0.0046113809 0.0043836841 0.0041967165 0.0040660505 0.00397862 0.003922388 0.0039067883 0.003904616][0.0039696265 0.0040358254 0.0041177766 0.0042715273 0.0043737628 0.0043784371 0.0042864871 0.0040909792 0.0038688157 0.0036794432 0.0035625596 0.0035151036 0.0035292213 0.0035958618 0.0036574982][0.0035700055 0.0035105539 0.0035098984 0.003637423 0.0037889432 0.0038980313 0.0039329072 0.0038534643 0.0037083456 0.0035399494 0.0034134048 0.0033535038 0.0033715044 0.0034643298 0.0035610092]]...]
INFO - root - 2017-12-11 03:47:30.955759: step 2410, loss = 0.75, batch loss = 0.69 (23.7 examples/sec; 0.337 sec/batch; 30h:54m:43s remains)
INFO - root - 2017-12-11 03:47:36.140506: step 2420, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 49h:49m:26s remains)
INFO - root - 2017-12-11 03:47:41.594530: step 2430, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.516 sec/batch; 47h:19m:29s remains)
INFO - root - 2017-12-11 03:47:46.935732: step 2440, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.525 sec/batch; 48h:05m:22s remains)
INFO - root - 2017-12-11 03:47:52.195477: step 2450, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:06m:18s remains)
INFO - root - 2017-12-11 03:47:57.630092: step 2460, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 49h:03m:45s remains)
INFO - root - 2017-12-11 03:48:03.066138: step 2470, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.546 sec/batch; 50h:01m:14s remains)
INFO - root - 2017-12-11 03:48:08.381831: step 2480, loss = 0.75, batch loss = 0.69 (15.8 examples/sec; 0.507 sec/batch; 46h:29m:07s remains)
INFO - root - 2017-12-11 03:48:13.704184: step 2490, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:46m:55s remains)
INFO - root - 2017-12-11 03:48:19.075308: step 2500, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:05m:56s remains)
2017-12-11 03:48:19.662637: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0030939805 0.0033082615 0.00340148 0.0034184717 0.0033725938 0.0033077474 0.0032491863 0.0031571013 0.0030357304 0.0029247098 0.0028291564 0.0027243476 0.0026020338 0.0024803225 0.002323925][0.0043992447 0.0048164381 0.00506579 0.0051942524 0.005191314 0.0051248618 0.0050349152 0.0048451493 0.0045786267 0.0043441881 0.0041735857 0.0040174443 0.0038442705 0.0036733605 0.0034234254][0.0059865029 0.0066739 0.0071408125 0.0074270191 0.0074880775 0.007426884 0.0073015778 0.0069926642 0.0065377806 0.0061333808 0.0058653168 0.0056494069 0.0054182946 0.0051817126 0.0048160851][0.007635802 0.00862507 0.0093385475 0.009806755 0.0099618845 0.0099471919 0.0098345028 0.00944505 0.00881266 0.0082238065 0.0078524584 0.0075731874 0.00726365 0.0069314195 0.0064228727][0.0089830207 0.010223255 0.011156872 0.01179199 0.012048181 0.012090566 0.0120139 0.011590221 0.01081717 0.010048095 0.0095527638 0.0091968346 0.0088030715 0.0083749853 0.007742045][0.0097278655 0.011159399 0.012251076 0.013029522 0.013400126 0.013513998 0.013489846 0.013086755 0.012234871 0.011301151 0.010643283 0.010169647 0.0096826172 0.0091670444 0.008459188][0.010101853 0.011668256 0.012859675 0.013748989 0.014249421 0.014490683 0.014585248 0.014277532 0.013424707 0.012359023 0.011510214 0.010852585 0.010215747 0.0095921941 0.008835528][0.010120307 0.011785294 0.01302467 0.013971793 0.014575713 0.014927399 0.015139907 0.014964009 0.014240185 0.013219782 0.01233143 0.011588397 0.010838262 0.010097083 0.0092497645][0.0098433765 0.01156015 0.012788313 0.013723662 0.01433245 0.014672261 0.014888161 0.014790964 0.014297227 0.013555743 0.012870364 0.012221636 0.011457617 0.010624605 0.0096346084][0.0094186543 0.011124975 0.012282073 0.013138086 0.013644869 0.013841332 0.013920859 0.013817 0.013561623 0.013204097 0.012847774 0.01239619 0.011714328 0.01086957 0.009773626][0.0089773182 0.010587528 0.011597835 0.012286386 0.0126077 0.012605199 0.012519157 0.01237793 0.012322688 0.012324183 0.012304598 0.012069021 0.011505103 0.010714396 0.0096011991][0.0083663855 0.0097787762 0.010574548 0.011022644 0.011108116 0.010894482 0.010646279 0.010451083 0.010540769 0.010839003 0.011143021 0.011142601 0.010720085 0.010024791 0.0089807613][0.0072220233 0.0083385212 0.0088821836 0.0090898518 0.0089604817 0.0085668089 0.0081858374 0.0079591246 0.0081376862 0.0086158281 0.0091247959 0.009321331 0.009078525 0.0085437717 0.007662368][0.00570442 0.0064770812 0.0067886012 0.0068195611 0.0065645394 0.00609086 0.0056622862 0.0054331259 0.0055985432 0.0060471017 0.006531687 0.0067772698 0.0066861026 0.0063594817 0.0057474738][0.0041452791 0.0045956359 0.0047237626 0.0046575665 0.0043994454 0.0039899116 0.0036261117 0.0034291064 0.0035062742 0.0037729151 0.00406212 0.0042261072 0.0042129206 0.0040770113 0.003765763]]...]
INFO - root - 2017-12-11 03:48:24.960350: step 2510, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.559 sec/batch; 51h:15m:46s remains)
INFO - root - 2017-12-11 03:48:29.967206: step 2520, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.546 sec/batch; 50h:03m:48s remains)
INFO - root - 2017-12-11 03:48:35.286768: step 2530, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:48m:02s remains)
INFO - root - 2017-12-11 03:48:40.623281: step 2540, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:41m:36s remains)
INFO - root - 2017-12-11 03:48:45.934934: step 2550, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.546 sec/batch; 50h:01m:09s remains)
INFO - root - 2017-12-11 03:48:51.348154: step 2560, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:05m:53s remains)
INFO - root - 2017-12-11 03:48:56.719117: step 2570, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:24m:03s remains)
INFO - root - 2017-12-11 03:49:01.995878: step 2580, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.528 sec/batch; 48h:24m:00s remains)
INFO - root - 2017-12-11 03:49:07.277138: step 2590, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:49m:43s remains)
INFO - root - 2017-12-11 03:49:12.570317: step 2600, loss = 0.75, batch loss = 0.69 (15.7 examples/sec; 0.508 sec/batch; 46h:35m:30s remains)
2017-12-11 03:49:13.075544: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0038265453 0.0040416322 0.0042993883 0.0046244157 0.0049826251 0.0053596883 0.005762347 0.006112264 0.0063469433 0.0064036059 0.0063110413 0.0061925966 0.00614849 0.0062110354 0.0063303066][0.0040549645 0.0042474633 0.0044990373 0.004876093 0.0053035822 0.0057632132 0.0062397867 0.0066767246 0.0069955294 0.0071232328 0.0070603071 0.0069120475 0.0068007866 0.0067924238 0.0068901586][0.0041996236 0.0043615261 0.0046018488 0.0050263912 0.0055295839 0.0060655493 0.0065992074 0.007102563 0.0074672317 0.0076005361 0.0074865259 0.0072396565 0.0070101288 0.006890262 0.00693427][0.0045895991 0.00473385 0.0049925977 0.0054944404 0.0060988874 0.0067289411 0.0073132557 0.0078272419 0.0081219468 0.0081188343 0.0078377379 0.0074251615 0.0070519373 0.0068161814 0.0068117618][0.0051265154 0.0053401696 0.0056668897 0.00626573 0.0069772447 0.00770459 0.0083217211 0.0087727606 0.0088892207 0.00864507 0.0081371367 0.0075352569 0.0070235124 0.0066883615 0.0066534043][0.0058112452 0.0062437411 0.0067213573 0.0074011311 0.0081467442 0.0088746743 0.0094208382 0.0096955774 0.0095590316 0.0090392837 0.0083020981 0.0075235 0.0068883277 0.0064741317 0.0064133806][0.0064444421 0.0071460367 0.0077921976 0.0085087614 0.009206905 0.0098329112 0.01020863 0.010230979 0.009822838 0.009071202 0.0081835808 0.0073243873 0.00666535 0.0062691951 0.0062431842][0.0067528524 0.0076333317 0.0083659431 0.0090664383 0.0096852714 0.010190221 0.010392125 0.010183659 0.0095597859 0.0086726043 0.0077401795 0.006904155 0.0063233771 0.0060444381 0.00612247][0.0067552282 0.0076744338 0.008377905 0.0089978855 0.0095011275 0.0098704053 0.0099142669 0.009537044 0.0088113789 0.0079195974 0.0070572486 0.0063294931 0.0058955182 0.0057983287 0.0060246075][0.0065560662 0.0073716016 0.007949031 0.0084144725 0.0087320693 0.0089026978 0.0087741 0.0082963975 0.0075980928 0.0068420153 0.0061652157 0.0056211464 0.00537123 0.005454401 0.0057978891][0.0059881443 0.0066425744 0.0070550018 0.0073522604 0.0074764686 0.0074448483 0.007171147 0.006670767 0.0060904422 0.0055476031 0.0051091486 0.0047805579 0.004715743 0.004932214 0.0053240522][0.0050903456 0.005511947 0.0057552229 0.0059247632 0.0059344964 0.0058123791 0.00551483 0.0050958083 0.0046780608 0.0043441718 0.0041118278 0.0039680381 0.0040474613 0.004335789 0.0047148843][0.0041084187 0.00430644 0.0044114729 0.0045013675 0.0044804006 0.0043796874 0.0041727959 0.0039073611 0.0036582134 0.0034796535 0.0033742117 0.0033352482 0.0034773119 0.0037670117 0.00409033][0.0032379541 0.0032930577 0.0033238286 0.0033747165 0.003359576 0.0033075362 0.0032009566 0.003069421 0.0029475885 0.0028666703 0.0028284278 0.0028356148 0.0029820316 0.003231341 0.0034930306][0.002458656 0.00248264 0.0025149509 0.002569699 0.0025812455 0.0025722121 0.0025346691 0.0024858003 0.0024377289 0.0024088547 0.00240175 0.0024202915 0.0025307909 0.0027080656 0.0028963]]...]
INFO - root - 2017-12-11 03:49:18.453419: step 2610, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:12m:27s remains)
INFO - root - 2017-12-11 03:49:23.542382: step 2620, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.515 sec/batch; 47h:14m:07s remains)
INFO - root - 2017-12-11 03:49:28.873279: step 2630, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 49h:52m:38s remains)
INFO - root - 2017-12-11 03:49:34.229916: step 2640, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:48m:23s remains)
INFO - root - 2017-12-11 03:49:39.502744: step 2650, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.513 sec/batch; 47h:01m:26s remains)
INFO - root - 2017-12-11 03:49:44.815150: step 2660, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:12m:20s remains)
INFO - root - 2017-12-11 03:49:50.143250: step 2670, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:16m:10s remains)
INFO - root - 2017-12-11 03:49:55.410006: step 2680, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:40m:06s remains)
INFO - root - 2017-12-11 03:50:00.751100: step 2690, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:08m:32s remains)
INFO - root - 2017-12-11 03:50:06.171949: step 2700, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 48h:58m:05s remains)
2017-12-11 03:50:06.749705: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0087606991 0.0081539154 0.0072879652 0.0065540448 0.0060444269 0.0057568485 0.0055736681 0.0054199151 0.00533896 0.0053104633 0.005283481 0.0052241464 0.00515179 0.0050861491 0.0050444747][0.0092534376 0.0085943583 0.0076668561 0.0069188382 0.0064336509 0.0062189754 0.0061366693 0.00606833 0.0060263523 0.0060086767 0.0059650275 0.0058583366 0.0057255942 0.0056058271 0.005527759][0.009066199 0.0085053882 0.0076482072 0.006990077 0.0066172075 0.0065458356 0.0066163125 0.0066652978 0.0066744373 0.0066468394 0.0065423129 0.006331719 0.0060817911 0.0058656512 0.0057222024][0.00883939 0.0083565144 0.0076336646 0.0071437876 0.0069594551 0.0070956559 0.0073653036 0.0075617433 0.0076228669 0.0075559332 0.0073447134 0.0069837649 0.006577617 0.0062476359 0.0060428907][0.0090142963 0.0086208535 0.0080208359 0.0076858667 0.0076975329 0.0080714328 0.0085792253 0.0089528281 0.0090865232 0.008996468 0.008686 0.0081709046 0.0075950632 0.0071551283 0.0069127693][0.0098467907 0.0095883431 0.0090404879 0.0087680472 0.0088981725 0.0094499923 0.010157086 0.010675677 0.010872818 0.010793511 0.010446974 0.0098475916 0.0091626542 0.0086640585 0.0084240343][0.010902327 0.010743001 0.010209064 0.0099511994 0.010133502 0.010757844 0.011539114 0.012100416 0.012321106 0.01228701 0.011996471 0.01144843 0.010799501 0.010354005 0.010182911][0.011633601 0.011497381 0.010968138 0.01071401 0.010904505 0.011516703 0.012266036 0.012791286 0.013013561 0.013056167 0.012896739 0.012519768 0.012045057 0.011755991 0.011721659][0.011718101 0.011624858 0.011127604 0.010874952 0.01102009 0.011528542 0.012148063 0.012577481 0.012789691 0.012937491 0.01297817 0.012877393 0.01269619 0.012650206 0.012809751][0.011271771 0.011243599 0.010812121 0.010577681 0.010660336 0.011028346 0.011485874 0.011794934 0.011977309 0.012213181 0.012449884 0.012635647 0.012759364 0.01294415 0.01325519][0.010557052 0.010645043 0.010355904 0.010205418 0.010272968 0.010539002 0.010860824 0.011053512 0.011164938 0.011390157 0.011701658 0.012061093 0.012385468 0.012671228 0.012970705][0.0098048234 0.010052693 0.009999847 0.010032612 0.010190596 0.010465576 0.010756136 0.010894815 0.010911347 0.011003954 0.011209977 0.011539984 0.011870231 0.012094734 0.012262065][0.0093783261 0.0097737378 0.0099612921 0.010223409 0.010554062 0.010943471 0.01131992 0.011493357 0.011435288 0.011340949 0.011319283 0.011435577 0.011562811 0.011572213 0.011531817][0.0094197178 0.0098736892 0.010202018 0.010613553 0.011064648 0.011543605 0.012009192 0.012243317 0.012156701 0.011925446 0.011688234 0.011538716 0.011372835 0.011108779 0.010844378][0.009769382 0.010208375 0.010527128 0.0109147 0.011317385 0.011737706 0.012183077 0.012441093 0.012366689 0.012095022 0.01174499 0.011407359 0.011002824 0.010515114 0.010081889]]...]
INFO - root - 2017-12-11 03:50:12.110347: step 2710, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 50h:08m:38s remains)
INFO - root - 2017-12-11 03:50:17.334138: step 2720, loss = 0.75, batch loss = 0.69 (21.1 examples/sec; 0.379 sec/batch; 34h:43m:00s remains)
INFO - root - 2017-12-11 03:50:22.544538: step 2730, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:35m:40s remains)
INFO - root - 2017-12-11 03:50:27.947941: step 2740, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:52m:44s remains)
INFO - root - 2017-12-11 03:50:33.364004: step 2750, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:33m:40s remains)
INFO - root - 2017-12-11 03:50:38.820923: step 2760, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:57m:04s remains)
INFO - root - 2017-12-11 03:50:44.142189: step 2770, loss = 0.75, batch loss = 0.69 (15.9 examples/sec; 0.503 sec/batch; 46h:06m:17s remains)
INFO - root - 2017-12-11 03:50:49.450518: step 2780, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:11m:34s remains)
INFO - root - 2017-12-11 03:50:54.854604: step 2790, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:38m:09s remains)
INFO - root - 2017-12-11 03:51:00.179779: step 2800, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:38m:34s remains)
2017-12-11 03:51:00.750593: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016608022 0.016978556 0.016860271 0.016526455 0.016326744 0.016529698 0.017063394 0.017656136 0.018009614 0.017724629 0.016431233 0.014071953 0.011104312 0.0081280423 0.00557196][0.017826108 0.018409612 0.018363617 0.018077595 0.01779657 0.017903198 0.018344419 0.018806668 0.019032354 0.018601039 0.017206592 0.014719289 0.011586205 0.0084712943 0.0058268686][0.017672813 0.018469589 0.01861654 0.018495299 0.018261312 0.018342204 0.018713146 0.019034538 0.019046022 0.018441465 0.016985204 0.014508059 0.011411575 0.0083712926 0.0058160741][0.017044943 0.017926157 0.018308517 0.018491814 0.018461384 0.018654149 0.01905499 0.019271366 0.019006265 0.018134717 0.016564028 0.014112994 0.011111105 0.0082034916 0.0057849535][0.016594602 0.017316984 0.017875511 0.018440876 0.018758122 0.019217428 0.019765195 0.019947382 0.019402657 0.018184479 0.016398985 0.013901548 0.01095876 0.0081467191 0.005832396][0.016310131 0.016857214 0.0175162 0.018446172 0.01917127 0.020006517 0.020826973 0.021130262 0.020448495 0.018951874 0.016924905 0.014269846 0.011235175 0.0083632628 0.0060250885][0.016166551 0.016572624 0.017299315 0.018527107 0.019638307 0.020886248 0.022037495 0.022536954 0.021845518 0.020186977 0.017937222 0.015017789 0.011747143 0.008691431 0.0062368186][0.016395064 0.01665928 0.017426327 0.018838048 0.020244736 0.021820527 0.023211883 0.023816274 0.023073841 0.021252437 0.018764444 0.015561809 0.012055933 0.0088429684 0.0062981448][0.017041527 0.01738039 0.018184148 0.019601749 0.021050202 0.022654394 0.024027117 0.024554022 0.023670094 0.02168012 0.019007012 0.015639435 0.012037395 0.0087872408 0.0062322868][0.017963208 0.018693738 0.019609768 0.020882919 0.022067288 0.023296097 0.024254965 0.024430279 0.023323594 0.021248924 0.018565133 0.015241457 0.011740777 0.00859733 0.0061129322][0.018856762 0.020065883 0.021088567 0.022104846 0.022848973 0.023467259 0.023782136 0.023454359 0.022116821 0.020082338 0.017588174 0.014522466 0.011285811 0.008347908 0.0059841014][0.019611668 0.020955244 0.021890871 0.022568075 0.022892099 0.022972958 0.022740968 0.021985864 0.020503834 0.018597202 0.016389867 0.013684567 0.01079258 0.0081083588 0.005877994][0.0194208 0.020609392 0.021282978 0.02162382 0.021655872 0.02140714 0.020860441 0.019893987 0.018419152 0.016724246 0.014866386 0.01260815 0.010152619 0.00779903 0.0057504703][0.017940097 0.018900914 0.019264963 0.019330598 0.019173108 0.018736847 0.01803996 0.017062344 0.015793763 0.014461039 0.013061945 0.011342759 0.0093988618 0.0074320743 0.0056012203][0.015476473 0.016157981 0.016250005 0.016136907 0.01593579 0.01552883 0.014932845 0.014189479 0.013314445 0.012441147 0.011505126 0.010259056 0.0087486869 0.0071094651 0.0054641333]]...]
INFO - root - 2017-12-11 03:51:06.009326: step 2810, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 50h:05m:56s remains)
INFO - root - 2017-12-11 03:51:11.425057: step 2820, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.532 sec/batch; 48h:42m:48s remains)
INFO - root - 2017-12-11 03:51:16.584077: step 2830, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:11m:24s remains)
INFO - root - 2017-12-11 03:51:21.943259: step 2840, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:05m:33s remains)
INFO - root - 2017-12-11 03:51:27.338049: step 2850, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 47h:58m:08s remains)
INFO - root - 2017-12-11 03:51:32.777367: step 2860, loss = 0.75, batch loss = 0.69 (13.6 examples/sec; 0.590 sec/batch; 53h:59m:47s remains)
INFO - root - 2017-12-11 03:51:38.108000: step 2870, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.518 sec/batch; 47h:24m:22s remains)
INFO - root - 2017-12-11 03:51:43.514510: step 2880, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:50m:21s remains)
INFO - root - 2017-12-11 03:51:48.969051: step 2890, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:46m:54s remains)
INFO - root - 2017-12-11 03:51:54.430097: step 2900, loss = 0.75, batch loss = 0.69 (13.7 examples/sec; 0.585 sec/batch; 53h:34m:24s remains)
2017-12-11 03:51:54.990055: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017530549 0.018295266 0.018622721 0.018920926 0.018996235 0.018863084 0.018514156 0.018302923 0.0183959 0.018680129 0.018961499 0.019046074 0.019099213 0.019136088 0.019034235][0.022897067 0.023754854 0.024031904 0.024244176 0.024292875 0.024196642 0.02390933 0.023715215 0.023854971 0.024323933 0.024789358 0.024941061 0.024895405 0.024751786 0.024426538][0.027200362 0.028110111 0.028294934 0.028451238 0.028503617 0.028540859 0.028418802 0.028277589 0.028420206 0.029001931 0.029663488 0.029872712 0.029644703 0.029205224 0.028573273][0.029917149 0.030919239 0.031050466 0.031196881 0.031337637 0.031635042 0.031806361 0.031790733 0.0319426 0.03262179 0.033445016 0.033643145 0.033161823 0.032375384 0.031435367][0.029834863 0.030872494 0.030939596 0.031133322 0.031525385 0.03234126 0.033138361 0.033594385 0.033982951 0.034803368 0.035687912 0.035770252 0.035002429 0.033936128 0.032865871][0.026937444 0.027926261 0.027936416 0.02829287 0.029134203 0.030715238 0.032440279 0.033722352 0.034625668 0.035607286 0.036396679 0.036258377 0.035252661 0.034103058 0.033218659][0.022873905 0.023820866 0.023859208 0.02441689 0.025699791 0.027956642 0.030478604 0.032531142 0.033927139 0.034945581 0.035458557 0.035029024 0.033921976 0.032924119 0.032508884][0.019314677 0.020152491 0.020300977 0.020986367 0.022488035 0.025062749 0.027969582 0.030475875 0.032168083 0.033104524 0.0332318 0.0324617 0.031318448 0.030556388 0.030661263][0.016805604 0.017495858 0.017719023 0.018405931 0.019864433 0.022362696 0.025294146 0.02804237 0.029990872 0.030978734 0.030871408 0.029846329 0.028630259 0.027989084 0.028419508][0.01591892 0.016420178 0.01661706 0.017155817 0.018297367 0.020345988 0.022982948 0.025745194 0.027868368 0.029010473 0.028912287 0.027835235 0.026524229 0.025825163 0.026302755][0.016722059 0.017101219 0.017192293 0.017445058 0.018046945 0.019359982 0.021334862 0.02363424 0.025469601 0.026461164 0.02632889 0.025280258 0.023968583 0.023250379 0.023712881][0.018457042 0.018672356 0.018526139 0.018358683 0.018338673 0.018886659 0.020079641 0.021618478 0.022755738 0.023225684 0.022851171 0.02178159 0.020562325 0.019971127 0.020513421][0.020026989 0.020016799 0.019575693 0.01896994 0.018375738 0.018301064 0.018875554 0.019781623 0.020318381 0.020319471 0.019739587 0.018707132 0.017649606 0.017219249 0.017804254][0.020516397 0.020465953 0.01979791 0.018837938 0.017745744 0.01711861 0.017188946 0.017694939 0.01798505 0.017898915 0.017462678 0.016787343 0.016104421 0.015877444 0.0164303][0.019997185 0.020058163 0.019253476 0.018044313 0.016578309 0.015493971 0.015143074 0.015396858 0.015688645 0.015796479 0.015764305 0.015687453 0.015577726 0.015649987 0.016164506]]...]
INFO - root - 2017-12-11 03:52:00.361812: step 2910, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.559 sec/batch; 51h:08m:54s remains)
INFO - root - 2017-12-11 03:52:05.649989: step 2920, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.524 sec/batch; 48h:00m:03s remains)
INFO - root - 2017-12-11 03:52:10.668133: step 2930, loss = 0.75, batch loss = 0.69 (15.6 examples/sec; 0.514 sec/batch; 47h:04m:30s remains)
INFO - root - 2017-12-11 03:52:16.085059: step 2940, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:31m:05s remains)
INFO - root - 2017-12-11 03:52:21.406320: step 2950, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.533 sec/batch; 48h:47m:09s remains)
INFO - root - 2017-12-11 03:52:26.684196: step 2960, loss = 0.75, batch loss = 0.69 (15.5 examples/sec; 0.516 sec/batch; 47h:11m:57s remains)
INFO - root - 2017-12-11 03:52:32.149180: step 2970, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.546 sec/batch; 49h:59m:13s remains)
INFO - root - 2017-12-11 03:52:37.511629: step 2980, loss = 0.75, batch loss = 0.69 (14.3 examples/sec; 0.558 sec/batch; 51h:04m:09s remains)
INFO - root - 2017-12-11 03:52:42.855885: step 2990, loss = 0.75, batch loss = 0.69 (15.3 examples/sec; 0.521 sec/batch; 47h:42m:26s remains)
INFO - root - 2017-12-11 03:52:48.241835: step 3000, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:13m:16s remains)
2017-12-11 03:52:48.740291: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026420206 0.024474889 0.021584244 0.019823117 0.019938352 0.021797957 0.024506494 0.026779866 0.027788758 0.027525125 0.026052309 0.023672035 0.021124238 0.01881914 0.016878551][0.027686542 0.025970312 0.023032388 0.021135395 0.021117913 0.022890406 0.025568098 0.02781718 0.028695341 0.028323835 0.026759779 0.024384188 0.021878425 0.019637061 0.017793301][0.02837697 0.027013406 0.024260174 0.022321586 0.022075487 0.023393568 0.025468443 0.027123867 0.027568331 0.02701216 0.02550338 0.023427991 0.021313496 0.01941905 0.017847789][0.029538894 0.028820273 0.02670731 0.025111899 0.024808744 0.0255773 0.026729686 0.027415479 0.027139984 0.026206983 0.024643628 0.022737684 0.020849921 0.019129075 0.017699067][0.030999715 0.031457547 0.030607862 0.0298878 0.029862542 0.030299867 0.030676538 0.030454729 0.029436952 0.0279989 0.026154993 0.024039356 0.021916989 0.019951748 0.018357815][0.031783696 0.033845626 0.034733888 0.035388734 0.036102556 0.036734298 0.036923487 0.036260422 0.034743119 0.032808069 0.030517019 0.027882418 0.025214568 0.022844398 0.021061117][0.030990152 0.034546718 0.037126031 0.039241184 0.040938411 0.042180706 0.042709924 0.042091593 0.04039745 0.03811343 0.03540073 0.032275617 0.029193245 0.026702337 0.025072409][0.028064046 0.032429002 0.036133777 0.039283808 0.041755233 0.043578975 0.044539012 0.04420086 0.042687949 0.04052148 0.037911143 0.034889482 0.032026432 0.030011959 0.029014135][0.023230745 0.027598396 0.03159333 0.035055362 0.037784018 0.039846476 0.0410558 0.041085038 0.040168162 0.038790368 0.037071116 0.034975052 0.03307474 0.032148644 0.032237805][0.017568268 0.021294678 0.024788901 0.02786332 0.030326623 0.032282539 0.033619661 0.03414065 0.034132395 0.034014259 0.033809662 0.0333085 0.032958273 0.033570133 0.0349985][0.012435341 0.015129552 0.017648809 0.019900667 0.021771196 0.023396717 0.024746493 0.025735013 0.026619626 0.027778924 0.029204892 0.030491414 0.031898621 0.034153283 0.036858633][0.008681057 0.010342261 0.011816636 0.013142092 0.014293893 0.015453422 0.01667111 0.017973386 0.019547803 0.021724755 0.024488609 0.027271973 0.030131985 0.033606954 0.03709894][0.0064201909 0.0072797588 0.0079547353 0.0085519189 0.0091049839 0.0098335519 0.010879541 0.012355742 0.014390637 0.017232187 0.020834275 0.024504691 0.028122528 0.032045003 0.035648126][0.0053378269 0.0057502319 0.0059678084 0.0061486019 0.0063534342 0.00680853 0.0077258339 0.0092570428 0.011487414 0.014558988 0.018370045 0.022199759 0.025818447 0.029476961 0.032667864][0.0049952012 0.0052290112 0.005258597 0.0052686143 0.0053195446 0.0056077209 0.0063592554 0.0077363532 0.0098044882 0.012628883 0.016068211 0.019478688 0.022597134 0.025595399 0.028122144]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 03:52:54.092967: step 3010, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.545 sec/batch; 49h:50m:41s remains)
INFO - root - 2017-12-11 03:52:59.445999: step 3020, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.554 sec/batch; 50h:43m:59s remains)
INFO - root - 2017-12-11 03:53:04.516767: step 3030, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.544 sec/batch; 49h:45m:18s remains)
INFO - root - 2017-12-11 03:53:09.867209: step 3040, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:31m:52s remains)
INFO - root - 2017-12-11 03:53:15.158195: step 3050, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:27m:23s remains)
INFO - root - 2017-12-11 03:53:20.558823: step 3060, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:01m:48s remains)
INFO - root - 2017-12-11 03:53:25.961139: step 3070, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:37m:50s remains)
INFO - root - 2017-12-11 03:53:31.343197: step 3080, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:49m:21s remains)
INFO - root - 2017-12-11 03:53:36.672937: step 3090, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:31m:15s remains)
INFO - root - 2017-12-11 03:53:42.037731: step 3100, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.545 sec/batch; 49h:52m:11s remains)
2017-12-11 03:53:42.574500: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022989068 0.023225924 0.02318052 0.02331274 0.02348754 0.024215188 0.025759641 0.027763126 0.028954353 0.028584221 0.027329097 0.025552286 0.024074549 0.023298943 0.023123123][0.023789123 0.024130749 0.02402699 0.024073331 0.024162425 0.024828972 0.026325643 0.028246652 0.029512376 0.029348623 0.028025258 0.026001234 0.024061356 0.022756603 0.022212591][0.022925172 0.023449736 0.023432594 0.023511678 0.023722619 0.024531076 0.02603453 0.027880173 0.02918034 0.029242883 0.028061461 0.025883783 0.023463827 0.02152935 0.020465475][0.022425035 0.023474637 0.023815367 0.024107378 0.024637979 0.025771074 0.027392734 0.029100994 0.030202234 0.030171964 0.028773634 0.026121594 0.022973983 0.020256214 0.018573791][0.022589989 0.024366327 0.02527192 0.025881272 0.026762437 0.028149348 0.029770939 0.031181127 0.031878304 0.031521611 0.029732896 0.026551275 0.022730917 0.019331587 0.01709901][0.02324442 0.025979441 0.027577909 0.028600568 0.029824307 0.0313438 0.032819193 0.033820748 0.033984415 0.033129659 0.030880848 0.027256072 0.022965128 0.019079782 0.016407963][0.023946894 0.02738479 0.029584479 0.030985279 0.032450035 0.033938356 0.035099123 0.035601564 0.035175942 0.033781771 0.031161562 0.027351242 0.022981627 0.019020192 0.016218442][0.023791213 0.027516099 0.029941153 0.031418022 0.032796122 0.033968996 0.034619812 0.034554742 0.03360419 0.031868864 0.029214993 0.025644857 0.021695673 0.018198766 0.015757727][0.022541508 0.025882402 0.027989741 0.029150814 0.030094976 0.030708503 0.030754047 0.030196186 0.0289757 0.027264675 0.025026498 0.022203336 0.019196056 0.016641865 0.014952011][0.02022868 0.022848021 0.024316799 0.025015047 0.025451139 0.025528412 0.025104402 0.024283085 0.023087161 0.021692973 0.020150332 0.01835632 0.016545773 0.015118496 0.014294799][0.017631989 0.019548396 0.020467341 0.020855349 0.020999311 0.020825123 0.020203596 0.019314904 0.018270032 0.017269427 0.016394718 0.015504016 0.014712122 0.014222634 0.014105695][0.015227495 0.016660659 0.017309928 0.017648896 0.017792696 0.017705312 0.017219612 0.016487718 0.015667504 0.014981308 0.014513511 0.014128173 0.013884063 0.013857117 0.014057492][0.013708402 0.01490001 0.015540842 0.016022926 0.016345235 0.016530275 0.016386246 0.01597501 0.015404071 0.014871091 0.014481789 0.014169525 0.01396745 0.013876736 0.013936789][0.013693238 0.014881126 0.015674131 0.01636463 0.016874127 0.017272705 0.017374389 0.017174264 0.016685782 0.016055919 0.015445862 0.014916066 0.014489304 0.014096667 0.013823893][0.014872415 0.016241943 0.017179951 0.017979611 0.018499481 0.018851804 0.01890647 0.018667443 0.018080084 0.017237287 0.016365649 0.015614564 0.014997933 0.014378425 0.013853942]]...]
INFO - root - 2017-12-11 03:53:47.900103: step 3110, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:31m:28s remains)
INFO - root - 2017-12-11 03:53:53.174859: step 3120, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.535 sec/batch; 48h:57m:57s remains)
INFO - root - 2017-12-11 03:53:58.565421: step 3130, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:33m:00s remains)
INFO - root - 2017-12-11 03:54:03.736324: step 3140, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:29m:58s remains)
INFO - root - 2017-12-11 03:54:09.116413: step 3150, loss = 0.75, batch loss = 0.69 (15.0 examples/sec; 0.535 sec/batch; 48h:54m:26s remains)
INFO - root - 2017-12-11 03:54:14.425825: step 3160, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:07m:46s remains)
INFO - root - 2017-12-11 03:54:19.789184: step 3170, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:05m:04s remains)
INFO - root - 2017-12-11 03:54:25.183606: step 3180, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.541 sec/batch; 49h:29m:28s remains)
INFO - root - 2017-12-11 03:54:30.415078: step 3190, loss = 0.75, batch loss = 0.69 (15.4 examples/sec; 0.520 sec/batch; 47h:35m:17s remains)
INFO - root - 2017-12-11 03:54:35.812599: step 3200, loss = 0.75, batch loss = 0.69 (15.1 examples/sec; 0.530 sec/batch; 48h:28m:42s remains)
2017-12-11 03:54:36.353093: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0079019889 0.0099718245 0.012077389 0.013969306 0.015303827 0.016057119 0.016272746 0.016109819 0.015656089 0.015080826 0.014535654 0.014038255 0.013583497 0.013108063 0.012481154][0.01145763 0.014957349 0.018515797 0.021744613 0.024146698 0.025744058 0.02652872 0.02662644 0.026135223 0.025376607 0.024537161 0.023601061 0.02263543 0.021686848 0.020608267][0.01541332 0.020720271 0.026147388 0.031096829 0.034916416 0.037667286 0.039240349 0.039713632 0.039272405 0.038529567 0.037598755 0.036305971 0.034709476 0.033031121 0.03122675][0.019557362 0.026922785 0.034485623 0.041311249 0.0465935 0.0504074 0.052488733 0.052897654 0.052099116 0.051190477 0.050149072 0.04855961 0.046395641 0.044063248 0.041633997][0.02277977 0.031856071 0.041307203 0.049872089 0.056582157 0.06147382 0.064088762 0.064354062 0.063018464 0.061720867 0.06041887 0.058459606 0.055698916 0.052718006 0.049713917][0.024327543 0.034416839 0.045081522 0.054843508 0.062685192 0.068617985 0.071974076 0.072381124 0.070750847 0.069226243 0.06787166 0.065804891 0.062621661 0.059003945 0.055322696][0.024362614 0.034882866 0.0461173 0.056370165 0.064745121 0.07130117 0.075193912 0.075692043 0.073858924 0.072264209 0.071050145 0.069150992 0.065809652 0.061753262 0.057503406][0.02346669 0.033998244 0.045274548 0.05531124 0.063464791 0.069963358 0.073826723 0.073958293 0.071642041 0.069838643 0.068773784 0.067218006 0.0640915 0.060112964 0.055790331][0.022064304 0.032109089 0.042838026 0.052103769 0.059509072 0.065482989 0.069012351 0.068693139 0.065976426 0.064070977 0.063280292 0.062256042 0.059649609 0.056138583 0.052033685][0.020308789 0.029456144 0.039186496 0.047389619 0.053828631 0.059099756 0.062319309 0.061937671 0.059435092 0.057946865 0.05775474 0.057448823 0.055477124 0.052468851 0.048537638][0.01850827 0.026697494 0.0352989 0.042318866 0.047680568 0.052136544 0.054987933 0.054724474 0.052791853 0.052068029 0.052612972 0.052960545 0.051497333 0.048857734 0.04510051][0.016676888 0.023998827 0.03156494 0.037539221 0.041953135 0.045611743 0.04798035 0.047719132 0.046227153 0.04607515 0.047043253 0.047685433 0.046552271 0.044260163 0.040812012][0.014617413 0.020958116 0.027461968 0.032564927 0.036212411 0.039077815 0.040843766 0.04048318 0.039269138 0.03937079 0.040429894 0.041144155 0.0403283 0.038501229 0.03560191][0.012215782 0.017310992 0.022589469 0.026883353 0.02991271 0.032068834 0.033283975 0.032992288 0.032148834 0.032276053 0.03302652 0.033494174 0.032847077 0.031422161 0.029099794][0.0096413782 0.013416926 0.017419651 0.020882078 0.02337309 0.025022985 0.025913205 0.025822496 0.025331605 0.025284955 0.025469275 0.025442105 0.024766656 0.023590093 0.021774473]]...]
INFO - root - 2017-12-11 03:54:41.721997: step 3210, loss = 0.75, batch loss = 0.69 (14.8 examples/sec; 0.542 sec/batch; 49h:35m:43s remains)
INFO - root - 2017-12-11 03:54:47.076028: step 3220, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.543 sec/batch; 49h:38m:15s remains)
INFO - root - 2017-12-11 03:54:52.499210: step 3230, loss = 0.74, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:07m:01s remains)
INFO - root - 2017-12-11 03:54:57.469248: step 3240, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.536 sec/batch; 49h:01m:09s remains)
INFO - root - 2017-12-11 03:55:02.940097: step 3250, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.547 sec/batch; 49h:59m:33s remains)
INFO - root - 2017-12-11 03:55:08.338009: step 3260, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:11m:36s remains)
INFO - root - 2017-12-11 03:55:13.620877: step 3270, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.528 sec/batch; 48h:15m:24s remains)
INFO - root - 2017-12-11 03:55:19.025285: step 3280, loss = 0.74, batch loss = 0.69 (15.1 examples/sec; 0.531 sec/batch; 48h:33m:16s remains)
INFO - root - 2017-12-11 03:55:24.353414: step 3290, loss = 0.74, batch loss = 0.69 (15.4 examples/sec; 0.521 sec/batch; 47h:37m:04s remains)
INFO - root - 2017-12-11 03:55:29.730286: step 3300, loss = 0.74, batch loss = 0.69 (15.2 examples/sec; 0.525 sec/batch; 48h:01m:26s remains)
2017-12-11 03:55:30.273379: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018517518 0.021888105 0.024134412 0.025231175 0.025464237 0.024942761 0.023724038 0.022287596 0.02114821 0.020511806 0.020323632 0.020587532 0.021275815 0.02210723 0.022687798][0.025631195 0.030975111 0.034884918 0.037299283 0.038547724 0.038464315 0.036900759 0.0345974 0.03246785 0.031069459 0.030433325 0.03071617 0.031909939 0.033465628 0.034510702][0.032800954 0.040353026 0.046391789 0.050789766 0.053784505 0.054727692 0.053156883 0.049974561 0.046533283 0.043862309 0.042290322 0.042320475 0.044013869 0.046459962 0.048033278][0.038883433 0.048553895 0.057024408 0.064027995 0.069449253 0.072002508 0.07084123 0.06694252 0.061962429 0.057497941 0.054485038 0.053979374 0.056076594 0.059288703 0.061153412][0.042589143 0.054064561 0.064979471 0.07488481 0.083164431 0.087859474 0.087671779 0.083433494 0.076864518 0.070283033 0.06551598 0.064255923 0.06649822 0.070021115 0.071630239][0.044040322 0.057045452 0.070184216 0.08269608 0.093557395 0.10027035 0.10106183 0.0965149 0.088398695 0.079883456 0.073632896 0.071685612 0.0737975 0.0770865 0.077964604][0.044044841 0.058433939 0.073437534 0.087782748 0.10023268 0.10798205 0.10891378 0.10354353 0.093965612 0.084199585 0.077320166 0.075149439 0.077022433 0.079676718 0.079652719][0.042930182 0.058296539 0.074494191 0.089581348 0.10217024 0.10950713 0.10948494 0.1030072 0.0926242 0.08288978 0.076653846 0.07497362 0.076704271 0.078517154 0.077558532][0.041074604 0.056862652 0.073434427 0.088175938 0.0994927 0.10487383 0.10277113 0.095113285 0.0849464 0.076727986 0.072374105 0.071915157 0.073810056 0.07496085 0.073319517][0.038511556 0.054025713 0.070105746 0.0835143 0.092467934 0.094860218 0.090149127 0.081499778 0.072379544 0.066633329 0.064903155 0.066152878 0.068605743 0.069596976 0.067850932][0.034999613 0.04932506 0.063912004 0.075134568 0.08121305 0.080642469 0.073908836 0.064952105 0.05735952 0.054106798 0.054790072 0.057657659 0.060878962 0.062200762 0.060861472][0.030212905 0.042185072 0.054083552 0.062514186 0.065967962 0.063593134 0.056524616 0.04858914 0.04291115 0.041580588 0.043759927 0.047572117 0.051255081 0.052868105 0.051891476][0.024510492 0.03328567 0.041683275 0.047149364 0.048650384 0.045843951 0.040015705 0.034192782 0.030592095 0.030508665 0.033152077 0.036932867 0.040326159 0.041764759 0.040891357][0.018767048 0.024340097 0.029288465 0.0321104 0.032322995 0.0299266 0.02599089 0.022468312 0.020622188 0.021093113 0.023333985 0.026244555 0.02873108 0.029717324 0.029023454][0.013312423 0.01621007 0.018490093 0.01950324 0.01914585 0.017580021 0.015494629 0.013853926 0.013191197 0.013690551 0.015073883 0.016737415 0.018099938 0.018594932 0.018199371]]...]
INFO - root - 2017-12-11 03:55:35.676281: step 3310, loss = 0.75, batch loss = 0.69 (14.6 examples/sec; 0.548 sec/batch; 50h:07m:26s remains)
INFO - root - 2017-12-11 03:55:41.041016: step 3320, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:10m:53s remains)
INFO - root - 2017-12-11 03:55:46.375079: step 3330, loss = 0.75, batch loss = 0.69 (15.2 examples/sec; 0.527 sec/batch; 48h:09m:31s remains)
INFO - root - 2017-12-11 03:55:51.465975: step 3340, loss = 0.74, batch loss = 0.69 (14.3 examples/sec; 0.561 sec/batch; 51h:19m:16s remains)
INFO - root - 2017-12-11 03:55:56.781147: step 3350, loss = 0.75, batch loss = 0.69 (14.4 examples/sec; 0.557 sec/batch; 50h:54m:19s remains)
INFO - root - 2017-12-11 03:56:02.139234: step 3360, loss = 0.74, batch loss = 0.69 (15.0 examples/sec; 0.534 sec/batch; 48h:50m:25s remains)
INFO - root - 2017-12-11 03:56:07.484752: step 3370, loss = 0.74, batch loss = 0.69 (14.9 examples/sec; 0.539 sec/batch; 49h:14m:31s remains)
INFO - root - 2017-12-11 03:56:12.819357: step 3380, loss = 0.74, batch loss = 0.68 (13.9 examples/sec; 0.577 sec/batch; 52h:46m:00s remains)
INFO - root - 2017-12-11 03:56:18.229221: step 3390, loss = 0.74, batch loss = 0.69 (14.5 examples/sec; 0.550 sec/batch; 50h:16m:36s remains)
INFO - root - 2017-12-11 03:56:23.555219: step 3400, loss = 0.74, batch loss = 0.68 (14.8 examples/sec; 0.539 sec/batch; 49h:17m:37s remains)
2017-12-11 03:56:24.122561: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074740157 0.075362593 0.0753981 0.07551416 0.075084604 0.074336991 0.072984681 0.0706294 0.068234332 0.066951387 0.067040168 0.067725651 0.068433329 0.069190055 0.070574336][0.078582071 0.078508057 0.077684358 0.076937184 0.076110169 0.07517378 0.073786885 0.071793079 0.070194952 0.070259355 0.07170587 0.073668152 0.075251654 0.076636553 0.078352869][0.07591936 0.076198205 0.075322896 0.074517623 0.073829591 0.0732114 0.072076842 0.070317015 0.069278389 0.070407949 0.073392995 0.076952659 0.07992176 0.082427174 0.08480449][0.071419612 0.072537377 0.072229736 0.07208553 0.072222 0.072447389 0.071838915 0.070274524 0.06930542 0.070646368 0.074187748 0.078511827 0.082303919 0.085694663 0.088903956][0.06945581 0.071580417 0.0723644 0.073351495 0.0746702 0.07596349 0.075880423 0.07417129 0.0724733 0.072926119 0.075602517 0.079090707 0.082140006 0.085163713 0.088525][0.072344847 0.075832658 0.077727988 0.079642341 0.081897706 0.084061466 0.084270522 0.082007267 0.078925543 0.077532865 0.0782101 0.07986071 0.081556581 0.083784357 0.086945571][0.075386 0.079926655 0.082590364 0.085234322 0.088356271 0.091352344 0.091993727 0.089442559 0.085126132 0.081626981 0.079849467 0.0795474 0.080264218 0.082133152 0.085400105][0.074951611 0.079812184 0.08282043 0.085972823 0.089694127 0.093216665 0.094284594 0.091841176 0.08701542 0.082277544 0.078819335 0.07719551 0.077282853 0.078819185 0.081947125][0.073452458 0.078035951 0.080683492 0.083567075 0.087031193 0.090288289 0.091325432 0.089093387 0.084641084 0.0801895 0.076615348 0.074694939 0.0744647 0.075453863 0.077697612][0.073443733 0.077884145 0.0800003 0.082080543 0.084483378 0.086579382 0.086894855 0.084547617 0.08071512 0.077319644 0.074615896 0.073289812 0.073440552 0.074460879 0.075997368][0.072815433 0.077731341 0.079922929 0.081732467 0.083289355 0.084081031 0.083463974 0.080824144 0.077504657 0.075037092 0.073293194 0.073004968 0.074214086 0.076104 0.077764392][0.070891909 0.075986072 0.078635208 0.080952577 0.082513034 0.082723223 0.081736095 0.079302922 0.076836981 0.075490788 0.074914314 0.07587482 0.078213654 0.080985941 0.08291173][0.070919074 0.075683363 0.078472309 0.081247836 0.083172455 0.083449528 0.082530931 0.080503069 0.078871138 0.078613229 0.079128571 0.080983177 0.083983667 0.087154359 0.089025065][0.072797105 0.076983482 0.079504348 0.082372457 0.084728859 0.085400775 0.08456327 0.082500964 0.080992483 0.081078425 0.08197996 0.084046178 0.08731997 0.090824611 0.092849][0.072132222 0.075622343 0.077728227 0.0805742 0.083383329 0.084521346 0.083726063 0.081437327 0.07968685 0.079630904 0.080472536 0.082378313 0.085597239 0.089282587 0.091638505]]...]
INFO - root - 2017-12-11 03:56:29.414906: step 3410, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.523 sec/batch; 47h:51m:14s remains)
INFO - root - 2017-12-11 03:56:34.794079: step 3420, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.537 sec/batch; 49h:04m:20s remains)
INFO - root - 2017-12-11 03:56:40.078891: step 3430, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.524 sec/batch; 47h:52m:38s remains)
INFO - root - 2017-12-11 03:56:45.269682: step 3440, loss = 0.75, batch loss = 0.69 (21.6 examples/sec; 0.370 sec/batch; 33h:47m:57s remains)
INFO - root - 2017-12-11 03:56:50.540598: step 3450, loss = 0.75, batch loss = 0.69 (14.7 examples/sec; 0.542 sec/batch; 49h:34m:55s remains)
INFO - root - 2017-12-11 03:56:55.839014: step 3460, loss = 0.74, batch loss = 0.68 (15.0 examples/sec; 0.532 sec/batch; 48h:39m:13s remains)
INFO - root - 2017-12-11 03:57:01.127536: step 3470, loss = 0.74, batch loss = 0.68 (14.6 examples/sec; 0.547 sec/batch; 50h:00m:43s remains)
INFO - root - 2017-12-11 03:57:06.434436: step 3480, loss = 0.74, batch loss = 0.68 (15.2 examples/sec; 0.526 sec/batch; 48h:06m:07s remains)
INFO - root - 2017-12-11 03:57:11.785888: step 3490, loss = 0.74, batch loss = 0.68 (14.5 examples/sec; 0.553 sec/batch; 50h:32m:10s remains)
INFO - root - 2017-12-11 03:57:17.182799: step 3500, loss = 0.74, batch loss = 0.68 (15.0 examples/sec; 0.532 sec/batch; 48h:39m:31s remains)
2017-12-11 03:57:17.724027: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075990848 0.079073079 0.080837473 0.082629323 0.084001638 0.084051639 0.083910637 0.083201647 0.082439944 0.081667848 0.080754086 0.080550626 0.0821264 0.085533179 0.091256186][0.0787172 0.082156107 0.08379259 0.085402593 0.086113907 0.085428908 0.084203169 0.082966372 0.081773989 0.080652192 0.079816207 0.080351874 0.083652943 0.0893137 0.0967049][0.0776297 0.08129593 0.083133176 0.085190132 0.086415485 0.085895427 0.084116854 0.08213938 0.080070607 0.078041144 0.076609038 0.077612773 0.082514405 0.089952394 0.098152563][0.075312361 0.079157263 0.081913672 0.085310474 0.087843604 0.0881632 0.086369321 0.083752759 0.080466852 0.076949634 0.074331969 0.075185396 0.080963276 0.089511827 0.097988911][0.069923148 0.074009307 0.078085 0.0832865 0.087643042 0.089452565 0.0883843 0.085532881 0.08107917 0.075828485 0.0714549 0.071167208 0.076525487 0.085098 0.093284957][0.061173279 0.065774374 0.071273625 0.078482777 0.084996939 0.088738084 0.088912494 0.086160652 0.080660984 0.073673755 0.067409247 0.065512083 0.069639236 0.077632144 0.085596986][0.050479282 0.055676002 0.062960856 0.072641969 0.081866674 0.087884054 0.089377671 0.086538613 0.079636753 0.070616856 0.062286817 0.0585463 0.061161112 0.068679787 0.077355191][0.039654627 0.045499079 0.054516256 0.066472381 0.078264393 0.086385854 0.088908866 0.085654527 0.077232532 0.066292822 0.05607662 0.050441921 0.051510423 0.058738537 0.068806216][0.031080309 0.037635207 0.047839116 0.061079871 0.074312821 0.083574422 0.086462118 0.0826911 0.07326436 0.061223626 0.049851332 0.042648606 0.042233877 0.048865184 0.05983422][0.026233986 0.033368841 0.043980248 0.057257872 0.070309393 0.079181351 0.081515089 0.077308394 0.067733444 0.055821575 0.044586789 0.036990777 0.035817564 0.041698053 0.052553385][0.025128148 0.032180492 0.042330038 0.054578815 0.066217072 0.0736221 0.074894384 0.07049074 0.061471675 0.050547641 0.040411815 0.033522762 0.032406811 0.037527174 0.047254056][0.025803087 0.032002989 0.040665872 0.050834537 0.060236756 0.065824509 0.066175953 0.062098172 0.054399669 0.045258384 0.036909722 0.03138195 0.0307559 0.034955081 0.042588089][0.026195191 0.031039812 0.037322596 0.044522244 0.05114441 0.054959163 0.054828029 0.051626682 0.045931395 0.039334133 0.033400696 0.029663412 0.029729277 0.032941826 0.0380632][0.02547027 0.028867561 0.032755084 0.037128579 0.041263413 0.043709323 0.043553092 0.041485492 0.037863661 0.033799455 0.030232154 0.028198507 0.028782859 0.031029828 0.033905815][0.023900053 0.02604126 0.028136902 0.030597128 0.03311478 0.034789126 0.034923259 0.033854365 0.031749655 0.029457353 0.02759451 0.026813637 0.027674772 0.02923977 0.03072584]]...]
INFO - root - 2017-12-11 03:57:23.036271: step 3510, loss = 0.74, batch loss = 0.68 (15.2 examples/sec; 0.527 sec/batch; 48h:08m:45s remains)
INFO - root - 2017-12-11 03:57:28.438279: step 3520, loss = 0.74, batch loss = 0.68 (15.6 examples/sec; 0.514 sec/batch; 46h:55m:38s remains)
INFO - root - 2017-12-11 03:57:33.827254: step 3530, loss = 0.74, batch loss = 0.68 (15.1 examples/sec; 0.530 sec/batch; 48h:24m:37s remains)
INFO - root - 2017-12-11 03:57:39.089220: step 3540, loss = 0.74, batch loss = 0.69 (15.2 examples/sec; 0.526 sec/batch; 48h:05m:17s remains)
INFO - root - 2017-12-11 03:57:43.660026: step 3550, loss = 0.74, batch loss = 0.68 (14.7 examples/sec; 0.542 sec/batch; 49h:33m:56s remains)
INFO - root - 2017-12-11 03:57:49.148181: step 3560, loss = 0.74, batch loss = 0.68 (14.5 examples/sec; 0.550 sec/batch; 50h:16m:07s remains)
INFO - root - 2017-12-11 03:57:54.410477: step 3570, loss = 0.74, batch loss = 0.68 (14.7 examples/sec; 0.544 sec/batch; 49h:43m:55s remains)
INFO - root - 2017-12-11 03:57:59.710224: step 3580, loss = 0.74, batch loss = 0.68 (15.5 examples/sec; 0.518 sec/batch; 47h:17m:26s remains)
INFO - root - 2017-12-11 03:58:05.051817: step 3590, loss = 0.74, batch loss = 0.68 (14.9 examples/sec; 0.536 sec/batch; 48h:56m:12s remains)
INFO - root - 2017-12-11 03:58:10.364310: step 3600, loss = 0.74, batch loss = 0.68 (15.5 examples/sec; 0.515 sec/batch; 47h:02m:54s remains)
2017-12-11 03:58:10.884497: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046438754 0.047969524 0.052140307 0.059978593 0.0702116 0.079352252 0.085175447 0.087862931 0.088294148 0.086830586 0.083295785 0.079187751 0.07539732 0.072100557 0.069176234][0.045940869 0.047167826 0.051113129 0.05974295 0.071106434 0.082169481 0.08991506 0.093555689 0.093560249 0.090397879 0.084552184 0.077946834 0.072741047 0.069120131 0.06684795][0.047447924 0.046838336 0.049238224 0.056973536 0.06836921 0.080541119 0.08984296 0.094370224 0.093980089 0.089100771 0.080831259 0.071768478 0.064956054 0.0610042 0.059658058][0.054885447 0.050846435 0.04997059 0.05537983 0.065864459 0.078423575 0.088802241 0.094009347 0.093384266 0.087265767 0.077096209 0.065876141 0.057543941 0.053314302 0.052791316][0.0644675 0.057445459 0.05313332 0.055744577 0.064700827 0.076858796 0.087514773 0.092914253 0.0919187 0.084804751 0.073192447 0.060323626 0.050737064 0.046215676 0.046253242][0.071894579 0.064743415 0.058835026 0.059591793 0.067122 0.078350775 0.088443637 0.093505815 0.091961659 0.08406356 0.071540624 0.057759084 0.047399078 0.04254033 0.042689417][0.074951269 0.069754787 0.064540222 0.064914823 0.071525611 0.081534959 0.090416223 0.09478946 0.092834324 0.084754728 0.072205983 0.058580462 0.048243869 0.043246873 0.04314144][0.07314229 0.0713017 0.06859675 0.070128955 0.076796055 0.085901439 0.093290478 0.096490972 0.093897462 0.085785985 0.07370539 0.060976572 0.051430307 0.046893433 0.046885721][0.067305468 0.069059543 0.069530472 0.072977632 0.080318086 0.088832065 0.094551496 0.096075766 0.0925033 0.084347047 0.073073067 0.061798166 0.053705741 0.050171722 0.050596442][0.058515862 0.063494019 0.066783987 0.072033249 0.079913169 0.08776319 0.091858707 0.091687091 0.087157682 0.079150766 0.069112524 0.059760641 0.053527758 0.051187295 0.051976435][0.05006953 0.056519117 0.061478481 0.0677843 0.075663045 0.082694836 0.08559528 0.084277652 0.079174064 0.071554437 0.062872231 0.0554014 0.050952747 0.049818378 0.05102405][0.045312427 0.05116576 0.056065422 0.062041007 0.068977714 0.0749193 0.077161074 0.075611085 0.070722125 0.06389676 0.05651404 0.050475389 0.047295444 0.04711514 0.048774321][0.043129735 0.047569294 0.051189311 0.055695217 0.060893886 0.065452881 0.067343868 0.066239074 0.062331691 0.056876469 0.050983593 0.046116728 0.043668598 0.043927509 0.045757677][0.040561102 0.043486796 0.045442354 0.04796068 0.050987978 0.053838547 0.055215374 0.054599781 0.051983345 0.048360821 0.044431146 0.041044079 0.039313868 0.03968545 0.041301493][0.036614139 0.038384162 0.039041627 0.039892223 0.041007448 0.042253856 0.04296596 0.042569626 0.040984146 0.039013565 0.036975939 0.035125479 0.034149315 0.0345384 0.035824791]]...]
INFO - root - 2017-12-11 03:58:16.187021: step 3610, loss = 0.74, batch loss = 0.68 (14.9 examples/sec; 0.536 sec/batch; 48h:58m:40s remains)
INFO - root - 2017-12-11 03:58:21.449140: step 3620, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.521 sec/batch; 47h:37m:33s remains)
INFO - root - 2017-12-11 03:58:26.834428: step 3630, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.523 sec/batch; 47h:44m:44s remains)
INFO - root - 2017-12-11 03:58:32.122168: step 3640, loss = 0.74, batch loss = 0.68 (14.9 examples/sec; 0.535 sec/batch; 48h:53m:31s remains)
INFO - root - 2017-12-11 03:58:37.150021: step 3650, loss = 0.74, batch loss = 0.68 (17.4 examples/sec; 0.459 sec/batch; 41h:57m:00s remains)
INFO - root - 2017-12-11 03:58:42.488936: step 3660, loss = 0.75, batch loss = 0.69 (14.9 examples/sec; 0.538 sec/batch; 49h:09m:52s remains)
INFO - root - 2017-12-11 03:58:47.857990: step 3670, loss = 0.74, batch loss = 0.68 (15.0 examples/sec; 0.532 sec/batch; 48h:33m:43s remains)
INFO - root - 2017-12-11 03:58:53.246547: step 3680, loss = 0.74, batch loss = 0.68 (15.4 examples/sec; 0.519 sec/batch; 47h:23m:18s remains)
INFO - root - 2017-12-11 03:58:58.589521: step 3690, loss = 0.74, batch loss = 0.68 (15.2 examples/sec; 0.527 sec/batch; 48h:09m:56s remains)
INFO - root - 2017-12-11 03:59:04.016529: step 3700, loss = 0.73, batch loss = 0.68 (14.8 examples/sec; 0.542 sec/batch; 49h:30m:22s remains)
2017-12-11 03:59:04.587278: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.093849145 0.10000783 0.1137445 0.13482329 0.15565944 0.16973044 0.17620538 0.1776979 0.17445424 0.1672212 0.1604097 0.15539281 0.15051053 0.14504007 0.14079963][0.11805904 0.12104638 0.13121863 0.14912373 0.16720487 0.17920914 0.18550318 0.18859483 0.18677351 0.18010332 0.17282392 0.16650032 0.15904716 0.15026635 0.14261115][0.13875605 0.13825338 0.14224076 0.15304382 0.16423661 0.17129546 0.17577204 0.17974424 0.17982128 0.17522654 0.16927502 0.16332737 0.15561958 0.14631942 0.13769574][0.15292026 0.15026361 0.14878602 0.15222329 0.15601072 0.15762281 0.15965955 0.16385451 0.16560188 0.16342127 0.15923665 0.15430158 0.14736183 0.13882871 0.13054518][0.15719242 0.15569766 0.15253565 0.15167128 0.15041746 0.14812241 0.1482047 0.15231422 0.15535569 0.15537356 0.15260114 0.14783402 0.14041795 0.13139232 0.122705][0.15293348 0.15612513 0.1558183 0.15492536 0.15214179 0.14820044 0.14709498 0.1504838 0.15353422 0.15431547 0.15152423 0.14515442 0.13509257 0.12336514 0.11274863][0.14068598 0.15048884 0.15564655 0.15781116 0.15631117 0.15242198 0.15051988 0.15237138 0.15399177 0.1539401 0.14991169 0.14132173 0.12827393 0.11356498 0.10069235][0.12052633 0.13572954 0.1461425 0.15200265 0.15280765 0.14978321 0.14742434 0.14766245 0.14763594 0.14655749 0.14185403 0.13254961 0.11845484 0.10254483 0.088522516][0.09418463 0.11101446 0.12442035 0.13289408 0.13577412 0.13401896 0.13181379 0.13112995 0.13018851 0.12877649 0.12467431 0.11664094 0.10390799 0.088920973 0.075142547][0.066504747 0.081911229 0.095231436 0.10431643 0.10817916 0.10748374 0.10573857 0.10462421 0.1032591 0.10190406 0.099050745 0.09341161 0.083697975 0.07144776 0.059438255][0.04461053 0.056382641 0.067149818 0.074825451 0.078244984 0.077752978 0.075980268 0.074171059 0.072252981 0.070907585 0.069350436 0.06639877 0.060463365 0.052162051 0.043326464][0.030524667 0.038292844 0.045518748 0.050782733 0.05300111 0.052233759 0.050160997 0.047657467 0.045206483 0.04372381 0.0430412 0.042257529 0.039769497 0.035502214 0.03036619][0.024142271 0.028714223 0.032617494 0.035270609 0.035968993 0.03470853 0.032377757 0.029568646 0.026966836 0.025439419 0.02515579 0.02551681 0.025253007 0.02394665 0.021863474][0.026237521 0.029038981 0.030636108 0.0310934 0.030235086 0.028242398 0.025584415 0.022624847 0.019995043 0.018391473 0.017991787 0.018436525 0.01899232 0.019197077 0.0189537][0.033888903 0.036387186 0.036857065 0.035854891 0.033688042 0.03085695 0.027781485 0.024710229 0.022142926 0.020572068 0.020002518 0.020191578 0.020789286 0.021437543 0.021920005]]...]
INFO - root - 2017-12-11 03:59:09.886532: step 3710, loss = 0.74, batch loss = 0.68 (15.2 examples/sec; 0.527 sec/batch; 48h:06m:01s remains)
INFO - root - 2017-12-11 03:59:15.268248: step 3720, loss = 0.74, batch loss = 0.68 (14.6 examples/sec; 0.547 sec/batch; 49h:57m:42s remains)
INFO - root - 2017-12-11 03:59:20.673749: step 3730, loss = 0.74, batch loss = 0.68 (15.4 examples/sec; 0.521 sec/batch; 47h:33m:58s remains)
INFO - root - 2017-12-11 03:59:25.929816: step 3740, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.522 sec/batch; 47h:39m:19s remains)
INFO - root - 2017-12-11 03:59:31.342617: step 3750, loss = 0.73, batch loss = 0.68 (14.6 examples/sec; 0.547 sec/batch; 49h:55m:15s remains)
INFO - root - 2017-12-11 03:59:36.361085: step 3760, loss = 0.74, batch loss = 0.68 (15.0 examples/sec; 0.532 sec/batch; 48h:33m:29s remains)
INFO - root - 2017-12-11 03:59:41.738909: step 3770, loss = 0.73, batch loss = 0.68 (14.5 examples/sec; 0.552 sec/batch; 50h:24m:48s remains)
INFO - root - 2017-12-11 03:59:47.065349: step 3780, loss = 0.73, batch loss = 0.67 (14.6 examples/sec; 0.547 sec/batch; 49h:54m:11s remains)
INFO - root - 2017-12-11 03:59:52.439197: step 3790, loss = 0.73, batch loss = 0.67 (15.0 examples/sec; 0.532 sec/batch; 48h:37m:06s remains)
INFO - root - 2017-12-11 03:59:57.767984: step 3800, loss = 0.74, batch loss = 0.68 (14.8 examples/sec; 0.540 sec/batch; 49h:17m:16s remains)
2017-12-11 03:59:58.300282: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036598068 0.042760707 0.047220938 0.049700245 0.050364465 0.049819361 0.048478425 0.047071122 0.046311621 0.046062354 0.045956515 0.045479275 0.044221085 0.041582245 0.037287623][0.051638387 0.063273273 0.072572768 0.0784205 0.08059404 0.080105439 0.077514686 0.07467211 0.073287219 0.07287012 0.072522767 0.071120687 0.068309784 0.063077979 0.055144005][0.066053212 0.085089661 0.10191263 0.11396679 0.11984383 0.120492 0.11646982 0.11114154 0.10792279 0.10637725 0.10529902 0.10253426 0.097611323 0.088928372 0.076300435][0.078209311 0.10512085 0.13085251 0.15114924 0.16274537 0.16576958 0.16041841 0.15162937 0.14489731 0.1407046 0.13823524 0.13410163 0.12733991 0.11540648 0.0981785][0.085424185 0.11879656 0.15282282 0.1819009 0.20085527 0.20830716 0.20341106 0.19204792 0.18151511 0.17400223 0.16962501 0.16380908 0.15510654 0.1399959 0.11841262][0.087571241 0.12532035 0.16579209 0.20295945 0.23042081 0.24519734 0.24455009 0.23372108 0.22048713 0.20893882 0.20088841 0.19142552 0.17920564 0.16021389 0.13448343][0.086574085 0.1264732 0.170304 0.21243519 0.24651402 0.26904574 0.275359 0.26872194 0.25557271 0.24072194 0.22803886 0.21330711 0.19598655 0.17254037 0.1435084][0.08458627 0.12418082 0.16751477 0.20963432 0.2454579 0.27221113 0.28433424 0.28328797 0.27315742 0.25783554 0.24209724 0.22281148 0.20054814 0.17316219 0.14237942][0.082864381 0.11983361 0.15905893 0.19657044 0.22904515 0.2548078 0.26858386 0.27132431 0.26516455 0.25246531 0.23710378 0.21644051 0.19176628 0.16252382 0.13199045][0.079424769 0.112046 0.14492404 0.1753159 0.20163615 0.22299065 0.2348032 0.23847258 0.23551798 0.22700703 0.21466148 0.19564329 0.17140524 0.14286725 0.11457139][0.072101034 0.09921059 0.12475313 0.14717841 0.166309 0.18174766 0.18946904 0.19162218 0.19017872 0.18542814 0.17710337 0.16181546 0.14063606 0.11532762 0.091157429][0.061078511 0.081925958 0.100064 0.11459726 0.12625349 0.13500974 0.13747595 0.1364944 0.13474464 0.13248681 0.12830289 0.11840285 0.10323659 0.084143184 0.065921761][0.04855248 0.063299522 0.075098775 0.083249912 0.088680029 0.0915231 0.089415655 0.085242666 0.081916645 0.080258764 0.078667149 0.073793687 0.065670617 0.054645129 0.043665264][0.035993926 0.045190543 0.052034508 0.055996884 0.057639394 0.057022326 0.052844513 0.047344163 0.043000989 0.04090637 0.039965991 0.03804709 0.0351115 0.03101415 0.026741009][0.024922581 0.029528394 0.032659497 0.034086954 0.034021828 0.032502849 0.028991209 0.024714138 0.021245457 0.019408138 0.018601473 0.017869009 0.017326588 0.016883884 0.016503451]]...]
INFO - root - 2017-12-11 04:00:03.701659: step 3810, loss = 0.74, batch loss = 0.68 (14.7 examples/sec; 0.542 sec/batch; 49h:31m:13s remains)
INFO - root - 2017-12-11 04:00:09.036428: step 3820, loss = 0.74, batch loss = 0.68 (15.3 examples/sec; 0.524 sec/batch; 47h:47m:52s remains)
INFO - root - 2017-12-11 04:00:14.351251: step 3830, loss = 0.73, batch loss = 0.67 (15.3 examples/sec; 0.522 sec/batch; 47h:42m:05s remains)
INFO - root - 2017-12-11 04:00:19.657113: step 3840, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.529 sec/batch; 48h:15m:44s remains)
INFO - root - 2017-12-11 04:00:25.021686: step 3850, loss = 0.73, batch loss = 0.68 (14.6 examples/sec; 0.549 sec/batch; 50h:05m:47s remains)
INFO - root - 2017-12-11 04:00:30.207159: step 3860, loss = 0.73, batch loss = 0.68 (14.2 examples/sec; 0.564 sec/batch; 51h:28m:15s remains)
INFO - root - 2017-12-11 04:00:35.536615: step 3870, loss = 0.74, batch loss = 0.68 (15.4 examples/sec; 0.521 sec/batch; 47h:34m:22s remains)
INFO - root - 2017-12-11 04:00:40.813497: step 3880, loss = 0.73, batch loss = 0.68 (15.4 examples/sec; 0.521 sec/batch; 47h:32m:21s remains)
INFO - root - 2017-12-11 04:00:46.155108: step 3890, loss = 0.74, batch loss = 0.68 (14.6 examples/sec; 0.547 sec/batch; 49h:57m:57s remains)
INFO - root - 2017-12-11 04:00:51.568423: step 3900, loss = 0.74, batch loss = 0.68 (15.1 examples/sec; 0.531 sec/batch; 48h:26m:38s remains)
2017-12-11 04:00:52.106939: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067702994 0.093738794 0.12256038 0.15090907 0.17240696 0.18585749 0.19190082 0.19095211 0.18441609 0.17609446 0.17268035 0.17284089 0.17406856 0.17535178 0.17719117][0.074228913 0.1036525 0.13657904 0.16885732 0.19350089 0.20764899 0.21255806 0.20977049 0.20126706 0.19093831 0.18579327 0.1852161 0.18565592 0.18546411 0.18598075][0.076072223 0.1074558 0.14294346 0.177465 0.20349933 0.21708448 0.22006831 0.21546891 0.20569462 0.1945806 0.18877891 0.18830301 0.18843555 0.18688932 0.18550268][0.075592905 0.10767873 0.14427531 0.17991476 0.20674409 0.21976441 0.22119902 0.21525386 0.20472251 0.19349921 0.18813361 0.18963654 0.19220273 0.19190179 0.19007362][0.072632082 0.10388307 0.14007951 0.17604719 0.20368065 0.21687345 0.21774586 0.211016 0.19975945 0.18803898 0.18287483 0.18643132 0.19224341 0.19427083 0.19255991][0.067892656 0.097250275 0.13197325 0.1675352 0.19570592 0.20965587 0.21121097 0.20462993 0.19304615 0.18053387 0.17458296 0.17801899 0.18462437 0.18745905 0.1856443][0.062021207 0.088418916 0.12023544 0.15376569 0.18104048 0.19542481 0.19853052 0.19349414 0.18315406 0.17115076 0.16471632 0.16654092 0.17149782 0.17331855 0.17104182][0.055864405 0.078454122 0.10580864 0.13520616 0.15946107 0.17294934 0.17740184 0.17500561 0.1681543 0.1593127 0.1543882 0.15523016 0.15816438 0.15834184 0.155157][0.050193261 0.068645589 0.090609856 0.1142445 0.13361296 0.14459988 0.14935598 0.14939553 0.14672886 0.14259031 0.14098747 0.14236319 0.14407636 0.14280537 0.13838345][0.045098864 0.05980194 0.076675564 0.094363242 0.10818995 0.11562558 0.11937812 0.1206614 0.12132492 0.12192933 0.12451556 0.12770854 0.12913218 0.12672447 0.12097862][0.041073158 0.052942578 0.065723211 0.078136235 0.086671725 0.090209812 0.09200345 0.093325883 0.096098766 0.10067979 0.1074201 0.11336361 0.11576809 0.11323772 0.10681652][0.03785393 0.047714423 0.057378236 0.065507025 0.069560885 0.069491625 0.068797231 0.069058351 0.072343484 0.079164281 0.088921852 0.097774938 0.10236556 0.10113664 0.095282353][0.034042008 0.042273317 0.0496269 0.054754741 0.055815142 0.053255558 0.050293721 0.048815474 0.05102824 0.057722528 0.068045042 0.078229085 0.084726274 0.085506849 0.081388563][0.02861012 0.035087183 0.040552404 0.043794092 0.043453798 0.040016882 0.035974152 0.033129822 0.033509795 0.03811609 0.046369098 0.055385657 0.062124636 0.064480849 0.062770084][0.023024464 0.027848054 0.031867385 0.034144174 0.033644486 0.030675938 0.026861805 0.023634717 0.022580124 0.024677198 0.029709142 0.035896715 0.041179035 0.043882117 0.044077311]]...]
INFO - root - 2017-12-11 04:00:57.416341: step 3910, loss = 0.74, batch loss = 0.68 (15.1 examples/sec; 0.531 sec/batch; 48h:26m:59s remains)
INFO - root - 2017-12-11 04:01:02.800580: step 3920, loss = 0.73, batch loss = 0.67 (15.5 examples/sec; 0.516 sec/batch; 47h:03m:03s remains)
INFO - root - 2017-12-11 04:01:08.261625: step 3930, loss = 0.74, batch loss = 0.68 (15.2 examples/sec; 0.527 sec/batch; 48h:07m:44s remains)
INFO - root - 2017-12-11 04:01:13.618173: step 3940, loss = 0.73, batch loss = 0.67 (15.3 examples/sec; 0.524 sec/batch; 47h:48m:49s remains)
INFO - root - 2017-12-11 04:01:18.876208: step 3950, loss = 0.73, batch loss = 0.67 (15.0 examples/sec; 0.534 sec/batch; 48h:43m:24s remains)
INFO - root - 2017-12-11 04:01:23.912097: step 3960, loss = 0.72, batch loss = 0.67 (14.9 examples/sec; 0.535 sec/batch; 48h:50m:10s remains)
INFO - root - 2017-12-11 04:01:29.346746: step 3970, loss = 0.73, batch loss = 0.68 (14.8 examples/sec; 0.540 sec/batch; 49h:18m:32s remains)
INFO - root - 2017-12-11 04:01:34.714501: step 3980, loss = 0.73, batch loss = 0.68 (15.0 examples/sec; 0.535 sec/batch; 48h:47m:32s remains)
INFO - root - 2017-12-11 04:01:40.079769: step 3990, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.538 sec/batch; 49h:04m:54s remains)
INFO - root - 2017-12-11 04:01:45.434194: step 4000, loss = 0.73, batch loss = 0.67 (14.8 examples/sec; 0.541 sec/batch; 49h:23m:18s remains)
2017-12-11 04:01:45.980563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04182934 0.04634013 0.052266322 0.059015393 0.065375783 0.069701113 0.070918091 0.068969257 0.065217733 0.061614476 0.05860148 0.056269258 0.0543037 0.053015225 0.053532854][0.064969145 0.072174393 0.081487291 0.093083471 0.10512475 0.11460985 0.1187785 0.11664435 0.11060547 0.10401948 0.097820975 0.092305943 0.087588757 0.084600806 0.084692985][0.092774391 0.10311466 0.11593138 0.13297984 0.15196873 0.16853879 0.17770612 0.17628138 0.16756488 0.15656255 0.1451558 0.1342461 0.12494507 0.1193066 0.11838059][0.12660703 0.14022434 0.15585402 0.17765877 0.20377104 0.22877534 0.24465762 0.24468666 0.23247547 0.21466875 0.19483411 0.17550221 0.15969859 0.15082586 0.14883439][0.16603297 0.18317658 0.20089135 0.22658832 0.25974908 0.29401082 0.31757241 0.31940165 0.30250227 0.27493128 0.2427852 0.21163008 0.18754567 0.17503704 0.17241885][0.207296 0.2282881 0.24740577 0.27562764 0.31403536 0.35566464 0.38543898 0.38810444 0.36526808 0.32633638 0.28073213 0.2375966 0.2060111 0.19068745 0.18823481][0.24646738 0.26949808 0.2872403 0.31422079 0.35360676 0.39839095 0.4313494 0.43368575 0.40504783 0.35633224 0.30054867 0.24999416 0.21509138 0.19962344 0.19915208][0.27955946 0.30241436 0.31574422 0.33666444 0.37118611 0.41356227 0.44659555 0.44950002 0.41920161 0.36680737 0.30770978 0.25598621 0.22137624 0.20693226 0.20931795][0.30101874 0.32438245 0.33358279 0.34599656 0.37004054 0.40322927 0.43161535 0.43565595 0.40962246 0.36250687 0.30889425 0.26190987 0.22959435 0.21554048 0.2200684][0.30289859 0.32856855 0.33653536 0.34145716 0.35236853 0.3713147 0.39043164 0.39489797 0.37750441 0.34339625 0.30299422 0.26553622 0.23710541 0.22293769 0.22841215][0.28552857 0.3118321 0.31883535 0.3173224 0.31617177 0.32171026 0.33236408 0.33820546 0.33112338 0.31240794 0.28750855 0.26087928 0.23671101 0.22259066 0.22847809][0.25508204 0.2797727 0.28568253 0.28040749 0.27275407 0.27154782 0.27900583 0.28781971 0.28959993 0.28256565 0.26856521 0.24883451 0.22670491 0.21232398 0.2186905][0.21643953 0.2395453 0.24678022 0.2431515 0.23650435 0.23567152 0.24371019 0.25424632 0.26001582 0.25804073 0.24854995 0.23149274 0.20999038 0.19560826 0.2022271][0.17214842 0.19336195 0.20423588 0.2073584 0.20769681 0.21155864 0.22051939 0.22887547 0.23222166 0.22904368 0.21979113 0.20426433 0.18547627 0.17383933 0.18134879][0.12739281 0.14469783 0.15772422 0.16747111 0.17579831 0.18528086 0.1952624 0.2005484 0.19976777 0.19351862 0.1833065 0.16975148 0.15595685 0.14926834 0.15799172]]...]
INFO - root - 2017-12-11 04:01:51.358921: step 4010, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.531 sec/batch; 48h:24m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:01:56.652224: step 4020, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.528 sec/batch; 48h:11m:22s remains)
INFO - root - 2017-12-11 04:02:02.027737: step 4030, loss = 0.72, batch loss = 0.67 (14.9 examples/sec; 0.535 sec/batch; 48h:51m:02s remains)
INFO - root - 2017-12-11 04:02:07.379990: step 4040, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 49h:38m:18s remains)
INFO - root - 2017-12-11 04:02:12.636982: step 4050, loss = 0.72, batch loss = 0.67 (15.2 examples/sec; 0.527 sec/batch; 48h:04m:26s remains)
INFO - root - 2017-12-11 04:02:18.058809: step 4060, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.542 sec/batch; 49h:29m:16s remains)
INFO - root - 2017-12-11 04:02:23.211747: step 4070, loss = 0.73, batch loss = 0.67 (14.5 examples/sec; 0.553 sec/batch; 50h:25m:48s remains)
INFO - root - 2017-12-11 04:02:28.644297: step 4080, loss = 0.73, batch loss = 0.67 (14.8 examples/sec; 0.542 sec/batch; 49h:28m:00s remains)
INFO - root - 2017-12-11 04:02:34.072863: step 4090, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 49h:52m:15s remains)
INFO - root - 2017-12-11 04:02:39.456287: step 4100, loss = 0.73, batch loss = 0.67 (14.3 examples/sec; 0.559 sec/batch; 50h:59m:20s remains)
2017-12-11 04:02:40.015893: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27217808 0.28193665 0.27339712 0.25701749 0.23936297 0.22592291 0.21990038 0.22300272 0.23367926 0.24304764 0.2475656 0.24487086 0.23386884 0.21189347 0.17814961][0.33133021 0.34263593 0.33159924 0.31102073 0.28818536 0.27036393 0.26210862 0.26463625 0.27562439 0.28532565 0.29012048 0.28621417 0.27102694 0.24240202 0.20071676][0.35907868 0.37155688 0.3616752 0.34280503 0.32050276 0.30192867 0.29163724 0.29036975 0.29610693 0.30086046 0.30226791 0.29589465 0.2776103 0.24534768 0.2002563][0.35408828 0.37067837 0.36819154 0.35879102 0.34479511 0.33099374 0.32028985 0.31329456 0.30952168 0.30428702 0.29756716 0.2857683 0.26390883 0.22965211 0.18432668][0.32727894 0.35312256 0.36392903 0.36929697 0.3682614 0.36259964 0.35334843 0.3412914 0.32805711 0.31223202 0.2955215 0.27585974 0.24845405 0.21115354 0.16536367][0.28944203 0.32575247 0.35184664 0.37365323 0.3877416 0.39306405 0.38873297 0.37520802 0.35562441 0.33114371 0.30404964 0.27420411 0.2384138 0.19585589 0.14833473][0.24790742 0.29247609 0.33178788 0.36888668 0.39876893 0.41751876 0.42147574 0.40987489 0.38629714 0.35385096 0.31553772 0.27387309 0.22869727 0.18100262 0.132803][0.21297155 0.26260969 0.31079078 0.35872853 0.40113387 0.43170974 0.44385916 0.43589646 0.41124269 0.37351486 0.32606861 0.27400118 0.22090918 0.16956986 0.12185001][0.18706843 0.23689409 0.28630644 0.33588395 0.38164493 0.41675556 0.43350306 0.43025503 0.40976667 0.37417752 0.32552651 0.26975349 0.21326596 0.16075711 0.11427031][0.16574617 0.2089622 0.25116572 0.29334059 0.33328927 0.36526883 0.38249373 0.38343781 0.36990252 0.34202218 0.2998265 0.2482288 0.19460206 0.14517584 0.10260817][0.14247604 0.17331123 0.2024398 0.23229726 0.26238522 0.28836629 0.30451125 0.30854926 0.30078346 0.2801246 0.24595225 0.20230351 0.15655059 0.11531299 0.081211291][0.11721358 0.13461234 0.14986825 0.16681749 0.18619108 0.20491733 0.2181021 0.22254135 0.21717395 0.20081665 0.17379305 0.13995896 0.10585715 0.076932684 0.054655816][0.091881216 0.098431006 0.10247219 0.10837942 0.11727037 0.12733108 0.13508399 0.13759701 0.13326347 0.121109 0.1021871 0.079727307 0.058598749 0.042206164 0.030742509][0.067072861 0.0664805 0.063118838 0.061350077 0.0619525 0.063954778 0.065762334 0.065709211 0.062603734 0.0556904 0.045708667 0.03448974 0.0246452 0.017742222 0.013431268][0.045027487 0.040978096 0.034652583 0.0296737 0.026389852 0.024148839 0.022496728 0.020761017 0.018532505 0.015458145 0.01183934 0.0083003519 0.0055959937 0.0041962243 0.0036214993]]...]
INFO - root - 2017-12-11 04:02:45.339978: step 4110, loss = 0.72, batch loss = 0.67 (14.7 examples/sec; 0.546 sec/batch; 49h:46m:14s remains)
INFO - root - 2017-12-11 04:02:50.665075: step 4120, loss = 0.73, batch loss = 0.67 (15.3 examples/sec; 0.523 sec/batch; 47h:42m:23s remains)
INFO - root - 2017-12-11 04:02:56.005945: step 4130, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.529 sec/batch; 48h:16m:41s remains)
INFO - root - 2017-12-11 04:03:01.391262: step 4140, loss = 0.72, batch loss = 0.67 (15.1 examples/sec; 0.529 sec/batch; 48h:13m:00s remains)
INFO - root - 2017-12-11 04:03:06.785339: step 4150, loss = 0.73, batch loss = 0.67 (14.7 examples/sec; 0.545 sec/batch; 49h:40m:24s remains)
INFO - root - 2017-12-11 04:03:12.136560: step 4160, loss = 0.73, batch loss = 0.67 (15.2 examples/sec; 0.526 sec/batch; 47h:59m:03s remains)
INFO - root - 2017-12-11 04:03:17.212573: step 4170, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.535 sec/batch; 48h:48m:48s remains)
INFO - root - 2017-12-11 04:03:22.699912: step 4180, loss = 0.72, batch loss = 0.67 (13.6 examples/sec; 0.590 sec/batch; 53h:47m:39s remains)
INFO - root - 2017-12-11 04:03:28.104082: step 4190, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.540 sec/batch; 49h:12m:36s remains)
INFO - root - 2017-12-11 04:03:33.372956: step 4200, loss = 0.72, batch loss = 0.67 (15.8 examples/sec; 0.506 sec/batch; 46h:06m:50s remains)
2017-12-11 04:03:33.896979: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067167275 0.069369592 0.071820632 0.078623973 0.091416948 0.10844577 0.12841004 0.14802213 0.16574688 0.17985982 0.18643954 0.18291208 0.16815245 0.14306404 0.11225832][0.090055153 0.093036048 0.097545534 0.1095499 0.13093936 0.15857925 0.18917136 0.21740969 0.2404156 0.25552583 0.25808865 0.2469728 0.2227024 0.18774295 0.14781901][0.11310954 0.11660103 0.12308902 0.14062767 0.17104371 0.20977439 0.25021929 0.28459835 0.30855656 0.318998 0.311842 0.28880888 0.25332329 0.210009 0.16518463][0.14267819 0.14391015 0.15063567 0.17220235 0.20954442 0.25648367 0.30250746 0.337531 0.35619661 0.35609522 0.33508217 0.29854134 0.25310877 0.20529386 0.16100933][0.1828571 0.17794493 0.18115379 0.20245317 0.24118538 0.28967747 0.33393073 0.36258334 0.37041807 0.35690743 0.32264712 0.27599213 0.22566791 0.17867161 0.13967875][0.22810678 0.21537116 0.21072063 0.22558902 0.2580201 0.29963079 0.33452278 0.35147566 0.34671551 0.3220697 0.2804493 0.23093905 0.1823256 0.14051093 0.10893525][0.26641652 0.24516004 0.2288685 0.23122969 0.2504639 0.27881739 0.3005088 0.30559155 0.29225296 0.26390004 0.224047 0.17999741 0.1385801 0.10393462 0.078945972][0.28409138 0.25487515 0.22651689 0.21398127 0.21715368 0.23018482 0.23969495 0.23791662 0.22445899 0.20252384 0.17388874 0.14197505 0.1103433 0.081781238 0.059829213][0.27210012 0.23942268 0.20307669 0.17831375 0.16726676 0.16692489 0.16820838 0.16610327 0.16069186 0.15304504 0.14153473 0.12501629 0.10343503 0.078804918 0.056076266][0.23168871 0.20167951 0.16365251 0.1324497 0.11223475 0.10305896 0.10064176 0.10262525 0.10868234 0.11696923 0.12231855 0.11990227 0.10696018 0.085258164 0.061146211][0.17261559 0.1487647 0.11503304 0.084226504 0.061762821 0.049798694 0.047775578 0.0551669 0.070828229 0.090808585 0.10785238 0.11537892 0.10933898 0.0916386 0.06930729][0.11018919 0.092734516 0.0675014 0.043072261 0.024738802 0.015057599 0.015480588 0.02653626 0.046647608 0.071453385 0.093650855 0.10695639 0.10748154 0.096984975 0.081673995][0.058491584 0.047407527 0.031977739 0.017020529 0.0061930451 0.0010373107 0.003790973 0.015585522 0.0351187 0.058969602 0.081415623 0.097971231 0.1058596 0.1061946 0.10313326][0.027543794 0.021632293 0.013684196 0.0066218362 0.0022018433 0.00088261039 0.0049928534 0.015903868 0.032785181 0.053420529 0.074163862 0.092895694 0.10850468 0.12144554 0.13390061][0.016889306 0.013836274 0.0098519307 0.0067380527 0.0053676493 0.0059171375 0.010655583 0.020861063 0.0358574 0.054082554 0.073371507 0.093697742 0.11605632 0.14088853 0.16932243]]...]
INFO - root - 2017-12-11 04:03:39.253134: step 4210, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 49h:03m:10s remains)
INFO - root - 2017-12-11 04:03:44.524598: step 4220, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.529 sec/batch; 48h:16m:02s remains)
INFO - root - 2017-12-11 04:03:49.845399: step 4230, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.537 sec/batch; 48h:58m:40s remains)
INFO - root - 2017-12-11 04:03:55.235084: step 4240, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 49h:48m:24s remains)
INFO - root - 2017-12-11 04:04:00.549255: step 4250, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 49h:00m:07s remains)
INFO - root - 2017-12-11 04:04:05.904083: step 4260, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.536 sec/batch; 48h:49m:58s remains)
INFO - root - 2017-12-11 04:04:10.944389: step 4270, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 48h:26m:37s remains)
INFO - root - 2017-12-11 04:04:16.359292: step 4280, loss = 0.72, batch loss = 0.66 (15.5 examples/sec; 0.515 sec/batch; 46h:54m:40s remains)
INFO - root - 2017-12-11 04:04:21.560038: step 4290, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.529 sec/batch; 48h:14m:57s remains)
INFO - root - 2017-12-11 04:04:26.850111: step 4300, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 48h:09m:23s remains)
2017-12-11 04:04:27.415035: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050269518 0.080292106 0.12815146 0.18772417 0.24180682 0.28068689 0.29736704 0.29920033 0.28854206 0.27265143 0.26423591 0.2642009 0.26367682 0.25742289 0.2481599][0.053874593 0.085356094 0.13746844 0.20401603 0.26496211 0.30923349 0.32892245 0.33301371 0.32343486 0.307851 0.30132747 0.30450752 0.30714741 0.30150557 0.28979206][0.054822903 0.086396344 0.14005136 0.20968837 0.27301884 0.31843096 0.33862448 0.34457332 0.3383871 0.32693976 0.32526216 0.33271784 0.33864924 0.33369508 0.32029861][0.054728877 0.086571224 0.14095551 0.21140568 0.27412531 0.3177062 0.3372381 0.34588057 0.3461886 0.34286585 0.34810945 0.359059 0.36524421 0.35724682 0.33977273][0.053402722 0.084891282 0.13778369 0.20523876 0.26397723 0.30418184 0.3249051 0.34002694 0.350715 0.35767895 0.36911213 0.38040712 0.38237593 0.36760554 0.34452289][0.051960934 0.082479551 0.13190803 0.19355921 0.24717648 0.2858333 0.31211269 0.33709231 0.35785371 0.37091133 0.38219562 0.38778657 0.381727 0.36043984 0.3353107][0.05214946 0.082489252 0.12937853 0.18709563 0.24009442 0.28301302 0.31846607 0.35134473 0.37437406 0.382567 0.38306409 0.37583491 0.3592985 0.3335624 0.31057748][0.05327997 0.083645895 0.12907922 0.18590263 0.24258488 0.29263476 0.33502036 0.36868033 0.38497296 0.38170505 0.36960667 0.35242286 0.33090535 0.30597836 0.28784829][0.05471088 0.084659874 0.12878412 0.18618156 0.24681108 0.30110696 0.34333533 0.36998588 0.37519884 0.3618885 0.34386626 0.32570881 0.30787903 0.2893891 0.27735212][0.056775413 0.08681228 0.13067032 0.18992665 0.25329256 0.30748159 0.34342325 0.35924929 0.35478604 0.33690822 0.31946486 0.30525619 0.29385963 0.28224665 0.27431574][0.056837387 0.086713895 0.13073865 0.19213359 0.25698251 0.30894336 0.33708397 0.34312287 0.33288077 0.31484133 0.30019417 0.28974837 0.2834264 0.27673373 0.27075735][0.051976282 0.079872228 0.1219486 0.1822571 0.24492586 0.29243043 0.31347492 0.31360751 0.30269477 0.28813055 0.27696157 0.2694869 0.26740193 0.26511425 0.26104519][0.042513765 0.066212118 0.10311464 0.15705629 0.21196888 0.25189692 0.26670718 0.26439625 0.2561262 0.24678291 0.24012925 0.23661661 0.23907052 0.24101965 0.23873737][0.032556098 0.052480891 0.084235542 0.12998283 0.17531326 0.20755124 0.21829495 0.21578483 0.2101955 0.20452033 0.20092177 0.19985479 0.20370972 0.20621873 0.20377187][0.025478967 0.043824829 0.073347524 0.11383582 0.15304382 0.18114549 0.19086158 0.1892653 0.18434201 0.17876679 0.17469607 0.17294376 0.17448 0.17408685 0.17023796]]...]
INFO - root - 2017-12-11 04:04:32.753191: step 4310, loss = 0.73, batch loss = 0.67 (14.8 examples/sec; 0.542 sec/batch; 49h:23m:24s remains)
INFO - root - 2017-12-11 04:04:38.106174: step 4320, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.528 sec/batch; 48h:08m:37s remains)
INFO - root - 2017-12-11 04:04:43.487192: step 4330, loss = 0.72, batch loss = 0.67 (15.2 examples/sec; 0.527 sec/batch; 48h:04m:15s remains)
INFO - root - 2017-12-11 04:04:48.764818: step 4340, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 48h:37m:00s remains)
INFO - root - 2017-12-11 04:04:54.086652: step 4350, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 48h:43m:00s remains)
INFO - root - 2017-12-11 04:04:59.462830: step 4360, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.550 sec/batch; 50h:06m:55s remains)
INFO - root - 2017-12-11 04:05:04.515148: step 4370, loss = 0.73, batch loss = 0.67 (26.1 examples/sec; 0.307 sec/batch; 27h:57m:36s remains)
INFO - root - 2017-12-11 04:05:09.898275: step 4380, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 48h:24m:04s remains)
INFO - root - 2017-12-11 04:05:15.219318: step 4390, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 49h:53m:21s remains)
INFO - root - 2017-12-11 04:05:20.591506: step 4400, loss = 0.73, batch loss = 0.67 (15.1 examples/sec; 0.531 sec/batch; 48h:24m:06s remains)
2017-12-11 04:05:21.211579: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.263694 0.27890983 0.28645796 0.28768525 0.29396743 0.31345266 0.3400633 0.36508375 0.38120669 0.38284782 0.36875269 0.34555587 0.32678518 0.32362223 0.3362627][0.32660666 0.3481088 0.35421473 0.35194051 0.35608408 0.37606862 0.4039413 0.42720255 0.43868655 0.43619785 0.42061183 0.40026245 0.38785934 0.39170286 0.40789077][0.38942975 0.41113144 0.412569 0.40485841 0.40464005 0.42127436 0.44526932 0.46210837 0.46625972 0.46009165 0.44818291 0.43739417 0.43698293 0.45090383 0.47109395][0.44424495 0.4626174 0.45819083 0.44735554 0.44499135 0.45802 0.47681144 0.48770839 0.4870981 0.48014584 0.47428811 0.47426221 0.48475456 0.50537026 0.52656496][0.46644408 0.48264065 0.47889671 0.47375685 0.47681451 0.49036464 0.50525504 0.51146537 0.50803393 0.500604 0.49773577 0.50351977 0.51855946 0.538699 0.5551312][0.43938342 0.45969272 0.46525839 0.47597134 0.49455625 0.5171653 0.53452235 0.54053825 0.53645825 0.52621084 0.51910967 0.52113509 0.53092545 0.54133695 0.54604411][0.36919233 0.39687416 0.41745895 0.44985512 0.49041745 0.52896053 0.55544317 0.56583428 0.56150079 0.54413247 0.52476257 0.5134663 0.50902164 0.50329477 0.49330884][0.27417359 0.30946228 0.34566891 0.39791611 0.45797619 0.51098561 0.54632241 0.56077284 0.55493033 0.52888703 0.49443245 0.46555263 0.44271892 0.41927138 0.3958329][0.17673071 0.21671711 0.26372612 0.3273499 0.39659908 0.45430642 0.49029428 0.50310719 0.49395838 0.46235073 0.41812965 0.37631568 0.33908743 0.30234054 0.26969174][0.10091505 0.13840324 0.18572275 0.24734089 0.31204593 0.36317253 0.39194143 0.39873433 0.38620889 0.35420388 0.30959016 0.26573151 0.225761 0.18716174 0.15460281][0.051949389 0.079889014 0.11683524 0.16413678 0.2126025 0.24908379 0.26717469 0.26825041 0.25506952 0.22849651 0.19306721 0.15913959 0.12983172 0.10327475 0.083388843][0.02627337 0.042550761 0.064888261 0.092792511 0.12011588 0.13930283 0.14716117 0.14534931 0.13536435 0.11900093 0.099215962 0.0823245 0.070588887 0.062903427 0.063016154][0.016209563 0.023448523 0.033372663 0.044997513 0.055382043 0.061838996 0.063686192 0.062383145 0.058409825 0.053824116 0.050211031 0.049840938 0.054190174 0.06342338 0.08220537][0.014634843 0.016664518 0.019347049 0.022198236 0.024427412 0.025761595 0.026435271 0.027580602 0.02923866 0.033070154 0.039809428 0.049546681 0.063631378 0.083450235 0.11486213][0.012992596 0.013437547 0.013469441 0.013747754 0.0143439 0.015446635 0.017347492 0.020856423 0.025903124 0.033435598 0.043922953 0.056953486 0.074113846 0.0976641 0.13457981]]...]
INFO - root - 2017-12-11 04:05:26.585425: step 4410, loss = 0.73, batch loss = 0.67 (15.3 examples/sec; 0.524 sec/batch; 47h:47m:30s remains)
INFO - root - 2017-12-11 04:05:31.916613: step 4420, loss = 0.74, batch loss = 0.68 (15.0 examples/sec; 0.532 sec/batch; 48h:28m:15s remains)
INFO - root - 2017-12-11 04:05:37.271868: step 4430, loss = 0.74, batch loss = 0.68 (14.6 examples/sec; 0.548 sec/batch; 49h:55m:53s remains)
INFO - root - 2017-12-11 04:05:42.564115: step 4440, loss = 0.72, batch loss = 0.66 (15.5 examples/sec; 0.517 sec/batch; 47h:07m:25s remains)
INFO - root - 2017-12-11 04:05:47.929724: step 4450, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.527 sec/batch; 48h:02m:08s remains)
INFO - root - 2017-12-11 04:05:53.237088: step 4460, loss = 0.73, batch loss = 0.67 (15.0 examples/sec; 0.535 sec/batch; 48h:43m:05s remains)
INFO - root - 2017-12-11 04:05:58.574011: step 4470, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.522 sec/batch; 47h:36m:22s remains)
INFO - root - 2017-12-11 04:06:03.584435: step 4480, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 48h:33m:11s remains)
INFO - root - 2017-12-11 04:06:09.011358: step 4490, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 48h:17m:07s remains)
INFO - root - 2017-12-11 04:06:14.285568: step 4500, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:41m:34s remains)
2017-12-11 04:06:14.818664: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12405448 0.11620893 0.11628177 0.12578019 0.14143555 0.15698023 0.16704929 0.1710588 0.17014533 0.16201517 0.14661671 0.12699315 0.10771484 0.091685228 0.079594776][0.14012149 0.13592182 0.14185624 0.16304713 0.19383983 0.22484809 0.24734138 0.2573874 0.2559748 0.23955093 0.20893477 0.17074649 0.13484612 0.10754081 0.0881658][0.14710316 0.14760228 0.16203153 0.19721407 0.2460753 0.29597795 0.33398443 0.35126457 0.34805134 0.32089967 0.27220395 0.21243595 0.15711738 0.11649186 0.088932589][0.15301932 0.15781339 0.17959321 0.22781242 0.29411805 0.36257273 0.41551557 0.43811622 0.43039748 0.39095777 0.3246817 0.24543178 0.17320754 0.12155376 0.088168472][0.16360615 0.16749616 0.1919612 0.24806532 0.32706723 0.41102642 0.47684231 0.50268227 0.488418 0.43594262 0.35401708 0.25980133 0.17642395 0.11937501 0.08485201][0.17508318 0.17627078 0.19755551 0.25359112 0.33635345 0.42798787 0.50152266 0.529441 0.50990528 0.44751272 0.35500666 0.25287455 0.16585284 0.10986044 0.078928337][0.18645398 0.18437923 0.19864225 0.24718742 0.32282293 0.40986896 0.48116532 0.50806385 0.48595402 0.42087904 0.32790577 0.22825381 0.14567399 0.095364325 0.070688233][0.20229053 0.19237605 0.19731879 0.23420954 0.29554433 0.3672353 0.42498714 0.4440231 0.41858119 0.35586098 0.27258289 0.18788417 0.11987213 0.0801919 0.063321687][0.21657693 0.19776982 0.19326311 0.22025271 0.26950765 0.32543048 0.36595777 0.3724089 0.34076864 0.27984926 0.2086312 0.14417054 0.096594594 0.070980117 0.062260669][0.22309932 0.19764863 0.18569139 0.20672248 0.24937822 0.29524183 0.3232533 0.31993324 0.28401136 0.22530663 0.16466685 0.11808536 0.089370959 0.076854154 0.074236162][0.22748917 0.19738327 0.17854685 0.19422998 0.23186155 0.27141884 0.29346618 0.28766233 0.25465837 0.203928 0.15585133 0.12497643 0.11121778 0.10877334 0.10959891][0.23452325 0.19630818 0.16923411 0.17781627 0.20895933 0.24249837 0.26178584 0.25862232 0.23499218 0.19937378 0.16953178 0.15697315 0.15860204 0.16637652 0.17083475][0.23511922 0.18747371 0.15121979 0.15141881 0.17569298 0.20411311 0.22244214 0.22399989 0.21152958 0.19283389 0.18294737 0.19022787 0.20871174 0.2284873 0.239191][0.22653629 0.17234068 0.12696147 0.11782878 0.1354 0.16111484 0.18145341 0.19038475 0.19016369 0.18675239 0.1927111 0.21474858 0.24626274 0.27611372 0.29331562][0.21402076 0.15739933 0.1046191 0.084559418 0.092696875 0.11466179 0.13756573 0.15503302 0.16765742 0.17773263 0.19505376 0.22582865 0.2653724 0.30202261 0.32376656]]...]
INFO - root - 2017-12-11 04:06:20.202851: step 4510, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 48h:07m:08s remains)
INFO - root - 2017-12-11 04:06:25.571960: step 4520, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 49h:02m:45s remains)
INFO - root - 2017-12-11 04:06:30.888403: step 4530, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 48h:32m:50s remains)
INFO - root - 2017-12-11 04:06:36.270850: step 4540, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 47h:14m:14s remains)
INFO - root - 2017-12-11 04:06:41.696819: step 4550, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 47h:41m:31s remains)
INFO - root - 2017-12-11 04:06:47.097551: step 4560, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 48h:09m:38s remains)
INFO - root - 2017-12-11 04:06:52.484739: step 4570, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:28m:31s remains)
INFO - root - 2017-12-11 04:06:57.507987: step 4580, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.535 sec/batch; 48h:42m:50s remains)
INFO - root - 2017-12-11 04:07:02.833156: step 4590, loss = 0.72, batch loss = 0.66 (15.9 examples/sec; 0.504 sec/batch; 45h:56m:29s remains)
INFO - root - 2017-12-11 04:07:08.251767: step 4600, loss = 0.73, batch loss = 0.67 (15.0 examples/sec; 0.535 sec/batch; 48h:42m:32s remains)
2017-12-11 04:07:08.820698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010816238 0.0013428064 0.014595652 0.026592202 0.035869572 0.041329741 0.042369638 0.039542604 0.034300722 0.028188886 0.021664865 0.015289371 0.010127223 0.0079924213 0.010429573][-0.00036153031 0.020125054 0.04282302 0.064095773 0.080845185 0.090644263 0.092378043 0.087199263 0.077688776 0.066263385 0.054172788 0.042424422 0.032723412 0.02783183 0.030348705][0.013764292 0.044609614 0.080135405 0.11463518 0.14221759 0.15813728 0.16075444 0.15246698 0.13705997 0.11830925 0.098363787 0.079067878 0.062634259 0.052644905 0.053055752][0.026175031 0.067653738 0.11790678 0.1683021 0.20884126 0.23193687 0.23568708 0.22436872 0.20244059 0.17504737 0.14551561 0.11692151 0.091668658 0.073852226 0.069025628][0.033420607 0.082669809 0.14551069 0.21003662 0.26227131 0.29270637 0.29982927 0.28892729 0.26324514 0.22845161 0.18929882 0.15055358 0.11494932 0.0869777 0.073842809][0.033041704 0.085249335 0.15553401 0.2289141 0.28896534 0.32640293 0.34081274 0.3363589 0.31241494 0.27381966 0.22697802 0.17869064 0.13260788 0.093819506 0.0709633][0.024947008 0.075341344 0.14657508 0.22210295 0.2851285 0.32853389 0.35307026 0.35975266 0.34220833 0.30370474 0.25267538 0.19791026 0.14412142 0.097056478 0.066405654][0.013127121 0.057903875 0.12372712 0.19440319 0.2547251 0.3008281 0.33466411 0.35329095 0.34464443 0.31005615 0.25985664 0.20407031 0.14817587 0.098357454 0.064783573][0.0028815081 0.0401627 0.096024513 0.15601316 0.2081002 0.25169563 0.28982285 0.31657022 0.31665486 0.28957382 0.24604346 0.19558012 0.14417201 0.098403074 0.067922659][-0.0022028466 0.029055135 0.074772336 0.12243278 0.16354686 0.19968349 0.23474251 0.26204133 0.26685756 0.24807879 0.21527012 0.17565928 0.13452381 0.098604262 0.07596463][-0.0030419312 0.025067728 0.063736625 0.10193189 0.1334796 0.15981379 0.18499558 0.20424795 0.20697434 0.19327717 0.17140457 0.14591804 0.11982445 0.098035447 0.085922807][-0.0019644795 0.025310649 0.060515635 0.093454778 0.11879641 0.13574259 0.1476941 0.15349106 0.14875659 0.13595961 0.12221581 0.1101579 0.100138 0.093140528 0.090932466][-0.00097799685 0.026370645 0.060245693 0.090786591 0.11286135 0.12304272 0.12351172 0.11636652 0.1032277 0.089200921 0.08015877 0.07742887 0.079333283 0.082867339 0.086529605][-0.0036876127 0.022263484 0.054130007 0.082575895 0.10280213 0.10964181 0.1037761 0.0888465 0.070181519 0.05493369 0.047889225 0.049558312 0.057212491 0.065320469 0.070564307][-0.012057994 0.0089359041 0.035545696 0.059774466 0.077826433 0.084090337 0.077739537 0.061955873 0.04300439 0.028481992 0.022306671 0.024750767 0.033068229 0.041153792 0.0454183]]...]
INFO - root - 2017-12-11 04:07:14.141541: step 4610, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 50h:43m:56s remains)
INFO - root - 2017-12-11 04:07:19.478541: step 4620, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 47h:41m:55s remains)
INFO - root - 2017-12-11 04:07:24.916254: step 4630, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:28m:03s remains)
INFO - root - 2017-12-11 04:07:30.345144: step 4640, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:27m:46s remains)
INFO - root - 2017-12-11 04:07:35.606037: step 4650, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 48h:06m:32s remains)
INFO - root - 2017-12-11 04:07:40.956643: step 4660, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 48h:28m:02s remains)
INFO - root - 2017-12-11 04:07:46.302970: step 4670, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 48h:26m:20s remains)
INFO - root - 2017-12-11 04:07:51.212660: step 4680, loss = 0.70, batch loss = 0.65 (19.0 examples/sec; 0.420 sec/batch; 38h:15m:30s remains)
INFO - root - 2017-12-11 04:07:56.586048: step 4690, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 49h:32m:38s remains)
INFO - root - 2017-12-11 04:08:01.999725: step 4700, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 48h:59m:15s remains)
2017-12-11 04:08:02.508951: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29689315 0.27879557 0.24976642 0.22467399 0.21122023 0.2099275 0.21708277 0.2264462 0.23472923 0.2438723 0.25926062 0.27677289 0.28943098 0.29295519 0.28842562][0.34929171 0.32149708 0.27702045 0.23694246 0.2131725 0.20846015 0.21613002 0.22668281 0.23437642 0.24094272 0.2547057 0.27291182 0.28915593 0.29646969 0.29202473][0.37599659 0.34284452 0.2892794 0.24103492 0.2128389 0.20766239 0.21629469 0.22749789 0.23353498 0.23559293 0.24388781 0.25859642 0.274473 0.28257927 0.27695754][0.36667538 0.3349764 0.28307298 0.23798133 0.21451454 0.21487729 0.22774461 0.24170171 0.24774116 0.2461284 0.24783543 0.25568929 0.26639137 0.27040309 0.2604461][0.31325567 0.28929627 0.24903691 0.2180323 0.20902619 0.22268079 0.24586797 0.2671302 0.27641276 0.27324414 0.26900381 0.26873964 0.27107593 0.26648247 0.24762431][0.22678006 0.21388583 0.19061732 0.17994688 0.19141285 0.22340733 0.26080278 0.29144379 0.30517286 0.30156785 0.29280502 0.28561723 0.28007346 0.26651633 0.23768871][0.12529877 0.12066552 0.114284 0.125052 0.15868284 0.21056654 0.263 0.30254725 0.31990662 0.31573141 0.30329716 0.290759 0.27939308 0.25961703 0.2242552][0.041008167 0.040776528 0.046399441 0.0728611 0.1228466 0.18943733 0.25296241 0.29861522 0.31843221 0.31470355 0.30095729 0.28574765 0.27086693 0.24734087 0.20840165][-0.0027344017 -0.0022461892 0.0083129425 0.040236477 0.094589122 0.16387074 0.22886193 0.27454692 0.29461792 0.29341856 0.28319696 0.27074516 0.25628236 0.23168153 0.19110414][-0.010427556 -0.012366433 -0.0040457384 0.023249971 0.070363663 0.13064864 0.18711993 0.22555503 0.24189928 0.2428128 0.23852509 0.23278359 0.22287458 0.20142554 0.16364221][0.00039454081 -0.0065064584 -0.0056259348 0.010214947 0.042668521 0.087013908 0.12938716 0.1569737 0.1675662 0.16933079 0.17020634 0.17130053 0.16763929 0.15279426 0.12275551][0.015786732 0.0036180022 -0.0046252846 -0.0025447006 0.012543279 0.038418107 0.065085389 0.082014456 0.087969571 0.09111008 0.096756548 0.10398465 0.10639603 0.098535053 0.077511124][0.02719425 0.011796672 -0.0032289485 -0.01235135 -0.012603361 -0.0041973204 0.0069183451 0.013629479 0.015643626 0.019954095 0.029343911 0.041322809 0.048679546 0.046732135 0.033875827][0.027924661 0.011923363 -0.0055765421 -0.020089818 -0.029484179 -0.033313759 -0.034569476 -0.036870878 -0.039413434 -0.036832396 -0.027937146 -0.015550416 -0.006298406 -0.0039243842 -0.00919925][0.019330315 0.0044166339 -0.011997165 -0.026504403 -0.037999988 -0.046239723 -0.052706998 -0.058852486 -0.063535072 -0.063381806 -0.0580796 -0.049687762 -0.042674128 -0.039228767 -0.039482027]]...]
INFO - root - 2017-12-11 04:08:07.794300: step 4710, loss = 0.71, batch loss = 0.66 (15.3 examples/sec; 0.524 sec/batch; 47h:44m:17s remains)
INFO - root - 2017-12-11 04:08:13.138304: step 4720, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 48h:15m:48s remains)
INFO - root - 2017-12-11 04:08:18.441235: step 4730, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 47h:35m:24s remains)
INFO - root - 2017-12-11 04:08:23.838182: step 4740, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 50h:28m:42s remains)
INFO - root - 2017-12-11 04:08:29.229138: step 4750, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:39m:08s remains)
INFO - root - 2017-12-11 04:08:34.615712: step 4760, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 48h:42m:41s remains)
INFO - root - 2017-12-11 04:08:39.974630: step 4770, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 48h:02m:44s remains)
INFO - root - 2017-12-11 04:08:45.318530: step 4780, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 48h:38m:04s remains)
INFO - root - 2017-12-11 04:08:50.392630: step 4790, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:27m:49s remains)
INFO - root - 2017-12-11 04:08:55.798465: step 4800, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 49h:07m:38s remains)
2017-12-11 04:08:56.345148: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12104206 0.13485129 0.1505283 0.16798566 0.18116015 0.18368125 0.17347611 0.15725733 0.13988046 0.12288708 0.10914833 0.10041634 0.096411459 0.0944263 0.096407935][0.12041297 0.13550098 0.15064642 0.16653782 0.17726164 0.1773589 0.16587059 0.14813915 0.13024896 0.11381802 0.10060473 0.09248735 0.090075783 0.091542087 0.096846059][0.11461878 0.12989044 0.14345987 0.15641175 0.16341923 0.16083646 0.14867273 0.13194251 0.11554196 0.10069401 0.08913523 0.082974322 0.083375305 0.088610515 0.0979716][0.11533352 0.13216445 0.14679988 0.16014427 0.16666795 0.16362782 0.15176834 0.13531005 0.11830552 0.10219365 0.089551091 0.083314337 0.08499451 0.09318801 0.1065235][0.12275484 0.14285882 0.1615098 0.17885572 0.18885005 0.18837199 0.1781439 0.16126435 0.14177105 0.12227318 0.10635097 0.097953685 0.098630406 0.10775277 0.12362583][0.13792174 0.16187607 0.18685398 0.21149048 0.22888198 0.23450702 0.22793989 0.21055116 0.18741594 0.16346164 0.14301884 0.13054302 0.12733257 0.13415271 0.15007877][0.16603902 0.19273704 0.22314152 0.25524631 0.28127539 0.2946344 0.292873 0.2748262 0.24700834 0.21773891 0.19174814 0.17338865 0.16393889 0.16608979 0.18074583][0.20653337 0.23434076 0.26604065 0.30110958 0.33215246 0.35071132 0.35244679 0.33353862 0.30156535 0.26794884 0.23705229 0.21226789 0.19546945 0.19204597 0.20547058][0.25124726 0.27612373 0.30305436 0.33430618 0.36386648 0.38263875 0.38503402 0.36554721 0.3320953 0.29771057 0.26519617 0.23596877 0.21293274 0.20430067 0.21584056][0.28594196 0.30311528 0.31935364 0.3403044 0.36165822 0.37558261 0.37612751 0.357235 0.32574105 0.29459482 0.26462328 0.23486584 0.20917222 0.19704191 0.20528322][0.2981534 0.30675924 0.31172156 0.32120797 0.33218893 0.33914211 0.33612806 0.31709406 0.28729862 0.25903583 0.23262373 0.20531036 0.18070765 0.1671288 0.17098612][0.2846224 0.286577 0.2838577 0.28518459 0.28783092 0.28830037 0.28053707 0.25915432 0.22846274 0.20031644 0.17617601 0.15237944 0.13084507 0.11712853 0.11611185][0.25433379 0.25071976 0.24392685 0.24114925 0.23909758 0.23541957 0.22368589 0.19989586 0.16811924 0.13954982 0.11744222 0.097865157 0.080334291 0.0672428 0.061330445][0.21930115 0.21089147 0.20115352 0.19606225 0.19164793 0.18626811 0.17352074 0.15097488 0.12215467 0.096387729 0.078191511 0.064096987 0.052002862 0.041420035 0.03239999][0.19335863 0.18161933 0.16925475 0.1621315 0.15660042 0.15140115 0.14071798 0.12386134 0.1032591 0.08499191 0.073626362 0.066744253 0.061661761 0.055414241 0.045529224]]...]
INFO - root - 2017-12-11 04:09:01.692802: step 4810, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:49m:56s remains)
INFO - root - 2017-12-11 04:09:07.066664: step 4820, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 48h:31m:09s remains)
INFO - root - 2017-12-11 04:09:12.415146: step 4830, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 48h:12m:40s remains)
INFO - root - 2017-12-11 04:09:17.735631: step 4840, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 52h:03m:18s remains)
INFO - root - 2017-12-11 04:09:23.080564: step 4850, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 48h:34m:52s remains)
INFO - root - 2017-12-11 04:09:28.400236: step 4860, loss = 0.72, batch loss = 0.66 (15.6 examples/sec; 0.513 sec/batch; 46h:40m:52s remains)
INFO - root - 2017-12-11 04:09:33.749888: step 4870, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 49h:13m:41s remains)
INFO - root - 2017-12-11 04:09:39.115969: step 4880, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 48h:32m:54s remains)
INFO - root - 2017-12-11 04:09:44.146138: step 4890, loss = 0.73, batch loss = 0.67 (14.8 examples/sec; 0.540 sec/batch; 49h:10m:53s remains)
INFO - root - 2017-12-11 04:09:49.537812: step 4900, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 51h:25m:31s remains)
2017-12-11 04:09:50.096982: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29145646 0.26532295 0.23360078 0.21782617 0.23199521 0.2712521 0.31187311 0.33219424 0.32479876 0.28889358 0.24504124 0.20709312 0.18021791 0.16892032 0.17076217][0.28159133 0.25286013 0.21774496 0.20133141 0.21722804 0.26078656 0.30663455 0.32989427 0.32289454 0.28469327 0.23472206 0.1904698 0.16468431 0.16193043 0.17342858][0.26278254 0.23334618 0.19847612 0.18382372 0.20098847 0.24493064 0.28998294 0.31117073 0.30103242 0.26063293 0.20898509 0.16453949 0.14420156 0.15101556 0.17165455][0.26198041 0.23201442 0.19720532 0.18467146 0.20470421 0.25009102 0.29401976 0.31131738 0.29536182 0.25105685 0.19900838 0.15796298 0.14506483 0.16095686 0.189111][0.27493954 0.24362126 0.20770872 0.19602083 0.21944965 0.2684125 0.31375164 0.32895938 0.30711609 0.25748226 0.20362739 0.16484034 0.15726231 0.17956243 0.21330589][0.27688316 0.24639936 0.21046346 0.20011079 0.2281893 0.28349361 0.33407488 0.35119247 0.32586855 0.27045038 0.21119162 0.16917798 0.16098383 0.18464881 0.22138038][0.26271147 0.23577331 0.20293508 0.19542609 0.22908281 0.29240826 0.35110772 0.37389338 0.3485575 0.28835291 0.22138304 0.17116901 0.15631546 0.17565691 0.21160264][0.23179197 0.21307129 0.18777098 0.18488279 0.22302379 0.29267132 0.35897645 0.38858521 0.36573464 0.30221444 0.22665858 0.16479279 0.13830341 0.14836664 0.18002234][0.19628263 0.19061813 0.17903738 0.18443738 0.22529028 0.29508838 0.36134025 0.39143312 0.36819649 0.301034 0.21740264 0.14379556 0.10472862 0.10397604 0.12960941][0.16516137 0.17662141 0.18445747 0.20273639 0.24544336 0.30790487 0.36291087 0.38255477 0.35246691 0.28098541 0.19336435 0.11418734 0.067694232 0.058634125 0.077255353][0.14450391 0.1713144 0.19789484 0.22858991 0.2705774 0.31915438 0.35421535 0.35553271 0.31508604 0.2420962 0.15907642 0.085618496 0.040729623 0.027700193 0.037485365][0.13816203 0.17324467 0.20946644 0.24423669 0.27883083 0.30870223 0.320992 0.30353516 0.25545821 0.18755448 0.11881477 0.061387025 0.025966555 0.01350332 0.014738972][0.12390893 0.15778856 0.19316714 0.22402766 0.24770908 0.26019463 0.25446421 0.22456175 0.17527857 0.11855735 0.069561072 0.032802355 0.010689133 0.0025564728 -0.00049516297][0.089392737 0.11516837 0.14217226 0.16391751 0.17639199 0.17633075 0.16049442 0.12672196 0.083115555 0.041924234 0.014566144 -0.00058932882 -0.0078710252 -0.0090934588 -0.012122083][0.036971752 0.050329402 0.064214237 0.073569037 0.075322315 0.067652635 0.049777802 0.0214529 -0.0089359675 -0.030913087 -0.0370358 -0.032834627 -0.026446372 -0.020333569 -0.02098925]]...]
INFO - root - 2017-12-11 04:09:55.374271: step 4910, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 48h:08m:35s remains)
INFO - root - 2017-12-11 04:10:00.765285: step 4920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 49h:15m:01s remains)
INFO - root - 2017-12-11 04:10:06.060925: step 4930, loss = 0.70, batch loss = 0.65 (15.8 examples/sec; 0.507 sec/batch; 46h:08m:44s remains)
INFO - root - 2017-12-11 04:10:11.401676: step 4940, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 50h:47m:46s remains)
INFO - root - 2017-12-11 04:10:16.754828: step 4950, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 49h:08m:49s remains)
INFO - root - 2017-12-11 04:10:22.107344: step 4960, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 49h:17m:56s remains)
INFO - root - 2017-12-11 04:10:27.434600: step 4970, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 47h:37m:06s remains)
INFO - root - 2017-12-11 04:10:32.765678: step 4980, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:46m:55s remains)
INFO - root - 2017-12-11 04:10:37.925039: step 4990, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 50h:30m:54s remains)
INFO - root - 2017-12-11 04:10:43.365309: step 5000, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 48h:29m:03s remains)
2017-12-11 04:10:43.887730: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17267109 0.15495946 0.18375123 0.25883919 0.35368964 0.43453524 0.47783813 0.47233561 0.42048749 0.34465727 0.27050307 0.2091442 0.15677412 0.11081894 0.0770919][0.20876476 0.18383369 0.20172502 0.26820058 0.36082432 0.44647709 0.49994549 0.50462109 0.4569625 0.3762452 0.28747451 0.20629881 0.13481419 0.075548239 0.034144845][0.26947951 0.23618627 0.2343903 0.27813518 0.35382712 0.43323854 0.49164069 0.50797683 0.4700104 0.39048097 0.29104075 0.1919729 0.10263433 0.031914994 -0.013274292][0.37302586 0.33024135 0.30721951 0.3246344 0.37856948 0.44763988 0.50720078 0.53099221 0.4987087 0.41731283 0.30740321 0.19412968 0.093228444 0.017527809 -0.027097216][0.47877249 0.42975685 0.38949147 0.38410687 0.41766462 0.47620803 0.53516519 0.56331021 0.53497231 0.452776 0.33790326 0.21848457 0.11442756 0.039943498 -0.0014410401][0.53256226 0.48670411 0.44251955 0.4290663 0.45452371 0.50972354 0.569584 0.59936148 0.57254481 0.49135417 0.37880734 0.26375651 0.16606069 0.098258764 0.061264254][0.51286197 0.47840211 0.44617358 0.44538972 0.48431566 0.55331933 0.62297916 0.65514237 0.62628746 0.54384845 0.43601033 0.33259317 0.24953212 0.19325899 0.16009012][0.43163419 0.41158453 0.39952895 0.42294502 0.48715028 0.57849771 0.66152334 0.69644964 0.66586906 0.58598316 0.4931125 0.41751951 0.36629468 0.33251679 0.3047834][0.29964828 0.29550046 0.30683115 0.35614932 0.44398147 0.55224985 0.643329 0.67986625 0.65237737 0.58431679 0.5181967 0.48210084 0.47295168 0.4692764 0.45285413][0.13077128 0.14579245 0.18142548 0.25419343 0.35962591 0.47557235 0.56621766 0.6008051 0.57773066 0.52425265 0.48371944 0.48131809 0.50684392 0.53040236 0.53196222][-0.016449891 0.0086381538 0.058006261 0.14079651 0.25105241 0.36443347 0.44852096 0.4797 0.46124017 0.42057949 0.39712018 0.41409957 0.45834908 0.49914119 0.51984632][-0.10529464 -0.084644981 -0.041655932 0.029379163 0.1248659 0.2223646 0.29460225 0.32352546 0.31364334 0.28814089 0.27953878 0.30851266 0.36131874 0.41002512 0.44297317][-0.1343573 -0.12592332 -0.102743 -0.060243849 0.001659237 0.067017034 0.11716982 0.13915522 0.1365214 0.12508167 0.12861645 0.16377114 0.21824344 0.26928779 0.30989456][-0.12291463 -0.12518027 -0.12084308 -0.10671458 -0.079919264 -0.048632167 -0.023289794 -0.012825466 -0.015698992 -0.023856388 -0.023045087 3.6676407e-05 0.039531078 0.082982451 0.12756802][-0.091406956 -0.099980965 -0.10763561 -0.11221811 -0.10992108 -0.1031897 -0.097182579 -0.097875416 -0.10544518 -0.11703537 -0.1264303 -0.1228787 -0.10496382 -0.074733041 -0.0324508]]...]
INFO - root - 2017-12-11 04:10:49.232479: step 5010, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 47h:38m:16s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:10:54.608024: step 5020, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 47h:11m:41s remains)
INFO - root - 2017-12-11 04:10:59.926921: step 5030, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 47h:55m:56s remains)
INFO - root - 2017-12-11 04:11:05.184347: step 5040, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 49h:12m:41s remains)
INFO - root - 2017-12-11 04:11:10.571642: step 5050, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 47h:28m:04s remains)
INFO - root - 2017-12-11 04:11:15.944579: step 5060, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:56m:54s remains)
INFO - root - 2017-12-11 04:11:21.289260: step 5070, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 48h:40m:06s remains)
INFO - root - 2017-12-11 04:11:26.679042: step 5080, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 49h:00m:29s remains)
INFO - root - 2017-12-11 04:11:32.010012: step 5090, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 49h:40m:49s remains)
INFO - root - 2017-12-11 04:11:36.991869: step 5100, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.526 sec/batch; 47h:47m:43s remains)
2017-12-11 04:11:37.597029: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048474424 0.0702012 0.096641973 0.12663566 0.15723173 0.17875919 0.18040043 0.16312565 0.13738482 0.11467279 0.10129841 0.092654981 0.082477219 0.068785526 0.054792188][0.079964012 0.10731268 0.13642627 0.16670042 0.19473393 0.21278588 0.21206108 0.19282179 0.16408363 0.13828157 0.12246265 0.11185306 0.099764258 0.083485864 0.0673452][0.10927178 0.13203457 0.14991572 0.16618414 0.1815636 0.19415994 0.19855651 0.19008458 0.17003062 0.14761806 0.13038667 0.11747211 0.10454346 0.087941982 0.072489828][0.14586121 0.15666573 0.15625428 0.15305686 0.15385489 0.16380565 0.1796395 0.1885384 0.18131471 0.16269994 0.1415305 0.12295329 0.10762691 0.09124618 0.078102365][0.18256605 0.18308944 0.16983861 0.15611462 0.15214346 0.16699265 0.19703388 0.22078341 0.2201573 0.19729887 0.16429895 0.13389555 0.11259113 0.095334843 0.084584087][0.19868141 0.19520454 0.18085836 0.17378515 0.18276475 0.21475889 0.26202476 0.29547483 0.29067373 0.25111526 0.19600424 0.14788538 0.11835728 0.10000426 0.0919445][0.19191881 0.18414567 0.17704856 0.19070704 0.22785978 0.28717488 0.35301772 0.39094558 0.37360737 0.30919582 0.22663593 0.15894592 0.12144744 0.10312499 0.098600745][0.17863059 0.16525367 0.1687773 0.20711957 0.27414453 0.3576279 0.43459132 0.46950677 0.43655658 0.34874129 0.24277258 0.15934345 0.11579866 0.09913031 0.099236749][0.16455397 0.15286088 0.16691215 0.22143291 0.30281436 0.39144084 0.46241847 0.48420632 0.43704489 0.33719862 0.22324769 0.13642643 0.092804082 0.0804174 0.085972905][0.14460711 0.14481893 0.16800545 0.22473837 0.29804462 0.36738014 0.41270465 0.41170618 0.35470515 0.25984213 0.1597133 0.086594947 0.051129714 0.044713281 0.054200906][0.12222786 0.13755463 0.16816685 0.21595314 0.26354846 0.29521713 0.30118898 0.27260065 0.21166977 0.13624714 0.06645067 0.019076234 -0.0028395311 -0.0039194184 0.0061084065][0.10971167 0.13887209 0.17495336 0.20913911 0.22569801 0.21654445 0.18363686 0.1314151 0.071092628 0.016885418 -0.024065591 -0.048361309 -0.058474232 -0.056403831 -0.047942534][0.11571822 0.15777312 0.19718222 0.21871641 0.21024907 0.17106934 0.11248658 0.047389705 -0.010333745 -0.051828511 -0.077661306 -0.090361774 -0.094391666 -0.091396093 -0.08485385][0.14331517 0.19739836 0.23805177 0.24882221 0.2227425 0.16524048 0.094499163 0.026925774 -0.025781918 -0.060592961 -0.081226 -0.091051072 -0.094505087 -0.093386158 -0.0897054][0.18745616 0.25166497 0.29018879 0.28904653 0.24670929 0.1750277 0.098471396 0.034697641 -0.0089566465 -0.036029883 -0.052611664 -0.062379844 -0.068407178 -0.071102269 -0.07092011]]...]
INFO - root - 2017-12-11 04:11:42.917954: step 5110, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 48h:59m:59s remains)
INFO - root - 2017-12-11 04:11:48.282655: step 5120, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 49h:01m:56s remains)
INFO - root - 2017-12-11 04:11:53.694298: step 5130, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.539 sec/batch; 48h:58m:38s remains)
INFO - root - 2017-12-11 04:11:59.078078: step 5140, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:31m:16s remains)
INFO - root - 2017-12-11 04:12:04.347010: step 5150, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 48h:13m:54s remains)
INFO - root - 2017-12-11 04:12:09.771200: step 5160, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 49h:10m:40s remains)
INFO - root - 2017-12-11 04:12:15.103549: step 5170, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:31m:52s remains)
INFO - root - 2017-12-11 04:12:20.431748: step 5180, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.515 sec/batch; 46h:47m:18s remains)
INFO - root - 2017-12-11 04:12:25.677205: step 5190, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 47h:50m:19s remains)
INFO - root - 2017-12-11 04:12:30.821549: step 5200, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 49h:13m:26s remains)
2017-12-11 04:12:31.351788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0054732556 -0.0040820967 -0.0015166715 0.00015367866 -0.00040385223 -0.0030340475 -0.0065430948 -0.0096565327 -0.011910904 -0.013596169 -0.014929996 -0.015588935 -0.015519772 -0.014735334 -0.01357385][0.0199918 0.016950311 0.019436847 0.023574263 0.024816604 0.021114431 0.013319829 0.0039152522 -0.0046322239 -0.011175702 -0.015523802 -0.017784305 -0.018472696 -0.01821338 -0.017723527][0.072869569 0.061681882 0.062752716 0.071440846 0.077755265 0.074753314 0.061174814 0.040714663 0.01924682 0.0012432957 -0.011306199 -0.018439429 -0.0214463 -0.022076644 -0.022069417][0.1546246 0.13139725 0.12903059 0.14473768 0.1617927 0.16482829 0.14698675 0.1119537 0.070131592 0.031729702 0.0029595529 -0.014764574 -0.023312597 -0.026054228 -0.02647596][0.25034198 0.21436504 0.20778462 0.23318428 0.26774934 0.28533784 0.26968214 0.2215341 0.155261 0.088450596 0.034227863 -0.0020161821 -0.021430269 -0.02905607 -0.030795183][0.33211976 0.28713128 0.27777725 0.31445393 0.37085724 0.4104721 0.40547681 0.35095498 0.26313192 0.16644184 0.08204703 0.021531068 -0.013668054 -0.029452203 -0.034133777][0.36965162 0.32199204 0.31314495 0.36099264 0.43917036 0.50392371 0.516881 0.46578395 0.36606523 0.24658293 0.13532019 0.050829396 -0.0013528138 -0.026833253 -0.035663698][0.34644163 0.30332544 0.29925025 0.35504633 0.44760412 0.531807 0.56377685 0.52488869 0.42723411 0.30001038 0.17432481 0.074268855 0.0097333072 -0.02342473 -0.035863589][0.26944476 0.24015562 0.24610941 0.30522236 0.3989895 0.48748216 0.52957535 0.50522447 0.42212063 0.30498928 0.18258314 0.08087182 0.012960545 -0.02286908 -0.036489319][0.17345655 0.1645568 0.18499659 0.24373099 0.32605168 0.40189803 0.44020405 0.4254747 0.36211771 0.26744819 0.1633286 0.072902136 0.010008538 -0.024208749 -0.037248757][0.098734893 0.1117712 0.14696802 0.20213792 0.26598117 0.32065204 0.34848711 0.34051815 0.29760647 0.228352 0.14641407 0.0703646 0.013513898 -0.019900445 -0.033965088][0.067791566 0.0993509 0.14426558 0.19200137 0.236099 0.27146056 0.29292592 0.29645398 0.27725813 0.23392558 0.17142043 0.10364078 0.044392705 0.0028284914 -0.019788828][0.081693791 0.12502868 0.17080553 0.2070144 0.23403309 0.25812241 0.28251144 0.30476022 0.31284428 0.29532796 0.2486828 0.18205404 0.11099289 0.050624505 0.0096256873][0.12604804 0.16962384 0.20601007 0.22712797 0.24136567 0.26356098 0.30051991 0.34804377 0.38607246 0.39403319 0.36032274 0.29000431 0.20157716 0.11622425 0.050563909][0.17337666 0.20770179 0.22658019 0.22971441 0.23341469 0.25794813 0.31089437 0.38469571 0.45224637 0.48515505 0.46443552 0.39147812 0.28798455 0.18024628 0.091538675]]...]
INFO - root - 2017-12-11 04:12:36.742487: step 5210, loss = 0.71, batch loss = 0.65 (13.8 examples/sec; 0.581 sec/batch; 52h:46m:56s remains)
INFO - root - 2017-12-11 04:12:42.022567: step 5220, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 48h:15m:31s remains)
INFO - root - 2017-12-11 04:12:47.380385: step 5230, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:34m:37s remains)
INFO - root - 2017-12-11 04:12:52.789974: step 5240, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 50h:27m:06s remains)
INFO - root - 2017-12-11 04:12:58.143162: step 5250, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 48h:07m:00s remains)
INFO - root - 2017-12-11 04:13:03.485127: step 5260, loss = 0.71, batch loss = 0.66 (15.3 examples/sec; 0.523 sec/batch; 47h:30m:15s remains)
INFO - root - 2017-12-11 04:13:08.849740: step 5270, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 49h:39m:01s remains)
INFO - root - 2017-12-11 04:13:14.074597: step 5280, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 48h:06m:39s remains)
INFO - root - 2017-12-11 04:13:19.449262: step 5290, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:50m:28s remains)
INFO - root - 2017-12-11 04:13:24.544850: step 5300, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.505 sec/batch; 45h:54m:18s remains)
2017-12-11 04:13:25.133211: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051092576 0.050590526 0.052208979 0.059762064 0.072470494 0.087096944 0.0998726 0.10719982 0.10747185 0.10149635 0.092149451 0.081464037 0.068058856 0.053794682 0.043165267][0.11368345 0.11102507 0.10925972 0.11694893 0.13384072 0.15557337 0.17605014 0.18896876 0.1908574 0.18223955 0.16793238 0.15195215 0.13176869 0.10906973 0.090093769][0.19491471 0.18832931 0.17904247 0.18305781 0.20141819 0.22917588 0.25779495 0.27794978 0.28308332 0.27186221 0.25102738 0.22819157 0.20109214 0.17108414 0.14473359][0.27520686 0.26275226 0.24239856 0.23905967 0.25552559 0.2868008 0.32213068 0.34941414 0.35842174 0.34472087 0.31655785 0.2861734 0.25329584 0.2192422 0.18925568][0.33910829 0.31941751 0.28748053 0.27666363 0.29149202 0.32650709 0.36803982 0.40074983 0.4111841 0.39247152 0.35453531 0.3144919 0.27579474 0.24050456 0.21121272][0.3706176 0.34235293 0.30019575 0.28308064 0.29738727 0.3369137 0.38405252 0.41958061 0.42807811 0.40212578 0.35386971 0.30440184 0.26171449 0.22872269 0.20478319][0.35881624 0.32433397 0.27629411 0.25512168 0.2691744 0.31156081 0.361197 0.39577457 0.3994852 0.36604562 0.30997184 0.25470683 0.21218401 0.18561417 0.17153837][0.3119227 0.27673537 0.23046167 0.21007825 0.22486047 0.26724026 0.31491122 0.34497869 0.34244585 0.30338857 0.24373433 0.18693373 0.14705761 0.12754676 0.12376891][0.25697681 0.22687602 0.18964759 0.17488705 0.19123413 0.23068035 0.27222803 0.29484355 0.28542835 0.24341792 0.18472572 0.13032393 0.093791284 0.078667529 0.081023261][0.21525636 0.19287875 0.16675177 0.15812737 0.1739271 0.20669913 0.23882189 0.25280297 0.23836362 0.19780868 0.14551027 0.098038256 0.066190787 0.053293142 0.057145007][0.19826429 0.18192808 0.16315782 0.15616392 0.1659687 0.18699875 0.20645732 0.21156774 0.1952967 0.16110106 0.12057956 0.085176744 0.06156021 0.052126404 0.056192297][0.20583515 0.19315507 0.177142 0.16761686 0.16817378 0.17570448 0.18231702 0.17974798 0.16387412 0.13784443 0.11019517 0.087781839 0.0735 0.068613723 0.073333964][0.21908973 0.20784459 0.19186869 0.1790603 0.17186074 0.16888747 0.16595241 0.15858948 0.14526422 0.12837397 0.11334085 0.10351364 0.098766088 0.098894976 0.10428154][0.21648173 0.20449686 0.18820022 0.1747193 0.16520597 0.15831624 0.15177618 0.14439075 0.13685101 0.13054913 0.12787513 0.12989709 0.13426825 0.13851571 0.14228843][0.1893186 0.17559794 0.16041209 0.15015259 0.1445456 0.14104678 0.13779023 0.13617229 0.13805324 0.14345747 0.15192014 0.16331904 0.17397724 0.17884143 0.17679879]]...]
INFO - root - 2017-12-11 04:13:30.481308: step 5310, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 49h:14m:48s remains)
INFO - root - 2017-12-11 04:13:35.901368: step 5320, loss = 0.71, batch loss = 0.66 (14.4 examples/sec; 0.555 sec/batch; 50h:26m:10s remains)
INFO - root - 2017-12-11 04:13:41.179623: step 5330, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 48h:20m:26s remains)
INFO - root - 2017-12-11 04:13:46.509523: step 5340, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:53m:27s remains)
INFO - root - 2017-12-11 04:13:51.850516: step 5350, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 48h:57m:35s remains)
INFO - root - 2017-12-11 04:13:57.113885: step 5360, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:41m:32s remains)
INFO - root - 2017-12-11 04:14:02.507399: step 5370, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 47h:15m:57s remains)
INFO - root - 2017-12-11 04:14:07.852545: step 5380, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:47m:28s remains)
INFO - root - 2017-12-11 04:14:13.112275: step 5390, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 47h:57m:22s remains)
INFO - root - 2017-12-11 04:14:18.297546: step 5400, loss = 0.69, batch loss = 0.64 (20.9 examples/sec; 0.382 sec/batch; 34h:42m:51s remains)
2017-12-11 04:14:18.771631: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050591648 0.065282032 0.071128026 0.069447152 0.063275471 0.05394147 0.046118803 0.046740856 0.056488954 0.06977962 0.080536216 0.086141877 0.084558681 0.072047733 0.049337987][0.0967344 0.12581652 0.14217393 0.14610109 0.14015494 0.12523828 0.11024141 0.1081126 0.12086916 0.13954218 0.15411191 0.16065846 0.15562665 0.13312104 0.095488526][0.14211325 0.18924205 0.22209617 0.23873454 0.23883305 0.22137782 0.19896924 0.19086006 0.20202811 0.22109525 0.23515382 0.23933718 0.22789161 0.19239697 0.13775206][0.17066059 0.23834752 0.29360914 0.33071271 0.34443861 0.3300215 0.30213654 0.28488296 0.28677112 0.29606307 0.29961723 0.29355013 0.27071682 0.22085172 0.15148954][0.17297325 0.26001906 0.340019 0.40326545 0.43916142 0.43732753 0.41056666 0.38401559 0.36882013 0.35596111 0.33609584 0.30959615 0.27067688 0.20867293 0.13183905][0.15353407 0.2512863 0.35007051 0.43773997 0.49962911 0.51931036 0.50278729 0.47090426 0.435456 0.39350554 0.34370247 0.29299867 0.2394858 0.17242219 0.097373366][0.12286431 0.22055116 0.32781312 0.43206328 0.51655996 0.56020093 0.5589788 0.52611649 0.47285804 0.40408403 0.32847261 0.25889352 0.19774023 0.13428849 0.068647146][0.095381208 0.18558799 0.2912496 0.40008879 0.495419 0.55474454 0.56585294 0.53553641 0.47457498 0.39365131 0.30774719 0.23192637 0.17129356 0.11592907 0.061051663][0.082275122 0.16165538 0.25676587 0.35593063 0.44490144 0.50454384 0.52085888 0.49855477 0.44679928 0.375947 0.30051374 0.2335894 0.1810611 0.1337021 0.084986463][0.08747685 0.15354864 0.22938111 0.30515811 0.37154049 0.41620654 0.42987582 0.41941243 0.3913931 0.34977108 0.30209967 0.25633264 0.21780333 0.1770896 0.12911443][0.10092614 0.15381715 0.20843999 0.2570782 0.29459998 0.31554064 0.31880239 0.31628579 0.31333888 0.30556098 0.29053038 0.26959115 0.24647987 0.21091297 0.16112055][0.10382758 0.14765026 0.187425 0.21617855 0.22978321 0.22658446 0.21376991 0.20975415 0.22138247 0.23820445 0.24856977 0.24762678 0.23628063 0.20354767 0.15242355][0.083042853 0.12136471 0.15324897 0.17072031 0.16882385 0.14657915 0.11687592 0.10374995 0.11768448 0.14546351 0.16933084 0.18005326 0.1759865 0.14671238 0.099700868][0.040736817 0.072631948 0.0984808 0.10925308 0.099355578 0.066996545 0.027091689 0.0050647729 0.013449306 0.03922046 0.063753806 0.0774646 0.0780151 0.056612566 0.021897031][-0.0096641257 0.011458177 0.028384062 0.032610897 0.019894155 -0.012033174 -0.050252505 -0.07390146 -0.072069 -0.055341695 -0.038242824 -0.02729422 -0.023642465 -0.033954881 -0.051854298]]...]
INFO - root - 2017-12-11 04:14:24.122148: step 5410, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 50h:03m:37s remains)
INFO - root - 2017-12-11 04:14:29.436164: step 5420, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 47h:23m:42s remains)
INFO - root - 2017-12-11 04:14:34.805107: step 5430, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 48h:22m:24s remains)
INFO - root - 2017-12-11 04:14:40.203810: step 5440, loss = 0.71, batch loss = 0.65 (15.6 examples/sec; 0.512 sec/batch; 46h:28m:41s remains)
INFO - root - 2017-12-11 04:14:45.437657: step 5450, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 46h:57m:13s remains)
INFO - root - 2017-12-11 04:14:50.831513: step 5460, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:37m:11s remains)
INFO - root - 2017-12-11 04:14:56.233501: step 5470, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 47h:31m:12s remains)
INFO - root - 2017-12-11 04:15:01.627022: step 5480, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:30m:39s remains)
INFO - root - 2017-12-11 04:15:06.889816: step 5490, loss = 0.72, batch loss = 0.66 (15.5 examples/sec; 0.516 sec/batch; 46h:51m:27s remains)
INFO - root - 2017-12-11 04:15:12.266839: step 5500, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 47h:26m:24s remains)
2017-12-11 04:15:12.777857: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33658755 0.37782541 0.41205534 0.441593 0.4586561 0.44724387 0.40783006 0.34983763 0.28489247 0.2289748 0.19404948 0.19055998 0.21196732 0.24034068 0.26001632][0.32532111 0.37268662 0.42293775 0.47208107 0.50379747 0.499356 0.46159536 0.40135387 0.32916138 0.26443648 0.2248286 0.22161978 0.24353911 0.26787603 0.27983612][0.31270945 0.35489142 0.40782174 0.46256408 0.49873868 0.49858513 0.46627438 0.41155347 0.34213144 0.27830791 0.24160931 0.24337904 0.26662916 0.28482118 0.28605357][0.33270794 0.36652917 0.41717011 0.46859312 0.50054204 0.50067258 0.47261634 0.42168316 0.3537471 0.29162073 0.26048496 0.26977444 0.29566935 0.30910349 0.30094007][0.37263253 0.40674651 0.45660415 0.50253987 0.52793854 0.52950794 0.50921446 0.46381748 0.39757031 0.33813486 0.31561449 0.33514363 0.36361319 0.37084556 0.35172689][0.42651716 0.46581942 0.51629728 0.55780989 0.58069986 0.59263909 0.59177691 0.56073076 0.50055379 0.4444544 0.42917049 0.45417929 0.47663569 0.46992561 0.43495533][0.50360525 0.54959404 0.59837615 0.63395965 0.655601 0.68166113 0.70392168 0.69052017 0.63851017 0.58367771 0.56880474 0.58745891 0.59164637 0.56285328 0.50952834][0.5983628 0.65507847 0.70080841 0.73100346 0.75220686 0.7874639 0.82238668 0.817142 0.76669675 0.70474958 0.67918706 0.67926306 0.65719265 0.60500616 0.53858227][0.67230743 0.72895503 0.76123196 0.78178883 0.80177176 0.84139675 0.88141716 0.88132632 0.83429456 0.76682639 0.72875541 0.70789754 0.66226566 0.59418559 0.52486074][0.68419045 0.72155917 0.72328484 0.72436196 0.73768723 0.77568018 0.81677675 0.82644117 0.79505581 0.73714006 0.69916356 0.66986245 0.61686003 0.54895878 0.48876148][0.63644862 0.63891488 0.6031214 0.5802725 0.58191556 0.610388 0.64505827 0.66276181 0.652786 0.61658651 0.59418505 0.57653475 0.53915828 0.48940921 0.44674623][0.56098247 0.53211462 0.46694189 0.42524058 0.41420087 0.42797208 0.4496448 0.46898493 0.47698298 0.46495804 0.46615937 0.47462374 0.46915442 0.44706711 0.42140096][0.4792124 0.42684922 0.3460803 0.29617152 0.27915344 0.28398922 0.29762051 0.31807691 0.338715 0.34618342 0.368015 0.40270698 0.42964303 0.43397114 0.42122307][0.39251289 0.32816073 0.24487174 0.19496463 0.1773126 0.18127412 0.19649006 0.22285314 0.25403795 0.27598116 0.31076905 0.36144057 0.40910706 0.43079543 0.42586187][0.3010858 0.2441524 0.17526145 0.13463357 0.12048747 0.12721342 0.147121 0.17860897 0.21428145 0.24104759 0.27560255 0.32503438 0.37574008 0.40251985 0.40055078]]...]
INFO - root - 2017-12-11 04:15:17.918626: step 5510, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:45m:47s remains)
INFO - root - 2017-12-11 04:15:23.284177: step 5520, loss = 0.72, batch loss = 0.66 (15.5 examples/sec; 0.517 sec/batch; 46h:55m:24s remains)
INFO - root - 2017-12-11 04:15:28.598524: step 5530, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 47h:56m:03s remains)
INFO - root - 2017-12-11 04:15:33.879996: step 5540, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 50h:00m:26s remains)
INFO - root - 2017-12-11 04:15:39.248259: step 5550, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 48h:31m:03s remains)
INFO - root - 2017-12-11 04:15:44.607693: step 5560, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 48h:34m:34s remains)
INFO - root - 2017-12-11 04:15:49.927964: step 5570, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 51h:33m:07s remains)
INFO - root - 2017-12-11 04:15:55.340998: step 5580, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 50h:05m:26s remains)
INFO - root - 2017-12-11 04:16:00.702113: step 5590, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 48h:08m:49s remains)
INFO - root - 2017-12-11 04:16:06.040227: step 5600, loss = 0.72, batch loss = 0.66 (15.8 examples/sec; 0.507 sec/batch; 46h:03m:42s remains)
2017-12-11 04:16:06.642823: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024889715 0.036524922 0.045316909 0.050992955 0.054512218 0.054423258 0.048613872 0.03644402 0.021508893 0.0077457991 -0.0022715856 -0.0082018757 -0.012298412 -0.017836722 -0.02710223][0.089058995 0.10946591 0.12401841 0.13578358 0.14669502 0.15313672 0.14995584 0.13376209 0.107669 0.079106852 0.055255655 0.039568089 0.029177867 0.017813234 0.00053374865][0.16081756 0.19059025 0.21238826 0.23503618 0.26106322 0.2833862 0.2903161 0.27260518 0.23155351 0.180384 0.13446987 0.10309128 0.083117746 0.064248145 0.037557952][0.2201975 0.25721228 0.28640389 0.32365876 0.37260279 0.42133796 0.44804233 0.43378407 0.37708852 0.29815829 0.22305131 0.16972591 0.13637856 0.10863014 0.07231231][0.26016736 0.29928413 0.33295807 0.38541806 0.4615685 0.54304433 0.59619051 0.5907045 0.52148825 0.41421023 0.30663344 0.22788948 0.17952153 0.1430479 0.098470539][0.28111377 0.31503817 0.34611693 0.40880293 0.509003 0.62196487 0.70342988 0.71245342 0.63869745 0.51066852 0.37685642 0.27668649 0.21560523 0.17139781 0.1192252][0.29395577 0.32010248 0.34458333 0.41087189 0.52571392 0.66064072 0.76558417 0.791488 0.72221684 0.58536941 0.4366557 0.32229608 0.2513603 0.19931865 0.13870502][0.29822475 0.31875318 0.33644912 0.39936268 0.51494789 0.65560818 0.77206194 0.811522 0.75349259 0.62104386 0.47135049 0.35256526 0.27633959 0.21809921 0.15027332][0.29034883 0.30846497 0.31989479 0.37262124 0.47549677 0.6051476 0.71795583 0.76283395 0.71763557 0.5994395 0.46152198 0.34970564 0.27573317 0.21661438 0.14634502][0.26344782 0.28018922 0.28499147 0.32255051 0.40312022 0.50955808 0.60715282 0.65151507 0.62060308 0.52437609 0.40797138 0.3116377 0.24584472 0.19084568 0.12380366][0.20926127 0.22465216 0.22439432 0.24671943 0.30236825 0.3807576 0.45788717 0.49856809 0.48262921 0.41285169 0.32340729 0.24610524 0.18995479 0.14135526 0.082721263][0.12710771 0.13873509 0.13566571 0.14625072 0.18015288 0.23160975 0.28603584 0.31767061 0.31045893 0.26298597 0.19849348 0.14036658 0.09594705 0.058755733 0.016613416][0.027718255 0.030932471 0.024739249 0.027206166 0.045184102 0.075175554 0.10875811 0.12851505 0.12329657 0.090879127 0.046332065 0.00659668 -0.022385499 -0.042388871 -0.061633334][-0.060973637 -0.0672586 -0.07623972 -0.078302875 -0.069461673 -0.052632898 -0.03322085 -0.022045456 -0.026219398 -0.047823306 -0.077667192 -0.10319693 -0.11863336 -0.12335413 -0.12264657][-0.11938602 -0.13250729 -0.14317851 -0.14805864 -0.14520951 -0.13659213 -0.12553591 -0.11851192 -0.12002625 -0.13190158 -0.14932463 -0.16372886 -0.16951071 -0.16452393 -0.15187897]]...]
INFO - root - 2017-12-11 04:16:11.818482: step 5610, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 49h:55m:19s remains)
INFO - root - 2017-12-11 04:16:17.177835: step 5620, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 49h:00m:07s remains)
INFO - root - 2017-12-11 04:16:22.539798: step 5630, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 50h:21m:14s remains)
INFO - root - 2017-12-11 04:16:27.905414: step 5640, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 47h:34m:25s remains)
INFO - root - 2017-12-11 04:16:33.171885: step 5650, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 47h:21m:24s remains)
INFO - root - 2017-12-11 04:16:38.581731: step 5660, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 48h:07m:22s remains)
INFO - root - 2017-12-11 04:16:43.962614: step 5670, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 48h:05m:37s remains)
INFO - root - 2017-12-11 04:16:49.340095: step 5680, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:42m:08s remains)
INFO - root - 2017-12-11 04:16:54.568551: step 5690, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:40m:14s remains)
INFO - root - 2017-12-11 04:16:59.957234: step 5700, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 48h:20m:52s remains)
2017-12-11 04:17:00.500604: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076911367 0.083291978 0.095524505 0.11027545 0.12452142 0.13777897 0.14728796 0.15197156 0.15187491 0.15120991 0.15122631 0.14998786 0.1445936 0.13428409 0.11804137][0.082423419 0.0888294 0.10148817 0.11907087 0.1388461 0.15758397 0.16957004 0.1733184 0.16980603 0.16406025 0.15991697 0.15541622 0.1468669 0.13135591 0.11052999][0.079929 0.081303261 0.087949865 0.10098722 0.11928347 0.13878962 0.15138645 0.15413994 0.14847842 0.13868955 0.12960535 0.12010182 0.10758978 0.089242451 0.068465441][0.079877377 0.072919652 0.068714455 0.071286887 0.082359321 0.098544575 0.11040978 0.1128386 0.10685454 0.095684476 0.083853908 0.070979007 0.056734607 0.03972926 0.02335754][0.081237614 0.0673282 0.052664764 0.044717088 0.048027057 0.060262769 0.071358688 0.073779218 0.067623615 0.055778865 0.042238165 0.027536483 0.013599568 0.00069636537 -0.0095858537][0.082048468 0.063682757 0.043101508 0.029380748 0.029002426 0.040372189 0.052972812 0.056910343 0.050965942 0.038577702 0.023478037 0.0071195811 -0.0072172857 -0.017837241 -0.024704641][0.080037147 0.060738634 0.039840151 0.02649009 0.027865335 0.042626437 0.059699014 0.0670685 0.061762005 0.048068024 0.030030567 0.01016941 -0.0073241848 -0.019367816 -0.026408754][0.07822527 0.060174145 0.042113557 0.032664586 0.039001383 0.059732027 0.083095819 0.095512487 0.092066869 0.077146 0.055033319 0.029531721 0.0060906126 -0.010763205 -0.021010838][0.078038342 0.062419664 0.048355706 0.044130679 0.056585427 0.083851032 0.11306039 0.1298333 0.12788606 0.11129957 0.084709369 0.053010225 0.02309815 0.000624218 -0.013258546][0.0794296 0.066532627 0.055127826 0.054563269 0.071294114 0.1026789 0.134858 0.15365958 0.15219364 0.13383766 0.10378361 0.067871384 0.034017626 0.0082721179 -0.0072513623][0.081598185 0.070460469 0.059646584 0.059802853 0.077160276 0.10867643 0.14033918 0.15895884 0.15767011 0.13874464 0.10751693 0.070601523 0.036523271 0.011001868 -0.003697037][0.082288094 0.07194785 0.060295105 0.058905419 0.073673025 0.10178142 0.13006239 0.14652546 0.14477143 0.12602364 0.095546961 0.060597919 0.029675394 0.0074752066 -0.004388405][0.080989093 0.070393071 0.05729825 0.052889988 0.062445581 0.0840701 0.10646483 0.1188227 0.11550073 0.097342886 0.0696538 0.039795961 0.014958436 -0.0018374177 -0.0098055573][0.077971995 0.06689024 0.052287884 0.044133492 0.0470162 0.060453463 0.075644277 0.083009154 0.077683 0.060865596 0.03795946 0.015600318 -0.0014435502 -0.012113467 -0.016236763][0.07744395 0.065635793 0.049356539 0.037214987 0.033262573 0.03847554 0.046601266 0.049269315 0.042266987 0.027489664 0.010320638 -0.0041388539 -0.013854856 -0.019269528 -0.020409698]]...]
INFO - root - 2017-12-11 04:17:05.523735: step 5710, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 47h:04m:48s remains)
INFO - root - 2017-12-11 04:17:10.834827: step 5720, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 48h:35m:56s remains)
INFO - root - 2017-12-11 04:17:16.183146: step 5730, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 47h:50m:48s remains)
INFO - root - 2017-12-11 04:17:21.540884: step 5740, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:37m:19s remains)
INFO - root - 2017-12-11 04:17:26.853931: step 5750, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:24m:03s remains)
INFO - root - 2017-12-11 04:17:32.174075: step 5760, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:28m:04s remains)
INFO - root - 2017-12-11 04:17:37.585522: step 5770, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 48h:31m:35s remains)
INFO - root - 2017-12-11 04:17:42.870860: step 5780, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 49h:27m:21s remains)
INFO - root - 2017-12-11 04:17:48.254496: step 5790, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:46m:25s remains)
INFO - root - 2017-12-11 04:17:53.663965: step 5800, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.570 sec/batch; 51h:43m:15s remains)
2017-12-11 04:17:54.203550: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31889766 0.31253365 0.32285985 0.349678 0.37354511 0.37467879 0.35174349 0.31642488 0.28292733 0.26336387 0.26175532 0.27662581 0.29573014 0.32102236 0.36224657][0.32780141 0.34886488 0.37718984 0.41134205 0.42919233 0.41426226 0.3699832 0.31845826 0.28050026 0.26747665 0.27678311 0.29836687 0.31702819 0.33492687 0.36864954][0.33547765 0.38196281 0.4220843 0.4547739 0.46174318 0.43154794 0.37106374 0.31045717 0.27421737 0.27250704 0.29511237 0.32414961 0.34179476 0.3479071 0.3623262][0.35750875 0.41473052 0.45570213 0.4811489 0.4802587 0.44640109 0.38572529 0.32969415 0.30132693 0.30834156 0.33671024 0.36568996 0.37768814 0.36899436 0.35874972][0.38055372 0.43820259 0.47452405 0.49463966 0.49622318 0.4754304 0.43309882 0.39374471 0.37375292 0.37791047 0.39400858 0.40584603 0.40026471 0.37117103 0.33500582][0.38992724 0.44061804 0.47136673 0.49239406 0.50877768 0.51637447 0.50651908 0.49049234 0.47533035 0.46390525 0.451014 0.43112874 0.39889395 0.3476764 0.29071677][0.3768051 0.41636649 0.44320622 0.47078151 0.50835681 0.54904664 0.57419497 0.58093697 0.568029 0.53754663 0.49283835 0.44044539 0.3841086 0.31819463 0.25255439][0.34410456 0.37345174 0.39959633 0.43607843 0.49199039 0.55727535 0.60675108 0.62862468 0.61735845 0.57483757 0.50963932 0.4372822 0.36940804 0.30218777 0.24162175][0.29709384 0.31942317 0.34610835 0.388 0.45142344 0.525285 0.58345234 0.61092752 0.60138661 0.55687088 0.4867802 0.41073582 0.34545785 0.28842092 0.24194746][0.22969602 0.24542014 0.26950502 0.30933559 0.3686963 0.43808264 0.49457014 0.52286577 0.5166083 0.47845918 0.41618958 0.34925148 0.29589275 0.2543568 0.2241976][0.14235611 0.14881805 0.16533518 0.19660163 0.24527879 0.30401805 0.35443342 0.38186818 0.38003093 0.35240644 0.30442488 0.2524837 0.21374252 0.18736283 0.1706669][0.046763543 0.044585552 0.053455688 0.075753786 0.11337141 0.16030754 0.20221522 0.22543788 0.22413631 0.20344271 0.16783743 0.1295369 0.10227114 0.08615233 0.077213123][-0.045046221 -0.052357335 -0.047071971 -0.030174788 -0.00090654951 0.035385694 0.068148226 0.0853198 0.081690304 0.064433828 0.038375467 0.011098011 -0.0085971514 -0.019927356 -0.02604701][-0.1200956 -0.12877381 -0.12371682 -0.10983091 -0.087861635 -0.061700959 -0.037304502 -0.024354583 -0.027514702 -0.038998008 -0.054011546 -0.070048258 -0.08348611 -0.092939928 -0.098797396][-0.16063623 -0.16952465 -0.16449034 -0.15396544 -0.13958921 -0.12249158 -0.10391622 -0.0908284 -0.08758685 -0.088153474 -0.089444205 -0.092938609 -0.099045515 -0.10588666 -0.11118693]]...]
INFO - root - 2017-12-11 04:17:59.409543: step 5810, loss = 0.70, batch loss = 0.65 (20.9 examples/sec; 0.383 sec/batch; 34h:44m:42s remains)
INFO - root - 2017-12-11 04:18:04.670020: step 5820, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 51h:13m:58s remains)
INFO - root - 2017-12-11 04:18:10.038572: step 5830, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 49h:58m:04s remains)
INFO - root - 2017-12-11 04:18:15.496398: step 5840, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:45m:22s remains)
INFO - root - 2017-12-11 04:18:20.908211: step 5850, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:49m:16s remains)
INFO - root - 2017-12-11 04:18:26.354641: step 5860, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 49h:17m:18s remains)
INFO - root - 2017-12-11 04:18:31.658855: step 5870, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 48h:01m:45s remains)
INFO - root - 2017-12-11 04:18:36.988130: step 5880, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 48h:01m:58s remains)
INFO - root - 2017-12-11 04:18:42.446168: step 5890, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 51h:18m:21s remains)
INFO - root - 2017-12-11 04:18:47.874692: step 5900, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 48h:41m:00s remains)
2017-12-11 04:18:48.408372: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14844786 0.15709487 0.15403925 0.142989 0.12874131 0.11782081 0.11393155 0.11298607 0.10900877 0.10199177 0.094306782 0.0875034 0.082969263 0.087435082 0.10038438][0.20428032 0.22354096 0.22697203 0.21932685 0.20591505 0.19498676 0.19186769 0.19148853 0.18483777 0.17154059 0.15577288 0.14130607 0.13183081 0.13840967 0.15878223][0.24747744 0.27768233 0.28875834 0.28701475 0.27823389 0.27155584 0.27283576 0.27580434 0.26793557 0.24784163 0.22149202 0.19559559 0.17781292 0.18461955 0.21217686][0.27445894 0.31136024 0.32685935 0.32943013 0.32625028 0.32680336 0.33604953 0.34571886 0.33952174 0.31445923 0.27760315 0.23875988 0.21078907 0.21592143 0.24976981][0.28309372 0.32012814 0.33411855 0.33647156 0.33680514 0.34466895 0.36328876 0.38234675 0.38202396 0.35671932 0.3130261 0.26341993 0.22585468 0.22738056 0.26495403][0.27929527 0.31174061 0.31933716 0.31673029 0.31652534 0.32851693 0.3542538 0.38181409 0.38895521 0.36716178 0.3211185 0.26484022 0.22000095 0.21615545 0.25294241][0.27088362 0.29630545 0.29530612 0.28534395 0.28190362 0.29533607 0.32543805 0.35851169 0.37103376 0.35231885 0.30492923 0.24365464 0.19300033 0.18282099 0.21538161][0.25534651 0.27329248 0.26556006 0.25126225 0.24749258 0.26431057 0.29881281 0.33503649 0.34914881 0.32974118 0.27860773 0.21181688 0.15600638 0.13947953 0.16613565][0.22536768 0.2375688 0.22770314 0.21511365 0.21607472 0.23866394 0.27655628 0.31237125 0.3236112 0.29993263 0.24405402 0.17374158 0.11639235 0.097087845 0.11921981][0.1834275 0.19227655 0.18456011 0.17854017 0.18757279 0.21608669 0.25448033 0.28573373 0.29049659 0.26068622 0.20124294 0.13132173 0.077222645 0.059685551 0.07974416][0.1386344 0.14492859 0.1401605 0.14069071 0.15647435 0.18814488 0.22373863 0.24764919 0.24474238 0.20980674 0.15016402 0.084868066 0.037625682 0.025217835 0.04589336][0.091928206 0.0948895 0.091209367 0.094743676 0.11271778 0.14346428 0.17404482 0.19065776 0.18222032 0.14630967 0.091551766 0.034986764 -0.0033189107 -0.0097953668 0.012040238][0.040024012 0.038722754 0.033513322 0.035232529 0.049166605 0.073145129 0.095586233 0.10562614 0.095724985 0.065102942 0.021902611 -0.020291183 -0.046343438 -0.046309069 -0.024291441][-0.008278395 -0.013619345 -0.021856287 -0.02543856 -0.020508593 -0.0084352791 0.0028747893 0.006589313 -0.0019300519 -0.022963509 -0.050202645 -0.074261069 -0.085699789 -0.078805223 -0.057638004][-0.038258553 -0.046066981 -0.056609973 -0.065571308 -0.069965348 -0.069733821 -0.068423174 -0.069201209 -0.074950054 -0.086171851 -0.098843157 -0.10744731 -0.10752077 -0.096959047 -0.079006575]]...]
INFO - root - 2017-12-11 04:18:53.698898: step 5910, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:48m:52s remains)
INFO - root - 2017-12-11 04:18:58.787974: step 5920, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:44m:44s remains)
INFO - root - 2017-12-11 04:19:04.193225: step 5930, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 49h:28m:31s remains)
INFO - root - 2017-12-11 04:19:09.645296: step 5940, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 50h:57m:00s remains)
INFO - root - 2017-12-11 04:19:14.911548: step 5950, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 47h:12m:29s remains)
INFO - root - 2017-12-11 04:19:20.227710: step 5960, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:59m:40s remains)
INFO - root - 2017-12-11 04:19:25.585231: step 5970, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 49h:28m:16s remains)
INFO - root - 2017-12-11 04:19:30.887170: step 5980, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 46h:54m:21s remains)
INFO - root - 2017-12-11 04:19:36.173007: step 5990, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 49h:38m:09s remains)
INFO - root - 2017-12-11 04:19:41.529511: step 6000, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 48h:09m:25s remains)
2017-12-11 04:19:42.126739: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29079679 0.26831183 0.22928455 0.19073646 0.1644526 0.15175879 0.14607905 0.13698329 0.12169334 0.11011968 0.11594944 0.14775728 0.19949949 0.2507745 0.28682521][0.43080744 0.3977699 0.33589137 0.27151278 0.22181192 0.19219151 0.17745025 0.16595927 0.15514088 0.15619177 0.18252011 0.237375 0.30796397 0.36661866 0.39256269][0.52730304 0.49127722 0.41986281 0.34403822 0.28147781 0.23937583 0.21502082 0.19935144 0.19233501 0.20631407 0.25280017 0.32735798 0.41000205 0.46814668 0.47751153][0.55799007 0.52612633 0.46161002 0.39458114 0.33796895 0.29601717 0.26668307 0.24504015 0.23528856 0.25259826 0.30734083 0.3885586 0.47128683 0.52219611 0.51614493][0.53332973 0.51241916 0.46724814 0.42350674 0.38677442 0.35621223 0.32895425 0.3033329 0.2875315 0.29962519 0.34901282 0.42047262 0.48819947 0.523406 0.50405967][0.46742848 0.46231723 0.44235539 0.42771959 0.41701069 0.40577692 0.39097363 0.37187591 0.357698 0.36582106 0.40160981 0.4476144 0.48266074 0.48914608 0.45385328][0.40324867 0.406631 0.40159416 0.40543631 0.41427794 0.42336181 0.43085891 0.43412429 0.43752363 0.45082393 0.47122678 0.48162735 0.47060809 0.43705881 0.3809278][0.37986389 0.37630615 0.36458471 0.36540616 0.37875661 0.40243214 0.4350636 0.47000754 0.50211775 0.52886879 0.53725183 0.51168561 0.45241311 0.3759115 0.296912][0.40534762 0.37991348 0.33920258 0.31409422 0.31418177 0.34284988 0.39846957 0.46938249 0.53859937 0.58932167 0.59724563 0.54706973 0.44928122 0.33508161 0.23321363][0.44598836 0.38896796 0.30785772 0.24510264 0.22217445 0.25005183 0.32671198 0.43394318 0.54273856 0.62268543 0.64029217 0.58060145 0.46210092 0.32538459 0.20803386][0.47391957 0.38169909 0.26131365 0.16418594 0.12039675 0.14711921 0.24098416 0.37689963 0.51559794 0.61737549 0.64524287 0.58664685 0.46551168 0.3258014 0.20644386][0.49210221 0.37165615 0.22228085 0.10330539 0.047396816 0.072586045 0.17227466 0.31734321 0.46434066 0.57150322 0.60445362 0.5538969 0.445444 0.32050607 0.21351409][0.51267678 0.38017666 0.22035988 0.095502518 0.035653353 0.055678591 0.14577655 0.27693692 0.40887916 0.50439334 0.53571862 0.49615762 0.40896505 0.30860153 0.22192186][0.55193245 0.42034596 0.26726887 0.14838825 0.087807983 0.098038927 0.16820146 0.27095094 0.37189806 0.44180033 0.46215624 0.42979231 0.36439914 0.29167283 0.22856729][0.58686179 0.46817639 0.33554524 0.2319407 0.1733654 0.17170238 0.21729141 0.28485548 0.34560323 0.37978923 0.37972134 0.34715676 0.29899251 0.25250253 0.21383114]]...]
INFO - root - 2017-12-11 04:19:47.568635: step 6010, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 49h:20m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:19:52.691544: step 6020, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 49h:05m:56s remains)
INFO - root - 2017-12-11 04:19:58.068884: step 6030, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:35m:49s remains)
INFO - root - 2017-12-11 04:20:03.438497: step 6040, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 47h:22m:56s remains)
INFO - root - 2017-12-11 04:20:08.809182: step 6050, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 48h:31m:01s remains)
INFO - root - 2017-12-11 04:20:14.143799: step 6060, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 49h:52m:40s remains)
INFO - root - 2017-12-11 04:20:19.537069: step 6070, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 49h:13m:38s remains)
INFO - root - 2017-12-11 04:20:24.895299: step 6080, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 48h:02m:56s remains)
INFO - root - 2017-12-11 04:20:30.293124: step 6090, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 48h:16m:58s remains)
INFO - root - 2017-12-11 04:20:35.633201: step 6100, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 48h:06m:28s remains)
2017-12-11 04:20:36.168682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01421477 -0.011332258 -0.0026113254 0.006163463 0.01234575 0.016739968 0.016875463 0.011551848 -0.00016784669 -0.013333825 -0.026270162 -0.0406985 -0.055871058 -0.065464616 -0.066007242][-0.053616233 -0.049928416 -0.0403174 -0.030875547 -0.024411483 -0.021654489 -0.024420969 -0.033893727 -0.049160134 -0.064955525 -0.078589678 -0.0899006 -0.096888721 -0.0951299 -0.084160231][-0.061318114 -0.055303618 -0.042586673 -0.029463371 -0.019612404 -0.015596185 -0.019172063 -0.029737009 -0.045516267 -0.062888205 -0.078664064 -0.090448968 -0.095140956 -0.090415567 -0.077800132][-0.030121885 -0.015052125 0.0068693315 0.028117474 0.044021334 0.051448237 0.048608515 0.037145961 0.019513078 -0.0015854636 -0.022932298 -0.040804591 -0.051607471 -0.05408169 -0.049829][0.02834614 0.059892043 0.097369924 0.13175119 0.15703923 0.16982177 0.16778098 0.15327477 0.1301586 0.10180588 0.07141979 0.042853855 0.020086031 0.0040225754 -0.0060937312][0.089443058 0.13904597 0.19436912 0.24445739 0.28235871 0.3038246 0.30462804 0.28691268 0.25612542 0.21790761 0.17614843 0.13483889 0.09839537 0.067565583 0.042509995][0.13005964 0.19289504 0.2608915 0.32283786 0.37196767 0.40289852 0.40882245 0.39052942 0.354318 0.3084797 0.25822464 0.20780528 0.16152534 0.11963903 0.083425559][0.14059407 0.20749536 0.27857015 0.3437373 0.39761344 0.43407747 0.44442868 0.42799693 0.39110947 0.34411129 0.2930727 0.24167754 0.19304837 0.147068 0.10658898][0.12175626 0.18295811 0.24687356 0.30529249 0.35453829 0.38878304 0.399953 0.38700607 0.35489962 0.31381351 0.26982325 0.22585823 0.18325968 0.14155367 0.10483626][0.082306616 0.12820148 0.17518729 0.21766108 0.25368744 0.27869058 0.28745148 0.279916 0.25807735 0.22857124 0.19619438 0.16390717 0.13231702 0.10128149 0.0752307][0.037096165 0.063024119 0.088048615 0.10965075 0.12815472 0.14135163 0.14765117 0.14811896 0.14096613 0.12722294 0.108941 0.089128591 0.068682678 0.048552126 0.03357939][-0.0019940559 0.0075259758 0.014935836 0.019560533 0.023726987 0.027921716 0.033134628 0.041014861 0.047547337 0.048908997 0.043584567 0.033587079 0.020454075 0.0069777104 -0.0012684814][-0.021447549 -0.020557128 -0.021525964 -0.024614926 -0.026650675 -0.025171122 -0.01901464 -0.006897056 0.0072891652 0.017632946 0.019525094 0.013184703 0.001133739 -0.011943625 -0.019691259][-0.014327493 -0.014836507 -0.01697474 -0.021437014 -0.024327558 -0.021522522 -0.013173056 0.00054183864 0.016814107 0.030204054 0.034231562 0.02706039 0.011844711 -0.0054484592 -0.017749477][0.01289835 0.014454967 0.013134048 0.0080574611 0.0040848018 0.0068966541 0.015339394 0.027893037 0.043220229 0.057304103 0.062513165 0.055061828 0.037635244 0.016458204 -0.0010473884]]...]
INFO - root - 2017-12-11 04:20:41.500487: step 6110, loss = 0.70, batch loss = 0.65 (15.6 examples/sec; 0.513 sec/batch; 46h:30m:33s remains)
INFO - root - 2017-12-11 04:20:46.526840: step 6120, loss = 0.72, batch loss = 0.67 (15.0 examples/sec; 0.532 sec/batch; 48h:12m:00s remains)
INFO - root - 2017-12-11 04:20:51.856094: step 6130, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:34m:47s remains)
INFO - root - 2017-12-11 04:20:57.168633: step 6140, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:57m:40s remains)
INFO - root - 2017-12-11 04:21:02.602181: step 6150, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 50h:13m:37s remains)
INFO - root - 2017-12-11 04:21:07.905565: step 6160, loss = 0.71, batch loss = 0.66 (15.5 examples/sec; 0.516 sec/batch; 46h:44m:41s remains)
INFO - root - 2017-12-11 04:21:13.274580: step 6170, loss = 0.73, batch loss = 0.67 (14.8 examples/sec; 0.540 sec/batch; 48h:57m:37s remains)
INFO - root - 2017-12-11 04:21:18.681958: step 6180, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 48h:27m:19s remains)
INFO - root - 2017-12-11 04:21:24.057368: step 6190, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 49h:29m:14s remains)
INFO - root - 2017-12-11 04:21:29.347425: step 6200, loss = 0.71, batch loss = 0.66 (15.8 examples/sec; 0.507 sec/batch; 45h:55m:14s remains)
2017-12-11 04:21:29.867838: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17019908 0.14963694 0.12614651 0.1141421 0.12178974 0.16320722 0.24100682 0.33410493 0.40814257 0.43564177 0.41119266 0.34048131 0.24225539 0.14025067 0.052063715][0.16032003 0.13088115 0.10032285 0.084892474 0.097280569 0.15269762 0.24806529 0.35456845 0.43348873 0.45816952 0.42377874 0.33784479 0.22316805 0.10817987 0.017298868][0.12139998 0.0851894 0.052037258 0.039159164 0.059934411 0.12826131 0.23479171 0.34615248 0.42500058 0.44500741 0.40370473 0.30990136 0.18894981 0.073432483 -0.0095502557][0.0705288 0.033640314 0.0052091219 0.0017070904 0.034691121 0.11456108 0.22705549 0.33660951 0.40815383 0.41719905 0.365974 0.26766104 0.14945069 0.044714004 -0.021479074][0.023911439 -0.0060328143 -0.023135195 -0.012752818 0.033757035 0.12124716 0.23242757 0.33189276 0.38818175 0.38060832 0.31699148 0.21722811 0.10987133 0.025296804 -0.017698152][-0.0035845721 -0.020906767 -0.023606226 0.0010136357 0.058717661 0.14819489 0.250421 0.33227158 0.36714405 0.34016567 0.26420319 0.16644184 0.076383606 0.018062012 0.0002218876][-0.0038043673 -0.0079661375 0.0024130403 0.038658671 0.10451403 0.19256632 0.28109896 0.3396188 0.34740558 0.29808229 0.21047506 0.11836591 0.050288327 0.020607714 0.025868978][0.023734758 0.029369982 0.048586644 0.09258078 0.16349337 0.24815746 0.32045743 0.35161877 0.32831845 0.25539842 0.15912473 0.077039808 0.033032138 0.029881204 0.053280432][0.066425942 0.076972723 0.099935338 0.1461432 0.21585062 0.2918143 0.34439716 0.34747806 0.29607919 0.20626798 0.11011623 0.043656297 0.022883264 0.040810231 0.077345766][0.11031585 0.12252313 0.14531098 0.1878968 0.24824205 0.30752242 0.33635786 0.31416446 0.24424885 0.1500337 0.0646772 0.017741144 0.016359262 0.048642371 0.094695151][0.15317264 0.16323137 0.18080693 0.21351701 0.25752443 0.29507664 0.30100712 0.26175863 0.18712327 0.1021263 0.036072619 0.0092528462 0.021785131 0.062227044 0.1141257][0.19046253 0.19260758 0.19888598 0.21678557 0.24171379 0.25815323 0.24682768 0.20111555 0.13439758 0.068461984 0.026300697 0.019135041 0.0429627 0.089683965 0.14577395][0.21111894 0.1996254 0.18971233 0.19058773 0.1979686 0.1992179 0.18099323 0.1413517 0.092233129 0.049128115 0.029452167 0.038684059 0.071631037 0.12370786 0.18276709][0.20207328 0.17817616 0.15536653 0.1440887 0.14074805 0.1357616 0.12058333 0.094724782 0.065342739 0.042271648 0.039745536 0.060212631 0.098035648 0.15037048 0.2065009][0.16883563 0.13825807 0.11063108 0.096338473 0.092176057 0.090364315 0.084715456 0.073937461 0.060649283 0.05175614 0.060041949 0.086083837 0.12286035 0.1663375 0.20775162]]...]
INFO - root - 2017-12-11 04:21:35.255924: step 6210, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 49h:28m:16s remains)
INFO - root - 2017-12-11 04:21:40.603504: step 6220, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:16m:43s remains)
INFO - root - 2017-12-11 04:21:45.635392: step 6230, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:33m:42s remains)
INFO - root - 2017-12-11 04:21:51.002677: step 6240, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 47h:45m:06s remains)
INFO - root - 2017-12-11 04:21:56.289769: step 6250, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 47h:35m:59s remains)
INFO - root - 2017-12-11 04:22:01.639214: step 6260, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 49h:27m:41s remains)
INFO - root - 2017-12-11 04:22:07.059162: step 6270, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:37m:42s remains)
INFO - root - 2017-12-11 04:22:12.449547: step 6280, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 48h:17m:32s remains)
INFO - root - 2017-12-11 04:22:17.696703: step 6290, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 46h:43m:00s remains)
INFO - root - 2017-12-11 04:22:23.115393: step 6300, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:40m:37s remains)
2017-12-11 04:22:23.670924: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11506591 0.096454546 0.08223249 0.078995146 0.085591085 0.10731518 0.14276701 0.17864491 0.21372993 0.24552725 0.27813667 0.30771881 0.32951117 0.34659094 0.36024091][0.11354117 0.092607483 0.074231833 0.068823121 0.07404054 0.095749341 0.13125627 0.17083734 0.20901851 0.23738918 0.25778446 0.2712914 0.27955797 0.28853157 0.30035415][0.13619481 0.11797456 0.098147556 0.089141555 0.0916829 0.11133201 0.14575042 0.18584998 0.22225352 0.24282286 0.24755327 0.24146247 0.23358744 0.23327482 0.24320106][0.18784666 0.17712513 0.16150574 0.15485801 0.16069297 0.18379191 0.2206717 0.2597039 0.28989178 0.29925239 0.28741577 0.262698 0.23963013 0.22994405 0.23555858][0.24770017 0.24944186 0.24773408 0.25539526 0.27672908 0.31343329 0.35909775 0.39754128 0.41958016 0.41703179 0.39114141 0.35290927 0.3188158 0.30014253 0.29599854][0.3029809 0.32224917 0.34317094 0.37457588 0.41967329 0.47525689 0.53043753 0.56492865 0.57546866 0.56116593 0.52605069 0.48201218 0.44399253 0.42010474 0.40494719][0.35647446 0.39652407 0.44257069 0.49781445 0.56391269 0.63277513 0.68774664 0.70713121 0.69724196 0.66794658 0.62560415 0.57963967 0.54172915 0.51732695 0.49722493][0.40329561 0.46012592 0.52642125 0.59678239 0.671078 0.73796183 0.776124 0.76635379 0.72581524 0.67648685 0.62623197 0.57937807 0.54403746 0.52537042 0.51323956][0.4114939 0.47698224 0.55290985 0.62580973 0.69315213 0.74227625 0.751276 0.70683008 0.63436753 0.56524545 0.50827628 0.46328634 0.43553856 0.4319056 0.44417492][0.35645148 0.41800496 0.48939365 0.55153036 0.59966016 0.62226957 0.60151869 0.53078604 0.43829879 0.35873482 0.30134821 0.26390278 0.25057393 0.27088696 0.31969339][0.23945323 0.28534338 0.3397828 0.38232622 0.40751383 0.40630195 0.36619905 0.2855773 0.19084826 0.11502674 0.067229562 0.044812091 0.051602624 0.098737411 0.18362257][0.09069103 0.11538381 0.14831577 0.17053522 0.17779925 0.16393806 0.12074778 0.048626855 -0.030777421 -0.088267289 -0.11490662 -0.11323022 -0.081196189 -0.0098272059 0.098363653][-0.046819095 -0.0382868 -0.021599595 -0.012696477 -0.013958829 -0.028802821 -0.0618117 -0.11238936 -0.16553447 -0.19667047 -0.19674975 -0.16714558 -0.10893738 -0.019597042 0.094648086][-0.12705456 -0.12606779 -0.11781728 -0.11451855 -0.11727321 -0.12744002 -0.14653075 -0.17387399 -0.20061882 -0.20723331 -0.18514469 -0.13353784 -0.055592425 0.043481227 0.15027469][-0.13680221 -0.13785857 -0.13276856 -0.13092637 -0.13321097 -0.13972665 -0.14977403 -0.1619408 -0.17089224 -0.16193876 -0.12814981 -0.0665527 0.02054554 0.12232581 0.21759532]]...]
INFO - root - 2017-12-11 04:22:28.932120: step 6310, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:59m:45s remains)
INFO - root - 2017-12-11 04:22:34.329831: step 6320, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 49h:17m:13s remains)
INFO - root - 2017-12-11 04:22:39.428175: step 6330, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:20m:28s remains)
INFO - root - 2017-12-11 04:22:44.717453: step 6340, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:54m:49s remains)
INFO - root - 2017-12-11 04:22:50.066366: step 6350, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 49h:34m:52s remains)
INFO - root - 2017-12-11 04:22:55.461832: step 6360, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 51h:01m:05s remains)
INFO - root - 2017-12-11 04:23:00.908462: step 6370, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 51h:32m:42s remains)
INFO - root - 2017-12-11 04:23:06.253071: step 6380, loss = 0.71, batch loss = 0.66 (15.4 examples/sec; 0.518 sec/batch; 46h:56m:14s remains)
INFO - root - 2017-12-11 04:23:11.567227: step 6390, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 49h:20m:24s remains)
INFO - root - 2017-12-11 04:23:16.917355: step 6400, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:57m:15s remains)
2017-12-11 04:23:17.435249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15049033 0.13498764 0.12162009 0.12248601 0.13874356 0.17158979 0.209951 0.24128148 0.2597051 0.264068 0.26035097 0.2526021 0.24705584 0.24310975 0.23304597][0.14675257 0.13015135 0.11866397 0.1267691 0.15363935 0.19547349 0.23823366 0.27003145 0.28796893 0.29271674 0.29054555 0.28400689 0.27910477 0.27466363 0.26212835][0.13193476 0.11596687 0.1093965 0.1277124 0.16676234 0.21696489 0.26049286 0.2874161 0.2998867 0.30205137 0.30088213 0.29645765 0.29401046 0.29084471 0.27788258][0.12975593 0.11576941 0.11516316 0.1426449 0.19105928 0.24582574 0.28712684 0.30776608 0.314415 0.31410137 0.31442711 0.31227234 0.31130806 0.3070949 0.29122365][0.13932176 0.12860452 0.13205345 0.16409183 0.21633865 0.27148074 0.30996621 0.32744759 0.33238435 0.33313397 0.3367058 0.33658051 0.33426589 0.32391837 0.29993844][0.15852638 0.14906132 0.15214296 0.18311821 0.23458131 0.28818572 0.32566693 0.34456682 0.35237533 0.35729927 0.36426562 0.36383772 0.35610098 0.33478633 0.29862759][0.18535042 0.17252052 0.16920905 0.19390105 0.24206801 0.2948747 0.33406773 0.3568815 0.36864668 0.37603933 0.38215855 0.37646461 0.3596161 0.32608369 0.27867448][0.20825773 0.18881562 0.17494795 0.1893553 0.23196468 0.28483325 0.32893538 0.35905805 0.37666708 0.38471743 0.38473424 0.36818323 0.33914205 0.29463291 0.24081147][0.2056949 0.18036816 0.157063 0.16057594 0.1959485 0.24832609 0.29921913 0.34065041 0.3678135 0.37850612 0.37254494 0.34566402 0.30688694 0.25691202 0.20374115][0.1702102 0.14065044 0.11178977 0.10700496 0.13531576 0.18590295 0.2424127 0.29463735 0.33141097 0.34605679 0.33712751 0.30590394 0.26644117 0.2222722 0.18053503][0.11239487 0.08107277 0.051253937 0.042420667 0.065240018 0.11203122 0.16872147 0.22405155 0.26354498 0.27858377 0.26861623 0.24042618 0.2116387 0.18709135 0.16942334][0.052592121 0.02230208 -0.0049679531 -0.014829426 0.0035113641 0.044778075 0.096181966 0.14625408 0.18062377 0.19219384 0.1833228 0.1652794 0.15636072 0.16085984 0.17455539][-0.003078026 -0.028687162 -0.049817484 -0.057725843 -0.042531531 -0.0072709965 0.036468916 0.077980109 0.10444639 0.11252916 0.10827555 0.10530712 0.11969388 0.15321335 0.19501509][-0.043034814 -0.062175307 -0.075480357 -0.079026118 -0.065020256 -0.035693072 -0.0014165039 0.028725743 0.044784915 0.048188888 0.049070258 0.061520167 0.09709774 0.15321171 0.21427478][-0.058000315 -0.070507035 -0.076926708 -0.07595145 -0.0622407 -0.03835161 -0.014333779 0.0025163919 0.0062838746 0.0026475298 0.0049313311 0.026245274 0.073786564 0.14044267 0.20796382]]...]
INFO - root - 2017-12-11 04:23:22.751358: step 6410, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 47h:02m:58s remains)
INFO - root - 2017-12-11 04:23:28.163698: step 6420, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:34m:08s remains)
INFO - root - 2017-12-11 04:23:33.291962: step 6430, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:56m:48s remains)
INFO - root - 2017-12-11 04:23:38.611580: step 6440, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 48h:13m:57s remains)
INFO - root - 2017-12-11 04:23:43.962173: step 6450, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 48h:28m:33s remains)
INFO - root - 2017-12-11 04:23:49.186712: step 6460, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 48h:20m:37s remains)
INFO - root - 2017-12-11 04:23:54.554567: step 6470, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 48h:02m:04s remains)
INFO - root - 2017-12-11 04:23:59.952524: step 6480, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 51h:58m:52s remains)
INFO - root - 2017-12-11 04:24:05.196823: step 6490, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:31m:54s remains)
INFO - root - 2017-12-11 04:24:10.550623: step 6500, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:52m:46s remains)
2017-12-11 04:24:11.068839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0095362552 0.0016130448 0.025197687 0.062146403 0.11749717 0.19965534 0.30465528 0.41112649 0.49027857 0.51900977 0.48595038 0.39527151 0.2747958 0.15893357 0.073210709][0.00085247809 0.016787477 0.050473392 0.1025192 0.17873435 0.28689224 0.41813582 0.54244888 0.62495387 0.64100975 0.58487904 0.46674728 0.32060459 0.1846244 0.087206677][0.01320279 0.03365802 0.077476934 0.14498243 0.24009794 0.36773551 0.51422304 0.64490604 0.72230279 0.72252405 0.64489484 0.50491673 0.34061149 0.19272429 0.091468267][0.018925065 0.038774293 0.088167086 0.1673654 0.27648661 0.41622847 0.56982666 0.70232463 0.77574515 0.76646841 0.67647684 0.52385145 0.34777391 0.19231403 0.090229534][0.022570008 0.03867789 0.088306628 0.1722865 0.28710806 0.42927602 0.58035159 0.70801127 0.77552825 0.76076823 0.66697711 0.513532 0.33736852 0.18482958 0.089867167][0.029907655 0.0439679 0.092933536 0.17773037 0.29255107 0.4303295 0.57077855 0.68399465 0.73670363 0.71122926 0.61382675 0.46647143 0.30284074 0.16768597 0.092000745][0.03982956 0.055336062 0.10696588 0.19495176 0.31073976 0.44250393 0.56715018 0.65680707 0.68408751 0.63954234 0.5358814 0.39761198 0.25458616 0.14658511 0.097917646][0.0512054 0.0708247 0.13072833 0.22950907 0.3535 0.48281154 0.59030652 0.649867 0.64236087 0.571575 0.45988679 0.33193141 0.21204066 0.13216327 0.10872201][0.060742572 0.085551463 0.15550944 0.26760229 0.4029032 0.532779 0.62616175 0.65779507 0.61699075 0.52097559 0.40077084 0.28106582 0.18187544 0.12633447 0.1219032][0.066504471 0.09620887 0.17272016 0.29154733 0.43031904 0.55434155 0.63199627 0.64052993 0.57631123 0.46471742 0.34003475 0.22883794 0.15019631 0.11767977 0.12873496][0.064198658 0.096136145 0.17303666 0.29003334 0.42229286 0.53205764 0.59055614 0.58066338 0.50452489 0.38961256 0.27006543 0.17391951 0.11847594 0.10777831 0.13081016][0.045156892 0.072676331 0.14222652 0.25042525 0.37045729 0.46373183 0.50526148 0.48555982 0.40997013 0.30361679 0.19921516 0.12378906 0.089505091 0.092799261 0.11941373][0.006051491 0.020246578 0.071405061 0.16032231 0.26208144 0.34106848 0.37499583 0.36065134 0.30179396 0.21686384 0.13423809 0.078765571 0.058128398 0.064958528 0.086396985][-0.041600194 -0.047870807 -0.026217936 0.029468048 0.10197472 0.1631458 0.19355647 0.19394223 0.16392194 0.1118039 0.057353739 0.021999527 0.012373616 0.020577019 0.036408532][-0.080043785 -0.10348991 -0.10926119 -0.088586964 -0.049986217 -0.012652713 0.0096471468 0.019921413 0.014772847 -0.0056121754 -0.030581893 -0.043932658 -0.04021059 -0.026813582 -0.011711966]]...]
INFO - root - 2017-12-11 04:24:16.406521: step 6510, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 49h:20m:18s remains)
INFO - root - 2017-12-11 04:24:21.792723: step 6520, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 50h:03m:29s remains)
INFO - root - 2017-12-11 04:24:27.127380: step 6530, loss = 0.69, batch loss = 0.63 (17.2 examples/sec; 0.465 sec/batch; 42h:04m:26s remains)
INFO - root - 2017-12-11 04:24:32.251704: step 6540, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 47h:45m:39s remains)
INFO - root - 2017-12-11 04:24:37.557804: step 6550, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 48h:38m:19s remains)
INFO - root - 2017-12-11 04:24:42.799528: step 6560, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:32m:56s remains)
INFO - root - 2017-12-11 04:24:48.161698: step 6570, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 47h:31m:44s remains)
INFO - root - 2017-12-11 04:24:53.612003: step 6580, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 47h:50m:42s remains)
INFO - root - 2017-12-11 04:24:58.977080: step 6590, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 49h:29m:27s remains)
INFO - root - 2017-12-11 04:25:04.198321: step 6600, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:41m:11s remains)
2017-12-11 04:25:04.731180: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10371302 0.19903351 0.31750748 0.44145229 0.54112095 0.6108914 0.64266276 0.63462794 0.59607869 0.56046331 0.5595783 0.57501465 0.59649354 0.60738528 0.60602289][0.10830995 0.21691088 0.35199028 0.492102 0.60650021 0.68920618 0.73176116 0.72967517 0.69198084 0.65373158 0.64847547 0.65686476 0.67195779 0.68395156 0.69013083][0.097606115 0.20973477 0.35075477 0.49672312 0.6182096 0.70786172 0.75750905 0.76092374 0.72581816 0.68577069 0.67281342 0.66970193 0.67312235 0.68014419 0.68752432][0.084791005 0.19533451 0.33566236 0.48025563 0.60083765 0.68816423 0.736793 0.74163973 0.70980316 0.67132509 0.65379041 0.64242518 0.63387114 0.63031691 0.6295315][0.075267233 0.18388464 0.3228763 0.46451169 0.58053362 0.66031778 0.7027328 0.70510435 0.67629874 0.64288 0.62551332 0.610189 0.5902527 0.57171077 0.55593938][0.069118761 0.17739876 0.31896466 0.46200269 0.57699847 0.65134209 0.68839127 0.68712813 0.65975422 0.6313749 0.61647731 0.60055506 0.571412 0.53648567 0.50330859][0.066634879 0.17713438 0.32649222 0.47756669 0.598683 0.67410392 0.70978254 0.70454645 0.67239869 0.64066607 0.62200928 0.60434103 0.56939983 0.52215564 0.47580272][0.067200318 0.18013379 0.33669302 0.49600762 0.62516195 0.706363 0.74601734 0.74027079 0.70159525 0.66130996 0.63472134 0.61432177 0.57808423 0.526365 0.47652817][0.063141309 0.17269398 0.32822457 0.48834908 0.62114495 0.70870906 0.7574169 0.75739896 0.71698719 0.67171061 0.64182752 0.62347472 0.59254968 0.54763705 0.50825453][0.049427692 0.14607359 0.28890973 0.4387863 0.56739187 0.65771139 0.71584994 0.72460645 0.68868226 0.64716488 0.62384737 0.61591238 0.59770352 0.5692268 0.54978442][0.029612595 0.10726807 0.22881898 0.35949448 0.47550046 0.56080806 0.62170315 0.6365304 0.60872161 0.57810217 0.56861246 0.5763247 0.57443219 0.56438369 0.56285709][0.0035824433 0.062115114 0.16102488 0.26999724 0.36821234 0.44127145 0.49617243 0.51116782 0.49016896 0.47067049 0.47502518 0.49688724 0.5092448 0.51363754 0.52175564][-0.032272555 0.007467316 0.08384 0.16998839 0.24675906 0.30191174 0.34370023 0.35375449 0.33752447 0.32673192 0.34100991 0.37223995 0.39588487 0.41240889 0.42610475][-0.075170994 -0.054061625 -0.00016971208 0.062864207 0.11792992 0.15523551 0.18323752 0.18696149 0.17285007 0.16541103 0.18155134 0.21405758 0.24301514 0.26779544 0.28419676][-0.10945188 -0.10295349 -0.068011314 -0.024309717 0.01395093 0.0389834 0.056941945 0.055470355 0.041119713 0.031727657 0.041127615 0.064993359 0.088857211 0.11079261 0.1202395]]...]
INFO - root - 2017-12-11 04:25:10.092282: step 6610, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:43m:25s remains)
INFO - root - 2017-12-11 04:25:15.365215: step 6620, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.557 sec/batch; 50h:27m:55s remains)
INFO - root - 2017-12-11 04:25:20.805072: step 6630, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 50h:32m:05s remains)
INFO - root - 2017-12-11 04:25:25.915542: step 6640, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.515 sec/batch; 46h:35m:44s remains)
INFO - root - 2017-12-11 04:25:31.296511: step 6650, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 46h:30m:20s remains)
INFO - root - 2017-12-11 04:25:36.737400: step 6660, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 49h:48m:22s remains)
INFO - root - 2017-12-11 04:25:42.075649: step 6670, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 47h:05m:43s remains)
INFO - root - 2017-12-11 04:25:47.390670: step 6680, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.511 sec/batch; 46h:17m:35s remains)
INFO - root - 2017-12-11 04:25:52.732136: step 6690, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 48h:01m:38s remains)
INFO - root - 2017-12-11 04:25:58.153843: step 6700, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:34m:27s remains)
2017-12-11 04:25:58.680534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028260496 0.013996846 0.076614566 0.14366452 0.19720973 0.227172 0.23217334 0.22346812 0.21632925 0.21953742 0.23062111 0.23527563 0.22219965 0.18732837 0.13660169][-0.036068257 0.0021624682 0.061667144 0.12530184 0.17953958 0.21666755 0.23632319 0.24773179 0.26335227 0.29128352 0.322145 0.33603981 0.31982103 0.27218938 0.20110554][-0.03925129 -0.0046943133 0.050172869 0.10924353 0.16373266 0.20892811 0.24477042 0.27873719 0.31943294 0.37262574 0.42339242 0.44606566 0.42706525 0.36766586 0.27730274][-0.036360133 -0.00054390717 0.053760462 0.11287581 0.17395362 0.2331742 0.28860387 0.34426171 0.40332547 0.47013149 0.52719623 0.54723638 0.51944935 0.44903824 0.34582695][-0.029772433 0.0098933112 0.066771396 0.13128982 0.20778793 0.28986177 0.37028491 0.44592309 0.51298493 0.57527906 0.61732161 0.61708128 0.57212 0.49254718 0.38679674][-0.022247605 0.022308869 0.085081317 0.16149375 0.2634469 0.37810469 0.48902628 0.58219093 0.64633185 0.68572426 0.69188017 0.6552254 0.58646131 0.49953231 0.39930859][-0.0156996 0.033844925 0.1057763 0.20048301 0.33556777 0.48889467 0.63139004 0.73597795 0.7845785 0.78470051 0.74023747 0.659683 0.56734329 0.47695968 0.38758743][-0.014314759 0.036711473 0.11627283 0.22920156 0.39529073 0.58241326 0.74739665 0.85168928 0.87308025 0.82562888 0.73022747 0.61414993 0.5097335 0.423471 0.34813082][-0.020543396 0.026714236 0.10776115 0.23059157 0.41310719 0.61490422 0.78125775 0.86946356 0.85917556 0.77186954 0.64393741 0.51539487 0.41823426 0.34647048 0.28776634][-0.031953417 0.0072797625 0.082291089 0.20224985 0.37936515 0.56917095 0.71225637 0.77254063 0.7379033 0.63472915 0.50540185 0.39114818 0.31697923 0.26598153 0.22366112][-0.047030739 -0.017961182 0.04454989 0.1495463 0.30118594 0.45569879 0.55732387 0.58472157 0.53679508 0.44096205 0.335751 0.25373998 0.20934537 0.17989974 0.15255049][-0.065729514 -0.0490399 -0.0046090167 0.075709857 0.18843278 0.29480764 0.34938827 0.34708235 0.29582119 0.22014794 0.15020323 0.10640524 0.092292175 0.083756216 0.070899121][-0.082550466 -0.076809146 -0.050055582 0.0032482273 0.074723668 0.13368244 0.14766255 0.12353467 0.074110463 0.019512895 -0.018282466 -0.029164007 -0.01770781 -0.0071873097 -0.0060202354][-0.088828772 -0.085230485 -0.06609381 -0.03085083 0.0094060143 0.032686494 0.018905172 -0.01911989 -0.06665495 -0.10689881 -0.12445851 -0.1163561 -0.0913676 -0.06960123 -0.058505625][-0.083222345 -0.073243193 -0.050341882 -0.020255942 0.0038697703 0.0066518118 -0.022222292 -0.067987889 -0.11537299 -0.1493811 -0.159509 -0.14693063 -0.12155631 -0.098656952 -0.08405897]]...]
INFO - root - 2017-12-11 04:26:04.144266: step 6710, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 47h:09m:25s remains)
INFO - root - 2017-12-11 04:26:09.531279: step 6720, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 47h:22m:21s remains)
INFO - root - 2017-12-11 04:26:14.910444: step 6730, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.517 sec/batch; 46h:48m:22s remains)
INFO - root - 2017-12-11 04:26:19.972077: step 6740, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:41m:16s remains)
INFO - root - 2017-12-11 04:26:25.260578: step 6750, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:18m:10s remains)
INFO - root - 2017-12-11 04:26:30.559788: step 6760, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 48h:15m:40s remains)
INFO - root - 2017-12-11 04:26:35.983120: step 6770, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 50h:42m:11s remains)
INFO - root - 2017-12-11 04:26:41.339122: step 6780, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 49h:20m:42s remains)
INFO - root - 2017-12-11 04:26:46.591551: step 6790, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:53m:02s remains)
INFO - root - 2017-12-11 04:26:51.968737: step 6800, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:54m:42s remains)
2017-12-11 04:26:52.494345: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074107654 0.13738106 0.20942026 0.27069971 0.30508745 0.30784351 0.2861509 0.2460399 0.20480907 0.17624585 0.16157167 0.15007237 0.13668531 0.12821594 0.12691623][0.072055 0.14485072 0.2329872 0.31310561 0.363854 0.37731841 0.35999069 0.31733781 0.26965854 0.23399098 0.21323071 0.19678773 0.17940618 0.1700594 0.1709902][0.057009287 0.13225032 0.22900997 0.32293794 0.38961259 0.41875449 0.41370273 0.37729537 0.32937497 0.28773829 0.2577354 0.23203786 0.20761497 0.19555549 0.19781022][0.035249759 0.10834013 0.20690183 0.30878308 0.39008859 0.43976834 0.45570287 0.43582967 0.39509585 0.34780368 0.30199403 0.25747982 0.21801612 0.19760957 0.19798754][0.014173547 0.082821652 0.17947786 0.28635195 0.38291851 0.4587197 0.50418746 0.510025 0.48193702 0.42709044 0.35637322 0.2802417 0.2147461 0.17854169 0.17352995][0.0043032458 0.0700765 0.16563083 0.2777386 0.39045388 0.49404183 0.57114643 0.60430437 0.58831596 0.52159607 0.41816908 0.30135992 0.20350663 0.14934093 0.14078471][0.0066071171 0.073001564 0.16922249 0.28596863 0.41149944 0.53640908 0.63768852 0.69142669 0.68308342 0.6032511 0.46845722 0.31423059 0.18846272 0.12243731 0.11763934][0.01725821 0.085654475 0.18227202 0.30009952 0.4304142 0.56364024 0.67479706 0.73768824 0.73270804 0.645711 0.49365467 0.31966236 0.18212543 0.11825071 0.12895359][0.033231873 0.10319991 0.19794999 0.31085265 0.43414482 0.55836987 0.66114372 0.72001147 0.71592438 0.63351196 0.48672372 0.31888357 0.19142976 0.14518271 0.18128602][0.053787354 0.12739372 0.22152311 0.32654223 0.43236342 0.53028864 0.6050657 0.64453822 0.63664776 0.56867695 0.44842404 0.31111038 0.21262151 0.19329335 0.25592646][0.070385925 0.15164252 0.2510688 0.35271633 0.4403066 0.50470227 0.53952372 0.54703552 0.52878678 0.47904348 0.39801568 0.30558687 0.24468467 0.25096974 0.32989845][0.078171082 0.17238593 0.28788576 0.3995657 0.48114693 0.51979643 0.51622045 0.48765522 0.45348921 0.41929853 0.37838522 0.33148861 0.30347207 0.32324746 0.39931428][0.078621559 0.18880311 0.32785279 0.46108571 0.550593 0.57758182 0.54615617 0.48571742 0.4334245 0.40788832 0.39823654 0.38760635 0.38206831 0.40318036 0.46175626][0.07492353 0.19865233 0.35814095 0.511158 0.61052954 0.63250345 0.58219266 0.49709341 0.42899349 0.40703657 0.41884023 0.43458363 0.44494483 0.46348831 0.50083733][0.062709138 0.18873383 0.35335842 0.51027828 0.60886246 0.62441725 0.56202179 0.46189296 0.38415614 0.36481223 0.39128548 0.426834 0.45215109 0.47289869 0.49620146]]...]
INFO - root - 2017-12-11 04:26:57.844559: step 6810, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 50h:35m:28s remains)
INFO - root - 2017-12-11 04:27:03.251515: step 6820, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:46m:05s remains)
INFO - root - 2017-12-11 04:27:08.705402: step 6830, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 48h:18m:55s remains)
INFO - root - 2017-12-11 04:27:13.873943: step 6840, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:39m:23s remains)
INFO - root - 2017-12-11 04:27:19.227849: step 6850, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 48h:06m:37s remains)
INFO - root - 2017-12-11 04:27:24.691904: step 6860, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:59m:47s remains)
INFO - root - 2017-12-11 04:27:29.970609: step 6870, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 48h:03m:58s remains)
INFO - root - 2017-12-11 04:27:35.316268: step 6880, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:57m:12s remains)
INFO - root - 2017-12-11 04:27:40.646784: step 6890, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 47h:08m:35s remains)
INFO - root - 2017-12-11 04:27:45.941015: step 6900, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:50m:28s remains)
2017-12-11 04:27:46.494250: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29466289 0.30395174 0.30071759 0.29232615 0.28090522 0.27775544 0.28643635 0.30490279 0.31104121 0.28695059 0.23257637 0.16097838 0.096318565 0.055807661 0.034693446][0.31141952 0.31033489 0.29778054 0.28403613 0.27254653 0.27580824 0.29207686 0.31583366 0.32341242 0.29757532 0.24129905 0.16622129 0.097701468 0.058484126 0.04746718][0.33102009 0.3235316 0.30372441 0.28516409 0.27549312 0.2886911 0.317683 0.35160342 0.36550516 0.34491485 0.29094678 0.21294278 0.13802396 0.096982665 0.093364775][0.35336056 0.34648657 0.32427081 0.30392456 0.29918957 0.32702282 0.37491488 0.42526865 0.44940946 0.43434152 0.37973088 0.29459766 0.20947608 0.16216645 0.16097702][0.36788198 0.36512387 0.34455022 0.32635146 0.33255991 0.38332716 0.45919412 0.53213274 0.5674032 0.55469751 0.49476308 0.39772889 0.29708424 0.23632404 0.23082848][0.3736414 0.3781791 0.36288577 0.35119185 0.37351689 0.45142013 0.55771548 0.65271646 0.695111 0.67847091 0.60797465 0.4961907 0.37885776 0.30189255 0.2869271][0.36866072 0.37960762 0.37393612 0.37444919 0.41465926 0.51409513 0.640538 0.74638283 0.78553224 0.75492448 0.6683107 0.54328614 0.41677529 0.33121309 0.30929202][0.34819216 0.3641631 0.37332895 0.39151442 0.44672251 0.55368912 0.68099147 0.78076047 0.8066172 0.75746769 0.65645337 0.52635074 0.4033311 0.32171494 0.29998314][0.32127163 0.33992177 0.36430991 0.399454 0.4609454 0.55719614 0.66304469 0.740329 0.75025141 0.69210583 0.59262264 0.47357565 0.36597413 0.2962991 0.27806824][0.30743581 0.32850552 0.36531135 0.41212374 0.47019148 0.54068023 0.60635149 0.64703166 0.64095634 0.58853137 0.51132894 0.4210754 0.33811513 0.28352627 0.267567][0.31550464 0.33961952 0.38394186 0.43482029 0.48244563 0.521376 0.54058683 0.53878629 0.51555741 0.475118 0.43012372 0.37901035 0.32885045 0.29565844 0.28434089][0.34474036 0.37649482 0.42412314 0.47054189 0.50254738 0.51262027 0.49423641 0.45858157 0.42157459 0.392021 0.37478274 0.35914925 0.34203264 0.33362153 0.32946625][0.38481686 0.42889753 0.47894251 0.51615018 0.53038758 0.51785547 0.4764131 0.42179951 0.37779257 0.35639486 0.35603297 0.36223641 0.36909795 0.3820442 0.38450396][0.42494828 0.48091543 0.53313118 0.56157875 0.56117809 0.533039 0.47851792 0.41350418 0.36369833 0.34424189 0.34847558 0.36125538 0.38032624 0.40767276 0.41675869][0.457218 0.51712126 0.566775 0.58742952 0.57743716 0.53987145 0.47744152 0.40521839 0.34729156 0.3221873 0.31997412 0.32806093 0.35134414 0.38807765 0.40665522]]...]
INFO - root - 2017-12-11 04:27:51.866502: step 6910, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 47h:45m:46s remains)
INFO - root - 2017-12-11 04:27:57.151392: step 6920, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 47h:42m:55s remains)
INFO - root - 2017-12-11 04:28:02.497815: step 6930, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:52m:08s remains)
INFO - root - 2017-12-11 04:28:07.929720: step 6940, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 48h:01m:47s remains)
INFO - root - 2017-12-11 04:28:13.016400: step 6950, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 48h:18m:42s remains)
INFO - root - 2017-12-11 04:28:18.425304: step 6960, loss = 0.70, batch loss = 0.65 (13.6 examples/sec; 0.586 sec/batch; 53h:00m:01s remains)
INFO - root - 2017-12-11 04:28:23.792748: step 6970, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 49h:25m:27s remains)
INFO - root - 2017-12-11 04:28:29.103168: step 6980, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 47h:14m:57s remains)
INFO - root - 2017-12-11 04:28:34.388672: step 6990, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 47h:42m:42s remains)
INFO - root - 2017-12-11 04:28:39.749494: step 7000, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:15m:31s remains)
2017-12-11 04:28:40.280573: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23207673 0.23873094 0.2604036 0.30140713 0.356434 0.41510859 0.46896994 0.5143007 0.54453689 0.55647576 0.55152607 0.53314441 0.49771512 0.44394302 0.39346421][0.27488253 0.28280693 0.3056877 0.34550196 0.39637598 0.44754255 0.49158022 0.52482635 0.54320854 0.54792851 0.54258037 0.53159952 0.50764477 0.46420059 0.4200395][0.30000198 0.31050381 0.33523655 0.37145188 0.4126907 0.44828358 0.47362167 0.48643127 0.48724625 0.48361233 0.47982612 0.47962335 0.47248903 0.44870889 0.42003673][0.30934289 0.32750997 0.35764772 0.39479262 0.43044603 0.45342493 0.45982376 0.44879574 0.42646211 0.40886569 0.40176108 0.40780458 0.41454268 0.41047993 0.40278852][0.30642429 0.33285806 0.37171829 0.41497195 0.45317581 0.47378188 0.47015753 0.43982691 0.39393795 0.35668364 0.33649728 0.337101 0.3475931 0.35662803 0.36983365][0.30710432 0.34255275 0.3878715 0.43488854 0.47711214 0.5008176 0.49498072 0.45453614 0.3943612 0.34285244 0.30849057 0.29519045 0.2964966 0.30419534 0.32495725][0.33683977 0.37842843 0.42276919 0.46519998 0.50612718 0.53152645 0.5259729 0.48271751 0.42037007 0.36768916 0.32958615 0.3053011 0.29115742 0.28385237 0.29390267][0.39344302 0.43590561 0.4710575 0.5006904 0.53343081 0.55604887 0.54973006 0.50800711 0.45324159 0.41134661 0.38149887 0.35563913 0.32887006 0.30282062 0.29309019][0.43797389 0.48093286 0.50507271 0.51925814 0.53948575 0.55545086 0.5474965 0.51086879 0.46970734 0.44549096 0.43133819 0.41231123 0.38181227 0.34515554 0.32120913][0.44364542 0.48679587 0.50223911 0.5026713 0.50955796 0.51730579 0.50877327 0.48131239 0.4579204 0.45409882 0.45919588 0.45416257 0.43152994 0.39793405 0.37122473][0.40880781 0.45255867 0.46289989 0.45554504 0.45478567 0.458609 0.45282453 0.4362869 0.42939925 0.44303414 0.4659445 0.47844943 0.47145474 0.4497118 0.42681935][0.36444992 0.40431347 0.41195992 0.40206197 0.39911726 0.40233064 0.39998782 0.39255089 0.39769778 0.42315206 0.45798105 0.48499611 0.49424037 0.48698336 0.47152603][0.33346453 0.37136263 0.3789342 0.36964938 0.36645141 0.36644903 0.36112472 0.35563207 0.36598977 0.39644769 0.43455341 0.46660262 0.48388919 0.48588619 0.47812939][0.33599782 0.38034257 0.39190307 0.38275149 0.37518409 0.36509308 0.34899604 0.33760607 0.34552667 0.37356797 0.40659437 0.43335664 0.44744852 0.44910172 0.44436643][0.36153215 0.41452602 0.43182909 0.42087814 0.40459412 0.38091335 0.35276228 0.33422405 0.33704075 0.35851833 0.38182935 0.3987698 0.40509939 0.40136209 0.39476791]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:28:45.664279: step 7010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:13m:27s remains)
INFO - root - 2017-12-11 04:28:51.095055: step 7020, loss = 0.71, batch loss = 0.66 (15.2 examples/sec; 0.525 sec/batch; 47h:28m:51s remains)
INFO - root - 2017-12-11 04:28:56.454128: step 7030, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 50h:50m:16s remains)
INFO - root - 2017-12-11 04:29:01.808701: step 7040, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 47h:31m:33s remains)
INFO - root - 2017-12-11 04:29:06.838472: step 7050, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:15m:10s remains)
INFO - root - 2017-12-11 04:29:12.259746: step 7060, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:34m:49s remains)
INFO - root - 2017-12-11 04:29:17.689886: step 7070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 48h:16m:02s remains)
INFO - root - 2017-12-11 04:29:23.007667: step 7080, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:55m:45s remains)
INFO - root - 2017-12-11 04:29:28.391448: step 7090, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 47h:45m:50s remains)
INFO - root - 2017-12-11 04:29:33.764198: step 7100, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 49h:16m:58s remains)
2017-12-11 04:29:34.301700: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095560066 0.083691962 0.07309556 0.073252305 0.093801148 0.13087523 0.17135043 0.20107326 0.21110259 0.20526102 0.18805493 0.16753045 0.15501785 0.14994246 0.15079895][0.1224676 0.11076431 0.096906178 0.092655994 0.10978431 0.14654131 0.18928045 0.22195885 0.2345226 0.23067747 0.21390165 0.19242196 0.17853676 0.17303877 0.17521006][0.16198808 0.15215412 0.13338086 0.11840659 0.1203561 0.14164764 0.17202003 0.19758557 0.21068735 0.2142918 0.2074236 0.19346109 0.18311624 0.17919877 0.18295673][0.21822125 0.20668562 0.18070398 0.1520634 0.13401131 0.13291727 0.14292759 0.15446302 0.16492485 0.17655882 0.18328784 0.18185765 0.17906772 0.17858833 0.18281376][0.27965882 0.26367235 0.229444 0.19004484 0.15785988 0.14159034 0.13689864 0.13661756 0.14264198 0.15777171 0.17307243 0.18067876 0.18361636 0.18515529 0.18633726][0.32964754 0.30780417 0.26643929 0.22283672 0.18975459 0.17652182 0.17559293 0.17615278 0.18086649 0.19365919 0.206546 0.21159796 0.21177419 0.21071723 0.20667541][0.34775847 0.31981713 0.27454031 0.23420246 0.21329695 0.21987665 0.240287 0.25561377 0.26498669 0.27308059 0.27598447 0.26936412 0.26015466 0.25397885 0.24699518][0.31481385 0.28211769 0.2393083 0.21050063 0.21033572 0.24350545 0.29051405 0.32596129 0.34368604 0.34862202 0.341653 0.32273617 0.30399796 0.29333448 0.28715619][0.23857789 0.20578328 0.17236933 0.16103102 0.1822903 0.23583497 0.30004567 0.34893265 0.37295821 0.37688026 0.36508787 0.33996782 0.31623611 0.30385789 0.30188382][0.17109093 0.14040209 0.11807114 0.12230976 0.15567015 0.21273325 0.27547711 0.32358181 0.34790811 0.35192192 0.34105587 0.31784773 0.29515746 0.28346235 0.28515956][0.16311534 0.13057449 0.11333062 0.12318368 0.15401438 0.19620374 0.23933975 0.27240685 0.28992024 0.29426721 0.28987476 0.27742878 0.26351047 0.25545505 0.25698093][0.21111959 0.17508897 0.1567618 0.16329528 0.18237519 0.20150125 0.21787739 0.22909077 0.23412828 0.23558672 0.23750235 0.23864421 0.23789103 0.23584107 0.23404002][0.26631638 0.23495948 0.21695921 0.21887007 0.22722556 0.22925332 0.22634767 0.22018273 0.21183582 0.20428568 0.202783 0.20712848 0.21241271 0.21400957 0.20964967][0.29161504 0.27777195 0.26666474 0.2664997 0.26797047 0.26209283 0.25184065 0.23847891 0.22129041 0.2026277 0.18959665 0.18556114 0.1868543 0.1880559 0.18394539][0.28537714 0.29181933 0.2906152 0.28967839 0.28581268 0.27669108 0.26721621 0.25690696 0.24036093 0.21781932 0.19774802 0.18670723 0.18359646 0.1836018 0.18048827]]...]
INFO - root - 2017-12-11 04:29:39.772054: step 7110, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.552 sec/batch; 49h:51m:31s remains)
INFO - root - 2017-12-11 04:29:45.129512: step 7120, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 49h:32m:21s remains)
INFO - root - 2017-12-11 04:29:50.359498: step 7130, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 46h:28m:59s remains)
INFO - root - 2017-12-11 04:29:55.766471: step 7140, loss = 0.72, batch loss = 0.67 (14.6 examples/sec; 0.548 sec/batch; 49h:31m:12s remains)
INFO - root - 2017-12-11 04:30:00.864352: step 7150, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:54m:57s remains)
INFO - root - 2017-12-11 04:30:06.223004: step 7160, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:36m:28s remains)
INFO - root - 2017-12-11 04:30:11.581744: step 7170, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:30m:02s remains)
INFO - root - 2017-12-11 04:30:16.963790: step 7180, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:07m:03s remains)
INFO - root - 2017-12-11 04:30:22.165439: step 7190, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 47h:19m:04s remains)
INFO - root - 2017-12-11 04:30:27.522202: step 7200, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:25m:15s remains)
2017-12-11 04:30:28.099311: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30938843 0.32190591 0.32200167 0.31576896 0.29756737 0.26810223 0.2410077 0.2217475 0.20336254 0.19096015 0.19709979 0.21924858 0.24514216 0.26083916 0.27459008][0.28001681 0.29582742 0.30158392 0.30166635 0.29012078 0.26924732 0.25098926 0.2380856 0.22557949 0.21939333 0.22906025 0.25339723 0.28242025 0.30191031 0.32220447][0.21545149 0.23927665 0.26271296 0.28283414 0.29045972 0.28736821 0.28099746 0.27264839 0.25863263 0.24773131 0.25071818 0.2676332 0.29392549 0.31640843 0.3450897][0.16283029 0.19592288 0.24341282 0.2956503 0.33480904 0.35740715 0.363981 0.35300583 0.32236391 0.28805357 0.26823023 0.26747096 0.28592509 0.31132263 0.35133818][0.14383329 0.18124422 0.24771833 0.33406568 0.41180792 0.46740836 0.49139926 0.47683829 0.42207876 0.35177946 0.29570702 0.26634932 0.26783881 0.2904596 0.33957523][0.15191029 0.18919425 0.26640534 0.38286814 0.50117332 0.5954296 0.64529854 0.63578773 0.56250042 0.45520115 0.35552096 0.28596047 0.25656489 0.26327157 0.31069198][0.16521657 0.20059846 0.28478321 0.42483869 0.577525 0.70513749 0.77901274 0.77799749 0.69478959 0.55964583 0.42131183 0.31094104 0.24516307 0.22819361 0.26617816][0.16210563 0.19680168 0.28672609 0.44300467 0.61853886 0.76631117 0.85409355 0.85866213 0.77141142 0.62050849 0.45797434 0.32091683 0.23089933 0.19952078 0.23335002][0.14291477 0.17254925 0.25910157 0.41625214 0.59661615 0.74881423 0.84097672 0.85030663 0.76760906 0.6168853 0.45006612 0.30792403 0.21508801 0.18695974 0.2270212][0.10886098 0.12869854 0.20054713 0.33901697 0.50181985 0.639813 0.72465944 0.73728788 0.66949725 0.53875172 0.39066768 0.26640013 0.19280775 0.18566383 0.24258882][0.058619738 0.069259919 0.1229779 0.23107673 0.35955209 0.46592703 0.52828753 0.53511137 0.48336864 0.38504171 0.27579513 0.1924133 0.15998088 0.19071968 0.27595875][-0.0076904604 -0.0056268312 0.030940492 0.106378 0.19552109 0.26414117 0.29600814 0.28752124 0.24405067 0.17670918 0.11212832 0.079430223 0.097975239 0.17383979 0.29272565][-0.062822342 -0.071603246 -0.052889742 -0.0086516077 0.044823367 0.082923271 0.0930954 0.074667089 0.036326122 -0.0098968586 -0.042515893 -0.038356096 0.016055712 0.12095347 0.2592217][-0.073286042 -0.093382284 -0.0939627 -0.079514816 -0.057436176 -0.041682377 -0.042000044 -0.060366265 -0.091000296 -0.12344051 -0.13982317 -0.12110192 -0.057906628 0.047130104 0.17871036][-0.053440876 -0.077912249 -0.09189871 -0.098941036 -0.099396259 -0.098338559 -0.10304104 -0.11740508 -0.13996074 -0.1637339 -0.17483138 -0.15809578 -0.10637112 -0.022322945 0.084106751]]...]
INFO - root - 2017-12-11 04:30:33.417506: step 7210, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:59m:21s remains)
INFO - root - 2017-12-11 04:30:38.762736: step 7220, loss = 0.70, batch loss = 0.65 (15.7 examples/sec; 0.509 sec/batch; 45h:56m:55s remains)
INFO - root - 2017-12-11 04:30:44.160729: step 7230, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:34m:40s remains)
INFO - root - 2017-12-11 04:30:49.458047: step 7240, loss = 0.68, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 46h:44m:43s remains)
INFO - root - 2017-12-11 04:30:54.763246: step 7250, loss = 0.71, batch loss = 0.65 (19.2 examples/sec; 0.417 sec/batch; 37h:42m:39s remains)
INFO - root - 2017-12-11 04:30:59.971590: step 7260, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 47h:43m:06s remains)
INFO - root - 2017-12-11 04:31:05.419692: step 7270, loss = 0.71, batch loss = 0.65 (13.8 examples/sec; 0.578 sec/batch; 52h:14m:46s remains)
INFO - root - 2017-12-11 04:31:10.815742: step 7280, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:15m:11s remains)
INFO - root - 2017-12-11 04:31:16.028333: step 7290, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 46h:17m:43s remains)
INFO - root - 2017-12-11 04:31:21.447267: step 7300, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 47h:43m:56s remains)
2017-12-11 04:31:22.003256: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16789311 0.17093295 0.1725776 0.17610744 0.17946777 0.18341719 0.18143465 0.16971931 0.14878343 0.12760951 0.12338938 0.13490482 0.15206087 0.16915968 0.18510465][0.16423184 0.16485779 0.16471024 0.16813093 0.17165965 0.17565919 0.17594478 0.16909586 0.15448657 0.14019518 0.1449752 0.16692978 0.19123206 0.21016999 0.22359782][0.13554217 0.13371623 0.13290836 0.13773702 0.14333421 0.14892277 0.15351838 0.15537384 0.15249142 0.15025617 0.16674808 0.19934052 0.23004048 0.24935429 0.25829011][0.10085526 0.10046726 0.10339849 0.11400956 0.12617807 0.13681003 0.14685129 0.15714933 0.16544172 0.17406119 0.19914064 0.23852488 0.27340275 0.29213783 0.29551879][0.073232554 0.081290685 0.095739886 0.11911426 0.14387034 0.16306117 0.17713243 0.19007353 0.20258875 0.21559849 0.24264717 0.28213418 0.31622604 0.33112586 0.32631546][0.06016105 0.083576187 0.11668866 0.15960908 0.20224269 0.23231354 0.24735254 0.25477356 0.26186141 0.27036408 0.29096434 0.32200849 0.34831324 0.35500205 0.33999062][0.052714288 0.0945752 0.14987446 0.2150951 0.27740818 0.31897381 0.33289871 0.32926935 0.32346016 0.31995609 0.32679847 0.3424888 0.35578316 0.35221806 0.32892054][0.033897538 0.092217557 0.16763841 0.25137097 0.32924768 0.37951726 0.39176014 0.37684318 0.35644796 0.33902243 0.33029208 0.32938552 0.32916543 0.31681594 0.29000068][0.0038688395 0.071747281 0.15779838 0.24777506 0.32916281 0.38010556 0.38927871 0.36668611 0.33677009 0.31087235 0.29182729 0.27932623 0.26974297 0.25310692 0.22946906][-0.02423377 0.043637093 0.12637772 0.20677702 0.27633402 0.31691658 0.31923789 0.29201266 0.25918928 0.23305358 0.21260995 0.19749629 0.18649894 0.17293614 0.16036573][-0.038229082 0.021244023 0.088943243 0.14740741 0.19241752 0.21241829 0.20276701 0.17184116 0.14131659 0.12223131 0.10912592 0.10049709 0.096440658 0.093161725 0.097088352][-0.036613878 0.0090310657 0.054772791 0.085294589 0.10039642 0.096453473 0.074016891 0.041576147 0.017624855 0.010499382 0.011237367 0.016000036 0.024372131 0.034225449 0.053456958][-0.027695397 0.0013246404 0.023605058 0.027612796 0.016620271 -0.0059228409 -0.035654731 -0.064788148 -0.078601047 -0.071800224 -0.05467033 -0.034011748 -0.012511071 0.0083436836 0.037066061][-0.025496474 -0.012258303 -0.00886895 -0.022698639 -0.048092108 -0.077313021 -0.10485991 -0.12469875 -0.1259356 -0.10631928 -0.076153733 -0.044404183 -0.015427672 0.010033039 0.041148555][-0.038337216 -0.036545802 -0.042774577 -0.061223261 -0.085700192 -0.10783309 -0.12336732 -0.128934 -0.11717258 -0.088441335 -0.052170794 -0.01761139 0.010608401 0.032924149 0.059529889]]...]
INFO - root - 2017-12-11 04:31:27.278125: step 7310, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.512 sec/batch; 46h:12m:45s remains)
INFO - root - 2017-12-11 04:31:32.710594: step 7320, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 48h:18m:46s remains)
INFO - root - 2017-12-11 04:31:38.083663: step 7330, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 46h:19m:10s remains)
INFO - root - 2017-12-11 04:31:43.452897: step 7340, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:23m:12s remains)
INFO - root - 2017-12-11 04:31:48.835352: step 7350, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 46h:48m:06s remains)
INFO - root - 2017-12-11 04:31:53.813621: step 7360, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:46m:05s remains)
INFO - root - 2017-12-11 04:31:59.206885: step 7370, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 49h:06m:07s remains)
INFO - root - 2017-12-11 04:32:04.625495: step 7380, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 50h:01m:04s remains)
INFO - root - 2017-12-11 04:32:10.038052: step 7390, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.528 sec/batch; 47h:42m:20s remains)
INFO - root - 2017-12-11 04:32:15.308144: step 7400, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 48h:01m:50s remains)
2017-12-11 04:32:15.804412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027918916 0.0073097995 0.059172451 0.12618056 0.2023761 0.27690843 0.33059239 0.36122763 0.36657211 0.34652731 0.32028744 0.3019498 0.30485818 0.32459432 0.34824783][-0.029282471 0.0099814767 0.068745226 0.14626041 0.23682979 0.32733548 0.39426863 0.43464375 0.44384047 0.41984329 0.38306546 0.35179737 0.34366852 0.35709831 0.38095388][-0.028905885 0.012351288 0.075617842 0.16011038 0.25946838 0.35877839 0.43333963 0.48056278 0.49297512 0.46653154 0.42152297 0.37773547 0.3555935 0.35733682 0.37599042][-0.030275239 0.011327164 0.076818652 0.16513102 0.26973975 0.3735469 0.45296437 0.50722653 0.52600247 0.50285906 0.45603278 0.40462655 0.3698889 0.35762382 0.367046][-0.032556254 0.0081750108 0.074803695 0.16604368 0.27487278 0.38096 0.46369007 0.52609032 0.55578721 0.54258817 0.50045007 0.44600418 0.4007934 0.3722623 0.36645192][-0.033504426 0.0057355883 0.07358589 0.16811764 0.28131038 0.38888717 0.47352034 0.54367334 0.58667272 0.58744508 0.55366004 0.4987227 0.44527844 0.4001717 0.37514785][-0.034540255 0.0028763048 0.070936553 0.16874176 0.28767714 0.39901412 0.48616147 0.56273204 0.61797637 0.63185692 0.60508293 0.54906893 0.48970354 0.43097049 0.38691613][-0.03640021 -0.0026438448 0.0625591 0.16158192 0.28613013 0.4036819 0.49417111 0.57352263 0.63505608 0.65491408 0.62940228 0.57077074 0.50972521 0.4450914 0.38955122][-0.03945367 -0.010612023 0.049167216 0.14722465 0.27534673 0.3979418 0.48843491 0.56199521 0.61812526 0.63136488 0.59748209 0.53301054 0.47399566 0.4143503 0.36203769][-0.043666147 -0.020660967 0.031651031 0.1244772 0.24859561 0.36764002 0.44962391 0.50767761 0.54718661 0.54353309 0.49463466 0.42395425 0.3712422 0.32734108 0.29369536][-0.051300991 -0.035729975 0.0056900219 0.086512513 0.1965843 0.30190969 0.36767995 0.40385103 0.42006311 0.395378 0.33146361 0.2581616 0.21591422 0.19524953 0.19273236][-0.062621087 -0.055510916 -0.027123505 0.036419794 0.12478605 0.20870255 0.25381467 0.26677093 0.26032707 0.21909587 0.14834447 0.080294088 0.051733926 0.05561683 0.086420387][-0.073824242 -0.0741385 -0.057646275 -0.012446893 0.051356889 0.11033784 0.13428836 0.12715383 0.10290329 0.052616943 -0.014809444 -0.069322489 -0.082629547 -0.058282506 -0.00027261354][-0.07936798 -0.082564428 -0.0717821 -0.038878825 0.0064503253 0.046203483 0.055612881 0.036809813 0.0020632755 -0.051624741 -0.1125658 -0.15500447 -0.15989356 -0.1271092 -0.05543955][-0.07633286 -0.075259633 -0.060242493 -0.02802241 0.012164723 0.045764569 0.051777422 0.030024992 -0.010036264 -0.066346079 -0.12493505 -0.16414888 -0.17164619 -0.14490864 -0.076994695]]...]
INFO - root - 2017-12-11 04:32:21.205075: step 7410, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 47h:26m:51s remains)
INFO - root - 2017-12-11 04:32:26.590582: step 7420, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:54m:31s remains)
INFO - root - 2017-12-11 04:32:32.040948: step 7430, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 50h:21m:57s remains)
INFO - root - 2017-12-11 04:32:37.354676: step 7440, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 47h:39m:28s remains)
INFO - root - 2017-12-11 04:32:42.661813: step 7450, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.521 sec/batch; 47h:04m:35s remains)
INFO - root - 2017-12-11 04:32:47.736133: step 7460, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 47h:32m:12s remains)
INFO - root - 2017-12-11 04:32:53.056287: step 7470, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 48h:03m:26s remains)
INFO - root - 2017-12-11 04:32:58.408103: step 7480, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:35m:20s remains)
INFO - root - 2017-12-11 04:33:03.744744: step 7490, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 48h:11m:52s remains)
INFO - root - 2017-12-11 04:33:09.059020: step 7500, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:54m:50s remains)
2017-12-11 04:33:09.615230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0738873 0.067693576 0.065291464 0.083351977 0.12793633 0.18558472 0.23273729 0.25147307 0.23935375 0.19622864 0.13686551 0.0905006 0.080515094 0.11050288 0.15977181][0.08833231 0.083038546 0.087423682 0.12477921 0.2017336 0.29409292 0.3661955 0.39157817 0.36845222 0.29903433 0.20792234 0.13858621 0.12326754 0.16612045 0.23536424][0.086347409 0.084532827 0.099208638 0.15780601 0.26638615 0.39102232 0.48567721 0.51686257 0.48392221 0.39326179 0.27901363 0.19489768 0.1775821 0.23063968 0.31313017][0.0750934 0.0797742 0.10576333 0.18076289 0.31003323 0.45505843 0.56439978 0.59999055 0.56233639 0.46279031 0.34219778 0.25705075 0.24310316 0.30204991 0.38742045][0.067471467 0.082371406 0.12035582 0.20545171 0.34158486 0.4907071 0.602628 0.63826615 0.60017359 0.50362295 0.39193794 0.31710422 0.30971646 0.36894754 0.44657868][0.070439383 0.098853931 0.14838701 0.23776105 0.36881241 0.50741065 0.60965359 0.63999718 0.60454875 0.52071351 0.42911777 0.37181842 0.37162155 0.42549708 0.48640049][0.10006738 0.14485821 0.20436446 0.29166925 0.40811342 0.52631986 0.61083412 0.63324833 0.60437661 0.54088277 0.47545865 0.43750632 0.4419449 0.48413959 0.52071685][0.17040606 0.23264903 0.29985046 0.37914973 0.47362611 0.56423426 0.62447989 0.63486832 0.61233634 0.57099384 0.531306 0.50989527 0.5166226 0.54614967 0.5581736][0.26029456 0.33635762 0.40617916 0.47282529 0.542387 0.60322613 0.63559347 0.62890857 0.60547346 0.57711506 0.55346513 0.54189014 0.55016309 0.57109779 0.56652331][0.32826173 0.40847152 0.47299808 0.52315277 0.5678854 0.60145181 0.60799241 0.583079 0.55164003 0.52544713 0.50860578 0.50319022 0.51470631 0.53247124 0.52114141][0.3392199 0.41401759 0.4681482 0.50209117 0.526236 0.53921264 0.52754432 0.49000433 0.45036855 0.4214367 0.4068467 0.40683722 0.42381757 0.44338295 0.43370232][0.28843993 0.35156193 0.39407784 0.41495949 0.42440683 0.42415774 0.40269902 0.35998839 0.31611562 0.2834889 0.26721931 0.26902923 0.28974584 0.31297487 0.31024942][0.18800549 0.23205304 0.25951168 0.26785249 0.2658014 0.25723958 0.23287456 0.19229868 0.14984635 0.11578231 0.096665 0.097226568 0.11859418 0.14449257 0.14972508][0.065289155 0.085775062 0.096125633 0.093208671 0.083688967 0.072237611 0.051807109 0.020787703 -0.013487207 -0.043944053 -0.062802434 -0.062744282 -0.042510781 -0.017242502 -0.0070949825][-0.042993739 -0.040299527 -0.040714145 -0.047816534 -0.057726298 -0.066809267 -0.079865761 -0.099445522 -0.12317373 -0.14629288 -0.16095698 -0.16033033 -0.14382249 -0.12337916 -0.11229807]]...]
INFO - root - 2017-12-11 04:33:14.991066: step 7510, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:46m:27s remains)
INFO - root - 2017-12-11 04:33:20.392313: step 7520, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:57m:03s remains)
INFO - root - 2017-12-11 04:33:25.772305: step 7530, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 49h:24m:31s remains)
INFO - root - 2017-12-11 04:33:31.080052: step 7540, loss = 0.71, batch loss = 0.65 (15.6 examples/sec; 0.513 sec/batch; 46h:17m:09s remains)
INFO - root - 2017-12-11 04:33:36.488659: step 7550, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:32m:30s remains)
INFO - root - 2017-12-11 04:33:41.586200: step 7560, loss = 0.71, batch loss = 0.65 (20.1 examples/sec; 0.398 sec/batch; 35h:55m:04s remains)
INFO - root - 2017-12-11 04:33:46.983261: step 7570, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:45m:36s remains)
INFO - root - 2017-12-11 04:33:52.356106: step 7580, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 50h:34m:12s remains)
INFO - root - 2017-12-11 04:33:57.632869: step 7590, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:27m:23s remains)
INFO - root - 2017-12-11 04:34:03.002788: step 7600, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:47m:09s remains)
2017-12-11 04:34:03.546808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013325567 -0.0086764693 -0.012731477 -0.026742063 -0.044275917 -0.059947971 -0.0703143 -0.071884081 -0.063614316 -0.047869448 -0.030061036 -0.015765896 -0.0096423486 -0.014693096 -0.029793113][0.048919111 0.060862526 0.056024518 0.033136405 0.0036977902 -0.0228197 -0.040757887 -0.044066239 -0.030412424 -0.0035256492 0.026839472 0.051083185 0.061222438 0.051575985 0.023042576][0.13246167 0.15631112 0.15382296 0.12454135 0.086606234 0.05422131 0.03326536 0.02987342 0.047435418 0.082282543 0.1212585 0.15161611 0.16259034 0.14518896 0.098940238][0.22480959 0.26501074 0.26878107 0.23743507 0.1977246 0.16962 0.15591605 0.15789458 0.17756301 0.21323261 0.25102738 0.27763495 0.28194189 0.25209886 0.18482786][0.29996863 0.35697457 0.36991274 0.34303463 0.31267405 0.30376765 0.31178883 0.3272723 0.34692255 0.372866 0.39447305 0.40254089 0.39009756 0.34440455 0.25778937][0.33536622 0.40630192 0.43074116 0.41629261 0.40757358 0.43096772 0.47093123 0.50391638 0.51957554 0.5256843 0.51826417 0.49588791 0.45934817 0.39768338 0.29790923][0.32898527 0.40963355 0.44808951 0.45377854 0.47495273 0.53438872 0.60470635 0.65080792 0.65806997 0.64031887 0.60019207 0.54409742 0.48178414 0.40664387 0.30188417][0.28527158 0.36800632 0.41820759 0.44521397 0.49487442 0.58183414 0.66959649 0.71895647 0.71629506 0.679659 0.6149137 0.53304935 0.45142013 0.36933222 0.26919952][0.21013477 0.28407073 0.33692232 0.37601241 0.44086403 0.5370841 0.62492353 0.66888791 0.65978926 0.61597353 0.54200232 0.45054755 0.363809 0.28603745 0.20088629][0.11404406 0.16960862 0.21467298 0.25289911 0.31594512 0.40247121 0.47600213 0.50958169 0.49853623 0.45804626 0.38942137 0.30587441 0.23035868 0.16829255 0.10550957][0.013865327 0.049119938 0.081867911 0.11120123 0.16058505 0.22537479 0.27688894 0.29755959 0.28636327 0.25437418 0.20054722 0.1372681 0.083632067 0.042694729 0.0030476036][-0.083774589 -0.068430685 -0.0486825 -0.029610604 0.0040897131 0.046594076 0.077733606 0.087135106 0.075846262 0.051565249 0.01349605 -0.027752271 -0.058657497 -0.079071626 -0.097100444][-0.16307668 -0.16473849 -0.15694188 -0.14637747 -0.12466002 -0.098182857 -0.080663338 -0.078113124 -0.088956535 -0.10772313 -0.13383725 -0.15847076 -0.17266463 -0.17735685 -0.17749652][-0.2037293 -0.21550201 -0.2162291 -0.21189497 -0.19840732 -0.18186188 -0.17172056 -0.171577 -0.18027955 -0.19414029 -0.21191922 -0.22659962 -0.23180789 -0.22775227 -0.21714282][-0.20823911 -0.22246155 -0.22597456 -0.22329313 -0.21305951 -0.20085399 -0.19352357 -0.19350141 -0.20005277 -0.21095271 -0.22450656 -0.2351637 -0.23781376 -0.23179127 -0.21836047]]...]
INFO - root - 2017-12-11 04:34:08.904934: step 7610, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 50h:15m:29s remains)
INFO - root - 2017-12-11 04:34:14.213506: step 7620, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:24m:11s remains)
INFO - root - 2017-12-11 04:34:19.458185: step 7630, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:07m:33s remains)
INFO - root - 2017-12-11 04:34:24.830513: step 7640, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 48h:00m:45s remains)
INFO - root - 2017-12-11 04:34:30.208664: step 7650, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:52m:46s remains)
INFO - root - 2017-12-11 04:34:35.479814: step 7660, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:55m:06s remains)
INFO - root - 2017-12-11 04:34:40.496641: step 7670, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 48h:44m:00s remains)
INFO - root - 2017-12-11 04:34:45.852478: step 7680, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:29m:29s remains)
INFO - root - 2017-12-11 04:34:51.138402: step 7690, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 47h:02m:02s remains)
INFO - root - 2017-12-11 04:34:56.455512: step 7700, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:23m:59s remains)
2017-12-11 04:34:57.032474: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19207163 0.17497481 0.15304866 0.14460593 0.14924343 0.1495923 0.13194285 0.0976652 0.061099712 0.038581993 0.037659593 0.05661609 0.088233039 0.12093677 0.14111863][0.35233411 0.32519105 0.28744984 0.27134639 0.27647617 0.2757771 0.247159 0.19380672 0.13887163 0.10650899 0.10611132 0.13248786 0.17405315 0.21701446 0.24472514][0.50884908 0.4726451 0.42124656 0.39818081 0.4035826 0.40362263 0.36859396 0.30058113 0.22903502 0.18483439 0.1798223 0.2062421 0.24793845 0.29048434 0.31730995][0.61049229 0.56866711 0.51472437 0.49183169 0.50103164 0.50717461 0.47655204 0.40670729 0.3270973 0.2715013 0.25554982 0.27184159 0.30048242 0.32752332 0.33991715][0.62087423 0.58307207 0.54287982 0.53468639 0.55876827 0.58216542 0.570678 0.5156703 0.44015181 0.37621394 0.34397176 0.33996141 0.34201229 0.3374784 0.32008851][0.5537793 0.52890116 0.51742345 0.53959769 0.5919081 0.64350516 0.66142774 0.63098979 0.565736 0.49470702 0.44285434 0.41282064 0.38152573 0.33798811 0.28376657][0.4542931 0.44548243 0.46653351 0.52623546 0.61426055 0.69806486 0.74544448 0.73804349 0.68090618 0.6009146 0.52626282 0.46648422 0.4009729 0.32137707 0.23522326][0.3628881 0.36312452 0.40717429 0.49649742 0.61246634 0.71797568 0.7815299 0.7861194 0.73141611 0.64223576 0.54774487 0.46525803 0.37837091 0.27953786 0.1780584][0.293979 0.29383114 0.34321222 0.44102493 0.56182009 0.66587394 0.72440118 0.72636944 0.67012817 0.57776588 0.47607124 0.38850361 0.30134678 0.20519744 0.10844716][0.24609199 0.23613067 0.27116188 0.35164922 0.44938385 0.52721268 0.5616008 0.54982048 0.49118006 0.40396538 0.31027254 0.23557805 0.16628106 0.09023536 0.01504261][0.21605958 0.188911 0.19371678 0.23528306 0.2888158 0.32518023 0.3278788 0.30059603 0.24406071 0.17143974 0.099471159 0.05076031 0.010550133 -0.035071183 -0.07951951][0.20584059 0.15879352 0.12275487 0.11056304 0.11010779 0.10386407 0.08080598 0.046636429 0.0025136853 -0.044791967 -0.085793413 -0.10531557 -0.11752761 -0.13422841 -0.14945221][0.20947175 0.14583078 0.070164941 0.0053717922 -0.043909412 -0.083434306 -0.11991823 -0.14856227 -0.17058533 -0.18688096 -0.19621916 -0.19181879 -0.18489867 -0.18237185 -0.17812043][0.21748941 0.14591502 0.046045672 -0.051723946 -0.13122359 -0.18838082 -0.22558485 -0.24120112 -0.24190196 -0.23576438 -0.22633328 -0.21123278 -0.19690013 -0.18572788 -0.17248322][0.22011147 0.14937495 0.044920735 -0.060391232 -0.14699024 -0.20640922 -0.23760231 -0.24193031 -0.23057541 -0.2151617 -0.20081535 -0.18605633 -0.17264469 -0.16034855 -0.14595036]]...]
INFO - root - 2017-12-11 04:35:02.366645: step 7710, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 47h:52m:03s remains)
INFO - root - 2017-12-11 04:35:07.673109: step 7720, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:37m:30s remains)
INFO - root - 2017-12-11 04:35:13.018297: step 7730, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:23m:36s remains)
INFO - root - 2017-12-11 04:35:18.497165: step 7740, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 49h:38m:35s remains)
INFO - root - 2017-12-11 04:35:23.831110: step 7750, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 48h:06m:21s remains)
INFO - root - 2017-12-11 04:35:29.137906: step 7760, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:32m:19s remains)
INFO - root - 2017-12-11 04:35:34.197422: step 7770, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.511 sec/batch; 46h:05m:50s remains)
INFO - root - 2017-12-11 04:35:39.544348: step 7780, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:48m:28s remains)
INFO - root - 2017-12-11 04:35:44.892114: step 7790, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 47h:31m:24s remains)
INFO - root - 2017-12-11 04:35:50.266054: step 7800, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:26m:38s remains)
2017-12-11 04:35:50.771476: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42126551 0.40777552 0.3846437 0.36724216 0.35909402 0.349084 0.3325918 0.30291224 0.25559962 0.20020227 0.15744138 0.14776187 0.16231818 0.18891685 0.21399367][0.46405515 0.45104679 0.42895237 0.41402754 0.4085463 0.40191898 0.38979593 0.36312276 0.31270191 0.24993348 0.19666502 0.17652234 0.18183184 0.20094651 0.22234952][0.45467746 0.44819632 0.43496677 0.42932716 0.43019304 0.42831376 0.42099175 0.39888796 0.34963289 0.28571236 0.22719567 0.19725615 0.19006336 0.19588202 0.2076613][0.42184722 0.42491135 0.42565253 0.43381941 0.44360125 0.44721133 0.44462097 0.42742527 0.38202852 0.32035762 0.26000634 0.22035545 0.19806492 0.18667679 0.18502727][0.38205105 0.39178798 0.40327886 0.42295629 0.44164291 0.45287982 0.45907557 0.45208067 0.41678011 0.36170179 0.30266023 0.25406271 0.21426682 0.18156962 0.16266063][0.35123152 0.36668214 0.38519204 0.41161469 0.43578392 0.45361397 0.47047848 0.47617748 0.45266318 0.40458223 0.34819698 0.2923238 0.23484129 0.17954747 0.14265858][0.33886483 0.35838664 0.38381442 0.41484872 0.44166452 0.46326235 0.48816916 0.50188792 0.48462519 0.43910128 0.38443142 0.32407889 0.25292224 0.17959291 0.12961426][0.32782698 0.34938449 0.38032278 0.41625682 0.44808656 0.47454023 0.50521338 0.52114552 0.50364536 0.45466042 0.39782077 0.33426052 0.25572407 0.17391129 0.12040164][0.296855 0.31676686 0.34970388 0.39036849 0.43129769 0.46754193 0.50582361 0.524152 0.50536275 0.45090786 0.38777491 0.31882137 0.2356143 0.15250655 0.10319336][0.23489091 0.25146028 0.28466317 0.32943371 0.37954107 0.42603204 0.47149989 0.49346909 0.47557259 0.41867217 0.35045427 0.27716792 0.19279648 0.11406884 0.073678389][0.14696595 0.15916902 0.19189833 0.23927158 0.29449841 0.34576988 0.39275411 0.41617489 0.40148968 0.34902173 0.28456932 0.21641909 0.1407313 0.074312948 0.045630764][0.052891627 0.059612874 0.089163542 0.13491258 0.19004083 0.24159622 0.28662148 0.31042257 0.30187926 0.26106375 0.21010163 0.15721765 0.099488549 0.05079123 0.032633539][-0.033483554 -0.032988306 -0.010452416 0.027772609 0.0769107 0.12576456 0.1694968 0.19680479 0.1984932 0.17374964 0.14011936 0.10509254 0.067030028 0.03596589 0.025621703][-0.10324252 -0.1081897 -0.094727121 -0.069358721 -0.033055019 0.0077820509 0.048842594 0.080751836 0.094727725 0.088414937 0.074117176 0.056896038 0.035901822 0.018146647 0.011884743][-0.13992676 -0.15005882 -0.14603135 -0.1353786 -0.1158172 -0.088510774 -0.055359006 -0.022909695 0.00071932224 0.011824997 0.017164605 0.017725315 0.011458062 0.0027179681 -0.0031028176]]...]
INFO - root - 2017-12-11 04:35:56.034448: step 7810, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 46h:46m:27s remains)
INFO - root - 2017-12-11 04:36:01.486818: step 7820, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 52h:10m:21s remains)
INFO - root - 2017-12-11 04:36:06.998544: step 7830, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 49h:13m:53s remains)
INFO - root - 2017-12-11 04:36:12.381940: step 7840, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 49h:07m:12s remains)
INFO - root - 2017-12-11 04:36:17.799428: step 7850, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 49h:22m:34s remains)
INFO - root - 2017-12-11 04:36:23.223937: step 7860, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 50h:21m:38s remains)
INFO - root - 2017-12-11 04:36:28.301110: step 7870, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:19m:30s remains)
INFO - root - 2017-12-11 04:36:33.633580: step 7880, loss = 0.68, batch loss = 0.62 (15.7 examples/sec; 0.509 sec/batch; 45h:51m:29s remains)
INFO - root - 2017-12-11 04:36:38.977494: step 7890, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:03m:48s remains)
INFO - root - 2017-12-11 04:36:44.414194: step 7900, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:54m:33s remains)
2017-12-11 04:36:44.960988: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056592852 0.058392752 0.0500525 0.0361394 0.023901047 0.018329518 0.01986295 0.024915254 0.030191364 0.031360462 0.026826451 0.017927267 0.0090371445 0.0047240071 0.0051002046][0.14151627 0.14845988 0.13462652 0.10826411 0.083537377 0.071944274 0.074969217 0.0850114 0.095496051 0.098600544 0.0919291 0.077843055 0.06361483 0.057051111 0.05823949][0.2358873 0.24748345 0.22631523 0.18601154 0.14897738 0.13359129 0.14152287 0.15952207 0.17710041 0.18279187 0.17383435 0.15396586 0.1333165 0.12371296 0.12567338][0.30801985 0.3216241 0.29340172 0.24270728 0.1989022 0.18551426 0.2030789 0.23197778 0.25780275 0.26627541 0.25491589 0.22892214 0.20093471 0.18680842 0.18827979][0.32890937 0.34108818 0.30993673 0.25903243 0.22000267 0.21733692 0.24915132 0.29009876 0.3227236 0.33155531 0.31556663 0.2813631 0.24353752 0.22162737 0.21965165][0.29491523 0.30205813 0.2736553 0.23459673 0.21329622 0.22992517 0.27877223 0.33039388 0.36574268 0.3703514 0.34570411 0.30018544 0.24959369 0.21630599 0.207785][0.23067296 0.2324239 0.21084735 0.19041689 0.19346274 0.23241216 0.29693529 0.35550281 0.38885781 0.38547093 0.34995624 0.29272494 0.23005734 0.18571232 0.17089456][0.17081416 0.16960584 0.15677066 0.15477629 0.1797533 0.23638612 0.30952749 0.36844638 0.39542934 0.38271347 0.33789977 0.27304289 0.20460516 0.15540001 0.13828872][0.1342757 0.13358888 0.12898344 0.14007694 0.17786649 0.24156779 0.31323674 0.3655746 0.38429281 0.3654429 0.31868941 0.25616348 0.19291124 0.14810777 0.13333964][0.12370237 0.12592974 0.1269802 0.1431035 0.18222417 0.2411703 0.30191037 0.34252822 0.35413575 0.33689106 0.30058515 0.25426796 0.20849772 0.17694673 0.16800049][0.13019925 0.13663052 0.1401533 0.15398927 0.1848727 0.2302172 0.27495989 0.30348489 0.31328732 0.30792731 0.29461092 0.27570865 0.25450322 0.23925237 0.2357921][0.13901336 0.15022413 0.15448783 0.16157651 0.17845096 0.2055781 0.23360634 0.25296506 0.26649 0.27951148 0.29464552 0.30569097 0.30778781 0.30538929 0.30353552][0.13666534 0.15251258 0.15693922 0.15667859 0.15972608 0.17052928 0.18631423 0.2014817 0.22168025 0.25176743 0.28951296 0.3216776 0.33741346 0.3400968 0.335155][0.11703002 0.13434568 0.1376472 0.13110581 0.12363745 0.12345961 0.1332531 0.1483321 0.17403978 0.2135223 0.26115105 0.30061612 0.31858853 0.31966341 0.31010553][0.073355079 0.088106267 0.0892793 0.079152629 0.0669715 0.063462988 0.073746018 0.09125191 0.11851987 0.15707248 0.20067231 0.23456226 0.24625874 0.24194816 0.2288015]]...]
INFO - root - 2017-12-11 04:36:50.251925: step 7910, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:37m:39s remains)
INFO - root - 2017-12-11 04:36:55.638442: step 7920, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 48h:03m:45s remains)
INFO - root - 2017-12-11 04:37:01.029466: step 7930, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 48h:07m:56s remains)
INFO - root - 2017-12-11 04:37:06.321730: step 7940, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 46h:59m:08s remains)
INFO - root - 2017-12-11 04:37:11.688113: step 7950, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.525 sec/batch; 47h:21m:16s remains)
INFO - root - 2017-12-11 04:37:17.021843: step 7960, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 47h:24m:27s remains)
INFO - root - 2017-12-11 04:37:22.453484: step 7970, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 47h:06m:30s remains)
INFO - root - 2017-12-11 04:37:27.488690: step 7980, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:45m:07s remains)
INFO - root - 2017-12-11 04:37:32.764387: step 7990, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:53m:31s remains)
INFO - root - 2017-12-11 04:37:38.095036: step 8000, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:22m:52s remains)
2017-12-11 04:37:38.729676: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011892844 -0.016744653 -0.04444921 -0.058454044 -0.051448114 -0.02765088 0.0017114902 0.023018727 0.030967066 0.023567097 0.0030582978 -0.021623319 -0.041833125 -0.052503016 -0.053212103][-0.024569606 -0.040360384 -0.049965478 -0.045093309 -0.024027357 0.0079427324 0.037091274 0.051204048 0.049284339 0.032351449 0.0060081286 -0.02180298 -0.041405741 -0.049930882 -0.04931327][-0.048775181 -0.04618375 -0.031911489 -0.00061008788 0.042946886 0.088554375 0.12084638 0.12887754 0.11420275 0.080993 0.040340662 0.0030788169 -0.021362178 -0.032675255 -0.034723427][-0.053441871 -0.036317065 -0.0028698961 0.050058305 0.11329783 0.17281929 0.21203269 0.21902826 0.19504544 0.14707862 0.091557376 0.042258281 0.0085665258 -0.010189522 -0.0185173][-0.042090211 -0.016312882 0.028023753 0.092622019 0.16677369 0.23491116 0.2803477 0.28909379 0.260224 0.20286469 0.13750906 0.078990616 0.035899371 0.0080688363 -0.0078501478][-0.020622453 0.010505077 0.060284037 0.129408 0.20745896 0.27964756 0.330443 0.34290954 0.31276494 0.25079116 0.1800992 0.11463442 0.062180918 0.024672464 0.0012531319][-0.0004145508 0.03552461 0.089276262 0.15940201 0.23628877 0.30791554 0.3618829 0.37928736 0.35231978 0.29197127 0.22138304 0.1525334 0.093005709 0.047632381 0.018504968][0.016674081 0.058801457 0.11617278 0.18426777 0.25413847 0.31765509 0.36763826 0.38661867 0.36503327 0.31242135 0.2494096 0.18508051 0.12607251 0.078640595 0.047328435][0.037692521 0.085869685 0.14492933 0.20857923 0.26864693 0.32000777 0.36111483 0.37906894 0.36414692 0.32293317 0.27282161 0.22045943 0.17021167 0.12671594 0.095658056][0.059841853 0.11109708 0.16896324 0.22811955 0.28138453 0.32463667 0.35981935 0.37930289 0.3733165 0.34509498 0.30951777 0.27226624 0.23485398 0.19781907 0.16730817][0.072134323 0.12283 0.17745489 0.23306289 0.28386751 0.32541522 0.36047024 0.38536838 0.39028782 0.37623686 0.35599664 0.3347092 0.3110373 0.28114825 0.25152078][0.073110454 0.11991594 0.16918382 0.22042665 0.2697224 0.31217605 0.34949532 0.38049442 0.39633009 0.39710277 0.39332166 0.38895845 0.38035229 0.36023909 0.33496317][0.070541382 0.10901703 0.14902064 0.19310811 0.23998442 0.28479645 0.32670903 0.36547059 0.39255214 0.40767726 0.41874251 0.4282786 0.43208423 0.42198691 0.40439349][0.066657431 0.093169995 0.12102103 0.15539527 0.19811688 0.24548024 0.29463422 0.34450331 0.38425153 0.41257656 0.43478191 0.45211709 0.46144041 0.45658121 0.44532517][0.066093922 0.080469884 0.09690617 0.12072618 0.15659052 0.20348357 0.25788289 0.31610465 0.36379275 0.39906281 0.42500165 0.44178629 0.44809732 0.44205958 0.43344659]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:37:44.121416: step 8010, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.539 sec/batch; 48h:32m:45s remains)
INFO - root - 2017-12-11 04:37:49.342504: step 8020, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:47m:58s remains)
INFO - root - 2017-12-11 04:37:54.730357: step 8030, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 48h:40m:11s remains)
INFO - root - 2017-12-11 04:38:00.047533: step 8040, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 46h:59m:48s remains)
INFO - root - 2017-12-11 04:38:05.382521: step 8050, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:31m:33s remains)
INFO - root - 2017-12-11 04:38:10.723108: step 8060, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:58m:15s remains)
INFO - root - 2017-12-11 04:38:16.088377: step 8070, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 46h:48m:18s remains)
INFO - root - 2017-12-11 04:38:21.214075: step 8080, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 46h:48m:29s remains)
INFO - root - 2017-12-11 04:38:26.617840: step 8090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:54m:14s remains)
INFO - root - 2017-12-11 04:38:31.905097: step 8100, loss = 0.70, batch loss = 0.65 (15.5 examples/sec; 0.517 sec/batch; 46h:34m:08s remains)
2017-12-11 04:38:32.438722: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18344867 0.18623261 0.17549784 0.15918586 0.1440348 0.13047001 0.11947397 0.11602548 0.12043716 0.12901384 0.1357262 0.13310942 0.11698997 0.085077077 0.04330156][0.23133512 0.23798212 0.22562163 0.20575114 0.1875421 0.17164586 0.15761559 0.14985958 0.14913544 0.15217999 0.15404862 0.14774154 0.12954213 0.096528567 0.053190876][0.28051496 0.28727978 0.27214667 0.25091258 0.23358677 0.2197696 0.20572352 0.19458878 0.18795143 0.18363026 0.17845643 0.16621171 0.14427339 0.10943183 0.064490288][0.32864335 0.33007717 0.3092097 0.28698546 0.272948 0.26513955 0.25561252 0.24539652 0.23616853 0.22623724 0.21369277 0.19338953 0.16481 0.12535056 0.076731183][0.36510941 0.3575359 0.32901388 0.30503455 0.29477841 0.29528308 0.2948187 0.29115322 0.2845009 0.27266219 0.25427887 0.22500034 0.18739149 0.14082749 0.086958066][0.37923384 0.36451918 0.33108246 0.30686972 0.30165717 0.31260732 0.32475451 0.33191508 0.33198056 0.32158816 0.29966229 0.2617057 0.21347941 0.15746671 0.096390143][0.36611196 0.3490032 0.31709594 0.29761073 0.30124274 0.32543683 0.35187787 0.37107155 0.37890261 0.3709586 0.34647292 0.30031413 0.24065812 0.17322417 0.1028727][0.32867914 0.31232414 0.28726706 0.27602547 0.29003552 0.32641926 0.36399543 0.39216724 0.40588245 0.40041932 0.37497464 0.32349178 0.25537533 0.17856815 0.10051485][0.27315146 0.2598598 0.24388979 0.2413148 0.26303235 0.30518374 0.34579217 0.37625247 0.392379 0.38978469 0.36690664 0.31623584 0.24724706 0.16817911 0.088198505][0.21442649 0.20516433 0.19775309 0.2023568 0.22712967 0.26648763 0.30089617 0.32650551 0.3417601 0.34220552 0.32502741 0.28154108 0.21996787 0.1466244 0.070607714][0.16666874 0.15912476 0.15812382 0.16778086 0.19180323 0.22212343 0.24421147 0.26045489 0.272987 0.276672 0.26684463 0.23448884 0.1854333 0.12263851 0.053530551][0.13376887 0.12657708 0.12781012 0.13793863 0.15722297 0.1761681 0.18541059 0.19224787 0.20211035 0.20948777 0.20774783 0.18798979 0.15289961 0.10187747 0.040184725][0.11106743 0.10272493 0.10088082 0.1055873 0.11614539 0.12415496 0.12411219 0.12488228 0.13398859 0.14650586 0.15403061 0.14707042 0.12535386 0.08591006 0.032341868][0.10106217 0.090197466 0.083217978 0.080554411 0.082174525 0.082490645 0.078246035 0.076424241 0.085589744 0.10196847 0.11650014 0.11853826 0.10607386 0.075983293 0.030635348][0.10211213 0.091759712 0.083535895 0.077434666 0.074476823 0.072217062 0.068317547 0.06580516 0.073607489 0.089298666 0.10450527 0.1083777 0.099038213 0.074621551 0.03586131]]...]
INFO - root - 2017-12-11 04:38:37.881274: step 8110, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 47h:34m:03s remains)
INFO - root - 2017-12-11 04:38:43.166647: step 8120, loss = 0.68, batch loss = 0.63 (15.6 examples/sec; 0.514 sec/batch; 46h:20m:33s remains)
INFO - root - 2017-12-11 04:38:48.447380: step 8130, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.542 sec/batch; 48h:52m:35s remains)
INFO - root - 2017-12-11 04:38:53.830653: step 8140, loss = 0.66, batch loss = 0.60 (14.0 examples/sec; 0.571 sec/batch; 51h:25m:14s remains)
INFO - root - 2017-12-11 04:38:59.175936: step 8150, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 48h:18m:12s remains)
INFO - root - 2017-12-11 04:39:04.485131: step 8160, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 50h:13m:11s remains)
INFO - root - 2017-12-11 04:39:09.886265: step 8170, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 50h:14m:08s remains)
INFO - root - 2017-12-11 04:39:14.973551: step 8180, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:22m:05s remains)
INFO - root - 2017-12-11 04:39:20.409277: step 8190, loss = 0.68, batch loss = 0.62 (13.8 examples/sec; 0.578 sec/batch; 52h:06m:43s remains)
INFO - root - 2017-12-11 04:39:25.820679: step 8200, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:40m:29s remains)
2017-12-11 04:39:26.409715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11117836 -0.09795849 -0.061843667 0.0028136598 0.085304677 0.16328919 0.20553027 0.20932078 0.19477442 0.17025977 0.14397293 0.11741251 0.088176295 0.049429573 -0.0005392342][-0.11313984 -0.095515333 -0.050513837 0.024419688 0.11671976 0.20005457 0.24131379 0.23813893 0.21300919 0.17649007 0.13663435 0.096999988 0.057127327 0.011018765 -0.04305248][-0.10594978 -0.08218465 -0.027004346 0.059820265 0.16193466 0.25042552 0.28998151 0.27781549 0.2360173 0.17830943 0.11693311 0.059682522 0.0085640606 -0.041864481 -0.093743563][-0.095170736 -0.065120369 -0.00048215679 0.0975602 0.20983523 0.30560994 0.34683284 0.32963979 0.27456862 0.19820505 0.11673816 0.041962158 -0.021102253 -0.07610558 -0.12492101][-0.083949171 -0.049086377 0.02185427 0.12752898 0.24812296 0.35276839 0.401092 0.38720858 0.32809731 0.24108331 0.14565082 0.05726368 -0.015805218 -0.075141892 -0.12212325][-0.075515024 -0.037624061 0.037134513 0.14928895 0.28129551 0.40365058 0.47235176 0.4757944 0.4236882 0.33129302 0.22194438 0.11745463 0.031928837 -0.034130961 -0.082462162][-0.072283149 -0.033803415 0.042382732 0.16094205 0.30922303 0.45943019 0.56096685 0.59217876 0.553156 0.45632106 0.3301169 0.20556484 0.10495284 0.0303965 -0.020784363][-0.075290896 -0.040146854 0.032930285 0.15505981 0.3202391 0.50167894 0.63947153 0.69913465 0.67211688 0.56978458 0.4270643 0.28449202 0.17201817 0.091608077 0.0388154][-0.080104172 -0.050286006 0.016016297 0.13631797 0.31121185 0.51397473 0.67602223 0.75233072 0.730912 0.6243217 0.47248891 0.32183334 0.20609999 0.12562877 0.074089371][-0.082514115 -0.056670062 0.0027923279 0.11679796 0.28901961 0.4912484 0.65084589 0.72064894 0.6935606 0.58547783 0.43686575 0.29298311 0.18613954 0.11468458 0.071006693][-0.085916869 -0.063324556 -0.0095591592 0.095643707 0.25375798 0.43376279 0.56474048 0.60663235 0.5638786 0.45704797 0.32244232 0.19795081 0.11089412 0.058277592 0.031287584][-0.092058696 -0.0744143 -0.027805036 0.064198852 0.19855861 0.34300843 0.43395025 0.44278157 0.38477188 0.28326353 0.16831298 0.068871111 0.0066376878 -0.022062249 -0.027537035][-0.097412996 -0.0851332 -0.046561204 0.029191624 0.13545214 0.24196509 0.29663002 0.28198037 0.2184892 0.12840852 0.035255212 -0.039663512 -0.079738177 -0.088814 -0.078050219][-0.097368352 -0.086470813 -0.051739395 0.011252781 0.093583636 0.16936646 0.19873326 0.17287505 0.11228327 0.036520291 -0.036842406 -0.09242072 -0.11779957 -0.11691394 -0.10051411][-0.093707904 -0.0810334 -0.045704111 0.012060475 0.082915433 0.14543593 0.16799107 0.1440466 0.089741148 0.022065181 -0.043115061 -0.091510653 -0.11252883 -0.11074104 -0.096067019]]...]
INFO - root - 2017-12-11 04:39:31.661249: step 8210, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:43m:55s remains)
INFO - root - 2017-12-11 04:39:37.038810: step 8220, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:45m:38s remains)
INFO - root - 2017-12-11 04:39:42.497130: step 8230, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 51h:44m:30s remains)
INFO - root - 2017-12-11 04:39:47.835875: step 8240, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:59m:35s remains)
INFO - root - 2017-12-11 04:39:53.264403: step 8250, loss = 0.68, batch loss = 0.63 (15.7 examples/sec; 0.509 sec/batch; 45h:50m:01s remains)
INFO - root - 2017-12-11 04:39:58.701251: step 8260, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:43m:21s remains)
INFO - root - 2017-12-11 04:40:04.144936: step 8270, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 49h:04m:30s remains)
INFO - root - 2017-12-11 04:40:09.285897: step 8280, loss = 0.69, batch loss = 0.63 (22.7 examples/sec; 0.353 sec/batch; 31h:45m:08s remains)
INFO - root - 2017-12-11 04:40:14.585776: step 8290, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:03m:26s remains)
INFO - root - 2017-12-11 04:40:19.960997: step 8300, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 47h:24m:01s remains)
2017-12-11 04:40:20.493832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064537637 -0.065762289 -0.064928792 -0.065461308 -0.066398881 -0.066934973 -0.066441804 -0.065524735 -0.064243495 -0.062768362 -0.061740611 -0.061335403 -0.062038194 -0.063681781 -0.066198163][-0.052473269 -0.049917694 -0.046480976 -0.04602043 -0.04727051 -0.048234235 -0.046964623 -0.04327986 -0.03871312 -0.034893207 -0.032695204 -0.032529373 -0.035293292 -0.041568089 -0.050987773][-0.018727783 -0.0077570984 0.0011635695 0.0033650543 0.0016008969 0.00020846773 0.0032124997 0.011111452 0.0209759 0.0293169 0.033901643 0.03369749 0.026708361 0.011846065 -0.0099347159][0.032185867 0.056857921 0.07530845 0.082151018 0.0816473 0.080965236 0.086985759 0.10100871 0.11853658 0.13297965 0.13982813 0.13685374 0.12056896 0.090275966 0.048759628][0.1007514 0.14536886 0.17901321 0.19490552 0.19780399 0.19762507 0.20543848 0.22527415 0.25223807 0.27496082 0.28474623 0.27667663 0.2449266 0.19089752 0.12149377][0.17628638 0.24457139 0.29757705 0.3267889 0.33593872 0.33649892 0.3440741 0.36706248 0.40218228 0.43264231 0.44437853 0.42890978 0.37786838 0.29599494 0.19539884][0.24415413 0.33490992 0.40679446 0.45047486 0.46822774 0.4714469 0.47878635 0.50130445 0.53785455 0.56815678 0.5749315 0.54843068 0.47925881 0.37482274 0.25015488][0.29525766 0.40429574 0.49052998 0.54522878 0.57017142 0.57593977 0.58301675 0.6014806 0.63153261 0.65191323 0.64576751 0.60595608 0.52417052 0.4086321 0.27350557][0.31626847 0.43655849 0.53183413 0.59441739 0.62523794 0.63242626 0.63723075 0.64645672 0.66241044 0.66643733 0.64524579 0.59568828 0.51075363 0.39721414 0.26557332][0.29893267 0.41854432 0.51420009 0.57864642 0.61174721 0.61786693 0.61674482 0.61160403 0.60906816 0.59720975 0.56762463 0.5191465 0.4450646 0.34832615 0.23364653][0.24230519 0.34694356 0.43289664 0.49237418 0.52373606 0.52718776 0.51854849 0.49854642 0.47874415 0.45648214 0.4282051 0.39171639 0.34038123 0.27217177 0.18458447][0.16267453 0.24269679 0.3119612 0.36122724 0.38666415 0.38504872 0.368179 0.33662629 0.30536488 0.28044 0.26170608 0.24383873 0.21987957 0.18347144 0.12623128][0.071458891 0.12303316 0.17149824 0.20663038 0.22280568 0.21538645 0.19301751 0.1575904 0.1252806 0.10640369 0.10204776 0.10345226 0.10320085 0.092958473 0.061730109][-0.021044817 0.0042673438 0.032221694 0.0526369 0.059307925 0.04837117 0.025922853 -0.0050491476 -0.029016322 -0.036009394 -0.025384011 -0.0085574063 0.0069477945 0.013377102 0.00028920939][-0.092535131 -0.087467305 -0.077471375 -0.071078628 -0.073089436 -0.085386947 -0.10348298 -0.12420304 -0.13483359 -0.12872832 -0.10675588 -0.080971792 -0.058227923 -0.044313725 -0.047043253]]...]
INFO - root - 2017-12-11 04:40:25.881874: step 8310, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 48h:34m:58s remains)
INFO - root - 2017-12-11 04:40:31.278184: step 8320, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 48h:52m:03s remains)
INFO - root - 2017-12-11 04:40:36.707413: step 8330, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 48h:11m:07s remains)
INFO - root - 2017-12-11 04:40:42.060775: step 8340, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:06m:38s remains)
INFO - root - 2017-12-11 04:40:47.342671: step 8350, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.519 sec/batch; 46h:45m:12s remains)
INFO - root - 2017-12-11 04:40:52.695329: step 8360, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:48m:14s remains)
INFO - root - 2017-12-11 04:40:58.162339: step 8370, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 50h:26m:55s remains)
INFO - root - 2017-12-11 04:41:03.544687: step 8380, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 46h:55m:35s remains)
INFO - root - 2017-12-11 04:41:08.542295: step 8390, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 47h:12m:45s remains)
INFO - root - 2017-12-11 04:41:13.797658: step 8400, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 47h:07m:59s remains)
2017-12-11 04:41:14.342703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024407106 -0.002213974 0.053141315 0.14268793 0.25356528 0.3604916 0.43290731 0.44446033 0.39029288 0.28949493 0.18303482 0.10413736 0.061569378 0.047701243 0.046889588][-0.040919412 -0.019709036 0.034104459 0.12087423 0.23183367 0.34398562 0.42531064 0.45038772 0.41307703 0.32833835 0.22719274 0.14260234 0.09064763 0.064958155 0.0527431][-0.061120577 -0.043163855 0.0047958833 0.084554784 0.19085205 0.30361262 0.39360598 0.43713486 0.42529103 0.36323756 0.27246717 0.18329664 0.1162983 0.071756057 0.041926213][-0.078956619 -0.063404746 -0.018806642 0.057846364 0.16431077 0.28336433 0.38707837 0.45151117 0.46422833 0.42189318 0.33698082 0.23676066 0.14731465 0.077232808 0.025010956][-0.088779688 -0.074942879 -0.032010064 0.04444271 0.15503895 0.2850928 0.40643641 0.49311051 0.52808756 0.50075144 0.41664115 0.30084383 0.18563141 0.088273317 0.013890084][-0.0901154 -0.0780359 -0.03703557 0.0387455 0.15322798 0.29425821 0.43299136 0.54036611 0.59423113 0.5779618 0.49217993 0.36117408 0.22316259 0.10353772 0.013439347][-0.086044744 -0.074379534 -0.034934558 0.039533664 0.1556955 0.30360755 0.45387885 0.57480741 0.6402837 0.62977993 0.54146427 0.39982814 0.24745503 0.11591247 0.020873403][-0.080415942 -0.066941343 -0.027364405 0.046100773 0.16122788 0.30913761 0.46029231 0.58239907 0.64829248 0.63668191 0.54605216 0.40163741 0.24740224 0.11764977 0.030175356][-0.076149985 -0.060492732 -0.020563493 0.050061055 0.15836893 0.29536322 0.43255797 0.53988934 0.59336883 0.57475281 0.48502731 0.34959903 0.20997463 0.098788336 0.032662265][-0.075836934 -0.060948681 -0.023953659 0.038468186 0.13125014 0.24466041 0.3529501 0.43111289 0.46201879 0.43394619 0.35168862 0.23944841 0.13212186 0.056293976 0.024721181][-0.080842324 -0.0710269 -0.042529743 0.0049588685 0.0745849 0.1565163 0.22892432 0.27307492 0.28026724 0.2460237 0.17805825 0.09827248 0.033383042 0.0018540765 0.011186146][-0.089318484 -0.0885488 -0.0734547 -0.045975409 -0.0034001018 0.0461105 0.085521139 0.10205086 0.094010487 0.061588459 0.013225743 -0.032125197 -0.054644939 -0.043638658 0.0032014449][-0.097793043 -0.10728326 -0.10697864 -0.10015955 -0.083521284 -0.062011804 -0.047285028 -0.047309052 -0.059690729 -0.082192808 -0.10747149 -0.12039791 -0.10683748 -0.062238343 0.011637444][-0.10293153 -0.12017899 -0.13184826 -0.14137104 -0.14494698 -0.14381754 -0.14413247 -0.15008861 -0.15816961 -0.16549638 -0.1662719 -0.15033576 -0.10935993 -0.04317512 0.044260997][-0.10255697 -0.12347202 -0.14130485 -0.15934299 -0.17373639 -0.1827455 -0.18760507 -0.18948244 -0.18588011 -0.1747192 -0.1521115 -0.11311281 -0.055237673 0.018915499 0.10500875]]...]
INFO - root - 2017-12-11 04:41:19.675476: step 8410, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:40m:41s remains)
INFO - root - 2017-12-11 04:41:25.099204: step 8420, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 48h:57m:43s remains)
INFO - root - 2017-12-11 04:41:30.507410: step 8430, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 48h:31m:19s remains)
INFO - root - 2017-12-11 04:41:35.900820: step 8440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:19m:03s remains)
INFO - root - 2017-12-11 04:41:41.162502: step 8450, loss = 0.67, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 46h:43m:28s remains)
INFO - root - 2017-12-11 04:41:46.498537: step 8460, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 47h:03m:36s remains)
INFO - root - 2017-12-11 04:41:51.904497: step 8470, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.582 sec/batch; 52h:21m:50s remains)
INFO - root - 2017-12-11 04:41:57.190691: step 8480, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:37m:06s remains)
INFO - root - 2017-12-11 04:42:02.280671: step 8490, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 49h:46m:10s remains)
INFO - root - 2017-12-11 04:42:07.656534: step 8500, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 49h:05m:35s remains)
2017-12-11 04:42:08.200833: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19793035 0.20723546 0.21508722 0.2346133 0.26340941 0.28844312 0.30253676 0.30032873 0.28136328 0.24779968 0.2107783 0.18437485 0.17284161 0.17031643 0.16780411][0.22901863 0.23217827 0.22986174 0.23791493 0.25701782 0.27784613 0.29293528 0.2967909 0.28531402 0.25774831 0.22371265 0.19696563 0.18443003 0.18076029 0.18142499][0.2495302 0.2500332 0.24284875 0.24518801 0.25913289 0.27789405 0.29490492 0.3025074 0.29575983 0.27356958 0.24353229 0.22077104 0.21296465 0.21706279 0.23119761][0.25222227 0.25146285 0.24396205 0.24505381 0.25738069 0.27616805 0.2962524 0.30832705 0.30675596 0.29050347 0.26681268 0.25415206 0.26042223 0.28425744 0.32121816][0.23661396 0.23887898 0.23569804 0.24041602 0.25631025 0.28017622 0.30753028 0.32637152 0.32899496 0.31451362 0.29145986 0.2839466 0.30279356 0.34776849 0.40943775][0.20245802 0.21275976 0.2209627 0.23814559 0.26738316 0.30556902 0.34696022 0.3759135 0.38108376 0.36075661 0.32543543 0.30602244 0.31960008 0.36820677 0.4405348][0.15602842 0.17584428 0.19836012 0.23234639 0.28103921 0.34146595 0.4038417 0.44750547 0.4558534 0.4260022 0.37004116 0.32386672 0.31113577 0.33923975 0.39963385][0.10836264 0.13376868 0.16675478 0.21453245 0.28343329 0.37175512 0.46207321 0.52542341 0.53772122 0.49602 0.41416943 0.331881 0.28031388 0.27446648 0.30968335][0.067522608 0.092753425 0.1281976 0.18142295 0.26512149 0.37995979 0.49922651 0.58477837 0.60282427 0.55103678 0.44484925 0.32745871 0.23904231 0.20148097 0.21257859][0.035015628 0.056029957 0.087053634 0.13693112 0.22448552 0.35233149 0.48698878 0.58621049 0.60962456 0.55639261 0.44210654 0.31094268 0.20796226 0.15565503 0.15181394][0.010646729 0.023578836 0.045827869 0.085644551 0.16459163 0.28597021 0.41514626 0.51190329 0.53718436 0.49293882 0.3934983 0.27962252 0.19318986 0.15074649 0.14678404][-0.0089949649 -0.0063742716 0.0049305535 0.032196119 0.09604615 0.19867441 0.30778527 0.38857681 0.41081843 0.38096711 0.31203073 0.23810893 0.19171482 0.18014099 0.1912581][-0.02869983 -0.03619286 -0.033723574 -0.014655815 0.037435953 0.12116433 0.20665202 0.26597345 0.28128561 0.26532751 0.23088719 0.20548561 0.20998795 0.23963262 0.27396262][-0.043849673 -0.058273364 -0.058436308 -0.038018458 0.012399316 0.084493965 0.14977403 0.18747988 0.19219062 0.18339165 0.17662504 0.19499153 0.24786681 0.31642538 0.37201566][-0.0485273 -0.0656535 -0.063372634 -0.035711661 0.018819032 0.084434278 0.13383886 0.15219049 0.14484492 0.13664015 0.14793104 0.19972798 0.290955 0.38939211 0.4598605]]...]
INFO - root - 2017-12-11 04:42:13.517903: step 8510, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 48h:00m:07s remains)
INFO - root - 2017-12-11 04:42:18.775883: step 8520, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 47h:11m:51s remains)
INFO - root - 2017-12-11 04:42:24.097233: step 8530, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 46h:44m:47s remains)
INFO - root - 2017-12-11 04:42:29.459934: step 8540, loss = 0.68, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 46h:31m:40s remains)
INFO - root - 2017-12-11 04:42:34.745480: step 8550, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 49h:01m:49s remains)
INFO - root - 2017-12-11 04:42:40.101159: step 8560, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:44m:43s remains)
INFO - root - 2017-12-11 04:42:45.501903: step 8570, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 48h:07m:20s remains)
INFO - root - 2017-12-11 04:42:50.859349: step 8580, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 50h:16m:48s remains)
INFO - root - 2017-12-11 04:42:56.000219: step 8590, loss = 0.68, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 46h:10m:44s remains)
INFO - root - 2017-12-11 04:43:01.317928: step 8600, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 46h:47m:26s remains)
2017-12-11 04:43:01.871563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081058629 0.080927752 0.079351768 0.077884831 0.076792784 0.076404482 0.076890558 0.078235678 0.079996146 0.08171308 0.083456911 0.085100159 0.086969256 0.1003031 0.12637927][0.10227697 0.1012657 0.098548815 0.096295379 0.094738267 0.094226912 0.094923005 0.096967533 0.099822573 0.10271358 0.1054041 0.1074001 0.10930032 0.12519485 0.15578149][0.11243271 0.11201209 0.10986431 0.10809892 0.10639347 0.10496315 0.10426361 0.10542963 0.10824136 0.11159433 0.1146291 0.11608617 0.11659554 0.13127789 0.16051705][0.11316536 0.11629412 0.11776883 0.11860112 0.11723369 0.11406141 0.11028167 0.10890049 0.11018711 0.11268448 0.11473235 0.11405272 0.11101656 0.12004472 0.14181323][0.11722338 0.12590167 0.13314667 0.13803002 0.13737367 0.13244829 0.12534843 0.12125147 0.12067033 0.12157866 0.12152427 0.11697562 0.10776298 0.10654015 0.11512946][0.12859344 0.14215492 0.1551006 0.16537353 0.16840656 0.16645658 0.160967 0.15743953 0.15602013 0.15449227 0.15066457 0.14020817 0.12260023 0.10859848 0.10143425][0.15367427 0.16731502 0.18155214 0.19469596 0.20291398 0.21001038 0.21403691 0.21662645 0.21585882 0.20965752 0.19747055 0.17611174 0.14691447 0.12029451 0.0998346][0.19276689 0.20342717 0.21473643 0.22607718 0.23689698 0.25400475 0.27109182 0.2826314 0.2832996 0.27113238 0.24708876 0.21043235 0.16734129 0.13087912 0.10338613][0.23762713 0.24732962 0.25562611 0.26235282 0.27056172 0.29064527 0.31350818 0.3274321 0.32522014 0.30558857 0.26964587 0.21835804 0.16391748 0.12416448 0.099054985][0.29247048 0.30302075 0.30829012 0.30902076 0.30960232 0.3223744 0.33756813 0.34129408 0.328037 0.29876328 0.25388321 0.19371623 0.13461284 0.098900132 0.084328838][0.34510097 0.35540247 0.35574648 0.34895563 0.33917272 0.33832109 0.33740202 0.32392067 0.29558337 0.25648224 0.20668772 0.14473008 0.08828719 0.0624536 0.063991427][0.36507896 0.37207705 0.36550984 0.35019639 0.32973367 0.31421492 0.29706109 0.26926643 0.23109771 0.18863162 0.14154734 0.086512089 0.039846592 0.027136765 0.046235155][0.32258132 0.3256847 0.31347823 0.29247957 0.26569191 0.24151751 0.21659794 0.18558504 0.14963517 0.11464976 0.079923421 0.041095514 0.010697373 0.011833001 0.044701938][0.22656502 0.22578979 0.21112767 0.18892083 0.16137031 0.13616104 0.11369612 0.091403678 0.06943877 0.050872713 0.034506112 0.016439771 0.0047266809 0.017591104 0.057279032][0.12322421 0.116763 0.10064437 0.079899587 0.056279317 0.036708005 0.023965204 0.016676156 0.013130888 0.012945052 0.014291337 0.014691534 0.01874131 0.039015573 0.078698106]]...]
INFO - root - 2017-12-11 04:43:07.200153: step 8610, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:36m:31s remains)
INFO - root - 2017-12-11 04:43:12.598864: step 8620, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:03m:13s remains)
INFO - root - 2017-12-11 04:43:17.891056: step 8630, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 46h:40m:25s remains)
INFO - root - 2017-12-11 04:43:23.247482: step 8640, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:51m:40s remains)
INFO - root - 2017-12-11 04:43:28.559804: step 8650, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:12m:08s remains)
INFO - root - 2017-12-11 04:43:33.831072: step 8660, loss = 0.68, batch loss = 0.63 (15.5 examples/sec; 0.518 sec/batch; 46h:34m:43s remains)
INFO - root - 2017-12-11 04:43:39.158353: step 8670, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:39m:05s remains)
INFO - root - 2017-12-11 04:43:44.563854: step 8680, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 47h:26m:39s remains)
INFO - root - 2017-12-11 04:43:49.927469: step 8690, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:11m:15s remains)
INFO - root - 2017-12-11 04:43:54.981977: step 8700, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 48h:45m:44s remains)
2017-12-11 04:43:55.512525: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45184186 0.45242709 0.43267062 0.41040224 0.38158059 0.34974068 0.32333118 0.30233943 0.28659952 0.27748415 0.28454047 0.30136439 0.31655526 0.32071191 0.31180608][0.46078819 0.46270803 0.43449137 0.39270747 0.34363058 0.30104432 0.2770637 0.27303308 0.28264564 0.30635011 0.34146798 0.37494996 0.3982383 0.40013808 0.37874886][0.40508032 0.41280961 0.38368377 0.33588552 0.28454176 0.24964537 0.24319503 0.26211554 0.29863185 0.34852186 0.40290925 0.44674715 0.4721379 0.46755844 0.42759493][0.31832975 0.32890132 0.30649036 0.267998 0.23452225 0.22743915 0.25219446 0.29759344 0.35405529 0.41456649 0.46895456 0.50431168 0.515824 0.49507546 0.43487206][0.20991211 0.22323054 0.2171274 0.2044283 0.20766398 0.24353223 0.30832291 0.38106188 0.44800031 0.49867049 0.52741039 0.53023684 0.50949377 0.46511623 0.39018834][0.098049738 0.11656232 0.13249463 0.1548758 0.20373234 0.28678596 0.39256641 0.49048457 0.55969971 0.58487296 0.56819767 0.52118021 0.45698982 0.38780689 0.30835289][0.027441332 0.048487138 0.08085116 0.13123649 0.21635176 0.33603811 0.47484097 0.59260708 0.65907389 0.65446734 0.58744258 0.48629257 0.37805763 0.28876626 0.21560818][0.04251929 0.063310653 0.097061433 0.15328911 0.24717447 0.37517741 0.52365714 0.64541191 0.70116377 0.67007142 0.56509256 0.42599413 0.2909781 0.19660559 0.14224604][0.13341853 0.1508095 0.17114295 0.20987257 0.28277102 0.38765332 0.51773 0.62346268 0.65891993 0.60674536 0.4833951 0.33413178 0.20127843 0.12297454 0.099297263][0.24150956 0.25045219 0.24898362 0.25600427 0.28760719 0.34821865 0.44167498 0.52130157 0.53682232 0.47505131 0.35562146 0.22437505 0.12082872 0.076218881 0.089009754][0.31250051 0.3057307 0.280152 0.25309244 0.23888557 0.25095117 0.30344319 0.3571578 0.36206532 0.30685437 0.21168406 0.11754975 0.05689846 0.050224554 0.093183748][0.31175104 0.28472623 0.23919204 0.18819478 0.14174132 0.11740696 0.13495249 0.166332 0.16849887 0.13114704 0.071903504 0.022748891 0.0070353854 0.034568142 0.097081214][0.23753336 0.1933568 0.13755088 0.078953385 0.022494067 -0.018248819 -0.02397625 -0.010035633 -0.0078050541 -0.026153486 -0.051107146 -0.059574377 -0.036972363 0.016942399 0.089573793][0.12331961 0.071395442 0.016397949 -0.036184657 -0.0842729 -0.12171251 -0.13462617 -0.12970555 -0.12634119 -0.13049884 -0.13156794 -0.11353857 -0.068345435 -0.0020260774 0.070693225][0.012460877 -0.033694703 -0.076847531 -0.11567019 -0.14835207 -0.17273918 -0.18049482 -0.176221 -0.17110527 -0.16730055 -0.15615797 -0.12733042 -0.07745865 -0.014158825 0.049360003]]...]
INFO - root - 2017-12-11 04:44:00.796048: step 8710, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 46h:28m:49s remains)
INFO - root - 2017-12-11 04:44:06.152635: step 8720, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:46m:24s remains)
INFO - root - 2017-12-11 04:44:11.544723: step 8730, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:51m:48s remains)
INFO - root - 2017-12-11 04:44:16.917723: step 8740, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 47h:07m:27s remains)
INFO - root - 2017-12-11 04:44:22.257719: step 8750, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:33m:57s remains)
INFO - root - 2017-12-11 04:44:27.634842: step 8760, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:38m:58s remains)
INFO - root - 2017-12-11 04:44:33.055061: step 8770, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 50h:10m:12s remains)
INFO - root - 2017-12-11 04:44:38.444414: step 8780, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:44m:54s remains)
INFO - root - 2017-12-11 04:44:43.876066: step 8790, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 47h:14m:31s remains)
INFO - root - 2017-12-11 04:44:49.024858: step 8800, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 49h:25m:41s remains)
2017-12-11 04:44:49.596927: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067420244 0.079164892 0.10931605 0.15244381 0.19153339 0.20786071 0.19318339 0.1556818 0.11399375 0.087825872 0.0822094 0.092585489 0.11710167 0.159809 0.21822807][0.10450944 0.10285738 0.11462095 0.14339009 0.17501149 0.18995304 0.17707096 0.14059 0.09725263 0.06668964 0.057332691 0.0674617 0.095222346 0.14224146 0.20367561][0.13974783 0.1221777 0.11284559 0.12585725 0.15038861 0.16627674 0.15924647 0.12970957 0.090438224 0.058624905 0.044382673 0.049009576 0.073547013 0.11903943 0.17906334][0.15277684 0.12503687 0.1056931 0.11530878 0.14390254 0.16909893 0.17271659 0.15180925 0.11652756 0.0819612 0.060235519 0.056898531 0.076488778 0.11914454 0.17554139][0.13720937 0.10648459 0.093399607 0.11676437 0.16395579 0.20828758 0.22676474 0.21366031 0.1780463 0.13500877 0.10082433 0.087024555 0.10123215 0.14079264 0.19236512][0.10367696 0.080590516 0.087494083 0.13756239 0.21377113 0.28405976 0.31929293 0.31049234 0.26701194 0.20709442 0.15452756 0.12821957 0.13798994 0.17632669 0.22429435][0.07104598 0.065110773 0.10051122 0.18208003 0.28900966 0.38393143 0.43115768 0.41908583 0.35969186 0.27729639 0.20463753 0.16713122 0.17463496 0.2129987 0.25730839][0.054663468 0.072498567 0.13519077 0.24141617 0.36955595 0.47954866 0.53052419 0.50927508 0.43150616 0.32855517 0.2405633 0.19607385 0.20203722 0.23763686 0.27490577][0.069203936 0.1068183 0.18153121 0.29209921 0.42273575 0.53592604 0.58611977 0.55630392 0.46382952 0.34640247 0.24928696 0.20114091 0.20413248 0.23325099 0.262071][0.11207318 0.16061069 0.22846453 0.3208946 0.43485224 0.541323 0.5903241 0.55670828 0.45540878 0.32860088 0.22550453 0.17408331 0.17213167 0.19377214 0.21713303][0.15725385 0.20937043 0.26056778 0.32285932 0.40658718 0.4955067 0.53988934 0.50700706 0.40577748 0.27780902 0.1732765 0.12003849 0.11402725 0.1317903 0.15647158][0.17481022 0.22356498 0.25874573 0.29237345 0.34247187 0.40529096 0.43907607 0.40955985 0.31886369 0.20151351 0.10367995 0.053191606 0.047188137 0.066444188 0.098241858][0.15565228 0.19647411 0.22037849 0.23524193 0.25915521 0.29583091 0.31562948 0.28937441 0.21470731 0.11606462 0.032264564 -0.0094547085 -0.0093968511 0.017014813 0.059124995][0.11485212 0.14569706 0.16314591 0.17048039 0.18085605 0.1995244 0.20811772 0.18591931 0.12913446 0.052338779 -0.013781134 -0.0428924 -0.032651208 0.0033270151 0.053668454][0.076511748 0.10026643 0.11478156 0.12113774 0.12808941 0.13988033 0.14623958 0.13396008 0.098252065 0.044508908 -0.0044990038 -0.022774762 -0.004547535 0.037204549 0.088742793]]...]
INFO - root - 2017-12-11 04:44:54.881436: step 8810, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 50h:03m:47s remains)
INFO - root - 2017-12-11 04:45:00.213151: step 8820, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 47h:05m:32s remains)
INFO - root - 2017-12-11 04:45:05.630457: step 8830, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 49h:41m:59s remains)
INFO - root - 2017-12-11 04:45:10.918356: step 8840, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.521 sec/batch; 46h:50m:40s remains)
INFO - root - 2017-12-11 04:45:16.266922: step 8850, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 47h:14m:46s remains)
INFO - root - 2017-12-11 04:45:21.652223: step 8860, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 46h:47m:33s remains)
INFO - root - 2017-12-11 04:45:26.973697: step 8870, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 47h:02m:45s remains)
INFO - root - 2017-12-11 04:45:32.311630: step 8880, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 48h:21m:21s remains)
INFO - root - 2017-12-11 04:45:37.652669: step 8890, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 48h:05m:31s remains)
INFO - root - 2017-12-11 04:45:42.772228: step 8900, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 48h:02m:25s remains)
2017-12-11 04:45:43.280459: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.57325542 0.58848393 0.54011917 0.4745512 0.42395458 0.40200904 0.39857054 0.39728811 0.39132753 0.38569266 0.39187592 0.41697413 0.46275267 0.51804346 0.55981183][0.61929995 0.62037253 0.55754805 0.48601365 0.43599185 0.42218283 0.42892584 0.43233445 0.42582506 0.41687471 0.42121002 0.447636 0.49418473 0.54368138 0.57501048][0.58569127 0.574041 0.51427585 0.45960233 0.43297213 0.44145504 0.46378174 0.47128078 0.45985746 0.44030678 0.43058249 0.44133419 0.46939713 0.49943537 0.51340121][0.48782578 0.47479743 0.43767086 0.41988349 0.43364644 0.4751201 0.51686966 0.52818632 0.50757205 0.468294 0.42982486 0.40590417 0.39860725 0.40131935 0.39941981][0.34292856 0.34089541 0.34026924 0.36910456 0.4279919 0.50258487 0.56089628 0.5730387 0.53961378 0.47623512 0.4044919 0.34079909 0.29593825 0.2745817 0.26516324][0.2078055 0.22514613 0.26448295 0.33672553 0.43184683 0.52771133 0.59106064 0.59504926 0.543165 0.45512068 0.35510793 0.26088691 0.190254 0.15643933 0.15000722][0.12175993 0.16292979 0.23773775 0.34327596 0.46042493 0.56214714 0.61584747 0.59884268 0.5203051 0.407702 0.28880268 0.182147 0.10671271 0.076606244 0.082720049][0.09119454 0.1474425 0.2435662 0.36564466 0.48734373 0.57996917 0.61356425 0.5703221 0.46608394 0.33677125 0.21427986 0.1161099 0.055849209 0.041943148 0.064846173][0.091838762 0.15037642 0.24647646 0.36092907 0.46463388 0.53236485 0.54170305 0.48119503 0.36876336 0.24269895 0.13517393 0.060567141 0.024592562 0.029056795 0.0652335][0.096869268 0.1469737 0.22699808 0.31640086 0.38722789 0.42101526 0.40706632 0.34254622 0.24213779 0.13847642 0.057982646 0.010781754 -0.0033098222 0.013521889 0.055129923][0.079243444 0.1150111 0.17319834 0.23373561 0.27179348 0.27543941 0.24490917 0.18568049 0.10805333 0.035758771 -0.01399209 -0.036700297 -0.035851154 -0.014729153 0.023314865][0.029322708 0.046974353 0.081263386 0.11388974 0.12554514 0.10970344 0.074060045 0.028792877 -0.020281442 -0.059200365 -0.079867132 -0.08268065 -0.0727822 -0.05302025 -0.02497578][-0.036660586 -0.036964227 -0.025888531 -0.017952938 -0.025082426 -0.048923705 -0.078577876 -0.10375644 -0.12357292 -0.13349572 -0.13210787 -0.12197561 -0.10805767 -0.092221394 -0.07496389][-0.095435187 -0.10698197 -0.11190114 -0.11986025 -0.13573255 -0.1578733 -0.17645548 -0.18390949 -0.1825204 -0.17395176 -0.16063948 -0.14547239 -0.1317095 -0.12007619 -0.11043029][-0.13069503 -0.14528456 -0.155545 -0.16798334 -0.18317832 -0.19851695 -0.20718519 -0.20479509 -0.19416837 -0.17897327 -0.16282682 -0.14844541 -0.13771665 -0.13021536 -0.12510069]]...]
INFO - root - 2017-12-11 04:45:48.622017: step 8910, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 47h:17m:27s remains)
INFO - root - 2017-12-11 04:45:53.998841: step 8920, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 46h:55m:38s remains)
INFO - root - 2017-12-11 04:45:59.389065: step 8930, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:19m:18s remains)
INFO - root - 2017-12-11 04:46:04.754717: step 8940, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 50h:03m:04s remains)
INFO - root - 2017-12-11 04:46:10.096297: step 8950, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:49m:01s remains)
INFO - root - 2017-12-11 04:46:15.441764: step 8960, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 48h:06m:23s remains)
INFO - root - 2017-12-11 04:46:20.691363: step 8970, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:28m:03s remains)
INFO - root - 2017-12-11 04:46:26.113263: step 8980, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 47h:49m:37s remains)
INFO - root - 2017-12-11 04:46:31.440359: step 8990, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 46h:58m:00s remains)
INFO - root - 2017-12-11 04:46:36.507135: step 9000, loss = 0.71, batch loss = 0.65 (23.9 examples/sec; 0.335 sec/batch; 30h:04m:06s remains)
2017-12-11 04:46:37.002694: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.44060504 0.4340848 0.41307741 0.39709681 0.38737574 0.37644371 0.36014608 0.33464473 0.3175199 0.32068196 0.33923978 0.35456634 0.34247321 0.3086133 0.26945132][0.43091193 0.42346746 0.39897731 0.37996292 0.3691788 0.35890213 0.34164828 0.31401283 0.2946257 0.29944184 0.32371378 0.34731641 0.3440879 0.31670964 0.28090277][0.38785714 0.38550445 0.36583871 0.3495582 0.33926362 0.32876062 0.30953622 0.28034514 0.25933057 0.263849 0.2901499 0.31864437 0.32431442 0.3063871 0.27804777][0.35824311 0.36264768 0.35344294 0.34356117 0.33309621 0.31864062 0.29388753 0.26174185 0.2382465 0.23900245 0.26067865 0.28499788 0.29288572 0.28213313 0.26262349][0.3569034 0.36805591 0.36931235 0.36601168 0.35536233 0.33715767 0.30921566 0.27772725 0.25299853 0.24662666 0.2561681 0.26664114 0.26805088 0.25897565 0.24578156][0.3886086 0.40096202 0.40609494 0.4057878 0.39487386 0.37542215 0.34838125 0.321535 0.29736885 0.28315425 0.27785444 0.27117139 0.26267907 0.25247517 0.24344291][0.43709072 0.44587189 0.4486078 0.44697225 0.4354656 0.41743642 0.39441517 0.37418714 0.35222274 0.33254942 0.31576753 0.29560566 0.27982202 0.27043116 0.26689982][0.46644306 0.47461629 0.4739576 0.46954611 0.45896137 0.4469682 0.43328747 0.4234705 0.40754393 0.38733461 0.36522114 0.33807021 0.31922206 0.31327224 0.31718355][0.4471983 0.45884627 0.46178353 0.46262681 0.46185139 0.46519589 0.46934357 0.47556221 0.47025588 0.45451725 0.43204907 0.40236169 0.38127998 0.37617779 0.38389403][0.37608293 0.39129347 0.40170845 0.41587394 0.43526423 0.46444258 0.49626285 0.52609986 0.537486 0.53064954 0.51010525 0.47786587 0.45106161 0.43943125 0.44230431][0.2641013 0.28091219 0.30016437 0.33079404 0.3745006 0.43426389 0.49859402 0.55583012 0.58717924 0.59071833 0.57095659 0.53182924 0.49220374 0.46601155 0.45754355][0.13488083 0.15271834 0.18021585 0.22436054 0.28765845 0.37270537 0.46470097 0.54488742 0.59262884 0.6034556 0.58168191 0.53217387 0.47605482 0.43261075 0.41191357][0.024588646 0.038495578 0.068148389 0.11637735 0.18610951 0.28042054 0.38412189 0.47412112 0.5286513 0.54162544 0.51766384 0.461541 0.39475223 0.33982256 0.31167749][-0.051802538 -0.048140079 -0.025701104 0.014807511 0.0756825 0.16026948 0.25596243 0.34024724 0.39210898 0.40520784 0.38384652 0.33109742 0.26646632 0.21258733 0.18715896][-0.097404353 -0.10584737 -0.096858189 -0.07335335 -0.033272188 0.027182095 0.099365726 0.16560541 0.20827229 0.22162579 0.20855607 0.16930434 0.11997057 0.081333049 0.071264133]]...]
INFO - root - 2017-12-11 04:46:42.367097: step 9010, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:51m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:46:47.724373: step 9020, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:14m:19s remains)
INFO - root - 2017-12-11 04:46:53.054379: step 9030, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 52h:09m:01s remains)
INFO - root - 2017-12-11 04:46:58.406543: step 9040, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:13m:28s remains)
INFO - root - 2017-12-11 04:47:03.800599: step 9050, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 46h:51m:18s remains)
INFO - root - 2017-12-11 04:47:09.063954: step 9060, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 46h:39m:43s remains)
INFO - root - 2017-12-11 04:47:14.390787: step 9070, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 49h:41m:52s remains)
INFO - root - 2017-12-11 04:47:19.783990: step 9080, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:35m:05s remains)
INFO - root - 2017-12-11 04:47:25.078894: step 9090, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 46h:51m:19s remains)
INFO - root - 2017-12-11 04:47:30.500254: step 9100, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:07m:01s remains)
2017-12-11 04:47:31.031151: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15106614 0.13631602 0.11662972 0.0988108 0.08794865 0.088064484 0.096742466 0.10839758 0.12574446 0.15255116 0.18534361 0.21715592 0.23908478 0.24257268 0.22437832][0.27332559 0.25745764 0.23237218 0.21125688 0.20151961 0.20626234 0.21894008 0.22889151 0.24433117 0.27414027 0.31597158 0.3594408 0.38897589 0.39221424 0.36312544][0.40263775 0.38836706 0.36222488 0.342921 0.33999684 0.3546232 0.37433732 0.38058311 0.38635179 0.40639493 0.44277966 0.48502374 0.51271749 0.51072121 0.47025013][0.50477886 0.49558821 0.47277722 0.45949775 0.46738493 0.49527225 0.52440137 0.525983 0.51555932 0.51300788 0.52644569 0.54969877 0.56266177 0.54953319 0.49845853][0.57153141 0.57135981 0.55457896 0.54815775 0.56668311 0.60689652 0.64572805 0.64353776 0.61379206 0.57882911 0.55247712 0.5369612 0.51953596 0.48937392 0.43220657][0.60529453 0.61706656 0.60615593 0.60323495 0.62746513 0.67528874 0.72180313 0.71838063 0.66976076 0.59581083 0.51673144 0.44675031 0.38693461 0.33785486 0.28511274][0.62090653 0.64599735 0.64176416 0.63851953 0.66070485 0.707338 0.75400418 0.74812025 0.68319952 0.57186151 0.43987778 0.31410015 0.2120021 0.14752054 0.10931648][0.63135797 0.67046523 0.67162549 0.661699 0.66966444 0.70101684 0.73527962 0.72217894 0.64667326 0.5129872 0.34839889 0.18843228 0.062998109 -0.0038851015 -0.017053941][0.6256367 0.6771366 0.68200761 0.661516 0.64675212 0.651771 0.66304547 0.63799071 0.558916 0.42388767 0.25680086 0.095682681 -0.02438508 -0.073421754 -0.048936527][0.58313644 0.6435082 0.65023 0.62020743 0.58379889 0.56170011 0.54852939 0.51197046 0.436596 0.31729439 0.17366292 0.040656909 -0.048778 -0.062057253 0.0096714478][0.49057257 0.55552018 0.5658443 0.53410095 0.48598909 0.44509244 0.41225195 0.36721709 0.29998761 0.203954 0.095429607 0.0032233812 -0.045932353 -0.019870805 0.09499836][0.3549374 0.42027017 0.4348442 0.40714413 0.35602686 0.30454841 0.25766471 0.20720452 0.15025696 0.079854615 0.0084764678 -0.043355249 -0.057356976 -0.0039616777 0.13411857][0.20159461 0.26000914 0.275371 0.25006577 0.197618 0.13934536 0.083865725 0.032709252 -0.011324399 -0.054949593 -0.090983175 -0.10827798 -0.097797722 -0.033999488 0.10271943][0.0631457 0.11208561 0.12641507 0.10265674 0.050763458 -0.010402387 -0.068693422 -0.11640219 -0.14808781 -0.16950548 -0.17823768 -0.17050539 -0.14581093 -0.085315235 0.028168287][-0.025014719 0.016717557 0.031407237 0.01070418 -0.038810033 -0.10058094 -0.15879613 -0.20199248 -0.22467519 -0.23240675 -0.22603409 -0.20612027 -0.17775379 -0.13106534 -0.055159234]]...]
INFO - root - 2017-12-11 04:47:36.106124: step 9110, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 48h:05m:41s remains)
INFO - root - 2017-12-11 04:47:41.557573: step 9120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:59m:20s remains)
INFO - root - 2017-12-11 04:47:46.876186: step 9130, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 47h:25m:42s remains)
INFO - root - 2017-12-11 04:47:52.255541: step 9140, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 47h:59m:24s remains)
INFO - root - 2017-12-11 04:47:57.607379: step 9150, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.532 sec/batch; 47h:44m:31s remains)
INFO - root - 2017-12-11 04:48:02.923677: step 9160, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 47h:23m:21s remains)
INFO - root - 2017-12-11 04:48:08.279314: step 9170, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:49m:45s remains)
INFO - root - 2017-12-11 04:48:13.567955: step 9180, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:32m:01s remains)
INFO - root - 2017-12-11 04:48:18.848365: step 9190, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:41m:01s remains)
INFO - root - 2017-12-11 04:48:24.255696: step 9200, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:14m:36s remains)
2017-12-11 04:48:24.818158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033264007 -0.012783425 0.016488543 0.049778678 0.085854113 0.11664678 0.13808621 0.14843298 0.15152983 0.14898966 0.13840406 0.12284035 0.10514888 0.08640299 0.06341967][-0.017881051 0.016507119 0.065145306 0.1225078 0.18632546 0.24303341 0.28374162 0.30101082 0.30182815 0.29265672 0.27338594 0.24657081 0.21378367 0.17942217 0.14034611][0.00377652 0.05677386 0.13406092 0.22884868 0.33551905 0.43207285 0.50166106 0.52594811 0.51395345 0.48276368 0.44191933 0.39572954 0.34174761 0.28707513 0.22859973][0.021116074 0.093667239 0.20383252 0.3423261 0.49895659 0.64171416 0.74287879 0.76871222 0.72993755 0.66201115 0.59113652 0.52448606 0.4517535 0.37981609 0.30482039][0.028638734 0.11604668 0.25419813 0.43073457 0.6305024 0.8131355 0.9392907 0.96064639 0.88971746 0.78349823 0.68684548 0.60731375 0.52456117 0.44229048 0.35654736][0.027988495 0.12250897 0.28016603 0.4851771 0.71644974 0.92695796 1.0686561 1.08426 0.98680753 0.850075 0.73500371 0.64888579 0.56331408 0.47675398 0.38476923][0.023433931 0.12170034 0.2928395 0.51838821 0.76908493 0.99210215 1.1369997 1.1453333 1.0292882 0.87068677 0.74269789 0.65442729 0.57081264 0.48324874 0.38655615][0.016859712 0.11653183 0.29437757 0.52892619 0.78245085 1.0004281 1.1358626 1.1343449 1.0080885 0.83955872 0.70839888 0.62530929 0.54996246 0.46592125 0.36690035][0.0035200196 0.097442217 0.26785988 0.49199486 0.72856158 0.92779338 1.0490636 1.0446222 0.92686051 0.77007622 0.65092683 0.57860464 0.51305228 0.43368644 0.33299696][-0.015975952 0.066138566 0.21852 0.41935629 0.62910014 0.80508953 0.91248739 0.91185904 0.81652075 0.6873377 0.58998328 0.52954346 0.471254 0.39420906 0.29012543][-0.036151614 0.035579994 0.16970295 0.3466424 0.52977836 0.68138945 0.771344 0.76955235 0.69329923 0.59097368 0.51444227 0.46430528 0.41166338 0.33774334 0.23348098][-0.057617929 0.0059903492 0.12411793 0.27843523 0.43451142 0.55842924 0.62486154 0.61291736 0.54578453 0.46315414 0.4036462 0.36431098 0.32069203 0.25647932 0.16147922][-0.085440911 -0.03510474 0.061907869 0.18744123 0.3102538 0.40264979 0.44501629 0.42321596 0.36349082 0.29897878 0.25618517 0.22909661 0.19822173 0.15070301 0.075314134][-0.11196736 -0.079461008 -0.0088951569 0.082217194 0.16804947 0.22821394 0.24923623 0.22303233 0.17469329 0.13027097 0.10457774 0.089755327 0.071910895 0.042329233 -0.0098976521][-0.12359767 -0.10650843 -0.0610445 -0.0031608965 0.047714345 0.078274168 0.080978446 0.053080305 0.015673064 -0.011876798 -0.023122728 -0.026227888 -0.031019155 -0.043355703 -0.072576918]]...]
INFO - root - 2017-12-11 04:48:29.935267: step 9210, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 48h:01m:34s remains)
INFO - root - 2017-12-11 04:48:35.327534: step 9220, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:59m:15s remains)
INFO - root - 2017-12-11 04:48:40.602844: step 9230, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:41m:27s remains)
INFO - root - 2017-12-11 04:48:45.938588: step 9240, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 49h:28m:14s remains)
INFO - root - 2017-12-11 04:48:51.329782: step 9250, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.579 sec/batch; 51h:59m:21s remains)
INFO - root - 2017-12-11 04:48:56.704762: step 9260, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 48h:21m:55s remains)
INFO - root - 2017-12-11 04:49:02.018937: step 9270, loss = 0.67, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 47h:57m:25s remains)
INFO - root - 2017-12-11 04:49:07.383282: step 9280, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:51m:18s remains)
INFO - root - 2017-12-11 04:49:12.726586: step 9290, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 48h:03m:56s remains)
INFO - root - 2017-12-11 04:49:18.055259: step 9300, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 48h:32m:20s remains)
2017-12-11 04:49:18.568536: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17911479 0.17476685 0.1684783 0.16883589 0.17188489 0.16753308 0.15769313 0.15188743 0.16040531 0.18451576 0.20798385 0.20783041 0.1854412 0.15445463 0.1301987][0.20965391 0.20262404 0.19458984 0.19503503 0.20048335 0.19916013 0.19290568 0.18448053 0.18364167 0.19653982 0.21333827 0.21203065 0.18972795 0.15863372 0.13364276][0.21766977 0.20944652 0.20164305 0.20189857 0.20899534 0.21377508 0.21473871 0.20611097 0.19448604 0.19060376 0.19388042 0.18781598 0.16726898 0.1414517 0.12302212][0.21399863 0.20637608 0.20145091 0.20251802 0.21071339 0.22096245 0.23033936 0.22548147 0.20716187 0.18823804 0.17727232 0.16622369 0.14906806 0.13093042 0.12121059][0.19812928 0.18897761 0.18610984 0.18916687 0.20010664 0.21705066 0.23756547 0.24306205 0.22516634 0.19529656 0.16972446 0.15277998 0.14013171 0.13143627 0.13076042][0.1859414 0.1720925 0.16573176 0.16658951 0.17757264 0.19948781 0.23206264 0.25417277 0.24511959 0.21022652 0.1707205 0.1461426 0.13774306 0.13981867 0.14879666][0.19102684 0.17333163 0.15900193 0.15078835 0.15405817 0.17361557 0.2133086 0.25240314 0.25760987 0.22479485 0.17569025 0.14358033 0.13758698 0.14908122 0.16643855][0.20569715 0.18832751 0.16643031 0.14586526 0.13451655 0.14255339 0.17889178 0.22630927 0.24404763 0.21950655 0.17070542 0.13720018 0.13379094 0.15195736 0.17460109][0.21414955 0.20130867 0.17738457 0.14910696 0.12457583 0.11843076 0.14309147 0.18632969 0.20844382 0.19344789 0.15418519 0.12721853 0.12732509 0.14747886 0.169594][0.19938278 0.19083704 0.16905694 0.13995317 0.1094514 0.0934841 0.10462858 0.13438222 0.15145963 0.1428014 0.11776375 0.10360686 0.10963134 0.12958603 0.14802703][0.16363001 0.15518661 0.13560084 0.10920712 0.078973986 0.059123866 0.059407584 0.073399141 0.081319429 0.076624073 0.066906907 0.067783423 0.080901787 0.099924386 0.11415952][0.10999305 0.10070253 0.084712073 0.064233206 0.039695859 0.021572005 0.015646344 0.016775552 0.015309559 0.011685045 0.012972496 0.024554495 0.041676305 0.057887405 0.067605346][0.044917285 0.037055392 0.027530435 0.016281106 0.0022909341 -0.0082168393 -0.013090509 -0.016890045 -0.023515642 -0.028430028 -0.024560738 -0.012230772 0.0012235909 0.010597812 0.014429239][-0.01300439 -0.018202491 -0.020789055 -0.022608807 -0.025307138 -0.026288608 -0.025817994 -0.027772438 -0.033992432 -0.039663717 -0.038673703 -0.032835405 -0.028445955 -0.028422395 -0.031449959][-0.053375125 -0.056778982 -0.055643067 -0.052258525 -0.048196115 -0.043016795 -0.038157925 -0.036525447 -0.03946897 -0.044056207 -0.046233337 -0.047237787 -0.05057339 -0.05690372 -0.06385316]]...]
INFO - root - 2017-12-11 04:49:23.586982: step 9310, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 47h:32m:56s remains)
INFO - root - 2017-12-11 04:49:28.968572: step 9320, loss = 0.67, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 46h:44m:45s remains)
INFO - root - 2017-12-11 04:49:34.263874: step 9330, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 47h:55m:32s remains)
INFO - root - 2017-12-11 04:49:39.706213: step 9340, loss = 0.72, batch loss = 0.66 (15.9 examples/sec; 0.503 sec/batch; 45h:07m:15s remains)
INFO - root - 2017-12-11 04:49:45.090783: step 9350, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:44m:36s remains)
INFO - root - 2017-12-11 04:49:50.399806: step 9360, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.513 sec/batch; 46h:01m:43s remains)
INFO - root - 2017-12-11 04:49:55.694013: step 9370, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:39m:36s remains)
INFO - root - 2017-12-11 04:50:01.012945: step 9380, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:41m:54s remains)
INFO - root - 2017-12-11 04:50:06.290460: step 9390, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:44m:32s remains)
INFO - root - 2017-12-11 04:50:11.721387: step 9400, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 48h:13m:35s remains)
2017-12-11 04:50:12.225496: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.64996052 0.62639117 0.58078653 0.53644681 0.49623379 0.46363303 0.43854952 0.41648045 0.41984388 0.46237332 0.53080779 0.5843401 0.58760393 0.52915615 0.39911294][0.59374613 0.57319343 0.53117055 0.49806145 0.47100124 0.44898537 0.4327383 0.41869667 0.42607564 0.46603477 0.5309273 0.58412528 0.58811104 0.531729 0.40346882][0.47167623 0.45425057 0.42772356 0.41667581 0.41087386 0.40375534 0.39760715 0.39016417 0.39609724 0.42449111 0.47457832 0.51860726 0.52071714 0.46839729 0.35123771][0.36832565 0.35935557 0.35554305 0.36913913 0.38398623 0.38903219 0.38748372 0.37812313 0.37568384 0.39041159 0.42406866 0.45479652 0.44836977 0.39439073 0.28588283][0.32835659 0.33935973 0.36517543 0.40501523 0.43754956 0.44905037 0.44551602 0.42810056 0.41422006 0.4169476 0.43531436 0.44863835 0.42340243 0.35415789 0.24115559][0.38115594 0.42499363 0.4847151 0.55005252 0.594918 0.6049037 0.592902 0.565095 0.54055542 0.53380215 0.53855044 0.53106374 0.47597975 0.37477463 0.2395115][0.50291556 0.58043069 0.67072451 0.75604743 0.80491936 0.805213 0.7799356 0.74198365 0.70988417 0.69659257 0.69168133 0.6652298 0.577006 0.435712 0.26796669][0.6047315 0.70475787 0.81781644 0.91833794 0.96915644 0.96240228 0.929092 0.88495183 0.84615362 0.82347113 0.80648869 0.76049691 0.64340508 0.47035348 0.27875057][0.60181528 0.70369285 0.8249954 0.93456644 0.9910773 0.99091744 0.96487021 0.92313367 0.87717181 0.83756924 0.79954308 0.73029238 0.59330958 0.40966329 0.21877176][0.46294615 0.53916454 0.64704782 0.75423187 0.81814057 0.83765733 0.83630097 0.81091958 0.76491708 0.71078414 0.6528495 0.5655939 0.424005 0.25384235 0.090315677][0.22118753 0.24879614 0.31903315 0.40520161 0.4692452 0.51011091 0.54045308 0.54239225 0.51047152 0.45827121 0.39777616 0.31112942 0.1848942 0.048081964 -0.070139304][-0.043196712 -0.064343236 -0.038470935 0.015624992 0.069360629 0.12077079 0.17172424 0.19690834 0.18593508 0.15144157 0.10844185 0.044903141 -0.045673631 -0.13582543 -0.20318085][-0.26086867 -0.306593 -0.30827022 -0.28084689 -0.24288438 -0.19592164 -0.1438854 -0.10955229 -0.10344314 -0.11566173 -0.13170707 -0.15832543 -0.1997854 -0.23750669 -0.25866789][-0.38683072 -0.42762408 -0.43234408 -0.41683924 -0.39171657 -0.35746223 -0.31816822 -0.28737026 -0.27053019 -0.26194027 -0.25081789 -0.23992918 -0.23374201 -0.22650656 -0.21690293][-0.39970094 -0.41948664 -0.41279063 -0.39715424 -0.37806788 -0.35516718 -0.33012015 -0.30832127 -0.28878626 -0.26765382 -0.23833542 -0.20164622 -0.16398853 -0.13098764 -0.11070357]]...]
INFO - root - 2017-12-11 04:50:17.559777: step 9410, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:31m:38s remains)
INFO - root - 2017-12-11 04:50:22.657781: step 9420, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:13m:20s remains)
INFO - root - 2017-12-11 04:50:28.028033: step 9430, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:51m:12s remains)
INFO - root - 2017-12-11 04:50:33.410244: step 9440, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:38m:52s remains)
INFO - root - 2017-12-11 04:50:38.659031: step 9450, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 47h:53m:06s remains)
INFO - root - 2017-12-11 04:50:44.045130: step 9460, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 51h:46m:16s remains)
INFO - root - 2017-12-11 04:50:49.500233: step 9470, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:18m:10s remains)
INFO - root - 2017-12-11 04:50:54.928951: step 9480, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 49h:01m:24s remains)
INFO - root - 2017-12-11 04:51:00.215037: step 9490, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:50m:30s remains)
INFO - root - 2017-12-11 04:51:05.575316: step 9500, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:18m:39s remains)
2017-12-11 04:51:06.126089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21838671 0.24646269 0.2579895 0.26603469 0.29256845 0.33670864 0.389579 0.42590073 0.42401052 0.38252985 0.31268984 0.2360175 0.16361716 0.10418757 0.062368188][0.18365104 0.2087471 0.22394657 0.23971891 0.27461025 0.32523194 0.38292491 0.42409188 0.4262532 0.38570169 0.3124885 0.2296015 0.1504135 0.085572846 0.040375125][0.14499867 0.15939908 0.17129844 0.19132514 0.23274221 0.28773549 0.34593427 0.38615021 0.38795945 0.34771249 0.27574542 0.19495752 0.11901779 0.058789138 0.017847562][0.11094301 0.11317173 0.1197514 0.14267951 0.1907634 0.25166571 0.31039107 0.34640157 0.34259927 0.29840314 0.22723295 0.15212119 0.086186744 0.037946798 0.0070230677][0.080380417 0.072763234 0.075384967 0.10105795 0.15531184 0.22206458 0.28150272 0.31318679 0.30356568 0.25609687 0.18748395 0.1205297 0.067406476 0.033244215 0.013391423][0.066156067 0.04960506 0.049166765 0.0780893 0.13867024 0.211545 0.27287808 0.30199364 0.28807995 0.23856527 0.17260624 0.11208576 0.068872333 0.045731857 0.03487169][0.07530807 0.05100479 0.047305241 0.078626573 0.1443526 0.22306809 0.2873567 0.31542751 0.298105 0.24626784 0.18148325 0.12413956 0.086855568 0.07158839 0.068207189][0.0914743 0.063032933 0.057391066 0.09114363 0.16139486 0.24602808 0.31467256 0.34378257 0.32478234 0.27072185 0.20519425 0.14691347 0.10957547 0.09571138 0.094502352][0.10372401 0.076170005 0.072651058 0.11050045 0.18515828 0.27505285 0.3477684 0.37741756 0.35605642 0.29819509 0.22880967 0.16490033 0.12086087 0.10093638 0.095393084][0.11558077 0.093216144 0.094093621 0.13473141 0.20932524 0.29852772 0.36994466 0.39605984 0.36975887 0.30650997 0.23231515 0.16215031 0.11045377 0.083748467 0.074312255][0.12915933 0.11404125 0.11943412 0.1604059 0.23026149 0.3124927 0.37648836 0.39483577 0.36096504 0.29103303 0.21248582 0.13915284 0.084858574 0.056964923 0.048894972][0.14492378 0.13692525 0.14494748 0.18255988 0.2430391 0.31234857 0.36317727 0.37045869 0.32839614 0.253604 0.17445683 0.10456603 0.056021828 0.03425613 0.032474872][0.16274743 0.16281916 0.17376877 0.20692155 0.25539127 0.30665746 0.33864728 0.33161187 0.28219464 0.20787676 0.13592948 0.0782851 0.042705625 0.030420598 0.034699883][0.17900848 0.18652287 0.20025533 0.2280533 0.26301616 0.29365748 0.30428767 0.28280434 0.22953597 0.16342627 0.10838256 0.072059244 0.055538513 0.054831903 0.06437137][0.18819301 0.20078938 0.21433753 0.23413286 0.25420007 0.26455823 0.25599539 0.22449644 0.17367665 0.12271996 0.090709016 0.080195166 0.085453987 0.097631872 0.11320782]]...]
INFO - root - 2017-12-11 04:51:11.435053: step 9510, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.514 sec/batch; 46h:05m:57s remains)
INFO - root - 2017-12-11 04:51:16.480338: step 9520, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:29m:33s remains)
INFO - root - 2017-12-11 04:51:21.833096: step 9530, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:32m:27s remains)
INFO - root - 2017-12-11 04:51:27.129550: step 9540, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 47h:08m:10s remains)
INFO - root - 2017-12-11 04:51:32.468400: step 9550, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:51m:58s remains)
INFO - root - 2017-12-11 04:51:37.820505: step 9560, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:48m:59s remains)
INFO - root - 2017-12-11 04:51:43.215781: step 9570, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:47m:03s remains)
INFO - root - 2017-12-11 04:51:48.491095: step 9580, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:55m:34s remains)
INFO - root - 2017-12-11 04:51:53.879977: step 9590, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:55m:11s remains)
INFO - root - 2017-12-11 04:51:59.282552: step 9600, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 48h:35m:18s remains)
2017-12-11 04:51:59.852626: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00098452 -0.0058945925 0.016185448 0.069168657 0.14511125 0.22819819 0.30267334 0.35345539 0.37545559 0.36948514 0.33937228 0.29327706 0.24074991 0.19397724 0.15658207][0.010395028 -0.0037417833 0.0095981751 0.054954134 0.12719619 0.21352272 0.29863185 0.36472344 0.40082046 0.40270022 0.37079409 0.31423497 0.24698305 0.18776457 0.14553037][0.041050281 0.02145559 0.024613794 0.057925716 0.11851344 0.19790988 0.28370386 0.35599741 0.39913976 0.40389052 0.36863866 0.30428493 0.22748198 0.16020428 0.11483604][0.094736673 0.070468709 0.063448124 0.085196495 0.13337457 0.203209 0.28501216 0.35684469 0.39935854 0.40003148 0.35767397 0.28586227 0.2019982 0.1285203 0.079598069][0.16011235 0.12843144 0.10917242 0.11958405 0.15642829 0.21696706 0.29332602 0.36166704 0.40000048 0.39370021 0.3433108 0.26533258 0.17651433 0.0974952 0.043758851][0.22684549 0.18615067 0.1525584 0.1512042 0.17777136 0.22962077 0.29985061 0.36308974 0.39564657 0.38141769 0.32275546 0.23954974 0.14799653 0.066165164 0.0094525227][0.29094693 0.24343525 0.19613476 0.18324547 0.20104471 0.24634059 0.31193203 0.37080708 0.39739457 0.37378663 0.30404007 0.21230131 0.1170624 0.035007853 -0.020401582][0.34288847 0.29377061 0.23858793 0.21831106 0.23171647 0.27488318 0.33878621 0.3950389 0.41689295 0.38457406 0.30251223 0.19836064 0.095517196 0.011437523 -0.043083612][0.36408874 0.32080904 0.26810834 0.24921398 0.26605067 0.31218866 0.37466267 0.42513222 0.43907559 0.3979629 0.30596614 0.19168219 0.08320763 -0.0013124085 -0.054425523][0.34468105 0.31446245 0.2735002 0.26286769 0.28772676 0.33821604 0.39637071 0.43619183 0.43872586 0.39006647 0.29409218 0.17792183 0.072171219 -0.00558532 -0.052481264][0.29461503 0.27987745 0.25411975 0.25135142 0.28077278 0.33201456 0.38366166 0.41291052 0.40655696 0.35541758 0.2630173 0.15329143 0.0578899 -0.0061050951 -0.040557727][0.22195964 0.21670011 0.20244911 0.20151475 0.22671869 0.27137989 0.31449819 0.33707234 0.32995939 0.28644755 0.20828779 0.11453798 0.035955768 -0.010314091 -0.029349541][0.13156791 0.12701376 0.11527996 0.10788942 0.12001171 0.15036762 0.1811187 0.1976463 0.19419467 0.16625592 0.1132606 0.047885679 -0.0035570967 -0.02588311 -0.02602393][0.045830753 0.036333691 0.019247804 0.00015457631 -0.004734416 0.0084001161 0.02641483 0.038563143 0.041042078 0.031002663 0.0062156129 -0.026255958 -0.045943238 -0.04108391 -0.02016014][-0.0049048634 -0.020374784 -0.043840643 -0.072423421 -0.09039031 -0.090497181 -0.080413744 -0.068119325 -0.0572526 -0.050836656 -0.052551385 -0.058882542 -0.054115307 -0.02878404 0.0076926122]]...]
INFO - root - 2017-12-11 04:52:05.180278: step 9610, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 47h:15m:50s remains)
INFO - root - 2017-12-11 04:52:10.207271: step 9620, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 46h:56m:06s remains)
INFO - root - 2017-12-11 04:52:15.611953: step 9630, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:01m:57s remains)
INFO - root - 2017-12-11 04:52:20.913652: step 9640, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 47h:13m:41s remains)
INFO - root - 2017-12-11 04:52:26.265027: step 9650, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:54m:03s remains)
INFO - root - 2017-12-11 04:52:31.664884: step 9660, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 48h:18m:02s remains)
INFO - root - 2017-12-11 04:52:36.995372: step 9670, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:51m:03s remains)
INFO - root - 2017-12-11 04:52:42.264502: step 9680, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:46m:42s remains)
INFO - root - 2017-12-11 04:52:47.693921: step 9690, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:15m:27s remains)
INFO - root - 2017-12-11 04:52:52.999962: step 9700, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:01m:05s remains)
2017-12-11 04:52:53.585447: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082327947 0.063177153 0.05151172 0.047508959 0.043195356 0.036504023 0.029353792 0.021171883 0.013052273 0.004655824 -0.0071762982 -0.024039131 -0.043104295 -0.059925608 -0.07257089][0.1966923 0.1683455 0.15048875 0.14347708 0.13593481 0.12608677 0.1179418 0.10935605 0.098358952 0.082894079 0.057876129 0.023286067 -0.013731499 -0.044725433 -0.067324787][0.31931931 0.28565419 0.26368269 0.25529081 0.24733618 0.23982765 0.23717533 0.23271793 0.21935518 0.19280757 0.14787002 0.088537134 0.028077135 -0.020155426 -0.054149561][0.40423781 0.37219816 0.35253823 0.34944233 0.35112551 0.36009157 0.377117 0.38609114 0.37231368 0.329847 0.25649351 0.16388975 0.074014135 0.00590654 -0.039890915][0.42476889 0.39905348 0.38877943 0.39993918 0.42536661 0.46915352 0.52321666 0.55714148 0.54605252 0.4842383 0.37605053 0.24357368 0.11955249 0.029506883 -0.027884401][0.38261798 0.36748576 0.37292951 0.406303 0.46618521 0.55616528 0.65510511 0.71693707 0.70760524 0.62437075 0.48055556 0.30883271 0.15228458 0.042320222 -0.023824494][0.28446624 0.28259674 0.30845767 0.36931115 0.46910885 0.60832077 0.75075185 0.83544976 0.82257271 0.71638274 0.54094559 0.3381297 0.15801932 0.035671815 -0.032878984][0.1735258 0.18229382 0.22598866 0.308904 0.44218192 0.62337661 0.801515 0.90165484 0.88009149 0.75238419 0.55264562 0.32991344 0.13825378 0.013374772 -0.050685052][0.11256855 0.12507284 0.1723223 0.25650004 0.39964244 0.59896743 0.7927832 0.896991 0.86525857 0.72480345 0.51467496 0.28693172 0.097341433 -0.019632516 -0.072781995][0.12183157 0.13391806 0.16772616 0.22815557 0.34905937 0.53036654 0.70866227 0.79979181 0.75778764 0.61812991 0.41801354 0.20691983 0.037454486 -0.05998607 -0.096447885][0.17127165 0.18046677 0.19053786 0.21168232 0.28783408 0.42356521 0.562192 0.62739897 0.57766074 0.45212379 0.28037924 0.10466817 -0.030630471 -0.10101995 -0.11880433][0.22154853 0.22172223 0.20375854 0.18123724 0.20439017 0.28349659 0.37335542 0.41085339 0.36161396 0.26282537 0.13306625 0.0035491525 -0.091885351 -0.13461564 -0.13578703][0.24296951 0.23378426 0.19031441 0.12827976 0.10009856 0.12283809 0.16610977 0.181923 0.14420092 0.081812918 0.00092133717 -0.0798595 -0.13698216 -0.15589923 -0.14485243][0.21906315 0.20463419 0.14825858 0.0633364 -0.0015570832 -0.023875607 -0.017072348 -0.013683899 -0.031969365 -0.056045931 -0.090551637 -0.12796542 -0.1538329 -0.15682314 -0.14150974][0.15206453 0.13771309 0.083278939 -0.0022070867 -0.079457454 -0.12420713 -0.13633981 -0.13520308 -0.13222401 -0.12402866 -0.12270796 -0.1288382 -0.13530836 -0.1332375 -0.12327399]]...]
INFO - root - 2017-12-11 04:52:58.936102: step 9710, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 47h:59m:25s remains)
INFO - root - 2017-12-11 04:53:04.193679: step 9720, loss = 0.68, batch loss = 0.63 (18.6 examples/sec; 0.431 sec/batch; 38h:36m:32s remains)
INFO - root - 2017-12-11 04:53:09.359037: step 9730, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:01m:26s remains)
INFO - root - 2017-12-11 04:53:14.645064: step 9740, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 47h:58m:17s remains)
INFO - root - 2017-12-11 04:53:19.965593: step 9750, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 47h:29m:01s remains)
INFO - root - 2017-12-11 04:53:25.378390: step 9760, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:33m:06s remains)
INFO - root - 2017-12-11 04:53:30.752298: step 9770, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 49h:31m:59s remains)
INFO - root - 2017-12-11 04:53:36.061341: step 9780, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.529 sec/batch; 47h:26m:15s remains)
INFO - root - 2017-12-11 04:53:41.328677: step 9790, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:28m:28s remains)
INFO - root - 2017-12-11 04:53:46.671128: step 9800, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 46h:57m:10s remains)
2017-12-11 04:53:47.201759: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34339458 0.38040555 0.38046482 0.35841668 0.32015803 0.2852256 0.26817277 0.26539648 0.26396808 0.25594276 0.2453485 0.23086698 0.21342812 0.20193945 0.20797417][0.20325135 0.22738972 0.22900264 0.21886963 0.20315829 0.19624647 0.20717926 0.23071833 0.25209227 0.26237124 0.26308551 0.25590625 0.24586625 0.24424586 0.26269704][0.06980449 0.078099981 0.079662353 0.083970152 0.095552281 0.12359211 0.16887368 0.22086287 0.26346931 0.28672194 0.29115668 0.28294861 0.27230927 0.27313533 0.29585072][0.017228829 0.0079938285 0.007215119 0.024500955 0.062291265 0.12267427 0.19732939 0.27017966 0.32368439 0.34861842 0.34457663 0.32182682 0.29559091 0.2827729 0.2937308][0.036516633 0.013687599 0.012312058 0.044170663 0.10790768 0.19659108 0.29261714 0.374542 0.42439744 0.43596676 0.41001114 0.36042818 0.30674207 0.26864415 0.25729093][0.086887591 0.057531726 0.06069874 0.10950657 0.19785106 0.30967122 0.41904745 0.49965778 0.53451908 0.52217776 0.46722147 0.38705871 0.30494186 0.24267505 0.21170054][0.12592193 0.10108607 0.11673124 0.18520173 0.29565951 0.42458126 0.5402987 0.61391139 0.63114214 0.59545493 0.51559067 0.41177773 0.30968085 0.23236293 0.19062656][0.14487156 0.13622627 0.17056981 0.25696293 0.38118654 0.51604539 0.62873465 0.69140834 0.6936841 0.64243358 0.54931074 0.4360984 0.32912779 0.25110316 0.21068549][0.16293465 0.17451617 0.22428794 0.31749842 0.44012609 0.5666334 0.66724396 0.71710014 0.70887619 0.651099 0.55771989 0.45084581 0.35503998 0.29010382 0.26190323][0.19801851 0.22457753 0.27654663 0.36035368 0.46566454 0.57236344 0.65588444 0.69509983 0.68426991 0.63134807 0.55022389 0.46142593 0.38611317 0.34022546 0.32749215][0.26728657 0.29607913 0.33502942 0.39458713 0.4697808 0.54825294 0.61220485 0.64488494 0.64042282 0.60259295 0.54183412 0.47509313 0.42003658 0.38931724 0.38613868][0.35785228 0.38079506 0.39716136 0.42367706 0.46064559 0.5035693 0.54262024 0.56675071 0.57086015 0.55222088 0.51506877 0.47086042 0.43352538 0.41304335 0.41314963][0.43324256 0.44892922 0.44325978 0.43713668 0.43504378 0.43941134 0.44922617 0.46077639 0.47140449 0.47131333 0.45694238 0.43398947 0.41287637 0.4021734 0.40589663][0.45552805 0.46927565 0.4529171 0.42628425 0.3958528 0.3698194 0.35575876 0.3568773 0.37258714 0.38715521 0.39033768 0.3838672 0.37544703 0.37317112 0.38158351][0.42269209 0.44133142 0.42862761 0.39751974 0.35406953 0.31022888 0.28026655 0.27276355 0.28853035 0.30934393 0.32153353 0.32499576 0.32593498 0.33222115 0.34821239]]...]
INFO - root - 2017-12-11 04:53:52.480047: step 9810, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:33m:36s remains)
INFO - root - 2017-12-11 04:53:57.851176: step 9820, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 47h:57m:25s remains)
INFO - root - 2017-12-11 04:54:03.005799: step 9830, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 49h:18m:41s remains)
INFO - root - 2017-12-11 04:54:08.364869: step 9840, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.535 sec/batch; 47h:59m:22s remains)
INFO - root - 2017-12-11 04:54:13.804870: step 9850, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 48h:55m:39s remains)
INFO - root - 2017-12-11 04:54:19.122016: step 9860, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 47h:07m:16s remains)
INFO - root - 2017-12-11 04:54:24.423119: step 9870, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:44m:33s remains)
INFO - root - 2017-12-11 04:54:29.836098: step 9880, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:09m:42s remains)
INFO - root - 2017-12-11 04:54:35.289635: step 9890, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:41m:14s remains)
INFO - root - 2017-12-11 04:54:40.753857: step 9900, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:21m:50s remains)
2017-12-11 04:54:41.315614: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33878708 0.31279767 0.25947198 0.21515974 0.19438715 0.19010882 0.1914472 0.19526851 0.19789375 0.19481173 0.19496182 0.20569572 0.22841719 0.26035163 0.29042029][0.47982162 0.4354409 0.36051178 0.30490455 0.28724751 0.29423866 0.30562559 0.31489608 0.31769595 0.30949751 0.30478114 0.31277487 0.33521494 0.37022561 0.4045][0.58870047 0.53171819 0.44682348 0.38871637 0.37872547 0.39931539 0.42294049 0.43869418 0.44138542 0.42764747 0.41631618 0.417154 0.4327378 0.46210673 0.49134165][0.62581033 0.56913429 0.49017838 0.43973184 0.44023708 0.47302872 0.50783086 0.52967751 0.5320586 0.51343656 0.49434879 0.48531598 0.49169955 0.51187646 0.53206104][0.59525043 0.54888231 0.48749042 0.45413736 0.46827987 0.51045448 0.55172414 0.5756709 0.57525891 0.55062604 0.520506 0.49875203 0.49710703 0.51217252 0.52943087][0.53740543 0.5038504 0.46091163 0.44562489 0.47158742 0.51777524 0.55881363 0.58070254 0.57656074 0.54606706 0.50282484 0.46583614 0.45582992 0.46847773 0.48789015][0.49522787 0.47311652 0.44207194 0.43625692 0.46443918 0.50582194 0.53995574 0.55762488 0.552011 0.51998913 0.46794003 0.41786188 0.39691633 0.40199068 0.41900498][0.48388681 0.47327334 0.45084876 0.44551855 0.46543568 0.49444386 0.51768541 0.52868366 0.51995218 0.48723087 0.43311313 0.37920523 0.35279411 0.35158774 0.36440009][0.49984828 0.50375015 0.49076107 0.48411915 0.49280149 0.5066511 0.51560956 0.51136839 0.48739824 0.44630489 0.39350855 0.34833151 0.33096433 0.33609745 0.35132247][0.53104222 0.54615581 0.54206461 0.53625554 0.53405416 0.52934128 0.51765966 0.48961362 0.4430756 0.3907629 0.34212735 0.3124637 0.31230491 0.33180505 0.35420355][0.53266943 0.54887211 0.549955 0.54801774 0.53887647 0.51653594 0.48431551 0.4371506 0.37867391 0.3258605 0.2864767 0.27080923 0.2837179 0.31378195 0.34106174][0.47570908 0.48649538 0.49006429 0.492763 0.48040986 0.44585183 0.40041712 0.34643558 0.29149669 0.24901795 0.22195035 0.21717393 0.23803341 0.27296895 0.30141875][0.35121843 0.35844612 0.36491781 0.37054604 0.35770246 0.31922504 0.27068856 0.21971717 0.17524177 0.14484446 0.12852393 0.13261627 0.15986031 0.19727723 0.2246628][0.1760001 0.18174729 0.18735728 0.18999659 0.17688696 0.14292017 0.10259595 0.064351834 0.03528985 0.017569879 0.010784538 0.021783421 0.051746402 0.086677983 0.10889513][-0.0014752779 0.00037426379 0.00015201569 -0.0039216997 -0.015413323 -0.037128158 -0.059986584 -0.077960759 -0.088132128 -0.092476018 -0.091041736 -0.077426754 -0.051207207 -0.02444678 -0.0097230356]]...]
INFO - root - 2017-12-11 04:54:46.733498: step 9910, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 48h:15m:44s remains)
INFO - root - 2017-12-11 04:54:52.099431: step 9920, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:04m:03s remains)
INFO - root - 2017-12-11 04:54:57.169037: step 9930, loss = 0.72, batch loss = 0.67 (15.0 examples/sec; 0.532 sec/batch; 47h:41m:42s remains)
INFO - root - 2017-12-11 04:55:02.474912: step 9940, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:32m:17s remains)
INFO - root - 2017-12-11 04:55:07.832323: step 9950, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 47h:39m:29s remains)
INFO - root - 2017-12-11 04:55:13.129233: step 9960, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:56m:31s remains)
INFO - root - 2017-12-11 04:55:18.514109: step 9970, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:31m:32s remains)
INFO - root - 2017-12-11 04:55:23.885720: step 9980, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 47h:52m:45s remains)
INFO - root - 2017-12-11 04:55:29.140867: step 9990, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:13m:21s remains)
INFO - root - 2017-12-11 04:55:34.582351: step 10000, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.559 sec/batch; 50h:05m:23s remains)
2017-12-11 04:55:35.180680: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22225814 0.24604285 0.25445667 0.2440183 0.22248958 0.19598807 0.17059696 0.1528587 0.16920786 0.22850636 0.30201831 0.35827559 0.37340218 0.34786293 0.27609274][0.24571736 0.27277228 0.27760988 0.25944135 0.22838251 0.19333188 0.16064508 0.13697715 0.14688404 0.19630778 0.26020217 0.30955726 0.32420582 0.30462974 0.24228364][0.25040945 0.28001758 0.28368282 0.26384211 0.23091871 0.19533159 0.16217621 0.13719511 0.13854538 0.16945554 0.21147554 0.24255507 0.24881597 0.23163438 0.18187584][0.24792829 0.28220627 0.2928609 0.28535855 0.26631325 0.24385926 0.22005431 0.19820872 0.1900339 0.19776927 0.21076041 0.2146284 0.20305833 0.17915311 0.13491565][0.23973446 0.2828168 0.31189448 0.33359829 0.34603155 0.35189632 0.34950092 0.3369098 0.32138222 0.30655295 0.2886025 0.25961658 0.22028129 0.17731631 0.12516041][0.22388187 0.27563691 0.32805651 0.3869001 0.4398019 0.48258245 0.509948 0.5133087 0.49748471 0.46908522 0.42790771 0.36910358 0.29855254 0.22839482 0.15653835][0.20866634 0.26416457 0.33533666 0.42563325 0.51370239 0.58872074 0.64330071 0.66395086 0.6553663 0.62873012 0.58213735 0.50752485 0.41243991 0.314281 0.21605042][0.21106552 0.25960085 0.33340937 0.43461794 0.53803849 0.62853211 0.69855136 0.73268962 0.73781019 0.72934788 0.69767296 0.62622523 0.519855 0.40025875 0.27643541][0.2429406 0.27153572 0.32556641 0.40837252 0.49844 0.58137548 0.6510734 0.6922732 0.71459442 0.73531842 0.73366117 0.68311477 0.58206612 0.45426762 0.31376976][0.30013832 0.29647174 0.30843672 0.34451711 0.39363265 0.44749174 0.50228 0.54371256 0.58211344 0.63384426 0.66753906 0.64892596 0.56917471 0.45096669 0.31047732][0.37092337 0.33057785 0.29000926 0.26663795 0.26281375 0.27778208 0.30855581 0.34192759 0.38714987 0.45889845 0.52021515 0.53382033 0.4850024 0.39102858 0.266654][0.44083375 0.37032503 0.28451467 0.21083778 0.16290332 0.14470883 0.15119846 0.16885868 0.20463337 0.27327311 0.34061384 0.3731887 0.35310626 0.28954905 0.19252208][0.49644858 0.41024047 0.2984491 0.19625556 0.12353197 0.085440435 0.073735841 0.072643153 0.086375982 0.13148682 0.18438604 0.21978927 0.21864712 0.18272747 0.11562619][0.52293253 0.44170919 0.3278347 0.22019853 0.14117432 0.095505945 0.073677734 0.057528492 0.047831032 0.062784515 0.093263395 0.12324973 0.13249299 0.11620724 0.07250379][0.51134241 0.4493635 0.35112208 0.25489384 0.18213536 0.13715824 0.11203879 0.088482372 0.064272046 0.057424229 0.070534654 0.094768465 0.10896871 0.10345266 0.072896659]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 04:55:41.280406: step 10010, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:08m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 04:55:46.725856: step 10020, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 48h:09m:10s remains)
INFO - root - 2017-12-11 04:55:51.785402: step 10030, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:21m:48s remains)
INFO - root - 2017-12-11 04:55:57.132967: step 10040, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:30m:53s remains)
INFO - root - 2017-12-11 04:56:02.493197: step 10050, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 46h:32m:57s remains)
INFO - root - 2017-12-11 04:56:07.799435: step 10060, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:05m:26s remains)
INFO - root - 2017-12-11 04:56:13.073259: step 10070, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 46h:38m:28s remains)
INFO - root - 2017-12-11 04:56:18.387703: step 10080, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 48h:11m:24s remains)
INFO - root - 2017-12-11 04:56:23.779660: step 10090, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.525 sec/batch; 47h:01m:52s remains)
INFO - root - 2017-12-11 04:56:29.120732: step 10100, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:11m:13s remains)
2017-12-11 04:56:29.679791: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16673753 0.16825587 0.16757438 0.16574113 0.16067731 0.15177055 0.13589622 0.11222123 0.087889217 0.069903113 0.060289532 0.057011653 0.060265325 0.0704833 0.081168823][0.1853385 0.19240285 0.1929166 0.18780223 0.17657286 0.16098671 0.14057548 0.11655366 0.094214119 0.077471554 0.067025162 0.06177488 0.063791871 0.073954388 0.085620031][0.18571508 0.19669686 0.1994168 0.19288896 0.17725937 0.15587373 0.1317582 0.10864502 0.088678487 0.072935626 0.062054913 0.056531005 0.058826059 0.068796873 0.080187187][0.1842635 0.19726536 0.20284611 0.19601074 0.1773558 0.15114371 0.12259205 0.097638078 0.076596908 0.060052365 0.048765771 0.043606687 0.046156548 0.055017378 0.064335018][0.17420697 0.18845789 0.19538608 0.18876603 0.16972001 0.14214788 0.11087363 0.083147824 0.059994213 0.042948715 0.032031506 0.027463494 0.030011471 0.037520748 0.044219814][0.15944566 0.17251255 0.17665321 0.1686911 0.15035672 0.12459835 0.094538532 0.06757129 0.04588899 0.031026589 0.022160044 0.018932713 0.022147352 0.02951109 0.034547549][0.14886262 0.15856755 0.15859982 0.14837389 0.12987643 0.1059304 0.078695983 0.055324581 0.037912782 0.027160237 0.022033041 0.021985536 0.02786709 0.037121493 0.042294871][0.14372104 0.15184604 0.1506169 0.14012463 0.12194373 0.099768892 0.075366952 0.055291731 0.041313235 0.034377705 0.033682507 0.038243745 0.048668634 0.061631877 0.068705186][0.14210045 0.1513126 0.1508932 0.14194128 0.12597306 0.10688908 0.085864507 0.068497352 0.057685375 0.054776486 0.058321558 0.066690892 0.080886647 0.096958421 0.1063357][0.13368617 0.1436924 0.14280021 0.13596933 0.12555525 0.11370713 0.10051358 0.090780154 0.088207968 0.092108406 0.098945469 0.10858899 0.12466037 0.14281118 0.15478106][0.12048361 0.12812349 0.12581691 0.12086079 0.11663631 0.11323401 0.11027582 0.11195454 0.12064738 0.1324161 0.14143825 0.15092903 0.16783018 0.18724015 0.20148362][0.10685498 0.11235483 0.10968995 0.10553575 0.10423075 0.10538254 0.10895801 0.11830783 0.13458537 0.15198204 0.16346362 0.17458984 0.19371837 0.21482868 0.23021798][0.08035291 0.085268937 0.08367119 0.080612935 0.080961049 0.084466696 0.091068849 0.1028877 0.12098577 0.13992181 0.15288109 0.16588527 0.18681577 0.20855094 0.22305672][0.033428039 0.036148418 0.035380956 0.03412864 0.037403494 0.043882035 0.052434288 0.064162754 0.080424778 0.097065821 0.10814791 0.11965232 0.1389028 0.15852638 0.17067915][-0.019109417 -0.021503437 -0.022452798 -0.022171715 -0.016448056 -0.0082422933 0.00070411497 0.011452452 0.025142672 0.038149346 0.045330908 0.052861203 0.06810715 0.084168673 0.093695477]]...]
INFO - root - 2017-12-11 04:56:35.013845: step 10110, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 46h:18m:49s remains)
INFO - root - 2017-12-11 04:56:40.439553: step 10120, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 51h:23m:52s remains)
INFO - root - 2017-12-11 04:56:45.574678: step 10130, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:35m:32s remains)
INFO - root - 2017-12-11 04:56:50.919196: step 10140, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 47h:55m:59s remains)
INFO - root - 2017-12-11 04:56:56.197515: step 10150, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.516 sec/batch; 46h:13m:02s remains)
INFO - root - 2017-12-11 04:57:01.528858: step 10160, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 48h:07m:28s remains)
INFO - root - 2017-12-11 04:57:06.924784: step 10170, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:30m:03s remains)
INFO - root - 2017-12-11 04:57:12.354771: step 10180, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 47h:59m:36s remains)
INFO - root - 2017-12-11 04:57:17.844177: step 10190, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:37m:42s remains)
INFO - root - 2017-12-11 04:57:23.239466: step 10200, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 46h:48m:06s remains)
2017-12-11 04:57:23.797996: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070821635 0.054813627 0.041327819 0.032320008 0.020452516 -0.0015358162 -0.02747315 -0.047013611 -0.055604015 -0.048626326 -0.028419152 -0.0052543292 0.01002472 0.013152577 0.0070286165][0.03793297 0.019448683 0.0058818422 -0.000890225 -0.0067889313 -0.013884002 -0.019208821 -0.0207334 -0.016181005 -0.0024835346 0.018488225 0.036922861 0.045202386 0.041464336 0.029657209][0.020661404 0.0032204792 -0.0068458933 -0.0081228679 -0.0042097378 0.0051224977 0.020009555 0.037501823 0.05487176 0.072086692 0.087547891 0.095048055 0.092228331 0.08011882 0.0626166][0.041194584 0.025002606 0.019086026 0.024114989 0.03799326 0.062393937 0.0939666 0.12519686 0.14847001 0.16194659 0.16572575 0.1578816 0.14196643 0.12195811 0.10166995][0.0875723 0.070627414 0.066872895 0.07743936 0.10117669 0.13984065 0.18630241 0.22868749 0.25420806 0.25989377 0.24877937 0.2243 0.19624656 0.17073743 0.15206875][0.13740061 0.119908 0.11814155 0.13423574 0.16779493 0.21968138 0.27870539 0.32884803 0.35306096 0.34845358 0.32199425 0.28310183 0.24754164 0.22136912 0.20962039][0.17045231 0.15440017 0.15689223 0.18061374 0.22490956 0.28825843 0.35538605 0.40691307 0.42389065 0.40563813 0.3645255 0.31602174 0.28097418 0.26252431 0.2649492][0.18343325 0.168146 0.17632696 0.21016337 0.26675943 0.33987355 0.40979034 0.45525882 0.45888227 0.42452061 0.37040794 0.31697085 0.28902134 0.28551906 0.30747896][0.1747736 0.15976189 0.17341207 0.21767205 0.28571144 0.36479712 0.43141431 0.46477309 0.45247346 0.40346688 0.34042242 0.28711969 0.27080381 0.28582957 0.32739529][0.14418457 0.12890086 0.14750449 0.20029533 0.27552548 0.3539156 0.41048244 0.4278093 0.40068743 0.34224248 0.2776264 0.23180495 0.23134129 0.26563075 0.32243246][0.086904071 0.07357958 0.097480215 0.15660973 0.2342609 0.3064093 0.34904537 0.35004985 0.31111154 0.24836811 0.18846357 0.15598246 0.1731032 0.22435392 0.29036489][0.015076775 0.0080939485 0.038400184 0.10087479 0.17568347 0.2381767 0.26791048 0.25782791 0.2134196 0.15174353 0.099919669 0.08031182 0.10780295 0.16424166 0.22775602][-0.041065425 -0.038618758 -0.0031675226 0.057262875 0.12401184 0.17593797 0.19732417 0.18410669 0.14224964 0.087364368 0.044993363 0.032734059 0.05714985 0.10099845 0.14666468][-0.057395779 -0.045993611 -0.0089939088 0.044066124 0.0988775 0.13994087 0.15690313 0.14684077 0.11398589 0.070371106 0.037759487 0.027582549 0.03897094 0.05833691 0.076457016][-0.029560452 -0.011328642 0.024951672 0.070147894 0.11444998 0.14716263 0.16279674 0.15881671 0.13710997 0.10497884 0.079009384 0.065340966 0.058017924 0.048602503 0.03687093]]...]
INFO - root - 2017-12-11 04:57:29.145949: step 10210, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:47m:34s remains)
INFO - root - 2017-12-11 04:57:34.461600: step 10220, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 48h:00m:17s remains)
INFO - root - 2017-12-11 04:57:39.809481: step 10230, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 47h:27m:33s remains)
INFO - root - 2017-12-11 04:57:44.846694: step 10240, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:21m:08s remains)
INFO - root - 2017-12-11 04:57:50.088167: step 10250, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:20m:54s remains)
INFO - root - 2017-12-11 04:57:55.437273: step 10260, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:33m:18s remains)
INFO - root - 2017-12-11 04:58:00.764138: step 10270, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 46h:46m:21s remains)
INFO - root - 2017-12-11 04:58:06.033580: step 10280, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 48h:41m:11s remains)
INFO - root - 2017-12-11 04:58:11.392373: step 10290, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:41m:09s remains)
INFO - root - 2017-12-11 04:58:16.654229: step 10300, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 45h:59m:22s remains)
2017-12-11 04:58:17.163952: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12923066 0.12603751 0.11704358 0.1078299 0.10042632 0.095511951 0.0938639 0.096306555 0.10127065 0.1062615 0.10801774 0.10398488 0.093274504 0.0793505 0.066585526][0.18296231 0.17270525 0.15903533 0.1496195 0.14462641 0.1427764 0.14268771 0.14570487 0.15189789 0.16010183 0.16674925 0.16835502 0.1630289 0.15334149 0.14355834][0.21789967 0.2002053 0.18199116 0.17247851 0.16913107 0.16899309 0.16951679 0.17315428 0.18169077 0.19510476 0.21003757 0.22249171 0.2292759 0.23043889 0.22857049][0.22902632 0.20735578 0.18871582 0.18311733 0.18440014 0.18778065 0.19037454 0.19595982 0.20787834 0.2268497 0.2503103 0.27486017 0.29648671 0.31190196 0.31969967][0.22909579 0.20998396 0.19862403 0.20511919 0.21909715 0.23331764 0.24408154 0.2550334 0.27121735 0.29449904 0.32341677 0.3561314 0.38897175 0.41598788 0.43031809][0.23218316 0.22537993 0.23119764 0.25848517 0.29336208 0.32622662 0.35109273 0.36947921 0.38869318 0.41300356 0.44182587 0.47494435 0.51053226 0.54151928 0.55572045][0.23849735 0.25419945 0.28635702 0.33890381 0.39652979 0.44887382 0.48676541 0.50859439 0.52493924 0.54433197 0.56628114 0.59090275 0.61947685 0.645496 0.65289825][0.2273937 0.27025843 0.33066505 0.40385726 0.47583595 0.53831315 0.58014494 0.59762126 0.60467923 0.61437917 0.62499082 0.63534117 0.65034807 0.66563636 0.66400892][0.17501667 0.23742837 0.31479689 0.39338702 0.46304241 0.51976651 0.55328566 0.56014788 0.55630052 0.55677223 0.55715942 0.5542013 0.55646765 0.56248814 0.55674738][0.081477545 0.14525749 0.21977632 0.28626126 0.33884779 0.37771419 0.39566255 0.39105785 0.37937993 0.374136 0.36803091 0.35608688 0.34994155 0.35141212 0.34853297][-0.015912412 0.031594831 0.086461484 0.13036628 0.16027379 0.17840473 0.18130156 0.16920237 0.15428677 0.1461059 0.13654578 0.12057497 0.11069957 0.11104004 0.11343478][-0.074322037 -0.051467698 -0.022680758 -0.0017821417 0.0090605281 0.01148158 0.0050362819 -0.0089144148 -0.0232944 -0.033301085 -0.045151636 -0.061823953 -0.072268896 -0.071809493 -0.065723404][-0.085372381 -0.08442045 -0.077990264 -0.07411176 -0.075179309 -0.081055775 -0.090704679 -0.10240856 -0.11405065 -0.12446961 -0.13674612 -0.15165529 -0.16088608 -0.16033931 -0.15345909][-0.071649849 -0.082339481 -0.088384233 -0.0933188 -0.099652 -0.10846863 -0.11828449 -0.1271532 -0.13592641 -0.14499494 -0.15526868 -0.16666402 -0.17412001 -0.17498457 -0.1709747][-0.06700173 -0.079006486 -0.0868731 -0.092980564 -0.099690609 -0.10856687 -0.11778121 -0.12498058 -0.13142747 -0.13777201 -0.14440037 -0.15157288 -0.1570445 -0.15969712 -0.15982403]]...]
INFO - root - 2017-12-11 04:58:22.481596: step 10310, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:08m:34s remains)
INFO - root - 2017-12-11 04:58:27.862659: step 10320, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 50h:18m:53s remains)
INFO - root - 2017-12-11 04:58:33.184425: step 10330, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 47h:16m:59s remains)
INFO - root - 2017-12-11 04:58:38.314662: step 10340, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:42m:31s remains)
INFO - root - 2017-12-11 04:58:43.716134: step 10350, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:57m:48s remains)
INFO - root - 2017-12-11 04:58:49.003813: step 10360, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:18m:01s remains)
INFO - root - 2017-12-11 04:58:54.311114: step 10370, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:25m:00s remains)
INFO - root - 2017-12-11 04:58:59.689315: step 10380, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:06m:42s remains)
INFO - root - 2017-12-11 04:59:05.034066: step 10390, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.518 sec/batch; 46h:23m:16s remains)
INFO - root - 2017-12-11 04:59:10.280065: step 10400, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:02m:08s remains)
2017-12-11 04:59:10.819613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043461155 -0.034759611 -0.0159867 0.013569793 0.047452837 0.073351063 0.079987705 0.065245077 0.037286241 0.0063022692 -0.01980005 -0.035606842 -0.041176617 -0.04046879 -0.037859838][-0.04125924 -0.028750224 -0.0039989706 0.032160636 0.071032077 0.098780923 0.10309979 0.082494892 0.046892654 0.0086804582 -0.022475345 -0.040685091 -0.046332572 -0.04459057 -0.0410941][-0.030778289 -0.011747459 0.022276964 0.069919907 0.11978503 0.15452506 0.15896741 0.13164255 0.084473982 0.032715686 -0.010202777 -0.036293246 -0.045726515 -0.045254316 -0.04208333][-0.0099988747 0.018202798 0.066393815 0.13289575 0.20243765 0.25106373 0.25798523 0.22129731 0.15643644 0.083239429 0.020378098 -0.020674963 -0.039207559 -0.043618992 -0.042867087][0.02855272 0.068636633 0.13629459 0.22911252 0.32630098 0.39449847 0.40544552 0.35693121 0.26868007 0.16671656 0.075973243 0.012271611 -0.02208909 -0.036349937 -0.040971689][0.076487437 0.12826063 0.21634722 0.33774033 0.4658086 0.55734134 0.5760107 0.51909679 0.40881979 0.27687004 0.15452702 0.062681422 0.0063001178 -0.023021577 -0.036146119][0.12092357 0.18037762 0.28343216 0.425499 0.57601351 0.68599808 0.71409535 0.65658247 0.53430128 0.38134146 0.2333013 0.11543994 0.036395885 -0.009447556 -0.031881426][0.14515129 0.20413111 0.31017262 0.45576125 0.61011273 0.72551405 0.76098144 0.71156818 0.59264612 0.43664593 0.27951515 0.14807688 0.054312784 -0.0030005341 -0.031560626][0.13801306 0.18877634 0.28444263 0.41400835 0.55061823 0.65574819 0.69416589 0.65932167 0.55778366 0.41745371 0.27115095 0.14363872 0.048852917 -0.009763672 -0.037606157][0.093829907 0.13172513 0.20798436 0.3094779 0.41611433 0.50235218 0.54079849 0.52302396 0.44631436 0.33277857 0.21081017 0.10114234 0.018111268 -0.031493373 -0.051202152][0.023657182 0.045039307 0.0954955 0.16275506 0.23446517 0.29737118 0.33218953 0.32893538 0.2787272 0.19763185 0.10846241 0.02735549 -0.032505352 -0.063694239 -0.069107518][-0.04383716 -0.03996437 -0.016824441 0.016407533 0.054072719 0.091945671 0.11842467 0.12328324 0.095995486 0.047309794 -0.0065491432 -0.054177765 -0.085220754 -0.093899466 -0.083606489][-0.094932795 -0.10408402 -0.10248305 -0.095191225 -0.083770685 -0.067347586 -0.051420186 -0.044202141 -0.054333057 -0.07567174 -0.098512016 -0.11584369 -0.12079271 -0.11040955 -0.088700049][-0.11976514 -0.13394767 -0.1425098 -0.14792582 -0.15041979 -0.1477295 -0.14181249 -0.13739665 -0.13922656 -0.14430839 -0.14747374 -0.14500806 -0.1332653 -0.112131 -0.085935488][-0.11440015 -0.12742373 -0.13670048 -0.14399821 -0.14968213 -0.15262425 -0.1534427 -0.15385911 -0.15509892 -0.15531141 -0.15150261 -0.14162351 -0.12460174 -0.10221573 -0.078460358]]...]
INFO - root - 2017-12-11 04:59:16.113852: step 10410, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 47h:25m:56s remains)
INFO - root - 2017-12-11 04:59:21.449962: step 10420, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:24m:43s remains)
INFO - root - 2017-12-11 04:59:26.791805: step 10430, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:32m:51s remains)
INFO - root - 2017-12-11 04:59:31.810937: step 10440, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:58m:18s remains)
INFO - root - 2017-12-11 04:59:37.134414: step 10450, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 46h:08m:33s remains)
INFO - root - 2017-12-11 04:59:42.453989: step 10460, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:40m:26s remains)
INFO - root - 2017-12-11 04:59:47.847243: step 10470, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.523 sec/batch; 46h:49m:04s remains)
INFO - root - 2017-12-11 04:59:53.182008: step 10480, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.515 sec/batch; 46h:01m:23s remains)
INFO - root - 2017-12-11 04:59:58.532229: step 10490, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:39m:07s remains)
INFO - root - 2017-12-11 05:00:03.922538: step 10500, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 48h:51m:54s remains)
2017-12-11 05:00:04.472108: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27950391 0.32789412 0.38040343 0.4044106 0.40222633 0.38862902 0.3694675 0.33723867 0.27968118 0.21015964 0.13543825 0.061422657 -0.0021187211 -0.049698863 -0.077077277][0.29903334 0.35362002 0.41444623 0.44345841 0.44298685 0.42883772 0.40001839 0.3506794 0.27634174 0.19574928 0.11587764 0.040698472 -0.019234849 -0.061167322 -0.084263958][0.28768936 0.34651038 0.41294339 0.44655874 0.45107314 0.43993464 0.40811443 0.34950045 0.2656059 0.17918594 0.097116157 0.023201883 -0.032882404 -0.069152042 -0.087735727][0.2574079 0.31951359 0.38829446 0.42513496 0.4386138 0.43870124 0.41424471 0.35646009 0.27167839 0.18432665 0.099562474 0.022438584 -0.035624292 -0.070712619 -0.087374233][0.20793803 0.27269185 0.34224293 0.38394484 0.4125002 0.43183893 0.42337555 0.37510335 0.29616585 0.21032929 0.12072591 0.035743434 -0.029416414 -0.067576639 -0.084865071][0.14015228 0.20334791 0.26989183 0.31649289 0.36339584 0.407074 0.42146897 0.39162669 0.32660964 0.24715251 0.15406929 0.060289692 -0.014096756 -0.058570176 -0.079356939][0.071526065 0.12535335 0.18166691 0.22894056 0.2908982 0.35765445 0.39798531 0.39356959 0.34950787 0.28149348 0.18889649 0.088946171 0.0062326319 -0.045678407 -0.0716646][0.0242409 0.062677518 0.10504512 0.15005 0.22122264 0.30465168 0.367119 0.38669398 0.36227271 0.30494538 0.21465302 0.11190023 0.023857327 -0.033980288 -0.064583376][0.011856087 0.032000151 0.060252264 0.10196928 0.17603846 0.2668052 0.34135798 0.3753362 0.36170742 0.31037784 0.22367536 0.1224081 0.033609614 -0.026686441 -0.059570663][0.032497391 0.035073038 0.0516684 0.089458533 0.15984392 0.24762908 0.32285687 0.36054206 0.34882361 0.29948223 0.21795644 0.12248377 0.037057478 -0.022857504 -0.056051906][0.07436727 0.063794196 0.0737056 0.10789222 0.17089552 0.24977182 0.31798774 0.35055074 0.3332082 0.28182936 0.20500858 0.11669416 0.036094274 -0.021968858 -0.054286692][0.11333228 0.099554963 0.11166722 0.14691938 0.20488441 0.27468649 0.33211032 0.35254347 0.32301283 0.26536432 0.19041252 0.10727202 0.030548608 -0.024897039 -0.0550848][0.13342316 0.12535077 0.14606819 0.1847304 0.23882036 0.29932693 0.34430304 0.35075375 0.30951893 0.24709029 0.1741505 0.095594868 0.022696694 -0.029327558 -0.056924392][0.13129823 0.13250515 0.16012043 0.19812976 0.24523644 0.29476967 0.32827353 0.32541037 0.28049007 0.22052179 0.15341111 0.080931976 0.012674607 -0.035384174 -0.060221698][0.11073352 0.11836533 0.14631623 0.17721033 0.21355551 0.25203535 0.27871412 0.27546236 0.23722816 0.18735233 0.12988764 0.06514357 0.0022903825 -0.041868098 -0.064300179]]...]
INFO - root - 2017-12-11 05:00:09.792745: step 10510, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:55m:44s remains)
INFO - root - 2017-12-11 05:00:15.125159: step 10520, loss = 0.66, batch loss = 0.61 (14.6 examples/sec; 0.546 sec/batch; 48h:51m:15s remains)
INFO - root - 2017-12-11 05:00:20.517545: step 10530, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:38m:31s remains)
INFO - root - 2017-12-11 05:00:25.811849: step 10540, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:02m:34s remains)
INFO - root - 2017-12-11 05:00:30.826355: step 10550, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:55m:20s remains)
INFO - root - 2017-12-11 05:00:36.172200: step 10560, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:42m:47s remains)
INFO - root - 2017-12-11 05:00:41.635725: step 10570, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:33m:03s remains)
INFO - root - 2017-12-11 05:00:47.012112: step 10580, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 49h:02m:43s remains)
INFO - root - 2017-12-11 05:00:52.420976: step 10590, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:49m:59s remains)
INFO - root - 2017-12-11 05:00:57.794541: step 10600, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:12m:38s remains)
2017-12-11 05:00:58.327786: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16010861 0.300286 0.43200767 0.527422 0.58639342 0.62430155 0.65299451 0.66360062 0.62949806 0.53983134 0.41418839 0.3016153 0.2435427 0.24640554 0.28210738][0.10327142 0.23516658 0.36722395 0.47384781 0.55340874 0.613697 0.6574074 0.66958195 0.62415481 0.51832438 0.37775984 0.24787922 0.16865058 0.14987613 0.17468683][0.018427361 0.12058214 0.2313565 0.332097 0.42145178 0.49912855 0.55620033 0.572549 0.52598161 0.42199257 0.2896896 0.16853166 0.092704929 0.075385749 0.11139742][-0.049860638 0.023192445 0.10911517 0.19576368 0.28101051 0.35964769 0.41821131 0.43742147 0.40139571 0.31849098 0.21431407 0.11925387 0.0615726 0.059229672 0.11688127][-0.06709075 -0.00684063 0.066695355 0.14484756 0.22337252 0.29512778 0.34741658 0.36603397 0.34119678 0.2793985 0.19962196 0.12373602 0.075599141 0.078447454 0.14449461][-0.023084261 0.046019778 0.12995572 0.22129589 0.31101808 0.38740268 0.43633485 0.44889107 0.42160469 0.35920128 0.27659786 0.19130595 0.12577955 0.10760708 0.15492769][0.054900613 0.14405338 0.25117269 0.36998305 0.48696053 0.58456945 0.64424378 0.65715176 0.62148768 0.53929609 0.42592782 0.30083987 0.19077061 0.12900376 0.13967644][0.11884727 0.22590785 0.35232398 0.49192047 0.62992704 0.7477572 0.8247121 0.84800589 0.81068712 0.70892268 0.55905813 0.38713765 0.22950862 0.1279567 0.11229183][0.1329097 0.2476124 0.38206783 0.52611452 0.66341847 0.77756667 0.85184658 0.8751111 0.8377524 0.73074549 0.56756961 0.37782219 0.20467518 0.0968411 0.0871012][0.0973185 0.20586424 0.33435944 0.46634942 0.58256435 0.66773808 0.71321839 0.71736789 0.6755352 0.57853216 0.43447658 0.27060637 0.12781709 0.053735673 0.07820484][0.033916291 0.11869827 0.2206925 0.32258675 0.4069584 0.45977789 0.47711709 0.46446839 0.42278609 0.34593382 0.23886238 0.12532683 0.037628431 0.013993241 0.077016234][-0.026468705 0.025499109 0.088637941 0.15132034 0.20348474 0.23390624 0.23862503 0.22276618 0.18829554 0.13290697 0.062438853 -0.0015652734 -0.036704645 -0.017966852 0.069187962][-0.057197627 -0.030511018 0.00030941391 0.029316477 0.053847421 0.067178294 0.065709762 0.051975593 0.026895495 -0.0094424672 -0.050005905 -0.076351508 -0.074848078 -0.033124112 0.055874109][-0.046745546 -0.027758302 -0.00991042 0.00294887 0.011108064 0.011347516 0.002526107 -0.012612102 -0.032660548 -0.056176461 -0.076850086 -0.081542291 -0.063752852 -0.0205186 0.049582727][-0.00037417986 0.030114474 0.057690877 0.077404357 0.08804743 0.086641192 0.073238589 0.052172024 0.027013017 0.0014281226 -0.017838357 -0.021986648 -0.0091200341 0.018446207 0.058394205]]...]
INFO - root - 2017-12-11 05:01:03.759708: step 10610, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 49h:30m:57s remains)
INFO - root - 2017-12-11 05:01:09.085536: step 10620, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 46h:51m:02s remains)
INFO - root - 2017-12-11 05:01:14.330369: step 10630, loss = 0.70, batch loss = 0.65 (15.7 examples/sec; 0.509 sec/batch; 45h:32m:59s remains)
INFO - root - 2017-12-11 05:01:19.678178: step 10640, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 49h:11m:18s remains)
INFO - root - 2017-12-11 05:01:24.772741: step 10650, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:41m:23s remains)
INFO - root - 2017-12-11 05:01:30.092779: step 10660, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 47h:11m:22s remains)
INFO - root - 2017-12-11 05:01:35.390051: step 10670, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 47h:45m:02s remains)
INFO - root - 2017-12-11 05:01:40.833434: step 10680, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 49h:08m:31s remains)
INFO - root - 2017-12-11 05:01:46.257889: step 10690, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:53m:51s remains)
INFO - root - 2017-12-11 05:01:51.697028: step 10700, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:53m:19s remains)
2017-12-11 05:01:52.315242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054831639 -0.010382401 0.06628263 0.15644565 0.24416463 0.31387019 0.35757971 0.36982289 0.34270546 0.28466743 0.20984861 0.132796 0.0829376 0.084668249 0.13607042][-0.060571123 -0.0069170687 0.09071324 0.21474706 0.34680593 0.46514237 0.54920769 0.58024514 0.54101956 0.44651443 0.33008286 0.22617997 0.17495798 0.19720031 0.28153017][-0.063829407 -0.0017890168 0.11564461 0.27464461 0.45410281 0.6234616 0.74614275 0.78677815 0.72456235 0.58579308 0.42877218 0.3109622 0.27690151 0.33719468 0.46533304][-0.063050725 0.0086194463 0.14631329 0.33858189 0.5601896 0.76926756 0.9137072 0.9476949 0.85400039 0.67481863 0.49168935 0.37952736 0.37894845 0.4833366 0.65133834][-0.059478335 0.021758027 0.1776491 0.39767152 0.65068686 0.88278627 1.031361 1.0492985 0.926818 0.72195011 0.53106946 0.43711367 0.47276509 0.61534172 0.81182462][-0.061544925 0.022805817 0.1872253 0.42209214 0.69132823 0.93265277 1.0805815 1.0929407 0.96428931 0.75921887 0.57483757 0.49321327 0.54416656 0.69947088 0.90343964][-0.072095737 0.0076798405 0.16964757 0.40570724 0.67733353 0.91985565 1.071221 1.0952941 0.98599356 0.80299979 0.63252056 0.54886681 0.58466178 0.71873987 0.90258318][-0.084791631 -0.013069611 0.14119436 0.37021783 0.63461834 0.87197912 1.0260588 1.0663071 0.98440039 0.82998854 0.67418206 0.58123827 0.5857597 0.67814761 0.82127506][-0.09671241 -0.033333346 0.10997176 0.32511789 0.57381833 0.7987749 0.949524 0.99849653 0.93627143 0.80326861 0.65841281 0.55775803 0.53433895 0.58615112 0.68463224][-0.11015351 -0.058120091 0.067681372 0.25833577 0.47953543 0.68055153 0.81754792 0.86384368 0.81047565 0.69134843 0.55573344 0.45242703 0.4089433 0.42768919 0.48679709][-0.1282838 -0.091729172 0.0081981663 0.162024 0.34090647 0.50179911 0.60993689 0.64114606 0.58926553 0.4848516 0.36681408 0.27352875 0.22146425 0.2153239 0.24106036][-0.1510042 -0.13394159 -0.067536458 0.039039537 0.1626095 0.26852307 0.33344582 0.3390989 0.28465852 0.19962214 0.11251563 0.048011087 0.0079215094 -0.0054052584 6.5685279e-05][-0.1799219 -0.18868382 -0.16382146 -0.11249021 -0.0504646 -0.0029650403 0.018575504 0.0021162948 -0.0495422 -0.1099265 -0.16015241 -0.18852091 -0.20513795 -0.21170993 -0.21196003][-0.20872667 -0.24548987 -0.26183477 -0.26185182 -0.25297871 -0.25089005 -0.25841725 -0.28326491 -0.32315761 -0.35887191 -0.37964156 -0.38237095 -0.38135487 -0.37902489 -0.37548351][-0.22540574 -0.2816965 -0.32649162 -0.36080095 -0.3850565 -0.40708178 -0.42618325 -0.44767386 -0.47126174 -0.48662356 -0.48937145 -0.48018715 -0.47077644 -0.4619211 -0.45140707]]...]
INFO - root - 2017-12-11 05:01:57.628677: step 10710, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 46h:44m:17s remains)
INFO - root - 2017-12-11 05:02:03.023952: step 10720, loss = 0.67, batch loss = 0.61 (14.4 examples/sec; 0.556 sec/batch; 49h:39m:53s remains)
INFO - root - 2017-12-11 05:02:08.445436: step 10730, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 50h:54m:49s remains)
INFO - root - 2017-12-11 05:02:13.804940: step 10740, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 47h:09m:17s remains)
INFO - root - 2017-12-11 05:02:18.810999: step 10750, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 47h:49m:04s remains)
INFO - root - 2017-12-11 05:02:24.118248: step 10760, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 46h:34m:22s remains)
INFO - root - 2017-12-11 05:02:29.482267: step 10770, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.538 sec/batch; 48h:02m:28s remains)
INFO - root - 2017-12-11 05:02:34.932931: step 10780, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.525 sec/batch; 46h:52m:50s remains)
INFO - root - 2017-12-11 05:02:40.287557: step 10790, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:29m:14s remains)
INFO - root - 2017-12-11 05:02:45.675420: step 10800, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:11m:45s remains)
2017-12-11 05:02:46.179853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030118464 -0.01061428 0.031623971 0.081930228 0.12990642 0.16649128 0.18382578 0.17973831 0.16014981 0.13632202 0.12102339 0.11622643 0.11718491 0.1169147 0.10849957][-0.029410528 -0.010174603 0.030790582 0.07902354 0.12614469 0.165726 0.19099693 0.198593 0.19221896 0.17921683 0.16822252 0.15872689 0.14657511 0.12951466 0.10455886][-0.024496404 -0.0078504486 0.027848519 0.0694325 0.11110784 0.15004161 0.18095215 0.19986229 0.20815565 0.20891814 0.20591362 0.19554685 0.17458342 0.14428623 0.10543042][-0.01156804 0.0023715745 0.032055587 0.065761581 0.098992452 0.13200733 0.16158329 0.18525176 0.20377403 0.2168745 0.22403467 0.21851888 0.19720066 0.16247115 0.11697803][0.014086079 0.025660131 0.048728075 0.073120348 0.094371751 0.1151769 0.13520192 0.15560937 0.17845929 0.20117851 0.21928431 0.2223299 0.20666422 0.17453693 0.1292143][0.048022479 0.058811828 0.076590583 0.092631489 0.10223881 0.10965342 0.11668949 0.12844275 0.14939134 0.17610843 0.20106834 0.21051359 0.20104663 0.17458178 0.13398285][0.07605499 0.088816851 0.10511724 0.11697087 0.11952861 0.11744334 0.11288703 0.11420168 0.12855066 0.15255326 0.1770267 0.18689056 0.18083945 0.16071929 0.12756003][0.082321428 0.0986111 0.11701839 0.12942542 0.13115838 0.12662065 0.11691787 0.11220165 0.12102318 0.13981263 0.15839887 0.16278099 0.15539527 0.1393519 0.11348702][0.062390994 0.08109042 0.10294642 0.11929194 0.12612879 0.12700468 0.12130743 0.1192703 0.12875964 0.14535294 0.15786509 0.15517484 0.14330469 0.12756185 0.10556689][0.026558092 0.046265911 0.071608536 0.093581423 0.10885242 0.11947924 0.12319647 0.12916693 0.14346382 0.16070664 0.16960533 0.1630908 0.14892842 0.13331489 0.11284814][-0.0095874984 0.0099399574 0.037658397 0.064990431 0.088617794 0.10854878 0.12128097 0.13480292 0.15330324 0.17075166 0.17762326 0.17188329 0.16146675 0.15066954 0.13363966][-0.03107319 -0.012374037 0.016046345 0.046990894 0.076557182 0.10238469 0.12015946 0.13688676 0.15570493 0.17074348 0.17588034 0.17386076 0.17176384 0.16973403 0.15822096][-0.035047762 -0.017747194 0.0096658 0.04182101 0.074309088 0.10275715 0.1221634 0.13833936 0.15409614 0.16500929 0.16897443 0.1714875 0.1778708 0.18370366 0.17610125][-0.024740145 -0.009292122 0.01586532 0.047059748 0.079563551 0.10761954 0.12571725 0.13807724 0.14783207 0.15309718 0.15588553 0.16236426 0.17549488 0.1867339 0.18067503][-0.010899319 0.002197878 0.024191368 0.0529024 0.083648212 0.11012785 0.12622786 0.13387336 0.13662314 0.13516627 0.13499828 0.14202367 0.15746231 0.17025888 0.16375452]]...]
INFO - root - 2017-12-11 05:02:51.514596: step 10810, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:39m:17s remains)
INFO - root - 2017-12-11 05:02:56.864328: step 10820, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:13m:30s remains)
INFO - root - 2017-12-11 05:03:02.202064: step 10830, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:53m:30s remains)
INFO - root - 2017-12-11 05:03:07.581475: step 10840, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 46h:55m:33s remains)
INFO - root - 2017-12-11 05:03:12.686134: step 10850, loss = 0.71, batch loss = 0.66 (18.8 examples/sec; 0.426 sec/batch; 38h:04m:16s remains)
INFO - root - 2017-12-11 05:03:18.019775: step 10860, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 48h:46m:34s remains)
INFO - root - 2017-12-11 05:03:23.340618: step 10870, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 49h:01m:23s remains)
INFO - root - 2017-12-11 05:03:28.696057: step 10880, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:27m:13s remains)
INFO - root - 2017-12-11 05:03:33.921587: step 10890, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 47h:00m:02s remains)
INFO - root - 2017-12-11 05:03:39.264243: step 10900, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 48h:38m:14s remains)
2017-12-11 05:03:39.805110: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053201724 0.074103296 0.10351709 0.13635427 0.16334699 0.18275538 0.1981108 0.21254756 0.22577128 0.23805824 0.25501361 0.28049642 0.29144979 0.28879637 0.27250507][0.13371176 0.16016211 0.19788225 0.24148233 0.27708596 0.30116767 0.31642976 0.32435441 0.32587671 0.32541639 0.33114886 0.34681815 0.3472068 0.33233562 0.30248398][0.24766716 0.277229 0.32065961 0.37291354 0.41513836 0.44107237 0.45226166 0.44779006 0.42855063 0.40429166 0.3866376 0.37981474 0.36158291 0.33128372 0.2896542][0.36425439 0.39579934 0.44242275 0.49998736 0.5454734 0.56956619 0.57298696 0.55340308 0.51076514 0.45929077 0.41518569 0.38420808 0.3497749 0.30981082 0.26429603][0.48682606 0.52064651 0.5666818 0.62477255 0.66975152 0.6880495 0.67943585 0.64238644 0.57568389 0.49730763 0.42903861 0.37981325 0.33942541 0.30162111 0.26242214][0.63091654 0.66964215 0.71151787 0.76544112 0.8070786 0.816252 0.790387 0.73114556 0.63796484 0.53295952 0.44358519 0.38359308 0.34736243 0.32124335 0.29333478][0.78500038 0.82313174 0.85236734 0.89287096 0.92543942 0.92211676 0.87533754 0.79179054 0.6751349 0.55071634 0.44817746 0.38541287 0.36011979 0.34988266 0.33319667][0.86990219 0.89666224 0.90298504 0.920054 0.936439 0.91846186 0.85353023 0.75385576 0.62976772 0.5061599 0.40864193 0.35569546 0.34774271 0.35571852 0.35122016][0.80967408 0.81848234 0.80034339 0.792751 0.79078484 0.76064187 0.68775386 0.58792287 0.476905 0.3758809 0.30249816 0.27225393 0.28763428 0.317086 0.329029][0.60845304 0.59944338 0.5659278 0.54210621 0.52736562 0.49378657 0.42752558 0.34301624 0.25738087 0.18786588 0.14510569 0.14053734 0.17803532 0.22767122 0.2577962][0.33024409 0.30808365 0.26944637 0.23760447 0.21618955 0.18748413 0.13929197 0.07990925 0.024377462 -0.013322877 -0.027295046 -0.010661003 0.040576715 0.10220654 0.14640431][0.057552867 0.027604772 -0.0099308938 -0.043153405 -0.065240815 -0.083760545 -0.10895372 -0.13952228 -0.16475566 -0.17380545 -0.16360274 -0.13206477 -0.0756539 -0.010931497 0.040863711][-0.13526893 -0.16531017 -0.19556491 -0.22160453 -0.23780492 -0.24530458 -0.25088874 -0.257302 -0.25938863 -0.2497645 -0.22604038 -0.18916667 -0.13661474 -0.0780068 -0.02776299][-0.2160438 -0.24149272 -0.26090461 -0.27512997 -0.28238612 -0.28163508 -0.27653733 -0.27066165 -0.26282233 -0.24828303 -0.22421181 -0.1927989 -0.15239964 -0.10737146 -0.066658005][-0.20357072 -0.22245185 -0.2321818 -0.23680438 -0.23711729 -0.23253891 -0.22518696 -0.21767716 -0.20982954 -0.19877568 -0.18201499 -0.1623674 -0.13827449 -0.1104207 -0.084096946]]...]
INFO - root - 2017-12-11 05:03:45.214504: step 10910, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:45m:36s remains)
INFO - root - 2017-12-11 05:03:50.575462: step 10920, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 48h:47m:54s remains)
INFO - root - 2017-12-11 05:03:55.886273: step 10930, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 46h:59m:46s remains)
INFO - root - 2017-12-11 05:04:01.211540: step 10940, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 47h:46m:31s remains)
INFO - root - 2017-12-11 05:04:06.631445: step 10950, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:21m:14s remains)
INFO - root - 2017-12-11 05:04:11.767846: step 10960, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 47h:08m:09s remains)
INFO - root - 2017-12-11 05:04:17.182032: step 10970, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 51h:32m:05s remains)
INFO - root - 2017-12-11 05:04:22.540395: step 10980, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:33m:33s remains)
INFO - root - 2017-12-11 05:04:27.963797: step 10990, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 47h:31m:08s remains)
INFO - root - 2017-12-11 05:04:33.324071: step 11000, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:16m:43s remains)
2017-12-11 05:04:33.883686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0040281289 0.0071198558 0.013689419 0.014673503 0.010483583 0.0015876986 -0.0090133054 -0.0168238 -0.01938119 -0.016231613 -0.009693563 -0.0022996715 0.00093684485 -0.0033215887 -0.015449896][0.053375039 0.077926926 0.091273077 0.093431205 0.086204082 0.0710333 0.053746507 0.04176015 0.038225334 0.042531811 0.050546665 0.059364483 0.061331529 0.051217321 0.0285117][0.13809596 0.18344441 0.20872889 0.21555339 0.20808381 0.18961413 0.16926792 0.15507393 0.14978889 0.15116684 0.15457004 0.15914017 0.1549672 0.1341363 0.09657114][0.24031022 0.31450936 0.35797024 0.37374678 0.37061051 0.355977 0.34067178 0.32859868 0.3194136 0.30918446 0.29563338 0.28433639 0.2653487 0.22811155 0.17301954][0.33828717 0.44543484 0.51255238 0.54392993 0.55520046 0.558528 0.56072295 0.55594659 0.5382511 0.50476223 0.45829991 0.414777 0.36783695 0.30734777 0.23414727][0.40292612 0.53811544 0.62847108 0.6802597 0.71817213 0.75745159 0.7927621 0.80283374 0.77499145 0.71163279 0.62194353 0.532672 0.44510671 0.35488638 0.26423177][0.41654038 0.56800616 0.67564529 0.74724936 0.81640148 0.89834183 0.97118878 0.99882323 0.96435946 0.87658668 0.74906874 0.61543435 0.48699012 0.36865151 0.26517269][0.37720081 0.52930045 0.64252079 0.72475994 0.81497818 0.92594582 1.0220723 1.0608867 1.0258758 0.92997074 0.78564262 0.62631792 0.47288677 0.33996493 0.23523712][0.29487818 0.43214184 0.53713119 0.61643815 0.70937431 0.82433522 0.919971 0.95826191 0.92823845 0.84378433 0.71048248 0.55480069 0.40312466 0.2760359 0.18351625][0.18567079 0.29571643 0.38040432 0.44394946 0.52194554 0.61741227 0.69248712 0.7208733 0.69966394 0.64062959 0.54033887 0.41584498 0.29337743 0.19313739 0.12482144][0.076337434 0.15395829 0.21115306 0.24871339 0.29622644 0.35285237 0.3916114 0.40087265 0.38648432 0.35665315 0.30092737 0.22735853 0.15578346 0.098909006 0.063414596][-0.011118082 0.038418259 0.070159763 0.081680305 0.09501493 0.10728817 0.10474018 0.089662768 0.074471861 0.064321958 0.047215715 0.025322821 0.0085116429 -0.001557541 -0.0029408534][-0.06837035 -0.039203335 -0.025006136 -0.030070925 -0.04008593 -0.058502208 -0.089184329 -0.12167831 -0.14150873 -0.14472257 -0.14045027 -0.12889136 -0.10890614 -0.086719863 -0.0641989][-0.09641441 -0.080830634 -0.076057926 -0.086592108 -0.10433145 -0.13246267 -0.17036134 -0.2065253 -0.22810674 -0.23144044 -0.22194034 -0.20023493 -0.16901428 -0.13602483 -0.10470416][-0.099951878 -0.092561953 -0.09172979 -0.1014004 -0.11773858 -0.14290711 -0.17526907 -0.20604149 -0.22565071 -0.23086075 -0.22443841 -0.20692536 -0.18101746 -0.15214233 -0.12328129]]...]
INFO - root - 2017-12-11 05:04:39.220552: step 11010, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:59m:51s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:04:44.650086: step 11020, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:39m:31s remains)
INFO - root - 2017-12-11 05:04:50.047729: step 11030, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:32m:03s remains)
INFO - root - 2017-12-11 05:04:55.397796: step 11040, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.532 sec/batch; 47h:27m:45s remains)
INFO - root - 2017-12-11 05:05:00.722753: step 11050, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 46h:56m:46s remains)
INFO - root - 2017-12-11 05:05:05.728423: step 11060, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:22m:29s remains)
INFO - root - 2017-12-11 05:05:11.049282: step 11070, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:54m:02s remains)
INFO - root - 2017-12-11 05:05:16.395766: step 11080, loss = 0.69, batch loss = 0.63 (15.8 examples/sec; 0.508 sec/batch; 45h:20m:34s remains)
INFO - root - 2017-12-11 05:05:21.708472: step 11090, loss = 0.71, batch loss = 0.66 (15.5 examples/sec; 0.517 sec/batch; 46h:11m:06s remains)
INFO - root - 2017-12-11 05:05:27.061547: step 11100, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:36m:39s remains)
2017-12-11 05:05:27.581561: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26093891 0.264599 0.26091674 0.25715864 0.25752857 0.26585823 0.2825124 0.30163017 0.31683359 0.322296 0.31424806 0.29421175 0.26679209 0.23691969 0.2075301][0.31474954 0.31427065 0.30868635 0.30893084 0.31924757 0.34045959 0.36822915 0.39127517 0.40034461 0.390865 0.36395973 0.32658157 0.28575793 0.2472202 0.21410915][0.31831953 0.32043153 0.32359716 0.34114668 0.37571767 0.42174411 0.46697676 0.49311259 0.48825654 0.45145822 0.393563 0.33088544 0.27358124 0.22769871 0.19578779][0.29009727 0.30107331 0.32236722 0.36867112 0.44054732 0.52378875 0.59549171 0.62833983 0.60647553 0.53314227 0.43293405 0.33652517 0.25947544 0.20804451 0.18285926][0.23541909 0.25935775 0.30529284 0.38906798 0.50914127 0.64120132 0.74962467 0.79679072 0.76113647 0.6490134 0.49997047 0.3625384 0.26142058 0.20501047 0.19175544][0.1684926 0.20901111 0.28321761 0.40767589 0.5777598 0.75903469 0.90431982 0.96831006 0.92547345 0.78455329 0.59613109 0.42403695 0.30341041 0.24695677 0.25116041][0.12408032 0.17691754 0.27317202 0.42798251 0.63254476 0.84449321 1.009892 1.082063 1.0368677 0.88629603 0.68462765 0.50255769 0.38166672 0.33793837 0.36538142][0.11293954 0.16673961 0.26898873 0.43358955 0.64705294 0.86156094 1.0215288 1.0852295 1.0366433 0.89348465 0.70757061 0.54605007 0.45010129 0.43508917 0.49098635][0.11783234 0.16572198 0.2632575 0.42141965 0.62060148 0.81014276 0.938047 0.97347647 0.91561192 0.79059535 0.64374936 0.52883953 0.47867733 0.503034 0.586341][0.12965339 0.17328677 0.2636131 0.4050658 0.57077432 0.71102387 0.7842831 0.77577734 0.70345575 0.60203415 0.50753397 0.45409989 0.4599 0.52470827 0.6267876][0.14901187 0.1909415 0.27315915 0.39122668 0.51221335 0.59106755 0.60108691 0.54466856 0.45673484 0.37897629 0.33729669 0.34528828 0.40398079 0.50027275 0.60599726][0.16443174 0.20241924 0.27350536 0.36711293 0.447282 0.47494492 0.43584019 0.34414908 0.24509053 0.18468976 0.18241113 0.23628739 0.33066639 0.438596 0.5274412][0.16313779 0.19544585 0.25421268 0.32742727 0.38003778 0.37870747 0.31627229 0.21071528 0.11008199 0.06147784 0.082270578 0.16202646 0.27113247 0.37050954 0.4260973][0.15277235 0.17861962 0.22358185 0.27779707 0.3116138 0.29834026 0.23372123 0.13435759 0.045430444 0.011771553 0.049104221 0.14133759 0.24896163 0.3258104 0.34115788][0.14345038 0.15893964 0.18615346 0.22049299 0.24039479 0.22604343 0.17219111 0.092073113 0.025254989 0.011734227 0.064415239 0.16161168 0.25764486 0.30485415 0.2803995]]...]
INFO - root - 2017-12-11 05:05:32.961921: step 11110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:10m:11s remains)
INFO - root - 2017-12-11 05:05:38.321694: step 11120, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 46h:26m:40s remains)
INFO - root - 2017-12-11 05:05:43.655426: step 11130, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 46h:47m:15s remains)
INFO - root - 2017-12-11 05:05:48.964144: step 11140, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:25m:08s remains)
INFO - root - 2017-12-11 05:05:54.280125: step 11150, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:17m:12s remains)
INFO - root - 2017-12-11 05:05:59.306851: step 11160, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 46h:19m:56s remains)
INFO - root - 2017-12-11 05:06:04.658239: step 11170, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:54m:56s remains)
INFO - root - 2017-12-11 05:06:10.058733: step 11180, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:03m:27s remains)
INFO - root - 2017-12-11 05:06:15.411351: step 11190, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 49h:59m:39s remains)
INFO - root - 2017-12-11 05:06:20.738498: step 11200, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 47h:01m:59s remains)
2017-12-11 05:06:21.373445: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10529666 0.094534874 0.089321613 0.085232764 0.0819092 0.081087165 0.084278286 0.096725136 0.12173325 0.15587586 0.1900038 0.21330023 0.21803075 0.20251304 0.17524965][0.14283463 0.13950707 0.14266379 0.14390045 0.14132395 0.1375076 0.13649051 0.14726594 0.17479737 0.21553461 0.25744972 0.28583014 0.29042241 0.26932058 0.23412441][0.15553956 0.16864084 0.1922992 0.21267588 0.2224557 0.22199397 0.21606678 0.21716236 0.23411061 0.26625848 0.30195075 0.32528189 0.324827 0.29886067 0.26120529][0.14815649 0.17948444 0.2304908 0.2824733 0.31939176 0.33377767 0.3267889 0.31216383 0.3032698 0.30617255 0.315115 0.31792983 0.30449581 0.27388349 0.24040724][0.12732571 0.17634471 0.25672537 0.34778059 0.42475027 0.4681322 0.46981838 0.43998396 0.39494795 0.35058522 0.31321883 0.28079689 0.24746755 0.21266468 0.18793361][0.094915025 0.15870315 0.26546174 0.39624172 0.52040976 0.60592568 0.63103515 0.59645563 0.5171659 0.41914651 0.32525092 0.24943803 0.19333656 0.15575458 0.14130859][0.05555724 0.12454757 0.24578768 0.40550375 0.57264429 0.70565045 0.76792204 0.74671966 0.65093172 0.51355094 0.37291139 0.26001167 0.18519862 0.14573359 0.13702095][0.015635727 0.074969135 0.1913726 0.35840881 0.5507502 0.723538 0.82839936 0.83884162 0.75449294 0.60944682 0.45042863 0.32143983 0.24048531 0.202102 0.19103457][-0.019743027 0.018251717 0.11080273 0.26005265 0.4511089 0.64293683 0.78228122 0.83273327 0.78573513 0.67035645 0.53184438 0.41818172 0.35097441 0.32020077 0.29996189][-0.049993824 -0.038817622 0.018021302 0.12981105 0.29409102 0.47897905 0.63339567 0.71775055 0.719824 0.66173184 0.57946134 0.51507968 0.48608053 0.4746266 0.44508955][-0.070385657 -0.08474645 -0.066491015 -0.0020446472 0.1170059 0.27075791 0.41645941 0.51811063 0.5647769 0.57249612 0.5647459 0.5705564 0.59672844 0.61584336 0.5852887][-0.0763296 -0.10999878 -0.12455402 -0.10654292 -0.040554002 0.065994449 0.18263826 0.28140613 0.35607815 0.41942519 0.48377147 0.56195158 0.6470111 0.70282668 0.68401796][-0.070523687 -0.11329577 -0.14832264 -0.16497646 -0.14569943 -0.088455953 -0.010967514 0.0679551 0.14793445 0.24152733 0.355261 0.48773786 0.61947948 0.70955926 0.71429163][-0.060808916 -0.10501746 -0.14800887 -0.18296242 -0.19472578 -0.17710853 -0.13784751 -0.087414257 -0.022152986 0.071902074 0.20183219 0.35786569 0.51206326 0.62490678 0.66061854][-0.052890383 -0.09455356 -0.13713543 -0.17698273 -0.20370801 -0.21144463 -0.20206918 -0.17976271 -0.14014322 -0.069374628 0.043820679 0.1895276 0.33844906 0.45765841 0.52475786]]...]
INFO - root - 2017-12-11 05:06:26.788728: step 11210, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:11m:33s remains)
INFO - root - 2017-12-11 05:06:32.153394: step 11220, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 48h:03m:52s remains)
INFO - root - 2017-12-11 05:06:37.547374: step 11230, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:34m:57s remains)
INFO - root - 2017-12-11 05:06:42.943275: step 11240, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:38m:33s remains)
INFO - root - 2017-12-11 05:06:48.387471: step 11250, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 47h:53m:56s remains)
INFO - root - 2017-12-11 05:06:53.491229: step 11260, loss = 0.70, batch loss = 0.64 (25.6 examples/sec; 0.312 sec/batch; 27h:51m:14s remains)
INFO - root - 2017-12-11 05:06:58.726039: step 11270, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:05m:01s remains)
INFO - root - 2017-12-11 05:07:04.024355: step 11280, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:28m:39s remains)
INFO - root - 2017-12-11 05:07:09.366004: step 11290, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:30m:48s remains)
INFO - root - 2017-12-11 05:07:14.607369: step 11300, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 46h:08m:55s remains)
2017-12-11 05:07:15.145114: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.334867 0.32171965 0.29726309 0.2787554 0.27502528 0.28163052 0.28182247 0.26762438 0.25480333 0.26833984 0.30154115 0.32483286 0.32273936 0.29309589 0.24194853][0.38170746 0.36750558 0.33470839 0.30813825 0.298404 0.30233252 0.30615598 0.29955235 0.29286316 0.31026885 0.34543154 0.36888888 0.36354628 0.3283796 0.27107036][0.3906849 0.3762168 0.341149 0.30988362 0.29504427 0.2968004 0.30519366 0.30722439 0.30669615 0.32539722 0.358829 0.38089517 0.37402347 0.33694252 0.27927625][0.39414993 0.38077071 0.34515792 0.30951497 0.28783077 0.28366548 0.29183796 0.29918092 0.30331892 0.32350862 0.35654426 0.38026813 0.3760266 0.34046704 0.28471583][0.37753183 0.37188524 0.33906588 0.30268997 0.27715987 0.26906797 0.2767618 0.28815866 0.29780996 0.32176125 0.35509309 0.37752467 0.37205771 0.33485234 0.27813521][0.34503779 0.34748739 0.3216415 0.29070592 0.26831394 0.26242977 0.27322614 0.28972921 0.30503103 0.33102831 0.3605029 0.37567788 0.36416864 0.32282642 0.26324847][0.31976882 0.32562417 0.30476362 0.27779976 0.25776717 0.25270349 0.26419938 0.28382128 0.30326524 0.32994953 0.3554692 0.36622658 0.35297096 0.31188717 0.25290209][0.31343561 0.31440136 0.29030809 0.25994283 0.23609419 0.226218 0.2328805 0.25171626 0.2733539 0.30118379 0.32595876 0.33836353 0.33058473 0.2956692 0.24206421][0.31467071 0.30925894 0.27928084 0.24415791 0.2161174 0.20174751 0.20340282 0.22010939 0.24278998 0.27208868 0.29673922 0.30897567 0.30364379 0.27246004 0.22325206][0.31805208 0.30812031 0.27218413 0.23382139 0.20512752 0.19156998 0.19324303 0.21038778 0.23616707 0.26936784 0.29501471 0.30304867 0.29190117 0.25663215 0.20570958][0.33321533 0.31668526 0.27277419 0.22780415 0.1954836 0.1822073 0.18566211 0.20580268 0.2384451 0.28057155 0.31334421 0.32257307 0.30759242 0.26731241 0.21168208][0.36169297 0.34020242 0.28622472 0.22952375 0.18689585 0.16756022 0.16998145 0.19431348 0.23688354 0.29069993 0.33375424 0.34880838 0.334068 0.28986844 0.22838329][0.38334572 0.35843298 0.29664961 0.22952077 0.17718275 0.15234713 0.15673018 0.19030528 0.24697913 0.31290469 0.36436561 0.3825528 0.36469376 0.31185102 0.2397414][0.38394475 0.36285889 0.30252194 0.23403563 0.17916426 0.15427598 0.16511793 0.21079661 0.28161287 0.35647318 0.41147244 0.42672366 0.39913192 0.33194584 0.24510133][0.381102 0.36735842 0.31437284 0.24877268 0.19290742 0.16689676 0.18001884 0.23066266 0.30657181 0.38180363 0.43391255 0.4437803 0.40787357 0.33108377 0.2354496]]...]
INFO - root - 2017-12-11 05:07:20.479416: step 11310, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 45h:39m:03s remains)
INFO - root - 2017-12-11 05:07:25.827881: step 11320, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:55m:48s remains)
INFO - root - 2017-12-11 05:07:31.160161: step 11330, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 47h:14m:58s remains)
INFO - root - 2017-12-11 05:07:36.532503: step 11340, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:10m:16s remains)
INFO - root - 2017-12-11 05:07:41.936862: step 11350, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:58m:15s remains)
INFO - root - 2017-12-11 05:07:47.258483: step 11360, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:26m:00s remains)
INFO - root - 2017-12-11 05:07:52.329423: step 11370, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.508 sec/batch; 45h:18m:16s remains)
INFO - root - 2017-12-11 05:07:57.626091: step 11380, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:25m:56s remains)
INFO - root - 2017-12-11 05:08:03.044360: step 11390, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 51h:46m:07s remains)
INFO - root - 2017-12-11 05:08:08.384333: step 11400, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 45h:37m:52s remains)
2017-12-11 05:08:08.952475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042779975 -0.045226473 -0.051070619 -0.06234391 -0.076751024 -0.091199592 -0.099640585 -0.09953662 -0.092078708 -0.081397861 -0.072585762 -0.070048057 -0.077216253 -0.091406867 -0.10636126][0.038111694 0.043979961 0.039432261 0.02221225 -0.0040187971 -0.034076747 -0.055378068 -0.060411371 -0.050450511 -0.032309785 -0.015894733 -0.01034973 -0.024116853 -0.052591927 -0.084025621][0.1438296 0.17235334 0.18335779 0.17187412 0.14097452 0.096299484 0.059186976 0.04353822 0.0511608 0.073337637 0.093656056 0.097751208 0.070749059 0.019941771 -0.036040254][0.23612495 0.29907569 0.343907 0.360041 0.34690353 0.30535311 0.26145667 0.23455918 0.23225738 0.24716753 0.25759456 0.24711533 0.19557416 0.11304448 0.025079278][0.28406996 0.38308424 0.47333419 0.538309 0.57155859 0.56307739 0.53523123 0.50614542 0.48902881 0.48203042 0.46292782 0.41955984 0.33049184 0.20869917 0.085000589][0.27682158 0.40069887 0.53321952 0.65466094 0.75317657 0.80470991 0.81946111 0.80600041 0.77961856 0.74472255 0.682506 0.59127235 0.45458534 0.28958726 0.13031812][0.25210497 0.38222027 0.53633046 0.69843948 0.8581419 0.9813605 1.0599693 1.0840784 1.0631962 1.0039285 0.89246708 0.74267727 0.55092043 0.3417646 0.15081352][0.25639564 0.38095468 0.53084797 0.69968516 0.88895351 1.0672793 1.2096956 1.281567 1.2766545 1.1974229 1.0364552 0.82592154 0.58175391 0.33838406 0.13046323][0.3141177 0.42693228 0.5492785 0.68627393 0.85732573 1.0486336 1.2288047 1.3389747 1.351482 1.2596397 1.0627655 0.80745351 0.529587 0.27327126 0.070721529][0.38558808 0.47903809 0.55703163 0.63364249 0.74511582 0.90286928 1.0829896 1.2109841 1.2394605 1.1507903 0.94885713 0.684288 0.40433824 0.16037115 -0.016337372][0.41499794 0.48364329 0.51430762 0.52420741 0.55924451 0.65355152 0.7995013 0.92132181 0.960906 0.89093703 0.71589625 0.47942415 0.22926535 0.019639863 -0.11760309][0.36534306 0.41104531 0.4072361 0.36660916 0.335312 0.36009622 0.44998208 0.54057264 0.57575649 0.5253855 0.39284602 0.20948671 0.014677636 -0.14068012 -0.22644113][0.23300001 0.25738972 0.23554374 0.17088056 0.10255744 0.076245248 0.110714 0.15891831 0.17641732 0.13964289 0.052348346 -0.067436233 -0.19350955 -0.28468257 -0.31637484][0.065657787 0.067969747 0.03747604 -0.02937505 -0.10397579 -0.15138218 -0.15182246 -0.13524181 -0.13198514 -0.15564859 -0.20130274 -0.26105914 -0.32274678 -0.35884628 -0.3520157][-0.093649589 -0.10931916 -0.14120139 -0.19627085 -0.25564295 -0.29820088 -0.30971837 -0.30723673 -0.30855441 -0.31927776 -0.33458486 -0.35127094 -0.36682364 -0.36638445 -0.3400985]]...]
INFO - root - 2017-12-11 05:08:14.272155: step 11410, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 48h:32m:20s remains)
INFO - root - 2017-12-11 05:08:19.624286: step 11420, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:21m:45s remains)
INFO - root - 2017-12-11 05:08:25.067803: step 11430, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:11m:50s remains)
INFO - root - 2017-12-11 05:08:30.376388: step 11440, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 47h:04m:23s remains)
INFO - root - 2017-12-11 05:08:35.745539: step 11450, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 48h:31m:02s remains)
INFO - root - 2017-12-11 05:08:41.137547: step 11460, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 49h:42m:08s remains)
INFO - root - 2017-12-11 05:08:46.209827: step 11470, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:06m:10s remains)
INFO - root - 2017-12-11 05:08:51.615733: step 11480, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:48m:09s remains)
INFO - root - 2017-12-11 05:08:57.017771: step 11490, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:24m:35s remains)
INFO - root - 2017-12-11 05:09:02.314323: step 11500, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 47h:17m:30s remains)
2017-12-11 05:09:02.861231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058436267 -0.056994349 -0.041626345 -0.012460976 0.028840395 0.077670947 0.12617114 0.16670369 0.1924863 0.19990544 0.19052844 0.16800687 0.13617027 0.09981171 0.064506732][-0.051425152 -0.044419363 -0.020433133 0.019811226 0.075241111 0.13987301 0.200528 0.24464637 0.26173127 0.24928822 0.21451943 0.16986692 0.12515709 0.086068973 0.056601956][-0.029824426 -0.011895123 0.027243432 0.08613383 0.16169889 0.24457805 0.31555769 0.35696957 0.35448676 0.30899486 0.2377549 0.1644318 0.10467236 0.064087853 0.044138934][0.0038810198 0.037748802 0.098665394 0.18357261 0.28469127 0.38693067 0.46480781 0.49752063 0.4693138 0.38629958 0.27621022 0.17282353 0.096399955 0.051568508 0.037481539][0.047238253 0.099310718 0.18494835 0.29892904 0.42656943 0.54547262 0.62509096 0.64483261 0.590721 0.47424358 0.33050781 0.20035334 0.10750721 0.055782761 0.043045681][0.093226507 0.1623096 0.27136061 0.41238165 0.56305826 0.69376254 0.771049 0.77676022 0.70090747 0.55971408 0.39168122 0.24098213 0.13379529 0.074573182 0.06172701][0.12973632 0.21163478 0.33769882 0.49622831 0.65912151 0.79262233 0.86334842 0.856194 0.76644218 0.61403555 0.43677327 0.27740762 0.16265187 0.098680422 0.086452872][0.14868456 0.23881873 0.37319693 0.5353263 0.6940586 0.81633097 0.872545 0.85175288 0.75536025 0.60598391 0.43725812 0.28528604 0.17391655 0.11091585 0.10032866][0.142836 0.23265889 0.36358362 0.51386929 0.65211582 0.74994379 0.78470761 0.75038433 0.65423709 0.51966053 0.37391603 0.24313742 0.14597696 0.091183826 0.0847134][0.11016409 0.18640809 0.29900509 0.42339757 0.52997965 0.59659547 0.609064 0.56583208 0.47758371 0.36583588 0.25097167 0.14907169 0.073099352 0.032201663 0.032384247][0.057039812 0.10840935 0.19003697 0.27845693 0.34753048 0.38176137 0.37522361 0.32961297 0.25630504 0.17356972 0.0948317 0.027037919 -0.023127012 -0.047671739 -0.041981004][-0.0030703316 0.021338942 0.068136618 0.11829633 0.15074557 0.15637346 0.13570127 0.092713639 0.038273774 -0.013206307 -0.054784007 -0.087362774 -0.1106668 -0.11991765 -0.11265798][-0.057743385 -0.057070967 -0.041765019 -0.024594652 -0.020023709 -0.032215033 -0.057855409 -0.092561468 -0.12733868 -0.15206741 -0.16424619 -0.16867994 -0.16923396 -0.164979 -0.15606739][-0.098559044 -0.1158745 -0.12407767 -0.12950449 -0.13928583 -0.15454224 -0.17288722 -0.19221388 -0.20737037 -0.21287644 -0.20820811 -0.19775285 -0.1856204 -0.17286113 -0.16128673][-0.1157148 -0.13927914 -0.15692213 -0.17065184 -0.18214366 -0.19125324 -0.19756809 -0.20154716 -0.20192686 -0.197088 -0.18724924 -0.17471987 -0.1616523 -0.14897282 -0.13827534]]...]
INFO - root - 2017-12-11 05:09:08.228519: step 11510, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 47h:39m:58s remains)
INFO - root - 2017-12-11 05:09:13.640322: step 11520, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 46h:33m:26s remains)
INFO - root - 2017-12-11 05:09:18.896675: step 11530, loss = 0.66, batch loss = 0.60 (15.0 examples/sec; 0.532 sec/batch; 47h:28m:35s remains)
INFO - root - 2017-12-11 05:09:24.248076: step 11540, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 46h:30m:00s remains)
INFO - root - 2017-12-11 05:09:29.557035: step 11550, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:31m:58s remains)
INFO - root - 2017-12-11 05:09:34.887742: step 11560, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 46h:23m:48s remains)
INFO - root - 2017-12-11 05:09:39.929493: step 11570, loss = 0.67, batch loss = 0.61 (15.5 examples/sec; 0.517 sec/batch; 46h:07m:37s remains)
INFO - root - 2017-12-11 05:09:45.294674: step 11580, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:30m:42s remains)
INFO - root - 2017-12-11 05:09:50.684759: step 11590, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 47h:03m:27s remains)
INFO - root - 2017-12-11 05:09:56.066436: step 11600, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 47h:42m:18s remains)
2017-12-11 05:09:56.669598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.010795908 -0.010419299 -0.0121717 -0.014819471 -0.016817993 -0.018157613 -0.020200429 -0.023107294 -0.025269154 -0.025453325 -0.023293626 -0.019357605 -0.016708216 -0.018314214 -0.02448866][0.025800891 0.030587882 0.032130331 0.031875905 0.031210275 0.030480215 0.026672635 0.019713806 0.013973003 0.012056843 0.013686996 0.0177352 0.019067228 0.012572408 -0.001807016][0.089993648 0.10401247 0.11154627 0.11499292 0.11637722 0.1181701 0.11457488 0.10436126 0.094734021 0.089807563 0.0885867 0.089255556 0.085258879 0.06931743 0.041404765][0.17384067 0.20207658 0.21737388 0.22377156 0.22554907 0.22993471 0.22808783 0.21617097 0.20370878 0.19647376 0.19279733 0.1888199 0.17692074 0.14825714 0.10216236][0.25380427 0.29792422 0.32163379 0.33085334 0.33276278 0.33997411 0.34101734 0.32797962 0.31290284 0.30375862 0.29877412 0.28986609 0.26867977 0.22648661 0.16178071][0.30591193 0.36044794 0.38982466 0.40256855 0.40805396 0.42155826 0.42896703 0.41725865 0.40116632 0.39075485 0.38425955 0.36831343 0.3346101 0.27739087 0.19620837][0.32993728 0.38359272 0.40942332 0.42154312 0.4325439 0.45592371 0.47389749 0.46872574 0.455974 0.44629005 0.43789145 0.4141033 0.36695352 0.29478261 0.20063207][0.32252491 0.36492285 0.37765151 0.38180226 0.39457443 0.42478791 0.45239854 0.45762017 0.4531785 0.44759539 0.4387776 0.41031471 0.35511714 0.27460843 0.17604747][0.27651978 0.30021328 0.2960301 0.28956777 0.30070534 0.33279914 0.36618739 0.38203025 0.38752151 0.38685063 0.37743485 0.34669387 0.29000637 0.21103233 0.119766][0.20876828 0.21194498 0.19438441 0.1822006 0.19445151 0.22958826 0.26846132 0.29356503 0.30657119 0.30697376 0.29191005 0.25492832 0.19648355 0.12285127 0.045021][0.14859191 0.13670914 0.11262845 0.10044467 0.11693461 0.15797544 0.20364809 0.23678075 0.25444043 0.25202906 0.22703846 0.17959978 0.11660974 0.047062617 -0.017986683][0.0998483 0.0814802 0.056231305 0.045206591 0.063239947 0.10617215 0.15486473 0.19297987 0.21307606 0.20718449 0.17421943 0.12022723 0.056535326 -0.0066071362 -0.058818348][0.039217714 0.019687569 -0.0035843812 -0.014059026 0.00015420534 0.036297359 0.079944246 0.11755387 0.13789655 0.13142459 0.098789081 0.049258787 -0.0052665207 -0.054985657 -0.0908312][-0.030742856 -0.048707366 -0.068273611 -0.079328783 -0.074118979 -0.052873258 -0.023059594 0.0063745761 0.023602396 0.020078294 -0.0031095957 -0.037227485 -0.072877325 -0.10207181 -0.11813226][-0.083475381 -0.097932838 -0.1131845 -0.12520231 -0.13001308 -0.12551285 -0.11335767 -0.097480386 -0.086389124 -0.086149208 -0.096647523 -0.11200937 -0.12618823 -0.1338615 -0.13184708]]...]
INFO - root - 2017-12-11 05:10:01.957251: step 11610, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 47h:17m:15s remains)
INFO - root - 2017-12-11 05:10:07.327036: step 11620, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 49h:01m:06s remains)
INFO - root - 2017-12-11 05:10:12.739529: step 11630, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 49h:14m:57s remains)
INFO - root - 2017-12-11 05:10:18.045788: step 11640, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:12m:28s remains)
INFO - root - 2017-12-11 05:10:23.363561: step 11650, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:17m:52s remains)
INFO - root - 2017-12-11 05:10:28.728560: step 11660, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:28m:29s remains)
INFO - root - 2017-12-11 05:10:34.044138: step 11670, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.501 sec/batch; 44h:38m:59s remains)
INFO - root - 2017-12-11 05:10:39.149960: step 11680, loss = 0.67, batch loss = 0.61 (14.4 examples/sec; 0.555 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-11 05:10:44.507829: step 11690, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:45m:34s remains)
INFO - root - 2017-12-11 05:10:49.874781: step 11700, loss = 0.68, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 46h:02m:11s remains)
2017-12-11 05:10:50.416261: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32078403 0.34297752 0.3463302 0.33959937 0.33593312 0.35237539 0.38272738 0.40842819 0.42018098 0.41110647 0.38828427 0.37968594 0.3935892 0.40501508 0.38534778][0.312615 0.34590736 0.35870662 0.35736111 0.35314724 0.36967194 0.40456966 0.43888217 0.46010408 0.45651516 0.43267089 0.41945824 0.42913565 0.43951255 0.41926736][0.28822625 0.32626167 0.34431323 0.34710938 0.34402117 0.35962167 0.39574039 0.43565103 0.46556425 0.46728384 0.44265538 0.42488998 0.43089089 0.44069088 0.42387882][0.28067112 0.3169958 0.3352021 0.34038889 0.33782989 0.35040906 0.38392043 0.42535752 0.4595429 0.46212047 0.43389478 0.40875825 0.40738791 0.41388214 0.40179628][0.29091838 0.32150385 0.33700007 0.34605035 0.347897 0.36173958 0.39513233 0.43730476 0.47006625 0.46570954 0.42677858 0.38724318 0.37101027 0.36830911 0.35917807][0.31428564 0.33756721 0.34868711 0.36263835 0.37295961 0.39444262 0.43314815 0.47723782 0.50342393 0.48464254 0.42798159 0.36787611 0.33162266 0.3178834 0.31374288][0.33054712 0.34683815 0.35216236 0.36899289 0.3883633 0.42168278 0.47073773 0.52024269 0.54128337 0.50953352 0.43640828 0.3561576 0.30039018 0.27711409 0.27986065][0.31725779 0.32615423 0.32403114 0.33888429 0.3630099 0.40582702 0.46524826 0.52199244 0.54283386 0.50682449 0.42771798 0.33745193 0.27177316 0.24598177 0.25833511][0.2721518 0.271102 0.25846541 0.26463223 0.28638095 0.33259249 0.39878348 0.4624747 0.48822585 0.45821258 0.38570562 0.29820943 0.23418397 0.21338935 0.23561707][0.20844033 0.19306578 0.16612272 0.15823308 0.17236756 0.21805835 0.29017776 0.36375868 0.40108517 0.38497469 0.3260856 0.24799134 0.19051515 0.17645377 0.20529139][0.14885935 0.11951467 0.081343189 0.059895836 0.0629529 0.10273636 0.17810041 0.26384902 0.31858283 0.32058784 0.27636921 0.20682295 0.15285081 0.139985 0.16803953][0.10465822 0.07255628 0.03621541 0.010292049 0.0035981294 0.033536151 0.10694914 0.20009449 0.26858565 0.28489271 0.25181505 0.18792228 0.1323068 0.11190912 0.12860964][0.082242906 0.060482759 0.041029878 0.023017894 0.011652646 0.030627018 0.095095471 0.18354794 0.25248903 0.27359346 0.24766101 0.18927135 0.13178951 0.10193145 0.10466018][0.088843316 0.086768195 0.090682335 0.086156085 0.073709875 0.082562029 0.13181812 0.20307592 0.2592015 0.27762458 0.2588174 0.2104584 0.15704176 0.12322836 0.11766994][0.12784718 0.14410295 0.16470778 0.16924283 0.15516114 0.15356271 0.18298452 0.22917271 0.26526937 0.27872261 0.27136225 0.24251771 0.20485128 0.17741838 0.17243879]]...]
INFO - root - 2017-12-11 05:10:55.767209: step 11710, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:04m:37s remains)
INFO - root - 2017-12-11 05:11:01.100401: step 11720, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 47h:04m:06s remains)
INFO - root - 2017-12-11 05:11:06.369616: step 11730, loss = 0.71, batch loss = 0.65 (16.0 examples/sec; 0.500 sec/batch; 44h:31m:32s remains)
INFO - root - 2017-12-11 05:11:11.732518: step 11740, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.509 sec/batch; 45h:19m:25s remains)
INFO - root - 2017-12-11 05:11:17.122326: step 11750, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:06m:48s remains)
INFO - root - 2017-12-11 05:11:22.595777: step 11760, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 50h:47m:52s remains)
INFO - root - 2017-12-11 05:11:27.903032: step 11770, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:55m:46s remains)
INFO - root - 2017-12-11 05:11:32.904892: step 11780, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 46h:18m:09s remains)
INFO - root - 2017-12-11 05:11:38.288155: step 11790, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:03m:27s remains)
INFO - root - 2017-12-11 05:11:43.653772: step 11800, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:43m:54s remains)
2017-12-11 05:11:44.182420: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14378941 0.13716872 0.12327007 0.10968067 0.10214981 0.10748066 0.12337792 0.1371754 0.14029399 0.13034895 0.11384722 0.090260245 0.059478369 0.026784604 0.001344162][0.19302827 0.18862437 0.17256492 0.15557139 0.1454192 0.14976956 0.16494708 0.17546093 0.17298359 0.15598008 0.13397568 0.10621464 0.071066014 0.033624649 0.0041147922][0.23195149 0.23451486 0.22073521 0.2032533 0.19168866 0.19339179 0.2036617 0.20613529 0.19505365 0.17025194 0.14215897 0.10954786 0.07034459 0.030852204 0.0011135788][0.25032905 0.26169842 0.25427553 0.24074885 0.23095933 0.23070152 0.23553729 0.22968018 0.21079442 0.18000272 0.14662729 0.10898624 0.066210516 0.026370721 -0.0019447557][0.25533685 0.27329427 0.2730124 0.26470867 0.25786129 0.25687581 0.25848335 0.24709782 0.22249489 0.18702851 0.14864382 0.10632695 0.060734462 0.021330522 -0.005476418][0.2618992 0.28636387 0.29289666 0.28774494 0.28108296 0.27932876 0.28145641 0.2697902 0.24237239 0.20329316 0.16021757 0.11282492 0.062489688 0.020366173 -0.0074737323][0.27217048 0.30270174 0.31477529 0.31065217 0.30243012 0.30193806 0.30975863 0.30404523 0.2775974 0.2362752 0.18901686 0.13550669 0.077566721 0.028520998 -0.0035797656][0.27286607 0.30582058 0.31995255 0.31587392 0.3082028 0.31483033 0.33528042 0.34115604 0.31905627 0.27642012 0.22527063 0.16551089 0.0991361 0.041291516 0.0032925417][0.25368577 0.28186876 0.29205018 0.28657392 0.28262788 0.30210096 0.34012213 0.35989016 0.34318867 0.29962415 0.2458289 0.18283214 0.11185044 0.048465881 0.006503975][0.2170535 0.23334394 0.23358992 0.22450922 0.22525294 0.25822511 0.31314042 0.34699547 0.33803633 0.29643649 0.24282292 0.18033354 0.10941643 0.044838183 0.001964493][0.17022642 0.1705395 0.1565865 0.14112993 0.14408094 0.18483931 0.24884647 0.2931138 0.29520923 0.26297894 0.21671361 0.16046885 0.094565086 0.03336649 -0.0070710527][0.1184969 0.10558496 0.079229832 0.056162976 0.055277165 0.091953695 0.15124919 0.19667105 0.2086225 0.1907367 0.1582776 0.11432052 0.059854873 0.0091249542 -0.022900956][0.061617091 0.043703318 0.013795663 -0.012595606 -0.020243356 0.0024612762 0.044695437 0.081077091 0.096999466 0.091742314 0.074449971 0.046765719 0.0098014437 -0.024208756 -0.043509092][0.0060177865 -0.0090428907 -0.03295406 -0.055150066 -0.065920733 -0.057612926 -0.035649557 -0.013938046 -0.0013807913 -0.0010327173 -0.0078312289 -0.020681245 -0.038911212 -0.054397333 -0.060396262][-0.037450228 -0.046693243 -0.060886152 -0.074880637 -0.0836375 -0.0834284 -0.076159656 -0.067488261 -0.06173237 -0.061486524 -0.06361369 -0.066879146 -0.071262196 -0.072807029 -0.06899628]]...]
INFO - root - 2017-12-11 05:11:49.507527: step 11810, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:10m:25s remains)
INFO - root - 2017-12-11 05:11:54.931676: step 11820, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 48h:01m:16s remains)
INFO - root - 2017-12-11 05:12:00.330914: step 11830, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.510 sec/batch; 45h:28m:12s remains)
INFO - root - 2017-12-11 05:12:05.641191: step 11840, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:27m:29s remains)
INFO - root - 2017-12-11 05:12:11.044248: step 11850, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 47h:51m:58s remains)
INFO - root - 2017-12-11 05:12:16.421521: step 11860, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 49h:16m:12s remains)
INFO - root - 2017-12-11 05:12:21.801780: step 11870, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 47h:51m:04s remains)
INFO - root - 2017-12-11 05:12:26.899413: step 11880, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:36m:35s remains)
INFO - root - 2017-12-11 05:12:32.262742: step 11890, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 47h:52m:09s remains)
INFO - root - 2017-12-11 05:12:37.584009: step 11900, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 47h:27m:54s remains)
2017-12-11 05:12:38.165012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0069200727 0.043276392 0.098058619 0.1464435 0.17962416 0.1847156 0.15753089 0.10561956 0.046282932 -0.0080573987 -0.0472748 -0.07208278 -0.085895494 -0.091636889 -0.092672214][-0.011238211 0.036984812 0.09089566 0.13674036 0.16662925 0.17034513 0.14365171 0.092159905 0.029906483 -0.026887143 -0.067849621 -0.092153035 -0.10277381 -0.10168131 -0.094574362][-0.011784532 0.037649829 0.094925113 0.145743 0.1807877 0.191636 0.17410964 0.13126752 0.075324006 0.022345636 -0.017368848 -0.041469149 -0.051324967 -0.048217073 -0.041277193][-0.0048052296 0.053381544 0.12597619 0.1972184 0.25333932 0.28438166 0.28544337 0.25613779 0.20673421 0.15426444 0.11095937 0.080882676 0.062604569 0.055626456 0.049627591][0.0076704184 0.0790296 0.1734221 0.27295986 0.35868013 0.41679439 0.44030598 0.42453948 0.37883234 0.32297826 0.27220538 0.23146927 0.19777213 0.17231502 0.14693765][0.02151587 0.10531119 0.22078936 0.34676379 0.46008503 0.54341739 0.5869506 0.58101737 0.5348497 0.47262129 0.41325459 0.36238632 0.3148039 0.27378651 0.2348831][0.028377587 0.12150503 0.25210139 0.39551765 0.52644634 0.62670815 0.68439627 0.68372047 0.63364428 0.56353658 0.49648702 0.43906683 0.38486925 0.33952656 0.30247506][0.024820214 0.12293836 0.26257882 0.4156442 0.55439484 0.66111028 0.72401679 0.72228664 0.66468203 0.58526421 0.51148725 0.45123202 0.3981117 0.36048451 0.34043866][0.0082072755 0.10412297 0.2452873 0.40059826 0.53841472 0.64061248 0.69783539 0.68733633 0.61919981 0.53170878 0.45683816 0.40334249 0.36410236 0.34794319 0.35592923][-0.020906145 0.065004244 0.20007406 0.35178909 0.48378095 0.57572806 0.62204349 0.60159707 0.52693266 0.43923786 0.3740201 0.33967465 0.32609633 0.33920938 0.37520075][-0.05672165 0.015277482 0.13959302 0.28377631 0.40650323 0.48496291 0.51931375 0.49474314 0.42558223 0.35119358 0.3062833 0.29761505 0.31188211 0.35114193 0.40668309][-0.087416016 -0.027098458 0.085734643 0.21858834 0.32706252 0.38842806 0.40988594 0.3870652 0.33601502 0.28879285 0.27216536 0.28903338 0.32469589 0.378061 0.43767372][-0.098441243 -0.041108195 0.063385248 0.18175477 0.27068564 0.31162691 0.3177157 0.29598761 0.26585481 0.25136158 0.26722908 0.30875459 0.35937 0.41438881 0.4622286][-0.08714959 -0.025078591 0.073621981 0.17461602 0.24020964 0.25942323 0.24770381 0.22088447 0.20361711 0.21586652 0.2618477 0.32637545 0.38903269 0.44082612 0.47202921][-0.056473881 0.014796906 0.10964791 0.19308457 0.23422022 0.23027527 0.19722159 0.15644337 0.13777404 0.16104256 0.22478797 0.30431959 0.37412062 0.42281649 0.44444159]]...]
INFO - root - 2017-12-11 05:12:43.645738: step 11910, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:04m:37s remains)
INFO - root - 2017-12-11 05:12:49.044125: step 11920, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:40m:38s remains)
INFO - root - 2017-12-11 05:12:54.362570: step 11930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:15m:16s remains)
INFO - root - 2017-12-11 05:12:59.714870: step 11940, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:50m:28s remains)
INFO - root - 2017-12-11 05:13:05.135980: step 11950, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 49h:02m:40s remains)
INFO - root - 2017-12-11 05:13:10.523007: step 11960, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 45h:59m:26s remains)
INFO - root - 2017-12-11 05:13:15.805351: step 11970, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 46h:45m:29s remains)
INFO - root - 2017-12-11 05:13:20.831945: step 11980, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 46h:15m:40s remains)
INFO - root - 2017-12-11 05:13:26.164656: step 11990, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:06m:43s remains)
INFO - root - 2017-12-11 05:13:31.515274: step 12000, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:25m:48s remains)
2017-12-11 05:13:32.050153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12989894 0.15051115 0.15487128 0.15233226 0.15158986 0.15630881 0.1613116 0.15793376 0.14345613 0.12557642 0.11487447 0.11406613 0.11856211 0.12336729 0.12348759][0.19887705 0.23015521 0.23791435 0.2367602 0.24012028 0.25446761 0.27115178 0.27366427 0.25342026 0.2213477 0.19633237 0.18734013 0.19038813 0.19914097 0.20364507][0.25959957 0.29829839 0.30751842 0.30520126 0.30917484 0.33038932 0.35882449 0.37065926 0.34917197 0.30643257 0.26810187 0.2489817 0.24694254 0.25698426 0.26599315][0.28301522 0.32864594 0.34315544 0.34248206 0.34552467 0.36827496 0.40327075 0.42359072 0.40494171 0.3584657 0.31239516 0.28396559 0.27305257 0.27887121 0.28940114][0.26946893 0.32387042 0.35012355 0.35747316 0.36262619 0.38547081 0.42367738 0.45158717 0.43977103 0.39694381 0.34978649 0.31437948 0.2917732 0.286745 0.29272786][0.23805332 0.29886231 0.33546624 0.35090756 0.35977754 0.38417274 0.42607668 0.4628678 0.46313307 0.43188486 0.39104959 0.35293996 0.31761152 0.29460248 0.2870121][0.2058769 0.26787233 0.30775416 0.32669508 0.33875 0.36592534 0.41205081 0.4563899 0.46864083 0.4507148 0.419891 0.38330042 0.3370176 0.29204062 0.26353014][0.1801077 0.24025007 0.27931 0.29905689 0.31302869 0.34143409 0.38886887 0.43552095 0.45419449 0.44501105 0.42265716 0.39055058 0.33912554 0.27711126 0.23020607][0.17710094 0.23635587 0.27275115 0.28868547 0.29694021 0.31852797 0.3616792 0.40722147 0.42846718 0.42335451 0.40540814 0.37781596 0.32680956 0.25790858 0.20279317][0.21048437 0.27635267 0.31144521 0.31618124 0.30424562 0.30413917 0.33302912 0.37203959 0.39135328 0.386158 0.37056375 0.34965843 0.30619624 0.24245591 0.19042036][0.28214771 0.36340442 0.40036091 0.38761026 0.34206355 0.30558214 0.30843481 0.33304292 0.3450225 0.33741713 0.32558492 0.31612605 0.28943685 0.243427 0.20459057][0.34881625 0.44753847 0.49138159 0.46657473 0.38999489 0.31478012 0.28397644 0.28600916 0.28645265 0.2773791 0.2729418 0.27840218 0.27458411 0.25564751 0.238455][0.37352517 0.47974527 0.52859014 0.49953547 0.40475184 0.30035132 0.23655659 0.21239115 0.20098454 0.19492179 0.20267205 0.22524962 0.24618316 0.25765526 0.26586011][0.34719953 0.44829088 0.49579236 0.46635267 0.36646071 0.24798036 0.16113874 0.1149727 0.09459427 0.094212949 0.11496849 0.152996 0.19468042 0.23118529 0.25898328][0.27404684 0.36210883 0.40405345 0.37758261 0.2854194 0.17030922 0.076027982 0.017521409 -0.0082223592 -0.0041654208 0.025563257 0.074032314 0.12946311 0.18157938 0.21864432]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:13:37.360358: step 12010, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 47h:39m:40s remains)
INFO - root - 2017-12-11 05:13:42.716792: step 12020, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 47h:37m:28s remains)
INFO - root - 2017-12-11 05:13:48.027789: step 12030, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:36m:18s remains)
INFO - root - 2017-12-11 05:13:53.435791: step 12040, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:47m:03s remains)
INFO - root - 2017-12-11 05:13:58.803403: step 12050, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:54m:13s remains)
INFO - root - 2017-12-11 05:14:04.108976: step 12060, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:39m:29s remains)
INFO - root - 2017-12-11 05:14:09.422760: step 12070, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:28m:03s remains)
INFO - root - 2017-12-11 05:14:14.789717: step 12080, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:55m:54s remains)
INFO - root - 2017-12-11 05:14:19.849488: step 12090, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 46h:46m:25s remains)
INFO - root - 2017-12-11 05:14:25.240516: step 12100, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:32m:49s remains)
2017-12-11 05:14:25.810483: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41301131 0.42049628 0.41923556 0.41914493 0.41128385 0.37361288 0.29817522 0.205255 0.13295482 0.1036332 0.1036093 0.11037458 0.11298347 0.10805661 0.099223562][0.4741106 0.50131106 0.508645 0.50598311 0.4905811 0.44471109 0.35901996 0.25098976 0.16317484 0.11742755 0.10087935 0.092712618 0.085855052 0.082153171 0.086165637][0.4639731 0.50970048 0.53115273 0.53489012 0.52281278 0.48406738 0.4071179 0.30230409 0.20879358 0.14954883 0.11595519 0.091327116 0.072128162 0.064399891 0.073461637][0.38428003 0.43961781 0.47562116 0.49514145 0.50136358 0.48852688 0.44137973 0.35980374 0.27566153 0.21292534 0.16905145 0.13170326 0.099238984 0.080315858 0.08068566][0.2819328 0.33693019 0.38428316 0.42371669 0.45706615 0.4801307 0.47363836 0.42578933 0.35895765 0.29921302 0.2509591 0.20544642 0.16103938 0.12732702 0.11037442][0.19955854 0.2488597 0.30481648 0.36417794 0.42498025 0.48104408 0.5107255 0.49374574 0.44297165 0.386222 0.33504486 0.28452432 0.23242755 0.18777755 0.15614527][0.14799096 0.19046387 0.25379011 0.33136094 0.41514322 0.4949742 0.54841852 0.55160391 0.51031607 0.45293972 0.39670411 0.34142828 0.28573397 0.23776451 0.20160066][0.11476343 0.14985298 0.21547972 0.30253071 0.39828467 0.489902 0.5551939 0.56982952 0.53455114 0.47639704 0.41584578 0.35744429 0.30305108 0.26107097 0.23313498][0.088966683 0.11389091 0.1728427 0.2560994 0.34924203 0.43966925 0.50721616 0.52902627 0.501145 0.44612634 0.38521737 0.32686713 0.277076 0.24521351 0.23104109][0.059481211 0.069756493 0.11297961 0.18111631 0.26110578 0.34235486 0.40705764 0.43494117 0.41858625 0.37389582 0.31940627 0.26531929 0.22107011 0.19688916 0.19214354][0.027327955 0.020844918 0.043329544 0.091016956 0.15482274 0.22647196 0.28862086 0.32239246 0.31886831 0.28845644 0.24482353 0.19752009 0.15754642 0.13571164 0.13280982][0.01102834 -0.011634544 -0.0088029141 0.01865539 0.066110678 0.12796256 0.18684648 0.22416569 0.23068161 0.21310686 0.18005334 0.13934958 0.10246989 0.079954535 0.074462779][0.016147919 -0.013973204 -0.022453932 -0.0097804479 0.022530459 0.072212406 0.12379697 0.15963067 0.17146605 0.16287109 0.1380529 0.10278928 0.068720475 0.04565344 0.037348811][0.036658198 0.015379008 0.0095999129 0.015972914 0.034992982 0.068923555 0.10781119 0.13673913 0.14858791 0.14404425 0.12446632 0.09451969 0.065083846 0.043825958 0.033510044][0.061667018 0.061173681 0.0681313 0.074175254 0.082346767 0.099751838 0.12288843 0.14073464 0.14722233 0.14154746 0.12478925 0.1020456 0.082225092 0.068429172 0.059552003]]...]
INFO - root - 2017-12-11 05:14:31.146818: step 12110, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 46h:22m:46s remains)
INFO - root - 2017-12-11 05:14:36.598903: step 12120, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:48m:01s remains)
INFO - root - 2017-12-11 05:14:41.927801: step 12130, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 47h:00m:04s remains)
INFO - root - 2017-12-11 05:14:47.202234: step 12140, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:39m:30s remains)
INFO - root - 2017-12-11 05:14:52.575136: step 12150, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 46h:28m:17s remains)
INFO - root - 2017-12-11 05:14:57.987067: step 12160, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:43m:23s remains)
INFO - root - 2017-12-11 05:15:03.313214: step 12170, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 47h:48m:01s remains)
INFO - root - 2017-12-11 05:15:08.666108: step 12180, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:37m:28s remains)
INFO - root - 2017-12-11 05:15:13.751163: step 12190, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 46h:56m:55s remains)
INFO - root - 2017-12-11 05:15:19.141054: step 12200, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 49h:24m:17s remains)
2017-12-11 05:15:19.741046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016675888 -0.014711201 -0.011896304 -0.0098417616 -0.0080107041 -0.0085584931 -0.01149343 -0.014727212 -0.015184602 -0.014110873 -0.013355326 -0.013945105 -0.016298536 -0.020537598 -0.026013872][0.019807287 0.029185614 0.036544617 0.040001061 0.042618882 0.041517917 0.036377862 0.029494183 0.026520411 0.0254745 0.023708152 0.020300014 0.014171679 0.0045388141 -0.0078339614][0.088205628 0.11180295 0.12797481 0.13411634 0.13723172 0.13498269 0.12743133 0.1158039 0.1079683 0.10149362 0.093405455 0.083747163 0.070449561 0.05185321 0.028981542][0.18032047 0.22255726 0.25002828 0.25971818 0.26296169 0.25940162 0.2508873 0.23619737 0.22382352 0.21049355 0.19294493 0.17257929 0.14677042 0.11408412 0.076577149][0.27798414 0.33931696 0.37766907 0.39036745 0.39233777 0.38689932 0.37975508 0.36638868 0.35400468 0.33763313 0.3122361 0.27899703 0.2351388 0.18250734 0.12641388][0.35425118 0.42996684 0.47554025 0.48903432 0.48819241 0.48187181 0.48109353 0.47668967 0.47187483 0.45915392 0.43069264 0.38577747 0.32137027 0.24508995 0.16826198][0.38853797 0.47068149 0.51731068 0.5279873 0.52264273 0.51684183 0.52749538 0.54065716 0.55314922 0.55298316 0.52906644 0.47727525 0.39423594 0.29459688 0.19741029][0.37495962 0.45527047 0.49730057 0.50180376 0.48996511 0.48370796 0.50560594 0.53997695 0.57740927 0.598298 0.58711123 0.53589559 0.44187361 0.32597804 0.21312611][0.31686246 0.38964546 0.42487758 0.42306417 0.40514663 0.39673617 0.4237563 0.47317192 0.53265029 0.57548904 0.581453 0.53878754 0.44568685 0.32662055 0.20843796][0.22448574 0.28532338 0.31446692 0.31065705 0.29211953 0.28311163 0.30826208 0.35967892 0.42682824 0.48086587 0.49916482 0.46793997 0.386588 0.27935758 0.1706499][0.12096062 0.16701275 0.19048755 0.18812998 0.173481 0.16480412 0.18146873 0.22153291 0.27972072 0.33070961 0.35305285 0.33336204 0.27179536 0.18850143 0.10220297][0.027051616 0.057311885 0.074176423 0.073401846 0.063145675 0.053679388 0.057855785 0.07920479 0.11848603 0.15732604 0.17798117 0.1692045 0.13151368 0.078722358 0.022279285][-0.046020485 -0.0305581 -0.020887289 -0.021437922 -0.028449833 -0.038168453 -0.04347232 -0.038608477 -0.019024445 0.0046937596 0.019962288 0.018969534 0.0024590113 -0.022033453 -0.049365822][-0.092403449 -0.088816211 -0.085162058 -0.085810661 -0.089865848 -0.0971294 -0.10455634 -0.1077223 -0.10177911 -0.091034777 -0.082633547 -0.080742128 -0.084543943 -0.090587117 -0.097132951][-0.10807682 -0.11130033 -0.11111767 -0.11143333 -0.11287931 -0.11635371 -0.12089096 -0.12442301 -0.12395627 -0.12052883 -0.11726319 -0.11572866 -0.11521751 -0.11401421 -0.11137593]]...]
INFO - root - 2017-12-11 05:15:25.115909: step 12210, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:28m:35s remains)
INFO - root - 2017-12-11 05:15:30.416720: step 12220, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:33m:19s remains)
INFO - root - 2017-12-11 05:15:35.883639: step 12230, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:43m:58s remains)
INFO - root - 2017-12-11 05:15:41.274743: step 12240, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:23m:22s remains)
INFO - root - 2017-12-11 05:15:46.589137: step 12250, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 46h:41m:32s remains)
INFO - root - 2017-12-11 05:15:51.952372: step 12260, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:49m:32s remains)
INFO - root - 2017-12-11 05:15:57.291787: step 12270, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:55m:29s remains)
INFO - root - 2017-12-11 05:16:02.627192: step 12280, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:44m:08s remains)
INFO - root - 2017-12-11 05:16:07.757998: step 12290, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:36m:57s remains)
INFO - root - 2017-12-11 05:16:13.137946: step 12300, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.551 sec/batch; 49h:00m:07s remains)
2017-12-11 05:16:13.679021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15177062 0.23621526 0.3172459 0.37099937 0.37675968 0.32857779 0.24368638 0.1580445 0.10790796 0.098826759 0.1154172 0.13828093 0.15275626 0.15454343 0.14319803][0.19522487 0.31226617 0.42361087 0.49767989 0.50851649 0.44905189 0.34230769 0.2341277 0.16722825 0.14388831 0.14070991 0.13847049 0.12977841 0.11753809 0.10182159][0.22043049 0.35971323 0.49056357 0.57858723 0.59912026 0.54372311 0.43680415 0.32629818 0.25297821 0.21384422 0.18049572 0.13892047 0.09425696 0.059601337 0.037217364][0.23357663 0.38650164 0.52711827 0.62211066 0.65398455 0.61647904 0.53024161 0.43613333 0.36621547 0.31451932 0.25218007 0.17133337 0.088728324 0.027646042 -0.0067139817][0.22240493 0.38126355 0.5271365 0.62826025 0.67613292 0.66805118 0.6153788 0.54559666 0.47965395 0.41426295 0.32554385 0.21207547 0.098463386 0.014477265 -0.032306366][0.17473799 0.32859766 0.4754675 0.58507788 0.6555469 0.68503571 0.67142761 0.62676352 0.56064868 0.47792304 0.36583892 0.2301335 0.09958569 0.0053208163 -0.045196474][0.099034294 0.23273423 0.37365213 0.49586517 0.5961886 0.66691506 0.69184053 0.66814834 0.59577632 0.4906716 0.35833514 0.21328558 0.0834833 -0.004627686 -0.047095813][0.033372652 0.13805304 0.266769 0.40025565 0.52674836 0.62807792 0.678322 0.66464508 0.58198959 0.45640805 0.31244019 0.17234175 0.058339823 -0.012980637 -0.043199159][-0.0025903932 0.07534159 0.18968792 0.32642886 0.46267658 0.57103837 0.62194139 0.60012752 0.50391829 0.36782789 0.22977541 0.11379021 0.031444948 -0.014400627 -0.03108977][-0.018035196 0.0374999 0.13555245 0.2639555 0.39230216 0.48807511 0.52156949 0.48116478 0.37460843 0.24230258 0.1276077 0.050879054 0.010697648 -0.0045161746 -0.00703756][-0.020849606 0.014427189 0.094842553 0.20890181 0.32239309 0.40026879 0.41502354 0.36172915 0.25656581 0.14183059 0.059541114 0.024050847 0.0217404 0.030226281 0.03577948][-0.011383839 0.0044435007 0.065143056 0.16145319 0.258589 0.32173166 0.32704428 0.27641681 0.18907505 0.10410873 0.057111066 0.05515746 0.07592161 0.0924494 0.094699807][0.00028730012 -0.0021996843 0.035445649 0.10814723 0.1847512 0.23523328 0.24054551 0.206724 0.15087342 0.10484519 0.094385743 0.11947432 0.15355064 0.16964017 0.16379954][-0.0025215913 -0.018006094 -0.00050059322 0.048251078 0.10340615 0.14266127 0.15303943 0.14207368 0.12216289 0.11577743 0.1374025 0.18209195 0.22348814 0.23748416 0.22632402][-0.025924303 -0.045621403 -0.038223017 -0.0048549958 0.036230918 0.069715142 0.08842583 0.099850923 0.1122232 0.13835295 0.18093888 0.23220204 0.26990581 0.27630964 0.26073191]]...]
INFO - root - 2017-12-11 05:16:18.990965: step 12310, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:50m:42s remains)
INFO - root - 2017-12-11 05:16:24.344356: step 12320, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:45m:31s remains)
INFO - root - 2017-12-11 05:16:29.627708: step 12330, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.510 sec/batch; 45h:20m:26s remains)
INFO - root - 2017-12-11 05:16:34.981476: step 12340, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:27m:43s remains)
INFO - root - 2017-12-11 05:16:40.363801: step 12350, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 46h:55m:19s remains)
INFO - root - 2017-12-11 05:16:45.654378: step 12360, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 46h:08m:48s remains)
INFO - root - 2017-12-11 05:16:50.942155: step 12370, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 46h:54m:15s remains)
INFO - root - 2017-12-11 05:16:56.250988: step 12380, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:34m:22s remains)
INFO - root - 2017-12-11 05:17:01.538637: step 12390, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.511 sec/batch; 45h:26m:59s remains)
INFO - root - 2017-12-11 05:17:06.477134: step 12400, loss = 0.69, batch loss = 0.64 (15.7 examples/sec; 0.511 sec/batch; 45h:23m:48s remains)
2017-12-11 05:17:07.039791: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20434529 0.19316883 0.15866572 0.10772474 0.054147653 0.014982141 -0.0010145627 0.0051851161 0.02453615 0.048258383 0.065728948 0.0642556 0.042689264 0.0082255257 -0.027501589][0.25474873 0.24376991 0.20784329 0.15709992 0.10686739 0.074156441 0.067557469 0.084537826 0.11236329 0.14089182 0.15652075 0.14594667 0.11026406 0.062714711 0.01808526][0.2569246 0.25223795 0.22405751 0.1835316 0.14552632 0.12633781 0.13241825 0.15810928 0.18876396 0.21426575 0.22131985 0.19955391 0.15528493 0.1050873 0.06315504][0.23320781 0.23834346 0.22039436 0.19125557 0.16517356 0.15691526 0.17108557 0.19834419 0.22341895 0.23727436 0.22931838 0.19631343 0.1510931 0.10961848 0.081934579][0.22693306 0.23996012 0.2283293 0.20764698 0.19350435 0.19730328 0.21883878 0.24351183 0.25595513 0.24981904 0.22110063 0.17834291 0.14058897 0.11826862 0.11365147][0.26037711 0.2737639 0.26273379 0.24931096 0.25150129 0.27387425 0.30549604 0.32538307 0.31988528 0.28901264 0.23978852 0.19373883 0.17340302 0.17898548 0.20095895][0.3161709 0.32502943 0.31327924 0.3098473 0.33345646 0.37840053 0.42022219 0.43334481 0.40913448 0.3567389 0.29499856 0.253962 0.25614625 0.28981 0.33263829][0.35787353 0.36427629 0.35564563 0.3651374 0.40850526 0.46988285 0.51572537 0.52047 0.48200804 0.41737875 0.35321233 0.32189944 0.3429853 0.39517111 0.44622463][0.36212513 0.3689127 0.36790922 0.38961527 0.44269815 0.50476056 0.54259378 0.53602564 0.48885396 0.42182618 0.36296535 0.34101912 0.36975783 0.42318624 0.46645632][0.33875567 0.34968776 0.36026692 0.3913089 0.44192532 0.48809823 0.50557381 0.48286155 0.42806545 0.36198664 0.30913812 0.28986967 0.30987096 0.34405357 0.36418757][0.32653755 0.34642935 0.37006456 0.40621728 0.44601771 0.4687975 0.46303529 0.42583132 0.36612651 0.30124664 0.2496468 0.22184142 0.21498694 0.21039958 0.19377717][0.33301228 0.36355072 0.39751521 0.4352096 0.46415126 0.4700262 0.45214877 0.41296127 0.3597005 0.30307114 0.2537204 0.21283704 0.17069083 0.11833921 0.058082055][0.33311367 0.36820829 0.40549758 0.44183096 0.46545997 0.4690468 0.45873347 0.437 0.40636325 0.36956057 0.32935113 0.28039265 0.20949341 0.11728314 0.019669449][0.30437592 0.33432496 0.36671132 0.3982397 0.42094576 0.43446457 0.44937989 0.46334696 0.46948645 0.4620133 0.4378655 0.38917795 0.30401933 0.19062984 0.072163612][0.24099723 0.25965807 0.28227514 0.30645618 0.32862929 0.35527349 0.40076721 0.45660031 0.50482196 0.53101355 0.52741307 0.48630404 0.40091109 0.28525203 0.16421132]]...]
INFO - root - 2017-12-11 05:17:12.365362: step 12410, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:36m:50s remains)
INFO - root - 2017-12-11 05:17:17.702612: step 12420, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.528 sec/batch; 46h:58m:58s remains)
INFO - root - 2017-12-11 05:17:23.018883: step 12430, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:04m:06s remains)
INFO - root - 2017-12-11 05:17:28.337675: step 12440, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:37m:44s remains)
INFO - root - 2017-12-11 05:17:33.647980: step 12450, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:16m:57s remains)
INFO - root - 2017-12-11 05:17:39.007987: step 12460, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 47h:12m:28s remains)
INFO - root - 2017-12-11 05:17:44.424836: step 12470, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:41m:56s remains)
INFO - root - 2017-12-11 05:17:49.878588: step 12480, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 47h:43m:03s remains)
INFO - root - 2017-12-11 05:17:55.170433: step 12490, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:20m:27s remains)
INFO - root - 2017-12-11 05:18:00.211521: step 12500, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.512 sec/batch; 45h:30m:51s remains)
2017-12-11 05:18:00.763833: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31632861 0.34848893 0.36130148 0.34793869 0.3247236 0.30600113 0.28477523 0.25555924 0.22726071 0.20167646 0.17091487 0.13216898 0.096892394 0.078368247 0.076562159][0.39336115 0.42380935 0.43598342 0.42402232 0.40032521 0.37941268 0.3533904 0.31541216 0.27340049 0.23340781 0.19324057 0.1507778 0.11506311 0.096897691 0.096265517][0.4315348 0.45208448 0.45870748 0.44900116 0.42798606 0.40692294 0.37766054 0.33486164 0.28432137 0.23637293 0.19534287 0.15813692 0.12867972 0.1130376 0.112487][0.41823137 0.42895803 0.43357497 0.43207076 0.42072836 0.4048121 0.37600133 0.33220494 0.27866563 0.22936228 0.19334131 0.16576269 0.14562105 0.13395005 0.13326359][0.36127314 0.37133375 0.38240463 0.39584967 0.40009603 0.3951166 0.37179732 0.33011013 0.27738485 0.2316609 0.20527306 0.19021213 0.18139406 0.17511083 0.17447542][0.28753266 0.30193609 0.32345241 0.35149261 0.37001327 0.37778214 0.36472842 0.32944414 0.28104129 0.24312548 0.23095612 0.23183736 0.23544019 0.23382756 0.23191014][0.21729179 0.23501642 0.26524711 0.30261123 0.33044064 0.34938282 0.3500351 0.32631183 0.28577474 0.25775409 0.26124427 0.27861995 0.29235837 0.29149693 0.28526044][0.15902829 0.17974225 0.218769 0.26370266 0.29819322 0.32531929 0.33868214 0.32680377 0.29342288 0.27241626 0.28655091 0.31551877 0.33476612 0.33184674 0.3209725][0.12110467 0.14448461 0.19157475 0.24421781 0.28443542 0.316206 0.3376075 0.33401355 0.30522719 0.28655371 0.302873 0.33487663 0.35417017 0.34875888 0.33531633][0.11853325 0.14024988 0.18872124 0.24482268 0.28718251 0.3176685 0.33878791 0.33672675 0.30901271 0.28776753 0.29774269 0.32415071 0.33946046 0.3336786 0.32178849][0.1462281 0.16040309 0.20198722 0.25489253 0.29513416 0.32014376 0.33511338 0.33032423 0.303573 0.28006133 0.28159934 0.29880854 0.30827576 0.30228207 0.29208642][0.17433661 0.18029945 0.21093562 0.25570258 0.29080951 0.309441 0.31739688 0.30986249 0.28751552 0.26756757 0.26600644 0.27647889 0.28056812 0.27338389 0.26277137][0.19164787 0.19051698 0.20880274 0.24234267 0.27050117 0.28359321 0.28587878 0.27630541 0.26033887 0.24856289 0.24909614 0.25557673 0.25471246 0.24581809 0.23423591][0.18895569 0.18046299 0.18561484 0.20621489 0.22659685 0.23621444 0.23632437 0.22717969 0.21871245 0.21676612 0.22174484 0.22487791 0.21843852 0.20773192 0.19647606][0.15326606 0.13804689 0.13140099 0.13920401 0.15146314 0.15881102 0.1596067 0.15381372 0.15403828 0.16280498 0.17439982 0.17625159 0.16581291 0.15451302 0.14500181]]...]
INFO - root - 2017-12-11 05:18:06.124029: step 12510, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:04m:25s remains)
INFO - root - 2017-12-11 05:18:11.525362: step 12520, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:22m:24s remains)
INFO - root - 2017-12-11 05:18:16.824796: step 12530, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 47h:02m:59s remains)
INFO - root - 2017-12-11 05:18:22.158096: step 12540, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:39m:55s remains)
INFO - root - 2017-12-11 05:18:27.540867: step 12550, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 50h:21m:15s remains)
INFO - root - 2017-12-11 05:18:32.833123: step 12560, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:17m:40s remains)
INFO - root - 2017-12-11 05:18:38.228888: step 12570, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 47h:41m:28s remains)
INFO - root - 2017-12-11 05:18:43.557876: step 12580, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:00m:37s remains)
INFO - root - 2017-12-11 05:18:48.873375: step 12590, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:07m:20s remains)
INFO - root - 2017-12-11 05:18:53.974941: step 12600, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 45h:50m:00s remains)
2017-12-11 05:18:54.547745: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11173595 0.081068225 0.042036638 0.0054235281 -0.013633132 -0.0094871791 0.011497364 0.045145039 0.07901305 0.10477035 0.10617844 0.080535986 0.043005925 0.0080788732 -0.017099813][0.13034013 0.0927815 0.045939915 0.0042452021 -0.017772347 -0.011202122 0.018993374 0.06467849 0.11014066 0.14372933 0.14812188 0.12006104 0.075861752 0.031662997 -0.0031637156][0.19992343 0.16176023 0.1092334 0.06090131 0.033204354 0.038861521 0.072816625 0.12187169 0.16607322 0.19337821 0.18986586 0.15361843 0.10059803 0.047593638 0.0058841021][0.32968491 0.2956318 0.2388957 0.18290205 0.14763461 0.14892745 0.17962211 0.2210573 0.24989901 0.2559796 0.23112184 0.17715597 0.11098574 0.049496517 0.004279831][0.48425075 0.45915115 0.40372145 0.34529242 0.30590108 0.30274579 0.32557368 0.35124874 0.35491753 0.3293241 0.27297255 0.19303119 0.10932446 0.039463822 -0.0064444127][0.60088962 0.58862472 0.54144752 0.48940724 0.45454019 0.45334595 0.47169626 0.48270217 0.46109775 0.40274745 0.31202447 0.20262171 0.099507779 0.021866761 -0.022822816][0.62481332 0.62794828 0.59608459 0.5589723 0.53754294 0.54772621 0.57136476 0.57816505 0.54246962 0.46231118 0.34536529 0.21104896 0.090472996 0.0054586488 -0.038310304][0.53530496 0.55477357 0.54349679 0.52675557 0.52327651 0.55043966 0.58809125 0.60376012 0.57180947 0.49023008 0.3660776 0.22043885 0.089760281 -0.00055845646 -0.044985797][0.35638469 0.38656229 0.39449158 0.39722484 0.41018543 0.45247698 0.50484836 0.53528661 0.52047914 0.45754895 0.34940463 0.21327043 0.086820021 -0.0011270371 -0.04417409][0.14811487 0.17560133 0.19224748 0.20573898 0.22791272 0.27799788 0.33880988 0.38180116 0.38879603 0.35633242 0.28198883 0.17545864 0.070452131 -0.0038352776 -0.040530007][-0.020046284 -0.0089623267 0.0021529123 0.014171218 0.035841659 0.084332563 0.144986 0.19469836 0.22187702 0.22269304 0.18980066 0.12413847 0.051484454 -0.0024127867 -0.031075722][-0.11556976 -0.12591486 -0.12820728 -0.12529615 -0.11092321 -0.07146918 -0.018253483 0.032176651 0.074562714 0.10453202 0.11080454 0.086183123 0.046535224 0.011918414 -0.011850586][-0.14918816 -0.17668305 -0.19081534 -0.19657172 -0.19006734 -0.16235828 -0.12089063 -0.074749328 -0.023779342 0.026947439 0.063192688 0.071791217 0.058945194 0.038282227 0.014997598][-0.13875255 -0.17484279 -0.19521137 -0.20611994 -0.20598133 -0.19029462 -0.16272895 -0.12473556 -0.0716789 -0.0089339074 0.046720169 0.078294866 0.083884194 0.071061075 0.044660721][-0.09437421 -0.12974627 -0.15025917 -0.16259213 -0.1672409 -0.16265804 -0.14944661 -0.12252478 -0.073930942 -0.0085504381 0.0551907 0.098219141 0.11335412 0.10311045 0.071840152]]...]
INFO - root - 2017-12-11 05:18:59.971727: step 12610, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 47h:32m:16s remains)
INFO - root - 2017-12-11 05:19:05.377768: step 12620, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:14m:34s remains)
INFO - root - 2017-12-11 05:19:10.759854: step 12630, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:10m:03s remains)
INFO - root - 2017-12-11 05:19:16.007413: step 12640, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 47h:13m:09s remains)
INFO - root - 2017-12-11 05:19:21.260544: step 12650, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.531 sec/batch; 47h:11m:05s remains)
INFO - root - 2017-12-11 05:19:26.696155: step 12660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:12m:38s remains)
INFO - root - 2017-12-11 05:19:32.096687: step 12670, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:44m:58s remains)
INFO - root - 2017-12-11 05:19:37.433438: step 12680, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:24m:32s remains)
INFO - root - 2017-12-11 05:19:42.749577: step 12690, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 48h:30m:29s remains)
INFO - root - 2017-12-11 05:19:47.847921: step 12700, loss = 0.68, batch loss = 0.62 (21.4 examples/sec; 0.375 sec/batch; 33h:16m:39s remains)
2017-12-11 05:19:48.395539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32418409 0.33170015 0.3210358 0.29327929 0.24795778 0.18862309 0.13111414 0.095920034 0.095879115 0.12826687 0.17408571 0.21235956 0.22796758 0.22558439 0.22195862][0.29823679 0.30432388 0.29648328 0.27634445 0.24119225 0.19073805 0.13334899 0.087801196 0.073529325 0.0915141 0.12610851 0.15641706 0.16693792 0.1610541 0.15275751][0.25548187 0.25727034 0.25053132 0.23845692 0.21715063 0.182469 0.13435343 0.085739858 0.0572555 0.054180134 0.06754683 0.081281148 0.082592055 0.072943509 0.0625653][0.22104585 0.21373115 0.20442925 0.19788025 0.19105145 0.17721829 0.14770798 0.10550757 0.066896424 0.040767584 0.027106332 0.018709527 0.00843253 -0.0037875443 -0.012981492][0.19923088 0.17968965 0.16340435 0.15838197 0.16443741 0.17358467 0.16957307 0.14340428 0.10300682 0.058761012 0.019262139 -0.012114106 -0.034710929 -0.049184915 -0.055919133][0.19635405 0.16487773 0.13648237 0.12658188 0.14024504 0.16970797 0.19255616 0.18803833 0.1539146 0.10038329 0.043237641 -0.0047671893 -0.036506072 -0.05273037 -0.058119465][0.215762 0.17539777 0.13158865 0.10998453 0.12338393 0.16629568 0.21160606 0.22845697 0.20485765 0.15061329 0.087475747 0.035425734 0.0040306053 -0.009801629 -0.014825989][0.2422099 0.19701904 0.14105792 0.10728525 0.11632713 0.16642278 0.22825667 0.26351869 0.25255275 0.20486271 0.14716816 0.10355346 0.082354955 0.075460605 0.069929838][0.26417857 0.21763359 0.15475647 0.11205386 0.11600813 0.16853724 0.24024788 0.28932297 0.29138637 0.25522208 0.21030247 0.18129927 0.1742499 0.17636286 0.17114866][0.26653314 0.22338916 0.16060367 0.11475743 0.11573656 0.16890122 0.24624714 0.30576009 0.31978476 0.29507214 0.26170912 0.24420433 0.24810702 0.25844276 0.2558324][0.23433955 0.19947176 0.14587632 0.10567475 0.10891303 0.16353464 0.24411885 0.31050122 0.33344582 0.31693229 0.28946346 0.27585521 0.28381938 0.29937068 0.30133027][0.16741017 0.14218339 0.10439233 0.07813929 0.090203986 0.1488947 0.2313769 0.3009541 0.32892725 0.3174606 0.29252192 0.27884725 0.28639671 0.30366877 0.30973434][0.081610911 0.065759905 0.045222383 0.036416449 0.060951523 0.12534213 0.20892179 0.27915558 0.30985963 0.30315971 0.28228122 0.27057517 0.2778542 0.29410562 0.30014846][0.00614827 -0.0040724222 -0.011998844 -0.0067416765 0.028146494 0.096910566 0.18076231 0.25024354 0.28234 0.28055 0.2654165 0.25766444 0.26529059 0.27897316 0.28234211][-0.039963372 -0.051800825 -0.055752512 -0.043007668 -0.0010827866 0.071815781 0.15759197 0.22735703 0.26041394 0.26183167 0.25067845 0.24495956 0.25087208 0.26025984 0.259791]]...]
INFO - root - 2017-12-11 05:19:53.801304: step 12710, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:59m:45s remains)
INFO - root - 2017-12-11 05:19:59.087631: step 12720, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 46h:55m:26s remains)
INFO - root - 2017-12-11 05:20:04.450941: step 12730, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 48h:33m:10s remains)
INFO - root - 2017-12-11 05:20:09.826021: step 12740, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:18m:07s remains)
INFO - root - 2017-12-11 05:20:15.118856: step 12750, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.501 sec/batch; 44h:31m:34s remains)
INFO - root - 2017-12-11 05:20:20.429182: step 12760, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:20m:54s remains)
INFO - root - 2017-12-11 05:20:25.798633: step 12770, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:45m:47s remains)
INFO - root - 2017-12-11 05:20:31.175079: step 12780, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.521 sec/batch; 46h:17m:51s remains)
INFO - root - 2017-12-11 05:20:36.557136: step 12790, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:09m:12s remains)
INFO - root - 2017-12-11 05:20:41.945603: step 12800, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:38m:04s remains)
2017-12-11 05:20:42.478288: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15680049 0.19361772 0.21860041 0.22306682 0.20584865 0.17185283 0.13245018 0.1035926 0.095265776 0.10802776 0.13601714 0.17013837 0.20012733 0.2194981 0.23037033][0.21209431 0.2554352 0.29025331 0.30579591 0.29786721 0.26748616 0.22450806 0.18664937 0.16765134 0.17189667 0.19676758 0.23351304 0.27051032 0.29922378 0.32113418][0.24400528 0.29064682 0.33527279 0.36779222 0.38057178 0.36777082 0.33329353 0.29213291 0.2594209 0.24538228 0.25387061 0.27982536 0.31263044 0.34378371 0.37398857][0.23728247 0.28543749 0.34157735 0.39746162 0.4422591 0.46133995 0.44887117 0.41213644 0.36420947 0.32257408 0.30092123 0.30149764 0.31714058 0.34158206 0.37504902][0.18535502 0.23370674 0.3018195 0.38449997 0.46783113 0.52847552 0.54948086 0.52639329 0.46598107 0.392206 0.33094636 0.29420263 0.28105056 0.28972319 0.32038265][0.10070989 0.14527723 0.22167967 0.32794312 0.4476459 0.55066925 0.61041695 0.60964566 0.54545635 0.44443697 0.34399512 0.26709041 0.22093612 0.20975718 0.23348919][0.011895738 0.047891703 0.12448967 0.24428529 0.39047357 0.52852374 0.62482506 0.6518954 0.59562212 0.47962034 0.34888846 0.23789228 0.16284214 0.13373883 0.1492126][-0.050280727 -0.027622361 0.040182244 0.15918177 0.3150149 0.47234857 0.59377331 0.64580178 0.60560817 0.48952782 0.344027 0.21258362 0.11953693 0.07954473 0.090158947][-0.0736221 -0.065423034 -0.015261582 0.086474791 0.23052236 0.38477871 0.5125398 0.57858145 0.55645239 0.45443568 0.31435147 0.18237433 0.087545164 0.047258675 0.0591412][-0.062304508 -0.066224091 -0.039510041 0.031409007 0.14349775 0.27164844 0.38428169 0.44957766 0.44280934 0.36557657 0.24986978 0.13727269 0.056514442 0.025625559 0.043310136][-0.027359167 -0.040380586 -0.0361173 -5.8740618e-05 0.070801057 0.15984742 0.24298209 0.29530039 0.29798454 0.24828346 0.16601506 0.083307885 0.024974087 0.0081911776 0.032405503][0.0071452889 -0.0084515614 -0.018521989 -0.010667154 0.022119846 0.071127735 0.12047483 0.15323621 0.15817161 0.13098316 0.080103211 0.027143527 -0.0086864093 -0.012687564 0.014436266][0.022723388 0.010061705 -0.0044733137 -0.012742965 -0.0074407021 0.0085743964 0.027357273 0.039466988 0.04067596 0.027503861 0.0012015811 -0.026468884 -0.043007314 -0.03756376 -0.011102259][0.013742842 0.0048197131 -0.0082189031 -0.022004453 -0.031677891 -0.037124772 -0.040028758 -0.043689903 -0.047336131 -0.052981149 -0.062025242 -0.06985908 -0.071175307 -0.059802417 -0.035995051][-0.014140311 -0.019750813 -0.028999669 -0.041633479 -0.055801414 -0.070619345 -0.084066629 -0.095396578 -0.10147055 -0.10324158 -0.10219 -0.097415276 -0.089521088 -0.076318577 -0.056480143]]...]
INFO - root - 2017-12-11 05:20:47.507921: step 12810, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:39m:30s remains)
INFO - root - 2017-12-11 05:20:52.873066: step 12820, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 48h:39m:35s remains)
INFO - root - 2017-12-11 05:20:58.222800: step 12830, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 46h:36m:10s remains)
INFO - root - 2017-12-11 05:21:03.540161: step 12840, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 50h:04m:37s remains)
INFO - root - 2017-12-11 05:21:08.873070: step 12850, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 47h:53m:08s remains)
INFO - root - 2017-12-11 05:21:14.250190: step 12860, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 45h:58m:32s remains)
INFO - root - 2017-12-11 05:21:19.611443: step 12870, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 48h:23m:49s remains)
INFO - root - 2017-12-11 05:21:25.064599: step 12880, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 50h:42m:52s remains)
INFO - root - 2017-12-11 05:21:30.420194: step 12890, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:59m:53s remains)
INFO - root - 2017-12-11 05:21:35.681389: step 12900, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 47h:10m:09s remains)
2017-12-11 05:21:36.251321: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033391032 0.017147955 0.01399151 0.0230684 0.042603079 0.065735891 0.084855549 0.097296454 0.10286842 0.10409185 0.1069866 0.11623429 0.12987331 0.13836774 0.13570331][0.051879037 0.03296322 0.033252623 0.054844171 0.093070909 0.13462006 0.16613349 0.18396744 0.18986158 0.18874507 0.19050798 0.20176329 0.21905589 0.22784437 0.22005339][0.072063543 0.052180205 0.058718767 0.09737841 0.15914252 0.22239408 0.26684988 0.28811744 0.2911109 0.2836476 0.27979285 0.28814888 0.30435112 0.31040603 0.29764995][0.090517446 0.071503334 0.086908065 0.14437562 0.2290872 0.31110048 0.36409378 0.38428366 0.38136339 0.36596677 0.35527503 0.35886732 0.3715851 0.37394518 0.35820472][0.097584255 0.083445817 0.10844583 0.18236324 0.28369832 0.37598166 0.42955124 0.44273731 0.43108404 0.40812245 0.39303991 0.39505526 0.40803787 0.41163084 0.39835823][0.09970139 0.090520985 0.12152347 0.20565498 0.31475794 0.40786913 0.45442975 0.45477974 0.4294053 0.39560592 0.37497419 0.37659162 0.39356613 0.40483806 0.40129489][0.11862323 0.10917415 0.13946174 0.22481254 0.33186597 0.4174839 0.45191103 0.43616709 0.39373288 0.34627607 0.31757221 0.31668478 0.33637479 0.35632649 0.36568981][0.15848941 0.14493172 0.16806975 0.24472599 0.33916861 0.40911049 0.42778456 0.39687228 0.34160575 0.2869015 0.2552751 0.2536172 0.27424565 0.29987323 0.31959206][0.20382243 0.18503584 0.19618092 0.25429776 0.32560322 0.37274912 0.3740803 0.33294198 0.27491754 0.22533268 0.20106584 0.20447211 0.22680046 0.25457844 0.27868053][0.24309072 0.22306842 0.22055876 0.25409219 0.296244 0.31814757 0.30418283 0.2589018 0.20552635 0.16744062 0.15494807 0.16624007 0.19142476 0.21980797 0.24416037][0.2827329 0.26583466 0.25198567 0.26031029 0.27320126 0.27220172 0.24818988 0.20515935 0.16061144 0.1342444 0.13166675 0.1488732 0.17549625 0.20322785 0.22641194][0.3211706 0.30941665 0.2851027 0.26753661 0.25179672 0.23148018 0.20413782 0.17121387 0.14212635 0.1309628 0.13985366 0.16333528 0.19092035 0.21669813 0.23735456][0.35598949 0.34783259 0.3109149 0.26596364 0.22213039 0.18549381 0.15929246 0.14167076 0.13323389 0.14066114 0.16236465 0.19159718 0.2183353 0.23883286 0.25328645][0.39061686 0.38682967 0.33960688 0.27266443 0.20678715 0.15821256 0.13366346 0.12811981 0.13449892 0.15266362 0.17872272 0.20578893 0.22546531 0.23593342 0.24085346][0.41941804 0.42314407 0.37510762 0.30099374 0.2272007 0.17457835 0.15016295 0.14678578 0.15355606 0.16738138 0.18485254 0.20047392 0.20801362 0.20688516 0.20212123]]...]
INFO - root - 2017-12-11 05:21:41.306259: step 12910, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:03m:34s remains)
INFO - root - 2017-12-11 05:21:46.666075: step 12920, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:04m:23s remains)
INFO - root - 2017-12-11 05:21:52.051677: step 12930, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:41m:34s remains)
INFO - root - 2017-12-11 05:21:57.420483: step 12940, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:33m:05s remains)
INFO - root - 2017-12-11 05:22:02.726642: step 12950, loss = 0.71, batch loss = 0.66 (15.6 examples/sec; 0.514 sec/batch; 45h:34m:54s remains)
INFO - root - 2017-12-11 05:22:08.088539: step 12960, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:07m:28s remains)
INFO - root - 2017-12-11 05:22:13.495272: step 12970, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 49h:25m:43s remains)
INFO - root - 2017-12-11 05:22:18.797942: step 12980, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 47h:27m:18s remains)
INFO - root - 2017-12-11 05:22:24.142029: step 12990, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.540 sec/batch; 47h:53m:38s remains)
INFO - root - 2017-12-11 05:22:29.474106: step 13000, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 46h:45m:00s remains)
2017-12-11 05:22:30.000173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070068888 -0.073119834 -0.07235384 -0.070111476 -0.067894332 -0.0673306 -0.069414005 -0.073495641 -0.077900335 -0.081361391 -0.083444804 -0.084489934 -0.084838517 -0.084949851 -0.08476302][-0.074694455 -0.078108832 -0.077909939 -0.076067843 -0.073883317 -0.072707772 -0.073189594 -0.074293338 -0.0746796 -0.073887959 -0.072392248 -0.071754046 -0.073068835 -0.076625034 -0.081029043][-0.076838672 -0.079961739 -0.079476975 -0.076895773 -0.0725815 -0.0675325 -0.062443841 -0.056583248 -0.049475603 -0.042068135 -0.036383253 -0.035505846 -0.041122843 -0.052728251 -0.066657074][-0.078801535 -0.080413319 -0.076640613 -0.068376146 -0.055132058 -0.038210597 -0.019602314 -0.00036242296 0.018152209 0.032816149 0.040070936 0.035885457 0.018944157 -0.0084919548 -0.039415747][-0.082059734 -0.081178613 -0.069860905 -0.04795498 -0.014679555 0.026871271 0.07126537 0.11296118 0.14677645 0.16617255 0.1668895 0.14618616 0.10606033 0.05283333 -0.0019688606][-0.0754506 -0.072888568 -0.053058933 -0.013290383 0.047732659 0.12380256 0.20381609 0.27453592 0.32417327 0.34170881 0.32414228 0.27382082 0.19972901 0.11458738 0.034494][-0.038980052 -0.036714 -0.013768204 0.037782252 0.12156994 0.22852103 0.34123629 0.43711537 0.496061 0.50244659 0.45688435 0.37044665 0.26124 0.14869793 0.051512837][0.035563357 0.034166902 0.0493643 0.096702874 0.18385813 0.30008188 0.42375588 0.52581936 0.58056533 0.56959969 0.49749532 0.38377675 0.25402284 0.13195339 0.035399478][0.14229837 0.13194288 0.1272473 0.1510969 0.21577103 0.31059426 0.4136658 0.49616042 0.53267103 0.50504541 0.41963661 0.30010426 0.17461215 0.0668676 -0.0093647279][0.27065614 0.24805908 0.21496968 0.20074718 0.22205101 0.2705909 0.32813606 0.37228948 0.38306519 0.34515816 0.26386133 0.16108622 0.061478626 -0.015393575 -0.061011616][0.41252694 0.37864649 0.31571442 0.25738949 0.22451879 0.21460864 0.216761 0.21795824 0.20487282 0.16522723 0.10080701 0.02740912 -0.037785668 -0.081359334 -0.099139817][0.53659642 0.4995262 0.41747364 0.32209027 0.23692542 0.16910031 0.11874404 0.08319024 0.054428738 0.021396866 -0.019355126 -0.060684107 -0.09399268 -0.11156162 -0.11101092][0.60642529 0.57630694 0.49145716 0.37565455 0.25258923 0.14055477 0.052328035 -0.0050826264 -0.036567003 -0.054318722 -0.06816797 -0.080628246 -0.0908616 -0.094320394 -0.0868071][0.61761069 0.59711558 0.51810229 0.39667574 0.25538498 0.12140723 0.018214127 -0.039540894 -0.055752855 -0.0469726 -0.031270061 -0.020891748 -0.020615136 -0.024993805 -0.023373216][0.58177519 0.5633738 0.4881708 0.36908984 0.22795661 0.096143164 0.0031331026 -0.032681346 -0.015999367 0.029723635 0.076605335 0.10320686 0.10130329 0.081758045 0.065946229]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:22:35.141888: step 13010, loss = 0.67, batch loss = 0.61 (14.0 examples/sec; 0.571 sec/batch; 50h:41m:34s remains)
INFO - root - 2017-12-11 05:22:40.532806: step 13020, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:49m:38s remains)
INFO - root - 2017-12-11 05:22:45.967180: step 13030, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.568 sec/batch; 50h:25m:58s remains)
INFO - root - 2017-12-11 05:22:51.391722: step 13040, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.576 sec/batch; 51h:07m:16s remains)
INFO - root - 2017-12-11 05:22:56.856091: step 13050, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:39m:42s remains)
INFO - root - 2017-12-11 05:23:02.188707: step 13060, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:57m:44s remains)
INFO - root - 2017-12-11 05:23:07.572800: step 13070, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:59m:04s remains)
INFO - root - 2017-12-11 05:23:13.027129: step 13080, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 47h:44m:53s remains)
INFO - root - 2017-12-11 05:23:18.336140: step 13090, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:19m:44s remains)
INFO - root - 2017-12-11 05:23:23.674132: step 13100, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.528 sec/batch; 46h:49m:27s remains)
2017-12-11 05:23:24.226523: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18260518 0.16720307 0.17622267 0.20599221 0.24108408 0.26484016 0.26748753 0.25254872 0.22866389 0.20409472 0.18487468 0.17429121 0.17771553 0.2028168 0.24519949][0.17478755 0.16089521 0.18156411 0.23265922 0.29150406 0.33302143 0.34205365 0.31945175 0.27522963 0.22103104 0.17002906 0.13377787 0.12551361 0.15298808 0.207046][0.15789221 0.14951719 0.18713595 0.26710147 0.35977682 0.43269539 0.46298471 0.44267842 0.37883762 0.28620103 0.19001088 0.11600249 0.085607074 0.10576203 0.16087213][0.13712467 0.13727053 0.1972229 0.31414241 0.45295444 0.57286924 0.639679 0.63186735 0.551337 0.41657963 0.2687819 0.15147571 0.094130471 0.10038223 0.14771101][0.10523552 0.1119934 0.1933962 0.34893176 0.54000956 0.71507072 0.82538235 0.83451504 0.74118793 0.57037127 0.37915719 0.226408 0.14646134 0.13934304 0.17798184][0.061348468 0.071995445 0.16718617 0.353045 0.58916306 0.812474 0.96022612 0.98490447 0.88536727 0.69279397 0.47465602 0.29952028 0.20428888 0.18767761 0.22186123][0.024850877 0.031950433 0.12678154 0.32232869 0.57906723 0.8272351 0.99593747 1.0337732 0.94064397 0.74895436 0.5254578 0.34105682 0.23603007 0.21193552 0.24328536][0.011968445 0.0081642158 0.08749216 0.26679173 0.51164639 0.75428969 0.9242658 0.97281575 0.89912969 0.73039061 0.52303737 0.34363431 0.23636885 0.20712984 0.23297153][0.020209443 0.0038322755 0.058775593 0.2027791 0.41139221 0.62666821 0.78438908 0.8402887 0.79021281 0.65391362 0.47476369 0.31152582 0.20990284 0.17836848 0.19529898][0.040717851 0.013706231 0.042253222 0.14380957 0.30629155 0.48598415 0.62619096 0.68521285 0.65516043 0.55013329 0.4026601 0.26126564 0.16880876 0.13472947 0.1393588][0.062331021 0.02747374 0.032162532 0.092871211 0.20889194 0.35092378 0.47039571 0.52809048 0.51517522 0.4417761 0.330284 0.21618849 0.13531776 0.096979611 0.086018644][0.071948983 0.034579426 0.022353066 0.0496909 0.1241321 0.22840309 0.32394463 0.37630427 0.37856829 0.33788064 0.2663368 0.1847831 0.11910149 0.078752026 0.054732967][0.067997657 0.034212723 0.014117093 0.019349739 0.062614083 0.13558085 0.21028292 0.2582044 0.27339077 0.258755 0.21862303 0.16327159 0.11082789 0.072530672 0.045002513][0.058505852 0.029180825 0.0055503771 -0.0015923519 0.02450072 0.082051165 0.15108067 0.20549366 0.23518565 0.23592696 0.20800056 0.15916489 0.10618956 0.066430852 0.042714763][0.057591278 0.030796122 0.0037082292 -0.0099046575 0.011671765 0.072192892 0.15485764 0.22972159 0.27633402 0.28201517 0.24595529 0.18047093 0.10826539 0.05711646 0.037552118]]...]
INFO - root - 2017-12-11 05:23:29.291569: step 13110, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 25h:14m:10s remains)
INFO - root - 2017-12-11 05:23:34.683924: step 13120, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:19m:27s remains)
INFO - root - 2017-12-11 05:23:40.145045: step 13130, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:19m:50s remains)
INFO - root - 2017-12-11 05:23:45.447672: step 13140, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.515 sec/batch; 45h:40m:50s remains)
INFO - root - 2017-12-11 05:23:50.801927: step 13150, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.532 sec/batch; 47h:09m:09s remains)
INFO - root - 2017-12-11 05:23:56.161500: step 13160, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:54m:53s remains)
INFO - root - 2017-12-11 05:24:01.530883: step 13170, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:53m:09s remains)
INFO - root - 2017-12-11 05:24:06.901913: step 13180, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:14m:51s remains)
INFO - root - 2017-12-11 05:24:12.219872: step 13190, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:30m:38s remains)
INFO - root - 2017-12-11 05:24:17.658999: step 13200, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 47h:29m:06s remains)
2017-12-11 05:24:18.251336: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058966443 0.075692534 0.084861986 0.08665283 0.084629595 0.079347178 0.073149018 0.069364808 0.063913316 0.057098225 0.049142919 0.047545962 0.053250156 0.059461139 0.06256783][0.13331659 0.16581632 0.18464753 0.19262528 0.19441575 0.18926522 0.18131551 0.17658095 0.16790277 0.15400092 0.13617849 0.12916876 0.1349643 0.14179753 0.14395697][0.22486764 0.28003865 0.31510773 0.33649042 0.34871012 0.34840041 0.34140316 0.33753407 0.32581389 0.3002919 0.26516238 0.24653445 0.24934153 0.25499204 0.25433189][0.30087298 0.38074759 0.43477702 0.47272986 0.49871904 0.507257 0.50489151 0.50402206 0.48968837 0.45082963 0.39602914 0.36428526 0.36362967 0.36902326 0.36527538][0.34182477 0.44076288 0.51030749 0.564065 0.6045779 0.62482631 0.63040817 0.63481885 0.62032181 0.57162273 0.50114125 0.4581902 0.45392823 0.45800665 0.44944662][0.34147048 0.44779417 0.52470344 0.59045 0.64487153 0.67814195 0.69371969 0.70573294 0.6965484 0.64633894 0.56867325 0.51865262 0.510148 0.50908661 0.49180797][0.30448613 0.40686879 0.48317805 0.55510455 0.61874747 0.66071814 0.68351007 0.7032848 0.70520085 0.66343617 0.58978128 0.54145348 0.53218365 0.52560848 0.49785683][0.25211653 0.34473506 0.41329482 0.480572 0.5412401 0.58113629 0.60379887 0.62786418 0.64289385 0.61848795 0.56145167 0.5254125 0.52169544 0.51536787 0.48192191][0.21122578 0.29524836 0.35162246 0.40257543 0.44648087 0.47213191 0.48514688 0.505936 0.53127187 0.52953011 0.49782425 0.48036832 0.48682728 0.48827249 0.45855734][0.20010118 0.28075123 0.32644734 0.35775116 0.37962857 0.38581324 0.38460481 0.39726505 0.42743546 0.44371083 0.43262404 0.4281235 0.44241697 0.45553818 0.43981102][0.20190068 0.28568122 0.32929695 0.34871107 0.35397339 0.34319562 0.32852167 0.331576 0.35986724 0.38304177 0.3808763 0.38087881 0.39885059 0.42402256 0.42697725][0.1920609 0.28171209 0.33222449 0.35105854 0.34976071 0.32925111 0.30462784 0.2963649 0.31335634 0.32877919 0.32251129 0.32065359 0.34108192 0.37938058 0.40242422][0.15073021 0.23892258 0.29315302 0.31339511 0.3104175 0.28591126 0.25461707 0.23379609 0.2335811 0.23403759 0.2191605 0.21343325 0.2351386 0.28330538 0.32236567][0.082390033 0.15307267 0.19713405 0.2111934 0.20452558 0.17844521 0.14382377 0.1130925 0.097904973 0.086785085 0.068170615 0.060710702 0.080517881 0.12859935 0.1729888][0.0023671323 0.044257708 0.068038985 0.070300251 0.058225885 0.03293214 0.00086646277 -0.030496199 -0.050693568 -0.064351074 -0.079188377 -0.08548563 -0.071952343 -0.035799224 0.0013825131]]...]
INFO - root - 2017-12-11 05:24:23.615082: step 13210, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:01m:34s remains)
INFO - root - 2017-12-11 05:24:28.707822: step 13220, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:12m:53s remains)
INFO - root - 2017-12-11 05:24:34.048927: step 13230, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:22m:51s remains)
INFO - root - 2017-12-11 05:24:39.379574: step 13240, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 45h:29m:01s remains)
INFO - root - 2017-12-11 05:24:44.743396: step 13250, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:56m:49s remains)
INFO - root - 2017-12-11 05:24:50.126951: step 13260, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:59m:45s remains)
INFO - root - 2017-12-11 05:24:55.495533: step 13270, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.532 sec/batch; 47h:08m:00s remains)
INFO - root - 2017-12-11 05:25:00.759437: step 13280, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:38m:16s remains)
INFO - root - 2017-12-11 05:25:06.118702: step 13290, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:05m:46s remains)
INFO - root - 2017-12-11 05:25:11.468260: step 13300, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 46h:08m:28s remains)
2017-12-11 05:25:12.013305: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27553859 0.2646493 0.25076881 0.24621531 0.26193544 0.29995143 0.34216586 0.36885124 0.37565589 0.3710424 0.36009091 0.34657553 0.3372542 0.33219728 0.3235586][0.27457634 0.259704 0.24504012 0.24249738 0.26090229 0.30356917 0.3512212 0.38183779 0.39115152 0.38679054 0.37300703 0.35683632 0.35190997 0.360681 0.36972427][0.24422185 0.2242866 0.21092069 0.21490166 0.24196978 0.294055 0.34941763 0.38457909 0.39431292 0.38576651 0.36511421 0.34434012 0.3442851 0.36894137 0.39852163][0.2224655 0.19687226 0.18540095 0.19847919 0.23857908 0.30428886 0.37046474 0.41162673 0.41996303 0.40180919 0.36722812 0.33474734 0.33206412 0.36623985 0.41309249][0.22409625 0.19142753 0.17654504 0.19199976 0.24035752 0.31804213 0.39537519 0.44301102 0.44963178 0.42144772 0.37329262 0.32933444 0.32034457 0.35701409 0.41578522][0.23769523 0.20499574 0.18620008 0.19730443 0.24462913 0.32586017 0.40860438 0.46000692 0.46598911 0.43297067 0.37897268 0.32959169 0.31253707 0.34141126 0.39876482][0.26305255 0.24194948 0.22513996 0.23143162 0.27177095 0.34654316 0.42379144 0.46998554 0.47101611 0.4346301 0.37917995 0.32771376 0.30022213 0.31134233 0.3528282][0.29187378 0.29094788 0.28539026 0.29351443 0.32826477 0.39052889 0.45072356 0.4784134 0.46379486 0.41850567 0.36096144 0.30913031 0.27269423 0.26421127 0.28424117][0.29551032 0.31522292 0.32699287 0.34411642 0.37730861 0.42543057 0.46267018 0.46424529 0.42763644 0.36986828 0.30935949 0.25895649 0.21973577 0.19905451 0.20245005][0.24552715 0.28075394 0.30895942 0.3377735 0.37270668 0.40952492 0.4268916 0.40676016 0.3536858 0.28815669 0.22793424 0.18175001 0.14659506 0.12413884 0.12067789][0.14824659 0.19042432 0.2289198 0.26711085 0.30575934 0.33731151 0.34402624 0.31356618 0.25529563 0.19063541 0.13623084 0.098383263 0.073407017 0.058273412 0.05781645][0.0506773 0.088105485 0.12666762 0.16679245 0.20651689 0.23598057 0.24011613 0.21021703 0.15739295 0.10152002 0.058367878 0.033619516 0.024245709 0.0234307 0.033130813][-0.0097323433 0.015174976 0.043697014 0.076864369 0.11247158 0.14017716 0.14691639 0.12588255 0.087157965 0.047384143 0.020647125 0.012512469 0.021027949 0.036741748 0.057790898][-0.018377818 -0.0060542836 0.0094591212 0.03139459 0.059177287 0.084085084 0.095547959 0.087827638 0.068475068 0.049711838 0.042070452 0.050053984 0.072203718 0.097880729 0.12224672][0.0255773 0.032165483 0.039157502 0.051943634 0.0723613 0.094267808 0.10938024 0.11251391 0.10783681 0.1035458 0.10708388 0.12202734 0.1471916 0.17178719 0.18964857]]...]
INFO - root - 2017-12-11 05:25:17.372289: step 13310, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:34m:00s remains)
INFO - root - 2017-12-11 05:25:22.343711: step 13320, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 48h:47m:18s remains)
INFO - root - 2017-12-11 05:25:27.727456: step 13330, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:13m:36s remains)
INFO - root - 2017-12-11 05:25:33.038776: step 13340, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:18m:56s remains)
INFO - root - 2017-12-11 05:25:38.406660: step 13350, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:39m:26s remains)
INFO - root - 2017-12-11 05:25:43.793336: step 13360, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 46h:35m:20s remains)
INFO - root - 2017-12-11 05:25:49.049390: step 13370, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 47h:17m:47s remains)
INFO - root - 2017-12-11 05:25:54.471006: step 13380, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 47h:22m:24s remains)
INFO - root - 2017-12-11 05:25:59.787200: step 13390, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 46h:39m:43s remains)
INFO - root - 2017-12-11 05:26:05.098125: step 13400, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 46h:16m:12s remains)
2017-12-11 05:26:05.625638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066767432 -0.05632323 -0.027211411 0.021449791 0.090326995 0.17900059 0.28381619 0.38571978 0.46440014 0.50572741 0.5020014 0.44560152 0.3445597 0.22586693 0.1100056][-0.069588959 -0.05652694 -0.021685777 0.035333786 0.11436889 0.21342146 0.32892933 0.4389576 0.51926523 0.55197722 0.52938962 0.44677565 0.3177872 0.17699742 0.052167468][-0.068404831 -0.05344858 -0.015420655 0.046070904 0.13004266 0.23230766 0.348797 0.4571183 0.53193963 0.55393291 0.515738 0.41559193 0.27027136 0.11875927 -0.0060680085][-0.065943681 -0.049210735 -0.008240357 0.057536043 0.14553183 0.2479645 0.35857478 0.45699593 0.52069229 0.53244215 0.48554647 0.37950978 0.2310586 0.079359077 -0.040989529][-0.06207953 -0.04417618 -0.000741272 0.069555253 0.16211951 0.26463586 0.36640736 0.44946694 0.49628314 0.49502638 0.44131526 0.33610335 0.19419622 0.050877396 -0.060521476][-0.058325749 -0.039536446 0.0061557316 0.081563577 0.17998289 0.28491136 0.38004792 0.44869947 0.4784368 0.46319717 0.40328705 0.30223653 0.17235743 0.041592263 -0.060137179][-0.055702396 -0.036614839 0.011748399 0.093842149 0.20171213 0.31567219 0.4141005 0.47877902 0.49859971 0.47013423 0.39957353 0.29599085 0.17162244 0.048284229 -0.047483888][-0.056769352 -0.038834207 0.010682031 0.09782587 0.21442637 0.33889678 0.44528222 0.51211333 0.52673841 0.48574984 0.40088472 0.2884109 0.16348106 0.044653095 -0.044572521][-0.064391062 -0.050610766 -0.0043120882 0.081620574 0.19904365 0.32493672 0.43056637 0.49278793 0.49902859 0.44731569 0.35384956 0.23980913 0.12229938 0.016881058 -0.057317935][-0.078544468 -0.072385825 -0.034818474 0.041570742 0.149414 0.26563683 0.36033836 0.41055909 0.4058426 0.347469 0.2542907 0.14963497 0.050813116 -0.030377286 -0.080975145][-0.094608925 -0.09777353 -0.0733913 -0.014476266 0.073878162 0.17189866 0.25130638 0.28962177 0.2774004 0.21821125 0.1313895 0.040971979 -0.035035882 -0.08807186 -0.11227874][-0.10889154 -0.12271323 -0.11499321 -0.080225341 -0.020472007 0.050701529 0.11034384 0.13802154 0.1242044 0.073042989 0.0020292664 -0.067124516 -0.11755306 -0.14321662 -0.14395832][-0.1188778 -0.14197266 -0.15115742 -0.14261805 -0.11627871 -0.079361558 -0.047115032 -0.034639671 -0.048862081 -0.084396623 -0.12763336 -0.16402262 -0.18300284 -0.18198426 -0.16395755][-0.11982089 -0.14764129 -0.16821879 -0.1794917 -0.18047416 -0.17381111 -0.1675151 -0.17007749 -0.18413904 -0.20331372 -0.21838818 -0.22302012 -0.21469919 -0.19432262 -0.16546397][-0.111389 -0.13921922 -0.16424775 -0.18592827 -0.2024401 -0.21327184 -0.22133821 -0.23036632 -0.24091244 -0.24777019 -0.24534807 -0.23206052 -0.20979843 -0.1816479 -0.15098283]]...]
INFO - root - 2017-12-11 05:26:10.981537: step 13410, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:32m:01s remains)
INFO - root - 2017-12-11 05:26:16.146208: step 13420, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:40m:37s remains)
INFO - root - 2017-12-11 05:26:21.661687: step 13430, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:40m:15s remains)
INFO - root - 2017-12-11 05:26:26.989005: step 13440, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:27m:28s remains)
INFO - root - 2017-12-11 05:26:32.260799: step 13450, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:26m:11s remains)
INFO - root - 2017-12-11 05:26:37.640714: step 13460, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 47h:43m:45s remains)
INFO - root - 2017-12-11 05:26:43.059406: step 13470, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:36m:23s remains)
INFO - root - 2017-12-11 05:26:48.362747: step 13480, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:54m:27s remains)
INFO - root - 2017-12-11 05:26:53.661982: step 13490, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:19m:23s remains)
INFO - root - 2017-12-11 05:26:59.035331: step 13500, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:16m:40s remains)
2017-12-11 05:26:59.680096: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045090418 0.04021889 0.033421014 0.029263094 0.029762121 0.0367402 0.048416987 0.059643786 0.071589179 0.0861908 0.10388944 0.11863901 0.12560864 0.1254199 0.11532196][0.062381815 0.0556965 0.047263872 0.04313422 0.04594031 0.057478521 0.0751348 0.091277041 0.10726547 0.12666048 0.15111461 0.17267783 0.18328682 0.18264438 0.1675306][0.077604175 0.0687363 0.05902173 0.05540178 0.061306581 0.078176275 0.10221999 0.12317359 0.14207673 0.16383505 0.19169457 0.21716669 0.22963832 0.22802515 0.20819435][0.083746135 0.073994324 0.06431739 0.06188149 0.070879094 0.092412174 0.12163756 0.14611198 0.16618046 0.18742909 0.21457718 0.23987231 0.25137225 0.24723664 0.22283314][0.085406736 0.077599645 0.07026466 0.070044637 0.081318244 0.10479765 0.13553245 0.16019194 0.17893873 0.19772303 0.22236738 0.246266 0.2562505 0.24933662 0.22107194][0.0856575 0.082841411 0.079668976 0.081994452 0.094323464 0.1166036 0.14389326 0.16373615 0.17722975 0.19098476 0.21137743 0.23328739 0.24254052 0.23469004 0.20537941][0.087675385 0.090419561 0.091341466 0.095426805 0.10768392 0.12731639 0.1492175 0.16206858 0.16797514 0.17395927 0.18625893 0.20198141 0.2078393 0.19947046 0.17220461][0.095845953 0.1025898 0.10508153 0.10817906 0.11788762 0.13337308 0.15001875 0.15727469 0.15693451 0.15504973 0.15701889 0.16284931 0.16219914 0.15247804 0.12870546][0.11440627 0.1236733 0.12636234 0.12672658 0.13148046 0.1402939 0.15023017 0.15254921 0.14808311 0.14106251 0.13500489 0.13217662 0.12567669 0.114491 0.0936122][0.14537986 0.15694633 0.16018009 0.15774226 0.15659718 0.15690675 0.15814406 0.15406656 0.14551453 0.13564856 0.12575375 0.11883441 0.11002135 0.098687612 0.079359092][0.17560744 0.18694487 0.18955454 0.18449448 0.17858444 0.17185533 0.16528468 0.15474048 0.14200176 0.13009891 0.11883496 0.11128312 0.10344891 0.094045341 0.076443866][0.18245704 0.19003044 0.19082624 0.18483351 0.17809066 0.16992415 0.16085714 0.14741841 0.13257533 0.11997898 0.10918395 0.1028126 0.097048968 0.089817278 0.073429585][0.1622021 0.16376612 0.16203627 0.1564744 0.15177317 0.14636204 0.13943662 0.12718444 0.11328286 0.10234266 0.094120674 0.090214513 0.086385913 0.080431648 0.064362817][0.12716302 0.12230141 0.11759955 0.11174602 0.10848521 0.10561582 0.10144606 0.092056826 0.081703618 0.075207882 0.071600482 0.0708371 0.068241015 0.062390957 0.046192866][0.086532757 0.077635 0.070312746 0.062769912 0.058610145 0.05612978 0.053376865 0.046682633 0.040834829 0.039860878 0.0411835 0.04301374 0.040802531 0.034445755 0.01869303]]...]
INFO - root - 2017-12-11 05:27:05.079468: step 13510, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:05m:38s remains)
INFO - root - 2017-12-11 05:27:10.369877: step 13520, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:55m:48s remains)
INFO - root - 2017-12-11 05:27:15.291120: step 13530, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 45h:46m:07s remains)
INFO - root - 2017-12-11 05:27:20.728897: step 13540, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:02m:22s remains)
INFO - root - 2017-12-11 05:27:26.053962: step 13550, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:28m:29s remains)
INFO - root - 2017-12-11 05:27:31.327751: step 13560, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:10m:47s remains)
INFO - root - 2017-12-11 05:27:36.698128: step 13570, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:55m:35s remains)
INFO - root - 2017-12-11 05:27:42.116816: step 13580, loss = 0.67, batch loss = 0.62 (14.2 examples/sec; 0.563 sec/batch; 49h:52m:52s remains)
INFO - root - 2017-12-11 05:27:47.515535: step 13590, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:18m:35s remains)
INFO - root - 2017-12-11 05:27:52.936890: step 13600, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:43m:38s remains)
2017-12-11 05:27:53.463363: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49132809 0.57316953 0.60103494 0.59072578 0.54083776 0.46564704 0.41292012 0.41298652 0.4656961 0.54209328 0.62149858 0.68038857 0.6829263 0.61490756 0.49992746][0.46543461 0.54959524 0.58501661 0.58665568 0.55490887 0.50063473 0.46481135 0.46958488 0.51402783 0.57163894 0.624226 0.65474695 0.63364422 0.55741286 0.4494485][0.42025191 0.51021284 0.56037241 0.58115125 0.5756582 0.55143 0.53790212 0.54573071 0.57036549 0.59395444 0.60332656 0.59118682 0.54353154 0.46850222 0.3850179][0.39928466 0.50593251 0.5799799 0.62484574 0.64722109 0.65377462 0.66268462 0.67259288 0.67541045 0.66424716 0.62849957 0.56963855 0.49399087 0.42388052 0.36918938][0.40702981 0.54128873 0.64791071 0.7217654 0.77272171 0.80764693 0.8348487 0.84363645 0.82349288 0.77895391 0.69943094 0.59360087 0.49000892 0.4234764 0.39381856][0.42448691 0.58812279 0.72813916 0.83246613 0.91218913 0.97280782 1.0139484 1.0201727 0.97988361 0.906648 0.78897744 0.64100844 0.51073837 0.4410437 0.42557111][0.41781336 0.597478 0.75886571 0.88650441 0.98973751 1.0694637 1.11868 1.1221516 1.068817 0.97677642 0.83471739 0.660385 0.51209843 0.43494651 0.42335317][0.3706435 0.53922623 0.69549841 0.82596767 0.93729138 1.0245477 1.0761846 1.0789695 1.0223072 0.92431241 0.77725261 0.60160536 0.45383775 0.37398794 0.36190045][0.30541959 0.43803379 0.5617249 0.66997319 0.76784474 0.84745872 0.89565837 0.89995658 0.84769195 0.75382608 0.61817491 0.46370777 0.33599579 0.26460895 0.25482491][0.25731015 0.34270284 0.41741636 0.48501649 0.55176938 0.61159837 0.65225857 0.65981936 0.61716932 0.53373706 0.41788766 0.29479665 0.19693811 0.14289331 0.14024659][0.25259617 0.29765284 0.32377377 0.34459084 0.36989313 0.3998256 0.426289 0.43378663 0.40055159 0.33010995 0.23691753 0.14632505 0.079252206 0.046605267 0.055015087][0.29896843 0.32626864 0.32226405 0.30671653 0.29263407 0.28731897 0.28931266 0.28421468 0.24937916 0.18573846 0.11092528 0.047081016 0.00572765 -0.0062103388 0.014830643][0.37951949 0.41499636 0.40847185 0.38166508 0.34747913 0.316629 0.29268554 0.2641708 0.21233809 0.1402287 0.068693906 0.01587983 -0.013042443 -0.012600934 0.016952012][0.45366523 0.51349789 0.52568656 0.51087683 0.47859594 0.43809578 0.39496994 0.34152871 0.26591092 0.17548153 0.094923384 0.040349878 0.012993569 0.017448114 0.049674455][0.48592073 0.57042563 0.60622466 0.61110467 0.5900861 0.54835474 0.49225131 0.41953796 0.32654139 0.22293171 0.13457061 0.077761322 0.05171689 0.058549348 0.090587758]]...]
INFO - root - 2017-12-11 05:27:58.771745: step 13610, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 48h:00m:54s remains)
INFO - root - 2017-12-11 05:28:04.094339: step 13620, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 47h:43m:38s remains)
INFO - root - 2017-12-11 05:28:09.128634: step 13630, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 47h:16m:34s remains)
INFO - root - 2017-12-11 05:28:14.514149: step 13640, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:04m:06s remains)
INFO - root - 2017-12-11 05:28:19.901116: step 13650, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 46h:10m:41s remains)
INFO - root - 2017-12-11 05:28:25.253907: step 13660, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:48m:50s remains)
INFO - root - 2017-12-11 05:28:30.642138: step 13670, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 46h:08m:21s remains)
INFO - root - 2017-12-11 05:28:35.907531: step 13680, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 47h:18m:33s remains)
INFO - root - 2017-12-11 05:28:41.318415: step 13690, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 47h:38m:38s remains)
INFO - root - 2017-12-11 05:28:46.740292: step 13700, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:55m:28s remains)
2017-12-11 05:28:47.310786: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19609106 0.23020209 0.24902424 0.25533924 0.25168669 0.23585778 0.21260938 0.18815131 0.17004962 0.163428 0.16188078 0.16140415 0.16433363 0.18398768 0.22138163][0.26921454 0.301455 0.31147555 0.30560011 0.28930941 0.26519105 0.23907493 0.21471786 0.19696164 0.19029206 0.18866202 0.19040377 0.20209619 0.23596714 0.29024106][0.32955158 0.35151353 0.34591451 0.32404315 0.29478884 0.26390272 0.23675962 0.21373816 0.19646628 0.18908848 0.18738346 0.19353364 0.21775147 0.26800996 0.33439833][0.3598612 0.36762768 0.34584934 0.31204829 0.27763864 0.24862622 0.22746846 0.21044941 0.19514023 0.18515348 0.18021861 0.18683644 0.21800554 0.27618393 0.34371907][0.35150439 0.34725261 0.31490412 0.27722773 0.24790537 0.23149504 0.22663692 0.22447337 0.21739483 0.20775667 0.19937453 0.20183387 0.2287834 0.2791841 0.33316919][0.30815092 0.29871345 0.26597586 0.23481408 0.22024149 0.22439131 0.24157111 0.25729498 0.25931591 0.25056708 0.2394083 0.23654029 0.25310609 0.28603327 0.31989184][0.25241098 0.24330525 0.21808527 0.20006394 0.20425132 0.22942677 0.26509658 0.29200846 0.29521367 0.28189179 0.26639456 0.26105216 0.27214125 0.29362684 0.31621614][0.21151543 0.20534334 0.1884214 0.18130665 0.19824904 0.23511815 0.27787691 0.30473036 0.29997826 0.27638137 0.25404876 0.24919388 0.26371726 0.28798506 0.31593633][0.18955441 0.18619193 0.174238 0.17077293 0.18851651 0.22244701 0.25845614 0.27551681 0.25927728 0.22525297 0.19596232 0.19193275 0.21380882 0.25005659 0.29405287][0.17740715 0.17500341 0.16346 0.15681981 0.16556609 0.18558003 0.20519125 0.20741162 0.18169451 0.14285786 0.11137132 0.1090382 0.1374065 0.18516678 0.24299823][0.16746488 0.16281568 0.14738949 0.13451858 0.13306153 0.13968424 0.14479642 0.13672036 0.10955852 0.074375719 0.046380341 0.045512587 0.074974418 0.12509185 0.18377851][0.16304685 0.15335672 0.13179059 0.11265298 0.10457637 0.10503618 0.10591118 0.099759817 0.083154708 0.060932789 0.041450143 0.040792588 0.063853092 0.10378556 0.14826283][0.16238816 0.14875372 0.12441311 0.10435331 0.096203037 0.098121554 0.10401835 0.10982677 0.1115198 0.10683681 0.096705467 0.093258582 0.10289156 0.12209625 0.1419749][0.16032957 0.1469043 0.12803291 0.11653074 0.11537373 0.12278613 0.13631654 0.1561223 0.17585579 0.18664293 0.18267456 0.17340162 0.16661893 0.16187 0.15559882][0.15741859 0.15090089 0.14705209 0.15261707 0.16290958 0.17621848 0.1946464 0.22303642 0.25304979 0.27196589 0.26984254 0.25460121 0.23483893 0.21240751 0.18803969]]...]
INFO - root - 2017-12-11 05:28:52.684067: step 13710, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:25m:34s remains)
INFO - root - 2017-12-11 05:28:58.048856: step 13720, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 51h:13m:06s remains)
INFO - root - 2017-12-11 05:29:03.163392: step 13730, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:39m:26s remains)
INFO - root - 2017-12-11 05:29:08.499563: step 13740, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:34m:16s remains)
INFO - root - 2017-12-11 05:29:13.861316: step 13750, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:14m:34s remains)
INFO - root - 2017-12-11 05:29:19.122615: step 13760, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 46h:23m:41s remains)
INFO - root - 2017-12-11 05:29:24.440649: step 13770, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:10m:07s remains)
INFO - root - 2017-12-11 05:29:29.799027: step 13780, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 47h:57m:37s remains)
INFO - root - 2017-12-11 05:29:35.178797: step 13790, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:54m:19s remains)
INFO - root - 2017-12-11 05:29:40.466570: step 13800, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:26m:50s remains)
2017-12-11 05:29:41.004479: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036341242 0.066340566 0.086133145 0.093051068 0.096014082 0.099370517 0.10078599 0.09850765 0.089610234 0.077319533 0.066623673 0.063380584 0.067141078 0.065709382 0.050297059][0.095644355 0.14537641 0.179758 0.19667108 0.2062235 0.21190111 0.21119843 0.20240577 0.18157539 0.15533142 0.13176721 0.11988685 0.1188083 0.11255255 0.089919016][0.16469279 0.23769557 0.2910699 0.32329541 0.34400123 0.35336384 0.34779072 0.32779455 0.2907536 0.24550937 0.20184404 0.17352934 0.1629592 0.15250303 0.12742046][0.2150456 0.31083438 0.385675 0.43760774 0.47302806 0.48684964 0.47387135 0.43869811 0.38276303 0.31653047 0.24941367 0.20084235 0.17943561 0.16797212 0.14695008][0.24714814 0.36478791 0.46399841 0.53937918 0.59152532 0.60938531 0.58670861 0.53292465 0.4549703 0.36647689 0.27685034 0.20996496 0.18050078 0.17130555 0.15756018][0.25768483 0.39524037 0.5204187 0.62190163 0.69204825 0.71343851 0.67981118 0.60508621 0.50270885 0.39098939 0.2819894 0.20311314 0.17265376 0.17071265 0.16582657][0.23039797 0.37706348 0.52211517 0.64702648 0.735364 0.76224077 0.72070986 0.62898386 0.50923991 0.38479769 0.27001181 0.19264263 0.17089173 0.17999421 0.18161397][0.16866127 0.31187966 0.4664475 0.60775751 0.71117258 0.74437928 0.69937336 0.60050786 0.48225573 0.37126172 0.27741042 0.22138196 0.21610914 0.23168391 0.2256846][0.10180071 0.23634902 0.39237025 0.54034245 0.647628 0.67816693 0.62673587 0.52755994 0.42756498 0.35385463 0.30444255 0.28550449 0.2984955 0.30761635 0.27357832][0.052654728 0.17932466 0.3328943 0.47797003 0.5746392 0.588225 0.52269691 0.42415705 0.3486045 0.32126552 0.326586 0.35098049 0.37851942 0.36853012 0.29449317][0.0083703157 0.12471534 0.26899725 0.40060139 0.47662452 0.46854296 0.39111531 0.29893529 0.25134775 0.26935539 0.32456678 0.38481855 0.41829047 0.38466257 0.27511078][-0.042040393 0.057758458 0.18354654 0.29180411 0.34223622 0.3167851 0.23942906 0.16714564 0.15549122 0.21644272 0.30960426 0.38947666 0.41615725 0.35805008 0.22479674][-0.084221117 -0.0030364648 0.098502569 0.17668021 0.19800819 0.15663871 0.083369754 0.032956682 0.052140832 0.14138232 0.251506 0.33085188 0.34143561 0.26697382 0.1321875][-0.10923319 -0.045705408 0.030663555 0.078626715 0.072884932 0.017231477 -0.054767571 -0.094112873 -0.065023385 0.024048585 0.12385656 0.18666795 0.18441443 0.11467617 0.0073115849][-0.12452303 -0.078571975 -0.02575306 -0.0013328324 -0.022389866 -0.080686875 -0.14580742 -0.17917359 -0.15736265 -0.091731265 -0.020395944 0.022321863 0.019512873 -0.025643777 -0.089857332]]...]
INFO - root - 2017-12-11 05:29:46.331677: step 13810, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 47h:41m:07s remains)
INFO - root - 2017-12-11 05:29:51.722526: step 13820, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:51m:56s remains)
INFO - root - 2017-12-11 05:29:56.860072: step 13830, loss = 0.69, batch loss = 0.63 (23.0 examples/sec; 0.348 sec/batch; 30h:49m:14s remains)
INFO - root - 2017-12-11 05:30:01.496650: step 13840, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:28m:23s remains)
INFO - root - 2017-12-11 05:30:06.823714: step 13850, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:54m:24s remains)
INFO - root - 2017-12-11 05:30:12.251834: step 13860, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.531 sec/batch; 47h:01m:41s remains)
INFO - root - 2017-12-11 05:30:17.535576: step 13870, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 45h:54m:25s remains)
INFO - root - 2017-12-11 05:30:22.960514: step 13880, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:58m:36s remains)
INFO - root - 2017-12-11 05:30:28.308888: step 13890, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 46h:56m:58s remains)
INFO - root - 2017-12-11 05:30:33.680564: step 13900, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 46h:01m:01s remains)
2017-12-11 05:30:34.266604: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038508188 0.061170954 0.072286122 0.0657559 0.040445559 0.0061355135 -0.025461795 -0.047499232 -0.056801472 -0.055374041 -0.046513867 -0.036577493 -0.031961396 -0.033036638 -0.039672349][0.06008666 0.082587272 0.094173849 0.087846823 0.061412569 0.02515211 -0.0085750818 -0.030857727 -0.03909431 -0.037014306 -0.02895562 -0.021928193 -0.020873098 -0.026834352 -0.037960924][0.088442363 0.11084735 0.12362452 0.11974096 0.096132919 0.062587619 0.030690221 0.0097772814 0.0013237534 0.0010637779 0.0045069009 0.0056981375 0.0014207592 -0.01005158 -0.026044097][0.10616239 0.13150801 0.1517036 0.15787238 0.14452018 0.11953438 0.092787027 0.0725899 0.059180751 0.049959879 0.0429889 0.034842562 0.023961917 0.0077874511 -0.011801963][0.1067505 0.137653 0.16993253 0.19253522 0.19671519 0.18816096 0.1742384 0.15838026 0.13796988 0.11408187 0.090175651 0.06764587 0.04670335 0.024592256 0.0020199453][0.095474966 0.13182472 0.17695683 0.21827723 0.24460627 0.25975934 0.26855525 0.26516581 0.24065565 0.20007595 0.15422045 0.11144965 0.074328743 0.041673496 0.014400151][0.075855352 0.11492896 0.16898033 0.22596358 0.27445009 0.31685835 0.35563344 0.37315977 0.35152856 0.29822305 0.23093888 0.16566539 0.10763466 0.059453353 0.024846254][0.054324977 0.090795904 0.14502969 0.20847654 0.27185157 0.33679023 0.40366408 0.44519341 0.43445611 0.37860456 0.29903403 0.21660689 0.13898744 0.073931463 0.030446352][0.047556095 0.075902358 0.11853661 0.17372397 0.23648469 0.30866411 0.39019063 0.44998387 0.45515347 0.40922803 0.3319501 0.24449204 0.1560767 0.079176255 0.028901827][0.0681326 0.087425694 0.11026723 0.14379571 0.18784098 0.24636869 0.32176006 0.38659427 0.40566167 0.37651789 0.31360379 0.23443516 0.14879911 0.071172543 0.020859368][0.11322555 0.12788694 0.13118124 0.13818805 0.15132245 0.1780784 0.22688498 0.28037533 0.30497885 0.29116315 0.24772584 0.18719825 0.11823422 0.053438861 0.01234282][0.16502072 0.18251334 0.17543463 0.16224015 0.14397989 0.131671 0.14243777 0.17158301 0.19067381 0.18483445 0.15929221 0.12222387 0.078888476 0.036596354 0.011273484][0.1936945 0.21950702 0.21434802 0.19396472 0.15482591 0.1108279 0.087224342 0.09080378 0.099109724 0.094443895 0.08093404 0.063889593 0.044662286 0.024517918 0.014187314][0.1746591 0.20764804 0.21148357 0.19508742 0.14982967 0.090650804 0.047137011 0.033744708 0.03366784 0.028564092 0.021893358 0.017172096 0.013391499 0.0081559578 0.0077238735][0.10150722 0.13380389 0.14492881 0.13663861 0.097436689 0.040770084 -0.0054529631 -0.024406523 -0.027509606 -0.031119768 -0.032760348 -0.03082874 -0.027510419 -0.025976857 -0.022622166]]...]
INFO - root - 2017-12-11 05:30:39.719768: step 13910, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:35m:05s remains)
INFO - root - 2017-12-11 05:30:45.024109: step 13920, loss = 0.69, batch loss = 0.64 (15.6 examples/sec; 0.511 sec/batch; 45h:15m:45s remains)
INFO - root - 2017-12-11 05:30:50.514417: step 13930, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.561 sec/batch; 49h:40m:50s remains)
INFO - root - 2017-12-11 05:30:55.670409: step 13940, loss = 0.69, batch loss = 0.63 (13.6 examples/sec; 0.586 sec/batch; 51h:53m:18s remains)
INFO - root - 2017-12-11 05:31:01.064202: step 13950, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:07m:23s remains)
INFO - root - 2017-12-11 05:31:06.440946: step 13960, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 47h:02m:53s remains)
INFO - root - 2017-12-11 05:31:11.807921: step 13970, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 46h:08m:25s remains)
INFO - root - 2017-12-11 05:31:17.050496: step 13980, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:56m:12s remains)
INFO - root - 2017-12-11 05:31:22.422309: step 13990, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 48h:41m:30s remains)
INFO - root - 2017-12-11 05:31:27.747344: step 14000, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 46h:34m:55s remains)
2017-12-11 05:31:28.309392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016784022 -0.014851676 -0.011412733 -0.0077276691 -0.0070714955 -0.010248131 -0.015199935 -0.020206895 -0.024770586 -0.027319867 -0.026524354 -0.023341306 -0.02040753 -0.020649936 -0.024868941][0.011344605 0.02022841 0.032853946 0.045899361 0.051774256 0.049116142 0.041534334 0.031417497 0.018689504 0.0093895188 0.0077687027 0.012060341 0.016855735 0.016321221 0.0089186421][0.049543422 0.070820451 0.10070754 0.1314422 0.14851014 0.14986506 0.14122182 0.12498365 0.09910512 0.077555135 0.070484489 0.07567247 0.082720213 0.080803864 0.067839578][0.0830607 0.12189422 0.17623985 0.23147964 0.26588315 0.27701724 0.27203432 0.25096938 0.20867236 0.17095405 0.15692811 0.1633233 0.17186493 0.16594982 0.14409125][0.10296023 0.16082557 0.24076694 0.32150325 0.37650764 0.40316153 0.4075444 0.38565105 0.32892573 0.27686349 0.25756904 0.26544353 0.27302104 0.25908193 0.22510597][0.10990279 0.18299517 0.28179976 0.38200521 0.45542148 0.49944264 0.51776689 0.50211483 0.43997726 0.38125393 0.36102182 0.37090033 0.37442163 0.34689248 0.29652596][0.12123601 0.19862193 0.30029383 0.40353128 0.48409507 0.53945023 0.57121217 0.56812519 0.51453167 0.46304879 0.45125285 0.46749222 0.46731076 0.42308369 0.35241157][0.1650852 0.23451698 0.31666476 0.39665616 0.46236679 0.51414877 0.55212992 0.5635854 0.53199464 0.5044049 0.51387513 0.54390776 0.544226 0.48609579 0.39611867][0.26445624 0.31750265 0.36155647 0.39428994 0.4224079 0.45276839 0.48554564 0.50809777 0.504096 0.50794917 0.54169804 0.58415079 0.58540088 0.51952708 0.41728324][0.40027535 0.43880519 0.44122651 0.42134932 0.40474203 0.40512344 0.42485482 0.45183346 0.4690021 0.49518856 0.54077977 0.58460891 0.5839417 0.51646847 0.41018867][0.52519822 0.55702955 0.53038192 0.47259203 0.42323837 0.39983332 0.40681049 0.43046743 0.45298085 0.48231196 0.52230018 0.55618715 0.54962027 0.48316151 0.37771797][0.59123904 0.62158388 0.58274359 0.50837946 0.44696787 0.4157922 0.41708103 0.4320499 0.44325346 0.45679674 0.47615451 0.49191722 0.47705394 0.4150956 0.31651673][0.55775267 0.58619988 0.54766268 0.47582692 0.42285532 0.40262654 0.410155 0.41860026 0.41095594 0.39749733 0.38581538 0.37611508 0.34992349 0.29584694 0.21296215][0.4064593 0.42822 0.39748269 0.3434557 0.31415182 0.31776562 0.34173352 0.35255659 0.33251274 0.295568 0.2538923 0.21756543 0.17811127 0.13099909 0.068594344][0.1788075 0.18851377 0.16659983 0.13512732 0.13049869 0.15520988 0.19388437 0.21343926 0.19619817 0.15496583 0.10229461 0.053230971 0.0072915922 -0.033942353 -0.07724268]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:31:33.665972: step 14010, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:52m:57s remains)
INFO - root - 2017-12-11 05:31:38.988536: step 14020, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:08m:27s remains)
INFO - root - 2017-12-11 05:31:44.287456: step 14030, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 49h:08m:00s remains)
INFO - root - 2017-12-11 05:31:49.446363: step 14040, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 45h:49m:19s remains)
INFO - root - 2017-12-11 05:31:54.805056: step 14050, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:50m:46s remains)
INFO - root - 2017-12-11 05:32:00.145172: step 14060, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:26m:24s remains)
INFO - root - 2017-12-11 05:32:05.455634: step 14070, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 45h:37m:46s remains)
INFO - root - 2017-12-11 05:32:10.833674: step 14080, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:03m:36s remains)
INFO - root - 2017-12-11 05:32:16.195891: step 14090, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:58m:09s remains)
INFO - root - 2017-12-11 05:32:21.489627: step 14100, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:30m:17s remains)
2017-12-11 05:32:22.129149: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071583733 0.063660093 0.053249378 0.04279159 0.034663234 0.029761616 0.031067591 0.037425194 0.042727407 0.043328919 0.039633211 0.035784602 0.034770429 0.036918696 0.041586038][0.067099743 0.060798749 0.05208141 0.044442635 0.039126948 0.035800669 0.036310524 0.039497528 0.041287646 0.038866177 0.032965273 0.027725652 0.025879277 0.027299726 0.030962149][0.058825992 0.05515866 0.049538683 0.045984861 0.044704828 0.044234272 0.0454215 0.047133386 0.045867708 0.039831795 0.031029379 0.023466507 0.019111749 0.017672997 0.018690312][0.046123262 0.045122541 0.043328319 0.044328161 0.047713455 0.0513321 0.055045813 0.057374828 0.054975789 0.046490777 0.034859434 0.024280738 0.016513707 0.011433618 0.0091971112][0.028996252 0.030558476 0.032710012 0.038198747 0.046433628 0.054853708 0.062156588 0.066098884 0.063444875 0.053240366 0.03913163 0.025713559 0.014947629 0.0070004617 0.0024351473][0.013487938 0.01661209 0.021759262 0.030627729 0.042742573 0.055552151 0.0664366 0.072000511 0.069142982 0.057712529 0.041840971 0.026395673 0.013679545 0.0042633843 -0.0010877009][0.0028658258 0.0066816793 0.01350922 0.024686081 0.039872758 0.056653589 0.070988968 0.078206412 0.075344838 0.063386731 0.046618562 0.029813955 0.015547942 0.0050017056 -0.00078829867][-0.0030701237 0.00069151883 0.0085488372 0.021911154 0.040441144 0.061409827 0.079238415 0.088082872 0.08532016 0.073141806 0.055494409 0.036861867 0.020203341 0.0076689464 0.00077069574][-0.0057267305 -0.0020805036 0.0068924734 0.022939783 0.045473062 0.0709386 0.09219785 0.10243314 0.099528953 0.087134659 0.068597086 0.047974985 0.0286452 0.013922582 0.0058702235][-0.007823824 -0.0042813323 0.0059306147 0.02469391 0.050925843 0.080081992 0.10383834 0.11481462 0.11173773 0.099829443 0.081823841 0.060931209 0.040494408 0.024714928 0.016042668][-0.0089033172 -0.0057856715 0.0046877805 0.024283223 0.05156289 0.081428811 0.10530423 0.1159983 0.11330266 0.1035472 0.088765658 0.070728593 0.052059688 0.037171632 0.028559865][-0.0079328045 -0.0057309307 0.0034927598 0.021416606 0.046443056 0.073451392 0.094643466 0.10391703 0.10197817 0.095527984 0.08587978 0.07303565 0.058324475 0.045733582 0.037494406][-0.0021183835 -0.0020493891 0.0034904387 0.016459398 0.035576981 0.056456976 0.072942019 0.080507189 0.080177635 0.077941693 0.074272446 0.067567185 0.057790078 0.048245616 0.040818751][0.010914708 0.0074188206 0.0069804387 0.012222648 0.022518415 0.035000756 0.045774903 0.051967245 0.054229923 0.056741141 0.058996208 0.058288995 0.05347053 0.04707722 0.040748663][0.027064454 0.020021 0.013954587 0.012057555 0.014013031 0.0186684 0.024419954 0.029833432 0.034523431 0.040274516 0.045948863 0.048793368 0.047159966 0.042868257 0.037471477]]...]
INFO - root - 2017-12-11 05:32:27.458943: step 14110, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 45h:51m:00s remains)
INFO - root - 2017-12-11 05:32:32.804946: step 14120, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:50m:15s remains)
INFO - root - 2017-12-11 05:32:38.125053: step 14130, loss = 0.67, batch loss = 0.62 (14.7 examples/sec; 0.542 sec/batch; 47h:58m:02s remains)
INFO - root - 2017-12-11 05:32:43.271402: step 14140, loss = 0.69, batch loss = 0.63 (21.0 examples/sec; 0.382 sec/batch; 33h:45m:57s remains)
INFO - root - 2017-12-11 05:32:48.463946: step 14150, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:54m:15s remains)
INFO - root - 2017-12-11 05:32:53.843583: step 14160, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 47h:30m:31s remains)
INFO - root - 2017-12-11 05:32:59.226944: step 14170, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 47h:41m:14s remains)
INFO - root - 2017-12-11 05:33:04.520865: step 14180, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 46h:13m:03s remains)
INFO - root - 2017-12-11 05:33:09.840352: step 14190, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:55m:48s remains)
INFO - root - 2017-12-11 05:33:15.237115: step 14200, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:54m:36s remains)
2017-12-11 05:33:15.782616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022378864 -0.022383135 -0.023331938 -0.025133889 -0.026288096 -0.025864005 -0.024350297 -0.023062348 -0.023147786 -0.024904622 -0.027524084 -0.029648473 -0.030510271 -0.030470105 -0.030104754][-0.0045341756 -0.0012767426 -0.0011523154 -0.0040545105 -0.0071278252 -0.0077204788 -0.0057865982 -0.0035565489 -0.0035752743 -0.007008201 -0.012608555 -0.018057523 -0.021404549 -0.022553643 -0.022471109][0.035800304 0.047679238 0.051912583 0.048126634 0.041473452 0.038372561 0.040871415 0.04539958 0.047083478 0.042642131 0.033282913 0.022047054 0.012802779 0.0076767341 0.0059376094][0.0964872 0.12278304 0.13494627 0.13201572 0.12254989 0.11840532 0.12479051 0.13617085 0.14458881 0.14258185 0.12964833 0.10908771 0.087407961 0.0720673 0.064174414][0.16424799 0.20765691 0.23001193 0.23089008 0.2229141 0.22375593 0.24232686 0.27041531 0.29600263 0.30416566 0.2899442 0.25691929 0.21485513 0.18012851 0.15825005][0.21330784 0.26949573 0.30048779 0.30845183 0.30919158 0.32442597 0.36679256 0.42507643 0.48077717 0.50783414 0.49464211 0.44755277 0.38000965 0.31819752 0.27395114][0.22198807 0.28057754 0.31447086 0.33144826 0.34800202 0.38646927 0.46233448 0.56136119 0.65535849 0.70567423 0.69432688 0.6332773 0.54002213 0.44803798 0.37714157][0.19068731 0.23878387 0.26658285 0.2894164 0.32245138 0.38547042 0.49443793 0.63236612 0.76021785 0.83010113 0.82165134 0.75166667 0.64032781 0.52376384 0.43078968][0.14137557 0.17036879 0.1842296 0.2049513 0.24566588 0.32139707 0.44620869 0.60231084 0.74338824 0.81969708 0.81372172 0.7434845 0.6296795 0.5058217 0.40751192][0.10262027 0.11323218 0.11156443 0.12271586 0.15785769 0.22575855 0.33653882 0.47515053 0.59698457 0.65979534 0.65281349 0.59103137 0.49321842 0.38679525 0.30746967][0.086001769 0.085033916 0.072323769 0.071599565 0.091794476 0.13585642 0.20965646 0.3031922 0.38237953 0.41789728 0.4068445 0.35889679 0.2889185 0.21775505 0.17409082][0.082964875 0.077329941 0.059874605 0.049749065 0.053646769 0.071165279 0.10556798 0.15101056 0.18638349 0.19484924 0.17961642 0.14648576 0.10511099 0.071538374 0.064304769][0.078110874 0.072976679 0.05635738 0.041947793 0.03470679 0.033925239 0.042118769 0.055292103 0.061578114 0.053336091 0.03664792 0.016014798 -0.0042637428 -0.01004363 0.0094887549][0.064356089 0.062475063 0.050057627 0.035928056 0.024549006 0.017953977 0.018867539 0.022470213 0.019451031 0.0067067617 -0.0089574056 -0.022636892 -0.03337539 -0.026197881 0.0068729976][0.048468783 0.050963521 0.043544214 0.032151792 0.022671802 0.020449702 0.028321911 0.039883178 0.043185089 0.034472153 0.019564101 0.0053008618 -0.0071232449 0.00015140486 0.035962231]]...]
INFO - root - 2017-12-11 05:33:21.138496: step 14210, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:32m:05s remains)
INFO - root - 2017-12-11 05:33:26.541008: step 14220, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 47h:07m:03s remains)
INFO - root - 2017-12-11 05:33:31.963672: step 14230, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 46h:08m:16s remains)
INFO - root - 2017-12-11 05:33:37.239969: step 14240, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 46h:24m:12s remains)
INFO - root - 2017-12-11 05:33:42.389773: step 14250, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 46h:26m:03s remains)
INFO - root - 2017-12-11 05:33:47.650784: step 14260, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:52m:17s remains)
INFO - root - 2017-12-11 05:33:53.080552: step 14270, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 47h:26m:14s remains)
INFO - root - 2017-12-11 05:33:58.446959: step 14280, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:24m:23s remains)
INFO - root - 2017-12-11 05:34:03.742425: step 14290, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 46h:55m:44s remains)
INFO - root - 2017-12-11 05:34:09.100179: step 14300, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 46h:06m:47s remains)
2017-12-11 05:34:09.640163: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12901594 0.12890925 0.11838037 0.10065039 0.081889041 0.080035038 0.11353855 0.1746594 0.2359682 0.28209609 0.31301484 0.32676765 0.32205966 0.31118187 0.30389971][0.054141056 0.0484272 0.039545216 0.028476553 0.020900579 0.031306729 0.071558625 0.12905397 0.17958567 0.21518683 0.23884879 0.25200236 0.25502047 0.25908539 0.26963085][-0.0096075516 -0.015325558 -0.015546967 -0.012272215 -0.0045670415 0.014779425 0.049570728 0.086122565 0.10884936 0.11722294 0.11880399 0.11879899 0.12266792 0.13958009 0.16776823][-0.033937875 -0.036072079 -0.020230947 0.0071829935 0.037774187 0.06720183 0.09192618 0.10210557 0.092144154 0.066194355 0.03526729 0.0095548881 -0.00033106425 0.012684999 0.042205922][-0.026400637 -0.021151468 0.014844324 0.073749907 0.13644269 0.18503511 0.20896833 0.2027404 0.1679657 0.11033969 0.04360541 -0.01602021 -0.053294327 -0.062013544 -0.048147369][0.00060543063 0.017680429 0.074661247 0.16481663 0.26178229 0.33726504 0.37395796 0.36721268 0.3198593 0.2375368 0.13723665 0.042026971 -0.02827088 -0.066151224 -0.0730034][0.015348283 0.048454683 0.12557726 0.24001214 0.36318 0.46347481 0.51941371 0.52394241 0.47620067 0.38192037 0.26137465 0.14196415 0.046414703 -0.014567536 -0.035222407][0.0039810333 0.050938945 0.14282337 0.26807204 0.39951161 0.50969672 0.57744032 0.59396714 0.55476284 0.46701992 0.35165691 0.23340687 0.1333424 0.065732591 0.044562638][-0.023733735 0.028089028 0.12019425 0.23533539 0.35035843 0.44723475 0.50963587 0.53049976 0.50471014 0.44159663 0.35783723 0.26788649 0.18639144 0.13033038 0.12194996][-0.051994249 -0.007898571 0.06740015 0.15433811 0.23556468 0.303696 0.34905759 0.36839002 0.35789487 0.32755637 0.28653246 0.23708196 0.18671039 0.15361771 0.16633615][-0.070041761 -0.042595297 0.0066144029 0.060106862 0.1071043 0.14792152 0.17750543 0.19471961 0.19609948 0.18998943 0.17992176 0.16211893 0.14085068 0.13372327 0.170735][-0.074899107 -0.064057283 -0.03958144 -0.014006764 0.0077481959 0.029123183 0.046939954 0.060295518 0.065347448 0.067791171 0.069604464 0.069464818 0.071925849 0.091282077 0.15280734][-0.068056948 -0.068871491 -0.062171284 -0.055948757 -0.051341221 -0.044766515 -0.038613066 -0.033754569 -0.033376016 -0.033019319 -0.029041056 -0.016508851 0.0094313892 0.056762423 0.14118592][-0.0584724 -0.065899372 -0.070082344 -0.076039881 -0.0830145 -0.088122934 -0.092775717 -0.097432934 -0.10380123 -0.10743317 -0.10191272 -0.076669149 -0.027793439 0.046301667 0.15015234][-0.050184406 -0.058233026 -0.064359814 -0.072418936 -0.081173107 -0.089062341 -0.097435787 -0.10657743 -0.11723269 -0.12467941 -0.1203945 -0.0894344 -0.027787993 0.062958211 0.17783211]]...]
INFO - root - 2017-12-11 05:34:14.981072: step 14310, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:48m:44s remains)
INFO - root - 2017-12-11 05:34:20.320739: step 14320, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 47h:02m:27s remains)
INFO - root - 2017-12-11 05:34:25.719495: step 14330, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.545 sec/batch; 48h:12m:26s remains)
INFO - root - 2017-12-11 05:34:31.050379: step 14340, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 47h:17m:13s remains)
INFO - root - 2017-12-11 05:34:36.139775: step 14350, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:20m:29s remains)
INFO - root - 2017-12-11 05:34:41.488004: step 14360, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:45m:16s remains)
INFO - root - 2017-12-11 05:34:46.773946: step 14370, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.508 sec/batch; 44h:53m:27s remains)
INFO - root - 2017-12-11 05:34:52.126129: step 14380, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:23m:22s remains)
INFO - root - 2017-12-11 05:34:57.526653: step 14390, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 49h:47m:11s remains)
INFO - root - 2017-12-11 05:35:02.850726: step 14400, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 45h:47m:27s remains)
2017-12-11 05:35:03.383184: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2470056 0.25141695 0.24258623 0.22897524 0.21507594 0.20188105 0.18750921 0.16345096 0.12480779 0.074473791 0.022883855 -0.019075127 -0.045476433 -0.05744176 -0.060422525][0.30781287 0.32449782 0.321804 0.30759123 0.28885522 0.27076161 0.25308809 0.22650969 0.18573165 0.13303354 0.077894747 0.030840993 -0.0017624703 -0.01994529 -0.027990991][0.34624067 0.37592149 0.38251153 0.37364572 0.35866803 0.34540454 0.33348495 0.31108314 0.2704308 0.21403079 0.15338425 0.10049157 0.061902445 0.037537191 0.023482637][0.36704305 0.40844509 0.42615128 0.42873868 0.426613 0.42691085 0.42713687 0.4117164 0.36916974 0.30306104 0.23095188 0.16956444 0.12537114 0.096120588 0.076209515][0.36658016 0.415994 0.44509977 0.46348518 0.48019516 0.49831283 0.51276165 0.50543588 0.4614211 0.38453931 0.29927132 0.22871299 0.17993741 0.14807917 0.1245388][0.34823239 0.39791617 0.4345963 0.47004279 0.5099867 0.55034226 0.58192056 0.58465827 0.54099452 0.4539521 0.35473442 0.27399245 0.22129212 0.18999426 0.16730785][0.321922 0.36233523 0.39779311 0.44573122 0.51039964 0.57920521 0.63469917 0.65161014 0.60979939 0.51224113 0.39816108 0.30774793 0.25478363 0.23073897 0.21590878][0.30187461 0.32559413 0.35066819 0.40253296 0.48729202 0.58445132 0.66492331 0.69550347 0.65481895 0.54780716 0.42267793 0.32985613 0.28694943 0.28105313 0.28244698][0.28897 0.29337436 0.30182207 0.34786677 0.44083241 0.5546934 0.65001333 0.68721968 0.64726865 0.53869778 0.4156194 0.33535549 0.31634083 0.33887023 0.362516][0.28146848 0.26947513 0.26062489 0.294044 0.38135889 0.49431846 0.58793694 0.62154996 0.58211613 0.48214492 0.37613487 0.32191682 0.33508411 0.38871652 0.434899][0.27856517 0.25694862 0.23590247 0.25562993 0.32835445 0.4266009 0.50533748 0.52708292 0.48628893 0.39973453 0.31822845 0.29425538 0.33743969 0.41498077 0.476481][0.26823947 0.24328423 0.21717982 0.22708119 0.28383753 0.36276922 0.42296046 0.43182126 0.38945356 0.31687808 0.26018816 0.26304418 0.32761097 0.41711253 0.482599][0.24623521 0.22290239 0.19923106 0.206179 0.25120151 0.31263882 0.35545644 0.35246053 0.30776143 0.2469911 0.21163592 0.23558667 0.31353393 0.4064675 0.46830642][0.21927086 0.20195046 0.18831226 0.20177034 0.24370372 0.29341912 0.32165945 0.30665565 0.25604424 0.20088866 0.17931126 0.21559294 0.29846746 0.38886172 0.44401097][0.20503071 0.19546506 0.19698931 0.22544457 0.27432796 0.31978393 0.3360185 0.30672982 0.2441885 0.18526162 0.16633333 0.20361726 0.28137249 0.36243716 0.40901047]]...]
INFO - root - 2017-12-11 05:35:08.824403: step 14410, loss = 0.67, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 46h:09m:15s remains)
INFO - root - 2017-12-11 05:35:14.091906: step 14420, loss = 0.68, batch loss = 0.62 (15.9 examples/sec; 0.502 sec/batch; 44h:21m:38s remains)
INFO - root - 2017-12-11 05:35:19.404630: step 14430, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.532 sec/batch; 46h:58m:19s remains)
INFO - root - 2017-12-11 05:35:24.724299: step 14440, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:10m:46s remains)
INFO - root - 2017-12-11 05:35:29.830976: step 14450, loss = 0.70, batch loss = 0.64 (26.6 examples/sec; 0.301 sec/batch; 26h:33m:25s remains)
INFO - root - 2017-12-11 05:35:35.195995: step 14460, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 50h:27m:51s remains)
INFO - root - 2017-12-11 05:35:40.554108: step 14470, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 46h:35m:53s remains)
INFO - root - 2017-12-11 05:35:45.937063: step 14480, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:55m:31s remains)
INFO - root - 2017-12-11 05:35:51.327665: step 14490, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:41m:20s remains)
INFO - root - 2017-12-11 05:35:56.661877: step 14500, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 47h:11m:55s remains)
2017-12-11 05:35:57.183802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016825562 0.011879151 0.047862712 0.11368535 0.20900658 0.31069136 0.39003724 0.42664143 0.41398123 0.37191719 0.32796523 0.30157009 0.30500296 0.33786064 0.37950879][-0.015198243 -0.00089330296 0.037691567 0.10807142 0.21266772 0.32941413 0.42868733 0.48482439 0.48730087 0.45384261 0.41161695 0.37826046 0.36869869 0.38806254 0.42036921][-0.015345536 -0.0046617817 0.0307931 0.10122291 0.20941651 0.33470473 0.4466238 0.51633388 0.5299387 0.50302827 0.46337333 0.4261277 0.40542072 0.4090685 0.42633829][0.0048929905 0.010738305 0.042270571 0.1127352 0.2232352 0.35216376 0.46816385 0.5404613 0.55421907 0.52467996 0.48290017 0.44347477 0.41670448 0.40787995 0.40851763][0.042473558 0.043844666 0.071162254 0.13950171 0.24714066 0.37047687 0.47888961 0.54212493 0.54823452 0.51362276 0.47220111 0.43736994 0.4124445 0.39610118 0.38028884][0.09363842 0.0881043 0.10778701 0.16934885 0.26830405 0.37869528 0.47114536 0.51814747 0.51458859 0.47978383 0.4478353 0.4280898 0.41433206 0.39633113 0.36562556][0.15013486 0.13180125 0.1367891 0.18565373 0.27336365 0.37128249 0.44915789 0.4816024 0.47112787 0.44063789 0.42280447 0.4220677 0.42332083 0.40741658 0.36445275][0.20334277 0.16966905 0.155676 0.18784168 0.26349694 0.35207605 0.42113686 0.4452889 0.43173811 0.40596232 0.39886507 0.41143444 0.422967 0.40825564 0.35572791][0.23764424 0.19322588 0.16147386 0.17373832 0.23238185 0.30969164 0.37379196 0.3992472 0.39291072 0.37836036 0.38246176 0.40439424 0.42182145 0.40721327 0.3480328][0.23910819 0.19541676 0.15492547 0.14855748 0.18429635 0.24363495 0.30089784 0.33319587 0.34396407 0.35038677 0.3712433 0.40382388 0.42724729 0.41458264 0.3528572][0.21287484 0.18502714 0.15245068 0.13875446 0.15408525 0.19112748 0.23490439 0.26819202 0.29249862 0.31849954 0.35572121 0.3988995 0.4297975 0.42249033 0.3628158][0.17051658 0.1709882 0.16351746 0.15898691 0.16405967 0.1795406 0.20287357 0.22532044 0.25011295 0.28442684 0.33087531 0.38149953 0.41974029 0.41990426 0.36585477][0.1228909 0.15345015 0.17779478 0.19243778 0.19658861 0.19494481 0.19655922 0.2029022 0.22107492 0.25704628 0.30868769 0.36580983 0.41118592 0.41863722 0.37063625][0.079546884 0.13021934 0.17966981 0.21291499 0.22072239 0.20839454 0.19294089 0.18570524 0.19810541 0.23579447 0.29276723 0.35712916 0.40967184 0.4235236 0.38094142][0.052319873 0.10950219 0.17197986 0.21832883 0.23254123 0.217243 0.19212997 0.17536506 0.18279566 0.22073668 0.28071845 0.34926021 0.40522835 0.42177883 0.38239297]]...]
INFO - root - 2017-12-11 05:36:02.596404: step 14510, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:02m:51s remains)
INFO - root - 2017-12-11 05:36:07.949090: step 14520, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:12m:00s remains)
INFO - root - 2017-12-11 05:36:13.253966: step 14530, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:54m:15s remains)
INFO - root - 2017-12-11 05:36:18.506884: step 14540, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 46h:35m:22s remains)
INFO - root - 2017-12-11 05:36:23.871488: step 14550, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:12m:15s remains)
INFO - root - 2017-12-11 05:36:28.966969: step 14560, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 46h:18m:15s remains)
INFO - root - 2017-12-11 05:36:34.268736: step 14570, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 46h:38m:39s remains)
INFO - root - 2017-12-11 05:36:39.614123: step 14580, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 48h:35m:55s remains)
INFO - root - 2017-12-11 05:36:44.985083: step 14590, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 45h:51m:59s remains)
INFO - root - 2017-12-11 05:36:50.282869: step 14600, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:08m:07s remains)
2017-12-11 05:36:50.841203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041385382 0.044666968 0.047682412 0.052940827 0.060918726 0.072011158 0.083469152 0.092084758 0.097594373 0.1080696 0.13483934 0.17056754 0.20679156 0.23397633 0.24661133][0.095553286 0.10342118 0.1096034 0.1183796 0.13130631 0.14911598 0.16702797 0.17953563 0.18582106 0.19610056 0.2246864 0.26107961 0.2941595 0.31260702 0.31192976][0.16806877 0.18227363 0.19469911 0.21129873 0.23447554 0.26453507 0.29336309 0.31128615 0.31587166 0.3191424 0.33897042 0.36361015 0.38032809 0.37828904 0.35526669][0.25612932 0.27575624 0.29659981 0.32635012 0.36724177 0.41789156 0.46508488 0.49197513 0.492435 0.47966966 0.476111 0.47263345 0.45790786 0.42482296 0.37394485][0.35812458 0.38045549 0.41038197 0.458067 0.52322656 0.60056645 0.67012405 0.70578265 0.69642854 0.65617037 0.612406 0.56427258 0.50597858 0.43616223 0.3577626][0.45890445 0.47948116 0.51764923 0.58639264 0.67886454 0.78395659 0.87462932 0.91503519 0.88938576 0.81389821 0.72067863 0.62072515 0.51651216 0.41327035 0.31497365][0.52749377 0.54105693 0.5872885 0.67639166 0.79194218 0.91852838 1.0232568 1.0606449 1.0137388 0.90401185 0.76667386 0.62263232 0.48287225 0.35948774 0.25526586][0.52680039 0.53105116 0.58305079 0.68465656 0.80915022 0.94101107 1.0458225 1.0720161 1.0049658 0.87348306 0.71309584 0.5486967 0.39719453 0.27564895 0.1834767][0.45385176 0.44547352 0.493147 0.58869451 0.69894439 0.81327057 0.90186542 0.91242987 0.8341884 0.70248973 0.55005151 0.39913139 0.26733035 0.17225337 0.10873171][0.33355013 0.30954793 0.33916754 0.40830857 0.48330948 0.56133991 0.62207085 0.61778778 0.54120928 0.43083742 0.31435964 0.20642795 0.11990651 0.068327159 0.041959766][0.20557606 0.1594246 0.15756528 0.18595445 0.21592943 0.25196049 0.28393513 0.27139682 0.20934947 0.13547638 0.071532562 0.021564428 -0.0095970389 -0.014709985 -0.0068185353][0.097011052 0.026116408 -0.012118015 -0.027161501 -0.039344806 -0.039588388 -0.030390553 -0.04419357 -0.0846852 -0.11839355 -0.1290285 -0.12294736 -0.10440338 -0.072655067 -0.041795868][0.0052910806 -0.081060626 -0.14481968 -0.18885218 -0.2258976 -0.24723645 -0.25153548 -0.26185653 -0.28112063 -0.28282028 -0.25539288 -0.21146241 -0.16201222 -0.11171629 -0.072943479][-0.080780119 -0.16704628 -0.23611878 -0.28673378 -0.32666767 -0.35082558 -0.35751936 -0.361592 -0.36469054 -0.34772748 -0.30326924 -0.24590172 -0.18898232 -0.13963518 -0.10641643][-0.16586332 -0.23710124 -0.29154262 -0.32908529 -0.35558096 -0.36997443 -0.37167996 -0.36956418 -0.36384037 -0.34145609 -0.29812545 -0.24736409 -0.20110337 -0.1655395 -0.14480038]]...]
INFO - root - 2017-12-11 05:36:56.314974: step 14610, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 48h:59m:14s remains)
INFO - root - 2017-12-11 05:37:01.706741: step 14620, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 49h:46m:17s remains)
INFO - root - 2017-12-11 05:37:07.094009: step 14630, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 47h:32m:00s remains)
INFO - root - 2017-12-11 05:37:12.380631: step 14640, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 45h:35m:40s remains)
INFO - root - 2017-12-11 05:37:17.809603: step 14650, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:42m:39s remains)
INFO - root - 2017-12-11 05:37:22.881610: step 14660, loss = 0.71, batch loss = 0.65 (16.0 examples/sec; 0.500 sec/batch; 44h:09m:20s remains)
INFO - root - 2017-12-11 05:37:28.236756: step 14670, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 47h:16m:01s remains)
INFO - root - 2017-12-11 05:37:33.609378: step 14680, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:04m:29s remains)
INFO - root - 2017-12-11 05:37:38.887007: step 14690, loss = 0.71, batch loss = 0.65 (15.9 examples/sec; 0.502 sec/batch; 44h:20m:10s remains)
INFO - root - 2017-12-11 05:37:44.243305: step 14700, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:01m:33s remains)
2017-12-11 05:37:44.799099: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03886912 0.042427842 0.054553911 0.074209139 0.093531206 0.10839696 0.11087723 0.097648323 0.07365717 0.045011226 0.017691473 -0.0079082819 -0.031946279 -0.053083096 -0.069298722][0.051144727 0.068442464 0.097739153 0.13504243 0.17204817 0.20397098 0.2192513 0.2124664 0.18762 0.15029106 0.10375725 0.052288696 0.0035362074 -0.036156684 -0.064871617][0.070681222 0.10374785 0.15229204 0.21100181 0.27214265 0.33017144 0.36618233 0.36979741 0.34378341 0.29306826 0.21906996 0.13154647 0.04990574 -0.012820123 -0.055281337][0.089499019 0.13623358 0.20321313 0.28504983 0.37483981 0.46485445 0.52606851 0.5415917 0.51277959 0.44510642 0.33954659 0.21209759 0.094906136 0.008046967 -0.048350472][0.11196589 0.1671851 0.2506012 0.35685164 0.47805351 0.60134941 0.68630129 0.70924836 0.67030263 0.57871807 0.43916562 0.27410531 0.12590808 0.019510789 -0.046897929][0.14278665 0.20086634 0.29538405 0.42240286 0.57130188 0.72207236 0.82437646 0.8475036 0.79024392 0.66895777 0.49628127 0.30105317 0.13112009 0.013806855 -0.055184465][0.17754526 0.23368368 0.33173823 0.46952882 0.63410139 0.79840225 0.9068172 0.92359304 0.84632158 0.69862211 0.50145018 0.28908753 0.11027325 -0.0076219793 -0.071820393][0.21535546 0.2632491 0.35265711 0.48334685 0.6426897 0.79996634 0.90046608 0.90655 0.81488019 0.65370995 0.448717 0.23677863 0.064028732 -0.043649219 -0.096578233][0.25708291 0.28767434 0.35298637 0.456928 0.59047812 0.72480083 0.80944204 0.80665433 0.71188504 0.55453461 0.35925046 0.1616661 0.004842476 -0.086654939 -0.12528929][0.30368337 0.3038615 0.32920039 0.38913253 0.48209018 0.58650482 0.65652579 0.65352648 0.57026064 0.43371 0.26209703 0.087103054 -0.050208841 -0.12557566 -0.15124735][0.34587985 0.31453052 0.29310986 0.30155075 0.34917009 0.42463404 0.48601919 0.49215266 0.43277186 0.3289037 0.18808131 0.037057575 -0.0836784 -0.14830482 -0.16657487][0.3610917 0.30697107 0.24907671 0.21622807 0.22814123 0.28347182 0.34288418 0.36353615 0.33283421 0.26332247 0.15122738 0.019880837 -0.089736253 -0.15021683 -0.16781992][0.3241438 0.2601451 0.18554524 0.13333452 0.12813468 0.17605197 0.23869017 0.27460349 0.26910251 0.22617286 0.13576263 0.019413453 -0.081990846 -0.1416166 -0.162116][0.23331006 0.1708097 0.097805947 0.047028072 0.041620705 0.09046869 0.15644065 0.203445 0.21566764 0.18934815 0.11456244 0.012141091 -0.078746013 -0.13519141 -0.15694331][0.12894101 0.0774344 0.018346298 -0.020373922 -0.019334879 0.029086376 0.091846913 0.14100416 0.16114789 0.142926 0.079881094 -0.0071445 -0.08325389 -0.13185662 -0.15144035]]...]
INFO - root - 2017-12-11 05:37:50.064398: step 14710, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:46m:51s remains)
INFO - root - 2017-12-11 05:37:55.447999: step 14720, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:19m:41s remains)
INFO - root - 2017-12-11 05:38:00.954175: step 14730, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:06m:38s remains)
INFO - root - 2017-12-11 05:38:06.293826: step 14740, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 47h:57m:01s remains)
INFO - root - 2017-12-11 05:38:11.661207: step 14750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:50m:44s remains)
INFO - root - 2017-12-11 05:38:16.736451: step 14760, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 48h:49m:08s remains)
INFO - root - 2017-12-11 05:38:22.119561: step 14770, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:16m:11s remains)
INFO - root - 2017-12-11 05:38:27.552774: step 14780, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 46h:16m:02s remains)
INFO - root - 2017-12-11 05:38:32.912481: step 14790, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 46h:10m:39s remains)
INFO - root - 2017-12-11 05:38:38.222087: step 14800, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:20m:24s remains)
2017-12-11 05:38:38.754116: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.53099257 0.35450518 0.17341158 0.051387053 -0.010064377 -0.031187333 -0.035722803 -0.038065493 -0.047486965 -0.061809424 -0.073694967 -0.083766133 -0.095853075 -0.11035799 -0.12355469][0.50122219 0.32927266 0.15346357 0.04032819 -0.0097181061 -0.021806238 -0.023845719 -0.030398408 -0.046971798 -0.070464149 -0.093421504 -0.11290751 -0.12873456 -0.14021048 -0.14424996][0.45641971 0.30554023 0.15275098 0.061480198 0.028564794 0.027666166 0.029123804 0.019024681 -0.0033198548 -0.034684148 -0.069187164 -0.10128722 -0.12534428 -0.13858353 -0.13788816][0.44177806 0.32216746 0.20489244 0.14600332 0.13651551 0.14829707 0.15279856 0.13775776 0.10818876 0.067544222 0.019647866 -0.028892331 -0.068454742 -0.094263889 -0.10206116][0.4679552 0.3836059 0.30907184 0.29149029 0.31458655 0.34514618 0.35590628 0.33618757 0.29596454 0.23925515 0.16923065 0.094799042 0.028703997 -0.022540666 -0.0536021][0.52425981 0.46953312 0.43348703 0.45482552 0.51168555 0.56491053 0.58829862 0.57121736 0.52448267 0.4508684 0.35395303 0.24714628 0.14652656 0.061059739 -0.0031154787][0.5517168 0.51579189 0.50801629 0.55876124 0.64362615 0.71986455 0.76276517 0.76054221 0.7211979 0.64340281 0.52959394 0.39706817 0.26493055 0.14577013 0.047176469][0.49684525 0.46771559 0.47522551 0.54279208 0.64657784 0.74358988 0.81087381 0.83446062 0.81798494 0.75464749 0.64394283 0.50447083 0.35687876 0.21763201 0.096104041][0.35613152 0.32275358 0.33351406 0.40631953 0.51980132 0.63406032 0.72688246 0.78235757 0.79879254 0.76375103 0.6727764 0.54434484 0.40017542 0.25962743 0.13379858][0.16354236 0.11853888 0.12433814 0.19372404 0.30738282 0.43097588 0.5425204 0.62599534 0.67422312 0.6704638 0.60703784 0.50146276 0.37633416 0.25252992 0.14266144][-0.030500527 -0.085881852 -0.086358421 -0.024603166 0.080396682 0.20074016 0.31577539 0.41090614 0.47540739 0.49239692 0.45459503 0.37773994 0.28304836 0.19029242 0.11220375][-0.17996958 -0.24129905 -0.24656673 -0.19382426 -0.10262588 0.0035559866 0.10572559 0.19153708 0.25014532 0.26883948 0.24510899 0.19373198 0.13260192 0.077172942 0.037684578][-0.26099756 -0.3214007 -0.32937223 -0.28657821 -0.21216667 -0.12625408 -0.046409577 0.016692502 0.053692829 0.057892207 0.034211863 -0.002680382 -0.038287476 -0.062554479 -0.068711124][-0.27494121 -0.32731587 -0.33483267 -0.30064762 -0.24096684 -0.17156418 -0.10925461 -0.065714508 -0.05089397 -0.067464262 -0.10295051 -0.14116304 -0.16833781 -0.17780213 -0.1657358][-0.22869122 -0.26997659 -0.276948 -0.25168622 -0.20395744 -0.14338794 -0.086228579 -0.048560422 -0.044738166 -0.07759621 -0.12995878 -0.18245067 -0.21850151 -0.23057091 -0.21649846]]...]
INFO - root - 2017-12-11 05:38:44.071968: step 14810, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 48h:13m:09s remains)
INFO - root - 2017-12-11 05:38:49.511432: step 14820, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 46h:29m:00s remains)
INFO - root - 2017-12-11 05:38:54.878343: step 14830, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:20m:22s remains)
INFO - root - 2017-12-11 05:39:00.334817: step 14840, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 45h:43m:16s remains)
INFO - root - 2017-12-11 05:39:05.655799: step 14850, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:45m:06s remains)
INFO - root - 2017-12-11 05:39:10.933523: step 14860, loss = 0.70, batch loss = 0.64 (20.7 examples/sec; 0.387 sec/batch; 34h:07m:55s remains)
INFO - root - 2017-12-11 05:39:16.083920: step 14870, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:39m:58s remains)
INFO - root - 2017-12-11 05:39:21.497710: step 14880, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.532 sec/batch; 46h:56m:00s remains)
INFO - root - 2017-12-11 05:39:26.897658: step 14890, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:02m:23s remains)
INFO - root - 2017-12-11 05:39:32.263501: step 14900, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:53m:24s remains)
2017-12-11 05:39:32.768853: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049866632 0.10891205 0.16947117 0.19800623 0.18317948 0.14283898 0.10533504 0.0892954 0.086732194 0.085292317 0.073002942 0.0575252 0.042155787 0.027989261 0.013892272][0.082134217 0.13820924 0.18812229 0.202391 0.17989506 0.14595821 0.12771374 0.13114534 0.13519195 0.12528081 0.098696887 0.07315807 0.05411882 0.039210506 0.023555767][0.10766769 0.15818785 0.19599943 0.19783603 0.17077559 0.14438519 0.14270681 0.15943375 0.1655387 0.14826643 0.11404224 0.085833371 0.068256944 0.054818239 0.038658224][0.11951504 0.16006263 0.18532473 0.17920616 0.15240534 0.13270013 0.13861559 0.15702897 0.16020957 0.14069144 0.11011822 0.08967115 0.080483995 0.073429458 0.0626637][0.10578851 0.13324152 0.14777523 0.14240053 0.12687418 0.12006341 0.13076071 0.14304218 0.13862969 0.11846618 0.096299045 0.087098867 0.087989934 0.089590617 0.089129381][0.085557364 0.1077375 0.1204625 0.12445977 0.12526417 0.13074732 0.14039387 0.14008275 0.12325284 0.099862054 0.082856253 0.079705223 0.085202135 0.094238482 0.10868064][0.082273021 0.11028965 0.129022 0.14143971 0.14873944 0.152538 0.14967136 0.13251954 0.10447352 0.080848485 0.069728449 0.069962308 0.074755378 0.086043432 0.11308968][0.096899718 0.13286631 0.15602526 0.16748188 0.16668162 0.15572096 0.13516502 0.10519314 0.075002864 0.059881918 0.060007326 0.065636441 0.068091355 0.075911589 0.10658766][0.11280353 0.15415411 0.18067673 0.18965343 0.17924227 0.15412593 0.12058552 0.08471512 0.057257373 0.051246807 0.060997028 0.071840137 0.073480964 0.077542752 0.10513049][0.1203301 0.16391447 0.19415088 0.20614228 0.19599919 0.16837001 0.13203366 0.094072834 0.06443499 0.05558686 0.063457191 0.074673168 0.078615464 0.084888108 0.11105654][0.12263947 0.16297361 0.19161685 0.20506439 0.19918333 0.1774893 0.14733103 0.11132809 0.076611556 0.056506466 0.053571012 0.060461454 0.068128906 0.081491545 0.11026558][0.1287719 0.16145609 0.18246424 0.1911158 0.18530653 0.16963197 0.1494502 0.12056743 0.084247865 0.053111628 0.036695443 0.03573006 0.044355132 0.062893778 0.09322761][0.13496195 0.16206621 0.17757612 0.18369654 0.17831703 0.16612506 0.15295286 0.13087282 0.095497914 0.056237746 0.028105851 0.019027542 0.025749329 0.044861857 0.072416246][0.13873883 0.16206977 0.17596702 0.18666911 0.18842788 0.18063502 0.16922969 0.1471462 0.10982067 0.064363495 0.02938251 0.016817002 0.022871068 0.04013291 0.061184976][0.14100327 0.15620646 0.16372734 0.17792393 0.18996684 0.18971695 0.17955005 0.15453775 0.11473007 0.067142136 0.031932149 0.022170674 0.031689856 0.049656406 0.065845996]]...]
INFO - root - 2017-12-11 05:39:38.033036: step 14910, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:38m:50s remains)
INFO - root - 2017-12-11 05:39:43.362534: step 14920, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 45h:55m:46s remains)
INFO - root - 2017-12-11 05:39:48.711077: step 14930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:49m:16s remains)
INFO - root - 2017-12-11 05:39:54.075363: step 14940, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 50h:30m:30s remains)
INFO - root - 2017-12-11 05:39:59.448223: step 14950, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:17m:56s remains)
INFO - root - 2017-12-11 05:40:04.694405: step 14960, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 46h:11m:07s remains)
INFO - root - 2017-12-11 05:40:09.764911: step 14970, loss = 0.67, batch loss = 0.61 (15.8 examples/sec; 0.507 sec/batch; 44h:44m:56s remains)
INFO - root - 2017-12-11 05:40:15.214989: step 14980, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:16m:02s remains)
INFO - root - 2017-12-11 05:40:20.514771: step 14990, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 45h:43m:07s remains)
INFO - root - 2017-12-11 05:40:25.898391: step 15000, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:42m:41s remains)
2017-12-11 05:40:26.425546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06331978 -0.062146869 -0.053693004 -0.036107745 -0.0082272226 0.0259055 0.055456381 0.072264284 0.071229957 0.052622978 0.020794023 -0.015845269 -0.046111535 -0.063516475 -0.067801543][-0.065584123 -0.060825273 -0.044697039 -0.012775626 0.036557145 0.095038377 0.14478767 0.17251737 0.17142457 0.14017753 0.085958943 0.023790529 -0.027626332 -0.058972787 -0.07072296][-0.061891343 -0.051758822 -0.025939673 0.024097288 0.099233769 0.1861186 0.25960436 0.30110049 0.3010163 0.25457954 0.17253275 0.078850456 0.0020675308 -0.045610473 -0.066727355][-0.053221248 -0.035617508 0.000988495 0.069140293 0.16832814 0.2813195 0.37683502 0.43131721 0.43173116 0.36954641 0.2595585 0.13452969 0.03264036 -0.031357661 -0.062436573][-0.033898927 -0.005905861 0.041661646 0.12375593 0.23930493 0.36919644 0.47776875 0.53750646 0.53368545 0.45692319 0.32609645 0.17922306 0.059634212 -0.016929651 -0.05670505][0.0058319704 0.049829829 0.11034661 0.20173405 0.32289907 0.455782 0.56302762 0.61467212 0.59818816 0.50951529 0.37091446 0.2181323 0.091256142 0.0053180619 -0.043795627][0.064115755 0.1301897 0.20675895 0.30427232 0.42031643 0.54049861 0.62947726 0.65906572 0.62391037 0.53035271 0.40041485 0.25739998 0.13105662 0.036588885 -0.02371544][0.12572396 0.21480915 0.30672336 0.40462771 0.50257182 0.59273535 0.6477614 0.64820856 0.59796071 0.51131016 0.40408647 0.28147632 0.16150385 0.061650056 -0.0073741307][0.16534302 0.26870295 0.3671003 0.45389479 0.51986665 0.56700957 0.58306742 0.56052816 0.50806242 0.44018984 0.3623957 0.26393905 0.15521909 0.057795573 -0.010741348][0.16085926 0.26359925 0.35448477 0.41760543 0.44320926 0.44530782 0.4300386 0.39978832 0.36039686 0.31868324 0.26959684 0.19447654 0.1020539 0.017429581 -0.039322909][0.1031739 0.1880427 0.25695869 0.28957105 0.2790767 0.24932367 0.21963532 0.19584379 0.17625593 0.15851827 0.13220511 0.079200827 0.0096169207 -0.051602453 -0.086854085][0.0043951417 0.055510644 0.091505341 0.094992436 0.064046733 0.024645982 -0.0022120802 -0.013580793 -0.017388687 -0.020061735 -0.030569712 -0.061351843 -0.10172391 -0.13138516 -0.13879995][-0.099659674 -0.088054 -0.085597351 -0.10210581 -0.13554606 -0.16636601 -0.18090813 -0.18097611 -0.17609493 -0.17167944 -0.17165464 -0.18178616 -0.19305515 -0.19231431 -0.17481586][-0.1738683 -0.19148037 -0.2102911 -0.23372041 -0.25866586 -0.27511543 -0.27771348 -0.27095136 -0.26235408 -0.25448379 -0.24733683 -0.2422208 -0.23296003 -0.21285617 -0.18206255][-0.19794957 -0.22547612 -0.2484718 -0.26805109 -0.28233898 -0.2884914 -0.28596485 -0.27856386 -0.27027676 -0.26164857 -0.25134248 -0.23884934 -0.22063118 -0.19456108 -0.16318858]]...]
INFO - root - 2017-12-11 05:40:31.786786: step 15010, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 46h:32m:07s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:40:37.058314: step 15020, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:16m:55s remains)
INFO - root - 2017-12-11 05:40:42.447788: step 15030, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:41m:57s remains)
INFO - root - 2017-12-11 05:40:47.723836: step 15040, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 46h:57m:28s remains)
INFO - root - 2017-12-11 05:40:53.073834: step 15050, loss = 0.67, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 47h:01m:19s remains)
INFO - root - 2017-12-11 05:40:58.314675: step 15060, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 48h:10m:19s remains)
INFO - root - 2017-12-11 05:41:03.413137: step 15070, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 45h:54m:39s remains)
INFO - root - 2017-12-11 05:41:08.781963: step 15080, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:42m:56s remains)
INFO - root - 2017-12-11 05:41:14.144278: step 15090, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:38m:29s remains)
INFO - root - 2017-12-11 05:41:19.438460: step 15100, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 46h:31m:30s remains)
2017-12-11 05:41:20.014992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.002931389 -0.0070437952 -0.0099657271 -0.010797497 -0.010050973 -0.0074320538 -0.0029052822 0.0020566212 0.0047111847 0.0026687672 -0.0032355117 -0.011612552 -0.020653205 -0.028623829 -0.034667742][0.024249718 0.018491479 0.011708497 0.00842293 0.0085925274 0.012788782 0.020656759 0.029790655 0.03478146 0.031596053 0.021984303 0.0076982761 -0.0082386844 -0.022575235 -0.033780903][0.066729389 0.060116962 0.048289347 0.041169688 0.039506074 0.045415647 0.05768263 0.072267629 0.080698304 0.076839112 0.063522533 0.0421403 0.016965859 -0.0062845615 -0.024866203][0.11534306 0.10913744 0.092600465 0.081072219 0.076115884 0.082257323 0.097655445 0.11658265 0.12751718 0.12267403 0.10622673 0.078111358 0.043288928 0.01044024 -0.015978849][0.15985258 0.15735188 0.13967711 0.12519558 0.11578809 0.11981269 0.13576673 0.15606098 0.16658288 0.15922417 0.14017642 0.10692654 0.064047307 0.022876127 -0.0099886935][0.19608103 0.20068412 0.18690172 0.17203829 0.15806741 0.15771607 0.17092092 0.1881929 0.19376774 0.18135026 0.16012098 0.12453897 0.076792307 0.029856035 -0.00725895][0.22212763 0.23582493 0.22913176 0.21464284 0.19564582 0.18904585 0.19741328 0.20868888 0.20591111 0.18635444 0.16292371 0.12761657 0.078562342 0.029096315 -0.0095234532][0.23435338 0.25778419 0.25995424 0.24652393 0.2236089 0.21110709 0.21410865 0.21737552 0.20397279 0.17569353 0.14879063 0.11462092 0.067080572 0.018588167 -0.018231362][0.23204827 0.26268789 0.27212542 0.26039776 0.23660755 0.2214523 0.22076593 0.216003 0.19196394 0.15490985 0.12312308 0.089453742 0.044884317 0.00053574372 -0.031395271][0.21650435 0.24884459 0.2594153 0.24711774 0.22365777 0.20846674 0.20715281 0.1988018 0.17001197 0.12931708 0.095018454 0.062458623 0.021721795 -0.016902287 -0.043097287][0.18995631 0.21583021 0.21920067 0.20109572 0.17551677 0.15903333 0.15787101 0.15166482 0.12834889 0.0942159 0.064348377 0.036664575 0.0020019875 -0.029884256 -0.051005375][0.13930781 0.15574668 0.14891703 0.12426076 0.09591338 0.076596484 0.074485019 0.073879413 0.064210214 0.04551398 0.027224945 0.0092748133 -0.016091187 -0.040022735 -0.056550127][0.064321645 0.071063206 0.058422413 0.033338733 0.0067213462 -0.01273823 -0.015554871 -0.010317696 -0.0056714909 -0.0077665006 -0.012823226 -0.019497856 -0.034094639 -0.04942685 -0.06125813][-0.015896615 -0.017127724 -0.029278878 -0.04832229 -0.068683028 -0.084768452 -0.087488435 -0.078887329 -0.065464512 -0.0565502 -0.051916666 -0.049308009 -0.05339564 -0.059819791 -0.066193134][-0.079717129 -0.085276514 -0.093312562 -0.10404065 -0.11643036 -0.12756002 -0.12990333 -0.12129495 -0.10615192 -0.093586475 -0.084887043 -0.077143565 -0.07294596 -0.071208492 -0.071353585]]...]
INFO - root - 2017-12-11 05:41:25.480864: step 15110, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:19m:01s remains)
INFO - root - 2017-12-11 05:41:30.868096: step 15120, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 46h:48m:17s remains)
INFO - root - 2017-12-11 05:41:36.206689: step 15130, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 45h:54m:09s remains)
INFO - root - 2017-12-11 05:41:41.545115: step 15140, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:26m:18s remains)
INFO - root - 2017-12-11 05:41:46.912332: step 15150, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 48h:17m:45s remains)
INFO - root - 2017-12-11 05:41:52.205223: step 15160, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 45h:47m:58s remains)
INFO - root - 2017-12-11 05:41:57.580736: step 15170, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 49h:16m:49s remains)
INFO - root - 2017-12-11 05:42:02.590351: step 15180, loss = 0.66, batch loss = 0.60 (14.3 examples/sec; 0.560 sec/batch; 49h:19m:29s remains)
INFO - root - 2017-12-11 05:42:07.960985: step 15190, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:21m:41s remains)
INFO - root - 2017-12-11 05:42:13.446150: step 15200, loss = 0.67, batch loss = 0.61 (13.4 examples/sec; 0.598 sec/batch; 52h:43m:22s remains)
2017-12-11 05:42:14.030513: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062926263 0.1014188 0.16119412 0.22318107 0.2697013 0.29362425 0.29972553 0.30242193 0.29552028 0.27180612 0.22437556 0.15041097 0.065711856 -0.0090826647 -0.065518849][-0.006065941 0.038849104 0.10682929 0.17436886 0.22419883 0.25439039 0.26923242 0.27457082 0.2676962 0.24364589 0.19516878 0.12054156 0.036729265 -0.032943565 -0.0832474][-0.039717648 0.0014337846 0.067287616 0.13517027 0.18821773 0.22841975 0.25716388 0.27304867 0.26996747 0.24383654 0.18905 0.10711371 0.018960584 -0.0497949 -0.095349275][-0.02961014 0.0015359154 0.057247471 0.1201874 0.17738417 0.23412538 0.28541481 0.3189469 0.32267976 0.29281691 0.22594731 0.1277301 0.025799338 -0.050618459 -0.097766794][0.012246361 0.030959409 0.07299307 0.12818022 0.18985334 0.26621461 0.34418502 0.39884764 0.41123891 0.37796667 0.29814735 0.18064971 0.058929827 -0.033238631 -0.089337714][0.066765 0.076363429 0.10818505 0.15725614 0.22295031 0.31638813 0.41706485 0.48972338 0.50932944 0.47288913 0.38220704 0.24789166 0.106165 -0.0055798953 -0.07557106][0.11235146 0.11709274 0.14325492 0.18771493 0.25443769 0.35762891 0.47277254 0.55870908 0.58500922 0.54718328 0.44943497 0.30357617 0.1462169 0.016972069 -0.066430993][0.13800846 0.14079182 0.16419144 0.20403291 0.26740211 0.37131938 0.4924109 0.587535 0.62075859 0.58290869 0.48013896 0.32723433 0.16120677 0.02233069 -0.068167888][0.14118476 0.14483289 0.16875407 0.20558962 0.26318327 0.35949072 0.47751024 0.57546103 0.61214888 0.57262623 0.46486086 0.30881193 0.14257136 0.0054102023 -0.08197365][0.12402875 0.13045728 0.15732318 0.19468607 0.24862278 0.33415064 0.44080734 0.53044951 0.56036514 0.51293826 0.39887282 0.24451947 0.088730425 -0.033134174 -0.10573194][0.0850695 0.093090564 0.12159742 0.16056395 0.21278858 0.28687122 0.37529084 0.44436532 0.45598644 0.3959654 0.27917415 0.13663223 0.0050134282 -0.087792464 -0.13553596][0.014728425 0.019527989 0.045640413 0.084497094 0.13515556 0.19850251 0.26646602 0.30966938 0.29895839 0.22895215 0.11946969 0.0018886757 -0.093205571 -0.14780255 -0.16562349][-0.088201553 -0.086025663 -0.062666588 -0.026548047 0.017837869 0.066340521 0.11120982 0.12961173 0.10304027 0.035266925 -0.05158823 -0.13150883 -0.18300954 -0.19815934 -0.18779436][-0.18941511 -0.18755963 -0.16465482 -0.13322058 -0.10037327 -0.071429983 -0.050598543 -0.051608704 -0.083091512 -0.13536817 -0.18904477 -0.22717775 -0.2383734 -0.22276014 -0.19354551][-0.24511056 -0.24423936 -0.22454087 -0.20118962 -0.18183316 -0.17073828 -0.16839539 -0.18006626 -0.20679738 -0.2375873 -0.25955936 -0.26449051 -0.2489571 -0.21775083 -0.18256812]]...]
INFO - root - 2017-12-11 05:42:19.382690: step 15210, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:45m:19s remains)
INFO - root - 2017-12-11 05:42:24.769259: step 15220, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:54m:06s remains)
INFO - root - 2017-12-11 05:42:30.149579: step 15230, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 46h:14m:52s remains)
INFO - root - 2017-12-11 05:42:35.448639: step 15240, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 46h:33m:45s remains)
INFO - root - 2017-12-11 05:42:40.814778: step 15250, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 46h:57m:52s remains)
INFO - root - 2017-12-11 05:42:46.156994: step 15260, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.518 sec/batch; 45h:36m:55s remains)
INFO - root - 2017-12-11 05:42:51.419764: step 15270, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:55m:49s remains)
INFO - root - 2017-12-11 05:42:56.443124: step 15280, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 46h:29m:13s remains)
INFO - root - 2017-12-11 05:43:01.826438: step 15290, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:08m:59s remains)
INFO - root - 2017-12-11 05:43:07.148514: step 15300, loss = 0.68, batch loss = 0.62 (15.7 examples/sec; 0.510 sec/batch; 44h:58m:13s remains)
2017-12-11 05:43:07.697072: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36673775 0.42015129 0.42153645 0.36913103 0.29424179 0.24643739 0.23720507 0.25632626 0.29880524 0.3631649 0.43311203 0.48827642 0.51905435 0.52829057 0.50726116][0.40179116 0.4662424 0.47382107 0.42297617 0.34968269 0.30618489 0.30354351 0.32416549 0.354516 0.39180848 0.42427197 0.4402245 0.43966696 0.42883414 0.40058291][0.42684355 0.49951547 0.515168 0.47266367 0.41079941 0.37846711 0.38370314 0.40241796 0.41211137 0.40818557 0.38720828 0.3529987 0.31922761 0.2940084 0.26765954][0.44084713 0.52053243 0.54698718 0.52050966 0.4796882 0.46495402 0.47939804 0.49476933 0.48433703 0.44310892 0.3760187 0.30072376 0.24051942 0.20436873 0.17990269][0.42511505 0.50849533 0.5490728 0.54838806 0.540537 0.55320692 0.58184677 0.5954929 0.5682227 0.49757192 0.39615652 0.2914198 0.2116784 0.16593644 0.14012265][0.38265961 0.46721423 0.52452481 0.55702394 0.59041226 0.63734579 0.68395215 0.69682372 0.65340823 0.55658132 0.42698243 0.29904458 0.20410439 0.1507345 0.12333468][0.33086166 0.41713077 0.4922764 0.55847806 0.63144034 0.71049637 0.77297181 0.78323209 0.72314864 0.60275185 0.45013937 0.30430886 0.1990688 0.14318153 0.11868507][0.28710294 0.37680307 0.46670896 0.55721825 0.65485525 0.75152725 0.81986642 0.82307535 0.74788004 0.61209863 0.44916838 0.29891196 0.19536415 0.14794941 0.1353883][0.24279925 0.33264041 0.42773831 0.52612674 0.6274175 0.72133154 0.78124529 0.77225661 0.68700546 0.55089784 0.40030104 0.2710216 0.19161831 0.16981944 0.18070169][0.19041954 0.2689859 0.35197508 0.43686157 0.51961023 0.59067476 0.628687 0.60499489 0.51890177 0.40139425 0.28825119 0.20683277 0.17446737 0.1930843 0.23494692][0.12586527 0.18106028 0.23568842 0.28858766 0.33626986 0.37306431 0.38533381 0.35303017 0.2801365 0.19858359 0.13912858 0.11808873 0.13953516 0.19797565 0.26550213][0.053567439 0.077914231 0.095816806 0.10862315 0.11676843 0.12027044 0.11315786 0.083171353 0.035375703 -0.0026750821 -0.0087064328 0.023055019 0.085937276 0.16884351 0.24720582][-0.0060978243 -0.01233698 -0.02847543 -0.049821105 -0.072379313 -0.092130847 -0.10985646 -0.13191481 -0.15414271 -0.15638462 -0.12585112 -0.064362168 0.015887344 0.10212102 0.17573917][-0.029764393 -0.057419464 -0.095021658 -0.13471352 -0.17150028 -0.20058575 -0.2209262 -0.23542547 -0.2410883 -0.22505578 -0.1824441 -0.11902823 -0.046834864 0.023938173 0.081627585][-0.0070943376 -0.042895775 -0.08777976 -0.13134904 -0.16931865 -0.19789159 -0.21595207 -0.2257739 -0.22560397 -0.20840171 -0.17412436 -0.12900543 -0.08106415 -0.035235088 0.0027441026]]...]
INFO - root - 2017-12-11 05:43:13.065772: step 15310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:29m:39s remains)
INFO - root - 2017-12-11 05:43:18.357020: step 15320, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:08m:03s remains)
INFO - root - 2017-12-11 05:43:23.726576: step 15330, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 50h:17m:03s remains)
INFO - root - 2017-12-11 05:43:29.058418: step 15340, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:19m:04s remains)
INFO - root - 2017-12-11 05:43:34.380120: step 15350, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 47h:09m:26s remains)
INFO - root - 2017-12-11 05:43:39.725912: step 15360, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:58m:56s remains)
INFO - root - 2017-12-11 05:43:45.116144: step 15370, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:19m:57s remains)
INFO - root - 2017-12-11 05:43:50.194363: step 15380, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 46h:48m:41s remains)
INFO - root - 2017-12-11 05:43:55.626212: step 15390, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:19m:24s remains)
INFO - root - 2017-12-11 05:44:01.057260: step 15400, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 48h:40m:08s remains)
2017-12-11 05:44:01.577983: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.098188289 0.080567837 0.05892615 0.039882831 0.028525734 0.025913898 0.030421419 0.038760059 0.0459262 0.04543205 0.0388182 0.031786773 0.027585141 0.023065114 0.014302568][0.22036226 0.18960559 0.15358382 0.12560001 0.11268334 0.11633832 0.12930612 0.14254379 0.14858429 0.1402812 0.12419512 0.11030225 0.10339686 0.096856095 0.082666561][0.36341962 0.32285151 0.27869323 0.24895318 0.24055736 0.25560614 0.27844009 0.29392654 0.29327327 0.27108502 0.24065587 0.2168259 0.20564982 0.19589236 0.17475817][0.4817217 0.43848494 0.39683539 0.37533027 0.3781015 0.40774557 0.44060266 0.45611978 0.44459078 0.40536323 0.35983193 0.32719857 0.31317398 0.3007426 0.27285668][0.54971409 0.50818545 0.47337788 0.46330109 0.47955105 0.525804 0.57271773 0.59317815 0.57373369 0.51977658 0.46234369 0.42420334 0.40920392 0.39422184 0.36004508][0.58856517 0.54939443 0.51765263 0.514649 0.54379737 0.61077058 0.68290371 0.72256535 0.70611393 0.64322573 0.5747878 0.52737051 0.5042854 0.47930869 0.43406326][0.61862832 0.5800879 0.54440236 0.53912896 0.5755778 0.66210818 0.76572162 0.8352952 0.83158571 0.76549065 0.68563956 0.62398243 0.58583009 0.545403 0.48710996][0.63173854 0.59619457 0.55511218 0.54198933 0.57859147 0.67422 0.79921538 0.89207816 0.89893621 0.83052617 0.7396996 0.66538334 0.61497217 0.56430638 0.5006395][0.62977546 0.60044324 0.55914837 0.54455167 0.58339578 0.67950749 0.809162 0.908446 0.91643858 0.84424734 0.74522352 0.66307211 0.60434771 0.54705507 0.48199734][0.63086927 0.61506176 0.5823794 0.5730719 0.6116671 0.69439411 0.80767238 0.89513606 0.89829618 0.82730484 0.72952533 0.64687049 0.58198863 0.51715475 0.44838014][0.6385901 0.64462715 0.62725693 0.62281483 0.65009367 0.70234877 0.78095061 0.84435856 0.84340996 0.78407866 0.70101923 0.62825483 0.56260645 0.49163267 0.41661456][0.63448632 0.66359556 0.66359 0.66015577 0.66443336 0.67351872 0.70512909 0.73700434 0.7325232 0.69176066 0.63498235 0.58576161 0.53310293 0.46615154 0.38873526][0.61690372 0.65745872 0.6691907 0.66486406 0.64732695 0.61943316 0.60950238 0.61406362 0.60915911 0.58989531 0.5644092 0.54535466 0.51346064 0.45567179 0.37662789][0.59991783 0.63143569 0.64088786 0.63226807 0.60110182 0.55279249 0.51886743 0.5111503 0.517051 0.52765191 0.53846973 0.54873639 0.53364062 0.47904584 0.39225966][0.60081655 0.61163074 0.60617417 0.5880487 0.54890823 0.49392366 0.45365578 0.44953984 0.47595072 0.52152544 0.56926531 0.60336524 0.59578264 0.533965 0.43140748]]...]
INFO - root - 2017-12-11 05:44:06.949624: step 15410, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.520 sec/batch; 45h:47m:19s remains)
INFO - root - 2017-12-11 05:44:12.342331: step 15420, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.516 sec/batch; 45h:26m:21s remains)
INFO - root - 2017-12-11 05:44:17.738236: step 15430, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 48h:07m:26s remains)
INFO - root - 2017-12-11 05:44:23.023802: step 15440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:10m:39s remains)
INFO - root - 2017-12-11 05:44:28.425183: step 15450, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:19m:23s remains)
INFO - root - 2017-12-11 05:44:33.761708: step 15460, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:38m:28s remains)
INFO - root - 2017-12-11 05:44:39.171219: step 15470, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:45m:30s remains)
INFO - root - 2017-12-11 05:44:44.332706: step 15480, loss = 0.68, batch loss = 0.62 (23.0 examples/sec; 0.349 sec/batch; 30h:41m:32s remains)
INFO - root - 2017-12-11 05:44:49.491214: step 15490, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:43m:29s remains)
INFO - root - 2017-12-11 05:44:54.855235: step 15500, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:13m:47s remains)
2017-12-11 05:44:55.405960: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.44927493 0.44939852 0.45281032 0.45285493 0.44087374 0.41426975 0.37544352 0.33265182 0.29332528 0.25765729 0.23559107 0.23015159 0.24520269 0.27474836 0.33413932][0.58524019 0.59285635 0.58932096 0.57102519 0.53248894 0.47988358 0.42256683 0.37011659 0.33212325 0.30356595 0.28977144 0.29094353 0.31000778 0.34232751 0.41216663][0.64370942 0.657269 0.64329672 0.60347462 0.54050237 0.47156411 0.41213346 0.36768073 0.3450577 0.33188367 0.32585549 0.32669979 0.33937213 0.36340427 0.43113443][0.60310537 0.61849028 0.59909457 0.55086446 0.487389 0.43158779 0.39842203 0.38332379 0.385949 0.388087 0.38003403 0.36437577 0.35156032 0.34906489 0.39569494][0.45573911 0.47524685 0.46773896 0.44123676 0.41953191 0.420119 0.44623318 0.47788396 0.50456327 0.50866628 0.47826904 0.42319566 0.36151046 0.3111175 0.31497166][0.24827637 0.28368482 0.31578287 0.34971681 0.41297272 0.50492424 0.60681796 0.68088 0.70816368 0.68323821 0.60330766 0.48885414 0.36359626 0.25307667 0.20074092][0.051215824 0.11887278 0.21527927 0.33582678 0.50185251 0.68770826 0.8526575 0.94478571 0.9413957 0.85757774 0.70941883 0.52758867 0.33843172 0.17194048 0.068844244][-0.081595309 0.029280093 0.1959137 0.39888388 0.64627069 0.89086086 1.0807807 1.1602976 1.107429 0.95598185 0.74221808 0.50483185 0.27141881 0.073016927 -0.055533573][-0.13013978 0.018873291 0.23636016 0.4882223 0.76618886 1.0117292 1.1769974 1.2152534 1.1103847 0.9072172 0.65527284 0.39583114 0.15589094 -0.03632262 -0.15411682][-0.10376682 0.064085044 0.29851967 0.55533648 0.81016284 1.0036569 1.1019405 1.0775875 0.92798358 0.70306462 0.45608371 0.22196636 0.021633089 -0.12435334 -0.19884907][-0.029225953 0.13822697 0.36009696 0.58685738 0.78274155 0.89607376 0.91023135 0.82015836 0.64214259 0.42964867 0.23121089 0.06680423 -0.056772768 -0.13190793 -0.15157682][0.064046219 0.22214226 0.41774914 0.59816563 0.72321522 0.75574493 0.69546878 0.55620694 0.37305459 0.20287275 0.083133623 0.011855775 -0.025181841 -0.035531756 -0.017764954][0.15174976 0.29916054 0.46723166 0.6007641 0.65899563 0.62354833 0.51041317 0.34845832 0.1862212 0.080036215 0.049002793 0.065640643 0.095000975 0.11883388 0.14244629][0.1958625 0.32818708 0.46888524 0.56203657 0.56816566 0.48703456 0.35161573 0.20053554 0.084120043 0.04933114 0.094159111 0.17023027 0.22992702 0.25185391 0.25396132][0.16458437 0.27255508 0.38352704 0.44503543 0.42176643 0.32535237 0.20068961 0.088166252 0.028749827 0.053954158 0.14500083 0.24392061 0.30168423 0.29981861 0.27084428]]...]
INFO - root - 2017-12-11 05:45:00.781775: step 15510, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:50m:29s remains)
INFO - root - 2017-12-11 05:45:06.112332: step 15520, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:46m:42s remains)
INFO - root - 2017-12-11 05:45:11.359477: step 15530, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:39m:15s remains)
INFO - root - 2017-12-11 05:45:16.706139: step 15540, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:14m:16s remains)
INFO - root - 2017-12-11 05:45:22.058155: step 15550, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:39m:28s remains)
INFO - root - 2017-12-11 05:45:27.435874: step 15560, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.528 sec/batch; 46h:28m:45s remains)
INFO - root - 2017-12-11 05:45:32.800388: step 15570, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:28m:02s remains)
INFO - root - 2017-12-11 05:45:38.224080: step 15580, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 49h:23m:33s remains)
INFO - root - 2017-12-11 05:45:43.309529: step 15590, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 49h:15m:12s remains)
INFO - root - 2017-12-11 05:45:48.586287: step 15600, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:35m:37s remains)
2017-12-11 05:45:49.134112: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052255936 0.076654732 0.11662928 0.15630189 0.17541821 0.1670384 0.1371738 0.10164918 0.0782254 0.072770618 0.0779904 0.073699176 0.047611397 0.0075412565 -0.033160675][0.056956552 0.09094815 0.14087668 0.18669754 0.20609397 0.19232859 0.15250911 0.10371453 0.066535369 0.051238317 0.052950658 0.050403342 0.029001847 -0.0054866089 -0.041092254][0.040202059 0.086032681 0.14922978 0.20491111 0.22919072 0.2165886 0.17465678 0.11934545 0.071510114 0.044662464 0.037293185 0.029782739 0.0079859244 -0.022799637 -0.052245334][0.006468466 0.064892411 0.14463803 0.21600716 0.25329357 0.25115043 0.21700266 0.16304798 0.1088645 0.069682807 0.047868479 0.027668038 -0.0022905504 -0.035087552 -0.061752532][-0.028983271 0.039029937 0.13303803 0.22046037 0.27446607 0.28915358 0.27107954 0.22615881 0.17085107 0.11973519 0.079652146 0.040757366 -0.0036784708 -0.043453597 -0.070641026][-0.054792583 0.017630337 0.11888117 0.21675247 0.28502187 0.31788978 0.32070094 0.29200026 0.24284984 0.18467896 0.1279785 0.069140516 0.0068218769 -0.044342972 -0.076271966][-0.065826118 0.0063935588 0.1071045 0.20692469 0.28237507 0.32892495 0.35151693 0.34309992 0.30777329 0.25134394 0.18521629 0.11045891 0.030758226 -0.034251291 -0.07471057][-0.060990922 0.0087897684 0.10374203 0.19762957 0.27027178 0.31900576 0.3504149 0.35624385 0.3354415 0.28771132 0.22187293 0.14080772 0.050991755 -0.024142114 -0.072201677][-0.04320585 0.023745485 0.11101584 0.19454943 0.25562325 0.29237658 0.31448147 0.31878582 0.304185 0.26691058 0.21076098 0.13653368 0.04982328 -0.025719885 -0.075680993][-0.019983225 0.045155562 0.12605734 0.19931817 0.24510564 0.26084834 0.2594547 0.2475877 0.2292679 0.20111924 0.16164541 0.10601451 0.035067637 -0.031575371 -0.078401908][0.0017224427 0.066120692 0.14323634 0.20927946 0.24182618 0.23629974 0.20799458 0.17384829 0.14575613 0.12401741 0.10301754 0.07107605 0.021950189 -0.031368375 -0.073371239][0.02272585 0.08808811 0.16377418 0.22537221 0.24872844 0.22704609 0.17701603 0.12339843 0.085558213 0.068081759 0.061951458 0.049712572 0.019185403 -0.022896772 -0.061830569][0.04590692 0.11458696 0.19126762 0.25089255 0.26923385 0.23827294 0.17543487 0.10994481 0.066712752 0.052850429 0.056243669 0.055358671 0.034827679 -0.0025825691 -0.043019194][0.067762315 0.14097932 0.22017455 0.27971441 0.29653594 0.26240718 0.19551453 0.12667967 0.084295377 0.075886816 0.086174756 0.090715781 0.072405696 0.032381773 -0.015483819][0.081628747 0.15714486 0.23689383 0.29526064 0.3113952 0.2774877 0.21254729 0.14705534 0.11086826 0.11035926 0.12706615 0.13468486 0.11500756 0.069047078 0.011412911]]...]
INFO - root - 2017-12-11 05:45:54.537028: step 15610, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:42m:07s remains)
INFO - root - 2017-12-11 05:45:59.853239: step 15620, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:16m:04s remains)
INFO - root - 2017-12-11 05:46:05.148249: step 15630, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:17m:11s remains)
INFO - root - 2017-12-11 05:46:10.410350: step 15640, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:31m:02s remains)
INFO - root - 2017-12-11 05:46:15.755812: step 15650, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 46h:59m:27s remains)
INFO - root - 2017-12-11 05:46:21.125148: step 15660, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 45h:46m:18s remains)
INFO - root - 2017-12-11 05:46:26.379625: step 15670, loss = 0.70, batch loss = 0.65 (15.9 examples/sec; 0.505 sec/batch; 44h:24m:23s remains)
INFO - root - 2017-12-11 05:46:31.731399: step 15680, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:14m:36s remains)
INFO - root - 2017-12-11 05:46:36.845709: step 15690, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.554 sec/batch; 48h:43m:00s remains)
INFO - root - 2017-12-11 05:46:42.213409: step 15700, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 47h:05m:39s remains)
2017-12-11 05:46:42.778622: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15776539 0.17615095 0.19784079 0.22467987 0.25596148 0.28914577 0.31221369 0.31456217 0.29295847 0.251294 0.19673201 0.13994572 0.09301576 0.0664463 0.070047922][0.23001425 0.2515105 0.27413669 0.30307287 0.33767527 0.37572676 0.40429005 0.41209444 0.39486167 0.35375613 0.29316095 0.22597454 0.16683157 0.12799749 0.12204006][0.30326235 0.32437384 0.34353811 0.37283349 0.41208133 0.45731589 0.49258617 0.50541961 0.492389 0.45345554 0.39112279 0.31886074 0.25067896 0.19800191 0.17570005][0.37363216 0.38425642 0.39331686 0.42192161 0.4710134 0.53131938 0.57819241 0.59408963 0.57707047 0.53014857 0.45997977 0.38212603 0.30778998 0.24444032 0.20660758][0.44792369 0.43894076 0.42933571 0.45295498 0.51341343 0.59419972 0.65764517 0.67667204 0.64915663 0.58295494 0.49376056 0.40244302 0.31870314 0.24601971 0.19590633][0.52482706 0.49098304 0.45661387 0.46929133 0.53727096 0.63768584 0.71963304 0.74475735 0.70814151 0.62074614 0.50801647 0.39710584 0.29914549 0.21636687 0.15649286][0.5753572 0.52042621 0.46462113 0.46724635 0.54230297 0.66190577 0.76287466 0.79549 0.75084245 0.64243424 0.50593281 0.37591112 0.26645043 0.17928246 0.1160841][0.5868271 0.51608986 0.44594052 0.44136018 0.52391165 0.663199 0.78544974 0.82847285 0.77651578 0.6459229 0.48399684 0.33636165 0.22207338 0.14057906 0.083850466][0.58973861 0.50954884 0.42905778 0.41358367 0.49383196 0.64160818 0.77810162 0.83020204 0.77297294 0.62475467 0.44216481 0.28240111 0.17069398 0.10270005 0.059685413][0.59254277 0.50941128 0.42209047 0.39146039 0.45421007 0.58801425 0.71745521 0.76777011 0.70886314 0.55914593 0.37680763 0.22319977 0.12635189 0.07774549 0.051236194][0.57812393 0.499949 0.41427431 0.37310842 0.4121848 0.51480651 0.61497438 0.64656442 0.58273244 0.44381672 0.28222939 0.15405121 0.084248342 0.060114749 0.052436754][0.56627613 0.49883169 0.42049628 0.37289244 0.38554743 0.44605887 0.50151569 0.50021416 0.4239274 0.29615846 0.16089265 0.064527906 0.025596254 0.028107911 0.041902315][0.59076697 0.53592873 0.46423036 0.40737122 0.3876459 0.39695218 0.39953837 0.36096749 0.27285272 0.15777554 0.049110606 -0.018350739 -0.032966837 -0.011465531 0.017737854][0.63559222 0.59382516 0.52606475 0.45651659 0.40131775 0.35709625 0.3076435 0.23702185 0.14631696 0.051541559 -0.02669327 -0.067036189 -0.064899258 -0.0354117 -0.0020320741][0.64627957 0.61832756 0.55775285 0.48175588 0.40196094 0.31893185 0.2328728 0.14431715 0.061620638 -0.008080964 -0.056561708 -0.074655406 -0.063133635 -0.034093462 -0.0043167421]]...]
INFO - root - 2017-12-11 05:46:48.163362: step 15710, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:36m:01s remains)
INFO - root - 2017-12-11 05:46:53.515404: step 15720, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:52m:55s remains)
INFO - root - 2017-12-11 05:46:58.898227: step 15730, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:24m:41s remains)
INFO - root - 2017-12-11 05:47:04.187380: step 15740, loss = 0.69, batch loss = 0.63 (15.8 examples/sec; 0.508 sec/batch; 44h:40m:11s remains)
INFO - root - 2017-12-11 05:47:09.518013: step 15750, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:58m:36s remains)
INFO - root - 2017-12-11 05:47:14.894225: step 15760, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:07m:52s remains)
INFO - root - 2017-12-11 05:47:20.206298: step 15770, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 46h:21m:10s remains)
INFO - root - 2017-12-11 05:47:25.566398: step 15780, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:27m:38s remains)
INFO - root - 2017-12-11 05:47:30.627613: step 15790, loss = 0.71, batch loss = 0.65 (21.9 examples/sec; 0.365 sec/batch; 32h:06m:29s remains)
INFO - root - 2017-12-11 05:47:35.951322: step 15800, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:45m:49s remains)
2017-12-11 05:47:36.542470: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23280685 0.21926972 0.18670794 0.14523897 0.10994852 0.098072261 0.12024514 0.17111507 0.23852324 0.30730554 0.36229676 0.38242531 0.36775228 0.32861984 0.27908814][0.18379542 0.17634977 0.15160966 0.11560563 0.085198365 0.080060355 0.11200225 0.17398062 0.24766716 0.31289852 0.3482036 0.33991402 0.29624692 0.23421703 0.17154336][0.14477229 0.14247461 0.12420683 0.097466327 0.080150194 0.09030176 0.13636377 0.20832993 0.28186658 0.32949719 0.33092153 0.28251374 0.20480017 0.12323447 0.058100313][0.13774341 0.13636325 0.12132619 0.10558585 0.10778558 0.14192997 0.21022984 0.29688719 0.3703399 0.39592141 0.35587624 0.26062045 0.14647272 0.049725529 -0.0093686339][0.15039924 0.14809555 0.13677679 0.13529408 0.16166358 0.22367118 0.31567428 0.41423428 0.48100722 0.47843328 0.39592603 0.25993305 0.12059579 0.020837652 -0.023155514][0.17435056 0.17271458 0.16778697 0.1828848 0.23395476 0.32013214 0.42654124 0.52168739 0.56530035 0.52654314 0.4084886 0.25122175 0.11004927 0.025687443 0.0058623394][0.20968379 0.21031037 0.2130447 0.24314536 0.31255025 0.41054934 0.51268363 0.58302647 0.58808208 0.51318389 0.3773061 0.22599052 0.10854971 0.054713756 0.061709978][0.24952857 0.25376284 0.26347378 0.30311567 0.37782797 0.46835819 0.54429692 0.57372868 0.53769308 0.44079676 0.31234682 0.19219464 0.11537076 0.096528664 0.1245406][0.27782357 0.28628346 0.30098712 0.3416706 0.40533534 0.46880361 0.50381875 0.49070016 0.42678648 0.33068269 0.2324907 0.15846677 0.12686703 0.13868086 0.17885162][0.27742708 0.29028809 0.30765527 0.3414731 0.38289648 0.41120005 0.40734085 0.36602327 0.29599935 0.22055854 0.16351753 0.13736978 0.14690037 0.18312813 0.22933012][0.24451759 0.25965369 0.27757564 0.30199739 0.32211536 0.32380435 0.29832774 0.24989153 0.19156408 0.14458701 0.1252705 0.13731053 0.17734504 0.23062047 0.2789087][0.18330601 0.19793157 0.21529782 0.23403473 0.2439968 0.2367809 0.20945534 0.16883922 0.12849185 0.10596649 0.11264725 0.14845598 0.20629887 0.26916987 0.31745374][0.10860276 0.11975919 0.13593934 0.15393828 0.16457227 0.1617357 0.14335957 0.11498439 0.0901659 0.084020369 0.10581107 0.15429682 0.22051176 0.28651577 0.33191532][0.040474229 0.045812078 0.059181429 0.076919772 0.090045691 0.092875436 0.082897924 0.064240262 0.050659772 0.0556756 0.086701833 0.14115518 0.20816718 0.26957476 0.30648217][-0.010741669 -0.010808731 -0.001927828 0.011968545 0.022748144 0.025809152 0.019292634 0.0064534685 0.000347538 0.012219458 0.046605933 0.09893766 0.15739997 0.20570783 0.22924876]]...]
INFO - root - 2017-12-11 05:47:41.869369: step 15810, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:53m:22s remains)
INFO - root - 2017-12-11 05:47:47.274626: step 15820, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 47h:38m:18s remains)
INFO - root - 2017-12-11 05:47:52.532033: step 15830, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 46h:00m:56s remains)
INFO - root - 2017-12-11 05:47:57.801023: step 15840, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 46h:11m:36s remains)
INFO - root - 2017-12-11 05:48:03.134719: step 15850, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:53m:07s remains)
INFO - root - 2017-12-11 05:48:08.429099: step 15860, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 45h:37m:04s remains)
INFO - root - 2017-12-11 05:48:13.751696: step 15870, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:49m:01s remains)
INFO - root - 2017-12-11 05:48:19.234953: step 15880, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:06m:50s remains)
INFO - root - 2017-12-11 05:48:24.679181: step 15890, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:15m:09s remains)
INFO - root - 2017-12-11 05:48:29.758910: step 15900, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:30m:48s remains)
2017-12-11 05:48:30.314531: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32858238 0.31840065 0.30389619 0.29457843 0.28264809 0.26306263 0.23342644 0.19746363 0.16456312 0.14848021 0.15341514 0.17066237 0.18077073 0.1688574 0.1398506][0.34157458 0.33247232 0.31162655 0.29033363 0.26784128 0.24596395 0.22578217 0.20787571 0.19798549 0.20061985 0.21441707 0.22905174 0.22890517 0.20369379 0.16037002][0.3192364 0.31309187 0.2881543 0.25821969 0.22998926 0.21260306 0.20901202 0.2176059 0.23898788 0.26738539 0.29269597 0.30308002 0.28784913 0.24396285 0.18239142][0.2606537 0.25691158 0.23397547 0.20618135 0.18459438 0.1820914 0.2005282 0.23496063 0.28320935 0.33228695 0.3650274 0.36824772 0.33627504 0.27366719 0.19597338][0.16885018 0.16938889 0.15771553 0.14706691 0.14846085 0.17255273 0.21596256 0.26872146 0.32788014 0.37936464 0.40551633 0.39538032 0.34794572 0.27292439 0.18805824][0.053876176 0.063094564 0.072912924 0.094411008 0.1346024 0.19612472 0.26719853 0.33070239 0.38263986 0.41367242 0.41451195 0.38191238 0.31919727 0.23800632 0.15421435][-0.055276141 -0.037010789 -0.0018639375 0.057643238 0.14271888 0.24521333 0.34379703 0.41257429 0.44402936 0.43744424 0.39808327 0.33518055 0.25700429 0.17521274 0.10043678][-0.11027417 -0.090093158 -0.038690872 0.048164681 0.16672629 0.29938337 0.41770548 0.48683849 0.49358076 0.44517815 0.3623893 0.26814711 0.17715773 0.10087574 0.043385867][-0.080760606 -0.0690171 -0.022018457 0.065534852 0.18885215 0.32600123 0.44607151 0.50877839 0.49538672 0.41713923 0.30528012 0.19284385 0.10078275 0.039941203 0.0075168307][0.011635018 0.00672551 0.029089974 0.089193687 0.18641663 0.30086884 0.4039385 0.4549886 0.43089613 0.34266755 0.22560054 0.11697806 0.040811509 0.0052013933 0.0011404877][0.10691901 0.08151754 0.068896979 0.0854211 0.13726644 0.21227106 0.28811464 0.32796851 0.30521697 0.2271544 0.12763315 0.042649146 -0.0050589489 -0.011516015 0.0095300721][0.1501766 0.10731152 0.06487 0.040846 0.046534203 0.079188265 0.1253458 0.15503927 0.14113319 0.08577922 0.017495172 -0.032996278 -0.047595613 -0.02712605 0.011747613][0.11830967 0.0674493 0.011113364 -0.035585839 -0.060352396 -0.059540793 -0.038318109 -0.018008156 -0.021715254 -0.051263079 -0.085058764 -0.099346042 -0.082421385 -0.040519387 0.0077110296][0.034103457 -0.016661821 -0.071893312 -0.1215281 -0.15524758 -0.16832511 -0.16164581 -0.14844064 -0.14479804 -0.15341744 -0.15928581 -0.14546849 -0.10642012 -0.052908737 -0.004057135][-0.057334367 -0.10191713 -0.14649412 -0.18569136 -0.21214741 -0.22299193 -0.21900533 -0.20861632 -0.20153001 -0.19868532 -0.18954086 -0.16271596 -0.11720596 -0.0652952 -0.024294984]]...]
INFO - root - 2017-12-11 05:48:35.579929: step 15910, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 47h:55m:49s remains)
INFO - root - 2017-12-11 05:48:40.934647: step 15920, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:30m:23s remains)
INFO - root - 2017-12-11 05:48:46.368137: step 15930, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 48h:19m:45s remains)
INFO - root - 2017-12-11 05:48:51.791583: step 15940, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.542 sec/batch; 47h:41m:44s remains)
INFO - root - 2017-12-11 05:48:57.082846: step 15950, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 46h:52m:58s remains)
INFO - root - 2017-12-11 05:49:02.462816: step 15960, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.517 sec/batch; 45h:29m:52s remains)
INFO - root - 2017-12-11 05:49:07.827798: step 15970, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 45h:37m:09s remains)
INFO - root - 2017-12-11 05:49:13.119149: step 15980, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:39m:34s remains)
INFO - root - 2017-12-11 05:49:18.437044: step 15990, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:07m:27s remains)
INFO - root - 2017-12-11 05:49:23.475852: step 16000, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:05m:45s remains)
2017-12-11 05:49:24.005531: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26768026 0.22613856 0.17838487 0.14081798 0.11753711 0.11117541 0.121245 0.13913345 0.16266856 0.20231794 0.25744221 0.31296441 0.351535 0.37120247 0.37582815][0.311126 0.26113206 0.20299736 0.15883394 0.13331909 0.12566879 0.13393588 0.15159848 0.17807162 0.22294681 0.28387067 0.34420374 0.38283986 0.39748275 0.39620936][0.32024804 0.27319479 0.21891014 0.18170168 0.16356936 0.15930104 0.16538829 0.17935897 0.20282969 0.24296078 0.29646346 0.34815148 0.37635738 0.38005325 0.37117243][0.31048307 0.27635753 0.23586018 0.21369636 0.20935591 0.21443819 0.22302379 0.2357122 0.2544758 0.28335539 0.31924129 0.35182071 0.36306557 0.35438979 0.33997285][0.29843175 0.27803245 0.25163272 0.24311188 0.25245768 0.27182463 0.29194021 0.31181538 0.328935 0.34273171 0.35314831 0.35855561 0.34953082 0.33071622 0.31584343][0.29807389 0.28687522 0.26719126 0.265043 0.28475007 0.32210907 0.36313465 0.39869109 0.41672951 0.41273969 0.39396641 0.37027979 0.34333271 0.32003593 0.30992439][0.3293522 0.3180759 0.29347572 0.28654 0.30908814 0.36119831 0.42356867 0.475906 0.49566853 0.4770391 0.43541017 0.39043653 0.35249317 0.32985848 0.32639283][0.40031531 0.37925339 0.33860403 0.31576514 0.33046412 0.38595271 0.45859194 0.51960665 0.54032838 0.51547945 0.46438968 0.41087306 0.37020579 0.35140833 0.35302743][0.4898251 0.45321012 0.39141136 0.34865034 0.34938344 0.3962529 0.46312973 0.51750576 0.53210044 0.50444537 0.45354536 0.4019784 0.36531758 0.35103846 0.35318118][0.55874836 0.50626475 0.42488876 0.36605135 0.35401484 0.38634041 0.43582484 0.47014603 0.46891531 0.43439487 0.38472652 0.33918914 0.30995667 0.30014631 0.30028677][0.57140923 0.50567275 0.41317996 0.34736764 0.3275429 0.34517887 0.37359866 0.38412166 0.36438087 0.3208912 0.27116954 0.231507 0.20957005 0.20338191 0.2017336][0.503876 0.43389717 0.34397346 0.28312555 0.26257837 0.26975998 0.28076369 0.2730774 0.24148002 0.19403143 0.14681743 0.11327071 0.09763857 0.094811268 0.094008334][0.37121028 0.3084031 0.23528092 0.19112928 0.17821671 0.18150677 0.18121473 0.16193567 0.12505056 0.0796897 0.040405519 0.016916612 0.0099507719 0.012659493 0.016346727][0.2228986 0.17796555 0.13104504 0.10878339 0.10646537 0.10847297 0.1004921 0.074872717 0.038756587 0.0024784224 -0.02187153 -0.029577598 -0.024205213 -0.013716961 -0.0035328218][0.11102037 0.090324447 0.070789248 0.066378795 0.069911443 0.068643659 0.05473895 0.028088415 -0.00018373299 -0.020387838 -0.024947427 -0.014580439 0.0038807031 0.022498026 0.03920234]]...]
INFO - root - 2017-12-11 05:49:29.445622: step 16010, loss = 0.70, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 50h:26m:22s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:49:34.833155: step 16020, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:28m:04s remains)
INFO - root - 2017-12-11 05:49:40.179064: step 16030, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:02m:52s remains)
INFO - root - 2017-12-11 05:49:45.469523: step 16040, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 45h:13m:13s remains)
INFO - root - 2017-12-11 05:49:50.829134: step 16050, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:10m:45s remains)
INFO - root - 2017-12-11 05:49:56.187251: step 16060, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:12m:52s remains)
INFO - root - 2017-12-11 05:50:01.573949: step 16070, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 47h:36m:31s remains)
INFO - root - 2017-12-11 05:50:07.010852: step 16080, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:50m:19s remains)
INFO - root - 2017-12-11 05:50:12.333386: step 16090, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:38m:41s remains)
INFO - root - 2017-12-11 05:50:17.500835: step 16100, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 48h:13m:05s remains)
2017-12-11 05:50:18.035738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022536187 -0.02574634 -0.03002107 -0.035840861 -0.041080687 -0.044898298 -0.047138218 -0.047600694 -0.046034016 -0.042937405 -0.038916834 -0.035370495 -0.033815604 -0.035483439 -0.039709147][-0.0011085472 -0.00269666 -0.0074602179 -0.015623189 -0.024312794 -0.031889502 -0.037689637 -0.0403565 -0.039320085 -0.035188626 -0.028340939 -0.021947449 -0.0187603 -0.021161584 -0.028138176][0.028867273 0.033791739 0.033801585 0.02781613 0.018938977 0.008559077 -0.0016026138 -0.0087214485 -0.011008939 -0.0086103389 -0.0014862343 0.0056414786 0.0087856939 0.0041153277 -0.0072187549][0.062484749 0.080199488 0.092292942 0.096171632 0.094000809 0.085385457 0.073268212 0.062323403 0.055349011 0.052466374 0.054463416 0.056213915 0.0534888 0.041547447 0.02183784][0.096386589 0.13209781 0.16314986 0.18498723 0.19766967 0.19733134 0.18820666 0.17731622 0.16705362 0.15703663 0.14838748 0.13686924 0.11951763 0.092220776 0.057835493][0.12319355 0.17780247 0.23034948 0.27481428 0.30820984 0.32220033 0.32194239 0.31592664 0.30416712 0.2859126 0.26228693 0.23142043 0.19278271 0.14419633 0.09103588][0.1311731 0.19858065 0.26703918 0.33078986 0.38383797 0.41406024 0.42669448 0.42937404 0.41824645 0.39240775 0.35322598 0.30255818 0.24280198 0.17439933 0.10564164][0.11387394 0.18102252 0.25083086 0.32043445 0.38272032 0.42450264 0.45020056 0.46368495 0.45665994 0.427983 0.37951833 0.3166903 0.24446914 0.16599295 0.091737382][0.081754856 0.13479067 0.18840438 0.24496339 0.29926851 0.34078023 0.37300283 0.39534247 0.39608577 0.37274325 0.32683596 0.26538074 0.19481388 0.12023172 0.052893054][0.057437539 0.089128539 0.11487946 0.14307491 0.17283835 0.19925804 0.22629702 0.25075257 0.25968385 0.24820189 0.21624546 0.16942002 0.11385891 0.055365033 0.0044838451][0.052460264 0.06537661 0.063252538 0.059136957 0.056775298 0.058209877 0.069698565 0.088707 0.1038763 0.10715877 0.09598159 0.071686149 0.038103089 0.00041930296 -0.032339316][0.060935441 0.062933929 0.042174824 0.013899467 -0.015111701 -0.037462983 -0.044171862 -0.034513392 -0.016124053 -0.00010699892 0.0083062733 0.0055469288 -0.0076475875 -0.027694039 -0.047435064][0.064172737 0.0639946 0.038062178 0.00030390263 -0.041640867 -0.077900916 -0.097823687 -0.097298957 -0.080013111 -0.058199625 -0.03864149 -0.027969828 -0.027802035 -0.03638418 -0.048634317][0.046741281 0.049364924 0.029283002 -0.0036123248 -0.0433527 -0.080689289 -0.1049894 -0.110254 -0.097332157 -0.077381827 -0.056787841 -0.042288758 -0.036446918 -0.03923684 -0.047598369][0.011545372 0.016476234 0.0062479032 -0.014212246 -0.041877277 -0.070238009 -0.090475 -0.096804231 -0.088698126 -0.074869722 -0.059914049 -0.048553467 -0.042909678 -0.0437941 -0.049828667]]...]
INFO - root - 2017-12-11 05:50:23.391116: step 16110, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:12m:18s remains)
INFO - root - 2017-12-11 05:50:28.694937: step 16120, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:30m:59s remains)
INFO - root - 2017-12-11 05:50:33.983911: step 16130, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 46h:04m:18s remains)
INFO - root - 2017-12-11 05:50:39.374542: step 16140, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 45h:45m:55s remains)
INFO - root - 2017-12-11 05:50:44.789689: step 16150, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 48h:25m:52s remains)
INFO - root - 2017-12-11 05:50:50.127568: step 16160, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:01m:50s remains)
INFO - root - 2017-12-11 05:50:55.511189: step 16170, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:46m:22s remains)
INFO - root - 2017-12-11 05:51:00.828417: step 16180, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 47h:13m:04s remains)
INFO - root - 2017-12-11 05:51:06.168190: step 16190, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:36m:28s remains)
INFO - root - 2017-12-11 05:51:11.486533: step 16200, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 48h:38m:45s remains)
2017-12-11 05:51:11.982269: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14389412 0.14500675 0.15178111 0.16067164 0.16536774 0.16224761 0.15673704 0.1562258 0.16635002 0.18674797 0.20629814 0.2146074 0.20620014 0.18385489 0.15664747][0.16435842 0.16698353 0.17540216 0.18677266 0.19205113 0.18587889 0.17474717 0.1694124 0.17641239 0.19379136 0.20904571 0.21216463 0.19895966 0.17345725 0.14577973][0.17000143 0.17364731 0.18222639 0.19558103 0.20299862 0.19692723 0.1828718 0.17330973 0.17452592 0.18317139 0.18779244 0.18193199 0.16503888 0.14241114 0.12242149][0.16392624 0.1689845 0.17783667 0.19448508 0.20789936 0.20765753 0.19560917 0.18292546 0.17563581 0.17067254 0.1611415 0.14682686 0.13109557 0.11867524 0.11285767][0.15088587 0.15583077 0.16348444 0.18260323 0.20341387 0.21282502 0.20747058 0.19456136 0.17875858 0.15811847 0.1330523 0.11184047 0.101292 0.10309546 0.11271159][0.14128144 0.14323191 0.14610271 0.1630955 0.18766408 0.20622523 0.21129388 0.20431453 0.18604091 0.1541725 0.11636177 0.089665517 0.084152319 0.098172948 0.11970412][0.1312588 0.1305251 0.12838185 0.14144804 0.16753863 0.19434458 0.21138605 0.2140419 0.19806682 0.16003893 0.11307859 0.081273243 0.077430256 0.09733852 0.1244373][0.11808319 0.11765368 0.11479952 0.12597895 0.1528258 0.18491748 0.21005079 0.21952391 0.20564038 0.16482532 0.11315191 0.078020327 0.073359 0.093110308 0.1192632][0.10352238 0.10659662 0.10677782 0.11835901 0.14481404 0.177122 0.20322374 0.21372858 0.19998413 0.15853812 0.10568488 0.06894774 0.062205058 0.077714644 0.097968161][0.086179644 0.092856176 0.097383887 0.11030798 0.13434941 0.16160986 0.18222807 0.18938139 0.17539188 0.13643339 0.086384848 0.050372254 0.041706331 0.051527649 0.064167783][0.066046633 0.075093925 0.083006896 0.09659934 0.11665358 0.1359656 0.14740767 0.14813855 0.13341135 0.099665873 0.056727748 0.024815179 0.01565755 0.02043375 0.026545396][0.04230123 0.052684657 0.06338416 0.0775656 0.094214775 0.10659578 0.10956375 0.10375319 0.0880379 0.06023474 0.0263323 0.00067943387 -0.007322676 -0.0055341781 -0.0031019766][0.018885156 0.028971404 0.039857689 0.051669702 0.06264177 0.067843966 0.064459667 0.055211019 0.041089129 0.020817421 -0.0026637579 -0.020156473 -0.024416536 -0.021739792 -0.018249355][-0.0052881748 0.0021100445 0.00986283 0.016100744 0.019770039 0.01881572 0.012537131 0.00391763 -0.0057876464 -0.017598774 -0.03049661 -0.039217509 -0.038615759 -0.033263195 -0.026768917][-0.0297157 -0.027277375 -0.025002891 -0.025234211 -0.027561594 -0.031517979 -0.036746077 -0.041250437 -0.044518471 -0.047479957 -0.050364602 -0.051135462 -0.046876069 -0.040166549 -0.032179754]]...]
INFO - root - 2017-12-11 05:51:17.054109: step 16210, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:55m:02s remains)
INFO - root - 2017-12-11 05:51:22.472692: step 16220, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 46h:02m:34s remains)
INFO - root - 2017-12-11 05:51:27.991807: step 16230, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 50h:45m:11s remains)
INFO - root - 2017-12-11 05:51:33.344704: step 16240, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:08m:55s remains)
INFO - root - 2017-12-11 05:51:38.718023: step 16250, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:14m:50s remains)
INFO - root - 2017-12-11 05:51:44.096657: step 16260, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 46h:02m:57s remains)
INFO - root - 2017-12-11 05:51:49.413443: step 16270, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.535 sec/batch; 47h:01m:51s remains)
INFO - root - 2017-12-11 05:51:54.724340: step 16280, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 46h:10m:36s remains)
INFO - root - 2017-12-11 05:52:00.094295: step 16290, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:39m:42s remains)
INFO - root - 2017-12-11 05:52:05.462664: step 16300, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 46h:22m:44s remains)
2017-12-11 05:52:06.019348: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.044346225 0.039371815 0.030186431 0.02273538 0.019399652 0.022412915 0.036617503 0.06389533 0.099460423 0.12885426 0.14098532 0.1303193 0.095292807 0.045834865 -0.003957375][0.037590541 0.032855451 0.022796474 0.013959762 0.010617079 0.01714476 0.0365533 0.069085792 0.10936832 0.14088276 0.15193129 0.13786669 0.098694779 0.04541415 -0.0074332338][0.02869755 0.023477236 0.013687182 0.0064459383 0.0084022637 0.023244577 0.051981114 0.09210071 0.13667509 0.16763377 0.17310117 0.15086664 0.10443167 0.046825897 -0.0081562735][0.027109545 0.022731373 0.015264561 0.013412102 0.025377449 0.051803026 0.091173284 0.13937941 0.18912937 0.22042421 0.21907972 0.18526438 0.12760085 0.061648205 0.00055570318][0.036052842 0.03757675 0.037715573 0.046067055 0.071783833 0.1104412 0.15723526 0.2091859 0.26219088 0.29522827 0.28822303 0.24182311 0.17071883 0.092942469 0.021011274][0.057399236 0.071479462 0.084256835 0.1053023 0.14550564 0.1955317 0.24577849 0.29593006 0.34812057 0.382329 0.36992756 0.30995956 0.22380947 0.13245101 0.047742873][0.089117825 0.11891081 0.14473253 0.17545564 0.22526553 0.28330347 0.33420014 0.3791112 0.425916 0.4576731 0.43783253 0.36270517 0.2607508 0.15715492 0.06368883][0.11882229 0.16018692 0.19334143 0.22635823 0.277008 0.33708376 0.38657412 0.42522618 0.46311519 0.48723948 0.45732671 0.36791763 0.25378698 0.14476615 0.052256603][0.1311522 0.1741652 0.20625649 0.23470393 0.27856532 0.33435687 0.38011396 0.41100648 0.43613997 0.44797596 0.40888888 0.31306317 0.19773945 0.095363222 0.015556031][0.11898909 0.15262316 0.17591479 0.19587782 0.2291626 0.27655336 0.31684127 0.33902979 0.34967512 0.34864131 0.30609432 0.2165089 0.11406608 0.029693937 -0.029346654][0.085465014 0.10288137 0.11339784 0.12408087 0.14562447 0.18172979 0.21522701 0.23036274 0.2298485 0.2201906 0.18226099 0.11081134 0.031598561 -0.030217309 -0.068325736][0.044902477 0.04623206 0.045939554 0.050019208 0.061569065 0.085021958 0.11043734 0.1207922 0.11489458 0.10194315 0.073125631 0.023542309 -0.031882819 -0.074764647 -0.09802869][0.017308617 0.0087512741 0.0022880288 0.0022195303 0.0059194523 0.017310735 0.03468379 0.043007173 0.038771898 0.029280016 0.012208208 -0.017151669 -0.054300603 -0.086281851 -0.10479123][0.012858205 0.00240868 -0.0055250782 -0.0081235776 -0.010423136 -0.0086771315 0.002347826 0.011918438 0.015424628 0.015316721 0.010806135 -0.0032246253 -0.030069711 -0.0603773 -0.084086828][0.025576597 0.020396119 0.015616289 0.012240716 0.0049600834 -0.0012689675 0.0044513945 0.016632175 0.030058274 0.040971585 0.046413552 0.04082955 0.017103577 -0.017789045 -0.051889818]]...]
INFO - root - 2017-12-11 05:52:11.118047: step 16310, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:46m:31s remains)
INFO - root - 2017-12-11 05:52:16.385731: step 16320, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:43m:08s remains)
INFO - root - 2017-12-11 05:52:21.648684: step 16330, loss = 0.68, batch loss = 0.62 (15.9 examples/sec; 0.504 sec/batch; 44h:14m:14s remains)
INFO - root - 2017-12-11 05:52:27.051613: step 16340, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 46h:46m:30s remains)
INFO - root - 2017-12-11 05:52:32.288194: step 16350, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:33m:13s remains)
INFO - root - 2017-12-11 05:52:37.718899: step 16360, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.522 sec/batch; 45h:49m:19s remains)
INFO - root - 2017-12-11 05:52:43.081479: step 16370, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:04m:02s remains)
INFO - root - 2017-12-11 05:52:48.423648: step 16380, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 45h:37m:55s remains)
INFO - root - 2017-12-11 05:52:53.763172: step 16390, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:26m:35s remains)
INFO - root - 2017-12-11 05:52:59.128197: step 16400, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 46h:19m:17s remains)
2017-12-11 05:52:59.696542: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20075013 0.24077007 0.30975127 0.41101167 0.53008908 0.62425321 0.65952718 0.6268301 0.53305429 0.40131661 0.25933778 0.13813329 0.048026964 -0.014485703 -0.054918315][0.193663 0.22555058 0.28482631 0.37993926 0.50239718 0.61044568 0.66672891 0.6555329 0.57901448 0.45421308 0.30639204 0.17146097 0.069115445 -0.00085857394 -0.047440067][0.21908751 0.23211497 0.2630806 0.33062074 0.44172505 0.56197238 0.64865321 0.67206693 0.62346095 0.51188451 0.35873997 0.20653789 0.087648496 0.0087694321 -0.042219065][0.31299672 0.30398792 0.2965447 0.32381949 0.4146322 0.5450297 0.66445786 0.72161722 0.69537479 0.58949953 0.42286298 0.24545008 0.10231815 0.0098527605 -0.04609222][0.455312 0.42878267 0.38827828 0.38012859 0.4535813 0.59212232 0.73674613 0.81356412 0.79175138 0.67787087 0.49134988 0.28734982 0.11799493 0.0082360767 -0.054933261][0.59322834 0.55815572 0.50545079 0.487907 0.5637399 0.71446246 0.8711189 0.94376081 0.89900416 0.75743228 0.5443722 0.31765378 0.12839805 0.0040638428 -0.065812424][0.67363149 0.63521707 0.5892033 0.58647764 0.6783334 0.83685565 0.9871574 1.0365542 0.95414668 0.77816659 0.54343581 0.30794016 0.11557049 -0.010772508 -0.079889104][0.65808964 0.62244153 0.59622991 0.62064552 0.72711653 0.87814176 1.0036116 1.0241624 0.91496879 0.72319084 0.48825088 0.26434621 0.086729981 -0.029052675 -0.091023155][0.54905814 0.52568823 0.52887428 0.58435255 0.69784576 0.82641387 0.91495049 0.90968281 0.79494947 0.61443913 0.40374884 0.20926964 0.058215562 -0.040501863 -0.09391962][0.3962445 0.38775727 0.41727635 0.49678633 0.61082822 0.71452957 0.76928985 0.74779278 0.643923 0.49176875 0.31780374 0.15911734 0.036360353 -0.045536045 -0.091680579][0.27878243 0.27964008 0.32060692 0.40707844 0.51307887 0.59618348 0.62909865 0.60215491 0.51547515 0.39297932 0.25194082 0.12253602 0.021611206 -0.047524821 -0.0882009][0.23573887 0.23562199 0.26622796 0.3357946 0.41985542 0.48405576 0.50816339 0.48924446 0.42574757 0.33013147 0.21259193 0.10102539 0.012639832 -0.048557352 -0.085264646][0.24786797 0.24144246 0.25257942 0.29586861 0.3540411 0.40224341 0.42392761 0.41636926 0.37319627 0.29649061 0.19150187 0.087320514 0.0044605522 -0.051438175 -0.084403165][0.2725628 0.25572929 0.24670158 0.26592413 0.30311963 0.33915174 0.35852826 0.35664862 0.32352504 0.25561127 0.15671249 0.058000937 -0.017623795 -0.064753346 -0.090288229][0.26883864 0.24194871 0.21707812 0.21740811 0.23963584 0.26746443 0.28426918 0.28271538 0.25122309 0.18674774 0.095098957 0.0071054958 -0.055291526 -0.088966742 -0.10328293]]...]
INFO - root - 2017-12-11 05:53:04.706949: step 16410, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 45h:40m:31s remains)
INFO - root - 2017-12-11 05:53:10.053355: step 16420, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:42m:22s remains)
INFO - root - 2017-12-11 05:53:15.340582: step 16430, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.542 sec/batch; 47h:37m:34s remains)
INFO - root - 2017-12-11 05:53:20.747646: step 16440, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 46h:16m:06s remains)
INFO - root - 2017-12-11 05:53:26.122608: step 16450, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 47h:20m:05s remains)
INFO - root - 2017-12-11 05:53:31.394198: step 16460, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:29m:33s remains)
INFO - root - 2017-12-11 05:53:36.802940: step 16470, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 48h:33m:06s remains)
INFO - root - 2017-12-11 05:53:42.234612: step 16480, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 46h:33m:51s remains)
INFO - root - 2017-12-11 05:53:47.623784: step 16490, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 45h:17m:46s remains)
INFO - root - 2017-12-11 05:53:52.973005: step 16500, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.520 sec/batch; 45h:36m:03s remains)
2017-12-11 05:53:53.482059: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050361149 0.055417217 0.054655932 0.048365228 0.040086385 0.032713421 0.02766897 0.025947865 0.02839843 0.032854304 0.036411829 0.040144682 0.047543108 0.061884508 0.081946626][0.075522542 0.082293317 0.083792418 0.082910269 0.082412384 0.082508154 0.080718227 0.078800142 0.079107404 0.078301 0.074359104 0.0707323 0.072626084 0.082586966 0.099749692][0.11307349 0.1217043 0.12527075 0.13151945 0.14193064 0.15237741 0.15647535 0.15688409 0.15732905 0.15167585 0.1384012 0.1237111 0.11544269 0.11552873 0.1242108][0.16579567 0.17631288 0.18137999 0.19423318 0.21486863 0.23429722 0.24345136 0.24694899 0.24979191 0.24178371 0.22132908 0.1967016 0.17787144 0.16654764 0.16523342][0.23629379 0.24977612 0.25699678 0.27471143 0.30167776 0.32522878 0.33540508 0.33867019 0.34157038 0.33147469 0.3068791 0.27661818 0.25099188 0.23145415 0.2220366][0.32186809 0.34024197 0.3495141 0.36787242 0.39360395 0.41270208 0.41640842 0.41270325 0.41111875 0.39966565 0.37645987 0.34824172 0.32247823 0.29999861 0.284026][0.39632142 0.42081606 0.43288717 0.44942552 0.46851888 0.47668085 0.46794504 0.45175666 0.44212523 0.43130657 0.41680604 0.39982188 0.381024 0.36072034 0.33967975][0.43861759 0.47131613 0.49022469 0.50668341 0.51828045 0.51351392 0.49011344 0.45834881 0.43724641 0.42697385 0.42560029 0.42565462 0.41838789 0.40426406 0.38201848][0.45093313 0.49340853 0.52182889 0.53992766 0.54454142 0.52717018 0.48990512 0.44343221 0.41033003 0.39973685 0.41174659 0.43010908 0.43646461 0.43112239 0.41201818][0.44274014 0.49244493 0.52880496 0.54865509 0.54789829 0.52121168 0.4748677 0.41982627 0.37953848 0.36981082 0.39281571 0.42603329 0.4445729 0.44778678 0.43371168][0.40228641 0.45447934 0.49527308 0.51809067 0.51715785 0.488709 0.44260252 0.39058205 0.3540563 0.35024756 0.38254571 0.42607474 0.45271286 0.45988509 0.44729507][0.33693728 0.38734284 0.43094504 0.45906228 0.46390235 0.44210517 0.40599403 0.36822617 0.34488726 0.34987909 0.38725483 0.43294966 0.45897922 0.4610076 0.44348145][0.29268116 0.33736753 0.378848 0.40856314 0.41824308 0.40504 0.38195637 0.36108649 0.35133332 0.36121607 0.39543486 0.43340597 0.45003244 0.44022733 0.41447803][0.2916075 0.32840261 0.36114508 0.38513753 0.39427578 0.38635135 0.37231025 0.36270016 0.36058393 0.36960623 0.39419731 0.41928774 0.42419398 0.40373462 0.37195686][0.31390345 0.34412935 0.36711341 0.38232797 0.38784376 0.38239172 0.37259951 0.36705071 0.36650357 0.37222785 0.38677597 0.39962411 0.39476362 0.36802852 0.33380648]]...]
INFO - root - 2017-12-11 05:53:58.571866: step 16510, loss = 0.69, batch loss = 0.63 (18.5 examples/sec; 0.434 sec/batch; 38h:03m:17s remains)
INFO - root - 2017-12-11 05:54:03.887282: step 16520, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 47h:45m:05s remains)
INFO - root - 2017-12-11 05:54:09.259940: step 16530, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 45h:56m:32s remains)
INFO - root - 2017-12-11 05:54:14.668881: step 16540, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 45h:50m:05s remains)
INFO - root - 2017-12-11 05:54:19.976482: step 16550, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 45h:27m:53s remains)
INFO - root - 2017-12-11 05:54:25.300207: step 16560, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 46h:11m:32s remains)
INFO - root - 2017-12-11 05:54:30.760780: step 16570, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:23m:11s remains)
INFO - root - 2017-12-11 05:54:36.126911: step 16580, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:24m:32s remains)
INFO - root - 2017-12-11 05:54:41.400659: step 16590, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:13m:07s remains)
INFO - root - 2017-12-11 05:54:46.754545: step 16600, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:33m:01s remains)
2017-12-11 05:54:47.312987: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47480378 0.58506781 0.64940351 0.66894484 0.62400323 0.5444563 0.47923794 0.44019234 0.41075534 0.38181594 0.36993995 0.35751316 0.32730111 0.3006075 0.28343245][0.48345754 0.598181 0.66731489 0.68605065 0.63499212 0.54401624 0.46700549 0.42242056 0.394571 0.37463632 0.37112466 0.35728243 0.3269484 0.30438855 0.29453754][0.4632175 0.5772965 0.64888555 0.66832274 0.62017351 0.53025675 0.45214081 0.40840042 0.38227969 0.36528823 0.35955238 0.33988824 0.30982736 0.29903427 0.30823359][0.45853382 0.57287383 0.64521658 0.66486079 0.62556958 0.55187041 0.48797598 0.45281142 0.42624587 0.40352553 0.38407072 0.34976313 0.31548637 0.31840578 0.35674974][0.47639775 0.59180653 0.66116208 0.67929667 0.65512872 0.6128121 0.58011621 0.56306583 0.53778863 0.50478047 0.46320182 0.40386844 0.35378557 0.36012349 0.4204911][0.49723458 0.61406112 0.68059593 0.70062912 0.70023584 0.7018097 0.71371287 0.72143108 0.69631219 0.6457057 0.57157689 0.4736459 0.38991079 0.37772402 0.43574971][0.52173626 0.6428948 0.71211636 0.73991227 0.76709181 0.81714934 0.88061845 0.91576529 0.88766396 0.81351715 0.70035726 0.553563 0.41955096 0.36779314 0.39875522][0.55259126 0.6833837 0.76074338 0.79670113 0.841891 0.92540872 1.0273472 1.0815846 1.0472386 0.95290726 0.81126422 0.62597704 0.44503897 0.34766105 0.33862808][0.57154191 0.71079165 0.79466069 0.83218527 0.87728041 0.96647429 1.0792615 1.1385409 1.100895 1.0027523 0.85965973 0.66815054 0.46817404 0.34024069 0.29451504][0.57894534 0.72281253 0.80636537 0.83468682 0.85905987 0.92224634 1.0124542 1.0581734 1.019027 0.934224 0.81550711 0.65137911 0.46685892 0.334296 0.26669851][0.58918762 0.73334938 0.80755985 0.81463134 0.80011147 0.81349725 0.85889411 0.88052052 0.84143972 0.7784822 0.69652927 0.57717079 0.43025863 0.31580025 0.24836147][0.59536105 0.73528904 0.791948 0.76977414 0.70872015 0.66386527 0.65606356 0.64857 0.61026114 0.57229382 0.53318018 0.4678078 0.37263629 0.29397511 0.24621367][0.56842893 0.69490659 0.72800803 0.6783967 0.58133352 0.49097714 0.437828 0.40488735 0.36988148 0.35493323 0.35473534 0.34353969 0.30855146 0.27706826 0.25823969][0.47803658 0.57804382 0.58666104 0.5226739 0.41494453 0.30950522 0.23732349 0.19666551 0.17324293 0.17785347 0.20314603 0.22994398 0.2452393 0.25746787 0.26611921][0.32579079 0.39444876 0.38657942 0.325285 0.23085582 0.1376742 0.071852341 0.039825663 0.032281823 0.050219677 0.08600425 0.12817979 0.16911425 0.20745277 0.23526262]]...]
INFO - root - 2017-12-11 05:54:52.631431: step 16610, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:55m:08s remains)
INFO - root - 2017-12-11 05:54:57.639907: step 16620, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:31m:07s remains)
INFO - root - 2017-12-11 05:55:02.948486: step 16630, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:39m:12s remains)
INFO - root - 2017-12-11 05:55:08.330750: step 16640, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:26m:18s remains)
INFO - root - 2017-12-11 05:55:13.651353: step 16650, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:29m:55s remains)
INFO - root - 2017-12-11 05:55:19.010055: step 16660, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 47h:56m:34s remains)
INFO - root - 2017-12-11 05:55:24.354554: step 16670, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 45h:34m:23s remains)
INFO - root - 2017-12-11 05:55:29.706296: step 16680, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 45h:59m:03s remains)
INFO - root - 2017-12-11 05:55:35.015134: step 16690, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:46m:01s remains)
INFO - root - 2017-12-11 05:55:40.377147: step 16700, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:35m:55s remains)
2017-12-11 05:55:40.980205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0068853688 0.017282693 0.039386783 0.052045781 0.054358356 0.048734073 0.038518503 0.028358156 0.022733189 0.024099328 0.029331191 0.03897294 0.054698206 0.077069439 0.10601612][-0.0039701159 0.0220139 0.044457685 0.055709925 0.055207949 0.046025615 0.032132279 0.018682837 0.010846007 0.011105247 0.016165448 0.026540963 0.043400109 0.068096489 0.1024527][-0.0095127495 0.015720857 0.037338674 0.047477849 0.045567583 0.034428574 0.018229436 0.0027210542 -0.0067896293 -0.007624764 -0.0031033335 0.0066399244 0.021568349 0.043854944 0.078227758][-0.013570786 0.01410418 0.038456056 0.051254462 0.050892919 0.040113352 0.024647592 0.010418341 0.0020824166 0.0020058157 0.00687288 0.014720289 0.023491662 0.03664466 0.06304796][-0.0046588974 0.030784564 0.063326634 0.083554022 0.087522693 0.079785258 0.06995479 0.061872318 0.058517795 0.061611857 0.067418292 0.070986666 0.068544373 0.065435529 0.07495565][0.019785112 0.070506521 0.12215141 0.1620764 0.18107815 0.18585362 0.19214985 0.19774617 0.20121419 0.20373878 0.20359474 0.19396715 0.17035319 0.14071339 0.12339715][0.047924638 0.12033775 0.20378029 0.27969027 0.33017868 0.36251616 0.39811027 0.42337123 0.42973489 0.42030171 0.40008023 0.36490768 0.3129518 0.25463521 0.21019849][0.065943129 0.16077216 0.28299555 0.40480542 0.497879 0.567141 0.63644707 0.67838919 0.67616493 0.6388883 0.58425081 0.51757967 0.44209358 0.36777243 0.309961][0.063541584 0.17214626 0.32493469 0.484095 0.61125869 0.70681137 0.79544777 0.83979809 0.81722063 0.7450487 0.65673369 0.57090992 0.49379891 0.42930287 0.37974405][0.034261506 0.1403494 0.30094007 0.47066641 0.60499781 0.70003748 0.78071707 0.80954224 0.76401615 0.66789705 0.56564558 0.48640758 0.43477944 0.40404728 0.38029689][-0.02019478 0.065952353 0.20739073 0.3578417 0.47179809 0.54309338 0.59647685 0.60213965 0.54264581 0.44400707 0.35201657 0.29904094 0.2852599 0.29393384 0.29899722][-0.09115091 -0.039032321 0.062128231 0.17240275 0.25126117 0.29237351 0.31767178 0.30717543 0.24925718 0.1671534 0.099533133 0.07501217 0.090309739 0.12468355 0.14876103][-0.15822537 -0.14313708 -0.088791229 -0.02421632 0.019575913 0.036704555 0.043322057 0.027055498 -0.018981589 -0.077257127 -0.12008924 -0.12562701 -0.097918719 -0.056059562 -0.025325075][-0.19767371 -0.21001 -0.19395182 -0.16714244 -0.14983959 -0.14681022 -0.1484302 -0.16239987 -0.19231781 -0.22729354 -0.25024882 -0.24623787 -0.21827975 -0.18095911 -0.15228085][-0.19786766 -0.22090378 -0.22491826 -0.21971607 -0.21669418 -0.21899375 -0.22212341 -0.23100673 -0.24722637 -0.2650165 -0.27510819 -0.26870742 -0.24740154 -0.22058447 -0.19881153]]...]
INFO - root - 2017-12-11 05:55:46.292082: step 16710, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:04m:46s remains)
INFO - root - 2017-12-11 05:55:51.343589: step 16720, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:07m:33s remains)
INFO - root - 2017-12-11 05:55:56.703252: step 16730, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.528 sec/batch; 46h:17m:30s remains)
INFO - root - 2017-12-11 05:56:02.088657: step 16740, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 46h:09m:35s remains)
INFO - root - 2017-12-11 05:56:07.358273: step 16750, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:50m:12s remains)
INFO - root - 2017-12-11 05:56:12.720745: step 16760, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:39m:58s remains)
INFO - root - 2017-12-11 05:56:18.054347: step 16770, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 50h:00m:36s remains)
INFO - root - 2017-12-11 05:56:23.428624: step 16780, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:56m:49s remains)
INFO - root - 2017-12-11 05:56:28.764428: step 16790, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:41m:19s remains)
INFO - root - 2017-12-11 05:56:34.058566: step 16800, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 46h:53m:24s remains)
2017-12-11 05:56:34.580088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02299712 -0.0067781187 0.018056648 0.049683321 0.084006228 0.11549889 0.13478281 0.13457391 0.11163468 0.078432649 0.0476002 0.027372958 0.018925244 0.021236677 0.025577858][0.0087556383 0.03251322 0.068746984 0.1178339 0.17478812 0.22957233 0.26696864 0.2736291 0.24191725 0.1888683 0.13569596 0.099195376 0.08168523 0.079271913 0.079437815][0.053260554 0.083383866 0.12950841 0.19645107 0.27983713 0.36428303 0.42668793 0.44393888 0.40216994 0.32300922 0.2380854 0.17661905 0.14445034 0.13439727 0.12852697][0.10295081 0.13693202 0.18496619 0.25752345 0.3527405 0.45409349 0.53423786 0.56334257 0.52159679 0.43127131 0.32843542 0.2505959 0.20718747 0.18829966 0.17157605][0.15644227 0.19369578 0.23730694 0.30167517 0.38724136 0.48063168 0.55788332 0.59157884 0.56101859 0.48444152 0.39306936 0.32290697 0.28209096 0.25682119 0.22291602][0.21661562 0.26098555 0.30179018 0.35534206 0.42317349 0.49569684 0.55576843 0.58414173 0.56409085 0.50923193 0.44179344 0.39025429 0.35783973 0.32654729 0.27237025][0.27657324 0.33441249 0.38000545 0.42976969 0.48565218 0.541037 0.58469278 0.60419971 0.58747375 0.545299 0.49269161 0.45046639 0.41832051 0.37695697 0.30368927][0.30522671 0.37832832 0.43229708 0.482418 0.53144193 0.57562172 0.60836458 0.62132645 0.60669416 0.57236183 0.52721965 0.48594394 0.44815138 0.39727706 0.31175148][0.29045522 0.37462348 0.43732992 0.49026945 0.53523058 0.56939065 0.5899781 0.593045 0.57738936 0.5483256 0.50967723 0.47086537 0.43204674 0.38044333 0.29449859][0.23254091 0.31673753 0.38412613 0.44192591 0.48804712 0.51745623 0.52758551 0.51762378 0.49332452 0.46078995 0.42344 0.3874698 0.35333082 0.31109053 0.23838682][0.14070484 0.21350414 0.27736041 0.3368578 0.38703117 0.41915485 0.42701522 0.40897268 0.37371749 0.33069935 0.28716505 0.24975364 0.22039601 0.19139607 0.14037421][0.015448666 0.06763979 0.11928096 0.1721302 0.22139861 0.25627896 0.26712954 0.24951036 0.21182288 0.166185 0.12328713 0.090524 0.070431046 0.057098117 0.030240105][-0.1065562 -0.0769182 -0.041041069 0.00024010945 0.042656619 0.075114176 0.086696237 0.0717758 0.038987245 0.0015559708 -0.029550185 -0.048249025 -0.053381685 -0.050174572 -0.055844672][-0.17603551 -0.16509828 -0.14389977 -0.11544202 -0.08416073 -0.059810936 -0.052244656 -0.066126592 -0.092619911 -0.11913232 -0.13600717 -0.13975038 -0.1315168 -0.11652892 -0.10835543][-0.19822952 -0.20121713 -0.19340177 -0.17881569 -0.161448 -0.14763071 -0.14434505 -0.15480378 -0.17235765 -0.18712537 -0.1917794 -0.1853061 -0.1702387 -0.15124296 -0.13734984]]...]
INFO - root - 2017-12-11 05:56:39.893290: step 16810, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:23m:44s remains)
INFO - root - 2017-12-11 05:56:44.980519: step 16820, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:21m:32s remains)
INFO - root - 2017-12-11 05:56:50.373309: step 16830, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:22m:10s remains)
INFO - root - 2017-12-11 05:56:55.750958: step 16840, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:56m:17s remains)
INFO - root - 2017-12-11 05:57:01.161106: step 16850, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:52m:40s remains)
INFO - root - 2017-12-11 05:57:06.460376: step 16860, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:24m:10s remains)
INFO - root - 2017-12-11 05:57:11.919137: step 16870, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:42m:14s remains)
INFO - root - 2017-12-11 05:57:17.315835: step 16880, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 47h:05m:23s remains)
INFO - root - 2017-12-11 05:57:22.638356: step 16890, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:40m:45s remains)
INFO - root - 2017-12-11 05:57:27.949001: step 16900, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:59m:40s remains)
2017-12-11 05:57:28.474264: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19471158 0.21351886 0.25314206 0.32284564 0.40012985 0.46578711 0.49865469 0.48115554 0.41359213 0.3113437 0.20313571 0.12094165 0.094548583 0.12487248 0.19143271][0.22265641 0.23982695 0.28180581 0.35942408 0.43863937 0.49948606 0.52393639 0.49824587 0.42703497 0.32645813 0.22464801 0.15114713 0.13636222 0.18245582 0.26946241][0.24453868 0.25564507 0.29154432 0.3663882 0.43899721 0.49187988 0.51299417 0.49364713 0.43786672 0.3550142 0.2670939 0.2022564 0.19321708 0.24526529 0.34146515][0.27304089 0.2770867 0.30172208 0.36538503 0.42675033 0.47208619 0.4940632 0.48779941 0.45410097 0.39380011 0.3206293 0.262092 0.25233588 0.29769459 0.38588789][0.29576004 0.29510456 0.30984354 0.36005086 0.41172424 0.45414406 0.47792941 0.47799483 0.45571688 0.41101059 0.35131729 0.30165413 0.29231864 0.32516089 0.39026693][0.30202565 0.3067525 0.325535 0.37615848 0.43223372 0.48071367 0.50130123 0.48715159 0.45017418 0.40143427 0.34705105 0.30550903 0.29804486 0.3173025 0.35168251][0.28463778 0.30256945 0.34237054 0.41402549 0.49167195 0.55282557 0.56331921 0.5169856 0.44340321 0.37262559 0.31472832 0.27889213 0.27412561 0.28395554 0.2935361][0.2440142 0.27663425 0.3408899 0.43922377 0.53950757 0.60979933 0.60856551 0.53189647 0.42455831 0.33282116 0.27363533 0.248691 0.25421143 0.26606655 0.26466012][0.19378123 0.23575391 0.3148872 0.42646518 0.5308457 0.5952642 0.58436072 0.49757883 0.38327464 0.29117066 0.24451879 0.24279712 0.27111974 0.29981947 0.30403295][0.12497655 0.16921283 0.25167295 0.36049691 0.45141414 0.49878567 0.48367777 0.41028395 0.31969386 0.25212389 0.23294739 0.26273513 0.32093036 0.37638181 0.39920568][0.040375903 0.080715969 0.15655288 0.25341505 0.32713106 0.35904169 0.34777859 0.30186671 0.25088906 0.22143956 0.23671296 0.29713163 0.37862355 0.45185566 0.48859969][-0.040872928 -0.018915353 0.034061708 0.10848106 0.16732061 0.19494803 0.20090348 0.19361122 0.1894903 0.20267375 0.25115392 0.3321119 0.41719282 0.48255172 0.51188487][-0.10156281 -0.10588402 -0.085419312 -0.039801128 0.0069473041 0.040904779 0.073587589 0.10853543 0.14885701 0.19840518 0.26798671 0.35130948 0.41857228 0.45318946 0.45439693][-0.13393842 -0.15746482 -0.16178416 -0.13949634 -0.10141744 -0.058570154 -0.001805977 0.066976242 0.13893415 0.20915304 0.28197664 0.35063666 0.38814715 0.38390535 0.34934261][-0.14710307 -0.17648816 -0.1897634 -0.17821677 -0.14442854 -0.095627062 -0.02483286 0.063340686 0.15177067 0.22869954 0.29333466 0.339912 0.34779128 0.31109396 0.24759881]]...]
INFO - root - 2017-12-11 05:57:33.759569: step 16910, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 46h:44m:42s remains)
INFO - root - 2017-12-11 05:57:39.165787: step 16920, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 49h:10m:17s remains)
INFO - root - 2017-12-11 05:57:44.235804: step 16930, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:38m:58s remains)
INFO - root - 2017-12-11 05:57:49.685429: step 16940, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:18m:20s remains)
INFO - root - 2017-12-11 05:57:54.926395: step 16950, loss = 0.70, batch loss = 0.64 (16.2 examples/sec; 0.494 sec/batch; 43h:19m:45s remains)
INFO - root - 2017-12-11 05:58:00.336162: step 16960, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:52m:49s remains)
INFO - root - 2017-12-11 05:58:05.756967: step 16970, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:23m:14s remains)
INFO - root - 2017-12-11 05:58:11.105633: step 16980, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 45h:33m:42s remains)
INFO - root - 2017-12-11 05:58:16.505564: step 16990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:07m:49s remains)
INFO - root - 2017-12-11 05:58:21.865352: step 17000, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 47h:09m:38s remains)
2017-12-11 05:58:22.425655: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1262241 0.11638383 0.10856925 0.11340909 0.13463102 0.16167313 0.18143444 0.18710551 0.19233109 0.19567561 0.19421558 0.18773612 0.17909877 0.17073317 0.14883463][0.097307868 0.10162859 0.10915156 0.12452834 0.14797348 0.16939323 0.17625913 0.16764817 0.16041937 0.15731202 0.1550269 0.15115328 0.1472825 0.14277372 0.12457491][0.090740055 0.10845605 0.12802824 0.14847803 0.16684884 0.17597799 0.16735125 0.14410786 0.12500797 0.11692771 0.11627064 0.11684413 0.11718164 0.11460354 0.0983944][0.12918335 0.15347929 0.17716359 0.19467963 0.20213063 0.1979693 0.17762662 0.14538175 0.11957628 0.10975358 0.11180472 0.11550324 0.11693703 0.1121842 0.093180224][0.19364125 0.21915428 0.24077381 0.25047597 0.24598786 0.23236614 0.20757847 0.17315525 0.14463659 0.1336038 0.13632648 0.13986631 0.13894737 0.12901855 0.10408913][0.24707906 0.27210084 0.29094914 0.29474807 0.28437272 0.26953706 0.24851429 0.21765101 0.18821789 0.17351659 0.17147794 0.16882312 0.16099186 0.14381751 0.11237504][0.2598263 0.28519991 0.30574471 0.31109244 0.30522725 0.29883802 0.28852952 0.26530761 0.23631099 0.21600278 0.20388083 0.18879326 0.16936111 0.14351758 0.10643039][0.22121646 0.24773546 0.27479798 0.28846198 0.29483056 0.30214253 0.30475882 0.29048073 0.26341653 0.23874475 0.21629673 0.18813899 0.15681775 0.12359674 0.083373338][0.13792339 0.16593039 0.20047374 0.22333598 0.241855 0.26063496 0.27270368 0.26554129 0.24175541 0.2166681 0.18913393 0.15397452 0.11664572 0.081567585 0.043424111][0.033203587 0.061039519 0.098412439 0.12486868 0.14884193 0.17226084 0.188 0.18494968 0.16568269 0.14480613 0.11925091 0.085344672 0.050070133 0.0201634 -0.0096137971][-0.062255 -0.040123064 -0.0079881959 0.014228598 0.035588372 0.056572068 0.07060311 0.068789728 0.05474101 0.041458156 0.023855248 -0.001230692 -0.026811425 -0.045480952 -0.062136918][-0.12538604 -0.11482358 -0.095874958 -0.0850217 -0.073249087 -0.060175382 -0.051765528 -0.05427644 -0.062816896 -0.066782832 -0.072501458 -0.083128534 -0.093004532 -0.096388072 -0.098044485][-0.14674802 -0.14883536 -0.14552242 -0.14843529 -0.14902951 -0.14585873 -0.14395545 -0.14722098 -0.15053569 -0.1462255 -0.14054017 -0.13662681 -0.13091227 -0.12067696 -0.11037963][-0.13498725 -0.14564145 -0.15361948 -0.1662558 -0.17591077 -0.17986955 -0.18183371 -0.18422115 -0.18305726 -0.1736791 -0.16127925 -0.14861357 -0.13390979 -0.11715 -0.10200117][-0.10538451 -0.11920038 -0.13169077 -0.1472328 -0.15973924 -0.1661648 -0.168971 -0.16953629 -0.16546908 -0.1545835 -0.14094618 -0.12647183 -0.11043462 -0.0944705 -0.08132185]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 05:58:27.760752: step 17010, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 46h:55m:13s remains)
INFO - root - 2017-12-11 05:58:33.148839: step 17020, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:04m:05s remains)
INFO - root - 2017-12-11 05:58:38.187585: step 17030, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 46h:47m:31s remains)
INFO - root - 2017-12-11 05:58:43.519911: step 17040, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 45h:38m:24s remains)
INFO - root - 2017-12-11 05:58:48.814487: step 17050, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 46h:29m:21s remains)
INFO - root - 2017-12-11 05:58:54.217114: step 17060, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 44h:59m:06s remains)
INFO - root - 2017-12-11 05:58:59.655703: step 17070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:37m:38s remains)
INFO - root - 2017-12-11 05:59:04.969408: step 17080, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 45h:38m:32s remains)
INFO - root - 2017-12-11 05:59:10.302624: step 17090, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 47h:10m:57s remains)
INFO - root - 2017-12-11 05:59:15.648868: step 17100, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 46h:43m:54s remains)
2017-12-11 05:59:16.166123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020933654 -0.00032774545 0.031396996 0.058587275 0.07339751 0.071974076 0.053929616 0.027052106 2.0486117e-05 -0.019277386 -0.027587777 -0.027695296 -0.02455071 -0.022657983 -0.023468319][-0.038931184 -0.0011698094 0.05639663 0.10974923 0.14353171 0.14968264 0.12491357 0.0793597 0.027377611 -0.014978189 -0.037803587 -0.042635471 -0.036674313 -0.029586952 -0.026328653][-0.054154839 0.0037088711 0.092926428 0.18070279 0.24241078 0.26384243 0.23691402 0.17188077 0.090293758 0.019138906 -0.022053553 -0.033017151 -0.024213029 -0.012467198 -0.0060785413][-0.052853692 0.021378951 0.1396134 0.26432562 0.36130622 0.40728116 0.38660029 0.30631912 0.1957169 0.094705127 0.035659082 0.020764608 0.034673125 0.050712671 0.058220707][-0.03328833 0.0440433 0.17494479 0.32590351 0.45815408 0.53774232 0.53613859 0.45189127 0.32051715 0.1958441 0.12628818 0.11617754 0.14216909 0.16498774 0.17320991][0.020877739 0.088053092 0.21127607 0.36805066 0.52219594 0.63336223 0.65821147 0.58197415 0.44201419 0.30619624 0.23869555 0.24567044 0.29329377 0.32974523 0.34076545][0.11997629 0.17334226 0.2746104 0.41336289 0.56411678 0.68913752 0.73506176 0.67451674 0.54166818 0.41083241 0.35431668 0.38059157 0.44994083 0.5028069 0.51667416][0.24432729 0.28733173 0.35895684 0.4610067 0.58129078 0.69431388 0.74760395 0.70775127 0.59775025 0.48429447 0.43809572 0.47310781 0.55322874 0.61821342 0.63569742][0.35314357 0.38717833 0.42786893 0.48662892 0.56032926 0.63898772 0.68343544 0.66410708 0.5888207 0.50405616 0.47000679 0.50783747 0.59131873 0.6655724 0.69013631][0.42338964 0.44796923 0.46251547 0.48195341 0.50713009 0.54123777 0.56370169 0.55642813 0.51708817 0.46885639 0.45462942 0.49755716 0.58240134 0.66241664 0.694422][0.45939425 0.4791384 0.47760412 0.46899119 0.45473957 0.44603854 0.43814945 0.42721847 0.41025996 0.39421964 0.40264517 0.45304531 0.53551787 0.61078519 0.64142817][0.46088132 0.480582 0.4700498 0.44403824 0.40482306 0.36675972 0.33377889 0.31111953 0.30104309 0.30380556 0.32705238 0.37960321 0.45152816 0.50854814 0.52365905][0.42520267 0.4415172 0.42243093 0.3854897 0.336081 0.28774697 0.2459823 0.21692486 0.20629893 0.21383946 0.24014887 0.28741238 0.34435222 0.38148963 0.38222554][0.34404683 0.35466176 0.32903522 0.28676778 0.23771326 0.19266905 0.15489046 0.12716755 0.1159721 0.12255179 0.14637209 0.18609415 0.22989362 0.25538898 0.25310344][0.22829868 0.23728339 0.21476415 0.1755145 0.13203415 0.094188146 0.062939771 0.038803019 0.028653616 0.034716256 0.055761032 0.087107562 0.11803971 0.13467319 0.13271913]]...]
INFO - root - 2017-12-11 05:59:21.560219: step 17110, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.554 sec/batch; 48h:29m:47s remains)
INFO - root - 2017-12-11 05:59:26.987235: step 17120, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.515 sec/batch; 45h:06m:07s remains)
INFO - root - 2017-12-11 05:59:31.965747: step 17130, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:31m:50s remains)
INFO - root - 2017-12-11 05:59:37.263259: step 17140, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 45h:57m:30s remains)
INFO - root - 2017-12-11 05:59:42.539980: step 17150, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:58m:36s remains)
INFO - root - 2017-12-11 05:59:47.939217: step 17160, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:26m:53s remains)
INFO - root - 2017-12-11 05:59:53.302739: step 17170, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:01m:32s remains)
INFO - root - 2017-12-11 05:59:58.516645: step 17180, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 45h:59m:56s remains)
INFO - root - 2017-12-11 06:00:03.851099: step 17190, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:17m:02s remains)
INFO - root - 2017-12-11 06:00:09.291757: step 17200, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 46h:11m:15s remains)
2017-12-11 06:00:09.805153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0030330392 0.0036339227 0.0089914389 0.022264268 0.044145938 0.070543446 0.094316334 0.10776942 0.10560399 0.087504148 0.057788741 0.024681604 -0.004138079 -0.023973493 -0.034181319][0.0034200097 0.0069286576 0.017995501 0.040830348 0.076391727 0.11858877 0.15576401 0.17530292 0.16919956 0.13805665 0.090595074 0.040253792 -0.00094332889 -0.027162636 -0.039278779][0.00389127 0.010955038 0.028120758 0.060687695 0.11020206 0.16883872 0.22070695 0.24809267 0.23948666 0.19553678 0.12906958 0.059947267 0.00531695 -0.027509751 -0.041464821][0.0050904085 0.015370332 0.037807014 0.078408495 0.13944428 0.21192959 0.27667427 0.31194097 0.30299506 0.24942969 0.16664167 0.0803214 0.012888817 -0.026484765 -0.042627379][0.0054010167 0.018111402 0.044124234 0.089745961 0.1579112 0.23899418 0.31196254 0.35290888 0.34512818 0.28656539 0.19316505 0.094832964 0.018250853 -0.02564509 -0.043216284][0.0042179949 0.018298406 0.0461515 0.093909532 0.16480921 0.24888216 0.32449663 0.36742672 0.36070395 0.30099288 0.2033146 0.099515796 0.018812859 -0.026684582 -0.0441863][0.0029356615 0.016321199 0.043562822 0.090493113 0.16047183 0.24356766 0.31844521 0.36125195 0.35551363 0.29735982 0.20046607 0.096605629 0.015702628 -0.029458074 -0.045924667][0.0046353685 0.013511109 0.036203213 0.078490831 0.14414734 0.22415923 0.29842252 0.34315363 0.34130335 0.28808308 0.19580057 0.094855249 0.014712996 -0.030980108 -0.047457881][0.011985207 0.011661007 0.025360499 0.058950122 0.1168546 0.1922805 0.26778537 0.31963277 0.32810739 0.28542739 0.20142728 0.1042174 0.022875568 -0.026967496 -0.046744417][0.02542373 0.012317928 0.013642117 0.03539481 0.082480438 0.15119159 0.22828454 0.2910642 0.31605449 0.2900373 0.21832253 0.12555838 0.040986139 -0.016412217 -0.042782515][0.042060811 0.015235933 0.0029438592 0.011508022 0.045637049 0.10474569 0.18071437 0.25370267 0.29735744 0.29172164 0.23565948 0.14912662 0.061814133 -0.0032153151 -0.036896214][0.055649 0.017657124 -0.00598901 -0.0091580013 0.011469189 0.057512544 0.12595178 0.20194563 0.25950876 0.27349114 0.23548838 0.15940951 0.074129388 0.0059962827 -0.03203465][0.056002077 0.014420507 -0.013430253 -0.022680372 -0.01249733 0.0192967 0.073323 0.14102767 0.20094666 0.22719075 0.20583075 0.14443396 0.068660751 0.0052272915 -0.031426892][0.037913639 0.0027338848 -0.019661924 -0.027009889 -0.021259926 -0.0011086202 0.035496723 0.08529444 0.13406238 0.16044129 0.14939976 0.10355788 0.043365993 -0.0077224867 -0.03672586][0.0054934048 -0.015814094 -0.025832083 -0.025489271 -0.018828347 -0.0060763159 0.014762322 0.043686744 0.073650636 0.090605758 0.082495525 0.049689822 0.00678728 -0.028245009 -0.046018679]]...]
INFO - root - 2017-12-11 06:00:15.121465: step 17210, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.542 sec/batch; 47h:30m:37s remains)
INFO - root - 2017-12-11 06:00:20.478201: step 17220, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 45h:31m:13s remains)
INFO - root - 2017-12-11 06:00:25.794215: step 17230, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:54m:55s remains)
INFO - root - 2017-12-11 06:00:30.901885: step 17240, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 45h:37m:16s remains)
INFO - root - 2017-12-11 06:00:36.233103: step 17250, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 46h:29m:30s remains)
INFO - root - 2017-12-11 06:00:41.567040: step 17260, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 46h:13m:53s remains)
INFO - root - 2017-12-11 06:00:47.032446: step 17270, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 45h:43m:59s remains)
INFO - root - 2017-12-11 06:00:52.309245: step 17280, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:38m:10s remains)
INFO - root - 2017-12-11 06:00:57.665606: step 17290, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:59m:59s remains)
INFO - root - 2017-12-11 06:01:03.024846: step 17300, loss = 0.67, batch loss = 0.62 (13.8 examples/sec; 0.578 sec/batch; 50h:38m:56s remains)
2017-12-11 06:01:03.628995: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23144954 0.22360685 0.2143805 0.20651016 0.19922815 0.18489566 0.15467066 0.11159959 0.069222264 0.048623081 0.055249922 0.074129157 0.090127192 0.095292427 0.095295139][0.22031124 0.22098522 0.20924573 0.19268006 0.17346433 0.15154798 0.12406327 0.091407567 0.064637005 0.057993449 0.075895458 0.1044531 0.12573813 0.12881546 0.12066353][0.22280551 0.23530497 0.22753215 0.20555107 0.17638004 0.14510426 0.11641014 0.092082173 0.077730507 0.079849355 0.1028313 0.13592863 0.16060758 0.16249736 0.1468332][0.258743 0.28717262 0.288604 0.26646078 0.23019357 0.18864371 0.15300061 0.12675716 0.11194182 0.11094314 0.12950404 0.16031174 0.18517606 0.18710138 0.16749][0.31224224 0.35932988 0.37535572 0.35977498 0.32115939 0.27121925 0.22635007 0.19112155 0.16510114 0.1505297 0.15601328 0.17718372 0.19594464 0.19516791 0.17306638][0.36179537 0.42604858 0.458453 0.45393002 0.41807666 0.36432788 0.312712 0.26794624 0.22729905 0.19475852 0.18185595 0.18686216 0.19252506 0.1847226 0.16068465][0.39650992 0.47420394 0.52184123 0.53061539 0.50270069 0.45165968 0.39859328 0.34786886 0.293739 0.24215665 0.20723198 0.19049262 0.17729655 0.15838511 0.13230358][0.42099753 0.50879288 0.56830215 0.58858925 0.56969208 0.5250532 0.47511616 0.42263925 0.35745686 0.28633535 0.22659333 0.18422659 0.14816593 0.11443245 0.084805004][0.42805523 0.518711 0.58208078 0.60612178 0.59050816 0.55089813 0.507258 0.45928919 0.39043203 0.30598089 0.22647023 0.16285677 0.10731173 0.059840802 0.026100595][0.40703118 0.48903674 0.54327869 0.55791211 0.533695 0.49092543 0.45146957 0.41328269 0.35310006 0.27189916 0.19016533 0.12157057 0.060236834 0.0080255438 -0.026507646][0.36702138 0.43126374 0.46533981 0.45977998 0.41669577 0.36144471 0.32000214 0.29163849 0.24971437 0.18884154 0.12540786 0.071322463 0.020923456 -0.023462065 -0.052404918][0.33033255 0.37605587 0.38720635 0.35821494 0.2918902 0.21758933 0.1660407 0.14129759 0.11926815 0.088975236 0.059944928 0.037341151 0.012542538 -0.012690323 -0.029038263][0.31954226 0.35043824 0.34335509 0.29712024 0.21330461 0.12183934 0.056850847 0.028948134 0.021879995 0.02201461 0.031755023 0.047635958 0.055167224 0.054165263 0.053662684][0.36255053 0.38357782 0.36354166 0.30776855 0.21538669 0.11395256 0.03870042 0.0042210468 0.0023391915 0.020017685 0.056298431 0.1010481 0.13416599 0.15322302 0.16688496][0.44644302 0.45719227 0.42448282 0.36284015 0.26992771 0.16895422 0.092234738 0.05384082 0.0506003 0.071790412 0.11660003 0.17178769 0.21455024 0.24160908 0.26236823]]...]
INFO - root - 2017-12-11 06:01:08.975112: step 17310, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:56m:56s remains)
INFO - root - 2017-12-11 06:01:14.341758: step 17320, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 45h:02m:19s remains)
INFO - root - 2017-12-11 06:01:19.743083: step 17330, loss = 0.66, batch loss = 0.60 (14.9 examples/sec; 0.537 sec/batch; 47h:02m:20s remains)
INFO - root - 2017-12-11 06:01:24.829013: step 17340, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:16m:06s remains)
INFO - root - 2017-12-11 06:01:30.186567: step 17350, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:14m:59s remains)
INFO - root - 2017-12-11 06:01:35.655493: step 17360, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:44m:09s remains)
INFO - root - 2017-12-11 06:01:40.984901: step 17370, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 45h:27m:21s remains)
INFO - root - 2017-12-11 06:01:46.298170: step 17380, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 46h:59m:20s remains)
INFO - root - 2017-12-11 06:01:51.719028: step 17390, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 48h:29m:06s remains)
INFO - root - 2017-12-11 06:01:57.015184: step 17400, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 45h:26m:54s remains)
2017-12-11 06:01:57.553933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017597238 0.067123041 0.18991587 0.32115388 0.42405257 0.48087257 0.48938105 0.45150185 0.38275278 0.30739686 0.24405931 0.19904222 0.1787212 0.18726228 0.2097616][-0.023440691 0.061186347 0.18741167 0.33033827 0.45371482 0.53426421 0.5601868 0.52527887 0.44852614 0.36082026 0.28355157 0.22728623 0.20282198 0.2161486 0.24772803][-0.033014145 0.041705806 0.16069089 0.30750403 0.44844902 0.55117548 0.591783 0.55709577 0.4710519 0.37286016 0.2877416 0.22853206 0.20617558 0.22583485 0.26474851][-0.04427829 0.019827904 0.13138807 0.28088453 0.43660286 0.55586797 0.60504615 0.5668599 0.47025892 0.36254439 0.27421567 0.21981837 0.20644292 0.23394063 0.27642721][-0.054096553 0.0030809252 0.10982415 0.25917464 0.42036915 0.54430681 0.59494573 0.5556885 0.45842624 0.35298744 0.27246979 0.23057362 0.22817129 0.25863954 0.29542384][-0.060569674 -0.0056253704 0.099630117 0.24619755 0.40157053 0.51538581 0.55830353 0.5207938 0.43584242 0.34928229 0.2908988 0.26806256 0.27364609 0.29811642 0.31980264][-0.066121563 -0.01200726 0.092321455 0.23343323 0.37524977 0.46997011 0.49929693 0.46471596 0.399429 0.34147817 0.31255621 0.3098858 0.31898424 0.33255655 0.33845705][-0.071464941 -0.019642415 0.080919735 0.21314417 0.33852932 0.41491109 0.43370292 0.40519333 0.36013064 0.32722333 0.32019827 0.32888341 0.33645982 0.34124947 0.34010023][-0.077341206 -0.032416109 0.0563014 0.17090999 0.27537626 0.33667958 0.35290805 0.33662063 0.31098759 0.29395083 0.29452929 0.30247641 0.30520695 0.30761606 0.31118369][-0.084814928 -0.051675994 0.016790837 0.1037382 0.17973703 0.22354569 0.23848307 0.23507166 0.22544432 0.2171592 0.2169347 0.21877378 0.21794178 0.22480057 0.24090154][-0.093999431 -0.0760634 -0.032689128 0.021945164 0.066188507 0.089178041 0.09694282 0.0973272 0.09368784 0.087337494 0.0838477 0.081503943 0.082485557 0.099691816 0.13123916][-0.10341521 -0.10110338 -0.082192406 -0.056366809 -0.037762392 -0.031717908 -0.033234406 -0.037058253 -0.043390419 -0.052503094 -0.059299883 -0.063168995 -0.058251802 -0.032315575 0.00941811][-0.10947134 -0.11918584 -0.11843377 -0.11216061 -0.10847785 -0.11111648 -0.11788109 -0.12582302 -0.1350985 -0.14555 -0.15319273 -0.15687962 -0.15033613 -0.12428043 -0.084661581][-0.11049516 -0.12679738 -0.13609701 -0.14060009 -0.14451551 -0.15055409 -0.15823515 -0.16607596 -0.17376542 -0.181317 -0.1867982 -0.18925659 -0.18401732 -0.16544603 -0.13774756][-0.10446256 -0.12156406 -0.13363925 -0.14228864 -0.14977604 -0.15722132 -0.16413932 -0.16970317 -0.17384025 -0.1772837 -0.1799484 -0.18123138 -0.17835075 -0.1685719 -0.15398034]]...]
INFO - root - 2017-12-11 06:02:02.856706: step 17410, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.526 sec/batch; 46h:02m:19s remains)
INFO - root - 2017-12-11 06:02:08.270665: step 17420, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:14m:34s remains)
INFO - root - 2017-12-11 06:02:13.531125: step 17430, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 46h:11m:58s remains)
INFO - root - 2017-12-11 06:02:18.596586: step 17440, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:48m:42s remains)
INFO - root - 2017-12-11 06:02:23.929451: step 17450, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:26m:31s remains)
INFO - root - 2017-12-11 06:02:29.344788: step 17460, loss = 0.69, batch loss = 0.63 (13.5 examples/sec; 0.595 sec/batch; 52h:02m:26s remains)
INFO - root - 2017-12-11 06:02:34.690773: step 17470, loss = 0.66, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 46h:48m:24s remains)
INFO - root - 2017-12-11 06:02:40.054343: step 17480, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 45h:09m:38s remains)
INFO - root - 2017-12-11 06:02:45.326532: step 17490, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:32m:02s remains)
INFO - root - 2017-12-11 06:02:50.714422: step 17500, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:48m:45s remains)
2017-12-11 06:02:51.309285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042205624 -0.038590796 -0.034077767 -0.031518869 -0.030909905 -0.033601116 -0.040597849 -0.051057778 -0.062577151 -0.073839664 -0.0835157 -0.089974284 -0.092672743 -0.092416033 -0.0901986][0.019964343 0.026066462 0.032280203 0.036955286 0.041002307 0.04067928 0.031652629 0.013786796 -0.0096090743 -0.035199907 -0.058853153 -0.075521342 -0.082702979 -0.081670493 -0.075325958][0.1336561 0.14218101 0.14696644 0.15198316 0.1612791 0.1684695 0.16223486 0.13737161 0.097210288 0.048793316 0.0017877122 -0.032060478 -0.04620672 -0.042051729 -0.026172822][0.29301509 0.30182084 0.3001807 0.30074438 0.31395292 0.33267623 0.33598158 0.30832264 0.24964204 0.17176735 0.092424475 0.03401529 0.011171845 0.024040451 0.059895374][0.47609842 0.48271286 0.46845114 0.45676997 0.46790278 0.49910766 0.520414 0.50054491 0.43054736 0.32576483 0.21228056 0.12550017 0.093046933 0.12042644 0.18740977][0.64608681 0.64442158 0.61236954 0.58120459 0.58328885 0.62697577 0.67651397 0.68058926 0.61504441 0.49580356 0.35583863 0.24330921 0.20282416 0.24954534 0.35546896][0.75490212 0.74280292 0.6928606 0.64062679 0.62963688 0.68301725 0.76369935 0.801336 0.75410795 0.63553047 0.48154476 0.34942913 0.30194446 0.3671788 0.51047212][0.78308034 0.76020366 0.69759679 0.62892538 0.60423404 0.66081887 0.76492447 0.8337487 0.81130385 0.70773745 0.55681437 0.41699398 0.36425537 0.44020876 0.60696113][0.72269034 0.69348919 0.62862688 0.5545845 0.52079529 0.572761 0.68430752 0.77185917 0.77455449 0.69865364 0.56984174 0.43829054 0.3846226 0.46033159 0.62853229][0.5914098 0.56117213 0.50312161 0.43484923 0.39867306 0.43866807 0.53875673 0.62725365 0.64891946 0.60555178 0.51157737 0.40322694 0.35535395 0.4210251 0.56790829][0.4215042 0.39623547 0.352006 0.29835132 0.26719874 0.29310036 0.36788651 0.44004565 0.46766424 0.4489336 0.38856629 0.3092702 0.27249697 0.32326582 0.43411443][0.24430813 0.23141344 0.20502087 0.17089805 0.14919199 0.16158921 0.20505486 0.24950948 0.26940602 0.26122573 0.22403944 0.17123953 0.14730568 0.1827068 0.25403157][0.094152451 0.093449153 0.0859188 0.071010038 0.057928327 0.057128355 0.070629038 0.086374477 0.09262985 0.085777439 0.0630535 0.032661587 0.0218981 0.045423839 0.084398456][-0.035085559 -0.026664736 -0.020448623 -0.021795733 -0.029015504 -0.038872786 -0.046324123 -0.051637411 -0.056980379 -0.064814642 -0.077578828 -0.090457313 -0.089909278 -0.072984383 -0.053213615][-0.12793204 -0.11831798 -0.10652997 -0.10241355 -0.10702886 -0.12034368 -0.13649502 -0.1509614 -0.16161682 -0.16961826 -0.17660224 -0.17905268 -0.17220525 -0.15905851 -0.14783625]]...]
INFO - root - 2017-12-11 06:02:56.642639: step 17510, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 47h:00m:22s remains)
INFO - root - 2017-12-11 06:03:02.066718: step 17520, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.559 sec/batch; 48h:54m:06s remains)
INFO - root - 2017-12-11 06:03:07.374527: step 17530, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 45h:51m:04s remains)
INFO - root - 2017-12-11 06:03:12.432346: step 17540, loss = 0.70, batch loss = 0.64 (18.0 examples/sec; 0.445 sec/batch; 38h:55m:22s remains)
INFO - root - 2017-12-11 06:03:17.753651: step 17550, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:45m:21s remains)
INFO - root - 2017-12-11 06:03:23.128415: step 17560, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:30m:22s remains)
INFO - root - 2017-12-11 06:03:28.424551: step 17570, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 44h:55m:15s remains)
INFO - root - 2017-12-11 06:03:33.787532: step 17580, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:30m:43s remains)
INFO - root - 2017-12-11 06:03:39.205519: step 17590, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 45h:49m:54s remains)
INFO - root - 2017-12-11 06:03:44.549694: step 17600, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:51m:53s remains)
2017-12-11 06:03:45.101009: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062770225 0.072239824 0.077204965 0.079347737 0.076186925 0.070720889 0.070490226 0.076263629 0.077398472 0.069977008 0.062327031 0.055504918 0.04328078 0.024232546 0.0044300728][0.11599258 0.1333096 0.142086 0.14757565 0.14513712 0.13691314 0.13337211 0.13883771 0.13950554 0.12797581 0.11477733 0.102234 0.081914887 0.051529661 0.019837074][0.1783752 0.20540231 0.21862872 0.22769614 0.22660267 0.21637449 0.2097636 0.21422102 0.21492603 0.19999124 0.18032679 0.15921171 0.1276513 0.084011786 0.039286796][0.23916407 0.27455622 0.28960702 0.29835141 0.29529426 0.2810345 0.27064002 0.27466068 0.27821425 0.26366028 0.24012983 0.21091168 0.16746086 0.11061308 0.053515244][0.29075354 0.33322772 0.34772897 0.35052997 0.33986133 0.31827629 0.30260012 0.304849 0.31148139 0.30039227 0.2769804 0.24308021 0.1907343 0.12407147 0.058342841][0.31807506 0.36591321 0.37893146 0.37256923 0.35209379 0.32432008 0.30539998 0.30465624 0.31229454 0.30560091 0.2848081 0.24860999 0.19057249 0.1192936 0.050844856][0.31941745 0.36687779 0.37539235 0.35764438 0.32754803 0.29737806 0.28081 0.28073451 0.2905418 0.28990895 0.27445921 0.23855162 0.17717884 0.10421538 0.036562458][0.31066141 0.34987521 0.34929946 0.31787673 0.27663773 0.24363987 0.23163263 0.23690853 0.25258023 0.26142141 0.25564978 0.22487904 0.16391887 0.091066174 0.025122467][0.29457694 0.323127 0.31379744 0.27123544 0.21990794 0.18337107 0.17499818 0.18603989 0.20697556 0.22442035 0.2288716 0.20600347 0.1492997 0.079158254 0.016442986][0.25719798 0.28012586 0.27060011 0.22779046 0.17496501 0.13923517 0.13499476 0.15028678 0.17302169 0.19419131 0.20415443 0.1861399 0.13312265 0.066424593 0.0080855712][0.20301273 0.22598121 0.2238414 0.19050924 0.14472951 0.11376633 0.11313059 0.1308884 0.15372708 0.17570151 0.18681999 0.16936606 0.11780263 0.054157462 0.00023255158][0.14598678 0.16747977 0.17046161 0.1461211 0.10798918 0.080700904 0.080723591 0.098868445 0.12113775 0.14272557 0.153491 0.13685189 0.089790352 0.033273667 -0.013006585][0.080529109 0.093138859 0.09343151 0.073009968 0.04134218 0.018609628 0.018988842 0.035393313 0.05441827 0.072854206 0.082287267 0.069285214 0.033236124 -0.0078986287 -0.039203409][0.0058653415 0.0064120525 0.000946146 -0.017139042 -0.040921405 -0.056012116 -0.052848656 -0.038144693 -0.023457818 -0.010127677 -0.0030724988 -0.011251887 -0.033473592 -0.055748984 -0.068924889][-0.055942941 -0.063718282 -0.072369687 -0.087313727 -0.10355147 -0.11203815 -0.10717854 -0.094940446 -0.083735436 -0.07353922 -0.067045204 -0.069776677 -0.079668544 -0.086489737 -0.08579284]]...]
INFO - root - 2017-12-11 06:03:50.485761: step 17610, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 45h:42m:53s remains)
INFO - root - 2017-12-11 06:03:55.907695: step 17620, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 47h:43m:12s remains)
INFO - root - 2017-12-11 06:04:01.318919: step 17630, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:59m:21s remains)
INFO - root - 2017-12-11 06:04:06.613322: step 17640, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:17m:23s remains)
INFO - root - 2017-12-11 06:04:11.645830: step 17650, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.525 sec/batch; 45h:52m:26s remains)
INFO - root - 2017-12-11 06:04:16.985712: step 17660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:14m:13s remains)
INFO - root - 2017-12-11 06:04:22.405464: step 17670, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:50m:32s remains)
INFO - root - 2017-12-11 06:04:27.671837: step 17680, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 45h:13m:05s remains)
INFO - root - 2017-12-11 06:04:32.966514: step 17690, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.514 sec/batch; 44h:57m:59s remains)
INFO - root - 2017-12-11 06:04:38.338577: step 17700, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 45h:33m:27s remains)
2017-12-11 06:04:38.876564: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.281848 0.28609571 0.28971592 0.29759979 0.3012937 0.29158503 0.26998749 0.24746181 0.23474629 0.23111205 0.22355308 0.20604521 0.18331447 0.16770247 0.16746558][0.34478298 0.35273275 0.35912642 0.36716169 0.36871722 0.3565371 0.33361447 0.30816284 0.28833258 0.27557638 0.26068404 0.24068446 0.21982826 0.20840047 0.2121236][0.38074195 0.38756514 0.39495018 0.40242049 0.40389416 0.39567658 0.38126394 0.36249062 0.34187666 0.32070705 0.29387787 0.26418048 0.24084748 0.23541465 0.24805455][0.39076731 0.38783991 0.38893691 0.39276916 0.39620364 0.3975758 0.39829972 0.39293596 0.37848371 0.35418642 0.31765521 0.27678311 0.24920583 0.25043777 0.27465966][0.367418 0.35856691 0.3510778 0.34883627 0.35176075 0.35810536 0.36638266 0.36923853 0.36305684 0.34589574 0.3154234 0.27802968 0.25399289 0.26061562 0.28964728][0.32058302 0.31359369 0.30370423 0.29740724 0.29836896 0.30497029 0.31363437 0.31956574 0.32162195 0.31797364 0.30470291 0.28184104 0.26491642 0.27002475 0.29206803][0.27258846 0.27446571 0.26814067 0.25971496 0.25789368 0.26346704 0.27282447 0.28429288 0.29796448 0.31057632 0.31450051 0.30359539 0.28784794 0.2835421 0.29302302][0.24410768 0.25556839 0.25258592 0.23972037 0.2310899 0.23268852 0.2423058 0.2596404 0.28449962 0.31183738 0.3308976 0.33139247 0.31785843 0.30591533 0.30445114][0.23120372 0.24732889 0.24527588 0.22706987 0.20931232 0.20291387 0.20832691 0.22594953 0.25541756 0.29137868 0.32454714 0.34264478 0.34245345 0.33216915 0.32235709][0.21582386 0.23339823 0.23290733 0.21398826 0.19026478 0.17528522 0.17366557 0.18771036 0.21705659 0.25551367 0.29766443 0.33176681 0.34858212 0.34558249 0.3305693][0.1885599 0.20563219 0.21033362 0.19829415 0.17565382 0.15610033 0.14946 0.16132221 0.19107762 0.23015687 0.27403182 0.31218857 0.33525491 0.33456787 0.31430465][0.16092002 0.17578803 0.18416968 0.17977433 0.162693 0.14434586 0.13713959 0.14889731 0.17854318 0.21605363 0.25569785 0.28844759 0.30844021 0.30599141 0.28237483][0.14842242 0.15532133 0.15731739 0.15224971 0.13986203 0.12738021 0.12462919 0.13832992 0.16631916 0.1988824 0.22981733 0.25355488 0.26910773 0.26757812 0.24664856][0.15890671 0.15346725 0.14052676 0.12667939 0.11647768 0.11262426 0.118591 0.13665958 0.1625929 0.1873697 0.20661885 0.22009769 0.23137946 0.23244071 0.21682948][0.17641367 0.15985183 0.13336755 0.11115853 0.1034228 0.11026363 0.12826365 0.15378818 0.17944285 0.19652979 0.20397916 0.20678183 0.21077882 0.2085612 0.19250429]]...]
INFO - root - 2017-12-11 06:04:44.222816: step 17710, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:13m:25s remains)
INFO - root - 2017-12-11 06:04:49.561665: step 17720, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:39m:57s remains)
INFO - root - 2017-12-11 06:04:54.861058: step 17730, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:05m:21s remains)
INFO - root - 2017-12-11 06:05:00.199424: step 17740, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:12m:37s remains)
INFO - root - 2017-12-11 06:05:05.303862: step 17750, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:19m:25s remains)
INFO - root - 2017-12-11 06:05:10.658922: step 17760, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:39m:46s remains)
INFO - root - 2017-12-11 06:05:16.138511: step 17770, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 49h:06m:38s remains)
INFO - root - 2017-12-11 06:05:21.471781: step 17780, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 48h:22m:51s remains)
INFO - root - 2017-12-11 06:05:26.791948: step 17790, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:21m:41s remains)
INFO - root - 2017-12-11 06:05:32.148181: step 17800, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 45h:39m:41s remains)
2017-12-11 06:05:32.689615: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049513027 0.058966991 0.060528871 0.055258553 0.044876728 0.031671539 0.018514173 0.0080121849 0.0021114494 0.00070936873 0.0009252234 0.001740142 0.0040288852 0.0085166516 0.016161818][0.081384189 0.093855344 0.0927191 0.081823759 0.065592751 0.047941942 0.031924877 0.019902432 0.013260268 0.011788867 0.012377873 0.014383473 0.019677535 0.028293222 0.039892338][0.11265416 0.12579392 0.11989171 0.10256609 0.081538759 0.061677326 0.044877309 0.032346833 0.02513303 0.023922976 0.025710877 0.030050714 0.039258968 0.052047085 0.066031061][0.13171868 0.14287837 0.13201378 0.10995156 0.087006286 0.067961857 0.052831743 0.041172244 0.034041453 0.033651844 0.037376221 0.044148058 0.055866305 0.069867529 0.081810594][0.13526744 0.14369114 0.13019285 0.1069665 0.085001737 0.06871561 0.056712382 0.047160145 0.040917479 0.0414628 0.046742365 0.054248612 0.064952366 0.075575933 0.081540018][0.12745717 0.13407655 0.12104711 0.0995854 0.079691656 0.066097915 0.057234135 0.050207362 0.044989306 0.045500029 0.050485972 0.056324821 0.063028991 0.0678449 0.067660086][0.11585407 0.12179583 0.11056914 0.091499008 0.073276527 0.061699517 0.0556795 0.051121995 0.0460722 0.044290666 0.046267267 0.048763119 0.050982416 0.051259775 0.04841258][0.10402492 0.1096345 0.10043897 0.083924763 0.067426212 0.057484586 0.053539325 0.050545163 0.044550788 0.03867184 0.035674203 0.034070559 0.032995608 0.031957451 0.030827863][0.095066637 0.10091573 0.094106726 0.080266789 0.064731941 0.053874988 0.048891138 0.045629665 0.03893476 0.030020094 0.022773797 0.017901162 0.015702024 0.016763709 0.020786107][0.094572745 0.10128185 0.096288793 0.083362743 0.065989934 0.05021603 0.040281083 0.034762248 0.028292853 0.019346654 0.010948121 0.0056101745 0.0053578387 0.011655047 0.02327349][0.10384005 0.11030363 0.10401729 0.088267706 0.066550262 0.044800021 0.02947475 0.021265013 0.015534999 0.0086249774 0.0017641116 -0.0015541726 0.0021519118 0.014907006 0.034333475][0.11886548 0.12360477 0.11248834 0.090720758 0.064235188 0.038966969 0.021076731 0.011443096 0.006377975 0.0015334568 -0.0034344369 -0.0047440762 0.0020888092 0.019704627 0.045055497][0.13224915 0.13421863 0.11622136 0.087132722 0.056880832 0.031556983 0.014893513 0.0062827989 0.0024399778 -0.00092658622 -0.0051284945 -0.0063648568 0.00075850874 0.019753274 0.048049495][0.13719948 0.13437134 0.1100148 0.075180739 0.042983312 0.01966475 0.0064871591 0.00091250613 -0.000731226 -0.0026215287 -0.0063532242 -0.0082655875 -0.002620558 0.014905301 0.043004576][0.12249926 0.11458288 0.087620191 0.052248005 0.021858728 0.002229067 -0.006951591 -0.0093675386 -0.0087778913 -0.0088515012 -0.011019082 -0.01241044 -0.0082274741 0.0063224491 0.031380672]]...]
INFO - root - 2017-12-11 06:05:38.042245: step 17810, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:05m:51s remains)
INFO - root - 2017-12-11 06:05:43.427912: step 17820, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 45h:32m:53s remains)
INFO - root - 2017-12-11 06:05:48.733341: step 17830, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:13m:16s remains)
INFO - root - 2017-12-11 06:05:54.061599: step 17840, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:12m:05s remains)
INFO - root - 2017-12-11 06:05:59.125931: step 17850, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.529 sec/batch; 46h:16m:32s remains)
INFO - root - 2017-12-11 06:06:04.442232: step 17860, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.521 sec/batch; 45h:33m:05s remains)
INFO - root - 2017-12-11 06:06:09.803993: step 17870, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:01m:02s remains)
INFO - root - 2017-12-11 06:06:15.084801: step 17880, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 45h:41m:01s remains)
INFO - root - 2017-12-11 06:06:20.498982: step 17890, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:52m:59s remains)
INFO - root - 2017-12-11 06:06:25.848120: step 17900, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:16m:25s remains)
2017-12-11 06:06:26.406706: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29661956 0.31817952 0.31288612 0.28202894 0.23880586 0.19522706 0.16400568 0.15864222 0.17795332 0.21375729 0.26829696 0.31850043 0.34435108 0.33007815 0.28037256][0.32682243 0.35377306 0.34346116 0.29932636 0.2411738 0.18785295 0.155231 0.15542564 0.18370368 0.22562192 0.27809021 0.31932735 0.33294183 0.3092981 0.25480583][0.32089475 0.34706521 0.33097443 0.2793704 0.21570365 0.16199093 0.13495062 0.14233521 0.176305 0.2184539 0.26088393 0.28537163 0.2831299 0.25095984 0.1968893][0.29807961 0.31040132 0.28755134 0.23885588 0.18505394 0.143399 0.12840541 0.1429106 0.17749089 0.21354334 0.24157844 0.24854609 0.23200744 0.19365345 0.14217462][0.26528928 0.25591472 0.22724104 0.19190207 0.16244537 0.14386711 0.14493431 0.16359821 0.19194527 0.21549506 0.22660564 0.21878371 0.19276191 0.1517857 0.10309903][0.24005504 0.21038054 0.17998642 0.16527277 0.16776799 0.1766931 0.19316922 0.21003385 0.22303192 0.22670038 0.21924824 0.2010124 0.17181696 0.13260852 0.087206304][0.23354359 0.19205384 0.16657656 0.17490596 0.20860822 0.24337682 0.27204329 0.28183907 0.27304578 0.25214383 0.22561869 0.19922002 0.16975634 0.13445246 0.092718892][0.24109058 0.19795847 0.18289647 0.21157572 0.26877418 0.32181966 0.35729304 0.35797182 0.32711428 0.28271395 0.2395485 0.20662816 0.177854 0.14703731 0.10950958][0.24642414 0.20918275 0.20581304 0.24674283 0.31427076 0.37476844 0.41227508 0.40648431 0.36130795 0.30279511 0.25173733 0.21718137 0.19098879 0.16499127 0.13201042][0.22967893 0.20111832 0.20438893 0.24570237 0.30923879 0.36717471 0.40421754 0.39931434 0.35438541 0.29818073 0.25391176 0.22734389 0.20882861 0.1892491 0.16051954][0.19503763 0.16569613 0.16505402 0.19442365 0.24217139 0.28978586 0.3252334 0.32944748 0.30029655 0.26396596 0.24330096 0.23736869 0.23360138 0.22190525 0.1958314][0.14565973 0.10991445 0.09776777 0.1089632 0.13616987 0.16950719 0.20130295 0.2164453 0.21001427 0.20361316 0.21738392 0.24120691 0.25834018 0.25692773 0.23307635][0.09248177 0.053666174 0.031415783 0.027568342 0.037566923 0.057214629 0.082718976 0.10374982 0.11594242 0.13739373 0.18510501 0.23947355 0.27897975 0.28939974 0.26856476][0.054063976 0.020479424 -0.0032818911 -0.01284398 -0.011089738 0.00013761902 0.018847344 0.038707271 0.058480997 0.096008293 0.1664401 0.2427597 0.29953834 0.32042429 0.30265841][0.03877762 0.014501358 -0.0034582692 -0.010949098 -0.0097739678 -0.0017573778 0.011005921 0.024953904 0.041771069 0.080894157 0.15757096 0.24216715 0.30744314 0.33494228 0.32038659]]...]
INFO - root - 2017-12-11 06:06:31.689312: step 17910, loss = 0.69, batch loss = 0.63 (15.9 examples/sec; 0.504 sec/batch; 44h:00m:39s remains)
INFO - root - 2017-12-11 06:06:37.077441: step 17920, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 46h:09m:34s remains)
INFO - root - 2017-12-11 06:06:42.334050: step 17930, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:38m:36s remains)
INFO - root - 2017-12-11 06:06:47.770929: step 17940, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:18m:18s remains)
INFO - root - 2017-12-11 06:06:53.115063: step 17950, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:28m:40s remains)
INFO - root - 2017-12-11 06:06:58.192601: step 17960, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:11m:45s remains)
INFO - root - 2017-12-11 06:07:03.525225: step 17970, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:37m:39s remains)
INFO - root - 2017-12-11 06:07:08.866928: step 17980, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:14m:07s remains)
INFO - root - 2017-12-11 06:07:14.226027: step 17990, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 49h:18m:22s remains)
INFO - root - 2017-12-11 06:07:19.596518: step 18000, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:14m:27s remains)
2017-12-11 06:07:20.172821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.089129411 -0.0805106 -0.05823268 -0.027632816 0.007356646 0.041370027 0.065024279 0.071304739 0.064702183 0.049963888 0.040386468 0.041705202 0.056642804 0.082813367 0.1101515][-0.0947881 -0.080816716 -0.047837965 -0.0020503923 0.051726736 0.10588289 0.1453553 0.15898994 0.1523734 0.13083857 0.1113467 0.10359291 0.1141634 0.14100178 0.17135531][-0.09253855 -0.073252894 -0.030904226 0.029215936 0.10251221 0.17854103 0.2369795 0.26119119 0.25662693 0.22818354 0.19465423 0.17047007 0.16791943 0.1877059 0.21536933][-0.092033736 -0.069501422 -0.020495145 0.051289592 0.14284253 0.24024589 0.31848463 0.35492316 0.35375932 0.31879541 0.26733807 0.21923168 0.19382584 0.1967274 0.21376765][-0.093377545 -0.069297165 -0.015497239 0.065843388 0.17383087 0.29071614 0.38771129 0.43653879 0.43891189 0.39786506 0.32640243 0.250178 0.19567087 0.17446342 0.17412844][-0.0948607 -0.070015863 -0.011319664 0.079415381 0.2028155 0.33702686 0.45012435 0.50856763 0.51034039 0.45978305 0.36657423 0.26265404 0.18016492 0.13610728 0.11994526][-0.097076558 -0.071263716 -0.0072151646 0.093309134 0.23104225 0.37983552 0.50502783 0.56811082 0.56288075 0.49769408 0.38332462 0.25887668 0.15944409 0.10404207 0.082733445][-0.099615462 -0.073518775 -0.0042237244 0.10581478 0.25537229 0.41443649 0.54651868 0.60935295 0.59291446 0.51127982 0.38018849 0.24575053 0.14290996 0.088519238 0.072162636][-0.10222162 -0.076851279 -0.0037048343 0.11417948 0.27213946 0.43706465 0.57108289 0.62992227 0.60160792 0.506667 0.36715072 0.23373632 0.13875806 0.093895316 0.086846359][-0.10519624 -0.082938634 -0.01138556 0.10714248 0.26497003 0.42799824 0.55786419 0.60998982 0.57248354 0.47133896 0.33412659 0.21184491 0.133401 0.1041334 0.10785486][-0.10831962 -0.091750495 -0.029255221 0.078782409 0.22397389 0.37383345 0.49140567 0.53420985 0.49269462 0.3944664 0.26952025 0.16462922 0.10553794 0.092800811 0.10606449][-0.11134303 -0.10174649 -0.054370772 0.032288957 0.15251425 0.278091 0.37536812 0.40682909 0.36787617 0.28386036 0.18109675 0.097840786 0.056395538 0.055816896 0.073478736][-0.11535596 -0.11304368 -0.083167151 -0.023222703 0.065913968 0.16196525 0.23539093 0.25500682 0.2225365 0.15958653 0.084874175 0.02551331 -0.0010461179 0.0045710071 0.01959604][-0.12228875 -0.12844157 -0.11748849 -0.086375356 -0.030726133 0.034179345 0.08391802 0.093781136 0.070553549 0.031113721 -0.01402188 -0.049147096 -0.062496848 -0.054253113 -0.046029616][-0.13048831 -0.14599138 -0.15416238 -0.15232132 -0.13206767 -0.10067729 -0.075425409 -0.073625021 -0.08603555 -0.10162389 -0.11770257 -0.12909238 -0.13035397 -0.12153634 -0.12080003]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:07:25.442981: step 18010, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:23m:53s remains)
INFO - root - 2017-12-11 06:07:30.791162: step 18020, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:35m:11s remains)
INFO - root - 2017-12-11 06:07:36.090044: step 18030, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.518 sec/batch; 45h:12m:32s remains)
INFO - root - 2017-12-11 06:07:41.423032: step 18040, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:25m:39s remains)
INFO - root - 2017-12-11 06:07:46.833794: step 18050, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:20m:03s remains)
INFO - root - 2017-12-11 06:07:51.914977: step 18060, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 46h:31m:50s remains)
INFO - root - 2017-12-11 06:07:57.315162: step 18070, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:44m:22s remains)
INFO - root - 2017-12-11 06:08:02.722108: step 18080, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:01m:53s remains)
INFO - root - 2017-12-11 06:08:08.120958: step 18090, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:29m:48s remains)
INFO - root - 2017-12-11 06:08:13.450512: step 18100, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:45m:39s remains)
2017-12-11 06:08:13.978889: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.066472776 0.047741979 0.039541796 0.0371123 0.037794009 0.037265666 0.03638741 0.036865938 0.039194651 0.042153426 0.043410853 0.046052165 0.053872734 0.071560323 0.10489816][0.11459928 0.095074832 0.085153058 0.077871338 0.072517186 0.065726064 0.05856853 0.05400008 0.053549748 0.056343775 0.059728969 0.065427348 0.076803789 0.10204779 0.15171336][0.16375904 0.15574983 0.1525728 0.14469333 0.13249809 0.11450938 0.094239108 0.077197708 0.066904254 0.063544154 0.064527668 0.069422252 0.0810398 0.11136879 0.17564029][0.21195266 0.23116791 0.24965924 0.25284806 0.23923559 0.20884977 0.1695182 0.13142928 0.10212427 0.083923616 0.074446112 0.0707772 0.075062305 0.10233834 0.17111838][0.25639656 0.31444135 0.37012252 0.40207955 0.40255067 0.37061498 0.31620461 0.25372908 0.19654584 0.15162535 0.11826202 0.092437431 0.077003755 0.088974789 0.14762361][0.29039678 0.39126927 0.49233237 0.56857038 0.60346109 0.590972 0.53649503 0.45480689 0.36552459 0.28321907 0.21112333 0.14705835 0.097533077 0.081485406 0.11657734][0.30383518 0.43696114 0.57844496 0.70192277 0.78527427 0.81439471 0.7819187 0.69596243 0.58021986 0.45808852 0.33874223 0.2259528 0.13358432 0.0833282 0.089248374][0.28991854 0.43228796 0.59220916 0.74577165 0.87272382 0.95187515 0.95780754 0.88603079 0.76049757 0.61147064 0.45287174 0.29642713 0.16607715 0.087517232 0.070308536][0.24293542 0.36631447 0.51236784 0.66420293 0.81009597 0.92607945 0.9700377 0.92401558 0.80917466 0.65781313 0.4846437 0.30770227 0.16057919 0.0727562 0.049851686][0.17705755 0.25775257 0.35850021 0.4742341 0.60551107 0.73020196 0.79629773 0.77657026 0.687442 0.55839896 0.40097851 0.23498239 0.099583015 0.025708513 0.016336855][0.12618817 0.15231651 0.18919098 0.24315499 0.32803577 0.42800853 0.4919644 0.49004278 0.43324095 0.34357482 0.22627513 0.098559685 -0.00089042285 -0.043393541 -0.027790131][0.11500026 0.092000768 0.065082 0.050610952 0.073330224 0.12681997 0.16921344 0.17387809 0.14688756 0.10106532 0.034668472 -0.040648784 -0.093029357 -0.0994925 -0.058984965][0.14502192 0.090371095 0.017097866 -0.051781215 -0.084467322 -0.077901751 -0.062142357 -0.05900858 -0.066423036 -0.078802817 -0.10177876 -0.12988225 -0.14132224 -0.12045322 -0.06484367][0.19292933 0.12902254 0.037191816 -0.059047569 -0.12426788 -0.14718039 -0.1477349 -0.14588037 -0.14307275 -0.13797566 -0.1363584 -0.13691911 -0.1278373 -0.098398365 -0.043986466][0.22458479 0.17443 0.092789933 -0.0010290748 -0.072626151 -0.104536 -0.10855958 -0.10358916 -0.094679847 -0.083351612 -0.074109592 -0.067917712 -0.060138196 -0.04139876 -0.0046197358]]...]
INFO - root - 2017-12-11 06:08:19.232283: step 18110, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:28m:49s remains)
INFO - root - 2017-12-11 06:08:24.611239: step 18120, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 47h:41m:22s remains)
INFO - root - 2017-12-11 06:08:29.974232: step 18130, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.508 sec/batch; 44h:20m:27s remains)
INFO - root - 2017-12-11 06:08:35.214601: step 18140, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:51m:55s remains)
INFO - root - 2017-12-11 06:08:40.605901: step 18150, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 49h:45m:45s remains)
INFO - root - 2017-12-11 06:08:45.788720: step 18160, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 46h:07m:32s remains)
INFO - root - 2017-12-11 06:08:51.158356: step 18170, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 50h:01m:20s remains)
INFO - root - 2017-12-11 06:08:56.486248: step 18180, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:13m:05s remains)
INFO - root - 2017-12-11 06:09:01.810701: step 18190, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:17m:16s remains)
INFO - root - 2017-12-11 06:09:07.191335: step 18200, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:59m:27s remains)
2017-12-11 06:09:07.777564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027211953 -0.013034183 0.0025045152 0.014733202 0.019976621 0.017097384 0.0092107346 -0.00010574055 -0.011118388 -0.022288293 -0.032013062 -0.039952479 -0.0475847 -0.055058867 -0.0599526][0.039632857 0.069363743 0.09908925 0.12379905 0.13818702 0.13733923 0.12494444 0.10842039 0.087818436 0.065286681 0.044247989 0.026972452 0.011712632 -0.0030165673 -0.013236071][0.14299381 0.19571085 0.24667709 0.29097226 0.32106659 0.32601619 0.30964985 0.28344375 0.24807529 0.2073874 0.16802834 0.13571991 0.10822158 0.082585774 0.065054171][0.25762731 0.33654779 0.41361836 0.48307997 0.53528386 0.55222672 0.53657275 0.50246418 0.4520269 0.39122391 0.32985005 0.27784857 0.23236063 0.19003883 0.16059904][0.34969178 0.45484391 0.56047231 0.65800661 0.73714757 0.773185 0.765365 0.72765648 0.665518 0.58770865 0.50559729 0.43314758 0.36698604 0.3043586 0.25834763][0.39372638 0.52255452 0.65673679 0.7834782 0.89147931 0.95053089 0.95665967 0.92036355 0.85121047 0.76144546 0.66289639 0.57314271 0.48820212 0.40586132 0.34112486][0.38221669 0.52569091 0.68237853 0.83397281 0.96679986 1.0484344 1.0735736 1.0455501 0.97733986 0.88469982 0.77959293 0.68183118 0.58585161 0.48954731 0.40730038][0.31935903 0.46401584 0.630649 0.79626065 0.94474351 1.0453671 1.0921729 1.0790526 1.022198 0.9407016 0.8445816 0.75244981 0.656612 0.55558872 0.45981392][0.23052467 0.36421484 0.52590978 0.69083768 0.84133792 0.95047617 1.0123496 1.0138541 0.97582412 0.92046481 0.852695 0.78411 0.70347905 0.61037093 0.50870585][0.15320238 0.26926228 0.41403782 0.56483829 0.70277715 0.80476564 0.86643207 0.87365323 0.85466814 0.83542824 0.81295556 0.7852034 0.73469579 0.66210967 0.56402212][0.11345364 0.21067056 0.33018127 0.4544825 0.56501222 0.64390004 0.68926 0.69084466 0.68569964 0.70308042 0.73229796 0.75431943 0.74250704 0.69757843 0.60991538][0.11490809 0.19605923 0.28632721 0.37333986 0.44316953 0.487224 0.50537217 0.49250779 0.49289981 0.53788686 0.61114407 0.67788982 0.70238143 0.68492091 0.61198139][0.13366205 0.20265831 0.26455587 0.31033838 0.33380538 0.33855316 0.32640791 0.29730445 0.29804504 0.35851148 0.45870021 0.55355406 0.60179782 0.60370719 0.54640454][0.13526246 0.19198327 0.2307477 0.24337724 0.22877976 0.19987284 0.16095503 0.11735319 0.11501654 0.17891932 0.28704971 0.39025173 0.44718629 0.4584285 0.41609055][0.094594419 0.13459255 0.15614623 0.14903975 0.11275756 0.064338043 0.0090758866 -0.044008918 -0.051928729 0.003864689 0.10202006 0.1961855 0.25116542 0.26743111 0.24129219]]...]
INFO - root - 2017-12-11 06:09:13.106569: step 18210, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:02m:43s remains)
INFO - root - 2017-12-11 06:09:18.486227: step 18220, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:26m:58s remains)
INFO - root - 2017-12-11 06:09:23.913175: step 18230, loss = 0.65, batch loss = 0.59 (14.8 examples/sec; 0.541 sec/batch; 47h:13m:41s remains)
INFO - root - 2017-12-11 06:09:29.210325: step 18240, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:59m:20s remains)
INFO - root - 2017-12-11 06:09:34.558561: step 18250, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:06m:31s remains)
INFO - root - 2017-12-11 06:09:39.771282: step 18260, loss = 0.69, batch loss = 0.63 (19.2 examples/sec; 0.417 sec/batch; 36h:24m:04s remains)
INFO - root - 2017-12-11 06:09:45.008177: step 18270, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:59m:59s remains)
INFO - root - 2017-12-11 06:09:50.432381: step 18280, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 45h:50m:40s remains)
INFO - root - 2017-12-11 06:09:55.780918: step 18290, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.532 sec/batch; 46h:23m:33s remains)
INFO - root - 2017-12-11 06:10:01.029381: step 18300, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:51m:40s remains)
2017-12-11 06:10:01.552569: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13578583 0.10986112 0.082358785 0.061567921 0.043229535 0.024751982 0.014591142 0.010366486 0.004151741 -0.0090069389 -0.027837839 -0.04716007 -0.061958496 -0.061993152 -0.040117458][0.10676256 0.082383409 0.066655092 0.064944386 0.0685048 0.0709787 0.074601106 0.075087115 0.064841509 0.040826704 0.0075107729 -0.026655721 -0.053940337 -0.061499391 -0.040645067][0.074732929 0.054898396 0.056205552 0.081893042 0.11755615 0.1493232 0.17326343 0.18122561 0.16462882 0.12328678 0.066830128 0.0095729679 -0.036231674 -0.056155719 -0.04083677][0.062232342 0.050545629 0.070491523 0.12487046 0.19538997 0.26050448 0.30816495 0.32471183 0.29975322 0.23546009 0.1478845 0.059773721 -0.010160058 -0.04604765 -0.038927965][0.06359981 0.063319325 0.10247134 0.18474449 0.28947988 0.3883391 0.46073774 0.48641732 0.45295572 0.36431357 0.24288407 0.12076817 0.024243135 -0.029258013 -0.032091737][0.071898073 0.083375104 0.13980119 0.24642506 0.38118392 0.50985348 0.60436296 0.63786662 0.59607106 0.48455122 0.33161616 0.17818521 0.057761148 -0.01088813 -0.022124872][0.087598406 0.10996037 0.18014103 0.30485165 0.46114045 0.61040533 0.71949965 0.75609535 0.70527625 0.57456291 0.39731279 0.22105582 0.084166571 0.005851761 -0.01102327][0.10587312 0.13608277 0.21429364 0.34813881 0.51445168 0.67262965 0.78694671 0.82139915 0.76225185 0.619643 0.43025282 0.24426974 0.1016285 0.020702882 0.00057859806][0.12938614 0.16111335 0.23793486 0.36756471 0.52766973 0.67910808 0.78699654 0.81463397 0.75090361 0.60885197 0.42574722 0.24891227 0.11507972 0.039974183 0.018671479][0.16287878 0.19118467 0.25601825 0.36631763 0.50189459 0.62890768 0.71712387 0.733346 0.66992825 0.542729 0.38674226 0.24070975 0.13241693 0.072395913 0.052233249][0.21343528 0.23528969 0.27984115 0.35764784 0.45221737 0.53857917 0.59514111 0.59690726 0.54044789 0.44249651 0.33234715 0.23540543 0.16599837 0.12678412 0.10755738][0.28084868 0.29437679 0.31368867 0.34991017 0.39206398 0.42794979 0.44752443 0.43614468 0.39478621 0.33823475 0.28674746 0.24930155 0.22504361 0.20753211 0.1866814][0.35576677 0.36128092 0.35443789 0.34675202 0.33379415 0.31798255 0.30161896 0.2808426 0.26033956 0.24975739 0.25775313 0.27694294 0.29345006 0.29315919 0.26849234][0.43241853 0.43394354 0.40749478 0.36477259 0.30634952 0.24724214 0.20093966 0.17397371 0.17389189 0.20273589 0.25720575 0.31697074 0.35965654 0.36611721 0.33361438][0.4795253 0.48382342 0.45023659 0.39047682 0.30654261 0.22115289 0.15618795 0.12633911 0.14087985 0.19409016 0.27278379 0.349941 0.40022153 0.4029206 0.36080104]]...]
INFO - root - 2017-12-11 06:10:06.888885: step 18310, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:46m:00s remains)
INFO - root - 2017-12-11 06:10:12.256566: step 18320, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 46h:42m:19s remains)
INFO - root - 2017-12-11 06:10:17.676103: step 18330, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:28m:15s remains)
INFO - root - 2017-12-11 06:10:23.089720: step 18340, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.545 sec/batch; 47h:31m:49s remains)
INFO - root - 2017-12-11 06:10:28.373454: step 18350, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 45h:05m:46s remains)
INFO - root - 2017-12-11 06:10:33.772892: step 18360, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:36m:30s remains)
INFO - root - 2017-12-11 06:10:38.867882: step 18370, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.576 sec/batch; 50h:17m:17s remains)
INFO - root - 2017-12-11 06:10:44.282311: step 18380, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 47h:37m:26s remains)
INFO - root - 2017-12-11 06:10:49.696800: step 18390, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:27m:22s remains)
INFO - root - 2017-12-11 06:10:55.058367: step 18400, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.520 sec/batch; 45h:21m:46s remains)
2017-12-11 06:10:55.660043: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1909748 0.22317734 0.24963494 0.26953202 0.2796565 0.28263813 0.29491195 0.32192346 0.34719467 0.35034698 0.32392105 0.26781079 0.18715705 0.098777622 0.023264363][0.16317433 0.1962627 0.22208993 0.23697603 0.23754478 0.22857989 0.22670943 0.24309959 0.26754585 0.28242642 0.2778545 0.25110489 0.19763267 0.12250829 0.045506079][0.120902 0.14425269 0.15988106 0.1649743 0.15709585 0.13988972 0.13028108 0.14615348 0.18240306 0.22432719 0.2563301 0.26607695 0.23786177 0.1694088 0.083213665][0.10709927 0.11607388 0.11785332 0.11142562 0.096832857 0.077461943 0.069559775 0.095050007 0.15088233 0.22216831 0.2856119 0.31936029 0.30048439 0.22494957 0.12149344][0.12781213 0.13075873 0.12665862 0.11531825 0.099478953 0.083708443 0.0818954 0.11624323 0.18353826 0.2681579 0.34250775 0.37998605 0.35594785 0.26694947 0.14660461][0.16402009 0.17148924 0.17371744 0.17047536 0.16588321 0.16428791 0.17361853 0.21170044 0.27396861 0.34562281 0.40241334 0.42086238 0.38002613 0.27753663 0.1468312][0.19041963 0.20671244 0.22344512 0.24087435 0.26259005 0.28955543 0.32003558 0.36174843 0.40715954 0.44351608 0.4581472 0.43806708 0.36974552 0.25367135 0.11965802][0.18986185 0.21492541 0.25053483 0.29884627 0.36003605 0.42701229 0.48678717 0.53423065 0.55891353 0.55144936 0.51433659 0.44774106 0.3464314 0.21571498 0.081588373][0.1518863 0.18534361 0.24318501 0.329389 0.43627414 0.543647 0.6287846 0.67701811 0.679098 0.63199568 0.55146706 0.44792867 0.32164973 0.18171254 0.050558146][0.080102041 0.11846916 0.19527107 0.31450209 0.45802006 0.59160453 0.68682778 0.72574711 0.70575911 0.63290852 0.53092825 0.41492623 0.28528249 0.15060262 0.029354706][0.0043323063 0.039676089 0.12135845 0.25094959 0.4017449 0.53187859 0.61536783 0.63776153 0.60451436 0.52914816 0.43549919 0.3370558 0.22913834 0.11645614 0.013528077][-0.043782711 -0.021267496 0.045000222 0.15216774 0.27169693 0.3655079 0.41818112 0.42393988 0.39366087 0.34280893 0.28657943 0.23032348 0.16177581 0.081150651 0.00090259558][-0.059880055 -0.05641127 -0.0201336 0.041869339 0.10651936 0.14802375 0.16393355 0.15838954 0.14426154 0.13359109 0.12856565 0.12279111 0.096978553 0.047982022 -0.011348664][-0.063823909 -0.076508626 -0.068973474 -0.05046647 -0.035633698 -0.037452206 -0.048196245 -0.056700848 -0.050985996 -0.025180234 0.012164786 0.0452901 0.050730053 0.023783196 -0.021468587][-0.075510606 -0.092470832 -0.098795004 -0.10402976 -0.11487158 -0.1354658 -0.15339242 -0.15601778 -0.13477045 -0.089513019 -0.033523202 0.014670588 0.031732358 0.01340852 -0.025781156]]...]
INFO - root - 2017-12-11 06:11:00.992600: step 18410, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:41m:48s remains)
INFO - root - 2017-12-11 06:11:06.336544: step 18420, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:03m:37s remains)
INFO - root - 2017-12-11 06:11:11.734553: step 18430, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:43m:17s remains)
INFO - root - 2017-12-11 06:11:16.991118: step 18440, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 45h:22m:18s remains)
INFO - root - 2017-12-11 06:11:22.331002: step 18450, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:51m:41s remains)
INFO - root - 2017-12-11 06:11:27.599069: step 18460, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:54m:00s remains)
INFO - root - 2017-12-11 06:11:32.666471: step 18470, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:12m:27s remains)
INFO - root - 2017-12-11 06:11:38.043712: step 18480, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 45h:31m:52s remains)
INFO - root - 2017-12-11 06:11:43.453808: step 18490, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:44m:05s remains)
INFO - root - 2017-12-11 06:11:48.866209: step 18500, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 49h:12m:36s remains)
2017-12-11 06:11:49.402253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032278474 -0.033133563 -0.033144768 -0.034070343 -0.036329865 -0.040003497 -0.044728737 -0.04946322 -0.052581653 -0.053088497 -0.052208308 -0.051215097 -0.049032364 -0.04225263 -0.031637285][-0.030317975 -0.026137691 -0.019235862 -0.01311186 -0.010799823 -0.01412562 -0.022051644 -0.031763047 -0.040180553 -0.045655977 -0.048310015 -0.049794536 -0.049565233 -0.043983195 -0.033691403][-0.021610932 -0.0064554852 0.014052203 0.033637531 0.045855273 0.046127766 0.035313651 0.018184375 0.0005077534 -0.013904154 -0.024169041 -0.031731591 -0.036680833 -0.035293534 -0.027865456][-0.011620667 0.018989343 0.059087247 0.098567531 0.12658013 0.13471341 0.12324437 0.098865092 0.070000246 0.042963319 0.020274952 0.0015286352 -0.013308874 -0.020098278 -0.018500542][-0.0019595223 0.044893984 0.10623528 0.16819717 0.21572538 0.23633316 0.2290356 0.20128131 0.16296512 0.12223483 0.083666556 0.049011648 0.019765599 0.0012197876 -0.0056504253][0.0062814411 0.064421721 0.1412634 0.22104175 0.28659371 0.3227458 0.32623854 0.30250245 0.26055524 0.20906876 0.15457544 0.10198191 0.056152321 0.024556246 0.0084886933][0.012980324 0.07291767 0.15320735 0.23937564 0.31553453 0.36615518 0.38472146 0.37155902 0.33259481 0.27604306 0.2099397 0.14249182 0.08293505 0.041465189 0.019046387][0.017548066 0.069243096 0.13963588 0.21844293 0.29412228 0.35289413 0.38475969 0.38361841 0.35168138 0.29641479 0.22642468 0.15221283 0.087033622 0.043357026 0.020933533][0.018593682 0.055424634 0.10654602 0.16705522 0.23111176 0.28816298 0.32586616 0.33302033 0.30885598 0.260113 0.19473484 0.12410674 0.064032286 0.027922563 0.013307267][0.013370751 0.033931412 0.062967844 0.10000896 0.14459293 0.19009927 0.22405882 0.23339041 0.21558098 0.17642252 0.12234656 0.064516053 0.019364512 -0.0004757767 -0.00038814166][-0.0016693517 0.004717126 0.014620095 0.029515279 0.052396204 0.080327995 0.10314182 0.10928182 0.095986426 0.067834191 0.029344313 -0.0094487313 -0.033353087 -0.032103367 -0.013813777][-0.023732459 -0.02929562 -0.033701371 -0.035584893 -0.031149575 -0.020676538 -0.010757186 -0.009483478 -0.01890791 -0.035810549 -0.057540111 -0.0758624 -0.077962421 -0.056151476 -0.020143546][-0.040670205 -0.05766305 -0.0728351 -0.08557111 -0.093237996 -0.094962135 -0.094119251 -0.09545964 -0.10042008 -0.10731717 -0.1147965 -0.11639076 -0.10146352 -0.063754104 -0.01296706][-0.033376392 -0.061910395 -0.086257651 -0.10597392 -0.12044358 -0.12890543 -0.13261975 -0.13411646 -0.13502228 -0.13529961 -0.13403782 -0.12586625 -0.10116838 -0.053318691 0.0091650914][0.010261193 -0.026688058 -0.059331048 -0.085570708 -0.10626283 -0.12064658 -0.12838353 -0.1306753 -0.12985867 -0.12742053 -0.12255 -0.11073783 -0.082479335 -0.029277982 0.041970812]]...]
INFO - root - 2017-12-11 06:11:54.868200: step 18510, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:11m:03s remains)
INFO - root - 2017-12-11 06:12:00.205895: step 18520, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 46h:43m:50s remains)
INFO - root - 2017-12-11 06:12:05.704954: step 18530, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 46h:31m:36s remains)
INFO - root - 2017-12-11 06:12:10.982892: step 18540, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 44h:40m:53s remains)
INFO - root - 2017-12-11 06:12:16.247893: step 18550, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:07m:59s remains)
INFO - root - 2017-12-11 06:12:21.534855: step 18560, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:50m:36s remains)
INFO - root - 2017-12-11 06:12:26.659230: step 18570, loss = 0.68, batch loss = 0.63 (19.9 examples/sec; 0.402 sec/batch; 35h:01m:56s remains)
INFO - root - 2017-12-11 06:12:31.926093: step 18580, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:41m:24s remains)
INFO - root - 2017-12-11 06:12:37.345096: step 18590, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 49h:46m:14s remains)
INFO - root - 2017-12-11 06:12:42.559119: step 18600, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 45h:11m:41s remains)
2017-12-11 06:12:43.106089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21594302 0.20714308 0.20096388 0.21301448 0.24946016 0.30110249 0.34753573 0.37956831 0.39589936 0.39685 0.38423935 0.35339916 0.29831994 0.2194009 0.13106169][0.25333378 0.25243613 0.24869822 0.2604183 0.29343775 0.33838531 0.37552693 0.39857906 0.40899906 0.40910184 0.4013921 0.38060352 0.3367157 0.2636548 0.17473689][0.26691586 0.27014089 0.26998988 0.28570291 0.32114741 0.36443806 0.39556304 0.41008279 0.41152802 0.40468714 0.39552251 0.38191769 0.35126171 0.28931719 0.20543231][0.26126122 0.26300076 0.26500946 0.28964114 0.33816004 0.39375466 0.43339589 0.45116332 0.44818953 0.42958564 0.40742633 0.38730416 0.35939252 0.30322072 0.22186759][0.24503811 0.24069065 0.2411983 0.27171871 0.33249122 0.40333712 0.45810995 0.48774993 0.48710671 0.45799324 0.41830719 0.38285023 0.3485769 0.29296762 0.21326225][0.22489405 0.21480888 0.20972522 0.23878965 0.30444357 0.3869822 0.45867664 0.50613159 0.51514739 0.48076412 0.42376509 0.36777815 0.31930396 0.25873047 0.18043685][0.21049254 0.19190623 0.17922881 0.20155933 0.26531085 0.35509351 0.44300917 0.51102185 0.535255 0.50250751 0.43299079 0.3565895 0.29019588 0.22158168 0.14481464][0.21437025 0.18905661 0.16929993 0.18147816 0.23554814 0.32221821 0.41588452 0.49703145 0.53581876 0.51326245 0.44554994 0.3630372 0.28813404 0.21731645 0.14638275][0.24269693 0.21516505 0.19062153 0.19092731 0.22750044 0.29745796 0.38081986 0.461172 0.50840807 0.501057 0.44973874 0.37924755 0.31053492 0.2462972 0.18572982][0.26987949 0.24423628 0.21895188 0.21109264 0.23124684 0.2815851 0.34855571 0.41998491 0.4697344 0.47669 0.44645804 0.39531577 0.3402862 0.2881473 0.24111848][0.27199447 0.250303 0.22643337 0.21496716 0.22509745 0.26230344 0.31733033 0.37991548 0.42843837 0.44497859 0.43118751 0.39542469 0.35286233 0.31374919 0.28119224][0.23142739 0.2143416 0.19491234 0.18491895 0.19092001 0.22068666 0.26754865 0.32151356 0.36513379 0.3858155 0.3832604 0.35944051 0.32869473 0.30294213 0.28448078][0.1465908 0.13378885 0.1206377 0.11461401 0.11904099 0.14303604 0.18189381 0.22624108 0.26284027 0.28488606 0.29131734 0.27867305 0.25949547 0.24474385 0.23599008][0.016497262 0.0061528934 -0.00053380779 -0.0018751088 0.002781128 0.023487119 0.057514954 0.095599107 0.12717949 0.14934283 0.16103823 0.1556083 0.14313005 0.13244584 0.12594292][-0.1241157 -0.13425419 -0.13581046 -0.13285074 -0.12683004 -0.10753502 -0.075606138 -0.040042583 -0.011026196 0.0098895933 0.022359168 0.019578507 0.0097117219 -0.0004917889 -0.0073292581]]...]
INFO - root - 2017-12-11 06:12:48.445737: step 18610, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 45h:54m:05s remains)
INFO - root - 2017-12-11 06:12:53.739966: step 18620, loss = 0.66, batch loss = 0.60 (15.2 examples/sec; 0.526 sec/batch; 45h:52m:31s remains)
INFO - root - 2017-12-11 06:12:59.094115: step 18630, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:27m:18s remains)
INFO - root - 2017-12-11 06:13:04.436110: step 18640, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:49m:15s remains)
INFO - root - 2017-12-11 06:13:09.757079: step 18650, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:11m:44s remains)
INFO - root - 2017-12-11 06:13:15.083869: step 18660, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.525 sec/batch; 45h:43m:47s remains)
INFO - root - 2017-12-11 06:13:20.434735: step 18670, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 46h:49m:06s remains)
INFO - root - 2017-12-11 06:13:25.610228: step 18680, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:48m:14s remains)
INFO - root - 2017-12-11 06:13:31.021825: step 18690, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:30m:32s remains)
INFO - root - 2017-12-11 06:13:36.290182: step 18700, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:48m:37s remains)
2017-12-11 06:13:36.833552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31698817 0.36365083 0.37733471 0.37410879 0.35339382 0.30777606 0.24583837 0.20396622 0.20228036 0.22894312 0.26409537 0.30936548 0.35422629 0.37719312 0.38571307][0.43702626 0.48662138 0.48959765 0.4753879 0.44228637 0.3831127 0.30604085 0.24711463 0.22911549 0.24155037 0.26808557 0.31230611 0.3644056 0.39954591 0.42103702][0.53087151 0.57780653 0.5660553 0.53947884 0.49636123 0.43025577 0.34636611 0.27598688 0.24113797 0.23403957 0.24472106 0.28005293 0.33307007 0.3781257 0.41315651][0.59154981 0.63082373 0.60561419 0.56882632 0.51933455 0.4521023 0.36813298 0.29151556 0.24116981 0.21379046 0.20653431 0.23019096 0.28145614 0.33537862 0.38444093][0.60984814 0.64103061 0.611522 0.57267737 0.52378321 0.46155313 0.38419551 0.30827582 0.24832521 0.20426531 0.17921917 0.18838339 0.2322786 0.28828272 0.34573367][0.57285547 0.601195 0.57828516 0.546728 0.50659287 0.45698023 0.39562771 0.33273646 0.27720204 0.22829542 0.19129252 0.18456978 0.21218151 0.25665736 0.30928138][0.49070609 0.51660585 0.50508648 0.48730898 0.46389851 0.43639317 0.40310088 0.36866328 0.33420923 0.29441306 0.25311241 0.23003273 0.23236826 0.25070274 0.2834101][0.394414 0.41420034 0.41140583 0.40859416 0.40697908 0.40993106 0.41484669 0.41965044 0.41642451 0.39256218 0.34883547 0.305602 0.27550569 0.25965315 0.26447266][0.30498859 0.31508851 0.31560916 0.32356533 0.3423976 0.37722993 0.42434165 0.47305205 0.50470722 0.49704891 0.44839907 0.381051 0.31447679 0.26150846 0.23549263][0.22171599 0.22196083 0.22164018 0.23614515 0.2708492 0.33134168 0.41281208 0.49784657 0.55789214 0.56155056 0.50653243 0.41902274 0.32359436 0.24132924 0.189638][0.14886686 0.13577195 0.13003919 0.14689177 0.19101857 0.26635787 0.3655678 0.46752968 0.53880852 0.544839 0.48572758 0.39083982 0.28579763 0.19366479 0.13164878][0.089348815 0.062425982 0.04866356 0.0631876 0.10813359 0.18486288 0.28298688 0.38046154 0.4448342 0.44487166 0.3852565 0.29660687 0.20197953 0.12088162 0.066492438][0.031598072 -0.002005745 -0.020155909 -0.01031839 0.026695432 0.0899535 0.1682464 0.24300562 0.28816661 0.27997333 0.22643813 0.15587488 0.08603619 0.029780947 -0.0057622148][-0.026180511 -0.058554173 -0.075043276 -0.069261357 -0.045134913 -0.0053773932 0.0407696 0.082166813 0.10311454 0.089076161 0.048692483 0.0043769134 -0.0337743 -0.060995568 -0.075464621][-0.070048578 -0.09811502 -0.11003833 -0.10657383 -0.094487071 -0.077437125 -0.061765652 -0.051391345 -0.052149348 -0.069848768 -0.096393883 -0.11650871 -0.12707178 -0.13014391 -0.12734617]]...]
INFO - root - 2017-12-11 06:13:42.201357: step 18710, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:08m:37s remains)
INFO - root - 2017-12-11 06:13:47.725703: step 18720, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 45h:30m:02s remains)
INFO - root - 2017-12-11 06:13:53.040855: step 18730, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:59m:33s remains)
INFO - root - 2017-12-11 06:13:58.350116: step 18740, loss = 0.66, batch loss = 0.60 (14.6 examples/sec; 0.549 sec/batch; 47h:52m:02s remains)
INFO - root - 2017-12-11 06:14:03.713884: step 18750, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:39m:47s remains)
INFO - root - 2017-12-11 06:14:09.118492: step 18760, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:20m:32s remains)
INFO - root - 2017-12-11 06:14:14.480023: step 18770, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:48m:29s remains)
INFO - root - 2017-12-11 06:14:19.578421: step 18780, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 45h:57m:02s remains)
INFO - root - 2017-12-11 06:14:24.905947: step 18790, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 45h:24m:52s remains)
INFO - root - 2017-12-11 06:14:30.170838: step 18800, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 45h:50m:35s remains)
2017-12-11 06:14:30.718136: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14327835 0.15558161 0.19237316 0.24301648 0.28848821 0.31386849 0.32609531 0.32300422 0.30479819 0.29249752 0.30802897 0.35177717 0.40550062 0.45040444 0.47068551][0.14389223 0.14863653 0.17904206 0.22818935 0.27824569 0.31024861 0.32917917 0.33168852 0.31945583 0.31356752 0.33582172 0.38868409 0.45118704 0.5023393 0.52112782][0.13550365 0.13052936 0.1486637 0.189653 0.24054301 0.2796714 0.30773342 0.31987184 0.31831825 0.32063106 0.34595931 0.3994059 0.46130559 0.51070219 0.52483022][0.13265556 0.12145101 0.12780708 0.15855582 0.20744246 0.25181252 0.28697133 0.307317 0.31495649 0.32302052 0.34639174 0.39242575 0.44518942 0.48583719 0.49188519][0.14684986 0.13882598 0.1414448 0.16615807 0.21140461 0.255435 0.2894184 0.30845386 0.3153213 0.32266939 0.34256771 0.38183653 0.42721424 0.46185702 0.46436837][0.17702684 0.18506356 0.19773243 0.22621249 0.26932693 0.30874515 0.33368203 0.34150583 0.33745059 0.33735731 0.3524105 0.38591149 0.42743868 0.46297646 0.47079548][0.21853973 0.25483418 0.28944087 0.32962579 0.37262502 0.40512526 0.41608608 0.40676162 0.38693941 0.37564027 0.38236934 0.40634811 0.44144863 0.47694203 0.48979646][0.25951436 0.32662413 0.38699406 0.44117746 0.48483729 0.5096606 0.50550133 0.47682273 0.43783298 0.40984702 0.40199858 0.41217539 0.4380815 0.46971065 0.4828003][0.28678507 0.37487262 0.45091456 0.50898826 0.54499394 0.55745631 0.5393222 0.49576911 0.44266671 0.40096688 0.38064933 0.38105834 0.40236947 0.43325913 0.44828334][0.29256108 0.38581896 0.46292508 0.51338059 0.53312159 0.52712989 0.49486026 0.44155183 0.38207752 0.33353457 0.30700639 0.30460936 0.32716084 0.36327687 0.3876065][0.27584103 0.35472718 0.4183431 0.45480561 0.45941153 0.43833354 0.39610809 0.33855349 0.27899206 0.22934359 0.19981062 0.19534028 0.21839516 0.26022384 0.29635438][0.22515073 0.27732915 0.31772876 0.33838078 0.33477232 0.3093603 0.26705611 0.21450959 0.16274431 0.11837939 0.088390425 0.080045432 0.099179931 0.14088854 0.18259215][0.16003361 0.18370695 0.20006219 0.20557778 0.19738366 0.17427418 0.13902675 0.097697936 0.058334462 0.023838721 -0.002275961 -0.011967027 0.0028220597 0.040614866 0.082021266][0.10255519 0.10725135 0.10712806 0.1015902 0.090051323 0.070349447 0.04394199 0.015164466 -0.010989198 -0.033721156 -0.052076332 -0.058848754 -0.046218488 -0.013682283 0.023412146][0.046115015 0.043004822 0.037683979 0.029164441 0.017733229 0.0015308908 -0.017944157 -0.037560329 -0.054548964 -0.068724751 -0.080142058 -0.083409838 -0.073012345 -0.048263345 -0.02003287]]...]
INFO - root - 2017-12-11 06:14:36.055184: step 18810, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:28m:49s remains)
INFO - root - 2017-12-11 06:14:41.386841: step 18820, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 45h:59m:12s remains)
INFO - root - 2017-12-11 06:14:46.720355: step 18830, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 47h:17m:43s remains)
INFO - root - 2017-12-11 06:14:52.055221: step 18840, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 46h:07m:59s remains)
INFO - root - 2017-12-11 06:14:57.416380: step 18850, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 45h:56m:41s remains)
INFO - root - 2017-12-11 06:15:02.806207: step 18860, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:03m:16s remains)
INFO - root - 2017-12-11 06:15:08.098762: step 18870, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:13m:49s remains)
INFO - root - 2017-12-11 06:15:13.245248: step 18880, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 46h:12m:00s remains)
INFO - root - 2017-12-11 06:15:18.628747: step 18890, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 47h:17m:03s remains)
INFO - root - 2017-12-11 06:15:24.039158: step 18900, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:19m:22s remains)
2017-12-11 06:15:24.581911: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19973417 0.19119877 0.17405485 0.16167159 0.1543192 0.1520059 0.1630366 0.18198319 0.19961873 0.20857085 0.20394138 0.18805939 0.15942249 0.12060142 0.077345967][0.30277878 0.29454628 0.27382442 0.25988734 0.25346187 0.25364518 0.271233 0.29893136 0.32336068 0.33144072 0.31784981 0.2875931 0.24140917 0.18395825 0.12366533][0.38973081 0.38442433 0.36187941 0.34772891 0.34340224 0.34693012 0.37079671 0.40625045 0.43638662 0.44295669 0.42147025 0.37840095 0.316751 0.24329937 0.16855113][0.43887115 0.43806216 0.41654614 0.40520185 0.40496403 0.41258195 0.44155484 0.48320851 0.518053 0.52321291 0.49617836 0.4441404 0.37039253 0.28392145 0.19748075][0.44449985 0.44814083 0.43042997 0.42502719 0.43133798 0.44587013 0.48107931 0.52851611 0.56710571 0.57004327 0.53773868 0.47763124 0.39286762 0.29580146 0.20176111][0.41164124 0.41884515 0.40746298 0.41002032 0.42314029 0.44567341 0.48786488 0.54196328 0.58381528 0.5839951 0.54623169 0.47868583 0.38489419 0.28159663 0.18571003][0.371978 0.37854537 0.37310347 0.38019967 0.3927674 0.41551027 0.45969114 0.51860034 0.56231266 0.56165832 0.5222317 0.45287916 0.35643655 0.25375351 0.16186246][0.35051209 0.36051857 0.36178577 0.37084925 0.37513971 0.38601598 0.42100734 0.47708338 0.51949054 0.5204646 0.48415861 0.4202213 0.32737365 0.2284355 0.14145468][0.34513545 0.3615773 0.37322 0.38639551 0.38225222 0.37607205 0.39300078 0.43477088 0.46661124 0.46552047 0.43301186 0.37803242 0.292034 0.19775124 0.1160401][0.32458735 0.35310048 0.37620491 0.39390522 0.38348538 0.36213461 0.36009404 0.38333252 0.40061134 0.39591128 0.36732811 0.32230812 0.24529554 0.15727204 0.082389712][0.28352326 0.31773132 0.34760943 0.36668324 0.35067287 0.31685939 0.298716 0.30688703 0.31561282 0.31574103 0.29950118 0.27003604 0.20653032 0.12719604 0.058842633][0.21252833 0.24891478 0.28263032 0.30228058 0.28334945 0.24090944 0.2089138 0.20351033 0.20835263 0.22098216 0.22660908 0.21988316 0.17528935 0.10805584 0.046017505][0.099479407 0.1357021 0.17356424 0.19783671 0.18466544 0.14353558 0.10355017 0.085240245 0.084993437 0.10732212 0.13354394 0.15027869 0.12955841 0.081491277 0.030675042][-0.029107207 0.00073680596 0.036846712 0.063613169 0.060183842 0.030685617 -0.005027384 -0.027417516 -0.031321697 -0.0064193653 0.029070178 0.058913052 0.058211155 0.032581847 -0.00036314584][-0.11466484 -0.099905632 -0.075526476 -0.054242473 -0.051941406 -0.066889964 -0.08835651 -0.10279979 -0.10371713 -0.079556242 -0.045145966 -0.01457525 -0.005353516 -0.014610577 -0.031837575]]...]
INFO - root - 2017-12-11 06:15:29.895573: step 18910, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 47h:34m:01s remains)
INFO - root - 2017-12-11 06:15:35.201746: step 18920, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 46h:36m:20s remains)
INFO - root - 2017-12-11 06:15:40.475726: step 18930, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 44h:55m:57s remains)
INFO - root - 2017-12-11 06:15:45.882618: step 18940, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 46h:26m:54s remains)
INFO - root - 2017-12-11 06:15:51.244584: step 18950, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.532 sec/batch; 46h:17m:37s remains)
INFO - root - 2017-12-11 06:15:56.584487: step 18960, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 44h:54m:29s remains)
INFO - root - 2017-12-11 06:16:01.902385: step 18970, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 45h:42m:33s remains)
INFO - root - 2017-12-11 06:16:07.240836: step 18980, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:13m:12s remains)
INFO - root - 2017-12-11 06:16:12.280495: step 18990, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:43m:39s remains)
INFO - root - 2017-12-11 06:16:17.659178: step 19000, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 46h:00m:43s remains)
2017-12-11 06:16:18.246012: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0858309 0.16314149 0.25458798 0.33082613 0.37728849 0.39252958 0.38679585 0.36693767 0.33989516 0.31526244 0.29340041 0.2611441 0.20640485 0.13220212 0.056179706][0.059848405 0.13663 0.23252335 0.32017678 0.38115305 0.41128138 0.4183912 0.41020074 0.39272779 0.37085766 0.34485769 0.30396119 0.23872238 0.1523968 0.064487889][0.027732896 0.097291373 0.19060604 0.28500673 0.36086226 0.41111097 0.43830135 0.44971278 0.4462322 0.42642021 0.39125735 0.33537018 0.25570181 0.15745154 0.06130166][0.013794312 0.083717011 0.1796824 0.28411525 0.37672085 0.45002973 0.49935091 0.52851683 0.53339785 0.50847715 0.45732406 0.37853652 0.27661964 0.16127016 0.055152081][0.025495514 0.10618294 0.21419261 0.33383736 0.44377413 0.53780729 0.60419047 0.64181286 0.64470631 0.60720795 0.53620154 0.43144453 0.30392122 0.16912699 0.051705744][0.055917665 0.15293808 0.27830344 0.4128564 0.53397715 0.6395331 0.71378988 0.75014055 0.74328727 0.69107193 0.60307425 0.47734123 0.32820174 0.17675141 0.049542483][0.086855248 0.20004062 0.34152791 0.48526213 0.60834515 0.71439719 0.78709573 0.81602228 0.79782081 0.73558986 0.63942808 0.50292283 0.34043074 0.17800114 0.044767976][0.10144341 0.22546962 0.37662685 0.52145934 0.637404 0.73383683 0.79708266 0.81603974 0.79080755 0.72873873 0.6371507 0.5029639 0.33770657 0.1715368 0.03682103][0.0950466 0.22111136 0.37295479 0.51184916 0.61529571 0.69664657 0.74667811 0.75703603 0.73095649 0.67724431 0.59851134 0.47610152 0.3177765 0.1555351 0.024495455][0.071028493 0.1902138 0.33594796 0.4666132 0.55899024 0.62746656 0.66811574 0.67541069 0.65220326 0.60629249 0.53797734 0.42773408 0.28071979 0.12820606 0.0058530732][0.037450425 0.14395462 0.27991083 0.40356719 0.49027678 0.55216658 0.58939 0.59718567 0.57430905 0.52785194 0.46108598 0.35844916 0.22402591 0.086173236 -0.021453248][0.0043699574 0.097328432 0.22349097 0.34308419 0.42942122 0.48878297 0.52349532 0.52785265 0.49686784 0.43954185 0.36605543 0.26767188 0.14813192 0.031109987 -0.05497767][-0.024068613 0.056528762 0.17318483 0.28898102 0.375352 0.43107557 0.45882469 0.45176151 0.40508515 0.33226159 0.25060758 0.15814683 0.058081962 -0.031794321 -0.09109699][-0.052487202 0.014322515 0.11699852 0.22247249 0.302276 0.34884706 0.36449888 0.34319252 0.28318378 0.20177794 0.12036684 0.041639835 -0.032922741 -0.092151135 -0.12388283][-0.086902656 -0.040941484 0.036852472 0.11975411 0.18284349 0.21538462 0.21937621 0.19077684 0.1304331 0.055967454 -0.01150754 -0.067300357 -0.11218809 -0.1411913 -0.14856131]]...]
INFO - root - 2017-12-11 06:16:23.628994: step 19010, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:46m:19s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:16:29.007083: step 19020, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 48h:01m:32s remains)
INFO - root - 2017-12-11 06:16:34.404231: step 19030, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:11m:35s remains)
INFO - root - 2017-12-11 06:16:39.774810: step 19040, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:42m:43s remains)
INFO - root - 2017-12-11 06:16:45.032638: step 19050, loss = 0.68, batch loss = 0.62 (15.9 examples/sec; 0.504 sec/batch; 43h:51m:25s remains)
INFO - root - 2017-12-11 06:16:50.456041: step 19060, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.531 sec/batch; 46h:13m:25s remains)
INFO - root - 2017-12-11 06:16:55.794001: step 19070, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 46h:14m:47s remains)
INFO - root - 2017-12-11 06:17:01.081544: step 19080, loss = 0.71, batch loss = 0.65 (15.8 examples/sec; 0.508 sec/batch; 44h:12m:41s remains)
INFO - root - 2017-12-11 06:17:06.145827: step 19090, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:33m:53s remains)
INFO - root - 2017-12-11 06:17:11.522881: step 19100, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:16m:15s remains)
2017-12-11 06:17:12.069987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041403033 -0.035976581 -0.029003583 -0.024042016 -0.020508116 -0.016693136 -0.012039932 -0.0088999551 -0.0076234154 -0.0095623117 -0.015528421 -0.027303357 -0.043474589 -0.059441131 -0.072339579][0.00280554 0.021308439 0.037879396 0.04960303 0.058203615 0.066811629 0.076932058 0.08457873 0.088834368 0.086203665 0.074227504 0.048793897 0.013105304 -0.02307957 -0.053235997][0.077287205 0.11828026 0.1516218 0.17541075 0.19232793 0.20801292 0.22685307 0.24150833 0.250424 0.24639729 0.22474822 0.17850378 0.11355379 0.046777923 -0.010040944][0.16837671 0.2404293 0.29821646 0.34030864 0.36914551 0.39269859 0.42004976 0.44050381 0.45189852 0.44239515 0.40546834 0.33264837 0.23287062 0.13028976 0.042010546][0.25691661 0.36467409 0.45157492 0.51645607 0.56038433 0.59095752 0.62222028 0.641903 0.64871264 0.62682652 0.56900126 0.468314 0.33623195 0.20212986 0.086592853][0.32298556 0.46825925 0.58896005 0.68300039 0.74775553 0.78504413 0.8126182 0.820236 0.81039965 0.76606739 0.68138283 0.55469459 0.39852819 0.2435779 0.11091273][0.36138391 0.54059184 0.69549316 0.8213703 0.9091683 0.95030695 0.965448 0.95011312 0.91284078 0.83873194 0.72555387 0.57937759 0.41084045 0.24770461 0.10956554][0.36641645 0.56649137 0.74502933 0.89382291 0.99699175 1.0341204 1.0286865 0.98561114 0.92010683 0.82043791 0.68757021 0.53546584 0.37002796 0.21267635 0.0808465][0.33265328 0.53300709 0.71449679 0.86598122 0.96852261 0.99295163 0.96567833 0.90225357 0.82212532 0.7153666 0.58234096 0.44143867 0.29323378 0.15237853 0.035083208][0.26613477 0.44805372 0.61274636 0.7464788 0.8319996 0.83842105 0.79500496 0.72564518 0.650012 0.55789822 0.44527951 0.32956734 0.20668635 0.08695548 -0.01280388][0.17313527 0.31761789 0.44534358 0.5414173 0.59423357 0.57970488 0.53060234 0.473501 0.42272341 0.36492172 0.28997922 0.21043077 0.11910017 0.024394205 -0.0554754][0.059710458 0.15063339 0.22419278 0.26809686 0.27853885 0.24439858 0.20031267 0.1694556 0.15715659 0.14526679 0.11845329 0.082340762 0.028387764 -0.035650961 -0.091568][-0.05587649 -0.018554773 0.0034805604 0.0020234957 -0.020967297 -0.064693138 -0.098030441 -0.10410535 -0.0846223 -0.059802365 -0.046369646 -0.044668384 -0.06239735 -0.093046121 -0.12189536][-0.14394341 -0.14389051 -0.15276827 -0.17662004 -0.21226023 -0.25290215 -0.27582166 -0.27026638 -0.24048287 -0.20284198 -0.17074451 -0.14739327 -0.13845097 -0.14001961 -0.14346048][-0.18415289 -0.20101096 -0.21925583 -0.24436457 -0.27497023 -0.30443656 -0.31946722 -0.31392661 -0.29106119 -0.26009735 -0.2287302 -0.20009091 -0.17803496 -0.16198242 -0.14894779]]...]
INFO - root - 2017-12-11 06:17:17.324693: step 19110, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:19m:30s remains)
INFO - root - 2017-12-11 06:17:22.672142: step 19120, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:04m:28s remains)
INFO - root - 2017-12-11 06:17:28.102958: step 19130, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 46h:00m:01s remains)
INFO - root - 2017-12-11 06:17:33.411506: step 19140, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 45h:35m:02s remains)
INFO - root - 2017-12-11 06:17:38.786183: step 19150, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:02m:33s remains)
INFO - root - 2017-12-11 06:17:44.171532: step 19160, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:13m:53s remains)
INFO - root - 2017-12-11 06:17:49.624599: step 19170, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:03m:11s remains)
INFO - root - 2017-12-11 06:17:55.038952: step 19180, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 47h:32m:53s remains)
INFO - root - 2017-12-11 06:18:00.168714: step 19190, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 48h:11m:00s remains)
INFO - root - 2017-12-11 06:18:05.607864: step 19200, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:44m:21s remains)
2017-12-11 06:18:06.148025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0236855 -0.014140072 -0.0086660944 -0.0088477563 -0.010194225 -0.012717461 -0.016185869 -0.016428357 -0.010085898 -0.0028139907 -0.0029474373 -0.013125597 -0.028863851 -0.045073573 -0.0601421][0.020281911 0.052478198 0.075775467 0.084223241 0.087762825 0.087513842 0.081603728 0.078105353 0.083671235 0.090375885 0.083420947 0.058539089 0.023700368 -0.012108307 -0.044401959][0.077240452 0.14008985 0.18883418 0.21039382 0.22045219 0.22453992 0.219922 0.21506779 0.21792352 0.21957807 0.20066893 0.15577728 0.095620252 0.033708934 -0.020352997][0.12877421 0.22347264 0.29989606 0.33626002 0.35208026 0.36020672 0.35978943 0.35804808 0.36033893 0.35618898 0.32401383 0.25928789 0.17354171 0.083754145 0.0058722156][0.16216958 0.286587 0.39178821 0.44626027 0.46880704 0.48116043 0.48787233 0.49561277 0.50391513 0.49761385 0.45334524 0.37076607 0.26121765 0.1426207 0.037417453][0.16784184 0.31448922 0.44580337 0.5208388 0.55247551 0.57004768 0.58620608 0.60904944 0.62946981 0.62469935 0.57068449 0.47431204 0.3457045 0.20062086 0.0677385][0.14672099 0.30164564 0.44869038 0.54009706 0.57983696 0.60203362 0.62749642 0.66581243 0.69822633 0.69506496 0.6339407 0.5294463 0.38982472 0.22760984 0.076543458][0.10220195 0.24842648 0.3947728 0.49027944 0.53074676 0.55173135 0.5784359 0.621578 0.65708631 0.65285677 0.59038454 0.48862508 0.353243 0.19359145 0.045249004][0.036774255 0.15957624 0.28821382 0.37336481 0.40640613 0.41999391 0.43898034 0.47555676 0.50676864 0.50181144 0.4458583 0.35821366 0.24271834 0.10579547 -0.01936979][-0.041578304 0.049707972 0.15035056 0.21629444 0.23756199 0.24038035 0.2456059 0.26702389 0.28846639 0.28470266 0.24358061 0.18015537 0.096136212 -0.0043117944 -0.093725555][-0.12301677 -0.066922463 0.0012706834 0.045595888 0.056905359 0.052462589 0.045992907 0.05081179 0.059046008 0.054626726 0.02918577 -0.007953505 -0.0569526 -0.11563212 -0.16392297][-0.18061553 -0.15586552 -0.11852234 -0.094588831 -0.090837792 -0.098562472 -0.11183269 -0.12044077 -0.12514488 -0.13213082 -0.14575401 -0.16189164 -0.18123807 -0.20260321 -0.21375215][-0.20321535 -0.20063803 -0.18748127 -0.18031138 -0.18285386 -0.19212455 -0.20731665 -0.22174872 -0.23230055 -0.23893574 -0.24367827 -0.24562608 -0.24489813 -0.24055678 -0.22712825][-0.19800802 -0.20684507 -0.20741692 -0.20940372 -0.21482237 -0.22368187 -0.23670748 -0.24943739 -0.25778884 -0.26046318 -0.2589862 -0.25318837 -0.24269323 -0.22695076 -0.20498973][-0.17407757 -0.18589267 -0.19044584 -0.19351032 -0.19683525 -0.20189317 -0.20949556 -0.216905 -0.22104228 -0.22109634 -0.21803769 -0.21137722 -0.20029044 -0.18462653 -0.16545673]]...]
INFO - root - 2017-12-11 06:18:11.533469: step 19210, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:55m:27s remains)
INFO - root - 2017-12-11 06:18:16.886770: step 19220, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:57m:24s remains)
INFO - root - 2017-12-11 06:18:22.316817: step 19230, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 46h:33m:39s remains)
INFO - root - 2017-12-11 06:18:27.811699: step 19240, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 49h:28m:33s remains)
INFO - root - 2017-12-11 06:18:33.167687: step 19250, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 44h:40m:51s remains)
INFO - root - 2017-12-11 06:18:38.469240: step 19260, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:12m:20s remains)
INFO - root - 2017-12-11 06:18:43.840716: step 19270, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:07m:53s remains)
INFO - root - 2017-12-11 06:18:49.224158: step 19280, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:09m:24s remains)
INFO - root - 2017-12-11 06:18:54.311131: step 19290, loss = 0.68, batch loss = 0.63 (16.7 examples/sec; 0.480 sec/batch; 41h:47m:24s remains)
INFO - root - 2017-12-11 06:18:59.630795: step 19300, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:38m:14s remains)
2017-12-11 06:19:00.192380: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14785008 0.13264163 0.12947936 0.13447154 0.13994169 0.14016892 0.13380592 0.12348495 0.11263685 0.10416703 0.10033038 0.10127803 0.10523827 0.1099763 0.11435818][0.22843398 0.19739595 0.18265989 0.18410382 0.19027936 0.1921681 0.18642958 0.17505778 0.16143093 0.14924391 0.14248683 0.14247181 0.14784771 0.15643063 0.16656896][0.30361477 0.25274777 0.2220315 0.2187272 0.22813648 0.23689692 0.23787431 0.23030145 0.21574531 0.19844176 0.18494712 0.17937204 0.18265308 0.19404897 0.21129921][0.3585898 0.29116514 0.24746089 0.24232194 0.26005536 0.28317341 0.29915541 0.30163532 0.28814417 0.26331481 0.23720983 0.21872704 0.21305141 0.22194374 0.24231102][0.38707447 0.311993 0.26253885 0.261855 0.29517126 0.34166881 0.38209596 0.4017683 0.39201364 0.35768172 0.31315467 0.27373755 0.25031433 0.24793711 0.26269788][0.39673889 0.31872979 0.26638258 0.27354407 0.3271026 0.40258524 0.47238451 0.51193333 0.50642 0.46169943 0.39698789 0.33443028 0.29022905 0.27203146 0.27485725][0.4037092 0.32136562 0.26593748 0.28005362 0.35276648 0.45519349 0.55018181 0.60427541 0.6000337 0.54700989 0.46910349 0.39278579 0.33581907 0.3053827 0.29545516][0.41422087 0.32496434 0.26684621 0.2843042 0.36975637 0.49116516 0.603026 0.66586822 0.66335744 0.61047035 0.53375614 0.45924252 0.4017854 0.36530021 0.34335044][0.41804782 0.32401359 0.26238945 0.27682197 0.36379522 0.49126944 0.6099301 0.6786 0.68421364 0.64503092 0.58559144 0.52625942 0.47668782 0.43809122 0.40571544][0.41107592 0.3144882 0.24451292 0.24613701 0.32065421 0.43950838 0.55464357 0.62682664 0.64603353 0.63044351 0.59844166 0.56213444 0.5265466 0.49193585 0.45711002][0.37921929 0.28351614 0.20531003 0.18863884 0.24038334 0.3387776 0.44130808 0.51290393 0.54544365 0.55500782 0.55180675 0.53990859 0.52167708 0.49759972 0.47057116][0.3132771 0.22465935 0.1464456 0.11585736 0.14491566 0.22016233 0.30724457 0.376138 0.41866994 0.44719896 0.4643304 0.46849227 0.46159875 0.44561264 0.42778614][0.22133091 0.14442386 0.073483378 0.037331834 0.051666789 0.10940182 0.18387346 0.24964894 0.29666421 0.33280921 0.3552976 0.36071557 0.35270849 0.3360337 0.32125598][0.130287 0.067715839 0.0069178813 -0.028914547 -0.022126881 0.023078274 0.085818276 0.14499719 0.18857202 0.22038279 0.23495823 0.23015402 0.21235979 0.18952931 0.1732607][0.053887531 0.0040222225 -0.045810241 -0.077139676 -0.072832361 -0.036968052 0.013690812 0.062272295 0.096262813 0.11703151 0.11953174 0.1040897 0.078596406 0.052103113 0.034782615]]...]
INFO - root - 2017-12-11 06:19:05.584091: step 19310, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:38m:18s remains)
INFO - root - 2017-12-11 06:19:11.021398: step 19320, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 46h:04m:33s remains)
INFO - root - 2017-12-11 06:19:16.417205: step 19330, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 46h:25m:54s remains)
INFO - root - 2017-12-11 06:19:21.700985: step 19340, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:18m:16s remains)
INFO - root - 2017-12-11 06:19:27.161023: step 19350, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:04m:22s remains)
INFO - root - 2017-12-11 06:19:32.501158: step 19360, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:01m:50s remains)
INFO - root - 2017-12-11 06:19:37.829605: step 19370, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 45h:41m:02s remains)
INFO - root - 2017-12-11 06:19:43.178563: step 19380, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:00m:51s remains)
INFO - root - 2017-12-11 06:19:48.546808: step 19390, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:49m:56s remains)
INFO - root - 2017-12-11 06:19:53.667333: step 19400, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 47h:28m:32s remains)
2017-12-11 06:19:54.201373: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034275711 0.042375788 0.0556562 0.069259264 0.076350316 0.072978653 0.067893773 0.0725679 0.086170681 0.099829242 0.10419104 0.097273111 0.076990604 0.042982273 0.0046783676][0.010355087 0.025358154 0.046214782 0.06628 0.077069342 0.075426392 0.070686333 0.075585164 0.090036795 0.10727362 0.11762833 0.11675765 0.099921271 0.065505154 0.024150982][0.015279114 0.034471631 0.061787039 0.089152977 0.10509425 0.10619175 0.1032939 0.11109205 0.13029554 0.15292288 0.16793697 0.16960305 0.1512147 0.10976692 0.057920385][0.064160496 0.084970854 0.11918143 0.15736185 0.18357129 0.19150066 0.19367109 0.2080258 0.23586619 0.26548246 0.28239864 0.27960232 0.24874854 0.18640104 0.11038752][0.14004883 0.16706455 0.21160577 0.26490363 0.30634743 0.32462567 0.33277681 0.3520447 0.38443917 0.41352412 0.42178506 0.4041433 0.35212824 0.26205835 0.1573638][0.21125661 0.25279543 0.31333891 0.38568431 0.44538844 0.47517744 0.487218 0.50530916 0.53205377 0.54703438 0.53308046 0.49139231 0.41537184 0.30089334 0.1744028][0.24968995 0.30995405 0.38969427 0.4824256 0.55996037 0.59963173 0.61176646 0.62160343 0.63218856 0.62289739 0.58113092 0.515574 0.42128718 0.29312015 0.15753272][0.25221232 0.33016622 0.42723653 0.53594214 0.62590617 0.67177111 0.68105608 0.6789155 0.66890275 0.63346905 0.56641775 0.48234737 0.37760049 0.24644087 0.11341023][0.22840431 0.31676862 0.42292354 0.53711021 0.62990177 0.67661655 0.68200457 0.66985095 0.64305997 0.58742034 0.50154251 0.40416071 0.29437417 0.16815357 0.047618989][0.19134074 0.27597156 0.37623826 0.4806056 0.5636946 0.60460538 0.60671413 0.59125787 0.55841452 0.49534845 0.40211049 0.29935157 0.19025022 0.075084083 -0.02566353][0.15682395 0.22633964 0.30583084 0.38433042 0.44235158 0.46579722 0.46093813 0.44777852 0.42366379 0.37433505 0.29626873 0.2063361 0.10922766 0.0094073266 -0.072413884][0.13868232 0.18746182 0.23947021 0.28552175 0.31142697 0.31026202 0.2947242 0.28549892 0.279079 0.25903744 0.21516114 0.15490474 0.079282865 -0.0052948804 -0.076111525][0.13506292 0.16456752 0.19181137 0.20979546 0.20833275 0.18589716 0.16168751 0.15734264 0.17006989 0.18125397 0.17414793 0.14616251 0.092184953 0.018207291 -0.05052356][0.12635413 0.14158833 0.15180926 0.15208216 0.1341825 0.099550158 0.072313905 0.074375205 0.10279996 0.13708416 0.15653493 0.15229724 0.11473574 0.048614338 -0.020061677][0.10514344 0.11390336 0.11710931 0.10948826 0.083489858 0.042810023 0.015008198 0.021402456 0.057807516 0.10357273 0.13729654 0.14700395 0.12095154 0.063396126 -0.001140543]]...]
INFO - root - 2017-12-11 06:19:59.516260: step 19410, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 47h:21m:47s remains)
INFO - root - 2017-12-11 06:20:04.882404: step 19420, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:00m:01s remains)
INFO - root - 2017-12-11 06:20:10.232612: step 19430, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:16m:18s remains)
INFO - root - 2017-12-11 06:20:15.651745: step 19440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:44m:20s remains)
INFO - root - 2017-12-11 06:20:20.947852: step 19450, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:52m:00s remains)
INFO - root - 2017-12-11 06:20:26.333847: step 19460, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:53m:07s remains)
INFO - root - 2017-12-11 06:20:31.683201: step 19470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:54m:59s remains)
INFO - root - 2017-12-11 06:20:37.047789: step 19480, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 47h:09m:05s remains)
INFO - root - 2017-12-11 06:20:42.473180: step 19490, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 47h:36m:01s remains)
INFO - root - 2017-12-11 06:20:47.525702: step 19500, loss = 0.67, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 46h:29m:01s remains)
2017-12-11 06:20:48.096023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035354391 -0.03422989 -0.028487511 -0.021086434 -0.014520129 -0.010739748 -0.010418053 -0.012807383 -0.01703418 -0.022118151 -0.026942544 -0.03059216 -0.032540385 -0.032240711 -0.02971793][-0.034368776 -0.030767264 -0.022652108 -0.014193122 -0.00834342 -0.0067327949 -0.0090746237 -0.013557297 -0.018607212 -0.023219723 -0.026890302 -0.029654449 -0.031679142 -0.032539651 -0.031906549][-0.0233859 -0.014369125 -0.0019968357 0.0078195026 0.011598083 0.0085052243 0.00048571493 -0.00900506 -0.017270483 -0.022997558 -0.026282538 -0.028356014 -0.030403949 -0.032406759 -0.03386113][0.0041081947 0.021583084 0.040538345 0.053409938 0.056049973 0.047951069 0.032373153 0.014324292 -0.0019425713 -0.013763656 -0.020886114 -0.025149869 -0.028753435 -0.032290261 -0.035473734][0.051902216 0.078957357 0.1066317 0.12691928 0.13422617 0.12604782 0.10465108 0.075671636 0.045302752 0.019520702 0.0010997105 -0.011020152 -0.019988049 -0.027459236 -0.033541277][0.11884314 0.15428382 0.19211242 0.22544627 0.24584709 0.2455624 0.22294253 0.18315645 0.13439728 0.087642111 0.05039452 0.023880815 0.0043357052 -0.011292459 -0.023472818][0.19805607 0.2391548 0.28665406 0.33583033 0.37447071 0.38787031 0.36891353 0.32123554 0.25435668 0.18449694 0.12483924 0.079723433 0.045462862 0.017948784 -0.0034768144][0.27757233 0.31895694 0.37054896 0.43040121 0.48368087 0.51041311 0.49900252 0.45031902 0.37290853 0.28608823 0.20765117 0.14479625 0.094765596 0.053651705 0.021309862][0.33928922 0.37446102 0.41923207 0.47521079 0.52855641 0.55916494 0.5546183 0.51248491 0.43749702 0.34807643 0.26333058 0.19171663 0.13181891 0.081118695 0.040456276][0.35465384 0.37791097 0.40468228 0.43998584 0.4750109 0.4952834 0.49192142 0.46023947 0.39997044 0.32498839 0.2515927 0.18694669 0.13045649 0.081366636 0.041074634][0.29583877 0.30503997 0.31013381 0.31807554 0.32601416 0.32754555 0.31993288 0.29851833 0.25918055 0.20976648 0.16120698 0.11770222 0.0785726 0.044145469 0.01547783][0.16745414 0.16419064 0.15289982 0.13947803 0.12531254 0.11072181 0.098510973 0.086080141 0.068036444 0.046814825 0.02739995 0.010845696 -0.0040073856 -0.016725952 -0.027236776][0.013371951 0.0025657597 -0.015604211 -0.037897013 -0.06140982 -0.08217562 -0.094677359 -0.0992024 -0.10024825 -0.098684572 -0.0940092 -0.087330133 -0.080171369 -0.072762929 -0.066167608][-0.10792343 -0.12133243 -0.13756269 -0.15612765 -0.17492755 -0.190014 -0.19691315 -0.19503359 -0.1872012 -0.17492643 -0.15912686 -0.1410954 -0.12208856 -0.10308971 -0.086117059][-0.15755482 -0.1706063 -0.1818281 -0.19176316 -0.19930083 -0.20263556 -0.20055012 -0.19336772 -0.18293372 -0.17026475 -0.15563543 -0.13898638 -0.12036553 -0.10075144 -0.082431875]]...]
INFO - root - 2017-12-11 06:20:53.514047: step 19510, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:47m:48s remains)
INFO - root - 2017-12-11 06:20:58.725258: step 19520, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:58m:59s remains)
INFO - root - 2017-12-11 06:21:04.047372: step 19530, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:28m:57s remains)
INFO - root - 2017-12-11 06:21:09.414040: step 19540, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 44h:40m:17s remains)
INFO - root - 2017-12-11 06:21:14.746936: step 19550, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:21m:35s remains)
INFO - root - 2017-12-11 06:21:20.102348: step 19560, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:43m:09s remains)
INFO - root - 2017-12-11 06:21:25.401001: step 19570, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:22m:48s remains)
INFO - root - 2017-12-11 06:21:30.616878: step 19580, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:42m:19s remains)
INFO - root - 2017-12-11 06:21:35.932767: step 19590, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:46m:16s remains)
INFO - root - 2017-12-11 06:21:40.960342: step 19600, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:58m:22s remains)
2017-12-11 06:21:41.557368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.083563432 -0.085816674 -0.083880842 -0.081303515 -0.0703187 -0.044767663 -0.0047802883 0.039809477 0.07414972 0.089149229 0.078565679 0.045029696 0.0024322271 -0.037742339 -0.069561116][-0.079071313 -0.07445015 -0.065385565 -0.055989079 -0.0372998 0.000147357 0.058061726 0.12178388 0.16946387 0.18954085 0.17497349 0.12717722 0.062121876 -0.0027107277 -0.05474703][-0.051782433 -0.035125114 -0.013024095 0.0070797657 0.034306526 0.080515072 0.15069315 0.22605446 0.28022063 0.3005335 0.27967882 0.2171313 0.12939493 0.040499415 -0.029804409][-0.00058849336 0.0333732 0.073632911 0.10815951 0.14377379 0.19328324 0.26633975 0.34070784 0.38788712 0.39652169 0.36253405 0.28519136 0.17962405 0.074505113 -0.0056529008][0.068482928 0.12288112 0.18418992 0.23521037 0.27818969 0.32557911 0.39264765 0.45444113 0.48205963 0.46736574 0.41363531 0.32270625 0.20630698 0.094860874 0.015020554][0.13270323 0.20466535 0.28553689 0.35362709 0.40423703 0.44762626 0.50429928 0.54741728 0.5492813 0.506409 0.43160135 0.33079523 0.2105917 0.10129771 0.029709177][0.16818218 0.2481153 0.342066 0.4252834 0.48500153 0.52681 0.57433295 0.60160571 0.58293986 0.51938283 0.4321757 0.33045158 0.21414088 0.11133108 0.048895761][0.16356711 0.23920204 0.33364972 0.42312324 0.48964176 0.53370339 0.57895792 0.60185546 0.57660526 0.50726706 0.42010298 0.32740679 0.22331779 0.13093024 0.076389283][0.11641788 0.17492504 0.25392178 0.33419997 0.39885709 0.44595543 0.49642971 0.52920192 0.51589334 0.45952472 0.38582614 0.30974928 0.22337669 0.14385477 0.095407642][0.036141541 0.06943614 0.12163263 0.1805433 0.23544149 0.28426534 0.34260684 0.39223921 0.40292034 0.37290761 0.3214103 0.26402256 0.195874 0.12967655 0.086443968][-0.049200661 -0.0431315 -0.020644922 0.012324529 0.052712664 0.098659657 0.15889503 0.21916431 0.25104514 0.24795501 0.21882734 0.17845191 0.12786797 0.077446662 0.043602385][-0.11157265 -0.12509055 -0.12716354 -0.11896295 -0.0973197 -0.063895829 -0.015665634 0.037887212 0.07447017 0.085936606 0.07286229 0.04851022 0.018658012 -0.0088551734 -0.024103908][-0.14118245 -0.16276681 -0.17874567 -0.18866903 -0.18596381 -0.1714929 -0.14541116 -0.11292902 -0.088078663 -0.076946542 -0.082021654 -0.092234351 -0.10076597 -0.10338321 -0.096886329][-0.14086041 -0.16132231 -0.18065611 -0.19938333 -0.21052845 -0.21433742 -0.21166724 -0.20391108 -0.19693306 -0.19313648 -0.19461329 -0.19334993 -0.18476759 -0.16873616 -0.14691126][-0.11916725 -0.13465655 -0.1510708 -0.17073303 -0.18851216 -0.20431061 -0.21809259 -0.22814023 -0.23450015 -0.23654744 -0.23513854 -0.22579862 -0.20749794 -0.183167 -0.15666956]]...]
INFO - root - 2017-12-11 06:21:46.844454: step 19610, loss = 0.67, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 48h:30m:25s remains)
INFO - root - 2017-12-11 06:21:52.156583: step 19620, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:26m:44s remains)
INFO - root - 2017-12-11 06:21:57.451704: step 19630, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:39m:32s remains)
INFO - root - 2017-12-11 06:22:02.825244: step 19640, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:33m:58s remains)
INFO - root - 2017-12-11 06:22:08.203542: step 19650, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 45h:50m:44s remains)
INFO - root - 2017-12-11 06:22:13.600764: step 19660, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.527 sec/batch; 45h:48m:56s remains)
INFO - root - 2017-12-11 06:22:18.940666: step 19670, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:41m:56s remains)
INFO - root - 2017-12-11 06:22:24.250347: step 19680, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:59m:26s remains)
INFO - root - 2017-12-11 06:22:29.648999: step 19690, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.520 sec/batch; 45h:09m:01s remains)
INFO - root - 2017-12-11 06:22:34.947078: step 19700, loss = 0.68, batch loss = 0.62 (17.9 examples/sec; 0.447 sec/batch; 38h:51m:28s remains)
2017-12-11 06:22:35.399048: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34425247 0.33478716 0.32173645 0.32786024 0.32854131 0.31179386 0.28568855 0.26880133 0.27399695 0.30181605 0.3391073 0.37013984 0.39206895 0.41141975 0.43778342][0.40958264 0.38822326 0.34837514 0.32875881 0.310803 0.28222743 0.2499769 0.22940724 0.23751661 0.27532002 0.33091322 0.38802126 0.43713126 0.47688261 0.51018924][0.51098365 0.48399752 0.41738552 0.36688381 0.32642654 0.28305244 0.24013565 0.20846367 0.20672308 0.23981126 0.2990647 0.3700524 0.43766135 0.49208766 0.53071475][0.63217735 0.61366969 0.53420877 0.46562088 0.41455135 0.36438131 0.30912268 0.25428244 0.22349726 0.22976267 0.26999944 0.33411121 0.40543064 0.46758345 0.5116623][0.7278223 0.73125404 0.65864569 0.5921281 0.54772735 0.50101238 0.43496546 0.35415474 0.28898269 0.2598466 0.26742676 0.30716488 0.36521509 0.4212805 0.46192178][0.76494145 0.80231386 0.76039636 0.72255868 0.70738667 0.67928249 0.61263967 0.51563358 0.42264882 0.35632291 0.31904203 0.3147223 0.3357012 0.36194026 0.38126221][0.71519691 0.7943154 0.80075032 0.81370139 0.84580743 0.85074884 0.79794347 0.69931328 0.58775103 0.48384815 0.39301282 0.33024526 0.29539755 0.27296317 0.25813767][0.57033265 0.68234342 0.74230474 0.81507158 0.89925003 0.94121641 0.90985966 0.81990886 0.69949549 0.56574428 0.42803529 0.31131393 0.22196177 0.1513463 0.10358909][0.34860212 0.46731693 0.56382716 0.67998761 0.79823184 0.86440849 0.85349452 0.78242606 0.67241669 0.53507292 0.38047 0.23742436 0.1174466 0.018567245 -0.048814792][0.11246778 0.20785035 0.31123939 0.43903664 0.56184447 0.63187051 0.63445252 0.58723211 0.50425726 0.39042738 0.25264305 0.11694866 -0.0022550202 -0.10253451 -0.17006232][-0.08260332 -0.027442865 0.056484211 0.16645585 0.26906008 0.32750672 0.33644137 0.31024247 0.25743631 0.17776628 0.0747163 -0.031260282 -0.12513849 -0.20309281 -0.25137964][-0.20080388 -0.18852173 -0.14193393 -0.071764566 -0.0066384585 0.029689278 0.037339829 0.024619479 -0.00316078 -0.048093013 -0.10880242 -0.17160757 -0.22448835 -0.26527151 -0.28359285][-0.22228639 -0.24578634 -0.23856771 -0.21213107 -0.18598102 -0.17338471 -0.17173749 -0.17787766 -0.18861222 -0.20549524 -0.22761916 -0.24741933 -0.25857058 -0.26207292 -0.25280294][-0.1662022 -0.20887028 -0.23019619 -0.23553509 -0.23660219 -0.24069652 -0.24454573 -0.24729408 -0.24804254 -0.24769551 -0.24483509 -0.2353266 -0.21809104 -0.19679765 -0.17028035][-0.094683789 -0.14177203 -0.17605564 -0.19558735 -0.20754014 -0.21658091 -0.22066136 -0.2199025 -0.21548869 -0.20834498 -0.19595753 -0.17473607 -0.14687476 -0.11688488 -0.084817834]]...]
INFO - root - 2017-12-11 06:22:40.708886: step 19710, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:34m:47s remains)
INFO - root - 2017-12-11 06:22:46.062353: step 19720, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:05m:07s remains)
INFO - root - 2017-12-11 06:22:51.367037: step 19730, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.508 sec/batch; 44h:05m:45s remains)
INFO - root - 2017-12-11 06:22:56.686726: step 19740, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:14m:21s remains)
INFO - root - 2017-12-11 06:23:02.193123: step 19750, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.540 sec/batch; 46h:55m:36s remains)
INFO - root - 2017-12-11 06:23:07.527183: step 19760, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 45h:51m:47s remains)
INFO - root - 2017-12-11 06:23:12.837268: step 19770, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:37m:01s remains)
INFO - root - 2017-12-11 06:23:18.178890: step 19780, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.524 sec/batch; 45h:29m:03s remains)
INFO - root - 2017-12-11 06:23:23.544806: step 19790, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.587 sec/batch; 51h:01m:18s remains)
INFO - root - 2017-12-11 06:23:28.894596: step 19800, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 46h:44m:57s remains)
2017-12-11 06:23:29.475157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019177858 0.027844807 0.10687407 0.22100978 0.36093193 0.48995554 0.5726198 0.58691996 0.53392673 0.42201692 0.28054005 0.15389942 0.076532595 0.054665111 0.069263011][-0.014714082 0.034465242 0.12621644 0.26476961 0.43966359 0.60532516 0.71432853 0.73694104 0.67617828 0.54560453 0.37906918 0.22366768 0.11823514 0.072917804 0.072863519][0.0036970216 0.046432354 0.1407278 0.2924321 0.489947 0.68283862 0.81837624 0.86126173 0.81175083 0.68356913 0.50840878 0.33287513 0.19806428 0.12188988 0.10004395][0.035758685 0.06675142 0.15431277 0.30422571 0.50410193 0.70685893 0.86159629 0.93030685 0.90663552 0.79883844 0.6306864 0.44481412 0.28448129 0.1796318 0.138726][0.07701499 0.093412645 0.16559973 0.29874754 0.48019782 0.67301321 0.83227241 0.92019057 0.92233437 0.839592 0.68852228 0.5059858 0.33490518 0.21582541 0.16772664][0.13018246 0.13122405 0.18521677 0.29921266 0.46025881 0.64016753 0.79501992 0.88555729 0.8911556 0.81225455 0.66815621 0.49339658 0.32740113 0.21192206 0.17086406][0.19523619 0.18682939 0.22695643 0.32909864 0.47920758 0.65083557 0.79434794 0.86716038 0.84631419 0.739607 0.58134866 0.4092046 0.25742233 0.15909494 0.13623638][0.25550321 0.2458889 0.27719149 0.36938241 0.50667936 0.66041857 0.77566415 0.81078672 0.74796921 0.60582584 0.43382153 0.26988235 0.1408727 0.06903524 0.070677206][0.30087823 0.28917882 0.30594063 0.37527907 0.48106569 0.59468961 0.66252422 0.65108168 0.55456084 0.398636 0.23525389 0.094677076 -0.0023525239 -0.03883278 -0.0047692112][0.32378274 0.30250093 0.29433697 0.32708615 0.38683406 0.44877404 0.46827462 0.42310482 0.31643197 0.17256089 0.035686258 -0.071728118 -0.13190474 -0.13009882 -0.062370371][0.31037983 0.27207303 0.23428099 0.22772419 0.24366806 0.26366341 0.25384337 0.19953026 0.10531284 -0.011598996 -0.11633188 -0.18931416 -0.21297055 -0.17659469 -0.083554968][0.24448371 0.18763962 0.12439178 0.084953621 0.067312576 0.060846355 0.041328281 -0.0015188905 -0.065694861 -0.14298795 -0.20851944 -0.24503879 -0.23635082 -0.17600439 -0.073647544][0.13049039 0.0608171 -0.016172113 -0.075882494 -0.11455266 -0.13472429 -0.15158202 -0.17290898 -0.19893827 -0.22866243 -0.24917205 -0.25009242 -0.21982081 -0.15296108 -0.05965611][0.0029243662 -0.067434877 -0.1407969 -0.20075332 -0.24125262 -0.26093024 -0.26827022 -0.26924604 -0.26431885 -0.25585908 -0.24233134 -0.2218426 -0.187544 -0.13225435 -0.062865578][-0.10722674 -0.16549921 -0.21927342 -0.25969246 -0.28314582 -0.29059827 -0.28690669 -0.27496973 -0.255067 -0.23055327 -0.20536841 -0.1814938 -0.15571506 -0.12127996 -0.082266152]]...]
INFO - root - 2017-12-11 06:23:34.610254: step 19810, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:59m:45s remains)
INFO - root - 2017-12-11 06:23:39.885620: step 19820, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 45h:12m:40s remains)
INFO - root - 2017-12-11 06:23:45.247728: step 19830, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:34m:36s remains)
INFO - root - 2017-12-11 06:23:50.687383: step 19840, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:17m:12s remains)
INFO - root - 2017-12-11 06:23:56.054263: step 19850, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:50m:54s remains)
INFO - root - 2017-12-11 06:24:01.465652: step 19860, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 48h:56m:51s remains)
INFO - root - 2017-12-11 06:24:06.812704: step 19870, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 45h:50m:15s remains)
INFO - root - 2017-12-11 06:24:12.105538: step 19880, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.517 sec/batch; 44h:55m:15s remains)
INFO - root - 2017-12-11 06:24:17.428266: step 19890, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 46h:48m:59s remains)
INFO - root - 2017-12-11 06:24:22.809304: step 19900, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:00m:11s remains)
2017-12-11 06:24:23.363220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031947009 -0.0011458435 0.0590609 0.14529124 0.23594965 0.30423719 0.34051698 0.34626687 0.32463986 0.30222371 0.30996424 0.35250568 0.39610964 0.40024558 0.36512166][-0.033616349 -0.0026953737 0.056772396 0.14060606 0.226515 0.28886867 0.32077906 0.32717848 0.31189436 0.29914284 0.31494635 0.35841921 0.39877445 0.39734715 0.35712993][-0.037231758 -0.012046196 0.038273882 0.11017101 0.18420479 0.23914091 0.27071759 0.28404659 0.28088546 0.28059644 0.3033185 0.34541342 0.38041 0.37521681 0.33589607][-0.041424744 -0.021702332 0.02017854 0.081928976 0.14784604 0.20147519 0.24002641 0.26670009 0.27792689 0.28679347 0.30968907 0.34479159 0.37244356 0.36451417 0.32970265][-0.045500141 -0.029410768 0.0077258991 0.06526646 0.13089408 0.19175747 0.24400361 0.28677559 0.30884761 0.31734413 0.32859567 0.34818843 0.36610928 0.35736758 0.32942775][-0.049382493 -0.034990732 0.0023245965 0.0638556 0.13989593 0.2182108 0.28998485 0.34723797 0.37058836 0.36371839 0.34803957 0.34252885 0.3487601 0.34270698 0.32495308][-0.051698815 -0.035952639 0.0070941471 0.080999866 0.177375 0.28099605 0.37451214 0.44090125 0.45381418 0.41762644 0.36335978 0.3273024 0.321691 0.32000557 0.31248519][-0.050933123 -0.031292055 0.021650918 0.11252442 0.23214903 0.36003473 0.47012559 0.53741145 0.53112906 0.46034402 0.36787084 0.30525923 0.29082716 0.29253459 0.29129645][-0.049569856 -0.025944352 0.036431186 0.14157304 0.27736557 0.41766047 0.53178555 0.59131008 0.56488281 0.46756405 0.3513867 0.2763271 0.26084045 0.2676467 0.27255848][-0.050797425 -0.026286088 0.039805911 0.14942382 0.28629252 0.42033425 0.52251756 0.56745875 0.52757776 0.4221296 0.30542049 0.23653619 0.22969413 0.24667232 0.263355][-0.054933887 -0.033199664 0.028565809 0.12968843 0.25027505 0.35994673 0.43634105 0.4627496 0.41881979 0.32460433 0.22858784 0.18085952 0.18965457 0.22123647 0.25567442][-0.05978509 -0.043082688 0.0081272051 0.090498291 0.1826385 0.25766677 0.30188343 0.30916888 0.26854303 0.19816053 0.13470292 0.1142479 0.13986245 0.18651925 0.23947257][-0.062993251 -0.050205842 -0.0099382326 0.052610781 0.11694076 0.16061738 0.17624724 0.16576491 0.12801208 0.079532966 0.044349421 0.044369761 0.080216832 0.13744648 0.20466603][-0.062286381 -0.050356675 -0.016310928 0.033291984 0.080045417 0.10482647 0.10243636 0.077944227 0.038130544 -0.00055306626 -0.023280345 -0.018448779 0.014834915 0.072053604 0.14380524][-0.058093783 -0.044258773 -0.011180855 0.033524361 0.073661417 0.092315555 0.0838576 0.052252688 0.0083738137 -0.031002816 -0.05631946 -0.062006831 -0.045269083 -0.0024663382 0.059522051]]...]
INFO - root - 2017-12-11 06:24:28.476484: step 19910, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.554 sec/batch; 48h:04m:08s remains)
INFO - root - 2017-12-11 06:24:33.866286: step 19920, loss = 0.66, batch loss = 0.60 (14.6 examples/sec; 0.548 sec/batch; 47h:36m:55s remains)
INFO - root - 2017-12-11 06:24:39.282772: step 19930, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 48h:23m:53s remains)
INFO - root - 2017-12-11 06:24:44.625289: step 19940, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:16m:40s remains)
INFO - root - 2017-12-11 06:24:49.996627: step 19950, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:20m:57s remains)
INFO - root - 2017-12-11 06:24:55.283715: step 19960, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.507 sec/batch; 43h:59m:36s remains)
INFO - root - 2017-12-11 06:25:00.604481: step 19970, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:41m:23s remains)
INFO - root - 2017-12-11 06:25:06.042384: step 19980, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:09m:58s remains)
INFO - root - 2017-12-11 06:25:11.485567: step 19990, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 46h:01m:26s remains)
INFO - root - 2017-12-11 06:25:16.791466: step 20000, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.516 sec/batch; 44h:48m:28s remains)
2017-12-11 06:25:17.324754: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068591371 0.056324847 0.039000951 0.021939171 0.0074294382 7.250786e-06 0.004236667 0.017305972 0.032713603 0.04828186 0.060532704 0.064459167 0.053551987 0.029290566 0.00023819543][0.17410459 0.15958491 0.1340673 0.10737923 0.084308892 0.07320299 0.081342943 0.10347985 0.12711649 0.14882454 0.16376227 0.16455311 0.14163531 0.09750066 0.046186555][0.29308042 0.27715659 0.24380112 0.20865165 0.1798591 0.16837846 0.1818831 0.21308945 0.24374095 0.26878992 0.28211305 0.27517021 0.23677452 0.17025106 0.095005184][0.39955977 0.38955736 0.3583768 0.32447788 0.29779065 0.28950152 0.3055301 0.33875251 0.36945754 0.39139915 0.39685407 0.37645778 0.31951058 0.23016851 0.13239801][0.45928323 0.47063121 0.46216604 0.44878623 0.43721223 0.43739644 0.45264784 0.47875077 0.49936503 0.50912887 0.49902096 0.45908046 0.38062817 0.2685498 0.15078464][0.47222525 0.51487488 0.54554081 0.57000649 0.58735245 0.60521913 0.62371367 0.64225852 0.64866912 0.63869935 0.60438687 0.53765535 0.43297562 0.29556617 0.15755747][0.46025738 0.53570837 0.60781312 0.67261177 0.7207492 0.75701541 0.78010941 0.79320717 0.78681082 0.75526357 0.69544548 0.60431421 0.47788808 0.31864992 0.16297759][0.42616469 0.52790725 0.63224918 0.72701323 0.79601991 0.84232205 0.86796796 0.87973106 0.86699694 0.81877059 0.73928833 0.63386256 0.49726945 0.32577321 0.16008919][0.35916236 0.469387 0.58400345 0.686864 0.75749505 0.80063117 0.82590687 0.84296119 0.83530474 0.7835263 0.69808668 0.59377629 0.46236989 0.29392621 0.13216563][0.26487306 0.35783127 0.45622659 0.54421055 0.60003906 0.63112426 0.655132 0.68425667 0.69150537 0.64860666 0.56957382 0.47915515 0.36588857 0.21585245 0.073110074][0.15639961 0.21587601 0.28038254 0.33790737 0.36903769 0.38206023 0.40055567 0.43872166 0.46103764 0.43319711 0.37075916 0.30463564 0.22098865 0.10406367 -0.0061175767][0.035783324 0.0625928 0.09273538 0.11800749 0.12381409 0.1175383 0.12498541 0.16170086 0.19075696 0.17760991 0.1367778 0.099177763 0.051042307 -0.023632405 -0.093040012][-0.078847 -0.076502658 -0.070818543 -0.067951493 -0.078009054 -0.095421396 -0.096518643 -0.066812016 -0.039410554 -0.04309006 -0.066353425 -0.082542226 -0.10223667 -0.13903549 -0.17030105][-0.14651144 -0.15898746 -0.16736437 -0.17623979 -0.19217038 -0.2113086 -0.2157501 -0.19482465 -0.17425312 -0.17345957 -0.18520828 -0.18961579 -0.19291112 -0.20423175 -0.20883186][-0.16384247 -0.18091649 -0.1933354 -0.20512727 -0.2200207 -0.23592761 -0.24144609 -0.22977017 -0.21763843 -0.21570128 -0.22038533 -0.2191105 -0.21478254 -0.2123563 -0.20308155]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 06:25:22.903943: step 20010, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 45h:57m:36s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:25:28.275098: step 20020, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 44h:42m:57s remains)
INFO - root - 2017-12-11 06:25:33.596747: step 20030, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:36m:50s remains)
INFO - root - 2017-12-11 06:25:38.912674: step 20040, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:30m:03s remains)
INFO - root - 2017-12-11 06:25:44.296459: step 20050, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:02m:42s remains)
INFO - root - 2017-12-11 06:25:49.728306: step 20060, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.586 sec/batch; 50h:51m:50s remains)
INFO - root - 2017-12-11 06:25:55.081195: step 20070, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:04m:07s remains)
INFO - root - 2017-12-11 06:26:00.352862: step 20080, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 46h:22m:17s remains)
INFO - root - 2017-12-11 06:26:05.734923: step 20090, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 47h:25m:24s remains)
INFO - root - 2017-12-11 06:26:11.102265: step 20100, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:03m:42s remains)
2017-12-11 06:26:11.695914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0044584754 -0.009042 -0.015754353 -0.019863373 -0.019892314 -0.015672434 -0.0098593123 -0.0041973391 0.000614769 0.0021683767 -0.0015031381 -0.010979699 -0.024262132 -0.037471879 -0.048832804][0.062234364 0.063476682 0.05633577 0.050362077 0.050805133 0.059656691 0.070501164 0.079225548 0.082617529 0.075915396 0.058364343 0.032164127 0.0032500841 -0.02220222 -0.041529197][0.15447947 0.16560942 0.16144818 0.15632524 0.15961573 0.17589413 0.19210924 0.20206165 0.19989716 0.17850144 0.13991438 0.0905801 0.041121583 -0.00066152768 -0.030275464][0.253584 0.27102059 0.26830652 0.26481566 0.27358985 0.30016744 0.32211542 0.33157459 0.32157522 0.28433478 0.22391468 0.1502609 0.079142928 0.019727727 -0.021264367][0.35697085 0.37776405 0.37467751 0.37230241 0.38699135 0.42335433 0.44796 0.45192647 0.42975411 0.37610182 0.29638046 0.20152198 0.11136865 0.03618449 -0.015333543][0.4626047 0.48590711 0.47946164 0.47258249 0.48578304 0.524985 0.54772037 0.54290342 0.50638169 0.43779239 0.34436524 0.23514296 0.13156354 0.045104038 -0.013815613][0.55105621 0.57757753 0.56826943 0.55345154 0.557888 0.5933941 0.61413687 0.60371315 0.55580628 0.47608906 0.37455976 0.25658676 0.14324947 0.048258822 -0.015437668][0.58744335 0.62010866 0.6171037 0.601693 0.59961623 0.63127029 0.65395063 0.64280242 0.58660328 0.49879849 0.39327344 0.27009585 0.14795034 0.045438737 -0.020540819][0.57433641 0.61620611 0.63215911 0.6314261 0.63382888 0.6655547 0.691271 0.67759866 0.60999846 0.51136869 0.40250367 0.27714631 0.14787716 0.038900759 -0.028100191][0.52317119 0.57320172 0.60968459 0.62923 0.6413449 0.67410868 0.7013737 0.6838333 0.60507762 0.49551961 0.38539004 0.26408717 0.13577057 0.026694231 -0.038375992][0.44408113 0.49819463 0.54703444 0.57881796 0.59461206 0.62330729 0.64985204 0.63294053 0.55318886 0.44139192 0.33496034 0.22347936 0.10507622 0.0043829349 -0.05384795][0.33553904 0.39424774 0.45370045 0.49599242 0.51484954 0.53738576 0.55909967 0.541916 0.466249 0.35909382 0.2595574 0.15972529 0.055912647 -0.030211207 -0.076685153][0.19380546 0.25693697 0.32729524 0.38324 0.41185892 0.43300813 0.44724527 0.42428681 0.35066202 0.25072289 0.1606137 0.074974142 -0.0096402215 -0.07534191 -0.10526631][0.039554298 0.094605193 0.16154318 0.22073513 0.25697249 0.27966803 0.28858724 0.26202428 0.19500892 0.11017091 0.037453085 -0.026646581 -0.085024238 -0.12441494 -0.13428837][-0.10371216 -0.070849717 -0.026478797 0.016018048 0.045882277 0.066536643 0.075515077 0.056830186 0.0098333806 -0.047480781 -0.093879364 -0.13065423 -0.15911578 -0.1701052 -0.15943934]]...]
INFO - root - 2017-12-11 06:26:16.682860: step 20110, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:06m:48s remains)
INFO - root - 2017-12-11 06:26:21.991651: step 20120, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.511 sec/batch; 44h:20m:55s remains)
INFO - root - 2017-12-11 06:26:27.398611: step 20130, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:24m:44s remains)
INFO - root - 2017-12-11 06:26:32.749584: step 20140, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.532 sec/batch; 46h:07m:13s remains)
INFO - root - 2017-12-11 06:26:38.172379: step 20150, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:08m:59s remains)
INFO - root - 2017-12-11 06:26:43.501960: step 20160, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 48h:52m:41s remains)
INFO - root - 2017-12-11 06:26:48.883198: step 20170, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:54m:40s remains)
INFO - root - 2017-12-11 06:26:54.302123: step 20180, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:55m:15s remains)
INFO - root - 2017-12-11 06:26:59.555389: step 20190, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 44h:58m:54s remains)
INFO - root - 2017-12-11 06:27:04.956850: step 20200, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:00m:13s remains)
2017-12-11 06:27:05.458734: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12675418 0.10129346 0.092522107 0.11103324 0.15166074 0.19607328 0.22997139 0.2463019 0.24170102 0.2182651 0.18351488 0.14856872 0.1188641 0.0995337 0.098520055][0.11629482 0.099392571 0.10616414 0.14394535 0.20124888 0.25405172 0.28537604 0.28985962 0.27022213 0.23677497 0.20060548 0.17070624 0.14783998 0.13204683 0.12965515][0.10529793 0.09718018 0.11462492 0.16211286 0.22494042 0.2777862 0.30334878 0.29817167 0.26846084 0.22925133 0.19357656 0.16807407 0.15016121 0.13644913 0.13211726][0.1087783 0.10435924 0.12267853 0.16697815 0.22329129 0.26928762 0.28948048 0.28095296 0.24924348 0.20868544 0.17223042 0.14551921 0.1263546 0.11198396 0.10723402][0.12473316 0.1225035 0.13942172 0.17819636 0.22579484 0.26375714 0.27917397 0.26972461 0.23858126 0.19638669 0.15516885 0.12108339 0.094908774 0.077252336 0.073722236][0.13813582 0.14213759 0.16347454 0.20566013 0.25467312 0.2930035 0.30811903 0.2982361 0.26533327 0.21681255 0.16599542 0.12131236 0.086534254 0.064971894 0.06354633][0.13090293 0.14697595 0.18267155 0.24193826 0.30718458 0.35872304 0.38150659 0.37363333 0.33778307 0.28028432 0.21860844 0.16485442 0.12486983 0.10241611 0.10442857][0.089586526 0.12221124 0.17969614 0.26613006 0.3593992 0.43558121 0.47517136 0.47524342 0.44070527 0.37928951 0.313654 0.25934944 0.22318476 0.20726033 0.21569102][0.017375031 0.065171808 0.1428481 0.25420997 0.37443075 0.4761152 0.53712624 0.55350953 0.53176677 0.48123428 0.4277502 0.38774928 0.36746609 0.36563936 0.38161287][-0.072850652 -0.018472718 0.067829981 0.18903589 0.32179108 0.43879771 0.51867914 0.55800164 0.562137 0.53957582 0.514953 0.50273663 0.50736177 0.52340937 0.54508668][-0.15429929 -0.10947798 -0.034708459 0.07269001 0.19569542 0.31136838 0.40157461 0.4630934 0.49774459 0.5101729 0.51948857 0.53639919 0.56428552 0.59505838 0.61929792][-0.20364732 -0.1794062 -0.1312671 -0.056468632 0.036695581 0.13275433 0.2183051 0.28987542 0.34525689 0.38435274 0.41886878 0.45477614 0.49503043 0.53169823 0.55468076][-0.21418415 -0.21078819 -0.19169557 -0.15461878 -0.10050765 -0.037532307 0.02629403 0.088635504 0.14564207 0.19446446 0.23964117 0.28169587 0.32239643 0.355616 0.3730396][-0.19548161 -0.20613091 -0.20799662 -0.20066655 -0.18145739 -0.1535213 -0.12039518 -0.0820914 -0.04142293 -0.0013279534 0.03821725 0.074114084 0.10619684 0.12990345 0.13968109][-0.16310731 -0.17882328 -0.1904749 -0.19935261 -0.20240352 -0.20020546 -0.19374654 -0.1817078 -0.16513404 -0.14515164 -0.1227648 -0.10165023 -0.083141893 -0.070971757 -0.0681944]]...]
INFO - root - 2017-12-11 06:27:10.619213: step 20210, loss = 0.71, batch loss = 0.65 (21.2 examples/sec; 0.377 sec/batch; 32h:41m:52s remains)
INFO - root - 2017-12-11 06:27:15.879941: step 20220, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 45h:28m:43s remains)
INFO - root - 2017-12-11 06:27:21.274095: step 20230, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:53m:51s remains)
INFO - root - 2017-12-11 06:27:26.658102: step 20240, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 44h:59m:12s remains)
INFO - root - 2017-12-11 06:27:31.988179: step 20250, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:27m:28s remains)
INFO - root - 2017-12-11 06:27:37.389613: step 20260, loss = 0.70, batch loss = 0.64 (13.3 examples/sec; 0.601 sec/batch; 52h:07m:49s remains)
INFO - root - 2017-12-11 06:27:42.822080: step 20270, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 45h:39m:24s remains)
INFO - root - 2017-12-11 06:27:48.099033: step 20280, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 46h:04m:10s remains)
INFO - root - 2017-12-11 06:27:53.474175: step 20290, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 49h:06m:27s remains)
INFO - root - 2017-12-11 06:27:58.824268: step 20300, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:30m:44s remains)
2017-12-11 06:27:59.351244: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1609536 0.16780967 0.16058728 0.1500981 0.14081261 0.13748427 0.14111945 0.14549136 0.14547111 0.1379918 0.12681985 0.11608036 0.10566054 0.090791523 0.0707102][0.20041324 0.20225465 0.18433864 0.16152267 0.13975169 0.12465394 0.11884125 0.11893493 0.1206781 0.11926769 0.11407314 0.10678129 0.097018257 0.082586035 0.066309474][0.21151567 0.212013 0.18964659 0.16228214 0.13542081 0.11358434 0.099853218 0.093754761 0.093317769 0.094129764 0.093087308 0.090257324 0.085454047 0.0777869 0.071254216][0.20930418 0.20334907 0.17655814 0.14701043 0.11906258 0.095745429 0.079464041 0.071235582 0.069947593 0.071956933 0.0734332 0.07423003 0.07507392 0.075568549 0.079329289][0.20547572 0.19028056 0.159337 0.1287944 0.10186115 0.080275185 0.065582678 0.058658268 0.057559837 0.059069294 0.059196662 0.05906843 0.061526779 0.0669645 0.078371771][0.21048878 0.1901807 0.15964518 0.13322379 0.11228934 0.096507467 0.086513564 0.081907608 0.079241276 0.075297 0.066481218 0.056370828 0.05148606 0.054261342 0.067515828][0.20277742 0.18359418 0.1589254 0.14323452 0.13555552 0.13216284 0.1318036 0.13180982 0.12717161 0.11486298 0.092476621 0.066656008 0.04821042 0.042609546 0.052809618][0.163108 0.15063009 0.13784513 0.13843191 0.14931257 0.16389094 0.17868002 0.18836085 0.1861283 0.16878197 0.13489974 0.093771651 0.060612537 0.044811342 0.049906958][0.10085082 0.095500693 0.095783882 0.11173008 0.13916831 0.17075388 0.20248668 0.22667734 0.23435853 0.22093366 0.18454659 0.13454597 0.089562587 0.06421791 0.064032413][0.037443232 0.037051231 0.04617523 0.070931889 0.10774361 0.15044644 0.19665708 0.23769006 0.26255509 0.26394996 0.23796433 0.19107288 0.14230677 0.11122083 0.10645147][-0.0035955813 -0.0037425996 0.0064334148 0.030954752 0.06826622 0.11424623 0.16853449 0.2226904 0.26444831 0.28414431 0.27610466 0.24359731 0.2025778 0.17366941 0.16672029][-0.024075357 -0.025237657 -0.018057328 0.00063433842 0.031017786 0.071250431 0.12297767 0.17934336 0.22913657 0.26256022 0.27347216 0.26251167 0.24019252 0.22292912 0.21831907][-0.031179147 -0.032446682 -0.027476568 -0.01542984 0.0045318604 0.032820761 0.072827138 0.11996291 0.16595285 0.20281996 0.22534578 0.23230751 0.22871706 0.2249718 0.2250886][-0.03480424 -0.035573527 -0.032049529 -0.025388278 -0.015249128 0.00012566758 0.025306916 0.057490543 0.091397 0.12151937 0.14433204 0.15787052 0.16347124 0.16696684 0.17011359][-0.039331749 -0.041736785 -0.040748879 -0.038188823 -0.034560684 -0.028574992 -0.015829373 0.0022156602 0.022399686 0.041469149 0.057450458 0.068897247 0.07538721 0.0796834 0.082462236]]...]
INFO - root - 2017-12-11 06:28:04.787662: step 20310, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:06m:12s remains)
INFO - root - 2017-12-11 06:28:09.797481: step 20320, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:56m:26s remains)
INFO - root - 2017-12-11 06:28:15.224812: step 20330, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 46h:18m:56s remains)
INFO - root - 2017-12-11 06:28:20.649338: step 20340, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 46h:06m:55s remains)
INFO - root - 2017-12-11 06:28:25.921502: step 20350, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 45h:09m:09s remains)
INFO - root - 2017-12-11 06:28:31.294646: step 20360, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 47h:00m:08s remains)
INFO - root - 2017-12-11 06:28:36.629592: step 20370, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 44h:25m:30s remains)
INFO - root - 2017-12-11 06:28:42.023770: step 20380, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:40m:49s remains)
INFO - root - 2017-12-11 06:28:47.318412: step 20390, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:59m:43s remains)
INFO - root - 2017-12-11 06:28:52.710781: step 20400, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 46h:41m:01s remains)
2017-12-11 06:28:53.240912: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11176393 0.122696 0.13792789 0.15212868 0.16044886 0.16189864 0.16036735 0.16656281 0.19178721 0.23495962 0.2754795 0.29448858 0.28453422 0.25399494 0.21683711][0.13397889 0.1552128 0.18343802 0.21088965 0.22744307 0.22989848 0.22403306 0.22741471 0.25777489 0.31501266 0.37462395 0.412278 0.41528323 0.3863681 0.33919024][0.14195031 0.1770948 0.22364792 0.26926684 0.2978031 0.30243921 0.2899231 0.28269073 0.30512574 0.36102393 0.42973429 0.48697963 0.51428908 0.50193417 0.45561075][0.14953038 0.19608603 0.26024827 0.32584962 0.37199053 0.38623747 0.37210891 0.35160318 0.35451046 0.39223194 0.45466542 0.52260822 0.57326204 0.58307433 0.54603177][0.17518976 0.22461015 0.29525331 0.3731882 0.43770114 0.47054359 0.46510231 0.43583387 0.41491768 0.42347395 0.46504825 0.5283407 0.58908081 0.61503482 0.59102678][0.2360031 0.27941808 0.33780721 0.40971628 0.48243847 0.53562427 0.54833925 0.52073461 0.48104566 0.4578093 0.46835104 0.51061141 0.56367642 0.59544683 0.58717722][0.33736384 0.367912 0.39581025 0.43918857 0.50106889 0.56546086 0.59946412 0.58556008 0.54134786 0.49567917 0.47523573 0.48684469 0.51763797 0.54269445 0.54597539][0.45651084 0.47081935 0.45617947 0.45187539 0.48046029 0.53728372 0.58685017 0.59628326 0.56904876 0.525746 0.49366462 0.48330569 0.49062395 0.50101447 0.50525486][0.54776818 0.54782856 0.49263343 0.43746758 0.42051709 0.45169374 0.50277215 0.53483415 0.53952336 0.5243175 0.50790864 0.49586055 0.48932141 0.4841159 0.48059928][0.57381815 0.56686676 0.48631838 0.39469296 0.33778968 0.33719629 0.37620375 0.42042404 0.45634156 0.47817415 0.4893589 0.48513156 0.46967605 0.45004562 0.43773985][0.53350747 0.52445394 0.4356426 0.32877859 0.24936919 0.22532687 0.25082421 0.29941422 0.35565895 0.40374032 0.43250826 0.42950383 0.40153819 0.36701092 0.34747741][0.4468765 0.43356869 0.34656733 0.24174391 0.15811872 0.12438918 0.14173162 0.18953308 0.25145614 0.30664217 0.33518398 0.32335806 0.28124359 0.23559742 0.2122108][0.32829136 0.30548078 0.2252014 0.13486454 0.062295277 0.030617755 0.043087304 0.083197623 0.13532646 0.18026994 0.19771774 0.17765953 0.13236959 0.08994256 0.072459787][0.18686637 0.15326685 0.086126335 0.021231031 -0.028291 -0.050164171 -0.042283785 -0.016901994 0.014294682 0.039703224 0.045422 0.026531091 -0.0056146872 -0.029241616 -0.031344522][0.057339694 0.020950494 -0.025463434 -0.05977539 -0.0835607 -0.096416354 -0.09697172 -0.091345042 -0.084326617 -0.077871971 -0.077724673 -0.08649841 -0.098093539 -0.10033017 -0.08844582]]...]
INFO - root - 2017-12-11 06:28:58.565312: step 20410, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 46h:00m:14s remains)
INFO - root - 2017-12-11 06:29:03.688486: step 20420, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:48m:32s remains)
INFO - root - 2017-12-11 06:29:09.021331: step 20430, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 46h:13m:47s remains)
INFO - root - 2017-12-11 06:29:14.401577: step 20440, loss = 0.66, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 46h:22m:47s remains)
INFO - root - 2017-12-11 06:29:19.716533: step 20450, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:17m:44s remains)
INFO - root - 2017-12-11 06:29:25.014159: step 20460, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:39m:11s remains)
INFO - root - 2017-12-11 06:29:30.378231: step 20470, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 48h:27m:50s remains)
INFO - root - 2017-12-11 06:29:35.679525: step 20480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:05m:59s remains)
INFO - root - 2017-12-11 06:29:41.008665: step 20490, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 46h:31m:47s remains)
INFO - root - 2017-12-11 06:29:46.425241: step 20500, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 45h:05m:31s remains)
2017-12-11 06:29:46.949091: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31173655 0.36578405 0.40006965 0.41436788 0.40258938 0.37494186 0.35803235 0.34947622 0.34975877 0.35279161 0.35654426 0.36291835 0.35719123 0.33570424 0.3208468][0.34219623 0.37411907 0.38538247 0.38300103 0.36129346 0.32929909 0.31239885 0.30664775 0.30915722 0.31350818 0.31886646 0.32881221 0.32636857 0.31098628 0.30562815][0.36772302 0.37889928 0.36855289 0.34914693 0.31639293 0.27904409 0.25984749 0.25288898 0.25292194 0.25477284 0.25997609 0.27286297 0.27525273 0.26853094 0.2740643][0.39523524 0.39034426 0.363208 0.32998762 0.28966421 0.25233871 0.2363195 0.2322353 0.23248836 0.23351353 0.24034797 0.25651351 0.26171342 0.25916621 0.26769066][0.41614839 0.39635557 0.35826188 0.32023218 0.28358081 0.25836715 0.25576463 0.26087251 0.26497865 0.26660684 0.2751444 0.29274231 0.2983889 0.29588178 0.29998091][0.4201 0.38734964 0.34889781 0.32249081 0.3059684 0.30397516 0.31917498 0.33131307 0.33344156 0.32973716 0.33551508 0.35312063 0.36079425 0.35955015 0.357634][0.39821407 0.35508418 0.32776323 0.33034414 0.35066837 0.38210005 0.41795117 0.43471706 0.42774695 0.41013435 0.40529439 0.41978151 0.43091604 0.43297032 0.42719933][0.34573832 0.29516762 0.28244966 0.31875032 0.381635 0.4525356 0.515751 0.54481518 0.53450948 0.50559151 0.49074239 0.5030545 0.51938 0.524944 0.51567179][0.27643088 0.22016239 0.21817313 0.27882177 0.37307769 0.47530293 0.56385988 0.60950738 0.60406184 0.57243824 0.55590773 0.573577 0.60233438 0.61860788 0.61350083][0.20249195 0.14359178 0.14323168 0.21015795 0.31380638 0.42783779 0.52924806 0.58765745 0.59099478 0.56480396 0.555141 0.58474606 0.63187057 0.66626245 0.67505342][0.14760804 0.086480886 0.076378144 0.12851241 0.21630037 0.31821766 0.41456938 0.47749376 0.490695 0.4761686 0.47877058 0.52145106 0.58555996 0.637501 0.6630652][0.13803747 0.075118594 0.046152573 0.065119989 0.11475655 0.18351616 0.25762609 0.3134129 0.33224794 0.32941541 0.34276941 0.39270958 0.46483028 0.52646744 0.56455278][0.18129264 0.12239328 0.07478141 0.05478799 0.056140933 0.078782976 0.11610968 0.14927638 0.16079704 0.16001353 0.175731 0.22332606 0.29200637 0.35333169 0.3957684][0.24852251 0.20589978 0.1551645 0.1125512 0.078433447 0.060267054 0.056914359 0.05473512 0.040985025 0.023525776 0.025077702 0.055421747 0.10625251 0.15455048 0.19065849][0.29735762 0.28245515 0.24809083 0.20851216 0.16527897 0.12717198 0.095003426 0.059832066 0.014674344 -0.03073539 -0.055642638 -0.053947963 -0.032801721 -0.01059002 0.0059239962]]...]
INFO - root - 2017-12-11 06:29:52.306370: step 20510, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 47h:54m:50s remains)
INFO - root - 2017-12-11 06:29:57.369229: step 20520, loss = 0.70, batch loss = 0.64 (23.5 examples/sec; 0.340 sec/batch; 29h:27m:08s remains)
INFO - root - 2017-12-11 06:30:02.735759: step 20530, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:21m:53s remains)
INFO - root - 2017-12-11 06:30:08.193714: step 20540, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 48h:55m:49s remains)
INFO - root - 2017-12-11 06:30:13.558504: step 20550, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:25m:11s remains)
INFO - root - 2017-12-11 06:30:18.827643: step 20560, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:31m:55s remains)
INFO - root - 2017-12-11 06:30:24.141072: step 20570, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:25m:37s remains)
INFO - root - 2017-12-11 06:30:29.462120: step 20580, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:50m:08s remains)
INFO - root - 2017-12-11 06:30:34.776434: step 20590, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:47m:25s remains)
INFO - root - 2017-12-11 06:30:40.125455: step 20600, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 46h:39m:03s remains)
2017-12-11 06:30:40.631724: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15524268 0.20059173 0.22346388 0.22116569 0.20523591 0.18912359 0.17523415 0.15543438 0.12764329 0.10318621 0.099681519 0.12317281 0.16182871 0.19181666 0.19428732][0.21406575 0.26387221 0.28231671 0.27069375 0.24915621 0.23658891 0.23263444 0.22107358 0.19448686 0.16414282 0.15151985 0.16711693 0.20155592 0.23179725 0.23721904][0.28388479 0.33203048 0.33846125 0.31218898 0.28308472 0.27541116 0.28471488 0.28506088 0.262022 0.22467232 0.19714032 0.19641624 0.22006725 0.24950725 0.26294434][0.36696517 0.41493258 0.40892634 0.36778069 0.33128724 0.32798237 0.34841377 0.35814875 0.33708978 0.29122806 0.24519521 0.22314547 0.23220044 0.26018685 0.2843473][0.44085547 0.49232113 0.47914827 0.42943695 0.39100617 0.39358607 0.42284665 0.43908322 0.41889662 0.36484382 0.30021307 0.255519 0.24820238 0.27349135 0.30727562][0.471106 0.52675956 0.5142045 0.46815503 0.43885458 0.45257124 0.490923 0.51194656 0.49156693 0.42972812 0.34832266 0.28333694 0.26148897 0.284178 0.32501021][0.43256423 0.48838407 0.48379084 0.45459753 0.44534487 0.47475097 0.52179337 0.54527783 0.52305305 0.45427981 0.36048362 0.28138271 0.25010183 0.27188545 0.31856093][0.33626685 0.38658571 0.39330944 0.38717425 0.40070412 0.44337 0.49402407 0.51600915 0.49098203 0.41863173 0.32074457 0.23755571 0.20373231 0.22691496 0.27957979][0.22225729 0.2639994 0.28158796 0.29649332 0.32728505 0.37533572 0.42230785 0.43882802 0.41027358 0.33779946 0.24381292 0.16631819 0.13650014 0.16314347 0.22092336][0.12586573 0.1593947 0.18502474 0.21465473 0.25538793 0.30254585 0.34148017 0.34977376 0.31590912 0.24383581 0.15742896 0.091553994 0.071555719 0.104472 0.16565803][0.055427954 0.080722831 0.10979284 0.14726929 0.1924915 0.23640546 0.26638553 0.26473427 0.22384879 0.15228601 0.075392276 0.024259897 0.018566193 0.060349729 0.12378375][0.00696344 0.023180626 0.049678218 0.086101912 0.12762806 0.16367093 0.18321781 0.17223734 0.12762742 0.0618941 -0.0011287585 -0.035466477 -0.02585227 0.023976106 0.088474385][-0.019481683 -0.01320715 0.0044662496 0.029919066 0.057835568 0.080286831 0.088961795 0.073264942 0.033096064 -0.018635631 -0.063206851 -0.081046939 -0.05950509 -0.0051737255 0.059115168][-0.020256452 -0.02369847 -0.019168667 -0.011337063 -0.0029688245 0.0036320149 0.0034440686 -0.011295036 -0.040182617 -0.072748207 -0.097466178 -0.10153275 -0.073553607 -0.019450264 0.042111415][-0.0043198206 -0.015605824 -0.024551049 -0.034095332 -0.043345798 -0.049057759 -0.053163 -0.062097307 -0.0771928 -0.091447949 -0.0997999 -0.094835661 -0.065593205 -0.015894275 0.039412964]]...]
INFO - root - 2017-12-11 06:30:45.944298: step 20610, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 46h:08m:50s remains)
INFO - root - 2017-12-11 06:30:51.307881: step 20620, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:05m:08s remains)
INFO - root - 2017-12-11 06:30:56.341977: step 20630, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 46h:23m:59s remains)
INFO - root - 2017-12-11 06:31:01.711430: step 20640, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 45h:48m:41s remains)
INFO - root - 2017-12-11 06:31:07.102113: step 20650, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:45m:42s remains)
INFO - root - 2017-12-11 06:31:12.467106: step 20660, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 46h:15m:21s remains)
INFO - root - 2017-12-11 06:31:17.825921: step 20670, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:58m:58s remains)
INFO - root - 2017-12-11 06:31:23.126090: step 20680, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 46h:38m:35s remains)
INFO - root - 2017-12-11 06:31:28.453912: step 20690, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 46h:17m:45s remains)
INFO - root - 2017-12-11 06:31:33.886201: step 20700, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:23m:09s remains)
2017-12-11 06:31:34.437506: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20274371 0.26792744 0.31432867 0.321406 0.28342593 0.21565549 0.13957819 0.075572841 0.034927294 0.01500031 0.0022196886 -0.013555085 -0.033035219 -0.054142132 -0.068262458][0.18133411 0.25677067 0.31441209 0.33059904 0.29849279 0.23198485 0.15250146 0.081014365 0.030945817 0.0026788446 -0.014504776 -0.030695016 -0.048656818 -0.068292335 -0.084405728][0.16175699 0.24858893 0.32279819 0.35809731 0.3453058 0.29412824 0.22222243 0.14931837 0.089494325 0.047608163 0.019431069 -0.0025410082 -0.022060303 -0.041696429 -0.060094167][0.16538547 0.26961568 0.37082064 0.44026005 0.4636136 0.44204232 0.38637188 0.31513813 0.24333411 0.1803249 0.12955469 0.088363886 0.054115206 0.023447039 -0.0034611893][0.19599053 0.32020587 0.4558171 0.57183671 0.64658606 0.66931826 0.64098549 0.57824546 0.49599591 0.40569794 0.31786823 0.23741664 0.16700657 0.10706823 0.06059505][0.23261124 0.37300488 0.5406056 0.7036835 0.83342022 0.90812129 0.91819638 0.87432867 0.78733808 0.66727453 0.53175038 0.39679852 0.27514 0.1746271 0.10291877][0.24179132 0.38217774 0.56261307 0.75348675 0.92313671 1.0439874 1.0972776 1.0831259 1.0040109 0.86492509 0.68974173 0.50624156 0.33829194 0.20115294 0.10680512][0.20120731 0.31973091 0.48639712 0.67672527 0.86075497 1.0099279 1.0997736 1.1185495 1.0592726 0.9215138 0.73231584 0.526361 0.33456355 0.17757113 0.070887879][0.12293334 0.20491861 0.33690992 0.5016681 0.67356008 0.82605934 0.93319416 0.97486144 0.93855608 0.81992286 0.64495504 0.44858974 0.26267189 0.11057499 0.00973584][0.031617846 0.07481017 0.1620128 0.28282258 0.41866386 0.548126 0.64836025 0.69632685 0.67804068 0.58779031 0.44731954 0.28691614 0.13531324 0.015278222 -0.058108043][-0.045366686 -0.033051003 0.0098677445 0.077869162 0.1606838 0.24474563 0.31527975 0.35240149 0.34430045 0.28634253 0.19387312 0.089464575 -0.0052436222 -0.072252259 -0.10289388][-0.08613807 -0.0902478 -0.078908756 -0.056895442 -0.028214073 0.0015318108 0.02804815 0.041184518 0.0354143 0.0084824413 -0.031880945 -0.072683342 -0.10203293 -0.11005326 -0.095651262][-0.085790642 -0.087722629 -0.087884367 -0.09433908 -0.10803283 -0.12713733 -0.14529735 -0.16032168 -0.17051603 -0.17556733 -0.17445707 -0.16303843 -0.13853064 -0.098443985 -0.046947375][-0.052303743 -0.040225737 -0.038398769 -0.059607409 -0.10277213 -0.15820521 -0.21046339 -0.2473557 -0.26231879 -0.25477907 -0.22906344 -0.18713035 -0.13124903 -0.062848568 0.011757783][-0.0042720642 0.026360998 0.036586583 0.0090756081 -0.052943535 -0.13170835 -0.20459515 -0.25381035 -0.27112892 -0.25781095 -0.2221823 -0.16987491 -0.10451077 -0.027799882 0.054377522]]...]
INFO - root - 2017-12-11 06:31:39.884748: step 20710, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:31m:24s remains)
INFO - root - 2017-12-11 06:31:45.180617: step 20720, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:39m:29s remains)
INFO - root - 2017-12-11 06:31:50.176887: step 20730, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:43m:54s remains)
INFO - root - 2017-12-11 06:31:55.411719: step 20740, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.511 sec/batch; 44h:15m:32s remains)
INFO - root - 2017-12-11 06:32:00.750678: step 20750, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 45h:49m:26s remains)
INFO - root - 2017-12-11 06:32:06.126964: step 20760, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.558 sec/batch; 48h:18m:47s remains)
INFO - root - 2017-12-11 06:32:11.500254: step 20770, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:51m:45s remains)
INFO - root - 2017-12-11 06:32:16.865764: step 20780, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:33m:07s remains)
INFO - root - 2017-12-11 06:32:22.204589: step 20790, loss = 0.72, batch loss = 0.67 (15.5 examples/sec; 0.515 sec/batch; 44h:37m:46s remains)
INFO - root - 2017-12-11 06:32:27.550464: step 20800, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 47h:59m:08s remains)
2017-12-11 06:32:28.095984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051554691 -0.050932344 -0.049417023 -0.049270838 -0.05030977 -0.051701747 -0.053758182 -0.056440871 -0.059386343 -0.0618871 -0.063754193 -0.065070584 -0.06595844 -0.066805139 -0.06758216][-0.052096356 -0.052686244 -0.049244311 -0.043781739 -0.038482636 -0.035480838 -0.036565181 -0.041988503 -0.050196424 -0.058421291 -0.064519234 -0.0681235 -0.06996832 -0.071189731 -0.07236208][-0.03882673 -0.039108843 -0.030680282 -0.015210349 0.0019027196 0.013830186 0.015154289 0.0043732016 -0.014765827 -0.035300378 -0.051190753 -0.060520019 -0.064564794 -0.066136658 -0.067427434][-0.012874776 -0.011343342 0.0054585813 0.036929224 0.0734762 0.10088105 0.10732758 0.08826232 0.050137669 0.0070227208 -0.027647698 -0.048318658 -0.056767188 -0.058658589 -0.059265375][0.019488748 0.023846824 0.051974528 0.10565465 0.17035659 0.22132447 0.23711967 0.20805036 0.14362323 0.067903183 0.0052596582 -0.032780718 -0.048229806 -0.050475188 -0.049210884][0.045695167 0.052607328 0.09243641 0.17064166 0.26795435 0.34726986 0.37575302 0.33754292 0.24516742 0.13370594 0.039693233 -0.018245634 -0.042062797 -0.044782411 -0.040753622][0.0536404 0.061319456 0.10887422 0.20563464 0.329432 0.43269977 0.47298309 0.42927927 0.31639704 0.17802557 0.060133036 -0.012776829 -0.042487539 -0.044710841 -0.037300896][0.038029328 0.044223517 0.092086919 0.193056 0.32517636 0.43676466 0.48195237 0.43799582 0.32063189 0.17604883 0.052861687 -0.022327306 -0.0513781 -0.05090601 -0.04007319][0.00472292 0.0071666874 0.046487872 0.13330726 0.24906412 0.34691721 0.38614959 0.34710526 0.24482358 0.12016843 0.015524308 -0.045773685 -0.066179492 -0.060998295 -0.047772326][-0.032834504 -0.036216855 -0.012644605 0.045484882 0.12536956 0.1925144 0.21781912 0.18791223 0.11544286 0.030232828 -0.038122803 -0.073912308 -0.080507405 -0.070201814 -0.056456983][-0.062148754 -0.072537422 -0.067492209 -0.042728432 -0.0044181235 0.027881434 0.038350541 0.019954151 -0.018487675 -0.059820794 -0.088504337 -0.097375222 -0.090489179 -0.076502919 -0.0640257][-0.076627806 -0.092593923 -0.10264159 -0.10478549 -0.099732168 -0.094639868 -0.0946751 -0.10237686 -0.11321215 -0.12061828 -0.11988942 -0.11024929 -0.095648579 -0.081204608 -0.07130675][-0.078482032 -0.095384806 -0.11159008 -0.12638657 -0.13793391 -0.14636512 -0.15113546 -0.15224208 -0.14834003 -0.13900733 -0.1252099 -0.10926623 -0.094318956 -0.082775913 -0.076064073][-0.074263126 -0.08717484 -0.10026141 -0.11381131 -0.1260947 -0.13548648 -0.14018925 -0.13940425 -0.13306037 -0.12240373 -0.10950515 -0.096868426 -0.086565427 -0.07955344 -0.075874113][-0.069372959 -0.076736622 -0.083255224 -0.090080261 -0.096539237 -0.1017367 -0.10448658 -0.10413592 -0.10071081 -0.095018 -0.088227794 -0.081736192 -0.076705709 -0.073543921 -0.072056159]]...]
INFO - root - 2017-12-11 06:32:33.426961: step 20810, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:18m:51s remains)
INFO - root - 2017-12-11 06:32:38.808903: step 20820, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 48h:44m:29s remains)
INFO - root - 2017-12-11 06:32:43.870365: step 20830, loss = 0.68, batch loss = 0.62 (22.2 examples/sec; 0.361 sec/batch; 31h:13m:13s remains)
INFO - root - 2017-12-11 06:32:49.210501: step 20840, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 45h:44m:30s remains)
INFO - root - 2017-12-11 06:32:54.526058: step 20850, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 45h:35m:17s remains)
INFO - root - 2017-12-11 06:32:59.815691: step 20860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:25m:17s remains)
INFO - root - 2017-12-11 06:33:05.122971: step 20870, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:53m:57s remains)
INFO - root - 2017-12-11 06:33:10.410416: step 20880, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:44m:15s remains)
INFO - root - 2017-12-11 06:33:15.714354: step 20890, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 46h:50m:38s remains)
INFO - root - 2017-12-11 06:33:21.120532: step 20900, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 47h:22m:59s remains)
2017-12-11 06:33:21.688917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016727326 -0.020121591 -0.020171911 -0.018701307 -0.015771283 -0.012219083 -0.0087256571 -0.006170013 -0.0047827526 -0.0042878268 -0.0044251503 -0.0046364465 -0.0052146963 -0.0066483035 -0.0086437548][-0.018608522 -0.021824067 -0.021108452 -0.019005755 -0.015856283 -0.012392251 -0.0086289831 -0.0047456226 -0.0012653947 0.0009926277 0.00098151492 -0.0010951805 -0.0045464938 -0.0084173493 -0.01137676][-0.019047711 -0.021609548 -0.020280572 -0.018055258 -0.015341229 -0.012230404 -0.007476755 -0.0004364225 0.00747793 0.013023087 0.01298693 0.0074001527 -0.0010143707 -0.0087324958 -0.013146855][-0.018258953 -0.020235747 -0.018806081 -0.017153207 -0.015275412 -0.012017069 -0.0045421096 0.0083490368 0.02335223 0.033545077 0.032948926 0.021858621 0.0062299035 -0.0068689426 -0.013483851][-0.016544931 -0.018350365 -0.01756571 -0.01705474 -0.015795216 -0.01091897 0.001795502 0.023321757 0.047347836 0.062522538 0.060118403 0.041475963 0.016818492 -0.0027987568 -0.012321898][-0.014767272 -0.016941061 -0.017369639 -0.018049093 -0.016365457 -0.0076137464 0.013278918 0.045827083 0.079557352 0.098493874 0.091949724 0.063559316 0.028635716 0.0020336844 -0.01078549][-0.013779891 -0.016692638 -0.018331571 -0.019331552 -0.015204789 0.00024509185 0.03190244 0.076278478 0.11794208 0.13719146 0.12316155 0.08322224 0.037982553 0.0050620651 -0.010534573][-0.01388822 -0.017578999 -0.019849349 -0.019640299 -0.010704103 0.013822255 0.057006825 0.11092945 0.15535039 0.16939008 0.14463009 0.093348056 0.040246986 0.0037223748 -0.012503194][-0.014450704 -0.018509507 -0.020479379 -0.017713439 -0.0026889602 0.031016482 0.083214156 0.14093855 0.18085708 0.18433087 0.14790824 0.088721789 0.032945834 -0.0024819577 -0.015783155][-0.014529999 -0.01841584 -0.019348523 -0.013636996 0.0066288244 0.046437368 0.10182102 0.15584803 0.1851248 0.1758481 0.13066746 0.0697546 0.017864134 -0.011253766 -0.017989784][-0.013831429 -0.017285867 -0.017096614 -0.0093395412 0.013274811 0.0537797 0.10512161 0.14897297 0.1648598 0.14487751 0.09690582 0.041469764 -0.00044765283 -0.0191243 -0.016990885][-0.012793784 -0.016047245 -0.015498855 -0.0078324862 0.012950257 0.048103753 0.089302287 0.11974883 0.12369181 0.098470643 0.054791439 0.011077518 -0.016842872 -0.023320291 -0.012391499][-0.012346806 -0.01594324 -0.016170735 -0.010886786 0.0044241739 0.029983707 0.058350522 0.07638336 0.073305771 0.049533993 0.015922768 -0.012902013 -0.026287012 -0.022086689 -0.0051595033][-0.013443013 -0.017777417 -0.019582497 -0.017863495 -0.0093791271 0.0060346196 0.023183627 0.033099763 0.028939588 0.011817272 -0.0092573129 -0.023976583 -0.025926229 -0.015397515 0.0028358407][-0.015885936 -0.020798748 -0.023978174 -0.025207574 -0.022139767 -0.014224812 -0.0042899717 0.0022521873 0.0011217328 -0.0073075611 -0.016942186 -0.02129055 -0.016990483 -0.0055230409 0.0092239194]]...]
INFO - root - 2017-12-11 06:33:27.080900: step 20910, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 47h:10m:38s remains)
INFO - root - 2017-12-11 06:33:32.445107: step 20920, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 49h:11m:30s remains)
INFO - root - 2017-12-11 06:33:37.809763: step 20930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:42m:32s remains)
INFO - root - 2017-12-11 06:33:42.847244: step 20940, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 44h:46m:53s remains)
INFO - root - 2017-12-11 06:33:48.181052: step 20950, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.543 sec/batch; 47h:00m:28s remains)
INFO - root - 2017-12-11 06:33:53.466650: step 20960, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:27m:20s remains)
INFO - root - 2017-12-11 06:33:58.764108: step 20970, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 45h:14m:31s remains)
INFO - root - 2017-12-11 06:34:04.127031: step 20980, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:24m:18s remains)
INFO - root - 2017-12-11 06:34:09.403306: step 20990, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 44h:55m:36s remains)
INFO - root - 2017-12-11 06:34:14.702755: step 21000, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 45h:53m:58s remains)
2017-12-11 06:34:15.266096: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072405592 0.076360993 0.079589754 0.086342208 0.0947847 0.094836168 0.086531691 0.074556217 0.07207831 0.076835357 0.076854251 0.066961609 0.048723556 0.02984477 0.0041603395][0.17412342 0.18291569 0.18822843 0.19816291 0.21020256 0.2110661 0.20064494 0.18400617 0.18002613 0.18482658 0.18256095 0.16611277 0.13670093 0.10650135 0.064664349][0.29894155 0.31785634 0.32541066 0.3358537 0.34749526 0.34813449 0.33766103 0.31902221 0.31296694 0.31479359 0.30885255 0.28628114 0.24685453 0.20586286 0.14738522][0.41969013 0.44978866 0.45747823 0.46617135 0.47541195 0.4755013 0.46502644 0.44427714 0.43319845 0.42736405 0.41500631 0.38636774 0.33996567 0.29142055 0.22011746][0.51753211 0.55198282 0.55486453 0.55957681 0.56694496 0.5684939 0.55945593 0.53678477 0.5181284 0.50073904 0.47882822 0.4426955 0.39063925 0.33704421 0.25833535][0.58229244 0.60919487 0.59860331 0.59256929 0.59596562 0.6018914 0.59900331 0.57789713 0.55233341 0.52289444 0.49004424 0.44492331 0.38675874 0.32923871 0.24970435][0.61734158 0.62662417 0.59503651 0.5695889 0.56203431 0.56838232 0.57062525 0.55418313 0.5279817 0.49334651 0.45287037 0.39870051 0.33339834 0.27240804 0.19802345][0.62348783 0.61356497 0.560044 0.51011264 0.48329383 0.48126906 0.4836891 0.47440347 0.45873633 0.43326396 0.39420873 0.33431971 0.26170564 0.19656569 0.12818785][0.59795851 0.57620144 0.50754297 0.43398607 0.38227955 0.36359748 0.36149684 0.36021328 0.36451888 0.36315164 0.33761322 0.27952078 0.20252815 0.13386363 0.069043905][0.53296721 0.50931579 0.43606552 0.3448109 0.26775381 0.22816078 0.21742433 0.22076033 0.24533693 0.27492297 0.2744 0.23182231 0.16233481 0.097954281 0.037767336][0.40998703 0.39282057 0.32836676 0.23541948 0.14565766 0.089419544 0.065293565 0.063165195 0.097381726 0.15349421 0.18478294 0.17303161 0.12860599 0.082062922 0.032201815][0.22921108 0.22247095 0.18059531 0.11000311 0.032628015 -0.024704333 -0.059803326 -0.074799329 -0.046019789 0.01949485 0.073996969 0.094656624 0.083568238 0.063311189 0.02871328][0.036625665 0.040411111 0.027018266 -0.0066338191 -0.052583963 -0.093795083 -0.12892182 -0.15436402 -0.14023072 -0.084504738 -0.026345434 0.011253744 0.024044272 0.025119269 0.0051953415][-0.10726324 -0.096620917 -0.087920025 -0.090786263 -0.10672496 -0.1273609 -0.15386365 -0.18138617 -0.18108337 -0.14485785 -0.099131681 -0.062352344 -0.040949725 -0.029334918 -0.038086176][-0.17217174 -0.1622989 -0.14516483 -0.13405283 -0.13325386 -0.13940866 -0.15551111 -0.17820963 -0.18587551 -0.16852997 -0.14032961 -0.11306139 -0.0930037 -0.079554632 -0.081126221]]...]
INFO - root - 2017-12-11 06:34:20.576113: step 21010, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:16m:08s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:34:25.984632: step 21020, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 45h:12m:52s remains)
INFO - root - 2017-12-11 06:34:31.355365: step 21030, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:00m:30s remains)
INFO - root - 2017-12-11 06:34:36.347665: step 21040, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:16m:43s remains)
INFO - root - 2017-12-11 06:34:41.686099: step 21050, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:07m:04s remains)
INFO - root - 2017-12-11 06:34:47.062876: step 21060, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 45h:54m:45s remains)
INFO - root - 2017-12-11 06:34:52.463582: step 21070, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 48h:44m:30s remains)
INFO - root - 2017-12-11 06:34:57.797260: step 21080, loss = 0.71, batch loss = 0.66 (15.2 examples/sec; 0.526 sec/batch; 45h:32m:32s remains)
INFO - root - 2017-12-11 06:35:03.206143: step 21090, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:09m:24s remains)
INFO - root - 2017-12-11 06:35:08.502441: step 21100, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 45h:03m:01s remains)
2017-12-11 06:35:09.047007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030451784 -0.043217126 -0.04165658 -0.026719263 4.0808678e-05 0.0335627 0.063528664 0.083151646 0.090075053 0.087520167 0.076577172 0.05720292 0.034898352 0.016210454 0.0078661982][-0.0048225177 -0.025534574 -0.022162927 0.0044266665 0.053188048 0.1144023 0.1691543 0.20372187 0.21511927 0.21093573 0.19328249 0.16183811 0.12351433 0.08779148 0.064920627][0.028056307 -0.0021383974 0.0037624361 0.045815077 0.12207235 0.21640266 0.30058119 0.35251924 0.36898166 0.36338252 0.33939779 0.29639027 0.24166459 0.18678762 0.14506745][0.063426 0.021047745 0.029205438 0.087608688 0.19133589 0.3168416 0.42766085 0.49492925 0.51623404 0.51094824 0.48468611 0.43572497 0.36907554 0.2957755 0.2306425][0.089873031 0.036716096 0.046383027 0.11794256 0.24280868 0.39084771 0.52004176 0.59760386 0.62261331 0.61959183 0.59558266 0.54761207 0.4758746 0.38830042 0.29897407][0.094246961 0.036511552 0.048861787 0.13120581 0.27208194 0.43628731 0.57802397 0.66246188 0.69023985 0.68982667 0.66924 0.62465006 0.55123192 0.45333537 0.3429271][0.079494908 0.026637102 0.047275957 0.142124 0.29657054 0.47208267 0.621474 0.71050334 0.74182731 0.74500459 0.72833592 0.68684363 0.6107527 0.50166351 0.37007165][0.059705827 0.018302415 0.048624042 0.15102914 0.30840689 0.48275045 0.62934953 0.71847188 0.75424016 0.76245344 0.75020134 0.71138459 0.632549 0.51365834 0.36494333][0.05421982 0.027744204 0.062696777 0.15863599 0.29870284 0.45028731 0.57613629 0.65526843 0.69299495 0.70754075 0.70282519 0.67122841 0.59718692 0.47965285 0.32902431][0.063705072 0.053064898 0.085782617 0.16170338 0.26827958 0.38242123 0.4775137 0.54135782 0.57883233 0.60009235 0.60512525 0.585396 0.52507657 0.42286816 0.28829873][0.075841859 0.080689356 0.10950612 0.16202366 0.23129353 0.30511183 0.36780012 0.41472626 0.44979587 0.47632226 0.490553 0.48322886 0.439934 0.35967982 0.25092131][0.086994268 0.10086782 0.12363398 0.15283161 0.18579844 0.21973322 0.24897547 0.27554551 0.30311304 0.33030981 0.35076973 0.35479283 0.32995498 0.27558613 0.19873394][0.10202802 0.11699348 0.12823369 0.13273987 0.13111044 0.12707274 0.12320034 0.12706231 0.1426781 0.16659792 0.19127843 0.20723255 0.20402576 0.18049417 0.14140055][0.11553881 0.12594154 0.12522879 0.11025498 0.083885364 0.053175006 0.025710313 0.011789556 0.016122475 0.035335228 0.062602542 0.089463614 0.10690837 0.11234418 0.10710958][0.11306998 0.11834557 0.11113838 0.088519558 0.054239556 0.015729968 -0.018509274 -0.038924977 -0.040805139 -0.026012477 0.00027413751 0.03143274 0.059853628 0.082028814 0.096437208]]...]
INFO - root - 2017-12-11 06:35:14.381937: step 21110, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:00m:24s remains)
INFO - root - 2017-12-11 06:35:19.818556: step 21120, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:07m:23s remains)
INFO - root - 2017-12-11 06:35:25.165618: step 21130, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.527 sec/batch; 45h:33m:44s remains)
INFO - root - 2017-12-11 06:35:30.237716: step 21140, loss = 0.69, batch loss = 0.63 (16.5 examples/sec; 0.486 sec/batch; 42h:03m:32s remains)
INFO - root - 2017-12-11 06:35:35.559178: step 21150, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 45h:07m:24s remains)
INFO - root - 2017-12-11 06:35:40.876334: step 21160, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:58m:41s remains)
INFO - root - 2017-12-11 06:35:46.312488: step 21170, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:24m:08s remains)
INFO - root - 2017-12-11 06:35:51.723995: step 21180, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 49h:14m:20s remains)
INFO - root - 2017-12-11 06:35:57.000009: step 21190, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 46h:05m:27s remains)
INFO - root - 2017-12-11 06:36:02.374217: step 21200, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:27m:31s remains)
2017-12-11 06:36:02.879657: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24320953 0.29148594 0.31976286 0.31867343 0.29144695 0.25461408 0.22804087 0.22039279 0.22761348 0.23260877 0.21629925 0.18260476 0.14879444 0.13521895 0.14914386][0.27513471 0.31941226 0.34307626 0.3394624 0.31247282 0.27839732 0.2547794 0.24981506 0.25811797 0.26102474 0.23907603 0.19950186 0.1633766 0.15137455 0.17000702][0.27269655 0.30444121 0.32069021 0.32008332 0.30848673 0.29823831 0.297482 0.3061249 0.31463462 0.30669188 0.27087587 0.22263554 0.18617769 0.17747223 0.19720383][0.24334553 0.25964993 0.27182549 0.28530493 0.30864045 0.34679207 0.39145836 0.42573366 0.43296552 0.40310133 0.34159878 0.27887219 0.2424759 0.23897907 0.25718597][0.18117251 0.18433808 0.2016077 0.24281263 0.31760538 0.42119962 0.52576762 0.59396136 0.60037589 0.54276347 0.4473736 0.36385792 0.32519558 0.32932168 0.35002205][0.087662421 0.084263705 0.11602709 0.19335072 0.32346645 0.4909912 0.65073061 0.74876159 0.75326735 0.67029887 0.54482192 0.44366872 0.40563363 0.42356324 0.45769563][-0.017365448 -0.021794114 0.028328691 0.13967115 0.31349212 0.52493829 0.71769434 0.82986772 0.8289867 0.73017603 0.59084785 0.48695061 0.45936313 0.49810195 0.55518925][-0.10442707 -0.10636385 -0.04369517 0.084872596 0.2740097 0.49419761 0.68793339 0.79512733 0.78824794 0.68945217 0.55891579 0.47089285 0.46471709 0.52990651 0.617169][-0.15172885 -0.15000556 -0.086906143 0.034415789 0.20436715 0.39546919 0.56015158 0.6490249 0.64226079 0.56342292 0.46491057 0.40928486 0.43155575 0.52357239 0.64102495][-0.15154563 -0.14699839 -0.095951222 -0.0019960329 0.12394536 0.26102346 0.3778365 0.44098213 0.43949831 0.39452142 0.34418702 0.33198112 0.38520002 0.49850002 0.63543171][-0.11482821 -0.11227641 -0.080931738 -0.022508057 0.053394068 0.13341881 0.20207508 0.24125327 0.24726388 0.23768063 0.2369002 0.26897132 0.3508071 0.4776485 0.61927795][-0.066131778 -0.069707446 -0.05803908 -0.030688128 0.0046003573 0.040145412 0.072110407 0.094094269 0.10637635 0.12358647 0.16071835 0.22739072 0.33068123 0.46415073 0.60076535][-0.024048524 -0.034284029 -0.036694713 -0.029958127 -0.021417283 -0.016201982 -0.011141946 -0.0034350129 0.010284562 0.042772979 0.10359957 0.19279869 0.30955896 0.44510552 0.57401383][0.0053386232 -0.0078293458 -0.01524656 -0.017154831 -0.022539414 -0.036919467 -0.053435169 -0.0621274 -0.055835016 -0.021504441 0.04886714 0.15038288 0.27537197 0.4115071 0.53377467][0.019530259 0.0097524719 0.0064337007 0.0071727945 0.00046395115 -0.021482171 -0.051454287 -0.075863376 -0.084028646 -0.060657755 0.0047658696 0.10745649 0.23544449 0.37159449 0.48996046]]...]
INFO - root - 2017-12-11 06:36:08.235643: step 21210, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:22m:23s remains)
INFO - root - 2017-12-11 06:36:13.652890: step 21220, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 45h:34m:40s remains)
INFO - root - 2017-12-11 06:36:18.964457: step 21230, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:36m:13s remains)
INFO - root - 2017-12-11 06:36:24.278345: step 21240, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:02m:49s remains)
INFO - root - 2017-12-11 06:36:29.304626: step 21250, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 46h:23m:23s remains)
INFO - root - 2017-12-11 06:36:34.614675: step 21260, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:40m:07s remains)
INFO - root - 2017-12-11 06:36:39.960900: step 21270, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.518 sec/batch; 44h:49m:21s remains)
INFO - root - 2017-12-11 06:36:45.239232: step 21280, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:28m:28s remains)
INFO - root - 2017-12-11 06:36:50.663342: step 21290, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:22m:29s remains)
INFO - root - 2017-12-11 06:36:56.084277: step 21300, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:54m:48s remains)
2017-12-11 06:36:56.609861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041607652 -0.034732953 -0.027128793 -0.022231264 -0.01837736 -0.016071556 -0.016100354 -0.016735615 -0.015522 -0.014398927 -0.016110642 -0.02070206 -0.027805962 -0.037381258 -0.048176397][-0.008078373 0.01149869 0.030065808 0.043280512 0.054121435 0.059416164 0.056812022 0.051596124 0.050599039 0.051322255 0.047709543 0.03911278 0.02546142 0.0050687613 -0.019287275][0.045785118 0.085047536 0.12022018 0.14581677 0.1666868 0.17614798 0.16995199 0.15931901 0.15626392 0.15726051 0.15158634 0.13726939 0.11356069 0.075709879 0.029330311][0.10781334 0.17254551 0.22787309 0.26698112 0.29762185 0.31082389 0.30077547 0.28562975 0.28192806 0.28337219 0.27358681 0.24936359 0.21124481 0.15167095 0.078806117][0.1630569 0.25413561 0.33006257 0.38171494 0.42038411 0.43719271 0.42580941 0.4099896 0.40868491 0.41070688 0.39406043 0.35558698 0.30002284 0.21869208 0.12087902][0.19193925 0.30368233 0.39712751 0.46022066 0.50585926 0.5264551 0.51714009 0.50520587 0.5094223 0.51343763 0.49071658 0.43828714 0.36615852 0.2664898 0.14905037][0.19234377 0.31407437 0.4186213 0.49135605 0.54237956 0.56486028 0.55704093 0.54843974 0.55635613 0.56123394 0.5346626 0.47254261 0.38823292 0.27675605 0.14949873][0.18155763 0.30476937 0.41385263 0.49346411 0.54731143 0.56776774 0.55660588 0.54441696 0.54831129 0.54861915 0.51848161 0.45223963 0.36363015 0.25074616 0.12608829][0.17044085 0.2902458 0.39810458 0.47905171 0.53066188 0.54458541 0.525447 0.50359243 0.4971554 0.48863968 0.4560312 0.39331052 0.31097308 0.20765807 0.095437281][0.16884269 0.28572795 0.38904434 0.46418545 0.50545245 0.50539351 0.47209573 0.43744725 0.42029777 0.40500203 0.37453976 0.32305565 0.25539663 0.16742164 0.070465937][0.16939433 0.28434289 0.38112655 0.44542927 0.47106963 0.45239997 0.40044057 0.351776 0.32654512 0.30910012 0.28503206 0.24799407 0.19809929 0.12726821 0.046308391][0.15145522 0.25851354 0.34456465 0.395825 0.40728924 0.37441775 0.30820224 0.25008249 0.22189766 0.20739032 0.19240819 0.16960728 0.13720222 0.083823949 0.019546159][0.093691409 0.1796471 0.24754603 0.28445277 0.28776541 0.25272614 0.18715383 0.13231042 0.1103903 0.10492878 0.10130972 0.0918834 0.075503781 0.039199527 -0.0088156173][0.0020178338 0.053615179 0.094676487 0.11434367 0.11370087 0.087341987 0.03937906 0.004149606 -0.00072473387 0.0081973029 0.016324509 0.017170867 0.012147574 -0.010283015 -0.043775126][-0.084502183 -0.066561475 -0.051893283 -0.048115909 -0.051085949 -0.06565737 -0.089711048 -0.099578895 -0.085606985 -0.064008884 -0.04796863 -0.04210303 -0.042624332 -0.05654069 -0.078448929]]...]
INFO - root - 2017-12-11 06:37:01.925059: step 21310, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 44h:59m:32s remains)
INFO - root - 2017-12-11 06:37:07.321664: step 21320, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:21m:57s remains)
INFO - root - 2017-12-11 06:37:12.644886: step 21330, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 44h:29m:38s remains)
INFO - root - 2017-12-11 06:37:18.001477: step 21340, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:19m:07s remains)
INFO - root - 2017-12-11 06:37:22.987299: step 21350, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 47h:08m:41s remains)
INFO - root - 2017-12-11 06:37:28.348112: step 21360, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 49h:37m:46s remains)
INFO - root - 2017-12-11 06:37:33.666591: step 21370, loss = 0.66, batch loss = 0.61 (15.6 examples/sec; 0.512 sec/batch; 44h:14m:00s remains)
INFO - root - 2017-12-11 06:37:38.982386: step 21380, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:01m:09s remains)
INFO - root - 2017-12-11 06:37:44.345797: step 21390, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 46h:12m:21s remains)
INFO - root - 2017-12-11 06:37:49.651636: step 21400, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:11m:32s remains)
2017-12-11 06:37:50.193654: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12175046 0.13210696 0.12632681 0.1063957 0.072547764 0.034302454 0.0067407126 -0.003561724 0.0026858908 0.017399648 0.039864261 0.067178272 0.089165971 0.096266896 0.083753996][0.15691946 0.16302073 0.14542222 0.11367234 0.073723368 0.036523912 0.013365845 0.0040342142 0.0086699948 0.021865385 0.044494327 0.072767258 0.091883473 0.091023639 0.068062112][0.20613863 0.21662344 0.19486716 0.15461254 0.10833813 0.070273057 0.047983374 0.034568369 0.029007249 0.02891314 0.037783541 0.053917121 0.062631063 0.0536803 0.02582938][0.2702629 0.2995705 0.28958496 0.25060549 0.19824412 0.15155216 0.11898064 0.090491414 0.063580133 0.037685763 0.019925259 0.013912442 0.0090168463 -0.0038753282 -0.028492548][0.33080006 0.39034352 0.40654847 0.38053149 0.32691038 0.26879948 0.21854195 0.16613035 0.10879679 0.048925448 -0.0016104965 -0.032423973 -0.048836343 -0.058751598 -0.07081838][0.370712 0.46351111 0.51361 0.51135361 0.466012 0.40140787 0.33291328 0.25248814 0.16046126 0.06566672 -0.014724061 -0.065518588 -0.088400781 -0.089730211 -0.082454383][0.38053164 0.50128132 0.58651519 0.61626518 0.58969927 0.52667248 0.4433023 0.3361676 0.21354578 0.092522465 -0.0064444584 -0.06768164 -0.0917756 -0.08345601 -0.057066761][0.35997492 0.496083 0.60844487 0.66918933 0.66514981 0.60980242 0.51869142 0.39462098 0.2552847 0.1243917 0.022680597 -0.037563067 -0.058329549 -0.042815082 -0.0033036806][0.32727 0.46312597 0.58511531 0.66255987 0.67308611 0.62538689 0.53466374 0.40871885 0.27063605 0.14628755 0.05453312 0.0031278231 -0.012346406 0.0067028124 0.052138019][0.30491561 0.42862946 0.54209161 0.61624014 0.62691766 0.58270806 0.49941289 0.38616562 0.26507166 0.15933475 0.084466405 0.043862108 0.031092089 0.047284547 0.089916214][0.29025698 0.39712819 0.4901163 0.54632246 0.54579473 0.50097436 0.42962039 0.33976507 0.24738605 0.16893578 0.11472097 0.083955742 0.0699416 0.077195317 0.10915041][0.26300538 0.34731022 0.41304988 0.4459883 0.43239328 0.388551 0.334278 0.27592212 0.22072145 0.17528471 0.14302647 0.12106834 0.10451453 0.10144864 0.11960386][0.20388907 0.25994062 0.29766232 0.31038883 0.29008541 0.2545689 0.22381113 0.20178184 0.18636622 0.17455412 0.16361654 0.15094244 0.13448071 0.12503371 0.13204129][0.11511301 0.14472976 0.16173218 0.16359323 0.14584965 0.12471006 0.11869945 0.13022643 0.15021722 0.16814506 0.17712376 0.17599134 0.16561833 0.1574398 0.16005762][0.017821588 0.028624734 0.035738382 0.036350776 0.026631974 0.018659376 0.028847368 0.060824137 0.10495809 0.14732634 0.17806377 0.19536403 0.20093513 0.20385717 0.21006805]]...]
INFO - root - 2017-12-11 06:37:55.452429: step 21410, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 44h:18m:44s remains)
INFO - root - 2017-12-11 06:38:00.794026: step 21420, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:53m:47s remains)
INFO - root - 2017-12-11 06:38:06.218744: step 21430, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.584 sec/batch; 50h:26m:35s remains)
INFO - root - 2017-12-11 06:38:11.642454: step 21440, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.541 sec/batch; 46h:43m:35s remains)
INFO - root - 2017-12-11 06:38:16.708075: step 21450, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 44h:35m:49s remains)
INFO - root - 2017-12-11 06:38:22.173196: step 21460, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 48h:44m:58s remains)
INFO - root - 2017-12-11 06:38:27.599418: step 21470, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:15m:24s remains)
INFO - root - 2017-12-11 06:38:32.972152: step 21480, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:00m:51s remains)
INFO - root - 2017-12-11 06:38:38.270418: step 21490, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:54m:12s remains)
INFO - root - 2017-12-11 06:38:43.603557: step 21500, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:40m:38s remains)
2017-12-11 06:38:44.122979: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32650071 0.30613983 0.29317844 0.29758638 0.31405032 0.33503792 0.35397694 0.34923145 0.30405536 0.22772671 0.13963455 0.05873283 -0.0031823122 -0.038352478 -0.052175652][0.33581853 0.31772897 0.30328232 0.30049959 0.30953225 0.32752442 0.34448376 0.33968019 0.29466438 0.21910731 0.13018177 0.047426451 -0.014463975 -0.049147815 -0.0620264][0.30227572 0.29479355 0.28790483 0.2868976 0.29062739 0.29913706 0.30817208 0.3026908 0.26164228 0.19106333 0.10916474 0.034203362 -0.021932768 -0.053873189 -0.065010406][0.257388 0.26502657 0.2723521 0.27910426 0.28215468 0.28494516 0.28781745 0.2808792 0.24285197 0.17839062 0.1051579 0.038279366 -0.013876645 -0.046143495 -0.058776796][0.20824106 0.22920011 0.25105563 0.26912478 0.27736768 0.28156379 0.28368878 0.27656466 0.24043466 0.18074171 0.1142619 0.052658606 0.0014991075 -0.033333559 -0.049088724][0.14107883 0.17095795 0.20649986 0.2401156 0.26213926 0.27724758 0.28585789 0.28201246 0.24844192 0.19238323 0.12977995 0.069798119 0.017053684 -0.02168086 -0.04134585][0.05651471 0.088984422 0.13471667 0.18407536 0.2255778 0.25964582 0.28173488 0.28467247 0.253745 0.19929178 0.13711105 0.076083176 0.021855485 -0.018760633 -0.040249534][-0.026113179 0.00070766069 0.048149034 0.10627344 0.16637857 0.22340342 0.26469073 0.27862781 0.25211719 0.1986565 0.13488375 0.0718836 0.017600102 -0.021782499 -0.042557333][-0.083460845 -0.068128385 -0.028322008 0.028943928 0.10228021 0.18171383 0.24475205 0.27301717 0.25422618 0.20383359 0.13878492 0.073039167 0.017611718 -0.021241723 -0.042079512][-0.0901236 -0.08744023 -0.061007388 -0.012772156 0.065189622 0.15936713 0.23899519 0.27940664 0.26849097 0.2220677 0.15553544 0.08560054 0.026630253 -0.01423943 -0.037200503][-0.04072994 -0.048889175 -0.038741235 -0.0064309407 0.065467507 0.16207556 0.24823527 0.29490677 0.28964734 0.24702807 0.17865068 0.10318845 0.038952902 -0.0051738666 -0.030708147][0.046537358 0.031951439 0.026234109 0.039276507 0.096560284 0.18374656 0.26611233 0.3122662 0.30939388 0.26947579 0.19898124 0.11710422 0.046569344 -0.00083653646 -0.027801741][0.1390508 0.12383695 0.10802889 0.10657737 0.14869225 0.22081068 0.29158691 0.32963049 0.32295045 0.28105134 0.20657843 0.1185333 0.04302052 -0.0055518346 -0.031358011][0.22012059 0.20902327 0.19044936 0.18215482 0.21180692 0.26691577 0.32136509 0.34568828 0.32929975 0.28040653 0.20098847 0.10918631 0.031795088 -0.015584935 -0.038564082][0.27267545 0.26760235 0.25086015 0.24043135 0.25948483 0.29846245 0.3377237 0.35026827 0.32569602 0.27082497 0.1892316 0.097868688 0.021540063 -0.024170747 -0.045234364]]...]
INFO - root - 2017-12-11 06:38:49.489318: step 21510, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 46h:31m:45s remains)
INFO - root - 2017-12-11 06:38:54.949359: step 21520, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:17m:34s remains)
INFO - root - 2017-12-11 06:39:00.322191: step 21530, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:01m:31s remains)
INFO - root - 2017-12-11 06:39:05.655431: step 21540, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:27m:07s remains)
INFO - root - 2017-12-11 06:39:10.963438: step 21550, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:15m:46s remains)
INFO - root - 2017-12-11 06:39:15.988380: step 21560, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:34m:46s remains)
INFO - root - 2017-12-11 06:39:21.265612: step 21570, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 44h:59m:12s remains)
INFO - root - 2017-12-11 06:39:26.612241: step 21580, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:19m:01s remains)
INFO - root - 2017-12-11 06:39:31.907696: step 21590, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:13m:01s remains)
INFO - root - 2017-12-11 06:39:37.268503: step 21600, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 45h:32m:14s remains)
2017-12-11 06:39:37.807532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12651774 -0.12801093 -0.12443858 -0.1170268 -0.10608768 -0.095309637 -0.08858414 -0.089018345 -0.097389311 -0.1110093 -0.12487209 -0.13406388 -0.13686493 -0.13389397 -0.12820625][-0.1389637 -0.13963749 -0.12956564 -0.10999643 -0.082412735 -0.054657951 -0.03565513 -0.03292967 -0.047511376 -0.074357808 -0.10415906 -0.12803239 -0.14159884 -0.14499825 -0.1422759][-0.1432151 -0.14131609 -0.12224894 -0.085159674 -0.031862702 0.025185125 0.069584645 0.084714465 0.066560321 0.02172846 -0.034408066 -0.085364491 -0.1209197 -0.13934523 -0.14508817][-0.14178586 -0.136919 -0.10817908 -0.050922737 0.035753585 0.13685794 0.22722985 0.27397668 0.26329145 0.19906709 0.10330488 0.0061319 -0.069851078 -0.11666282 -0.13873605][-0.1343307 -0.1273867 -0.091028139 -0.014790888 0.10853136 0.26424617 0.41801062 0.51483089 0.52479488 0.44458756 0.3015973 0.14384688 0.011620446 -0.076017536 -0.12205251][-0.12407029 -0.11836698 -0.079465754 0.0094299512 0.16418625 0.37367117 0.59513748 0.74936765 0.78758544 0.697539 0.51065743 0.29284713 0.10292883 -0.027451441 -0.09892901][-0.11518726 -0.11504079 -0.080596335 0.01164613 0.18608002 0.43647292 0.71365565 0.91834438 0.98504382 0.89266968 0.6747579 0.41082662 0.17558134 0.012384278 -0.077234417][-0.1102327 -0.11814062 -0.093535632 -0.0072532734 0.1705572 0.43729609 0.74012178 0.97067606 1.055356 0.96781754 0.74083084 0.45794454 0.20200643 0.025466761 -0.067400716][-0.11010346 -0.12673075 -0.11413822 -0.041174937 0.12145732 0.37172535 0.65699565 0.874149 0.953642 0.87167996 0.65977228 0.395275 0.15640946 -0.0039007952 -0.078866415][-0.11644753 -0.13954969 -0.13761368 -0.081444271 0.052148648 0.25764287 0.48575637 0.65154314 0.70144629 0.62104219 0.44093713 0.2255443 0.039088946 -0.074992336 -0.11087056][-0.12764451 -0.15293574 -0.15813835 -0.11917118 -0.02115253 0.12606411 0.27947223 0.37845731 0.3899692 0.31104487 0.17082249 0.017775802 -0.10154729 -0.15732756 -0.14641188][-0.13886781 -0.16264367 -0.1721652 -0.15023597 -0.088992208 -0.00048888969 0.08181189 0.12134741 0.10292383 0.031881753 -0.066398941 -0.15829062 -0.21452552 -0.2178569 -0.16685733][-0.14673136 -0.16708523 -0.17958617 -0.17399047 -0.14649352 -0.10788456 -0.0807863 -0.082668662 -0.11605608 -0.17171204 -0.2308701 -0.27196449 -0.27875251 -0.24315163 -0.16442776][-0.15043823 -0.16669513 -0.18046033 -0.18714891 -0.18457943 -0.17989714 -0.18563703 -0.20726268 -0.24122937 -0.27812603 -0.30561343 -0.31164008 -0.28802347 -0.23099132 -0.13892142][-0.14332069 -0.15692009 -0.17148235 -0.18626539 -0.19889341 -0.21243012 -0.23160164 -0.2556347 -0.27990034 -0.29767707 -0.30210602 -0.28803423 -0.25137189 -0.18856724 -0.095368654]]...]
INFO - root - 2017-12-11 06:39:43.135574: step 21610, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:47m:50s remains)
INFO - root - 2017-12-11 06:39:48.569087: step 21620, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:16m:07s remains)
INFO - root - 2017-12-11 06:39:53.896905: step 21630, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:24m:43s remains)
INFO - root - 2017-12-11 06:39:59.341846: step 21640, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:40m:32s remains)
INFO - root - 2017-12-11 06:40:04.668800: step 21650, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:42m:36s remains)
INFO - root - 2017-12-11 06:40:09.641780: step 21660, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:32m:42s remains)
INFO - root - 2017-12-11 06:40:15.057405: step 21670, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:10m:40s remains)
INFO - root - 2017-12-11 06:40:20.365323: step 21680, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 46h:29m:41s remains)
INFO - root - 2017-12-11 06:40:25.649080: step 21690, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:03m:42s remains)
INFO - root - 2017-12-11 06:40:31.045871: step 21700, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:16m:15s remains)
2017-12-11 06:40:31.647011: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35751137 0.35265103 0.33976215 0.32390693 0.30509877 0.30071163 0.31424519 0.3372784 0.34713849 0.33632952 0.30938408 0.25774252 0.18606085 0.10627554 0.035520669][0.40404591 0.41260427 0.41148648 0.40200782 0.38484055 0.37976661 0.39478293 0.42216542 0.43522373 0.42301127 0.38884097 0.32451192 0.23568609 0.13769825 0.051248424][0.409501 0.4364363 0.45075911 0.44982535 0.43738264 0.43384334 0.45143485 0.48349923 0.50123525 0.48986518 0.44970688 0.37446174 0.27126327 0.15829173 0.059665095][0.37692556 0.42325947 0.45400715 0.46336374 0.46000439 0.46284163 0.48716316 0.52860308 0.55687445 0.55179346 0.50796276 0.42171633 0.30297023 0.17338543 0.062161211][0.33827615 0.39883485 0.44051191 0.45530874 0.45712471 0.46258408 0.4896282 0.53784066 0.57743275 0.5831328 0.54158366 0.45068756 0.32208842 0.18045299 0.059972446][0.33005318 0.3904815 0.42928609 0.4378525 0.43473923 0.43503934 0.45851773 0.50765491 0.555264 0.57219923 0.53807425 0.45069247 0.32166156 0.17701843 0.053690959][0.34523964 0.39229688 0.41624454 0.41233802 0.40060458 0.3949191 0.41594929 0.4663797 0.52056056 0.54584074 0.51862848 0.43653923 0.31066096 0.16750215 0.0446263][0.35614857 0.38522968 0.39311054 0.37960413 0.36238509 0.35500622 0.3779833 0.43317756 0.49420497 0.52475578 0.501475 0.42229423 0.29817152 0.15622936 0.033902779][0.36095363 0.37489685 0.37180519 0.35335204 0.33422366 0.32818058 0.35328209 0.41078982 0.47395831 0.5050047 0.48243207 0.40423125 0.28162205 0.14225245 0.022354616][0.36173275 0.36918655 0.3612054 0.34075734 0.32253462 0.31987229 0.34460053 0.39669663 0.45243114 0.47705635 0.45146817 0.37307021 0.25426346 0.12236588 0.010290573][0.34501156 0.35639235 0.35044569 0.33242628 0.31975082 0.32466695 0.3505671 0.39521906 0.43998209 0.45484623 0.42278484 0.34093195 0.22450247 0.10052542 -0.0018367844][0.30094144 0.32095867 0.32209668 0.31067935 0.30645454 0.31968996 0.34777388 0.38763162 0.42498237 0.43285504 0.39530694 0.30987763 0.1944856 0.076612033 -0.017164094][0.23163176 0.25804585 0.26574418 0.25968143 0.26048017 0.27648848 0.30324581 0.3389034 0.37230393 0.37797853 0.34046969 0.25764382 0.14842218 0.040183444 -0.042447042][0.13908432 0.16546333 0.17410776 0.16791554 0.16714107 0.17894258 0.19925642 0.22752488 0.25557464 0.26110396 0.22984003 0.15954942 0.068149939 -0.019237805 -0.081695057][0.03359282 0.051623013 0.054389246 0.04204388 0.03300108 0.034119625 0.043344267 0.061316587 0.082406238 0.089122824 0.069850378 0.02191077 -0.039782245 -0.094892643 -0.12849328]]...]
INFO - root - 2017-12-11 06:40:36.925814: step 21710, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 45h:47m:12s remains)
INFO - root - 2017-12-11 06:40:42.279282: step 21720, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:32m:14s remains)
INFO - root - 2017-12-11 06:40:47.631293: step 21730, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:15m:58s remains)
INFO - root - 2017-12-11 06:40:52.948639: step 21740, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:50m:17s remains)
INFO - root - 2017-12-11 06:40:58.311659: step 21750, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:24m:51s remains)
INFO - root - 2017-12-11 06:41:03.360762: step 21760, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:31m:06s remains)
INFO - root - 2017-12-11 06:41:08.766200: step 21770, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 47h:23m:56s remains)
INFO - root - 2017-12-11 06:41:14.071302: step 21780, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:38m:18s remains)
INFO - root - 2017-12-11 06:41:19.420651: step 21790, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:29m:34s remains)
INFO - root - 2017-12-11 06:41:24.874256: step 21800, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 44h:19m:05s remains)
2017-12-11 06:41:25.415812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.092395753 -0.095997915 -0.095447637 -0.09112066 -0.081724979 -0.068324104 -0.05065947 -0.038118307 -0.034318011 -0.031857777 -0.02649188 -0.013857806 0.0098156212 0.040101595 0.067227535][-0.096441567 -0.10004112 -0.097622909 -0.087491028 -0.067849442 -0.041475262 -0.01080266 0.011260611 0.017012468 0.012349716 0.0047504692 0.0038371671 0.017500233 0.042469617 0.068575487][-0.091728292 -0.091653183 -0.083732054 -0.064124092 -0.030332854 0.013348 0.060882892 0.094564155 0.1021921 0.087487459 0.060744725 0.03811818 0.03343818 0.046544131 0.067361966][-0.080380067 -0.071656056 -0.052030135 -0.015989861 0.038827375 0.10595556 0.17450877 0.22118184 0.22877976 0.19875661 0.14516695 0.092271455 0.061402116 0.056693148 0.068487249][-0.06235731 -0.039173935 -0.00073261262 0.059392251 0.14159229 0.23658235 0.32744643 0.38546968 0.38883591 0.33769047 0.25102985 0.16204225 0.099862553 0.073150009 0.072826184][-0.041118152 0.000164917 0.062974811 0.15287538 0.26565328 0.3882058 0.49717355 0.56012112 0.55278713 0.47631857 0.35549113 0.23107123 0.13853745 0.090318136 0.0779266][-0.022626847 0.037127923 0.12604782 0.24696073 0.38901088 0.53394496 0.6526674 0.71172768 0.68751419 0.58484477 0.4347789 0.28271917 0.1672603 0.1027399 0.081204876][-0.017201904 0.054991029 0.16357002 0.30900246 0.47377956 0.63334161 0.75365824 0.802744 0.76000768 0.63559562 0.46660882 0.30042875 0.17543747 0.10495052 0.080508508][-0.033583574 0.037228212 0.14993958 0.30401504 0.47763532 0.64043194 0.75532919 0.7933498 0.73814291 0.60470086 0.43395296 0.27235314 0.1555174 0.092805758 0.073961608][-0.066666812 -0.012454247 0.084219031 0.22398643 0.38614166 0.53754485 0.64071196 0.66969854 0.61245179 0.48718587 0.33512196 0.19860086 0.10790758 0.066553988 0.061537433][-0.096700296 -0.068932846 -0.0047155726 0.098847538 0.22795039 0.3518008 0.43618754 0.45864806 0.41021809 0.30948377 0.19318897 0.097079426 0.044575024 0.033031464 0.046670284][-0.10408235 -0.10294893 -0.078037143 -0.023323994 0.057043292 0.13908659 0.19537574 0.20885979 0.17387025 0.10703861 0.036058519 -0.012033904 -0.021408547 -0.00042482378 0.032888155][-0.077536568 -0.09690538 -0.10793736 -0.10191078 -0.074526511 -0.040760919 -0.018853718 -0.01923413 -0.043070316 -0.0767006 -0.10349557 -0.10621796 -0.076250322 -0.026572535 0.023426984][-0.020077446 -0.051015023 -0.087510765 -0.11963212 -0.13816349 -0.14864159 -0.15894672 -0.17350489 -0.19044346 -0.19929428 -0.19284718 -0.16259122 -0.10589507 -0.038068231 0.021463376][0.049305126 0.016073186 -0.032199092 -0.086550452 -0.1367299 -0.18035753 -0.21519144 -0.24066103 -0.25395346 -0.24778324 -0.22159065 -0.17394665 -0.10604635 -0.033000633 0.027598016]]...]
INFO - root - 2017-12-11 06:41:30.714631: step 21810, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 44h:48m:44s remains)
INFO - root - 2017-12-11 06:41:36.049849: step 21820, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:20m:27s remains)
INFO - root - 2017-12-11 06:41:41.311966: step 21830, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:17m:24s remains)
INFO - root - 2017-12-11 06:41:46.727155: step 21840, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:56m:11s remains)
INFO - root - 2017-12-11 06:41:52.114853: step 21850, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 45h:51m:37s remains)
INFO - root - 2017-12-11 06:41:57.497784: step 21860, loss = 0.68, batch loss = 0.62 (18.2 examples/sec; 0.440 sec/batch; 37h:59m:05s remains)
INFO - root - 2017-12-11 06:42:02.730240: step 21870, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:30m:05s remains)
INFO - root - 2017-12-11 06:42:08.184277: step 21880, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:58m:22s remains)
INFO - root - 2017-12-11 06:42:13.530155: step 21890, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.521 sec/batch; 44h:57m:26s remains)
INFO - root - 2017-12-11 06:42:18.838392: step 21900, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 45h:31m:07s remains)
2017-12-11 06:42:19.395943: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076850079 0.058582157 0.033616945 0.020477505 0.023853064 0.032558758 0.04083696 0.044845063 0.044443358 0.041370835 0.038488511 0.035049174 0.022394208 -0.00083206751 -0.029209407][0.2003569 0.17021266 0.12547636 0.10208803 0.10996616 0.12986219 0.14961463 0.16071174 0.16147739 0.15510733 0.14709891 0.13694909 0.10968015 0.06348037 0.0097222906][0.34277794 0.30341682 0.24058683 0.20832695 0.22131395 0.25203332 0.28261331 0.2997812 0.30093017 0.28981525 0.27382758 0.25380933 0.20851915 0.13493644 0.051105242][0.48153281 0.44065133 0.3696841 0.33499062 0.35314259 0.39066786 0.42537472 0.44012848 0.432809 0.40938088 0.38006416 0.34730604 0.28478876 0.18708095 0.077066593][0.61201811 0.5834713 0.51943868 0.49068663 0.51255906 0.550772 0.58052784 0.58039832 0.550032 0.50063568 0.44837344 0.39695197 0.31763449 0.20246184 0.076757483][0.73209429 0.72996968 0.68811363 0.67200273 0.69408047 0.72610593 0.74303138 0.71841192 0.65459245 0.57055646 0.49051133 0.417135 0.3209461 0.19284913 0.059863437][0.81320024 0.84406728 0.83252078 0.83269018 0.85397768 0.87755686 0.87882453 0.82690328 0.72886652 0.6130808 0.51046807 0.41932163 0.31017855 0.17476822 0.040735841][0.82393849 0.87972307 0.89320844 0.90640622 0.92690581 0.94545656 0.93723506 0.86722833 0.74663275 0.61211497 0.49862331 0.39896753 0.28535068 0.15166631 0.02414206][0.75413567 0.81918365 0.84487164 0.86377221 0.88358891 0.90256679 0.89555317 0.82496011 0.70212513 0.56727552 0.45571411 0.35679764 0.24637429 0.12188288 0.0076413578][0.60623294 0.6656177 0.69087952 0.71005422 0.73092526 0.75460476 0.75787938 0.70517343 0.6037845 0.48977315 0.39355838 0.30418646 0.2027445 0.09096723 -0.0078959968][0.39823112 0.43620682 0.45211309 0.46901527 0.49338996 0.52424061 0.54239517 0.51865792 0.45442778 0.37605569 0.30571732 0.234868 0.14989294 0.055493694 -0.026350817][0.16663405 0.17867622 0.18184091 0.19573335 0.22354804 0.25951791 0.28933117 0.29215011 0.26397049 0.22064057 0.1767233 0.12818006 0.066535965 -0.0027747958 -0.061296094][-0.024934301 -0.033786327 -0.040975809 -0.031151094 -0.0051025948 0.028224241 0.060027756 0.077532336 0.073004551 0.05401494 0.030164365 0.002012084 -0.034800131 -0.075634345 -0.10687833][-0.13785124 -0.15787272 -0.16856232 -0.16128214 -0.13967842 -0.11378062 -0.087803423 -0.068088904 -0.062901273 -0.06963753 -0.08145716 -0.095182568 -0.11293677 -0.13135561 -0.14106955][-0.18139875 -0.20358703 -0.21350272 -0.20780358 -0.19116391 -0.17302375 -0.15509276 -0.13987347 -0.1333441 -0.13511162 -0.1403082 -0.1452111 -0.15097119 -0.15562844 -0.15324856]]...]
INFO - root - 2017-12-11 06:42:24.685679: step 21910, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 45h:24m:25s remains)
INFO - root - 2017-12-11 06:42:30.060062: step 21920, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:53m:23s remains)
INFO - root - 2017-12-11 06:42:35.460774: step 21930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:37m:26s remains)
INFO - root - 2017-12-11 06:42:40.841319: step 21940, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:02m:46s remains)
INFO - root - 2017-12-11 06:42:46.087341: step 21950, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:38m:44s remains)
INFO - root - 2017-12-11 06:42:51.438350: step 21960, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:24m:54s remains)
INFO - root - 2017-12-11 06:42:56.547440: step 21970, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 44h:15m:16s remains)
INFO - root - 2017-12-11 06:43:01.910538: step 21980, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:46m:17s remains)
INFO - root - 2017-12-11 06:43:07.199281: step 21990, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.518 sec/batch; 44h:38m:54s remains)
INFO - root - 2017-12-11 06:43:12.515043: step 22000, loss = 0.65, batch loss = 0.59 (15.4 examples/sec; 0.520 sec/batch; 44h:50m:19s remains)
2017-12-11 06:43:13.062661: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039400287 -0.014275901 0.023790607 0.064466283 0.097844295 0.11632204 0.11644372 0.10062302 0.073989078 0.046086248 0.023804653 0.011428101 0.011787456 0.026119381 0.051522564][-0.024947312 0.015402551 0.074573785 0.13869445 0.19438861 0.23138772 0.24269941 0.22821611 0.19151008 0.14643489 0.10459994 0.075727828 0.067260288 0.0828692 0.11880272][-0.0067538912 0.048711147 0.12968348 0.21984354 0.30382705 0.36824825 0.40088269 0.39706495 0.35529372 0.29060864 0.21864429 0.15959607 0.13153897 0.14294268 0.188236][0.0049475711 0.072279118 0.17142299 0.28596303 0.40047872 0.49766392 0.55833524 0.571829 0.5298323 0.446962 0.34108675 0.24368119 0.18720721 0.18764295 0.23739064][0.0059570009 0.078115113 0.18810745 0.32133394 0.46375707 0.593813 0.68337089 0.71455485 0.67302674 0.57485992 0.43986818 0.30736005 0.22223845 0.20869996 0.25722349][-0.0047991336 0.064486735 0.17675239 0.32076162 0.48499417 0.64508474 0.76325279 0.81246859 0.772784 0.66345119 0.50755578 0.3501423 0.24326549 0.21593699 0.25736934][-0.020247865 0.042278484 0.15126687 0.29965279 0.47895682 0.66282094 0.80643505 0.87392986 0.84004235 0.72749549 0.56330365 0.39704645 0.28104806 0.24209011 0.26882321][-0.036296844 0.01831324 0.12070951 0.2685152 0.45428574 0.648711 0.80524516 0.88517094 0.86099911 0.75757986 0.60364312 0.44982949 0.34112209 0.29491481 0.30083266][-0.052096866 -0.0062964023 0.087372206 0.2302597 0.41188723 0.60033047 0.752986 0.83545047 0.82188088 0.73649806 0.60822487 0.48328513 0.39422685 0.34593695 0.33119509][-0.068167731 -0.033420954 0.048941005 0.18286914 0.35340646 0.52571404 0.66339517 0.74108213 0.73716664 0.67180854 0.57170367 0.47757506 0.41035587 0.3633838 0.33271694][-0.086423732 -0.064446412 0.0025238039 0.1213252 0.27489915 0.42528743 0.54195213 0.61084312 0.61606181 0.56987321 0.49360982 0.4240151 0.37573302 0.33405152 0.29618013][-0.10647742 -0.099166386 -0.052544985 0.042103931 0.16870676 0.2888639 0.37806398 0.43211007 0.44075778 0.408319 0.35024574 0.29915845 0.26856345 0.24049065 0.21044147][-0.12829611 -0.13618626 -0.1132497 -0.050595555 0.039539341 0.12313268 0.18133213 0.21615943 0.22145933 0.19659878 0.15210681 0.11568674 0.10095491 0.090682819 0.078228123][-0.14530997 -0.1659254 -0.1637086 -0.13206546 -0.078387327 -0.029187966 0.0020026711 0.019064443 0.018908737 -0.0012586442 -0.034026522 -0.058283914 -0.062361937 -0.059617087 -0.056488261][-0.15401874 -0.18080674 -0.19147791 -0.18213508 -0.15636338 -0.13220289 -0.11872111 -0.1132946 -0.11724218 -0.13229492 -0.15305355 -0.16601314 -0.16351539 -0.15356059 -0.14027596]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:43:18.301515: step 22010, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 44h:51m:29s remains)
INFO - root - 2017-12-11 06:43:23.652427: step 22020, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 48h:47m:01s remains)
INFO - root - 2017-12-11 06:43:28.954639: step 22030, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:22m:44s remains)
INFO - root - 2017-12-11 06:43:34.267155: step 22040, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 46h:51m:27s remains)
INFO - root - 2017-12-11 06:43:39.605210: step 22050, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 46h:04m:14s remains)
INFO - root - 2017-12-11 06:43:44.979904: step 22060, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:09m:46s remains)
INFO - root - 2017-12-11 06:43:50.062944: step 22070, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:04m:47s remains)
INFO - root - 2017-12-11 06:43:55.468669: step 22080, loss = 0.66, batch loss = 0.60 (15.1 examples/sec; 0.529 sec/batch; 45h:34m:56s remains)
INFO - root - 2017-12-11 06:44:00.830115: step 22090, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 45h:11m:05s remains)
INFO - root - 2017-12-11 06:44:06.118103: step 22100, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 46h:34m:35s remains)
2017-12-11 06:44:06.689457: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11194256 0.13264057 0.13890281 0.13462543 0.12645453 0.12309428 0.13229717 0.15412879 0.17969529 0.191779 0.16765992 0.11003867 0.039450914 -0.022683425 -0.065536045][0.24496527 0.2659474 0.26382053 0.251628 0.23907986 0.23780522 0.25473055 0.2858083 0.31655264 0.32499886 0.28061739 0.1898834 0.085165717 -0.0033066217 -0.062899619][0.41583785 0.42584878 0.40428787 0.37769175 0.3599593 0.36514845 0.39460477 0.43635139 0.4695853 0.46946046 0.39908776 0.27077016 0.13058466 0.017432587 -0.055971835][0.6054464 0.58917928 0.54022431 0.49528188 0.47268558 0.48726103 0.53042716 0.57956719 0.60775346 0.59080696 0.49039522 0.32642874 0.15692633 0.02618921 -0.054856312][0.76881176 0.72243351 0.64806849 0.59120327 0.5732311 0.60584182 0.66599154 0.71866238 0.73139191 0.68512672 0.54871744 0.35268781 0.16288556 0.0234245 -0.059055775][0.86070478 0.78873 0.69450843 0.634449 0.63279545 0.69522536 0.78152138 0.842828 0.83986032 0.76022732 0.58570206 0.36027363 0.15521568 0.01180568 -0.068598576][0.87705177 0.78877419 0.68313175 0.62389773 0.64008009 0.73110574 0.844064 0.91902411 0.90898937 0.80560756 0.60232973 0.3549768 0.13878758 -0.0069602514 -0.083836034][0.819572 0.73538482 0.63703549 0.587334 0.61900437 0.72551465 0.850482 0.93115616 0.91643637 0.79917067 0.58251166 0.32814991 0.11096351 -0.032312166 -0.1031185][0.68325061 0.61954886 0.54977864 0.52314091 0.5714407 0.68230826 0.80388665 0.87896454 0.85731 0.7324242 0.51646322 0.27241975 0.068749145 -0.063471608 -0.12417044][0.49672806 0.45995748 0.42044014 0.41592085 0.47320259 0.57656103 0.68349445 0.7479192 0.72330558 0.60291815 0.40411597 0.18703674 0.0109516 -0.10014831 -0.145839][0.32068187 0.29295114 0.26744306 0.27027068 0.32174268 0.40470803 0.48794129 0.53860843 0.51695919 0.41560221 0.25075048 0.074802816 -0.063264474 -0.14439051 -0.16929628][0.14654504 0.11494092 0.092233278 0.095710985 0.13764729 0.19967665 0.25872925 0.29373091 0.27549475 0.19922774 0.077621855 -0.048656888 -0.14200434 -0.18814851 -0.18957347][-0.038756814 -0.079851292 -0.1020976 -0.09667138 -0.058467738 -0.0075560194 0.036886849 0.061833292 0.048600376 -0.0024832382 -0.080275804 -0.15586424 -0.20393698 -0.21649309 -0.19771573][-0.20231666 -0.24782187 -0.2709443 -0.26684907 -0.23367439 -0.18895058 -0.14937666 -0.12468027 -0.12671329 -0.15156516 -0.18951315 -0.22161885 -0.23297721 -0.22108585 -0.19072315][-0.2971057 -0.34018794 -0.36210006 -0.3626973 -0.34019381 -0.30594307 -0.27203763 -0.24637046 -0.23673953 -0.2390698 -0.24620648 -0.24736312 -0.23501597 -0.20966323 -0.17571686]]...]
INFO - root - 2017-12-11 06:44:12.086575: step 22110, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:37m:51s remains)
INFO - root - 2017-12-11 06:44:17.387930: step 22120, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 45h:01m:57s remains)
INFO - root - 2017-12-11 06:44:22.736843: step 22130, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:58m:53s remains)
INFO - root - 2017-12-11 06:44:28.081279: step 22140, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:25m:01s remains)
INFO - root - 2017-12-11 06:44:33.475481: step 22150, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 46h:05m:35s remains)
INFO - root - 2017-12-11 06:44:38.809063: step 22160, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 45h:59m:40s remains)
INFO - root - 2017-12-11 06:44:43.799661: step 22170, loss = 0.68, batch loss = 0.63 (20.5 examples/sec; 0.389 sec/batch; 33h:34m:16s remains)
INFO - root - 2017-12-11 06:44:49.160645: step 22180, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 46h:30m:18s remains)
INFO - root - 2017-12-11 06:44:54.506836: step 22190, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:57m:18s remains)
INFO - root - 2017-12-11 06:44:59.848948: step 22200, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 46h:04m:52s remains)
2017-12-11 06:45:00.389763: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1821854 0.2017502 0.21825409 0.21148758 0.17729338 0.13470872 0.097970419 0.071729861 0.062417522 0.08409474 0.1433707 0.22864741 0.32781628 0.422711 0.48805258][0.27447787 0.30303282 0.32578102 0.32029518 0.27787855 0.22105011 0.16921507 0.13049182 0.1127376 0.13361506 0.19793741 0.28878036 0.39176258 0.48611394 0.54527688][0.36366233 0.4050132 0.43681228 0.43723571 0.39268771 0.32513079 0.2583352 0.20612906 0.17907573 0.19619843 0.25674471 0.33928585 0.42944103 0.50711334 0.54773921][0.43005607 0.48271364 0.52457017 0.53488 0.49646693 0.42656767 0.35101765 0.28986847 0.25705117 0.268862 0.3165597 0.3775157 0.44189015 0.4940469 0.51354104][0.46869266 0.53102064 0.58060241 0.60235649 0.57815504 0.518352 0.44854349 0.39297169 0.36599863 0.375116 0.40249732 0.42907605 0.45445442 0.47229806 0.46935329][0.49737746 0.56865615 0.62096232 0.65173107 0.64481777 0.6030584 0.54986191 0.5115875 0.49961776 0.50790143 0.50964832 0.49399 0.4728348 0.45202267 0.42641222][0.53146362 0.60302055 0.648361 0.67954338 0.68690383 0.66586167 0.63364178 0.61560243 0.61785316 0.62137687 0.59471852 0.53823024 0.47488517 0.42093801 0.37797019][0.55745208 0.61476588 0.63996875 0.65831828 0.671054 0.66842031 0.65761489 0.65809351 0.66876537 0.66305953 0.61206353 0.52642548 0.4345288 0.35888898 0.30794543][0.54647666 0.57902384 0.57594144 0.57113975 0.57715029 0.58447295 0.5915035 0.60852069 0.626558 0.61715782 0.55733818 0.46177 0.35733825 0.27012563 0.21697648][0.48700184 0.49607429 0.46800905 0.44001395 0.43079183 0.43545511 0.4487333 0.4740594 0.4967064 0.48977467 0.43578848 0.34774464 0.24715294 0.16071476 0.1115516][0.3795217 0.37317351 0.32966334 0.28393415 0.25686684 0.25055081 0.26021504 0.28523156 0.30765182 0.30415326 0.26245144 0.19201657 0.10860802 0.036779489 0.00084629824][0.2270997 0.21383253 0.16725563 0.11466425 0.07807181 0.06626872 0.075661622 0.10155647 0.12434126 0.12613882 0.099627577 0.050824784 -0.0094930595 -0.060054392 -0.079172917][0.071471132 0.053873751 0.013491892 -0.032940466 -0.065161578 -0.072438933 -0.058471058 -0.030677237 -0.0064391051 0.0029479896 -0.0071915705 -0.034746476 -0.07288973 -0.10366468 -0.10997887][-0.040771876 -0.06143339 -0.091193654 -0.12323814 -0.14269422 -0.1416554 -0.12392364 -0.096800081 -0.072093219 -0.056065887 -0.053623788 -0.066054188 -0.088357531 -0.10553782 -0.10568467][-0.10125307 -0.11987513 -0.13861224 -0.15520579 -0.16151685 -0.15391359 -0.13538057 -0.11108854 -0.087696433 -0.069156229 -0.061165422 -0.066355824 -0.079878449 -0.089514367 -0.087803416]]...]
INFO - root - 2017-12-11 06:45:05.779828: step 22210, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 45h:01m:20s remains)
INFO - root - 2017-12-11 06:45:11.128233: step 22220, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:51m:54s remains)
INFO - root - 2017-12-11 06:45:16.582969: step 22230, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 44h:57m:55s remains)
INFO - root - 2017-12-11 06:45:21.886321: step 22240, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:53m:37s remains)
INFO - root - 2017-12-11 06:45:27.233056: step 22250, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:41m:59s remains)
INFO - root - 2017-12-11 06:45:32.596184: step 22260, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:02m:37s remains)
INFO - root - 2017-12-11 06:45:38.001527: step 22270, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:14m:07s remains)
INFO - root - 2017-12-11 06:45:42.985913: step 22280, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:00m:29s remains)
INFO - root - 2017-12-11 06:45:48.273087: step 22290, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 45h:33m:57s remains)
INFO - root - 2017-12-11 06:45:53.658198: step 22300, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 44h:19m:00s remains)
2017-12-11 06:45:54.226133: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053451244 0.1009777 0.14588907 0.17796907 0.19020177 0.18693592 0.1732318 0.1592131 0.15085863 0.15534618 0.16797154 0.1773068 0.18037394 0.18059614 0.188486][0.082755439 0.14845771 0.21157734 0.25790966 0.27777576 0.27764443 0.26254448 0.24448571 0.23124819 0.23257789 0.24432564 0.25231278 0.25417635 0.25537741 0.27003059][0.10515644 0.18704529 0.26662332 0.32629693 0.35444406 0.36051145 0.34863329 0.33043981 0.31468984 0.31290936 0.32184839 0.3243646 0.3195627 0.31625921 0.33238879][0.11749252 0.21305208 0.30770126 0.38066164 0.41895735 0.43483207 0.43058395 0.41528812 0.39874178 0.39386526 0.39821005 0.391561 0.37500954 0.36192337 0.37325883][0.1202092 0.22709908 0.33513948 0.42001691 0.4684684 0.49522659 0.49948755 0.48642033 0.46837798 0.46049297 0.46060282 0.44545487 0.4171578 0.3932569 0.39554146][0.11216289 0.22618833 0.34395882 0.43757516 0.49433023 0.52994776 0.53974062 0.52574533 0.5054487 0.49714348 0.49727738 0.47954917 0.44575423 0.41515833 0.40662545][0.093353622 0.20907719 0.33147395 0.42962894 0.49185869 0.533595 0.54608607 0.529261 0.5072751 0.5026539 0.509363 0.49786195 0.46826258 0.43827581 0.42105794][0.066636465 0.17887336 0.30045426 0.39925972 0.46482527 0.51001644 0.52232957 0.50108689 0.47704825 0.4775992 0.49475735 0.49637625 0.47991768 0.4594481 0.44200164][0.029103685 0.13060784 0.24352787 0.3368628 0.40191337 0.447384 0.45772916 0.43325672 0.40882137 0.41458467 0.4414072 0.45772102 0.459077 0.45466036 0.44596547][-0.016449219 0.065792285 0.16036434 0.23971504 0.29815376 0.34046346 0.34905949 0.32555839 0.30495536 0.31641829 0.34973577 0.37685776 0.39294577 0.40199918 0.40219447][-0.0600265 -0.0022974855 0.067376256 0.12677754 0.17321441 0.20876256 0.21628697 0.19806178 0.18472584 0.20083617 0.23487172 0.26361781 0.28407794 0.29709992 0.30060923][-0.093302161 -0.060145441 -0.016172873 0.022423614 0.054725956 0.080931276 0.086444169 0.073119655 0.06437964 0.078738682 0.10449923 0.12386522 0.13704696 0.14516999 0.1478564][-0.11237684 -0.099704474 -0.077838354 -0.057324782 -0.038315196 -0.022251835 -0.020516532 -0.032199528 -0.041456118 -0.035975683 -0.025523251 -0.021523684 -0.020666279 -0.01999443 -0.018372549][-0.1159906 -0.11756786 -0.11228263 -0.10624556 -0.099386051 -0.093760364 -0.097173378 -0.10934455 -0.12080398 -0.12454989 -0.1271181 -0.13400665 -0.14092015 -0.14431061 -0.14248995][-0.10713096 -0.11654335 -0.12159746 -0.12557667 -0.12823424 -0.13129044 -0.1391528 -0.15136054 -0.16259116 -0.16967015 -0.1763729 -0.18471009 -0.19145831 -0.19403997 -0.19111452]]...]
INFO - root - 2017-12-11 06:45:59.549828: step 22310, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:35m:37s remains)
INFO - root - 2017-12-11 06:46:04.954644: step 22320, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 46h:44m:08s remains)
INFO - root - 2017-12-11 06:46:10.318274: step 22330, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:42m:40s remains)
INFO - root - 2017-12-11 06:46:15.683638: step 22340, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 46h:18m:01s remains)
INFO - root - 2017-12-11 06:46:20.992498: step 22350, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:58m:56s remains)
INFO - root - 2017-12-11 06:46:26.517804: step 22360, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 49h:28m:22s remains)
INFO - root - 2017-12-11 06:46:31.880698: step 22370, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:36m:24s remains)
INFO - root - 2017-12-11 06:46:36.965859: step 22380, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 46h:12m:08s remains)
INFO - root - 2017-12-11 06:46:42.388947: step 22390, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:34m:42s remains)
INFO - root - 2017-12-11 06:46:47.682898: step 22400, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:45m:41s remains)
2017-12-11 06:46:48.274326: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0067085726 0.034592673 0.066690326 0.09536425 0.11366867 0.11904065 0.11532091 0.10614776 0.09454713 0.08281555 0.074714728 0.0753451 0.0839522 0.095852911 0.10564838][0.026487932 0.064443804 0.10729524 0.14511333 0.16955376 0.17739198 0.17382151 0.16288315 0.14819175 0.13238773 0.1207279 0.12241299 0.13809782 0.16115791 0.18284135][0.044889886 0.090120994 0.14116696 0.18705291 0.21986438 0.23626468 0.24123898 0.23617887 0.22331889 0.2047856 0.18785731 0.18715681 0.20656416 0.23954181 0.27438766][0.053786226 0.10356157 0.16105033 0.215457 0.26062191 0.29342818 0.31653687 0.32550272 0.31950065 0.29984084 0.27589491 0.26801291 0.28479671 0.32247847 0.36742032][0.05077466 0.102511 0.16575691 0.2310766 0.29448661 0.35261112 0.4026213 0.431713 0.43546811 0.41465095 0.38131967 0.3604286 0.36654055 0.40110734 0.45009008][0.037699893 0.0903943 0.16036008 0.23994742 0.32647982 0.41516927 0.49553767 0.54538417 0.55683953 0.53157496 0.48611447 0.44766572 0.4350144 0.45691395 0.50118524][0.023598909 0.079061493 0.15838288 0.2548525 0.36477575 0.48087698 0.58496988 0.6473192 0.65859276 0.62446219 0.56672364 0.51159078 0.47798541 0.48137698 0.51308268][0.013557435 0.074330628 0.16453068 0.27658474 0.403345 0.53481525 0.6471957 0.70780033 0.70888209 0.66349518 0.5982675 0.53577834 0.49086094 0.48034602 0.49832711][0.00638913 0.071865588 0.16979383 0.2903834 0.4220359 0.55232221 0.65548158 0.70152491 0.6867305 0.63189632 0.56687891 0.51058745 0.47168171 0.46091041 0.47222731][-0.0030338441 0.06218845 0.16049993 0.27936536 0.40338224 0.51873469 0.60203677 0.62979841 0.60195315 0.54444486 0.4873935 0.4477635 0.42816886 0.42912847 0.44154546][-0.017052369 0.04129139 0.13171443 0.23926093 0.34612808 0.43897367 0.50059676 0.51434809 0.48140341 0.42955634 0.38619247 0.36827737 0.37386373 0.3925848 0.4107267][-0.036030155 0.0081857 0.08188802 0.16986203 0.2541371 0.32346767 0.3677693 0.37493154 0.34497771 0.30391493 0.27509964 0.27725524 0.30775887 0.34671369 0.3748351][-0.058121983 -0.032482438 0.018451791 0.081718937 0.14138502 0.18905436 0.22069651 0.22587498 0.2026979 0.17187828 0.15256466 0.16593546 0.21217777 0.26630044 0.30428702][-0.076560877 -0.067787409 -0.039238203 -0.00061935239 0.035786998 0.064230338 0.084212333 0.086481728 0.068766348 0.045381088 0.030757505 0.045520946 0.092873663 0.14887762 0.18934686][-0.0884213 -0.091151051 -0.080087438 -0.061781071 -0.044345047 -0.031463966 -0.022548905 -0.025065733 -0.040210873 -0.058638368 -0.070472479 -0.060131002 -0.023563758 0.021152107 0.054669224]]...]
INFO - root - 2017-12-11 06:46:53.600182: step 22410, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:38m:07s remains)
INFO - root - 2017-12-11 06:46:58.911671: step 22420, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:31m:27s remains)
INFO - root - 2017-12-11 06:47:04.228617: step 22430, loss = 0.66, batch loss = 0.60 (14.4 examples/sec; 0.556 sec/batch; 47h:50m:50s remains)
INFO - root - 2017-12-11 06:47:09.669554: step 22440, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:32m:44s remains)
INFO - root - 2017-12-11 06:47:15.116805: step 22450, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 46h:42m:27s remains)
INFO - root - 2017-12-11 06:47:20.543872: step 22460, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 44h:56m:34s remains)
INFO - root - 2017-12-11 06:47:25.930093: step 22470, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:20m:06s remains)
INFO - root - 2017-12-11 06:47:30.933109: step 22480, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:38m:21s remains)
INFO - root - 2017-12-11 06:47:36.362600: step 22490, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:05m:24s remains)
INFO - root - 2017-12-11 06:47:41.709830: step 22500, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.538 sec/batch; 46h:17m:05s remains)
2017-12-11 06:47:42.244092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030483132 -0.031757005 -0.030486079 -0.028248303 -0.027022142 -0.02880522 -0.032270968 -0.031943358 -0.020757547 0.0045749629 0.040170331 0.075572059 0.10654223 0.13118131 0.15279883][-0.044569362 -0.045090284 -0.040889941 -0.033983279 -0.026364818 -0.019662442 -0.013388631 -0.0039700186 0.013819505 0.041516468 0.074007466 0.10217066 0.12332117 0.13839658 0.15463364][-0.049790721 -0.048968196 -0.041250728 -0.028138028 -0.011683907 0.0056297933 0.022748379 0.041525561 0.06430991 0.090044364 0.11296938 0.12611328 0.12974584 0.12895313 0.1358805][-0.047064371 -0.045216389 -0.034917764 -0.016626835 0.0077465116 0.034513116 0.060659975 0.0867441 0.11295975 0.13570333 0.14837846 0.14627405 0.13291918 0.11712592 0.11504774][-0.04419715 -0.042314574 -0.031056339 -0.0097460179 0.019931743 0.053145122 0.085188724 0.11610366 0.14534776 0.16792475 0.17649317 0.16707517 0.14467658 0.11949506 0.10952412][-0.04481696 -0.043763816 -0.032119084 -0.008929017 0.024190191 0.061478764 0.096896954 0.13062344 0.16293894 0.18930301 0.20177881 0.1948556 0.17264903 0.14439248 0.12755673][-0.045685623 -0.045448225 -0.032865863 -0.0073093036 0.029585004 0.071263939 0.11030699 0.14665374 0.18198347 0.2138488 0.23410062 0.23453262 0.21680856 0.18805973 0.16400252][-0.045518469 -0.045265831 -0.029985823 0.00044207767 0.044440463 0.094615504 0.14140707 0.18292488 0.22139131 0.25724775 0.2827619 0.28755024 0.2712931 0.23970281 0.20680198][-0.0451653 -0.044475712 -0.024978433 0.012910477 0.068194985 0.13290891 0.19454214 0.24701363 0.290938 0.32889405 0.35402095 0.35544822 0.33294266 0.29304761 0.2491402][-0.04547793 -0.044749696 -0.020708557 0.026092958 0.095679753 0.17986976 0.2632854 0.33361074 0.38724077 0.42692289 0.44687736 0.43766966 0.40101072 0.34696993 0.29015687][-0.045819551 -0.046048891 -0.019527329 0.033802781 0.11510998 0.21664646 0.32192424 0.41251478 0.47939986 0.52326429 0.539255 0.5200581 0.46887636 0.40026721 0.33155286][-0.04157741 -0.044006266 -0.019451356 0.033505954 0.11703771 0.22473668 0.34190658 0.44682872 0.52627885 0.5774948 0.5961262 0.57622164 0.51999795 0.44480312 0.3701365][-0.027252108 -0.03300235 -0.015485963 0.028137835 0.10031144 0.19696294 0.3087602 0.41490933 0.50102288 0.56091642 0.59009576 0.58222342 0.53545159 0.46694189 0.39625558][0.0011203156 -0.007754555 -0.00057036593 0.026460916 0.075452626 0.14534357 0.23455736 0.32681775 0.40996546 0.475164 0.51748246 0.52925515 0.50348186 0.45448193 0.39914712][0.041338459 0.0310187 0.027502816 0.034856584 0.054395419 0.08863204 0.14381167 0.20997454 0.2790893 0.34110859 0.39169776 0.42286694 0.42379126 0.40414661 0.37509689]]...]
INFO - root - 2017-12-11 06:47:47.676276: step 22510, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:35m:00s remains)
INFO - root - 2017-12-11 06:47:53.054184: step 22520, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:48m:45s remains)
INFO - root - 2017-12-11 06:47:58.339632: step 22530, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 44h:55m:00s remains)
INFO - root - 2017-12-11 06:48:03.647478: step 22540, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 47h:36m:28s remains)
INFO - root - 2017-12-11 06:48:09.015465: step 22550, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:17m:18s remains)
INFO - root - 2017-12-11 06:48:14.356465: step 22560, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 45h:23m:34s remains)
INFO - root - 2017-12-11 06:48:19.712143: step 22570, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 45h:08m:56s remains)
INFO - root - 2017-12-11 06:48:24.855364: step 22580, loss = 0.68, batch loss = 0.62 (23.0 examples/sec; 0.347 sec/batch; 29h:53m:27s remains)
INFO - root - 2017-12-11 06:48:30.053355: step 22590, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 45h:00m:55s remains)
INFO - root - 2017-12-11 06:48:35.480792: step 22600, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 45h:28m:09s remains)
2017-12-11 06:48:36.071293: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13817056 0.16213448 0.21120557 0.28582406 0.37234938 0.44920149 0.49046469 0.4752903 0.40043136 0.28538984 0.16222118 0.060245708 -0.0072041783 -0.040412553 -0.051863551][0.21181631 0.21234837 0.21656351 0.23565926 0.26967341 0.30935279 0.33517233 0.32490656 0.27002585 0.18135758 0.083825357 0.002654461 -0.047999613 -0.068660349 -0.069878891][0.27925387 0.25975314 0.22762467 0.20198429 0.19258718 0.19889127 0.20611081 0.19353211 0.15048636 0.082327843 0.0073338933 -0.054545697 -0.090167895 -0.098650716 -0.089581028][0.30900446 0.2816543 0.24127159 0.21073973 0.20009877 0.20684464 0.21408695 0.20038551 0.1555289 0.084983237 0.0067734951 -0.059230465 -0.09887331 -0.10985629 -0.1009733][0.28476107 0.26083827 0.23750941 0.23971511 0.27064848 0.31629911 0.35052192 0.34811598 0.29885146 0.21067521 0.10586624 0.010179345 -0.0567462 -0.089509211 -0.095460236][0.2197933 0.20368205 0.20785463 0.257974 0.34968385 0.45557031 0.53720683 0.56285918 0.51881182 0.41329297 0.27376083 0.13496296 0.026184503 -0.041675594 -0.073314063][0.14923482 0.1431559 0.16947468 0.25376967 0.38934717 0.5430972 0.66930497 0.73076028 0.70655262 0.59964764 0.43837693 0.26427373 0.11670653 0.013766678 -0.044979852][0.11521584 0.1194511 0.15478268 0.24330251 0.38148397 0.54127467 0.68007088 0.76258194 0.76236624 0.67373359 0.51821452 0.33605123 0.17153092 0.048999492 -0.026337288][0.13607281 0.15244101 0.1863635 0.25317395 0.35413402 0.47286868 0.57949662 0.64975274 0.65944117 0.59632093 0.47018284 0.31116682 0.15984172 0.042375248 -0.03112423][0.20213002 0.23310952 0.26243207 0.29791889 0.34363961 0.39574018 0.44050214 0.46890736 0.46558985 0.41740763 0.3237333 0.20220801 0.084352158 -0.0073142168 -0.061726909][0.2890732 0.33005393 0.35552213 0.3669067 0.36774459 0.36131424 0.34559077 0.32320446 0.2897931 0.2369502 0.15999693 0.068261027 -0.015665086 -0.075855941 -0.10452547][0.36596367 0.40850446 0.43119264 0.43561727 0.42267734 0.39374819 0.34778389 0.28930449 0.22157677 0.14543268 0.061323557 -0.022361405 -0.088579349 -0.12770404 -0.13738081][0.40970731 0.44355431 0.46299508 0.47720674 0.48498422 0.47869191 0.44685322 0.38491875 0.29577747 0.18891111 0.07667166 -0.025140367 -0.098820031 -0.1379582 -0.14474189][0.40973505 0.42674181 0.44026944 0.47105527 0.51836413 0.56462294 0.58306658 0.55294979 0.46801639 0.34052089 0.19417374 0.057083577 -0.045184687 -0.1040329 -0.12368628][0.3751227 0.37419859 0.37803584 0.41866183 0.49880281 0.59590584 0.6722458 0.69197547 0.63425636 0.50696194 0.33969042 0.1716065 0.038591821 -0.045592446 -0.084466413]]...]
INFO - root - 2017-12-11 06:48:41.451768: step 22610, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:42m:54s remains)
INFO - root - 2017-12-11 06:48:46.649016: step 22620, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:35m:47s remains)
INFO - root - 2017-12-11 06:48:52.076061: step 22630, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:10m:33s remains)
INFO - root - 2017-12-11 06:48:57.424779: step 22640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:13m:10s remains)
INFO - root - 2017-12-11 06:49:02.751858: step 22650, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 44h:42m:39s remains)
INFO - root - 2017-12-11 06:49:08.045232: step 22660, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 44h:43m:16s remains)
INFO - root - 2017-12-11 06:49:13.441390: step 22670, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:51m:50s remains)
INFO - root - 2017-12-11 06:49:18.862254: step 22680, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 47h:48m:47s remains)
INFO - root - 2017-12-11 06:49:23.978158: step 22690, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 46h:51m:13s remains)
INFO - root - 2017-12-11 06:49:29.318883: step 22700, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 45h:26m:24s remains)
2017-12-11 06:49:29.891200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.046104472 -0.017266197 0.020846359 0.065915346 0.11588042 0.16849235 0.22051017 0.2636427 0.28753749 0.28039172 0.24203996 0.18320546 0.12816484 0.0993487 0.10675324][-0.025731523 0.019066453 0.076354228 0.14169776 0.21225123 0.286599 0.36070195 0.42160127 0.45538118 0.44728041 0.39740121 0.31937417 0.24801119 0.21572371 0.23475675][-0.0012076569 0.060595021 0.13954465 0.22832133 0.32171309 0.41680714 0.50692606 0.57505471 0.60653013 0.58833557 0.52389187 0.43210816 0.35660863 0.33558047 0.37761831][0.017489182 0.095280886 0.19816434 0.31627047 0.43972206 0.55860543 0.6592927 0.72127366 0.73274237 0.68838835 0.60058463 0.49661246 0.42596844 0.42645377 0.49793059][0.029243836 0.12054651 0.24746184 0.39822811 0.55684942 0.70271903 0.81243438 0.86142021 0.84221137 0.76142251 0.64455378 0.52848876 0.46412238 0.48373491 0.577791][0.034967471 0.13645029 0.28262571 0.46024862 0.64880258 0.81891406 0.93865269 0.97920424 0.93580204 0.82572049 0.68526578 0.55943739 0.49729568 0.524149 0.62336791][0.038874604 0.14872462 0.30838618 0.50289738 0.710203 0.89713395 1.0279453 1.0712706 1.0226792 0.90320355 0.75171673 0.61815715 0.54969358 0.567866 0.6545592][0.036894687 0.14961511 0.31315547 0.51187712 0.72431314 0.91835994 1.0597595 1.1170228 1.0839605 0.97738856 0.83062863 0.69353557 0.61176807 0.6088056 0.67013723][0.02768567 0.13390277 0.28663588 0.46991861 0.66480184 0.84449184 0.98272824 1.053302 1.0489415 0.97682649 0.85720313 0.73261017 0.64519119 0.62319583 0.65818948][0.0077401586 0.097037539 0.22352543 0.37112 0.52420753 0.66503996 0.78016615 0.85327721 0.87750995 0.85056353 0.77646768 0.68387663 0.60701615 0.57881057 0.59805185][-0.025852906 0.038821045 0.12918536 0.23022501 0.33026576 0.42062074 0.49975842 0.56156474 0.60189551 0.61266941 0.58438426 0.53014994 0.473959 0.45047751 0.46626467][-0.071233176 -0.03643892 0.013994901 0.067862965 0.11719888 0.15894881 0.1985739 0.23757252 0.27540877 0.3035374 0.30624828 0.28513497 0.25366685 0.2421504 0.26111051][-0.11618049 -0.11097737 -0.097514041 -0.084355012 -0.076400727 -0.07427565 -0.06961365 -0.056866404 -0.033891305 -0.006613858 0.010341131 0.01244681 0.004258296 0.006639454 0.029159088][-0.14340092 -0.15845215 -0.17003094 -0.18367669 -0.20216331 -0.22475615 -0.24334104 -0.25050893 -0.24375504 -0.22586623 -0.20735554 -0.19464089 -0.18856348 -0.17807065 -0.15620756][-0.15022297 -0.17381763 -0.19612816 -0.22060117 -0.2482655 -0.27768925 -0.3026652 -0.31712049 -0.31943527 -0.31095448 -0.29797444 -0.28646338 -0.27865002 -0.26968178 -0.25430328]]...]
INFO - root - 2017-12-11 06:49:35.244634: step 22710, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 46h:04m:01s remains)
INFO - root - 2017-12-11 06:49:40.636833: step 22720, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 47h:40m:20s remains)
INFO - root - 2017-12-11 06:49:45.976805: step 22730, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 45h:28m:03s remains)
INFO - root - 2017-12-11 06:49:51.290151: step 22740, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 45h:51m:48s remains)
INFO - root - 2017-12-11 06:49:56.784926: step 22750, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:32m:23s remains)
INFO - root - 2017-12-11 06:50:02.204946: step 22760, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 45h:48m:14s remains)
INFO - root - 2017-12-11 06:50:07.517303: step 22770, loss = 0.72, batch loss = 0.66 (15.7 examples/sec; 0.510 sec/batch; 43h:53m:21s remains)
INFO - root - 2017-12-11 06:50:12.807346: step 22780, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 44h:45m:04s remains)
INFO - root - 2017-12-11 06:50:17.881199: step 22790, loss = 0.66, batch loss = 0.60 (14.7 examples/sec; 0.543 sec/batch; 46h:45m:05s remains)
INFO - root - 2017-12-11 06:50:23.298599: step 22800, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:07m:39s remains)
2017-12-11 06:50:23.850610: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0032096198 -0.01458672 -0.027719593 -0.027633987 -0.010893759 0.020776194 0.056470372 0.0833404 0.094250575 0.089491948 0.0764455 0.060474437 0.045326181 0.031422991 0.02438188][0.037633631 0.014837166 0.00029533196 0.0044169649 0.028064361 0.065824993 0.10106076 0.11880406 0.11573132 0.09745314 0.07622347 0.057744659 0.044593792 0.034587458 0.02950318][0.072920322 0.057059869 0.055298962 0.075411886 0.11329061 0.15873633 0.19170144 0.1962118 0.17277983 0.13427658 0.098842777 0.07231833 0.055617057 0.044708852 0.038586956][0.099305712 0.10620006 0.13497898 0.18678556 0.24978034 0.30757189 0.33990943 0.3320457 0.28603709 0.22228572 0.16479935 0.12052851 0.088839173 0.065955132 0.051838357][0.11933077 0.16227348 0.23787057 0.33675867 0.43544343 0.51065779 0.54475415 0.52564245 0.45464137 0.35994589 0.27254027 0.20088615 0.14241435 0.096137427 0.066996321][0.13359831 0.21660115 0.34522635 0.49618667 0.63280034 0.72716153 0.76619053 0.74102962 0.65092182 0.52938342 0.41182619 0.30761367 0.21329853 0.13370641 0.082262047][0.14435934 0.25965884 0.43199971 0.62511134 0.79109037 0.90170395 0.94932836 0.92791057 0.83362538 0.69850236 0.55743396 0.42121717 0.2889632 0.17287932 0.095740713][0.15371019 0.28440484 0.47864217 0.692554 0.87243485 0.99268633 1.0510216 1.0424256 0.95784795 0.82151306 0.66561711 0.50465542 0.34266305 0.19838858 0.10189071][0.1512939 0.27724209 0.4654167 0.6728074 0.84767985 0.96794927 1.035482 1.0438311 0.97749937 0.85072935 0.69312638 0.52377975 0.350808 0.19757056 0.096656196][0.12513305 0.22527978 0.37794849 0.54980725 0.69934672 0.80852956 0.88113737 0.90719211 0.86634833 0.76290447 0.62367797 0.46948478 0.31021306 0.17203078 0.084411807][0.0792681 0.13825788 0.23446211 0.35012722 0.45902029 0.54650569 0.616001 0.65543556 0.64319742 0.57546741 0.47346309 0.35603169 0.23258252 0.12975207 0.069472566][0.030491166 0.046465378 0.085440524 0.14397362 0.2087591 0.26789069 0.32358181 0.36509645 0.37419775 0.34347311 0.28690314 0.2178372 0.14307475 0.086061195 0.059003893][-0.0069719013 -0.022098104 -0.023617325 -0.0076443562 0.019740131 0.049933102 0.085038468 0.11828507 0.13809332 0.1385355 0.1255233 0.10369904 0.075758547 0.059304014 0.058074009][-0.027027467 -0.053844117 -0.073070094 -0.081390336 -0.080559544 -0.073562756 -0.057584651 -0.035664141 -0.012536334 0.010542167 0.031810533 0.045771778 0.050715692 0.059280783 0.070418283][-0.024995245 -0.044774145 -0.061710563 -0.077570714 -0.091611758 -0.099832229 -0.098419361 -0.087101705 -0.065343723 -0.030925561 0.0091384351 0.041572493 0.061830863 0.079516679 0.092137463]]...]
INFO - root - 2017-12-11 06:50:29.158343: step 22810, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 44h:34m:25s remains)
INFO - root - 2017-12-11 06:50:34.530637: step 22820, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:24m:22s remains)
INFO - root - 2017-12-11 06:50:39.909314: step 22830, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 46h:23m:41s remains)
INFO - root - 2017-12-11 06:50:45.240714: step 22840, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 44h:06m:03s remains)
INFO - root - 2017-12-11 06:50:50.611814: step 22850, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 47h:12m:22s remains)
INFO - root - 2017-12-11 06:50:55.909337: step 22860, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:11m:18s remains)
INFO - root - 2017-12-11 06:51:01.236288: step 22870, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.523 sec/batch; 44h:57m:49s remains)
INFO - root - 2017-12-11 06:51:06.582101: step 22880, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 45h:15m:24s remains)
INFO - root - 2017-12-11 06:51:11.617519: step 22890, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:30m:09s remains)
INFO - root - 2017-12-11 06:51:16.959813: step 22900, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.534 sec/batch; 45h:52m:57s remains)
2017-12-11 06:51:17.530877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013513891 -0.0091576809 -0.0010613299 0.0087365415 0.018801529 0.0271864 0.031492241 0.0320218 0.030936921 0.030656213 0.031237554 0.031319149 0.029713159 0.025892248 0.020140309][0.0034914706 0.011327717 0.023336461 0.038911767 0.0567865 0.072924182 0.0814355 0.080877155 0.073965073 0.064885087 0.055311251 0.0461143 0.0386661 0.033066731 0.028335711][0.039338268 0.053397559 0.071201757 0.0949128 0.12415301 0.15282972 0.16957375 0.16945343 0.15496725 0.13237296 0.10536043 0.078549661 0.058429867 0.047118288 0.042415291][0.097681969 0.12435429 0.15537928 0.19497025 0.24247657 0.2900978 0.32009348 0.32193965 0.29720211 0.25472993 0.20089585 0.14591318 0.10366613 0.080118731 0.071927905][0.17772886 0.22700439 0.28279728 0.34909895 0.42288414 0.49540424 0.54343325 0.54897285 0.5122577 0.44410387 0.354391 0.2610178 0.18611045 0.1411421 0.12209634][0.27298033 0.35025966 0.43646035 0.53316653 0.63388461 0.73084843 0.79862773 0.81025839 0.76342821 0.66938621 0.5419426 0.40682 0.29304561 0.21896845 0.18165804][0.36475489 0.46657705 0.5789668 0.69953573 0.8181982 0.93004179 1.0116242 1.0287615 0.9758476 0.8624838 0.7054134 0.53560597 0.38595316 0.28172687 0.22358863][0.41365188 0.52788556 0.65396672 0.78503048 0.90761966 1.0201229 1.1039244 1.1219075 1.0662932 0.94509661 0.776115 0.59066832 0.42143762 0.29894531 0.228446][0.39170918 0.49750972 0.61614484 0.7376138 0.846772 0.94453055 1.0185798 1.0335847 0.98032939 0.86484438 0.70426571 0.52778828 0.36506411 0.24788091 0.1834209][0.30611774 0.38206893 0.46874553 0.55756992 0.63548052 0.70563823 0.76159084 0.77291006 0.7291652 0.63293678 0.49921829 0.35436732 0.22341871 0.13449715 0.092985108][0.19229376 0.22783864 0.26954615 0.31266436 0.34947193 0.38547444 0.41896322 0.42582315 0.39467779 0.32558724 0.23110464 0.13346256 0.050496511 0.002641205 -0.0078045889][0.096913271 0.099113271 0.10194121 0.10526992 0.10744908 0.11483652 0.12769103 0.12948081 0.10981335 0.0686139 0.015699107 -0.033172313 -0.068462811 -0.078821413 -0.064205423][0.042405535 0.026297266 0.0070147165 -0.011321284 -0.026161844 -0.032888658 -0.03206123 -0.03406556 -0.045988992 -0.065254353 -0.08458434 -0.095832154 -0.097256668 -0.084505841 -0.057917792][0.030359732 0.011348689 -0.012170312 -0.035287272 -0.053918391 -0.065250017 -0.069896877 -0.073221788 -0.078900419 -0.083222523 -0.082089759 -0.073453851 -0.060604751 -0.042680196 -0.01870559][0.0329823 0.022222254 0.0069751856 -0.010284356 -0.025977019 -0.037494529 -0.044092439 -0.047101282 -0.047816012 -0.043719474 -0.033935815 -0.020391488 -0.0083102332 0.0020924674 0.013453793]]...]
INFO - root - 2017-12-11 06:51:22.818360: step 22910, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 48h:00m:39s remains)
INFO - root - 2017-12-11 06:51:28.277085: step 22920, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 49h:47m:48s remains)
INFO - root - 2017-12-11 06:51:33.728057: step 22930, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 48h:48m:13s remains)
INFO - root - 2017-12-11 06:51:39.133417: step 22940, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:04m:58s remains)
INFO - root - 2017-12-11 06:51:44.488590: step 22950, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.509 sec/batch; 43h:45m:12s remains)
INFO - root - 2017-12-11 06:51:49.858235: step 22960, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 45h:05m:34s remains)
INFO - root - 2017-12-11 06:51:55.245250: step 22970, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:12m:16s remains)
INFO - root - 2017-12-11 06:52:00.571689: step 22980, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:14m:10s remains)
INFO - root - 2017-12-11 06:52:05.862534: step 22990, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:30m:48s remains)
INFO - root - 2017-12-11 06:52:10.995500: step 23000, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 44h:43m:46s remains)
2017-12-11 06:52:11.583540: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2686294 0.24631089 0.23421919 0.24613117 0.271139 0.29971015 0.33289313 0.36268529 0.37638709 0.38320106 0.39601886 0.41604385 0.42964822 0.41645303 0.38029236][0.31365979 0.28027621 0.25755727 0.26496309 0.2934213 0.33077574 0.37566367 0.41509768 0.43181667 0.43292496 0.4373678 0.4530085 0.47067183 0.4687624 0.44610229][0.33963531 0.29142717 0.25384346 0.25169685 0.27875641 0.32220957 0.37753528 0.42704788 0.44904053 0.444522 0.43552256 0.43667459 0.44748586 0.45185035 0.44453266][0.35887781 0.30643752 0.26188362 0.25286931 0.27503383 0.31845057 0.37796113 0.43339866 0.46025115 0.45203659 0.42946225 0.4111509 0.40522814 0.40368021 0.40086347][0.36842573 0.33232266 0.29797456 0.29072157 0.30687028 0.34255493 0.39523387 0.44539937 0.46902189 0.45490047 0.42001462 0.38398436 0.35914302 0.34332761 0.33339089][0.37757242 0.37495226 0.36696512 0.37236056 0.38693196 0.41147956 0.44785026 0.47963351 0.48684749 0.45969331 0.41356575 0.36689955 0.33025336 0.30149418 0.280206][0.38337481 0.41626352 0.44050485 0.46595281 0.48635772 0.502722 0.52032524 0.52723157 0.50876522 0.46214119 0.40410927 0.35181507 0.31133589 0.27673975 0.2489537][0.37612379 0.43132833 0.48049477 0.52445436 0.55433816 0.56768531 0.57059044 0.55539751 0.51151353 0.44416708 0.37376043 0.31816491 0.28028223 0.24943282 0.22538002][0.35623834 0.41530502 0.47154617 0.52214211 0.55685169 0.5687151 0.56220132 0.53244895 0.47077546 0.38816839 0.30854121 0.25196913 0.21976794 0.19804932 0.18422824][0.33704749 0.38295206 0.42739698 0.46858639 0.49880049 0.50754923 0.49522957 0.458188 0.38964814 0.3028146 0.22240771 0.16814557 0.14015132 0.12433299 0.11675914][0.32366833 0.35172579 0.3737618 0.39312795 0.40996492 0.41442427 0.40153775 0.36636618 0.3034296 0.22349241 0.14783631 0.093374245 0.062039815 0.043557178 0.034345772][0.32423481 0.34181812 0.34375161 0.3379465 0.33562574 0.33314541 0.32321009 0.29855204 0.25310722 0.19062854 0.12414052 0.066627584 0.025138056 -0.0032162629 -0.019210748][0.34423664 0.35815984 0.3475624 0.32236549 0.30273709 0.29307517 0.28803268 0.27799207 0.25465798 0.21375242 0.15901169 0.10107606 0.052807108 0.01895218 0.0015749742][0.36716723 0.37908694 0.35892189 0.31883839 0.28494063 0.26928389 0.27087221 0.2780408 0.27912277 0.26324117 0.22646871 0.17879871 0.13598728 0.10736512 0.097378165][0.37104782 0.38132444 0.35240406 0.29923347 0.25177363 0.22858433 0.23394868 0.25594878 0.27935213 0.28905568 0.27585751 0.24918197 0.22351876 0.20922624 0.21256073]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 06:52:17.004051: step 23010, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:03m:55s remains)
INFO - root - 2017-12-11 06:52:22.435432: step 23020, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 44h:37m:51s remains)
INFO - root - 2017-12-11 06:52:27.873391: step 23030, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:37m:19s remains)
INFO - root - 2017-12-11 06:52:33.179398: step 23040, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 44h:51m:54s remains)
INFO - root - 2017-12-11 06:52:38.513984: step 23050, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:12m:00s remains)
INFO - root - 2017-12-11 06:52:43.894747: step 23060, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:49m:35s remains)
INFO - root - 2017-12-11 06:52:49.260262: step 23070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 45h:58m:40s remains)
INFO - root - 2017-12-11 06:52:54.650580: step 23080, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 46h:19m:19s remains)
INFO - root - 2017-12-11 06:53:00.013543: step 23090, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:52m:47s remains)
INFO - root - 2017-12-11 06:53:05.094653: step 23100, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:36m:42s remains)
2017-12-11 06:53:05.616470: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15683167 0.1830934 0.2236906 0.26681793 0.29200232 0.29788116 0.29293641 0.28076172 0.26185426 0.23518458 0.20805271 0.18990523 0.18117034 0.18326585 0.19400994][0.21364284 0.25092778 0.30196798 0.35680825 0.39010528 0.40054733 0.40008119 0.39222214 0.37407425 0.33982003 0.29774341 0.26384085 0.24324434 0.23887847 0.24760173][0.26938161 0.31480888 0.3677378 0.42374796 0.45893994 0.47351497 0.48103821 0.48220736 0.4683049 0.42856333 0.37290731 0.32461184 0.29377326 0.2831383 0.28778636][0.32647431 0.3794139 0.42985532 0.47745734 0.5054853 0.51819837 0.52926254 0.53461015 0.51989967 0.47211686 0.40473691 0.34628284 0.30841249 0.29073969 0.287036][0.36802259 0.43323153 0.48340079 0.52103227 0.53780216 0.54401618 0.55233145 0.55365992 0.52973282 0.46810296 0.38740394 0.31922072 0.27480456 0.24928305 0.23526216][0.37924719 0.45798165 0.51240104 0.54493862 0.55351233 0.554518 0.55893254 0.55371451 0.51815861 0.44229263 0.35075015 0.27695087 0.23118909 0.20305696 0.18194769][0.36178312 0.44629532 0.5066461 0.54177785 0.55230165 0.55636597 0.56118596 0.550622 0.50273091 0.41288096 0.31412396 0.24258012 0.20572136 0.18420507 0.16079046][0.32332262 0.4025909 0.46584502 0.50828 0.52917713 0.54284734 0.55122042 0.5369485 0.47892177 0.37846771 0.27790791 0.21632111 0.19570249 0.18550651 0.16120005][0.2784479 0.34868389 0.41298091 0.46383241 0.49642652 0.51884788 0.5284251 0.50965548 0.44614419 0.34435666 0.25269336 0.20892584 0.20807298 0.20940562 0.1831996][0.24181996 0.3081893 0.37566805 0.43493727 0.47628394 0.50162214 0.50627536 0.48090962 0.41720045 0.32546529 0.25277671 0.23043637 0.24810494 0.25929376 0.23167671][0.22401854 0.29415795 0.36895686 0.43350142 0.47463247 0.49188039 0.48444545 0.45272124 0.39611563 0.32420504 0.2755388 0.27326369 0.30511543 0.323451 0.29496145][0.22054695 0.30182749 0.38777187 0.45568478 0.4893066 0.48909962 0.46415749 0.42684829 0.3828114 0.33488277 0.30833513 0.31801003 0.35360056 0.37228543 0.34230897][0.22628254 0.31662083 0.41220948 0.48464262 0.51420754 0.50090832 0.4615992 0.41899931 0.38629091 0.3590889 0.34725428 0.35615179 0.38078859 0.39050502 0.35891438][0.24851075 0.33839816 0.43467617 0.50957221 0.54193509 0.5285601 0.48555496 0.43826982 0.40733451 0.38781735 0.37619093 0.36894262 0.36854836 0.36209959 0.33198208][0.28323331 0.36334896 0.45140383 0.52346551 0.56018972 0.55456632 0.51698613 0.46830463 0.43256652 0.40840527 0.38357395 0.35063887 0.31983316 0.29603866 0.27030548]]...]
INFO - root - 2017-12-11 06:53:11.074040: step 23110, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:53m:44s remains)
INFO - root - 2017-12-11 06:53:16.391484: step 23120, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.518 sec/batch; 44h:30m:03s remains)
INFO - root - 2017-12-11 06:53:21.770554: step 23130, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 49h:14m:06s remains)
INFO - root - 2017-12-11 06:53:27.109594: step 23140, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.525 sec/batch; 45h:04m:56s remains)
INFO - root - 2017-12-11 06:53:32.428233: step 23150, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:12m:25s remains)
INFO - root - 2017-12-11 06:53:37.803309: step 23160, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 44h:31m:59s remains)
INFO - root - 2017-12-11 06:53:43.082234: step 23170, loss = 0.69, batch loss = 0.63 (15.8 examples/sec; 0.505 sec/batch; 43h:24m:07s remains)
INFO - root - 2017-12-11 06:53:48.358671: step 23180, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 45h:06m:28s remains)
INFO - root - 2017-12-11 06:53:53.715506: step 23190, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 46h:04m:19s remains)
INFO - root - 2017-12-11 06:53:58.751625: step 23200, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 47h:45m:30s remains)
2017-12-11 06:53:59.305476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041794918 -0.033236723 -0.021955831 -0.013008573 -0.0095227547 -0.013454338 -0.023575865 -0.036168147 -0.04627765 -0.050432283 -0.048887551 -0.044030279 -0.038649853 -0.034830634 -0.033764526][-0.051156521 -0.043886673 -0.033233829 -0.024354825 -0.020004863 -0.022102531 -0.0298769 -0.040475745 -0.049548618 -0.053853855 -0.053240821 -0.049461223 -0.044863518 -0.04144565 -0.040544182][-0.058780476 -0.053547218 -0.044649549 -0.03635522 -0.03055148 -0.029087361 -0.032302558 -0.039159592 -0.046924952 -0.052746464 -0.055312213 -0.054620206 -0.051880334 -0.049003322 -0.0479182][-0.061196595 -0.058022112 -0.050381366 -0.040985763 -0.030636914 -0.021558503 -0.016334634 -0.017159196 -0.023909017 -0.034253567 -0.044772837 -0.052162953 -0.055075694 -0.0547107 -0.053848665][-0.053011417 -0.051175173 -0.043030694 -0.028792383 -0.0080125015 0.016056173 0.037040625 0.047476672 0.043064367 0.024889592 -0.00072274642 -0.025098024 -0.041826535 -0.049727745 -0.052178044][-0.034612734 -0.03288354 -0.021835934 0.0021734459 0.041536935 0.091820635 0.14108357 0.1732143 0.17574245 0.14635772 0.095191568 0.039579745 -0.0046485607 -0.030779887 -0.042065639][-0.01244738 -0.010711308 0.0044828453 0.041981958 0.10762911 0.1964682 0.28902644 0.35628769 0.37279928 0.32989925 0.2425016 0.14027888 0.053429551 -0.0021351015 -0.028405275][0.007998093 0.0071744542 0.024447618 0.07464385 0.16827349 0.30067536 0.44384465 0.55366993 0.58873516 0.53260332 0.40555465 0.2512266 0.11623719 0.027085787 -0.01641961][0.023107134 0.015139375 0.028571321 0.083698176 0.1953747 0.35993129 0.5422067 0.68592054 0.73641509 0.67138332 0.51565748 0.32368094 0.15456052 0.042249963 -0.012266793][0.030008653 0.010227066 0.011539605 0.057708103 0.16654693 0.33607394 0.52822709 0.68208355 0.73778135 0.67169935 0.5111481 0.3139331 0.14217478 0.030341676 -0.021154596][0.03011905 -0.00431839 -0.022197319 0.00042617609 0.081985533 0.22258133 0.3877573 0.52284688 0.57346195 0.51904047 0.3847563 0.22161822 0.083044879 -0.0031795695 -0.038187116][0.030489312 -0.01786311 -0.057332776 -0.066759534 -0.029381379 0.057313994 0.16825794 0.26413906 0.30446413 0.27422559 0.19107601 0.090269528 0.0080588078 -0.038520742 -0.051651604][0.034579284 -0.02237986 -0.07879439 -0.11748534 -0.12419347 -0.093876883 -0.0399646 0.0155035 0.047435891 0.045459617 0.017345544 -0.019809293 -0.047805082 -0.059200205 -0.055557307][0.039679598 -0.01861608 -0.082379766 -0.13788252 -0.17250447 -0.17900041 -0.16153726 -0.13003294 -0.099528685 -0.079168357 -0.0699012 -0.067649081 -0.065565787 -0.060402285 -0.051620971][0.0467312 -0.0059644128 -0.066662878 -0.12375173 -0.1648794 -0.18132545 -0.17307273 -0.14591114 -0.11205935 -0.082170583 -0.061979022 -0.052476496 -0.049484733 -0.048261452 -0.045316283]]...]
INFO - root - 2017-12-11 06:54:04.680430: step 23210, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:02m:23s remains)
INFO - root - 2017-12-11 06:54:10.082768: step 23220, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:01m:09s remains)
INFO - root - 2017-12-11 06:54:15.482552: step 23230, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:38m:38s remains)
INFO - root - 2017-12-11 06:54:20.794042: step 23240, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.512 sec/batch; 43h:59m:09s remains)
INFO - root - 2017-12-11 06:54:26.137896: step 23250, loss = 0.66, batch loss = 0.60 (15.3 examples/sec; 0.523 sec/batch; 44h:57m:53s remains)
INFO - root - 2017-12-11 06:54:31.548871: step 23260, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 46h:22m:23s remains)
INFO - root - 2017-12-11 06:54:36.795990: step 23270, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:14m:59s remains)
INFO - root - 2017-12-11 06:54:42.129336: step 23280, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 44h:49m:24s remains)
INFO - root - 2017-12-11 06:54:47.496751: step 23290, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 46h:35m:29s remains)
INFO - root - 2017-12-11 06:54:52.634654: step 23300, loss = 0.72, batch loss = 0.66 (24.9 examples/sec; 0.321 sec/batch; 27h:34m:44s remains)
2017-12-11 06:54:53.206960: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20855051 0.19790559 0.20134397 0.21554451 0.23268753 0.24978182 0.26319826 0.26824671 0.24977739 0.22071981 0.19743749 0.19086389 0.19645785 0.19770622 0.18883456][0.24263518 0.24552946 0.25555196 0.26472631 0.26829723 0.26915678 0.26732561 0.26460448 0.250723 0.23521458 0.22756542 0.23160703 0.24074017 0.23726411 0.21830378][0.24782206 0.26834589 0.28928736 0.30027384 0.29776338 0.28742746 0.27268225 0.26094443 0.24834834 0.24280791 0.2482253 0.2622048 0.27408162 0.26633656 0.23841889][0.2309442 0.27016908 0.30615422 0.328517 0.33278322 0.32422236 0.30668506 0.28943491 0.27337429 0.26730573 0.27478725 0.29048255 0.30016103 0.28701168 0.25211626][0.21772008 0.26933828 0.32083932 0.36093867 0.38260159 0.38749716 0.37661719 0.35662168 0.33075166 0.31133774 0.30689022 0.31339991 0.31518334 0.296043 0.25699049][0.2149784 0.2757037 0.34291494 0.40658683 0.45435756 0.48172718 0.48539749 0.46634141 0.4273763 0.385528 0.35884726 0.34803146 0.33746988 0.31121996 0.26888949][0.21497837 0.28230372 0.36570442 0.45507571 0.53129643 0.5827387 0.60335219 0.58727968 0.5356276 0.46977165 0.41862333 0.38742658 0.36124349 0.32553044 0.2795189][0.20615324 0.27696908 0.37319037 0.48349917 0.58289725 0.65338188 0.68893719 0.67758 0.6182189 0.53447527 0.46364903 0.41324654 0.36977124 0.32202694 0.27138346][0.17832604 0.24624257 0.34436727 0.46037659 0.56665319 0.64284545 0.68486941 0.67704093 0.61681581 0.52826852 0.45208505 0.39447591 0.34291098 0.28950965 0.23853138][0.11278619 0.16811009 0.25408295 0.3577289 0.45302096 0.52145386 0.56090009 0.55566704 0.5025211 0.42430511 0.36034048 0.31323856 0.27017605 0.2247659 0.18269081][0.014654481 0.05336101 0.12139573 0.20589103 0.28418997 0.34079677 0.37444115 0.37133288 0.32826534 0.26622847 0.22138312 0.19416285 0.17054838 0.14326243 0.11687163][-0.083863609 -0.059982263 -0.0089850947 0.057161305 0.11989739 0.16639973 0.19446325 0.19239506 0.15826346 0.111268 0.083458684 0.075428031 0.073059954 0.067288287 0.058484532][-0.14422351 -0.12858866 -0.089584343 -0.0377817 0.012584876 0.051137716 0.073823139 0.070964009 0.042986695 0.0074906335 -0.0075208438 -6.5451626e-05 0.01739171 0.03365932 0.042114951][-0.1516858 -0.13677098 -0.10304249 -0.060371768 -0.019470524 0.011402634 0.027080433 0.020366352 -0.0052070906 -0.032297157 -0.03620971 -0.015523366 0.018712472 0.05368619 0.076126263][-0.11789409 -0.095728278 -0.059632678 -0.019601533 0.015464248 0.038754746 0.045593739 0.03208302 0.004850084 -0.017414482 -0.012321235 0.019356083 0.065939069 0.11303156 0.14299332]]...]
INFO - root - 2017-12-11 06:54:58.558215: step 23310, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 44h:29m:04s remains)
INFO - root - 2017-12-11 06:55:03.811848: step 23320, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 44h:13m:47s remains)
INFO - root - 2017-12-11 06:55:09.207239: step 23330, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:51m:25s remains)
INFO - root - 2017-12-11 06:55:14.541150: step 23340, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:03m:11s remains)
INFO - root - 2017-12-11 06:55:19.965917: step 23350, loss = 0.71, batch loss = 0.66 (15.4 examples/sec; 0.520 sec/batch; 44h:41m:15s remains)
INFO - root - 2017-12-11 06:55:25.325123: step 23360, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:18m:09s remains)
INFO - root - 2017-12-11 06:55:30.723573: step 23370, loss = 0.73, batch loss = 0.67 (15.0 examples/sec; 0.532 sec/batch; 45h:40m:39s remains)
INFO - root - 2017-12-11 06:55:36.109239: step 23380, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:30m:26s remains)
INFO - root - 2017-12-11 06:55:41.462596: step 23390, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:25m:53s remains)
INFO - root - 2017-12-11 06:55:46.804141: step 23400, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 45h:56m:30s remains)
2017-12-11 06:55:47.279354: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15454346 0.099165663 0.07608787 0.10334316 0.18289854 0.29939595 0.41836515 0.49382156 0.50085789 0.44782943 0.35667807 0.25397936 0.16397402 0.10056768 0.061548475][0.14649415 0.086551644 0.064015627 0.099235989 0.1903185 0.31935352 0.44920179 0.53301096 0.54338628 0.48662606 0.38525233 0.26705524 0.15847006 0.077279665 0.026329217][0.13006401 0.06910333 0.046082482 0.082660407 0.17325273 0.29838929 0.42336291 0.50599462 0.52004111 0.46839455 0.37080684 0.25295445 0.14105964 0.05474003 0.000201355][0.12734216 0.065591648 0.039755207 0.072139382 0.15607139 0.27125606 0.38624138 0.4646349 0.48169881 0.43721512 0.34671819 0.23413444 0.1255551 0.042306222 -0.007929787][0.13586649 0.079995438 0.054593258 0.081592619 0.15488897 0.25505462 0.35545471 0.42618838 0.44448608 0.40740708 0.32572851 0.22121233 0.11846337 0.039878111 -0.0057636187][0.15483613 0.11102125 0.091016576 0.11376611 0.17503895 0.25744694 0.34033033 0.40017402 0.41666502 0.38485527 0.31129324 0.21500789 0.117254 0.040148977 -0.0059997942][0.16494334 0.13917534 0.12915257 0.14930715 0.19912986 0.264421 0.330497 0.37888619 0.3919743 0.36395589 0.29768118 0.2095516 0.11654041 0.038965173 -0.011360062][0.15815251 0.15426023 0.15687488 0.17407185 0.21023029 0.25713736 0.30745396 0.34694973 0.35950917 0.33814517 0.28157133 0.20330539 0.11657618 0.039450623 -0.014819249][0.14641136 0.166719 0.18349592 0.19533588 0.21140815 0.23315611 0.26416406 0.29558906 0.31181189 0.30227852 0.2599988 0.19421522 0.1159928 0.041791473 -0.013715172][0.13739039 0.183317 0.21242377 0.21598186 0.20743187 0.19893129 0.20663923 0.22820146 0.24870552 0.25309986 0.2281476 0.17748716 0.111141 0.044447761 -0.0076877521][0.13457817 0.19779781 0.23191509 0.22448616 0.1932999 0.15887327 0.14638823 0.15843031 0.18165414 0.19794911 0.18972401 0.15603358 0.10654283 0.054471973 0.011536179][0.1341929 0.20140351 0.2310604 0.21101609 0.16301405 0.11207388 0.086749777 0.092261426 0.11765344 0.1435387 0.14995652 0.13371113 0.10475348 0.073020361 0.043815013][0.1291092 0.18773673 0.20564485 0.17423734 0.11707322 0.059552141 0.029138736 0.032608896 0.06164328 0.096450433 0.11553856 0.1151896 0.10549673 0.093185306 0.077069886][0.091955714 0.13435785 0.14038007 0.10605861 0.053582158 0.00423518 -0.019376444 -0.010290671 0.02554895 0.068837121 0.097533353 0.10856795 0.11186962 0.11215683 0.1039613][0.017622102 0.041690387 0.041030206 0.014862023 -0.018772794 -0.045337953 -0.04926778 -0.024900185 0.02306891 0.076364234 0.11385228 0.13336581 0.14352539 0.14784642 0.1395672]]...]
INFO - root - 2017-12-11 06:55:52.368037: step 23410, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 44h:36m:01s remains)
INFO - root - 2017-12-11 06:55:57.620200: step 23420, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 44h:44m:05s remains)
INFO - root - 2017-12-11 06:56:03.002078: step 23430, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:19m:00s remains)
INFO - root - 2017-12-11 06:56:08.266080: step 23440, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:42m:51s remains)
INFO - root - 2017-12-11 06:56:13.677672: step 23450, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:25m:09s remains)
INFO - root - 2017-12-11 06:56:19.072533: step 23460, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:05m:17s remains)
INFO - root - 2017-12-11 06:56:24.400174: step 23470, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 45h:20m:39s remains)
INFO - root - 2017-12-11 06:56:29.622922: step 23480, loss = 0.68, batch loss = 0.63 (16.0 examples/sec; 0.500 sec/batch; 42h:57m:33s remains)
INFO - root - 2017-12-11 06:56:34.979316: step 23490, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:09m:10s remains)
INFO - root - 2017-12-11 06:56:40.354266: step 23500, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 44h:41m:14s remains)
2017-12-11 06:56:40.901366: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.020604271 0.023829652 0.014293725 -0.0033826954 -0.023049463 -0.039585937 -0.048655275 -0.048899505 -0.043164764 -0.033934161 -0.022647489 -0.010603948 -0.0021251608 -0.0036918051 -0.018535744][0.11023383 0.12380721 0.11337686 0.090086751 0.063117169 0.040214956 0.028457535 0.029499998 0.037557404 0.048390243 0.061022919 0.074682206 0.082937017 0.074255854 0.043817461][0.23849186 0.26822093 0.26120868 0.23646766 0.20629525 0.18077901 0.16941684 0.17355148 0.18299226 0.19075789 0.19714436 0.20366566 0.20297235 0.17874213 0.12550683][0.37252909 0.42082641 0.42257625 0.40437862 0.38203919 0.3661688 0.36472547 0.37568858 0.38340372 0.37898508 0.36678547 0.35295051 0.33085313 0.28133804 0.19963498][0.47138351 0.5355131 0.55133969 0.55026078 0.55091423 0.56093472 0.58082241 0.60172009 0.60268849 0.57615405 0.53492832 0.4914794 0.43995062 0.35878745 0.24578017][0.5263949 0.60185969 0.63371414 0.654514 0.68398941 0.72455806 0.7668848 0.79379469 0.78026247 0.72414857 0.64985591 0.57680774 0.49811551 0.38883314 0.24993636][0.55336505 0.63641822 0.68419278 0.72672164 0.78246397 0.848797 0.90715104 0.93198884 0.89576972 0.80521607 0.69739205 0.59898645 0.50073409 0.3734009 0.21977894][0.54614007 0.63372576 0.6952616 0.75410622 0.82561588 0.90491664 0.96941644 0.98721832 0.92842644 0.80843985 0.67458749 0.55964136 0.45246339 0.32004178 0.16577269][0.50197786 0.58866954 0.65730119 0.71971554 0.78570354 0.85363638 0.90671378 0.91403669 0.84392595 0.71564418 0.57864177 0.46693343 0.36804733 0.24563646 0.10360143][0.44150314 0.518604 0.58025688 0.6276387 0.66434717 0.69495255 0.71573842 0.70647138 0.6374231 0.52547061 0.41178992 0.326456 0.25547087 0.15974604 0.042268764][0.37305146 0.43290779 0.4758338 0.49712223 0.49675342 0.4846794 0.46923244 0.44237989 0.37960467 0.293793 0.21417271 0.16394202 0.12889801 0.069673017 -0.014246995][0.25236586 0.29064181 0.31025735 0.30515194 0.27634344 0.23719135 0.20181145 0.16984764 0.12173586 0.065066576 0.017909467 -0.0023307344 -0.0066031953 -0.030530633 -0.078466982][0.069960415 0.081641562 0.076662607 0.04933279 0.003345974 -0.046835896 -0.085928895 -0.10897949 -0.13150595 -0.15363972 -0.16804634 -0.16231513 -0.1426183 -0.1375239 -0.1527258][-0.11265188 -0.12745441 -0.15094036 -0.18931152 -0.23791902 -0.28481007 -0.31727466 -0.32753369 -0.32556194 -0.31633374 -0.30156723 -0.27502063 -0.23993845 -0.21635914 -0.20889178][-0.24351701 -0.27423576 -0.30299395 -0.33661953 -0.37153456 -0.40181258 -0.42064142 -0.42158791 -0.40844569 -0.38556296 -0.35795113 -0.32376924 -0.28522256 -0.254666 -0.23510572]]...]
INFO - root - 2017-12-11 06:56:46.141906: step 23510, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 47h:14m:02s remains)
INFO - root - 2017-12-11 06:56:51.496416: step 23520, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.539 sec/batch; 46h:13m:36s remains)
INFO - root - 2017-12-11 06:56:56.810373: step 23530, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 45h:54m:46s remains)
INFO - root - 2017-12-11 06:57:02.206552: step 23540, loss = 0.71, batch loss = 0.66 (15.4 examples/sec; 0.519 sec/batch; 44h:34m:35s remains)
INFO - root - 2017-12-11 06:57:07.610036: step 23550, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:22m:54s remains)
INFO - root - 2017-12-11 06:57:12.932321: step 23560, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:54m:37s remains)
INFO - root - 2017-12-11 06:57:18.215083: step 23570, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:56m:49s remains)
INFO - root - 2017-12-11 06:57:23.586878: step 23580, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 45h:01m:58s remains)
INFO - root - 2017-12-11 06:57:29.017130: step 23590, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:00m:15s remains)
INFO - root - 2017-12-11 06:57:34.362548: step 23600, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:59m:42s remains)
2017-12-11 06:57:34.942404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064304516 -0.062708586 -0.049546551 -0.028005434 7.0141796e-05 0.030788563 0.055267002 0.064537771 0.053741075 0.026593531 -0.0060461755 -0.033377673 -0.049103487 -0.055272594 -0.056243926][-0.064912222 -0.057122309 -0.030962409 0.010359392 0.063045971 0.11893191 0.16266407 0.17913398 0.15850855 0.10543581 0.040034268 -0.014289923 -0.04446831 -0.055286206 -0.056634143][-0.06084916 -0.043951783 0.0007695923 0.071438894 0.16112159 0.25381795 0.32419196 0.34974903 0.31516087 0.22652021 0.11566192 0.023283605 -0.027716948 -0.04544713 -0.047830921][-0.058329556 -0.031911291 0.034172054 0.14101334 0.27715492 0.41413897 0.51250136 0.5434463 0.48952267 0.36072871 0.20068076 0.06719625 -0.006839436 -0.032350305 -0.036053155][-0.059130725 -0.02661572 0.057487775 0.19807911 0.37897459 0.556811 0.67631191 0.704304 0.62682134 0.46135452 0.26178193 0.097449854 0.0067452397 -0.023577631 -0.027300371][-0.062837422 -0.029890008 0.062774636 0.22314371 0.43225005 0.63533121 0.76467866 0.78408349 0.68569565 0.49585769 0.27627638 0.1001601 0.0048643039 -0.024903763 -0.026586382][-0.068003543 -0.039820109 0.04983199 0.21036577 0.42241874 0.62774146 0.75461721 0.76561528 0.65677577 0.46055806 0.24319255 0.075111173 -0.012350022 -0.036442645 -0.034322802][-0.073208258 -0.052596662 0.025981469 0.17153366 0.36484793 0.5508787 0.66365165 0.66796637 0.56097585 0.37632447 0.17989509 0.034368951 -0.037060983 -0.052736133 -0.046388522][-0.077781208 -0.064665481 0.00079902651 0.12680548 0.29273793 0.44899035 0.54133672 0.54041851 0.44302964 0.28014123 0.11293121 -0.0053683589 -0.059360705 -0.067205943 -0.05799631][-0.080491081 -0.0736577 -0.018995222 0.091568567 0.23475075 0.36515394 0.43993193 0.43647304 0.350483 0.20902278 0.067195088 -0.029935079 -0.071872741 -0.0759023 -0.066640913][-0.0817783 -0.078029931 -0.031026138 0.069004811 0.19720085 0.3108454 0.37481594 0.37147939 0.29610464 0.17227055 0.048474751 -0.036481544 -0.07373824 -0.0786182 -0.071806043][-0.080405883 -0.077009067 -0.035411377 0.055447027 0.17130646 0.27294046 0.32958835 0.32650951 0.26043829 0.15193728 0.042620115 -0.034381837 -0.070325196 -0.077627592 -0.073723949][-0.077046007 -0.072401509 -0.034640394 0.046186924 0.1480612 0.23721854 0.2855337 0.28015921 0.22134867 0.12745669 0.033155963 -0.034847077 -0.068164416 -0.076274522 -0.073860079][-0.074428424 -0.069122635 -0.035436638 0.032711037 0.11638534 0.1890156 0.22591725 0.21642652 0.16471596 0.086912446 0.010519968 -0.044755954 -0.071722895 -0.077576116 -0.074447252][-0.074906774 -0.071583256 -0.045143493 0.0056867315 0.066260561 0.11833797 0.14256787 0.13063794 0.088971026 0.030534208 -0.024711434 -0.063644573 -0.0809228 -0.081985295 -0.076394238]]...]
INFO - root - 2017-12-11 06:57:40.068366: step 23610, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.531 sec/batch; 45h:32m:29s remains)
INFO - root - 2017-12-11 06:57:45.420436: step 23620, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 46h:25m:40s remains)
INFO - root - 2017-12-11 06:57:50.825099: step 23630, loss = 0.67, batch loss = 0.62 (13.7 examples/sec; 0.583 sec/batch; 50h:00m:11s remains)
INFO - root - 2017-12-11 06:57:56.141643: step 23640, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:52m:40s remains)
INFO - root - 2017-12-11 06:58:01.397559: step 23650, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:58m:34s remains)
INFO - root - 2017-12-11 06:58:06.642784: step 23660, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 44h:16m:24s remains)
INFO - root - 2017-12-11 06:58:11.916915: step 23670, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 46h:09m:38s remains)
INFO - root - 2017-12-11 06:58:17.291711: step 23680, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:59m:36s remains)
INFO - root - 2017-12-11 06:58:22.542738: step 23690, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:06m:30s remains)
INFO - root - 2017-12-11 06:58:27.992221: step 23700, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 44h:36m:31s remains)
2017-12-11 06:58:28.568730: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035437714 0.028587477 0.023913762 0.02272377 0.023849472 0.026465947 0.030216377 0.03365536 0.033877362 0.03083238 0.028254606 0.028904369 0.032486703 0.036121339 0.038519461][0.092272535 0.085525632 0.079463348 0.077332467 0.077701844 0.080003649 0.0838908 0.087438591 0.087238513 0.083060533 0.079766 0.0800804 0.083458714 0.086270668 0.087600015][0.1549502 0.1521754 0.14689632 0.14517546 0.14552082 0.14734544 0.15006614 0.15205088 0.150628 0.14492466 0.14012751 0.13933171 0.14249 0.1449239 0.14548832][0.20263951 0.20890202 0.20905995 0.21069099 0.21231885 0.21349955 0.21360116 0.21208026 0.20842357 0.20111229 0.19440421 0.19274276 0.1975023 0.20234998 0.20414515][0.23092875 0.25090533 0.26120478 0.2680442 0.27028286 0.26924026 0.26520845 0.25884488 0.25231472 0.24371718 0.23541896 0.23333293 0.24050504 0.24949196 0.25430179][0.24420643 0.28207177 0.30507109 0.31657112 0.31684878 0.3111299 0.30163613 0.2896978 0.28003314 0.2708182 0.2618258 0.25867143 0.26533046 0.27558473 0.28232491][0.24368668 0.30093253 0.33746275 0.35342827 0.35141224 0.34092021 0.32586831 0.30755287 0.29295486 0.28135931 0.26991451 0.26222375 0.26265413 0.26850334 0.27392474][0.22671568 0.29515186 0.34040523 0.35853127 0.35496509 0.34210071 0.32401523 0.30097285 0.28099677 0.26549402 0.24991944 0.23544317 0.22696598 0.22598988 0.2287885][0.1902649 0.25696447 0.30177146 0.31798425 0.31318334 0.3004187 0.28285637 0.25916067 0.23627815 0.21756838 0.19888905 0.17968248 0.16483353 0.15897025 0.16042112][0.14012864 0.19565074 0.232986 0.24507098 0.23996626 0.22871283 0.21305802 0.19079886 0.16702604 0.14636381 0.12626614 0.10516339 0.087512776 0.079491965 0.080956534][0.085485831 0.12727503 0.15591165 0.16666423 0.1662046 0.16060826 0.14918442 0.12954196 0.10577416 0.083546363 0.061933424 0.039701294 0.021202657 0.01237412 0.013567343][0.041574489 0.070398636 0.091887042 0.10413401 0.11172303 0.11535867 0.1117762 0.098790072 0.079541966 0.059518147 0.03828194 0.016022358 -0.0021241398 -0.011317322 -0.011495482][0.0093682716 0.02950084 0.046932917 0.061717205 0.076579608 0.088968068 0.094570793 0.091636352 0.08216 0.070011526 0.054449596 0.036941987 0.022733165 0.014894811 0.013354116][-0.015346549 0.0010741444 0.018338358 0.036911264 0.058120009 0.07798484 0.092291586 0.099582292 0.10116646 0.098928332 0.092002966 0.0824058 0.074276812 0.068873785 0.066322863][-0.03297963 -0.018564049 -0.00064210134 0.020600345 0.045912016 0.07119336 0.092563123 0.10844177 0.11985281 0.12724002 0.12944105 0.12800266 0.12559313 0.12270194 0.11996397]]...]
INFO - root - 2017-12-11 06:58:33.743542: step 23710, loss = 0.69, batch loss = 0.63 (23.4 examples/sec; 0.342 sec/batch; 29h:17m:48s remains)
INFO - root - 2017-12-11 06:58:39.170275: step 23720, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 48h:32m:32s remains)
INFO - root - 2017-12-11 06:58:44.503438: step 23730, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:47m:55s remains)
INFO - root - 2017-12-11 06:58:49.852233: step 23740, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.526 sec/batch; 45h:08m:28s remains)
INFO - root - 2017-12-11 06:58:55.228704: step 23750, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:59m:35s remains)
INFO - root - 2017-12-11 06:59:00.580590: step 23760, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:22m:56s remains)
INFO - root - 2017-12-11 06:59:05.983832: step 23770, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:34m:23s remains)
INFO - root - 2017-12-11 06:59:11.310500: step 23780, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 44h:33m:19s remains)
INFO - root - 2017-12-11 06:59:16.638738: step 23790, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:42m:17s remains)
INFO - root - 2017-12-11 06:59:21.994704: step 23800, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 47h:10m:49s remains)
2017-12-11 06:59:22.609000: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035526138 0.041945171 0.052569278 0.062183168 0.068297587 0.070101768 0.0682533 0.063662067 0.056606244 0.051160067 0.05020969 0.056931444 0.070681125 0.085312821 0.095804639][0.051398713 0.056757163 0.066079 0.073967412 0.079478323 0.082006127 0.082247026 0.079642288 0.073658347 0.0682304 0.066726886 0.07248801 0.084540732 0.096371613 0.10259858][0.067053549 0.07160373 0.078891695 0.084919229 0.089686237 0.092664436 0.094559982 0.093713947 0.089036979 0.084325381 0.083697528 0.089928627 0.10093453 0.10946789 0.10974046][0.084346175 0.092134953 0.10151979 0.1090553 0.11483323 0.1183138 0.12044477 0.11881788 0.11316302 0.10807502 0.10953089 0.11904455 0.13230188 0.14066643 0.13756351][0.099761993 0.11570547 0.13354996 0.14901502 0.16108234 0.16828053 0.17106596 0.16685718 0.15782727 0.15056138 0.15428129 0.16913156 0.18762366 0.19858341 0.19469532][0.10626237 0.13328093 0.164572 0.19451748 0.22010148 0.2369207 0.24330068 0.23723505 0.2240793 0.21300912 0.21794711 0.23884068 0.26491749 0.28058529 0.27786028][0.098520972 0.13514793 0.1800233 0.22718649 0.27112836 0.30281252 0.31703734 0.31267336 0.29729602 0.28202087 0.28653857 0.31274873 0.34707326 0.36802441 0.36721918][0.076932266 0.116611 0.16927445 0.23033343 0.2919735 0.33967537 0.36453536 0.36649647 0.3528865 0.33485413 0.33748153 0.36665511 0.40721315 0.43229896 0.43397537][0.050062928 0.083529063 0.13392061 0.1997527 0.27135634 0.32975358 0.36422145 0.37560046 0.3680037 0.34995553 0.34971389 0.37751222 0.41821918 0.44318241 0.44693831][0.032931048 0.053569596 0.093077779 0.15344611 0.22314845 0.28189254 0.32027936 0.33954632 0.33847439 0.32162136 0.31731588 0.33857936 0.37178683 0.39153802 0.39541462][0.036047321 0.044599216 0.072054066 0.12276092 0.18241185 0.23292676 0.26828218 0.28934786 0.29034829 0.27249381 0.2619915 0.27292112 0.29490209 0.30891472 0.3125155][0.059240665 0.062619738 0.084812276 0.13113141 0.1834203 0.22619587 0.25661698 0.27454 0.27232885 0.2492978 0.22864369 0.22509497 0.23363376 0.24313617 0.24670689][0.0906561 0.096843265 0.12183699 0.17254069 0.22732426 0.27073336 0.30057976 0.31563643 0.30898026 0.27821046 0.24299245 0.21997581 0.21229215 0.21643198 0.21865186][0.118205 0.12948731 0.15958264 0.21800977 0.28178656 0.33330563 0.36847365 0.38530681 0.378062 0.3406601 0.28817841 0.24097215 0.21326295 0.20789368 0.2050043][0.13614357 0.15017734 0.18089026 0.24124122 0.30944845 0.36581334 0.40503407 0.42604756 0.42336488 0.38489783 0.31920472 0.25029722 0.20312501 0.1857786 0.17555034]]...]
INFO - root - 2017-12-11 06:59:27.890628: step 23810, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:41m:58s remains)
INFO - root - 2017-12-11 06:59:32.957115: step 23820, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:26m:25s remains)
INFO - root - 2017-12-11 06:59:38.359259: step 23830, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:43m:37s remains)
INFO - root - 2017-12-11 06:59:43.765554: step 23840, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:12m:44s remains)
INFO - root - 2017-12-11 06:59:49.072904: step 23850, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:51m:44s remains)
INFO - root - 2017-12-11 06:59:54.519718: step 23860, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:40m:05s remains)
INFO - root - 2017-12-11 06:59:59.964381: step 23870, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:10m:30s remains)
INFO - root - 2017-12-11 07:00:05.279677: step 23880, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:54m:50s remains)
INFO - root - 2017-12-11 07:00:10.527732: step 23890, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 45h:35m:26s remains)
INFO - root - 2017-12-11 07:00:15.861932: step 23900, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 47h:04m:39s remains)
2017-12-11 07:00:16.398237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24169114 0.22962438 0.22276235 0.22976565 0.24137868 0.25795814 0.28393948 0.31236416 0.33425629 0.35304388 0.38382313 0.42169583 0.43884262 0.43206221 0.41878095][0.23611642 0.22387211 0.22526823 0.2447138 0.26916969 0.29725498 0.32828298 0.35353157 0.36485559 0.36515242 0.37159365 0.38796633 0.39755765 0.39674819 0.3989974][0.2177763 0.21273346 0.23085995 0.26787513 0.3071965 0.34883872 0.38946205 0.41569582 0.41533202 0.39118832 0.36425117 0.34979349 0.34403074 0.3452785 0.36491048][0.22097525 0.22958067 0.271211 0.32840517 0.38182247 0.43638015 0.49035069 0.52316189 0.51412636 0.46650112 0.40645587 0.357784 0.32821387 0.32033175 0.3480725][0.26010421 0.29112995 0.3610546 0.4374181 0.49879745 0.55876762 0.62210357 0.65988952 0.64160538 0.57269007 0.48555753 0.40602741 0.34800112 0.32033306 0.34253776][0.32949686 0.39163053 0.49123383 0.5825603 0.64360523 0.69904029 0.76399273 0.80235285 0.77508295 0.69030255 0.58575791 0.4818157 0.3927682 0.3350406 0.33520693][0.40652615 0.50325173 0.62876934 0.72867787 0.78213185 0.82503349 0.88376677 0.91822416 0.88353968 0.788694 0.67492461 0.55325347 0.43488079 0.34273034 0.31048843][0.45389578 0.57669938 0.71622306 0.81580377 0.85716659 0.88372672 0.93004149 0.95629466 0.91637659 0.81675804 0.6999324 0.56789654 0.42858705 0.30906293 0.24757737][0.43955696 0.56838763 0.70135719 0.78862667 0.8170051 0.83021438 0.86373317 0.88147134 0.84210187 0.74793458 0.63825554 0.50841093 0.36402619 0.23424473 0.15766308][0.37196133 0.48244438 0.58584428 0.64770716 0.6622569 0.66690677 0.690552 0.7027815 0.67076558 0.59378046 0.5033381 0.39106691 0.26122487 0.14274234 0.070605047][0.28764173 0.35820365 0.41686556 0.44530013 0.44515848 0.4441582 0.4593215 0.46603182 0.44173446 0.38549525 0.32050186 0.23728284 0.13900216 0.051210895 0.0017685166][0.21445642 0.2356589 0.2464398 0.2411916 0.22676767 0.22080009 0.22761019 0.22664618 0.20575212 0.1671913 0.12747034 0.0772374 0.017873356 -0.030774537 -0.049959388][0.17345287 0.15345648 0.12764846 0.097461 0.072413348 0.062056933 0.060394853 0.049036656 0.025111631 -0.0024648495 -0.021654734 -0.042359579 -0.066803806 -0.081218645 -0.075146116][0.18005954 0.139663 0.098928712 0.0599988 0.031831462 0.018893527 0.0092843007 -0.014583291 -0.04829111 -0.074096665 -0.08146026 -0.081873082 -0.082905434 -0.076511063 -0.057853512][0.20984751 0.17325824 0.14235567 0.11441313 0.092969537 0.080031149 0.0625957 0.024829736 -0.023436608 -0.05643088 -0.061835196 -0.052912753 -0.043092433 -0.029501798 -0.010307686]]...]
INFO - root - 2017-12-11 07:00:21.847230: step 23910, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:31m:59s remains)
INFO - root - 2017-12-11 07:00:26.841629: step 23920, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 45h:04m:33s remains)
INFO - root - 2017-12-11 07:00:32.191354: step 23930, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 45h:16m:33s remains)
INFO - root - 2017-12-11 07:00:37.476079: step 23940, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:08m:52s remains)
INFO - root - 2017-12-11 07:00:42.899374: step 23950, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 47h:08m:52s remains)
INFO - root - 2017-12-11 07:00:48.278694: step 23960, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:16m:04s remains)
INFO - root - 2017-12-11 07:00:53.638405: step 23970, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 44h:45m:23s remains)
INFO - root - 2017-12-11 07:00:58.904371: step 23980, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 45h:12m:56s remains)
INFO - root - 2017-12-11 07:01:04.321299: step 23990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:55m:26s remains)
INFO - root - 2017-12-11 07:01:09.722977: step 24000, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 47h:47m:03s remains)
2017-12-11 07:01:10.289236: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22051051 0.19911464 0.17622797 0.16269526 0.16538894 0.17759435 0.19769278 0.22280078 0.2545622 0.28146017 0.28156257 0.25390455 0.20203203 0.13448857 0.066053778][0.259031 0.23959942 0.21462227 0.19636425 0.19389613 0.20212966 0.21865748 0.23689875 0.26013389 0.278831 0.2711966 0.23707709 0.18177263 0.11475229 0.049279407][0.28132468 0.2622458 0.23440462 0.20787758 0.19504905 0.19304878 0.19982278 0.2080071 0.22365572 0.23995312 0.23369257 0.20169455 0.1486946 0.085520141 0.025221918][0.28558153 0.27398849 0.25257844 0.226555 0.20771287 0.19631651 0.19261067 0.18919127 0.19499378 0.20622529 0.19943431 0.16793065 0.11626433 0.056823071 0.0018859254][0.26367909 0.27605227 0.28112087 0.27525511 0.26379061 0.25108889 0.24168928 0.22716731 0.21688096 0.21211918 0.1938162 0.15439241 0.098629817 0.03995204 -0.011127396][0.20944844 0.25621721 0.30231208 0.33284435 0.34376338 0.343695 0.34288377 0.32919064 0.306443 0.2804659 0.24093921 0.18313965 0.11287378 0.045528311 -0.0090127112][0.1246215 0.20372097 0.29003075 0.35763279 0.395796 0.4185507 0.44171336 0.44506267 0.42169282 0.37911415 0.31766888 0.23738924 0.14588654 0.062625133 -0.00096775824][0.043658204 0.13621752 0.24348231 0.33099073 0.38767156 0.43346977 0.48607516 0.51283824 0.49609736 0.44378304 0.36730531 0.2712743 0.16422851 0.069473475 0.00074895483][-0.016724084 0.065703407 0.16562913 0.24852674 0.30733913 0.36577329 0.43863 0.48183233 0.47155026 0.41717678 0.33948654 0.24490416 0.13942884 0.047511559 -0.014885811][-0.053118128 0.0015873337 0.073527336 0.13526341 0.18369727 0.24083641 0.31530032 0.36000776 0.35259652 0.30553609 0.24302006 0.16861747 0.081853054 0.0050876583 -0.043530714][-0.071435563 -0.04915978 -0.011490641 0.023432076 0.054765183 0.099199668 0.15991431 0.19647749 0.19318673 0.163612 0.12764134 0.08266072 0.021213567 -0.037377294 -0.072717555][-0.060883526 -0.061168548 -0.051139079 -0.039787065 -0.026474271 0.0017276774 0.046225917 0.076216891 0.081029095 0.069720961 0.054410227 0.028121211 -0.018049318 -0.065394215 -0.092527948][-0.011866851 -0.022656068 -0.029469989 -0.033556864 -0.03313433 -0.014678101 0.024399923 0.058362719 0.075320728 0.075535536 0.065882221 0.039245084 -0.0084504085 -0.057135444 -0.085787676][0.050667733 0.038424231 0.025394527 0.014864103 0.0088513074 0.025224319 0.06843546 0.11253101 0.13969249 0.14231852 0.12711646 0.0897428 0.032527491 -0.023872133 -0.059743833][0.09806525 0.0886689 0.076343969 0.06588538 0.058743723 0.077329546 0.12573275 0.17523892 0.2036189 0.20065281 0.17644893 0.12949266 0.065293744 0.0035073091 -0.037826449]]...]
INFO - root - 2017-12-11 07:01:15.602835: step 24010, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 44h:19m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:01:20.620224: step 24020, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.512 sec/batch; 43h:53m:37s remains)
INFO - root - 2017-12-11 07:01:25.999216: step 24030, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 46h:14m:03s remains)
INFO - root - 2017-12-11 07:01:31.279616: step 24040, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:42m:18s remains)
INFO - root - 2017-12-11 07:01:36.646759: step 24050, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:22m:59s remains)
INFO - root - 2017-12-11 07:01:41.971425: step 24060, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 44h:27m:20s remains)
INFO - root - 2017-12-11 07:01:47.329470: step 24070, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 46h:19m:20s remains)
INFO - root - 2017-12-11 07:01:52.672165: step 24080, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 44h:36m:18s remains)
INFO - root - 2017-12-11 07:01:58.059007: step 24090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:34m:46s remains)
INFO - root - 2017-12-11 07:02:03.372931: step 24100, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 45h:36m:24s remains)
2017-12-11 07:02:03.910977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059293143 -0.065934196 -0.072724812 -0.0774638 -0.069514193 -0.046212792 -0.01313113 0.019896276 0.041347671 0.045042895 0.030502392 0.006143495 -0.013873959 -0.017713038 -0.0031433259][-0.075155169 -0.089088485 -0.096902072 -0.092492834 -0.064940751 -0.014140065 0.048129804 0.10435045 0.13816494 0.14059608 0.11215416 0.066048607 0.022312012 -0.0026483794 -0.0051491684][-0.071855046 -0.095211409 -0.10673469 -0.094820619 -0.048169196 0.030126952 0.12233479 0.20286287 0.24933204 0.24933521 0.20528333 0.13550083 0.065427348 0.015749969 -0.0075335773][-0.037319634 -0.069194935 -0.086184449 -0.071248628 -0.013240098 0.082389392 0.19411476 0.29047722 0.34428287 0.34062415 0.28358954 0.19545247 0.10467032 0.034443531 -0.0075868047][0.015110528 -0.022152558 -0.044157654 -0.030719811 0.027663564 0.12465759 0.23806007 0.33515221 0.38816458 0.38223815 0.32244939 0.23165354 0.13678786 0.059893262 0.0082811667][0.058414776 0.02004664 -0.0052879718 0.0029107018 0.051942054 0.13621022 0.23611693 0.32215217 0.37047136 0.36804476 0.31956732 0.24420719 0.16300128 0.094115637 0.043057408][0.072446764 0.038678404 0.013551606 0.014926679 0.048474945 0.11100306 0.1884407 0.25778824 0.30181363 0.30983466 0.28467613 0.23834969 0.18287149 0.13091166 0.085726][0.056769922 0.032365791 0.011351765 0.0064670034 0.022494959 0.059655361 0.11048225 0.16021192 0.19887932 0.21846037 0.219229 0.20516899 0.17922036 0.14812343 0.11251043][0.023595113 0.0097056506 -0.0052337251 -0.014498601 -0.01429585 -0.0013685055 0.02278151 0.051742036 0.082193933 0.10863787 0.12875031 0.13996537 0.13873225 0.127143 0.10361841][-0.014772988 -0.020246586 -0.029655024 -0.041468624 -0.052647788 -0.058290303 -0.055759747 -0.044907775 -0.02379776 0.0037284126 0.033176735 0.057937585 0.071400292 0.073308848 0.060929589][-0.051931612 -0.052243356 -0.057337772 -0.069253907 -0.085089795 -0.099780627 -0.10874338 -0.10819371 -0.09363994 -0.0682094 -0.037885912 -0.010878152 0.0059545082 0.012226088 0.0055466862][-0.082933359 -0.080728151 -0.082659408 -0.092379168 -0.10697097 -0.12219228 -0.13362473 -0.1365829 -0.126531 -0.10609157 -0.081097461 -0.05907229 -0.04549418 -0.040369831 -0.045211263][-0.10034626 -0.098346651 -0.098834649 -0.10534232 -0.11541561 -0.1262857 -0.13503529 -0.13800913 -0.13225144 -0.11990673 -0.10504355 -0.092501029 -0.085254721 -0.082946129 -0.086437143][-0.10219576 -0.10244621 -0.10302214 -0.10632341 -0.11086877 -0.11556949 -0.11931229 -0.12029085 -0.11751202 -0.1128527 -0.10838456 -0.10582928 -0.10573824 -0.10740837 -0.11096569][-0.091495655 -0.094753422 -0.096515663 -0.0976995 -0.097409822 -0.095994025 -0.094029255 -0.091385923 -0.088491276 -0.087383978 -0.089128628 -0.093408637 -0.099288777 -0.10564983 -0.11182462]]...]
INFO - root - 2017-12-11 07:02:09.317206: step 24110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:02m:13s remains)
INFO - root - 2017-12-11 07:02:14.654075: step 24120, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 48h:40m:38s remains)
INFO - root - 2017-12-11 07:02:19.176464: step 24130, loss = 0.66, batch loss = 0.61 (15.0 examples/sec; 0.532 sec/batch; 45h:34m:22s remains)
INFO - root - 2017-12-11 07:02:24.522667: step 24140, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:27m:58s remains)
INFO - root - 2017-12-11 07:02:29.825918: step 24150, loss = 0.69, batch loss = 0.63 (15.9 examples/sec; 0.504 sec/batch; 43h:11m:35s remains)
INFO - root - 2017-12-11 07:02:35.200236: step 24160, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 45h:27m:10s remains)
INFO - root - 2017-12-11 07:02:40.746298: step 24170, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 47h:05m:15s remains)
INFO - root - 2017-12-11 07:02:46.145350: step 24180, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:19m:49s remains)
INFO - root - 2017-12-11 07:02:51.465784: step 24190, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 45h:30m:29s remains)
INFO - root - 2017-12-11 07:02:56.854917: step 24200, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.509 sec/batch; 43h:36m:57s remains)
2017-12-11 07:02:57.380067: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30377847 0.3093442 0.31412083 0.337682 0.38577124 0.45366153 0.51395196 0.52846313 0.4805795 0.38133073 0.26896814 0.17636484 0.11718066 0.0925077 0.087069355][0.295075 0.30644354 0.31856605 0.34608418 0.39352331 0.45698988 0.50985628 0.51731753 0.4655101 0.36583295 0.25207558 0.15491639 0.089699976 0.061930757 0.058971811][0.27746487 0.29115993 0.30577418 0.32976252 0.36768836 0.4164944 0.45483676 0.45272329 0.40051344 0.30673829 0.19906585 0.10503376 0.04051343 0.013149583 0.013338662][0.27997607 0.29221767 0.30339506 0.31941786 0.34677702 0.38378578 0.41289058 0.40638888 0.35576472 0.26603693 0.15954688 0.06242786 -0.0068226932 -0.03874534 -0.039703675][0.28593555 0.29423574 0.29989406 0.30753553 0.32553366 0.35545781 0.38261276 0.37926674 0.33401239 0.24890739 0.14200248 0.038583577 -0.039441217 -0.080409117 -0.087033287][0.28335449 0.28497389 0.28421855 0.28466788 0.29537824 0.3211562 0.35018563 0.35371554 0.31581768 0.23720163 0.13340628 0.028939294 -0.052884247 -0.099194519 -0.11048541][0.27025038 0.26566058 0.25997296 0.25639933 0.26302272 0.28555721 0.31431958 0.32070619 0.28651872 0.21331748 0.11631931 0.019016476 -0.056982469 -0.10012995 -0.11129456][0.25453132 0.2461922 0.23800753 0.23362115 0.23923345 0.25867462 0.2818462 0.28201443 0.2432736 0.17048162 0.08064878 -0.00440704 -0.067360245 -0.10076621 -0.10778252][0.25890082 0.24687085 0.23421951 0.2266366 0.22881928 0.24130914 0.25202817 0.23746018 0.18758452 0.11129427 0.028100805 -0.042305574 -0.0880114 -0.10749265 -0.10739913][0.29457408 0.27639461 0.25328439 0.236273 0.22911602 0.22858855 0.22053438 0.18614239 0.12334662 0.044089749 -0.030994145 -0.085123017 -0.11200861 -0.1159954 -0.10755872][0.35615456 0.33238569 0.2966125 0.2646659 0.2410717 0.2208216 0.19025666 0.13592663 0.063259676 -0.015113831 -0.080350928 -0.11907785 -0.12959398 -0.12052114 -0.10547723][0.42299524 0.39861843 0.35360441 0.3048307 0.25928006 0.21363008 0.15899299 0.088226274 0.011898545 -0.059224423 -0.11148103 -0.13577902 -0.13369498 -0.11613668 -0.09824802][0.46878073 0.44714594 0.3957637 0.33012149 0.25969845 0.18637806 0.11007302 0.030639505 -0.039603662 -0.094303578 -0.1277784 -0.13661401 -0.1245397 -0.10270564 -0.085790865][0.47940856 0.4598836 0.40361631 0.32435384 0.23276004 0.13663165 0.045822408 -0.033339586 -0.089608714 -0.12210746 -0.13370717 -0.12711112 -0.10761265 -0.084856704 -0.071498781][0.4537093 0.43654677 0.37913924 0.29315212 0.19024253 0.08268638 -0.012206864 -0.083791949 -0.12382644 -0.13623308 -0.13021351 -0.11258586 -0.089505516 -0.068412736 -0.059288975]]...]
INFO - root - 2017-12-11 07:03:02.754125: step 24210, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:45m:53s remains)
INFO - root - 2017-12-11 07:03:08.235672: step 24220, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:28m:29s remains)
INFO - root - 2017-12-11 07:03:13.369261: step 24230, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:48m:37s remains)
INFO - root - 2017-12-11 07:03:18.696828: step 24240, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 45h:16m:45s remains)
INFO - root - 2017-12-11 07:03:24.032097: step 24250, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 44h:30m:41s remains)
INFO - root - 2017-12-11 07:03:29.395954: step 24260, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 46h:54m:15s remains)
INFO - root - 2017-12-11 07:03:34.779305: step 24270, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:47m:09s remains)
INFO - root - 2017-12-11 07:03:40.158423: step 24280, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:40m:22s remains)
INFO - root - 2017-12-11 07:03:45.509618: step 24290, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 46h:46m:04s remains)
INFO - root - 2017-12-11 07:03:50.857529: step 24300, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 45h:58m:11s remains)
2017-12-11 07:03:51.387736: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0070086825 0.0012191936 -0.0094028749 -0.017538186 -0.016444398 -0.0040074633 0.01617519 0.038317475 0.054856248 0.059117567 0.048404191 0.025049996 -0.0024553577 -0.026378678 -0.042053077][0.047306404 0.040186327 0.023678858 0.01159885 0.014555054 0.035289377 0.068530366 0.10551631 0.133267 0.14038947 0.12144908 0.07980936 0.030835219 -0.011110708 -0.038048014][0.10887652 0.10412253 0.085522041 0.073729292 0.081920445 0.11298805 0.16033894 0.21175428 0.24824336 0.253484 0.22005555 0.15403184 0.078473106 0.015015019 -0.025174325][0.17792057 0.1805978 0.16669779 0.16135097 0.17792819 0.21940833 0.27869502 0.34028485 0.37964308 0.37678325 0.32431337 0.23228544 0.13027829 0.045571223 -0.0085280761][0.23650672 0.25181878 0.24968064 0.25583676 0.28197762 0.33182108 0.39860833 0.46389675 0.49881756 0.48239571 0.41115367 0.29889634 0.17761958 0.0763752 0.0097478181][0.27885425 0.31167573 0.32687879 0.34710354 0.38027373 0.43171573 0.49733281 0.55739 0.58159345 0.55131447 0.46971351 0.35099268 0.22183245 0.10879517 0.028900553][0.30118981 0.35350972 0.38918737 0.42428347 0.460921 0.50530481 0.55861133 0.60470372 0.61687493 0.58132547 0.50471544 0.39576039 0.26876029 0.1458445 0.049753435][0.29327598 0.35900712 0.40994045 0.45576522 0.49277127 0.5264715 0.5630855 0.59478372 0.60095525 0.57137185 0.51136351 0.42098728 0.302222 0.1729632 0.063366964][0.26110139 0.32566893 0.37725607 0.42238921 0.454881 0.47965372 0.50529909 0.53121126 0.54142082 0.52505243 0.48349854 0.41007239 0.30070361 0.17155357 0.058131061][0.21803559 0.26976055 0.30774394 0.33957687 0.36088064 0.37817106 0.40134194 0.43315175 0.45782843 0.45917317 0.4335044 0.37111178 0.26860842 0.14417011 0.035854448][0.16018385 0.18981434 0.2058574 0.21809368 0.22608393 0.2390102 0.26618484 0.30899787 0.34896609 0.36473909 0.34984267 0.29525623 0.20202412 0.091147676 -0.0011700402][0.080626018 0.084416546 0.0774821 0.072001971 0.072052389 0.086130947 0.11597423 0.15843286 0.1954387 0.20849763 0.19475365 0.14987972 0.078886211 0.00058930687 -0.058036853][0.012645257 -0.0037611886 -0.027913319 -0.046196032 -0.051079195 -0.038089272 -0.016749652 0.006633305 0.020497136 0.016555265 -0.0013019238 -0.03256651 -0.070898242 -0.104761 -0.12054757][-0.024119554 -0.050084982 -0.081265755 -0.10686541 -0.11888178 -0.11583068 -0.11129274 -0.1110304 -0.11843494 -0.13298683 -0.14846727 -0.16358906 -0.17354573 -0.17305189 -0.15826486][-0.05695935 -0.083130024 -0.11240247 -0.13882181 -0.15627836 -0.16376387 -0.1705922 -0.18010338 -0.19163524 -0.2022904 -0.20808384 -0.20945397 -0.20389827 -0.18845244 -0.16296372]]...]
INFO - root - 2017-12-11 07:03:56.810318: step 24310, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:17m:24s remains)
INFO - root - 2017-12-11 07:04:02.221878: step 24320, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:11m:10s remains)
INFO - root - 2017-12-11 07:04:07.253134: step 24330, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.287 sec/batch; 24h:35m:42s remains)
INFO - root - 2017-12-11 07:04:12.477323: step 24340, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 44h:07m:12s remains)
INFO - root - 2017-12-11 07:04:17.832160: step 24350, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:52m:28s remains)
INFO - root - 2017-12-11 07:04:23.208003: step 24360, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.520 sec/batch; 44h:28m:22s remains)
INFO - root - 2017-12-11 07:04:28.434384: step 24370, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 44h:15m:00s remains)
INFO - root - 2017-12-11 07:04:33.778477: step 24380, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:56m:50s remains)
INFO - root - 2017-12-11 07:04:39.189856: step 24390, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:03m:07s remains)
INFO - root - 2017-12-11 07:04:44.490381: step 24400, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:47m:52s remains)
2017-12-11 07:04:45.049116: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26993325 0.21068999 0.13815989 0.081884153 0.050146803 0.038387649 0.035420932 0.033832435 0.036067806 0.059023209 0.12400495 0.23358592 0.35617974 0.44905496 0.485595][0.39820316 0.32557711 0.23141842 0.15886596 0.12022048 0.10689864 0.098463386 0.083562165 0.06841968 0.0749046 0.13061546 0.24244119 0.37817982 0.48984095 0.54330271][0.50274217 0.43185318 0.33181697 0.25761762 0.22608267 0.22231995 0.2133081 0.18219596 0.13900368 0.11249229 0.13665655 0.22499926 0.35061845 0.46535525 0.52890104][0.56269473 0.51070035 0.42511359 0.36686489 0.35739693 0.37421644 0.37165329 0.32677519 0.25292665 0.18610364 0.16579451 0.21317676 0.30941629 0.40925944 0.46982333][0.57323378 0.55125815 0.49874756 0.4725748 0.49612436 0.53935 0.5481348 0.49574879 0.39727893 0.2940647 0.22931336 0.23090763 0.28776583 0.35841525 0.40186146][0.53392327 0.54785079 0.5401659 0.55609304 0.6154235 0.68353617 0.70616424 0.65718156 0.54964429 0.42521679 0.32678261 0.2878263 0.30597267 0.3444142 0.36512581][0.45260695 0.49897289 0.53911871 0.59854716 0.68571115 0.76769012 0.80097693 0.76464862 0.66730386 0.54502654 0.43304044 0.36746708 0.3554776 0.36828232 0.36999813][0.3648307 0.42831552 0.50306588 0.59177357 0.68966764 0.77034879 0.805609 0.78283465 0.70832741 0.60998344 0.50782365 0.43293437 0.40041181 0.39483634 0.38356644][0.29821736 0.36171263 0.44493446 0.53814185 0.62864947 0.69619864 0.7234695 0.70610887 0.65519983 0.59298176 0.51967615 0.45150453 0.40746009 0.38842323 0.36924651][0.22780192 0.28329742 0.35584211 0.43504006 0.50626254 0.55329323 0.56494093 0.5450623 0.51326078 0.48825058 0.45207533 0.4014287 0.35508794 0.32748231 0.30534855][0.12305225 0.16660675 0.22270967 0.28208753 0.33085534 0.35658234 0.35286161 0.3304114 0.31271204 0.31425607 0.30814138 0.27814919 0.23991847 0.21278383 0.19356938][-0.0067685437 0.019232042 0.059639424 0.10116559 0.13138816 0.14213777 0.13078973 0.10977221 0.0985967 0.10755839 0.11312396 0.098737858 0.075133763 0.057448383 0.046608325][-0.12878437 -0.11854026 -0.093518823 -0.068298273 -0.052308343 -0.049855366 -0.061173957 -0.077361725 -0.087114275 -0.083817676 -0.080736376 -0.087703288 -0.098673761 -0.1053528 -0.10610008][-0.2117039 -0.21237534 -0.20129032 -0.19044261 -0.1855305 -0.1876034 -0.19558185 -0.20618738 -0.21549602 -0.21944466 -0.22157533 -0.22596572 -0.22971092 -0.22855727 -0.22023562][-0.24510178 -0.25307673 -0.25093195 -0.24925102 -0.25047213 -0.2540907 -0.25880763 -0.26475936 -0.27210349 -0.27836132 -0.28239951 -0.28551906 -0.28659868 -0.28233016 -0.26926422]]...]
INFO - root - 2017-12-11 07:04:50.444507: step 24410, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 46h:23m:51s remains)
INFO - root - 2017-12-11 07:04:55.808565: step 24420, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 45h:15m:57s remains)
INFO - root - 2017-12-11 07:05:01.113050: step 24430, loss = 0.72, batch loss = 0.67 (15.0 examples/sec; 0.535 sec/batch; 45h:45m:43s remains)
INFO - root - 2017-12-11 07:05:06.267098: step 24440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:51m:26s remains)
INFO - root - 2017-12-11 07:05:11.663527: step 24450, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:01m:50s remains)
INFO - root - 2017-12-11 07:05:17.042102: step 24460, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:30m:14s remains)
INFO - root - 2017-12-11 07:05:22.387452: step 24470, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:37m:33s remains)
INFO - root - 2017-12-11 07:05:27.737803: step 24480, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:39m:36s remains)
INFO - root - 2017-12-11 07:05:33.147629: step 24490, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:12m:55s remains)
INFO - root - 2017-12-11 07:05:38.508940: step 24500, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:15m:05s remains)
2017-12-11 07:05:39.036267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29816255 0.3186695 0.32315749 0.30852303 0.27821428 0.24751209 0.2281934 0.21340443 0.19644718 0.20187934 0.22504988 0.24851778 0.2620582 0.26173857 0.24044897][0.36187339 0.39479837 0.40617505 0.39103436 0.35644966 0.32471237 0.30948371 0.29780659 0.2793749 0.28403163 0.308277 0.33243102 0.34437531 0.33945531 0.30817392][0.38486463 0.42988646 0.44985738 0.43786821 0.40448889 0.37792328 0.37186342 0.36759323 0.35117555 0.3523297 0.36921877 0.38412067 0.38522005 0.36870021 0.32472512][0.38427407 0.44050246 0.47032309 0.46556711 0.43937096 0.42231819 0.42517355 0.42720258 0.41253233 0.4052217 0.40451235 0.39860633 0.37880751 0.34376991 0.28598768][0.369082 0.43624514 0.47691932 0.48257157 0.4697814 0.46869132 0.48134291 0.48603398 0.47009525 0.45074019 0.42594728 0.39059108 0.34374186 0.28812891 0.21951284][0.34957168 0.42834058 0.48442537 0.50819534 0.51783526 0.54100245 0.5671978 0.57130736 0.54824841 0.51209939 0.45990774 0.39147547 0.31676495 0.24245143 0.16562492][0.34513944 0.43513256 0.51046062 0.558904 0.59736085 0.64946187 0.69174528 0.69281971 0.65607268 0.59897625 0.52032363 0.42300698 0.32648671 0.23925804 0.1560733][0.35670021 0.45192939 0.53967667 0.60778743 0.67139542 0.74755192 0.8020134 0.79842412 0.74864906 0.67646736 0.58267045 0.47162452 0.36823925 0.27919871 0.19351646][0.36232632 0.45264009 0.53761351 0.609533 0.68441778 0.77209324 0.83182192 0.8262918 0.77609783 0.7099002 0.62519521 0.52336895 0.4303135 0.34967986 0.26390702][0.33912322 0.41637468 0.48621145 0.54605132 0.61413604 0.69558781 0.75071537 0.74674648 0.71132386 0.673954 0.62366813 0.55312783 0.48328343 0.41600293 0.33014125][0.282868 0.34174773 0.38913649 0.42527235 0.46923739 0.52505213 0.5640077 0.56368774 0.55356628 0.55754751 0.55587882 0.53063345 0.49214321 0.44062129 0.35612804][0.19863318 0.23563838 0.25753915 0.2646938 0.27494362 0.29332638 0.30719683 0.30802497 0.3228204 0.36616561 0.41112325 0.43257877 0.42846394 0.39638522 0.32032961][0.093037352 0.11075718 0.1118498 0.095159896 0.075735904 0.060429864 0.050163891 0.048373722 0.077902988 0.14458078 0.21845073 0.27231887 0.29478121 0.28161144 0.22125588][-0.023401603 -0.02012872 -0.029672436 -0.056467358 -0.08990185 -0.1229637 -0.1477631 -0.15463673 -0.1257614 -0.059700761 0.016709378 0.079392247 0.11327095 0.11443879 0.075525895][-0.12935331 -0.13532883 -0.14722727 -0.1715305 -0.20251848 -0.23545091 -0.26258644 -0.27497658 -0.26040041 -0.21778496 -0.16436459 -0.11566636 -0.084381774 -0.073225193 -0.086965166]]...]
INFO - root - 2017-12-11 07:05:44.407895: step 24510, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:18m:04s remains)
INFO - root - 2017-12-11 07:05:49.784849: step 24520, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 44h:56m:34s remains)
INFO - root - 2017-12-11 07:05:55.035959: step 24530, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.515 sec/batch; 44h:05m:50s remains)
INFO - root - 2017-12-11 07:06:00.051633: step 24540, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:49m:08s remains)
INFO - root - 2017-12-11 07:06:05.325407: step 24550, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.505 sec/batch; 43h:10m:54s remains)
INFO - root - 2017-12-11 07:06:10.727480: step 24560, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 46h:12m:38s remains)
INFO - root - 2017-12-11 07:06:16.077184: step 24570, loss = 0.67, batch loss = 0.62 (15.4 examples/sec; 0.518 sec/batch; 44h:20m:19s remains)
INFO - root - 2017-12-11 07:06:21.507994: step 24580, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:39m:47s remains)
INFO - root - 2017-12-11 07:06:26.854921: step 24590, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:51m:30s remains)
INFO - root - 2017-12-11 07:06:32.170306: step 24600, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.511 sec/batch; 43h:44m:04s remains)
2017-12-11 07:06:32.735411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082755469 -0.066039354 -0.037949976 -0.0065961694 0.023532765 0.043652896 0.049908411 0.043697175 0.027277661 0.0048628775 -0.017364396 -0.032416973 -0.037775554 -0.034563195 -0.022799481][-0.074066378 -0.04119461 0.011153069 0.070467517 0.12719205 0.16626582 0.18032126 0.16910917 0.13659887 0.092158288 0.049855977 0.021033378 0.010430723 0.01681719 0.039188378][-0.057755664 -0.003296158 0.081894644 0.18064344 0.27659357 0.34696159 0.37849015 0.36544466 0.31418869 0.24225962 0.17309971 0.12302341 0.098937467 0.10267814 0.13324459][-0.045352627 0.032467257 0.15362617 0.29847232 0.44280478 0.55464417 0.613578 0.60554153 0.53862488 0.440009 0.34378126 0.2716651 0.2292584 0.22248726 0.25187483][-0.038060885 0.05764816 0.20843597 0.39294392 0.57885587 0.7270298 0.81288058 0.81525666 0.741959 0.6280269 0.517953 0.43719068 0.38403833 0.36442563 0.38202652][-0.038187806 0.0654909 0.23349188 0.44221333 0.65133852 0.81976932 0.92434633 0.94038421 0.87261766 0.76187223 0.6624819 0.59656239 0.54887897 0.52038127 0.51868981][-0.045374833 0.058352605 0.23313615 0.454029 0.67403704 0.85280854 0.97021294 1.0020263 0.94863629 0.85565358 0.78500718 0.749191 0.71890157 0.68781483 0.6654197][-0.057667408 0.039535236 0.21029586 0.4300459 0.64923769 0.82931387 0.95190918 0.99665892 0.96012783 0.89008039 0.85288012 0.8515451 0.84877926 0.8294335 0.79837859][-0.071532428 0.012582261 0.16806589 0.37134767 0.57371563 0.738271 0.84983307 0.895711 0.87034965 0.82033831 0.81185776 0.84299159 0.87713093 0.889032 0.87145633][-0.085789755 -0.019522371 0.1115555 0.28487203 0.45585912 0.59001249 0.67788416 0.71473175 0.69399995 0.65884668 0.6705364 0.72523922 0.79252034 0.83928424 0.84697849][-0.1003847 -0.051501788 0.051542338 0.18887785 0.32356149 0.425341 0.4901554 0.5173766 0.49906576 0.47237077 0.48960316 0.54707694 0.62381166 0.68656361 0.71399713][-0.11627744 -0.0838471 -0.0086090015 0.092983931 0.1935377 0.26766565 0.31385249 0.3323229 0.31523341 0.29146016 0.30035385 0.34013996 0.40115958 0.45840248 0.49520102][-0.13240272 -0.11612734 -0.067315005 0.00028632738 0.068019606 0.11486034 0.14047542 0.14568011 0.12562305 0.10144424 0.098866969 0.1178402 0.15693155 0.20108955 0.2391921][-0.14222971 -0.13785613 -0.10958955 -0.068778746 -0.028025066 -0.0052964692 -0.00041144658 -0.010975724 -0.037351847 -0.0632849 -0.072860278 -0.065551959 -0.041103136 -0.0088794334 0.022127626][-0.14615044 -0.14903186 -0.13450766 -0.11170612 -0.08917895 -0.082176819 -0.090961292 -0.1118504 -0.1413361 -0.16596241 -0.17588675 -0.17117676 -0.1546412 -0.13431592 -0.11878529]]...]
INFO - root - 2017-12-11 07:06:38.070928: step 24610, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:50m:43s remains)
INFO - root - 2017-12-11 07:06:43.450321: step 24620, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 45h:10m:26s remains)
INFO - root - 2017-12-11 07:06:48.822889: step 24630, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 45h:02m:58s remains)
INFO - root - 2017-12-11 07:06:53.973203: step 24640, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:51m:07s remains)
INFO - root - 2017-12-11 07:06:59.364109: step 24650, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:54m:51s remains)
INFO - root - 2017-12-11 07:07:04.768181: step 24660, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:04m:09s remains)
INFO - root - 2017-12-11 07:07:10.123656: step 24670, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:30m:09s remains)
INFO - root - 2017-12-11 07:07:15.551217: step 24680, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:34m:47s remains)
INFO - root - 2017-12-11 07:07:20.965084: step 24690, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 47h:55m:50s remains)
INFO - root - 2017-12-11 07:07:26.279656: step 24700, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 44h:58m:33s remains)
2017-12-11 07:07:26.855419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014043597 -0.01759691 -0.019772947 -0.019749453 -0.017848924 -0.013637295 -0.0077143731 -0.0011473119 0.0036127879 0.0058035213 0.0049328236 0.0012319504 -0.0043477635 -0.0099287285 -0.013508931][-0.032168809 -0.037117276 -0.040178131 -0.040622216 -0.038350243 -0.032441366 -0.023419278 -0.012737668 -0.0033617704 0.0028387739 0.0042472938 0.00059148885 -0.0067001902 -0.014701447 -0.020551676][-0.035998605 -0.03905119 -0.0395543 -0.037530247 -0.032429628 -0.022325434 -0.0081694052 0.0076518622 0.02218966 0.03259505 0.035302363 0.029420771 0.01797189 0.006415619 -0.001822937][-0.018448625 -0.013960248 -0.0063715996 0.0030935747 0.015695781 0.03557647 0.060186289 0.084440745 0.10542931 0.11952169 0.120445 0.10676505 0.084974751 0.065717168 0.05312492][0.012401505 0.030023767 0.051724762 0.074987978 0.102124 0.14040948 0.18328016 0.21951863 0.24645323 0.2608299 0.2540043 0.22434525 0.18430085 0.15250449 0.13328934][0.043832179 0.077976532 0.11897445 0.16299351 0.21313703 0.27906367 0.34679273 0.39458644 0.42132694 0.42765364 0.40542689 0.35353646 0.29149264 0.24643579 0.2215655][0.0696216 0.12279914 0.18739952 0.25717187 0.33461732 0.429564 0.51888114 0.56865513 0.58276969 0.57058746 0.5281626 0.45491403 0.37557596 0.32301438 0.29765725][0.083623663 0.15442492 0.24146916 0.33430263 0.43274161 0.54499614 0.64117849 0.678265 0.66817629 0.63140237 0.57056659 0.48446232 0.39873463 0.34723479 0.32699195][0.074852943 0.15516496 0.25493467 0.35803884 0.46021155 0.5676564 0.64973378 0.6619693 0.62472022 0.56721157 0.49779746 0.41295773 0.33460519 0.29274964 0.28144085][0.035602655 0.11040136 0.20523737 0.29951838 0.3852829 0.46713331 0.52023929 0.50711137 0.45262888 0.38801214 0.32361758 0.25411451 0.19560304 0.17126204 0.17263161][-0.021668535 0.03384079 0.107106 0.1772147 0.23464219 0.28271571 0.30524078 0.27675021 0.22092335 0.164414 0.1163778 0.071612485 0.039924752 0.036165047 0.049994949][-0.074411333 -0.043392271 0.00085684017 0.041380975 0.070137635 0.08950533 0.090869375 0.060643602 0.016783379 -0.021980356 -0.049366459 -0.069332041 -0.077002421 -0.065196887 -0.044790514][-0.10840438 -0.09914203 -0.081052996 -0.064726152 -0.055428386 -0.051943891 -0.058514562 -0.080201074 -0.10564951 -0.1240088 -0.13232273 -0.13368012 -0.12652485 -0.10902658 -0.09029229][-0.11659217 -0.12051029 -0.11783712 -0.11374163 -0.11128782 -0.11106618 -0.1157932 -0.12608366 -0.13510308 -0.13716644 -0.13191618 -0.12205564 -0.10822842 -0.091859736 -0.0793039][-0.095559664 -0.0999326 -0.098953329 -0.095663592 -0.092248164 -0.090367638 -0.091729164 -0.095171832 -0.09583348 -0.090085894 -0.078703843 -0.064537309 -0.050018325 -0.038019121 -0.032522958]]...]
INFO - root - 2017-12-11 07:07:32.160288: step 24710, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:32m:41s remains)
INFO - root - 2017-12-11 07:07:37.569887: step 24720, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:27m:01s remains)
INFO - root - 2017-12-11 07:07:42.960025: step 24730, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 44h:32m:14s remains)
INFO - root - 2017-12-11 07:07:48.250452: step 24740, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.518 sec/batch; 44h:14m:53s remains)
INFO - root - 2017-12-11 07:07:53.272069: step 24750, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.547 sec/batch; 46h:46m:38s remains)
INFO - root - 2017-12-11 07:07:58.535007: step 24760, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.520 sec/batch; 44h:26m:02s remains)
INFO - root - 2017-12-11 07:08:03.824305: step 24770, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 45h:09m:44s remains)
INFO - root - 2017-12-11 07:08:09.209128: step 24780, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.500 sec/batch; 42h:45m:52s remains)
INFO - root - 2017-12-11 07:08:14.597611: step 24790, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.535 sec/batch; 45h:44m:28s remains)
INFO - root - 2017-12-11 07:08:19.827026: step 24800, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:57m:28s remains)
2017-12-11 07:08:20.328143: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.090925172 0.079289615 0.064158008 0.055742204 0.058415242 0.074716404 0.096373908 0.1152939 0.12654036 0.1279131 0.12325867 0.11618239 0.11144429 0.11329774 0.11968624][0.15933533 0.14198069 0.1188662 0.10739793 0.1139558 0.14002366 0.17200656 0.19858295 0.21363527 0.21445613 0.20682371 0.19762415 0.19511305 0.20449162 0.22019386][0.21025361 0.18716539 0.1569743 0.14402829 0.15626386 0.19242586 0.23404436 0.26718128 0.28491455 0.28443182 0.27333343 0.261953 0.26176217 0.27856147 0.30334792][0.23022212 0.20098494 0.16574737 0.15254469 0.17025095 0.21410111 0.26269817 0.30065629 0.32023579 0.31916419 0.30596539 0.29275727 0.29258075 0.31160909 0.33929545][0.23190609 0.19752118 0.15954523 0.14658353 0.16776554 0.21602415 0.27038649 0.31475246 0.33848414 0.33943158 0.3262037 0.31042728 0.30466425 0.31569791 0.33528841][0.2358323 0.19842613 0.15904756 0.14655048 0.16920508 0.22035119 0.2820577 0.33712381 0.36900496 0.37469435 0.36286369 0.34327814 0.32562545 0.31804329 0.31891119][0.26683724 0.22595336 0.18606934 0.17442347 0.19656014 0.24851459 0.31610009 0.38116193 0.42090529 0.43147641 0.42267698 0.40189707 0.37354952 0.3462635 0.3260912][0.32097065 0.2801778 0.2438422 0.23617162 0.257831 0.30831653 0.37635443 0.44404951 0.48523629 0.49710181 0.49294969 0.47805026 0.44955227 0.41371492 0.38139084][0.37629697 0.34342808 0.31567168 0.31441659 0.33539245 0.38123316 0.44202918 0.50171709 0.53592217 0.54526621 0.54864991 0.54914528 0.53625488 0.51053029 0.48179215][0.41624013 0.39672732 0.37826577 0.38082337 0.39810592 0.43469161 0.48083371 0.52358717 0.54460126 0.54881585 0.56056255 0.58096868 0.59329355 0.59089881 0.57795292][0.42695731 0.42095071 0.40856892 0.40938488 0.419226 0.44352096 0.47108749 0.49195826 0.49583864 0.49265212 0.51027179 0.54803848 0.58565336 0.610048 0.6179682][0.41452077 0.41789669 0.40649679 0.39986086 0.39959198 0.41163522 0.42229867 0.42301682 0.41058895 0.39929596 0.41792068 0.46442488 0.51864958 0.56365716 0.59014153][0.39380142 0.39929923 0.38306886 0.36634097 0.35763898 0.36298367 0.36533406 0.35479397 0.33199656 0.3140209 0.32798949 0.37162393 0.42895621 0.48288602 0.52064222][0.38381362 0.38000482 0.35461035 0.32998621 0.31944397 0.32760262 0.33297616 0.32190162 0.29609162 0.27436519 0.28015381 0.3111099 0.35824037 0.40816787 0.44764993][0.37128365 0.35508719 0.32430103 0.30107644 0.29875436 0.31821755 0.33339882 0.32693928 0.30100861 0.27689621 0.27381444 0.28903437 0.320405 0.35967818 0.39420819]]...]
INFO - root - 2017-12-11 07:08:25.658261: step 24810, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:29m:53s remains)
INFO - root - 2017-12-11 07:08:30.973040: step 24820, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 44h:58m:20s remains)
INFO - root - 2017-12-11 07:08:36.322469: step 24830, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:34m:07s remains)
INFO - root - 2017-12-11 07:08:41.658921: step 24840, loss = 0.66, batch loss = 0.60 (14.6 examples/sec; 0.547 sec/batch; 46h:43m:40s remains)
INFO - root - 2017-12-11 07:08:46.686350: step 24850, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 45h:08m:05s remains)
INFO - root - 2017-12-11 07:08:52.013356: step 24860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:48m:39s remains)
INFO - root - 2017-12-11 07:08:57.358310: step 24870, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 44h:51m:22s remains)
INFO - root - 2017-12-11 07:09:02.638317: step 24880, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:42m:41s remains)
INFO - root - 2017-12-11 07:09:08.001033: step 24890, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 47h:20m:01s remains)
INFO - root - 2017-12-11 07:09:13.452764: step 24900, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:54m:13s remains)
2017-12-11 07:09:14.051864: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.073620595 0.10671568 0.16864562 0.2492799 0.33156288 0.39864755 0.44278431 0.46529305 0.47618276 0.4751572 0.45662475 0.413207 0.35092908 0.28582636 0.23125471][0.085639805 0.1335535 0.21767235 0.32570955 0.43502441 0.52345926 0.57871133 0.60009474 0.59898663 0.58208823 0.54951429 0.495479 0.42674068 0.36282229 0.31607282][0.10292804 0.16265883 0.26234317 0.38887721 0.51580763 0.61700356 0.67648268 0.69181508 0.67599529 0.64193648 0.59684783 0.5385496 0.47462112 0.42566773 0.40054202][0.12118355 0.18820763 0.29722419 0.43526876 0.57284379 0.68055725 0.73960531 0.74539775 0.7123124 0.66000819 0.60389966 0.54643804 0.49448648 0.46705085 0.46916419][0.11126484 0.18090537 0.29712915 0.44639722 0.59565556 0.71188283 0.773835 0.77463859 0.72800595 0.65934306 0.59337914 0.5379138 0.4974888 0.48722714 0.51024193][0.065677933 0.13225864 0.2517972 0.40931582 0.56899905 0.69549221 0.76572537 0.77166361 0.72384489 0.64970148 0.58071959 0.52917016 0.496528 0.49292246 0.52268392][0.013242035 0.070372343 0.18356057 0.33746517 0.49739924 0.62919182 0.70957184 0.72921276 0.69378471 0.6286214 0.56643337 0.52168179 0.494282 0.49085715 0.51760459][-0.012324326 0.033689883 0.13387033 0.27474275 0.42782542 0.56258929 0.65592778 0.69377136 0.67498982 0.62021869 0.56097949 0.51426625 0.48310363 0.47513711 0.49846539][0.0021410065 0.042301022 0.13032842 0.25543526 0.39778438 0.53164506 0.63422292 0.68443018 0.67417657 0.61910313 0.5501129 0.48924679 0.44624898 0.43258408 0.45663533][0.066862635 0.10258814 0.17465845 0.276748 0.39707112 0.51515675 0.60992205 0.65644693 0.64180881 0.57743806 0.49386954 0.41931373 0.36947426 0.35774666 0.38951162][0.17794125 0.20085816 0.24552414 0.31075531 0.39181939 0.4734273 0.53896821 0.56443775 0.53496778 0.45954812 0.36682442 0.28898829 0.24534285 0.24846983 0.29805103][0.27688771 0.28035313 0.29148042 0.31507027 0.351628 0.38960052 0.41668472 0.41229346 0.36380655 0.28091651 0.18858524 0.1183166 0.090039961 0.115025 0.18731427][0.29398665 0.28086892 0.2656512 0.25659883 0.256582 0.25607538 0.24628086 0.21250853 0.14906931 0.068305366 -0.0105303 -0.062243931 -0.070447281 -0.026198694 0.061569095][0.22111942 0.19742662 0.16631739 0.13511707 0.10727138 0.076495647 0.038922589 -0.012190469 -0.075646549 -0.13913181 -0.1907554 -0.21446067 -0.20034608 -0.14467277 -0.057947714][0.10751291 0.078854173 0.041296363 -0.001849762 -0.046148509 -0.093103081 -0.1415022 -0.19230261 -0.24186257 -0.28151056 -0.30491719 -0.30345052 -0.27347732 -0.21724471 -0.14569934]]...]
INFO - root - 2017-12-11 07:09:19.344790: step 24910, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:05m:01s remains)
INFO - root - 2017-12-11 07:09:24.711687: step 24920, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 45h:14m:34s remains)
INFO - root - 2017-12-11 07:09:29.976047: step 24930, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:33m:34s remains)
INFO - root - 2017-12-11 07:09:35.308128: step 24940, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 44h:52m:34s remains)
INFO - root - 2017-12-11 07:09:40.432207: step 24950, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 45h:43m:33s remains)
INFO - root - 2017-12-11 07:09:45.825849: step 24960, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:38m:38s remains)
INFO - root - 2017-12-11 07:09:51.195263: step 24970, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:23m:48s remains)
INFO - root - 2017-12-11 07:09:56.481534: step 24980, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 44h:44m:59s remains)
INFO - root - 2017-12-11 07:10:01.871811: step 24990, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:10m:27s remains)
INFO - root - 2017-12-11 07:10:07.239239: step 25000, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:29m:22s remains)
2017-12-11 07:10:07.835753: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51758242 0.48470515 0.39173341 0.27392823 0.16612059 0.10235642 0.10362546 0.17468736 0.28765327 0.39269814 0.45275033 0.43891102 0.35104942 0.22269084 0.097109459][0.63904649 0.576742 0.4454329 0.29828432 0.17505942 0.1128502 0.13197105 0.22766492 0.36063093 0.47190768 0.52334392 0.48958382 0.37701735 0.22641698 0.086634666][0.69312388 0.60279095 0.44802377 0.29638359 0.18356043 0.14177799 0.18478675 0.29909852 0.43725547 0.53639418 0.563016 0.49983937 0.36233658 0.19697072 0.054777406][0.69057828 0.59101713 0.44043 0.31212458 0.23042858 0.2187328 0.28287867 0.40386075 0.53057325 0.6006608 0.59064454 0.494542 0.33432725 0.15911089 0.019194398][0.65203732 0.56949192 0.45276922 0.36887905 0.32941726 0.34882259 0.426069 0.53950244 0.63862038 0.66652858 0.61256009 0.48425466 0.3063077 0.12602486 -0.0099935][0.60167128 0.56326652 0.50545341 0.47984102 0.48728505 0.53268385 0.61030084 0.69939256 0.75402492 0.72827381 0.62498182 0.46507496 0.27442592 0.093890369 -0.036283892][0.5400033 0.55887818 0.57070869 0.60678089 0.65862495 0.72227359 0.78963196 0.84406376 0.84764737 0.76829982 0.62050354 0.43439308 0.2373182 0.061903309 -0.0595824][0.47223839 0.54051709 0.61361486 0.70168447 0.78626966 0.85844928 0.91001946 0.93026286 0.89155293 0.77482754 0.60032839 0.39992493 0.20306118 0.0367934 -0.074243687][0.39681965 0.49615362 0.61363775 0.73935151 0.84432596 0.91901273 0.9555729 0.94855577 0.88000262 0.74308389 0.56011027 0.3588866 0.16983302 0.017062593 -0.080930196][0.3195965 0.43487647 0.58059794 0.73109961 0.84718037 0.9217732 0.946816 0.92076665 0.83242488 0.68507344 0.50373328 0.31144929 0.13744599 0.0019245911 -0.081590608][0.23983428 0.35839766 0.51574045 0.67807281 0.79731834 0.86885417 0.88505495 0.84907085 0.75313032 0.60519 0.43352798 0.25879717 0.10687049 -0.0085492861 -0.078319475][0.1655574 0.27473104 0.4237811 0.57844645 0.68758279 0.7470929 0.75129277 0.70937133 0.61620092 0.48033169 0.33073878 0.18552783 0.06440337 -0.025911348 -0.080046482][0.11541122 0.20844355 0.33189118 0.45739159 0.54086888 0.577554 0.56412166 0.5157941 0.43091932 0.31693667 0.19892466 0.090849631 0.0052319947 -0.056484621 -0.091826983][0.075306967 0.14673038 0.2338421 0.31768754 0.36924067 0.38221872 0.35378775 0.3032043 0.23276027 0.14757185 0.06466084 -0.0068823779 -0.058883343 -0.09301728 -0.10906314][0.024947355 0.071825944 0.1215945 0.16400824 0.18682462 0.18367335 0.15080185 0.1065124 0.055463996 0.000431664 -0.050127413 -0.091156565 -0.11567837 -0.12662822 -0.12571883]]...]
INFO - root - 2017-12-11 07:10:13.219588: step 25010, loss = 0.67, batch loss = 0.62 (14.2 examples/sec; 0.565 sec/batch; 48h:14m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:10:18.595425: step 25020, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:55m:53s remains)
INFO - root - 2017-12-11 07:10:23.968318: step 25030, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:29m:20s remains)
INFO - root - 2017-12-11 07:10:29.254155: step 25040, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:15m:16s remains)
INFO - root - 2017-12-11 07:10:34.661238: step 25050, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:46m:18s remains)
INFO - root - 2017-12-11 07:10:39.799989: step 25060, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:31m:31s remains)
INFO - root - 2017-12-11 07:10:45.232270: step 25070, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 45h:34m:27s remains)
INFO - root - 2017-12-11 07:10:50.576969: step 25080, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:02m:20s remains)
INFO - root - 2017-12-11 07:10:55.890687: step 25090, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:30m:41s remains)
INFO - root - 2017-12-11 07:11:01.176643: step 25100, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:13m:39s remains)
2017-12-11 07:11:01.715895: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16626023 0.15663663 0.15077735 0.14557289 0.14688082 0.16872621 0.21464735 0.26168075 0.28066185 0.25819942 0.2092122 0.14276029 0.06980437 0.00866565 -0.031681191][0.20433667 0.20054029 0.19755024 0.19262947 0.18992093 0.20594376 0.24637984 0.28846636 0.30123085 0.27260321 0.21969883 0.15083368 0.074838839 0.010601441 -0.032210108][0.24689071 0.25089619 0.25303507 0.25079483 0.24621736 0.25570434 0.28707138 0.31887513 0.32067347 0.28278211 0.22353147 0.15149179 0.07369598 0.0090232855 -0.033478264][0.28947335 0.30320215 0.31273791 0.31413355 0.30743292 0.3095597 0.3302812 0.34959111 0.33906883 0.29138434 0.22487147 0.1489272 0.06952858 0.0052593844 -0.03619504][0.31957349 0.34384084 0.36319965 0.37179726 0.36647794 0.3635008 0.37293258 0.37749255 0.35260463 0.29430917 0.21999075 0.14023662 0.060329303 -0.0021014253 -0.041150697][0.31997356 0.35269228 0.38457581 0.4079774 0.41533646 0.41814864 0.42266071 0.41479886 0.37539369 0.30528402 0.2210163 0.13504989 0.0532167 -0.0083577959 -0.045590218][0.29480457 0.32991293 0.37139058 0.41171518 0.44011796 0.46040183 0.47079971 0.45894361 0.41034353 0.3305386 0.23598328 0.14144678 0.054935955 -0.0087267533 -0.046498507][0.26240939 0.29438069 0.33738917 0.38469577 0.42571306 0.46126691 0.4840984 0.48073336 0.43652159 0.35685512 0.25771838 0.15575525 0.062981412 -0.00546254 -0.045966662][0.23406444 0.258071 0.29401204 0.33517432 0.37278247 0.41101393 0.44395828 0.45736989 0.43188527 0.36608005 0.27281532 0.16908167 0.071885109 -0.0017315217 -0.045914967][0.21375556 0.22714363 0.25092918 0.2786825 0.30368853 0.33504263 0.37248591 0.40255728 0.39987034 0.35341895 0.27227107 0.17299385 0.076354846 0.00048013308 -0.046201788][0.20057264 0.20254569 0.21206391 0.22389275 0.2338219 0.25441462 0.29124731 0.33186349 0.34675416 0.31808847 0.25198483 0.16398597 0.075073354 0.0022780229 -0.044558942][0.1946055 0.18583915 0.1800013 0.17476881 0.16938618 0.17842436 0.211293 0.25542438 0.28001729 0.26476124 0.21431583 0.14298217 0.067936696 0.0032761614 -0.041269731][0.19310796 0.17730916 0.15790406 0.13672456 0.11712419 0.11447848 0.1397243 0.17981002 0.20608628 0.19900674 0.16219437 0.10914291 0.051561326 -0.00062839512 -0.039784707][0.20804085 0.1894657 0.15907022 0.12366932 0.090722568 0.075059518 0.087513387 0.11632387 0.13760707 0.13332084 0.10533705 0.067210577 0.026526418 -0.011155358 -0.04183929][0.22659968 0.20859788 0.17104854 0.12540637 0.082785159 0.056594651 0.05609959 0.071933322 0.0854971 0.08043658 0.056599054 0.027771186 -0.00044185258 -0.025371442 -0.04656129]]...]
INFO - root - 2017-12-11 07:11:07.080113: step 25110, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:06m:24s remains)
INFO - root - 2017-12-11 07:11:12.462849: step 25120, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:20m:32s remains)
INFO - root - 2017-12-11 07:11:17.799884: step 25130, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:18m:29s remains)
INFO - root - 2017-12-11 07:11:23.155263: step 25140, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 48h:01m:25s remains)
INFO - root - 2017-12-11 07:11:28.488076: step 25150, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:06m:21s remains)
INFO - root - 2017-12-11 07:11:33.599724: step 25160, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:16m:09s remains)
INFO - root - 2017-12-11 07:11:38.990088: step 25170, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:51m:03s remains)
INFO - root - 2017-12-11 07:11:44.299446: step 25180, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.518 sec/batch; 44h:13m:41s remains)
INFO - root - 2017-12-11 07:11:49.572636: step 25190, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 46h:02m:16s remains)
INFO - root - 2017-12-11 07:11:54.968696: step 25200, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 47h:49m:22s remains)
2017-12-11 07:11:55.535878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059429094 -0.019324189 0.050039478 0.14595445 0.25603345 0.36928219 0.48647159 0.58791131 0.6476261 0.66228545 0.630833 0.55391312 0.42433921 0.26908204 0.13061133][-0.066191792 -0.018870302 0.063759185 0.17873622 0.31395641 0.45688426 0.61238927 0.7544235 0.84459537 0.87502617 0.84178638 0.74416167 0.57360244 0.36858273 0.19053282][-0.071020313 -0.020468934 0.070477635 0.1981017 0.34959549 0.51112294 0.69054139 0.85890639 0.967569 1.0061184 0.97190553 0.86340839 0.6722095 0.44314548 0.25144148][-0.079651237 -0.02923549 0.06658826 0.20323892 0.36678359 0.53962719 0.72868508 0.90571535 1.0165697 1.0501416 1.0094995 0.89696562 0.70737392 0.48549262 0.31154174][-0.0893086 -0.041632477 0.055519 0.19701022 0.3666825 0.54184377 0.72611153 0.89484686 0.9936775 1.011516 0.9611674 0.85312569 0.687234 0.50273806 0.3746177][-0.095894277 -0.051190905 0.045739215 0.18917678 0.35898808 0.52766019 0.69396591 0.84025407 0.9168961 0.91427362 0.85704052 0.76434553 0.63953555 0.51089174 0.44128194][-0.097503938 -0.05405359 0.044086169 0.19037764 0.36037374 0.52211243 0.6677106 0.78693169 0.83746696 0.81448734 0.755387 0.68592507 0.6081062 0.53412414 0.51484311][-0.097047217 -0.053562853 0.046112597 0.19541618 0.36687875 0.524874 0.65408009 0.75004339 0.77944571 0.74417365 0.69081116 0.64699244 0.60824573 0.57143176 0.58026993][-0.094827808 -0.051793583 0.045602731 0.19045399 0.35389423 0.49982893 0.60982877 0.68682033 0.70784515 0.67884409 0.64488637 0.62855631 0.61624616 0.59633321 0.61052871][-0.09200988 -0.051563509 0.036302265 0.16365354 0.30240974 0.42006966 0.50118232 0.5587334 0.58062774 0.57238066 0.56743133 0.57741159 0.58115715 0.564687 0.57067078][-0.090445518 -0.055952333 0.015449654 0.11410274 0.21631563 0.29666817 0.34490833 0.38287345 0.40700734 0.41977674 0.43964565 0.46720186 0.47855002 0.46007827 0.45457658][-0.088689931 -0.060818609 -0.0078686569 0.058405038 0.12225617 0.16702156 0.18630451 0.20508443 0.22401558 0.24350189 0.27201143 0.30426303 0.31822962 0.30204895 0.2928347][-0.086364411 -0.063220449 -0.024851976 0.015115218 0.04831659 0.065376237 0.060811732 0.057227977 0.058005247 0.065057792 0.083827414 0.10870336 0.12335958 0.11876876 0.11991543][-0.0837583 -0.062914096 -0.032641008 -0.0079904664 0.006476542 0.0049642553 -0.01891309 -0.047020122 -0.072527774 -0.088615008 -0.088772282 -0.0763081 -0.060588654 -0.047498643 -0.025042634][-0.081489034 -0.061164055 -0.033321679 -0.01345968 -0.0056595765 -0.015183835 -0.0498083 -0.095682 -0.1427549 -0.17899683 -0.19576612 -0.19464695 -0.17890929 -0.15154381 -0.10838866]]...]
INFO - root - 2017-12-11 07:12:00.891887: step 25210, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:39m:43s remains)
INFO - root - 2017-12-11 07:12:06.329895: step 25220, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 47h:21m:40s remains)
INFO - root - 2017-12-11 07:12:11.683020: step 25230, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 45h:07m:15s remains)
INFO - root - 2017-12-11 07:12:16.984367: step 25240, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.507 sec/batch; 43h:14m:04s remains)
INFO - root - 2017-12-11 07:12:22.335420: step 25250, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 45h:05m:08s remains)
INFO - root - 2017-12-11 07:12:27.390592: step 25260, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 45h:08m:01s remains)
INFO - root - 2017-12-11 07:12:32.789352: step 25270, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:34m:36s remains)
INFO - root - 2017-12-11 07:12:38.112774: step 25280, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:46m:14s remains)
INFO - root - 2017-12-11 07:12:43.486409: step 25290, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 47h:32m:23s remains)
INFO - root - 2017-12-11 07:12:48.799144: step 25300, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:37m:16s remains)
2017-12-11 07:12:49.307612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036374513 -0.04340703 -0.049961604 -0.045579426 -0.014999039 0.051204469 0.15348247 0.27570185 0.40112546 0.51264834 0.59420091 0.63480937 0.65151286 0.66988629 0.69666439][-0.12505376 -0.12924805 -0.11692833 -0.080270283 -0.010873871 0.09071552 0.21663047 0.34943905 0.47575375 0.5839709 0.6597892 0.69498992 0.7046681 0.70952839 0.71439618][-0.18695334 -0.17820269 -0.13794829 -0.063118242 0.047543123 0.18099234 0.32058102 0.44810811 0.5533728 0.6319688 0.67698228 0.68858165 0.67978692 0.66382056 0.64361447][-0.20239112 -0.17805226 -0.11151844 -0.0022031709 0.14643633 0.31007192 0.46110168 0.57611704 0.64488637 0.67178518 0.66398454 0.63647574 0.60412449 0.57364613 0.54361844][-0.17532556 -0.13830331 -0.051152788 0.088695109 0.27605811 0.47527173 0.64288342 0.74345231 0.76282543 0.7164793 0.63321525 0.54995191 0.48997039 0.45504761 0.43357274][-0.12790272 -0.082247958 0.020411203 0.18865174 0.41695929 0.65720659 0.846471 0.93303132 0.89653367 0.76536268 0.59419423 0.44563404 0.35482916 0.31956825 0.31513637][-0.0916081 -0.041925058 0.071587957 0.26386291 0.52923769 0.80776995 1.0178977 1.0916414 1.0038507 0.79618913 0.54587358 0.33688265 0.2149647 0.17514719 0.18157449][-0.074920535 -0.023908708 0.094338804 0.30031139 0.5881772 0.89047521 1.1122261 1.173618 1.0473534 0.78596485 0.48144564 0.22993098 0.083321691 0.034734376 0.041549318][-0.061541475 -0.0092559662 0.10862067 0.31593516 0.60654795 0.9107548 1.1277517 1.1722833 1.0178885 0.72390544 0.39037427 0.11807537 -0.04066366 -0.095460884 -0.091979951][-0.03567566 0.021562273 0.13800022 0.33598402 0.60743809 0.884801 1.071119 1.0862005 0.90983027 0.60450143 0.27089044 0.0048678438 -0.14765577 -0.19975094 -0.19588655][0.0030988313 0.0727131 0.19286746 0.37806857 0.61514735 0.84054726 0.97086304 0.9432677 0.75026828 0.45413315 0.14665528 -0.090132862 -0.22082481 -0.26133686 -0.25147575][0.052224725 0.13987947 0.26793617 0.43946406 0.63457024 0.79612237 0.85975051 0.78532004 0.58145875 0.30826923 0.04285292 -0.15119395 -0.25077176 -0.27412182 -0.25524998][0.092163935 0.19593827 0.33089852 0.487694 0.64016229 0.739482 0.74021196 0.62485576 0.41701576 0.17332166 -0.044498708 -0.1908287 -0.25423372 -0.25613046 -0.22629689][0.095103674 0.20273636 0.33357167 0.46749258 0.57469893 0.617441 0.56837237 0.42946339 0.23438257 0.032119021 -0.1319491 -0.22824873 -0.2543838 -0.23395424 -0.19391786][0.047910281 0.14160612 0.250331 0.34692776 0.40277308 0.39586118 0.31606269 0.1787755 0.019529797 -0.12482877 -0.22592525 -0.26953748 -0.26009688 -0.22057071 -0.17411841]]...]
INFO - root - 2017-12-11 07:12:54.727590: step 25310, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:45m:05s remains)
INFO - root - 2017-12-11 07:13:00.095779: step 25320, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:14m:14s remains)
INFO - root - 2017-12-11 07:13:05.537324: step 25330, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:19m:34s remains)
INFO - root - 2017-12-11 07:13:10.947180: step 25340, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:27m:55s remains)
INFO - root - 2017-12-11 07:13:16.207328: step 25350, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 45h:01m:14s remains)
INFO - root - 2017-12-11 07:13:21.484562: step 25360, loss = 0.70, batch loss = 0.65 (19.6 examples/sec; 0.409 sec/batch; 34h:54m:31s remains)
INFO - root - 2017-12-11 07:13:26.630191: step 25370, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:33m:03s remains)
INFO - root - 2017-12-11 07:13:32.061119: step 25380, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:25m:25s remains)
INFO - root - 2017-12-11 07:13:37.530296: step 25390, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 46h:16m:17s remains)
INFO - root - 2017-12-11 07:13:42.880072: step 25400, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:08m:05s remains)
2017-12-11 07:13:43.426652: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00922933 0.025654025 0.044420727 0.064420618 0.081554 0.093925014 0.101551 0.10329018 0.09822572 0.0827349 0.055108193 0.0181866 -0.020198392 -0.0527412 -0.075115725][0.073780134 0.10132951 0.13214472 0.16613601 0.19653873 0.21916746 0.23291498 0.23471001 0.222821 0.19291569 0.14371504 0.080527321 0.016370252 -0.036680233 -0.072961964][0.16928037 0.20838583 0.25089222 0.29931411 0.34376889 0.37657252 0.39567891 0.39603382 0.37546432 0.32901022 0.25546312 0.16237043 0.068152882 -0.0090550389 -0.061636645][0.28943008 0.32708412 0.37164125 0.42655692 0.47902748 0.51843148 0.54148477 0.54052913 0.51219475 0.45107016 0.35605955 0.23607707 0.11408144 0.014549775 -0.052443203][0.42591205 0.45514128 0.49093002 0.54128993 0.59209293 0.63153237 0.654554 0.65016025 0.61325628 0.53881323 0.42688376 0.28711742 0.14469638 0.029019326 -0.047574382][0.55210012 0.56923878 0.58947963 0.62661636 0.66764265 0.70138985 0.72125423 0.7130509 0.66935962 0.58533818 0.46294454 0.31261733 0.15902427 0.03429744 -0.046989258][0.61838377 0.61941141 0.62205553 0.64476573 0.67663872 0.70807916 0.73020095 0.72479451 0.68039793 0.59191555 0.46418357 0.31050351 0.15452807 0.028394869 -0.051991932][0.60907024 0.58873338 0.57232744 0.58051592 0.60532695 0.64014411 0.67231584 0.67800343 0.64066583 0.55492258 0.42825192 0.27843791 0.12943447 0.010967225 -0.062200289][0.52440578 0.49081984 0.46240517 0.4613038 0.48290518 0.52395642 0.5673908 0.58498317 0.55781919 0.48083022 0.3622621 0.22346544 0.089596167 -0.013227723 -0.074295096][0.37592056 0.34881595 0.32257789 0.32123351 0.34469318 0.39197311 0.442229 0.46650594 0.44790107 0.38283765 0.27755439 0.15450484 0.040348057 -0.042023022 -0.087657087][0.19074568 0.17420109 0.16008464 0.16554359 0.1952775 0.24878116 0.30321041 0.33194229 0.32191136 0.2714299 0.18271255 0.077259719 -0.016523877 -0.077058494 -0.10523866][0.014443795 0.0087109571 0.0076957666 0.020608511 0.054519862 0.10829089 0.1599412 0.18817149 0.18504603 0.15042317 0.081236817 -0.0040063565 -0.076264113 -0.11480922 -0.12527294][-0.11835679 -0.1137896 -0.10386666 -0.087105326 -0.056088276 -0.012142839 0.026315659 0.045566019 0.043717772 0.0223416 -0.025498684 -0.085957572 -0.13311622 -0.14941531 -0.14342673][-0.1994438 -0.18590879 -0.16664699 -0.14900126 -0.12637177 -0.098977543 -0.079332218 -0.073731877 -0.079566039 -0.093323655 -0.12222674 -0.15746106 -0.17953874 -0.17645137 -0.15741897][-0.22695307 -0.21086027 -0.18768431 -0.17189872 -0.15867731 -0.14715724 -0.1440594 -0.15017547 -0.16067496 -0.17093109 -0.18667167 -0.20313869 -0.20720704 -0.19190875 -0.16544697]]...]
INFO - root - 2017-12-11 07:13:48.787206: step 25410, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 46h:36m:13s remains)
INFO - root - 2017-12-11 07:13:54.105415: step 25420, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:11m:19s remains)
INFO - root - 2017-12-11 07:13:59.453165: step 25430, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 45h:56m:47s remains)
INFO - root - 2017-12-11 07:14:04.771840: step 25440, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:04m:31s remains)
INFO - root - 2017-12-11 07:14:10.123842: step 25450, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.535 sec/batch; 45h:39m:21s remains)
INFO - root - 2017-12-11 07:14:15.495415: step 25460, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:24m:16s remains)
INFO - root - 2017-12-11 07:14:20.616771: step 25470, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:18m:28s remains)
INFO - root - 2017-12-11 07:14:25.934592: step 25480, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:57m:45s remains)
INFO - root - 2017-12-11 07:14:31.166107: step 25490, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 45h:12m:34s remains)
INFO - root - 2017-12-11 07:14:36.577907: step 25500, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 47h:04m:28s remains)
2017-12-11 07:14:37.122803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.074187808 -0.053776056 -0.0001378708 0.096687816 0.2312265 0.38012052 0.49665004 0.54734266 0.5213992 0.42362228 0.29800656 0.18765645 0.13774484 0.14705433 0.18109597][-0.078420125 -0.052105952 0.014936257 0.13560355 0.30149922 0.48245674 0.62302762 0.68222719 0.644938 0.52051711 0.36195016 0.2243522 0.15986319 0.16436312 0.19800557][-0.079468355 -0.048928857 0.02880555 0.17080329 0.36321059 0.57050115 0.72989994 0.794137 0.74488068 0.59410924 0.40391886 0.23949687 0.16113642 0.16325799 0.20155041][-0.083054893 -0.049732346 0.035125948 0.19334112 0.40724942 0.63572437 0.808747 0.87459326 0.81447697 0.642122 0.42509207 0.23697993 0.1483018 0.1546707 0.20699421][-0.088788942 -0.055725969 0.031043047 0.19704871 0.42394024 0.66610187 0.84700972 0.91198528 0.84438825 0.66031218 0.42943767 0.23057438 0.14205781 0.16068505 0.23144862][-0.096185721 -0.067010775 0.015181069 0.17748055 0.40380108 0.64747381 0.82915336 0.89301223 0.82505172 0.64458364 0.42077413 0.23244539 0.15862931 0.19514677 0.28249744][-0.10295643 -0.07920073 -0.0059751589 0.14365885 0.35717 0.59070939 0.76763535 0.83341354 0.77456439 0.61192632 0.41198671 0.24890848 0.19485383 0.24336225 0.3354789][-0.10761292 -0.089258946 -0.026496446 0.1071784 0.30295449 0.52220768 0.694362 0.76743436 0.72566485 0.58781409 0.41565138 0.27845794 0.23886724 0.28767318 0.371106][-0.1091509 -0.094811626 -0.040832095 0.078698456 0.25716931 0.46032509 0.624424 0.70195913 0.67711043 0.56497759 0.41994882 0.30613345 0.27712911 0.32119805 0.39157978][-0.1069422 -0.094099931 -0.045062732 0.064281322 0.22741355 0.411441 0.558367 0.62802351 0.61165041 0.52330422 0.40795404 0.32172641 0.30809492 0.35276258 0.41639325][-0.10203278 -0.087361149 -0.03891968 0.064894184 0.21667925 0.38293403 0.50925952 0.56210357 0.54316556 0.46973872 0.38012704 0.32224509 0.32893246 0.38240507 0.44869864][-0.097081825 -0.07907819 -0.027608583 0.076819479 0.22666086 0.38698724 0.50464237 0.54731596 0.52187186 0.44918606 0.36730728 0.32234395 0.34159634 0.403902 0.47648305][-0.094806768 -0.074148387 -0.018545076 0.090261117 0.24521542 0.40978578 0.53172815 0.576615 0.54970181 0.47195974 0.38402632 0.33600226 0.35617486 0.42195722 0.49818751][-0.095845111 -0.074186154 -0.016357895 0.096103042 0.25632209 0.42666993 0.556856 0.61157584 0.59206975 0.51536405 0.42239457 0.36669964 0.37978992 0.44015402 0.51037216][-0.10036423 -0.080324419 -0.023605058 0.089912556 0.25313607 0.42742097 0.56393075 0.62886745 0.62120473 0.552128 0.45974389 0.39732102 0.39851412 0.44476923 0.49825382]]...]
INFO - root - 2017-12-11 07:14:42.490649: step 25510, loss = 0.67, batch loss = 0.61 (13.9 examples/sec; 0.577 sec/batch; 49h:14m:03s remains)
INFO - root - 2017-12-11 07:14:47.816819: step 25520, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:01m:32s remains)
INFO - root - 2017-12-11 07:14:53.236648: step 25530, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:32m:01s remains)
INFO - root - 2017-12-11 07:14:58.584377: step 25540, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:56m:47s remains)
INFO - root - 2017-12-11 07:15:03.840798: step 25550, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:38m:53s remains)
INFO - root - 2017-12-11 07:15:09.196568: step 25560, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:31m:15s remains)
INFO - root - 2017-12-11 07:15:14.227408: step 25570, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:45m:34s remains)
INFO - root - 2017-12-11 07:15:19.559705: step 25580, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 45h:53m:34s remains)
INFO - root - 2017-12-11 07:15:24.841596: step 25590, loss = 0.69, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 43h:41m:52s remains)
INFO - root - 2017-12-11 07:15:30.194416: step 25600, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:51m:57s remains)
2017-12-11 07:15:30.753414: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18153287 0.23305184 0.27865413 0.32467091 0.36856097 0.39901644 0.40243679 0.36867571 0.29499343 0.19270079 0.087013647 0.0036517489 -0.047041375 -0.068858974 -0.071748719][0.20689988 0.26446494 0.31521589 0.36444679 0.40837753 0.43737745 0.43689144 0.39641157 0.31300068 0.20004062 0.086216107 -0.0021435854 -0.0542317 -0.076015905 -0.077757649][0.19586843 0.25361508 0.30509853 0.35444993 0.39802295 0.42773941 0.42975754 0.39185944 0.30953005 0.19606285 0.081490204 -0.0071160891 -0.058857996 -0.079655074 -0.0799137][0.15048634 0.20504673 0.25675729 0.30877578 0.3574076 0.39486593 0.40720889 0.37999019 0.30601808 0.19660495 0.082430422 -0.0073645785 -0.060132571 -0.081123628 -0.080972247][0.078143708 0.12636098 0.17756762 0.2341318 0.29226044 0.34301034 0.37108943 0.35974714 0.29919967 0.19771115 0.085371666 -0.0059399111 -0.060537603 -0.082269266 -0.082045719][-0.0054146503 0.032987576 0.081260458 0.14111237 0.20905568 0.27439007 0.3195492 0.32551163 0.28080592 0.1909835 0.0840116 -0.006440579 -0.061812632 -0.083883576 -0.083317339][-0.075654693 -0.049625676 -0.0091533884 0.047743075 0.11927021 0.19414166 0.25302935 0.27478865 0.24702825 0.17289925 0.076408692 -0.0089519881 -0.0629174 -0.084673062 -0.083773993][-0.11763423 -0.10495673 -0.076687835 -0.029406605 0.038176622 0.11625098 0.18490419 0.220835 0.21012221 0.15325861 0.069602616 -0.0088558663 -0.060917437 -0.08299312 -0.082767323][-0.1292287 -0.12855583 -0.11365382 -0.0787745 -0.01910108 0.058314808 0.13384317 0.18205628 0.18638322 0.14411363 0.070692964 -0.0029712955 -0.054837372 -0.078826845 -0.080654457][-0.11586168 -0.12397664 -0.12072246 -0.098612323 -0.049495943 0.023203835 0.10130218 0.15816188 0.17418094 0.14309414 0.077044591 0.0060576997 -0.046854656 -0.073571272 -0.078149177][-0.091978975 -0.10371342 -0.10778674 -0.096436255 -0.059233844 0.0042333435 0.078877926 0.13885663 0.16254947 0.14038627 0.08127407 0.013211832 -0.040235352 -0.0690607 -0.075949319][-0.072510928 -0.083232217 -0.089357734 -0.084598176 -0.057812788 -0.0053551844 0.061657611 0.11991774 0.14744771 0.13245231 0.080675036 0.016772836 -0.035971992 -0.06570977 -0.0739668][-0.062099926 -0.069705933 -0.07448037 -0.072717816 -0.053823374 -0.012082107 0.0450818 0.097979173 0.12593386 0.11668049 0.073041752 0.015378114 -0.034456711 -0.063361354 -0.071850926][-0.058773413 -0.063220128 -0.0658811 -0.065804265 -0.053784274 -0.023114378 0.02197499 0.066161461 0.0916972 0.087611377 0.054246273 0.0065628644 -0.036589984 -0.062116772 -0.0695786][-0.05669285 -0.059783131 -0.061862156 -0.064137638 -0.058921609 -0.039240982 -0.0064187865 0.02853841 0.051135138 0.052429073 0.030783605 -0.0045561106 -0.038984638 -0.060206845 -0.066598147]]...]
INFO - root - 2017-12-11 07:15:36.111684: step 25610, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:21m:48s remains)
INFO - root - 2017-12-11 07:15:41.475663: step 25620, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 44h:13m:37s remains)
INFO - root - 2017-12-11 07:15:46.893663: step 25630, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.584 sec/batch; 49h:45m:26s remains)
INFO - root - 2017-12-11 07:15:52.188361: step 25640, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:09m:43s remains)
INFO - root - 2017-12-11 07:15:57.530855: step 25650, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:22m:15s remains)
INFO - root - 2017-12-11 07:16:02.881054: step 25660, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:06m:09s remains)
INFO - root - 2017-12-11 07:16:07.940521: step 25670, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 24h:49m:00s remains)
INFO - root - 2017-12-11 07:16:13.287582: step 25680, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 45h:03m:08s remains)
INFO - root - 2017-12-11 07:16:18.690062: step 25690, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:40m:29s remains)
INFO - root - 2017-12-11 07:16:23.981319: step 25700, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:09m:55s remains)
2017-12-11 07:16:24.486948: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014017747 0.013996975 0.036545146 0.057496171 0.059648167 0.046050847 0.026864549 0.011605641 0.013285714 0.044003353 0.10778075 0.19649543 0.28121945 0.33075708 0.32990479][0.0094586341 0.010426479 0.035803113 0.064501852 0.080996022 0.08045356 0.063723676 0.040640973 0.028585752 0.046653673 0.1043938 0.19629857 0.289847 0.34672818 0.34518224][0.025089685 0.037525184 0.076340117 0.12412492 0.16455206 0.18402466 0.17148453 0.13514307 0.095979437 0.081098743 0.10956196 0.18061374 0.26369649 0.31786707 0.31768337][0.057861242 0.092885435 0.15538438 0.2316477 0.30398357 0.3493633 0.34509584 0.29456246 0.22118686 0.15930243 0.13768281 0.16451304 0.216887 0.25697479 0.25808129][0.091615111 0.15273747 0.24152467 0.3475804 0.45245329 0.52525693 0.53433663 0.47653511 0.3744123 0.264983 0.18360557 0.15023416 0.15563475 0.17118946 0.17149381][0.10523742 0.1866312 0.2933332 0.41926771 0.5477435 0.64396709 0.67144763 0.61982554 0.50610775 0.36387137 0.230425 0.13781846 0.093323171 0.0805956 0.077386938][0.0898269 0.17747413 0.28664002 0.41462576 0.54954064 0.65789819 0.7037521 0.67086226 0.56683093 0.41627511 0.25378269 0.12068671 0.038903765 0.0036619417 -0.0036381227][0.049684763 0.129413 0.22482462 0.33567244 0.45639327 0.560054 0.61675835 0.60800415 0.53034526 0.39631554 0.23412974 0.089004695 -0.0085539706 -0.055276241 -0.0654155][-0.0023423233 0.056716859 0.12482407 0.20363231 0.29347274 0.37665406 0.4325532 0.44464198 0.40049633 0.3010475 0.16582216 0.036668941 -0.054696836 -0.10103783 -0.11110025][-0.045233838 -0.013998968 0.019212563 0.058177888 0.10847586 0.16210783 0.20783299 0.23300837 0.22009651 0.16155638 0.068480343 -0.026558075 -0.09638299 -0.13279028 -0.13918024][-0.060398426 -0.053141005 -0.050992936 -0.048214249 -0.034796562 -0.010078043 0.021171032 0.049801961 0.057702653 0.033703998 -0.017300393 -0.074561253 -0.1181689 -0.1405112 -0.14164785][-0.044865277 -0.050509159 -0.067196786 -0.088059634 -0.10016182 -0.096283361 -0.078261331 -0.052988589 -0.035867296 -0.038093142 -0.058143128 -0.08621645 -0.10893185 -0.11951523 -0.11699811][-0.013749355 -0.021651566 -0.044328891 -0.075005144 -0.098950565 -0.10473707 -0.093653537 -0.072316214 -0.052692242 -0.044736877 -0.047983583 -0.060531724 -0.073376291 -0.079628132 -0.0783309][0.0073271985 0.0037494523 -0.015643729 -0.04519473 -0.069490477 -0.075082973 -0.064137757 -0.044254996 -0.024957223 -0.014828408 -0.013175535 -0.022352414 -0.035609841 -0.04478367 -0.0489167][-0.0018099719 0.0007244444 -0.010406408 -0.031776614 -0.049227081 -0.049498867 -0.035832915 -0.016391106 0.00094642071 0.0088024773 0.0085148588 -0.0038631766 -0.022313496 -0.03730087 -0.047685087]]...]
INFO - root - 2017-12-11 07:16:29.793326: step 25710, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 43h:53m:00s remains)
INFO - root - 2017-12-11 07:16:35.141776: step 25720, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 45h:29m:29s remains)
INFO - root - 2017-12-11 07:16:40.528202: step 25730, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 45h:02m:46s remains)
INFO - root - 2017-12-11 07:16:45.876548: step 25740, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:29m:08s remains)
INFO - root - 2017-12-11 07:16:51.337669: step 25750, loss = 0.69, batch loss = 0.63 (13.5 examples/sec; 0.592 sec/batch; 50h:24m:16s remains)
INFO - root - 2017-12-11 07:16:56.759510: step 25760, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:53m:46s remains)
INFO - root - 2017-12-11 07:17:02.083638: step 25770, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:17m:24s remains)
INFO - root - 2017-12-11 07:17:07.060996: step 25780, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 43h:52m:07s remains)
INFO - root - 2017-12-11 07:17:12.484777: step 25790, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.583 sec/batch; 49h:39m:34s remains)
INFO - root - 2017-12-11 07:17:17.929960: step 25800, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 44h:02m:29s remains)
2017-12-11 07:17:18.486814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050112169 -0.025804766 0.017662182 0.073057629 0.12662645 0.16716112 0.18687068 0.19036709 0.18635862 0.18708165 0.20065752 0.224842 0.25782087 0.29456061 0.34081897][-0.0548731 -0.026384935 0.026454233 0.095613375 0.16177283 0.20699136 0.21933305 0.20642756 0.18262295 0.16553198 0.1668098 0.18538162 0.21619989 0.25237706 0.29780608][-0.055706136 -0.022032471 0.042184118 0.12782121 0.20810404 0.25751004 0.261779 0.23236397 0.18965553 0.15599911 0.14553586 0.15652969 0.18026258 0.20839094 0.24452217][-0.054714672 -0.015944695 0.058579914 0.15953562 0.25495991 0.31440052 0.32190064 0.28849551 0.23616774 0.18983723 0.16658935 0.16548412 0.17643042 0.19121236 0.21482301][-0.05343654 -0.014063962 0.062560268 0.16890113 0.27373281 0.34653279 0.37005332 0.35003191 0.30346349 0.25328887 0.21846007 0.20179808 0.19647969 0.19638495 0.20672919][-0.052269373 -0.016089635 0.055708565 0.15885566 0.26730508 0.35383254 0.39999878 0.40495357 0.37796456 0.33606887 0.29636762 0.26694947 0.24808815 0.23590396 0.23233156][-0.052546803 -0.019902825 0.046374422 0.14622526 0.25935432 0.36058691 0.4284789 0.45765439 0.45232776 0.42333892 0.38349032 0.34653443 0.32246915 0.30856928 0.29905716][-0.051838413 -0.019700814 0.044997506 0.14474651 0.26249689 0.37341791 0.45369002 0.49816641 0.50965512 0.49341166 0.45684597 0.41861734 0.39954323 0.39650092 0.39217404][-0.050072856 -0.016242692 0.049931049 0.15065455 0.26904383 0.37995419 0.45903951 0.50596434 0.5236991 0.51369047 0.47969162 0.44524989 0.44046029 0.45912781 0.47108287][-0.050686587 -0.015786042 0.052276414 0.15302779 0.26700148 0.36806998 0.43327481 0.47061375 0.48433992 0.47419819 0.44313934 0.41729367 0.43054202 0.47236794 0.50422263][-0.057696171 -0.024862306 0.041332323 0.13761573 0.24164343 0.32678103 0.3723276 0.39331374 0.39544582 0.37862739 0.34781674 0.3288863 0.3542603 0.40999722 0.45685139][-0.069882743 -0.043408968 0.013391374 0.095611267 0.18083911 0.24482755 0.27023554 0.27505538 0.26513204 0.24128172 0.21096854 0.19749156 0.22758919 0.28572389 0.33905268][-0.079694144 -0.059645459 -0.01490746 0.048890997 0.11100649 0.15177958 0.15837914 0.14949094 0.13115074 0.1049596 0.079533972 0.07361263 0.10521531 0.15940757 0.21392402][-0.084870733 -0.068513133 -0.031443618 0.021736236 0.071195476 0.098938465 0.093621463 0.073280208 0.046389576 0.01683717 -0.0050332644 -0.003934911 0.029056653 0.078997865 0.13390122][-0.082658239 -0.065289691 -0.027945736 0.027746 0.08150249 0.11202408 0.10458133 0.075619116 0.038610034 0.001156209 -0.022236085 -0.017090762 0.017957788 0.064387031 0.11567081]]...]
INFO - root - 2017-12-11 07:17:23.846890: step 25810, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:25m:26s remains)
INFO - root - 2017-12-11 07:17:29.205091: step 25820, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.570 sec/batch; 48h:33m:12s remains)
INFO - root - 2017-12-11 07:17:34.586223: step 25830, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 44h:29m:01s remains)
INFO - root - 2017-12-11 07:17:39.994737: step 25840, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:19m:31s remains)
INFO - root - 2017-12-11 07:17:45.307557: step 25850, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.560 sec/batch; 47h:43m:27s remains)
INFO - root - 2017-12-11 07:17:50.677487: step 25860, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 44h:57m:38s remains)
INFO - root - 2017-12-11 07:17:56.104070: step 25870, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 46h:09m:59s remains)
INFO - root - 2017-12-11 07:18:01.196746: step 25880, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 45h:42m:34s remains)
INFO - root - 2017-12-11 07:18:06.563416: step 25890, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:53m:30s remains)
INFO - root - 2017-12-11 07:18:11.911147: step 25900, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:45m:29s remains)
2017-12-11 07:18:12.505457: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.083611317 0.07373327 0.052770618 0.035083994 0.026225589 0.023416292 0.02303916 0.022366272 0.019157095 0.016249316 0.018328236 0.027587403 0.039954741 0.046038162 0.036956623][0.19892359 0.18564719 0.15221739 0.12462427 0.11381788 0.11421602 0.11665978 0.11549079 0.10775937 0.099704675 0.099568672 0.11087191 0.12704869 0.1324773 0.11234688][0.30492681 0.28960127 0.25014117 0.22459489 0.22757904 0.24698997 0.26366627 0.26658085 0.25211337 0.23139481 0.21770106 0.21865785 0.22841161 0.22660983 0.19196177][0.37257025 0.35539436 0.31844252 0.30946273 0.34297168 0.39686087 0.43944535 0.45215595 0.43001327 0.38846213 0.34624696 0.31868288 0.3062081 0.28809357 0.23748508][0.39949685 0.38321432 0.35675836 0.37392861 0.4454982 0.5381763 0.60801256 0.62978542 0.59766489 0.53082335 0.45410323 0.39095375 0.34953624 0.31131855 0.2462972][0.38215432 0.37115762 0.36435965 0.41652054 0.53186905 0.66559684 0.7617994 0.78874606 0.74144852 0.64475846 0.53191543 0.43381429 0.36481008 0.30833858 0.23304093][0.31488249 0.31185767 0.32983112 0.42040887 0.5821538 0.76089925 0.88932 0.92709523 0.86903232 0.745063 0.59685481 0.46262455 0.36414298 0.28962734 0.20752627][0.256236 0.25931019 0.2919288 0.40425453 0.59411287 0.80239457 0.9538179 1.0022619 0.94276714 0.80632532 0.637752 0.4800691 0.3612493 0.27520034 0.18989611][0.27605024 0.27964655 0.30689555 0.41011038 0.59193426 0.7928859 0.93611193 0.97741872 0.91730744 0.78598177 0.62654918 0.47948849 0.36963955 0.29028356 0.2058261][0.35759732 0.35628507 0.36077121 0.42992315 0.57349575 0.7360853 0.844831 0.86364228 0.80282831 0.69200063 0.56993425 0.46719652 0.39533323 0.33833721 0.25596678][0.4433918 0.43138063 0.40563878 0.43217498 0.52806133 0.64523858 0.71656942 0.71534258 0.66053694 0.57940936 0.50227433 0.44803944 0.41482446 0.37755176 0.29420063][0.49397072 0.46708876 0.40995705 0.39265615 0.43877658 0.50988621 0.5454514 0.5286532 0.48416024 0.43602407 0.40244839 0.38989195 0.38686067 0.36442149 0.28348079][0.46637639 0.42670047 0.34921846 0.29912075 0.302071 0.32834268 0.32656747 0.29101577 0.25134319 0.23024856 0.23451518 0.25876719 0.28378925 0.27961776 0.2126115][0.34901115 0.3016623 0.22019188 0.15637684 0.13189568 0.12416378 0.094614811 0.046485119 0.011389565 0.0096915783 0.039932664 0.08885251 0.13320628 0.14541706 0.098880522][0.18634538 0.13933286 0.066377081 0.0051156925 -0.028633676 -0.051354662 -0.08876963 -0.13381955 -0.15867288 -0.14872432 -0.11032091 -0.058218807 -0.012856099 0.0049368707 -0.022769753]]...]
INFO - root - 2017-12-11 07:18:17.846154: step 25910, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.529 sec/batch; 45h:01m:50s remains)
INFO - root - 2017-12-11 07:18:23.199449: step 25920, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:37m:27s remains)
INFO - root - 2017-12-11 07:18:28.500909: step 25930, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 45h:58m:54s remains)
INFO - root - 2017-12-11 07:18:33.843420: step 25940, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 47h:16m:10s remains)
INFO - root - 2017-12-11 07:18:39.220375: step 25950, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 44h:40m:44s remains)
INFO - root - 2017-12-11 07:18:44.675997: step 25960, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 46h:48m:44s remains)
INFO - root - 2017-12-11 07:18:49.854805: step 25970, loss = 0.67, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 44h:22m:17s remains)
INFO - root - 2017-12-11 07:18:54.945857: step 25980, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 45h:40m:49s remains)
INFO - root - 2017-12-11 07:19:00.407913: step 25990, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:00m:11s remains)
INFO - root - 2017-12-11 07:19:05.778975: step 26000, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 45h:41m:06s remains)
2017-12-11 07:19:06.351236: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34407759 0.31076771 0.27475974 0.25593463 0.2420584 0.22673611 0.20816019 0.1880452 0.16734168 0.148322 0.13676526 0.1440911 0.16819851 0.19489884 0.21286552][0.33478519 0.30912033 0.28105116 0.2713142 0.26728791 0.259238 0.24433112 0.2250569 0.2024619 0.17903301 0.16093257 0.16113257 0.17775217 0.19627045 0.20504335][0.27390784 0.261429 0.24921276 0.25482616 0.26501557 0.26956847 0.26417211 0.2519637 0.23284741 0.20759735 0.18215202 0.1696732 0.17103672 0.17428903 0.17039151][0.2003438 0.20469224 0.21204378 0.23425461 0.25837454 0.27648249 0.28460622 0.28491816 0.27469006 0.25124913 0.21872361 0.19081512 0.1733698 0.15943222 0.1430485][0.14544217 0.16770944 0.19561516 0.23275435 0.26718375 0.2952607 0.31631178 0.33081374 0.33280739 0.31516805 0.27854779 0.23668815 0.20121165 0.17155585 0.14493524][0.128078 0.16852407 0.21438144 0.26253679 0.30269179 0.33573642 0.36561123 0.39073291 0.40265167 0.39126149 0.35399458 0.30439198 0.257912 0.21891788 0.18692669][0.15040573 0.20685284 0.26690066 0.32240802 0.36469606 0.39820039 0.43077835 0.45891312 0.4731001 0.46329036 0.42646059 0.3763912 0.32923278 0.29044747 0.25922906][0.18299562 0.25363407 0.326873 0.39117938 0.43725002 0.46997049 0.49972391 0.52252418 0.53012031 0.51529551 0.47773319 0.43266997 0.39464155 0.36521125 0.34001344][0.20015387 0.28176022 0.36700881 0.44203445 0.4945538 0.5259856 0.54734844 0.55772537 0.55341965 0.53089768 0.49293843 0.45684317 0.43409809 0.4192566 0.40292341][0.19377537 0.2810522 0.37373793 0.45726046 0.51578206 0.54476053 0.55385381 0.548183 0.53149134 0.50403261 0.46906376 0.44475552 0.43914729 0.43999082 0.43385017][0.16571167 0.24972557 0.34083527 0.42609158 0.48707578 0.51322192 0.51173574 0.492533 0.46635833 0.43730488 0.40757447 0.39564833 0.40568459 0.42051861 0.42445371][0.12578693 0.19669685 0.27564603 0.35338667 0.41013968 0.43155214 0.4236432 0.39791232 0.36839503 0.34106421 0.3168973 0.31549537 0.33761224 0.36326772 0.3757925][0.081036277 0.13239168 0.19169196 0.2537885 0.29895246 0.31315243 0.30351543 0.27994704 0.25489643 0.23266321 0.21318635 0.21803151 0.24592894 0.27622542 0.29317632][0.037326351 0.068929955 0.10639364 0.1473804 0.17467944 0.17909613 0.17061856 0.15536793 0.14034812 0.12591721 0.11141375 0.11994822 0.14877962 0.17807278 0.19475904][-0.015314644 -0.0016950703 0.015815005 0.036197312 0.045823541 0.042061396 0.037700962 0.033874217 0.030967908 0.025707418 0.017535968 0.029099142 0.056146473 0.08057718 0.093027294]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:19:11.728710: step 26010, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:38m:50s remains)
INFO - root - 2017-12-11 07:19:17.084171: step 26020, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 44h:56m:02s remains)
INFO - root - 2017-12-11 07:19:22.378139: step 26030, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 44h:47m:05s remains)
INFO - root - 2017-12-11 07:19:27.654787: step 26040, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:11m:32s remains)
INFO - root - 2017-12-11 07:19:33.065901: step 26050, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:57m:45s remains)
INFO - root - 2017-12-11 07:19:38.348372: step 26060, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.518 sec/batch; 44h:05m:42s remains)
INFO - root - 2017-12-11 07:19:43.672098: step 26070, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:07m:50s remains)
INFO - root - 2017-12-11 07:19:49.069491: step 26080, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 45h:14m:00s remains)
INFO - root - 2017-12-11 07:19:54.201463: step 26090, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:57m:12s remains)
INFO - root - 2017-12-11 07:19:59.464371: step 26100, loss = 0.68, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 43h:55m:39s remains)
2017-12-11 07:20:00.008203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11902268 0.11436941 0.10434277 0.1030115 0.1179736 0.14700928 0.17867869 0.20540047 0.2226333 0.22288384 0.21159583 0.20488651 0.206993 0.20542458 0.19195385][0.14317603 0.13803372 0.12899464 0.13345537 0.15996629 0.20314832 0.2466685 0.27886188 0.29295638 0.28187275 0.25545397 0.2359771 0.22975975 0.22415204 0.20901895][0.16595316 0.16086036 0.15350531 0.16446006 0.20332226 0.26140207 0.31605476 0.34931555 0.35363489 0.32629684 0.28247708 0.24947801 0.23598552 0.23039077 0.22085254][0.2036346 0.19607113 0.18687698 0.20052753 0.24870643 0.3192443 0.38216427 0.41285861 0.40467286 0.36161152 0.30291036 0.25883082 0.24043192 0.23773417 0.23711115][0.24657619 0.23839962 0.22669882 0.23966862 0.29065254 0.3649056 0.42805246 0.45337585 0.43710589 0.38824672 0.32461479 0.27555615 0.25359365 0.25128487 0.25529251][0.26176462 0.2596626 0.25239861 0.26775038 0.31868905 0.38985971 0.4469924 0.46589518 0.44616944 0.39913511 0.33702517 0.28599715 0.25995964 0.25491706 0.25967681][0.23616196 0.24382044 0.24769315 0.27158955 0.32604611 0.39478314 0.44501108 0.45660689 0.43218678 0.38541096 0.32278854 0.26763594 0.23712073 0.230884 0.23998994][0.17875481 0.19586347 0.21566056 0.2563273 0.32350174 0.39689425 0.4432601 0.44585928 0.4101696 0.35504949 0.28538582 0.22332959 0.18969291 0.18644597 0.20523921][0.11026881 0.13472737 0.16996004 0.22807944 0.30913997 0.38849828 0.43284768 0.42750904 0.37941188 0.3134194 0.23619443 0.16987219 0.13697997 0.14025812 0.17079934][0.046611644 0.0758425 0.12359922 0.19608045 0.28781247 0.37065908 0.41240305 0.39867017 0.33741939 0.25960672 0.17704691 0.11317496 0.0899968 0.10869095 0.15730262][0.00091044622 0.028991658 0.083388634 0.16621156 0.26668352 0.35208875 0.39023396 0.36646777 0.29075906 0.20023659 0.11388727 0.057562653 0.051732276 0.094918184 0.17073582][-0.023088144 -0.0036119006 0.048452035 0.13449408 0.23861402 0.32283613 0.35482693 0.3211346 0.23486899 0.13795398 0.054837216 0.012976655 0.029500455 0.0995536 0.20281067][-0.035277706 -0.030088663 0.0090104947 0.084402688 0.17850128 0.25238726 0.27651843 0.23972215 0.15698378 0.070198216 0.0044661369 -0.01397083 0.026613528 0.11894795 0.24006031][-0.040145051 -0.050223544 -0.03237683 0.017286893 0.0847529 0.13738091 0.15222369 0.12046321 0.055989411 -0.0059243357 -0.044777445 -0.037753366 0.021769891 0.1263445 0.25258827][-0.042962145 -0.062036067 -0.06232246 -0.039408628 -0.0021985609 0.026563901 0.03190374 0.0077348063 -0.035325162 -0.072748631 -0.090632483 -0.069745757 -0.0061280313 0.095386013 0.21408801]]...]
INFO - root - 2017-12-11 07:20:05.339978: step 26110, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:15m:00s remains)
INFO - root - 2017-12-11 07:20:10.657800: step 26120, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.525 sec/batch; 44h:41m:12s remains)
INFO - root - 2017-12-11 07:20:15.988518: step 26130, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:40m:11s remains)
INFO - root - 2017-12-11 07:20:21.373834: step 26140, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 43h:46m:07s remains)
INFO - root - 2017-12-11 07:20:26.710855: step 26150, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:46m:59s remains)
INFO - root - 2017-12-11 07:20:32.090487: step 26160, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.512 sec/batch; 43h:33m:19s remains)
INFO - root - 2017-12-11 07:20:37.399780: step 26170, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:34m:16s remains)
INFO - root - 2017-12-11 07:20:42.707657: step 26180, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 44h:52m:56s remains)
INFO - root - 2017-12-11 07:20:47.880251: step 26190, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 47h:08m:41s remains)
INFO - root - 2017-12-11 07:20:53.277997: step 26200, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:17m:12s remains)
2017-12-11 07:20:53.864890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06763728 -0.058796018 -0.043930762 -0.030097654 -0.019917699 -0.014921 -0.01481536 -0.017135566 -0.016738972 -0.012028208 -0.0057521556 -0.0022834151 -0.0039551584 -0.013170959 -0.030589769][-0.053496208 -0.031311393 0.0017587319 0.034772605 0.061713647 0.077881075 0.082189351 0.077155448 0.073202558 0.075210512 0.081053779 0.0838715 0.0791102 0.060989942 0.026705682][-0.027128611 0.016305355 0.077184573 0.14073838 0.19615473 0.23341276 0.24849726 0.24196874 0.23044145 0.22460365 0.22486524 0.22236067 0.20990546 0.17797178 0.11974626][0.0011537857 0.070005417 0.16472095 0.26534843 0.35580876 0.420611 0.45208791 0.44732878 0.42936212 0.41656592 0.41125992 0.40180019 0.377843 0.32725373 0.23872022][0.023817003 0.11844453 0.24889722 0.38893679 0.51731765 0.61349684 0.66478229 0.66265446 0.63590467 0.61315531 0.59930509 0.57910061 0.53890222 0.46644881 0.3476277][0.033486176 0.14740503 0.30732954 0.48140258 0.6447919 0.7732029 0.84721595 0.84966165 0.81134647 0.77206081 0.74163133 0.70258832 0.63979632 0.54438257 0.40246788][0.02893535 0.15198807 0.32881752 0.52374816 0.7107017 0.86457866 0.95984709 0.97026408 0.92312616 0.86602324 0.814194 0.75091225 0.66273683 0.54655296 0.39207363][0.012289627 0.13324755 0.31066281 0.50669152 0.69571012 0.85508293 0.95733005 0.97232062 0.92233354 0.85571074 0.78878176 0.70594 0.59863538 0.4709197 0.31860015][-0.018149033 0.08736252 0.24691966 0.42371443 0.59338993 0.73690504 0.82865644 0.84045929 0.79041791 0.72231191 0.64944059 0.55646569 0.44108766 0.31532705 0.18217234][-0.0599855 0.018562188 0.14469628 0.28704289 0.42423818 0.54041994 0.61324626 0.61869967 0.5699932 0.50297326 0.42795137 0.33241275 0.21988282 0.10843217 0.0071769031][-0.10175152 -0.056651056 0.025888862 0.12264235 0.21780077 0.29935846 0.34995258 0.35098758 0.30965486 0.25085184 0.18157722 0.094822973 -0.00082751276 -0.086112015 -0.15002327][-0.13576154 -0.12073479 -0.080200732 -0.029298501 0.021924248 0.066029556 0.092367604 0.08914873 0.05949134 0.017101619 -0.03521667 -0.10019244 -0.16702582 -0.21909617 -0.24782693][-0.15709534 -0.16514066 -0.15723273 -0.14271177 -0.12707731 -0.11449887 -0.10958996 -0.11749688 -0.13604248 -0.15892471 -0.18733384 -0.22341356 -0.25899082 -0.28136411 -0.28484523][-0.16394046 -0.18748054 -0.20128992 -0.21019359 -0.21607767 -0.22130765 -0.22752808 -0.23619597 -0.24553353 -0.25309497 -0.26118654 -0.27247304 -0.28352955 -0.28628939 -0.27650076][-0.15249915 -0.18057548 -0.20309487 -0.22215281 -0.23650709 -0.24673975 -0.25395939 -0.25930622 -0.26215798 -0.26178789 -0.25962204 -0.25754952 -0.25535017 -0.24929476 -0.23627034]]...]
INFO - root - 2017-12-11 07:20:59.156134: step 26210, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 45h:42m:21s remains)
INFO - root - 2017-12-11 07:21:04.537992: step 26220, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 44h:32m:43s remains)
INFO - root - 2017-12-11 07:21:09.971730: step 26230, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:20m:19s remains)
INFO - root - 2017-12-11 07:21:15.268578: step 26240, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:35m:08s remains)
INFO - root - 2017-12-11 07:21:20.656896: step 26250, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:06m:57s remains)
INFO - root - 2017-12-11 07:21:25.991922: step 26260, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 46h:28m:32s remains)
INFO - root - 2017-12-11 07:21:31.385680: step 26270, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:10m:26s remains)
INFO - root - 2017-12-11 07:21:36.684542: step 26280, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:06m:22s remains)
INFO - root - 2017-12-11 07:21:41.741743: step 26290, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 44h:20m:17s remains)
INFO - root - 2017-12-11 07:21:47.135311: step 26300, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:50m:11s remains)
2017-12-11 07:21:47.704035: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49175602 0.41408059 0.32250983 0.259216 0.25257328 0.29673347 0.36023983 0.41720533 0.4486413 0.44857258 0.42176014 0.38001195 0.34932294 0.34646007 0.376655][0.52985775 0.44178683 0.33621451 0.26243061 0.25648561 0.31175342 0.39185116 0.46451798 0.50625366 0.5096302 0.47822684 0.42534742 0.38111043 0.36645833 0.38736752][0.47914168 0.40150827 0.30549613 0.24120043 0.24700263 0.31613728 0.40966138 0.49297634 0.53927588 0.540166 0.49915332 0.43234757 0.37251973 0.34175065 0.34493098][0.38148332 0.33027408 0.26346672 0.2250607 0.25198132 0.33775556 0.44370541 0.53440613 0.57956547 0.5707249 0.51348305 0.42907423 0.35273135 0.30635986 0.29180971][0.28090894 0.26212573 0.2328891 0.22615615 0.27580333 0.37697628 0.4940463 0.59037405 0.63078606 0.60643274 0.52722675 0.42256376 0.33088934 0.27412063 0.24868025][0.20836766 0.21824038 0.22282445 0.24481182 0.3143771 0.42837009 0.55485976 0.65462112 0.68693006 0.64297366 0.53971457 0.41683227 0.31597278 0.25801209 0.23234515][0.18750612 0.21246034 0.23589146 0.27576062 0.35885811 0.48233286 0.61604351 0.71631753 0.737355 0.67274517 0.54894757 0.41624048 0.31843165 0.27230671 0.25949243][0.22014588 0.24005112 0.25949788 0.2992458 0.38394776 0.50962126 0.64552945 0.74326128 0.75464016 0.67752868 0.54724509 0.42159364 0.34301132 0.32079595 0.32879937][0.27800879 0.27541682 0.27091312 0.29357108 0.36594066 0.48268336 0.61230493 0.70401335 0.71239251 0.6405645 0.528425 0.43379188 0.39142355 0.40007725 0.42720297][0.31114212 0.27969539 0.24577223 0.2460333 0.29978022 0.39984578 0.51564467 0.59756875 0.60843897 0.55768043 0.48480687 0.43950728 0.44150302 0.47767869 0.51211721][0.30687806 0.25258678 0.19621128 0.17732215 0.21120268 0.28966072 0.38616511 0.45722347 0.47586912 0.45606443 0.43169484 0.43819243 0.4767651 0.52389908 0.54675668][0.27286261 0.21165617 0.14617305 0.11424766 0.12857668 0.18230219 0.256702 0.31893268 0.35026219 0.36392522 0.383125 0.42680752 0.48087633 0.51751089 0.51318437][0.22258684 0.17355381 0.11671051 0.083114222 0.083431005 0.11432784 0.1674397 0.22205637 0.2647219 0.30232173 0.34512398 0.3990199 0.44255644 0.45198184 0.41605404][0.18310913 0.16279225 0.13138495 0.11046871 0.10796137 0.12442801 0.16003336 0.20392613 0.24555376 0.28434131 0.32162642 0.35801649 0.37349415 0.35348284 0.29709852][0.17873016 0.19520286 0.19897959 0.20192887 0.20821859 0.22027001 0.24244766 0.26948011 0.29232034 0.30680537 0.31354973 0.3162874 0.30180985 0.26461482 0.2088715]]...]
INFO - root - 2017-12-11 07:21:53.049270: step 26310, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 45h:07m:41s remains)
INFO - root - 2017-12-11 07:21:58.442900: step 26320, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 44h:26m:00s remains)
INFO - root - 2017-12-11 07:22:03.875117: step 26330, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:39m:43s remains)
INFO - root - 2017-12-11 07:22:09.199714: step 26340, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:58m:09s remains)
INFO - root - 2017-12-11 07:22:14.474377: step 26350, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:57m:43s remains)
INFO - root - 2017-12-11 07:22:19.849078: step 26360, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.499 sec/batch; 42h:27m:06s remains)
INFO - root - 2017-12-11 07:22:25.278570: step 26370, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:32m:43s remains)
INFO - root - 2017-12-11 07:22:30.629363: step 26380, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.508 sec/batch; 43h:12m:40s remains)
INFO - root - 2017-12-11 07:22:35.754150: step 26390, loss = 0.69, batch loss = 0.64 (20.6 examples/sec; 0.389 sec/batch; 33h:03m:49s remains)
INFO - root - 2017-12-11 07:22:40.991211: step 26400, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:51m:29s remains)
2017-12-11 07:22:41.556349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051071595 -0.043058634 -0.028588716 -0.012826703 -0.0027745983 -0.00310786 -0.013342992 -0.028606806 -0.044551764 -0.057574831 -0.064974293 -0.065230273 -0.061432019 -0.057473432 -0.054083057][-0.052959103 -0.034502227 -0.0037656606 0.030052228 0.054127 0.060192324 0.047054864 0.021916291 -0.0065796473 -0.032515485 -0.051693648 -0.061679438 -0.06501089 -0.065572239 -0.064274438][-0.051431477 -0.019658152 0.032825019 0.092660926 0.14015876 0.16143635 0.15144028 0.11827796 0.075276196 0.031485409 -0.0062672314 -0.033302814 -0.050331507 -0.060610957 -0.065544695][-0.050261229 -0.0064490112 0.06796889 0.15632871 0.23293591 0.27793425 0.28061152 0.2470154 0.1922974 0.12846979 0.066822529 0.016156502 -0.020795094 -0.04582658 -0.0610032][-0.049549021 -5.097008e-05 0.088025749 0.1976869 0.30103797 0.37396756 0.39954135 0.3782621 0.32174787 0.24310118 0.15871276 0.082294725 0.021292955 -0.02299444 -0.052118402][-0.0496086 -0.0047498322 0.08254046 0.19837382 0.31816956 0.41695657 0.47230348 0.47615194 0.43117315 0.34891012 0.25003481 0.15250917 0.06872654 0.00447419 -0.039522722][-0.051791292 -0.020714119 0.0508014 0.15470384 0.27466667 0.38936797 0.47333619 0.50930309 0.48775512 0.415881 0.31570986 0.20743397 0.10778297 0.028002396 -0.027359262][-0.055830874 -0.042551666 0.0039311755 0.081718616 0.18497688 0.29998836 0.40203297 0.46706381 0.47429162 0.42448226 0.336894 0.23106675 0.12672842 0.040263057 -0.019585153][-0.060729526 -0.063676961 -0.043482084 0.002391882 0.077274457 0.17654972 0.27986932 0.36115351 0.39337039 0.37094578 0.30659 0.21617825 0.12014536 0.038084518 -0.018284714][-0.06362804 -0.0782353 -0.079252921 -0.06254752 -0.019660125 0.052466452 0.13994941 0.21949604 0.26430044 0.26508009 0.22744143 0.16189088 0.086842656 0.021163369 -0.023473142][-0.062836312 -0.083352655 -0.097481236 -0.10157193 -0.086760305 -0.046003431 0.013513472 0.074753486 0.11637335 0.12921338 0.11423758 0.076896749 0.031037297 -0.00912917 -0.035252668][-0.058190264 -0.080351859 -0.10009132 -0.11561333 -0.12086954 -0.1088304 -0.081383042 -0.047751535 -0.021091759 -0.0075181318 -0.0073960363 -0.018939821 -0.034433205 -0.046705171 -0.052310903][-0.052366998 -0.073237255 -0.092789263 -0.11146758 -0.12705037 -0.13513832 -0.13452204 -0.1274831 -0.11918975 -0.1117892 -0.10479337 -0.098542385 -0.091222733 -0.082137167 -0.07184688][-0.048908953 -0.066694587 -0.0823754 -0.0979604 -0.11424019 -0.13032025 -0.14437613 -0.15459345 -0.16062975 -0.16145271 -0.15511717 -0.14248802 -0.12534249 -0.10672378 -0.08927007][-0.048536368 -0.0618372 -0.071633019 -0.080531232 -0.09071172 -0.10393146 -0.11935142 -0.13444279 -0.14690082 -0.1541976 -0.1535114 -0.14490436 -0.13061997 -0.11405256 -0.09817484]]...]
INFO - root - 2017-12-11 07:22:47.002231: step 26410, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 44h:27m:48s remains)
INFO - root - 2017-12-11 07:22:52.258292: step 26420, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:15m:11s remains)
INFO - root - 2017-12-11 07:22:57.611848: step 26430, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:07m:42s remains)
INFO - root - 2017-12-11 07:23:02.901101: step 26440, loss = 0.70, batch loss = 0.64 (15.9 examples/sec; 0.503 sec/batch; 42h:48m:07s remains)
INFO - root - 2017-12-11 07:23:08.259851: step 26450, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.518 sec/batch; 44h:01m:39s remains)
INFO - root - 2017-12-11 07:23:13.617739: step 26460, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:07m:41s remains)
INFO - root - 2017-12-11 07:23:19.000572: step 26470, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:53m:02s remains)
INFO - root - 2017-12-11 07:23:24.362427: step 26480, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.512 sec/batch; 43h:29m:34s remains)
INFO - root - 2017-12-11 07:23:29.642012: step 26490, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 44h:51m:51s remains)
INFO - root - 2017-12-11 07:23:34.767407: step 26500, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 48h:39m:26s remains)
2017-12-11 07:23:35.277649: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0094792023 0.011630143 0.038020745 0.0782148 0.12157249 0.1553071 0.17886588 0.18992209 0.18849626 0.17066276 0.1342361 0.086501181 0.049153078 0.048602268 0.085298777][-0.036117643 -0.021770142 0.029629102 0.098969057 0.16579002 0.21170728 0.23027237 0.22609159 0.2038039 0.16449739 0.11097881 0.055470355 0.021378612 0.032997012 0.086526796][-0.055972084 -0.028992735 0.044863872 0.13987987 0.22676125 0.28172657 0.2969819 0.28034872 0.24245459 0.18866283 0.1259049 0.067219563 0.035186507 0.049923636 0.10301851][-0.044700403 -0.0078371931 0.079346836 0.18891673 0.28825569 0.35121158 0.36989698 0.35339481 0.3146612 0.25886133 0.19363581 0.13187195 0.095367447 0.1005567 0.13785835][-0.022772165 0.018089639 0.10648916 0.21714234 0.32011324 0.39054182 0.42095363 0.41896829 0.39360866 0.34533784 0.27992573 0.21145414 0.16423133 0.1543819 0.1740437][-0.00036074829 0.036391284 0.11417847 0.21378395 0.31263086 0.39067748 0.44023579 0.46331322 0.45906314 0.42168581 0.35472384 0.27697411 0.21876164 0.19801255 0.20699427][0.023670968 0.050053209 0.10861363 0.18771312 0.27507263 0.35806254 0.42778382 0.47678706 0.49223036 0.46367964 0.39575374 0.31384829 0.25283584 0.22976249 0.23477995][0.058855664 0.077203311 0.11609089 0.17146839 0.24117543 0.31983176 0.39756903 0.45831084 0.481993 0.45779213 0.394787 0.32138321 0.27009577 0.2525638 0.25789407][0.11342356 0.13031156 0.15515916 0.18930475 0.23742582 0.29888716 0.36422649 0.41543156 0.43387347 0.4133144 0.36575863 0.31501722 0.28366315 0.27627113 0.28399146][0.18738101 0.20694567 0.22360075 0.24236637 0.27026924 0.30889732 0.35084707 0.38203174 0.38942811 0.3731772 0.34434596 0.31665573 0.30056062 0.29626542 0.30050188][0.26849216 0.2913346 0.30301118 0.31165215 0.32458314 0.34452808 0.36608449 0.3791633 0.37556183 0.36000642 0.34103841 0.32346731 0.30915138 0.29721692 0.29043141][0.32400972 0.3465983 0.35633871 0.3623006 0.37049654 0.38264546 0.39293841 0.39295033 0.37803957 0.35609218 0.33574978 0.31790406 0.29971322 0.27922958 0.26195255][0.32277203 0.34294879 0.35536647 0.36751407 0.38161045 0.39543319 0.40031737 0.38921168 0.36163223 0.3290987 0.30306849 0.28443652 0.26794606 0.24867065 0.22985041][0.2638005 0.28112093 0.29809594 0.31846038 0.33985776 0.35613009 0.35723886 0.33891138 0.3044031 0.2675553 0.2418642 0.22909518 0.22247905 0.21332817 0.19990079][0.16819143 0.18436463 0.20401029 0.22604683 0.24532698 0.255942 0.2511473 0.23099169 0.20072706 0.17291594 0.15936318 0.16118772 0.1699657 0.17371204 0.16645746]]...]
INFO - root - 2017-12-11 07:23:40.539781: step 26510, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:33m:01s remains)
INFO - root - 2017-12-11 07:23:45.937920: step 26520, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:49m:50s remains)
INFO - root - 2017-12-11 07:23:51.249562: step 26530, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 44h:50m:54s remains)
INFO - root - 2017-12-11 07:23:56.517845: step 26540, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:49m:22s remains)
INFO - root - 2017-12-11 07:24:01.910298: step 26550, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:09m:30s remains)
INFO - root - 2017-12-11 07:24:07.275761: step 26560, loss = 0.66, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 45h:15m:58s remains)
INFO - root - 2017-12-11 07:24:12.644771: step 26570, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:25m:16s remains)
INFO - root - 2017-12-11 07:24:17.883312: step 26580, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 46h:12m:19s remains)
INFO - root - 2017-12-11 07:24:23.313295: step 26590, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:26m:17s remains)
INFO - root - 2017-12-11 07:24:28.337338: step 26600, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:47m:07s remains)
2017-12-11 07:24:28.855412: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16070051 0.15561719 0.14166506 0.12595484 0.11757041 0.12386586 0.14465502 0.17279433 0.19829383 0.21367869 0.21458909 0.20215894 0.18426129 0.17190298 0.17257512][0.12875842 0.13934788 0.14193159 0.13858682 0.13514678 0.13993986 0.15711482 0.18271875 0.20804122 0.22533005 0.22859356 0.21617 0.19493644 0.17741305 0.17314576][0.09918464 0.13278437 0.16186538 0.18125461 0.19165516 0.19880575 0.21050355 0.22824253 0.24698073 0.25987825 0.25909558 0.24073581 0.21064453 0.18207712 0.16620131][0.090658769 0.1525874 0.21734677 0.27156571 0.30891573 0.32848805 0.3383497 0.34571186 0.35198605 0.35431352 0.34381631 0.31482294 0.27199343 0.22826676 0.19508289][0.098232061 0.18604435 0.28861076 0.38439032 0.4596566 0.50381368 0.52018225 0.51894963 0.50955266 0.49734628 0.4750292 0.43664595 0.38519117 0.33081657 0.28130558][0.10683534 0.21181926 0.34421965 0.4771747 0.59048879 0.66376346 0.69271851 0.68721664 0.66512233 0.64248532 0.61447084 0.57421815 0.52295017 0.46709257 0.40641752][0.10789869 0.21442842 0.35849759 0.5108164 0.64714068 0.74059838 0.77997738 0.77315992 0.74561566 0.72379196 0.7029922 0.67218536 0.63057935 0.58254987 0.51872486][0.10345957 0.19045728 0.32009271 0.46469635 0.59856993 0.6936205 0.735548 0.72928566 0.70360965 0.69142008 0.68626875 0.67187589 0.64623892 0.61429191 0.55931306][0.09857168 0.14273432 0.22977446 0.33895296 0.44505471 0.52292335 0.55932927 0.55552357 0.53775209 0.53895563 0.55106 0.55402774 0.54531175 0.53260636 0.49600416][0.092951365 0.080868974 0.10813021 0.16479476 0.22850996 0.27896667 0.30594313 0.30591413 0.29788709 0.31006819 0.33439267 0.35005382 0.35397059 0.35635775 0.33958614][0.089987092 0.029349744 0.0018710786 0.0089684669 0.032858875 0.058037914 0.076428168 0.079595968 0.077511676 0.091978721 0.11828087 0.13816431 0.14748742 0.1570987 0.15587249][0.074517883 -0.0098128924 -0.067604721 -0.089405648 -0.08846958 -0.078049272 -0.065777287 -0.062333453 -0.064685211 -0.057202563 -0.039413303 -0.02365428 -0.014763745 -0.0031920129 0.0072503779][0.027446117 -0.05255568 -0.11027281 -0.1346558 -0.13573331 -0.12697311 -0.11702496 -0.11517489 -0.12122715 -0.12406962 -0.11882699 -0.11083224 -0.10391314 -0.091367625 -0.072577477][-0.037491031 -0.092739314 -0.12780873 -0.13685192 -0.128103 -0.11484591 -0.10606889 -0.10733863 -0.11809809 -0.12921958 -0.13354069 -0.13154025 -0.12446348 -0.1086439 -0.083075911][-0.089537315 -0.11302659 -0.11752895 -0.10640371 -0.0870497 -0.070333324 -0.063671917 -0.068931215 -0.083653994 -0.099175975 -0.10775238 -0.10740396 -0.097223535 -0.076077729 -0.045153648]]...]
INFO - root - 2017-12-11 07:24:34.175845: step 26610, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 45h:30m:19s remains)
INFO - root - 2017-12-11 07:24:39.459768: step 26620, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:18m:11s remains)
INFO - root - 2017-12-11 07:24:44.782677: step 26630, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:07m:44s remains)
INFO - root - 2017-12-11 07:24:50.097168: step 26640, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.507 sec/batch; 43h:03m:02s remains)
INFO - root - 2017-12-11 07:24:55.511532: step 26650, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 46h:21m:26s remains)
INFO - root - 2017-12-11 07:25:00.910411: step 26660, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 44h:12m:07s remains)
INFO - root - 2017-12-11 07:25:06.241255: step 26670, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:21m:15s remains)
INFO - root - 2017-12-11 07:25:11.617374: step 26680, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 44h:40m:01s remains)
INFO - root - 2017-12-11 07:25:16.976131: step 26690, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 44h:13m:12s remains)
INFO - root - 2017-12-11 07:25:21.987236: step 26700, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 24h:02m:05s remains)
2017-12-11 07:25:22.553354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029507516 -0.033442911 -0.025269173 -0.0041359463 0.026675379 0.058544956 0.08242172 0.090298288 0.076758228 0.04562379 0.0068333568 -0.028990643 -0.057517763 -0.076932251 -0.085487463][7.5492862e-05 -0.004654116 0.010221728 0.048327114 0.10284263 0.15865505 0.19929484 0.21178432 0.18732242 0.13422258 0.068390213 0.0074611627 -0.040917166 -0.074779317 -0.091470405][0.044578549 0.039652184 0.061983891 0.12071279 0.20580745 0.293488 0.35700172 0.37561986 0.33705941 0.25571841 0.15519609 0.062646411 -0.0096801538 -0.060557373 -0.087495267][0.096437261 0.088902123 0.11598472 0.19356535 0.30882707 0.42964002 0.51871854 0.54619336 0.49514505 0.38527831 0.24823624 0.12176578 0.024357121 -0.043479159 -0.080901518][0.16268012 0.14611964 0.16856635 0.25558132 0.39327312 0.542382 0.65593547 0.69473833 0.63588804 0.50310659 0.334613 0.17694202 0.055961557 -0.027088182 -0.073653743][0.25553676 0.21742283 0.21616404 0.29426917 0.44129688 0.61244881 0.75153458 0.80877769 0.75219482 0.60694849 0.41540751 0.23094682 0.08731664 -0.010800591 -0.06610509][0.36911318 0.2968432 0.25157323 0.29897717 0.43740639 0.61983246 0.78343791 0.86795151 0.82926029 0.6878798 0.48607558 0.28120914 0.11604509 0.0027392427 -0.060560182][0.48494384 0.37285864 0.27251288 0.27419826 0.38774666 0.56830412 0.75080717 0.86650234 0.85914421 0.7394225 0.54305446 0.32573122 0.13976759 0.010259186 -0.060309146][0.58333141 0.43384954 0.27789918 0.22832604 0.30784506 0.47613341 0.66893291 0.81263471 0.8413012 0.75411004 0.57630545 0.35684404 0.15484357 0.01062619 -0.066129215][0.6420905 0.46928507 0.27200514 0.179294 0.22578776 0.37767202 0.57272625 0.73580813 0.79492325 0.73901516 0.5823313 0.36681131 0.15584528 0.0023444521 -0.077302769][0.63393897 0.4569006 0.24477313 0.12788278 0.150194 0.28575632 0.4747214 0.644043 0.722611 0.69170725 0.55371183 0.34518522 0.13404368 -0.018847352 -0.094509937][0.54361653 0.386961 0.19158742 0.074537173 0.08392866 0.20303553 0.37637308 0.53622705 0.61970049 0.60468227 0.48283511 0.2859683 0.085146934 -0.055502445 -0.119141][0.39395845 0.27297625 0.11753304 0.019023487 0.022936601 0.12254624 0.27049458 0.40720144 0.48139554 0.47306812 0.36816341 0.19244651 0.014729615 -0.10287366 -0.14802715][0.2230708 0.13795096 0.027657995 -0.045635656 -0.043121688 0.034168985 0.1480429 0.2518231 0.30853048 0.30245015 0.21670781 0.071993 -0.06979055 -0.15478826 -0.17660043][0.063486978 0.0055611688 -0.0677314 -0.11810956 -0.11548987 -0.05924131 0.020413497 0.090767317 0.1293565 0.12545297 0.06102344 -0.048140198 -0.14945534 -0.19994859 -0.19889931]]...]
INFO - root - 2017-12-11 07:25:27.869797: step 26710, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:11m:02s remains)
INFO - root - 2017-12-11 07:25:33.194855: step 26720, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:13m:50s remains)
INFO - root - 2017-12-11 07:25:38.464989: step 26730, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:34m:36s remains)
INFO - root - 2017-12-11 07:25:43.802846: step 26740, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:31m:29s remains)
INFO - root - 2017-12-11 07:25:49.209728: step 26750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:43m:38s remains)
INFO - root - 2017-12-11 07:25:54.453926: step 26760, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 45h:27m:38s remains)
INFO - root - 2017-12-11 07:25:59.828898: step 26770, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:56m:20s remains)
INFO - root - 2017-12-11 07:26:05.179586: step 26780, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 46h:03m:24s remains)
INFO - root - 2017-12-11 07:26:10.553181: step 26790, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 46h:02m:48s remains)
INFO - root - 2017-12-11 07:26:15.891904: step 26800, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:28m:26s remains)
2017-12-11 07:26:16.438445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053835545 -0.025732363 0.02687927 0.10281173 0.1894175 0.26747584 0.31887683 0.32977408 0.311048 0.2783758 0.25375527 0.25283512 0.28861275 0.35500294 0.42923194][-0.059131518 -0.028621294 0.029687669 0.11598116 0.21842298 0.3155708 0.38474256 0.41002822 0.40046176 0.37254059 0.3504557 0.3503024 0.3829816 0.43924618 0.4964284][-0.060703922 -0.0286251 0.034419436 0.13133594 0.25227782 0.37337732 0.46611744 0.5112533 0.51200455 0.48372829 0.45356843 0.44154182 0.45966733 0.49605826 0.52891457][-0.062513888 -0.027575143 0.042197932 0.15215673 0.29429007 0.44133914 0.558742 0.62333864 0.63157195 0.59543461 0.54518938 0.50810218 0.50144219 0.51314348 0.52204567][-0.065396652 -0.026593858 0.051430788 0.17608829 0.34051275 0.51283062 0.65184128 0.72985858 0.7386089 0.68799794 0.61205709 0.54604232 0.51303071 0.50076461 0.48704773][-0.070427634 -0.029529184 0.055972345 0.1953408 0.38219354 0.57900441 0.73591584 0.81756878 0.81434381 0.73980659 0.63531959 0.54486787 0.49499357 0.46994039 0.44169509][-0.075956255 -0.033459459 0.059928995 0.21629016 0.42944887 0.65514618 0.83096874 0.907394 0.87493068 0.75832796 0.61321574 0.49497902 0.43191531 0.40286654 0.37096685][-0.081145316 -0.037532862 0.0625928 0.23478721 0.47199336 0.72336978 0.91304433 0.975501 0.90299004 0.73555243 0.54612803 0.40052739 0.32640481 0.29791123 0.27173764][-0.086797319 -0.044952463 0.057434313 0.23813072 0.48568863 0.74352008 0.92801386 0.9666366 0.85842174 0.65557331 0.44144768 0.28439945 0.20858866 0.18679704 0.1749167][-0.091795534 -0.054271951 0.04561061 0.22513534 0.46521187 0.70447242 0.86062592 0.86763817 0.73613292 0.52386671 0.31321552 0.1662952 0.10141004 0.093186192 0.10124718][-0.09599486 -0.065452613 0.025843296 0.19270104 0.40937132 0.61304975 0.72911888 0.70653003 0.56646389 0.36581519 0.1791999 0.060173661 0.021078805 0.036922656 0.070245408][-0.10022978 -0.079333484 -0.0042544403 0.13672574 0.31582677 0.47579122 0.55484354 0.519418 0.39536482 0.23073922 0.087520249 0.00985173 0.0051476518 0.047556672 0.10211535][-0.1048218 -0.093975641 -0.0384575 0.07053563 0.20826831 0.32964468 0.38806361 0.36354727 0.27871853 0.16729145 0.0760109 0.038908653 0.06094259 0.11889607 0.18350545][-0.1070433 -0.10340465 -0.063828982 0.018801484 0.12671837 0.2289515 0.29083478 0.29767951 0.26175022 0.20286585 0.15492003 0.14418833 0.17601481 0.2331008 0.29499629][-0.1055925 -0.10459355 -0.0718079 0.0014794579 0.10446155 0.21485865 0.30042371 0.34324527 0.34466738 0.31649271 0.28750953 0.281887 0.30777422 0.35284469 0.40409064]]...]
INFO - root - 2017-12-11 07:26:21.363888: step 26810, loss = 0.67, batch loss = 0.61 (15.5 examples/sec; 0.515 sec/batch; 43h:41m:34s remains)
INFO - root - 2017-12-11 07:26:26.716241: step 26820, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 44h:24m:54s remains)
INFO - root - 2017-12-11 07:26:32.191455: step 26830, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:53m:09s remains)
INFO - root - 2017-12-11 07:26:37.616519: step 26840, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:14m:24s remains)
INFO - root - 2017-12-11 07:26:43.000903: step 26850, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:36m:21s remains)
INFO - root - 2017-12-11 07:26:48.281936: step 26860, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 46h:56m:29s remains)
INFO - root - 2017-12-11 07:26:53.708275: step 26870, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:21m:48s remains)
INFO - root - 2017-12-11 07:26:59.014231: step 26880, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:21m:06s remains)
INFO - root - 2017-12-11 07:27:04.357037: step 26890, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:43m:17s remains)
INFO - root - 2017-12-11 07:27:09.640892: step 26900, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 44h:06m:01s remains)
2017-12-11 07:27:10.173370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024129482 -0.02446367 -0.022929031 -0.018912584 -0.012570852 -0.0050444561 0.0017104741 0.0053393715 0.0042809756 -0.001066689 -0.00957891 -0.019241394 -0.026677866 -0.029389864 -0.02720027][-0.016603846 -0.015938554 -0.01212167 -0.0045276931 0.0057614548 0.016270656 0.024176395 0.026421441 0.021756861 0.011319413 -0.0030591732 -0.018431176 -0.030434664 -0.035616763 -0.03390893][0.016541217 0.020727849 0.030116066 0.045331914 0.063205145 0.078721486 0.087815695 0.087156504 0.076347567 0.0579005 0.034877162 0.011545246 -0.0066947234 -0.015349635 -0.014421943][0.08276438 0.094347782 0.11458821 0.1441883 0.17621504 0.2008491 0.21188499 0.205977 0.18479697 0.15355743 0.1176443 0.083318539 0.057002634 0.044133954 0.044377085][0.17663696 0.19847645 0.2342485 0.2853272 0.33912417 0.37791744 0.39158207 0.37711361 0.34006575 0.2904776 0.2376996 0.19078334 0.1564616 0.14005114 0.13995282][0.26843134 0.30172968 0.35551086 0.43214682 0.51249659 0.56877732 0.58554268 0.56052631 0.50449818 0.43401986 0.36386424 0.30631155 0.2670652 0.2492774 0.24907812][0.3275865 0.37006605 0.43914413 0.53769141 0.6413281 0.71291894 0.73155487 0.69617337 0.62321091 0.53624964 0.45558915 0.39572608 0.35931832 0.34465176 0.34465334][0.33215466 0.37736759 0.45226374 0.55927026 0.67191541 0.74828285 0.76435804 0.72093093 0.63917887 0.54832119 0.47250417 0.42579708 0.40513021 0.40093839 0.40264133][0.27441251 0.31389165 0.38179269 0.47909212 0.58111989 0.64784145 0.65622842 0.6095624 0.53126079 0.4521929 0.39688233 0.37589067 0.37923783 0.38917062 0.39309081][0.17014726 0.19818503 0.2495876 0.32309961 0.39925393 0.44558683 0.44350266 0.3990795 0.33482081 0.27873218 0.25206116 0.25937319 0.28501585 0.30579078 0.30990532][0.059495527 0.076220974 0.10935062 0.15512159 0.20062068 0.22377045 0.21176104 0.17276591 0.12608223 0.094122849 0.092592411 0.11941724 0.15627025 0.17893785 0.17907408][-0.016469391 -0.0046612173 0.016783964 0.041543383 0.062052123 0.065972373 0.045893624 0.01155611 -0.021100702 -0.035443269 -0.022765635 0.00834507 0.040802684 0.054793917 0.047281314][-0.04659944 -0.028482029 -0.0055751042 0.012735019 0.02178191 0.015879909 -0.0085095624 -0.0406318 -0.066679619 -0.075074852 -0.064060792 -0.046416957 -0.034302041 -0.038788952 -0.056931857][-0.041979123 -0.0107664 0.022824073 0.046023685 0.055337261 0.049335435 0.025718056 -0.0047517586 -0.029523099 -0.040658809 -0.041784279 -0.047322139 -0.062837332 -0.089186981 -0.11838523][-0.025860108 0.015175763 0.058212414 0.087871313 0.10085963 0.098540381 0.078669131 0.050922751 0.026708042 0.011638037 -0.00089234888 -0.025299251 -0.061777487 -0.10333558 -0.1392062]]...]
INFO - root - 2017-12-11 07:27:15.222226: step 26910, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:06m:23s remains)
INFO - root - 2017-12-11 07:27:20.536901: step 26920, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 44h:51m:07s remains)
INFO - root - 2017-12-11 07:27:25.939933: step 26930, loss = 0.70, batch loss = 0.65 (15.7 examples/sec; 0.508 sec/batch; 43h:08m:08s remains)
INFO - root - 2017-12-11 07:27:31.338700: step 26940, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 44h:16m:10s remains)
INFO - root - 2017-12-11 07:27:36.592485: step 26950, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:28m:45s remains)
INFO - root - 2017-12-11 07:27:41.963418: step 26960, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.540 sec/batch; 45h:49m:49s remains)
INFO - root - 2017-12-11 07:27:47.254835: step 26970, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 44h:24m:24s remains)
INFO - root - 2017-12-11 07:27:52.566732: step 26980, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 47h:24m:28s remains)
INFO - root - 2017-12-11 07:27:57.931463: step 26990, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 43h:46m:35s remains)
INFO - root - 2017-12-11 07:28:03.227804: step 27000, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.505 sec/batch; 42h:52m:55s remains)
2017-12-11 07:28:03.758231: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013470804 0.04375406 0.072650991 0.08742895 0.087544873 0.07427454 0.052460581 0.030098813 0.014498495 0.0090296017 0.0068538794 0.0038235094 -0.0029873524 -0.016150596 -0.034658372][0.065470338 0.12393861 0.18691532 0.23040709 0.24636979 0.23379926 0.19976154 0.15876564 0.1211974 0.097306266 0.081434757 0.069331959 0.05557356 0.032483235 0.00046794512][0.13209382 0.22481254 0.32919967 0.41101968 0.452396 0.44572824 0.4000335 0.33667761 0.27022982 0.21958491 0.18301475 0.15817454 0.13637668 0.10234855 0.054039996][0.1835383 0.31164867 0.45745936 0.58137375 0.65588784 0.66320109 0.61140811 0.52758521 0.43012345 0.346301 0.28145343 0.2385764 0.20725773 0.16434275 0.10307185][0.20246319 0.36368659 0.54946315 0.7162438 0.82811064 0.85588878 0.80291754 0.70157111 0.575331 0.45834079 0.36353689 0.2999056 0.25744554 0.20747031 0.13831827][0.2038471 0.39135122 0.60931939 0.80974394 0.95011181 0.99292642 0.93595344 0.81766886 0.66822135 0.52691895 0.41240975 0.3351928 0.28527319 0.2308493 0.15765685][0.19714171 0.39708057 0.62980825 0.84659564 1.0028259 1.0556787 0.99669713 0.86930704 0.70944875 0.557447 0.43331239 0.34780428 0.29217088 0.2346033 0.16007921][0.17040503 0.36525586 0.59226626 0.807314 0.96804839 1.0301104 0.98055696 0.86440527 0.71696144 0.57144451 0.4445022 0.34951913 0.28343323 0.2192069 0.14460087][0.12071893 0.29787931 0.5059703 0.70560014 0.85968477 0.92587978 0.89030057 0.79744095 0.67857754 0.55467606 0.43464369 0.33337697 0.25565103 0.18315741 0.11036702][0.069114655 0.22184011 0.40508655 0.58010709 0.715685 0.77564812 0.74824953 0.67615426 0.58755332 0.49376056 0.39379823 0.29855132 0.21688218 0.13940078 0.06962432][0.027729463 0.15074722 0.30212134 0.4453395 0.55606365 0.60674822 0.588353 0.53750879 0.4768199 0.4120644 0.33557647 0.25294134 0.17349093 0.094550371 0.028197572][-0.015208306 0.0754535 0.19214821 0.30431125 0.39430282 0.4411366 0.43713942 0.41017348 0.37449083 0.331259 0.27027407 0.19611338 0.1198127 0.042959511 -0.017632859][-0.071412079 -0.014022095 0.068323024 0.15124819 0.2213846 0.26166329 0.26681575 0.25684413 0.23930536 0.21180879 0.16447952 0.10225599 0.037189923 -0.027388277 -0.074402][-0.1194748 -0.089857534 -0.037692767 0.016353186 0.061781354 0.085348636 0.085726812 0.078884818 0.069602147 0.054901119 0.02540566 -0.015254393 -0.057812985 -0.099927552 -0.12739845][-0.14564799 -0.13579954 -0.10835192 -0.080817915 -0.060702786 -0.055804718 -0.06432192 -0.073612928 -0.08037892 -0.086263768 -0.099032424 -0.11686549 -0.13493551 -0.15279034 -0.16090304]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:28:08.819227: step 27010, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.511 sec/batch; 43h:20m:13s remains)
INFO - root - 2017-12-11 07:28:14.285561: step 27020, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 45h:18m:40s remains)
INFO - root - 2017-12-11 07:28:19.570371: step 27030, loss = 0.67, batch loss = 0.61 (15.9 examples/sec; 0.502 sec/batch; 42h:35m:46s remains)
INFO - root - 2017-12-11 07:28:24.969007: step 27040, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:41m:09s remains)
INFO - root - 2017-12-11 07:28:30.350527: step 27050, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:07m:59s remains)
INFO - root - 2017-12-11 07:28:35.827756: step 27060, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:54m:42s remains)
INFO - root - 2017-12-11 07:28:41.259363: step 27070, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 44h:41m:39s remains)
INFO - root - 2017-12-11 07:28:46.510290: step 27080, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:04m:29s remains)
INFO - root - 2017-12-11 07:28:51.983244: step 27090, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:44m:58s remains)
INFO - root - 2017-12-11 07:28:57.357540: step 27100, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:08m:32s remains)
2017-12-11 07:28:57.912307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065386005 -0.068057641 -0.065315746 -0.061204173 -0.057590354 -0.055417314 -0.054566853 -0.054246943 -0.054111078 -0.054404 -0.055194125 -0.056044918 -0.05668861 -0.057295293 -0.057268228][-0.040278673 -0.039586674 -0.0316904 -0.018935964 -0.0065421755 0.0024203134 0.0071525462 0.0091298549 0.00966126 0.0079576345 0.0042112558 -0.00062431791 -0.0055240528 -0.010994435 -0.016079102][0.011729655 0.018927513 0.034293108 0.06055849 0.087960452 0.11057871 0.12444254 0.13014011 0.13091925 0.1256569 0.11524231 0.10034194 0.084625386 0.068612292 0.0531431][0.076071389 0.089117117 0.11409465 0.15757969 0.20535722 0.24794115 0.27547103 0.28530067 0.28369445 0.27026293 0.24730332 0.21493247 0.18137358 0.15056986 0.12246013][0.14734097 0.16437685 0.19804072 0.25818616 0.32742131 0.39182794 0.43363634 0.44527009 0.43708181 0.41060916 0.37040693 0.31684238 0.26288089 0.21701573 0.17728664][0.21325362 0.23342335 0.27100527 0.34124872 0.4255707 0.50550729 0.55563563 0.56386304 0.54437417 0.50170988 0.44387656 0.37260425 0.3040542 0.24969071 0.20446183][0.26102823 0.280937 0.3153365 0.38501611 0.47238302 0.55548841 0.6036033 0.60245985 0.56877947 0.51021266 0.43910781 0.35896561 0.28750232 0.23700891 0.19859545][0.28039339 0.29695609 0.323755 0.38348463 0.46091437 0.53266263 0.56726444 0.55141771 0.50266325 0.4316645 0.35486835 0.27726582 0.21620861 0.1829223 0.16424465][0.28363055 0.30006343 0.32056725 0.36663392 0.42508543 0.47330931 0.48475295 0.45035017 0.38706636 0.30814233 0.23378699 0.17047496 0.13132669 0.12326987 0.12939021][0.27264312 0.29536027 0.3143014 0.35056147 0.39103004 0.41415933 0.40175405 0.35116884 0.27757826 0.19528846 0.12859009 0.086327605 0.074158721 0.090789951 0.11779527][0.24880379 0.27877578 0.30174884 0.33621192 0.36667755 0.37261331 0.34581679 0.29046685 0.21803516 0.14135177 0.087524384 0.067472726 0.078086555 0.1090947 0.1440452][0.22233726 0.25498778 0.28424749 0.32442623 0.35630158 0.3592712 0.33350018 0.28841352 0.2307869 0.16975707 0.13172077 0.127942 0.14756274 0.17497815 0.19979152][0.20755208 0.23741703 0.2697486 0.31690419 0.35746333 0.37007645 0.35953617 0.33626038 0.30149624 0.26029405 0.23563759 0.23758163 0.24992433 0.25789979 0.26039228][0.21197736 0.23214069 0.25717133 0.3017334 0.3466565 0.37266976 0.38518459 0.3906512 0.38273257 0.36158913 0.34571767 0.34479484 0.34127596 0.32573941 0.30673197][0.23730302 0.24123277 0.24662855 0.27280131 0.30713654 0.33711529 0.36803177 0.40043435 0.41979623 0.420239 0.41358376 0.409066 0.39111304 0.35827553 0.32800078]]...]
INFO - root - 2017-12-11 07:29:03.287274: step 27110, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 46h:03m:32s remains)
INFO - root - 2017-12-11 07:29:08.402699: step 27120, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 47h:37m:17s remains)
INFO - root - 2017-12-11 07:29:13.759914: step 27130, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:27m:39s remains)
INFO - root - 2017-12-11 07:29:19.117877: step 27140, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 43h:40m:34s remains)
INFO - root - 2017-12-11 07:29:24.441128: step 27150, loss = 0.67, batch loss = 0.61 (14.0 examples/sec; 0.573 sec/batch; 48h:36m:56s remains)
INFO - root - 2017-12-11 07:29:29.883102: step 27160, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 46h:59m:07s remains)
INFO - root - 2017-12-11 07:29:35.255099: step 27170, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 44h:05m:53s remains)
INFO - root - 2017-12-11 07:29:40.577368: step 27180, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:21m:16s remains)
INFO - root - 2017-12-11 07:29:45.924514: step 27190, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:07m:46s remains)
INFO - root - 2017-12-11 07:29:51.288286: step 27200, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:53m:46s remains)
2017-12-11 07:29:51.873153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014285211 -0.0010107175 -0.014863297 -0.021984506 -0.022790944 -0.020838682 -0.021271905 -0.025450364 -0.03134954 -0.03652012 -0.037955318 -0.033924539 -0.025426205 -0.015450912 -0.00824181][0.026991425 0.012127395 0.0029259627 0.0050390479 0.013632283 0.021202706 0.020470796 0.0096573532 -0.006375866 -0.022476479 -0.031937927 -0.030921724 -0.022075335 -0.010571796 -0.0033887073][0.032791819 0.026267225 0.033495404 0.056598835 0.0842333 0.1036364 0.10408234 0.082262836 0.04808956 0.01266474 -0.01172638 -0.018244246 -0.012401075 -0.0021209451 0.0032438489][0.032854334 0.041313931 0.073840238 0.12693432 0.18189996 0.21964827 0.22436124 0.18940544 0.13045719 0.067905866 0.022794526 0.00585356 0.0069338344 0.013831807 0.015198929][0.030292738 0.056590848 0.11739574 0.2036605 0.29080781 0.35319197 0.3680898 0.32251355 0.23611183 0.14065079 0.069902956 0.040385518 0.036170516 0.038966805 0.035661075][0.026160378 0.067144766 0.15145689 0.26566762 0.38200226 0.47007051 0.50014603 0.45003456 0.34010044 0.21358314 0.11777385 0.07573586 0.066611409 0.0661094 0.059409089][0.021974977 0.071225159 0.16895805 0.29923025 0.43446192 0.54217494 0.5869422 0.53712493 0.41198877 0.26529986 0.15532357 0.1088901 0.0999521 0.098711617 0.089186281][0.01856778 0.069682054 0.17047983 0.30492738 0.44760793 0.56511813 0.6182037 0.56992918 0.43997493 0.29100254 0.18689755 0.15255439 0.15475865 0.15601078 0.13933027][0.015313775 0.06290894 0.15843421 0.2881155 0.42844012 0.54579324 0.60055345 0.55694169 0.43657926 0.30624348 0.22951253 0.22407374 0.24814014 0.25295192 0.2203922][0.015418625 0.055213269 0.13751426 0.25227413 0.37924072 0.48676455 0.54005522 0.51040041 0.42080909 0.3328934 0.30098692 0.33234769 0.37909254 0.38317227 0.32746646][0.025482796 0.056223452 0.11932473 0.20954011 0.31176111 0.40029398 0.45025107 0.44330856 0.40216953 0.37474146 0.3967154 0.46226498 0.52307487 0.5212338 0.44486642][0.050137322 0.073223017 0.11586002 0.17763984 0.250054 0.31518146 0.35903543 0.37431753 0.38236514 0.41415107 0.48295474 0.5711596 0.63621926 0.62968045 0.54573393][0.085201874 0.10509164 0.13066499 0.16623713 0.21037851 0.25257036 0.28735641 0.3164854 0.35919812 0.43074676 0.52628493 0.62195659 0.68428433 0.67682034 0.59744239][0.12203681 0.14302839 0.15469371 0.16562702 0.18237209 0.2022099 0.22481385 0.2566413 0.31363219 0.39950916 0.49894705 0.5881052 0.64327121 0.63925892 0.57450271][0.15188447 0.17533675 0.17455815 0.161669 0.15197712 0.15119584 0.16046534 0.18565613 0.23872691 0.31772396 0.4033148 0.47529897 0.51890075 0.52021217 0.4782905]]...]
INFO - root - 2017-12-11 07:29:57.159672: step 27210, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 46h:39m:22s remains)
INFO - root - 2017-12-11 07:30:02.216203: step 27220, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 44h:04m:16s remains)
INFO - root - 2017-12-11 07:30:07.597831: step 27230, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:53m:39s remains)
INFO - root - 2017-12-11 07:30:12.860965: step 27240, loss = 0.71, batch loss = 0.65 (15.9 examples/sec; 0.502 sec/batch; 42h:32m:27s remains)
INFO - root - 2017-12-11 07:30:18.155323: step 27250, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 43h:51m:52s remains)
INFO - root - 2017-12-11 07:30:23.487535: step 27260, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:49m:15s remains)
INFO - root - 2017-12-11 07:30:28.893994: step 27270, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.525 sec/batch; 44h:32m:16s remains)
INFO - root - 2017-12-11 07:30:34.152303: step 27280, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:51m:24s remains)
INFO - root - 2017-12-11 07:30:39.563557: step 27290, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.554 sec/batch; 46h:55m:39s remains)
INFO - root - 2017-12-11 07:30:44.973191: step 27300, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 45h:21m:23s remains)
2017-12-11 07:30:45.574894: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34462067 0.31260487 0.27218616 0.23939304 0.21942493 0.20321488 0.18218778 0.16194603 0.1494102 0.15206492 0.16810307 0.19610243 0.23818325 0.29952481 0.37725034][0.384474 0.37026975 0.34196144 0.3197231 0.31143042 0.30734116 0.29503676 0.27833527 0.26444283 0.26326197 0.27261445 0.29087135 0.32369065 0.38117722 0.46323827][0.39233315 0.39198107 0.37942296 0.37712464 0.39309469 0.41423854 0.42145735 0.41346279 0.39650241 0.38319156 0.37301394 0.36825606 0.38089275 0.42559755 0.5045923][0.39303064 0.39510298 0.39138162 0.40751997 0.45071396 0.50288314 0.53552485 0.53911257 0.51747924 0.48595366 0.44832402 0.41505775 0.40560788 0.43772221 0.51331758][0.37659663 0.37674174 0.37863225 0.41084102 0.47937685 0.56177711 0.62096024 0.63856971 0.61377132 0.563221 0.49706414 0.43593177 0.40594772 0.42577407 0.49455458][0.34681794 0.34392631 0.34793475 0.39103916 0.47834927 0.58489347 0.66808289 0.70186794 0.67855704 0.61346269 0.52343386 0.43771702 0.38809633 0.39252031 0.4479903][0.32508847 0.314816 0.31401139 0.358187 0.45326859 0.57431006 0.67631853 0.72708184 0.71098137 0.63906235 0.53257066 0.42631158 0.3565757 0.34172371 0.37840402][0.31688905 0.29854003 0.28766021 0.32290092 0.41307148 0.5363909 0.64864028 0.71477151 0.71197355 0.64368558 0.53162068 0.41276705 0.32636312 0.29214522 0.30800954][0.30727586 0.28520811 0.26579463 0.28754783 0.3620384 0.47266951 0.58073252 0.65335238 0.66488111 0.61053675 0.50857115 0.39315167 0.30214107 0.25548908 0.25248539][0.27796289 0.25669634 0.23222235 0.24022061 0.29283845 0.37855789 0.46803311 0.53542119 0.55644768 0.52100247 0.44246951 0.34814662 0.26930225 0.22313382 0.21039881][0.23445933 0.21301261 0.18484636 0.17946388 0.20820189 0.26370212 0.32652327 0.37947255 0.40224305 0.38336062 0.33296511 0.270999 0.21977155 0.1908 0.18502671][0.19526598 0.17936964 0.15256467 0.13719535 0.14304011 0.16657259 0.19719113 0.22640221 0.24084379 0.23199423 0.20945881 0.18660602 0.17489442 0.17712626 0.19106971][0.16540228 0.1645465 0.1495952 0.13342468 0.1226235 0.1164071 0.11209987 0.11066176 0.10954724 0.10604775 0.10927822 0.12574081 0.15494621 0.18922627 0.22092292][0.12817065 0.14515536 0.14769804 0.13969226 0.12255887 0.09571062 0.062927336 0.0344237 0.018128607 0.017130723 0.039196186 0.085044242 0.14423206 0.19997859 0.23970647][0.07886029 0.10269626 0.11555651 0.11526038 0.09934181 0.06599012 0.021439774 -0.019775519 -0.044357363 -0.045326747 -0.015714452 0.042108934 0.11306693 0.17605866 0.21611735]]...]
INFO - root - 2017-12-11 07:30:50.878728: step 27310, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:08m:36s remains)
INFO - root - 2017-12-11 07:30:55.976605: step 27320, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:24m:34s remains)
INFO - root - 2017-12-11 07:31:01.318288: step 27330, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:51m:43s remains)
INFO - root - 2017-12-11 07:31:06.633058: step 27340, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 45h:09m:15s remains)
INFO - root - 2017-12-11 07:31:12.104645: step 27350, loss = 0.68, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 48h:38m:22s remains)
INFO - root - 2017-12-11 07:31:17.406741: step 27360, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:44m:42s remains)
INFO - root - 2017-12-11 07:31:22.840315: step 27370, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:13m:05s remains)
INFO - root - 2017-12-11 07:31:28.180086: step 27380, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:50m:55s remains)
INFO - root - 2017-12-11 07:31:33.441743: step 27390, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 44h:44m:09s remains)
INFO - root - 2017-12-11 07:31:38.720904: step 27400, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 43h:57m:29s remains)
2017-12-11 07:31:39.274209: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36160249 0.34635872 0.32041955 0.29282162 0.26630858 0.24264181 0.22525649 0.21624367 0.21068878 0.19929093 0.17944805 0.15912245 0.154498 0.17608707 0.22054119][0.44919991 0.4412649 0.41588765 0.38402754 0.35158455 0.32514051 0.31085646 0.30867416 0.30750039 0.29256141 0.26153567 0.22900793 0.21944007 0.24582881 0.29989615][0.51516736 0.51830971 0.49666417 0.46234211 0.425721 0.40055233 0.39576712 0.40713987 0.41460788 0.39672089 0.35203585 0.30319449 0.2849009 0.31285897 0.37167275][0.56001931 0.577099 0.56229413 0.52639973 0.48545113 0.46142045 0.46601138 0.49071255 0.50758374 0.48922566 0.43339777 0.36828226 0.33857802 0.36443672 0.42389619][0.55023259 0.58658242 0.583843 0.5520249 0.51271927 0.49347737 0.50499284 0.53435886 0.5513711 0.52903128 0.4656803 0.390738 0.35293233 0.37557095 0.43320382][0.49960569 0.55037248 0.55874217 0.53547394 0.50679773 0.50149643 0.52381313 0.553669 0.56126893 0.52696025 0.45474392 0.37461787 0.33252412 0.35020357 0.4015213][0.46251893 0.51921368 0.53275049 0.51555759 0.49850041 0.51006043 0.54677707 0.57852858 0.57329935 0.519763 0.43268594 0.3470206 0.30425331 0.32058918 0.36857536][0.45807391 0.51389879 0.52843583 0.51451176 0.50705594 0.53478384 0.58875912 0.62716717 0.6114701 0.53490984 0.42414418 0.32407254 0.27604312 0.29192013 0.34207916][0.454584 0.50544631 0.51957023 0.51050824 0.51198363 0.548638 0.61035609 0.6511054 0.62968045 0.53873897 0.41005382 0.29595152 0.24033917 0.25437227 0.30750722][0.44365695 0.48320407 0.49170896 0.48800942 0.5000568 0.540897 0.59961611 0.6345762 0.61042273 0.51829177 0.3876462 0.27147353 0.21498449 0.2309812 0.28867733][0.45444161 0.46944925 0.45966849 0.45358646 0.47541428 0.52251029 0.57909489 0.60777473 0.58275056 0.4968724 0.37686732 0.27228418 0.22545399 0.24909291 0.31117648][0.47562233 0.46579972 0.43210509 0.41589096 0.44358486 0.50074834 0.56232929 0.58878493 0.562032 0.48110592 0.37509257 0.28851533 0.25662154 0.2872819 0.34701476][0.47941372 0.45769963 0.4117153 0.38926515 0.41962951 0.48343822 0.54804766 0.57046795 0.5381912 0.4584524 0.36521223 0.29548505 0.27483952 0.30356783 0.3498089][0.44076231 0.41534096 0.36935055 0.34730911 0.37583619 0.43676668 0.4976517 0.5181067 0.48664236 0.41379189 0.33588651 0.28165522 0.26834166 0.2903353 0.3210263][0.36347908 0.33468845 0.29296884 0.27457944 0.30000645 0.35475415 0.41167974 0.4382908 0.41880575 0.36170247 0.30189204 0.26022607 0.2481164 0.2585226 0.2731086]]...]
INFO - root - 2017-12-11 07:31:44.700030: step 27410, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 45h:28m:32s remains)
INFO - root - 2017-12-11 07:31:49.991109: step 27420, loss = 0.69, batch loss = 0.63 (16.8 examples/sec; 0.476 sec/batch; 40h:18m:51s remains)
INFO - root - 2017-12-11 07:31:55.302768: step 27430, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 46h:15m:46s remains)
INFO - root - 2017-12-11 07:32:00.682216: step 27440, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 44h:43m:58s remains)
INFO - root - 2017-12-11 07:32:06.096321: step 27450, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.526 sec/batch; 44h:34m:12s remains)
INFO - root - 2017-12-11 07:32:11.413489: step 27460, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.532 sec/batch; 45h:02m:15s remains)
INFO - root - 2017-12-11 07:32:16.822172: step 27470, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.523 sec/batch; 44h:21m:14s remains)
INFO - root - 2017-12-11 07:32:22.142558: step 27480, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:14m:37s remains)
INFO - root - 2017-12-11 07:32:27.429209: step 27490, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 44h:16m:22s remains)
INFO - root - 2017-12-11 07:32:32.812570: step 27500, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:04m:04s remains)
2017-12-11 07:32:33.344814: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24717075 0.23964144 0.22716627 0.23761241 0.25699785 0.26593995 0.25922614 0.25330859 0.24996793 0.24421996 0.23936403 0.26130044 0.30077961 0.3219527 0.30243891][0.27062675 0.24962437 0.22393626 0.2248408 0.23942417 0.25189906 0.25716543 0.26694265 0.27809274 0.28209987 0.28157505 0.30071428 0.33051282 0.33808029 0.30341086][0.27935836 0.24349645 0.20112361 0.18616587 0.18993196 0.20354374 0.22268403 0.25224525 0.28043604 0.29539156 0.29965726 0.31305879 0.327834 0.31784216 0.2710042][0.29238504 0.24571188 0.18990469 0.15896292 0.14933456 0.16012174 0.18880773 0.23278163 0.27221146 0.29356298 0.29982188 0.30637875 0.3072055 0.28288019 0.22859327][0.31007886 0.25919566 0.19528608 0.15059055 0.12761648 0.13309199 0.1664274 0.21840332 0.26308125 0.28643313 0.29218689 0.29228511 0.28239542 0.2484535 0.1907206][0.33368343 0.28400114 0.21720429 0.16180636 0.125963 0.12275659 0.15366511 0.20525071 0.24888581 0.27050391 0.2751658 0.27290243 0.25992316 0.22423427 0.16778339][0.34524959 0.30088887 0.23689094 0.17638326 0.13075797 0.11703555 0.13945137 0.18453658 0.22361024 0.24116059 0.24392676 0.2432925 0.23562823 0.20760417 0.15795499][0.32847986 0.2926136 0.23656622 0.17760684 0.12772568 0.10485145 0.11621203 0.15218812 0.18579072 0.198538 0.19889195 0.2023904 0.20554568 0.19134983 0.15180935][0.27729344 0.25090626 0.20597431 0.15416355 0.10612186 0.078742288 0.082000367 0.11124743 0.14121321 0.15041733 0.14885224 0.15627818 0.16892472 0.16677506 0.13584107][0.19333772 0.17528929 0.14162439 0.10058753 0.059528489 0.033315495 0.033705842 0.059719887 0.087699108 0.095094092 0.0936486 0.10486887 0.12264905 0.12571362 0.098704629][0.084188037 0.071013421 0.047074262 0.017563736 -0.014034919 -0.036056604 -0.036454264 -0.013526621 0.012331367 0.020447826 0.023138277 0.040213522 0.061970927 0.0671792 0.043056872][-0.03077155 -0.04028986 -0.055096559 -0.073389865 -0.094644658 -0.1104635 -0.11004866 -0.090089388 -0.066236034 -0.055179819 -0.046663389 -0.025158904 -0.0019265881 0.0043284553 -0.014803515][-0.11527651 -0.12106687 -0.12762053 -0.13652235 -0.14869688 -0.15831555 -0.1567881 -0.14071186 -0.12095457 -0.10906295 -0.097839192 -0.077000514 -0.05617696 -0.049458705 -0.061061669][-0.15733299 -0.16024561 -0.16044699 -0.16254538 -0.16787751 -0.1722237 -0.16939916 -0.15726654 -0.14286955 -0.13298343 -0.12311448 -0.10764541 -0.092691228 -0.086696766 -0.090836719][-0.1575731 -0.15979357 -0.15688454 -0.1551652 -0.15587893 -0.15619242 -0.15230073 -0.14355926 -0.13396223 -0.12660116 -0.11899534 -0.10887673 -0.099462464 -0.094730474 -0.094392791]]...]
INFO - root - 2017-12-11 07:32:38.699400: step 27510, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:00m:28s remains)
INFO - root - 2017-12-11 07:32:44.067037: step 27520, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.521 sec/batch; 44h:09m:50s remains)
INFO - root - 2017-12-11 07:32:49.149511: step 27530, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:01m:25s remains)
INFO - root - 2017-12-11 07:32:54.485523: step 27540, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:23m:48s remains)
INFO - root - 2017-12-11 07:32:59.886266: step 27550, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:14m:46s remains)
INFO - root - 2017-12-11 07:33:05.184940: step 27560, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:11m:44s remains)
INFO - root - 2017-12-11 07:33:10.581600: step 27570, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 44h:44m:23s remains)
INFO - root - 2017-12-11 07:33:15.977497: step 27580, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:18m:57s remains)
INFO - root - 2017-12-11 07:33:21.298305: step 27590, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 43h:42m:29s remains)
INFO - root - 2017-12-11 07:33:26.609353: step 27600, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:27m:42s remains)
2017-12-11 07:33:27.130702: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36451241 0.37372002 0.36341166 0.33846694 0.29356521 0.24961998 0.22941923 0.24005349 0.27084821 0.30067864 0.32071772 0.30891714 0.26786157 0.22569594 0.199965][0.39390096 0.4143118 0.40924349 0.38331106 0.3312701 0.2710436 0.22535767 0.20475474 0.20547257 0.21378365 0.22313125 0.21058 0.16944824 0.12195445 0.083057739][0.40585175 0.4371163 0.43756583 0.41204703 0.35699159 0.28630745 0.22116081 0.17463674 0.14895113 0.13676621 0.1324698 0.11593128 0.076462589 0.027454713 -0.018888567][0.41946131 0.45598525 0.46004176 0.43572119 0.38281098 0.31250429 0.24518724 0.19367124 0.16160277 0.14344673 0.13144156 0.10961699 0.066642344 0.010404541 -0.049142428][0.43357325 0.47070789 0.47654513 0.45640674 0.41263729 0.35512257 0.30429763 0.27111098 0.25613976 0.25112036 0.24291094 0.2175872 0.16471647 0.091564842 0.0087883538][0.45963 0.48980632 0.49254975 0.47584194 0.44272739 0.40319651 0.37827826 0.37652066 0.39330685 0.41429192 0.41889742 0.39243588 0.32650337 0.2312527 0.12180631][0.50081795 0.51716959 0.51148885 0.49439156 0.46939445 0.44629779 0.44504717 0.47076261 0.514153 0.55581981 0.56914508 0.53868687 0.45969048 0.34597141 0.21725826][0.53387731 0.53484261 0.51859152 0.49894875 0.48027205 0.47057259 0.48402137 0.52024651 0.56858343 0.610417 0.61822051 0.57808149 0.4900808 0.37063766 0.2399455][0.5247643 0.50974029 0.48340547 0.4627406 0.45153889 0.45345315 0.47271308 0.50268656 0.53573942 0.56046557 0.55393517 0.50458771 0.41682839 0.30731991 0.19212705][0.46732035 0.4363755 0.39826855 0.3741388 0.36658254 0.3735899 0.38956633 0.40329665 0.41200197 0.41416475 0.39467975 0.34450492 0.27042106 0.18663222 0.10217229][0.37045291 0.32850319 0.28133509 0.25150487 0.2419284 0.24639416 0.25311562 0.24838273 0.23270106 0.21324281 0.18441126 0.14083299 0.089724176 0.040887866 -0.0041686557][0.22655833 0.19147746 0.14850102 0.12079135 0.11261027 0.11523628 0.11481611 0.097247876 0.064198568 0.028382065 -0.0070357611 -0.042484697 -0.0715558 -0.088835806 -0.0979352][0.059598878 0.038056616 0.0097747445 -0.0066949865 -0.0067604412 -6.4563756e-05 -1.2012482e-05 -0.019003514 -0.055199891 -0.094943568 -0.13015571 -0.15717293 -0.17049326 -0.16757172 -0.15384562][-0.079050079 -0.091984674 -0.10482544 -0.10923818 -0.10116419 -0.089064173 -0.084211655 -0.095405437 -0.12097191 -0.15077128 -0.1769357 -0.19403556 -0.19727515 -0.18560673 -0.16400923][-0.1577732 -0.16440727 -0.16588645 -0.1621919 -0.14996842 -0.13556734 -0.12720828 -0.12981056 -0.14116746 -0.15567166 -0.16872641 -0.17560163 -0.17245254 -0.15928847 -0.13989997]]...]
INFO - root - 2017-12-11 07:33:32.489440: step 27610, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 45h:49m:36s remains)
INFO - root - 2017-12-11 07:33:37.771720: step 27620, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:20m:58s remains)
INFO - root - 2017-12-11 07:33:42.745711: step 27630, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 47h:07m:41s remains)
INFO - root - 2017-12-11 07:33:48.024350: step 27640, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 45h:00m:12s remains)
INFO - root - 2017-12-11 07:33:53.408155: step 27650, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.542 sec/batch; 45h:52m:03s remains)
INFO - root - 2017-12-11 07:33:58.707717: step 27660, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 44h:58m:13s remains)
INFO - root - 2017-12-11 07:34:04.092122: step 27670, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.557 sec/batch; 47h:12m:22s remains)
INFO - root - 2017-12-11 07:34:09.464436: step 27680, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:29m:24s remains)
INFO - root - 2017-12-11 07:34:14.926156: step 27690, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 45h:03m:18s remains)
INFO - root - 2017-12-11 07:34:20.245990: step 27700, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:04m:31s remains)
2017-12-11 07:34:20.800706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013710602 0.047015745 0.13042577 0.2257577 0.31360403 0.37238806 0.40234512 0.39594477 0.34973434 0.27814376 0.20177869 0.14248814 0.10924023 0.11390153 0.17610773][-0.022241533 0.032980625 0.11627997 0.21923262 0.31966722 0.38944679 0.42488542 0.41572839 0.35810897 0.26970288 0.17904741 0.11344924 0.0805555 0.09241394 0.17244379][-0.026423257 0.024130579 0.10836019 0.21902959 0.33136973 0.41228682 0.45594645 0.44936991 0.38700509 0.2881403 0.18554716 0.10935111 0.066113614 0.071139127 0.15268724][-0.0245775 0.027859177 0.11979124 0.24198942 0.36728773 0.46205094 0.52002269 0.52525681 0.46823585 0.36749977 0.25553688 0.16129154 0.092854761 0.07262671 0.13449201][-0.01853331 0.042961832 0.14919929 0.28548405 0.42387789 0.53574234 0.61522365 0.64236927 0.60120869 0.50721544 0.38945851 0.27277943 0.16879715 0.10890719 0.13177393][-0.011873551 0.064130083 0.19061717 0.34499279 0.49910477 0.63230181 0.73794949 0.79022133 0.766379 0.67986691 0.556887 0.4179396 0.2790961 0.17895885 0.15683478][-0.007469269 0.083779484 0.23078616 0.4034493 0.5726546 0.72529155 0.85201013 0.92077965 0.9050054 0.81973511 0.69202447 0.5397957 0.38210708 0.25895736 0.20412403][-0.010527008 0.089951158 0.2488955 0.43105853 0.60463989 0.76141274 0.88936841 0.95414054 0.93153155 0.84154129 0.7148152 0.56711841 0.41688377 0.29770064 0.23251267][-0.02125757 0.079330921 0.23610081 0.41086543 0.5684967 0.70352703 0.80425763 0.84168595 0.80115646 0.70842737 0.59524232 0.47308213 0.35518202 0.26314765 0.20789504][-0.038144305 0.053252093 0.19398494 0.34492779 0.46897203 0.561844 0.61540878 0.6121074 0.5505951 0.46042189 0.37020898 0.28570172 0.21214318 0.15782683 0.12388538][-0.063225545 0.009024106 0.1196522 0.23256615 0.31268024 0.35737544 0.3633588 0.32439521 0.24984279 0.16870463 0.10401955 0.056765169 0.025505498 0.0084600607 0.0004178696][-0.0909139 -0.04453145 0.026180223 0.091734625 0.12470069 0.12652279 0.098538369 0.042025331 -0.029073969 -0.090926118 -0.12801597 -0.14339551 -0.14236636 -0.13130298 -0.11756017][-0.10430592 -0.079801321 -0.04507165 -0.02223121 -0.028423067 -0.056971978 -0.10270348 -0.16043915 -0.21628828 -0.25379348 -0.26570684 -0.25868824 -0.2396367 -0.21509352 -0.19108891][-0.092914373 -0.078600891 -0.065128177 -0.068582743 -0.097404592 -0.14092802 -0.19148612 -0.24217673 -0.28113538 -0.29862353 -0.29429939 -0.277068 -0.25358972 -0.22861116 -0.20534773][-0.061366279 -0.047270671 -0.040872026 -0.0542704 -0.090624005 -0.13731903 -0.18536872 -0.2273104 -0.25368291 -0.25991073 -0.24972273 -0.23233125 -0.21272443 -0.19339643 -0.17537971]]...]
INFO - root - 2017-12-11 07:34:26.236197: step 27710, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:28m:23s remains)
INFO - root - 2017-12-11 07:34:31.581235: step 27720, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 44h:31m:39s remains)
INFO - root - 2017-12-11 07:34:36.648148: step 27730, loss = 0.69, batch loss = 0.63 (22.4 examples/sec; 0.357 sec/batch; 30h:11m:05s remains)
INFO - root - 2017-12-11 07:34:41.905414: step 27740, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:25m:10s remains)
INFO - root - 2017-12-11 07:34:47.260093: step 27750, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:49m:08s remains)
INFO - root - 2017-12-11 07:34:52.591081: step 27760, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:09m:17s remains)
INFO - root - 2017-12-11 07:34:57.890506: step 27770, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 43h:40m:10s remains)
INFO - root - 2017-12-11 07:35:03.234072: step 27780, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:57m:27s remains)
INFO - root - 2017-12-11 07:35:08.623031: step 27790, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:53m:50s remains)
INFO - root - 2017-12-11 07:35:14.034267: step 27800, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:23m:04s remains)
2017-12-11 07:35:14.636498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05733848 -0.055229485 -0.052009556 -0.052788951 -0.053510509 -0.051937502 -0.047709212 -0.041431367 -0.035470873 -0.033153597 -0.037035543 -0.045763332 -0.05509128 -0.060669247 -0.060321297][-0.058092412 -0.056049366 -0.052457977 -0.051105935 -0.045852926 -0.034145985 -0.016901242 0.0025212895 0.017713517 0.022119928 0.012442451 -0.0079554068 -0.030616075 -0.047959484 -0.055992808][-0.053890612 -0.047624417 -0.037617724 -0.026152605 -0.0057047741 0.025065266 0.062076751 0.097291924 0.11945066 0.11938149 0.094543733 0.052416511 0.0077344347 -0.027842782 -0.048734765][-0.041654572 -0.024209574 0.0016792794 0.035155695 0.083196647 0.14370629 0.20732525 0.25955823 0.28324056 0.26770574 0.21448533 0.13789065 0.060813487 -0.00022856332 -0.03824142][-0.020108629 0.014510873 0.065312 0.13292263 0.22060606 0.31998903 0.41458783 0.48247689 0.50006008 0.45715353 0.36373973 0.24360108 0.12712729 0.035109539 -0.02351661][0.0050857239 0.058876805 0.13927893 0.247698 0.37901521 0.51565367 0.6342144 0.70736235 0.70765811 0.62847805 0.49178615 0.33118007 0.18021925 0.061500248 -0.014094376][0.023944886 0.093203552 0.19894737 0.34103149 0.50212276 0.65578103 0.77577287 0.83575088 0.81027687 0.69789833 0.53129178 0.35060245 0.18656029 0.059418645 -0.019859619][0.02695833 0.10243789 0.219152 0.37308273 0.53509182 0.67407936 0.76744074 0.79790545 0.74763054 0.61889815 0.44898048 0.27894026 0.13170907 0.021194931 -0.0442961][0.0071019521 0.074096106 0.17908491 0.31442422 0.44553262 0.54344279 0.59412831 0.59296513 0.5311777 0.41086271 0.26596442 0.13332629 0.027043818 -0.046634991 -0.084238522][-0.032907683 0.011457652 0.084105924 0.17640866 0.2575599 0.306865 0.31929448 0.29921639 0.243528 0.15127167 0.048310656 -0.035397716 -0.092239887 -0.12216951 -0.12720744][-0.0772847 -0.060704004 -0.027501805 0.015497605 0.04768905 0.058596827 0.048876476 0.025290847 -0.013490509 -0.07072603 -0.12913784 -0.16707258 -0.18076597 -0.17402208 -0.15301655][-0.10762969 -0.11251765 -0.10939366 -0.10231629 -0.10133372 -0.10980532 -0.12579945 -0.14429788 -0.16597749 -0.19377135 -0.21741365 -0.2235315 -0.21041837 -0.18353425 -0.15067583][-0.11684499 -0.13268809 -0.14413631 -0.15320726 -0.16315512 -0.17484172 -0.18763083 -0.19928432 -0.20963068 -0.21963567 -0.22351775 -0.21414071 -0.19121101 -0.160217 -0.12828153][-0.10853408 -0.12639724 -0.14021271 -0.15124398 -0.16000123 -0.16729523 -0.1742599 -0.18067311 -0.18555093 -0.18804203 -0.18472825 -0.17214888 -0.15076764 -0.12504487 -0.10042224][-0.090504549 -0.10453448 -0.11430454 -0.1212568 -0.12538706 -0.12782684 -0.1305792 -0.1340282 -0.13684614 -0.1374936 -0.1339203 -0.12448496 -0.10969599 -0.092536166 -0.076613694]]...]
INFO - root - 2017-12-11 07:35:19.947461: step 27810, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 46h:02m:09s remains)
INFO - root - 2017-12-11 07:35:25.327624: step 27820, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:44m:15s remains)
INFO - root - 2017-12-11 07:35:30.593544: step 27830, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 44h:10m:57s remains)
INFO - root - 2017-12-11 07:35:35.738753: step 27840, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:36m:47s remains)
INFO - root - 2017-12-11 07:35:41.066656: step 27850, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:27m:19s remains)
INFO - root - 2017-12-11 07:35:46.372638: step 27860, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:28m:51s remains)
INFO - root - 2017-12-11 07:35:51.763446: step 27870, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 45h:44m:09s remains)
INFO - root - 2017-12-11 07:35:57.070587: step 27880, loss = 0.68, batch loss = 0.62 (15.7 examples/sec; 0.508 sec/batch; 43h:00m:42s remains)
INFO - root - 2017-12-11 07:36:02.441209: step 27890, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:22m:44s remains)
INFO - root - 2017-12-11 07:36:07.809631: step 27900, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:43m:01s remains)
2017-12-11 07:36:08.335209: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36589327 0.38644889 0.38388687 0.36795616 0.33800277 0.28796369 0.22755219 0.17988352 0.15639329 0.15814689 0.18135376 0.22020881 0.26750642 0.3033475 0.31622037][0.40801948 0.42656726 0.40340275 0.35861436 0.30476472 0.24073832 0.17541181 0.12429895 0.099378206 0.10438935 0.13517493 0.18748462 0.24966532 0.29980949 0.32459417][0.42094046 0.44044676 0.400779 0.332442 0.26040438 0.18982022 0.12622172 0.077767409 0.054252781 0.059703276 0.09236905 0.14766763 0.21185841 0.26278993 0.28867343][0.41104409 0.43507197 0.39258865 0.31819242 0.24230735 0.17548664 0.12011269 0.077477284 0.054716446 0.056106634 0.082818031 0.13106813 0.18579884 0.22597863 0.2420281][0.38694188 0.42093271 0.39479613 0.33779457 0.27761358 0.22585548 0.181779 0.14303106 0.11504012 0.10394534 0.1147297 0.14694065 0.18481092 0.20724274 0.2061974][0.3623068 0.40555727 0.40328774 0.37682527 0.34548038 0.31746697 0.28750104 0.252859 0.2182613 0.19190966 0.18239219 0.19390479 0.21112183 0.21133763 0.18874091][0.35043916 0.39464438 0.411065 0.412313 0.40691516 0.40022984 0.38399389 0.35644117 0.32114014 0.28671044 0.26330152 0.25731435 0.25472686 0.23138508 0.18509306][0.35443944 0.39124018 0.41595593 0.43236578 0.44060928 0.44773805 0.44630396 0.43424398 0.41099936 0.38216338 0.35615975 0.33736244 0.31497958 0.26726505 0.19794779][0.36453676 0.39268881 0.41734278 0.43624014 0.44672403 0.46136427 0.47610962 0.48682973 0.48623684 0.47442695 0.45548373 0.42749491 0.38472968 0.31301206 0.22539878][0.36443266 0.385154 0.4036229 0.4161264 0.42220864 0.43970218 0.46751344 0.50050265 0.52571851 0.53703272 0.53302324 0.50312239 0.44638303 0.35750151 0.25980407][0.34444571 0.35918126 0.36917785 0.37318978 0.37420094 0.39296705 0.42888713 0.47712135 0.52323371 0.55640876 0.56999785 0.54467845 0.48345894 0.38882059 0.29017377][0.31328961 0.32525849 0.32919821 0.3267135 0.32417926 0.343087 0.38236964 0.43750179 0.49525934 0.54232377 0.567666 0.54580015 0.48366126 0.39069557 0.29758808][0.29193643 0.30187163 0.30116767 0.29382554 0.28918543 0.30632371 0.34362811 0.39720321 0.45639351 0.50643 0.53305274 0.50914687 0.44487625 0.35516098 0.26874354][0.27110243 0.28180361 0.28183156 0.27557495 0.2723079 0.28724825 0.31841442 0.36303285 0.41330707 0.45449057 0.47183752 0.44160494 0.37547171 0.290472 0.21064994][0.24337919 0.260651 0.27015734 0.27387878 0.27730581 0.29213852 0.31706154 0.34965694 0.38412362 0.40746865 0.40751132 0.36796042 0.30139393 0.22375229 0.15110892]]...]
INFO - root - 2017-12-11 07:36:13.705377: step 27910, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 47h:33m:05s remains)
INFO - root - 2017-12-11 07:36:19.106656: step 27920, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:54m:28s remains)
INFO - root - 2017-12-11 07:36:24.470932: step 27930, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 44h:10m:54s remains)
INFO - root - 2017-12-11 07:36:29.497211: step 27940, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:15m:49s remains)
INFO - root - 2017-12-11 07:36:34.792104: step 27950, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:47m:03s remains)
INFO - root - 2017-12-11 07:36:40.144926: step 27960, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:19m:45s remains)
INFO - root - 2017-12-11 07:36:45.493908: step 27970, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 44h:39m:50s remains)
INFO - root - 2017-12-11 07:36:50.903414: step 27980, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:04m:11s remains)
INFO - root - 2017-12-11 07:36:56.277672: step 27990, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 45h:56m:45s remains)
INFO - root - 2017-12-11 07:37:01.674617: step 28000, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:38m:49s remains)
2017-12-11 07:37:02.231347: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19888923 0.18631297 0.17378026 0.16227113 0.13862991 0.11610508 0.11035518 0.12995826 0.16464125 0.19344346 0.19721113 0.17274413 0.13210209 0.091248646 0.066133231][0.25104874 0.23986684 0.2216288 0.20381451 0.18226486 0.17456339 0.19496919 0.24334159 0.29797867 0.32617885 0.30796656 0.25176346 0.18366008 0.128763 0.10473359][0.28199831 0.27559754 0.25707528 0.23925588 0.22687903 0.24019194 0.28841481 0.36193556 0.42828596 0.44604111 0.39771244 0.30522847 0.21011882 0.14670917 0.13066192][0.28292257 0.28480428 0.27209035 0.26079553 0.26258513 0.29715481 0.36690781 0.45359749 0.5181337 0.51651508 0.4371174 0.31339094 0.19849361 0.13302311 0.12914318][0.24119559 0.2579675 0.25830379 0.26058987 0.28146011 0.33762175 0.42428383 0.51561606 0.56928957 0.5444032 0.43775064 0.29144275 0.16510524 0.10142075 0.10970701][0.1825594 0.21589543 0.2333606 0.25133637 0.28924423 0.360836 0.45604345 0.54505694 0.58516473 0.54130644 0.41745785 0.25949463 0.12893814 0.06911502 0.0874412][0.14844392 0.19214125 0.22113621 0.24856435 0.29325989 0.36598524 0.45492414 0.532558 0.56025249 0.50903749 0.38592592 0.23330517 0.1089942 0.055148136 0.078877032][0.14929362 0.19271518 0.22174449 0.24940291 0.29142702 0.35415035 0.42595857 0.48618415 0.50409204 0.45615211 0.34891158 0.21613786 0.10731455 0.060292553 0.0828828][0.17790647 0.21226723 0.23125064 0.25238606 0.2879383 0.33897579 0.39253464 0.43439785 0.44321004 0.40068409 0.3122634 0.20314436 0.1132746 0.074061371 0.093704663][0.22219199 0.24141596 0.24522504 0.25811976 0.28868431 0.33263186 0.37406155 0.40146828 0.40117848 0.36015797 0.28415748 0.1937208 0.12167179 0.092810825 0.1135089][0.27173728 0.26948434 0.2541469 0.25845337 0.28639358 0.32791838 0.36376187 0.38196707 0.37421754 0.33281574 0.26524055 0.19054507 0.13654129 0.12178335 0.14727661][0.3170903 0.28952169 0.25230688 0.24589097 0.26914063 0.30682343 0.33819216 0.35040143 0.33974236 0.30284688 0.24799412 0.19326068 0.16109775 0.16335051 0.19417386][0.36237696 0.31553066 0.25994363 0.24057305 0.2523312 0.27868465 0.30103937 0.30731711 0.29803151 0.271849 0.23488852 0.20235926 0.19093361 0.20775653 0.24307394][0.40351307 0.35212263 0.2889182 0.25861397 0.25419354 0.26303598 0.27268153 0.27316141 0.267681 0.25504059 0.23656288 0.22197777 0.2220618 0.24179539 0.2736612][0.42479274 0.38236442 0.3225562 0.28628004 0.26668018 0.25795734 0.25569296 0.25300032 0.25343472 0.25453919 0.25187898 0.24801445 0.2479016 0.25530621 0.26917702]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:37:07.579512: step 28010, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:18m:14s remains)
INFO - root - 2017-12-11 07:37:12.858507: step 28020, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 44h:22m:47s remains)
INFO - root - 2017-12-11 07:37:18.349743: step 28030, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:09m:15s remains)
INFO - root - 2017-12-11 07:37:23.429203: step 28040, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.506 sec/batch; 42h:48m:51s remains)
INFO - root - 2017-12-11 07:37:28.801673: step 28050, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:40m:00s remains)
INFO - root - 2017-12-11 07:37:34.184501: step 28060, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:54m:38s remains)
INFO - root - 2017-12-11 07:37:39.601349: step 28070, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 45h:16m:41s remains)
INFO - root - 2017-12-11 07:37:44.896525: step 28080, loss = 0.68, batch loss = 0.62 (15.9 examples/sec; 0.503 sec/batch; 42h:30m:52s remains)
INFO - root - 2017-12-11 07:37:50.298044: step 28090, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:08m:51s remains)
INFO - root - 2017-12-11 07:37:55.640605: step 28100, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:44m:51s remains)
2017-12-11 07:37:56.213976: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069826372 0.066875726 0.05235973 0.038311105 0.031249937 0.03160255 0.034091357 0.036516424 0.038123012 0.039623994 0.040441997 0.040063273 0.03931836 0.038960248 0.039201885][0.10547739 0.10455766 0.084903017 0.065596692 0.05674709 0.059079003 0.063797288 0.067188263 0.069182731 0.071850561 0.073529877 0.071631774 0.067687012 0.064429991 0.062909693][0.13860664 0.14055344 0.11730216 0.095014468 0.086186863 0.091336034 0.098349608 0.10245539 0.10497487 0.10914166 0.11158247 0.10701571 0.098105744 0.090374567 0.085795537][0.15509893 0.15864457 0.1339177 0.11074767 0.1026419 0.11044914 0.12030791 0.12685005 0.13195676 0.13858686 0.14120325 0.13270172 0.11788151 0.10550629 0.098367952][0.15331171 0.15663417 0.13257755 0.10994175 0.10264628 0.11268613 0.12603877 0.13690448 0.14634745 0.15561555 0.15748814 0.14432642 0.12384539 0.10814346 0.10051956][0.1344021 0.1382578 0.11625757 0.094851673 0.088285558 0.099731676 0.11564946 0.12990251 0.14254609 0.15321134 0.15411548 0.13741449 0.11282355 0.09523046 0.088959463][0.1068398 0.11207371 0.093046449 0.073713295 0.068170339 0.0800601 0.096879371 0.11259107 0.12641102 0.13700515 0.13677356 0.1177213 0.090201981 0.070818789 0.065990344][0.0747803 0.079645343 0.064817831 0.049884863 0.047510229 0.06048746 0.077873141 0.094856113 0.10935343 0.11862485 0.11543639 0.092983007 0.062421314 0.040883105 0.036732171][0.0451025 0.049415126 0.039862219 0.031399753 0.034315996 0.049569663 0.067900509 0.08647538 0.10172976 0.10832416 0.099530108 0.071427785 0.037218414 0.013849904 0.0099520134][0.030897122 0.035316352 0.029226756 0.02529707 0.031929307 0.048608165 0.067335345 0.087268628 0.10351194 0.10762283 0.093161225 0.059480142 0.022342935 -0.0019602338 -0.0063324929][0.036789503 0.043114427 0.0373231 0.033413094 0.039577171 0.054794706 0.071750715 0.09142331 0.10801698 0.11047583 0.092058592 0.054753009 0.016159717 -0.0082658492 -0.013538515][0.060795657 0.06895218 0.060347918 0.051988132 0.053732615 0.064751938 0.077341631 0.0936857 0.10883425 0.11063498 0.091362543 0.053603929 0.015396238 -0.0082284622 -0.013757035][0.094731331 0.10421071 0.090023927 0.074094534 0.069129474 0.074593693 0.080519915 0.089738026 0.10107078 0.10344943 0.087482132 0.053789172 0.019021764 -0.0020451813 -0.0067870067][0.12590118 0.13459304 0.1132756 0.089012973 0.077284917 0.077879094 0.077429473 0.078697123 0.085595831 0.089943223 0.081027023 0.0561655 0.028767332 0.012741541 0.0099324482][0.13622136 0.14232256 0.11643645 0.087589219 0.072224215 0.070747562 0.067261048 0.06348455 0.066977352 0.073681235 0.073279135 0.059892863 0.042071141 0.031740025 0.030593066]]...]
INFO - root - 2017-12-11 07:38:01.551326: step 28110, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:07m:02s remains)
INFO - root - 2017-12-11 07:38:06.963471: step 28120, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 45h:41m:27s remains)
INFO - root - 2017-12-11 07:38:12.312698: step 28130, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 45h:30m:03s remains)
INFO - root - 2017-12-11 07:38:17.707111: step 28140, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 45h:44m:21s remains)
INFO - root - 2017-12-11 07:38:22.754594: step 28150, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:40m:09s remains)
INFO - root - 2017-12-11 07:38:28.208427: step 28160, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 45h:03m:52s remains)
INFO - root - 2017-12-11 07:38:33.595755: step 28170, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 44h:27m:14s remains)
INFO - root - 2017-12-11 07:38:38.854230: step 28180, loss = 0.67, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 43h:26m:48s remains)
INFO - root - 2017-12-11 07:38:44.261860: step 28190, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:43m:02s remains)
INFO - root - 2017-12-11 07:38:49.677267: step 28200, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 44h:11m:11s remains)
2017-12-11 07:38:50.207472: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4332892 0.40720844 0.326083 0.21583742 0.11826304 0.071744874 0.09189225 0.17920502 0.31314614 0.45703682 0.56784475 0.61044192 0.58736283 0.51501805 0.42381886][0.45808083 0.40660056 0.30892128 0.19324711 0.098171785 0.058607493 0.0893745 0.18761164 0.33125642 0.48058194 0.59056479 0.62572968 0.58922243 0.50242352 0.39853474][0.42602408 0.35211435 0.25014448 0.14848167 0.074671954 0.052869409 0.09207315 0.18678834 0.31550121 0.44169998 0.52770805 0.54569495 0.50109065 0.41527224 0.31748462][0.3646557 0.28114179 0.1935555 0.12758785 0.09511745 0.10370225 0.15152402 0.2298155 0.32074317 0.39674619 0.43612251 0.42734346 0.37931326 0.30869219 0.23460154][0.31526387 0.23513363 0.17526345 0.15812264 0.18014294 0.22951439 0.28800574 0.34180781 0.37942302 0.3880066 0.36841047 0.33073369 0.2871733 0.24569672 0.21058249][0.30176771 0.23320891 0.20380886 0.23721704 0.31866702 0.41749081 0.49378669 0.52509868 0.50652134 0.44424587 0.36559024 0.30315822 0.27202821 0.26895425 0.28036824][0.30407864 0.2529124 0.25293484 0.33237579 0.47050929 0.62101388 0.72238332 0.74059635 0.67585206 0.55334228 0.42571718 0.34482056 0.3282567 0.36138797 0.41262367][0.28262392 0.24868202 0.27327 0.38756484 0.5683229 0.75887489 0.88339394 0.89774722 0.80599087 0.64874941 0.49696028 0.41257307 0.41195258 0.47096467 0.54502153][0.21454987 0.18932188 0.22626774 0.35452619 0.55041116 0.75427616 0.88850516 0.90567011 0.81284463 0.65991443 0.52437013 0.46515581 0.49129716 0.57062918 0.6523757][0.099121235 0.075034812 0.10962994 0.22793743 0.40837392 0.59664285 0.72632426 0.75434333 0.68831182 0.57923734 0.49889955 0.49285617 0.55910569 0.65956247 0.74291015][-0.037113223 -0.064893052 -0.042543184 0.047896534 0.18919957 0.33985102 0.45383295 0.4970665 0.47481206 0.43218952 0.42706615 0.4867366 0.59738129 0.71720743 0.79883641][-0.1470336 -0.17881413 -0.17322679 -0.12079682 -0.033157669 0.0654894 0.15303305 0.20607969 0.22595945 0.24810071 0.31397891 0.43313655 0.58055753 0.71203011 0.78784269][-0.1943327 -0.22659495 -0.23605312 -0.2203369 -0.18548091 -0.13950425 -0.084551945 -0.034512255 0.0087720342 0.071156725 0.18015867 0.33197004 0.49395263 0.6225639 0.68822366][-0.1839529 -0.21144265 -0.22808298 -0.23631787 -0.23832451 -0.23174477 -0.2078325 -0.17436939 -0.13466413 -0.068756625 0.041975092 0.18696474 0.33213308 0.4399662 0.49231949][-0.14102042 -0.15890586 -0.17398028 -0.19090715 -0.21078856 -0.22791316 -0.23055625 -0.22264345 -0.20661156 -0.16574232 -0.086442478 0.019790947 0.12314369 0.19707875 0.23465922]]...]
INFO - root - 2017-12-11 07:38:55.569872: step 28210, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:38m:52s remains)
INFO - root - 2017-12-11 07:39:00.937826: step 28220, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 43h:53m:36s remains)
INFO - root - 2017-12-11 07:39:06.267184: step 28230, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:49m:18s remains)
INFO - root - 2017-12-11 07:39:11.653909: step 28240, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 45h:44m:34s remains)
INFO - root - 2017-12-11 07:39:16.713984: step 28250, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:23m:01s remains)
INFO - root - 2017-12-11 07:39:22.122088: step 28260, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 44h:04m:22s remains)
INFO - root - 2017-12-11 07:39:27.649808: step 28270, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 47h:12m:51s remains)
INFO - root - 2017-12-11 07:39:32.865957: step 28280, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:15m:24s remains)
INFO - root - 2017-12-11 07:39:38.206533: step 28290, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:20m:15s remains)
INFO - root - 2017-12-11 07:39:43.583822: step 28300, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:48m:52s remains)
2017-12-11 07:39:44.104570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04949744 -0.0523577 -0.024797114 0.041649509 0.142522 0.25842181 0.36251813 0.43559331 0.46584848 0.4565846 0.4235909 0.38566041 0.36036196 0.35357925 0.36584431][-0.037652835 -0.032478273 0.0077539678 0.0911235 0.21229342 0.34924293 0.47299552 0.56132376 0.59822094 0.58347929 0.53547305 0.47842038 0.43690762 0.42558134 0.44770256][-0.0078535927 0.0037231904 0.050635867 0.14054853 0.2688497 0.41356313 0.54619277 0.64178729 0.67937243 0.65586907 0.59224361 0.51789397 0.46294406 0.44958237 0.48388448][0.031103609 0.047188569 0.094286107 0.18074845 0.30470157 0.44628692 0.57961261 0.67724073 0.71335357 0.68026954 0.60025054 0.50802422 0.44000447 0.42445686 0.46893373][0.079190537 0.096620671 0.13753182 0.21163327 0.32089797 0.44939816 0.57514322 0.66801769 0.69783366 0.65293235 0.55617225 0.44786927 0.37067637 0.356011 0.40973172][0.13008384 0.14417031 0.17450508 0.23229988 0.32214341 0.43167135 0.54119682 0.61856961 0.63220608 0.5703097 0.45604831 0.33492434 0.25421095 0.2447219 0.30776516][0.15812168 0.17036045 0.19590725 0.2460673 0.32540897 0.42181197 0.5138855 0.56780553 0.5556286 0.46910232 0.33328554 0.19922087 0.11634945 0.11087607 0.17875865][0.14978111 0.16702497 0.20055711 0.26093784 0.34969473 0.44931453 0.53079909 0.55817568 0.51197779 0.39202902 0.22988597 0.082548112 -0.002649727 -0.0083011016 0.055747982][0.11269354 0.14000146 0.19132252 0.27598593 0.39009705 0.50581259 0.583003 0.58437306 0.50120705 0.34537703 0.15808378 0.00094789127 -0.084289059 -0.092246674 -0.038550455][0.059067041 0.09704677 0.16729593 0.27578869 0.41109133 0.53556484 0.60270292 0.57941675 0.46844271 0.29204467 0.097032845 -0.056667577 -0.13571241 -0.14501911 -0.10366734][-0.003337021 0.04068419 0.12160742 0.24095349 0.38051492 0.49713194 0.54549575 0.50280142 0.38176712 0.20994511 0.031506211 -0.10287211 -0.16905209 -0.17739509 -0.14669216][-0.067423388 -0.026777105 0.051673003 0.16595429 0.29351172 0.38980922 0.41621548 0.36299312 0.24934363 0.10266546 -0.041270163 -0.14409308 -0.19055553 -0.19309941 -0.16803046][-0.12154903 -0.095729634 -0.035761271 0.055440158 0.15523003 0.22298463 0.23010978 0.17819215 0.087223373 -0.020730171 -0.11988661 -0.18303487 -0.20304491 -0.19405341 -0.17006747][-0.15529694 -0.15009178 -0.11934236 -0.065093949 -0.0046974071 0.031825386 0.027244508 -0.01117551 -0.068544149 -0.13179493 -0.18456338 -0.20911497 -0.2040263 -0.18346418 -0.15924019][-0.16738097 -0.18080176 -0.17865357 -0.16194259 -0.1397405 -0.1284361 -0.13586372 -0.15493149 -0.17837544 -0.20119376 -0.21431205 -0.20844084 -0.18598381 -0.15875785 -0.13522615]]...]
INFO - root - 2017-12-11 07:39:49.447865: step 28310, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.521 sec/batch; 44h:02m:40s remains)
INFO - root - 2017-12-11 07:39:54.836539: step 28320, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 45h:34m:37s remains)
INFO - root - 2017-12-11 07:40:00.180765: step 28330, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:27m:13s remains)
INFO - root - 2017-12-11 07:40:05.532813: step 28340, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 43h:59m:33s remains)
INFO - root - 2017-12-11 07:40:10.532558: step 28350, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 45h:52m:13s remains)
INFO - root - 2017-12-11 07:40:15.963460: step 28360, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:09m:02s remains)
INFO - root - 2017-12-11 07:40:21.273263: step 28370, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 44h:28m:42s remains)
INFO - root - 2017-12-11 07:40:26.606943: step 28380, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:47m:30s remains)
INFO - root - 2017-12-11 07:40:31.935543: step 28390, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 44h:09m:28s remains)
INFO - root - 2017-12-11 07:40:37.335106: step 28400, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:57m:50s remains)
2017-12-11 07:40:37.894643: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039014664 -0.026161913 -0.0068536117 0.00850476 0.015216483 0.013894408 0.007465594 0.00084490067 1.4641762e-05 0.0089725 0.025336917 0.040605359 0.046867214 0.042263322 0.027648669][-0.036494624 -0.014804532 0.016104512 0.043394648 0.057906341 0.057116017 0.044099089 0.026955381 0.014732711 0.014180623 0.025539063 0.040541086 0.048610587 0.046111941 0.03501375][-0.024365596 0.010848063 0.059094556 0.10463288 0.13306697 0.13744605 0.12035385 0.092693269 0.066068172 0.050652985 0.049706645 0.056176394 0.058334507 0.052205287 0.041046634][-0.0061510396 0.046762042 0.11742242 0.18725437 0.23760174 0.25710654 0.24539673 0.21308023 0.17343028 0.14003132 0.11814261 0.10300837 0.085788943 0.065161049 0.045304827][0.011846025 0.081396006 0.17435306 0.27077055 0.3506363 0.39873096 0.40780035 0.38301605 0.33690766 0.28538513 0.23357652 0.18146913 0.12959412 0.083906271 0.048550416][0.021743516 0.09873876 0.20517689 0.32243037 0.43347096 0.51959622 0.56268752 0.55769944 0.5140835 0.4481144 0.36274591 0.26565224 0.17228737 0.099034026 0.048093736][0.01681987 0.088190578 0.19382524 0.32016838 0.45611885 0.5797978 0.66035122 0.68084425 0.64650857 0.57255656 0.45950809 0.32384586 0.19655991 0.10344675 0.043173622][-0.0014972077 0.051937565 0.14168346 0.26146474 0.40668905 0.55318564 0.65959489 0.69994867 0.67646682 0.60309452 0.47850692 0.32568026 0.18554971 0.088678643 0.029935258][-0.02837703 0.00015119172 0.062899061 0.16048527 0.29372197 0.43858469 0.54933369 0.59650737 0.58106679 0.51570719 0.3986536 0.25550035 0.12890244 0.04766991 0.003681351][-0.057221361 -0.054264717 -0.023512483 0.040391788 0.14189924 0.26062989 0.35479689 0.39787507 0.39017892 0.34184042 0.25113121 0.14173616 0.0506749 -0.0003564072 -0.021257924][-0.081111081 -0.097051516 -0.094646722 -0.066290326 -0.0060989019 0.072290264 0.13850981 0.17402159 0.17896737 0.15835735 0.10906809 0.048101943 0.0015814772 -0.017955195 -0.019855037][-0.092477366 -0.11611158 -0.12977348 -0.12640648 -0.10148134 -0.060869403 -0.020216689 0.010844837 0.032091264 0.042160731 0.035309434 0.019462988 0.0086901132 0.0086721331 0.013623719][-0.088309228 -0.10814922 -0.12324873 -0.12969275 -0.12560609 -0.11001319 -0.085262194 -0.054196104 -0.017807493 0.017005559 0.041391406 0.054812863 0.062143564 0.066661671 0.067838483][-0.071514964 -0.079585359 -0.085180834 -0.088456362 -0.09004271 -0.0843832 -0.065277755 -0.03175104 0.014359623 0.06296356 0.10241285 0.12618606 0.13471667 0.1325811 0.12427753][-0.048244592 -0.04212239 -0.033239681 -0.025261892 -0.020976942 -0.013366669 0.0067642713 0.041641142 0.089371793 0.13963349 0.17886649 0.1977403 0.19647498 0.18239591 0.16340342]]...]
INFO - root - 2017-12-11 07:40:43.271064: step 28410, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:28m:42s remains)
INFO - root - 2017-12-11 07:40:48.750074: step 28420, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 48h:17m:55s remains)
INFO - root - 2017-12-11 07:40:54.100052: step 28430, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.524 sec/batch; 44h:18m:00s remains)
INFO - root - 2017-12-11 07:40:59.451494: step 28440, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:56m:13s remains)
INFO - root - 2017-12-11 07:41:04.554641: step 28450, loss = 0.68, batch loss = 0.62 (22.1 examples/sec; 0.363 sec/batch; 30h:37m:03s remains)
INFO - root - 2017-12-11 07:41:09.804129: step 28460, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:34m:40s remains)
INFO - root - 2017-12-11 07:41:15.122495: step 28470, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.514 sec/batch; 43h:27m:01s remains)
INFO - root - 2017-12-11 07:41:20.421145: step 28480, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 43h:38m:55s remains)
INFO - root - 2017-12-11 07:41:25.760321: step 28490, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:07m:35s remains)
INFO - root - 2017-12-11 07:41:31.092766: step 28500, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:42m:01s remains)
2017-12-11 07:41:31.644548: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017086664 0.014977029 0.0058876518 -0.0054188389 -0.010647601 -0.0063633383 0.0082322247 0.031886466 0.057725374 0.07817883 0.086312659 0.081541732 0.063618995 0.034867868 0.0011192623][0.073825367 0.077319607 0.068707153 0.056558184 0.05310059 0.063709848 0.09098421 0.13168584 0.1718961 0.19846079 0.20010234 0.17671117 0.13279529 0.077778019 0.022428827][0.14993814 0.16754982 0.16722746 0.15961897 0.16135043 0.1809707 0.22220506 0.2774612 0.32595533 0.34948692 0.33274367 0.27766445 0.19756822 0.11249278 0.037503812][0.23430844 0.27287519 0.2872301 0.2891269 0.30057392 0.33478695 0.39284968 0.45752603 0.50319463 0.51198292 0.46707985 0.37156817 0.24880904 0.13162522 0.0395332][0.30579993 0.36723718 0.40173849 0.42146015 0.45143318 0.50853229 0.58646435 0.65371513 0.68302941 0.66467083 0.58626372 0.45013797 0.28653347 0.13966779 0.034050424][0.34120876 0.42602384 0.49051902 0.5432238 0.60373724 0.69044423 0.7879411 0.84856427 0.84577423 0.78588957 0.66977662 0.49828181 0.3026219 0.13390769 0.020810243][0.32840553 0.43530697 0.53845376 0.63619596 0.73327315 0.8462593 0.95336771 0.99531943 0.94769537 0.83561277 0.67969656 0.48332283 0.27376586 0.10073145 -0.0083107][0.27929392 0.39935371 0.53359842 0.66599184 0.7826373 0.89699852 0.98999226 1.0028107 0.91348308 0.758777 0.57649088 0.37671146 0.18086067 0.028651338 -0.059840046][0.20186594 0.3167682 0.4566921 0.59311211 0.69725478 0.7807315 0.83617359 0.81848556 0.70917469 0.54501003 0.36949283 0.19706495 0.043276355 -0.0655192 -0.11908872][0.10359976 0.19295277 0.30937967 0.42004251 0.4897396 0.52783769 0.53973305 0.49987984 0.39538574 0.25575739 0.11919091 -0.0011465455 -0.09538722 -0.14972329 -0.16309565][0.007507626 0.056876909 0.12820366 0.19402964 0.223514 0.22280414 0.20440775 0.1596891 0.081555896 -0.010894018 -0.09199623 -0.15324512 -0.18956418 -0.19632129 -0.17819153][-0.061865866 -0.05147459 -0.027668394 -0.0073435442 -0.010080296 -0.032030825 -0.060123179 -0.093555219 -0.13586752 -0.1780576 -0.20800662 -0.22203812 -0.21820891 -0.19716175 -0.16540286][-0.10332979 -0.11927062 -0.12887573 -0.13945572 -0.15905708 -0.18405131 -0.20523866 -0.22002468 -0.23118305 -0.23701125 -0.23422176 -0.22172609 -0.19929686 -0.16999324 -0.139583][-0.12105593 -0.14704943 -0.16968361 -0.19107698 -0.21229738 -0.22961716 -0.23826011 -0.23751569 -0.23045997 -0.21872687 -0.20261537 -0.18202645 -0.15780927 -0.13343732 -0.11247505][-0.11673985 -0.13964775 -0.15914328 -0.17630953 -0.18996783 -0.19765303 -0.1976459 -0.19057126 -0.17917559 -0.16544132 -0.15036033 -0.13413318 -0.11782384 -0.10374781 -0.093152121]]...]
INFO - root - 2017-12-11 07:41:36.980390: step 28510, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:31m:03s remains)
INFO - root - 2017-12-11 07:41:42.354412: step 28520, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:45m:24s remains)
INFO - root - 2017-12-11 07:41:47.737652: step 28530, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:18m:44s remains)
INFO - root - 2017-12-11 07:41:53.071812: step 28540, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:13m:45s remains)
INFO - root - 2017-12-11 07:41:58.438361: step 28550, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:47m:36s remains)
INFO - root - 2017-12-11 07:42:03.583040: step 28560, loss = 0.71, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 44h:50m:05s remains)
INFO - root - 2017-12-11 07:42:09.048419: step 28570, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 44h:16m:25s remains)
INFO - root - 2017-12-11 07:42:14.344575: step 28580, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:37m:18s remains)
INFO - root - 2017-12-11 07:42:19.689772: step 28590, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 45h:07m:26s remains)
INFO - root - 2017-12-11 07:42:24.982816: step 28600, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 45h:46m:10s remains)
2017-12-11 07:42:25.561260: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075864695 0.09374778 0.16855231 0.28818354 0.4236666 0.53364277 0.586942 0.56325024 0.4729735 0.36067635 0.26815638 0.20950072 0.17655478 0.15028436 0.11701143][0.071671642 0.084324867 0.15567346 0.27395704 0.4120281 0.53143424 0.596563 0.58162051 0.49123463 0.37004048 0.26453242 0.19482188 0.15763317 0.13272321 0.10278352][0.066122182 0.069321 0.1286093 0.23426872 0.36276355 0.47923627 0.54777133 0.53833032 0.45334974 0.3344194 0.22714628 0.15441182 0.11744973 0.09721633 0.074112058][0.0636356 0.055940241 0.098450504 0.18702699 0.30225879 0.41215807 0.48001564 0.47430193 0.39646628 0.28463155 0.1813762 0.11160348 0.079425186 0.066649362 0.051961016][0.050533649 0.037154704 0.06811063 0.14533226 0.25143161 0.35487947 0.41897637 0.41403073 0.34307528 0.24028462 0.14509977 0.083747275 0.061453752 0.059335414 0.053534754][0.015447876 0.0055853236 0.035965938 0.11207852 0.21727891 0.31856069 0.37987721 0.37531769 0.31161347 0.21900836 0.13525687 0.08796981 0.080971 0.092279226 0.093878448][-0.029758295 -0.03426557 -0.0003749466 0.077866718 0.18546824 0.2899974 0.35622358 0.36131382 0.31350118 0.23822136 0.17106862 0.13956116 0.14480689 0.16335543 0.16561237][-0.069567047 -0.072932757 -0.039626405 0.036981389 0.14566529 0.25763881 0.3383528 0.36490232 0.34411454 0.29563478 0.25083882 0.23369312 0.24276455 0.25656977 0.24968864][-0.086527534 -0.099242039 -0.073020637 -0.0018133165 0.10631296 0.22663575 0.32534322 0.37671334 0.38450477 0.36169574 0.33613446 0.32758486 0.3328788 0.33440173 0.31452334][-0.078024559 -0.10762309 -0.095048189 -0.0340665 0.067872368 0.19016209 0.30107918 0.37335658 0.40521866 0.4011651 0.38661247 0.37961143 0.37659457 0.3642281 0.33229214][-0.048477642 -0.0985793 -0.10611475 -0.063132294 0.022188501 0.13469343 0.24715117 0.33322638 0.38451347 0.39545473 0.38620758 0.37434471 0.35879794 0.33236203 0.29094502][0.00016231157 -0.071460962 -0.10353159 -0.085930571 -0.026448801 0.065082595 0.16721553 0.25599796 0.31823531 0.34006581 0.33383435 0.31413656 0.2843596 0.24524243 0.19834235][0.050798453 -0.040000886 -0.095405512 -0.10382697 -0.071611971 -0.0055494616 0.077509694 0.15594119 0.2151894 0.2389307 0.23254125 0.20591244 0.16619419 0.12099816 0.076188][0.083379507 -0.019700056 -0.092384435 -0.12236704 -0.11491907 -0.075413719 -0.017215444 0.041507713 0.087765642 0.1068931 0.10023818 0.073149227 0.033700235 -0.0075739673 -0.042716417][0.084592551 -0.017390728 -0.0950328 -0.13668904 -0.14772682 -0.13229313 -0.09999688 -0.063897528 -0.03341319 -0.019681195 -0.023243906 -0.042810943 -0.072467625 -0.10277437 -0.12589432]]...]
INFO - root - 2017-12-11 07:42:30.963676: step 28610, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:44m:14s remains)
INFO - root - 2017-12-11 07:42:36.260549: step 28620, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 43h:43m:27s remains)
INFO - root - 2017-12-11 07:42:41.567343: step 28630, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:58m:25s remains)
INFO - root - 2017-12-11 07:42:47.073894: step 28640, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:34m:01s remains)
INFO - root - 2017-12-11 07:42:52.466701: step 28650, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:34m:01s remains)
INFO - root - 2017-12-11 07:42:57.549312: step 28660, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:31m:23s remains)
INFO - root - 2017-12-11 07:43:02.963852: step 28670, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 47h:58m:37s remains)
INFO - root - 2017-12-11 07:43:08.309135: step 28680, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:37m:27s remains)
INFO - root - 2017-12-11 07:43:13.628447: step 28690, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:41m:10s remains)
INFO - root - 2017-12-11 07:43:19.013193: step 28700, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 46h:55m:02s remains)
2017-12-11 07:43:19.530780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.6333462 0.66285294 0.58126527 0.42266041 0.24175297 0.094215333 0.0042594378 -0.03458057 -0.046333637 -0.042347673 -0.029462678 -0.016292725 -0.0061021349 0.0015578843 0.0072276918][0.47893411 0.52287477 0.4760403 0.35567325 0.20958389 0.089025542 0.019979658 -0.010668236 -0.024073083 -0.027538262 -0.024345338 -0.018445672 -0.011431679 -0.0034862063 0.0040096054][0.26878884 0.31547439 0.30651277 0.24816574 0.17037812 0.10539103 0.070174895 0.051694255 0.032605942 0.012528703 -0.0039263843 -0.013819405 -0.016082527 -0.011546624 -0.0037371484][0.10708332 0.1364025 0.15210365 0.15524267 0.15698765 0.16378616 0.17334707 0.16886097 0.13844839 0.09156853 0.044020113 0.0082021225 -0.011462945 -0.016335556 -0.011931357][0.043397997 0.039382908 0.060964588 0.11171678 0.18659586 0.26646721 0.32627311 0.33891696 0.29473215 0.21321113 0.12397326 0.051683582 0.0057251723 -0.01524001 -0.018858654][0.098195083 0.053035405 0.057919573 0.13040744 0.25525406 0.39378172 0.49963194 0.53017813 0.47330952 0.35596558 0.22187097 0.1083813 0.031074334 -0.010379975 -0.025545862][0.2357654 0.14800335 0.11664275 0.18075232 0.32416457 0.49889272 0.642913 0.69614947 0.63730264 0.49438021 0.32182688 0.16928849 0.060590051 -0.0024870529 -0.031084841][0.38915372 0.26125908 0.18111286 0.21190272 0.34479058 0.53090006 0.70157647 0.78312618 0.74079704 0.59389836 0.39933693 0.2174333 0.083546974 0.0037183077 -0.035614688][0.50048304 0.34427035 0.21670906 0.202993 0.30545521 0.47831026 0.65451926 0.75621879 0.73954105 0.60963637 0.41582707 0.22336644 0.078744121 -0.0053834841 -0.045603275][0.5227958 0.36506116 0.21184291 0.16246977 0.22931592 0.37042421 0.52616996 0.62636626 0.6278708 0.52445734 0.35252669 0.17367314 0.038442332 -0.03470714 -0.064069979][0.4533242 0.32124519 0.17783873 0.11928862 0.16255374 0.268253 0.38584498 0.46223196 0.4648827 0.38400528 0.24308212 0.094000772 -0.017210785 -0.070520334 -0.083554067][0.33959982 0.24601975 0.1444473 0.10876673 0.1512415 0.23260529 0.30899096 0.34560987 0.32719484 0.25126687 0.13458452 0.016475793 -0.067671388 -0.10090455 -0.098445348][0.22848813 0.17987001 0.13797861 0.14944734 0.21438359 0.28994849 0.33396083 0.32462642 0.26444185 0.16724345 0.055020474 -0.042953085 -0.10529946 -0.12264653 -0.10837068][0.14909646 0.13947299 0.15621132 0.22275347 0.32526314 0.41526365 0.44700411 0.40304366 0.29811347 0.16396895 0.03504153 -0.06149511 -0.11537641 -0.1266727 -0.10847273][0.11062365 0.12561698 0.18368877 0.29484865 0.43641827 0.55585784 0.5998131 0.54592395 0.4126274 0.24379054 0.086489327 -0.027173219 -0.089532666 -0.10772721 -0.095920868]]...]
INFO - root - 2017-12-11 07:43:24.913140: step 28710, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:14m:47s remains)
INFO - root - 2017-12-11 07:43:30.274625: step 28720, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 44h:02m:48s remains)
INFO - root - 2017-12-11 07:43:35.526238: step 28730, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 44h:08m:56s remains)
INFO - root - 2017-12-11 07:43:40.831645: step 28740, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 43h:50m:40s remains)
INFO - root - 2017-12-11 07:43:46.168329: step 28750, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:37m:41s remains)
INFO - root - 2017-12-11 07:43:51.208919: step 28760, loss = 0.67, batch loss = 0.62 (19.4 examples/sec; 0.413 sec/batch; 34h:48m:40s remains)
INFO - root - 2017-12-11 07:43:56.536077: step 28770, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:17m:02s remains)
INFO - root - 2017-12-11 07:44:01.884038: step 28780, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 46h:16m:34s remains)
INFO - root - 2017-12-11 07:44:07.313586: step 28790, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 46h:44m:16s remains)
INFO - root - 2017-12-11 07:44:12.772723: step 28800, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:01m:42s remains)
2017-12-11 07:44:13.339926: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12029508 0.10641395 0.098220274 0.097687729 0.10097679 0.10625304 0.11642795 0.13671042 0.16710027 0.20303883 0.23170412 0.24237455 0.23599294 0.22091988 0.20512216][0.14591181 0.13264464 0.12585513 0.12837981 0.13596082 0.14671843 0.16414255 0.1938553 0.23509678 0.28242695 0.31972978 0.33399078 0.32744464 0.31123671 0.29599854][0.15096821 0.14122318 0.1382852 0.14684276 0.16382056 0.18609557 0.2143167 0.25131604 0.29517463 0.34173578 0.37540847 0.38468117 0.37468439 0.3585569 0.34832034][0.14484736 0.14131819 0.14331549 0.15901414 0.18882404 0.22816102 0.27145648 0.31503773 0.35429129 0.38750568 0.40365285 0.39750162 0.37895253 0.36166713 0.35747692][0.13690275 0.14131451 0.14761198 0.16813596 0.2093125 0.26685348 0.32801566 0.38026223 0.41395864 0.42860314 0.41900104 0.38918653 0.35545027 0.33203694 0.33021125][0.13426331 0.14880201 0.15965702 0.18212983 0.22963062 0.30050826 0.37711775 0.43818656 0.46740004 0.46345809 0.42634007 0.3692199 0.31472516 0.27893308 0.27359208][0.14786097 0.17347738 0.18782355 0.20592578 0.24889366 0.32119641 0.40460852 0.47112027 0.49757785 0.48013246 0.42177874 0.34182972 0.26755995 0.21759954 0.20457551][0.18347718 0.22137645 0.23564467 0.23948206 0.26249596 0.31913453 0.39546344 0.46060088 0.48606145 0.46424395 0.39814407 0.30798855 0.22237062 0.16194347 0.14042753][0.23752736 0.28417638 0.29184169 0.2726371 0.26397628 0.29170063 0.34927464 0.4069429 0.4327898 0.41535947 0.35498983 0.2688013 0.18374291 0.12139823 0.096029721][0.2874589 0.33536896 0.33259627 0.29216132 0.25543866 0.25572494 0.29290539 0.34060976 0.36575007 0.35436007 0.30446279 0.23017567 0.15512784 0.10013812 0.077784613][0.32029229 0.36234558 0.35129705 0.30082256 0.25111088 0.23626091 0.25964805 0.29845032 0.31963614 0.30850849 0.264069 0.20064524 0.13891096 0.096622564 0.082089923][0.33136943 0.36551526 0.34840971 0.29617152 0.24551612 0.22764687 0.24630672 0.28058496 0.29733589 0.28215569 0.237213 0.17896919 0.12771094 0.097985558 0.092585742][0.31745565 0.34213522 0.32098934 0.27243555 0.22862394 0.21550886 0.23483726 0.26697892 0.27964652 0.25948578 0.21157204 0.15425065 0.10896265 0.088245243 0.090123557][0.26217073 0.27717671 0.25543162 0.2165506 0.18612821 0.1826818 0.20472625 0.23457597 0.24346378 0.21971636 0.16978319 0.11274379 0.071370229 0.056567833 0.06286408][0.16310316 0.17088571 0.15383527 0.12897108 0.11461438 0.12121256 0.14465626 0.1703617 0.17567657 0.15140216 0.10327213 0.049456917 0.012627077 0.0014181481 0.008402939]]...]
INFO - root - 2017-12-11 07:44:18.708398: step 28810, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 47h:18m:07s remains)
INFO - root - 2017-12-11 07:44:24.101644: step 28820, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 46h:04m:02s remains)
INFO - root - 2017-12-11 07:44:29.507710: step 28830, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:40m:11s remains)
INFO - root - 2017-12-11 07:44:34.985401: step 28840, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:44m:12s remains)
INFO - root - 2017-12-11 07:44:40.191737: step 28850, loss = 0.67, batch loss = 0.61 (15.8 examples/sec; 0.506 sec/batch; 42h:41m:01s remains)
INFO - root - 2017-12-11 07:44:45.576890: step 28860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:18m:09s remains)
INFO - root - 2017-12-11 07:44:50.645186: step 28870, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:48m:38s remains)
INFO - root - 2017-12-11 07:44:56.037379: step 28880, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 44h:33m:51s remains)
INFO - root - 2017-12-11 07:45:01.381665: step 28890, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 44h:13m:32s remains)
INFO - root - 2017-12-11 07:45:06.764771: step 28900, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 43h:33m:19s remains)
2017-12-11 07:45:07.371824: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21115527 0.31074911 0.39276791 0.45079648 0.49458441 0.528309 0.54247689 0.53602606 0.50790554 0.45333049 0.36481079 0.25124371 0.13690722 0.036570039 -0.041387752][0.23839712 0.3551324 0.45378456 0.52614963 0.58535731 0.63728029 0.66638428 0.66661417 0.63504952 0.56598192 0.45154247 0.3083041 0.16997619 0.055899676 -0.026333772][0.24175884 0.36724371 0.47594243 0.55752724 0.6277194 0.692943 0.73355556 0.73994118 0.70736676 0.62914079 0.49797207 0.33639333 0.18590274 0.069036886 -0.00992894][0.2177866 0.34023646 0.45084077 0.53763777 0.61696815 0.69264561 0.74285907 0.75520307 0.72349417 0.64134336 0.50339794 0.33631191 0.18482168 0.072402135 -9.70459e-06][0.16922005 0.27685618 0.38130161 0.46998298 0.55725956 0.64201874 0.70188135 0.7211172 0.69239324 0.61192244 0.47799221 0.31810844 0.17457236 0.070265584 0.0046276706][0.12127535 0.21131843 0.30825111 0.39919496 0.49485913 0.5881446 0.65679139 0.6811493 0.65395522 0.57625663 0.4504455 0.30211964 0.16766578 0.069389515 0.0073164525][0.089010946 0.16462085 0.25503358 0.34627405 0.44632843 0.54346228 0.617103 0.64281851 0.61461413 0.5394004 0.42250064 0.28544486 0.15872471 0.06514179 0.0057896273][0.065766275 0.12900922 0.21283506 0.30193579 0.40332979 0.5025242 0.58006239 0.6057784 0.57498735 0.50116318 0.39134818 0.26261708 0.14157526 0.053004429 -0.0018818284][0.0404636 0.091093943 0.16519284 0.2470049 0.34354696 0.44056246 0.51925451 0.54423451 0.5120424 0.44198355 0.34083563 0.22073075 0.10649844 0.02581859 -0.020592842][0.0073486944 0.044061478 0.10313564 0.16914935 0.24971859 0.33430594 0.40607154 0.42817494 0.39822432 0.33771217 0.25105205 0.14477891 0.043005 -0.02439422 -0.057067264][-0.04365604 -0.024583984 0.013610328 0.056617934 0.11265852 0.17631537 0.23367307 0.25032595 0.22512624 0.17835829 0.11167638 0.026638381 -0.053623796 -0.099814966 -0.11242057][-0.10949052 -0.11185515 -0.098157465 -0.080762744 -0.051792104 -0.012323022 0.027028367 0.037088085 0.017601522 -0.014051318 -0.057546902 -0.11456323 -0.16489217 -0.18338536 -0.17206372][-0.16070233 -0.18148494 -0.18916123 -0.19338292 -0.18802848 -0.17115916 -0.14987086 -0.1459347 -0.15935583 -0.17644191 -0.19709492 -0.22481522 -0.24484782 -0.23861878 -0.20916983][-0.1821329 -0.21226741 -0.23219237 -0.24925324 -0.25960919 -0.25979629 -0.25327238 -0.25368097 -0.26075104 -0.26545802 -0.2679823 -0.27240619 -0.27003366 -0.24934593 -0.21330208][-0.1778311 -0.20882042 -0.23200364 -0.25295931 -0.26945382 -0.2782931 -0.28015479 -0.28177705 -0.28286511 -0.27926454 -0.27166751 -0.26326275 -0.24981053 -0.22558422 -0.19275685]]...]
INFO - root - 2017-12-11 07:45:12.699391: step 28910, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:57m:28s remains)
INFO - root - 2017-12-11 07:45:18.055602: step 28920, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 46h:12m:25s remains)
INFO - root - 2017-12-11 07:45:23.411894: step 28930, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 44h:19m:18s remains)
INFO - root - 2017-12-11 07:45:28.699019: step 28940, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 45h:29m:28s remains)
INFO - root - 2017-12-11 07:45:34.048848: step 28950, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:19m:06s remains)
INFO - root - 2017-12-11 07:45:39.437485: step 28960, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 43h:30m:26s remains)
INFO - root - 2017-12-11 07:45:44.568998: step 28970, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 46h:35m:42s remains)
INFO - root - 2017-12-11 07:45:49.913263: step 28980, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:51m:29s remains)
INFO - root - 2017-12-11 07:45:55.379757: step 28990, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 43h:54m:35s remains)
INFO - root - 2017-12-11 07:46:00.658093: step 29000, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 48h:44m:18s remains)
2017-12-11 07:46:01.258588: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14371821 0.13097972 0.11711235 0.10597333 0.09991727 0.09642262 0.093927518 0.086132124 0.076282457 0.068941914 0.069350094 0.079621114 0.093923643 0.11978091 0.16262132][0.19964835 0.18446945 0.16319524 0.14479199 0.13288224 0.1245414 0.11765545 0.10592425 0.0935922 0.0844261 0.0855089 0.10126503 0.12404833 0.15942149 0.2117101][0.23785283 0.22282481 0.19842216 0.17602229 0.16100907 0.15162973 0.14343305 0.1294726 0.1145879 0.10246524 0.10112216 0.11750052 0.14381722 0.18351668 0.23880403][0.25906235 0.24521479 0.22141613 0.19943754 0.18514387 0.17994098 0.17526267 0.16283675 0.14762875 0.13382502 0.12905173 0.14205748 0.16624285 0.20264132 0.25130922][0.26586658 0.25514752 0.23591755 0.21900383 0.20793685 0.2085678 0.20872849 0.20025586 0.18852614 0.17657484 0.16980585 0.17672318 0.19391064 0.22013222 0.25353277][0.25365692 0.25108764 0.24453245 0.23997709 0.23634699 0.24303821 0.24592008 0.23893777 0.22950812 0.21839412 0.20798652 0.20483801 0.21065855 0.22267458 0.23756194][0.23127189 0.24152495 0.25472388 0.26767346 0.2745198 0.28644827 0.28956032 0.28069031 0.26976711 0.25467065 0.23568858 0.21759847 0.20783302 0.20477766 0.20517252][0.21400854 0.2383859 0.27164897 0.30009952 0.3146694 0.32769471 0.32746205 0.31447315 0.30069396 0.28129235 0.25359094 0.22095563 0.19558914 0.1802016 0.17299372][0.21205094 0.24700907 0.29327402 0.32938823 0.34418604 0.35081586 0.34194905 0.322369 0.30523983 0.28426325 0.25431839 0.21681923 0.18545385 0.16664222 0.15970059][0.22249666 0.26186809 0.31081048 0.34482279 0.35296452 0.34773982 0.32730514 0.29938772 0.27773654 0.25660688 0.23107892 0.20036662 0.17487854 0.16210081 0.16166838][0.24048503 0.27914256 0.32316938 0.34827161 0.34615317 0.32794961 0.29700464 0.26099768 0.23295709 0.2101173 0.1910681 0.17435469 0.16264057 0.16126941 0.16920365][0.26010478 0.29456162 0.32875356 0.34166795 0.327853 0.29794121 0.25955409 0.21872437 0.18576343 0.16119501 0.14861082 0.14814661 0.15287095 0.16327396 0.17749631][0.27461547 0.30521297 0.32982504 0.33183876 0.30679455 0.265776 0.22074474 0.17735657 0.14334685 0.12017958 0.11436683 0.12841693 0.14767002 0.16666147 0.18247244][0.27891064 0.3084386 0.32772574 0.32356167 0.2908811 0.24071416 0.18925194 0.14388922 0.11133946 0.091257952 0.090113349 0.11210319 0.13867909 0.15992652 0.17283726][0.27596247 0.307326 0.32637987 0.32092682 0.28613874 0.23180968 0.17622361 0.12866114 0.097159445 0.079785772 0.079886384 0.10152978 0.12515405 0.13911793 0.14346434]]...]
INFO - root - 2017-12-11 07:46:06.669243: step 29010, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 44h:05m:44s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:46:11.949443: step 29020, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 43h:26m:15s remains)
INFO - root - 2017-12-11 07:46:17.279208: step 29030, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.521 sec/batch; 43h:53m:45s remains)
INFO - root - 2017-12-11 07:46:22.612444: step 29040, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:48m:00s remains)
INFO - root - 2017-12-11 07:46:27.972191: step 29050, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 46h:48m:30s remains)
INFO - root - 2017-12-11 07:46:33.356684: step 29060, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:05m:50s remains)
INFO - root - 2017-12-11 07:46:38.437293: step 29070, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:28m:47s remains)
INFO - root - 2017-12-11 07:46:43.809710: step 29080, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:51m:16s remains)
INFO - root - 2017-12-11 07:46:49.044973: step 29090, loss = 0.67, batch loss = 0.61 (15.6 examples/sec; 0.512 sec/batch; 43h:08m:34s remains)
INFO - root - 2017-12-11 07:46:54.360040: step 29100, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 45h:15m:13s remains)
2017-12-11 07:46:54.924585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041000541 -0.038458247 -0.036018625 -0.034968842 -0.03463139 -0.036303833 -0.040376958 -0.044900909 -0.04765496 -0.048318751 -0.047705237 -0.044080079 -0.038043946 -0.032892037 -0.03038267][-0.023009656 -0.012929362 -0.0039084689 0.0023455874 0.0051499121 0.0035910467 -0.0025596218 -0.0098956516 -0.017089285 -0.02486092 -0.033282567 -0.037611563 -0.037030533 -0.035086047 -0.033063523][0.0046345321 0.027860878 0.049845673 0.067478992 0.078802295 0.081555881 0.074763127 0.062516525 0.045477983 0.022997148 -0.002881713 -0.023565929 -0.035226136 -0.04082058 -0.040803786][0.0382258 0.076941468 0.11698864 0.15401579 0.1834754 0.19886161 0.19626145 0.17898582 0.14703456 0.1013352 0.048298411 0.0017940942 -0.029904298 -0.048054814 -0.052206304][0.081396595 0.13758276 0.19938242 0.26123348 0.31491834 0.34854808 0.35411426 0.33332938 0.28560957 0.21402195 0.13032831 0.054037221 -0.0018261109 -0.036397785 -0.048631158][0.13151003 0.20573579 0.289293 0.37584859 0.45339406 0.50528896 0.52027321 0.49925655 0.44134125 0.35031372 0.24171527 0.13949354 0.060965646 0.0090220422 -0.015146302][0.18301697 0.27146694 0.37025353 0.47398531 0.56833392 0.63458014 0.66034794 0.64606988 0.58966249 0.49208385 0.36989975 0.25049812 0.15558597 0.089917846 0.053803638][0.23324597 0.327237 0.42972174 0.53746235 0.636637 0.71067643 0.74923426 0.75141835 0.71132451 0.62313604 0.50080812 0.37464437 0.27074414 0.19596416 0.14932831][0.2734108 0.36419711 0.45804486 0.55425256 0.64253652 0.71315616 0.76075321 0.78326732 0.7704401 0.70803815 0.60296988 0.48475435 0.38167956 0.30330375 0.24857755][0.29872438 0.38073629 0.45657447 0.52801794 0.59052634 0.64466459 0.69139284 0.72910088 0.745327 0.71924782 0.64767641 0.55297405 0.46069285 0.38298783 0.32194096][0.3197237 0.38817722 0.4416135 0.48322865 0.513709 0.54289919 0.57899386 0.62352979 0.66393828 0.67397696 0.63943183 0.5717302 0.49074277 0.41200981 0.3438049][0.35169822 0.40645149 0.43959376 0.45531145 0.45686507 0.4595829 0.47830972 0.52163076 0.57786137 0.61586195 0.61136693 0.56336492 0.48482767 0.39531589 0.312123][0.4092339 0.45129824 0.46911314 0.46726224 0.4485859 0.42826754 0.4242149 0.45491561 0.51063836 0.55905819 0.57047385 0.53373152 0.45339739 0.34882018 0.24629325][0.48054579 0.51506835 0.52455544 0.51524979 0.48691115 0.4493694 0.41968745 0.42648733 0.46444121 0.50394493 0.51533216 0.48290488 0.4038322 0.29141822 0.17664777][0.55266458 0.58297086 0.58834904 0.578808 0.55132574 0.5082432 0.46141997 0.44389135 0.45583102 0.47303405 0.47060513 0.43424165 0.35651687 0.2422885 0.1226707]]...]
INFO - root - 2017-12-11 07:47:00.213064: step 29110, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 46h:34m:44s remains)
INFO - root - 2017-12-11 07:47:05.635517: step 29120, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:41m:32s remains)
INFO - root - 2017-12-11 07:47:11.048472: step 29130, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 44h:54m:50s remains)
INFO - root - 2017-12-11 07:47:16.283391: step 29140, loss = 0.68, batch loss = 0.62 (15.8 examples/sec; 0.507 sec/batch; 42h:43m:53s remains)
INFO - root - 2017-12-11 07:47:21.669013: step 29150, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 48h:00m:02s remains)
INFO - root - 2017-12-11 07:47:27.064047: step 29160, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.539 sec/batch; 45h:22m:51s remains)
INFO - root - 2017-12-11 07:47:32.139221: step 29170, loss = 0.69, batch loss = 0.63 (25.2 examples/sec; 0.318 sec/batch; 26h:45m:08s remains)
INFO - root - 2017-12-11 07:47:37.453995: step 29180, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:30m:00s remains)
INFO - root - 2017-12-11 07:47:42.886141: step 29190, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.518 sec/batch; 43h:36m:23s remains)
INFO - root - 2017-12-11 07:47:48.127373: step 29200, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 44h:56m:07s remains)
2017-12-11 07:47:48.688272: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0408782 0.11329323 0.19879416 0.26560256 0.30561629 0.33687127 0.35519618 0.34887996 0.32304031 0.30532986 0.30381185 0.30837533 0.308982 0.30605128 0.32857418][0.067032821 0.17328522 0.30068991 0.40424868 0.469248 0.52104604 0.55188841 0.5448854 0.50728464 0.4778204 0.46820974 0.46752125 0.46400085 0.4607873 0.49787971][0.082387 0.21715377 0.37745833 0.508838 0.59141189 0.65809262 0.69423068 0.68175274 0.63365054 0.59991044 0.59103894 0.59219581 0.58859706 0.58454639 0.62392372][0.080049932 0.23150893 0.4102478 0.55720252 0.64689374 0.71922153 0.7562055 0.74084073 0.69190276 0.66535187 0.66803116 0.6754452 0.66794586 0.6493023 0.66327494][0.068682715 0.22693984 0.41526043 0.57382643 0.66930592 0.74674839 0.79021806 0.78291017 0.7438786 0.72658408 0.73823768 0.74499881 0.72255278 0.67471844 0.64638257][0.053078447 0.21340318 0.41030964 0.58384037 0.69059348 0.77683693 0.83379787 0.843101 0.81855708 0.80412507 0.81089443 0.8011393 0.7482661 0.65861052 0.58019722][0.032861877 0.19410475 0.40110871 0.59389639 0.716453 0.81525517 0.89215213 0.92225432 0.91175771 0.89259183 0.88580585 0.85224676 0.76172829 0.62634611 0.49836317][0.01426854 0.17513388 0.38715115 0.59178275 0.72452217 0.834006 0.93087268 0.97755837 0.97532266 0.94650233 0.92608064 0.871123 0.74818778 0.57914442 0.42078531][-0.0069555361 0.14215542 0.34185377 0.5375917 0.66611373 0.77844244 0.88842714 0.94402927 0.94352829 0.90492272 0.87437665 0.80730742 0.66968071 0.4941164 0.33873922][-0.038559694 0.085585892 0.25530457 0.423134 0.53360927 0.63511729 0.7416122 0.79534733 0.79157317 0.74653655 0.7122277 0.64724183 0.51706606 0.35952914 0.23034051][-0.089772366 0.00014402771 0.13039836 0.26213142 0.34941995 0.43207332 0.52201766 0.56519866 0.5562802 0.51079911 0.479475 0.42738578 0.31995437 0.19306448 0.096075885][-0.14998433 -0.096753635 -0.0083199348 0.08545617 0.14944434 0.2124842 0.28085372 0.30864191 0.29379624 0.25075835 0.22353286 0.18465899 0.10374758 0.01293039 -0.048313197][-0.19292261 -0.17307045 -0.1251189 -0.070522219 -0.032276385 0.0088212825 0.052718591 0.062795132 0.041739818 0.0032006323 -0.01997555 -0.048249882 -0.10318241 -0.15652762 -0.18074866][-0.20474243 -0.21027172 -0.19569629 -0.17534733 -0.162416 -0.14584871 -0.12800087 -0.13270234 -0.15553623 -0.1849993 -0.20071284 -0.21595149 -0.24377428 -0.2639567 -0.2606056][-0.19634512 -0.21647497 -0.2223234 -0.22270566 -0.22525106 -0.2260652 -0.22719197 -0.23992744 -0.25996512 -0.2796236 -0.28833425 -0.29249 -0.29899141 -0.29868558 -0.28460258]]...]
INFO - root - 2017-12-11 07:47:53.957256: step 29210, loss = 0.67, batch loss = 0.61 (15.8 examples/sec; 0.505 sec/batch; 42h:33m:39s remains)
INFO - root - 2017-12-11 07:47:59.281896: step 29220, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.509 sec/batch; 42h:54m:02s remains)
INFO - root - 2017-12-11 07:48:04.577893: step 29230, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:55m:53s remains)
INFO - root - 2017-12-11 07:48:09.875947: step 29240, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 45h:13m:48s remains)
INFO - root - 2017-12-11 07:48:15.274630: step 29250, loss = 0.66, batch loss = 0.60 (14.8 examples/sec; 0.539 sec/batch; 45h:25m:55s remains)
INFO - root - 2017-12-11 07:48:20.638438: step 29260, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:08m:42s remains)
INFO - root - 2017-12-11 07:48:25.984665: step 29270, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 44h:57m:18s remains)
INFO - root - 2017-12-11 07:48:31.023713: step 29280, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:23m:09s remains)
INFO - root - 2017-12-11 07:48:36.465952: step 29290, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:19m:30s remains)
INFO - root - 2017-12-11 07:48:41.852534: step 29300, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 45h:40m:41s remains)
2017-12-11 07:48:42.405939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064781755 -0.060461473 -0.053506143 -0.051167138 -0.052564707 -0.055257518 -0.059002407 -0.064710543 -0.069992833 -0.07151857 -0.070290133 -0.07103397 -0.074949868 -0.07928431 -0.083010256][-0.034371048 -0.017206857 0.0018256989 0.010979119 0.011309203 0.0087798024 0.003429597 -0.008007857 -0.021556769 -0.028997535 -0.030783491 -0.037005227 -0.049764641 -0.063084394 -0.073820107][0.016844003 0.058773648 0.10354033 0.13172987 0.14181381 0.14638358 0.14411353 0.12526299 0.095603004 0.072047561 0.058267664 0.036775116 0.0048834193 -0.026311046 -0.050572157][0.078854725 0.15864098 0.24475874 0.3065196 0.33627591 0.35526332 0.35975954 0.33065969 0.27620137 0.22507329 0.18931486 0.14338613 0.083293431 0.025406122 -0.019713731][0.13849907 0.26407149 0.40262076 0.50899839 0.56668848 0.6079089 0.62675846 0.59220374 0.51262915 0.42865285 0.36374569 0.28539607 0.18795785 0.093140677 0.018591203][0.1818267 0.35137838 0.54339653 0.69745648 0.78718364 0.85861063 0.90299344 0.87345392 0.77351665 0.655532 0.55675787 0.43798763 0.29403931 0.15594418 0.050974675][0.19738056 0.40004534 0.63635963 0.83092558 0.9497993 1.0546646 1.1288846 1.1074297 0.98797375 0.83684951 0.70483524 0.54572976 0.35876441 0.18608505 0.06141112][0.18416326 0.40641946 0.67037797 0.8877421 1.021825 1.15018 1.2421558 1.2176805 1.0748802 0.89715582 0.74524933 0.56335247 0.35376319 0.16688927 0.038752567][0.14599939 0.37242678 0.64149553 0.85570216 0.98122662 1.109835 1.1973736 1.1580629 0.99607223 0.80823582 0.65770394 0.47881222 0.27398494 0.096658908 -0.018094696][0.088433154 0.30133685 0.551134 0.73613095 0.82918793 0.93124777 0.99302238 0.93407971 0.76621872 0.58838391 0.45796254 0.30476651 0.13064957 -0.013315568 -0.097170077][0.011232506 0.19217926 0.4004302 0.53857768 0.5873639 0.64695865 0.67241251 0.59759033 0.44108388 0.29251644 0.19630286 0.084379777 -0.040219326 -0.13350496 -0.17459707][-0.082880251 0.051663559 0.20442387 0.29304945 0.30611598 0.32888931 0.324937 0.24757406 0.12009074 0.01392888 -0.042518556 -0.10829657 -0.17875618 -0.22121148 -0.2247279][-0.16982141 -0.086101122 0.0094985338 0.057011545 0.051582467 0.053908456 0.037083827 -0.027528577 -0.11667281 -0.18030049 -0.20257287 -0.22864243 -0.25482258 -0.2605052 -0.2416639][-0.22710532 -0.18882389 -0.14044867 -0.12005597 -0.12888218 -0.13215363 -0.14735316 -0.18957227 -0.24067692 -0.27077889 -0.27179232 -0.27187774 -0.26963216 -0.2549513 -0.22785924][-0.24623328 -0.24132411 -0.22520663 -0.21938835 -0.22393355 -0.22437218 -0.23054817 -0.24890339 -0.26859644 -0.27539259 -0.26599044 -0.25366655 -0.23863018 -0.21781589 -0.19272892]]...]
INFO - root - 2017-12-11 07:48:47.690940: step 29310, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:10m:39s remains)
INFO - root - 2017-12-11 07:48:53.057264: step 29320, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:55m:57s remains)
INFO - root - 2017-12-11 07:48:58.455898: step 29330, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:52m:50s remains)
INFO - root - 2017-12-11 07:49:03.865432: step 29340, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 43h:42m:46s remains)
INFO - root - 2017-12-11 07:49:09.148568: step 29350, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:18m:58s remains)
INFO - root - 2017-12-11 07:49:14.431195: step 29360, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:35m:21s remains)
INFO - root - 2017-12-11 07:49:19.906113: step 29370, loss = 0.71, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 46h:54m:23s remains)
INFO - root - 2017-12-11 07:49:25.026798: step 29380, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:20m:05s remains)
INFO - root - 2017-12-11 07:49:30.425971: step 29390, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.552 sec/batch; 46h:29m:11s remains)
INFO - root - 2017-12-11 07:49:35.761750: step 29400, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 46h:18m:21s remains)
2017-12-11 07:49:36.466528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0038507844 -0.0090751443 -0.0079911286 0.004567361 0.021499364 0.034712832 0.042534266 0.0458773 0.044337381 0.035405062 0.018441606 -0.0033616135 -0.026837308 -0.048326854 -0.064035349][0.058321577 0.048520252 0.047118541 0.069482058 0.10239476 0.13062985 0.14934652 0.15744829 0.15180381 0.12906028 0.090941638 0.04709639 0.0047311708 -0.032129481 -0.059156336][0.14471188 0.12948374 0.12591295 0.1618458 0.21659769 0.26540962 0.29764745 0.30966905 0.29582843 0.25342312 0.18737525 0.11509588 0.049511775 -0.0054457095 -0.045511432][0.23567076 0.21503514 0.21424565 0.26798356 0.34683341 0.41504183 0.45606259 0.46613351 0.43980935 0.37702945 0.28478476 0.1848018 0.095059209 0.021119904 -0.032412659][0.30446786 0.28282359 0.29272121 0.36670932 0.4663541 0.54584473 0.58540457 0.586027 0.54669338 0.46945852 0.35957569 0.23851949 0.1275724 0.036940034 -0.027245851][0.33218616 0.31843203 0.34656727 0.43947431 0.55178809 0.6320219 0.66138405 0.6481424 0.59778422 0.51313758 0.39434442 0.26005447 0.13406138 0.033010982 -0.035008561][0.32518348 0.32779196 0.37583119 0.48213559 0.5988335 0.67476249 0.69373786 0.66762072 0.60692382 0.51639664 0.39231938 0.25118598 0.11843248 0.014928162 -0.050219193][0.30422243 0.32488126 0.38773268 0.5000264 0.61559719 0.68678963 0.69948125 0.66337979 0.58999544 0.48969412 0.36067048 0.2190164 0.089466885 -0.008144333 -0.065499924][0.27472386 0.3088755 0.38122439 0.49381649 0.60411114 0.66879559 0.6757372 0.63095 0.5435698 0.43156868 0.30000535 0.16552189 0.049851429 -0.033189844 -0.07830397][0.22459944 0.26665181 0.34391382 0.45138225 0.55077338 0.60419011 0.60190028 0.5494377 0.45349947 0.3372727 0.21204193 0.094280936 0.0012341691 -0.060688794 -0.090130284][0.15123326 0.19383009 0.27127007 0.36939263 0.45239061 0.48905754 0.47368068 0.4152762 0.32038021 0.21304655 0.10631616 0.014438698 -0.051084895 -0.089211866 -0.10188109][0.065086335 0.10166774 0.17299448 0.2569342 0.31990543 0.33849394 0.31149703 0.25202221 0.16814241 0.0811942 0.0018226663 -0.059536483 -0.097087033 -0.1128312 -0.11076388][-0.028179741 -0.0034305975 0.052581266 0.1139705 0.15357628 0.1568348 0.12622856 0.075899616 0.013785647 -0.044083778 -0.091319554 -0.12175788 -0.13353068 -0.13000058 -0.11631478][-0.11357599 -0.10297966 -0.068882525 -0.035043854 -0.018051416 -0.023712721 -0.047765546 -0.079022862 -0.11237175 -0.13845496 -0.15507475 -0.15981866 -0.15267287 -0.13663787 -0.11674694][-0.16819771 -0.16923659 -0.1562455 -0.14610346 -0.14506644 -0.1527696 -0.1652512 -0.17604655 -0.1836897 -0.1847335 -0.17994745 -0.16902676 -0.15241809 -0.1319886 -0.11159997]]...]
INFO - root - 2017-12-11 07:49:41.799062: step 29410, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 43h:30m:24s remains)
INFO - root - 2017-12-11 07:49:47.102764: step 29420, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 45h:54m:51s remains)
INFO - root - 2017-12-11 07:49:52.459076: step 29430, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:22m:49s remains)
INFO - root - 2017-12-11 07:49:57.758664: step 29440, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 44h:17m:32s remains)
INFO - root - 2017-12-11 07:50:03.106173: step 29450, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 48h:15m:35s remains)
INFO - root - 2017-12-11 07:50:08.526444: step 29460, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 44h:29m:07s remains)
INFO - root - 2017-12-11 07:50:13.878422: step 29470, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:36m:10s remains)
INFO - root - 2017-12-11 07:50:18.923053: step 29480, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:17m:38s remains)
INFO - root - 2017-12-11 07:50:24.188833: step 29490, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:22m:28s remains)
INFO - root - 2017-12-11 07:50:29.498326: step 29500, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:28m:36s remains)
2017-12-11 07:50:30.070800: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38045159 0.44035763 0.48108786 0.49708068 0.49530575 0.49199995 0.4848367 0.4637922 0.42969498 0.39935118 0.39160079 0.42390221 0.48555362 0.55718714 0.61012316][0.52000624 0.6048699 0.6601187 0.67570317 0.6615364 0.64110613 0.61842483 0.58537346 0.54064894 0.50269216 0.49260291 0.52748543 0.59185064 0.66426319 0.71405047][0.64186573 0.74559063 0.80932003 0.81932974 0.78952712 0.75075132 0.71388 0.67146647 0.61697239 0.57043946 0.55410403 0.58274931 0.63740534 0.69760555 0.73524833][0.71355116 0.824454 0.88852352 0.89179164 0.85162467 0.80235219 0.75879669 0.71302009 0.65245748 0.59924567 0.57578808 0.59374297 0.6320309 0.67320347 0.69506681][0.72243673 0.83047795 0.89020479 0.89053887 0.84953761 0.79913771 0.75482231 0.70901418 0.64562035 0.58832133 0.55976969 0.56847793 0.59193361 0.61502725 0.62104982][0.6983062 0.80313593 0.86208963 0.86761945 0.83438504 0.78805524 0.74451971 0.69838238 0.63394791 0.57524228 0.5452137 0.54908508 0.5627718 0.57171255 0.56185734][0.65270084 0.76046467 0.82640481 0.84598029 0.8262884 0.786324 0.74413681 0.69816029 0.63627225 0.58209759 0.55799818 0.5649175 0.57533318 0.57394522 0.54774261][0.57799393 0.688179 0.76135474 0.79600614 0.79173964 0.76105791 0.72411114 0.68283945 0.63046926 0.58858395 0.57812554 0.5956049 0.60834259 0.6003527 0.55969238][0.47717726 0.58167952 0.6538462 0.69624108 0.70380354 0.68294448 0.65488875 0.62317944 0.58593106 0.55974633 0.56353432 0.5913716 0.6075263 0.59660262 0.54806626][0.36049965 0.4492695 0.51063538 0.550694 0.56254262 0.54782289 0.52828753 0.50703931 0.48452255 0.47090229 0.48230925 0.51253617 0.52797437 0.51709753 0.47115052][0.22021507 0.28288257 0.32535979 0.35560915 0.36646813 0.35635886 0.34554982 0.33442476 0.32324135 0.31647375 0.3278254 0.35265553 0.36375228 0.35567161 0.32163242][0.048903506 0.076658435 0.093534306 0.10759673 0.11332827 0.10742599 0.10543909 0.10419562 0.10171648 0.098382227 0.10657772 0.12476529 0.13412915 0.13370407 0.1179791][-0.10457402 -0.10924066 -0.11647554 -0.12013499 -0.12248114 -0.12691988 -0.1238617 -0.11911539 -0.11739039 -0.12040041 -0.1166301 -0.10406709 -0.093485676 -0.084624261 -0.08209531][-0.19899173 -0.22208463 -0.24269541 -0.2581307 -0.26787123 -0.27394161 -0.27130938 -0.26639229 -0.26513129 -0.26916906 -0.26870185 -0.26010036 -0.24846418 -0.23388371 -0.21957204][-0.23667075 -0.26405653 -0.28687271 -0.30548194 -0.31784987 -0.32467255 -0.32390907 -0.32099167 -0.32110044 -0.32504267 -0.32587618 -0.32055026 -0.31107029 -0.29728061 -0.28052637]]...]
INFO - root - 2017-12-11 07:50:35.462264: step 29510, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:10m:28s remains)
INFO - root - 2017-12-11 07:50:40.800109: step 29520, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:12m:17s remains)
INFO - root - 2017-12-11 07:50:46.176274: step 29530, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 43h:47m:24s remains)
INFO - root - 2017-12-11 07:50:51.439288: step 29540, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 44h:38m:07s remains)
INFO - root - 2017-12-11 07:50:56.798606: step 29550, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:22m:43s remains)
INFO - root - 2017-12-11 07:51:02.181184: step 29560, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 44h:44m:20s remains)
INFO - root - 2017-12-11 07:51:07.650201: step 29570, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:32m:38s remains)
INFO - root - 2017-12-11 07:51:12.980755: step 29580, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.457 sec/batch; 38h:26m:01s remains)
INFO - root - 2017-12-11 07:51:18.187715: step 29590, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 44h:19m:29s remains)
INFO - root - 2017-12-11 07:51:23.520332: step 29600, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:49m:05s remains)
2017-12-11 07:51:24.026323: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.44327998 0.39921582 0.35462472 0.32650983 0.31492662 0.30977586 0.3006258 0.28732094 0.27969804 0.29463297 0.3378962 0.3975969 0.45407608 0.48922226 0.49215379][0.40467718 0.36222336 0.32542452 0.31076378 0.31575391 0.32639113 0.33008486 0.32581291 0.32359588 0.33979574 0.38074252 0.43576443 0.48684663 0.51666558 0.51564592][0.31751361 0.28558111 0.26781684 0.27854079 0.31245086 0.34972358 0.37352017 0.38026375 0.37953305 0.38442361 0.40161461 0.4262591 0.44977206 0.46028113 0.44978234][0.24092564 0.22358714 0.23018943 0.27511561 0.34921059 0.42447224 0.47515985 0.49278116 0.48658016 0.46589512 0.43838271 0.40842858 0.3828077 0.35973409 0.33398157][0.21512711 0.21383193 0.24736455 0.3298912 0.44700167 0.56227481 0.63860136 0.6617322 0.64093709 0.58438647 0.50303954 0.41040295 0.32794431 0.26408485 0.21759567][0.23869617 0.25761598 0.322708 0.44662103 0.60797662 0.76189953 0.85927433 0.87975258 0.83431256 0.73590052 0.60251796 0.45416549 0.32166144 0.22056191 0.1524283][0.26176941 0.30429292 0.40494904 0.57129931 0.77384132 0.96237946 1.0779358 1.0925901 1.0187643 0.88008916 0.70566994 0.51965404 0.35625532 0.23423137 0.15475377][0.22539453 0.2886976 0.420089 0.61627591 0.84129596 1.0462689 1.1703752 1.1795257 1.0855092 0.9236024 0.73488337 0.54531068 0.38640162 0.2745308 0.2061159][0.11288256 0.18570016 0.33000249 0.52873391 0.74327707 0.93231058 1.0436319 1.0435169 0.94371176 0.78775978 0.62324005 0.47368145 0.36214623 0.29741395 0.26721007][-0.03675941 0.030176263 0.16700608 0.34522367 0.52712029 0.68108934 0.76789707 0.75739771 0.66173619 0.5293597 0.41041639 0.32442161 0.28389645 0.28891149 0.31602633][-0.16058864 -0.1122244 -0.0021889955 0.14019151 0.28272119 0.40250042 0.47095743 0.45887282 0.37571794 0.27019036 0.19394836 0.16573058 0.19045755 0.26151577 0.34606466][-0.19311289 -0.17024909 -0.098574556 -0.0008492737 0.099883378 0.18729679 0.24060079 0.23108009 0.16361162 0.0815927 0.035032161 0.04437894 0.11116993 0.22556233 0.35057947][-0.12924859 -0.12561186 -0.088178031 -0.030699831 0.032163143 0.088662028 0.12413977 0.1125341 0.055734061 -0.011134773 -0.043055438 -0.018758297 0.06229515 0.19069627 0.33227053][-0.023734346 -0.027233506 -0.0084460452 0.026964335 0.069152847 0.10787028 0.13180836 0.11709533 0.064166732 0.0010797845 -0.03140479 -0.0114849 0.062136889 0.18210429 0.32066578][0.060707949 0.0566617 0.067850031 0.094753 0.13131745 0.16754733 0.19243158 0.18313645 0.13882715 0.079911046 0.042182904 0.049849391 0.10707773 0.20901366 0.33381733]]...]
INFO - root - 2017-12-11 07:51:29.408695: step 29610, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 48h:39m:38s remains)
INFO - root - 2017-12-11 07:51:34.750029: step 29620, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:34m:22s remains)
INFO - root - 2017-12-11 07:51:40.102601: step 29630, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:54m:07s remains)
INFO - root - 2017-12-11 07:51:45.450057: step 29640, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 43h:39m:25s remains)
INFO - root - 2017-12-11 07:51:50.727274: step 29650, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:12m:09s remains)
INFO - root - 2017-12-11 07:51:56.116163: step 29660, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 44h:57m:48s remains)
INFO - root - 2017-12-11 07:52:01.448041: step 29670, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 43h:40m:52s remains)
INFO - root - 2017-12-11 07:52:06.882357: step 29680, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 46h:38m:34s remains)
INFO - root - 2017-12-11 07:52:11.871753: step 29690, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 43h:45m:58s remains)
INFO - root - 2017-12-11 07:52:17.240868: step 29700, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:19m:35s remains)
2017-12-11 07:52:17.867292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050985247 -0.052417725 -0.047978327 -0.034642182 -0.0081132278 0.032269403 0.079369515 0.12132575 0.15598959 0.1923347 0.23710255 0.28903046 0.33014759 0.34355062 0.32809275][-0.055913087 -0.056952931 -0.051218957 -0.03600049 -0.0051966477 0.043676157 0.10360762 0.15895776 0.2025774 0.24447811 0.29291406 0.34852737 0.38824636 0.39101449 0.35840333][-0.056288805 -0.055847235 -0.048870943 -0.033224694 -0.0011051331 0.051346581 0.11847582 0.18190514 0.22850917 0.26728657 0.30804148 0.35565823 0.38766739 0.38102341 0.33692306][-0.056585852 -0.05444986 -0.04552339 -0.028302362 0.0056123431 0.060396824 0.13182868 0.20037904 0.24736363 0.27787751 0.30245435 0.332311 0.35130867 0.33821583 0.29094389][-0.05677871 -0.053869076 -0.043789003 -0.025042497 0.010489716 0.06679856 0.1404091 0.21137549 0.25659463 0.276136 0.28164154 0.29125741 0.29697475 0.28095877 0.23712333][-0.056288075 -0.054518707 -0.045113802 -0.026608637 0.0086216815 0.064510882 0.13726284 0.20660493 0.24653007 0.25325155 0.24132551 0.2356284 0.23558858 0.22515276 0.19345328][-0.055419356 -0.055493761 -0.047899228 -0.031126054 0.001837845 0.055196851 0.12411971 0.18782087 0.21889096 0.2115538 0.1844669 0.16841672 0.17028052 0.174366 0.16431507][-0.054807056 -0.056057304 -0.050192781 -0.035193354 -0.0043197959 0.046893634 0.1125456 0.17128217 0.19479606 0.17556383 0.13470106 0.10781451 0.11029018 0.12887761 0.14382398][-0.054186955 -0.056573756 -0.052110109 -0.038525309 -0.0091053564 0.040432647 0.10382474 0.16017485 0.18124638 0.15735514 0.10765617 0.069204658 0.065455586 0.089537106 0.12245607][-0.054000422 -0.057541087 -0.0543625 -0.041951057 -0.014119397 0.032689273 0.0925571 0.14642173 0.16802658 0.14678623 0.096855737 0.051616352 0.039247263 0.060898561 0.1014104][-0.054038577 -0.059034031 -0.057064861 -0.045793388 -0.019438351 0.025088 0.082041875 0.1336149 0.1557229 0.13903053 0.093941323 0.048164882 0.030702727 0.048794307 0.090935595][-0.054961067 -0.06069015 -0.059302647 -0.048769318 -0.023145031 0.021178834 0.07862372 0.13089336 0.15420011 0.14096633 0.099979423 0.055009078 0.034213848 0.048920237 0.090780295][-0.056277193 -0.062220994 -0.060437884 -0.050090525 -0.025151975 0.019385807 0.078643948 0.13348556 0.15891007 0.14889848 0.11158224 0.067820482 0.043579366 0.052697808 0.091583684][-0.057469945 -0.063250706 -0.060606364 -0.050523762 -0.02769134 0.013913373 0.070754856 0.12470885 0.15090957 0.145136 0.11432024 0.076089069 0.052117158 0.05625407 0.08968816][-0.057924677 -0.063429765 -0.060240097 -0.050882339 -0.031321872 0.0039102966 0.052565653 0.099429309 0.12248194 0.11989233 0.097225077 0.0692505 0.052244037 0.057517737 0.087494649]]...]
INFO - root - 2017-12-11 07:52:23.213060: step 29710, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 43h:59m:10s remains)
INFO - root - 2017-12-11 07:52:28.503780: step 29720, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 44h:29m:54s remains)
INFO - root - 2017-12-11 07:52:33.866918: step 29730, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:36m:17s remains)
INFO - root - 2017-12-11 07:52:39.187823: step 29740, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:52m:15s remains)
INFO - root - 2017-12-11 07:52:44.556900: step 29750, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:42m:09s remains)
INFO - root - 2017-12-11 07:52:49.917768: step 29760, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 47h:14m:42s remains)
INFO - root - 2017-12-11 07:52:55.373533: step 29770, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 47h:59m:46s remains)
INFO - root - 2017-12-11 07:53:00.687914: step 29780, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 43h:17m:01s remains)
INFO - root - 2017-12-11 07:53:05.734498: step 29790, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 44h:11m:32s remains)
INFO - root - 2017-12-11 07:53:11.024929: step 29800, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.508 sec/batch; 42h:41m:12s remains)
2017-12-11 07:53:11.583137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.046448797 -0.053587098 -0.053300411 -0.046401184 -0.03455263 -0.019194495 -0.0037583204 0.0088195847 0.01616827 0.017049346 0.013085779 0.0072131939 0.0019995242 -0.0028147355 -0.0076856865][-0.046018891 -0.058173519 -0.060138579 -0.054653727 -0.044602491 -0.031351708 -0.016700745 -0.0019352246 0.010986178 0.019769315 0.023800662 0.023642087 0.020189634 0.01305868 0.0031093203][-0.037000086 -0.056116253 -0.061297748 -0.056669664 -0.046230227 -0.031538174 -0.013011134 0.0093371226 0.032999795 0.053622216 0.067016907 0.07041496 0.063046612 0.045865867 0.023322256][-0.02190249 -0.047043618 -0.053228945 -0.044832155 -0.026897756 -0.0023886964 0.027666677 0.0628153 0.098912492 0.12894386 0.14565499 0.14413284 0.12349564 0.087796509 0.046429589][-0.0036611864 -0.03215275 -0.034472659 -0.014602086 0.021725839 0.068871081 0.12199906 0.17671683 0.22509064 0.25631404 0.26170653 0.2382959 0.19001505 0.12701082 0.063781008][0.011183793 -0.015415551 -0.0075306017 0.03242803 0.099134132 0.18336603 0.27258432 0.35337809 0.41071206 0.42899615 0.40189445 0.33551833 0.24564148 0.15111625 0.069128633][0.021700127 0.0050021973 0.030377679 0.097336091 0.20088497 0.32714534 0.45322219 0.55387884 0.60668945 0.59407395 0.51774293 0.39832759 0.26598048 0.14702733 0.057150494][0.029592378 0.026782731 0.072855681 0.16650294 0.30114013 0.45721704 0.60227454 0.70221454 0.73247856 0.67954147 0.55620635 0.39584768 0.2381628 0.11241071 0.02951159][0.025686128 0.035641961 0.097921707 0.20766625 0.35479468 0.51478821 0.65110785 0.728829 0.72844976 0.64481282 0.49906191 0.32876062 0.17378005 0.061025441 -0.0035140687][0.0028341524 0.023356957 0.091894694 0.19972719 0.333887 0.4692184 0.57344288 0.61908239 0.59560561 0.5054189 0.37105072 0.22456488 0.097806908 0.011930916 -0.029866312][-0.023710512 0.0058683092 0.071867384 0.16150935 0.26139307 0.35153988 0.4114489 0.42651749 0.39448148 0.32048351 0.22151303 0.11843124 0.031747047 -0.023756182 -0.045323182][-0.037567977 -0.0014391728 0.056213837 0.11927461 0.17656031 0.21722087 0.23504204 0.2283228 0.1993053 0.15107782 0.092235312 0.032742854 -0.016722877 -0.046867084 -0.054311369][-0.041377965 -0.0032768862 0.042324238 0.078403585 0.098334663 0.10096445 0.090904891 0.074241959 0.053783342 0.029004533 0.0017229823 -0.025310937 -0.047510713 -0.059239071 -0.056516122][-0.037633978 0.00043111946 0.035319597 0.051549971 0.04746487 0.028889714 0.0058215763 -0.012535281 -0.023851957 -0.031534027 -0.038139313 -0.044833306 -0.049913749 -0.049396969 -0.038589228][-0.014311477 0.024367336 0.053976677 0.061310947 0.047477145 0.020845506 -0.0070160711 -0.024892366 -0.029020865 -0.024316881 -0.016961843 -0.01139498 -0.0075929542 -0.0012360669 0.014556034]]...]
INFO - root - 2017-12-11 07:53:16.973583: step 29810, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 45h:56m:41s remains)
INFO - root - 2017-12-11 07:53:22.342493: step 29820, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:30m:41s remains)
INFO - root - 2017-12-11 07:53:27.660142: step 29830, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 44h:23m:05s remains)
INFO - root - 2017-12-11 07:53:33.005917: step 29840, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:07m:31s remains)
INFO - root - 2017-12-11 07:53:38.288588: step 29850, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.507 sec/batch; 42h:35m:23s remains)
INFO - root - 2017-12-11 07:53:43.689597: step 29860, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:45m:10s remains)
INFO - root - 2017-12-11 07:53:49.033712: step 29870, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:30m:04s remains)
INFO - root - 2017-12-11 07:53:54.309469: step 29880, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.510 sec/batch; 42h:52m:18s remains)
INFO - root - 2017-12-11 07:53:59.425098: step 29890, loss = 0.71, batch loss = 0.65 (20.7 examples/sec; 0.386 sec/batch; 32h:24m:57s remains)
INFO - root - 2017-12-11 07:54:04.794069: step 29900, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.524 sec/batch; 44h:00m:57s remains)
2017-12-11 07:54:05.324291: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29586467 0.31167537 0.30966124 0.30308926 0.28463614 0.25922182 0.23911336 0.24403016 0.27932674 0.32966581 0.37745905 0.40860596 0.4230912 0.42787883 0.42235792][0.35378906 0.35574278 0.33205947 0.30826509 0.27592382 0.24077497 0.21436064 0.22121315 0.26759571 0.3371433 0.41019529 0.4649086 0.49259138 0.49810085 0.48640555][0.4380824 0.43025783 0.3849737 0.34060535 0.2907356 0.24239776 0.20611951 0.20433408 0.24600053 0.31736609 0.40062624 0.46929327 0.50679344 0.5140335 0.50001627][0.52997327 0.52276915 0.46740502 0.41058621 0.35187742 0.29719651 0.25125021 0.23150752 0.25260088 0.30611512 0.37800533 0.44203669 0.47904009 0.48691958 0.47461691][0.59636718 0.60795516 0.56112039 0.50590348 0.44999611 0.39684427 0.34517708 0.30843189 0.30650589 0.33337483 0.37581447 0.41265571 0.42973304 0.42632183 0.410905][0.60963285 0.65781391 0.63887137 0.60084254 0.55896038 0.515968 0.46745723 0.42308557 0.40393516 0.40193233 0.40268821 0.39299747 0.36929953 0.33759665 0.30866629][0.55120009 0.64127785 0.66064757 0.64983124 0.62875205 0.60274321 0.56674755 0.52508926 0.49460831 0.46429056 0.4198029 0.35659561 0.28297797 0.21515168 0.16746233][0.41033888 0.52709568 0.58086455 0.59958839 0.601531 0.59461343 0.57453585 0.5413053 0.50516391 0.45594314 0.38060886 0.2788755 0.16713271 0.070888214 0.00800245][0.21807648 0.33203205 0.40111408 0.4385809 0.45620355 0.462683 0.45594004 0.43411434 0.40177044 0.35278067 0.27521491 0.1657476 0.04225748 -0.063370384 -0.13060719][0.026392182 0.11152253 0.17520842 0.21582073 0.23691505 0.24728096 0.24863422 0.23793256 0.21541074 0.18017463 0.12231928 0.0318787 -0.07645528 -0.16832101 -0.22236317][-0.12272902 -0.081747793 -0.040486392 -0.011324467 0.0024965918 0.010101344 0.016280441 0.015357358 0.0037103731 -0.01450136 -0.045200076 -0.10303771 -0.1775756 -0.2371244 -0.26282281][-0.19650523 -0.19794144 -0.18658866 -0.17761606 -0.17672452 -0.17534907 -0.16732179 -0.16141531 -0.1634451 -0.16583903 -0.17083943 -0.19373338 -0.22824012 -0.25036287 -0.24742539][-0.19262299 -0.22179362 -0.23507619 -0.2434828 -0.252809 -0.25662023 -0.24860252 -0.23830628 -0.23191364 -0.22133519 -0.20718436 -0.20334671 -0.20703374 -0.2023114 -0.1827497][-0.14048731 -0.17966583 -0.20592155 -0.22218896 -0.2338029 -0.23804703 -0.23063426 -0.21881627 -0.20761637 -0.19076319 -0.1687617 -0.15300913 -0.14281917 -0.12741759 -0.10409198][-0.090589233 -0.12552148 -0.1518257 -0.16698444 -0.17514746 -0.17672217 -0.17057075 -0.16087925 -0.15042379 -0.13496371 -0.11533517 -0.099804007 -0.088693947 -0.074461527 -0.055879969]]...]
INFO - root - 2017-12-11 07:54:10.588986: step 29910, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.539 sec/batch; 45h:16m:16s remains)
INFO - root - 2017-12-11 07:54:16.021245: step 29920, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 44h:02m:41s remains)
INFO - root - 2017-12-11 07:54:21.400671: step 29930, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:54m:41s remains)
INFO - root - 2017-12-11 07:54:26.710005: step 29940, loss = 0.71, batch loss = 0.65 (15.6 examples/sec; 0.512 sec/batch; 43h:01m:14s remains)
INFO - root - 2017-12-11 07:54:32.067533: step 29950, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 44h:28m:42s remains)
INFO - root - 2017-12-11 07:54:37.384444: step 29960, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:14m:28s remains)
INFO - root - 2017-12-11 07:54:42.773944: step 29970, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:49m:18s remains)
INFO - root - 2017-12-11 07:54:48.069063: step 29980, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:02m:49s remains)
INFO - root - 2017-12-11 07:54:53.460044: step 29990, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:03m:44s remains)
INFO - root - 2017-12-11 07:54:58.575033: step 30000, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:19m:04s remains)
2017-12-11 07:54:59.133127: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24080065 0.24234556 0.22550218 0.19494866 0.16012003 0.12133686 0.085587196 0.061147757 0.041449666 0.018177476 -0.018884519 -0.067719057 -0.10523282 -0.11747328 -0.097259372][0.2779142 0.26505589 0.22947523 0.18095575 0.12846155 0.071921892 0.015418322 -0.033357788 -0.073665716 -0.10697325 -0.14013331 -0.17182896 -0.18627517 -0.17988753 -0.15125687][0.30828306 0.29096779 0.25002363 0.19898516 0.14635886 0.0881207 0.023061503 -0.043380946 -0.10332622 -0.14974263 -0.1840608 -0.20375751 -0.2000562 -0.17847304 -0.14293571][0.31108564 0.30060634 0.2722168 0.23965065 0.20989424 0.17323069 0.12217652 0.056486018 -0.012695359 -0.072587729 -0.1179359 -0.141607 -0.13805166 -0.11599208 -0.081863][0.26312989 0.26511481 0.26279518 0.26655284 0.28050151 0.28897128 0.27770254 0.23777126 0.1772463 0.10961158 0.045860194 0.00096386723 -0.017280778 -0.013992505 0.0066452716][0.16684341 0.1810021 0.21000993 0.25885522 0.32789046 0.39559889 0.43951991 0.44083083 0.40096125 0.33118117 0.24787135 0.17417957 0.12318605 0.097466715 0.09903799][0.056348041 0.079587445 0.13760914 0.23062159 0.35309729 0.477796 0.5734309 0.61285806 0.59150827 0.51961178 0.42064932 0.32480219 0.24934612 0.20230728 0.1912045][-0.022954591 0.0088107837 0.092249937 0.22374518 0.38838682 0.55244011 0.67669588 0.73020035 0.70902896 0.62936074 0.52443689 0.4257336 0.34828046 0.29861438 0.28692073][-0.034492478 0.00656485 0.10759648 0.26269323 0.44695652 0.62151724 0.7421602 0.779121 0.73708922 0.64579368 0.55049568 0.47511384 0.42304936 0.39038816 0.38618881][0.01599469 0.060164433 0.16226463 0.31693491 0.49225262 0.64808744 0.73933953 0.74212539 0.67277771 0.57632673 0.50704062 0.47473928 0.46643326 0.46279404 0.46787795][0.08667428 0.1207462 0.20290238 0.33058333 0.46975037 0.58361804 0.63292038 0.60187769 0.51614118 0.42991275 0.39764655 0.41344693 0.44976869 0.47463304 0.48713443][0.13171005 0.14024104 0.18268238 0.26149574 0.34700733 0.41043195 0.42291945 0.37582329 0.2971161 0.23852558 0.24527413 0.29821339 0.3628673 0.40437958 0.42082179][0.13766673 0.11050874 0.10360087 0.12507504 0.15434253 0.17367445 0.16543128 0.12453977 0.07242424 0.048501547 0.083282545 0.15132222 0.22209606 0.26946419 0.29349685][0.13049458 0.068217337 0.017847231 -0.0071873553 -0.019292157 -0.02708398 -0.039543346 -0.06147493 -0.083585054 -0.082854219 -0.04128078 0.019520372 0.081924818 0.133963 0.17576353][0.15915947 0.072468385 -0.0032983553 -0.050877512 -0.079424135 -0.093755022 -0.098507553 -0.10032111 -0.10272915 -0.097273722 -0.07125327 -0.033817537 0.013552987 0.070638806 0.13382846]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 07:55:05.333650: step 30010, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.539 sec/batch; 45h:17m:08s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 07:55:10.657971: step 30020, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.535 sec/batch; 44h:57m:46s remains)
INFO - root - 2017-12-11 07:55:16.007329: step 30030, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:26m:14s remains)
INFO - root - 2017-12-11 07:55:21.313591: step 30040, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:26m:44s remains)
INFO - root - 2017-12-11 07:55:26.744703: step 30050, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:43m:04s remains)
INFO - root - 2017-12-11 07:55:32.103100: step 30060, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.542 sec/batch; 45h:31m:37s remains)
INFO - root - 2017-12-11 07:55:37.427530: step 30070, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 44h:36m:29s remains)
INFO - root - 2017-12-11 07:55:42.794106: step 30080, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:41m:00s remains)
INFO - root - 2017-12-11 07:55:48.231924: step 30090, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 44h:12m:10s remains)
INFO - root - 2017-12-11 07:55:53.370479: step 30100, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 46h:16m:35s remains)
2017-12-11 07:55:53.871110: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17849135 0.25575325 0.30646706 0.31778768 0.29504633 0.2391011 0.1626832 0.097143918 0.064596333 0.068826817 0.10168047 0.15157728 0.20423011 0.24517818 0.26522261][0.22075123 0.30976167 0.36203146 0.36549753 0.33066964 0.25939742 0.16833408 0.088707 0.045358431 0.042216171 0.070587508 0.12122382 0.18290354 0.24105412 0.27997065][0.25392565 0.35421768 0.41093141 0.41341025 0.37573096 0.30041754 0.20430097 0.11641092 0.061337568 0.042915307 0.053449191 0.089677766 0.14914051 0.22053464 0.28138682][0.27419093 0.3924976 0.46711248 0.48705202 0.46599671 0.40209872 0.31001496 0.2164232 0.14636879 0.10541834 0.087608188 0.097959429 0.1439942 0.22080943 0.30209193][0.27724895 0.41097793 0.50637937 0.55123079 0.5572899 0.5171724 0.441772 0.35382044 0.27667513 0.2187527 0.17636044 0.16086435 0.18711427 0.25894159 0.3519319][0.26337689 0.40191174 0.50867325 0.57147306 0.60081756 0.58697414 0.5379293 0.47088704 0.40265888 0.34077236 0.28445166 0.24912502 0.25508019 0.31751877 0.41865438][0.22572979 0.35681385 0.46203706 0.53140289 0.57520854 0.58432639 0.56406832 0.52601761 0.47836488 0.42422798 0.36409718 0.31449282 0.30142108 0.35415259 0.46401337][0.16437951 0.27806094 0.37311891 0.44223467 0.49524176 0.52395415 0.53064072 0.52035779 0.4937897 0.44979188 0.38930693 0.32859445 0.29796624 0.33968166 0.45277828][0.1056341 0.19845901 0.28042695 0.34604794 0.40321705 0.44423202 0.46878102 0.47595206 0.46193334 0.42260996 0.3596738 0.28917944 0.24319492 0.27092052 0.37653834][0.075677052 0.15528665 0.23081382 0.29473764 0.35047719 0.39000133 0.41205108 0.41387373 0.39281932 0.3456991 0.27695802 0.20144604 0.14893375 0.1676856 0.2629014][0.076586336 0.15834816 0.23911427 0.30698824 0.35883433 0.38430923 0.38322788 0.35618386 0.30778158 0.23989829 0.16261776 0.089270286 0.042998772 0.062914252 0.15010452][0.093288973 0.18539457 0.27678311 0.35031068 0.39808628 0.4072746 0.37957698 0.3192721 0.23926793 0.14954484 0.067014791 0.0032480548 -0.029351732 -0.0070646368 0.064344279][0.11939713 0.22325334 0.32450816 0.40161777 0.4450489 0.44269457 0.39791906 0.31543043 0.21420924 0.11211304 0.031972673 -0.016854234 -0.034915995 -0.015649773 0.028734712][0.13816684 0.24907051 0.35496739 0.43191436 0.47101307 0.46288991 0.41210127 0.32214844 0.21590574 0.11662766 0.050096225 0.021655977 0.018502878 0.033009805 0.049261667][0.1333288 0.24230084 0.34499592 0.4169232 0.45083967 0.43971252 0.38867804 0.30203211 0.20631713 0.12641251 0.085884422 0.083753079 0.098519161 0.11384521 0.11233533]]...]
INFO - root - 2017-12-11 07:55:59.158682: step 30110, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:38m:59s remains)
INFO - root - 2017-12-11 07:56:04.534439: step 30120, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 44h:33m:52s remains)
INFO - root - 2017-12-11 07:56:09.937021: step 30130, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:30m:32s remains)
INFO - root - 2017-12-11 07:56:15.264017: step 30140, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.519 sec/batch; 43h:33m:47s remains)
INFO - root - 2017-12-11 07:56:20.538436: step 30150, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 43h:47m:23s remains)
INFO - root - 2017-12-11 07:56:25.888675: step 30160, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 44h:57m:39s remains)
INFO - root - 2017-12-11 07:56:31.221495: step 30170, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:34m:39s remains)
INFO - root - 2017-12-11 07:56:36.527344: step 30180, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 47h:01m:31s remains)
INFO - root - 2017-12-11 07:56:41.843142: step 30190, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:43m:56s remains)
INFO - root - 2017-12-11 07:56:46.946400: step 30200, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:05m:55s remains)
2017-12-11 07:56:47.518544: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0994392 0.21070944 0.31636915 0.38524592 0.40730163 0.40165681 0.3977803 0.41722247 0.47089693 0.54248959 0.5995093 0.61392188 0.58520716 0.53695756 0.48406532][0.099230379 0.21843214 0.33454788 0.41144329 0.43504953 0.42117983 0.39972296 0.39770558 0.43213698 0.4914639 0.54870391 0.57596534 0.56492531 0.53066826 0.48304445][0.080265388 0.19772585 0.31690559 0.40009356 0.42919311 0.41352546 0.37909222 0.355654 0.36570272 0.40654257 0.45848209 0.49496126 0.49917197 0.47601777 0.433946][0.0543481 0.16624263 0.2861959 0.37737104 0.41826448 0.41071835 0.37376425 0.33538187 0.32228553 0.34259439 0.38519713 0.42511809 0.43900263 0.42359707 0.38579798][0.028219255 0.13347334 0.25436288 0.35672596 0.41659981 0.42835939 0.40085328 0.35618991 0.32294556 0.31989461 0.34599552 0.38088089 0.39858621 0.38972527 0.35752082][0.0058573228 0.1068974 0.23348065 0.355317 0.44546786 0.48893067 0.48100856 0.43606681 0.38402832 0.35441115 0.35778126 0.38189825 0.40085274 0.39928749 0.37348402][-0.012118699 0.0879155 0.22572997 0.37454855 0.50250751 0.58251637 0.59553581 0.5497067 0.47893447 0.42495438 0.40936339 0.42672193 0.44992566 0.45711869 0.4361091][-0.023944046 0.077601969 0.22719355 0.39895517 0.55452633 0.65616089 0.67696244 0.6253987 0.54252434 0.48057169 0.46615818 0.49243471 0.52720839 0.54366106 0.52358556][-0.030585099 0.072594859 0.22756343 0.40622967 0.56477892 0.66127664 0.67182547 0.61293083 0.53148758 0.48183671 0.487126 0.53251404 0.58132696 0.60632694 0.5873003][-0.04444658 0.054057278 0.20201997 0.36783376 0.50635654 0.57922447 0.57219744 0.51066428 0.44216377 0.41394508 0.44102934 0.50117809 0.558764 0.58966273 0.57403064][-0.0777664 0.0030457384 0.12713689 0.26250812 0.367975 0.41410866 0.39677855 0.34337318 0.29463997 0.28543651 0.3233138 0.38460714 0.4407551 0.47258353 0.4614625][-0.12365363 -0.073340923 0.011143487 0.10189786 0.16665494 0.187344 0.16804224 0.13162434 0.10686629 0.11450135 0.15591121 0.20959774 0.2560986 0.28215763 0.27288866][-0.16204947 -0.14399427 -0.10196735 -0.056270335 -0.028546348 -0.026946329 -0.04314233 -0.061306093 -0.065285005 -0.046202481 -0.0065415674 0.03613472 0.070025362 0.086883791 0.077504247][-0.18090822 -0.18664722 -0.17661442 -0.16283581 -0.157954 -0.16466627 -0.17527664 -0.18058154 -0.17382906 -0.15358573 -0.12327108 -0.0948875 -0.074509174 -0.066487506 -0.074725911][-0.1804626 -0.19998424 -0.20845598 -0.21176822 -0.21609892 -0.22248454 -0.22652134 -0.22464384 -0.21554004 -0.20073789 -0.18312418 -0.16886248 -0.16025847 -0.1586899 -0.16506936]]...]
INFO - root - 2017-12-11 07:56:52.784508: step 30210, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 43h:31m:44s remains)
INFO - root - 2017-12-11 07:56:58.125537: step 30220, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.523 sec/batch; 43h:52m:49s remains)
INFO - root - 2017-12-11 07:57:03.569201: step 30230, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 44h:07m:27s remains)
INFO - root - 2017-12-11 07:57:08.941399: step 30240, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 45h:10m:23s remains)
INFO - root - 2017-12-11 07:57:14.317892: step 30250, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 44h:46m:26s remains)
INFO - root - 2017-12-11 07:57:19.648547: step 30260, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:16m:50s remains)
INFO - root - 2017-12-11 07:57:25.048120: step 30270, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 43h:52m:08s remains)
INFO - root - 2017-12-11 07:57:30.290033: step 30280, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:30m:19s remains)
INFO - root - 2017-12-11 07:57:35.627875: step 30290, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:45m:06s remains)
INFO - root - 2017-12-11 07:57:40.693801: step 30300, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:23m:27s remains)
2017-12-11 07:57:41.297183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.055127006 -0.042698413 -0.026028493 -0.011320172 -0.0012597304 0.0031214287 0.0044170017 0.0042351065 0.0019669363 -0.0042732195 -0.014087523 -0.023710642 -0.02792988 -0.022733495 -0.011536059][-0.045572694 -0.020772936 0.011789099 0.043402944 0.068619393 0.083688445 0.090934679 0.09140332 0.085865058 0.072385766 0.052115373 0.031621475 0.019620275 0.023281559 0.03750262][-0.027856465 0.012386731 0.065444171 0.11994722 0.16684934 0.19852403 0.21660513 0.22121306 0.21451561 0.19498596 0.16401969 0.13093778 0.10843499 0.10683771 0.11925597][-0.013933808 0.04078095 0.11423382 0.19215856 0.26189637 0.31257719 0.34589681 0.36039522 0.35855469 0.33878124 0.30198947 0.25970465 0.2283621 0.22038206 0.22619574][-0.010422142 0.054315135 0.14424789 0.24199562 0.33174127 0.40066403 0.45085365 0.4784911 0.48538244 0.47079074 0.43636757 0.39479154 0.36331946 0.35283083 0.34948781][-0.016737992 0.05272758 0.15318544 0.26479128 0.37025741 0.45579022 0.52201474 0.56126845 0.57483512 0.5663982 0.54171693 0.51380223 0.4952229 0.48939547 0.47719002][-0.030960694 0.036491036 0.13886166 0.25662905 0.37323081 0.47449332 0.5556187 0.60309678 0.61890793 0.61584371 0.60684705 0.6031751 0.6075744 0.61229849 0.59313226][-0.049151491 0.0091223605 0.10364763 0.21876696 0.33968982 0.45140752 0.54151845 0.59112769 0.60504884 0.60815382 0.61992991 0.6462726 0.67706752 0.69402093 0.67011684][-0.066019684 -0.021211747 0.059200671 0.16508375 0.28196397 0.3920328 0.47628903 0.51623976 0.5230211 0.53261358 0.56711775 0.62367046 0.67908746 0.70721316 0.68081242][-0.079915635 -0.047976129 0.019553497 0.11585454 0.22372808 0.32010743 0.38274077 0.40016288 0.39133936 0.39973509 0.44712228 0.5226751 0.59400177 0.63083881 0.60705727][-0.092351645 -0.07248155 -0.017687207 0.06606333 0.15875602 0.23343928 0.26971707 0.2645143 0.24010511 0.24007975 0.28396547 0.35832083 0.42889136 0.46646175 0.44834206][-0.10481016 -0.097386047 -0.058795746 0.0045156404 0.072795033 0.12081133 0.1351344 0.11777492 0.087156214 0.078748085 0.10793954 0.16382238 0.21766707 0.24625841 0.23298134][-0.1160995 -0.12038172 -0.10088416 -0.064032137 -0.026083902 -0.0054117735 -0.0069362479 -0.027279677 -0.055118877 -0.067447774 -0.0537717 -0.020388106 0.012847357 0.029292213 0.018918924][-0.12655595 -0.14015663 -0.13818353 -0.1256969 -0.11395537 -0.1138924 -0.12554847 -0.1457838 -0.16856167 -0.18169709 -0.17849451 -0.16120948 -0.14169386 -0.13221522 -0.13852181][-0.13193084 -0.15125543 -0.16140398 -0.1656732 -0.17021441 -0.18057378 -0.1960023 -0.21485971 -0.23409989 -0.24738264 -0.24986452 -0.241521 -0.22894426 -0.22107512 -0.22138478]]...]
INFO - root - 2017-12-11 07:57:46.671905: step 30310, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:37m:47s remains)
INFO - root - 2017-12-11 07:57:52.085464: step 30320, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:30m:41s remains)
INFO - root - 2017-12-11 07:57:57.491672: step 30330, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 44h:58m:42s remains)
INFO - root - 2017-12-11 07:58:02.860631: step 30340, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:30m:52s remains)
INFO - root - 2017-12-11 07:58:08.251208: step 30350, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:45m:38s remains)
INFO - root - 2017-12-11 07:58:13.535811: step 30360, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 43h:41m:51s remains)
INFO - root - 2017-12-11 07:58:18.940496: step 30370, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 45h:19m:35s remains)
INFO - root - 2017-12-11 07:58:24.333624: step 30380, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 43h:49m:26s remains)
INFO - root - 2017-12-11 07:58:29.624548: step 30390, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:48m:00s remains)
INFO - root - 2017-12-11 07:58:34.906221: step 30400, loss = 0.69, batch loss = 0.63 (19.1 examples/sec; 0.419 sec/batch; 35h:07m:17s remains)
2017-12-11 07:58:35.398666: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046168286 0.040400591 0.033014245 0.026276238 0.020968502 0.016902888 0.012891635 0.008411454 0.0048282635 0.0032786676 0.0039790333 0.0067313886 0.011500663 0.017672364 0.023489214][0.029926172 0.027512673 0.02450599 0.022380337 0.021219589 0.020179646 0.017700221 0.013223694 0.0083974516 0.0050808489 0.0043205954 0.0062089958 0.010563144 0.0166112 0.022605158][0.014395741 0.015802076 0.018633815 0.023329182 0.028789643 0.03297165 0.033470698 0.029002775 0.021272134 0.013128415 0.0073330174 0.0051034624 0.0065286169 0.010692255 0.015507636][0.014508327 0.020109793 0.029510545 0.04198559 0.055057094 0.065390609 0.069759995 0.065603651 0.054025073 0.03890793 0.025382632 0.016276682 0.01220805 0.012274902 0.014009777][0.034102548 0.045553442 0.062610239 0.083785713 0.10513372 0.12212071 0.13093445 0.12790437 0.11274537 0.090143837 0.067868307 0.050562143 0.0392138 0.033019129 0.029596465][0.063318975 0.082385123 0.10906954 0.14108689 0.17308584 0.19913894 0.21451154 0.21462138 0.19732507 0.16817279 0.13789353 0.1131287 0.094549276 0.08090207 0.070422709][0.083701827 0.11032666 0.14665654 0.18962367 0.23361735 0.271243 0.29609233 0.30229586 0.28586382 0.25321752 0.2179877 0.18925498 0.1668267 0.14813961 0.13175416][0.083984263 0.1148295 0.15685254 0.20614512 0.25833258 0.30550396 0.33969206 0.35380659 0.34284329 0.31348231 0.28024429 0.25385582 0.23291984 0.21349858 0.19433296][0.064288668 0.09326005 0.13417917 0.18260661 0.23580852 0.28635982 0.32544893 0.34584686 0.34311792 0.32356673 0.29958966 0.28180832 0.26762125 0.25223777 0.23465586][0.034031384 0.055285521 0.088464849 0.12940364 0.17666098 0.22370498 0.26149333 0.28372663 0.28760922 0.27896187 0.26655135 0.2595897 0.25476095 0.24696687 0.23508421][0.0083267754 0.018727429 0.040232714 0.0695403 0.1060283 0.14448164 0.17622294 0.19584362 0.20231226 0.20122847 0.19806965 0.20017754 0.20417556 0.20511353 0.20089811][-0.0020011559 -0.0027689536 0.0066236537 0.024079226 0.049563494 0.079644851 0.10624623 0.12350861 0.1312519 0.13487156 0.13744164 0.14411058 0.15213756 0.15753995 0.15733856][0.0062922826 -0.0031655238 -0.0031969245 0.0054524424 0.023220019 0.0488133 0.074704796 0.092974946 0.10276685 0.10906196 0.11403717 0.12053841 0.12681486 0.13055445 0.12881497][0.025292749 0.0089870188 0.00186623 0.0048759654 0.018711168 0.043941736 0.07343246 0.096687354 0.11070255 0.11936756 0.12510107 0.12903984 0.13067062 0.12964682 0.12377677][0.044596545 0.023655865 0.01121015 0.009873677 0.020752113 0.045967229 0.078977361 0.1076558 0.12677847 0.13879487 0.1463823 0.1491456 0.14740598 0.14226989 0.13251841]]...]
INFO - root - 2017-12-11 07:58:40.682009: step 30410, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:23m:40s remains)
INFO - root - 2017-12-11 07:58:45.972681: step 30420, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:40m:47s remains)
INFO - root - 2017-12-11 07:58:51.308594: step 30430, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:32m:46s remains)
INFO - root - 2017-12-11 07:58:56.606583: step 30440, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.508 sec/batch; 42h:35m:19s remains)
INFO - root - 2017-12-11 07:59:01.882677: step 30450, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 46h:05m:20s remains)
INFO - root - 2017-12-11 07:59:07.231424: step 30460, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:00m:07s remains)
INFO - root - 2017-12-11 07:59:12.616791: step 30470, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:59m:19s remains)
INFO - root - 2017-12-11 07:59:17.955432: step 30480, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 44h:42m:59s remains)
INFO - root - 2017-12-11 07:59:23.278455: step 30490, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 43h:42m:28s remains)
INFO - root - 2017-12-11 07:59:28.680421: step 30500, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:21m:34s remains)
2017-12-11 07:59:29.225993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037860233 -0.043860782 -0.047551736 -0.049481578 -0.049278326 -0.047870349 -0.0468255 -0.047324143 -0.049450662 -0.051916987 -0.053097058 -0.052006572 -0.048870828 -0.044914287 -0.041718118][-0.024591023 -0.030527959 -0.033056766 -0.032214671 -0.028396696 -0.024236215 -0.022886774 -0.025784651 -0.032131571 -0.038888093 -0.042713068 -0.041846056 -0.036840539 -0.030062074 -0.024165131][-0.007423053 -0.0083592683 -0.003623073 0.0068878182 0.02070068 0.032034334 0.035228692 0.028194554 0.012932429 -0.0049817157 -0.019172257 -0.025663007 -0.024076384 -0.017236982 -0.0088908756][0.007578644 0.019095145 0.040202893 0.070247069 0.10394397 0.13152543 0.14358659 0.13570972 0.10965221 0.072757691 0.036001284 0.0086247651 -0.0051821568 -0.0072652819 -0.0018536282][0.018659012 0.047345549 0.091145985 0.14943442 0.21552736 0.27611989 0.31541905 0.32064211 0.28776917 0.22333339 0.14563383 0.075795941 0.028351571 0.0046154633 -0.0011586533][0.02701455 0.071040452 0.13683467 0.2269195 0.33572689 0.446334 0.53262532 0.56659234 0.53198093 0.43232474 0.29709303 0.16671003 0.072140336 0.019022042 -0.0025762331][0.032762438 0.0857841 0.16718532 0.28507608 0.43595621 0.59872931 0.73548269 0.80139816 0.7668007 0.63258374 0.44037297 0.25070187 0.11134133 0.031714845 -0.0039218906][0.029162019 0.082601219 0.16933712 0.30228874 0.47865477 0.672777 0.83894694 0.92328006 0.88916069 0.73579675 0.51167035 0.28867444 0.12495074 0.03241469 -0.0092206728][0.0084913792 0.051031459 0.12700132 0.25175089 0.42198539 0.60962188 0.76943886 0.8515535 0.82182151 0.67756456 0.46414986 0.2505815 0.094803721 0.010072327 -0.024708925][-0.026853884 -0.0058096163 0.042558312 0.13386722 0.26541588 0.41178292 0.53596961 0.60087794 0.580788 0.47072792 0.3053064 0.13955045 0.021901887 -0.035235863 -0.050555002][-0.062583745 -0.065541007 -0.051229525 -0.0057328017 0.070659645 0.15959105 0.23608659 0.27793792 0.26864082 0.20147236 0.098919071 -0.0014868317 -0.065938361 -0.086051792 -0.077064425][-0.08363001 -0.10367801 -0.1156641 -0.10904422 -0.0817088 -0.043928012 -0.0097115906 0.01040011 0.0074848053 -0.02453867 -0.072388411 -0.11393078 -0.13064456 -0.11971067 -0.092929527][-0.082045406 -0.10909394 -0.13389227 -0.14949474 -0.15262398 -0.14722015 -0.14004868 -0.13526861 -0.13667978 -0.14783646 -0.16110586 -0.16527778 -0.15345524 -0.12718694 -0.094794452][-0.057582263 -0.085216329 -0.11378802 -0.13833998 -0.15530708 -0.16496271 -0.17027403 -0.17320153 -0.17522614 -0.17750022 -0.17552218 -0.16440354 -0.14331485 -0.11555868 -0.086445995][-0.020193387 -0.046203364 -0.074703492 -0.10080107 -0.12099694 -0.13446487 -0.14286575 -0.14805061 -0.15104453 -0.15167789 -0.14711164 -0.13528058 -0.11709216 -0.095352113 -0.073262431]]...]
INFO - root - 2017-12-11 07:59:34.276846: step 30510, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 43h:04m:22s remains)
INFO - root - 2017-12-11 07:59:39.663616: step 30520, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:04m:41s remains)
INFO - root - 2017-12-11 07:59:44.969205: step 30530, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:22m:26s remains)
INFO - root - 2017-12-11 07:59:50.275872: step 30540, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 43h:30m:13s remains)
INFO - root - 2017-12-11 07:59:55.639311: step 30550, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:34m:22s remains)
INFO - root - 2017-12-11 08:00:01.021603: step 30560, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 44h:01m:24s remains)
INFO - root - 2017-12-11 08:00:06.425967: step 30570, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 43h:58m:10s remains)
INFO - root - 2017-12-11 08:00:11.816532: step 30580, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.540 sec/batch; 45h:18m:11s remains)
INFO - root - 2017-12-11 08:00:17.139862: step 30590, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:23m:31s remains)
INFO - root - 2017-12-11 08:00:22.478064: step 30600, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 45h:05m:52s remains)
2017-12-11 08:00:23.022653: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.144907 0.14451575 0.13983026 0.13517304 0.12862459 0.12417516 0.12355791 0.12489563 0.12732291 0.13129614 0.13484877 0.13356675 0.12561072 0.11697078 0.1115493][0.16556339 0.17178243 0.17510791 0.18001173 0.18086377 0.18022913 0.17930134 0.17638443 0.17196678 0.16919696 0.1690086 0.16733746 0.16191912 0.15765181 0.15696618][0.16854346 0.18837762 0.20945121 0.23395306 0.25077903 0.25989875 0.26082397 0.25224182 0.23697126 0.22266495 0.21567 0.21321198 0.21274805 0.21618962 0.22228119][0.16751249 0.21014573 0.25989655 0.31476226 0.356534 0.38202697 0.38801724 0.37360078 0.34387776 0.31251022 0.2938033 0.28761631 0.29075703 0.30127713 0.31319964][0.16050367 0.22631395 0.3064326 0.39459956 0.46546048 0.51268935 0.5295822 0.51472718 0.47286844 0.4230631 0.38891238 0.37397772 0.37425297 0.38510987 0.39838144][0.14627108 0.22734189 0.33029836 0.44584379 0.54390633 0.61666262 0.65348577 0.64988166 0.60569388 0.54216772 0.49186262 0.46132559 0.44621274 0.44252467 0.44609204][0.11877793 0.20461492 0.3199493 0.45377809 0.57413918 0.67332923 0.73685992 0.75424141 0.71639717 0.642709 0.5739032 0.51897633 0.47483853 0.4419353 0.42587173][0.06885875 0.14871034 0.26476571 0.405587 0.53993618 0.66096693 0.75135195 0.79339921 0.76801294 0.68930519 0.60356915 0.52204448 0.44461247 0.37832031 0.34050739][-0.0018734132 0.060538858 0.16334035 0.29559961 0.4293716 0.55910683 0.66691691 0.72911716 0.718887 0.64426863 0.5525735 0.45592263 0.3573193 0.26933363 0.21854286][-0.072952449 -0.0400889 0.033675104 0.13967237 0.25562844 0.37741086 0.48833707 0.56171912 0.56725138 0.50729895 0.42460173 0.33037949 0.23022468 0.13939109 0.088361613][-0.12982848 -0.12816045 -0.088665165 -0.017116476 0.070364647 0.1712999 0.27148286 0.34407693 0.35995907 0.3177253 0.25222576 0.17289409 0.086925171 0.010025437 -0.029473023][-0.15865433 -0.18264404 -0.17421791 -0.13695289 -0.0817513 -0.010158427 0.066964261 0.12630476 0.14425544 0.11803751 0.073347248 0.017589044 -0.042241234 -0.091927253 -0.11023613][-0.15070936 -0.19418558 -0.2115027 -0.2048391 -0.18085612 -0.14096583 -0.092814669 -0.05328 -0.038095403 -0.049437668 -0.071007021 -0.097963467 -0.12535004 -0.14226995 -0.13620657][-0.11104814 -0.1657175 -0.20078181 -0.21619682 -0.21542025 -0.20051689 -0.17678085 -0.15470004 -0.14284632 -0.14230868 -0.14476956 -0.14730191 -0.14716484 -0.13774058 -0.11383145][-0.061012674 -0.1181716 -0.16197212 -0.18906678 -0.20021406 -0.19854741 -0.18862599 -0.17626792 -0.165136 -0.15655439 -0.14819308 -0.13724121 -0.1210273 -0.096412659 -0.063423254]]...]
INFO - root - 2017-12-11 08:00:28.123178: step 30610, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:47m:14s remains)
INFO - root - 2017-12-11 08:00:33.398366: step 30620, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:51m:24s remains)
INFO - root - 2017-12-11 08:00:38.788774: step 30630, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:57m:13s remains)
INFO - root - 2017-12-11 08:00:44.157941: step 30640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 45h:49m:05s remains)
INFO - root - 2017-12-11 08:00:49.445360: step 30650, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 44h:43m:42s remains)
INFO - root - 2017-12-11 08:00:54.836045: step 30660, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:03m:02s remains)
INFO - root - 2017-12-11 08:01:00.215209: step 30670, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 44h:02m:14s remains)
INFO - root - 2017-12-11 08:01:05.587985: step 30680, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:21m:12s remains)
INFO - root - 2017-12-11 08:01:10.930080: step 30690, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 44h:03m:38s remains)
INFO - root - 2017-12-11 08:01:16.256109: step 30700, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:33m:18s remains)
2017-12-11 08:01:16.823090: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0062780841 0.020103997 0.02904862 0.029471785 0.024566332 0.016710689 0.010365981 0.0094948411 0.0149287 0.023045987 0.029975954 0.035024863 0.03673353 0.033382885 0.024163764][0.067234479 0.095397793 0.11079673 0.11037996 0.10054965 0.08502198 0.072161272 0.069767125 0.080362312 0.096903384 0.11129245 0.12129883 0.12443367 0.11804345 0.10023532][0.14524326 0.18931127 0.2101693 0.2071898 0.1917311 0.16897233 0.15181214 0.15150686 0.17115253 0.19876844 0.22123703 0.2346988 0.23583156 0.22250071 0.19207454][0.21846782 0.27584395 0.29835731 0.28955 0.26791942 0.24125408 0.22614704 0.23459937 0.26861045 0.31138805 0.3450453 0.36264586 0.35877147 0.33325154 0.2845082][0.2716774 0.33910757 0.36155334 0.34680721 0.32149413 0.29602084 0.28730035 0.30491102 0.3501105 0.40465498 0.44789493 0.46897554 0.45869467 0.41919938 0.35143018][0.29266974 0.36885354 0.39448518 0.38025358 0.35809514 0.33922914 0.33761775 0.35847205 0.40366444 0.45748708 0.50021452 0.51860356 0.50014412 0.44975349 0.37097073][0.27638313 0.36134216 0.39740127 0.39471644 0.38361123 0.3738713 0.37669528 0.39379847 0.42799792 0.46771154 0.49757174 0.50529951 0.47719279 0.42110491 0.3427251][0.23135486 0.32357657 0.37463197 0.38936484 0.39047763 0.38654068 0.38859475 0.39655536 0.41354713 0.4320088 0.44333395 0.43978319 0.40708369 0.35318807 0.28459138][0.17939813 0.2758086 0.33905667 0.36675167 0.37340751 0.36883956 0.3657994 0.36264053 0.36095956 0.35700843 0.35117352 0.34230471 0.31509236 0.27267152 0.2198237][0.13072759 0.22527947 0.29273745 0.32493532 0.33139351 0.32348487 0.314424 0.30100495 0.28285956 0.25927153 0.23928863 0.22852987 0.21153753 0.18473122 0.14904231][0.079994142 0.16467844 0.22714624 0.25729659 0.26157898 0.25037831 0.23597328 0.2151642 0.18651961 0.1503682 0.1217074 0.11076435 0.1030747 0.09034507 0.069375239][0.019056791 0.082816638 0.13128012 0.15488569 0.15767136 0.14591719 0.12928897 0.10658255 0.07632155 0.038688883 0.0098575708 0.0010192376 -0.00096621516 -0.0055931513 -0.017423185][-0.048517376 -0.013571225 0.014431403 0.028624229 0.031171679 0.023629282 0.011430417 -0.0051812232 -0.02800514 -0.057187609 -0.079467282 -0.085329942 -0.085418746 -0.087503649 -0.094849437][-0.10578388 -0.0971481 -0.088322327 -0.08309973 -0.080330707 -0.081452549 -0.085409187 -0.092479005 -0.10473841 -0.12252043 -0.1368567 -0.14130968 -0.14254896 -0.14487052 -0.14917056][-0.13887493 -0.14622121 -0.1491354 -0.15010548 -0.1484946 -0.14679232 -0.14657353 -0.14902507 -0.15515399 -0.16494112 -0.17357431 -0.17780201 -0.18021888 -0.18155509 -0.18130428]]...]
INFO - root - 2017-12-11 08:01:21.804078: step 30710, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:08m:42s remains)
INFO - root - 2017-12-11 08:01:27.152112: step 30720, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:47m:14s remains)
INFO - root - 2017-12-11 08:01:32.471307: step 30730, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 44h:06m:33s remains)
INFO - root - 2017-12-11 08:01:37.890136: step 30740, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:02m:09s remains)
INFO - root - 2017-12-11 08:01:43.255341: step 30750, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 43h:18m:20s remains)
INFO - root - 2017-12-11 08:01:48.645365: step 30760, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 44h:45m:52s remains)
INFO - root - 2017-12-11 08:01:54.039844: step 30770, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:42m:27s remains)
INFO - root - 2017-12-11 08:01:59.422889: step 30780, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:55m:55s remains)
INFO - root - 2017-12-11 08:02:04.723770: step 30790, loss = 0.66, batch loss = 0.60 (15.3 examples/sec; 0.523 sec/batch; 43h:47m:40s remains)
INFO - root - 2017-12-11 08:02:10.092471: step 30800, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:52m:48s remains)
2017-12-11 08:02:10.633929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011820761 0.0081781717 0.034630287 0.06136439 0.0842497 0.099423163 0.10380849 0.098276556 0.087910756 0.076171592 0.0663474 0.061438303 0.062359415 0.066512242 0.069976814][0.0029974596 0.037556961 0.08295536 0.13071947 0.17451976 0.20763204 0.22205366 0.21737638 0.20393519 0.18639833 0.16888937 0.1575067 0.15795168 0.16474599 0.16753778][0.022701593 0.073798791 0.14040694 0.21263687 0.28181079 0.3378934 0.36726192 0.36809218 0.35661939 0.33734211 0.31324911 0.29331118 0.29054537 0.29705647 0.29447547][0.041113537 0.11029417 0.19825581 0.29372811 0.38524559 0.46085072 0.50171357 0.505735 0.49696767 0.47907948 0.45170066 0.42420632 0.41772071 0.42249861 0.41302833][0.054815508 0.14191899 0.25060737 0.36655554 0.47401926 0.56172782 0.60624123 0.60543972 0.59262913 0.57334197 0.54528028 0.51460105 0.5078072 0.51275843 0.49894571][0.06497632 0.16874406 0.29692978 0.43044862 0.54854167 0.64244533 0.68595612 0.67649317 0.65534329 0.63250434 0.60651577 0.57702416 0.57221204 0.57805365 0.56021106][0.072993457 0.19142318 0.33590022 0.48199391 0.60410923 0.69720477 0.73598319 0.7171393 0.68758708 0.66414016 0.64659268 0.62490237 0.62446547 0.63156706 0.608907][0.076740757 0.20522557 0.3601042 0.51190096 0.62940067 0.71191216 0.73993778 0.70903248 0.66847235 0.644484 0.64025736 0.63361275 0.64077288 0.65016615 0.62564075][0.075966008 0.20868382 0.36818519 0.520444 0.628142 0.69386333 0.70607048 0.65906781 0.602201 0.57227564 0.57922107 0.590137 0.6075536 0.62124896 0.59938735][0.06708318 0.19629887 0.35464424 0.50485551 0.6045922 0.65573931 0.65417409 0.59304136 0.52068335 0.48223773 0.49441081 0.51711333 0.5410502 0.55741453 0.53937513][0.040647957 0.155712 0.30251724 0.44558269 0.54110283 0.58547878 0.57911825 0.51459354 0.43778878 0.39702839 0.41057184 0.43668166 0.4608613 0.47674394 0.45975041][-0.0045527653 0.082796149 0.20278355 0.32659417 0.41379571 0.45509169 0.45227584 0.39707381 0.32839039 0.29119644 0.30240169 0.32581171 0.34739667 0.36276436 0.34792134][-0.054307804 -0.0034920636 0.076900057 0.16740049 0.23491883 0.26829568 0.27008253 0.23117729 0.17770165 0.14532621 0.15015024 0.16657215 0.18329389 0.19749308 0.18846764][-0.090204425 -0.071155295 -0.029770426 0.022736201 0.063642822 0.083968878 0.085958809 0.060890857 0.023679292 -0.0018364297 -0.0028007117 0.00447921 0.013503134 0.0239797 0.021196062][-0.10941775 -0.11143347 -0.099331729 -0.07871902 -0.061508134 -0.053529575 -0.05398922 -0.069911271 -0.092477515 -0.10786679 -0.109359 -0.10812248 -0.10610729 -0.10053904 -0.10016545]]...]
INFO - root - 2017-12-11 08:02:15.991462: step 30810, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:00m:44s remains)
INFO - root - 2017-12-11 08:02:21.041681: step 30820, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 45h:02m:35s remains)
INFO - root - 2017-12-11 08:02:26.438740: step 30830, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:45m:39s remains)
INFO - root - 2017-12-11 08:02:31.814384: step 30840, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 45h:09m:50s remains)
INFO - root - 2017-12-11 08:02:37.253911: step 30850, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:22m:00s remains)
INFO - root - 2017-12-11 08:02:42.598791: step 30860, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 45h:45m:20s remains)
INFO - root - 2017-12-11 08:02:47.928886: step 30870, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:31m:13s remains)
INFO - root - 2017-12-11 08:02:53.237519: step 30880, loss = 0.70, batch loss = 0.65 (15.8 examples/sec; 0.506 sec/batch; 42h:21m:15s remains)
INFO - root - 2017-12-11 08:02:58.553101: step 30890, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:33m:49s remains)
INFO - root - 2017-12-11 08:03:03.899341: step 30900, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:19m:40s remains)
2017-12-11 08:03:04.425350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21300755 0.21979092 0.21759152 0.21381354 0.21440245 0.21998845 0.23076667 0.24004695 0.24343082 0.24057384 0.23345304 0.22799569 0.22606029 0.22692256 0.22642444][0.24136318 0.25443235 0.25791439 0.2603721 0.26509237 0.27010462 0.2748951 0.27488735 0.268344 0.258372 0.24944241 0.24876247 0.25437486 0.26197457 0.26371884][0.24919853 0.26759332 0.27712068 0.28715327 0.29742062 0.30441147 0.30588973 0.29832232 0.28033391 0.25825626 0.24227554 0.24348128 0.256921 0.27299622 0.27882347][0.22188614 0.24597315 0.26413137 0.28473648 0.30426565 0.31814754 0.32238957 0.31422305 0.29031238 0.25725251 0.23105982 0.22877838 0.2464108 0.26954189 0.28049836][0.15927407 0.18747255 0.21501522 0.24892543 0.28369802 0.31234694 0.32732838 0.32563838 0.30128628 0.26014039 0.22326633 0.21337757 0.23064783 0.25774083 0.27291933][0.086379312 0.11346894 0.14526726 0.18919986 0.24072026 0.28927046 0.32146168 0.33160403 0.31147882 0.26662403 0.22107257 0.20107117 0.21294725 0.23960777 0.25729749][0.024741197 0.04358054 0.070274949 0.11445246 0.17574763 0.24198836 0.293492 0.31914693 0.30817354 0.26596308 0.21692643 0.18803327 0.19233604 0.21594825 0.23515761][-0.020493569 -0.012691243 0.0043097422 0.041729722 0.10352544 0.17838532 0.24304695 0.28112715 0.28007084 0.24547309 0.20020662 0.1691259 0.16956343 0.19117665 0.21138589][-0.04893579 -0.050729148 -0.042474233 -0.013688779 0.041501183 0.11381655 0.17986695 0.22106376 0.22500461 0.19825318 0.16185383 0.13682646 0.14020839 0.16344944 0.18538004][-0.064847194 -0.073152885 -0.072347008 -0.053085651 -0.0093261665 0.052183878 0.11028891 0.14693829 0.15042293 0.12768266 0.098637916 0.081150822 0.089312017 0.11512549 0.13962701][-0.068475753 -0.083762258 -0.092043594 -0.085700676 -0.05906048 -0.015152674 0.029597243 0.059063088 0.061430208 0.0423612 0.020085575 0.0087835435 0.019208031 0.045196872 0.071552113][-0.066409014 -0.086235166 -0.10232604 -0.1086735 -0.10080428 -0.078071535 -0.050782759 -0.031327911 -0.03106462 -0.046221614 -0.061470632 -0.0671162 -0.055952594 -0.031771056 -0.0048446697][-0.061615236 -0.083118379 -0.10210939 -0.11543867 -0.12053931 -0.11545695 -0.10554946 -0.09890189 -0.10455597 -0.1195295 -0.13142584 -0.13422355 -0.12397097 -0.10312435 -0.077610433][-0.058013983 -0.078173548 -0.096203446 -0.11089678 -0.1212727 -0.12598538 -0.12763852 -0.13114673 -0.14268608 -0.1599462 -0.17357719 -0.17929426 -0.17437123 -0.15987925 -0.13862085][-0.057260334 -0.075064994 -0.090401329 -0.1036742 -0.11457495 -0.12257503 -0.12898761 -0.13700531 -0.15013538 -0.16688094 -0.18156636 -0.19119167 -0.19326963 -0.18662357 -0.17193475]]...]
INFO - root - 2017-12-11 08:03:09.743725: step 30910, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 44h:29m:18s remains)
INFO - root - 2017-12-11 08:03:14.810035: step 30920, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:12m:58s remains)
INFO - root - 2017-12-11 08:03:20.301631: step 30930, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:32m:51s remains)
INFO - root - 2017-12-11 08:03:25.722812: step 30940, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:05m:28s remains)
INFO - root - 2017-12-11 08:03:31.153495: step 30950, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 47h:25m:08s remains)
INFO - root - 2017-12-11 08:03:36.585735: step 30960, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:05m:28s remains)
INFO - root - 2017-12-11 08:03:41.873990: step 30970, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 43h:29m:48s remains)
INFO - root - 2017-12-11 08:03:47.229378: step 30980, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:22m:59s remains)
INFO - root - 2017-12-11 08:03:52.614030: step 30990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:51m:46s remains)
INFO - root - 2017-12-11 08:03:57.885883: step 31000, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 43h:56m:30s remains)
2017-12-11 08:03:58.464404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.068224363 -0.070964865 -0.069113538 -0.067654312 -0.067408264 -0.068104692 -0.069436051 -0.0707729 -0.070945479 -0.069588371 -0.067177832 -0.065013856 -0.064284652 -0.064873658 -0.065639071][-0.069719627 -0.070737086 -0.066848852 -0.064228266 -0.063862361 -0.065246664 -0.067864291 -0.070514739 -0.070902534 -0.068181761 -0.063603878 -0.060234644 -0.060680546 -0.06425824 -0.067873932][-0.040031306 -0.039531969 -0.034736756 -0.030371705 -0.0274756 -0.025886001 -0.026541563 -0.029478041 -0.031705629 -0.031123636 -0.028879454 -0.029376322 -0.036537152 -0.048320387 -0.05845657][0.040231831 0.036309693 0.036291886 0.042521924 0.053720903 0.06739255 0.076875672 0.076365009 0.067674033 0.055683549 0.042874567 0.026804861 0.0036698468 -0.022826459 -0.043146223][0.18039435 0.15985022 0.14571689 0.15484767 0.18434019 0.2238009 0.25378138 0.25700977 0.23283683 0.19143915 0.14462528 0.097516179 0.04904237 0.003908352 -0.027236653][0.34759349 0.297871 0.26241681 0.27739692 0.3352026 0.4111582 0.46700069 0.47087497 0.42063746 0.33489528 0.23995005 0.15283351 0.076939248 0.016398735 -0.020642472][0.46943393 0.38740531 0.33241013 0.35641536 0.44395137 0.55355495 0.62902433 0.62713516 0.54691726 0.41686732 0.27818349 0.15928431 0.067223579 0.0033924105 -0.029846285][0.48074737 0.37608162 0.31293496 0.34598696 0.45159355 0.57655555 0.65588081 0.64373744 0.54331952 0.39033863 0.23409693 0.10814772 0.020715265 -0.030596193 -0.050457597][0.36329445 0.25855669 0.20322023 0.24114035 0.34458333 0.45946777 0.525726 0.50510156 0.40559036 0.26387152 0.12633204 0.022762924 -0.040485706 -0.06850414 -0.070909552][0.17007355 0.086291842 0.049977113 0.086196952 0.16928266 0.25508186 0.29930893 0.27738914 0.19978729 0.0971433 0.0041441321 -0.059030745 -0.089719839 -0.09375488 -0.081777178][-0.011630602 -0.065649688 -0.081448883 -0.052568555 0.0024178496 0.0545905 0.0782883 0.062285785 0.016163763 -0.039541826 -0.084400438 -0.10818084 -0.11141622 -0.099155217 -0.080047823][-0.11997752 -0.14701487 -0.14804734 -0.12788637 -0.097102895 -0.070791073 -0.060235873 -0.068015084 -0.087347768 -0.10738547 -0.11882458 -0.11803643 -0.10691568 -0.088941939 -0.069737688][-0.14939994 -0.15751994 -0.15074107 -0.13688275 -0.12108231 -0.10976819 -0.10609203 -0.10871799 -0.11358321 -0.11605651 -0.11296935 -0.10366856 -0.08997646 -0.073802315 -0.0582211][-0.13269517 -0.13221805 -0.12280098 -0.11250731 -0.10392582 -0.099432528 -0.098885715 -0.099911228 -0.10004099 -0.097676545 -0.092030488 -0.0832982 -0.07262478 -0.060772702 -0.049342692][-0.104978 -0.10217876 -0.093139008 -0.085151695 -0.079768 -0.077701434 -0.077991739 -0.078532048 -0.077733964 -0.075131506 -0.070746332 -0.064982377 -0.058294337 -0.050662935 -0.042983703]]...]
INFO - root - 2017-12-11 08:04:03.813010: step 31010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:40m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:04:08.835775: step 31020, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:26m:31s remains)
INFO - root - 2017-12-11 08:04:14.293000: step 31030, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:43m:15s remains)
INFO - root - 2017-12-11 08:04:19.557271: step 31040, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 44h:25m:03s remains)
INFO - root - 2017-12-11 08:04:24.904096: step 31050, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.550 sec/batch; 46h:03m:26s remains)
INFO - root - 2017-12-11 08:04:30.283341: step 31060, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 44h:56m:33s remains)
INFO - root - 2017-12-11 08:04:35.700881: step 31070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:41m:46s remains)
INFO - root - 2017-12-11 08:04:41.005325: step 31080, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:04m:35s remains)
INFO - root - 2017-12-11 08:04:46.365616: step 31090, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:47m:27s remains)
INFO - root - 2017-12-11 08:04:51.741042: step 31100, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:23m:50s remains)
2017-12-11 08:04:52.319015: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2568869 0.31736243 0.37020206 0.4164907 0.44691575 0.44432023 0.39451325 0.30430695 0.20666799 0.13794589 0.12194204 0.15899794 0.23118877 0.3082591 0.35525674][0.29952997 0.35962397 0.40604478 0.44151509 0.46444771 0.4638468 0.42565379 0.34985188 0.26093978 0.18873918 0.15524331 0.16721238 0.21529742 0.27389389 0.30808011][0.32972389 0.38629434 0.41935575 0.43690705 0.44951993 0.45708346 0.44540641 0.401973 0.33319527 0.2566348 0.19370817 0.16302755 0.17068699 0.20054422 0.22261438][0.35429853 0.41362488 0.43985251 0.44681931 0.45803583 0.48343739 0.50622576 0.49925587 0.44991735 0.3646194 0.26538715 0.18444523 0.14524473 0.14363752 0.15591672][0.36222497 0.43165034 0.46311879 0.4749828 0.49935916 0.55112934 0.60724282 0.62732744 0.585647 0.48385164 0.34778878 0.22118144 0.14063874 0.11169342 0.1155794][0.35421503 0.44019833 0.48832113 0.51650083 0.55874634 0.63063538 0.7036618 0.730378 0.67972279 0.55595118 0.39196756 0.23941749 0.14028962 0.1024468 0.10673074][0.34725377 0.451666 0.51824927 0.55975723 0.607324 0.67780894 0.74346387 0.75789982 0.69074297 0.55168813 0.37992525 0.2296949 0.14175537 0.12003792 0.14029066][0.33624497 0.44955802 0.52355468 0.56475478 0.59957415 0.64746583 0.68776309 0.68118167 0.60184747 0.46384335 0.30870435 0.18695498 0.1334013 0.14537293 0.19232586][0.30928689 0.41724071 0.4832004 0.5092063 0.5174672 0.52999377 0.53667885 0.508043 0.42442423 0.30247915 0.18253739 0.10676542 0.099359594 0.1495454 0.22143519][0.268165 0.36571771 0.4161565 0.41727534 0.38867727 0.35630968 0.32348925 0.27323428 0.19272365 0.09869425 0.026054734 0.0053878482 0.04702739 0.13158156 0.22035041][0.23166838 0.32237259 0.35834205 0.3320556 0.26180819 0.18012638 0.10633519 0.038266122 -0.030471111 -0.087211467 -0.10751295 -0.073923878 0.011460846 0.12398321 0.22500947][0.20464677 0.28635108 0.30925936 0.26169106 0.16016595 0.042049065 -0.060095474 -0.13533255 -0.18526216 -0.20412236 -0.18045174 -0.10807557 0.0041417656 0.13227826 0.24047565][0.17574222 0.24177577 0.2516655 0.19291383 0.079758577 -0.0504806 -0.15934519 -0.22898783 -0.25886068 -0.25001445 -0.20179392 -0.11363179 0.0054868604 0.13696574 0.24821578][0.14845273 0.19941287 0.19946219 0.13805018 0.028645864 -0.093539283 -0.19049734 -0.24425705 -0.25603676 -0.23298211 -0.18118781 -0.099743687 0.0079615274 0.13050897 0.23899947][0.12863962 0.17262945 0.17049369 0.11383753 0.016698176 -0.087578535 -0.16429673 -0.19914652 -0.19758482 -0.17143336 -0.12936096 -0.0664782 0.021453485 0.12840708 0.22827172]]...]
INFO - root - 2017-12-11 08:04:57.638625: step 31110, loss = 0.70, batch loss = 0.65 (15.5 examples/sec; 0.518 sec/batch; 43h:20m:09s remains)
INFO - root - 2017-12-11 08:05:02.715784: step 31120, loss = 0.71, batch loss = 0.65 (21.9 examples/sec; 0.365 sec/batch; 30h:33m:55s remains)
INFO - root - 2017-12-11 08:05:08.047871: step 31130, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 44h:12m:28s remains)
INFO - root - 2017-12-11 08:05:13.389185: step 31140, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:18m:35s remains)
INFO - root - 2017-12-11 08:05:18.692604: step 31150, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:09m:22s remains)
INFO - root - 2017-12-11 08:05:24.119382: step 31160, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:47m:12s remains)
INFO - root - 2017-12-11 08:05:29.399498: step 31170, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:08m:22s remains)
INFO - root - 2017-12-11 08:05:34.759300: step 31180, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 43h:34m:40s remains)
INFO - root - 2017-12-11 08:05:40.132740: step 31190, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:39m:25s remains)
INFO - root - 2017-12-11 08:05:45.475491: step 31200, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:05m:08s remains)
2017-12-11 08:05:46.025729: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088009708 0.083517686 0.088932104 0.099608883 0.10895695 0.11591607 0.11932258 0.11854877 0.11748931 0.12817298 0.15930118 0.20965564 0.26899281 0.32626453 0.36796957][0.13532096 0.14281857 0.16249196 0.1878611 0.20790799 0.21894468 0.21869662 0.20668852 0.189123 0.18321235 0.20363185 0.25268093 0.31928834 0.38864675 0.44105819][0.16706458 0.18904179 0.22540107 0.26872584 0.30467486 0.32640764 0.3288343 0.30883363 0.27238303 0.2401668 0.23387064 0.26237002 0.31865883 0.38793412 0.44653374][0.17954242 0.21339887 0.26319709 0.32413378 0.38155362 0.42593074 0.4466157 0.432105 0.38227144 0.31719786 0.26671469 0.25196475 0.27714896 0.33233944 0.39196351][0.18465222 0.21834175 0.27329367 0.34961227 0.43590486 0.5197376 0.58041787 0.59014112 0.5355745 0.43303907 0.32227191 0.24368882 0.22071852 0.2522797 0.31036082][0.18418831 0.20725428 0.2571964 0.34608132 0.4690465 0.60872954 0.72811061 0.77622885 0.7222389 0.57893461 0.39868587 0.24548957 0.16575661 0.16863707 0.22290184][0.18411136 0.18773872 0.22527835 0.32157573 0.4794617 0.67500418 0.85215592 0.93691796 0.88309747 0.70368278 0.46349397 0.24669302 0.11879466 0.09700729 0.14685398][0.180553 0.16250092 0.18582828 0.28392202 0.46444175 0.69650722 0.90918326 1.0144755 0.95869428 0.75818777 0.48576722 0.23601767 0.083490662 0.049763307 0.098246373][0.1669964 0.1294564 0.13895653 0.23138329 0.41471291 0.65245491 0.8674618 0.97148663 0.91305017 0.71260172 0.4429788 0.19780084 0.050377745 0.021631768 0.074300177][0.14033347 0.085214391 0.078689933 0.15449664 0.31814736 0.53123218 0.71992522 0.80674404 0.749361 0.57025647 0.33515656 0.12690862 0.0092059523 -0.00048557285 0.061211634][0.10722528 0.038894333 0.01462883 0.062932856 0.18630724 0.35026631 0.49302492 0.55484885 0.50498796 0.36395612 0.18468903 0.033542871 -0.040152367 -0.024240285 0.046868347][0.07457114 -0.0018729287 -0.041120987 -0.023398146 0.051227298 0.15658531 0.24688961 0.28182667 0.24209775 0.14411074 0.025814846 -0.064551584 -0.093156368 -0.053529177 0.022973856][0.037200328 -0.038216349 -0.085288428 -0.091410339 -0.057659626 -0.0015889152 0.045386277 0.058328982 0.027068121 -0.036190946 -0.10637014 -0.1504965 -0.14772284 -0.096653 -0.024619618][-0.012122813 -0.077045582 -0.1231805 -0.14299943 -0.13686205 -0.11475051 -0.0967972 -0.097671553 -0.12183242 -0.16111159 -0.20051722 -0.21856138 -0.20320477 -0.15567316 -0.098152846][-0.071339905 -0.11681642 -0.15413849 -0.17889813 -0.19017066 -0.19109888 -0.1919912 -0.20019776 -0.21757765 -0.24006078 -0.26050177 -0.26642573 -0.25140831 -0.21682993 -0.17802498]]...]
INFO - root - 2017-12-11 08:05:51.352023: step 31210, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 43h:45m:45s remains)
INFO - root - 2017-12-11 08:05:56.720544: step 31220, loss = 0.70, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 43h:32m:42s remains)
INFO - root - 2017-12-11 08:06:01.813869: step 31230, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:53m:55s remains)
INFO - root - 2017-12-11 08:06:07.167052: step 31240, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 44h:22m:45s remains)
INFO - root - 2017-12-11 08:06:12.584121: step 31250, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:54m:36s remains)
INFO - root - 2017-12-11 08:06:17.947559: step 31260, loss = 0.70, batch loss = 0.65 (15.5 examples/sec; 0.517 sec/batch; 43h:17m:06s remains)
INFO - root - 2017-12-11 08:06:23.227095: step 31270, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.570 sec/batch; 47h:43m:30s remains)
INFO - root - 2017-12-11 08:06:28.618826: step 31280, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:13m:18s remains)
INFO - root - 2017-12-11 08:06:33.982530: step 31290, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:34m:03s remains)
INFO - root - 2017-12-11 08:06:39.384544: step 31300, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 44h:34m:14s remains)
2017-12-11 08:06:39.949013: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13636915 0.13957094 0.14167847 0.14127372 0.13442819 0.12414333 0.11194042 0.10491939 0.10914467 0.13079494 0.16270393 0.19012754 0.20913154 0.21875006 0.21788764][0.089711748 0.094303966 0.09928339 0.0985968 0.090988748 0.080510549 0.070008948 0.065300159 0.073522866 0.099699855 0.1328758 0.15954661 0.17727847 0.18615872 0.18618158][0.033839833 0.036949947 0.041676212 0.043879285 0.042217545 0.038764555 0.034492869 0.03341008 0.040724792 0.05987839 0.080846488 0.094318382 0.10147136 0.10473356 0.1052717][0.011659247 0.00458976 0.0049849725 0.012383877 0.022895066 0.03319459 0.039072655 0.040367838 0.039977614 0.041064225 0.038037065 0.028458744 0.017985914 0.011490265 0.01037353][0.017075274 -0.0011046887 -0.0049780854 0.010573436 0.038005572 0.067851231 0.0894399 0.096619874 0.089137964 0.070059694 0.039985113 0.0046594697 -0.025289265 -0.042601157 -0.04640118][0.029306898 0.0097787194 0.0094185583 0.036956094 0.082274377 0.13242613 0.1715226 0.18777968 0.17721927 0.14288224 0.090966448 0.034128271 -0.012179211 -0.038496677 -0.044091426][0.028236585 0.018519251 0.029735766 0.070752725 0.13070929 0.19653569 0.24937727 0.273914 0.26355383 0.22167549 0.15820239 0.089657106 0.03493464 0.0050724451 0.0016408082][0.0081362845 0.010911469 0.035469122 0.086943455 0.15470318 0.22755732 0.28609264 0.31476247 0.30714071 0.267451 0.20682748 0.14129002 0.090709127 0.066317424 0.070850894][-0.02102882 -0.0093300864 0.023219865 0.0771193 0.14218907 0.20988815 0.26305225 0.28932205 0.28539279 0.25653911 0.21229075 0.1640657 0.12967493 0.11905194 0.13549136][-0.04154367 -0.028499693 0.0024794617 0.048345361 0.099506356 0.15070225 0.18950751 0.20849943 0.20842655 0.19513416 0.17447323 0.15137927 0.13960062 0.14713688 0.17614125][-0.046986621 -0.039263275 -0.01802934 0.012722302 0.044833146 0.076134235 0.099104241 0.11002623 0.11254174 0.11295936 0.11384458 0.1150187 0.12596394 0.15232508 0.19276881][-0.041995592 -0.041612916 -0.031993691 -0.016437011 -0.00075621606 0.014732295 0.025713177 0.029825045 0.032050196 0.039567254 0.053830631 0.07267388 0.10199382 0.14449406 0.1946121][-0.031744622 -0.036836054 -0.035742763 -0.030710394 -0.025392555 -0.019958135 -0.017107869 -0.019159703 -0.020416489 -0.012011307 0.0085614752 0.03929875 0.082721837 0.13856611 0.19788313][-0.022413133 -0.029957948 -0.032923479 -0.032817803 -0.03193095 -0.031570513 -0.033716727 -0.040880494 -0.046910033 -0.041069303 -0.018418808 0.02066515 0.075622216 0.14333582 0.21256477][-0.017378526 -0.023685807 -0.024887994 -0.022200543 -0.017869493 -0.015425565 -0.01723494 -0.026008463 -0.036142126 -0.035955127 -0.017423 0.022173891 0.080892555 0.15368992 0.22825752]]...]
INFO - root - 2017-12-11 08:06:45.236967: step 31310, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:33m:25s remains)
INFO - root - 2017-12-11 08:06:50.598231: step 31320, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 45h:03m:34s remains)
INFO - root - 2017-12-11 08:06:55.666263: step 31330, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:01m:32s remains)
INFO - root - 2017-12-11 08:07:00.972623: step 31340, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 44h:36m:54s remains)
INFO - root - 2017-12-11 08:07:06.390862: step 31350, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:33m:08s remains)
INFO - root - 2017-12-11 08:07:11.760898: step 31360, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 45h:18m:15s remains)
INFO - root - 2017-12-11 08:07:17.061753: step 31370, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 47h:54m:10s remains)
INFO - root - 2017-12-11 08:07:22.427211: step 31380, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 44h:33m:48s remains)
INFO - root - 2017-12-11 08:07:27.788669: step 31390, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 44h:06m:43s remains)
INFO - root - 2017-12-11 08:07:33.069170: step 31400, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:21m:21s remains)
2017-12-11 08:07:33.620440: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.02031807 0.061660267 0.1110901 0.16097084 0.19990346 0.22323969 0.23525672 0.23925523 0.23381776 0.22100987 0.20912088 0.21090136 0.24035722 0.30092448 0.37887192][0.018530535 0.064519092 0.12267703 0.18287234 0.23117973 0.2570959 0.26286092 0.2498097 0.21847475 0.18017365 0.15440744 0.16099478 0.21420677 0.31076175 0.42719677][0.016482437 0.0697188 0.14191762 0.21951133 0.28304586 0.31504688 0.315859 0.28221616 0.22015317 0.15149345 0.10725135 0.11521204 0.18732542 0.31290409 0.45784473][0.020594666 0.089950413 0.18784104 0.2958996 0.38694012 0.43560639 0.44014531 0.39040369 0.2972267 0.19364335 0.12338793 0.1216294 0.19856998 0.33918145 0.49973243][0.02936429 0.11904524 0.24828005 0.39356986 0.52023023 0.59480327 0.61296368 0.55595636 0.43595237 0.29565954 0.19093032 0.16442439 0.22650737 0.363111 0.52449334][0.034061495 0.14096043 0.29690361 0.47433126 0.63422769 0.73555344 0.77077073 0.71363455 0.57708091 0.40966195 0.27319041 0.21455382 0.24652612 0.36183709 0.50994378][0.02917885 0.14523041 0.31663156 0.51345724 0.69578183 0.81644964 0.8647629 0.81034368 0.66765207 0.48878109 0.33463937 0.25037116 0.25019014 0.3360002 0.46068943][0.014063248 0.12889121 0.30183566 0.50215131 0.69136149 0.81891412 0.87261248 0.82025671 0.67991191 0.50708592 0.35916322 0.27197635 0.25576082 0.31785673 0.41651031][-0.0083290106 0.095490925 0.2562252 0.44222084 0.61855775 0.73642731 0.78700149 0.73961818 0.61528593 0.47109586 0.35873827 0.30036044 0.29470509 0.3479903 0.42370215][-0.030901002 0.0556382 0.1943707 0.35106725 0.49603257 0.58920962 0.63133276 0.59600872 0.50366968 0.40816191 0.35391167 0.34711525 0.37009138 0.42421028 0.47974777][-0.053136002 0.013775719 0.12576091 0.24616438 0.350378 0.41160402 0.44362897 0.42640322 0.37616506 0.33766249 0.3465834 0.39366603 0.44538552 0.49799126 0.53215969][-0.077175155 -0.02979954 0.056839354 0.14417973 0.20976928 0.24005653 0.26186606 0.26198646 0.24966566 0.25798866 0.31506154 0.4001283 0.46793431 0.51123929 0.52289855][-0.098717265 -0.068250611 -0.0034706155 0.057683811 0.09319856 0.099706359 0.11216146 0.12317415 0.13529912 0.17062183 0.25092766 0.34904611 0.41567037 0.44281945 0.43401194][-0.11510014 -0.098939426 -0.055153843 -0.017034516 -0.0038031465 -0.012571678 -0.0085666124 0.0045906277 0.025347361 0.067168586 0.14537059 0.23254582 0.28421053 0.29462275 0.27324426][-0.13077334 -0.12741774 -0.10453014 -0.087864779 -0.091474183 -0.10926909 -0.11355001 -0.10663139 -0.091511741 -0.060487658 -0.0035252515 0.056871228 0.087559856 0.086093485 0.063040733]]...]
INFO - root - 2017-12-11 08:07:38.917310: step 31410, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 44h:13m:27s remains)
INFO - root - 2017-12-11 08:07:44.287517: step 31420, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:02m:11s remains)
INFO - root - 2017-12-11 08:07:49.319200: step 31430, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.513 sec/batch; 42h:56m:09s remains)
INFO - root - 2017-12-11 08:07:54.640113: step 31440, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:42m:36s remains)
INFO - root - 2017-12-11 08:07:59.975363: step 31450, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 43h:41m:22s remains)
INFO - root - 2017-12-11 08:08:05.295467: step 31460, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:43m:47s remains)
INFO - root - 2017-12-11 08:08:10.681845: step 31470, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:43m:27s remains)
INFO - root - 2017-12-11 08:08:16.071642: step 31480, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.558 sec/batch; 46h:39m:25s remains)
INFO - root - 2017-12-11 08:08:21.350346: step 31490, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 47h:48m:41s remains)
INFO - root - 2017-12-11 08:08:26.711869: step 31500, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:53m:27s remains)
2017-12-11 08:08:27.264666: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018170884 0.0083597414 0.016980393 0.049832843 0.10645419 0.189142 0.28261098 0.36761609 0.44404659 0.51643115 0.58068275 0.62395412 0.64324659 0.64601517 0.62727165][0.013642243 -0.014930955 -0.032915164 -0.032288682 -0.008028199 0.049195483 0.13042307 0.21699297 0.3044312 0.39469463 0.48343807 0.55653191 0.60797673 0.6424529 0.6555236][0.037152804 0.005931458 -0.027293161 -0.054120395 -0.064672083 -0.043072894 0.010712708 0.0819677 0.16509672 0.25877962 0.35602686 0.44218981 0.51106972 0.56572396 0.59998232][0.10447367 0.092673846 0.064633131 0.026830293 -0.0091579286 -0.022544146 -0.0040260777 0.038166769 0.099336348 0.17558092 0.25654548 0.32652462 0.38158754 0.42760655 0.45933512][0.21430388 0.24212728 0.24046604 0.2120446 0.16605848 0.12639968 0.11064221 0.11732429 0.14309976 0.18180761 0.22074789 0.24425997 0.25325239 0.2605626 0.26606375][0.34219927 0.41881087 0.45523006 0.45004252 0.40835413 0.35518876 0.31446534 0.28988716 0.27837354 0.26981813 0.25066367 0.20893292 0.15431647 0.1081575 0.076701447][0.44780368 0.56397986 0.63218474 0.64923471 0.61752081 0.5612452 0.50729191 0.46246848 0.42295897 0.3735112 0.30063865 0.19875349 0.088077329 -0.0023329698 -0.061981868][0.496815 0.62502688 0.70161796 0.72527 0.69844133 0.64268887 0.58334434 0.52826905 0.47376573 0.40176651 0.29979482 0.16753848 0.03178151 -0.074912988 -0.14060076][0.48492128 0.59068018 0.64537907 0.65387881 0.620706 0.56299549 0.50067955 0.44072005 0.38175413 0.30721131 0.20670894 0.082240865 -0.039942551 -0.13057195 -0.17943723][0.44453493 0.50135291 0.50949484 0.48342922 0.43162283 0.36707649 0.3022773 0.24125399 0.18576756 0.12457909 0.049414925 -0.03861941 -0.11970291 -0.17287281 -0.19228816][0.39892983 0.39745295 0.35022357 0.28258362 0.20831233 0.13817567 0.076637141 0.022607388 -0.020851433 -0.059026591 -0.098885469 -0.14196107 -0.17658302 -0.19083419 -0.18354611][0.34506702 0.29536191 0.20475002 0.10645345 0.018611871 -0.048735198 -0.097155049 -0.13356836 -0.157154 -0.17028406 -0.17911924 -0.18693833 -0.1883302 -0.17796955 -0.15732732][0.25810957 0.18253486 0.075628355 -0.02886424 -0.11171778 -0.16370068 -0.18957824 -0.20088609 -0.20140135 -0.19407056 -0.18399277 -0.1739465 -0.1611301 -0.14335741 -0.12262751][0.1284014 0.051254395 -0.042913593 -0.12765713 -0.18672523 -0.21321699 -0.21360265 -0.20125824 -0.18344183 -0.16417997 -0.14708047 -0.13319173 -0.1198021 -0.10525674 -0.090840839][-0.014660008 -0.074211277 -0.13541394 -0.18467109 -0.21153326 -0.21261403 -0.19434534 -0.16847523 -0.1426333 -0.12079614 -0.10463358 -0.093651637 -0.084899314 -0.076473 -0.068701178]]...]
INFO - root - 2017-12-11 08:08:32.643873: step 31510, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:27m:30s remains)
INFO - root - 2017-12-11 08:08:38.036835: step 31520, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 43h:24m:07s remains)
INFO - root - 2017-12-11 08:08:43.476408: step 31530, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:17m:40s remains)
INFO - root - 2017-12-11 08:08:48.507940: step 31540, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.520 sec/batch; 43h:30m:18s remains)
INFO - root - 2017-12-11 08:08:53.881394: step 31550, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:09m:22s remains)
INFO - root - 2017-12-11 08:08:59.185855: step 31560, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 44h:54m:40s remains)
INFO - root - 2017-12-11 08:09:04.609982: step 31570, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 44h:12m:00s remains)
INFO - root - 2017-12-11 08:09:10.030464: step 31580, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:11m:14s remains)
INFO - root - 2017-12-11 08:09:15.444412: step 31590, loss = 0.70, batch loss = 0.65 (13.9 examples/sec; 0.573 sec/batch; 47h:56m:10s remains)
INFO - root - 2017-12-11 08:09:20.788258: step 31600, loss = 0.67, batch loss = 0.62 (15.5 examples/sec; 0.516 sec/batch; 43h:08m:07s remains)
2017-12-11 08:09:21.402539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012609013 0.0013249971 -0.0024558564 -0.00064299395 0.0040182648 0.009530575 0.012196736 0.013470562 0.024562966 0.050491184 0.089710161 0.13026546 0.16078182 0.17991666 0.18703754][0.041133475 0.024679566 0.018437397 0.020980831 0.026950765 0.035047989 0.04304947 0.054126538 0.079733863 0.12476197 0.18214175 0.23110905 0.25712946 0.26341161 0.25561044][0.077392496 0.065401062 0.066843964 0.0804372 0.096717887 0.11294805 0.12809543 0.14656049 0.18143976 0.23687945 0.3007907 0.34649962 0.35782868 0.343099 0.31451955][0.11541185 0.1193914 0.14105937 0.17750435 0.21380423 0.24353462 0.26590848 0.2866219 0.3190071 0.36590427 0.41438374 0.43894485 0.42553857 0.38738176 0.34120157][0.14833607 0.17670064 0.22649755 0.29211989 0.35328093 0.39889839 0.42688033 0.44324797 0.46002388 0.47902384 0.49122915 0.47943842 0.43567714 0.37785873 0.32298928][0.17171755 0.2275954 0.3061718 0.3984111 0.48050967 0.53735363 0.5652498 0.5691554 0.55994463 0.54020935 0.507907 0.45697898 0.38669604 0.31840473 0.26556858][0.18840875 0.2675584 0.36698246 0.47373948 0.56401682 0.62126273 0.63993728 0.62321895 0.5823403 0.52418226 0.45400238 0.37474743 0.290915 0.22495915 0.18503775][0.19913484 0.28953764 0.39521536 0.49946025 0.5815261 0.62714362 0.62924647 0.58901811 0.51902747 0.43260959 0.34088323 0.25125271 0.17006323 0.1182739 0.099935278][0.19409889 0.2787385 0.37315762 0.45820895 0.517855 0.54384047 0.529371 0.47316009 0.38758475 0.29089534 0.19786012 0.11651469 0.052612446 0.023890214 0.032447565][0.1702483 0.2346659 0.30379239 0.35827595 0.38707346 0.39023256 0.36369458 0.30442786 0.22220847 0.13539378 0.059496362 0.0012476578 -0.034544565 -0.034367159 0.0026451952][0.13932835 0.17599741 0.21342932 0.23515344 0.23401028 0.21708934 0.18434252 0.13265184 0.067720287 0.0038577653 -0.044800039 -0.072904468 -0.076355815 -0.046206277 0.016415494][0.10775924 0.11740295 0.12632419 0.12412656 0.10569935 0.080356561 0.050559044 0.013210829 -0.029729551 -0.068892471 -0.091281317 -0.091297612 -0.065874577 -0.010329764 0.070377856][0.076292485 0.066351295 0.0565447 0.04373455 0.023304349 0.0020211143 -0.017486174 -0.038523592 -0.060857441 -0.079149105 -0.081263475 -0.060042895 -0.013701981 0.058421448 0.14636314][0.050569795 0.03215085 0.015519287 0.0028943806 -0.010100108 -0.021654729 -0.030333741 -0.03882318 -0.045705069 -0.047852892 -0.035166875 0.00064723351 0.059474114 0.13814569 0.22234406][0.036371354 0.020545518 0.0074932254 0.00091592222 -0.0024741709 -0.0048460658 -0.0061861463 -0.0076850168 -0.0053493427 0.0032977639 0.025089383 0.067839622 0.12932853 0.20427942 0.27675232]]...]
INFO - root - 2017-12-11 08:09:26.904891: step 31610, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:38m:09s remains)
INFO - root - 2017-12-11 08:09:32.280882: step 31620, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.525 sec/batch; 43h:54m:27s remains)
INFO - root - 2017-12-11 08:09:37.628931: step 31630, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 44h:53m:44s remains)
INFO - root - 2017-12-11 08:09:42.711281: step 31640, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.542 sec/batch; 45h:19m:16s remains)
INFO - root - 2017-12-11 08:09:48.024122: step 31650, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 43h:49m:35s remains)
INFO - root - 2017-12-11 08:09:53.356562: step 31660, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 44h:56m:07s remains)
INFO - root - 2017-12-11 08:09:58.768371: step 31670, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 44h:06m:24s remains)
INFO - root - 2017-12-11 08:10:04.144327: step 31680, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:27m:28s remains)
INFO - root - 2017-12-11 08:10:09.408153: step 31690, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:10m:25s remains)
INFO - root - 2017-12-11 08:10:14.738275: step 31700, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 44h:42m:55s remains)
2017-12-11 08:10:15.339780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22064541 0.24746159 0.28191721 0.31871846 0.34328198 0.35393378 0.35132962 0.34475 0.3289459 0.30608383 0.28070492 0.24892128 0.21052702 0.16379432 0.10839714][0.29004627 0.3268221 0.36042589 0.38928565 0.39984462 0.39255831 0.37631106 0.361997 0.34254402 0.31985125 0.29960495 0.27635676 0.24218088 0.19230314 0.12916049][0.34682402 0.39196184 0.42549789 0.44119105 0.42796138 0.3933841 0.35858673 0.34113365 0.33255333 0.32703608 0.32326367 0.3109436 0.27767688 0.21905109 0.14442743][0.37588358 0.4247131 0.45721292 0.46123335 0.42647788 0.36586294 0.31422034 0.29896092 0.30999807 0.32958895 0.34466627 0.33996809 0.30274493 0.23258491 0.14619181][0.39791211 0.44846031 0.47824684 0.47253406 0.42195764 0.34017625 0.27055311 0.24892141 0.26685366 0.29902747 0.32430303 0.32470119 0.28820625 0.21710359 0.12990554][0.44802162 0.50071275 0.52743763 0.51291162 0.45034385 0.35310566 0.26586169 0.22797297 0.23383567 0.25847504 0.2808547 0.28355023 0.25469872 0.19465917 0.11713796][0.51499426 0.56649464 0.58660316 0.5620358 0.4917931 0.39226016 0.30297098 0.25897816 0.25169644 0.25970069 0.26756239 0.2628713 0.236725 0.18584469 0.11645878][0.54899949 0.59408534 0.60629725 0.57383484 0.503372 0.41676942 0.34477723 0.31160921 0.29896668 0.28993076 0.27700886 0.25642487 0.22460206 0.17614096 0.1112164][0.51340723 0.55156 0.55813915 0.52279532 0.45958164 0.3931731 0.34376243 0.32361415 0.3063401 0.28310621 0.25404012 0.22090989 0.18341565 0.13687338 0.079190038][0.38796389 0.41952765 0.4244504 0.39334482 0.34401703 0.29893836 0.26753008 0.253252 0.23071684 0.20169406 0.1712873 0.13891926 0.10377835 0.063658737 0.018245835][0.20555228 0.23004921 0.23544483 0.2145445 0.18473597 0.16301717 0.149728 0.14098611 0.11688965 0.089423731 0.066909358 0.042288922 0.012632127 -0.020715483 -0.053271785][0.024947539 0.042722587 0.048783943 0.038257822 0.027760889 0.028468572 0.033809684 0.033670384 0.013101636 -0.0092227273 -0.023614034 -0.041656412 -0.066322029 -0.092958577 -0.11272839][-0.083679169 -0.073591009 -0.068274476 -0.072673008 -0.071025044 -0.057161968 -0.041428272 -0.035099007 -0.048973463 -0.064096838 -0.07277137 -0.086602025 -0.10655448 -0.12645014 -0.13570863][-0.11931388 -0.11554217 -0.11094599 -0.11219938 -0.10711288 -0.092431322 -0.077277482 -0.0698462 -0.076986894 -0.084931388 -0.089497276 -0.099167325 -0.11307478 -0.1260097 -0.1290786][-0.13585696 -0.13671429 -0.13189568 -0.13003312 -0.12454431 -0.11387328 -0.10285098 -0.096001625 -0.0972417 -0.099744193 -0.10148546 -0.10646162 -0.11303878 -0.11851302 -0.11797705]]...]
INFO - root - 2017-12-11 08:10:20.661230: step 31710, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:22m:00s remains)
INFO - root - 2017-12-11 08:10:26.049039: step 31720, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:20m:35s remains)
INFO - root - 2017-12-11 08:10:31.484937: step 31730, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 45h:28m:41s remains)
INFO - root - 2017-12-11 08:10:36.611268: step 31740, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 46h:37m:10s remains)
INFO - root - 2017-12-11 08:10:41.982938: step 31750, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:03m:14s remains)
INFO - root - 2017-12-11 08:10:47.298414: step 31760, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 43h:03m:45s remains)
INFO - root - 2017-12-11 08:10:52.685814: step 31770, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 45h:37m:08s remains)
INFO - root - 2017-12-11 08:10:58.058792: step 31780, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:10m:57s remains)
INFO - root - 2017-12-11 08:11:03.319799: step 31790, loss = 0.67, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 43h:53m:54s remains)
INFO - root - 2017-12-11 08:11:08.739628: step 31800, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 43h:02m:55s remains)
2017-12-11 08:11:09.307248: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16621482 0.24124198 0.29162544 0.31829658 0.33395833 0.34930941 0.36737955 0.37805396 0.36507753 0.32316536 0.26631102 0.22862947 0.23778337 0.30163142 0.40008008][0.16095832 0.24082878 0.30310193 0.34900081 0.38742635 0.4231874 0.45499074 0.4661271 0.438337 0.3711662 0.28503826 0.21768951 0.19575292 0.22726852 0.29569674][0.14814274 0.22870044 0.30210438 0.37099695 0.43825874 0.50036246 0.54855448 0.55964261 0.51305085 0.415061 0.29599237 0.19520609 0.13571253 0.12351234 0.14690816][0.13299796 0.21342066 0.2989997 0.39417827 0.49629679 0.59164643 0.66073841 0.67216873 0.60350418 0.46899074 0.3132309 0.18083854 0.090814978 0.043887239 0.03083368][0.11567704 0.19433291 0.289375 0.40801558 0.54446584 0.67606068 0.7716319 0.78914446 0.70301348 0.53502178 0.34663078 0.19076985 0.084084965 0.020759828 -0.0090226289][0.096844979 0.17176664 0.27165505 0.40810022 0.5755043 0.74457365 0.87244862 0.90535712 0.81280881 0.62137967 0.40804178 0.23685001 0.12570098 0.062819295 0.034255754][0.077647157 0.14677463 0.24602491 0.39295846 0.58537769 0.79111272 0.95683962 1.0167769 0.93258286 0.73059881 0.49874672 0.31475747 0.203154 0.14853419 0.1306289][0.05927242 0.12141448 0.21496379 0.36324868 0.56874305 0.80118752 1.0021092 1.0967463 1.0362164 0.83847368 0.59482139 0.39724275 0.28391311 0.24009037 0.23715439][0.044690598 0.10019153 0.18529503 0.32626089 0.52802336 0.76510161 0.98088729 1.1003729 1.0684227 0.89001614 0.64939755 0.44671395 0.33560422 0.30653706 0.32220897][0.035104465 0.085804641 0.16265544 0.29081354 0.473551 0.69031709 0.891533 1.0112548 0.99736542 0.84566116 0.62787867 0.44058129 0.34381774 0.33350721 0.3675873][0.036517825 0.084925205 0.15392365 0.26452741 0.41627291 0.59286606 0.75536233 0.8526153 0.84329218 0.72014678 0.5413081 0.38946784 0.32009897 0.33002326 0.37749368][0.049590152 0.097383171 0.15733816 0.24438886 0.35520533 0.47857997 0.58942723 0.65372705 0.64323986 0.5508045 0.42081729 0.3162646 0.28060389 0.30873552 0.36269102][0.077005744 0.1280385 0.18018523 0.24118996 0.30719259 0.3732239 0.42961177 0.45927835 0.4459399 0.38331723 0.30213264 0.24492055 0.2396213 0.27927926 0.33280519][0.11791498 0.17723577 0.22540565 0.26301262 0.28806818 0.30191684 0.30980921 0.30940413 0.29491451 0.25968429 0.22087038 0.20284328 0.21862626 0.26016152 0.30498865][0.1687379 0.24107414 0.2901662 0.31007379 0.302445 0.27487153 0.24590591 0.22438972 0.21176793 0.19883031 0.1895525 0.19540794 0.21876647 0.25174513 0.2807034]]...]
INFO - root - 2017-12-11 08:11:14.669378: step 31810, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.528 sec/batch; 44h:04m:49s remains)
INFO - root - 2017-12-11 08:11:19.936207: step 31820, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:14m:51s remains)
INFO - root - 2017-12-11 08:11:25.317353: step 31830, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:51m:57s remains)
INFO - root - 2017-12-11 08:11:30.449031: step 31840, loss = 0.70, batch loss = 0.64 (22.3 examples/sec; 0.358 sec/batch; 29h:56m:02s remains)
INFO - root - 2017-12-11 08:11:35.764940: step 31850, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:02m:44s remains)
INFO - root - 2017-12-11 08:11:41.146696: step 31860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 44h:41m:19s remains)
INFO - root - 2017-12-11 08:11:46.400746: step 31870, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 44h:00m:35s remains)
INFO - root - 2017-12-11 08:11:51.743486: step 31880, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:43m:06s remains)
INFO - root - 2017-12-11 08:11:57.085355: step 31890, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:59m:29s remains)
INFO - root - 2017-12-11 08:12:02.346025: step 31900, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 43h:10m:51s remains)
2017-12-11 08:12:02.903956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030253259 0.029032152 0.07723517 0.12405434 0.15744978 0.17716025 0.18554081 0.19163164 0.19790892 0.19590776 0.17742701 0.13890867 0.085736953 0.027476065 -0.020008748][-0.022648511 0.0016427422 0.04790381 0.099944033 0.14437909 0.17733075 0.19718443 0.20900783 0.2149114 0.20624763 0.17656654 0.12535027 0.062536463 -0.0004079857 -0.049184959][-0.044601951 -0.030507447 0.011703511 0.0690555 0.12643613 0.17498769 0.20896168 0.22856186 0.23279607 0.21368562 0.16777459 0.10020317 0.025868677 -0.041378662 -0.08881148][-0.059667032 -0.05472111 -0.015204349 0.049499605 0.1218411 0.18711314 0.2350142 0.26114663 0.26074496 0.22733906 0.16204175 0.076671526 -0.0092702927 -0.079765707 -0.12368037][-0.05620496 -0.0571783 -0.017776098 0.055785231 0.14300153 0.22364269 0.28302211 0.31299576 0.3050383 0.255154 0.17011021 0.068496488 -0.026680276 -0.098238617 -0.13636601][-0.031490654 -0.030964132 0.012674699 0.096170112 0.19604957 0.28836465 0.35537148 0.38641196 0.36930814 0.30283651 0.19932616 0.083396725 -0.019488968 -0.09137997 -0.12302331][0.01117273 0.016998509 0.064731129 0.15334305 0.25777572 0.35344329 0.4220148 0.45173475 0.42698425 0.34890583 0.23399422 0.11085367 0.0053729555 -0.064526871 -0.089701176][0.065180928 0.0759682 0.12319103 0.20785575 0.30596495 0.39513245 0.45885891 0.48566481 0.45725802 0.37605625 0.26068631 0.14121112 0.041667972 -0.021784989 -0.040594567][0.12285577 0.13655868 0.17690589 0.24815664 0.33022162 0.40496904 0.45922625 0.48235452 0.45509258 0.37964749 0.27446285 0.16860397 0.082825489 0.030115793 0.017506311][0.16844672 0.18456782 0.21487045 0.26732069 0.3275483 0.38281548 0.4238745 0.44167683 0.41819438 0.35485098 0.26772109 0.18203467 0.11464556 0.075150713 0.068239726][0.18600345 0.19958149 0.21508223 0.24360391 0.27743152 0.309295 0.33376619 0.34429896 0.32639936 0.28058234 0.21889073 0.16018911 0.11651646 0.093826011 0.093985051][0.17169273 0.17670655 0.17410393 0.17686525 0.18404427 0.19322082 0.20194724 0.20600322 0.19534604 0.16961777 0.13682172 0.10896793 0.092835926 0.090659648 0.10133594][0.12925892 0.12587328 0.1098441 0.093949579 0.0822213 0.075839035 0.074497223 0.075420447 0.072295971 0.064310364 0.056602787 0.055638511 0.063984662 0.08114022 0.10383764][0.064754248 0.056218211 0.035377633 0.012259468 -0.0073349238 -0.020322707 -0.025571313 -0.024597211 -0.021124097 -0.015824866 -0.0051231943 0.013421371 0.039557893 0.071279563 0.10422061][-0.0026018983 -0.011609551 -0.029085759 -0.0494131 -0.067509606 -0.080055863 -0.08490508 -0.082006961 -0.073413745 -0.059595056 -0.038102083 -0.0091236355 0.025465246 0.063344613 0.1002871]]...]
INFO - root - 2017-12-11 08:12:08.303536: step 31910, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:47m:57s remains)
INFO - root - 2017-12-11 08:12:13.598234: step 31920, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 44h:04m:31s remains)
INFO - root - 2017-12-11 08:12:18.938166: step 31930, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 44h:03m:26s remains)
INFO - root - 2017-12-11 08:12:24.332901: step 31940, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:25m:03s remains)
INFO - root - 2017-12-11 08:12:29.566346: step 31950, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 43h:31m:29s remains)
INFO - root - 2017-12-11 08:12:34.938944: step 31960, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 44h:54m:21s remains)
INFO - root - 2017-12-11 08:12:40.182331: step 31970, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:16m:09s remains)
INFO - root - 2017-12-11 08:12:45.622690: step 31980, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 43h:54m:50s remains)
INFO - root - 2017-12-11 08:12:50.957520: step 31990, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 44h:19m:13s remains)
INFO - root - 2017-12-11 08:12:56.301428: step 32000, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:00m:41s remains)
2017-12-11 08:12:56.846376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.047012087 -0.023437258 0.0077603459 0.046782363 0.096547388 0.15709397 0.21887244 0.26806167 0.29494819 0.29244393 0.2626847 0.21829653 0.17189264 0.12788598 0.081567049][-0.056782536 -0.03372417 0.00050166325 0.049797058 0.1194822 0.2078875 0.29897591 0.37041238 0.40763196 0.40161636 0.35746861 0.29419777 0.22970894 0.17089005 0.11141019][-0.06559936 -0.045660816 -0.011627426 0.044236511 0.13055079 0.24358842 0.36141849 0.4533338 0.49968043 0.48974073 0.43125695 0.34872705 0.26487845 0.19018136 0.11886592][-0.072182558 -0.05396558 -0.020119248 0.039889611 0.13759485 0.26701126 0.40254775 0.50768113 0.55876029 0.54449075 0.47440416 0.37624997 0.27558437 0.18638931 0.10581296][-0.06836433 -0.046603493 -0.0078210449 0.058411885 0.16430061 0.30084306 0.44155627 0.54797328 0.59558749 0.57482183 0.49594364 0.386926 0.27304 0.1709408 0.08270786][-0.05071944 -0.018615205 0.034514155 0.11504895 0.23144601 0.36937916 0.50293839 0.59636521 0.62940669 0.59745842 0.51029795 0.39314252 0.26821658 0.15397492 0.058654949][-0.02288443 0.022855416 0.097069427 0.19947918 0.32979065 0.46552536 0.58182937 0.64961451 0.65712243 0.60791975 0.51123536 0.38806492 0.25636455 0.1349277 0.037106667][0.0080662239 0.063160092 0.1554728 0.27764165 0.41842213 0.54637259 0.63830137 0.67378372 0.65033782 0.58060336 0.47483751 0.350967 0.22286516 0.10715401 0.01788904][0.034859255 0.088375807 0.18513048 0.312745 0.45147091 0.5637536 0.62872905 0.63414848 0.58414406 0.4986451 0.39081508 0.27759326 0.16925187 0.076518595 0.0082920687][0.054275315 0.092547022 0.17383271 0.28471091 0.40269995 0.48982272 0.52853531 0.51263744 0.44889256 0.36168745 0.26822427 0.18481116 0.11644632 0.063813947 0.025717614][0.064780444 0.077306047 0.12487517 0.19778939 0.27722773 0.33033028 0.34364346 0.31522429 0.25287905 0.18335249 0.12718467 0.097369246 0.089480579 0.090211958 0.085543826][0.06492231 0.050733626 0.0589085 0.086332157 0.12172241 0.14041178 0.13322885 0.10091757 0.052984066 0.017068138 0.01488608 0.052493129 0.11247729 0.16649026 0.1883456][0.054908168 0.025810761 0.0072376425 0.0017588311 0.0053137937 0.0014129945 -0.015515134 -0.043295238 -0.069875032 -0.068458788 -0.018803064 0.076497957 0.18801655 0.27616909 0.30734849][0.03649627 0.008843598 -0.014919475 -0.030251794 -0.037390318 -0.046722636 -0.061210223 -0.07855083 -0.085048325 -0.056081414 0.025691003 0.15044236 0.28159243 0.3755464 0.39812985][0.010859818 -0.0053755189 -0.018762661 -0.026213292 -0.027563859 -0.0295152 -0.033732064 -0.039002649 -0.032531746 0.0072907489 0.095215932 0.21802263 0.33836508 0.41620007 0.42273787]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:13:02.191313: step 32010, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.576 sec/batch; 48h:03m:07s remains)
INFO - root - 2017-12-11 08:13:07.422573: step 32020, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:33m:24s remains)
INFO - root - 2017-12-11 08:13:12.814363: step 32030, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 45h:00m:31s remains)
INFO - root - 2017-12-11 08:13:18.143369: step 32040, loss = 0.72, batch loss = 0.66 (16.1 examples/sec; 0.497 sec/batch; 41h:31m:12s remains)
INFO - root - 2017-12-11 08:13:23.199773: step 32050, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 43h:49m:22s remains)
INFO - root - 2017-12-11 08:13:28.593538: step 32060, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 46h:34m:32s remains)
INFO - root - 2017-12-11 08:13:33.882056: step 32070, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:40m:41s remains)
INFO - root - 2017-12-11 08:13:39.203107: step 32080, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:38m:52s remains)
INFO - root - 2017-12-11 08:13:44.566548: step 32090, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:27m:30s remains)
INFO - root - 2017-12-11 08:13:49.941441: step 32100, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:47m:40s remains)
2017-12-11 08:13:50.528933: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.097861849 0.19049776 0.26651242 0.31783402 0.34423035 0.34696788 0.33069509 0.30729282 0.28178772 0.24823363 0.20763049 0.16310212 0.11812691 0.077834234 0.0472696][0.11857692 0.21830568 0.29841125 0.35073781 0.37875807 0.3860541 0.3756049 0.36016065 0.34478688 0.31997055 0.28295931 0.23660305 0.18649675 0.13905334 0.09920609][0.14310575 0.24316071 0.31911919 0.36323249 0.38610733 0.39620516 0.39500079 0.39342776 0.39285496 0.38019234 0.35019135 0.30534127 0.2529147 0.19988701 0.15172696][0.16872959 0.27023837 0.34399825 0.38071322 0.39764461 0.40810171 0.41133004 0.41578358 0.420903 0.413547 0.38737771 0.34438953 0.29298252 0.23948638 0.18915091][0.18948515 0.29736415 0.37615997 0.41236439 0.42648 0.43502098 0.43361628 0.42950734 0.42540902 0.41209096 0.38308907 0.34044948 0.29398018 0.24772888 0.20429704][0.19915965 0.31889075 0.41116476 0.45654428 0.47369164 0.47975826 0.46563661 0.43906161 0.41236934 0.38339114 0.34567142 0.30204138 0.26473147 0.23409516 0.20612255][0.19367538 0.32505184 0.43351355 0.49523181 0.52235371 0.52924961 0.50025511 0.44486365 0.38967758 0.34149289 0.29396373 0.25082636 0.22694337 0.21764256 0.20903385][0.17718193 0.31445527 0.43393132 0.51116472 0.55021781 0.56046206 0.52016282 0.4405472 0.36160445 0.29779688 0.24334635 0.20333837 0.19472402 0.20730814 0.21650086][0.14704378 0.28183985 0.40304 0.48821568 0.5350886 0.54792362 0.50215328 0.40926814 0.31647602 0.24263871 0.18423004 0.14882867 0.15458684 0.18635716 0.21050164][0.093020327 0.21492952 0.32783979 0.41155559 0.45964146 0.4719789 0.42553568 0.33104518 0.23483759 0.15738991 0.099248156 0.070306212 0.087772615 0.13348921 0.16940705][0.021838598 0.11939603 0.2126203 0.28404072 0.32585868 0.335099 0.29334664 0.20918231 0.12078039 0.047787294 -0.0044825678 -0.025144339 -0.0003642464 0.051580295 0.094629318][-0.047821268 0.018161722 0.082972467 0.13258643 0.16083945 0.16455919 0.13188499 0.068415269 -0.00036242846 -0.058871251 -0.099207312 -0.11111599 -0.085172385 -0.036410838 0.006727959][-0.099229239 -0.062541418 -0.024764085 0.002933119 0.016699867 0.014666147 -0.0085085286 -0.049510416 -0.0948396 -0.1344077 -0.16095728 -0.16640624 -0.14534344 -0.10747591 -0.071168631][-0.12255253 -0.10609862 -0.086158164 -0.071642965 -0.065630466 -0.069392368 -0.084305786 -0.10829201 -0.13529322 -0.15940511 -0.17548053 -0.1782202 -0.16499707 -0.14066164 -0.11482187][-0.11314641 -0.107549 -0.0967497 -0.087094262 -0.081799977 -0.082661994 -0.090182856 -0.10320283 -0.11894748 -0.13375525 -0.14416461 -0.14710759 -0.14137833 -0.12876558 -0.11333011]]...]
INFO - root - 2017-12-11 08:13:55.870507: step 32110, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:23m:07s remains)
INFO - root - 2017-12-11 08:14:01.309110: step 32120, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 46h:38m:20s remains)
INFO - root - 2017-12-11 08:14:06.689566: step 32130, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 44h:03m:30s remains)
INFO - root - 2017-12-11 08:14:12.137023: step 32140, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 44h:30m:20s remains)
INFO - root - 2017-12-11 08:14:17.143274: step 32150, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.525 sec/batch; 43h:45m:46s remains)
INFO - root - 2017-12-11 08:14:22.475298: step 32160, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:09m:54s remains)
INFO - root - 2017-12-11 08:14:27.808690: step 32170, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:00m:11s remains)
INFO - root - 2017-12-11 08:14:33.198803: step 32180, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:56m:15s remains)
INFO - root - 2017-12-11 08:14:38.480219: step 32190, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 42h:52m:05s remains)
INFO - root - 2017-12-11 08:14:43.731973: step 32200, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:22m:40s remains)
2017-12-11 08:14:44.277414: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33679333 0.3249259 0.27730495 0.21286069 0.16412193 0.14811614 0.14948234 0.15783641 0.17449245 0.20051512 0.23226412 0.259618 0.2764228 0.2711316 0.2279778][0.35882393 0.34476385 0.29772964 0.2399698 0.19804366 0.1829375 0.17872778 0.17648108 0.17977771 0.19471812 0.22261912 0.25139534 0.26793218 0.258835 0.21222685][0.34045321 0.3254092 0.2881549 0.25165325 0.22686256 0.21656896 0.2086338 0.1963045 0.18327253 0.18063602 0.19866151 0.22506015 0.23848741 0.22334038 0.17448439][0.30615193 0.29080209 0.26666918 0.25496835 0.25175595 0.25241894 0.25057986 0.23949914 0.21797973 0.19740896 0.1960333 0.20778273 0.20702492 0.17902705 0.12662867][0.27857214 0.26993686 0.26221389 0.27422565 0.29297796 0.30902779 0.31944722 0.31663918 0.29253885 0.25476551 0.22670512 0.21358629 0.19033268 0.14446756 0.0859776][0.26263827 0.26381594 0.27041134 0.30120909 0.33999845 0.37489125 0.40322873 0.41498026 0.39530408 0.3466976 0.29491198 0.25586155 0.20776765 0.14087065 0.070533656][0.25675765 0.26494622 0.27819517 0.31795216 0.369068 0.420649 0.46977982 0.5026896 0.49809688 0.45108443 0.38550672 0.32474497 0.25282484 0.16446562 0.0780846][0.26142788 0.27540988 0.29029435 0.32855818 0.37868094 0.43453911 0.49540749 0.54713124 0.56272352 0.52775246 0.45998979 0.38567752 0.29525441 0.18981786 0.08981435][0.28236556 0.30625746 0.32381392 0.35791627 0.40097886 0.4510881 0.50902337 0.56449622 0.59003347 0.56479651 0.50015968 0.42017776 0.31906804 0.2023401 0.092550613][0.31353071 0.34362167 0.36038598 0.38843185 0.42335352 0.46499714 0.5118975 0.55855477 0.58259517 0.56333148 0.50644487 0.43012288 0.32896456 0.20879728 0.09305416][0.33867925 0.36635548 0.3737455 0.38889676 0.41070485 0.44119933 0.47579527 0.51217383 0.53369069 0.52388942 0.48308468 0.42067474 0.3298384 0.21533583 0.099745519][0.34443703 0.36771178 0.36539307 0.3666862 0.37309965 0.39017639 0.41413578 0.44482908 0.46850678 0.47284842 0.45308909 0.40913376 0.33207637 0.22606239 0.11291306][0.32765248 0.34653884 0.34150141 0.336807 0.3355096 0.3454603 0.36522514 0.39593545 0.42544809 0.44400984 0.44223022 0.41323087 0.3455067 0.24268718 0.12821963][0.29684791 0.30978572 0.30776587 0.30721515 0.30948982 0.32134846 0.34290695 0.37559095 0.40865603 0.43441394 0.44068417 0.4183481 0.35422933 0.25160134 0.1356363][0.24283381 0.24861775 0.24990514 0.25705737 0.26778907 0.28599775 0.31249368 0.34703621 0.37920874 0.40420184 0.4096289 0.38751265 0.32580075 0.2286915 0.12016625]]...]
INFO - root - 2017-12-11 08:14:49.593941: step 32210, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:18m:04s remains)
INFO - root - 2017-12-11 08:14:55.034538: step 32220, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.550 sec/batch; 45h:53m:13s remains)
INFO - root - 2017-12-11 08:15:00.377746: step 32230, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:23m:34s remains)
INFO - root - 2017-12-11 08:15:05.791345: step 32240, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 47h:19m:22s remains)
INFO - root - 2017-12-11 08:15:11.078159: step 32250, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 43h:37m:17s remains)
INFO - root - 2017-12-11 08:15:16.237218: step 32260, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 45h:52m:47s remains)
INFO - root - 2017-12-11 08:15:21.759018: step 32270, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 45h:51m:19s remains)
INFO - root - 2017-12-11 08:15:27.072420: step 32280, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 43h:11m:05s remains)
INFO - root - 2017-12-11 08:15:32.409896: step 32290, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:39m:31s remains)
INFO - root - 2017-12-11 08:15:37.836240: step 32300, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:44m:05s remains)
2017-12-11 08:15:38.385861: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35656789 0.2425141 0.15858212 0.13284263 0.14920996 0.18796018 0.23314875 0.26938269 0.28187147 0.26462 0.21704811 0.15197924 0.084342763 0.033135355 0.010541794][0.36117435 0.260629 0.19721912 0.19560847 0.23679622 0.29764831 0.35916054 0.40385184 0.41688558 0.3924489 0.32745197 0.23616236 0.13940369 0.063723885 0.024367921][0.32761711 0.2623702 0.23967268 0.27391365 0.34484535 0.42809677 0.50198805 0.54768682 0.55378932 0.51770049 0.43473557 0.31819141 0.19303627 0.092945158 0.036324281][0.28210622 0.26651511 0.29584837 0.37018588 0.46845663 0.56723779 0.644328 0.68133861 0.67168862 0.61714828 0.51438367 0.37583005 0.22843456 0.10996467 0.041335329][0.22812818 0.26884472 0.35189214 0.46423814 0.58300549 0.68772769 0.75850409 0.77930748 0.74766183 0.668918 0.544857 0.38999352 0.23173591 0.10736366 0.037753571][0.16530086 0.25499922 0.38177249 0.52361149 0.65525585 0.75878507 0.81763285 0.8191126 0.76426071 0.66111004 0.51844984 0.35610706 0.20138803 0.087034106 0.0306075][0.097663932 0.21578556 0.36693758 0.52296114 0.65745485 0.75364745 0.79812926 0.78171784 0.70827997 0.587371 0.43440375 0.27618486 0.14040525 0.052716557 0.024471009][0.031365022 0.1545763 0.30566108 0.45684928 0.5838474 0.668757 0.69977134 0.67020464 0.5857833 0.45769742 0.30644259 0.16503577 0.062138539 0.015610093 0.028695757][-0.021370621 0.084493116 0.21353976 0.34399998 0.45577827 0.52683157 0.54520923 0.50674856 0.4185625 0.2949281 0.1586152 0.045843836 -0.013918232 -0.0099211046 0.052587841][-0.052255511 0.021425661 0.11409131 0.21245913 0.30047455 0.35291475 0.35764596 0.3136186 0.22867064 0.12045117 0.011134697 -0.064652175 -0.078025319 -0.023259668 0.087751649][-0.05272378 -0.019480089 0.029469835 0.0889888 0.14613298 0.17634058 0.16864543 0.12442307 0.05093167 -0.033517659 -0.1094619 -0.14775822 -0.12140264 -0.027582174 0.11806414][-0.023813233 -0.028338952 -0.020072564 0.0036522334 0.0315102 0.043024555 0.029270852 -0.00813284 -0.064461276 -0.12422622 -0.1709123 -0.18212898 -0.13358784 -0.02343592 0.13227142][0.019798828 -0.0099617774 -0.029555567 -0.029793177 -0.020726457 -0.017024597 -0.026279118 -0.049643643 -0.086912431 -0.12752049 -0.15707386 -0.1592337 -0.11267637 -0.013847805 0.12477796][0.064716935 0.025130548 -0.0067218747 -0.017238922 -0.014245098 -0.0075382409 -0.0047767661 -0.011157286 -0.031189902 -0.060137805 -0.085785359 -0.095608644 -0.070900276 -0.0049527171 0.09640035][0.10793893 0.072416924 0.04201971 0.032952227 0.039591495 0.054515462 0.069234595 0.076019533 0.067407578 0.043194313 0.012363017 -0.013869798 -0.018750565 0.00727335 0.06478627]]...]
INFO - root - 2017-12-11 08:15:43.734188: step 32310, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:25m:28s remains)
INFO - root - 2017-12-11 08:15:49.104419: step 32320, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 43h:54m:02s remains)
INFO - root - 2017-12-11 08:15:54.370077: step 32330, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:23m:22s remains)
INFO - root - 2017-12-11 08:15:59.739925: step 32340, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 46h:31m:47s remains)
INFO - root - 2017-12-11 08:16:05.098835: step 32350, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:09m:26s remains)
INFO - root - 2017-12-11 08:16:10.207191: step 32360, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:41m:54s remains)
INFO - root - 2017-12-11 08:16:15.585203: step 32370, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 46h:15m:24s remains)
INFO - root - 2017-12-11 08:16:20.910692: step 32380, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 44h:38m:46s remains)
INFO - root - 2017-12-11 08:16:26.274936: step 32390, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:20m:02s remains)
INFO - root - 2017-12-11 08:16:31.579843: step 32400, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:13m:13s remains)
2017-12-11 08:16:32.154883: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24561119 0.24995948 0.2324633 0.20617853 0.18857819 0.18610783 0.19407274 0.19586445 0.17896274 0.1529787 0.13410771 0.13630287 0.16209617 0.20421095 0.25210553][0.24633662 0.25319165 0.23989864 0.21897057 0.20747249 0.21113169 0.22284991 0.22481602 0.2061201 0.17725746 0.15537165 0.15396424 0.17863065 0.2248615 0.28049198][0.22210948 0.23704746 0.2344445 0.22542587 0.22508399 0.23905748 0.25772125 0.26186734 0.24243326 0.20959233 0.18021168 0.16833274 0.18501648 0.23308821 0.30128646][0.20764552 0.23318073 0.24370858 0.24892302 0.26342353 0.29288244 0.32363495 0.33239737 0.31045964 0.26787153 0.22230074 0.19370376 0.20189658 0.25546265 0.34162527][0.20045057 0.23628043 0.25999138 0.28058162 0.31318152 0.36253488 0.40976563 0.42567974 0.39980641 0.34272969 0.27544188 0.22816423 0.23099396 0.29445732 0.40025473][0.19334775 0.23615529 0.27024841 0.30539328 0.35692057 0.4267922 0.4900696 0.5117619 0.47906062 0.40495461 0.31677333 0.255038 0.25719637 0.33156562 0.45222464][0.18351319 0.23018798 0.27276471 0.32177797 0.3912859 0.47825277 0.55117679 0.57090861 0.52387345 0.4287301 0.32136005 0.24962141 0.25280529 0.33339515 0.45796695][0.16403942 0.21219242 0.26399255 0.32807949 0.41352981 0.51094794 0.58434016 0.59358084 0.52618617 0.4076201 0.28220761 0.20073035 0.19973327 0.27477571 0.3881214][0.13876374 0.18335795 0.23903354 0.31099182 0.40074781 0.49458113 0.55718136 0.55204654 0.46844748 0.33566362 0.20188917 0.11465923 0.10459331 0.16245417 0.25206766][0.1144539 0.14792316 0.19453503 0.25645709 0.32922006 0.39993465 0.44055748 0.42241493 0.33637571 0.21009126 0.0877859 0.007476517 -0.0085370261 0.029447572 0.09232147][0.097109362 0.11648089 0.14479055 0.18214342 0.22233586 0.25744739 0.27016225 0.2414737 0.16426846 0.060767397 -0.03394537 -0.094226189 -0.10704283 -0.082162254 -0.0403013][0.074588269 0.084287904 0.096416354 0.10943639 0.11866623 0.12217624 0.11193494 0.0774508 0.01422491 -0.061472088 -0.12427884 -0.15935193 -0.161724 -0.14082058 -0.11110482][0.036088686 0.039334796 0.040334456 0.036626935 0.02647284 0.012367184 -0.0078748381 -0.038987268 -0.083315931 -0.13032731 -0.16343391 -0.17551878 -0.1672506 -0.14730215 -0.12534317][-0.00578792 -0.0087427832 -0.015788335 -0.02943697 -0.048086375 -0.06694144 -0.085512273 -0.10580605 -0.12905112 -0.14970826 -0.15881006 -0.154334 -0.13979256 -0.12231646 -0.10747257][-0.035705864 -0.041786179 -0.051362302 -0.066910572 -0.084780671 -0.099853456 -0.11119024 -0.12013537 -0.12727205 -0.13005236 -0.12455235 -0.11157184 -0.095369317 -0.081366859 -0.072177008]]...]
INFO - root - 2017-12-11 08:16:37.491981: step 32410, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:25m:30s remains)
INFO - root - 2017-12-11 08:16:42.818398: step 32420, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:12m:03s remains)
INFO - root - 2017-12-11 08:16:48.130344: step 32430, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 43h:17m:06s remains)
INFO - root - 2017-12-11 08:16:53.539508: step 32440, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.559 sec/batch; 46h:37m:27s remains)
INFO - root - 2017-12-11 08:16:58.885401: step 32450, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 43h:13m:55s remains)
INFO - root - 2017-12-11 08:17:03.920048: step 32460, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 44h:48m:58s remains)
INFO - root - 2017-12-11 08:17:09.274983: step 32470, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 42h:57m:51s remains)
INFO - root - 2017-12-11 08:17:14.625738: step 32480, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 44h:35m:24s remains)
INFO - root - 2017-12-11 08:17:19.972459: step 32490, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:48m:04s remains)
INFO - root - 2017-12-11 08:17:25.313289: step 32500, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:52m:34s remains)
2017-12-11 08:17:25.842246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0055199894 0.12068323 0.28884292 0.45015758 0.56253242 0.6051181 0.59178483 0.54878175 0.506743 0.49931076 0.52921176 0.56642538 0.57789493 0.55408525 0.51997048][-0.033209778 0.0789773 0.23203504 0.37998715 0.48189172 0.51811856 0.50709486 0.47273672 0.44172907 0.44374803 0.4812873 0.52620542 0.54404837 0.52826554 0.50243288][-0.054044422 0.040353052 0.17306632 0.30359474 0.39493233 0.42956629 0.42309004 0.39532253 0.37224293 0.37924159 0.41815689 0.46239012 0.48119083 0.47048795 0.4504576][-0.057602916 0.029410364 0.1528057 0.27673769 0.36757553 0.40803111 0.40573761 0.37690926 0.35174677 0.354934 0.38806114 0.42522067 0.43996871 0.42933273 0.4079285][-0.042999323 0.050773617 0.18227099 0.31662086 0.41894034 0.4707737 0.47075331 0.43383414 0.39780211 0.38936478 0.41009 0.43367916 0.43743369 0.41964185 0.39023966][-0.012830368 0.10304199 0.26386234 0.42957133 0.55774456 0.62689012 0.62916631 0.58020633 0.52514178 0.49351314 0.48866689 0.48479936 0.46305546 0.4263525 0.38433111][0.019555185 0.16094068 0.35722658 0.55915427 0.71508294 0.80150688 0.80855972 0.75163519 0.67924231 0.622969 0.58886033 0.55306506 0.49923673 0.43740165 0.38072255][0.03865758 0.19311833 0.40953079 0.63023919 0.79879314 0.89274776 0.90434629 0.84636348 0.7636795 0.68895793 0.63269663 0.57528907 0.5009973 0.42501944 0.3633132][0.042532533 0.19267522 0.40579197 0.61908716 0.77750456 0.86400616 0.87619269 0.8228265 0.73862422 0.655712 0.58985955 0.52668387 0.4503307 0.37781319 0.32596278][0.039677233 0.17549092 0.37056661 0.56079859 0.6958313 0.765548 0.774388 0.72927558 0.65293443 0.57435209 0.51208782 0.45510647 0.38837972 0.32995507 0.29712942][0.032792389 0.14955615 0.31952345 0.48235503 0.593548 0.64776736 0.65483594 0.62275738 0.565073 0.50418872 0.45687234 0.41327402 0.36177883 0.32145992 0.30849344][0.029407861 0.12805569 0.27162141 0.4093121 0.50235826 0.54744512 0.55677211 0.54039323 0.50624812 0.4681851 0.43938869 0.41155079 0.37869152 0.35923257 0.36407283][0.041196115 0.13086909 0.25675672 0.37988737 0.4651798 0.50892609 0.52338296 0.52067518 0.50576663 0.48388374 0.46557945 0.44572392 0.42544544 0.42083445 0.43496925][0.064167075 0.15259138 0.2706311 0.3903797 0.47831979 0.52814519 0.54971719 0.55650944 0.55237615 0.53636283 0.51835769 0.49749497 0.4803448 0.47903582 0.4895958][0.082668111 0.16829045 0.27959508 0.39840075 0.49259675 0.5520131 0.58173394 0.59481752 0.59454042 0.57721061 0.55363858 0.52773708 0.50819278 0.50199491 0.50094104]]...]
INFO - root - 2017-12-11 08:17:31.250565: step 32510, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:02m:14s remains)
INFO - root - 2017-12-11 08:17:36.703716: step 32520, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:52m:38s remains)
INFO - root - 2017-12-11 08:17:42.057002: step 32530, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 44h:19m:52s remains)
INFO - root - 2017-12-11 08:17:47.318180: step 32540, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.538 sec/batch; 44h:50m:57s remains)
INFO - root - 2017-12-11 08:17:52.693769: step 32550, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:18m:02s remains)
INFO - root - 2017-12-11 08:17:57.819147: step 32560, loss = 0.69, batch loss = 0.63 (22.0 examples/sec; 0.364 sec/batch; 30h:18m:30s remains)
INFO - root - 2017-12-11 08:18:03.321269: step 32570, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:56m:47s remains)
INFO - root - 2017-12-11 08:18:08.607924: step 32580, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 44h:02m:31s remains)
INFO - root - 2017-12-11 08:18:13.908362: step 32590, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 43h:46m:51s remains)
INFO - root - 2017-12-11 08:18:19.277488: step 32600, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.558 sec/batch; 46h:28m:39s remains)
2017-12-11 08:18:19.848279: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15231556 0.11851004 0.10574821 0.11650542 0.14861195 0.19059755 0.22537848 0.23545003 0.20916471 0.14857145 0.06961377 -0.0047017653 -0.059938412 -0.091735207 -0.10348155][0.22573984 0.18440153 0.16099513 0.16706632 0.20078819 0.24880494 0.29266444 0.31086993 0.28618145 0.21779801 0.12276674 0.030039368 -0.041062191 -0.085065171 -0.10536512][0.30970636 0.27301133 0.23883097 0.23348705 0.25998133 0.30582222 0.35372731 0.38006017 0.36087739 0.29205677 0.18852568 0.081583083 -0.0051104627 -0.062861949 -0.093618318][0.4099156 0.38574159 0.34713581 0.33465523 0.35839862 0.40746412 0.4639951 0.49941012 0.48137924 0.40536654 0.28596082 0.15619645 0.045001276 -0.033620112 -0.078764349][0.51719105 0.50318956 0.47199416 0.46955428 0.50942165 0.58036518 0.65763545 0.70249754 0.67292249 0.5697397 0.41447085 0.24507028 0.098234 -0.006471741 -0.066909395][0.59596014 0.58846343 0.573924 0.59873563 0.67345381 0.78272748 0.8887046 0.93976128 0.88608909 0.738378 0.53154767 0.31321084 0.12928742 0.0026706087 -0.066961914][0.63026416 0.62875795 0.63205165 0.68475735 0.79170692 0.931507 1.0528601 1.0972509 1.0135229 0.82192558 0.57075477 0.31696227 0.11311904 -0.018708177 -0.084574342][0.64700252 0.651998 0.65948206 0.713856 0.81787491 0.94797236 1.0470791 1.0631731 0.9535889 0.74208343 0.4809947 0.22929691 0.038972247 -0.072973914 -0.11910252][0.67526537 0.67947853 0.66484988 0.67776543 0.72697157 0.79580688 0.83418888 0.80807245 0.68898153 0.49549606 0.27135903 0.0667437 -0.075790368 -0.14639096 -0.16166778][0.69251519 0.67881805 0.61876363 0.55964249 0.52207059 0.50476956 0.47435468 0.41597447 0.31340259 0.17285669 0.020935228 -0.10848413 -0.18690376 -0.21068464 -0.19622974][0.64396942 0.59584433 0.4861955 0.36002705 0.24840714 0.16728228 0.10057071 0.043671373 -0.016321855 -0.08779034 -0.16122803 -0.2193187 -0.24486217 -0.23639311 -0.20624754][0.50926471 0.41831356 0.27054319 0.11042551 -0.026345093 -0.11513438 -0.1636695 -0.18012108 -0.18308567 -0.192479 -0.20969652 -0.2258793 -0.22824568 -0.21307519 -0.18654808][0.34071586 0.21089101 0.04469255 -0.10948803 -0.21972233 -0.26662868 -0.26109633 -0.22275841 -0.17688663 -0.14792925 -0.14142494 -0.14882065 -0.15578409 -0.15478827 -0.14609274][0.20108542 0.052827626 -0.10878454 -0.22980285 -0.28806844 -0.27804103 -0.21944779 -0.14142053 -0.070838489 -0.02966393 -0.024211776 -0.043170303 -0.0688249 -0.090058684 -0.10259093][0.11083696 -0.036994893 -0.17809471 -0.25981858 -0.27097774 -0.2181875 -0.12934648 -0.037130222 0.03449453 0.069906332 0.0638647 0.029642725 -0.012706605 -0.049923394 -0.075534157]]...]
INFO - root - 2017-12-11 08:18:25.154668: step 32610, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:15m:53s remains)
INFO - root - 2017-12-11 08:18:30.546301: step 32620, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:59m:16s remains)
INFO - root - 2017-12-11 08:18:35.897398: step 32630, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 43h:16m:27s remains)
INFO - root - 2017-12-11 08:18:41.241366: step 32640, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:20m:32s remains)
INFO - root - 2017-12-11 08:18:46.515635: step 32650, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 44h:39m:29s remains)
INFO - root - 2017-12-11 08:18:51.879929: step 32660, loss = 0.67, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:11m:10s remains)
INFO - root - 2017-12-11 08:18:56.958399: step 32670, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:38m:52s remains)
INFO - root - 2017-12-11 08:19:02.357689: step 32680, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:23m:39s remains)
INFO - root - 2017-12-11 08:19:07.797356: step 32690, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 44h:14m:28s remains)
INFO - root - 2017-12-11 08:19:13.163251: step 32700, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:25m:02s remains)
2017-12-11 08:19:13.690756: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42752022 0.37930295 0.3024388 0.25041384 0.24704081 0.28144026 0.32361457 0.35654134 0.38273555 0.41535103 0.463478 0.50145465 0.4840591 0.38206029 0.21970415][0.35398233 0.31123 0.23953453 0.18720061 0.1852179 0.22881992 0.29323104 0.35746694 0.41225684 0.45525178 0.47895467 0.46541905 0.39665002 0.268753 0.11071996][0.24102814 0.21101134 0.15894511 0.12538342 0.13967022 0.20105906 0.28825635 0.3765226 0.44449344 0.47550532 0.45813426 0.38882926 0.27506465 0.13358134 -0.0059178774][0.11817369 0.10732929 0.0913748 0.099372469 0.15378562 0.25166145 0.36759061 0.4681133 0.52224123 0.51109636 0.43388575 0.30807239 0.16100028 0.019027734 -0.094908893][0.00537188 0.014769715 0.042085137 0.10462183 0.2144888 0.36072782 0.50673246 0.60690767 0.62572855 0.55404472 0.411903 0.23837227 0.072659731 -0.059545323 -0.14648473][-0.068114184 -0.041270372 0.024535805 0.13711742 0.29728833 0.48270041 0.64400792 0.72741717 0.70136684 0.57343829 0.38403755 0.18546851 0.019237541 -0.095637389 -0.15837897][-0.10059486 -0.059894428 0.033811089 0.18191163 0.37434939 0.57702917 0.73087627 0.78170449 0.70969754 0.54042661 0.3280862 0.12874806 -0.020995907 -0.11166029 -0.15174094][-0.11450268 -0.065949865 0.043901004 0.212593 0.42037958 0.622623 0.75504863 0.76883435 0.65841 0.46410069 0.24748163 0.06264057 -0.061519321 -0.1249733 -0.14421239][-0.12050268 -0.073773 0.036250032 0.20667928 0.4117623 0.60053939 0.70900655 0.69584286 0.56563407 0.36612484 0.16082172 -0.00033602526 -0.096362069 -0.13502224 -0.13788196][-0.11770945 -0.080830932 0.013888144 0.16400392 0.34335274 0.50307542 0.58663094 0.56084436 0.43454078 0.25427696 0.078401625 -0.0507672 -0.11930708 -0.13884449 -0.13140377][-0.11556458 -0.090853274 -0.019077847 0.097209476 0.23540519 0.35536385 0.412919 0.3836937 0.2775051 0.13429922 0.001171731 -0.090556465 -0.13337439 -0.13921209 -0.12647854][-0.12156466 -0.10904323 -0.061117575 0.018990377 0.11401125 0.19427854 0.22850487 0.2010612 0.12166414 0.02142101 -0.066073783 -0.1208577 -0.14074403 -0.13642527 -0.12144608][-0.13122663 -0.13058665 -0.10510341 -0.058095608 -0.0010332376 0.046145327 0.063379146 0.042024225 -0.0094828932 -0.069104739 -0.11564162 -0.13882633 -0.14044054 -0.12928259 -0.11406413][-0.13403516 -0.14334153 -0.13706104 -0.11763888 -0.090956733 -0.068315089 -0.060950819 -0.073093571 -0.099365838 -0.12653603 -0.14296879 -0.14481059 -0.13546553 -0.12135854 -0.10715923][-0.12756306 -0.14188989 -0.14741315 -0.14603983 -0.13953944 -0.13264441 -0.13040647 -0.13468164 -0.14322914 -0.14982677 -0.14950301 -0.14176767 -0.12956844 -0.11659586 -0.10454938]]...]
INFO - root - 2017-12-11 08:19:19.072706: step 32710, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:12m:37s remains)
INFO - root - 2017-12-11 08:19:24.365351: step 32720, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:19m:14s remains)
INFO - root - 2017-12-11 08:19:29.672197: step 32730, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 43h:56m:28s remains)
INFO - root - 2017-12-11 08:19:34.981572: step 32740, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 43h:42m:20s remains)
INFO - root - 2017-12-11 08:19:40.426150: step 32750, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:22m:49s remains)
INFO - root - 2017-12-11 08:19:45.811096: step 32760, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 43h:14m:59s remains)
INFO - root - 2017-12-11 08:19:50.900420: step 32770, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:32m:40s remains)
INFO - root - 2017-12-11 08:19:56.278827: step 32780, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 42h:50m:11s remains)
INFO - root - 2017-12-11 08:20:01.684373: step 32790, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:32m:50s remains)
INFO - root - 2017-12-11 08:20:06.935319: step 32800, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 43h:58m:59s remains)
2017-12-11 08:20:07.509643: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0041525271 -0.017513802 -0.02379128 -0.015276772 0.0014370194 0.024333086 0.056408513 0.095430151 0.13479611 0.16876359 0.19019429 0.19440031 0.18760727 0.17820902 0.17424183][0.026665796 0.0051669925 2.5913239e-05 0.012321912 0.032950342 0.057669908 0.090209261 0.129397 0.16954198 0.205389 0.22890911 0.23166947 0.21664283 0.19621731 0.18414344][0.058227763 0.045456205 0.04876025 0.0690946 0.092688084 0.11190432 0.13290699 0.15906291 0.18878281 0.21855699 0.24013777 0.24206333 0.22082451 0.19147363 0.17309794][0.087848239 0.087644584 0.10271653 0.13219558 0.15748721 0.16783316 0.17283286 0.18252054 0.20048736 0.22422813 0.24467351 0.24771425 0.22269857 0.18623994 0.1614749][0.10417175 0.11466514 0.13963482 0.17742902 0.20601298 0.21180692 0.20605265 0.20369783 0.21289356 0.23219304 0.25208783 0.25638932 0.22840562 0.18562037 0.1542266][0.10932758 0.12481989 0.1573952 0.20612337 0.24527638 0.25660822 0.249538 0.24102914 0.24266727 0.25568604 0.27178103 0.27415407 0.24170771 0.19191526 0.1536763][0.10252833 0.12131606 0.16340655 0.22760195 0.28296572 0.30562702 0.30140382 0.28738663 0.2778317 0.27867976 0.28542963 0.28302902 0.24791306 0.19586314 0.15702558][0.074053884 0.096811682 0.15017883 0.22945613 0.2995151 0.33344749 0.3344045 0.3179169 0.29841065 0.28566763 0.2807993 0.27348864 0.24017683 0.19296755 0.16028605][0.02919277 0.05418697 0.11402416 0.19883703 0.27359265 0.31476614 0.32504824 0.31495956 0.29347244 0.27120858 0.25627959 0.24571875 0.21766309 0.17916074 0.15494198][-0.017537979 0.0046247141 0.06159604 0.1401255 0.20985624 0.2548784 0.27749532 0.27991316 0.26243648 0.23461698 0.21204561 0.19959934 0.17756446 0.14781712 0.13088892][-0.060086247 -0.044336084 0.0036702731 0.070334405 0.13178773 0.17838913 0.2107687 0.22330831 0.20948039 0.17790793 0.14967182 0.13559309 0.11843343 0.096258491 0.085791968][-0.095994718 -0.087546229 -0.049475726 0.0044415742 0.056791309 0.10113433 0.13565077 0.14999829 0.13505475 0.099690385 0.0669722 0.050576109 0.036904592 0.022655794 0.020792961][-0.11603629 -0.11283816 -0.083383515 -0.041713327 -0.00033293772 0.035743888 0.063320324 0.072136365 0.054526769 0.019194642 -0.013496097 -0.030335821 -0.04025998 -0.046392221 -0.040198136][-0.1172954 -0.11623395 -0.093883634 -0.064534947 -0.037255693 -0.015429202 -0.0012090435 -0.001355837 -0.019740513 -0.0489844 -0.074956194 -0.088193789 -0.093584783 -0.092887051 -0.081830554][-0.10767188 -0.10894077 -0.094753407 -0.0786951 -0.066139787 -0.058578342 -0.056406081 -0.062655106 -0.077941805 -0.096936814 -0.11230748 -0.11949126 -0.12130933 -0.11774915 -0.10667989]]...]
INFO - root - 2017-12-11 08:20:12.853213: step 32810, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:40m:19s remains)
INFO - root - 2017-12-11 08:20:18.266675: step 32820, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.551 sec/batch; 45h:51m:56s remains)
INFO - root - 2017-12-11 08:20:23.648817: step 32830, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 44h:18m:31s remains)
INFO - root - 2017-12-11 08:20:29.029170: step 32840, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:25m:35s remains)
INFO - root - 2017-12-11 08:20:34.333278: step 32850, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:58m:12s remains)
INFO - root - 2017-12-11 08:20:39.726484: step 32860, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 44h:16m:01s remains)
INFO - root - 2017-12-11 08:20:44.793978: step 32870, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:31m:05s remains)
INFO - root - 2017-12-11 08:20:50.176263: step 32880, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 45h:52m:34s remains)
INFO - root - 2017-12-11 08:20:55.505205: step 32890, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 44h:41m:18s remains)
INFO - root - 2017-12-11 08:21:00.836079: step 32900, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:30m:47s remains)
2017-12-11 08:21:01.397181: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30962256 0.3295624 0.33563942 0.33695209 0.33343208 0.31765065 0.28735298 0.2472882 0.21813402 0.20192264 0.19106038 0.18217276 0.17288157 0.15984623 0.13322681][0.40068671 0.42612177 0.43144852 0.43109232 0.42726657 0.41214785 0.38090315 0.33729234 0.30371231 0.28007382 0.25794059 0.23530629 0.21079618 0.18408798 0.14798589][0.45837408 0.48437566 0.4858672 0.4845157 0.48569748 0.4816564 0.46240905 0.42706665 0.39727682 0.36963129 0.33513552 0.29477325 0.25001851 0.20488259 0.15579301][0.48418123 0.50637454 0.50333554 0.50247657 0.51132196 0.52342713 0.52440971 0.50734192 0.49060956 0.46776408 0.42804596 0.37407687 0.30882898 0.24124518 0.1726663][0.47854385 0.49673024 0.49488989 0.501809 0.52316272 0.55349672 0.57757735 0.58475095 0.5887394 0.5812732 0.54817003 0.4902941 0.40778914 0.31408241 0.21767987][0.431526 0.45153388 0.46409407 0.492909 0.53565872 0.58519673 0.63104421 0.66333103 0.68980467 0.70098519 0.67908937 0.622076 0.52473068 0.40357253 0.27540269][0.34486508 0.37202179 0.40999019 0.46930775 0.53660625 0.60139495 0.66394925 0.718661 0.76510739 0.79201031 0.77910817 0.7229172 0.61282712 0.46736458 0.31149694][0.23854415 0.27617839 0.33988294 0.42416385 0.50636488 0.57377857 0.64067775 0.70803857 0.76808584 0.80774647 0.80436307 0.7525208 0.63760054 0.47972703 0.30979824][0.12677583 0.17340566 0.25389263 0.34928086 0.43187562 0.48817435 0.544824 0.61034441 0.67401075 0.72222942 0.7307381 0.6907233 0.58566606 0.43586317 0.27269632][0.031137209 0.081072859 0.16591072 0.2588633 0.33172163 0.37074283 0.40836158 0.45889688 0.51415068 0.56145036 0.57672638 0.54935771 0.46420062 0.33981189 0.20153196][-0.026971018 0.017234292 0.091377683 0.16796489 0.22311912 0.24454814 0.26295498 0.29353964 0.33404583 0.37339938 0.38923514 0.371103 0.30732164 0.21359703 0.106607][-0.048185308 -0.017001847 0.034695297 0.084541939 0.11655205 0.12245911 0.12599631 0.13890873 0.16566461 0.19789188 0.21508656 0.206066 0.16126533 0.093788877 0.01402151][-0.059488107 -0.041969404 -0.013652055 0.01050356 0.02202379 0.01692291 0.01011875 0.0091620367 0.024458274 0.050966769 0.069918081 0.068071112 0.037120529 -0.010645485 -0.067749679][-0.078434616 -0.071533807 -0.059314463 -0.05078188 -0.050234675 -0.059452906 -0.070771791 -0.079315573 -0.072685681 -0.054126352 -0.040071152 -0.043171685 -0.068476491 -0.1034041 -0.14136755][-0.096621059 -0.096496366 -0.092714116 -0.090733923 -0.092976458 -0.10052659 -0.1101996 -0.11890464 -0.11683643 -0.10732157 -0.10352028 -0.11394701 -0.1390022 -0.166441 -0.19034664]]...]
INFO - root - 2017-12-11 08:21:06.766430: step 32910, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:10m:06s remains)
INFO - root - 2017-12-11 08:21:12.095399: step 32920, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:31m:43s remains)
INFO - root - 2017-12-11 08:21:17.425541: step 32930, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:21m:54s remains)
INFO - root - 2017-12-11 08:21:22.733583: step 32940, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 46h:48m:35s remains)
INFO - root - 2017-12-11 08:21:28.144300: step 32950, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 44h:21m:21s remains)
INFO - root - 2017-12-11 08:21:33.482076: step 32960, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 45h:40m:43s remains)
INFO - root - 2017-12-11 08:21:38.854245: step 32970, loss = 0.69, batch loss = 0.63 (15.8 examples/sec; 0.508 sec/batch; 42h:14m:55s remains)
INFO - root - 2017-12-11 08:21:44.059009: step 32980, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 44h:24m:18s remains)
INFO - root - 2017-12-11 08:21:49.479275: step 32990, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:22m:45s remains)
INFO - root - 2017-12-11 08:21:54.840693: step 33000, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 43h:57m:01s remains)
2017-12-11 08:21:55.417359: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10117261 0.11056516 0.1443764 0.2051165 0.2751731 0.34839934 0.42336336 0.4871313 0.54229605 0.57939595 0.59193164 0.57320231 0.533624 0.4960638 0.46715635][0.10584284 0.10422088 0.12863314 0.18506736 0.25356594 0.32457864 0.39520273 0.45261815 0.50050694 0.52786148 0.53000593 0.50510752 0.46930137 0.44546932 0.43513951][0.1232523 0.11023916 0.11923865 0.16358569 0.22189946 0.28103045 0.33685592 0.37877268 0.41244662 0.4277508 0.42002463 0.39150062 0.36342922 0.35765505 0.36956111][0.16422768 0.1358517 0.12919812 0.16095658 0.20934004 0.25842357 0.30219945 0.33201993 0.35384107 0.35899693 0.34048066 0.302746 0.27255744 0.27356765 0.29870033][0.21722271 0.17515175 0.1580445 0.18359478 0.23056819 0.28281167 0.33140573 0.36605808 0.38831231 0.38928983 0.35711423 0.29702064 0.2434966 0.22833325 0.24859776][0.27795765 0.23058842 0.21421412 0.24501145 0.3021487 0.37238479 0.44222057 0.49449149 0.52226448 0.51664954 0.46220455 0.36479181 0.26871124 0.21985573 0.22344375][0.30669308 0.26385027 0.2552591 0.29853007 0.3721076 0.46776193 0.56619549 0.63920462 0.66816431 0.64669108 0.56355935 0.42736241 0.28878471 0.20604059 0.19284053][0.28544176 0.24961375 0.2506555 0.30608341 0.39312848 0.50845623 0.62770414 0.71309584 0.73468906 0.69080728 0.58313715 0.42472523 0.26419035 0.16291744 0.14087354][0.24268843 0.21337706 0.2208073 0.28010488 0.36735788 0.48284018 0.60130453 0.68254817 0.68959463 0.62560511 0.50775361 0.35325438 0.20099127 0.10445853 0.084843956][0.23013195 0.21063463 0.21780115 0.26703069 0.33430648 0.42251354 0.51220781 0.56999403 0.55836213 0.48407203 0.37560096 0.25070444 0.13365272 0.061608102 0.051642679][0.26203278 0.24907404 0.24578136 0.27023086 0.30041561 0.3415148 0.3843495 0.40786257 0.38060018 0.31099179 0.23000053 0.15107723 0.082860753 0.042674106 0.040572122][0.31718817 0.29973722 0.27479389 0.26709536 0.25768557 0.25149006 0.24891725 0.24150552 0.20853986 0.15859409 0.11653697 0.087412976 0.0658752 0.051914081 0.049860444][0.38371491 0.35404035 0.30447879 0.26679909 0.22480263 0.18195124 0.14554681 0.11625817 0.085661463 0.063049331 0.063378692 0.080482014 0.096143194 0.098420367 0.089860462][0.41522366 0.37572196 0.31344742 0.26405805 0.2101233 0.15201834 0.10073999 0.062396374 0.039711524 0.044019509 0.081718691 0.13517533 0.17481883 0.18178652 0.1600863][0.38329896 0.33917448 0.27989486 0.23936737 0.19468911 0.14231126 0.0946933 0.061423533 0.052763723 0.081069067 0.14725444 0.22403818 0.27418464 0.27672744 0.23908456]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:22:00.729985: step 33010, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.518 sec/batch; 43h:07m:37s remains)
INFO - root - 2017-12-11 08:22:06.086773: step 33020, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.514 sec/batch; 42h:46m:14s remains)
INFO - root - 2017-12-11 08:22:11.361143: step 33030, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 42h:51m:32s remains)
INFO - root - 2017-12-11 08:22:16.722834: step 33040, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:20m:27s remains)
INFO - root - 2017-12-11 08:22:22.101762: step 33050, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:53m:59s remains)
INFO - root - 2017-12-11 08:22:27.469186: step 33060, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 46h:55m:45s remains)
INFO - root - 2017-12-11 08:22:32.738939: step 33070, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 43h:49m:06s remains)
INFO - root - 2017-12-11 08:22:37.789700: step 33080, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 43h:44m:31s remains)
INFO - root - 2017-12-11 08:22:43.170016: step 33090, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:23m:18s remains)
INFO - root - 2017-12-11 08:22:48.486326: step 33100, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:08m:36s remains)
2017-12-11 08:22:49.085178: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18795048 0.22280383 0.2341713 0.22264852 0.1935859 0.16956377 0.17389993 0.20614824 0.24748717 0.28012842 0.29131392 0.28083298 0.25559655 0.2300407 0.214211][0.27940208 0.33138385 0.35040441 0.33788437 0.30151227 0.2743564 0.2875112 0.33750814 0.39489406 0.4348841 0.44237375 0.4193953 0.37999111 0.34517071 0.32558051][0.35566142 0.42134863 0.44688684 0.43518668 0.39699206 0.37369514 0.39958119 0.4660165 0.53352141 0.57374048 0.57249451 0.53472841 0.48231173 0.44186369 0.42179015][0.39194313 0.46389815 0.4931334 0.48482463 0.45244679 0.44065639 0.48060393 0.55730313 0.62459797 0.65576714 0.64197648 0.59143019 0.53230458 0.4938558 0.47946674][0.38602719 0.45636392 0.48713011 0.48514211 0.46565682 0.47016439 0.52118266 0.59801567 0.6532355 0.667049 0.6385529 0.57909244 0.51944023 0.48846343 0.48447162][0.36961019 0.43579119 0.4664315 0.4690569 0.46016371 0.47479516 0.52647221 0.59179389 0.62735093 0.62197965 0.58062148 0.51662147 0.46022943 0.43801925 0.44462585][0.37576577 0.44194043 0.47213832 0.47334078 0.46338597 0.47182116 0.50856429 0.55244458 0.56634879 0.54591507 0.4972001 0.43344659 0.3825857 0.36888641 0.38509876][0.40877259 0.48031896 0.50898373 0.50186878 0.47840634 0.4651475 0.4743394 0.49219483 0.48876593 0.46198502 0.413766 0.35512909 0.3114441 0.30567348 0.33078361][0.44930032 0.5278756 0.55344474 0.53402585 0.49217421 0.45489693 0.43910035 0.43887818 0.42999634 0.4071717 0.36495078 0.31070319 0.26947784 0.26595628 0.29554224][0.48251051 0.5673992 0.58998781 0.55944532 0.50215286 0.44902456 0.420614 0.41440469 0.40933892 0.39472908 0.35836774 0.30433211 0.25915027 0.25190553 0.28132132][0.50157386 0.59307623 0.61716396 0.58305639 0.51921088 0.46076304 0.42888716 0.42025495 0.41603148 0.40283859 0.36724398 0.31189266 0.2641432 0.25672638 0.28933796][0.50683379 0.60787636 0.64141577 0.61492682 0.5554027 0.49981445 0.46727854 0.45178774 0.43729267 0.4140597 0.37382591 0.3195056 0.27671424 0.27889881 0.32340652][0.49592355 0.60672474 0.65512675 0.64509344 0.59838396 0.55024171 0.51660073 0.49017617 0.45949343 0.42179477 0.37688586 0.32928482 0.30091298 0.32127577 0.3830623][0.47139025 0.58522928 0.64495254 0.65105176 0.6189512 0.58035553 0.54762834 0.51356792 0.471368 0.42546597 0.38076803 0.34340411 0.33159631 0.36894587 0.44380903][0.4287391 0.53788555 0.60015965 0.61447591 0.59106976 0.55850929 0.52759707 0.49295816 0.45021737 0.40770108 0.370724 0.34455609 0.34343746 0.38689736 0.46383336]]...]
INFO - root - 2017-12-11 08:22:54.329643: step 33110, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 44h:30m:02s remains)
INFO - root - 2017-12-11 08:22:59.670702: step 33120, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:03m:52s remains)
INFO - root - 2017-12-11 08:23:05.044591: step 33130, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 44h:36m:41s remains)
INFO - root - 2017-12-11 08:23:10.422277: step 33140, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 44h:43m:26s remains)
INFO - root - 2017-12-11 08:23:15.781108: step 33150, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.539 sec/batch; 44h:51m:28s remains)
INFO - root - 2017-12-11 08:23:21.220415: step 33160, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:35m:13s remains)
INFO - root - 2017-12-11 08:23:26.620861: step 33170, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:05m:34s remains)
INFO - root - 2017-12-11 08:23:31.651700: step 33180, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:51m:51s remains)
INFO - root - 2017-12-11 08:23:37.019124: step 33190, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 45h:00m:50s remains)
INFO - root - 2017-12-11 08:23:42.421329: step 33200, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:51m:19s remains)
2017-12-11 08:23:43.000787: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06766919 0.069638073 0.07884708 0.10586353 0.15830997 0.23592676 0.32341212 0.39542806 0.43559048 0.44590774 0.43156371 0.40359536 0.36619785 0.31988528 0.25634164][0.082168885 0.083099395 0.094871752 0.13108492 0.2018865 0.3063575 0.42301923 0.5165754 0.56403971 0.57026833 0.54501879 0.50309962 0.4511323 0.39192057 0.31496114][0.11966988 0.11422182 0.12097613 0.15775509 0.23753372 0.35822764 0.49242854 0.59644312 0.64318544 0.64197123 0.60878175 0.55898857 0.49826816 0.4302299 0.3442722][0.18305297 0.16242407 0.153192 0.17927122 0.25559458 0.37797567 0.51520216 0.62014771 0.66553187 0.66337603 0.63398254 0.58792579 0.52570415 0.45055965 0.3555266][0.27237728 0.22633062 0.18973838 0.19625624 0.26107365 0.3756642 0.50650543 0.60726517 0.65298069 0.65514052 0.636184 0.59984267 0.53968555 0.45816833 0.35400736][0.37414473 0.29933351 0.23203886 0.21720994 0.26939976 0.37453282 0.49612665 0.58901328 0.6308791 0.63154083 0.61569756 0.58381069 0.52417594 0.43798202 0.32817304][0.45533779 0.36172894 0.27497786 0.24783401 0.29439327 0.39609733 0.512094 0.59614432 0.62752831 0.61517423 0.588357 0.55020154 0.48645708 0.39576438 0.28260446][0.51787353 0.41684988 0.32460538 0.29637307 0.34539604 0.44904137 0.56078553 0.63176113 0.64284229 0.60599089 0.5574972 0.50735128 0.43899426 0.3462792 0.23293604][0.5663271 0.46400473 0.37139472 0.34227321 0.3883397 0.48537531 0.5857439 0.64097565 0.63512427 0.58464378 0.52738518 0.4764798 0.41123319 0.32062551 0.20820925][0.56661147 0.47073612 0.38263506 0.35091725 0.38496396 0.46262097 0.542106 0.5827508 0.57504606 0.53679937 0.496282 0.46166047 0.40800121 0.32289359 0.21208632][0.48139125 0.39853114 0.32399195 0.29545003 0.31863472 0.37589681 0.43500942 0.46783981 0.47251487 0.46331766 0.45242932 0.43888915 0.39706641 0.31819338 0.21191598][0.33463964 0.26866236 0.21280815 0.19309373 0.21126667 0.25437972 0.29929921 0.32957843 0.34934282 0.36638537 0.37887555 0.37936208 0.34564468 0.27547681 0.181197][0.18943439 0.14036144 0.10057495 0.086654745 0.098471791 0.12901533 0.16332905 0.1922248 0.22309022 0.25673103 0.2826061 0.29040742 0.2631672 0.20481698 0.12767118][0.064339712 0.036762182 0.013885834 0.004848795 0.0097288061 0.028356889 0.053012997 0.078370392 0.11391617 0.15469415 0.18606313 0.19662733 0.17454767 0.12757272 0.066135347][-0.040253028 -0.046383288 -0.05059943 -0.050496131 -0.046107277 -0.033884004 -0.017412558 0.00059830042 0.031760495 0.067985266 0.0948767 0.10270606 0.084072195 0.047195431 -0.00068255427]]...]
INFO - root - 2017-12-11 08:23:48.322659: step 33210, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 45h:07m:25s remains)
INFO - root - 2017-12-11 08:23:53.691694: step 33220, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:16m:40s remains)
INFO - root - 2017-12-11 08:23:58.922668: step 33230, loss = 0.71, batch loss = 0.65 (15.8 examples/sec; 0.506 sec/batch; 42h:02m:39s remains)
INFO - root - 2017-12-11 08:24:04.221813: step 33240, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 43h:37m:12s remains)
INFO - root - 2017-12-11 08:24:09.609552: step 33250, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:55m:46s remains)
INFO - root - 2017-12-11 08:24:14.934529: step 33260, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 43h:20m:42s remains)
INFO - root - 2017-12-11 08:24:20.229961: step 33270, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:13m:03s remains)
INFO - root - 2017-12-11 08:24:25.372294: step 33280, loss = 0.71, batch loss = 0.65 (21.1 examples/sec; 0.380 sec/batch; 31h:32m:59s remains)
INFO - root - 2017-12-11 08:24:30.788904: step 33290, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 43h:53m:38s remains)
INFO - root - 2017-12-11 08:24:36.179617: step 33300, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 44h:17m:16s remains)
2017-12-11 08:24:36.730995: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12757373 0.1285647 0.12553169 0.13277605 0.16317242 0.21212415 0.26266822 0.29331544 0.28978842 0.24390718 0.16532972 0.080139533 0.011001206 -0.033703841 -0.057321385][0.17154635 0.16820595 0.1595138 0.16632827 0.20690247 0.27556556 0.34890568 0.39626417 0.39622623 0.33746958 0.233159 0.11830073 0.024925694 -0.034241673 -0.063933387][0.20052925 0.19174521 0.17833076 0.18583195 0.2361127 0.32208943 0.4149338 0.47560513 0.47704256 0.40684268 0.28171048 0.14373639 0.032148913 -0.036792282 -0.069126725][0.21534127 0.20338111 0.18784013 0.19664732 0.25281054 0.34851107 0.45212087 0.518615 0.51764768 0.43844941 0.30070162 0.14984098 0.028517358 -0.044681963 -0.07655292][0.21835448 0.20637161 0.19121504 0.20225361 0.26165131 0.36149383 0.46894038 0.53559291 0.52979821 0.44362146 0.29947758 0.14306556 0.017706314 -0.056309596 -0.085785113][0.20710158 0.19448303 0.18271622 0.19910057 0.26177266 0.36155647 0.46562812 0.52638113 0.51353604 0.42223567 0.27706081 0.12176666 -0.001838894 -0.072767176 -0.097307354][0.19854991 0.18276441 0.17464446 0.19735143 0.26226166 0.35680297 0.448921 0.49570853 0.47174215 0.37664896 0.23625977 0.090177588 -0.024293218 -0.087791339 -0.10606574][0.21391587 0.19432691 0.18833268 0.21500459 0.27885291 0.36323658 0.43725795 0.46405536 0.42500451 0.32564059 0.19276914 0.060987156 -0.039424852 -0.093432613 -0.10685159][0.23176081 0.21334757 0.21206522 0.24213551 0.30226192 0.37347159 0.42786294 0.43503529 0.3834486 0.28308871 0.15993926 0.043343037 -0.0435443 -0.089937665 -0.10153354][0.22449537 0.21338129 0.22064826 0.25393516 0.30729836 0.36228639 0.39692774 0.38853604 0.3324466 0.2394271 0.13095444 0.03019055 -0.045021504 -0.085672706 -0.096385412][0.19135267 0.1898796 0.20700961 0.24132563 0.28405666 0.32046515 0.33647504 0.3177017 0.26447946 0.18589832 0.096723013 0.013601368 -0.049385458 -0.083764434 -0.092576079][0.15269254 0.16148268 0.18608373 0.2172218 0.24600382 0.26391634 0.26425508 0.23875366 0.19032331 0.12663513 0.056707416 -0.0083860671 -0.057759717 -0.0838248 -0.089139514][0.11256781 0.12945314 0.15612584 0.18016918 0.19515345 0.19935338 0.19112028 0.16410844 0.12120254 0.069328055 0.014641093 -0.034948733 -0.07098902 -0.087419145 -0.087783031][0.086224519 0.10597938 0.127266 0.14010827 0.1416094 0.13578798 0.12413859 0.099621862 0.063269153 0.021294445 -0.021438845 -0.058916897 -0.084298588 -0.092818841 -0.08869718][0.093468145 0.11093934 0.12187609 0.12140809 0.10974265 0.094544038 0.079133168 0.05588387 0.023555674 -0.012061073 -0.046491239 -0.075500906 -0.093704768 -0.097352572 -0.090337493]]...]
INFO - root - 2017-12-11 08:24:42.096504: step 33310, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:05m:56s remains)
INFO - root - 2017-12-11 08:24:47.411243: step 33320, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:10m:19s remains)
INFO - root - 2017-12-11 08:24:52.801012: step 33330, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:23m:09s remains)
INFO - root - 2017-12-11 08:24:58.231170: step 33340, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:16m:22s remains)
INFO - root - 2017-12-11 08:25:03.546498: step 33350, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 43h:17m:09s remains)
INFO - root - 2017-12-11 08:25:08.911793: step 33360, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 45h:06m:44s remains)
INFO - root - 2017-12-11 08:25:14.329448: step 33370, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 43h:56m:36s remains)
INFO - root - 2017-12-11 08:25:19.652315: step 33380, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 44h:14m:20s remains)
INFO - root - 2017-12-11 08:25:24.826021: step 33390, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:51m:15s remains)
INFO - root - 2017-12-11 08:25:30.151894: step 33400, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:03m:06s remains)
2017-12-11 08:25:30.701039: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36362255 0.36783198 0.36955094 0.37268069 0.37415168 0.36904463 0.35042754 0.32347479 0.28730613 0.2445475 0.20816392 0.19739746 0.21703953 0.25362056 0.31332844][0.46874648 0.47174108 0.46052235 0.44515428 0.42890024 0.41084218 0.3831645 0.35346103 0.32016367 0.28013328 0.24457435 0.23280764 0.25116113 0.28761292 0.35611483][0.51060206 0.51076174 0.48713133 0.45564336 0.42692539 0.40483394 0.37983572 0.35864583 0.33739677 0.30538544 0.26944104 0.24923182 0.25483847 0.27989745 0.34499827][0.47439393 0.47413272 0.4487161 0.4185867 0.39986217 0.3967815 0.39514825 0.396126 0.39226326 0.36647227 0.32097945 0.2781294 0.25457153 0.25346589 0.29842234][0.36485404 0.37286475 0.36563003 0.364285 0.38758302 0.43315339 0.47772011 0.51126623 0.52178264 0.49102083 0.42044178 0.33860415 0.26990852 0.22806096 0.23563248][0.22382487 0.25706488 0.29396147 0.35039094 0.44359073 0.55813116 0.6581341 0.71886474 0.72507727 0.66684306 0.55245668 0.41886908 0.29605594 0.20517576 0.16540916][0.095988117 0.16991575 0.26975927 0.39987 0.57002759 0.7497319 0.89255488 0.96156365 0.94124126 0.8371709 0.67020941 0.48323044 0.30894282 0.17277923 0.090572163][0.011874787 0.12939638 0.289887 0.48281071 0.70714152 0.92149663 1.0771487 1.1329813 1.0762813 0.92658585 0.71634734 0.4905892 0.28299519 0.12102459 0.017354142][-0.018619798 0.12949042 0.32584578 0.54705071 0.78252608 0.98756284 1.1199259 1.1433867 1.0519097 0.87335086 0.64536232 0.41036353 0.20054562 0.043184161 -0.052298494][-0.0029428559 0.15550075 0.35683396 0.5692699 0.77452093 0.9308455 1.0079921 0.98323578 0.86260104 0.67841321 0.4653849 0.25711179 0.080040991 -0.042196397 -0.10391904][0.049548939 0.20613362 0.39216322 0.57092583 0.71882069 0.80122972 0.80368328 0.72348589 0.58294654 0.4161427 0.24787709 0.097300686 -0.020224687 -0.089389317 -0.10876714][0.13233843 0.28566036 0.44968447 0.58430469 0.66536587 0.66865164 0.5966084 0.46822536 0.32171211 0.19068369 0.085965864 0.0084528355 -0.04258313 -0.062882483 -0.052612703][0.22836603 0.38050407 0.523006 0.61383986 0.63303035 0.56934011 0.44112879 0.2863892 0.15407139 0.073408656 0.0379073 0.02812285 0.027334932 0.031986255 0.046735208][0.29549468 0.44257364 0.56468177 0.61910093 0.59185314 0.48531643 0.33282471 0.18439466 0.089761831 0.067852326 0.094705068 0.13295828 0.15436964 0.15418632 0.1489615][0.28949791 0.42016539 0.52045166 0.54928678 0.49861854 0.38131595 0.24080954 0.12892401 0.086304821 0.11906786 0.19039364 0.25240234 0.27192158 0.24742472 0.21024761]]...]
INFO - root - 2017-12-11 08:25:36.096483: step 33410, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 45h:44m:03s remains)
INFO - root - 2017-12-11 08:25:41.476443: step 33420, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 44h:05m:13s remains)
INFO - root - 2017-12-11 08:25:46.795675: step 33430, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.567 sec/batch; 47h:05m:58s remains)
INFO - root - 2017-12-11 08:25:52.111972: step 33440, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:55m:52s remains)
INFO - root - 2017-12-11 08:25:57.454330: step 33450, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 44h:03m:03s remains)
INFO - root - 2017-12-11 08:26:02.749574: step 33460, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 43h:04m:19s remains)
INFO - root - 2017-12-11 08:26:08.164275: step 33470, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:47m:10s remains)
INFO - root - 2017-12-11 08:26:13.528982: step 33480, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.546 sec/batch; 45h:20m:55s remains)
INFO - root - 2017-12-11 08:26:18.662906: step 33490, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:29m:25s remains)
INFO - root - 2017-12-11 08:26:24.080246: step 33500, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:56m:59s remains)
2017-12-11 08:26:24.685989: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48947445 0.509908 0.50451386 0.49181503 0.48073032 0.47185606 0.47201112 0.48410076 0.49565348 0.48980403 0.47294661 0.47320765 0.50703073 0.56421757 0.60696042][0.53815174 0.5715543 0.57050371 0.55577654 0.54047883 0.53082073 0.53458565 0.55216026 0.56666362 0.55923378 0.53767562 0.53193718 0.5609318 0.6155507 0.65517151][0.52644956 0.5622555 0.56067473 0.54451638 0.52889109 0.52374464 0.53549051 0.56030959 0.57797223 0.5705111 0.5464415 0.53334618 0.54992431 0.59028804 0.61621726][0.50683033 0.53309524 0.52235508 0.50392115 0.49348 0.50094306 0.5285874 0.56704605 0.59170693 0.58548391 0.55601758 0.527443 0.5197475 0.53328305 0.53678983][0.4862577 0.49351859 0.46708423 0.44636419 0.44654977 0.47403896 0.52394044 0.58014005 0.6139065 0.60863453 0.5701952 0.51864678 0.47732261 0.45567483 0.43344739][0.454895 0.43545839 0.39225873 0.37302342 0.39068761 0.44414183 0.51899844 0.59159088 0.63093692 0.62230295 0.57131189 0.49449989 0.41731158 0.35830691 0.31019792][0.43838894 0.39093134 0.33177003 0.31528261 0.35173556 0.43067569 0.52757239 0.61141968 0.6502915 0.63223261 0.56485367 0.46375936 0.35492295 0.26365486 0.19393873][0.45202902 0.38018587 0.30736083 0.29167441 0.34079143 0.43604156 0.54504567 0.63155347 0.66386908 0.6325742 0.54831576 0.42813554 0.29844436 0.18871324 0.10966944][0.47179538 0.38164175 0.29611582 0.27524653 0.32337111 0.41543025 0.51685989 0.59179366 0.61206341 0.57084 0.48016 0.35714447 0.22675116 0.11925916 0.047603764][0.47159445 0.36761519 0.27049506 0.23866691 0.27133018 0.33868867 0.40996876 0.45768467 0.4618682 0.419331 0.3398672 0.23632985 0.12803748 0.042269167 -0.0083269961][0.45965651 0.34594393 0.23844689 0.19170487 0.19963408 0.23004462 0.25999984 0.27570581 0.26735291 0.23449573 0.18117534 0.11250181 0.03982617 -0.014742523 -0.039825838][0.44394645 0.33387423 0.22390644 0.1643078 0.14530161 0.13692249 0.12674789 0.11599399 0.10363725 0.088961467 0.067881979 0.037054811 0.00030423739 -0.025578447 -0.030550187][0.42293593 0.32742372 0.22508836 0.15988889 0.12027496 0.081673406 0.041640058 0.013734826 0.0030545504 0.0063268435 0.012048608 0.010708104 0.0011602021 -0.0043619541 0.0025761109][0.39004353 0.31515738 0.22722279 0.16360505 0.11394881 0.060444973 0.0069864164 -0.026617432 -0.033038829 -0.018920705 0.00031404116 0.013142348 0.017268658 0.021976914 0.033753283][0.34128061 0.28920025 0.22129914 0.16693638 0.11995911 0.069490284 0.021048252 -0.0063917008 -0.0072270627 0.0089654848 0.027300797 0.039376669 0.045497064 0.052960444 0.065027043]]...]
INFO - root - 2017-12-11 08:26:29.968965: step 33510, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:38m:40s remains)
INFO - root - 2017-12-11 08:26:35.359516: step 33520, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 44h:34m:09s remains)
INFO - root - 2017-12-11 08:26:40.729668: step 33530, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 46h:48m:11s remains)
INFO - root - 2017-12-11 08:26:46.020661: step 33540, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:09m:25s remains)
INFO - root - 2017-12-11 08:26:51.502244: step 33550, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 43h:55m:19s remains)
INFO - root - 2017-12-11 08:26:56.857289: step 33560, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:44m:11s remains)
INFO - root - 2017-12-11 08:27:02.284999: step 33570, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 45h:57m:02s remains)
INFO - root - 2017-12-11 08:27:07.677482: step 33580, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:17m:18s remains)
INFO - root - 2017-12-11 08:27:12.760782: step 33590, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:02m:03s remains)
INFO - root - 2017-12-11 08:27:18.091577: step 33600, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 45h:19m:13s remains)
2017-12-11 08:27:18.625244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.023320809 0.011276208 0.057394054 0.10414542 0.14018057 0.16133775 0.17311896 0.17970251 0.18018317 0.17350481 0.16122068 0.14234839 0.11871589 0.097392946 0.0875725][-0.02388815 0.012844052 0.064492494 0.11884095 0.16215368 0.18722939 0.19757681 0.19750243 0.189412 0.17745551 0.16565919 0.15096149 0.13070159 0.10985992 0.098266773][-0.016909439 0.023088684 0.079709128 0.13950431 0.18718812 0.21333516 0.21953014 0.20959003 0.18879369 0.1669904 0.15198787 0.14066657 0.12640385 0.10998157 0.099115029][-0.0022954445 0.043667894 0.10675611 0.17209348 0.2234228 0.25001243 0.25180963 0.23257798 0.19954795 0.16660774 0.14489958 0.13302909 0.12281381 0.11101609 0.10164508][0.014275807 0.066716693 0.13551761 0.20529145 0.26022023 0.2892693 0.29066232 0.26746118 0.22772953 0.18719462 0.15924819 0.14533028 0.13788018 0.13054062 0.12271535][0.025859598 0.082437389 0.15423019 0.22699946 0.28677619 0.32299578 0.33204496 0.31478575 0.277912 0.2365803 0.20561381 0.19028267 0.18490641 0.18117465 0.17383383][0.025322022 0.08006157 0.14951706 0.22206654 0.28648147 0.33315834 0.3571769 0.35621697 0.33289906 0.29981875 0.272348 0.25987026 0.2585851 0.25898045 0.25228441][0.013352936 0.05992097 0.12126114 0.18877839 0.2539925 0.30838507 0.34761739 0.3667911 0.36363709 0.34639111 0.32921219 0.32401478 0.32870743 0.33406022 0.33046755][-0.0030484239 0.032833587 0.082512394 0.14001724 0.19886129 0.25147972 0.29619965 0.32827389 0.3420293 0.34088156 0.33599213 0.33896178 0.34896553 0.35869765 0.36002082][-0.021453515 0.004703667 0.042937502 0.088343248 0.13558327 0.17786138 0.21631339 0.24843206 0.26788375 0.27495345 0.27735561 0.28406447 0.29466638 0.30533743 0.3108772][-0.043632936 -0.026399152 0.0016292954 0.035225503 0.069413356 0.098063178 0.12350719 0.14526439 0.15893762 0.16464745 0.16790809 0.17437756 0.18249144 0.19113575 0.19822243][-0.065946132 -0.057005458 -0.037546668 -0.013838417 0.0087154815 0.024399636 0.035210174 0.041565791 0.042043984 0.039077874 0.03785152 0.041746981 0.047459804 0.054453272 0.062518291][-0.079535268 -0.07651256 -0.063447833 -0.046789639 -0.032376602 -0.02577403 -0.02622905 -0.033260759 -0.045949914 -0.05935546 -0.067825608 -0.068771377 -0.066267096 -0.06112849 -0.053429849][-0.082164384 -0.0825767 -0.074396655 -0.063016772 -0.053814806 -0.052079663 -0.058377728 -0.072445117 -0.092194676 -0.11195928 -0.12629478 -0.1326717 -0.13433866 -0.13226588 -0.1270079][-0.079827853 -0.082729347 -0.07875143 -0.071980424 -0.066212609 -0.065885849 -0.072676226 -0.086706437 -0.10570287 -0.12487015 -0.14004734 -0.14878525 -0.15262601 -0.15249988 -0.14934896]]...]
INFO - root - 2017-12-11 08:27:23.887902: step 33610, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.519 sec/batch; 43h:06m:13s remains)
INFO - root - 2017-12-11 08:27:29.257614: step 33620, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:10m:45s remains)
INFO - root - 2017-12-11 08:27:34.601013: step 33630, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 42h:54m:06s remains)
INFO - root - 2017-12-11 08:27:39.925577: step 33640, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:55m:19s remains)
INFO - root - 2017-12-11 08:27:45.275960: step 33650, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 45h:50m:55s remains)
INFO - root - 2017-12-11 08:27:50.608789: step 33660, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:38m:10s remains)
INFO - root - 2017-12-11 08:27:55.947775: step 33670, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 43h:39m:25s remains)
INFO - root - 2017-12-11 08:28:01.262001: step 33680, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:44m:57s remains)
INFO - root - 2017-12-11 08:28:06.548123: step 33690, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.456 sec/batch; 37h:52m:33s remains)
INFO - root - 2017-12-11 08:28:11.564468: step 33700, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.588 sec/batch; 48h:49m:13s remains)
2017-12-11 08:28:12.121572: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3335847 0.36278817 0.36402944 0.34876239 0.32791504 0.31293994 0.31827393 0.34228542 0.37428963 0.39853165 0.40352651 0.38452327 0.33873817 0.2738286 0.20787968][0.338516 0.38375434 0.39772367 0.38829878 0.3667295 0.35043022 0.35499474 0.37977192 0.41392258 0.43866715 0.43915626 0.41049564 0.3534525 0.27804866 0.20531477][0.30526251 0.37039974 0.40296617 0.40643415 0.39133951 0.37824276 0.38407111 0.40788966 0.43831483 0.45401296 0.43972018 0.39420643 0.32487726 0.24563569 0.17768876][0.27545553 0.35176417 0.39975095 0.41781288 0.41415232 0.4092139 0.41935378 0.44276157 0.46610281 0.46630183 0.43054843 0.36467764 0.284238 0.20629264 0.1482808][0.24906413 0.32778615 0.38472873 0.41581148 0.42544386 0.43048254 0.44444641 0.46447945 0.4755953 0.45750663 0.40301728 0.32472873 0.2427327 0.1734892 0.12750153][0.2273604 0.30406407 0.36666024 0.40946653 0.43112904 0.44341147 0.45721623 0.4685148 0.46181422 0.42422929 0.35694209 0.27691692 0.20362204 0.14980824 0.11862931][0.20979464 0.27892548 0.34315243 0.39624849 0.42998791 0.4493064 0.46277446 0.46581644 0.44394428 0.39203742 0.31908393 0.24382745 0.18296088 0.14508179 0.12762441][0.19471708 0.24779719 0.30517393 0.36408174 0.40929848 0.43612915 0.45013618 0.44845259 0.41943166 0.36412805 0.29579058 0.23280907 0.18779483 0.16488409 0.15747733][0.19893421 0.22870547 0.27001265 0.32617062 0.375354 0.40296772 0.41242036 0.40546203 0.37555555 0.32814655 0.27661464 0.23654382 0.21475504 0.2094451 0.21020119][0.21770683 0.22674473 0.2498571 0.29597628 0.33902088 0.35805482 0.35661358 0.34291342 0.31700325 0.28602546 0.25957814 0.24834245 0.25345749 0.26701942 0.27490127][0.23478194 0.23343885 0.24562421 0.28140292 0.31348813 0.3190231 0.30265218 0.27971095 0.25851515 0.24674954 0.2478997 0.26548344 0.29507142 0.32355806 0.33449158][0.25322357 0.25651565 0.26919723 0.29891232 0.31973854 0.30895573 0.27138302 0.23116136 0.20877573 0.21344081 0.24181321 0.28759906 0.33929512 0.37800059 0.3856653][0.28364408 0.30303332 0.32426804 0.35024884 0.35794693 0.32647669 0.26101434 0.19605571 0.16539782 0.17994094 0.2292717 0.2973595 0.36570841 0.41032097 0.41207179][0.31350738 0.35223398 0.38309616 0.40259951 0.39372122 0.34087807 0.25013536 0.16427642 0.12692183 0.14794487 0.20896925 0.28657445 0.3592824 0.4021613 0.39673203][0.311922 0.36465007 0.39943478 0.40801841 0.38233754 0.31427404 0.21013221 0.11546527 0.07820008 0.10506401 0.1705621 0.24738042 0.31521124 0.35200918 0.34124026]]...]
INFO - root - 2017-12-11 08:28:17.487286: step 33710, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 43h:49m:47s remains)
INFO - root - 2017-12-11 08:28:22.830833: step 33720, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:13m:44s remains)
INFO - root - 2017-12-11 08:28:28.190426: step 33730, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:35m:27s remains)
INFO - root - 2017-12-11 08:28:33.620175: step 33740, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:55m:23s remains)
INFO - root - 2017-12-11 08:28:38.909467: step 33750, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.517 sec/batch; 42h:55m:42s remains)
INFO - root - 2017-12-11 08:28:44.349269: step 33760, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.542 sec/batch; 45h:00m:35s remains)
INFO - root - 2017-12-11 08:28:49.669994: step 33770, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:36m:37s remains)
INFO - root - 2017-12-11 08:28:55.088011: step 33780, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 44h:22m:43s remains)
INFO - root - 2017-12-11 08:29:00.353355: step 33790, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:12m:56s remains)
INFO - root - 2017-12-11 08:29:05.389485: step 33800, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:22m:15s remains)
2017-12-11 08:29:05.943107: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056233745 0.069872312 0.095517077 0.12561992 0.15632744 0.19000864 0.2310175 0.2760804 0.31221637 0.32278597 0.30296138 0.25885254 0.20023726 0.13710558 0.079765275][0.073858768 0.0848319 0.11386959 0.15884669 0.21450029 0.27503219 0.33383319 0.37837386 0.3942115 0.37268379 0.32126734 0.25445303 0.1823083 0.11245643 0.053662475][0.098847762 0.11086618 0.14405338 0.20039192 0.27533397 0.35800898 0.43191323 0.47483492 0.47052404 0.41867995 0.33853793 0.25222698 0.171043 0.10074329 0.046842195][0.12210062 0.13827476 0.17612472 0.24004567 0.32742462 0.42623839 0.51383573 0.56105942 0.5478456 0.47671297 0.37305176 0.26616865 0.17338327 0.10189971 0.053468294][0.14002147 0.15580228 0.19251503 0.25782588 0.35088915 0.45994729 0.55969954 0.61739385 0.60814506 0.53110355 0.41301364 0.28983003 0.18758366 0.11671897 0.0747922][0.15536551 0.1620854 0.18774737 0.24495286 0.33460811 0.44545102 0.55321717 0.62473232 0.63089615 0.56375921 0.44690546 0.320756 0.21923544 0.15601833 0.12403655][0.16694129 0.15696406 0.16376676 0.20560896 0.28667179 0.39732471 0.5155496 0.6067782 0.6364063 0.58788067 0.47870496 0.353485 0.25535497 0.20306778 0.18536362][0.17899337 0.14500709 0.12651935 0.14842419 0.22068872 0.33646891 0.4737334 0.59190357 0.64753455 0.61538976 0.5066157 0.37206265 0.26779166 0.22084045 0.21713877][0.19677636 0.13328478 0.085220933 0.085491739 0.14932404 0.27129409 0.4263947 0.56664282 0.64031959 0.61610514 0.50159067 0.35343671 0.23888314 0.19236207 0.19868183][0.20896934 0.11864999 0.044684365 0.02616642 0.080278583 0.20011799 0.35740039 0.50138408 0.57839626 0.556381 0.44240761 0.29351521 0.17890479 0.13493684 0.14687119][0.20297056 0.094173171 0.0049122926 -0.025850415 0.017740933 0.12676615 0.27163291 0.40283954 0.47091654 0.44767332 0.34074563 0.20254274 0.097222328 0.059446268 0.075896539][0.17818081 0.062168412 -0.034268141 -0.0743451 -0.044646159 0.045672342 0.16826886 0.27774116 0.331445 0.30658671 0.21056318 0.088760473 -0.0028821984 -0.032265946 -0.010315469][0.14189994 0.028580075 -0.0677448 -0.11607058 -0.10596912 -0.044731859 0.044098672 0.12360226 0.16099754 0.13945805 0.063797005 -0.030744622 -0.10022498 -0.11744181 -0.090600289][0.0945814 -0.0049355919 -0.0898655 -0.13901581 -0.14695054 -0.11872463 -0.070363693 -0.026001206 -0.0058194394 -0.020111252 -0.067101747 -0.12444811 -0.16346705 -0.16555992 -0.13673884][0.043202631 -0.035980538 -0.10239498 -0.14539678 -0.16459227 -0.16254711 -0.14736781 -0.13106824 -0.1232692 -0.12918696 -0.14942837 -0.17275904 -0.18371819 -0.17269082 -0.14435278]]...]
INFO - root - 2017-12-11 08:29:11.311105: step 33810, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 42h:50m:43s remains)
INFO - root - 2017-12-11 08:29:16.589649: step 33820, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 43h:59m:40s remains)
INFO - root - 2017-12-11 08:29:22.033092: step 33830, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 43h:17m:26s remains)
INFO - root - 2017-12-11 08:29:27.364123: step 33840, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:08m:19s remains)
INFO - root - 2017-12-11 08:29:32.710468: step 33850, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:18m:11s remains)
INFO - root - 2017-12-11 08:29:37.973025: step 33860, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 42h:57m:20s remains)
INFO - root - 2017-12-11 08:29:43.236301: step 33870, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 43h:48m:14s remains)
INFO - root - 2017-12-11 08:29:48.554144: step 33880, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:44m:29s remains)
INFO - root - 2017-12-11 08:29:53.901316: step 33890, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 43h:46m:32s remains)
INFO - root - 2017-12-11 08:29:58.932351: step 33900, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 43h:55m:58s remains)
2017-12-11 08:29:59.443591: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26702449 0.23406377 0.18930252 0.16956192 0.17773785 0.20102301 0.22629298 0.24530669 0.2531988 0.25202081 0.24161041 0.22722144 0.21470258 0.20895769 0.21085444][0.312763 0.27633879 0.2245581 0.19969785 0.20719676 0.23370428 0.26350808 0.28500402 0.29104269 0.28442231 0.26850128 0.25155464 0.2387255 0.23201108 0.23114234][0.32135177 0.28626588 0.23534639 0.21159971 0.22123945 0.25149405 0.28402337 0.303717 0.30250949 0.2871615 0.26614827 0.25027081 0.24166386 0.23731373 0.23488873][0.30170661 0.27225965 0.22964686 0.21526149 0.23394696 0.27215558 0.30814052 0.32290792 0.30923086 0.28057534 0.25291178 0.23934376 0.238091 0.23923065 0.23802947][0.26544833 0.24293993 0.2129921 0.21317689 0.24530995 0.29375356 0.33349088 0.34174892 0.3134729 0.26928407 0.23347914 0.22189817 0.22877602 0.23713118 0.23897521][0.22232112 0.20875816 0.19435392 0.21178602 0.25856832 0.31639221 0.35817322 0.358967 0.31660736 0.25771716 0.21424268 0.20456961 0.21950892 0.23573141 0.2419702][0.18250647 0.17516206 0.17486754 0.20627829 0.26360434 0.3275491 0.37065667 0.36661905 0.31471479 0.24511303 0.19487187 0.18534893 0.20571111 0.22897182 0.241178][0.15599042 0.15396035 0.16395432 0.20245512 0.26299068 0.32846233 0.37308291 0.368949 0.31410968 0.2387525 0.1821049 0.16858722 0.18868034 0.21581501 0.23479296][0.16623722 0.16947 0.1843148 0.22048961 0.27288386 0.32974139 0.37008473 0.36618802 0.31300062 0.23767489 0.177735 0.15855904 0.17318553 0.19902684 0.22251518][0.19952027 0.20901503 0.22435416 0.25049293 0.28434 0.32148379 0.34911874 0.34298676 0.2949338 0.22708581 0.17164378 0.15094645 0.16000251 0.18191522 0.20699741][0.22393741 0.24075438 0.25557867 0.26935127 0.28066018 0.29310122 0.30395973 0.29504842 0.25568461 0.20180391 0.15832734 0.14208104 0.14954858 0.16850956 0.19341168][0.21446058 0.23776881 0.2512849 0.25398421 0.24567197 0.23585685 0.23156188 0.22195177 0.19475462 0.15876569 0.13050076 0.12143014 0.12998284 0.14768422 0.17189988][0.17714839 0.20282155 0.21505652 0.21104121 0.19126503 0.16730145 0.15224051 0.14225006 0.12553261 0.10520322 0.090130515 0.087282687 0.097239234 0.11526506 0.14010619][0.12453994 0.14953665 0.16039529 0.15350853 0.13028531 0.10169554 0.081842333 0.07168068 0.061848827 0.052432809 0.047493115 0.050282791 0.0627277 0.082932428 0.11004894][0.080023453 0.10208311 0.11077191 0.10343628 0.082983084 0.058573697 0.041470852 0.033475757 0.028158147 0.02527958 0.027259642 0.035465065 0.051750761 0.07557641 0.10483555]]...]
INFO - root - 2017-12-11 08:30:04.737640: step 33910, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 43h:27m:23s remains)
INFO - root - 2017-12-11 08:30:10.046590: step 33920, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 43h:55m:23s remains)
INFO - root - 2017-12-11 08:30:15.307423: step 33930, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 43h:15m:35s remains)
INFO - root - 2017-12-11 08:30:20.708549: step 33940, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:24m:24s remains)
INFO - root - 2017-12-11 08:30:26.001182: step 33950, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 43h:09m:18s remains)
INFO - root - 2017-12-11 08:30:31.365304: step 33960, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 43h:15m:19s remains)
INFO - root - 2017-12-11 08:30:36.725679: step 33970, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 45h:30m:08s remains)
INFO - root - 2017-12-11 08:30:42.092339: step 33980, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:27m:18s remains)
INFO - root - 2017-12-11 08:30:47.347960: step 33990, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:49m:23s remains)
INFO - root - 2017-12-11 08:30:52.716351: step 34000, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:21m:53s remains)
2017-12-11 08:30:53.160422: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.040319826 0.065023854 0.085232548 0.093019418 0.08928562 0.07627558 0.062016994 0.050812434 0.044187885 0.044633228 0.051662315 0.072081193 0.10966583 0.16098045 0.20908479][0.073799506 0.11432782 0.15113081 0.17136879 0.17343542 0.1585921 0.13769506 0.11874436 0.10459339 0.099289492 0.10404126 0.12901174 0.17732354 0.24159333 0.300471][0.099045143 0.15616766 0.21185596 0.24841766 0.26028872 0.2473183 0.22166508 0.1952323 0.17174302 0.15658912 0.15368512 0.17755118 0.23036301 0.29985642 0.36303076][0.10917435 0.17966694 0.25253749 0.30561683 0.32942715 0.3231146 0.29855111 0.26941881 0.2381608 0.21058781 0.19397676 0.20814164 0.25517079 0.31917357 0.37892032][0.10609581 0.18469255 0.26965532 0.33591837 0.37187955 0.37821859 0.36529568 0.34393412 0.31137636 0.27273449 0.23927248 0.23667607 0.2681686 0.31672451 0.36546752][0.096394107 0.17854388 0.26925966 0.34196967 0.38584024 0.40691963 0.413584 0.40972251 0.38489893 0.34237996 0.29648525 0.27824223 0.29277059 0.32308602 0.35653785][0.087233387 0.17030442 0.26058975 0.33160824 0.37594056 0.40771943 0.43290424 0.44740024 0.435027 0.39716029 0.34848219 0.32192057 0.32350519 0.33704364 0.35366529][0.080060981 0.16223684 0.2472913 0.31043568 0.3491869 0.38547555 0.42233267 0.44895792 0.44801939 0.42044991 0.37831336 0.35132289 0.34478796 0.34449622 0.34522229][0.072735995 0.15053304 0.2257199 0.27676514 0.30585986 0.3396911 0.37858132 0.40820649 0.41490334 0.40065551 0.37254554 0.3521736 0.34300798 0.33461583 0.32535902][0.05527138 0.12306629 0.18391813 0.22030909 0.23758802 0.26263991 0.29346234 0.31651723 0.32516164 0.32309014 0.31150192 0.30244339 0.29709029 0.28882039 0.27855587][0.021668443 0.073955908 0.11743465 0.13895416 0.14500318 0.15823905 0.17477091 0.18483661 0.189585 0.19376256 0.19394976 0.19462952 0.19560401 0.19273585 0.1879579][-0.019904347 0.014528563 0.041343004 0.051297348 0.049580824 0.051904082 0.054047633 0.051197588 0.049168807 0.051949345 0.05439467 0.056927525 0.059857123 0.061089553 0.062986039][-0.055782072 -0.037991997 -0.02483982 -0.022598505 -0.028522169 -0.033781573 -0.041407615 -0.051914833 -0.058808316 -0.059898626 -0.060103472 -0.060166553 -0.058432665 -0.055366114 -0.048642375][-0.07841038 -0.074018046 -0.070748128 -0.07321471 -0.080697492 -0.089283861 -0.10054024 -0.11267459 -0.12082383 -0.12435006 -0.12706135 -0.12931739 -0.12874177 -0.12506397 -0.1164398][-0.087425329 -0.09126848 -0.093509935 -0.09756 -0.10356374 -0.11080645 -0.12010351 -0.12942496 -0.13612968 -0.14044954 -0.14482917 -0.14856872 -0.14946218 -0.14708614 -0.14011891]]...]
INFO - root - 2017-12-11 08:30:58.385428: step 34010, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:40m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:31:03.820700: step 34020, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 44h:52m:10s remains)
INFO - root - 2017-12-11 08:31:09.228052: step 34030, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:29m:36s remains)
INFO - root - 2017-12-11 08:31:14.615701: step 34040, loss = 0.67, batch loss = 0.61 (15.4 examples/sec; 0.519 sec/batch; 43h:01m:47s remains)
INFO - root - 2017-12-11 08:31:19.862363: step 34050, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:04m:24s remains)
INFO - root - 2017-12-11 08:31:25.278193: step 34060, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.521 sec/batch; 43h:13m:34s remains)
INFO - root - 2017-12-11 08:31:30.540239: step 34070, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.510 sec/batch; 42h:17m:29s remains)
INFO - root - 2017-12-11 08:31:35.887114: step 34080, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:02m:29s remains)
INFO - root - 2017-12-11 08:31:41.265296: step 34090, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:26m:54s remains)
INFO - root - 2017-12-11 08:31:46.676903: step 34100, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:31m:56s remains)
2017-12-11 08:31:47.235319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033854563 -0.035006084 -0.034422562 -0.031086221 -0.02365873 -0.013713418 -0.003696643 0.0044972897 0.0068824561 0.0018311525 -0.0082961079 -0.019976804 -0.031327225 -0.039251193 -0.041013639][-0.048159894 -0.047691077 -0.043129947 -0.033510666 -0.017731912 0.00027991869 0.015458228 0.023869287 0.021703092 0.0081484122 -0.012220566 -0.033343915 -0.051335264 -0.063602529 -0.070379965][-0.066753753 -0.061761159 -0.04896358 -0.026890676 0.0049040397 0.038542949 0.06442064 0.075308651 0.066772878 0.0393979 0.0013694246 -0.036138386 -0.065744147 -0.085295185 -0.098048322][-0.073567338 -0.060803313 -0.035241287 0.0056593944 0.061301198 0.11828277 0.16048551 0.17585932 0.15867193 0.11181435 0.04873896 -0.012012113 -0.058180943 -0.087570354 -0.10598034][-0.061053913 -0.03823781 0.002720379 0.066172592 0.15041167 0.23562247 0.29778278 0.31907716 0.29204574 0.22264326 0.13035494 0.042329717 -0.023758026 -0.065041229 -0.089357667][-0.04188174 -0.010142788 0.045054149 0.13036725 0.24334177 0.35761082 0.44061363 0.46858048 0.43225703 0.34095058 0.21998979 0.10509032 0.01882381 -0.034783363 -0.06484244][-0.030306313 0.0064002271 0.071212053 0.17266239 0.30740926 0.4440558 0.54311836 0.57648838 0.53352678 0.42659077 0.28535891 0.15141676 0.050335586 -0.012808968 -0.047560222][-0.022578599 0.015015786 0.084312886 0.19402781 0.33910763 0.48577309 0.59111041 0.62641221 0.5801177 0.4663454 0.31699505 0.17545351 0.067735136 -0.00057865144 -0.038122859][-0.010572358 0.026162522 0.095056817 0.20213467 0.341103 0.48002982 0.57820863 0.61043692 0.56444418 0.45409149 0.31047067 0.17415564 0.069378592 0.0018113557 -0.034778688][0.0092907716 0.045185991 0.10952766 0.20372841 0.32111079 0.43503758 0.51272058 0.53600734 0.49260086 0.39425036 0.26826924 0.14845186 0.055221133 -0.00575309 -0.037169427][0.036119554 0.072795682 0.1311474 0.20697702 0.29329512 0.37064925 0.41825733 0.42763135 0.38826847 0.30778128 0.20665158 0.10943259 0.032083515 -0.019378494 -0.043919567][0.062443707 0.10330199 0.15738595 0.21470341 0.26832294 0.30744493 0.32450923 0.31954721 0.28461584 0.22264035 0.14639872 0.071099013 0.0087216608 -0.033684604 -0.052246634][0.074947022 0.12043648 0.17117259 0.2128513 0.23929137 0.24789928 0.24208423 0.22751945 0.19833645 0.15301014 0.097557828 0.039960008 -0.010585743 -0.045840129 -0.060335636][0.063716233 0.10898059 0.15369116 0.1816248 0.18792363 0.17648603 0.15749583 0.13860393 0.11585005 0.08541327 0.048853628 0.0088249706 -0.028611116 -0.055397566 -0.066238888][0.029273354 0.066382684 0.099469334 0.11371213 0.10649163 0.086367927 0.06506934 0.048365306 0.033828165 0.018209688 0.0013502217 -0.018219164 -0.039044492 -0.055464283 -0.063541017]]...]
INFO - root - 2017-12-11 08:31:52.248061: step 34110, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 43h:11m:54s remains)
INFO - root - 2017-12-11 08:31:57.672076: step 34120, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:45m:11s remains)
INFO - root - 2017-12-11 08:32:03.029695: step 34130, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 43h:15m:49s remains)
INFO - root - 2017-12-11 08:32:08.462169: step 34140, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 45h:16m:18s remains)
INFO - root - 2017-12-11 08:32:13.867371: step 34150, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 43h:13m:36s remains)
INFO - root - 2017-12-11 08:32:19.141476: step 34160, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.509 sec/batch; 42h:11m:08s remains)
INFO - root - 2017-12-11 08:32:24.542528: step 34170, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 44h:24m:32s remains)
INFO - root - 2017-12-11 08:32:29.994086: step 34180, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 44h:51m:57s remains)
INFO - root - 2017-12-11 08:32:35.371156: step 34190, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:57m:14s remains)
INFO - root - 2017-12-11 08:32:40.833719: step 34200, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:02m:06s remains)
2017-12-11 08:32:41.389193: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31645286 0.35737419 0.39046603 0.41608989 0.42978492 0.42893159 0.41458756 0.38156959 0.33296347 0.29386967 0.28184208 0.29664242 0.32247561 0.3397226 0.33503026][0.35102496 0.38081303 0.40503517 0.42753252 0.44095895 0.44021317 0.4256961 0.39414033 0.35058504 0.314623 0.29887861 0.30311728 0.31210521 0.31536683 0.30019656][0.37005538 0.38164204 0.38910651 0.40127477 0.41272989 0.41513884 0.40543577 0.38130915 0.34907362 0.32421234 0.31223074 0.31109735 0.30888137 0.30031574 0.27468848][0.38128236 0.37642521 0.36631414 0.36638921 0.37507442 0.38190809 0.37985834 0.36680168 0.34912026 0.33827889 0.33441645 0.33489919 0.32924861 0.31520805 0.28360787][0.38149685 0.36634579 0.34287477 0.33341223 0.34072211 0.35395271 0.36252326 0.36273208 0.35957488 0.36069864 0.3642917 0.3690218 0.36462241 0.34983003 0.31678113][0.37439412 0.35512111 0.32335666 0.30737793 0.31399703 0.33442482 0.35531038 0.36977947 0.37875149 0.38623294 0.39193311 0.39814335 0.39570391 0.38338324 0.35428387][0.36289549 0.34317145 0.30794933 0.28941396 0.29723012 0.32559791 0.35909197 0.38671783 0.4037537 0.41098914 0.4112303 0.41219562 0.40903163 0.40124056 0.38086891][0.34825298 0.33037618 0.29560524 0.27703491 0.28543794 0.31875196 0.36080855 0.39808667 0.42086276 0.42656276 0.41995594 0.4130533 0.40667957 0.40165919 0.38832474][0.32837787 0.31478739 0.2836211 0.26661095 0.27355695 0.30599996 0.34908137 0.38983831 0.41527012 0.41971174 0.40791669 0.39377034 0.38279244 0.37657711 0.36480775][0.30803433 0.29570821 0.26679632 0.24980028 0.25238362 0.27878267 0.31640893 0.35502213 0.38055208 0.38520533 0.37179053 0.35259131 0.33650759 0.32660872 0.31386086][0.29157162 0.27676812 0.24644716 0.22656639 0.22330759 0.24208455 0.27178192 0.30456513 0.32706279 0.33198354 0.31988037 0.29923329 0.28062227 0.26875606 0.25685561][0.27717024 0.25895661 0.22629465 0.20355175 0.19606769 0.20900333 0.23198469 0.25829902 0.27619642 0.28119791 0.27211037 0.25312328 0.23486336 0.22324738 0.21338941][0.26448837 0.24526417 0.21233428 0.18847808 0.17899349 0.18785146 0.20555894 0.22539787 0.23830192 0.24368228 0.23902301 0.22388221 0.20710857 0.19537713 0.18648158][0.25011259 0.2337967 0.20404086 0.1819478 0.17347836 0.18044567 0.19419341 0.20693929 0.21313764 0.21626584 0.21356517 0.20205829 0.18822657 0.178443 0.17212094][0.23236319 0.22161302 0.1985728 0.18147175 0.17721063 0.18493462 0.19592051 0.2009863 0.19877286 0.19616856 0.19147071 0.18229975 0.17369545 0.17082013 0.17178364]]...]
INFO - root - 2017-12-11 08:32:46.387916: step 34210, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.513 sec/batch; 42h:29m:03s remains)
INFO - root - 2017-12-11 08:32:51.723545: step 34220, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:11m:02s remains)
INFO - root - 2017-12-11 08:32:57.109884: step 34230, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:06m:56s remains)
INFO - root - 2017-12-11 08:33:02.401068: step 34240, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 44h:42m:41s remains)
INFO - root - 2017-12-11 08:33:07.673187: step 34250, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 43h:24m:04s remains)
INFO - root - 2017-12-11 08:33:12.953635: step 34260, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:20m:55s remains)
INFO - root - 2017-12-11 08:33:18.282961: step 34270, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:36m:39s remains)
INFO - root - 2017-12-11 08:33:23.638756: step 34280, loss = 0.70, batch loss = 0.64 (15.6 examples/sec; 0.514 sec/batch; 42h:35m:34s remains)
INFO - root - 2017-12-11 08:33:29.022626: step 34290, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 44h:33m:09s remains)
INFO - root - 2017-12-11 08:33:34.464186: step 34300, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:27m:36s remains)
2017-12-11 08:33:34.981355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.046170056 -0.050483868 -0.04315922 -0.02990545 -0.017985461 -0.013961489 -0.021049704 -0.038657572 -0.060141385 -0.076454088 -0.080470279 -0.068323024 -0.046437107 -0.023088489 -0.0062627723][-0.029615982 -0.018361337 0.007727094 0.040076446 0.065194294 0.07130345 0.054443069 0.018798921 -0.022901205 -0.05719867 -0.074427873 -0.069669634 -0.052090187 -0.031936869 -0.016245633][0.0027700749 0.03486954 0.086869694 0.14644966 0.1918907 0.20329188 0.17429854 0.11407305 0.044039804 -0.015410776 -0.051900182 -0.060217708 -0.05168977 -0.038358442 -0.026295675][0.036055706 0.090413474 0.17160961 0.26150751 0.329623 0.34804341 0.3091324 0.22731648 0.13166061 0.049892027 -0.004028759 -0.026485937 -0.030358849 -0.028271677 -0.025051989][0.059657279 0.13171817 0.23884702 0.35462061 0.44103613 0.46646738 0.42519939 0.33541921 0.22850801 0.13622126 0.072037958 0.035974763 0.014856637 -0.00024560548 -0.0099703986][0.0561814 0.13854441 0.26389876 0.39848959 0.49971807 0.53813773 0.51138943 0.4360168 0.33746657 0.24652188 0.17578489 0.12271279 0.077706769 0.04083984 0.01833358][0.022227235 0.10750549 0.24069731 0.38424343 0.4957349 0.55359685 0.558168 0.51688045 0.44216323 0.35979652 0.28307945 0.21110673 0.14186643 0.085736915 0.054402705][-0.025877863 0.056128122 0.1878453 0.33200911 0.450074 0.5301249 0.57246178 0.57102484 0.52602559 0.45877224 0.38480067 0.30661342 0.22764345 0.1629473 0.12592424][-0.075251184 -0.0056504365 0.11231886 0.2474253 0.36748269 0.46714386 0.54351681 0.57807755 0.56607318 0.5269962 0.47744438 0.41992921 0.35527706 0.29517731 0.25305727][-0.11502693 -0.064973444 0.028998353 0.14503701 0.25924104 0.36903405 0.46676967 0.52710253 0.54882288 0.55073935 0.54573071 0.53055483 0.49671248 0.44699025 0.395341][-0.14206465 -0.11360054 -0.049103014 0.038804706 0.13609834 0.24082592 0.34262604 0.4177385 0.47252223 0.52185148 0.56684512 0.59407532 0.5857926 0.53806776 0.46800393][-0.15353641 -0.14381146 -0.10997828 -0.056505907 0.01233841 0.095337488 0.18452333 0.26543093 0.35036126 0.44398797 0.528838 0.58115268 0.57960027 0.52140766 0.42884728][-0.15036815 -0.15305494 -0.14228785 -0.11862707 -0.080096588 -0.026115159 0.041308615 0.11941815 0.22089472 0.33798158 0.43774581 0.49121308 0.48052669 0.41033682 0.30628955][-0.13555329 -0.14412208 -0.14634036 -0.14205787 -0.12775101 -0.10120633 -0.059026476 0.0043667755 0.098011129 0.20633478 0.29224828 0.32852143 0.30436495 0.23219083 0.1362849][-0.10505118 -0.112487 -0.11834165 -0.12367004 -0.1259201 -0.12168882 -0.10481614 -0.066848435 -0.0029994317 0.070955217 0.12446453 0.13650107 0.10280757 0.039597049 -0.034281123]]...]
INFO - root - 2017-12-11 08:33:40.096766: step 34310, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.529 sec/batch; 43h:51m:29s remains)
INFO - root - 2017-12-11 08:33:45.428088: step 34320, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:58m:47s remains)
INFO - root - 2017-12-11 08:33:50.779791: step 34330, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 43h:21m:28s remains)
INFO - root - 2017-12-11 08:33:56.031384: step 34340, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 42h:47m:53s remains)
INFO - root - 2017-12-11 08:34:01.411195: step 34350, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 44h:04m:52s remains)
INFO - root - 2017-12-11 08:34:06.960942: step 34360, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 47h:37m:17s remains)
INFO - root - 2017-12-11 08:34:12.330302: step 34370, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:26m:47s remains)
INFO - root - 2017-12-11 08:34:17.694277: step 34380, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 43h:36m:23s remains)
INFO - root - 2017-12-11 08:34:23.008648: step 34390, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:16m:52s remains)
INFO - root - 2017-12-11 08:34:28.443642: step 34400, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:02m:22s remains)
2017-12-11 08:34:28.965066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04246838 -0.051167391 -0.057967242 -0.062249042 -0.062392261 -0.059013139 -0.054952074 -0.05243263 -0.052192286 -0.05284708 -0.053728692 -0.056025822 -0.061004262 -0.067694604 -0.074311405][0.0067592775 -0.0087300194 -0.023949444 -0.033955097 -0.035591576 -0.029235194 -0.018623803 -0.0083617419 -0.0030656368 -0.0024590574 -0.0054044155 -0.012883084 -0.02643333 -0.043094214 -0.058678091][0.086671546 0.067243494 0.043951094 0.028203469 0.02458402 0.033952974 0.053574383 0.07639461 0.0905702 0.092534885 0.083595768 0.065214224 0.035883296 0.0013590584 -0.02993069][0.17513278 0.15901521 0.13352214 0.11639453 0.11283347 0.12545104 0.15487498 0.19201347 0.21581614 0.2175965 0.19762874 0.16227047 0.11086515 0.052710209 0.0012014314][0.26477924 0.26294151 0.24454497 0.23144068 0.22898972 0.24290736 0.28043067 0.33101746 0.36368012 0.36346346 0.32834697 0.27119088 0.19284901 0.10693181 0.032225795][0.35385138 0.37717566 0.3756403 0.37033918 0.36759359 0.37907955 0.42202345 0.48396009 0.52307963 0.51748788 0.46343148 0.38136977 0.27312678 0.15704103 0.058140155][0.43393892 0.48603243 0.50611013 0.50976026 0.50502759 0.51210111 0.5596084 0.63110173 0.67361796 0.65966505 0.58531851 0.47849867 0.3408317 0.19528374 0.073867433][0.4749673 0.54861772 0.58516109 0.59310073 0.5832119 0.584448 0.63481838 0.712444 0.75602692 0.7354635 0.64726341 0.52479118 0.36866051 0.20451705 0.070320964][0.45911759 0.540147 0.58160657 0.58618629 0.56712639 0.55887789 0.60576153 0.68159974 0.72290772 0.70001978 0.60991848 0.48747152 0.33223602 0.16960093 0.03964534][0.39741245 0.47053429 0.50513679 0.50035512 0.47047824 0.45066723 0.48667648 0.55147731 0.58526409 0.56103289 0.47765335 0.36882344 0.23296286 0.092006706 -0.016650833][0.29310352 0.34592405 0.36612764 0.35168391 0.31557739 0.28864437 0.31197649 0.36079976 0.38342562 0.35766506 0.28534669 0.19759147 0.092285804 -0.013960907 -0.090009876][0.15268634 0.1797457 0.18264693 0.16055278 0.12374156 0.095455118 0.10774545 0.14025602 0.15303175 0.12969828 0.073785663 0.011521497 -0.058650374 -0.12520394 -0.16478649][0.0097858356 0.015651856 0.0057026255 -0.019652585 -0.05308168 -0.0798196 -0.078295924 -0.062627517 -0.058886036 -0.077600554 -0.1152976 -0.15239467 -0.18917759 -0.21811706 -0.22414444][-0.091022812 -0.097136043 -0.11192726 -0.13485518 -0.16144538 -0.18406563 -0.19059619 -0.18878998 -0.19269557 -0.2079311 -0.23147105 -0.25012827 -0.26302114 -0.26541871 -0.24938847][-0.14154637 -0.15151991 -0.16474466 -0.18149769 -0.19951643 -0.21543448 -0.22411241 -0.22879179 -0.2359814 -0.24814549 -0.26253065 -0.27084121 -0.27142552 -0.26135558 -0.23749861]]...]
INFO - root - 2017-12-11 08:34:34.322008: step 34410, loss = 0.72, batch loss = 0.67 (14.9 examples/sec; 0.538 sec/batch; 44h:31m:08s remains)
INFO - root - 2017-12-11 08:34:38.594730: step 34420, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 44h:33m:15s remains)
INFO - root - 2017-12-11 08:34:44.007123: step 34430, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 44h:52m:34s remains)
INFO - root - 2017-12-11 08:34:49.285213: step 34440, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 43h:01m:34s remains)
INFO - root - 2017-12-11 08:34:54.577177: step 34450, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 43h:28m:02s remains)
INFO - root - 2017-12-11 08:35:00.004291: step 34460, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:04m:32s remains)
INFO - root - 2017-12-11 08:35:05.346830: step 34470, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:02m:13s remains)
INFO - root - 2017-12-11 08:35:10.696038: step 34480, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 43h:46m:05s remains)
INFO - root - 2017-12-11 08:35:16.042970: step 34490, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:50m:59s remains)
INFO - root - 2017-12-11 08:35:21.346559: step 34500, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:20m:27s remains)
2017-12-11 08:35:21.905323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045262147 -0.0016797639 0.071168229 0.16573466 0.27064085 0.37517139 0.46228194 0.52479273 0.5789119 0.62797487 0.66367143 0.66998297 0.63754463 0.56701332 0.45479417][-0.055352289 -0.0045342259 0.081281193 0.19193529 0.31453165 0.43898654 0.54516733 0.6177848 0.66590172 0.69266158 0.69030249 0.650211 0.57175726 0.46744555 0.34121767][-0.063983709 -0.010288819 0.082314424 0.20280933 0.33711365 0.47468522 0.59319592 0.66990578 0.70433265 0.69881648 0.651293 0.5613001 0.43913752 0.30820736 0.18106754][-0.072908558 -0.019052552 0.077918753 0.20767133 0.35390076 0.50230038 0.62751377 0.70269781 0.71946633 0.68024844 0.59123719 0.46103069 0.30896717 0.16322069 0.042865612][-0.079056382 -0.025489748 0.075588092 0.21562023 0.37487361 0.5321542 0.65794808 0.7247715 0.72077137 0.64985454 0.52812129 0.37380141 0.21126252 0.066366941 -0.040692654][-0.081170306 -0.027535845 0.077060051 0.22671914 0.39784566 0.56164491 0.68350953 0.73751837 0.7122066 0.61431092 0.46947616 0.30522656 0.14819047 0.018212784 -0.068723716][-0.082241669 -0.030241 0.073715694 0.2264329 0.40142381 0.56420672 0.67636764 0.71500295 0.67224294 0.55804616 0.40467611 0.24570055 0.10759836 0.0038925936 -0.056429721][-0.084992275 -0.038386859 0.056706782 0.19933859 0.36271498 0.51023412 0.60348588 0.62463003 0.57121253 0.45510781 0.31073475 0.17307968 0.065775469 -0.0035864108 -0.033971712][-0.0900595 -0.05327123 0.02476478 0.14379343 0.27954683 0.39731294 0.46333203 0.46610579 0.40857509 0.30356503 0.18272163 0.0775965 0.0070086252 -0.026549386 -0.028649507][-0.0987417 -0.075470924 -0.020298284 0.0654307 0.16178209 0.23954086 0.27359983 0.2592665 0.20371614 0.11950172 0.031815074 -0.034835391 -0.06780991 -0.068255708 -0.046123903][-0.10966369 -0.10076929 -0.069032885 -0.018208262 0.036313932 0.072722048 0.076627 0.049678419 0.00096496777 -0.057973068 -0.10963111 -0.13806683 -0.13728119 -0.11176877 -0.073187649][-0.11338114 -0.11271676 -0.096610725 -0.071999513 -0.050149795 -0.045395847 -0.062771074 -0.096370891 -0.13601431 -0.1716743 -0.19313218 -0.19265242 -0.17017667 -0.1320789 -0.089185461][-0.10306001 -0.10146526 -0.089150742 -0.07614778 -0.070785575 -0.081639744 -0.1077559 -0.1411355 -0.1717454 -0.191351 -0.1951526 -0.18113486 -0.15290143 -0.11685464 -0.0810988][-0.08091788 -0.072098263 -0.055649746 -0.042762965 -0.040002175 -0.052674145 -0.077329077 -0.10557235 -0.12842655 -0.13941343 -0.1366073 -0.12069178 -0.096924148 -0.0709225 -0.047665223][-0.052271433 -0.033988781 -0.011201072 0.0055713071 0.01127917 0.0022237254 -0.018002812 -0.040723391 -0.057638403 -0.064040832 -0.059394393 -0.045413777 -0.028011369 -0.012311102 -0.0010854817]]...]
INFO - root - 2017-12-11 08:35:27.290132: step 34510, loss = 0.69, batch loss = 0.63 (13.6 examples/sec; 0.589 sec/batch; 48h:43m:18s remains)
INFO - root - 2017-12-11 08:35:32.446643: step 34520, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:51m:42s remains)
INFO - root - 2017-12-11 08:35:37.833656: step 34530, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:42m:32s remains)
INFO - root - 2017-12-11 08:35:43.189688: step 34540, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:10m:16s remains)
INFO - root - 2017-12-11 08:35:48.548188: step 34550, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 43h:56m:28s remains)
INFO - root - 2017-12-11 08:35:53.961340: step 34560, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:02m:36s remains)
INFO - root - 2017-12-11 08:35:59.302982: step 34570, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:35m:45s remains)
INFO - root - 2017-12-11 08:36:04.694320: step 34580, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:35m:49s remains)
INFO - root - 2017-12-11 08:36:10.176081: step 34590, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 46h:52m:17s remains)
INFO - root - 2017-12-11 08:36:15.563654: step 34600, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:29m:02s remains)
2017-12-11 08:36:16.154001: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22962135 0.1998354 0.17365316 0.15701123 0.1535463 0.15645285 0.15487345 0.14538784 0.1302045 0.11626708 0.11305685 0.12424488 0.14820132 0.1728619 0.19745356][0.21725936 0.19442165 0.17549658 0.16770513 0.17420034 0.18762031 0.19470793 0.19068168 0.17781992 0.16310206 0.1567346 0.16717166 0.19797166 0.24137223 0.29456452][0.18625174 0.17132258 0.1654492 0.1760733 0.20334217 0.23792416 0.26296124 0.27029744 0.26109752 0.24249749 0.22715892 0.23044179 0.26425862 0.3285206 0.41819012][0.1476292 0.14214653 0.15638445 0.1967572 0.25826705 0.32800594 0.38353106 0.40932253 0.40381238 0.37533504 0.34105891 0.32483882 0.34955946 0.42612591 0.54668778][0.096893556 0.10527773 0.14760546 0.22751188 0.33442709 0.45040974 0.54537296 0.59329081 0.58908594 0.54463196 0.48430565 0.43932059 0.44171712 0.51409566 0.64590627][0.045626465 0.073404267 0.14833193 0.27127749 0.42651528 0.58989173 0.72310865 0.78903568 0.77842492 0.70975018 0.61757082 0.53844625 0.50806731 0.55611467 0.67082155][0.023912676 0.071296878 0.17245214 0.32868692 0.52140349 0.72067374 0.882049 0.9590798 0.93638027 0.83905864 0.71270967 0.59776747 0.5279364 0.53338242 0.60383081][0.05265706 0.11058935 0.21952763 0.38616094 0.59209937 0.8039313 0.975134 1.0546942 1.0210695 0.90164727 0.74857968 0.60426307 0.49728173 0.45405364 0.46854112][0.13772044 0.19811133 0.29508427 0.44349787 0.62745154 0.8137818 0.96175784 1.0253289 0.98123491 0.85482633 0.69638675 0.54403442 0.41817209 0.33900696 0.30647808][0.25898647 0.31809613 0.38958633 0.49501264 0.62385672 0.74770677 0.83864594 0.86639291 0.81189096 0.69392431 0.55308032 0.418204 0.30025929 0.2107157 0.15495981][0.37609506 0.42965841 0.4691512 0.51907635 0.57617605 0.62101263 0.64143914 0.6265955 0.56467444 0.46549147 0.35665873 0.25761232 0.17074411 0.096907787 0.043272439][0.44211826 0.484303 0.48983204 0.48354298 0.4708817 0.44436666 0.40673512 0.36183774 0.30316311 0.23216048 0.16405766 0.11033154 0.065940827 0.022768075 -0.013236428][0.42920735 0.45667246 0.43577597 0.38882321 0.32848653 0.25811645 0.19003551 0.13453905 0.088962123 0.049915645 0.023613634 0.013836525 0.009089089 -0.0027085955 -0.017642083][0.35209069 0.36564529 0.33254987 0.27097714 0.1959974 0.11664623 0.048330408 0.00067129522 -0.026303414 -0.0372325 -0.030684534 -0.0093777962 0.011154553 0.020003457 0.020225694][0.25123987 0.25559577 0.22662339 0.17747371 0.1206469 0.063644789 0.018360255 -0.0092825666 -0.017716918 -0.010466568 0.011986333 0.043539558 0.068756215 0.080876634 0.083858989]]...]
INFO - root - 2017-12-11 08:36:21.568276: step 34610, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 42h:37m:17s remains)
INFO - root - 2017-12-11 08:36:26.657773: step 34620, loss = 0.69, batch loss = 0.63 (23.0 examples/sec; 0.348 sec/batch; 28h:46m:17s remains)
INFO - root - 2017-12-11 08:36:31.953816: step 34630, loss = 0.71, batch loss = 0.65 (15.8 examples/sec; 0.505 sec/batch; 41h:48m:47s remains)
INFO - root - 2017-12-11 08:36:37.307111: step 34640, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 45h:29m:03s remains)
INFO - root - 2017-12-11 08:36:42.545032: step 34650, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 43h:12m:18s remains)
INFO - root - 2017-12-11 08:36:47.881234: step 34660, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 43h:31m:46s remains)
INFO - root - 2017-12-11 08:36:53.251977: step 34670, loss = 0.66, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 43h:50m:03s remains)
INFO - root - 2017-12-11 08:36:58.552377: step 34680, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:57m:04s remains)
INFO - root - 2017-12-11 08:37:03.886418: step 34690, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:24m:52s remains)
INFO - root - 2017-12-11 08:37:09.202972: step 34700, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:24m:16s remains)
2017-12-11 08:37:09.796331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0474466 -0.049349811 -0.051009867 -0.053281453 -0.055209238 -0.056478709 -0.056928784 -0.056074563 -0.054667521 -0.053131495 -0.051742733 -0.050253604 -0.048854694 -0.048016444 -0.047513418][-0.047716502 -0.048177142 -0.048508268 -0.050114524 -0.052266929 -0.054330036 -0.055834968 -0.055808138 -0.054733958 -0.053118963 -0.051251605 -0.049434911 -0.048282683 -0.048252225 -0.048566982][-0.037378497 -0.032566853 -0.028022269 -0.02641446 -0.027858492 -0.031203028 -0.034930956 -0.037171733 -0.037859742 -0.037574183 -0.036698956 -0.036090091 -0.036816973 -0.039252147 -0.041858368][-0.014579636 0.00094402221 0.016104812 0.025686357 0.027641354 0.023769723 0.017453194 0.012003182 0.0078207431 0.0040389039 0.00050322426 -0.0038687671 -0.010260468 -0.018791253 -0.026736412][0.022163171 0.054962292 0.088302739 0.11293143 0.12352947 0.12266664 0.11591143 0.10769223 0.097951546 0.085801683 0.072120078 0.056210537 0.0376169 0.017143873 -0.00051726343][0.069502413 0.1249099 0.18294007 0.22933604 0.25475889 0.26276565 0.26036829 0.25126877 0.23394567 0.2080828 0.1769675 0.14140919 0.10293452 0.0645585 0.033732612][0.11674488 0.19435406 0.27763015 0.34704316 0.39002365 0.41193381 0.41914481 0.41138819 0.38493508 0.341665 0.28873971 0.22921209 0.16764677 0.11040923 0.06751544][0.14781258 0.2386778 0.33749855 0.42105579 0.47624132 0.511325 0.52971035 0.5244242 0.48992056 0.43205592 0.36221731 0.2852087 0.20820405 0.14057741 0.093486384][0.1558098 0.24354771 0.33924195 0.41903621 0.47272646 0.51194173 0.53638518 0.53260589 0.49487376 0.43330625 0.36138424 0.28362736 0.20808463 0.14566268 0.10664593][0.14736353 0.21533018 0.28791627 0.34497112 0.38206214 0.41318238 0.43518668 0.43151373 0.39810696 0.34739235 0.29107058 0.23118086 0.17458668 0.13190986 0.11084465][0.13831218 0.17836528 0.21729289 0.24195372 0.25441724 0.26909959 0.28188717 0.27787384 0.25571409 0.22599395 0.19540563 0.16316666 0.13445686 0.11762659 0.11671041][0.14244142 0.15799077 0.16636728 0.16254097 0.15351756 0.15196224 0.15427852 0.15111703 0.14331336 0.13538288 0.12755419 0.11841041 0.11326687 0.11716889 0.13020761][0.16127123 0.16337021 0.1536081 0.13495193 0.11657124 0.10698618 0.10327185 0.10273208 0.10705142 0.11288991 0.11508875 0.11431295 0.11849575 0.12969808 0.14495601][0.18197072 0.18335426 0.16976106 0.15066379 0.13564518 0.12842539 0.1255943 0.12893067 0.13970786 0.14832756 0.14719212 0.14021467 0.13856398 0.14285947 0.14891495][0.19871409 0.20745732 0.19853289 0.18503271 0.17688708 0.1755337 0.1768571 0.18364082 0.19559009 0.19976646 0.18891056 0.1696364 0.15510423 0.14620011 0.13913976]]...]
INFO - root - 2017-12-11 08:37:15.145772: step 34710, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:57m:36s remains)
INFO - root - 2017-12-11 08:37:20.501577: step 34720, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 45h:12m:39s remains)
INFO - root - 2017-12-11 08:37:25.589436: step 34730, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 46h:40m:32s remains)
INFO - root - 2017-12-11 08:37:31.026448: step 34740, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.576 sec/batch; 47h:37m:49s remains)
INFO - root - 2017-12-11 08:37:36.437108: step 34750, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:11m:27s remains)
INFO - root - 2017-12-11 08:37:41.771765: step 34760, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:03m:59s remains)
INFO - root - 2017-12-11 08:37:47.138316: step 34770, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 44h:50m:17s remains)
INFO - root - 2017-12-11 08:37:52.527632: step 34780, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:45m:21s remains)
INFO - root - 2017-12-11 08:37:57.875338: step 34790, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:33m:34s remains)
INFO - root - 2017-12-11 08:38:03.172523: step 34800, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:53m:05s remains)
2017-12-11 08:38:03.738068: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19242777 0.21996756 0.2380911 0.24204494 0.24802667 0.26505023 0.27954859 0.29332709 0.31190741 0.32584211 0.3289496 0.32270455 0.31217039 0.3083539 0.31341517][0.19400905 0.20722643 0.21336293 0.21926332 0.2436298 0.283062 0.31071791 0.3222881 0.32767609 0.32736742 0.32132122 0.31393293 0.30674815 0.30871257 0.3197791][0.17382281 0.17087512 0.16356876 0.17070653 0.21232575 0.27352816 0.31621805 0.32893226 0.31979156 0.30180049 0.28327328 0.27263933 0.26907828 0.27768904 0.29431307][0.1310178 0.11903244 0.10781112 0.12242495 0.18209034 0.26562697 0.32832229 0.34971032 0.3318249 0.29732686 0.26358595 0.24301465 0.23478179 0.2417302 0.256915][0.068936914 0.054818556 0.04812314 0.074242681 0.14994888 0.25375676 0.34123808 0.38303506 0.36999908 0.32654926 0.27745578 0.23833746 0.21089737 0.20020135 0.20210303][0.01259578 0.002473633 0.003742615 0.0373504 0.117309 0.22915974 0.33847812 0.40828991 0.41372928 0.37184802 0.31159526 0.25049227 0.19362976 0.15192717 0.1300488][-0.024514368 -0.025413895 -0.013743012 0.023745842 0.097264 0.20467331 0.32835782 0.42438543 0.45135769 0.41481295 0.34606043 0.26490363 0.17946123 0.10627168 0.058981888][-0.040849146 -0.030046746 -0.0063632051 0.0335111 0.094748415 0.18789391 0.31433749 0.42610902 0.46887174 0.43631929 0.36096624 0.26612279 0.16234484 0.068802781 0.0043141521][-0.03987363 -0.017948868 0.015504601 0.054272752 0.09900751 0.16937356 0.28319106 0.39433572 0.44305629 0.41313589 0.33551118 0.23618773 0.12796174 0.030758504 -0.036011007][-0.027557259 -0.00020005036 0.03602967 0.067670375 0.092451744 0.13437694 0.22133665 0.31524673 0.35955524 0.33269149 0.25993818 0.16701885 0.068388969 -0.016351186 -0.07077907][-0.01741343 0.0088386182 0.040919505 0.062746853 0.069958121 0.085586227 0.13923921 0.20368706 0.2333695 0.20861816 0.14784351 0.072303422 -0.0042130244 -0.064364836 -0.097292364][-0.025824288 -0.0058752443 0.019330654 0.03437005 0.03371745 0.034684118 0.060776766 0.094710425 0.10588478 0.081896111 0.036050636 -0.016925512 -0.066479959 -0.0996473 -0.11161929][-0.053248849 -0.04248048 -0.025709566 -0.016059145 -0.018743826 -0.023270799 -0.015232657 -0.0042613954 -0.0072313673 -0.029079515 -0.059845865 -0.090783477 -0.11552133 -0.12632485 -0.1232583][-0.088904895 -0.087959893 -0.0806734 -0.076874018 -0.080309257 -0.0855143 -0.0857856 -0.086028114 -0.094214156 -0.11104595 -0.1295051 -0.14434889 -0.15181336 -0.14833316 -0.137006][-0.11523896 -0.12170716 -0.12196641 -0.1225381 -0.12572892 -0.1288961 -0.13001905 -0.1318555 -0.13798459 -0.14776847 -0.15699977 -0.16222052 -0.16079549 -0.15141639 -0.137981]]...]
INFO - root - 2017-12-11 08:38:09.105721: step 34810, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 43h:10m:01s remains)
INFO - root - 2017-12-11 08:38:14.493088: step 34820, loss = 0.72, batch loss = 0.66 (13.7 examples/sec; 0.584 sec/batch; 48h:18m:35s remains)
INFO - root - 2017-12-11 08:38:19.573712: step 34830, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:19m:39s remains)
INFO - root - 2017-12-11 08:38:24.825767: step 34840, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.528 sec/batch; 43h:40m:33s remains)
INFO - root - 2017-12-11 08:38:30.220016: step 34850, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 43h:59m:36s remains)
INFO - root - 2017-12-11 08:38:35.491967: step 34860, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:19m:30s remains)
INFO - root - 2017-12-11 08:38:40.820814: step 34870, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:33m:56s remains)
INFO - root - 2017-12-11 08:38:46.177586: step 34880, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 43h:38m:09s remains)
INFO - root - 2017-12-11 08:38:51.639419: step 34890, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 43h:28m:34s remains)
INFO - root - 2017-12-11 08:38:57.043536: step 34900, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:00m:53s remains)
2017-12-11 08:38:57.582861: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07730858 0.075230449 0.076154105 0.090639405 0.11628098 0.14044243 0.15093167 0.14072819 0.12145235 0.10856409 0.10192156 0.093545817 0.078312844 0.057735492 0.032643676][0.17560658 0.17213471 0.17129868 0.19229728 0.22956638 0.2616089 0.27026141 0.24836843 0.21537797 0.19003238 0.17099179 0.14969406 0.12291928 0.094774671 0.063605323][0.30307442 0.29886276 0.29320157 0.3153725 0.35618466 0.38751256 0.38859046 0.3548885 0.31151026 0.27481881 0.24080803 0.2025125 0.162076 0.12676691 0.0911611][0.43102553 0.42723751 0.41347796 0.42780489 0.45914918 0.47951832 0.47134787 0.43480819 0.39378244 0.35360208 0.30567792 0.24832775 0.19273509 0.14864045 0.1069012][0.52486724 0.52833349 0.51033819 0.51357967 0.52788019 0.53290373 0.52005196 0.4948833 0.47248179 0.43980259 0.38164449 0.30517432 0.23267093 0.17426057 0.11907032][0.56244195 0.57810879 0.56419092 0.56022006 0.55713689 0.54725522 0.53445178 0.5297 0.53582132 0.52088666 0.46221903 0.37420791 0.28863892 0.21324925 0.13819094][0.53250158 0.55926055 0.55515838 0.54940259 0.532304 0.50963581 0.49832729 0.51505774 0.55185491 0.56349891 0.52067661 0.4379383 0.35035691 0.26142034 0.16555482][0.43691108 0.46946964 0.47638521 0.4747338 0.45356497 0.42728421 0.421494 0.45691046 0.52033335 0.56126416 0.54568458 0.48200169 0.40217057 0.30563316 0.19301832][0.29429311 0.32986438 0.34848335 0.3579753 0.3464705 0.33056083 0.33662587 0.38458419 0.46210951 0.52197123 0.52978772 0.48692831 0.41852275 0.32181871 0.20244581][0.13753434 0.16999064 0.19757144 0.2193376 0.22283918 0.22304747 0.24119718 0.29045436 0.36266986 0.42212969 0.44039729 0.41343325 0.35848087 0.27400839 0.16662508][0.0018720513 0.022824349 0.049587056 0.074233465 0.084651247 0.093001924 0.1146394 0.1542851 0.20747004 0.25323972 0.27228668 0.25707144 0.21806218 0.15720408 0.079582751][-0.097867437 -0.093889058 -0.076016895 -0.05728288 -0.048961263 -0.042732783 -0.027479034 -0.0030522814 0.028630588 0.058857858 0.076233834 0.07180129 0.050409619 0.016805712 -0.025142981][-0.16001728 -0.1691069 -0.16100956 -0.15029085 -0.14695075 -0.14677818 -0.14162144 -0.13181658 -0.11749544 -0.10012595 -0.085424133 -0.0812249 -0.08665888 -0.096705317 -0.10868104][-0.17756683 -0.19310695 -0.19202583 -0.18722537 -0.186602 -0.18954353 -0.19150014 -0.19179636 -0.18906617 -0.1817244 -0.17198014 -0.16475444 -0.16035625 -0.15553486 -0.14944839][-0.15477064 -0.17046653 -0.17305702 -0.17145258 -0.17086205 -0.17264122 -0.17571555 -0.17953578 -0.18225263 -0.18183403 -0.17857572 -0.17448713 -0.16914824 -0.16081752 -0.15002225]]...]
INFO - root - 2017-12-11 08:39:02.926696: step 34910, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:53m:53s remains)
INFO - root - 2017-12-11 08:39:08.260499: step 34920, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 44h:41m:12s remains)
INFO - root - 2017-12-11 08:39:13.386038: step 34930, loss = 0.70, batch loss = 0.64 (13.5 examples/sec; 0.592 sec/batch; 48h:58m:12s remains)
INFO - root - 2017-12-11 08:39:18.747550: step 34940, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:30m:05s remains)
INFO - root - 2017-12-11 08:39:24.099856: step 34950, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 43h:37m:17s remains)
INFO - root - 2017-12-11 08:39:29.364327: step 34960, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 42h:33m:27s remains)
INFO - root - 2017-12-11 08:39:34.710115: step 34970, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 44h:14m:49s remains)
INFO - root - 2017-12-11 08:39:40.062955: step 34980, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:07m:00s remains)
INFO - root - 2017-12-11 08:39:45.345180: step 34990, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:36m:38s remains)
INFO - root - 2017-12-11 08:39:50.747794: step 35000, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 43h:44m:35s remains)
2017-12-11 08:39:51.332358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.092270344 -0.069900326 0.00039920807 0.13481705 0.29660672 0.44076657 0.51967192 0.54042506 0.51192367 0.43232504 0.34710222 0.30136243 0.29907832 0.29554108 0.26740524][-0.10299505 -0.0832113 -0.018033898 0.10486968 0.25810769 0.39206663 0.46189 0.47608125 0.44828045 0.37882864 0.30221835 0.25878882 0.2530514 0.2464233 0.21902795][-0.10900068 -0.094297737 -0.039321057 0.063888654 0.19325246 0.30529866 0.36268359 0.37137651 0.3449136 0.28989488 0.22934206 0.19499314 0.1897746 0.18372941 0.16327494][-0.11003672 -0.098357089 -0.051120512 0.034621958 0.14139766 0.23295645 0.27878365 0.28427285 0.26038089 0.21834645 0.1740153 0.14934793 0.14359833 0.13419421 0.11705524][-0.10518786 -0.092375219 -0.047491167 0.026978953 0.11704284 0.19396946 0.23339167 0.24084684 0.22299315 0.19339426 0.16465555 0.1489809 0.14029874 0.12062258 0.096838][-0.094389975 -0.076125644 -0.026909459 0.046174556 0.13076301 0.20370658 0.24596846 0.2636202 0.25825661 0.24175826 0.22441749 0.21196324 0.19421385 0.15524919 0.11170414][-0.0818381 -0.056395065 0.0017285844 0.083887771 0.17695294 0.25996217 0.31687465 0.35439137 0.36768112 0.3608312 0.34185988 0.31874216 0.28390005 0.22275321 0.15639655][-0.072962031 -0.041471194 0.026718339 0.1256163 0.23983763 0.34609991 0.42653668 0.48682937 0.51495814 0.50433117 0.46379855 0.4141596 0.35973391 0.28651491 0.2113626][-0.069276981 -0.03295923 0.045123857 0.16439974 0.30594161 0.43964925 0.54206413 0.61600614 0.64515567 0.61427963 0.53742605 0.45464537 0.38717598 0.3192696 0.25573429][-0.07031627 -0.030627847 0.055471353 0.19305693 0.35864624 0.51315272 0.62629759 0.69728065 0.71038604 0.6477986 0.53310889 0.42349818 0.3540729 0.30492264 0.26671454][-0.074727766 -0.034874462 0.054209329 0.20133165 0.37850264 0.5385015 0.64548635 0.69636089 0.68125725 0.58712184 0.44821721 0.32986805 0.27098739 0.24674571 0.23824836][-0.077727608 -0.041426621 0.043731119 0.18723039 0.35739449 0.50322044 0.58732492 0.60804909 0.56401503 0.45072544 0.30956969 0.20373149 0.16520756 0.16511436 0.18294588][-0.075865939 -0.046461254 0.02726943 0.15333644 0.29872161 0.41571936 0.4698472 0.46424612 0.40398514 0.29093838 0.1666365 0.085030064 0.067167521 0.08386974 0.11923829][-0.072607726 -0.052153941 0.0044060824 0.1028418 0.2128727 0.29718435 0.3267405 0.3095789 0.25235829 0.15819459 0.061452333 0.0043313145 0.00078221707 0.026270723 0.071059473][-0.070403539 -0.05803569 -0.018728096 0.05103948 0.12753968 0.18603253 0.20249698 0.18688798 0.14358005 0.074635878 0.0043185456 -0.03543615 -0.031375114 -0.0016418686 0.047867078]]...]
INFO - root - 2017-12-11 08:39:56.734745: step 35010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:05m:04s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:40:02.018621: step 35020, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:04m:08s remains)
INFO - root - 2017-12-11 08:40:07.291303: step 35030, loss = 0.69, batch loss = 0.63 (18.7 examples/sec; 0.428 sec/batch; 35h:23m:54s remains)
INFO - root - 2017-12-11 08:40:12.481165: step 35040, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:29m:13s remains)
INFO - root - 2017-12-11 08:40:17.803948: step 35050, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 43h:57m:33s remains)
INFO - root - 2017-12-11 08:40:23.073681: step 35060, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:08m:21s remains)
INFO - root - 2017-12-11 08:40:28.494396: step 35070, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 43h:59m:37s remains)
INFO - root - 2017-12-11 08:40:33.804909: step 35080, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 45h:15m:53s remains)
INFO - root - 2017-12-11 08:40:39.222739: step 35090, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 43h:13m:11s remains)
INFO - root - 2017-12-11 08:40:44.468509: step 35100, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 42h:59m:32s remains)
2017-12-11 08:40:45.010950: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28923485 0.28399554 0.27477393 0.2715776 0.27080873 0.28464288 0.302437 0.30301246 0.2803559 0.23194753 0.18434128 0.14352432 0.10793322 0.084439732 0.0837126][0.3317779 0.3230052 0.30940869 0.30033484 0.29314053 0.29709396 0.29780003 0.27960625 0.24270271 0.19546969 0.15582952 0.12280461 0.093787625 0.074448638 0.076739907][0.37273023 0.36060455 0.34198648 0.32630762 0.31356466 0.30857861 0.29326788 0.25927153 0.21330176 0.17181084 0.14393964 0.12271744 0.10595708 0.096821152 0.10319229][0.4131808 0.40263221 0.38244253 0.3631579 0.34930432 0.34302345 0.32384023 0.28737545 0.24099946 0.20651332 0.18745521 0.17468299 0.1683009 0.16971083 0.18040371][0.4356029 0.43542537 0.41876003 0.39770415 0.3837502 0.37996724 0.36605787 0.3383691 0.30078915 0.27689445 0.26720113 0.26248455 0.2655718 0.27697298 0.29135406][0.4301469 0.44987631 0.44604072 0.42863867 0.41606098 0.41589263 0.41033962 0.394744 0.3688401 0.35558242 0.35405684 0.35519862 0.36417377 0.38218415 0.4000712][0.40318507 0.44527757 0.46055654 0.45399338 0.44829613 0.45578998 0.46093062 0.4570989 0.44045952 0.43114522 0.42836097 0.42469192 0.42877439 0.44593185 0.46740595][0.36941573 0.42326853 0.45370939 0.46047837 0.4659653 0.48455629 0.50204527 0.50932574 0.49853116 0.48578802 0.47210497 0.45333579 0.44274718 0.45174849 0.47530887][0.35935059 0.4079037 0.43870655 0.45143154 0.46439257 0.49171987 0.52018064 0.53712308 0.53014207 0.51110095 0.48424947 0.44791263 0.419752 0.41695991 0.43979883][0.38517997 0.41106045 0.423727 0.42816663 0.43785444 0.46464494 0.49693918 0.51886243 0.51441848 0.49140579 0.45735782 0.41192871 0.37503156 0.36739102 0.39330253][0.42981267 0.42231342 0.40234524 0.38427636 0.3787781 0.39518178 0.42384261 0.44654861 0.44454598 0.42176041 0.38896546 0.34766811 0.3173658 0.31835306 0.35527959][0.4682225 0.42645094 0.368867 0.31940767 0.28826571 0.28443715 0.30197945 0.32184535 0.32285884 0.30487055 0.28141981 0.25663239 0.24724214 0.26908967 0.32218528][0.47388735 0.40873349 0.3231681 0.24647553 0.18867183 0.1604252 0.16135584 0.17361122 0.17439139 0.16074999 0.14828579 0.14410645 0.16200802 0.21187596 0.28440696][0.43091163 0.35739106 0.26176167 0.17213762 0.09761712 0.049995128 0.0343269 0.036340445 0.033011597 0.020124475 0.014296128 0.026040806 0.06699872 0.1417992 0.23185557][0.33399346 0.261125 0.16985384 0.08251337 0.0063854945 -0.04775716 -0.071834505 -0.076396838 -0.084082939 -0.099051081 -0.10387206 -0.0835066 -0.028341463 0.061897565 0.16316074]]...]
INFO - root - 2017-12-11 08:40:50.444191: step 35110, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 42h:57m:55s remains)
INFO - root - 2017-12-11 08:40:55.723464: step 35120, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.518 sec/batch; 42h:45m:05s remains)
INFO - root - 2017-12-11 08:41:01.044139: step 35130, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 43h:01m:00s remains)
INFO - root - 2017-12-11 08:41:06.123835: step 35140, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 43h:50m:52s remains)
INFO - root - 2017-12-11 08:41:11.454667: step 35150, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:59m:51s remains)
INFO - root - 2017-12-11 08:41:16.905011: step 35160, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:00m:17s remains)
INFO - root - 2017-12-11 08:41:22.279774: step 35170, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.526 sec/batch; 43h:24m:51s remains)
INFO - root - 2017-12-11 08:41:27.530690: step 35180, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 43h:55m:11s remains)
INFO - root - 2017-12-11 08:41:32.923824: step 35190, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 44h:54m:05s remains)
INFO - root - 2017-12-11 08:41:38.321864: step 35200, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 44h:37m:12s remains)
2017-12-11 08:41:38.861146: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1496 0.1466292 0.11931927 0.084212564 0.04988033 0.028893586 0.026136728 0.041860774 0.069774278 0.10359161 0.14088441 0.17652994 0.20689659 0.23131706 0.24191731][0.13983236 0.13256913 0.10295745 0.068744332 0.040067311 0.028145757 0.03463817 0.05652022 0.088049442 0.12623011 0.16823684 0.20905228 0.24314784 0.27256742 0.28718033][0.11447062 0.10525148 0.078496464 0.051547911 0.033300944 0.0328529 0.048781477 0.075738005 0.10871007 0.14859568 0.19426879 0.2402216 0.2787869 0.31160793 0.32944024][0.08788386 0.079025328 0.0579756 0.04039881 0.033350002 0.043329507 0.067049459 0.097656287 0.13089891 0.17127781 0.22055009 0.27233997 0.31571698 0.35024008 0.3694863][0.062386874 0.056473732 0.043226667 0.035999246 0.039782092 0.058290865 0.087088674 0.11875309 0.15012822 0.18923755 0.24082097 0.29758367 0.3444728 0.37777376 0.39474368][0.044515412 0.044977866 0.04253193 0.047811128 0.063494705 0.090381786 0.1224779 0.15227975 0.17773707 0.20990685 0.25614375 0.30917525 0.35185722 0.37780946 0.38761476][0.040421382 0.051325448 0.063470371 0.084669083 0.11526822 0.15263638 0.18805218 0.21352749 0.22846058 0.24607967 0.27551809 0.3110384 0.33718461 0.34722561 0.34367236][0.046919443 0.0700625 0.098695517 0.13719231 0.18308982 0.22988628 0.26595971 0.28330153 0.28363433 0.28189749 0.28772384 0.29780868 0.30086422 0.29187584 0.273076][0.055899478 0.090459533 0.13369392 0.18614866 0.24204926 0.2914094 0.32104683 0.32447904 0.30643103 0.28370339 0.26553005 0.25101188 0.2336334 0.2103086 0.18113263][0.056430273 0.097089328 0.14769688 0.20546596 0.26153675 0.30389583 0.3196314 0.3053028 0.2684834 0.2274206 0.19048652 0.15840757 0.12890169 0.1003388 0.070184253][0.046023753 0.0825239 0.12816767 0.17854513 0.22346239 0.25166494 0.25239751 0.22554621 0.17954603 0.13178921 0.089120746 0.052731518 0.023661347 0.0011591683 -0.019600106][0.025206905 0.05102431 0.083188437 0.11744989 0.1443346 0.15596205 0.14541028 0.11536158 0.073385276 0.03376127 0.0015271092 -0.022873016 -0.037228595 -0.042975564 -0.046812128][0.007231269 0.021911878 0.038964123 0.055349585 0.063646682 0.060632553 0.044093005 0.018635817 -0.010345921 -0.032937277 -0.045539219 -0.048334081 -0.039699174 -0.023400448 -0.007750114][0.00949141 0.018461948 0.024117231 0.026611142 0.021155255 0.009372185 -0.0064231684 -0.02086159 -0.032221276 -0.035950273 -0.029792078 -0.014025016 0.012087111 0.042398639 0.0685461][0.040461015 0.050760172 0.052765332 0.048979662 0.036917802 0.022304015 0.010813267 0.0077376962 0.011101172 0.020530412 0.037313242 0.06145528 0.09323588 0.12495339 0.1496044]]...]
INFO - root - 2017-12-11 08:41:44.232531: step 35210, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:11m:32s remains)
INFO - root - 2017-12-11 08:41:49.595310: step 35220, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:28m:35s remains)
INFO - root - 2017-12-11 08:41:54.932201: step 35230, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 45h:04m:41s remains)
INFO - root - 2017-12-11 08:42:00.035768: step 35240, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:21m:16s remains)
INFO - root - 2017-12-11 08:42:05.434706: step 35250, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 45h:54m:33s remains)
INFO - root - 2017-12-11 08:42:10.842586: step 35260, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 44h:04m:39s remains)
INFO - root - 2017-12-11 08:42:16.181552: step 35270, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:29m:52s remains)
INFO - root - 2017-12-11 08:42:21.493454: step 35280, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 43h:06m:30s remains)
INFO - root - 2017-12-11 08:42:26.816893: step 35290, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 44h:08m:39s remains)
INFO - root - 2017-12-11 08:42:32.161011: step 35300, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 43h:04m:01s remains)
2017-12-11 08:42:32.748101: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15067132 0.16592622 0.18880977 0.20965187 0.22444688 0.23807053 0.2611483 0.2951636 0.32920745 0.34112942 0.31363809 0.24335474 0.14646417 0.050779182 -0.020584634][0.17676361 0.18594114 0.20185308 0.21818458 0.23312601 0.25289121 0.28386492 0.3214553 0.35238069 0.35483822 0.31531355 0.23461831 0.13177295 0.035480928 -0.032964256][0.20164786 0.21036351 0.22430514 0.23874269 0.25293398 0.2757377 0.31042522 0.34942067 0.37645435 0.368919 0.31662416 0.22466893 0.11551369 0.018779419 -0.0460427][0.22265504 0.23834381 0.25859356 0.27777693 0.29352364 0.31755167 0.35173884 0.38797209 0.40800568 0.38867378 0.3229517 0.21943742 0.10350697 0.0053019868 -0.057143111][0.24162474 0.2692754 0.30210453 0.33047351 0.34828529 0.36965182 0.39711273 0.4238368 0.43165696 0.39873934 0.32184198 0.21112968 0.0922269 -0.0057708248 -0.066093333][0.25222245 0.29505685 0.34306452 0.38050622 0.3973498 0.40995646 0.42390394 0.43506819 0.42753083 0.38240528 0.29979396 0.18926731 0.074011981 -0.019484796 -0.075758442][0.24654646 0.30227891 0.36228451 0.40515059 0.41786414 0.41805232 0.41484317 0.40743098 0.38388497 0.32935172 0.24649489 0.14286003 0.037866764 -0.045227021 -0.092642337][0.22188646 0.28123721 0.34537348 0.38850492 0.39547348 0.38385943 0.36545753 0.34189308 0.30558863 0.24594033 0.16728812 0.075098537 -0.014707139 -0.082101226 -0.11592231][0.1807137 0.23048511 0.28775018 0.3243916 0.32406858 0.30284745 0.27404156 0.24092138 0.20012324 0.14414726 0.076746933 0.0008368378 -0.070447907 -0.11974088 -0.13859855][0.11856998 0.14751336 0.18850173 0.21487267 0.20972127 0.18526375 0.15398526 0.11940747 0.081725456 0.036467232 -0.014193372 -0.070510715 -0.12216619 -0.15374939 -0.15868746][0.044338044 0.043805081 0.062289156 0.078162722 0.074302018 0.056649264 0.03353912 0.0061642746 -0.023513837 -0.056429144 -0.091052927 -0.12952261 -0.1638433 -0.1805741 -0.17440943][-0.021134458 -0.049441706 -0.051626537 -0.042339694 -0.038338806 -0.040785875 -0.046895884 -0.059457142 -0.077376023 -0.099192344 -0.12308212 -0.15106745 -0.17609146 -0.18608905 -0.17619069][-0.067647561 -0.10965443 -0.12178739 -0.11260128 -0.097501971 -0.082816243 -0.069738418 -0.0645881 -0.06876649 -0.0801619 -0.097245663 -0.1215587 -0.14629021 -0.15977079 -0.15620966][-0.09402328 -0.13168219 -0.14173117 -0.12849553 -0.10434652 -0.077377789 -0.049978673 -0.029957723 -0.021169132 -0.022754179 -0.034794748 -0.058762532 -0.088046469 -0.11096907 -0.12020019][-0.10465287 -0.12712467 -0.12839232 -0.11044715 -0.081492789 -0.048367906 -0.0129499 0.017025996 0.035852727 0.042371139 0.034675963 0.010818478 -0.023291627 -0.056265224 -0.079218879]]...]
INFO - root - 2017-12-11 08:42:38.125225: step 35310, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 43h:26m:01s remains)
INFO - root - 2017-12-11 08:42:43.545451: step 35320, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:54m:59s remains)
INFO - root - 2017-12-11 08:42:48.946029: step 35330, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.532 sec/batch; 43h:53m:16s remains)
INFO - root - 2017-12-11 08:42:53.941406: step 35340, loss = 0.69, batch loss = 0.63 (17.1 examples/sec; 0.469 sec/batch; 38h:42m:29s remains)
INFO - root - 2017-12-11 08:42:59.228594: step 35350, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 44h:53m:01s remains)
INFO - root - 2017-12-11 08:43:04.671142: step 35360, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:48m:50s remains)
INFO - root - 2017-12-11 08:43:10.062221: step 35370, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.526 sec/batch; 43h:23m:43s remains)
INFO - root - 2017-12-11 08:43:15.519999: step 35380, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:28m:56s remains)
INFO - root - 2017-12-11 08:43:20.979185: step 35390, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:05m:18s remains)
INFO - root - 2017-12-11 08:43:26.357693: step 35400, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:21m:58s remains)
2017-12-11 08:43:26.909875: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34003383 0.33699971 0.288399 0.22633983 0.17700386 0.15830073 0.15988645 0.15692323 0.14988233 0.1413351 0.13284023 0.12700526 0.12570773 0.12800495 0.13201644][0.31650317 0.32525972 0.29045442 0.24285388 0.20458117 0.19098462 0.19225277 0.18186881 0.16121699 0.1387822 0.12145665 0.11074628 0.10687757 0.11059477 0.11898154][0.25004914 0.27246663 0.26514405 0.24966961 0.24025518 0.24401534 0.24801888 0.22646974 0.18493006 0.1403982 0.10670685 0.087003686 0.080218889 0.086165622 0.10063142][0.19629526 0.23394486 0.25793675 0.28057647 0.30668637 0.33431074 0.34457985 0.31106132 0.24377432 0.17103913 0.1162668 0.085292704 0.074714877 0.081931412 0.10179909][0.17266606 0.22220598 0.27353087 0.33250633 0.39516169 0.45014808 0.47054392 0.42660046 0.33217654 0.2287913 0.15103889 0.10758922 0.091218233 0.095565215 0.1157483][0.18376374 0.237797 0.30444807 0.38847315 0.48089725 0.56180507 0.59515834 0.5460155 0.42938381 0.29902279 0.20037794 0.14486371 0.12047832 0.11606689 0.12822758][0.21060538 0.26362452 0.33164966 0.4236798 0.53227985 0.6324988 0.68049651 0.63468063 0.50806427 0.36166495 0.24893649 0.18483309 0.15286426 0.13598745 0.13249977][0.23754236 0.28345978 0.33830822 0.41887709 0.52613372 0.63615984 0.69991893 0.66890311 0.55071485 0.40463552 0.28697717 0.21729499 0.17842522 0.14816079 0.12485088][0.26193479 0.29581177 0.32591471 0.37762636 0.46469131 0.570786 0.646388 0.63787252 0.54491132 0.41561788 0.30265924 0.23011892 0.18493249 0.14261615 0.099911682][0.28803003 0.31241363 0.314767 0.3276448 0.37874472 0.4639844 0.53886491 0.54956108 0.48824936 0.38522947 0.28309685 0.20992352 0.16087238 0.11257627 0.058108676][0.30880073 0.33038607 0.31238309 0.29046395 0.30198258 0.35491458 0.41368997 0.43046165 0.39443061 0.31721827 0.22732687 0.15445088 0.10390293 0.057303842 0.0039450228][0.31568623 0.33800361 0.31027558 0.26728883 0.24986245 0.27298346 0.3093265 0.32023293 0.2969932 0.23845281 0.15980372 0.089097038 0.039019793 -0.00087520224 -0.042768367][0.3062852 0.332035 0.30374983 0.25384596 0.22094229 0.2214466 0.23445708 0.23195106 0.2106123 0.16445197 0.099018812 0.037612639 -0.0055531659 -0.034103595 -0.058845509][0.28321478 0.31359065 0.29195479 0.24736276 0.21174318 0.19876036 0.19207813 0.17378548 0.14718144 0.1086117 0.059264846 0.015435067 -0.01273214 -0.026446901 -0.033553515][0.24807926 0.27892885 0.2651504 0.23293141 0.20562465 0.19187136 0.17730372 0.15078773 0.12130396 0.090932295 0.060401432 0.038546436 0.028939744 0.028973237 0.033806078]]...]
INFO - root - 2017-12-11 08:43:32.275224: step 35410, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 42h:49m:15s remains)
INFO - root - 2017-12-11 08:43:37.677059: step 35420, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 46h:01m:13s remains)
INFO - root - 2017-12-11 08:43:43.114475: step 35430, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:16m:29s remains)
INFO - root - 2017-12-11 08:43:48.423277: step 35440, loss = 0.70, batch loss = 0.64 (15.9 examples/sec; 0.503 sec/batch; 41h:32m:23s remains)
INFO - root - 2017-12-11 08:43:53.531034: step 35450, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 44h:14m:10s remains)
INFO - root - 2017-12-11 08:43:58.847399: step 35460, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:15m:13s remains)
INFO - root - 2017-12-11 08:44:04.240553: step 35470, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:16m:29s remains)
INFO - root - 2017-12-11 08:44:09.706396: step 35480, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 43h:51m:55s remains)
INFO - root - 2017-12-11 08:44:15.047001: step 35490, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 44h:30m:10s remains)
INFO - root - 2017-12-11 08:44:20.346140: step 35500, loss = 0.67, batch loss = 0.61 (14.1 examples/sec; 0.569 sec/batch; 46h:54m:32s remains)
2017-12-11 08:44:20.944449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029833294 -0.0032032225 0.039952293 0.088407494 0.13282132 0.16559963 0.18658493 0.18655363 0.16431062 0.13269766 0.10545842 0.085907787 0.073120333 0.066511877 0.061217625][-0.018536782 0.022399468 0.088733144 0.1669264 0.24388838 0.30511937 0.34807438 0.35852757 0.33465266 0.29307389 0.25211075 0.2194825 0.19755319 0.18374135 0.16920863][-0.0098700952 0.042747714 0.12887378 0.2324955 0.33754069 0.42322409 0.48553318 0.50900584 0.49264088 0.45436549 0.4122563 0.37494576 0.34829688 0.327325 0.30018666][-0.012981583 0.047275394 0.14825521 0.2723406 0.40046996 0.50656724 0.58642691 0.62631536 0.62685329 0.60606152 0.57970768 0.55166173 0.52661866 0.49699619 0.45172665][-0.022103105 0.042200197 0.1540685 0.29423407 0.44095784 0.56389248 0.66000831 0.71954405 0.74388754 0.74905688 0.7481187 0.73781109 0.71730089 0.67566848 0.60628444][-0.031951204 0.033542849 0.15139174 0.30173004 0.46229342 0.60193473 0.71894169 0.80634892 0.863219 0.89837676 0.9204033 0.92111582 0.8978864 0.839607 0.745003][-0.045038927 0.016193742 0.13075559 0.28022206 0.44478425 0.59698611 0.73439348 0.85079777 0.94067556 1.0004578 1.0329403 1.0291554 0.99260849 0.91729605 0.80668366][-0.066851914 -0.016856844 0.084095679 0.2203465 0.37670255 0.5320732 0.68056917 0.81472719 0.92558551 0.99798948 1.0297449 1.0135566 0.96246451 0.87753493 0.76416367][-0.08746393 -0.049588092 0.034825593 0.15156522 0.29036549 0.43653512 0.5786227 0.7066772 0.81298256 0.87891871 0.90078467 0.87240124 0.814364 0.73249674 0.6299938][-0.10222024 -0.076565892 -0.011372086 0.080468923 0.19417571 0.32068637 0.44310653 0.54795742 0.63038206 0.67477125 0.67904258 0.63863409 0.57909751 0.50887978 0.42599246][-0.11700173 -0.10616064 -0.0659301 -0.0065567098 0.073073588 0.17011955 0.26475233 0.33963007 0.39245191 0.41243294 0.39873457 0.34856862 0.29059505 0.23600394 0.17759797][-0.12947419 -0.13355714 -0.11955082 -0.095672444 -0.05654072 0.00044802192 0.057236936 0.096972629 0.12022264 0.12092271 0.098046884 0.04842234 -0.00055775931 -0.03630596 -0.0680605][-0.13243149 -0.14526416 -0.14903526 -0.15111482 -0.14493155 -0.12519506 -0.10481713 -0.096421249 -0.096541479 -0.10654414 -0.1280697 -0.16630892 -0.19971873 -0.2169985 -0.22689529][-0.12432621 -0.13924001 -0.15092455 -0.16567224 -0.17785823 -0.1821539 -0.1854893 -0.19601978 -0.20909916 -0.22292706 -0.2388619 -0.26291221 -0.28132051 -0.28596297 -0.2838451][-0.11316001 -0.12776706 -0.14246207 -0.16153128 -0.18093531 -0.1965344 -0.21031764 -0.22650334 -0.24178427 -0.25410119 -0.2645762 -0.27767795 -0.28638193 -0.28560057 -0.27981782]]...]
INFO - root - 2017-12-11 08:44:26.298879: step 35510, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:06m:00s remains)
INFO - root - 2017-12-11 08:44:31.555884: step 35520, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 43h:33m:47s remains)
INFO - root - 2017-12-11 08:44:36.924521: step 35530, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 43h:50m:06s remains)
INFO - root - 2017-12-11 08:44:42.239583: step 35540, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:21m:30s remains)
INFO - root - 2017-12-11 08:44:47.306479: step 35550, loss = 0.70, batch loss = 0.65 (15.6 examples/sec; 0.512 sec/batch; 42h:14m:17s remains)
INFO - root - 2017-12-11 08:44:52.604709: step 35560, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.512 sec/batch; 42h:12m:02s remains)
INFO - root - 2017-12-11 08:44:57.945127: step 35570, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 44h:07m:07s remains)
INFO - root - 2017-12-11 08:45:03.312910: step 35580, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 44h:24m:30s remains)
INFO - root - 2017-12-11 08:45:08.643002: step 35590, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 43h:19m:03s remains)
INFO - root - 2017-12-11 08:45:13.900391: step 35600, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:06m:40s remains)
2017-12-11 08:45:14.447284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0090490421 0.043520618 0.13562119 0.26368105 0.40351105 0.52169651 0.58260256 0.56682622 0.47713107 0.3425867 0.20218687 0.087627575 0.010975762 -0.033556644 -0.0539863][-0.0093205422 0.053403597 0.16697386 0.32810187 0.50677234 0.66047239 0.74326622 0.72688663 0.61332387 0.44101197 0.26213089 0.11759414 0.022625605 -0.031066453 -0.055320956][-0.013586548 0.053557638 0.18079069 0.36663276 0.57731068 0.76155025 0.86377275 0.84726113 0.71288419 0.50778079 0.29692864 0.1295917 0.023912348 -0.032171052 -0.055551853][-0.023764009 0.042351775 0.17421047 0.3734391 0.60569435 0.81318766 0.932163 0.91789925 0.76914483 0.53966838 0.30520663 0.12278248 0.013392571 -0.038453095 -0.05604117][-0.036705751 0.025007669 0.15434897 0.3558923 0.59680837 0.81583053 0.94503552 0.93520647 0.78235489 0.54264867 0.297417 0.10876069 0.0005993729 -0.043909319 -0.053272158][-0.046950031 0.0074115987 0.12819256 0.32278541 0.56113213 0.78129172 0.91477025 0.9110406 0.76385963 0.52828264 0.28567505 0.099693306 -0.0035225146 -0.040126972 -0.041167498][-0.051472764 -0.0073898472 0.099089943 0.27920321 0.50761157 0.72433442 0.86194026 0.86860895 0.7353211 0.51323479 0.28142023 0.10350555 0.007242409 -0.022003449 -0.016265199][-0.0499455 -0.018781556 0.067898 0.2253051 0.43506226 0.6426686 0.78387195 0.8051914 0.69387251 0.49396396 0.27965993 0.1143297 0.027259957 0.0062313084 0.018594254][-0.042401675 -0.024522645 0.040141642 0.16911884 0.35118926 0.54068387 0.6794166 0.714079 0.62903053 0.45949635 0.27112088 0.12426191 0.049534541 0.038458161 0.059031047][-0.028248338 -0.021050135 0.024238225 0.12515074 0.27571511 0.4397912 0.56709743 0.6077528 0.54534256 0.40712628 0.24894033 0.12571356 0.067211993 0.068152189 0.0983452][-0.016166154 -0.017765226 0.010496632 0.084983192 0.20317744 0.33773836 0.44625849 0.484541 0.43756223 0.3269524 0.19932622 0.10250474 0.063352361 0.078368224 0.11910706][-0.015670812 -0.026628565 -0.01670979 0.028814446 0.11039284 0.20938599 0.29246181 0.32302961 0.28858003 0.20610243 0.11218411 0.045417253 0.028088469 0.057526831 0.10769285][-0.023792367 -0.043848421 -0.052527349 -0.037577018 0.0040950445 0.062620789 0.11480951 0.13391285 0.11080956 0.057967775 0.00093460944 -0.032931976 -0.027642293 0.014133307 0.070090041][-0.035759013 -0.060550056 -0.081550226 -0.089895561 -0.081119657 -0.058422487 -0.035616443 -0.029408321 -0.044691276 -0.072325461 -0.096193753 -0.10069443 -0.077339038 -0.029585008 0.025095979][-0.052595682 -0.078010313 -0.1040609 -0.12486538 -0.13568392 -0.13635854 -0.13398071 -0.13771389 -0.14891523 -0.16043532 -0.16256151 -0.14814031 -0.11519663 -0.068751588 -0.021646312]]...]
INFO - root - 2017-12-11 08:45:19.830940: step 35610, loss = 0.67, batch loss = 0.62 (13.9 examples/sec; 0.575 sec/batch; 47h:23m:15s remains)
INFO - root - 2017-12-11 08:45:25.138203: step 35620, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:41m:56s remains)
INFO - root - 2017-12-11 08:45:30.395502: step 35630, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.528 sec/batch; 43h:30m:06s remains)
INFO - root - 2017-12-11 08:45:35.772929: step 35640, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.529 sec/batch; 43h:36m:43s remains)
INFO - root - 2017-12-11 08:45:40.785528: step 35650, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:15m:42s remains)
INFO - root - 2017-12-11 08:45:46.258341: step 35660, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:56m:23s remains)
INFO - root - 2017-12-11 08:45:51.567850: step 35670, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.539 sec/batch; 44h:26m:02s remains)
INFO - root - 2017-12-11 08:45:56.876202: step 35680, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:44m:02s remains)
INFO - root - 2017-12-11 08:46:02.277040: step 35690, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:24m:59s remains)
INFO - root - 2017-12-11 08:46:07.571034: step 35700, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:25m:53s remains)
2017-12-11 08:46:08.153533: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22201698 0.24059209 0.23903048 0.22385165 0.20123392 0.18232295 0.17347428 0.17190881 0.16923201 0.16075107 0.15322255 0.15121679 0.16162059 0.18699946 0.22223543][0.24350978 0.264105 0.26538089 0.25530696 0.23676595 0.21850045 0.20627148 0.19845617 0.18973441 0.17806853 0.16842976 0.16361418 0.17275144 0.19755277 0.23087941][0.2266469 0.24526726 0.24833982 0.245537 0.23699498 0.22721948 0.21907349 0.21167322 0.20407835 0.19562921 0.18984973 0.18781538 0.19642304 0.21503943 0.2362754][0.19163647 0.20327309 0.20565251 0.2090604 0.21234049 0.21580933 0.21931358 0.22123773 0.2232693 0.22495683 0.22762948 0.23092799 0.23916544 0.25043377 0.25770846][0.16124259 0.16617969 0.1696841 0.1807379 0.19664732 0.2145326 0.23185469 0.24640399 0.25967932 0.26988587 0.27617615 0.27819327 0.27941066 0.27964503 0.273974][0.1390276 0.14477763 0.15819585 0.18464851 0.21811739 0.25297323 0.28445539 0.31022474 0.32974061 0.3394739 0.3375245 0.32482693 0.30636036 0.28748289 0.268466][0.1158888 0.12825587 0.1588946 0.20859428 0.26682815 0.32401523 0.37121814 0.40372336 0.41895327 0.41497305 0.39292192 0.35759288 0.31633088 0.27980861 0.25260246][0.088386066 0.1074933 0.15254484 0.2215317 0.30068409 0.37636647 0.43467605 0.46636793 0.4675996 0.44247621 0.40016568 0.35144857 0.3037414 0.26673311 0.24445704][0.0645137 0.086767219 0.13788301 0.21318965 0.29699764 0.37395504 0.42875117 0.45001492 0.43367863 0.39079377 0.33933625 0.2951895 0.2633844 0.24709471 0.24570045][0.043431941 0.066520467 0.11752149 0.1884055 0.2615647 0.32196254 0.35807058 0.36354008 0.33574128 0.28903788 0.24483141 0.21963203 0.21601164 0.2306015 0.2574127][0.01506273 0.0359526 0.083389141 0.14634171 0.20554829 0.24560173 0.2594969 0.25019145 0.21913202 0.18147779 0.15605079 0.15529217 0.18069059 0.2265832 0.28225306][-0.023969835 -0.010814583 0.027371384 0.078670293 0.12407699 0.14782551 0.14559504 0.12833048 0.1034724 0.085946687 0.087873109 0.1135004 0.16348691 0.23382649 0.31004035][-0.067124017 -0.063288443 -0.038121931 -0.0032391665 0.025862135 0.036882993 0.028402772 0.013914654 0.0044914936 0.012498597 0.043125276 0.092064716 0.15882112 0.2416178 0.32356161][-0.10746555 -0.10945418 -0.094617113 -0.075131349 -0.060932744 -0.057489373 -0.063389458 -0.068090819 -0.060844176 -0.033181902 0.015278978 0.075257637 0.14661396 0.2292044 0.30458471][-0.13904151 -0.14111325 -0.12997645 -0.11867875 -0.11193674 -0.10968248 -0.10968988 -0.10589392 -0.089905135 -0.056610923 -0.0082417279 0.04604632 0.10961658 0.18260297 0.24502735]]...]
INFO - root - 2017-12-11 08:46:13.498546: step 35710, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.548 sec/batch; 45h:10m:31s remains)
INFO - root - 2017-12-11 08:46:18.920465: step 35720, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.535 sec/batch; 44h:07m:12s remains)
INFO - root - 2017-12-11 08:46:24.262463: step 35730, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:37m:19s remains)
INFO - root - 2017-12-11 08:46:29.547370: step 35740, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:31m:18s remains)
INFO - root - 2017-12-11 08:46:34.839280: step 35750, loss = 0.69, batch loss = 0.63 (20.5 examples/sec; 0.390 sec/batch; 32h:09m:10s remains)
INFO - root - 2017-12-11 08:46:40.125853: step 35760, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 44h:06m:11s remains)
INFO - root - 2017-12-11 08:46:45.512245: step 35770, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:04m:58s remains)
INFO - root - 2017-12-11 08:46:50.860330: step 35780, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:21m:46s remains)
INFO - root - 2017-12-11 08:46:56.122695: step 35790, loss = 0.71, batch loss = 0.66 (15.3 examples/sec; 0.524 sec/batch; 43h:09m:26s remains)
INFO - root - 2017-12-11 08:47:01.521072: step 35800, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:33m:22s remains)
2017-12-11 08:47:02.100371: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38105258 0.26478273 0.14282832 0.051587321 0.0079430472 0.022597836 0.10983948 0.25953615 0.43282235 0.57321852 0.62528884 0.5626424 0.41325828 0.25192279 0.13601677][0.23576741 0.13548549 0.045179509 -0.010450723 -0.029652212 -0.015497998 0.036252938 0.11938027 0.21268323 0.28633803 0.30876267 0.26504371 0.17369708 0.08415024 0.031765748][0.077446654 0.0041006282 -0.039387446 -0.041545182 -0.014787913 0.023146996 0.061774746 0.093102328 0.10814966 0.10152155 0.069128618 0.015017278 -0.04310099 -0.077024221 -0.074506842][-0.064676374 -0.098230653 -0.085417487 -0.022982877 0.067247041 0.15479895 0.21129799 0.21968848 0.17904042 0.10214467 0.00986484 -0.075134955 -0.13135237 -0.14598377 -0.12181246][-0.17638135 -0.16118851 -0.086436175 0.04857162 0.21662959 0.37489679 0.47425491 0.48248881 0.40228519 0.26102972 0.10518675 -0.023331359 -0.098377168 -0.11971509 -0.10108202][-0.23003653 -0.173529 -0.046646494 0.15399256 0.40167654 0.64338529 0.80755669 0.84088248 0.73924631 0.53942263 0.31361148 0.12701523 0.013961926 -0.033184525 -0.036964625][-0.22601309 -0.14855459 0.0055917208 0.24588396 0.55150551 0.8658427 1.0995895 1.1781691 1.0808891 0.84374851 0.55753577 0.31136087 0.15274671 0.072851568 0.043328386][-0.19213383 -0.11935134 0.030198274 0.27254713 0.59543824 0.94698679 1.2313632 1.35907 1.2887149 1.0460875 0.72883552 0.44375369 0.25290573 0.15296127 0.11191219][-0.14384922 -0.098552041 0.017155519 0.22429077 0.51903862 0.86039531 1.158293 1.3206804 1.2896047 1.0754421 0.7682817 0.47986996 0.28250733 0.18372583 0.15282494][-0.086714342 -0.083110206 -0.021844361 0.12230425 0.35308751 0.64384234 0.91902214 1.0944152 1.1044325 0.94437492 0.68471706 0.42704266 0.24745384 0.16595845 0.15974101][-0.030125901 -0.069057263 -0.070053183 -0.0015743409 0.14676775 0.36285824 0.59052682 0.76016128 0.80815423 0.718031 0.53430611 0.33463326 0.19002986 0.130911 0.14697862][0.022680428 -0.046145931 -0.098553993 -0.097772144 -0.028349523 0.10951446 0.28038463 0.43303871 0.51369256 0.49918225 0.40645674 0.280786 0.17776865 0.1325019 0.14893956][0.073242053 -0.0053182072 -0.085553586 -0.12908708 -0.11699369 -0.0407368 0.083609 0.22264205 0.33427292 0.39110541 0.38502157 0.32870656 0.25686055 0.20338444 0.18691975][0.11738909 0.046792269 -0.036665469 -0.097220413 -0.11485006 -0.0752264 0.020816468 0.15478833 0.29472962 0.41010866 0.47289607 0.468678 0.4115096 0.33025342 0.25627142][0.15379633 0.10276835 0.035224747 -0.020570368 -0.047777683 -0.029202977 0.046236496 0.17388652 0.33096325 0.48506454 0.59406096 0.62423849 0.57253283 0.46179441 0.33086765]]...]
INFO - root - 2017-12-11 08:47:07.372781: step 35810, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.514 sec/batch; 42h:20m:33s remains)
INFO - root - 2017-12-11 08:47:12.670533: step 35820, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 42h:59m:23s remains)
INFO - root - 2017-12-11 08:47:18.024030: step 35830, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 43h:24m:16s remains)
INFO - root - 2017-12-11 08:47:23.356349: step 35840, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 44h:49m:22s remains)
INFO - root - 2017-12-11 08:47:28.712457: step 35850, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 43h:07m:04s remains)
INFO - root - 2017-12-11 08:47:33.763677: step 35860, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.517 sec/batch; 42h:35m:36s remains)
INFO - root - 2017-12-11 08:47:39.109134: step 35870, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:18m:55s remains)
INFO - root - 2017-12-11 08:47:44.420511: step 35880, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.515 sec/batch; 42h:26m:45s remains)
INFO - root - 2017-12-11 08:47:49.804268: step 35890, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 46h:21m:10s remains)
INFO - root - 2017-12-11 08:47:55.254331: step 35900, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:03m:08s remains)
2017-12-11 08:47:55.777204: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18606606 0.18688177 0.18340932 0.1970561 0.2263419 0.25482196 0.27414802 0.28475976 0.28468907 0.27505007 0.27206129 0.28784835 0.31259525 0.33024317 0.32953048][0.16042325 0.17098518 0.18172531 0.21156849 0.25765029 0.30108643 0.33148146 0.34918672 0.35259828 0.34418249 0.34215561 0.35759676 0.3783201 0.38574702 0.36820728][0.1106964 0.13459615 0.16577955 0.21627796 0.28093886 0.34061107 0.38333145 0.40924489 0.41781676 0.41280237 0.40995455 0.41742781 0.42412898 0.41175193 0.36889765][0.043642115 0.083860248 0.14031836 0.21464519 0.29796511 0.37221515 0.42523453 0.458374 0.47349954 0.47511145 0.47110915 0.46485192 0.4500438 0.41193306 0.34110519][-0.015984505 0.041722942 0.12420161 0.2220661 0.32256788 0.40872416 0.46860725 0.50528419 0.52405542 0.53033638 0.522711 0.49975279 0.4609842 0.39624888 0.29909182][-0.04041063 0.031249573 0.13284613 0.24777283 0.36149696 0.45748609 0.52280045 0.55992609 0.57686162 0.58036363 0.56340814 0.52272612 0.46223 0.37503836 0.25727063][-0.03296474 0.043599542 0.15164661 0.27285266 0.39283979 0.49470145 0.56355119 0.59873623 0.60861152 0.60070908 0.56766 0.50890815 0.431954 0.33244362 0.20835078][-0.025428148 0.046173211 0.14921612 0.26706859 0.38512024 0.48510081 0.55091071 0.5793649 0.578381 0.55537319 0.5056774 0.43387935 0.35068733 0.25436056 0.14367494][-0.039951336 0.017570969 0.10614409 0.21245125 0.32052884 0.40978259 0.46399283 0.48023283 0.46660256 0.43021968 0.37046492 0.2968784 0.22231458 0.14681031 0.067934617][-0.071241289 -0.035270769 0.030306097 0.11667431 0.20732185 0.28006151 0.31860396 0.321614 0.29702932 0.25190973 0.19064304 0.12730168 0.075496979 0.035163522 -0.00016876603][-0.10403853 -0.092080042 -0.054352671 0.0049811117 0.071317807 0.12408783 0.14758933 0.14135019 0.11147292 0.064842559 0.011089708 -0.032350946 -0.052757144 -0.052165695 -0.042636618][-0.12466118 -0.13185407 -0.11922859 -0.088596992 -0.051299237 -0.0226491 -0.014805635 -0.02733363 -0.056282826 -0.095012538 -0.13076851 -0.14701691 -0.13512115 -0.099930719 -0.057226419][-0.13133869 -0.15045388 -0.15592746 -0.14970511 -0.1395998 -0.13421731 -0.14028324 -0.15633924 -0.17959131 -0.20377684 -0.2161992 -0.20512559 -0.16816227 -0.11300556 -0.056064032][-0.12759295 -0.15236212 -0.16844024 -0.17847694 -0.18670656 -0.19623789 -0.20943266 -0.22485803 -0.24053451 -0.25130674 -0.24659254 -0.21908094 -0.17113267 -0.1121029 -0.056840379][-0.11601228 -0.14052229 -0.15981266 -0.17696638 -0.19331239 -0.2085039 -0.22241655 -0.23443913 -0.24365705 -0.24663021 -0.23596385 -0.20689055 -0.16335876 -0.11412703 -0.07102225]]...]
INFO - root - 2017-12-11 08:48:01.178939: step 35910, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 43h:29m:45s remains)
INFO - root - 2017-12-11 08:48:06.584385: step 35920, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:42m:44s remains)
INFO - root - 2017-12-11 08:48:11.928040: step 35930, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:23m:08s remains)
INFO - root - 2017-12-11 08:48:17.190523: step 35940, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 44h:07m:08s remains)
INFO - root - 2017-12-11 08:48:22.515182: step 35950, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 42h:51m:13s remains)
INFO - root - 2017-12-11 08:48:27.609744: step 35960, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:10m:57s remains)
INFO - root - 2017-12-11 08:48:32.894164: step 35970, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.519 sec/batch; 42h:44m:28s remains)
INFO - root - 2017-12-11 08:48:38.175305: step 35980, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.518 sec/batch; 42h:39m:01s remains)
INFO - root - 2017-12-11 08:48:43.519964: step 35990, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.531 sec/batch; 43h:46m:09s remains)
INFO - root - 2017-12-11 08:48:48.931450: step 36000, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:54m:38s remains)
2017-12-11 08:48:49.505215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053481884 -0.05238948 -0.04040464 -0.015937753 0.020463685 0.065777481 0.11352913 0.15051673 0.16851929 0.16870506 0.16081068 0.15480791 0.15483814 0.16020752 0.16793711][-0.060266741 -0.059515815 -0.046001367 -0.017597897 0.025367714 0.079661064 0.13707127 0.18187334 0.2076305 0.21689445 0.21810682 0.21965908 0.2245041 0.23134705 0.23646772][-0.060614314 -0.060856652 -0.048121482 -0.019424515 0.025147928 0.082324073 0.14338037 0.19283664 0.22655226 0.24814981 0.26310766 0.27578416 0.28633261 0.29262283 0.29138812][-0.060881771 -0.061933517 -0.050549325 -0.023102526 0.020361008 0.076809704 0.13775326 0.18920884 0.22891237 0.26065046 0.28731582 0.30908847 0.32377633 0.3281545 0.32036519][-0.061250612 -0.062552355 -0.052014727 -0.026063532 0.015041489 0.068857014 0.12733005 0.17820099 0.22094432 0.25913686 0.29288954 0.31874603 0.33373508 0.33404222 0.31952894][-0.061334047 -0.063173927 -0.053406827 -0.028986918 0.0094236992 0.060203604 0.1160444 0.1672093 0.21415818 0.25875941 0.29689163 0.32238862 0.33298314 0.32538721 0.30144042][-0.061126504 -0.063807212 -0.055149648 -0.032344539 0.0035175325 0.051185731 0.10511946 0.15919633 0.21368377 0.26657367 0.30853158 0.33181217 0.3352983 0.3169795 0.28187549][-0.060997166 -0.063948639 -0.056046948 -0.034439638 -0.0006399994 0.043855149 0.095384784 0.15102103 0.21049708 0.26882455 0.31320113 0.33432978 0.33167738 0.30454114 0.26187807][-0.060907979 -0.063677594 -0.055822752 -0.034874748 -0.0028018267 0.038664456 0.086637378 0.1392107 0.19594546 0.25245968 0.29613161 0.3159427 0.31050625 0.27979416 0.23627445][-0.061012786 -0.063784771 -0.056413483 -0.036803517 -0.0067192959 0.032889359 0.078896128 0.12813017 0.17883259 0.22855309 0.26712191 0.28389564 0.27703694 0.24704549 0.20756833][-0.061122302 -0.063914694 -0.057621576 -0.039873194 -0.011091816 0.028989926 0.076664634 0.12657447 0.17389289 0.21654496 0.2467424 0.2572538 0.24832068 0.22129907 0.1879724][-0.060648382 -0.063052639 -0.057746161 -0.042031135 -0.014695454 0.02538183 0.074485831 0.12580338 0.17185651 0.20986372 0.23315266 0.23815535 0.22802003 0.20568398 0.18006748][-0.059566148 -0.06105151 -0.056241184 -0.042692833 -0.018651646 0.017379541 0.062673852 0.1109227 0.15482304 0.19154629 0.21422756 0.21962772 0.211831 0.19539781 0.176978][-0.058479182 -0.058982145 -0.054376867 -0.042759556 -0.022744611 0.0072665559 0.045795374 0.088023953 0.12850776 0.16481368 0.18977378 0.19894207 0.19553456 0.18509537 0.17252652][-0.057579119 -0.057269204 -0.052743539 -0.042222049 -0.024633065 0.0015264817 0.035315376 0.073690541 0.11234076 0.14872147 0.17595357 0.18935487 0.19133317 0.18662524 0.17816153]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:48:54.772336: step 36010, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 43h:23m:56s remains)
INFO - root - 2017-12-11 08:49:00.212258: step 36020, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:02m:56s remains)
INFO - root - 2017-12-11 08:49:05.618737: step 36030, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:55m:07s remains)
INFO - root - 2017-12-11 08:49:10.995328: step 36040, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:06m:34s remains)
INFO - root - 2017-12-11 08:49:16.323752: step 36050, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:18m:15s remains)
INFO - root - 2017-12-11 08:49:21.302199: step 36060, loss = 0.71, batch loss = 0.65 (22.6 examples/sec; 0.354 sec/batch; 29h:08m:03s remains)
INFO - root - 2017-12-11 08:49:26.744518: step 36070, loss = 0.66, batch loss = 0.61 (14.4 examples/sec; 0.554 sec/batch; 45h:38m:24s remains)
INFO - root - 2017-12-11 08:49:32.154979: step 36080, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:11m:06s remains)
INFO - root - 2017-12-11 08:49:37.466409: step 36090, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:29m:54s remains)
INFO - root - 2017-12-11 08:49:42.741267: step 36100, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:21m:51s remains)
2017-12-11 08:49:43.297553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11051341 0.094797388 0.076989777 0.0621955 0.053520612 0.050017852 0.04742657 0.042412635 0.032168243 0.015977578 -0.0013988343 -0.015987892 -0.024107274 -0.020619184 -0.0030889055][0.2468424 0.22608605 0.19589521 0.17003725 0.15456863 0.14947307 0.14621064 0.13711308 0.11729211 0.086240225 0.053390015 0.025535787 0.008785164 0.011624791 0.039075471][0.40243325 0.38059926 0.33947074 0.30354133 0.28215811 0.27516004 0.26987812 0.25431058 0.22177142 0.17160225 0.1180933 0.073175147 0.045731463 0.046922494 0.083973967][0.51794648 0.49838883 0.45338047 0.4158681 0.39661729 0.39287367 0.38799948 0.3670311 0.32269123 0.25425664 0.17922448 0.11582652 0.076207541 0.073483281 0.11552704][0.55477494 0.5407536 0.5020113 0.47481212 0.46896362 0.47627458 0.476 0.45229959 0.39893803 0.31597087 0.22255082 0.14289798 0.09240818 0.084849261 0.12623429][0.5144251 0.51144785 0.49044335 0.48484993 0.50117958 0.52451241 0.53070706 0.50514817 0.4443942 0.35110843 0.24485387 0.15354434 0.0953705 0.083298877 0.12019639][0.40998432 0.42227447 0.42851564 0.4519178 0.49391806 0.53373766 0.54578263 0.51903528 0.45401973 0.35743061 0.24757363 0.15205054 0.090690292 0.074191138 0.10252312][0.2664625 0.29327947 0.32752633 0.37862092 0.4433496 0.49814621 0.51791739 0.49558336 0.434663 0.34540516 0.24368873 0.15285514 0.092815533 0.071797013 0.088208839][0.13589652 0.17355646 0.22753876 0.29671523 0.37564474 0.442014 0.47234932 0.46041304 0.41070339 0.33588821 0.25018984 0.17012092 0.11441421 0.08976981 0.094003417][0.071171336 0.11427958 0.17371854 0.24485204 0.32413894 0.39445102 0.43486533 0.43706986 0.4039478 0.34771466 0.28130788 0.21404524 0.16265586 0.13463572 0.12831959][0.078953527 0.12001217 0.17063563 0.22776718 0.29240596 0.35574958 0.40149453 0.41858512 0.40618449 0.37241042 0.32780346 0.2746785 0.22627367 0.19349863 0.1776453][0.12841782 0.16303627 0.19678129 0.23077087 0.27205175 0.32086921 0.36716694 0.3965826 0.4034996 0.39065766 0.36611018 0.32714468 0.28150511 0.24313498 0.21718495][0.19095343 0.21889751 0.23625095 0.24970365 0.2719222 0.3094593 0.35699904 0.39508009 0.4127472 0.40885931 0.3913534 0.35787845 0.31173062 0.26749071 0.23230386][0.23708177 0.26107723 0.26971132 0.27549914 0.29328182 0.33087495 0.38406545 0.42767024 0.44544074 0.43450209 0.40588275 0.36377516 0.31104735 0.26071888 0.21890323][0.2481405 0.27064732 0.27833846 0.28772271 0.31230468 0.35674927 0.4162297 0.46204826 0.47436753 0.44936612 0.40174413 0.34435779 0.28251058 0.22674684 0.18131147]]...]
INFO - root - 2017-12-11 08:49:48.645650: step 36110, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:10m:14s remains)
INFO - root - 2017-12-11 08:49:53.923644: step 36120, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:13m:41s remains)
INFO - root - 2017-12-11 08:49:59.283622: step 36130, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:16m:29s remains)
INFO - root - 2017-12-11 08:50:04.641811: step 36140, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 43h:58m:48s remains)
INFO - root - 2017-12-11 08:50:09.961813: step 36150, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:11m:26s remains)
INFO - root - 2017-12-11 08:50:15.339814: step 36160, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 44h:20m:11s remains)
INFO - root - 2017-12-11 08:50:20.357702: step 36170, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:55m:21s remains)
INFO - root - 2017-12-11 08:50:25.740488: step 36180, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 43h:28m:08s remains)
INFO - root - 2017-12-11 08:50:31.093713: step 36190, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:02m:16s remains)
INFO - root - 2017-12-11 08:50:36.518464: step 36200, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:33m:21s remains)
2017-12-11 08:50:37.111869: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2018528 0.16651616 0.14192022 0.14546517 0.17805783 0.22871037 0.27254972 0.28531402 0.24949647 0.171525 0.078161336 -0.0018470364 -0.052914742 -0.074680246 -0.077599354][0.15409189 0.1237205 0.11073844 0.13414003 0.19429787 0.27424848 0.34021574 0.36077178 0.31745985 0.22170743 0.10698389 0.009571326 -0.051973879 -0.078154452 -0.082199417][0.098819815 0.0785918 0.079233341 0.12009151 0.20105191 0.30125093 0.38094 0.40466145 0.35448751 0.24597444 0.11804862 0.011340823 -0.054288987 -0.080769777 -0.08358103][0.043629359 0.034325536 0.04721136 0.10029264 0.19259237 0.30070856 0.38338551 0.40494898 0.3497856 0.23658915 0.10633913 0.00029296114 -0.062653229 -0.085642546 -0.085499711][-0.00313546 -0.0046680979 0.01764434 0.078050055 0.17444657 0.28206256 0.36112863 0.37826839 0.32088953 0.20957716 0.084694155 -0.014623274 -0.071593553 -0.090031862 -0.087024085][-0.028279623 -0.025781117 0.0016567345 0.063740931 0.15747125 0.25847033 0.3301346 0.3421835 0.28465608 0.17871235 0.062345039 -0.028318968 -0.07875786 -0.093146868 -0.088073187][-0.02700969 -0.023742251 0.0035248643 0.062054209 0.14819506 0.23884875 0.30106276 0.30743942 0.25002468 0.14997625 0.042306047 -0.03982161 -0.084183484 -0.0952076 -0.088704959][-0.01106828 -0.009733431 0.013895154 0.066643566 0.14419514 0.22470397 0.27799609 0.27889204 0.22104709 0.12583007 0.02576052 -0.048850961 -0.08803349 -0.09637522 -0.088853262][0.0081145708 0.0062019997 0.0257066 0.0745181 0.14767575 0.22378266 0.27333474 0.27144319 0.21239986 0.1182509 0.020886565 -0.050793011 -0.0881519 -0.095789351 -0.088221163][0.031534646 0.025700629 0.041302346 0.088948175 0.16368827 0.24318661 0.29567882 0.29414347 0.23224071 0.13324551 0.030948492 -0.044592578 -0.084609352 -0.093888834 -0.08723785][0.057574626 0.046922598 0.057391047 0.1029727 0.1797816 0.26417685 0.321107 0.32076311 0.25571787 0.15051572 0.041662086 -0.038858078 -0.081896909 -0.09284997 -0.086900249][0.0807642 0.065875366 0.070649236 0.11138508 0.1853679 0.26839724 0.32412958 0.32221198 0.25487798 0.1474632 0.037778359 -0.042076223 -0.08393909 -0.094166905 -0.087643042][0.088089675 0.070798196 0.071468733 0.10621957 0.17279759 0.24791089 0.29692343 0.29184297 0.22536485 0.12222351 0.019126507 -0.053783827 -0.09028402 -0.097500294 -0.089250751][0.0776964 0.060524791 0.05942899 0.088679112 0.14651255 0.21172903 0.25314197 0.24641901 0.18447493 0.089792348 -0.0033983318 -0.067305252 -0.097400919 -0.10113893 -0.091019258][0.068034075 0.05181944 0.051426776 0.078076512 0.12926243 0.18522453 0.2183737 0.20859125 0.14947949 0.061761044 -0.022736866 -0.078685954 -0.10317567 -0.10393142 -0.092396043]]...]
INFO - root - 2017-12-11 08:50:42.412318: step 36210, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 43h:04m:10s remains)
INFO - root - 2017-12-11 08:50:47.811308: step 36220, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:22m:25s remains)
INFO - root - 2017-12-11 08:50:53.206952: step 36230, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 44h:06m:20s remains)
INFO - root - 2017-12-11 08:50:58.604404: step 36240, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 44h:17m:47s remains)
INFO - root - 2017-12-11 08:51:03.917353: step 36250, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 44h:35m:15s remains)
INFO - root - 2017-12-11 08:51:09.304883: step 36260, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 43h:00m:34s remains)
INFO - root - 2017-12-11 08:51:14.375832: step 36270, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:10m:01s remains)
INFO - root - 2017-12-11 08:51:19.802694: step 36280, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 44h:34m:09s remains)
INFO - root - 2017-12-11 08:51:25.171710: step 36290, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 45h:47m:17s remains)
INFO - root - 2017-12-11 08:51:30.581871: step 36300, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.575 sec/batch; 47h:17m:28s remains)
2017-12-11 08:51:31.142584: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048285533 0.040553767 0.041049305 0.046377961 0.053007122 0.064888075 0.07880196 0.10733438 0.13984059 0.16391334 0.17287187 0.17052986 0.15665616 0.11690071 0.0630929][0.063650586 0.058795493 0.058723383 0.062930286 0.067454733 0.075477861 0.083119296 0.10477517 0.13403828 0.15644419 0.16493995 0.16534574 0.15595445 0.11898707 0.064626716][0.088484392 0.092732169 0.0985377 0.10844031 0.11514272 0.11905437 0.11651994 0.12585677 0.1445576 0.15797034 0.16167517 0.16173252 0.15405439 0.11778191 0.063378677][0.12252466 0.1431839 0.16553147 0.191526 0.20847541 0.21261697 0.20021421 0.19403665 0.19630487 0.19383383 0.18609866 0.17956834 0.16801652 0.12751208 0.068431035][0.16585632 0.20924211 0.25672364 0.30734083 0.34285554 0.35455546 0.33667892 0.31577161 0.29956141 0.27708083 0.25157261 0.23061043 0.20753559 0.15571357 0.08505109][0.20990717 0.27712318 0.3509081 0.42738736 0.48463756 0.51034296 0.49441436 0.46579525 0.43736103 0.39951485 0.35666823 0.31715265 0.27619109 0.20564088 0.11554378][0.23453425 0.31624228 0.40645435 0.49983951 0.57414734 0.61591762 0.6097846 0.58518428 0.55801672 0.51735276 0.46522591 0.40978232 0.35033423 0.25900772 0.1466818][0.22721156 0.305975 0.39446464 0.48791179 0.56645775 0.61809605 0.62417692 0.61229831 0.598346 0.56814784 0.51890773 0.45696568 0.38644424 0.2816743 0.15444742][0.18737096 0.24668385 0.31585222 0.39197195 0.45855764 0.50733221 0.52067089 0.52241689 0.52610856 0.51470023 0.47973228 0.42390719 0.3544203 0.25004047 0.12336511][0.11846595 0.1498625 0.19077766 0.23972026 0.2833204 0.31763551 0.32998037 0.33941695 0.35614944 0.36285496 0.34727144 0.30754372 0.25083241 0.16187896 0.053857189][0.032396447 0.038051795 0.05229114 0.073513351 0.09178099 0.10745422 0.11419643 0.12478194 0.1451029 0.16075674 0.16024245 0.1387524 0.101508 0.03919214 -0.035834562][-0.048164956 -0.058281612 -0.06063322 -0.058914844 -0.059461504 -0.058824845 -0.057388186 -0.048714954 -0.03192186 -0.017325921 -0.012222373 -0.020798627 -0.039831627 -0.073813468 -0.11321991][-0.10377792 -0.11758912 -0.12367828 -0.12912022 -0.13733144 -0.14406161 -0.14614812 -0.14092918 -0.13046552 -0.12129901 -0.11674352 -0.11873045 -0.1253745 -0.13786031 -0.15033191][-0.12713093 -0.13783243 -0.14016101 -0.1440551 -0.15089193 -0.15755279 -0.16073681 -0.15912054 -0.15451477 -0.15006778 -0.1470097 -0.14597738 -0.14607526 -0.14696163 -0.14561222][-0.12103662 -0.12810405 -0.12657934 -0.12687545 -0.12934883 -0.13258268 -0.13485873 -0.13539958 -0.13467133 -0.13346036 -0.13191685 -0.13007151 -0.12770343 -0.12449564 -0.11925403]]...]
INFO - root - 2017-12-11 08:51:36.459422: step 36310, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 46h:05m:52s remains)
INFO - root - 2017-12-11 08:51:41.835227: step 36320, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 43h:57m:45s remains)
INFO - root - 2017-12-11 08:51:47.215718: step 36330, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 45h:46m:06s remains)
INFO - root - 2017-12-11 08:51:52.575802: step 36340, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 43h:02m:28s remains)
INFO - root - 2017-12-11 08:51:57.969137: step 36350, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:56m:39s remains)
INFO - root - 2017-12-11 08:52:03.293877: step 36360, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 43h:59m:48s remains)
INFO - root - 2017-12-11 08:52:08.380801: step 36370, loss = 0.69, batch loss = 0.63 (13.3 examples/sec; 0.600 sec/batch; 49h:21m:05s remains)
INFO - root - 2017-12-11 08:52:13.638279: step 36380, loss = 0.69, batch loss = 0.63 (15.9 examples/sec; 0.505 sec/batch; 41h:30m:12s remains)
INFO - root - 2017-12-11 08:52:18.993141: step 36390, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 44h:33m:06s remains)
INFO - root - 2017-12-11 08:52:24.369719: step 36400, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 44h:27m:34s remains)
2017-12-11 08:52:24.951787: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17678331 0.23594272 0.26616991 0.26891556 0.26091036 0.25655434 0.26035425 0.27845803 0.30972952 0.33958602 0.34985891 0.33543178 0.31251952 0.29256076 0.27532989][0.20922139 0.27517316 0.3052026 0.3003251 0.2784299 0.25876454 0.25311431 0.27233157 0.31545398 0.36368886 0.39292562 0.39335042 0.37582526 0.34998116 0.31826532][0.21337354 0.28246489 0.31561965 0.31106395 0.28269368 0.25012851 0.2304821 0.237536 0.27290839 0.32085413 0.35983443 0.37785262 0.37658066 0.35685882 0.32148832][0.19175489 0.26133424 0.30310696 0.31098428 0.29113093 0.25824654 0.22870587 0.21501054 0.22281303 0.24653193 0.27577883 0.30049172 0.3142077 0.30846292 0.28280464][0.15816724 0.22457463 0.2760337 0.30402607 0.30697829 0.29087591 0.26495779 0.23433088 0.20709287 0.19115324 0.19135779 0.20342629 0.21740142 0.22025387 0.20747791][0.12767355 0.18750896 0.24553053 0.29470789 0.32924184 0.34518409 0.34047803 0.30877516 0.25712875 0.20498236 0.17026208 0.15574597 0.15230927 0.14823896 0.13779947][0.10687305 0.15773495 0.21608005 0.28031358 0.34580693 0.40106505 0.43213046 0.42077798 0.36950353 0.30270937 0.24509625 0.20461269 0.17553905 0.15091889 0.12699178][0.090989843 0.13178505 0.18340248 0.25182727 0.3383292 0.4288924 0.50098133 0.52693874 0.50220013 0.44813031 0.3886542 0.33357382 0.28181952 0.23282753 0.185711][0.071302682 0.10088695 0.13956095 0.19930747 0.28877363 0.39689952 0.49870849 0.56223929 0.57592547 0.55276865 0.51014471 0.45717883 0.39733014 0.33542469 0.27072346][0.046771783 0.064288445 0.085695773 0.12535629 0.19766633 0.29801747 0.40492156 0.48812881 0.53181046 0.53982 0.52106363 0.48301616 0.43247211 0.37698182 0.3128328][0.031085478 0.038124286 0.041826885 0.056059521 0.097875677 0.16973011 0.25698319 0.33500436 0.38699567 0.41045329 0.40816191 0.38599464 0.35240212 0.31523064 0.26780105][0.038377702 0.040292505 0.030764306 0.022664269 0.03264977 0.06853155 0.12311321 0.17865069 0.21922597 0.23957646 0.24036866 0.22626655 0.20589691 0.18629201 0.16007698][0.076463915 0.081956834 0.067865871 0.046750605 0.031995781 0.034437213 0.054394223 0.082310081 0.10398337 0.11267899 0.10923029 0.097084694 0.083911061 0.075934947 0.067334957][0.14110319 0.15908889 0.15051423 0.12799807 0.099644586 0.077610143 0.070340984 0.077642366 0.086669587 0.087480843 0.081074663 0.06916301 0.057755746 0.05325881 0.053168535][0.21441224 0.25045264 0.25550559 0.24178676 0.21148585 0.17677943 0.15379797 0.15144494 0.15746967 0.157487 0.15271036 0.14301679 0.13138445 0.12437315 0.12355759]]...]
INFO - root - 2017-12-11 08:52:30.307344: step 36410, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 42h:27m:49s remains)
INFO - root - 2017-12-11 08:52:35.688997: step 36420, loss = 0.67, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 43h:47m:23s remains)
INFO - root - 2017-12-11 08:52:40.965222: step 36430, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 43h:20m:41s remains)
INFO - root - 2017-12-11 08:52:46.321439: step 36440, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:09m:12s remains)
INFO - root - 2017-12-11 08:52:51.647605: step 36450, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 44h:10m:31s remains)
INFO - root - 2017-12-11 08:52:56.951326: step 36460, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.558 sec/batch; 45h:54m:21s remains)
INFO - root - 2017-12-11 08:53:02.309488: step 36470, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 43h:04m:00s remains)
INFO - root - 2017-12-11 08:53:07.444420: step 36480, loss = 0.67, batch loss = 0.61 (14.2 examples/sec; 0.564 sec/batch; 46h:20m:22s remains)
INFO - root - 2017-12-11 08:53:12.868277: step 36490, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 43h:41m:31s remains)
INFO - root - 2017-12-11 08:53:18.178574: step 36500, loss = 0.68, batch loss = 0.62 (15.6 examples/sec; 0.513 sec/batch; 42h:10m:48s remains)
2017-12-11 08:53:18.769211: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28277314 0.2714518 0.22977799 0.17430629 0.12804349 0.10436124 0.09930215 0.10040328 0.10219596 0.10395803 0.10569159 0.1281397 0.17482994 0.22594613 0.25723115][0.36929911 0.36932173 0.33025068 0.26648515 0.20627825 0.17148896 0.16273695 0.16358982 0.16303584 0.16105899 0.15878539 0.1830342 0.23740141 0.29546863 0.32788983][0.41404775 0.43133882 0.40726471 0.3502554 0.28912532 0.25173432 0.24334934 0.24460371 0.23797597 0.22566563 0.21197349 0.22800489 0.27750903 0.33042979 0.35545349][0.41517437 0.44958603 0.44707423 0.4120073 0.3690578 0.3453702 0.34682947 0.35118493 0.33496568 0.30427545 0.26923186 0.26420969 0.29488784 0.33139345 0.34329453][0.38568589 0.42622554 0.44208118 0.43758485 0.43134832 0.44212291 0.46834907 0.48188791 0.45483783 0.39858073 0.33185884 0.29276943 0.29282448 0.30680573 0.30676457][0.34931129 0.38317019 0.40421975 0.42501608 0.46179289 0.52079558 0.58608425 0.6181072 0.58477151 0.5023337 0.39927211 0.31946167 0.28425154 0.27529448 0.26775968][0.3278392 0.34665403 0.35791805 0.38754883 0.45578676 0.55903196 0.6670289 0.72594976 0.69782972 0.60088456 0.47084391 0.35787818 0.29358789 0.26577887 0.2530787][0.32452449 0.32347891 0.31332603 0.33307981 0.41025886 0.53730386 0.675998 0.76108181 0.748773 0.65497148 0.51766115 0.39082089 0.31241897 0.27396309 0.25770077][0.33154219 0.31157491 0.27560872 0.27149868 0.33357045 0.45603475 0.60145974 0.70067853 0.70665425 0.63011622 0.506449 0.38839936 0.31411213 0.276019 0.2595894][0.33484024 0.30420661 0.24796592 0.21597138 0.24565738 0.33717361 0.4617767 0.55619854 0.57512152 0.52285308 0.42802495 0.33754048 0.28431603 0.25917688 0.24936366][0.34784958 0.3151446 0.24690014 0.18951353 0.17999113 0.22543477 0.30930433 0.38078761 0.40168369 0.37135637 0.31030545 0.25783175 0.23782228 0.23696692 0.24160185][0.38195795 0.35252669 0.27758053 0.19869776 0.15297824 0.1541366 0.19512397 0.23715323 0.25110382 0.2346096 0.20085138 0.18363133 0.19827069 0.22558768 0.24876529][0.42947304 0.40913785 0.33416891 0.2418015 0.1707598 0.14121725 0.15141788 0.16989522 0.17446533 0.16330716 0.14472003 0.1511735 0.19233805 0.24214973 0.27865675][0.45432088 0.44670621 0.37977186 0.2850374 0.20190114 0.15642317 0.15106815 0.15799595 0.15768085 0.1489906 0.13739789 0.1567418 0.21401368 0.27624786 0.31674585][0.43310735 0.4356752 0.38090938 0.29222879 0.20649932 0.15414582 0.14284804 0.1474424 0.14768361 0.14238895 0.13528629 0.16033775 0.22287703 0.28588116 0.32108089]]...]
INFO - root - 2017-12-11 08:53:24.165223: step 36510, loss = 0.67, batch loss = 0.61 (14.8 examples/sec; 0.539 sec/batch; 44h:18m:55s remains)
INFO - root - 2017-12-11 08:53:29.529483: step 36520, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:41m:05s remains)
INFO - root - 2017-12-11 08:53:34.771015: step 36530, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 45h:58m:10s remains)
INFO - root - 2017-12-11 08:53:40.180474: step 36540, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.522 sec/batch; 42h:53m:50s remains)
INFO - root - 2017-12-11 08:53:45.534790: step 36550, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 42h:58m:10s remains)
INFO - root - 2017-12-11 08:53:50.892838: step 36560, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 43h:59m:51s remains)
INFO - root - 2017-12-11 08:53:56.138557: step 36570, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.519 sec/batch; 42h:41m:07s remains)
INFO - root - 2017-12-11 08:54:01.169265: step 36580, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:36m:06s remains)
INFO - root - 2017-12-11 08:54:06.527305: step 36590, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 43h:32m:00s remains)
INFO - root - 2017-12-11 08:54:11.850985: step 36600, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 43h:04m:06s remains)
2017-12-11 08:54:12.443214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01910671 -0.013319734 -0.0074616312 -0.0048138951 -0.0071400967 -0.013136536 -0.018611092 -0.022631513 -0.025510918 -0.02729618 -0.027260739 -0.025218092 -0.02238333 -0.020642145 -0.021719357][0.0019259125 0.018792465 0.037100226 0.048181336 0.045903072 0.032825578 0.020438336 0.010395701 0.00099751435 -0.0076716254 -0.012121728 -0.010921699 -0.00675101 -0.0031266252 -0.0038220135][0.033570506 0.0707533 0.11208527 0.13981336 0.14063574 0.11938351 0.098419711 0.0796523 0.058898266 0.036546711 0.021592038 0.017652616 0.019847402 0.02337311 0.02266846][0.068753712 0.13286987 0.20595282 0.25838086 0.26768893 0.24174955 0.21485667 0.18846886 0.15398376 0.11202034 0.080092728 0.065116584 0.059411176 0.057945751 0.054696839][0.098240927 0.18934421 0.29541928 0.37554726 0.39908847 0.37737522 0.35402626 0.32804725 0.28399995 0.2231174 0.17304267 0.14358026 0.1225461 0.10671521 0.093538247][0.11204057 0.2226471 0.35373545 0.45643741 0.4958635 0.48711815 0.47813436 0.46318629 0.4196977 0.35065144 0.29231581 0.25349921 0.21389884 0.17386775 0.14024939][0.10684744 0.22444817 0.36591926 0.47964987 0.5306592 0.53785586 0.54864359 0.55262733 0.52149034 0.46093172 0.41211474 0.37669587 0.32251406 0.25400066 0.19282328][0.089979462 0.20128614 0.33561617 0.4442918 0.49564743 0.51226723 0.53824681 0.5615415 0.55053765 0.51424336 0.49367088 0.47769612 0.42168489 0.33244264 0.24699874][0.06770318 0.16234885 0.27548683 0.36546817 0.40591055 0.42136678 0.45245904 0.48912862 0.49908164 0.49339071 0.50903058 0.52223372 0.47977355 0.38742489 0.29176298][0.042300411 0.11514653 0.20093162 0.26624259 0.29044193 0.29740888 0.3234866 0.361726 0.38259107 0.39880496 0.44207615 0.48081094 0.45973963 0.38166654 0.29395303][0.012611382 0.06348177 0.1235275 0.167213 0.17782044 0.17581379 0.19197498 0.22136049 0.23922187 0.25944793 0.30977404 0.35755903 0.35289064 0.29662228 0.23018815][-0.020104881 0.010647074 0.048886433 0.076306559 0.0787693 0.071619116 0.079036944 0.097338438 0.1064181 0.11868105 0.15824631 0.19779964 0.19855332 0.16087331 0.1176995][-0.050457314 -0.036676504 -0.017123777 -0.003106931 -0.0055423537 -0.01440907 -0.012310436 -0.002092083 0.0007844792 0.0046239942 0.027803127 0.051569339 0.050553009 0.025450872 4.6304704e-05][-0.075299993 -0.077017322 -0.074452735 -0.073234849 -0.079678819 -0.0880266 -0.087403111 -0.079883486 -0.076014973 -0.0728721 -0.059642103 -0.047121506 -0.049261414 -0.0654717 -0.080378115][-0.08900556 -0.10199051 -0.11222921 -0.12111205 -0.13037616 -0.13690016 -0.1354064 -0.12739994 -0.11911504 -0.11128856 -0.099643089 -0.090080082 -0.089485936 -0.097726509 -0.10599601]]...]
INFO - root - 2017-12-11 08:54:17.814625: step 36610, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 44h:16m:12s remains)
INFO - root - 2017-12-11 08:54:23.145282: step 36620, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 43h:54m:45s remains)
INFO - root - 2017-12-11 08:54:28.518222: step 36630, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 43h:48m:54s remains)
INFO - root - 2017-12-11 08:54:33.908670: step 36640, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 44h:15m:53s remains)
INFO - root - 2017-12-11 08:54:39.323344: step 36650, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:19m:14s remains)
INFO - root - 2017-12-11 08:54:44.636943: step 36660, loss = 0.71, batch loss = 0.66 (15.4 examples/sec; 0.520 sec/batch; 42h:43m:23s remains)
INFO - root - 2017-12-11 08:54:50.079159: step 36670, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 44h:48m:53s remains)
INFO - root - 2017-12-11 08:54:55.197333: step 36680, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 43h:45m:03s remains)
INFO - root - 2017-12-11 08:55:00.634859: step 36690, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 46h:19m:11s remains)
INFO - root - 2017-12-11 08:55:06.006001: step 36700, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.535 sec/batch; 43h:59m:16s remains)
2017-12-11 08:55:06.610377: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15546681 0.19047438 0.19299942 0.16663101 0.12417965 0.081873491 0.054057654 0.043820821 0.044045866 0.043830767 0.038161352 0.032168649 0.029987678 0.030983541 0.036663622][0.29751369 0.35663253 0.36491862 0.32902676 0.26822785 0.20660585 0.16701408 0.15379851 0.15476023 0.15168375 0.13776472 0.12364694 0.11581283 0.11277945 0.11663416][0.452137 0.53576124 0.55078948 0.50991255 0.43901971 0.36807075 0.32561624 0.31490377 0.31701148 0.3065162 0.27769294 0.24905898 0.23006612 0.21824186 0.21560878][0.57632232 0.67750114 0.70023447 0.66627812 0.60532886 0.5477162 0.52012926 0.5200606 0.52140814 0.49464357 0.44116139 0.39020145 0.35445583 0.3300378 0.31687662][0.63576263 0.74261868 0.77465349 0.76309317 0.73486686 0.71399504 0.71730614 0.73307884 0.72962135 0.67798477 0.592789 0.51674622 0.46589157 0.43207541 0.40900087][0.61370069 0.71538633 0.76118523 0.78728092 0.80822575 0.83505744 0.8705706 0.895964 0.87982434 0.79947895 0.68379056 0.58766729 0.529667 0.49467924 0.46614888][0.52301228 0.61632031 0.68370587 0.75920814 0.83746439 0.91021413 0.96551931 0.98345268 0.94194716 0.83085209 0.68956715 0.58032709 0.52177525 0.49083656 0.4609431][0.38282582 0.47043297 0.56479043 0.6905759 0.82070309 0.92534369 0.98114032 0.97338235 0.89669961 0.75796711 0.6014623 0.48924533 0.43597126 0.41166067 0.383514][0.2161095 0.29847482 0.41418257 0.57314324 0.73098892 0.84250867 0.87985641 0.83811307 0.73016244 0.57883978 0.42695564 0.32703638 0.28575185 0.26915139 0.24554276][0.060868137 0.13655457 0.2600477 0.42448738 0.57649612 0.66725826 0.67364365 0.60257638 0.47989964 0.33755603 0.21098667 0.13639021 0.11086196 0.1020991 0.08635851][-0.044706076 0.023259558 0.14183384 0.2901344 0.4136126 0.46953949 0.44522676 0.3582972 0.23908469 0.12031174 0.026966402 -0.021204332 -0.033867631 -0.036319893 -0.041838609][-0.089847676 -0.031325217 0.071117327 0.18981034 0.27604645 0.29818684 0.25424641 0.16835462 0.068387076 -0.018418839 -0.077067755 -0.10038117 -0.10183899 -0.098935038 -0.096094117][-0.085655339 -0.042994697 0.030726306 0.10930184 0.15537646 0.14985876 0.10015257 0.031472534 -0.035853516 -0.084011175 -0.10725272 -0.10616332 -0.095123224 -0.086384356 -0.078557678][-0.050388947 -0.028111355 0.011101886 0.047715805 0.057438426 0.032661889 -0.012326396 -0.056269154 -0.089005306 -0.1031247 -0.099789523 -0.082333714 -0.062757969 -0.049413081 -0.038722366][-0.014586173 -0.008146029 0.0040944805 0.010035224 -0.004341187 -0.038672991 -0.075939171 -0.10114928 -0.11336055 -0.11205923 -0.10029028 -0.079871975 -0.059467375 -0.044717669 -0.032602027]]...]
INFO - root - 2017-12-11 08:55:11.998414: step 36710, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:47m:47s remains)
INFO - root - 2017-12-11 08:55:17.434208: step 36720, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 44h:45m:04s remains)
INFO - root - 2017-12-11 08:55:22.775663: step 36730, loss = 0.68, batch loss = 0.62 (15.7 examples/sec; 0.509 sec/batch; 41h:50m:42s remains)
INFO - root - 2017-12-11 08:55:28.107736: step 36740, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:25m:04s remains)
INFO - root - 2017-12-11 08:55:33.431112: step 36750, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 45h:38m:47s remains)
INFO - root - 2017-12-11 08:55:38.874684: step 36760, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:14m:03s remains)
INFO - root - 2017-12-11 08:55:44.118177: step 36770, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 43h:54m:34s remains)
INFO - root - 2017-12-11 08:55:49.325445: step 36780, loss = 0.69, batch loss = 0.63 (20.4 examples/sec; 0.391 sec/batch; 32h:09m:20s remains)
INFO - root - 2017-12-11 08:55:54.592077: step 36790, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:29m:45s remains)
INFO - root - 2017-12-11 08:55:59.937014: step 36800, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 43h:17m:03s remains)
2017-12-11 08:56:00.560647: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40534928 0.47766945 0.52210772 0.53236419 0.49984634 0.43810618 0.3682811 0.30625245 0.26187333 0.23998187 0.24175389 0.259479 0.290121 0.33239615 0.38109797][0.45459214 0.53751272 0.59020352 0.60595465 0.57090861 0.49858803 0.41158921 0.33112806 0.26617086 0.22852398 0.22709328 0.25280043 0.29826829 0.35501888 0.4139314][0.4636761 0.54904431 0.60726959 0.63432539 0.61233085 0.54807889 0.45953193 0.367696 0.28083667 0.21894643 0.20116496 0.22166261 0.27093118 0.33593565 0.40339303][0.44664481 0.52797252 0.59117639 0.6367625 0.64399332 0.60847628 0.53660578 0.4436346 0.33543459 0.24141121 0.19337766 0.19342193 0.23284045 0.29816219 0.37442368][0.42113003 0.49222896 0.55513245 0.61667925 0.65725535 0.66155636 0.62090617 0.53940153 0.41761869 0.29245359 0.20789187 0.17758134 0.19700566 0.25597513 0.340345][0.39903635 0.45124832 0.50205219 0.56628609 0.62944973 0.66961271 0.6641885 0.604017 0.48289022 0.34018439 0.22781952 0.17117743 0.17106596 0.22058406 0.30780149][0.37991613 0.40788224 0.43687409 0.48891577 0.55736876 0.61915958 0.64278561 0.60709459 0.49971998 0.35788423 0.23591827 0.16812281 0.15981051 0.20490679 0.29146633][0.37994096 0.38490149 0.39032558 0.42121378 0.47793347 0.540759 0.57684809 0.55894655 0.47188097 0.34746686 0.23583741 0.17465438 0.17162208 0.21961774 0.30326763][0.40028679 0.39274439 0.3814857 0.39117074 0.42588982 0.47105885 0.49877578 0.48289466 0.41157454 0.31227183 0.22601427 0.18583174 0.19735594 0.25109625 0.32787538][0.42787588 0.41895854 0.40246496 0.39975327 0.41411301 0.434796 0.44112685 0.41291043 0.34601077 0.26753336 0.21056873 0.19692212 0.22459804 0.28002375 0.34263533][0.44074437 0.44097561 0.43001798 0.4269855 0.43201911 0.43527186 0.42162514 0.37752587 0.30650386 0.23761094 0.20021383 0.20496626 0.24125624 0.2915675 0.3364217][0.4303976 0.44102839 0.4409388 0.44575977 0.45296553 0.45218942 0.4302012 0.37714931 0.30141208 0.23342457 0.20160805 0.20984291 0.24197206 0.28016269 0.30765921][0.39407721 0.41466558 0.42542166 0.44030964 0.45465273 0.45750007 0.43569049 0.38205522 0.30676663 0.23947051 0.20747952 0.21134762 0.23350957 0.25765455 0.2706514][0.34726971 0.37302759 0.38997513 0.41055769 0.42940041 0.43579537 0.41784561 0.3705619 0.30354097 0.24292995 0.21237451 0.2117144 0.22539166 0.23975235 0.24560863][0.31480113 0.33971629 0.35467073 0.37234962 0.38876477 0.39510822 0.38257515 0.34777412 0.29716834 0.2504169 0.22456096 0.22018354 0.22645804 0.23425178 0.23972502]]...]
INFO - root - 2017-12-11 08:56:05.885775: step 36810, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:49m:22s remains)
INFO - root - 2017-12-11 08:56:11.201510: step 36820, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 43h:37m:54s remains)
INFO - root - 2017-12-11 08:56:16.619196: step 36830, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 44h:23m:31s remains)
INFO - root - 2017-12-11 08:56:21.904857: step 36840, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.518 sec/batch; 42h:33m:50s remains)
INFO - root - 2017-12-11 08:56:27.255510: step 36850, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:22m:08s remains)
INFO - root - 2017-12-11 08:56:32.596367: step 36860, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.532 sec/batch; 43h:41m:03s remains)
INFO - root - 2017-12-11 08:56:37.840473: step 36870, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:45m:12s remains)
INFO - root - 2017-12-11 08:56:43.293906: step 36880, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 44h:34m:55s remains)
INFO - root - 2017-12-11 08:56:48.362227: step 36890, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:32m:11s remains)
INFO - root - 2017-12-11 08:56:53.748448: step 36900, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:21m:55s remains)
2017-12-11 08:56:54.307117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33367434 0.32335538 0.29277855 0.25526702 0.21733311 0.19117838 0.17425542 0.1742848 0.21403287 0.28565851 0.35400051 0.38877195 0.39464733 0.38095441 0.34452558][0.34474522 0.33555964 0.30103469 0.25936562 0.21991062 0.19902533 0.19257078 0.20715755 0.26624575 0.36011329 0.44620791 0.48513627 0.48270759 0.4547103 0.40066296][0.3429887 0.33648327 0.30315185 0.26434776 0.23163615 0.22158429 0.22871196 0.25638023 0.32449958 0.42428377 0.51177192 0.54566407 0.531257 0.48964107 0.4231132][0.36026138 0.356538 0.32820472 0.29944706 0.28075126 0.28459191 0.3032828 0.33655173 0.39984271 0.48710483 0.5598377 0.58081353 0.55449516 0.50451142 0.43484792][0.39168164 0.38996309 0.36786574 0.35260946 0.35248819 0.37158078 0.39895251 0.43074757 0.47888565 0.54070413 0.58617717 0.5864054 0.54599136 0.48980603 0.42220119][0.42004156 0.42192146 0.40760776 0.40721723 0.42824692 0.46581504 0.50499517 0.53657627 0.56797385 0.59856361 0.60741448 0.57691276 0.5148403 0.44734463 0.37907407][0.44701314 0.45527932 0.44820592 0.45701224 0.49160996 0.54444611 0.59849107 0.63616788 0.65664077 0.65884554 0.63050103 0.5661602 0.478091 0.39336544 0.31897792][0.48754627 0.51059276 0.51212108 0.52134073 0.55305672 0.60497916 0.6624108 0.70108891 0.71313441 0.695963 0.64230311 0.554344 0.44667539 0.34624174 0.26465821][0.53433949 0.57387757 0.58145869 0.58160967 0.59325111 0.622747 0.66319233 0.69075859 0.69639993 0.675356 0.61766738 0.52549195 0.41355297 0.30784261 0.22557972][0.562835 0.61208695 0.62202471 0.60947269 0.59237015 0.58459586 0.59108728 0.5959307 0.59320694 0.57574838 0.52658033 0.44377273 0.34193853 0.24634492 0.1782652][0.54885209 0.59541762 0.60259271 0.578641 0.53613859 0.49264356 0.46337917 0.44252518 0.42816389 0.4112826 0.37079629 0.302706 0.2225904 0.1536928 0.11749519][0.47514936 0.50729859 0.50637734 0.47522348 0.41899341 0.35352755 0.29798853 0.25421402 0.22402485 0.2001106 0.16470233 0.11606514 0.069666512 0.043277826 0.05279883][0.341969 0.35158744 0.33697957 0.30008876 0.24145588 0.17222536 0.10854145 0.054161496 0.013445531 -0.015680054 -0.042107295 -0.0635008 -0.067109711 -0.043635454 0.013011406][0.15778483 0.14122853 0.11091936 0.070000336 0.018751858 -0.036501646 -0.086201675 -0.12990199 -0.16332425 -0.18358587 -0.19098042 -0.17974164 -0.14132829 -0.074131228 0.018142404][-0.025549013 -0.061348561 -0.1001274 -0.13898906 -0.1767534 -0.21048465 -0.23628576 -0.25713363 -0.27046528 -0.27112073 -0.25385842 -0.21097273 -0.13855268 -0.039997533 0.074125633]]...]
INFO - root - 2017-12-11 08:56:59.628986: step 36910, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 46h:13m:45s remains)
INFO - root - 2017-12-11 08:57:05.004070: step 36920, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:43m:16s remains)
INFO - root - 2017-12-11 08:57:10.318800: step 36930, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:22m:24s remains)
INFO - root - 2017-12-11 08:57:15.753291: step 36940, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:06m:36s remains)
INFO - root - 2017-12-11 08:57:21.100709: step 36950, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:39m:08s remains)
INFO - root - 2017-12-11 08:57:26.498939: step 36960, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:03m:23s remains)
INFO - root - 2017-12-11 08:57:31.847843: step 36970, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 43h:29m:54s remains)
INFO - root - 2017-12-11 08:57:37.259962: step 36980, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 46h:20m:55s remains)
INFO - root - 2017-12-11 08:57:42.339640: step 36990, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:21m:08s remains)
INFO - root - 2017-12-11 08:57:47.705074: step 37000, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 43h:02m:53s remains)
2017-12-11 08:57:48.274334: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43446913 0.50917721 0.49860692 0.42396185 0.3284941 0.25514138 0.22733018 0.23531884 0.25871944 0.27478978 0.26657748 0.24553534 0.23647578 0.254763 0.2911998][0.42616248 0.47951207 0.44550607 0.35837764 0.26660976 0.21288526 0.21399251 0.25067219 0.29304805 0.31491211 0.30061054 0.2614941 0.23013152 0.23062633 0.25927639][0.39193857 0.42017388 0.36513186 0.27346209 0.19742426 0.17618039 0.21622071 0.28662208 0.35355005 0.38760689 0.37033004 0.31063178 0.24522883 0.20812763 0.20740338][0.37987989 0.39617398 0.33350965 0.24582219 0.18732387 0.19448623 0.26669526 0.36536703 0.45497671 0.50414819 0.48845202 0.40865698 0.30172029 0.21350653 0.16969512][0.4101519 0.43436858 0.37878418 0.30015779 0.25344786 0.27577129 0.36482427 0.47837296 0.58048624 0.63883841 0.62279409 0.52282357 0.37381575 0.23335417 0.14489175][0.47819382 0.52403021 0.48338088 0.41161281 0.36463588 0.38440084 0.47243744 0.585138 0.68638819 0.74430054 0.72550577 0.61208528 0.43288445 0.25207922 0.12584838][0.55521756 0.62448108 0.59732288 0.52500927 0.46579897 0.46879482 0.54160684 0.64012331 0.72845513 0.77867275 0.7607438 0.65057617 0.46510735 0.26659095 0.11697713][0.60059428 0.6868571 0.66914845 0.59255034 0.51786733 0.49930295 0.54853594 0.62211847 0.68743289 0.72527385 0.71409011 0.6272428 0.46645886 0.28187677 0.13061561][0.5912832 0.68420523 0.673882 0.59787744 0.51524073 0.47963265 0.50380826 0.54745203 0.58547181 0.609944 0.60964078 0.55864352 0.44485587 0.29964039 0.16603829][0.52635342 0.612916 0.60474294 0.53308105 0.45062721 0.40407228 0.40538973 0.42082122 0.43552333 0.4518688 0.46695802 0.45629257 0.39651608 0.29982021 0.19323185][0.40879565 0.47988468 0.47012794 0.40397075 0.327097 0.27731648 0.26426852 0.26050696 0.26020476 0.27277312 0.30095309 0.32364684 0.3109208 0.25895384 0.18094726][0.25094107 0.30500612 0.29591113 0.2395328 0.17468034 0.13126917 0.11570725 0.10539451 0.10033228 0.11211556 0.14590718 0.18634669 0.20447102 0.18678102 0.13553233][0.086281165 0.12427134 0.11865731 0.077077582 0.030210627 0.0008558245 -0.007875137 -0.014323461 -0.016160863 -0.0034054434 0.027836656 0.068121314 0.095562294 0.095307015 0.063673109][-0.046950385 -0.02270175 -0.02504264 -0.052423794 -0.081692994 -0.096156433 -0.095626786 -0.095371068 -0.093188144 -0.082101189 -0.060338605 -0.032587223 -0.011919474 -0.0095540853 -0.030278308][-0.13090736 -0.11651799 -0.11509821 -0.13066237 -0.14692286 -0.1521502 -0.14720485 -0.14396805 -0.14158866 -0.13607015 -0.12668915 -0.11433183 -0.10450916 -0.10386871 -0.11645113]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-nosplit-clip50-initconv1-4-fixqian
INFO - root - 2017-12-11 08:57:53.618692: step 37010, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 43h:04m:30s remains)
INFO - root - 2017-12-11 08:57:58.936213: step 37020, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:12m:47s remains)
INFO - root - 2017-12-11 08:58:04.320789: step 37030, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 43h:21m:33s remains)
INFO - root - 2017-12-11 08:58:09.755798: step 37040, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:00m:16s remains)
INFO - root - 2017-12-11 08:58:15.061057: step 37050, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:42m:22s remains)
INFO - root - 2017-12-11 08:58:20.455032: step 37060, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:11m:02s remains)
INFO - root - 2017-12-11 08:58:25.836555: step 37070, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:20m:15s remains)
INFO - root - 2017-12-11 08:58:31.179722: step 37080, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.528 sec/batch; 43h:21m:32s remains)
INFO - root - 2017-12-11 08:58:36.204158: step 37090, loss = 0.68, batch loss = 0.63 (21.0 examples/sec; 0.382 sec/batch; 31h:18m:26s remains)
INFO - root - 2017-12-11 08:58:41.546443: step 37100, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 42h:58m:22s remains)
2017-12-11 08:58:42.052611: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035805572 0.077349551 0.11948307 0.14875068 0.16085175 0.157736 0.13986644 0.1150502 0.0958819 0.090481944 0.09621501 0.10660915 0.1174287 0.12697668 0.13167746][0.075204864 0.14341398 0.21631499 0.27217788 0.30178127 0.30580583 0.28339568 0.24566072 0.21220993 0.19660652 0.19656418 0.20513527 0.21769077 0.23125935 0.23854111][0.12103465 0.21757996 0.3232061 0.40864119 0.45985582 0.4759542 0.45384732 0.40646076 0.35830036 0.32801843 0.31397635 0.31111827 0.31748635 0.33052057 0.33925793][0.14824888 0.27044564 0.40617579 0.5197652 0.59235835 0.62084693 0.60062146 0.54612052 0.48566213 0.44207382 0.41437033 0.39945853 0.39824727 0.40920848 0.41801918][0.15175174 0.29116103 0.44843838 0.58340895 0.67341042 0.71327496 0.69737625 0.640946 0.57360941 0.52009088 0.4814029 0.45778865 0.45357162 0.46725845 0.4806191][0.14312223 0.29083711 0.45893708 0.60620278 0.7091369 0.76151675 0.75696194 0.70852238 0.64141709 0.57823354 0.52389944 0.48664874 0.47759706 0.49596521 0.51978618][0.12988798 0.27794012 0.44682205 0.59736222 0.70903933 0.7759195 0.79077363 0.76061636 0.70079243 0.62794536 0.55107081 0.4900218 0.46654555 0.4831714 0.51661146][0.10837305 0.24887359 0.40856326 0.55320591 0.66776234 0.74638665 0.77990323 0.76891142 0.71966827 0.64129621 0.54526269 0.46225831 0.42558089 0.44084013 0.48334622][0.079819262 0.20631176 0.34943858 0.48166075 0.59451747 0.68028015 0.7249766 0.72567952 0.68547422 0.60800844 0.50432312 0.41333133 0.37709284 0.40137988 0.45702615][0.047112796 0.1554835 0.27862674 0.3949936 0.50236189 0.59088618 0.63977653 0.64496219 0.61139518 0.54 0.4381935 0.34820402 0.31893528 0.35666364 0.4277271][0.0040519717 0.091943748 0.19369817 0.29237032 0.39105478 0.47874972 0.52897352 0.53605092 0.50830728 0.44624349 0.35103276 0.26186579 0.23233563 0.27407113 0.3530843][-0.052630894 0.0092985919 0.085648105 0.16224906 0.24605699 0.3262727 0.3736794 0.38038418 0.35664138 0.30518976 0.22200593 0.1381077 0.10558945 0.14221543 0.21830446][-0.10794454 -0.073350355 -0.025100613 0.025245711 0.085998848 0.14780661 0.18340482 0.18528968 0.16353913 0.12443601 0.062276565 -0.0010475807 -0.023391284 0.011625775 0.080235064][-0.14435749 -0.13257514 -0.10899251 -0.083283246 -0.048559397 -0.011652743 0.0068513122 0.0024864485 -0.016157337 -0.041305676 -0.077842109 -0.1116699 -0.11462031 -0.077729419 -0.019135933][-0.16343784 -0.16834351 -0.16304152 -0.1559743 -0.14322467 -0.12847942 -0.12349537 -0.13058728 -0.14359589 -0.15620089 -0.17137283 -0.18167138 -0.1715229 -0.13783219 -0.093088314]]...]
INFO - root - 2017-12-11 08:58:47.360144: step 37110, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:19m:32s remains)
INFO - root - 2017-12-11 08:58:52.699165: step 37120, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 43h:56m:47s remains)
INFO - root - 2017-12-11 08:58:57.930482: step 37130, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 42h:49m:26s remains)
INFO - root - 2017-12-11 08:59:03.249865: step 37140, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 42h:20m:20s remains)
INFO - root - 2017-12-11 08:59:08.762426: step 37150, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 46h:00m:22s remains)
INFO - root - 2017-12-11 08:59:13.997152: step 37160, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:21m:56s remains)
INFO - root - 2017-12-11 08:59:19.358169: step 37170, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 43h:54m:41s remains)
INFO - root - 2017-12-11 08:59:24.676059: step 37180, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:30m:22s remains)
INFO - root - 2017-12-11 08:59:30.061363: step 37190, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.525 sec/batch; 43h:03m:06s remains)
INFO - root - 2017-12-11 08:59:35.151553: step 37200, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 43h:41m:36s remains)
2017-12-11 08:59:35.732000: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12288561 0.12490752 0.10772739 0.083427228 0.058627084 0.040423103 0.030391136 0.028116431 0.034599382 0.047262963 0.061337482 0.072841607 0.07888855 0.07285513 0.050497409][0.23563975 0.24291493 0.21837892 0.18185987 0.14430234 0.11772064 0.1043015 0.10141768 0.10960901 0.12494866 0.14134915 0.15352178 0.15826428 0.14553677 0.10898354][0.34055263 0.35536736 0.32678571 0.28171477 0.23598307 0.20631473 0.19373059 0.19156128 0.19885583 0.21133943 0.22357704 0.23036376 0.22914658 0.20735243 0.15698448][0.39703292 0.4203442 0.3948324 0.35147065 0.3090362 0.28622377 0.28086361 0.2811836 0.28394806 0.28592885 0.28530389 0.27957314 0.26782015 0.23591273 0.17508428][0.39163527 0.42166096 0.406681 0.376928 0.350373 0.34342054 0.35028476 0.35424763 0.34977219 0.33574384 0.31622621 0.29353571 0.26883107 0.22771965 0.16144986][0.34630984 0.37971133 0.37800613 0.36727345 0.36220291 0.37417081 0.39437228 0.40208119 0.39030024 0.36050764 0.32240751 0.28307363 0.2453901 0.19651584 0.12894981][0.29464698 0.3279247 0.33629572 0.34059322 0.35265535 0.38005304 0.41186696 0.42328584 0.40668055 0.36690527 0.3172791 0.26644024 0.21770997 0.16187659 0.094877347][0.25752231 0.28677315 0.29838258 0.30771491 0.32601678 0.36120063 0.40128317 0.4169029 0.4003675 0.36002266 0.31011677 0.25591373 0.19943146 0.13685249 0.070086688][0.23380905 0.25660631 0.26429027 0.26828453 0.28132606 0.31562641 0.35972023 0.38046816 0.37064686 0.33996329 0.30041143 0.25075129 0.19097862 0.1236212 0.056872178][0.20601659 0.22086661 0.21981056 0.21228263 0.21333119 0.24047419 0.28405237 0.30983031 0.31190145 0.29827082 0.27516377 0.2352435 0.17753753 0.11046835 0.046193864][0.14954893 0.15697581 0.14705442 0.12912537 0.11965641 0.13889341 0.17926919 0.20930794 0.22507279 0.23080862 0.22523402 0.19731383 0.14689301 0.086117275 0.028011179][0.059136283 0.059600204 0.045215152 0.023793397 0.010946108 0.025420103 0.061425723 0.093408205 0.11956258 0.14027414 0.14873758 0.13308613 0.094550014 0.045667712 -0.0021762582][-0.037685145 -0.042658709 -0.056370955 -0.0746986 -0.085032351 -0.07290595 -0.042399783 -0.01173701 0.018634377 0.046008915 0.061718363 0.05597651 0.031066628 -0.0028294793 -0.03776307][-0.10720138 -0.11526464 -0.12520097 -0.13728318 -0.14297421 -0.13289697 -0.10935756 -0.083536983 -0.055339236 -0.029181082 -0.013017458 -0.012729542 -0.02558245 -0.044915196 -0.0667707][-0.13573091 -0.14441264 -0.14977665 -0.15570121 -0.15737419 -0.14975424 -0.13387145 -0.11533013 -0.093902096 -0.073855057 -0.06133724 -0.058889955 -0.063850336 -0.072445124 -0.083706044]]...]
INFO - root - 2017-12-11 08:59:41.075527: step 37210, loss = 0.66, batch loss = 0.60 (15.3 examples/sec; 0.524 sec/batch; 43h:01m:11s remains)
INFO - root - 2017-12-11 08:59:46.470873: step 37220, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 42h:47m:32s remains)
INFO - root - 2017-12-11 08:59:51.802679: step 37230, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 43h:52m:53s remains)
INFO - root - 2017-12-11 08:59:57.129598: step 37240, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 44h:51m:55s remains)
INFO - root - 2017-12-11 09:00:02.488209: step 37250, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:57m:41s remains)
INFO - root - 2017-12-11 09:00:07.891511: step 37260, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:22m:32s remains)
INFO - root - 2017-12-11 09:00:13.216910: step 37270, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 42h:58m:01s remains)
INFO - root - 2017-12-11 09:00:18.607316: step 37280, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.565 sec/batch; 46h:19m:45s remains)
INFO - root - 2017-12-11 09:00:23.899168: step 37290, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:01m:21s remains)
INFO - root - 2017-12-11 09:00:29.024130: step 37300, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 43h:54m:41s remains)
2017-12-11 09:00:29.562192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088987246 -0.08694461 -0.084553346 -0.084678121 -0.0871184 -0.090122409 -0.09252888 -0.093910493 -0.093790375 -0.092806585 -0.091983281 -0.092281252 -0.094015032 -0.096508287 -0.098868459][-0.05878273 -0.048452765 -0.042036157 -0.043643113 -0.05139963 -0.060921155 -0.068749741 -0.073534958 -0.075247042 -0.076043323 -0.077766955 -0.081407852 -0.087207429 -0.093834 -0.10023418][0.0083236676 0.03347078 0.048344575 0.047537047 0.035385709 0.021304697 0.012945126 0.010880652 0.012364816 0.011675476 0.0043845079 -0.01053098 -0.031175772 -0.052403819 -0.072075985][0.10394617 0.15029269 0.1787049 0.18328969 0.17280227 0.16408619 0.16916533 0.18344687 0.19634071 0.19733472 0.17802516 0.13716526 0.082852215 0.030433839 -0.013564648][0.20687039 0.27659628 0.32206887 0.33930811 0.34215644 0.35397792 0.38862586 0.432264 0.46164468 0.46102566 0.42028913 0.34025756 0.23819163 0.14250737 0.066797428][0.27730238 0.3663924 0.43046194 0.47015646 0.50203 0.55073613 0.62746096 0.70766521 0.75465924 0.74917126 0.68439227 0.56601292 0.41783449 0.27655634 0.16320838][0.27852163 0.37189496 0.44946152 0.51718408 0.59120452 0.69032186 0.81703883 0.93675077 1.0026119 0.99430186 0.91118973 0.76557428 0.5835225 0.40305752 0.25166732][0.22439033 0.30701378 0.38828206 0.47881615 0.59213454 0.73714989 0.90457457 1.0530162 1.1297046 1.11466 1.0151548 0.8523472 0.65427768 0.45535079 0.28509426][0.14776422 0.21104492 0.2842851 0.38045228 0.5099892 0.67234927 0.84894556 0.99993646 1.0733805 1.0492747 0.93977118 0.77376753 0.58328605 0.39775446 0.24192806][0.059059709 0.10016764 0.15716749 0.24207318 0.36119595 0.50693244 0.65774983 0.7854467 0.84814858 0.82482374 0.72517931 0.57762128 0.41592494 0.26645458 0.14737305][-0.028837366 -0.0086718071 0.030611681 0.096986547 0.19160871 0.30188283 0.40874824 0.49941114 0.54709381 0.531422 0.45535752 0.34102395 0.21966666 0.11497876 0.038828839][-0.089053512 -0.08744847 -0.065744326 -0.019766616 0.047654696 0.12110755 0.1849324 0.23657924 0.26267084 0.24802808 0.19213776 0.11159385 0.032450732 -0.026943337 -0.061907414][-0.10771687 -0.11909626 -0.11399281 -0.089445837 -0.047653008 -0.003541817 0.029315244 0.050397996 0.054462928 0.034155305 -0.0090230489 -0.061377719 -0.1034421 -0.12511091 -0.12857996][-0.10572357 -0.12171577 -0.12632214 -0.11729367 -0.093945004 -0.068157472 -0.051001441 -0.045117624 -0.052781437 -0.075930685 -0.11009597 -0.14213324 -0.1588531 -0.15726359 -0.14395998][-0.10408758 -0.11547247 -0.11744327 -0.10926325 -0.089087404 -0.066960938 -0.052189458 -0.04925403 -0.05948627 -0.083335146 -0.11536556 -0.14082159 -0.14866541 -0.13892584 -0.12090931]]...]
INFO - root - 2017-12-11 09:00:34.831186: step 37310, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.521 sec/batch; 42h:41m:54s remains)
INFO - root - 2017-12-11 09:00:40.212604: step 37320, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 43h:54m:02s remains)
INFO - root - 2017-12-11 09:00:45.567776: step 37330, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.511 sec/batch; 41h:52m:09s remains)
INFO - root - 2017-12-11 09:00:50.792691: step 37340, loss = 0.71, batch loss = 0.65 (15.7 examples/sec; 0.509 sec/batch; 41h:43m:54s remains)
INFO - root - 2017-12-11 09:00:56.201346: step 37350, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 43h:33m:00s remains)
INFO - root - 2017-12-11 09:01:01.659650: step 37360, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 43h:49m:09s remains)
INFO - root - 2017-12-11 09:01:07.050552: step 37370, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 43h:49m:02s remains)
INFO - root - 2017-12-11 09:01:12.382833: step 37380, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 42h:48m:14s remains)
INFO - root - 2017-12-11 09:01:17.711665: step 37390, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:25m:02s remains)
INFO - root - 2017-12-11 09:01:22.753756: step 37400, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 46h:39m:11s remains)
2017-12-11 09:01:23.310766: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13754484 0.10688193 0.075754188 0.057071839 0.054244831 0.058708061 0.059688032 0.058908053 0.072931953 0.10832035 0.16403705 0.23302989 0.31132674 0.38529506 0.43799731][0.094793558 0.0664655 0.037903264 0.01972126 0.01418895 0.016192392 0.01973916 0.029149706 0.058035154 0.10957532 0.18021533 0.25807211 0.3355132 0.40046674 0.43908411][0.071299031 0.052127093 0.033214364 0.021540426 0.016893499 0.018236611 0.024681116 0.041859087 0.077702276 0.13188963 0.2001792 0.26792023 0.32666314 0.36975026 0.39184263][0.078032248 0.074035279 0.073124237 0.07757055 0.083934069 0.093270347 0.10685192 0.12991388 0.16404453 0.20681305 0.25498816 0.29495946 0.32226387 0.33652821 0.34274098][0.11091248 0.12640631 0.15123667 0.18253785 0.21317191 0.24198726 0.26750672 0.29490736 0.32051551 0.3411648 0.35545704 0.35629588 0.34724924 0.33126438 0.32200795][0.16283803 0.20104393 0.25621733 0.32069546 0.38407639 0.43934461 0.47814649 0.50681794 0.51814854 0.51055884 0.48545682 0.44593918 0.40369239 0.36045009 0.33602744][0.22243322 0.28198814 0.363428 0.45765713 0.55234313 0.63279086 0.68187487 0.70729959 0.70094728 0.66491318 0.60479677 0.53320676 0.46797609 0.407552 0.37422711][0.273368 0.34320647 0.43418142 0.54243267 0.65509826 0.75006658 0.80336481 0.82378119 0.803127 0.74636686 0.6644451 0.57641488 0.50306827 0.44003916 0.40817711][0.30059743 0.36456978 0.44388837 0.5452354 0.65516806 0.74721831 0.79624635 0.81163913 0.78425246 0.71903843 0.63128477 0.54200029 0.47194967 0.4163579 0.39195034][0.29915512 0.34188962 0.39328912 0.46968952 0.55643439 0.6272378 0.66188383 0.67007732 0.6411292 0.57798809 0.4982295 0.42043719 0.36151859 0.31717312 0.2997967][0.26371217 0.27767816 0.29415193 0.33552864 0.38717806 0.42764309 0.4449966 0.44666702 0.42086533 0.36816356 0.30635309 0.24879621 0.20534368 0.17246205 0.1595072][0.20734541 0.19242206 0.17644072 0.18312834 0.20012207 0.21314517 0.21796663 0.21787083 0.20067213 0.16454396 0.12507729 0.0903789 0.063131735 0.040166039 0.02823253][0.14488584 0.10924058 0.072091021 0.053885181 0.04624271 0.042184122 0.042024765 0.044930242 0.038903 0.020628618 0.0021683341 -0.012267931 -0.025530485 -0.04144565 -0.055331685][0.085170306 0.0412362 -0.00273419 -0.032134768 -0.050813958 -0.060160983 -0.058820736 -0.051107243 -0.047482833 -0.050918169 -0.053062756 -0.05290613 -0.05659283 -0.06887766 -0.086230144][0.036362186 -0.0061577111 -0.047513764 -0.078301869 -0.098086171 -0.10489769 -0.098673582 -0.0860309 -0.075033285 -0.067387395 -0.058399357 -0.049648814 -0.048103046 -0.058669839 -0.078245357]]...]
INFO - root - 2017-12-11 09:01:28.716592: step 37410, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 46h:43m:24s remains)
INFO - root - 2017-12-11 09:01:34.132736: step 37420, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 45h:45m:41s remains)
INFO - root - 2017-12-11 09:01:39.508477: step 37430, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 43h:12m:52s remains)
INFO - root - 2017-12-11 09:01:44.870277: step 37440, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 43h:01m:48s remains)
INFO - root - 2017-12-11 09:01:50.202203: step 37450, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 43h:05m:45s remains)
INFO - root - 2017-12-11 09:01:55.530461: step 37460, loss = 0.67, batch loss = 0.61 (15.2 examples/sec; 0.526 sec/batch; 43h:08m:01s remains)
INFO - root - 2017-12-11 09:02:00.838431: step 37470, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:45m:07s remains)
INFO - root - 2017-12-11 09:02:06.203544: step 37480, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 43h:36m:59s remains)
INFO - root - 2017-12-11 09:02:11.612740: step 37490, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 45h:45m:31s remains)
INFO - root - 2017-12-11 09:02:16.885610: step 37500, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:26m:34s remains)
2017-12-11 09:02:17.367974: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39132002 0.41367671 0.41945767 0.41346669 0.40023068 0.39303386 0.39611843 0.40081218 0.39589119 0.38043204 0.35648057 0.32252687 0.28839263 0.26862088 0.26207572][0.38957685 0.4046486 0.40369728 0.39314997 0.37909752 0.3795577 0.39869055 0.42162898 0.43032551 0.4179064 0.3848494 0.33460432 0.28108305 0.24409929 0.22886133][0.3349759 0.3460187 0.34022465 0.32486728 0.31121889 0.32253671 0.36113796 0.40532351 0.42859846 0.417848 0.37529162 0.31060416 0.24321312 0.19612511 0.17633379][0.24474616 0.25229853 0.24680828 0.23436016 0.22984074 0.25801337 0.31755602 0.3807205 0.41415665 0.40117988 0.34773311 0.27163562 0.19703421 0.14738478 0.12861103][0.13901412 0.1418723 0.13881028 0.13826975 0.15468821 0.20692125 0.286294 0.36105511 0.39562386 0.37421474 0.30757263 0.22090735 0.14188468 0.092716679 0.077173352][0.05932207 0.053873215 0.051596027 0.065018728 0.1049336 0.17860837 0.26828986 0.34154481 0.36641967 0.33248812 0.25470954 0.16161245 0.081164792 0.032659668 0.018152146][0.029465867 0.015225678 0.011836171 0.03418538 0.088529818 0.17064115 0.25635788 0.3157112 0.32356828 0.27629927 0.19216067 0.099673666 0.023673723 -0.02112592 -0.034183115][0.043784879 0.024391819 0.018476043 0.04239618 0.0980831 0.17379928 0.24403916 0.28376722 0.27447888 0.21760328 0.1336242 0.049451403 -0.014752269 -0.049520358 -0.056274612][0.082732372 0.059271928 0.048268039 0.066137634 0.11129732 0.1693667 0.21820021 0.23915669 0.21903935 0.16130137 0.085702114 0.016276468 -0.030730486 -0.049586244 -0.044109043][0.125364 0.099987969 0.083438635 0.090401813 0.11706425 0.15039039 0.17490265 0.17920774 0.15492533 0.10537211 0.046563584 -0.0023646681 -0.02862351 -0.028871648 -0.0090637058][0.1480827 0.1260477 0.10811756 0.10402956 0.10912401 0.11508082 0.1150456 0.10500734 0.080924943 0.045261398 0.0089269076 -0.015137198 -0.018236302 0.00037060931 0.032615662][0.13917741 0.12752183 0.11497171 0.1035517 0.088955164 0.0697515 0.048258986 0.026991628 0.005531216 -0.014675764 -0.027802195 -0.027355539 -0.0097995689 0.022666357 0.060639869][0.12105644 0.12225892 0.1154943 0.095879026 0.062001154 0.021016274 -0.016042585 -0.041726816 -0.054878969 -0.056745067 -0.04767729 -0.028017126 0.00023189546 0.033004329 0.063187033][0.1220997 0.1362505 0.13166612 0.099710166 0.044977672 -0.015761649 -0.063964032 -0.089003876 -0.089957058 -0.072517291 -0.044383828 -0.012752354 0.01569715 0.036429465 0.047912944][0.1580036 0.18625265 0.18080559 0.13359757 0.056727197 -0.023586283 -0.082157932 -0.10615628 -0.09657196 -0.064053655 -0.022834733 0.014389178 0.037610598 0.04218198 0.032025021]]...]
INFO - root - 2017-12-11 09:02:22.581247: step 37510, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 45h:02m:20s remains)
INFO - root - 2017-12-11 09:02:27.924700: step 37520, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.548 sec/batch; 44h:53m:00s remains)
INFO - root - 2017-12-11 09:02:33.291770: step 37530, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 43h:52m:38s remains)
INFO - root - 2017-12-11 09:02:38.624320: step 37540, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 43h:21m:10s remains)
INFO - root - 2017-12-11 09:02:44.003756: step 37550, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 43h:43m:05s remains)
INFO - root - 2017-12-11 09:02:49.329736: step 37560, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.522 sec/batch; 42h:47m:10s remains)
INFO - root - 2017-12-11 09:02:54.661971: step 37570, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 45h:17m:25s remains)
INFO - root - 2017-12-11 09:03:00.046641: step 37580, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 44h:57m:19s remains)
INFO - root - 2017-12-11 09:03:05.397132: step 37590, loss = 0.68, batch loss = 0.62 (15.3 examples/sec; 0.523 sec/batch; 42h:51m:15s remains)
INFO - root - 2017-12-11 09:03:10.702393: step 37600, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:27m:51s remains)
2017-12-11 09:03:11.250911: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17871483 0.1598607 0.16122319 0.1911993 0.24238445 0.28576893 0.29553807 0.2638675 0.19836427 0.11439755 0.033858392 -0.027832707 -0.064561419 -0.079027772 -0.078648932][0.20382679 0.19305745 0.20465863 0.24687816 0.30841362 0.35981122 0.37239206 0.33788869 0.26462844 0.16982603 0.076072529 0.00035137561 -0.048766248 -0.072597384 -0.078212179][0.19185986 0.19421895 0.22103149 0.27873084 0.35240927 0.41538247 0.43909827 0.4099808 0.33589894 0.23466362 0.12896922 0.037606359 -0.026845323 -0.061501957 -0.073639005][0.15132494 0.16670939 0.20968944 0.28576723 0.37728682 0.45927712 0.50152552 0.48298463 0.41044357 0.30219734 0.18211384 0.072686709 -0.008152863 -0.053302005 -0.070890494][0.099618509 0.1263933 0.1814265 0.27152523 0.37946731 0.48116267 0.542955 0.53617591 0.46617123 0.35167879 0.21827763 0.093123212 -0.00045008853 -0.052322879 -0.0720811][0.070811264 0.10265636 0.15915185 0.24950539 0.36072317 0.47048318 0.54235882 0.54319096 0.47669792 0.36089212 0.22182466 0.090100959 -0.0072338185 -0.05930433 -0.077077292][0.092387185 0.12411869 0.170672 0.24441221 0.33952609 0.43657988 0.49998379 0.4979412 0.43351996 0.3224172 0.18871501 0.063329674 -0.026365811 -0.071416274 -0.083661504][0.15350083 0.18781531 0.22445506 0.27684814 0.34625775 0.41548073 0.45372352 0.43590647 0.36714363 0.26092365 0.13825472 0.027337113 -0.0477533 -0.082172208 -0.088078447][0.23157918 0.27454254 0.30986512 0.34929061 0.39741537 0.43701595 0.44343245 0.40091041 0.31875783 0.21080923 0.096312471 -0.00061764527 -0.061643612 -0.086847588 -0.088262454][0.30793387 0.36059615 0.40127942 0.43831113 0.47492355 0.49246982 0.47073787 0.4028 0.3024841 0.18628535 0.073832177 -0.014532456 -0.066454224 -0.086185418 -0.085533872][0.36738625 0.42663041 0.47403657 0.51460749 0.54675555 0.55080932 0.50927562 0.42144209 0.30437043 0.17788124 0.063702367 -0.02056711 -0.067601144 -0.08456549 -0.083084062][0.39217696 0.45158175 0.50187576 0.54638904 0.57824928 0.57791203 0.52914006 0.43343928 0.30850258 0.17567751 0.059412129 -0.023493424 -0.068658337 -0.084524512 -0.082638569][0.37848872 0.42709357 0.46825525 0.50775635 0.53732747 0.53989065 0.4991976 0.41378516 0.29794598 0.17057772 0.05729311 -0.024023294 -0.068994515 -0.085122794 -0.083494864][0.34130666 0.36799133 0.38544381 0.40616173 0.42675498 0.43539849 0.41530794 0.35700592 0.26760378 0.15933238 0.056274798 -0.021480126 -0.06678056 -0.084247649 -0.083929345][0.30674025 0.30940571 0.29807094 0.29493269 0.30524284 0.32192373 0.32685092 0.30104107 0.2436946 0.15844777 0.06602098 -0.010444306 -0.058859043 -0.079886511 -0.082626276]]...]
INFO - root - 2017-12-11 09:03:16.437539: step 37610, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 44h:23m:10s remains)
INFO - root - 2017-12-11 09:03:21.768209: step 37620, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 44h:48m:15s remains)
INFO - root - 2017-12-11 09:03:27.085402: step 37630, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.522 sec/batch; 42h:46m:22s remains)
INFO - root - 2017-12-11 09:03:32.415348: step 37640, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 45h:44m:11s remains)
INFO - root - 2017-12-11 09:03:37.703642: step 37650, loss = 0.71, batch loss = 0.65 (15.6 examples/sec; 0.514 sec/batch; 42h:05m:53s remains)
