INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "298"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-16 07:48:17.345878: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:48:17.345917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:48:17.345924: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:48:17.345928: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:48:17.345932: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:48:18.373582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 07:48:18.373619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 07:48:18.373626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 07:48:18.373634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 07:48:21.870609: step 0, loss = 1.68, batch loss = 1.39 (3.1 examples/sec; 2.606 sec/batch; 240h:42m:24s remains)
2017-12-16 07:48:22.765938: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.2066684 3.3968148 3.5190058 3.6126242 3.7268281 3.868408 4.00784 4.1442695 4.2964911 4.3870854 4.345964 4.1893082 4.0461111 3.9025779 3.80532][3.3601098 3.5395224 3.6535602 3.7577152 3.911788 4.0926652 4.2401004 4.3733282 4.5193477 4.59504 4.5520463 4.4269171 4.3622308 4.3525577 4.4002123][3.6332023 3.7739673 3.8627558 3.9558647 4.1122088 4.3007832 4.46388 4.6090975 4.755713 4.8436093 4.8480988 4.8076944 4.8518085 4.9648418 5.1289325][4.0281067 4.1036329 4.1533303 4.2180567 4.350172 4.5381961 4.7356129 4.9145117 5.0774765 5.1904011 5.2447348 5.2760711 5.3893962 5.5712533 5.8250833][4.5082459 4.5314503 4.5497084 4.585978 4.6866779 4.8683057 5.0912304 5.3119364 5.5059443 5.6430545 5.7450576 5.844933 6.0192204 6.2466793 6.5580192][5.0696135 5.045012 5.0142965 5.0294571 5.1266785 5.3303084 5.6222105 5.9323688 6.1642156 6.2921453 6.3886309 6.4965763 6.6592174 6.8683891 7.166327][5.7897625 5.6828437 5.54902 5.4954453 5.571795 5.8052912 6.1873312 6.6110497 6.8915224 7.0051064 7.0467162 7.0962806 7.1919861 7.3219414 7.5085998][6.604218 6.419075 6.1891465 6.0365343 6.0547609 6.2491326 6.5819793 6.9560719 7.2464123 7.403583 7.4644575 7.4955044 7.5328207 7.5546532 7.5698094][7.3986607 7.1548848 6.875092 6.6813455 6.6623211 6.8183947 7.0964503 7.3985643 7.6518755 7.8331475 7.9054527 7.8915558 7.8213587 7.6825314 7.5007482][8.0996523 7.8825336 7.6421666 7.4764218 7.4578128 7.5792117 7.8140044 8.0475521 8.2285786 8.3413467 8.3478661 8.211627 7.963007 7.6246567 7.2477474][8.6312256 8.4768972 8.3076782 8.1992216 8.2050018 8.3157139 8.532217 8.7242327 8.8295116 8.8241453 8.6738663 8.3382206 7.8584061 7.3210821 6.8218112][9.0010071 8.8841658 8.7412014 8.6339321 8.6078663 8.6737375 8.8355837 8.9879866 9.0478544 8.9640579 8.6792431 8.1942816 7.5811915 6.948987 6.4118638][9.0897522 9.0318136 8.928133 8.8336029 8.7964144 8.8208113 8.9137344 8.9962111 8.9822712 8.8099585 8.4389515 7.9049287 7.2846007 6.6570654 6.129704][8.8622866 8.865407 8.8198147 8.76207 8.7343636 8.7447062 8.7942324 8.8185081 8.7425785 8.5018272 8.1167173 7.6320934 7.1003594 6.5649166 6.118578][8.4636927 8.5188169 8.5272388 8.5090752 8.4900827 8.462492 8.4499435 8.4136515 8.2975769 8.04751 7.7200685 7.356133 6.9683847 6.5976262 6.302598]]...]
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
[u'siamese_fc/conv1/weights:0',
 u'siamese_fc/conv1/BatchNorm/beta:0',
 u'siamese_fc/conv1/BatchNorm/gamma:0',
 u'siamese_fc/conv1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b1/weights:0',
 u'siamese_fc/conv2/b1/BatchNorm/beta:0',
 u'siamese_fc/conv2/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b2/weights:0',
 u'siamese_fc/conv2/b2/BatchNorm/beta:0',
 u'siamese_fc/conv2/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv3/weights:0',
 u'siamese_fc/conv3/BatchNorm/beta:0',
 u'siamese_fc/conv3/BatchNorm/gamma:0',
 u'siamese_fc/conv3/BatchNorm/moving_mean:0',
 u'siamese_fc/conv3/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b1/weights:0',
 u'siamese_fc/conv4/b1/BatchNorm/beta:0',
 u'siamese_fc/conv4/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b2/weights:0',
 u'siamese_fc/conv4/b2/BatchNorm/beta:0',
 u'siamese_fc/conv4/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv5/b1/weights:0',
 u'siamese_fc/conv5/b1/biases:0',
 u'siamese_fc/conv5/b2/weights:0',
 u'siamese_fc/conv5/b2/biases:0',
 u'detection/biases:0',
 u'global_step:0',
 u'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b1/biases/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b2/biases/Momentum:0',
 u'OptimizeLoss/detection/biases/Momentum:0']
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:48:25.449302: step 10, loss = 1.07, batch loss = 0.78 (48.4 examples/sec; 0.165 sec/batch; 15h:15m:03s remains)
INFO - root - 2017-12-16 07:48:27.089618: step 20, loss = 0.89, batch loss = 0.61 (47.4 examples/sec; 0.169 sec/batch; 15h:35m:08s remains)
INFO - root - 2017-12-16 07:48:28.739608: step 30, loss = 0.86, batch loss = 0.57 (48.3 examples/sec; 0.166 sec/batch; 15h:17m:09s remains)
INFO - root - 2017-12-16 07:48:30.365611: step 40, loss = 0.91, batch loss = 0.62 (50.2 examples/sec; 0.159 sec/batch; 14h:42m:54s remains)
INFO - root - 2017-12-16 07:48:32.027215: step 50, loss = 0.89, batch loss = 0.60 (48.7 examples/sec; 0.164 sec/batch; 15h:10m:25s remains)
INFO - root - 2017-12-16 07:48:33.709022: step 60, loss = 0.87, batch loss = 0.58 (48.6 examples/sec; 0.165 sec/batch; 15h:12m:36s remains)
INFO - root - 2017-12-16 07:48:35.339405: step 70, loss = 0.88, batch loss = 0.60 (50.7 examples/sec; 0.158 sec/batch; 14h:35m:03s remains)
INFO - root - 2017-12-16 07:48:37.000775: step 80, loss = 0.80, batch loss = 0.51 (47.5 examples/sec; 0.168 sec/batch; 15h:33m:22s remains)
INFO - root - 2017-12-16 07:48:38.673939: step 90, loss = 0.81, batch loss = 0.52 (49.2 examples/sec; 0.163 sec/batch; 15h:01m:25s remains)
INFO - root - 2017-12-16 07:48:40.317214: step 100, loss = 0.84, batch loss = 0.56 (48.7 examples/sec; 0.164 sec/batch; 15h:10m:25s remains)
2017-12-16 07:48:40.812930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.73596537 -0.84737325 -0.81282008 -0.76318443 -0.85730278 -1.0231386 -1.102687 -1.1389639 -1.1787534 -1.1126951 -0.87749231 -0.63245952 -0.43261585 -0.41769427 -0.48465821][0.18490675 0.015726492 -0.0013903975 -0.076918632 -0.19785067 -0.35101455 -0.43469539 -0.53753066 -0.6644727 -0.66155446 -0.44928083 -0.16273351 -0.041223131 -0.056585394 -0.1379272][0.32452896 0.29187036 0.33991054 0.27864215 0.20611495 0.21070272 0.3022995 0.25588137 0.17486537 0.12282577 0.24382889 0.36928311 0.30290103 0.27097833 0.20710748][-0.13205346 -0.091677368 -0.055734016 -0.057463177 -0.11239651 -0.035220392 0.079319105 0.15167591 0.20296699 0.301345 0.48810115 0.63188434 0.65834939 0.63206959 0.4902986][-0.14980713 -0.12791122 -0.094320685 -0.012923867 0.0077558309 -0.013450034 0.087730929 0.25013006 0.313387 0.32999638 0.39158353 0.5092597 0.73427415 0.81601191 0.61938775][-0.020929895 0.087512746 0.23012367 0.3848522 0.537691 0.48120198 0.51460779 0.72621453 0.796229 0.75005054 0.65207982 0.63959 0.78375161 0.83510196 0.6494137][0.0070867091 0.16001785 0.40860251 0.61227381 0.97665918 0.98842895 0.92681181 1.0681945 1.1843996 1.2885123 1.2300597 1.119647 1.1860261 1.2109559 0.97761226][0.59214568 0.70551729 0.87801492 1.0485276 1.3144931 1.2814647 1.1482873 1.1518145 1.2853596 1.4856495 1.5311494 1.4357417 1.5394816 1.6682006 1.5127112][1.4550879 1.568395 1.7022523 1.7397391 1.8810937 1.7528208 1.5137973 1.3884699 1.407339 1.3872349 1.1992059 0.97471058 0.97547972 1.1568683 1.1580766][1.1291761 1.2743297 1.4165283 1.478196 1.6401436 1.5365527 1.2218019 1.0915751 1.1279631 1.0878217 0.89925778 0.7123214 0.67828465 0.7723918 0.84542096][0.71496511 0.77052152 0.80597138 0.81134033 0.95255554 0.99305391 0.79533553 0.73365068 0.77427828 0.78763449 0.77342975 0.73968923 0.68775427 0.71266329 0.80811393][0.3864117 0.47003576 0.53306866 0.50028849 0.51568627 0.55562425 0.53154957 0.53354907 0.55272019 0.56357813 0.59177005 0.56528795 0.53643978 0.54549253 0.61239374][0.30400214 0.38840851 0.50517535 0.50080657 0.46023974 0.42301998 0.39630941 0.39289281 0.37491879 0.37951508 0.37313494 0.34216025 0.31879455 0.3262791 0.39168426][0.19758195 0.19429243 0.22391561 0.21618176 0.19746822 0.14239353 0.088839144 0.10305278 0.10063577 0.11913529 0.10793675 0.086434469 0.076958075 0.077297807 0.12295306][-0.056693703 -0.10419723 -0.12337271 -0.15014535 -0.14704177 -0.1846585 -0.21245508 -0.21344861 -0.21594048 -0.20268251 -0.20720252 -0.22497913 -0.23288445 -0.23065564 -0.20068063]]...]
INFO - root - 2017-12-16 07:48:42.467936: step 110, loss = 0.85, batch loss = 0.57 (48.1 examples/sec; 0.166 sec/batch; 15h:21m:49s remains)
INFO - root - 2017-12-16 07:48:44.115181: step 120, loss = 0.82, batch loss = 0.53 (48.6 examples/sec; 0.165 sec/batch; 15h:12m:45s remains)
INFO - root - 2017-12-16 07:48:45.776236: step 130, loss = 0.83, batch loss = 0.54 (45.2 examples/sec; 0.177 sec/batch; 16h:19m:26s remains)
INFO - root - 2017-12-16 07:48:47.415700: step 140, loss = 0.80, batch loss = 0.51 (49.5 examples/sec; 0.162 sec/batch; 14h:54m:44s remains)
INFO - root - 2017-12-16 07:48:49.056643: step 150, loss = 0.82, batch loss = 0.54 (48.2 examples/sec; 0.166 sec/batch; 15h:18m:36s remains)
INFO - root - 2017-12-16 07:48:50.683888: step 160, loss = 0.84, batch loss = 0.55 (49.8 examples/sec; 0.161 sec/batch; 14h:49m:21s remains)
INFO - root - 2017-12-16 07:48:52.320709: step 170, loss = 0.85, batch loss = 0.56 (49.3 examples/sec; 0.162 sec/batch; 14h:58m:58s remains)
INFO - root - 2017-12-16 07:48:54.009375: step 180, loss = 0.80, batch loss = 0.52 (47.9 examples/sec; 0.167 sec/batch; 15h:25m:39s remains)
INFO - root - 2017-12-16 07:48:55.680811: step 190, loss = 0.82, batch loss = 0.54 (47.4 examples/sec; 0.169 sec/batch; 15h:35m:24s remains)
INFO - root - 2017-12-16 07:48:57.345642: step 200, loss = 0.83, batch loss = 0.54 (48.8 examples/sec; 0.164 sec/batch; 15h:07m:46s remains)
2017-12-16 07:48:57.850961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.47735709 -0.50173342 -0.54882205 -0.58307779 -0.57743138 -0.52672923 -0.42217803 -0.21757057 0.0025147647 0.059724182 0.02197051 -0.024997324 -0.067196861 -0.11140718 -0.1904947][-0.41334605 -0.46810585 -0.53316295 -0.51745224 -0.48406753 -0.42596573 -0.30329484 -0.10291632 0.10891479 0.18049029 0.16111186 0.0842554 -0.0098426342 -0.10491574 -0.19380131][-0.40897888 -0.48919865 -0.53868324 -0.48271784 -0.42561918 -0.408208 -0.30899969 -0.15317178 0.037675321 0.13126227 0.10735622 0.037314266 -0.071341991 -0.17800671 -0.24884549][-0.41665047 -0.48850185 -0.52508247 -0.44123572 -0.36890933 -0.37695378 -0.314408 -0.19334753 -0.031498417 0.020607948 -0.037196681 -0.10087998 -0.17768346 -0.23636591 -0.27554736][-0.372581 -0.42105836 -0.39194727 -0.27703917 -0.20465785 -0.21262376 -0.17354572 -0.12467048 -0.048227012 -0.069252506 -0.18035203 -0.25107679 -0.28904453 -0.32404941 -0.33932629][-0.23928045 -0.23117487 -0.1458797 0.002949968 0.12495586 0.14072904 0.12377235 0.097127736 0.096399784 -0.022105262 -0.21490034 -0.32432091 -0.37244487 -0.39570552 -0.39399159][-0.043779686 0.042284936 0.18796715 0.37792495 0.557351 0.65736008 0.62707424 0.5187521 0.37665579 0.14937323 -0.114011 -0.25631264 -0.32400686 -0.37771282 -0.385894][0.21762851 0.28939584 0.43560567 0.64208651 0.84812593 1.0936152 1.2066721 1.1111164 0.92933738 0.52929389 0.15521193 -0.0691665 -0.21463694 -0.31459761 -0.34878644][0.4055604 0.4493334 0.55923009 0.71549392 0.88042784 1.1128184 1.2920232 1.3320574 1.2269293 0.796065 0.38243631 0.089568913 -0.13409242 -0.26677185 -0.32685891][0.38608083 0.45038745 0.5111649 0.5515908 0.57872355 0.62822437 0.71276617 0.85218406 0.887532 0.63022196 0.30930457 0.036962271 -0.18978634 -0.30659902 -0.34649962][-0.016181231 0.063138187 0.15947175 0.25802013 0.30974975 0.33795205 0.36080357 0.40019163 0.40673116 0.24133009 0.00060904026 -0.18118213 -0.32530767 -0.38689238 -0.39346191][-0.35849437 -0.29537249 -0.17983107 -0.031621203 0.13102263 0.2023485 0.24188858 0.26340693 0.24468458 0.059282154 -0.16469088 -0.32329506 -0.42737338 -0.46356788 -0.43695846][-0.38331166 -0.40240398 -0.34943712 -0.25270924 -0.080946 0.017114669 0.1189456 0.256237 0.32874331 0.18018928 -0.0681508 -0.2560418 -0.39267296 -0.46074152 -0.44457716][-0.1255596 -0.21774387 -0.28821087 -0.31189346 -0.23127767 -0.1255199 0.040886521 0.31660733 0.5043087 0.36935911 0.10607859 -0.11751359 -0.2986733 -0.40157092 -0.41241491][0.16264513 0.00074619055 -0.169912 -0.25876468 -0.23830293 -0.1540193 0.048909485 0.43781492 0.66874886 0.50582969 0.22969291 -0.014810801 -0.23221841 -0.35674971 -0.38455623]]...]
INFO - root - 2017-12-16 07:48:59.477897: step 210, loss = 0.78, batch loss = 0.49 (49.5 examples/sec; 0.162 sec/batch; 14h:54m:34s remains)
INFO - root - 2017-12-16 07:49:01.129555: step 220, loss = 0.83, batch loss = 0.54 (48.8 examples/sec; 0.164 sec/batch; 15h:08m:11s remains)
INFO - root - 2017-12-16 07:49:02.785261: step 230, loss = 0.77, batch loss = 0.48 (48.8 examples/sec; 0.164 sec/batch; 15h:08m:07s remains)
INFO - root - 2017-12-16 07:49:04.442859: step 240, loss = 0.75, batch loss = 0.47 (48.0 examples/sec; 0.167 sec/batch; 15h:22m:07s remains)
INFO - root - 2017-12-16 07:49:06.138566: step 250, loss = 0.80, batch loss = 0.52 (47.5 examples/sec; 0.169 sec/batch; 15h:33m:32s remains)
INFO - root - 2017-12-16 07:49:07.857403: step 260, loss = 0.76, batch loss = 0.48 (46.4 examples/sec; 0.172 sec/batch; 15h:54m:38s remains)
INFO - root - 2017-12-16 07:49:09.541404: step 270, loss = 0.84, batch loss = 0.56 (44.5 examples/sec; 0.180 sec/batch; 16h:35m:47s remains)
INFO - root - 2017-12-16 07:49:11.217276: step 280, loss = 0.78, batch loss = 0.49 (48.8 examples/sec; 0.164 sec/batch; 15h:07m:40s remains)
INFO - root - 2017-12-16 07:49:12.912513: step 290, loss = 0.79, batch loss = 0.50 (48.4 examples/sec; 0.165 sec/batch; 15h:15m:20s remains)
INFO - root - 2017-12-16 07:49:14.565468: step 300, loss = 0.75, batch loss = 0.47 (48.2 examples/sec; 0.166 sec/batch; 15h:18m:31s remains)
2017-12-16 07:49:15.072305: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36080462 0.29544997 0.20793438 0.13306406 0.090614825 0.062588394 0.0734058 0.12222132 0.20454776 0.27113771 0.3223604 0.3331089 0.30296403 0.24078798 0.20681214][-0.15581523 -0.16869624 -0.24048024 -0.30854091 -0.3928014 -0.48812348 -0.50602591 -0.41452226 -0.28440848 -0.16602248 -0.068646818 -0.0034582317 -0.0092740357 -0.061088979 -0.061639458][-0.61352581 -0.53542006 -0.51817942 -0.56891412 -0.72083628 -0.89423019 -0.97083569 -0.89553344 -0.76071286 -0.62824357 -0.50064951 -0.43675134 -0.4522258 -0.48901257 -0.46028972][-0.84865987 -0.68372613 -0.58320534 -0.6092577 -0.791099 -1.0423547 -1.1588947 -1.1313546 -1.0576411 -0.95905977 -0.85708344 -0.834439 -0.88494325 -0.92277724 -0.86690336][-0.82510459 -0.55607593 -0.38205829 -0.36975729 -0.57382226 -0.83485973 -0.94689178 -0.94755238 -0.93770659 -0.94329053 -0.94683939 -1.0100818 -1.096158 -1.1211526 -1.0379844][-0.46791118 -0.13837343 0.047728002 0.076778591 -0.059411556 -0.23768449 -0.29984504 -0.30630505 -0.37961957 -0.508028 -0.68132043 -0.8358981 -0.95951867 -0.98895049 -0.91198194][-0.16157217 0.17363811 0.35969502 0.45160294 0.47959304 0.53382576 0.63247967 0.70330846 0.59281462 0.31776482 -0.01128915 -0.30228364 -0.520449 -0.63267195 -0.60322773][-0.093089208 0.16860044 0.34828681 0.549678 0.76581478 1.0876281 1.4427459 1.6417753 1.5669599 1.2738234 0.85628569 0.45622724 0.11020285 -0.096494853 -0.11901966][-0.25513774 -0.061994582 0.050724626 0.2199347 0.50788826 0.85802686 1.2032396 1.4591292 1.5707359 1.4877715 1.2625811 0.9808166 0.71434963 0.54384643 0.50642633][-0.45025772 -0.37480527 -0.3552351 -0.26654649 -0.046156049 0.21982461 0.45133835 0.66670704 0.87939131 1.041881 1.1122 1.1029998 1.0487376 1.0207285 1.0367339][-0.41959009 -0.45654124 -0.50785625 -0.50770605 -0.41735816 -0.3024829 -0.21066408 -0.10307124 0.051110774 0.26280677 0.48932546 0.67054582 0.8074795 0.91026831 0.9578836][-0.088982016 -0.18148069 -0.30189574 -0.37281862 -0.44576794 -0.56589967 -0.68495917 -0.731847 -0.68745518 -0.51167047 -0.25824052 -0.0069068372 0.20553225 0.33578688 0.37030798][0.3594619 0.29319602 0.13174257 -0.077989221 -0.35578465 -0.68427491 -0.98808104 -1.2010362 -1.2316484 -1.1116753 -0.90381604 -0.67236084 -0.46988449 -0.36752066 -0.35406977][0.393569 0.40184158 0.25058812 -0.048713416 -0.43361512 -0.84234405 -1.2219598 -1.4575452 -1.5247835 -1.4634749 -1.3317455 -1.1719413 -1.0317066 -0.951471 -0.950652][-0.069284528 0.05952999 -0.016107023 -0.32887223 -0.71675205 -1.0669987 -1.4041932 -1.630385 -1.7054307 -1.6627648 -1.5898536 -1.5128107 -1.4374394 -1.3907493 -1.3828254]]...]
INFO - root - 2017-12-16 07:49:16.738245: step 310, loss = 0.73, batch loss = 0.44 (48.0 examples/sec; 0.167 sec/batch; 15h:22m:08s remains)
INFO - root - 2017-12-16 07:49:18.364270: step 320, loss = 0.88, batch loss = 0.60 (49.0 examples/sec; 0.163 sec/batch; 15h:04m:12s remains)
INFO - root - 2017-12-16 07:49:20.043313: step 330, loss = 0.73, batch loss = 0.45 (48.9 examples/sec; 0.164 sec/batch; 15h:05m:54s remains)
INFO - root - 2017-12-16 07:49:21.694153: step 340, loss = 0.76, batch loss = 0.48 (48.6 examples/sec; 0.165 sec/batch; 15h:11m:07s remains)
INFO - root - 2017-12-16 07:49:23.347280: step 350, loss = 0.77, batch loss = 0.48 (48.0 examples/sec; 0.167 sec/batch; 15h:22m:49s remains)
INFO - root - 2017-12-16 07:49:25.032631: step 360, loss = 0.78, batch loss = 0.50 (46.4 examples/sec; 0.172 sec/batch; 15h:53m:54s remains)
INFO - root - 2017-12-16 07:49:26.716772: step 370, loss = 0.76, batch loss = 0.48 (49.8 examples/sec; 0.161 sec/batch; 14h:49m:34s remains)
INFO - root - 2017-12-16 07:49:28.365856: step 380, loss = 0.71, batch loss = 0.42 (47.6 examples/sec; 0.168 sec/batch; 15h:29m:23s remains)
INFO - root - 2017-12-16 07:49:30.026068: step 390, loss = 0.76, batch loss = 0.48 (48.7 examples/sec; 0.164 sec/batch; 15h:09m:47s remains)
INFO - root - 2017-12-16 07:49:31.658843: step 400, loss = 0.75, batch loss = 0.46 (46.6 examples/sec; 0.172 sec/batch; 15h:50m:04s remains)
2017-12-16 07:49:32.120359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25239933 -0.18898481 -0.13571927 -0.09592393 -0.069583744 -0.0753499 -0.14804244 -0.207622 -0.23535976 -0.25619471 -0.24707724 -0.20901783 -0.16314419 -0.10950306 -0.079156637][-0.37758654 -0.29293913 -0.21952456 -0.15724063 -0.1052494 -0.0919314 -0.15411595 -0.26581931 -0.34257132 -0.37163213 -0.3564696 -0.28811935 -0.22387294 -0.14986813 -0.11430189][-0.49136746 -0.39588693 -0.29721838 -0.21456718 -0.14592081 -0.12376058 -0.17675556 -0.30262581 -0.41473517 -0.461281 -0.45236787 -0.39000729 -0.30932134 -0.22835778 -0.17800972][-0.57771772 -0.4716379 -0.3568452 -0.25467032 -0.16651408 -0.13615158 -0.20172411 -0.32701361 -0.43639895 -0.49674341 -0.49480009 -0.46004084 -0.39823094 -0.33100024 -0.26812068][-0.62564027 -0.5264954 -0.40346691 -0.29586309 -0.20485385 -0.18543756 -0.25584 -0.39990205 -0.48841351 -0.541975 -0.54853666 -0.54044175 -0.5008871 -0.44844341 -0.37079075][-0.61595905 -0.53593206 -0.42872718 -0.35679293 -0.29167137 -0.29398313 -0.37990731 -0.51748145 -0.58380145 -0.61121207 -0.63208789 -0.64043045 -0.60978109 -0.54037964 -0.44806522][-0.57995343 -0.54164439 -0.46966103 -0.45473137 -0.42369413 -0.43713042 -0.53322005 -0.65169942 -0.67778736 -0.69095397 -0.69727516 -0.69986391 -0.66790891 -0.58896828 -0.49564627][-0.53556812 -0.53837746 -0.51809907 -0.53043962 -0.52821487 -0.550668 -0.64240122 -0.73064816 -0.73978436 -0.72190773 -0.702027 -0.66167659 -0.62655658 -0.571077 -0.50400412][-0.48219749 -0.50162709 -0.5357523 -0.59626383 -0.60388267 -0.61936086 -0.70648223 -0.75402558 -0.75479746 -0.71065366 -0.64388347 -0.55290216 -0.50537091 -0.49363214 -0.46855703][-0.43566945 -0.45436016 -0.52443951 -0.61859077 -0.65080583 -0.64625359 -0.71140581 -0.72989559 -0.72149408 -0.66711378 -0.55928707 -0.42996818 -0.36503527 -0.37623164 -0.41533652][-0.45302579 -0.45586538 -0.54016888 -0.62980425 -0.65297627 -0.63639879 -0.67596662 -0.67552674 -0.6588918 -0.59861839 -0.45678654 -0.28967783 -0.22043705 -0.26843476 -0.35597965][-0.50449318 -0.50014108 -0.5726366 -0.63047868 -0.60553813 -0.58251274 -0.61161464 -0.62051725 -0.60065818 -0.53570825 -0.37382361 -0.19860059 -0.14221245 -0.22304654 -0.33691296][-0.522236 -0.52784008 -0.582005 -0.6071533 -0.54236406 -0.51129574 -0.54629636 -0.55165708 -0.50976712 -0.43042117 -0.28576073 -0.15876719 -0.1413599 -0.23934354 -0.35147014][-0.47149682 -0.49758768 -0.53750765 -0.54840672 -0.48335171 -0.44361478 -0.45443407 -0.44455197 -0.38813978 -0.30499187 -0.19574578 -0.12744442 -0.15372053 -0.26012474 -0.3552551][-0.35272312 -0.37700355 -0.40820527 -0.424615 -0.38321102 -0.35051191 -0.3485491 -0.33327284 -0.28850275 -0.22086617 -0.14213148 -0.11237001 -0.17064327 -0.27007025 -0.34276718]]...]
INFO - root - 2017-12-16 07:49:33.796506: step 410, loss = 0.75, batch loss = 0.47 (47.0 examples/sec; 0.170 sec/batch; 15h:42m:24s remains)
INFO - root - 2017-12-16 07:49:35.466798: step 420, loss = 0.72, batch loss = 0.44 (48.4 examples/sec; 0.165 sec/batch; 15h:15m:19s remains)
INFO - root - 2017-12-16 07:49:37.138995: step 430, loss = 0.74, batch loss = 0.45 (46.4 examples/sec; 0.172 sec/batch; 15h:54m:02s remains)
INFO - root - 2017-12-16 07:49:38.832330: step 440, loss = 0.71, batch loss = 0.42 (44.2 examples/sec; 0.181 sec/batch; 16h:41m:34s remains)
INFO - root - 2017-12-16 07:49:40.485376: step 450, loss = 0.76, batch loss = 0.48 (47.8 examples/sec; 0.167 sec/batch; 15h:25m:45s remains)
INFO - root - 2017-12-16 07:49:42.146191: step 460, loss = 0.74, batch loss = 0.46 (46.6 examples/sec; 0.172 sec/batch; 15h:49m:31s remains)
INFO - root - 2017-12-16 07:49:43.819349: step 470, loss = 0.75, batch loss = 0.47 (48.7 examples/sec; 0.164 sec/batch; 15h:08m:48s remains)
INFO - root - 2017-12-16 07:49:45.482252: step 480, loss = 0.74, batch loss = 0.46 (48.3 examples/sec; 0.166 sec/batch; 15h:16m:23s remains)
INFO - root - 2017-12-16 07:49:47.154978: step 490, loss = 0.79, batch loss = 0.51 (46.8 examples/sec; 0.171 sec/batch; 15h:45m:15s remains)
INFO - root - 2017-12-16 07:49:48.881530: step 500, loss = 0.69, batch loss = 0.40 (47.3 examples/sec; 0.169 sec/batch; 15h:35m:37s remains)
2017-12-16 07:49:49.360809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.7599386 -0.77959645 -0.77262008 -0.78432751 -0.82348233 -0.85122275 -0.84037322 -0.80234754 -0.75683093 -0.71767896 -0.67795396 -0.642578 -0.62563753 -0.64338577 -0.66418386][-0.844915 -0.857208 -0.83039832 -0.8331449 -0.89739668 -0.96085078 -0.96506423 -0.89658868 -0.80852085 -0.73891914 -0.68885529 -0.65792918 -0.61526483 -0.5800364 -0.51910031][-0.89231861 -0.8998912 -0.84195578 -0.81364155 -0.916546 -1.0574087 -1.1197152 -1.0653054 -0.9718641 -0.88024151 -0.83136219 -0.80660081 -0.73206073 -0.606908 -0.39992282][-0.86545074 -0.82782733 -0.71024275 -0.62831795 -0.722446 -0.94643629 -1.0923415 -1.1007637 -1.0487132 -0.99362946 -1.0035123 -1.0421221 -0.97592777 -0.78700417 -0.48256892][-0.72153664 -0.57480484 -0.35416263 -0.20544952 -0.19770861 -0.43026945 -0.65569478 -0.7567715 -0.84161639 -0.93277717 -1.0929514 -1.265435 -1.292997 -1.1266731 -0.75112748][-0.5436843 -0.25579083 0.11574557 0.41540596 0.579146 0.47258946 0.28475592 0.11266842 -0.091267437 -0.33860639 -0.730111 -1.1364602 -1.3570222 -1.3302747 -1.0673263][-0.485118 -0.12359223 0.30571797 0.73298 1.1240295 1.3220296 1.4071009 1.332689 1.0942729 0.72337234 0.16212693 -0.49376333 -0.98398709 -1.2048665 -1.1536304][-0.61820912 -0.3037158 0.08335945 0.51930726 1.0763365 1.6694078 2.204829 2.4386435 2.2488441 1.8287437 1.0869972 0.23113963 -0.48770785 -0.89569187 -1.0185645][-0.79968309 -0.63804114 -0.37906206 -0.082155764 0.45039883 1.1433785 1.84027 2.314575 2.3259187 1.9399366 1.2194389 0.42008296 -0.26178581 -0.71038592 -0.89908719][-0.93328834 -0.93522608 -0.82199264 -0.59225154 -0.077595949 0.5300833 1.104525 1.4998821 1.5199909 1.1722091 0.6803118 0.14835468 -0.35461962 -0.66941392 -0.86968452][-0.91261971 -0.99683166 -0.94260371 -0.65699857 -0.21347645 0.2182956 0.55390859 0.70047057 0.61923027 0.38467178 0.04527387 -0.30390888 -0.56025863 -0.74281693 -0.86965728][-0.80900925 -0.86096871 -0.7450332 -0.47107702 -0.11489359 0.15600142 0.25306454 0.16975692 -0.0045204461 -0.23867026 -0.48617196 -0.69643682 -0.77870876 -0.83130145 -0.84002042][-0.757625 -0.77125323 -0.64509267 -0.42913646 -0.17913002 -0.037743777 -0.094041437 -0.23883626 -0.43816289 -0.64262766 -0.80056792 -0.85305691 -0.78373265 -0.724682 -0.67235571][-0.74063993 -0.76190424 -0.72078013 -0.640115 -0.57118243 -0.56170577 -0.63189059 -0.71749926 -0.80846667 -0.90194541 -0.92690539 -0.86520219 -0.7383675 -0.61691236 -0.55782735][-0.72813207 -0.77730322 -0.81774616 -0.88709986 -1.0040281 -1.0862098 -1.1233644 -1.1434349 -1.1353778 -1.0900176 -1.0120646 -0.92066556 -0.81938076 -0.70152396 -0.63201624]]...]
INFO - root - 2017-12-16 07:49:51.018232: step 510, loss = 0.72, batch loss = 0.43 (47.6 examples/sec; 0.168 sec/batch; 15h:30m:07s remains)
INFO - root - 2017-12-16 07:49:52.722575: step 520, loss = 0.81, batch loss = 0.52 (47.8 examples/sec; 0.167 sec/batch; 15h:26m:01s remains)
INFO - root - 2017-12-16 07:49:54.388340: step 530, loss = 0.72, batch loss = 0.43 (49.2 examples/sec; 0.163 sec/batch; 14h:59m:16s remains)
INFO - root - 2017-12-16 07:49:56.054996: step 540, loss = 0.74, batch loss = 0.46 (47.3 examples/sec; 0.169 sec/batch; 15h:34m:55s remains)
INFO - root - 2017-12-16 07:49:57.742589: step 550, loss = 0.78, batch loss = 0.50 (46.9 examples/sec; 0.171 sec/batch; 15h:43m:43s remains)
INFO - root - 2017-12-16 07:49:59.411250: step 560, loss = 0.75, batch loss = 0.47 (48.6 examples/sec; 0.164 sec/batch; 15h:09m:50s remains)
INFO - root - 2017-12-16 07:50:01.117030: step 570, loss = 0.78, batch loss = 0.50 (45.9 examples/sec; 0.174 sec/batch; 16h:03m:59s remains)
INFO - root - 2017-12-16 07:50:02.751206: step 580, loss = 0.72, batch loss = 0.44 (48.7 examples/sec; 0.164 sec/batch; 15h:08m:19s remains)
INFO - root - 2017-12-16 07:50:04.433230: step 590, loss = 0.79, batch loss = 0.51 (47.6 examples/sec; 0.168 sec/batch; 15h:30m:07s remains)
INFO - root - 2017-12-16 07:50:06.098122: step 600, loss = 0.69, batch loss = 0.41 (48.1 examples/sec; 0.166 sec/batch; 15h:19m:14s remains)
2017-12-16 07:50:06.587100: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1539588 0.20615715 0.24841166 0.26342243 0.26770037 0.23142731 0.13470793 0.047785103 -0.023055375 -0.12069538 -0.21844149 -0.25108919 -0.19201058 -0.10185635 -0.068301111][0.23159993 0.28411257 0.34216386 0.38685149 0.42367232 0.40479356 0.30237579 0.18481416 0.043272316 -0.13395953 -0.29525015 -0.37594432 -0.35493225 -0.27890232 -0.26198155][0.41638237 0.4182542 0.43317848 0.48136717 0.57113153 0.58811373 0.49336475 0.33340281 0.13224137 -0.089445233 -0.29221529 -0.43909609 -0.51910341 -0.52330714 -0.51667362][0.63734442 0.60825914 0.55513114 0.55690724 0.62490588 0.65950042 0.609145 0.473656 0.27968252 0.040280342 -0.22344941 -0.46666756 -0.6443941 -0.70935994 -0.69238007][0.711657 0.74370748 0.68545288 0.63515478 0.64572138 0.69095773 0.69318742 0.58871919 0.4014498 0.15219259 -0.13587248 -0.42144489 -0.62013507 -0.69598651 -0.68152773][0.6114431 0.7321071 0.76869088 0.76651472 0.77859551 0.81513911 0.81328744 0.69070727 0.49402362 0.25920027 0.0065441132 -0.23897207 -0.42821681 -0.51281059 -0.50976408][0.43344694 0.62238365 0.7768808 0.88439494 0.94764513 0.97146755 0.95122021 0.84257907 0.67497665 0.46864992 0.25517255 0.051992595 -0.12555584 -0.23736647 -0.2642301][0.23279369 0.44901162 0.65239054 0.77869278 0.83517343 0.86482686 0.90645391 0.92452317 0.88179761 0.75294918 0.57050985 0.38123089 0.21369869 0.095220566 0.040886164][-0.004848361 0.19669586 0.37450868 0.46578449 0.50664061 0.55691427 0.64988691 0.76562828 0.85755295 0.87124294 0.789192 0.65476853 0.51943547 0.42167056 0.38348824][-0.26472825 -0.13221207 -0.015084326 0.062121511 0.11826074 0.19108152 0.30126005 0.44037288 0.59828204 0.73844379 0.80055577 0.76590425 0.68156785 0.60023338 0.60415095][-0.49828327 -0.44771004 -0.39906406 -0.35033441 -0.29116631 -0.21043009 -0.10048744 0.044460118 0.23123121 0.43348861 0.59108871 0.63948554 0.59384686 0.527067 0.55534858][-0.64400083 -0.65470415 -0.65516835 -0.6425423 -0.60814208 -0.5490194 -0.4606342 -0.3249402 -0.13917464 0.0641979 0.23528963 0.32521111 0.31274647 0.26774669 0.28851497][-0.67368287 -0.71568143 -0.74512351 -0.76140261 -0.76173121 -0.742712 -0.68475056 -0.57600069 -0.42508277 -0.27496073 -0.15758699 -0.090262264 -0.079309464 -0.077543259 -0.051357448][-0.61349028 -0.65627545 -0.69443971 -0.72864306 -0.75437379 -0.76108569 -0.73533547 -0.67426449 -0.5904144 -0.516559 -0.47204441 -0.45195305 -0.447088 -0.4264397 -0.39344519][-0.52189946 -0.562698 -0.60129291 -0.63927484 -0.67213863 -0.69178581 -0.69475579 -0.68753022 -0.67389244 -0.66452962 -0.66473043 -0.6715644 -0.67933953 -0.67523807 -0.66016632]]...]
INFO - root - 2017-12-16 07:50:08.287307: step 610, loss = 0.79, batch loss = 0.50 (48.5 examples/sec; 0.165 sec/batch; 15h:11m:47s remains)
INFO - root - 2017-12-16 07:50:09.964646: step 620, loss = 0.72, batch loss = 0.43 (47.2 examples/sec; 0.170 sec/batch; 15h:37m:47s remains)
INFO - root - 2017-12-16 07:50:11.682791: step 630, loss = 0.77, batch loss = 0.49 (47.6 examples/sec; 0.168 sec/batch; 15h:29m:11s remains)
INFO - root - 2017-12-16 07:50:13.412821: step 640, loss = 0.78, batch loss = 0.50 (48.2 examples/sec; 0.166 sec/batch; 15h:17m:58s remains)
INFO - root - 2017-12-16 07:50:15.110244: step 650, loss = 0.69, batch loss = 0.40 (47.4 examples/sec; 0.169 sec/batch; 15h:33m:53s remains)
INFO - root - 2017-12-16 07:50:16.820440: step 660, loss = 0.71, batch loss = 0.43 (47.6 examples/sec; 0.168 sec/batch; 15h:28m:50s remains)
INFO - root - 2017-12-16 07:50:18.485207: step 670, loss = 0.75, batch loss = 0.46 (48.7 examples/sec; 0.164 sec/batch; 15h:07m:53s remains)
INFO - root - 2017-12-16 07:50:20.179817: step 680, loss = 0.80, batch loss = 0.52 (47.0 examples/sec; 0.170 sec/batch; 15h:40m:57s remains)
INFO - root - 2017-12-16 07:50:21.866917: step 690, loss = 0.67, batch loss = 0.39 (48.4 examples/sec; 0.165 sec/batch; 15h:13m:59s remains)
INFO - root - 2017-12-16 07:50:23.532210: step 700, loss = 0.76, batch loss = 0.48 (48.0 examples/sec; 0.167 sec/batch; 15h:20m:53s remains)
2017-12-16 07:50:24.048155: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2143358 1.2545933 1.2646596 1.2594475 1.242184 1.2222946 1.2131717 1.213717 1.2188468 1.228506 1.2405888 1.248376 1.2547501 1.2590797 1.2660297][0.8126713 0.87971342 0.88300478 0.8450259 0.77751696 0.70258474 0.65712297 0.64542663 0.65862632 0.68140709 0.70259297 0.72782481 0.75116432 0.767951 0.78197575][-0.1020596 -0.00084596872 0.010313034 -0.079230249 -0.21098951 -0.34069848 -0.42694068 -0.46162137 -0.43844283 -0.40050805 -0.37380731 -0.34685609 -0.31580055 -0.29854119 -0.28963172][-0.87830168 -0.71153128 -0.67649418 -0.78468078 -0.94373512 -1.0944939 -1.2023832 -1.2518861 -1.2391193 -1.2292274 -1.2302922 -1.2265074 -1.2129166 -1.2064941 -1.2153963][-1.3389024 -1.0757918 -0.91802609 -0.90692985 -0.9814527 -1.0794427 -1.164216 -1.2104025 -1.2282802 -1.2677119 -1.348307 -1.4324417 -1.511518 -1.5694082 -1.6280429][-1.3942647 -1.0645354 -0.69076854 -0.43724293 -0.30353814 -0.24019694 -0.20065579 -0.1860503 -0.23023155 -0.36212415 -0.58460385 -0.81964344 -1.0363449 -1.221553 -1.3704679][-1.3836632 -0.98656613 -0.44082713 0.084097087 0.4512223 0.7348727 0.94885683 1.0922046 1.0870702 0.90136826 0.59392667 0.22322321 -0.16372088 -0.5191263 -0.76728761][-1.4494187 -1.1783082 -0.675834 -0.10241175 0.37983191 0.77353942 1.0737826 1.3006939 1.4010769 1.3075584 1.1053771 0.79159355 0.34884495 -0.091647565 -0.41917843][-1.3145134 -1.2810903 -1.0613515 -0.7335245 -0.38852507 -0.11997193 0.050907671 0.16079992 0.24511194 0.30230695 0.3461659 0.30961925 0.05842495 -0.254458 -0.52761281][-0.96539193 -1.1767435 -1.3267351 -1.3925797 -1.4179218 -1.4223354 -1.4411986 -1.425548 -1.3811719 -1.237255 -1.0169368 -0.82533276 -0.80878174 -0.90752858 -1.0302019][-0.5486182 -0.86216837 -1.2762141 -1.7081445 -2.0766335 -2.3673215 -2.5190587 -2.5704417 -2.55929 -2.4335434 -2.204154 -1.9368412 -1.7295804 -1.6453445 -1.6092651][-0.21405774 -0.502902 -1.0608125 -1.6644701 -2.1845713 -2.5656242 -2.7439222 -2.7931209 -2.7714343 -2.6859422 -2.5074749 -2.2720778 -2.0398722 -1.8771982 -1.8208448][-0.15874138 -0.33735177 -0.82931733 -1.3146876 -1.7096174 -2.0260296 -2.1854463 -2.2253685 -2.2213295 -2.168489 -2.007247 -1.7872981 -1.5804287 -1.4093529 -1.3612196][-0.40273815 -0.46543318 -0.66255867 -0.84362578 -1.0022669 -1.1783118 -1.3401973 -1.4296849 -1.4585252 -1.4234209 -1.27371 -1.0487843 -0.83531743 -0.64744687 -0.53968692][-0.57733309 -0.51522803 -0.45201921 -0.36832109 -0.33581585 -0.43388113 -0.65386784 -0.83438504 -0.95766413 -0.97227162 -0.89413589 -0.73379791 -0.55243373 -0.36018464 -0.19056159]]...]
INFO - root - 2017-12-16 07:50:25.718977: step 710, loss = 0.74, batch loss = 0.46 (48.2 examples/sec; 0.166 sec/batch; 15h:18m:35s remains)
INFO - root - 2017-12-16 07:50:27.409786: step 720, loss = 0.74, batch loss = 0.46 (44.3 examples/sec; 0.180 sec/batch; 16h:37m:47s remains)
INFO - root - 2017-12-16 07:50:29.083557: step 730, loss = 0.65, batch loss = 0.37 (48.2 examples/sec; 0.166 sec/batch; 15h:18m:10s remains)
INFO - root - 2017-12-16 07:50:30.752502: step 740, loss = 0.71, batch loss = 0.43 (48.6 examples/sec; 0.165 sec/batch; 15h:10m:52s remains)
INFO - root - 2017-12-16 07:50:32.392902: step 750, loss = 0.72, batch loss = 0.44 (47.7 examples/sec; 0.168 sec/batch; 15h:26m:32s remains)
INFO - root - 2017-12-16 07:50:34.072574: step 760, loss = 0.82, batch loss = 0.54 (48.7 examples/sec; 0.164 sec/batch; 15h:07m:39s remains)
INFO - root - 2017-12-16 07:50:35.748907: step 770, loss = 0.75, batch loss = 0.47 (46.7 examples/sec; 0.171 sec/batch; 15h:47m:21s remains)
INFO - root - 2017-12-16 07:50:37.418273: step 780, loss = 0.75, batch loss = 0.47 (47.0 examples/sec; 0.170 sec/batch; 15h:40m:33s remains)
INFO - root - 2017-12-16 07:50:39.132947: step 790, loss = 0.73, batch loss = 0.45 (48.3 examples/sec; 0.166 sec/batch; 15h:15m:09s remains)
INFO - root - 2017-12-16 07:50:40.841623: step 800, loss = 0.75, batch loss = 0.46 (47.9 examples/sec; 0.167 sec/batch; 15h:24m:02s remains)
2017-12-16 07:50:41.375897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.90785789 -1.0262637 -0.96880746 -0.73929256 -0.36824971 0.0032495856 0.53148615 0.8895793 0.87015414 0.52057719 -0.1892609 -0.67121357 -0.86240375 -0.93045777 -0.73349208][-0.40222681 -0.571821 -0.53709269 -0.32745779 -0.080836475 0.039922297 0.26624691 0.36401784 0.31690919 0.23659706 -0.25035229 -0.59512228 -0.7195608 -0.75864649 -0.57306468][-0.16372722 -0.39623556 -0.4320935 -0.23774165 -0.023551404 -0.09300065 -0.081440866 -0.14279389 -0.2936036 -0.29552767 -0.52832246 -0.72824275 -0.78912818 -0.77366275 -0.62279391][-0.40232468 -0.65047669 -0.69229931 -0.47895905 -0.16420043 -0.13782126 -0.20149544 -0.31913552 -0.66578537 -0.77162492 -0.84530938 -0.99143875 -1.019927 -0.99593967 -0.90310121][-0.853412 -1.1057396 -1.1152322 -0.81874657 -0.31415579 -0.023201764 0.082974195 -0.025612831 -0.5897398 -0.95527363 -1.1366726 -1.288857 -1.2953997 -1.276692 -1.2351575][-1.3248171 -1.5562298 -1.5379084 -1.1557177 -0.43585679 0.16926116 0.57470751 0.53396773 -0.16455328 -0.84706676 -1.230759 -1.4421661 -1.4046841 -1.3721876 -1.3456674][-1.4964547 -1.7592683 -1.752293 -1.3966222 -0.61260664 0.24662191 0.969182 1.0706222 0.36040628 -0.43483183 -1.0518533 -1.3378707 -1.3012202 -1.2547185 -1.2436421][-1.1639559 -1.4305155 -1.5077065 -1.3234167 -0.73935807 0.10965395 0.97137773 1.2470397 0.62983119 -0.15028226 -0.77840185 -1.0782156 -1.0584223 -0.9864338 -0.95677662][-0.42533284 -0.62229419 -0.75714344 -0.80898875 -0.60149914 -0.047987103 0.61526966 0.91732383 0.49620473 -0.098075986 -0.64859152 -0.91274554 -0.8892588 -0.78908288 -0.73127031][0.122096 0.10061687 -0.0057294369 -0.21176645 -0.33802438 -0.21224031 0.1227476 0.35138583 0.17640084 -0.24692652 -0.69895816 -0.93212205 -0.89745212 -0.79406369 -0.71455193][0.056771696 0.19948578 0.27316523 0.10448623 -0.23230281 -0.4015207 -0.32716912 -0.19191468 -0.20243049 -0.45028657 -0.78686154 -0.98417735 -0.97614491 -0.89138418 -0.81115347][-0.35455152 -0.17544287 -0.034723103 -0.12794614 -0.44647974 -0.660483 -0.67525458 -0.59340763 -0.512887 -0.60080087 -0.80603206 -0.95220149 -0.96831381 -0.90549952 -0.83221889][-0.76700276 -0.69225818 -0.53705037 -0.522817 -0.70378846 -0.85626686 -0.8946557 -0.82419574 -0.6984148 -0.65065622 -0.7352826 -0.8318485 -0.8652069 -0.82636994 -0.7783367][-0.96585697 -0.96811694 -0.86822176 -0.78665346 -0.82296187 -0.89726079 -0.92443073 -0.88264334 -0.7609598 -0.65889245 -0.6585151 -0.71252275 -0.74216032 -0.72189975 -0.68947816][-0.97683597 -1.0020771 -0.94129211 -0.83913189 -0.790923 -0.7991854 -0.82246184 -0.81105059 -0.74123538 -0.6665262 -0.65993667 -0.69561923 -0.71721667 -0.70602053 -0.67818838]]...]
INFO - root - 2017-12-16 07:50:43.053670: step 810, loss = 0.64, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 15h:26m:13s remains)
INFO - root - 2017-12-16 07:50:44.705607: step 820, loss = 0.69, batch loss = 0.40 (47.7 examples/sec; 0.168 sec/batch; 15h:27m:48s remains)
INFO - root - 2017-12-16 07:50:46.385671: step 830, loss = 0.73, batch loss = 0.45 (46.9 examples/sec; 0.171 sec/batch; 15h:43m:47s remains)
INFO - root - 2017-12-16 07:50:48.067635: step 840, loss = 0.65, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 15h:22m:34s remains)
INFO - root - 2017-12-16 07:50:49.742333: step 850, loss = 0.74, batch loss = 0.45 (48.7 examples/sec; 0.164 sec/batch; 15h:08m:41s remains)
INFO - root - 2017-12-16 07:50:51.433597: step 860, loss = 0.78, batch loss = 0.50 (47.2 examples/sec; 0.169 sec/batch; 15h:36m:19s remains)
INFO - root - 2017-12-16 07:50:53.132241: step 870, loss = 0.70, batch loss = 0.42 (47.5 examples/sec; 0.168 sec/batch; 15h:30m:57s remains)
INFO - root - 2017-12-16 07:50:54.788816: step 880, loss = 0.75, batch loss = 0.46 (49.4 examples/sec; 0.162 sec/batch; 14h:55m:29s remains)
INFO - root - 2017-12-16 07:50:56.456297: step 890, loss = 0.72, batch loss = 0.44 (46.5 examples/sec; 0.172 sec/batch; 15h:50m:35s remains)
INFO - root - 2017-12-16 07:50:58.153527: step 900, loss = 0.70, batch loss = 0.42 (45.9 examples/sec; 0.174 sec/batch; 16h:04m:02s remains)
2017-12-16 07:50:58.647115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8478909 -1.7536516 -1.7041225 -1.6478864 -1.524642 -1.3578131 -1.3058586 -1.3954301 -1.4604745 -1.5334792 -1.5232636 -1.4226505 -1.2925627 -1.1065892 -0.81947678][-1.8942943 -1.6977131 -1.476126 -1.1281444 -0.68723583 -0.3313674 -0.27530041 -0.5211364 -0.91783392 -1.2940657 -1.4179116 -1.2100884 -0.80596662 -0.30575132 0.18570828][-1.4970642 -1.2668839 -0.99358118 -0.53386837 -0.0036200285 0.3265304 0.32527536 -0.035610437 -0.66781807 -1.1593122 -1.22435 -0.89786363 -0.31153893 0.36565167 0.94524163][-0.4646869 -0.40308452 -0.37462625 -0.2033475 0.055546761 0.2673946 0.26327771 -0.10770774 -0.64355904 -0.98518252 -0.95394075 -0.62538695 -0.14984244 0.32814449 0.77941352][0.63744777 0.37174183 -0.068524957 -0.36360955 -0.44398883 -0.36916476 -0.282727 -0.31954744 -0.38363984 -0.42769602 -0.39807579 -0.3291181 -0.3303473 -0.27981907 -0.11229986][0.79715866 0.24796277 -0.39280778 -0.86443484 -1.095368 -1.0695603 -0.77250206 -0.32820195 0.11774629 0.35607284 0.3615182 -0.010905206 -0.62132561 -1.0933679 -1.2748895][0.098080993 -0.50066543 -1.0110779 -1.2889144 -1.2612269 -0.9585892 -0.40865663 0.30822247 0.90888995 1.1144257 0.92102319 0.15288663 -0.85526168 -1.6236465 -1.9952681][-1.0548604 -1.4119689 -1.5052758 -1.2769136 -0.79117131 -0.1880005 0.48855954 1.1461916 1.4961724 1.3811743 0.89012867 0.034885645 -0.91780627 -1.6379478 -2.01017][-1.7386074 -1.8788302 -1.6744424 -1.1368538 -0.45014334 0.15460455 0.58791763 0.77771968 0.79019994 0.61931831 0.26424533 -0.2590152 -0.8417663 -1.2906138 -1.5435365][-1.7786133 -1.8333483 -1.5750942 -1.0897455 -0.57350069 -0.24231529 -0.14933527 -0.31318557 -0.46085086 -0.58988386 -0.66445142 -0.76537675 -0.921486 -1.0194898 -1.1115767][-1.4967568 -1.5044336 -1.3227477 -1.0849389 -0.94037104 -0.97926116 -1.1253992 -1.3341264 -1.4102409 -1.4104925 -1.3514969 -1.2974273 -1.2161715 -1.059938 -0.98755693][-1.2356385 -1.1567367 -1.0688541 -1.0964053 -1.3073109 -1.5860484 -1.8136206 -1.9176912 -1.8648024 -1.8023226 -1.7166014 -1.5926344 -1.3856561 -1.0604262 -0.84664881][-0.9973824 -0.74044371 -0.68989015 -0.90996945 -1.3154449 -1.6970768 -1.9421077 -2.0175433 -1.9677222 -1.8672597 -1.6995857 -1.469433 -1.1121428 -0.67622334 -0.37069818][-0.58789074 -0.23026478 -0.24893409 -0.6028139 -1.0704383 -1.398428 -1.5730858 -1.6443505 -1.60639 -1.5197766 -1.3115325 -0.91965336 -0.40955016 0.069005728 0.3391301][-0.19656479 0.049011171 -0.10640645 -0.49045944 -0.80953932 -0.91313428 -0.91465831 -0.9070701 -0.88527089 -0.82680809 -0.63325006 -0.25021029 0.24485964 0.63594383 0.77004784]]...]
INFO - root - 2017-12-16 07:51:00.335992: step 910, loss = 0.70, batch loss = 0.42 (46.3 examples/sec; 0.173 sec/batch; 15h:55m:50s remains)
INFO - root - 2017-12-16 07:51:02.026075: step 920, loss = 0.80, batch loss = 0.52 (48.7 examples/sec; 0.164 sec/batch; 15h:08m:11s remains)
INFO - root - 2017-12-16 07:51:03.719193: step 930, loss = 0.76, batch loss = 0.48 (48.3 examples/sec; 0.166 sec/batch; 15h:14m:52s remains)
INFO - root - 2017-12-16 07:51:05.395049: step 940, loss = 0.67, batch loss = 0.39 (48.2 examples/sec; 0.166 sec/batch; 15h:16m:20s remains)
INFO - root - 2017-12-16 07:51:07.090983: step 950, loss = 0.69, batch loss = 0.41 (38.8 examples/sec; 0.206 sec/batch; 18h:59m:56s remains)
INFO - root - 2017-12-16 07:51:08.777661: step 960, loss = 0.72, batch loss = 0.44 (48.0 examples/sec; 0.167 sec/batch; 15h:20m:48s remains)
INFO - root - 2017-12-16 07:51:10.435750: step 970, loss = 0.71, batch loss = 0.43 (48.6 examples/sec; 0.165 sec/batch; 15h:09m:53s remains)
INFO - root - 2017-12-16 07:51:12.113235: step 980, loss = 0.64, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 15h:35m:03s remains)
INFO - root - 2017-12-16 07:51:13.769980: step 990, loss = 0.75, batch loss = 0.47 (48.3 examples/sec; 0.166 sec/batch; 15h:14m:41s remains)
INFO - root - 2017-12-16 07:51:15.444688: step 1000, loss = 0.71, batch loss = 0.43 (46.1 examples/sec; 0.174 sec/batch; 15h:59m:11s remains)
2017-12-16 07:51:15.937960: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.063154817 -0.23788005 -0.5450989 -0.76848233 -0.87363833 -0.92296731 -1.0062308 -1.1344547 -1.2102679 -1.1918193 -1.0730549 -0.89972115 -0.75740004 -0.66923833 -0.6066469][-0.12727225 -0.56731534 -0.9204371 -1.1306876 -1.2327881 -1.2850728 -1.3334613 -1.3902688 -1.3922483 -1.3048331 -1.1356813 -0.93939322 -0.7786991 -0.68111897 -0.63599][-0.57013917 -0.93482411 -1.2059964 -1.3398943 -1.4054511 -1.4698936 -1.5327044 -1.5704839 -1.5222414 -1.361769 -1.1317647 -0.89381135 -0.7282306 -0.66889256 -0.6937409][-1.0314215 -1.2385014 -1.3829761 -1.4528419 -1.5084066 -1.5778869 -1.6092554 -1.5424042 -1.3431194 -1.041195 -0.73649693 -0.508653 -0.41025355 -0.45782864 -0.606822][-1.2004218 -1.290431 -1.3605282 -1.4171929 -1.4558687 -1.4364944 -1.2875402 -0.9687767 -0.53679049 -0.11815363 0.172845 0.24166846 0.0837273 -0.19315284 -0.45232809][-1.0523621 -1.1211269 -1.1828384 -1.2041993 -1.1369263 -0.9158709 -0.48064464 0.15551817 0.81843281 1.1765609 1.1336015 0.79542446 0.34390891 -0.07710427 -0.37329653][-0.76953524 -0.85168546 -0.91321212 -0.8313396 -0.57072091 -0.11165875 0.53234124 1.3228356 2.0738378 2.041687 1.3730174 0.63506377 0.056960344 -0.31228307 -0.51979458][-0.56492907 -0.6183207 -0.65525806 -0.44470185 0.020776272 0.56996632 1.038489 1.3430043 1.425187 1.0808402 0.43444204 -0.18656969 -0.59086162 -0.7581628 -0.80825156][-0.47152585 -0.48356691 -0.4261055 -0.12082881 0.35633087 0.71254206 0.70483875 0.39682734 -0.012128532 -0.41110232 -0.792564 -1.0783051 -1.19831 -1.1512648 -1.0291207][-0.30439 -0.33852538 -0.24515963 0.039388895 0.27829719 0.20028162 -0.22867405 -0.78046554 -1.2486277 -1.5176774 -1.6142073 -1.5833334 -1.450965 -1.2275052 -0.99726892][-0.0988633 -0.24051279 -0.23839933 -0.11280745 -0.1462971 -0.47021 -0.99546456 -1.4944217 -1.8340772 -1.924036 -1.801443 -1.5779215 -1.3203837 -1.0637797 -0.84950757][-0.0152542 -0.25961363 -0.39188698 -0.43854597 -0.61909258 -0.96335918 -1.3606327 -1.6438477 -1.7145903 -1.6040661 -1.3747132 -1.1336561 -0.93419576 -0.802163 -0.734028][-0.06021142 -0.39898685 -0.583429 -0.689875 -0.85926288 -1.0973095 -1.2821783 -1.2985133 -1.1355815 -0.86914039 -0.6249525 -0.49800876 -0.517241 -0.62319529 -0.74217278][-0.17995483 -0.57809395 -0.7868005 -0.90262765 -0.9989779 -1.0731363 -1.0201433 -0.79019362 -0.45291245 -0.11767876 0.031609714 -0.058143377 -0.32108805 -0.61330187 -0.82778352][-0.4556627 -0.81433672 -1.0094323 -1.0890101 -1.0801394 -0.96883118 -0.70922554 -0.31185395 0.09233886 0.32233644 0.26596534 -0.0169155 -0.35805753 -0.65167922 -0.84169972]]...]
INFO - root - 2017-12-16 07:51:17.617032: step 1010, loss = 0.79, batch loss = 0.50 (47.7 examples/sec; 0.168 sec/batch; 15h:25m:58s remains)
INFO - root - 2017-12-16 07:51:19.269427: step 1020, loss = 0.72, batch loss = 0.43 (47.8 examples/sec; 0.167 sec/batch; 15h:23m:56s remains)
INFO - root - 2017-12-16 07:51:20.939418: step 1030, loss = 0.77, batch loss = 0.49 (48.4 examples/sec; 0.165 sec/batch; 15h:14m:04s remains)
INFO - root - 2017-12-16 07:51:22.605735: step 1040, loss = 0.75, batch loss = 0.46 (46.6 examples/sec; 0.172 sec/batch; 15h:48m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 07:51:24.277300: step 1050, loss = 0.69, batch loss = 0.41 (46.9 examples/sec; 0.170 sec/batch; 15h:41m:30s remains)
INFO - root - 2017-12-16 07:51:25.946429: step 1060, loss = 0.64, batch loss = 0.36 (49.5 examples/sec; 0.162 sec/batch; 14h:53m:04s remains)
INFO - root - 2017-12-16 07:51:27.594052: step 1070, loss = 0.68, batch loss = 0.40 (48.8 examples/sec; 0.164 sec/batch; 15h:06m:25s remains)
INFO - root - 2017-12-16 07:51:29.283698: step 1080, loss = 0.66, batch loss = 0.38 (46.4 examples/sec; 0.172 sec/batch; 15h:52m:16s remains)
INFO - root - 2017-12-16 07:51:30.926569: step 1090, loss = 0.73, batch loss = 0.45 (48.8 examples/sec; 0.164 sec/batch; 15h:05m:06s remains)
INFO - root - 2017-12-16 07:51:32.589689: step 1100, loss = 0.65, batch loss = 0.37 (47.8 examples/sec; 0.168 sec/batch; 15h:25m:14s remains)
2017-12-16 07:51:33.125755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3968871 -1.0761678 -0.62127936 -0.30205804 -0.27624542 -0.36414328 -0.52540827 -0.82435364 -0.98504579 -0.74181443 -0.33347702 -0.48935783 -1.1687948 -1.7345455 -1.784526][-1.1391811 -0.73804671 -0.32907784 -0.10315204 -0.19199973 -0.37132841 -0.63302445 -1.010179 -1.182385 -0.90596563 -0.5702672 -0.75886476 -1.2674652 -1.5561626 -1.4351298][-0.75131786 -0.37909403 -0.10403645 -0.13380861 -0.425156 -0.72948408 -0.99704939 -1.2569344 -1.1105034 -0.58773267 -0.31325024 -0.65959924 -1.1862379 -1.4291849 -1.2886914][-0.56757319 -0.29696029 -0.20637888 -0.44872117 -0.89130616 -1.2204908 -1.3708298 -1.1556846 -0.49793503 0.26157445 0.30133396 -0.35280663 -1.1301336 -1.4706075 -1.3570102][-0.61195153 -0.39393994 -0.35533071 -0.66494155 -1.1187922 -1.3618431 -1.2183816 -0.4101795 0.80347604 1.5552373 0.9769966 -0.23863119 -1.308893 -1.7416661 -1.5682658][-0.67458922 -0.50172526 -0.42962274 -0.66358948 -1.0159397 -1.038131 -0.5223369 0.79495281 2.4787309 2.8387918 1.4085829 -0.45665851 -1.7707295 -2.2147346 -1.9197619][-0.60805941 -0.4113389 -0.30653012 -0.4525159 -0.67977613 -0.46220157 0.46041507 2.1491876 4.015317 3.6390216 1.4269331 -0.86967617 -2.2900438 -2.6562164 -2.2604251][-0.36834335 -0.18396056 -0.095854163 -0.17931181 -0.27796239 0.10191804 1.2080336 2.9611466 4.3830075 3.2498839 0.81773847 -1.4079912 -2.6139677 -2.7531652 -2.2329631][-0.17248857 -0.0090315342 0.045686007 0.019724071 0.017710328 0.38900107 1.3592026 2.703819 3.3227355 2.0054407 -0.075217307 -1.8265353 -2.6049893 -2.4332886 -1.7959697][-0.19393224 -0.050938964 0.03375572 0.047722995 0.11418247 0.40892917 1.0525737 1.7631104 1.7812867 0.72137362 -0.80725479 -1.9383645 -2.2512586 -1.8499107 -1.1772654][-0.39848676 -0.30357862 -0.20020056 -0.138484 -0.040981293 0.14439934 0.46836489 0.73497266 0.55983919 -0.15959579 -1.1309584 -1.7234857 -1.7183189 -1.2489605 -0.65987748][-0.56055933 -0.48965898 -0.40290278 -0.34320116 -0.27937132 -0.16371697 -0.037186503 0.056362689 -0.044458449 -0.48882604 -1.0686777 -1.333268 -1.1939939 -0.804513 -0.39130124][-0.6760112 -0.62362772 -0.5446527 -0.4909927 -0.45921335 -0.42799228 -0.408057 -0.38160214 -0.42356509 -0.62542456 -0.89348412 -1.00176 -0.87061477 -0.63232112 -0.43851379][-0.70909047 -0.68125546 -0.60088617 -0.54029197 -0.50339067 -0.51161015 -0.52680957 -0.56949079 -0.61861622 -0.68899333 -0.77988559 -0.80896616 -0.76116866 -0.67978936 -0.59793168][-0.73822892 -0.68957448 -0.5830512 -0.49450672 -0.45784116 -0.46170187 -0.51862127 -0.62395084 -0.673285 -0.66091144 -0.678742 -0.73921353 -0.76944995 -0.77148932 -0.72829896]]...]
INFO - root - 2017-12-16 07:51:34.786435: step 1110, loss = 0.73, batch loss = 0.45 (48.3 examples/sec; 0.166 sec/batch; 15h:15m:27s remains)
INFO - root - 2017-12-16 07:51:36.437541: step 1120, loss = 0.65, batch loss = 0.36 (48.7 examples/sec; 0.164 sec/batch; 15h:06m:51s remains)
INFO - root - 2017-12-16 07:51:38.135836: step 1130, loss = 0.63, batch loss = 0.34 (46.4 examples/sec; 0.172 sec/batch; 15h:52m:08s remains)
INFO - root - 2017-12-16 07:51:39.824587: step 1140, loss = 0.71, batch loss = 0.43 (47.4 examples/sec; 0.169 sec/batch; 15h:32m:54s remains)
INFO - root - 2017-12-16 07:51:41.485071: step 1150, loss = 0.71, batch loss = 0.43 (48.8 examples/sec; 0.164 sec/batch; 15h:04m:57s remains)
INFO - root - 2017-12-16 07:51:43.140775: step 1160, loss = 0.64, batch loss = 0.36 (49.1 examples/sec; 0.163 sec/batch; 14h:58m:54s remains)
INFO - root - 2017-12-16 07:51:44.801068: step 1170, loss = 0.76, batch loss = 0.48 (49.3 examples/sec; 0.162 sec/batch; 14h:56m:35s remains)
INFO - root - 2017-12-16 07:51:46.456176: step 1180, loss = 0.67, batch loss = 0.39 (47.2 examples/sec; 0.169 sec/batch; 15h:35m:39s remains)
INFO - root - 2017-12-16 07:51:48.109371: step 1190, loss = 0.75, batch loss = 0.47 (47.7 examples/sec; 0.168 sec/batch; 15h:26m:11s remains)
INFO - root - 2017-12-16 07:51:49.752313: step 1200, loss = 0.64, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 15h:20m:46s remains)
2017-12-16 07:51:50.223754: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13199639 -0.058766246 -0.33195204 -0.43342716 -0.33754611 -0.18585718 -0.28220898 -0.59457493 -0.80452639 -0.78373349 -0.68923974 -0.72981733 -0.85133386 -0.83654994 -0.67990619][0.022206664 -0.21124369 -0.46508092 -0.50359178 -0.41332728 -0.29540145 -0.31664091 -0.58356822 -0.82651812 -0.89454406 -0.85589349 -0.91573864 -1.0259571 -0.98382878 -0.80828011][-0.43748161 -0.70520079 -0.900102 -0.82689637 -0.66612446 -0.47427261 -0.34122092 -0.48185802 -0.78479373 -0.95456171 -0.97777736 -1.0445994 -1.1249112 -1.0870646 -0.94939047][-0.86599648 -1.1596631 -1.323341 -1.1961892 -0.92556632 -0.58874488 -0.24847001 -0.23035955 -0.5492698 -0.81377846 -0.87185818 -0.91033441 -0.93274188 -0.90848464 -0.86762619][-0.8792752 -1.1987741 -1.3744935 -1.228027 -0.9225924 -0.51647151 -0.027177274 0.18346918 -0.10287166 -0.41333622 -0.48623398 -0.40898046 -0.35589951 -0.33150971 -0.46032619][-0.23395276 -0.62754595 -0.79827619 -0.65258753 -0.38403636 0.0049355626 0.60099185 0.96168315 0.60647964 0.18553698 0.0693295 0.215693 0.35931003 0.4048748 0.16716754][0.6840328 0.28100324 0.062892437 0.15491438 0.38575935 0.70007789 1.4338217 2.0719342 1.5355673 0.88679242 0.63776815 0.67592621 0.79387879 0.91509974 0.71496379][1.154228 0.73114789 0.4786514 0.52612221 0.69347548 1.0440069 2.0419223 3.2606215 2.4951172 1.5274696 1.0086282 0.78210378 0.82132292 1.0192213 0.97537243][0.84551406 0.33062708 0.026327908 0.031864822 0.18432367 0.58807755 1.5685933 2.7563384 2.3005395 1.4365227 0.85609758 0.49538708 0.45042896 0.69195127 0.76352394][0.059278488 -0.43176767 -0.67670488 -0.65822989 -0.5159601 -0.15892422 0.60683489 1.4490697 1.3954501 0.90892053 0.46160519 0.15454495 0.045991182 0.16958356 0.25993204][-0.41832858 -0.75738335 -0.88917905 -0.87052113 -0.796752 -0.62903464 -0.15493619 0.38972938 0.44929314 0.22808027 -0.0098587275 -0.17048633 -0.26487374 -0.25755024 -0.2203303][-0.32754689 -0.46720353 -0.4285982 -0.42808425 -0.44535336 -0.53707135 -0.4457846 -0.19075704 -0.22845691 -0.40941715 -0.45342234 -0.36498904 -0.30383462 -0.30377859 -0.31881934][-0.0017645359 0.074216545 0.28075421 0.30716443 0.25066519 -0.017435133 -0.24291182 -0.23806494 -0.42177841 -0.70548111 -0.60663986 -0.23356754 0.12175989 0.15626907 -0.048915684][0.23324299 0.40534627 0.72873271 0.79874837 0.79610372 0.5312705 0.20119226 0.093662918 -0.18649417 -0.62708008 -0.56042594 0.053043187 0.76675546 0.87108159 0.31925511][0.18114555 0.26012945 0.51419461 0.60725331 0.71454287 0.56503236 0.4163065 0.46644056 0.15543664 -0.36457372 -0.29962927 0.41773045 1.3628187 1.4766154 0.56031466]]...]
INFO - root - 2017-12-16 07:51:51.879316: step 1210, loss = 0.77, batch loss = 0.49 (48.9 examples/sec; 0.164 sec/batch; 15h:03m:41s remains)
INFO - root - 2017-12-16 07:51:53.555149: step 1220, loss = 0.64, batch loss = 0.36 (48.6 examples/sec; 0.165 sec/batch; 15h:08m:53s remains)
INFO - root - 2017-12-16 07:51:55.255724: step 1230, loss = 0.68, batch loss = 0.40 (47.7 examples/sec; 0.168 sec/batch; 15h:26m:49s remains)
INFO - root - 2017-12-16 07:51:56.926532: step 1240, loss = 0.71, batch loss = 0.42 (47.9 examples/sec; 0.167 sec/batch; 15h:22m:02s remains)
INFO - root - 2017-12-16 07:51:58.580258: step 1250, loss = 0.72, batch loss = 0.44 (49.2 examples/sec; 0.163 sec/batch; 14h:58m:19s remains)
INFO - root - 2017-12-16 07:52:00.253956: step 1260, loss = 0.70, batch loss = 0.42 (46.1 examples/sec; 0.173 sec/batch; 15h:57m:33s remains)
INFO - root - 2017-12-16 07:52:01.928000: step 1270, loss = 0.74, batch loss = 0.46 (48.3 examples/sec; 0.166 sec/batch; 15h:14m:22s remains)
INFO - root - 2017-12-16 07:52:03.608593: step 1280, loss = 0.62, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 15h:10m:51s remains)
INFO - root - 2017-12-16 07:52:05.264762: step 1290, loss = 0.69, batch loss = 0.41 (48.2 examples/sec; 0.166 sec/batch; 15h:15m:59s remains)
INFO - root - 2017-12-16 07:52:06.943303: step 1300, loss = 0.64, batch loss = 0.36 (47.5 examples/sec; 0.169 sec/batch; 15h:30m:16s remains)
2017-12-16 07:52:07.466117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1836963 -1.0545813 -1.1151237 -1.2739635 -1.4593332 -1.4619348 -1.3994589 -1.3229564 -1.2273029 -1.1562943 -1.0020132 -0.81760794 -0.68096268 -0.68885422 -0.78448468][-1.1957185 -1.2303967 -1.2847807 -1.3098409 -1.3410933 -1.1835158 -0.90442115 -0.69170159 -0.58185565 -0.64882785 -0.7262615 -0.70149761 -0.649292 -0.63622314 -0.74742246][-1.2626384 -1.3523333 -1.2606428 -1.0546877 -0.88432723 -0.57942438 -0.22590709 0.014618039 0.034865379 -0.19730157 -0.51151919 -0.71134746 -0.77741909 -0.76528293 -0.81941235][-1.3808951 -1.2989521 -0.84454316 -0.3589775 -0.074678361 -0.030513942 0.040678144 0.18046248 0.20161533 -0.06553936 -0.49310818 -0.84616363 -0.9900912 -0.965781 -0.92867523][-1.4317629 -1.0595593 -0.24968982 0.46000588 0.81835556 0.46811247 0.063248038 -0.14922667 -0.30345201 -0.437028 -0.75389373 -1.0569857 -1.2161416 -1.1946135 -1.1051561][-1.3461136 -0.72877085 0.34258842 1.3390689 1.7511773 1.1019747 0.29445481 -0.2439543 -0.60519481 -0.8215211 -1.067188 -1.2437116 -1.3533634 -1.3197892 -1.2162303][-1.2133887 -0.81797695 0.019046426 1.1125753 1.8395441 1.7395709 1.2024589 0.45353281 -0.3485105 -0.87348354 -1.1128439 -1.1559048 -1.1858147 -1.1557014 -1.125272][-1.1904991 -1.1590416 -0.64035416 0.3800894 1.6416061 2.6273038 2.9120178 2.02393 0.49142754 -0.55725461 -1.0043901 -1.028493 -0.96402931 -0.93511873 -1.0144241][-1.2677646 -1.5899709 -1.462677 -0.734808 0.60051322 1.9261339 2.5948641 1.9445894 0.51884854 -0.58358896 -1.0556573 -1.0321851 -0.85910475 -0.82510453 -0.93056071][-1.1934893 -1.4776487 -1.5555382 -1.171425 -0.37650877 0.51183796 1.0995893 0.80593681 -0.15289086 -0.95476991 -1.1894248 -1.0025066 -0.74981582 -0.672168 -0.76659828][-0.9212693 -0.90743637 -0.91159332 -0.865943 -0.71237826 -0.36221391 -0.067652524 -0.26205868 -0.81256741 -1.1562778 -1.0991732 -0.73023069 -0.49162802 -0.49916071 -0.63239276][-0.716375 -0.23239839 -0.0039942265 -0.17025191 -0.5286237 -0.81664485 -1.031437 -1.2515323 -1.3752172 -1.2184682 -0.8823598 -0.52500719 -0.34343261 -0.33770549 -0.43714911][-0.46098274 0.252272 0.68038726 0.47503173 -0.21157032 -0.87665832 -1.3771479 -1.598013 -1.5244868 -1.2554455 -0.91557133 -0.6525839 -0.55436409 -0.54501659 -0.59695303][-0.15517426 0.468125 0.89172769 0.64224279 -0.16595519 -0.979548 -1.4569442 -1.5841181 -1.464402 -1.2249917 -1.0263768 -0.93824106 -0.9477073 -0.97483188 -0.97437453][-0.067372322 0.34864342 0.65926254 0.40252995 -0.29339278 -0.9462049 -1.2548921 -1.266093 -1.1560261 -1.0279275 -1.0201025 -1.1029227 -1.2224278 -1.2541496 -1.2046574]]...]
INFO - root - 2017-12-16 07:52:09.129167: step 1310, loss = 0.70, batch loss = 0.42 (48.6 examples/sec; 0.165 sec/batch; 15h:08m:04s remains)
INFO - root - 2017-12-16 07:52:10.817436: step 1320, loss = 0.71, batch loss = 0.42 (48.3 examples/sec; 0.166 sec/batch; 15h:14m:14s remains)
INFO - root - 2017-12-16 07:52:12.494143: step 1330, loss = 0.70, batch loss = 0.42 (48.6 examples/sec; 0.165 sec/batch; 15h:08m:21s remains)
INFO - root - 2017-12-16 07:52:14.150155: step 1340, loss = 0.69, batch loss = 0.41 (48.9 examples/sec; 0.164 sec/batch; 15h:03m:50s remains)
INFO - root - 2017-12-16 07:52:15.803399: step 1350, loss = 0.73, batch loss = 0.45 (49.2 examples/sec; 0.163 sec/batch; 14h:57m:58s remains)
INFO - root - 2017-12-16 07:52:17.453276: step 1360, loss = 0.79, batch loss = 0.51 (47.4 examples/sec; 0.169 sec/batch; 15h:31m:01s remains)
INFO - root - 2017-12-16 07:52:19.137295: step 1370, loss = 0.85, batch loss = 0.57 (46.0 examples/sec; 0.174 sec/batch; 16h:00m:17s remains)
INFO - root - 2017-12-16 07:52:20.788315: step 1380, loss = 0.72, batch loss = 0.44 (47.6 examples/sec; 0.168 sec/batch; 15h:27m:16s remains)
INFO - root - 2017-12-16 07:52:22.431918: step 1390, loss = 0.70, batch loss = 0.41 (49.1 examples/sec; 0.163 sec/batch; 14h:59m:21s remains)
INFO - root - 2017-12-16 07:52:24.083929: step 1400, loss = 0.75, batch loss = 0.47 (47.8 examples/sec; 0.167 sec/batch; 15h:23m:31s remains)
2017-12-16 07:52:24.581293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.676595 -1.8705275 -1.8647176 -1.7096258 -1.5460625 -1.5958416 -1.7203494 -1.8267753 -1.8472292 -1.8045101 -1.7379358 -1.7632155 -2.0593722 -2.506983 -2.751188][-1.7150147 -1.8464236 -1.9785419 -1.9632158 -1.9064596 -1.9035754 -1.8701212 -1.7710335 -1.5728238 -1.4008232 -1.3098518 -1.3607624 -1.7132854 -2.2484159 -2.5600646][-1.4828342 -1.3662049 -1.3981605 -1.4766569 -1.5868244 -1.5932662 -1.3952458 -1.1399872 -0.7901696 -0.4636243 -0.36985195 -0.43330842 -0.86566341 -1.4404504 -1.7235603][-0.90587282 -0.58913076 -0.55714023 -0.69560069 -0.88761693 -1.0387679 -0.91844451 -0.58305162 -0.18581235 0.094147146 0.16275233 0.17438358 -0.25269687 -0.74807131 -0.97733241][-0.28467774 0.13244933 0.23672408 0.1003024 -0.10687506 -0.33893257 -0.41838133 -0.32136482 -0.15102768 -0.050402641 -0.028073907 0.051620781 -0.2974571 -0.70959044 -0.92491972][0.35965008 0.717428 0.90842813 0.93430811 0.91508788 0.70672625 0.47333592 0.34086984 0.24114674 0.11517936 0.071840107 0.044697225 -0.23551899 -0.54132247 -0.80477464][0.45069891 0.81632227 1.1706996 1.5585065 1.9664431 2.0063462 1.8344531 1.6263208 1.4386885 1.2781503 1.2748492 1.2802734 0.92486173 0.38130242 -0.015455902][0.083777845 0.30923873 0.61480623 1.1483166 1.8533719 2.3952141 2.6972291 2.7840831 2.6908038 2.643523 2.8737447 3.1131763 2.6905415 1.860913 1.1651514][-0.18756437 -0.086678028 0.073573053 0.39413029 0.90519279 1.4510992 2.0141907 2.3461411 2.4209545 2.5194786 2.9662237 3.3737702 3.2375512 2.5959098 1.8666461][0.26555103 0.34011704 0.31233352 0.37265962 0.55692869 0.65296704 0.75041062 0.78589982 0.75849611 0.89028674 1.3022914 1.7265913 1.8526661 1.7243679 1.2512136][1.3513157 1.3334396 1.2495594 1.1995523 1.0591214 0.70663148 0.24841279 -0.2491191 -0.61382246 -0.73711288 -0.54579473 -0.20631158 0.058420122 0.13096851 -0.053575277][2.0291998 1.9988585 2.029731 2.0633397 1.7975683 1.2719395 0.55178362 -0.24979675 -0.9573946 -1.3887584 -1.521872 -1.4232624 -1.2291377 -1.1318806 -1.1377466][1.6777785 1.6139493 1.6239598 1.6546307 1.5714769 1.2452438 0.6233936 -0.15606409 -0.85274488 -1.3661582 -1.6611741 -1.7190266 -1.6491429 -1.602771 -1.5916424][0.27520806 0.14735347 0.16792411 0.29259342 0.44334537 0.41907781 0.14985472 -0.3093906 -0.78762275 -1.1579131 -1.4142628 -1.5658884 -1.6567519 -1.7302399 -1.7824707][-1.0661505 -1.1622736 -1.1214188 -0.97557706 -0.78216928 -0.66696262 -0.68091178 -0.828885 -1.0426844 -1.2188163 -1.3970945 -1.5326442 -1.658149 -1.7593108 -1.8423128]]...]
INFO - root - 2017-12-16 07:52:26.293530: step 1410, loss = 0.66, batch loss = 0.37 (49.4 examples/sec; 0.162 sec/batch; 14h:53m:40s remains)
INFO - root - 2017-12-16 07:52:27.993682: step 1420, loss = 0.64, batch loss = 0.36 (48.5 examples/sec; 0.165 sec/batch; 15h:10m:42s remains)
INFO - root - 2017-12-16 07:52:29.635278: step 1430, loss = 0.66, batch loss = 0.38 (49.0 examples/sec; 0.163 sec/batch; 15h:01m:25s remains)
INFO - root - 2017-12-16 07:52:31.285309: step 1440, loss = 0.69, batch loss = 0.41 (49.7 examples/sec; 0.161 sec/batch; 14h:48m:58s remains)
INFO - root - 2017-12-16 07:52:32.937095: step 1450, loss = 0.72, batch loss = 0.44 (47.4 examples/sec; 0.169 sec/batch; 15h:30m:30s remains)
INFO - root - 2017-12-16 07:52:34.642801: step 1460, loss = 0.69, batch loss = 0.41 (45.8 examples/sec; 0.175 sec/batch; 16h:03m:17s remains)
INFO - root - 2017-12-16 07:52:36.350482: step 1470, loss = 0.82, batch loss = 0.54 (44.5 examples/sec; 0.180 sec/batch; 16h:31m:26s remains)
INFO - root - 2017-12-16 07:52:38.090228: step 1480, loss = 0.70, batch loss = 0.42 (47.0 examples/sec; 0.170 sec/batch; 15h:39m:25s remains)
INFO - root - 2017-12-16 07:52:39.764015: step 1490, loss = 0.65, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 15h:03m:40s remains)
INFO - root - 2017-12-16 07:52:41.398798: step 1500, loss = 0.85, batch loss = 0.57 (47.9 examples/sec; 0.167 sec/batch; 15h:21m:30s remains)
2017-12-16 07:52:41.881220: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48337471 0.24158955 -0.025254965 -0.21772069 -0.37585253 -0.53249073 -0.55503273 -0.44753313 -0.14746588 0.34060907 0.85956609 1.1657146 1.0656465 0.56650913 -0.063839614][0.51696396 0.26190197 0.069609761 -0.063321471 -0.3096332 -0.53504455 -0.64624077 -0.46422911 -0.090576231 0.3518312 0.64818478 0.6743449 0.49273062 0.10910892 -0.22926301][0.18227434 0.0055139065 0.0014036894 -0.003020525 -0.15208256 -0.39102817 -0.57641762 -0.41466844 0.047998071 0.40885639 0.53264987 0.43309951 0.24943483 0.10388803 0.10319114][-0.20387489 -0.22519547 0.023416877 0.16541672 0.13117421 -0.12651253 -0.3731811 -0.21857589 0.33972979 0.79415059 0.82326686 0.7193464 0.59263396 0.43949413 0.43379641][-0.4100967 -0.26600552 0.082055688 0.26376462 0.18111932 -0.070904016 -0.16655505 0.11779082 0.71075976 1.2923292 1.4008037 1.2904378 1.0249625 0.6727879 0.47936714][-0.405972 -0.29455525 -0.086608112 0.011022091 -0.07861501 -0.047926962 0.24142814 0.72205281 1.2573935 1.6960374 1.7860738 1.5179909 1.063769 0.60736465 0.25220549][-0.42176723 -0.45294583 -0.39468604 -0.27878404 -0.047501087 0.51495695 1.4177197 2.1421385 2.2814674 2.0852633 1.6646014 1.17197 0.67241049 0.25896084 -0.10499746][-0.439614 -0.41015714 -0.32355517 -0.10649395 0.39305091 1.4126865 2.8900509 3.6969767 3.0215321 2.0089178 1.141704 0.42603731 -0.050189018 -0.32901186 -0.54918176][-0.188416 -0.017745733 0.22894895 0.55671203 1.151822 2.1051521 3.1340151 3.337276 2.4503956 1.2740802 0.30545652 -0.37731349 -0.73486722 -0.84431404 -0.9392696][0.23800313 0.67819917 1.0276674 1.3592693 1.7528471 2.1697245 2.2423282 1.8373424 1.0406734 0.15909755 -0.52704704 -0.92304486 -1.0808709 -1.1416537 -1.2149035][0.71913993 1.399119 1.8080307 1.8544139 1.7604476 1.5447427 1.0584337 0.45958805 -0.20301253 -0.7518369 -1.0480112 -1.1469898 -1.1982083 -1.2953668 -1.3473859][1.2176303 1.8763009 2.0965862 1.8180338 1.308498 0.72155631 0.028597116 -0.5626049 -0.97989535 -1.2087495 -1.2706552 -1.2703063 -1.3265626 -1.4115641 -1.3931179][1.7047826 1.9268447 1.705187 1.1885372 0.54251945 -0.087740004 -0.65712297 -1.061814 -1.2406683 -1.2745073 -1.2730591 -1.3350585 -1.4289757 -1.4764854 -1.3478343][1.8937172 1.5955795 1.0208122 0.36188889 -0.22027123 -0.70886552 -1.0415976 -1.2528113 -1.322313 -1.2970736 -1.3152242 -1.4162127 -1.4931208 -1.4487797 -1.1540899][1.6248003 1.0334307 0.35025835 -0.294622 -0.761855 -1.0574993 -1.2051383 -1.2906889 -1.3293122 -1.3249114 -1.398266 -1.5161214 -1.5378292 -1.3145003 -0.85392356]]...]
INFO - root - 2017-12-16 07:52:43.539978: step 1510, loss = 0.81, batch loss = 0.53 (48.9 examples/sec; 0.164 sec/batch; 15h:03m:13s remains)
INFO - root - 2017-12-16 07:52:45.205363: step 1520, loss = 0.72, batch loss = 0.43 (47.6 examples/sec; 0.168 sec/batch; 15h:27m:43s remains)
INFO - root - 2017-12-16 07:52:46.842563: step 1530, loss = 0.70, batch loss = 0.42 (49.1 examples/sec; 0.163 sec/batch; 14h:59m:03s remains)
INFO - root - 2017-12-16 07:52:48.504473: step 1540, loss = 0.68, batch loss = 0.39 (48.5 examples/sec; 0.165 sec/batch; 15h:10m:22s remains)
INFO - root - 2017-12-16 07:52:50.164711: step 1550, loss = 0.68, batch loss = 0.40 (47.8 examples/sec; 0.167 sec/batch; 15h:23m:15s remains)
INFO - root - 2017-12-16 07:52:51.856137: step 1560, loss = 0.78, batch loss = 0.50 (47.0 examples/sec; 0.170 sec/batch; 15h:38m:54s remains)
INFO - root - 2017-12-16 07:52:53.552701: step 1570, loss = 0.80, batch loss = 0.52 (45.4 examples/sec; 0.176 sec/batch; 16h:11m:54s remains)
INFO - root - 2017-12-16 07:52:55.233416: step 1580, loss = 0.66, batch loss = 0.38 (46.1 examples/sec; 0.174 sec/batch; 15h:57m:34s remains)
INFO - root - 2017-12-16 07:52:56.900450: step 1590, loss = 0.74, batch loss = 0.45 (47.5 examples/sec; 0.168 sec/batch; 15h:28m:01s remains)
INFO - root - 2017-12-16 07:52:58.563708: step 1600, loss = 0.73, batch loss = 0.45 (47.0 examples/sec; 0.170 sec/batch; 15h:37m:48s remains)
2017-12-16 07:52:59.070510: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50553513 0.39014912 0.55463386 0.90475261 1.2961272 1.433822 1.1003567 0.9313345 1.2543179 1.3991932 0.96001947 0.4373312 0.0880332 -0.12017149 -0.10304928][0.43785262 0.39002073 0.39752913 0.41792667 0.58088994 0.6119206 0.29793012 0.18454218 0.48467374 0.7718966 0.6382941 0.27956021 -0.10686827 -0.47718203 -0.58655715][0.11883736 0.08221519 -0.10191965 -0.36572075 -0.35194224 -0.31681573 -0.54333818 -0.65272409 -0.42659742 -0.074019372 0.032763362 -0.10753649 -0.43692237 -0.87898332 -1.0719181][-0.23211527 -0.24579507 -0.45322114 -0.83836031 -0.96747196 -0.95433426 -1.0386424 -1.0615363 -0.85409468 -0.53550506 -0.3203209 -0.29523879 -0.56233162 -1.0074307 -1.3273904][-0.33434635 -0.36461145 -0.56123388 -0.88814938 -1.1222948 -1.2079737 -1.1992253 -1.1226851 -0.95028007 -0.65101874 -0.35975212 -0.15538657 -0.32523417 -0.76731 -1.1656163][-0.58020055 -0.59637374 -0.67810023 -0.85713726 -1.0491 -1.1420296 -1.1581948 -1.0449668 -0.86330652 -0.65243846 -0.43943751 -0.24483961 -0.2409091 -0.46089822 -0.75495535][-0.75123858 -0.75534 -0.73211473 -0.74094868 -0.75937045 -0.79030854 -0.79853773 -0.80601954 -0.768901 -0.798652 -0.805912 -0.69058943 -0.51950961 -0.42436987 -0.46991313][-0.74506789 -0.7192238 -0.63257897 -0.4630177 -0.28313673 -0.12743115 -0.14172262 -0.43257082 -0.72186732 -0.97147405 -1.1480132 -1.1380222 -0.90516788 -0.57631278 -0.40237671][-0.49430948 -0.45503724 -0.40189117 -0.19032168 0.10357463 0.36572111 0.28616273 -0.21419656 -0.7194261 -1.0516949 -1.2472667 -1.3165245 -1.1485102 -0.8122139 -0.48937941][-0.20449501 -0.20315236 -0.18598479 -0.049003124 0.14830947 0.26580334 0.097165465 -0.38616723 -0.83685392 -1.0861523 -1.2150848 -1.2919283 -1.1767763 -0.88768983 -0.62582207][0.059471488 0.0308007 -0.023893237 -0.033254385 0.01331234 -0.04668653 -0.26974404 -0.615064 -0.85011977 -0.96790743 -1.0575566 -1.1064265 -0.99712634 -0.78753996 -0.68381971][0.2847954 0.25255251 0.13792515 -0.035009384 -0.17411786 -0.34384245 -0.50493371 -0.63143528 -0.66495109 -0.6791687 -0.77688646 -0.82644403 -0.75033551 -0.635908 -0.73297507][0.33347964 0.35414779 0.25979531 0.023961544 -0.26386887 -0.52000809 -0.61889362 -0.55114162 -0.45425892 -0.47408521 -0.59789753 -0.69156015 -0.73385155 -0.76553226 -0.95728439][0.25599539 0.26879239 0.22475684 0.039562583 -0.34080398 -0.69492531 -0.83233023 -0.68116063 -0.49478579 -0.47821534 -0.56024259 -0.65454483 -0.74246681 -0.83697069 -1.034242][0.13968086 0.16557324 0.11998892 -0.070157111 -0.52807891 -0.97626185 -1.0899664 -0.87755835 -0.60321212 -0.51177186 -0.53643656 -0.61144346 -0.71477669 -0.80704474 -0.95261991]]...]
INFO - root - 2017-12-16 07:53:00.718401: step 1610, loss = 0.61, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 15h:10m:13s remains)
INFO - root - 2017-12-16 07:53:02.410178: step 1620, loss = 0.69, batch loss = 0.41 (45.8 examples/sec; 0.175 sec/batch; 16h:02m:22s remains)
INFO - root - 2017-12-16 07:53:04.064656: step 1630, loss = 0.62, batch loss = 0.34 (48.8 examples/sec; 0.164 sec/batch; 15h:03m:47s remains)
INFO - root - 2017-12-16 07:53:05.736126: step 1640, loss = 0.66, batch loss = 0.38 (47.8 examples/sec; 0.168 sec/batch; 15h:23m:45s remains)
INFO - root - 2017-12-16 07:53:07.414249: step 1650, loss = 0.67, batch loss = 0.39 (47.4 examples/sec; 0.169 sec/batch; 15h:29m:53s remains)
INFO - root - 2017-12-16 07:53:09.092901: step 1660, loss = 0.78, batch loss = 0.50 (47.3 examples/sec; 0.169 sec/batch; 15h:32m:12s remains)
INFO - root - 2017-12-16 07:53:10.756924: step 1670, loss = 0.62, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 15h:07m:12s remains)
INFO - root - 2017-12-16 07:53:12.428078: step 1680, loss = 0.64, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 15h:31m:45s remains)
INFO - root - 2017-12-16 07:53:14.139853: step 1690, loss = 0.70, batch loss = 0.42 (44.9 examples/sec; 0.178 sec/batch; 16h:22m:32s remains)
INFO - root - 2017-12-16 07:53:15.853972: step 1700, loss = 0.76, batch loss = 0.48 (46.1 examples/sec; 0.174 sec/batch; 15h:57m:29s remains)
2017-12-16 07:53:16.365568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9487524 -2.0980527 -1.7081954 -0.72915661 0.4204632 0.87872958 0.25955451 -0.68503004 -1.3013215 -1.278978 -0.935982 -1.0501401 -1.3094838 -1.5200312 -1.4800442][-2.0868053 -2.1514716 -1.8041575 -0.95142674 0.0743593 0.64261651 0.15581036 -0.52528477 -0.90306938 -0.8321805 -0.64249647 -0.97314119 -1.462703 -1.8199613 -1.8565081][-2.1676836 -2.1629667 -1.8407758 -1.1325773 -0.20768183 0.47838509 0.19617367 -0.22146159 -0.35785449 -0.32267708 -0.40442276 -1.0542294 -1.7531826 -2.1892974 -2.1947222][-2.1707821 -2.076112 -1.8001065 -1.156137 -0.37145537 0.37651253 0.44568324 0.28079712 0.28766942 0.19821596 -0.28072351 -1.211122 -2.0550144 -2.4474378 -2.3670645][-2.0607 -1.8421574 -1.5649946 -0.98384506 -0.31874537 0.44595563 0.94708443 1.0333261 1.0266244 0.69185829 -0.16887885 -1.3591009 -2.2556205 -2.5738788 -2.4325457][-1.7558941 -1.4239428 -1.1513876 -0.74677551 -0.21746546 0.60951006 1.5734766 2.0267329 1.84849 1.1765649 0.01405406 -1.3563772 -2.3180556 -2.5890684 -2.3994088][-1.2642175 -0.845404 -0.70244914 -0.56170535 -0.12799776 0.8225677 2.1956387 3.1423254 2.7906106 1.8038592 0.27688956 -1.2443547 -2.2687147 -2.529515 -2.3273356][-0.68600935 -0.21212465 -0.28956121 -0.39305657 -0.10550898 0.9944036 2.6871297 4.1050043 3.7124057 2.4399457 0.61348665 -1.0786479 -2.1166763 -2.4194975 -2.2489932][-0.17459333 0.26822186 -0.010343432 -0.39615828 -0.22032899 0.89522672 2.7244649 4.5098977 4.2628751 2.9171395 0.90494132 -0.9027884 -1.9597675 -2.3107297 -2.1743665][0.14078295 0.44073689 -0.026617527 -0.56134576 -0.48124415 0.59121621 2.3185856 4.0755382 4.1320086 2.9489341 0.98153162 -0.79555333 -1.8147465 -2.1818101 -2.0698206][0.16697562 0.283051 -0.34842205 -0.92006707 -0.88511431 0.11009276 1.6810131 3.1699705 3.4170361 2.4250796 0.7188623 -0.86690158 -1.7209064 -2.0129042 -1.9327245][-0.082901716 -0.089230418 -0.74971044 -1.2888154 -1.2369578 -0.34011757 0.9941752 2.2015238 2.4274287 1.6356771 0.26529551 -1.014644 -1.6705954 -1.8456054 -1.7922418][-0.47447556 -0.50744522 -1.0492117 -1.5163198 -1.509552 -0.81226981 0.22654557 1.1807206 1.3837311 0.76228821 -0.24130547 -1.1713376 -1.6272086 -1.7243401 -1.6938238][-0.88225067 -0.88385618 -1.2437762 -1.6012712 -1.6925651 -1.2839037 -0.53588885 0.18984163 0.3888936 -0.019775271 -0.72297728 -1.3197844 -1.6011686 -1.6621094 -1.6408014][-1.2378113 -1.1991626 -1.3640038 -1.5951247 -1.7233603 -1.5545688 -1.1403662 -0.64316154 -0.43230546 -0.615231 -1.0226821 -1.3782074 -1.5585833 -1.6075234 -1.5973859]]...]
INFO - root - 2017-12-16 07:53:18.038062: step 1710, loss = 0.61, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 15h:32m:42s remains)
INFO - root - 2017-12-16 07:53:19.739518: step 1720, loss = 0.67, batch loss = 0.39 (46.1 examples/sec; 0.174 sec/batch; 15h:56m:44s remains)
INFO - root - 2017-12-16 07:53:21.433832: step 1730, loss = 0.71, batch loss = 0.43 (46.8 examples/sec; 0.171 sec/batch; 15h:41m:24s remains)
INFO - root - 2017-12-16 07:53:23.134053: step 1740, loss = 0.75, batch loss = 0.47 (46.1 examples/sec; 0.174 sec/batch; 15h:56m:44s remains)
INFO - root - 2017-12-16 07:53:24.819474: step 1750, loss = 0.67, batch loss = 0.39 (45.3 examples/sec; 0.176 sec/batch; 16h:12m:30s remains)
INFO - root - 2017-12-16 07:53:26.517670: step 1760, loss = 0.68, batch loss = 0.39 (49.0 examples/sec; 0.163 sec/batch; 14h:59m:28s remains)
INFO - root - 2017-12-16 07:53:28.191746: step 1770, loss = 0.74, batch loss = 0.46 (47.5 examples/sec; 0.168 sec/batch; 15h:28m:10s remains)
INFO - root - 2017-12-16 07:53:29.889344: step 1780, loss = 0.65, batch loss = 0.37 (47.5 examples/sec; 0.168 sec/batch; 15h:28m:24s remains)
INFO - root - 2017-12-16 07:53:31.558595: step 1790, loss = 0.66, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 15h:12m:19s remains)
INFO - root - 2017-12-16 07:53:33.218591: step 1800, loss = 0.66, batch loss = 0.38 (49.1 examples/sec; 0.163 sec/batch; 14h:57m:23s remains)
2017-12-16 07:53:33.666485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9255865 -3.0604043 -3.0598838 -3.0100498 -2.9534817 -2.9197788 -2.8526068 -2.7317843 -2.6264102 -2.5572784 -2.5237412 -2.4137182 -2.1712484 -1.8775821 -1.6410136][-3.3323188 -3.4026475 -3.432333 -3.4538231 -3.4751244 -3.4515409 -3.340693 -3.1346216 -2.909533 -2.823709 -2.9060738 -3.0177717 -3.0104177 -2.8884535 -2.7852087][-3.0746856 -3.1225038 -3.1962481 -3.2870865 -3.3356476 -3.2522726 -2.9482703 -2.4704905 -2.0513892 -1.9716116 -2.3285315 -2.8144059 -3.1350651 -3.2239366 -3.2318058][-2.610934 -2.683403 -2.8037953 -2.9174657 -2.903681 -2.5935295 -1.9027183 -0.98262858 -0.27489716 -0.31725347 -1.1773432 -2.2227182 -2.921123 -3.1632085 -3.1613135][-2.1936975 -2.3136191 -2.4759769 -2.5473957 -2.3241742 -1.6377894 -0.39134711 1.0199524 1.8482267 1.3581282 -0.19605541 -1.7916932 -2.7331717 -2.9543552 -2.7750468][-1.7972651 -1.9344765 -2.0637968 -1.9995377 -1.4901748 -0.30671591 1.5354325 3.3335266 3.7901034 2.3488755 0.093497634 -1.7685004 -2.6487317 -2.6079907 -2.1491721][-1.2022729 -1.2863908 -1.354979 -1.1612693 -0.40065998 1.2081679 3.5956817 5.4247022 4.7619672 2.2032032 -0.39294827 -2.1097298 -2.5528054 -1.9936352 -1.1752168][-0.3168577 -0.28846055 -0.31402624 -0.15997785 0.6213094 2.3930774 4.8393335 5.8534479 3.9473753 1.0552107 -1.3382192 -2.4937077 -2.1775107 -0.9835676 0.16122341][0.40100324 0.53585827 0.46605349 0.43704844 0.9901756 2.4896684 4.0654306 3.9190021 1.894838 -0.61838925 -2.3486037 -2.6676292 -1.580759 0.14570665 1.3406776][0.52238727 0.69208157 0.53373051 0.27206159 0.49275279 1.4154614 2.0768986 1.3986992 -0.38407469 -2.1926827 -3.124197 -2.6744027 -1.041724 0.77210581 1.5772368][0.14614439 0.29300785 0.020364523 -0.46467364 -0.56003696 -0.19707763 -0.12363863 -0.945912 -2.2726259 -3.3158855 -3.5179234 -2.6129684 -0.9347412 0.54532182 0.83822882][-0.43030798 -0.38593161 -0.81726491 -1.3991455 -1.6908768 -1.7139301 -1.9084964 -2.5431752 -3.3293843 -3.7748232 -3.5405207 -2.5502262 -1.2100304 -0.29687506 -0.38518494][-0.83509982 -0.91208732 -1.4194348 -2.0460181 -2.4650702 -2.6235867 -2.8338552 -3.1755323 -3.558639 -3.6461701 -3.2650414 -2.4950917 -1.7318164 -1.3639399 -1.6563458][-0.81898963 -0.9961496 -1.5123191 -2.1371059 -2.5726135 -2.7831545 -2.9109936 -3.0088491 -3.1324334 -3.0961421 -2.8213084 -2.4166021 -2.1118324 -2.0484152 -2.3372121][-0.571369 -0.88396025 -1.4053695 -1.9084311 -2.2146347 -2.3226953 -2.3332934 -2.3080835 -2.3294361 -2.281317 -2.1662359 -2.0242066 -1.9670341 -2.0146332 -2.1857708]]...]
INFO - root - 2017-12-16 07:53:35.319105: step 1810, loss = 0.71, batch loss = 0.43 (49.3 examples/sec; 0.162 sec/batch; 14h:54m:08s remains)
INFO - root - 2017-12-16 07:53:36.999796: step 1820, loss = 0.69, batch loss = 0.41 (47.8 examples/sec; 0.167 sec/batch; 15h:22m:01s remains)
INFO - root - 2017-12-16 07:53:38.689082: step 1830, loss = 0.68, batch loss = 0.40 (47.7 examples/sec; 0.168 sec/batch; 15h:24m:48s remains)
INFO - root - 2017-12-16 07:53:40.350734: step 1840, loss = 0.83, batch loss = 0.55 (48.6 examples/sec; 0.165 sec/batch; 15h:08m:05s remains)
INFO - root - 2017-12-16 07:53:42.026081: step 1850, loss = 0.76, batch loss = 0.48 (47.5 examples/sec; 0.168 sec/batch; 15h:28m:04s remains)
INFO - root - 2017-12-16 07:53:43.732241: step 1860, loss = 0.68, batch loss = 0.40 (47.9 examples/sec; 0.167 sec/batch; 15h:19m:29s remains)
INFO - root - 2017-12-16 07:53:45.421413: step 1870, loss = 0.63, batch loss = 0.35 (47.1 examples/sec; 0.170 sec/batch; 15h:36m:54s remains)
INFO - root - 2017-12-16 07:53:47.111690: step 1880, loss = 0.80, batch loss = 0.52 (49.1 examples/sec; 0.163 sec/batch; 14h:57m:00s remains)
INFO - root - 2017-12-16 07:53:48.766821: step 1890, loss = 0.68, batch loss = 0.40 (48.4 examples/sec; 0.165 sec/batch; 15h:11m:04s remains)
INFO - root - 2017-12-16 07:53:50.387332: step 1900, loss = 0.71, batch loss = 0.43 (49.8 examples/sec; 0.161 sec/batch; 14h:45m:25s remains)
2017-12-16 07:53:50.876136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.732878 -1.7894692 -1.8238137 -1.8393881 -1.8467431 -1.8523746 -1.8581827 -1.8650739 -1.8976294 -1.9722698 -2.1165285 -2.3153167 -2.3911047 -2.31111 -2.1016045][-2.047703 -2.1599593 -2.2314305 -2.259675 -2.2688246 -2.2692778 -2.2693262 -2.2677288 -2.2962427 -2.3816071 -2.5623913 -2.7865753 -2.8592067 -2.7624917 -2.4956536][-2.1628852 -2.31016 -2.4196529 -2.48724 -2.5296698 -2.5512395 -2.5608137 -2.5742614 -2.6161411 -2.7214117 -2.9291553 -3.1633639 -3.2499886 -3.1568027 -2.8706243][-1.9310673 -2.0237482 -2.1046507 -2.1891212 -2.2708588 -2.3421037 -2.4072793 -2.4735529 -2.5511618 -2.6848106 -2.9336677 -3.2165947 -3.3492303 -3.298367 -3.0471683][-1.1845101 -1.1226488 -1.0850565 -1.1213967 -1.1920189 -1.2704254 -1.361057 -1.4557527 -1.5839715 -1.7915933 -2.1434958 -2.5537143 -2.8450027 -2.9380078 -2.8171065][0.30552673 0.6038506 0.79287863 0.82345581 0.78560948 0.71663308 0.62134981 0.52761662 0.39350927 0.12511516 -0.36716676 -0.99540466 -1.5399832 -1.9116344 -2.0810163][2.2580638 2.8490281 3.2256126 3.3596869 3.3708057 3.326458 3.2416821 3.1551085 2.9860187 2.6599936 2.0121708 1.1683635 0.34380043 -0.36892468 -0.9189325][3.2354717 3.9147987 4.3508949 4.5578041 4.671505 4.7805338 4.9013543 5.0473194 4.9957461 4.7340755 4.0972204 3.1246347 2.0673513 1.0375546 0.14609718][2.1175513 2.5366707 2.7100592 2.7149186 2.6782346 2.7092595 2.8308845 3.0333605 3.2274289 3.3213439 3.1104646 2.5297871 1.78576 0.94663155 0.14932334][0.076816559 0.25988126 0.28813815 0.22100651 0.13594055 0.12815058 0.201599 0.35388327 0.54777265 0.698079 0.65659094 0.3956027 0.025246978 -0.4012301 -0.80489039][-1.6157415 -1.5764825 -1.5907629 -1.6683779 -1.7338837 -1.7346714 -1.6852274 -1.577934 -1.4367814 -1.3358564 -1.3254144 -1.4303333 -1.5714909 -1.6819909 -1.7594539][-2.2760665 -2.2972755 -2.3195522 -2.3843722 -2.4320154 -2.4337549 -2.4101353 -2.3664842 -2.3110557 -2.2881894 -2.2958231 -2.3339875 -2.3618042 -2.3177404 -2.2152295][-2.3136635 -2.3496232 -2.3675218 -2.3978884 -2.4154477 -2.4152224 -2.4168646 -2.4177294 -2.4372144 -2.4722898 -2.5047569 -2.5006595 -2.4570684 -2.3459177 -2.1796963][-1.9849288 -2.0123019 -2.012764 -2.0170517 -2.0257673 -2.0304952 -2.0408747 -2.0549147 -2.0830395 -2.1223085 -2.1490631 -2.1424317 -2.0911274 -1.9997158 -1.8719729][-1.6305976 -1.647119 -1.6468748 -1.6448197 -1.6460773 -1.6477488 -1.6499907 -1.6587429 -1.6762421 -1.701498 -1.7164507 -1.7123604 -1.6875622 -1.6434233 -1.582408]]...]
INFO - root - 2017-12-16 07:53:52.554361: step 1910, loss = 0.75, batch loss = 0.47 (46.8 examples/sec; 0.171 sec/batch; 15h:42m:43s remains)
INFO - root - 2017-12-16 07:53:54.258345: step 1920, loss = 0.73, batch loss = 0.45 (47.8 examples/sec; 0.167 sec/batch; 15h:22m:17s remains)
INFO - root - 2017-12-16 07:53:55.934033: step 1930, loss = 0.64, batch loss = 0.36 (48.3 examples/sec; 0.166 sec/batch; 15h:13m:19s remains)
INFO - root - 2017-12-16 07:53:57.592646: step 1940, loss = 0.64, batch loss = 0.36 (48.2 examples/sec; 0.166 sec/batch; 15h:13m:57s remains)
INFO - root - 2017-12-16 07:53:59.284381: step 1950, loss = 0.82, batch loss = 0.54 (48.3 examples/sec; 0.166 sec/batch; 15h:12m:20s remains)
INFO - root - 2017-12-16 07:54:00.940580: step 1960, loss = 0.61, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:30s remains)
INFO - root - 2017-12-16 07:54:02.616464: step 1970, loss = 0.62, batch loss = 0.34 (48.3 examples/sec; 0.166 sec/batch; 15h:12m:38s remains)
INFO - root - 2017-12-16 07:54:04.296583: step 1980, loss = 0.61, batch loss = 0.33 (45.9 examples/sec; 0.174 sec/batch; 16h:00m:53s remains)
INFO - root - 2017-12-16 07:54:05.939712: step 1990, loss = 0.62, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 15h:00m:52s remains)
INFO - root - 2017-12-16 07:54:07.627845: step 2000, loss = 0.60, batch loss = 0.32 (42.2 examples/sec; 0.190 sec/batch; 17h:24m:53s remains)
2017-12-16 07:54:08.155688: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.82510316 0.87951577 0.91125977 0.91200745 0.89130533 0.86279738 0.8473016 0.85271943 0.87509096 0.96988237 1.1879925 1.5251642 1.730986 1.6021224 1.0818752][0.76964808 0.89483058 0.9986769 1.0245014 0.97418129 0.85840094 0.73785686 0.66398358 0.66482973 0.80706418 1.1435891 1.66388 2.092618 2.1278458 1.6582049][-0.28651667 -0.16933751 -0.028371453 0.046193957 0.011167169 -0.13429141 -0.32345718 -0.47883987 -0.5432201 -0.41326517 -0.044772148 0.58175266 1.2101077 1.4838399 1.2636873][-1.4571846 -1.4340928 -1.3088788 -1.2000571 -1.1809542 -1.3035154 -1.4940155 -1.69356 -1.8251331 -1.8027239 -1.5231984 -0.96466708 -0.28208882 0.15120518 0.20619202][-2.0569189 -2.2052097 -2.1668649 -2.0529983 -1.9848719 -2.0076637 -2.0762129 -2.2131195 -2.3960719 -2.5241241 -2.4735878 -2.1604886 -1.6226658 -1.1381783 -0.88188183][-1.9388801 -2.2418563 -2.2676792 -2.149395 -1.9971602 -1.8585505 -1.7150822 -1.6912196 -1.863143 -2.15374 -2.4146996 -2.4831791 -2.2867608 -1.9576542 -1.649326][-1.4957019 -1.8407571 -1.8080964 -1.5234559 -1.1679449 -0.77854693 -0.34943008 -0.11290348 -0.23850405 -0.64282864 -1.2373998 -1.7796348 -2.0413795 -2.0526795 -1.8963764][-1.3068572 -1.5292273 -1.3171772 -0.79690772 -0.1776607 0.46381295 1.1598951 1.6484302 1.6076154 1.1161443 0.30325329 -0.60476667 -1.2910243 -1.6906493 -1.7904589][-1.624334 -1.7044177 -1.3558826 -0.71225011 -0.016206145 0.63566172 1.2965587 1.8574225 2.0310287 1.7390441 1.0493897 0.093341708 -0.79813075 -1.4189386 -1.7023532][-2.25651 -2.2240009 -1.8182448 -1.1704353 -0.5593822 -0.095174193 0.35243154 0.76978445 1.0100788 0.97168362 0.62065125 -0.094582677 -0.91138387 -1.5270988 -1.8001726][-2.7603271 -2.7152581 -2.4069247 -1.930115 -1.543336 -1.3286567 -1.1368086 -0.90269995 -0.64752233 -0.51466858 -0.58866692 -0.93593168 -1.4191494 -1.812042 -1.9420862][-2.8451221 -2.8940737 -2.7845154 -2.5429254 -2.3466358 -2.2812886 -2.270438 -2.1945024 -2.0089288 -1.7875191 -1.6581004 -1.6778419 -1.8197482 -1.9449458 -1.9574018][-2.47827 -2.6194229 -2.6616375 -2.6022382 -2.5241375 -2.5463171 -2.6585751 -2.7083862 -2.6157808 -2.3949504 -2.1868119 -2.012651 -1.9012349 -1.8720683 -1.8381108][-1.901268 -2.0479774 -2.124953 -2.1391449 -2.1285396 -2.1814315 -2.3334293 -2.4677491 -2.4690843 -2.3169043 -2.1040881 -1.8701321 -1.6811426 -1.6331512 -1.6378301][-1.5991004 -1.6209347 -1.6146894 -1.5830842 -1.5173912 -1.4785161 -1.5505461 -1.6920433 -1.7579753 -1.719939 -1.591827 -1.4292585 -1.2890294 -1.2544528 -1.332635]]...]
INFO - root - 2017-12-16 07:54:09.869034: step 2010, loss = 0.75, batch loss = 0.47 (45.9 examples/sec; 0.174 sec/batch; 16h:00m:13s remains)
INFO - root - 2017-12-16 07:54:11.564234: step 2020, loss = 0.66, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 15h:20m:11s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 07:54:13.252499: step 2030, loss = 0.69, batch loss = 0.41 (48.8 examples/sec; 0.164 sec/batch; 15h:02m:54s remains)
INFO - root - 2017-12-16 07:54:14.911839: step 2040, loss = 0.75, batch loss = 0.47 (48.4 examples/sec; 0.165 sec/batch; 15h:10m:06s remains)
INFO - root - 2017-12-16 07:54:16.579317: step 2050, loss = 0.64, batch loss = 0.36 (47.6 examples/sec; 0.168 sec/batch; 15h:25m:18s remains)
INFO - root - 2017-12-16 07:54:18.251613: step 2060, loss = 0.62, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:43s remains)
INFO - root - 2017-12-16 07:54:19.917135: step 2070, loss = 0.75, batch loss = 0.47 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:21s remains)
INFO - root - 2017-12-16 07:54:21.580297: step 2080, loss = 0.66, batch loss = 0.38 (46.8 examples/sec; 0.171 sec/batch; 15h:41m:57s remains)
INFO - root - 2017-12-16 07:54:23.249635: step 2090, loss = 0.66, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 15h:20m:37s remains)
INFO - root - 2017-12-16 07:54:24.967770: step 2100, loss = 0.65, batch loss = 0.37 (45.3 examples/sec; 0.176 sec/batch; 16h:11m:47s remains)
2017-12-16 07:54:25.469745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.74164736 -0.728446 -0.70304024 -0.65174538 -0.55108362 -0.48296124 -0.52964813 -0.720828 -1.0344511 -1.2529989 -1.1816593 -0.83156025 -0.48281789 -0.3755604 -0.60252208][-0.88596773 -0.87578094 -0.82809412 -0.73978484 -0.471628 -0.1892215 -0.17717636 -0.50635153 -1.0984888 -1.5879396 -1.5986946 -1.1839359 -0.68846869 -0.4461562 -0.61867207][-1.0787517 -1.0941088 -1.0784487 -0.92477524 -0.38450873 0.28198361 0.50959837 0.028218508 -0.94178277 -1.8196845 -2.0895956 -1.701933 -1.080184 -0.67956704 -0.74661922][-1.1976814 -1.2504512 -1.2746053 -1.0566187 -0.29143119 0.80935454 1.3747871 0.81680965 -0.46331048 -1.7785404 -2.4019868 -2.1782928 -1.5405024 -1.0182134 -0.94725609][-1.2278372 -1.3353794 -1.390083 -1.1528518 -0.27450371 1.1462803 2.1925185 1.9072585 0.40792155 -1.3772101 -2.4125817 -2.4597321 -1.8889618 -1.3441633 -1.1901513][-1.2333121 -1.3986284 -1.5283767 -1.3242732 -0.40357369 1.2166634 2.7176335 2.9959671 1.4045367 -0.69443983 -2.1275229 -2.5157232 -2.0785286 -1.5646842 -1.3910367][-1.241483 -1.4600739 -1.6810509 -1.592051 -0.7403959 1.0059481 2.969662 3.9036324 2.340879 0.00075685978 -1.7545278 -2.3799706 -2.0188859 -1.5204589 -1.3888273][-1.2536963 -1.5054475 -1.7906826 -1.8134623 -1.1370397 0.54786432 2.8057582 4.14843 2.8617818 0.37498581 -1.4549682 -2.0209687 -1.5693369 -1.0415981 -1.063043][-1.2432188 -1.4787076 -1.7868592 -1.8703792 -1.4128839 0.044608116 2.1107137 3.4620464 2.5583479 0.38212085 -1.271068 -1.540096 -0.77972639 -0.13645077 -0.44771945][-1.1856728 -1.3637726 -1.655197 -1.7823697 -1.5172284 -0.39959818 1.23491 2.3834169 1.7692189 0.040466785 -1.1124967 -0.93457431 0.18644083 0.87944412 0.20774317][-1.0805779 -1.1958148 -1.4787736 -1.6835295 -1.547441 -0.75120687 0.50031519 1.3449593 0.89351726 -0.3693676 -0.98314631 -0.35037273 1.0351353 1.6070545 0.55698633][-0.96219063 -1.082571 -1.3853272 -1.622002 -1.5229399 -0.86664617 0.10596573 0.63057888 0.18264163 -0.6633442 -0.85466605 0.092252135 1.5299246 1.8900506 0.5007776][-0.95651084 -1.1054281 -1.3851105 -1.5791866 -1.4505537 -0.81953371 0.0099320412 0.28127837 -0.19084692 -0.81347883 -0.78375185 0.25758719 1.6090086 1.7653491 0.2314204][-1.1544553 -1.2992058 -1.4386946 -1.4347923 -1.1526173 -0.54640871 0.077178121 0.14859176 -0.41504973 -0.97263342 -0.80545157 0.3256588 1.5500965 1.6242471 0.074483871][-1.451189 -1.6014385 -1.5140542 -1.2098168 -0.72067916 -0.15785456 0.13436818 -0.15271997 -0.817519 -1.2543621 -0.85424292 0.43086898 1.7205281 1.7994852 0.1639235]]...]
INFO - root - 2017-12-16 07:54:27.142289: step 2110, loss = 0.72, batch loss = 0.44 (48.1 examples/sec; 0.166 sec/batch; 15h:15m:11s remains)
INFO - root - 2017-12-16 07:54:28.800223: step 2120, loss = 0.68, batch loss = 0.40 (49.4 examples/sec; 0.162 sec/batch; 14h:52m:36s remains)
INFO - root - 2017-12-16 07:54:30.460093: step 2130, loss = 0.74, batch loss = 0.46 (49.0 examples/sec; 0.163 sec/batch; 14h:58m:49s remains)
INFO - root - 2017-12-16 07:54:32.129535: step 2140, loss = 0.79, batch loss = 0.51 (49.5 examples/sec; 0.162 sec/batch; 14h:50m:07s remains)
INFO - root - 2017-12-16 07:54:33.784155: step 2150, loss = 0.68, batch loss = 0.40 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:21s remains)
INFO - root - 2017-12-16 07:54:35.449610: step 2160, loss = 0.71, batch loss = 0.43 (47.9 examples/sec; 0.167 sec/batch; 15h:19m:28s remains)
INFO - root - 2017-12-16 07:54:37.136263: step 2170, loss = 0.61, batch loss = 0.33 (48.4 examples/sec; 0.165 sec/batch; 15h:10m:35s remains)
INFO - root - 2017-12-16 07:54:38.801756: step 2180, loss = 0.65, batch loss = 0.37 (48.6 examples/sec; 0.165 sec/batch; 15h:05m:54s remains)
INFO - root - 2017-12-16 07:54:40.447575: step 2190, loss = 0.67, batch loss = 0.39 (50.0 examples/sec; 0.160 sec/batch; 14h:40m:49s remains)
INFO - root - 2017-12-16 07:54:42.160584: step 2200, loss = 0.61, batch loss = 0.33 (47.5 examples/sec; 0.168 sec/batch; 15h:26m:52s remains)
2017-12-16 07:54:42.686814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2648834 -1.3786628 -1.5486571 -1.7691292 -2.0094843 -2.1897764 -2.2825098 -2.3147693 -2.3103812 -2.3257647 -2.3750865 -2.3128543 -2.1151481 -1.8564348 -1.6461775][-1.3365117 -1.4904943 -1.745253 -2.0653059 -2.4209862 -2.6861537 -2.7656238 -2.7096193 -2.5981164 -2.5845866 -2.619627 -2.4699452 -2.0735431 -1.6667032 -1.3984174][-1.3921071 -1.511075 -1.7346911 -2.0554445 -2.4237213 -2.7001491 -2.7484806 -2.5966682 -2.4046865 -2.3562555 -2.4520628 -2.3007627 -1.7687175 -1.2302073 -0.99919236][-1.3848219 -1.3933923 -1.4672185 -1.6751139 -1.9165565 -2.0548925 -1.9860951 -1.7365813 -1.5495349 -1.5307168 -1.759347 -1.7409431 -1.3142776 -0.81569093 -0.74381346][-1.2868005 -1.1544639 -1.055271 -1.0222195 -0.93232822 -0.78049523 -0.43242866 0.017578244 0.11045194 -0.20455813 -0.74866539 -1.0086592 -0.80497813 -0.55052537 -0.70889843][-1.0040802 -0.62297875 -0.30576438 -0.072893023 0.32109284 0.94224107 1.7212704 2.4634075 2.4226956 1.5157558 0.47794402 -0.15444446 -0.3131029 -0.40392488 -0.77143341][-0.57535177 0.11388862 0.67967188 1.0456072 1.5372919 2.5539713 3.8848238 5.0164247 4.5457735 3.0880666 1.6764759 0.70545173 0.17350817 -0.23426175 -0.76407218][-0.20040178 0.624861 1.2520818 1.6063133 2.1109519 3.2415814 4.6297565 5.9131989 5.1992283 3.6303067 2.3595629 1.4296545 0.74140584 0.04787457 -0.68753082][-0.058001757 0.70911169 1.1025695 1.2407113 1.58544 2.5032072 3.4771471 4.0986185 3.7557592 2.8387718 2.0862594 1.423015 0.70388663 -0.062045813 -0.81529266][-0.26613832 0.15693164 0.21644878 0.13973367 0.2713182 0.84844863 1.3239983 1.4949929 1.3975815 1.0548245 0.74020827 0.31594539 -0.24448907 -0.7953248 -1.3453239][-0.71181935 -0.64066088 -0.874371 -1.0912192 -1.0529919 -0.82221544 -0.70222121 -0.70602709 -0.77243006 -0.93903983 -1.0234988 -1.2403896 -1.5514096 -1.8189158 -2.0043714][-1.1281416 -1.2719158 -1.6272569 -1.9071882 -1.9808695 -1.9335961 -1.9863241 -2.0776086 -2.1667106 -2.2185271 -2.2678037 -2.3765509 -2.4644976 -2.4637074 -2.3550198][-1.2857305 -1.482442 -1.7906122 -2.045871 -2.1749532 -2.227541 -2.324698 -2.449743 -2.5658474 -2.628648 -2.6599126 -2.7054434 -2.6620417 -2.5125332 -2.2717545][-1.2842312 -1.4259859 -1.6297371 -1.8090608 -1.9118552 -1.9687475 -2.0585485 -2.1765788 -2.2781932 -2.3353291 -2.3623061 -2.3548093 -2.2750864 -2.1093822 -1.8789785][-1.1933038 -1.270045 -1.3786299 -1.4752165 -1.5307137 -1.5662335 -1.6142397 -1.6883992 -1.735193 -1.7605824 -1.7675408 -1.733453 -1.663396 -1.5659382 -1.4339203]]...]
INFO - root - 2017-12-16 07:54:44.355007: step 2210, loss = 0.82, batch loss = 0.54 (48.9 examples/sec; 0.164 sec/batch; 15h:00m:19s remains)
INFO - root - 2017-12-16 07:54:46.045923: step 2220, loss = 0.62, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 15h:25m:40s remains)
INFO - root - 2017-12-16 07:54:47.727681: step 2230, loss = 0.58, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 15h:17m:40s remains)
INFO - root - 2017-12-16 07:54:49.405423: step 2240, loss = 0.69, batch loss = 0.41 (47.3 examples/sec; 0.169 sec/batch; 15h:30m:05s remains)
INFO - root - 2017-12-16 07:54:51.085124: step 2250, loss = 0.60, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 15h:31m:58s remains)
INFO - root - 2017-12-16 07:54:52.764344: step 2260, loss = 0.65, batch loss = 0.37 (45.9 examples/sec; 0.174 sec/batch; 16h:00m:11s remains)
INFO - root - 2017-12-16 07:54:54.443517: step 2270, loss = 0.68, batch loss = 0.40 (47.8 examples/sec; 0.167 sec/batch; 15h:20m:42s remains)
INFO - root - 2017-12-16 07:54:56.101219: step 2280, loss = 0.64, batch loss = 0.36 (45.8 examples/sec; 0.175 sec/batch; 16h:01m:15s remains)
INFO - root - 2017-12-16 07:54:57.789811: step 2290, loss = 0.73, batch loss = 0.45 (47.4 examples/sec; 0.169 sec/batch; 15h:29m:25s remains)
INFO - root - 2017-12-16 07:54:59.463572: step 2300, loss = 0.82, batch loss = 0.54 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:08s remains)
2017-12-16 07:54:59.941922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.90817237 -0.73691905 -0.687414 -0.70604062 -0.72344542 -0.75494444 -0.83428121 -0.96434462 -1.0776285 -1.1255873 -1.1316509 -1.1014636 -1.0511612 -1.0057044 -0.99014753][-1.7651612 -1.5864365 -1.5413864 -1.5663618 -1.5888939 -1.6371267 -1.7863163 -1.9980639 -2.16252 -2.241502 -2.2543721 -2.2456393 -2.2402751 -2.2475035 -2.2680857][-2.2341793 -2.1632123 -2.1329684 -2.1402385 -2.1643295 -2.2378111 -2.3655987 -2.5055113 -2.5390816 -2.5153913 -2.4882843 -2.4994082 -2.5492282 -2.6191497 -2.7190347][-2.0545545 -2.1318243 -2.1614337 -2.1985679 -2.2242303 -2.251328 -2.32315 -2.3446236 -2.1857667 -1.9873343 -1.8908331 -1.8948016 -1.8907654 -1.9174259 -1.9933476][-1.5255237 -1.7592666 -1.9005775 -2.0197597 -2.064626 -2.0356872 -2.0378618 -1.9725358 -1.7256576 -1.4376028 -1.2641256 -1.1637858 -1.0003322 -0.894104 -0.8902638][-1.2532896 -1.5364927 -1.7684623 -2.0130632 -2.125916 -2.1054876 -1.9531817 -1.7396936 -1.4572277 -1.1567204 -0.87260056 -0.61461782 -0.3114391 -0.098302722 0.013304949][-0.96690339 -1.1738167 -1.4922916 -1.8006051 -1.9835858 -2.0211735 -1.8409158 -1.5486422 -1.2356739 -0.91857678 -0.56214327 -0.28056955 -0.043910146 0.13999128 0.32425129][-0.52073872 -0.58538926 -0.83776993 -1.1076963 -1.2861038 -1.3323889 -1.2135324 -0.97623354 -0.71716952 -0.38815719 -0.035756469 0.12302864 0.1007545 0.052535295 0.16607559][-0.15359437 -0.04328239 -0.10110009 -0.26544321 -0.43746924 -0.54070562 -0.46568745 -0.27396774 -0.026833415 0.30696356 0.53495347 0.47817755 0.19902527 -0.095710635 -0.13194525][-0.4391312 -0.314385 -0.28928715 -0.34803361 -0.41092014 -0.37717575 -0.17995524 0.13320827 0.46090221 0.79292762 0.91921151 0.72826755 0.33811164 0.0057109594 -0.02445066][-0.862522 -0.85806084 -0.90480447 -0.90294421 -0.8357178 -0.620062 -0.23683298 0.18977523 0.53875148 0.82682168 0.91651833 0.79317319 0.57698011 0.42761636 0.44794583][-1.0250944 -1.1537544 -1.270901 -1.2586429 -1.1568474 -0.90841615 -0.46923321 -0.052170157 0.1370157 0.21068394 0.22992909 0.251624 0.27035916 0.32686949 0.393337][-1.080377 -1.3225546 -1.4912143 -1.5035646 -1.4149262 -1.2564207 -0.99758756 -0.8018291 -0.8351981 -0.98263264 -1.0558827 -0.99945915 -0.92239535 -0.82178867 -0.77768147][-0.99590844 -1.1250174 -1.3278626 -1.4846429 -1.6044776 -1.6750891 -1.6617323 -1.6985519 -1.9040576 -2.1560221 -2.2920718 -2.3013139 -2.2803848 -2.2285528 -2.2247739][-1.1734641 -1.1467141 -1.222844 -1.439781 -1.6901029 -1.9006925 -2.0800595 -2.2633305 -2.5056295 -2.7592707 -2.9497566 -3.0125585 -3.0404572 -3.0457277 -3.0584681]]...]
INFO - root - 2017-12-16 07:55:01.601979: step 2310, loss = 0.66, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 15h:13m:06s remains)
INFO - root - 2017-12-16 07:55:03.271249: step 2320, loss = 0.63, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 15h:07m:56s remains)
INFO - root - 2017-12-16 07:55:04.932736: step 2330, loss = 0.70, batch loss = 0.42 (48.5 examples/sec; 0.165 sec/batch; 15h:07m:11s remains)
INFO - root - 2017-12-16 07:55:06.626659: step 2340, loss = 0.69, batch loss = 0.41 (46.7 examples/sec; 0.171 sec/batch; 15h:41m:42s remains)
INFO - root - 2017-12-16 07:55:08.339027: step 2350, loss = 0.71, batch loss = 0.43 (48.1 examples/sec; 0.166 sec/batch; 15h:16m:02s remains)
INFO - root - 2017-12-16 07:55:10.026047: step 2360, loss = 0.67, batch loss = 0.39 (47.2 examples/sec; 0.169 sec/batch; 15h:31m:38s remains)
INFO - root - 2017-12-16 07:55:11.729623: step 2370, loss = 0.75, batch loss = 0.47 (48.8 examples/sec; 0.164 sec/batch; 15h:02m:38s remains)
INFO - root - 2017-12-16 07:55:13.439636: step 2380, loss = 0.62, batch loss = 0.34 (48.5 examples/sec; 0.165 sec/batch; 15h:08m:28s remains)
INFO - root - 2017-12-16 07:55:15.123558: step 2390, loss = 0.69, batch loss = 0.41 (46.5 examples/sec; 0.172 sec/batch; 15h:46m:41s remains)
INFO - root - 2017-12-16 07:55:16.814082: step 2400, loss = 0.68, batch loss = 0.40 (48.0 examples/sec; 0.167 sec/batch; 15h:16m:56s remains)
2017-12-16 07:55:17.283535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.337654 -1.3991375 -1.4846475 -1.5757724 -1.6373502 -1.6691784 -1.6982472 -1.7476588 -1.7940993 -1.8003845 -1.7394614 -1.6093822 -1.4461678 -1.3102174 -1.2455512][-1.392988 -1.4371862 -1.5104328 -1.5867958 -1.6122967 -1.5843899 -1.5552993 -1.6038712 -1.696064 -1.7741641 -1.7631168 -1.6479816 -1.4533041 -1.2598861 -1.1472917][-1.4827459 -1.53349 -1.6124716 -1.6538225 -1.5693024 -1.3735077 -1.1797152 -1.1636533 -1.3529072 -1.6051247 -1.7471467 -1.7146611 -1.5335466 -1.2873784 -1.1117605][-1.5188738 -1.6089759 -1.7087564 -1.7048163 -1.4610705 -1.0035168 -0.51802415 -0.34711605 -0.59970456 -1.1218667 -1.5596565 -1.7490532 -1.6803484 -1.4380852 -1.2021203][-1.4488363 -1.614656 -1.7562428 -1.6842474 -1.1963758 -0.307521 0.63015795 1.0487107 0.67589462 -0.22917569 -1.1429871 -1.6966269 -1.8441484 -1.6897182 -1.4332659][-1.3202748 -1.5681801 -1.7673659 -1.6193236 -0.84888828 0.54655504 2.0747013 2.8380694 2.3102112 0.93084967 -0.46352714 -1.4313309 -1.8517311 -1.8532109 -1.6121677][-1.2217368 -1.514945 -1.7566204 -1.611673 -0.77549976 0.82250178 2.6229243 3.6551261 3.3849912 2.0857692 0.43459094 -0.94257045 -1.7179809 -1.9084659 -1.7021188][-1.1945184 -1.489203 -1.770035 -1.7385418 -1.0920269 0.34439683 2.2410059 3.7600937 4.157371 3.2110991 1.3752707 -0.46819675 -1.6150317 -1.9588878 -1.7910832][-1.2803314 -1.5129567 -1.7877719 -1.8928161 -1.4839936 -0.26683414 1.5702578 3.3845205 4.3166933 3.6177268 1.6288306 -0.42209548 -1.6320865 -1.9675319 -1.7831779][-1.3855366 -1.5490034 -1.7849733 -1.9601004 -1.7752402 -0.94100088 0.51841784 2.1210823 3.040339 2.4947906 0.90037572 -0.69883555 -1.5985503 -1.7579856 -1.5132087][-1.427606 -1.5682025 -1.7767485 -1.9856442 -2.0059888 -1.5663388 -0.65173751 0.42966318 1.0705985 0.83503163 -0.0594666 -0.94694757 -1.389485 -1.3473556 -1.0957302][-1.4143476 -1.5416666 -1.7225008 -1.9465694 -2.0880575 -1.9629619 -1.5111914 -0.85213125 -0.39167041 -0.38233674 -0.70892143 -1.03809 -1.1297594 -0.97896183 -0.76806784][-1.3828744 -1.4941576 -1.6360283 -1.8250353 -2.0095134 -2.0674019 -1.9123738 -1.5642679 -1.2542343 -1.1204357 -1.130183 -1.1072447 -0.97412908 -0.77485406 -0.63274693][-1.374263 -1.4509003 -1.5367963 -1.6617017 -1.8204734 -1.9410403 -1.9422127 -1.8221617 -1.6588099 -1.5188328 -1.369203 -1.1666018 -0.95569181 -0.77675134 -0.71453482][-1.406949 -1.4323827 -1.4577106 -1.5096754 -1.6040256 -1.7058271 -1.7591529 -1.7456392 -1.6610687 -1.5208523 -1.3459735 -1.1615276 -1.0094537 -0.92848015 -0.95124948]]...]
INFO - root - 2017-12-16 07:55:18.944627: step 2410, loss = 0.70, batch loss = 0.42 (46.3 examples/sec; 0.173 sec/batch; 15h:50m:13s remains)
INFO - root - 2017-12-16 07:55:20.585129: step 2420, loss = 0.61, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 15h:20m:54s remains)
INFO - root - 2017-12-16 07:55:22.278951: step 2430, loss = 0.71, batch loss = 0.43 (48.1 examples/sec; 0.166 sec/batch; 15h:15m:02s remains)
INFO - root - 2017-12-16 07:55:23.983617: step 2440, loss = 0.65, batch loss = 0.37 (46.5 examples/sec; 0.172 sec/batch; 15h:45m:49s remains)
INFO - root - 2017-12-16 07:55:25.689112: step 2450, loss = 0.68, batch loss = 0.40 (48.9 examples/sec; 0.164 sec/batch; 14h:59m:51s remains)
INFO - root - 2017-12-16 07:55:27.370800: step 2460, loss = 0.69, batch loss = 0.41 (48.4 examples/sec; 0.165 sec/batch; 15h:09m:50s remains)
INFO - root - 2017-12-16 07:55:29.045608: step 2470, loss = 0.64, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 15h:14m:47s remains)
INFO - root - 2017-12-16 07:55:30.703541: step 2480, loss = 0.66, batch loss = 0.38 (47.2 examples/sec; 0.169 sec/batch; 15h:31m:43s remains)
INFO - root - 2017-12-16 07:55:32.380154: step 2490, loss = 0.67, batch loss = 0.39 (48.0 examples/sec; 0.167 sec/batch; 15h:17m:23s remains)
INFO - root - 2017-12-16 07:55:34.034782: step 2500, loss = 0.72, batch loss = 0.44 (47.4 examples/sec; 0.169 sec/batch; 15h:27m:56s remains)
2017-12-16 07:55:34.516740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9776133 -2.0217853 -2.1307287 -2.1802425 -2.1746879 -2.1623065 -2.1713581 -2.0769494 -1.9604298 -1.9010146 -1.844179 -1.8659515 -1.916425 -1.9382111 -1.8361237][-1.8609761 -1.8014128 -1.8588965 -1.9884461 -2.092937 -2.1435761 -2.1943941 -2.1230435 -1.9021106 -1.6674522 -1.5302593 -1.5313754 -1.6142182 -1.7502151 -1.8500738][-1.3597053 -1.2343609 -1.2874855 -1.4493357 -1.6106019 -1.7069628 -1.7777988 -1.7247317 -1.5412399 -1.277307 -1.084053 -1.0260853 -1.1062853 -1.28046 -1.5009873][-0.69843584 -0.53529578 -0.61385208 -0.80606192 -0.993381 -1.1306926 -1.2044874 -1.2318928 -1.2088089 -1.0933034 -0.90267265 -0.73543727 -0.75489026 -0.8688556 -1.026726][-0.16910422 -0.0408715 -0.13500011 -0.34713411 -0.5240643 -0.59088749 -0.68120533 -0.79488117 -0.9319303 -1.0160983 -0.9150067 -0.73079956 -0.6607188 -0.63375604 -0.59799576][0.0066874027 0.073684573 -0.031926155 -0.19360459 -0.22259974 -0.036118865 0.064290166 -0.014902711 -0.33331251 -0.6473074 -0.79298121 -0.81537038 -0.73697549 -0.642924 -0.51197183][-0.26419711 -0.16212201 -0.23578501 -0.2985884 -0.091656089 0.48781967 1.0490375 1.2766161 0.79041004 0.0946368 -0.41512877 -0.72008246 -0.83164853 -0.90970588 -0.83279192][-0.64565843 -0.62089372 -0.70087969 -0.65131229 -0.14831018 0.8514502 1.9986112 2.8050537 1.9069149 0.82794142 0.037978172 -0.5225448 -0.95785975 -1.3171787 -1.4531192][-1.0503316 -1.1497939 -1.3138969 -1.2214633 -0.49224931 0.62145329 1.7302289 2.2806275 1.567771 0.6373446 -0.1179291 -0.71308941 -1.1972249 -1.7188795 -2.0375116][-1.4004308 -1.5343028 -1.7298328 -1.6190417 -0.9427917 -0.053348303 0.54173863 0.58448911 0.14618003 -0.3367604 -0.77797109 -1.1326799 -1.4760286 -1.9093819 -2.2028246][-1.5563875 -1.6586146 -1.8098378 -1.7044581 -1.23022 -0.76685858 -0.66014558 -0.90238255 -1.2325661 -1.3944395 -1.4349414 -1.4775435 -1.5915736 -1.8035774 -1.94606][-1.4387852 -1.476657 -1.5788496 -1.5299779 -1.3355516 -1.2373366 -1.4325972 -1.7692804 -1.9999719 -1.9569018 -1.7394443 -1.5783548 -1.4860499 -1.514535 -1.5707554][-1.0802486 -1.0719147 -1.173014 -1.2082239 -1.2371156 -1.4085718 -1.7422186 -2.0775146 -2.2048235 -2.0590234 -1.764056 -1.5125778 -1.3280025 -1.242363 -1.2666055][-0.68166751 -0.62867123 -0.69929719 -0.76775169 -0.88339591 -1.1808959 -1.5889816 -1.9262266 -2.0034378 -1.8397026 -1.6002116 -1.3736832 -1.18208 -1.0753102 -1.0936075][-0.508461 -0.42572498 -0.38419092 -0.36874843 -0.47527015 -0.8215059 -1.2182887 -1.5062783 -1.5381103 -1.4037058 -1.218012 -1.0549881 -0.91101158 -0.84635162 -0.86974645]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-2500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:55:36.650193: step 2510, loss = 0.64, batch loss = 0.36 (49.5 examples/sec; 0.162 sec/batch; 14h:49m:29s remains)
INFO - root - 2017-12-16 07:55:38.283367: step 2520, loss = 0.65, batch loss = 0.37 (48.6 examples/sec; 0.165 sec/batch; 15h:06m:06s remains)
INFO - root - 2017-12-16 07:55:39.966004: step 2530, loss = 0.67, batch loss = 0.39 (47.0 examples/sec; 0.170 sec/batch; 15h:35m:08s remains)
INFO - root - 2017-12-16 07:55:41.690925: step 2540, loss = 0.65, batch loss = 0.37 (46.3 examples/sec; 0.173 sec/batch; 15h:50m:59s remains)
INFO - root - 2017-12-16 07:55:43.352535: step 2550, loss = 0.64, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 15h:25m:24s remains)
INFO - root - 2017-12-16 07:55:45.027990: step 2560, loss = 0.71, batch loss = 0.43 (48.6 examples/sec; 0.165 sec/batch; 15h:04m:57s remains)
INFO - root - 2017-12-16 07:55:46.694093: step 2570, loss = 0.64, batch loss = 0.36 (47.2 examples/sec; 0.170 sec/batch; 15h:32m:08s remains)
INFO - root - 2017-12-16 07:55:48.365189: step 2580, loss = 0.69, batch loss = 0.41 (49.3 examples/sec; 0.162 sec/batch; 14h:51m:53s remains)
INFO - root - 2017-12-16 07:55:50.037559: step 2590, loss = 0.62, batch loss = 0.34 (49.5 examples/sec; 0.162 sec/batch; 14h:49m:04s remains)
INFO - root - 2017-12-16 07:55:51.726538: step 2600, loss = 0.61, batch loss = 0.33 (48.4 examples/sec; 0.165 sec/batch; 15h:08m:03s remains)
2017-12-16 07:55:52.214398: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6333694 1.3272343 0.77815914 0.33362985 0.043542624 -0.14871848 -0.28349817 -0.36164433 -0.32863104 -0.2148875 -0.027201414 0.245947 0.36946702 -0.080865383 -1.0303388][1.244004 0.88139081 0.35266256 -0.000713706 -0.17441738 -0.26926112 -0.45224887 -0.71207851 -0.92542148 -1.0343685 -0.94342321 -0.60875887 -0.28811395 -0.51586032 -1.3489267][0.32475555 -0.054728389 -0.49063623 -0.74280208 -0.84808439 -0.96387768 -1.2430091 -1.6392173 -1.9745668 -2.1257639 -2.0119185 -1.595799 -1.0755351 -1.008386 -1.5524685][-0.95585251 -1.196435 -1.4293572 -1.6003321 -1.784587 -2.0427518 -2.3883102 -2.7796562 -3.073509 -3.1344991 -2.872076 -2.2885275 -1.5527625 -1.1889949 -1.4312611][-2.094219 -1.943876 -1.813581 -1.8753133 -2.2368577 -2.7137558 -3.0826998 -3.378267 -3.6104949 -3.6201408 -3.2442279 -2.456511 -1.5248444 -0.97887945 -1.0369147][-2.647965 -1.8834796 -1.1799493 -1.0529972 -1.598496 -2.2701077 -2.6155939 -2.7850976 -3.0087337 -3.1902003 -3.0087523 -2.2636454 -1.3494803 -0.81766033 -0.77897751][-2.3939161 -1.0602365 0.19033635 0.54135275 -0.16230166 -0.92687774 -1.1372898 -1.1203897 -1.4245188 -1.984354 -2.2963853 -1.9608426 -1.3123788 -0.93302143 -0.88285106][-1.5348308 -0.080597758 1.3759701 1.8535111 1.0963051 0.37060595 0.44121552 0.78552532 0.44744945 -0.54153806 -1.4027938 -1.4622697 -1.1459105 -1.0440617 -1.180617][-0.47985947 0.39740431 1.4578013 1.8181834 1.2378554 0.82020569 1.2326102 1.8534 1.5453789 0.29117751 -0.91806525 -1.1817464 -1.0154611 -1.0597661 -1.374261][0.47718549 0.31777751 0.53104126 0.51099432 0.14423573 0.020757079 0.59084868 1.3383989 1.2397866 0.10170841 -0.99045461 -1.1543878 -0.8521167 -0.87185347 -1.3768387][1.2871289 0.15239251 -0.50801528 -0.90488583 -1.1821109 -1.2230598 -0.77642733 -0.16655374 -0.099467158 -0.76911205 -1.3738658 -1.1894317 -0.56542432 -0.46442646 -1.0649143][1.6935585 0.029849529 -1.1686113 -1.8264443 -2.13901 -2.2068281 -1.9107757 -1.4161414 -1.195091 -1.4034214 -1.5012572 -0.98576647 -0.15238953 0.027120709 -0.64986813][1.1996503 -0.46907026 -1.6801201 -2.2994158 -2.52045 -2.5431466 -2.329294 -1.9103236 -1.5440259 -1.4128423 -1.2132696 -0.5956617 0.17299223 0.26822293 -0.44805539][-0.017355919 -1.3017784 -2.1665049 -2.4973297 -2.4839787 -2.3394582 -2.069602 -1.6334388 -1.2097704 -0.99274504 -0.79936689 -0.40021056 0.052924752 -0.01813364 -0.71795362][-1.3259163 -2.1006827 -2.490119 -2.4493239 -2.1442957 -1.7912991 -1.4346852 -1.0393846 -0.68005127 -0.55323428 -0.58883679 -0.59479612 -0.55165172 -0.776765 -1.3719883]]...]
INFO - root - 2017-12-16 07:55:53.897128: step 2610, loss = 0.60, batch loss = 0.32 (46.9 examples/sec; 0.171 sec/batch; 15h:38m:38s remains)
INFO - root - 2017-12-16 07:55:55.544223: step 2620, loss = 0.67, batch loss = 0.39 (48.6 examples/sec; 0.164 sec/batch; 15h:04m:05s remains)
INFO - root - 2017-12-16 07:55:57.216215: step 2630, loss = 0.74, batch loss = 0.47 (45.6 examples/sec; 0.175 sec/batch; 16h:03m:34s remains)
INFO - root - 2017-12-16 07:55:58.874029: step 2640, loss = 0.72, batch loss = 0.44 (48.8 examples/sec; 0.164 sec/batch; 15h:01m:42s remains)
INFO - root - 2017-12-16 07:56:00.533715: step 2650, loss = 0.60, batch loss = 0.32 (46.6 examples/sec; 0.172 sec/batch; 15h:43m:07s remains)
INFO - root - 2017-12-16 07:56:02.197940: step 2660, loss = 0.54, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 15h:18m:38s remains)
INFO - root - 2017-12-16 07:56:03.871855: step 2670, loss = 0.69, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 15h:17m:57s remains)
INFO - root - 2017-12-16 07:56:05.546533: step 2680, loss = 0.63, batch loss = 0.35 (46.1 examples/sec; 0.174 sec/batch; 15h:54m:01s remains)
INFO - root - 2017-12-16 07:56:07.229310: step 2690, loss = 0.59, batch loss = 0.31 (48.9 examples/sec; 0.163 sec/batch; 14h:58m:31s remains)
INFO - root - 2017-12-16 07:56:08.940749: step 2700, loss = 0.61, batch loss = 0.34 (45.9 examples/sec; 0.174 sec/batch; 15h:57m:28s remains)
2017-12-16 07:56:09.435123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.522921 -1.5238031 -1.5254858 -1.5295367 -1.5345972 -1.537892 -1.5404772 -1.5440334 -1.5513787 -1.5576831 -1.5587738 -1.5553895 -1.547707 -1.5423621 -1.5378498][-1.5687441 -1.5876871 -1.6027482 -1.6164278 -1.6302623 -1.643425 -1.6544199 -1.6662688 -1.679635 -1.6853817 -1.678143 -1.6529773 -1.6232708 -1.5980623 -1.5775207][-2.0312965 -2.0851076 -2.1303256 -2.1680737 -2.2109022 -2.2438221 -2.27338 -2.2950926 -2.31092 -2.302928 -2.2666802 -2.1984882 -2.1234782 -2.0570912 -1.9945531][-2.7124574 -2.7972279 -2.8721471 -2.95099 -3.0232821 -3.0758514 -3.129173 -3.1464491 -3.1444612 -3.111546 -3.0443838 -2.9320745 -2.8124843 -2.7157679 -2.6075191][-3.1513464 -3.20894 -3.2806163 -3.3611946 -3.4351897 -3.5156641 -3.6179557 -3.6694145 -3.6812158 -3.6529188 -3.5732059 -3.4477229 -3.3375139 -3.2378178 -3.1103494][-2.8955617 -2.8304212 -2.7686653 -2.7269387 -2.7261341 -2.8083813 -2.99868 -3.1805978 -3.2996197 -3.3350153 -3.3262711 -3.3193684 -3.3308477 -3.3258338 -3.2635777][-1.8811499 -1.619697 -1.3030437 -0.964442 -0.7606644 -0.79929364 -1.1201358 -1.4681067 -1.7018127 -1.8799657 -2.0846889 -2.380652 -2.6965647 -2.9256468 -3.0083282][-0.648552 -0.11097312 0.57084572 1.2748 1.7254649 1.7111322 1.2937528 0.8868233 0.63454378 0.31278682 -0.12986946 -0.78755248 -1.5038373 -2.0372353 -2.3790584][-0.020138741 0.63665664 1.4588217 2.3559756 2.9952059 3.1441097 2.8496084 2.6462297 2.6016998 2.3613973 1.7947 0.841859 -0.18936002 -1.0508159 -1.6949377][-0.26951349 0.34851992 1.0687929 1.8606175 2.4340506 2.6445031 2.6849723 2.9168863 3.2508559 3.3163638 2.8942475 1.9981662 0.90034497 -0.24990761 -1.1672144][-0.98740554 -0.34229469 0.32071006 0.89907229 1.2120897 1.2244092 1.2513126 1.5575296 2.1472478 2.5006485 2.4401493 1.9224547 1.0572215 -0.057683706 -0.98070359][-1.4715894 -0.65731591 0.10204732 0.53265834 0.49575925 0.14914811 -0.14971507 -0.023890495 0.53429914 0.98673165 1.1896273 1.0861803 0.5773555 -0.25786853 -0.96772742][-1.6731869 -0.61674124 0.36494088 0.8731066 0.81756842 0.19862974 -0.56873876 -0.80810279 -0.47830456 -0.040759087 0.29415691 0.40047765 0.15220594 -0.37412071 -0.81296068][-1.8390687 -0.7027027 0.43068731 1.1611692 1.3330506 0.73966372 -0.27591693 -0.755304 -0.59687185 -0.18680024 0.16133189 0.25835514 0.069324374 -0.35233688 -0.67909163][-2.1938612 -1.2051634 -0.037149787 0.88585603 1.3451837 1.0027844 0.2119168 -0.24612486 -0.19448769 0.20805609 0.46238196 0.45326209 0.20632637 -0.27753937 -0.74418133]]...]
INFO - root - 2017-12-16 07:56:11.113605: step 2710, loss = 0.57, batch loss = 0.29 (45.5 examples/sec; 0.176 sec/batch; 16h:07m:25s remains)
INFO - root - 2017-12-16 07:56:12.798151: step 2720, loss = 0.65, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 15h:05m:44s remains)
INFO - root - 2017-12-16 07:56:14.461236: step 2730, loss = 0.64, batch loss = 0.36 (47.6 examples/sec; 0.168 sec/batch; 15h:23m:16s remains)
INFO - root - 2017-12-16 07:56:16.156877: step 2740, loss = 0.71, batch loss = 0.43 (47.8 examples/sec; 0.167 sec/batch; 15h:20m:14s remains)
INFO - root - 2017-12-16 07:56:17.807871: step 2750, loss = 0.68, batch loss = 0.40 (48.3 examples/sec; 0.166 sec/batch; 15h:09m:54s remains)
INFO - root - 2017-12-16 07:56:19.466420: step 2760, loss = 0.62, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 15h:22m:44s remains)
INFO - root - 2017-12-16 07:56:21.136941: step 2770, loss = 0.85, batch loss = 0.57 (47.1 examples/sec; 0.170 sec/batch; 15h:32m:34s remains)
INFO - root - 2017-12-16 07:56:22.809215: step 2780, loss = 0.63, batch loss = 0.35 (44.8 examples/sec; 0.179 sec/batch; 16h:20m:57s remains)
INFO - root - 2017-12-16 07:56:24.465095: step 2790, loss = 0.67, batch loss = 0.39 (48.8 examples/sec; 0.164 sec/batch; 15h:01m:08s remains)
INFO - root - 2017-12-16 07:56:26.124569: step 2800, loss = 0.60, batch loss = 0.32 (47.1 examples/sec; 0.170 sec/batch; 15h:32m:50s remains)
2017-12-16 07:56:26.624595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3363905 -2.3873146 -2.3880124 -2.3531544 -2.3286955 -2.2957382 -2.2523856 -2.2182093 -2.2599173 -2.3592134 -2.461412 -2.5502572 -2.6564579 -2.7245574 -2.7270017][-1.8966217 -2.0084641 -2.0642507 -2.0902715 -2.1632705 -2.2245917 -2.2432094 -2.2514172 -2.3366046 -2.4684958 -2.5901394 -2.6860094 -2.7665734 -2.7628965 -2.7218621][-1.6279901 -1.8190658 -1.9280847 -2.0951407 -2.302341 -2.4526834 -2.4707861 -2.403764 -2.3507879 -2.3864787 -2.4212289 -2.4328589 -2.4612536 -2.4665847 -2.4881468][-1.4905305 -1.8182721 -2.0566034 -2.3234754 -2.5585399 -2.6043577 -2.442589 -2.1530392 -1.8841509 -1.7374996 -1.5996583 -1.4979348 -1.4909824 -1.5892829 -1.7466507][-1.437063 -1.9231594 -2.2966862 -2.4821477 -2.5031271 -2.2762506 -1.7615416 -1.168014 -0.71213555 -0.49493557 -0.31190491 -0.11787069 -0.070254207 -0.31761432 -0.63318884][-1.2922882 -1.7850192 -2.0666492 -2.0334635 -1.68962 -1.0210254 -0.095598817 0.72581077 1.1920061 1.3123825 1.3916898 1.5324066 1.4251497 0.939908 0.43426621][-1.1763592 -1.3567188 -1.2382963 -0.79318541 0.020238161 1.1778803 2.5332091 3.5738587 3.7221889 3.2563844 2.7501473 2.3488073 1.8514209 1.2514484 0.77817106][-1.0885886 -0.83884907 -0.2752049 0.59903026 1.7391579 3.1665769 4.8031154 5.8202667 5.1538858 3.7297297 2.5039372 1.6233485 0.95568037 0.40471089 0.068779945][-1.0100329 -0.43998915 0.413553 1.3219922 2.0908518 2.9804249 4.0530868 4.4086571 3.5713906 2.1881981 1.0339465 0.23543644 -0.32617879 -0.75872189 -1.0644251][-0.88670468 -0.16368055 0.54489744 0.93707466 0.86579204 1.0615599 1.6162834 1.812263 1.3119454 0.3599236 -0.505867 -1.1159983 -1.4874871 -1.7836096 -2.0409017][-0.58415258 -0.24659657 -0.12848914 -0.39581752 -0.90468913 -0.96845913 -0.52010739 -0.20840967 -0.392753 -0.94799876 -1.5178958 -1.9220228 -2.1252098 -2.2736998 -2.4308591][-0.20691943 -0.27281582 -0.58302337 -1.2739259 -2.0144839 -2.1979835 -1.8138599 -1.3572164 -1.2547415 -1.4861941 -1.8142545 -2.0993981 -2.2275429 -2.2673492 -2.3029008][-0.02812314 -0.30658329 -0.83070266 -1.6984104 -2.4119091 -2.5208135 -2.0992675 -1.6425667 -1.470981 -1.5599551 -1.7391801 -1.8910513 -1.9372505 -1.929044 -1.9486203][-0.64841962 -1.0391908 -1.6180437 -2.3114679 -2.7592726 -2.7005951 -2.2474234 -1.7773552 -1.5274457 -1.5040119 -1.5156119 -1.4868164 -1.3980333 -1.3617582 -1.3686122][-1.8820084 -2.3516686 -2.8256793 -3.1864114 -3.304255 -3.1116865 -2.7407084 -2.3918691 -2.1495504 -1.9877871 -1.737522 -1.3928683 -1.1186435 -0.99028075 -0.97957385]]...]
INFO - root - 2017-12-16 07:56:28.317803: step 2810, loss = 0.81, batch loss = 0.54 (46.3 examples/sec; 0.173 sec/batch; 15h:48m:25s remains)
INFO - root - 2017-12-16 07:56:30.023968: step 2820, loss = 0.66, batch loss = 0.38 (47.5 examples/sec; 0.169 sec/batch; 15h:25m:59s remains)
INFO - root - 2017-12-16 07:56:31.721135: step 2830, loss = 0.53, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 15h:34m:20s remains)
INFO - root - 2017-12-16 07:56:33.428174: step 2840, loss = 0.58, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 15h:07m:17s remains)
INFO - root - 2017-12-16 07:56:35.088047: step 2850, loss = 0.69, batch loss = 0.41 (48.8 examples/sec; 0.164 sec/batch; 14h:59m:47s remains)
INFO - root - 2017-12-16 07:56:36.771932: step 2860, loss = 0.71, batch loss = 0.43 (46.4 examples/sec; 0.173 sec/batch; 15h:47m:47s remains)
INFO - root - 2017-12-16 07:56:38.441761: step 2870, loss = 0.58, batch loss = 0.30 (47.8 examples/sec; 0.167 sec/batch; 15h:19m:12s remains)
INFO - root - 2017-12-16 07:56:40.096350: step 2880, loss = 0.70, batch loss = 0.43 (49.9 examples/sec; 0.160 sec/batch; 14h:39m:59s remains)
INFO - root - 2017-12-16 07:56:41.769895: step 2890, loss = 0.69, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 15h:17m:23s remains)
INFO - root - 2017-12-16 07:56:43.452853: step 2900, loss = 0.60, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 15h:11m:17s remains)
2017-12-16 07:56:43.937479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0796328 -1.0820785 -1.0463905 -1.0449443 -1.1609067 -1.3924501 -1.54503 -1.5183872 -1.3772197 -1.1690193 -0.99449575 -0.90185326 -0.85054666 -0.81233191 -0.78651422][-1.0860394 -1.0070709 -0.86215174 -0.79664838 -0.99482542 -1.4318531 -1.8184209 -1.9196858 -1.7675236 -1.4845324 -1.2022104 -1.0137157 -0.90802675 -0.81528968 -0.74335][-1.129362 -0.97496831 -0.70044 -0.5071646 -0.71084833 -1.2727844 -1.9315615 -2.2596219 -2.1909966 -1.9321319 -1.612253 -1.3270521 -1.143427 -0.96971691 -0.81651467][-1.2348225 -1.0007173 -0.55661356 -0.089337707 -0.057628393 -0.65630364 -1.590281 -2.2592678 -2.4294186 -2.322947 -2.060782 -1.7720704 -1.5351514 -1.305402 -1.0801283][-1.4087592 -1.1193366 -0.50033796 0.40139914 0.99287713 0.58161891 -0.4866358 -1.5818517 -2.231 -2.4753468 -2.4514704 -2.2794545 -2.0365961 -1.7570335 -1.4645998][-1.600054 -1.3222826 -0.548024 0.7413367 2.0414243 2.2505488 1.3048669 -0.089784861 -1.2896328 -2.1061683 -2.5523844 -2.6620183 -2.5101342 -2.2175851 -1.8731736][-1.7159265 -1.52281 -0.76378441 0.65629375 2.4375415 3.6310191 3.351634 1.9791869 0.41541135 -1.0182414 -2.1406372 -2.7040179 -2.7530155 -2.5181439 -2.1838808][-1.7418584 -1.6823379 -1.0941414 0.19892764 2.0613685 3.9630942 4.7691941 3.9707131 2.3444648 0.45131445 -1.2916815 -2.3953497 -2.6997671 -2.5502963 -2.2579017][-1.7258041 -1.8222911 -1.5275902 -0.60239249 0.97765648 2.9198041 4.4838662 4.7391067 3.5109863 1.5766073 -0.4134903 -1.8480828 -2.3588295 -2.311136 -2.0739839][-1.6646944 -1.8515577 -1.8492233 -1.42764 -0.36147118 1.1513349 2.7662296 3.7427692 3.4180837 2.0152292 0.22200131 -1.2667915 -1.9068714 -1.9586377 -1.7870566][-1.6050141 -1.8156482 -1.9511472 -1.8916426 -1.2752559 -0.25322342 0.98888171 1.929562 2.1166015 1.4496115 0.20763278 -0.96051 -1.5663511 -1.6738486 -1.5550249][-1.5363277 -1.7015507 -1.8616929 -1.9185157 -1.6491685 -1.0371611 -0.25470161 0.32251596 0.51751673 0.26963639 -0.34496653 -1.0324126 -1.4829649 -1.5743198 -1.4532263][-1.4338425 -1.531161 -1.6605213 -1.7680764 -1.6761941 -1.3465679 -0.89474273 -0.61002105 -0.52734971 -0.6001845 -0.76466811 -1.0669802 -1.3191957 -1.392329 -1.3139081][-1.280046 -1.3385755 -1.4511635 -1.591011 -1.5981778 -1.4186095 -1.1562676 -1.0113895 -0.96753073 -0.91655219 -0.84085435 -0.83167124 -0.92434359 -1.02826 -1.0691482][-1.1323791 -1.166224 -1.2686574 -1.4342973 -1.5121827 -1.4179274 -1.2269645 -1.1273036 -1.0901539 -0.96129322 -0.74826151 -0.57582027 -0.54804653 -0.64690948 -0.75658292]]...]
INFO - root - 2017-12-16 07:56:45.594868: step 2910, loss = 0.60, batch loss = 0.32 (49.3 examples/sec; 0.162 sec/batch; 14h:51m:19s remains)
INFO - root - 2017-12-16 07:56:47.262423: step 2920, loss = 0.63, batch loss = 0.35 (49.7 examples/sec; 0.161 sec/batch; 14h:44m:34s remains)
INFO - root - 2017-12-16 07:56:48.899234: step 2930, loss = 0.63, batch loss = 0.35 (49.0 examples/sec; 0.163 sec/batch; 14h:56m:58s remains)
INFO - root - 2017-12-16 07:56:50.566229: step 2940, loss = 0.74, batch loss = 0.46 (47.9 examples/sec; 0.167 sec/batch; 15h:16m:31s remains)
INFO - root - 2017-12-16 07:56:52.227444: step 2950, loss = 0.65, batch loss = 0.37 (48.6 examples/sec; 0.165 sec/batch; 15h:04m:25s remains)
INFO - root - 2017-12-16 07:56:53.901648: step 2960, loss = 0.65, batch loss = 0.37 (49.2 examples/sec; 0.163 sec/batch; 14h:52m:40s remains)
INFO - root - 2017-12-16 07:56:55.556552: step 2970, loss = 0.65, batch loss = 0.37 (46.9 examples/sec; 0.171 sec/batch; 15h:37m:19s remains)
INFO - root - 2017-12-16 07:56:57.268568: step 2980, loss = 0.59, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 15h:33m:39s remains)
INFO - root - 2017-12-16 07:56:58.921774: step 2990, loss = 0.59, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 15h:03m:04s remains)
INFO - root - 2017-12-16 07:57:00.588316: step 3000, loss = 0.66, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 15h:17m:22s remains)
2017-12-16 07:57:01.070800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3982949 -2.3090155 -2.1056552 -2.0773158 -2.1879404 -2.0754323 -1.4290953 -0.3684417 0.67625928 1.0803185 0.88961816 0.4162606 -0.057863355 -0.46529424 -0.95665705][-2.5348241 -2.33785 -2.0968099 -2.0922592 -2.2707949 -2.2207224 -1.5984014 -0.46012473 0.78978062 1.3553886 1.1994364 0.58587742 -0.13265729 -0.75325835 -1.2484713][-2.2250562 -2.0427585 -1.884465 -1.9729249 -2.2964926 -2.4782271 -2.109868 -1.1296792 0.11604464 0.91645193 0.99291205 0.55625105 -0.14868617 -0.818628 -1.3247365][-1.8181428 -1.7286844 -1.675828 -1.9351995 -2.4105473 -2.76256 -2.6507006 -1.9597968 -0.81294572 0.15790951 0.52719855 0.37083614 -0.12765646 -0.75631785 -1.2431055][-1.676362 -1.7130986 -1.7958642 -2.1680956 -2.6783938 -3.0022149 -2.8642197 -2.2436669 -1.3334249 -0.509987 -0.094265223 -0.014914632 -0.24922633 -0.62337077 -0.99148178][-1.7560853 -1.9213811 -2.1263733 -2.500598 -2.8695326 -2.9113946 -2.4753766 -1.657372 -0.84595031 -0.4516753 -0.35766613 -0.30847418 -0.37205756 -0.54294604 -0.81066078][-1.7592059 -1.9674649 -2.1995039 -2.5229335 -2.7145121 -2.4163644 -1.5773628 -0.52601612 0.3094492 0.36582196 -0.060054302 -0.36537826 -0.4699446 -0.59258962 -0.80345953][-1.6480352 -1.77715 -1.9338148 -2.1770093 -2.2568686 -1.7874918 -0.70301133 0.52895057 1.5208941 1.4799776 0.56059408 -0.14202392 -0.40886152 -0.55948818 -0.75900924][-1.4962574 -1.5062034 -1.5744131 -1.7990251 -1.9002153 -1.4144844 -0.40694332 0.71735835 1.6744766 1.6653423 0.82704735 0.17107046 -0.18630576 -0.47735155 -0.78532779][-1.4784455 -1.4393436 -1.5111209 -1.7918676 -1.9948509 -1.6847103 -0.94869757 -0.021898985 0.91168833 1.2022283 0.88177252 0.59814024 0.34414637 -0.040027976 -0.50801182][-1.6124361 -1.5541166 -1.6540946 -1.9912863 -2.2838445 -2.206548 -1.735238 -1.0140808 -0.11339498 0.48789752 0.7302382 0.96390367 0.9631989 0.52691066 -0.10240102][-1.786742 -1.7725466 -1.9021432 -2.2357025 -2.5805635 -2.682744 -2.3674669 -1.7738246 -0.92835665 -0.15719807 0.477113 1.0967767 1.3938487 1.0516167 0.42179048][-1.8315902 -1.8543864 -1.9710647 -2.2366161 -2.5582657 -2.7238998 -2.532871 -2.1082778 -1.4871011 -0.77100104 -0.029205441 0.75438571 1.2781949 1.1405334 0.63333917][-1.747733 -1.7835569 -1.8679731 -2.0408366 -2.2499118 -2.3333631 -2.23004 -2.0584457 -1.7668736 -1.3441306 -0.77055675 -0.052753925 0.50010383 0.50405085 0.21841574][-1.5742129 -1.6240456 -1.665939 -1.7541909 -1.8630886 -1.9012932 -1.8781675 -1.9231882 -1.9808861 -1.8802183 -1.5608777 -1.0274675 -0.5483281 -0.48637986 -0.63524258]]...]
INFO - root - 2017-12-16 07:57:02.792225: step 3010, loss = 0.76, batch loss = 0.48 (47.1 examples/sec; 0.170 sec/batch; 15h:33m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 07:57:04.453227: step 3020, loss = 0.59, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 15h:20m:23s remains)
INFO - root - 2017-12-16 07:57:06.106313: step 3030, loss = 0.78, batch loss = 0.51 (46.0 examples/sec; 0.174 sec/batch; 15h:54m:40s remains)
INFO - root - 2017-12-16 07:57:07.776523: step 3040, loss = 0.61, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 15h:22m:45s remains)
INFO - root - 2017-12-16 07:57:09.462933: step 3050, loss = 0.56, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 15h:17m:47s remains)
INFO - root - 2017-12-16 07:57:11.138979: step 3060, loss = 0.55, batch loss = 0.27 (47.5 examples/sec; 0.168 sec/batch; 15h:23m:58s remains)
INFO - root - 2017-12-16 07:57:12.798681: step 3070, loss = 0.65, batch loss = 0.37 (48.7 examples/sec; 0.164 sec/batch; 15h:02m:03s remains)
INFO - root - 2017-12-16 07:57:14.440470: step 3080, loss = 0.66, batch loss = 0.39 (47.8 examples/sec; 0.167 sec/batch; 15h:18m:48s remains)
INFO - root - 2017-12-16 07:57:16.160418: step 3090, loss = 0.65, batch loss = 0.37 (49.0 examples/sec; 0.163 sec/batch; 14h:56m:51s remains)
INFO - root - 2017-12-16 07:57:17.837559: step 3100, loss = 0.63, batch loss = 0.35 (46.3 examples/sec; 0.173 sec/batch; 15h:47m:55s remains)
2017-12-16 07:57:18.291958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0725765 -2.3029413 -2.37277 -2.4053109 -2.325912 -2.2165003 -2.1717405 -2.2058897 -2.290463 -2.3851116 -2.4425406 -2.4043713 -2.3040249 -2.1994843 -2.1249998][-2.1933711 -2.4697282 -2.5468936 -2.5188742 -2.3511474 -2.1480341 -2.0224886 -2.0192814 -2.1347628 -2.3564882 -2.5339892 -2.5647907 -2.4727902 -2.3262167 -2.214267][-2.3389997 -2.6086719 -2.6022515 -2.4171569 -2.0755382 -1.7401075 -1.5812936 -1.631736 -1.8326627 -2.20977 -2.5389495 -2.6777167 -2.6097517 -2.4603171 -2.3431282][-2.3881159 -2.5910869 -2.4231102 -1.9552402 -1.313252 -0.81708878 -0.69406128 -0.89218861 -1.2692384 -1.8402325 -2.400048 -2.7008848 -2.6956196 -2.5464027 -2.4003332][-2.2582285 -2.3542597 -2.0036957 -1.2104805 -0.23685813 0.42882156 0.51460314 0.12445784 -0.50362295 -1.3421404 -2.1185341 -2.5926116 -2.64809 -2.4422867 -2.2004256][-2.0560074 -2.0577555 -1.5411409 -0.46486235 0.82006526 1.6684456 1.628242 1.0122247 0.16263354 -0.85898131 -1.7493799 -2.2764723 -2.3319895 -2.0138664 -1.5604959][-1.8640246 -1.7806565 -1.1868671 -0.019817948 1.4659424 2.564877 2.5543571 1.7632203 0.73794866 -0.31649435 -1.1902078 -1.6448555 -1.6421381 -1.181841 -0.50527281][-1.5747801 -1.4292313 -0.89785182 0.11821866 1.5136614 2.667294 2.8207636 2.124954 1.1179976 0.1764307 -0.53086072 -0.82767063 -0.6945399 -0.14064503 0.69042015][-1.0243993 -0.7828458 -0.36972249 0.36892092 1.4288301 2.3149438 2.4468863 1.9062514 1.1434047 0.46883321 0.012358665 -0.15121806 0.039369702 0.5488658 1.2073767][-0.26824236 0.10608244 0.36853969 0.72038937 1.2885878 1.750675 1.7200446 1.3697646 0.92304206 0.51207232 0.21642435 0.089188457 0.14438772 0.40161097 0.76833034][0.40791535 0.82050562 0.910897 0.87119937 0.90309644 0.97707129 0.936347 0.73116112 0.41850936 0.1207968 -0.077104688 -0.16194463 -0.1269697 0.0548563 0.294438][0.62572908 1.0403347 1.0325949 0.79313517 0.53068495 0.36024904 0.2882911 0.15073514 -0.1069901 -0.33464992 -0.43122756 -0.40681326 -0.30370772 -0.12350118 0.031811357][0.3226794 0.79516387 0.85199404 0.6359334 0.27276814 -0.024391055 -0.16162324 -0.24766064 -0.44127488 -0.61625773 -0.6641314 -0.55627257 -0.37261307 -0.16051805 -0.017434359][0.047446251 0.54912114 0.69822693 0.57120538 0.20861113 -0.15541685 -0.37903249 -0.48929298 -0.61781216 -0.70064437 -0.68655908 -0.54801613 -0.3295697 -0.10568714 0.015044212][0.0039865971 0.53665543 0.77835941 0.7467308 0.4346292 0.027413607 -0.299325 -0.473576 -0.57562369 -0.6224165 -0.573036 -0.44112206 -0.26239824 -0.091430783 -0.015918255]]...]
INFO - root - 2017-12-16 07:57:19.968386: step 3110, loss = 0.64, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 15h:18m:59s remains)
INFO - root - 2017-12-16 07:57:21.630915: step 3120, loss = 0.63, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 15h:15m:14s remains)
INFO - root - 2017-12-16 07:57:23.286950: step 3130, loss = 0.68, batch loss = 0.40 (49.3 examples/sec; 0.162 sec/batch; 14h:50m:57s remains)
INFO - root - 2017-12-16 07:57:24.947010: step 3140, loss = 0.65, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 15h:16m:42s remains)
INFO - root - 2017-12-16 07:57:26.625581: step 3150, loss = 0.63, batch loss = 0.35 (47.7 examples/sec; 0.168 sec/batch; 15h:20m:32s remains)
INFO - root - 2017-12-16 07:57:28.282529: step 3160, loss = 0.64, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 15h:07m:02s remains)
INFO - root - 2017-12-16 07:57:29.916400: step 3170, loss = 0.71, batch loss = 0.43 (48.7 examples/sec; 0.164 sec/batch; 15h:02m:20s remains)
INFO - root - 2017-12-16 07:57:31.613675: step 3180, loss = 0.66, batch loss = 0.38 (46.8 examples/sec; 0.171 sec/batch; 15h:37m:52s remains)
INFO - root - 2017-12-16 07:57:33.282968: step 3190, loss = 0.73, batch loss = 0.46 (47.6 examples/sec; 0.168 sec/batch; 15h:22m:59s remains)
INFO - root - 2017-12-16 07:57:34.932531: step 3200, loss = 0.64, batch loss = 0.36 (48.3 examples/sec; 0.166 sec/batch; 15h:08m:54s remains)
2017-12-16 07:57:35.432049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9758937 -2.0231848 -2.2090921 -2.4154031 -2.5903091 -2.6401868 -2.4109752 -1.8922871 -1.3739139 -1.2461327 -1.4539549 -1.6168386 -1.5108136 -1.3148574 -1.1044853][-2.0490599 -2.2091582 -2.4679022 -2.6729016 -2.7982607 -2.789742 -2.5031638 -1.9072722 -1.3537608 -1.1643031 -1.2487419 -1.391215 -1.3650906 -1.2361907 -1.0846722][-2.3204525 -2.5048351 -2.7277806 -2.8546004 -2.8742366 -2.7952728 -2.442575 -1.8087339 -1.2583609 -1.0993479 -1.2235411 -1.4262183 -1.4648639 -1.3533968 -1.1838439][-2.6712551 -2.7318058 -2.8142886 -2.7923827 -2.7010095 -2.5560207 -2.153096 -1.5230236 -1.0539895 -0.97809285 -1.2077154 -1.5342854 -1.6386062 -1.525636 -1.31822][-2.9328418 -2.8369441 -2.6869707 -2.4634807 -2.1984766 -1.9182701 -1.4536921 -0.8649317 -0.52894467 -0.67625552 -1.1594528 -1.6159083 -1.7628366 -1.6473817 -1.4196312][-2.8056664 -2.5626247 -2.2056942 -1.7569263 -1.2569093 -0.7569828 -0.21530926 0.28008568 0.38930583 -0.065926194 -0.85495222 -1.5027156 -1.7374498 -1.6620283 -1.4374682][-2.3586922 -1.9936645 -1.4408497 -0.75931311 0.0084283352 0.74438667 1.3440473 1.7007837 1.5247464 0.70908928 -0.39317286 -1.2410015 -1.5977511 -1.5966897 -1.3989693][-1.867003 -1.4517044 -0.79395813 0.038355708 0.97398567 1.9721568 2.7504802 3.0134454 2.6010075 1.4825394 0.13852489 -0.89394063 -1.4144832 -1.50454 -1.3567475][-1.5786809 -1.2029779 -0.619433 0.15590441 1.0548995 2.1117394 3.0128112 3.3867989 3.0186596 1.8540647 0.45499611 -0.688644 -1.344044 -1.5178146 -1.3965298][-1.5341076 -1.2527738 -0.82563609 -0.2475034 0.45235848 1.2788258 2.0690994 2.5815053 2.39075 1.441823 0.21695673 -0.8302958 -1.464802 -1.6224918 -1.4926671][-1.8191197 -1.5894288 -1.2653239 -0.80065739 -0.20872831 0.47013068 1.1793947 1.6843524 1.5566173 0.76197004 -0.28297472 -1.1488 -1.6617016 -1.7427464 -1.579487][-2.0512009 -1.9047959 -1.6857009 -1.3176768 -0.757555 -0.11082375 0.59057713 1.1099713 1.0657468 0.39415836 -0.51927114 -1.2923799 -1.7148049 -1.7811633 -1.6216793][-1.9313948 -1.9625052 -1.959591 -1.800348 -1.3617191 -0.72064674 0.070108891 0.79119062 0.96498084 0.46101236 -0.3728354 -1.147082 -1.5890658 -1.7050464 -1.5849606][-1.42562 -1.6333394 -1.8955824 -2.0007455 -1.7661558 -1.2127588 -0.32418275 0.64209509 1.0692885 0.736202 -0.056589246 -0.88108248 -1.3894956 -1.5791057 -1.5148119][-1.0504674 -1.4097269 -1.8376901 -2.0936706 -1.9967448 -1.5066794 -0.50986266 0.67963266 1.3188291 1.0730388 0.24561894 -0.674375 -1.2724117 -1.5301043 -1.5007716]]...]
INFO - root - 2017-12-16 07:57:37.086094: step 3210, loss = 0.66, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 15h:01m:44s remains)
INFO - root - 2017-12-16 07:57:38.760837: step 3220, loss = 0.61, batch loss = 0.33 (45.7 examples/sec; 0.175 sec/batch; 16h:01m:23s remains)
INFO - root - 2017-12-16 07:57:40.433781: step 3230, loss = 0.69, batch loss = 0.42 (45.8 examples/sec; 0.175 sec/batch; 15h:57m:54s remains)
INFO - root - 2017-12-16 07:57:42.082272: step 3240, loss = 0.62, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 15h:01m:23s remains)
INFO - root - 2017-12-16 07:57:43.714618: step 3250, loss = 0.63, batch loss = 0.36 (49.1 examples/sec; 0.163 sec/batch; 14h:54m:45s remains)
INFO - root - 2017-12-16 07:57:45.394430: step 3260, loss = 0.69, batch loss = 0.42 (46.6 examples/sec; 0.172 sec/batch; 15h:41m:15s remains)
INFO - root - 2017-12-16 07:57:47.058817: step 3270, loss = 0.71, batch loss = 0.43 (48.8 examples/sec; 0.164 sec/batch; 14h:59m:34s remains)
INFO - root - 2017-12-16 07:57:48.698484: step 3280, loss = 0.71, batch loss = 0.43 (46.0 examples/sec; 0.174 sec/batch; 15h:53m:40s remains)
INFO - root - 2017-12-16 07:57:50.367730: step 3290, loss = 0.60, batch loss = 0.32 (46.3 examples/sec; 0.173 sec/batch; 15h:48m:52s remains)
INFO - root - 2017-12-16 07:57:52.058410: step 3300, loss = 0.85, batch loss = 0.57 (46.7 examples/sec; 0.171 sec/batch; 15h:40m:18s remains)
2017-12-16 07:57:52.574394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3448956 -2.6213379 -2.7680831 -2.6059351 -2.2779422 -2.0795739 -2.0332744 -1.9314926 -1.6305496 -0.77699381 0.33878779 1.330945 1.7876139 1.5146964 0.66696382][-2.0412676 -2.3453283 -2.6800346 -2.6796329 -2.4395096 -2.2117429 -2.0899184 -1.9843254 -1.728653 -0.99614072 0.016083002 0.94761491 1.4623721 1.2777178 0.59460497][-1.7010401 -2.0407126 -2.5365605 -2.7185116 -2.5494626 -2.2193336 -1.9462395 -1.7515149 -1.6106435 -1.2819787 -0.70237756 -0.0870229 0.32175589 0.24129665 -0.15372694][-1.3308736 -1.8041183 -2.3795495 -2.7008042 -2.5310884 -1.9851583 -1.4267424 -1.0824567 -1.030211 -1.0961083 -1.0454072 -0.89921284 -0.7843861 -0.83166182 -1.0317595][-1.043156 -1.5617841 -2.16593 -2.4677162 -2.2212014 -1.4131771 -0.50470078 0.035075665 0.037786126 -0.33522153 -0.78429985 -1.1374305 -1.4044939 -1.5588003 -1.675442][-0.6104936 -1.1743839 -1.7380084 -2.0035775 -1.6497602 -0.631548 0.63270235 1.3924994 1.3126533 0.63103533 -0.23770535 -1.0071299 -1.5496783 -1.8206711 -1.9210289][-0.040149927 -0.61944532 -1.2255841 -1.4766743 -1.0662043 0.068110824 1.5116909 2.3513823 2.1857541 1.2813962 0.13996625 -0.83104432 -1.4470338 -1.7320507 -1.7976587][0.15785718 -0.35722196 -1.0257463 -1.3961775 -1.1085516 -0.015180111 1.387145 2.1745241 1.887692 0.96657109 -0.10353851 -0.9693585 -1.47219 -1.6665765 -1.6752692][-0.1194551 -0.6304552 -1.3430133 -1.8430481 -1.7870244 -1.0757812 0.0028095245 0.676945 0.44650817 -0.19068086 -0.91235483 -1.4434761 -1.7307748 -1.7743849 -1.7082274][-0.52236938 -0.9478991 -1.530899 -2.002038 -2.192555 -2.0058966 -1.5668426 -1.2317163 -1.2841067 -1.5720669 -1.8732129 -2.0474069 -2.0588536 -1.9299251 -1.7607161][-0.49708116 -0.75743705 -1.1079566 -1.4867749 -1.8654196 -2.1305075 -2.3624754 -2.5024958 -2.6074669 -2.722054 -2.7332444 -2.6235981 -2.4291294 -2.1671357 -1.8803531][-0.39937842 -0.45135379 -0.5187552 -0.73380965 -1.1479007 -1.7371662 -2.4551673 -3.018034 -3.3124671 -3.4119704 -3.3052397 -3.067616 -2.7940001 -2.4551549 -2.0348854][-0.49306095 -0.41937447 -0.18328536 -0.12549853 -0.41749489 -1.0867229 -2.0111527 -2.8552036 -3.3331909 -3.4435701 -3.3689494 -3.1778049 -2.9670238 -2.6956487 -2.2117193][-0.54614729 -0.44133162 -0.066120744 0.21770859 0.12002587 -0.51069629 -1.4538451 -2.3477547 -2.8502378 -2.9456387 -2.8842382 -2.7754817 -2.6906676 -2.5991917 -2.1862645][-0.49497163 -0.47246802 -0.16918588 0.14804149 0.15342975 -0.35033524 -1.1454219 -1.8911616 -2.2905142 -2.3041482 -2.1648068 -2.0549722 -2.1123269 -2.247571 -2.0919771]]...]
INFO - root - 2017-12-16 07:57:54.262977: step 3310, loss = 0.70, batch loss = 0.42 (45.6 examples/sec; 0.175 sec/batch; 16h:02m:16s remains)
INFO - root - 2017-12-16 07:57:55.946279: step 3320, loss = 0.76, batch loss = 0.48 (46.9 examples/sec; 0.170 sec/batch; 15h:35m:14s remains)
INFO - root - 2017-12-16 07:57:57.644349: step 3330, loss = 0.67, batch loss = 0.39 (47.5 examples/sec; 0.168 sec/batch; 15h:23m:10s remains)
INFO - root - 2017-12-16 07:57:59.306445: step 3340, loss = 0.61, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 15h:15m:40s remains)
INFO - root - 2017-12-16 07:58:00.968933: step 3350, loss = 0.59, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 15h:27m:05s remains)
INFO - root - 2017-12-16 07:58:02.622011: step 3360, loss = 0.65, batch loss = 0.37 (48.3 examples/sec; 0.166 sec/batch; 15h:08m:33s remains)
INFO - root - 2017-12-16 07:58:04.271778: step 3370, loss = 0.64, batch loss = 0.36 (48.7 examples/sec; 0.164 sec/batch; 15h:00m:50s remains)
INFO - root - 2017-12-16 07:58:05.928308: step 3380, loss = 0.63, batch loss = 0.35 (48.9 examples/sec; 0.164 sec/batch; 14h:56m:57s remains)
INFO - root - 2017-12-16 07:58:07.580285: step 3390, loss = 0.67, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 15h:15m:39s remains)
INFO - root - 2017-12-16 07:58:09.254256: step 3400, loss = 0.77, batch loss = 0.49 (47.5 examples/sec; 0.168 sec/batch; 15h:23m:57s remains)
2017-12-16 07:58:09.780243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1928551 -1.1695324 -1.1853924 -1.2409886 -1.2985088 -1.3289216 -1.3418331 -1.3640902 -1.393253 -1.4014399 -1.3782694 -1.3077543 -1.1983442 -1.1052847 -1.0515795][-1.3099124 -1.2247665 -1.1923523 -1.2144297 -1.3088294 -1.4174694 -1.4882457 -1.5381998 -1.5848886 -1.6041677 -1.559938 -1.4476559 -1.2837553 -1.1364493 -1.0404246][-1.4590899 -1.3074672 -1.2167431 -1.2086601 -1.3209273 -1.516564 -1.6632825 -1.7498877 -1.7956821 -1.81282 -1.7688355 -1.6463751 -1.4545614 -1.2664148 -1.1205419][-1.5127785 -1.3140066 -1.1812229 -1.1646874 -1.3158408 -1.5790483 -1.8170326 -1.9437811 -1.9664743 -1.9700366 -1.9283819 -1.8239552 -1.6294236 -1.4119756 -1.2396606][-1.438641 -1.2051861 -1.0646933 -1.0901928 -1.3315222 -1.6901166 -2.0005794 -2.1454391 -2.1270444 -2.0875249 -2.0477464 -1.9693239 -1.7953361 -1.567621 -1.359077][-1.3193567 -1.076601 -0.97117209 -1.072186 -1.4016008 -1.8308277 -2.1776505 -2.3025455 -2.2125385 -2.1197357 -2.0934346 -2.0379956 -1.9004501 -1.699342 -1.4756181][-1.2143209 -1.0126735 -0.96681428 -1.1242638 -1.4801414 -1.9182394 -2.2453856 -2.301054 -2.1250584 -1.9779936 -1.9814968 -2.0046554 -1.9439864 -1.7983391 -1.5839165][-1.230926 -1.066164 -1.0713217 -1.255221 -1.5697283 -1.9515381 -2.2133334 -2.1864276 -1.895622 -1.7023228 -1.7575771 -1.9073229 -1.9535419 -1.8682559 -1.6580364][-1.4454966 -1.2853978 -1.292019 -1.4456744 -1.6681625 -1.9526281 -2.1144831 -1.9802191 -1.5739455 -1.3514315 -1.4863032 -1.7611105 -1.9330319 -1.9093388 -1.6960571][-1.751856 -1.6195041 -1.5761744 -1.6788223 -1.8051934 -1.9318805 -1.9508095 -1.6887608 -1.1955895 -0.9711501 -1.1863037 -1.5715392 -1.8521883 -1.9014559 -1.6973572][-2.0433195 -1.9585221 -1.8981383 -1.8942105 -1.8858782 -1.8563516 -1.7647833 -1.403303 -0.83661354 -0.62292564 -0.906411 -1.3594137 -1.7224233 -1.8339975 -1.6613595][-2.2668107 -2.194876 -2.1138914 -2.0391531 -1.9332293 -1.7839525 -1.5748082 -1.1583432 -0.62160391 -0.45742095 -0.80417585 -1.2802521 -1.6436543 -1.7630146 -1.6113863][-2.3069119 -2.232707 -2.1563768 -2.0594611 -1.9060373 -1.6717644 -1.3739307 -0.9515816 -0.5021466 -0.44345784 -0.83348322 -1.3137441 -1.6362963 -1.7184114 -1.5747058][-2.1373603 -2.0515327 -1.9829103 -1.9059976 -1.7552471 -1.5138533 -1.2080786 -0.81964111 -0.471761 -0.502929 -0.93431884 -1.3968031 -1.6775945 -1.7090467 -1.5585717][-1.8797642 -1.7973814 -1.7269285 -1.6733406 -1.5577193 -1.3588231 -1.1000113 -0.78430259 -0.56569135 -0.66449142 -1.0825975 -1.4969138 -1.7083043 -1.6753957 -1.5125662]]...]
INFO - root - 2017-12-16 07:58:11.483848: step 3410, loss = 0.64, batch loss = 0.36 (48.6 examples/sec; 0.164 sec/batch; 15h:01m:56s remains)
INFO - root - 2017-12-16 07:58:13.134949: step 3420, loss = 0.60, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 15h:09m:52s remains)
INFO - root - 2017-12-16 07:58:14.811618: step 3430, loss = 0.59, batch loss = 0.31 (48.1 examples/sec; 0.166 sec/batch; 15h:11m:36s remains)
INFO - root - 2017-12-16 07:58:16.492392: step 3440, loss = 0.89, batch loss = 0.62 (47.6 examples/sec; 0.168 sec/batch; 15h:21m:01s remains)
INFO - root - 2017-12-16 07:58:18.190033: step 3450, loss = 0.65, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 15h:05m:57s remains)
INFO - root - 2017-12-16 07:58:19.822677: step 3460, loss = 0.66, batch loss = 0.38 (51.3 examples/sec; 0.156 sec/batch; 14h:15m:29s remains)
INFO - root - 2017-12-16 07:58:21.491192: step 3470, loss = 0.81, batch loss = 0.53 (47.6 examples/sec; 0.168 sec/batch; 15h:21m:15s remains)
INFO - root - 2017-12-16 07:58:23.169720: step 3480, loss = 0.62, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 15h:00m:03s remains)
INFO - root - 2017-12-16 07:58:24.869487: step 3490, loss = 0.57, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 15h:16m:39s remains)
INFO - root - 2017-12-16 07:58:26.553018: step 3500, loss = 0.55, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 15h:29m:50s remains)
2017-12-16 07:58:27.072709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1278157 -2.3077636 -2.5345082 -2.752512 -2.8971362 -3.0125644 -3.0144424 -2.9100704 -2.7771115 -2.6910963 -2.6585915 -2.601126 -2.488471 -2.306778 -2.0760155][-2.2016516 -2.4754257 -2.8743386 -3.2648168 -3.549427 -3.753093 -3.7659695 -3.5818715 -3.3768504 -3.2264881 -3.1315346 -3.0186753 -2.8356457 -2.5549445 -2.2192779][-2.1554275 -2.4129424 -2.8905406 -3.426302 -3.8782632 -4.1491165 -4.145215 -3.9925668 -3.788445 -3.6358426 -3.5312543 -3.3606384 -3.0746002 -2.7053673 -2.2783594][-1.9054124 -2.0102341 -2.3734672 -2.8641665 -3.3356957 -3.6936953 -3.6722639 -3.5792441 -3.4878192 -3.497201 -3.4492483 -3.2636938 -2.9752154 -2.5722666 -2.1506195][-1.4865268 -1.2350471 -1.2730231 -1.4829875 -1.7974863 -2.0914714 -2.0101528 -1.8934481 -2.0083222 -2.3167257 -2.5831714 -2.6441967 -2.4979749 -2.2238088 -1.9387355][-0.94998068 -0.36793292 0.064484358 0.21652186 0.26600516 0.41161907 0.94974852 1.2415416 0.71533108 -0.10710478 -0.83352166 -1.4045786 -1.69096 -1.6588051 -1.5994583][-0.21065557 0.5976367 1.2938414 1.7487545 2.2236464 3.0848956 4.6305351 5.494194 4.0754795 2.4658027 1.1858382 0.15952671 -0.55078232 -0.88333708 -1.1043832][0.323673 1.2466562 2.137774 2.7066355 3.4209623 4.6955519 7.0191431 8.6529684 6.288229 4.1259737 2.7169514 1.4994595 0.53464055 -0.0037200451 -0.49368143][-0.025393128 0.75098133 1.395324 1.7454066 2.2518535 3.2324233 4.700305 5.3523936 4.0499511 2.8244739 2.2081518 1.4136653 0.73971772 0.35423839 -0.18060338][-1.1597168 -0.70333081 -0.30385244 -0.11410284 0.057829618 0.39789891 0.89758873 0.94176221 0.48205376 0.3525939 0.44552493 0.25383997 0.11040688 0.078650713 -0.36475921][-2.2844706 -2.1974342 -2.0645773 -2.0751789 -2.13481 -2.1575511 -2.1537991 -2.3401859 -2.4931881 -2.173089 -1.7200598 -1.4794666 -1.2139697 -0.90706575 -1.0758944][-2.8307848 -2.989857 -3.0593886 -3.1783152 -3.3495746 -3.5115809 -3.7074964 -3.9340036 -3.9412971 -3.6115158 -3.1710641 -2.8359008 -2.4073176 -1.9355018 -1.8660002][-2.8909364 -3.1285295 -3.2708292 -3.4195025 -3.5388558 -3.6492317 -3.8252406 -4.0375991 -4.0145955 -3.795269 -3.4539046 -3.2024279 -2.852874 -2.4212337 -2.2179804][-2.5687766 -2.7502007 -2.8472667 -2.9120314 -2.9333508 -2.9894009 -3.0992599 -3.1986337 -3.2127645 -3.1498241 -3.0381923 -2.908715 -2.68545 -2.4278545 -2.2449088][-2.0972722 -2.1790574 -2.2077513 -2.2180123 -2.2072937 -2.2282963 -2.2735815 -2.3105142 -2.3335781 -2.36056 -2.3946996 -2.3643587 -2.2444556 -2.1398888 -2.0440819]]...]
INFO - root - 2017-12-16 07:58:28.741832: step 3510, loss = 0.71, batch loss = 0.44 (48.6 examples/sec; 0.165 sec/batch; 15h:03m:24s remains)
INFO - root - 2017-12-16 07:58:30.370367: step 3520, loss = 0.69, batch loss = 0.41 (49.4 examples/sec; 0.162 sec/batch; 14h:47m:53s remains)
INFO - root - 2017-12-16 07:58:32.036689: step 3530, loss = 0.67, batch loss = 0.39 (48.8 examples/sec; 0.164 sec/batch; 14h:58m:09s remains)
INFO - root - 2017-12-16 07:58:33.713830: step 3540, loss = 0.52, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 14h:57m:17s remains)
INFO - root - 2017-12-16 07:58:35.376683: step 3550, loss = 0.65, batch loss = 0.37 (48.2 examples/sec; 0.166 sec/batch; 15h:10m:18s remains)
INFO - root - 2017-12-16 07:58:37.042477: step 3560, loss = 0.65, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 15h:14m:31s remains)
INFO - root - 2017-12-16 07:58:38.700796: step 3570, loss = 0.52, batch loss = 0.24 (47.3 examples/sec; 0.169 sec/batch; 15h:27m:21s remains)
INFO - root - 2017-12-16 07:58:40.339368: step 3580, loss = 0.70, batch loss = 0.43 (48.7 examples/sec; 0.164 sec/batch; 15h:00m:01s remains)
INFO - root - 2017-12-16 07:58:42.008230: step 3590, loss = 0.63, batch loss = 0.36 (47.9 examples/sec; 0.167 sec/batch; 15h:15m:05s remains)
INFO - root - 2017-12-16 07:58:43.679746: step 3600, loss = 0.64, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 15h:05m:47s remains)
2017-12-16 07:58:44.175333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2137042 -1.0727329 -1.0422353 -1.077266 -1.0792884 -1.0383511 -0.98846686 -0.95012707 -0.92707 -0.92277288 -0.94472855 -0.98120517 -0.99532086 -0.95421016 -0.86514604][-1.3363405 -1.1586477 -1.1125832 -1.1505276 -1.1245641 -1.0277293 -0.93017626 -0.90235823 -0.95123309 -1.0799123 -1.2504381 -1.4141535 -1.5489943 -1.5599601 -1.450092][-1.8613353 -1.7070833 -1.6845318 -1.7453098 -1.7264955 -1.6238816 -1.523266 -1.4983439 -1.5937769 -1.773681 -2.0046906 -2.2104659 -2.3868222 -2.443604 -2.3495078][-2.2894278 -2.160008 -2.1477199 -2.2038863 -2.1716835 -2.0147202 -1.8416246 -1.7352986 -1.8094788 -2.0360365 -2.3175836 -2.5945039 -2.8144555 -2.8826075 -2.774322][-2.3469603 -2.1869938 -2.130981 -2.0988054 -1.9650319 -1.6757717 -1.3697692 -1.1122158 -1.1051552 -1.3554249 -1.7030324 -2.057725 -2.3603444 -2.4995632 -2.4424381][-2.1521783 -1.9406526 -1.7793729 -1.5690813 -1.2876236 -0.89000243 -0.46882105 -0.074128866 0.042191029 -0.18016624 -0.5196811 -0.88672143 -1.2597681 -1.5131627 -1.5247138][-1.6737871 -1.4294149 -1.1887172 -0.96398777 -0.69251084 -0.32999551 0.06380403 0.54622173 0.8611021 0.83341312 0.54631495 0.10804617 -0.390715 -0.82025051 -1.0572953][-1.227133 -1.0572636 -0.88012981 -0.74459124 -0.57964569 -0.34389687 -0.031598926 0.402835 0.82995582 0.97358704 0.78080416 0.40442276 -0.065743208 -0.55958593 -0.93205887][-1.157987 -1.2403725 -1.2987537 -1.2583908 -1.1553285 -1.0207528 -0.78182846 -0.42169237 -0.0071187019 0.25337577 0.22901964 0.028618455 -0.30034709 -0.66394067 -1.0113983][-1.2083546 -1.5687621 -1.8620956 -1.9864789 -1.985587 -1.9065349 -1.7535937 -1.5379483 -1.2239345 -0.954337 -0.82933784 -0.79502231 -0.84355426 -0.93476945 -1.0881758][-1.3249476 -1.7429335 -2.1410356 -2.3425739 -2.3579736 -2.2930353 -2.2153957 -2.093677 -1.876961 -1.6314437 -1.4231865 -1.2532468 -1.1244439 -1.0395434 -1.0625985][-1.0610194 -1.3718425 -1.7157391 -1.9258538 -1.9688922 -1.9566834 -1.93883 -1.8986485 -1.7885431 -1.5882335 -1.3904927 -1.2342901 -1.0614672 -0.93736726 -0.93043536][-0.76545107 -0.83436084 -1.0235958 -1.1877356 -1.2303108 -1.2277787 -1.2361659 -1.2367046 -1.1706796 -1.0073136 -0.8638109 -0.79359335 -0.72577119 -0.67465454 -0.67637616][-0.48606884 -0.3779906 -0.39410245 -0.46305668 -0.48210633 -0.46463156 -0.45933127 -0.45763278 -0.42249858 -0.33569992 -0.24528682 -0.22672892 -0.24669623 -0.24937904 -0.23438895][-0.29663086 -0.12299514 -0.10764766 -0.16581678 -0.1679852 -0.13779688 -0.12068772 -0.11441863 -0.10373509 -0.072153807 -0.038063765 -0.035644889 -0.034157634 0.0026991367 0.063703537]]...]
INFO - root - 2017-12-16 07:58:45.850324: step 3610, loss = 0.74, batch loss = 0.46 (47.8 examples/sec; 0.167 sec/batch; 15h:17m:48s remains)
INFO - root - 2017-12-16 07:58:47.490301: step 3620, loss = 0.62, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 15h:07m:35s remains)
INFO - root - 2017-12-16 07:58:49.151186: step 3630, loss = 0.72, batch loss = 0.44 (48.5 examples/sec; 0.165 sec/batch; 15h:03m:36s remains)
INFO - root - 2017-12-16 07:58:50.800749: step 3640, loss = 0.59, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 15h:16m:18s remains)
INFO - root - 2017-12-16 07:58:52.434651: step 3650, loss = 0.71, batch loss = 0.43 (49.3 examples/sec; 0.162 sec/batch; 14h:49m:11s remains)
INFO - root - 2017-12-16 07:58:54.088407: step 3660, loss = 0.66, batch loss = 0.38 (49.4 examples/sec; 0.162 sec/batch; 14h:48m:03s remains)
INFO - root - 2017-12-16 07:58:55.729196: step 3670, loss = 0.63, batch loss = 0.35 (50.6 examples/sec; 0.158 sec/batch; 14h:26m:27s remains)
INFO - root - 2017-12-16 07:58:57.456058: step 3680, loss = 0.58, batch loss = 0.30 (46.7 examples/sec; 0.171 sec/batch; 15h:38m:41s remains)
INFO - root - 2017-12-16 07:58:59.102579: step 3690, loss = 0.61, batch loss = 0.33 (48.3 examples/sec; 0.165 sec/batch; 15h:06m:54s remains)
INFO - root - 2017-12-16 07:59:00.751741: step 3700, loss = 0.57, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 14h:51m:13s remains)
2017-12-16 07:59:01.234664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2416116 -0.76245236 -0.643277 -0.80425864 -1.1600814 -1.6053345 -1.8918772 -1.9458201 -1.8833503 -1.8162664 -1.73936 -1.6483836 -1.575316 -1.5256431 -1.4325602][-0.1190002 0.5110333 0.66667485 0.41449559 -0.11219192 -0.72383964 -1.1193104 -1.2488246 -1.2558351 -1.26967 -1.2914212 -1.2786503 -1.2787223 -1.2827885 -1.212406][0.7305932 1.4136007 1.5883813 1.3284376 0.78092122 0.16199505 -0.26297045 -0.39651167 -0.38536632 -0.4165113 -0.48618531 -0.57137978 -0.66473508 -0.68125743 -0.585282][0.6196959 1.2004836 1.3644872 1.1732435 0.72303319 0.16685879 -0.25734472 -0.3572737 -0.27988923 -0.24270487 -0.26584303 -0.32478023 -0.37700808 -0.31149471 -0.12817717][0.29838085 0.74666905 0.92970395 0.85858035 0.5322206 0.038195729 -0.40241385 -0.57315755 -0.57112086 -0.60150152 -0.68930036 -0.73804927 -0.66151071 -0.4250747 -0.04820025][0.17114353 0.61965585 0.83007836 0.8357017 0.60845995 0.25177824 -0.095305204 -0.31027639 -0.4562757 -0.70864826 -1.0478432 -1.264172 -1.218061 -0.91617322 -0.45118296][-0.038946986 0.34178925 0.53405547 0.55145645 0.46311212 0.36167288 0.30954397 0.28671372 0.14951646 -0.2596761 -0.87705916 -1.3488971 -1.4811286 -1.2773232 -0.90022469][-0.3990823 -0.14895797 -0.034244657 -0.040460229 -0.035820603 0.10429096 0.39553165 0.67211962 0.74343991 0.39514208 -0.31313562 -0.97648883 -1.3040943 -1.2617435 -1.0771534][-0.48230517 -0.36507189 -0.36427867 -0.4787147 -0.54797447 -0.4288013 -0.067279816 0.35008204 0.64306664 0.550349 0.038182974 -0.54190183 -0.89457905 -0.94582468 -0.89064157][-0.27251542 -0.24231243 -0.35663593 -0.62384915 -0.92736119 -1.0664859 -0.93251413 -0.59599012 -0.2169075 -0.084551692 -0.29612577 -0.6121496 -0.78090644 -0.76312965 -0.68223077][-0.12371099 -0.089957476 -0.22389352 -0.59263855 -1.1098695 -1.5197359 -1.6722839 -1.609972 -1.3629205 -1.172492 -1.1625044 -1.195164 -1.138023 -0.99379843 -0.85961318][-0.057964444 0.001136899 -0.15540242 -0.62372154 -1.2892258 -1.8902669 -2.292479 -2.4823897 -2.4220448 -2.291945 -2.1949763 -2.0390599 -1.8173602 -1.5851398 -1.415243][-0.27560377 -0.14835846 -0.22333741 -0.61046928 -1.217463 -1.8694967 -2.4474554 -2.842001 -2.979636 -2.9703536 -2.8875966 -2.6778135 -2.3927672 -2.1579111 -2.0116353][-0.33838332 -0.14181471 -0.10100198 -0.3326056 -0.7820549 -1.3605697 -1.9575531 -2.4075766 -2.6641262 -2.7410746 -2.6892037 -2.4887495 -2.212213 -1.9725637 -1.8596243][-0.47109604 -0.36798179 -0.32430625 -0.388955 -0.50447357 -0.71310765 -1.0256158 -1.3156719 -1.5331886 -1.6646854 -1.7207162 -1.6069268 -1.3850933 -1.1730795 -1.0802534]]...]
INFO - root - 2017-12-16 07:59:02.895532: step 3710, loss = 0.70, batch loss = 0.43 (48.4 examples/sec; 0.165 sec/batch; 15h:05m:23s remains)
INFO - root - 2017-12-16 07:59:04.560706: step 3720, loss = 0.64, batch loss = 0.37 (46.8 examples/sec; 0.171 sec/batch; 15h:37m:11s remains)
INFO - root - 2017-12-16 07:59:06.238147: step 3730, loss = 0.60, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 15h:25m:58s remains)
INFO - root - 2017-12-16 07:59:07.926769: step 3740, loss = 0.55, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 15h:30m:37s remains)
INFO - root - 2017-12-16 07:59:09.647741: step 3750, loss = 0.64, batch loss = 0.36 (43.7 examples/sec; 0.183 sec/batch; 16h:43m:18s remains)
INFO - root - 2017-12-16 07:59:11.308040: step 3760, loss = 0.54, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 15h:15m:02s remains)
INFO - root - 2017-12-16 07:59:12.948607: step 3770, loss = 0.59, batch loss = 0.31 (50.2 examples/sec; 0.159 sec/batch; 14h:33m:52s remains)
INFO - root - 2017-12-16 07:59:14.607235: step 3780, loss = 0.74, batch loss = 0.47 (48.5 examples/sec; 0.165 sec/batch; 15h:03m:48s remains)
INFO - root - 2017-12-16 07:59:16.298929: step 3790, loss = 0.63, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 15h:17m:25s remains)
INFO - root - 2017-12-16 07:59:17.978573: step 3800, loss = 0.57, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 15h:42m:28s remains)
2017-12-16 07:59:18.443344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8418705 -1.9238868 -2.01342 -2.106952 -2.1035 -2.0698264 -1.9465473 -1.716949 -1.3538814 -1.0418301 -1.1666791 -1.7686225 -2.2479448 -2.0794568 -1.1772126][-1.7479012 -1.830757 -1.9320605 -1.9918222 -1.8789641 -1.6341569 -1.3118238 -1.0398531 -0.74858922 -0.48650551 -0.7570219 -1.5726175 -2.2664006 -2.2379584 -1.3563551][-1.7118289 -1.8000917 -1.9029135 -1.9106992 -1.6261345 -1.003401 -0.31754351 0.030589819 0.14635789 0.12913108 -0.38890421 -1.423147 -2.3286853 -2.43367 -1.5274198][-1.7147003 -1.7991534 -1.8597183 -1.8253117 -1.3492657 -0.29646611 0.82183146 1.2919335 1.1245959 0.74923396 -0.070651412 -1.3215172 -2.4313173 -2.6603251 -1.7488204][-1.6237552 -1.7455525 -1.7819068 -1.6867422 -1.0079024 0.44311428 2.013412 2.607661 2.1279044 1.2988758 0.1769408 -1.2103143 -2.4282684 -2.7119627 -1.883868][-1.513261 -1.604031 -1.61722 -1.412004 -0.59353042 1.1773341 3.2451112 4.0942097 3.2062891 1.8277984 0.39752269 -1.091399 -2.3744602 -2.672092 -1.915252][-1.5769573 -1.5585763 -1.5226264 -1.1940786 -0.25675595 1.7493737 4.23067 5.41214 4.0718069 2.2732046 0.58331585 -1.0222709 -2.328362 -2.6423392 -1.9108174][-1.6841714 -1.5902269 -1.5082194 -1.1528091 -0.18253446 1.7707319 4.2899609 5.8008986 4.3275852 2.4396293 0.65561318 -1.0376043 -2.3384895 -2.6654353 -1.9267037][-1.8354144 -1.6699553 -1.6427988 -1.4649823 -0.69125044 0.91541886 2.9893682 4.4790764 3.5827234 2.0901895 0.45512772 -1.2005495 -2.5185251 -2.8119328 -2.0711708][-1.8302183 -1.5947365 -1.6654943 -1.7860113 -1.3546101 -0.18866229 1.3740258 2.6239574 2.3300202 1.3765459 0.10359418 -1.4150277 -2.7075155 -3.0303216 -2.3085904][-1.876544 -1.4813344 -1.6196619 -1.9593492 -1.8393866 -0.9773342 0.16010749 1.2199469 1.2757573 0.71567726 -0.25388563 -1.6483214 -2.9022908 -3.2835987 -2.6085014][-1.9190756 -1.4073322 -1.545012 -1.9958868 -2.0112023 -1.3778807 -0.573926 0.29644632 0.52887154 0.26504266 -0.42049718 -1.6608644 -2.8589435 -3.3038373 -2.7244916][-2.0832665 -1.5900103 -1.6222808 -2.0065868 -2.062644 -1.6197733 -1.0322008 -0.33554137 0.08869791 0.12179077 -0.33381772 -1.2812674 -2.3976684 -2.9425645 -2.5583084][-2.3677444 -2.00253 -1.9639254 -2.1715205 -2.2099586 -1.8523996 -1.4788622 -0.85904664 -0.23322642 0.16988397 0.015090227 -0.64560956 -1.6847318 -2.3373353 -2.2261052][-2.4891481 -2.3036125 -2.3189304 -2.4202809 -2.4198987 -2.1530814 -1.9555535 -1.4965194 -0.78254867 -0.15743637 0.029091 -0.27368605 -1.138923 -1.8174708 -1.8866571]]...]
INFO - root - 2017-12-16 07:59:20.116565: step 3810, loss = 0.73, batch loss = 0.45 (47.9 examples/sec; 0.167 sec/batch; 15h:14m:09s remains)
INFO - root - 2017-12-16 07:59:21.768879: step 3820, loss = 0.61, batch loss = 0.34 (49.2 examples/sec; 0.163 sec/batch; 14h:51m:03s remains)
INFO - root - 2017-12-16 07:59:23.436043: step 3830, loss = 0.64, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 15h:12m:41s remains)
INFO - root - 2017-12-16 07:59:25.127816: step 3840, loss = 0.56, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 15h:22m:07s remains)
INFO - root - 2017-12-16 07:59:26.779131: step 3850, loss = 0.67, batch loss = 0.39 (47.4 examples/sec; 0.169 sec/batch; 15h:24m:06s remains)
INFO - root - 2017-12-16 07:59:28.442535: step 3860, loss = 0.58, batch loss = 0.31 (49.8 examples/sec; 0.161 sec/batch; 14h:39m:28s remains)
INFO - root - 2017-12-16 07:59:30.074995: step 3870, loss = 0.60, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 15h:02m:17s remains)
INFO - root - 2017-12-16 07:59:31.724607: step 3880, loss = 0.67, batch loss = 0.40 (47.7 examples/sec; 0.168 sec/batch; 15h:19m:31s remains)
INFO - root - 2017-12-16 07:59:33.389036: step 3890, loss = 0.71, batch loss = 0.43 (48.3 examples/sec; 0.166 sec/batch; 15h:06m:49s remains)
INFO - root - 2017-12-16 07:59:35.010852: step 3900, loss = 0.65, batch loss = 0.38 (50.2 examples/sec; 0.159 sec/batch; 14h:33m:27s remains)
2017-12-16 07:59:35.453874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6928338 -1.3974172 -1.2535112 -1.1492424 -1.0605639 -1.0503867 -1.0039415 -0.87196469 -0.99699283 -1.194279 -1.2939097 -1.3676447 -1.30755 -1.1361481 -1.0225481][-2.0397058 -1.8576182 -1.7282102 -1.5831254 -1.4559522 -1.4075167 -1.3335047 -1.1449268 -1.1615137 -1.304933 -1.410714 -1.5093147 -1.45909 -1.2225604 -1.0374396][-2.2934153 -2.326611 -2.3208981 -2.196044 -2.0231385 -1.8404027 -1.6031933 -1.237347 -1.0486405 -1.0997734 -1.2829559 -1.4879383 -1.5036455 -1.2676919 -1.0795336][-2.1780257 -2.4850883 -2.634398 -2.5603988 -2.390955 -2.1266611 -1.7044474 -1.0790148 -0.6159153 -0.53019476 -0.82480389 -1.2027954 -1.3999326 -1.3856645 -1.3191459][-1.7211642 -2.2188029 -2.4487531 -2.3724897 -2.1537316 -1.7924653 -1.2501512 -0.50258684 0.20845187 0.52688837 0.26584744 -0.40242732 -1.0813036 -1.4565133 -1.6094799][-1.1874709 -1.6846664 -1.6986338 -1.3998662 -0.99358475 -0.4378227 0.1268369 0.74501705 1.3861952 1.7419569 1.4394999 0.41841984 -0.77673376 -1.5622565 -1.9250124][-0.8529312 -1.1512572 -0.75807256 0.0034067631 0.91175342 1.8815606 2.4466424 2.6428747 2.693758 2.4126434 1.6485059 0.39070368 -0.94920486 -1.814026 -2.1933498][-0.56085324 -0.54207611 0.13606822 1.3766239 2.8426485 4.5341825 5.3273611 4.4214163 3.1618018 1.9915447 0.84416342 -0.30702841 -1.356678 -2.0077436 -2.2558234][-0.43013 -0.11174738 0.67459655 1.9166009 3.3916483 4.9396482 5.3869805 3.9630604 2.2530084 0.89118743 -0.15637481 -0.99318719 -1.6319426 -1.9975545 -2.0964098][-0.41076553 0.14185166 0.81662869 1.5182309 2.1303995 2.5875936 2.5137725 1.6295915 0.39225483 -0.56914246 -1.1608633 -1.5240248 -1.7449862 -1.8348598 -1.8155439][-0.79930604 -0.2275157 0.12093663 0.14667666 0.051096082 -0.14251029 -0.50312424 -1.0912848 -1.7077506 -2.0844908 -2.11552 -1.9538926 -1.789887 -1.6942254 -1.6360216][-1.5178952 -1.1661656 -1.0749612 -1.3779302 -1.8635805 -2.4263465 -2.9234281 -3.2487955 -3.3292828 -3.1925004 -2.8026621 -2.3571484 -2.0193362 -1.8024682 -1.6943028][-2.0520763 -1.9467061 -2.0125146 -2.4786644 -3.132488 -3.7441931 -4.1134272 -4.1210127 -3.8210502 -3.3622642 -2.9051309 -2.5225229 -2.2389293 -2.0397792 -1.8933218][-2.279573 -2.330534 -2.528317 -2.9982047 -3.5478067 -3.9408016 -4.0195794 -3.6837869 -3.1358652 -2.657433 -2.4047019 -2.2928426 -2.242445 -2.205446 -2.1141119][-2.4094677 -2.5125535 -2.7157903 -3.0315557 -3.3137696 -3.3656914 -3.0420294 -2.3778841 -1.7080971 -1.401421 -1.4710491 -1.7476557 -2.0260344 -2.184382 -2.1968372]]...]
INFO - root - 2017-12-16 07:59:37.160633: step 3910, loss = 0.69, batch loss = 0.42 (45.3 examples/sec; 0.177 sec/batch; 16h:07m:48s remains)
INFO - root - 2017-12-16 07:59:38.838255: step 3920, loss = 0.57, batch loss = 0.30 (46.1 examples/sec; 0.174 sec/batch; 15h:50m:25s remains)
INFO - root - 2017-12-16 07:59:40.500481: step 3930, loss = 0.61, batch loss = 0.33 (47.5 examples/sec; 0.169 sec/batch; 15h:23m:02s remains)
INFO - root - 2017-12-16 07:59:42.176997: step 3940, loss = 0.73, batch loss = 0.45 (50.1 examples/sec; 0.160 sec/batch; 14h:34m:52s remains)
INFO - root - 2017-12-16 07:59:43.858081: step 3950, loss = 0.62, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 15h:10m:51s remains)
INFO - root - 2017-12-16 07:59:45.511037: step 3960, loss = 0.69, batch loss = 0.42 (48.6 examples/sec; 0.165 sec/batch; 15h:01m:52s remains)
INFO - root - 2017-12-16 07:59:47.168534: step 3970, loss = 0.60, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 15h:04m:16s remains)
INFO - root - 2017-12-16 07:59:48.808358: step 3980, loss = 0.83, batch loss = 0.56 (49.1 examples/sec; 0.163 sec/batch; 14h:52m:17s remains)
INFO - root - 2017-12-16 07:59:50.501073: step 3990, loss = 0.56, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 15h:20m:12s remains)
INFO - root - 2017-12-16 07:59:52.175053: step 4000, loss = 0.59, batch loss = 0.31 (45.3 examples/sec; 0.176 sec/batch; 16h:05m:58s remains)
2017-12-16 07:59:52.638906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.91302407 -1.071564 -1.3765922 -1.7739065 -2.0879974 -2.1917071 -2.105015 -1.9564481 -1.8178705 -1.7220703 -1.6481614 -1.5972769 -1.5612723 -1.539867 -1.4989761][-1.9018229 -1.9783909 -2.1143689 -2.282249 -2.3778865 -2.3511155 -2.2403417 -2.1240137 -2.0101502 -1.9012811 -1.7852954 -1.6757641 -1.60518 -1.5844512 -1.5644653][-2.4005506 -2.4113708 -2.4476147 -2.5096197 -2.5555511 -2.5509088 -2.4989886 -2.4057293 -2.2667527 -2.1005731 -1.9241287 -1.7747244 -1.7081361 -1.7138308 -1.7097609][-2.5038333 -2.471209 -2.4855008 -2.5765781 -2.6953206 -2.7565246 -2.6860995 -2.4827042 -2.1952629 -1.9002403 -1.6828458 -1.595758 -1.6229467 -1.6958514 -1.7187228][-2.3584752 -2.3382611 -2.3913765 -2.5078683 -2.6095498 -2.5503254 -2.2350488 -1.7252616 -1.2174376 -0.90501368 -0.82633352 -0.97738123 -1.2386441 -1.450255 -1.5203712][-2.0670009 -2.0732672 -2.1244838 -2.1425338 -2.0385289 -1.6255018 -0.83428437 0.062731028 0.65864229 0.72304726 0.39004278 -0.17382753 -0.740737 -1.1030333 -1.2174542][-1.747012 -1.7310838 -1.6285491 -1.3599476 -0.8786056 -0.06873858 1.0992405 2.2745931 2.7211764 2.0881977 1.0513897 0.099243045 -0.58940279 -0.95730114 -1.064543][-1.4622638 -1.3785791 -1.0532618 -0.4626683 0.31774008 1.246578 2.2584321 3.0191481 2.9190414 1.8185952 0.55040455 -0.40675414 -0.93145382 -1.1284006 -1.1380849][-1.2508451 -1.0661789 -0.58583891 0.15008581 0.89665389 1.4498866 1.7250147 1.6236432 1.0680149 0.1501627 -0.745103 -1.32042 -1.4956341 -1.4148802 -1.279279][-1.0579612 -0.7745769 -0.280182 0.24473786 0.51329589 0.43065619 0.10382795 -0.37981057 -0.94954526 -1.5092893 -1.9032829 -2.0023909 -1.8223687 -1.5124277 -1.2450718][-0.80848444 -0.5400455 -0.2647661 -0.15831196 -0.37643862 -0.84703672 -1.3951961 -1.8650212 -2.1903403 -2.3280079 -2.2764285 -2.0473306 -1.6914991 -1.3217436 -1.0665209][-0.66205651 -0.54489374 -0.53269982 -0.70237142 -1.0945878 -1.5925808 -2.0349269 -2.3081017 -2.3684611 -2.2429841 -2.0058713 -1.7069869 -1.3937645 -1.1360335 -0.98982823][-0.63058388 -0.74319071 -0.96089894 -1.2126957 -1.4933715 -1.7640592 -1.9577241 -2.0496504 -2.0071607 -1.848033 -1.6476204 -1.4598098 -1.3164017 -1.2206504 -1.18933][-0.719943 -1.057119 -1.4149612 -1.6219155 -1.6917207 -1.7297072 -1.7518746 -1.7713909 -1.7447295 -1.6695194 -1.5981736 -1.5475345 -1.5222185 -1.496361 -1.4813422][-0.91717416 -1.3816122 -1.7940292 -1.9482411 -1.9035013 -1.8141532 -1.7455364 -1.7046084 -1.6642069 -1.6451439 -1.6607189 -1.7025782 -1.733223 -1.7128096 -1.6662238]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 07:59:54.355996: step 4010, loss = 0.60, batch loss = 0.33 (46.9 examples/sec; 0.171 sec/batch; 15h:33m:38s remains)
INFO - root - 2017-12-16 07:59:56.026496: step 4020, loss = 0.68, batch loss = 0.41 (48.5 examples/sec; 0.165 sec/batch; 15h:02m:20s remains)
INFO - root - 2017-12-16 07:59:57.689718: step 4030, loss = 0.73, batch loss = 0.45 (48.0 examples/sec; 0.167 sec/batch; 15h:12m:35s remains)
INFO - root - 2017-12-16 07:59:59.349599: step 4040, loss = 0.60, batch loss = 0.32 (46.6 examples/sec; 0.172 sec/batch; 15h:40m:13s remains)
INFO - root - 2017-12-16 08:00:01.001793: step 4050, loss = 0.63, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 15h:10m:14s remains)
INFO - root - 2017-12-16 08:00:02.691198: step 4060, loss = 0.61, batch loss = 0.34 (45.2 examples/sec; 0.177 sec/batch; 16h:08m:54s remains)
INFO - root - 2017-12-16 08:00:04.337073: step 4070, loss = 0.57, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 14h:57m:53s remains)
INFO - root - 2017-12-16 08:00:05.995314: step 4080, loss = 0.56, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 15h:00m:28s remains)
INFO - root - 2017-12-16 08:00:07.666534: step 4090, loss = 0.56, batch loss = 0.29 (50.1 examples/sec; 0.160 sec/batch; 14h:33m:45s remains)
INFO - root - 2017-12-16 08:00:09.368612: step 4100, loss = 0.61, batch loss = 0.34 (43.8 examples/sec; 0.183 sec/batch; 16h:40m:14s remains)
2017-12-16 08:00:09.907690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1194506 -1.1262791 -1.2023838 -1.3170995 -1.4392734 -1.5830244 -1.757779 -1.9348053 -2.0552628 -2.081182 -2.0044432 -1.8357936 -1.653734 -1.5105482 -1.408596][-0.82416064 -0.80089933 -0.84096479 -0.89537042 -0.94730306 -1.0493329 -1.270617 -1.5718833 -1.8351858 -1.9519634 -1.8977392 -1.7153349 -1.5235008 -1.410019 -1.3618828][-0.71711385 -0.673792 -0.65947992 -0.62349761 -0.55380535 -0.52738571 -0.64915335 -0.97594976 -1.3416076 -1.5975233 -1.6522274 -1.553993 -1.4281168 -1.3877237 -1.4129455][-0.78121525 -0.7218619 -0.65073472 -0.48217595 -0.18147302 0.16297519 0.33717132 0.1074754 -0.38524485 -0.88302636 -1.2170706 -1.3648272 -1.4275841 -1.5079917 -1.5966799][-0.93677586 -0.86038762 -0.72472471 -0.38494861 0.2422533 1.0629276 1.6981827 1.7474705 1.2117115 0.36037481 -0.44548929 -1.0478047 -1.4486375 -1.697245 -1.8207794][-1.1276224 -1.0565356 -0.888045 -0.42844665 0.48917568 1.8170666 3.1055088 3.7118316 3.3092904 2.1237979 0.73105872 -0.46212125 -1.2705667 -1.7362804 -1.8955162][-1.2600427 -1.2356849 -1.0931039 -0.637282 0.39207852 2.0805235 4.093276 5.4980383 5.395638 3.9526873 2.0359387 0.30452847 -0.8797701 -1.5550696 -1.8050669][-1.2557776 -1.3068993 -1.259045 -0.92057824 0.006637454 1.7590626 4.1571326 6.2187872 6.447114 4.93916 2.8280535 0.8739084 -0.53935218 -1.3489488 -1.6242549][-1.0788742 -1.2307922 -1.3211224 -1.1812332 -0.52248669 0.91150749 3.0127683 4.9185314 5.5042586 4.4358478 2.6307354 0.86774814 -0.47614443 -1.2657098 -1.527938][-0.81447053 -1.0775057 -1.3301243 -1.432531 -1.1390462 -0.2332077 1.2115837 2.6336036 3.3136592 2.8194561 1.573397 0.2121253 -0.84585822 -1.4656082 -1.6621509][-0.594965 -0.94032407 -1.3113722 -1.6194671 -1.6637453 -1.2786416 -0.47108293 0.45676696 1.0471579 0.90471637 0.15613961 -0.76455653 -1.5103588 -1.9242156 -2.0075653][-0.53631449 -0.87499738 -1.2913619 -1.7150294 -1.9975505 -1.9988379 -1.6913018 -1.1981277 -0.80571419 -0.8346563 -1.2412876 -1.7761378 -2.2154465 -2.4338081 -2.4251676][-0.59594917 -0.90706587 -1.3241707 -1.7888005 -2.1767662 -2.3833544 -2.3648071 -2.1740756 -1.9905527 -1.9909588 -2.1894581 -2.4499693 -2.6357617 -2.6789808 -2.5919805][-0.7519024 -1.032249 -1.3925689 -1.800226 -2.1752191 -2.445296 -2.5671964 -2.5439749 -2.4694989 -2.4520674 -2.5087466 -2.5722504 -2.585079 -2.5236835 -2.4042165][-0.84732419 -1.0546559 -1.315774 -1.610222 -1.8986312 -2.1316559 -2.2722435 -2.3169005 -2.3000667 -2.2803721 -2.2720356 -2.2423952 -2.175262 -2.083333 -1.9772269]]...]
INFO - root - 2017-12-16 08:00:11.583152: step 4110, loss = 0.58, batch loss = 0.31 (46.0 examples/sec; 0.174 sec/batch; 15h:51m:56s remains)
INFO - root - 2017-12-16 08:00:13.247087: step 4120, loss = 0.66, batch loss = 0.39 (48.2 examples/sec; 0.166 sec/batch; 15h:07m:51s remains)
INFO - root - 2017-12-16 08:00:14.915443: step 4130, loss = 0.61, batch loss = 0.33 (46.9 examples/sec; 0.170 sec/batch; 15h:32m:33s remains)
INFO - root - 2017-12-16 08:00:16.570846: step 4140, loss = 0.62, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 15h:07m:18s remains)
INFO - root - 2017-12-16 08:00:18.257145: step 4150, loss = 0.71, batch loss = 0.44 (47.8 examples/sec; 0.168 sec/batch; 15h:16m:45s remains)
INFO - root - 2017-12-16 08:00:19.920499: step 4160, loss = 0.66, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 15h:12m:37s remains)
INFO - root - 2017-12-16 08:00:21.598868: step 4170, loss = 0.61, batch loss = 0.33 (47.5 examples/sec; 0.168 sec/batch; 15h:21m:43s remains)
INFO - root - 2017-12-16 08:00:23.275990: step 4180, loss = 0.55, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 15h:06m:12s remains)
INFO - root - 2017-12-16 08:00:24.928630: step 4190, loss = 0.62, batch loss = 0.35 (49.6 examples/sec; 0.161 sec/batch; 14h:42m:29s remains)
INFO - root - 2017-12-16 08:00:26.590383: step 4200, loss = 0.63, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 15h:24m:27s remains)
2017-12-16 08:00:27.089606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5119851 -1.116611 -0.87510258 -0.90363777 -1.081421 -1.3544838 -1.451715 -1.2742819 -1.0160794 -0.708494 -0.46417058 -0.59445965 -0.92781413 -1.2584485 -1.4139148][-1.4148605 -0.98425233 -0.62026083 -0.56296659 -0.86509961 -1.3735187 -1.6785955 -1.5231373 -1.0996983 -0.61149943 -0.24562633 -0.21218622 -0.52307832 -1.0289922 -1.4501374][-1.0682628 -0.68707913 -0.31120932 -0.25265706 -0.66235614 -1.300132 -1.7360579 -1.686964 -1.1572011 -0.50502813 -0.078320384 0.10349941 -0.044945598 -0.45949495 -0.86071682][-0.41815174 -0.18071711 0.10189712 0.095033646 -0.287233 -0.91916662 -1.3373071 -1.2869853 -0.8032316 -0.21644425 0.17250288 0.39847064 0.3973968 0.24271071 0.10613286][0.23397458 0.28791404 0.43582773 0.48317003 0.29300451 -0.11339724 -0.31419814 -0.14635527 0.19370449 0.45846295 0.605145 0.70909762 0.78421879 0.86398172 0.97930074][0.50018191 0.51615858 0.66639686 0.87672353 1.0146801 1.0153413 1.2026565 1.491271 1.5491779 1.3194306 0.949985 0.73428345 0.73446321 0.84133577 1.084461][0.25327265 0.35173249 0.6263814 1.0703032 1.5974295 2.1347017 2.8394189 3.3527288 2.8890433 2.0082407 1.1519151 0.57411551 0.30640137 0.30769086 0.50748038][-0.36749852 -0.17512584 0.26634753 0.91959214 1.7280772 2.6625962 3.7872505 4.6677985 3.5692883 2.1915474 1.0516365 0.22273564 -0.22761595 -0.31767225 -0.207273][-1.1583302 -0.89613777 -0.38599157 0.36539698 1.2432344 2.2322202 3.1401768 3.462563 2.6063757 1.4235957 0.37208533 -0.41386878 -0.84125149 -0.91354823 -0.81661135][-1.6958236 -1.3996693 -0.8738268 -0.17414856 0.5914917 1.3627706 1.8901007 1.8260629 1.1421866 0.25015295 -0.53310835 -1.0873592 -1.3605533 -1.3290416 -1.2279685][-1.8851602 -1.6141051 -1.1445284 -0.56777 0.010115266 0.49565983 0.70678377 0.47101665 -0.087546945 -0.704256 -1.1602751 -1.4419194 -1.5440737 -1.4772288 -1.371196][-1.7937955 -1.6181012 -1.3004484 -0.93762916 -0.58860362 -0.31266391 -0.28472888 -0.56996346 -0.98056054 -1.3147895 -1.4780731 -1.5377886 -1.5301844 -1.4467045 -1.3511682][-1.6559826 -1.607458 -1.4632015 -1.2744983 -1.0760114 -0.9451223 -1.0031588 -1.2726091 -1.5236537 -1.6482906 -1.6398736 -1.5903991 -1.5335778 -1.452484 -1.3488493][-1.5671629 -1.6469126 -1.6715826 -1.5773546 -1.4213798 -1.3681635 -1.4748683 -1.6714838 -1.8421346 -1.8787054 -1.7918676 -1.7127588 -1.6558392 -1.5947036 -1.5030729][-1.5897603 -1.7578661 -1.8733355 -1.8303877 -1.6781862 -1.6192292 -1.7362142 -1.9191716 -2.0782254 -2.1204827 -2.0495253 -2.0010359 -1.9781809 -1.9477487 -1.8751289]]...]
INFO - root - 2017-12-16 08:00:28.749266: step 4210, loss = 0.71, batch loss = 0.43 (48.0 examples/sec; 0.167 sec/batch; 15h:11m:47s remains)
INFO - root - 2017-12-16 08:00:30.412347: step 4220, loss = 0.67, batch loss = 0.40 (46.4 examples/sec; 0.172 sec/batch; 15h:43m:09s remains)
INFO - root - 2017-12-16 08:00:32.084506: step 4230, loss = 0.68, batch loss = 0.41 (46.4 examples/sec; 0.173 sec/batch; 15h:44m:16s remains)
INFO - root - 2017-12-16 08:00:33.750289: step 4240, loss = 0.81, batch loss = 0.53 (49.4 examples/sec; 0.162 sec/batch; 14h:45m:32s remains)
INFO - root - 2017-12-16 08:00:35.410574: step 4250, loss = 0.59, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 15h:29m:16s remains)
INFO - root - 2017-12-16 08:00:37.090421: step 4260, loss = 0.62, batch loss = 0.35 (48.9 examples/sec; 0.164 sec/batch; 14h:54m:50s remains)
INFO - root - 2017-12-16 08:00:38.804397: step 4270, loss = 0.65, batch loss = 0.37 (46.0 examples/sec; 0.174 sec/batch; 15h:52m:10s remains)
INFO - root - 2017-12-16 08:00:40.512852: step 4280, loss = 0.63, batch loss = 0.36 (47.5 examples/sec; 0.169 sec/batch; 15h:21m:57s remains)
INFO - root - 2017-12-16 08:00:42.182589: step 4290, loss = 0.66, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 15h:07m:18s remains)
INFO - root - 2017-12-16 08:00:43.844228: step 4300, loss = 0.60, batch loss = 0.33 (46.9 examples/sec; 0.171 sec/batch; 15h:33m:28s remains)
2017-12-16 08:00:44.324959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2947712 -2.4697416 -2.6543908 -2.9102492 -2.8963823 -2.5822825 -2.1475396 -1.7524742 -1.4528124 -1.3760686 -1.526571 -1.7579783 -1.8906811 -1.8616608 -1.7484902][-2.7377415 -2.9554992 -3.0143805 -2.9974227 -2.6274397 -1.8800493 -1.170476 -0.78362256 -0.76881224 -1.0229099 -1.4499565 -1.93587 -2.2560775 -2.3646502 -2.3480291][-2.9232054 -3.193435 -3.1331782 -2.7592344 -1.9132993 -0.75624126 0.20096672 0.54079044 0.21028852 -0.494846 -1.2776586 -1.9742012 -2.4718671 -2.7133965 -2.7848806][-2.9338205 -3.1965179 -2.9686332 -2.2190533 -0.95873451 0.56320012 1.6161124 1.7014638 1.0184416 -0.059884071 -1.1136845 -1.9322702 -2.4490886 -2.6307654 -2.6974316][-2.7496252 -2.932229 -2.5314705 -1.43657 0.3104707 2.2834067 3.384388 3.0732212 1.7913481 0.18343115 -1.0841289 -1.8178318 -2.1367083 -2.1756122 -2.1424131][-2.4095178 -2.5277858 -2.1026621 -0.89284205 1.2125162 3.7501183 5.2521873 4.7414308 2.7129288 0.42250884 -1.124925 -1.7905636 -1.8189044 -1.5324372 -1.3467852][-2.0914204 -2.1861784 -1.8223183 -0.60506487 1.6657084 4.7491732 7.1494856 6.8865528 4.0268321 0.8562125 -1.1735904 -1.8287535 -1.537884 -0.90278721 -0.58687091][-1.6466669 -1.8319243 -1.5983874 -0.55438876 1.6516095 5.0448422 8.4630642 8.7830038 5.2098784 1.3736144 -1.0707262 -1.8078825 -1.2876778 -0.44204354 -0.081301093][-1.140651 -1.3931516 -1.393314 -0.76436543 0.9182297 3.5935564 6.1262765 6.495266 4.2179418 1.1395468 -1.0198002 -1.6443529 -1.0656515 -0.20905054 0.16603482][-0.49509144 -0.72700453 -0.98036343 -0.82289773 0.15847969 1.8889102 3.5885949 3.9934163 2.6938176 0.6433686 -0.86111444 -1.1992153 -0.66503978 0.030058146 0.32153857][0.33566105 0.16087675 -0.42258811 -0.76567465 -0.44528222 0.47927821 1.5167211 1.8600031 1.2247568 0.13659394 -0.68056923 -0.74116629 -0.30687606 0.1268965 0.22994065][1.0964001 0.89933693 -0.0022056103 -0.78284919 -0.93939316 -0.49690974 0.16838193 0.443025 0.1663332 -0.321146 -0.6443342 -0.55928445 -0.23396683 0.0040392876 -0.082811832][1.5722669 1.3956093 0.28146851 -0.83455712 -1.2999561 -1.1087728 -0.6585753 -0.37888074 -0.38040555 -0.538002 -0.63133931 -0.55024958 -0.33856726 -0.20327735 -0.36737514][1.5617977 1.4552573 0.2484709 -1.0926228 -1.7451181 -1.6902446 -1.3253068 -0.93193769 -0.756036 -0.78545225 -0.85013765 -0.7753523 -0.62751329 -0.50453377 -0.62311184][1.0424052 1.085127 0.021390557 -1.3097568 -2.0042322 -2.0110903 -1.6196351 -1.1001694 -0.85925007 -0.91007531 -1.0086107 -1.0092731 -0.89537317 -0.74234182 -0.72255182]]...]
INFO - root - 2017-12-16 08:00:46.004895: step 4310, loss = 0.63, batch loss = 0.36 (47.9 examples/sec; 0.167 sec/batch; 15h:12m:44s remains)
INFO - root - 2017-12-16 08:00:47.706498: step 4320, loss = 0.58, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 15h:38m:20s remains)
INFO - root - 2017-12-16 08:00:49.400980: step 4330, loss = 0.68, batch loss = 0.40 (48.0 examples/sec; 0.167 sec/batch; 15h:11m:49s remains)
INFO - root - 2017-12-16 08:00:51.094105: step 4340, loss = 0.66, batch loss = 0.38 (48.4 examples/sec; 0.165 sec/batch; 15h:04m:33s remains)
INFO - root - 2017-12-16 08:00:52.777167: step 4350, loss = 0.54, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 15h:12m:31s remains)
INFO - root - 2017-12-16 08:00:54.488371: step 4360, loss = 0.64, batch loss = 0.37 (45.0 examples/sec; 0.178 sec/batch; 16h:12m:43s remains)
INFO - root - 2017-12-16 08:00:56.187287: step 4370, loss = 0.56, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 15h:12m:18s remains)
INFO - root - 2017-12-16 08:00:57.868998: step 4380, loss = 0.73, batch loss = 0.45 (48.1 examples/sec; 0.166 sec/batch; 15h:09m:24s remains)
INFO - root - 2017-12-16 08:00:59.574149: step 4390, loss = 0.65, batch loss = 0.38 (47.1 examples/sec; 0.170 sec/batch; 15h:29m:23s remains)
INFO - root - 2017-12-16 08:01:01.241578: step 4400, loss = 0.54, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 15h:19m:47s remains)
2017-12-16 08:01:01.741686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75594497 -0.92006713 -1.076617 -1.0896842 -1.0162154 -0.97660506 -0.92570567 -0.77292675 -0.6154722 -0.61814928 -0.73671931 -0.89697164 -1.0699832 -1.2494752 -1.3950859][-0.93654132 -1.0716777 -1.2057648 -1.2353088 -1.1894858 -1.1380789 -1.0587016 -0.89334434 -0.66283226 -0.49296844 -0.4766705 -0.59422469 -0.7806887 -1.0210537 -1.2208207][-0.7735 -0.89475787 -1.0495216 -1.1069958 -1.0462596 -0.95319712 -0.903648 -0.81431687 -0.57764947 -0.2777499 -0.11900425 -0.10846329 -0.22006869 -0.52977276 -0.81440669][-0.39675319 -0.50781751 -0.72981548 -0.81846422 -0.70683372 -0.52385581 -0.45585704 -0.44947386 -0.32127821 -0.06162703 0.1627959 0.32010722 0.33195186 0.0052440166 -0.3419944][-0.2716819 -0.3650018 -0.60255921 -0.68631089 -0.49376178 -0.1623621 0.042081952 0.070832491 0.054605603 0.11562335 0.23946655 0.41567898 0.513942 0.30130827 0.023296595][-0.546973 -0.58726621 -0.72719932 -0.74543542 -0.48225784 -0.034342766 0.34441543 0.50099826 0.47965765 0.36349726 0.27359688 0.33918142 0.44560456 0.39462352 0.27558827][-0.97129256 -0.92324752 -0.92049086 -0.83818614 -0.523461 -0.033204556 0.4073925 0.71158147 0.81226993 0.6171782 0.3535521 0.28440356 0.3728745 0.4095962 0.38150477][-1.3199565 -1.1934822 -1.0628836 -0.87755328 -0.55301714 -0.11185062 0.28138053 0.59960914 0.76474857 0.5779767 0.2691462 0.18685508 0.32986116 0.42217016 0.41449904][-1.5170801 -1.3696289 -1.1967852 -0.97092181 -0.68012786 -0.34352076 -0.037554264 0.20825505 0.3223114 0.17425263 -0.033941746 -0.026407957 0.19170856 0.34611535 0.37721729][-1.6251372 -1.5185437 -1.375717 -1.1627243 -0.89210749 -0.61703706 -0.40888357 -0.23677623 -0.17704129 -0.28916681 -0.3969537 -0.33291996 -0.0700562 0.1757158 0.30491543][-1.6191987 -1.5702326 -1.4966513 -1.3472219 -1.1137754 -0.87193614 -0.69511056 -0.54729974 -0.49438441 -0.55569124 -0.61286676 -0.5277698 -0.26308382 0.00886929 0.18569434][-1.5311759 -1.5356243 -1.5295027 -1.4700503 -1.3266995 -1.1372945 -0.97200668 -0.82672828 -0.73368174 -0.73655504 -0.73471421 -0.62833381 -0.41556787 -0.21226358 -0.089708567][-1.4596192 -1.4971112 -1.5269808 -1.5354749 -1.5174475 -1.4381324 -1.3167491 -1.205279 -1.1187963 -1.063652 -0.98698872 -0.839847 -0.65367424 -0.51319027 -0.4550401][-1.424994 -1.4697771 -1.5072203 -1.5466448 -1.6140128 -1.6555761 -1.6497775 -1.6145357 -1.564883 -1.4872141 -1.3694429 -1.2179168 -1.0615427 -0.968603 -0.94176918][-1.4306182 -1.4683539 -1.5022073 -1.5500137 -1.654812 -1.776051 -1.8656304 -1.9146177 -1.9263409 -1.879684 -1.7771237 -1.6578275 -1.5421344 -1.4662532 -1.4301777]]...]
INFO - root - 2017-12-16 08:01:03.411943: step 4410, loss = 0.62, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 15h:09m:38s remains)
INFO - root - 2017-12-16 08:01:05.059920: step 4420, loss = 0.67, batch loss = 0.40 (46.9 examples/sec; 0.170 sec/batch; 15h:31m:51s remains)
INFO - root - 2017-12-16 08:01:06.735257: step 4430, loss = 0.61, batch loss = 0.33 (46.9 examples/sec; 0.171 sec/batch; 15h:32m:20s remains)
INFO - root - 2017-12-16 08:01:08.391706: step 4440, loss = 0.62, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 14h:50m:15s remains)
INFO - root - 2017-12-16 08:01:10.089770: step 4450, loss = 0.66, batch loss = 0.38 (47.8 examples/sec; 0.167 sec/batch; 15h:14m:20s remains)
INFO - root - 2017-12-16 08:01:11.749556: step 4460, loss = 0.54, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 15h:29m:42s remains)
INFO - root - 2017-12-16 08:01:13.449147: step 4470, loss = 0.56, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 15h:35m:37s remains)
INFO - root - 2017-12-16 08:01:15.189812: step 4480, loss = 0.61, batch loss = 0.34 (43.0 examples/sec; 0.186 sec/batch; 16h:58m:07s remains)
INFO - root - 2017-12-16 08:01:16.844733: step 4490, loss = 0.66, batch loss = 0.38 (47.3 examples/sec; 0.169 sec/batch; 15h:24m:44s remains)
INFO - root - 2017-12-16 08:01:18.516802: step 4500, loss = 0.62, batch loss = 0.35 (48.4 examples/sec; 0.165 sec/batch; 15h:04m:15s remains)
2017-12-16 08:01:18.991864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9318198 -1.8926735 -1.9512401 -2.0313094 -2.0895329 -2.1033013 -2.0577748 -1.9977953 -1.9561509 -1.9025446 -1.8309817 -1.7395961 -1.6655592 -1.6412116 -1.6193138][-1.9141046 -1.9510632 -2.1395485 -2.3811731 -2.5766454 -2.6371775 -2.5483291 -2.3762949 -2.2033248 -1.9943283 -1.7849126 -1.5985149 -1.4993972 -1.4689854 -1.4574759][-1.971787 -2.08785 -2.4114778 -2.8154321 -3.1465278 -3.2931881 -3.1927285 -2.9152029 -2.514411 -2.0772283 -1.7216674 -1.4881803 -1.4209433 -1.4184138 -1.4370375][-2.0862598 -2.3169122 -2.8084552 -3.3704166 -3.8250713 -4.0147653 -3.8210478 -3.2378345 -2.475894 -1.7672158 -1.2672974 -1.1008041 -1.1606623 -1.2827554 -1.4043598][-2.1777468 -2.5147638 -3.1560397 -3.8773727 -4.4330735 -4.561326 -4.0638003 -3.0065029 -1.7899394 -0.86595684 -0.47352421 -0.62310505 -0.96424228 -1.2635859 -1.4944322][-2.2371943 -2.639564 -3.333415 -4.1096892 -4.6449256 -4.5429425 -3.5944514 -2.0386436 -0.5489223 0.18422425 0.093169212 -0.49228573 -1.1845809 -1.7007204 -1.9346845][-2.2954514 -2.6884463 -3.3128891 -3.9538655 -4.2614861 -3.8171754 -2.4343367 -0.5246979 0.91967 1.1525406 0.45366037 -0.66098428 -1.6501281 -2.2445269 -2.3851802][-2.3390186 -2.685708 -3.148294 -3.5938246 -3.636375 -2.8750868 -1.2154353 0.69960177 1.7642506 1.4783326 0.35378039 -0.98349148 -2.0191495 -2.4685524 -2.4219537][-2.3393607 -2.6566172 -3.0741138 -3.4325814 -3.3270462 -2.455101 -0.91144145 0.60079849 1.2209874 0.71120751 -0.31890607 -1.4954193 -2.3487439 -2.5670154 -2.2985508][-2.3385072 -2.6463833 -3.0904484 -3.4171107 -3.3161688 -2.6141052 -1.5133284 -0.5354296 -0.29357016 -0.74688458 -1.4630309 -2.190984 -2.6423831 -2.5375235 -2.0444882][-2.3322513 -2.5830984 -2.9737921 -3.2989781 -3.3152843 -2.9234295 -2.229773 -1.590871 -1.4841437 -1.7930773 -2.1561713 -2.3964691 -2.349998 -1.9400632 -1.3324113][-2.3002071 -2.48253 -2.7778788 -3.0751042 -3.2420583 -3.120173 -2.666604 -2.1361375 -1.8962481 -1.8519869 -1.8352629 -1.7337168 -1.3679867 -0.786748 -0.12937057][-2.3111777 -2.4477639 -2.6492853 -2.9530144 -3.2258115 -3.1659212 -2.7052412 -2.0556736 -1.5034819 -1.0609031 -0.76990181 -0.57196939 -0.2814219 0.20793688 0.71491683][-2.2972155 -2.3992276 -2.5816011 -2.882895 -3.1320667 -3.0365896 -2.5355902 -1.8184493 -1.0443896 -0.35384524 0.077434182 0.14919722 0.14792538 0.36313093 0.70296156][-2.2730379 -2.3624976 -2.5284839 -2.8086519 -3.0273561 -2.9495597 -2.5302744 -1.8588663 -1.1053112 -0.41272819 -0.060632706 -0.15653098 -0.39810467 -0.42887497 -0.25869775]]...]
INFO - root - 2017-12-16 08:01:20.697790: step 4510, loss = 0.74, batch loss = 0.46 (47.7 examples/sec; 0.168 sec/batch; 15h:16m:51s remains)
INFO - root - 2017-12-16 08:01:22.376769: step 4520, loss = 0.70, batch loss = 0.43 (48.3 examples/sec; 0.166 sec/batch; 15h:04m:50s remains)
INFO - root - 2017-12-16 08:01:24.090250: step 4530, loss = 0.61, batch loss = 0.34 (46.7 examples/sec; 0.171 sec/batch; 15h:36m:22s remains)
INFO - root - 2017-12-16 08:01:25.756859: step 4540, loss = 0.60, batch loss = 0.32 (48.3 examples/sec; 0.166 sec/batch; 15h:05m:49s remains)
INFO - root - 2017-12-16 08:01:27.434305: step 4550, loss = 0.69, batch loss = 0.42 (48.0 examples/sec; 0.167 sec/batch; 15h:10m:11s remains)
INFO - root - 2017-12-16 08:01:29.104573: step 4560, loss = 0.58, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 15h:18m:46s remains)
INFO - root - 2017-12-16 08:01:30.769474: step 4570, loss = 0.65, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 15h:06m:59s remains)
INFO - root - 2017-12-16 08:01:32.448901: step 4580, loss = 0.74, batch loss = 0.47 (47.8 examples/sec; 0.167 sec/batch; 15h:14m:52s remains)
INFO - root - 2017-12-16 08:01:34.125665: step 4590, loss = 0.62, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 15h:11m:29s remains)
INFO - root - 2017-12-16 08:01:35.798058: step 4600, loss = 0.60, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 15h:15m:43s remains)
2017-12-16 08:01:36.279934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3665445 -2.3057594 -2.2917914 -2.2890873 -2.2763634 -2.2946029 -2.306464 -2.2422025 -2.167918 -2.109832 -2.0072389 -1.8822666 -1.8287883 -1.8130834 -1.7800336][-3.1455531 -3.0551496 -3.0014048 -2.9570718 -2.9119439 -2.8781035 -2.841013 -2.7362971 -2.6775324 -2.6600223 -2.531827 -2.340416 -2.1796074 -2.0977459 -2.0633097][-3.3846512 -3.3432164 -3.2881627 -3.1943433 -3.1251888 -3.0528345 -2.9595532 -2.86906 -2.9123654 -3.0282559 -2.94492 -2.6801338 -2.3670251 -2.1329184 -2.079551][-2.7465825 -2.7828197 -2.759501 -2.6502938 -2.4958687 -2.3638475 -2.2230062 -2.2375431 -2.587975 -2.9814997 -3.0790503 -2.8070111 -2.3677592 -2.0114758 -1.9670978][-1.4759204 -1.5787486 -1.5856854 -1.4728909 -1.2049395 -0.89871454 -0.65730512 -0.82824361 -1.6098505 -2.3722887 -2.7011006 -2.5167761 -2.07307 -1.6937157 -1.7155867][-0.348271 -0.44431317 -0.49217725 -0.36655021 0.072414637 0.77053523 1.3637009 1.117023 -0.069319487 -1.1948283 -1.7726455 -1.7931458 -1.4918854 -1.1941491 -1.2786119][0.024564981 -0.012881517 -0.029307365 0.17094016 0.89318347 2.1032057 3.168823 2.892873 1.5016274 0.22043025 -0.47100198 -0.6865716 -0.591565 -0.44759941 -0.5615871][-0.48302567 -0.42749739 -0.29667604 0.10099912 1.0941703 2.6021187 3.8846972 3.4468586 2.1542156 1.1018047 0.55326104 0.3128674 0.3026588 0.33134174 0.18803382][-1.2599316 -1.160403 -0.98489553 -0.46914113 0.56494 1.7878859 2.4668229 2.0586948 1.2127476 0.65427494 0.53356528 0.57399869 0.65495539 0.674757 0.53628516][-1.6004547 -1.5776782 -1.4801368 -1.1063912 -0.42116785 0.17952847 0.1666255 -0.27358222 -0.72939479 -0.82552195 -0.51975536 -0.072030425 0.23546064 0.32252288 0.26247609][-1.3324714 -1.4423103 -1.5304474 -1.4815767 -1.345248 -1.3773819 -1.778056 -2.327626 -2.6264763 -2.4610963 -1.8875946 -1.2200829 -0.77694213 -0.606928 -0.54332268][-0.73754942 -0.93966711 -1.1943244 -1.445465 -1.7794571 -2.3026321 -2.9345107 -3.5135407 -3.7807696 -3.5421922 -2.9247222 -2.2550015 -1.849355 -1.695816 -1.5554335][-0.14453948 -0.3779335 -0.762809 -1.2217853 -1.7818011 -2.4279969 -3.0708952 -3.5892711 -3.8057663 -3.6115775 -3.1507132 -2.6759143 -2.4558914 -2.4097638 -2.2881331][0.22166216 -0.015638947 -0.46880233 -0.97606933 -1.4928441 -1.9836009 -2.4335051 -2.8229299 -3.0028148 -2.907212 -2.6363225 -2.4065325 -2.3625369 -2.4014132 -2.3567479][0.44076085 0.16985893 -0.26330841 -0.75770712 -1.1513135 -1.3672304 -1.5328617 -1.7217722 -1.8146554 -1.7623011 -1.6471951 -1.6001861 -1.6785952 -1.7793841 -1.7990028]]...]
INFO - root - 2017-12-16 08:01:37.989696: step 4610, loss = 0.64, batch loss = 0.36 (47.2 examples/sec; 0.170 sec/batch; 15h:26m:51s remains)
INFO - root - 2017-12-16 08:01:39.696534: step 4620, loss = 0.73, batch loss = 0.45 (46.7 examples/sec; 0.171 sec/batch; 15h:36m:16s remains)
INFO - root - 2017-12-16 08:01:41.370911: step 4630, loss = 0.61, batch loss = 0.34 (46.8 examples/sec; 0.171 sec/batch; 15h:33m:33s remains)
INFO - root - 2017-12-16 08:01:43.046987: step 4640, loss = 0.62, batch loss = 0.35 (48.4 examples/sec; 0.165 sec/batch; 15h:03m:08s remains)
INFO - root - 2017-12-16 08:01:44.743577: step 4650, loss = 0.64, batch loss = 0.36 (46.4 examples/sec; 0.173 sec/batch; 15h:42m:42s remains)
INFO - root - 2017-12-16 08:01:46.436613: step 4660, loss = 0.57, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 15h:15m:12s remains)
INFO - root - 2017-12-16 08:01:48.091141: step 4670, loss = 0.52, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 15h:02m:19s remains)
INFO - root - 2017-12-16 08:01:49.773204: step 4680, loss = 0.57, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 15h:12m:41s remains)
INFO - root - 2017-12-16 08:01:51.426729: step 4690, loss = 0.56, batch loss = 0.29 (49.3 examples/sec; 0.162 sec/batch; 14h:46m:59s remains)
INFO - root - 2017-12-16 08:01:53.112653: step 4700, loss = 0.67, batch loss = 0.40 (46.4 examples/sec; 0.172 sec/batch; 15h:41m:27s remains)
2017-12-16 08:01:53.633694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1178288 -3.1405358 -3.0634499 -2.9354527 -2.895746 -3.0194981 -3.2542467 -3.4511518 -3.2902591 -2.7034545 -1.7423683 -0.60262358 0.1553576 -0.17183721 -1.2965708][-3.1060305 -3.0498204 -2.861856 -2.6578951 -2.6031184 -2.7811341 -3.1071362 -3.351192 -3.1911726 -2.5710707 -1.5934191 -0.48274946 0.25356591 -0.087648511 -1.2302595][-2.844521 -2.6686056 -2.3254671 -1.9946761 -1.864122 -2.0662837 -2.4868183 -2.8040714 -2.68063 -2.0943532 -1.2192315 -0.24614656 0.39358628 0.0069941282 -1.1538601][-2.4899559 -2.2827649 -1.810382 -1.2837114 -0.97429216 -1.0761392 -1.5156021 -1.9448811 -1.9734818 -1.5138565 -0.80644464 -0.021774292 0.4713155 0.039761782 -1.1004367][-2.2579246 -2.1123705 -1.5610646 -0.83068031 -0.25101686 -0.11191297 -0.47896886 -1.0389106 -1.2728893 -1.0022799 -0.4626013 0.17339611 0.53993595 0.040300012 -1.1107867][-2.0153773 -1.9913651 -1.4310757 -0.51287448 0.41972959 0.92841184 0.75423682 0.022997618 -0.55904007 -0.5314759 -0.13263249 0.44326437 0.75559461 0.15045667 -1.0875224][-1.7892708 -1.8842609 -1.3303946 -0.26657581 0.99938428 1.8823649 1.9503068 1.0862118 0.11896658 -0.13233316 0.16359222 0.81670225 1.1851102 0.45960748 -0.94682682][-1.6874242 -1.8874477 -1.3773762 -0.22390711 1.2700051 2.4142575 2.6426167 1.67867 0.44806731 0.012464523 0.32096827 1.1404966 1.6451544 0.86262357 -0.7245791][-1.694515 -1.9287797 -1.4822503 -0.32901645 1.1692029 2.3462381 2.594202 1.6602329 0.42298234 -0.0007121563 0.43494475 1.4282583 2.0338573 1.2111577 -0.51574206][-1.8735663 -2.0210581 -1.6001658 -0.54342771 0.79723275 1.82334 1.9891962 1.1739551 0.21200502 -0.0068725348 0.61432707 1.7188216 2.3237715 1.4341713 -0.38412046][-2.1148984 -2.1344461 -1.7348822 -0.84469128 0.28018773 1.1212407 1.2260181 0.63356817 0.068567038 0.13808787 0.95115888 2.099381 2.6007147 1.592997 -0.29013813][-2.2737994 -2.1856382 -1.7935017 -1.0688584 -0.15298569 0.53384125 0.64268291 0.33352149 0.15870106 0.48855412 1.3691391 2.4462829 2.7896113 1.634894 -0.27707708][-2.3971968 -2.2517664 -1.9171815 -1.3439891 -0.62445033 -0.050482273 0.16261196 0.14913023 0.2750777 0.7535013 1.5986162 2.5051131 2.6656394 1.4307207 -0.41735518][-2.4652252 -2.3186417 -2.0633056 -1.6296896 -1.1129341 -0.66370761 -0.3685863 -0.16147852 0.17701566 0.72513044 1.466657 2.1699657 2.1708603 0.98750079 -0.68459177][-2.4719822 -2.3513393 -2.1545331 -1.8537976 -1.5231977 -1.2003129 -0.89220512 -0.56050193 -0.12660575 0.40928161 1.009894 1.52254 1.4601728 0.46256459 -0.91713965]]...]
INFO - root - 2017-12-16 08:01:55.289993: step 4710, loss = 0.54, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 15h:14m:00s remains)
INFO - root - 2017-12-16 08:01:56.984973: step 4720, loss = 0.64, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 15h:02m:41s remains)
INFO - root - 2017-12-16 08:01:58.649281: step 4730, loss = 0.56, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 15h:16m:26s remains)
INFO - root - 2017-12-16 08:02:00.363804: step 4740, loss = 0.65, batch loss = 0.38 (47.4 examples/sec; 0.169 sec/batch; 15h:22m:12s remains)
INFO - root - 2017-12-16 08:02:02.033250: step 4750, loss = 0.52, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 15h:14m:42s remains)
INFO - root - 2017-12-16 08:02:03.724479: step 4760, loss = 0.62, batch loss = 0.35 (45.2 examples/sec; 0.177 sec/batch; 16h:06m:23s remains)
INFO - root - 2017-12-16 08:02:05.426565: step 4770, loss = 0.62, batch loss = 0.34 (45.2 examples/sec; 0.177 sec/batch; 16h:05m:58s remains)
INFO - root - 2017-12-16 08:02:07.105099: step 4780, loss = 0.65, batch loss = 0.37 (49.0 examples/sec; 0.163 sec/batch; 14h:50m:56s remains)
INFO - root - 2017-12-16 08:02:08.787896: step 4790, loss = 0.62, batch loss = 0.34 (46.2 examples/sec; 0.173 sec/batch; 15h:45m:06s remains)
INFO - root - 2017-12-16 08:02:10.436600: step 4800, loss = 0.62, batch loss = 0.35 (48.6 examples/sec; 0.165 sec/batch; 14h:59m:22s remains)
2017-12-16 08:02:10.920338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2402425 -1.2275312 -1.2064381 -1.1997485 -1.2263844 -1.2954369 -1.3712809 -1.4360375 -1.5038762 -1.5498906 -1.5151193 -1.3709345 -1.1742907 -1.0765393 -1.2030252][-1.3491416 -1.3696362 -1.3057275 -1.2635409 -1.3029827 -1.4309766 -1.5913701 -1.7343801 -1.8593843 -1.873403 -1.650336 -1.176954 -0.57803452 -0.15393245 -0.18293011][-1.7677875 -1.7570765 -1.5787984 -1.406728 -1.3787144 -1.5226705 -1.7595874 -2.0219648 -2.2759752 -2.3237417 -1.9581208 -1.157913 -0.13673806 0.64883804 0.78033328][-2.3445668 -2.1867342 -1.7760814 -1.3227084 -1.0597144 -1.1219332 -1.4444159 -1.92446 -2.4182901 -2.6228335 -2.2561293 -1.3216925 -0.070120096 0.95880532 1.2376795][-2.6479383 -2.1857667 -1.3497427 -0.41152239 0.24037969 0.35893512 -0.063907862 -0.84875894 -1.7409019 -2.2996914 -2.1868076 -1.4163027 -0.26327991 0.773381 1.1264892][-2.4206252 -1.5549405 -0.19104016 1.3284547 2.46051 2.8041251 2.2441037 1.1207883 -0.2396282 -1.3192106 -1.6751059 -1.2909167 -0.45445371 0.38938951 0.72603083][-1.9198412 -0.75670791 0.93439651 2.7911389 4.2568693 4.7680092 4.1854572 2.8543684 1.1841707 -0.32958925 -1.1336913 -1.179285 -0.7220273 -0.091770768 0.23666477][-1.711832 -0.59541857 0.97730923 2.6487296 3.9313471 4.4006252 3.9952905 2.8078229 1.2456729 -0.25086439 -1.1600158 -1.3538263 -1.0740664 -0.54475856 -0.17370963][-1.7214375 -1.0367754 -0.073710084 0.98155284 1.8179793 2.1124597 1.81041 0.97876382 -0.1006583 -1.054408 -1.5962341 -1.6197351 -1.3064334 -0.77418709 -0.32772624][-1.6857373 -1.553923 -1.3110592 -0.97106278 -0.66197717 -0.55494022 -0.75581038 -1.2064271 -1.6586812 -1.9022684 -1.8674719 -1.6161057 -1.1677418 -0.6168673 -0.15580463][-1.4623364 -1.7403282 -2.0611463 -2.2307844 -2.3010635 -2.3592713 -2.457202 -2.5748324 -2.5381975 -2.2122493 -1.7462001 -1.2967198 -0.77446651 -0.21505189 0.17274499][-1.2966087 -1.6828482 -2.1886206 -2.5744236 -2.8026061 -2.8983302 -2.8942566 -2.7900934 -2.5086613 -1.9828817 -1.4304874 -0.95323849 -0.44323552 0.033134103 0.2687192][-1.0961359 -1.3266697 -1.7190509 -2.0884113 -2.3216181 -2.392909 -2.3560448 -2.203675 -1.9199986 -1.4993297 -1.1184602 -0.78660804 -0.46520972 -0.2100656 -0.17826974][-0.94049996 -0.93272549 -1.0975926 -1.306107 -1.4576565 -1.5165101 -1.4942726 -1.3726743 -1.2029941 -1.0334462 -0.95044404 -0.92786568 -0.92514235 -0.94360673 -1.0716636][-1.1166475 -0.9586364 -0.94730431 -1.0154498 -1.063077 -1.0719981 -1.0564861 -1.0035088 -0.96286368 -1.0260071 -1.2095234 -1.4459521 -1.6775775 -1.8597343 -2.0319395]]...]
INFO - root - 2017-12-16 08:02:12.609477: step 4810, loss = 0.69, batch loss = 0.42 (47.2 examples/sec; 0.170 sec/batch; 15h:25m:56s remains)
INFO - root - 2017-12-16 08:02:14.326320: step 4820, loss = 0.59, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 15h:38m:28s remains)
INFO - root - 2017-12-16 08:02:16.045611: step 4830, loss = 0.74, batch loss = 0.46 (47.5 examples/sec; 0.168 sec/batch; 15h:20m:05s remains)
INFO - root - 2017-12-16 08:02:17.737699: step 4840, loss = 0.67, batch loss = 0.40 (46.7 examples/sec; 0.171 sec/batch; 15h:35m:57s remains)
INFO - root - 2017-12-16 08:02:19.451577: step 4850, loss = 0.66, batch loss = 0.38 (47.4 examples/sec; 0.169 sec/batch; 15h:22m:07s remains)
INFO - root - 2017-12-16 08:02:21.147488: step 4860, loss = 0.55, batch loss = 0.27 (45.3 examples/sec; 0.177 sec/batch; 16h:04m:26s remains)
INFO - root - 2017-12-16 08:02:22.846833: step 4870, loss = 0.59, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 15h:37m:04s remains)
INFO - root - 2017-12-16 08:02:24.521503: step 4880, loss = 0.54, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 15h:28m:03s remains)
INFO - root - 2017-12-16 08:02:26.199973: step 4890, loss = 0.64, batch loss = 0.37 (47.5 examples/sec; 0.168 sec/batch; 15h:19m:35s remains)
INFO - root - 2017-12-16 08:02:27.897639: step 4900, loss = 0.62, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 15h:24m:03s remains)
2017-12-16 08:02:28.392019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.016351 -2.1256325 -2.2078319 -2.2512321 -2.276305 -2.2866523 -2.2881768 -2.2889302 -2.2927186 -2.2980218 -2.30171 -2.3036995 -2.3149605 -2.3548398 -2.4180212][-2.2622266 -2.4792371 -2.6679628 -2.8027444 -2.8975458 -2.954423 -2.9826133 -2.9993505 -3.0104172 -3.0185885 -3.0188777 -3.0145516 -3.0233526 -3.0723763 -3.1504729][-2.3560913 -2.6122308 -2.8488789 -3.0260119 -3.1592169 -3.246367 -3.2947021 -3.325676 -3.3541338 -3.3802013 -3.3966603 -3.3970661 -3.4048929 -3.4498696 -3.5424302][-2.2539504 -2.4511464 -2.6271324 -2.7439413 -2.8228674 -2.8717957 -2.9016948 -2.9241729 -2.9636874 -3.0197444 -3.0727024 -3.1115026 -3.1482735 -3.2198329 -3.3524222][-2.028399 -2.0854549 -2.1153018 -2.0968652 -2.0565977 -2.0272095 -2.013586 -2.0244498 -2.0740294 -2.1599824 -2.2550673 -2.3460922 -2.4379048 -2.5560524 -2.750457][-1.7258414 -1.5922495 -1.4107213 -1.1957752 -1.007502 -0.88736516 -0.82710069 -0.8153035 -0.86186206 -0.96806264 -1.1032232 -1.249169 -1.3974291 -1.5642087 -1.8370304][-1.4127662 -1.0606267 -0.64353061 -0.204005 0.16414738 0.39860368 0.49757814 0.49648714 0.41618633 0.27016783 0.10530186 -0.056133151 -0.21348035 -0.3905338 -0.70998871][-1.0722793 -0.51345229 0.10947537 0.73577571 1.2433648 1.5796652 1.7090728 1.6864033 1.568013 1.389683 1.1908233 1.0109711 0.87468672 0.71286321 0.38045931][-0.7330476 -0.037362933 0.67605138 1.3536475 1.8767388 2.2071333 2.3346648 2.3297262 2.2693338 2.1745157 2.0432045 1.9296894 1.8666568 1.7261879 1.3378048][-0.47497809 0.30567956 1.0027032 1.6257694 2.0629256 2.297729 2.3426361 2.2886362 2.2750807 2.2959838 2.31564 2.376658 2.4588413 2.3677759 1.9419363][-0.6140337 0.11762464 0.70137668 1.177109 1.4723141 1.5598722 1.4960215 1.3840697 1.3662031 1.4308593 1.5082314 1.638176 1.7887719 1.7228236 1.3297138][-1.2068863 -0.65048766 -0.23144758 0.069188356 0.23087716 0.19034433 0.0441041 -0.077179074 -0.097022057 -0.050859809 0.030594707 0.15990567 0.31340241 0.28941178 -0.0023581982][-1.9536598 -1.6491239 -1.3819138 -1.1805995 -1.0953889 -1.1897035 -1.33935 -1.4353501 -1.4566977 -1.4252883 -1.3514589 -1.2359865 -1.1134088 -1.1076014 -1.2809157][-2.4614954 -2.3782358 -2.2343962 -2.0964062 -2.0390913 -2.1079135 -2.2109418 -2.2632675 -2.2796237 -2.2758389 -2.2495534 -2.1984253 -2.1363888 -2.1288247 -2.1996646][-2.5895703 -2.6241641 -2.5726142 -2.4937432 -2.4482343 -2.4693451 -2.5053816 -2.5084164 -2.5018876 -2.5033991 -2.508564 -2.5110972 -2.5141366 -2.5312223 -2.5671148]]...]
INFO - root - 2017-12-16 08:02:30.055993: step 4910, loss = 0.65, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 15h:09m:23s remains)
INFO - root - 2017-12-16 08:02:31.745535: step 4920, loss = 0.75, batch loss = 0.47 (47.4 examples/sec; 0.169 sec/batch; 15h:22m:14s remains)
INFO - root - 2017-12-16 08:02:33.389770: step 4930, loss = 0.67, batch loss = 0.39 (49.2 examples/sec; 0.163 sec/batch; 14h:47m:37s remains)
INFO - root - 2017-12-16 08:02:35.080289: step 4940, loss = 0.65, batch loss = 0.38 (46.4 examples/sec; 0.172 sec/batch; 15h:41m:14s remains)
INFO - root - 2017-12-16 08:02:36.773330: step 4950, loss = 0.65, batch loss = 0.38 (48.5 examples/sec; 0.165 sec/batch; 15h:00m:09s remains)
INFO - root - 2017-12-16 08:02:38.479691: step 4960, loss = 0.58, batch loss = 0.30 (46.4 examples/sec; 0.172 sec/batch; 15h:41m:01s remains)
INFO - root - 2017-12-16 08:02:40.189958: step 4970, loss = 0.60, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 15h:07m:29s remains)
INFO - root - 2017-12-16 08:02:41.875689: step 4980, loss = 0.57, batch loss = 0.30 (45.5 examples/sec; 0.176 sec/batch; 15h:59m:24s remains)
INFO - root - 2017-12-16 08:02:43.597384: step 4990, loss = 0.66, batch loss = 0.38 (46.7 examples/sec; 0.171 sec/batch; 15h:35m:02s remains)
INFO - root - 2017-12-16 08:02:45.292166: step 5000, loss = 0.51, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 15h:16m:34s remains)
2017-12-16 08:02:45.771484: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.97638321 0.95741868 0.88381815 0.767472 0.6320467 0.52812171 0.48497128 0.54109883 0.79518771 1.2869477 1.740571 1.8392556 1.428822 0.61165905 -0.39541292][1.2094109 1.2465873 1.1942213 1.0293744 0.80088806 0.61912107 0.54342818 0.61323166 0.96550894 1.6450388 2.3246551 2.6698966 2.4329238 1.6131229 0.48111558][-0.071962953 0.010609865 0.019786835 -0.10420227 -0.34943521 -0.58994424 -0.72899663 -0.70570505 -0.37243593 0.35554123 1.1829245 1.7778184 1.8318901 1.2827165 0.36815906][-1.7775209 -1.742273 -1.6889215 -1.7795621 -2.0041075 -2.2315164 -2.3716516 -2.4030659 -2.1906092 -1.6165036 -0.86417633 -0.17253768 0.11065733 -0.091422677 -0.61615622][-2.7249689 -2.8667197 -2.8758173 -2.9623523 -3.0759716 -3.1284804 -3.1299853 -3.1412182 -3.1093426 -2.8381023 -2.3846734 -1.8659596 -1.5788536 -1.5574193 -1.7150731][-2.5458891 -2.9513493 -3.1271644 -3.1995969 -3.1056895 -2.8538003 -2.5843487 -2.5126953 -2.6525745 -2.7644961 -2.7481084 -2.6420989 -2.60169 -2.5705917 -2.4963903][-1.6691239 -2.3406985 -2.6100421 -2.5435362 -2.1791925 -1.5569931 -0.93760657 -0.69067 -0.92872036 -1.3877724 -1.8529546 -2.3081145 -2.7112026 -2.8984594 -2.8343983][-0.96646029 -1.6641164 -1.8093024 -1.4898051 -0.81500912 0.18537843 1.2082229 1.7135766 1.5219281 0.86337924 -0.041058898 -1.0504167 -1.9620461 -2.5295544 -2.7254384][-1.035063 -1.4787015 -1.3547733 -0.75980318 0.12620687 1.3063509 2.5663881 3.2542028 3.1920662 2.5714803 1.530766 0.22156262 -1.0531373 -1.9483695 -2.3818069][-1.6633071 -1.8212796 -1.4869516 -0.82600838 0.0044367313 1.0186758 2.0806608 2.7316251 2.8501258 2.4990573 1.6863379 0.50183868 -0.74991977 -1.6884649 -2.1496327][-2.4264247 -2.4724095 -2.1448302 -1.6232253 -1.0084472 -0.27142823 0.41422772 0.83267832 0.99578667 0.93321824 0.49800754 -0.31059504 -1.2588971 -1.9687266 -2.2529941][-2.9726763 -3.0591173 -2.9009051 -2.5950124 -2.2339852 -1.8298436 -1.5095304 -1.3356307 -1.193959 -1.1135676 -1.1769329 -1.5421849 -2.0948136 -2.5073624 -2.604969][-3.3187351 -3.4376168 -3.419661 -3.3045671 -3.1553402 -2.9843795 -2.8709531 -2.801414 -2.6479559 -2.4535813 -2.2894535 -2.3548517 -2.6614814 -2.917542 -2.9094582][-3.4038188 -3.4564648 -3.4490004 -3.4083142 -3.2958121 -3.2002831 -3.1584163 -3.0918758 -2.9327023 -2.7243304 -2.5108216 -2.4933574 -2.7232769 -2.9560761 -2.9386098][-2.9270277 -2.9043069 -2.862556 -2.7911532 -2.6441953 -2.5332248 -2.4686828 -2.3737574 -2.1984053 -2.0283487 -1.885034 -1.89843 -2.1751273 -2.4824445 -2.5717936]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:02:47.964691: step 5010, loss = 0.59, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 15h:18m:34s remains)
INFO - root - 2017-12-16 08:02:49.624805: step 5020, loss = 0.62, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 14h:59m:21s remains)
INFO - root - 2017-12-16 08:02:51.283355: step 5030, loss = 0.64, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 14h:54m:12s remains)
INFO - root - 2017-12-16 08:02:53.007844: step 5040, loss = 0.56, batch loss = 0.29 (46.1 examples/sec; 0.173 sec/batch; 15h:46m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:02:54.689529: step 5050, loss = 0.59, batch loss = 0.31 (47.4 examples/sec; 0.169 sec/batch; 15h:21m:29s remains)
INFO - root - 2017-12-16 08:02:56.394305: step 5060, loss = 0.56, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 15h:38m:53s remains)
INFO - root - 2017-12-16 08:02:58.083278: step 5070, loss = 0.60, batch loss = 0.33 (46.2 examples/sec; 0.173 sec/batch; 15h:44m:12s remains)
INFO - root - 2017-12-16 08:02:59.788713: step 5080, loss = 0.58, batch loss = 0.31 (46.4 examples/sec; 0.173 sec/batch; 15h:41m:23s remains)
INFO - root - 2017-12-16 08:03:01.507397: step 5090, loss = 0.67, batch loss = 0.40 (47.3 examples/sec; 0.169 sec/batch; 15h:22m:30s remains)
INFO - root - 2017-12-16 08:03:03.174319: step 5100, loss = 0.53, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 15h:15m:01s remains)
2017-12-16 08:03:03.664084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6663942 -1.693445 -1.7041861 -1.7180315 -1.7585719 -1.8417487 -1.9203367 -2.0075755 -2.042479 -2.0254655 -1.9771144 -1.8972675 -1.8094022 -1.7731791 -1.7639328][-1.7101061 -1.8015783 -1.8912786 -1.9900762 -2.101393 -2.2370493 -2.3996754 -2.5477953 -2.6035314 -2.580519 -2.5095122 -2.39468 -2.24263 -2.1374497 -2.0801606][-1.7417406 -1.8743051 -2.0354912 -2.1707788 -2.2818303 -2.4060628 -2.5308564 -2.6632636 -2.7659581 -2.798007 -2.8253775 -2.7629056 -2.5835164 -2.449039 -2.348742][-1.7516575 -1.8593541 -1.9790897 -1.9985789 -1.932241 -1.8255669 -1.7151823 -1.74758 -1.9274064 -2.192703 -2.4553554 -2.5764003 -2.5322409 -2.473141 -2.3844781][-1.7599714 -1.8410008 -1.8765054 -1.6179134 -1.0434275 -0.39042938 0.19518244 0.49959362 0.14267755 -0.53841686 -1.1582416 -1.6562966 -1.9033731 -2.0358939 -2.0655546][-1.8699884 -1.9429232 -1.8261174 -1.232615 -0.054848194 1.361447 2.6828594 3.5002589 2.9047284 1.8145741 0.821725 -0.13359296 -0.75103843 -1.1484752 -1.3034058][-1.9765072 -2.0018332 -1.7370485 -0.8105765 0.88900626 2.9960866 5.0563951 6.4639516 5.4535675 3.977488 2.8491235 1.5397836 0.58046353 -0.0039111376 -0.17732739][-1.969202 -1.9163055 -1.5563176 -0.50845635 1.3830546 3.6276155 5.7416906 7.1470647 6.0816417 4.6379027 3.7578988 2.5066209 1.3964199 0.80786908 0.63328445][-1.9458766 -1.779754 -1.3658191 -0.38041413 1.3419086 3.2031989 4.5631905 4.9810872 4.0575352 3.1841569 2.8340535 2.0757012 1.1385134 0.72588861 0.63639843][-2.0312788 -1.8599479 -1.4578476 -0.59303641 0.70025384 1.8819591 2.3764687 2.0481148 1.2107712 0.67739785 0.62690127 0.43335688 -0.069110394 -0.27864206 -0.2369957][-2.2711668 -2.2237844 -1.9312762 -1.2660708 -0.429523 0.12869978 0.033493996 -0.64898467 -1.3975847 -1.6826957 -1.5341884 -1.4646013 -1.6628232 -1.74998 -1.6677508][-2.5537279 -2.7062988 -2.5955193 -2.193872 -1.7466131 -1.6076448 -2.0240655 -2.7355714 -3.2523079 -3.3054986 -3.047698 -2.8684862 -2.8717856 -2.9061489 -2.879343][-2.6688211 -2.9244878 -2.9380341 -2.7344666 -2.597404 -2.7118118 -3.1734009 -3.7228534 -3.9742236 -3.8433733 -3.5900643 -3.3937709 -3.307941 -3.3274536 -3.2897291][-2.573483 -2.8266666 -2.9074011 -2.8068647 -2.8023267 -2.9951479 -3.3745863 -3.6854169 -3.7220237 -3.53258 -3.2940688 -3.1299758 -3.0725942 -3.0688837 -3.0324633][-2.3564003 -2.5594215 -2.6642754 -2.6599057 -2.7116756 -2.8503106 -3.0334578 -3.1623516 -3.0809188 -2.882895 -2.6711445 -2.5450392 -2.513207 -2.517493 -2.5204587]]...]
INFO - root - 2017-12-16 08:03:05.317430: step 5110, loss = 0.78, batch loss = 0.51 (49.0 examples/sec; 0.163 sec/batch; 14h:50m:54s remains)
INFO - root - 2017-12-16 08:03:06.989744: step 5120, loss = 0.59, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 15h:07m:28s remains)
INFO - root - 2017-12-16 08:03:08.706452: step 5130, loss = 0.65, batch loss = 0.37 (47.2 examples/sec; 0.170 sec/batch; 15h:25m:06s remains)
INFO - root - 2017-12-16 08:03:10.426451: step 5140, loss = 0.58, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 15h:15m:23s remains)
INFO - root - 2017-12-16 08:03:12.103053: step 5150, loss = 0.55, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 15h:08m:10s remains)
INFO - root - 2017-12-16 08:03:13.755639: step 5160, loss = 0.56, batch loss = 0.29 (49.5 examples/sec; 0.162 sec/batch; 14h:42m:25s remains)
INFO - root - 2017-12-16 08:03:15.415628: step 5170, loss = 0.61, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 15h:17m:35s remains)
INFO - root - 2017-12-16 08:03:17.084492: step 5180, loss = 0.63, batch loss = 0.36 (47.2 examples/sec; 0.170 sec/batch; 15h:25m:31s remains)
INFO - root - 2017-12-16 08:03:18.780357: step 5190, loss = 0.61, batch loss = 0.34 (45.0 examples/sec; 0.178 sec/batch; 16h:09m:02s remains)
INFO - root - 2017-12-16 08:03:20.467557: step 5200, loss = 0.59, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 15h:28m:04s remains)
2017-12-16 08:03:20.944956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4393468 -2.5029783 -2.5717096 -2.6073771 -2.63162 -2.6999888 -2.8242946 -2.9513044 -2.9203005 -2.7628319 -2.5817943 -2.3523459 -2.08552 -1.853621 -1.6345444][-2.313334 -2.3084781 -2.3291276 -2.3185685 -2.2919054 -2.3526795 -2.4816558 -2.6256609 -2.6794472 -2.6533172 -2.6126137 -2.4873004 -2.2460074 -1.888509 -1.5301518][-1.9904078 -1.9329445 -1.9659377 -1.9565004 -1.8272655 -1.7374755 -1.7622784 -1.8934546 -2.1054041 -2.3901772 -2.6932476 -2.8182766 -2.7312784 -2.36648 -1.8796877][-1.7479023 -1.6547852 -1.6675341 -1.6069766 -1.3285371 -0.90806371 -0.5771662 -0.619483 -1.1057305 -1.8844187 -2.6279068 -3.1421344 -3.2916706 -2.9252663 -2.2743173][-1.7541909 -1.6491262 -1.5785973 -1.3104591 -0.73509848 0.15087187 0.97949755 1.0564283 0.19705307 -1.1598539 -2.3910136 -3.2878933 -3.6158986 -3.2223225 -2.3800361][-1.7646124 -1.6008884 -1.4474964 -1.0064287 -0.098772764 1.245047 2.6802778 2.9218888 1.5775665 -0.32277369 -2.0460427 -3.2698429 -3.735364 -3.2970405 -2.399123][-1.492072 -1.2199129 -0.9551217 -0.41792405 0.72074664 2.4493155 4.17068 4.1575003 2.442543 0.21550882 -1.7979746 -3.2253087 -3.696696 -3.2205367 -2.3746245][-0.90150946 -0.49386621 -0.083066463 0.53808677 1.6109849 3.0332909 4.1395721 3.8120475 2.1879096 0.096644521 -1.8133492 -3.1179423 -3.3893933 -2.8371594 -2.0553138][-0.16121328 0.39678681 0.93312728 1.4480895 2.01342 2.563509 2.7852631 2.2900157 1.0569264 -0.54312956 -1.9408882 -2.6888304 -2.6048067 -1.9687889 -1.3200214][0.29023254 0.89599288 1.4452072 1.7256008 1.8186334 1.6584624 1.3320295 0.69147527 -0.19142747 -1.2094285 -1.9012891 -2.1069229 -1.6878886 -0.99916083 -0.509361][0.13897252 0.64914143 1.0085171 1.1081525 0.981316 0.6203624 0.072868109 -0.57695758 -1.2247263 -1.7118306 -1.9150076 -1.7526877 -1.1272004 -0.4393301 -0.085412979][-0.52594018 -0.1884073 0.034757733 0.058946252 -0.12903929 -0.50833619 -1.04881 -1.60538 -2.0221853 -2.1731038 -2.0987339 -1.7712933 -1.1496825 -0.56557977 -0.30648339][-1.3456204 -1.1555934 -1.0235502 -1.0534863 -1.2433221 -1.5512633 -1.9363097 -2.3125556 -2.5096865 -2.5162611 -2.3705339 -2.05925 -1.5796115 -1.1723636 -1.0197126][-1.9681966 -1.8956791 -1.8568113 -1.8939772 -1.9883423 -2.1639876 -2.3784406 -2.5529687 -2.5991259 -2.5774848 -2.5022125 -2.3388524 -2.0658267 -1.8183335 -1.7169608][-2.2289724 -2.2049303 -2.1957402 -2.2135699 -2.2460403 -2.3279014 -2.4040539 -2.4455271 -2.4545555 -2.4452281 -2.4334688 -2.3911185 -2.296412 -2.1701932 -2.1113133]]...]
INFO - root - 2017-12-16 08:03:22.616210: step 5210, loss = 0.52, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 15h:18m:37s remains)
INFO - root - 2017-12-16 08:03:24.311344: step 5220, loss = 0.57, batch loss = 0.30 (48.6 examples/sec; 0.164 sec/batch; 14h:57m:03s remains)
INFO - root - 2017-12-16 08:03:25.982851: step 5230, loss = 0.51, batch loss = 0.23 (47.7 examples/sec; 0.168 sec/batch; 15h:13m:59s remains)
INFO - root - 2017-12-16 08:03:27.678560: step 5240, loss = 0.67, batch loss = 0.40 (48.1 examples/sec; 0.166 sec/batch; 15h:07m:14s remains)
INFO - root - 2017-12-16 08:03:29.366223: step 5250, loss = 0.59, batch loss = 0.32 (47.1 examples/sec; 0.170 sec/batch; 15h:27m:17s remains)
INFO - root - 2017-12-16 08:03:31.052290: step 5260, loss = 0.53, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 15h:06m:40s remains)
INFO - root - 2017-12-16 08:03:32.726279: step 5270, loss = 0.59, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 15h:14m:40s remains)
INFO - root - 2017-12-16 08:03:34.392194: step 5280, loss = 0.65, batch loss = 0.38 (47.8 examples/sec; 0.167 sec/batch; 15h:13m:24s remains)
INFO - root - 2017-12-16 08:03:36.082254: step 5290, loss = 0.57, batch loss = 0.30 (47.6 examples/sec; 0.168 sec/batch; 15h:16m:39s remains)
INFO - root - 2017-12-16 08:03:37.768602: step 5300, loss = 0.63, batch loss = 0.36 (46.0 examples/sec; 0.174 sec/batch; 15h:48m:50s remains)
2017-12-16 08:03:38.248369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0963759 -3.0998592 -3.1208091 -3.1520586 -3.2002516 -3.190773 -3.1095972 -2.9964767 -2.9135842 -2.8958297 -2.9697864 -3.0550826 -3.0170994 -2.9498277 -2.9668636][-3.5760257 -3.534523 -3.5377336 -3.5218427 -3.4502256 -3.3261604 -3.11196 -2.8867414 -2.7307549 -2.7077556 -2.8450108 -2.9693954 -2.9673057 -2.9315844 -2.8961694][-3.2812202 -3.1191509 -3.0346677 -2.9258769 -2.728689 -2.5116012 -2.2097244 -1.8541069 -1.6655608 -1.673568 -1.9013989 -2.1303861 -2.2467229 -2.3594716 -2.3708155][-2.4022729 -2.1408746 -2.0115755 -1.8541061 -1.6067727 -1.3083987 -0.9270528 -0.51517773 -0.30733085 -0.34837222 -0.60851634 -0.99595815 -1.2785633 -1.6032041 -1.7388687][-1.5031462 -1.1238368 -0.93592179 -0.73455977 -0.47222733 -0.16112077 0.2452786 0.71780825 0.94801044 0.90432644 0.64379716 0.12646246 -0.43729603 -0.953259 -1.2116708][-1.0181298 -0.62232387 -0.35908532 -0.084042072 0.20620871 0.6071682 1.1521356 1.7611525 2.0541658 2.0365143 1.7969618 1.1499925 0.35515976 -0.3848989 -0.81450474][-1.068532 -0.66299629 -0.34573662 0.021219492 0.43316293 1.0421436 1.7958262 2.4734008 2.7182024 2.7460983 2.6081946 1.9274094 1.0150485 0.16999698 -0.42982435][-1.2647015 -0.85057831 -0.47286487 0.01225841 0.43206859 0.99787521 1.77139 2.4154828 2.5121534 2.4993665 2.490206 2.016597 1.2387936 0.43660188 -0.17146397][-1.4471471 -0.98094213 -0.63854468 -0.26125693 0.015128374 0.35984254 0.87190127 1.3011546 1.2733934 1.1900089 1.2846622 1.1314266 0.74374914 0.30785632 -0.053540826][-1.5325305 -1.0790439 -0.80653548 -0.64133751 -0.62795508 -0.62646317 -0.52588844 -0.39585876 -0.39199471 -0.39296627 -0.22458994 -0.11254108 -0.093542218 -0.11522818 -0.12152267][-1.0217024 -0.52364266 -0.23060465 -0.22721934 -0.50138927 -0.8696394 -1.1197071 -1.2238164 -1.2640703 -1.196532 -0.96759909 -0.73131847 -0.50713217 -0.24821997 -0.035866261][0.027409911 0.677191 1.0061007 0.92262626 0.40728378 -0.23377645 -0.72248793 -0.99278992 -1.1298006 -1.1036601 -0.93754786 -0.70749795 -0.47397864 -0.22137213 -0.03371644][0.959111 1.7919798 2.1581492 2.024581 1.3534322 0.52930951 -0.086445332 -0.42605233 -0.61087108 -0.69047964 -0.65751576 -0.57459605 -0.45925975 -0.37418437 -0.34235334][1.0953777 2.0560923 2.5046332 2.3467033 1.6674442 0.87901497 0.29108953 -0.0411098 -0.23826599 -0.37556088 -0.44621861 -0.48879433 -0.50603235 -0.54380596 -0.59046769][0.35144353 1.1950324 1.6439757 1.5029085 0.92180562 0.33056092 -0.039948344 -0.26210952 -0.42159271 -0.56297922 -0.67216611 -0.74302864 -0.79005921 -0.85159653 -0.90563649]]...]
INFO - root - 2017-12-16 08:03:39.937402: step 5310, loss = 0.67, batch loss = 0.40 (45.8 examples/sec; 0.175 sec/batch; 15h:51m:59s remains)
INFO - root - 2017-12-16 08:03:41.645144: step 5320, loss = 0.63, batch loss = 0.35 (45.7 examples/sec; 0.175 sec/batch; 15h:54m:06s remains)
INFO - root - 2017-12-16 08:03:43.353517: step 5330, loss = 0.51, batch loss = 0.24 (48.4 examples/sec; 0.165 sec/batch; 15h:00m:41s remains)
INFO - root - 2017-12-16 08:03:45.057541: step 5340, loss = 0.59, batch loss = 0.32 (45.7 examples/sec; 0.175 sec/batch; 15h:53m:59s remains)
INFO - root - 2017-12-16 08:03:46.755963: step 5350, loss = 0.73, batch loss = 0.45 (46.5 examples/sec; 0.172 sec/batch; 15h:37m:48s remains)
INFO - root - 2017-12-16 08:03:48.468117: step 5360, loss = 0.62, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 15h:15m:38s remains)
INFO - root - 2017-12-16 08:03:50.143769: step 5370, loss = 0.70, batch loss = 0.43 (47.0 examples/sec; 0.170 sec/batch; 15h:27m:13s remains)
INFO - root - 2017-12-16 08:03:51.837417: step 5380, loss = 0.54, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 14h:57m:56s remains)
INFO - root - 2017-12-16 08:03:53.542739: step 5390, loss = 0.56, batch loss = 0.29 (46.7 examples/sec; 0.171 sec/batch; 15h:34m:02s remains)
INFO - root - 2017-12-16 08:03:55.215351: step 5400, loss = 0.50, batch loss = 0.22 (47.2 examples/sec; 0.170 sec/batch; 15h:24m:24s remains)
2017-12-16 08:03:55.741997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85717309 -0.74046493 -0.65394616 -0.62611973 -0.67604423 -0.73882413 -0.77208745 -0.75041318 -0.6956799 -0.61989522 -0.52071226 -0.42580104 -0.35928357 -0.322796 -0.30330563][-1.657727 -1.5163492 -1.4264355 -1.4417008 -1.55614 -1.6615664 -1.6978762 -1.6478634 -1.5368154 -1.3737017 -1.1742921 -0.98547089 -0.83393407 -0.74258411 -0.68446422][-2.6203086 -2.5884683 -2.6189938 -2.7417693 -2.927072 -3.0441914 -3.040308 -2.9445505 -2.7646284 -2.5660808 -2.3290479 -2.0833416 -1.8752493 -1.7269703 -1.6068763][-2.9300046 -3.1384439 -3.3841767 -3.6457653 -3.8330121 -3.8586435 -3.7091837 -3.531024 -3.3844066 -3.3264477 -3.2419019 -3.1297727 -3.0025775 -2.8785043 -2.7458909][-2.5551929 -3.0130208 -3.4490359 -3.7148104 -3.7438922 -3.4724071 -2.9749813 -2.6663384 -2.6944487 -3.0265079 -3.3532891 -3.5440688 -3.6414225 -3.6384869 -3.5311666][-1.6177609 -2.2195566 -2.67026 -2.7801766 -2.4105117 -1.5935597 -0.5349443 0.0048513412 -0.35397625 -1.2789968 -2.156642 -2.7727127 -3.1542583 -3.3059106 -3.265759][-0.85378754 -1.3532948 -1.650834 -1.4961267 -0.67066574 0.77516377 2.4852147 3.4021211 2.6702919 1.1958288 -0.10213208 -1.0267344 -1.6273857 -1.9220034 -1.9651002][-0.65877473 -0.97300571 -1.0879564 -0.73903155 0.42586529 2.2870631 4.4596229 5.7384472 4.6174197 2.83287 1.4186457 0.49882114 -0.038197517 -0.33311486 -0.42662919][-1.0327752 -1.1503367 -1.1511958 -0.86610091 0.10185671 1.6879388 3.4429245 4.3505244 3.5306411 2.1521797 1.1431829 0.67116868 0.522827 0.4668256 0.44293725][-1.4471462 -1.3375635 -1.2804074 -1.3125744 -0.99660528 -0.22956014 0.66550267 1.0282487 0.52501 -0.25534284 -0.65383506 -0.50823033 -0.15269208 0.1078974 0.22004926][-1.401613 -1.0852852 -0.98351228 -1.3074644 -1.7417439 -1.9604465 -1.9793839 -2.1153302 -2.462852 -2.7917256 -2.7162955 -2.1707811 -1.4952983 -1.0241814 -0.80974078][-0.96560019 -0.42436826 -0.17257237 -0.64863253 -1.6692709 -2.6759963 -3.3807716 -3.8517256 -4.13891 -4.2041049 -3.8871889 -3.2363424 -2.5619745 -2.1078124 -1.8823676][-0.593961 0.13199103 0.55261004 0.12834859 -1.0481383 -2.3278704 -3.3132448 -3.8674746 -4.0717773 -3.9892282 -3.6234379 -3.1063471 -2.6553915 -2.3908098 -2.2405529][-0.49182618 0.39815056 1.0539423 0.8247298 -0.23763025 -1.4610274 -2.3184536 -2.6934936 -2.7174821 -2.5457761 -2.2886152 -2.0831826 -2.0077848 -2.0248024 -2.0208137][-0.63806009 0.19168675 0.88236606 0.9004811 0.084100842 -0.87137324 -1.3955268 -1.4870999 -1.3316214 -1.0764136 -0.94494492 -1.0227246 -1.2594066 -1.5170223 -1.6613114]]...]
INFO - root - 2017-12-16 08:03:57.410622: step 5410, loss = 0.65, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 15h:05m:07s remains)
INFO - root - 2017-12-16 08:03:59.083771: step 5420, loss = 0.55, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 15h:12m:22s remains)
INFO - root - 2017-12-16 08:04:00.725868: step 5430, loss = 0.78, batch loss = 0.50 (48.9 examples/sec; 0.164 sec/batch; 14h:52m:42s remains)
INFO - root - 2017-12-16 08:04:02.387744: step 5440, loss = 0.75, batch loss = 0.48 (48.1 examples/sec; 0.166 sec/batch; 15h:06m:46s remains)
INFO - root - 2017-12-16 08:04:04.060064: step 5450, loss = 0.60, batch loss = 0.33 (47.5 examples/sec; 0.168 sec/batch; 15h:18m:13s remains)
INFO - root - 2017-12-16 08:04:05.720172: step 5460, loss = 0.66, batch loss = 0.39 (47.0 examples/sec; 0.170 sec/batch; 15h:26m:59s remains)
INFO - root - 2017-12-16 08:04:07.372840: step 5470, loss = 0.53, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 15h:02m:53s remains)
INFO - root - 2017-12-16 08:04:09.051749: step 5480, loss = 0.62, batch loss = 0.35 (47.5 examples/sec; 0.168 sec/batch; 15h:17m:53s remains)
INFO - root - 2017-12-16 08:04:10.702349: step 5490, loss = 0.58, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 14h:49m:00s remains)
INFO - root - 2017-12-16 08:04:12.376916: step 5500, loss = 0.70, batch loss = 0.42 (48.5 examples/sec; 0.165 sec/batch; 14h:59m:38s remains)
2017-12-16 08:04:12.860671: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025997639 -0.22973228 -0.46745634 -0.50021589 -0.37567067 -0.36270165 -0.46369207 -0.51695037 -0.60633779 -0.68472934 -0.668231 -0.51775885 -0.32846391 -0.32542551 -0.39945161][0.50964093 0.098613024 -0.18754494 -0.20210183 -0.0081800222 0.086799026 0.036885858 -0.11766696 -0.3391192 -0.51265609 -0.55430305 -0.4412291 -0.28793657 -0.22897375 -0.24083853][0.73580432 0.19518328 -0.10912108 -0.083644867 0.23520207 0.49431133 0.5448308 0.34058714 0.06799674 -0.1303916 -0.25971711 -0.27883446 -0.27283132 -0.22872949 -0.22440362][0.84878349 0.18799496 -0.14175713 -0.052969098 0.42947578 0.909158 1.1332178 0.97858787 0.6555953 0.33962107 0.0928303 -0.14301383 -0.27965605 -0.26495266 -0.30417681][0.79176807 0.096941233 -0.25217843 -0.08200562 0.58544183 1.3122272 1.7869601 1.7480853 1.3105147 0.80148697 0.39256 0.015537381 -0.19772005 -0.2652328 -0.44104064][0.466537 -0.1522975 -0.48153007 -0.19832873 0.67548537 1.7200177 2.5837252 2.5783131 1.8650787 1.1712906 0.68292 0.30634785 0.043438554 -0.19069338 -0.58039677][-0.0851655 -0.641029 -0.820583 -0.37442732 0.60787606 1.8379116 3.0242836 2.9685552 1.9801943 1.2100718 0.73096585 0.40928221 0.15297031 -0.22120678 -0.81539953][-0.6491257 -1.1722341 -1.2216812 -0.624398 0.31222796 1.333189 2.0806246 1.9495695 1.2560122 0.6177547 0.24971962 0.0889163 -0.035280228 -0.36040318 -1.0415289][-1.0034506 -1.4617187 -1.4367168 -0.82257175 -0.10894775 0.46108747 0.68701363 0.45833993 0.042229176 -0.32724595 -0.46668947 -0.40874398 -0.32008719 -0.48530197 -1.0630157][-1.1914828 -1.5436189 -1.4535351 -0.91678536 -0.481452 -0.36903703 -0.54342413 -0.80417395 -1.0713363 -1.2323896 -1.1198742 -0.8213917 -0.53004181 -0.5048095 -0.87219048][-1.126416 -1.3654015 -1.2357098 -0.91803682 -0.818848 -0.97611666 -1.3045697 -1.548499 -1.7466328 -1.7602534 -1.4931549 -1.0116839 -0.5886544 -0.41012049 -0.58058155][-0.8877061 -1.0325395 -0.97733986 -0.88521826 -0.98616385 -1.2340801 -1.5995169 -1.8149254 -1.9787545 -1.9098614 -1.5951656 -1.0812073 -0.62071109 -0.3692919 -0.34527111][-0.68280661 -0.79840195 -0.829147 -0.83499527 -0.94342685 -1.1950898 -1.5303485 -1.7042348 -1.8367218 -1.7748581 -1.526804 -1.0704025 -0.69003296 -0.43159044 -0.31086266][-0.52500391 -0.62244582 -0.68652248 -0.67137086 -0.71760106 -0.92632842 -1.2531133 -1.4187412 -1.4556886 -1.3990561 -1.2661161 -0.96419847 -0.6991365 -0.53099144 -0.42310071][-0.46507132 -0.51459217 -0.50698507 -0.41136312 -0.36803663 -0.53581548 -0.862064 -1.0386688 -1.0588574 -1.000453 -0.89629203 -0.67895997 -0.51491094 -0.46154034 -0.45554364]]...]
INFO - root - 2017-12-16 08:04:14.548657: step 5510, loss = 0.56, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 15h:05m:37s remains)
INFO - root - 2017-12-16 08:04:16.206416: step 5520, loss = 0.58, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 15h:03m:42s remains)
INFO - root - 2017-12-16 08:04:17.887770: step 5530, loss = 0.60, batch loss = 0.33 (47.8 examples/sec; 0.168 sec/batch; 15h:12m:56s remains)
INFO - root - 2017-12-16 08:04:19.578022: step 5540, loss = 0.50, batch loss = 0.22 (45.7 examples/sec; 0.175 sec/batch; 15h:53m:57s remains)
INFO - root - 2017-12-16 08:04:21.275060: step 5550, loss = 0.70, batch loss = 0.43 (47.6 examples/sec; 0.168 sec/batch; 15h:16m:25s remains)
INFO - root - 2017-12-16 08:04:22.941011: step 5560, loss = 0.60, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 15h:20m:18s remains)
INFO - root - 2017-12-16 08:04:24.617959: step 5570, loss = 0.63, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 15h:02m:27s remains)
INFO - root - 2017-12-16 08:04:26.292635: step 5580, loss = 0.61, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 14h:57m:52s remains)
INFO - root - 2017-12-16 08:04:27.948380: step 5590, loss = 0.54, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 14h:49m:52s remains)
INFO - root - 2017-12-16 08:04:29.627209: step 5600, loss = 0.56, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 15h:03m:01s remains)
2017-12-16 08:04:30.104235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1328716 -3.8793256 -2.9816544 -1.8059719 -1.1388159 -1.3134283 -1.7078422 -1.5918366 -1.3484139 -1.7067814 -2.5214334 -3.1253181 -3.1441073 -2.7425237 -2.2731924][-4.5910521 -4.4491682 -3.7729402 -2.8418911 -2.2706139 -2.3031614 -2.4713526 -2.2279992 -1.8410828 -2.0055985 -2.6255348 -3.1202769 -3.1066091 -2.7314725 -2.2961078][-4.9500861 -4.8561683 -4.3660059 -3.7068889 -3.2913222 -3.3013406 -3.2751045 -2.7274003 -2.0322554 -1.8901454 -2.2877991 -2.7831798 -2.8702326 -2.5975709 -2.2252021][-5.224164 -5.1673622 -4.7198524 -4.1672974 -3.7691855 -3.5766425 -3.2259293 -2.3849564 -1.5091121 -1.2120361 -1.5703216 -2.19228 -2.5412805 -2.464889 -2.1621284][-5.1160083 -5.08543 -4.6004376 -3.8094869 -2.9136229 -2.2007642 -1.5204759 -0.65161204 -0.029611349 -0.090421438 -0.79809058 -1.776177 -2.392746 -2.4278316 -2.1396689][-4.5768919 -4.4755821 -3.7606657 -2.3825512 -0.57652235 0.86023593 1.6559205 2.0678844 1.8288486 0.80710459 -0.62974095 -1.9637468 -2.6027069 -2.5329909 -2.1370797][-3.9050331 -3.6529562 -2.61322 -0.57004893 2.1650083 4.349719 5.1260977 4.6575241 3.125324 0.88965964 -1.2544848 -2.685534 -3.1000953 -2.747529 -2.1858211][-3.4315517 -3.1009703 -1.8253267 0.55894828 3.7649105 6.4852915 6.9452667 5.6882315 3.2053516 0.20960903 -2.3056297 -3.632292 -3.691273 -2.9962831 -2.2560074][-3.0773349 -2.9472156 -1.861397 0.12863529 2.6408317 4.6920519 5.3373928 4.4885149 2.0903351 -0.97017676 -3.3950396 -4.400877 -4.0762444 -3.1107101 -2.2261357][-2.9750829 -3.2040145 -2.5297558 -1.1798027 0.43140221 1.85344 2.6221149 2.3644598 0.60674644 -1.9160262 -3.9006298 -4.6249075 -4.0921555 -3.0424986 -2.1429503][-3.149425 -3.699023 -3.416698 -2.5954802 -1.6530544 -0.67996252 0.16876864 0.44232321 -0.51122487 -2.2674868 -3.7773104 -4.2923803 -3.7604032 -2.7979312 -2.023385][-3.1841531 -3.9708321 -3.9946296 -3.5488834 -3.0071406 -2.2990119 -1.4421954 -0.81713021 -1.1549008 -2.3073428 -3.4664218 -3.8351791 -3.3790455 -2.567322 -1.9244709][-3.1682315 -3.9019067 -3.9700358 -3.6654525 -3.3024638 -2.7805233 -2.0169516 -1.3028244 -1.3738921 -2.2295036 -3.1291959 -3.3847094 -2.9645209 -2.2872579 -1.7783176][-3.1983368 -3.6409378 -3.5863347 -3.2365816 -2.8888147 -2.47017 -1.8748102 -1.294376 -1.3357332 -2.0453289 -2.7846675 -2.9602695 -2.5516686 -1.9612622 -1.5892875][-3.0800111 -3.2217717 -2.9862452 -2.5777271 -2.2240815 -1.8896989 -1.4563804 -1.0281131 -1.1525 -1.825161 -2.4707873 -2.579531 -2.1842854 -1.6428857 -1.3936541]]...]
INFO - root - 2017-12-16 08:04:31.846873: step 5610, loss = 0.63, batch loss = 0.36 (48.7 examples/sec; 0.164 sec/batch; 14h:54m:04s remains)
INFO - root - 2017-12-16 08:04:33.552694: step 5620, loss = 0.55, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 15h:32m:50s remains)
INFO - root - 2017-12-16 08:04:35.238318: step 5630, loss = 0.65, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 15h:08m:14s remains)
INFO - root - 2017-12-16 08:04:36.894415: step 5640, loss = 0.69, batch loss = 0.41 (48.5 examples/sec; 0.165 sec/batch; 14h:59m:06s remains)
INFO - root - 2017-12-16 08:04:38.587171: step 5650, loss = 0.59, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 15h:26m:39s remains)
INFO - root - 2017-12-16 08:04:40.250213: step 5660, loss = 0.58, batch loss = 0.31 (48.6 examples/sec; 0.165 sec/batch; 14h:57m:21s remains)
INFO - root - 2017-12-16 08:04:41.917354: step 5670, loss = 0.57, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 15h:17m:02s remains)
INFO - root - 2017-12-16 08:04:43.606917: step 5680, loss = 0.54, batch loss = 0.27 (46.7 examples/sec; 0.171 sec/batch; 15h:33m:14s remains)
INFO - root - 2017-12-16 08:04:45.298385: step 5690, loss = 0.62, batch loss = 0.35 (47.7 examples/sec; 0.168 sec/batch; 15h:13m:57s remains)
INFO - root - 2017-12-16 08:04:46.992265: step 5700, loss = 0.75, batch loss = 0.48 (48.0 examples/sec; 0.167 sec/batch; 15h:08m:31s remains)
2017-12-16 08:04:47.463234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.53618968 -0.3965894 -0.39584935 -0.50937569 -0.64004576 -0.71837604 -0.75103223 -0.8057735 -0.90871489 -1.0474637 -1.1667188 -1.1855646 -1.1091509 -0.93011707 -0.71400368][-1.0358363 -0.92858416 -0.977509 -1.1122975 -1.2301915 -1.2480054 -1.2194594 -1.2285516 -1.2971302 -1.4238999 -1.5167925 -1.4721334 -1.3076105 -1.0566952 -0.85662544][-1.5981394 -1.5513991 -1.6378251 -1.7608004 -1.8130195 -1.7290099 -1.5884957 -1.5461967 -1.6329691 -1.7964313 -1.900089 -1.8020532 -1.5068793 -1.1247653 -0.87031949][-1.6751368 -1.7169635 -1.8312765 -1.9143238 -1.8854326 -1.6561742 -1.3771651 -1.3026459 -1.4712137 -1.7469472 -1.9174323 -1.8126193 -1.4430404 -0.95668679 -0.62113214][-1.0350697 -1.1401269 -1.2495509 -1.2613825 -1.1445595 -0.79591405 -0.35795557 -0.22353768 -0.52085376 -0.99345684 -1.2939206 -1.2471895 -0.91104484 -0.42292261 -0.068046212][-0.055808187 -0.17366505 -0.20192885 -0.080896974 0.1727438 0.68593073 1.3398194 1.5908098 1.1446748 0.38405108 -0.13303375 -0.19993556 0.031167984 0.39924479 0.68876553][0.57492352 0.50933003 0.62406754 0.94513941 1.4152513 2.1226888 2.9969258 3.4424043 2.9070897 1.8320901 0.99490023 0.67197347 0.70187449 0.901634 1.0758727][0.3363142 0.35390806 0.6108768 1.0966661 1.7393551 2.605197 3.6605277 4.33933 3.7787733 2.4718418 1.3524828 0.74102974 0.56934738 0.61561751 0.66585231][-0.5089376 -0.4505223 -0.14873433 0.36198545 1.0351379 1.8641639 2.7301292 3.2655506 2.8794312 1.779521 0.66369605 -0.086755633 -0.38921523 -0.41188931 -0.38529551][-1.1490041 -1.1046768 -0.85398006 -0.45680416 0.046694517 0.61953306 1.1573563 1.4833372 1.2736259 0.51403737 -0.40521979 -1.1314564 -1.4660437 -1.4773829 -1.3410356][-1.0843544 -1.0286715 -0.893419 -0.69994485 -0.44917786 -0.19141257 0.035231948 0.18939519 0.058577061 -0.43029797 -1.1041107 -1.6905899 -1.9744279 -1.9652207 -1.739082][-0.58449125 -0.45591593 -0.40129578 -0.39618802 -0.38770092 -0.40263212 -0.41040218 -0.38962746 -0.49391139 -0.81698275 -1.2645293 -1.6552906 -1.833084 -1.7902526 -1.5546359][-0.13962209 0.076732635 0.10914624 -0.0019956827 -0.15610898 -0.32662749 -0.43930054 -0.44954455 -0.51032817 -0.70341718 -0.951696 -1.1440878 -1.1970246 -1.1184305 -0.92561895][-0.054004192 0.17443705 0.19301152 0.040980458 -0.15059793 -0.31290579 -0.37147772 -0.3138541 -0.31037652 -0.40291262 -0.46335471 -0.4691124 -0.39248323 -0.26088595 -0.095165968][-0.2590431 -0.15356445 -0.19237614 -0.32901525 -0.45652115 -0.507028 -0.4421097 -0.30385125 -0.24986839 -0.25799906 -0.18885183 -0.044789672 0.15631557 0.35671234 0.55848169]]...]
INFO - root - 2017-12-16 08:04:49.135708: step 5710, loss = 0.58, batch loss = 0.31 (47.4 examples/sec; 0.169 sec/batch; 15h:20m:11s remains)
INFO - root - 2017-12-16 08:04:50.805568: step 5720, loss = 0.65, batch loss = 0.38 (49.2 examples/sec; 0.163 sec/batch; 14h:45m:53s remains)
INFO - root - 2017-12-16 08:04:52.471824: step 5730, loss = 0.76, batch loss = 0.49 (48.6 examples/sec; 0.165 sec/batch; 14h:57m:05s remains)
INFO - root - 2017-12-16 08:04:54.173700: step 5740, loss = 0.57, batch loss = 0.30 (49.1 examples/sec; 0.163 sec/batch; 14h:46m:28s remains)
INFO - root - 2017-12-16 08:04:55.876193: step 5750, loss = 0.60, batch loss = 0.32 (46.9 examples/sec; 0.171 sec/batch; 15h:28m:43s remains)
INFO - root - 2017-12-16 08:04:57.588819: step 5760, loss = 0.63, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 15h:07m:03s remains)
INFO - root - 2017-12-16 08:04:59.275284: step 5770, loss = 0.71, batch loss = 0.44 (47.1 examples/sec; 0.170 sec/batch; 15h:25m:03s remains)
INFO - root - 2017-12-16 08:05:00.950472: step 5780, loss = 0.73, batch loss = 0.46 (48.1 examples/sec; 0.166 sec/batch; 15h:04m:46s remains)
INFO - root - 2017-12-16 08:05:02.637931: step 5790, loss = 0.60, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 15h:13m:41s remains)
INFO - root - 2017-12-16 08:05:04.298216: step 5800, loss = 0.52, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 15h:17m:12s remains)
2017-12-16 08:05:04.781435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4895597 -1.2094733 -1.0698345 -1.0080004 -1.1831915 -1.6783783 -2.2505033 -2.5028207 -2.3627472 -1.9710603 -1.7702268 -1.963697 -2.208791 -1.8974426 -1.1102216][-0.67605639 -0.669052 -0.82321632 -1.0496938 -1.4281037 -1.8876947 -2.2068911 -2.2253227 -1.9919415 -1.7170084 -1.6554575 -1.8151165 -1.843061 -1.2842859 -0.29352462][-0.0074108839 -0.29767942 -0.76880908 -1.2792392 -1.7770262 -2.0462186 -1.8986262 -1.5181427 -1.1310824 -1.0021398 -1.1289743 -1.3874893 -1.3600407 -0.7787379 0.13982606][0.16347766 -0.44028151 -1.0776021 -1.7377248 -2.1576538 -2.03051 -1.2618489 -0.32449198 0.25111675 0.19832087 -0.22534156 -0.7749778 -0.93176508 -0.55509269 0.081551671][-0.42496252 -1.2338068 -1.8645821 -2.3605332 -2.3637028 -1.5880235 -0.15005326 1.2557631 1.8569448 1.4478195 0.60300779 -0.29648018 -0.76799345 -0.69491827 -0.40458691][-1.4109317 -2.2457681 -2.7005455 -2.8589203 -2.2848697 -0.81223273 1.1994653 2.9445605 3.3189015 2.4275703 1.1230285 -0.12191737 -0.88867176 -1.1145349 -1.086689][-2.2305892 -2.913022 -3.1129847 -2.8475137 -1.7360102 0.25322485 2.6358809 4.4844084 4.4367208 3.0461702 1.3636913 -0.11152053 -1.0389189 -1.4539715 -1.5717387][-2.6330233 -3.0275676 -2.8811536 -2.2423136 -0.86585951 1.3590522 3.8078113 5.4261479 4.7835169 2.972455 1.0691924 -0.42794788 -1.2810601 -1.6780077 -1.8101091][-2.70268 -2.8111036 -2.4130592 -1.5961791 -0.20925999 1.8626695 4.0288525 5.177917 4.2040596 2.2880702 0.41744137 -0.92171508 -1.5970275 -1.8802421 -1.9233381][-2.6132913 -2.5678234 -2.1325622 -1.3719871 -0.17686379 1.5591919 3.3361058 4.0040784 2.9916387 1.2586002 -0.37803662 -1.4617069 -1.9270506 -2.0859249 -2.0383952][-2.5005898 -2.4501586 -2.1470466 -1.6307606 -0.74440527 0.62211227 2.0502961 2.4292645 1.5203354 0.12247729 -1.1671934 -1.9396725 -2.2049143 -2.2318153 -2.0959325][-2.5253623 -2.4573348 -2.3003154 -2.0757902 -1.5234638 -0.5207088 0.54279733 0.74832892 0.025042295 -1.0314856 -1.9357536 -2.3884306 -2.4524889 -2.3607225 -2.1350081][-2.511518 -2.4177151 -2.3747241 -2.3668454 -2.0987368 -1.4411297 -0.74547851 -0.65158892 -1.212135 -1.9590969 -2.5255842 -2.7080593 -2.6181831 -2.4256454 -2.1287494][-2.2687564 -2.1157043 -2.1722672 -2.3508666 -2.3173351 -1.9313284 -1.5074017 -1.4862729 -1.9218185 -2.4278562 -2.7389014 -2.7652292 -2.6092687 -2.3726783 -2.0577292][-1.7977626 -1.6159902 -1.7509567 -2.0245962 -2.1105506 -1.8856097 -1.6758494 -1.7257303 -2.07401 -2.427402 -2.5887325 -2.545085 -2.3925829 -2.1867514 -1.9389771]]...]
INFO - root - 2017-12-16 08:05:06.438142: step 5810, loss = 0.65, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 15h:09m:11s remains)
INFO - root - 2017-12-16 08:05:08.146255: step 5820, loss = 0.61, batch loss = 0.34 (47.0 examples/sec; 0.170 sec/batch; 15h:25m:51s remains)
INFO - root - 2017-12-16 08:05:09.827334: step 5830, loss = 0.57, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 15h:31m:26s remains)
INFO - root - 2017-12-16 08:05:11.488941: step 5840, loss = 0.50, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 14h:57m:32s remains)
INFO - root - 2017-12-16 08:05:13.166528: step 5850, loss = 0.60, batch loss = 0.33 (47.5 examples/sec; 0.169 sec/batch; 15h:17m:47s remains)
INFO - root - 2017-12-16 08:05:14.865387: step 5860, loss = 0.58, batch loss = 0.31 (46.3 examples/sec; 0.173 sec/batch; 15h:40m:40s remains)
INFO - root - 2017-12-16 08:05:16.569752: step 5870, loss = 0.55, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 15h:42m:27s remains)
INFO - root - 2017-12-16 08:05:18.255329: step 5880, loss = 0.49, batch loss = 0.22 (48.2 examples/sec; 0.166 sec/batch; 15h:02m:40s remains)
INFO - root - 2017-12-16 08:05:19.916757: step 5890, loss = 0.63, batch loss = 0.36 (48.6 examples/sec; 0.164 sec/batch; 14h:55m:19s remains)
INFO - root - 2017-12-16 08:05:21.586270: step 5900, loss = 0.64, batch loss = 0.37 (49.9 examples/sec; 0.160 sec/batch; 14h:32m:14s remains)
2017-12-16 08:05:22.055212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1914759 -1.6462555 -0.85424829 0.068845987 0.94479573 1.5471708 1.596368 1.2792858 1.1098987 1.0884231 0.9136349 0.45921671 -0.043309927 -0.54582572 -1.1328909][-2.1317296 -1.8912535 -1.3207583 -0.44081461 0.44160068 1.0727283 1.1619829 0.95376 0.83328974 0.82723653 0.621555 0.12763774 -0.36228263 -0.8735348 -1.5567276][-1.6970736 -1.7882192 -1.4843937 -0.83923125 -0.17071259 0.25646889 0.29866922 0.12891877 -0.0053055286 -0.081728578 -0.34818482 -0.78868032 -1.1931996 -1.6000572 -2.207952][-0.9791615 -1.3019562 -1.2718663 -0.8669368 -0.42500031 -0.22004509 -0.24795771 -0.4329766 -0.65793526 -0.89166439 -1.2800333 -1.7870111 -2.1662145 -2.4504786 -2.8876262][-0.39661944 -0.75772107 -0.81060994 -0.49440086 -0.1704551 -0.12966132 -0.28567111 -0.51327109 -0.86317956 -1.3181815 -1.883496 -2.4552481 -2.8559332 -3.0291452 -3.239512][-0.33252323 -0.4921627 -0.47318029 -0.16617465 0.17739284 0.19795215 -0.049907804 -0.41404867 -0.87264478 -1.526992 -2.2247 -2.8231833 -3.1534133 -3.1522846 -3.0725935][-0.743345 -0.67016494 -0.53501844 -0.19662642 0.25338995 0.32025611 -0.020519614 -0.40319681 -0.88723779 -1.606997 -2.3493171 -2.8774078 -3.0894289 -2.9619994 -2.673758][-1.3351207 -1.1330631 -0.96865124 -0.63123703 -0.15115929 -0.031021714 -0.32253265 -0.53019345 -0.822106 -1.4816462 -2.1592083 -2.5632639 -2.6729317 -2.5837641 -2.3542278][-1.7712584 -1.6425216 -1.6097727 -1.395195 -0.92980283 -0.77059615 -1.0093474 -1.0503143 -1.0567091 -1.4594141 -1.8877217 -2.0284829 -2.0465007 -2.1916461 -2.264416][-1.8082234 -1.8142577 -1.9809475 -2.0063908 -1.6998928 -1.5723073 -1.7625854 -1.6460124 -1.3821981 -1.438958 -1.5808564 -1.4737616 -1.4488969 -1.8992245 -2.4308681][-1.3714066 -1.4794904 -1.7665552 -1.9798951 -1.9505576 -1.9661564 -2.1306636 -1.9826969 -1.5940994 -1.3872839 -1.2690663 -0.97731382 -1.0228425 -1.7505381 -2.6363153][-0.88963091 -0.883466 -1.0939045 -1.3959918 -1.5487306 -1.6902235 -1.9511827 -1.8942544 -1.5863123 -1.2567015 -0.91835791 -0.54553163 -0.70770025 -1.5213351 -2.4027784][-0.69092464 -0.43464291 -0.39410973 -0.57041025 -0.72494328 -0.89941454 -1.187938 -1.2147248 -0.975401 -0.65177989 -0.25153863 0.17363703 0.055448651 -0.64192009 -1.3923314][-0.62097442 -0.16091287 0.13594997 0.20308912 0.23322332 0.15172803 -0.044741154 -0.0036143064 0.20321214 0.426283 0.77857053 1.1765584 1.1738664 0.70695961 0.20291126][-0.54616141 0.012518048 0.43842113 0.69352758 0.91959345 0.99803936 0.9741863 1.2161056 1.498618 1.5922538 1.8148202 2.2026534 2.318459 2.0423121 1.7563044]]...]
INFO - root - 2017-12-16 08:05:23.723461: step 5910, loss = 0.64, batch loss = 0.37 (47.0 examples/sec; 0.170 sec/batch; 15h:26m:45s remains)
INFO - root - 2017-12-16 08:05:25.387357: step 5920, loss = 0.65, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 15h:09m:19s remains)
INFO - root - 2017-12-16 08:05:27.051980: step 5930, loss = 0.57, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 14h:54m:09s remains)
INFO - root - 2017-12-16 08:05:28.687931: step 5940, loss = 0.78, batch loss = 0.51 (49.3 examples/sec; 0.162 sec/batch; 14h:43m:48s remains)
INFO - root - 2017-12-16 08:05:30.345271: step 5950, loss = 0.63, batch loss = 0.36 (48.5 examples/sec; 0.165 sec/batch; 14h:57m:49s remains)
INFO - root - 2017-12-16 08:05:32.032645: step 5960, loss = 0.75, batch loss = 0.48 (47.5 examples/sec; 0.168 sec/batch; 15h:16m:49s remains)
INFO - root - 2017-12-16 08:05:33.734371: step 5970, loss = 0.67, batch loss = 0.40 (46.0 examples/sec; 0.174 sec/batch; 15h:46m:02s remains)
INFO - root - 2017-12-16 08:05:35.404357: step 5980, loss = 0.70, batch loss = 0.42 (47.0 examples/sec; 0.170 sec/batch; 15h:26m:10s remains)
INFO - root - 2017-12-16 08:05:37.100331: step 5990, loss = 0.68, batch loss = 0.41 (48.2 examples/sec; 0.166 sec/batch; 15h:03m:18s remains)
INFO - root - 2017-12-16 08:05:38.792192: step 6000, loss = 0.72, batch loss = 0.45 (45.6 examples/sec; 0.175 sec/batch; 15h:53m:47s remains)
2017-12-16 08:05:39.264242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.16518176 -0.53351843 -0.89575636 -0.91354984 -0.78554189 -0.620316 -0.15832579 0.43907905 0.72613811 0.80472469 0.69961739 0.11007643 -0.84768057 -1.6164482 -1.8731744][-1.2318888 -1.6272619 -1.9618617 -1.9305431 -1.7348819 -1.5295166 -1.1267731 -0.66491735 -0.41874647 -0.3501991 -0.43294513 -0.84104669 -1.4224128 -1.8409182 -1.9424114][-2.0973306 -2.4682131 -2.758219 -2.6788616 -2.4046969 -2.2332218 -2.0055549 -1.7700691 -1.6506081 -1.623928 -1.619184 -1.7209667 -1.863641 -1.921742 -1.8655639][-2.3204298 -2.6714625 -2.9241858 -2.7543087 -2.4002562 -2.1823354 -2.0224214 -1.9668567 -2.0697618 -2.1546464 -2.1069055 -2.0502019 -2.0163369 -1.9617336 -1.8654277][-1.9941952 -2.2988322 -2.4864421 -2.204222 -1.7149236 -1.2968526 -0.94301009 -0.82161248 -1.1164744 -1.4723599 -1.6595209 -1.790166 -1.9513824 -2.0485988 -1.9973065][-1.4721428 -1.6907438 -1.736728 -1.2906446 -0.50930357 0.31434059 1.1401453 1.6294003 1.2669103 0.45426536 -0.30637157 -1.0080422 -1.6620231 -2.0622678 -2.108453][-0.89332557 -1.0162721 -0.92578894 -0.36491263 0.69547248 1.9910028 3.4816663 4.5946312 4.2195511 2.7212551 1.1472147 -0.23533118 -1.3763206 -2.0714707 -2.2748098][-0.54739642 -0.56550062 -0.44718349 0.06880486 1.1074095 2.6202295 4.5886831 6.3077726 5.9856567 3.8242867 1.6409147 -0.022322059 -1.2989753 -2.107013 -2.3510966][-0.70751989 -0.6919688 -0.62338269 -0.31324506 0.4502573 1.7400987 3.405091 4.6017532 4.2728062 2.4712012 0.57953978 -0.70543814 -1.5920988 -2.1385126 -2.1759536][-1.3008046 -1.233954 -1.1929672 -1.0598634 -0.55225253 0.39423943 1.4787171 2.1143839 1.7029459 0.29118514 -1.1500621 -1.9263771 -2.2706673 -2.3312049 -1.9289837][-2.0532904 -1.9131089 -1.8013394 -1.6897249 -1.3130645 -0.65535164 -0.046414137 0.12661219 -0.42660213 -1.5848292 -2.6544261 -3.0380177 -2.9629335 -2.6157897 -1.7832835][-2.6272702 -2.3834693 -2.1002216 -1.9027553 -1.5825697 -1.1455696 -0.8783797 -1.0587399 -1.7805668 -2.7823567 -3.5986187 -3.7693334 -3.4864721 -2.9311452 -1.9379498][-2.8789601 -2.5012345 -2.0307403 -1.7923126 -1.5437207 -1.2674816 -1.1925173 -1.5692647 -2.3665876 -3.264112 -3.9573212 -4.0810614 -3.8227553 -3.3583832 -2.5679431][-2.7672954 -2.380106 -1.832903 -1.5692806 -1.3871202 -1.253536 -1.2918591 -1.7161413 -2.4786718 -3.2453942 -3.9041529 -4.1401877 -4.0736275 -3.8382869 -3.3468795][-2.4944208 -2.223774 -1.7424959 -1.5429904 -1.4971296 -1.472596 -1.5332786 -1.8956406 -2.5152013 -3.1616077 -3.7564158 -4.0663056 -4.1025286 -4.0036726 -3.7115979]]...]
INFO - root - 2017-12-16 08:05:40.977369: step 6010, loss = 0.54, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 15h:04m:02s remains)
INFO - root - 2017-12-16 08:05:42.675630: step 6020, loss = 0.59, batch loss = 0.32 (44.3 examples/sec; 0.181 sec/batch; 16h:22m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:05:44.390514: step 6030, loss = 0.60, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 15h:13m:30s remains)
INFO - root - 2017-12-16 08:05:46.053996: step 6040, loss = 0.71, batch loss = 0.44 (47.9 examples/sec; 0.167 sec/batch; 15h:09m:37s remains)
INFO - root - 2017-12-16 08:05:47.748537: step 6050, loss = 0.67, batch loss = 0.39 (47.4 examples/sec; 0.169 sec/batch; 15h:17m:23s remains)
INFO - root - 2017-12-16 08:05:49.450383: step 6060, loss = 0.62, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 15h:04m:45s remains)
INFO - root - 2017-12-16 08:05:51.126733: step 6070, loss = 0.51, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 15h:19m:03s remains)
INFO - root - 2017-12-16 08:05:52.795626: step 6080, loss = 0.57, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 15h:30m:05s remains)
INFO - root - 2017-12-16 08:05:54.477922: step 6090, loss = 0.63, batch loss = 0.36 (48.5 examples/sec; 0.165 sec/batch; 14h:56m:56s remains)
INFO - root - 2017-12-16 08:05:56.177355: step 6100, loss = 0.64, batch loss = 0.37 (44.7 examples/sec; 0.179 sec/batch; 16h:12m:43s remains)
2017-12-16 08:05:56.664254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9987038 -1.941519 -1.7309456 -1.4061744 -1.072646 -0.79752123 -0.67439497 -0.67356086 -0.79862845 -1.0225842 -1.3133572 -1.5472676 -1.6066549 -1.4329118 -1.1348392][-2.4824491 -2.5668905 -2.4520707 -2.1052067 -1.6704472 -1.302103 -1.0179975 -0.82921314 -0.77799463 -0.83886182 -1.0684683 -1.3218199 -1.4452199 -1.3096489 -1.0836574][-2.7549562 -2.988297 -3.0895233 -2.875371 -2.4958966 -2.1130552 -1.7730405 -1.5000429 -1.3764338 -1.3720191 -1.4897134 -1.6250386 -1.6476483 -1.4916774 -1.2942348][-2.8256512 -3.0850635 -3.3087332 -3.1867898 -2.8473217 -2.4558294 -2.1506524 -1.9646348 -1.9528015 -2.0176787 -2.0514908 -2.026268 -1.9691712 -1.8112282 -1.5854899][-2.54371 -2.6627541 -2.6977553 -2.4891474 -2.1260109 -1.7383677 -1.4824166 -1.4113185 -1.5897114 -1.7948025 -1.8109016 -1.7335566 -1.6701778 -1.6438692 -1.5067788][-1.9158137 -1.8194872 -1.5962867 -1.2206223 -0.77549422 -0.29597485 -0.023662567 -0.041595459 -0.29555547 -0.53361547 -0.61886597 -0.62995028 -0.71085989 -0.98889923 -1.1459808][-1.1314237 -1.0248168 -0.68998218 -0.20002854 0.34253168 1.0010793 1.5471287 1.6203458 1.2789462 0.99095511 0.82249165 0.661778 0.31990981 -0.31817293 -0.85989332][-0.62585521 -0.64267707 -0.39566755 -0.011422515 0.4713819 1.2461755 2.0445368 2.3409719 2.1347709 1.9204195 1.7206695 1.3442295 0.66996837 -0.24752605 -1.0414338][-0.73996389 -0.84892488 -0.77323759 -0.62454891 -0.3768028 0.205086 0.89588165 1.1903701 1.2271881 1.1868355 1.0402164 0.62034464 -0.061355948 -0.87803161 -1.511017][-1.5204873 -1.706906 -1.7444936 -1.7530935 -1.7275348 -1.4798425 -1.0503355 -0.8109746 -0.71080327 -0.65464866 -0.7322557 -1.0191982 -1.3736901 -1.7009574 -1.9045575][-2.3343875 -2.4718959 -2.536418 -2.6718607 -2.8130305 -2.7972806 -2.632257 -2.4794974 -2.3826487 -2.2804785 -2.220108 -2.2275808 -2.2250214 -2.1070359 -1.8795149][-2.6786323 -2.7692695 -2.8276658 -2.9707391 -3.1451664 -3.2305489 -3.2157292 -3.1310093 -3.0459018 -2.9288874 -2.7727613 -2.5911508 -2.3764784 -2.0718091 -1.7067589][-2.6092572 -2.6893501 -2.7590613 -2.8531911 -2.9661756 -3.0500941 -3.0861201 -3.050405 -2.9805489 -2.884048 -2.7535748 -2.5767679 -2.3621478 -2.0958271 -1.8113828][-2.3137915 -2.398623 -2.4675698 -2.5288596 -2.5939965 -2.6583536 -2.7082129 -2.7073629 -2.6974154 -2.6726627 -2.6183043 -2.5239911 -2.4157791 -2.2844889 -2.1347997][-1.8947506 -1.953625 -2.0151398 -2.0553741 -2.09408 -2.1446157 -2.1866152 -2.1961288 -2.2021773 -2.2113509 -2.211916 -2.1910191 -2.1682272 -2.1408942 -2.0797868]]...]
INFO - root - 2017-12-16 08:05:58.339816: step 6110, loss = 0.51, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 15h:03m:57s remains)
INFO - root - 2017-12-16 08:06:00.014656: step 6120, loss = 0.66, batch loss = 0.39 (48.1 examples/sec; 0.166 sec/batch; 15h:04m:04s remains)
INFO - root - 2017-12-16 08:06:01.709111: step 6130, loss = 0.64, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 14h:57m:53s remains)
INFO - root - 2017-12-16 08:06:03.387292: step 6140, loss = 0.62, batch loss = 0.35 (46.6 examples/sec; 0.172 sec/batch; 15h:33m:07s remains)
INFO - root - 2017-12-16 08:06:05.089447: step 6150, loss = 0.49, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 14h:56m:21s remains)
INFO - root - 2017-12-16 08:06:06.754068: step 6160, loss = 0.66, batch loss = 0.38 (48.5 examples/sec; 0.165 sec/batch; 14h:56m:53s remains)
INFO - root - 2017-12-16 08:06:08.427122: step 6170, loss = 0.66, batch loss = 0.39 (50.2 examples/sec; 0.159 sec/batch; 14h:26m:34s remains)
INFO - root - 2017-12-16 08:06:10.104262: step 6180, loss = 0.62, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 15h:04m:46s remains)
INFO - root - 2017-12-16 08:06:11.766969: step 6190, loss = 0.66, batch loss = 0.39 (46.9 examples/sec; 0.171 sec/batch; 15h:27m:32s remains)
INFO - root - 2017-12-16 08:06:13.438801: step 6200, loss = 0.54, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 15h:18m:51s remains)
2017-12-16 08:06:13.940644: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47471321 -0.74623728 -1.6930157 -2.013216 -2.0430653 -2.204726 -2.5377126 -2.7184496 -2.4657991 -1.7851728 -1.0921582 -0.88924611 -1.4302117 -2.2922583 -2.8011987][0.5362879 -0.72502291 -1.7135963 -2.0031786 -1.9444938 -2.0528388 -2.3893459 -2.5603964 -2.1340954 -1.1431645 -0.15072477 0.050147057 -0.73486352 -1.946079 -2.689425][-0.113873 -1.212081 -2.1136878 -2.3321352 -2.193011 -2.1705842 -2.3616378 -2.4204204 -1.7842931 -0.45046186 0.85067189 1.0473722 -0.027898192 -1.6013775 -2.6283298][-1.1460943 -2.0275967 -2.7767475 -2.8900533 -2.6237817 -2.404108 -2.3903258 -2.2976735 -1.5000339 0.097640395 1.6631423 1.8840171 0.54636681 -1.3178964 -2.550159][-1.9570909 -2.5967908 -3.1013508 -3.0663671 -2.54868 -2.1114435 -2.0265248 -1.9470507 -1.1649354 0.52266324 2.1682129 2.3548212 0.82584918 -1.2152519 -2.5235858][-2.2567046 -2.6850538 -2.9435532 -2.650403 -1.8121023 -1.1227564 -1.0576692 -1.2387705 -0.75263345 0.71889603 2.2128868 2.2232704 0.62324035 -1.3602664 -2.5655472][-2.4867272 -2.7584915 -2.7271743 -2.1002455 -0.91458976 0.064294934 0.043146014 -0.5670855 -0.62849832 0.34398329 1.4277397 1.2487906 -0.18413675 -1.8489711 -2.7747097][-2.5673122 -2.772233 -2.5995309 -1.8047791 -0.43382812 0.7601558 0.72958934 -0.25274098 -0.84433079 -0.49977219 0.098683476 -0.1913693 -1.3534198 -2.6005576 -3.1986461][-2.5178874 -2.8241656 -2.750087 -1.9996762 -0.66436696 0.59784997 0.61297071 -0.43616772 -1.2817789 -1.3585372 -1.1285856 -1.4280251 -2.3317657 -3.2516079 -3.6158614][-2.3166075 -2.819905 -3.0563672 -2.6134162 -1.5376043 -0.43910909 -0.35769451 -1.1577508 -1.9397835 -2.1481977 -2.0120463 -2.1272745 -2.8158636 -3.5730586 -3.7837305][-1.7822675 -2.491097 -3.0730016 -2.9891317 -2.314559 -1.5642753 -1.4489813 -1.9367557 -2.4462414 -2.549149 -2.3776402 -2.3447318 -2.8413196 -3.4726391 -3.5992358][-1.0627437 -1.8292347 -2.5669048 -2.7610857 -2.4922271 -2.0951226 -1.9933939 -2.2155738 -2.4429407 -2.4058666 -2.1995704 -2.1491342 -2.5221472 -3.0472274 -3.1524549][-0.54463768 -1.21789 -1.9011147 -2.2129993 -2.2202313 -2.090064 -2.0393851 -2.1144891 -2.1840639 -2.0903895 -1.8963188 -1.8422062 -2.1174269 -2.5058222 -2.5798278][-0.57852435 -1.0373687 -1.5269561 -1.8065956 -1.8978064 -1.8880063 -1.8882605 -1.9101748 -1.9215986 -1.8518543 -1.7218719 -1.6839664 -1.8476294 -2.053972 -2.0663912][-0.91643608 -1.1524568 -1.41949 -1.5923193 -1.6726958 -1.6986196 -1.7052814 -1.7009815 -1.6920109 -1.6575278 -1.6110328 -1.6035306 -1.6621501 -1.7336196 -1.7038518]]...]
INFO - root - 2017-12-16 08:06:15.590508: step 6210, loss = 0.77, batch loss = 0.50 (48.5 examples/sec; 0.165 sec/batch; 14h:56m:22s remains)
INFO - root - 2017-12-16 08:06:17.257009: step 6220, loss = 0.58, batch loss = 0.31 (49.5 examples/sec; 0.162 sec/batch; 14h:38m:38s remains)
INFO - root - 2017-12-16 08:06:18.920936: step 6230, loss = 0.61, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 15h:09m:16s remains)
INFO - root - 2017-12-16 08:06:20.618536: step 6240, loss = 0.60, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 15h:17m:38s remains)
INFO - root - 2017-12-16 08:06:22.320508: step 6250, loss = 0.69, batch loss = 0.42 (47.3 examples/sec; 0.169 sec/batch; 15h:19m:29s remains)
INFO - root - 2017-12-16 08:06:24.008718: step 6260, loss = 0.69, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 15h:12m:12s remains)
INFO - root - 2017-12-16 08:06:25.683164: step 6270, loss = 0.49, batch loss = 0.22 (49.1 examples/sec; 0.163 sec/batch; 14h:45m:22s remains)
INFO - root - 2017-12-16 08:06:27.363809: step 6280, loss = 0.55, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 14h:58m:30s remains)
INFO - root - 2017-12-16 08:06:29.029032: step 6290, loss = 0.55, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:50m:59s remains)
INFO - root - 2017-12-16 08:06:30.675527: step 6300, loss = 0.59, batch loss = 0.32 (50.3 examples/sec; 0.159 sec/batch; 14h:25m:14s remains)
2017-12-16 08:06:31.189814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5627892 -1.4060028 -1.3051188 -1.2243724 -1.1776576 -1.1750896 -1.1639073 -1.1349483 -1.0956411 -1.0513625 -0.99175256 -0.92565012 -0.87750423 -0.83313787 -0.78434587][-2.2566011 -2.1707044 -2.10807 -2.0377326 -1.9865166 -1.939503 -1.8227441 -1.6614063 -1.5053869 -1.3721576 -1.2505112 -1.1506548 -1.0728385 -0.9910171 -0.90271008][-2.5700114 -2.6975298 -2.7809796 -2.8365893 -2.9280477 -2.9487429 -2.7765894 -2.5200057 -2.3022623 -2.1291304 -1.9835463 -1.8905518 -1.8093166 -1.6848537 -1.5367351][-2.0403895 -2.4616544 -2.7624362 -3.039602 -3.3945723 -3.5475912 -3.3583107 -3.1072512 -2.9815111 -2.9081533 -2.8297718 -2.8188457 -2.7808878 -2.6290355 -2.4190311][-0.61373532 -1.3242133 -1.8413658 -2.313086 -2.8872106 -3.128118 -2.9027565 -2.7127054 -2.7966995 -2.9277015 -2.9942384 -3.1648602 -3.3132596 -3.2412581 -3.043386][0.8640039 0.051054597 -0.51915479 -0.99304 -1.5270951 -1.6427653 -1.2613586 -1.130022 -1.4616635 -1.8342872 -2.0672112 -2.461642 -2.8669593 -2.963316 -2.8652945][1.7262897 1.0286667 0.5663619 0.25007606 -0.048726439 0.14015961 0.78838181 0.92744637 0.33513951 -0.1954596 -0.48008311 -0.98056996 -1.6238651 -1.9150438 -1.9418916][1.4828827 1.1062059 0.94629383 0.94061089 0.93303418 1.4278853 2.3283803 2.368273 1.5451386 1.0964634 0.97722507 0.49100542 -0.2480408 -0.66225588 -0.79559731][0.48516107 0.42089105 0.5639379 0.74338078 0.89412975 1.4746845 2.2831256 2.2280514 1.5922532 1.4130297 1.4759746 1.1342075 0.54412174 0.16977096 0.019768715][-0.3737489 -0.36835504 -0.2451055 -0.17939115 -0.13796496 0.27581644 0.77190614 0.71137977 0.4147079 0.46310639 0.61245751 0.48183775 0.1983521 0.0072331429 -0.05766058][-0.83314395 -1.0680919 -1.2700572 -1.4507548 -1.6022948 -1.4750396 -1.2743399 -1.3050034 -1.3920012 -1.2341435 -1.0644712 -1.0624105 -1.0898033 -1.0743639 -1.0260623][-0.74990535 -1.3371484 -2.0139596 -2.4601793 -2.7369404 -2.854866 -2.8753576 -2.9095931 -2.8708825 -2.7065187 -2.587024 -2.5682869 -2.5479906 -2.481874 -2.4160216][-0.22769487 -1.0952306 -2.0875971 -2.745656 -3.0747788 -3.3098774 -3.4364181 -3.4422779 -3.3355279 -3.2061305 -3.1756554 -3.18211 -3.1906497 -3.2042127 -3.2255201][0.54820204 -0.41377842 -1.5135863 -2.2694514 -2.6026421 -2.8277183 -2.9510145 -2.9240611 -2.8127346 -2.7340748 -2.7365198 -2.7724731 -2.818599 -2.8895354 -2.9668505][1.1247227 0.22119236 -0.81535506 -1.5064706 -1.7788904 -1.9095957 -1.9723321 -1.922345 -1.8242478 -1.7769076 -1.7756612 -1.7763516 -1.7674012 -1.7979577 -1.8949264]]...]
INFO - root - 2017-12-16 08:06:32.824125: step 6310, loss = 0.67, batch loss = 0.40 (49.1 examples/sec; 0.163 sec/batch; 14h:46m:35s remains)
INFO - root - 2017-12-16 08:06:34.495333: step 6320, loss = 0.77, batch loss = 0.50 (47.4 examples/sec; 0.169 sec/batch; 15h:17m:19s remains)
INFO - root - 2017-12-16 08:06:36.169612: step 6330, loss = 0.51, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 14h:50m:06s remains)
INFO - root - 2017-12-16 08:06:37.829777: step 6340, loss = 0.60, batch loss = 0.33 (46.4 examples/sec; 0.173 sec/batch; 15h:37m:43s remains)
INFO - root - 2017-12-16 08:06:39.528816: step 6350, loss = 0.64, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 15h:08m:38s remains)
INFO - root - 2017-12-16 08:06:41.224743: step 6360, loss = 0.64, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 15h:08m:45s remains)
INFO - root - 2017-12-16 08:06:42.899949: step 6370, loss = 0.57, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 14h:54m:32s remains)
INFO - root - 2017-12-16 08:06:44.564384: step 6380, loss = 0.59, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 15h:11m:06s remains)
INFO - root - 2017-12-16 08:06:46.266526: step 6390, loss = 0.67, batch loss = 0.40 (47.5 examples/sec; 0.168 sec/batch; 15h:15m:37s remains)
INFO - root - 2017-12-16 08:06:47.962847: step 6400, loss = 0.62, batch loss = 0.35 (47.9 examples/sec; 0.167 sec/batch; 15h:08m:19s remains)
2017-12-16 08:06:48.410637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.358922 -2.5737407 -2.7426386 -2.7415032 -2.5311091 -2.3058784 -2.439985 -2.8556986 -3.3831818 -3.8715444 -4.2789121 -4.5849352 -4.6921754 -4.5144415 -4.2290754][-2.6735945 -2.8627539 -2.9946084 -2.9494996 -2.6582417 -2.3896341 -2.5143535 -2.9842343 -3.5128314 -4.0018816 -4.5630507 -5.0750246 -5.2632666 -5.021102 -4.7132273][-2.9728339 -3.1124344 -3.0695562 -2.8346684 -2.4419615 -2.0254612 -1.987841 -2.3228378 -2.852001 -3.4100819 -4.0700254 -4.72607 -5.0600224 -4.8884296 -4.49897][-3.1421816 -3.1924472 -2.9266052 -2.3983092 -1.6943855 -0.96254927 -0.6405046 -0.91884434 -1.4452708 -2.1368809 -2.9945693 -3.8396902 -4.316721 -4.294323 -3.882169][-3.237838 -3.2166882 -2.7470665 -1.843441 -0.76312518 0.34043002 1.0631416 0.92489696 0.21934557 -0.72383296 -1.718084 -2.7705438 -3.4434319 -3.6229057 -3.33187][-3.2147532 -3.1357784 -2.4828 -1.3405015 0.15429235 1.8045022 3.0356517 3.0460811 2.1130409 0.73355365 -0.6259253 -1.9108495 -2.772377 -3.0650749 -2.8547029][-3.1180797 -2.97582 -2.2290757 -0.93819642 0.84520888 3.0767412 5.1816797 5.5996571 4.1007991 2.0060668 0.18140483 -1.3236079 -2.3231502 -2.6415176 -2.4562821][-2.9541242 -2.8009183 -2.0270431 -0.66737318 1.2397802 3.7837491 6.7045765 7.613184 5.1151972 2.5442562 0.514956 -1.0349286 -2.0779173 -2.381242 -2.1705773][-2.8608902 -2.7487712 -2.0720417 -0.86827981 0.78551149 2.8910365 4.9608321 5.7017379 4.1998162 2.2301402 0.51807213 -0.85152566 -1.8820468 -2.2413573 -2.0313234][-2.8581655 -2.8122373 -2.2759562 -1.3182123 -0.11737108 1.2840078 2.6088538 3.2257051 2.6933651 1.7464662 0.64156461 -0.49604917 -1.5455029 -2.0434787 -2.0177271][-2.9050691 -2.9563036 -2.6174183 -1.9541339 -1.2070317 -0.36500943 0.50050616 1.0274985 1.1299856 1.007205 0.60476518 -0.26521564 -1.3482864 -1.9848884 -2.1513684][-2.9123032 -3.0506823 -2.8916488 -2.5354576 -2.1136689 -1.6789911 -1.253727 -0.85572481 -0.44685864 -0.0244174 0.10333204 -0.38012445 -1.3186151 -2.1129789 -2.4670422][-2.8056915 -2.9971862 -2.9952559 -2.860106 -2.6544514 -2.4742994 -2.3317809 -2.0870562 -1.6514823 -1.0559375 -0.68891597 -0.89208269 -1.5759317 -2.3589108 -2.866817][-2.5663571 -2.7754946 -2.8340192 -2.7833872 -2.7167697 -2.693327 -2.6885645 -2.591481 -2.2889872 -1.8533323 -1.5456876 -1.5849502 -1.9782525 -2.5472057 -3.0020823][-2.2648921 -2.4224699 -2.4986155 -2.4710708 -2.4623594 -2.4965386 -2.5791688 -2.5830593 -2.4891167 -2.2699175 -2.0899394 -2.0422032 -2.2274795 -2.5750961 -2.8945913]]...]
INFO - root - 2017-12-16 08:06:50.096992: step 6410, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.169 sec/batch; 15h:15m:48s remains)
INFO - root - 2017-12-16 08:06:51.781900: step 6420, loss = 0.65, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 15h:06m:07s remains)
INFO - root - 2017-12-16 08:06:53.464371: step 6430, loss = 0.63, batch loss = 0.36 (47.6 examples/sec; 0.168 sec/batch; 15h:12m:55s remains)
INFO - root - 2017-12-16 08:06:55.109762: step 6440, loss = 0.60, batch loss = 0.33 (46.6 examples/sec; 0.172 sec/batch; 15h:32m:21s remains)
INFO - root - 2017-12-16 08:06:56.781587: step 6450, loss = 0.53, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 15h:05m:21s remains)
INFO - root - 2017-12-16 08:06:58.447731: step 6460, loss = 0.56, batch loss = 0.29 (49.5 examples/sec; 0.162 sec/batch; 14h:37m:57s remains)
INFO - root - 2017-12-16 08:07:00.130613: step 6470, loss = 0.68, batch loss = 0.42 (46.5 examples/sec; 0.172 sec/batch; 15h:34m:10s remains)
INFO - root - 2017-12-16 08:07:01.807293: step 6480, loss = 0.78, batch loss = 0.51 (49.0 examples/sec; 0.163 sec/batch; 14h:47m:07s remains)
INFO - root - 2017-12-16 08:07:03.472228: step 6490, loss = 0.55, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 15h:12m:05s remains)
INFO - root - 2017-12-16 08:07:05.152350: step 6500, loss = 0.58, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 15h:22m:33s remains)
2017-12-16 08:07:05.663494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3016508 -2.2796471 -2.1967447 -2.1562345 -2.2062552 -2.31736 -2.4204562 -2.4598706 -2.3940508 -2.2419527 -2.0715823 -1.9383124 -1.8638399 -1.8591352 -1.9151351][-2.3509617 -2.2181845 -2.0113657 -1.9367349 -2.1147306 -2.4224589 -2.6806889 -2.7745697 -2.6574173 -2.3674607 -2.0528681 -1.8391745 -1.7443466 -1.738834 -1.8034958][-2.3604872 -2.0379202 -1.6531334 -1.519909 -1.8417774 -2.4122756 -2.8766451 -3.0473218 -2.8709846 -2.4700196 -2.0710306 -1.825588 -1.742138 -1.7589297 -1.8347654][-2.2629192 -1.7133367 -1.0865984 -0.8069272 -1.1822035 -1.9614762 -2.6345763 -2.859206 -2.6362545 -2.1687117 -1.8065419 -1.6549259 -1.6952252 -1.8233962 -1.9709003][-2.0299816 -1.2702739 -0.37300205 0.21144927 -0.011146545 -0.86051679 -1.6171637 -1.8488878 -1.5804324 -1.17187 -0.94285035 -0.98285347 -1.2875863 -1.672796 -2.0094073][-1.7945547 -0.82144773 0.43055594 1.3861269 1.4341882 0.691435 -0.03537643 -0.23413801 0.05002892 0.38803113 0.5143708 0.32895434 -0.26682425 -0.99494988 -1.6876402][-1.6939721 -0.55743837 0.9997617 2.3042293 2.58139 1.937965 1.2424062 1.131977 1.4980749 1.8519512 1.9858233 1.79749 1.0711058 0.029882789 -1.0170268][-1.6615412 -0.46589923 1.1576418 2.5251613 2.8092012 2.1394796 1.5113429 1.5800775 2.0626664 2.4277067 2.6668129 2.6118836 1.894677 0.68898809 -0.550833][-1.6180923 -0.52096927 0.78277266 1.79822 1.9008466 1.3289934 0.86746824 0.99782407 1.4986621 1.884703 2.1684828 2.2053537 1.6479419 0.61417711 -0.51239479][-1.4128228 -0.48213828 0.41290152 0.89370143 0.704075 0.21485865 -0.14574909 -0.040218115 0.37079561 0.76855671 1.0853671 1.243184 0.96384418 0.27564442 -0.52893162][-1.1338091 -0.19737279 0.43765152 0.46442759 -0.01732254 -0.55204475 -0.90918028 -0.91510522 -0.67633712 -0.3504293 -0.045609593 0.17843354 0.11844218 -0.20997858 -0.62936497][-0.96300489 0.056896567 0.60863173 0.37160718 -0.34916306 -0.98585004 -1.3948305 -1.5013697 -1.4272652 -1.2157978 -0.99673343 -0.82937574 -0.78133619 -0.83187342 -0.89138114][-1.1627114 -0.18082285 0.30122626 -0.039763331 -0.83169591 -1.5008028 -1.8500665 -1.9746515 -1.9755901 -1.8976822 -1.7818338 -1.6438856 -1.5144968 -1.3817997 -1.2223673][-1.8054514 -1.0643764 -0.742334 -1.0318198 -1.6564568 -2.1812732 -2.354804 -2.3701515 -2.3613825 -2.329618 -2.2336893 -2.082037 -1.9071126 -1.7235142 -1.5457325][-2.5522637 -2.0860469 -1.846755 -1.9585015 -2.3135345 -2.606281 -2.6267824 -2.5167859 -2.4442065 -2.3926289 -2.3017666 -2.1562481 -2.0018175 -1.8717839 -1.8291701]]...]
INFO - root - 2017-12-16 08:07:07.345421: step 6510, loss = 0.67, batch loss = 0.40 (48.3 examples/sec; 0.166 sec/batch; 14h:59m:41s remains)
INFO - root - 2017-12-16 08:07:09.023160: step 6520, loss = 0.59, batch loss = 0.32 (48.5 examples/sec; 0.165 sec/batch; 14h:55m:19s remains)
INFO - root - 2017-12-16 08:07:10.681424: step 6530, loss = 0.55, batch loss = 0.28 (46.9 examples/sec; 0.171 sec/batch; 15h:27m:25s remains)
INFO - root - 2017-12-16 08:07:12.351714: step 6540, loss = 0.86, batch loss = 0.59 (46.6 examples/sec; 0.172 sec/batch; 15h:32m:02s remains)
INFO - root - 2017-12-16 08:07:14.046117: step 6550, loss = 0.65, batch loss = 0.38 (46.8 examples/sec; 0.171 sec/batch; 15h:28m:38s remains)
INFO - root - 2017-12-16 08:07:15.738675: step 6560, loss = 0.60, batch loss = 0.33 (45.7 examples/sec; 0.175 sec/batch; 15h:51m:33s remains)
INFO - root - 2017-12-16 08:07:17.390734: step 6570, loss = 0.58, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 14h:57m:52s remains)
INFO - root - 2017-12-16 08:07:19.067195: step 6580, loss = 0.60, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 14h:48m:46s remains)
INFO - root - 2017-12-16 08:07:20.716208: step 6590, loss = 0.52, batch loss = 0.25 (49.6 examples/sec; 0.161 sec/batch; 14h:36m:51s remains)
INFO - root - 2017-12-16 08:07:22.394659: step 6600, loss = 0.54, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 15h:05m:13s remains)
2017-12-16 08:07:22.943021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6168537 -2.3746066 -2.0534658 -1.7265006 -1.4902189 -1.4271238 -1.4623157 -1.4827771 -1.4587505 -1.4327573 -1.3951592 -1.3461388 -1.2936467 -1.2680085 -1.3805028][-2.9199002 -2.6410184 -2.2542048 -1.8424051 -1.5373956 -1.4743639 -1.5399654 -1.6014673 -1.6102419 -1.5955632 -1.5654476 -1.521685 -1.490517 -1.4697797 -1.6185431][-3.394372 -3.110167 -2.6495934 -2.1200709 -1.7046242 -1.5820254 -1.6467783 -1.7470125 -1.836213 -1.8898518 -1.9289324 -1.959829 -1.9864904 -2.0008655 -2.1446946][-3.9740953 -3.7371359 -3.1565373 -2.3931038 -1.7753121 -1.5102948 -1.4989895 -1.5978914 -1.7249513 -1.7965685 -1.8449643 -1.9473541 -2.0480092 -2.1141229 -2.2479672][-4.2893147 -4.0078812 -3.2216134 -2.1348615 -1.2089126 -0.67544556 -0.50164461 -0.5492475 -0.72487462 -0.84397495 -0.92154634 -1.1213684 -1.3567829 -1.5210743 -1.7021424][-4.1897631 -3.8126388 -2.7819109 -1.4065282 -0.16833627 0.64542139 0.99806392 1.0120288 0.81649435 0.67673266 0.56287825 0.31447208 -0.038620472 -0.33185971 -0.64505136][-3.7712255 -3.2523561 -2.0668545 -0.55394626 0.86418283 1.8155953 2.2320561 2.2670822 2.0666513 1.8921405 1.7511245 1.4267665 0.98661172 0.57058489 0.073270917][-3.0657759 -2.4640369 -1.2510078 0.24518788 1.5699996 2.4246922 2.7980843 2.7443814 2.5207553 2.3202691 2.0786295 1.7146899 1.3917428 1.0901991 0.51430523][-2.1947687 -1.4783635 -0.29691923 0.9739536 1.9435757 2.48735 2.6933661 2.5732884 2.3353381 2.0924706 1.8149115 1.4838296 1.3575484 1.2979785 0.82538235][-1.7460231 -1.1298366 -0.13362682 0.88602436 1.518325 1.7149502 1.6672052 1.5545572 1.4404157 1.3427695 1.1766881 1.0353001 1.1660694 1.3549534 1.0801035][-1.7325687 -1.2433546 -0.42804706 0.3867861 0.82621586 0.80173171 0.54195297 0.40527046 0.3784579 0.38738811 0.36151063 0.46894515 0.836408 1.1948086 1.0319263][-1.7546809 -1.2692266 -0.52535963 0.19940102 0.58437073 0.46579063 0.083499551 -0.096591592 -0.047049403 0.048402429 0.13056052 0.37454069 0.8602885 1.1962656 1.0235835][-1.8200848 -1.2466216 -0.4698751 0.29674733 0.69344604 0.5558008 0.1430887 -0.044955969 -0.0050541162 0.078714728 0.1828686 0.4217881 0.84020412 1.0752059 0.8254534][-2.2690964 -1.7608639 -1.0655364 -0.33759558 0.087282062 0.010394216 -0.37240493 -0.56241465 -0.54410052 -0.4937582 -0.42593265 -0.23632538 0.084648252 0.22064126 -0.011356592][-2.7754533 -2.4614737 -1.9610622 -1.3883977 -0.98348987 -0.9857797 -1.2661619 -1.4382079 -1.4488187 -1.4385293 -1.4103872 -1.265801 -1.0561597 -0.9675144 -1.0558808]]...]
INFO - root - 2017-12-16 08:07:24.629718: step 6610, loss = 0.61, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 15h:16m:34s remains)
INFO - root - 2017-12-16 08:07:26.317447: step 6620, loss = 0.63, batch loss = 0.36 (47.0 examples/sec; 0.170 sec/batch; 15h:23m:54s remains)
INFO - root - 2017-12-16 08:07:27.991241: step 6630, loss = 0.63, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 15h:03m:13s remains)
INFO - root - 2017-12-16 08:07:29.682706: step 6640, loss = 0.54, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 15h:16m:35s remains)
INFO - root - 2017-12-16 08:07:31.352067: step 6650, loss = 0.55, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 15h:03m:56s remains)
INFO - root - 2017-12-16 08:07:33.017605: step 6660, loss = 0.65, batch loss = 0.38 (46.1 examples/sec; 0.174 sec/batch; 15h:42m:15s remains)
INFO - root - 2017-12-16 08:07:34.708720: step 6670, loss = 0.67, batch loss = 0.40 (47.9 examples/sec; 0.167 sec/batch; 15h:07m:34s remains)
INFO - root - 2017-12-16 08:07:36.385274: step 6680, loss = 0.54, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 14h:53m:43s remains)
INFO - root - 2017-12-16 08:07:38.050513: step 6690, loss = 0.51, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 15h:07m:33s remains)
INFO - root - 2017-12-16 08:07:39.717100: step 6700, loss = 0.64, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 15h:05m:55s remains)
2017-12-16 08:07:40.225467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6180345 -1.5992862 -1.5873157 -1.5767961 -1.5614486 -1.5488315 -1.5445914 -1.5459032 -1.5448657 -1.5410223 -1.5341663 -1.5217512 -1.5023675 -1.4817516 -1.465179][-1.7849369 -1.78004 -1.7811186 -1.770756 -1.7298492 -1.6837034 -1.6618979 -1.664912 -1.6656793 -1.66452 -1.6648887 -1.656472 -1.6316156 -1.5968765 -1.5672981][-1.7224762 -1.7490257 -1.7775317 -1.772611 -1.7102203 -1.6241802 -1.5760558 -1.5769105 -1.5929677 -1.6130087 -1.6376681 -1.6509752 -1.632006 -1.584927 -1.5346006][-1.5636307 -1.6380073 -1.7104126 -1.7234938 -1.6414183 -1.5168843 -1.4555438 -1.4862207 -1.5618088 -1.6329288 -1.7019572 -1.7440643 -1.72311 -1.6508802 -1.5625875][-1.1809418 -1.246166 -1.2903404 -1.2572601 -1.1105198 -0.94105041 -0.89130604 -1.0115316 -1.2107551 -1.3779151 -1.5310668 -1.654896 -1.6832693 -1.6043698 -1.4933356][-0.54596543 -0.53247142 -0.4480722 -0.25085473 0.045026541 0.31277776 0.33917069 0.082670212 -0.28569353 -0.57924879 -0.83861613 -1.0634763 -1.1727302 -1.1475065 -1.063127][0.18758869 0.33953357 0.61889458 1.0272985 1.497695 1.8505135 1.8200817 1.4205041 0.9244616 0.54595971 0.21814704 -0.071848273 -0.26174855 -0.3363508 -0.33689523][0.64147878 0.875788 1.2435417 1.7198567 2.1851025 2.4955192 2.4013643 1.9372375 1.4138541 1.0391896 0.78088474 0.57863379 0.43563867 0.35056043 0.2828002][0.42017126 0.47218657 0.5989635 0.8071928 1.0330307 1.1634161 1.0976393 0.79379058 0.44057536 0.22378349 0.15372419 0.1774292 0.24338007 0.27633882 0.22387385][-0.15435886 -0.28960693 -0.35037029 -0.30963743 -0.22236371 -0.18887365 -0.26297295 -0.47265911 -0.68500364 -0.775154 -0.70885205 -0.540022 -0.34376776 -0.22063649 -0.25125587][-0.77559209 -0.99403733 -1.099681 -1.1051024 -1.0947256 -1.1221576 -1.2135086 -1.3535929 -1.4565148 -1.4279196 -1.2601674 -0.98513192 -0.70043004 -0.52749753 -0.54623115][-1.2681832 -1.4195957 -1.4371269 -1.3718076 -1.3573942 -1.3931265 -1.4872851 -1.5763254 -1.6080878 -1.4902308 -1.1926694 -0.8078264 -0.46093976 -0.26473975 -0.28765178][-1.7330729 -1.773456 -1.6520432 -1.4894776 -1.4249713 -1.431711 -1.4687614 -1.460551 -1.4095988 -1.2164338 -0.81516743 -0.34020889 0.055348158 0.27994394 0.24079514][-1.9049797 -1.8560174 -1.6493647 -1.4673078 -1.4329845 -1.4818794 -1.518443 -1.4666996 -1.3455403 -1.0872918 -0.64070487 -0.161322 0.20902205 0.389848 0.30860114][-1.7702774 -1.7225366 -1.5910327 -1.5389519 -1.6581984 -1.8139914 -1.8877757 -1.8445292 -1.6993768 -1.4070094 -0.98784316 -0.59239542 -0.33895504 -0.25435936 -0.34870279]]...]
INFO - root - 2017-12-16 08:07:41.909014: step 6710, loss = 0.76, batch loss = 0.49 (48.3 examples/sec; 0.166 sec/batch; 14h:59m:05s remains)
INFO - root - 2017-12-16 08:07:43.595118: step 6720, loss = 0.65, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 14h:59m:16s remains)
INFO - root - 2017-12-16 08:07:45.265725: step 6730, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 15h:07m:00s remains)
INFO - root - 2017-12-16 08:07:46.915733: step 6740, loss = 0.69, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 15h:10m:15s remains)
INFO - root - 2017-12-16 08:07:48.599091: step 6750, loss = 0.64, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 15h:09m:43s remains)
INFO - root - 2017-12-16 08:07:50.263986: step 6760, loss = 0.61, batch loss = 0.34 (47.1 examples/sec; 0.170 sec/batch; 15h:22m:03s remains)
INFO - root - 2017-12-16 08:07:51.955091: step 6770, loss = 0.52, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 15h:08m:09s remains)
INFO - root - 2017-12-16 08:07:53.626018: step 6780, loss = 0.55, batch loss = 0.28 (45.6 examples/sec; 0.175 sec/batch; 15h:51m:57s remains)
INFO - root - 2017-12-16 08:07:55.289028: step 6790, loss = 0.60, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 15h:04m:52s remains)
INFO - root - 2017-12-16 08:07:56.974274: step 6800, loss = 0.57, batch loss = 0.30 (47.5 examples/sec; 0.169 sec/batch; 15h:14m:52s remains)
2017-12-16 08:07:57.447645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.35793483 0.22360146 0.056043267 -0.89092159 -1.8104669 -2.1084518 -2.2019179 -2.3167052 -2.3696392 -2.4925117 -3.0207291 -3.5405092 -3.7224021 -3.4240615 -2.7132983][0.43915117 1.0203842 0.69222629 -0.56547379 -1.7258114 -2.0656657 -1.957375 -1.7360091 -1.4986577 -1.6502433 -2.3579137 -3.0335479 -3.4182703 -3.3102407 -2.7105844][0.79494464 1.0941912 0.65551841 -0.51257849 -1.4944056 -1.6514881 -1.2981679 -0.8784014 -0.46525776 -0.63714182 -1.5132171 -2.3751454 -2.9674611 -3.1424608 -2.7076046][0.7642411 0.69270337 0.26226962 -0.65615892 -1.1920576 -0.94672394 -0.26930344 0.34436166 0.77166069 0.40676916 -0.68226194 -1.7003769 -2.510576 -2.9847651 -2.7335742][0.39647448 -0.0725162 -0.44519722 -0.99998146 -0.98626506 -0.32177758 0.74865758 1.6189398 1.8999914 1.1091727 -0.26913691 -1.3360758 -2.2097838 -2.8531125 -2.7082062][-0.32895386 -0.98498714 -1.2376056 -1.470654 -1.0585271 0.015436053 1.4833447 2.6342072 2.7149611 1.4345502 -0.069990158 -1.0769322 -1.9152304 -2.6450644 -2.6327932][-1.3645501 -2.0123191 -2.105787 -2.0567951 -1.2811061 0.24723876 2.1120334 3.5753894 3.3636413 1.6171342 -0.055099845 -1.0336332 -1.7836895 -2.5639756 -2.6216078][-2.4831269 -3.1095932 -3.1580238 -2.8573117 -1.6978081 0.2377063 2.4500208 4.1562548 3.5672441 1.4309958 -0.40641332 -1.46226 -2.0793319 -2.6726398 -2.672811][-3.2347555 -3.8717232 -3.928057 -3.5134392 -2.1048779 0.11769021 2.4648643 4.0615692 3.1006713 0.67359579 -1.2690008 -2.3442986 -2.7248449 -2.9450312 -2.6814611][-3.4354987 -4.1184015 -4.3130412 -3.8745079 -2.3371308 -0.028191328 2.3847451 3.8564777 2.6834092 0.038824916 -2.0310214 -3.0614629 -3.2515802 -3.1495795 -2.5665023][-3.0991511 -3.8926146 -4.3583655 -4.0330577 -2.5945756 -0.23191154 2.2396808 3.5929108 2.2136922 -0.69314933 -2.8281016 -3.6688385 -3.5576005 -3.1873446 -2.4472005][-2.7049925 -3.4796009 -4.1252513 -4.0472407 -2.8439016 -0.71988642 1.5636297 2.7105517 1.1867024 -1.6458904 -3.5770187 -4.1482348 -3.8206487 -3.2383614 -2.3811722][-2.4376926 -3.0880833 -3.7429829 -3.8870575 -3.1448765 -1.6170326 0.18170774 1.0625716 -0.25162792 -2.6361966 -4.226264 -4.5665383 -4.0874138 -3.312263 -2.33756][-2.1552296 -2.6794579 -3.2620375 -3.5474589 -3.2606716 -2.4114351 -1.2639825 -0.60845697 -1.4613322 -3.2253103 -4.4716954 -4.6889572 -4.1441178 -3.2408381 -2.2185144][-1.8167944 -2.2061026 -2.6464911 -2.9137731 -2.9237683 -2.6081674 -2.0132403 -1.549616 -2.0850983 -3.3303998 -4.2920995 -4.43498 -3.9117575 -3.0040584 -2.0245578]]...]
INFO - root - 2017-12-16 08:07:59.150357: step 6810, loss = 0.52, batch loss = 0.25 (49.3 examples/sec; 0.162 sec/batch; 14h:41m:33s remains)
INFO - root - 2017-12-16 08:08:00.848081: step 6820, loss = 0.50, batch loss = 0.23 (47.5 examples/sec; 0.169 sec/batch; 15h:14m:56s remains)
INFO - root - 2017-12-16 08:08:02.508507: step 6830, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 14h:45m:32s remains)
INFO - root - 2017-12-16 08:08:04.148165: step 6840, loss = 0.62, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 14h:50m:22s remains)
INFO - root - 2017-12-16 08:08:05.818041: step 6850, loss = 0.62, batch loss = 0.35 (48.6 examples/sec; 0.165 sec/batch; 14h:53m:25s remains)
INFO - root - 2017-12-16 08:08:07.481658: step 6860, loss = 0.61, batch loss = 0.34 (50.1 examples/sec; 0.160 sec/batch; 14h:27m:18s remains)
INFO - root - 2017-12-16 08:08:09.181493: step 6870, loss = 0.54, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 14h:48m:12s remains)
INFO - root - 2017-12-16 08:08:10.847428: step 6880, loss = 0.61, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 15h:05m:10s remains)
INFO - root - 2017-12-16 08:08:12.529925: step 6890, loss = 0.57, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 15h:03m:54s remains)
INFO - root - 2017-12-16 08:08:14.219116: step 6900, loss = 0.60, batch loss = 0.33 (45.9 examples/sec; 0.174 sec/batch; 15h:46m:36s remains)
2017-12-16 08:08:14.664407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6580843 -1.3254416 -1.3306456 -1.3850263 -1.3363254 -1.253788 -1.1611803 -1.0174272 -0.83388865 -0.66161287 -0.52119958 -0.474442 -0.53199077 -0.64342928 -0.70356894][-1.1350564 -0.86556411 -0.99825323 -1.1448991 -1.1462291 -1.0966377 -1.034126 -0.91895211 -0.789899 -0.65512192 -0.54640365 -0.55791104 -0.69436359 -0.85558069 -0.95398629][-0.63184237 -0.37365746 -0.58930254 -0.80255938 -0.83039761 -0.82509506 -0.83019662 -0.79295731 -0.77324927 -0.79657972 -0.85783696 -1.0112947 -1.2083921 -1.4120834 -1.5361545][0.094819784 0.33373284 0.09892416 -0.18609416 -0.27752769 -0.32665515 -0.39445853 -0.45705676 -0.56222403 -0.72335958 -0.9177953 -1.1848263 -1.4630483 -1.694653 -1.828686][0.94730473 1.3324301 1.2254491 0.99066496 0.93745756 0.920753 0.87252092 0.74735308 0.53582382 0.26466393 0.012418509 -0.29713869 -0.67420256 -0.98677957 -1.1525667][1.3252075 1.995399 2.2239656 2.2696843 2.4231281 2.5326452 2.5310831 2.3696299 2.1050334 1.7653103 1.4818408 1.2162294 0.84070635 0.44971347 0.18993258][1.0393827 1.9792078 2.6683884 3.2029972 3.713635 4.0458784 4.1471348 4.0114875 3.585598 3.1240401 2.8362598 2.5990419 2.1879501 1.6648786 1.2862153][0.12156558 1.1279783 2.0910125 2.9932871 3.9381361 4.6307826 5.0052123 4.997963 4.3356614 3.6329923 3.1949892 2.8197474 2.2756262 1.5919862 1.0434101][-0.97865796 -0.19892681 0.63237405 1.592643 2.5546241 3.2239852 3.5202637 3.3607497 2.7858415 2.1516008 1.7289269 1.3736827 0.83840275 0.19880366 -0.28453612][-2.2138126 -1.7436081 -1.1646094 -0.41433072 0.33383584 0.83119726 1.0149267 0.88420987 0.492774 -0.029669523 -0.40344346 -0.68589509 -1.0564573 -1.4527116 -1.7281799][-3.221251 -2.9875507 -2.7017009 -2.2525783 -1.7694117 -1.4643033 -1.3486748 -1.4049716 -1.6030627 -1.9423014 -2.2109575 -2.4038086 -2.5834551 -2.6525438 -2.6266422][-3.6961589 -3.6278348 -3.55521 -3.374022 -3.1574507 -3.0094004 -2.9249156 -2.8910866 -2.9381638 -3.0686488 -3.19561 -3.2749331 -3.2559094 -3.0700684 -2.7861736][-3.1797166 -3.1643548 -3.24163 -3.2576194 -3.1884112 -3.0933094 -2.9807391 -2.861598 -2.792377 -2.7823315 -2.7706919 -2.7282681 -2.5955324 -2.3292117 -1.9868088][-2.1266572 -1.9786587 -2.0678098 -2.1729994 -2.1560152 -2.0815849 -1.9786726 -1.8337922 -1.7155267 -1.6314721 -1.5568743 -1.4619129 -1.3102498 -1.0978353 -0.83184576][-1.0295675 -0.675732 -0.696797 -0.81181657 -0.81616724 -0.75277138 -0.64989579 -0.51548755 -0.3996793 -0.30531526 -0.2192564 -0.13857865 -0.078120947 -0.017569661 0.070019007]]...]
INFO - root - 2017-12-16 08:08:16.387293: step 6910, loss = 0.55, batch loss = 0.28 (45.4 examples/sec; 0.176 sec/batch; 15h:56m:07s remains)
INFO - root - 2017-12-16 08:08:18.090386: step 6920, loss = 0.60, batch loss = 0.33 (49.1 examples/sec; 0.163 sec/batch; 14h:44m:37s remains)
INFO - root - 2017-12-16 08:08:19.729835: step 6930, loss = 0.59, batch loss = 0.32 (49.5 examples/sec; 0.162 sec/batch; 14h:36m:50s remains)
INFO - root - 2017-12-16 08:08:21.398751: step 6940, loss = 0.67, batch loss = 0.40 (49.8 examples/sec; 0.161 sec/batch; 14h:32m:11s remains)
INFO - root - 2017-12-16 08:08:23.049109: step 6950, loss = 0.74, batch loss = 0.47 (49.6 examples/sec; 0.161 sec/batch; 14h:35m:33s remains)
INFO - root - 2017-12-16 08:08:24.720956: step 6960, loss = 0.55, batch loss = 0.29 (49.4 examples/sec; 0.162 sec/batch; 14h:39m:02s remains)
INFO - root - 2017-12-16 08:08:26.368080: step 6970, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 14h:58m:49s remains)
INFO - root - 2017-12-16 08:08:28.074816: step 6980, loss = 0.58, batch loss = 0.31 (46.8 examples/sec; 0.171 sec/batch; 15h:26m:54s remains)
INFO - root - 2017-12-16 08:08:29.760244: step 6990, loss = 0.62, batch loss = 0.35 (47.2 examples/sec; 0.170 sec/batch; 15h:19m:52s remains)
INFO - root - 2017-12-16 08:08:31.423973: step 7000, loss = 0.58, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:55m:18s remains)
2017-12-16 08:08:31.934551: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.4334433 1.0474741 0.8466382 0.84138489 0.7675004 0.59476709 0.34817982 -0.14869702 -0.910164 -1.6014017 -2.1454339 -2.5889046 -2.9451234 -3.1884131 -3.2574339][1.0606036 0.79502583 0.66200042 0.67601848 0.51563382 0.10567188 -0.46621442 -1.1997365 -2.0320077 -2.6723733 -3.0899954 -3.3406718 -3.445931 -3.4291785 -3.2845893][-0.1478951 -0.26575565 -0.28804147 -0.27959812 -0.50588238 -1.0563303 -1.7451519 -2.4214823 -2.9486396 -3.2176485 -3.2760954 -3.2101784 -3.110971 -2.9728804 -2.7705259][-1.7121367 -1.7360051 -1.6951427 -1.7305939 -2.001734 -2.4686954 -2.9446526 -3.2020104 -3.1494551 -2.8244026 -2.3926716 -2.0767217 -1.9789598 -1.9552562 -1.8602767][-2.7906728 -2.7656236 -2.71462 -2.7542021 -2.9206784 -3.098402 -3.1063991 -2.7708836 -2.0974481 -1.2691693 -0.58172572 -0.29387665 -0.44623744 -0.72213018 -0.83435357][-2.8902538 -2.8288379 -2.7611258 -2.7354603 -2.7055645 -2.4874911 -1.9152837 -1.0507755 -0.10794199 0.733315 1.203227 1.0839818 0.52196574 -0.0082001686 -0.18541443][-1.9890841 -1.8702384 -1.7935737 -1.7083862 -1.4780169 -0.87985134 0.19978714 1.3564806 2.0689983 2.2757449 2.0609617 1.4131358 0.5767622 -0.016675472 -0.14360595][-0.64586568 -0.47313714 -0.38744605 -0.3077451 -0.022762537 0.75161338 2.0246468 3.0587659 3.0895767 2.3787837 1.4634626 0.56464624 -0.18153417 -0.5774889 -0.55608749][0.1659441 0.35780239 0.46608686 0.48951268 0.678663 1.2482691 2.1593027 2.7208157 2.2764249 1.1355147 -0.092840314 -0.96277261 -1.3504069 -1.3442124 -1.1053152][0.062054157 0.082685947 0.089489222 0.055496454 0.16292024 0.46577382 0.88865685 1.0856245 0.65451622 -0.39840758 -1.5276635 -2.2389507 -2.3175993 -1.9941764 -1.5261022][-0.5875814 -0.82949507 -1.0057548 -1.0515149 -0.854516 -0.51684797 -0.23885572 -0.13538921 -0.42201149 -1.2027161 -2.1353376 -2.7401447 -2.7191753 -2.2427981 -1.606764][-1.2117846 -1.6814206 -1.9404044 -1.8313551 -1.3608522 -0.76178813 -0.42047632 -0.3582058 -0.58633649 -1.1567476 -1.8910674 -2.4460549 -2.4967074 -2.0527229 -1.3674848][-1.4848264 -2.0039971 -2.1296096 -1.76111 -0.97766507 -0.073276639 0.34793258 0.31376123 0.028397083 -0.39798784 -1.0140407 -1.6260002 -1.8587848 -1.5680132 -0.98890376][-1.4215736 -1.7919954 -1.7005645 -1.1415523 -0.2267679 0.80486417 1.2443564 1.0605793 0.67655611 0.35026741 -0.077670932 -0.66339684 -1.0304005 -0.96131361 -0.63367248][-1.2169292 -1.3523316 -1.1258752 -0.59688938 0.18388724 1.0461309 1.4153707 1.1652353 0.75569582 0.56312633 0.43293643 0.096302748 -0.2797364 -0.47610569 -0.44553733]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:08:33.665735: step 7010, loss = 0.59, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 15h:22m:46s remains)
INFO - root - 2017-12-16 08:08:35.377474: step 7020, loss = 0.66, batch loss = 0.40 (48.3 examples/sec; 0.166 sec/batch; 14h:58m:49s remains)
INFO - root - 2017-12-16 08:08:37.086599: step 7030, loss = 0.66, batch loss = 0.39 (47.3 examples/sec; 0.169 sec/batch; 15h:17m:45s remains)
INFO - root - 2017-12-16 08:08:38.803205: step 7040, loss = 0.56, batch loss = 0.29 (47.0 examples/sec; 0.170 sec/batch; 15h:23m:48s remains)
INFO - root - 2017-12-16 08:08:40.469276: step 7050, loss = 0.54, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 15h:38m:50s remains)
INFO - root - 2017-12-16 08:08:42.136725: step 7060, loss = 0.64, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 15h:03m:10s remains)
INFO - root - 2017-12-16 08:08:43.786919: step 7070, loss = 0.56, batch loss = 0.30 (47.1 examples/sec; 0.170 sec/batch; 15h:20m:29s remains)
INFO - root - 2017-12-16 08:08:45.464017: step 7080, loss = 0.62, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 14h:54m:06s remains)
INFO - root - 2017-12-16 08:08:47.140908: step 7090, loss = 0.68, batch loss = 0.41 (48.6 examples/sec; 0.165 sec/batch; 14h:53m:33s remains)
INFO - root - 2017-12-16 08:08:48.828378: step 7100, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.165 sec/batch; 14h:57m:24s remains)
2017-12-16 08:08:49.305328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.384733 -1.4037821 -1.4749928 -1.5716392 -1.6540105 -1.7166009 -1.7904024 -1.8707792 -1.9310144 -1.9330988 -1.8519187 -1.7116505 -1.5936611 -1.5317844 -1.5014198][-1.2945108 -1.3181255 -1.3838573 -1.4539971 -1.4872468 -1.4936814 -1.5531068 -1.7101802 -1.8938485 -1.9928465 -1.9368354 -1.7379435 -1.5495279 -1.4632182 -1.4545671][-1.2511826 -1.2748804 -1.3432791 -1.4000796 -1.3890276 -1.3099495 -1.2824738 -1.4267356 -1.6927722 -1.8847861 -1.8742845 -1.6721588 -1.4463267 -1.3478923 -1.3717692][-1.279392 -1.3219953 -1.4047251 -1.4474692 -1.349226 -1.0975807 -0.87406671 -0.88270521 -1.1152325 -1.3952708 -1.4850273 -1.4004347 -1.2903502 -1.3026315 -1.3960285][-1.3741062 -1.4696481 -1.5835787 -1.5961951 -1.3421141 -0.79654634 -0.19967675 0.089276552 -0.033926368 -0.381822 -0.6617831 -0.84955 -1.0549418 -1.329316 -1.5582478][-1.5642625 -1.7221493 -1.8742291 -1.8651295 -1.4700876 -0.57240665 0.53075624 1.2454572 1.3717425 1.0594425 0.57569766 0.0051515102 -0.6343354 -1.2437453 -1.6466514][-1.7973372 -1.9948175 -2.1865945 -2.2024813 -1.7417949 -0.60835683 0.94084048 2.2088344 2.7395895 2.5242841 1.8406506 0.8815372 -0.15814888 -1.0667217 -1.6447222][-1.8966769 -2.1139829 -2.3550897 -2.4599345 -2.0935905 -0.94452596 0.87171721 2.6680281 3.6265805 3.4878647 2.5736096 1.3143151 0.014653444 -1.0389583 -1.686054][-1.7259109 -1.9851433 -2.3009744 -2.5405841 -2.3649454 -1.4191254 0.30387807 2.2240164 3.4117692 3.3430912 2.3656743 1.0392008 -0.28035963 -1.3040974 -1.8683754][-1.3316368 -1.6481955 -2.0510712 -2.4256191 -2.4995942 -1.9287471 -0.64234412 0.91776729 1.9630079 1.99841 1.2145581 0.13673162 -0.92653286 -1.7526788 -2.1693065][-0.95141673 -1.2993257 -1.750421 -2.2173584 -2.5063705 -2.3540072 -1.664691 -0.69086707 0.056075096 0.21114063 -0.23509932 -0.91126239 -1.6041908 -2.1271672 -2.3646014][-0.67559195 -0.99836135 -1.4447342 -1.9386649 -2.36662 -2.5444956 -2.3685498 -1.9299369 -1.4993862 -1.319787 -1.4507289 -1.7848252 -2.1570342 -2.4312215 -2.5182207][-0.55143666 -0.82430828 -1.2368078 -1.7234901 -2.2008989 -2.5419655 -2.6611874 -2.5531845 -2.3483927 -2.1947212 -2.1750031 -2.2837129 -2.4507465 -2.561358 -2.572443][-0.65238595 -0.89559758 -1.26293 -1.6922474 -2.1172125 -2.4560456 -2.6469724 -2.6634896 -2.5763412 -2.4552794 -2.3630979 -2.3344011 -2.3617578 -2.3876331 -2.3793883][-0.93564987 -1.1274027 -1.4031067 -1.7091632 -1.9957129 -2.2181509 -2.3477855 -2.37723 -2.337081 -2.2524214 -2.1660371 -2.0927529 -2.0496902 -2.0314982 -2.0235076]]...]
INFO - root - 2017-12-16 08:08:50.990669: step 7110, loss = 0.66, batch loss = 0.39 (49.2 examples/sec; 0.163 sec/batch; 14h:42m:17s remains)
INFO - root - 2017-12-16 08:08:52.634168: step 7120, loss = 0.68, batch loss = 0.41 (51.0 examples/sec; 0.157 sec/batch; 14h:10m:36s remains)
INFO - root - 2017-12-16 08:08:54.350863: step 7130, loss = 0.55, batch loss = 0.28 (44.8 examples/sec; 0.179 sec/batch; 16h:08m:26s remains)
INFO - root - 2017-12-16 08:08:56.041039: step 7140, loss = 0.50, batch loss = 0.23 (49.4 examples/sec; 0.162 sec/batch; 14h:38m:42s remains)
INFO - root - 2017-12-16 08:08:57.703537: step 7150, loss = 0.64, batch loss = 0.38 (47.2 examples/sec; 0.169 sec/batch; 15h:18m:52s remains)
INFO - root - 2017-12-16 08:08:59.381094: step 7160, loss = 0.53, batch loss = 0.26 (46.5 examples/sec; 0.172 sec/batch; 15h:31m:55s remains)
INFO - root - 2017-12-16 08:09:01.050513: step 7170, loss = 0.51, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 15h:01m:48s remains)
INFO - root - 2017-12-16 08:09:02.682907: step 7180, loss = 0.66, batch loss = 0.39 (48.5 examples/sec; 0.165 sec/batch; 14h:55m:03s remains)
INFO - root - 2017-12-16 08:09:04.346734: step 7190, loss = 0.58, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 15h:11m:15s remains)
INFO - root - 2017-12-16 08:09:06.029965: step 7200, loss = 0.52, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 15h:14m:58s remains)
2017-12-16 08:09:06.518606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.81468248 -1.0125374 -1.2249556 -1.3640008 -1.4087631 -1.3662244 -1.2715144 -1.2027401 -1.2169143 -1.2047658 -1.1347678 -1.0111036 -0.8562603 -0.69252884 -0.57000995][-0.96275437 -1.2302465 -1.4507754 -1.4721401 -1.4028044 -1.3516755 -1.3326987 -1.3268738 -1.3708184 -1.3884909 -1.4046239 -1.3563156 -1.2323864 -1.1391716 -1.1120101][-0.97635591 -1.2615283 -1.3778229 -1.2032597 -0.99763978 -1.0108788 -1.142809 -1.2454221 -1.2677844 -1.2994223 -1.3814201 -1.3698713 -1.3004147 -1.2944512 -1.3379774][-0.53135908 -0.80054271 -0.77321935 -0.43070495 -0.19769251 -0.32954335 -0.68117082 -0.94379878 -0.96919417 -0.96310413 -1.0538611 -1.1098912 -1.092756 -1.0864782 -1.1178831][0.33708096 0.14723253 0.3185246 0.72785568 0.89554167 0.55293322 -0.068457127 -0.47339272 -0.52447546 -0.510131 -0.62229335 -0.76238 -0.85531259 -0.81429994 -0.74525118][1.2221246 1.1930864 1.523705 1.9998374 2.0702143 1.4168205 0.5596509 0.088661671 0.039690971 0.048376083 -0.1602875 -0.4451673 -0.64925385 -0.57886612 -0.36201525][1.5479531 1.7182922 2.1966944 2.7879386 2.768919 1.9447632 1.0654795 0.65417147 0.66031957 0.65501022 0.33693647 -0.16129017 -0.49962211 -0.41637647 -0.11997986][0.971719 1.2474654 1.7199714 2.2232003 2.2539988 1.7102697 1.1291807 0.88643146 0.99533558 1.0205021 0.642261 -0.024609327 -0.44751656 -0.3878063 -0.1054132][-0.20425594 0.075806856 0.52766228 1.0146544 1.1313152 0.94702744 0.77431846 0.77178335 0.95025373 1.0085793 0.66761446 -0.023456573 -0.4813745 -0.502808 -0.35926211][-1.3427728 -1.0742273 -0.66582179 -0.19456017 -0.001185894 0.065077782 0.17599082 0.32685471 0.57042146 0.65682006 0.35141969 -0.25083017 -0.66834021 -0.75616693 -0.74677432][-2.0776653 -1.8185946 -1.479197 -1.101222 -0.86736345 -0.67106676 -0.46077967 -0.28860998 -0.07925415 -0.030880213 -0.31332743 -0.75275862 -1.0788043 -1.1615875 -1.2107089][-2.2961698 -2.0666378 -1.8592175 -1.6307955 -1.4221748 -1.2251337 -1.0726906 -0.98541749 -0.88674963 -0.95640075 -1.1559861 -1.4061875 -1.6325269 -1.6900446 -1.7006022][-2.1438005 -1.9770886 -1.8965684 -1.7477551 -1.5916876 -1.4620799 -1.3758432 -1.3941905 -1.4428368 -1.6375487 -1.7827655 -1.8665969 -1.9581435 -1.9818395 -1.9286987][-1.8631573 -1.7800639 -1.7778378 -1.6964132 -1.5967488 -1.5430782 -1.5090146 -1.5469965 -1.6712317 -1.9040908 -2.0141885 -2.0008457 -1.9666328 -1.9166243 -1.7793251][-1.7441491 -1.7038393 -1.7619236 -1.7841678 -1.74431 -1.7538409 -1.7605393 -1.8086281 -1.8941995 -2.0445008 -2.1011653 -2.0251262 -1.8994118 -1.7476466 -1.5475403]]...]
INFO - root - 2017-12-16 08:09:08.217501: step 7210, loss = 0.58, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 14h:58m:03s remains)
INFO - root - 2017-12-16 08:09:09.878089: step 7220, loss = 0.54, batch loss = 0.27 (49.1 examples/sec; 0.163 sec/batch; 14h:44m:05s remains)
INFO - root - 2017-12-16 08:09:11.545578: step 7230, loss = 0.54, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 14h:59m:19s remains)
INFO - root - 2017-12-16 08:09:13.201791: step 7240, loss = 0.71, batch loss = 0.44 (47.6 examples/sec; 0.168 sec/batch; 15h:10m:55s remains)
INFO - root - 2017-12-16 08:09:14.866338: step 7250, loss = 0.54, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 14h:52m:58s remains)
INFO - root - 2017-12-16 08:09:16.552083: step 7260, loss = 0.61, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 14h:51m:57s remains)
INFO - root - 2017-12-16 08:09:18.207022: step 7270, loss = 0.57, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 15h:16m:12s remains)
INFO - root - 2017-12-16 08:09:19.845007: step 7280, loss = 0.57, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 14h:55m:38s remains)
INFO - root - 2017-12-16 08:09:21.504417: step 7290, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 15h:07m:13s remains)
INFO - root - 2017-12-16 08:09:23.187222: step 7300, loss = 0.61, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 15h:07m:30s remains)
2017-12-16 08:09:23.699565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4036481 -1.2646189 -0.98287582 -1.0161743 -1.4222727 -1.6969306 -1.7146633 -1.5586679 -1.3139585 -0.85136414 -0.3574959 -0.45906985 -1.0236654 -1.4498632 -1.6326505][-1.8655934 -1.8574466 -1.7110611 -1.8139123 -2.2594481 -2.56117 -2.643712 -2.5264952 -2.2705727 -1.7953469 -1.310914 -1.3643737 -1.7959561 -2.0758634 -2.1047585][-2.2422843 -2.278013 -2.1816363 -2.3924952 -2.8807092 -3.1366696 -3.2298212 -3.1895566 -2.9698658 -2.5586975 -2.1281693 -2.1060381 -2.3875573 -2.5744696 -2.5583992][-2.4359672 -2.4333043 -2.2970111 -2.5348225 -2.9418736 -2.9936309 -2.9440403 -2.9976721 -2.9127097 -2.6898823 -2.4341364 -2.4632492 -2.6809149 -2.8621149 -2.8706694][-2.5877221 -2.5010462 -2.284692 -2.3891153 -2.435339 -1.9716177 -1.5828468 -1.7587984 -1.9681853 -2.0685873 -2.1639359 -2.4090433 -2.6836808 -2.9616647 -3.05239][-2.8224063 -2.7025166 -2.4182129 -2.1904626 -1.6077511 -0.39005268 0.44152522 0.20252156 -0.38941824 -0.91754746 -1.4414239 -2.0214331 -2.5182586 -2.9351742 -3.1322236][-3.0545053 -2.9876583 -2.655777 -2.0899236 -1.0011071 0.89362931 2.21908 2.0593271 1.2051632 0.28352785 -0.60844886 -1.5090419 -2.2399442 -2.7752812 -3.0728016][-3.1933157 -3.2555184 -2.9304838 -2.2366033 -0.90129721 1.3196549 2.9807377 2.9705038 1.9074204 0.78512573 -0.17015541 -1.0821692 -1.8269715 -2.4151375 -2.7948031][-3.1094632 -3.3661556 -3.1463616 -2.5419321 -1.335916 0.55375075 2.1177793 2.2213502 1.2011223 0.26291919 -0.33080232 -0.91596067 -1.495675 -2.0091658 -2.3986361][-2.7567289 -3.2654517 -3.3108602 -2.9822061 -2.1670549 -0.86052084 0.26673031 0.32725024 -0.44051135 -0.90562761 -0.9127171 -0.96319056 -1.2433708 -1.5990701 -1.9328445][-2.1482875 -2.8784521 -3.2827981 -3.3347039 -2.9799907 -2.2891815 -1.6494364 -1.6409893 -2.0559621 -2.0462306 -1.5041974 -1.1183441 -1.1506257 -1.3625726 -1.6008052][-1.6197305 -2.3611248 -3.0218127 -3.3978376 -3.4398332 -3.2093594 -2.9114027 -2.8891535 -3.0128832 -2.6586874 -1.7586269 -1.1360047 -1.1049807 -1.3218579 -1.4727695][-1.4358101 -1.9580514 -2.6202817 -3.1194987 -3.3415315 -3.3582268 -3.2378316 -3.2152367 -3.1768675 -2.636945 -1.6220461 -0.99420047 -1.0836732 -1.3588187 -1.4964833][-1.6063213 -1.8264409 -2.2822378 -2.6801996 -2.863101 -2.9122002 -2.8566484 -2.8479438 -2.7168283 -2.1868825 -1.3260429 -0.9251411 -1.2065423 -1.5502543 -1.6979902][-1.9276683 -1.8918338 -2.0728562 -2.2444296 -2.2712798 -2.2316704 -2.1855476 -2.1867828 -2.1041532 -1.7370579 -1.1944226 -1.0517781 -1.4388525 -1.8576925 -2.0553832]]...]
INFO - root - 2017-12-16 08:09:25.374420: step 7310, loss = 0.62, batch loss = 0.35 (47.1 examples/sec; 0.170 sec/batch; 15h:19m:43s remains)
INFO - root - 2017-12-16 08:09:27.021653: step 7320, loss = 0.66, batch loss = 0.40 (46.8 examples/sec; 0.171 sec/batch; 15h:26m:46s remains)
INFO - root - 2017-12-16 08:09:28.703408: step 7330, loss = 0.61, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 15h:01m:24s remains)
INFO - root - 2017-12-16 08:09:30.417663: step 7340, loss = 0.65, batch loss = 0.39 (45.8 examples/sec; 0.175 sec/batch; 15h:46m:47s remains)
INFO - root - 2017-12-16 08:09:32.081756: step 7350, loss = 0.72, batch loss = 0.45 (48.8 examples/sec; 0.164 sec/batch; 14h:47m:55s remains)
INFO - root - 2017-12-16 08:09:33.721164: step 7360, loss = 0.55, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 15h:06m:59s remains)
INFO - root - 2017-12-16 08:09:35.384440: step 7370, loss = 0.61, batch loss = 0.34 (47.3 examples/sec; 0.169 sec/batch; 15h:15m:52s remains)
INFO - root - 2017-12-16 08:09:37.093793: step 7380, loss = 0.55, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 15h:26m:01s remains)
INFO - root - 2017-12-16 08:09:38.753785: step 7390, loss = 0.64, batch loss = 0.38 (46.6 examples/sec; 0.172 sec/batch; 15h:29m:26s remains)
INFO - root - 2017-12-16 08:09:40.446903: step 7400, loss = 0.62, batch loss = 0.35 (46.3 examples/sec; 0.173 sec/batch; 15h:35m:31s remains)
2017-12-16 08:09:40.968545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3823617 -1.3095431 -1.2983284 -1.3898919 -1.4941592 -1.4595425 -1.2657471 -0.96526766 -0.67888379 -0.59305143 -0.8559016 -1.4157403 -2.0119774 -2.4355512 -2.5878158][-1.7286605 -1.8213229 -1.8805342 -1.9650313 -2.0432594 -2.0039158 -1.82152 -1.54964 -1.312199 -1.2549287 -1.4478306 -1.8012178 -2.1333153 -2.3342996 -2.3192215][-1.746292 -1.9630959 -2.081038 -2.1574547 -2.221312 -2.1861339 -1.9911398 -1.693174 -1.4595218 -1.4172504 -1.6148059 -1.9042871 -2.1090868 -2.1451385 -1.9442451][-1.4825161 -1.7753247 -1.9169334 -1.9536841 -1.9784707 -1.9336332 -1.7276115 -1.4009292 -1.1275191 -1.0848591 -1.3433783 -1.6975012 -1.9330269 -1.9215447 -1.5618731][-1.0485535 -1.3842524 -1.5253458 -1.5050795 -1.4741566 -1.4387543 -1.247977 -0.87881637 -0.55036044 -0.49365282 -0.78131604 -1.248507 -1.6275626 -1.6708033 -1.2353861][-0.67647111 -1.0305471 -1.147532 -1.0609467 -0.97578907 -0.95046139 -0.84405172 -0.53322124 -0.17837906 -0.020185232 -0.224105 -0.77081442 -1.2870703 -1.4106541 -0.9912113][-0.65582752 -0.92269003 -0.94587564 -0.81147623 -0.71971619 -0.69938242 -0.65690386 -0.4416256 -0.07805407 0.18499804 0.06337595 -0.45596886 -0.9867214 -1.1613384 -0.87306][-0.97941494 -1.0878642 -0.98511457 -0.81106997 -0.73530018 -0.71598446 -0.67396772 -0.48018372 -0.12113619 0.153332 0.066671848 -0.37179947 -0.82257879 -0.9982543 -0.85215211][-1.4290278 -1.4032285 -1.2230194 -1.0195732 -0.92157137 -0.87691939 -0.80793726 -0.611362 -0.29992461 -0.061516047 -0.15450549 -0.50775695 -0.83815718 -0.96881855 -0.89049637][-1.7799335 -1.6736033 -1.4756651 -1.2530375 -1.0805032 -0.99024332 -0.96120226 -0.88698161 -0.70917785 -0.49452484 -0.47860217 -0.64054167 -0.7944963 -0.82922435 -0.79261732][-1.9305617 -1.7776979 -1.580596 -1.3322473 -1.0878696 -0.98832476 -1.0775495 -1.1873846 -1.1665301 -0.9994179 -0.82212007 -0.70730412 -0.60063171 -0.50424933 -0.50917089][-1.8741875 -1.7157972 -1.5191485 -1.2599663 -1.0289749 -0.98996484 -1.1924365 -1.4518468 -1.5453047 -1.4177897 -1.1270659 -0.78333592 -0.440287 -0.21346498 -0.25841534][-1.7672701 -1.6437174 -1.4611032 -1.2143676 -1.0381062 -1.098278 -1.3956349 -1.7273643 -1.8579454 -1.7133663 -1.3693706 -0.94082439 -0.53237545 -0.26793516 -0.31075919][-1.7467403 -1.6659777 -1.5200799 -1.320783 -1.2123671 -1.3381286 -1.6696745 -2.0022581 -2.122143 -1.9669157 -1.6200051 -1.2179911 -0.86984897 -0.688797 -0.75421059][-1.8299533 -1.7830215 -1.6881622 -1.5555048 -1.5042367 -1.6329728 -1.9063554 -2.1653605 -2.2514725 -2.1198702 -1.8400991 -1.5441325 -1.3071699 -1.2035158 -1.2474391]]...]
INFO - root - 2017-12-16 08:09:42.677147: step 7410, loss = 0.56, batch loss = 0.29 (45.8 examples/sec; 0.175 sec/batch; 15h:46m:48s remains)
INFO - root - 2017-12-16 08:09:44.371538: step 7420, loss = 0.60, batch loss = 0.33 (49.1 examples/sec; 0.163 sec/batch; 14h:42m:20s remains)
INFO - root - 2017-12-16 08:09:46.101022: step 7430, loss = 0.72, batch loss = 0.45 (46.5 examples/sec; 0.172 sec/batch; 15h:33m:06s remains)
INFO - root - 2017-12-16 08:09:47.766897: step 7440, loss = 0.53, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 14h:42m:43s remains)
INFO - root - 2017-12-16 08:09:49.453854: step 7450, loss = 0.57, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 15h:30m:44s remains)
INFO - root - 2017-12-16 08:09:51.146506: step 7460, loss = 0.52, batch loss = 0.25 (46.5 examples/sec; 0.172 sec/batch; 15h:31m:31s remains)
INFO - root - 2017-12-16 08:09:52.874018: step 7470, loss = 0.63, batch loss = 0.37 (43.3 examples/sec; 0.185 sec/batch; 16h:41m:47s remains)
INFO - root - 2017-12-16 08:09:54.526573: step 7480, loss = 0.55, batch loss = 0.28 (50.3 examples/sec; 0.159 sec/batch; 14h:20m:44s remains)
INFO - root - 2017-12-16 08:09:56.179732: step 7490, loss = 0.53, batch loss = 0.26 (49.0 examples/sec; 0.163 sec/batch; 14h:45m:08s remains)
INFO - root - 2017-12-16 08:09:57.834332: step 7500, loss = 0.74, batch loss = 0.47 (49.2 examples/sec; 0.163 sec/batch; 14h:41m:36s remains)
2017-12-16 08:09:58.319471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3177481 -2.2155244 -1.7405152 -2.293256 -3.0652404 -3.422179 -3.4451256 -3.4640276 -3.5387268 -3.4365697 -3.0620103 -2.5041938 -2.2498593 -2.6414895 -3.4849768][-3.8000717 -2.4679859 -1.9364139 -2.5866094 -3.3775337 -3.5800743 -3.4648118 -3.5198956 -3.7489293 -3.7868352 -3.5019386 -3.0578086 -2.8854389 -3.2120626 -3.8718553][-3.5954857 -2.0399065 -1.4068727 -2.1079266 -2.9017873 -2.8775239 -2.5374336 -2.5170748 -2.9168801 -3.1957459 -3.1157074 -2.8764994 -2.8772578 -3.15198 -3.5992239][-2.7637522 -0.98000848 -0.26480639 -0.94714105 -1.7001033 -1.4584517 -0.83065319 -0.64105988 -1.1485138 -1.7565415 -1.9861625 -1.9896728 -2.1901824 -2.4980989 -2.822283][-1.8502338 0.075143576 0.87792468 0.27589822 -0.41961324 -0.0028681755 1.0311413 1.4860642 0.93367505 -0.075598 -0.78071594 -1.0766053 -1.4074658 -1.6878159 -1.9100456][-1.0327833 0.96548963 1.80387 1.2166166 0.69644856 1.3210185 2.7511344 3.5949116 2.9788275 1.4131649 0.17669797 -0.38769376 -0.67979348 -0.88870585 -1.0487087][-0.064604282 1.9712105 2.8637633 2.3936796 1.9760337 2.7584591 4.5559707 5.7574463 4.9460688 3.0226412 1.4009533 0.61022496 0.39070535 0.25337362 0.017059803][0.681803 2.7636809 3.73636 3.3947868 2.92809 3.6215191 5.371985 6.6274672 5.8267508 4.0082836 2.4076509 1.6405523 1.5377505 1.4612949 1.1089838][0.41169572 2.4540873 3.444056 3.0157857 2.3740759 2.7779984 3.9820895 4.6998334 4.164412 2.9483724 1.8194351 1.4161415 1.5814812 1.7017968 1.3091605][-0.32219994 1.6478934 2.5612025 1.9907904 1.1709681 1.3091538 1.9990072 2.3351049 2.0532107 1.3893378 0.85633874 0.90747213 1.3595405 1.5641148 1.0567703][-1.3536987 0.44694209 1.2466738 0.56524992 -0.32847166 -0.3982873 -0.053445578 0.12512517 0.015075684 -0.16959858 -0.23342121 0.12032914 0.747972 0.90406919 0.23913383][-2.6239085 -1.0399566 -0.38602197 -1.1343994 -2.0777116 -2.3316534 -2.2178867 -2.1264403 -2.0518079 -1.9272511 -1.7736794 -1.3131788 -0.72326124 -0.64445007 -1.3181872][-3.9454851 -2.6936226 -2.1493013 -2.8130836 -3.7151389 -4.0800266 -4.0990438 -4.0729752 -3.9366665 -3.7313364 -3.5089252 -3.0476959 -2.5415688 -2.4842904 -2.9738855][-4.3642111 -3.4139256 -2.9623051 -3.4702859 -4.2169104 -4.57948 -4.6548643 -4.647665 -4.527132 -4.3732061 -4.2128658 -3.8954029 -3.5222411 -3.4355302 -3.6669662][-3.7198992 -3.0207093 -2.6584682 -3.0117314 -3.5745883 -3.8923745 -3.9719944 -3.9491839 -3.8618817 -3.7833767 -3.7134509 -3.5610142 -3.3453341 -3.1977878 -3.2212448]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-7500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:10:00.719030: step 7510, loss = 0.55, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 15h:27m:51s remains)
INFO - root - 2017-12-16 08:10:02.425676: step 7520, loss = 0.56, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 15h:01m:44s remains)
INFO - root - 2017-12-16 08:10:04.107288: step 7530, loss = 0.64, batch loss = 0.38 (47.7 examples/sec; 0.168 sec/batch; 15h:07m:47s remains)
INFO - root - 2017-12-16 08:10:05.770505: step 7540, loss = 0.52, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 15h:09m:03s remains)
INFO - root - 2017-12-16 08:10:07.453320: step 7550, loss = 0.53, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:46m:27s remains)
INFO - root - 2017-12-16 08:10:09.104625: step 7560, loss = 0.51, batch loss = 0.24 (49.4 examples/sec; 0.162 sec/batch; 14h:36m:44s remains)
INFO - root - 2017-12-16 08:10:10.741593: step 7570, loss = 0.61, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:49m:06s remains)
INFO - root - 2017-12-16 08:10:12.460133: step 7580, loss = 0.64, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 14h:54m:58s remains)
INFO - root - 2017-12-16 08:10:14.170322: step 7590, loss = 0.61, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 15h:10m:06s remains)
INFO - root - 2017-12-16 08:10:15.849518: step 7600, loss = 0.52, batch loss = 0.25 (47.2 examples/sec; 0.169 sec/batch; 15h:17m:48s remains)
2017-12-16 08:10:16.314319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8153298 -1.8391804 -1.7268754 -1.5205092 -1.3509263 -1.4350471 -1.8278098 -2.3016865 -2.6057177 -2.641979 -2.4261072 -2.0758467 -1.67197 -1.3824768 -1.2845891][-2.1337242 -2.0890226 -1.9266658 -1.7231476 -1.5694153 -1.6767962 -2.1489859 -2.7059653 -3.0527072 -3.1376214 -2.9282143 -2.5206966 -1.9870032 -1.5447305 -1.3651617][-2.4408166 -2.3458292 -2.1099334 -1.8305278 -1.5393884 -1.4535277 -1.9042057 -2.5549107 -3.0609396 -3.3503237 -3.3244658 -2.9747179 -2.378684 -1.8097293 -1.521806][-2.583642 -2.4611404 -2.1211979 -1.6211299 -1.0003364 -0.41562843 -0.62413418 -1.4234357 -2.2594781 -2.954463 -3.3419409 -3.2534199 -2.743053 -2.1015127 -1.71046][-2.5155325 -2.3917484 -1.9373473 -1.1472962 -0.095150709 1.0843058 1.4454331 0.550107 -0.732458 -1.979534 -2.9406478 -3.2662594 -2.9663179 -2.3419492 -1.8500673][-2.3438008 -2.2702744 -1.7739455 -0.80373955 0.5750041 2.3064094 3.6261525 3.0289683 1.2953749 -0.6322211 -2.2255542 -3.0547309 -3.0568953 -2.5333035 -1.9758613][-2.2015829 -2.2393913 -1.8449504 -0.8958565 0.60877872 2.6512942 4.9247212 5.1497822 3.1289597 0.62557912 -1.4992437 -2.7498655 -3.0422053 -2.6943705 -2.1363921][-2.1324565 -2.2740843 -2.1102865 -1.4648199 -0.13762975 1.8227208 4.3493776 5.5729208 3.7400002 1.1401234 -1.1129478 -2.5695238 -3.0619686 -2.8567891 -2.3123953][-2.1311231 -2.3320119 -2.3876586 -2.12503 -1.2123446 0.36971045 2.5435557 4.1269078 3.1092787 0.90909171 -1.1072609 -2.5131397 -3.0589004 -2.9625306 -2.476738][-2.0869555 -2.2991009 -2.5029678 -2.5364194 -2.0584235 -0.9001714 0.80395365 2.3199415 2.1530461 0.47934461 -1.2750043 -2.4994004 -3.0063255 -2.9485474 -2.54261][-2.016309 -2.1677365 -2.4021606 -2.5827742 -2.4135735 -1.6394308 -0.35703671 0.88307905 1.1331027 -0.073306322 -1.5543308 -2.5315261 -2.8953753 -2.8087997 -2.4527481][-1.9581141 -2.0340018 -2.2390385 -2.4852087 -2.5116329 -2.0690639 -1.1605892 -0.24600983 0.023139 -0.802161 -1.886392 -2.5537467 -2.7066009 -2.5558777 -2.260921][-1.9143926 -1.9190764 -2.0601423 -2.3080184 -2.451076 -2.2762876 -1.716619 -1.1307533 -0.97688317 -1.4873047 -2.1202149 -2.4129157 -2.3816159 -2.1937292 -1.9526522][-1.8336093 -1.8022478 -1.8707372 -2.0724304 -2.2623248 -2.2563255 -1.9876776 -1.6878662 -1.6310844 -1.8860003 -2.1380615 -2.1505628 -1.9748155 -1.7380911 -1.5109727][-1.7408899 -1.7055857 -1.7035341 -1.8280265 -1.9754469 -2.0424604 -1.9819131 -1.896387 -1.8854892 -1.9705995 -1.9663794 -1.7950068 -1.5282459 -1.2294784 -1.0026202]]...]
INFO - root - 2017-12-16 08:10:17.982347: step 7610, loss = 0.58, batch loss = 0.31 (48.1 examples/sec; 0.166 sec/batch; 15h:00m:00s remains)
INFO - root - 2017-12-16 08:10:19.617535: step 7620, loss = 0.84, batch loss = 0.57 (47.4 examples/sec; 0.169 sec/batch; 15h:14m:47s remains)
INFO - root - 2017-12-16 08:10:21.274880: step 7630, loss = 0.54, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 14h:49m:07s remains)
INFO - root - 2017-12-16 08:10:22.953587: step 7640, loss = 0.61, batch loss = 0.35 (46.0 examples/sec; 0.174 sec/batch; 15h:42m:31s remains)
INFO - root - 2017-12-16 08:10:24.619580: step 7650, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 15h:00m:12s remains)
INFO - root - 2017-12-16 08:10:26.324782: step 7660, loss = 0.61, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:49m:34s remains)
INFO - root - 2017-12-16 08:10:27.977484: step 7670, loss = 0.65, batch loss = 0.38 (48.7 examples/sec; 0.164 sec/batch; 14h:49m:18s remains)
INFO - root - 2017-12-16 08:10:29.636120: step 7680, loss = 0.57, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 15h:24m:50s remains)
INFO - root - 2017-12-16 08:10:31.341084: step 7690, loss = 0.63, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 15h:11m:12s remains)
INFO - root - 2017-12-16 08:10:33.010660: step 7700, loss = 0.53, batch loss = 0.26 (47.5 examples/sec; 0.169 sec/batch; 15h:12m:29s remains)
2017-12-16 08:10:33.485985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2976052 -1.3120902 -1.5930454 -1.9660532 -2.2301447 -2.3944068 -2.5299528 -2.6339891 -2.6273339 -2.517415 -2.4080839 -2.3949039 -2.4099629 -2.4101586 -2.4028962][-1.1275687 -1.1849174 -1.4152032 -1.6981655 -1.8300749 -1.8758087 -2.0203762 -2.2148151 -2.2983758 -2.2503757 -2.2083189 -2.2443147 -2.2713826 -2.2593811 -2.2422378][-1.3459084 -1.4183812 -1.5028338 -1.5968351 -1.4942851 -1.2990525 -1.2681175 -1.4530246 -1.6687999 -1.7665156 -1.8330667 -1.9366809 -2.0303264 -2.048048 -1.9657722][-1.672745 -1.7495075 -1.6810896 -1.4932654 -1.0849607 -0.56612074 -0.28632343 -0.43098903 -0.88228595 -1.2822363 -1.5261786 -1.6258816 -1.6780279 -1.7243831 -1.6765916][-1.8656197 -1.8427403 -1.6325538 -1.2688117 -0.644631 0.12669921 0.66809416 0.66428804 0.017288208 -0.74101818 -1.1129115 -1.0585599 -1.0051978 -1.1515936 -1.190184][-1.8657148 -1.7617679 -1.4628706 -0.98512828 -0.26543486 0.67055535 1.5633934 1.8345573 1.0903172 0.099601746 -0.30189121 -0.014072895 0.11364961 -0.25548124 -0.50130129][-1.7541202 -1.602564 -1.3324759 -0.90515363 -0.23175108 0.86327147 2.1744161 2.7446284 2.1157203 1.1062033 0.77926874 1.3018684 1.4374552 0.70262837 0.10988593][-1.6863949 -1.5986879 -1.5034492 -1.326956 -0.87223577 0.22889638 1.840687 2.7800651 2.51152 1.7226748 1.5435271 2.167819 2.1682925 1.1573179 0.21332455][-1.7905147 -1.7894576 -1.8609098 -1.9054221 -1.6636229 -0.64127648 1.0803554 2.2636762 2.2304187 1.5787547 1.3539195 1.8406832 1.6514535 0.53375244 -0.59076595][-2.008728 -2.1180604 -2.2954628 -2.4545722 -2.3176966 -1.3882918 0.2161634 1.3349934 1.3712115 0.77338529 0.44649553 0.64642334 0.32143092 -0.65076554 -1.6715275][-2.2430742 -2.420867 -2.6626568 -2.898459 -2.8513672 -2.0162003 -0.56619489 0.41612244 0.44169664 -0.18213785 -0.72153652 -0.81427836 -1.1473196 -1.8669982 -2.6030223][-2.2653642 -2.430892 -2.6339068 -2.8119621 -2.753695 -2.0450079 -0.83885896 -0.090745449 -0.2041961 -0.99864316 -1.7696153 -2.0898175 -2.3649464 -2.7607894 -3.1104491][-2.015902 -2.1204295 -2.2508714 -2.3408823 -2.2676139 -1.7438277 -0.83827221 -0.32465649 -0.595829 -1.4940803 -2.3761103 -2.7784419 -2.9350255 -3.0712512 -3.1041956][-1.4319336 -1.4736795 -1.5201806 -1.5659263 -1.5617378 -1.2532704 -0.63555312 -0.3682524 -0.79283452 -1.6660445 -2.41753 -2.7654283 -2.8099685 -2.7603445 -2.6212249][-0.72381115 -0.73028409 -0.69542658 -0.71628034 -0.8063674 -0.76417673 -0.49041152 -0.3933953 -0.77605748 -1.4285604 -1.9435738 -2.1843066 -2.2344959 -2.2009509 -2.0607758]]...]
INFO - root - 2017-12-16 08:10:35.171402: step 7710, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.166 sec/batch; 14h:56m:37s remains)
INFO - root - 2017-12-16 08:10:36.802985: step 7720, loss = 0.55, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 15h:08m:53s remains)
INFO - root - 2017-12-16 08:10:38.478669: step 7730, loss = 0.58, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 14h:56m:58s remains)
INFO - root - 2017-12-16 08:10:40.156187: step 7740, loss = 0.52, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 14h:50m:50s remains)
INFO - root - 2017-12-16 08:10:41.810026: step 7750, loss = 0.77, batch loss = 0.50 (49.1 examples/sec; 0.163 sec/batch; 14h:41m:56s remains)
INFO - root - 2017-12-16 08:10:43.467640: step 7760, loss = 0.62, batch loss = 0.35 (47.7 examples/sec; 0.168 sec/batch; 15h:08m:25s remains)
INFO - root - 2017-12-16 08:10:45.142697: step 7770, loss = 0.56, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 15h:19m:28s remains)
INFO - root - 2017-12-16 08:10:46.846719: step 7780, loss = 0.58, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:57m:42s remains)
INFO - root - 2017-12-16 08:10:48.514767: step 7790, loss = 0.63, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 15h:00m:08s remains)
INFO - root - 2017-12-16 08:10:50.191972: step 7800, loss = 0.67, batch loss = 0.40 (47.6 examples/sec; 0.168 sec/batch; 15h:10m:12s remains)
2017-12-16 08:10:50.657708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5104464 -1.6169658 -1.8010443 -2.0221899 -2.1940918 -2.2380815 -2.1630719 -2.0642185 -2.0165954 -2.0818262 -2.2015355 -2.3159621 -2.3598983 -2.3195837 -2.2167325][-1.39164 -1.53048 -1.8015932 -2.1163728 -2.3619668 -2.423789 -2.3189113 -2.1470273 -2.0455914 -2.076124 -2.2362769 -2.3948555 -2.474385 -2.426733 -2.2405279][-1.2243196 -1.3950895 -1.7125733 -2.087132 -2.3703306 -2.3824174 -2.1763172 -1.8687828 -1.6446236 -1.6383646 -1.8906711 -2.208781 -2.3856573 -2.3336504 -2.0293994][-1.05937 -1.2396607 -1.5791324 -1.9839418 -2.2195983 -2.0608184 -1.6069744 -1.0053579 -0.57679 -0.6279217 -1.2009213 -1.8832115 -2.2329547 -2.157573 -1.679921][-0.940752 -1.1129396 -1.4501932 -1.8312465 -1.9086924 -1.439235 -0.59275496 0.4173882 1.0309711 0.62728214 -0.53300536 -1.6396477 -2.1713967 -2.0264616 -1.3768805][-0.90984964 -1.0737841 -1.4172816 -1.7059387 -1.5414852 -0.69926906 0.66358709 2.1076236 2.6013403 1.4399545 -0.26926517 -1.6430111 -2.2338781 -1.9756405 -1.2226142][-0.94888151 -1.1242712 -1.4454482 -1.5993456 -1.1728997 0.031334877 1.8768892 3.6954713 3.444941 1.5430629 -0.43448246 -1.8475366 -2.336587 -1.9560734 -1.1732389][-0.96761477 -1.1231356 -1.3593247 -1.3391256 -0.70762908 0.72865868 2.7993011 4.4279466 3.2321787 1.1316903 -0.78711236 -2.0047133 -2.2521007 -1.7691166 -1.0036222][-0.92457807 -0.99694943 -1.0914354 -0.93453884 -0.19803083 1.2602024 2.9736724 3.5401254 2.2777023 0.44449306 -1.1710103 -2.0537646 -2.0171676 -1.442945 -0.76338029][-0.90492356 -0.88464189 -0.86671746 -0.61651492 0.089161396 1.2662375 2.2447543 2.1772485 1.1081586 -0.3060782 -1.5404752 -2.0851109 -1.830214 -1.2328292 -0.69186234][-0.97093439 -0.87899685 -0.80434632 -0.58410227 -0.080528736 0.6398983 1.0886772 0.86479759 0.062427759 -0.98287606 -1.8800015 -2.1350555 -1.7787416 -1.2323325 -0.80242538][-1.0278002 -0.92665541 -0.86545813 -0.80675554 -0.59804296 -0.22031522 0.0064983368 -0.1685394 -0.74869418 -1.5476587 -2.1604345 -2.2354565 -1.8945185 -1.4813399 -1.1339557][-1.1290722 -1.0942969 -1.1420522 -1.2772827 -1.2555122 -1.0125364 -0.82209563 -0.96577263 -1.4372452 -2.062551 -2.4549448 -2.4591358 -2.1830332 -1.8719914 -1.5677602][-1.2650821 -1.3387566 -1.573163 -1.8207812 -1.8320761 -1.5916724 -1.3916514 -1.5065948 -1.9312456 -2.4158897 -2.6826925 -2.64947 -2.4333389 -2.1758456 -1.910676][-1.4019006 -1.5745544 -1.9263052 -2.1967185 -2.1877613 -1.9520564 -1.7634811 -1.8225719 -2.1542473 -2.5117106 -2.6986868 -2.6718521 -2.4955955 -2.2771692 -2.0933442]]...]
INFO - root - 2017-12-16 08:10:52.329723: step 7810, loss = 0.50, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 15h:02m:00s remains)
INFO - root - 2017-12-16 08:10:53.986252: step 7820, loss = 0.61, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 14h:59m:41s remains)
INFO - root - 2017-12-16 08:10:55.668288: step 7830, loss = 0.71, batch loss = 0.44 (48.8 examples/sec; 0.164 sec/batch; 14h:47m:51s remains)
INFO - root - 2017-12-16 08:10:57.360777: step 7840, loss = 0.56, batch loss = 0.29 (46.9 examples/sec; 0.171 sec/batch; 15h:22m:44s remains)
INFO - root - 2017-12-16 08:10:59.033140: step 7850, loss = 0.56, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 15h:15m:35s remains)
INFO - root - 2017-12-16 08:11:00.718053: step 7860, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 15h:07m:34s remains)
INFO - root - 2017-12-16 08:11:02.390167: step 7870, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:52m:43s remains)
INFO - root - 2017-12-16 08:11:04.041565: step 7880, loss = 0.58, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 15h:09m:40s remains)
INFO - root - 2017-12-16 08:11:05.698659: step 7890, loss = 0.57, batch loss = 0.30 (48.5 examples/sec; 0.165 sec/batch; 14h:51m:35s remains)
INFO - root - 2017-12-16 08:11:07.383352: step 7900, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:59m:52s remains)
2017-12-16 08:11:07.923674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9125736 -2.0058551 -2.0744252 -2.1251214 -2.178664 -2.2216725 -2.2501712 -2.278352 -2.3060203 -2.3350234 -2.3846338 -2.3946338 -2.315563 -2.163677 -1.955423][-2.2999034 -2.4932811 -2.641417 -2.7528081 -2.860455 -2.9519897 -3.0164957 -3.0606952 -3.0842772 -3.1190648 -3.2127314 -3.2339664 -3.1077428 -2.8414607 -2.458689][-2.5655217 -2.7789083 -2.9244218 -3.0173073 -3.1039982 -3.1920397 -3.2678449 -3.3152761 -3.3401165 -3.4100308 -3.5976138 -3.7097003 -3.6420546 -3.376874 -2.9057157][-2.5604727 -2.6483145 -2.6598558 -2.626147 -2.6053603 -2.6329832 -2.6990395 -2.7435102 -2.7737541 -2.9127679 -3.25133 -3.5423608 -3.6288955 -3.4925365 -3.0888081][-2.2501435 -2.0998619 -1.8787481 -1.6283257 -1.4265015 -1.3568541 -1.4026163 -1.4521283 -1.4714234 -1.6918914 -2.2302353 -2.7632697 -3.0936306 -3.2019775 -3.0181882][-1.6409839 -1.1923008 -0.68657207 -0.19392395 0.2117722 0.39703965 0.37290597 0.34333014 0.37348652 0.045324802 -0.75398529 -1.5874426 -2.209686 -2.636642 -2.7841103][-0.94256914 -0.17264807 0.615839 1.3514819 1.9536204 2.255486 2.2457118 2.2266965 2.2288046 1.6930921 0.59579372 -0.5352447 -1.4458885 -2.1713173 -2.5785379][-0.7660358 0.19359612 1.1501024 2.0678744 2.8706517 3.3268909 3.3892193 3.4193802 3.3177009 2.5268655 1.1640263 -0.15601027 -1.2606547 -2.1713574 -2.6634986][-1.3464537 -0.49306643 0.34482741 1.1463025 1.8911111 2.3114944 2.3452306 2.3954411 2.3153782 1.5612664 0.28824043 -0.92588913 -1.9658673 -2.7778833 -3.0692406][-2.0393431 -1.4093778 -0.77386093 -0.15160453 0.44562244 0.74906659 0.70903206 0.68590164 0.58899188 -0.014208794 -1.0145757 -1.9755545 -2.8282704 -3.4052908 -3.3882039][-2.5888104 -2.2313647 -1.8113587 -1.3518424 -0.92981529 -0.75809205 -0.82037556 -0.86001766 -0.93537509 -1.3399916 -2.0308316 -2.7319613 -3.3754914 -3.7178411 -3.4827182][-2.6943877 -2.5611789 -2.3479087 -2.0740774 -1.8652136 -1.8367907 -1.9346042 -1.9861465 -2.0244524 -2.2468474 -2.6397667 -3.0848067 -3.4938307 -3.6225777 -3.3033082][-2.443397 -2.4012175 -2.2908635 -2.1471505 -2.0741992 -2.1193657 -2.2217724 -2.2872944 -2.3362813 -2.4706247 -2.7074461 -2.9740617 -3.1998434 -3.2077041 -2.921859][-2.0678892 -2.0517669 -1.9873886 -1.9030302 -1.8614085 -1.8894378 -1.9492563 -2.0051243 -2.0700457 -2.1787632 -2.3477478 -2.5217979 -2.6421289 -2.6124351 -2.4197762][-1.7306899 -1.7013618 -1.6460491 -1.5793419 -1.5248574 -1.5086925 -1.5117435 -1.5307422 -1.5828403 -1.6699616 -1.7929792 -1.9145912 -1.9917946 -1.9892902 -1.910184]]...]
INFO - root - 2017-12-16 08:11:09.598240: step 7910, loss = 0.52, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 15h:13m:10s remains)
INFO - root - 2017-12-16 08:11:11.246799: step 7920, loss = 0.51, batch loss = 0.25 (49.4 examples/sec; 0.162 sec/batch; 14h:36m:47s remains)
INFO - root - 2017-12-16 08:11:12.898622: step 7930, loss = 0.53, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 15h:25m:10s remains)
INFO - root - 2017-12-16 08:11:14.570635: step 7940, loss = 0.71, batch loss = 0.45 (47.2 examples/sec; 0.169 sec/batch; 15h:16m:17s remains)
INFO - root - 2017-12-16 08:11:16.242885: step 7950, loss = 0.53, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 15h:23m:28s remains)
INFO - root - 2017-12-16 08:11:17.930215: step 7960, loss = 0.73, batch loss = 0.47 (47.2 examples/sec; 0.169 sec/batch; 15h:15m:54s remains)
INFO - root - 2017-12-16 08:11:19.607060: step 7970, loss = 0.65, batch loss = 0.38 (49.4 examples/sec; 0.162 sec/batch; 14h:35m:19s remains)
INFO - root - 2017-12-16 08:11:21.319503: step 7980, loss = 0.56, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 15h:30m:42s remains)
INFO - root - 2017-12-16 08:11:23.002820: step 7990, loss = 0.57, batch loss = 0.30 (48.5 examples/sec; 0.165 sec/batch; 14h:51m:22s remains)
INFO - root - 2017-12-16 08:11:24.666710: step 8000, loss = 0.57, batch loss = 0.31 (49.5 examples/sec; 0.162 sec/batch; 14h:34m:02s remains)
2017-12-16 08:11:25.183444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.93400872 -0.81318974 -0.86365044 -0.90022981 -0.86613333 -0.79443336 -0.70913088 -0.61932814 -0.52775955 -0.46441507 -0.36931884 -0.16634035 0.037740707 -0.070355654 -0.44082344][-1.6736666 -1.6255677 -1.7060055 -1.7586333 -1.7506678 -1.7360507 -1.7127211 -1.6935923 -1.6819834 -1.6733325 -1.6160392 -1.438112 -1.2322778 -1.2727871 -1.5237885][-2.5434017 -2.5449812 -2.6402237 -2.7117977 -2.747957 -2.8326478 -2.9003131 -2.9601841 -3.048053 -3.1190085 -3.1377869 -3.0543282 -2.9301398 -2.9313097 -3.0240273][-3.0730333 -3.0106952 -3.0615096 -3.1275151 -3.1869535 -3.2997072 -3.3707008 -3.4270794 -3.5948081 -3.8119593 -3.9562354 -4.0242958 -4.087224 -4.1493998 -4.1439657][-2.7211394 -2.5184293 -2.3966737 -2.3186703 -2.313935 -2.3832252 -2.3222215 -2.2566807 -2.5071571 -2.9572954 -3.3219731 -3.6283164 -3.9477551 -4.1613531 -4.1850185][-1.8218689 -1.4322808 -0.99997497 -0.62376475 -0.44380546 -0.30219877 0.07445097 0.36275363 0.0098807812 -0.70212853 -1.2784536 -1.7866185 -2.3307447 -2.7432556 -2.8757558][-0.9791038 -0.4275949 0.29526734 0.94636059 1.3104649 1.6833432 2.3943865 2.9807208 2.6647022 1.7873397 1.1247313 0.62299991 0.021480083 -0.52365208 -0.796708][-0.86536312 -0.28551328 0.56847286 1.3117671 1.6672232 2.0275505 2.8354113 3.5811775 3.3003309 2.441582 1.9304292 1.6711619 1.3195875 0.9212358 0.60707188][-1.6849751 -1.3096528 -0.60041356 -0.029022694 0.091849804 0.15546751 0.72111368 1.3457158 1.2075956 0.672241 0.44150972 0.50252056 0.5535717 0.48237658 0.29464626][-2.5510907 -2.4441833 -2.0801415 -1.8554796 -2.0188372 -2.2351139 -1.9904357 -1.5685859 -1.5329884 -1.7292602 -1.707276 -1.3806014 -1.0100583 -0.81440318 -0.87827647][-2.5314546 -2.5863829 -2.5287294 -2.6395652 -3.0270007 -3.4330707 -3.4099076 -3.1493783 -3.0086656 -3.0131404 -2.8285522 -2.3879786 -1.8923763 -1.6157237 -1.6354396][-1.6296084 -1.7147604 -1.7990601 -2.0945063 -2.5646148 -3.0053439 -3.0834427 -2.87715 -2.6688936 -2.5423882 -2.2935882 -1.8477192 -1.3996633 -1.2275813 -1.3479288][-0.37595439 -0.30494094 -0.36526656 -0.65408087 -1.0310254 -1.3627808 -1.4302261 -1.2392924 -0.99298632 -0.79403496 -0.54844427 -0.18700492 0.1076386 0.040940523 -0.31320465][0.68560147 0.94154954 0.99508762 0.83902717 0.63823557 0.47625518 0.49047637 0.70174 0.94850755 1.1207879 1.2858899 1.5258198 1.6236935 1.2903736 0.68661451][0.746181 1.0930429 1.2066331 1.1320283 1.0568886 1.0247641 1.0849531 1.2647471 1.4655433 1.5767002 1.6524584 1.7798948 1.7811503 1.351656 0.67638612]]...]
INFO - root - 2017-12-16 08:11:26.877492: step 8010, loss = 0.73, batch loss = 0.46 (49.1 examples/sec; 0.163 sec/batch; 14h:40m:24s remains)
INFO - root - 2017-12-16 08:11:28.533132: step 8020, loss = 0.59, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 15h:16m:20s remains)
INFO - root - 2017-12-16 08:11:30.197436: step 8030, loss = 0.60, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:59m:00s remains)
INFO - root - 2017-12-16 08:11:31.867604: step 8040, loss = 0.58, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 15h:22m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:11:33.555296: step 8050, loss = 0.66, batch loss = 0.39 (49.2 examples/sec; 0.163 sec/batch; 14h:39m:40s remains)
INFO - root - 2017-12-16 08:11:35.202995: step 8060, loss = 0.74, batch loss = 0.48 (49.6 examples/sec; 0.161 sec/batch; 14h:31m:36s remains)
INFO - root - 2017-12-16 08:11:36.869023: step 8070, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 15h:04m:24s remains)
INFO - root - 2017-12-16 08:11:38.573997: step 8080, loss = 0.53, batch loss = 0.26 (46.1 examples/sec; 0.173 sec/batch; 15h:37m:32s remains)
INFO - root - 2017-12-16 08:11:40.247432: step 8090, loss = 0.65, batch loss = 0.38 (49.2 examples/sec; 0.163 sec/batch; 14h:39m:21s remains)
INFO - root - 2017-12-16 08:11:41.914855: step 8100, loss = 0.53, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 15h:25m:03s remains)
2017-12-16 08:11:42.456627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3483102 -2.5710647 -2.8400064 -3.0744734 -3.182394 -3.144186 -3.0315268 -2.9052949 -2.7531114 -2.5787528 -2.3406334 -2.1303372 -1.9950747 -1.9135089 -1.870537][-2.4024398 -2.7030356 -3.0381677 -3.3285832 -3.4403706 -3.4043915 -3.3674164 -3.3766437 -3.3563972 -3.2388949 -3.0054569 -2.7592607 -2.579268 -2.4715555 -2.4060447][-2.2867148 -2.5250735 -2.7419598 -2.9205801 -2.9289424 -2.9220066 -3.0550833 -3.4313979 -3.8043296 -3.9387767 -3.8246312 -3.6024787 -3.4026866 -3.2637365 -3.1486335][-2.1165082 -2.1537676 -2.0734489 -1.9169682 -1.7017772 -1.5246928 -1.6598285 -2.4079065 -3.3469238 -3.9450321 -4.1348705 -4.1290169 -4.0563693 -3.9331408 -3.7388124][-2.0135365 -1.892327 -1.5530629 -0.87995958 -0.20132399 0.45924258 0.64264941 -0.28645098 -1.6137748 -2.6127365 -3.2421618 -3.6752172 -3.9261231 -3.93513 -3.7779679][-2.1594737 -1.9673454 -1.4150314 -0.24923968 1.1531968 2.5538688 3.393137 2.6906443 1.2227626 -0.054582119 -1.0853219 -2.0459468 -2.6521826 -2.8795276 -2.8934639][-2.4297519 -2.2758019 -1.5950084 -0.1402303 1.8821812 4.0611472 5.6042118 5.6269035 4.2492671 2.8224206 1.51687 0.20525384 -0.66725945 -0.99004805 -1.1184306][-2.6268153 -2.5612319 -1.8925914 -0.34320807 1.786104 4.1025238 5.9830065 6.8294373 5.8096457 4.3779578 3.2089181 1.8348656 0.90276766 0.656512 0.55759454][-2.6315403 -2.5886831 -2.0159314 -0.7468431 0.98565149 2.8128552 4.3193097 4.9855356 4.4549227 3.6065292 2.9931922 2.0376663 1.2690659 1.2037909 1.236486][-2.588836 -2.5520835 -2.1830792 -1.3763648 -0.29659843 0.74749255 1.5841949 1.7581441 1.4055231 1.1146991 1.0490551 0.74950123 0.38722563 0.59409475 0.80816984][-2.6096518 -2.6911197 -2.5895226 -2.2653847 -1.8242209 -1.4008274 -1.1244367 -1.2956216 -1.7038672 -1.7371337 -1.5234253 -1.3876923 -1.3164623 -0.94816244 -0.65534389][-2.7562242 -2.9743829 -3.121954 -3.153446 -3.0720043 -2.9772882 -3.0441704 -3.4053888 -3.8303571 -3.8118935 -3.5051155 -3.2212226 -2.9350359 -2.5621939 -2.3012846][-2.8323731 -3.1443892 -3.4032636 -3.5371037 -3.5514209 -3.5921011 -3.7851856 -4.1974611 -4.6278896 -4.6659079 -4.4233327 -4.08071 -3.7323189 -3.3828762 -3.1883328][-2.7662761 -3.0875196 -3.3440654 -3.4488873 -3.4092302 -3.4283242 -3.6338632 -4.031889 -4.3950167 -4.4674754 -4.2695861 -3.9745045 -3.6345968 -3.3239846 -3.169404][-2.5680523 -2.8217432 -3.0314665 -3.1032393 -3.0284307 -3.0076523 -3.161108 -3.4300265 -3.6651802 -3.717195 -3.589669 -3.3784332 -3.103128 -2.8620276 -2.7453585]]...]
INFO - root - 2017-12-16 08:11:44.138495: step 8110, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 15h:05m:59s remains)
INFO - root - 2017-12-16 08:11:45.830477: step 8120, loss = 0.53, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 15h:13m:53s remains)
INFO - root - 2017-12-16 08:11:47.514470: step 8130, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 15h:10m:30s remains)
INFO - root - 2017-12-16 08:11:49.151043: step 8140, loss = 0.55, batch loss = 0.29 (49.3 examples/sec; 0.162 sec/batch; 14h:37m:43s remains)
INFO - root - 2017-12-16 08:11:50.834964: step 8150, loss = 0.75, batch loss = 0.48 (46.4 examples/sec; 0.173 sec/batch; 15h:32m:52s remains)
INFO - root - 2017-12-16 08:11:52.541547: step 8160, loss = 0.66, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 15h:02m:27s remains)
INFO - root - 2017-12-16 08:11:54.251374: step 8170, loss = 0.57, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 15h:22m:47s remains)
INFO - root - 2017-12-16 08:11:55.974441: step 8180, loss = 0.55, batch loss = 0.29 (45.0 examples/sec; 0.178 sec/batch; 16h:01m:02s remains)
INFO - root - 2017-12-16 08:11:57.653224: step 8190, loss = 0.53, batch loss = 0.26 (47.5 examples/sec; 0.169 sec/batch; 15h:10m:50s remains)
INFO - root - 2017-12-16 08:11:59.324757: step 8200, loss = 0.63, batch loss = 0.36 (47.9 examples/sec; 0.167 sec/batch; 15h:03m:05s remains)
2017-12-16 08:11:59.844727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9462737 -2.1687567 -2.456517 -2.7249117 -2.9093628 -2.9713547 -2.9565287 -2.9411716 -2.8781319 -2.7221346 -2.4604383 -2.1726174 -1.897113 -1.676953 -1.5655726][-2.3855286 -2.7196093 -3.0849175 -3.3291767 -3.423408 -3.3667879 -3.2631226 -3.2444434 -3.184546 -3.0478625 -2.8288279 -2.550354 -2.2462738 -1.9848229 -1.844636][-2.7994123 -3.1468041 -3.436799 -3.5383792 -3.4121294 -3.1480527 -2.9065585 -2.8861184 -2.8498595 -2.8381052 -2.85751 -2.7580113 -2.5577817 -2.3429916 -2.2208257][-3.0309024 -3.3051567 -3.4123595 -3.2668769 -2.840817 -2.2975295 -1.8505992 -1.7128837 -1.6996262 -1.9431013 -2.3556707 -2.5877986 -2.5828712 -2.5176718 -2.4819965][-3.0286589 -3.1372378 -3.0026069 -2.5914423 -1.8947694 -1.0666748 -0.31330609 0.12243938 0.080234289 -0.4876523 -1.3910936 -2.0423141 -2.3023753 -2.4587026 -2.6183739][-2.8328953 -2.7582834 -2.3422341 -1.7233702 -0.80970776 0.32770538 1.4851971 2.3102269 2.1274357 1.1394575 -0.21412265 -1.1335772 -1.6404068 -2.1228228 -2.5927215][-2.2476454 -1.9424504 -1.343735 -0.5614382 0.45429921 1.8252518 3.4381509 4.784987 4.1021986 2.5905709 1.0615885 0.0973959 -0.60796547 -1.5037746 -2.3296831][-1.3373013 -0.84695315 -0.23062396 0.48663259 1.4125848 2.8388724 4.7270188 6.4695587 5.0387425 3.2604609 1.9318924 1.087317 0.28854561 -0.86664665 -1.9682686][-0.61569369 -0.0407629 0.31372619 0.69424295 1.3459735 2.5032005 3.9547544 4.853847 3.8064494 2.4862747 1.6441214 1.0587521 0.31231618 -0.75471187 -1.8165346][-0.62560081 -0.10584164 0.02562499 0.052621365 0.37900805 1.0939617 1.8772783 2.1653104 1.4710903 0.70510054 0.33578062 0.057150126 -0.47423983 -1.2417831 -1.9778384][-1.3638965 -0.99104118 -0.92402685 -1.058689 -1.0016969 -0.68864715 -0.44719648 -0.49422991 -0.91908014 -1.2726524 -1.2925721 -1.3121314 -1.5913115 -2.0269191 -2.343677][-2.342001 -2.1242774 -2.0626714 -2.29593 -2.4693894 -2.4506154 -2.5000949 -2.655441 -2.8547189 -2.9248602 -2.7482405 -2.5767145 -2.6183724 -2.6969702 -2.6649446][-3.0343904 -2.9408937 -2.9303186 -3.1691744 -3.4336381 -3.5298102 -3.6233125 -3.7197757 -3.760411 -3.6881623 -3.4493747 -3.2139206 -3.0922775 -2.9438891 -2.7241933][-3.2351851 -3.2279167 -3.2669463 -3.4643278 -3.6725633 -3.7666211 -3.7860484 -3.736546 -3.6784723 -3.5997319 -3.4096355 -3.2030673 -3.0008869 -2.7798891 -2.5302026][-2.9182415 -2.9707561 -3.0601437 -3.2038887 -3.328253 -3.3657789 -3.3493323 -3.2645931 -3.1937616 -3.1485085 -3.0417218 -2.8898997 -2.6998248 -2.4857037 -2.2954144]]...]
INFO - root - 2017-12-16 08:12:01.593510: step 8210, loss = 0.62, batch loss = 0.35 (46.8 examples/sec; 0.171 sec/batch; 15h:24m:24s remains)
INFO - root - 2017-12-16 08:12:03.284187: step 8220, loss = 0.56, batch loss = 0.29 (46.0 examples/sec; 0.174 sec/batch; 15h:40m:55s remains)
INFO - root - 2017-12-16 08:12:04.990668: step 8230, loss = 0.56, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 15h:13m:39s remains)
INFO - root - 2017-12-16 08:12:06.705412: step 8240, loss = 0.60, batch loss = 0.34 (44.6 examples/sec; 0.179 sec/batch; 16h:09m:45s remains)
INFO - root - 2017-12-16 08:12:08.409853: step 8250, loss = 0.63, batch loss = 0.36 (46.2 examples/sec; 0.173 sec/batch; 15h:36m:10s remains)
INFO - root - 2017-12-16 08:12:10.128728: step 8260, loss = 0.64, batch loss = 0.38 (48.8 examples/sec; 0.164 sec/batch; 14h:45m:35s remains)
INFO - root - 2017-12-16 08:12:11.840833: step 8270, loss = 0.71, batch loss = 0.44 (48.1 examples/sec; 0.166 sec/batch; 14h:58m:50s remains)
INFO - root - 2017-12-16 08:12:13.495003: step 8280, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 14h:41m:59s remains)
INFO - root - 2017-12-16 08:12:15.133158: step 8290, loss = 0.62, batch loss = 0.35 (48.9 examples/sec; 0.164 sec/batch; 14h:44m:19s remains)
INFO - root - 2017-12-16 08:12:16.817537: step 8300, loss = 0.58, batch loss = 0.32 (45.4 examples/sec; 0.176 sec/batch; 15h:52m:51s remains)
2017-12-16 08:12:17.280103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8817924 -1.7002292 -1.5523901 -1.5273156 -1.6232688 -1.8120025 -2.0133731 -2.1318431 -2.13118 -2.0353408 -1.9369231 -1.9098725 -1.9113425 -1.9486977 -1.9746962][-1.5158108 -1.292221 -1.10204 -1.0901424 -1.2629392 -1.5622201 -1.8482596 -2.0024755 -1.9921474 -1.8698906 -1.7204217 -1.6041408 -1.530301 -1.5882335 -1.7373855][-1.1198809 -0.90807307 -0.73822343 -0.76868141 -1.0432742 -1.4664865 -1.8196986 -1.9994637 -1.979736 -1.8234159 -1.6082265 -1.3711641 -1.1978484 -1.2711828 -1.5349648][-0.79942966 -0.64835942 -0.582888 -0.7317239 -1.099779 -1.5896032 -1.949919 -2.1067481 -2.0613279 -1.8695577 -1.6064011 -1.2895839 -1.0556287 -1.1330457 -1.4415298][-0.64830935 -0.61620343 -0.70709729 -0.96840477 -1.3576095 -1.7994421 -2.097331 -2.2094769 -2.1553521 -1.9801975 -1.7195482 -1.4114256 -1.1842374 -1.2499099 -1.5180966][-0.73853683 -0.82833922 -1.0408955 -1.3599939 -1.7139729 -2.0445137 -2.2556472 -2.3479531 -2.3152907 -2.1857629 -1.9931755 -1.7481269 -1.5301827 -1.5076551 -1.6530299][-0.97237992 -1.1514885 -1.4344049 -1.7516574 -1.9998872 -2.1883492 -2.3102317 -2.3968682 -2.3971987 -2.320811 -2.1815369 -1.9478599 -1.7084351 -1.6025779 -1.6275817][-1.1429349 -1.3586633 -1.6353884 -1.8871945 -2.0167537 -2.0728204 -2.1027079 -2.1302135 -2.1202374 -2.0771399 -1.9645053 -1.7315538 -1.4870412 -1.3632886 -1.3820422][-1.2320731 -1.4299006 -1.6490542 -1.7958218 -1.8143845 -1.7634697 -1.7231529 -1.6916283 -1.6648353 -1.6426419 -1.5596796 -1.3584771 -1.1497223 -1.1018411 -1.2509038][-1.3104157 -1.4521489 -1.5704315 -1.6230412 -1.5942852 -1.5082259 -1.4201534 -1.3448817 -1.3252921 -1.3479187 -1.3408701 -1.2530701 -1.1630592 -1.2505031 -1.5033069][-1.4695435 -1.549427 -1.6126016 -1.6457549 -1.6369705 -1.5802379 -1.499162 -1.4124924 -1.3839221 -1.4297533 -1.5032582 -1.5371016 -1.5731862 -1.7244599 -1.9640186][-1.6678402 -1.6914973 -1.735456 -1.7890977 -1.8136593 -1.796566 -1.7466798 -1.675557 -1.6310706 -1.6691723 -1.7697109 -1.8616689 -1.9410267 -2.0685716 -2.2222][-1.8306236 -1.8218775 -1.8443066 -1.8901194 -1.9226867 -1.9264889 -1.9123386 -1.8809456 -1.8583509 -1.8834777 -1.9572814 -2.0353384 -2.0844977 -2.139241 -2.1997941][-1.8185019 -1.787909 -1.775943 -1.7781243 -1.7932466 -1.8069096 -1.817531 -1.8181802 -1.8196172 -1.8302594 -1.8648628 -1.9012377 -1.8981135 -1.8977017 -1.9172677][-1.621147 -1.5630071 -1.5122974 -1.4754953 -1.4710677 -1.486316 -1.5037432 -1.5127176 -1.5167733 -1.5183951 -1.5235692 -1.5182564 -1.4838748 -1.4827596 -1.5272948]]...]
INFO - root - 2017-12-16 08:12:18.935507: step 8310, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:55m:20s remains)
INFO - root - 2017-12-16 08:12:20.608075: step 8320, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 14h:53m:31s remains)
INFO - root - 2017-12-16 08:12:22.273851: step 8330, loss = 0.49, batch loss = 0.22 (48.9 examples/sec; 0.163 sec/batch; 14h:43m:19s remains)
INFO - root - 2017-12-16 08:12:23.980514: step 8340, loss = 0.64, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 14h:55m:26s remains)
INFO - root - 2017-12-16 08:12:25.656736: step 8350, loss = 0.63, batch loss = 0.37 (48.2 examples/sec; 0.166 sec/batch; 14h:56m:11s remains)
INFO - root - 2017-12-16 08:12:27.338917: step 8360, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:59m:15s remains)
INFO - root - 2017-12-16 08:12:28.987024: step 8370, loss = 0.57, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 14h:46m:24s remains)
INFO - root - 2017-12-16 08:12:30.656865: step 8380, loss = 0.67, batch loss = 0.40 (46.7 examples/sec; 0.171 sec/batch; 15h:24m:36s remains)
INFO - root - 2017-12-16 08:12:32.320215: step 8390, loss = 0.63, batch loss = 0.37 (46.6 examples/sec; 0.172 sec/batch; 15h:26m:58s remains)
INFO - root - 2017-12-16 08:12:34.008267: step 8400, loss = 0.63, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 14h:59m:41s remains)
2017-12-16 08:12:34.545392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3103726 -1.6152445 -1.9197583 -2.1223595 -2.2276034 -2.2801836 -2.3042657 -2.279839 -2.1879506 -2.062618 -1.9934423 -1.9669658 -1.9529417 -1.921231 -1.8595272][-1.6474011 -2.2523863 -2.823396 -3.1928754 -3.4186463 -3.5384464 -3.5629563 -3.4866948 -3.2891054 -3.026716 -2.8603556 -2.8111949 -2.816716 -2.8220849 -2.8483927][-1.9371091 -2.7165492 -3.397414 -3.8646655 -4.1085739 -4.1748738 -4.0833325 -3.8696351 -3.508635 -3.0533438 -2.7323678 -2.6474466 -2.7078919 -2.8442342 -3.0445454][-2.2625175 -2.9008646 -3.4191628 -3.7580509 -3.8652186 -3.7278285 -3.4361546 -3.04869 -2.5018182 -1.8417532 -1.3950851 -1.3490733 -1.5672784 -1.9126402 -2.3625348][-2.403748 -2.6360724 -2.7978883 -2.8362517 -2.6525812 -2.2059686 -1.6429926 -1.0640473 -0.3903439 0.40156484 0.85015559 0.73697448 0.22615266 -0.4054389 -1.1217784][-2.1015089 -1.9569131 -1.7279981 -1.4022632 -0.94797397 -0.29212391 0.43546319 1.0447612 1.6626835 2.3415976 2.6242967 2.3056865 1.5377793 0.64297891 -0.23497069][-1.1794236 -0.86522651 -0.5094918 -0.066437483 0.45063019 1.1450341 1.7982183 2.2486749 2.5397863 2.810369 2.7713218 2.2593808 1.446099 0.52880239 -0.246122][-0.040580273 0.1098392 0.20914054 0.42154503 0.79039288 1.3257942 1.7865379 1.9614453 1.8848596 1.7283843 1.376529 0.756973 0.011661291 -0.66178977 -1.1261611][0.64127374 0.42365408 0.13805175 0.018021584 0.089731932 0.3056035 0.43846393 0.29052687 -0.015370131 -0.36930239 -0.776124 -1.3358489 -1.8748801 -2.2486093 -2.4292259][0.364429 -0.21596849 -0.7588644 -0.99253 -1.0329018 -1.0832758 -1.2829926 -1.6702598 -2.0917048 -2.4182336 -2.6567628 -2.9196389 -3.1439629 -3.2978809 -3.3707488][-0.64885032 -1.4192215 -1.9067116 -2.044765 -2.0628731 -2.2688468 -2.7126348 -3.1874526 -3.5412245 -3.6592481 -3.581203 -3.4279504 -3.3346875 -3.4032011 -3.5543427][-1.5508245 -2.2758205 -2.568126 -2.5274322 -2.4881797 -2.7447329 -3.2507839 -3.6736012 -3.8729916 -3.7750762 -3.453181 -3.0725899 -2.8640921 -2.9287996 -3.1213803][-2.14578 -2.5508735 -2.5453534 -2.3411775 -2.2690082 -2.5159838 -2.9450803 -3.2216272 -3.2032752 -2.9199252 -2.5433087 -2.2447612 -2.1639977 -2.2970817 -2.4918637][-2.2721989 -2.2939329 -2.0950048 -1.8220719 -1.7077603 -1.8322297 -2.070277 -2.1353898 -1.9227092 -1.5184991 -1.1769464 -1.1029401 -1.303772 -1.6110005 -1.8388048][-1.9360994 -1.7083526 -1.4437604 -1.2183874 -1.1383905 -1.1917574 -1.232862 -1.1196431 -0.8077507 -0.40239084 -0.19376707 -0.395046 -0.85071552 -1.2704213 -1.44828]]...]
INFO - root - 2017-12-16 08:12:36.221531: step 8410, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 15h:04m:27s remains)
INFO - root - 2017-12-16 08:12:37.884965: step 8420, loss = 0.57, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 14h:47m:09s remains)
INFO - root - 2017-12-16 08:12:39.584002: step 8430, loss = 0.61, batch loss = 0.35 (47.4 examples/sec; 0.169 sec/batch; 15h:11m:04s remains)
INFO - root - 2017-12-16 08:12:41.279935: step 8440, loss = 0.60, batch loss = 0.34 (47.7 examples/sec; 0.168 sec/batch; 15h:06m:02s remains)
INFO - root - 2017-12-16 08:12:42.976447: step 8450, loss = 0.61, batch loss = 0.34 (47.3 examples/sec; 0.169 sec/batch; 15h:12m:32s remains)
INFO - root - 2017-12-16 08:12:44.687787: step 8460, loss = 0.64, batch loss = 0.38 (47.3 examples/sec; 0.169 sec/batch; 15h:12m:42s remains)
INFO - root - 2017-12-16 08:12:46.407496: step 8470, loss = 0.59, batch loss = 0.33 (46.2 examples/sec; 0.173 sec/batch; 15h:35m:45s remains)
INFO - root - 2017-12-16 08:12:48.103591: step 8480, loss = 0.78, batch loss = 0.52 (47.6 examples/sec; 0.168 sec/batch; 15h:07m:46s remains)
INFO - root - 2017-12-16 08:12:49.762376: step 8490, loss = 0.71, batch loss = 0.45 (48.7 examples/sec; 0.164 sec/batch; 14h:47m:21s remains)
INFO - root - 2017-12-16 08:12:51.435775: step 8500, loss = 0.63, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:53m:26s remains)
2017-12-16 08:12:51.903272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3237255 -1.3350914 -1.3781657 -1.4930925 -1.7112812 -1.9921017 -2.3165989 -2.6684544 -2.9467654 -3.069459 -3.0492237 -2.8677838 -2.6197813 -2.4317436 -2.2542315][-1.4087026 -1.5317814 -1.7345896 -2.05333 -2.4728737 -2.9015694 -3.2616925 -3.60929 -3.8624251 -3.9098492 -3.7941623 -3.4782462 -3.1383493 -2.9328685 -2.7178752][-1.5906844 -1.8987551 -2.298887 -2.790283 -3.3075573 -3.6641574 -3.8023903 -3.88904 -3.9303136 -3.8487368 -3.67972 -3.4007268 -3.1529706 -3.106029 -2.9906015][-1.8917235 -2.4129596 -2.9122787 -3.3421221 -3.6557932 -3.6257191 -3.2828326 -2.9745128 -2.7471261 -2.6158564 -2.5572226 -2.4877553 -2.6015399 -2.9318027 -3.0720644][-2.2804797 -2.8981979 -3.3080442 -3.4206669 -3.1658318 -2.4734275 -1.4538511 -0.671772 -0.29729295 -0.31947517 -0.58031249 -0.968433 -1.6620129 -2.5812054 -3.0167582][-2.6371622 -3.1856585 -3.3178806 -2.8722966 -1.7967198 -0.31069946 1.3266914 2.5281575 2.9052393 2.4762771 1.5801127 0.55204487 -0.77951169 -2.1923461 -2.9353936][-2.8535762 -3.2600627 -3.0087335 -1.8778428 -0.032764673 2.1114628 4.2493172 5.7796717 5.8578777 4.5832615 2.862417 1.2447193 -0.47573936 -2.1261387 -2.9901309][-2.9205446 -3.2001359 -2.6228979 -0.9735465 1.3991523 3.8511779 6.2241793 8.0380793 6.9788761 4.6632309 2.4935372 0.67689705 -1.0672766 -2.5579159 -3.2191236][-2.8833911 -3.1107409 -2.4803352 -0.75168586 1.5923476 3.6660116 5.1298609 5.6677265 4.6294336 2.664943 0.77795434 -0.78432512 -2.1267722 -3.1545253 -3.4309692][-2.7873285 -3.0650742 -2.6532602 -1.3482085 0.36815 1.7133822 2.3477972 2.3577602 1.5602467 0.11719465 -1.2786365 -2.3502133 -3.1382685 -3.5882902 -3.4604449][-2.5795939 -2.9535613 -2.8124623 -2.0536292 -1.018628 -0.3469075 -0.1907382 -0.41651726 -1.0368611 -1.9909728 -2.8726778 -3.4740393 -3.771915 -3.7501383 -3.2917][-2.2441666 -2.6156051 -2.6780837 -2.326143 -1.8028901 -1.5486026 -1.701812 -2.0572002 -2.4900846 -3.0229528 -3.5089924 -3.7759852 -3.7853072 -3.4909339 -2.9106562][-1.817701 -2.1137185 -2.2089369 -2.0427005 -1.8073995 -1.8066716 -2.0782971 -2.4338119 -2.7072384 -2.9616182 -3.1806297 -3.2627149 -3.1802061 -2.8800189 -2.3770032][-1.4847744 -1.6461174 -1.7157278 -1.6222069 -1.5113813 -1.5466448 -1.7450459 -2.0033596 -2.1926558 -2.3462942 -2.4693379 -2.5012393 -2.4216371 -2.2288854 -1.9181993][-1.2937024 -1.3546237 -1.3671516 -1.2843466 -1.2025118 -1.2181883 -1.3475499 -1.5207101 -1.65107 -1.7559919 -1.8562659 -1.8833652 -1.8480631 -1.7712412 -1.619328]]...]
INFO - root - 2017-12-16 08:12:53.576061: step 8510, loss = 0.68, batch loss = 0.41 (45.6 examples/sec; 0.175 sec/batch; 15h:46m:38s remains)
INFO - root - 2017-12-16 08:12:55.265396: step 8520, loss = 0.50, batch loss = 0.24 (47.1 examples/sec; 0.170 sec/batch; 15h:17m:25s remains)
INFO - root - 2017-12-16 08:12:56.929563: step 8530, loss = 0.57, batch loss = 0.31 (48.8 examples/sec; 0.164 sec/batch; 14h:45m:44s remains)
INFO - root - 2017-12-16 08:12:58.599442: step 8540, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 15h:00m:43s remains)
INFO - root - 2017-12-16 08:13:00.337739: step 8550, loss = 0.73, batch loss = 0.46 (46.9 examples/sec; 0.171 sec/batch; 15h:20m:46s remains)
INFO - root - 2017-12-16 08:13:02.020055: step 8560, loss = 0.67, batch loss = 0.41 (48.7 examples/sec; 0.164 sec/batch; 14h:46m:14s remains)
INFO - root - 2017-12-16 08:13:03.705568: step 8570, loss = 0.66, batch loss = 0.40 (46.8 examples/sec; 0.171 sec/batch; 15h:23m:10s remains)
INFO - root - 2017-12-16 08:13:05.390334: step 8580, loss = 0.57, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 15h:18m:01s remains)
INFO - root - 2017-12-16 08:13:07.102360: step 8590, loss = 0.58, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 15h:26m:35s remains)
INFO - root - 2017-12-16 08:13:08.838058: step 8600, loss = 0.53, batch loss = 0.27 (43.3 examples/sec; 0.185 sec/batch; 16h:36m:41s remains)
2017-12-16 08:13:09.319098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2824669 -3.1362481 -2.927958 -2.7910893 -2.6269186 -2.1453221 -1.3335176 -0.63008106 -0.61289907 -1.2736731 -2.4013271 -3.4757075 -3.9535546 -3.7874281 -3.3830554][-2.9842768 -2.383846 -1.8908119 -1.7723247 -1.8294599 -1.5257738 -0.72050655 -0.0018687248 -0.025772095 -0.81946313 -2.0957322 -3.1949778 -3.6522141 -3.4778128 -3.0622413][-2.1986117 -0.9815836 -0.13220525 -0.10208154 -0.57291579 -0.6884743 -0.13702941 0.45142865 0.33609772 -0.55656767 -1.8527733 -2.8510606 -3.2227547 -3.0488539 -2.6119196][-1.2955209 0.48824286 1.646975 1.5122447 0.55272865 -0.060601711 0.12077332 0.52104235 0.44050193 -0.33569753 -1.4600899 -2.31533 -2.6432462 -2.5419936 -2.1497748][-0.66943872 1.1755271 2.2460456 1.9650574 0.88003612 0.12893987 0.17155957 0.5754087 0.654778 0.10199785 -0.87136161 -1.6419046 -2.0156138 -2.0590141 -1.8031588][-0.60633814 0.78371596 1.5574963 1.2893109 0.536366 0.176373 0.56655693 1.248981 1.5032759 0.92186832 -0.16009974 -1.06849 -1.5554528 -1.7314243 -1.6385608][-1.1354114 -0.4217267 0.014067888 -0.0830152 -0.258556 0.16123009 1.2814732 2.4804468 2.8786383 2.0086794 0.51516747 -0.71663404 -1.3561704 -1.5955751 -1.6414649][-1.8099563 -1.7114441 -1.5573215 -1.5056567 -1.1542443 -0.052855253 1.7218626 3.3854342 3.8817167 2.7093968 0.86463785 -0.60603786 -1.2943695 -1.532204 -1.6644801][-2.2498217 -2.434792 -2.3747902 -2.2032475 -1.6763639 -0.47379947 1.2963905 2.7726984 3.0710387 2.0552301 0.4977653 -0.69941556 -1.248682 -1.4022908 -1.539489][-2.4923015 -2.6258509 -2.4819536 -2.2386096 -1.7792546 -0.87896144 0.36093163 1.3453605 1.5093729 0.78987288 -0.24563122 -0.97772777 -1.2247262 -1.2131505 -1.2306942][-2.5098479 -2.376513 -2.0217249 -1.7280251 -1.5401919 -1.2454709 -0.72346485 -0.25035989 -0.11398315 -0.45338857 -0.9565264 -1.1939644 -1.1525596 -1.0373111 -0.96027553][-2.2695084 -1.8540241 -1.2478788 -0.93521678 -1.0764017 -1.3618568 -1.4524236 -1.3551457 -1.2995827 -1.4374368 -1.5755732 -1.4827321 -1.2004904 -0.95982349 -0.80205441][-1.9618406 -1.3326733 -0.59120905 -0.33329439 -0.71148729 -1.3095753 -1.6573803 -1.7036388 -1.6773717 -1.7279741 -1.743516 -1.5758522 -1.2959049 -1.0579559 -0.8241055][-1.60179 -0.90189254 -0.24258745 -0.14504528 -0.62832808 -1.2196844 -1.4942851 -1.4199331 -1.252318 -1.1363603 -1.1071734 -1.0962571 -1.0868545 -1.037109 -0.83261538][-1.2653 -0.6298933 -0.19105983 -0.30657887 -0.79806447 -1.1986883 -1.185128 -0.80589151 -0.27037477 0.15692282 0.24000549 -0.050594568 -0.52386034 -0.83255816 -0.7922045]]...]
INFO - root - 2017-12-16 08:13:11.045038: step 8610, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 15h:11m:24s remains)
INFO - root - 2017-12-16 08:13:12.736297: step 8620, loss = 0.65, batch loss = 0.38 (49.4 examples/sec; 0.162 sec/batch; 14h:34m:48s remains)
INFO - root - 2017-12-16 08:13:14.390743: step 8630, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 15h:01m:20s remains)
INFO - root - 2017-12-16 08:13:16.074090: step 8640, loss = 0.56, batch loss = 0.29 (49.0 examples/sec; 0.163 sec/batch; 14h:41m:29s remains)
INFO - root - 2017-12-16 08:13:17.757232: step 8650, loss = 0.60, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 14h:57m:46s remains)
INFO - root - 2017-12-16 08:13:19.457842: step 8660, loss = 0.58, batch loss = 0.32 (45.5 examples/sec; 0.176 sec/batch; 15h:48m:37s remains)
INFO - root - 2017-12-16 08:13:21.122042: step 8670, loss = 0.56, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 14h:56m:05s remains)
INFO - root - 2017-12-16 08:13:22.773531: step 8680, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 15h:17m:29s remains)
INFO - root - 2017-12-16 08:13:24.507319: step 8690, loss = 0.57, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 15h:23m:24s remains)
INFO - root - 2017-12-16 08:13:26.182426: step 8700, loss = 0.61, batch loss = 0.35 (45.7 examples/sec; 0.175 sec/batch; 15h:44m:21s remains)
2017-12-16 08:13:26.670704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4432013 -1.4224728 -1.5523421 -1.7041973 -1.696167 -1.4839381 -1.2487239 -1.2273388 -1.4813433 -1.9324753 -2.357235 -2.5819473 -2.4478836 -1.9979531 -1.4161694][-1.7315495 -1.7542653 -2.0108268 -2.288805 -2.3104424 -2.0666244 -1.770215 -1.6297624 -1.7524953 -2.1174927 -2.5777617 -2.9246731 -2.900646 -2.5005574 -1.9325571][-2.1142378 -2.1615045 -2.5040081 -2.8908162 -2.9590971 -2.7250991 -2.4066546 -2.1897631 -2.1910906 -2.4690537 -2.9168682 -3.3224669 -3.352479 -2.9462297 -2.3486824][-2.3308747 -2.344399 -2.6915236 -3.0952649 -3.1907735 -2.9639773 -2.6157892 -2.3370118 -2.2692909 -2.5037079 -2.9566703 -3.4160161 -3.4869354 -3.0728333 -2.4483616][-2.2222152 -2.168735 -2.4356065 -2.8046045 -2.9167783 -2.6792436 -2.2486441 -1.8396707 -1.653549 -1.8700241 -2.3920677 -2.9411628 -3.0947454 -2.7274446 -2.1582296][-1.8183326 -1.7275503 -1.8923557 -2.1500061 -2.1545272 -1.821981 -1.2560382 -0.651507 -0.29984117 -0.48173475 -1.1667163 -1.9296683 -2.2445061 -1.9687347 -1.4871294][-1.4155927 -1.3313222 -1.3590944 -1.3716068 -1.1566045 -0.67850828 0.015674829 0.8358686 1.4314525 1.3115509 0.37179494 -0.72913814 -1.2881477 -1.1330186 -0.74066246][-1.2850248 -1.2218392 -1.0898795 -0.85918975 -0.46954811 0.091852427 0.84866285 1.8369009 2.6664431 2.6172521 1.3764801 -0.06815052 -0.89736533 -0.89577425 -0.58920217][-1.5535271 -1.596801 -1.3870018 -1.0256904 -0.55986285 -0.030193329 0.64415669 1.5218341 2.282037 2.213495 0.95185542 -0.55723536 -1.5357912 -1.6476835 -1.3116522][-2.0142031 -2.1831896 -1.9889083 -1.6004305 -1.1687582 -0.7866776 -0.34728789 0.22798347 0.74242735 0.64214444 -0.35411012 -1.6466714 -2.5829504 -2.6792469 -2.2184153][-2.1245029 -2.3718 -2.2807412 -1.988402 -1.6859884 -1.4663203 -1.2214223 -0.87410986 -0.53596151 -0.602556 -1.2995896 -2.2883148 -3.0767591 -3.1354866 -2.6200111][-1.8498261 -2.0635459 -2.0360925 -1.85474 -1.7037756 -1.6342375 -1.527595 -1.295608 -1.0194855 -1.0180801 -1.469172 -2.2151332 -2.8715427 -2.9329841 -2.4754632][-1.4701107 -1.5395026 -1.4998953 -1.3938694 -1.3642561 -1.4156241 -1.3894246 -1.1781417 -0.89072335 -0.84717166 -1.1797537 -1.7583588 -2.2284944 -2.2361989 -1.8832538][-1.3527157 -1.2704517 -1.1742648 -1.0875219 -1.1040218 -1.1930585 -1.174188 -0.93759251 -0.65982068 -0.639838 -0.92404258 -1.3398356 -1.5676868 -1.451061 -1.1619762][-1.4935412 -1.3881056 -1.2996869 -1.2410171 -1.2388601 -1.2653193 -1.182587 -0.90672815 -0.65152526 -0.68524444 -0.93705428 -1.1708177 -1.123011 -0.80581176 -0.50333774]]...]
INFO - root - 2017-12-16 08:13:28.350765: step 8710, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 15h:04m:56s remains)
INFO - root - 2017-12-16 08:13:30.043128: step 8720, loss = 0.59, batch loss = 0.33 (48.6 examples/sec; 0.165 sec/batch; 14h:49m:00s remains)
INFO - root - 2017-12-16 08:13:31.704769: step 8730, loss = 0.53, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 15h:12m:18s remains)
INFO - root - 2017-12-16 08:13:33.379191: step 8740, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:50m:46s remains)
INFO - root - 2017-12-16 08:13:35.050976: step 8750, loss = 0.53, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 15h:05m:14s remains)
INFO - root - 2017-12-16 08:13:36.729486: step 8760, loss = 0.58, batch loss = 0.32 (45.7 examples/sec; 0.175 sec/batch; 15h:44m:29s remains)
INFO - root - 2017-12-16 08:13:38.436667: step 8770, loss = 0.62, batch loss = 0.36 (47.1 examples/sec; 0.170 sec/batch; 15h:16m:09s remains)
INFO - root - 2017-12-16 08:13:40.124302: step 8780, loss = 0.64, batch loss = 0.37 (46.8 examples/sec; 0.171 sec/batch; 15h:22m:07s remains)
INFO - root - 2017-12-16 08:13:41.852526: step 8790, loss = 0.68, batch loss = 0.42 (47.0 examples/sec; 0.170 sec/batch; 15h:18m:31s remains)
INFO - root - 2017-12-16 08:13:43.531106: step 8800, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 15h:08m:12s remains)
2017-12-16 08:13:44.001149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8883588 -1.6137745 -1.4528527 -1.6992122 -2.2151456 -2.7145309 -3.0554259 -3.1219606 -3.0434597 -2.9596093 -2.9162517 -2.8020678 -2.5570717 -2.1282313 -1.5844356][-1.8406906 -1.626657 -1.6289489 -1.9568683 -2.4317625 -2.852962 -3.0917044 -3.0673652 -2.8908668 -2.7223272 -2.6063645 -2.443675 -2.1924171 -1.8322932 -1.418216][-1.553831 -1.5170622 -1.7966144 -2.2793531 -2.7867215 -3.1070762 -3.1373682 -2.9016633 -2.5786681 -2.3800566 -2.2923369 -2.1912363 -2.0350423 -1.7897146 -1.4857597][-0.95318568 -1.0798998 -1.6198729 -2.3290925 -2.9082985 -3.1280513 -2.8926435 -2.3667026 -1.952371 -1.7650225 -1.820588 -1.8942611 -1.9559723 -1.8410308 -1.5664718][-0.29426265 -0.44513381 -1.0705165 -1.9166384 -2.528348 -2.6388817 -2.2125895 -1.5815129 -1.2273088 -1.2349094 -1.5207832 -1.8123816 -2.0357237 -1.9713378 -1.6230813][0.18784213 0.1120069 -0.44403279 -1.2457469 -1.7602445 -1.7796162 -1.3607947 -0.90568113 -0.78905284 -1.0581568 -1.5462811 -1.9607959 -2.2263174 -2.1486125 -1.712148][0.27455568 0.33674026 -0.070005894 -0.63109958 -0.9157728 -0.79387963 -0.53189123 -0.45484245 -0.68936288 -1.1459875 -1.697964 -2.1501138 -2.3505838 -2.179431 -1.752353][0.14945388 0.30203581 0.042347193 -0.23467124 -0.25159168 -0.069783926 -0.022427082 -0.30821979 -0.80421817 -1.3508925 -1.8391116 -2.1859527 -2.26697 -2.0246882 -1.6436024][-0.093597651 0.01723671 -0.13759041 -0.17303944 -0.020596981 0.11642218 -0.031118155 -0.50500667 -1.0477939 -1.5307162 -1.8928453 -2.1042562 -2.0331752 -1.7574339 -1.4484071][-0.41876376 -0.43730187 -0.56283879 -0.45722008 -0.21160805 -0.074023962 -0.2752918 -0.75904238 -1.2974335 -1.7131332 -1.9433174 -1.9807252 -1.7826881 -1.4861468 -1.2715478][-0.6941247 -0.83590949 -0.96813762 -0.83521914 -0.51518452 -0.31473827 -0.45003867 -0.848776 -1.3475174 -1.742749 -1.881372 -1.7886698 -1.5434388 -1.3063675 -1.2186422][-0.821306 -1.0248107 -1.1734657 -1.0433813 -0.70330751 -0.45712113 -0.50599766 -0.8213433 -1.2416029 -1.5934818 -1.7141426 -1.6282023 -1.4449427 -1.3038278 -1.324358][-0.866361 -1.0444862 -1.153859 -0.99975872 -0.67922342 -0.44034612 -0.46117735 -0.7690208 -1.1324542 -1.4602072 -1.5909333 -1.5454118 -1.4434991 -1.3851287 -1.4834276][-0.87177014 -0.95755315 -0.98484612 -0.80109334 -0.53811204 -0.39308429 -0.46799278 -0.827108 -1.2301985 -1.5480433 -1.6471372 -1.5877647 -1.4806137 -1.444653 -1.5966016][-0.83873212 -0.80720079 -0.70881116 -0.51919127 -0.38896525 -0.40499449 -0.60569441 -1.067571 -1.5125387 -1.728714 -1.7071031 -1.5735391 -1.4158905 -1.3884294 -1.5976121]]...]
INFO - root - 2017-12-16 08:13:45.679954: step 8810, loss = 0.54, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 15h:07m:19s remains)
INFO - root - 2017-12-16 08:13:47.352715: step 8820, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 15h:10m:45s remains)
INFO - root - 2017-12-16 08:13:49.035522: step 8830, loss = 0.67, batch loss = 0.41 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:44s remains)
INFO - root - 2017-12-16 08:13:50.724834: step 8840, loss = 0.54, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 15h:01m:26s remains)
INFO - root - 2017-12-16 08:13:52.376091: step 8850, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:49m:59s remains)
INFO - root - 2017-12-16 08:13:54.037802: step 8860, loss = 0.79, batch loss = 0.52 (47.0 examples/sec; 0.170 sec/batch; 15h:17m:33s remains)
INFO - root - 2017-12-16 08:13:55.718737: step 8870, loss = 0.51, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:54m:29s remains)
INFO - root - 2017-12-16 08:13:57.411897: step 8880, loss = 0.63, batch loss = 0.36 (47.2 examples/sec; 0.169 sec/batch; 15h:14m:12s remains)
INFO - root - 2017-12-16 08:13:59.076084: step 8890, loss = 0.61, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 15h:10m:00s remains)
INFO - root - 2017-12-16 08:14:00.770128: step 8900, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 15h:00m:16s remains)
2017-12-16 08:14:01.228922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35574722 -0.30286193 -0.59803164 -0.73604131 -0.87891781 -0.996078 -1.0740751 -1.1905812 -1.4735388 -1.8367695 -2.1650522 -2.3294315 -2.3495607 -2.2405603 -1.9871244][0.66583276 0.12355375 -0.11018491 -0.21540558 -0.36332726 -0.50732243 -0.57716167 -0.64524579 -0.89183855 -1.272167 -1.7150923 -2.0878835 -2.2823784 -2.2877192 -2.0526044][0.76783872 0.4142592 0.28852153 0.23374677 0.10731411 0.058341503 0.16806531 0.26320124 0.03210783 -0.36816144 -0.91259706 -1.4294128 -1.7664628 -1.9316654 -1.8165747][0.87064719 0.71786737 0.70240974 0.69754004 0.680315 0.83723664 1.1615081 1.4369812 1.2245677 0.71549034 0.071144342 -0.52036357 -0.98104405 -1.3238827 -1.3921871][0.74636912 0.82683706 0.96694613 1.0916128 1.2353189 1.5960143 2.0731525 2.4268937 2.2032323 1.6152987 0.97774839 0.43330121 -0.097737074 -0.64909303 -0.90679729][0.44136071 0.68934941 0.98425794 1.2480252 1.5915534 2.1438355 2.7713847 3.216969 2.9650774 2.3041902 1.7064087 1.2632504 0.73905587 0.060204744 -0.42130065][0.21645761 0.46888876 0.77383161 1.0929368 1.5816603 2.3131509 3.1233263 3.7475286 3.4833212 2.8136697 2.2855158 1.9188733 1.4479256 0.732738 0.088521242][-0.0065696239 0.0914979 0.25203967 0.52893662 0.99002075 1.6847615 2.4760718 3.1465607 3.1118708 2.7651968 2.4685631 2.1488023 1.7472544 1.0984161 0.44499803][-0.26291811 -0.35828769 -0.38349402 -0.24810469 0.043909311 0.52805233 1.1193469 1.6762223 1.8961778 1.9632397 1.9801064 1.8200464 1.5265403 1.0433609 0.54518819][-0.48412859 -0.72237635 -0.89413416 -0.8965255 -0.79027355 -0.53206921 -0.15069461 0.33279371 0.72569513 1.0560007 1.236227 1.1091247 0.90251517 0.65392637 0.42367506][-0.607383 -0.92639053 -1.1649555 -1.2512327 -1.2488997 -1.1823572 -1.02729 -0.68894422 -0.27226603 0.19494653 0.45922375 0.38099623 0.2566812 0.20500684 0.22430515][-0.61388695 -0.90491271 -1.1135432 -1.1877137 -1.2258662 -1.283041 -1.3294951 -1.1924878 -0.8992219 -0.47135127 -0.15948105 -0.17163944 -0.22853327 -0.18960595 -0.029827595][-0.63090253 -0.77383316 -0.82467175 -0.75831616 -0.71609414 -0.79619014 -0.96529245 -1.0524021 -0.98136926 -0.74454463 -0.54362369 -0.57611609 -0.59109068 -0.494035 -0.28439057][-0.75987732 -0.73299503 -0.55383885 -0.27974498 -0.089780092 -0.097435951 -0.29872668 -0.55835783 -0.70472527 -0.70109224 -0.700477 -0.8086158 -0.83241594 -0.72792256 -0.50173211][-1.0336939 -0.9618206 -0.66048384 -0.20577002 0.13784218 0.22567749 0.031396151 -0.31449211 -0.60355306 -0.76171196 -0.90057147 -1.0336877 -1.0484657 -0.94020951 -0.74860585]]...]
INFO - root - 2017-12-16 08:14:02.888090: step 8910, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.165 sec/batch; 14h:52m:30s remains)
INFO - root - 2017-12-16 08:14:04.563750: step 8920, loss = 0.69, batch loss = 0.43 (48.7 examples/sec; 0.164 sec/batch; 14h:45m:25s remains)
INFO - root - 2017-12-16 08:14:06.234224: step 8930, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:49m:50s remains)
INFO - root - 2017-12-16 08:14:07.942388: step 8940, loss = 0.65, batch loss = 0.38 (47.3 examples/sec; 0.169 sec/batch; 15h:12m:11s remains)
INFO - root - 2017-12-16 08:14:09.601003: step 8950, loss = 0.72, batch loss = 0.46 (47.9 examples/sec; 0.167 sec/batch; 15h:00m:57s remains)
INFO - root - 2017-12-16 08:14:11.266652: step 8960, loss = 0.60, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:56m:05s remains)
INFO - root - 2017-12-16 08:14:12.955523: step 8970, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 14h:59m:04s remains)
INFO - root - 2017-12-16 08:14:14.630744: step 8980, loss = 0.51, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 14h:45m:20s remains)
INFO - root - 2017-12-16 08:14:16.315434: step 8990, loss = 0.62, batch loss = 0.36 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:38s remains)
INFO - root - 2017-12-16 08:14:18.004821: step 9000, loss = 0.58, batch loss = 0.32 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:05s remains)
2017-12-16 08:14:18.533847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2385225 -2.3925872 -2.6589923 -2.8336327 -2.6099491 -2.0742962 -1.6323562 -1.3113282 -1.2030498 -1.3933568 -1.8052059 -2.3120513 -2.5525334 -2.2543864 -1.7127624][-3.6223664 -3.6430745 -3.8456197 -4.0328856 -3.8035061 -3.1838679 -2.5422714 -2.191746 -2.2584553 -2.4945524 -2.788708 -3.3032584 -3.6020391 -3.2348661 -2.5977178][-4.3060946 -4.1755233 -4.3672762 -4.5954833 -4.4417734 -3.8222461 -3.0351405 -2.6015184 -2.7574296 -2.9962375 -3.2657032 -3.7988367 -4.1309462 -3.7693241 -2.9983904][-3.5951762 -3.3361382 -3.5356336 -4.0151291 -4.1443763 -3.5834923 -2.6241109 -2.0381749 -2.2192452 -2.5484338 -2.9274492 -3.5879426 -4.0668807 -3.7702332 -2.9267554][-1.8199228 -1.2581499 -1.4750497 -2.2919786 -2.7770445 -2.2772226 -1.163661 -0.45054138 -0.69986582 -1.2744017 -1.8879064 -2.7054803 -3.391367 -3.2950811 -2.5005202][0.44486332 1.46801 1.286886 0.08714056 -0.77669835 -0.27082562 1.1092322 2.0404713 1.6754184 0.73294139 -0.29056013 -1.474036 -2.4738226 -2.7148781 -2.1566012][2.135411 3.57736 3.4918835 2.0660717 1.060811 1.7294583 3.5068758 4.7122889 4.1195831 2.6378229 1.0918519 -0.51017785 -1.8726524 -2.4471307 -2.0862689][2.150357 3.662235 3.6980355 2.5895169 1.9191487 2.8739011 5.0745296 6.502574 5.3937206 3.3119166 1.361815 -0.44160807 -1.9347854 -2.6175196 -2.3425021][0.618413 1.9484956 2.1063011 1.4541397 1.1314065 2.1138294 4.0712328 5.2228889 4.2583265 2.3321469 0.46320367 -1.2151482 -2.5178275 -3.088002 -2.8014417][-1.3423302 -0.37648833 -0.21129084 -0.520442 -0.56168759 0.30537295 1.8403933 2.7404435 2.0626824 0.57506871 -0.97367632 -2.3468642 -3.2973642 -3.5962639 -3.2801604][-2.9980903 -2.4436407 -2.3372178 -2.4629452 -2.3612103 -1.6611934 -0.53301859 0.10209703 -0.34493959 -1.3804876 -2.4926255 -3.4954042 -4.0472631 -4.0724955 -3.6873724][-3.9820061 -3.7283216 -3.7152469 -3.82346 -3.7072902 -3.208456 -2.4616711 -2.0872386 -2.4074936 -3.097625 -3.8131018 -4.3731289 -4.5154662 -4.2723961 -3.81258][-4.0276675 -3.9523587 -3.9902148 -4.1118746 -4.0840292 -3.8277564 -3.4348543 -3.2876577 -3.5473719 -3.9898334 -4.3754683 -4.5338039 -4.36623 -3.9648986 -3.4914076][-3.6050148 -3.4914937 -3.4512324 -3.4986277 -3.4736714 -3.3462496 -3.1968882 -3.2268591 -3.4887953 -3.7960684 -3.9769573 -3.948462 -3.6686134 -3.2748828 -2.8932273][-3.1943705 -2.9554167 -2.792695 -2.6800678 -2.5049865 -2.3160222 -2.216331 -2.3021324 -2.5499606 -2.7786176 -2.8552895 -2.7500951 -2.5243564 -2.2753806 -2.0286458]]...]
INFO - root - 2017-12-16 08:14:20.223034: step 9010, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 14h:44m:59s remains)
INFO - root - 2017-12-16 08:14:21.871392: step 9020, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 14h:57m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:14:23.542545: step 9030, loss = 0.49, batch loss = 0.23 (45.7 examples/sec; 0.175 sec/batch; 15h:43m:11s remains)
INFO - root - 2017-12-16 08:14:25.225012: step 9040, loss = 0.59, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 15h:00m:27s remains)
INFO - root - 2017-12-16 08:14:26.895580: step 9050, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 14h:54m:17s remains)
INFO - root - 2017-12-16 08:14:28.561031: step 9060, loss = 0.62, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:34s remains)
INFO - root - 2017-12-16 08:14:30.231200: step 9070, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.162 sec/batch; 14h:35m:40s remains)
INFO - root - 2017-12-16 08:14:31.899983: step 9080, loss = 0.54, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:11s remains)
INFO - root - 2017-12-16 08:14:33.553384: step 9090, loss = 0.54, batch loss = 0.28 (48.6 examples/sec; 0.164 sec/batch; 14h:46m:27s remains)
INFO - root - 2017-12-16 08:14:35.231820: step 9100, loss = 0.59, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 15h:04m:58s remains)
2017-12-16 08:14:35.691092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3674314 -2.2979159 -2.0138249 -1.7576752 -1.6788025 -1.6526651 -1.3937287 -1.131979 -1.1296798 -1.4456561 -1.8359957 -2.1285248 -2.2262838 -2.1537855 -2.0473926][-2.0711989 -2.0515394 -1.9440845 -1.8786973 -1.9024189 -1.9276338 -1.7897624 -1.6641016 -1.8031141 -2.2664497 -2.8712683 -3.3260307 -3.4564042 -3.2778807 -3.0410702][-1.7144238 -1.7485571 -1.8051894 -1.9232619 -2.0629315 -2.1147773 -1.9902806 -1.9214444 -2.1391826 -2.7160761 -3.3891182 -3.9166746 -4.1116295 -3.9271588 -3.7102][-1.4037304 -1.4266689 -1.5482347 -1.7636287 -1.929915 -1.9555849 -1.7740852 -1.6961824 -1.9374341 -2.5573466 -3.185014 -3.631278 -3.8428779 -3.7310925 -3.6148558][-0.88233662 -0.85307741 -0.98890233 -1.1851065 -1.2979767 -1.2539558 -1.0122548 -0.91203 -1.120629 -1.7433169 -2.3558481 -2.7253733 -2.8786819 -2.8516049 -2.8015344][-0.16723228 -0.010307312 -0.084402561 -0.29422092 -0.38361335 -0.27597082 0.016474962 0.15134549 -0.050743341 -0.64390087 -1.2594606 -1.6375828 -1.7993517 -1.8642645 -1.8768202][0.35746503 0.64466286 0.59470344 0.35062575 0.24231195 0.43000579 0.895437 1.2191973 1.059808 0.46503997 -0.16681194 -0.58643711 -0.84356248 -1.0377662 -1.1660703][0.78334069 0.95515704 0.750849 0.3247292 0.13419008 0.41999006 1.1572905 1.8431704 1.8661363 1.3773234 0.78539491 0.31706715 -0.074872255 -0.45144641 -0.72032654][1.027765 0.94111943 0.49138284 -0.074374437 -0.33003676 -0.04172945 0.77618718 1.6143148 1.8836353 1.649931 1.2176135 0.74706483 0.2844274 -0.22255039 -0.57183564][0.55190229 0.31934404 -0.16209316 -0.64836776 -0.75713336 -0.3765341 0.45774484 1.2970908 1.6575999 1.5690272 1.1821775 0.66308427 0.10926795 -0.42225766 -0.75616229][-0.57591355 -0.75041735 -1.08685 -1.3905976 -1.3542297 -0.95458269 -0.24532175 0.42994308 0.76632595 0.74697304 0.44035482 -0.023317099 -0.52389765 -0.8929621 -1.0576882][-1.8878335 -1.9010653 -2.0781851 -2.2302155 -2.1375806 -1.8025314 -1.3123667 -0.90921843 -0.726573 -0.74215496 -0.92529392 -1.1983083 -1.4591511 -1.5242412 -1.4575711][-2.9025257 -2.8381629 -2.8714907 -2.9090693 -2.806643 -2.5930669 -2.3398614 -2.1672502 -2.135602 -2.2339094 -2.3703146 -2.4866335 -2.506108 -2.3094501 -2.0495009][-3.4671831 -3.366549 -3.3284879 -3.3227944 -3.2582211 -3.1779869 -3.1307192 -3.1419289 -3.2212088 -3.3491046 -3.4485826 -3.4614542 -3.3126392 -2.9982924 -2.6780069][-3.3849616 -3.3257165 -3.3136215 -3.3179312 -3.2900081 -3.2776458 -3.3249364 -3.4329097 -3.56638 -3.689364 -3.7455587 -3.6753559 -3.4589925 -3.1311917 -2.8213398]]...]
INFO - root - 2017-12-16 08:14:37.349332: step 9110, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:42m:38s remains)
INFO - root - 2017-12-16 08:14:39.001323: step 9120, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 15h:03m:26s remains)
INFO - root - 2017-12-16 08:14:40.694994: step 9130, loss = 0.55, batch loss = 0.28 (46.5 examples/sec; 0.172 sec/batch; 15h:26m:46s remains)
INFO - root - 2017-12-16 08:14:42.372671: step 9140, loss = 0.61, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 14h:53m:00s remains)
INFO - root - 2017-12-16 08:14:44.052328: step 9150, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 14h:52m:44s remains)
INFO - root - 2017-12-16 08:14:45.688054: step 9160, loss = 0.64, batch loss = 0.38 (48.9 examples/sec; 0.164 sec/batch; 14h:41m:27s remains)
INFO - root - 2017-12-16 08:14:47.335783: step 9170, loss = 0.60, batch loss = 0.33 (48.4 examples/sec; 0.165 sec/batch; 14h:51m:00s remains)
INFO - root - 2017-12-16 08:14:49.016221: step 9180, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:50m:52s remains)
INFO - root - 2017-12-16 08:14:50.687519: step 9190, loss = 0.67, batch loss = 0.40 (48.3 examples/sec; 0.165 sec/batch; 14h:51m:47s remains)
INFO - root - 2017-12-16 08:14:52.360707: step 9200, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 15h:07m:06s remains)
2017-12-16 08:14:52.891885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2829194 -2.0873697 -1.8919811 -1.6888882 -1.4959861 -1.4641912 -1.5844197 -1.6743839 -1.6355975 -1.5912315 -1.7400503 -1.9802 -2.1466498 -2.2088852 -2.1839421][-2.60805 -2.4236419 -2.2413082 -2.0274212 -1.8152615 -1.6913815 -1.6207741 -1.5117979 -1.3611242 -1.3119948 -1.4911585 -1.7225953 -1.841357 -1.8204194 -1.7791721][-2.9283767 -2.7549982 -2.5838594 -2.3662622 -2.1295226 -1.9632593 -1.7980011 -1.6258771 -1.4950385 -1.5392065 -1.7485673 -1.931703 -1.9485875 -1.8348895 -1.7757162][-3.0746727 -2.9341114 -2.8071313 -2.620846 -2.392504 -2.2192755 -2.0336695 -1.8237745 -1.7048557 -1.7922306 -2.02371 -2.1852815 -2.16235 -1.997836 -1.8803829][-2.9822264 -2.9142046 -2.8736622 -2.7235024 -2.4821892 -2.2261257 -1.9071741 -1.55307 -1.3389218 -1.3876617 -1.6291201 -1.8316827 -1.85247 -1.6934788 -1.5674449][-2.7242577 -2.7051511 -2.7066555 -2.5529377 -2.2320974 -1.7797933 -1.1975951 -0.58586657 -0.14297175 -0.091478586 -0.38602936 -0.70504332 -0.834733 -0.76997232 -0.74780428][-2.3225384 -2.2820785 -2.2218738 -1.9974413 -1.5390902 -0.87897956 -0.024038553 0.86469865 1.4928515 1.5050471 0.982913 0.50542879 0.27565122 0.23986363 0.077008963][-1.8419771 -1.767162 -1.6372094 -1.3159542 -0.75890338 -0.0046856403 0.96071339 2.0166883 2.6196132 2.318399 1.5562091 0.94486141 0.65201807 0.51924968 0.12800026][-1.4869167 -1.4550669 -1.3101048 -0.963629 -0.44576526 0.1981585 1.0124683 1.821806 2.1927938 1.8544769 1.1703849 0.62397361 0.33800936 0.065932035 -0.41897571][-1.4709096 -1.4779022 -1.3501642 -1.0594587 -0.692407 -0.26774073 0.28194451 0.83390927 1.1365852 0.97519684 0.48183155 0.036315918 -0.27097321 -0.60969508 -1.065992][-1.5843548 -1.5763298 -1.4553597 -1.2272005 -0.98762858 -0.73952365 -0.41842496 -0.031958342 0.22777128 0.1697433 -0.17364359 -0.58341169 -0.9173671 -1.2749225 -1.6884327][-1.361123 -1.3134212 -1.1934855 -0.99986219 -0.82928109 -0.68395936 -0.502079 -0.24982238 -0.10197353 -0.19793987 -0.56871223 -0.99861681 -1.3797597 -1.760684 -2.1341493][-0.83207285 -0.79538572 -0.70205605 -0.55279648 -0.43487382 -0.31954288 -0.15226912 0.034422874 0.087230921 -0.13133979 -0.61770809 -1.1506672 -1.6566278 -2.1145115 -2.4665043][-0.45799792 -0.4648236 -0.4101584 -0.30706644 -0.23893917 -0.12904358 0.073751211 0.25018835 0.24633169 -0.046137333 -0.58908725 -1.2232337 -1.857133 -2.3808143 -2.6826527][-0.59694588 -0.61823511 -0.58576834 -0.52740538 -0.50534058 -0.38972032 -0.14178705 0.085479975 0.11906147 -0.10811377 -0.59416187 -1.2509729 -1.9628587 -2.5024786 -2.7535076]]...]
INFO - root - 2017-12-16 08:14:54.589730: step 9210, loss = 0.63, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 14h:48m:04s remains)
INFO - root - 2017-12-16 08:14:56.265105: step 9220, loss = 0.60, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 15h:09m:30s remains)
INFO - root - 2017-12-16 08:14:57.921441: step 9230, loss = 0.57, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 15h:16m:20s remains)
INFO - root - 2017-12-16 08:14:59.616907: step 9240, loss = 0.64, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 14h:53m:19s remains)
INFO - root - 2017-12-16 08:15:01.292352: step 9250, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 14h:46m:38s remains)
INFO - root - 2017-12-16 08:15:02.989290: step 9260, loss = 0.56, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 14h:36m:30s remains)
INFO - root - 2017-12-16 08:15:04.678873: step 9270, loss = 0.55, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 15h:08m:57s remains)
INFO - root - 2017-12-16 08:15:06.337326: step 9280, loss = 0.53, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 15h:04m:31s remains)
INFO - root - 2017-12-16 08:15:08.015024: step 9290, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 14h:48m:37s remains)
INFO - root - 2017-12-16 08:15:09.727178: step 9300, loss = 0.70, batch loss = 0.43 (47.6 examples/sec; 0.168 sec/batch; 15h:05m:21s remains)
2017-12-16 08:15:10.215904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.70110905 -0.40309191 -0.18706346 0.31416798 0.91081548 0.95136523 0.94452929 1.2271707 1.1515059 0.72658205 0.87307954 1.2900214 0.98277521 0.23858547 -0.19984841][-0.694487 -0.434911 -0.3096261 0.042696238 0.41156936 0.24114585 0.07374382 0.29199648 0.36017561 0.18558621 0.52780437 1.0625966 0.86655331 0.27363086 -0.04605484][-0.61765254 -0.49035156 -0.58832824 -0.44167614 -0.36312902 -0.82375228 -1.2363776 -1.173066 -1.0901244 -1.1043419 -0.660841 -0.15203881 -0.26546812 -0.63081717 -0.71560109][-0.34641767 -0.38472378 -0.691851 -0.75015414 -0.92348862 -1.6414647 -2.2045655 -2.1993618 -2.1169293 -2.1032588 -1.7620828 -1.4857632 -1.7302783 -1.9708161 -1.8859773][0.02195406 -0.077898741 -0.39008772 -0.46374381 -0.72693741 -1.4812677 -1.9400545 -1.8073522 -1.7630951 -1.9101443 -1.9022427 -2.0506151 -2.5653648 -2.8806379 -2.8431382][0.18066502 0.17954922 0.066409111 0.17080021 0.062278032 -0.46421182 -0.48685074 0.029350519 0.080944777 -0.38696706 -0.79054117 -1.3389449 -2.2105529 -2.7983866 -2.9661043][0.11987138 0.27589059 0.38509226 0.77839017 0.94312906 0.76664686 1.2329805 2.2043803 2.3452299 1.5472085 0.8072257 -0.0054249763 -1.1363178 -1.9767672 -2.350666][-0.096901894 0.13865805 0.43010521 1.0488169 1.4206576 1.5108213 2.2691448 3.4568503 3.6728909 2.7893064 1.9559472 1.069792 -0.11789846 -1.0038172 -1.4959686][-0.32440364 -0.13382339 0.15731716 0.71203804 1.0060585 1.0944293 1.7956727 2.8039234 2.9706376 2.387593 1.9015357 1.2806749 0.35424852 -0.33526886 -0.775036][-0.4508518 -0.52884817 -0.44794691 -0.2222631 -0.29701674 -0.42980039 -0.0042665005 0.68424368 0.83473086 0.68688464 0.63908839 0.38886046 -0.050265312 -0.33372474 -0.53182018][-0.51777518 -0.8953141 -1.1011665 -1.2846327 -1.7267089 -2.0722792 -2.0075505 -1.7569516 -1.640177 -1.4476542 -1.1635675 -1.0849397 -1.0028707 -0.84009564 -0.75310659][-0.42828333 -0.98643231 -1.3776234 -1.8327608 -2.5051866 -2.9533811 -3.1837344 -3.3281984 -3.3034234 -2.9985623 -2.6201088 -2.3392327 -1.8620617 -1.3804258 -1.1644924][-0.15931773 -0.72293079 -1.1379211 -1.7044184 -2.4252474 -2.9113953 -3.2498794 -3.5666814 -3.6216929 -3.3313665 -3.0818081 -2.7775567 -2.056227 -1.4669061 -1.3169484][0.13739491 -0.31684995 -0.69652259 -1.2340062 -1.9108169 -2.4073546 -2.7512569 -2.9982288 -2.9897428 -2.8102283 -2.7460246 -2.4592209 -1.6260495 -0.97965729 -0.93117368][0.69746256 0.36449409 0.0027701855 -0.51862073 -1.1167539 -1.6007636 -1.9256916 -1.9961195 -1.8694998 -1.8258369 -1.95432 -1.6628393 -0.7291038 -0.070931673 -0.054059505]]...]
INFO - root - 2017-12-16 08:15:11.876507: step 9310, loss = 0.60, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 15h:10m:08s remains)
INFO - root - 2017-12-16 08:15:13.593390: step 9320, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:56m:00s remains)
INFO - root - 2017-12-16 08:15:15.278433: step 9330, loss = 0.57, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 14h:44m:25s remains)
INFO - root - 2017-12-16 08:15:16.962381: step 9340, loss = 0.60, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 14h:46m:58s remains)
INFO - root - 2017-12-16 08:15:18.657050: step 9350, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 15h:05m:10s remains)
INFO - root - 2017-12-16 08:15:20.337958: step 9360, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:53m:38s remains)
INFO - root - 2017-12-16 08:15:22.004307: step 9370, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 15h:14m:46s remains)
INFO - root - 2017-12-16 08:15:23.689847: step 9380, loss = 0.51, batch loss = 0.25 (45.8 examples/sec; 0.175 sec/batch; 15h:40m:22s remains)
INFO - root - 2017-12-16 08:15:25.345137: step 9390, loss = 0.58, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 14h:52m:56s remains)
INFO - root - 2017-12-16 08:15:27.015851: step 9400, loss = 0.71, batch loss = 0.45 (47.7 examples/sec; 0.168 sec/batch; 15h:02m:25s remains)
2017-12-16 08:15:27.484260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.049334 -2.912082 -2.395555 -1.6519668 -0.934785 -0.37046969 -0.2330873 -0.43749189 -1.0363553 -1.6865396 -2.1734369 -2.6287863 -2.9342995 -3.1151946 -3.1643331][-3.1902471 -3.0017853 -2.5204067 -1.7893143 -1.0616367 -0.51334369 -0.26512671 -0.33291233 -0.85630763 -1.5020013 -2.0016458 -2.546473 -2.9344316 -3.1964843 -3.2880883][-3.214417 -3.0149109 -2.5875053 -1.9060972 -1.1947523 -0.60031378 -0.25797236 -0.19931149 -0.57966554 -1.1270437 -1.6809063 -2.3761563 -2.8636777 -3.204308 -3.3234916][-3.1305554 -2.8998778 -2.4869242 -1.8191895 -1.1021342 -0.48409212 0.007212162 0.21239328 -0.010822296 -0.50353169 -1.154007 -2.046963 -2.6964061 -3.1094646 -3.2813749][-2.8644555 -2.5457931 -2.1620116 -1.5333369 -0.79966307 -0.16928577 0.47130203 0.82932758 0.71535397 0.21662402 -0.54128277 -1.6496871 -2.4935508 -3.0072784 -3.1965013][-2.4009449 -2.0277307 -1.7015127 -1.1812613 -0.49257028 0.25083804 1.0543275 1.6094854 1.5224445 0.94882965 0.044201612 -1.2709954 -2.3026905 -2.9142148 -3.1106846][-1.726774 -1.3665433 -1.1898931 -0.80970383 -0.16857743 0.72821355 1.8059294 2.6369565 2.5324175 1.8228247 0.68858719 -0.85621548 -2.1021183 -2.8020453 -3.002075][-1.0021082 -0.69255364 -0.67439544 -0.47431958 0.12747192 1.2104506 2.556649 3.6659663 3.4726436 2.5806267 1.1688769 -0.55690014 -1.9283602 -2.7012527 -2.9024038][-0.5286274 -0.27726221 -0.41109979 -0.36851609 0.13002849 1.1894929 2.5735843 3.8785865 3.7015283 2.7064722 1.1848207 -0.57437932 -1.910084 -2.6604128 -2.8264661][-0.41429913 -0.30314934 -0.59961617 -0.66706765 -0.2955507 0.62394714 1.9090455 3.2188509 3.1889517 2.226445 0.75387335 -0.87075448 -2.0397596 -2.6631975 -2.769444][-0.60733008 -0.6825695 -1.0636152 -1.1855325 -0.90656495 -0.10130477 1.0325749 2.1257851 2.1489303 1.303643 0.050286531 -1.3022463 -2.2271445 -2.6823058 -2.7110932][-0.96089792 -1.1904589 -1.5945275 -1.7487261 -1.5209823 -0.82321417 0.067435741 0.82200623 0.79128551 0.13306975 -0.80225933 -1.8009889 -2.4444604 -2.6978524 -2.6480927][-1.399969 -1.6584914 -2.042917 -2.2187254 -2.0570631 -1.5816758 -1.016921 -0.593711 -0.67713726 -1.1029004 -1.6969793 -2.3035758 -2.6445632 -2.6970882 -2.5703394][-1.8196208 -2.0447283 -2.3508079 -2.5029488 -2.437032 -2.2103286 -1.9073272 -1.6846848 -1.7484305 -2.0025041 -2.3246922 -2.6277788 -2.7494676 -2.6615915 -2.4768584][-2.0834651 -2.2375007 -2.4355607 -2.5677171 -2.5908713 -2.5214291 -2.3752193 -2.2186363 -2.2312512 -2.3700023 -2.540452 -2.6734636 -2.6792052 -2.5391963 -2.3516777]]...]
INFO - root - 2017-12-16 08:15:29.130138: step 9410, loss = 0.65, batch loss = 0.39 (47.7 examples/sec; 0.168 sec/batch; 15h:03m:01s remains)
INFO - root - 2017-12-16 08:15:30.792162: step 9420, loss = 0.74, batch loss = 0.47 (48.5 examples/sec; 0.165 sec/batch; 14h:48m:45s remains)
INFO - root - 2017-12-16 08:15:32.455009: step 9430, loss = 0.60, batch loss = 0.33 (46.6 examples/sec; 0.172 sec/batch; 15h:23m:51s remains)
INFO - root - 2017-12-16 08:15:34.115464: step 9440, loss = 0.52, batch loss = 0.25 (46.5 examples/sec; 0.172 sec/batch; 15h:26m:42s remains)
INFO - root - 2017-12-16 08:15:35.793977: step 9450, loss = 0.56, batch loss = 0.30 (47.2 examples/sec; 0.170 sec/batch; 15h:13m:20s remains)
INFO - root - 2017-12-16 08:15:37.501632: step 9460, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:56m:19s remains)
INFO - root - 2017-12-16 08:15:39.191839: step 9470, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.165 sec/batch; 14h:50m:53s remains)
INFO - root - 2017-12-16 08:15:40.876586: step 9480, loss = 0.66, batch loss = 0.39 (48.0 examples/sec; 0.167 sec/batch; 14h:56m:46s remains)
INFO - root - 2017-12-16 08:15:42.563535: step 9490, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 15h:03m:40s remains)
INFO - root - 2017-12-16 08:15:44.208625: step 9500, loss = 0.62, batch loss = 0.36 (49.9 examples/sec; 0.160 sec/batch; 14h:22m:23s remains)
2017-12-16 08:15:44.706052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7022221 -3.3553998 -4.1742134 -4.5285726 -4.2192588 -3.2643423 -2.1290643 -1.2590426 -1.884002 -2.9900866 -3.5603428 -3.507834 -3.1524332 -2.8552918 -2.8820214][-2.5739107 -3.1424294 -3.8598056 -4.1668491 -3.7459619 -2.6032984 -1.2872024 -0.330109 -1.1918392 -2.5905509 -3.31817 -3.3325868 -3.0350845 -2.8137937 -2.9506257][-2.2765419 -2.6745949 -3.2240651 -3.3892438 -2.870847 -1.627739 -0.21795297 0.77827191 -0.27635288 -1.8853983 -2.7009602 -2.7308805 -2.4966772 -2.429312 -2.688633][-1.9592056 -2.1656675 -2.5371509 -2.5905731 -1.9582546 -0.6629492 0.9033587 2.055305 0.8842845 -0.89757228 -1.8295901 -1.9501121 -1.8372314 -1.9239147 -2.313174][-1.6580192 -1.7600229 -2.0425906 -2.0440845 -1.323702 0.12706208 1.9728031 3.4068789 2.2020807 0.2401464 -0.82360923 -1.0901742 -1.1871418 -1.5051374 -1.9562819][-1.4650271 -1.5116843 -1.7767949 -1.7663512 -0.91363561 0.71739292 2.9250565 4.797543 3.6133504 1.4222019 0.13020682 -0.32520294 -0.70902717 -1.3359144 -1.8587117][-1.3847126 -1.4547125 -1.7057114 -1.616946 -0.68974745 1.0956907 3.7413321 6.1647935 4.9368162 2.4374013 0.8243072 0.15160275 -0.45143223 -1.2992623 -1.8455949][-1.2449753 -1.3561304 -1.5493705 -1.3575841 -0.45697272 1.3336165 4.1091681 6.7178178 5.4503264 2.7979612 1.0131912 0.26576138 -0.3619796 -1.177083 -1.6360412][-1.1430961 -1.2551156 -1.3725728 -1.0946082 -0.32908869 1.2660346 3.7447968 5.9274077 4.8676534 2.504034 0.83987 0.16403961 -0.31052494 -0.92629206 -1.2246143][-1.1778525 -1.2840915 -1.4342422 -1.2253721 -0.64898086 0.64671159 2.600132 4.2861924 3.4797635 1.5595198 0.16904593 -0.27022398 -0.48336709 -0.85127258 -0.97543645][-1.3791549 -1.5654655 -1.8263631 -1.7299494 -1.3108213 -0.34542692 1.1007018 2.3772898 1.767344 0.2297473 -0.82343459 -1.0751964 -1.0632725 -1.1938072 -1.1975967][-1.737735 -2.0450313 -2.4075003 -2.4572613 -2.204278 -1.5616677 -0.537011 0.38727856 -0.053793669 -1.1697712 -1.8903757 -1.9893136 -1.8528773 -1.8211105 -1.754144][-2.0229144 -2.4054406 -2.8589003 -3.0829885 -3.0414276 -2.6920111 -2.0745666 -1.4987131 -1.74179 -2.4193878 -2.8426375 -2.8465106 -2.6461706 -2.521697 -2.4113622][-2.1220579 -2.5067945 -2.9827456 -3.3048553 -3.3809342 -3.2567887 -2.9374473 -2.6430979 -2.7486875 -3.0809517 -3.2855291 -3.2494245 -3.0923388 -2.9870634 -2.9051592][-2.1136868 -2.4281125 -2.8169675 -3.1020021 -3.2176943 -3.2026114 -3.0336268 -2.8881073 -2.9250984 -3.0658307 -3.1531928 -3.1265998 -3.0422583 -2.9791496 -2.9306412]]...]
INFO - root - 2017-12-16 08:15:46.385058: step 9510, loss = 0.69, batch loss = 0.42 (48.1 examples/sec; 0.166 sec/batch; 14h:55m:47s remains)
INFO - root - 2017-12-16 08:15:48.045985: step 9520, loss = 0.73, batch loss = 0.47 (48.9 examples/sec; 0.164 sec/batch; 14h:40m:24s remains)
INFO - root - 2017-12-16 08:15:49.713302: step 9530, loss = 0.53, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 15h:08m:16s remains)
INFO - root - 2017-12-16 08:15:51.358475: step 9540, loss = 0.75, batch loss = 0.49 (48.0 examples/sec; 0.167 sec/batch; 14h:56m:14s remains)
INFO - root - 2017-12-16 08:15:53.056745: step 9550, loss = 0.57, batch loss = 0.30 (46.2 examples/sec; 0.173 sec/batch; 15h:32m:47s remains)
INFO - root - 2017-12-16 08:15:54.776641: step 9560, loss = 0.59, batch loss = 0.33 (45.8 examples/sec; 0.175 sec/batch; 15h:39m:59s remains)
INFO - root - 2017-12-16 08:15:56.461701: step 9570, loss = 0.65, batch loss = 0.39 (48.6 examples/sec; 0.165 sec/batch; 14h:46m:37s remains)
INFO - root - 2017-12-16 08:15:58.166358: step 9580, loss = 0.58, batch loss = 0.32 (49.1 examples/sec; 0.163 sec/batch; 14h:36m:50s remains)
INFO - root - 2017-12-16 08:15:59.846643: step 9590, loss = 0.72, batch loss = 0.46 (48.5 examples/sec; 0.165 sec/batch; 14h:47m:24s remains)
INFO - root - 2017-12-16 08:16:01.536352: step 9600, loss = 0.56, batch loss = 0.30 (44.4 examples/sec; 0.180 sec/batch; 16h:09m:01s remains)
2017-12-16 08:16:02.029642: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2165217 0.33149171 -0.69207919 -1.6613138 -2.5719452 -3.44032 -4.1292562 -4.5152545 -4.581758 -4.2790442 -3.5941117 -2.4990485 -1.2821722 -0.5226351 -0.39856696][-0.17193866 -1.0692898 -1.97486 -2.7069662 -3.2582352 -3.705205 -4.1078124 -4.5536003 -4.9544568 -5.0193524 -4.5322313 -3.5081217 -2.2507415 -1.3832865 -1.1191888][-1.8892493 -2.6264882 -3.2807143 -3.6992576 -3.7534966 -3.595325 -3.5470457 -3.893712 -4.6204572 -5.2201395 -5.2801714 -4.67472 -3.6451705 -2.7636793 -2.3645458][-3.3863096 -3.9143839 -4.2967839 -4.392487 -3.9700356 -3.155498 -2.52445 -2.5510039 -3.4434357 -4.581594 -5.327981 -5.317606 -4.7564688 -4.1354175 -3.716][-3.8285985 -4.2206964 -4.4432421 -4.2796736 -3.4458089 -2.082026 -0.77618146 -0.3419621 -1.2973793 -2.9847999 -4.4806581 -5.1366062 -5.053009 -4.8077 -4.5336509][-3.1486876 -3.4473615 -3.6064696 -3.348393 -2.2886534 -0.39113116 1.7087047 2.6921144 1.6188674 -0.70908535 -2.9651322 -4.1944456 -4.4517136 -4.4229994 -4.3812065][-2.1118739 -2.3004756 -2.3612673 -2.0311735 -0.80792153 1.4159629 4.1579914 5.7142234 4.4243016 1.5414736 -1.2430298 -2.7366679 -3.0576975 -3.1501632 -3.3658602][-1.2711083 -1.220853 -1.0911576 -0.63361168 0.63215876 2.8499546 5.5804644 7.2070856 5.9648943 3.0556636 0.31739879 -1.1126438 -1.3805561 -1.5633147 -2.0478234][-0.86245883 -0.5685426 -0.24427843 0.30448723 1.4222369 3.1444755 5.1887527 6.2240782 5.262495 3.0673375 1.0424037 -0.000626564 -0.1951983 -0.39691734 -1.0465367][-1.0224838 -0.789387 -0.46999907 0.0082564354 0.78117251 1.7941055 2.9130054 3.352519 2.7689519 1.4656851 0.26428127 -0.29754734 -0.3360666 -0.46497369 -1.0921956][-1.6753759 -1.6724415 -1.5061111 -1.1379718 -0.61202812 -0.18652654 0.15598083 0.18096328 -0.16498661 -0.80575156 -1.4664075 -1.7869163 -1.6744001 -1.6074488 -2.007988][-2.3218207 -2.539247 -2.5275123 -2.2209358 -1.9021139 -1.9153024 -2.0926321 -2.3392754 -2.5604289 -2.8258333 -3.1497865 -3.3304694 -3.1467962 -2.8568156 -2.8812542][-2.6043589 -2.917114 -3.0269573 -2.8600707 -2.7576318 -2.9861624 -3.3717222 -3.7040906 -3.8774133 -3.9186597 -4.0016913 -4.0800509 -3.8443003 -3.423389 -3.1419888][-2.4698794 -2.7587018 -2.8821707 -2.8650527 -2.9202375 -3.1968668 -3.5758321 -3.8289456 -3.8918483 -3.8279827 -3.787693 -3.7670979 -3.5306606 -3.1662765 -2.818944][-2.0945904 -2.3034654 -2.408778 -2.4298186 -2.4952443 -2.6630485 -2.8830895 -3.0183089 -3.0158741 -2.9230807 -2.8746414 -2.8591752 -2.7186251 -2.4920793 -2.2553031]]...]
INFO - root - 2017-12-16 08:16:03.718447: step 9610, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:59m:00s remains)
INFO - root - 2017-12-16 08:16:05.382214: step 9620, loss = 0.58, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 15h:14m:06s remains)
INFO - root - 2017-12-16 08:16:07.046951: step 9630, loss = 0.57, batch loss = 0.31 (47.2 examples/sec; 0.170 sec/batch; 15h:12m:18s remains)
INFO - root - 2017-12-16 08:16:08.718619: step 9640, loss = 0.64, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 14h:57m:35s remains)
INFO - root - 2017-12-16 08:16:10.388455: step 9650, loss = 0.50, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 14h:37m:45s remains)
INFO - root - 2017-12-16 08:16:12.076469: step 9660, loss = 0.47, batch loss = 0.21 (47.8 examples/sec; 0.167 sec/batch; 15h:00m:14s remains)
INFO - root - 2017-12-16 08:16:13.754914: step 9670, loss = 0.60, batch loss = 0.34 (47.7 examples/sec; 0.168 sec/batch; 15h:02m:04s remains)
INFO - root - 2017-12-16 08:16:15.428175: step 9680, loss = 0.67, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 14h:58m:31s remains)
INFO - root - 2017-12-16 08:16:17.091887: step 9690, loss = 0.58, batch loss = 0.32 (48.8 examples/sec; 0.164 sec/batch; 14h:41m:56s remains)
INFO - root - 2017-12-16 08:16:18.743955: step 9700, loss = 0.49, batch loss = 0.23 (47.2 examples/sec; 0.169 sec/batch; 15h:11m:54s remains)
2017-12-16 08:16:19.270626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3362327 -2.1033578 -1.6550617 -1.5150712 -1.9884636 -2.5517743 -2.75776 -2.6865616 -2.4849958 -2.2554297 -1.6693392 -0.86922252 -0.51572585 -0.61437237 -0.54853117][-2.2801235 -2.2505543 -2.0312953 -2.1039243 -2.7471356 -3.464745 -3.7793722 -3.7586236 -3.5287485 -3.1964223 -2.601867 -1.930233 -1.6946647 -1.7991596 -1.607991][-2.0231662 -2.0799751 -1.9920906 -2.1394525 -2.8020575 -3.5674524 -3.992208 -4.0769262 -3.9504933 -3.6782513 -3.2850945 -2.9272757 -2.929431 -3.1042504 -2.8871403][-1.5361006 -1.580735 -1.4929703 -1.5594863 -2.1036668 -2.7800908 -3.2028346 -3.3361225 -3.335639 -3.2511768 -3.1309137 -3.1230006 -3.3654861 -3.68393 -3.596616][-1.1967481 -1.1527416 -0.89380038 -0.72332156 -1.0060159 -1.5302925 -1.8966029 -1.9777973 -1.9391704 -1.9247376 -2.0370641 -2.3602495 -2.8740954 -3.3591471 -3.450053][-1.1190588 -1.0358338 -0.61599219 -0.21413994 -0.25751352 -0.61172974 -0.816074 -0.71021938 -0.49887204 -0.44276786 -0.65111697 -1.1337126 -1.7383804 -2.3000495 -2.5225234][-1.307827 -1.2740853 -0.82215095 -0.33008063 -0.25701904 -0.4406271 -0.48759866 -0.2168138 0.14785337 0.29218674 0.1013732 -0.34314156 -0.83613348 -1.2428328 -1.3816485][-1.5364001 -1.6573508 -1.3375304 -0.98991978 -0.96214795 -1.0613179 -0.99772573 -0.65584159 -0.23071933 0.027489185 -0.036625147 -0.31901014 -0.56609225 -0.65998983 -0.520331][-1.3837686 -1.5841402 -1.4535801 -1.3767329 -1.5351591 -1.7066631 -1.7049167 -1.4460229 -1.0645784 -0.73470509 -0.60338831 -0.6307596 -0.59805107 -0.40543747 -0.056879044][-0.777696 -0.97486424 -1.0238638 -1.308264 -1.8016859 -2.1751096 -2.3570144 -2.2504978 -1.9022267 -1.45476 -1.0439256 -0.74931705 -0.47410357 -0.1677444 0.16432047][-0.22097111 -0.32971871 -0.50596654 -1.0764859 -1.8317665 -2.4105988 -2.7910233 -2.8483651 -2.5423915 -1.9645326 -1.2904887 -0.71064568 -0.28870928 -0.050003052 0.01206708][0.05327487 0.14170909 -0.02909708 -0.712682 -1.6096251 -2.299746 -2.8020561 -3.0435059 -2.8675635 -2.3169382 -1.5878012 -0.95163751 -0.57328284 -0.48019683 -0.64309943][0.13370705 0.39805031 0.29792523 -0.33678365 -1.1659038 -1.8756084 -2.5239518 -3.0018156 -3.0706692 -2.7488718 -2.209146 -1.6821072 -1.3811768 -1.3399568 -1.5426704][0.04214263 0.3716414 0.32035184 -0.25074697 -0.9549371 -1.6249077 -2.3751826 -3.0351906 -3.3702147 -3.356812 -3.0907633 -2.716316 -2.4027259 -2.2701719 -2.2951527][-0.050194025 0.22033024 0.10052323 -0.47900248 -1.07858 -1.6613976 -2.3988585 -3.1064618 -3.5615981 -3.7916527 -3.7612271 -3.4815798 -3.10534 -2.8462188 -2.6124763]]...]
INFO - root - 2017-12-16 08:16:20.988439: step 9710, loss = 0.56, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 14h:41m:29s remains)
INFO - root - 2017-12-16 08:16:22.686567: step 9720, loss = 0.59, batch loss = 0.32 (46.0 examples/sec; 0.174 sec/batch; 15h:35m:13s remains)
INFO - root - 2017-12-16 08:16:24.389330: step 9730, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:42m:18s remains)
INFO - root - 2017-12-16 08:16:26.097564: step 9740, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 15h:07m:04s remains)
INFO - root - 2017-12-16 08:16:27.805377: step 9750, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 14h:44m:03s remains)
INFO - root - 2017-12-16 08:16:29.485374: step 9760, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:57m:22s remains)
INFO - root - 2017-12-16 08:16:31.179361: step 9770, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:51m:36s remains)
INFO - root - 2017-12-16 08:16:32.889314: step 9780, loss = 0.48, batch loss = 0.22 (46.6 examples/sec; 0.172 sec/batch; 15h:23m:36s remains)
INFO - root - 2017-12-16 08:16:34.565470: step 9790, loss = 0.63, batch loss = 0.37 (46.8 examples/sec; 0.171 sec/batch; 15h:19m:35s remains)
INFO - root - 2017-12-16 08:16:36.268069: step 9800, loss = 0.57, batch loss = 0.31 (46.7 examples/sec; 0.171 sec/batch; 15h:22m:07s remains)
2017-12-16 08:16:36.726354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.3755163 -0.62412012 -0.76365387 -0.91291416 -0.90009058 -0.80488527 -0.68260252 -0.81611454 -1.2864878 -1.8502939 -2.5101519 -2.8754344 -2.7051117 -2.6470222 -2.9213824][-0.271307 -0.7358433 -1.1486955 -1.4794436 -1.5959463 -1.3813171 -0.8045435 -0.547999 -0.900264 -1.6334188 -2.4971139 -2.9739885 -2.8123851 -2.7552924 -2.9910049][-0.63759625 -1.2770027 -1.8415647 -2.2603743 -2.3468077 -1.8022372 -0.67325664 0.079918861 -0.2164197 -1.2239202 -2.3677785 -2.983274 -2.9219184 -2.9165523 -3.1091685][-1.2750313 -2.0761647 -2.6011529 -2.8832004 -2.7355359 -1.676435 0.10918379 1.3291831 0.88620019 -0.5119257 -1.982758 -2.837414 -2.9381027 -3.0297542 -3.196409][-1.9075395 -2.7856345 -3.2121024 -3.2981691 -2.8088298 -1.1406636 1.2729273 2.9078329 2.1804364 0.21397066 -1.6666884 -2.7357895 -2.953737 -3.0709467 -3.2039328][-2.4315536 -3.2830575 -3.5890675 -3.4714348 -2.595741 -0.41458595 2.5339148 4.4759493 3.4283035 0.900517 -1.3718886 -2.6114502 -2.959517 -3.1015949 -3.1824002][-2.8740346 -3.619534 -3.807488 -3.4790149 -2.243897 0.41599202 3.9844878 6.3163061 4.7422152 1.5931211 -1.0299212 -2.4630015 -2.959702 -3.1920304 -3.2359738][-3.2195892 -3.7739606 -3.8881168 -3.4478545 -2.0563231 0.86700583 4.9027109 7.5768204 5.5073032 1.9047143 -0.94708204 -2.4772441 -3.0548792 -3.3756862 -3.3009155][-3.244329 -3.6911941 -3.8588881 -3.4892118 -2.2296517 0.4691627 4.1284418 6.2572556 4.7399712 1.5305445 -1.1376389 -2.6094904 -3.1991181 -3.4626615 -3.217803][-2.8481989 -3.2603736 -3.5686891 -3.3807871 -2.3707807 -0.27898622 2.6124227 4.3797989 3.4386756 0.90048528 -1.3443508 -2.63556 -3.1584687 -3.3489521 -2.9888837][-2.0091951 -2.4698982 -2.9919543 -3.0603073 -2.4989631 -1.1117842 0.98437262 2.3957098 1.7852633 -0.13935065 -1.8411158 -2.76924 -3.1425469 -3.2391415 -2.8580194][-1.3099718 -1.7672122 -2.5056472 -2.8738985 -2.7126441 -1.9717391 -0.58849609 0.43884611 0.00054788589 -1.3996427 -2.5269182 -3.0210395 -3.215528 -3.2410576 -2.8735936][-1.211041 -1.611443 -2.3182781 -2.83851 -3.0288997 -2.7942498 -1.9617838 -1.1846993 -1.4739087 -2.4718661 -3.1371233 -3.2488737 -3.3246295 -3.3646762 -3.0169921][-1.6597494 -1.896262 -2.3916166 -2.8754058 -3.2129879 -3.259095 -2.7857761 -2.1455133 -2.3336535 -3.0736794 -3.5323098 -3.5782471 -3.6138978 -3.6002817 -3.2177384][-2.2426288 -2.3306735 -2.6262343 -2.9597087 -3.2718539 -3.4064789 -3.0864174 -2.5837748 -2.67437 -3.2179871 -3.6202137 -3.7394447 -3.7346811 -3.6276226 -3.2642179]]...]
INFO - root - 2017-12-16 08:16:38.409340: step 9810, loss = 0.61, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 14h:56m:27s remains)
INFO - root - 2017-12-16 08:16:40.086746: step 9820, loss = 0.57, batch loss = 0.30 (49.6 examples/sec; 0.161 sec/batch; 14h:26m:45s remains)
INFO - root - 2017-12-16 08:16:41.720683: step 9830, loss = 0.56, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 14h:46m:33s remains)
INFO - root - 2017-12-16 08:16:43.403889: step 9840, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 15h:10m:52s remains)
INFO - root - 2017-12-16 08:16:45.093375: step 9850, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 14h:56m:33s remains)
INFO - root - 2017-12-16 08:16:46.809136: step 9860, loss = 0.63, batch loss = 0.37 (45.5 examples/sec; 0.176 sec/batch; 15h:45m:02s remains)
INFO - root - 2017-12-16 08:16:48.483083: step 9870, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 14h:53m:16s remains)
INFO - root - 2017-12-16 08:16:50.145810: step 9880, loss = 0.48, batch loss = 0.21 (47.3 examples/sec; 0.169 sec/batch; 15h:09m:14s remains)
INFO - root - 2017-12-16 08:16:51.819936: step 9890, loss = 0.68, batch loss = 0.42 (47.8 examples/sec; 0.167 sec/batch; 15h:00m:06s remains)
INFO - root - 2017-12-16 08:16:53.488872: step 9900, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 14h:56m:13s remains)
2017-12-16 08:16:53.964499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1812735 -2.9927127 -2.7944741 -2.74325 -2.6741915 -2.5557783 -2.5112355 -2.6406097 -2.6076341 -2.3894579 -2.2212069 -2.0994308 -2.1888444 -2.4699016 -2.7079005][-2.376663 -2.1460135 -1.9488811 -1.9343151 -1.7502495 -1.5548919 -1.6089159 -1.8650289 -2.0222723 -1.9260392 -1.811282 -1.681545 -1.69275 -1.9853225 -2.4099793][-1.2227743 -1.0522186 -0.935552 -1.0707195 -0.90105593 -0.68679762 -0.83221221 -1.1505995 -1.3680048 -1.2941313 -1.1420127 -0.89670265 -0.74052358 -0.96120548 -1.451673][0.0085494518 0.058991432 0.0571239 -0.20164394 -0.082068682 0.052449942 -0.23452044 -0.55721414 -0.68838406 -0.45971823 -0.12728834 0.2228055 0.50867271 0.45446205 -0.046523094][0.877022 0.80819511 0.76612234 0.68986988 0.85069108 0.89450312 0.52711987 0.30339861 0.33616567 0.63246775 1.1001964 1.5571849 1.8269439 1.7935929 1.2425156][0.87995911 0.95804667 1.1499758 1.3966653 1.7753186 1.8652906 1.5008621 1.3141711 1.2677159 1.3737748 1.8071566 2.2933416 2.4713435 2.3964319 1.9463539][0.40156984 0.55630231 0.96162271 1.5275071 2.1808753 2.5118933 2.3386483 2.0562229 1.6995194 1.4596293 1.7227697 2.0685511 2.1050835 1.9976258 1.7270379][-0.37986422 -0.22419572 0.22870564 0.93564534 1.7850242 2.3998356 2.5412526 2.2490768 1.5504372 1.0045815 0.97153306 1.0273807 0.84407496 0.73670959 0.68007779][-1.2621914 -1.1827493 -0.76166034 -0.088798761 0.66744208 1.2664006 1.5182514 1.2762606 0.56817651 -0.00090885162 -0.18445206 -0.30115819 -0.5812459 -0.73703647 -0.67359889][-1.9786679 -1.9425216 -1.682019 -1.2638372 -0.74738717 -0.33037496 -0.14064741 -0.31403744 -0.79683459 -1.2281584 -1.4012768 -1.5074801 -1.6934712 -1.7981706 -1.7175456][-2.3519385 -2.3688464 -2.3259478 -2.1474662 -1.8291619 -1.6232369 -1.564348 -1.6726704 -1.9461012 -2.1494029 -2.2399058 -2.2984288 -2.400126 -2.4425046 -2.3762176][-2.2798166 -2.3753591 -2.5079975 -2.5254972 -2.3917704 -2.3426237 -2.4108927 -2.5058587 -2.5940044 -2.5696895 -2.539535 -2.5310259 -2.5103993 -2.4391072 -2.3414462][-1.9212271 -2.0728519 -2.2658706 -2.4044 -2.4101753 -2.4458017 -2.5473528 -2.6184692 -2.5832181 -2.4363298 -2.2931879 -2.168067 -2.0630691 -1.9283532 -1.7825941][-1.470124 -1.6294186 -1.8244565 -1.9732072 -2.0273302 -2.0512145 -2.0910478 -2.1196623 -2.0757053 -1.927577 -1.7537329 -1.6028383 -1.4735079 -1.3369364 -1.1936172][-1.1306821 -1.2564217 -1.4172425 -1.5361437 -1.5769372 -1.5570986 -1.5256518 -1.5049853 -1.4667606 -1.3719749 -1.2488245 -1.1359112 -1.0430286 -0.96553588 -0.88074374]]...]
INFO - root - 2017-12-16 08:16:55.637112: step 9910, loss = 0.64, batch loss = 0.38 (47.6 examples/sec; 0.168 sec/batch; 15h:04m:29s remains)
INFO - root - 2017-12-16 08:16:57.307703: step 9920, loss = 0.73, batch loss = 0.46 (48.7 examples/sec; 0.164 sec/batch; 14h:42m:24s remains)
INFO - root - 2017-12-16 08:16:58.982993: step 9930, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 14h:43m:06s remains)
INFO - root - 2017-12-16 08:17:00.652414: step 9940, loss = 0.71, batch loss = 0.45 (46.9 examples/sec; 0.171 sec/batch; 15h:16m:57s remains)
INFO - root - 2017-12-16 08:17:02.330494: step 9950, loss = 0.66, batch loss = 0.40 (47.7 examples/sec; 0.168 sec/batch; 15h:00m:49s remains)
INFO - root - 2017-12-16 08:17:04.042220: step 9960, loss = 0.61, batch loss = 0.35 (46.9 examples/sec; 0.171 sec/batch; 15h:17m:24s remains)
INFO - root - 2017-12-16 08:17:05.721208: step 9970, loss = 0.54, batch loss = 0.27 (46.5 examples/sec; 0.172 sec/batch; 15h:25m:37s remains)
INFO - root - 2017-12-16 08:17:07.413975: step 9980, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 14h:36m:09s remains)
INFO - root - 2017-12-16 08:17:09.107039: step 9990, loss = 0.57, batch loss = 0.31 (49.3 examples/sec; 0.162 sec/batch; 14h:31m:36s remains)
INFO - root - 2017-12-16 08:17:10.759103: step 10000, loss = 0.59, batch loss = 0.33 (48.8 examples/sec; 0.164 sec/batch; 14h:40m:15s remains)
2017-12-16 08:17:11.198595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0921949 -0.97176981 -0.6763854 -0.44899893 -0.575354 -0.93523467 -1.2665993 -1.4047608 -1.3938601 -1.3761268 -1.4931829 -1.6628182 -1.7418948 -1.7386398 -1.7440355][-0.44581938 -0.51576769 -0.48853147 -0.54748285 -0.91158593 -1.4843985 -1.9019897 -2.0516346 -2.0666759 -2.0905411 -2.217135 -2.3582661 -2.3779991 -2.2791269 -2.1453657][0.11519337 -0.076861858 -0.23474765 -0.50424135 -1.0475094 -1.7605572 -2.2691023 -2.5026283 -2.6192923 -2.7150679 -2.8315229 -2.8944895 -2.8098438 -2.60227 -2.344806][0.59714913 0.37224841 0.080765963 -0.36454356 -0.99486291 -1.6832618 -2.1819253 -2.4897032 -2.7374694 -2.9326136 -3.0399711 -3.0341272 -2.9022441 -2.6813841 -2.4266851][0.9058218 0.72489452 0.35313296 -0.21653485 -0.81293547 -1.2259945 -1.4691969 -1.763512 -2.1778722 -2.5110188 -2.6369481 -2.6247778 -2.5467179 -2.462007 -2.3535912][0.96498489 0.93431377 0.61359668 0.092569828 -0.30525339 -0.29799092 -0.17714858 -0.42975819 -1.0343063 -1.548779 -1.7139788 -1.7111378 -1.7066309 -1.7924787 -1.9085996][0.55239892 0.74948525 0.67264557 0.40515232 0.32933354 0.74745512 1.222815 1.045584 0.2583518 -0.41250134 -0.62829459 -0.60391963 -0.62433505 -0.77485156 -1.0007032][-0.22828031 0.11886048 0.27653909 0.29396629 0.50494194 1.1506026 1.8282084 1.8045893 1.0532722 0.399225 0.17842627 0.2289083 0.27468681 0.20615888 0.041661024][-1.0153822 -0.68659472 -0.46908534 -0.32658553 -0.048277855 0.52820468 1.1227372 1.1860476 0.75925541 0.36372089 0.24600554 0.31360269 0.42477512 0.4725666 0.4493413][-1.5842218 -1.4062827 -1.2732968 -1.1595471 -0.96899343 -0.58778179 -0.17400813 -0.047564507 -0.18384194 -0.31773198 -0.34072161 -0.30504322 -0.2408061 -0.16476464 -0.095842838][-1.8168864 -1.7973694 -1.7791512 -1.7641625 -1.7131991 -1.5447514 -1.3028803 -1.1737294 -1.16883 -1.1691688 -1.1613731 -1.2012419 -1.2586551 -1.243258 -1.1679039][-1.7112458 -1.787452 -1.8653536 -1.9366624 -1.9860792 -1.9642115 -1.8803492 -1.8158712 -1.7940632 -1.7783471 -1.7977915 -1.8894787 -2.0162094 -2.0731528 -2.0274734][-1.3348503 -1.4616503 -1.6130767 -1.7454488 -1.8426778 -1.8823326 -1.8724029 -1.8601331 -1.8673723 -1.9067755 -1.9932141 -2.1308036 -2.3050177 -2.4159737 -2.4044464][-0.88125741 -1.0120487 -1.2033464 -1.3807259 -1.4928312 -1.5294214 -1.5126032 -1.4988263 -1.5168746 -1.5963937 -1.7494189 -1.9486294 -2.1667352 -2.3236189 -2.3657079][-0.54821587 -0.67862964 -0.89120448 -1.0725608 -1.1638936 -1.1522177 -1.0617158 -0.95939803 -0.92633462 -1.0088071 -1.198743 -1.4500704 -1.7006304 -1.8932575 -1.9946176]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:17:13.416855: step 10010, loss = 0.45, batch loss = 0.19 (49.5 examples/sec; 0.162 sec/batch; 14h:29m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:17:15.083985: step 10020, loss = 0.47, batch loss = 0.21 (48.1 examples/sec; 0.166 sec/batch; 14h:53m:12s remains)
INFO - root - 2017-12-16 08:17:16.756893: step 10030, loss = 0.52, batch loss = 0.25 (47.5 examples/sec; 0.169 sec/batch; 15h:05m:52s remains)
INFO - root - 2017-12-16 08:17:18.427189: step 10040, loss = 0.48, batch loss = 0.22 (47.6 examples/sec; 0.168 sec/batch; 15h:04m:07s remains)
INFO - root - 2017-12-16 08:17:20.088481: step 10050, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:52m:58s remains)
INFO - root - 2017-12-16 08:17:21.740641: step 10060, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:53m:54s remains)
INFO - root - 2017-12-16 08:17:23.440378: step 10070, loss = 0.63, batch loss = 0.37 (45.6 examples/sec; 0.175 sec/batch; 15h:42m:40s remains)
INFO - root - 2017-12-16 08:17:25.102036: step 10080, loss = 0.50, batch loss = 0.24 (49.1 examples/sec; 0.163 sec/batch; 14h:35m:27s remains)
INFO - root - 2017-12-16 08:17:26.768372: step 10090, loss = 0.57, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 14h:42m:12s remains)
INFO - root - 2017-12-16 08:17:28.443557: step 10100, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 14h:52m:36s remains)
2017-12-16 08:17:28.930240: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1810365 -4.8134723 -4.7504845 -4.2017889 -3.5469961 -2.8614507 -2.613029 -3.1821384 -3.9964242 -4.0488586 -3.5891514 -3.3047664 -3.3636773 -3.4951129 -3.4599612][-4.265234 -4.7770996 -4.6464024 -4.0434141 -3.3297067 -2.4446087 -2.0160351 -2.5806727 -3.5002244 -3.5840468 -3.0837345 -2.8562212 -3.0807076 -3.3396096 -3.2767189][-4.0877352 -4.3603964 -4.1092186 -3.4789262 -2.735409 -1.699702 -1.2069277 -1.8957249 -2.9758165 -3.1380031 -2.6733916 -2.495481 -2.8428774 -3.190294 -3.0798225][-3.7160263 -3.6874676 -3.2346437 -2.5072472 -1.608199 -0.42669952 0.031462431 -0.90149939 -2.2821684 -2.7327271 -2.460227 -2.4158261 -2.8426149 -3.1980593 -3.0412121][-3.3641756 -3.0753577 -2.3676875 -1.4143863 -0.25663924 1.0878351 1.5517859 0.3574152 -1.3919572 -2.2635019 -2.35715 -2.5084214 -2.9897709 -3.3523459 -3.1872873][-3.1646438 -2.7970867 -1.9198081 -0.76643169 0.62428713 2.1087053 2.5948312 1.3585348 -0.56173909 -1.8007973 -2.231899 -2.5918114 -3.1237092 -3.4966123 -3.3723426][-2.9737518 -2.765135 -1.9757121 -0.79701138 0.64494896 2.157778 2.7347524 1.6893151 -0.10473251 -1.4202688 -1.9934442 -2.4616678 -3.0836709 -3.5303383 -3.5005026][-2.6162393 -2.8045585 -2.3811474 -1.3255726 0.086899996 1.5704482 2.2031715 1.5118325 0.12988043 -0.96074533 -1.5454173 -2.0617571 -2.7530515 -3.3121169 -3.4882519][-2.0052519 -2.6594095 -2.7848732 -2.0294414 -0.65446913 0.790035 1.4956019 1.2162464 0.37898493 -0.30947542 -0.80890095 -1.3638315 -2.0943222 -2.7605255 -3.1826124][-1.4654131 -2.538873 -3.032969 -2.5606043 -1.2861081 0.14346004 1.0193865 1.1752596 0.82956362 0.46292377 0.061580181 -0.5433234 -1.3243445 -2.0913796 -2.7367051][-1.3685595 -2.5667782 -3.1863329 -2.8589108 -1.6950476 -0.23867035 0.80266 1.267488 1.3204293 1.1800258 0.760072 0.022252321 -0.83058488 -1.6546686 -2.3854048][-1.4938909 -2.5836158 -3.0815969 -2.7170134 -1.6377327 -0.23592854 0.80040741 1.2519698 1.3997445 1.3475521 0.87375 0.0474782 -0.87825632 -1.6773891 -2.3516183][-1.6310595 -2.5004025 -2.804687 -2.3160188 -1.2520797 -0.030099869 0.81389022 1.0194216 0.96681118 0.82822347 0.35818982 -0.53067887 -1.4451089 -2.0327795 -2.4384897][-1.5659988 -2.1762061 -2.3545024 -1.8958156 -1.0339684 -0.046071768 0.57112241 0.55672431 0.31989026 0.043725014 -0.42452252 -1.2446122 -2.0387125 -2.3678994 -2.4090681][-1.2700896 -1.7326677 -1.9115853 -1.6326339 -1.0703822 -0.39617455 0.042232275 -0.03461957 -0.332996 -0.62340021 -0.98017907 -1.6305995 -2.3053493 -2.5143304 -2.3286946]]...]
INFO - root - 2017-12-16 08:17:30.612636: step 10110, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 14h:54m:19s remains)
INFO - root - 2017-12-16 08:17:32.284385: step 10120, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:51m:55s remains)
INFO - root - 2017-12-16 08:17:33.948841: step 10130, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 15h:05m:14s remains)
INFO - root - 2017-12-16 08:17:35.613721: step 10140, loss = 0.62, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 14h:52m:58s remains)
INFO - root - 2017-12-16 08:17:37.312381: step 10150, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.169 sec/batch; 15h:09m:40s remains)
INFO - root - 2017-12-16 08:17:38.997480: step 10160, loss = 0.56, batch loss = 0.30 (46.4 examples/sec; 0.172 sec/batch; 15h:25m:40s remains)
INFO - root - 2017-12-16 08:17:40.676583: step 10170, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:38m:21s remains)
INFO - root - 2017-12-16 08:17:42.325967: step 10180, loss = 0.60, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 15h:07m:30s remains)
INFO - root - 2017-12-16 08:17:43.995357: step 10190, loss = 0.55, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 14h:33m:55s remains)
INFO - root - 2017-12-16 08:17:45.670794: step 10200, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:51m:49s remains)
2017-12-16 08:17:46.180326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7656467 -1.7405858 -1.8175921 -1.9197872 -2.0264406 -2.126884 -2.1494603 -2.1151547 -2.0659773 -1.9081624 -1.7436284 -1.7831485 -1.9557337 -2.09494 -2.2134795][-1.5511868 -1.5606781 -1.5915866 -1.5426297 -1.4671088 -1.5082914 -1.5891315 -1.6673816 -1.6748534 -1.4860377 -1.2802182 -1.2922761 -1.508821 -1.7594845 -2.0220981][-0.847895 -0.86801052 -0.84982157 -0.63216579 -0.44549263 -0.52323782 -0.76853859 -1.0093689 -1.1008979 -0.90015674 -0.673766 -0.62612796 -0.74909794 -0.95480227 -1.2312829][0.094768286 0.15639114 0.30233574 0.67951751 0.94317293 0.765471 0.29183531 -0.1587131 -0.37954581 -0.25714469 -0.079411983 -0.054569006 -0.11415339 -0.21566606 -0.38367975][0.78011322 0.93279457 1.2095718 1.7465219 2.1338778 1.9190946 1.3104174 0.67503047 0.2919457 0.23029804 0.26016736 0.21983361 0.11312795 0.018462658 -0.026416302][0.94664693 1.1259298 1.4508617 2.1011472 2.6927361 2.5416698 1.970252 1.3290424 0.87843108 0.69717622 0.53635335 0.29848027 0.0079433918 -0.16680384 -0.14908361][0.27550197 0.412884 0.707546 1.2890973 1.8957815 2.0549173 1.8168764 1.4396324 1.1915751 1.0945323 0.78842211 0.236691 -0.30271828 -0.54712057 -0.5009321][-1.0865244 -0.97029865 -0.71852756 -0.24743271 0.30553508 0.68444967 0.85016513 0.986248 1.163712 1.2821698 0.90803051 0.15735483 -0.5658828 -0.90329087 -0.93354917][-2.3206761 -2.1934407 -1.9901409 -1.6425425 -1.1794121 -0.77280438 -0.37649512 0.16251659 0.75624013 1.0761721 0.693532 -0.068054914 -0.7717886 -1.1538 -1.3154713][-2.9180174 -2.7336233 -2.5469093 -2.3085225 -2.0196552 -1.6983645 -1.2460985 -0.59121013 0.095665932 0.48359418 0.22077346 -0.41568506 -1.0177734 -1.4002547 -1.5998757][-2.98146 -2.7712233 -2.5854182 -2.430469 -2.2474186 -2.0366652 -1.7255257 -1.2690834 -0.78980863 -0.48894668 -0.59297 -0.99125767 -1.4149741 -1.7150071 -1.8396661][-2.6452131 -2.4840043 -2.3491862 -2.2596142 -2.1964209 -2.1786418 -2.098191 -1.9378686 -1.7201989 -1.545711 -1.5478199 -1.7132709 -1.9042084 -2.0423226 -1.9985635][-2.0273337 -1.9873323 -1.9680648 -1.973477 -2.0516121 -2.1948576 -2.2905169 -2.3295331 -2.2880714 -2.1997392 -2.1348281 -2.1146245 -2.0982032 -2.0488579 -1.8462994][-1.5292385 -1.5925926 -1.695279 -1.7934581 -1.9807209 -2.2080665 -2.3703086 -2.4605265 -2.5016258 -2.4027584 -2.2347841 -2.0612783 -1.9027246 -1.7287276 -1.46737][-1.3832188 -1.524123 -1.6937728 -1.8637068 -2.100759 -2.3197045 -2.4661322 -2.5592165 -2.6089556 -2.4592543 -2.1742282 -1.9338198 -1.7379129 -1.5435038 -1.319829]]...]
INFO - root - 2017-12-16 08:17:47.879734: step 10210, loss = 0.59, batch loss = 0.33 (47.1 examples/sec; 0.170 sec/batch; 15h:11m:51s remains)
INFO - root - 2017-12-16 08:17:49.523605: step 10220, loss = 0.49, batch loss = 0.23 (46.8 examples/sec; 0.171 sec/batch; 15h:18m:12s remains)
INFO - root - 2017-12-16 08:17:51.222600: step 10230, loss = 0.61, batch loss = 0.34 (49.1 examples/sec; 0.163 sec/batch; 14h:34m:17s remains)
INFO - root - 2017-12-16 08:17:52.896054: step 10240, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 14h:48m:25s remains)
INFO - root - 2017-12-16 08:17:54.566536: step 10250, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:38m:32s remains)
INFO - root - 2017-12-16 08:17:56.211777: step 10260, loss = 0.64, batch loss = 0.38 (48.9 examples/sec; 0.164 sec/batch; 14h:38m:45s remains)
INFO - root - 2017-12-16 08:17:57.872299: step 10270, loss = 0.47, batch loss = 0.21 (48.1 examples/sec; 0.166 sec/batch; 14h:52m:51s remains)
INFO - root - 2017-12-16 08:17:59.553221: step 10280, loss = 0.60, batch loss = 0.34 (49.6 examples/sec; 0.161 sec/batch; 14h:26m:34s remains)
INFO - root - 2017-12-16 08:18:01.215730: step 10290, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:44m:55s remains)
INFO - root - 2017-12-16 08:18:02.927152: step 10300, loss = 0.58, batch loss = 0.31 (42.3 examples/sec; 0.189 sec/batch; 16h:55m:13s remains)
2017-12-16 08:18:03.389454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3593876 -2.1870885 -2.2052121 -2.2294636 -2.1418126 -2.0966423 -2.3040733 -2.7960339 -3.2815864 -3.5050926 -3.5435908 -3.4944677 -3.3662434 -3.2080371 -3.1131754][-2.3807194 -2.1564319 -2.1829369 -2.3271656 -2.3775644 -2.3878508 -2.5367303 -2.9263153 -3.3670964 -3.6777933 -3.8335056 -3.8811736 -3.7797935 -3.5394335 -3.2332692][-2.5896144 -2.3282096 -2.3328254 -2.4944189 -2.5886695 -2.563488 -2.6039941 -2.8518119 -3.2183995 -3.5657477 -3.8256505 -4.0038118 -3.9321332 -3.4391427 -2.6728833][-3.0107594 -2.7578309 -2.6119196 -2.5056736 -2.2764013 -1.9652377 -1.8619807 -2.0529625 -2.3801732 -2.7733567 -3.2046516 -3.5501835 -3.48532 -2.7085073 -1.4268229][-3.3484354 -3.1289406 -2.7266176 -2.0819726 -1.1746615 -0.33763826 -0.024723291 -0.28817964 -0.70881748 -1.1958452 -1.8606777 -2.4766159 -2.5573306 -1.7411642 -0.24517298][-3.2324171 -3.0099516 -2.3654094 -1.1101229 0.65338278 2.1880364 2.6284194 2.0732622 1.3355484 0.61278987 -0.34893906 -1.2869303 -1.6413128 -1.1404967 -0.0055480003][-2.4453034 -2.210151 -1.4738756 0.17635155 2.6407127 4.8320456 5.2535229 4.2241731 2.96986 1.892488 0.63604474 -0.46038842 -1.0074276 -0.99527228 -0.48448467][-1.3887689 -1.1797184 -0.5445143 1.0285909 3.5576534 5.7763782 6.0185151 4.7266421 3.1790972 1.9393978 0.69991803 -0.30954206 -0.9438256 -1.2978425 -1.2822824][-0.5467838 -0.40124619 -0.075252295 0.90137243 2.5497732 3.911149 3.9879165 2.9457932 1.6489282 0.640147 -0.21662545 -0.881297 -1.354923 -1.7305217 -1.9431686][-0.29149175 -0.3422997 -0.39057255 -0.18207836 0.40833998 0.90402579 0.84986711 0.22752786 -0.49429333 -1.0186539 -1.4213085 -1.7414329 -1.970015 -2.1451981 -2.3981662][-0.7069906 -0.96359694 -1.3263359 -1.6735251 -1.8392276 -1.893562 -2.0544477 -2.308624 -2.4345229 -2.3376927 -2.2578204 -2.2538354 -2.1976326 -2.166338 -2.3840814][-1.4555209 -1.7852716 -2.2623403 -2.8528092 -3.3500123 -3.650804 -3.8381886 -3.8237152 -3.4680619 -2.8277619 -2.4092908 -2.2159164 -1.9565907 -1.7971452 -1.919337][-2.0533643 -2.3507073 -2.7612736 -3.2717371 -3.7121706 -4.0319753 -4.17569 -3.9814649 -3.3774652 -2.5632324 -2.1356444 -1.975433 -1.745548 -1.5463494 -1.5487118][-2.2050483 -2.3534555 -2.5646329 -2.8533602 -3.1327608 -3.3670797 -3.460587 -3.1941338 -2.6030495 -1.9506725 -1.7310896 -1.767628 -1.7382321 -1.6796222 -1.7076886][-2.0244908 -2.0338714 -2.0345669 -2.0757723 -2.1710989 -2.2952733 -2.3331749 -2.1358166 -1.772069 -1.4103082 -1.4065911 -1.6320004 -1.8293486 -1.9919848 -2.148632]]...]
INFO - root - 2017-12-16 08:18:05.075985: step 10310, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:59m:07s remains)
INFO - root - 2017-12-16 08:18:06.735339: step 10320, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.163 sec/batch; 14h:32m:40s remains)
INFO - root - 2017-12-16 08:18:08.400931: step 10330, loss = 0.63, batch loss = 0.37 (45.8 examples/sec; 0.175 sec/batch; 15h:37m:33s remains)
INFO - root - 2017-12-16 08:18:10.107922: step 10340, loss = 0.60, batch loss = 0.34 (42.6 examples/sec; 0.188 sec/batch; 16h:47m:39s remains)
INFO - root - 2017-12-16 08:18:11.795434: step 10350, loss = 0.65, batch loss = 0.39 (44.2 examples/sec; 0.181 sec/batch; 16h:11m:58s remains)
INFO - root - 2017-12-16 08:18:13.487401: step 10360, loss = 0.55, batch loss = 0.29 (49.8 examples/sec; 0.161 sec/batch; 14h:22m:19s remains)
INFO - root - 2017-12-16 08:18:15.138587: step 10370, loss = 0.57, batch loss = 0.31 (48.9 examples/sec; 0.164 sec/batch; 14h:38m:41s remains)
INFO - root - 2017-12-16 08:18:16.786506: step 10380, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:55m:55s remains)
INFO - root - 2017-12-16 08:18:18.432711: step 10390, loss = 0.49, batch loss = 0.23 (45.7 examples/sec; 0.175 sec/batch; 15h:40m:41s remains)
INFO - root - 2017-12-16 08:18:20.123043: step 10400, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 14h:49m:57s remains)
2017-12-16 08:18:20.610692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8334391 -1.4360251 -1.2555703 -1.2361993 -1.2469168 -1.1863301 -1.0742574 -1.0117166 -1.0739331 -1.2383537 -1.4625939 -1.6578809 -1.7835898 -1.7619867 -1.4352272][-2.1302609 -1.7968849 -1.6693807 -1.680071 -1.6656644 -1.5375242 -1.3507545 -1.2300774 -1.2397882 -1.4074535 -1.6330984 -1.80428 -1.9032007 -1.9124937 -1.6494758][-2.2504365 -2.0570383 -2.0366731 -2.113483 -2.1046214 -1.9490442 -1.721621 -1.5801866 -1.5979567 -1.8057383 -2.0724218 -2.2776575 -2.3955727 -2.3977392 -2.1177812][-2.1610272 -2.1092207 -2.1769626 -2.3249779 -2.3720195 -2.2087936 -1.9418886 -1.7932409 -1.8547337 -2.1474702 -2.5158567 -2.8257878 -2.992862 -2.9641879 -2.6404495][-1.9299598 -1.9746639 -2.0859668 -2.2481337 -2.2420356 -1.9579214 -1.571182 -1.3762364 -1.4922869 -1.8664939 -2.3554239 -2.8099291 -3.0612817 -3.044245 -2.7243094][-1.8745147 -1.885515 -1.9374852 -1.9751346 -1.782129 -1.2938437 -0.70898294 -0.37072992 -0.42265165 -0.79236567 -1.3386424 -1.880411 -2.2432649 -2.3558426 -2.1864049][-2.1043603 -1.9909052 -1.8517115 -1.6369677 -1.1956455 -0.49760306 0.2561872 0.74404883 0.80461168 0.50502968 -0.0018632412 -0.55548429 -1.0569258 -1.3955846 -1.4748228][-2.60318 -2.3612759 -2.0316491 -1.5946589 -1.0068108 -0.25467396 0.520545 1.1100571 1.2837868 1.0580132 0.56186247 -0.016925573 -0.6263926 -1.1135485 -1.31989][-3.3091815 -3.0746229 -2.67306 -2.1892743 -1.6252911 -0.94527328 -0.21573949 0.37545466 0.60785055 0.46582675 0.055894136 -0.47741175 -1.0869471 -1.6161411 -1.8915392][-3.7152233 -3.6107316 -3.3516269 -2.9796839 -2.5510514 -2.0419943 -1.4642036 -0.9736073 -0.74256718 -0.800048 -1.0657017 -1.4634789 -1.9430922 -2.3635511 -2.5862379][-3.5993834 -3.6176796 -3.549396 -3.393204 -3.1773634 -2.886023 -2.5267889 -2.1909823 -1.9907491 -1.970277 -2.1041749 -2.3436902 -2.6420996 -2.8609462 -2.912039][-3.1386368 -3.1527569 -3.1769581 -3.1933558 -3.1847038 -3.1008186 -2.9307508 -2.7305641 -2.5865417 -2.5404499 -2.6304114 -2.7761486 -2.8950911 -2.9015932 -2.720479][-2.4555295 -2.3594816 -2.3347623 -2.411191 -2.5393243 -2.5986595 -2.5508592 -2.4605107 -2.3935368 -2.3992794 -2.4950905 -2.5903418 -2.6056314 -2.4918323 -2.1491477][-1.8095725 -1.526829 -1.3865874 -1.4362818 -1.6218669 -1.7498555 -1.757504 -1.7274845 -1.7286146 -1.7834098 -1.8775095 -1.9416853 -1.9267381 -1.8080152 -1.4475634][-1.6551081 -1.2584072 -1.0386416 -1.0685781 -1.2754446 -1.4351239 -1.4758964 -1.4786364 -1.4999723 -1.5460888 -1.6004241 -1.6341773 -1.6272112 -1.5457379 -1.2465619]]...]
INFO - root - 2017-12-16 08:18:22.328518: step 10410, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:53m:28s remains)
INFO - root - 2017-12-16 08:18:23.996940: step 10420, loss = 0.61, batch loss = 0.35 (47.1 examples/sec; 0.170 sec/batch; 15h:11m:01s remains)
INFO - root - 2017-12-16 08:18:25.652906: step 10430, loss = 0.72, batch loss = 0.46 (49.2 examples/sec; 0.163 sec/batch; 14h:32m:43s remains)
INFO - root - 2017-12-16 08:18:27.281970: step 10440, loss = 0.53, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 14h:30m:41s remains)
INFO - root - 2017-12-16 08:18:28.942184: step 10450, loss = 0.71, batch loss = 0.45 (47.9 examples/sec; 0.167 sec/batch; 14h:56m:44s remains)
INFO - root - 2017-12-16 08:18:30.635287: step 10460, loss = 0.64, batch loss = 0.38 (47.4 examples/sec; 0.169 sec/batch; 15h:05m:26s remains)
INFO - root - 2017-12-16 08:18:32.297046: step 10470, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 15h:02m:16s remains)
INFO - root - 2017-12-16 08:18:33.971545: step 10480, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:52m:00s remains)
INFO - root - 2017-12-16 08:18:35.630320: step 10490, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:45m:03s remains)
INFO - root - 2017-12-16 08:18:37.304068: step 10500, loss = 0.62, batch loss = 0.36 (49.5 examples/sec; 0.162 sec/batch; 14h:28m:01s remains)
2017-12-16 08:18:37.785706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9196713 -2.3439207 -2.650867 -2.7536712 -2.7756269 -2.8317618 -2.8114331 -2.490309 -1.6290373 -0.36311567 0.84134865 1.5609996 1.5719893 0.97517395 -0.050075293][-1.7089128 -2.102057 -2.2753358 -2.2463977 -2.2495646 -2.3678076 -2.4532557 -2.0976281 -1.0742512 0.32170272 1.3631232 1.6949024 1.3238003 0.43033147 -0.66414535][-2.1714973 -2.3604116 -2.2470996 -2.0115798 -1.9337885 -2.0413606 -2.0502639 -1.5582931 -0.3902359 1.0129559 1.783807 1.7187831 1.0038121 -0.092070341 -1.1454659][-2.8696182 -2.8465562 -2.4692397 -2.0879657 -1.8970301 -1.8199883 -1.5321395 -0.74060595 0.57440972 1.9339221 2.4243281 1.9667766 0.92737579 -0.32356513 -1.4047267][-3.3629332 -3.1295214 -2.6476212 -2.1945474 -1.7840022 -1.254833 -0.35126722 0.86531258 2.1744835 3.1089132 3.0734317 2.1337721 0.80730438 -0.58073235 -1.6355004][-3.3564029 -3.0026052 -2.5228145 -1.985427 -1.2182391 -0.049685478 1.6152043 3.2509015 4.2203617 4.1838875 3.2293441 1.6928656 0.0938375 -1.2620424 -2.0733216][-2.7785127 -2.3806033 -1.9179466 -1.1925154 0.0429101 1.840255 4.1128874 5.9221344 5.9172144 4.3780136 2.4412973 0.60232687 -1.0061643 -2.153331 -2.6061766][-1.9591552 -1.5183945 -0.95600367 0.089882135 1.6018033 3.5068591 5.6834335 7.0644207 5.86473 3.3642137 1.0629592 -0.71937919 -2.0687184 -2.8862731 -2.9874558][-1.0193068 -0.41846704 0.35787749 1.5149879 2.7819984 3.9429176 4.8917112 4.9263372 3.4490402 1.2540557 -0.66176581 -1.9754192 -2.8224039 -3.1996324 -2.921144][-0.057230473 0.64658189 1.4966016 2.3524458 2.902427 3.0076993 2.6857326 1.8501499 0.51253867 -0.97965753 -2.2462807 -3.0096638 -3.2629161 -3.0797167 -2.4088838][0.85931158 1.4492428 1.9763262 2.2073753 1.9913466 1.3707569 0.37148142 -0.75718117 -1.7555637 -2.6161222 -3.2527425 -3.48948 -3.2373426 -2.5477378 -1.6095138][1.7213089 2.0166957 1.9785759 1.5191274 0.6879878 -0.32922149 -1.5087967 -2.5485969 -3.2135837 -3.5408607 -3.6166773 -3.3949516 -2.7547336 -1.7985482 -0.77177608][2.4100034 2.3228261 1.7659976 0.767457 -0.38719642 -1.529689 -2.5688641 -3.2979221 -3.5763569 -3.4865551 -3.1929085 -2.6808546 -1.9119022 -0.9137845 0.0598979][2.6833432 2.2310627 1.2519023 0.0075731277 -1.2136375 -2.191493 -2.8883977 -3.1856904 -3.0918322 -2.7940686 -2.36644 -1.7972796 -1.0439129 -0.11319852 0.6970973][2.0679958 1.287298 0.13525343 -1.0043305 -1.9280164 -2.5044599 -2.7274804 -2.6403837 -2.4029169 -2.1099823 -1.7130349 -1.2131404 -0.53026319 0.27117634 0.8332727]]...]
INFO - root - 2017-12-16 08:18:39.458182: step 10510, loss = 0.74, batch loss = 0.48 (47.5 examples/sec; 0.168 sec/batch; 15h:03m:20s remains)
INFO - root - 2017-12-16 08:18:41.119438: step 10520, loss = 0.59, batch loss = 0.33 (49.5 examples/sec; 0.162 sec/batch; 14h:27m:16s remains)
INFO - root - 2017-12-16 08:18:42.764257: step 10530, loss = 0.55, batch loss = 0.29 (50.5 examples/sec; 0.158 sec/batch; 14h:09m:50s remains)
INFO - root - 2017-12-16 08:18:44.434227: step 10540, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:56m:51s remains)
INFO - root - 2017-12-16 08:18:46.095869: step 10550, loss = 0.54, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 14h:48m:49s remains)
INFO - root - 2017-12-16 08:18:47.765070: step 10560, loss = 0.47, batch loss = 0.20 (46.7 examples/sec; 0.171 sec/batch; 15h:18m:22s remains)
INFO - root - 2017-12-16 08:18:49.416515: step 10570, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 14h:47m:11s remains)
INFO - root - 2017-12-16 08:18:51.129849: step 10580, loss = 0.63, batch loss = 0.37 (46.1 examples/sec; 0.174 sec/batch; 15h:31m:55s remains)
INFO - root - 2017-12-16 08:18:52.809164: step 10590, loss = 0.49, batch loss = 0.23 (46.8 examples/sec; 0.171 sec/batch; 15h:17m:23s remains)
INFO - root - 2017-12-16 08:18:54.483685: step 10600, loss = 0.65, batch loss = 0.39 (48.4 examples/sec; 0.165 sec/batch; 14h:46m:35s remains)
2017-12-16 08:18:54.998283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5509973 -2.7978206 -3.087837 -3.1678097 -3.1277361 -3.1168025 -3.0571797 -2.8651531 -2.6836109 -2.581131 -2.5150506 -2.3816068 -2.2791426 -2.3450279 -2.4078393][-2.7781563 -2.9689119 -3.2026176 -3.2471364 -3.1916606 -3.2032402 -3.1313457 -2.8802619 -2.5939441 -2.4571118 -2.3927338 -2.2492816 -2.1712604 -2.2487714 -2.3182144][-2.3519945 -2.4027553 -2.580126 -2.5986342 -2.5465066 -2.6209762 -2.6738198 -2.5106084 -2.3164663 -2.3031175 -2.3374321 -2.3009586 -2.2694132 -2.283819 -2.2470226][-1.407212 -1.3596184 -1.4683862 -1.4136188 -1.2825044 -1.4220684 -1.6659591 -1.7510352 -1.8287625 -2.1566474 -2.4745626 -2.618257 -2.7093766 -2.670584 -2.465996][-0.46631825 -0.41864467 -0.48613584 -0.30397511 -0.0082337856 -0.094462872 -0.44089282 -0.73767281 -1.0934496 -1.7644992 -2.4248438 -2.890564 -3.2031183 -3.1920576 -2.8983345][-0.0399549 -0.0031170845 -0.0878849 0.2201879 0.72116041 0.81845713 0.52421904 0.21303225 -0.21625328 -1.0180658 -1.8810824 -2.6514761 -3.2480717 -3.3965745 -3.1377544][-0.36739194 -0.34160233 -0.42771256 -0.0061068535 0.7383697 1.1157589 1.0476263 0.97003365 0.61910176 -0.11012793 -0.9784137 -1.9315888 -2.7250223 -3.0577579 -2.8751569][-1.3085679 -1.2132428 -1.2297013 -0.73366344 0.15281034 0.75525856 1.0405545 1.2491589 1.0551364 0.58476663 -0.13061166 -1.1263469 -2.0161991 -2.4302495 -2.2199593][-2.1008272 -1.8742104 -1.797034 -1.3452991 -0.61399996 0.007515192 0.53676033 0.84341 0.77416134 0.620126 0.20019436 -0.61164808 -1.3354008 -1.6475208 -1.2627397][-2.3226724 -1.9690099 -1.8626995 -1.5195842 -1.0670967 -0.64237618 -0.1655643 -0.062779665 -0.17464447 -0.10556531 -0.2316556 -0.73848927 -1.1240273 -1.125374 -0.46401453][-1.9264845 -1.489939 -1.2886753 -1.0716221 -0.95336664 -0.85813951 -0.76230955 -1.0743685 -1.3920498 -1.2705401 -1.13785 -1.286227 -1.3059045 -1.0125532 -0.12910676][-1.1964631 -0.62639523 -0.27144909 -0.078167915 -0.24004769 -0.54779851 -0.96613741 -1.7953267 -2.4713335 -2.4930866 -2.3017154 -2.1601779 -1.8506553 -1.3131344 -0.43114638][-0.54129136 0.12838721 0.63271284 0.88028264 0.58484888 0.0028607845 -0.9040966 -2.1994195 -3.173496 -3.3806009 -3.2538555 -3.0225379 -2.5877779 -2.0492253 -1.3874133][-0.1682229 0.60448575 1.2364664 1.5512855 1.2508466 0.53682017 -0.63172626 -2.1941881 -3.3696895 -3.77416 -3.7552919 -3.5490236 -3.2089019 -2.89234 -2.5677457][0.21038842 1.0348582 1.7093379 2.0570862 1.7924111 1.0638735 -0.24455285 -1.9334713 -3.2273014 -3.8023248 -3.9307215 -3.827702 -3.6208096 -3.5451689 -3.5400686]]...]
INFO - root - 2017-12-16 08:18:56.665826: step 10610, loss = 0.69, batch loss = 0.43 (48.0 examples/sec; 0.167 sec/batch; 14h:54m:22s remains)
INFO - root - 2017-12-16 08:18:58.327793: step 10620, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:55m:50s remains)
INFO - root - 2017-12-16 08:18:59.984188: step 10630, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 14h:40m:27s remains)
INFO - root - 2017-12-16 08:19:01.663238: step 10640, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 14h:46m:01s remains)
INFO - root - 2017-12-16 08:19:03.370940: step 10650, loss = 0.54, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 15h:19m:19s remains)
INFO - root - 2017-12-16 08:19:05.042113: step 10660, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 14h:54m:39s remains)
INFO - root - 2017-12-16 08:19:06.700272: step 10670, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 15h:00m:30s remains)
INFO - root - 2017-12-16 08:19:08.366000: step 10680, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 14h:33m:50s remains)
INFO - root - 2017-12-16 08:19:10.029096: step 10690, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 15h:02m:44s remains)
INFO - root - 2017-12-16 08:19:11.696351: step 10700, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 15h:00m:23s remains)
2017-12-16 08:19:12.164020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3109984 -1.9509279 -1.6000817 -1.4233472 -1.0756536 -0.44738889 -0.30716932 -0.97656226 -1.7557251 -2.1757638 -2.3967695 -2.6559763 -2.7758031 -2.6518717 -2.5452113][-2.3568156 -2.334866 -2.3266959 -2.3153791 -1.9498847 -1.2536339 -1.0224875 -1.5435156 -2.2449231 -2.6893182 -2.9014854 -3.0540295 -3.1172037 -3.0407639 -2.9283581][-2.1388347 -2.5755723 -2.9620628 -3.169245 -2.8824883 -2.1715848 -1.7582955 -2.0156419 -2.5455723 -2.9913213 -3.2473564 -3.3463092 -3.32285 -3.2244091 -2.9958425][-1.6324412 -2.5576756 -3.3158727 -3.6999006 -3.4604146 -2.6686437 -1.9758656 -1.868813 -2.1859517 -2.6801538 -3.1036029 -3.2474413 -3.1523762 -2.9986598 -2.5656905][-1.1780493 -2.4699996 -3.453969 -3.9012547 -3.58291 -2.5855584 -1.4718232 -0.88823295 -1.0599174 -1.7406145 -2.5097241 -2.881752 -2.7763791 -2.4953933 -1.8206929][-1.1898502 -2.5737114 -3.56782 -3.9001729 -3.4298387 -2.194191 -0.66019118 0.40667486 0.34449625 -0.6763469 -1.9446144 -2.6764398 -2.6316233 -2.149333 -1.226487][-1.5231818 -2.7324688 -3.5413795 -3.7188206 -3.0904729 -1.6780521 0.19376707 1.7343521 1.8367867 0.40438676 -1.4509186 -2.5792451 -2.6917422 -2.1115203 -1.0896261][-1.9448053 -2.7700915 -3.3349459 -3.3795075 -2.6512177 -1.1431286 0.95881009 2.8321419 3.1240239 1.3905132 -0.97596562 -2.4854712 -2.8596826 -2.3583605 -1.4517281][-2.4077375 -2.8163085 -3.1454358 -3.1127248 -2.4077826 -0.98324406 1.0353663 2.9140553 3.3108649 1.5638115 -0.95710862 -2.6723654 -3.2179482 -2.8562794 -2.1424599][-2.7755075 -2.8659616 -3.0050707 -2.968595 -2.4276505 -1.2760648 0.30842066 1.7791877 2.125854 0.75006461 -1.4189698 -2.9715834 -3.5089631 -3.3118756 -2.7994716][-2.7697082 -2.6726806 -2.6767952 -2.6867523 -2.3715637 -1.5812902 -0.51420486 0.52864742 0.86052632 -0.051397085 -1.6175838 -2.8130479 -3.2231231 -3.1183834 -2.7883368][-2.3216822 -2.1046805 -2.06099 -2.1870918 -2.1941054 -1.8112661 -1.1738197 -0.48341405 -0.12404251 -0.51407671 -1.3901314 -2.1031203 -2.3631749 -2.3600583 -2.2178514][-1.5682447 -1.3649263 -1.3988408 -1.6940197 -2.0109723 -2.0207753 -1.671577 -1.1440498 -0.66302717 -0.60148823 -0.95899045 -1.3479784 -1.5200335 -1.6063834 -1.5474488][-0.93498039 -0.85208523 -0.98392081 -1.3553352 -1.8512921 -2.1305678 -1.9229128 -1.3520989 -0.6784296 -0.28988695 -0.402012 -0.75934839 -1.0387763 -1.2032564 -1.1510315][-0.68747532 -0.72751 -0.89332056 -1.206643 -1.6276382 -1.9056532 -1.7055386 -1.0314035 -0.15427589 0.44800234 0.38146853 -0.21027684 -0.78629756 -1.1162161 -1.1125079]]...]
INFO - root - 2017-12-16 08:19:13.836886: step 10710, loss = 0.53, batch loss = 0.27 (47.0 examples/sec; 0.170 sec/batch; 15h:13m:26s remains)
INFO - root - 2017-12-16 08:19:15.478979: step 10720, loss = 0.55, batch loss = 0.29 (48.9 examples/sec; 0.164 sec/batch; 14h:38m:08s remains)
INFO - root - 2017-12-16 08:19:17.125090: step 10730, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.169 sec/batch; 15h:03m:55s remains)
INFO - root - 2017-12-16 08:19:18.799257: step 10740, loss = 0.55, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 15h:05m:49s remains)
INFO - root - 2017-12-16 08:19:20.461401: step 10750, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 14h:39m:14s remains)
INFO - root - 2017-12-16 08:19:22.124624: step 10760, loss = 0.70, batch loss = 0.44 (43.2 examples/sec; 0.185 sec/batch; 16h:33m:07s remains)
INFO - root - 2017-12-16 08:19:23.795137: step 10770, loss = 0.49, batch loss = 0.23 (47.2 examples/sec; 0.169 sec/batch; 15h:08m:48s remains)
INFO - root - 2017-12-16 08:19:25.477946: step 10780, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 14h:57m:10s remains)
INFO - root - 2017-12-16 08:19:27.160401: step 10790, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 15h:05m:56s remains)
INFO - root - 2017-12-16 08:19:28.836474: step 10800, loss = 0.60, batch loss = 0.34 (46.8 examples/sec; 0.171 sec/batch; 15h:16m:44s remains)
2017-12-16 08:19:29.289828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7799778 -1.9284232 -2.0575535 -2.1564023 -2.3188555 -2.5230243 -2.7189624 -2.8244343 -2.8745131 -2.8835073 -2.8147008 -2.6728482 -2.4926848 -2.317199 -2.1270809][-2.1248143 -2.3847241 -2.6297336 -2.7983589 -3.0051234 -3.2646723 -3.5332894 -3.6607313 -3.6844721 -3.6883264 -3.6219063 -3.4134645 -3.109859 -2.8161349 -2.5353365][-2.7563646 -3.1662073 -3.4901571 -3.6005869 -3.6762214 -3.8909464 -4.1593008 -4.2786922 -4.2677879 -4.3150711 -4.3550797 -4.1558685 -3.7698994 -3.3831511 -3.0002074][-3.4465742 -3.9127822 -4.1504483 -3.9963515 -3.6944103 -3.685607 -3.8791947 -3.9526129 -3.9869356 -4.2032275 -4.4688396 -4.3969994 -4.1118646 -3.8000383 -3.3745041][-3.9294386 -4.2686014 -4.1672192 -3.525404 -2.6862915 -2.2151322 -2.1177435 -2.1511207 -2.3629892 -2.9143753 -3.5050373 -3.7372925 -3.7702956 -3.7402205 -3.4663591][-4.0543923 -4.1538353 -3.6006465 -2.3982389 -1.0101764 0.0047905445 0.57876062 0.72162652 0.28876829 -0.67310727 -1.5959393 -2.1994843 -2.7006912 -3.1310914 -3.17413][-3.8075681 -3.6983576 -2.8029635 -1.2100735 0.54557776 2.0492532 3.2136204 3.6599934 2.9727743 1.7150161 0.66701484 -0.28200722 -1.2616043 -2.156369 -2.5519357][-3.4578686 -3.2633059 -2.243624 -0.59302723 1.199424 2.9296019 4.4855795 5.2582493 4.44285 3.1744807 2.1960323 1.1253958 -0.13942242 -1.3216031 -1.9741176][-3.4423916 -3.3950787 -2.5601165 -1.1541872 0.39971423 1.938514 3.2762191 3.8896229 3.5020411 2.7320664 2.1348684 1.2699373 0.025994778 -1.1823946 -1.8736362][-3.6617584 -3.8481934 -3.4325647 -2.5484765 -1.4466165 -0.31159711 0.60720587 0.99842691 0.93945646 0.78521395 0.67080092 0.17200494 -0.7804147 -1.7032334 -2.1897497][-3.796906 -4.2106366 -4.2000341 -3.8093538 -3.1676586 -2.51213 -2.0165594 -1.7984546 -1.6438049 -1.4313701 -1.2183793 -1.4099333 -1.9786483 -2.4799442 -2.7089996][-3.6578774 -4.1592851 -4.3692055 -4.2730875 -3.9527667 -3.6520596 -3.5416589 -3.4569271 -3.2126656 -2.8968072 -2.6287353 -2.6637478 -2.8944793 -3.0659685 -3.1132255][-3.2015858 -3.6431468 -3.9284821 -3.9901643 -3.8837552 -3.82936 -3.9287682 -3.9352703 -3.7302504 -3.4715853 -3.2606626 -3.1925828 -3.2259908 -3.2339685 -3.1577332][-2.5968008 -2.9126325 -3.1576052 -3.2823191 -3.3284192 -3.4215817 -3.5395193 -3.56361 -3.4571695 -3.3042297 -3.1573536 -3.0680785 -3.0098906 -2.9394395 -2.8243804][-2.1018028 -2.2892075 -2.4474881 -2.5644999 -2.6651149 -2.7632461 -2.8374765 -2.8490229 -2.8104234 -2.7474675 -2.6692324 -2.6082754 -2.5533555 -2.4713347 -2.3646948]]...]
INFO - root - 2017-12-16 08:19:30.946110: step 10810, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:54m:10s remains)
INFO - root - 2017-12-16 08:19:32.617838: step 10820, loss = 0.57, batch loss = 0.31 (46.3 examples/sec; 0.173 sec/batch; 15h:27m:09s remains)
INFO - root - 2017-12-16 08:19:34.270419: step 10830, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 14h:33m:29s remains)
INFO - root - 2017-12-16 08:19:35.948853: step 10840, loss = 0.47, batch loss = 0.21 (48.2 examples/sec; 0.166 sec/batch; 14h:50m:35s remains)
INFO - root - 2017-12-16 08:19:37.578665: step 10850, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 14h:32m:36s remains)
INFO - root - 2017-12-16 08:19:39.217707: step 10860, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:54m:53s remains)
INFO - root - 2017-12-16 08:19:40.863047: step 10870, loss = 0.60, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 14h:53m:28s remains)
INFO - root - 2017-12-16 08:19:42.520050: step 10880, loss = 0.64, batch loss = 0.37 (46.7 examples/sec; 0.171 sec/batch; 15h:17m:46s remains)
INFO - root - 2017-12-16 08:19:44.220211: step 10890, loss = 0.52, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 15h:10m:01s remains)
INFO - root - 2017-12-16 08:19:45.880002: step 10900, loss = 0.74, batch loss = 0.48 (48.6 examples/sec; 0.165 sec/batch; 14h:42m:16s remains)
2017-12-16 08:19:46.396428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1527781 -2.9859009 -2.9294417 -2.9643998 -2.9965432 -2.7499294 -2.0485511 -1.3475592 -1.0899277 -1.3134953 -1.6797627 -1.9493695 -1.926481 -1.6446385 -1.3324938][-2.8281918 -2.7275739 -2.7243721 -2.8154678 -2.9594233 -2.8267324 -2.2166829 -1.5430732 -1.2209599 -1.3478506 -1.6475613 -1.9454829 -1.9749984 -1.6982398 -1.3996413][-2.6845198 -2.6428509 -2.6481342 -2.7804348 -3.0349627 -3.0179415 -2.4598303 -1.7647299 -1.4339579 -1.5885482 -1.9546549 -2.2949123 -2.2894258 -1.9520193 -1.5921301][-2.6872036 -2.6031055 -2.5492318 -2.6964178 -2.9717412 -2.9320612 -2.2762153 -1.5342768 -1.3217061 -1.6899877 -2.2661922 -2.6713955 -2.6309402 -2.2322352 -1.8068426][-2.7739937 -2.5685444 -2.3794324 -2.4281273 -2.5559976 -2.2598038 -1.3798645 -0.62069404 -0.67216349 -1.4629865 -2.3664992 -2.9010961 -2.8552527 -2.3990734 -1.9308621][-2.6682239 -2.3282561 -1.985105 -1.819324 -1.5995286 -0.86842191 0.28927279 1.008357 0.54202938 -0.79536569 -2.1097014 -2.853056 -2.8747597 -2.42316 -1.9514985][-2.2194664 -1.8189871 -1.3299582 -0.85326838 -0.12940574 1.0922246 2.4703367 3.0071118 2.0854228 0.22595429 -1.5186372 -2.5427582 -2.7094903 -2.3499875 -1.9226804][-1.7501433 -1.3590784 -0.79857361 -0.068050623 1.0507917 2.7163565 4.323637 4.7225647 3.4820344 1.237988 -0.84652817 -2.1493392 -2.53864 -2.3067558 -1.9251255][-1.750253 -1.3532171 -0.77196383 0.071659327 1.3131943 3.0501511 4.6551113 5.0631847 3.8549988 1.6095386 -0.52697396 -1.9537407 -2.5128171 -2.3833158 -2.0095031][-2.1347308 -1.7923193 -1.2222433 -0.41716933 0.66115189 2.0659029 3.3658984 3.8054445 2.8621991 0.99450469 -0.86987269 -2.1653957 -2.673233 -2.5253956 -2.1253462][-2.7262433 -2.4839485 -1.9774008 -1.232677 -0.31673825 0.80649495 1.8771288 2.2490633 1.4773993 -0.044977427 -1.5689452 -2.5786886 -2.8955557 -2.637958 -2.1932471][-2.947859 -2.8955188 -2.5693984 -1.9808031 -1.1772376 -0.16350436 0.85501313 1.2536945 0.65357304 -0.63627446 -1.9402022 -2.7863657 -2.9686842 -2.6679454 -2.2249615][-2.6317034 -2.7877874 -2.7590466 -2.4531024 -1.8394898 -0.87433708 0.2399466 0.8507123 0.49254155 -0.60263836 -1.8118315 -2.6339316 -2.8216178 -2.5721974 -2.1951289][-1.9872143 -2.2727983 -2.5092032 -2.49646 -2.1279149 -1.2822171 -0.076056 0.77692342 0.66596413 -0.26821065 -1.4308822 -2.3104148 -2.5956204 -2.4443092 -2.143712][-1.6754773 -2.0133324 -2.3095207 -2.4046144 -2.1565883 -1.345754 0.0066587925 1.063575 1.0967453 0.23023987 -0.96892154 -1.9891509 -2.4386525 -2.3900425 -2.1278021]]...]
INFO - root - 2017-12-16 08:19:48.076739: step 10910, loss = 0.57, batch loss = 0.31 (46.3 examples/sec; 0.173 sec/batch; 15h:25m:55s remains)
INFO - root - 2017-12-16 08:19:49.766115: step 10920, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 14h:50m:15s remains)
INFO - root - 2017-12-16 08:19:51.445360: step 10930, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:54m:58s remains)
INFO - root - 2017-12-16 08:19:53.117090: step 10940, loss = 0.51, batch loss = 0.25 (46.3 examples/sec; 0.173 sec/batch; 15h:26m:59s remains)
INFO - root - 2017-12-16 08:19:54.761626: step 10950, loss = 0.65, batch loss = 0.39 (48.2 examples/sec; 0.166 sec/batch; 14h:48m:55s remains)
INFO - root - 2017-12-16 08:19:56.425615: step 10960, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 15h:00m:45s remains)
INFO - root - 2017-12-16 08:19:58.098396: step 10970, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 14h:36m:56s remains)
INFO - root - 2017-12-16 08:19:59.765084: step 10980, loss = 0.53, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 14h:39m:50s remains)
INFO - root - 2017-12-16 08:20:01.401871: step 10990, loss = 0.57, batch loss = 0.31 (50.9 examples/sec; 0.157 sec/batch; 14h:02m:24s remains)
INFO - root - 2017-12-16 08:20:03.063426: step 11000, loss = 0.71, batch loss = 0.45 (47.7 examples/sec; 0.168 sec/batch; 14h:58m:18s remains)
2017-12-16 08:20:03.503427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.535327 -1.3672255 -1.3547539 -1.3683925 -1.3990569 -1.3650525 -1.3087083 -1.4364443 -1.7096124 -1.9049637 -2.0072289 -1.8852574 -1.5662816 -1.274531 -1.1156541][-1.0008683 -0.70980966 -0.59172368 -0.61348009 -0.69025159 -0.66087389 -0.566659 -0.74720585 -1.2418925 -1.7280117 -2.0785179 -2.14881 -1.9552541 -1.6633277 -1.426299][-0.4606111 -0.13820457 0.028570175 -0.11942434 -0.39766169 -0.61188579 -0.657547 -0.95697427 -1.6232119 -2.3244865 -2.8378484 -3.0623651 -2.9895487 -2.7369077 -2.4560452][-0.076817989 0.18661332 0.38997889 0.23662686 -0.2286973 -0.79251111 -1.1521132 -1.604265 -2.3950253 -3.1967402 -3.708647 -3.9974933 -4.0233688 -3.8302209 -3.5614138][0.0033490658 0.23067975 0.57997704 0.72277093 0.36566854 -0.33196247 -0.91773248 -1.5124552 -2.395422 -3.3145623 -3.9046001 -4.2561216 -4.4132934 -4.3202767 -4.0861325][-0.16100192 0.18824077 0.81015038 1.5129418 1.5897098 1.0188832 0.535269 0.096905231 -0.7643162 -1.8920223 -2.7633312 -3.3742757 -3.8150501 -3.9205849 -3.7769558][-0.69686639 -0.16154003 0.83305097 2.0524213 2.6283891 2.449321 2.5513222 2.7098711 1.9258192 0.5110693 -0.7132864 -1.7137063 -2.5481148 -2.9398625 -2.9607196][-1.6076322 -0.96276045 0.16479206 1.405997 2.1074417 2.5649354 3.4697316 4.5271168 4.0015182 2.4214585 1.0443518 -0.15262651 -1.2298062 -1.8822701 -2.0668766][-2.6920977 -2.134208 -1.2643468 -0.36698806 0.19775772 0.87570858 2.0993617 3.4344251 3.4607832 2.4292829 1.3795919 0.38781452 -0.56727552 -1.2507416 -1.5379474][-3.5539289 -3.1634603 -2.6580281 -2.1932559 -1.9684069 -1.5543749 -0.60627508 0.54481912 1.0632572 0.83484674 0.34499884 -0.17781091 -0.71822667 -1.2363532 -1.5435][-3.7534814 -3.5006092 -3.2600539 -3.1969976 -3.3857369 -3.5043368 -3.1429198 -2.3883522 -1.7402241 -1.4713942 -1.4519341 -1.4987144 -1.6201631 -1.9199982 -2.2329032][-3.3166857 -3.1015718 -3.0029535 -3.2459054 -3.75285 -4.2847037 -4.4789491 -4.212162 -3.6980913 -3.271162 -3.00743 -2.7661612 -2.6231527 -2.7625694 -3.0476356][-2.5362556 -2.3146734 -2.3302512 -2.7565014 -3.4334612 -4.2154617 -4.8315263 -4.9620085 -4.6296463 -4.2183008 -3.8686335 -3.4753194 -3.1811342 -3.201808 -3.4144204][-1.4545519 -1.2028111 -1.3165751 -1.8386357 -2.5513985 -3.4433994 -4.2874441 -4.68976 -4.5774813 -4.2548728 -3.9126625 -3.5157487 -3.1768508 -3.0646524 -3.1573472][-0.27857375 -0.0026443005 -0.11340976 -0.60556006 -1.2800468 -2.143914 -3.0934901 -3.7089143 -3.8229203 -3.6724951 -3.4608746 -3.2317986 -2.95932 -2.7639153 -2.7121167]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:20:05.208606: step 11010, loss = 0.58, batch loss = 0.32 (48.3 examples/sec; 0.166 sec/batch; 14h:48m:09s remains)
INFO - root - 2017-12-16 08:20:06.891628: step 11020, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 14h:52m:18s remains)
INFO - root - 2017-12-16 08:20:08.564925: step 11030, loss = 0.52, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 14h:31m:29s remains)
INFO - root - 2017-12-16 08:20:10.231273: step 11040, loss = 0.58, batch loss = 0.32 (48.3 examples/sec; 0.166 sec/batch; 14h:47m:14s remains)
INFO - root - 2017-12-16 08:20:11.880249: step 11050, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 14h:32m:38s remains)
INFO - root - 2017-12-16 08:20:13.539247: step 11060, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:39m:23s remains)
INFO - root - 2017-12-16 08:20:15.192482: step 11070, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:57m:14s remains)
INFO - root - 2017-12-16 08:20:16.843639: step 11080, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:54m:39s remains)
INFO - root - 2017-12-16 08:20:18.511732: step 11090, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 14h:53m:10s remains)
INFO - root - 2017-12-16 08:20:20.177997: step 11100, loss = 0.72, batch loss = 0.46 (47.4 examples/sec; 0.169 sec/batch; 15h:04m:00s remains)
2017-12-16 08:20:20.644275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8240944 -1.8030908 -1.8299752 -1.9385154 -2.0139804 -1.9187816 -1.8001744 -2.0161128 -2.4901412 -2.8656669 -2.919374 -2.6204944 -2.1641948 -1.7912374 -1.6014255][-1.8883851 -1.8958942 -1.9503514 -2.0956924 -2.143117 -1.8681464 -1.5629797 -1.8111365 -2.5727901 -3.2951841 -3.5335991 -3.2111447 -2.5524204 -1.9770584 -1.663029][-1.9245192 -1.9695647 -2.041981 -2.2037394 -2.1786268 -1.5723965 -0.85463357 -0.93853879 -1.9562708 -3.1611891 -3.8148842 -3.646636 -2.9098601 -2.173104 -1.7474542][-1.9666033 -2.0518713 -2.1464732 -2.3113475 -2.1535513 -1.1156293 0.25452662 0.53033137 -0.63908947 -2.3633044 -3.5729225 -3.7249184 -3.0850425 -2.2929864 -1.8107588][-2.015187 -2.13706 -2.2678537 -2.439167 -2.1720743 -0.71326041 1.3052146 2.126766 0.97056842 -1.1587269 -2.8939216 -3.463994 -3.0530019 -2.3167841 -1.8444037][-2.0678983 -2.2137294 -2.3725767 -2.5509381 -2.1681275 -0.38987613 2.1681764 3.5343206 2.5370476 0.14106512 -2.0257626 -2.9888935 -2.8462248 -2.2670822 -1.852715][-2.1242964 -2.2689395 -2.4291003 -2.5967159 -2.165132 -0.25655031 2.575155 4.2747488 3.5291059 0.98203278 -1.4375818 -2.6324837 -2.7016156 -2.255677 -1.8713242][-2.1839516 -2.3132167 -2.4616547 -2.5946133 -2.1524944 -0.31569397 2.4246905 4.0771523 3.3504684 0.81359315 -1.5148969 -2.6783078 -2.7772493 -2.3428793 -1.9322001][-2.2614589 -2.3428185 -2.4378166 -2.516161 -2.0882354 -0.45192838 1.9108922 3.1016762 2.0555775 -0.31843066 -2.3052685 -3.1720097 -3.0501428 -2.4840906 -1.9941424][-2.3263142 -2.3146436 -2.3084292 -2.3316469 -1.957666 -0.53704846 1.4312298 2.0627229 0.61688995 -1.6614232 -3.3186355 -3.7805786 -3.3383279 -2.5738962 -2.0133703][-2.3433533 -2.2064934 -2.1279426 -2.1397111 -1.7992167 -0.53890979 1.0691555 1.2323287 -0.39233184 -2.5515196 -3.9321079 -4.1022644 -3.4298973 -2.5397408 -1.9638519][-2.3400731 -2.1281686 -1.9446813 -1.8936613 -1.579165 -0.41548038 1.021704 1.038955 -0.62474763 -2.7319825 -3.9765575 -4.0463452 -3.3060884 -2.4216149 -1.8784897][-2.3652873 -2.0746756 -1.7593737 -1.5818156 -1.2129257 -0.033258438 1.4703064 1.5036244 -0.24081397 -2.4248185 -3.7227697 -3.8283525 -3.1379809 -2.3112895 -1.8143044][-2.3653066 -2.023978 -1.6074378 -1.3337798 -0.85748994 0.5200243 2.2100141 2.2876966 0.42134333 -1.9440796 -3.3864727 -3.5892842 -2.9949465 -2.2356026 -1.7758733][-2.1217961 -1.8264236 -1.4038646 -1.0681056 -0.48393226 1.0774827 2.816222 2.7085259 0.61462021 -1.8714702 -3.3068745 -3.4914775 -2.9354291 -2.205189 -1.7600533]]...]
INFO - root - 2017-12-16 08:20:22.347362: step 11110, loss = 0.50, batch loss = 0.24 (44.1 examples/sec; 0.182 sec/batch; 16h:12m:35s remains)
INFO - root - 2017-12-16 08:20:24.019212: step 11120, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:36m:54s remains)
INFO - root - 2017-12-16 08:20:25.683955: step 11130, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 15h:04m:14s remains)
INFO - root - 2017-12-16 08:20:27.364223: step 11140, loss = 0.59, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 14h:58m:50s remains)
INFO - root - 2017-12-16 08:20:29.026076: step 11150, loss = 0.81, batch loss = 0.55 (48.7 examples/sec; 0.164 sec/batch; 14h:40m:22s remains)
INFO - root - 2017-12-16 08:20:30.671051: step 11160, loss = 0.62, batch loss = 0.35 (46.7 examples/sec; 0.171 sec/batch; 15h:17m:49s remains)
INFO - root - 2017-12-16 08:20:32.372457: step 11170, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 15h:02m:10s remains)
INFO - root - 2017-12-16 08:20:34.066106: step 11180, loss = 0.62, batch loss = 0.36 (46.8 examples/sec; 0.171 sec/batch; 15h:14m:54s remains)
INFO - root - 2017-12-16 08:20:35.718164: step 11190, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 14h:59m:37s remains)
INFO - root - 2017-12-16 08:20:37.376711: step 11200, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:59m:45s remains)
2017-12-16 08:20:37.885122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.225149 -1.1762986 -1.1896355 -1.255536 -1.3057284 -1.3004134 -1.2405916 -1.1388643 -1.0557162 -1.039862 -1.0211229 -0.91926146 -0.7286787 -0.57032061 -0.52371943][-1.5209681 -1.6158178 -1.8107182 -2.0570621 -2.2589869 -2.3570778 -2.3825297 -2.3837779 -2.3650723 -2.3554668 -2.3243661 -2.1491234 -1.8876483 -1.6583915 -1.517669][-2.08593 -2.3842473 -2.7277625 -3.042717 -3.2485902 -3.2977688 -3.2804456 -3.3963127 -3.5299044 -3.5731392 -3.53548 -3.3666883 -3.1279256 -2.8792863 -2.6634119][-2.6175668 -3.0460796 -3.412714 -3.6016881 -3.5904522 -3.3905931 -3.1441913 -3.2906206 -3.6407943 -3.8501687 -3.9258513 -3.9421229 -3.8886614 -3.7409372 -3.4853554][-2.6469505 -2.9740887 -3.1132567 -2.9526742 -2.5689659 -1.9293673 -1.2393367 -1.3109138 -2.0273254 -2.6538668 -3.1160965 -3.5558619 -3.9043651 -3.9699564 -3.7786345][-2.1243205 -2.1030383 -1.7993739 -1.1000178 -0.16279864 1.1435878 2.5044842 2.663197 1.4491699 0.064658165 -1.0544732 -2.1709592 -3.0956564 -3.572722 -3.6227341][-1.5313607 -1.157207 -0.40720785 0.83188319 2.4206843 4.5122223 6.6855416 7.2900844 5.6613445 3.4408336 1.5287552 -0.27002621 -1.795548 -2.7822075 -3.2238944][-1.4451962 -1.0632077 -0.23580956 1.1639128 3.1222692 5.7104764 8.4696617 9.6062527 7.9817815 5.4133663 3.0941048 1.0073872 -0.76026225 -1.9975226 -2.7419538][-1.958077 -1.9895482 -1.5960712 -0.58212912 1.1033568 3.4001055 5.8396945 7.0058675 6.05718 4.11775 2.2166686 0.49840951 -0.91965258 -1.9045982 -2.5938983][-2.5721765 -3.1563597 -3.4706268 -3.1662951 -2.1552939 -0.60444677 1.1030788 2.0295715 1.6874998 0.719558 -0.28588343 -1.2842282 -2.0779438 -2.5361433 -2.8837872][-2.7654932 -3.6758571 -4.5052233 -4.8544064 -4.5359249 -3.7132709 -2.7330434 -2.1159947 -2.1516333 -2.4230735 -2.6093976 -2.8834252 -3.1216958 -3.1287861 -3.0694242][-2.4650002 -3.3359537 -4.2644935 -4.8999062 -5.0087442 -4.6891403 -4.2274847 -3.9655266 -4.0016112 -3.9745717 -3.6882467 -3.4560473 -3.3350368 -3.1256268 -2.8002765][-1.9779279 -2.6055188 -3.3255363 -3.8999162 -4.1105251 -4.0099978 -3.8208704 -3.8041396 -3.9677663 -3.9375772 -3.554976 -3.1342144 -2.8735719 -2.6573498 -2.2924752][-1.5512331 -1.8982762 -2.3408082 -2.7277195 -2.897326 -2.8565145 -2.7914541 -2.904654 -3.134685 -3.1649041 -2.8696442 -2.4966087 -2.2929575 -2.184093 -1.9505171][-1.3160608 -1.4697311 -1.6884625 -1.8956885 -1.9994183 -1.9927118 -1.9884403 -2.1204362 -2.2970421 -2.3426089 -2.212765 -2.0320206 -1.9673593 -1.9756107 -1.851034]]...]
INFO - root - 2017-12-16 08:20:39.537395: step 11210, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 14h:34m:43s remains)
INFO - root - 2017-12-16 08:20:41.215193: step 11220, loss = 0.51, batch loss = 0.25 (49.1 examples/sec; 0.163 sec/batch; 14h:33m:07s remains)
INFO - root - 2017-12-16 08:20:42.900870: step 11230, loss = 0.72, batch loss = 0.46 (48.2 examples/sec; 0.166 sec/batch; 14h:48m:32s remains)
INFO - root - 2017-12-16 08:20:44.580330: step 11240, loss = 0.66, batch loss = 0.40 (47.8 examples/sec; 0.167 sec/batch; 14h:55m:14s remains)
INFO - root - 2017-12-16 08:20:46.230087: step 11250, loss = 0.60, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 14h:36m:24s remains)
INFO - root - 2017-12-16 08:20:47.893054: step 11260, loss = 0.70, batch loss = 0.44 (47.4 examples/sec; 0.169 sec/batch; 15h:03m:52s remains)
INFO - root - 2017-12-16 08:20:49.554102: step 11270, loss = 0.52, batch loss = 0.26 (45.3 examples/sec; 0.177 sec/batch; 15h:45m:41s remains)
INFO - root - 2017-12-16 08:20:51.249765: step 11280, loss = 0.62, batch loss = 0.36 (46.9 examples/sec; 0.171 sec/batch; 15h:12m:58s remains)
INFO - root - 2017-12-16 08:20:52.938478: step 11290, loss = 0.60, batch loss = 0.33 (46.0 examples/sec; 0.174 sec/batch; 15h:31m:24s remains)
INFO - root - 2017-12-16 08:20:54.615078: step 11300, loss = 0.53, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 14h:48m:55s remains)
2017-12-16 08:20:55.066052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.86169994 -0.64316475 -0.46712947 -0.62276268 -0.99205673 -1.3600597 -1.7239033 -2.0643854 -2.2063048 -2.1495919 -2.050838 -2.0074234 -2.0040691 -1.9806831 -1.9184495][-0.35542881 -0.17008519 -0.10570717 -0.35609937 -0.76394486 -1.1203054 -1.4858303 -1.8484299 -2.0673373 -2.146605 -2.1846597 -2.2358558 -2.2820661 -2.3217013 -2.3481631][0.066967249 0.17786789 0.16803455 -0.08759594 -0.45542121 -0.7556932 -1.0691178 -1.3990097 -1.6749194 -1.9069244 -2.1393509 -2.3458207 -2.5094879 -2.6571302 -2.7859867][0.35068178 0.43264031 0.4575901 0.28075457 -0.030608416 -0.32693529 -0.577971 -0.83235753 -1.1390023 -1.5196829 -1.9520698 -2.3405421 -2.6354578 -2.8883233 -3.0903842][0.51702237 0.66065097 0.76264906 0.68571949 0.46512985 0.21692252 0.039022446 -0.15852237 -0.53349435 -1.06211 -1.6306133 -2.1166122 -2.4679453 -2.7577741 -2.9897177][0.5952282 0.89893389 1.1594167 1.2589707 1.1781535 1.0164447 0.90311313 0.67770529 0.19175076 -0.46054137 -1.1142386 -1.654875 -2.0234714 -2.3025405 -2.5331976][0.41028595 0.92664671 1.3804615 1.7095916 1.826524 1.8179948 1.7930663 1.5797937 1.0534272 0.36256433 -0.32041216 -0.89642215 -1.2808248 -1.5527072 -1.8026141][-0.013463736 0.63836932 1.2151971 1.6348329 1.8427384 1.9370501 2.039573 2.0014689 1.6536493 1.1063406 0.533885 0.040268183 -0.28535151 -0.50075567 -0.73577857][-0.67115653 -0.050171614 0.4769001 0.84258461 1.0418222 1.1822476 1.3834515 1.5480978 1.5211058 1.3332083 1.0808024 0.83736444 0.68946958 0.60608077 0.47401285][-1.5410411 -1.0730143 -0.66325366 -0.36741889 -0.1738565 0.0087859631 0.26305842 0.54583716 0.787071 0.99503136 1.1286552 1.218344 1.2919676 1.3521903 1.3487797][-2.2850494 -2.0593107 -1.8338864 -1.6350663 -1.4611347 -1.2654166 -1.0076756 -0.67880106 -0.26506877 0.22210026 0.67851591 1.0289173 1.2617049 1.410115 1.5037541][-2.5816977 -2.5723403 -2.5293126 -2.4661262 -2.380985 -2.2605865 -2.0800681 -1.8035338 -1.359056 -0.76763475 -0.16060925 0.31831408 0.60569024 0.76504874 0.93301296][-2.4860919 -2.5590672 -2.6098914 -2.6608419 -2.7092078 -2.733983 -2.70199 -2.5685046 -2.2555921 -1.7713562 -1.2352285 -0.78216136 -0.50190973 -0.35164249 -0.15962291][-2.2350702 -2.2807992 -2.3379037 -2.4347856 -2.5621912 -2.6882195 -2.7781961 -2.7984011 -2.6789293 -2.419431 -2.0956566 -1.7965269 -1.6038949 -1.4888791 -1.3035235][-1.8956053 -1.858319 -1.8734035 -1.9809333 -2.1601248 -2.3471179 -2.4986546 -2.5965183 -2.5979292 -2.4994636 -2.3648381 -2.2586744 -2.2108583 -2.1865294 -2.0853751]]...]
INFO - root - 2017-12-16 08:20:56.719673: step 11310, loss = 0.63, batch loss = 0.37 (48.8 examples/sec; 0.164 sec/batch; 14h:37m:24s remains)
INFO - root - 2017-12-16 08:20:58.379131: step 11320, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:56m:02s remains)
INFO - root - 2017-12-16 08:21:00.043134: step 11330, loss = 0.54, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 15h:00m:05s remains)
INFO - root - 2017-12-16 08:21:01.746284: step 11340, loss = 0.48, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 14h:49m:38s remains)
INFO - root - 2017-12-16 08:21:03.426370: step 11350, loss = 0.59, batch loss = 0.33 (49.1 examples/sec; 0.163 sec/batch; 14h:32m:55s remains)
INFO - root - 2017-12-16 08:21:05.100887: step 11360, loss = 0.57, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 14h:36m:35s remains)
INFO - root - 2017-12-16 08:21:06.752284: step 11370, loss = 0.49, batch loss = 0.23 (46.6 examples/sec; 0.172 sec/batch; 15h:19m:48s remains)
INFO - root - 2017-12-16 08:21:08.449303: step 11380, loss = 0.57, batch loss = 0.31 (46.1 examples/sec; 0.174 sec/batch; 15h:28m:59s remains)
INFO - root - 2017-12-16 08:21:10.127421: step 11390, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 14h:42m:52s remains)
INFO - root - 2017-12-16 08:21:11.808170: step 11400, loss = 0.51, batch loss = 0.25 (48.5 examples/sec; 0.165 sec/batch; 14h:42m:36s remains)
2017-12-16 08:21:12.280305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.84010541 -0.8632865 -0.91206336 -0.97108412 -1.0219445 -1.0625004 -1.0850844 -1.0871075 -1.0668285 -1.0311692 -0.98723984 -0.93065536 -0.87100434 -0.815138 -0.77113962][-1.6161079 -1.644071 -1.6896991 -1.7448869 -1.7914802 -1.8231448 -1.829497 -1.8255618 -1.8039178 -1.7810059 -1.7535508 -1.7205877 -1.6816986 -1.6343977 -1.581349][-2.7216766 -2.73551 -2.7363765 -2.7297983 -2.7040074 -2.65192 -2.5824018 -2.5324275 -2.5052702 -2.5197511 -2.5620296 -2.6354036 -2.7139122 -2.7700534 -2.7708237][-3.7115076 -3.7040064 -3.6390181 -3.514226 -3.33399 -3.1216414 -2.9042144 -2.729161 -2.6468484 -2.6741302 -2.8242638 -3.0665998 -3.3612585 -3.6235023 -3.774549][-4.2627091 -4.2090077 -4.0649452 -3.8213024 -3.4979351 -3.1300573 -2.7525163 -2.4183762 -2.2192512 -2.2118936 -2.4058652 -2.7685459 -3.2476988 -3.7314792 -4.1236806][-4.3294849 -4.2478976 -4.0759096 -3.8132234 -3.4898226 -3.1131258 -2.6664214 -2.200387 -1.8622341 -1.7297318 -1.8019967 -2.0691276 -2.5065432 -3.0771432 -3.6769924][-3.8733292 -3.7899463 -3.6713181 -3.5325425 -3.3737295 -3.1138361 -2.7652206 -2.3851101 -2.0327048 -1.7551808 -1.5610723 -1.4672773 -1.5724914 -1.9827 -2.6088283][-2.8240581 -2.8511283 -2.9419677 -3.080333 -3.186471 -3.149565 -3.0320873 -2.9328194 -2.7961864 -2.5245066 -2.1396108 -1.6449797 -1.2656214 -1.2455828 -1.6179218][-1.5883045 -1.786757 -2.1304541 -2.5821347 -2.9847634 -3.2086372 -3.4242907 -3.6969161 -3.8861232 -3.80928 -3.3899279 -2.6316993 -1.8432944 -1.3514601 -1.3194175][-0.64948809 -0.968096 -1.4653629 -2.1035454 -2.7155197 -3.1736491 -3.6589022 -4.2617292 -4.7369404 -4.8843069 -4.5988455 -3.82861 -2.8405328 -2.0168092 -1.6025841][-0.26703024 -0.5088712 -0.94092393 -1.5868313 -2.2767427 -2.8751836 -3.5122373 -4.285924 -4.9602966 -5.2772331 -5.1420889 -4.5270767 -3.560266 -2.6044955 -1.9769081][-0.3075428 -0.29372048 -0.46495223 -0.93107796 -1.5919256 -2.254601 -2.8922973 -3.6530492 -4.3918924 -4.8185658 -4.8030396 -4.3299251 -3.4882665 -2.534416 -1.8122096][-0.52859867 -0.2408998 -0.075018406 -0.24806857 -0.72477591 -1.2578577 -1.7096331 -2.2660055 -2.9333377 -3.4567342 -3.645787 -3.3654273 -2.6516211 -1.7313056 -1.0021967][-0.71749496 -0.28219056 0.10273123 0.18751192 -0.020047188 -0.27667832 -0.4741255 -0.80537903 -1.3233966 -1.8851268 -2.2933578 -2.2224278 -1.5862219 -0.59147835 0.23354244][-0.72094655 -0.24524331 0.19441962 0.37252355 0.36278415 0.34485126 0.33948207 0.15759087 -0.28990674 -0.89942682 -1.4433055 -1.5237491 -0.93722391 0.1439898 1.1889253]]...]
INFO - root - 2017-12-16 08:21:13.971769: step 11410, loss = 0.60, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 14h:35m:12s remains)
INFO - root - 2017-12-16 08:21:15.616866: step 11420, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:49m:18s remains)
INFO - root - 2017-12-16 08:21:17.267830: step 11430, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 14h:50m:28s remains)
INFO - root - 2017-12-16 08:21:18.946050: step 11440, loss = 0.69, batch loss = 0.43 (48.5 examples/sec; 0.165 sec/batch; 14h:43m:31s remains)
INFO - root - 2017-12-16 08:21:20.597404: step 11450, loss = 0.65, batch loss = 0.39 (49.1 examples/sec; 0.163 sec/batch; 14h:31m:23s remains)
INFO - root - 2017-12-16 08:21:22.269483: step 11460, loss = 0.51, batch loss = 0.24 (48.0 examples/sec; 0.167 sec/batch; 14h:51m:10s remains)
INFO - root - 2017-12-16 08:21:23.918816: step 11470, loss = 0.54, batch loss = 0.28 (49.5 examples/sec; 0.162 sec/batch; 14h:24m:28s remains)
INFO - root - 2017-12-16 08:21:25.598383: step 11480, loss = 0.56, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 14h:38m:59s remains)
INFO - root - 2017-12-16 08:21:27.282918: step 11490, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 14h:52m:54s remains)
INFO - root - 2017-12-16 08:21:28.938966: step 11500, loss = 0.60, batch loss = 0.34 (49.6 examples/sec; 0.161 sec/batch; 14h:23m:35s remains)
2017-12-16 08:21:29.408800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5281062 -1.3999732 -1.4711988 -1.5810047 -1.6604447 -1.7777517 -1.9382427 -1.9754252 -1.8911533 -1.773626 -1.6490936 -1.5730588 -1.4294937 -1.3397553 -1.4883332][-1.6004386 -1.4820099 -1.6637832 -1.8552687 -1.9573704 -2.0896559 -2.2504745 -2.2432225 -2.1509433 -1.988899 -1.7820857 -1.6167238 -1.3919842 -1.2257267 -1.4399202][-1.8023169 -1.6866224 -1.9780022 -2.2499609 -2.3751531 -2.5327339 -2.6716144 -2.6207721 -2.5507669 -2.3793993 -2.1009407 -1.8817611 -1.6186439 -1.3678331 -1.6220586][-1.979175 -1.7855864 -2.0429876 -2.3191805 -2.5023382 -2.722033 -2.8146014 -2.7625186 -2.8129361 -2.6731911 -2.3194029 -2.0759592 -1.8661478 -1.593974 -1.8870785][-2.0832696 -1.8143319 -1.8919511 -2.0586517 -2.2544317 -2.4353354 -2.3656497 -2.1914003 -2.3888063 -2.4274116 -2.1548028 -1.9948163 -1.8609337 -1.5639553 -1.8666677][-2.1445339 -1.8558464 -1.8201461 -1.841218 -1.9293323 -1.935524 -1.5298865 -1.1211925 -1.4884565 -1.8095845 -1.7166066 -1.6624489 -1.6282396 -1.3314669 -1.6662264][-2.0946665 -1.8387541 -1.7882228 -1.7415361 -1.7242689 -1.5897198 -0.88872612 -0.24106407 -0.75663114 -1.3801005 -1.4539087 -1.4236333 -1.4305243 -1.2114204 -1.5283897][-1.9934119 -1.7388639 -1.6780732 -1.6043525 -1.5803047 -1.4115627 -0.59875739 0.18548608 -0.46310127 -1.2875268 -1.4101895 -1.3288095 -1.3083751 -1.1725106 -1.4671832][-1.9384643 -1.7170415 -1.6251929 -1.5090544 -1.4549263 -1.2601051 -0.43601811 0.40689135 -0.23778534 -1.1829091 -1.4034286 -1.3488495 -1.3221357 -1.2856429 -1.5379108][-1.9866967 -1.8493311 -1.796701 -1.7041266 -1.6087537 -1.3726223 -0.53087533 0.3360765 -0.23883867 -1.2018169 -1.6014316 -1.6074333 -1.5854158 -1.5769818 -1.7156702][-2.0246239 -2.0164177 -2.0661886 -2.0721221 -2.0143528 -1.8316472 -1.0820156 -0.29058456 -0.75481594 -1.6157513 -2.0538433 -2.0748734 -2.04583 -2.0091126 -1.9621687][-1.9164724 -1.9814843 -2.108856 -2.2015767 -2.2287924 -2.1515133 -1.6218383 -1.0419631 -1.3816159 -1.9925689 -2.3297985 -2.3510811 -2.3120515 -2.2272525 -2.0625465][-1.7536035 -1.8242983 -1.9689697 -2.0904133 -2.1447902 -2.144495 -1.8634012 -1.5409126 -1.7364606 -2.0681677 -2.2404871 -2.2540991 -2.2514515 -2.1718755 -2.01157][-1.6352128 -1.7019546 -1.8270161 -1.934323 -1.9950693 -2.0221052 -1.8893135 -1.7335181 -1.8127375 -1.9615477 -2.0657427 -2.0880866 -2.0805395 -2.0047622 -1.8811748][-1.567655 -1.6292442 -1.7314521 -1.819057 -1.8734752 -1.9023226 -1.8482484 -1.7802589 -1.7974327 -1.8504196 -1.9175746 -1.9370692 -1.9137951 -1.8409886 -1.7453054]]...]
INFO - root - 2017-12-16 08:21:31.042467: step 11510, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:37m:52s remains)
INFO - root - 2017-12-16 08:21:32.671991: step 11520, loss = 0.49, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 14h:51m:02s remains)
INFO - root - 2017-12-16 08:21:34.310393: step 11530, loss = 0.61, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 14h:48m:42s remains)
INFO - root - 2017-12-16 08:21:36.006227: step 11540, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 15h:00m:49s remains)
INFO - root - 2017-12-16 08:21:37.657336: step 11550, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:54m:53s remains)
INFO - root - 2017-12-16 08:21:39.314755: step 11560, loss = 0.55, batch loss = 0.29 (46.6 examples/sec; 0.172 sec/batch; 15h:17m:50s remains)
INFO - root - 2017-12-16 08:21:40.989122: step 11570, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:53m:24s remains)
INFO - root - 2017-12-16 08:21:42.656689: step 11580, loss = 0.52, batch loss = 0.26 (46.2 examples/sec; 0.173 sec/batch; 15h:26m:04s remains)
INFO - root - 2017-12-16 08:21:44.338538: step 11590, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 14h:57m:26s remains)
INFO - root - 2017-12-16 08:21:46.027229: step 11600, loss = 0.53, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 14h:25m:54s remains)
2017-12-16 08:21:46.519673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1096363 -2.3800478 -2.638721 -2.6432328 -2.2822945 -1.5287883 -0.8049109 -0.60913587 -1.3887554 -2.4959896 -3.192996 -3.2429287 -2.6271098 -1.6010795 -0.66967571][-2.689661 -2.7690504 -2.6779764 -2.3540995 -1.814844 -1.0127958 -0.22409129 0.057002544 -0.64945996 -1.8868978 -2.7480392 -2.8438673 -2.2590241 -1.3437433 -0.46267617][-3.1990919 -3.0631256 -2.6795812 -2.02625 -1.2438524 -0.37506926 0.46232629 0.78670526 0.16324139 -1.1723844 -2.2433085 -2.4807088 -2.0720632 -1.3509111 -0.62796283][-3.4975438 -3.1848321 -2.6112821 -1.7616572 -0.745237 0.2401979 1.1727457 1.6087453 0.98050451 -0.50027561 -1.7140009 -2.0679095 -1.9177394 -1.5601535 -1.0992582][-3.4613328 -3.0347106 -2.4353437 -1.4975719 -0.3299309 0.80581236 1.8730428 2.4228117 1.8173473 0.24968767 -1.0655835 -1.5733728 -1.6526349 -1.661819 -1.5023333][-3.0583446 -2.6667042 -2.1854804 -1.2829775 -0.11020017 1.0228815 2.1409156 2.8013108 2.2205112 0.68468595 -0.58725631 -1.1346065 -1.4392655 -1.7628951 -1.8712147][-2.43499 -2.1107974 -1.7791886 -1.1002903 -0.18586087 0.81769633 1.9125173 2.5736387 2.072614 0.75281882 -0.36311448 -0.92899609 -1.436131 -1.9613712 -2.2203894][-1.6532217 -1.3726605 -1.1416235 -0.76263976 -0.24295115 0.48555684 1.4278553 2.0489118 1.7251813 0.6869514 -0.22767949 -0.81576145 -1.4593427 -2.1126297 -2.4048805][-0.92713237 -0.57713711 -0.30485833 -0.22601509 -0.16601276 0.14466619 0.78627634 1.3786209 1.3255901 0.70879483 0.053285837 -0.49218524 -1.2389812 -2.038681 -2.3025746][-0.43428767 -0.0044255257 0.34577966 0.28307104 -0.022214174 -0.14601135 0.20202541 0.7399478 0.94933152 0.76832223 0.36293173 -0.18603563 -1.117816 -2.0753675 -2.3206127][-0.17041683 0.26291466 0.57246447 0.37246394 -0.083619118 -0.42302847 -0.28169394 0.20687652 0.60178518 0.69650936 0.44059134 -0.21251845 -1.3386589 -2.4336231 -2.7380772][-0.23643088 0.096391678 0.26486182 -0.045888662 -0.63022184 -1.0205777 -1.0140356 -0.69570374 -0.2911253 -0.056465387 -0.22580314 -0.87231517 -1.9971329 -3.0143833 -3.2612655][-0.46873486 -0.2946043 -0.3238337 -0.73572445 -1.3361912 -1.7157006 -1.7977948 -1.6767036 -1.3759127 -1.0808103 -1.1413393 -1.6634976 -2.5679951 -3.3247898 -3.4178705][-0.82591152 -0.81997418 -0.93384504 -1.2580823 -1.6823294 -1.9260312 -1.9551624 -1.9238837 -1.7467511 -1.4844459 -1.5389564 -1.9790902 -2.6453249 -3.1226544 -3.0609002][-1.3834937 -1.4693103 -1.4865381 -1.5726712 -1.6721255 -1.6433959 -1.5253868 -1.518584 -1.4396446 -1.2794074 -1.3728353 -1.7136531 -2.1529553 -2.4447055 -2.3352382]]...]
INFO - root - 2017-12-16 08:21:48.250682: step 11610, loss = 0.57, batch loss = 0.31 (47.4 examples/sec; 0.169 sec/batch; 15h:03m:02s remains)
INFO - root - 2017-12-16 08:21:49.949305: step 11620, loss = 0.76, batch loss = 0.50 (47.2 examples/sec; 0.169 sec/batch; 15h:05m:37s remains)
INFO - root - 2017-12-16 08:21:51.621550: step 11630, loss = 0.55, batch loss = 0.29 (48.4 examples/sec; 0.165 sec/batch; 14h:44m:28s remains)
INFO - root - 2017-12-16 08:21:53.289143: step 11640, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 14h:55m:17s remains)
INFO - root - 2017-12-16 08:21:54.956880: step 11650, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:53m:16s remains)
INFO - root - 2017-12-16 08:21:56.639011: step 11660, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:55m:57s remains)
INFO - root - 2017-12-16 08:21:58.332372: step 11670, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 14h:56m:07s remains)
INFO - root - 2017-12-16 08:22:00.018438: step 11680, loss = 0.58, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:40m:38s remains)
INFO - root - 2017-12-16 08:22:01.708588: step 11690, loss = 0.65, batch loss = 0.39 (46.2 examples/sec; 0.173 sec/batch; 15h:26m:42s remains)
INFO - root - 2017-12-16 08:22:03.396818: step 11700, loss = 0.54, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 14h:45m:21s remains)
2017-12-16 08:22:03.862033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8945711 -1.7721353 -1.792443 -1.8929633 -2.0691011 -2.2819035 -2.4227104 -2.3751035 -2.1978819 -2.0401497 -1.9539878 -1.9492409 -2.0048051 -2.1212492 -2.2074199][-2.8440886 -2.8182371 -2.8042872 -2.842407 -3.0196643 -3.2723339 -3.4099088 -3.3271556 -3.0610013 -2.7697225 -2.5362267 -2.3546088 -2.1685464 -2.028163 -1.9737818][-3.4590721 -3.4904485 -3.4261084 -3.357368 -3.3958776 -3.5563011 -3.669755 -3.5621161 -3.2422841 -2.8847802 -2.599611 -2.3202343 -1.9232519 -1.5432491 -1.3482739][-3.2986648 -3.3124619 -3.1528692 -2.9274583 -2.7513793 -2.780195 -2.8648224 -2.8027678 -2.5613737 -2.3005011 -2.1552193 -1.9610677 -1.5375457 -1.0683049 -0.83932626][-2.2941656 -2.2493346 -2.0220945 -1.6334177 -1.2581302 -1.1489598 -1.1825217 -1.1739613 -1.0254705 -0.9389683 -1.0546641 -1.1042627 -0.83697903 -0.46924913 -0.31312215][-0.98860061 -0.88258553 -0.5946542 -0.061601639 0.54540181 0.80310988 0.74302244 0.66884804 0.78090262 0.8123281 0.51863241 0.22336221 0.22636604 0.36240482 0.35390353][-0.057978868 0.13118577 0.52988696 1.267174 2.0215585 2.3281138 2.277313 2.2478821 2.4229138 2.451102 2.0197651 1.4544141 1.1562457 1.0532691 0.84947753][0.47628546 0.78593612 1.2547898 2.0907228 2.8606942 3.0890524 2.9546344 2.9888232 3.3153179 3.351207 2.7239487 1.9464009 1.4420271 1.099334 0.69364262][0.66338825 0.956372 1.3446467 1.9869187 2.5249851 2.6017225 2.4345109 2.4604795 2.7353351 2.6810505 1.9905298 1.2010577 0.66151714 0.24409342 -0.21055269][0.24387145 0.42846131 0.64476824 0.98705268 1.2179174 1.1253905 0.92190576 0.906024 1.0541201 0.96049786 0.41811562 -0.22306108 -0.70415413 -1.0707039 -1.3793781][-0.6880796 -0.55250585 -0.510797 -0.4919647 -0.53570843 -0.71838045 -0.89992714 -0.97120905 -0.90354109 -0.99922109 -1.3779681 -1.831811 -2.1589544 -2.3572509 -2.4396641][-1.2387474 -1.1128922 -1.215211 -1.405509 -1.5998018 -1.8078967 -1.9588317 -2.0263166 -2.0399852 -2.1415732 -2.4188867 -2.7056623 -2.8462844 -2.8149 -2.624099][-1.28937 -1.1163694 -1.264918 -1.5196011 -1.7223985 -1.8738768 -1.9692854 -2.0127404 -2.051147 -2.1380374 -2.2943807 -2.416014 -2.3825362 -2.1794817 -1.7886375][-0.86010504 -0.59548473 -0.67795396 -0.88420641 -0.98593068 -1.0066235 -1.0042471 -1.0018958 -1.0408316 -1.10771 -1.1664922 -1.1823387 -1.1032813 -0.89476013 -0.5029484][-0.28747225 0.00072550774 -0.038709879 -0.19404006 -0.22381282 -0.15320158 -0.094707489 -0.088427305 -0.13048935 -0.17792487 -0.1933372 -0.18616366 -0.13176608 -0.0049016476 0.29016638]]...]
INFO - root - 2017-12-16 08:22:05.555272: step 11710, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:43m:24s remains)
INFO - root - 2017-12-16 08:22:07.252929: step 11720, loss = 0.52, batch loss = 0.26 (46.3 examples/sec; 0.173 sec/batch; 15h:23m:58s remains)
INFO - root - 2017-12-16 08:22:08.975675: step 11730, loss = 0.63, batch loss = 0.37 (47.4 examples/sec; 0.169 sec/batch; 15h:02m:11s remains)
INFO - root - 2017-12-16 08:22:10.633471: step 11740, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.167 sec/batch; 14h:54m:08s remains)
INFO - root - 2017-12-16 08:22:12.344403: step 11750, loss = 0.48, batch loss = 0.22 (41.6 examples/sec; 0.192 sec/batch; 17h:06m:59s remains)
INFO - root - 2017-12-16 08:22:14.020316: step 11760, loss = 0.55, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 14h:57m:10s remains)
INFO - root - 2017-12-16 08:22:15.698108: step 11770, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 15h:01m:56s remains)
INFO - root - 2017-12-16 08:22:17.389979: step 11780, loss = 0.63, batch loss = 0.37 (48.8 examples/sec; 0.164 sec/batch; 14h:36m:43s remains)
INFO - root - 2017-12-16 08:22:19.073535: step 11790, loss = 0.59, batch loss = 0.33 (48.4 examples/sec; 0.165 sec/batch; 14h:43m:05s remains)
INFO - root - 2017-12-16 08:22:20.781263: step 11800, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 14h:59m:51s remains)
2017-12-16 08:22:21.277540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8934412 -1.6811916 -1.4863745 -1.0799183 -0.59024978 -0.56558979 -0.88326979 -1.0700078 -1.1000693 -1.2812757 -1.6669487 -1.9838724 -1.9772154 -1.626893 -1.2059977][-1.8535149 -1.9083188 -1.9613198 -1.6510768 -1.0790179 -0.79335582 -0.9830662 -1.3633964 -1.7830093 -2.3063087 -2.8032756 -3.0622754 -2.8673499 -2.3165035 -1.8263744][-1.7778044 -2.1317031 -2.3865671 -2.1349671 -1.5330145 -1.031298 -1.0714238 -1.5802919 -2.2910974 -3.0703692 -3.6238892 -3.781692 -3.4181619 -2.6981795 -2.1830294][-1.4991553 -2.057776 -2.3727193 -2.1390536 -1.5728022 -1.0497134 -1.0411785 -1.644588 -2.5884893 -3.4459209 -3.9296815 -3.9130824 -3.4511321 -2.7045434 -2.1409423][-1.128211 -1.6872492 -2.0258873 -1.8427485 -1.3133036 -0.85247409 -0.86238313 -1.5559996 -2.5431089 -3.2471468 -3.554702 -3.4033036 -2.936007 -2.2821355 -1.7949996][-0.719326 -1.0920484 -1.4346544 -1.3595011 -0.90528607 -0.53106821 -0.60873139 -1.3198516 -2.1980999 -2.7020361 -2.7998257 -2.5971763 -2.2263658 -1.7835293 -1.444392][-0.31591988 -0.42522919 -0.689886 -0.706377 -0.343009 -0.057802439 -0.23005795 -0.93410325 -1.682796 -2.0270112 -2.0213969 -1.8174855 -1.5355049 -1.2428758 -1.0483435][0.053725243 0.17394257 0.051558971 -0.035474539 0.16601396 0.30300856 0.075749159 -0.5810442 -1.2467598 -1.5153847 -1.4843363 -1.3309021 -1.1176068 -0.8925916 -0.77585661][0.079021215 0.41981387 0.43538451 0.26493931 0.26720595 0.3231945 0.13552475 -0.45535505 -1.0643499 -1.3215882 -1.3515272 -1.3228292 -1.167587 -0.94626224 -0.84089947][-0.23219037 0.24193215 0.32181287 0.064591646 -0.0019197464 0.047149181 -0.13336515 -0.61983061 -1.1496133 -1.4235322 -1.5500457 -1.5907543 -1.4557781 -1.2213168 -1.1139803][-0.66843355 -0.24214029 -0.21655583 -0.53869092 -0.59603322 -0.49518657 -0.53888309 -0.85910153 -1.253232 -1.5044467 -1.6921489 -1.7727127 -1.6833825 -1.5020206 -1.4365107][-1.0567034 -0.85026956 -0.96023011 -1.2538335 -1.2712756 -1.0789014 -0.91506851 -0.97845805 -1.2128713 -1.4351119 -1.6422985 -1.7386954 -1.7096961 -1.6311646 -1.6467136][-1.2628074 -1.3031343 -1.5253296 -1.7354825 -1.6455059 -1.3201256 -0.98160064 -0.85796237 -0.94171119 -1.1566426 -1.3839874 -1.5206147 -1.5487669 -1.5729456 -1.6626896][-1.2980483 -1.4728262 -1.6959774 -1.7780169 -1.5344422 -1.0793862 -0.7114166 -0.55523121 -0.59335208 -0.82224119 -1.0788907 -1.2598013 -1.3212701 -1.3923421 -1.5318112][-1.1411765 -1.3158309 -1.4596348 -1.3871303 -1.0245545 -0.54348731 -0.25320935 -0.18652892 -0.30218744 -0.61966991 -0.92899954 -1.086744 -1.1065857 -1.1491063 -1.274447]]...]
INFO - root - 2017-12-16 08:22:22.975469: step 11810, loss = 0.59, batch loss = 0.33 (46.1 examples/sec; 0.173 sec/batch; 15h:26m:45s remains)
INFO - root - 2017-12-16 08:22:24.707958: step 11820, loss = 0.67, batch loss = 0.41 (46.0 examples/sec; 0.174 sec/batch; 15h:29m:00s remains)
INFO - root - 2017-12-16 08:22:26.418456: step 11830, loss = 0.50, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 14h:57m:45s remains)
INFO - root - 2017-12-16 08:22:28.100696: step 11840, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:40m:49s remains)
INFO - root - 2017-12-16 08:22:29.799541: step 11850, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 15h:06m:12s remains)
INFO - root - 2017-12-16 08:22:31.511683: step 11860, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 15h:00m:09s remains)
INFO - root - 2017-12-16 08:22:33.194765: step 11870, loss = 0.44, batch loss = 0.18 (46.9 examples/sec; 0.171 sec/batch; 15h:12m:23s remains)
INFO - root - 2017-12-16 08:22:34.883566: step 11880, loss = 0.50, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 15h:16m:18s remains)
INFO - root - 2017-12-16 08:22:36.552140: step 11890, loss = 0.62, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 14h:36m:23s remains)
INFO - root - 2017-12-16 08:22:38.209881: step 11900, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 14h:34m:06s remains)
2017-12-16 08:22:38.697710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7524064 -1.9693761 -1.9787837 -1.6964037 -1.3120329 -1.1116583 -1.0556754 -1.0930238 -1.230256 -1.4068863 -1.4333546 -1.3360004 -1.2574298 -1.3478878 -1.490229][-1.7609854 -2.0053439 -1.9917479 -1.7115867 -1.3977177 -1.2653939 -1.2149247 -1.176193 -1.1255643 -1.0958074 -0.99281847 -0.88575375 -0.87901771 -1.0465117 -1.1966962][-1.675808 -1.7645187 -1.6327927 -1.3687708 -1.2689476 -1.3268278 -1.36833 -1.3594944 -1.2246193 -1.0091612 -0.77954078 -0.64279437 -0.6854111 -0.87180209 -0.99341369][-1.5121853 -1.3527911 -1.0444528 -0.75382626 -0.79111874 -0.99893689 -1.235999 -1.4424783 -1.4321442 -1.2547882 -1.0350728 -0.93270504 -0.98092687 -1.0920012 -1.0888238][-1.4338646 -1.1548344 -0.73872828 -0.4027071 -0.39855778 -0.65243685 -1.0641367 -1.5479481 -1.7676549 -1.7521529 -1.7040298 -1.7074987 -1.72929 -1.697297 -1.4893332][-1.7148526 -1.4989743 -1.0672674 -0.61372852 -0.46403253 -0.6245079 -1.0502787 -1.6819408 -2.0667768 -2.1929762 -2.2892044 -2.3967829 -2.4033122 -2.2523212 -1.9432802][-2.1750748 -2.0972121 -1.6333221 -1.0511667 -0.691514 -0.59895372 -0.86361194 -1.5359168 -2.0345523 -2.2293236 -2.4170682 -2.6019974 -2.6196966 -2.4774339 -2.2492154][-2.3946695 -2.3889313 -1.8971345 -1.1624728 -0.53126514 -0.12393761 -0.18105555 -0.92682409 -1.5707127 -1.8354875 -2.0731964 -2.2839515 -2.2699654 -2.1728721 -2.0998304][-2.1320345 -2.1796458 -1.6857522 -0.82571173 0.078938723 0.81790185 0.92206359 0.024646521 -0.83224428 -1.2548664 -1.5462974 -1.6752121 -1.497128 -1.3253022 -1.3845983][-1.8322043 -1.9570298 -1.5479155 -0.65051961 0.49098825 1.5290294 1.8755829 0.97488928 -0.031508923 -0.60908282 -0.98658264 -1.0539412 -0.72131109 -0.4784224 -0.60131574][-1.9021825 -2.1524067 -1.937848 -1.1943247 -0.11006927 1.0391576 1.6594629 1.1974986 0.43645644 -0.074646711 -0.519894 -0.65055859 -0.35702622 -0.082794666 -0.10722733][-2.1027658 -2.4818466 -2.5521998 -2.1788006 -1.409389 -0.48698223 0.18008494 0.28230953 0.12918305 -0.1073153 -0.50511992 -0.65421104 -0.42066967 -0.14342833 -0.051497698][-2.1247416 -2.6576941 -3.0229273 -3.0129385 -2.5888205 -1.9578702 -1.3306262 -0.9425416 -0.73933971 -0.81350493 -1.0539424 -1.0425223 -0.72235525 -0.4322176 -0.28072786][-1.7973768 -2.4432967 -3.0251224 -3.2403519 -3.0702431 -2.6478026 -2.1619313 -1.8044972 -1.6361604 -1.6715204 -1.7345607 -1.5355649 -1.0776279 -0.80439818 -0.67032051][-1.1941556 -1.8089917 -2.4171371 -2.6889536 -2.6435351 -2.3943207 -2.09398 -1.9141438 -1.973757 -2.079185 -2.0433266 -1.7441462 -1.292814 -1.1670309 -1.1905279]]...]
INFO - root - 2017-12-16 08:22:40.365147: step 11910, loss = 0.57, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 15h:07m:46s remains)
INFO - root - 2017-12-16 08:22:42.067524: step 11920, loss = 0.61, batch loss = 0.34 (47.0 examples/sec; 0.170 sec/batch; 15h:09m:17s remains)
INFO - root - 2017-12-16 08:22:43.744446: step 11930, loss = 0.51, batch loss = 0.25 (46.8 examples/sec; 0.171 sec/batch; 15h:12m:42s remains)
INFO - root - 2017-12-16 08:22:45.434872: step 11940, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 15h:11m:47s remains)
INFO - root - 2017-12-16 08:22:47.132998: step 11950, loss = 0.62, batch loss = 0.36 (47.0 examples/sec; 0.170 sec/batch; 15h:08m:41s remains)
INFO - root - 2017-12-16 08:22:48.817177: step 11960, loss = 0.59, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 14h:41m:04s remains)
INFO - root - 2017-12-16 08:22:50.485587: step 11970, loss = 0.47, batch loss = 0.21 (46.5 examples/sec; 0.172 sec/batch; 15h:18m:52s remains)
INFO - root - 2017-12-16 08:22:52.158233: step 11980, loss = 0.58, batch loss = 0.32 (49.7 examples/sec; 0.161 sec/batch; 14h:19m:43s remains)
INFO - root - 2017-12-16 08:22:53.845514: step 11990, loss = 0.52, batch loss = 0.26 (45.2 examples/sec; 0.177 sec/batch; 15h:46m:12s remains)
INFO - root - 2017-12-16 08:22:55.547411: step 12000, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 14h:43m:14s remains)
2017-12-16 08:22:56.036366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2383516 -3.3971448 -2.9458199 -2.3105834 -1.9343247 -1.8031762 -1.9059775 -2.2683938 -2.7056692 -3.0611532 -3.1856542 -3.0482864 -2.7305918 -2.3966546 -2.1676459][-3.4993134 -3.3932765 -2.6115036 -1.5984504 -1.0128815 -0.88148296 -1.1480596 -1.6991518 -2.2443841 -2.7152138 -3.0100822 -3.0766587 -2.9366555 -2.7378397 -2.6235368][-3.6166506 -3.1311893 -1.8990192 -0.46190786 0.39095783 0.476017 -0.17642021 -1.0087436 -1.7065427 -2.2734315 -2.6975338 -2.9265442 -2.9479251 -2.8761759 -2.8453717][-3.577266 -2.6553955 -0.96401048 0.76492548 1.7017646 1.6140878 0.7361815 -0.32242584 -1.2227508 -1.9218451 -2.391511 -2.6251967 -2.7201767 -2.7449734 -2.7459981][-3.3875089 -2.1219871 -0.089261532 1.9025884 2.9617834 2.8937941 1.8511605 0.52212977 -0.62761819 -1.5230188 -2.0631576 -2.2414753 -2.3364391 -2.3918664 -2.400722][-3.094686 -1.8481016 0.047261 2.0945921 3.6003208 4.0447326 3.2600689 1.8734136 0.38903785 -0.91337919 -1.6584805 -1.7954609 -1.8237205 -1.8667768 -1.8526391][-2.8264549 -1.878635 -0.35077369 1.5355909 3.4209352 4.6096535 4.6463981 3.6082926 1.7179689 -0.15144062 -1.1933284 -1.3418434 -1.2397251 -1.1968567 -1.1538599][-2.8077528 -2.2114828 -1.1173716 0.40118742 2.2296748 3.8873744 4.8117223 4.4021 2.5327902 0.41281295 -0.84985662 -1.0148201 -0.76055932 -0.55631232 -0.45781469][-3.066859 -2.9227636 -2.3860435 -1.3416145 0.19981933 1.7799788 3.0415893 3.3809524 2.3180389 0.44512582 -0.87608409 -1.0103753 -0.60041964 -0.23050904 -0.084702015][-3.2837224 -3.4567945 -3.371866 -2.7723937 -1.5374866 -0.00034093857 1.4812632 2.3204756 1.7406626 0.11028838 -1.1928369 -1.3645587 -0.91860867 -0.4754802 -0.23830485][-3.2883747 -3.6468136 -3.8640389 -3.582583 -2.5566742 -1.0540047 0.47186065 1.4730217 1.080662 -0.46939397 -1.7959284 -2.0714858 -1.6740468 -1.2253323 -0.94647324][-3.0366952 -3.5210309 -3.8752542 -3.782249 -2.9702959 -1.6301808 -0.22763062 0.70796633 0.350389 -1.0758538 -2.3807738 -2.7777696 -2.5192418 -2.0809982 -1.7536783][-2.5717642 -3.114152 -3.5496316 -3.6149473 -3.0152321 -1.9982963 -0.78885567 0.055295229 -0.18826199 -1.4044538 -2.6139266 -3.0990489 -2.9551077 -2.6161492 -2.3400538][-2.0609913 -2.6307104 -3.1159537 -3.3118389 -2.9577615 -2.1958973 -1.1776465 -0.38402319 -0.44784427 -1.4010787 -2.5382297 -3.069346 -3.0149221 -2.7058697 -2.4394867][-1.7535129 -2.2687743 -2.7538271 -3.0138066 -2.8563414 -2.3000364 -1.5403144 -0.76598418 -0.56572223 -1.1896533 -2.1130023 -2.6419351 -2.6545618 -2.3806999 -2.0765061]]...]
INFO - root - 2017-12-16 08:22:57.760146: step 12010, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:58m:46s remains)
INFO - root - 2017-12-16 08:22:59.450763: step 12020, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:47m:46s remains)
INFO - root - 2017-12-16 08:23:01.148262: step 12030, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 15h:24m:30s remains)
INFO - root - 2017-12-16 08:23:02.846160: step 12040, loss = 0.62, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 14h:55m:18s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:23:04.551066: step 12050, loss = 0.51, batch loss = 0.25 (45.5 examples/sec; 0.176 sec/batch; 15h:38m:41s remains)
INFO - root - 2017-12-16 08:23:06.236982: step 12060, loss = 0.55, batch loss = 0.29 (45.8 examples/sec; 0.175 sec/batch; 15h:33m:28s remains)
INFO - root - 2017-12-16 08:23:07.965615: step 12070, loss = 0.48, batch loss = 0.22 (45.6 examples/sec; 0.176 sec/batch; 15h:37m:19s remains)
INFO - root - 2017-12-16 08:23:09.672559: step 12080, loss = 0.49, batch loss = 0.23 (46.9 examples/sec; 0.171 sec/batch; 15h:10m:36s remains)
INFO - root - 2017-12-16 08:23:11.412718: step 12090, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:51m:02s remains)
INFO - root - 2017-12-16 08:23:13.080112: step 12100, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 14h:29m:23s remains)
2017-12-16 08:23:13.551699: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50829768 0.45772362 0.39072847 0.40022588 0.58545732 0.6255734 0.57200861 0.5601716 0.53075528 0.3825314 0.0561316 -0.38474762 -0.648456 -0.46623814 -0.12423706][0.43434358 0.50577116 0.53939533 0.6954031 1.0432048 1.1853919 1.0947962 1.0123208 0.90927482 0.60645247 0.0017642975 -0.80833089 -1.4838217 -1.7676444 -1.8054397][-0.43338561 -0.35614526 -0.32716358 -0.21970797 0.11154866 0.32056332 0.3003118 0.296499 0.23189473 -0.052930355 -0.74386704 -1.697239 -2.5410326 -3.0732458 -3.2809072][-1.585652 -1.620971 -1.7794337 -1.8564554 -1.648415 -1.4116857 -1.283569 -1.1747186 -1.1687293 -1.4117105 -2.0859094 -2.9791536 -3.7290702 -4.1559663 -4.2142005][-2.5499587 -2.6763644 -2.9826531 -3.1826448 -3.0130143 -2.5770414 -2.1303735 -1.8295898 -1.8345156 -2.1937609 -2.8514717 -3.6252761 -4.1949964 -4.347435 -4.0883327][-2.8237605 -3.0222282 -3.350915 -3.4996619 -3.1406724 -2.2569284 -1.3143559 -0.78398883 -0.873567 -1.4957294 -2.3340387 -3.0914507 -3.4717007 -3.3511529 -2.8369756][-2.2303457 -2.4375215 -2.7399993 -2.7792418 -2.1198819 -0.6950469 0.76494122 1.4348781 1.030071 -0.055848837 -1.2611179 -2.1294391 -2.3939974 -2.0577831 -1.3806889][-1.0573503 -1.3095732 -1.6001807 -1.5441236 -0.65550578 1.1091971 2.9012945 3.3717072 2.3833025 0.77629471 -0.76975739 -1.7810175 -2.0289176 -1.6188736 -0.9332056][-0.14181137 -0.47477865 -0.74100745 -0.61849046 0.28077602 1.8351238 3.1677482 3.2297795 2.0242159 0.28691077 -1.3362625 -2.4105225 -2.6939111 -2.3523612 -1.7951008][-0.34940577 -0.70624232 -0.84118044 -0.54965961 0.24091935 1.299577 2.0664632 1.9005649 0.76447463 -0.84860539 -2.3542795 -3.3109426 -3.631846 -3.3994513 -2.9990489][-1.5135403 -1.6898127 -1.46615 -0.85488856 -0.091225624 0.56057239 0.94181705 0.66361189 -0.33447468 -1.6952181 -2.8588822 -3.5410995 -3.7725039 -3.6791005 -3.4841905][-2.6011658 -2.4614685 -1.8704809 -0.99722862 -0.24816608 0.1581471 0.31388998 0.071935654 -0.61883962 -1.581629 -2.3617098 -2.7811553 -2.9706447 -3.0772703 -3.1674113][-3.1271973 -2.7985277 -2.0542634 -1.1468776 -0.49123204 -0.21873641 -0.076510906 -0.11706734 -0.41136098 -0.87251616 -1.2126793 -1.4198987 -1.6716723 -2.0402257 -2.456387][-3.0013835 -2.6852851 -2.0475435 -1.3093495 -0.8090843 -0.61456072 -0.46340907 -0.30287576 -0.26091504 -0.29643297 -0.23964429 -0.23138523 -0.52436376 -1.1049607 -1.8125042][-2.5218921 -2.338474 -1.8458543 -1.2648711 -0.8409189 -0.6286453 -0.44694757 -0.25619674 -0.13765121 -0.045614243 0.15722084 0.28516078 -0.019556046 -0.71515846 -1.5625772]]...]
INFO - root - 2017-12-16 08:23:15.227641: step 12110, loss = 0.67, batch loss = 0.41 (47.0 examples/sec; 0.170 sec/batch; 15h:08m:17s remains)
INFO - root - 2017-12-16 08:23:16.914506: step 12120, loss = 0.51, batch loss = 0.25 (47.2 examples/sec; 0.170 sec/batch; 15h:05m:08s remains)
INFO - root - 2017-12-16 08:23:18.590398: step 12130, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 14h:47m:32s remains)
INFO - root - 2017-12-16 08:23:20.262331: step 12140, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 15h:08m:34s remains)
INFO - root - 2017-12-16 08:23:21.922957: step 12150, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 14h:45m:33s remains)
INFO - root - 2017-12-16 08:23:23.625469: step 12160, loss = 0.50, batch loss = 0.24 (44.3 examples/sec; 0.181 sec/batch; 16h:05m:05s remains)
INFO - root - 2017-12-16 08:23:25.343430: step 12170, loss = 0.61, batch loss = 0.35 (46.9 examples/sec; 0.171 sec/batch; 15h:10m:30s remains)
INFO - root - 2017-12-16 08:23:27.047913: step 12180, loss = 0.66, batch loss = 0.40 (46.7 examples/sec; 0.171 sec/batch; 15h:15m:24s remains)
INFO - root - 2017-12-16 08:23:28.724249: step 12190, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.166 sec/batch; 14h:44m:59s remains)
INFO - root - 2017-12-16 08:23:30.411701: step 12200, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 15h:06m:59s remains)
2017-12-16 08:23:30.900948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5663916 -1.8938941 -2.109808 -2.1232724 -2.1022904 -2.187254 -2.2293773 -1.9633778 -1.2797968 -0.28194857 0.65406609 1.2603507 1.3246872 0.83851552 -0.039305925][-1.5942225 -1.7709198 -1.7638927 -1.5959156 -1.5367187 -1.7432642 -1.9571527 -1.7209319 -0.86176479 0.1826942 0.91077256 1.1979969 0.95398068 0.22816944 -0.60880911][-2.1308827 -1.9662514 -1.6478471 -1.3449485 -1.1824303 -1.3470899 -1.5962226 -1.3064221 -0.23033786 0.88256025 1.3934579 1.3097184 0.7905128 -0.094221354 -0.94862175][-2.6103833 -2.1942031 -1.6870005 -1.3720384 -1.1677639 -1.1453232 -1.0790727 -0.48805797 0.7583127 1.9252796 2.2504067 1.7942061 0.96902895 -0.14598632 -1.1313158][-2.7062926 -2.2630053 -1.7616515 -1.4874763 -1.1870928 -0.72301912 -0.035078287 1.015367 2.2712803 3.1441603 3.0048175 2.0570831 0.85702944 -0.44642282 -1.4795117][-2.4886706 -2.2082217 -1.8060899 -1.4405276 -0.88630974 0.16579056 1.6433134 3.1371956 4.0040493 3.9924011 3.1053195 1.6857908 0.13868928 -1.1758617 -2.0200422][-1.9603492 -1.8775346 -1.5681368 -0.96563447 0.044365406 1.749342 3.998764 5.7845793 5.5425677 4.0976939 2.3712921 0.70896983 -0.83913839 -1.9664867 -2.5204451][-1.3109854 -1.2339073 -0.80738497 0.077116728 1.3794918 3.2159362 5.60964 7.2899981 5.6541557 3.1870894 1.1350746 -0.45370615 -1.7445621 -2.5637743 -2.7812672][-0.45004225 -0.2286315 0.30918479 1.2751789 2.4417849 3.7058034 4.8604732 5.0189018 3.4614534 1.2992053 -0.42997134 -1.6000512 -2.3952866 -2.8176954 -2.7126994][0.31627798 0.76033044 1.317204 2.0643282 2.7075076 2.9708409 2.7829018 2.0254312 0.77515268 -0.68839741 -1.8433862 -2.4701087 -2.7344868 -2.7009857 -2.3273783][0.855412 1.3484583 1.742908 1.9651389 1.9389925 1.463238 0.53016233 -0.51906288 -1.4142083 -2.1831641 -2.699445 -2.8638561 -2.7161808 -2.3090372 -1.7287657][1.2473342 1.5603964 1.5303807 1.2114911 0.65018511 -0.22954774 -1.3518152 -2.3559172 -2.8938942 -3.0281103 -3.008393 -2.8344135 -2.4325738 -1.8246933 -1.1360799][1.6322107 1.6184173 1.1272683 0.35996222 -0.5319463 -1.5274192 -2.5577621 -3.2620952 -3.3630054 -3.0818973 -2.781853 -2.4357786 -1.948181 -1.2914306 -0.61393428][1.9641361 1.5202463 0.62825847 -0.39458394 -1.3611598 -2.2366731 -2.9295912 -3.1966922 -2.9678097 -2.5529356 -2.1798394 -1.8309546 -1.3702135 -0.7343725 -0.041709661][1.7515149 0.93270278 -0.12937784 -1.1097159 -1.8700343 -2.3951869 -2.6474159 -2.5338335 -2.1984439 -1.8757312 -1.5967333 -1.3129294 -0.87011993 -0.21515989 0.4611423]]...]
INFO - root - 2017-12-16 08:23:32.599563: step 12210, loss = 0.57, batch loss = 0.31 (47.2 examples/sec; 0.169 sec/batch; 15h:04m:47s remains)
INFO - root - 2017-12-16 08:23:34.304080: step 12220, loss = 0.76, batch loss = 0.50 (45.8 examples/sec; 0.175 sec/batch; 15h:31m:33s remains)
INFO - root - 2017-12-16 08:23:35.968257: step 12230, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 15h:00m:50s remains)
INFO - root - 2017-12-16 08:23:37.679829: step 12240, loss = 0.57, batch loss = 0.31 (45.7 examples/sec; 0.175 sec/batch; 15h:33m:26s remains)
INFO - root - 2017-12-16 08:23:39.384369: step 12250, loss = 0.59, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 14h:56m:52s remains)
INFO - root - 2017-12-16 08:23:41.034228: step 12260, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.164 sec/batch; 14h:37m:42s remains)
INFO - root - 2017-12-16 08:23:42.697307: step 12270, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.163 sec/batch; 14h:28m:17s remains)
INFO - root - 2017-12-16 08:23:44.357726: step 12280, loss = 0.58, batch loss = 0.32 (48.8 examples/sec; 0.164 sec/batch; 14h:35m:10s remains)
INFO - root - 2017-12-16 08:23:46.029270: step 12290, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:48m:14s remains)
INFO - root - 2017-12-16 08:23:47.709373: step 12300, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 14h:54m:33s remains)
2017-12-16 08:23:48.157714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9554396 -1.9139161 -1.8139778 -1.8564882 -2.0134611 -2.1655805 -2.1783972 -2.0230336 -1.8378701 -1.7281775 -1.6313068 -1.5228658 -1.3779216 -1.2612996 -1.1989627][-2.5992146 -2.505363 -2.3440421 -2.2961497 -2.3761122 -2.5790155 -2.7033381 -2.5601139 -2.3162932 -2.1387959 -2.0060382 -1.9292246 -1.8524736 -1.8262486 -1.8689802][-2.7460237 -2.7243912 -2.5918903 -2.4798608 -2.4549489 -2.6605139 -2.9042573 -2.84962 -2.612421 -2.4218481 -2.3674679 -2.4150486 -2.52131 -2.6715593 -2.8110437][-2.1461647 -2.2446935 -2.1989124 -2.0236154 -1.909229 -2.133805 -2.5130262 -2.5912538 -2.4400983 -2.3729744 -2.5748188 -2.9564576 -3.3756368 -3.6993039 -3.8539479][-0.82009113 -1.0424043 -1.0776383 -0.76967037 -0.52730942 -0.7716974 -1.1862178 -1.3658389 -1.3908311 -1.5901713 -2.1188574 -2.9129665 -3.7133455 -4.177454 -4.29299][0.46660614 0.30507588 0.37052059 0.80596352 1.2186835 1.1717796 0.95418835 0.86617327 0.60801625 0.0441854 -0.81340659 -1.9466269 -3.0993943 -3.7032261 -3.7714772][0.77466774 0.93553448 1.2793963 1.8759232 2.4226408 2.7396789 3.057848 3.203742 2.6582994 1.7347336 0.70337844 -0.5892092 -1.9167171 -2.6211045 -2.6621482][0.29125142 0.69569659 1.2594366 2.0349112 2.7240381 3.3060002 4.080265 4.4326992 3.6518817 2.6154442 1.7040825 0.61517596 -0.65347993 -1.3480182 -1.4141519][-0.566136 -0.21342993 0.32961917 0.98910475 1.5656884 2.1527748 2.8727221 3.1389737 2.5257344 1.8517771 1.4593558 0.92936778 0.13817644 -0.39438558 -0.50405228][-1.1095188 -1.0501909 -0.91931629 -0.73400795 -0.51414359 -0.20338011 0.24579906 0.45453882 0.16074157 -0.064911842 0.019144297 0.073842764 -0.16129017 -0.46423554 -0.57364523][-1.1007935 -1.3921844 -1.7225162 -2.1460285 -2.4028132 -2.4893076 -2.3787274 -2.3084617 -2.3982348 -2.3382781 -1.9915634 -1.5997186 -1.4495718 -1.5020914 -1.5447531][-0.62579548 -1.1202213 -1.7638372 -2.7363617 -3.4672742 -3.9044471 -4.07337 -4.1138964 -4.0982771 -3.8988056 -3.4700737 -2.9981287 -2.7041745 -2.5773454 -2.5274622][0.10408759 -0.38902974 -1.1381081 -2.4749291 -3.6495066 -4.3243527 -4.6168489 -4.680541 -4.5957947 -4.3522058 -3.9754434 -3.6067905 -3.3299997 -3.1357098 -3.019012][0.82804227 0.49844503 -0.13087416 -1.4890203 -2.85379 -3.6624815 -3.9727919 -4.0361466 -3.8857234 -3.6054294 -3.3359988 -3.1617336 -3.0489836 -2.9476533 -2.9477367][1.0189188 0.88039279 0.5398674 -0.44096923 -1.6774762 -2.4538479 -2.7254524 -2.758656 -2.5413575 -2.2630095 -2.0661173 -2.0359402 -2.0467026 -2.083519 -2.2094839]]...]
INFO - root - 2017-12-16 08:23:49.826000: step 12310, loss = 0.55, batch loss = 0.29 (46.0 examples/sec; 0.174 sec/batch; 15h:28m:04s remains)
INFO - root - 2017-12-16 08:23:51.543760: step 12320, loss = 0.55, batch loss = 0.29 (45.8 examples/sec; 0.174 sec/batch; 15h:31m:10s remains)
INFO - root - 2017-12-16 08:23:53.257901: step 12330, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:43m:23s remains)
INFO - root - 2017-12-16 08:23:54.964852: step 12340, loss = 0.61, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 14h:48m:50s remains)
INFO - root - 2017-12-16 08:23:56.627044: step 12350, loss = 0.54, batch loss = 0.28 (49.7 examples/sec; 0.161 sec/batch; 14h:19m:00s remains)
INFO - root - 2017-12-16 08:23:58.290298: step 12360, loss = 0.44, batch loss = 0.17 (48.5 examples/sec; 0.165 sec/batch; 14h:40m:28s remains)
INFO - root - 2017-12-16 08:23:59.950688: step 12370, loss = 0.60, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:45m:58s remains)
INFO - root - 2017-12-16 08:24:01.600317: step 12380, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 15h:00m:27s remains)
INFO - root - 2017-12-16 08:24:03.280989: step 12390, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 14h:44m:25s remains)
INFO - root - 2017-12-16 08:24:04.962881: step 12400, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 15h:00m:04s remains)
2017-12-16 08:24:05.429873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3569551 -2.191988 -2.0321522 -1.9557354 -2.0945976 -2.4070818 -2.542861 -2.5208392 -2.4891791 -2.5083156 -2.502073 -2.4877357 -2.355063 -2.145617 -2.0399239][-2.332963 -2.1580756 -1.9925808 -1.9152352 -2.2047915 -2.7959213 -3.0996113 -3.1427948 -3.0991104 -3.0364258 -2.9098938 -2.792305 -2.5062447 -2.0392089 -1.7509573][-2.2024138 -2.1070037 -2.0300975 -2.0479646 -2.4419873 -3.1653125 -3.5419106 -3.5807996 -3.4749317 -3.3025258 -3.1550035 -3.0689487 -2.7219532 -2.1050439 -1.6709125][-1.771518 -1.7530656 -1.822183 -1.8818413 -2.2409787 -2.8600817 -3.120774 -3.0524979 -2.873292 -2.7178731 -2.6859159 -2.7550125 -2.5117967 -1.897059 -1.4654207][-1.16427 -1.2273494 -1.3819238 -1.3023645 -1.444591 -1.7533269 -1.7402146 -1.4385478 -1.1599423 -1.1546915 -1.4344242 -1.7978179 -1.8161652 -1.3693905 -1.0532331][-0.53133476 -0.64414644 -0.72638595 -0.41978669 -0.15367532 0.038444996 0.4757607 1.0400233 1.2910633 1.0018535 0.36922646 -0.3468684 -0.73350763 -0.61936462 -0.56389844][0.25255442 0.23976088 0.26459146 0.72355843 1.2971864 1.8909533 2.6653039 3.4198697 3.507621 2.880266 1.9487059 1.0105252 0.3990891 0.23266768 0.013405561][0.43737149 0.59964585 0.72647381 1.2925293 1.9669344 2.6457088 3.3826215 3.9361556 3.7902868 2.9930842 1.9515488 1.0865355 0.54183578 0.37528872 0.058935881][-0.51393223 -0.27140784 -0.046111584 0.51914239 1.1292903 1.5309181 1.7511322 1.8372056 1.5777631 0.9209199 0.0708251 -0.54462385 -0.78026271 -0.70100248 -0.84228325][-1.9697356 -1.7340888 -1.5130119 -1.0569965 -0.70950532 -0.76947546 -1.0780091 -1.3608382 -1.5680733 -1.925282 -2.3728983 -2.6551707 -2.6245546 -2.3491921 -2.2215912][-3.3589969 -3.1694276 -2.9661293 -2.6720498 -2.5948186 -2.9784496 -3.5658922 -3.9666815 -4.0484047 -4.1223822 -4.2450023 -4.3016682 -4.143827 -3.7740173 -3.4907978][-4.4667974 -4.3509536 -4.1649418 -3.9326892 -3.9334431 -4.4012103 -5.019474 -5.3488007 -5.342514 -5.2281566 -5.1618156 -5.1618958 -5.0046186 -4.7014589 -4.3400559][-4.6926394 -4.5852365 -4.3673811 -4.1286492 -4.1549153 -4.6199884 -5.1268406 -5.2800708 -5.1608839 -4.9235077 -4.7923 -4.8329515 -4.8604975 -4.7206163 -4.3840952][-3.9686975 -3.8021092 -3.5053344 -3.2564235 -3.2598412 -3.6227508 -3.9863439 -4.0480561 -3.9155002 -3.6298532 -3.4731708 -3.6277311 -3.8672385 -3.9266262 -3.6948895][-3.0076339 -2.7837982 -2.4590008 -2.1958513 -2.190954 -2.4931748 -2.7872384 -2.8208444 -2.6431286 -2.3830011 -2.2749763 -2.51543 -2.8383646 -2.9922051 -2.8846636]]...]
INFO - root - 2017-12-16 08:24:07.177487: step 12410, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:42m:01s remains)
INFO - root - 2017-12-16 08:24:08.871286: step 12420, loss = 0.53, batch loss = 0.27 (47.5 examples/sec; 0.168 sec/batch; 14h:57m:38s remains)
INFO - root - 2017-12-16 08:24:10.544802: step 12430, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 14h:49m:29s remains)
INFO - root - 2017-12-16 08:24:12.232893: step 12440, loss = 0.66, batch loss = 0.40 (45.4 examples/sec; 0.176 sec/batch; 15h:40m:31s remains)
INFO - root - 2017-12-16 08:24:13.930522: step 12450, loss = 0.54, batch loss = 0.28 (43.8 examples/sec; 0.183 sec/batch; 16h:13m:39s remains)
INFO - root - 2017-12-16 08:24:15.604530: step 12460, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 14h:49m:21s remains)
INFO - root - 2017-12-16 08:24:17.279505: step 12470, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 14h:34m:32s remains)
INFO - root - 2017-12-16 08:24:18.912531: step 12480, loss = 0.56, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 14h:35m:20s remains)
INFO - root - 2017-12-16 08:24:20.587728: step 12490, loss = 0.57, batch loss = 0.31 (46.3 examples/sec; 0.173 sec/batch; 15h:21m:45s remains)
INFO - root - 2017-12-16 08:24:22.268914: step 12500, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 14h:58m:04s remains)
2017-12-16 08:24:22.748082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2119079 -2.3171337 -2.4399197 -2.537745 -2.5678504 -2.6164 -2.8249834 -3.0859694 -3.2301409 -3.2180135 -3.0197744 -2.6173072 -2.1306067 -1.9545276 -2.0704627][-2.4385805 -2.67893 -2.8639302 -2.9999385 -3.0917742 -3.1659563 -3.2725425 -3.3136358 -3.2014775 -2.9844761 -2.6438868 -2.0900095 -1.5375373 -1.3879027 -1.5375009][-2.768517 -3.1326988 -3.3600979 -3.511621 -3.6154218 -3.6459584 -3.5364535 -3.2494924 -2.8469648 -2.4729583 -2.0318332 -1.4230936 -0.89630103 -0.80361307 -0.94015491][-3.0426834 -3.4784937 -3.6755521 -3.724087 -3.7130146 -3.5763979 -3.1369789 -2.5068505 -2.0065777 -1.7477443 -1.3822141 -0.79147255 -0.399989 -0.34778333 -0.40795875][-2.8870463 -3.2595878 -3.2512629 -3.0665166 -2.8769104 -2.4944241 -1.6409266 -0.77884138 -0.536399 -0.74754786 -0.64417553 -0.21473408 -0.00077295303 0.016838551 0.0065486431][-2.1425924 -2.3186772 -2.0140095 -1.5886447 -1.2462418 -0.63911307 0.60615039 1.6328604 1.3362918 0.42972326 0.1234107 0.30015516 0.28930426 0.17987275 0.033252954][-1.177829 -1.1414087 -0.60074604 0.023320913 0.46419406 1.2691715 3.0132935 4.3226929 3.3272216 1.596477 0.8094995 0.67643166 0.38308883 0.045212507 -0.32466006][-0.5885874 -0.35558796 0.39649868 1.0851107 1.4665768 2.370127 4.4813023 6.1543369 4.5367994 2.2948921 1.1725059 0.66097188 0.14711547 -0.25111413 -0.71307063][-0.63553095 -0.3013804 0.47672272 1.0762594 1.219027 1.783632 3.3547184 4.4325171 3.3191149 1.5874527 0.63435888 0.035170317 -0.47637916 -0.73938262 -1.0593373][-1.0329758 -0.79978454 -0.24946284 0.16324759 0.10827589 0.28182173 1.1907604 1.7474859 0.99924088 -0.13836741 -0.78821516 -1.2072828 -1.4827821 -1.508584 -1.5854838][-1.407771 -1.3828366 -1.1739211 -1.0127475 -1.1745666 -1.2289735 -0.84191835 -0.616081 -1.1598945 -1.8616203 -2.2374249 -2.4107289 -2.4482245 -2.290529 -2.1674664][-1.6332792 -1.7285289 -1.7661955 -1.7896029 -1.9966934 -2.20537 -2.1350915 -2.1289523 -2.5118272 -2.9093502 -3.0596051 -3.0252297 -2.8903785 -2.6749713 -2.4719815][-1.7144597 -1.8510607 -2.0003712 -2.132843 -2.3801088 -2.6729512 -2.7893722 -2.8712239 -3.0802946 -3.2504094 -3.2552936 -3.1126375 -2.9365165 -2.7520709 -2.5660172][-1.7302541 -1.8543469 -2.0048563 -2.2007751 -2.4961312 -2.8341904 -3.0309074 -3.1121535 -3.1706681 -3.1769469 -3.0985396 -2.9528034 -2.810149 -2.6943843 -2.5913796][-1.8616377 -1.9435587 -2.0447285 -2.2119913 -2.4642544 -2.7212858 -2.8683369 -2.9104214 -2.8750834 -2.794076 -2.6896181 -2.5871177 -2.5245175 -2.5229695 -2.5373988]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-12500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:24:25.056936: step 12510, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 14h:57m:42s remains)
INFO - root - 2017-12-16 08:24:26.733403: step 12520, loss = 0.55, batch loss = 0.29 (46.6 examples/sec; 0.172 sec/batch; 15h:16m:15s remains)
INFO - root - 2017-12-16 08:24:28.411969: step 12530, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 14h:39m:43s remains)
INFO - root - 2017-12-16 08:24:30.085765: step 12540, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.168 sec/batch; 14h:53m:23s remains)
INFO - root - 2017-12-16 08:24:31.764087: step 12550, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:54m:16s remains)
INFO - root - 2017-12-16 08:24:33.415026: step 12560, loss = 0.61, batch loss = 0.35 (47.1 examples/sec; 0.170 sec/batch; 15h:05m:17s remains)
INFO - root - 2017-12-16 08:24:35.119979: step 12570, loss = 0.58, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 15h:07m:44s remains)
INFO - root - 2017-12-16 08:24:36.851280: step 12580, loss = 0.51, batch loss = 0.25 (46.2 examples/sec; 0.173 sec/batch; 15h:24m:07s remains)
INFO - root - 2017-12-16 08:24:38.547873: step 12590, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:53m:58s remains)
INFO - root - 2017-12-16 08:24:40.245252: step 12600, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:49m:40s remains)
2017-12-16 08:24:40.704727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3009493 -2.8472705 -3.1233511 -3.1653693 -2.9969075 -2.5945067 -2.1636646 -1.8391912 -1.8067741 -2.0885375 -2.5326402 -2.8257973 -2.8885245 -2.6420212 -2.1365657][-2.1732945 -2.9956408 -3.460928 -3.585537 -3.4248393 -2.9236372 -2.2823439 -1.7183535 -1.4554548 -1.6857277 -2.2367382 -2.7242074 -2.9371982 -2.7407477 -2.1180902][-1.6677071 -2.8261375 -3.5160542 -3.7073438 -3.5315731 -2.9836257 -2.1856236 -1.4300463 -0.94483709 -1.1016306 -1.7992356 -2.520474 -2.88759 -2.7067895 -1.948068][-0.5418011 -1.9883904 -2.9616802 -3.2334158 -3.1373916 -2.7485502 -1.9982991 -1.1829287 -0.55049706 -0.56947184 -1.2870593 -2.1679881 -2.6362438 -2.394268 -1.5155826][0.88605595 -0.7423569 -1.9403917 -2.4475827 -2.6475263 -2.5807939 -2.03751 -1.2684268 -0.54286039 -0.40499794 -1.0463591 -1.9031104 -2.3225889 -2.0208333 -1.1294318][1.9688005 0.25853252 -1.1157579 -1.8215884 -2.2527328 -2.4285908 -2.0335276 -1.3542848 -0.59566689 -0.31775975 -0.8731972 -1.7018778 -2.1795666 -2.0257092 -1.2892016][2.0728216 0.52065897 -0.83684349 -1.584585 -1.9883974 -2.1306996 -1.730449 -1.0552133 -0.29926682 0.0053546429 -0.5289818 -1.5206095 -2.2579517 -2.3622453 -1.9216151][1.2084005 -0.028182507 -1.1713386 -1.7249286 -1.8680698 -1.7710261 -1.2623062 -0.58328092 0.10957432 0.39238143 -0.15513921 -1.2868242 -2.3149924 -2.7708902 -2.7116244][0.049025536 -0.82484019 -1.6546776 -1.988996 -1.9040607 -1.6329305 -1.1073496 -0.54489207 0.081931353 0.44304085 0.021287203 -1.0783749 -2.2426212 -3.0020933 -3.28715][-0.88565814 -1.5217705 -2.1223888 -2.3174469 -2.1354988 -1.7570622 -1.2816573 -0.87448442 -0.37751389 -0.05173111 -0.33375931 -1.2195799 -2.2411613 -2.9961529 -3.386672][-1.2849514 -1.7963998 -2.3050463 -2.4902763 -2.3608282 -2.0052018 -1.6165447 -1.3806409 -1.0474848 -0.8035965 -0.95246089 -1.5014317 -2.1715584 -2.6854804 -2.9664388][-1.2085639 -1.6041117 -2.0934613 -2.357688 -2.3284812 -2.06409 -1.7878597 -1.6637468 -1.4725111 -1.3358312 -1.3976045 -1.6321806 -1.9117734 -2.1233819 -2.2211685][-0.87949812 -1.167321 -1.689191 -2.0203264 -2.0670319 -1.8479338 -1.5915234 -1.534066 -1.5034053 -1.4633708 -1.4819973 -1.503345 -1.4506062 -1.3717637 -1.3154089][-0.47723603 -0.71563959 -1.2716025 -1.6173179 -1.710248 -1.5510935 -1.3036759 -1.2903763 -1.3898125 -1.4420645 -1.3687046 -1.1364133 -0.85386479 -0.58945715 -0.42000163][-0.10778379 -0.35196424 -0.90878475 -1.2062744 -1.3201054 -1.2733455 -1.1661189 -1.2431678 -1.4279697 -1.5155332 -1.315829 -0.845976 -0.35748911 0.015944481 0.20903039]]...]
INFO - root - 2017-12-16 08:24:42.373635: step 12610, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:45m:00s remains)
INFO - root - 2017-12-16 08:24:44.032287: step 12620, loss = 0.49, batch loss = 0.23 (49.5 examples/sec; 0.161 sec/batch; 14h:20m:52s remains)
INFO - root - 2017-12-16 08:24:45.686816: step 12630, loss = 0.62, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 14h:33m:37s remains)
INFO - root - 2017-12-16 08:24:47.358394: step 12640, loss = 0.49, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 14h:56m:04s remains)
INFO - root - 2017-12-16 08:24:49.045091: step 12650, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 14h:34m:31s remains)
INFO - root - 2017-12-16 08:24:50.712466: step 12660, loss = 0.46, batch loss = 0.20 (48.7 examples/sec; 0.164 sec/batch; 14h:35m:59s remains)
INFO - root - 2017-12-16 08:24:52.381167: step 12670, loss = 0.57, batch loss = 0.31 (48.8 examples/sec; 0.164 sec/batch; 14h:33m:01s remains)
INFO - root - 2017-12-16 08:24:54.047367: step 12680, loss = 0.70, batch loss = 0.44 (46.7 examples/sec; 0.171 sec/batch; 15h:12m:37s remains)
INFO - root - 2017-12-16 08:24:55.729002: step 12690, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 14h:52m:00s remains)
INFO - root - 2017-12-16 08:24:57.438664: step 12700, loss = 0.46, batch loss = 0.20 (45.6 examples/sec; 0.175 sec/batch; 15h:34m:32s remains)
2017-12-16 08:24:57.923858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8301649 -2.7364988 -2.6183088 -2.4097648 -2.2650754 -2.3842704 -2.5783567 -2.6036391 -2.4637034 -2.309871 -2.129941 -1.8704596 -1.8373731 -2.1453485 -2.3815887][-3.0407529 -2.9539344 -2.8403544 -2.5830419 -2.3611465 -2.444489 -2.7055917 -2.778506 -2.6460986 -2.5392456 -2.403542 -2.0981584 -2.0564382 -2.4013715 -2.6443675][-2.8106885 -2.7411666 -2.6913416 -2.4900084 -2.2395692 -2.2783871 -2.566268 -2.7388473 -2.764957 -2.8678553 -2.9310651 -2.7502303 -2.7228251 -2.9763007 -3.1006255][-2.141629 -2.0242796 -2.0465894 -1.9845738 -1.7651715 -1.7019489 -1.9809804 -2.2571261 -2.5047414 -2.9069331 -3.3395863 -3.4859834 -3.5658846 -3.7116666 -3.6959736][-1.3358097 -1.11852 -1.1485635 -1.1407528 -0.9345789 -0.73983479 -0.893705 -1.1504774 -1.5768411 -2.3546154 -3.21808 -3.7452784 -3.9879017 -4.0821681 -3.9539354][-0.846948 -0.54155517 -0.49100626 -0.42687154 -0.093106508 0.36086583 0.57727504 0.5355444 0.01150012 -1.129665 -2.4071264 -3.2907095 -3.7160685 -3.8431945 -3.6362207][-1.0083886 -0.69821644 -0.47802198 -0.21258473 0.36351371 1.248827 1.9955888 2.320436 1.8331623 0.49012542 -1.015732 -2.068464 -2.5940337 -2.7488155 -2.5161629][-1.6882373 -1.4336714 -1.1238091 -0.65435481 0.16662812 1.4363596 2.6968923 3.3639188 3.0149236 1.7322435 0.27001667 -0.6861788 -1.0778514 -1.1015515 -0.82958424][-2.3358791 -2.2579043 -2.049031 -1.6144422 -0.83120465 0.43974066 1.8300066 2.7252851 2.6743603 1.7231622 0.66821957 0.07788229 0.00054359436 0.20388341 0.54993391][-2.4547858 -2.5750747 -2.6117337 -2.4674604 -2.0459931 -1.1060832 0.080044746 0.94551587 1.0733724 0.59056616 0.085144758 -0.035234213 0.2612927 0.73197246 1.1430168][-1.8308827 -2.1557553 -2.5080533 -2.8877034 -3.0870109 -2.7352529 -1.9538887 -1.293092 -1.1147337 -1.2299182 -1.2711368 -1.0010316 -0.43612075 0.18974042 0.58152604][-0.74691534 -1.2462943 -1.9451754 -2.8469665 -3.7001736 -4.0293617 -3.7740283 -3.4008694 -3.2186503 -3.0917835 -2.83034 -2.2982676 -1.5838873 -0.93424082 -0.61184645][0.15840459 -0.40975344 -1.3353968 -2.6291275 -3.9818978 -4.8224497 -5.0266638 -4.9746275 -4.8259048 -4.5731082 -4.2138596 -3.6554303 -2.940593 -2.3271365 -2.0249574][0.44131327 -0.010854721 -0.91050148 -2.3273122 -3.8828549 -4.9700212 -5.5009041 -5.6896 -5.6341763 -5.4162526 -5.1603851 -4.7812905 -4.2199144 -3.6782565 -3.3546305][0.35349989 0.095527649 -0.58601642 -1.863595 -3.332891 -4.415525 -5.1049805 -5.4580245 -5.5291443 -5.4586568 -5.4043784 -5.2617865 -4.9138775 -4.4831543 -4.156672]]...]
INFO - root - 2017-12-16 08:24:59.633810: step 12710, loss = 0.49, batch loss = 0.23 (46.1 examples/sec; 0.173 sec/batch; 15h:24m:16s remains)
INFO - root - 2017-12-16 08:25:01.307495: step 12720, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:56m:44s remains)
INFO - root - 2017-12-16 08:25:02.973013: step 12730, loss = 0.65, batch loss = 0.39 (49.1 examples/sec; 0.163 sec/batch; 14h:28m:37s remains)
INFO - root - 2017-12-16 08:25:04.630470: step 12740, loss = 0.61, batch loss = 0.35 (47.5 examples/sec; 0.169 sec/batch; 14h:58m:27s remains)
INFO - root - 2017-12-16 08:25:06.321156: step 12750, loss = 0.53, batch loss = 0.27 (46.4 examples/sec; 0.172 sec/batch; 15h:17m:50s remains)
INFO - root - 2017-12-16 08:25:07.980649: step 12760, loss = 0.71, batch loss = 0.45 (45.8 examples/sec; 0.175 sec/batch; 15h:29m:57s remains)
INFO - root - 2017-12-16 08:25:09.669539: step 12770, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:43m:12s remains)
INFO - root - 2017-12-16 08:25:11.337139: step 12780, loss = 0.65, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 14h:35m:53s remains)
INFO - root - 2017-12-16 08:25:12.994696: step 12790, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.166 sec/batch; 14h:42m:26s remains)
INFO - root - 2017-12-16 08:25:14.629022: step 12800, loss = 0.61, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 14h:27m:55s remains)
2017-12-16 08:25:15.089542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2799662 -1.0998616 -0.8641566 -0.68803918 -1.1668024 -2.113317 -2.9598618 -3.0418735 -2.8970232 -2.5709605 -1.6226947 -0.37638712 0.40091491 0.59414172 -0.052965164][-1.7596039 -1.9823642 -2.0616376 -2.0682659 -2.5276332 -3.245908 -3.8341486 -3.8123732 -3.6916037 -3.4696455 -2.7126379 -1.6982114 -0.92620444 -0.53189993 -0.8656106][-1.9740815 -2.4033217 -2.6471987 -2.7492085 -3.0295253 -3.3500834 -3.6642609 -3.6527622 -3.7055688 -3.7604918 -3.2772441 -2.4596388 -1.691993 -1.1296157 -1.16183][-1.9824485 -2.3802934 -2.6037467 -2.7139151 -2.7234416 -2.6464353 -2.8115802 -2.9388723 -3.1997311 -3.5464559 -3.4001327 -2.824904 -2.1351759 -1.4343191 -1.1249443][-1.9413791 -2.1786737 -2.2859848 -2.3165369 -2.1016128 -1.775309 -1.8710127 -2.1500204 -2.6599181 -3.3110321 -3.4604478 -3.1006892 -2.4903355 -1.7143748 -1.1760688][-1.9091763 -1.8690557 -1.7442024 -1.5796497 -1.1230862 -0.59825373 -0.676512 -1.1937894 -2.0472372 -2.9389615 -3.3517358 -3.2166834 -2.6266782 -1.8205464 -1.2747892][-1.5429118 -1.1536256 -0.70450139 -0.18191385 0.7111702 1.4547102 1.1860185 0.1497879 -1.1246144 -2.3562865 -3.104497 -3.2005699 -2.6066868 -1.7679155 -1.2771757][-0.90261173 -0.20948148 0.52944493 1.4047735 2.622313 3.6051822 2.9922853 1.4328113 -0.12991381 -1.6369172 -2.6641779 -2.9703228 -2.430841 -1.556039 -1.107776][-0.58578622 0.25470281 1.099031 2.0756178 3.4279327 4.3394976 3.445014 1.9081006 0.4996047 -0.9808929 -2.0915697 -2.4882712 -2.0997636 -1.3290952 -0.95788515][-0.91125429 -0.049918652 0.71344995 1.4913108 2.5831032 3.204814 2.5198255 1.4224663 0.2410574 -1.0385181 -2.0019543 -2.3606544 -2.0031707 -1.2661017 -0.98757136][-1.5915546 -0.85151148 -0.28795791 0.18720484 0.7666018 0.99140644 0.579386 -0.0042595863 -0.73221672 -1.6344997 -2.2417357 -2.3250442 -1.7919575 -1.0699059 -0.82739019][-2.6359968 -2.0905604 -1.6300669 -1.3546777 -1.2193488 -1.2881896 -1.5513953 -1.7395891 -1.9280406 -2.2743187 -2.3883808 -2.1636922 -1.4939672 -0.89388263 -0.87316656][-3.836432 -3.4425716 -2.9958405 -2.7840848 -2.9113016 -3.1673422 -3.4343448 -3.3934398 -3.1348767 -2.8951182 -2.5142405 -2.0461683 -1.4827045 -1.1107447 -1.2853807][-4.7102146 -4.3713961 -4.0055814 -3.9035573 -4.1549377 -4.4971161 -4.8486533 -4.8064032 -4.3318238 -3.6885252 -2.9263577 -2.3256736 -1.9697938 -1.9080744 -2.1450536][-5.0384769 -4.7606091 -4.5040622 -4.4975719 -4.7445564 -5.0977855 -5.5252705 -5.6546459 -5.3027992 -4.6006246 -3.7315073 -3.1165528 -2.9311042 -3.0145533 -3.1692152]]...]
INFO - root - 2017-12-16 08:25:16.736674: step 12810, loss = 0.57, batch loss = 0.31 (49.5 examples/sec; 0.162 sec/batch; 14h:21m:48s remains)
INFO - root - 2017-12-16 08:25:18.410177: step 12820, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:31m:40s remains)
INFO - root - 2017-12-16 08:25:20.066196: step 12830, loss = 0.52, batch loss = 0.26 (45.2 examples/sec; 0.177 sec/batch; 15h:43m:23s remains)
INFO - root - 2017-12-16 08:25:21.767921: step 12840, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:59m:17s remains)
INFO - root - 2017-12-16 08:25:23.482186: step 12850, loss = 0.59, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 14h:53m:34s remains)
INFO - root - 2017-12-16 08:25:25.153231: step 12860, loss = 0.61, batch loss = 0.35 (47.6 examples/sec; 0.168 sec/batch; 14h:56m:15s remains)
INFO - root - 2017-12-16 08:25:26.832609: step 12870, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 15h:03m:35s remains)
INFO - root - 2017-12-16 08:25:28.525552: step 12880, loss = 0.56, batch loss = 0.30 (46.2 examples/sec; 0.173 sec/batch; 15h:22m:33s remains)
INFO - root - 2017-12-16 08:25:30.232333: step 12890, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 14h:51m:55s remains)
INFO - root - 2017-12-16 08:25:31.908294: step 12900, loss = 0.48, batch loss = 0.22 (48.7 examples/sec; 0.164 sec/batch; 14h:34m:13s remains)
2017-12-16 08:25:32.330960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1287262 -2.4144411 -2.6813264 -2.8865473 -3.0629215 -3.2009881 -3.2518992 -3.209305 -3.1914682 -3.1579745 -3.0029185 -2.7217579 -2.4691041 -2.279253 -2.0954623][-2.4156811 -2.7939343 -3.1461141 -3.3958468 -3.6315074 -3.8019719 -3.8341727 -3.7741616 -3.7623777 -3.7802773 -3.6271172 -3.2950835 -2.9598255 -2.6538444 -2.3699129][-2.7125931 -3.1081653 -3.4283643 -3.5796185 -3.7241235 -3.8206971 -3.8128493 -3.7864151 -3.8861806 -4.0914488 -4.0627465 -3.7524295 -3.3312864 -2.9348986 -2.5799198][-2.9004812 -3.1956332 -3.317879 -3.2154872 -3.1033735 -2.9782526 -2.8287187 -2.8365879 -3.121274 -3.5847087 -3.8280487 -3.7505088 -3.4601345 -3.0976689 -2.7272136][-2.8433762 -2.8948777 -2.701889 -2.2700083 -1.7555982 -1.2520682 -0.84015858 -0.85851538 -1.3742518 -2.0945263 -2.7936642 -3.2144804 -3.3011932 -3.1062536 -2.7574105][-2.5442207 -2.2498493 -1.6998758 -0.88441384 0.11982131 1.1188247 1.8077223 1.8056281 1.0693321 0.0034453869 -1.1803448 -2.1538608 -2.710681 -2.8047915 -2.5916286][-2.0998952 -1.5490454 -0.64716339 0.55958152 1.9734228 3.3977382 4.4095764 4.349308 3.3125088 1.882664 0.34023952 -1.0263635 -1.9370894 -2.2850342 -2.2886598][-1.762398 -1.0625597 0.081759453 1.5925174 3.2526619 4.94777 6.142477 5.8495417 4.4190435 2.8754675 1.3051183 -0.20574617 -1.32639 -1.9115769 -2.091166][-1.6937094 -0.95555043 0.21294641 1.6927869 3.2793195 4.7739239 5.6877985 5.2599783 3.9198406 2.6377223 1.3133447 -0.097465515 -1.2689503 -1.9374133 -2.1346552][-1.8470302 -1.2238545 -0.34568274 0.73905087 1.915395 2.8199098 3.1834996 2.8214424 2.0469387 1.251056 0.29874516 -0.86445165 -1.8256456 -2.3603983 -2.4307482][-2.14337 -1.8516896 -1.42016 -0.87347972 -0.278538 0.10814619 0.15375209 -0.1240921 -0.45810878 -0.76278138 -1.2828709 -2.0825639 -2.7124441 -2.9737604 -2.8342032][-2.4500523 -2.5058627 -2.4829142 -2.4212179 -2.3059776 -2.2698419 -2.3681333 -2.5453079 -2.6077895 -2.6228805 -2.8470333 -3.2627773 -3.5314612 -3.4787073 -3.1269033][-2.5240343 -2.777854 -3.0490642 -3.2811849 -3.4402153 -3.5875111 -3.7679596 -3.911628 -3.8758698 -3.7846134 -3.8156428 -3.9145617 -3.894326 -3.6098919 -3.1210604][-2.407155 -2.7087705 -3.0355029 -3.3285217 -3.5756269 -3.768887 -3.9189177 -4.0296621 -4.0006061 -3.9177315 -3.8531668 -3.7721512 -3.609714 -3.2726743 -2.8312707][-2.162499 -2.4031243 -2.6622386 -2.8861966 -3.0746169 -3.200017 -3.2876744 -3.3463686 -3.3428438 -3.2845211 -3.1931581 -3.083796 -2.9236383 -2.6792207 -2.3737912]]...]
INFO - root - 2017-12-16 08:25:34.008746: step 12910, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 14h:34m:54s remains)
INFO - root - 2017-12-16 08:25:35.664092: step 12920, loss = 0.56, batch loss = 0.30 (47.8 examples/sec; 0.168 sec/batch; 14h:52m:10s remains)
INFO - root - 2017-12-16 08:25:37.315573: step 12930, loss = 0.47, batch loss = 0.21 (48.5 examples/sec; 0.165 sec/batch; 14h:38m:11s remains)
INFO - root - 2017-12-16 08:25:38.996478: step 12940, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 14h:36m:30s remains)
INFO - root - 2017-12-16 08:25:40.640582: step 12950, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 15h:06m:25s remains)
INFO - root - 2017-12-16 08:25:42.341740: step 12960, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:49m:48s remains)
INFO - root - 2017-12-16 08:25:44.033857: step 12970, loss = 0.51, batch loss = 0.25 (48.9 examples/sec; 0.164 sec/batch; 14h:31m:14s remains)
INFO - root - 2017-12-16 08:25:45.698431: step 12980, loss = 0.76, batch loss = 0.50 (47.6 examples/sec; 0.168 sec/batch; 14h:55m:47s remains)
INFO - root - 2017-12-16 08:25:47.370352: step 12990, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 14h:53m:49s remains)
INFO - root - 2017-12-16 08:25:49.067560: step 13000, loss = 0.64, batch loss = 0.38 (42.8 examples/sec; 0.187 sec/batch; 16h:35m:52s remains)
2017-12-16 08:25:49.526619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2662206 -1.1474793 -1.0356457 -0.95889771 -0.9162432 -0.88912535 -0.84967875 -0.82249784 -0.8188529 -0.84632516 -0.87326396 -0.86122191 -0.82093334 -0.76242256 -0.7114526][-2.2625468 -2.1273787 -1.9874362 -1.8722421 -1.7867589 -1.699137 -1.5786486 -1.4815683 -1.4612931 -1.5270557 -1.5612537 -1.5373361 -1.4800304 -1.3648503 -1.2760478][-2.8588667 -2.7679815 -2.6827278 -2.6013908 -2.5495129 -2.4483931 -2.2839825 -2.1377256 -2.1221454 -2.2313716 -2.3034372 -2.3170042 -2.2876658 -2.1499045 -1.9660876][-2.6943734 -2.6846325 -2.7003081 -2.6996951 -2.7198117 -2.6675906 -2.5109227 -2.3837476 -2.4031045 -2.5233469 -2.6302161 -2.7325647 -2.8321974 -2.7975755 -2.6219592][-2.1121445 -2.1734605 -2.265547 -2.3643808 -2.438452 -2.4433184 -2.3212354 -2.209512 -2.2298915 -2.3319888 -2.4736753 -2.6638622 -2.919683 -3.0767255 -3.017343][-1.6959438 -1.7629914 -1.8985322 -1.9999093 -2.0300703 -2.0312679 -1.9320824 -1.827938 -1.8350139 -1.9165308 -2.0384927 -2.2217 -2.5391808 -2.8210969 -2.8921208][-1.6189959 -1.5630429 -1.5265071 -1.4857684 -1.3915569 -1.3552985 -1.3376141 -1.3153527 -1.4029255 -1.4776893 -1.5088696 -1.591953 -1.8193784 -2.0494702 -2.194232][-1.530643 -1.3430074 -1.191898 -1.0546409 -0.88746417 -0.86777687 -1.0104692 -1.1615 -1.2869742 -1.2953991 -1.2044194 -1.1508752 -1.2307403 -1.3405535 -1.4217207][-1.0618945 -0.797909 -0.7015245 -0.63831306 -0.53061569 -0.61028206 -0.86901 -1.1213008 -1.2592969 -1.1895707 -1.0333717 -0.90880394 -0.91488111 -0.92283452 -0.93357027][-0.29466295 -0.064305067 -0.1189878 -0.24315643 -0.25759172 -0.34145927 -0.57737255 -0.77353168 -0.82774651 -0.78547919 -0.728845 -0.72912967 -0.84584856 -0.88507438 -0.88885283][0.15964985 0.14458323 -0.045792103 -0.26339912 -0.23923492 -0.15031791 -0.14835787 -0.12230945 -0.034312963 -0.067434549 -0.21271372 -0.47758782 -0.87612915 -1.1750022 -1.3537641][-0.049173355 -0.20534778 -0.38580489 -0.52535748 -0.39055932 -0.10500669 0.15527344 0.38257813 0.52227449 0.39674592 0.027534485 -0.50405872 -1.1578544 -1.679655 -2.0443175][-0.72377264 -0.79965961 -0.82863748 -0.80028057 -0.53986824 -0.15516639 0.15412402 0.34740186 0.35399461 0.062649488 -0.43918228 -1.0571135 -1.6721766 -2.1250248 -2.450062][-1.5242196 -1.4326086 -1.3176358 -1.1374556 -0.82661819 -0.50509119 -0.37179959 -0.40446925 -0.61662209 -1.0007451 -1.4265919 -1.8388603 -2.1426992 -2.2606022 -2.3310127][-2.0020783 -1.7641432 -1.6025827 -1.4434259 -1.2360657 -1.0941268 -1.1474285 -1.3847365 -1.6899352 -2.0132067 -2.2570326 -2.4070473 -2.3985183 -2.1994669 -1.9717703]]...]
INFO - root - 2017-12-16 08:25:51.226596: step 13010, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 14h:45m:14s remains)
INFO - root - 2017-12-16 08:25:52.915517: step 13020, loss = 0.65, batch loss = 0.39 (44.5 examples/sec; 0.180 sec/batch; 15h:57m:52s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:25:54.601162: step 13030, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:43m:39s remains)
INFO - root - 2017-12-16 08:25:56.315230: step 13040, loss = 0.52, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 15h:07m:13s remains)
INFO - root - 2017-12-16 08:25:58.026009: step 13050, loss = 0.53, batch loss = 0.27 (44.0 examples/sec; 0.182 sec/batch; 16h:07m:27s remains)
INFO - root - 2017-12-16 08:25:59.755428: step 13060, loss = 0.74, batch loss = 0.48 (46.3 examples/sec; 0.173 sec/batch; 15h:20m:48s remains)
INFO - root - 2017-12-16 08:26:01.447122: step 13070, loss = 0.73, batch loss = 0.47 (47.8 examples/sec; 0.167 sec/batch; 14h:51m:08s remains)
INFO - root - 2017-12-16 08:26:03.121006: step 13080, loss = 0.49, batch loss = 0.23 (48.6 examples/sec; 0.164 sec/batch; 14h:35m:26s remains)
INFO - root - 2017-12-16 08:26:04.768334: step 13090, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 14h:38m:22s remains)
INFO - root - 2017-12-16 08:26:06.418206: step 13100, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:33m:22s remains)
2017-12-16 08:26:06.898392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2054718 -3.5235929 -3.5990529 -3.4724822 -3.5188947 -3.8048809 -4.118248 -4.240016 -4.2362432 -4.2092152 -3.8717856 -3.324368 -2.6708064 -2.044899 -1.6918821][-3.2625666 -3.4396605 -3.2542462 -2.7441759 -2.5835931 -2.879859 -3.3712676 -3.810596 -4.1505718 -4.4267483 -4.1980834 -3.5660591 -2.7506361 -1.9684205 -1.4966787][-3.0520229 -3.0127189 -2.529458 -1.5707064 -1.0451936 -1.2792692 -1.9848523 -2.8510771 -3.6862173 -4.3738894 -4.3829713 -3.8148992 -2.9690285 -2.0945692 -1.5230834][-2.7430897 -2.4112558 -1.5855882 -0.12531996 0.919693 0.88683629 -0.043217182 -1.3744339 -2.7438881 -3.8826239 -4.2652583 -3.9767523 -3.2928481 -2.4481554 -1.7862587][-2.5651681 -1.8744245 -0.66864622 1.2929049 2.9175885 3.2786992 2.3063495 0.69079065 -1.124436 -2.7623539 -3.7331142 -3.9949079 -3.6799808 -2.9980929 -2.2961926][-2.7703121 -1.7952616 -0.32194698 1.8919237 3.9566848 4.8972187 4.3146496 2.8014953 0.88645244 -1.1945313 -2.7944715 -3.7151284 -3.90903 -3.4968257 -2.8387668][-3.2485201 -2.2605665 -0.84907913 1.2119689 3.3731911 4.8397589 4.9975252 4.0089521 2.2868583 0.15082288 -1.7462142 -3.1503379 -3.7857275 -3.6860738 -3.1858523][-3.5423312 -2.7656789 -1.6404892 -0.045586348 1.8296654 3.51063 4.3792028 4.0845366 2.7980788 0.99522018 -0.81377959 -2.3907874 -3.3030214 -3.5086951 -3.259263][-3.3504472 -2.8602283 -2.1440351 -1.1807282 0.21240878 1.9367445 3.2765315 3.5157855 2.7927 1.6327248 0.1431334 -1.4542384 -2.5770664 -3.0706477 -3.1323001][-2.878819 -2.6473062 -2.3256664 -1.9125715 -1.1073632 0.32249856 1.8503706 2.576957 2.5289967 2.0011952 0.910486 -0.620381 -1.8319895 -2.5106764 -2.8578346][-2.2875617 -2.2817781 -2.3172076 -2.3465228 -2.1376655 -1.2390692 0.13064289 1.2107635 1.7133534 1.6606531 0.93779492 -0.25570393 -1.280612 -1.986328 -2.5375557][-1.5611657 -1.7122433 -2.0320261 -2.4325113 -2.7491422 -2.4408262 -1.4848876 -0.37067664 0.40719461 0.67159033 0.3854003 -0.33029926 -0.98082745 -1.534814 -2.1499929][-0.74592495 -0.7804594 -1.1930581 -1.8536763 -2.5659792 -2.7424829 -2.2697093 -1.4534246 -0.76474357 -0.35900033 -0.29008913 -0.49316597 -0.73739004 -1.0760652 -1.5847127][-0.10016727 0.13088226 -0.17863941 -0.8423475 -1.6896466 -2.1819706 -2.169127 -1.8012252 -1.398102 -1.1059647 -0.91463935 -0.77780759 -0.64531517 -0.70444274 -0.95685065][0.1091218 0.58475947 0.5478189 0.10560656 -0.56302381 -1.083362 -1.4268624 -1.6001174 -1.6387372 -1.6054382 -1.4786081 -1.1974436 -0.80888176 -0.57659483 -0.45869887]]...]
INFO - root - 2017-12-16 08:26:08.581630: step 13110, loss = 0.60, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 14h:59m:06s remains)
INFO - root - 2017-12-16 08:26:10.318055: step 13120, loss = 0.59, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 14h:48m:30s remains)
INFO - root - 2017-12-16 08:26:11.988416: step 13130, loss = 0.59, batch loss = 0.33 (49.2 examples/sec; 0.163 sec/batch; 14h:25m:09s remains)
INFO - root - 2017-12-16 08:26:13.723417: step 13140, loss = 0.61, batch loss = 0.35 (45.8 examples/sec; 0.175 sec/batch; 15h:29m:05s remains)
INFO - root - 2017-12-16 08:26:15.406085: step 13150, loss = 0.61, batch loss = 0.35 (45.9 examples/sec; 0.174 sec/batch; 15h:27m:56s remains)
INFO - root - 2017-12-16 08:26:17.082466: step 13160, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 15h:21m:18s remains)
INFO - root - 2017-12-16 08:26:18.758288: step 13170, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 14h:47m:13s remains)
INFO - root - 2017-12-16 08:26:20.439893: step 13180, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 14h:46m:34s remains)
INFO - root - 2017-12-16 08:26:22.095226: step 13190, loss = 0.53, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 14h:34m:11s remains)
INFO - root - 2017-12-16 08:26:23.758389: step 13200, loss = 0.61, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 14h:34m:51s remains)
2017-12-16 08:26:24.227780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1694634 -2.1918316 -2.1682858 -2.1870432 -2.299165 -2.329026 -2.2181139 -2.0528605 -1.8578812 -1.7035754 -1.6480966 -1.754158 -1.9869714 -2.1414909 -2.1558452][-1.645695 -1.7369565 -1.8159268 -1.8979467 -1.9606682 -2.0110571 -1.9395175 -1.7429204 -1.5006347 -1.3419478 -1.2637888 -1.3080339 -1.4757202 -1.6382785 -1.7514951][-0.94445622 -1.1498491 -1.3214475 -1.4062728 -1.4559144 -1.5178573 -1.4793408 -1.3699653 -1.2796326 -1.1675123 -1.0092059 -0.93260455 -1.0682907 -1.2694234 -1.4110751][-0.29995966 -0.61874664 -0.83668947 -0.88847816 -0.85231829 -0.83671033 -0.89481878 -1.0284468 -1.1913042 -1.1702851 -0.96223354 -0.79478621 -0.90048134 -1.1302577 -1.2703506][0.098390818 -0.22465873 -0.42385566 -0.405828 -0.2695868 -0.16605854 -0.29725575 -0.64512813 -1.009091 -1.0564106 -0.85749066 -0.6765213 -0.77739191 -0.99590373 -1.0910095][0.12686491 -0.090975046 -0.17547345 -0.088803291 0.16603446 0.35827351 0.24990344 -0.16658545 -0.55198371 -0.64374173 -0.56324637 -0.53297639 -0.67510414 -0.83863616 -0.88345039][-0.18243313 -0.22057486 -0.098729372 0.15459037 0.53691339 0.82750511 0.78672028 0.37224722 0.00053524971 -0.11403489 -0.16382766 -0.31896746 -0.47100866 -0.643155 -0.82063758][-0.55465972 -0.33179617 0.077388287 0.60863686 1.166549 1.603631 1.7005417 1.2318146 0.73032427 0.45265293 0.20741415 -0.015115261 -0.17162085 -0.520496 -0.97921741][-0.90714049 -0.37533259 0.34573507 1.1920164 1.9852912 2.6615217 2.8930967 2.1355283 1.3209283 0.77153563 0.29544067 0.036984921 -0.15586472 -0.72318172 -1.4221473][-1.45093 -0.80131817 0.15471435 1.2932448 2.2562902 2.9077709 2.9221084 2.0452726 1.1333995 0.41924119 -0.14690995 -0.39481759 -0.62116158 -1.2273442 -1.9414399][-2.0365562 -1.4737666 -0.43971014 0.78197622 1.6336045 1.9174063 1.5566676 0.74322057 0.051569939 -0.46012616 -0.8607223 -1.045249 -1.241282 -1.6988397 -2.2305255][-2.3150804 -1.8977845 -1.0025903 -0.052181005 0.45633888 0.40561724 -0.12406445 -0.78258026 -1.2214782 -1.4022198 -1.4677854 -1.472723 -1.5312731 -1.7794619 -2.1140962][-2.156064 -1.8741535 -1.2503142 -0.71259785 -0.60812008 -0.90834081 -1.4360178 -1.9044223 -2.1296029 -2.0301409 -1.8222286 -1.60724 -1.4883034 -1.5686908 -1.7303731][-1.6686411 -1.5458214 -1.2091796 -1.04653 -1.2145823 -1.6383288 -2.133718 -2.4614148 -2.4970496 -2.2480035 -1.8744287 -1.5048486 -1.286208 -1.2241269 -1.264774][-1.3025398 -1.32813 -1.2372271 -1.2988751 -1.6040192 -2.0876617 -2.501801 -2.6670604 -2.5226855 -2.1658034 -1.7470833 -1.3752507 -1.1289228 -0.99332333 -0.97876215]]...]
INFO - root - 2017-12-16 08:26:25.888224: step 13210, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 14h:56m:22s remains)
INFO - root - 2017-12-16 08:26:27.549040: step 13220, loss = 0.58, batch loss = 0.32 (46.3 examples/sec; 0.173 sec/batch; 15h:18m:44s remains)
INFO - root - 2017-12-16 08:26:29.211756: step 13230, loss = 0.62, batch loss = 0.36 (49.0 examples/sec; 0.163 sec/batch; 14h:29m:05s remains)
INFO - root - 2017-12-16 08:26:30.864447: step 13240, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:45m:24s remains)
INFO - root - 2017-12-16 08:26:32.549410: step 13250, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:57m:44s remains)
INFO - root - 2017-12-16 08:26:34.220248: step 13260, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 14h:56m:09s remains)
INFO - root - 2017-12-16 08:26:35.885487: step 13270, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:30m:29s remains)
INFO - root - 2017-12-16 08:26:37.563567: step 13280, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 14h:54m:54s remains)
INFO - root - 2017-12-16 08:26:39.226277: step 13290, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:52m:49s remains)
INFO - root - 2017-12-16 08:26:40.895800: step 13300, loss = 0.63, batch loss = 0.37 (49.1 examples/sec; 0.163 sec/batch; 14h:27m:32s remains)
2017-12-16 08:26:41.356395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.227958 -3.3415396 -2.8070269 -1.5218499 -0.35005939 -0.21387887 -0.70191658 -0.98022103 -1.2267199 -2.0333524 -3.123208 -3.8093381 -3.7036326 -3.0046449 -2.2699673][-3.6601448 -3.8813567 -3.5741439 -2.6747458 -1.8415525 -1.7419698 -1.9385397 -1.860232 -1.8255168 -2.4171333 -3.3257298 -3.8811488 -3.7509804 -3.1002176 -2.3815463][-4.1463327 -4.3932495 -4.2427325 -3.6530004 -3.0704494 -2.8766947 -2.7029696 -2.2702911 -2.0494776 -2.5199542 -3.2209294 -3.5796409 -3.4452515 -2.9112153 -2.30981][-4.5672169 -4.8571529 -4.7753248 -4.330081 -3.7447476 -3.2079451 -2.5705347 -1.9127693 -1.8071536 -2.3788421 -2.8995402 -3.007916 -2.8695166 -2.5494156 -2.1797566][-4.626636 -4.9302807 -4.8903942 -4.4711761 -3.651958 -2.4744816 -1.1912184 -0.37530088 -0.60213292 -1.5683842 -2.263972 -2.3882837 -2.3395326 -2.2484546 -2.0755911][-4.2055011 -4.4912772 -4.3081441 -3.7444282 -2.530863 -0.4151572 1.8042233 2.6957915 1.6332574 -0.31639075 -1.6966012 -2.1543033 -2.2063186 -2.1697838 -2.0204868][-3.8217521 -3.9620924 -3.4989891 -2.6193938 -0.98193252 2.025295 5.32732 6.3163719 3.9954126 0.58787012 -1.7394722 -2.5737631 -2.6074994 -2.378377 -2.0540307][-3.7279139 -3.7562447 -3.0196595 -1.907653 -0.088709831 3.4117877 7.5813913 8.5189514 5.0444117 0.51858473 -2.4207346 -3.4301486 -3.3081098 -2.7364521 -2.1183441][-3.5775061 -3.7027559 -2.9083173 -1.7941976 -0.27125812 2.6497986 6.1436987 7.0894775 3.9813917 -0.47292542 -3.4131613 -4.3085504 -3.9091611 -2.9885998 -2.0938954][-3.4374228 -3.7198048 -3.0108812 -2.0094795 -0.91460848 1.089803 3.7027104 4.5943117 2.2267172 -1.5441251 -4.1512752 -4.8517618 -4.2308297 -3.0581298 -1.9855049][-3.4362459 -3.8622322 -3.2927 -2.4787431 -1.7638865 -0.47528863 1.400064 2.2223532 0.62431812 -2.2082491 -4.2794414 -4.8073387 -4.1262975 -2.9221623 -1.8386334][-3.3743815 -3.9287534 -3.5612128 -2.9941757 -2.5894289 -1.7841508 -0.35326517 0.4625051 -0.49679875 -2.5102396 -4.0865059 -4.4620843 -3.8338373 -2.7269702 -1.7489846][-3.4244163 -3.965579 -3.7110949 -3.3076146 -3.0198689 -2.4181049 -1.2664962 -0.52531207 -1.0937793 -2.5327752 -3.6960783 -3.9645123 -3.4018037 -2.4456961 -1.6392944][-3.5518556 -3.94411 -3.6759872 -3.2699816 -2.9535575 -2.3754215 -1.4237915 -0.8204608 -1.2369801 -2.3266728 -3.2064443 -3.3921475 -2.8690562 -2.0617878 -1.4514794][-3.442863 -3.6182256 -3.309669 -2.8596578 -2.4548213 -1.8685056 -1.0720109 -0.60876715 -1.0073463 -1.9443489 -2.6930151 -2.8279176 -2.3426743 -1.6512322 -1.2205907]]...]
INFO - root - 2017-12-16 08:26:43.044366: step 13310, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 14h:40m:46s remains)
INFO - root - 2017-12-16 08:26:44.712851: step 13320, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 15h:01m:17s remains)
INFO - root - 2017-12-16 08:26:46.358512: step 13330, loss = 0.58, batch loss = 0.32 (49.5 examples/sec; 0.162 sec/batch; 14h:20m:19s remains)
INFO - root - 2017-12-16 08:26:48.052196: step 13340, loss = 0.47, batch loss = 0.21 (46.2 examples/sec; 0.173 sec/batch; 15h:21m:33s remains)
INFO - root - 2017-12-16 08:26:49.750074: step 13350, loss = 0.52, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 15h:08m:29s remains)
INFO - root - 2017-12-16 08:26:51.460398: step 13360, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 15h:06m:57s remains)
INFO - root - 2017-12-16 08:26:53.169695: step 13370, loss = 0.59, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 14h:46m:02s remains)
INFO - root - 2017-12-16 08:26:54.880129: step 13380, loss = 0.51, batch loss = 0.25 (46.6 examples/sec; 0.172 sec/batch; 15h:13m:25s remains)
INFO - root - 2017-12-16 08:26:56.581651: step 13390, loss = 0.60, batch loss = 0.34 (43.9 examples/sec; 0.182 sec/batch; 16h:08m:45s remains)
INFO - root - 2017-12-16 08:26:58.284735: step 13400, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.169 sec/batch; 14h:56m:24s remains)
2017-12-16 08:26:58.810869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2608923 -1.2843236 -1.3321565 -1.4788822 -1.5813999 -1.4623792 -1.1142097 -0.64130628 -0.22117257 -0.10143113 -0.44613028 -1.1880727 -2.0351241 -2.6842732 -2.9688871][-1.734323 -1.8881297 -2.0145702 -2.1766582 -2.2695456 -2.1334457 -1.7932851 -1.3661561 -1.0284362 -0.94180608 -1.2006965 -1.7115686 -2.2439716 -2.5580258 -2.5309765][-1.7396603 -2.0090678 -2.2180018 -2.3879859 -2.4751217 -2.3580885 -2.0177245 -1.5606289 -1.1988852 -1.1121743 -1.3850863 -1.8296945 -2.1711385 -2.2074094 -1.8719298][-1.37393 -1.7255266 -1.9731143 -2.1091347 -2.172364 -2.0709276 -1.7101605 -1.1774912 -0.72643089 -0.64266372 -1.0147399 -1.5473768 -1.8824276 -1.7998087 -1.2558436][-0.815096 -1.2167362 -1.4634838 -1.5151043 -1.5103561 -1.4245484 -1.0622989 -0.45501423 0.033539057 0.093749762 -0.32921374 -0.98024988 -1.4426759 -1.4180059 -0.800012][-0.36073685 -0.7682445 -0.94550037 -0.88787532 -0.81579792 -0.75049114 -0.46855044 0.077684164 0.5678637 0.69233704 0.33899975 -0.38021255 -0.98156822 -1.0648402 -0.48279238][-0.32441926 -0.61734414 -0.65695465 -0.50131941 -0.38993371 -0.32185411 -0.14384317 0.26836586 0.7221384 0.9352951 0.6801877 0.011490345 -0.57630825 -0.68050241 -0.18311024][-0.66053212 -0.77723742 -0.64782751 -0.38805389 -0.25114965 -0.21072054 -0.084588766 0.24531913 0.66392517 0.90161061 0.70936489 0.17834592 -0.28108788 -0.34389913 0.036361933][-1.1312675 -1.1072482 -0.8720938 -0.54819274 -0.38719904 -0.34998691 -0.25457311 0.0063135624 0.37265897 0.61849403 0.50796008 0.13388634 -0.19092631 -0.22216225 0.048337221][-1.5641025 -1.4664494 -1.2286845 -0.90540195 -0.69124734 -0.61305749 -0.591256 -0.51084852 -0.2848382 -0.0022637844 0.099651337 0.0088629723 -0.12210131 -0.10544634 0.045276642][-1.8250606 -1.6822927 -1.4629352 -1.1553025 -0.87404168 -0.77267611 -0.88569069 -1.0178664 -0.97069287 -0.70869958 -0.36869836 -0.11916685 0.028637409 0.14119601 0.19407535][-1.8364173 -1.6750951 -1.4591997 -1.1464826 -0.86859727 -0.81461477 -1.0519249 -1.3645974 -1.4759769 -1.2713451 -0.813131 -0.2974503 0.13781357 0.40670156 0.41157889][-1.7582264 -1.6131613 -1.3911545 -1.0836073 -0.86296713 -0.91567445 -1.2669934 -1.6915655 -1.9132657 -1.7618852 -1.2735778 -0.6546222 -0.097388029 0.27182627 0.31164169][-1.7533621 -1.630878 -1.4347708 -1.1811563 -1.065913 -1.2372415 -1.6609478 -2.1134131 -2.351954 -2.2240424 -1.7820182 -1.2150344 -0.69286 -0.34788835 -0.28185797][-1.8688318 -1.7695949 -1.633347 -1.4835024 -1.472735 -1.6840632 -2.0576098 -2.4263124 -2.6199205 -2.5349097 -2.1994748 -1.757688 -1.3563939 -1.0854591 -0.98670387]]...]
INFO - root - 2017-12-16 08:27:00.463818: step 13410, loss = 0.58, batch loss = 0.32 (49.6 examples/sec; 0.161 sec/batch; 14h:17m:44s remains)
INFO - root - 2017-12-16 08:27:02.201644: step 13420, loss = 0.78, batch loss = 0.52 (46.3 examples/sec; 0.173 sec/batch; 15h:17m:53s remains)
INFO - root - 2017-12-16 08:27:03.872967: step 13430, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 14h:56m:59s remains)
INFO - root - 2017-12-16 08:27:05.545486: step 13440, loss = 0.64, batch loss = 0.38 (47.2 examples/sec; 0.169 sec/batch; 15h:01m:16s remains)
INFO - root - 2017-12-16 08:27:07.216931: step 13450, loss = 0.70, batch loss = 0.44 (48.6 examples/sec; 0.165 sec/batch; 14h:35m:36s remains)
INFO - root - 2017-12-16 08:27:08.872672: step 13460, loss = 0.69, batch loss = 0.43 (48.2 examples/sec; 0.166 sec/batch; 14h:42m:06s remains)
INFO - root - 2017-12-16 08:27:10.538274: step 13470, loss = 0.59, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:50m:30s remains)
INFO - root - 2017-12-16 08:27:12.209652: step 13480, loss = 0.55, batch loss = 0.28 (45.7 examples/sec; 0.175 sec/batch; 15h:31m:33s remains)
INFO - root - 2017-12-16 08:27:13.888163: step 13490, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:48m:43s remains)
INFO - root - 2017-12-16 08:27:15.584067: step 13500, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 15h:08m:20s remains)
2017-12-16 08:27:16.068531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0235655 -3.0699124 -3.1486881 -3.1652231 -3.1092017 -2.9989042 -2.8288598 -2.6402233 -2.5474205 -2.6240125 -2.7844939 -2.9218452 -2.9861851 -3.0048022 -3.0213561][-2.9179745 -2.9740219 -3.151685 -3.3334832 -3.4412107 -3.3976219 -3.181623 -2.9051313 -2.7593715 -2.8129904 -2.9542572 -3.0150142 -2.943989 -2.8506029 -2.8419225][-2.7082272 -2.8295119 -3.1632586 -3.572156 -3.8672526 -3.8529737 -3.5189233 -3.059202 -2.7828944 -2.8003106 -2.9700754 -3.0316014 -2.9387822 -2.8698878 -2.9227881][-2.3867288 -2.6917076 -3.2231412 -3.7494807 -3.9643438 -3.7130008 -3.099179 -2.360678 -1.9078596 -1.9509069 -2.2732716 -2.5360942 -2.6649442 -2.799283 -2.9325693][-2.0234342 -2.5827091 -3.2144651 -3.5947516 -3.479248 -2.7599132 -1.7117064 -0.72297752 -0.238724 -0.4393605 -1.0136145 -1.5439568 -1.9517492 -2.3302805 -2.6397569][-1.7708509 -2.4400275 -2.9430931 -2.9764054 -2.3404436 -1.0095198 0.60742068 1.8202641 2.084368 1.4043036 0.3567493 -0.52608955 -1.1718785 -1.7319473 -2.1481566][-1.5687101 -2.0360515 -2.1258307 -1.6707919 -0.47215962 1.5608296 3.794564 5.20296 4.8972673 3.3823078 1.6519699 0.32891107 -0.56508493 -1.2191153 -1.6485728][-1.4219154 -1.5059342 -1.1066911 -0.25151277 1.2686162 3.5424058 6.0109921 7.3558178 6.2531815 4.0223017 1.8695123 0.33888102 -0.56773531 -1.0914237 -1.3917284][-1.4062366 -1.0964473 -0.41513455 0.37593508 1.4448063 2.9724839 4.6100311 5.2838993 4.2282476 2.2524617 0.39894056 -0.76191449 -1.2874634 -1.4787393 -1.6136737][-1.6188035 -1.0249872 -0.30155873 0.022978783 0.16743088 0.62540627 1.3510616 1.6903989 1.0225487 -0.33133233 -1.6015698 -2.2973671 -2.4501691 -2.3227684 -2.2814767][-1.9751016 -1.3481486 -0.83136404 -0.94376612 -1.4489784 -1.696074 -1.496429 -1.316685 -1.6014075 -2.312202 -2.9926605 -3.2770238 -3.1881359 -2.8861952 -2.7121968][-2.1959765 -1.7159257 -1.4316831 -1.7807221 -2.5458343 -3.1583309 -3.2681181 -3.1689322 -3.154222 -3.3096435 -3.4729905 -3.4459548 -3.1608477 -2.7681725 -2.5806861][-2.200645 -1.7515767 -1.5529107 -1.9527853 -2.776731 -3.5047443 -3.7818973 -3.7970767 -3.6917582 -3.5292022 -3.3375793 -3.0027337 -2.5056803 -2.0353332 -1.891688][-1.9629672 -1.4366045 -1.2714541 -1.6365685 -2.3358932 -3.0390182 -3.4974437 -3.6960738 -3.6602869 -3.4193761 -3.0424738 -2.4808249 -1.7448349 -1.1232647 -0.90610588][-1.6289198 -1.1119984 -1.040013 -1.2875515 -1.726126 -2.3333616 -2.9023678 -3.3173461 -3.4450545 -3.2866158 -2.9069798 -2.2812445 -1.4132516 -0.60012543 -0.19826603]]...]
INFO - root - 2017-12-16 08:27:17.806631: step 13510, loss = 0.51, batch loss = 0.25 (46.1 examples/sec; 0.174 sec/batch; 15h:23m:31s remains)
INFO - root - 2017-12-16 08:27:19.474500: step 13520, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 14h:36m:50s remains)
INFO - root - 2017-12-16 08:27:21.141526: step 13530, loss = 0.65, batch loss = 0.39 (47.1 examples/sec; 0.170 sec/batch; 15h:03m:23s remains)
INFO - root - 2017-12-16 08:27:22.791466: step 13540, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:41m:22s remains)
INFO - root - 2017-12-16 08:27:24.429798: step 13550, loss = 0.68, batch loss = 0.42 (49.0 examples/sec; 0.163 sec/batch; 14h:27m:25s remains)
INFO - root - 2017-12-16 08:27:26.106437: step 13560, loss = 0.67, batch loss = 0.41 (47.4 examples/sec; 0.169 sec/batch; 14h:57m:16s remains)
INFO - root - 2017-12-16 08:27:27.757511: step 13570, loss = 0.62, batch loss = 0.36 (49.6 examples/sec; 0.161 sec/batch; 14h:17m:12s remains)
INFO - root - 2017-12-16 08:27:29.403540: step 13580, loss = 0.43, batch loss = 0.17 (47.9 examples/sec; 0.167 sec/batch; 14h:47m:37s remains)
INFO - root - 2017-12-16 08:27:31.067199: step 13590, loss = 0.63, batch loss = 0.37 (47.8 examples/sec; 0.167 sec/batch; 14h:49m:15s remains)
INFO - root - 2017-12-16 08:27:32.747514: step 13600, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:51m:04s remains)
2017-12-16 08:27:33.197321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14458609 -0.097529888 -0.21810079 -0.53629887 -0.94778585 -1.2854477 -1.5159123 -1.664364 -1.7673202 -1.8324814 -1.9153055 -2.0543094 -2.0193753 -1.742867 -1.3668866][-0.71544349 -0.65476131 -0.75768006 -1.0011384 -1.3325379 -1.6216967 -1.9069862 -2.2293043 -2.5378346 -2.7119157 -2.7695365 -2.7996593 -2.6240518 -2.2672429 -1.9284617][-1.3360322 -1.3022569 -1.4330108 -1.6825194 -1.9978441 -2.3015258 -2.60189 -2.9584124 -3.3309422 -3.5176258 -3.4764953 -3.3331356 -3.0301409 -2.5968337 -2.2440124][-1.6401238 -1.6247203 -1.7700377 -2.040838 -2.3460898 -2.6077843 -2.7906232 -3.0287755 -3.3590398 -3.5559676 -3.5005879 -3.3358231 -3.0628812 -2.6355064 -2.2219737][-1.5378904 -1.4694016 -1.5392557 -1.6981919 -1.8515885 -1.8768916 -1.7721024 -1.777002 -2.0676084 -2.3905089 -2.5451853 -2.6121953 -2.6103485 -2.385354 -2.0022211][-1.1224504 -0.94162095 -0.85105217 -0.77415133 -0.60098636 -0.23607421 0.28090239 0.53435683 0.23431087 -0.35451889 -0.90480256 -1.3568041 -1.7460341 -1.8730657 -1.6735809][-0.57085419 -0.28585029 0.0098700523 0.39425659 0.92650294 1.6964464 2.5788665 3.0410318 2.5994582 1.7202201 0.81631732 0.033127546 -0.65316319 -1.102255 -1.1844857][-0.28121614 0.065652847 0.52132654 1.0927041 1.82869 2.7697797 3.8082404 4.3553042 3.6976023 2.7058024 1.7912464 1.0922391 0.51286674 0.032035112 -0.25472879][-0.59148824 -0.30670214 0.13217378 0.63224363 1.2286754 1.9657969 2.6652575 2.9651127 2.6036725 2.055572 1.6266484 1.4600229 1.3810594 1.2077005 0.89951444][-1.4158804 -1.2975954 -1.002066 -0.68695223 -0.38210702 0.015945911 0.37521243 0.52179241 0.47294211 0.49341559 0.7129283 1.2010283 1.696589 1.9555254 1.8293548][-2.2554467 -2.2842512 -2.1740384 -2.0377209 -1.9296072 -1.7597516 -1.5824356 -1.4716312 -1.3465011 -0.94125307 -0.23135209 0.68825054 1.5256004 2.0055981 2.0536146][-2.671006 -2.8043709 -2.881506 -2.9478626 -2.9884722 -2.961092 -2.8613007 -2.7030346 -2.4227107 -1.8380835 -0.930284 0.064092636 0.87795949 1.3094764 1.3340919][-2.4235575 -2.6356683 -2.8875179 -3.1311848 -3.3104525 -3.3746653 -3.2989886 -3.1251507 -2.8251731 -2.2771854 -1.4319909 -0.62880719 -0.092457294 0.10034704 -0.00016593933][-1.8489165 -2.0799 -2.4383819 -2.749032 -2.9618237 -3.0720351 -3.0431786 -2.9265332 -2.7150598 -2.3224201 -1.7051778 -1.1505243 -0.90090358 -0.94183064 -1.1600031][-1.372952 -1.6363993 -2.0636284 -2.3795266 -2.5839953 -2.7160254 -2.7394617 -2.6694164 -2.5649757 -2.3697047 -2.0002096 -1.666665 -1.592931 -1.7346857 -1.9893426]]...]
INFO - root - 2017-12-16 08:27:34.847132: step 13610, loss = 0.53, batch loss = 0.27 (49.5 examples/sec; 0.162 sec/batch; 14h:19m:06s remains)
INFO - root - 2017-12-16 08:27:36.551063: step 13620, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.166 sec/batch; 14h:44m:53s remains)
INFO - root - 2017-12-16 08:27:38.213100: step 13630, loss = 0.59, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:35m:26s remains)
INFO - root - 2017-12-16 08:27:39.907116: step 13640, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 14h:34m:27s remains)
INFO - root - 2017-12-16 08:27:41.589636: step 13650, loss = 0.50, batch loss = 0.24 (45.8 examples/sec; 0.174 sec/batch; 15h:27m:16s remains)
INFO - root - 2017-12-16 08:27:43.242125: step 13660, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 14h:26m:14s remains)
INFO - root - 2017-12-16 08:27:44.908781: step 13670, loss = 0.55, batch loss = 0.29 (46.4 examples/sec; 0.172 sec/batch; 15h:16m:21s remains)
INFO - root - 2017-12-16 08:27:46.575138: step 13680, loss = 0.52, batch loss = 0.26 (46.5 examples/sec; 0.172 sec/batch; 15h:14m:40s remains)
INFO - root - 2017-12-16 08:27:48.230859: step 13690, loss = 0.59, batch loss = 0.33 (49.8 examples/sec; 0.161 sec/batch; 14h:14m:10s remains)
INFO - root - 2017-12-16 08:27:49.866931: step 13700, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:56m:52s remains)
2017-12-16 08:27:50.372417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8522549 -2.8894095 -2.8863645 -2.7279551 -2.3736515 -2.0775023 -1.857973 -1.6730527 -1.8038762 -2.252214 -2.5877678 -2.8146644 -2.7998743 -2.2770607 -1.7427996][-4.4033613 -4.4636078 -4.5103025 -4.3791232 -3.9497705 -3.3513579 -2.7301273 -2.2873797 -2.3756771 -2.8492465 -3.2891006 -3.6148057 -3.6316421 -3.1668499 -2.763103][-5.1753616 -5.374783 -5.5735965 -5.5004511 -5.0202827 -4.1930151 -3.1938553 -2.4123018 -2.341321 -2.8036034 -3.3614135 -3.8082752 -3.8725243 -3.5339065 -3.26481][-4.8110332 -5.2914548 -5.6645241 -5.6356163 -5.0664282 -4.0437231 -2.7252631 -1.6025511 -1.422231 -1.9825091 -2.7464714 -3.3229089 -3.4921665 -3.2927942 -3.0677559][-3.845294 -4.5824866 -4.9614677 -4.83828 -4.043972 -2.7484562 -1.082574 0.33578229 0.53492236 -0.29339957 -1.3453884 -2.1370404 -2.5206296 -2.5822947 -2.4529831][-2.6750991 -3.6566339 -3.9683614 -3.5890782 -2.4730957 -0.75266623 1.4093287 3.1585252 3.2719839 2.0548942 0.55725646 -0.65140295 -1.4629754 -1.8513576 -1.8047516][-1.7973278 -2.8939054 -3.0654223 -2.3923602 -0.92925096 1.2697284 4.0111256 6.1375952 5.9997368 4.1771984 2.1740983 0.49045706 -0.79351711 -1.5140706 -1.4945781][-1.7646449 -2.8131464 -2.8353457 -1.914282 -0.15750194 2.4018986 5.5512447 8.0081978 7.4839458 5.0108452 2.6303356 0.67309 -0.81857014 -1.7220848 -1.7898185][-2.5500193 -3.3882978 -3.2966754 -2.3391891 -0.62251806 1.7608898 4.4912319 6.4339724 5.9230719 3.7620308 1.5858324 -0.14173293 -1.4907386 -2.3508575 -2.4952888][-3.5862517 -4.1710043 -4.0442238 -3.2784433 -1.896506 -0.037576675 1.9916313 3.3383315 2.9902632 1.4005811 -0.24442863 -1.572181 -2.5803621 -3.2244592 -3.3893728][-4.5862274 -4.9141922 -4.7615576 -4.2223206 -3.255127 -1.9777688 -0.61772931 0.2078495 -0.049991608 -1.0676441 -2.1573658 -3.0847921 -3.708111 -4.0129776 -3.9834545][-5.0217619 -5.1612363 -5.0374994 -4.7263412 -4.1346893 -3.3647702 -2.5824039 -2.1484859 -2.3492444 -2.9788404 -3.6493158 -4.1703792 -4.3657637 -4.27362 -4.0059795][-4.6151414 -4.6459732 -4.5632849 -4.4115853 -4.1287718 -3.7646017 -3.3925633 -3.2340443 -3.4329345 -3.8228307 -4.1917648 -4.40211 -4.2636619 -3.9244559 -3.5242147][-3.7340221 -3.5826402 -3.4746754 -3.3930669 -3.2483468 -3.1006222 -3.01523 -3.0852404 -3.3320684 -3.6234915 -3.8291826 -3.8402205 -3.5750747 -3.2054992 -2.8153646][-2.9672532 -2.6245089 -2.4316461 -2.2893076 -2.1131589 -1.9833844 -1.9993923 -2.1862252 -2.4547565 -2.6942832 -2.7818432 -2.6960623 -2.4664555 -2.200078 -1.9413618]]...]
INFO - root - 2017-12-16 08:27:52.033326: step 13710, loss = 0.52, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 15h:05m:14s remains)
INFO - root - 2017-12-16 08:27:53.732161: step 13720, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:51m:44s remains)
INFO - root - 2017-12-16 08:27:55.420647: step 13730, loss = 0.48, batch loss = 0.22 (47.5 examples/sec; 0.169 sec/batch; 14h:55m:25s remains)
INFO - root - 2017-12-16 08:27:57.094107: step 13740, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 14h:42m:02s remains)
INFO - root - 2017-12-16 08:27:58.769267: step 13750, loss = 0.68, batch loss = 0.42 (47.9 examples/sec; 0.167 sec/batch; 14h:47m:57s remains)
INFO - root - 2017-12-16 08:28:00.427684: step 13760, loss = 0.55, batch loss = 0.29 (48.8 examples/sec; 0.164 sec/batch; 14h:31m:22s remains)
INFO - root - 2017-12-16 08:28:02.095738: step 13770, loss = 0.51, batch loss = 0.25 (46.6 examples/sec; 0.172 sec/batch; 15h:11m:25s remains)
INFO - root - 2017-12-16 08:28:03.755202: step 13780, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:42m:53s remains)
INFO - root - 2017-12-16 08:28:05.436417: step 13790, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 14h:52m:31s remains)
INFO - root - 2017-12-16 08:28:07.089409: step 13800, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:50m:38s remains)
2017-12-16 08:28:07.511236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3805529 -1.2188545 -1.1820747 -1.2455856 -1.2310921 -1.1653931 -1.1206871 -1.3897128 -1.8345028 -2.3691826 -2.8194509 -3.0239692 -2.9473705 -2.7411151 -2.5407586][-1.4689364 -1.410164 -1.4718812 -1.5322669 -1.3788021 -1.0949074 -0.79060066 -0.84861934 -1.2318034 -1.8510702 -2.4190824 -2.6986127 -2.6267347 -2.3690767 -2.0842485][-1.5547376 -1.6048017 -1.7369529 -1.7532425 -1.451472 -0.84607613 -0.20600915 -0.0055935383 -0.28969979 -0.91134512 -1.58961 -1.9416726 -1.9210598 -1.6835314 -1.3794621][-1.6261634 -1.7455008 -1.9033575 -1.8914164 -1.4525111 -0.58695316 0.44752097 1.0243151 0.9121182 0.24771261 -0.5298059 -0.98908269 -1.1019921 -1.0112505 -0.82465541][-1.6556773 -1.8223093 -1.9753301 -1.9641755 -1.5399959 -0.55473471 0.71778774 1.6647599 1.8020556 1.0976238 0.19212341 -0.43042696 -0.7162379 -0.7986505 -0.7493366][-1.6338365 -1.8146276 -2.0016813 -2.0239782 -1.6592817 -0.77777636 0.49097133 1.5763335 1.859041 1.1836112 0.17699957 -0.58375823 -0.94939709 -1.1040225 -1.163783][-1.6044062 -1.7822285 -2.0225298 -2.0865932 -1.7741739 -1.0519484 -0.00808382 0.98311591 1.2846003 0.67979383 -0.36534178 -1.1695527 -1.5095804 -1.6160705 -1.6613997][-1.6175263 -1.7970775 -2.0684125 -2.1839449 -1.9221748 -1.3324089 -0.51653433 0.3071804 0.64171529 0.17237186 -0.79248023 -1.5466921 -1.8337462 -1.86854 -1.84892][-1.6499618 -1.8602132 -2.1421576 -2.2907104 -2.1134696 -1.6745634 -1.0100381 -0.25655913 0.14210415 -0.16702628 -0.95161867 -1.62095 -1.8699734 -1.8308446 -1.7303405][-1.6654317 -1.895816 -2.2092593 -2.3996925 -2.3432822 -2.0747111 -1.5162473 -0.74287593 -0.25796962 -0.38839281 -0.97824049 -1.5593827 -1.7835715 -1.7747616 -1.7091372][-1.6456888 -1.8770685 -2.2330725 -2.5195856 -2.5820532 -2.465718 -2.0452058 -1.3611028 -0.806687 -0.7684952 -1.2148423 -1.7176011 -1.9436576 -1.9717867 -1.9141827][-1.5809805 -1.8207164 -2.2181747 -2.5999575 -2.8296115 -2.8945448 -2.6608994 -2.1052814 -1.5399432 -1.3547878 -1.6291156 -2.0403476 -2.2547114 -2.3278008 -2.3058429][-1.5259612 -1.7646546 -2.1821642 -2.6204576 -2.9873266 -3.2257841 -3.1727712 -2.7754312 -2.2376471 -1.9621437 -2.071795 -2.3707323 -2.5751123 -2.7027354 -2.7314239][-1.5071759 -1.7407268 -2.1360056 -2.5724518 -2.9905455 -3.3331773 -3.4124641 -3.1432745 -2.6809993 -2.3814933 -2.3837166 -2.5897427 -2.7796488 -2.9504032 -3.0383043][-1.5041833 -1.7219993 -2.0801387 -2.467521 -2.8798928 -3.2661881 -3.4229183 -3.2662601 -2.9292431 -2.6838326 -2.6357632 -2.7670047 -2.9259961 -3.0810206 -3.1702197]]...]
INFO - root - 2017-12-16 08:28:09.208273: step 13810, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:44m:17s remains)
INFO - root - 2017-12-16 08:28:10.871872: step 13820, loss = 0.62, batch loss = 0.36 (49.5 examples/sec; 0.161 sec/batch; 14h:17m:42s remains)
INFO - root - 2017-12-16 08:28:12.531732: step 13830, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:41m:45s remains)
INFO - root - 2017-12-16 08:28:14.184527: step 13840, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 14h:28m:55s remains)
INFO - root - 2017-12-16 08:28:15.863569: step 13850, loss = 0.50, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 14h:52m:31s remains)
INFO - root - 2017-12-16 08:28:17.567209: step 13860, loss = 0.48, batch loss = 0.22 (46.8 examples/sec; 0.171 sec/batch; 15h:08m:44s remains)
INFO - root - 2017-12-16 08:28:19.259459: step 13870, loss = 0.49, batch loss = 0.23 (45.6 examples/sec; 0.175 sec/batch; 15h:31m:56s remains)
INFO - root - 2017-12-16 08:28:20.959415: step 13880, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:29m:31s remains)
INFO - root - 2017-12-16 08:28:22.619243: step 13890, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:47m:16s remains)
INFO - root - 2017-12-16 08:28:24.307640: step 13900, loss = 0.58, batch loss = 0.32 (47.1 examples/sec; 0.170 sec/batch; 15h:02m:50s remains)
2017-12-16 08:28:24.751210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7761616 -2.1753941 -2.5751357 -2.8774703 -3.1272464 -3.261822 -3.17322 -3.0228138 -2.8552735 -2.6457715 -2.478405 -2.5386031 -2.8702178 -3.3584831 -3.7663798][-2.0949459 -2.463939 -2.8720527 -3.1891992 -3.3988492 -3.3494673 -3.0368316 -2.6376171 -2.236289 -1.8516176 -1.7668805 -2.0532196 -2.7055979 -3.5186014 -4.1701856][-2.6425927 -2.89602 -3.1849918 -3.4319477 -3.5482492 -3.3284316 -2.6998107 -1.865533 -1.1197343 -0.61232126 -0.74154472 -1.4422164 -2.5456111 -3.6659455 -4.4488158][-3.0813186 -3.1467876 -3.2747927 -3.4173498 -3.3888497 -2.8981347 -1.9058012 -0.6957618 0.2564497 0.55190611 -0.066463709 -1.313067 -2.7805481 -3.9830503 -4.6480045][-3.221571 -3.1190548 -3.1008821 -3.1189096 -2.8864315 -2.1499512 -0.9322319 0.39102602 1.3153965 1.2284913 0.022997856 -1.6635392 -3.2710316 -4.3618989 -4.7293568][-3.2215242 -2.9903405 -2.8498976 -2.6773708 -2.2282093 -1.2830911 0.0030217171 1.2980046 1.984175 1.4332008 -0.14867592 -2.0334299 -3.6828189 -4.5787196 -4.6144753][-3.1409323 -2.82575 -2.5486298 -2.1445322 -1.4144716 -0.33403492 0.98754525 2.1301157 2.421592 1.4057055 -0.41610742 -2.3305233 -3.8574255 -4.4991817 -4.2705555][-3.0308352 -2.6213114 -2.1848221 -1.5471958 -0.6028806 0.52595496 1.7717416 2.7252886 2.494447 1.0663209 -0.81225419 -2.5988829 -3.8365302 -4.1328268 -3.7329526][-2.9501538 -2.520242 -2.02703 -1.3608183 -0.37280023 0.74095011 1.8145716 2.4627736 1.9446075 0.48987985 -1.2155535 -2.717241 -3.5698447 -3.5787764 -3.1343632][-2.9153659 -2.5597982 -2.1022887 -1.4688632 -0.60148072 0.40705848 1.2844701 1.6525102 1.1267517 -0.078168631 -1.5014048 -2.6753471 -3.2283297 -3.1190684 -2.7071984][-2.6708925 -2.4070106 -2.0058327 -1.4156466 -0.68407691 0.11896706 0.81395411 1.1426227 0.77594781 -0.20522428 -1.3599503 -2.3156846 -2.7793264 -2.6669686 -2.3059647][-2.2369428 -2.0741889 -1.6914635 -1.1422526 -0.50970292 0.12290454 0.72992325 1.0942109 0.8916986 0.13004088 -0.7921536 -1.5954014 -2.0157461 -1.9893672 -1.7699969][-1.9860493 -1.8364637 -1.4270525 -0.90029347 -0.37298143 0.10358882 0.61025071 1.0497899 1.0245638 0.55989623 -0.10266376 -0.71214116 -1.115967 -1.2610139 -1.1671641][-2.05589 -1.7770967 -1.2587079 -0.75552237 -0.40084517 -0.11252904 0.29319358 0.73932505 0.85042071 0.65390873 0.304991 -0.083954811 -0.48333836 -0.71625841 -0.65358019][-2.2006586 -1.7275484 -1.0845339 -0.62818933 -0.46880639 -0.38027394 -0.12347865 0.20629382 0.34960055 0.45939136 0.48233175 0.29309416 -0.018849373 -0.22582269 -0.17926955]]...]
INFO - root - 2017-12-16 08:28:26.429609: step 13910, loss = 0.60, batch loss = 0.34 (45.1 examples/sec; 0.177 sec/batch; 15h:42m:16s remains)
INFO - root - 2017-12-16 08:28:28.143521: step 13920, loss = 0.65, batch loss = 0.39 (45.8 examples/sec; 0.175 sec/batch; 15h:27m:36s remains)
INFO - root - 2017-12-16 08:28:29.837560: step 13930, loss = 0.61, batch loss = 0.35 (43.3 examples/sec; 0.185 sec/batch; 16h:21m:59s remains)
INFO - root - 2017-12-16 08:28:31.494426: step 13940, loss = 0.68, batch loss = 0.42 (49.5 examples/sec; 0.162 sec/batch; 14h:18m:49s remains)
INFO - root - 2017-12-16 08:28:33.159550: step 13950, loss = 0.63, batch loss = 0.37 (49.6 examples/sec; 0.161 sec/batch; 14h:16m:28s remains)
INFO - root - 2017-12-16 08:28:34.808720: step 13960, loss = 0.62, batch loss = 0.36 (49.2 examples/sec; 0.163 sec/batch; 14h:23m:49s remains)
INFO - root - 2017-12-16 08:28:36.463391: step 13970, loss = 0.53, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 14h:57m:08s remains)
INFO - root - 2017-12-16 08:28:38.152287: step 13980, loss = 0.58, batch loss = 0.32 (45.2 examples/sec; 0.177 sec/batch; 15h:39m:24s remains)
INFO - root - 2017-12-16 08:28:39.840804: step 13990, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:48m:23s remains)
INFO - root - 2017-12-16 08:28:41.528551: step 14000, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 14h:28m:26s remains)
2017-12-16 08:28:42.005740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.10241 -1.8959377 -1.7067127 -1.5601096 -1.4848007 -1.4413731 -1.4090261 -1.3625319 -1.2920716 -1.2096316 -1.1204404 -1.0234449 -0.92504048 -0.83453369 -0.75398231][-2.7249296 -2.5810943 -2.3991508 -2.2748206 -2.2091305 -2.1208892 -2.0054851 -1.8555901 -1.7043067 -1.5762186 -1.4522368 -1.316388 -1.1746941 -1.0346618 -0.91086078][-3.0922649 -3.1176679 -3.0164902 -3.0246389 -3.1042383 -3.0484614 -2.8770583 -2.6955156 -2.5520637 -2.4325602 -2.3166828 -2.1793311 -2.0105488 -1.8114804 -1.6435995][-2.8348296 -3.0913167 -3.1010914 -3.2914138 -3.53937 -3.5059323 -3.2968123 -3.2139111 -3.2504797 -3.251632 -3.2420442 -3.2094226 -3.0652776 -2.8105557 -2.5930009][-1.808804 -2.3398569 -2.4341018 -2.6780629 -2.9965909 -2.8902268 -2.5971603 -2.6765349 -2.9828207 -3.203733 -3.4041588 -3.6114869 -3.6005273 -3.3757133 -3.1889186][-0.58457994 -1.217362 -1.2424831 -1.3963237 -1.6303105 -1.262647 -0.77879417 -1.0413015 -1.6723773 -2.1225734 -2.5575342 -3.1057937 -3.3364532 -3.2246535 -3.1329594][0.28697395 -0.2321713 -0.067281961 -0.029068232 -0.048121691 0.67136288 1.3827868 0.93249345 -0.0076212883 -0.5358417 -1.08831 -1.9263296 -2.4120517 -2.4667809 -2.5152776][0.45914102 0.22588062 0.60522652 0.82634115 1.040431 2.0903637 2.9867923 2.2861392 1.2150488 0.7966125 0.27562451 -0.71583796 -1.3389986 -1.5060918 -1.6424797][-0.12831259 -0.07597065 0.42557359 0.76997566 1.0779757 2.1208684 2.9171312 2.2822621 1.3703167 1.0742044 0.71883774 -0.12152934 -0.66464961 -0.84300041 -0.97445583][-0.79609883 -0.76562715 -0.39793015 -0.16906524 0.015436411 0.71473145 1.2893753 0.92430043 0.27032232 0.067551136 -0.080724716 -0.58695424 -0.9113754 -1.0057628 -1.0541596][-1.2117268 -1.4780059 -1.4496437 -1.4313285 -1.5158772 -1.2346838 -0.90549874 -1.1405786 -1.5825167 -1.6962709 -1.688988 -1.8934966 -1.9982563 -1.9684838 -1.9261608][-1.2131079 -1.8654816 -2.238426 -2.421541 -2.7185502 -2.8143573 -2.7547374 -2.9391778 -3.209558 -3.2690411 -3.2105484 -3.249949 -3.2450891 -3.1740088 -3.08321][-0.90980279 -1.8266954 -2.4994273 -2.8434331 -3.2338543 -3.5197806 -3.5946 -3.7158966 -3.8245969 -3.8411345 -3.7647438 -3.7319307 -3.7277188 -3.7092743 -3.6741991][-0.3679862 -1.3598154 -2.2108996 -2.6698604 -3.0135956 -3.3102796 -3.430954 -3.4342895 -3.415678 -3.381393 -3.3114727 -3.2686102 -3.2902322 -3.3528936 -3.4385993][0.077542543 -0.87634206 -1.7407078 -2.1888397 -2.3498492 -2.496541 -2.539521 -2.4342206 -2.3092337 -2.2404883 -2.1827211 -2.1432426 -2.205725 -2.389966 -2.6817222]]...]
INFO - root - 2017-12-16 08:28:43.717547: step 14010, loss = 0.45, batch loss = 0.19 (48.8 examples/sec; 0.164 sec/batch; 14h:30m:59s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:28:45.344433: step 14020, loss = 0.52, batch loss = 0.26 (50.2 examples/sec; 0.159 sec/batch; 14h:06m:30s remains)
INFO - root - 2017-12-16 08:28:46.999626: step 14030, loss = 0.45, batch loss = 0.19 (47.8 examples/sec; 0.167 sec/batch; 14h:47m:27s remains)
INFO - root - 2017-12-16 08:28:48.658858: step 14040, loss = 0.53, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 14h:57m:39s remains)
INFO - root - 2017-12-16 08:28:50.346795: step 14050, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:40m:58s remains)
INFO - root - 2017-12-16 08:28:51.973830: step 14060, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 14h:39m:11s remains)
INFO - root - 2017-12-16 08:28:53.689587: step 14070, loss = 0.59, batch loss = 0.33 (46.6 examples/sec; 0.172 sec/batch; 15h:11m:58s remains)
INFO - root - 2017-12-16 08:28:55.361043: step 14080, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 14h:30m:55s remains)
INFO - root - 2017-12-16 08:28:57.059673: step 14090, loss = 0.47, batch loss = 0.21 (46.3 examples/sec; 0.173 sec/batch; 15h:16m:38s remains)
INFO - root - 2017-12-16 08:28:58.742542: step 14100, loss = 0.64, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 14h:39m:29s remains)
2017-12-16 08:28:59.204646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4780576 -1.471951 -1.4839613 -1.5536933 -1.6854881 -1.8704063 -2.0106916 -2.0168786 -1.9012854 -1.8395752 -1.9599006 -2.221554 -2.4196117 -2.4062562 -2.2274742][-1.7016079 -1.7096099 -1.7863281 -1.9846475 -2.2901931 -2.5990324 -2.757257 -2.635361 -2.3157787 -2.0638003 -2.0831554 -2.3811419 -2.6429341 -2.6325119 -2.4133863][-1.9461899 -1.9870967 -2.1514552 -2.4928324 -2.9522078 -3.2729626 -3.2806926 -2.9335365 -2.4001889 -2.0576749 -2.085731 -2.4474347 -2.7872996 -2.7922757 -2.5447342][-2.0891919 -2.2013545 -2.4394174 -2.8608379 -3.349165 -3.4841905 -3.1331806 -2.4718282 -1.8237381 -1.6123157 -1.8971416 -2.415163 -2.8560779 -2.8626657 -2.6298306][-2.0752497 -2.2653944 -2.5581934 -2.9422212 -3.2709355 -3.0625477 -2.2504485 -1.218161 -0.48448098 -0.58567321 -1.3190749 -2.1557412 -2.7492094 -2.859561 -2.6955366][-2.0462024 -2.2963552 -2.5985181 -2.8180792 -2.797303 -2.1512887 -0.889168 0.48790264 1.3115056 0.80627465 -0.49751925 -1.7748652 -2.5985947 -2.8717673 -2.7570944][-2.0451441 -2.3004317 -2.5471232 -2.5283756 -2.1472983 -1.1742058 0.30219507 1.8395565 2.6634462 1.8930809 0.097136021 -1.5726678 -2.5782249 -2.927007 -2.8271132][-2.0431743 -2.188163 -2.331779 -2.1443231 -1.525287 -0.37623608 1.0634906 2.377969 3.0925243 2.2332742 0.21592379 -1.7522559 -2.8124998 -3.0453172 -2.8746014][-2.0137367 -1.9863652 -1.929805 -1.6425319 -0.94943142 0.16098332 1.299288 2.2106035 2.7736008 2.1017935 0.062995434 -2.02685 -3.0538404 -3.1326346 -2.8630776][-2.0217419 -1.8872039 -1.6535642 -1.3179892 -0.69808912 0.23444366 1.0809715 1.7410762 2.2266324 1.8061068 -0.047132492 -2.0929365 -3.1469021 -3.1103189 -2.7877421][-2.187372 -2.0893481 -1.8353471 -1.4827185 -0.96579444 -0.25494766 0.40889239 1.0052385 1.5506105 1.3415813 -0.22924495 -2.116606 -3.1283877 -3.0409029 -2.7110267][-2.48733 -2.533376 -2.3626304 -2.0670283 -1.6893046 -1.1046486 -0.39276063 0.30470657 0.82941127 0.748436 -0.45572925 -2.0728712 -2.98355 -2.910975 -2.6481051][-2.8097167 -2.9658577 -2.9152193 -2.6525409 -2.3074341 -1.7056905 -0.795547 0.10801673 0.58634162 0.4416883 -0.54546463 -1.9271251 -2.7768764 -2.8036895 -2.607343][-2.9876351 -3.1582077 -3.1590602 -2.9182348 -2.4935052 -1.7406459 -0.64677322 0.31484842 0.64304161 0.32319355 -0.554281 -1.8047719 -2.6226006 -2.7562869 -2.59314][-2.817893 -2.8613808 -2.8848825 -2.746964 -2.2962837 -1.4534273 -0.32165527 0.55483627 0.6766181 0.19342041 -0.64480972 -1.8218311 -2.6607385 -2.8105011 -2.610734]]...]
INFO - root - 2017-12-16 08:29:00.923319: step 14110, loss = 0.61, batch loss = 0.35 (47.2 examples/sec; 0.169 sec/batch; 14h:58m:27s remains)
INFO - root - 2017-12-16 08:29:02.575223: step 14120, loss = 0.61, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 14h:24m:41s remains)
INFO - root - 2017-12-16 08:29:04.225652: step 14130, loss = 0.66, batch loss = 0.40 (46.2 examples/sec; 0.173 sec/batch; 15h:19m:08s remains)
INFO - root - 2017-12-16 08:29:05.973095: step 14140, loss = 0.61, batch loss = 0.35 (44.8 examples/sec; 0.179 sec/batch; 15h:47m:16s remains)
INFO - root - 2017-12-16 08:29:07.666748: step 14150, loss = 0.70, batch loss = 0.44 (47.6 examples/sec; 0.168 sec/batch; 14h:52m:13s remains)
INFO - root - 2017-12-16 08:29:09.342472: step 14160, loss = 0.54, batch loss = 0.28 (51.0 examples/sec; 0.157 sec/batch; 13h:52m:49s remains)
INFO - root - 2017-12-16 08:29:11.020461: step 14170, loss = 0.65, batch loss = 0.39 (47.6 examples/sec; 0.168 sec/batch; 14h:52m:17s remains)
INFO - root - 2017-12-16 08:29:12.696741: step 14180, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 14h:47m:36s remains)
INFO - root - 2017-12-16 08:29:14.379940: step 14190, loss = 0.53, batch loss = 0.27 (49.3 examples/sec; 0.162 sec/batch; 14h:20m:19s remains)
INFO - root - 2017-12-16 08:29:16.049088: step 14200, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:39m:43s remains)
2017-12-16 08:29:16.543732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5610111 -1.5365331 -1.4974511 -1.4857421 -1.5156882 -1.5773408 -1.6163756 -1.6461384 -1.7064581 -1.7728584 -1.7890122 -1.7252014 -1.5553999 -1.3013444 -1.0449258][-1.9086093 -2.007823 -2.07335 -2.1123674 -2.1846356 -2.3172915 -2.4429579 -2.5982766 -2.7852271 -2.9303422 -2.9447596 -2.8236134 -2.5307505 -2.0847101 -1.613523][-2.3375578 -2.5627439 -2.6703982 -2.6245539 -2.5530727 -2.5595348 -2.6399922 -2.9201059 -3.32019 -3.6504984 -3.7880852 -3.7400913 -3.4702559 -3.0121748 -2.4793391][-2.6045318 -2.9035215 -2.978971 -2.7010396 -2.2940261 -1.9828509 -1.8250866 -2.0698793 -2.6419663 -3.2284982 -3.6568995 -3.8827729 -3.8544524 -3.609374 -3.201602][-2.5663085 -2.8051517 -2.7360322 -2.1316333 -1.2959059 -0.5218451 0.0855062 0.1068275 -0.4893961 -1.3469076 -2.2163949 -2.9602556 -3.4176521 -3.5840755 -3.4346392][-2.3391833 -2.4104853 -2.1396179 -1.1575983 0.16730881 1.493644 2.7070735 3.1906006 2.7664864 1.7305262 0.34592485 -1.104268 -2.2784162 -3.0788407 -3.3135929][-2.2319272 -2.165489 -1.6808798 -0.33496666 1.483531 3.3595912 5.1551819 6.2561855 6.1288147 5.1616669 3.38319 1.2333815 -0.73059618 -2.2825544 -3.0151582][-2.3897977 -2.315146 -1.78191 -0.37410295 1.6602044 3.8284037 5.917285 7.3762016 7.5900917 7.0753222 5.445734 3.1020057 0.71754432 -1.3491912 -2.5640376][-2.6676195 -2.7904062 -2.5164618 -1.5000846 0.14927554 1.9990981 3.8087533 5.1169872 5.6744604 5.7969742 4.9356556 3.0857131 0.85280585 -1.2111491 -2.5802581][-2.7925036 -3.223362 -3.4377642 -3.11376 -2.2153454 -1.0235722 0.23349977 1.1821251 1.8148682 2.362983 2.3027074 1.1641934 -0.50685513 -2.18477 -3.358501][-2.7065063 -3.4093013 -4.1024332 -4.3908858 -4.1526513 -3.5791659 -2.8127859 -2.164134 -1.6483796 -0.99112475 -0.61454928 -1.1626228 -2.2112226 -3.3281531 -4.1806049][-2.4718523 -3.2351956 -4.1056786 -4.7380419 -4.9287539 -4.7544279 -4.3571281 -3.9719563 -3.6354179 -3.0522232 -2.4318891 -2.4691856 -2.97168 -3.5979347 -4.1661868][-2.1318524 -2.7614872 -3.5636535 -4.2490067 -4.6205463 -4.6755605 -4.501298 -4.3249359 -4.1720338 -3.6886468 -2.9269669 -2.5555005 -2.6085167 -2.8698053 -3.2349842][-1.7876011 -2.1907289 -2.7663622 -3.3388751 -3.7041523 -3.8430796 -3.8177726 -3.7693095 -3.7313278 -3.3666568 -2.6506553 -2.0927968 -1.8659867 -1.8894987 -2.0895119][-1.5498836 -1.7552148 -2.0882044 -2.4497559 -2.7046223 -2.8241804 -2.8460524 -2.8745055 -2.9042666 -2.6911094 -2.1868746 -1.7285941 -1.4771607 -1.4175217 -1.4975147]]...]
INFO - root - 2017-12-16 08:29:18.249166: step 14210, loss = 0.50, batch loss = 0.24 (47.2 examples/sec; 0.170 sec/batch; 14h:59m:53s remains)
INFO - root - 2017-12-16 08:29:19.911059: step 14220, loss = 0.68, batch loss = 0.42 (46.6 examples/sec; 0.172 sec/batch; 15h:10m:23s remains)
INFO - root - 2017-12-16 08:29:21.593139: step 14230, loss = 0.57, batch loss = 0.31 (47.2 examples/sec; 0.170 sec/batch; 14h:59m:16s remains)
INFO - root - 2017-12-16 08:29:23.280065: step 14240, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 14h:31m:12s remains)
INFO - root - 2017-12-16 08:29:24.944962: step 14250, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:45m:53s remains)
INFO - root - 2017-12-16 08:29:26.645360: step 14260, loss = 0.50, batch loss = 0.24 (44.3 examples/sec; 0.181 sec/batch; 15h:58m:12s remains)
INFO - root - 2017-12-16 08:29:28.308487: step 14270, loss = 0.75, batch loss = 0.49 (47.7 examples/sec; 0.168 sec/batch; 14h:50m:05s remains)
INFO - root - 2017-12-16 08:29:30.037603: step 14280, loss = 0.47, batch loss = 0.21 (48.1 examples/sec; 0.166 sec/batch; 14h:42m:28s remains)
INFO - root - 2017-12-16 08:29:31.717417: step 14290, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:45m:34s remains)
INFO - root - 2017-12-16 08:29:33.379940: step 14300, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 14h:30m:45s remains)
2017-12-16 08:29:33.835944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4847959 -1.2996118 -1.3067718 -1.4379977 -1.5445099 -1.5820386 -1.5653285 -1.4526829 -1.3859429 -1.419378 -1.4767803 -1.5676057 -1.7305171 -1.8592342 -1.9286848][-1.0819896 -0.8684262 -0.80599868 -0.84756339 -0.87003636 -0.89217234 -1.0188496 -1.2039814 -1.4209931 -1.6147993 -1.7368197 -1.8446667 -1.9821278 -2.0753603 -2.1174674][-0.64913929 -0.51533413 -0.44026971 -0.35267568 -0.20849729 -0.16647053 -0.42913067 -0.87295234 -1.2945554 -1.6189526 -1.8353008 -2.0306044 -2.2212582 -2.334295 -2.3656039][-0.066501141 -0.0696969 -0.065873146 0.079310179 0.352535 0.42418838 0.12754726 -0.37702835 -0.87179053 -1.3167949 -1.7251718 -2.1149948 -2.4247303 -2.5792818 -2.6100285][0.40193677 0.32373953 0.27249432 0.40063643 0.70656586 0.865782 0.66504836 0.20723963 -0.32509494 -0.920208 -1.5495384 -2.1324534 -2.5440493 -2.7394989 -2.7739885][0.43732595 0.43475461 0.47034407 0.66495156 1.010366 1.2657254 1.2066102 0.82861257 0.2403059 -0.49655604 -1.2748947 -1.9487474 -2.4015446 -2.6409214 -2.724437][-0.012809277 0.1685009 0.415123 0.78391767 1.2420385 1.6601088 1.8081226 1.5591073 0.95342731 0.1598978 -0.65506852 -1.3278068 -1.7847135 -2.0901983 -2.2711766][-0.7425822 -0.34003663 0.12470889 0.63729191 1.1684153 1.6659868 1.9834328 1.9453912 1.4919672 0.82291842 0.13309312 -0.45356381 -0.87157249 -1.176017 -1.4200161][-1.4920552 -0.96631587 -0.41044772 0.10489345 0.5274744 0.89454484 1.2184813 1.397634 1.3316514 1.0614517 0.66858768 0.26799345 -0.030681133 -0.25533748 -0.4705][-2.2323084 -1.7764325 -1.2877295 -0.84616446 -0.50801313 -0.23393893 0.051653147 0.35279536 0.60227752 0.74415064 0.73893118 0.61933494 0.47780752 0.35440898 0.25020218][-2.7213466 -2.4814219 -2.2039306 -1.9170222 -1.6707411 -1.4611144 -1.2133193 -0.86827385 -0.43060935 0.017893553 0.36382103 0.52791905 0.53171849 0.48529482 0.49359417][-2.8315783 -2.8138022 -2.7552309 -2.6549976 -2.5431826 -2.4242003 -2.2474213 -1.9414572 -1.4604405 -0.88820529 -0.36876166 -0.033550978 0.062448263 0.067260265 0.15689492][-2.6554875 -2.749243 -2.8091109 -2.8397841 -2.8616319 -2.8735852 -2.8308289 -2.6672726 -2.317733 -1.8438463 -1.3622254 -1.0004314 -0.85163224 -0.81433761 -0.69852042][-2.36066 -2.5037858 -2.6307018 -2.741158 -2.834609 -2.9131835 -2.9590869 -2.9265769 -2.760458 -2.4926717 -2.1840219 -1.9121952 -1.7731383 -1.7300699 -1.6142759][-2.0569694 -2.2374976 -2.405143 -2.5420623 -2.6359231 -2.7058375 -2.762543 -2.7794197 -2.7358556 -2.6471355 -2.5279734 -2.4095564 -2.3416579 -2.318188 -2.2372203]]...]
INFO - root - 2017-12-16 08:29:35.491310: step 14310, loss = 0.51, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 14h:29m:21s remains)
INFO - root - 2017-12-16 08:29:37.178483: step 14320, loss = 0.63, batch loss = 0.37 (47.0 examples/sec; 0.170 sec/batch; 15h:03m:31s remains)
INFO - root - 2017-12-16 08:29:38.867818: step 14330, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:46m:04s remains)
INFO - root - 2017-12-16 08:29:40.558443: step 14340, loss = 0.60, batch loss = 0.34 (48.4 examples/sec; 0.165 sec/batch; 14h:36m:15s remains)
INFO - root - 2017-12-16 08:29:42.229132: step 14350, loss = 0.62, batch loss = 0.36 (47.6 examples/sec; 0.168 sec/batch; 14h:51m:23s remains)
INFO - root - 2017-12-16 08:29:43.894034: step 14360, loss = 0.48, batch loss = 0.22 (49.6 examples/sec; 0.161 sec/batch; 14h:15m:35s remains)
INFO - root - 2017-12-16 08:29:45.556148: step 14370, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 14h:55m:55s remains)
INFO - root - 2017-12-16 08:29:47.231648: step 14380, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:46m:55s remains)
INFO - root - 2017-12-16 08:29:48.901068: step 14390, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 15h:02m:31s remains)
INFO - root - 2017-12-16 08:29:50.605077: step 14400, loss = 0.53, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:59m:52s remains)
2017-12-16 08:29:51.084884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0434602 -0.79468131 -0.68542755 -0.51668656 -0.31286359 -0.3879745 -0.90108478 -1.6733434 -2.214257 -2.2006545 -1.7557919 -1.3383708 -1.2666754 -1.5371245 -1.8766936][-1.1355373 -0.81959939 -0.60975826 -0.36377096 -0.15045619 -0.20945072 -0.741596 -1.541044 -2.0944955 -2.081749 -1.7132673 -1.441779 -1.5138474 -1.8568548 -2.1707928][-1.6503825 -1.3433492 -1.0005991 -0.63251913 -0.33871877 -0.27680302 -0.64128423 -1.2871915 -1.7432859 -1.7548115 -1.5557824 -1.5091274 -1.7182918 -2.0326734 -2.2088578][-2.0629478 -1.7246703 -1.2080014 -0.67542827 -0.28139687 -0.084607124 -0.26154566 -0.7606678 -1.1958964 -1.3440609 -1.4049298 -1.6346382 -1.9485416 -2.1410182 -2.0695305][-2.0443482 -1.6745996 -1.015765 -0.2959609 0.24762917 0.5600481 0.51103497 0.098658562 -0.35876346 -0.70988452 -1.0875744 -1.6394858 -2.1165664 -2.2680359 -2.0166223][-1.8218259 -1.4776087 -0.75757754 0.18562174 0.99163938 1.4880328 1.5307231 1.1758065 0.68376946 0.1414938 -0.54338288 -1.4134403 -2.0899971 -2.2789793 -1.9772246][-1.9212465 -1.7086282 -1.0549339 0.0062053204 1.0160477 1.6668298 1.8583782 1.6273599 1.2159009 0.674284 -0.099413395 -1.0873853 -1.8296933 -2.0265942 -1.7283657][-2.3876762 -2.3392248 -1.861186 -0.85271132 0.16488361 0.86048985 1.1816924 1.111974 0.88182378 0.50580525 -0.13537431 -0.96695578 -1.5709755 -1.6474223 -1.2756009][-2.9275308 -2.9393358 -2.5203118 -1.5746524 -0.65312159 -0.076337576 0.20503569 0.16966295 0.023156404 -0.17433381 -0.60996056 -1.2056907 -1.5366201 -1.3328825 -0.79268897][-2.9046361 -2.8461385 -2.3998625 -1.5719858 -0.8784368 -0.59600616 -0.6024307 -0.81104612 -0.971521 -1.0604743 -1.2914273 -1.5916164 -1.5907681 -1.1210189 -0.41519046][-2.0986741 -1.9339591 -1.4486297 -0.79238451 -0.4635191 -0.63857353 -1.0901375 -1.5893962 -1.858672 -1.9159873 -1.9953157 -2.0346739 -1.7854359 -1.2210197 -0.48541307][-0.99725747 -0.79286504 -0.296489 0.18579507 0.16777802 -0.42624319 -1.2909807 -2.0775321 -2.502986 -2.5895896 -2.576633 -2.4820573 -2.1847963 -1.7167585 -1.1544685][-0.20054674 -0.028748751 0.47834253 0.878927 0.613142 -0.25399923 -1.3346925 -2.2677486 -2.8056173 -2.9838209 -2.9898808 -2.9085007 -2.7381167 -2.5124958 -2.2362549][0.031260252 0.18927097 0.72879219 1.1228092 0.79133177 -0.11970091 -1.1955564 -2.1207478 -2.7132597 -2.9974284 -3.1088684 -3.1419187 -3.1849675 -3.2101064 -3.1758604][-0.053666353 0.21348405 0.87336111 1.3250761 0.99927592 0.1057322 -0.91495931 -1.8334435 -2.4650269 -2.8097467 -2.9801903 -3.1001582 -3.2636359 -3.4107294 -3.4455991]]...]
INFO - root - 2017-12-16 08:29:52.776269: step 14410, loss = 0.57, batch loss = 0.31 (45.3 examples/sec; 0.176 sec/batch; 15h:35m:36s remains)
INFO - root - 2017-12-16 08:29:54.461978: step 14420, loss = 0.49, batch loss = 0.23 (50.5 examples/sec; 0.158 sec/batch; 13h:59m:30s remains)
INFO - root - 2017-12-16 08:29:56.140870: step 14430, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:45m:48s remains)
INFO - root - 2017-12-16 08:29:57.798812: step 14440, loss = 0.64, batch loss = 0.38 (49.2 examples/sec; 0.163 sec/batch; 14h:22m:20s remains)
INFO - root - 2017-12-16 08:29:59.461440: step 14450, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:28m:17s remains)
INFO - root - 2017-12-16 08:30:01.120948: step 14460, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 14h:46m:19s remains)
INFO - root - 2017-12-16 08:30:02.794645: step 14470, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 14h:20m:41s remains)
INFO - root - 2017-12-16 08:30:04.441694: step 14480, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 14h:39m:02s remains)
INFO - root - 2017-12-16 08:30:06.102146: step 14490, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:43m:28s remains)
INFO - root - 2017-12-16 08:30:07.752528: step 14500, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:41m:47s remains)
2017-12-16 08:30:08.227516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9160104 -3.5067463 -3.7831879 -3.6365261 -3.3080492 -2.8730168 -2.3680804 -1.8896835 -1.6345582 -1.7061408 -2.2024426 -2.8884091 -3.354563 -3.6210155 -3.7123189][-3.3078618 -3.7896123 -3.9203234 -3.5060809 -2.7817924 -1.9824059 -1.2918988 -0.81624842 -0.7570399 -1.0632929 -1.8300172 -2.7541962 -3.3385279 -3.574492 -3.5909486][-3.6109447 -3.9646635 -3.9503455 -3.2890592 -2.1372771 -0.90225279 0.025342703 0.41456723 0.27006197 -0.27445555 -1.3087368 -2.4949527 -3.2200351 -3.4335828 -3.3481436][-3.4769573 -3.7913003 -3.7342219 -2.92104 -1.4478493 0.15582156 1.3187556 1.738708 1.4140093 0.58646536 -0.68665695 -2.0797322 -2.9429619 -3.187278 -3.0791578][-2.9300728 -3.2993693 -3.2825973 -2.4619539 -0.80807483 1.0639989 2.4827309 3.0285759 2.4817224 1.3434658 -0.13240099 -1.6762338 -2.6476214 -2.9412832 -2.8265574][-2.0710731 -2.4484632 -2.4984467 -1.7445807 -0.06180954 1.9032125 3.4973793 4.1101971 3.2672653 1.7747059 0.14410615 -1.3635372 -2.2757437 -2.5302122 -2.3904328][-1.1137017 -1.390589 -1.4299662 -0.79006362 0.610271 2.3141832 3.865366 4.5017505 3.5332522 1.87849 0.20800924 -1.1612331 -1.9467543 -2.1431475 -1.9830863][-0.68047917 -0.78612852 -0.80736351 -0.38890231 0.624521 1.9919429 3.4325376 4.1137204 3.2617779 1.6747465 0.047558069 -1.1622776 -1.7953759 -1.8996389 -1.7211158][-0.85837984 -0.81866121 -0.82141471 -0.55335951 0.18013906 1.2939887 2.6295576 3.3634696 2.6754265 1.2970581 -0.18085003 -1.2082103 -1.6933663 -1.701278 -1.478961][-1.2733114 -1.0875001 -1.1108042 -0.991861 -0.47702694 0.38953781 1.5192654 2.2123642 1.7847128 0.72574449 -0.47204769 -1.2631712 -1.6103203 -1.5774827 -1.3738773][-1.6989787 -1.4837091 -1.571638 -1.6184096 -1.3129882 -0.70138395 0.16510296 0.76439428 0.5928185 -0.13039327 -0.99866986 -1.5839691 -1.8341637 -1.8093699 -1.6556025][-2.2386067 -2.1078162 -2.2905703 -2.4677613 -2.3118293 -1.899009 -1.2583908 -0.77152693 -0.80071688 -1.2770995 -1.8566232 -2.2598128 -2.4405744 -2.4427497 -2.3296318][-2.777791 -2.7358387 -2.9141679 -3.117852 -3.045079 -2.7702367 -2.3314083 -2.0074852 -2.018292 -2.3057218 -2.6470017 -2.8730896 -2.9866848 -3.0034952 -2.9507151][-2.9011688 -2.8733878 -2.986212 -3.1275945 -3.1105928 -2.9844856 -2.7676075 -2.6134698 -2.6434617 -2.7968755 -2.9577746 -3.0473249 -3.1055677 -3.144697 -3.1328754][-2.6248827 -2.5993373 -2.6614256 -2.7704058 -2.8084455 -2.8028903 -2.7420039 -2.6850858 -2.6861436 -2.7497036 -2.8213496 -2.8561745 -2.892909 -2.9296894 -2.9241395]]...]
INFO - root - 2017-12-16 08:30:09.905294: step 14510, loss = 0.57, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 14h:31m:15s remains)
INFO - root - 2017-12-16 08:30:11.577935: step 14520, loss = 0.54, batch loss = 0.28 (49.6 examples/sec; 0.161 sec/batch; 14h:14m:31s remains)
INFO - root - 2017-12-16 08:30:13.257451: step 14530, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:34m:39s remains)
INFO - root - 2017-12-16 08:30:14.922840: step 14540, loss = 0.69, batch loss = 0.43 (48.8 examples/sec; 0.164 sec/batch; 14h:28m:07s remains)
INFO - root - 2017-12-16 08:30:16.593360: step 14550, loss = 0.60, batch loss = 0.34 (50.1 examples/sec; 0.160 sec/batch; 14h:06m:51s remains)
INFO - root - 2017-12-16 08:30:18.248798: step 14560, loss = 0.67, batch loss = 0.41 (47.7 examples/sec; 0.168 sec/batch; 14h:49m:07s remains)
INFO - root - 2017-12-16 08:30:19.947694: step 14570, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 15h:13m:09s remains)
INFO - root - 2017-12-16 08:30:21.649505: step 14580, loss = 0.62, batch loss = 0.36 (50.6 examples/sec; 0.158 sec/batch; 13h:57m:10s remains)
INFO - root - 2017-12-16 08:30:23.302630: step 14590, loss = 0.73, batch loss = 0.47 (47.1 examples/sec; 0.170 sec/batch; 15h:00m:14s remains)
INFO - root - 2017-12-16 08:30:24.963181: step 14600, loss = 0.49, batch loss = 0.23 (47.5 examples/sec; 0.168 sec/batch; 14h:52m:17s remains)
2017-12-16 08:30:25.453280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9868165 -2.2402806 -2.58216 -2.8049436 -2.7880077 -2.5610175 -2.4758759 -2.7397861 -2.8536565 -2.6824427 -2.4573593 -2.3442578 -2.2038021 -1.9028394 -1.501887][-1.8475225 -1.9818685 -2.2392008 -2.4500091 -2.4646602 -2.3082554 -2.3112707 -2.6791494 -2.8145611 -2.5829194 -2.3091636 -2.2644637 -2.2525058 -2.1576514 -1.9330178][-1.9597445 -2.0281074 -2.1389225 -2.1850755 -2.0769558 -1.8507223 -1.8745141 -2.2854848 -2.4123344 -2.1412628 -1.9085846 -2.0070004 -2.2273128 -2.3721378 -2.2680604][-2.2250507 -2.2187216 -2.1333814 -1.9339648 -1.590026 -1.2164572 -1.2223357 -1.6603235 -1.786926 -1.5383925 -1.3564925 -1.6053581 -2.04732 -2.361517 -2.3516207][-2.4373238 -2.3889105 -2.1358221 -1.6848099 -1.0727849 -0.52779472 -0.49999022 -0.95192111 -1.0830022 -0.84185159 -0.69790137 -1.1138884 -1.7654071 -2.1927857 -2.2133741][-2.4756165 -2.3670454 -1.9671094 -1.3354263 -0.53497064 0.1805141 0.25892806 -0.22197485 -0.38710761 -0.17824721 -0.070442677 -0.57941675 -1.4220016 -1.9056313 -1.7906473][-2.3265169 -2.1790979 -1.7245505 -1.0076804 -0.21850228 0.53576732 0.71554875 0.28379154 0.11993837 0.29575515 0.44034123 -0.0878942 -1.0332761 -1.5168601 -1.2902813][-2.1441329 -1.9874159 -1.5643587 -0.98622894 -0.41523719 0.2102232 0.45810771 0.11118531 -0.010411263 0.21842122 0.44109797 -0.044256926 -0.97057986 -1.3723719 -1.1105896][-2.079185 -1.9746587 -1.6936691 -1.346987 -1.0703926 -0.64266825 -0.38315547 -0.69165754 -0.79265618 -0.47333515 -0.18354321 -0.56885278 -1.3765218 -1.7227217 -1.5036203][-2.2105038 -2.2366648 -2.1835678 -2.1008952 -2.0264077 -1.7497115 -1.5227997 -1.8156534 -1.9454044 -1.5847546 -1.2265068 -1.4393076 -2.0510998 -2.3983734 -2.2966919][-2.4685369 -2.6744919 -2.820174 -2.901629 -2.9122252 -2.6722679 -2.4981606 -2.7734232 -2.8937876 -2.5644023 -2.2116559 -2.2982383 -2.7462454 -3.0644066 -3.070262][-2.6646037 -2.9844949 -3.201925 -3.3195596 -3.2897701 -3.0711646 -2.920584 -3.1374023 -3.2488546 -3.0194201 -2.7695725 -2.8190279 -3.141845 -3.4169636 -3.4863582][-2.6394277 -2.968744 -3.1560698 -3.2039557 -3.1022341 -2.8950951 -2.7914839 -2.9644485 -3.0720091 -2.9306221 -2.7865658 -2.8466408 -3.0786819 -3.296268 -3.3875086][-2.4476113 -2.70746 -2.8284466 -2.7859328 -2.6267407 -2.4463534 -2.3930213 -2.5152788 -2.5964141 -2.5327859 -2.46611 -2.5199511 -2.6864469 -2.8586948 -2.9504232][-2.1821339 -2.3376384 -2.4011798 -2.317333 -2.1471362 -1.9958205 -1.9579766 -2.0325437 -2.1018713 -2.0962062 -2.0684934 -2.0914245 -2.1966982 -2.320271 -2.410027]]...]
INFO - root - 2017-12-16 08:30:27.133904: step 14610, loss = 0.59, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 14h:42m:51s remains)
INFO - root - 2017-12-16 08:30:28.813126: step 14620, loss = 0.66, batch loss = 0.40 (49.1 examples/sec; 0.163 sec/batch; 14h:22m:24s remains)
INFO - root - 2017-12-16 08:30:30.482746: step 14630, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 14h:55m:39s remains)
INFO - root - 2017-12-16 08:30:32.156151: step 14640, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:47m:34s remains)
INFO - root - 2017-12-16 08:30:33.829071: step 14650, loss = 0.48, batch loss = 0.22 (48.6 examples/sec; 0.165 sec/batch; 14h:31m:39s remains)
INFO - root - 2017-12-16 08:30:35.485179: step 14660, loss = 0.48, batch loss = 0.22 (49.2 examples/sec; 0.162 sec/batch; 14h:20m:48s remains)
INFO - root - 2017-12-16 08:30:37.135748: step 14670, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 14h:26m:25s remains)
INFO - root - 2017-12-16 08:30:38.805425: step 14680, loss = 0.48, batch loss = 0.22 (49.3 examples/sec; 0.162 sec/batch; 14h:19m:15s remains)
INFO - root - 2017-12-16 08:30:40.465006: step 14690, loss = 0.60, batch loss = 0.34 (46.9 examples/sec; 0.170 sec/batch; 15h:02m:50s remains)
INFO - root - 2017-12-16 08:30:42.125703: step 14700, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:44m:22s remains)
2017-12-16 08:30:42.614840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.767962 -2.4750667 -2.3791761 -2.5334444 -2.7340257 -2.8112257 -2.7142391 -2.5670443 -2.3919649 -2.0726418 -1.6275346 -1.3815099 -1.6149991 -2.2256956 -2.9773915][-3.352016 -3.032716 -2.9527566 -3.2174666 -3.55327 -3.7407708 -3.6509683 -3.4298024 -3.1704247 -2.7425091 -2.1113381 -1.6890731 -1.8340509 -2.4739935 -3.3321638][-3.4611478 -3.0762942 -2.9986715 -3.3309455 -3.7307334 -3.9718156 -3.8960757 -3.627466 -3.3594785 -2.9505191 -2.2684369 -1.7398899 -1.7944224 -2.4486766 -3.3942237][-3.0466056 -2.5530684 -2.393657 -2.6986415 -3.1014197 -3.3634231 -3.2912912 -3.0109479 -2.8326893 -2.6047335 -2.0736208 -1.6033645 -1.6653497 -2.3799274 -3.4092698][-2.0842583 -1.4603932 -1.2028693 -1.4189782 -1.7283026 -1.903496 -1.8166978 -1.5394392 -1.4158955 -1.4359751 -1.232033 -1.0814279 -1.3977468 -2.270721 -3.3739133][-0.76114678 -0.083203316 0.2397356 0.11378622 -0.11588192 -0.191787 -0.054490566 0.2301681 0.30796885 0.059582233 -0.15122771 -0.4725368 -1.1555196 -2.2040284 -3.3302889][0.271739 0.94587326 1.3078177 1.1679616 0.97462177 1.0229669 1.2349064 1.5671051 1.6495681 1.235738 0.64593768 -0.17364025 -1.2213663 -2.4513638 -3.5612569][0.37700677 1.0474133 1.3869462 1.2931283 1.193886 1.37428 1.7089415 2.118576 2.2078176 1.7019014 0.95028329 -0.0767498 -1.3583637 -2.7033188 -3.812037][0.27473116 1.0660956 1.3536561 1.1015759 0.90947771 1.052532 1.4122915 1.9162807 2.1443572 1.7832985 1.2422824 0.34556556 -0.94717157 -2.346761 -3.5109179][0.2860167 1.221714 1.4847522 1.0346715 0.59717464 0.54362941 0.77559352 1.167304 1.4274163 1.4350905 1.2917743 0.78598952 -0.23857975 -1.5692642 -2.8541539][0.29978538 1.2985082 1.5458877 0.90000749 0.14915228 -0.14705253 -0.10837317 0.15025115 0.47341275 0.82059741 1.0987439 0.98826241 0.23497748 -1.0238991 -2.4103332][0.12742686 1.0900729 1.2814119 0.5603385 -0.29126525 -0.68741393 -0.79713345 -0.65048325 -0.30524015 0.24662757 0.81510854 0.92012668 0.34758091 -0.81233025 -2.2647243][-0.38197494 0.44633818 0.56606555 -0.12483215 -0.89134526 -1.2558087 -1.3945954 -1.32908 -1.0163336 -0.40242922 0.27447534 0.54988837 0.15640569 -0.93087661 -2.4327438][-1.2216611 -0.5038619 -0.4158026 -1.0496348 -1.7189841 -1.981612 -2.0336237 -1.9900165 -1.7516505 -1.2078309 -0.5267185 -0.12273526 -0.31810093 -1.2216456 -2.589237][-2.2506082 -1.6934757 -1.606655 -2.0617795 -2.5305874 -2.6739802 -2.6557574 -2.6162331 -2.4777513 -2.095825 -1.5185926 -1.0254229 -0.97040009 -1.5790092 -2.6777236]]...]
INFO - root - 2017-12-16 08:30:44.279366: step 14710, loss = 0.64, batch loss = 0.38 (47.8 examples/sec; 0.167 sec/batch; 14h:45m:46s remains)
INFO - root - 2017-12-16 08:30:45.963460: step 14720, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:38m:50s remains)
INFO - root - 2017-12-16 08:30:47.632511: step 14730, loss = 0.69, batch loss = 0.43 (48.0 examples/sec; 0.167 sec/batch; 14h:41m:50s remains)
INFO - root - 2017-12-16 08:30:49.310870: step 14740, loss = 0.47, batch loss = 0.21 (46.7 examples/sec; 0.171 sec/batch; 15h:07m:54s remains)
INFO - root - 2017-12-16 08:30:50.978017: step 14750, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.167 sec/batch; 14h:45m:32s remains)
INFO - root - 2017-12-16 08:30:52.628805: step 14760, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:36m:27s remains)
INFO - root - 2017-12-16 08:30:54.329433: step 14770, loss = 0.50, batch loss = 0.24 (49.3 examples/sec; 0.162 sec/batch; 14h:19m:08s remains)
INFO - root - 2017-12-16 08:30:56.030366: step 14780, loss = 0.67, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 14h:44m:23s remains)
INFO - root - 2017-12-16 08:30:57.753815: step 14790, loss = 0.68, batch loss = 0.42 (45.0 examples/sec; 0.178 sec/batch; 15h:41m:07s remains)
INFO - root - 2017-12-16 08:30:59.453502: step 14800, loss = 0.50, batch loss = 0.24 (49.8 examples/sec; 0.160 sec/batch; 14h:09m:46s remains)
2017-12-16 08:30:59.962359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.127004 -1.2770685 -1.4245017 -1.5289359 -1.5889373 -1.6000371 -1.5535035 -1.4759128 -1.4047518 -1.37271 -1.3908801 -1.4348178 -1.4786335 -1.4936199 -1.4721384][-1.0080291 -1.2025448 -1.4030762 -1.5587516 -1.6597371 -1.7015436 -1.689631 -1.6557477 -1.6193237 -1.6045645 -1.6239195 -1.6587523 -1.6801996 -1.6588275 -1.5955184][-1.0788009 -1.311377 -1.5541244 -1.7395511 -1.8507395 -1.8960464 -1.9104712 -1.9145443 -1.9145175 -1.9188691 -1.9517351 -1.9896573 -1.9895315 -1.9142569 -1.7805647][-1.4046154 -1.6558647 -1.8989328 -2.0382473 -2.0426238 -1.9576466 -1.8649377 -1.8110487 -1.8144206 -1.895847 -2.0463576 -2.1943178 -2.2539256 -2.1696372 -1.9804809][-1.8360415 -2.0825508 -2.2467687 -2.1874461 -1.8570631 -1.3738706 -0.94120085 -0.70903027 -0.78219604 -1.1665294 -1.6780118 -2.1150413 -2.3434813 -2.3134136 -2.1119046][-2.1046667 -2.2919137 -2.2862756 -1.8874341 -1.0454634 0.025463104 0.95677948 1.3957386 1.1213374 0.22336936 -0.85894346 -1.7201264 -2.1709738 -2.2343636 -2.0549269][-2.1685002 -2.2895756 -2.1139057 -1.4089723 -0.10567212 1.5242763 2.9254894 3.512351 2.9604721 1.5275586 -0.066301107 -1.2656051 -1.8937266 -2.0446837 -1.895125][-2.1263187 -2.2309504 -1.9923067 -1.167527 0.35002279 2.236269 3.7766113 4.3381562 3.6375093 2.0178189 0.27059484 -1.0440645 -1.7413285 -1.9236145 -1.7835217][-2.0814943 -2.2451565 -2.0901747 -1.3759003 0.0040450096 1.7112536 3.0356555 3.447113 2.8080144 1.3995655 -0.15776563 -1.3458471 -1.9618628 -2.0784552 -1.8990405][-2.1609828 -2.384038 -2.3508947 -1.834854 -0.75044346 0.59827185 1.6313684 1.9193478 1.4055946 0.27162528 -1.0013614 -1.9670823 -2.4008372 -2.3737664 -2.1121106][-2.2816765 -2.539501 -2.6245878 -2.3135855 -1.5488641 -0.55133355 0.1868875 0.349648 -0.084768057 -0.9744581 -1.9209433 -2.5595787 -2.7259176 -2.516484 -2.1780813][-2.2818549 -2.5487204 -2.7311485 -2.6274896 -2.1785376 -1.5396314 -1.0730387 -1.0000883 -1.3416071 -1.9518356 -2.5107727 -2.7669935 -2.6643925 -2.3396208 -1.9987773][-2.0294778 -2.2952151 -2.5431044 -2.6152167 -2.4615526 -2.1574452 -1.9022789 -1.848454 -2.03622 -2.3391764 -2.5498285 -2.5309403 -2.2951958 -1.9763752 -1.7031353][-1.5453916 -1.7788854 -2.0376718 -2.2277811 -2.2815964 -2.2037477 -2.0900776 -2.0374088 -2.0793507 -2.1514933 -2.151706 -2.0312262 -1.8233417 -1.6071318 -1.4396179][-1.097598 -1.2704662 -1.4827836 -1.6801879 -1.8092595 -1.8473364 -1.8192308 -1.7731085 -1.7421141 -1.7177325 -1.6558492 -1.5536849 -1.4427085 -1.3529546 -1.2881598]]...]
INFO - root - 2017-12-16 08:31:01.667305: step 14810, loss = 0.64, batch loss = 0.38 (41.1 examples/sec; 0.194 sec/batch; 17h:09m:45s remains)
INFO - root - 2017-12-16 08:31:03.379620: step 14820, loss = 0.49, batch loss = 0.23 (39.2 examples/sec; 0.204 sec/batch; 18h:01m:36s remains)
INFO - root - 2017-12-16 08:31:05.108616: step 14830, loss = 0.54, batch loss = 0.28 (45.6 examples/sec; 0.175 sec/batch; 15h:29m:08s remains)
INFO - root - 2017-12-16 08:31:06.772693: step 14840, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:41m:13s remains)
INFO - root - 2017-12-16 08:31:08.442060: step 14850, loss = 0.67, batch loss = 0.41 (49.4 examples/sec; 0.162 sec/batch; 14h:17m:49s remains)
INFO - root - 2017-12-16 08:31:10.104150: step 14860, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.168 sec/batch; 14h:46m:57s remains)
INFO - root - 2017-12-16 08:31:11.774629: step 14870, loss = 0.52, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 14h:31m:52s remains)
INFO - root - 2017-12-16 08:31:13.424064: step 14880, loss = 0.63, batch loss = 0.37 (49.1 examples/sec; 0.163 sec/batch; 14h:21m:42s remains)
INFO - root - 2017-12-16 08:31:15.069400: step 14890, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:43m:00s remains)
INFO - root - 2017-12-16 08:31:16.748918: step 14900, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:28m:06s remains)
2017-12-16 08:31:17.247976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6354635 -1.5870076 -1.5837739 -1.492973 -1.246542 -0.81510711 -0.45423365 -0.6063081 -1.2777447 -1.8335139 -2.0440876 -2.0627165 -2.0485568 -1.9664569 -1.9270982][-1.2800243 -1.4964545 -1.686974 -1.7734275 -1.6794163 -1.3882618 -1.0239549 -1.012713 -1.513397 -1.9739134 -2.108927 -2.0805795 -2.0549569 -1.9875834 -1.9354388][-1.1322653 -1.4994117 -1.7400857 -1.8319576 -1.7252216 -1.5096941 -1.1883733 -1.0446237 -1.4076271 -1.9088981 -2.1676624 -2.2171896 -2.1947861 -2.1158519 -2.022342][-1.272429 -1.6014838 -1.749634 -1.7195777 -1.4682043 -1.1209835 -0.74198854 -0.53298128 -0.89830303 -1.6212395 -2.189564 -2.4453771 -2.4586806 -2.3440356 -2.174211][-1.4969687 -1.6362213 -1.5677285 -1.3207403 -0.815223 -0.19203687 0.37469172 0.56604385 -0.048100948 -1.137941 -2.0941844 -2.6146176 -2.7045953 -2.5507486 -2.3110948][-1.5721087 -1.4981444 -1.1695421 -0.69124877 0.091930389 1.1158762 2.0146685 2.1660676 1.0904207 -0.48684227 -1.8926753 -2.712558 -2.8923631 -2.6759796 -2.3652546][-1.3633645 -1.1901131 -0.71020508 -0.10765815 0.818336 2.2436733 3.6587882 3.9058218 2.2901645 0.23947787 -1.5425055 -2.6375134 -2.9429176 -2.6913903 -2.336041][-0.9524647 -0.84152222 -0.46405149 0.068836212 0.91678476 2.390429 4.1351905 4.6777363 2.869874 0.68830132 -1.1645796 -2.3881631 -2.7910197 -2.5693414 -2.2362273][-0.67627156 -0.68205261 -0.52540135 -0.23965311 0.31429076 1.4115961 2.8551488 3.3974447 2.1556954 0.38934612 -1.1591079 -2.1983695 -2.573035 -2.3808346 -2.1120944][-0.68108213 -0.79368269 -0.80684996 -0.73297 -0.47049463 0.21857381 1.2658799 1.6986618 0.848793 -0.48455453 -1.6332657 -2.3327422 -2.5229845 -2.2814503 -2.0426667][-1.0009755 -1.1132064 -1.1589895 -1.2044519 -1.1144425 -0.62826836 0.18952394 0.49717808 -0.21168137 -1.326504 -2.2228792 -2.6796682 -2.6752856 -2.3658841 -2.0887][-1.3018976 -1.3883214 -1.4060216 -1.4746439 -1.4861546 -1.1087635 -0.43080354 -0.19005084 -0.80095732 -1.767627 -2.5506577 -2.9138565 -2.8388913 -2.5044446 -2.1888258][-1.3377876 -1.4120059 -1.4033864 -1.4873819 -1.549129 -1.2688456 -0.72848535 -0.53985238 -1.0764471 -1.9595776 -2.6995862 -3.047965 -2.9699996 -2.6490016 -2.2981439][-1.0457008 -1.1575069 -1.15723 -1.2255601 -1.2832086 -1.0834354 -0.66849053 -0.51147711 -1.0407546 -1.9628198 -2.7676041 -3.1641874 -3.1088667 -2.7908547 -2.3935685][-0.62070584 -0.87613559 -0.94836104 -0.96338308 -0.96323395 -0.75378442 -0.32606149 -0.0979538 -0.62457442 -1.6512659 -2.5905895 -3.1112432 -3.1399479 -2.8476837 -2.4362082]]...]
INFO - root - 2017-12-16 08:31:18.920172: step 14910, loss = 0.63, batch loss = 0.37 (48.7 examples/sec; 0.164 sec/batch; 14h:28m:52s remains)
INFO - root - 2017-12-16 08:31:20.588153: step 14920, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 14h:37m:36s remains)
INFO - root - 2017-12-16 08:31:22.234705: step 14930, loss = 0.56, batch loss = 0.30 (47.6 examples/sec; 0.168 sec/batch; 14h:48m:47s remains)
INFO - root - 2017-12-16 08:31:23.904878: step 14940, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 14h:34m:54s remains)
INFO - root - 2017-12-16 08:31:25.589327: step 14950, loss = 0.47, batch loss = 0.21 (48.2 examples/sec; 0.166 sec/batch; 14h:38m:25s remains)
INFO - root - 2017-12-16 08:31:27.244163: step 14960, loss = 0.69, batch loss = 0.43 (49.4 examples/sec; 0.162 sec/batch; 14h:17m:29s remains)
INFO - root - 2017-12-16 08:31:28.897263: step 14970, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:47m:35s remains)
INFO - root - 2017-12-16 08:31:30.579709: step 14980, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 14h:57m:56s remains)
INFO - root - 2017-12-16 08:31:32.294221: step 14990, loss = 0.68, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 14h:47m:28s remains)
INFO - root - 2017-12-16 08:31:34.016752: step 15000, loss = 0.48, batch loss = 0.22 (47.4 examples/sec; 0.169 sec/batch; 14h:53m:17s remains)
2017-12-16 08:31:34.497590: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086805344 -0.1306839 -0.42922854 -0.86649752 -1.2561817 -1.3487172 -1.1292905 -0.77387154 -0.62945807 -0.85200179 -1.3356892 -1.5112565 -1.2014387 -0.77494228 -0.8371911][0.32770205 0.070944548 -0.28298211 -0.77666414 -1.2851307 -1.5477173 -1.4900272 -1.2219622 -1.0522851 -1.2218744 -1.6276311 -1.7433815 -1.3983194 -0.98280847 -1.1709915][0.098272562 -0.158391 -0.56905043 -1.0895011 -1.5914073 -1.9424001 -2.0259545 -1.8743159 -1.6975415 -1.744169 -1.9826493 -1.9636865 -1.529027 -1.1539067 -1.4858463][-0.64165735 -0.87578106 -1.286046 -1.7667723 -2.1809664 -2.5220325 -2.6963127 -2.6654294 -2.4995849 -2.3847866 -2.3434725 -2.0721397 -1.4704912 -1.0652398 -1.4356971][-1.5212309 -1.6706703 -2.0180323 -2.4197783 -2.7450318 -3.0361695 -3.2394395 -3.2916608 -3.1962061 -3.0463676 -2.8056936 -2.2873626 -1.4504514 -0.82727385 -1.083693][-2.0823936 -2.0554752 -2.2990346 -2.5881839 -2.7871094 -2.9625545 -3.0820222 -3.1851757 -3.2516935 -3.2359278 -3.0633178 -2.5143065 -1.4641908 -0.5534209 -0.58947206][-2.0732446 -1.7657287 -1.86514 -2.0177407 -2.0245538 -1.9728937 -1.9608423 -2.1361828 -2.4266994 -2.716866 -2.8619103 -2.4832215 -1.3509932 -0.22464299 -0.045544386][-1.4780464 -0.90342665 -0.8884424 -0.90668786 -0.67694879 -0.40635633 -0.27261686 -0.47973228 -0.99724865 -1.6729035 -2.2457471 -2.1627092 -1.1636097 -0.061997175 0.17110229][-0.58150494 0.044710159 0.13527203 0.27299619 0.66738248 1.0759847 1.35867 1.196552 0.54145265 -0.45511484 -1.4410071 -1.691864 -1.0083075 -0.19131613 -0.02918911][0.23117542 0.55530214 0.56943321 0.73995972 1.1403129 1.5485203 1.9538875 2.0144773 1.4717207 0.40408087 -0.77515042 -1.2601625 -0.86633337 -0.41425204 -0.41372561][0.80523419 0.58057308 0.34201193 0.34923482 0.55989981 0.8610456 1.2953084 1.5921714 1.3041735 0.40487981 -0.6289587 -1.0718029 -0.82453144 -0.62407458 -0.74595428][1.099715 0.3307333 -0.22375083 -0.40927982 -0.36412144 -0.12215686 0.30747366 0.66711283 0.50625706 -0.15769553 -0.92509556 -1.2166874 -0.95303941 -0.79151189 -0.95273113][0.74665904 -0.33386374 -1.0369977 -1.2561935 -1.1358527 -0.80760121 -0.39406085 -0.15356088 -0.31355381 -0.82604456 -1.3353122 -1.4802487 -1.2169268 -1.0268817 -1.1606263][-0.49288905 -1.4938697 -2.0610285 -2.0494905 -1.6512527 -1.1328466 -0.70574164 -0.56505597 -0.7771039 -1.2229069 -1.6199822 -1.6991057 -1.4701796 -1.2730191 -1.3597515][-1.8609662 -2.554482 -2.809936 -2.4691288 -1.7628118 -1.0940641 -0.72777319 -0.74982989 -1.0535175 -1.4901481 -1.8299966 -1.8826368 -1.7096813 -1.5459938 -1.5599761]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:31:36.675806: step 15010, loss = 0.65, batch loss = 0.39 (48.8 examples/sec; 0.164 sec/batch; 14h:27m:15s remains)
INFO - root - 2017-12-16 08:31:38.352408: step 15020, loss = 0.49, batch loss = 0.23 (47.2 examples/sec; 0.169 sec/batch; 14h:55m:54s remains)
INFO - root - 2017-12-16 08:31:40.004852: step 15030, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:37m:21s remains)
INFO - root - 2017-12-16 08:31:41.659689: step 15040, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:44m:15s remains)
INFO - root - 2017-12-16 08:31:43.313729: step 15050, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.162 sec/batch; 14h:19m:32s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:31:44.987496: step 15060, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:36m:33s remains)
INFO - root - 2017-12-16 08:31:46.661944: step 15070, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:26m:02s remains)
INFO - root - 2017-12-16 08:31:48.311999: step 15080, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:39m:17s remains)
INFO - root - 2017-12-16 08:31:49.978443: step 15090, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:38m:40s remains)
INFO - root - 2017-12-16 08:31:51.672458: step 15100, loss = 0.54, batch loss = 0.28 (45.7 examples/sec; 0.175 sec/batch; 15h:25m:52s remains)
2017-12-16 08:31:52.165171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1807412 -1.2751013 -1.2741823 -1.3553979 -1.4123747 -1.5438547 -1.7454283 -1.7218409 -1.4557512 -1.1725246 -0.80800176 -0.56302178 -0.58732438 -0.55114269 -0.35920298][-0.40273178 -0.57114232 -0.55742693 -0.559149 -0.62864316 -0.83853817 -1.1431937 -1.2294071 -1.0244151 -0.81372941 -0.65261233 -0.7282089 -1.0219607 -1.1292444 -0.961956][0.24613404 0.050268888 0.080795765 0.1529181 0.1253109 -0.12887502 -0.52919459 -0.74065757 -0.64022028 -0.49253571 -0.45642924 -0.7609359 -1.2397366 -1.45431 -1.2769618][0.5688014 0.33708525 0.42308831 0.586838 0.5866344 0.29350638 -0.18518662 -0.43049037 -0.40020394 -0.18800831 -0.11677742 -0.49361074 -1.0952332 -1.374789 -1.2153864][0.48242044 0.21388865 0.37726569 0.7417686 0.81545448 0.43549109 -0.094433069 -0.31042457 -0.17608118 0.25008631 0.54778314 0.24949408 -0.371441 -0.70828426 -0.68049979][0.055372715 -0.02507925 0.36449218 0.8579669 0.92785215 0.52853513 0.017324686 -0.15901089 0.24095154 1.0724399 1.8053396 1.8065159 1.3235576 0.85563087 0.59296823][-0.6082195 -0.45121717 0.18386149 0.784991 0.92909026 0.61476922 0.19243073 0.0985806 0.74111891 2.033973 3.3233073 3.7680323 3.3033416 2.4738104 1.743964][-1.012253 -0.63671505 0.1393044 0.87723112 1.1054947 0.88324165 0.61391115 0.58319473 1.2871447 2.7898295 4.342824 4.9386759 4.3011885 2.9547212 1.8512881][-1.1638291 -0.54864895 0.44636488 1.2479439 1.4306314 1.2003582 0.90354061 0.79985356 1.2997139 2.4539464 3.5368326 3.8610103 3.2769363 2.1764534 1.276423][-1.4800341 -0.5783397 0.58344913 1.3352511 1.3317046 0.95481563 0.52880383 0.36507797 0.68822908 1.4061754 2.0209157 2.1368864 1.7132189 1.0154037 0.60422492][-1.9808608 -1.1187851 -0.048490763 0.57896829 0.4313128 -0.11205125 -0.57854342 -0.74004579 -0.51531971 -0.097419024 0.17712164 0.21778607 0.02699399 -0.22125959 -0.15689516][-2.5528095 -1.8923254 -1.065396 -0.62341189 -0.78071976 -1.3348823 -1.8109381 -1.9997296 -1.8870289 -1.6642907 -1.5655408 -1.5553296 -1.5562432 -1.4619445 -1.1255744][-3.1034894 -2.738894 -2.1616468 -1.8361892 -1.9776101 -2.4191296 -2.8270955 -3.013185 -2.954215 -2.8133638 -2.7135191 -2.6868393 -2.6307678 -2.4760246 -2.1300714][-3.5576575 -3.4547143 -3.0883322 -2.8599913 -2.9682124 -3.307713 -3.6177564 -3.7200704 -3.64228 -3.5068185 -3.4034786 -3.3543856 -3.3181744 -3.2529523 -3.0507171][-3.8637471 -3.9327865 -3.736443 -3.5877285 -3.6368518 -3.8416204 -4.0232019 -4.0519929 -3.9627733 -3.8578892 -3.7949915 -3.7818444 -3.8252366 -3.880055 -3.8361559]]...]
INFO - root - 2017-12-16 08:31:53.844625: step 15110, loss = 0.59, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 14h:31m:24s remains)
INFO - root - 2017-12-16 08:31:55.520128: step 15120, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:35m:59s remains)
INFO - root - 2017-12-16 08:31:57.202644: step 15130, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 14h:37m:17s remains)
INFO - root - 2017-12-16 08:31:58.853879: step 15140, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 14h:58m:21s remains)
INFO - root - 2017-12-16 08:32:00.551701: step 15150, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:50m:39s remains)
INFO - root - 2017-12-16 08:32:02.257853: step 15160, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 14h:45m:37s remains)
INFO - root - 2017-12-16 08:32:03.962484: step 15170, loss = 0.57, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 15h:00m:44s remains)
INFO - root - 2017-12-16 08:32:05.659526: step 15180, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:52m:47s remains)
INFO - root - 2017-12-16 08:32:07.330706: step 15190, loss = 0.63, batch loss = 0.37 (48.8 examples/sec; 0.164 sec/batch; 14h:27m:07s remains)
INFO - root - 2017-12-16 08:32:09.019309: step 15200, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 14h:53m:11s remains)
2017-12-16 08:32:09.500489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9994063 -1.825289 -1.6163697 -1.4529459 -1.4182783 -1.5641687 -1.8607885 -2.1273634 -2.2429872 -2.2300975 -2.1599989 -2.0555086 -1.9321668 -1.7969663 -1.6259687][-2.0767274 -1.8933395 -1.6978893 -1.534714 -1.443488 -1.534269 -1.83793 -2.1264355 -2.1944616 -2.0868547 -1.9498478 -1.8139325 -1.7078469 -1.6050465 -1.462636][-2.246645 -2.0553665 -1.8847291 -1.7095444 -1.5130498 -1.4726479 -1.6687759 -1.8472821 -1.8176322 -1.6447284 -1.5089554 -1.4312985 -1.4400613 -1.4842993 -1.5026034][-2.3690479 -2.1089785 -1.8550125 -1.5813816 -1.2716668 -1.1404178 -1.2300107 -1.316655 -1.1996875 -1.0241578 -0.99747407 -1.1433296 -1.3873162 -1.6370875 -1.8274665][-2.4994671 -2.1438453 -1.7528651 -1.3449665 -0.935032 -0.75150859 -0.86000967 -0.94810176 -0.81180048 -0.61791551 -0.67109942 -1.0307964 -1.4680527 -1.9122655 -2.2742496][-2.689455 -2.2538543 -1.6913784 -1.1015877 -0.57212377 -0.38553977 -0.63807273 -0.89917505 -0.853933 -0.74832761 -0.81615746 -1.2272321 -1.7437321 -2.2873683 -2.6803329][-2.9342418 -2.4493148 -1.7447977 -0.99963021 -0.36187136 -0.21960998 -0.67226374 -1.1976719 -1.4093267 -1.4428384 -1.4674792 -1.7587875 -2.2375536 -2.705456 -2.9571948][-3.1504397 -2.6928031 -1.966167 -1.116509 -0.36968851 -0.28183842 -0.96072197 -1.7950242 -2.2747366 -2.4042931 -2.3787673 -2.482373 -2.7850411 -3.0560956 -3.0901849][-3.2943478 -2.892447 -2.1522579 -1.2330964 -0.44130552 -0.47585034 -1.4422362 -2.5229802 -3.0496202 -3.0907192 -2.9891763 -3.0273211 -3.2237017 -3.3098125 -3.1620965][-3.3298876 -2.9544706 -2.2404971 -1.3414621 -0.61399364 -0.73458064 -1.7869039 -2.8377757 -3.1905544 -3.1114743 -3.0019097 -3.0892353 -3.2842858 -3.3340642 -3.1454165][-3.2589667 -2.9293716 -2.3101068 -1.5330074 -0.84992063 -0.94920039 -1.8535391 -2.6889286 -2.841042 -2.6707537 -2.6360073 -2.8067882 -3.0427299 -3.1454768 -2.982677][-3.1923647 -2.9732516 -2.4631398 -1.8195783 -1.2026604 -1.1487806 -1.7137505 -2.2399569 -2.2615323 -2.123791 -2.2086024 -2.4707108 -2.7257512 -2.8121784 -2.6404457][-3.1980028 -3.0871642 -2.7274008 -2.1914544 -1.5697856 -1.2476865 -1.3114426 -1.4570484 -1.4378403 -1.4684572 -1.7696128 -2.1254141 -2.3730314 -2.3980474 -2.2160978][-3.2546022 -3.2388787 -2.9627945 -2.465482 -1.7813548 -1.1714694 -0.83780766 -0.67757249 -0.58079171 -0.70031321 -1.0851804 -1.5056083 -1.7731729 -1.780515 -1.6334991][-3.1756942 -3.2509897 -3.0685334 -2.6404059 -1.972403 -1.2611948 -0.73412204 -0.32817769 -0.010535955 0.034235477 -0.2690556 -0.71173668 -1.0039468 -1.0434321 -0.95045948]]...]
INFO - root - 2017-12-16 08:32:11.230977: step 15210, loss = 0.71, batch loss = 0.45 (47.6 examples/sec; 0.168 sec/batch; 14h:48m:09s remains)
INFO - root - 2017-12-16 08:32:12.897668: step 15220, loss = 0.65, batch loss = 0.39 (45.6 examples/sec; 0.175 sec/batch; 15h:27m:33s remains)
INFO - root - 2017-12-16 08:32:14.577088: step 15230, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 14h:26m:00s remains)
INFO - root - 2017-12-16 08:32:16.241185: step 15240, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:46m:43s remains)
INFO - root - 2017-12-16 08:32:17.921217: step 15250, loss = 0.49, batch loss = 0.23 (46.7 examples/sec; 0.171 sec/batch; 15h:05m:20s remains)
INFO - root - 2017-12-16 08:32:19.642872: step 15260, loss = 0.49, batch loss = 0.23 (46.8 examples/sec; 0.171 sec/batch; 15h:04m:16s remains)
INFO - root - 2017-12-16 08:32:21.335035: step 15270, loss = 0.60, batch loss = 0.34 (46.8 examples/sec; 0.171 sec/batch; 15h:04m:21s remains)
INFO - root - 2017-12-16 08:32:23.069041: step 15280, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 14h:53m:04s remains)
INFO - root - 2017-12-16 08:32:24.759666: step 15290, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:42m:53s remains)
INFO - root - 2017-12-16 08:32:26.445516: step 15300, loss = 0.61, batch loss = 0.35 (47.4 examples/sec; 0.169 sec/batch; 14h:51m:32s remains)
2017-12-16 08:32:26.941951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9794401 -2.2998972 -2.4492004 -2.5199862 -2.5497749 -2.5393281 -2.4606793 -2.3186567 -2.1267338 -1.9992285 -1.96983 -1.9751128 -2.0046825 -2.0404212 -2.0147586][-2.1811714 -2.5707712 -2.8080931 -2.9802964 -3.0731785 -3.0464091 -2.8607752 -2.5874066 -2.2819743 -2.0925665 -2.0700774 -2.0920455 -2.100625 -2.1083426 -2.0643711][-2.13196 -2.6223707 -2.9812849 -3.2824173 -3.4687452 -3.4264364 -3.1113143 -2.6715641 -2.2927485 -2.1239657 -2.2267404 -2.3227932 -2.3108857 -2.2611361 -2.1390574][-1.710063 -2.2799981 -2.7613268 -3.1958618 -3.4346218 -3.3250616 -2.8439202 -2.2918193 -1.9500798 -1.9670542 -2.2894578 -2.50147 -2.4635172 -2.3263445 -2.1392589][-0.93333137 -1.5223813 -2.0016394 -2.4521594 -2.6828964 -2.4682331 -1.8492724 -1.3020954 -1.2547597 -1.6705453 -2.2770429 -2.6357398 -2.5745182 -2.3226249 -2.0643091][-0.20612526 -0.66426039 -0.98418236 -1.3264161 -1.4273942 -1.0465219 -0.33145595 0.10576868 -0.29127455 -1.2571427 -2.1907213 -2.7043846 -2.7017241 -2.4155397 -2.1294384][0.079526424 -0.16420197 -0.24862194 -0.39996207 -0.23842669 0.46345973 1.2318394 1.4621227 0.63494539 -0.74484611 -1.8945196 -2.4824345 -2.5519319 -2.3309128 -2.0695877][-0.07619524 -0.16991353 -0.077777147 -0.0066416264 0.48565745 1.5103507 2.3566508 2.3351498 1.2533782 -0.27505207 -1.4465317 -2.0616069 -2.1856704 -2.0142756 -1.7899172][-0.52475154 -0.51813734 -0.38211119 -0.10870552 0.64671707 1.8571138 2.7318611 2.4610558 1.2385592 -0.20802712 -1.24229 -1.7517343 -1.8058736 -1.6689289 -1.451822][-1.0723349 -1.0629187 -0.97905433 -0.59070122 0.33864641 1.6005423 2.3644614 1.9001865 0.65017414 -0.667771 -1.4942588 -1.817742 -1.7220784 -1.5544608 -1.3308038][-1.1850768 -1.2603627 -1.3544531 -1.1015614 -0.35746765 0.6242125 1.1562338 0.61365247 -0.50605404 -1.5676515 -2.1644309 -2.3095493 -2.0649862 -1.7972124 -1.5389519][-0.86045647 -1.1448851 -1.5813036 -1.7038922 -1.3763766 -0.89010894 -0.68127036 -1.17135 -1.9658819 -2.6094093 -2.9309177 -2.9106255 -2.5814719 -2.2501559 -1.9783592][-0.12712693 -0.687642 -1.59395 -2.1899693 -2.3874335 -2.403564 -2.4669993 -2.8105664 -3.218416 -3.4780614 -3.5720689 -3.4604104 -3.107728 -2.7814438 -2.5098035][0.86764884 0.032508135 -1.2606484 -2.2742186 -2.907536 -3.2494075 -3.4759634 -3.6929474 -3.8199759 -3.847759 -3.8081656 -3.6735928 -3.3769994 -3.0922196 -2.8729477][1.7029581 0.70027995 -0.78918231 -2.0043888 -2.8320727 -3.2896669 -3.5700402 -3.6942472 -3.6562374 -3.5491922 -3.464098 -3.410666 -3.2388632 -3.0567517 -2.9340444]]...]
INFO - root - 2017-12-16 08:32:28.612961: step 15310, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:53m:01s remains)
INFO - root - 2017-12-16 08:32:30.299123: step 15320, loss = 0.54, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 14h:48m:02s remains)
INFO - root - 2017-12-16 08:32:31.965099: step 15330, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.165 sec/batch; 14h:34m:46s remains)
INFO - root - 2017-12-16 08:32:33.633403: step 15340, loss = 0.63, batch loss = 0.37 (48.8 examples/sec; 0.164 sec/batch; 14h:25m:40s remains)
INFO - root - 2017-12-16 08:32:35.293463: step 15350, loss = 0.65, batch loss = 0.39 (46.7 examples/sec; 0.171 sec/batch; 15h:04m:41s remains)
INFO - root - 2017-12-16 08:32:36.992843: step 15360, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:43m:31s remains)
INFO - root - 2017-12-16 08:32:38.730735: step 15370, loss = 0.67, batch loss = 0.41 (45.3 examples/sec; 0.177 sec/batch; 15h:32m:59s remains)
INFO - root - 2017-12-16 08:32:40.443870: step 15380, loss = 0.61, batch loss = 0.35 (44.3 examples/sec; 0.181 sec/batch; 15h:54m:26s remains)
INFO - root - 2017-12-16 08:32:42.120512: step 15390, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:33m:04s remains)
INFO - root - 2017-12-16 08:32:43.797864: step 15400, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 14h:57m:02s remains)
2017-12-16 08:32:44.306766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0503107 -1.2037116 -1.3373491 -1.4904573 -1.4835224 -1.0613471 -0.37124717 0.14108014 0.31913614 0.34707117 0.23658729 -0.16619468 -0.85292017 -1.6540046 -2.4802303][-0.36514461 -0.48336458 -0.51280749 -0.51650894 -0.41576219 -0.022155285 0.53334808 0.81857896 0.69396639 0.41425371 0.086761951 -0.35822928 -0.96588051 -1.6873021 -2.4873879][-0.03157568 -0.15225863 -0.12482095 -0.0098962784 0.21287012 0.65931869 1.2013028 1.3647876 1.0031343 0.44525886 -0.090907335 -0.587139 -1.1278322 -1.7630527 -2.5100474][-0.15877366 -0.32216859 -0.30396986 -0.12355375 0.22877574 0.8207438 1.4902799 1.7134886 1.2853534 0.55469513 -0.16427302 -0.74351084 -1.2696061 -1.8507931 -2.5471857][-0.50946939 -0.66452181 -0.6911006 -0.52480328 -0.10207772 0.66842413 1.5715497 2.0042028 1.6617119 0.86603022 -0.005130291 -0.72311413 -1.3271137 -1.9185233 -2.5909009][-0.82194138 -0.92466915 -1.0041085 -0.88049185 -0.43295443 0.46167159 1.5630333 2.2423773 2.0902534 1.3412104 0.37056494 -0.52070403 -1.2665677 -1.932165 -2.620887][-1.0126896 -1.1257625 -1.2977661 -1.2451047 -0.78639865 0.15754437 1.3728173 2.2882185 2.435132 1.8328657 0.81522322 -0.23585129 -1.1449833 -1.9113054 -2.6367965][-1.0905563 -1.2785077 -1.5659621 -1.598464 -1.165073 -0.21679997 1.0456388 2.1361217 2.5858679 2.2055292 1.2037814 0.048498392 -0.988693 -1.8556132 -2.6308823][-1.1451297 -1.3630943 -1.7169516 -1.8341997 -1.4575315 -0.55500889 0.67593884 1.8336678 2.524693 2.408565 1.4845433 0.29391241 -0.81626654 -1.7636423 -2.5997896][-1.3470397 -1.5003154 -1.8116488 -1.9544609 -1.6436476 -0.83475614 0.30721784 1.4799204 2.3314137 2.4428992 1.6535275 0.49578571 -0.639488 -1.6521435 -2.54938][-1.6660951 -1.7005091 -1.8731771 -1.9832577 -1.7248716 -1.0173485 0.020344973 1.168206 2.1070862 2.413928 1.7957292 0.68256187 -0.47230852 -1.5424377 -2.4979968][-1.9485545 -1.8830738 -1.9175525 -1.9492288 -1.688369 -1.0436503 -0.11888885 0.96798229 1.9768968 2.4788179 2.0239935 0.91251111 -0.29065824 -1.4304111 -2.4495509][-2.0957711 -1.9413497 -1.8597291 -1.8032422 -1.5092113 -0.90325475 -0.10309076 0.894722 1.9539728 2.6235328 2.3249431 1.1711595 -0.11665821 -1.3271599 -2.4072506][-2.10119 -1.8226917 -1.6248492 -1.5210545 -1.2520266 -0.71598649 -0.047781467 0.79773641 1.8296509 2.5918431 2.4284291 1.2903943 -0.037655115 -1.2742182 -2.3801241][-2.0066903 -1.5773969 -1.2752265 -1.1797954 -1.0035946 -0.58607221 -0.096252918 0.52364087 1.4175532 2.1657767 2.114018 1.1309879 -0.1043117 -1.2823391 -2.3624249]]...]
INFO - root - 2017-12-16 08:32:45.978538: step 15410, loss = 0.46, batch loss = 0.20 (47.4 examples/sec; 0.169 sec/batch; 14h:51m:15s remains)
INFO - root - 2017-12-16 08:32:47.675432: step 15420, loss = 0.46, batch loss = 0.20 (47.9 examples/sec; 0.167 sec/batch; 14h:41m:51s remains)
INFO - root - 2017-12-16 08:32:49.376860: step 15430, loss = 0.69, batch loss = 0.43 (47.3 examples/sec; 0.169 sec/batch; 14h:53m:37s remains)
INFO - root - 2017-12-16 08:32:51.075202: step 15440, loss = 0.52, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:34m:54s remains)
INFO - root - 2017-12-16 08:32:52.737988: step 15450, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:42m:34s remains)
INFO - root - 2017-12-16 08:32:54.428638: step 15460, loss = 0.53, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 14h:52m:20s remains)
INFO - root - 2017-12-16 08:32:56.075650: step 15470, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:33m:36s remains)
INFO - root - 2017-12-16 08:32:57.740803: step 15480, loss = 0.49, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 14h:48m:10s remains)
INFO - root - 2017-12-16 08:32:59.408741: step 15490, loss = 0.66, batch loss = 0.40 (48.2 examples/sec; 0.166 sec/batch; 14h:36m:40s remains)
INFO - root - 2017-12-16 08:33:01.083030: step 15500, loss = 0.64, batch loss = 0.38 (44.9 examples/sec; 0.178 sec/batch; 15h:41m:15s remains)
2017-12-16 08:33:01.563465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2708027 -3.2961373 -3.1918762 -3.0626085 -3.0783651 -3.176167 -3.1805687 -3.1303456 -3.0993137 -3.1592393 -3.2681823 -3.2036469 -2.8546352 -2.2835472 -1.6868346][-3.8838425 -4.0323625 -4.0989833 -4.1246448 -4.1720939 -4.1546068 -3.9342089 -3.6268587 -3.472815 -3.5942221 -3.8584566 -3.9945626 -3.8473406 -3.4425139 -2.911731][-3.9175358 -4.1866846 -4.4283895 -4.5951743 -4.6054749 -4.3293848 -3.741046 -3.1300135 -2.9023018 -3.1972513 -3.7785199 -4.2448416 -4.3483181 -4.1267385 -3.7293153][-3.7114105 -4.0292783 -4.3494844 -4.5333548 -4.3836317 -3.7339025 -2.7380733 -1.8908018 -1.7025001 -2.32352 -3.3392873 -4.1959643 -4.5464993 -4.47456 -4.1258759][-3.4175911 -3.729804 -4.0200644 -4.0949655 -3.6351323 -2.5080996 -1.1042479 -0.12870836 -0.12280011 -1.2198672 -2.7367127 -3.9406772 -4.4670563 -4.4351053 -4.0531445][-3.0628166 -3.3403506 -3.5546575 -3.4175415 -2.5724449 -0.96906555 0.78322577 1.7889671 1.394773 -0.23809862 -2.1592765 -3.5137041 -4.0111294 -3.8905563 -3.4464617][-2.7442303 -2.943845 -3.0582604 -2.6982436 -1.4901451 0.53398848 2.6310349 3.5806661 2.6683073 0.46574426 -1.689841 -2.9708908 -3.2421663 -2.8909183 -2.3610542][-2.3924146 -2.5172052 -2.5699263 -2.0740426 -0.65435171 1.6032341 3.8398228 4.5825167 3.1148267 0.60465169 -1.5319173 -2.562803 -2.5064518 -1.8199134 -1.142849][-1.9629769 -2.0538335 -2.0954835 -1.6192287 -0.24289608 1.8094568 3.633678 3.9372663 2.3424687 0.0087828636 -1.8123231 -2.515486 -2.1063011 -1.0520128 -0.14573145][-1.4183247 -1.5869131 -1.6789854 -1.3461547 -0.22240305 1.367152 2.6163406 2.4859948 0.86558318 -1.1247747 -2.5071046 -2.8077564 -2.0174651 -0.66196716 0.40572977][-0.75605071 -1.0614544 -1.3166994 -1.2257847 -0.47176039 0.64676833 1.342849 0.81805944 -0.79021955 -2.4258623 -3.3863935 -3.3196993 -2.2636528 -0.79636467 0.23377705][-0.19166207 -0.63013268 -1.0752323 -1.2500297 -0.89314175 -0.27034211 -0.063195705 -0.81127727 -2.2570155 -3.541111 -4.1305518 -3.8859496 -2.8276997 -1.4363914 -0.474689][0.26963782 -0.26766729 -0.85029542 -1.3041341 -1.3841088 -1.24469 -1.390065 -2.2183356 -3.3857317 -4.2905335 -4.572504 -4.274889 -3.3681169 -2.1673899 -1.3560846][0.36336708 -0.18428588 -0.76821876 -1.4021224 -1.8681195 -2.0762877 -2.4294684 -3.1888971 -4.0155482 -4.5453262 -4.5913811 -4.2812357 -3.59654 -2.718328 -2.1376419][0.031019688 -0.48587263 -1.0204769 -1.6948748 -2.2933443 -2.6474795 -3.0229316 -3.5806813 -4.0871048 -4.3104157 -4.208724 -3.9074888 -3.4294155 -2.8836575 -2.5557117]]...]
INFO - root - 2017-12-16 08:33:03.244190: step 15510, loss = 0.69, batch loss = 0.44 (46.3 examples/sec; 0.173 sec/batch; 15h:13m:43s remains)
INFO - root - 2017-12-16 08:33:04.902673: step 15520, loss = 0.64, batch loss = 0.38 (48.8 examples/sec; 0.164 sec/batch; 14h:26m:24s remains)
INFO - root - 2017-12-16 08:33:06.595272: step 15530, loss = 0.55, batch loss = 0.29 (46.0 examples/sec; 0.174 sec/batch; 15h:19m:44s remains)
INFO - root - 2017-12-16 08:33:08.294751: step 15540, loss = 0.57, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 14h:59m:42s remains)
INFO - root - 2017-12-16 08:33:10.028037: step 15550, loss = 0.69, batch loss = 0.43 (45.6 examples/sec; 0.175 sec/batch; 15h:26m:36s remains)
INFO - root - 2017-12-16 08:33:11.697638: step 15560, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:41m:57s remains)
INFO - root - 2017-12-16 08:33:13.351059: step 15570, loss = 0.65, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 14h:41m:35s remains)
INFO - root - 2017-12-16 08:33:15.009301: step 15580, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 14h:18m:24s remains)
INFO - root - 2017-12-16 08:33:16.669568: step 15590, loss = 0.65, batch loss = 0.39 (48.0 examples/sec; 0.166 sec/batch; 14h:39m:23s remains)
INFO - root - 2017-12-16 08:33:18.355031: step 15600, loss = 0.55, batch loss = 0.29 (45.7 examples/sec; 0.175 sec/batch; 15h:25m:07s remains)
2017-12-16 08:33:18.840719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8634959 -1.8933182 -1.8934789 -1.8711456 -1.8629194 -1.867596 -1.8824563 -1.9188237 -1.9450089 -2.0013361 -2.1009762 -2.1580718 -2.1422307 -2.0853026 -1.9454622][-2.1721206 -2.3113723 -2.3963108 -2.4412839 -2.5071888 -2.5821364 -2.666508 -2.7453263 -2.7714577 -2.8358855 -2.9853451 -3.0298684 -2.9319878 -2.7462561 -2.4075794][-2.5233567 -2.7219167 -2.8316782 -2.8808048 -2.9630716 -3.0701039 -3.1997566 -3.3176007 -3.3527558 -3.4647145 -3.7297606 -3.8558636 -3.7548914 -3.4920716 -2.9732985][-2.7599533 -2.8864317 -2.8943918 -2.846113 -2.8500166 -2.9158621 -3.0504227 -3.190248 -3.2476215 -3.45034 -3.8862221 -4.1605334 -4.1614308 -3.9484959 -3.3920951][-2.6343865 -2.5316451 -2.3011158 -2.0424876 -1.869061 -1.82376 -1.9301784 -2.064508 -2.1452246 -2.4871769 -3.16804 -3.6957846 -3.9440432 -3.962441 -3.5594816][-2.0929685 -1.6642041 -1.1392604 -0.65156841 -0.28690839 -0.13467431 -0.2321403 -0.35641837 -0.43494391 -0.95209122 -1.9216826 -2.7533267 -3.3036373 -3.6348262 -3.5168839][-1.413136 -0.64140832 0.17532921 0.90534616 1.476815 1.7555809 1.6848025 1.6019433 1.4567101 0.65780663 -0.6598897 -1.7927432 -2.6000879 -3.227473 -3.3666372][-1.0667427 -0.059050083 0.96616769 1.9112468 2.7115464 3.1844559 3.2461448 3.2698913 3.0158205 1.8863168 0.24773622 -1.0727099 -2.0752826 -2.9134181 -3.2036762][-1.356693 -0.3727119 0.60838532 1.5130086 2.3203893 2.8581243 3.034615 3.2274613 3.0107231 1.8046746 0.12565327 -1.1746085 -2.1891241 -3.0290065 -3.268295][-1.9592789 -1.1717963 -0.38476658 0.34379578 1.0211396 1.4658012 1.5882518 1.7240047 1.517813 0.49442339 -0.90199065 -1.9726511 -2.8255498 -3.4685326 -3.4841042][-2.5831614 -2.0608697 -1.4748974 -0.89993739 -0.36729991 -0.046135664 0.0041811466 0.0834136 -0.061977863 -0.80907893 -1.8411834 -2.6595342 -3.3530164 -3.7690933 -3.5590563][-2.9252734 -2.6690016 -2.2894194 -1.8880603 -1.5396115 -1.373147 -1.4048371 -1.3993244 -1.4759383 -1.9141426 -2.549011 -3.1013389 -3.5951614 -3.7849779 -3.4390202][-2.9144218 -2.8357308 -2.6334496 -2.3977878 -2.2166951 -2.1789217 -2.2846978 -2.3432579 -2.4015925 -2.6370521 -2.982985 -3.3087859 -3.5838718 -3.5906405 -3.2077935][-2.631645 -2.6357715 -2.5374835 -2.4086773 -2.3271542 -2.3438976 -2.4526739 -2.5309381 -2.5835054 -2.7046952 -2.8943529 -3.0825818 -3.2122245 -3.1481457 -2.8420825][-2.2657216 -2.2803266 -2.2227294 -2.1418471 -2.07995 -2.0770388 -2.1211655 -2.1674895 -2.210716 -2.291239 -2.4088845 -2.5211666 -2.5836136 -2.5334332 -2.3615155]]...]
INFO - root - 2017-12-16 08:33:20.522649: step 15610, loss = 0.48, batch loss = 0.22 (49.7 examples/sec; 0.161 sec/batch; 14h:10m:30s remains)
INFO - root - 2017-12-16 08:33:22.179560: step 15620, loss = 0.56, batch loss = 0.30 (49.2 examples/sec; 0.163 sec/batch; 14h:19m:06s remains)
INFO - root - 2017-12-16 08:33:23.838363: step 15630, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:42m:31s remains)
INFO - root - 2017-12-16 08:33:25.532495: step 15640, loss = 0.55, batch loss = 0.29 (46.9 examples/sec; 0.171 sec/batch; 15h:00m:30s remains)
INFO - root - 2017-12-16 08:33:27.214265: step 15650, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 15h:10m:25s remains)
INFO - root - 2017-12-16 08:33:28.908533: step 15660, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 14h:33m:38s remains)
INFO - root - 2017-12-16 08:33:30.643038: step 15670, loss = 0.63, batch loss = 0.37 (45.1 examples/sec; 0.177 sec/batch; 15h:36m:38s remains)
INFO - root - 2017-12-16 08:33:32.336940: step 15680, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:26m:23s remains)
INFO - root - 2017-12-16 08:33:33.978792: step 15690, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 14h:51m:55s remains)
INFO - root - 2017-12-16 08:33:35.667343: step 15700, loss = 0.53, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 15h:03m:51s remains)
2017-12-16 08:33:36.133253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4563768 -1.5272714 -1.607679 -1.6781224 -1.739219 -1.6420161 -1.1357417 -0.48300374 -0.37185097 -0.70817053 -1.1957005 -2.0446291 -3.0279722 -3.4432569 -3.1801634][-2.4186676 -2.529655 -2.6558931 -2.7455158 -2.7949753 -2.7149634 -2.275116 -1.7039647 -1.5622056 -1.7812543 -2.115315 -2.7678418 -3.5316367 -3.7587857 -3.4268875][-3.2406206 -3.3764548 -3.5229139 -3.5984125 -3.5897388 -3.4661417 -3.011513 -2.4795163 -2.3468435 -2.5833335 -2.925828 -3.4816709 -4.0261674 -4.0830736 -3.7151046][-3.8717623 -4.0045509 -4.1178608 -4.1074381 -3.9269562 -3.5856118 -2.9591405 -2.3593085 -2.2590842 -2.6119454 -3.1053567 -3.7114685 -4.2294517 -4.27446 -3.9144666][-4.0074244 -4.0629358 -4.010169 -3.7540035 -3.3016577 -2.6494262 -1.7071698 -0.90974522 -0.8268621 -1.3791203 -2.125102 -3.0078201 -3.751255 -3.98626 -3.759871][-3.3493276 -3.2375095 -2.9110065 -2.3461533 -1.6408451 -0.74337339 0.50500226 1.514605 1.5133405 0.72760534 -0.29982567 -1.5338609 -2.6996412 -3.3069363 -3.2927537][-2.2339768 -1.9958069 -1.4773493 -0.71057177 0.14303446 1.1330514 2.5621414 3.7169685 3.5784254 2.5541992 1.3644469 -0.13020468 -1.6500012 -2.568691 -2.7504516][-1.2175317 -1.054793 -0.59254205 0.1316483 0.8870697 1.7500715 3.1306314 4.27069 3.9996529 2.8806715 1.7358394 0.29963517 -1.2765006 -2.2457905 -2.461971][-0.933645 -0.94417179 -0.75893855 -0.38818491 0.024378538 0.58515096 1.707346 2.6524978 2.3919163 1.4891953 0.64359808 -0.45441604 -1.6769712 -2.4146068 -2.5163283][-1.4733894 -1.6430106 -1.8323035 -1.9143105 -1.864274 -1.5723665 -0.76956868 -0.019564629 -0.084903717 -0.59255719 -1.055159 -1.7083218 -2.4390059 -2.7998495 -2.7579124][-2.0757153 -2.3258145 -2.7937675 -3.2555037 -3.5048802 -3.413095 -2.8460083 -2.2321415 -2.0706582 -2.1987891 -2.347878 -2.6396019 -2.9421146 -3.0174608 -2.8940187][-2.1608436 -2.41917 -3.0011227 -3.6511436 -4.0609241 -4.12969 -3.7729635 -3.2574821 -2.9280486 -2.7901998 -2.7712147 -2.8399096 -2.9059794 -2.8725488 -2.7565079][-1.8262868 -2.0347366 -2.5463593 -3.1768913 -3.6083083 -3.7239962 -3.4900026 -3.0480685 -2.6339962 -2.4134643 -2.3694043 -2.3984303 -2.4454679 -2.4487627 -2.4200451][-1.4901515 -1.5482079 -1.8052796 -2.2166531 -2.544286 -2.6223516 -2.4510708 -2.0625668 -1.6795001 -1.4747573 -1.4668033 -1.6013942 -1.7724897 -1.9129879 -2.0164821][-1.4152689 -1.2681341 -1.2226681 -1.3358819 -1.4787166 -1.5038285 -1.367559 -1.0907402 -0.83071077 -0.72201657 -0.80176079 -1.0607637 -1.3507464 -1.5701929 -1.751184]]...]
INFO - root - 2017-12-16 08:33:37.802133: step 15710, loss = 0.72, batch loss = 0.46 (47.5 examples/sec; 0.168 sec/batch; 14h:48m:29s remains)
INFO - root - 2017-12-16 08:33:39.475797: step 15720, loss = 0.74, batch loss = 0.48 (48.0 examples/sec; 0.167 sec/batch; 14h:40m:01s remains)
INFO - root - 2017-12-16 08:33:41.165836: step 15730, loss = 0.65, batch loss = 0.39 (46.9 examples/sec; 0.171 sec/batch; 15h:00m:29s remains)
INFO - root - 2017-12-16 08:33:42.854178: step 15740, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 14h:58m:24s remains)
INFO - root - 2017-12-16 08:33:44.504744: step 15750, loss = 0.49, batch loss = 0.23 (48.3 examples/sec; 0.165 sec/batch; 14h:33m:32s remains)
INFO - root - 2017-12-16 08:33:46.184629: step 15760, loss = 0.62, batch loss = 0.36 (47.8 examples/sec; 0.168 sec/batch; 14h:44m:23s remains)
INFO - root - 2017-12-16 08:33:47.858796: step 15770, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.165 sec/batch; 14h:28m:47s remains)
INFO - root - 2017-12-16 08:33:49.532342: step 15780, loss = 0.63, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 14h:30m:44s remains)
INFO - root - 2017-12-16 08:33:51.212042: step 15790, loss = 0.54, batch loss = 0.28 (46.5 examples/sec; 0.172 sec/batch; 15h:08m:07s remains)
INFO - root - 2017-12-16 08:33:52.947874: step 15800, loss = 0.61, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:33m:18s remains)
2017-12-16 08:33:53.453196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8058113 -1.7946286 -1.5097132 -1.1643283 -1.3100119 -2.1631045 -2.9847021 -3.2954655 -3.3223643 -3.2007396 -2.5507231 -1.5118665 -0.50677907 0.33371425 0.49352455][-2.2055602 -2.4888551 -2.5543005 -2.4889617 -2.6107991 -3.2202773 -3.7718153 -3.9471006 -3.9389029 -3.8538609 -3.3952613 -2.6408367 -1.803772 -0.98984456 -0.6995759][-2.0047693 -2.4308233 -2.7478113 -2.9408264 -3.0435405 -3.361834 -3.5741973 -3.6766655 -3.780849 -3.9842162 -3.8301506 -3.2977598 -2.6398859 -1.8500483 -1.4097774][-1.5540593 -1.9513842 -2.2936213 -2.6272502 -2.7736237 -2.7978525 -2.7771447 -2.894707 -3.148788 -3.6255906 -3.7936077 -3.462482 -2.9716325 -2.2538998 -1.6596069][-1.3573825 -1.5490085 -1.7353778 -2.0235386 -2.1274388 -1.9022663 -1.7179346 -1.8637514 -2.2867947 -3.029232 -3.5242572 -3.4297206 -3.0536931 -2.3781261 -1.7803615][-1.4939483 -1.3922632 -1.3626046 -1.3756824 -1.152896 -0.52842033 -0.12204313 -0.4731406 -1.1739848 -2.1847746 -3.0086484 -3.198997 -2.9161143 -2.2847104 -1.7257717][-1.5810399 -1.2108479 -0.82659948 -0.43269217 0.37113929 1.6203749 2.2349281 1.4635727 0.1617353 -1.2704562 -2.4690008 -2.9182916 -2.6972282 -2.0456126 -1.5539041][-1.5136147 -0.94771743 -0.31060171 0.4802177 1.8823428 3.7753234 4.4530492 3.1251101 1.3321474 -0.4426353 -1.9336126 -2.6055498 -2.4273257 -1.7453448 -1.3047023][-1.7057836 -1.1857047 -0.49535429 0.50450873 2.1930447 4.2451763 4.7878776 3.5258269 1.8132601 0.022029638 -1.5611135 -2.3208351 -2.2259958 -1.6013556 -1.2617339][-2.2207196 -1.7294667 -1.0746557 -0.17450833 1.2019241 2.7975073 3.2958941 2.6127448 1.3299813 -0.2529254 -1.7356403 -2.4389236 -2.3294823 -1.7206826 -1.4558773][-2.6533859 -2.2701058 -1.781472 -1.1689278 -0.344602 0.59297013 0.95805573 0.73968172 0.030787706 -1.0600953 -2.144249 -2.5470741 -2.2145925 -1.5637665 -1.3727413][-3.2989645 -3.0049086 -2.600034 -2.1875229 -1.8216333 -1.4814267 -1.3704555 -1.3926409 -1.5890532 -1.9928619 -2.3980656 -2.3544483 -1.7906559 -1.188535 -1.1839594][-4.2027378 -3.881454 -3.4765143 -3.18142 -3.1355944 -3.2096441 -3.312521 -3.3734159 -3.2920754 -3.137567 -2.8145247 -2.2807186 -1.5374652 -1.0824817 -1.2819015][-4.8902698 -4.53691 -4.1458082 -3.977663 -4.1591859 -4.4761214 -4.7314434 -4.9393253 -4.8139768 -4.3446212 -3.5787826 -2.7335072 -1.9687908 -1.6067207 -1.8934549][-5.040926 -4.7435937 -4.4218054 -4.3348389 -4.5639362 -4.9830441 -5.3746424 -5.725153 -5.7793264 -5.3846374 -4.5985174 -3.6800454 -2.9652972 -2.6755607 -2.8729756]]...]
INFO - root - 2017-12-16 08:33:55.146753: step 15810, loss = 0.53, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:55m:41s remains)
INFO - root - 2017-12-16 08:33:56.847618: step 15820, loss = 0.49, batch loss = 0.23 (45.9 examples/sec; 0.174 sec/batch; 15h:19m:33s remains)
INFO - root - 2017-12-16 08:33:58.513611: step 15830, loss = 0.61, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 14h:20m:39s remains)
INFO - root - 2017-12-16 08:34:00.230258: step 15840, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 14h:57m:22s remains)
INFO - root - 2017-12-16 08:34:01.927239: step 15850, loss = 0.56, batch loss = 0.30 (42.4 examples/sec; 0.189 sec/batch; 16h:35m:17s remains)
INFO - root - 2017-12-16 08:34:03.620733: step 15860, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:48m:07s remains)
INFO - root - 2017-12-16 08:34:05.292510: step 15870, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.169 sec/batch; 14h:49m:14s remains)
INFO - root - 2017-12-16 08:34:07.008585: step 15880, loss = 0.65, batch loss = 0.39 (47.5 examples/sec; 0.168 sec/batch; 14h:47m:57s remains)
INFO - root - 2017-12-16 08:34:08.680677: step 15890, loss = 0.65, batch loss = 0.39 (48.4 examples/sec; 0.165 sec/batch; 14h:31m:27s remains)
INFO - root - 2017-12-16 08:34:10.373950: step 15900, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 14h:49m:38s remains)
2017-12-16 08:34:10.811363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9143658 -3.8922267 -3.860976 -3.9166241 -3.9027371 -3.7203417 -3.5060747 -3.2982848 -3.1412287 -3.0334263 -2.8557208 -2.5937512 -2.2922692 -1.958781 -1.5917434][-4.5029488 -4.37342 -4.2673268 -4.2575541 -4.16037 -3.92627 -3.7558572 -3.6378183 -3.5433421 -3.4954123 -3.3920648 -3.2104909 -2.982497 -2.6487088 -2.2315505][-4.0816221 -3.8370824 -3.644526 -3.5668974 -3.3480253 -3.0767305 -2.9920125 -3.0572772 -3.1353788 -3.2115438 -3.2646246 -3.3146915 -3.2737901 -3.0445275 -2.6693392][-2.8968782 -2.4845951 -2.2644091 -2.141016 -1.86816 -1.5965637 -1.5871153 -1.8101888 -2.0676932 -2.2552347 -2.4929335 -2.794172 -3.0062842 -3.0015557 -2.8066709][-1.5361907 -0.85870111 -0.51503706 -0.31420541 0.035975933 0.34424663 0.37877631 0.10102129 -0.25321388 -0.62980843 -1.1580282 -1.8639122 -2.4430146 -2.7417877 -2.7649503][-0.29262233 0.79416823 1.4682863 1.8764763 2.3421912 2.7728519 2.9092431 2.7787948 2.2467546 1.4055145 0.32847905 -0.96405315 -2.0154977 -2.5785949 -2.7330229][0.4275775 1.9455972 3.0711346 3.8116817 4.5055833 5.1356311 5.4872932 5.4980559 4.6780558 3.2103114 1.4712369 -0.32211089 -1.7169847 -2.4653716 -2.6253662][0.22674489 1.9624968 3.4129562 4.5423565 5.5322433 6.2718534 6.7640476 6.7825842 5.7500811 3.9332981 1.8548551 -0.083995342 -1.5152435 -2.2289772 -2.3320396][-0.86425567 0.713233 2.1735859 3.5306849 4.68958 5.3988376 5.7766409 5.6215072 4.568336 3.0005932 1.1404212 -0.527038 -1.6837399 -2.1572425 -2.1637363][-2.3831105 -1.0959587 0.22798705 1.5266387 2.5416946 2.9675932 2.9126134 2.4895482 1.7097259 0.73405838 -0.49260795 -1.644871 -2.3822577 -2.5778534 -2.4906032][-3.9112511 -3.012886 -1.9092758 -0.86963665 -0.25519252 -0.19264627 -0.615322 -1.1268271 -1.493607 -1.8462012 -2.3681552 -2.9293051 -3.2064095 -3.21618 -3.1547518][-4.9649534 -4.4888716 -3.7170506 -3.0582654 -2.8579423 -3.082588 -3.586693 -3.9115922 -3.8725362 -3.6971669 -3.6802111 -3.7744327 -3.772562 -3.693831 -3.6596565][-5.0249271 -4.857132 -4.4689951 -4.1917229 -4.2856245 -4.6280918 -5.0111265 -5.1003761 -4.8235598 -4.3856697 -4.081727 -3.9487033 -3.7986696 -3.6600301 -3.6346889][-4.1520562 -4.1378441 -4.0047879 -3.9882517 -4.185719 -4.480257 -4.7414675 -4.7277126 -4.4150605 -3.9771333 -3.6549914 -3.4378979 -3.2380648 -3.0545144 -2.9949956][-2.9321654 -2.9898038 -3.0248575 -3.1224051 -3.3092728 -3.5261364 -3.6685348 -3.6398406 -3.4090681 -3.0864267 -2.8075144 -2.5683806 -2.3695717 -2.2072821 -2.1275659]]...]
INFO - root - 2017-12-16 08:34:12.482803: step 15910, loss = 0.72, batch loss = 0.46 (48.3 examples/sec; 0.165 sec/batch; 14h:33m:10s remains)
INFO - root - 2017-12-16 08:34:14.204277: step 15920, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 14h:57m:43s remains)
INFO - root - 2017-12-16 08:34:15.874605: step 15930, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 14h:44m:14s remains)
INFO - root - 2017-12-16 08:34:17.539525: step 15940, loss = 0.66, batch loss = 0.40 (48.5 examples/sec; 0.165 sec/batch; 14h:29m:37s remains)
INFO - root - 2017-12-16 08:34:19.232542: step 15950, loss = 0.50, batch loss = 0.24 (46.3 examples/sec; 0.173 sec/batch; 15h:10m:42s remains)
INFO - root - 2017-12-16 08:34:20.916355: step 15960, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:40m:33s remains)
INFO - root - 2017-12-16 08:34:22.586202: step 15970, loss = 0.56, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 15h:01m:26s remains)
INFO - root - 2017-12-16 08:34:24.268788: step 15980, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.169 sec/batch; 14h:49m:04s remains)
INFO - root - 2017-12-16 08:34:25.943476: step 15990, loss = 0.60, batch loss = 0.34 (47.0 examples/sec; 0.170 sec/batch; 14h:57m:21s remains)
INFO - root - 2017-12-16 08:34:27.637775: step 16000, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 14h:43m:27s remains)
2017-12-16 08:34:28.125366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3901384 -2.3611727 -2.3088603 -2.2564898 -2.2362955 -2.2754021 -2.3136735 -2.2010705 -1.9113002 -1.6458987 -1.6355994 -1.8961074 -2.2537892 -2.5359466 -2.6402376][-2.6653552 -2.7342746 -2.8026755 -2.8427525 -2.9241662 -3.1149192 -3.2989898 -3.2441926 -2.8597448 -2.4319718 -2.2871726 -2.4249823 -2.6488776 -2.776916 -2.7611086][-2.7885768 -2.8616464 -2.9544637 -3.0303371 -3.1904745 -3.5344377 -3.9223645 -4.0359263 -3.6666207 -3.1380939 -2.8849778 -2.9346316 -3.0504851 -3.0545568 -2.92392][-2.6288631 -2.5646217 -2.4600091 -2.3052371 -2.3560557 -2.7615473 -3.3061514 -3.6424012 -3.4675751 -3.0468931 -2.7986789 -2.8866472 -3.0815518 -3.1618662 -3.050354][-2.3628788 -2.0201797 -1.4292957 -0.71219671 -0.41565955 -0.689715 -1.3068745 -1.8780687 -2.0147612 -1.8684804 -1.8408363 -2.1400557 -2.6222889 -3.0132682 -3.0873165][-2.0034533 -1.316645 -0.16464996 1.2164235 2.0919592 2.2223613 1.7569945 1.1701322 0.811805 0.58252454 0.10265112 -0.80564106 -1.8343562 -2.6891942 -3.0783806][-1.6667567 -0.72111285 0.80768394 2.5522115 3.9484847 4.7498655 4.8782549 4.6594439 4.3529558 3.8240664 2.7621882 1.1483636 -0.62228739 -2.1130717 -2.8573568][-1.6155965 -0.78373325 0.57230282 2.1624587 3.6590674 4.9163017 5.7117376 6.22174 6.431077 5.8471489 4.3820448 2.2277486 -0.0082776546 -1.7950697 -2.7261786][-1.8935356 -1.3756776 -0.4802593 0.55516386 1.6094959 2.6013815 3.5012615 4.3758507 5.1232748 4.9442205 3.5002573 1.3893118 -0.64861488 -2.1630623 -2.9320202][-2.3039236 -2.0532556 -1.6623871 -1.2441468 -0.80850089 -0.40440166 0.16011882 1.0018144 2.0431707 2.3762691 1.4140308 -0.16144753 -1.6491251 -2.6508739 -3.0783339][-2.7504969 -2.6443162 -2.5904937 -2.6253121 -2.7474759 -2.8665719 -2.7254047 -2.0937247 -1.0816137 -0.471447 -0.8926816 -1.8565421 -2.7467988 -3.2045364 -3.2526588][-3.0113809 -2.9692395 -3.0473826 -3.2983472 -3.7142944 -4.0924006 -4.2135243 -3.8628714 -3.0712881 -2.5110514 -2.6662774 -3.2242143 -3.6382434 -3.6736917 -3.4153581][-3.0670066 -3.0022433 -3.0622711 -3.3344436 -3.7694657 -4.1912613 -4.4392705 -4.287447 -3.7239559 -3.3224194 -3.4201655 -3.7628121 -3.9495292 -3.7976084 -3.4143834][-2.8960977 -2.7767494 -2.7548113 -2.9180341 -3.2261262 -3.5518894 -3.7764981 -3.7301323 -3.4293861 -3.218482 -3.3242931 -3.5769055 -3.68986 -3.5477719 -3.2022831][-2.5966904 -2.4226744 -2.310885 -2.3278422 -2.4694364 -2.6542449 -2.7974124 -2.8190694 -2.7164626 -2.664355 -2.8052573 -3.0369227 -3.1741843 -3.117686 -2.8799465]]...]
INFO - root - 2017-12-16 08:34:29.852669: step 16010, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 14h:37m:45s remains)
INFO - root - 2017-12-16 08:34:31.542914: step 16020, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:47m:51s remains)
INFO - root - 2017-12-16 08:34:33.233675: step 16030, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 14h:56m:05s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:34:34.956450: step 16040, loss = 0.63, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 14h:41m:01s remains)
INFO - root - 2017-12-16 08:34:36.666529: step 16050, loss = 0.53, batch loss = 0.27 (46.0 examples/sec; 0.174 sec/batch; 15h:16m:56s remains)
INFO - root - 2017-12-16 08:34:38.342473: step 16060, loss = 0.61, batch loss = 0.35 (47.9 examples/sec; 0.167 sec/batch; 14h:40m:36s remains)
INFO - root - 2017-12-16 08:34:40.041840: step 16070, loss = 0.58, batch loss = 0.32 (46.2 examples/sec; 0.173 sec/batch; 15h:14m:07s remains)
INFO - root - 2017-12-16 08:34:41.736137: step 16080, loss = 0.58, batch loss = 0.32 (46.4 examples/sec; 0.172 sec/batch; 15h:08m:29s remains)
INFO - root - 2017-12-16 08:34:43.454218: step 16090, loss = 0.45, batch loss = 0.19 (44.0 examples/sec; 0.182 sec/batch; 15h:59m:36s remains)
INFO - root - 2017-12-16 08:34:45.121242: step 16100, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 14h:26m:39s remains)
2017-12-16 08:34:45.595877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2612135 -2.20414 -2.2351789 -2.3544261 -2.5355737 -2.7846222 -3.053704 -3.3098295 -3.3860025 -3.2332656 -2.9303522 -2.6047709 -2.3620758 -2.1589162 -1.9805609][-2.4638064 -2.4821904 -2.577255 -2.746769 -3.01156 -3.3204124 -3.6757526 -4.0732603 -4.2962523 -4.2565789 -4.0102715 -3.6300581 -3.2671022 -2.8898723 -2.5335543][-2.4806035 -2.5726602 -2.750912 -2.938561 -3.2135105 -3.48779 -3.8116772 -4.2135153 -4.5621014 -4.7032461 -4.5219536 -4.1575084 -3.7649834 -3.3288598 -2.904017][-2.2421813 -2.3325734 -2.4436266 -2.5262208 -2.6955714 -2.8067217 -2.9171786 -3.1896348 -3.6368604 -4.0580912 -4.1232028 -3.9220896 -3.6945937 -3.4116538 -3.07703][-1.8724531 -1.9715389 -1.976415 -1.842546 -1.8159999 -1.6119347 -1.2379215 -1.1452063 -1.661031 -2.4576869 -2.90805 -2.9472525 -2.9618444 -2.9777193 -2.8987877][-1.4673917 -1.5133412 -1.3989334 -1.0913737 -0.87874162 -0.39749348 0.57552838 1.2334468 0.65375352 -0.56801856 -1.4078557 -1.6485314 -1.9002491 -2.23753 -2.4520915][-1.0736473 -1.1068572 -0.93568528 -0.62578058 -0.40894949 0.25769663 1.7270703 3.0363388 2.3412523 0.72851849 -0.27444124 -0.62541461 -1.0343786 -1.5628338 -1.959239][-0.96283638 -0.99592674 -0.8916589 -0.76842952 -0.61222756 0.15871811 1.8948245 3.554265 2.86383 1.240711 0.28797913 -0.065895081 -0.48992634 -1.0649356 -1.5136642][-1.2485374 -1.2853067 -1.2612032 -1.296404 -1.2880567 -0.6807276 0.65866017 1.9614539 1.6861329 0.67647409 0.082635641 -0.024533272 -0.24674964 -0.72766662 -1.1673169][-1.3838155 -1.430961 -1.4928339 -1.6738765 -1.8637291 -1.5866683 -0.73849022 0.22824621 0.31092453 -0.10309339 -0.31190538 -0.12616158 -0.14035034 -0.48656464 -0.854697][-1.084626 -1.1460624 -1.2507989 -1.5455167 -1.9084978 -1.9239047 -1.4688578 -0.79496992 -0.54731226 -0.62112272 -0.51778054 -0.15953398 -0.035915852 -0.22166657 -0.48861206][-0.39453006 -0.4359194 -0.54624689 -0.85137677 -1.2353021 -1.4565825 -1.3382567 -0.93985736 -0.6729188 -0.58019519 -0.32587862 0.14675689 0.41467285 0.24954247 -0.025407314][0.066039324 0.081566334 0.064658165 -0.11835265 -0.43484354 -0.71230567 -0.76386 -0.56716049 -0.35929167 -0.22529244 0.055919409 0.49185896 0.66155481 0.41111279 0.094110012][-0.0434556 -0.0001707077 0.080210924 0.036333561 -0.14753294 -0.36928177 -0.48136652 -0.42337728 -0.32867241 -0.24520302 -0.00090193748 0.32909632 0.3855145 0.1695044 0.0079913139][-0.92242289 -0.89715505 -0.82533693 -0.80055642 -0.89054942 -1.0279062 -1.1224688 -1.1366751 -1.1097035 -1.0481609 -0.82206595 -0.52839291 -0.45210862 -0.53626204 -0.46956885]]...]
INFO - root - 2017-12-16 08:34:47.256188: step 16110, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.163 sec/batch; 14h:22m:02s remains)
INFO - root - 2017-12-16 08:34:48.933698: step 16120, loss = 0.63, batch loss = 0.37 (45.7 examples/sec; 0.175 sec/batch; 15h:22m:43s remains)
INFO - root - 2017-12-16 08:34:50.631315: step 16130, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 14h:46m:40s remains)
INFO - root - 2017-12-16 08:34:52.294700: step 16140, loss = 0.50, batch loss = 0.24 (48.0 examples/sec; 0.166 sec/batch; 14h:37m:53s remains)
INFO - root - 2017-12-16 08:34:54.011350: step 16150, loss = 0.62, batch loss = 0.36 (47.0 examples/sec; 0.170 sec/batch; 14h:58m:20s remains)
INFO - root - 2017-12-16 08:34:55.708393: step 16160, loss = 0.56, batch loss = 0.30 (47.1 examples/sec; 0.170 sec/batch; 14h:55m:11s remains)
INFO - root - 2017-12-16 08:34:57.417752: step 16170, loss = 0.52, batch loss = 0.26 (46.3 examples/sec; 0.173 sec/batch; 15h:10m:23s remains)
INFO - root - 2017-12-16 08:34:59.128258: step 16180, loss = 0.58, batch loss = 0.32 (46.5 examples/sec; 0.172 sec/batch; 15h:07m:29s remains)
INFO - root - 2017-12-16 08:35:00.819029: step 16190, loss = 0.49, batch loss = 0.23 (48.6 examples/sec; 0.165 sec/batch; 14h:28m:28s remains)
INFO - root - 2017-12-16 08:35:02.521481: step 16200, loss = 0.60, batch loss = 0.34 (45.4 examples/sec; 0.176 sec/batch; 15h:29m:43s remains)
2017-12-16 08:35:03.029287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9339267 -1.9296232 -2.0757265 -2.1988323 -2.2040534 -2.2010958 -2.1611643 -2.0666769 -1.9457837 -1.8719658 -1.7423909 -1.5221277 -1.3633096 -1.4821014 -1.7729523][-1.2121532 -1.1448747 -1.3333126 -1.5703926 -1.6443305 -1.5965543 -1.47211 -1.3354325 -1.2208531 -1.2343701 -1.2627684 -1.1553619 -1.0065356 -1.0425916 -1.2445426][-0.1330719 -0.10279131 -0.40847909 -0.76590657 -0.87949955 -0.75601077 -0.52463162 -0.37275875 -0.34371781 -0.53980839 -0.82651746 -0.94182551 -0.87844682 -0.80839372 -0.84215891][0.88554978 0.78552866 0.31788421 -0.1317184 -0.20391703 0.05337882 0.38342094 0.5317657 0.45107174 -0.038966417 -0.65999854 -1.0592862 -1.1310838 -0.99893558 -0.88039267][1.4444616 1.1887197 0.61017537 0.17890048 0.20390272 0.56907511 0.99908829 1.1953776 1.0203571 0.29899549 -0.54999852 -1.1963872 -1.3753116 -1.2398903 -1.038772][1.3389173 1.0251417 0.46630335 0.12736964 0.24082303 0.67460513 1.2635479 1.6285348 1.4665504 0.64755273 -0.31469989 -1.0122861 -1.2320588 -1.1361216 -0.98975861][0.46736956 0.31640816 -0.03741312 -0.16906643 0.078584433 0.58321977 1.3624048 2.066046 2.1010597 1.2603862 0.20719123 -0.46746445 -0.67311966 -0.66230452 -0.69305909][-0.79124284 -0.64842045 -0.66911089 -0.51823974 -0.066607 0.60129452 1.6221378 2.7354143 3.0754073 2.1872199 1.0591378 0.336761 0.028780222 -0.09303236 -0.32902551][-1.8484088 -1.4077215 -1.1490389 -0.77210009 -0.16865468 0.57547832 1.6977499 2.9584424 3.3874981 2.6136825 1.5961409 0.87134981 0.42357707 0.14029646 -0.21956778][-2.4011295 -1.7797446 -1.4385929 -1.0142754 -0.35642219 0.37995362 1.3500962 2.29191 2.4594285 1.8465841 1.1251547 0.61526275 0.20919871 -0.1463511 -0.59516764][-2.4598567 -1.8144226 -1.4987743 -1.1354254 -0.53744709 0.032231331 0.67489505 1.1806459 1.0443783 0.48005581 0.031211853 -0.23232818 -0.49797654 -0.80406177 -1.2124102][-2.186024 -1.6008086 -1.3158848 -1.0707297 -0.67220914 -0.3831923 -0.11206388 -0.024457216 -0.35145819 -0.85615516 -1.137116 -1.2390357 -1.3057836 -1.4236825 -1.6352898][-1.6023052 -1.1026688 -0.91687977 -0.86358368 -0.75772929 -0.80244327 -0.85134375 -1.0825413 -1.5491755 -1.986593 -2.1596875 -2.078038 -1.9239454 -1.798043 -1.7074943][-0.95204556 -0.53152812 -0.51033235 -0.66014421 -0.82348621 -1.1085638 -1.4505446 -1.9397378 -2.4724071 -2.8596973 -2.8669355 -2.5833638 -2.1942472 -1.834501 -1.5060298][-0.42784202 -0.20449281 -0.38416934 -0.65881205 -0.94793916 -1.3320668 -1.8771026 -2.5159991 -3.0142918 -3.255446 -3.1110651 -2.6593621 -2.082701 -1.616545 -1.2376395]]...]
INFO - root - 2017-12-16 08:35:04.742766: step 16210, loss = 0.59, batch loss = 0.33 (44.6 examples/sec; 0.179 sec/batch; 15h:45m:17s remains)
INFO - root - 2017-12-16 08:35:06.476668: step 16220, loss = 0.50, batch loss = 0.24 (45.2 examples/sec; 0.177 sec/batch; 15h:32m:08s remains)
INFO - root - 2017-12-16 08:35:08.137766: step 16230, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 14h:28m:48s remains)
INFO - root - 2017-12-16 08:35:09.823357: step 16240, loss = 0.68, batch loss = 0.42 (47.0 examples/sec; 0.170 sec/batch; 14h:56m:22s remains)
INFO - root - 2017-12-16 08:35:11.502454: step 16250, loss = 0.48, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:37m:06s remains)
INFO - root - 2017-12-16 08:35:13.199013: step 16260, loss = 0.48, batch loss = 0.22 (47.9 examples/sec; 0.167 sec/batch; 14h:40m:29s remains)
INFO - root - 2017-12-16 08:35:14.882871: step 16270, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 14h:37m:04s remains)
INFO - root - 2017-12-16 08:35:16.568040: step 16280, loss = 0.66, batch loss = 0.40 (47.4 examples/sec; 0.169 sec/batch; 14h:48m:41s remains)
INFO - root - 2017-12-16 08:35:18.252463: step 16290, loss = 0.58, batch loss = 0.32 (46.2 examples/sec; 0.173 sec/batch; 15h:12m:21s remains)
INFO - root - 2017-12-16 08:35:19.921113: step 16300, loss = 0.50, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 15h:02m:54s remains)
2017-12-16 08:35:20.357656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1944487 -3.2118406 -3.235585 -3.242996 -3.2381854 -3.2410686 -3.2631741 -3.2399616 -3.0699453 -2.8587968 -2.7663131 -2.7676542 -2.7277312 -2.5667129 -2.4029548][-3.4730821 -3.5414615 -3.5709791 -3.566587 -3.5618863 -3.5147128 -3.4205718 -3.2402978 -2.9207885 -2.60014 -2.4755471 -2.5371826 -2.5656736 -2.4752123 -2.4189713][-3.1223202 -3.38386 -3.5214167 -3.5073733 -3.4244628 -3.2914124 -3.06357 -2.758425 -2.4235821 -2.2230186 -2.272382 -2.5679324 -2.7955585 -2.8481503 -2.9351935][-1.9422939 -2.4230773 -2.6505923 -2.5576549 -2.304563 -2.0407677 -1.7470284 -1.4717977 -1.3425547 -1.4964404 -1.953599 -2.5326428 -2.9169564 -3.1419022 -3.3635986][-0.5492481 -1.1404916 -1.3042114 -1.0636657 -0.58100641 -0.12357593 0.19121361 0.30719137 0.051321268 -0.6972934 -1.6969831 -2.5401349 -2.9863188 -3.1998262 -3.3582044][0.0362432 -0.4445622 -0.38902605 0.10804391 0.80816007 1.4167101 1.7708182 1.634181 0.86712027 -0.44749987 -1.8689134 -2.840121 -3.1374497 -2.9952774 -2.7627661][-0.28914118 -0.56977558 -0.32125044 0.29388952 1.1592076 1.9274435 2.3152795 2.0119677 0.89998031 -0.77095747 -2.324213 -3.1373336 -3.0896018 -2.4339182 -1.6563513][-1.13602 -1.34282 -1.0263542 -0.36406505 0.48229933 1.1937881 1.5005209 1.073791 -0.10429358 -1.5671637 -2.7652414 -3.1528754 -2.65325 -1.5427495 -0.42088497][-1.8890808 -2.1435061 -1.968021 -1.4007267 -0.70453238 -0.23733854 -0.14243412 -0.62874424 -1.5331168 -2.4716454 -3.052614 -2.9198542 -2.0139258 -0.68982852 0.49218178][-2.2696908 -2.6129742 -2.5778742 -2.1597407 -1.694345 -1.5159202 -1.6609466 -2.1340683 -2.6259472 -2.9491553 -2.9630723 -2.4826322 -1.4242954 -0.13774419 0.87499452][-2.3203523 -2.6337495 -2.6685405 -2.3883638 -2.1313279 -2.1906068 -2.5166514 -2.9023471 -3.0608916 -3.0110343 -2.7623706 -2.1561108 -1.2004368 -0.10708714 0.6630199][-2.4949572 -2.7357049 -2.7510834 -2.5033436 -2.3282294 -2.4692318 -2.794359 -3.0171826 -2.9689522 -2.8343196 -2.6369627 -2.1981533 -1.4595253 -0.50759137 0.16059303][-2.9301648 -3.0457008 -2.9922006 -2.7312515 -2.5953312 -2.7506356 -3.0304339 -3.1075196 -2.9645128 -2.8687901 -2.7638171 -2.4965768 -1.9290029 -1.1133416 -0.49199772][-3.0923662 -3.070411 -2.9404187 -2.6262114 -2.469624 -2.661556 -2.9635003 -3.0442429 -2.989361 -3.0112517 -2.9691348 -2.7162461 -2.2163541 -1.526059 -0.94502783][-2.4729364 -2.3033681 -2.084192 -1.7637993 -1.6965039 -2.0174971 -2.468117 -2.8026114 -3.0187368 -3.1206284 -2.9967289 -2.6682987 -2.2659297 -1.8496236 -1.4910736]]...]
INFO - root - 2017-12-16 08:35:22.023937: step 16310, loss = 0.64, batch loss = 0.38 (49.9 examples/sec; 0.160 sec/batch; 14h:04m:38s remains)
INFO - root - 2017-12-16 08:35:23.716159: step 16320, loss = 0.53, batch loss = 0.27 (44.9 examples/sec; 0.178 sec/batch; 15h:38m:41s remains)
INFO - root - 2017-12-16 08:35:25.377281: step 16330, loss = 0.56, batch loss = 0.30 (48.5 examples/sec; 0.165 sec/batch; 14h:29m:34s remains)
INFO - root - 2017-12-16 08:35:27.036491: step 16340, loss = 0.58, batch loss = 0.32 (45.3 examples/sec; 0.177 sec/batch; 15h:31m:28s remains)
INFO - root - 2017-12-16 08:35:28.738061: step 16350, loss = 0.55, batch loss = 0.30 (46.7 examples/sec; 0.171 sec/batch; 15h:01m:47s remains)
INFO - root - 2017-12-16 08:35:30.450311: step 16360, loss = 0.52, batch loss = 0.26 (45.5 examples/sec; 0.176 sec/batch; 15h:27m:10s remains)
INFO - root - 2017-12-16 08:35:32.125040: step 16370, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:42m:02s remains)
INFO - root - 2017-12-16 08:35:33.849954: step 16380, loss = 0.57, batch loss = 0.31 (45.6 examples/sec; 0.176 sec/batch; 15h:25m:05s remains)
INFO - root - 2017-12-16 08:35:35.513381: step 16390, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 14h:42m:10s remains)
INFO - root - 2017-12-16 08:35:37.207707: step 16400, loss = 0.47, batch loss = 0.21 (47.3 examples/sec; 0.169 sec/batch; 14h:51m:18s remains)
2017-12-16 08:35:37.683070: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.33098 -1.1125038 -1.058217 -1.1898247 -1.3673242 -1.4624047 -1.3669143 -1.1263968 -0.99208283 -1.2264416 -1.6723912 -2.0545244 -2.1803663 -2.0183396 -1.5889044][-1.2792759 -1.0150871 -0.99159944 -1.2240995 -1.5116405 -1.624602 -1.4699764 -1.1353928 -0.90255642 -1.0316032 -1.4169844 -1.8326762 -2.0741987 -2.0441537 -1.735431][-1.6323822 -1.350549 -1.3531495 -1.6802274 -2.0682847 -2.1957891 -1.9840837 -1.6310904 -1.3506386 -1.3299732 -1.5590875 -1.911422 -2.2342103 -2.3458185 -2.1293561][-2.1367354 -1.8785743 -1.8686838 -2.20473 -2.6431472 -2.7921727 -2.6002951 -2.3142006 -2.0887704 -1.9719685 -2.0308623 -2.2654459 -2.593318 -2.7506368 -2.5489089][-2.3459542 -2.1174197 -2.0589638 -2.3805861 -2.901335 -3.14069 -3.0142491 -2.8018956 -2.621984 -2.4656692 -2.3848412 -2.4999554 -2.8386173 -3.0548205 -2.8547673][-2.1273234 -1.8807348 -1.7873412 -2.1082683 -2.6776769 -3.0180492 -2.9724545 -2.7848155 -2.5735664 -2.3738606 -2.2080984 -2.2722616 -2.6736891 -2.9708464 -2.8470395][-1.6935797 -1.4033108 -1.2970241 -1.5651662 -2.0988688 -2.4337938 -2.3765786 -2.09657 -1.7580141 -1.4568801 -1.2259241 -1.3154902 -1.8483198 -2.3198771 -2.346736][-1.3656331 -1.0566924 -0.913919 -1.0564409 -1.3999934 -1.6014898 -1.4580065 -1.0356778 -0.51653755 -0.054876566 0.22758532 0.063218594 -0.66230452 -1.3785006 -1.6456732][-1.3480082 -1.1607274 -1.0137163 -0.98622561 -1.0506028 -1.0324521 -0.77787673 -0.22055483 0.49485779 1.1273525 1.509119 1.2675974 0.25115943 -0.76611674 -1.2985177][-1.6183442 -1.6611151 -1.5928265 -1.4623618 -1.3264506 -1.1498594 -0.8311522 -0.26164174 0.48807502 1.1530349 1.538033 1.2208436 0.10539269 -1.050449 -1.7290895][-1.8699732 -2.1438053 -2.211375 -2.1081347 -1.9240961 -1.7186207 -1.4422576 -1.0093545 -0.42565298 0.18107343 0.54311013 0.24060774 -0.72504938 -1.7983739 -2.4703491][-1.9255685 -2.2773452 -2.4240241 -2.3985007 -2.2933855 -2.1932261 -2.0406334 -1.7832428 -1.3744097 -0.8299582 -0.44171941 -0.63183939 -1.4136701 -2.3546188 -2.9748096][-1.7836466 -2.0296006 -2.1532531 -2.1859748 -2.2095981 -2.2530441 -2.2684529 -2.1949048 -1.909578 -1.3752716 -0.90734994 -0.98199129 -1.6557341 -2.5209446 -3.096019][-1.6319513 -1.6839373 -1.7069681 -1.7392428 -1.8459668 -2.0060418 -2.1522596 -2.1999061 -1.9810945 -1.4380691 -0.92595577 -0.9625535 -1.6261151 -2.4270697 -2.9184604][-1.718322 -1.6242523 -1.5434577 -1.5148284 -1.5936387 -1.7480438 -1.9011844 -1.9546813 -1.736053 -1.2070296 -0.78862107 -0.934991 -1.6257458 -2.3100638 -2.6281137]]...]
INFO - root - 2017-12-16 08:35:39.333347: step 16410, loss = 0.47, batch loss = 0.21 (49.4 examples/sec; 0.162 sec/batch; 14h:13m:08s remains)
INFO - root - 2017-12-16 08:35:41.009813: step 16420, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:35m:18s remains)
INFO - root - 2017-12-16 08:35:42.699320: step 16430, loss = 0.50, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 14h:34m:49s remains)
INFO - root - 2017-12-16 08:35:44.359724: step 16440, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:35m:52s remains)
INFO - root - 2017-12-16 08:35:46.031348: step 16450, loss = 0.79, batch loss = 0.53 (47.9 examples/sec; 0.167 sec/batch; 14h:40m:05s remains)
INFO - root - 2017-12-16 08:35:47.702850: step 16460, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 14h:36m:06s remains)
INFO - root - 2017-12-16 08:35:49.345840: step 16470, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 14h:14m:19s remains)
INFO - root - 2017-12-16 08:35:51.032258: step 16480, loss = 0.70, batch loss = 0.44 (46.9 examples/sec; 0.170 sec/batch; 14h:57m:55s remains)
INFO - root - 2017-12-16 08:35:52.752888: step 16490, loss = 0.47, batch loss = 0.21 (47.3 examples/sec; 0.169 sec/batch; 14h:50m:57s remains)
INFO - root - 2017-12-16 08:35:54.438593: step 16500, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 14h:36m:48s remains)
2017-12-16 08:35:54.923254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0501215 -2.9948745 -2.9323843 -3.036849 -3.1787419 -3.1735945 -3.0383568 -2.9210799 -2.8906326 -2.8223033 -2.697293 -2.5827916 -2.5170975 -2.3965914 -2.2532825][-3.3543031 -3.21776 -3.1193538 -3.3061249 -3.5996757 -3.664166 -3.4916935 -3.3333173 -3.2872579 -3.157675 -2.9511881 -2.8851936 -2.9027495 -2.7989328 -2.6037548][-3.2636042 -3.0545404 -2.9224076 -3.141643 -3.5116868 -3.5989494 -3.3235376 -3.076458 -3.0293872 -2.915483 -2.673619 -2.7085555 -2.8830507 -2.8428364 -2.6340356][-2.6042264 -2.38594 -2.2665553 -2.501997 -2.8220711 -2.7489839 -2.2046881 -1.7815311 -1.8403282 -1.9364641 -1.7673907 -1.8910811 -2.1765764 -2.2674727 -2.1673834][-1.607013 -1.4750688 -1.4574752 -1.6778461 -1.7833605 -1.3179691 -0.32697248 0.30767012 -0.066958427 -0.57311654 -0.6486696 -0.75726938 -1.1408565 -1.4081099 -1.4681242][-0.63307643 -0.62513983 -0.71805906 -0.86910009 -0.68276179 0.31458974 1.9170365 2.9448938 2.1080232 0.878911 0.39219165 0.17642832 -0.25613594 -0.68394244 -0.830614][-0.26090479 -0.28362203 -0.39170218 -0.35261393 0.1928997 1.7152996 4.0678511 5.6775355 4.109458 2.10465 1.1631756 0.75592852 0.19610977 -0.24452472 -0.39733505][-0.881636 -0.80798984 -0.79094362 -0.53893948 0.33038187 2.1601477 5.00187 7.3802905 4.9226589 2.408361 1.2608063 0.72708392 0.11946177 -0.18572235 -0.19425297][-1.9991803 -1.840407 -1.6377006 -1.1400557 -0.15173006 1.5828309 3.8927135 5.2413449 3.4842095 1.4349446 0.48609352 0.0038564205 -0.50346291 -0.53568578 -0.30996513][-2.7016089 -2.4193609 -1.9912567 -1.3871448 -0.45142674 0.86694813 2.2202377 2.6435509 1.3179905 -0.11479259 -0.71721804 -1.0047622 -1.1943557 -0.960397 -0.45088327][-2.7900839 -2.3537152 -1.7538773 -1.0673399 -0.27714014 0.47342658 0.93271518 0.68913627 -0.44470358 -1.4204787 -1.6567543 -1.5479093 -1.3288443 -0.908604 -0.33106327][-2.6926084 -2.1643488 -1.4404547 -0.72152317 -0.16244841 0.094550133 0.014510632 -0.52729321 -1.5247494 -2.0875139 -1.9697907 -1.4444318 -0.87173188 -0.40873158 -0.042032957][-2.6276538 -2.0855641 -1.3328631 -0.72952437 -0.45486987 -0.40781438 -0.60135829 -1.1446763 -1.9250696 -2.216331 -1.8540114 -1.125843 -0.32999921 0.14433289 0.28035116][-2.6159983 -2.0854783 -1.367641 -0.912925 -0.81418145 -0.768157 -0.8618356 -1.2576373 -1.9408355 -2.1920881 -1.7501073 -0.981076 -0.14657211 0.37778878 0.34756041][-2.6437521 -2.1322441 -1.4691193 -1.1034679 -1.0355552 -0.90391672 -0.83004665 -1.0563269 -1.801554 -2.122452 -1.7261741 -1.0239631 -0.28765988 0.12261319 -0.089949846]]...]
INFO - root - 2017-12-16 08:35:56.632813: step 16510, loss = 0.59, batch loss = 0.33 (43.4 examples/sec; 0.184 sec/batch; 16h:09m:47s remains)
INFO - root - 2017-12-16 08:35:58.354288: step 16520, loss = 0.61, batch loss = 0.35 (47.2 examples/sec; 0.170 sec/batch; 14h:52m:44s remains)
INFO - root - 2017-12-16 08:36:00.026724: step 16530, loss = 0.62, batch loss = 0.36 (47.1 examples/sec; 0.170 sec/batch; 14h:53m:35s remains)
INFO - root - 2017-12-16 08:36:01.706526: step 16540, loss = 0.53, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 14h:24m:51s remains)
INFO - root - 2017-12-16 08:36:03.404966: step 16550, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 14h:37m:56s remains)
INFO - root - 2017-12-16 08:36:05.124836: step 16560, loss = 0.57, batch loss = 0.31 (46.1 examples/sec; 0.173 sec/batch; 15h:13m:21s remains)
INFO - root - 2017-12-16 08:36:06.823463: step 16570, loss = 0.51, batch loss = 0.25 (47.2 examples/sec; 0.170 sec/batch; 14h:53m:05s remains)
INFO - root - 2017-12-16 08:36:08.520008: step 16580, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 14h:32m:13s remains)
INFO - root - 2017-12-16 08:36:10.208574: step 16590, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 15h:08m:13s remains)
INFO - root - 2017-12-16 08:36:11.908946: step 16600, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 14h:42m:45s remains)
2017-12-16 08:36:12.408681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.801959 -1.7759612 -1.7047415 -1.6051219 -1.4995003 -1.3456783 -1.223591 -1.1350064 -1.1132077 -1.1671234 -1.3452601 -1.6271943 -1.9165137 -2.105582 -2.2276371][-2.3281181 -2.3938348 -2.2886524 -2.1078243 -1.9985675 -1.9824229 -2.0551894 -2.1143732 -2.1128213 -2.1021905 -2.1396654 -2.2847037 -2.4748025 -2.5651248 -2.5847676][-2.7033489 -2.8214755 -2.6397345 -2.3602254 -2.1955197 -2.30835 -2.5974176 -2.846518 -2.9053264 -2.8089471 -2.6733735 -2.608578 -2.6054273 -2.5293109 -2.4083867][-2.7747717 -2.838861 -2.5201278 -2.0652924 -1.7502525 -1.9035403 -2.309541 -2.6841488 -2.8382485 -2.7741346 -2.5792241 -2.3987386 -2.2502141 -2.0344372 -1.7329326][-2.5048809 -2.4784865 -2.0227828 -1.3734858 -0.87519884 -0.84704709 -1.1415517 -1.4891708 -1.7366984 -1.8258175 -1.7562072 -1.6803467 -1.5629375 -1.2802657 -0.80445552][-2.0457509 -2.0310421 -1.5666832 -0.80712771 -0.085586786 0.37213326 0.46364307 0.35376024 0.076069355 -0.21464467 -0.41456163 -0.62175834 -0.71449077 -0.49254823 0.041090488][-1.3900664 -1.542913 -1.295385 -0.57532394 0.40519428 1.3868101 2.0476005 2.4114058 2.139456 1.5328085 0.91792512 0.2736249 -0.14468455 -0.042075634 0.46217275][-0.52927351 -1.0123289 -1.1377101 -0.63848329 0.36605954 1.6582205 2.850877 3.7700045 3.4527924 2.4661925 1.4783521 0.47936606 -0.13323164 -0.05542016 0.42428851][0.33213902 -0.53786516 -1.0708468 -0.916661 -0.10937119 1.0907729 2.3236082 3.1960919 2.9057424 1.9956777 0.99636722 0.014583349 -0.56599069 -0.47983468 -0.038211584][0.85236073 -0.28135157 -1.106786 -1.2660478 -0.77806067 0.15288949 1.1576903 1.7299016 1.4343574 0.69021869 -0.10062909 -0.83104587 -1.2639687 -1.1782582 -0.89920175][0.82198954 -0.32534409 -1.2447355 -1.612767 -1.3843001 -0.73316884 0.0099930763 0.32020497 0.0173378 -0.57422972 -1.1357553 -1.6282934 -1.9171685 -1.8634932 -1.7066417][0.29160261 -0.71887839 -1.563477 -1.992342 -1.9719664 -1.5594562 -1.063295 -0.86985147 -1.0864273 -1.4191017 -1.7506212 -2.0978472 -2.2988336 -2.3191938 -2.2965455][-0.44892859 -1.2884576 -1.9874835 -2.3549857 -2.4256113 -2.1929526 -1.8728297 -1.6765988 -1.6702271 -1.7254951 -1.9017311 -2.2213163 -2.4958904 -2.68761 -2.8411806][-0.93907237 -1.6021641 -2.1054368 -2.3061292 -2.3773525 -2.2734075 -2.0590882 -1.8538387 -1.6550426 -1.5339849 -1.6341195 -2.0162771 -2.4513574 -2.8548567 -3.16613][-1.1423813 -1.5640972 -1.8059886 -1.7720045 -1.7972732 -1.8120991 -1.7461538 -1.6540835 -1.4137678 -1.1920586 -1.2778536 -1.7064755 -2.2219677 -2.7254848 -3.0974262]]...]
INFO - root - 2017-12-16 08:36:14.053586: step 16610, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.166 sec/batch; 14h:32m:42s remains)
INFO - root - 2017-12-16 08:36:15.721903: step 16620, loss = 0.49, batch loss = 0.23 (46.6 examples/sec; 0.171 sec/batch; 15h:02m:50s remains)
INFO - root - 2017-12-16 08:36:17.401811: step 16630, loss = 0.54, batch loss = 0.29 (49.4 examples/sec; 0.162 sec/batch; 14h:13m:09s remains)
INFO - root - 2017-12-16 08:36:19.085122: step 16640, loss = 0.57, batch loss = 0.31 (46.8 examples/sec; 0.171 sec/batch; 14h:59m:02s remains)
INFO - root - 2017-12-16 08:36:20.773511: step 16650, loss = 0.61, batch loss = 0.35 (46.5 examples/sec; 0.172 sec/batch; 15h:06m:20s remains)
INFO - root - 2017-12-16 08:36:22.473607: step 16660, loss = 0.67, batch loss = 0.41 (48.0 examples/sec; 0.167 sec/batch; 14h:37m:07s remains)
INFO - root - 2017-12-16 08:36:24.192341: step 16670, loss = 0.46, batch loss = 0.21 (45.8 examples/sec; 0.175 sec/batch; 15h:20m:20s remains)
INFO - root - 2017-12-16 08:36:25.883486: step 16680, loss = 0.57, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 14h:53m:49s remains)
INFO - root - 2017-12-16 08:36:27.557156: step 16690, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:50m:07s remains)
INFO - root - 2017-12-16 08:36:29.257828: step 16700, loss = 0.47, batch loss = 0.21 (46.6 examples/sec; 0.172 sec/batch; 15h:04m:07s remains)
2017-12-16 08:36:29.771034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5049574 -1.0135517 -0.18689775 0.31778336 0.30870461 -0.25177526 -1.4004047 -2.6208825 -3.0968397 -2.8667681 -2.6529691 -2.4628468 -2.0194409 -1.3259368 -0.85172689][-1.4806066 -0.87428427 -0.095659018 0.093231678 -0.26107121 -0.92588687 -1.8567654 -2.7657347 -3.071651 -2.8761082 -2.7622356 -2.6942906 -2.4163258 -1.9281663 -1.5540144][-1.9478201 -1.4566338 -0.91445661 -0.97904229 -1.413259 -1.8800528 -2.3616271 -2.85003 -3.0035746 -2.907907 -2.891803 -2.9145169 -2.8050733 -2.5692933 -2.3608649][-2.5746462 -2.2983246 -1.9919686 -2.0949914 -2.2971866 -2.2752776 -2.17074 -2.2390339 -2.3950946 -2.5712159 -2.7637949 -2.9015386 -2.9298275 -2.8648338 -2.68496][-2.9059875 -2.8875616 -2.75725 -2.7560647 -2.5265942 -1.8193977 -0.97608471 -0.63948286 -0.98993623 -1.6857264 -2.2643087 -2.5783653 -2.7167921 -2.7419887 -2.4962394][-2.7410567 -2.9765515 -2.9487131 -2.7608044 -2.0772736 -0.63319814 1.0018482 1.6993201 1.0226612 -0.34118748 -1.4528161 -2.0626361 -2.3687093 -2.4639275 -2.0936477][-2.110127 -2.5524769 -2.615833 -2.2802069 -1.216114 0.86832047 3.2410047 4.247015 3.0682957 0.93911743 -0.74841988 -1.6485224 -2.0927298 -2.2036362 -1.7370567][-1.3297075 -1.88457 -1.9687598 -1.5868452 -0.44333172 1.8676283 4.5912809 5.7015 4.0467472 1.3502233 -0.74847746 -1.8760953 -2.3598657 -2.3639927 -1.7800062][-0.88388348 -1.407149 -1.5392563 -1.3015389 -0.49037337 1.2505143 3.2776363 3.9656169 2.4862287 0.094442606 -1.8020302 -2.7629635 -3.0945835 -2.9232564 -2.1959527][-0.93872452 -1.303871 -1.4492607 -1.4516797 -1.109525 -0.16342187 0.974324 1.1786897 0.0018136501 -1.7377369 -3.0644133 -3.6216412 -3.6923487 -3.3497486 -2.5002744][-1.0845807 -1.1479552 -1.3122982 -1.5749351 -1.6818941 -1.4520836 -1.0765514 -1.2134973 -2.0348463 -3.0871313 -3.7948496 -3.9746289 -3.8429556 -3.3826675 -2.4334528][-1.0697308 -0.74414527 -0.78834832 -1.2211328 -1.6936193 -2.0117409 -2.2427123 -2.6498423 -3.210084 -3.7210352 -3.9845414 -3.9376941 -3.7044864 -3.2239647 -2.2944615][-1.1129384 -0.48454463 -0.35429537 -0.80951309 -1.4375765 -2.0221395 -2.6285555 -3.1962461 -3.5588512 -3.70606 -3.7044 -3.592936 -3.4098585 -3.0892587 -2.3779328][-1.394315 -0.59426486 -0.2914331 -0.66931474 -1.3116693 -1.9948001 -2.7316253 -3.3080935 -3.4736812 -3.3486352 -3.1956048 -3.128659 -3.0806713 -2.9621994 -2.539041][-1.7732728 -0.95363712 -0.55027306 -0.76860464 -1.2783912 -1.9500505 -2.7795892 -3.371242 -3.3938832 -3.0508213 -2.7373195 -2.6157391 -2.62052 -2.6887479 -2.5580974]]...]
INFO - root - 2017-12-16 08:36:31.447033: step 16710, loss = 0.50, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 15h:01m:06s remains)
INFO - root - 2017-12-16 08:36:33.142965: step 16720, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:43m:07s remains)
INFO - root - 2017-12-16 08:36:34.810538: step 16730, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:22m:00s remains)
INFO - root - 2017-12-16 08:36:36.468647: step 16740, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 14h:29m:04s remains)
INFO - root - 2017-12-16 08:36:38.142953: step 16750, loss = 0.45, batch loss = 0.19 (49.0 examples/sec; 0.163 sec/batch; 14h:19m:19s remains)
INFO - root - 2017-12-16 08:36:39.828265: step 16760, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 14h:29m:18s remains)
INFO - root - 2017-12-16 08:36:41.492746: step 16770, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:35m:22s remains)
INFO - root - 2017-12-16 08:36:43.176333: step 16780, loss = 0.55, batch loss = 0.29 (47.0 examples/sec; 0.170 sec/batch; 14h:56m:03s remains)
INFO - root - 2017-12-16 08:36:44.851419: step 16790, loss = 0.63, batch loss = 0.37 (48.6 examples/sec; 0.165 sec/batch; 14h:26m:37s remains)
INFO - root - 2017-12-16 08:36:46.501873: step 16800, loss = 0.55, batch loss = 0.29 (45.7 examples/sec; 0.175 sec/batch; 15h:21m:14s remains)
2017-12-16 08:36:46.991202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0571947 -1.5180017 -1.3377794 -1.3707618 -1.3341571 -1.3744118 -1.4423873 -1.4866283 -1.5183133 -1.6781214 -2.0333319 -2.4851913 -2.8746889 -3.098165 -3.0096729][-2.3422949 -1.7270303 -1.470479 -1.3877565 -1.2791106 -1.3883097 -1.5560215 -1.5620561 -1.5049794 -1.6201946 -1.939546 -2.3224647 -2.6197958 -2.6770482 -2.5055563][-2.0229297 -1.4678066 -1.210953 -1.0715358 -1.0361004 -1.2655799 -1.5066144 -1.5203754 -1.4151969 -1.5966518 -2.0741978 -2.4709377 -2.6773419 -2.5126641 -2.0920622][-1.3714077 -1.0223386 -0.80604196 -0.59079432 -0.54576743 -0.78640079 -1.0453806 -1.0286736 -0.9561919 -1.2529249 -1.8906276 -2.4337695 -2.562119 -2.2201865 -1.5552696][-0.50958025 -0.33918667 -0.11533904 0.18872285 0.3185873 0.064889193 -0.14608908 -0.031277418 0.066836119 -0.30296922 -1.1227515 -1.8206782 -1.9551712 -1.5687027 -0.85763645][0.48799133 0.574095 0.86038613 1.3056915 1.4935567 1.3502679 1.3458467 1.6227064 1.7244053 1.2239599 0.22229028 -0.58451974 -0.88348234 -0.5867461 0.046258688][1.1384389 1.2410667 1.6777349 2.3413873 2.664681 2.7231307 3.0749168 3.7020941 3.8807616 3.2277927 2.0717049 1.1014495 0.60666203 0.64181161 0.96981096][1.2626524 1.4015601 1.9858909 2.7147255 3.0428281 3.2035322 3.8532076 4.8914804 5.3043022 4.6488967 3.4595437 2.4120741 1.7488966 1.5295579 1.4542921][0.53205705 0.56515455 1.1009259 1.7782097 2.1060882 2.323833 2.9590626 3.9430909 4.3682957 3.9484177 3.1927147 2.4795771 1.7967978 1.3491786 0.92674375][-0.61601782 -0.77625728 -0.42229378 0.097458839 0.36658764 0.57088017 1.1340775 1.941669 2.3338494 2.0722666 1.6411953 1.2196157 0.709321 0.23003411 -0.31366587][-1.9000813 -2.2845116 -2.1969626 -1.8536084 -1.6407473 -1.5383825 -1.1672803 -0.62609994 -0.33692694 -0.46527827 -0.68811822 -0.85613728 -1.0931187 -1.4249218 -1.8867409][-2.9063401 -3.3534188 -3.4406521 -3.3238802 -3.2336969 -3.2203588 -3.084537 -2.8302507 -2.6657381 -2.7322357 -2.8281403 -2.8236756 -2.8071759 -2.8963592 -3.0689316][-3.3741603 -3.7373834 -3.9170418 -3.9592485 -4.0181642 -4.1049051 -4.1237411 -4.0725975 -3.9967494 -3.9571924 -3.9117866 -3.7785008 -3.5842285 -3.4534731 -3.3831863][-3.3707533 -3.5308661 -3.6583524 -3.7395792 -3.8361056 -3.9577634 -4.0206847 -4.0219617 -3.9629378 -3.8760786 -3.7687416 -3.6169548 -3.4029231 -3.1804185 -2.9798503][-2.9318194 -2.9330449 -2.9678395 -3.0086083 -3.0625062 -3.1385744 -3.1850343 -3.1952691 -3.1470528 -3.0557184 -2.9334617 -2.7799053 -2.5823233 -2.3454044 -2.1045356]]...]
INFO - root - 2017-12-16 08:36:48.717804: step 16810, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 14h:17m:58s remains)
INFO - root - 2017-12-16 08:36:50.410028: step 16820, loss = 0.68, batch loss = 0.42 (46.6 examples/sec; 0.171 sec/batch; 15h:02m:18s remains)
INFO - root - 2017-12-16 08:36:52.070739: step 16830, loss = 0.54, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 14h:32m:24s remains)
INFO - root - 2017-12-16 08:36:53.772585: step 16840, loss = 0.58, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 14h:34m:03s remains)
INFO - root - 2017-12-16 08:36:55.499405: step 16850, loss = 0.57, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 14h:57m:52s remains)
INFO - root - 2017-12-16 08:36:57.165568: step 16860, loss = 0.83, batch loss = 0.57 (48.9 examples/sec; 0.163 sec/batch; 14h:19m:54s remains)
INFO - root - 2017-12-16 08:36:58.836449: step 16870, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 14h:28m:29s remains)
INFO - root - 2017-12-16 08:37:00.508015: step 16880, loss = 0.58, batch loss = 0.33 (49.4 examples/sec; 0.162 sec/batch; 14h:12m:27s remains)
INFO - root - 2017-12-16 08:37:02.137933: step 16890, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:32m:27s remains)
INFO - root - 2017-12-16 08:37:03.799577: step 16900, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:30m:56s remains)
2017-12-16 08:37:04.290286: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2948525 -3.0782783 -2.9875898 -3.0349679 -3.1138487 -2.9016206 -2.2153668 -1.496255 -1.1606424 -1.2411951 -1.4826659 -1.7624449 -1.8150984 -1.6053061 -1.3114785][-2.9909518 -2.8262353 -2.7670052 -2.8474436 -3.0207629 -2.9227428 -2.3374686 -1.6496112 -1.2460574 -1.2290512 -1.4118872 -1.7164155 -1.8324312 -1.63279 -1.3816023][-2.8629544 -2.7617319 -2.7040269 -2.8203692 -3.0870881 -3.0650229 -2.492749 -1.7725867 -1.3752371 -1.4136752 -1.6848164 -2.0524809 -2.1364944 -1.891541 -1.5948894][-2.8559346 -2.7493167 -2.6288512 -2.7203398 -2.9475064 -2.8104255 -2.0874236 -1.3394027 -1.1271828 -1.4477568 -1.9919002 -2.4444175 -2.4816961 -2.1752219 -1.819114][-2.8464758 -2.646116 -2.4205029 -2.3941145 -2.4361608 -1.9783403 -0.98096848 -0.23387408 -0.37261295 -1.2026898 -2.0939376 -2.660398 -2.6975119 -2.3389969 -1.9432566][-2.6194527 -2.3205833 -1.9661695 -1.7222931 -1.3693509 -0.44118726 0.85651946 1.5174499 0.90761852 -0.50459039 -1.8224697 -2.5987012 -2.7145329 -2.3643861 -1.9605727][-2.1326134 -1.7602714 -1.2796426 -0.7504766 0.0919404 1.5338814 3.0743527 3.5561967 2.4852219 0.53640747 -1.2222384 -2.3075714 -2.5931149 -2.3278646 -1.9430381][-1.6966833 -1.2698472 -0.71689355 -0.0023393631 1.140934 2.9321823 4.6858244 5.0726004 3.7561321 1.4754896 -0.61030948 -1.9871802 -2.4847584 -2.3260276 -1.9564512][-1.7501137 -1.309274 -0.71715283 0.098868608 1.332736 3.1154194 4.7737026 5.1549397 3.9028859 1.6982374 -0.41252875 -1.8925501 -2.5148976 -2.4262314 -2.053802][-2.1957221 -1.8270011 -1.2660276 -0.48926818 0.59756517 2.0747166 3.4222512 3.7997603 2.8186321 0.99047852 -0.85213649 -2.1659234 -2.7081859 -2.5785251 -2.1767235][-2.7077823 -2.4631021 -2.0162928 -1.3755782 -0.4491061 0.77184534 1.9143686 2.267632 1.4784024 -0.009926796 -1.5043201 -2.5486608 -2.9067965 -2.6908274 -2.2501702][-2.7047544 -2.669553 -2.4663489 -2.0415196 -1.2770309 -0.17417002 0.91252065 1.3195267 0.71684623 -0.53172755 -1.7990515 -2.6753848 -2.9321165 -2.6975863 -2.2708192][-2.2402711 -2.3959298 -2.4605975 -2.3298998 -1.7811124 -0.7885654 0.34270692 0.9214251 0.54880881 -0.5115844 -1.656969 -2.4930775 -2.7681522 -2.5939763 -2.2354574][-1.6507182 -1.9150914 -2.1787584 -2.2853334 -1.9652894 -1.0978526 0.097055674 0.89348888 0.73878479 -0.16075253 -1.2759387 -2.1904695 -2.5753982 -2.4923592 -2.1962347][-1.527704 -1.8771819 -2.1616092 -2.3129957 -2.0682235 -1.1605955 0.18645 1.156148 1.1557024 0.33430076 -0.8368237 -1.9055102 -2.4765444 -2.4962575 -2.2204795]]...]
INFO - root - 2017-12-16 08:37:05.961358: step 16910, loss = 0.63, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 14h:26m:48s remains)
INFO - root - 2017-12-16 08:37:07.634772: step 16920, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:34m:13s remains)
INFO - root - 2017-12-16 08:37:09.338025: step 16930, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:21m:15s remains)
INFO - root - 2017-12-16 08:37:11.005827: step 16940, loss = 0.58, batch loss = 0.32 (49.0 examples/sec; 0.163 sec/batch; 14h:19m:28s remains)
INFO - root - 2017-12-16 08:37:12.651365: step 16950, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:30m:30s remains)
INFO - root - 2017-12-16 08:37:14.350874: step 16960, loss = 0.49, batch loss = 0.23 (47.7 examples/sec; 0.168 sec/batch; 14h:42m:50s remains)
INFO - root - 2017-12-16 08:37:16.039251: step 16970, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 14h:35m:10s remains)
INFO - root - 2017-12-16 08:37:17.716125: step 16980, loss = 0.51, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:21m:07s remains)
INFO - root - 2017-12-16 08:37:19.410927: step 16990, loss = 0.57, batch loss = 0.31 (49.6 examples/sec; 0.161 sec/batch; 14h:08m:51s remains)
INFO - root - 2017-12-16 08:37:21.070476: step 17000, loss = 0.48, batch loss = 0.22 (48.6 examples/sec; 0.165 sec/batch; 14h:25m:26s remains)
2017-12-16 08:37:21.534636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5834842 -2.3712633 -2.101527 -2.0021646 -2.0331926 -2.0460975 -2.1028423 -2.3888388 -2.6105311 -2.6431637 -2.4911072 -2.353246 -2.2871926 -2.0831425 -1.8276947][-2.1534691 -2.2093976 -2.1933482 -2.1830709 -2.2025459 -2.2971923 -2.5450737 -3.0544205 -3.4652629 -3.6201429 -3.5005116 -3.3379869 -3.2305524 -2.9731138 -2.6161563][-1.2705197 -1.7242706 -2.1767504 -2.4279673 -2.5738103 -2.8737912 -3.3348641 -3.9347548 -4.4028187 -4.5839844 -4.48858 -4.3431439 -4.2359691 -3.9937863 -3.6116123][-0.19865918 -1.0326266 -1.8980219 -2.352443 -2.651598 -3.1618938 -3.7276235 -4.2662921 -4.6729126 -4.805316 -4.6945825 -4.6146259 -4.6497188 -4.5469766 -4.2671518][0.60966206 -0.29858041 -1.254847 -1.7051212 -2.0738342 -2.5893893 -3.0187857 -3.3783066 -3.7084651 -3.8603311 -3.778501 -3.8480353 -4.1412735 -4.3054185 -4.2154126][0.61521959 -0.02700448 -0.70393884 -0.86227167 -1.0474997 -1.264639 -1.2251016 -1.2650883 -1.6693909 -1.985827 -1.9747751 -2.2104588 -2.8580232 -3.3607848 -3.4601712][-0.19470429 -0.46750641 -0.62217665 -0.28076553 0.025246143 0.34428787 1.0492265 1.3876908 0.74618506 0.062424421 -0.056434631 -0.44095862 -1.428529 -2.1999426 -2.4398403][-1.1582528 -1.1830575 -0.90531552 -0.15105581 0.48532128 1.2345154 2.492537 3.1139216 2.1621437 1.1380181 0.9783721 0.56257653 -0.53954256 -1.3649138 -1.6461029][-2.0003223 -2.0437744 -1.604465 -0.73188281 -0.10435724 0.73812056 2.0776153 2.6719217 1.7056355 0.71250868 0.60723329 0.31245136 -0.66422486 -1.3600745 -1.5541337][-2.543103 -2.7268505 -2.3381777 -1.5830878 -1.1987711 -0.6575911 0.33323431 0.72258949 -0.039822578 -0.76879478 -0.802415 -0.99615133 -1.7350605 -2.2039521 -2.1941557][-2.7201812 -3.1084445 -2.938302 -2.4688396 -2.353946 -2.1880047 -1.6244588 -1.4133129 -1.9498363 -2.4071805 -2.4045846 -2.5438766 -3.0160081 -3.2390785 -3.0654621][-2.7295003 -3.2255855 -3.2720022 -3.1268387 -3.1798179 -3.1765356 -2.9720056 -2.9414964 -3.2942343 -3.5190811 -3.4828758 -3.5344787 -3.7696629 -3.7876825 -3.5079989][-2.7976775 -3.2858579 -3.4709196 -3.4834497 -3.4874043 -3.5062654 -3.4923115 -3.5723455 -3.7445779 -3.7732668 -3.689302 -3.6607394 -3.6775565 -3.5166144 -3.2065079][-2.9448812 -3.2788138 -3.4540548 -3.4465022 -3.3216112 -3.2845349 -3.325371 -3.4117808 -3.4529932 -3.405719 -3.2919297 -3.1709309 -3.0396545 -2.7893579 -2.4724908][-3.028945 -3.1794386 -3.2810471 -3.17114 -2.888864 -2.7460566 -2.780633 -2.8370337 -2.8158631 -2.7688744 -2.71123 -2.5380878 -2.2943878 -1.9919934 -1.7387817]]...]
INFO - root - 2017-12-16 08:37:23.251473: step 17010, loss = 0.65, batch loss = 0.39 (48.0 examples/sec; 0.167 sec/batch; 14h:37m:12s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:37:24.943952: step 17020, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:27m:51s remains)
INFO - root - 2017-12-16 08:37:26.664319: step 17030, loss = 0.60, batch loss = 0.34 (46.6 examples/sec; 0.172 sec/batch; 15h:02m:15s remains)
INFO - root - 2017-12-16 08:37:28.375412: step 17040, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 15h:09m:35s remains)
INFO - root - 2017-12-16 08:37:30.044302: step 17050, loss = 0.49, batch loss = 0.23 (46.8 examples/sec; 0.171 sec/batch; 14h:58m:17s remains)
INFO - root - 2017-12-16 08:37:31.723055: step 17060, loss = 0.63, batch loss = 0.37 (48.7 examples/sec; 0.164 sec/batch; 14h:22m:51s remains)
INFO - root - 2017-12-16 08:37:33.409111: step 17070, loss = 0.68, batch loss = 0.42 (48.8 examples/sec; 0.164 sec/batch; 14h:22m:22s remains)
INFO - root - 2017-12-16 08:37:35.067590: step 17080, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 14h:28m:40s remains)
INFO - root - 2017-12-16 08:37:36.737967: step 17090, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 14h:23m:43s remains)
INFO - root - 2017-12-16 08:37:38.424163: step 17100, loss = 0.56, batch loss = 0.30 (45.9 examples/sec; 0.174 sec/batch; 15h:15m:40s remains)
2017-12-16 08:37:38.894783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.85323 -2.874959 -2.5581894 -1.924938 -1.132466 -0.57927513 -0.42072713 -0.59031892 -1.1322656 -1.7383409 -2.0952611 -2.4715078 -2.7527769 -2.7761667 -2.7458963][-3.0015442 -3.0273674 -2.7168052 -2.0102856 -1.1587152 -0.60567331 -0.42267442 -0.51836824 -0.91907728 -1.4007776 -1.7457854 -2.1678469 -2.4746063 -2.5542493 -2.5983231][-3.1134534 -3.080003 -2.7190821 -1.9811771 -1.1576959 -0.60803115 -0.39021611 -0.36271858 -0.50465631 -0.85337949 -1.2315553 -1.7790899 -2.1962709 -2.3593168 -2.4920359][-3.0900478 -2.8775945 -2.4034138 -1.6082783 -0.856506 -0.31877327 -0.0080063343 0.12523913 0.18810678 -0.0938921 -0.53843892 -1.2491568 -1.878756 -2.1870506 -2.3982356][-2.843044 -2.3951752 -1.8184927 -1.0674194 -0.43856692 0.029726028 0.46300578 0.85586929 1.0889459 0.87105417 0.32923436 -0.58073151 -1.5137146 -2.0198855 -2.2882266][-2.3558726 -1.7995956 -1.2236583 -0.61826587 -0.1096313 0.31676841 0.9776907 1.7440176 2.1877542 1.9606247 1.2146566 0.0022392273 -1.2136308 -1.8931308 -2.1960981][-1.6672798 -1.0589045 -0.56312859 -0.057039738 0.38121486 0.91438317 1.8533807 2.9519491 3.4832993 3.1559792 2.0841813 0.54009295 -0.93806756 -1.7635913 -2.0991192][-0.9965924 -0.31918573 0.13593411 0.59461617 0.97240925 1.5532587 2.5854511 3.7567434 4.19178 3.7513838 2.4549894 0.75383139 -0.82256258 -1.7293216 -2.05897][-0.65608847 0.028073072 0.35847497 0.68606758 0.96805215 1.4986355 2.4796619 3.66967 4.1094389 3.6051145 2.2560515 0.56321907 -0.9929018 -1.8844373 -2.1564705][-0.72523391 -0.18639135 -0.056744814 0.093283415 0.28224564 0.7753861 1.7150593 2.9209375 3.3410859 2.8349609 1.5987515 0.018025398 -1.3824191 -2.1414604 -2.3201129][-1.031477 -0.72902894 -0.7769767 -0.75253952 -0.56659055 -0.073131084 0.80882025 1.8498993 2.1094179 1.5561333 0.48918056 -0.82246149 -1.9184275 -2.4453719 -2.4979572][-1.4215395 -1.3546749 -1.4910501 -1.5276814 -1.3309909 -0.90249944 -0.2840786 0.38877916 0.53413844 0.090598106 -0.68651438 -1.6406171 -2.3700604 -2.6334546 -2.5706377][-1.8307992 -1.9218993 -2.055927 -2.1528018 -2.0391736 -1.8275237 -1.5236523 -1.1308686 -1.0032851 -1.2337306 -1.6877214 -2.2672429 -2.6554623 -2.6828992 -2.5225389][-2.1359184 -2.2896307 -2.4343281 -2.5634229 -2.5724339 -2.5332918 -2.37473 -2.0766337 -1.9039941 -1.991686 -2.2410247 -2.565362 -2.7298403 -2.6176057 -2.40286][-2.2305431 -2.3782234 -2.505239 -2.6248786 -2.7011695 -2.7303557 -2.5990379 -2.3445549 -2.1913188 -2.248769 -2.4018478 -2.573467 -2.6014733 -2.4424944 -2.2221787]]...]
INFO - root - 2017-12-16 08:37:40.602275: step 17110, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.170 sec/batch; 14h:51m:35s remains)
INFO - root - 2017-12-16 08:37:42.293099: step 17120, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:38m:17s remains)
INFO - root - 2017-12-16 08:37:43.950226: step 17130, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 14h:29m:03s remains)
INFO - root - 2017-12-16 08:37:45.633577: step 17140, loss = 0.56, batch loss = 0.30 (47.6 examples/sec; 0.168 sec/batch; 14h:42m:32s remains)
INFO - root - 2017-12-16 08:37:47.333991: step 17150, loss = 0.57, batch loss = 0.31 (49.7 examples/sec; 0.161 sec/batch; 14h:05m:37s remains)
INFO - root - 2017-12-16 08:37:48.989874: step 17160, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 14h:36m:37s remains)
INFO - root - 2017-12-16 08:37:50.665924: step 17170, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.169 sec/batch; 14h:50m:06s remains)
INFO - root - 2017-12-16 08:37:52.326237: step 17180, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 14h:25m:07s remains)
INFO - root - 2017-12-16 08:37:54.004524: step 17190, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 14h:23m:25s remains)
INFO - root - 2017-12-16 08:37:55.675764: step 17200, loss = 0.63, batch loss = 0.37 (49.1 examples/sec; 0.163 sec/batch; 14h:16m:25s remains)
2017-12-16 08:37:56.176499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.758657 -2.8737204 -3.0542417 -3.2922454 -3.4819522 -3.5205855 -3.4854207 -3.4585972 -3.4370933 -3.4185076 -3.3654466 -3.2261152 -2.9684608 -2.6803048 -2.4308281][-3.0603566 -3.2782931 -3.6353941 -4.0679197 -4.3583374 -4.3502626 -4.1860309 -4.0759659 -4.03481 -3.970407 -3.872582 -3.6596661 -3.3203475 -2.9062946 -2.5456893][-3.0916579 -3.3221655 -3.7530193 -4.2604747 -4.5687881 -4.5040483 -4.2684264 -4.0953979 -4.0589809 -4.050364 -3.9831171 -3.7755795 -3.4376366 -2.9889154 -2.5953872][-2.8323052 -2.915617 -3.2883821 -3.7120509 -3.9548075 -3.8513858 -3.6101575 -3.4460766 -3.4178362 -3.5066669 -3.531528 -3.4657569 -3.2265894 -2.870368 -2.562094][-2.437798 -2.2871013 -2.4251773 -2.604248 -2.5846748 -2.3143022 -2.0319135 -1.9998437 -2.0952146 -2.2422163 -2.4482832 -2.6640568 -2.692378 -2.5650344 -2.4188175][-1.9083207 -1.4271573 -1.2466573 -1.0153998 -0.58697975 0.057309866 0.39015484 0.21408367 -0.096646547 -0.42462242 -0.93490756 -1.4992982 -1.8548928 -2.0145633 -2.1024122][-1.4977782 -0.69588685 -0.21593475 0.42393112 1.3446918 2.4165056 2.7352417 2.2767227 1.8305428 1.3432899 0.49552178 -0.33582497 -0.86534989 -1.25216 -1.5824261][-1.4396369 -0.51130235 0.14526725 0.945977 2.2107322 3.504297 3.6957591 2.9937065 2.6785967 2.2744639 1.3469486 0.49454522 -0.11179471 -0.60082459 -1.0709914][-1.8966933 -1.084631 -0.44469309 0.31392932 1.5455773 2.6907175 2.7979715 2.1509974 2.0751679 1.9703782 1.26666 0.59114003 0.097761869 -0.3533591 -0.81403279][-2.6984756 -2.1757116 -1.714293 -1.119784 -0.22947192 0.53599381 0.56814432 0.2019825 0.40682149 0.68220782 0.40858078 0.047415257 -0.24449778 -0.57982171 -0.99191856][-3.3818436 -3.1988974 -3.0299554 -2.7176566 -2.2275794 -1.8254328 -1.9217911 -2.1506464 -1.7808053 -1.2240032 -1.1001892 -1.2079418 -1.3215793 -1.4773854 -1.6552007][-3.6552434 -3.7238531 -3.7818775 -3.7136135 -3.5688627 -3.4602871 -3.6535926 -3.8524661 -3.4746084 -2.9655297 -2.7445912 -2.699825 -2.6340597 -2.5768776 -2.5391178][-3.498246 -3.6829524 -3.8395512 -3.9317551 -3.950305 -3.9727111 -4.1631479 -4.3366919 -4.159193 -3.860899 -3.6769638 -3.5706742 -3.4212103 -3.2780943 -3.0849736][-3.0374856 -3.22904 -3.410327 -3.5334196 -3.5977354 -3.671752 -3.8096709 -3.9428372 -3.94835 -3.8645921 -3.7813282 -3.7235069 -3.6025476 -3.4205132 -3.1860497][-2.5846071 -2.6966403 -2.8105431 -2.9036632 -2.9702568 -3.0335946 -3.1116564 -3.2054455 -3.2846465 -3.303894 -3.3120513 -3.3111603 -3.2665958 -3.139663 -2.9403481]]...]
INFO - root - 2017-12-16 08:37:57.824951: step 17210, loss = 0.63, batch loss = 0.37 (46.2 examples/sec; 0.173 sec/batch; 15h:09m:26s remains)
INFO - root - 2017-12-16 08:37:59.489321: step 17220, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:30m:06s remains)
INFO - root - 2017-12-16 08:38:01.144502: step 17230, loss = 0.62, batch loss = 0.36 (49.4 examples/sec; 0.162 sec/batch; 14h:11m:19s remains)
INFO - root - 2017-12-16 08:38:02.812523: step 17240, loss = 0.50, batch loss = 0.24 (48.8 examples/sec; 0.164 sec/batch; 14h:20m:45s remains)
INFO - root - 2017-12-16 08:38:04.482434: step 17250, loss = 0.70, batch loss = 0.44 (49.0 examples/sec; 0.163 sec/batch; 14h:18m:08s remains)
INFO - root - 2017-12-16 08:38:06.138822: step 17260, loss = 0.46, batch loss = 0.20 (48.8 examples/sec; 0.164 sec/batch; 14h:22m:01s remains)
INFO - root - 2017-12-16 08:38:07.850019: step 17270, loss = 0.62, batch loss = 0.36 (44.1 examples/sec; 0.182 sec/batch; 15h:54m:07s remains)
INFO - root - 2017-12-16 08:38:09.522583: step 17280, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.166 sec/batch; 14h:30m:53s remains)
INFO - root - 2017-12-16 08:38:11.181752: step 17290, loss = 0.70, batch loss = 0.44 (48.8 examples/sec; 0.164 sec/batch; 14h:21m:21s remains)
INFO - root - 2017-12-16 08:38:12.870829: step 17300, loss = 0.59, batch loss = 0.33 (49.1 examples/sec; 0.163 sec/batch; 14h:16m:46s remains)
2017-12-16 08:38:13.316575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.58097 -1.8219414 -2.0054944 -2.0224752 -1.9269168 -1.803158 -1.6873019 -1.5890791 -1.529606 -1.5026941 -1.461267 -1.2274708 -0.88556957 -0.84517109 -1.1872452][-1.629895 -1.9717379 -2.263454 -2.333791 -2.2566388 -2.1131926 -1.9455065 -1.8199091 -1.7673728 -1.7597401 -1.725596 -1.3882434 -0.890046 -0.77706289 -1.1370509][-1.8151965 -2.272491 -2.6293676 -2.7467744 -2.7087059 -2.5804217 -2.3902767 -2.2384827 -2.2451138 -2.3088975 -2.3447757 -2.020875 -1.4273266 -1.2116553 -1.4693389][-1.6689632 -2.2279437 -2.694123 -2.8424733 -2.8500168 -2.7313793 -2.4317698 -2.1868126 -2.2741208 -2.4797363 -2.6999493 -2.5592895 -2.0341713 -1.8050331 -1.9421966][-0.82695329 -1.4587533 -2.0499854 -2.2846379 -2.2996106 -2.0794885 -1.5682038 -1.2021787 -1.3813614 -1.8258152 -2.3267939 -2.5557356 -2.3191843 -2.2273836 -2.3412495][0.62279725 0.055178165 -0.60328925 -0.86584616 -0.83860242 -0.45702589 0.24682713 0.69129896 0.40010691 -0.2928977 -1.1253077 -1.7504668 -1.9116837 -2.076772 -2.3304336][1.983176 1.6519449 1.1334178 0.92832828 1.0176141 1.611583 2.6057851 3.1086571 2.6752388 1.7641709 0.636271 -0.32716131 -0.77314007 -1.1860123 -1.6981686][2.5433714 2.3823454 2.1349118 2.0797741 2.2408035 3.0602691 4.253108 4.7942343 4.29177 3.2726514 1.9738882 0.84111071 0.25231004 -0.26896429 -0.99341238][1.7041719 1.6527002 1.575151 1.5844748 1.7867672 2.5418589 3.5709593 3.97416 3.5055983 2.6562016 1.5894375 0.6790905 0.24400687 -0.13059425 -0.81317651][-0.050608158 -0.095277309 -0.090289831 -0.032840014 0.14996934 0.70451474 1.4453835 1.7800415 1.4738774 0.91043067 0.23045921 -0.32872534 -0.49175489 -0.65783918 -1.1641353][-1.8794458 -2.03634 -2.0787697 -1.9882756 -1.8035388 -1.4061956 -0.96926105 -0.74732471 -0.92729175 -1.2292968 -1.5810001 -1.8647649 -1.7732115 -1.6683493 -1.8810644][-2.8683977 -3.1295125 -3.24105 -3.1867414 -3.0536678 -2.8134911 -2.5924332 -2.4943569 -2.5754528 -2.7028754 -2.8636763 -2.927964 -2.6405888 -2.3828237 -2.4859056][-2.7050931 -3.1227407 -3.3718848 -3.3833203 -3.289896 -3.1264277 -2.9995422 -2.9324298 -2.9670546 -3.0238373 -3.0934403 -3.0191197 -2.6185694 -2.3706126 -2.5337534][-1.6619208 -2.181848 -2.57159 -2.6692173 -2.6234276 -2.5338795 -2.468118 -2.4321501 -2.4447582 -2.4875641 -2.5020263 -2.2831881 -1.8116767 -1.6410704 -1.9916093][-0.5976367 -1.1050442 -1.5083444 -1.6339718 -1.6336079 -1.6037657 -1.5785515 -1.5667907 -1.5828431 -1.610406 -1.55511 -1.174624 -0.61267054 -0.52695429 -1.0646138]]...]
INFO - root - 2017-12-16 08:38:14.995303: step 17310, loss = 0.50, batch loss = 0.24 (49.5 examples/sec; 0.162 sec/batch; 14h:08m:44s remains)
INFO - root - 2017-12-16 08:38:16.687713: step 17320, loss = 0.59, batch loss = 0.33 (49.0 examples/sec; 0.163 sec/batch; 14h:17m:55s remains)
INFO - root - 2017-12-16 08:38:18.379115: step 17330, loss = 0.56, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 14h:38m:51s remains)
INFO - root - 2017-12-16 08:38:20.044481: step 17340, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 14h:36m:58s remains)
INFO - root - 2017-12-16 08:38:21.725955: step 17350, loss = 0.65, batch loss = 0.39 (48.1 examples/sec; 0.166 sec/batch; 14h:33m:35s remains)
INFO - root - 2017-12-16 08:38:23.388609: step 17360, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 14h:21m:57s remains)
INFO - root - 2017-12-16 08:38:25.054587: step 17370, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 14h:44m:55s remains)
INFO - root - 2017-12-16 08:38:26.742678: step 17380, loss = 0.61, batch loss = 0.35 (48.8 examples/sec; 0.164 sec/batch; 14h:20m:36s remains)
INFO - root - 2017-12-16 08:38:28.457398: step 17390, loss = 0.56, batch loss = 0.30 (46.6 examples/sec; 0.172 sec/batch; 15h:01m:25s remains)
INFO - root - 2017-12-16 08:38:30.137848: step 17400, loss = 0.79, batch loss = 0.53 (46.6 examples/sec; 0.172 sec/batch; 15h:02m:30s remains)
2017-12-16 08:38:30.615547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1251888 -1.9218928 -1.7298858 -1.6089072 -1.5264877 -1.5866815 -1.7529905 -1.8678722 -1.8723885 -1.8778437 -1.922387 -1.9365041 -1.9168521 -1.9236685 -2.0115039][-1.7196248 -1.3705158 -1.081857 -0.9566139 -0.92294526 -1.0949168 -1.431325 -1.7012814 -1.8067707 -1.8926079 -1.9771683 -2.0061371 -1.9815849 -1.9619763 -2.0201313][-1.4110587 -0.96871948 -0.58574235 -0.41714036 -0.41637707 -0.66427326 -1.1292356 -1.5015736 -1.6799331 -1.8270972 -1.9379227 -1.9758077 -1.9636722 -1.9428378 -1.9820092][-1.3945289 -0.91467381 -0.45660663 -0.20041203 -0.11041999 -0.28230023 -0.76261842 -1.2190803 -1.4710622 -1.6454167 -1.7220018 -1.7215961 -1.6864922 -1.6714878 -1.7005547][-1.5796423 -1.1163191 -0.61234725 -0.24964952 0.0029830933 -0.0015761852 -0.40064359 -0.87476838 -1.1956754 -1.4310205 -1.5086234 -1.4417152 -1.3271929 -1.2315235 -1.164807][-1.7247074 -1.2018067 -0.65929806 -0.21637678 0.19013071 0.34339023 0.050383806 -0.39070344 -0.76243782 -1.0743951 -1.2148885 -1.1543401 -1.0289875 -0.90606666 -0.790354][-1.7097938 -1.1273887 -0.58365142 -0.13854122 0.31268 0.49311495 0.25839806 -0.099623919 -0.39962816 -0.70526314 -0.88619781 -0.862501 -0.80501723 -0.74207497 -0.66797936][-1.7958153 -1.229795 -0.71687472 -0.32588696 0.058062077 0.20230484 -0.0063385963 -0.2296083 -0.37744117 -0.58240891 -0.67708933 -0.61313677 -0.59860408 -0.65848267 -0.70740235][-1.7498469 -1.1963428 -0.75412941 -0.48173177 -0.25107217 -0.23541021 -0.52018821 -0.70602107 -0.7412951 -0.8167696 -0.74137676 -0.51445568 -0.43591118 -0.56111133 -0.72197318][-1.3334225 -0.82749259 -0.5373708 -0.53351319 -0.64315009 -0.91998637 -1.3713671 -1.6011738 -1.5958862 -1.5371273 -1.2868522 -0.85348904 -0.64346361 -0.72688305 -0.88507032][-0.68219638 -0.28140569 -0.21822929 -0.52519965 -0.990407 -1.5735478 -2.1900058 -2.4882059 -2.5025516 -2.4294448 -2.1384015 -1.6426991 -1.3511016 -1.3515215 -1.411077][-0.074894905 0.22021675 0.13057518 -0.34257269 -1.0275054 -1.8180218 -2.5818162 -3.0237205 -3.196677 -3.2574675 -3.0491736 -2.6322608 -2.3665769 -2.2995787 -2.2547045][0.14873242 0.33116555 0.1735692 -0.27605939 -0.91087377 -1.6653935 -2.4547706 -3.0225985 -3.3912234 -3.602777 -3.5128756 -3.2093606 -2.9774845 -2.8855231 -2.7971098][0.15723658 0.27153254 0.1045537 -0.23057652 -0.63012421 -1.1503676 -1.791012 -2.3155422 -2.7407358 -3.0162311 -2.9948363 -2.7932947 -2.6203694 -2.5252173 -2.451375][0.23462844 0.20773625 -0.054233313 -0.38347387 -0.63277709 -0.88626337 -1.2003369 -1.447984 -1.6685065 -1.8154483 -1.7430459 -1.6035523 -1.4952424 -1.426965 -1.3745097]]...]
INFO - root - 2017-12-16 08:38:32.371551: step 17410, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:48m:41s remains)
INFO - root - 2017-12-16 08:38:34.086530: step 17420, loss = 0.62, batch loss = 0.36 (45.2 examples/sec; 0.177 sec/batch; 15h:29m:33s remains)
INFO - root - 2017-12-16 08:38:35.777620: step 17430, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.169 sec/batch; 14h:45m:19s remains)
INFO - root - 2017-12-16 08:38:37.472212: step 17440, loss = 0.62, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 14h:35m:43s remains)
INFO - root - 2017-12-16 08:38:39.151780: step 17450, loss = 0.66, batch loss = 0.40 (46.1 examples/sec; 0.173 sec/batch; 15h:10m:41s remains)
INFO - root - 2017-12-16 08:38:40.841363: step 17460, loss = 0.68, batch loss = 0.43 (45.6 examples/sec; 0.176 sec/batch; 15h:21m:54s remains)
INFO - root - 2017-12-16 08:38:42.496056: step 17470, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 14h:23m:45s remains)
INFO - root - 2017-12-16 08:38:44.126038: step 17480, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:20m:43s remains)
INFO - root - 2017-12-16 08:38:45.795224: step 17490, loss = 0.51, batch loss = 0.25 (46.5 examples/sec; 0.172 sec/batch; 15h:02m:38s remains)
INFO - root - 2017-12-16 08:38:47.493509: step 17500, loss = 0.59, batch loss = 0.33 (47.2 examples/sec; 0.169 sec/batch; 14h:48m:54s remains)
2017-12-16 08:38:48.013356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4148192 -2.6020839 -2.7921393 -2.7892814 -2.6505938 -2.3682883 -2.0177143 -1.7858186 -1.9751434 -2.6235459 -3.4548612 -4.0765295 -4.1737156 -3.5963006 -2.6627245][-2.179677 -2.4654663 -2.714669 -2.7650502 -2.5303087 -1.9970223 -1.4638572 -1.0859369 -1.1905704 -1.9339391 -2.92703 -3.7735209 -4.0739269 -3.5904899 -2.6674502][-2.0893023 -2.4412205 -2.7387223 -2.7535381 -2.3290462 -1.5010092 -0.67496812 -0.12610054 -0.1811564 -1.0937423 -2.2864702 -3.358211 -3.9169846 -3.640439 -2.7699103][-2.1823146 -2.50411 -2.710067 -2.5315776 -1.8156215 -0.64474535 0.56603026 1.2316041 1.0579569 -0.089886189 -1.5591455 -2.9078834 -3.7355886 -3.7211785 -2.9812839][-2.081789 -2.3817396 -2.4762523 -2.0697284 -1.0547158 0.55144525 2.170866 2.955699 2.5302758 1.0432725 -0.73625588 -2.3941078 -3.4706769 -3.7212057 -3.1426282][-1.5922375 -1.903343 -1.930768 -1.3862598 -0.13310266 1.8674989 3.9009967 4.8644552 4.1969008 2.3029075 0.074192286 -1.9249344 -3.2496853 -3.6669803 -3.1913402][-1.0531631 -1.4207661 -1.4396273 -0.89761508 0.47585583 2.6607046 5.0634322 6.4334559 5.6005335 3.3440547 0.7404654 -1.5847893 -3.1217394 -3.5774591 -3.1661332][-0.97009397 -1.4545608 -1.5298055 -1.0715247 0.23287678 2.2736931 4.7495289 6.4637771 5.7459483 3.5071678 0.894269 -1.4783239 -3.0820484 -3.5304012 -3.0999372][-1.6673491 -2.2333109 -2.3919525 -2.0186841 -0.90249693 0.7703135 2.8773417 4.5633254 4.3655839 2.7347097 0.56366992 -1.5269864 -3.0431798 -3.5508857 -3.0899429][-2.6267302 -3.1053853 -3.3014979 -3.0343466 -2.1236968 -0.78473949 0.90964746 2.5295224 2.8090749 1.8138661 0.17737007 -1.5660553 -2.9884052 -3.530827 -3.1004548][-3.6306915 -3.9126239 -4.0254011 -3.7964449 -3.0414732 -1.9170606 -0.53592587 0.952646 1.5032496 0.9671185 -0.2165947 -1.5972385 -2.8557162 -3.387352 -3.0407462][-4.4170847 -4.5377178 -4.5695653 -4.3503146 -3.7907135 -2.9452119 -1.8926795 -0.64703226 -0.032919407 -0.2372911 -0.8685106 -1.7385321 -2.6903119 -3.1839509 -2.9598944][-4.6053591 -4.6083364 -4.5874271 -4.4236555 -4.1257329 -3.7192287 -3.1129453 -2.1984141 -1.6317158 -1.6679542 -1.8973999 -2.2900648 -2.8536975 -3.2301483 -3.0308363][-4.3000503 -4.2203684 -4.1785355 -4.1550031 -4.1788063 -4.2525368 -4.1548767 -3.5969176 -3.1537333 -3.0891085 -3.0774391 -3.1487694 -3.3545115 -3.4978683 -3.2306097][-3.7435446 -3.6418815 -3.6605272 -3.8720884 -4.22507 -4.726264 -5.0683732 -4.913085 -4.6274414 -4.4917116 -4.3272028 -4.1113191 -4.027348 -3.8834062 -3.46526]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-17500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:38:50.350169: step 17510, loss = 0.48, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 14h:33m:44s remains)
INFO - root - 2017-12-16 08:38:52.003002: step 17520, loss = 0.53, batch loss = 0.27 (49.1 examples/sec; 0.163 sec/batch; 14h:15m:41s remains)
INFO - root - 2017-12-16 08:38:53.661907: step 17530, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 14h:42m:40s remains)
INFO - root - 2017-12-16 08:38:55.345315: step 17540, loss = 0.70, batch loss = 0.45 (46.6 examples/sec; 0.172 sec/batch; 15h:01m:29s remains)
INFO - root - 2017-12-16 08:38:57.025755: step 17550, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:36m:07s remains)
INFO - root - 2017-12-16 08:38:58.709197: step 17560, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 14h:32m:06s remains)
INFO - root - 2017-12-16 08:39:00.403015: step 17570, loss = 0.48, batch loss = 0.22 (47.6 examples/sec; 0.168 sec/batch; 14h:42m:37s remains)
INFO - root - 2017-12-16 08:39:02.088937: step 17580, loss = 0.52, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:29m:23s remains)
INFO - root - 2017-12-16 08:39:03.753732: step 17590, loss = 0.61, batch loss = 0.35 (49.4 examples/sec; 0.162 sec/batch; 14h:09m:53s remains)
INFO - root - 2017-12-16 08:39:05.413241: step 17600, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 14h:13m:38s remains)
2017-12-16 08:39:05.895176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4430914 -3.026736 -2.4007766 -1.9570475 -1.9588829 -2.0734119 -2.0644934 -2.076988 -2.1032224 -2.042383 -1.925648 -1.8416457 -1.8143073 -1.7770083 -1.6392136][-3.5125558 -3.1258001 -2.4614367 -1.9756818 -2.002187 -2.1240005 -2.0863779 -2.1087735 -2.1565962 -2.0556366 -1.8679883 -1.7252729 -1.6892223 -1.6797736 -1.5428658][-3.462677 -3.0782278 -2.3524833 -1.8305591 -1.8862683 -2.0311222 -2.0124216 -2.0840054 -2.2015254 -2.1165104 -1.8829143 -1.6695161 -1.6083343 -1.6159239 -1.4537075][-2.9884326 -2.5461292 -1.799166 -1.2605014 -1.3380933 -1.5008833 -1.519865 -1.6492538 -1.8297822 -1.8082345 -1.5733353 -1.3711069 -1.3812523 -1.4964291 -1.4081165][-2.0936685 -1.5445445 -0.70036077 -0.083202124 -0.083064795 -0.24345732 -0.25580597 -0.42328775 -0.71216679 -0.82652617 -0.68806267 -0.56983531 -0.72736704 -1.0320879 -1.0954878][-1.2566198 -0.53381515 0.50561142 1.3682902 1.6702266 1.6754827 1.6979904 1.5496838 1.2120476 0.97046947 0.96102118 0.9436276 0.6085372 0.068215132 -0.19573832][-0.84877419 0.068487406 1.289865 2.4670548 3.244236 3.597043 3.7711267 3.6599808 3.1746912 2.6116819 2.3986363 2.323606 1.9088554 1.2313576 0.79485559][-1.0431697 -0.05244875 1.211359 2.5495028 3.6985598 4.4818544 5.00207 5.1851716 4.5814662 3.6382194 3.1214685 2.8478646 2.2340093 1.4060605 0.80621886][-1.6963921 -0.8498919 0.22100568 1.3744221 2.3786335 3.183773 3.7288384 3.8433304 3.3064613 2.4388094 1.9042525 1.6367762 1.1722305 0.59600067 0.13282132][-2.5094485 -1.9635856 -1.2532007 -0.49465621 0.19194007 0.8297224 1.2817781 1.3216925 0.91912937 0.26887226 -0.13713455 -0.31739569 -0.55306017 -0.7654314 -0.88776159][-2.9949107 -2.7435408 -2.3902669 -2.0508504 -1.7475221 -1.3984127 -1.1114509 -1.0928043 -1.3464276 -1.7568507 -2.0298204 -2.1420171 -2.1608415 -2.0571034 -1.8432086][-3.132638 -3.0835087 -2.9294262 -2.8049474 -2.7772059 -2.734134 -2.6602223 -2.6708941 -2.7814727 -2.9642425 -3.0929563 -3.073066 -2.929605 -2.6017861 -2.1682467][-2.9219742 -2.9888299 -2.9483624 -2.8912308 -3.0165906 -3.1558738 -3.1571164 -3.1007547 -3.0524588 -3.0337133 -2.9850659 -2.8450754 -2.6126649 -2.1872609 -1.6729039][-2.5984809 -2.6319759 -2.46842 -2.2719469 -2.3593636 -2.56242 -2.6038067 -2.5227337 -2.3998342 -2.2785804 -2.1594169 -1.9834511 -1.7728649 -1.3996023 -0.95806384][-2.3960876 -2.297091 -1.9107831 -1.464299 -1.4108266 -1.6187587 -1.7087004 -1.6559014 -1.5474924 -1.4347732 -1.3533139 -1.2750857 -1.1851252 -0.95198333 -0.68145692]]...]
INFO - root - 2017-12-16 08:39:07.553097: step 17610, loss = 0.65, batch loss = 0.39 (46.7 examples/sec; 0.171 sec/batch; 14h:59m:27s remains)
INFO - root - 2017-12-16 08:39:09.244843: step 17620, loss = 0.50, batch loss = 0.24 (47.2 examples/sec; 0.169 sec/batch; 14h:49m:15s remains)
INFO - root - 2017-12-16 08:39:10.904967: step 17630, loss = 0.60, batch loss = 0.34 (49.4 examples/sec; 0.162 sec/batch; 14h:09m:47s remains)
INFO - root - 2017-12-16 08:39:12.562025: step 17640, loss = 0.60, batch loss = 0.34 (49.3 examples/sec; 0.162 sec/batch; 14h:11m:54s remains)
INFO - root - 2017-12-16 08:39:14.257056: step 17650, loss = 0.52, batch loss = 0.26 (46.7 examples/sec; 0.171 sec/batch; 14h:58m:56s remains)
INFO - root - 2017-12-16 08:39:15.923835: step 17660, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 14h:21m:37s remains)
INFO - root - 2017-12-16 08:39:17.588672: step 17670, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:31m:03s remains)
INFO - root - 2017-12-16 08:39:19.265940: step 17680, loss = 0.52, batch loss = 0.26 (45.1 examples/sec; 0.178 sec/batch; 15h:31m:28s remains)
INFO - root - 2017-12-16 08:39:20.941935: step 17690, loss = 0.69, batch loss = 0.43 (48.9 examples/sec; 0.164 sec/batch; 14h:18m:11s remains)
INFO - root - 2017-12-16 08:39:22.586138: step 17700, loss = 0.55, batch loss = 0.29 (49.1 examples/sec; 0.163 sec/batch; 14h:15m:05s remains)
2017-12-16 08:39:23.084727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4161433 -1.6237953 -1.784842 -1.8380206 -1.6834422 -1.324133 -0.92878127 -0.73475444 -0.80006266 -1.1038421 -1.5283631 -1.8722886 -1.9812804 -1.86826 -1.6982944][-1.4328747 -1.5349333 -1.6786013 -1.8179976 -1.7856178 -1.5172663 -1.1414726 -0.90763843 -0.89768016 -1.1536766 -1.628562 -2.0930688 -2.2846406 -2.2233291 -2.1119022][-1.3253294 -1.3085597 -1.4328012 -1.6826414 -1.8244091 -1.7183416 -1.436877 -1.1965996 -1.0916039 -1.2326512 -1.6767747 -2.1885579 -2.421793 -2.3926394 -2.3256497][-1.192337 -1.113464 -1.248403 -1.5987794 -1.8984218 -1.9534013 -1.7799027 -1.5230865 -1.278128 -1.2513807 -1.5904262 -2.0721014 -2.2997983 -2.2624679 -2.1820982][-1.2329834 -1.1583091 -1.2992523 -1.6531794 -1.9785838 -2.08557 -1.9443504 -1.6294553 -1.2373888 -1.0683223 -1.3236866 -1.7991664 -2.055346 -2.0055661 -1.8465093][-1.5163827 -1.4506077 -1.5156996 -1.7110817 -1.8719238 -1.8824518 -1.676132 -1.2643722 -0.76777303 -0.55614364 -0.83747053 -1.4018492 -1.7954247 -1.8303828 -1.6149271][-1.7998016 -1.7223246 -1.6356516 -1.549758 -1.4074984 -1.2160778 -0.87983727 -0.36139965 0.18455029 0.35754895 -0.039473534 -0.77345669 -1.4061949 -1.6505371 -1.4910712][-1.8544629 -1.7450917 -1.4869361 -1.1294997 -0.71292436 -0.33991098 0.10168624 0.67365837 1.198874 1.2839723 0.77624083 -0.078770161 -0.92718124 -1.4277298 -1.433838][-1.6799597 -1.5516217 -1.2171054 -0.75613272 -0.26689005 0.13120294 0.56551194 1.1002219 1.5442841 1.5815301 1.117897 0.29969549 -0.62488878 -1.2965057 -1.5176339][-1.4647899 -1.3732102 -1.1022245 -0.73977423 -0.40320468 -0.14453173 0.164361 0.59425187 0.95970511 1.0644696 0.81110168 0.22271514 -0.54915237 -1.1911656 -1.5259066][-1.3653648 -1.3243246 -1.1842433 -1.0309074 -0.92264307 -0.84791589 -0.71060658 -0.44521058 -0.14681435 0.042139292 0.041354179 -0.19436908 -0.65153277 -1.0930082 -1.3631278][-1.3604324 -1.3695716 -1.3657224 -1.4033694 -1.4749966 -1.547224 -1.5644195 -1.4551036 -1.240538 -1.0236799 -0.87015712 -0.842837 -1.0042437 -1.1979423 -1.3212186][-1.4556334 -1.4960761 -1.5619719 -1.6775162 -1.8218703 -1.9560322 -2.0466716 -2.0394382 -1.9147756 -1.7380297 -1.5756748 -1.4804338 -1.5128746 -1.5910022 -1.6256609][-1.63961 -1.6465641 -1.6831087 -1.7527609 -1.8551433 -1.9629424 -2.0584543 -2.1132739 -2.0894964 -2.0156169 -1.9498237 -1.9216573 -1.9702888 -2.0314236 -2.0322096][-1.7774242 -1.7156762 -1.6659439 -1.6343901 -1.6477404 -1.698843 -1.7886997 -1.8949096 -1.9728451 -2.0265117 -2.081181 -2.1548312 -2.2447984 -2.307591 -2.2972293]]...]
INFO - root - 2017-12-16 08:39:24.746179: step 17710, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:27m:52s remains)
INFO - root - 2017-12-16 08:39:26.418713: step 17720, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:32m:48s remains)
INFO - root - 2017-12-16 08:39:28.077486: step 17730, loss = 0.57, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 14h:30m:15s remains)
INFO - root - 2017-12-16 08:39:29.753045: step 17740, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:46m:19s remains)
INFO - root - 2017-12-16 08:39:31.386006: step 17750, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:32m:24s remains)
INFO - root - 2017-12-16 08:39:33.063223: step 17760, loss = 0.60, batch loss = 0.34 (47.3 examples/sec; 0.169 sec/batch; 14h:48m:05s remains)
INFO - root - 2017-12-16 08:39:34.725980: step 17770, loss = 0.49, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 14h:40m:53s remains)
INFO - root - 2017-12-16 08:39:36.366557: step 17780, loss = 0.64, batch loss = 0.38 (49.2 examples/sec; 0.163 sec/batch; 14h:13m:35s remains)
INFO - root - 2017-12-16 08:39:38.029470: step 17790, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:46m:27s remains)
INFO - root - 2017-12-16 08:39:39.727011: step 17800, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:37m:30s remains)
2017-12-16 08:39:40.178481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.98250723 -0.69872403 -0.51131129 -0.49494135 -0.37534142 0.10728216 0.58783865 0.66467714 0.29596639 -0.51010191 -1.4470598 -2.1458223 -2.320385 -2.11411 -1.8933442][0.40303564 0.6265955 0.59255648 0.23569107 -0.067233562 0.10800648 0.42850924 0.38627791 -0.021096945 -0.78620553 -1.6704383 -2.3462551 -2.4770954 -2.2057528 -1.8983215][1.1367474 1.22049 0.98479891 0.30790615 -0.41212106 -0.56818748 -0.37205243 -0.45893526 -0.859203 -1.5215071 -2.227639 -2.7533305 -2.7528405 -2.3642449 -1.9386406][0.81038952 0.6866498 0.37083507 -0.36436021 -1.1381855 -1.3404258 -1.0680788 -1.0258549 -1.3571978 -2.025826 -2.7349367 -3.178689 -3.049304 -2.5047276 -1.9803362][-0.22230148 -0.39062667 -0.61803555 -1.1140463 -1.4893882 -1.3252709 -0.70226312 -0.36484337 -0.62553072 -1.5720943 -2.6299648 -3.2370787 -3.1125128 -2.5178089 -1.9739729][-1.2044988 -1.2830762 -1.3188537 -1.4251547 -1.2625027 -0.51014638 0.66905761 1.475852 1.361692 -0.0089893341 -1.7047145 -2.7846904 -2.8692393 -2.3749228 -1.8985295][-1.9297013 -1.8357546 -1.6528461 -1.3354725 -0.62699103 0.58922195 2.1601388 3.4571517 3.7173922 2.0922391 -0.22514534 -1.8789785 -2.3301671 -2.0732653 -1.7202001][-2.3917832 -2.0778303 -1.679168 -1.0519922 -0.058625937 1.4347854 3.1804669 4.619401 5.208313 3.7468412 1.1186545 -0.919045 -1.6932874 -1.6861334 -1.4848385][-3.0436764 -2.61614 -2.1190073 -1.4568838 -0.43134141 1.0976582 2.6476943 3.8825657 4.3582306 3.4183915 1.3306298 -0.52067089 -1.2681382 -1.3433055 -1.2584996][-3.776288 -3.4029229 -2.9926977 -2.5296776 -1.7248359 -0.41293037 0.77288866 1.5209351 1.7621357 1.2671773 0.12376952 -1.0601069 -1.434727 -1.3134568 -1.1804917][-4.2821407 -4.06262 -3.8865728 -3.7856603 -3.3636668 -2.372647 -1.4730988 -1.1018369 -1.029816 -1.2320911 -1.7231692 -2.250536 -2.2062969 -1.7811553 -1.3904169][-4.4022527 -4.3981266 -4.4971795 -4.7207613 -4.7085857 -4.1158409 -3.5520153 -3.4138994 -3.3695002 -3.3316777 -3.4039423 -3.4743276 -3.1236405 -2.4526896 -1.7501116][-4.3831415 -4.5267487 -4.7396507 -5.1175723 -5.319931 -5.1326456 -4.9010715 -4.9520698 -4.9081268 -4.6665039 -4.3850164 -4.1179118 -3.5771723 -2.7799971 -1.9526933][-4.0287628 -4.1870193 -4.4021568 -4.7286649 -4.9750037 -4.9886742 -4.9608221 -5.0865355 -5.0874205 -4.8636885 -4.4779949 -3.9969492 -3.380831 -2.6316311 -1.8798448][-3.198441 -3.3139689 -3.4384813 -3.6169291 -3.7943425 -3.8748875 -3.9312563 -4.073534 -4.165843 -4.0872059 -3.8107853 -3.3557835 -2.7958531 -2.1791434 -1.6246989]]...]
INFO - root - 2017-12-16 08:39:41.843511: step 17810, loss = 0.58, batch loss = 0.32 (48.8 examples/sec; 0.164 sec/batch; 14h:20m:00s remains)
INFO - root - 2017-12-16 08:39:43.489057: step 17820, loss = 0.66, batch loss = 0.40 (46.8 examples/sec; 0.171 sec/batch; 14h:55m:34s remains)
INFO - root - 2017-12-16 08:39:45.180769: step 17830, loss = 0.54, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 14h:40m:23s remains)
INFO - root - 2017-12-16 08:39:46.846624: step 17840, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 14h:21m:28s remains)
INFO - root - 2017-12-16 08:39:48.526770: step 17850, loss = 0.44, batch loss = 0.18 (47.5 examples/sec; 0.169 sec/batch; 14h:43m:59s remains)
INFO - root - 2017-12-16 08:39:50.206023: step 17860, loss = 0.60, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 14h:34m:46s remains)
INFO - root - 2017-12-16 08:39:51.884652: step 17870, loss = 0.66, batch loss = 0.40 (47.5 examples/sec; 0.168 sec/batch; 14h:42m:28s remains)
INFO - root - 2017-12-16 08:39:53.535573: step 17880, loss = 0.66, batch loss = 0.40 (47.8 examples/sec; 0.167 sec/batch; 14h:38m:04s remains)
INFO - root - 2017-12-16 08:39:55.227172: step 17890, loss = 0.64, batch loss = 0.39 (48.3 examples/sec; 0.166 sec/batch; 14h:28m:48s remains)
INFO - root - 2017-12-16 08:39:56.900800: step 17900, loss = 0.60, batch loss = 0.34 (46.2 examples/sec; 0.173 sec/batch; 15h:07m:02s remains)
2017-12-16 08:39:57.421688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.478253 -1.6390638 -1.5463824 -1.2536244 -1.1322589 -1.3513658 -1.7175543 -1.9517868 -1.8411658 -1.6349163 -1.40921 -1.2964646 -1.2948315 -1.313024 -1.2698495][-1.6229026 -1.7550192 -1.6399056 -1.3047959 -1.1014328 -1.2582055 -1.6331379 -2.022737 -2.1455243 -2.1446919 -2.007406 -1.8482747 -1.6818687 -1.5112443 -1.3634529][-1.7164099 -1.8730024 -1.7677425 -1.3613658 -1.0562658 -1.0344503 -1.2714483 -1.7068571 -2.0366516 -2.239157 -2.268364 -2.1766467 -1.9507067 -1.6629133 -1.4773862][-1.8073461 -1.9308772 -1.7306523 -1.1932539 -0.65304041 -0.31938338 -0.31552148 -0.80464315 -1.3856773 -1.837931 -2.0768087 -2.1000156 -1.8657377 -1.5554056 -1.4186761][-1.6204991 -1.6703579 -1.2719266 -0.500671 0.38443542 1.1345463 1.3175037 0.60703611 -0.34346914 -1.111513 -1.5866716 -1.6932969 -1.4344089 -1.0981281 -1.0312577][-1.1101729 -1.1250939 -0.60614824 0.40593505 1.6139059 2.7446032 3.1161413 2.093431 0.7619698 -0.32316923 -1.0076066 -1.1631308 -0.85836315 -0.47981346 -0.44456315][-0.75913811 -0.902714 -0.44121575 0.57008433 1.8332934 3.1054893 3.7508826 2.8182435 1.4443946 0.25675774 -0.57248545 -0.82870758 -0.50201952 -0.072173119 -0.050619364][-0.98846364 -1.338444 -1.0779346 -0.34677052 0.63206148 1.7013674 2.5095172 2.1593375 1.2550788 0.33124352 -0.4523319 -0.73886168 -0.43596315 0.0007545948 0.088584185][-1.4653558 -1.9844257 -1.9910314 -1.6780183 -1.137376 -0.39752483 0.39611268 0.50607944 0.16133118 -0.36288559 -0.84177721 -0.93975306 -0.55765843 -0.064100266 0.11186695][-1.682636 -2.3135231 -2.6344969 -2.7099271 -2.471355 -1.9689986 -1.2902733 -1.0711889 -1.1553254 -1.3621681 -1.4521465 -1.1873245 -0.57787657 0.0078392029 0.17461276][-1.3710871 -2.0485277 -2.605613 -2.8654618 -2.7647629 -2.3946028 -1.9103636 -1.8181615 -1.8804914 -1.8823721 -1.6942368 -1.1700487 -0.42652059 0.14603853 0.22436762][-0.7790947 -1.3528163 -1.9082661 -2.1376505 -2.0760489 -1.8907531 -1.6478479 -1.662662 -1.7475742 -1.6361661 -1.2802678 -0.75167489 -0.22214079 0.11471748 0.054106712][-0.27693439 -0.62350988 -0.97789049 -1.0241032 -0.92380583 -0.91618764 -0.87400723 -0.93129528 -1.0088944 -0.79693055 -0.37216771 -0.012582779 0.0664165 0.014539003 -0.27601981][-0.20123291 -0.33716631 -0.45589554 -0.26941609 -0.052708864 -0.057469845 -0.075183868 -0.14913034 -0.18999386 0.080321074 0.57845783 0.77008533 0.47506237 0.12322021 -0.37536335][-0.70490539 -0.75930512 -0.64800191 -0.23541021 0.081754684 0.1589663 0.19706154 0.17787886 0.16629457 0.45273185 0.9678514 1.177671 0.8017633 0.36817217 -0.19333577]]...]
INFO - root - 2017-12-16 08:39:59.093393: step 17910, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:17m:57s remains)
INFO - root - 2017-12-16 08:40:00.736767: step 17920, loss = 0.56, batch loss = 0.30 (50.0 examples/sec; 0.160 sec/batch; 13h:58m:24s remains)
INFO - root - 2017-12-16 08:40:02.411351: step 17930, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 15h:03m:54s remains)
INFO - root - 2017-12-16 08:40:04.126895: step 17940, loss = 0.68, batch loss = 0.43 (46.6 examples/sec; 0.172 sec/batch; 15h:00m:53s remains)
INFO - root - 2017-12-16 08:40:05.828751: step 17950, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 15h:07m:26s remains)
INFO - root - 2017-12-16 08:40:07.527766: step 17960, loss = 0.62, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 14h:39m:01s remains)
INFO - root - 2017-12-16 08:40:09.206512: step 17970, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:46m:07s remains)
INFO - root - 2017-12-16 08:40:10.921658: step 17980, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.169 sec/batch; 14h:43m:24s remains)
INFO - root - 2017-12-16 08:40:12.600956: step 17990, loss = 0.55, batch loss = 0.29 (46.7 examples/sec; 0.171 sec/batch; 14h:58m:40s remains)
INFO - root - 2017-12-16 08:40:14.263616: step 18000, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 14h:24m:45s remains)
2017-12-16 08:40:14.757895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2236049 -1.8285443 -1.4783638 -1.3121459 -1.1205568 -0.98784888 -1.0309483 -1.1462518 -1.1322362 -0.83360219 -0.4131887 -0.27200842 -0.55993795 -0.9057523 -1.3835092][-3.2266247 -3.00758 -2.7807369 -2.6176944 -2.3779461 -2.1816502 -2.1459081 -2.2128289 -2.1504703 -1.888635 -1.4825027 -1.2785472 -1.4024549 -1.5542231 -1.8204508][-3.9572954 -3.8588233 -3.711803 -3.4796095 -3.1394157 -2.9106686 -2.8763919 -2.9415526 -2.8703823 -2.6745617 -2.3120811 -2.1597297 -2.309449 -2.4076355 -2.5321484][-4.2054553 -4.1153088 -3.9356079 -3.5423703 -2.9224451 -2.4760056 -2.443541 -2.5779009 -2.6468952 -2.6522419 -2.4754174 -2.5033329 -2.840451 -3.078552 -3.1788647][-4.0016837 -3.8731971 -3.5800004 -2.9004297 -1.7914456 -0.93397748 -0.82633781 -1.0362741 -1.4032273 -1.7811153 -1.9602553 -2.2473812 -2.8251493 -3.388104 -3.6458364][-3.7587991 -3.5213516 -3.01672 -1.9431411 -0.22394514 1.0786033 1.3436525 1.1302092 0.46913576 -0.38137448 -1.0310464 -1.670542 -2.5402143 -3.4190743 -3.8660903][-3.6387696 -3.3539062 -2.6684034 -1.322876 0.68687057 2.3313396 3.018904 3.1455142 2.4431112 1.1562285 0.045599222 -0.93948722 -2.0408895 -3.05307 -3.5840669][-3.5943604 -3.4398489 -2.767468 -1.4991828 0.38941908 2.2079499 3.4398429 4.0644627 3.5908287 2.1522663 0.81742477 -0.30105948 -1.3868551 -2.3365431 -2.8956151][-3.3940778 -3.5574708 -3.1658099 -2.2545013 -0.80877042 0.83638549 2.2311265 3.1190164 2.9105513 1.7931392 0.74437284 -0.11906004 -0.93333972 -1.6869373 -2.2287023][-2.8647656 -3.4234238 -3.5292158 -3.2096605 -2.4084594 -1.2182827 -0.023518562 0.84315562 0.92208505 0.40629578 0.00074744225 -0.38277388 -0.89334631 -1.445654 -1.9170024][-2.0962007 -3.0960414 -3.770061 -4.0430803 -3.8366568 -3.1620455 -2.3167098 -1.5668473 -1.2184429 -1.0492018 -0.72437322 -0.564039 -0.83951771 -1.2477763 -1.6989875][-1.3044695 -2.5474403 -3.6134892 -4.2964482 -4.5016108 -4.2372847 -3.7019308 -3.0931163 -2.5826643 -1.9307828 -1.0902764 -0.58036828 -0.70062137 -1.0565664 -1.4483728][-0.62073231 -1.8092622 -2.9179535 -3.7276082 -4.1362524 -4.1472368 -3.8651252 -3.4355893 -2.9129741 -2.1021504 -1.1040676 -0.52974284 -0.58123863 -0.89711356 -1.251556][-0.23918819 -1.1399375 -2.0233364 -2.6574204 -3.0169148 -3.1195686 -3.0474296 -2.8736811 -2.5349736 -1.8538469 -1.0074124 -0.52211368 -0.57036936 -0.87961113 -1.2362932][-0.3386035 -0.86797428 -1.3982612 -1.7172816 -1.8611494 -1.9072495 -1.9613953 -2.0339353 -1.9372745 -1.5038221 -0.92087448 -0.58320582 -0.66400266 -0.98361611 -1.374505]]...]
INFO - root - 2017-12-16 08:40:16.483440: step 18010, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:35m:22s remains)
INFO - root - 2017-12-16 08:40:18.179529: step 18020, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:45m:25s remains)
INFO - root - 2017-12-16 08:40:19.873885: step 18030, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 14h:32m:07s remains)
INFO - root - 2017-12-16 08:40:21.531180: step 18040, loss = 0.87, batch loss = 0.61 (47.9 examples/sec; 0.167 sec/batch; 14h:36m:13s remains)
INFO - root - 2017-12-16 08:40:23.187508: step 18050, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:28m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:40:24.885171: step 18060, loss = 0.79, batch loss = 0.54 (46.8 examples/sec; 0.171 sec/batch; 14h:55m:36s remains)
INFO - root - 2017-12-16 08:40:26.586125: step 18070, loss = 0.68, batch loss = 0.42 (45.9 examples/sec; 0.174 sec/batch; 15h:13m:33s remains)
INFO - root - 2017-12-16 08:40:28.299107: step 18080, loss = 0.53, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 14h:23m:41s remains)
INFO - root - 2017-12-16 08:40:29.996598: step 18090, loss = 0.50, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 14h:14m:56s remains)
INFO - root - 2017-12-16 08:40:31.709754: step 18100, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 14h:50m:47s remains)
2017-12-16 08:40:32.180139: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051195145 0.088915586 0.050078154 -0.23146629 -0.58211696 -0.7690711 -0.67672324 -0.33548689 -0.061733961 -0.28171062 -1.1472278 -2.2409465 -2.9815824 -3.2206981 -3.1156383][0.99145985 0.93440843 0.71022511 0.1336596 -0.6730032 -1.5078051 -2.1208558 -2.407146 -2.580442 -2.84302 -3.3531032 -3.9049711 -4.127244 -3.9311366 -3.5252426][1.0153477 1.0228794 0.76760888 0.043711424 -1.0270741 -2.217052 -3.2042551 -3.8349125 -4.275404 -4.5699844 -4.803875 -4.8932266 -4.6732039 -4.136342 -3.4760549][-0.28589749 -0.22459769 -0.34348583 -0.86780047 -1.736768 -2.7031527 -3.5367188 -4.1266394 -4.5623045 -4.8069029 -4.8398108 -4.6476941 -4.1769581 -3.4862337 -2.7614782][-2.3017933 -2.1134934 -1.9843649 -1.993819 -2.1218519 -2.3537326 -2.6105819 -2.8873994 -3.2194138 -3.4327545 -3.4103789 -3.1752419 -2.7073467 -2.0770707 -1.477623][-3.8464584 -3.5027473 -2.9792376 -2.2857161 -1.4112008 -0.593042 -0.08605814 -0.049440384 -0.42580771 -0.83988261 -1.0521729 -1.0491123 -0.78613758 -0.38376284 -0.041545868][-4.1171446 -3.7108715 -2.9007401 -1.6226454 0.11929488 1.9710779 3.2246647 3.2851782 2.5014272 1.6209795 0.96418262 0.61150074 0.59550095 0.75611186 0.84850669][-3.2754316 -3.0616908 -2.2760203 -0.7701205 1.3770361 3.8003573 5.4055123 4.9276061 3.5360327 2.2555795 1.3235934 0.80010223 0.66274214 0.73134232 0.70262361][-2.1486909 -2.4077132 -2.0445642 -0.85365427 0.92294836 2.7262077 3.7474051 3.3322415 2.0499148 0.84167004 0.033347607 -0.37326217 -0.45441914 -0.34000564 -0.28170466][-1.2196684 -1.9937207 -2.2848392 -1.8228662 -0.85518277 0.1149497 0.60785627 0.2939024 -0.57034528 -1.3739326 -1.8198174 -1.9264052 -1.7599082 -1.4703641 -1.2131218][-0.50136924 -1.7071176 -2.6492963 -2.9535797 -2.7263269 -2.3834722 -2.2182102 -2.3714285 -2.753727 -3.0602205 -3.10323 -2.8558886 -2.4275739 -1.9957712 -1.6396481][0.023473501 -1.3867356 -2.6745229 -3.4706333 -3.7470622 -3.7459304 -3.6334286 -3.5412948 -3.5018933 -3.4400239 -3.2502277 -2.8638225 -2.3852997 -1.9486896 -1.6218921][0.4573276 -0.87021124 -2.2651963 -3.3037462 -3.7767649 -3.8108635 -3.5670781 -3.2602606 -3.0224431 -2.8171732 -2.6216791 -2.3518164 -2.0370357 -1.744895 -1.5472629][0.762336 -0.32266545 -1.584777 -2.6246319 -3.0945282 -3.0300541 -2.6660249 -2.3341827 -2.1195474 -1.9545598 -1.8859531 -1.8202245 -1.7391136 -1.659727 -1.6323913][1.0620248 0.33243871 -0.61678255 -1.453535 -1.845979 -1.8063622 -1.5683916 -1.4453938 -1.4788724 -1.4830253 -1.51087 -1.5993927 -1.7229362 -1.8375165 -1.9353343]]...]
INFO - root - 2017-12-16 08:40:33.856569: step 18110, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 14h:52m:15s remains)
INFO - root - 2017-12-16 08:40:35.534882: step 18120, loss = 0.55, batch loss = 0.29 (46.4 examples/sec; 0.172 sec/batch; 15h:03m:06s remains)
INFO - root - 2017-12-16 08:40:37.173816: step 18130, loss = 0.56, batch loss = 0.30 (49.5 examples/sec; 0.162 sec/batch; 14h:07m:28s remains)
INFO - root - 2017-12-16 08:40:38.826090: step 18140, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 14h:20m:40s remains)
INFO - root - 2017-12-16 08:40:40.515758: step 18150, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 14h:25m:43s remains)
INFO - root - 2017-12-16 08:40:42.176068: step 18160, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:36m:29s remains)
INFO - root - 2017-12-16 08:40:43.830281: step 18170, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 14h:20m:19s remains)
INFO - root - 2017-12-16 08:40:45.503701: step 18180, loss = 0.50, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 14h:40m:45s remains)
INFO - root - 2017-12-16 08:40:47.188505: step 18190, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 14h:36m:05s remains)
INFO - root - 2017-12-16 08:40:48.877324: step 18200, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 14h:23m:12s remains)
2017-12-16 08:40:49.383527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.46951 -3.9984784 -3.6431642 -3.6832316 -3.9613504 -4.1894207 -4.3462172 -4.4325681 -4.3921847 -4.1360464 -3.6546087 -3.1225257 -2.6878517 -2.5855045 -2.9582407][-4.7165537 -4.052218 -3.6419563 -3.8202188 -4.2844596 -4.5670567 -4.6800518 -4.6789494 -4.5928569 -4.289506 -3.7805822 -3.16203 -2.6803792 -2.530602 -2.7911861][-4.4148693 -3.5467129 -3.0941539 -3.3599582 -3.9165308 -4.12012 -4.0141582 -3.791111 -3.6246967 -3.4610424 -3.1120982 -2.6258266 -2.1942315 -2.0309427 -2.1879659][-3.8002617 -2.861383 -2.39703 -2.6938496 -3.0477321 -2.8546739 -2.3000491 -1.8141912 -1.6812876 -1.8501844 -1.9180528 -1.7180358 -1.4277198 -1.295898 -1.413432][-3.3099833 -2.3532631 -1.923007 -2.1031733 -2.0863364 -1.3212272 -0.13194942 0.74042439 0.79195189 0.069272518 -0.68039584 -0.85522747 -0.74278593 -0.65937185 -0.77917063][-2.5831892 -1.722253 -1.4064913 -1.5032518 -1.1490998 0.15006638 2.0103266 3.2590773 3.0615923 1.7300985 0.34580994 -0.21131182 -0.2210815 -0.20409966 -0.42774236][-1.4505081 -0.55877912 -0.33987761 -0.40321743 0.077728987 1.6130855 3.8127367 5.3737373 4.8340445 2.9316471 1.1681488 0.40976286 0.34768438 0.27855563 -0.10354495][-0.41489255 0.52890849 0.77064395 0.67036986 1.072572 2.4100587 4.4133492 5.9627991 5.3475523 3.2832172 1.4403274 0.68088746 0.74439669 0.73041892 0.29202485][-0.22019553 0.77841091 1.0738213 0.92417073 1.0211201 1.9117649 3.1736929 4.0681925 3.7097404 2.1365917 0.68589163 0.22143507 0.61283112 0.860266 0.47794771][-0.83488762 0.28919411 0.70450449 0.50109363 0.27274346 0.5891552 1.1923661 1.6412318 1.4315567 0.4294014 -0.49544036 -0.6234417 0.10238552 0.68540168 0.37572289][-2.0943742 -0.96185219 -0.33364773 -0.36450398 -0.64542711 -0.62077546 -0.34797215 -0.10015154 -0.29027557 -0.97467303 -1.6007683 -1.4997456 -0.58185565 0.22044873 -0.075987339][-3.5538132 -2.5224214 -1.7568091 -1.6049221 -1.7873919 -1.7889184 -1.6048541 -1.4451468 -1.5672467 -2.0890465 -2.5484798 -2.3198543 -1.3903511 -0.56546724 -0.79179478][-4.5078955 -3.6122589 -2.8556657 -2.6818833 -2.829479 -2.7985613 -2.6466355 -2.5463142 -2.6303325 -2.9685676 -3.2000437 -2.9344258 -2.191494 -1.4560578 -1.4907714][-4.5731034 -3.8416286 -3.2042167 -3.0662642 -3.2162142 -3.2296178 -3.1279888 -3.0800014 -3.1489785 -3.2866564 -3.3064086 -3.0615683 -2.5373793 -1.9947095 -1.9452398][-3.9604924 -3.4067321 -2.90307 -2.7936707 -2.9322 -3.0039215 -2.9549274 -2.9057951 -2.9183412 -2.9248319 -2.8751788 -2.7118437 -2.3864858 -2.0397191 -1.963779]]...]
INFO - root - 2017-12-16 08:40:51.076214: step 18210, loss = 0.61, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 14h:27m:02s remains)
INFO - root - 2017-12-16 08:40:52.775909: step 18220, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 14h:44m:57s remains)
INFO - root - 2017-12-16 08:40:54.477666: step 18230, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 14h:24m:14s remains)
INFO - root - 2017-12-16 08:40:56.179067: step 18240, loss = 0.55, batch loss = 0.29 (43.5 examples/sec; 0.184 sec/batch; 16h:02m:13s remains)
INFO - root - 2017-12-16 08:40:57.905813: step 18250, loss = 0.73, batch loss = 0.47 (42.1 examples/sec; 0.190 sec/batch; 16h:34m:08s remains)
INFO - root - 2017-12-16 08:40:59.571144: step 18260, loss = 0.62, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:25m:54s remains)
INFO - root - 2017-12-16 08:41:01.246499: step 18270, loss = 0.63, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 14h:38m:43s remains)
INFO - root - 2017-12-16 08:41:02.912199: step 18280, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:34m:36s remains)
INFO - root - 2017-12-16 08:41:04.596390: step 18290, loss = 0.59, batch loss = 0.34 (45.2 examples/sec; 0.177 sec/batch; 15h:27m:19s remains)
INFO - root - 2017-12-16 08:41:06.311147: step 18300, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 14h:23m:45s remains)
2017-12-16 08:41:06.793833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0663695 -2.067143 -2.0858338 -2.0869758 -2.0805838 -2.1040177 -2.1507304 -2.1509972 -2.1124997 -2.0339613 -1.9347924 -1.8553927 -1.8344519 -1.8233545 -1.6031168][-2.5113051 -2.611347 -2.7164342 -2.7569597 -2.7702644 -2.792645 -2.822535 -2.7962022 -2.7212574 -2.6107674 -2.4828064 -2.3968823 -2.3883805 -2.3910396 -2.156544][-3.0154352 -3.2593322 -3.4610548 -3.5310984 -3.5249994 -3.4975655 -3.4318686 -3.3377383 -3.2265642 -3.1609809 -3.1251466 -3.153616 -3.2362537 -3.2828531 -3.053452][-3.3050809 -3.5933397 -3.8265 -3.8785825 -3.8038487 -3.6381814 -3.381844 -3.1606371 -3.08676 -3.1983569 -3.4147577 -3.6817656 -3.9087629 -4.0129876 -3.821249][-3.0944686 -3.2776649 -3.4328246 -3.3611307 -3.144341 -2.7596931 -2.2279325 -1.8337685 -1.9223222 -2.4291773 -3.0430675 -3.641757 -4.0681648 -4.2533836 -4.1143255][-2.2159817 -2.2176881 -2.2263191 -2.0204618 -1.6764467 -1.047358 -0.095161676 0.58658886 0.15104961 -0.83917332 -1.8802278 -2.8520291 -3.4893761 -3.7740264 -3.6278765][-0.95440066 -0.83793378 -0.77223158 -0.50066769 -0.13798738 0.74799848 2.2193291 3.1587412 2.3361757 0.82762051 -0.46941578 -1.610657 -2.3908367 -2.7044079 -2.5150545][0.18103218 0.30758858 0.31634402 0.419075 0.74603081 1.7273939 3.49355 4.5269804 3.4982698 1.7386591 0.39552617 -0.70563781 -1.4488735 -1.6428118 -1.2773715][0.87526894 0.84750128 0.67861319 0.56140614 0.73402143 1.5534356 3.1209686 3.9552939 3.0245402 1.5202451 0.41889644 -0.44368923 -0.9962579 -1.0065864 -0.48587275][1.3348355 1.0492895 0.78387046 0.54790616 0.49978209 1.1034365 2.3361161 2.8473995 2.1246464 0.98438525 0.13231826 -0.50376308 -0.87878227 -0.81750405 -0.24742985][1.6829979 1.1777229 0.8624568 0.62273574 0.39302206 0.71441364 1.666177 2.0713131 1.5746019 0.77211857 0.15762949 -0.28417492 -0.53861046 -0.5795604 -0.11597395][1.5497081 0.83584 0.63901377 0.46898961 0.13138032 0.264395 1.0621712 1.4902539 1.1746669 0.65126133 0.29832697 0.00749135 -0.14751911 -0.22328186 0.077978849][0.56097746 -0.16408896 -0.21942544 -0.36471725 -0.70728028 -0.63408518 0.044201612 0.47914791 0.23613954 -0.1255188 -0.26994109 -0.31856608 -0.3035686 -0.28260541 -0.023576736][-0.86124253 -1.4514172 -1.4370894 -1.6047121 -1.9691002 -1.9273405 -1.3935002 -1.1055683 -1.37666 -1.6405067 -1.539778 -1.3081496 -1.0375783 -0.76700795 -0.42642713][-2.2480495 -2.6464281 -2.6503026 -2.8411348 -3.1573653 -3.1770535 -2.8048205 -2.6559849 -2.89012 -3.0507431 -2.806325 -2.415837 -1.9414802 -1.4530548 -0.97376585]]...]
INFO - root - 2017-12-16 08:41:08.472226: step 18310, loss = 0.61, batch loss = 0.35 (47.5 examples/sec; 0.169 sec/batch; 14h:42m:27s remains)
INFO - root - 2017-12-16 08:41:10.157264: step 18320, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:33m:50s remains)
INFO - root - 2017-12-16 08:41:11.876124: step 18330, loss = 0.49, batch loss = 0.23 (45.5 examples/sec; 0.176 sec/batch; 15h:20m:30s remains)
INFO - root - 2017-12-16 08:41:13.590040: step 18340, loss = 0.51, batch loss = 0.25 (46.5 examples/sec; 0.172 sec/batch; 15h:00m:18s remains)
INFO - root - 2017-12-16 08:41:15.279231: step 18350, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:34m:11s remains)
INFO - root - 2017-12-16 08:41:16.976625: step 18360, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 14h:24m:28s remains)
INFO - root - 2017-12-16 08:41:18.655778: step 18370, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 14h:24m:32s remains)
INFO - root - 2017-12-16 08:41:20.386662: step 18380, loss = 0.61, batch loss = 0.35 (46.5 examples/sec; 0.172 sec/batch; 15h:00m:37s remains)
INFO - root - 2017-12-16 08:41:22.071513: step 18390, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 14h:25m:22s remains)
INFO - root - 2017-12-16 08:41:23.736277: step 18400, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:18m:42s remains)
2017-12-16 08:41:24.167764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.79031 -2.0102072 -2.8205733 -3.0386281 -2.9169114 -2.8185751 -2.8382106 -2.9146447 -2.9060841 -2.8113406 -2.7424126 -2.6656289 -2.6394451 -2.6314836 -2.5713725][0.034801483 -1.3535309 -2.3339872 -2.7809556 -2.7530954 -2.6773803 -2.7075038 -2.7391093 -2.6727061 -2.5676768 -2.5632539 -2.5648177 -2.4866068 -2.3802578 -2.2801886][1.2008212 -0.4023056 -1.674021 -2.3542194 -2.4973984 -2.4549055 -2.4474444 -2.3641298 -2.1473212 -2.0388241 -2.2125063 -2.3476162 -2.1756294 -1.8817313 -1.6671046][1.6022217 0.057298422 -1.2327487 -1.9909478 -2.1137817 -2.0051513 -1.8952694 -1.6701758 -1.3279994 -1.2986523 -1.7907884 -2.1382918 -1.9035019 -1.4758317 -1.1414032][0.90396094 -0.22983265 -1.1460733 -1.6498821 -1.560569 -1.2475173 -0.91581 -0.536021 -0.171736 -0.34285855 -1.3107014 -2.0171733 -1.8447577 -1.3464059 -0.98968089][-0.18588734 -0.9094522 -1.3626742 -1.4817796 -1.0080419 -0.26584983 0.45487571 0.99385333 1.3459923 0.93084741 -0.51512635 -1.6359273 -1.7381899 -1.4517791 -1.3018348][-0.80056858 -1.2952451 -1.4587151 -1.2828237 -0.46403468 0.7140882 1.718972 2.3335774 2.6014578 1.9706023 0.18956733 -1.1902241 -1.5571289 -1.540524 -1.6330324][-1.0025113 -1.3477534 -1.3023396 -0.96369457 0.0029776096 1.3622396 2.5012257 3.1788232 3.3347313 2.5419557 0.67841315 -0.8393079 -1.4100428 -1.5831951 -1.8176222][-1.2421715 -1.5878577 -1.5037364 -1.123365 -0.26979423 0.97669005 2.2303727 3.0275333 3.2409089 2.4723003 0.6396997 -0.94044805 -1.6387863 -1.902959 -2.1436028][-1.4044931 -1.8095548 -1.9410961 -1.8009059 -1.3228774 -0.40259612 0.72167826 1.5512679 1.8396175 1.2126312 -0.28693438 -1.6346024 -2.2191987 -2.4599569 -2.6147683][-1.36308 -1.8844969 -2.1937501 -2.3369873 -2.2412198 -1.7256417 -0.95300579 -0.28476667 0.009570837 -0.48163271 -1.6686273 -2.6949713 -3.0969672 -3.1184912 -3.0414238][-1.072912 -1.7011542 -2.0815997 -2.3284521 -2.4490175 -2.3246577 -1.9946855 -1.6072707 -1.3867543 -1.7831774 -2.6950917 -3.4261279 -3.6354206 -3.4663889 -3.1553411][-0.63218141 -1.3052646 -1.70951 -2.0069265 -2.2392056 -2.3759263 -2.3865237 -2.19195 -1.9419343 -2.1523616 -2.8389113 -3.391608 -3.4952359 -3.2692099 -2.8770094][-0.1855607 -0.88176668 -1.3106773 -1.6378067 -1.9742901 -2.2602794 -2.4446108 -2.325454 -2.0046709 -1.9907378 -2.455631 -2.8828769 -2.9585998 -2.7662039 -2.4503183][-0.36729169 -1.0142356 -1.3854692 -1.6351421 -1.9233958 -2.1943727 -2.3787887 -2.26737 -1.9278635 -1.7863762 -2.0532172 -2.3852015 -2.4613001 -2.3413038 -2.184649]]...]
INFO - root - 2017-12-16 08:41:25.859449: step 18410, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 14h:20m:39s remains)
INFO - root - 2017-12-16 08:41:27.544171: step 18420, loss = 0.62, batch loss = 0.36 (45.2 examples/sec; 0.177 sec/batch; 15h:27m:30s remains)
INFO - root - 2017-12-16 08:41:29.255336: step 18430, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 14h:36m:00s remains)
INFO - root - 2017-12-16 08:41:30.933179: step 18440, loss = 0.49, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 14h:40m:14s remains)
INFO - root - 2017-12-16 08:41:32.628213: step 18450, loss = 0.60, batch loss = 0.34 (46.5 examples/sec; 0.172 sec/batch; 15h:01m:19s remains)
INFO - root - 2017-12-16 08:41:34.310067: step 18460, loss = 0.63, batch loss = 0.38 (48.6 examples/sec; 0.165 sec/batch; 14h:21m:23s remains)
INFO - root - 2017-12-16 08:41:35.990700: step 18470, loss = 0.62, batch loss = 0.36 (46.9 examples/sec; 0.171 sec/batch; 14h:52m:58s remains)
INFO - root - 2017-12-16 08:41:37.689906: step 18480, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:21m:56s remains)
INFO - root - 2017-12-16 08:41:39.409609: step 18490, loss = 0.61, batch loss = 0.35 (46.6 examples/sec; 0.172 sec/batch; 14h:57m:49s remains)
INFO - root - 2017-12-16 08:41:41.072235: step 18500, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:23m:21s remains)
2017-12-16 08:41:41.559059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9592123 -2.1080415 -2.2181368 -2.3164458 -2.4564114 -2.6925323 -2.9411445 -3.030334 -3.0186226 -2.9534395 -2.841733 -2.6767983 -2.4650555 -2.2452586 -2.0412397][-2.4871252 -2.7132547 -2.9134595 -3.0665197 -3.2460055 -3.5319369 -3.8574619 -3.9526668 -3.9021223 -3.8316262 -3.7374098 -3.5087576 -3.1579313 -2.7873173 -2.4510248][-3.2238696 -3.5492382 -3.7826214 -3.8663125 -3.91914 -4.1842289 -4.4867382 -4.5506306 -4.4870739 -4.5704389 -4.6578889 -4.4760637 -4.035625 -3.5170963 -3.0365405][-3.868279 -4.1712341 -4.277318 -4.0307112 -3.7091534 -3.7482169 -3.941432 -3.9521844 -3.9733343 -4.3963223 -4.8583775 -4.9158373 -4.6192703 -4.1706042 -3.5897574][-4.2466445 -4.3420405 -4.0375018 -3.2104037 -2.3177128 -1.9251078 -1.7706642 -1.6365287 -1.8703823 -2.8331058 -3.8449922 -4.3426952 -4.4572325 -4.3376503 -3.8314652][-4.2176003 -4.0562468 -3.2215962 -1.7142258 -0.17553687 0.82622576 1.5579455 2.0076475 1.5023959 -0.17133951 -1.767421 -2.7536159 -3.4355631 -3.8533683 -3.6226366][-3.7914746 -3.4223883 -2.2062159 -0.17082381 1.8746114 3.458744 5.0131526 6.0470948 5.0476794 2.6807995 0.72327566 -0.65682447 -1.9216778 -2.9149461 -3.047298][-3.3691297 -2.9167318 -1.5350674 0.60450315 2.8122911 4.7248845 6.9734406 8.9430428 7.1675086 4.4207792 2.4528465 0.88939643 -0.7733345 -2.0866196 -2.5337007][-3.3490767 -3.0609298 -1.83586 -0.0043468475 1.7955008 3.4279785 5.1720958 6.3877435 5.6109023 3.6928644 2.2665319 0.99721432 -0.61052728 -1.9541078 -2.4692063][-3.590982 -3.5540838 -2.8280993 -1.6956079 -0.47493994 0.67366266 1.7208381 2.3886318 2.179193 1.2855368 0.56879139 -0.19051337 -1.3678544 -2.4371374 -2.7999244][-3.8047404 -4.0516729 -3.8478212 -3.3571506 -2.7095828 -2.0992975 -1.5601721 -1.1753898 -1.0969458 -1.3347309 -1.5297949 -1.8607726 -2.5689719 -3.1662271 -3.2656615][-3.78088 -4.1895022 -4.3341022 -4.2701964 -4.0240917 -3.7684786 -3.5620725 -3.3777227 -3.13728 -3.0486917 -3.0353177 -3.1426821 -3.4353724 -3.6391332 -3.5385351][-3.4019172 -3.8190584 -4.094202 -4.265028 -4.2597485 -4.21546 -4.2458963 -4.1803627 -3.9512966 -3.7640791 -3.6795564 -3.6713138 -3.7219658 -3.6866107 -3.4713717][-2.7985713 -3.1129529 -3.3729424 -3.5661166 -3.6613762 -3.7410228 -3.8328881 -3.8381457 -3.7072582 -3.5541506 -3.4613466 -3.4068763 -3.3593345 -3.247726 -3.0419211][-2.2450793 -2.4191632 -2.5662358 -2.6774445 -2.7621026 -2.8473239 -2.926122 -2.9494855 -2.8931675 -2.8226917 -2.7596869 -2.7092459 -2.6640077 -2.5909944 -2.4725628]]...]
INFO - root - 2017-12-16 08:41:43.238104: step 18510, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 14h:18m:13s remains)
INFO - root - 2017-12-16 08:41:44.910709: step 18520, loss = 0.59, batch loss = 0.33 (46.4 examples/sec; 0.172 sec/batch; 15h:02m:33s remains)
INFO - root - 2017-12-16 08:41:46.582372: step 18530, loss = 0.54, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 14h:39m:45s remains)
INFO - root - 2017-12-16 08:41:48.267132: step 18540, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.168 sec/batch; 14h:36m:39s remains)
INFO - root - 2017-12-16 08:41:49.923328: step 18550, loss = 0.47, batch loss = 0.21 (47.2 examples/sec; 0.170 sec/batch; 14h:47m:15s remains)
INFO - root - 2017-12-16 08:41:51.592452: step 18560, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.165 sec/batch; 14h:25m:47s remains)
INFO - root - 2017-12-16 08:41:53.271607: step 18570, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 14h:32m:34s remains)
INFO - root - 2017-12-16 08:41:54.938585: step 18580, loss = 0.62, batch loss = 0.36 (48.2 examples/sec; 0.166 sec/batch; 14h:28m:45s remains)
INFO - root - 2017-12-16 08:41:56.628049: step 18590, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 14h:48m:18s remains)
INFO - root - 2017-12-16 08:41:58.292985: step 18600, loss = 0.52, batch loss = 0.26 (49.4 examples/sec; 0.162 sec/batch; 14h:06m:57s remains)
2017-12-16 08:41:58.736129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.014596 -2.2671452 -2.75608 -3.2614665 -3.6247578 -3.6685095 -3.3379869 -3.0068164 -2.9527211 -3.0979545 -3.2031732 -3.2019176 -3.3328271 -3.5732303 -3.7714212][-2.3049903 -2.6468956 -3.2045045 -3.6161702 -3.7793522 -3.5564997 -2.9335194 -2.3620112 -2.1287947 -2.3025799 -2.6495676 -2.9517343 -3.2885273 -3.6794231 -4.03675][-2.6317329 -3.0349522 -3.6050146 -3.9104829 -3.8014657 -3.2796898 -2.3691764 -1.4576101 -0.89530075 -0.98686528 -1.6376207 -2.3870153 -3.0890729 -3.710598 -4.2107439][-2.9613552 -3.280899 -3.7770691 -3.9533591 -3.5632076 -2.6725957 -1.4526472 -0.27055216 0.47346187 0.22570324 -0.83580852 -2.1067641 -3.20786 -3.9186301 -4.3427176][-2.9523821 -3.1709745 -3.5784693 -3.6002667 -2.9309664 -1.7476518 -0.31910229 0.89359975 1.5192599 1.0085328 -0.46426606 -2.1541064 -3.5292859 -4.2848158 -4.4748][-2.6403461 -2.7392044 -3.0639048 -3.0229287 -2.2025304 -0.84465945 0.67294431 1.8629649 2.2749035 1.4692092 -0.20365381 -2.0832267 -3.6133695 -4.3784838 -4.38956][-2.21053 -2.1874895 -2.3962622 -2.3471534 -1.4917547 -0.0033545494 1.5548444 2.6775119 2.8064601 1.7351582 -0.036298275 -1.9425688 -3.4653697 -4.1890397 -4.0718069][-1.7721717 -1.5384614 -1.5626709 -1.3940961 -0.48321927 1.0000081 2.4263961 3.3151062 2.9785354 1.5519783 -0.3146162 -2.1205182 -3.4384186 -3.9166017 -3.6297646][-1.3273989 -0.7995832 -0.57710648 -0.33076406 0.48586488 1.7669733 2.9028327 3.4105551 2.5373485 0.85551381 -0.955124 -2.4972544 -3.4175873 -3.5300298 -3.1018896][-1.0543784 -0.36251903 -0.080417156 0.071245193 0.58238077 1.5204315 2.374131 2.5556018 1.606015 0.050102472 -1.5080277 -2.6665158 -3.1558409 -2.9944773 -2.5799453][-1.1042156 -0.47998428 -0.22422099 -0.15768194 0.19277644 0.8581686 1.5363429 1.6651871 0.88379312 -0.38977706 -1.6359944 -2.4936073 -2.7216573 -2.4716685 -2.1894755][-1.2505813 -0.82175422 -0.61139154 -0.42381167 0.0037827492 0.61762047 1.1866765 1.2359209 0.51542616 -0.55005813 -1.5771294 -2.2485678 -2.38247 -2.1257503 -1.923028][-1.2162774 -0.98454857 -0.86880612 -0.566164 -0.046654224 0.55203295 0.97853374 0.92993808 0.26358843 -0.63303089 -1.4610837 -1.9610822 -2.00707 -1.7212064 -1.5174955][-1.0422648 -0.95050395 -1.0095986 -0.8105787 -0.32267356 0.25003529 0.66186929 0.72828984 0.3513658 -0.23944473 -0.8237493 -1.2581204 -1.3276532 -1.0960921 -0.89882636][-1.1766126 -1.1319934 -1.2782371 -1.2225229 -0.87663579 -0.25561547 0.33181071 0.64762712 0.66767406 0.45359588 0.042875528 -0.43683124 -0.62905097 -0.50139058 -0.22956276]]...]
INFO - root - 2017-12-16 08:42:00.416267: step 18610, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:34m:10s remains)
INFO - root - 2017-12-16 08:42:02.107730: step 18620, loss = 0.47, batch loss = 0.21 (47.0 examples/sec; 0.170 sec/batch; 14h:50m:10s remains)
INFO - root - 2017-12-16 08:42:03.788073: step 18630, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:42m:34s remains)
INFO - root - 2017-12-16 08:42:05.513401: step 18640, loss = 0.56, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 15h:00m:39s remains)
INFO - root - 2017-12-16 08:42:07.203574: step 18650, loss = 0.55, batch loss = 0.29 (45.9 examples/sec; 0.174 sec/batch; 15h:11m:36s remains)
INFO - root - 2017-12-16 08:42:08.884951: step 18660, loss = 0.47, batch loss = 0.21 (49.0 examples/sec; 0.163 sec/batch; 14h:14m:14s remains)
INFO - root - 2017-12-16 08:42:10.585291: step 18670, loss = 0.65, batch loss = 0.40 (47.2 examples/sec; 0.169 sec/batch; 14h:46m:23s remains)
INFO - root - 2017-12-16 08:42:12.305740: step 18680, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 15h:05m:36s remains)
INFO - root - 2017-12-16 08:42:14.017591: step 18690, loss = 0.54, batch loss = 0.28 (45.6 examples/sec; 0.175 sec/batch; 15h:16m:44s remains)
INFO - root - 2017-12-16 08:42:15.731409: step 18700, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:21m:48s remains)
2017-12-16 08:42:16.223952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.497506 -1.5683982 -1.6542935 -1.7458812 -1.8338339 -1.7963115 -1.4769199 -1.12554 -1.1789631 -1.4978793 -1.8550241 -2.4149139 -2.9865212 -3.1430173 -2.9480653][-2.7491927 -2.8755827 -3.010298 -3.1239104 -3.2127497 -3.2018013 -2.9514155 -2.6627078 -2.6816094 -2.9084411 -3.131407 -3.4385579 -3.69841 -3.5768032 -3.2313867][-3.8276591 -3.9893184 -4.1338778 -4.2109714 -4.2216878 -4.1425819 -3.8504138 -3.5713372 -3.579474 -3.8194702 -4.040483 -4.2367425 -4.2591195 -3.9260273 -3.4803152][-4.4646006 -4.5759082 -4.6375594 -4.5723772 -4.3768835 -4.0774159 -3.6075759 -3.2658436 -3.2964077 -3.6333692 -3.9906259 -4.2698021 -4.3047743 -3.9958282 -3.5786004][-4.473464 -4.4482565 -4.2750311 -3.9187489 -3.4235711 -2.8046906 -2.0550015 -1.5493655 -1.6020103 -2.0909703 -2.6658874 -3.2075243 -3.5353241 -3.5070891 -3.289161][-3.6590633 -3.4595957 -3.0135036 -2.3356094 -1.5571003 -0.64740086 0.42756772 1.1157579 0.938164 0.20973659 -0.59584129 -1.4807725 -2.2740266 -2.6839275 -2.7700987][-2.4400194 -2.1613092 -1.5471981 -0.68663573 0.22632313 1.2193756 2.431073 3.2044785 2.8474152 1.8944633 0.94115472 -0.12639236 -1.2490976 -1.996612 -2.3095238][-1.4290115 -1.2642852 -0.76059639 0.0040974617 0.77441788 1.5922549 2.6768944 3.3596542 2.8920915 1.8839576 1.0037494 0.0083971024 -1.1005158 -1.8530971 -2.1744308][-1.1063298 -1.1835717 -1.0167803 -0.6302824 -0.20694685 0.27134752 1.0563588 1.5689521 1.1233792 0.28151059 -0.37861204 -1.085539 -1.8381253 -2.2905285 -2.4050603][-1.6168318 -1.862262 -2.0346634 -2.0884354 -2.0448272 -1.8698189 -1.3806562 -1.0211816 -1.2992458 -1.8401611 -2.2047849 -2.5302477 -2.7948985 -2.8394678 -2.7399979][-2.3197184 -2.6474724 -3.0556767 -3.4422903 -3.6731756 -3.7023344 -3.4335656 -3.1483881 -3.20155 -3.4261699 -3.5326207 -3.5380745 -3.3966157 -3.1463723 -2.9200521][-2.5593443 -2.9215534 -3.4363973 -3.9795442 -4.3414841 -4.4702539 -4.2997327 -4.0094461 -3.8376393 -3.7796278 -3.7150269 -3.5633039 -3.2783704 -2.9678519 -2.7529633][-2.2230568 -2.4852805 -2.9094703 -3.4235234 -3.8010955 -3.9224672 -3.7810988 -3.4813128 -3.1873541 -3.0210421 -2.9386868 -2.8234692 -2.6477466 -2.4730835 -2.3908577][-1.8278427 -1.8525689 -2.0172796 -2.3082576 -2.5684636 -2.6072102 -2.4496651 -2.1653416 -1.9015113 -1.7791355 -1.7699431 -1.8389832 -1.9119043 -1.963461 -2.05143][-1.6902921 -1.4850694 -1.3461387 -1.36636 -1.4572753 -1.4180168 -1.2648957 -1.0780169 -0.95238495 -0.93713295 -1.0383847 -1.2793539 -1.5352762 -1.729466 -1.9039645]]...]
INFO - root - 2017-12-16 08:42:17.925376: step 18710, loss = 0.66, batch loss = 0.40 (47.4 examples/sec; 0.169 sec/batch; 14h:43m:02s remains)
INFO - root - 2017-12-16 08:42:19.638142: step 18720, loss = 0.82, batch loss = 0.56 (43.6 examples/sec; 0.183 sec/batch; 15h:59m:33s remains)
INFO - root - 2017-12-16 08:42:21.330529: step 18730, loss = 0.62, batch loss = 0.36 (46.1 examples/sec; 0.173 sec/batch; 15h:07m:14s remains)
INFO - root - 2017-12-16 08:42:23.025747: step 18740, loss = 0.59, batch loss = 0.34 (47.1 examples/sec; 0.170 sec/batch; 14h:48m:34s remains)
INFO - root - 2017-12-16 08:42:24.729481: step 18750, loss = 0.60, batch loss = 0.34 (46.1 examples/sec; 0.174 sec/batch; 15h:08m:23s remains)
INFO - root - 2017-12-16 08:42:26.393451: step 18760, loss = 0.61, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 14h:25m:51s remains)
INFO - root - 2017-12-16 08:42:28.084034: step 18770, loss = 0.49, batch loss = 0.23 (46.7 examples/sec; 0.171 sec/batch; 14h:55m:11s remains)
INFO - root - 2017-12-16 08:42:29.743712: step 18780, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:33m:05s remains)
INFO - root - 2017-12-16 08:42:31.433778: step 18790, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.170 sec/batch; 14h:46m:55s remains)
INFO - root - 2017-12-16 08:42:33.099635: step 18800, loss = 0.65, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 14h:33m:39s remains)
2017-12-16 08:42:33.559134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8266394 -1.8711095 -1.9151968 -1.9213428 -1.8653108 -1.77021 -1.7069037 -1.7650132 -1.9295743 -2.0913262 -2.1314702 -2.0039685 -1.8065572 -1.6675875 -1.6177505][-1.5505471 -1.654932 -1.7765956 -1.8365865 -1.7476121 -1.5544887 -1.4416134 -1.5286438 -1.7944158 -2.1002326 -2.1919944 -2.0219152 -1.7616727 -1.5741715 -1.507146][-1.2686822 -1.4481111 -1.658422 -1.7454151 -1.56458 -1.2126051 -0.98786438 -1.0869058 -1.5272474 -2.0329223 -2.239953 -2.0971296 -1.7953591 -1.5528646 -1.4482138][-1.2259287 -1.4727461 -1.7129405 -1.7187271 -1.3535125 -0.70827842 -0.24477506 -0.37812853 -1.0741813 -1.8721879 -2.3070309 -2.2964563 -2.000489 -1.6855998 -1.5009789][-1.3531816 -1.6191748 -1.802025 -1.6386523 -0.97787786 0.11138749 0.95931387 0.75636673 -0.24429703 -1.4514701 -2.2574422 -2.5000944 -2.2866435 -1.9504728 -1.6999227][-1.5406451 -1.769511 -1.8649733 -1.4948626 -0.51604688 1.0448992 2.39745 2.2729187 0.93476748 -0.782092 -2.0607421 -2.6337192 -2.6226537 -2.3248143 -2.0200582][-1.6565394 -1.8444939 -1.8422363 -1.3431163 -0.12840939 1.799531 3.6164479 3.6721549 2.0730081 -0.07809782 -1.7860994 -2.681917 -2.8948629 -2.6853759 -2.3683848][-1.7034657 -1.8735143 -1.8269546 -1.3049226 0.0040619373 2.0591283 4.0859871 4.2401729 2.5828767 0.305161 -1.5411446 -2.6241133 -3.004389 -2.9182816 -2.6209288][-1.7456391 -1.9239542 -1.9066406 -1.4534004 -0.28037667 1.5030959 3.2784381 3.4379678 2.1347651 0.2554872 -1.3186278 -2.357497 -2.8660138 -2.9423509 -2.7064939][-1.8146651 -2.0481007 -2.1221225 -1.8010755 -0.90290225 0.40660286 1.6879816 1.9466858 1.2168863 0.073489189 -1.0128641 -1.8864046 -2.4658122 -2.7119021 -2.5669055][-1.9528961 -2.2801619 -2.4377019 -2.2282224 -1.6004974 -0.73030651 0.088639736 0.40311813 0.23645854 -0.12186456 -0.58593595 -1.2003584 -1.8734515 -2.2715569 -2.2777863][-2.1732948 -2.5198667 -2.6881776 -2.5488167 -2.0869055 -1.4823151 -1.0286509 -0.78038883 -0.57438362 -0.26682639 -0.22175193 -0.64956355 -1.4000221 -1.9271255 -2.0774121][-2.3946855 -2.6175146 -2.755764 -2.6180854 -2.2170835 -1.7455978 -1.5101366 -1.4320269 -1.0789413 -0.46200228 -0.16309166 -0.52017415 -1.3468891 -1.9503449 -2.2010076][-2.5072706 -2.503751 -2.4463549 -2.2411108 -1.8435318 -1.4245187 -1.3614403 -1.5409728 -1.3546826 -0.78424811 -0.43100107 -0.72288835 -1.5254695 -2.1322365 -2.4372][-2.4648077 -2.1606741 -1.8307489 -1.4935665 -1.0493619 -0.74960065 -0.90033853 -1.4296985 -1.5099661 -1.0434017 -0.60791707 -0.72380388 -1.4746842 -2.1213236 -2.5051165]]...]
INFO - root - 2017-12-16 08:42:35.234305: step 18810, loss = 0.47, batch loss = 0.21 (47.6 examples/sec; 0.168 sec/batch; 14h:39m:02s remains)
INFO - root - 2017-12-16 08:42:36.926656: step 18820, loss = 0.51, batch loss = 0.25 (48.5 examples/sec; 0.165 sec/batch; 14h:22m:33s remains)
INFO - root - 2017-12-16 08:42:38.632977: step 18830, loss = 0.74, batch loss = 0.48 (47.6 examples/sec; 0.168 sec/batch; 14h:38m:15s remains)
INFO - root - 2017-12-16 08:42:40.316224: step 18840, loss = 0.51, batch loss = 0.25 (48.5 examples/sec; 0.165 sec/batch; 14h:22m:56s remains)
INFO - root - 2017-12-16 08:42:41.985162: step 18850, loss = 0.58, batch loss = 0.32 (46.9 examples/sec; 0.171 sec/batch; 14h:51m:22s remains)
INFO - root - 2017-12-16 08:42:43.650195: step 18860, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:29m:28s remains)
INFO - root - 2017-12-16 08:42:45.296250: step 18870, loss = 0.47, batch loss = 0.21 (47.6 examples/sec; 0.168 sec/batch; 14h:39m:00s remains)
INFO - root - 2017-12-16 08:42:47.005397: step 18880, loss = 0.54, batch loss = 0.28 (45.9 examples/sec; 0.174 sec/batch; 15h:11m:22s remains)
INFO - root - 2017-12-16 08:42:48.762474: step 18890, loss = 0.53, batch loss = 0.28 (46.1 examples/sec; 0.174 sec/batch; 15h:07m:55s remains)
INFO - root - 2017-12-16 08:42:50.443859: step 18900, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:34m:12s remains)
2017-12-16 08:42:50.924840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6700659 -3.8357849 -4.1962247 -4.4655046 -4.5584574 -4.5628614 -4.5258694 -4.4512339 -4.3202505 -4.0740976 -3.6542606 -3.1180248 -2.663492 -2.2704675 -1.8463347][-3.315325 -3.3907175 -3.7514267 -4.0820351 -4.2498035 -4.261652 -4.232944 -4.3369265 -4.4889708 -4.5301619 -4.2752881 -3.8180022 -3.4179492 -3.0618823 -2.6263087][-1.9455752 -1.8298699 -2.1137187 -2.5168037 -2.7664483 -2.803586 -2.847573 -3.202806 -3.7563643 -4.1626692 -4.1522436 -3.8122478 -3.5062623 -3.2670224 -3.0064335][-0.19028091 0.2170527 0.076898336 -0.32900763 -0.60944676 -0.6643337 -0.7882396 -1.4546441 -2.4388585 -3.1865385 -3.3798914 -3.1924481 -2.9912145 -2.8595698 -2.8133101][1.2184832 2.0479281 2.1973608 1.9807417 1.7580063 1.67359 1.4868152 0.51819563 -0.78477657 -1.8316882 -2.311187 -2.420666 -2.4464657 -2.408041 -2.4778407][1.7540219 2.9905684 3.5876014 3.8191407 3.8213265 3.8061035 3.6812546 2.637109 1.1431136 -0.21697974 -1.1344695 -1.7224532 -2.1420553 -2.3201177 -2.4568605][0.9551158 2.2891686 3.2769253 4.1124992 4.6146383 4.9484243 5.1081171 4.3518009 2.8603952 1.1883798 -0.12672138 -1.0741209 -1.8962154 -2.3689039 -2.6320057][-0.59439886 0.49404716 1.5557051 2.7958272 3.7684362 4.4583492 4.9035578 4.5758228 3.300854 1.6107469 0.2869277 -0.70651066 -1.6805387 -2.3931639 -2.8176918][-2.1977637 -1.4724803 -0.53225017 0.71465755 1.7766049 2.483969 2.9652455 2.9278977 1.9528921 0.606302 -0.44100606 -1.1624002 -1.9607317 -2.6090403 -2.9886191][-3.5083392 -3.1250329 -2.4639385 -1.4891901 -0.62954664 -0.12487817 0.27781868 0.35586309 -0.287529 -1.2432426 -1.9746187 -2.3764005 -2.7989833 -3.1536636 -3.3542461][-4.3535271 -4.319663 -4.021863 -3.4769545 -2.9609299 -2.6212928 -2.3021171 -2.2140727 -2.6341276 -3.2793636 -3.6713958 -3.7199175 -3.7071462 -3.7185459 -3.6991084][-4.4249458 -4.6824117 -4.7466621 -4.580802 -4.3718462 -4.2098484 -4.0215063 -3.9957345 -4.2722464 -4.6102734 -4.6842833 -4.4007998 -4.0294065 -3.8045657 -3.6496515][-3.79595 -4.1601114 -4.4230013 -4.5440702 -4.542263 -4.5250597 -4.4700832 -4.4850712 -4.6109748 -4.6591907 -4.4884534 -4.0296326 -3.5453351 -3.2573948 -3.0391278][-2.7399542 -3.0611184 -3.3720269 -3.5858231 -3.7057991 -3.7697878 -3.8050952 -3.8156636 -3.8327413 -3.7508509 -3.4779506 -3.0317705 -2.6006029 -2.2964835 -2.0672164][-1.8294675 -2.0121384 -2.2414904 -2.4301622 -2.5470016 -2.6084487 -2.6610286 -2.686002 -2.6720135 -2.5812266 -2.3697932 -2.0587702 -1.7494597 -1.5021791 -1.2910068]]...]
INFO - root - 2017-12-16 08:42:52.630806: step 18910, loss = 0.58, batch loss = 0.32 (46.4 examples/sec; 0.172 sec/batch; 15h:01m:23s remains)
INFO - root - 2017-12-16 08:42:54.336858: step 18920, loss = 0.67, batch loss = 0.41 (47.0 examples/sec; 0.170 sec/batch; 14h:48m:51s remains)
INFO - root - 2017-12-16 08:42:56.012032: step 18930, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 14h:40m:06s remains)
INFO - root - 2017-12-16 08:42:57.662748: step 18940, loss = 0.59, batch loss = 0.33 (46.5 examples/sec; 0.172 sec/batch; 14h:59m:19s remains)
INFO - root - 2017-12-16 08:42:59.371747: step 18950, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 14h:46m:35s remains)
INFO - root - 2017-12-16 08:43:01.054680: step 18960, loss = 0.56, batch loss = 0.30 (45.9 examples/sec; 0.174 sec/batch; 15h:10m:03s remains)
INFO - root - 2017-12-16 08:43:02.718746: step 18970, loss = 0.62, batch loss = 0.36 (48.3 examples/sec; 0.165 sec/batch; 14h:24m:45s remains)
INFO - root - 2017-12-16 08:43:04.359353: step 18980, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 14h:15m:02s remains)
INFO - root - 2017-12-16 08:43:06.042862: step 18990, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:21m:48s remains)
INFO - root - 2017-12-16 08:43:07.719865: step 19000, loss = 0.52, batch loss = 0.26 (47.1 examples/sec; 0.170 sec/batch; 14h:47m:11s remains)
2017-12-16 08:43:08.239930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6680703 -2.1310604 -1.559516 -1.0068394 -0.72718871 -0.86261511 -1.1873173 -1.2834692 -1.0951711 -0.63911414 -0.20148039 0.10812283 0.44692707 0.873224 1.1694231][-1.8531462 -1.1222013 -0.4738555 0.024815083 0.12732196 -0.26152468 -0.7853626 -1.067657 -0.93388069 -0.47142208 -0.11148667 0.05496645 0.29180121 0.72667694 1.0566347][-0.80083358 -0.057703257 0.45217061 0.80998015 0.71307254 0.14204502 -0.54057217 -1.0161307 -1.0730196 -0.86608768 -0.7814759 -0.8967967 -0.9417187 -0.71974552 -0.51719654][0.19599652 0.77656794 1.0750566 1.3242178 1.244705 0.68516707 -0.060933828 -0.59066 -0.87351859 -1.0914905 -1.4625564 -1.8489289 -2.1426167 -2.204035 -2.2305589][0.86139488 1.3505387 1.5500417 1.7965307 1.8347697 1.3926437 0.73412323 0.19823074 -0.24238753 -0.79804265 -1.5238376 -2.1567085 -2.6418066 -2.9094918 -3.167685][0.91923857 1.3528562 1.6034133 1.9646249 2.2764525 2.1644435 1.7542033 1.364305 0.91024685 0.17769408 -0.7529341 -1.5385612 -2.0710311 -2.4120405 -2.7644572][0.44587326 0.81825471 1.1371534 1.6987643 2.2798581 2.5342326 2.4757981 2.391078 2.0347028 1.3832059 0.52991676 -0.24354601 -0.61703515 -0.90432 -1.2304014][0.060180902 0.4001255 0.73745584 1.2663419 1.8475628 2.3148637 2.5545282 2.6826453 2.4538207 2.0525074 1.6481142 1.149209 1.0122428 0.93219829 0.80697846][0.063742876 0.33363509 0.62300372 1.0474198 1.5431197 2.0261045 2.3087034 2.4524593 2.3981147 2.3748813 2.3914361 2.1203341 2.0474582 2.0730381 2.0023317][0.29188848 0.41742039 0.64519 1.0448782 1.4779701 1.8305845 2.0619211 2.1596842 2.2558742 2.4535475 2.6671314 2.4945745 2.3086028 2.10255 1.7220397][0.38412976 0.5211556 0.84039378 1.3524065 1.7710314 1.9247026 1.9827986 2.0017381 2.0420113 2.208807 2.4065828 2.1825695 1.8107033 1.2899208 0.6081655][0.22832727 0.43915892 0.88291383 1.4572349 1.8281574 1.8793221 1.8154454 1.6827588 1.5860803 1.5999207 1.5925658 1.2374032 0.71853018 0.11778307 -0.58922613][-0.21049237 0.11825848 0.59022784 1.1057923 1.3768582 1.3246546 1.123287 0.8776722 0.62423205 0.43941188 0.20830894 -0.23592114 -0.76085591 -1.2729402 -1.805474][-1.2007029 -0.85026395 -0.44034314 -0.050433874 0.11386347 0.01818347 -0.23178339 -0.55967212 -0.89305222 -1.1355222 -1.4034824 -1.7506028 -2.1220102 -2.4206772 -2.6631505][-2.4567685 -2.1569419 -1.8669052 -1.6202343 -1.5354311 -1.648932 -1.8981358 -2.1981697 -2.4670677 -2.6294911 -2.7719562 -2.8949091 -2.9988177 -3.0412071 -3.0163064]]...]
INFO - root - 2017-12-16 08:43:09.994338: step 19010, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:28m:03s remains)
INFO - root - 2017-12-16 08:43:11.723672: step 19020, loss = 0.57, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 14h:46m:55s remains)
INFO - root - 2017-12-16 08:43:13.388842: step 19030, loss = 0.48, batch loss = 0.22 (45.7 examples/sec; 0.175 sec/batch; 15h:14m:28s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:43:15.133518: step 19040, loss = 0.50, batch loss = 0.24 (47.3 examples/sec; 0.169 sec/batch; 14h:42m:59s remains)
INFO - root - 2017-12-16 08:43:16.808150: step 19050, loss = 0.57, batch loss = 0.31 (45.8 examples/sec; 0.175 sec/batch; 15h:12m:04s remains)
INFO - root - 2017-12-16 08:43:18.495263: step 19060, loss = 0.52, batch loss = 0.26 (47.1 examples/sec; 0.170 sec/batch; 14h:47m:22s remains)
INFO - root - 2017-12-16 08:43:20.171283: step 19070, loss = 0.54, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 14h:54m:39s remains)
INFO - root - 2017-12-16 08:43:21.833206: step 19080, loss = 0.66, batch loss = 0.40 (49.8 examples/sec; 0.161 sec/batch; 13h:59m:52s remains)
INFO - root - 2017-12-16 08:43:23.484038: step 19090, loss = 0.61, batch loss = 0.35 (48.6 examples/sec; 0.165 sec/batch; 14h:19m:30s remains)
INFO - root - 2017-12-16 08:43:25.162761: step 19100, loss = 0.56, batch loss = 0.31 (46.9 examples/sec; 0.170 sec/batch; 14h:50m:03s remains)
2017-12-16 08:43:25.657052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9789429 -2.8694582 -2.6162364 -2.4089916 -2.4808595 -2.8525171 -3.3069775 -3.6624608 -3.7991626 -3.7257349 -3.4611707 -3.0545378 -2.6952825 -2.6619015 -2.897753][-2.0043211 -2.0565395 -1.9460758 -1.7608676 -1.7441306 -2.07195 -2.5821385 -3.0998402 -3.43746 -3.5047185 -3.3480034 -3.0794146 -2.7810695 -2.6892693 -2.8829212][-1.4709913 -1.8104904 -1.8287616 -1.5527496 -1.2998551 -1.4080789 -1.8817698 -2.4643636 -2.9034369 -3.1186376 -3.1261175 -3.0344696 -2.840775 -2.7934649 -3.0556045][-1.7425865 -2.2267389 -2.2362547 -1.6955754 -0.99879754 -0.70845938 -0.96658266 -1.4700379 -1.9274014 -2.2920396 -2.5192509 -2.6097143 -2.5202415 -2.5910583 -3.0344429][-2.5071607 -2.9956639 -2.8288887 -1.9046314 -0.71850181 0.023674726 0.11267495 -0.16249466 -0.65552473 -1.2752197 -1.7549418 -1.9714677 -1.942096 -2.0430281 -2.6158462][-3.3785996 -3.7687414 -3.3536308 -2.0464692 -0.49354506 0.59052324 1.0752478 1.0586777 0.46747875 -0.4489249 -1.1338226 -1.3441017 -1.2205178 -1.2518903 -1.9001858][-3.7641995 -4.044312 -3.5137675 -2.0644078 -0.45763779 0.73408532 1.4918303 1.6586709 0.99727869 -0.12144399 -0.90823078 -0.99736142 -0.61836922 -0.467929 -1.0864892][-3.6770289 -3.9121461 -3.3520935 -2.0328603 -0.67545521 0.26002288 1.0111153 1.3105726 0.66986036 -0.45488393 -1.1449742 -1.0017793 -0.38342369 -0.033462048 -0.46057343][-3.0744004 -3.3898425 -2.9967966 -1.974122 -1.0486358 -0.50981283 0.0038480759 0.29553461 -0.21142173 -1.0718615 -1.5048559 -1.1566706 -0.35813904 0.21535945 0.0077528954][-2.0839183 -2.5790758 -2.4643118 -1.8511693 -1.3494213 -1.1478424 -0.88334656 -0.61417449 -0.92796957 -1.4829841 -1.6291646 -1.1528072 -0.27344823 0.42276263 0.34738088][-1.144256 -1.7653172 -1.8805246 -1.6191261 -1.3840015 -1.3562621 -1.2353307 -1.001055 -1.0947002 -1.376882 -1.3871247 -0.91218221 -0.087447405 0.55150604 0.45817184][-1.1444062 -1.6727557 -1.8910115 -1.807191 -1.6815784 -1.6683741 -1.6007462 -1.417604 -1.3983428 -1.495785 -1.4502678 -1.0238316 -0.26737356 0.20735979 -0.020394087][-1.7662315 -2.0751615 -2.2338319 -2.1993248 -2.0987861 -2.0216429 -1.9369514 -1.7821281 -1.7163763 -1.7605143 -1.7340721 -1.3705459 -0.75167346 -0.52346349 -0.90179312][-2.44873 -2.5888686 -2.6404083 -2.5672452 -2.4452806 -2.3273425 -2.2262678 -2.1069293 -2.0538225 -2.1233575 -2.1750205 -1.8765832 -1.3602815 -1.3415079 -1.8218523][-2.9846413 -3.0208588 -3.0134125 -2.9195426 -2.7883868 -2.668843 -2.56941 -2.4684782 -2.4285321 -2.5125175 -2.5963597 -2.3248751 -1.8853331 -1.9792465 -2.4464221]]...]
INFO - root - 2017-12-16 08:43:27.314354: step 19110, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:27m:07s remains)
INFO - root - 2017-12-16 08:43:28.989769: step 19120, loss = 0.51, batch loss = 0.25 (47.3 examples/sec; 0.169 sec/batch; 14h:43m:09s remains)
INFO - root - 2017-12-16 08:43:30.623534: step 19130, loss = 0.51, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 14h:15m:07s remains)
INFO - root - 2017-12-16 08:43:32.268312: step 19140, loss = 0.51, batch loss = 0.26 (46.1 examples/sec; 0.173 sec/batch; 15h:05m:43s remains)
INFO - root - 2017-12-16 08:43:33.956449: step 19150, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 14h:30m:42s remains)
INFO - root - 2017-12-16 08:43:35.626758: step 19160, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 14h:25m:24s remains)
INFO - root - 2017-12-16 08:43:37.303803: step 19170, loss = 0.49, batch loss = 0.24 (47.1 examples/sec; 0.170 sec/batch; 14h:46m:25s remains)
INFO - root - 2017-12-16 08:43:38.984877: step 19180, loss = 0.62, batch loss = 0.36 (47.1 examples/sec; 0.170 sec/batch; 14h:47m:17s remains)
INFO - root - 2017-12-16 08:43:40.642882: step 19190, loss = 0.68, batch loss = 0.42 (47.0 examples/sec; 0.170 sec/batch; 14h:48m:25s remains)
INFO - root - 2017-12-16 08:43:42.326420: step 19200, loss = 0.51, batch loss = 0.25 (45.9 examples/sec; 0.174 sec/batch; 15h:09m:11s remains)
2017-12-16 08:43:42.784329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1908751 -1.2804172 -1.4308434 -1.6566516 -1.9712555 -2.3581374 -2.7487564 -3.1010563 -3.3387535 -3.4491162 -3.3957176 -3.1616273 -2.8238592 -2.5110283 -2.2254419][-1.4498345 -1.7162893 -2.0816474 -2.5393987 -3.0583658 -3.5556769 -3.9572325 -4.3155313 -4.527535 -4.5678673 -4.4198952 -4.1043243 -3.6908584 -3.2940569 -2.8619685][-1.8537127 -2.34144 -2.9050107 -3.496671 -4.0668535 -4.423481 -4.5491352 -4.6618509 -4.7417011 -4.7298574 -4.6203294 -4.4081893 -4.0964241 -3.7750044 -3.29793][-2.3225729 -2.9829335 -3.5817449 -4.0472655 -4.328927 -4.2324791 -3.8354635 -3.5652354 -3.4957373 -3.549247 -3.6267953 -3.6811931 -3.7350113 -3.7473948 -3.3761292][-2.782629 -3.4675388 -3.8410504 -3.8417873 -3.4404857 -2.6449318 -1.6054554 -0.86952984 -0.68302512 -1.0288771 -1.5497099 -2.1028032 -2.7379436 -3.2997403 -3.1988516][-3.1029828 -3.639962 -3.5984674 -2.9243944 -1.6913559 -0.12799954 1.5383492 2.7477908 2.9234772 2.0151076 0.80812359 -0.34087682 -1.6022062 -2.6953034 -2.9348464][-3.2354372 -3.5452657 -3.0501418 -1.7314012 0.20445776 2.3323226 4.5172205 6.1747432 6.0632215 4.3711686 2.3992596 0.67156196 -1.0147606 -2.4046621 -2.839361][-3.2033658 -3.356286 -2.5849652 -0.82719123 1.4999437 3.8707361 6.3841643 8.3559265 7.0448136 4.5644369 2.2754035 0.40135789 -1.3391037 -2.6582141 -2.9924595][-3.0429759 -3.1681461 -2.393723 -0.64064729 1.5424378 3.4626598 4.990665 5.7089963 4.75982 2.701395 0.67704535 -0.94450331 -2.3107555 -3.2238057 -3.2279892][-2.8145814 -3.0666161 -2.5725944 -1.2673302 0.24666548 1.3606758 2.0778403 2.267292 1.5907905 0.088507891 -1.4267547 -2.5510476 -3.3875175 -3.7407336 -3.3628833][-2.5738964 -2.9460373 -2.8084774 -2.1027431 -1.2630317 -0.80434382 -0.67810822 -0.76427984 -1.2505746 -2.2057116 -3.1546793 -3.7407527 -4.0566988 -3.9783778 -3.3353][-2.2648492 -2.6932321 -2.8150949 -2.5841243 -2.2541964 -2.1731551 -2.3501751 -2.5789235 -2.8541121 -3.331557 -3.8157144 -4.0564332 -4.0466328 -3.7290397 -3.0369854][-1.8431773 -2.198436 -2.3835635 -2.359539 -2.3042955 -2.4030628 -2.6770253 -2.9190147 -3.0595546 -3.2248635 -3.4151354 -3.4975381 -3.4140801 -3.1086302 -2.541626][-1.4063079 -1.6064808 -1.7276199 -1.7160892 -1.7004259 -1.8010399 -2.005949 -2.2029276 -2.3076396 -2.3628378 -2.4659078 -2.5398545 -2.5167794 -2.3532879 -1.9957806][-1.1354172 -1.174324 -1.1612487 -1.0812988 -1.0179342 -1.05467 -1.1729525 -1.3193944 -1.4185787 -1.4893764 -1.6012077 -1.7047772 -1.753153 -1.7160368 -1.5573535]]...]
INFO - root - 2017-12-16 08:43:44.457960: step 19210, loss = 0.58, batch loss = 0.32 (46.9 examples/sec; 0.171 sec/batch; 14h:51m:32s remains)
INFO - root - 2017-12-16 08:43:46.117156: step 19220, loss = 0.46, batch loss = 0.20 (48.6 examples/sec; 0.165 sec/batch; 14h:20m:16s remains)
INFO - root - 2017-12-16 08:43:47.771393: step 19230, loss = 0.80, batch loss = 0.54 (46.9 examples/sec; 0.171 sec/batch; 14h:50m:35s remains)
INFO - root - 2017-12-16 08:43:49.485333: step 19240, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:40m:34s remains)
INFO - root - 2017-12-16 08:43:51.178353: step 19250, loss = 0.55, batch loss = 0.29 (48.4 examples/sec; 0.165 sec/batch; 14h:23m:38s remains)
INFO - root - 2017-12-16 08:43:52.847282: step 19260, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 14h:46m:15s remains)
INFO - root - 2017-12-16 08:43:54.511223: step 19270, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:25m:31s remains)
INFO - root - 2017-12-16 08:43:56.175044: step 19280, loss = 0.47, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 14h:28m:58s remains)
INFO - root - 2017-12-16 08:43:57.846254: step 19290, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 14h:41m:02s remains)
INFO - root - 2017-12-16 08:43:59.525745: step 19300, loss = 0.50, batch loss = 0.24 (47.1 examples/sec; 0.170 sec/batch; 14h:47m:12s remains)
2017-12-16 08:44:00.016793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4820876 -3.6686032 -3.7161498 -3.6975412 -3.5124359 -3.0105064 -2.1735451 -1.4995632 -1.494101 -2.1026163 -2.7365422 -3.199024 -3.4737625 -3.3598537 -2.8799536][-3.641813 -4.1858416 -4.5002909 -4.588007 -4.3354669 -3.5233212 -2.2868266 -1.3208755 -1.2358242 -1.8353302 -2.480603 -2.9688447 -3.2855165 -3.1607432 -2.6299739][-3.3294249 -4.3072791 -4.8913565 -5.0459986 -4.6649456 -3.5324464 -1.9166927 -0.67771387 -0.48641169 -1.1316447 -1.9832957 -2.6618242 -3.0574982 -2.8804927 -2.3069403][-2.77055 -4.1323028 -4.9019547 -5.045393 -4.5401125 -3.2007351 -1.3105092 0.140831 0.37805557 -0.38744438 -1.514421 -2.4121456 -2.8672535 -2.6656239 -2.0701048][-2.2430778 -3.7394679 -4.5466404 -4.626523 -3.9553819 -2.4148202 -0.3762728 1.1241348 1.336025 0.39422894 -0.96863246 -2.0745099 -2.6506538 -2.5913334 -2.1238167][-1.8375587 -3.1511798 -3.832427 -3.8363867 -3.0148032 -1.2566093 0.92368007 2.3168893 2.2374477 1.0715184 -0.41288841 -1.6032802 -2.3454537 -2.5420513 -2.2970998][-1.7208369 -2.6236787 -3.0675244 -2.9846389 -2.1067727 -0.24607682 2.0450144 3.4280758 3.0531597 1.6363397 0.05799222 -1.1328915 -1.9845781 -2.4375484 -2.4500718][-1.9093049 -2.3788233 -2.5636923 -2.3678987 -1.5204866 0.32073998 2.6305885 3.9872127 3.4113374 1.8640709 0.26283288 -0.88737404 -1.7541176 -2.3323069 -2.5788202][-2.2149258 -2.4123187 -2.4559953 -2.2651792 -1.5984066 -0.085526943 1.8364906 2.9419665 2.5626631 1.3356261 0.041152239 -0.92039287 -1.6199937 -2.1523142 -2.5020833][-2.4867911 -2.607708 -2.603297 -2.4668968 -2.0219767 -0.91975462 0.50264192 1.2859733 1.1426203 0.42021728 -0.43448913 -1.1037642 -1.5419537 -1.8468013 -2.1443908][-2.3230636 -2.3552568 -2.421561 -2.5017605 -2.4489717 -1.8890233 -1.0004534 -0.43170297 -0.32045722 -0.51751721 -0.86113632 -1.1958696 -1.3994222 -1.4510236 -1.5851202][-1.6864657 -1.6102898 -1.791188 -2.1904666 -2.5687766 -2.5387988 -2.1679108 -1.8151004 -1.5207376 -1.2116116 -0.9785167 -0.91126633 -0.92166519 -0.91062939 -0.9999913][-0.96224654 -0.80131066 -1.0905092 -1.7424121 -2.4080656 -2.6956046 -2.6523752 -2.4443951 -2.0454638 -1.3627594 -0.60085654 -0.13484955 -0.084098816 -0.22924328 -0.47890091][-0.61687911 -0.41949928 -0.73915839 -1.4344049 -2.1012604 -2.4196792 -2.4344177 -2.2271593 -1.7557318 -0.88402379 0.19365859 0.906791 0.92915535 0.48486018 -0.060684681][-0.83309674 -0.73675692 -0.95422792 -1.3712425 -1.7228501 -1.8330617 -1.8025445 -1.6488123 -1.2419645 -0.3975879 0.71892715 1.4800062 1.4014111 0.72005606 -0.063286781]]...]
INFO - root - 2017-12-16 08:44:01.708837: step 19310, loss = 0.50, batch loss = 0.25 (46.9 examples/sec; 0.170 sec/batch; 14h:49m:49s remains)
INFO - root - 2017-12-16 08:44:03.391042: step 19320, loss = 0.57, batch loss = 0.31 (46.1 examples/sec; 0.174 sec/batch; 15h:06m:29s remains)
INFO - root - 2017-12-16 08:44:05.078428: step 19330, loss = 0.63, batch loss = 0.37 (48.9 examples/sec; 0.163 sec/batch; 14h:13m:18s remains)
INFO - root - 2017-12-16 08:44:06.743236: step 19340, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:21m:32s remains)
INFO - root - 2017-12-16 08:44:08.418496: step 19350, loss = 0.47, batch loss = 0.21 (48.5 examples/sec; 0.165 sec/batch; 14h:20m:57s remains)
INFO - root - 2017-12-16 08:44:10.072310: step 19360, loss = 0.53, batch loss = 0.27 (49.2 examples/sec; 0.163 sec/batch; 14h:09m:18s remains)
INFO - root - 2017-12-16 08:44:11.749379: step 19370, loss = 0.57, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 14h:50m:10s remains)
INFO - root - 2017-12-16 08:44:13.424131: step 19380, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:27m:57s remains)
INFO - root - 2017-12-16 08:44:15.098820: step 19390, loss = 0.53, batch loss = 0.27 (47.5 examples/sec; 0.169 sec/batch; 14h:39m:42s remains)
INFO - root - 2017-12-16 08:44:16.755800: step 19400, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.165 sec/batch; 14h:23m:35s remains)
2017-12-16 08:44:17.202514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0822763 -1.9679499 -2.0243843 -2.126698 -2.1543074 -2.0723593 -1.9883614 -1.9265567 -1.7762225 -1.4644688 -1.2049147 -1.3112822 -1.6810259 -1.9928842 -2.1313524][-1.2812384 -1.1166699 -1.2331889 -1.4293659 -1.516167 -1.4984245 -1.4792018 -1.4623666 -1.357897 -1.0028746 -0.66145575 -0.77124774 -1.235384 -1.6048541 -1.6978717][-0.46182787 -0.25157237 -0.36231804 -0.62928116 -0.7688 -0.85965526 -0.97324312 -1.0672932 -1.0620883 -0.78944016 -0.49517083 -0.68689489 -1.25484 -1.6936617 -1.7793375][-0.059856653 0.39142489 0.36265206 0.068342447 -0.15442133 -0.35218024 -0.52696741 -0.65020204 -0.77940404 -0.71046913 -0.62050164 -0.97437859 -1.6721929 -2.1702638 -2.2692552][-0.099816084 0.69925761 0.852978 0.5369463 0.21837306 -0.047815323 -0.18455172 -0.16351414 -0.30693245 -0.4678607 -0.60965836 -1.1297853 -1.9351865 -2.5163059 -2.6991136][-0.13019466 0.81417131 1.0320983 0.68012619 0.33027363 0.10204244 0.12559056 0.36412311 0.31093931 0.069293022 -0.14580894 -0.70091355 -1.5415084 -2.2263696 -2.5335782][-0.010647297 0.93233514 1.0553043 0.58158803 0.15959525 0.064399958 0.28316402 0.7508781 0.91489744 0.78930974 0.64897323 0.24202609 -0.47626686 -1.2379774 -1.7916002][0.18636084 1.0802176 1.1136436 0.45941734 -0.12627149 -0.25215316 0.0927155 0.70692754 1.0469105 1.1374466 1.3056598 1.1615016 0.58929467 -0.24262714 -1.0013243][0.23501968 1.0914309 1.037879 0.22335291 -0.56943691 -0.79941845 -0.43352604 0.23595119 0.66335559 0.93766856 1.3530416 1.3954854 0.86155105 0.0476048 -0.70740509][-0.0976429 0.77172279 0.76964569 -0.059305191 -0.95845842 -1.2536651 -0.97246039 -0.43281972 -0.022530317 0.36527681 0.91295171 1.0284982 0.51710725 -0.28599119 -0.93587017][-0.54010224 0.33206844 0.458668 -0.27466154 -1.1480881 -1.4770796 -1.3889329 -1.0789416 -0.76039636 -0.26042032 0.377558 0.4970746 -0.039879322 -0.75977159 -1.2499901][-1.0712242 -0.13813758 0.19691706 -0.35922217 -1.1365298 -1.457119 -1.4693018 -1.3453032 -1.0983845 -0.55495918 0.079086542 0.15765834 -0.36555433 -0.9326247 -1.2066393][-1.5740923 -0.59794641 -0.077037573 -0.43749058 -1.0449635 -1.2741199 -1.3051013 -1.2650869 -1.0424929 -0.46596408 0.12314081 0.10958385 -0.41818511 -0.8254745 -0.90148652][-2.2397232 -1.23609 -0.58761454 -0.75718868 -1.2191662 -1.3616804 -1.3502603 -1.3351767 -1.1678687 -0.67012393 -0.17781544 -0.21361828 -0.63084853 -0.87743151 -0.87235761][-3.1181638 -2.255456 -1.6310818 -1.6586235 -1.9887803 -2.0847056 -2.0569274 -2.0454464 -1.9481381 -1.609853 -1.2554196 -1.2600106 -1.4698159 -1.5158241 -1.424825]]...]
INFO - root - 2017-12-16 08:44:18.871542: step 19410, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 14h:38m:33s remains)
INFO - root - 2017-12-16 08:44:20.536591: step 19420, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:42m:22s remains)
INFO - root - 2017-12-16 08:44:22.231446: step 19430, loss = 0.67, batch loss = 0.42 (47.4 examples/sec; 0.169 sec/batch; 14h:41m:26s remains)
INFO - root - 2017-12-16 08:44:23.926650: step 19440, loss = 0.57, batch loss = 0.31 (47.4 examples/sec; 0.169 sec/batch; 14h:41m:32s remains)
INFO - root - 2017-12-16 08:44:25.558962: step 19450, loss = 0.48, batch loss = 0.22 (49.2 examples/sec; 0.163 sec/batch; 14h:08m:35s remains)
INFO - root - 2017-12-16 08:44:27.288454: step 19460, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:31m:59s remains)
INFO - root - 2017-12-16 08:44:28.978091: step 19470, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 14h:29m:29s remains)
INFO - root - 2017-12-16 08:44:30.670749: step 19480, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 15h:04m:17s remains)
INFO - root - 2017-12-16 08:44:32.366730: step 19490, loss = 0.46, batch loss = 0.20 (48.5 examples/sec; 0.165 sec/batch; 14h:20m:14s remains)
INFO - root - 2017-12-16 08:44:34.038645: step 19500, loss = 0.48, batch loss = 0.22 (47.2 examples/sec; 0.169 sec/batch; 14h:43m:27s remains)
2017-12-16 08:44:34.513931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0450728 -1.6951154 -1.3351918 -0.92539835 -0.70137358 -0.90005362 -1.4388332 -1.8308377 -1.7508025 -1.5610254 -1.6911864 -1.8445637 -1.7853564 -1.6109059 -1.4398664][-1.6461828 -1.2358303 -0.984349 -0.76976275 -0.654811 -0.98597336 -1.7976542 -2.483609 -2.6839256 -2.7885149 -3.0346737 -3.1146507 -2.8256831 -2.4425702 -2.150403][-1.2930526 -1.0437168 -1.0791636 -1.1109092 -1.1244048 -1.5263064 -2.3455083 -2.9919093 -3.2179248 -3.4107487 -3.7289524 -3.7631655 -3.3978477 -2.9342282 -2.5874484][-0.99477577 -1.0335609 -1.3143215 -1.5659454 -1.7302458 -2.0070629 -2.4990916 -2.8004775 -2.8395553 -2.9944916 -3.3302014 -3.478322 -3.2911525 -2.947489 -2.6114864][-0.98127174 -1.2351817 -1.5696273 -1.8616995 -1.995383 -1.8929735 -1.6287279 -1.3392932 -1.11247 -1.307368 -1.7921765 -2.2464395 -2.4692476 -2.4578278 -2.2490423][-1.1208271 -1.3649459 -1.6064975 -1.7735946 -1.635008 -0.96172321 0.1543541 1.1500611 1.5162113 1.0024736 0.11051965 -0.80807424 -1.5271318 -1.9381163 -1.9094386][-1.1577361 -1.1766984 -1.2033858 -1.1877112 -0.721146 0.46474028 2.3000839 3.8819067 4.0094528 2.7696335 1.3552995 0.096456766 -0.96469831 -1.6133002 -1.7402632][-0.89908206 -0.63982403 -0.42857122 -0.23867989 0.43183041 1.7715361 3.8181593 5.6785955 5.0829773 3.1182225 1.4235539 0.13071179 -0.90767217 -1.5722228 -1.6939392][-0.57877624 -0.13602948 0.14529848 0.36529422 0.92499781 1.9639466 3.368408 4.3595953 3.560745 1.8694551 0.4314599 -0.55443752 -1.3413799 -1.7981355 -1.8343024][-0.46750534 -4.7922134e-05 0.18825531 0.22924209 0.43913555 0.9234035 1.5546904 1.8366072 1.1961145 0.071357012 -0.88364136 -1.5184815 -1.9750738 -2.1696754 -2.0416079][-0.67268825 -0.334862 -0.33819103 -0.55445683 -0.69476223 -0.69008791 -0.52119279 -0.4625715 -0.83692777 -1.442183 -2.01937 -2.3859019 -2.5553327 -2.4888039 -2.2377687][-1.0627648 -0.95405519 -1.1633209 -1.4952164 -1.7867167 -1.9440728 -1.9256172 -1.9082254 -2.0035925 -2.2300713 -2.5267115 -2.7360816 -2.7380071 -2.5492053 -2.290365][-1.3721689 -1.4938909 -1.7715503 -2.0314741 -2.1679511 -2.2210031 -2.1736302 -2.0982733 -2.0380545 -2.1567588 -2.3452668 -2.4756775 -2.4531703 -2.310138 -2.1847839][-1.4210722 -1.6626917 -1.8740171 -1.9432547 -1.8185204 -1.6570667 -1.5430057 -1.4193659 -1.4030206 -1.5691781 -1.8109409 -1.9714795 -2.0053589 -1.9545527 -1.9565203][-1.1802161 -1.4086442 -1.5016758 -1.3630011 -0.98681855 -0.67282271 -0.53188848 -0.49290287 -0.6858139 -1.0444229 -1.3899817 -1.5896482 -1.6318197 -1.6098962 -1.6695698]]...]
INFO - root - 2017-12-16 08:44:36.254794: step 19510, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 14h:23m:47s remains)
INFO - root - 2017-12-16 08:44:37.920797: step 19520, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 14h:22m:02s remains)
INFO - root - 2017-12-16 08:44:39.588133: step 19530, loss = 0.54, batch loss = 0.28 (49.2 examples/sec; 0.163 sec/batch; 14h:08m:06s remains)
INFO - root - 2017-12-16 08:44:41.255030: step 19540, loss = 0.68, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 14h:34m:51s remains)
INFO - root - 2017-12-16 08:44:42.905759: step 19550, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:33m:58s remains)
INFO - root - 2017-12-16 08:44:44.593590: step 19560, loss = 0.56, batch loss = 0.30 (45.6 examples/sec; 0.175 sec/batch; 15h:14m:19s remains)
INFO - root - 2017-12-16 08:44:46.291733: step 19570, loss = 0.63, batch loss = 0.37 (45.2 examples/sec; 0.177 sec/batch; 15h:23m:22s remains)
INFO - root - 2017-12-16 08:44:47.985869: step 19580, loss = 0.50, batch loss = 0.25 (46.6 examples/sec; 0.172 sec/batch; 14h:54m:37s remains)
INFO - root - 2017-12-16 08:44:49.646998: step 19590, loss = 0.59, batch loss = 0.33 (46.2 examples/sec; 0.173 sec/batch; 15h:02m:50s remains)
INFO - root - 2017-12-16 08:44:51.318414: step 19600, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:26m:53s remains)
2017-12-16 08:44:51.818534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.70367587 -1.5654387 -2.2528594 -2.5478468 -2.4865365 -2.3502998 -2.3303919 -2.4450552 -2.6879766 -2.9323382 -3.0598552 -2.9943326 -2.8226361 -2.6227863 -2.3558629][-0.14126301 -0.91472363 -1.4871011 -1.7640121 -1.8179753 -1.8585382 -2.0101752 -2.226867 -2.5287161 -2.8137 -3.0135298 -3.104403 -3.1183014 -3.05655 -2.859592][0.21659327 -0.29824996 -0.76548696 -1.0377839 -1.1667305 -1.2787504 -1.452471 -1.6542861 -1.9567598 -2.2776821 -2.5460711 -2.7513323 -2.993217 -3.1522613 -3.081177][0.38439727 0.091686726 -0.24552631 -0.46478724 -0.51437581 -0.49517965 -0.53714311 -0.65417814 -1.0192066 -1.454589 -1.8222016 -2.1387472 -2.5642264 -2.88856 -2.9185095][0.34750414 0.23915696 0.072244406 -0.0089309216 0.15202951 0.47788453 0.67500424 0.54938793 0.030222178 -0.57399189 -1.0286463 -1.3471042 -1.8834282 -2.3564203 -2.4797523][-0.026299238 0.078700781 0.18411207 0.33390617 0.71120238 1.332377 1.8283014 1.7497959 1.0469449 0.35045671 -0.097166538 -0.3678925 -0.967149 -1.5741844 -1.8328307][-0.41667271 -0.16903949 0.15544963 0.50421691 1.0815454 1.9991837 2.9399896 3.0926371 2.253561 1.4453514 1.0611453 0.84990597 0.21298051 -0.53369677 -1.0637963][-0.62164307 -0.44465005 -0.1271286 0.26297975 0.89453244 1.9367042 3.253685 3.7702494 2.9631581 2.1987443 1.917851 1.7621908 1.1199734 0.31883526 -0.33265615][-0.86318481 -0.8507489 -0.69612396 -0.429497 0.069341421 0.89759779 1.9859405 2.5361037 2.1925607 1.7818928 1.7609935 1.8060551 1.3468659 0.69999766 0.1448369][-1.1384357 -1.2539792 -1.2507421 -1.1473404 -0.86870933 -0.35860085 0.35786319 0.83892679 0.84470272 0.81593633 1.0480664 1.2922423 1.0978417 0.64054441 0.2606256][-1.2364982 -1.4322078 -1.5297486 -1.5645573 -1.4826474 -1.2377247 -0.84122705 -0.48539162 -0.28150511 -0.010000944 0.38387752 0.70732784 0.60671496 0.28366303 0.087206125][-1.143732 -1.3426149 -1.4418302 -1.5118117 -1.5763559 -1.5678926 -1.4703977 -1.2928892 -1.0517921 -0.66401279 -0.20351696 0.10433364 -0.013156414 -0.2703526 -0.32628632][-1.0993341 -1.1634988 -1.1551642 -1.1704159 -1.2507191 -1.327142 -1.3810854 -1.3660929 -1.2428435 -0.95107508 -0.61560214 -0.48250723 -0.65822816 -0.87167776 -0.79637504][-1.2573248 -1.1621716 -0.99116611 -0.86451018 -0.825469 -0.80943561 -0.84685683 -0.92894721 -0.96178424 -0.8857429 -0.79497743 -0.89521694 -1.1311886 -1.3032842 -1.1509953][-1.4829931 -1.3591609 -1.1388754 -0.89326465 -0.67695177 -0.48718488 -0.45474017 -0.57795274 -0.72606635 -0.8050487 -0.87278163 -1.0876738 -1.3503416 -1.4788253 -1.2664381]]...]
INFO - root - 2017-12-16 08:44:53.509457: step 19610, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 14h:27m:17s remains)
INFO - root - 2017-12-16 08:44:55.184154: step 19620, loss = 0.54, batch loss = 0.28 (46.4 examples/sec; 0.173 sec/batch; 14h:59m:51s remains)
INFO - root - 2017-12-16 08:44:56.875648: step 19630, loss = 0.52, batch loss = 0.26 (46.1 examples/sec; 0.174 sec/batch; 15h:04m:48s remains)
INFO - root - 2017-12-16 08:44:58.549303: step 19640, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 14h:10m:28s remains)
INFO - root - 2017-12-16 08:45:00.221637: step 19650, loss = 0.47, batch loss = 0.21 (48.8 examples/sec; 0.164 sec/batch; 14h:15m:06s remains)
INFO - root - 2017-12-16 08:45:01.894533: step 19660, loss = 0.61, batch loss = 0.35 (46.2 examples/sec; 0.173 sec/batch; 15h:02m:42s remains)
INFO - root - 2017-12-16 08:45:03.548023: step 19670, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:19m:02s remains)
INFO - root - 2017-12-16 08:45:05.186355: step 19680, loss = 0.60, batch loss = 0.34 (48.4 examples/sec; 0.165 sec/batch; 14h:21m:46s remains)
INFO - root - 2017-12-16 08:45:06.856398: step 19690, loss = 0.62, batch loss = 0.36 (46.9 examples/sec; 0.171 sec/batch; 14h:49m:10s remains)
INFO - root - 2017-12-16 08:45:08.569704: step 19700, loss = 0.46, batch loss = 0.20 (46.5 examples/sec; 0.172 sec/batch; 14h:57m:13s remains)
2017-12-16 08:45:09.054460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0970207 -2.1475933 -2.6544378 -2.5537894 -2.3586719 -2.3123615 -2.0760314 -1.5454302 -1.0516406 -0.89695728 -1.0767391 -1.3286827 -1.4079753 -1.2540735 -1.3760424][-1.1828525 -2.1586912 -2.7347939 -2.7611825 -2.6617897 -2.6368644 -2.425971 -1.9839957 -1.556824 -1.4177918 -1.5564426 -1.7411728 -1.7221895 -1.4779308 -1.5262611][-1.6720275 -2.3980482 -2.9026914 -3.0012341 -2.9576013 -2.8616364 -2.6376882 -2.3444567 -2.1367681 -2.1827569 -2.3898568 -2.5332677 -2.4157012 -2.0413036 -1.8866391][-2.1683125 -2.621031 -2.9943435 -3.0324664 -2.8826959 -2.5666301 -2.2082911 -2.0144205 -2.1017957 -2.4042897 -2.7305207 -2.9330909 -2.8024189 -2.3338184 -1.8997992][-2.3484726 -2.5953007 -2.7849288 -2.6465194 -2.1884823 -1.4593726 -0.77859068 -0.60014296 -1.0246996 -1.7062889 -2.2668192 -2.6051295 -2.4931307 -1.9081897 -1.223709][-2.2505462 -2.3120248 -2.2765672 -1.8746612 -1.0099577 0.30112076 1.4687777 1.6918898 0.79979372 -0.42244625 -1.3006147 -1.7616923 -1.6939578 -1.129006 -0.37802017][-1.9106178 -1.8624997 -1.6574657 -1.0693387 0.15791321 2.0804176 3.8525558 4.0936275 2.5264373 0.66200089 -0.51949704 -1.0259961 -1.0148516 -0.59384263 -0.047894955][-1.6042122 -1.5258021 -1.2977868 -0.72742283 0.54617953 2.5924644 4.4287133 4.4374409 2.5371866 0.47136116 -0.70666671 -1.1277983 -1.1443789 -0.86156762 -0.50374949][-1.6245333 -1.5582937 -1.3388163 -0.90953994 0.10186172 1.7437081 3.076982 2.787447 0.94321966 -0.87452066 -1.8122444 -2.0719719 -2.0772192 -1.9023535 -1.588369][-1.8655162 -1.80057 -1.5822809 -1.2237154 -0.46017194 0.71558833 1.5261092 0.98561239 -0.72152746 -2.250464 -2.9315445 -3.0495379 -3.0291822 -2.9119921 -2.5994854][-2.0171309 -1.9116943 -1.6402535 -1.2669023 -0.66640639 0.13597631 0.57099748 -0.068888664 -1.6177315 -2.9502254 -3.4597397 -3.4609423 -3.4089074 -3.3413095 -3.1120124][-1.9429688 -1.7889196 -1.3707845 -0.9046607 -0.39915216 0.11212873 0.24775004 -0.50239325 -1.9363729 -3.092278 -3.4398472 -3.2597804 -3.1116972 -3.0648115 -2.9955902][-1.8160008 -1.5907116 -0.98397875 -0.3473103 0.19068098 0.57044816 0.54125714 -0.35513496 -1.8296952 -2.8974743 -3.1010389 -2.7742915 -2.5122283 -2.4596214 -2.5528939][-2.0111971 -1.7441964 -1.0098307 -0.21915841 0.41884184 0.82748437 0.74108148 -0.2355144 -1.70743 -2.7184074 -2.8179495 -2.3885486 -2.0641263 -2.0057406 -2.1533408][-2.7292333 -2.4573026 -1.7107397 -0.91311479 -0.29531908 0.07874918 0.016637087 -0.78781354 -1.9644229 -2.7844481 -2.7985854 -2.3273702 -1.979517 -1.900466 -2.005338]]...]
INFO - root - 2017-12-16 08:45:10.739831: step 19710, loss = 0.55, batch loss = 0.29 (49.0 examples/sec; 0.163 sec/batch; 14h:10m:25s remains)
INFO - root - 2017-12-16 08:45:12.415851: step 19720, loss = 0.67, batch loss = 0.42 (47.4 examples/sec; 0.169 sec/batch; 14h:39m:02s remains)
INFO - root - 2017-12-16 08:45:14.066027: step 19730, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 14h:21m:45s remains)
INFO - root - 2017-12-16 08:45:15.740804: step 19740, loss = 0.50, batch loss = 0.24 (48.0 examples/sec; 0.167 sec/batch; 14h:28m:11s remains)
INFO - root - 2017-12-16 08:45:17.468862: step 19750, loss = 0.56, batch loss = 0.30 (43.7 examples/sec; 0.183 sec/batch; 15h:54m:39s remains)
INFO - root - 2017-12-16 08:45:19.119414: step 19760, loss = 0.62, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:20m:53s remains)
INFO - root - 2017-12-16 08:45:20.775444: step 19770, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 14h:29m:37s remains)
INFO - root - 2017-12-16 08:45:22.443817: step 19780, loss = 0.65, batch loss = 0.40 (49.3 examples/sec; 0.162 sec/batch; 14h:05m:50s remains)
INFO - root - 2017-12-16 08:45:24.129705: step 19790, loss = 0.45, batch loss = 0.19 (47.9 examples/sec; 0.167 sec/batch; 14h:31m:16s remains)
INFO - root - 2017-12-16 08:45:25.832831: step 19800, loss = 0.53, batch loss = 0.27 (46.5 examples/sec; 0.172 sec/batch; 14h:57m:15s remains)
2017-12-16 08:45:26.322593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75246131 -1.0400412 -1.3320425 -1.4994817 -1.5090293 -1.3956752 -1.2948521 -1.4772409 -1.6686435 -1.8723983 -1.9561536 -1.6880982 -1.1895005 -0.79952037 -0.62035263][-0.071626425 -0.38457561 -0.71738052 -0.95057356 -0.95027626 -0.76921177 -0.65395641 -0.83296943 -1.0012341 -1.1842864 -1.2681606 -1.0806578 -0.70098376 -0.5148977 -0.51615131][0.015996695 -0.32685781 -0.67814541 -0.91170394 -0.91364169 -0.72173238 -0.58594573 -0.66017222 -0.73074794 -0.78379118 -0.76312339 -0.5764575 -0.32167292 -0.24618316 -0.25358868][-0.014600992 -0.29789281 -0.58349586 -0.76525366 -0.72468138 -0.54493904 -0.37822616 -0.40452063 -0.40204144 -0.44616222 -0.41182184 -0.18193841 0.051369667 0.080938578 0.1404779][-0.033663511 -0.32219458 -0.51289093 -0.58834529 -0.53089726 -0.36495948 -0.2033031 -0.16025972 -0.084748268 -0.083365679 0.01530695 0.32407665 0.57937479 0.69786525 0.81523442][-0.15806341 -0.44685519 -0.60315275 -0.64811885 -0.56997228 -0.36754656 -0.15334249 0.0153234 0.16040611 0.20879245 0.33834362 0.787411 1.2673872 1.5329695 1.7998602][-0.46066427 -0.798995 -0.95446813 -0.98818862 -0.8432219 -0.57909405 -0.21310711 0.083781719 0.2416048 0.28581333 0.48113751 1.0988388 1.8245132 2.2836964 2.6216662][-0.62137032 -1.0443388 -1.3112181 -1.2802777 -0.989743 -0.56721151 -0.042748213 0.33122063 0.45893359 0.45811462 0.64331675 1.2425082 1.9526322 2.3901846 2.649761][-0.86375105 -1.2990849 -1.4913691 -1.2287816 -0.71984267 -0.11168575 0.57665825 0.93413091 0.94095278 0.84754252 0.98121381 1.3859172 1.883116 2.1189864 2.1670067][-1.1338388 -1.4705617 -1.4909998 -1.0449727 -0.37317657 0.33004618 1.0505769 1.3419302 1.1904473 1.0028236 1.0296495 1.3035805 1.6548512 1.7893026 1.6743567][-1.2801741 -1.5855086 -1.5765376 -1.0605364 -0.40134239 0.25859451 0.85241604 1.0098262 0.71447253 0.37317514 0.28998184 0.52552581 0.85860777 0.95858145 0.8003428][-1.4332995 -1.8426571 -1.8869972 -1.4912779 -0.97567093 -0.48620772 -0.052800417 0.012911081 -0.37090683 -0.81116569 -0.94621 -0.74325 -0.43234456 -0.33379579 -0.49116075][-1.7587054 -2.232578 -2.3141716 -2.0186803 -1.6779664 -1.4137614 -1.1962055 -1.2078996 -1.5849824 -2.0121324 -2.1184633 -1.9233396 -1.7008724 -1.6511467 -1.7833867][-2.183918 -2.6184785 -2.7373385 -2.5393348 -2.3604295 -2.3262904 -2.3310428 -2.4168069 -2.6928008 -2.9674933 -3.0318298 -2.8934352 -2.7365749 -2.6955192 -2.7741067][-2.5354156 -2.9695663 -3.1562479 -3.0844619 -3.0685141 -3.2345953 -3.3954167 -3.5252008 -3.6787822 -3.7920704 -3.8021531 -3.7140298 -3.6330514 -3.5914495 -3.620563]]...]
INFO - root - 2017-12-16 08:45:28.022328: step 19810, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:32m:19s remains)
INFO - root - 2017-12-16 08:45:29.720842: step 19820, loss = 0.66, batch loss = 0.40 (46.7 examples/sec; 0.171 sec/batch; 14h:51m:59s remains)
INFO - root - 2017-12-16 08:45:31.386941: step 19830, loss = 0.60, batch loss = 0.34 (49.1 examples/sec; 0.163 sec/batch; 14h:09m:47s remains)
INFO - root - 2017-12-16 08:45:33.070490: step 19840, loss = 0.65, batch loss = 0.40 (47.0 examples/sec; 0.170 sec/batch; 14h:46m:42s remains)
INFO - root - 2017-12-16 08:45:34.751280: step 19850, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 14h:28m:51s remains)
INFO - root - 2017-12-16 08:45:36.415966: step 19860, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 14h:19m:13s remains)
INFO - root - 2017-12-16 08:45:38.070695: step 19870, loss = 0.56, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 14h:55m:52s remains)
INFO - root - 2017-12-16 08:45:39.761751: step 19880, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 14h:13m:42s remains)
INFO - root - 2017-12-16 08:45:41.451432: step 19890, loss = 0.63, batch loss = 0.37 (46.4 examples/sec; 0.172 sec/batch; 14h:57m:34s remains)
INFO - root - 2017-12-16 08:45:43.143818: step 19900, loss = 0.64, batch loss = 0.39 (47.2 examples/sec; 0.170 sec/batch; 14h:43m:45s remains)
2017-12-16 08:45:43.640594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3130474 -2.5866494 -2.7132964 -2.5506055 -2.2361465 -2.0618732 -2.0327349 -2.1093419 -2.1387463 -2.2026546 -2.3393838 -2.47767 -2.5233173 -2.4330318 -2.2790155][-2.5546513 -2.8068364 -2.8515661 -2.5308869 -2.0255451 -1.6752622 -1.5600042 -1.6579142 -1.8187408 -2.0383284 -2.3334589 -2.5714321 -2.6845765 -2.608676 -2.3994813][-2.5623763 -2.7782693 -2.7545846 -2.3390439 -1.7257392 -1.2537433 -1.1155087 -1.3071216 -1.6468599 -2.0069683 -2.4023504 -2.7511854 -2.949224 -2.8771732 -2.5529547][-2.3415613 -2.4997866 -2.473968 -2.07402 -1.47379 -0.98572528 -0.87067533 -1.1502515 -1.6112356 -2.0649028 -2.5274816 -2.9435058 -3.1394508 -2.9819889 -2.5172656][-1.96693 -2.0255525 -2.0036128 -1.6881373 -1.167949 -0.71364582 -0.61571991 -0.90897894 -1.4215192 -1.9690681 -2.4794195 -2.8536549 -2.9501445 -2.6827669 -2.1558547][-1.5240352 -1.4278661 -1.4027789 -1.2000539 -0.78119874 -0.34324217 -0.17863035 -0.36886072 -0.8632282 -1.522476 -2.0940282 -2.3698225 -2.3268189 -2.0227892 -1.5910103][-1.1441352 -0.92555773 -0.922889 -0.82452655 -0.46037436 0.024651289 0.33871436 0.28447604 -0.23349833 -0.9962883 -1.6258986 -1.8493004 -1.7287581 -1.4753189 -1.2132758][-0.99485552 -0.74082816 -0.80485892 -0.80158746 -0.45315945 0.091872931 0.49764252 0.48571682 -0.056828022 -0.86025107 -1.5094385 -1.7296402 -1.6067771 -1.4132199 -1.2364239][-1.1977917 -1.0228714 -1.1414338 -1.187597 -0.87849534 -0.34118938 0.05852294 0.081745148 -0.33569193 -1.0335189 -1.6723545 -1.9578046 -1.8910928 -1.7063982 -1.4753957][-1.5065104 -1.4293528 -1.5830927 -1.6719007 -1.4286267 -0.97673464 -0.61309934 -0.54305685 -0.81930304 -1.3917289 -1.9835157 -2.3045602 -2.2938449 -2.0697043 -1.7381973][-1.7270842 -1.6863906 -1.8591936 -1.9625717 -1.7898526 -1.4564903 -1.1923194 -1.1632828 -1.3840525 -1.866605 -2.3693151 -2.6512451 -2.6349993 -2.3701625 -1.9647251][-1.7918112 -1.7502778 -1.9007485 -2.000576 -1.9060972 -1.7270229 -1.6218256 -1.6928723 -1.9261121 -2.3125653 -2.6616192 -2.832418 -2.7560368 -2.4586418 -2.0644503][-1.866514 -1.842684 -1.9679852 -2.060967 -2.0322237 -1.96399 -1.971608 -2.0820808 -2.2711561 -2.4992242 -2.6694996 -2.7155323 -2.5885105 -2.3252056 -2.0389867][-2.0303309 -2.0867739 -2.2359 -2.3211458 -2.2947681 -2.2196693 -2.2065287 -2.2562449 -2.3292153 -2.3905666 -2.4253187 -2.3851237 -2.263953 -2.1089683 -1.9786508][-2.1466832 -2.2974367 -2.5013041 -2.5940907 -2.5074332 -2.3423705 -2.2340863 -2.2023809 -2.1881921 -2.1473994 -2.1129112 -2.0721846 -2.0038903 -1.9391879 -1.8847407]]...]
INFO - root - 2017-12-16 08:45:45.295024: step 19910, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 14h:25m:58s remains)
INFO - root - 2017-12-16 08:45:46.920850: step 19920, loss = 0.51, batch loss = 0.25 (49.7 examples/sec; 0.161 sec/batch; 13h:57m:52s remains)
INFO - root - 2017-12-16 08:45:48.608804: step 19930, loss = 0.73, batch loss = 0.47 (47.4 examples/sec; 0.169 sec/batch; 14h:38m:38s remains)
INFO - root - 2017-12-16 08:45:50.286577: step 19940, loss = 0.61, batch loss = 0.35 (47.2 examples/sec; 0.169 sec/batch; 14h:42m:38s remains)
INFO - root - 2017-12-16 08:45:52.003694: step 19950, loss = 0.59, batch loss = 0.33 (45.7 examples/sec; 0.175 sec/batch; 15h:12m:47s remains)
INFO - root - 2017-12-16 08:45:53.689392: step 19960, loss = 0.63, batch loss = 0.37 (46.6 examples/sec; 0.172 sec/batch; 14h:54m:04s remains)
INFO - root - 2017-12-16 08:45:55.389140: step 19970, loss = 0.57, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:31m:24s remains)
INFO - root - 2017-12-16 08:45:57.057438: step 19980, loss = 0.61, batch loss = 0.36 (46.6 examples/sec; 0.172 sec/batch; 14h:53m:36s remains)
INFO - root - 2017-12-16 08:45:58.712975: step 19990, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 14h:11m:11s remains)
INFO - root - 2017-12-16 08:46:00.369527: step 20000, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:31m:03s remains)
2017-12-16 08:46:00.844312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2186053 -1.139564 -0.88994789 -0.7793448 -1.1515726 -1.7339506 -1.9015228 -1.5942061 -1.3652097 -1.3941764 -1.2516158 -0.78271556 -0.57795191 -0.83613563 -1.2371353][-1.4083645 -1.5175219 -1.3487796 -1.2389448 -1.594135 -2.1893978 -2.3169677 -1.9103155 -1.6363323 -1.6641531 -1.5556622 -1.0634407 -0.72250807 -0.77157307 -0.97713768][-1.6088412 -1.9309852 -1.969883 -1.9744394 -2.3116555 -2.823772 -2.9196248 -2.572556 -2.379674 -2.5096157 -2.5536468 -2.217845 -1.774758 -1.54345 -1.4278483][-1.4461889 -1.9015219 -2.1334155 -2.2088437 -2.4966047 -2.9261217 -3.0526037 -2.8966603 -2.8484683 -3.1381702 -3.4360795 -3.3570266 -2.9521823 -2.4731128 -2.1027906][-0.92903423 -1.3090321 -1.5506501 -1.521524 -1.6219513 -1.9574338 -2.2374904 -2.3059754 -2.4213965 -2.8991752 -3.4899108 -3.7347069 -3.4973755 -3.0009685 -2.5551689][-0.45384789 -0.53704107 -0.569823 -0.2628386 0.019323111 -0.086566687 -0.42208695 -0.68987405 -0.93713415 -1.4895439 -2.2873049 -2.8535595 -2.9216609 -2.6577797 -2.3757749][-0.47687483 -0.22054172 -0.012624502 0.53589535 1.2404373 1.6024101 1.4185126 1.0632043 0.80712724 0.37982011 -0.40840304 -1.1857665 -1.5333915 -1.541971 -1.4694967][-0.89710259 -0.41310847 -0.051584721 0.57367873 1.5555878 2.348289 2.3672194 1.9465389 1.6891236 1.5083761 0.920449 0.086073875 -0.41076517 -0.57829618 -0.54112709][-1.3124813 -0.89452493 -0.61448967 -0.12118578 0.75953555 1.6058075 1.7865357 1.3886957 1.1473894 1.1900091 0.86075807 0.15790415 -0.34602714 -0.54666114 -0.41374123][-1.4330751 -1.3182896 -1.3525162 -1.2767394 -0.79916203 -0.16050124 -0.019066095 -0.37511241 -0.59919631 -0.49002767 -0.58175194 -0.98193955 -1.2742158 -1.332375 -1.0305694][-1.1803344 -1.41735 -1.8289397 -2.2469647 -2.2922022 -2.0667076 -2.1209838 -2.4756818 -2.6757915 -2.5423253 -2.4323487 -2.4752057 -2.4756374 -2.3991618 -1.9940935][-0.86544168 -1.3244276 -2.0253556 -2.8728366 -3.3934908 -3.5831089 -3.8259559 -4.1547112 -4.3049917 -4.1333122 -3.8361397 -3.5624762 -3.3279562 -3.1010835 -2.6384594][-0.73428643 -1.2564068 -2.1034584 -3.1708062 -3.9786792 -4.472477 -4.8384748 -5.0998278 -5.147572 -4.8843126 -4.3947067 -3.8958778 -3.5025082 -3.180037 -2.7573543][-0.77287042 -1.2380934 -2.0968966 -3.1963568 -4.0440788 -4.5983553 -4.9491014 -5.1165829 -5.0147667 -4.5964031 -3.9700365 -3.3818738 -2.9012277 -2.5216153 -2.2365134][-0.7174623 -1.0959139 -1.9044223 -2.9225688 -3.6215096 -4.03242 -4.2850351 -4.3350453 -4.0492973 -3.4669437 -2.852469 -2.3248541 -1.8093013 -1.3870156 -1.2496517]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:46:03.029858: step 20010, loss = 0.48, batch loss = 0.22 (47.6 examples/sec; 0.168 sec/batch; 14h:35m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:46:04.718474: step 20020, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 14h:36m:26s remains)
INFO - root - 2017-12-16 08:46:06.374835: step 20030, loss = 0.55, batch loss = 0.29 (49.1 examples/sec; 0.163 sec/batch; 14h:08m:42s remains)
INFO - root - 2017-12-16 08:46:08.017725: step 20040, loss = 0.47, batch loss = 0.21 (47.9 examples/sec; 0.167 sec/batch; 14h:30m:31s remains)
INFO - root - 2017-12-16 08:46:09.700175: step 20050, loss = 0.73, batch loss = 0.47 (47.0 examples/sec; 0.170 sec/batch; 14h:46m:00s remains)
INFO - root - 2017-12-16 08:46:11.397742: step 20060, loss = 0.59, batch loss = 0.33 (47.2 examples/sec; 0.170 sec/batch; 14h:43m:02s remains)
INFO - root - 2017-12-16 08:46:13.076818: step 20070, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 14h:20m:11s remains)
INFO - root - 2017-12-16 08:46:14.732656: step 20080, loss = 0.49, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:17m:02s remains)
INFO - root - 2017-12-16 08:46:16.397448: step 20090, loss = 0.63, batch loss = 0.37 (48.1 examples/sec; 0.166 sec/batch; 14h:26m:09s remains)
INFO - root - 2017-12-16 08:46:18.053657: step 20100, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 14h:10m:15s remains)
2017-12-16 08:46:18.496572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9257567 -2.8971477 -2.5376294 -1.8941623 -1.1062613 -0.60887086 -0.5329771 -0.74083424 -1.3733969 -1.9452648 -2.1507454 -2.4221313 -2.6820312 -2.7239063 -2.6728191][-3.0719278 -3.0427446 -2.7044857 -1.9826707 -1.1125711 -0.61923659 -0.56508887 -0.77912891 -1.3401692 -1.8235843 -2.0002239 -2.2690339 -2.5262375 -2.5883574 -2.6063473][-3.1837449 -3.1198494 -2.7703009 -2.0004113 -1.1251045 -0.59759772 -0.50557554 -0.66292703 -1.0070523 -1.3930812 -1.5981823 -1.9573672 -2.3139858 -2.4468296 -2.5525818][-3.1854856 -2.9765306 -2.5260258 -1.696614 -0.85554039 -0.31377149 -0.092047453 -0.12524629 -0.2997992 -0.62130082 -0.88349104 -1.3996179 -1.9659429 -2.2568674 -2.4855714][-2.9638617 -2.5504651 -2.0101283 -1.2216405 -0.48440146 0.032326937 0.40389204 0.65350318 0.69885254 0.47017694 0.10532212 -0.62779784 -1.5087856 -2.0520916 -2.400835][-2.5059505 -1.9981139 -1.4256104 -0.75070381 -0.10838318 0.33756423 0.93163466 1.6314943 1.9687812 1.7495954 1.1583991 0.087964535 -1.1199913 -1.8873968 -2.3177936][-1.8650236 -1.2927648 -0.74798882 -0.13587046 0.41634965 0.9302721 1.8123629 2.8917639 3.4587815 3.1619627 2.1739833 0.719614 -0.79300225 -1.7518473 -2.2342691][-1.1828309 -0.51873863 0.026438236 0.5979147 1.0491774 1.5729632 2.5385811 3.7179391 4.3075542 3.9499996 2.7128046 1.0347092 -0.62908936 -1.7013098 -2.1980245][-0.75591409 -0.053893805 0.37007093 0.76643634 1.0762722 1.5307224 2.4669626 3.6926801 4.3215246 3.9801257 2.6851842 0.96069527 -0.73966968 -1.835812 -2.2884402][-0.70024574 -0.10601163 0.072707891 0.23855734 0.43497324 0.8551414 1.7733619 3.0560396 3.6809189 3.3469727 2.1706016 0.50621748 -1.0910343 -2.0878015 -2.455164][-0.94133794 -0.56261551 -0.59522033 -0.58355379 -0.40319514 0.063033104 0.93506622 2.0847862 2.532542 2.1421607 1.1047039 -0.33775449 -1.6429024 -2.4099326 -2.6494803][-1.332036 -1.1862772 -1.3287239 -1.3861225 -1.2064083 -0.76813734 -0.13890624 0.62409544 0.93592715 0.626493 -0.15411973 -1.2388219 -2.1634805 -2.64931 -2.7461529][-1.7933524 -1.823227 -1.9682522 -2.0918226 -1.9982085 -1.7688198 -1.4288048 -0.95779431 -0.71803892 -0.85614979 -1.3038852 -1.9915788 -2.5499132 -2.7637906 -2.7178187][-2.1705403 -2.2858474 -2.4362247 -2.595346 -2.6315069 -2.5636704 -2.3636055 -1.9998902 -1.7308255 -1.767391 -2.0103838 -2.4296041 -2.7294221 -2.7460241 -2.6006145][-2.3182845 -2.4459715 -2.5802104 -2.7242548 -2.8185475 -2.8220689 -2.6539297 -2.336946 -2.1063943 -2.1351244 -2.2942796 -2.5456934 -2.6706371 -2.5864041 -2.4032204]]...]
INFO - root - 2017-12-16 08:46:20.185777: step 20110, loss = 0.65, batch loss = 0.39 (47.1 examples/sec; 0.170 sec/batch; 14h:43m:32s remains)
INFO - root - 2017-12-16 08:46:21.846205: step 20120, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 14h:28m:04s remains)
INFO - root - 2017-12-16 08:46:23.541253: step 20130, loss = 0.57, batch loss = 0.31 (45.9 examples/sec; 0.174 sec/batch; 15h:06m:49s remains)
INFO - root - 2017-12-16 08:46:25.226037: step 20140, loss = 0.54, batch loss = 0.28 (45.8 examples/sec; 0.175 sec/batch; 15h:09m:29s remains)
INFO - root - 2017-12-16 08:46:26.927933: step 20150, loss = 0.48, batch loss = 0.22 (47.3 examples/sec; 0.169 sec/batch; 14h:40m:16s remains)
INFO - root - 2017-12-16 08:46:28.593947: step 20160, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 14h:35m:49s remains)
INFO - root - 2017-12-16 08:46:30.268695: step 20170, loss = 0.51, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 14h:24m:10s remains)
INFO - root - 2017-12-16 08:46:31.904047: step 20180, loss = 0.49, batch loss = 0.23 (49.1 examples/sec; 0.163 sec/batch; 14h:08m:36s remains)
INFO - root - 2017-12-16 08:46:33.571618: step 20190, loss = 0.59, batch loss = 0.33 (49.2 examples/sec; 0.163 sec/batch; 14h:06m:52s remains)
INFO - root - 2017-12-16 08:46:35.247334: step 20200, loss = 0.66, batch loss = 0.40 (44.8 examples/sec; 0.178 sec/batch; 15h:28m:37s remains)
2017-12-16 08:46:35.703090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.8376031 -4.4872103 -3.8707209 -3.0366449 -2.8777995 -3.7073069 -4.4638624 -4.6929264 -4.5191507 -4.22051 -4.1112761 -4.284492 -4.4143538 -4.3752303 -4.1901083][-4.3860545 -3.8919067 -3.1112485 -2.0797873 -1.9310794 -3.0226362 -4.0854549 -4.4567795 -4.3048754 -3.9818506 -3.9198871 -4.1883678 -4.3371716 -4.2906713 -4.0164475][-3.3993201 -2.7661185 -1.8272717 -0.70952237 -0.54358685 -1.885586 -3.2035587 -3.757725 -3.6044488 -3.216795 -3.1648495 -3.5067105 -3.7370045 -3.7440524 -3.4047415][-2.4388368 -1.6343069 -0.50766647 0.81803179 1.0995245 -0.4926219 -2.0942211 -2.8218448 -2.7023048 -2.2232697 -2.1470463 -2.5258009 -2.8207595 -2.9065115 -2.5851347][-1.7104292 -0.79232395 0.5296309 2.136781 2.6289208 0.85277748 -0.95100749 -1.8380463 -1.7598059 -1.262584 -1.1197894 -1.5619426 -1.9464259 -2.116643 -1.811344][-1.269093 -0.27561378 1.1861663 3.0975139 3.8361156 1.993634 0.085079908 -0.90925264 -0.87811589 -0.40291035 -0.30533147 -0.85466671 -1.3666965 -1.6511035 -1.4116403][-1.2526098 -0.23210526 1.4073188 3.6124279 4.5218019 2.5953434 0.64185572 -0.42456079 -0.49127817 -0.061717272 -0.019573689 -0.670262 -1.2998706 -1.5905939 -1.3989098][-1.539126 -0.48333228 1.2626441 3.4930489 4.2583361 2.353873 0.45852447 -0.59350216 -0.63045824 -0.15031147 -0.12855268 -0.75623834 -1.3276943 -1.5697631 -1.3738151][-1.6176298 -0.58140552 1.119911 3.1411827 3.6862328 1.9385917 0.1910181 -0.73041034 -0.65400517 -0.12587881 -0.07856822 -0.5966146 -1.0254499 -1.1035644 -0.7970947][-1.3907368 -0.43191743 1.1182077 2.9566534 3.4643314 1.9382551 0.34295416 -0.50592685 -0.43806458 -0.042316675 -0.053776264 -0.44744205 -0.64472651 -0.43474972 -0.025326014][-1.3204467 -0.43070996 0.96659708 2.6095979 3.0475471 1.7961757 0.44758224 -0.34250784 -0.45570898 -0.29200673 -0.39974511 -0.66177237 -0.62087834 -0.22114992 0.20969439][-1.7424402 -0.94638586 0.24969172 1.5851138 1.9424827 1.0100732 -0.0439322 -0.73927581 -0.97307813 -0.96258152 -1.0978181 -1.231479 -1.0890121 -0.70058954 -0.31020212][-2.4770167 -1.8827078 -0.95107281 0.075020075 0.36781073 -0.28549743 -1.0139219 -1.5515666 -1.8429941 -1.9513255 -2.0717888 -2.1100097 -1.9497967 -1.6700699 -1.4189806][-3.2707248 -2.9455559 -2.3426616 -1.6566327 -1.4203923 -1.7751335 -2.1745508 -2.5338969 -2.8434596 -3.0227733 -3.158906 -3.17448 -3.0557327 -2.8735301 -2.7001908][-3.902245 -3.8633151 -3.592052 -3.2045684 -2.9885862 -3.0800593 -3.2120037 -3.3958004 -3.67486 -3.9055052 -4.0558825 -4.0550866 -3.9821422 -3.8840466 -3.7621999]]...]
INFO - root - 2017-12-16 08:46:37.368586: step 20210, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:38m:28s remains)
INFO - root - 2017-12-16 08:46:39.005777: step 20220, loss = 0.52, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 13h:51m:48s remains)
INFO - root - 2017-12-16 08:46:40.639607: step 20230, loss = 0.50, batch loss = 0.24 (48.8 examples/sec; 0.164 sec/batch; 14h:13m:02s remains)
INFO - root - 2017-12-16 08:46:42.293527: step 20240, loss = 0.63, batch loss = 0.38 (49.4 examples/sec; 0.162 sec/batch; 14h:02m:58s remains)
INFO - root - 2017-12-16 08:46:43.938563: step 20250, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 14h:36m:00s remains)
INFO - root - 2017-12-16 08:46:45.594358: step 20260, loss = 0.52, batch loss = 0.26 (49.6 examples/sec; 0.161 sec/batch; 13h:59m:16s remains)
INFO - root - 2017-12-16 08:46:47.288097: step 20270, loss = 0.50, batch loss = 0.24 (46.8 examples/sec; 0.171 sec/batch; 14h:49m:42s remains)
INFO - root - 2017-12-16 08:46:48.970499: step 20280, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:26m:59s remains)
INFO - root - 2017-12-16 08:46:50.630145: step 20290, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 14h:20m:37s remains)
INFO - root - 2017-12-16 08:46:52.295299: step 20300, loss = 0.61, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 14h:26m:49s remains)
2017-12-16 08:46:52.831297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3459092 -1.3658316 -1.6286347 -2.0464649 -2.6716216 -3.4153395 -3.8576322 -3.9681866 -4.0577326 -4.0645084 -3.4449303 -2.0614469 -1.0307952 -1.0458666 -1.4612789][-1.4510287 -1.5884593 -2.067095 -2.7167194 -3.4822421 -4.0925045 -4.2580295 -4.1167846 -4.1766834 -4.3081818 -3.7858443 -2.405571 -1.351663 -1.2763236 -1.5271556][-1.7279493 -1.9955962 -2.662677 -3.4744954 -4.1535034 -4.348218 -3.9328952 -3.3927567 -3.4154742 -3.8000212 -3.7042017 -2.7825971 -2.0005393 -1.9015393 -1.951067][-2.1823545 -2.5300994 -3.2138751 -3.9496951 -4.3229141 -3.9716618 -2.8947463 -1.8915703 -1.9109888 -2.7027826 -3.2938747 -3.1277397 -2.8306363 -2.8135362 -2.7709718][-2.59307 -2.9312458 -3.4225364 -3.8443775 -3.7714148 -2.797889 -1.1187271 0.24863338 0.1937604 -1.0840888 -2.4202473 -3.0228302 -3.2268968 -3.4594855 -3.543571][-2.7661195 -3.0119059 -3.1564281 -3.1299405 -2.4937172 -0.92259073 1.343199 2.9725537 2.6265616 0.797734 -1.1743672 -2.3448648 -2.9425504 -3.3960848 -3.685744][-2.5322518 -2.5830295 -2.3583572 -1.8623736 -0.7529614 1.20877 3.8538456 5.5776854 4.7611332 2.3940949 0.041479588 -1.4181541 -2.1652625 -2.74785 -3.2344832][-2.1336746 -1.982723 -1.517158 -0.70960164 0.69026589 2.6820793 4.9901576 6.3026338 5.3915849 3.1350293 0.83210015 -0.56726122 -1.3380656 -2.0535572 -2.7037749][-2.1906214 -2.0029826 -1.4364949 -0.50960648 0.82984924 2.2520523 3.5995302 4.4183917 3.949306 2.3809309 0.64208531 -0.43765712 -1.1209356 -1.9196354 -2.5786772][-2.6153729 -2.494489 -1.994887 -1.1640208 -0.28215909 0.39471483 0.933692 1.3684943 1.2581043 0.4376421 -0.58420062 -1.2195208 -1.7040353 -2.3970532 -2.87323][-3.0654602 -3.0432699 -2.6438797 -1.9894191 -1.5743239 -1.5997645 -1.6333086 -1.3904742 -1.2449087 -1.5666871 -2.1055174 -2.4060268 -2.6662066 -3.1078641 -3.2734327][-3.2619965 -3.347646 -3.0830998 -2.7281675 -2.6743002 -2.9919248 -3.2957208 -3.2196157 -2.9966884 -3.0825841 -3.3526044 -3.5285487 -3.6513791 -3.7343659 -3.5821264][-3.1812749 -3.3156683 -3.2427731 -3.1700234 -3.3271294 -3.7070532 -4.07485 -4.1050797 -3.9234433 -3.8702946 -3.9291892 -3.9822383 -3.9689837 -3.8270373 -3.4821105][-2.8022904 -2.9710925 -3.03496 -3.1089728 -3.3271089 -3.6324039 -3.8873281 -3.9101615 -3.7570107 -3.6312985 -3.5609102 -3.5144107 -3.4285135 -3.225178 -2.8982887][-2.2373512 -2.3686054 -2.4518864 -2.5588083 -2.7212543 -2.8906584 -2.9970808 -2.9802971 -2.8572726 -2.7364666 -2.6476684 -2.5877562 -2.5026047 -2.3707075 -2.1904457]]...]
INFO - root - 2017-12-16 08:46:54.547749: step 20310, loss = 0.59, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 14h:28m:55s remains)
INFO - root - 2017-12-16 08:46:56.188423: step 20320, loss = 0.68, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 14h:32m:52s remains)
INFO - root - 2017-12-16 08:46:57.880745: step 20330, loss = 0.55, batch loss = 0.29 (46.6 examples/sec; 0.172 sec/batch; 14h:52m:49s remains)
INFO - root - 2017-12-16 08:46:59.555663: step 20340, loss = 0.55, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 14h:55m:49s remains)
INFO - root - 2017-12-16 08:47:01.237087: step 20350, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:38m:38s remains)
INFO - root - 2017-12-16 08:47:02.907351: step 20360, loss = 0.52, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 14h:17m:10s remains)
INFO - root - 2017-12-16 08:47:04.600801: step 20370, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:26m:49s remains)
INFO - root - 2017-12-16 08:47:06.263508: step 20380, loss = 0.60, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 14h:11m:18s remains)
INFO - root - 2017-12-16 08:47:07.945903: step 20390, loss = 0.52, batch loss = 0.26 (50.8 examples/sec; 0.157 sec/batch; 13h:38m:34s remains)
INFO - root - 2017-12-16 08:47:09.636866: step 20400, loss = 0.65, batch loss = 0.39 (48.6 examples/sec; 0.165 sec/batch; 14h:15m:42s remains)
2017-12-16 08:47:10.156017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3610332 -1.4183726 -1.5266829 -1.7440856 -1.9047266 -1.8654408 -1.807111 -1.8210266 -2.1535602 -2.726546 -3.2775047 -3.6330776 -3.7533112 -3.7518163 -3.7179461][-0.75166655 -0.74486053 -0.80053663 -1.1171907 -1.4039173 -1.411963 -1.3111278 -1.3571086 -1.8723772 -2.6294272 -3.3290708 -3.7999005 -3.9311094 -3.8958178 -3.8489492][-0.81738043 -0.68418837 -0.66237354 -1.0336611 -1.3745971 -1.3982363 -1.3035275 -1.344979 -1.8828166 -2.6278377 -3.2764738 -3.7170897 -3.810457 -3.7159142 -3.6446915][-1.5857296 -1.3922369 -1.2842425 -1.5799453 -1.8425915 -1.7639279 -1.5813751 -1.5769169 -1.9586779 -2.501622 -2.9479897 -3.2545609 -3.2851498 -3.1494911 -3.0680013][-2.5942895 -2.3628926 -2.1059554 -2.16005 -2.1963589 -1.9495655 -1.5808749 -1.4236526 -1.6114223 -1.917702 -2.1202509 -2.2469184 -2.2098842 -2.0798554 -2.0268197][-3.0608411 -2.7381003 -2.3961513 -2.2061782 -2.0081656 -1.5995281 -0.9517076 -0.52498245 -0.53537333 -0.70093215 -0.79232919 -0.75853896 -0.67600167 -0.59310997 -0.56925678][-2.5835559 -2.2389529 -1.9238691 -1.638242 -1.3478692 -0.848081 0.094516993 0.8179841 0.9291122 0.69265556 0.52156615 0.57588053 0.64681816 0.71959448 0.80883861][-1.4010665 -1.258484 -1.1724182 -0.95290041 -0.68757939 -0.038517714 1.1395462 1.9870238 2.051939 1.5398309 1.075913 0.97779131 1.0400982 1.2095518 1.4371359][-0.22688985 -0.42139137 -0.70422781 -0.67328095 -0.41019118 0.37625623 1.5170286 2.17312 2.0360136 1.3185143 0.61355996 0.36215305 0.54314971 0.93308449 1.3352473][0.43035769 -0.0758996 -0.65532863 -0.81406689 -0.53619456 0.17239475 1.0095921 1.4313469 1.1645372 0.41342211 -0.26419902 -0.479241 -0.16660976 0.38339734 0.8614161][0.59775805 -0.046319246 -0.69366062 -0.90567553 -0.65428591 -0.095850229 0.43509698 0.64944315 0.44577551 -0.10188341 -0.63603866 -0.76724446 -0.42321789 0.027174234 0.30207062][0.51183391 -0.015605688 -0.51214135 -0.74405336 -0.60154617 -0.17161989 0.16760516 0.30831814 0.30523372 0.065628767 -0.33764791 -0.46285725 -0.27382541 -0.13922811 -0.30730391][0.24849486 0.067362309 -0.11724496 -0.30976772 -0.2931447 -0.019923925 0.27067804 0.62145543 0.85332608 0.76409674 0.37928391 0.088575363 -0.012556314 -0.27399206 -0.89926028][-0.20270634 -0.040384531 0.028011084 -0.15545082 -0.26538181 -0.12184215 0.23678827 0.85115552 1.3194873 1.2564523 0.74336386 0.17474985 -0.19168329 -0.65012896 -1.3950841][-1.0057271 -0.69846737 -0.51742792 -0.70879519 -0.95750856 -0.94873846 -0.5710454 0.16838717 0.82369947 0.82743382 0.22678781 -0.40401804 -0.76634753 -1.1799575 -1.7046659]]...]
INFO - root - 2017-12-16 08:47:11.841031: step 20410, loss = 0.49, batch loss = 0.23 (44.6 examples/sec; 0.179 sec/batch; 15h:32m:38s remains)
INFO - root - 2017-12-16 08:47:13.526008: step 20420, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.165 sec/batch; 14h:20m:44s remains)
INFO - root - 2017-12-16 08:47:15.221038: step 20430, loss = 0.64, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 14h:28m:39s remains)
INFO - root - 2017-12-16 08:47:16.879446: step 20440, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:27m:52s remains)
INFO - root - 2017-12-16 08:47:18.533535: step 20450, loss = 0.62, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:19m:07s remains)
INFO - root - 2017-12-16 08:47:20.200299: step 20460, loss = 0.63, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 14h:27m:54s remains)
INFO - root - 2017-12-16 08:47:21.883877: step 20470, loss = 0.57, batch loss = 0.31 (46.5 examples/sec; 0.172 sec/batch; 14h:54m:28s remains)
INFO - root - 2017-12-16 08:47:23.569053: step 20480, loss = 0.63, batch loss = 0.38 (47.2 examples/sec; 0.170 sec/batch; 14h:42m:07s remains)
INFO - root - 2017-12-16 08:47:25.256524: step 20490, loss = 0.70, batch loss = 0.44 (48.9 examples/sec; 0.164 sec/batch; 14h:11m:00s remains)
INFO - root - 2017-12-16 08:47:26.919630: step 20500, loss = 0.63, batch loss = 0.38 (47.1 examples/sec; 0.170 sec/batch; 14h:43m:03s remains)
2017-12-16 08:47:27.399439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.84542418 -0.93919313 -0.58079731 -0.598601 -0.92770851 -1.0558635 -0.70751107 -0.35076857 -0.5893234 -1.7383516 -2.8234069 -3.2467475 -2.9010971 -2.5916531 -2.6328115][-0.9024756 -1.2371466 -1.1983943 -1.515938 -1.9538329 -1.6881771 -0.6994946 0.24534845 0.065120935 -1.5280403 -3.0231247 -3.5850339 -3.2897274 -2.9326723 -2.8467932][-1.1190288 -1.8434408 -2.0772612 -2.4634366 -2.8011425 -2.0832796 -0.3231554 1.3325541 1.1492419 -0.9920094 -2.9416602 -3.73568 -3.5161576 -3.1210406 -2.9605293][-1.6764247 -2.7324388 -2.9948978 -3.1682312 -3.195972 -1.7821324 0.88875175 3.2212832 2.8449171 0.036566973 -2.4214334 -3.5589991 -3.4976513 -3.1316936 -2.9550612][-2.2331376 -3.5092621 -3.690824 -3.5526075 -3.0489478 -0.92631865 2.5911272 5.5419693 4.662961 1.0788298 -1.9086314 -3.2945404 -3.3917687 -3.1043644 -2.8739467][-2.4824643 -3.7822084 -3.8497231 -3.3509936 -2.2932379 0.41314816 4.6185303 8.1353989 6.5116558 1.9695532 -1.5967038 -3.2515259 -3.4358678 -3.1748676 -2.8656704][-2.7479389 -3.8440452 -3.6750059 -2.7608156 -1.2269419 2.0447276 7.1635504 11.518536 8.7249193 2.9823925 -1.2242126 -3.1739352 -3.4560866 -3.2941689 -2.9654431][-3.1382823 -4.0110259 -3.580946 -2.4652605 -0.69197536 3.066591 9.0184355 14.326049 10.196625 3.5075791 -1.0901102 -3.2132359 -3.5790648 -3.4650102 -3.0850041][-3.2624688 -3.9508286 -3.5196309 -2.6087759 -0.92747855 2.6976955 8.2765789 12.401761 9.1716738 3.0576184 -1.2944045 -3.3410552 -3.7218275 -3.5762224 -3.0478039][-3.0273924 -3.5845447 -3.3501425 -2.6721084 -1.3019055 1.7756984 6.4256029 9.5996227 7.2763281 2.251415 -1.5298419 -3.2974291 -3.6243787 -3.4602029 -2.8300927][-2.6579356 -3.243398 -3.3078597 -2.8147969 -1.7129188 0.6710701 4.2805738 6.6906471 5.0001612 1.021764 -2.0413556 -3.3289926 -3.5650511 -3.3873696 -2.6943207][-2.4894063 -3.1102715 -3.4425106 -3.2769263 -2.4758639 -0.79106951 1.9366281 3.6714056 2.3774045 -0.7048279 -2.8843992 -3.6274655 -3.6864648 -3.5162728 -2.7600591][-2.6225448 -3.1435986 -3.5969911 -3.7082715 -3.3541813 -2.310631 -0.40232718 0.94915938 0.05280757 -2.1711354 -3.5990117 -3.8688459 -3.7937443 -3.6660662 -2.9616675][-2.9085116 -3.2527063 -3.70154 -3.9677248 -3.8886309 -3.3412385 -2.1032968 -0.9854393 -1.467551 -3.0799439 -4.0596437 -4.1409445 -4.0267048 -3.8928518 -3.2026834][-3.233377 -3.439244 -3.7247951 -3.979459 -3.9774616 -3.664824 -2.8874717 -2.0877657 -2.3145533 -3.43652 -4.227344 -4.3193216 -4.2150979 -4.0089417 -3.295331]]...]
INFO - root - 2017-12-16 08:47:29.044077: step 20510, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:33m:41s remains)
INFO - root - 2017-12-16 08:47:30.744601: step 20520, loss = 0.68, batch loss = 0.42 (47.6 examples/sec; 0.168 sec/batch; 14h:33m:02s remains)
INFO - root - 2017-12-16 08:47:32.459927: step 20530, loss = 0.48, batch loss = 0.22 (47.0 examples/sec; 0.170 sec/batch; 14h:45m:02s remains)
INFO - root - 2017-12-16 08:47:34.130246: step 20540, loss = 0.63, batch loss = 0.37 (47.1 examples/sec; 0.170 sec/batch; 14h:43m:14s remains)
INFO - root - 2017-12-16 08:47:35.808642: step 20550, loss = 0.63, batch loss = 0.37 (47.3 examples/sec; 0.169 sec/batch; 14h:38m:34s remains)
INFO - root - 2017-12-16 08:47:37.466305: step 20560, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 14h:19m:20s remains)
INFO - root - 2017-12-16 08:47:39.177484: step 20570, loss = 0.53, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:44m:28s remains)
INFO - root - 2017-12-16 08:47:40.843951: step 20580, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.165 sec/batch; 14h:20m:21s remains)
INFO - root - 2017-12-16 08:47:42.488381: step 20590, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 14h:18m:18s remains)
INFO - root - 2017-12-16 08:47:44.170793: step 20600, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 14h:32m:04s remains)
2017-12-16 08:47:44.654318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.448617 -3.1959631 -2.7054858 -2.1594057 -1.8288186 -1.8394744 -2.0214555 -2.0920169 -2.0284576 -1.8902197 -1.7515838 -1.6297697 -1.7666204 -2.30092 -2.9174304][-3.9706504 -3.6843505 -3.0841751 -2.3547277 -1.8850462 -1.9400282 -2.2978988 -2.5197036 -2.5056074 -2.3563955 -2.1606236 -1.9481263 -2.0907373 -2.7965565 -3.6219063][-4.3713427 -4.0198669 -3.2748661 -2.3036995 -1.576903 -1.531769 -1.9500411 -2.2683263 -2.2822044 -2.1313899 -1.915638 -1.6745284 -1.8345759 -2.6334419 -3.6552198][-4.4157324 -3.8621316 -2.8585823 -1.6050754 -0.6626718 -0.52206647 -1.0014747 -1.4128482 -1.552785 -1.4603298 -1.2255727 -0.99251568 -1.2220134 -2.1362646 -3.2913775][-3.8406329 -2.9208255 -1.5916324 -0.063015938 1.1077156 1.226656 0.58002305 0.061733484 -0.11253667 -0.024986506 0.19784498 0.33427715 -0.13880324 -1.2957481 -2.6574214][-3.11855 -1.9200509 -0.34505296 1.5152459 2.9651811 3.0331037 2.26176 1.6820438 1.5151663 1.5718052 1.7293108 1.6344402 0.80886173 -0.63247061 -2.1883388][-2.5702987 -1.2058747 0.69640541 3.1648529 5.1744347 5.206398 4.2385082 3.51669 3.2294824 3.1948235 3.2308586 2.9692442 1.8979757 0.24969268 -1.474141][-2.1496906 -0.49847984 1.8280046 4.8578749 7.325264 7.2118731 5.9679804 5.0198917 4.5271063 4.3581047 4.3088951 3.924185 2.7212861 0.97015119 -0.86649859][-2.6783719 -1.1512322 0.93914437 3.3193238 5.0348988 4.9994164 4.0143881 3.1504047 2.6863277 2.4571502 2.4598129 2.3344781 1.493613 0.018456697 -1.5884942][-3.34252 -2.0627561 -0.33982491 1.5131981 2.7681086 2.7747591 2.0483911 1.3613763 0.96141505 0.74192953 0.79910827 0.92220736 0.4289763 -0.79217744 -2.1445758][-3.6925888 -2.5490606 -0.98065245 0.6139679 1.6194432 1.553911 0.89015293 0.25846124 -0.12731051 -0.3363719 -0.23487973 0.058769226 -0.15783429 -1.2074236 -2.4505472][-3.8251843 -2.8404036 -1.4071587 0.019971848 0.86699963 0.69603157 -0.065087318 -0.72687173 -1.0515465 -1.160903 -0.98358583 -0.63100338 -0.76164043 -1.6365855 -2.7283006][-3.9129212 -3.122118 -1.9116501 -0.69242609 0.088373184 -0.023206949 -0.68887532 -1.3078288 -1.5570557 -1.5761523 -1.3774389 -1.0876228 -1.2629137 -2.0520372 -2.985827][-3.7828627 -3.1442177 -2.1488447 -1.1498256 -0.39236617 -0.31686735 -0.79633343 -1.3156527 -1.5245829 -1.5332658 -1.3781081 -1.179388 -1.3807033 -2.072928 -2.8896565][-3.4174829 -2.9294074 -2.2122717 -1.5154115 -0.95001984 -0.82901454 -1.1439295 -1.561017 -1.7468919 -1.7681322 -1.666327 -1.5622147 -1.7426946 -2.2352996 -2.8347354]]...]
INFO - root - 2017-12-16 08:47:46.308883: step 20610, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 14h:02m:55s remains)
INFO - root - 2017-12-16 08:47:47.941599: step 20620, loss = 0.55, batch loss = 0.29 (50.2 examples/sec; 0.159 sec/batch; 13h:48m:34s remains)
INFO - root - 2017-12-16 08:47:49.579062: step 20630, loss = 0.49, batch loss = 0.23 (50.8 examples/sec; 0.157 sec/batch; 13h:38m:15s remains)
INFO - root - 2017-12-16 08:47:51.244328: step 20640, loss = 0.54, batch loss = 0.29 (49.6 examples/sec; 0.161 sec/batch; 13h:58m:51s remains)
INFO - root - 2017-12-16 08:47:52.893229: step 20650, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:13m:24s remains)
INFO - root - 2017-12-16 08:47:54.522992: step 20660, loss = 0.49, batch loss = 0.23 (49.5 examples/sec; 0.162 sec/batch; 14h:00m:23s remains)
INFO - root - 2017-12-16 08:47:56.163994: step 20670, loss = 0.60, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 14h:09m:46s remains)
INFO - root - 2017-12-16 08:47:57.822204: step 20680, loss = 0.69, batch loss = 0.43 (48.0 examples/sec; 0.167 sec/batch; 14h:25m:30s remains)
INFO - root - 2017-12-16 08:47:59.509522: step 20690, loss = 0.60, batch loss = 0.34 (49.3 examples/sec; 0.162 sec/batch; 14h:03m:02s remains)
INFO - root - 2017-12-16 08:48:01.187672: step 20700, loss = 0.47, batch loss = 0.21 (48.3 examples/sec; 0.166 sec/batch; 14h:21m:06s remains)
2017-12-16 08:48:01.651663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0261641 -3.0827916 -2.9968011 -2.8138113 -2.6586998 -2.5333993 -2.4462349 -2.5357897 -2.7166922 -2.9242342 -3.1050935 -3.3080511 -3.4639456 -3.5712485 -3.6261234][-3.4071279 -3.4432309 -3.2464461 -2.8685265 -2.5038569 -2.1830506 -2.087323 -2.1787179 -2.3078091 -2.4681687 -2.611769 -2.8511105 -3.1114666 -3.2724266 -3.3546505][-3.6481724 -3.6537194 -3.3503482 -2.8173208 -2.2745881 -1.7081997 -1.4697835 -1.4738164 -1.5736182 -1.6593761 -1.7925165 -2.084774 -2.4335878 -2.59412 -2.640336][-3.7564356 -3.7055326 -3.2555611 -2.5281777 -1.6900665 -0.82134485 -0.34136868 -0.22183108 -0.32211423 -0.44877267 -0.67744029 -0.98814619 -1.326041 -1.4936016 -1.4681618][-3.8519325 -3.7168064 -3.0322628 -1.9291847 -0.57862043 0.71333432 1.4832969 1.6719067 1.4549465 1.0790906 0.68256736 0.32612014 0.13461566 0.17439342 0.30143738][-4.0274792 -3.8417146 -3.0034199 -1.5355244 0.35814548 2.2201288 3.5105608 3.8357871 3.3506262 2.5487783 1.8133214 1.3560565 1.3281898 1.682934 2.1023295][-4.2395892 -4.0918255 -3.1969891 -1.5238383 0.60892725 2.7969239 4.4615498 4.9422522 4.205327 3.0475819 1.9887755 1.4617031 1.6474392 2.3811533 3.1692226][-4.3325138 -4.1998811 -3.3151076 -1.7477006 0.23701072 2.1862557 3.5287588 3.6928089 2.9322832 1.8098567 0.8313489 0.47591162 0.87088823 1.6715009 2.537379][-4.2286425 -4.0658989 -3.1708257 -1.7421118 -0.064873457 1.414999 2.2636368 2.0667222 1.2148433 0.20614505 -0.59613955 -0.71032846 -0.14366961 0.59061861 1.2131245][-4.0453205 -3.7869573 -2.9111362 -1.6702845 -0.36306739 0.66334271 1.0600226 0.636322 -0.25584769 -1.2136637 -1.8782594 -1.8321457 -1.2320693 -0.53684807 -0.048322439][-3.9921293 -3.7203953 -2.9446735 -1.9446543 -0.89503396 -0.19858122 -0.13823748 -0.65725279 -1.4792583 -2.2705641 -2.7479603 -2.6622536 -2.1161427 -1.5212691 -1.1130484][-4.1802077 -3.9498894 -3.1792729 -2.1857014 -1.2349708 -0.64153433 -0.62347054 -1.1917062 -1.9553802 -2.5994723 -2.9562321 -2.9010394 -2.5639646 -2.2080717 -1.966783][-4.3677564 -4.1764646 -3.3522518 -2.2774365 -1.251623 -0.64266276 -0.56400013 -1.0578256 -1.7769086 -2.4017477 -2.7569952 -2.8213758 -2.758131 -2.6257145 -2.5617483][-4.213892 -3.9337361 -3.0092344 -1.7909226 -0.68605161 -0.10612369 -0.11403108 -0.65334773 -1.4297249 -2.1307912 -2.6012518 -2.8644614 -2.936902 -2.9039104 -2.9065785][-3.6821172 -3.2243698 -2.1543832 -0.85166514 0.28105164 0.7039907 0.4487586 -0.31950831 -1.2488251 -1.9509015 -2.4598403 -2.8368332 -3.0152576 -3.03754 -3.0368161]]...]
INFO - root - 2017-12-16 08:48:03.308460: step 20710, loss = 0.57, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 14h:13m:33s remains)
INFO - root - 2017-12-16 08:48:04.980310: step 20720, loss = 0.59, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 14h:30m:53s remains)
INFO - root - 2017-12-16 08:48:06.649373: step 20730, loss = 0.50, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 14h:34m:03s remains)
INFO - root - 2017-12-16 08:48:08.358248: step 20740, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:24m:39s remains)
INFO - root - 2017-12-16 08:48:10.090727: step 20750, loss = 0.61, batch loss = 0.35 (37.6 examples/sec; 0.213 sec/batch; 18h:24m:11s remains)
INFO - root - 2017-12-16 08:48:11.769367: step 20760, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:15m:00s remains)
INFO - root - 2017-12-16 08:48:13.433788: step 20770, loss = 0.67, batch loss = 0.41 (47.8 examples/sec; 0.167 sec/batch; 14h:28m:54s remains)
INFO - root - 2017-12-16 08:48:15.093200: step 20780, loss = 0.54, batch loss = 0.28 (48.9 examples/sec; 0.163 sec/batch; 14h:09m:05s remains)
INFO - root - 2017-12-16 08:48:16.751232: step 20790, loss = 0.53, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 14h:11m:46s remains)
INFO - root - 2017-12-16 08:48:18.369761: step 20800, loss = 0.48, batch loss = 0.23 (49.1 examples/sec; 0.163 sec/batch; 14h:06m:24s remains)
2017-12-16 08:48:18.833327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7371006 -2.5259004 -2.1770377 -1.6500165 -1.0634873 -0.65085328 -0.6512742 -1.3172814 -2.2660646 -3.0292211 -3.5833693 -3.9317183 -3.7654011 -3.0602379 -2.4280519][-3.1950371 -3.1995413 -3.0707605 -2.710829 -2.2794087 -1.9286103 -1.8018961 -2.2107062 -2.896524 -3.3916411 -3.585645 -3.6299565 -3.2723656 -2.4621415 -1.8225327][-3.5606117 -3.7191722 -3.7209933 -3.4933753 -3.1718357 -2.8245034 -2.5546966 -2.6871097 -3.16802 -3.4597969 -3.3028543 -2.9882267 -2.4523511 -1.5803715 -0.82507789][-3.7215815 -3.8790631 -3.8732471 -3.6318159 -3.2377806 -2.8029463 -2.4185224 -2.3936534 -2.735137 -2.9692855 -2.6764588 -2.191081 -1.5255283 -0.68646026 0.069187164][-3.6330421 -3.6244297 -3.3770108 -2.9076355 -2.2887819 -1.6106303 -1.0081786 -0.78072667 -1.1892186 -1.6460774 -1.5311446 -1.0960647 -0.51908636 0.056617975 0.61106539][-3.309082 -3.01357 -2.3726003 -1.4800096 -0.48987627 0.48409033 1.4556541 2.1554246 1.6045902 0.53413343 0.17596269 0.37291455 0.51510096 0.46720481 0.58944154][-2.9020078 -2.3948321 -1.3608674 -0.0026376247 1.2395866 2.4532676 4.0339451 5.675446 4.8769474 2.7884398 1.7759137 1.590219 1.2038264 0.46260715 0.081259966][-2.6624978 -2.14148 -0.94431496 0.61708665 1.8906364 3.1223464 5.1365418 7.7528977 6.674252 3.7457471 2.0863514 1.5352528 0.79571581 -0.27355742 -0.97756314][-2.8013136 -2.4345851 -1.4872618 -0.1503005 0.91250658 1.8454556 3.4745646 5.3510561 4.7406411 2.4856796 0.89172125 0.17096281 -0.60146213 -1.5301371 -2.1410575][-3.0376964 -2.9146829 -2.4074459 -1.6052651 -0.90494 -0.334486 0.65740347 1.7111135 1.4990582 0.14047766 -0.97375238 -1.5591013 -2.2199712 -2.8339803 -3.1323895][-3.1490903 -3.2945616 -3.1712785 -2.8717985 -2.5450487 -2.3383236 -1.9482448 -1.4743726 -1.4407921 -2.0545371 -2.6065092 -2.9751275 -3.4114571 -3.7489672 -3.8124142][-3.0824108 -3.395865 -3.5102825 -3.5460308 -3.5477037 -3.6074028 -3.6220295 -3.4994769 -3.4020059 -3.5546684 -3.7446511 -3.8683772 -4.01859 -4.1184335 -4.078043][-2.7990441 -3.1431458 -3.3945017 -3.610846 -3.774684 -3.9780679 -4.1582088 -4.1615267 -4.101634 -4.0644703 -4.0671892 -4.0196762 -3.9479825 -3.9096572 -3.8226209][-2.4437478 -2.6906898 -2.9417815 -3.1711493 -3.3612726 -3.5805492 -3.7723234 -3.8128209 -3.7497678 -3.6712608 -3.6073053 -3.502768 -3.3431706 -3.2459226 -3.1455805][-2.1767986 -2.3030243 -2.4393346 -2.5813937 -2.700681 -2.8684273 -3.017041 -3.0552139 -3.0100675 -2.9469872 -2.892807 -2.8038211 -2.6536889 -2.5654898 -2.4914908]]...]
INFO - root - 2017-12-16 08:48:20.544886: step 20810, loss = 0.56, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 14h:47m:13s remains)
INFO - root - 2017-12-16 08:48:22.229436: step 20820, loss = 0.48, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 14h:37m:50s remains)
INFO - root - 2017-12-16 08:48:23.926885: step 20830, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.165 sec/batch; 14h:19m:32s remains)
INFO - root - 2017-12-16 08:48:25.616330: step 20840, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 14h:17m:42s remains)
INFO - root - 2017-12-16 08:48:27.258105: step 20850, loss = 0.51, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 13h:48m:35s remains)
INFO - root - 2017-12-16 08:48:28.914291: step 20860, loss = 0.58, batch loss = 0.32 (48.5 examples/sec; 0.165 sec/batch; 14h:17m:04s remains)
INFO - root - 2017-12-16 08:48:30.580700: step 20870, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 14h:16m:21s remains)
INFO - root - 2017-12-16 08:48:32.228602: step 20880, loss = 0.60, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 14h:14m:37s remains)
INFO - root - 2017-12-16 08:48:33.922935: step 20890, loss = 0.56, batch loss = 0.31 (43.7 examples/sec; 0.183 sec/batch; 15h:50m:43s remains)
INFO - root - 2017-12-16 08:48:35.595081: step 20900, loss = 0.69, batch loss = 0.44 (47.1 examples/sec; 0.170 sec/batch; 14h:42m:01s remains)
2017-12-16 08:48:36.098639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.373832 -2.6786885 -2.8553162 -2.9155452 -2.8916032 -2.8943954 -2.9768186 -2.9537597 -2.7728455 -2.3230832 -1.8551629 -1.7858785 -1.9560592 -2.1328313 -2.1459036][-1.9639056 -2.2839167 -2.4732919 -2.5782061 -2.6987414 -2.9074945 -3.1450555 -3.1590462 -2.8850899 -2.2421176 -1.5287416 -1.2883716 -1.5176573 -1.9077312 -2.1383479][-1.1692072 -1.5600035 -1.8367722 -2.0308712 -2.2905874 -2.6817889 -3.0353487 -3.0935669 -2.7211354 -1.9825248 -1.2461665 -0.89896894 -1.0225346 -1.4658105 -1.7732639][-0.225806 -0.69107223 -1.0019948 -1.2198319 -1.5243566 -1.9609132 -2.310708 -2.3167138 -1.9238012 -1.3057667 -0.78896868 -0.56830704 -0.64898694 -0.93390083 -1.1285032][0.41236353 -0.086250305 -0.33873248 -0.40957236 -0.53533185 -0.82570577 -0.95651376 -0.78178573 -0.45634556 -0.17389989 -0.10242939 -0.20931244 -0.37235379 -0.44810653 -0.38455796][0.25597715 -0.11532474 -0.10434747 0.087914944 0.33189178 0.46723652 0.74095559 1.146069 1.2829304 1.0310106 0.48341274 -0.035038948 -0.33332372 -0.36614513 -0.09239459][-0.42004406 -0.60430789 -0.2673943 0.33781314 1.0774074 1.776027 2.5828807 3.2234619 2.9940464 2.0929511 0.95319986 0.0095105171 -0.59519541 -0.78098869 -0.46316338][-1.2229651 -1.1817516 -0.5778141 0.37474108 1.533215 2.6776235 3.9952705 5.004344 4.2415323 2.7745798 1.3065658 0.061324358 -0.83505654 -1.1634896 -0.85613179][-2.2368035 -1.9827949 -1.2102517 -0.089420319 1.2008061 2.4807742 3.8555419 4.764679 3.9481289 2.4927332 1.0837326 -0.17820692 -1.0942624 -1.3847997 -1.0677145][-3.0091333 -2.6513033 -1.8520677 -0.75914073 0.4302907 1.5410488 2.5488408 2.9867685 2.3574493 1.2560031 0.16510487 -0.8304249 -1.5235558 -1.6797647 -1.3767575][-3.2712021 -2.9127071 -2.1871989 -1.2505593 -0.29748821 0.49152136 1.086098 1.20221 0.66717148 -0.10433292 -0.81808484 -1.4317002 -1.8330188 -1.8215388 -1.5186698][-3.1016223 -2.8499143 -2.3592794 -1.7211083 -1.0738918 -0.541618 -0.20744967 -0.29112005 -0.75918674 -1.233196 -1.5611252 -1.7881446 -1.8972092 -1.7468002 -1.469119][-2.6179082 -2.5714688 -2.3736374 -2.0418372 -1.6533444 -1.2889674 -1.1006845 -1.2770635 -1.6509367 -1.9115521 -2.0058608 -1.9788865 -1.8881454 -1.6886375 -1.4686723][-2.020191 -2.2035728 -2.2560885 -2.1187315 -1.8248476 -1.5459015 -1.4542276 -1.6417415 -1.9487851 -2.1191573 -2.1020093 -1.9678781 -1.8193041 -1.6518064 -1.5076954][-1.6694118 -2.0222898 -2.2355094 -2.1691017 -1.8548794 -1.5831063 -1.5472351 -1.7543833 -2.0404525 -2.1967077 -2.1424787 -1.9990735 -1.8774229 -1.7747282 -1.6805294]]...]
INFO - root - 2017-12-16 08:48:37.761199: step 20910, loss = 0.62, batch loss = 0.36 (46.9 examples/sec; 0.171 sec/batch; 14h:46m:04s remains)
INFO - root - 2017-12-16 08:48:39.426887: step 20920, loss = 0.58, batch loss = 0.32 (48.9 examples/sec; 0.164 sec/batch; 14h:09m:48s remains)
INFO - root - 2017-12-16 08:48:41.092501: step 20930, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 14h:38m:43s remains)
INFO - root - 2017-12-16 08:48:42.755903: step 20940, loss = 0.57, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 14h:23m:28s remains)
INFO - root - 2017-12-16 08:48:44.425019: step 20950, loss = 0.49, batch loss = 0.23 (45.5 examples/sec; 0.176 sec/batch; 15h:12m:44s remains)
INFO - root - 2017-12-16 08:48:46.133255: step 20960, loss = 0.54, batch loss = 0.28 (46.3 examples/sec; 0.173 sec/batch; 14h:56m:27s remains)
INFO - root - 2017-12-16 08:48:47.816017: step 20970, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 14h:14m:30s remains)
INFO - root - 2017-12-16 08:48:49.487863: step 20980, loss = 0.57, batch loss = 0.31 (46.0 examples/sec; 0.174 sec/batch; 15h:03m:02s remains)
INFO - root - 2017-12-16 08:48:51.189635: step 20990, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:30m:10s remains)
INFO - root - 2017-12-16 08:48:52.873663: step 21000, loss = 0.50, batch loss = 0.24 (47.0 examples/sec; 0.170 sec/batch; 14h:44m:02s remains)
2017-12-16 08:48:53.352269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.059444 -3.0972013 -2.6439097 -1.6596204 -0.86991513 -0.78418446 -1.0888795 -1.3184103 -1.737046 -2.4259679 -3.2654262 -3.8376374 -3.7508297 -3.0444918 -2.2101436][-3.3159251 -3.5635085 -3.3676665 -2.7291536 -2.2785835 -2.3049192 -2.3951674 -2.3220499 -2.4064763 -2.9082148 -3.5435741 -3.9493303 -3.8401716 -3.1443241 -2.29841][-3.7281711 -4.1192255 -4.0778017 -3.7124987 -3.4000268 -3.3489506 -3.1314657 -2.7219322 -2.5300853 -2.8021998 -3.2564492 -3.5973074 -3.5477874 -2.9725103 -2.2199378][-4.3089657 -4.7255273 -4.7545204 -4.3880062 -3.8859622 -3.4286337 -2.8224363 -2.2216852 -2.0570703 -2.2846227 -2.6100159 -2.9249158 -3.0339916 -2.683285 -2.094697][-4.6841984 -5.0655203 -5.0187902 -4.4294491 -3.3943863 -2.2707856 -1.1767701 -0.55513871 -0.76446271 -1.3833554 -1.9237695 -2.3954442 -2.6642454 -2.4907176 -2.0026071][-4.4727654 -4.7148042 -4.4466219 -3.4080181 -1.587508 0.47363114 2.1448987 2.508806 1.4013109 -0.22716856 -1.4713041 -2.2885852 -2.6508057 -2.4778283 -1.9598739][-4.0082507 -4.056931 -3.4621041 -1.9762728 0.57630587 3.6649735 6.0920725 6.0395479 3.4951341 0.45576835 -1.6602364 -2.7499259 -3.0040345 -2.6472816 -1.9583617][-3.5856531 -3.5070276 -2.7348709 -1.1113676 1.6825149 5.4471464 8.6190968 7.9156713 4.3175011 0.36962914 -2.3072591 -3.4859252 -3.5234356 -2.9131093 -1.9916341][-3.1672339 -3.1150365 -2.3781664 -1.0827091 1.1075442 4.1407347 6.8030739 6.5703545 3.2930882 -0.6054852 -3.2891273 -4.2829266 -4.000289 -3.0918086 -1.9707487][-2.8565845 -2.9390872 -2.3745296 -1.4519472 -0.049024343 2.0422924 4.1260223 4.1650715 1.5533657 -1.7559488 -4.0230732 -4.7343721 -4.2247658 -3.1379762 -1.9163493][-2.9726667 -3.1116354 -2.7123117 -2.1326351 -1.3208932 -0.016116619 1.5392542 1.8216522 0.080254555 -2.3802176 -4.113378 -4.6132746 -4.0778618 -3.0317469 -1.8325047][-3.2741694 -3.4790936 -3.2487473 -2.9670272 -2.5751519 -1.7376652 -0.49713659 0.020079374 -0.9774009 -2.6429806 -3.9249644 -4.2877421 -3.8169098 -2.8732183 -1.7588687][-3.6528385 -3.9414058 -3.8556461 -3.7064686 -3.4655833 -2.7723091 -1.663656 -1.0216715 -1.5461285 -2.6765833 -3.630585 -3.9126558 -3.4899626 -2.6338499 -1.6323693][-3.8906283 -4.1604586 -4.0905857 -3.8906546 -3.5609431 -2.8606839 -1.8588321 -1.2387575 -1.5415642 -2.411907 -3.2149425 -3.4797583 -3.0900905 -2.2989416 -1.4180253][-3.6961484 -3.8407874 -3.6463971 -3.3077917 -2.8714356 -2.1672883 -1.2805429 -0.74938655 -1.0696459 -1.9204128 -2.7123628 -2.9902482 -2.6492541 -1.9141569 -1.143309]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:48:55.079873: step 21010, loss = 0.52, batch loss = 0.26 (43.2 examples/sec; 0.185 sec/batch; 16h:00m:26s remains)
INFO - root - 2017-12-16 08:48:56.722873: step 21020, loss = 0.55, batch loss = 0.29 (49.8 examples/sec; 0.161 sec/batch; 13h:54m:29s remains)
INFO - root - 2017-12-16 08:48:58.378332: step 21030, loss = 0.53, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 14h:12m:25s remains)
INFO - root - 2017-12-16 08:49:00.079328: step 21040, loss = 0.59, batch loss = 0.33 (46.7 examples/sec; 0.171 sec/batch; 14h:49m:56s remains)
INFO - root - 2017-12-16 08:49:01.801232: step 21050, loss = 0.60, batch loss = 0.34 (44.9 examples/sec; 0.178 sec/batch; 15h:25m:22s remains)
INFO - root - 2017-12-16 08:49:03.486404: step 21060, loss = 0.61, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 14h:12m:13s remains)
INFO - root - 2017-12-16 08:49:05.216146: step 21070, loss = 0.52, batch loss = 0.26 (46.7 examples/sec; 0.171 sec/batch; 14h:49m:32s remains)
INFO - root - 2017-12-16 08:49:06.872768: step 21080, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:27m:41s remains)
INFO - root - 2017-12-16 08:49:08.549263: step 21090, loss = 0.53, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 14h:20m:43s remains)
INFO - root - 2017-12-16 08:49:10.235446: step 21100, loss = 0.60, batch loss = 0.34 (47.1 examples/sec; 0.170 sec/batch; 14h:41m:03s remains)
2017-12-16 08:49:10.726774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3599449 -1.4497041 -1.5656371 -1.5326011 -1.2924902 -0.91623676 -0.49321187 -0.23469019 -0.35211873 -0.90927637 -1.711924 -2.5154672 -3.0708568 -3.2255015 -3.1303954][-2.0907412 -2.1720479 -2.2564595 -2.1898372 -1.9526759 -1.6140039 -1.2703635 -1.0484848 -1.0921135 -1.4520656 -1.9961756 -2.5522292 -2.922565 -2.9574676 -2.7615161][-2.330771 -2.4082468 -2.465445 -2.4027045 -2.2041643 -1.9087589 -1.5867043 -1.3363501 -1.2815831 -1.5152746 -1.9288934 -2.373842 -2.6583073 -2.6086965 -2.2844708][-2.0963421 -2.1380026 -2.1739755 -2.1497421 -2.0046558 -1.7433888 -1.4387022 -1.1649005 -1.0499403 -1.2246679 -1.6039269 -2.0400405 -2.3326569 -2.2698135 -1.8316295][-1.5734285 -1.5520952 -1.5512315 -1.5670778 -1.475208 -1.2297013 -0.90163374 -0.65072644 -0.56453311 -0.73388374 -1.1464611 -1.5984557 -1.890865 -1.8240247 -1.313906][-1.0710862 -0.99887109 -0.98911691 -1.0721679 -1.0046213 -0.72111344 -0.34518647 -0.13592315 -0.13151431 -0.37109089 -0.80233383 -1.169718 -1.3112373 -1.1458501 -0.67345989][-0.87221897 -0.83109343 -0.89132607 -0.99624825 -0.83046186 -0.3904438 0.068178415 0.26084971 0.12223959 -0.27761197 -0.6770649 -0.79109848 -0.59898937 -0.23992491 0.10832548][-0.98246551 -0.99856472 -1.15532 -1.2251374 -0.84519863 -0.090984106 0.57452512 0.72381592 0.34000182 -0.30896783 -0.67858624 -0.49350607 0.091065884 0.7034905 0.90439892][-1.2250667 -1.3066021 -1.5396761 -1.5238962 -0.88276982 0.23636794 1.206285 1.3330638 0.67100739 -0.2253952 -0.636268 -0.30946493 0.52837348 1.3073196 1.3217642][-1.4195566 -1.5788703 -1.8760769 -1.7986838 -0.98186827 0.38867021 1.6040547 1.7616341 0.93642569 -0.073880911 -0.52438152 -0.23433352 0.52975821 1.1629701 1.058151][-1.4004641 -1.625598 -2.0192685 -1.9820852 -1.202237 0.069667816 1.1711454 1.3810601 0.77903509 -0.027947903 -0.43537664 -0.25204515 0.2874403 0.69899344 0.58233809][-1.2071037 -1.4708374 -1.9377412 -2.0533659 -1.5307471 -0.55322981 0.31057787 0.56660986 0.26819563 -0.20880103 -0.48022068 -0.40494835 -0.081638813 0.18129826 0.14083719][-1.0840681 -1.3870945 -1.9103847 -2.2040973 -1.9854672 -1.3539805 -0.7300725 -0.44565439 -0.49061751 -0.66949451 -0.80025971 -0.78231704 -0.62195516 -0.42783928 -0.30126667][-1.2676933 -1.6108762 -2.139437 -2.5257816 -2.5470781 -2.2292936 -1.8096763 -1.5013361 -1.3467966 -1.283789 -1.2866325 -1.2948308 -1.2186638 -1.0153596 -0.68663335][-1.6673762 -1.9925002 -2.4325769 -2.7935352 -2.9355841 -2.8223829 -2.5595508 -2.2519093 -1.9769897 -1.78619 -1.7176363 -1.7527459 -1.7539604 -1.5534587 -1.1014276]]...]
INFO - root - 2017-12-16 08:49:12.402355: step 21110, loss = 0.63, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 14h:30m:56s remains)
INFO - root - 2017-12-16 08:49:14.054669: step 21120, loss = 0.56, batch loss = 0.30 (47.1 examples/sec; 0.170 sec/batch; 14h:41m:18s remains)
INFO - root - 2017-12-16 08:49:15.709884: step 21130, loss = 0.66, batch loss = 0.40 (47.5 examples/sec; 0.169 sec/batch; 14h:34m:54s remains)
INFO - root - 2017-12-16 08:49:17.371191: step 21140, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 14h:28m:46s remains)
INFO - root - 2017-12-16 08:49:19.035327: step 21150, loss = 0.48, batch loss = 0.22 (48.6 examples/sec; 0.164 sec/batch; 14h:13m:24s remains)
INFO - root - 2017-12-16 08:49:20.686492: step 21160, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:32m:23s remains)
INFO - root - 2017-12-16 08:49:22.366825: step 21170, loss = 0.57, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:14m:17s remains)
INFO - root - 2017-12-16 08:49:24.044789: step 21180, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:26m:14s remains)
INFO - root - 2017-12-16 08:49:25.711235: step 21190, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.165 sec/batch; 14h:18m:32s remains)
INFO - root - 2017-12-16 08:49:27.357688: step 21200, loss = 0.53, batch loss = 0.27 (49.6 examples/sec; 0.161 sec/batch; 13h:56m:41s remains)
2017-12-16 08:49:27.802574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1171227 -3.755857 -3.2655 -3.0978942 -3.2161057 -3.3811474 -3.4364591 -3.437274 -3.3890836 -3.1906884 -2.8103437 -2.3276978 -1.9894617 -2.0965052 -2.5926409][-4.5381503 -4.127358 -3.6353838 -3.5078444 -3.6840887 -3.8291321 -3.8464568 -3.8540897 -3.8637748 -3.6900756 -3.2701368 -2.7852638 -2.4149396 -2.4035769 -2.7785256][-4.5092316 -3.9158692 -3.3335321 -3.1772871 -3.3384423 -3.3588483 -3.2153358 -3.2218132 -3.3794374 -3.3455014 -3.053566 -2.7443783 -2.4880638 -2.418937 -2.6830771][-4.1523972 -3.3090212 -2.544775 -2.2747138 -2.2624526 -2.0117939 -1.6168362 -1.6010944 -1.9415915 -2.2135158 -2.2182355 -2.2097604 -2.1667337 -2.1117387 -2.2919133][-3.6749172 -2.632163 -1.7333298 -1.2820306 -0.97481871 -0.36262155 0.41226935 0.52547026 -0.10373163 -0.80411768 -1.1585726 -1.3990557 -1.511914 -1.4906766 -1.5671511][-3.1312535 -1.9769174 -1.0553777 -0.50570345 0.1115272 1.2060027 2.5145082 2.767674 1.7862854 0.56953859 -0.13538051 -0.54442418 -0.77416646 -0.82942426 -0.84935081][-2.4812064 -1.1855135 -0.15561581 0.47005868 1.2490335 2.7230363 4.5446482 4.8351736 3.3349748 1.6569886 0.64660311 0.16448331 -0.055730343 -0.13835406 -0.21050096][-1.8764204 -0.44175136 0.70404172 1.3152678 2.0267076 3.3437786 5.0827107 5.2888584 3.5243134 1.7514772 0.77084613 0.44172645 0.43967032 0.44859123 0.2837534][-1.9141074 -0.52099895 0.70038533 1.2812257 1.67482 2.3550091 3.185698 3.161305 1.9432755 0.69702792 0.12863374 0.20473099 0.57793355 0.76744676 0.49544][-2.5259962 -1.143154 0.21597886 0.77945161 0.81083393 0.83250356 1.0610304 0.98304629 0.30836868 -0.3928833 -0.58403432 -0.17648983 0.57038903 0.93729329 0.527683][-3.4751744 -2.1916168 -0.72885633 -0.083121061 -0.18294716 -0.48428059 -0.5106194 -0.59517848 -0.99897158 -1.434954 -1.4296483 -0.82586777 0.11117268 0.51599813 0.0088067055][-4.524066 -3.5871224 -2.2254272 -1.4533563 -1.4935173 -1.7827792 -1.8654236 -1.9536796 -2.2776577 -2.6004803 -2.5135198 -1.8722274 -0.93838441 -0.55481553 -1.0332383][-5.1495571 -4.5359178 -3.496469 -2.8425694 -2.83048 -3.0357809 -3.1316333 -3.1612909 -3.3096673 -3.4653425 -3.3350818 -2.8371465 -2.1365752 -1.8246825 -2.1611657][-4.9840307 -4.5883169 -3.7892528 -3.3110044 -3.3595228 -3.582315 -3.6590164 -3.6155772 -3.5944419 -3.5725787 -3.4220603 -3.1129513 -2.7072039 -2.5089631 -2.75903][-4.1660032 -3.8656626 -3.2727518 -2.9423578 -3.0681796 -3.3165636 -3.3955073 -3.3169179 -3.2067337 -3.1141467 -2.9972575 -2.8373761 -2.6148629 -2.4729097 -2.6058962]]...]
INFO - root - 2017-12-16 08:49:29.452265: step 21210, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 14h:15m:06s remains)
INFO - root - 2017-12-16 08:49:31.113136: step 21220, loss = 0.50, batch loss = 0.24 (49.2 examples/sec; 0.163 sec/batch; 14h:03m:46s remains)
INFO - root - 2017-12-16 08:49:32.749184: step 21230, loss = 0.52, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:19m:59s remains)
INFO - root - 2017-12-16 08:49:34.411690: step 21240, loss = 0.59, batch loss = 0.33 (48.6 examples/sec; 0.165 sec/batch; 14h:14m:41s remains)
INFO - root - 2017-12-16 08:49:36.076685: step 21250, loss = 0.68, batch loss = 0.42 (48.6 examples/sec; 0.165 sec/batch; 14h:13m:50s remains)
INFO - root - 2017-12-16 08:49:37.721913: step 21260, loss = 0.59, batch loss = 0.33 (47.1 examples/sec; 0.170 sec/batch; 14h:40m:19s remains)
INFO - root - 2017-12-16 08:49:39.410761: step 21270, loss = 0.60, batch loss = 0.34 (46.4 examples/sec; 0.172 sec/batch; 14h:53m:36s remains)
INFO - root - 2017-12-16 08:49:41.099083: step 21280, loss = 0.50, batch loss = 0.24 (47.2 examples/sec; 0.169 sec/batch; 14h:38m:46s remains)
INFO - root - 2017-12-16 08:49:42.797731: step 21290, loss = 0.54, batch loss = 0.28 (46.6 examples/sec; 0.172 sec/batch; 14h:49m:36s remains)
INFO - root - 2017-12-16 08:49:44.481244: step 21300, loss = 0.51, batch loss = 0.25 (49.4 examples/sec; 0.162 sec/batch; 13h:59m:24s remains)
2017-12-16 08:49:44.966483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0225246 -1.1519164 -1.0343994 -0.52572727 0.14164829 0.53117347 0.21552014 -0.53363478 -1.2056104 -1.5732225 -1.5882554 -1.3348331 -0.98207355 -0.70018911 -0.56247532][-1.090148 -1.2358992 -1.1034894 -0.50069213 0.34246564 0.9008975 0.65706158 -0.13383293 -0.91773307 -1.423524 -1.5679833 -1.3874445 -1.0526277 -0.75374234 -0.58979762][-1.0978999 -1.2109798 -1.0397155 -0.35773778 0.62261319 1.2892623 1.0686202 0.19326138 -0.68270063 -1.3221189 -1.5826085 -1.457902 -1.1336327 -0.80042696 -0.5983032][-1.0912832 -1.1534811 -0.91919184 -0.19062257 0.82321262 1.5162673 1.267724 0.31164551 -0.65967119 -1.4243464 -1.7740176 -1.670814 -1.2998818 -0.87734163 -0.59558094][-1.0935189 -1.0692719 -0.72587681 0.053310633 1.0283337 1.6610968 1.3573058 0.31839156 -0.76358986 -1.6508241 -2.0885258 -1.9794827 -1.5068493 -0.94977677 -0.5556066][-1.0851607 -0.95227468 -0.46854854 0.39740205 1.3205225 1.8699892 1.5097687 0.39618659 -0.81303263 -1.8043535 -2.30085 -2.1746304 -1.6249049 -0.96660018 -0.4814378][-1.0448707 -0.74914992 -0.098912477 0.85447359 1.8052781 2.3422935 1.9759686 0.82778382 -0.51244462 -1.6415374 -2.2461262 -2.2021756 -1.6737974 -0.98520136 -0.46474969][-1.0464559 -0.629218 0.14417458 1.1501329 2.1789243 2.8484023 2.6551821 1.5665441 0.12383938 -1.1998391 -1.9779757 -2.1158614 -1.6984665 -1.0575352 -0.54527056][-1.1083791 -0.68021584 0.080913782 1.0925362 2.1681616 2.96362 2.9901578 2.0857866 0.63912082 -0.85377061 -1.8017637 -2.0693631 -1.7471256 -1.1678411 -0.71036088][-1.1738242 -0.8280102 -0.21040654 0.65884829 1.6224251 2.3891881 2.5241921 1.7994468 0.50960708 -0.92643225 -1.8650694 -2.1048231 -1.783806 -1.2545283 -0.86178255][-1.1755891 -0.97740793 -0.57894862 -0.0075116158 0.68281364 1.3111792 1.4813046 0.949631 -0.091845512 -1.2496731 -1.9602925 -2.0646534 -1.7044632 -1.2276458 -0.90317309][-1.119625 -1.0881327 -0.91231596 -0.62414169 -0.23183918 0.17187977 0.34886789 0.035357 -0.71177852 -1.5239935 -1.9557428 -1.8771341 -1.4905692 -1.0819664 -0.82943344][-1.0155503 -1.0830069 -1.0770708 -1.0162112 -0.88351953 -0.67917562 -0.53603292 -0.70747733 -1.1745051 -1.6231108 -1.7623997 -1.5544436 -1.1714247 -0.8414315 -0.66497445][-0.89516771 -0.97954082 -1.0584112 -1.1369494 -1.1836673 -1.1315036 -1.0566111 -1.1465338 -1.3500831 -1.475903 -1.4050103 -1.1495976 -0.83212864 -0.595261 -0.4851613][-0.8066262 -0.87114012 -0.97508514 -1.114645 -1.2350572 -1.250488 -1.2236232 -1.2359673 -1.2414304 -1.1699263 -1.0138235 -0.79638803 -0.59688497 -0.45724678 -0.39332843]]...]
INFO - root - 2017-12-16 08:49:46.645334: step 21310, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 14h:36m:08s remains)
INFO - root - 2017-12-16 08:49:48.303679: step 21320, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:19m:50s remains)
INFO - root - 2017-12-16 08:49:49.945500: step 21330, loss = 0.61, batch loss = 0.35 (49.5 examples/sec; 0.161 sec/batch; 13h:57m:20s remains)
INFO - root - 2017-12-16 08:49:51.606093: step 21340, loss = 0.57, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 14h:32m:21s remains)
INFO - root - 2017-12-16 08:49:53.293194: step 21350, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:09m:13s remains)
INFO - root - 2017-12-16 08:49:54.954369: step 21360, loss = 0.66, batch loss = 0.40 (48.3 examples/sec; 0.166 sec/batch; 14h:19m:10s remains)
INFO - root - 2017-12-16 08:49:56.635986: step 21370, loss = 0.61, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 14h:04m:54s remains)
INFO - root - 2017-12-16 08:49:58.297642: step 21380, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:23m:07s remains)
INFO - root - 2017-12-16 08:49:59.949100: step 21390, loss = 0.63, batch loss = 0.38 (48.4 examples/sec; 0.165 sec/batch; 14h:17m:38s remains)
INFO - root - 2017-12-16 08:50:01.600932: step 21400, loss = 0.65, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 14h:10m:55s remains)
2017-12-16 08:50:02.040791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6261857 -2.5938654 -2.2643616 -1.7055423 -1.2378025 -1.0698671 -1.0941455 -1.3744887 -1.8829838 -2.4003158 -2.8288081 -3.1279893 -3.1438928 -2.6767483 -1.7791823][-2.8956814 -2.9470372 -2.6579807 -2.143827 -1.6773441 -1.4016248 -1.2225288 -1.295149 -1.6785376 -2.1923869 -2.6634102 -2.9889598 -3.0756714 -2.747895 -1.8627489][-2.9631095 -3.0421803 -2.8133225 -2.3195708 -1.7128103 -1.11451 -0.62594974 -0.51608539 -0.88874424 -1.5812572 -2.2332385 -2.7063832 -2.8894153 -2.629653 -1.6935521][-2.7164619 -2.8358288 -2.6522663 -2.1416571 -1.3899472 -0.54771578 0.24056149 0.46986556 0.045492887 -0.93589413 -1.873489 -2.5145247 -2.8288603 -2.6329184 -1.6430624][-2.2679157 -2.4286959 -2.3180859 -1.8299994 -1.0324209 0.0013420582 1.0264387 1.3366697 0.74422407 -0.56167877 -1.7784975 -2.5713491 -2.9774609 -2.785728 -1.7593548][-1.6761062 -1.9826176 -2.0940318 -1.7675705 -1.0222651 0.093390465 1.1896911 1.4349699 0.59924793 -0.85106814 -2.0444837 -2.7171834 -2.9922829 -2.7541504 -1.7629143][-1.0195674 -1.5109004 -1.8486441 -1.6543491 -0.89110029 0.31565022 1.4859533 1.6886582 0.61459851 -1.0775748 -2.3307681 -2.8792298 -2.9386334 -2.5291619 -1.5352083][-0.53519762 -1.1904331 -1.6760902 -1.5821238 -0.82509208 0.49363589 1.7912645 2.0047674 0.69735813 -1.2868489 -2.6106203 -2.9143507 -2.7066388 -2.2017388 -1.3763525][-0.18979502 -1.0155686 -1.6435778 -1.6167315 -0.88598287 0.42900562 1.6870432 1.8966398 0.58241987 -1.437853 -2.7491117 -2.8574581 -2.3817003 -1.7739004 -1.1743978][0.053655863 -0.91111183 -1.6643963 -1.7421794 -1.0397416 0.1251328 1.1152048 1.3446419 0.2809031 -1.4686178 -2.5933805 -2.5538771 -1.960048 -1.3304427 -0.8782196][-0.034208775 -1.0264438 -1.8353615 -1.9675639 -1.3618792 -0.44656169 0.24702907 0.44772792 -0.19965887 -1.3929433 -2.1864264 -2.028868 -1.4531858 -0.88380933 -0.5373683][-0.38403022 -1.3006753 -2.0634375 -2.1597342 -1.6608233 -0.94677591 -0.41113925 -0.25692558 -0.56949162 -1.2454144 -1.7254982 -1.6163828 -1.1929163 -0.77007174 -0.53324389][-0.98181379 -1.8043824 -2.447727 -2.4511154 -1.9638402 -1.3641649 -0.92620111 -0.68698776 -0.76250613 -1.1647977 -1.5586807 -1.6206913 -1.3983819 -1.0774605 -0.86977637][-1.5767759 -2.3052928 -2.8300297 -2.7516873 -2.288667 -1.7386284 -1.3280596 -1.0576652 -1.0849756 -1.3936293 -1.6771369 -1.7420864 -1.5611341 -1.2359209 -1.0504495][-1.8837776 -2.6084549 -3.1479709 -3.1599193 -2.7660415 -2.2121756 -1.6940227 -1.2901026 -1.1800734 -1.3730747 -1.6155765 -1.6828715 -1.5010327 -1.1855745 -1.0367332]]...]
INFO - root - 2017-12-16 08:50:03.695951: step 21410, loss = 0.55, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 14h:31m:08s remains)
INFO - root - 2017-12-16 08:50:05.359505: step 21420, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 14h:27m:22s remains)
INFO - root - 2017-12-16 08:50:07.015347: step 21430, loss = 0.58, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:20m:53s remains)
INFO - root - 2017-12-16 08:50:08.667340: step 21440, loss = 0.55, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 14h:24m:57s remains)
INFO - root - 2017-12-16 08:50:10.304363: step 21450, loss = 0.71, batch loss = 0.45 (48.0 examples/sec; 0.167 sec/batch; 14h:23m:16s remains)
INFO - root - 2017-12-16 08:50:11.971934: step 21460, loss = 0.64, batch loss = 0.39 (48.5 examples/sec; 0.165 sec/batch; 14h:14m:59s remains)
INFO - root - 2017-12-16 08:50:13.629407: step 21470, loss = 0.50, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:25m:13s remains)
INFO - root - 2017-12-16 08:50:15.274607: step 21480, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:25m:25s remains)
INFO - root - 2017-12-16 08:50:16.939365: step 21490, loss = 0.53, batch loss = 0.27 (47.0 examples/sec; 0.170 sec/batch; 14h:42m:04s remains)
INFO - root - 2017-12-16 08:50:18.613787: step 21500, loss = 0.60, batch loss = 0.34 (47.9 examples/sec; 0.167 sec/batch; 14h:26m:13s remains)
2017-12-16 08:50:19.097772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.951867 -2.0784538 -2.1864762 -2.260546 -2.3702772 -2.575901 -2.8040681 -2.8963385 -2.8940065 -2.8437338 -2.7361779 -2.5883286 -2.4147675 -2.2303605 -2.0580211][-2.4308832 -2.6351793 -2.8249598 -2.9437144 -3.0946848 -3.3699961 -3.6933646 -3.7920649 -3.744657 -3.6983 -3.60255 -3.3880267 -3.0786324 -2.7539277 -2.4485643][-3.1527033 -3.456388 -3.6818755 -3.7367144 -3.7884276 -4.0722675 -4.3942137 -4.4696083 -4.41975 -4.5211263 -4.5904541 -4.4018631 -3.9934857 -3.4986804 -3.0333304][-3.841532 -4.1406374 -4.24158 -3.9665031 -3.6631804 -3.7642336 -3.9781241 -3.9840171 -4.0285692 -4.481739 -4.9054856 -4.9395657 -4.6623535 -4.217803 -3.6281276][-4.3084669 -4.4554691 -4.1285563 -3.2708173 -2.4161587 -2.0454078 -1.8573384 -1.7133493 -2.0295033 -3.0692823 -4.0372229 -4.4882493 -4.6123619 -4.4823861 -3.9498892][-4.3988061 -4.3158131 -3.4731514 -1.9449911 -0.40758193 0.63626361 1.5110576 2.0219285 1.3674951 -0.46209478 -2.0600638 -2.997623 -3.6821132 -4.0900865 -3.832974][-4.0784874 -3.8248372 -2.6260021 -0.57577348 1.5022314 3.2155297 5.0631952 6.2377129 5.0282383 2.4744737 0.5117836 -0.83773959 -2.1120265 -3.1550407 -3.2832465][-3.7207823 -3.4110382 -2.0710523 0.067778111 2.3155706 4.4633188 7.1536703 9.2800531 7.2844534 4.3555069 2.4239981 0.89558244 -0.80961978 -2.2292838 -2.6980455][-3.7293248 -3.585093 -2.4451764 -0.66743374 1.2129588 3.1215832 5.320466 6.7495861 5.7841444 3.7099741 2.3745782 1.1650419 -0.51083195 -1.9838238 -2.5643308][-3.9428132 -4.0600271 -3.4584141 -2.3729298 -1.0933285 0.27678037 1.6886008 2.5436757 2.2571213 1.2724135 0.65064144 -0.014807701 -1.2616152 -2.4370096 -2.8622735][-4.1071534 -4.5045872 -4.4227285 -3.984807 -3.3085337 -2.559386 -1.7898238 -1.2581066 -1.2260017 -1.4946717 -1.5732753 -1.8228488 -2.5561726 -3.2003295 -3.3422737][-4.0243883 -4.5442719 -4.802072 -4.7929268 -4.5382752 -4.1982536 -3.8701148 -3.5959549 -3.3778372 -3.2974603 -3.2061586 -3.2393558 -3.5326447 -3.7500448 -3.6618862][-3.5673122 -4.0541677 -4.406251 -4.628973 -4.6278038 -4.5434432 -4.526391 -4.4322433 -4.2075348 -4.0229464 -3.8889327 -3.8257279 -3.8695688 -3.8388622 -3.6283088][-2.8963904 -3.2521498 -3.5581884 -3.7848904 -3.8990803 -3.9724512 -4.0501871 -4.0424623 -3.9137893 -3.7591221 -3.6476912 -3.5632253 -3.5187361 -3.4100296 -3.2010043][-2.3072863 -2.4935858 -2.6539423 -2.7897279 -2.9003165 -2.9951162 -3.0678406 -3.0884271 -3.0343266 -2.9627037 -2.8941104 -2.8416047 -2.80122 -2.7319775 -2.6123118]]...]
INFO - root - 2017-12-16 08:50:20.753701: step 21510, loss = 0.51, batch loss = 0.25 (46.2 examples/sec; 0.173 sec/batch; 14h:58m:04s remains)
INFO - root - 2017-12-16 08:50:22.443876: step 21520, loss = 0.47, batch loss = 0.22 (49.6 examples/sec; 0.161 sec/batch; 13h:55m:14s remains)
INFO - root - 2017-12-16 08:50:24.134395: step 21530, loss = 0.61, batch loss = 0.35 (49.4 examples/sec; 0.162 sec/batch; 13h:58m:48s remains)
INFO - root - 2017-12-16 08:50:25.805544: step 21540, loss = 0.52, batch loss = 0.26 (46.4 examples/sec; 0.173 sec/batch; 14h:54m:04s remains)
INFO - root - 2017-12-16 08:50:27.471199: step 21550, loss = 0.60, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 14h:22m:18s remains)
INFO - root - 2017-12-16 08:50:29.102812: step 21560, loss = 0.59, batch loss = 0.33 (51.0 examples/sec; 0.157 sec/batch; 13h:32m:26s remains)
INFO - root - 2017-12-16 08:50:30.808117: step 21570, loss = 0.64, batch loss = 0.38 (47.7 examples/sec; 0.168 sec/batch; 14h:28m:56s remains)
INFO - root - 2017-12-16 08:50:32.482613: step 21580, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 14h:16m:17s remains)
INFO - root - 2017-12-16 08:50:34.143345: step 21590, loss = 0.61, batch loss = 0.36 (46.4 examples/sec; 0.172 sec/batch; 14h:53m:50s remains)
INFO - root - 2017-12-16 08:50:35.798012: step 21600, loss = 0.60, batch loss = 0.34 (49.2 examples/sec; 0.163 sec/batch; 14h:02m:08s remains)
2017-12-16 08:50:36.273996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.122519 -2.9787564 -2.9303782 -3.0067468 -3.0864553 -2.8594682 -2.1527481 -1.4491007 -1.1485795 -1.2565686 -1.4724551 -1.7311835 -1.7781763 -1.5860965 -1.3253652][-2.882947 -2.7700763 -2.7370293 -2.8604152 -3.0673952 -2.9729447 -2.3785036 -1.7051594 -1.3264275 -1.3239932 -1.4823236 -1.7411559 -1.8280398 -1.6248093 -1.3854287][-2.8302886 -2.7402158 -2.6978998 -2.8585632 -3.1864495 -3.2272387 -2.6761422 -1.973231 -1.5863931 -1.6180062 -1.8519373 -2.1374021 -2.1503472 -1.8663036 -1.5643215][-2.8818326 -2.7309058 -2.6102748 -2.7598612 -3.0972605 -3.08204 -2.4185479 -1.6825838 -1.463414 -1.7557278 -2.2250173 -2.5759742 -2.5228982 -2.1672115 -1.7968202][-2.8935339 -2.6174402 -2.3767893 -2.4419513 -2.6055167 -2.2905235 -1.3800673 -0.65373969 -0.77832913 -1.5832393 -2.4017284 -2.8694034 -2.7989316 -2.365103 -1.9500202][-2.7286706 -2.3219237 -1.9350355 -1.7812378 -1.5848802 -0.79752564 0.4234786 1.0878382 0.48315763 -0.93198442 -2.1972833 -2.8779378 -2.8686705 -2.4299273 -1.998062][-2.2730455 -1.816545 -1.3054529 -0.84461641 -0.10434914 1.2256973 2.6987591 3.1889534 2.1056967 0.13794255 -1.5928314 -2.5842967 -2.7541618 -2.4024913 -1.9898224][-1.8251433 -1.3971564 -0.84849823 -0.14780569 1.0055945 2.813416 4.5548048 4.94846 3.5779443 1.2333927 -0.86306047 -2.1796317 -2.6067698 -2.3890138 -1.9997185][-1.9011981 -1.4969999 -0.94794548 -0.13835812 1.1553593 3.0489383 4.8181691 5.2743325 3.994729 1.6672144 -0.49186146 -1.9651922 -2.5746338 -2.4647136 -2.0862594][-2.352217 -2.0273542 -1.4903852 -0.70637619 0.43092418 1.9936175 3.4705272 3.9909034 3.0241218 1.1026423 -0.79847813 -2.1611845 -2.730485 -2.6051016 -2.2049484][-2.9480855 -2.7424474 -2.290664 -1.5669696 -0.58785319 0.68258572 1.925808 2.3877172 1.6085148 0.062664032 -1.4907602 -2.5748456 -2.959681 -2.728301 -2.2803543][-3.1177626 -3.0831075 -2.7877679 -2.2294688 -1.4116287 -0.29218793 0.8732841 1.3543189 0.73218465 -0.57762158 -1.9015987 -2.8091125 -3.0574 -2.7758946 -2.3199821][-2.7642283 -2.9070506 -2.8726559 -2.5841961 -1.9558012 -0.90967727 0.32269883 0.98975086 0.57506061 -0.56760347 -1.7915403 -2.6670866 -2.9314196 -2.6992939 -2.3027191][-2.1073091 -2.3801544 -2.6172688 -2.6033666 -2.1883466 -1.2393081 0.079583168 0.95035696 0.75129652 -0.23745084 -1.4115599 -2.3471992 -2.7140744 -2.5822809 -2.2572136][-1.7786044 -2.0946846 -2.4042239 -2.5125279 -2.237654 -1.3206593 0.13810182 1.2202604 1.1748409 0.26452684 -0.9381001 -2.0073953 -2.5425911 -2.5226359 -2.2444763]]...]
INFO - root - 2017-12-16 08:50:37.945323: step 21610, loss = 0.57, batch loss = 0.31 (50.1 examples/sec; 0.160 sec/batch; 13h:47m:39s remains)
INFO - root - 2017-12-16 08:50:39.655265: step 21620, loss = 0.48, batch loss = 0.22 (47.2 examples/sec; 0.170 sec/batch; 14h:38m:25s remains)
INFO - root - 2017-12-16 08:50:41.349952: step 21630, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:34m:14s remains)
INFO - root - 2017-12-16 08:50:43.031804: step 21640, loss = 0.48, batch loss = 0.22 (48.6 examples/sec; 0.165 sec/batch; 14h:12m:34s remains)
INFO - root - 2017-12-16 08:50:44.713968: step 21650, loss = 0.47, batch loss = 0.21 (47.3 examples/sec; 0.169 sec/batch; 14h:35m:57s remains)
INFO - root - 2017-12-16 08:50:46.403185: step 21660, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:25m:37s remains)
INFO - root - 2017-12-16 08:50:48.074174: step 21670, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.170 sec/batch; 14h:38m:48s remains)
INFO - root - 2017-12-16 08:50:49.739349: step 21680, loss = 0.47, batch loss = 0.21 (49.6 examples/sec; 0.161 sec/batch; 13h:54m:47s remains)
INFO - root - 2017-12-16 08:50:51.405284: step 21690, loss = 0.53, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 14h:30m:47s remains)
INFO - root - 2017-12-16 08:50:53.064341: step 21700, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:35m:39s remains)
2017-12-16 08:50:53.569546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0505807 -3.3179743 -3.3422213 -3.2393498 -3.0062327 -2.6488347 -2.1854811 -1.7732136 -1.3754727 -0.803144 -0.075858355 0.43406725 0.50466347 0.16526079 -0.32474542][-2.4245758 -2.8084564 -3.0755312 -3.2092998 -3.0217452 -2.5008419 -1.8216894 -1.4042847 -1.2448375 -1.0242711 -0.5551492 -0.12549305 0.088055849 -0.087666512 -0.53138936][-1.8649297 -2.5226402 -3.0808177 -3.3766146 -3.1700754 -2.4299514 -1.5292885 -1.1094224 -1.2228837 -1.4553533 -1.4198925 -1.2374569 -1.0391918 -1.0912209 -1.3231225][-1.3497895 -2.2578406 -3.0504227 -3.4214153 -3.1492314 -2.1992784 -1.1093475 -0.61572683 -0.90123296 -1.5311346 -2.0318124 -2.2492721 -2.2560258 -2.2899487 -2.3619697][-0.90032113 -1.9755161 -2.8204026 -3.1458664 -2.7655458 -1.6169175 -0.23823619 0.44191265 0.12198305 -0.79119754 -1.809597 -2.5403976 -2.8091772 -2.9158723 -2.9867182][-0.41641605 -1.5938513 -2.4442701 -2.7453446 -2.2574284 -0.94478643 0.72387409 1.6728451 1.4797299 0.40477657 -0.98829722 -2.1001492 -2.6222837 -2.8252189 -2.9455087][-0.324054 -1.4851835 -2.3586962 -2.6472006 -2.0892615 -0.61673856 1.2194736 2.3290317 2.2314279 1.0899761 -0.47085583 -1.7028512 -2.2532783 -2.4521563 -2.587194][-0.61456931 -1.6379671 -2.5035775 -2.8987792 -2.42376 -1.0576891 0.69660926 1.8304541 1.7988546 0.78610396 -0.64385545 -1.7374542 -2.2298014 -2.40478 -2.5312552][-0.85584509 -1.6979785 -2.5449567 -3.083477 -2.8831892 -1.931477 -0.57695782 0.37188625 0.4411962 -0.31784153 -1.3732904 -2.1411345 -2.4536617 -2.5588925 -2.6574407][-0.98119414 -1.5085115 -2.2253766 -2.9241762 -3.1231112 -2.7543905 -2.0863545 -1.5185249 -1.3568333 -1.8167969 -2.466203 -2.8293903 -2.8288298 -2.7519455 -2.7420788][-0.90512145 -1.1005241 -1.6041346 -2.3173792 -2.8462408 -3.105325 -3.1754479 -3.0627129 -2.9650903 -3.211122 -3.5114062 -3.5456462 -3.2905512 -2.9859254 -2.7810352][-0.67790818 -0.52963138 -0.73523819 -1.3963227 -2.1335747 -2.8732722 -3.5713103 -3.9291131 -3.9814405 -4.1367779 -4.2265182 -4.0246749 -3.6331463 -3.1743736 -2.7875326][-0.45845807 -0.088953495 -0.038614273 -0.49964929 -1.2548033 -2.2540891 -3.3560555 -4.0140629 -4.2110434 -4.2960835 -4.2520261 -3.947417 -3.5501826 -3.1121707 -2.6108994][-0.26909685 0.1348207 0.21293855 -0.13149714 -0.77060187 -1.7743766 -2.9103055 -3.5725718 -3.7276261 -3.6793635 -3.5225348 -3.273315 -3.0359445 -2.7080424 -2.1143742][-0.086537838 0.14589572 0.085020542 -0.26038766 -0.82706571 -1.7031929 -2.6649787 -3.1467571 -3.0640807 -2.7266085 -2.3779898 -2.2548702 -2.3456838 -2.3026102 -1.8211074]]...]
INFO - root - 2017-12-16 08:50:55.225588: step 21710, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 14h:07m:48s remains)
INFO - root - 2017-12-16 08:50:56.920269: step 21720, loss = 0.50, batch loss = 0.24 (45.8 examples/sec; 0.175 sec/batch; 15h:05m:37s remains)
INFO - root - 2017-12-16 08:50:58.589537: step 21730, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:21m:20s remains)
INFO - root - 2017-12-16 08:51:00.278419: step 21740, loss = 0.50, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 14h:39m:55s remains)
INFO - root - 2017-12-16 08:51:01.994482: step 21750, loss = 0.55, batch loss = 0.29 (46.0 examples/sec; 0.174 sec/batch; 15h:00m:19s remains)
INFO - root - 2017-12-16 08:51:03.739108: step 21760, loss = 0.57, batch loss = 0.31 (46.4 examples/sec; 0.172 sec/batch; 14h:53m:20s remains)
INFO - root - 2017-12-16 08:51:05.434655: step 21770, loss = 0.71, batch loss = 0.45 (48.1 examples/sec; 0.166 sec/batch; 14h:21m:20s remains)
INFO - root - 2017-12-16 08:51:07.096021: step 21780, loss = 0.48, batch loss = 0.22 (47.9 examples/sec; 0.167 sec/batch; 14h:24m:12s remains)
INFO - root - 2017-12-16 08:51:08.803387: step 21790, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 14h:10m:18s remains)
INFO - root - 2017-12-16 08:51:10.458427: step 21800, loss = 0.51, batch loss = 0.25 (46.7 examples/sec; 0.171 sec/batch; 14h:46m:45s remains)
2017-12-16 08:51:10.944084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6914935 -1.6469505 -1.5984707 -1.628871 -1.6877086 -1.595808 -1.4278822 -1.4664799 -1.7422276 -2.0348625 -2.1757185 -2.0861788 -1.8306031 -1.613577 -1.5184369][-1.782317 -1.7498814 -1.7397131 -1.8948572 -2.1219327 -2.0911424 -1.8705232 -1.8942912 -2.2703383 -2.6648457 -2.7973838 -2.5814319 -2.1348479 -1.7579958 -1.563828][-1.8304029 -1.837333 -1.9126173 -2.2203069 -2.5552385 -2.4406514 -1.9454422 -1.8479471 -2.4005625 -3.0854721 -3.3851266 -3.1318278 -2.5157902 -1.9484738 -1.6430707][-1.8679649 -1.9471378 -2.1457775 -2.5609379 -2.8219728 -2.3469551 -1.329159 -0.9795661 -1.7877293 -2.9812217 -3.6938159 -3.5598316 -2.8484957 -2.1177566 -1.7074131][-1.9215058 -2.0981078 -2.4180243 -2.8441753 -2.860945 -1.7848753 -0.074384689 0.59851313 -0.53924787 -2.3796823 -3.6015558 -3.7008324 -3.0251155 -2.2092195 -1.7405251][-1.9922048 -2.2675207 -2.65773 -2.9921241 -2.66233 -0.944597 1.4586153 2.4559157 1.0131397 -1.3403702 -3.0244949 -3.4471393 -2.9632318 -2.2097261 -1.7423187][-2.0536046 -2.3769479 -2.7726264 -2.9760716 -2.3309081 -0.10729718 2.8588431 4.2108917 2.6456125 -0.094002485 -2.1430495 -2.9281583 -2.738827 -2.1675229 -1.7503654][-2.0803113 -2.3948059 -2.7538903 -2.8396406 -2.0007093 0.41631651 3.6447251 5.3495188 3.8580205 0.94904113 -1.394232 -2.5086102 -2.5919602 -2.1743352 -1.7954609][-2.0724247 -2.3450937 -2.6435459 -2.6394994 -1.7817816 0.46284103 3.4192913 5.0103312 3.5805809 0.83138466 -1.4486578 -2.5860198 -2.6871991 -2.2751536 -1.8759403][-2.0422924 -2.2559319 -2.5004668 -2.484164 -1.8018844 -0.01417923 2.2868707 3.3257864 1.9428136 -0.42079115 -2.2756593 -3.0779829 -2.946439 -2.404165 -1.9494495][-2.034658 -2.1702013 -2.395308 -2.4688561 -2.0572233 -0.76458848 0.88522243 1.3884265 0.11506772 -1.7861543 -3.1440771 -3.5201774 -3.1155114 -2.4369206 -1.9493062][-2.1331511 -2.1619873 -2.2889159 -2.4079185 -2.2377772 -1.2682112 0.068322182 0.3242321 -0.88142967 -2.4801192 -3.5159311 -3.6495304 -3.08106 -2.3563485 -1.8933539][-2.4133239 -2.2862132 -2.1808045 -2.2512391 -2.1514325 -1.2408873 0.096116781 0.31969 -0.8896898 -2.4283283 -3.4031985 -3.528049 -2.9580588 -2.2592366 -1.838402][-2.7862568 -2.4937499 -2.1027262 -2.0115001 -1.8150196 -0.66619623 0.86741686 1.0442677 -0.31057191 -1.9985523 -3.0905302 -3.3268096 -2.8327687 -2.1895 -1.7967312][-2.974997 -2.5635421 -2.02638 -1.7868659 -1.3511503 0.11582565 1.7800448 1.743984 0.084669828 -1.8039205 -2.9679267 -3.2354386 -2.7897923 -2.1666758 -1.781305]]...]
INFO - root - 2017-12-16 08:51:12.642568: step 21810, loss = 0.64, batch loss = 0.38 (48.8 examples/sec; 0.164 sec/batch; 14h:09m:31s remains)
INFO - root - 2017-12-16 08:51:14.330638: step 21820, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 14h:19m:00s remains)
INFO - root - 2017-12-16 08:51:15.999896: step 21830, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.164 sec/batch; 14h:07m:48s remains)
INFO - root - 2017-12-16 08:51:17.666788: step 21840, loss = 0.56, batch loss = 0.31 (46.4 examples/sec; 0.173 sec/batch; 14h:53m:25s remains)
INFO - root - 2017-12-16 08:51:19.320059: step 21850, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:28m:05s remains)
INFO - root - 2017-12-16 08:51:20.986316: step 21860, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:21m:20s remains)
INFO - root - 2017-12-16 08:51:22.672048: step 21870, loss = 0.57, batch loss = 0.31 (44.6 examples/sec; 0.179 sec/batch; 15h:28m:21s remains)
INFO - root - 2017-12-16 08:51:24.363529: step 21880, loss = 0.72, batch loss = 0.46 (46.3 examples/sec; 0.173 sec/batch; 14h:53m:42s remains)
INFO - root - 2017-12-16 08:51:26.036606: step 21890, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 14h:18m:25s remains)
INFO - root - 2017-12-16 08:51:27.698142: step 21900, loss = 0.47, batch loss = 0.21 (47.7 examples/sec; 0.168 sec/batch; 14h:27m:22s remains)
2017-12-16 08:51:28.243717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4832186 -1.5096798 -1.5844032 -1.6840315 -1.7862355 -1.8487866 -1.8983004 -1.9682966 -2.073288 -2.1167476 -2.0575473 -1.9097733 -1.6576729 -1.3500395 -1.0467905][-1.7225597 -1.8430645 -2.0140946 -2.2125115 -2.428896 -2.5994778 -2.7124302 -2.8672826 -3.086221 -3.2111764 -3.2002506 -3.0892889 -2.8102703 -2.3800945 -1.8634274][-1.9556799 -2.1440797 -2.3206434 -2.4517875 -2.60675 -2.7390401 -2.7825422 -2.9953768 -3.4247465 -3.7717924 -3.9631317 -4.0476565 -3.9172158 -3.5412631 -2.9452052][-2.1044545 -2.307771 -2.3910635 -2.2686131 -2.1419733 -1.99419 -1.7877312 -1.9741807 -2.6779585 -3.4139688 -3.9663539 -4.3229241 -4.453609 -4.2738724 -3.7231545][-2.1381648 -2.2843606 -2.1606426 -1.6176529 -0.98421395 -0.32215595 0.39549255 0.39506865 -0.57839727 -1.8440062 -2.9042435 -3.6599665 -4.1475878 -4.2038326 -3.7441516][-2.1091757 -2.1200628 -1.7281207 -0.6956203 0.61031842 2.0258498 3.4536815 3.8035264 2.5683475 0.70282793 -0.94256747 -2.2080526 -3.1751661 -3.5828006 -3.2608783][-2.1642625 -2.0104487 -1.3949214 -0.031434536 1.78969 3.9314914 6.1161242 6.9641342 5.7389889 3.510252 1.4032676 -0.43527448 -2.0434492 -2.9647307 -2.9143245][-2.3759263 -2.1736739 -1.5515122 -0.17196727 1.7977614 4.2355089 6.7767944 8.0692825 7.1815958 5.0559282 2.9626732 0.89994478 -1.1364586 -2.5003951 -2.7951856][-2.6460495 -2.6676762 -2.3214197 -1.2482939 0.40913296 2.5138531 4.7734051 6.0478354 5.5900455 4.2228761 2.7537065 0.95374417 -1.1106958 -2.57443 -2.9523389][-2.8124561 -3.158529 -3.2016671 -2.6088297 -1.4912322 -0.0476377 1.5820715 2.43856 2.1281643 1.4576716 0.66525149 -0.6662432 -2.3591056 -3.4996104 -3.6216245][-2.7904303 -3.3300335 -3.7086658 -3.5531092 -2.9031928 -1.9999437 -0.95593309 -0.54115951 -0.94209325 -1.4137619 -1.8934412 -2.8057528 -3.9144979 -4.5441647 -4.3253489][-2.6369255 -3.1905673 -3.7084742 -3.8233373 -3.5388474 -2.9954989 -2.3082709 -2.1226199 -2.5970647 -3.1219149 -3.4998448 -4.0618505 -4.5985851 -4.7601728 -4.3895597][-2.4048467 -2.8547113 -3.3232749 -3.5725408 -3.5019951 -3.1468346 -2.5983613 -2.4012456 -2.8448045 -3.4388163 -3.7939885 -4.0851188 -4.1771517 -4.0178604 -3.6403356][-2.1224046 -2.4423685 -2.8123844 -3.0943961 -3.1371872 -2.9114828 -2.4590821 -2.1937742 -2.4726245 -2.9541547 -3.214927 -3.2631431 -3.0741606 -2.7877574 -2.5451174][-1.8390076 -2.0563784 -2.3306828 -2.5687447 -2.6597226 -2.5153048 -2.1778564 -1.9338756 -2.0303307 -2.2971675 -2.4310064 -2.3588598 -2.1040287 -1.8430051 -1.7292644]]...]
INFO - root - 2017-12-16 08:51:29.929665: step 21910, loss = 0.52, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 14h:09m:44s remains)
INFO - root - 2017-12-16 08:51:31.588144: step 21920, loss = 0.59, batch loss = 0.34 (48.6 examples/sec; 0.164 sec/batch; 14h:11m:15s remains)
INFO - root - 2017-12-16 08:51:33.254328: step 21930, loss = 0.51, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 14h:22m:00s remains)
INFO - root - 2017-12-16 08:51:34.928943: step 21940, loss = 0.60, batch loss = 0.34 (44.5 examples/sec; 0.180 sec/batch; 15h:31m:07s remains)
INFO - root - 2017-12-16 08:51:36.585813: step 21950, loss = 0.64, batch loss = 0.38 (49.4 examples/sec; 0.162 sec/batch; 13h:57m:29s remains)
INFO - root - 2017-12-16 08:51:38.255490: step 21960, loss = 0.48, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 14h:21m:20s remains)
INFO - root - 2017-12-16 08:51:39.924832: step 21970, loss = 0.52, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:25m:52s remains)
INFO - root - 2017-12-16 08:51:41.593754: step 21980, loss = 0.78, batch loss = 0.52 (47.4 examples/sec; 0.169 sec/batch; 14h:33m:14s remains)
INFO - root - 2017-12-16 08:51:43.283089: step 21990, loss = 0.60, batch loss = 0.34 (48.8 examples/sec; 0.164 sec/batch; 14h:08m:31s remains)
INFO - root - 2017-12-16 08:51:44.971278: step 22000, loss = 0.55, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 14h:18m:25s remains)
2017-12-16 08:51:45.465727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4281287 -1.2837123 -1.0961337 -0.92998481 -0.85908151 -0.93492818 -1.1676319 -1.2205464 -0.95840323 -0.61242151 -0.46954536 -0.56086707 -0.78442764 -1.0337031 -1.2471269][-1.0056777 -0.86478674 -0.58173382 -0.19070315 0.17660928 0.26200032 -0.053871393 -0.39967299 -0.47821569 -0.42034292 -0.5684092 -0.91673231 -1.2731283 -1.5273013 -1.6502199][-0.48447061 -0.40187252 -0.065482616 0.5141468 1.1195493 1.3283296 0.95157003 0.32956433 -0.082109451 -0.32277107 -0.70887804 -1.2261122 -1.702275 -1.9826969 -2.039813][0.0921967 0.2010746 0.52600765 1.1199839 1.7201056 1.9155254 1.5103011 0.76812649 0.18461561 -0.23949337 -0.75915396 -1.3849688 -1.9371954 -2.2663958 -2.3266416][0.44759011 0.69414926 1.0683627 1.5553906 1.908165 1.8866572 1.4929121 0.89342666 0.3509686 -0.14118767 -0.69830251 -1.323972 -1.8925431 -2.2788956 -2.4042976][0.22879291 0.66445541 1.184238 1.6473551 1.8181233 1.6519918 1.3457263 0.99904013 0.64054537 0.24831963 -0.24038434 -0.82442892 -1.4534299 -1.973325 -2.2564313][-0.49767208 0.053359509 0.67347479 1.1828947 1.3794322 1.3406279 1.3118081 1.3295708 1.30983 1.1610415 0.77032113 0.1365521 -0.6510452 -1.373093 -1.8393691][-1.3907692 -0.80410516 -0.21134973 0.29771256 0.6421082 0.89011955 1.167202 1.5168889 1.8564296 2.0036998 1.7215948 1.0161865 0.090227127 -0.72699451 -1.1947914][-2.2150688 -1.6728044 -1.1449909 -0.68305171 -0.266845 0.12339139 0.51192284 0.96809483 1.4909382 1.8768144 1.8103132 1.2396436 0.40121174 -0.2718153 -0.51813054][-2.9653661 -2.578198 -2.1355999 -1.7020625 -1.2795622 -0.89064014 -0.53190744 -0.10403419 0.43983078 0.94423294 1.1013589 0.82206583 0.27693439 -0.1159234 -0.10143781][-3.3307986 -3.1697617 -2.894165 -2.5692921 -2.2447221 -1.9512048 -1.6779165 -1.3153235 -0.80042326 -0.25423312 0.077935457 0.079698324 -0.14674711 -0.24775338 -0.013138771][-3.1869583 -3.2070076 -3.1297247 -2.9985559 -2.8635857 -2.7462831 -2.6002417 -2.3219097 -1.8715518 -1.3518686 -0.95013082 -0.77871013 -0.79542494 -0.71012688 -0.37707448][-2.8129737 -2.8969824 -2.922507 -2.9335771 -2.9652174 -3.0159535 -3.0235391 -2.8993716 -2.6120958 -2.2239034 -1.8702266 -1.6749219 -1.626094 -1.4896655 -1.1570771][-2.4521048 -2.5285053 -2.5784714 -2.652149 -2.7596941 -2.8816419 -2.9764762 -2.9894052 -2.8902044 -2.6998024 -2.4994841 -2.385987 -2.3555164 -2.259948 -1.9950302][-2.1315632 -2.1947994 -2.262466 -2.3593795 -2.4761078 -2.59341 -2.7047935 -2.77306 -2.7848687 -2.7434559 -2.6914692 -2.6772501 -2.6796956 -2.6142404 -2.438241]]...]
INFO - root - 2017-12-16 08:51:47.182205: step 22010, loss = 0.53, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:40m:56s remains)
INFO - root - 2017-12-16 08:51:48.908781: step 22020, loss = 0.61, batch loss = 0.36 (45.6 examples/sec; 0.175 sec/batch; 15h:07m:46s remains)
INFO - root - 2017-12-16 08:51:50.648147: step 22030, loss = 0.55, batch loss = 0.29 (46.6 examples/sec; 0.172 sec/batch; 14h:47m:49s remains)
INFO - root - 2017-12-16 08:51:52.374903: step 22040, loss = 0.61, batch loss = 0.35 (47.6 examples/sec; 0.168 sec/batch; 14h:30m:21s remains)
INFO - root - 2017-12-16 08:51:54.103289: step 22050, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 14h:26m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:51:55.795723: step 22060, loss = 0.58, batch loss = 0.32 (46.0 examples/sec; 0.174 sec/batch; 15h:00m:10s remains)
INFO - root - 2017-12-16 08:51:57.490863: step 22070, loss = 0.61, batch loss = 0.35 (46.8 examples/sec; 0.171 sec/batch; 14h:44m:17s remains)
INFO - root - 2017-12-16 08:51:59.190588: step 22080, loss = 0.62, batch loss = 0.36 (45.5 examples/sec; 0.176 sec/batch; 15h:08m:46s remains)
INFO - root - 2017-12-16 08:52:00.917827: step 22090, loss = 0.55, batch loss = 0.29 (46.8 examples/sec; 0.171 sec/batch; 14h:44m:09s remains)
INFO - root - 2017-12-16 08:52:02.589979: step 22100, loss = 0.71, batch loss = 0.46 (47.5 examples/sec; 0.168 sec/batch; 14h:30m:44s remains)
2017-12-16 08:52:03.067843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.68881476 -0.80371511 -0.88626003 -0.94071424 -0.95696056 -0.92464995 -0.86569 -0.79730797 -0.72183287 -0.64260352 -0.56995463 -0.50170052 -0.43282104 -0.37205458 -0.32714176][-1.2043611 -1.3451158 -1.4299214 -1.4709339 -1.4423999 -1.3535804 -1.2422496 -1.1385181 -1.0532608 -0.99143982 -0.95699918 -0.91660237 -0.85842013 -0.79185092 -0.72454906][-1.8136799 -1.9437605 -2.0337775 -2.0247805 -1.9134806 -1.7572103 -1.5962784 -1.4623125 -1.4246762 -1.4863706 -1.6098363 -1.7100303 -1.750087 -1.7306371 -1.6459732][-2.33465 -2.4239867 -2.5071924 -2.4211645 -2.1896656 -1.9510386 -1.7357969 -1.564405 -1.588057 -1.8659711 -2.2681613 -2.6221213 -2.8676999 -2.9765 -2.9019506][-2.3145823 -2.345217 -2.4569669 -2.3481467 -2.0243618 -1.7183645 -1.4301685 -1.1450675 -1.125807 -1.547668 -2.2008471 -2.832375 -3.3817129 -3.7163453 -3.7266362][-1.7465706 -1.7194972 -1.8754538 -1.7621994 -1.3740942 -1.043335 -0.67169464 -0.15413213 0.017862558 -0.42575634 -1.1924886 -2.0359764 -2.9065158 -3.5496092 -3.7703013][-0.96114731 -0.93266606 -1.1479743 -1.0810663 -0.72538829 -0.45352244 0.025084257 0.84152913 1.2347276 0.90995312 0.25869131 -0.60007012 -1.6863754 -2.6231046 -3.1040788][-0.61560833 -0.63685358 -0.92052186 -0.94730425 -0.74949765 -0.6296649 -0.080173731 0.91581821 1.4974339 1.4432471 1.1857367 0.56796169 -0.52897096 -1.6133704 -2.2836196][-0.88916159 -0.9672879 -1.2156099 -1.2556887 -1.2301886 -1.2970892 -0.86618853 0.0037741661 0.52676582 0.66834641 0.80325437 0.53573751 -0.36904502 -1.3080425 -1.934551][-1.1924882 -1.2300804 -1.3461825 -1.3297824 -1.4516761 -1.7794228 -1.6533434 -1.2036176 -1.0311786 -0.9371295 -0.61321688 -0.558023 -1.0797349 -1.6458874 -2.0620756][-1.149564 -1.1082686 -1.1002891 -1.0162201 -1.230181 -1.7674234 -2.0050468 -2.0404298 -2.3617516 -2.5218964 -2.243979 -1.9862477 -2.0961902 -2.2476902 -2.368223][-0.81555474 -0.63340235 -0.51583409 -0.34459066 -0.55779624 -1.1669328 -1.7174441 -2.2863088 -3.1207385 -3.597456 -3.5086079 -3.253757 -3.1066294 -2.960602 -2.8138075][-0.30946279 -0.013345003 0.18196607 0.43762279 0.28249478 -0.34678817 -1.1383703 -2.1362257 -3.3443453 -4.1278577 -4.2636867 -4.1142659 -3.9095466 -3.6301093 -3.3454132][0.23655009 0.58771896 0.8168149 1.1294563 1.0695939 0.54472327 -0.28031063 -1.5233791 -2.999517 -4.067904 -4.445405 -4.4303651 -4.2508483 -3.9439139 -3.6451883][0.71741533 1.0553131 1.2890127 1.6583455 1.6503384 1.2326877 0.51074123 -0.77552342 -2.3904696 -3.6302056 -4.1473956 -4.1955576 -4.0439286 -3.7586808 -3.5337572]]...]
INFO - root - 2017-12-16 08:52:04.774512: step 22110, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 14h:43m:30s remains)
INFO - root - 2017-12-16 08:52:06.431470: step 22120, loss = 0.57, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:12m:13s remains)
INFO - root - 2017-12-16 08:52:08.110982: step 22130, loss = 0.61, batch loss = 0.35 (47.0 examples/sec; 0.170 sec/batch; 14h:39m:51s remains)
INFO - root - 2017-12-16 08:52:09.773240: step 22140, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:12m:38s remains)
INFO - root - 2017-12-16 08:52:11.432230: step 22150, loss = 0.63, batch loss = 0.37 (48.6 examples/sec; 0.165 sec/batch; 14h:12m:10s remains)
INFO - root - 2017-12-16 08:52:13.069724: step 22160, loss = 0.51, batch loss = 0.25 (47.3 examples/sec; 0.169 sec/batch; 14h:35m:11s remains)
INFO - root - 2017-12-16 08:52:14.754498: step 22170, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 14h:33m:48s remains)
INFO - root - 2017-12-16 08:52:16.450406: step 22180, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 14h:10m:18s remains)
INFO - root - 2017-12-16 08:52:18.141231: step 22190, loss = 0.46, batch loss = 0.20 (47.8 examples/sec; 0.167 sec/batch; 14h:25m:02s remains)
INFO - root - 2017-12-16 08:52:19.797959: step 22200, loss = 0.60, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 14h:20m:54s remains)
2017-12-16 08:52:20.332598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7800524 -2.9769833 -2.9585061 -2.6208968 -2.0472426 -1.5069816 -1.2122065 -0.9269079 -0.70250559 -0.8070817 -1.5366268 -2.4715452 -3.0228345 -3.2310646 -3.1546855][-3.1078587 -3.2667367 -3.1325464 -2.612721 -1.897688 -1.3038908 -0.96923757 -0.65794086 -0.43777812 -0.58016646 -1.3674872 -2.4285722 -3.0814898 -3.2920704 -3.1731157][-3.576138 -3.691113 -3.4292231 -2.7494705 -1.8392611 -1.0850241 -0.6892817 -0.33656931 -0.20414662 -0.55499446 -1.4289628 -2.5924962 -3.317868 -3.4738445 -3.2750547][-3.8307416 -3.9094069 -3.5896392 -2.8194187 -1.7200804 -0.79662013 -0.29886603 0.051953316 -0.041839838 -0.63298738 -1.603178 -2.7874448 -3.5202756 -3.6543207 -3.3892794][-3.6893783 -3.7409344 -3.445827 -2.6343291 -1.4180181 -0.29110551 0.43139243 0.75662661 0.37043929 -0.54310775 -1.6788099 -2.8585725 -3.6020577 -3.7317183 -3.4416108][-3.2252483 -3.3058624 -3.0901639 -2.2862015 -0.93469226 0.48424244 1.5107167 1.8794892 1.1792066 -0.11703491 -1.4799485 -2.6930492 -3.4507654 -3.6213698 -3.3908451][-2.6150091 -2.7315011 -2.5746298 -1.8414614 -0.50981891 1.059696 2.3135531 2.7888949 1.9222033 0.39218855 -1.1450315 -2.396733 -3.1087227 -3.2826703 -3.1191339][-1.9909731 -2.0849428 -1.9855021 -1.4139639 -0.3336997 1.0133369 2.2354729 2.837218 2.0932915 0.631387 -0.89303637 -2.053894 -2.6645772 -2.7959239 -2.6482136][-1.3952059 -1.4770561 -1.4491965 -1.0627372 -0.29605818 0.56806278 1.4583066 2.0418236 1.6011498 0.48338437 -0.74649072 -1.6587569 -2.1360402 -2.2092063 -2.0225132][-0.99569356 -1.0263246 -1.0140058 -0.79676247 -0.36245775 0.041019917 0.51061964 0.93393636 0.78072119 0.056609631 -0.79100955 -1.408015 -1.7525871 -1.7913036 -1.5949147][-1.1248872 -1.1049073 -1.0077004 -0.82407367 -0.61250067 -0.48616648 -0.30216646 -0.080041409 -0.10660696 -0.53765523 -1.0658686 -1.4636838 -1.6899046 -1.6884379 -1.4989591][-1.5370586 -1.5070276 -1.3295362 -1.1199341 -0.98659313 -0.99865985 -1.0169929 -0.98118603 -1.0344131 -1.2782304 -1.5654685 -1.777488 -1.9031677 -1.8622922 -1.6940873][-2.0523324 -2.0607417 -1.8995352 -1.6814457 -1.5679348 -1.6345143 -1.79896 -1.9202015 -2.0124404 -2.1396573 -2.2346539 -2.3006017 -2.3364077 -2.2494757 -2.0987027][-2.627744 -2.720423 -2.6463523 -2.4659648 -2.31165 -2.3423355 -2.5344343 -2.69668 -2.7626092 -2.7931051 -2.7620978 -2.7309268 -2.7029819 -2.5962009 -2.4762123][-3.0336039 -3.186187 -3.2069793 -3.0894308 -2.9024057 -2.8206565 -2.8929088 -2.9938874 -3.0140104 -2.9840572 -2.9047596 -2.8469253 -2.8075304 -2.7254777 -2.6445518]]...]
INFO - root - 2017-12-16 08:52:21.982091: step 22210, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:10m:51s remains)
INFO - root - 2017-12-16 08:52:23.664201: step 22220, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.170 sec/batch; 14h:37m:23s remains)
INFO - root - 2017-12-16 08:52:25.333857: step 22230, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.163 sec/batch; 14h:01m:25s remains)
INFO - root - 2017-12-16 08:52:27.025047: step 22240, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:18m:26s remains)
INFO - root - 2017-12-16 08:52:28.719896: step 22250, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:21m:52s remains)
INFO - root - 2017-12-16 08:52:30.406084: step 22260, loss = 0.55, batch loss = 0.29 (46.4 examples/sec; 0.172 sec/batch; 14h:51m:49s remains)
INFO - root - 2017-12-16 08:52:32.093600: step 22270, loss = 0.58, batch loss = 0.32 (48.5 examples/sec; 0.165 sec/batch; 14h:12m:01s remains)
INFO - root - 2017-12-16 08:52:33.772481: step 22280, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:09m:22s remains)
INFO - root - 2017-12-16 08:52:35.474514: step 22290, loss = 0.44, batch loss = 0.18 (46.3 examples/sec; 0.173 sec/batch; 14h:53m:12s remains)
INFO - root - 2017-12-16 08:52:37.165294: step 22300, loss = 0.70, batch loss = 0.44 (48.2 examples/sec; 0.166 sec/batch; 14h:17m:29s remains)
2017-12-16 08:52:37.624639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9130286 -1.7240889 -1.6760495 -1.6014166 -1.5267302 -1.5695436 -1.4293604 -1.3048549 -1.6372297 -1.9221333 -1.8915983 -1.9842814 -2.1040885 -1.9267358 -1.504957][-2.2822156 -2.3014255 -2.3091915 -2.1990209 -2.126406 -2.1558874 -1.8943139 -1.5741196 -1.7504013 -1.9743183 -1.9665886 -2.074326 -2.2002494 -2.0004017 -1.5475483][-2.3366542 -2.6453543 -2.7876778 -2.7203653 -2.6317515 -2.5150964 -2.0224361 -1.3962343 -1.3005093 -1.4397588 -1.5634733 -1.8901248 -2.1833758 -2.1009254 -1.6832681][-2.0727558 -2.6884258 -2.9619768 -2.8898208 -2.7094893 -2.4253242 -1.7440671 -0.87036657 -0.50585115 -0.52791584 -0.79517305 -1.4133048 -2.0082891 -2.169776 -1.9002783][-1.4515941 -2.1347988 -2.3893116 -2.2489152 -1.9341321 -1.4716196 -0.73916149 0.13565803 0.56264186 0.57803845 0.16260695 -0.76222062 -1.6829149 -2.1449964 -2.1316371][-0.67084539 -1.2044845 -1.2368335 -0.84562469 -0.23834777 0.46600437 1.2167706 1.8068476 1.8420806 1.5057225 0.7986753 -0.40252841 -1.6029406 -2.2746651 -2.4087403][0.18912482 -0.10346603 0.17062593 0.78508854 1.7138767 2.8906016 3.756207 3.7871213 3.0009923 1.836658 0.594908 -0.74316657 -1.9150342 -2.5203574 -2.6088405][0.70894814 0.75800085 1.2413223 1.9592609 3.0059056 4.6504931 5.8046651 4.8782687 3.1035724 1.3734126 -0.065460205 -1.3393571 -2.3278923 -2.7228241 -2.645061][0.61766505 1.0209584 1.5878439 2.0965948 2.7613573 3.6995754 4.2052264 3.4233084 1.7891908 0.26982856 -0.86193621 -1.8707558 -2.5803509 -2.685972 -2.4322941][-0.023153543 0.53424835 1.0846133 1.2106254 1.2768271 1.5068634 1.5407031 0.93311906 -0.20700836 -1.1588898 -1.7778114 -2.3394647 -2.6981235 -2.5518982 -2.126173][-1.2089789 -0.78586006 -0.4285177 -0.50265265 -0.78021586 -0.9804647 -1.1631068 -1.5810102 -2.2112625 -2.5968435 -2.6624391 -2.7351885 -2.7377381 -2.4147682 -1.9385235][-2.350281 -2.1835971 -2.0619795 -2.291755 -2.721035 -3.1235683 -3.3513238 -3.473258 -3.6319642 -3.5431936 -3.2290006 -2.9704728 -2.7608056 -2.3855004 -1.9558833][-2.8967845 -2.9304626 -2.9707277 -3.3524547 -3.8890519 -4.2830486 -4.3511405 -4.1387067 -3.8818231 -3.5142164 -3.0787697 -2.7497287 -2.5307589 -2.2923026 -2.0482185][-2.7462788 -2.8900788 -3.098562 -3.5587163 -4.0511417 -4.2734594 -4.0795488 -3.5518212 -3.0194666 -2.6283855 -2.3726137 -2.2243092 -2.164012 -2.1601651 -2.1331165][-2.4971304 -2.6839886 -2.9328437 -3.2965615 -3.5537615 -3.5346968 -3.0644708 -2.2724819 -1.6481502 -1.4493032 -1.5308973 -1.7204655 -1.9360406 -2.1177766 -2.1870427]]...]
INFO - root - 2017-12-16 08:52:39.310137: step 22310, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:22m:43s remains)
INFO - root - 2017-12-16 08:52:41.012424: step 22320, loss = 0.66, batch loss = 0.41 (44.8 examples/sec; 0.179 sec/batch; 15h:23m:48s remains)
INFO - root - 2017-12-16 08:52:42.718410: step 22330, loss = 0.62, batch loss = 0.36 (46.3 examples/sec; 0.173 sec/batch; 14h:53m:31s remains)
INFO - root - 2017-12-16 08:52:44.431236: step 22340, loss = 0.62, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 14h:34m:23s remains)
INFO - root - 2017-12-16 08:52:46.166849: step 22350, loss = 0.49, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 14h:27m:07s remains)
INFO - root - 2017-12-16 08:52:47.851055: step 22360, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:33m:06s remains)
INFO - root - 2017-12-16 08:52:49.567153: step 22370, loss = 0.60, batch loss = 0.35 (47.5 examples/sec; 0.168 sec/batch; 14h:30m:36s remains)
INFO - root - 2017-12-16 08:52:51.267848: step 22380, loss = 0.47, batch loss = 0.21 (46.1 examples/sec; 0.173 sec/batch; 14h:56m:06s remains)
INFO - root - 2017-12-16 08:52:52.980384: step 22390, loss = 0.52, batch loss = 0.26 (44.9 examples/sec; 0.178 sec/batch; 15h:20m:21s remains)
INFO - root - 2017-12-16 08:52:54.694075: step 22400, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 14h:18m:49s remains)
2017-12-16 08:52:55.159153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8947723 -4.0207825 -4.2121511 -4.4119544 -4.4250784 -4.1787419 -3.7867932 -3.4375288 -3.2138426 -3.084543 -3.050704 -3.0330818 -3.0835958 -3.2772152 -3.5008969][-4.6630745 -4.6706705 -4.7456436 -4.8242559 -4.7064042 -4.3535347 -3.9199057 -3.5048208 -3.2494555 -3.1190805 -3.0735011 -2.9670715 -2.8106875 -2.7838335 -2.791007][-4.6922855 -4.6465759 -4.5786114 -4.4819427 -4.2315464 -3.8433371 -3.4608016 -3.065074 -2.8777621 -2.8534608 -2.8655186 -2.7443259 -2.5232801 -2.3079619 -2.0174279][-3.94801 -3.8977125 -3.808 -3.6193585 -3.2845972 -2.8755434 -2.5988781 -2.3037336 -2.2819092 -2.4031625 -2.5344379 -2.5260279 -2.3928962 -2.1323247 -1.6452222][-2.7209649 -2.6787512 -2.571991 -2.2714338 -1.8438236 -1.4691406 -1.3165028 -1.133464 -1.2000997 -1.4394166 -1.7140033 -1.8728094 -1.9637859 -1.8822342 -1.4779055][-1.4560871 -1.4464431 -1.3126893 -0.82740271 -0.23664165 0.16436028 0.30589485 0.463665 0.40902305 0.17000341 -0.15340185 -0.50785077 -0.86602628 -1.0816361 -0.96999335][-0.67794931 -0.71361458 -0.59567368 0.026527166 0.83271289 1.4267457 1.7855213 2.067704 2.1492803 1.9740841 1.6873667 1.2848115 0.64289188 0.0005428791 -0.30231047][-0.4049418 -0.41498184 -0.2651093 0.32519865 1.2030137 1.9679453 2.5531309 3.0374782 3.3256114 3.2616746 2.9299095 2.4581435 1.6897342 0.7522831 0.10745192][-0.30153322 -0.39268053 -0.27718902 0.17510009 0.84267187 1.5010333 2.1211717 2.6343057 2.9272511 2.8727524 2.5305116 2.0683048 1.2808912 0.36587691 -0.33502078][-0.29615021 -0.48297644 -0.53513563 -0.40730619 -0.14042592 0.20025969 0.61109805 0.94508052 1.1023433 1.0267591 0.78498197 0.41454029 -0.21181417 -0.88623428 -1.3735416][-0.72335172 -1.0882462 -1.3153787 -1.4832443 -1.5448024 -1.4986265 -1.2970107 -1.1309254 -1.0844921 -1.1512103 -1.2832191 -1.5161853 -1.8782387 -2.2383931 -2.4283991][-1.8130915 -2.2367823 -2.527307 -2.7743948 -2.9375248 -2.9867983 -2.9121444 -2.8376377 -2.8259184 -2.8521557 -2.885052 -2.9524808 -3.0800257 -3.1347713 -3.0900104][-2.2792161 -2.6707087 -2.9246948 -3.1297331 -3.2755406 -3.3386524 -3.3269553 -3.2945445 -3.2930887 -3.2920361 -3.2509906 -3.196981 -3.1263049 -3.0015454 -2.8214152][-1.8541768 -2.1386237 -2.3238885 -2.4649973 -2.534667 -2.5609567 -2.5721231 -2.5737705 -2.5807238 -2.5686386 -2.5036161 -2.4022086 -2.2639782 -2.0955696 -1.8931997][-1.119838 -1.2741871 -1.3568845 -1.4096824 -1.4230607 -1.4196918 -1.4478502 -1.4925572 -1.5201743 -1.5009079 -1.408911 -1.2777048 -1.1411488 -1.0068276 -0.86828387]]...]
INFO - root - 2017-12-16 08:52:56.824573: step 22410, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:10m:59s remains)
INFO - root - 2017-12-16 08:52:58.514413: step 22420, loss = 0.57, batch loss = 0.31 (46.5 examples/sec; 0.172 sec/batch; 14h:49m:41s remains)
INFO - root - 2017-12-16 08:53:00.198452: step 22430, loss = 0.54, batch loss = 0.28 (46.9 examples/sec; 0.171 sec/batch; 14h:41m:34s remains)
INFO - root - 2017-12-16 08:53:01.909625: step 22440, loss = 0.56, batch loss = 0.30 (46.8 examples/sec; 0.171 sec/batch; 14h:42m:56s remains)
INFO - root - 2017-12-16 08:53:03.613383: step 22450, loss = 0.53, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 14h:36m:40s remains)
INFO - root - 2017-12-16 08:53:05.310220: step 22460, loss = 0.64, batch loss = 0.38 (46.1 examples/sec; 0.174 sec/batch; 14h:57m:15s remains)
INFO - root - 2017-12-16 08:53:06.999568: step 22470, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 14h:30m:27s remains)
INFO - root - 2017-12-16 08:53:08.708987: step 22480, loss = 0.46, batch loss = 0.21 (46.1 examples/sec; 0.173 sec/batch; 14h:55m:58s remains)
INFO - root - 2017-12-16 08:53:10.389221: step 22490, loss = 0.46, batch loss = 0.20 (46.8 examples/sec; 0.171 sec/batch; 14h:43m:13s remains)
INFO - root - 2017-12-16 08:53:12.081119: step 22500, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 14h:27m:52s remains)
2017-12-16 08:53:12.544772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2352335 -1.2625575 -1.3004042 -1.3321984 -1.3665271 -1.4123707 -1.4669421 -1.5299789 -1.591343 -1.6195198 -1.5679641 -1.414299 -1.2026381 -1.013785 -0.90140879][-1.3559287 -1.4290127 -1.494278 -1.50108 -1.4642997 -1.4269009 -1.4341432 -1.530452 -1.6705899 -1.7743758 -1.7651238 -1.5909123 -1.304258 -1.0052468 -0.77359259][-1.3879509 -1.518312 -1.5918194 -1.5025964 -1.3039438 -1.1100538 -1.0633448 -1.2589396 -1.6003268 -1.8860863 -1.9950185 -1.8632249 -1.5459597 -1.153445 -0.797806][-1.3205827 -1.5270615 -1.626642 -1.4503521 -1.0402761 -0.61214077 -0.42713702 -0.66001892 -1.2139709 -1.7782722 -2.1143153 -2.1260173 -1.8512547 -1.4137512 -0.97060561][-1.2584856 -1.5634995 -1.7503082 -1.571236 -0.95215273 -0.099709749 0.56039023 0.61389422 -0.046698332 -1.0360131 -1.8079002 -2.1531713 -2.063699 -1.697053 -1.2448516][-1.3216796 -1.7284009 -2.0544734 -1.9314198 -1.0706754 0.42183471 1.9415083 2.6721449 1.9879313 0.42047548 -1.0615938 -1.9271699 -2.1456778 -1.9486009 -1.5579402][-1.5328254 -1.9914794 -2.450069 -2.4093144 -1.3882785 0.63576961 2.994226 4.5059614 3.8486037 1.7737708 -0.28224397 -1.5493121 -2.0512516 -2.0534663 -1.7690974][-1.7564918 -2.2024713 -2.7213304 -2.791018 -1.8586692 0.19299579 2.7813606 4.6799369 4.317265 2.2946496 0.21321225 -1.143326 -1.8177104 -1.9744463 -1.7932453][-1.8458833 -2.2490311 -2.7614062 -2.9655082 -2.3997738 -0.89386547 1.1992288 2.9265251 3.1125112 1.7835474 0.2252028 -0.94659972 -1.667294 -1.9198142 -1.8016932][-1.7838904 -2.1472733 -2.6236832 -2.9812608 -2.9090323 -2.17135 -0.82336044 0.5663991 1.1389132 0.6119616 -0.30584693 -1.1492211 -1.7765867 -1.9986862 -1.8813663][-1.6055591 -1.9130507 -2.3434141 -2.8050911 -3.121248 -3.0246649 -2.362529 -1.3940485 -0.713532 -0.73024547 -1.1397656 -1.6242684 -2.006048 -2.091258 -1.9325163][-1.3703519 -1.6080236 -1.9747763 -2.4757776 -2.9755919 -3.2378044 -3.055995 -2.5198023 -2.0089107 -1.8462481 -1.9279512 -2.1080666 -2.2240427 -2.173641 -1.9791641][-1.1829975 -1.3493168 -1.6272223 -2.0675514 -2.5772965 -2.9649382 -3.059175 -2.8723457 -2.6114552 -2.4692252 -2.4088686 -2.3992789 -2.3431787 -2.2007835 -1.9740551][-1.0700433 -1.1725458 -1.3599554 -1.6765203 -2.077745 -2.4501867 -2.6592453 -2.6854784 -2.61685 -2.5455439 -2.463 -2.3626771 -2.2123492 -1.9964246 -1.7390528][-1.0870756 -1.15227 -1.275838 -1.4689591 -1.7139587 -1.9688014 -2.15519 -2.2460847 -2.2602742 -2.2277088 -2.1407037 -1.9971173 -1.7945542 -1.534106 -1.2686431]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-22500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 08:53:14.732934: step 22510, loss = 0.54, batch loss = 0.28 (44.4 examples/sec; 0.180 sec/batch; 15h:29m:52s remains)
INFO - root - 2017-12-16 08:53:16.431608: step 22520, loss = 0.52, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:27m:10s remains)
INFO - root - 2017-12-16 08:53:18.117365: step 22530, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 14h:37m:58s remains)
INFO - root - 2017-12-16 08:53:19.853137: step 22540, loss = 0.62, batch loss = 0.37 (44.7 examples/sec; 0.179 sec/batch; 15h:24m:52s remains)
INFO - root - 2017-12-16 08:53:21.545304: step 22550, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:23m:25s remains)
INFO - root - 2017-12-16 08:53:23.272643: step 22560, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:26m:08s remains)
INFO - root - 2017-12-16 08:53:24.946348: step 22570, loss = 0.49, batch loss = 0.23 (48.6 examples/sec; 0.165 sec/batch; 14h:10m:12s remains)
INFO - root - 2017-12-16 08:53:26.611087: step 22580, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:20m:43s remains)
INFO - root - 2017-12-16 08:53:28.268982: step 22590, loss = 0.61, batch loss = 0.35 (47.5 examples/sec; 0.169 sec/batch; 14h:30m:26s remains)
INFO - root - 2017-12-16 08:53:29.927773: step 22600, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 14h:10m:09s remains)
2017-12-16 08:53:30.404617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9873351 -2.3607934 -2.3927925 -1.9840695 -1.4584751 -1.1721636 -1.10685 -1.1807065 -1.3342144 -1.3367969 -1.131408 -0.94840646 -1.0051602 -1.2861508 -1.5178916][-2.0377536 -2.3536394 -2.3489897 -1.9623936 -1.5637717 -1.4312003 -1.4618356 -1.498893 -1.4465482 -1.224167 -0.88993692 -0.720104 -0.83596992 -1.136178 -1.3412604][-1.8186789 -1.9405538 -1.8585508 -1.5408959 -1.354211 -1.4265388 -1.6448643 -1.8198129 -1.7511414 -1.4081964 -1.0099748 -0.84427857 -0.94195724 -1.1742132 -1.2855698][-1.4503318 -1.3953612 -1.2536232 -1.016119 -0.93812025 -1.1216526 -1.5464678 -1.9494326 -2.0254998 -1.7997384 -1.5311694 -1.4624166 -1.5173085 -1.585906 -1.5108829][-1.3969736 -1.3338137 -1.1844722 -0.93982327 -0.75016308 -0.88333595 -1.4014789 -1.9820749 -2.2400408 -2.2336907 -2.2222767 -2.2989256 -2.3067586 -2.20028 -1.9468335][-1.7845244 -1.8146193 -1.6453279 -1.3135059 -0.93915641 -0.85042286 -1.2325183 -1.8689964 -2.2957833 -2.5013206 -2.6981487 -2.851579 -2.8102014 -2.6263638 -2.3507571][-2.2494073 -2.363837 -2.1426604 -1.6600399 -1.0306313 -0.57495296 -0.68361306 -1.3461848 -1.9710314 -2.3737156 -2.6885839 -2.8507004 -2.7727449 -2.6265178 -2.4457698][-2.3595486 -2.4805217 -2.2156997 -1.5604155 -0.62188697 0.17463017 0.31119704 -0.40662634 -1.2014773 -1.7650466 -2.1652327 -2.2898245 -2.1133108 -1.9817951 -1.9679587][-2.0391135 -2.1709771 -1.8961358 -1.1301206 0.012415409 1.1042287 1.4129057 0.63155913 -0.28645658 -0.95395815 -1.4033142 -1.4360114 -1.0806696 -0.92221165 -1.0734185][-1.6773643 -1.9190761 -1.7604063 -1.0885534 0.077204466 1.42312 1.9771116 1.3180549 0.46006727 -0.20726871 -0.68524516 -0.65202808 -0.21874332 -0.091457605 -0.28583336][-1.4888701 -1.9604139 -2.1560884 -1.8352375 -0.90902781 0.3614285 1.1783929 1.0825558 0.69338822 0.20962167 -0.25826931 -0.33275151 -0.02583456 0.076408863 0.015397072][-1.4312336 -2.1402481 -2.7016535 -2.7983902 -2.2608917 -1.279971 -0.38192022 0.0075750351 0.10736012 -0.087689877 -0.37307978 -0.48543775 -0.27963948 -0.14269233 -0.07312417][-1.3333356 -2.2019348 -2.9693732 -3.3283682 -3.0889726 -2.3987229 -1.624752 -1.1418399 -0.9208988 -0.91591358 -0.96607304 -0.91128993 -0.60608518 -0.42556667 -0.33640051][-1.0099562 -1.9159329 -2.7012539 -3.1113038 -3.006052 -2.5483317 -2.0761349 -1.8403963 -1.8078768 -1.7679416 -1.5826982 -1.295531 -0.88357866 -0.74372125 -0.74043226][-0.62624753 -1.4038447 -1.9857225 -2.2165573 -2.1288776 -1.8594375 -1.6486359 -1.687378 -1.9133996 -1.985376 -1.7037635 -1.2875509 -0.91791093 -0.88753319 -1.0853964]]...]
INFO - root - 2017-12-16 08:53:32.059861: step 22610, loss = 0.53, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 14h:30m:48s remains)
INFO - root - 2017-12-16 08:53:33.763224: step 22620, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 14h:41m:00s remains)
INFO - root - 2017-12-16 08:53:35.433435: step 22630, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:32m:27s remains)
INFO - root - 2017-12-16 08:53:37.122886: step 22640, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:32m:27s remains)
INFO - root - 2017-12-16 08:53:38.793299: step 22650, loss = 0.58, batch loss = 0.32 (46.7 examples/sec; 0.171 sec/batch; 14h:44m:32s remains)
INFO - root - 2017-12-16 08:53:40.467875: step 22660, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 14h:10m:44s remains)
INFO - root - 2017-12-16 08:53:42.170777: step 22670, loss = 0.59, batch loss = 0.33 (46.7 examples/sec; 0.171 sec/batch; 14h:43m:57s remains)
INFO - root - 2017-12-16 08:53:43.870059: step 22680, loss = 0.71, batch loss = 0.45 (48.0 examples/sec; 0.167 sec/batch; 14h:20m:38s remains)
INFO - root - 2017-12-16 08:53:45.524586: step 22690, loss = 0.47, batch loss = 0.21 (49.1 examples/sec; 0.163 sec/batch; 14h:01m:41s remains)
INFO - root - 2017-12-16 08:53:47.204116: step 22700, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 14h:28m:25s remains)
2017-12-16 08:53:47.694845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6488857 -2.6484344 -3.1129613 -3.7235694 -3.6269426 -2.8829861 -2.2129974 -1.9386679 -2.1084478 -2.6108227 -3.0950222 -3.0262625 -2.5881357 -2.5174341 -2.7462289][-2.5520093 -2.51317 -2.9782259 -3.5748453 -3.495173 -2.766022 -2.0622017 -1.7549298 -1.9750352 -2.6074929 -3.1963158 -3.1078248 -2.5190806 -2.2862859 -2.4026561][-2.5393395 -2.4792945 -2.9246178 -3.4462225 -3.3618217 -2.6800849 -2.0017257 -1.6981213 -1.9202818 -2.5112917 -3.0268033 -2.816977 -2.0458944 -1.6708519 -1.713855][-2.5724916 -2.4701452 -2.8699579 -3.2621081 -3.1039822 -2.4545631 -1.846499 -1.5788028 -1.7678745 -2.2592869 -2.6143322 -2.2164102 -1.2546065 -0.73861682 -0.77802479][-2.4431934 -2.2743895 -2.5637281 -2.7355459 -2.3706634 -1.6662377 -1.1070468 -0.91780162 -1.1540626 -1.6741524 -1.9473646 -1.3592427 -0.24314904 0.3112123 0.23403955][-2.0377197 -1.9176219 -2.0568395 -1.9030776 -1.2435185 -0.37699759 0.20193338 0.27835298 -0.15556049 -0.85232353 -1.209922 -0.6134522 0.68546581 1.3117564 1.1750741][-1.3715717 -1.4040575 -1.473711 -1.0842501 -0.18415928 0.82108355 1.3882515 1.2383265 0.54412556 -0.3521874 -0.8649776 -0.39132214 0.83230662 1.5190177 1.4922118][-0.70646608 -0.8514508 -0.88946438 -0.38832843 0.62542892 1.6120059 1.9958127 1.5424743 0.5790689 -0.49261296 -1.0963665 -0.77070677 0.16728497 0.757607 0.86872125][-0.2995317 -0.47955191 -0.53753793 -0.026337624 0.90768647 1.7156713 1.8048275 1.1413622 0.064209223 -1.0418512 -1.5599868 -1.2142074 -0.48438895 -0.027133703 0.19045544][-0.34250784 -0.52826726 -0.63612771 -0.27943969 0.4088676 0.92590451 0.81514025 0.16078758 -0.79488492 -1.6612388 -1.9521861 -1.5930624 -0.9725343 -0.68814182 -0.58144855][-0.60050082 -0.79236627 -0.96314275 -0.83582437 -0.50337315 -0.24236035 -0.34973645 -0.83072591 -1.4991696 -2.0318379 -2.0642467 -1.6772593 -1.2297753 -1.2110492 -1.4311762][-0.85217392 -1.0806897 -1.3633883 -1.527601 -1.5536631 -1.4780014 -1.4500365 -1.6162592 -1.9007766 -2.0780368 -1.8819529 -1.4573703 -1.1565098 -1.4658594 -2.0849373][-1.1728772 -1.4853843 -1.8758495 -2.2300417 -2.4606383 -2.4349053 -2.2038305 -1.9900318 -1.9461354 -1.9160204 -1.6127012 -1.1957 -1.0641532 -1.6865444 -2.6227517][-1.8289618 -2.2377975 -2.6576638 -2.995693 -3.181838 -3.0578387 -2.616693 -2.0754235 -1.8081214 -1.7666824 -1.6213944 -1.3720739 -1.4559672 -2.2235668 -3.2379231][-2.7687247 -3.1797884 -3.4983683 -3.65341 -3.6582146 -3.3980689 -2.7769494 -2.017545 -1.664006 -1.6897988 -1.757097 -1.7474713 -2.0209842 -2.7929196 -3.6456561]]...]
INFO - root - 2017-12-16 08:53:49.396334: step 22710, loss = 0.63, batch loss = 0.37 (46.6 examples/sec; 0.172 sec/batch; 14h:46m:55s remains)
INFO - root - 2017-12-16 08:53:51.059083: step 22720, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.164 sec/batch; 14h:09m:16s remains)
INFO - root - 2017-12-16 08:53:52.724232: step 22730, loss = 0.62, batch loss = 0.36 (47.1 examples/sec; 0.170 sec/batch; 14h:37m:08s remains)
INFO - root - 2017-12-16 08:53:54.403514: step 22740, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:22m:02s remains)
INFO - root - 2017-12-16 08:53:56.072612: step 22750, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 14h:25m:47s remains)
INFO - root - 2017-12-16 08:53:57.801038: step 22760, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:33m:25s remains)
INFO - root - 2017-12-16 08:53:59.464626: step 22770, loss = 0.50, batch loss = 0.24 (49.5 examples/sec; 0.162 sec/batch; 13h:54m:54s remains)
INFO - root - 2017-12-16 08:54:01.122743: step 22780, loss = 0.63, batch loss = 0.37 (47.8 examples/sec; 0.167 sec/batch; 14h:24m:01s remains)
INFO - root - 2017-12-16 08:54:02.793533: step 22790, loss = 0.68, batch loss = 0.42 (46.7 examples/sec; 0.171 sec/batch; 14h:43m:35s remains)
INFO - root - 2017-12-16 08:54:04.456386: step 22800, loss = 0.60, batch loss = 0.34 (48.2 examples/sec; 0.166 sec/batch; 14h:16m:06s remains)
2017-12-16 08:54:04.968717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4854109 -1.2477101 -1.0639018 -1.0699704 -1.1463078 -1.1933926 -1.0798807 -0.79360509 -0.66971004 -0.89377439 -1.1893656 -1.2374814 -1.0964479 -0.97499323 -0.89673126][-1.1230233 -0.82150614 -0.49892664 -0.40291262 -0.34755659 -0.27028775 -0.10234547 0.11326599 -0.019868612 -0.53564632 -1.0183361 -1.1950883 -1.0994338 -0.99492705 -0.90059626][-0.42926955 -0.26911783 -0.11437058 -0.20386529 -0.25726104 -0.25118613 -0.166332 -0.020151615 -0.23042965 -0.77089965 -1.2729591 -1.3942546 -1.2756857 -1.1114354 -0.92930543][0.056341887 0.067021608 -0.012775421 -0.36556029 -0.68837094 -0.88949203 -0.9581207 -0.86685026 -0.97459567 -1.3619468 -1.6811166 -1.7436924 -1.6293151 -1.3865221 -1.0958939][0.44572139 0.38282967 0.20419836 -0.28364229 -0.78704882 -1.1757179 -1.3383671 -1.3272179 -1.3770802 -1.5535256 -1.746053 -1.812541 -1.7720374 -1.5289849 -1.1831303][0.44038486 0.42417598 0.33921933 0.0074284077 -0.32248878 -0.64851964 -0.81103623 -0.81512487 -0.8009311 -0.81521821 -0.93846989 -1.1326001 -1.2879866 -1.2627252 -1.1209338][0.061057091 0.096382141 0.19724083 0.15058851 0.10332727 -0.016775846 0.043153524 0.2903235 0.35445809 0.27787709 0.091334581 -0.22101855 -0.592569 -0.90528226 -1.0511249][-0.82055545 -0.797369 -0.57317734 -0.33474159 -0.0702641 0.20524645 0.618104 1.0816545 1.1098225 0.82108188 0.48588681 0.082464218 -0.45822489 -0.99588323 -1.3979721][-1.8094602 -1.82724 -1.6743209 -1.4440254 -1.060196 -0.53620374 0.13330269 0.63805437 0.58141851 0.16765261 -0.24839544 -0.64764321 -1.2040898 -1.7545564 -2.1893475][-2.8331203 -2.8953037 -2.9114962 -2.7999034 -2.4782844 -1.9309351 -1.3134341 -0.91670775 -1.0313195 -1.4660597 -1.8291798 -2.1252537 -2.508528 -2.9387927 -3.1925979][-3.5034018 -3.5876951 -3.7401032 -3.7861531 -3.6308722 -3.2890968 -2.8663931 -2.576561 -2.6707561 -2.9732094 -3.1820018 -3.3074794 -3.4859362 -3.6650386 -3.6348596][-3.6622624 -3.7077966 -3.8759766 -3.970437 -3.893168 -3.7102504 -3.4295835 -3.1910191 -3.1727822 -3.3085842 -3.3816705 -3.4293408 -3.5154333 -3.5482006 -3.3649867][-3.17836 -3.1732676 -3.275948 -3.3554366 -3.3182163 -3.2025957 -3.0275526 -2.8595877 -2.7708097 -2.7825248 -2.8090587 -2.8507748 -2.8981917 -2.8874528 -2.6795151][-2.4977131 -2.4668019 -2.519618 -2.570473 -2.5617628 -2.5129383 -2.4322853 -2.3313081 -2.249522 -2.2272632 -2.2404683 -2.2453983 -2.2180233 -2.1356456 -1.9628103][-2.1338987 -2.1395192 -2.1897981 -2.2174067 -2.2044 -2.165967 -2.0969567 -2.0253942 -1.9366666 -1.8742247 -1.8532518 -1.8269496 -1.7555367 -1.6864902 -1.6052494]]...]
INFO - root - 2017-12-16 08:54:06.658533: step 22810, loss = 0.52, batch loss = 0.27 (47.5 examples/sec; 0.169 sec/batch; 14h:29m:57s remains)
INFO - root - 2017-12-16 08:54:08.338164: step 22820, loss = 0.52, batch loss = 0.26 (50.4 examples/sec; 0.159 sec/batch; 13h:38m:53s remains)
INFO - root - 2017-12-16 08:54:10.014235: step 22830, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 14h:17m:25s remains)
INFO - root - 2017-12-16 08:54:11.719353: step 22840, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 14h:22m:40s remains)
INFO - root - 2017-12-16 08:54:13.433013: step 22850, loss = 0.48, batch loss = 0.22 (46.2 examples/sec; 0.173 sec/batch; 14h:54m:11s remains)
INFO - root - 2017-12-16 08:54:15.154561: step 22860, loss = 0.69, batch loss = 0.43 (47.9 examples/sec; 0.167 sec/batch; 14h:21m:01s remains)
INFO - root - 2017-12-16 08:54:16.851704: step 22870, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 14h:23m:01s remains)
INFO - root - 2017-12-16 08:54:18.530519: step 22880, loss = 0.54, batch loss = 0.28 (46.9 examples/sec; 0.171 sec/batch; 14h:40m:30s remains)
INFO - root - 2017-12-16 08:54:20.197346: step 22890, loss = 0.56, batch loss = 0.30 (47.8 examples/sec; 0.167 sec/batch; 14h:23m:16s remains)
INFO - root - 2017-12-16 08:54:21.905226: step 22900, loss = 0.64, batch loss = 0.38 (46.4 examples/sec; 0.173 sec/batch; 14h:50m:30s remains)
2017-12-16 08:54:22.391136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6887023 -1.8841484 -2.2096364 -2.55584 -2.907954 -3.1670785 -3.334908 -3.460367 -3.4884696 -3.4072363 -3.2252748 -2.9645343 -2.7157717 -2.4450974 -2.1683366][-1.6252344 -1.6877065 -1.894551 -2.1786973 -2.5540843 -2.8988149 -3.1229949 -3.2649698 -3.276823 -3.1309681 -2.8572686 -2.5655272 -2.3141053 -2.0798326 -1.8906562][-1.7775171 -1.6422197 -1.6604718 -1.8037554 -2.1452377 -2.4869261 -2.702534 -2.8453848 -2.8691728 -2.7334681 -2.4588151 -2.1784382 -1.9188496 -1.6757488 -1.5120237][-2.0688963 -1.7111802 -1.4686892 -1.4168569 -1.6391745 -1.9220935 -2.1142516 -2.2441411 -2.3030214 -2.3056769 -2.2250464 -2.1058345 -1.9329765 -1.660414 -1.3999257][-2.4105914 -1.8615527 -1.3477886 -1.0413312 -1.0871599 -1.266257 -1.3881913 -1.4434822 -1.5157719 -1.7224863 -1.9980105 -2.2191949 -2.2480857 -1.9962268 -1.5851877][-2.5961576 -1.9363945 -1.2183598 -0.69547355 -0.51682496 -0.5243628 -0.50520408 -0.45771468 -0.57803392 -1.0531617 -1.74892 -2.3296964 -2.5835223 -2.3765628 -1.841054][-2.49675 -1.8316724 -1.0322205 -0.35377884 0.028331995 0.18805146 0.36100221 0.50929856 0.27965498 -0.50280261 -1.5116186 -2.3214912 -2.6762242 -2.4794052 -1.8917782][-2.1486747 -1.574872 -0.834473 -0.10817957 0.3818996 0.62136889 0.82604074 0.94627976 0.58249283 -0.35540509 -1.4634035 -2.2701125 -2.5951183 -2.3565118 -1.781033][-1.9118844 -1.5306301 -0.97460532 -0.39455605 -0.011955261 0.14084005 0.22920108 0.27551055 0.024269581 -0.63475037 -1.4579105 -2.088861 -2.3048425 -2.0397606 -1.4998485][-1.956352 -1.7341084 -1.3465208 -0.94161403 -0.73087144 -0.70680773 -0.71399426 -0.65790558 -0.70322406 -0.98551416 -1.4162371 -1.7553395 -1.8529611 -1.6294317 -1.1712583][-2.1606739 -2.0252283 -1.7126136 -1.3990721 -1.2928017 -1.3632032 -1.4222083 -1.3519377 -1.2643092 -1.2564526 -1.3536543 -1.4537532 -1.4917247 -1.393965 -1.1021739][-2.4374092 -2.3658762 -2.132293 -1.8964729 -1.8353661 -1.9275888 -1.9958084 -1.9428684 -1.8146148 -1.6554387 -1.5372463 -1.4532888 -1.4239435 -1.3997231 -1.2242489][-2.5968466 -2.6020105 -2.4709935 -2.3458714 -2.3485346 -2.4390426 -2.4816864 -2.4297621 -2.3051503 -2.1291883 -1.9466629 -1.7634706 -1.6443377 -1.5778953 -1.4111047][-2.4847193 -2.5284729 -2.4552584 -2.400142 -2.439126 -2.5136902 -2.5292478 -2.4740729 -2.3642807 -2.2255652 -2.0788066 -1.9352382 -1.8348579 -1.7491437 -1.6062307][-2.3507261 -2.3965788 -2.3402097 -2.3097596 -2.3692122 -2.4373245 -2.434243 -2.3740475 -2.285228 -2.201092 -2.1492794 -2.133601 -2.13706 -2.1129107 -2.0176632]]...]
INFO - root - 2017-12-16 08:54:24.117894: step 22910, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:08m:02s remains)
INFO - root - 2017-12-16 08:54:25.811301: step 22920, loss = 0.77, batch loss = 0.51 (48.8 examples/sec; 0.164 sec/batch; 14h:05m:43s remains)
INFO - root - 2017-12-16 08:54:27.512845: step 22930, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 14h:23m:06s remains)
INFO - root - 2017-12-16 08:54:29.197656: step 22940, loss = 0.51, batch loss = 0.25 (45.8 examples/sec; 0.175 sec/batch; 15h:01m:55s remains)
INFO - root - 2017-12-16 08:54:30.846980: step 22950, loss = 0.67, batch loss = 0.41 (49.6 examples/sec; 0.161 sec/batch; 13h:52m:33s remains)
INFO - root - 2017-12-16 08:54:32.545613: step 22960, loss = 0.59, batch loss = 0.33 (46.3 examples/sec; 0.173 sec/batch; 14h:51m:39s remains)
INFO - root - 2017-12-16 08:54:34.260793: step 22970, loss = 0.75, batch loss = 0.49 (48.5 examples/sec; 0.165 sec/batch; 14h:11m:21s remains)
INFO - root - 2017-12-16 08:54:35.942465: step 22980, loss = 0.62, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 14h:31m:13s remains)
INFO - root - 2017-12-16 08:54:37.618735: step 22990, loss = 0.61, batch loss = 0.35 (46.5 examples/sec; 0.172 sec/batch; 14h:48m:00s remains)
INFO - root - 2017-12-16 08:54:39.338875: step 23000, loss = 0.49, batch loss = 0.23 (48.3 examples/sec; 0.166 sec/batch; 14h:14m:35s remains)
2017-12-16 08:54:39.860181: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2585001 0.48456693 0.50025892 0.39564419 0.16851807 0.15461278 0.33643126 0.54761887 0.5446291 0.44033694 0.14897609 -0.22143197 -0.2939136 0.062712669 0.39363003][-0.27080822 -0.29962635 -0.47425067 -0.707237 -0.88291967 -0.86101651 -0.74194467 -0.61603367 -0.60714447 -0.65415597 -0.97185278 -1.4489572 -1.7229965 -1.5302603 -1.2306415][-0.87067091 -1.0244777 -1.2645143 -1.5288963 -1.6062841 -1.5734434 -1.6128201 -1.7243749 -1.8649043 -1.9513768 -2.2308273 -2.7123687 -3.0923257 -3.105083 -2.9425192][-1.4025819 -1.4949625 -1.5872135 -1.5690055 -1.3617526 -1.2536772 -1.4244988 -1.7164583 -1.9378947 -2.0862353 -2.5096216 -3.1131983 -3.6138272 -3.8554401 -3.8238711][-1.9427791 -1.8806942 -1.5945743 -1.1377647 -0.6413132 -0.41260195 -0.53998089 -0.83309174 -0.99350405 -1.1608489 -1.6858184 -2.3874223 -2.9797621 -3.3294861 -3.4302969][-2.7392054 -2.4402237 -1.7612702 -0.96140575 -0.28928232 0.014292955 0.0097985268 -0.15933418 -0.27003455 -0.44677746 -0.98831737 -1.6226966 -2.0397744 -2.3006425 -2.3402085][-3.4065113 -2.9253616 -2.0346475 -1.0428873 -0.28405404 0.010341883 0.031996965 -0.02298975 -0.11336589 -0.32114339 -0.78616929 -1.2225996 -1.3457998 -1.3496144 -1.2849468][-3.4649739 -2.9320431 -2.057961 -1.1268159 -0.426821 -0.12427998 -0.037505388 -0.048049212 -0.24299169 -0.54753053 -0.8891834 -0.98928046 -0.75781393 -0.5119015 -0.41725874][-3.0922027 -2.7017379 -2.0420876 -1.3499826 -0.8233273 -0.60411048 -0.54392576 -0.61046159 -0.91508031 -1.1811965 -1.2451084 -0.95065141 -0.40057564 -0.07296443 -0.024224043][-2.4519403 -2.2879939 -2.0099936 -1.7143047 -1.4954631 -1.5157504 -1.578935 -1.7016852 -1.9818319 -2.0792594 -1.8083079 -1.1933457 -0.56859243 -0.31576896 -0.32957458][-1.4676702 -1.5146632 -1.6298394 -1.7112117 -1.857795 -2.1706309 -2.3542414 -2.4787986 -2.6087337 -2.5502489 -2.1363199 -1.5450785 -1.1084572 -1.0322124 -1.1374632][-0.70354927 -0.86029375 -1.1424658 -1.4069159 -1.7565898 -2.2085147 -2.4275134 -2.4807284 -2.5023687 -2.4128389 -2.1777644 -1.8915614 -1.778632 -1.8809034 -2.0175681][-0.36225963 -0.52434969 -0.89041793 -1.2138176 -1.5637608 -1.9689019 -2.1286559 -2.0784366 -2.1123033 -2.2052951 -2.2886164 -2.3994155 -2.5442142 -2.7392247 -2.8881419][-0.26653767 -0.42478788 -0.82068837 -1.1330746 -1.4274516 -1.7540805 -1.8804753 -1.8071809 -1.8980262 -2.2058284 -2.5565572 -2.9405015 -3.228862 -3.4362679 -3.55824][-0.26397014 -0.45109582 -0.88869011 -1.1652944 -1.3987393 -1.7027352 -1.8152585 -1.7350385 -1.8023794 -2.2075124 -2.6949403 -3.1749315 -3.4880412 -3.6481433 -3.7030008]]...]
INFO - root - 2017-12-16 08:54:41.568136: step 23010, loss = 0.59, batch loss = 0.33 (49.6 examples/sec; 0.161 sec/batch; 13h:52m:30s remains)
INFO - root - 2017-12-16 08:54:43.245035: step 23020, loss = 0.62, batch loss = 0.36 (48.3 examples/sec; 0.166 sec/batch; 14h:14m:55s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:54:44.920831: step 23030, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 14h:14m:11s remains)
INFO - root - 2017-12-16 08:54:46.608968: step 23040, loss = 0.57, batch loss = 0.31 (45.7 examples/sec; 0.175 sec/batch; 15h:02m:36s remains)
INFO - root - 2017-12-16 08:54:48.302488: step 23050, loss = 0.63, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 14h:19m:20s remains)
INFO - root - 2017-12-16 08:54:49.993501: step 23060, loss = 0.48, batch loss = 0.22 (46.3 examples/sec; 0.173 sec/batch; 14h:51m:50s remains)
INFO - root - 2017-12-16 08:54:51.690963: step 23070, loss = 0.72, batch loss = 0.46 (47.1 examples/sec; 0.170 sec/batch; 14h:36m:28s remains)
INFO - root - 2017-12-16 08:54:53.395293: step 23080, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 14h:15m:02s remains)
INFO - root - 2017-12-16 08:54:55.082584: step 23090, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 14h:37m:24s remains)
INFO - root - 2017-12-16 08:54:56.793792: step 23100, loss = 0.44, batch loss = 0.18 (45.8 examples/sec; 0.174 sec/batch; 14h:59m:44s remains)
2017-12-16 08:54:57.294829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9339201 -2.5166276 -2.1423893 -2.005929 -2.2509458 -2.6876855 -3.1532047 -3.4685111 -3.5750279 -3.5507474 -3.5876184 -3.55198 -3.2430341 -2.6786134 -2.2216871][-2.4921973 -1.9784429 -1.5631213 -1.4382067 -1.7071929 -2.2975485 -2.95054 -3.463171 -3.6584036 -3.5903792 -3.5193133 -3.3265922 -2.8614943 -2.2255743 -1.7556856][-2.099371 -1.5812955 -1.1949215 -1.1278918 -1.4304755 -2.0718968 -2.7986221 -3.4054976 -3.6104798 -3.5135474 -3.3536406 -3.0356872 -2.5358982 -1.9414326 -1.5156419][-1.8151462 -1.3354645 -1.0422006 -1.0878017 -1.4202375 -2.0139923 -2.6716986 -3.2720649 -3.4705057 -3.3754473 -3.1852982 -2.8197057 -2.2556651 -1.6499739 -1.2737221][-1.6699862 -1.1484233 -0.84829926 -1.0143461 -1.4175079 -1.8834741 -2.42876 -2.9431953 -3.0450337 -2.9467349 -2.8443446 -2.5797763 -2.0375071 -1.4659041 -1.1364396][-1.6721836 -1.0155349 -0.65926385 -0.82852042 -1.1521333 -1.4280689 -1.7408341 -1.948936 -1.8056378 -1.6754066 -1.7807965 -1.8717703 -1.6421348 -1.2558175 -1.0280668][-1.8000258 -0.98441684 -0.39684117 -0.39624977 -0.54645836 -0.59458756 -0.64183152 -0.5607115 -0.13938069 0.10582542 -0.25442672 -0.73856509 -1.0228934 -0.94245982 -0.88380361][-2.1092641 -1.2011558 -0.4024173 -0.11944318 0.010122538 0.19902253 0.35459638 0.68981123 1.3727491 1.6648781 1.0168488 0.18330002 -0.54852748 -0.796515 -0.86466][-2.5685887 -1.7279257 -0.91612709 -0.41883755 -0.0038280487 0.46385741 0.85909104 1.3568816 2.1420105 2.3282516 1.3903506 0.3495214 -0.60181594 -0.99970925 -1.1700902][-3.2284546 -2.531759 -1.8215728 -1.3376938 -0.82172382 -0.20910239 0.36456275 0.79496408 1.3744724 1.3176575 0.26452446 -0.75675213 -1.6031582 -1.9524896 -2.1158302][-3.8494129 -3.3915882 -2.9327941 -2.6541331 -2.2454321 -1.6874793 -1.2010275 -1.0347033 -0.86879063 -1.0727159 -1.8852128 -2.63866 -3.1865211 -3.3609848 -3.423574][-4.00895 -3.7644241 -3.5759602 -3.513309 -3.3101432 -2.9198663 -2.6416316 -2.6586924 -2.7221115 -2.9330418 -3.4294684 -3.8459802 -4.0965662 -4.0975647 -4.0474491][-3.6452036 -3.5173323 -3.5005641 -3.5697296 -3.5538273 -3.3310642 -3.1958668 -3.284476 -3.3656042 -3.5158095 -3.76757 -3.9541388 -3.9952452 -3.9114175 -3.8497365][-3.1380444 -3.0764561 -3.1455612 -3.2981796 -3.35509 -3.215601 -3.1407349 -3.2193141 -3.2974575 -3.3621736 -3.4340489 -3.4896004 -3.4511542 -3.3639352 -3.3293524][-2.7305398 -2.7314026 -2.8180232 -2.9438646 -2.9692171 -2.890965 -2.8417175 -2.887485 -2.9317236 -2.923934 -2.8781059 -2.8358796 -2.7996476 -2.7706559 -2.7819667]]...]
INFO - root - 2017-12-16 08:54:59.004658: step 23110, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:30m:01s remains)
INFO - root - 2017-12-16 08:55:00.712022: step 23120, loss = 0.50, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 14h:43m:29s remains)
INFO - root - 2017-12-16 08:55:02.419872: step 23130, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:23m:11s remains)
INFO - root - 2017-12-16 08:55:04.081730: step 23140, loss = 0.62, batch loss = 0.36 (48.3 examples/sec; 0.166 sec/batch; 14h:14m:41s remains)
INFO - root - 2017-12-16 08:55:05.738692: step 23150, loss = 0.60, batch loss = 0.35 (49.0 examples/sec; 0.163 sec/batch; 14h:02m:27s remains)
INFO - root - 2017-12-16 08:55:07.418808: step 23160, loss = 0.49, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 14h:29m:58s remains)
INFO - root - 2017-12-16 08:55:09.139639: step 23170, loss = 0.58, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:08m:44s remains)
INFO - root - 2017-12-16 08:55:10.805825: step 23180, loss = 0.59, batch loss = 0.34 (45.5 examples/sec; 0.176 sec/batch; 15h:05m:50s remains)
INFO - root - 2017-12-16 08:55:12.489845: step 23190, loss = 0.56, batch loss = 0.30 (49.7 examples/sec; 0.161 sec/batch; 13h:49m:22s remains)
INFO - root - 2017-12-16 08:55:14.187483: step 23200, loss = 0.49, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 14h:26m:26s remains)
2017-12-16 08:55:14.658473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4107852 -2.5461197 -2.660027 -2.8190656 -2.9522266 -3.0074918 -3.0049028 -2.8949666 -2.6443777 -2.4133704 -2.1589303 -1.7245297 -1.2842907 -1.189103 -1.2740223][-2.5546753 -2.8115692 -3.0215645 -3.23671 -3.4217997 -3.4829483 -3.32687 -2.96354 -2.5099778 -2.2421374 -1.9927859 -1.5404277 -1.270858 -1.3533112 -1.3268392][-2.7412436 -3.1538293 -3.4610946 -3.6850138 -3.8372331 -3.8142786 -3.4542871 -2.8021898 -2.2074084 -1.9561198 -1.6990875 -1.263603 -1.1744388 -1.353622 -1.1956835][-2.9501059 -3.4746845 -3.7789712 -3.8859324 -3.8509843 -3.6604049 -3.0532541 -2.1480772 -1.5689747 -1.5069513 -1.314571 -0.93654037 -0.96704066 -1.1740618 -0.94706118][-2.906848 -3.4044304 -3.4906497 -3.2935934 -3.0256038 -2.6595554 -1.778265 -0.73272812 -0.4674288 -0.87320828 -0.92150331 -0.62709856 -0.71368408 -0.87541664 -0.61629152][-2.335813 -2.6797104 -2.4332616 -1.8844777 -1.4216151 -0.92736828 0.20812511 1.3769879 1.1412175 -0.012148857 -0.43146873 -0.25159764 -0.40020776 -0.51867473 -0.30897021][-1.5225105 -1.6717376 -1.1211076 -0.3092885 0.27653909 0.88441038 2.349755 3.7567446 3.0870206 1.2432523 0.33027744 0.19415188 -0.15023756 -0.37370729 -0.35056758][-0.96732938 -0.90661788 -0.11851835 0.84224534 1.4032717 2.0604904 3.7711027 5.4547014 4.4867115 2.2643745 1.0332017 0.503355 -0.096959352 -0.4619031 -0.65996552][-0.94430423 -0.73769414 0.071111679 0.96049833 1.3235807 1.7721403 3.1455042 4.3521757 3.5640347 1.8662169 0.83923936 0.23625088 -0.38823819 -0.77767169 -1.0312574][-1.2205122 -1.1224858 -0.58176506 0.029033422 0.22013783 0.40040016 1.2120481 1.8970969 1.3639059 0.30946231 -0.34964037 -0.77894306 -1.2236989 -1.4978455 -1.6463239][-1.6006758 -1.6641252 -1.4822352 -1.1703657 -1.1097734 -1.1246451 -0.80215526 -0.49577582 -0.82612443 -1.4164588 -1.778954 -2.0513709 -2.28706 -2.3848722 -2.3803294][-1.8466146 -2.0019958 -2.0851722 -2.0069888 -2.0446143 -2.1872029 -2.1742313 -2.1405878 -2.3735633 -2.6725609 -2.818728 -2.9139338 -2.9622312 -2.8970013 -2.7644329][-1.9289821 -2.1092784 -2.3137422 -2.3812683 -2.4938776 -2.7078507 -2.8699956 -2.9920013 -3.1929336 -3.3352053 -3.3513303 -3.2793233 -3.1666002 -3.0007896 -2.8334987][-1.9346151 -2.1015112 -2.298023 -2.4315143 -2.6068108 -2.8511353 -3.0645714 -3.2050097 -3.3263855 -3.343534 -3.2613876 -3.139396 -2.9991674 -2.8494415 -2.7391279][-2.0533648 -2.145247 -2.27277 -2.3857167 -2.5330262 -2.716855 -2.879817 -2.9674478 -2.9748704 -2.9087822 -2.8084447 -2.70651 -2.6103265 -2.5487666 -2.5398886]]...]
INFO - root - 2017-12-16 08:55:16.360874: step 23210, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 14h:06m:41s remains)
INFO - root - 2017-12-16 08:55:17.999160: step 23220, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:59m:08s remains)
INFO - root - 2017-12-16 08:55:19.677845: step 23230, loss = 0.49, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:23m:16s remains)
INFO - root - 2017-12-16 08:55:21.344412: step 23240, loss = 0.65, batch loss = 0.39 (48.8 examples/sec; 0.164 sec/batch; 14h:04m:19s remains)
INFO - root - 2017-12-16 08:55:23.024610: step 23250, loss = 0.63, batch loss = 0.37 (46.3 examples/sec; 0.173 sec/batch; 14h:49m:52s remains)
INFO - root - 2017-12-16 08:55:24.726760: step 23260, loss = 0.51, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 14h:12m:22s remains)
INFO - root - 2017-12-16 08:55:26.408778: step 23270, loss = 0.53, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 14h:31m:23s remains)
INFO - root - 2017-12-16 08:55:28.129340: step 23280, loss = 0.47, batch loss = 0.22 (46.9 examples/sec; 0.170 sec/batch; 14h:38m:29s remains)
INFO - root - 2017-12-16 08:55:29.829802: step 23290, loss = 0.53, batch loss = 0.27 (45.2 examples/sec; 0.177 sec/batch; 15h:12m:57s remains)
INFO - root - 2017-12-16 08:55:31.501821: step 23300, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 14h:03m:35s remains)
2017-12-16 08:55:31.992194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.700278 -2.8857908 -3.0312505 -3.1165535 -3.1715503 -3.2170422 -3.2580705 -3.2865291 -3.3006186 -3.3151822 -3.3299186 -3.3371453 -3.3351002 -3.3318353 -3.3403015][-2.7759404 -3.0454407 -3.2740381 -3.4331379 -3.5481033 -3.6514404 -3.73812 -3.7940946 -3.8205218 -3.8454328 -3.8700395 -3.8825207 -3.878262 -3.8721342 -3.8819487][-2.7308869 -2.9950657 -3.2233505 -3.3885195 -3.5106421 -3.63215 -3.7396574 -3.8279662 -3.8868971 -3.945765 -4.0076866 -4.0497713 -4.0657687 -4.0748506 -4.100791][-2.5173774 -2.6580038 -2.7596748 -2.8095558 -2.8277271 -2.8760722 -2.949769 -3.0350997 -3.1065347 -3.193579 -3.29845 -3.381485 -3.4364047 -3.4849954 -3.5464461][-2.1304176 -2.0572505 -1.9339154 -1.774267 -1.6239114 -1.5548776 -1.5854313 -1.6842859 -1.7931098 -1.9262668 -2.0846767 -2.2123351 -2.3051403 -2.3930123 -2.4818189][-1.7326047 -1.3900889 -0.96932685 -0.52333343 -0.13064289 0.1179235 0.16180921 0.049907923 -0.12305808 -0.33684254 -0.55119312 -0.70647991 -0.81232035 -0.90789485 -0.9974978][-1.4927062 -0.9096396 -0.2117238 0.52735853 1.2081907 1.706723 1.9012759 1.817589 1.5789597 1.270153 0.970067 0.76168919 0.632442 0.55084491 0.50536489][-1.4617274 -0.77565444 0.043032169 0.93742943 1.8195169 2.541178 2.9373596 2.9863231 2.7837961 2.4595668 2.1052868 1.8475873 1.6915424 1.6199839 1.6330435][-1.53178 -0.88327992 -0.12958002 0.69977593 1.5579054 2.2775424 2.7191026 2.8749406 2.8288896 2.6606386 2.4118731 2.1926486 2.0312178 1.9635804 2.0227854][-1.6969166 -1.13377 -0.53857195 0.098261356 0.78507924 1.3908162 1.7418683 1.8520424 1.8613899 1.8137796 1.6854274 1.5638359 1.4894776 1.4917483 1.6082909][-2.037811 -1.6256306 -1.2042292 -0.77249575 -0.28019691 0.16968513 0.40490055 0.40427685 0.37688518 0.36977267 0.30413628 0.21321726 0.15372729 0.17708683 0.31493235][-2.4252629 -2.1621318 -1.861377 -1.5712992 -1.2501376 -0.9375422 -0.79650044 -0.85473156 -0.89622223 -0.9089855 -0.94932497 -1.0339631 -1.0893041 -1.0615072 -0.92250037][-2.6867993 -2.5534832 -2.3249812 -2.0921154 -1.8589307 -1.6798178 -1.6563715 -1.756228 -1.8164265 -1.8371532 -1.8586395 -1.91518 -1.9407045 -1.8926578 -1.7485121][-2.7949331 -2.7700403 -2.5944319 -2.3730896 -2.2005761 -2.1459312 -2.240263 -2.3682072 -2.4172461 -2.4357765 -2.4503729 -2.4803302 -2.4759569 -2.4239769 -2.296201][-2.7724967 -2.8147733 -2.6907787 -2.5051913 -2.4074483 -2.4341819 -2.5790029 -2.6887381 -2.7016187 -2.6979671 -2.7024105 -2.7198949 -2.7109644 -2.6826518 -2.5950646]]...]
INFO - root - 2017-12-16 08:55:33.666445: step 23310, loss = 0.47, batch loss = 0.21 (47.8 examples/sec; 0.167 sec/batch; 14h:21m:57s remains)
INFO - root - 2017-12-16 08:55:35.301034: step 23320, loss = 0.62, batch loss = 0.37 (47.8 examples/sec; 0.167 sec/batch; 14h:22m:46s remains)
INFO - root - 2017-12-16 08:55:37.004656: step 23330, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 14h:26m:51s remains)
INFO - root - 2017-12-16 08:55:38.708063: step 23340, loss = 0.54, batch loss = 0.28 (46.4 examples/sec; 0.173 sec/batch; 14h:49m:04s remains)
INFO - root - 2017-12-16 08:55:40.403371: step 23350, loss = 0.61, batch loss = 0.35 (46.9 examples/sec; 0.170 sec/batch; 14h:37m:57s remains)
INFO - root - 2017-12-16 08:55:42.084257: step 23360, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 14h:13m:13s remains)
INFO - root - 2017-12-16 08:55:43.759148: step 23370, loss = 0.65, batch loss = 0.39 (47.7 examples/sec; 0.168 sec/batch; 14h:23m:39s remains)
INFO - root - 2017-12-16 08:55:45.473389: step 23380, loss = 0.63, batch loss = 0.37 (46.9 examples/sec; 0.170 sec/batch; 14h:38m:12s remains)
INFO - root - 2017-12-16 08:55:47.151665: step 23390, loss = 0.49, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 14h:31m:22s remains)
INFO - root - 2017-12-16 08:55:48.825196: step 23400, loss = 0.53, batch loss = 0.27 (46.7 examples/sec; 0.171 sec/batch; 14h:41m:59s remains)
2017-12-16 08:55:49.285642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.26660037 -0.46611285 -0.54697061 -0.56265795 -0.74996042 -0.97709143 -0.9816885 -1.0535976 -1.393576 -1.8265604 -2.1622906 -2.4563687 -2.5098386 -2.0965881 -1.4168162][-0.11669803 -0.47033226 -0.61905634 -0.61800849 -0.886027 -1.2023708 -1.2545104 -1.3038311 -1.686584 -2.2016621 -2.614404 -2.9529414 -3.0033705 -2.6222472 -2.0208828][-0.6693722 -1.114109 -1.2545937 -1.2419387 -1.3810569 -1.6083729 -1.5997701 -1.5409591 -1.8060137 -2.3001196 -2.8012359 -3.1757936 -3.2397382 -2.9578011 -2.4930539][-1.5908635 -2.0407081 -2.1404057 -2.0428746 -1.9609492 -1.9072031 -1.7007051 -1.5379713 -1.7919798 -2.3054452 -2.8112795 -3.1457951 -3.1958215 -3.0011039 -2.6722202][-2.17583 -2.5538335 -2.6044638 -2.3490145 -1.9343576 -1.4223453 -1.0272073 -0.85892737 -1.1944152 -1.8323965 -2.336133 -2.6031682 -2.6069551 -2.5167994 -2.3528247][-1.9319228 -2.2043254 -2.1784089 -1.6760278 -0.859434 0.11522293 0.70861268 0.61011314 -0.075569868 -1.0632299 -1.7077063 -1.8176759 -1.6963065 -1.6641762 -1.6024625][-1.3244281 -1.4665627 -1.2527087 -0.41562104 0.96570492 2.618504 3.2465696 2.5955338 1.2896895 -0.23541617 -1.0439664 -1.0067273 -0.69415057 -0.70552707 -0.84387982][-1.1575831 -1.1465902 -0.76610458 0.27879286 2.0932188 4.3949652 4.9322128 3.71492 2.00112 0.19949222 -0.59175289 -0.30143332 0.11909604 -0.034044266 -0.40784502][-1.4959767 -1.3625133 -1.0731157 -0.24591804 1.1416082 2.6814995 3.0777998 2.289432 0.93484831 -0.51751304 -1.0096902 -0.56545162 -0.23196101 -0.48742986 -0.9720571][-1.8123674 -1.5727916 -1.3902351 -1.0056765 -0.38042498 0.40195203 0.72723484 0.3150208 -0.56051922 -1.4551897 -1.6627319 -1.3623486 -1.2959791 -1.6358564 -2.0491271][-1.6372738 -1.2809844 -1.2258488 -1.2866491 -1.242219 -0.93188524 -0.77613688 -1.0215373 -1.4936674 -1.925125 -1.971033 -1.8884795 -2.0531373 -2.3750575 -2.6568451][-1.1060766 -0.61660516 -0.58514392 -0.92554164 -1.2419925 -1.2463052 -1.2163067 -1.3820741 -1.5699866 -1.6274538 -1.6026447 -1.7306665 -2.0330696 -2.3324747 -2.503654][-0.86529267 -0.28640914 -0.1503346 -0.47922337 -0.84285438 -0.91013324 -0.85626292 -0.93884671 -0.99136889 -0.89827585 -0.86601627 -1.0959933 -1.4548097 -1.6950076 -1.7324977][-1.2076327 -0.69945467 -0.4716984 -0.61883438 -0.86784339 -0.90130115 -0.86005843 -0.90384638 -0.91445255 -0.8316586 -0.78534722 -0.92439425 -1.0948341 -1.143762 -0.99265444][-1.8887811 -1.4971319 -1.2652607 -1.3452411 -1.4773396 -1.5311222 -1.4797161 -1.4813371 -1.4260453 -1.3344368 -1.2913535 -1.3219619 -1.3217835 -1.1853395 -0.9094367]]...]
INFO - root - 2017-12-16 08:55:50.966597: step 23410, loss = 0.53, batch loss = 0.27 (49.9 examples/sec; 0.160 sec/batch; 13h:45m:07s remains)
INFO - root - 2017-12-16 08:55:52.651478: step 23420, loss = 0.71, batch loss = 0.45 (48.0 examples/sec; 0.167 sec/batch; 14h:18m:11s remains)
INFO - root - 2017-12-16 08:55:54.343038: step 23430, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 14h:25m:27s remains)
INFO - root - 2017-12-16 08:55:56.042202: step 23440, loss = 0.47, batch loss = 0.21 (47.3 examples/sec; 0.169 sec/batch; 14h:30m:51s remains)
INFO - root - 2017-12-16 08:55:57.730805: step 23450, loss = 0.65, batch loss = 0.39 (47.0 examples/sec; 0.170 sec/batch; 14h:37m:25s remains)
INFO - root - 2017-12-16 08:55:59.403658: step 23460, loss = 0.58, batch loss = 0.33 (48.6 examples/sec; 0.165 sec/batch; 14h:07m:22s remains)
INFO - root - 2017-12-16 08:56:01.083427: step 23470, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.169 sec/batch; 14h:28m:19s remains)
INFO - root - 2017-12-16 08:56:02.762839: step 23480, loss = 0.65, batch loss = 0.39 (47.6 examples/sec; 0.168 sec/batch; 14h:25m:50s remains)
INFO - root - 2017-12-16 08:56:04.437219: step 23490, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:16m:52s remains)
INFO - root - 2017-12-16 08:56:06.106559: step 23500, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 13h:57m:15s remains)
2017-12-16 08:56:06.602911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6104312 -1.4749386 -1.2218841 -1.1111424 -1.5719063 -2.3210754 -2.8611298 -2.8909097 -2.7195976 -2.3709753 -2.0523882 -1.6670121 -1.4148018 -1.5679693 -2.2546439][-1.7313291 -1.7233667 -1.6011872 -1.4420795 -1.7630275 -2.3643906 -2.694766 -2.7100856 -2.6193054 -2.414223 -2.2831182 -2.1354086 -2.055316 -2.2620692 -2.7823122][-1.6624537 -1.7611625 -1.643155 -1.4026515 -1.5226276 -1.8407553 -2.0288486 -2.086731 -2.1588373 -2.2220047 -2.2190361 -2.1381059 -2.0365899 -2.1966448 -2.6051002][-1.5238608 -1.6355592 -1.536068 -1.3463508 -1.245711 -1.2198868 -1.1501327 -1.1687349 -1.3968201 -1.7899041 -1.9916549 -1.9590116 -1.782104 -1.8070548 -2.1131938][-1.4711025 -1.5149904 -1.4408475 -1.1978661 -0.85246229 -0.52299297 -0.28934669 -0.25066495 -0.5562427 -1.218215 -1.7008882 -1.830204 -1.6017799 -1.4882374 -1.7068102][-1.5764886 -1.4999142 -1.3400358 -0.971905 -0.35090971 0.16945338 0.37612486 0.2652688 -0.17464519 -1.0150168 -1.7094722 -1.9153899 -1.6308181 -1.3958483 -1.5429311][-1.5916834 -1.3666377 -1.0473292 -0.41574955 0.44889688 1.0129437 0.94515824 0.49543142 -0.24762416 -1.2760633 -2.0742245 -2.3136175 -1.9143116 -1.5140347 -1.5494741][-1.5677879 -1.1109512 -0.58274877 0.19625902 1.1539967 1.5979075 1.1785872 0.30582356 -0.65196919 -1.7197069 -2.4673653 -2.6386795 -2.0620863 -1.4473057 -1.3981953][-1.7173948 -1.1643362 -0.51924646 0.31154442 1.1572278 1.2532918 0.60795784 -0.35542631 -1.2492806 -2.2244623 -2.8619514 -2.8317513 -1.9964496 -1.1743448 -1.0699604][-2.0734828 -1.4479603 -0.81311655 -0.21640849 0.26231813 0.090592861 -0.545346 -1.3939371 -2.1339662 -2.918031 -3.3669195 -3.0352871 -1.9092115 -0.87700582 -0.74303591][-2.4356508 -1.8999258 -1.5067145 -1.2272731 -1.0788962 -1.3913467 -1.9338931 -2.5788336 -3.1178393 -3.545475 -3.7199903 -3.122597 -1.8281825 -0.71405232 -0.56615746][-2.8728728 -2.5987265 -2.4358475 -2.3633742 -2.3986161 -2.7004323 -3.17888 -3.6733813 -3.9166858 -3.907742 -3.7052875 -2.9999106 -1.8735294 -0.96553075 -0.92576218][-3.3461032 -3.2961955 -3.1908839 -3.1631553 -3.3026583 -3.5785155 -4.0315351 -4.413322 -4.440434 -4.0746117 -3.523787 -2.8906219 -2.155112 -1.6811905 -1.866003][-3.6320095 -3.6645114 -3.6052222 -3.5972433 -3.7661114 -4.0317287 -4.44558 -4.7959385 -4.697927 -4.1455278 -3.4449768 -2.9876783 -2.6797633 -2.6230936 -2.8927934][-3.679862 -3.6772389 -3.63756 -3.6425197 -3.7903354 -4.0212703 -4.4162731 -4.7453742 -4.6971207 -4.2493896 -3.6400459 -3.3113685 -3.2709408 -3.4276433 -3.618906]]...]
INFO - root - 2017-12-16 08:56:08.305597: step 23510, loss = 0.50, batch loss = 0.25 (45.8 examples/sec; 0.175 sec/batch; 14h:59m:02s remains)
INFO - root - 2017-12-16 08:56:10.012257: step 23520, loss = 0.67, batch loss = 0.41 (47.7 examples/sec; 0.168 sec/batch; 14h:23m:50s remains)
INFO - root - 2017-12-16 08:56:11.728118: step 23530, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.170 sec/batch; 14h:37m:41s remains)
INFO - root - 2017-12-16 08:56:13.396421: step 23540, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 14h:05m:33s remains)
INFO - root - 2017-12-16 08:56:15.061464: step 23550, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:26m:13s remains)
INFO - root - 2017-12-16 08:56:16.756148: step 23560, loss = 0.65, batch loss = 0.39 (45.7 examples/sec; 0.175 sec/batch; 15h:01m:58s remains)
INFO - root - 2017-12-16 08:56:18.450137: step 23570, loss = 0.63, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 14h:24m:16s remains)
INFO - root - 2017-12-16 08:56:20.126586: step 23580, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:23m:11s remains)
INFO - root - 2017-12-16 08:56:21.792128: step 23590, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:19m:30s remains)
INFO - root - 2017-12-16 08:56:23.479212: step 23600, loss = 0.48, batch loss = 0.23 (49.2 examples/sec; 0.163 sec/batch; 13h:57m:50s remains)
2017-12-16 08:56:23.991622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8938847 -2.9025085 -2.9334388 -2.9634469 -3.0371056 -3.2464139 -3.4641576 -3.6615102 -3.8415031 -4.0085526 -4.0753069 -3.885035 -3.3391342 -2.6201918 -2.0868223][-2.9077184 -2.8914011 -2.8032274 -2.6387455 -2.4838033 -2.5811822 -2.8700838 -3.2735302 -3.6609411 -3.9822674 -4.1625943 -4.0606971 -3.50223 -2.704771 -2.0741394][-2.7184124 -2.7192864 -2.5231657 -2.1108789 -1.6795827 -1.5144706 -1.7572771 -2.4115746 -3.0992413 -3.6227589 -4.00522 -4.085115 -3.6708212 -2.9167204 -2.2242541][-2.5394118 -2.5842974 -2.3066642 -1.6991365 -0.94549727 -0.3694644 -0.42373192 -1.2043489 -2.1925526 -3.0209165 -3.6724558 -3.9839644 -3.7824278 -3.1456347 -2.4918346][-2.5183508 -2.6389647 -2.3022704 -1.4756405 -0.34179544 0.74841905 1.0960429 0.32036281 -0.96602714 -2.2018147 -3.1649814 -3.705389 -3.7096577 -3.2571757 -2.6535611][-2.5810263 -2.7243886 -2.3152852 -1.3176773 0.20023203 1.7949626 2.5195396 1.8327143 0.31804967 -1.2618293 -2.5480368 -3.318943 -3.4964781 -3.1857762 -2.6341333][-2.5191455 -2.6285856 -2.1519687 -1.0496185 0.63690376 2.4535329 3.4350574 2.860615 1.3238764 -0.49741817 -2.0085185 -2.8793674 -3.1700835 -2.9041476 -2.3768144][-2.1012185 -2.0728972 -1.629076 -0.7039026 0.82468629 2.4443629 3.2581275 2.7326248 1.3516657 -0.36594725 -1.8392749 -2.6764035 -2.9014769 -2.5976043 -2.1161587][-1.5710959 -1.4174557 -1.1060115 -0.54762387 0.5811193 1.5975266 1.9528954 1.4939756 0.42165923 -0.98937368 -2.1892295 -2.8329456 -2.9105895 -2.5657883 -2.1555874][-1.3189669 -1.1918081 -1.0957767 -0.894768 -0.20665574 0.27518272 0.307374 -0.0048043728 -0.76133049 -1.8369462 -2.7023482 -3.0642321 -3.0087161 -2.7217927 -2.4075418][-1.506326 -1.4421389 -1.5313168 -1.5690699 -1.2118067 -0.99606419 -1.0466565 -1.2444166 -1.7585098 -2.5222931 -3.0451365 -3.1559751 -3.025162 -2.8263752 -2.6315923][-1.8930546 -1.9142987 -2.0826437 -2.2289286 -2.0879245 -1.9617033 -2.069555 -2.2340376 -2.5524108 -2.9775763 -3.1787949 -3.1218982 -2.9825401 -2.8562365 -2.7307146][-2.3421354 -2.4332795 -2.62285 -2.7832539 -2.7352903 -2.6402562 -2.7037435 -2.8142531 -2.9511204 -3.1236858 -3.1345203 -2.9993291 -2.8534398 -2.7444677 -2.661135][-2.5958889 -2.717015 -2.8705492 -2.958637 -2.9171109 -2.8204727 -2.7928436 -2.8223445 -2.8530364 -2.8790617 -2.8162982 -2.6841826 -2.5693154 -2.4862781 -2.4264007][-2.5102861 -2.6036637 -2.6764665 -2.6854837 -2.6271434 -2.5223193 -2.4432042 -2.4089887 -2.3925111 -2.3708169 -2.3214133 -2.2489631 -2.1824841 -2.1400821 -2.1176503]]...]
INFO - root - 2017-12-16 08:56:25.645037: step 23610, loss = 0.47, batch loss = 0.21 (47.7 examples/sec; 0.168 sec/batch; 14h:23m:31s remains)
INFO - root - 2017-12-16 08:56:27.313803: step 23620, loss = 0.53, batch loss = 0.27 (47.5 examples/sec; 0.168 sec/batch; 14h:26m:53s remains)
INFO - root - 2017-12-16 08:56:28.973214: step 23630, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:20m:22s remains)
INFO - root - 2017-12-16 08:56:30.644003: step 23640, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:08m:39s remains)
INFO - root - 2017-12-16 08:56:32.337660: step 23650, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 14h:30m:11s remains)
INFO - root - 2017-12-16 08:56:34.004800: step 23660, loss = 0.55, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 14h:24m:07s remains)
INFO - root - 2017-12-16 08:56:35.743375: step 23670, loss = 0.55, batch loss = 0.30 (43.1 examples/sec; 0.186 sec/batch; 15h:56m:07s remains)
INFO - root - 2017-12-16 08:56:37.442491: step 23680, loss = 0.50, batch loss = 0.24 (49.8 examples/sec; 0.161 sec/batch; 13h:46m:50s remains)
INFO - root - 2017-12-16 08:56:39.124534: step 23690, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:23m:03s remains)
INFO - root - 2017-12-16 08:56:40.799259: step 23700, loss = 0.50, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 14h:18m:21s remains)
2017-12-16 08:56:41.293148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1948191 -0.99498475 -0.57263434 -0.28204155 -0.44264364 -0.83241439 -1.1696141 -1.3422898 -1.3263177 -1.2630023 -1.3533158 -1.5202094 -1.6230919 -1.6957352 -1.839202][-0.31147861 -0.27068996 -0.12911272 -0.18129301 -0.6534034 -1.3570294 -1.8470035 -2.07503 -2.12038 -2.1114998 -2.2033799 -2.3448436 -2.4119778 -2.381099 -2.3512852][0.54711485 0.46887469 0.38979983 0.073015451 -0.62664258 -1.5241852 -2.1731524 -2.5425625 -2.7382545 -2.8298821 -2.9187388 -2.9943924 -2.9496834 -2.7547765 -2.5287793][1.3273394 1.1818395 0.91062045 0.38265967 -0.41582096 -1.314463 -1.9991661 -2.4732764 -2.8443985 -3.0824943 -3.2051637 -3.2430825 -3.1439536 -2.8859344 -2.5798063][1.7671549 1.6261222 1.2269974 0.57125545 -0.15876555 -0.73044693 -1.1144719 -1.5665127 -2.1316149 -2.583102 -2.809165 -2.8926463 -2.8777392 -2.7509108 -2.5451682][1.7151425 1.7503006 1.4181685 0.86094069 0.41561937 0.36543965 0.43660808 0.058047533 -0.74978352 -1.4592526 -1.8028035 -1.9725094 -2.0921431 -2.1720903 -2.2035596][0.99708748 1.3248072 1.2979364 1.0734482 1.0762501 1.5818143 2.1281865 1.866708 0.84565043 -0.0738585 -0.528088 -0.72525108 -0.90301812 -1.0852642 -1.3159482][-0.19475985 0.32501125 0.58561015 0.72171187 1.1343358 2.0209792 2.8773739 2.8429077 1.854831 0.93924165 0.50660968 0.38923311 0.324641 0.20191884 -0.038388252][-1.3546405 -0.88004065 -0.558578 -0.27522945 0.25293398 1.1311045 1.958801 2.0561097 1.4786682 0.92957282 0.72153616 0.76991153 0.88880992 0.92609191 0.786957][-2.1770172 -1.9145596 -1.7146618 -1.4934894 -1.0884957 -0.44250107 0.1848979 0.38200879 0.2064991 0.061030149 0.10663962 0.263623 0.44842505 0.5940752 0.60520363][-2.4811614 -2.4591918 -2.4368019 -2.3648615 -2.178349 -1.8376186 -1.4505974 -1.2401371 -1.1813533 -1.1056808 -0.94783115 -0.78385258 -0.631565 -0.45781851 -0.34783506][-2.3177037 -2.4479008 -2.574434 -2.644876 -2.6334829 -2.5247655 -2.3592794 -2.2210939 -2.1256204 -2.0109553 -1.8846322 -1.7769232 -1.6911424 -1.5658355 -1.4224806][-1.815854 -2.0114274 -2.2299235 -2.3943863 -2.481566 -2.4896097 -2.4396076 -2.3763535 -2.3268809 -2.2972739 -2.2876759 -2.2974265 -2.3093443 -2.2679543 -2.1409864][-1.2254353 -1.4310436 -1.6928589 -1.9102739 -2.0337391 -2.0600123 -2.0151136 -1.9552749 -1.9272045 -1.9607439 -2.0512125 -2.1780338 -2.3069673 -2.3615811 -2.3160579][-0.76956344 -0.97167706 -1.2369608 -1.4413933 -1.5325854 -1.5114927 -1.3996892 -1.258875 -1.1877941 -1.2371703 -1.3840141 -1.5972207 -1.8127795 -1.962719 -2.0233302]]...]
INFO - root - 2017-12-16 08:56:42.984905: step 23710, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.170 sec/batch; 14h:32m:27s remains)
INFO - root - 2017-12-16 08:56:44.643072: step 23720, loss = 0.48, batch loss = 0.22 (49.9 examples/sec; 0.160 sec/batch; 13h:45m:21s remains)
INFO - root - 2017-12-16 08:56:46.335890: step 23730, loss = 0.47, batch loss = 0.22 (48.0 examples/sec; 0.167 sec/batch; 14h:17m:49s remains)
INFO - root - 2017-12-16 08:56:47.993345: step 23740, loss = 0.57, batch loss = 0.31 (50.4 examples/sec; 0.159 sec/batch; 13h:37m:16s remains)
INFO - root - 2017-12-16 08:56:49.679450: step 23750, loss = 0.65, batch loss = 0.39 (43.9 examples/sec; 0.182 sec/batch; 15h:38m:20s remains)
INFO - root - 2017-12-16 08:56:51.344385: step 23760, loss = 0.53, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 13h:53m:35s remains)
INFO - root - 2017-12-16 08:56:53.039523: step 23770, loss = 0.53, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:34m:03s remains)
INFO - root - 2017-12-16 08:56:54.704999: step 23780, loss = 0.52, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:59m:44s remains)
INFO - root - 2017-12-16 08:56:56.384174: step 23790, loss = 0.48, batch loss = 0.22 (47.9 examples/sec; 0.167 sec/batch; 14h:18m:56s remains)
INFO - root - 2017-12-16 08:56:58.077353: step 23800, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 14h:11m:54s remains)
2017-12-16 08:56:58.584980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4136896 -3.7899637 -3.739851 -3.3747249 -3.1098886 -2.9205456 -2.923034 -3.1653614 -3.7280624 -4.2106571 -3.9923148 -3.3054433 -2.5683846 -2.0157895 -1.6380557][-3.5217428 -3.7112412 -3.4093487 -2.6632044 -2.1568995 -1.8281174 -1.8069131 -2.277662 -3.2945502 -4.2573152 -4.3328567 -3.6386833 -2.7744057 -2.0688088 -1.5449179][-3.3102026 -3.230243 -2.6924777 -1.5870054 -0.6735183 -0.10502124 -0.11705375 -0.89396679 -2.4067271 -3.9141283 -4.3805494 -3.8711114 -3.0658255 -2.3404441 -1.7342741][-3.0637932 -2.6524565 -1.8014765 -0.30151582 1.1383631 2.007529 1.9793065 0.96503544 -0.94505918 -2.9792128 -3.9638498 -3.8934011 -3.3747392 -2.7731738 -2.1755259][-3.013588 -2.2870836 -1.1145326 0.77448511 2.6446378 3.9240458 4.0396709 2.9797361 0.84082651 -1.5783882 -3.136323 -3.7097404 -3.6529257 -3.2489073 -2.74014][-3.2388291 -2.5004187 -1.2042552 0.78115749 2.926317 4.6897869 5.2676678 4.4291563 2.4141533 -0.092652559 -2.0990181 -3.2817802 -3.7077193 -3.5755503 -3.2017317][-3.5871353 -3.0843723 -2.0714312 -0.33482909 1.8230321 3.8515108 5.0034666 4.7501869 3.2584908 1.0976987 -0.95867908 -2.495733 -3.3294721 -3.5110235 -3.3529711][-3.6138506 -3.3752384 -2.7747726 -1.5138705 0.31665945 2.3468277 3.8171299 4.1302319 3.3303115 1.839751 0.094062328 -1.5559692 -2.6666329 -3.1411824 -3.1981077][-3.1850569 -3.19256 -2.9403191 -2.178889 -0.81726015 1.0019898 2.566613 3.2734172 3.1387498 2.4288614 1.1400626 -0.57617486 -1.9127347 -2.6325741 -2.9096019][-2.6909413 -2.7820952 -2.7988763 -2.4654529 -1.635093 -0.2967 1.1625228 2.1666415 2.6929572 2.6839225 1.730736 0.095243216 -1.3237863 -2.1342225 -2.5657442][-2.2206743 -2.3454092 -2.5113122 -2.5457094 -2.2909517 -1.5360605 -0.41822481 0.67076492 1.5235877 1.8799326 1.2532532 0.020641804 -1.0855752 -1.7742493 -2.2556129][-1.798249 -1.8277314 -2.0362117 -2.3040793 -2.54332 -2.3784764 -1.7390922 -0.84292305 -0.011731863 0.42123556 0.19940329 -0.54813814 -1.1567935 -1.5589707 -2.0009208][-1.4259516 -1.1396812 -1.2421446 -1.6534116 -2.2334554 -2.5282533 -2.3022046 -1.7119529 -1.0524434 -0.70464921 -0.79022682 -1.1627407 -1.3529224 -1.435945 -1.6895871][-1.2563524 -0.63002491 -0.53117454 -0.93558979 -1.6305268 -2.1306343 -2.192976 -1.8828578 -1.4371908 -1.2982464 -1.4919218 -1.7586287 -1.7059462 -1.5042427 -1.4576191][-1.2938107 -0.54560423 -0.19155049 -0.41944218 -0.94751346 -1.3847811 -1.5309656 -1.5006814 -1.4650228 -1.5673913 -1.907703 -2.1602979 -2.0781708 -1.7074094 -1.351115]]...]
INFO - root - 2017-12-16 08:57:00.290416: step 23810, loss = 0.74, batch loss = 0.48 (48.5 examples/sec; 0.165 sec/batch; 14h:09m:12s remains)
INFO - root - 2017-12-16 08:57:01.945623: step 23820, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.169 sec/batch; 14h:31m:10s remains)
INFO - root - 2017-12-16 08:57:03.617939: step 23830, loss = 0.54, batch loss = 0.28 (46.0 examples/sec; 0.174 sec/batch; 14h:54m:11s remains)
INFO - root - 2017-12-16 08:57:05.318305: step 23840, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 14h:11m:26s remains)
INFO - root - 2017-12-16 08:57:07.022774: step 23850, loss = 0.58, batch loss = 0.32 (46.6 examples/sec; 0.172 sec/batch; 14h:42m:23s remains)
INFO - root - 2017-12-16 08:57:08.701175: step 23860, loss = 0.60, batch loss = 0.34 (48.8 examples/sec; 0.164 sec/batch; 14h:03m:35s remains)
INFO - root - 2017-12-16 08:57:10.397332: step 23870, loss = 0.52, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 14h:06m:13s remains)
INFO - root - 2017-12-16 08:57:12.046182: step 23880, loss = 0.55, batch loss = 0.29 (48.4 examples/sec; 0.165 sec/batch; 14h:09m:32s remains)
INFO - root - 2017-12-16 08:57:13.709775: step 23890, loss = 0.61, batch loss = 0.35 (46.4 examples/sec; 0.172 sec/batch; 14h:46m:19s remains)
INFO - root - 2017-12-16 08:57:15.391897: step 23900, loss = 0.56, batch loss = 0.30 (46.2 examples/sec; 0.173 sec/batch; 14h:51m:12s remains)
2017-12-16 08:57:15.884132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.22029 -2.2282295 -2.1717007 -2.1951478 -2.3320789 -2.3961375 -2.3070171 -2.1400125 -1.9850502 -1.8081703 -1.6463296 -1.6763568 -1.9505378 -2.2095313 -2.3789225][-1.5303534 -1.6351943 -1.7319225 -1.8421551 -1.9497685 -2.0337603 -1.9924768 -1.7831632 -1.5373267 -1.3576537 -1.2081336 -1.1837362 -1.372131 -1.6326786 -1.8635507][-0.58815038 -0.82642055 -1.0964721 -1.2648718 -1.3400965 -1.4147606 -1.3724397 -1.1972327 -1.0838442 -0.98178959 -0.79294705 -0.67575443 -0.82057548 -1.0879186 -1.3016047][0.23543811 -0.14931846 -0.49072015 -0.63475823 -0.57466125 -0.50754333 -0.52017379 -0.57136989 -0.71082103 -0.76479006 -0.61504412 -0.43832159 -0.51668251 -0.78709137 -0.98943126][0.63614869 0.24250364 -0.047947407 -0.083398581 0.15081477 0.38463259 0.29730558 -0.047228336 -0.42963731 -0.60800385 -0.55831707 -0.35591173 -0.37348819 -0.61229122 -0.76155674][0.535012 0.30121303 0.20582747 0.31789637 0.70375228 1.0268831 0.86864877 0.34367752 -0.10786986 -0.3225491 -0.4088186 -0.36255121 -0.39980197 -0.55702293 -0.62044144][-0.068146944 -0.038986206 0.18062973 0.56181288 1.1262357 1.5241041 1.3388679 0.78716516 0.41427326 0.22680974 -0.0049557686 -0.19357228 -0.29456615 -0.47163451 -0.64901567][-0.797215 -0.48768353 0.060898066 0.76310444 1.5543859 2.1500971 2.131397 1.5739348 1.1956151 0.94473076 0.58011127 0.215518 0.039443731 -0.33635759 -0.80184436][-1.3933305 -0.812093 0.016346931 0.99363303 2.0237639 2.8731463 3.1032197 2.429734 1.8535788 1.3773456 0.84473658 0.47107458 0.22996998 -0.39922464 -1.1286453][-1.9881016 -1.2381645 -0.25526834 0.86717582 1.990062 2.8574607 3.0228546 2.3484447 1.6696632 1.0594902 0.48019385 0.12653685 -0.14878058 -0.86022449 -1.6466072][-2.5584257 -1.8370012 -0.80764496 0.36984777 1.3342021 1.8380282 1.6709507 1.088191 0.56029868 0.097156048 -0.36044383 -0.70132446 -1.044552 -1.595482 -2.1818895][-2.8650498 -2.29499 -1.2749624 -0.26340508 0.26752234 0.302886 -0.073811293 -0.55670345 -0.92247427 -1.1294839 -1.3164588 -1.5091959 -1.7743208 -2.0951941 -2.4259584][-2.623276 -2.2272444 -1.4681838 -0.85859776 -0.79655337 -1.112144 -1.5893366 -1.9792049 -2.2024441 -2.1653538 -2.0609107 -1.9941158 -2.0548351 -2.1622524 -2.2519844][-2.0028408 -1.7694926 -1.3634729 -1.2379842 -1.5102248 -2.0312555 -2.5811148 -2.89134 -2.9542782 -2.7157617 -2.37956 -2.0856578 -1.9344908 -1.8239071 -1.734993][-1.4849883 -1.4352156 -1.338928 -1.52596 -1.9865828 -2.6212687 -3.1861672 -3.3850825 -3.2445331 -2.8176534 -2.3279042 -1.883597 -1.5865161 -1.3242551 -1.1646295]]...]
INFO - root - 2017-12-16 08:57:17.605627: step 23910, loss = 0.57, batch loss = 0.31 (45.4 examples/sec; 0.176 sec/batch; 15h:05m:19s remains)
INFO - root - 2017-12-16 08:57:19.324364: step 23920, loss = 0.49, batch loss = 0.23 (47.2 examples/sec; 0.169 sec/batch; 14h:31m:33s remains)
INFO - root - 2017-12-16 08:57:20.999343: step 23930, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:30m:29s remains)
INFO - root - 2017-12-16 08:57:22.664477: step 23940, loss = 0.63, batch loss = 0.37 (44.7 examples/sec; 0.179 sec/batch; 15h:20m:58s remains)
INFO - root - 2017-12-16 08:57:24.359250: step 23950, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 14h:39m:27s remains)
INFO - root - 2017-12-16 08:57:26.019914: step 23960, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 14h:11m:17s remains)
INFO - root - 2017-12-16 08:57:27.699591: step 23970, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 14h:22m:51s remains)
INFO - root - 2017-12-16 08:57:29.352287: step 23980, loss = 0.46, batch loss = 0.20 (47.4 examples/sec; 0.169 sec/batch; 14h:28m:34s remains)
INFO - root - 2017-12-16 08:57:31.064183: step 23990, loss = 0.57, batch loss = 0.31 (46.5 examples/sec; 0.172 sec/batch; 14h:44m:52s remains)
INFO - root - 2017-12-16 08:57:32.764266: step 24000, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 14h:22m:56s remains)
2017-12-16 08:57:33.279132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3330741 -3.1257269 -2.3961601 -1.0910369 -0.28825402 -0.49476993 -0.93949962 -0.87688243 -0.94111 -1.7708983 -2.9578588 -3.7515936 -3.7273264 -3.088238 -2.373004][-3.7567897 -3.7902732 -3.3272932 -2.3860612 -1.813621 -1.905606 -2.0198669 -1.6200461 -1.4183877 -2.025681 -3.0666311 -3.7985058 -3.8000221 -3.187686 -2.4617794][-4.2282248 -4.3457704 -4.0829859 -3.4351563 -2.9745088 -2.9054689 -2.634563 -1.9240187 -1.4691598 -1.8616924 -2.7363276 -3.4490969 -3.5436034 -3.0440872 -2.4079][-4.6898994 -4.7970171 -4.5758991 -3.9654219 -3.3618717 -2.9659204 -2.3915634 -1.5769166 -1.1301496 -1.401446 -2.1149035 -2.8374348 -3.1004348 -2.8028347 -2.2906942][-4.8500366 -4.9069295 -4.6267262 -3.8195024 -2.7215288 -1.7767723 -0.939119 -0.23970175 -0.12559128 -0.65022361 -1.4716675 -2.3271627 -2.7638922 -2.6272743 -2.1990213][-4.5103922 -4.4127836 -3.8903823 -2.6543953 -0.73350227 1.0079854 2.0213897 2.2066762 1.3870845 0.013693094 -1.333052 -2.3326333 -2.7616465 -2.6019561 -2.1407871][-4.0392933 -3.7112846 -2.8002188 -1.056188 1.6768363 4.29099 5.4591341 4.811429 2.6155641 -0.014941931 -1.9603502 -2.9397564 -3.11376 -2.7145045 -2.1012132][-3.7619512 -3.2978022 -2.1598537 -0.21988297 2.9120986 6.2054243 7.3551388 6.0441437 2.8367789 -0.65586436 -2.9800425 -3.8195798 -3.629456 -2.9000823 -2.0830443][-3.6350465 -3.22079 -2.1199281 -0.50693774 1.9701879 4.6551437 6.03459 5.0707073 1.8463886 -1.7872086 -4.0590429 -4.6062212 -4.029779 -2.9929225 -2.0052133][-3.6324692 -3.4083819 -2.4479444 -1.2246002 0.39671922 2.3387907 3.7446768 3.2583654 0.55296135 -2.6856432 -4.6418715 -4.9498963 -4.1647153 -2.975215 -1.907213][-3.8121841 -3.7596815 -2.9894979 -2.1125484 -1.1290346 0.15411448 1.4453945 1.4938719 -0.39136267 -2.921819 -4.5087442 -4.7325497 -3.9926906 -2.8527861 -1.8225452][-3.8909726 -4.0426965 -3.5716348 -3.0402317 -2.4636858 -1.5759917 -0.37407994 0.10136628 -0.99749041 -2.8312538 -4.1009121 -4.3194108 -3.732775 -2.7312844 -1.7920742][-3.9103789 -4.2027354 -3.9664311 -3.6313825 -3.2234905 -2.476537 -1.3762683 -0.72207975 -1.3281927 -2.667546 -3.6950717 -3.9225428 -3.4432139 -2.5669487 -1.7588215][-3.890635 -4.1576209 -3.9864941 -3.6557689 -3.2175658 -2.5386918 -1.6000879 -0.96338224 -1.3350087 -2.3883202 -3.2596612 -3.4927745 -3.0795007 -2.3136539 -1.6457974][-3.6609025 -3.7928615 -3.5464706 -3.1518865 -2.6833258 -2.0981958 -1.3310233 -0.81220281 -1.15243 -2.09266 -2.8727357 -3.0863242 -2.7002709 -1.9963005 -1.4633783]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 08:57:34.929901: step 24010, loss = 0.49, batch loss = 0.23 (49.8 examples/sec; 0.161 sec/batch; 13h:46m:21s remains)
INFO - root - 2017-12-16 08:57:36.593818: step 24020, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 14h:06m:34s remains)
INFO - root - 2017-12-16 08:57:38.280973: step 24030, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 14h:03m:55s remains)
INFO - root - 2017-12-16 08:57:39.932299: step 24040, loss = 0.51, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:07m:45s remains)
INFO - root - 2017-12-16 08:57:41.564586: step 24050, loss = 0.53, batch loss = 0.27 (51.1 examples/sec; 0.156 sec/batch; 13h:24m:15s remains)
INFO - root - 2017-12-16 08:57:43.223389: step 24060, loss = 0.46, batch loss = 0.20 (49.7 examples/sec; 0.161 sec/batch; 13h:46m:54s remains)
INFO - root - 2017-12-16 08:57:44.852801: step 24070, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 14h:05m:02s remains)
INFO - root - 2017-12-16 08:57:46.562691: step 24080, loss = 0.63, batch loss = 0.37 (46.3 examples/sec; 0.173 sec/batch; 14h:48m:00s remains)
INFO - root - 2017-12-16 08:57:48.218771: step 24090, loss = 0.47, batch loss = 0.21 (48.9 examples/sec; 0.164 sec/batch; 14h:00m:44s remains)
INFO - root - 2017-12-16 08:57:49.937525: step 24100, loss = 0.51, batch loss = 0.25 (46.5 examples/sec; 0.172 sec/batch; 14h:44m:09s remains)
2017-12-16 08:57:50.433534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.501151 -1.2628098 -1.1839552 -1.3139201 -1.5008396 -1.599282 -1.4870467 -1.2103915 -1.048393 -1.3078125 -1.8106322 -2.2570295 -2.4110582 -2.2211347 -1.7203114][-1.4263151 -1.133585 -1.0862335 -1.3261262 -1.6250906 -1.7436469 -1.566939 -1.1826652 -0.92117906 -1.083908 -1.5346391 -2.0206077 -2.2945514 -2.2538004 -1.8862747][-1.7395508 -1.4147575 -1.4017904 -1.7464783 -2.1496832 -2.2880096 -2.0528989 -1.6533906 -1.3512604 -1.3597867 -1.646822 -2.0629487 -2.4215426 -2.5314615 -2.2658515][-2.1726069 -1.8655989 -1.8447638 -2.2000821 -2.6496649 -2.8105738 -2.6030822 -2.290092 -2.0544243 -1.9586675 -2.0561886 -2.3337102 -2.7002997 -2.863184 -2.614346][-2.3211005 -2.0470221 -1.9873862 -2.3312266 -2.8564668 -3.1018167 -2.9637694 -2.735558 -2.5448332 -2.3942537 -2.3329003 -2.4782379 -2.8562243 -3.0756738 -2.84147][-2.1101029 -1.8193879 -1.7292598 -2.0751107 -2.6516418 -2.9946394 -2.9363379 -2.7303507 -2.4970932 -2.2863665 -2.1204448 -2.2044928 -2.6374826 -2.9379292 -2.7899923][-1.7121559 -1.3938105 -1.2940739 -1.5875912 -2.1290755 -2.4704816 -2.3997014 -2.0924547 -1.7139541 -1.3828301 -1.1384684 -1.2487085 -1.8176689 -2.2870166 -2.2893929][-1.4010458 -1.0814524 -0.95114791 -1.1189897 -1.4727719 -1.6762028 -1.5175943 -1.0589579 -0.48679268 0.01961112 0.32606173 0.12943006 -0.64994121 -1.3755223 -1.6111259][-1.3878145 -1.206242 -1.0687046 -1.0481789 -1.1141082 -1.0959258 -0.82922256 -0.22181654 0.55644369 1.2454386 1.652602 1.3596659 0.25910282 -0.79195237 -1.2895359][-1.6793981 -1.7442353 -1.6834353 -1.5486797 -1.4075975 -1.2245307 -0.88584518 -0.26160598 0.55702567 1.2784817 1.6879604 1.3179357 0.10640693 -1.0899284 -1.7399997][-1.9592992 -2.2656717 -2.3442159 -2.2350588 -2.0409603 -1.8214564 -1.5155073 -1.0301553 -0.39046013 0.25844646 0.64745927 0.30888963 -0.73600864 -1.8523203 -2.4979794][-2.0169959 -2.4029369 -2.5668876 -2.5443771 -2.4317698 -2.3234818 -2.1411762 -1.8423374 -1.393779 -0.81749797 -0.39453948 -0.607059 -1.453948 -2.4230902 -3.014214][-1.8494489 -2.1267455 -2.2698879 -2.3117106 -2.3430684 -2.3866839 -2.3882465 -2.2947874 -1.9882786 -1.4225919 -0.91104412 -0.99131739 -1.709963 -2.5851374 -3.1440253][-1.6890961 -1.7625201 -1.8034756 -1.8487993 -1.9719504 -2.1433051 -2.2910535 -2.33255 -2.0922215 -1.5125804 -0.95669305 -0.99291 -1.6790028 -2.4887018 -2.9795837][-1.8101501 -1.7221775 -1.6429629 -1.6168692 -1.7052493 -1.8727171 -2.0328846 -2.0870011 -1.8485211 -1.2852656 -0.83018541 -0.97345519 -1.680892 -2.3698089 -2.6920657]]...]
INFO - root - 2017-12-16 08:57:52.131087: step 24110, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:18m:25s remains)
INFO - root - 2017-12-16 08:57:53.790385: step 24120, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 14h:23m:50s remains)
INFO - root - 2017-12-16 08:57:55.448623: step 24130, loss = 0.48, batch loss = 0.22 (46.5 examples/sec; 0.172 sec/batch; 14h:43m:16s remains)
INFO - root - 2017-12-16 08:57:57.120144: step 24140, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.167 sec/batch; 14h:20m:08s remains)
INFO - root - 2017-12-16 08:57:58.805303: step 24150, loss = 0.62, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 14h:20m:22s remains)
INFO - root - 2017-12-16 08:58:00.465274: step 24160, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 14h:24m:12s remains)
INFO - root - 2017-12-16 08:58:02.126904: step 24170, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:18m:49s remains)
INFO - root - 2017-12-16 08:58:03.797150: step 24180, loss = 0.62, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 14h:17m:43s remains)
INFO - root - 2017-12-16 08:58:05.465959: step 24190, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 14h:28m:31s remains)
INFO - root - 2017-12-16 08:58:07.138342: step 24200, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 14h:20m:39s remains)
2017-12-16 08:58:07.646864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0137787 -2.3460548 -1.8940721 -2.1600215 -2.7523887 -3.1245708 -3.2621479 -3.3685391 -3.4678855 -3.3829179 -2.8673062 -1.9711088 -1.4291987 -1.787987 -2.8217065][-2.9600708 -2.1462531 -1.7205037 -2.1590176 -2.850117 -3.1853704 -3.2924879 -3.4681838 -3.6768427 -3.656929 -3.0644844 -2.0404775 -1.3687656 -1.6427219 -2.6916311][-2.9514589 -2.0152137 -1.577615 -2.0284727 -2.6082509 -2.681077 -2.6451685 -2.8001328 -3.112114 -3.2259569 -2.7350676 -1.8013031 -1.1380898 -1.3532236 -2.3354046][-2.7546351 -1.8179721 -1.3789852 -1.6955588 -2.0058563 -1.7292993 -1.3608134 -1.3476628 -1.7107666 -2.099668 -1.9398102 -1.2685853 -0.79152358 -1.0259548 -1.9369515][-2.3684781 -1.4896934 -1.1237136 -1.2588255 -1.2530166 -0.65027487 0.19920158 0.64580727 0.23146892 -0.61781371 -1.0335063 -0.794917 -0.56168044 -0.84452724 -1.66891][-1.7903619 -0.85445547 -0.49343526 -0.54850674 -0.37349772 0.53169966 1.8775072 2.8122663 2.3422327 0.89908957 -0.18209457 -0.43869364 -0.49811172 -0.87083268 -1.6711351][-1.0065438 0.061623812 0.66204906 0.73249197 1.0111985 2.0501223 3.761241 5.1870489 4.6929889 2.5832925 0.93235564 0.31183219 0.052051544 -0.44257224 -1.3425419][-0.39136386 0.83104992 1.6688638 1.9805212 2.2989969 3.2671332 4.9200635 6.2600107 5.730464 3.5366979 1.7102852 0.98198986 0.71375895 0.17685318 -0.79355681][-0.76957464 0.37668562 1.2692766 1.647037 1.7862058 2.5007062 3.7334986 4.3663435 3.5840712 1.9141321 0.55150652 0.12846303 0.146842 -0.20506096 -1.1160139][-1.7681782 -0.6945442 0.21560073 0.48750353 0.37886834 0.7916913 1.5754864 1.7637691 1.0222218 -0.10731387 -0.82185483 -0.79731596 -0.53841865 -0.78088748 -1.6991005][-2.6655474 -1.6455183 -0.75215721 -0.61610293 -0.86844909 -0.72374868 -0.27875352 -0.21032405 -0.75083244 -1.4094512 -1.5577784 -1.0995582 -0.66080165 -0.95109391 -2.0081437][-3.3619142 -2.3643446 -1.4979568 -1.5564066 -2.01795 -2.1540451 -1.959836 -1.9486561 -2.2589872 -2.5027552 -2.1931918 -1.433074 -0.84409547 -1.1425824 -2.2857018][-3.8313055 -2.7856526 -1.978457 -2.1866715 -2.8387251 -3.1451697 -3.10427 -3.1065302 -3.1952071 -3.1245139 -2.6097863 -1.7662625 -1.1270467 -1.3797047 -2.4470289][-4.0065455 -2.9360986 -2.1674709 -2.3816679 -3.037744 -3.4225683 -3.4999404 -3.5013943 -3.4706702 -3.2750506 -2.75607 -2.0059538 -1.4809164 -1.6121434 -2.4371655][-3.8818884 -2.9916019 -2.3573055 -2.4901233 -2.9924388 -3.3342333 -3.4455156 -3.422827 -3.3271112 -3.1259432 -2.7444603 -2.2176149 -1.8189615 -1.802735 -2.3231363]]...]
INFO - root - 2017-12-16 08:58:09.330365: step 24210, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 14h:19m:44s remains)
INFO - root - 2017-12-16 08:58:11.046135: step 24220, loss = 0.53, batch loss = 0.27 (46.7 examples/sec; 0.171 sec/batch; 14h:39m:40s remains)
INFO - root - 2017-12-16 08:58:12.718936: step 24230, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:22m:26s remains)
INFO - root - 2017-12-16 08:58:14.390049: step 24240, loss = 0.53, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 14h:11m:38s remains)
INFO - root - 2017-12-16 08:58:16.070223: step 24250, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.164 sec/batch; 14h:05m:04s remains)
INFO - root - 2017-12-16 08:58:17.770614: step 24260, loss = 0.57, batch loss = 0.31 (47.2 examples/sec; 0.170 sec/batch; 14h:31m:01s remains)
INFO - root - 2017-12-16 08:58:19.451302: step 24270, loss = 0.47, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 14h:14m:06s remains)
INFO - root - 2017-12-16 08:58:21.140121: step 24280, loss = 0.50, batch loss = 0.24 (48.8 examples/sec; 0.164 sec/batch; 14h:01m:47s remains)
INFO - root - 2017-12-16 08:58:22.821310: step 24290, loss = 0.62, batch loss = 0.36 (45.8 examples/sec; 0.175 sec/batch; 14h:58m:10s remains)
INFO - root - 2017-12-16 08:58:24.539870: step 24300, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.164 sec/batch; 14h:04m:47s remains)
2017-12-16 08:58:25.029128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.81276762 -0.60841942 -0.59154928 -0.706234 -0.87686253 -1.1548127 -1.6203829 -2.2505631 -2.9166694 -3.2119513 -3.0177875 -2.529844 -2.1295609 -1.9552197 -1.8461409][-0.45231783 -0.015940189 0.23093629 0.23404717 0.1120398 -0.2161293 -0.85627794 -1.7298367 -2.5932467 -3.0876989 -3.0378916 -2.6582971 -2.30556 -2.1134112 -1.9313995][-0.12115192 0.38081384 0.86919665 1.0073538 0.95961642 0.57761192 -0.21099615 -1.1850702 -2.1293504 -2.7708032 -2.8845506 -2.6367166 -2.351294 -2.1243582 -1.8665249][0.099238396 0.44463563 0.96510673 1.1376634 1.1227379 0.859082 0.19637871 -0.69071817 -1.6204367 -2.3415165 -2.5879123 -2.4631977 -2.2067378 -1.8855152 -1.5569631][0.40795541 0.48372054 0.73046374 0.80726147 0.80184913 0.75247836 0.43570638 -0.2265842 -1.0859321 -1.8258146 -2.2145116 -2.2127519 -1.9531895 -1.5635018 -1.1904274][0.96806026 0.86434126 0.88111567 0.82089925 0.83042574 0.98711658 0.91890526 0.39095879 -0.43388855 -1.2721092 -1.8339992 -1.9769093 -1.7480545 -1.3651142 -0.98343241][1.3706763 1.2763631 1.3100228 1.3038712 1.3942211 1.7443333 1.8076162 1.3248224 0.44646287 -0.59875071 -1.3807404 -1.7018509 -1.5663059 -1.2431345 -0.86042547][1.1136639 1.0879843 1.2245159 1.3884838 1.8079934 2.5151758 2.7632637 2.29733 1.3441176 0.10992384 -0.90247869 -1.4243548 -1.4728374 -1.2072394 -0.84469962][0.24953914 0.31839323 0.4951365 0.82691932 1.5600331 2.6197939 3.16183 2.8455653 1.908699 0.58890319 -0.60135853 -1.278012 -1.4757897 -1.3011713 -1.0045043][-0.93926597 -0.81947958 -0.58152366 -0.038445473 0.90672207 2.0316005 2.7017922 2.6112485 1.7744246 0.51960588 -0.63174546 -1.3043994 -1.5701373 -1.5235406 -1.3222432][-2.0234561 -1.8494208 -1.5533323 -0.90493667 0.054667473 1.0170269 1.6018915 1.5540731 0.84712529 -0.19163299 -1.0670599 -1.5381544 -1.71069 -1.7159503 -1.5797049][-2.7412741 -2.5762606 -2.256685 -1.5823777 -0.707574 0.030266285 0.41969562 0.25885963 -0.38136029 -1.149689 -1.631305 -1.770697 -1.7515724 -1.7262824 -1.6330565][-2.8157303 -2.7346909 -2.4429829 -1.8316489 -1.1316155 -0.59396517 -0.43280554 -0.71272624 -1.2685575 -1.7703203 -1.9266289 -1.821481 -1.659225 -1.5847833 -1.5385683][-2.1727254 -2.1445305 -1.8825598 -1.4051797 -0.89605713 -0.59684682 -0.7352612 -1.1641252 -1.6143081 -1.9045947 -1.8828946 -1.6737127 -1.5009587 -1.474746 -1.5464211][-1.211761 -1.1481508 -0.84303641 -0.46643245 -0.17286849 -0.14314604 -0.49207807 -1.0356855 -1.5178437 -1.6928766 -1.5879633 -1.3761214 -1.2585464 -1.3404408 -1.5740247]]...]
INFO - root - 2017-12-16 08:58:26.715604: step 24310, loss = 0.62, batch loss = 0.36 (45.7 examples/sec; 0.175 sec/batch; 14h:59m:21s remains)
INFO - root - 2017-12-16 08:58:28.421269: step 24320, loss = 0.50, batch loss = 0.25 (49.4 examples/sec; 0.162 sec/batch; 13h:51m:26s remains)
INFO - root - 2017-12-16 08:58:30.082495: step 24330, loss = 0.57, batch loss = 0.31 (48.1 examples/sec; 0.166 sec/batch; 14h:13m:27s remains)
INFO - root - 2017-12-16 08:58:31.757965: step 24340, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 14h:20m:11s remains)
INFO - root - 2017-12-16 08:58:33.442344: step 24350, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 14h:34m:00s remains)
INFO - root - 2017-12-16 08:58:35.116968: step 24360, loss = 0.51, batch loss = 0.25 (48.9 examples/sec; 0.163 sec/batch; 13h:59m:21s remains)
INFO - root - 2017-12-16 08:58:36.759955: step 24370, loss = 0.57, batch loss = 0.31 (49.8 examples/sec; 0.161 sec/batch; 13h:44m:23s remains)
INFO - root - 2017-12-16 08:58:38.450676: step 24380, loss = 0.48, batch loss = 0.22 (48.9 examples/sec; 0.164 sec/batch; 14h:00m:01s remains)
INFO - root - 2017-12-16 08:58:40.137979: step 24390, loss = 0.62, batch loss = 0.36 (46.7 examples/sec; 0.171 sec/batch; 14h:40m:12s remains)
INFO - root - 2017-12-16 08:58:41.823247: step 24400, loss = 0.55, batch loss = 0.29 (49.0 examples/sec; 0.163 sec/batch; 13h:58m:38s remains)
2017-12-16 08:58:42.289491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3383648 -3.6693697 -3.7777166 -3.5778475 -2.9832392 -2.2257628 -1.7052696 -1.5728909 -1.5577807 -1.8895009 -2.4602726 -2.8334715 -2.7181952 -2.2113249 -1.6102374][-4.5763559 -4.790658 -4.6922531 -4.3792143 -3.7721419 -2.9086826 -2.2877686 -2.2999187 -2.6498144 -3.0760975 -3.3756647 -3.5278959 -3.3190079 -2.8310575 -2.1967554][-5.2137814 -5.2328305 -4.88676 -4.3968916 -3.8363554 -3.0863028 -2.4689939 -2.5757298 -3.1721096 -3.6627018 -3.7391744 -3.7370763 -3.5608268 -3.1680896 -2.4967668][-4.7424259 -4.4812293 -3.8375189 -3.1219053 -2.6877155 -2.2915173 -1.8312759 -1.9368447 -2.6624174 -3.3155313 -3.4018109 -3.3796246 -3.3295221 -3.0720146 -2.3389506][-3.6562586 -2.9199576 -1.8393035 -0.77536786 -0.38485765 -0.3615737 -0.12649083 -0.27853894 -1.1468946 -2.0859678 -2.4103847 -2.50121 -2.6225834 -2.5197773 -1.7710159][-2.4720435 -1.0483731 0.600508 1.9855237 2.3538804 2.0719662 2.2191505 2.1692219 1.1983936 -0.070399046 -0.87271094 -1.2935463 -1.6987722 -1.9073482 -1.3683308][-1.7980958 0.28188014 2.5073705 4.1341519 4.4795532 4.1306586 4.4531932 4.6569376 3.5647678 1.9151163 0.57761216 -0.29767418 -1.0442785 -1.5455085 -1.2570961][-1.9098794 0.5131526 2.9253793 4.4541969 4.7352672 4.6532083 5.3767357 5.8774233 4.6169562 2.6929359 1.0175784 -0.18278861 -1.1112537 -1.721555 -1.6388209][-2.7184432 -0.39039671 1.7144012 2.8195677 2.9532385 3.1771584 4.133975 4.6802821 3.621222 1.8648548 0.26390457 -0.93269336 -1.8585641 -2.4100752 -2.3414907][-3.6318698 -1.8912653 -0.31417227 0.39315462 0.46585131 0.83911419 1.818254 2.3025756 1.4650118 0.047374725 -1.2836206 -2.2536752 -2.9568627 -3.302815 -3.1772172][-4.268261 -3.3451598 -2.4877689 -2.1676185 -2.0938916 -1.6377552 -0.74293077 -0.30006361 -0.85972655 -1.8569533 -2.835525 -3.5236111 -3.9235363 -3.9907131 -3.7298448][-4.5142078 -4.2721395 -4.0395484 -4.03765 -4.0181952 -3.5905795 -2.8306179 -2.4376283 -2.7915301 -3.4671009 -4.0727243 -4.3916321 -4.398602 -4.1566119 -3.7219312][-4.2652044 -4.3172407 -4.3556323 -4.4734116 -4.4892883 -4.2227745 -3.758285 -3.536442 -3.7721267 -4.2249188 -4.5528584 -4.5909114 -4.2984772 -3.8467147 -3.3106575][-3.8533115 -3.7721477 -3.7288766 -3.7778883 -3.7890005 -3.6308517 -3.3787107 -3.2863972 -3.5061526 -3.8562834 -4.0777359 -4.0080624 -3.6687026 -3.2338371 -2.7452519][-3.4618444 -3.0994687 -2.8414047 -2.7421782 -2.6716466 -2.5138266 -2.3571086 -2.3350308 -2.547219 -2.8579903 -3.0457306 -2.9895766 -2.7799928 -2.523849 -2.1918478]]...]
INFO - root - 2017-12-16 08:58:43.942458: step 24410, loss = 0.62, batch loss = 0.36 (50.2 examples/sec; 0.159 sec/batch; 13h:38m:17s remains)
INFO - root - 2017-12-16 08:58:45.579141: step 24420, loss = 0.49, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 14h:16m:04s remains)
INFO - root - 2017-12-16 08:58:47.248004: step 24430, loss = 0.55, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 14h:24m:55s remains)
INFO - root - 2017-12-16 08:58:48.915121: step 24440, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.165 sec/batch; 14h:09m:37s remains)
INFO - root - 2017-12-16 08:58:50.575591: step 24450, loss = 0.61, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 14h:03m:28s remains)
INFO - root - 2017-12-16 08:58:52.210327: step 24460, loss = 0.61, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:08m:59s remains)
INFO - root - 2017-12-16 08:58:53.888367: step 24470, loss = 0.61, batch loss = 0.35 (45.3 examples/sec; 0.177 sec/batch; 15h:07m:38s remains)
INFO - root - 2017-12-16 08:58:55.553452: step 24480, loss = 0.55, batch loss = 0.29 (49.2 examples/sec; 0.162 sec/batch; 13h:53m:55s remains)
INFO - root - 2017-12-16 08:58:57.193579: step 24490, loss = 0.68, batch loss = 0.42 (48.1 examples/sec; 0.166 sec/batch; 14h:13m:35s remains)
INFO - root - 2017-12-16 08:58:58.870342: step 24500, loss = 0.63, batch loss = 0.37 (47.0 examples/sec; 0.170 sec/batch; 14h:34m:21s remains)
2017-12-16 08:58:59.328538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.77229929 -0.81582737 -1.0839684 -1.691149 -2.4248588 -2.9429924 -3.0749629 -2.95708 -2.741925 -2.5652955 -2.4949911 -2.3661253 -2.1177003 -1.8513753 -1.6288618][-1.0586898 -1.2005805 -1.4567674 -1.9256848 -2.5087483 -2.8764315 -2.8425364 -2.5429223 -2.2624772 -2.1377029 -2.1033843 -1.9609953 -1.6978362 -1.4386311 -1.2417446][-1.3267914 -1.5369871 -1.7528939 -1.9965155 -2.210604 -2.2353659 -1.9452727 -1.4926562 -1.1930828 -1.234236 -1.351153 -1.3299146 -1.2280375 -1.1360213 -1.0719092][-1.3886937 -1.591908 -1.7572682 -1.8272775 -1.6710961 -1.2646019 -0.64401436 -0.066637516 0.16055083 -0.10484838 -0.46701682 -0.7283107 -0.93944275 -1.0933242 -1.1904018][-1.3224574 -1.4750257 -1.5864627 -1.57162 -1.2463678 -0.55298829 0.27922273 0.81356287 0.89944148 0.50231934 -0.069913387 -0.63293052 -1.1542765 -1.4570884 -1.6299911][-1.3368987 -1.3953158 -1.4114351 -1.4036729 -1.0297265 -0.24482179 0.641006 1.1022258 1.0767941 0.59497476 -0.17601347 -0.99107933 -1.6400154 -1.9788954 -2.0927188][-1.5190969 -1.4995743 -1.4437004 -1.4262089 -1.1626145 -0.44101906 0.46874738 1.0255303 1.0544801 0.54748297 -0.33820009 -1.2727988 -1.9361669 -2.2169614 -2.197324][-1.6910973 -1.6689923 -1.6172431 -1.640946 -1.4560053 -0.84196556 -0.0055551529 0.57868171 0.75729752 0.37576628 -0.44008398 -1.3072759 -1.8363833 -1.9315152 -1.685328][-1.8259296 -1.8417619 -1.7969713 -1.818285 -1.7239687 -1.2547568 -0.619046 -0.13505411 0.067459106 -0.16432095 -0.75399947 -1.3356339 -1.571919 -1.3854988 -0.85383368][-1.9473758 -2.0257003 -1.9988405 -1.9758871 -1.9421904 -1.6421287 -1.2105669 -0.91009927 -0.76433539 -0.86006296 -1.1530823 -1.3621335 -1.2813476 -0.8591367 -0.07721138][-2.0543916 -2.1844766 -2.2037354 -2.1921878 -2.1759272 -1.9728261 -1.6820109 -1.5230628 -1.4701543 -1.43947 -1.4857589 -1.4243089 -1.1638312 -0.68077874 0.10975671][-2.1196835 -2.265579 -2.3145139 -2.3183379 -2.3013105 -2.1501431 -1.9942081 -1.9326355 -1.9337521 -1.9135987 -1.8989266 -1.7514138 -1.4333646 -0.97053373 -0.34990358][-2.0887752 -2.2345481 -2.2941008 -2.3162026 -2.3396254 -2.28026 -2.239383 -2.2587996 -2.3161309 -2.3714542 -2.3993647 -2.2869489 -1.9795542 -1.5923949 -1.163293][-1.9908395 -2.120996 -2.1913602 -2.2385037 -2.3081255 -2.3389289 -2.3987541 -2.4822083 -2.5828068 -2.685324 -2.7417839 -2.6756825 -2.4465461 -2.180068 -1.9511044][-1.8449879 -1.9506905 -2.0273428 -2.0862892 -2.1679842 -2.2374704 -2.3365107 -2.4456213 -2.5608208 -2.6604538 -2.7213383 -2.7224164 -2.6129103 -2.4586589 -2.3530245]]...]
INFO - root - 2017-12-16 08:59:00.997492: step 24510, loss = 0.48, batch loss = 0.22 (47.9 examples/sec; 0.167 sec/batch; 14h:18m:06s remains)
INFO - root - 2017-12-16 08:59:02.665203: step 24520, loss = 0.54, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 14h:22m:01s remains)
INFO - root - 2017-12-16 08:59:04.326787: step 24530, loss = 0.59, batch loss = 0.33 (48.6 examples/sec; 0.164 sec/batch; 14h:04m:02s remains)
INFO - root - 2017-12-16 08:59:06.002342: step 24540, loss = 0.56, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 13h:58m:22s remains)
INFO - root - 2017-12-16 08:59:07.672811: step 24550, loss = 0.58, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 14h:21m:51s remains)
INFO - root - 2017-12-16 08:59:09.329387: step 24560, loss = 0.56, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 14h:07m:53s remains)
INFO - root - 2017-12-16 08:59:10.985721: step 24570, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 14h:11m:08s remains)
INFO - root - 2017-12-16 08:59:12.655871: step 24580, loss = 0.60, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 14h:14m:46s remains)
INFO - root - 2017-12-16 08:59:14.333725: step 24590, loss = 0.48, batch loss = 0.22 (49.0 examples/sec; 0.163 sec/batch; 13h:58m:19s remains)
INFO - root - 2017-12-16 08:59:16.001185: step 24600, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 14h:36m:29s remains)
2017-12-16 08:59:16.493097: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29423809 -0.2095592 -0.69260788 -1.1526161 -1.7031391 -2.1725373 -2.4028862 -2.5354893 -2.4661109 -2.22994 -2.18219 -2.5668244 -3.280231 -3.7383618 -3.60709][1.2800293 0.41404223 -0.34184623 -0.93088746 -1.5600578 -2.0112579 -2.1883175 -2.3087797 -2.3424828 -2.2320883 -2.1637931 -2.462559 -3.1229322 -3.5199504 -3.278481][1.4371181 0.39569139 -0.53308809 -1.2086303 -1.7456574 -1.9856908 -1.9474156 -1.9198383 -1.9654624 -2.0303109 -2.1312881 -2.4948254 -3.0764275 -3.3201976 -2.9479403][0.87264252 -0.18839478 -1.1395187 -1.7195406 -1.9296564 -1.7278179 -1.3975253 -1.3156976 -1.4512143 -1.790318 -2.204633 -2.7496727 -3.2334127 -3.3021009 -2.8426323][-0.13240719 -1.0349267 -1.8234346 -2.0493047 -1.6823287 -0.94230318 -0.26310563 -0.13036895 -0.47364616 -1.2174567 -2.0914149 -2.9433467 -3.4204416 -3.3730559 -2.873558][-1.0422864 -1.5981302 -1.8816018 -1.4813676 -0.40010154 0.93108368 1.9165239 2.0358672 1.3348114 0.042362213 -1.3991978 -2.6338422 -3.3029027 -3.3254097 -2.9123287][-1.7100359 -1.9512303 -1.6997318 -0.61549854 1.1119936 3.0295649 4.2827096 4.329114 3.3207417 1.5112543 -0.40496349 -1.9637632 -2.831706 -3.01516 -2.76623][-2.6897264 -2.8387427 -2.3040676 -0.85038066 1.2487617 3.599278 5.1066203 5.0858884 3.9351168 2.0362034 0.0252738 -1.6252921 -2.5519426 -2.7986341 -2.6212151][-3.4260027 -3.7677789 -3.4468822 -2.145601 -0.10492539 2.2191067 3.8033528 3.7437787 2.7042556 1.2690461 -0.33765531 -1.8304772 -2.7363055 -2.9564445 -2.724771][-3.4609652 -4.1086726 -4.268322 -3.5706427 -1.9943943 -0.060123205 1.3205302 1.3466439 0.71747351 -0.056679487 -1.0493919 -2.2492187 -3.1334269 -3.3665075 -3.0180345][-2.9551289 -3.9156694 -4.57085 -4.4795876 -3.5084333 -2.1623533 -1.2182831 -1.0873215 -1.2882015 -1.5034325 -1.9261043 -2.8182092 -3.646626 -3.8754063 -3.4382029][-2.2702534 -3.4565802 -4.4410639 -4.7591581 -4.3499994 -3.5757627 -3.031251 -2.8557997 -2.7433939 -2.5910611 -2.6987231 -3.348115 -4.0682707 -4.2398634 -3.7578011][-1.5764625 -2.787389 -3.853375 -4.3349266 -4.2417779 -3.8659449 -3.6075664 -3.4887972 -3.2806849 -2.9853225 -2.9562576 -3.4134979 -3.9985018 -4.1430063 -3.6791523][-1.0440361 -2.0293448 -2.8945594 -3.3126318 -3.3703659 -3.2547414 -3.1734812 -3.1301539 -2.9671159 -2.6902575 -2.6183264 -2.9132657 -3.3466134 -3.4723833 -3.1417904][-0.79310572 -1.4233037 -1.9786665 -2.2606843 -2.3624625 -2.3786654 -2.3933532 -2.4084647 -2.3308954 -2.1526253 -2.0948617 -2.2695904 -2.544529 -2.6683412 -2.5055261]]...]
INFO - root - 2017-12-16 08:59:18.167426: step 24610, loss = 0.61, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 14h:10m:42s remains)
INFO - root - 2017-12-16 08:59:19.841779: step 24620, loss = 0.52, batch loss = 0.27 (48.6 examples/sec; 0.164 sec/batch; 14h:03m:54s remains)
INFO - root - 2017-12-16 08:59:21.503118: step 24630, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:12m:57s remains)
INFO - root - 2017-12-16 08:59:23.198737: step 24640, loss = 0.61, batch loss = 0.35 (44.8 examples/sec; 0.178 sec/batch; 15h:15m:41s remains)
INFO - root - 2017-12-16 08:59:24.929650: step 24650, loss = 0.71, batch loss = 0.45 (48.5 examples/sec; 0.165 sec/batch; 14h:07m:10s remains)
INFO - root - 2017-12-16 08:59:26.635072: step 24660, loss = 0.62, batch loss = 0.36 (45.2 examples/sec; 0.177 sec/batch; 15h:07m:20s remains)
INFO - root - 2017-12-16 08:59:28.323671: step 24670, loss = 0.54, batch loss = 0.29 (48.4 examples/sec; 0.165 sec/batch; 14h:08m:08s remains)
INFO - root - 2017-12-16 08:59:29.969958: step 24680, loss = 0.62, batch loss = 0.36 (48.2 examples/sec; 0.166 sec/batch; 14h:11m:18s remains)
INFO - root - 2017-12-16 08:59:31.636891: step 24690, loss = 0.55, batch loss = 0.30 (48.3 examples/sec; 0.165 sec/batch; 14h:08m:56s remains)
INFO - root - 2017-12-16 08:59:33.320257: step 24700, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 14h:07m:30s remains)
2017-12-16 08:59:33.786093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.918581 -1.7393017 -1.8781173 -2.1666181 -2.2541013 -1.9627028 -1.4839501 -0.82367241 -0.13671875 0.36285758 0.33010221 -0.35932231 -1.330595 -1.9558005 -2.130038][-1.2663126 -1.151754 -1.3474598 -1.7131462 -1.9347525 -1.8226937 -1.4198613 -0.75583911 0.029759407 0.71070766 0.82374525 0.24848247 -0.82175171 -1.627676 -1.9509056][-0.52667689 -0.46045554 -0.6877358 -1.1876097 -1.5553811 -1.6821942 -1.5001593 -1.08932 -0.46719015 0.15637946 0.30570006 -0.13859034 -1.1315329 -1.9463623 -2.2901897][0.2651186 0.26426387 -0.048568249 -0.60601056 -1.0159749 -1.2522171 -1.4132068 -1.4675366 -1.2757964 -0.97302008 -0.97103417 -1.3864324 -2.1938903 -2.8487427 -3.0845716][0.74002075 0.78956723 0.51158834 0.0052120686 -0.34797668 -0.57809889 -1.0584817 -1.5872853 -1.8769894 -1.9490788 -2.2060552 -2.6995559 -3.3873048 -3.8831863 -3.974967][0.59120226 0.81190634 0.87549281 0.57465148 0.5101943 0.55137253 0.083972692 -0.7594496 -1.4372852 -1.9044486 -2.4607744 -3.2176862 -3.9750202 -4.432375 -4.4489579][0.12813044 0.58779407 0.97963452 0.92489362 1.247761 1.7435384 1.4297369 0.54002094 -0.335083 -1.0529325 -1.8603988 -2.8610644 -3.787189 -4.3318634 -4.4385095][-0.3886385 0.11777377 0.59641075 0.87613297 1.4619627 2.2344213 2.0234065 1.1922746 0.41640043 -0.25247812 -1.0707434 -2.188098 -3.2537155 -3.9287672 -4.2542849][-0.87390172 -0.580608 -0.26872563 0.026272058 0.55586672 1.16102 1.0296247 0.42918944 0.010911226 -0.26092911 -0.80744267 -1.8351911 -2.8652859 -3.5816355 -4.10733][-1.1373152 -1.1894789 -1.1988848 -1.2229143 -1.0869083 -0.83446336 -0.92755687 -1.1616528 -1.0704255 -0.85602665 -1.0673622 -1.8513331 -2.6614037 -3.2915192 -3.9441934][-1.2163554 -1.590204 -1.888746 -2.3014567 -2.6542928 -2.7428119 -2.8774242 -2.7921073 -2.2165594 -1.554496 -1.4296691 -1.917783 -2.4550045 -3.0083382 -3.6892128][-0.98666632 -1.5410653 -2.045686 -2.8622351 -3.6138315 -3.9461365 -4.1107254 -3.8143358 -2.8620737 -1.7879837 -1.3937335 -1.7313032 -2.1278334 -2.517848 -3.0599306][-0.55782807 -1.1533377 -1.7513459 -2.7569537 -3.6950378 -4.1361594 -4.2517676 -3.8041649 -2.6125472 -1.3343111 -0.78925121 -0.97222316 -1.1757104 -1.3840348 -1.8038104][-0.11881804 -0.69031107 -1.1812928 -2.0655236 -2.9313293 -3.3525243 -3.3852997 -2.8115296 -1.5629194 -0.3362391 0.25601077 0.28847814 0.21471429 0.07460475 -0.21309352][0.26888323 -0.18607044 -0.48059261 -1.0604626 -1.6681802 -1.9831774 -1.9822538 -1.439834 -0.34827328 0.73060584 1.3237641 1.5078566 1.5822496 1.4525063 1.2034757]]...]
INFO - root - 2017-12-16 08:59:35.448918: step 24710, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 14h:47m:37s remains)
INFO - root - 2017-12-16 08:59:37.117455: step 24720, loss = 0.47, batch loss = 0.21 (48.6 examples/sec; 0.165 sec/batch; 14h:04m:00s remains)
INFO - root - 2017-12-16 08:59:38.793890: step 24730, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 14h:13m:09s remains)
INFO - root - 2017-12-16 08:59:40.459909: step 24740, loss = 0.61, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 14h:14m:18s remains)
INFO - root - 2017-12-16 08:59:42.096477: step 24750, loss = 0.52, batch loss = 0.26 (49.4 examples/sec; 0.162 sec/batch; 13h:49m:54s remains)
INFO - root - 2017-12-16 08:59:43.736991: step 24760, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 14h:01m:20s remains)
INFO - root - 2017-12-16 08:59:45.435037: step 24770, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 14h:23m:40s remains)
INFO - root - 2017-12-16 08:59:47.066424: step 24780, loss = 0.54, batch loss = 0.29 (50.2 examples/sec; 0.159 sec/batch; 13h:37m:11s remains)
INFO - root - 2017-12-16 08:59:48.758310: step 24790, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 14h:12m:41s remains)
INFO - root - 2017-12-16 08:59:50.432653: step 24800, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 14h:48m:54s remains)
2017-12-16 08:59:50.944244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7041554 -1.69051 -1.683418 -1.6788521 -1.6579105 -1.6035945 -1.5272417 -1.4960508 -1.5434146 -1.6439105 -1.727176 -1.7268728 -1.6564848 -1.575492 -1.5325712][-1.8348351 -1.8030952 -1.7914132 -1.7884048 -1.7562207 -1.6614119 -1.5287443 -1.4673781 -1.5285432 -1.6757044 -1.8118882 -1.8355434 -1.7437352 -1.6211025 -1.5515679][-1.9046508 -1.8390622 -1.8072513 -1.7985805 -1.7535558 -1.6147188 -1.4187508 -1.3295768 -1.4173069 -1.6236669 -1.8256402 -1.9019128 -1.8109646 -1.6602411 -1.5655386][-1.9156368 -1.7940674 -1.7068346 -1.6510532 -1.5629585 -1.379396 -1.1313889 -1.0328418 -1.1709443 -1.4553173 -1.7403653 -1.8938866 -1.8355056 -1.6799668 -1.5714529][-1.989364 -1.8036388 -1.6230536 -1.4671748 -1.276064 -1.0045774 -0.70895159 -0.62494719 -0.83531213 -1.2079486 -1.5830843 -1.8264482 -1.8243282 -1.683326 -1.5733383][-2.171963 -1.9625077 -1.6991198 -1.4228075 -1.0929909 -0.68645513 -0.30159855 -0.24278212 -0.54713178 -1.0021701 -1.4501705 -1.7567978 -1.803247 -1.6831446 -1.5754136][-2.4815598 -2.3115168 -2.020582 -1.6523783 -1.198362 -0.66518474 -0.16258073 -0.070664406 -0.44064009 -0.93908882 -1.4045897 -1.722828 -1.7885035 -1.6842179 -1.5811969][-2.8087335 -2.7467041 -2.5177042 -2.1180787 -1.5744346 -0.95688 -0.38605392 -0.1987536 -0.53595006 -1.017428 -1.4460996 -1.726207 -1.7799935 -1.6805542 -1.5843132][-2.9487257 -3.03784 -2.9433761 -2.5855806 -2.0078397 -1.3475415 -0.74517941 -0.47565043 -0.69857728 -1.122985 -1.498988 -1.7244021 -1.7569988 -1.6628158 -1.5774052][-2.8057017 -3.046916 -3.1484089 -2.9077234 -2.3695362 -1.6967123 -1.0831414 -0.763219 -0.87670875 -1.2196761 -1.5382023 -1.7097414 -1.7212417 -1.6366633 -1.5638958][-2.4960692 -2.8799727 -3.1716537 -3.0984573 -2.6524153 -2.0110886 -1.4067407 -1.0562651 -1.0781904 -1.3262581 -1.566726 -1.6864047 -1.6821226 -1.6061237 -1.5452914][-2.0981798 -2.6296632 -3.1099851 -3.2129316 -2.8701775 -2.2806134 -1.7069025 -1.3428918 -1.2922504 -1.4418743 -1.5981359 -1.6638037 -1.6428607 -1.5701497 -1.5176195][-1.7489992 -2.3888247 -3.0380282 -3.296289 -3.0549722 -2.5184755 -1.9773688 -1.605773 -1.5014358 -1.5740912 -1.6635973 -1.6831679 -1.6376083 -1.5554098 -1.4997017][-1.5216169 -2.2380977 -3.0146859 -3.3772922 -3.2162042 -2.726105 -2.228528 -1.8762163 -1.7555802 -1.8045808 -1.8493173 -1.8175664 -1.7195491 -1.5954013 -1.51109][-1.3701179 -2.1416979 -3.0172276 -3.4383049 -3.3363659 -2.891078 -2.4645925 -2.1868522 -2.1116285 -2.1738753 -2.1981976 -2.1148715 -1.931636 -1.7232848 -1.5725894]]...]
INFO - root - 2017-12-16 08:59:52.673308: step 24810, loss = 0.64, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 14h:10m:01s remains)
INFO - root - 2017-12-16 08:59:54.387495: step 24820, loss = 0.56, batch loss = 0.30 (46.9 examples/sec; 0.171 sec/batch; 14h:35m:10s remains)
INFO - root - 2017-12-16 08:59:56.039144: step 24830, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 13h:55m:46s remains)
INFO - root - 2017-12-16 08:59:57.664948: step 24840, loss = 0.69, batch loss = 0.43 (51.5 examples/sec; 0.155 sec/batch; 13h:17m:03s remains)
INFO - root - 2017-12-16 08:59:59.327611: step 24850, loss = 0.59, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 14h:13m:51s remains)
INFO - root - 2017-12-16 09:00:01.020914: step 24860, loss = 0.47, batch loss = 0.21 (47.8 examples/sec; 0.167 sec/batch; 14h:17m:45s remains)
INFO - root - 2017-12-16 09:00:02.733776: step 24870, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:26m:26s remains)
INFO - root - 2017-12-16 09:00:04.513992: step 24880, loss = 0.51, batch loss = 0.25 (45.6 examples/sec; 0.175 sec/batch; 14h:59m:43s remains)
INFO - root - 2017-12-16 09:00:06.192704: step 24890, loss = 0.48, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 14h:25m:35s remains)
INFO - root - 2017-12-16 09:00:07.846165: step 24900, loss = 0.54, batch loss = 0.28 (49.8 examples/sec; 0.161 sec/batch; 13h:44m:11s remains)
2017-12-16 09:00:08.335306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3371115 -5.0328922 -5.3044548 -5.0277863 -4.2152824 -3.17565 -2.2155237 -2.5468891 -3.418036 -3.7871957 -3.6962285 -3.3668542 -3.1150928 -3.1395738 -3.3414514][-4.1955209 -4.8333335 -5.0421667 -4.6192179 -3.638936 -2.4455433 -1.3772688 -1.807066 -2.9262114 -3.542408 -3.5708876 -3.277204 -3.09686 -3.2075982 -3.4296012][-3.5345967 -4.0203528 -4.0884724 -3.5742059 -2.5049851 -1.2562362 -0.1448853 -0.68128514 -2.021162 -2.8309808 -2.917789 -2.6868296 -2.6061523 -2.8387337 -3.0944366][-2.6619956 -3.0033751 -2.9633985 -2.3256445 -1.1501327 0.24315524 1.5065696 0.89170218 -0.72843754 -1.766404 -1.9603455 -1.8648946 -1.966814 -2.3698921 -2.6527407][-1.9538375 -2.2140157 -2.133023 -1.3711736 -0.040959835 1.6119754 3.1349056 2.4499166 0.57825518 -0.62569273 -0.99569154 -1.0650948 -1.3984218 -1.9565362 -2.2206519][-1.5429161 -1.7560515 -1.6704642 -0.83731985 0.60938787 2.5445416 4.4059153 3.7690465 1.6891224 0.29019451 -0.25973773 -0.509773 -1.0626692 -1.7390898 -2.01888][-1.4407295 -1.6390435 -1.5177877 -0.60835123 0.93622231 3.142916 5.413805 4.8377333 2.4976823 0.87601686 0.1398859 -0.280262 -0.97012997 -1.769546 -2.0331547][-1.4772067 -1.644116 -1.4062655 -0.43529522 1.127682 3.4656494 5.9054022 5.2869015 2.7660282 0.97770977 0.078979969 -0.38739634 -1.0166243 -1.716853 -1.8644898][-1.5655909 -1.6611905 -1.2964381 -0.35092402 1.118886 3.3012345 5.4845171 4.8531151 2.5191987 0.80109406 -0.039817572 -0.43259406 -0.91241884 -1.3780224 -1.3716639][-1.7655333 -1.8130789 -1.4611144 -0.66118741 0.5905304 2.3903463 4.1317682 3.6404717 1.7502544 0.28052568 -0.36347842 -0.60530961 -0.938457 -1.2223742 -1.0872689][-2.149334 -2.2716851 -2.0312164 -1.4064931 -0.40588415 1.0667126 2.4777734 2.1135323 0.62702966 -0.57490993 -1.0753751 -1.1733294 -1.3554443 -1.4965917 -1.3350037][-2.7117558 -2.9828064 -2.9068298 -2.4653292 -1.7045095 -0.52304494 0.63626575 0.41086316 -0.71164489 -1.6471497 -2.027298 -2.0756731 -2.1434612 -2.1902924 -2.0210884][-3.2268853 -3.6411986 -3.7745073 -3.5365663 -3.0252092 -2.177573 -1.3508295 -1.4273287 -2.1625783 -2.8113439 -3.0380797 -3.0227611 -3.0073233 -3.0074644 -2.8915009][-3.4153872 -3.9158602 -4.2201 -4.1876988 -3.9302425 -3.4388182 -2.923862 -2.8945894 -3.295208 -3.6647258 -3.7618704 -3.706593 -3.6922388 -3.7065496 -3.6365085][-3.2933874 -3.7474341 -4.0733232 -4.1652937 -4.078876 -3.7996211 -3.4928246 -3.4444716 -3.627876 -3.7882352 -3.8316326 -3.7992225 -3.7896357 -3.8052988 -3.766614]]...]
INFO - root - 2017-12-16 09:00:10.043987: step 24910, loss = 0.52, batch loss = 0.26 (45.6 examples/sec; 0.175 sec/batch; 14h:59m:40s remains)
INFO - root - 2017-12-16 09:00:11.766303: step 24920, loss = 0.54, batch loss = 0.28 (46.9 examples/sec; 0.171 sec/batch; 14h:34m:40s remains)
INFO - root - 2017-12-16 09:00:13.470523: step 24930, loss = 0.53, batch loss = 0.27 (47.5 examples/sec; 0.168 sec/batch; 14h:23m:27s remains)
INFO - root - 2017-12-16 09:00:15.164585: step 24940, loss = 0.50, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 14h:17m:33s remains)
INFO - root - 2017-12-16 09:00:16.837322: step 24950, loss = 0.61, batch loss = 0.35 (47.4 examples/sec; 0.169 sec/batch; 14h:24m:17s remains)
INFO - root - 2017-12-16 09:00:18.542600: step 24960, loss = 0.56, batch loss = 0.30 (46.9 examples/sec; 0.171 sec/batch; 14h:34m:55s remains)
INFO - root - 2017-12-16 09:00:20.235943: step 24970, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 14h:47m:52s remains)
INFO - root - 2017-12-16 09:00:21.930060: step 24980, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 14h:06m:18s remains)
INFO - root - 2017-12-16 09:00:23.611688: step 24990, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 14h:20m:15s remains)
INFO - root - 2017-12-16 09:00:25.279701: step 25000, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:19m:29s remains)
2017-12-16 09:00:25.751784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.38095486 -0.77637136 -1.1786442 -1.5289087 -1.7519265 -1.8547273 -1.8855383 -2.0074778 -2.35866 -2.797385 -2.7195237 -1.7644016 -0.40621495 0.61969805 0.96613932][-0.42786872 -0.94764125 -1.3801512 -1.6419331 -1.7168791 -1.6892488 -1.6108845 -1.6219923 -1.9129659 -2.4057219 -2.4189551 -1.5169809 -0.087165356 1.0692735 1.5495641][-0.92208529 -1.4690121 -1.8081751 -1.873711 -1.7429575 -1.5945139 -1.4151525 -1.3042399 -1.5011069 -2.0018692 -2.1707368 -1.566141 -0.42966413 0.55390549 1.0460212][-1.5530252 -2.0792289 -2.2772219 -2.1199539 -1.8106039 -1.5676894 -1.3124571 -1.1290061 -1.2044182 -1.6113003 -1.8651452 -1.5913857 -0.98879993 -0.40904713 -0.004899025][-1.8820511 -2.3220243 -2.4580467 -2.2423158 -1.8822621 -1.5254838 -1.1398537 -0.87876678 -0.85354996 -1.0749352 -1.2500302 -1.2020175 -1.0862323 -0.94593227 -0.63945639][-1.7679112 -2.1552896 -2.2362669 -2.0412126 -1.7015928 -1.3216276 -0.898901 -0.61313343 -0.5244689 -0.55332458 -0.55189 -0.5377723 -0.67792439 -0.77769816 -0.47057581][-1.3844817 -1.8820683 -2.0228417 -1.8172047 -1.4345032 -0.99337947 -0.50834525 -0.29759026 -0.31307936 -0.28644466 -0.16503358 -0.1028738 -0.30218148 -0.4207505 0.043918133][-1.2154368 -1.8245144 -1.9579359 -1.6854978 -1.2666261 -0.87382293 -0.46731222 -0.34060383 -0.4373759 -0.48099232 -0.36974311 -0.30572057 -0.43657112 -0.41414273 0.27505207][-1.4165996 -2.1148629 -2.239193 -1.88277 -1.4269822 -1.1405869 -0.94681752 -0.9404093 -1.1310451 -1.2640634 -1.2332467 -1.2344079 -1.2991492 -1.0798227 -0.14171553][-1.9232074 -2.653187 -2.7851167 -2.3302038 -1.7483261 -1.4276872 -1.381974 -1.5834546 -1.9387475 -2.2422006 -2.3669164 -2.4488666 -2.5040462 -2.21226 -1.1810585][-2.1597753 -2.8964818 -3.044498 -2.49384 -1.710212 -1.2210288 -1.1457512 -1.4089577 -1.8752728 -2.3853362 -2.7883298 -3.096684 -3.2751336 -3.0527675 -2.1669486][-1.6564057 -2.4615562 -2.6896298 -2.1289878 -1.1583251 -0.38573062 -0.059688807 -0.23508453 -0.82054651 -1.6354694 -2.3967896 -2.98878 -3.3380737 -3.2847939 -2.6919162][-0.68617928 -1.5630465 -1.9127051 -1.4571861 -0.49888587 0.45071006 1.0241027 1.0036504 0.40948367 -0.56472552 -1.5915668 -2.3867583 -2.889601 -3.0347247 -2.7839689][0.060559034 -0.83454919 -1.282855 -1.0060894 -0.2184782 0.63716078 1.2006156 1.2067356 0.65278244 -0.26846218 -1.2625979 -2.0508509 -2.5780687 -2.8334682 -2.8483357][0.060849667 -0.74929035 -1.2505249 -1.2008327 -0.72248673 -0.11003995 0.31020141 0.28913689 -0.14143682 -0.82135 -1.5195885 -2.0671647 -2.4431567 -2.6604462 -2.776104]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:00:28.091332: step 25010, loss = 0.55, batch loss = 0.29 (47.0 examples/sec; 0.170 sec/batch; 14h:32m:44s remains)
INFO - root - 2017-12-16 09:00:29.798828: step 25020, loss = 0.56, batch loss = 0.31 (48.6 examples/sec; 0.165 sec/batch; 14h:03m:06s remains)
INFO - root - 2017-12-16 09:00:31.501599: step 25030, loss = 0.48, batch loss = 0.22 (46.2 examples/sec; 0.173 sec/batch; 14h:47m:23s remains)
INFO - root - 2017-12-16 09:00:33.194456: step 25040, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 14h:26m:18s remains)
INFO - root - 2017-12-16 09:00:34.889812: step 25050, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 14h:19m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:00:36.589528: step 25060, loss = 0.54, batch loss = 0.28 (45.6 examples/sec; 0.176 sec/batch; 14h:59m:51s remains)
INFO - root - 2017-12-16 09:00:38.309145: step 25070, loss = 0.58, batch loss = 0.32 (46.0 examples/sec; 0.174 sec/batch; 14h:50m:47s remains)
INFO - root - 2017-12-16 09:00:40.019844: step 25080, loss = 0.54, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 14h:35m:23s remains)
INFO - root - 2017-12-16 09:00:41.712310: step 25090, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.169 sec/batch; 14h:27m:32s remains)
INFO - root - 2017-12-16 09:00:43.415586: step 25100, loss = 0.59, batch loss = 0.33 (46.9 examples/sec; 0.170 sec/batch; 14h:33m:18s remains)
2017-12-16 09:00:43.890859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1220393 -2.1580279 -2.1839058 -2.1917839 -2.1731279 -2.14059 -2.1000071 -2.0634148 -2.0298173 -2.000524 -1.9697257 -1.9326576 -1.8923248 -1.8556631 -1.8270934][-2.3096747 -2.3863535 -2.4468868 -2.4758513 -2.4599419 -2.4147732 -2.3637147 -2.3236976 -2.2825356 -2.2451329 -2.2056513 -2.1660476 -2.1259692 -2.0875041 -2.0524726][-2.2880843 -2.4125314 -2.5047626 -2.5275178 -2.4909306 -2.4201064 -2.3505549 -2.3136983 -2.29232 -2.2867656 -2.2678428 -2.2514579 -2.2422435 -2.2274225 -2.1922808][-2.1516562 -2.2887156 -2.3964515 -2.3897519 -2.3160384 -2.2255428 -2.1503313 -2.1144941 -2.13348 -2.1921539 -2.2270398 -2.2609704 -2.2958078 -2.3254666 -2.3036089][-1.970258 -2.1049764 -2.1843553 -2.1181746 -2.0276232 -1.9537008 -1.8798103 -1.825547 -1.8705783 -1.9955732 -2.1064138 -2.204845 -2.2662363 -2.2610097 -2.2053692][-1.7990136 -1.9013357 -1.9294353 -1.8232404 -1.7099195 -1.6478796 -1.5694249 -1.4837668 -1.5316139 -1.7030921 -1.8731508 -2.0134451 -2.0650046 -1.9257522 -1.7481774][-1.379968 -1.5120385 -1.5342603 -1.4617927 -1.3779647 -1.3106344 -1.2068626 -1.0978764 -1.1803098 -1.3979837 -1.6530551 -1.8728938 -1.8541392 -1.4872284 -1.0992808][-0.98664558 -1.2080909 -1.22292 -1.179196 -1.1244385 -1.0290648 -0.85175955 -0.68520033 -0.74975181 -1.0493242 -1.4478927 -1.8444362 -1.8616168 -1.3377419 -0.73249674][-0.59742427 -0.88453782 -0.97342384 -0.96579468 -0.90473783 -0.82716227 -0.60356176 -0.37137842 -0.36903358 -0.71808612 -1.2952234 -1.9562544 -2.1960466 -1.7141032 -0.9894619][-0.51902449 -0.84742546 -0.93734908 -0.83334887 -0.7061094 -0.64532638 -0.48321187 -0.24874949 -0.23312187 -0.59432197 -1.3162701 -2.1879733 -2.6785583 -2.4124224 -1.7649524][-0.73413026 -1.0524402 -1.0347903 -0.76640987 -0.56640911 -0.59859538 -0.55201137 -0.36727786 -0.31224823 -0.64155221 -1.3821232 -2.300113 -2.8887625 -2.8790863 -2.42297][-0.76045454 -1.0111142 -0.89460623 -0.57748568 -0.4770478 -0.72041619 -0.83771253 -0.65449822 -0.48243797 -0.66105676 -1.2766862 -2.1007292 -2.6551793 -2.7581298 -2.5139062][-0.58513772 -0.73595095 -0.55305171 -0.29283714 -0.4199146 -0.92797685 -1.2490959 -1.1154613 -0.82903039 -0.80678737 -1.211354 -1.8116503 -2.21354 -2.3084445 -2.1931381][-0.41327548 -0.46046185 -0.21026587 0.005522728 -0.30483866 -1.0805984 -1.700778 -1.78878 -1.5255377 -1.3087168 -1.4357077 -1.7553968 -1.946556 -1.9567713 -1.8922879][-0.3952688 -0.34548831 0.0015332699 0.23476458 -0.169317 -1.1091006 -1.9974685 -2.4200382 -2.338383 -2.0684972 -1.9285845 -1.9431404 -1.8644657 -1.7031493 -1.6278328]]...]
INFO - root - 2017-12-16 09:00:45.551942: step 25110, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.169 sec/batch; 14h:23m:34s remains)
INFO - root - 2017-12-16 09:00:47.244204: step 25120, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 13h:59m:39s remains)
INFO - root - 2017-12-16 09:00:48.913132: step 25130, loss = 0.53, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:32m:15s remains)
INFO - root - 2017-12-16 09:00:50.587171: step 25140, loss = 0.56, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 14h:41m:43s remains)
INFO - root - 2017-12-16 09:00:52.249299: step 25150, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:55m:02s remains)
INFO - root - 2017-12-16 09:00:53.939893: step 25160, loss = 0.48, batch loss = 0.22 (48.8 examples/sec; 0.164 sec/batch; 13h:59m:34s remains)
INFO - root - 2017-12-16 09:00:55.607330: step 25170, loss = 0.48, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 14h:15m:28s remains)
INFO - root - 2017-12-16 09:00:57.284237: step 25180, loss = 0.56, batch loss = 0.30 (45.5 examples/sec; 0.176 sec/batch; 15h:00m:39s remains)
INFO - root - 2017-12-16 09:00:58.957987: step 25190, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 14h:16m:33s remains)
INFO - root - 2017-12-16 09:01:00.609682: step 25200, loss = 0.52, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 13h:37m:39s remains)
2017-12-16 09:01:01.115330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2438326 -2.8162074 -2.2975378 -1.6500236 -0.91174424 -0.49018288 -0.34674096 -0.76600337 -1.5426729 -2.3511081 -3.0518303 -3.3662336 -3.3783226 -3.33425 -3.3058453][-3.3494518 -2.9298513 -2.3828661 -1.6160847 -0.78069067 -0.19994879 0.0485487 -0.31028771 -1.1431899 -2.0628972 -3.0392118 -3.6167202 -3.7594638 -3.7452984 -3.7257757][-3.431778 -3.1099291 -2.5671442 -1.7003722 -0.7534833 -0.079061985 0.18230987 -0.10335851 -0.81207585 -1.7077377 -2.8241768 -3.6664634 -4.032557 -4.088336 -4.0233688][-3.6729116 -3.384438 -2.7508249 -1.7294692 -0.55869794 0.26365972 0.51874352 0.19015574 -0.54980624 -1.4761343 -2.5930107 -3.6261225 -4.1608663 -4.2660394 -4.1437736][-3.7700047 -3.5327959 -2.8709402 -1.670557 -0.14797091 0.91220951 1.2824178 0.91066384 -0.072431326 -1.2389274 -2.405761 -3.4690053 -4.0608516 -4.1305208 -3.9372792][-3.5973773 -3.5398598 -2.9221818 -1.6156118 0.10291219 1.4838407 2.1909897 1.7924178 0.4813478 -0.98492849 -2.2038493 -3.2119145 -3.7755704 -3.7907023 -3.5651193][-2.92164 -3.1177843 -2.7321768 -1.5252143 0.24960613 1.8993819 2.9137547 2.537411 0.97817659 -0.70109105 -2.0138609 -2.9173555 -3.3669853 -3.3356631 -3.0743721][-1.8644992 -2.2275136 -2.1355033 -1.2435405 0.34791088 2.0618007 3.2422473 2.9333856 1.3005452 -0.49287665 -1.8586419 -2.660759 -3.0033424 -2.8980181 -2.5801826][-1.1143818 -1.4946246 -1.5612617 -0.97691929 0.35130262 1.9028065 3.002754 2.8169181 1.3239195 -0.37785268 -1.6592395 -2.3634493 -2.5928364 -2.3918407 -2.0055852][-1.155957 -1.5305269 -1.5995784 -1.1391597 -0.020183086 1.2442091 2.1961563 2.2148907 1.0936842 -0.40737975 -1.5617418 -2.1598802 -2.2821705 -2.0229852 -1.6281525][-1.596364 -2.1059062 -2.2241898 -1.7889031 -0.79013169 0.32222486 1.1755893 1.4275396 0.77596354 -0.36186957 -1.341205 -1.8791332 -1.9706864 -1.7236135 -1.3855515][-1.7755072 -2.4059484 -2.6222141 -2.3105223 -1.4870708 -0.48914778 0.347744 0.79674172 0.57679152 -0.15945148 -0.93173325 -1.3910697 -1.4862845 -1.3139288 -1.0699654][-1.5405563 -2.1770453 -2.509084 -2.394341 -1.8183119 -1.0027541 -0.23178029 0.25171447 0.18587446 -0.26872849 -0.80458188 -1.1559173 -1.2159094 -1.0636163 -0.86572385][-1.1959577 -1.6851006 -2.0546646 -2.1432076 -1.8720264 -1.316635 -0.7476089 -0.43203819 -0.57513094 -0.97338283 -1.3789436 -1.5960665 -1.5998164 -1.4484463 -1.2398098][-0.99574697 -1.3118544 -1.6494191 -1.8783567 -1.8647904 -1.5964103 -1.2857866 -1.1904862 -1.4515748 -1.8951442 -2.2531602 -2.3919532 -2.3610919 -2.2480974 -2.0547142]]...]
INFO - root - 2017-12-16 09:01:02.806549: step 25210, loss = 0.60, batch loss = 0.34 (46.9 examples/sec; 0.171 sec/batch; 14h:33m:37s remains)
INFO - root - 2017-12-16 09:01:04.522278: step 25220, loss = 0.76, batch loss = 0.50 (46.1 examples/sec; 0.174 sec/batch; 14h:49m:16s remains)
INFO - root - 2017-12-16 09:01:06.208246: step 25230, loss = 0.68, batch loss = 0.42 (47.9 examples/sec; 0.167 sec/batch; 14h:15m:34s remains)
INFO - root - 2017-12-16 09:01:07.906360: step 25240, loss = 0.55, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 14h:26m:05s remains)
INFO - root - 2017-12-16 09:01:09.629118: step 25250, loss = 0.49, batch loss = 0.23 (43.6 examples/sec; 0.183 sec/batch; 15h:38m:43s remains)
INFO - root - 2017-12-16 09:01:11.356692: step 25260, loss = 0.61, batch loss = 0.35 (47.1 examples/sec; 0.170 sec/batch; 14h:29m:44s remains)
INFO - root - 2017-12-16 09:01:13.040483: step 25270, loss = 0.60, batch loss = 0.34 (49.5 examples/sec; 0.162 sec/batch; 13h:48m:11s remains)
INFO - root - 2017-12-16 09:01:14.714491: step 25280, loss = 0.49, batch loss = 0.23 (45.8 examples/sec; 0.175 sec/batch; 14h:55m:03s remains)
INFO - root - 2017-12-16 09:01:16.443221: step 25290, loss = 0.54, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 14h:20m:42s remains)
INFO - root - 2017-12-16 09:01:18.142857: step 25300, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:09m:10s remains)
2017-12-16 09:01:18.603193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1527643 -3.1728468 -3.1972103 -3.1214252 -2.7730513 -2.2336652 -1.7785032 -1.7603993 -2.0195234 -2.4052262 -2.7924957 -3.0301075 -3.189986 -3.1674953 -3.0811188][-3.2860377 -3.4400358 -3.5456138 -3.3784637 -2.8778048 -2.2807255 -1.9528668 -2.0614874 -2.3991547 -2.8393075 -3.2552752 -3.512347 -3.7224178 -3.786561 -3.641551][-3.2584791 -3.4471471 -3.5672143 -3.336303 -2.5972703 -1.8083107 -1.357502 -1.4423209 -1.9174163 -2.521349 -3.1022398 -3.4866612 -3.7897797 -3.9436228 -3.8038392][-3.13684 -3.3314147 -3.376266 -2.9859571 -1.9758768 -0.8231858 -0.0036218166 0.037161827 -0.61154032 -1.483462 -2.2922342 -2.8872926 -3.3595614 -3.5976019 -3.5400991][-3.0981069 -3.2850206 -3.1821034 -2.5449319 -1.2306952 0.5137558 1.8828025 2.0730934 1.1767879 -0.01539588 -1.0532458 -1.903424 -2.5684085 -2.9532685 -2.9821339][-2.9877262 -3.1209197 -2.8681359 -1.995945 -0.39758873 1.8627644 3.83295 4.1299353 2.9377322 1.45609 0.24677587 -0.764568 -1.6275978 -2.1857133 -2.2992854][-2.9134164 -2.8778343 -2.442539 -1.4474685 0.23345709 2.7013807 5.1464481 5.7221022 4.2966504 2.5714273 1.2384491 0.099399567 -0.8596251 -1.5090864 -1.702867][-2.9221888 -2.6484482 -2.1232021 -1.1872526 0.3197515 2.547379 4.923543 5.68948 4.3950882 2.6569676 1.2432084 0.0309937 -0.95628178 -1.5972612 -1.7661278][-3.0149837 -2.5314231 -1.8848847 -1.0427537 0.085177422 1.706037 3.4415488 4.1239872 3.2043867 1.7585845 0.51599526 -0.62210858 -1.5466651 -2.1699207 -2.3083811][-3.0813465 -2.3855891 -1.6054912 -0.8427614 -0.020951509 1.0696647 2.1956511 2.5435486 1.8777094 0.75045061 -0.20594144 -1.1539755 -1.9990714 -2.5174456 -2.6175017][-3.0137808 -2.1704123 -1.2951957 -0.6612525 -0.033200979 0.85890555 1.5933843 1.6811881 1.0336318 0.056790113 -0.79621267 -1.5932989 -2.3087962 -2.7128763 -2.7348061][-2.6425595 -1.7312629 -0.75772166 -0.20609927 0.23346496 0.99004841 1.5391448 1.4355803 0.69299555 -0.35613394 -1.2028534 -1.9358501 -2.522774 -2.8198643 -2.7648602][-1.9889814 -1.1332133 -0.13469458 0.28785563 0.53086782 1.0346618 1.3283057 1.0808346 0.33120251 -0.58236313 -1.2581146 -1.8249874 -2.3064005 -2.5547159 -2.5325861][-1.3018088 -0.70389092 0.087826967 0.38805389 0.4431138 0.5507412 0.55838442 0.24433017 -0.30770111 -0.81008506 -1.0915619 -1.3545635 -1.6891639 -2.0212309 -2.1804442][-0.79412222 -0.57275963 -0.062916279 0.19103241 0.073965549 -0.20865846 -0.48979294 -0.73158944 -0.91879916 -0.92381847 -0.75319004 -0.6901871 -0.90293944 -1.2919447 -1.6254715]]...]
INFO - root - 2017-12-16 09:01:20.284874: step 25310, loss = 0.58, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 14h:30m:53s remains)
INFO - root - 2017-12-16 09:01:21.966176: step 25320, loss = 0.49, batch loss = 0.23 (46.5 examples/sec; 0.172 sec/batch; 14h:40m:24s remains)
INFO - root - 2017-12-16 09:01:23.649238: step 25330, loss = 0.55, batch loss = 0.29 (43.4 examples/sec; 0.185 sec/batch; 15h:44m:42s remains)
INFO - root - 2017-12-16 09:01:25.323203: step 25340, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 14h:01m:29s remains)
INFO - root - 2017-12-16 09:01:26.997281: step 25350, loss = 0.52, batch loss = 0.26 (46.7 examples/sec; 0.171 sec/batch; 14h:37m:33s remains)
INFO - root - 2017-12-16 09:01:28.689756: step 25360, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.169 sec/batch; 14h:22m:51s remains)
INFO - root - 2017-12-16 09:01:30.381957: step 25370, loss = 0.48, batch loss = 0.22 (49.7 examples/sec; 0.161 sec/batch; 13h:43m:13s remains)
INFO - root - 2017-12-16 09:01:32.051408: step 25380, loss = 0.55, batch loss = 0.29 (51.0 examples/sec; 0.157 sec/batch; 13h:23m:27s remains)
INFO - root - 2017-12-16 09:01:33.715693: step 25390, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:15m:33s remains)
INFO - root - 2017-12-16 09:01:35.401286: step 25400, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.164 sec/batch; 14h:01m:45s remains)
2017-12-16 09:01:35.852962: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3342404 1.2198215 0.65732837 0.16477466 -0.023465157 -0.22927022 -0.71385264 -1.2556037 -1.6306562 -1.9275435 -2.2012417 -2.5593145 -3.1234479 -3.7087407 -4.1625776][0.39731097 0.35102558 -0.043053627 -0.35443354 -0.44094181 -0.6308285 -1.1836987 -1.9442852 -2.5893865 -3.0962713 -3.4718213 -3.7714663 -4.0878477 -4.3170748 -4.4103403][-1.1695139 -1.244513 -1.5328393 -1.7031668 -1.6936226 -1.801476 -2.2363667 -2.9114494 -3.4933147 -3.9235015 -4.1769381 -4.2904229 -4.3138895 -4.2424 -4.0545869][-2.7343268 -2.8274462 -3.0578027 -3.1592975 -3.0807624 -3.0028729 -3.0858119 -3.3071396 -3.5132666 -3.6499405 -3.6978636 -3.6835136 -3.5910242 -3.4318151 -3.1748724][-3.6970475 -3.7905478 -3.9329872 -3.9715769 -3.7745905 -3.3947995 -2.9441533 -2.5453148 -2.2382336 -2.084204 -2.1089489 -2.2328568 -2.2912936 -2.2458978 -2.0648239][-3.6572437 -3.6904376 -3.7417278 -3.6998973 -3.3697352 -2.6492171 -1.6158397 -0.65789366 -0.051973104 0.074795485 -0.23438072 -0.757774 -1.1608628 -1.3107052 -1.2016797][-2.7579734 -2.7067363 -2.6224539 -2.4891553 -2.007761 -0.97945249 0.49455905 1.7724178 2.2126515 1.8230951 0.92646337 -0.064000368 -0.76468992 -1.0510931 -0.97885072][-1.660172 -1.5513586 -1.2695581 -0.94793367 -0.33248425 0.83118916 2.3830698 3.5217798 3.3489702 2.2025793 0.72872972 -0.53345108 -1.2625568 -1.4615902 -1.3321069][-1.1856349 -0.99493515 -0.55908763 -0.07410574 0.55284142 1.4822149 2.5791 3.142482 2.4462082 1.0023766 -0.51170254 -1.6625937 -2.2246337 -2.2509282 -1.9856799][-1.1543864 -0.8787446 -0.51455593 -0.19877338 0.13052988 0.54778504 0.96930242 0.99434876 0.32232451 -0.76751828 -1.838042 -2.610815 -2.9545329 -2.8669398 -2.51715][-1.0679498 -0.71226585 -0.59221327 -0.68883359 -0.79453874 -0.80995739 -0.72897553 -0.77333045 -1.1435989 -1.7324588 -2.309478 -2.7089303 -2.8932915 -2.76137 -2.448705][-0.84972441 -0.45647895 -0.60810316 -1.068526 -1.4373264 -1.5128722 -1.3448911 -1.1333174 -1.0794628 -1.2826469 -1.6571538 -2.0159013 -2.180444 -2.07566 -1.884335][-0.78545642 -0.42940915 -0.65464103 -1.1361036 -1.3642273 -1.1613472 -0.63559568 -0.036486626 0.39015818 0.37762213 -0.14839935 -0.77871287 -1.1775907 -1.2419552 -1.260689][-1.0555189 -0.83818853 -0.980147 -1.1361816 -0.919786 -0.28514647 0.51743841 1.2441537 1.8074982 1.9019201 1.2715695 0.37124872 -0.27717781 -0.54627681 -0.73348939][-1.6704097 -1.6285377 -1.586549 -1.3032621 -0.60684812 0.32480288 1.1449645 1.7075493 2.0820296 2.1468499 1.62836 0.770015 0.053530931 -0.30955148 -0.46190822]]...]
INFO - root - 2017-12-16 09:01:37.520792: step 25410, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 14h:10m:34s remains)
INFO - root - 2017-12-16 09:01:39.187290: step 25420, loss = 0.54, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:31m:29s remains)
INFO - root - 2017-12-16 09:01:40.846391: step 25430, loss = 0.60, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 14h:10m:07s remains)
INFO - root - 2017-12-16 09:01:42.505992: step 25440, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:16m:10s remains)
INFO - root - 2017-12-16 09:01:44.196669: step 25450, loss = 0.56, batch loss = 0.30 (47.8 examples/sec; 0.167 sec/batch; 14h:16m:44s remains)
INFO - root - 2017-12-16 09:01:45.901092: step 25460, loss = 0.48, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:10m:35s remains)
INFO - root - 2017-12-16 09:01:47.578141: step 25470, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 14h:11m:13s remains)
INFO - root - 2017-12-16 09:01:49.274708: step 25480, loss = 0.59, batch loss = 0.33 (46.2 examples/sec; 0.173 sec/batch; 14h:45m:06s remains)
INFO - root - 2017-12-16 09:01:50.977703: step 25490, loss = 0.64, batch loss = 0.39 (48.2 examples/sec; 0.166 sec/batch; 14h:08m:47s remains)
INFO - root - 2017-12-16 09:01:52.671742: step 25500, loss = 0.56, batch loss = 0.30 (46.1 examples/sec; 0.173 sec/batch; 14h:47m:31s remains)
2017-12-16 09:01:53.210845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2925026 -2.4499424 -2.6787066 -2.9751129 -3.2432485 -3.438448 -3.5147364 -3.398587 -3.0007222 -2.5261872 -2.1500926 -2.0313535 -2.1311908 -2.3127956 -2.4397917][-1.3823688 -1.6848557 -2.20668 -2.8568268 -3.4071484 -3.6992245 -3.7422533 -3.4868898 -2.8504646 -2.0968554 -1.5971366 -1.5517118 -1.8441412 -2.1958849 -2.4449425][0.061921597 -0.4321599 -1.3100661 -2.3904176 -3.3028879 -3.752728 -3.7837839 -3.3948069 -2.529207 -1.5160569 -0.85924339 -0.86963475 -1.3628436 -1.9228019 -2.3295798][1.4076447 0.81083655 -0.27055955 -1.6239592 -2.7088761 -3.2186236 -3.2414498 -2.8069124 -1.9432819 -0.89542842 -0.19418478 -0.25478959 -0.89735556 -1.6273613 -2.1582088][1.79227 1.3247836 0.46234965 -0.67220259 -1.497807 -1.7530603 -1.5834035 -1.1472834 -0.66279685 -0.089170456 0.29205632 0.01442337 -0.75027931 -1.5546746 -2.1270492][0.76140094 0.58738136 0.36685705 -0.01658082 -0.089517832 0.39233017 1.1301622 1.6109078 1.4568145 1.1420257 0.85190678 0.12971401 -0.82046282 -1.682605 -2.233434][-0.58699906 -0.52346611 -0.14195657 0.35668683 1.1519961 2.5076067 4.0848169 4.7411633 3.8605964 2.6322129 1.5656714 0.28865123 -0.96983421 -1.9054803 -2.3295805][-1.3105817 -1.2145125 -0.71276367 0.16105795 1.4791574 3.3081377 5.3635168 6.369873 5.2982969 3.6563561 2.171469 0.48838568 -1.092031 -2.1533046 -2.465004][-1.5051578 -1.5413949 -1.28498 -0.64480114 0.52843285 2.0556986 3.6492398 4.6179495 4.1104259 2.8697536 1.5677261 0.074953794 -1.3198771 -2.3062346 -2.5288062][-1.8023833 -2.0701091 -2.1325281 -1.8413692 -1.0655469 -0.12044764 0.79761434 1.4914966 1.4026077 0.7128427 -0.1775732 -1.0975929 -1.9205496 -2.4890771 -2.5101848][-2.5253253 -2.960865 -3.2424631 -3.2319379 -2.8866789 -2.4489992 -1.9858096 -1.5180931 -1.3852178 -1.7264216 -2.2073393 -2.6030338 -2.8531575 -2.9278545 -2.6540923][-3.469811 -3.9272082 -4.2882504 -4.5094423 -4.556 -4.5281591 -4.3325868 -3.9798183 -3.7678905 -3.8419108 -3.9686942 -4.0142012 -3.8540049 -3.4911256 -2.9440818][-4.1831412 -4.5731382 -4.9187756 -5.2951097 -5.5509453 -5.6938953 -5.6686354 -5.5058575 -5.3204956 -5.2475548 -5.1398492 -4.9300365 -4.5005116 -3.8518519 -3.1277566][-4.224555 -4.4905949 -4.7388864 -5.042891 -5.3137627 -5.469111 -5.4959259 -5.4443359 -5.3503108 -5.2609854 -5.072999 -4.7773581 -4.3160381 -3.658814 -2.9617314][-3.5738668 -3.7303061 -3.8517697 -4.0059 -4.1698723 -4.2617803 -4.2723079 -4.248085 -4.2168326 -4.16131 -3.9850354 -3.7340188 -3.4125564 -2.9952569 -2.537056]]...]
INFO - root - 2017-12-16 09:01:54.876759: step 25510, loss = 0.57, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 14h:01m:45s remains)
INFO - root - 2017-12-16 09:01:56.585726: step 25520, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 14h:16m:49s remains)
INFO - root - 2017-12-16 09:01:58.279448: step 25530, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 14h:18m:26s remains)
INFO - root - 2017-12-16 09:01:59.929031: step 25540, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.166 sec/batch; 14h:11m:46s remains)
INFO - root - 2017-12-16 09:02:01.626187: step 25550, loss = 0.64, batch loss = 0.38 (44.8 examples/sec; 0.179 sec/batch; 15h:13m:42s remains)
INFO - root - 2017-12-16 09:02:03.325540: step 25560, loss = 0.55, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 14h:40m:07s remains)
INFO - root - 2017-12-16 09:02:04.997368: step 25570, loss = 0.59, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 14h:14m:07s remains)
INFO - root - 2017-12-16 09:02:06.684892: step 25580, loss = 0.57, batch loss = 0.31 (46.5 examples/sec; 0.172 sec/batch; 14h:39m:18s remains)
INFO - root - 2017-12-16 09:02:08.372051: step 25590, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 14h:13m:38s remains)
INFO - root - 2017-12-16 09:02:10.036396: step 25600, loss = 0.50, batch loss = 0.24 (47.3 examples/sec; 0.169 sec/batch; 14h:25m:30s remains)
2017-12-16 09:02:10.512156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4119086 -2.5994997 -2.8686271 -3.1168396 -3.1862631 -3.0476329 -2.7155547 -2.284909 -1.9514761 -1.7949558 -1.9395539 -2.1826038 -2.1726098 -1.8621609 -1.2780674][-2.26908 -2.4183636 -2.7362852 -3.1112614 -3.3750339 -3.4454107 -3.2393317 -2.7613325 -2.2043781 -1.7976351 -1.767078 -2.0066168 -2.1797307 -2.1022186 -1.6535881][-1.4089522 -1.6145587 -2.0554054 -2.6148016 -3.1022367 -3.3869729 -3.2626679 -2.7380421 -2.029161 -1.470144 -1.3134983 -1.4752212 -1.7916611 -1.9223909 -1.5773277][-0.15033984 -0.43936491 -0.9926877 -1.7131753 -2.313009 -2.6880283 -2.6640477 -2.217658 -1.5637546 -1.0540192 -0.87660527 -0.96050644 -1.2404993 -1.3846734 -1.0418024][1.0672586 0.74205446 0.14148903 -0.55639172 -1.083635 -1.3742113 -1.2938027 -0.95122266 -0.59933734 -0.40486753 -0.38927877 -0.44366837 -0.55859768 -0.57530808 -0.20547605][1.3538287 1.108129 0.68518519 0.33790231 0.21929669 0.32951427 0.62208676 0.84576607 0.72393966 0.36222291 0.035464525 -0.064509392 -0.017068148 0.15408134 0.501075][0.63032246 0.57414985 0.521605 0.762553 1.2986832 1.9669249 2.5891502 2.6500094 1.949517 0.97022581 0.19010472 -0.14933205 -0.10858226 0.15749073 0.46434641][-0.574183 -0.48657811 -0.085902929 0.73231292 1.87902 3.0905893 4.1505928 4.0638733 2.7506068 1.3457179 0.25713348 -0.35715413 -0.42504883 -0.15978122 0.092712164][-2.0103533 -1.786154 -1.0948415 0.05594945 1.4553881 2.80455 3.8785589 3.7273576 2.3991187 1.0255919 -0.061689615 -0.72760332 -0.82882988 -0.59940517 -0.41027415][-3.0935128 -2.7837276 -2.0039537 -0.82579005 0.46588707 1.5452609 2.1847765 1.9701321 1.0391238 0.0014929771 -0.84237647 -1.3537229 -1.4219227 -1.243893 -1.1325814][-3.4834337 -3.1823604 -2.478512 -1.4914284 -0.51265228 0.20192432 0.49522781 0.2461307 -0.39704823 -1.0429999 -1.5218284 -1.8278587 -1.8257565 -1.6415597 -1.5604006][-3.2248728 -3.0456433 -2.5835583 -1.9415088 -1.3288609 -0.91339767 -0.80157793 -1.0027806 -1.4000487 -1.7051563 -1.8469952 -1.9185804 -1.8166256 -1.6220185 -1.573822][-2.6457405 -2.6400023 -2.4529498 -2.1240187 -1.7963049 -1.5894098 -1.5765502 -1.7380617 -1.956921 -2.0570061 -1.9958013 -1.9129182 -1.7711312 -1.6425583 -1.6541368][-2.2251318 -2.4098952 -2.4377966 -2.2976773 -2.1121285 -2.0239866 -2.0817542 -2.229816 -2.3445568 -2.340354 -2.2063549 -2.0698209 -1.9488453 -1.8783908 -1.9286278][-2.2779815 -2.5552509 -2.7120886 -2.6840582 -2.5524046 -2.4989123 -2.5788369 -2.7168088 -2.8047769 -2.7654421 -2.627656 -2.497503 -2.4088955 -2.38502 -2.4478106]]...]
INFO - root - 2017-12-16 09:02:12.244299: step 25610, loss = 0.57, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 14h:29m:06s remains)
INFO - root - 2017-12-16 09:02:13.933346: step 25620, loss = 0.56, batch loss = 0.30 (46.3 examples/sec; 0.173 sec/batch; 14h:44m:16s remains)
INFO - root - 2017-12-16 09:02:15.648355: step 25630, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 14h:18m:00s remains)
INFO - root - 2017-12-16 09:02:17.338295: step 25640, loss = 0.66, batch loss = 0.40 (48.1 examples/sec; 0.166 sec/batch; 14h:10m:19s remains)
INFO - root - 2017-12-16 09:02:19.013472: step 25650, loss = 0.49, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 14h:24m:54s remains)
INFO - root - 2017-12-16 09:02:20.688584: step 25660, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:03m:52s remains)
INFO - root - 2017-12-16 09:02:22.354144: step 25670, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:57m:38s remains)
INFO - root - 2017-12-16 09:02:24.045179: step 25680, loss = 0.47, batch loss = 0.22 (46.8 examples/sec; 0.171 sec/batch; 14h:34m:04s remains)
INFO - root - 2017-12-16 09:02:25.717639: step 25690, loss = 0.54, batch loss = 0.28 (49.3 examples/sec; 0.162 sec/batch; 13h:49m:50s remains)
INFO - root - 2017-12-16 09:02:27.399699: step 25700, loss = 0.55, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 14h:20m:15s remains)
2017-12-16 09:02:27.878401: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11163378 0.030560732 -0.340693 -0.70240235 -0.97964287 -1.2293923 -1.4433525 -1.5707018 -1.6331623 -1.6023088 -1.413739 -1.2192869 -1.3682797 -1.9716916 -2.6846824][-1.0990548 -1.2255141 -1.4970906 -1.7013237 -1.8556659 -2.0649686 -2.2662413 -2.3275361 -2.2606175 -2.0652041 -1.7432439 -1.4346099 -1.3955648 -1.7645104 -2.3383329][-2.2987347 -2.3753281 -2.5272989 -2.622618 -2.7036028 -2.8856986 -3.0488765 -3.0042543 -2.7946224 -2.426316 -1.9590569 -1.5043929 -1.3174154 -1.5102738 -1.9137414][-2.8889155 -2.7194178 -2.6206961 -2.5974622 -2.660615 -2.8106775 -2.9176114 -2.8053553 -2.5855203 -2.258466 -1.8296365 -1.3694322 -1.1299487 -1.2532356 -1.5448899][-2.7393565 -2.1778762 -1.6996734 -1.5045016 -1.5297854 -1.6122527 -1.6283145 -1.5827661 -1.6745086 -1.7228531 -1.5814183 -1.271191 -1.0333078 -1.1006345 -1.3327159][-1.932229 -0.95435023 -0.058973789 0.31115818 0.30462813 0.2785573 0.33270025 0.28849411 -0.17809558 -0.82022357 -1.2061344 -1.2247901 -1.1204579 -1.1520337 -1.3144538][-0.84113109 0.31627464 1.439362 1.9429386 1.9552805 2.00692 2.2386105 2.2295353 1.4101532 0.1323812 -0.85979509 -1.2855239 -1.4104991 -1.4576685 -1.509913][-0.042192221 0.89511919 1.8625968 2.4414084 2.6209376 2.8928535 3.4020836 3.5150039 2.4211347 0.68203926 -0.69921684 -1.437006 -1.7250509 -1.7880597 -1.7744209][0.18561172 0.56450486 1.0761359 1.5604072 1.845752 2.2106507 2.8202875 3.0180523 2.0837877 0.50767732 -0.8095094 -1.547334 -1.8299657 -1.8968762 -1.886351][0.010697842 -0.24647641 -0.25516462 -0.11029053 0.052043676 0.42405915 1.050277 1.3742726 0.83220911 -0.26931167 -1.2037971 -1.6182793 -1.7284484 -1.8132749 -1.8330312][-0.36750007 -1.2010869 -1.6358365 -1.8150215 -1.7848241 -1.4490128 -0.8288039 -0.35563993 -0.54058218 -1.1669724 -1.568399 -1.5698073 -1.4622593 -1.5568336 -1.6276503][-0.89215076 -1.9885751 -2.5991061 -2.9523139 -3.0243142 -2.7254281 -2.1802025 -1.6675535 -1.5998666 -1.8225048 -1.7783519 -1.436484 -1.1764141 -1.2898018 -1.4245303][-1.3741624 -2.3754547 -2.8734066 -3.209784 -3.3199031 -3.0316339 -2.5471272 -2.1200535 -2.0238383 -2.0396087 -1.7757977 -1.3021533 -1.0256362 -1.1859189 -1.4134881][-1.7876945 -2.4236028 -2.6356375 -2.7759433 -2.7519596 -2.4460819 -2.0469496 -1.8087411 -1.8113906 -1.8414042 -1.6099756 -1.2066255 -1.030539 -1.2788818 -1.6170542][-2.0297532 -2.2922423 -2.2317779 -2.1205254 -1.8903128 -1.5240722 -1.2052999 -1.1426003 -1.2981377 -1.450774 -1.3535879 -1.1139964 -1.0725156 -1.4214579 -1.8746006]]...]
INFO - root - 2017-12-16 09:02:29.535284: step 25710, loss = 0.48, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 13h:58m:20s remains)
INFO - root - 2017-12-16 09:02:31.212121: step 25720, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:52m:36s remains)
INFO - root - 2017-12-16 09:02:32.900927: step 25730, loss = 0.65, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 14h:13m:33s remains)
INFO - root - 2017-12-16 09:02:34.543453: step 25740, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 14h:04m:12s remains)
INFO - root - 2017-12-16 09:02:36.265871: step 25750, loss = 0.44, batch loss = 0.18 (45.9 examples/sec; 0.174 sec/batch; 14h:51m:43s remains)
INFO - root - 2017-12-16 09:02:37.959956: step 25760, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 14h:07m:16s remains)
INFO - root - 2017-12-16 09:02:39.696981: step 25770, loss = 0.70, batch loss = 0.44 (46.7 examples/sec; 0.171 sec/batch; 14h:35m:53s remains)
INFO - root - 2017-12-16 09:02:41.359754: step 25780, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:15m:00s remains)
INFO - root - 2017-12-16 09:02:43.041815: step 25790, loss = 0.62, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 14h:20m:29s remains)
INFO - root - 2017-12-16 09:02:44.737091: step 25800, loss = 0.51, batch loss = 0.25 (46.9 examples/sec; 0.171 sec/batch; 14h:32m:38s remains)
2017-12-16 09:02:45.233142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12346792 -0.032792091 -0.084095716 -0.1852169 -0.21626687 -0.03385973 0.40640473 0.8967998 1.1623499 1.2282934 1.3905318 1.6157017 1.5118401 0.91079354 0.034482956][-0.98579979 -1.0053436 -1.1002495 -1.2175684 -1.2993814 -1.1745986 -0.79778326 -0.35539913 -0.040769577 0.15833282 0.45288825 0.76388621 0.70556116 0.096616268 -0.80791712][-2.0883565 -2.2520015 -2.3744705 -2.5121024 -2.6376331 -2.6002734 -2.3813384 -2.1320353 -1.90408 -1.6436398 -1.2459399 -0.83669865 -0.74671388 -1.1683395 -1.8815114][-3.0942755 -3.2678094 -3.30968 -3.2928991 -3.2601984 -3.1771257 -3.0890303 -3.1019242 -3.1040123 -2.9724455 -2.6228802 -2.2346835 -2.0657947 -2.257616 -2.69285][-3.3727565 -3.4367812 -3.2494624 -2.8704383 -2.4525895 -2.1944077 -2.2373872 -2.5492067 -2.8881588 -3.0457973 -2.9067233 -2.668107 -2.5303788 -2.5869586 -2.7959251][-2.6252344 -2.561739 -2.04094 -1.1487904 -0.19340324 0.30224991 0.068053007 -0.6761291 -1.436762 -1.9622127 -2.1139076 -2.0741081 -2.0334682 -2.0772347 -2.2293262][-0.9556421 -0.79659688 0.0029344559 1.3871133 2.8245213 3.4820249 2.9020336 1.6099679 0.40262032 -0.45434892 -0.85043705 -0.963176 -1.0267733 -1.2013291 -1.4882506][0.93995523 1.1377685 2.0356162 3.6140363 5.2303476 5.7916622 4.7027416 2.9217746 1.3616383 0.31302357 -0.21070027 -0.39096498 -0.52863514 -0.80057359 -1.1903212][2.0222237 2.1779239 2.8250911 4.04891 5.2121229 5.2837753 4.0379038 2.2397735 0.67450619 -0.32139397 -0.78024852 -0.89831257 -1.0039238 -1.2733139 -1.5907338][1.526026 1.5383253 1.8140352 2.433295 2.9580233 2.6696107 1.5388808 0.056761026 -1.2106982 -1.9283292 -2.1384845 -2.0631766 -1.9996579 -2.1079679 -2.251956][-0.20349836 -0.35084367 -0.3797127 -0.17080045 0.0024414062 -0.32581496 -1.1805723 -2.2309875 -3.0477648 -3.3984404 -3.3115184 -3.0143394 -2.7643225 -2.674927 -2.6521902][-2.0202565 -2.2432096 -2.4475818 -2.4841225 -2.4849391 -2.6949537 -3.1870658 -3.7621412 -4.1319203 -4.1186461 -3.7861383 -3.3443704 -2.971055 -2.7875943 -2.7078156][-3.15171 -3.3650146 -3.5827107 -3.6949775 -3.7083645 -3.7686882 -3.9394984 -4.119813 -4.1573005 -3.9314637 -3.5224152 -3.0823212 -2.7341828 -2.5982997 -2.5915601][-3.2548275 -3.3909571 -3.5350637 -3.6311805 -3.6352093 -3.6008155 -3.5831227 -3.565609 -3.4451976 -3.1912041 -2.8674877 -2.5738828 -2.3928576 -2.4136274 -2.5617557][-2.6216757 -2.7090821 -2.8157418 -2.8876588 -2.8841403 -2.8161535 -2.732614 -2.6627512 -2.5740311 -2.4391866 -2.2891846 -2.1962395 -2.2189527 -2.4060023 -2.6756144]]...]
INFO - root - 2017-12-16 09:02:46.875285: step 25810, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:11m:59s remains)
INFO - root - 2017-12-16 09:02:48.531313: step 25820, loss = 0.47, batch loss = 0.21 (47.0 examples/sec; 0.170 sec/batch; 14h:30m:49s remains)
INFO - root - 2017-12-16 09:02:50.232021: step 25830, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 14h:22m:59s remains)
INFO - root - 2017-12-16 09:02:51.921437: step 25840, loss = 0.60, batch loss = 0.34 (47.9 examples/sec; 0.167 sec/batch; 14h:14m:12s remains)
INFO - root - 2017-12-16 09:02:53.633133: step 25850, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 13h:55m:47s remains)
INFO - root - 2017-12-16 09:02:55.319943: step 25860, loss = 0.55, batch loss = 0.29 (46.3 examples/sec; 0.173 sec/batch; 14h:43m:35s remains)
INFO - root - 2017-12-16 09:02:57.026637: step 25870, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:10m:40s remains)
INFO - root - 2017-12-16 09:02:58.733344: step 25880, loss = 0.62, batch loss = 0.37 (47.1 examples/sec; 0.170 sec/batch; 14h:27m:54s remains)
INFO - root - 2017-12-16 09:03:00.445122: step 25890, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 13h:59m:06s remains)
INFO - root - 2017-12-16 09:03:02.108648: step 25900, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 14h:01m:21s remains)
2017-12-16 09:03:02.606686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0559297 -2.1499257 -2.3948367 -2.8084311 -3.2806256 -3.6695511 -3.84966 -3.6994634 -3.3442655 -3.1292379 -3.2049351 -3.442965 -3.6121531 -3.5745416 -3.4874268][-2.1237307 -2.293196 -2.6457663 -3.1945362 -3.8270144 -4.3541303 -4.5401535 -4.2196383 -3.5135741 -3.0660796 -3.136518 -3.3830328 -3.540349 -3.4880223 -3.4060068][-2.1675165 -2.4014251 -2.8251252 -3.4301257 -4.0987926 -4.6780496 -4.8075247 -4.1570582 -2.9209516 -2.0367026 -1.9759034 -2.2410402 -2.503408 -2.5620837 -2.6082013][-2.1311741 -2.365495 -2.7989967 -3.3640032 -3.9511523 -4.4020491 -4.3995647 -3.4375968 -1.6825538 -0.35959148 -0.15433526 -0.57017183 -1.0856868 -1.4058446 -1.6270435][-2.0367014 -2.2326798 -2.6402266 -3.1326334 -3.5757241 -3.849133 -3.698267 -2.5046785 -0.46796608 1.0783033 1.2982991 0.68930411 -0.20392728 -0.91652668 -1.2884579][-1.9624485 -2.1088014 -2.4737322 -2.9099698 -3.187305 -3.309643 -3.1190605 -1.9374398 0.055817842 1.5079765 1.5820346 0.72878146 -0.45206118 -1.3605124 -1.7237887][-1.9508356 -2.0764658 -2.4014816 -2.7725627 -2.9391365 -2.9783826 -2.8477559 -1.8876853 -0.23396969 0.93482614 0.71216917 -0.31484771 -1.4505169 -2.1675012 -2.3686044][-1.9708266 -2.1250877 -2.4671125 -2.8386872 -3.005486 -3.0872638 -3.0653856 -2.3589876 -0.979756 -0.10015583 -0.6202985 -1.6550817 -2.5603893 -2.9718421 -2.8034945][-1.9638252 -2.1379914 -2.5272789 -2.9511743 -3.1809773 -3.4090168 -3.5464234 -2.9791267 -1.7778504 -1.0782744 -1.656314 -2.5190232 -3.1304047 -3.1891677 -2.764643][-1.9381499 -2.1230812 -2.5470548 -2.990262 -3.2724781 -3.5412679 -3.7041659 -3.1741476 -2.0972757 -1.5284622 -1.9548362 -2.518441 -2.8171065 -2.7000282 -2.1653116][-1.917845 -2.1159637 -2.5481753 -3.0138872 -3.3421521 -3.590307 -3.6498065 -3.0201762 -2.0131228 -1.4491527 -1.5508361 -1.7432468 -1.9028925 -1.8325864 -1.3215083][-1.9008372 -2.101238 -2.5435755 -3.0398769 -3.4469137 -3.7008367 -3.5744219 -2.7963898 -1.7947567 -1.1517482 -0.873711 -0.75409639 -0.84334052 -0.89721406 -0.5982455][-1.9290466 -2.1079857 -2.4938667 -2.9933939 -3.4450259 -3.6358008 -3.3277626 -2.4801311 -1.5473503 -0.9199816 -0.43828094 -0.10176468 -0.16517663 -0.346426 -0.32762074][-2.0029221 -2.155998 -2.4550841 -2.8865032 -3.3188586 -3.4671481 -3.1333153 -2.4221289 -1.7167039 -1.1628621 -0.61308026 -0.20576596 -0.24706197 -0.49028265 -0.61637342][-2.0694265 -2.1990366 -2.4575903 -2.8286328 -3.2220209 -3.3916788 -3.232856 -2.837976 -2.4018326 -1.960024 -1.4300036 -1.0749384 -1.1622221 -1.428093 -1.6169549]]...]
INFO - root - 2017-12-16 09:03:04.315014: step 25910, loss = 0.51, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 14h:30m:27s remains)
INFO - root - 2017-12-16 09:03:06.020014: step 25920, loss = 0.66, batch loss = 0.40 (46.8 examples/sec; 0.171 sec/batch; 14h:33m:54s remains)
INFO - root - 2017-12-16 09:03:07.697959: step 25930, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:10m:57s remains)
INFO - root - 2017-12-16 09:03:09.369914: step 25940, loss = 0.50, batch loss = 0.24 (49.6 examples/sec; 0.161 sec/batch; 13h:44m:30s remains)
INFO - root - 2017-12-16 09:03:11.028993: step 25950, loss = 0.53, batch loss = 0.27 (46.1 examples/sec; 0.174 sec/batch; 14h:47m:27s remains)
INFO - root - 2017-12-16 09:03:12.722532: step 25960, loss = 0.50, batch loss = 0.24 (45.1 examples/sec; 0.178 sec/batch; 15h:07m:09s remains)
INFO - root - 2017-12-16 09:03:14.381746: step 25970, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:13m:46s remains)
INFO - root - 2017-12-16 09:03:16.051427: step 25980, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 14h:07m:37s remains)
INFO - root - 2017-12-16 09:03:17.722103: step 25990, loss = 0.62, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 14h:04m:03s remains)
INFO - root - 2017-12-16 09:03:19.380203: step 26000, loss = 0.49, batch loss = 0.23 (50.1 examples/sec; 0.160 sec/batch; 13h:35m:26s remains)
2017-12-16 09:03:19.861800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9033185 -2.4510026 -2.7158866 -2.6939967 -2.5347042 -2.3955324 -2.3467412 -2.2678981 -2.184305 -2.1724141 -2.1747904 -2.1507356 -2.1114454 -2.2296596 -2.454987][-2.3648283 -2.758652 -2.9512262 -2.9252703 -2.8050117 -2.6884778 -2.6540437 -2.4993262 -2.3028529 -2.2495408 -2.2881565 -2.2157154 -2.0948994 -2.1737568 -2.4095936][-2.27897 -2.5165877 -2.6941884 -2.728322 -2.6267779 -2.5408657 -2.5710773 -2.4532692 -2.3388596 -2.4286685 -2.57534 -2.491101 -2.2917347 -2.2418795 -2.3461297][-1.7767091 -1.8895812 -2.066474 -2.1096203 -1.9462142 -1.7997203 -1.9012384 -1.9491987 -2.1019752 -2.5348239 -2.9614267 -2.9700985 -2.7720141 -2.6235526 -2.528306][-1.2306745 -1.2189473 -1.3666586 -1.3490623 -1.0076704 -0.71939552 -0.84016883 -1.0394248 -1.4592295 -2.2835307 -3.1077433 -3.3801565 -3.3313136 -3.1896219 -2.9681187][-0.7948128 -0.75714159 -0.87025452 -0.71756315 -0.098482609 0.46887708 0.41920161 0.12194228 -0.52040815 -1.6609471 -2.7864864 -3.3220675 -3.4791307 -3.4435434 -3.2187281][-0.81296265 -0.78829288 -0.84973145 -0.49814606 0.45896816 1.3479528 1.4669685 1.1995091 0.45953107 -0.7980417 -2.0190547 -2.6992514 -2.976047 -3.0076618 -2.8119693][-1.3223873 -1.3320861 -1.2862908 -0.78843391 0.34980631 1.451793 1.8032815 1.6646745 0.94960093 -0.16931653 -1.2249089 -1.8513085 -2.0975423 -2.08012 -1.8299015][-1.9635699 -1.9690558 -1.8593191 -1.2998248 -0.22897863 0.84137869 1.3833132 1.3061182 0.6818552 -0.11996317 -0.81178689 -1.1677972 -1.1646303 -0.95273423 -0.53397429][-2.2264674 -2.2151272 -2.071336 -1.5323014 -0.74319863 0.030632734 0.4421804 0.28687048 -0.2210238 -0.66557133 -1.0129192 -1.0502455 -0.69451666 -0.18602276 0.47928023][-1.9493823 -1.981418 -1.8065491 -1.3511771 -0.970176 -0.71289015 -0.684989 -0.98617637 -1.4432356 -1.6097662 -1.6414423 -1.3794316 -0.72111726 0.064206123 0.94164443][-1.3167801 -1.3383369 -1.1000259 -0.68022978 -0.63822174 -0.94370556 -1.4188795 -2.0699623 -2.6214535 -2.6621456 -2.4971809 -2.0747437 -1.2634215 -0.29387379 0.70338368][-0.64239013 -0.62436688 -0.21434045 0.29812789 0.13689184 -0.6268568 -1.6518133 -2.7440426 -3.4894309 -3.5826454 -3.4162419 -2.9917123 -2.2029612 -1.2629998 -0.39060473][-0.16510272 -0.077395678 0.504858 1.1628525 0.93499947 -0.086652756 -1.4424751 -2.859535 -3.8619184 -4.1317625 -4.0983667 -3.7839594 -3.1712039 -2.4522722 -1.8355136][0.13882589 0.2987411 0.96751046 1.6886117 1.4374559 0.39014578 -0.99211526 -2.5401673 -3.7429852 -4.2266321 -4.3591719 -4.2213974 -3.858007 -3.4271188 -3.0966053]]...]
INFO - root - 2017-12-16 09:03:21.553468: step 26010, loss = 0.64, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 14h:08m:01s remains)
INFO - root - 2017-12-16 09:03:23.259544: step 26020, loss = 0.51, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 14h:19m:56s remains)
INFO - root - 2017-12-16 09:03:24.964862: step 26030, loss = 0.63, batch loss = 0.37 (48.7 examples/sec; 0.164 sec/batch; 13h:59m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:03:26.616365: step 26040, loss = 0.53, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 14h:21m:44s remains)
INFO - root - 2017-12-16 09:03:28.286704: step 26050, loss = 0.61, batch loss = 0.36 (48.7 examples/sec; 0.164 sec/batch; 13h:58m:28s remains)
INFO - root - 2017-12-16 09:03:29.974144: step 26060, loss = 0.57, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 13h:53m:32s remains)
INFO - root - 2017-12-16 09:03:31.649193: step 26070, loss = 0.62, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 14h:09m:41s remains)
INFO - root - 2017-12-16 09:03:33.297520: step 26080, loss = 0.55, batch loss = 0.29 (50.0 examples/sec; 0.160 sec/batch; 13h:37m:07s remains)
INFO - root - 2017-12-16 09:03:34.971543: step 26090, loss = 0.58, batch loss = 0.33 (47.5 examples/sec; 0.168 sec/batch; 14h:19m:17s remains)
INFO - root - 2017-12-16 09:03:36.662768: step 26100, loss = 0.62, batch loss = 0.36 (46.3 examples/sec; 0.173 sec/batch; 14h:42m:26s remains)
2017-12-16 09:03:37.136245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8742995 -3.8716235 -3.8726075 -3.8861184 -3.9358983 -4.0450749 -3.8769412 -3.4883811 -3.1057668 -2.7722495 -1.9912598 -1.0722774 -0.68689287 -1.1996429 -1.9551365][-3.6606104 -3.577219 -3.4894376 -3.4510994 -3.4335923 -3.4998546 -3.3465142 -3.0030422 -2.7125919 -2.4546189 -1.7190199 -0.80418932 -0.35253072 -0.7987268 -1.5555716][-3.2543271 -3.2015235 -3.0941 -2.9902365 -2.8578103 -2.8800819 -2.8075721 -2.6065423 -2.4208632 -2.2398298 -1.6303403 -0.8595736 -0.44976413 -0.86299229 -1.5685711][-2.7393622 -2.7728255 -2.6816022 -2.4494412 -2.0942996 -1.9501007 -1.8986347 -1.8804836 -1.8470006 -1.8116213 -1.4755402 -0.99477756 -0.75065053 -1.1957093 -1.8800595][-2.3992085 -2.4454777 -2.2582536 -1.8334868 -1.1895026 -0.74880362 -0.6045928 -0.71648645 -0.8263557 -1.0512447 -1.1329489 -1.0357426 -1.002232 -1.5193576 -2.1860721][-2.1006527 -2.0262382 -1.6766131 -1.0208046 -0.070317268 0.73471546 1.1055055 0.94688869 0.63138604 -0.0062890053 -0.54657459 -0.80854023 -0.97808695 -1.5833491 -2.2752573][-1.6842892 -1.3958249 -0.8462491 -0.0635345 1.0644605 2.1935897 2.8311539 2.6610699 2.062932 1.0015333 0.10585308 -0.37023044 -0.63793063 -1.3139069 -2.1303673][-1.516842 -0.9930315 -0.32252073 0.40300393 1.4231868 2.6083035 3.4461756 3.39615 2.6524243 1.4365163 0.52494884 0.088058233 -0.16690779 -0.9153049 -1.9121392][-1.8785219 -1.2202433 -0.52624345 -0.0093891621 0.623446 1.4723332 2.2214179 2.386538 1.8721385 1.0007205 0.55710888 0.44417 0.31159687 -0.50913239 -1.6659291][-2.4231899 -1.7682073 -1.1590164 -0.89783859 -0.7042141 -0.33457494 0.19692397 0.56541514 0.555259 0.3865416 0.66477013 0.98520184 0.91552448 0.0033686161 -1.2668695][-3.0083621 -2.4705448 -1.9481971 -1.8577569 -1.9486051 -1.9165435 -1.486818 -0.87120378 -0.34282613 0.13724113 1.003355 1.6409097 1.5765011 0.5772624 -0.73752618][-3.7271006 -3.4024382 -2.9511981 -2.9305711 -3.1525431 -3.2489896 -2.7679033 -1.8840116 -0.981488 -0.16881943 0.91090989 1.61378 1.524164 0.52923942 -0.66986191][-4.3551116 -4.2101345 -3.818171 -3.8532083 -4.1250162 -4.2422056 -3.6744943 -2.648355 -1.6683581 -0.88461864 0.030750513 0.59616184 0.41382289 -0.44108093 -1.3237858][-4.6190534 -4.5390191 -4.2477117 -4.2694464 -4.5199027 -4.6242294 -4.0966086 -3.1796741 -2.41638 -1.8924028 -1.3020405 -0.96019816 -1.2171758 -1.8474355 -2.3633223][-4.5757208 -4.5008373 -4.2667732 -4.2246037 -4.3909197 -4.4877367 -4.1197824 -3.5266902 -3.1720998 -3.0359914 -2.7952 -2.6578088 -2.8863025 -3.2397413 -3.3681307]]...]
INFO - root - 2017-12-16 09:03:38.792021: step 26110, loss = 0.51, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 14h:00m:07s remains)
INFO - root - 2017-12-16 09:03:40.486518: step 26120, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 14h:01m:35s remains)
INFO - root - 2017-12-16 09:03:42.140370: step 26130, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 13h:59m:02s remains)
INFO - root - 2017-12-16 09:03:43.840120: step 26140, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 14h:39m:50s remains)
INFO - root - 2017-12-16 09:03:45.542543: step 26150, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 14h:30m:38s remains)
INFO - root - 2017-12-16 09:03:47.224290: step 26160, loss = 0.66, batch loss = 0.40 (48.0 examples/sec; 0.167 sec/batch; 14h:11m:15s remains)
INFO - root - 2017-12-16 09:03:48.922537: step 26170, loss = 0.65, batch loss = 0.39 (46.5 examples/sec; 0.172 sec/batch; 14h:38m:19s remains)
INFO - root - 2017-12-16 09:03:50.594983: step 26180, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 14h:08m:40s remains)
INFO - root - 2017-12-16 09:03:52.244966: step 26190, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.163 sec/batch; 13h:54m:39s remains)
INFO - root - 2017-12-16 09:03:53.910003: step 26200, loss = 0.50, batch loss = 0.24 (49.6 examples/sec; 0.161 sec/batch; 13h:44m:12s remains)
2017-12-16 09:03:54.346655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5298862 -2.64118 -2.7515514 -2.8450019 -2.8821971 -2.9379601 -3.101501 -3.3352401 -3.49614 -3.5058289 -3.3549302 -2.9697552 -2.4219046 -2.1313488 -2.1906164][-2.6189814 -2.8898778 -3.098084 -3.2576876 -3.3682492 -3.4641116 -3.5664327 -3.5944471 -3.4348798 -3.2051744 -2.9511456 -2.4592881 -1.8404649 -1.5332938 -1.5913014][-2.808989 -3.2206457 -3.5047379 -3.6940274 -3.7965834 -3.8493454 -3.7604842 -3.4575386 -2.9671807 -2.5701449 -2.2703533 -1.7242503 -1.1034346 -0.85982823 -0.91754031][-3.0149198 -3.5159421 -3.7651238 -3.8391209 -3.7933667 -3.669065 -3.280431 -2.6065261 -1.9566884 -1.6433539 -1.4518912 -0.94762874 -0.4461627 -0.29224396 -0.31548071][-2.8849807 -3.3259711 -3.3434641 -3.1429348 -2.8972085 -2.5392008 -1.7512894 -0.7944144 -0.34500813 -0.54900622 -0.64187193 -0.25934458 0.055751324 0.13050437 0.13617516][-2.2217894 -2.4445155 -2.1027277 -1.6162623 -1.2246776 -0.67967296 0.49757624 1.6848264 1.6472111 0.66355395 0.15576077 0.33440375 0.39574742 0.29269624 0.19881272][-1.3349322 -1.3082857 -0.66307652 0.067474127 0.55656028 1.2478688 2.8748274 4.3876595 3.7219872 1.8935647 0.91480756 0.79539275 0.50527239 0.15334296 -0.16255903][-0.83309078 -0.56335759 0.34909558 1.2021348 1.6297755 2.3920646 4.3345966 6.251318 5.0149245 2.6346135 1.3426611 0.80208111 0.20953369 -0.20656419 -0.54995167][-0.87923646 -0.49773932 0.43313456 1.2176683 1.4078658 1.8857846 3.3255372 4.507381 3.5984807 1.8179979 0.77173424 0.12846899 -0.48545778 -0.79304862 -0.97577417][-1.2335829 -0.96709967 -0.31443071 0.22285938 0.24592042 0.37645268 1.1817105 1.7852259 1.1352127 -0.021645069 -0.71994758 -1.2069873 -1.5885491 -1.6616814 -1.6267493][-1.5852675 -1.5213809 -1.2773285 -1.0018517 -1.0638843 -1.1567785 -0.83531523 -0.5873009 -1.0240052 -1.7464288 -2.1740105 -2.4554322 -2.6016152 -2.4992812 -2.3369236][-1.7865856 -1.8358905 -1.8552859 -1.7916691 -1.9213544 -2.1266568 -2.1270843 -2.1442847 -2.455971 -2.8652337 -3.0843287 -3.1493211 -3.0742579 -2.8886061 -2.6732235][-1.8344474 -1.9361329 -2.076313 -2.1311665 -2.282887 -2.5547438 -2.7287357 -2.8680313 -3.0940735 -3.2762377 -3.3423648 -3.2827363 -3.1093688 -2.9175792 -2.7329121][-1.8083433 -1.914104 -2.0650964 -2.1815889 -2.3847923 -2.6870179 -2.93811 -3.0801384 -3.1753476 -3.1904721 -3.1456995 -3.0333939 -2.8796556 -2.7514427 -2.6642127][-1.8861963 -1.9400711 -2.0484309 -2.1613326 -2.3443539 -2.587827 -2.7753673 -2.8598547 -2.8490987 -2.7745097 -2.6780276 -2.5809262 -2.4992371 -2.4714699 -2.5043492]]...]
INFO - root - 2017-12-16 09:03:56.019039: step 26210, loss = 0.51, batch loss = 0.25 (49.3 examples/sec; 0.162 sec/batch; 13h:48m:18s remains)
INFO - root - 2017-12-16 09:03:57.669849: step 26220, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:01m:24s remains)
INFO - root - 2017-12-16 09:03:59.334469: step 26230, loss = 0.67, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 14h:12m:02s remains)
INFO - root - 2017-12-16 09:04:01.025643: step 26240, loss = 0.55, batch loss = 0.29 (49.8 examples/sec; 0.161 sec/batch; 13h:39m:39s remains)
INFO - root - 2017-12-16 09:04:02.696470: step 26250, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.169 sec/batch; 14h:25m:01s remains)
INFO - root - 2017-12-16 09:04:04.367247: step 26260, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:22m:48s remains)
INFO - root - 2017-12-16 09:04:06.050787: step 26270, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 14h:06m:26s remains)
INFO - root - 2017-12-16 09:04:07.717627: step 26280, loss = 0.55, batch loss = 0.29 (46.9 examples/sec; 0.171 sec/batch; 14h:30m:22s remains)
INFO - root - 2017-12-16 09:04:09.406773: step 26290, loss = 0.62, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 14h:14m:16s remains)
INFO - root - 2017-12-16 09:04:11.070805: step 26300, loss = 0.56, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 13h:57m:24s remains)
2017-12-16 09:04:11.546929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2559146 -1.3046451 -1.7410852 -2.3399575 -2.9910741 -3.560658 -3.9579892 -4.2133074 -4.2834835 -4.0421848 -3.344975 -2.2901595 -1.4363391 -1.343533 -1.652317][-1.9384252 -1.9558759 -2.4232972 -3.1593127 -3.8524098 -4.1597466 -4.21426 -4.3307786 -4.5808372 -4.6026735 -4.12852 -3.1943843 -2.2908258 -2.0294192 -2.2075844][-2.7007935 -2.6153882 -2.9950273 -3.7674246 -4.3157296 -4.0777164 -3.5395515 -3.4190586 -3.8732281 -4.3512793 -4.452765 -4.0285969 -3.3457861 -3.0259898 -2.9755626][-3.2403698 -3.0397477 -3.2694712 -3.9281998 -4.1936893 -3.4344363 -2.2854433 -1.8116965 -2.3301163 -3.2792082 -4.0238314 -4.22668 -3.9442685 -3.7198269 -3.6061187][-3.3990042 -3.1186514 -3.2018752 -3.6190786 -3.4700141 -2.1877625 -0.48666179 0.44821167 -0.17297983 -1.5720509 -3.0012546 -3.8297019 -3.9277244 -3.8994408 -3.8591037][-3.2436733 -2.8517435 -2.7272935 -2.7560372 -2.1478853 -0.38041723 1.99894 3.4328904 2.5723152 0.47976851 -1.6438518 -2.8924015 -3.2499838 -3.35163 -3.4408798][-2.7438951 -2.1877983 -1.7849697 -1.4581292 -0.58396173 1.4876642 4.4825311 6.4393768 5.1775041 2.2348328 -0.33734679 -1.7575662 -2.20132 -2.3242457 -2.5169163][-2.2305315 -1.4489394 -0.84220433 -0.38190532 0.56076694 2.5466828 5.6470966 7.8062868 6.326807 3.0673313 0.46888685 -0.86885619 -1.1976846 -1.3416854 -1.6585388][-2.1086049 -1.3065389 -0.72078371 -0.2320199 0.57082796 2.1789665 4.508081 5.874999 4.6956677 2.2086244 0.21954846 -0.71858668 -0.88270807 -1.0320587 -1.5144536][-2.5654633 -2.0262125 -1.5353398 -1.0742476 -0.38447523 0.67356372 1.9341063 2.4265332 1.4503157 -0.15724182 -1.3139074 -1.6991925 -1.6620091 -1.7661468 -2.2586899][-3.1953342 -2.9906354 -2.6881926 -2.2209523 -1.5668037 -1.0639969 -0.74922121 -0.89170969 -1.6622667 -2.5883229 -3.1002567 -3.0816128 -2.853261 -2.8729722 -3.2495441][-3.5373225 -3.6368651 -3.5457382 -3.1631708 -2.6903276 -2.5697589 -2.7965212 -3.2071567 -3.7683969 -4.2251391 -4.3386021 -4.1026897 -3.7996466 -3.6814909 -3.7309275][-3.4295413 -3.696137 -3.7175031 -3.5310841 -3.3130715 -3.4071465 -3.7788873 -4.1853466 -4.4932537 -4.5746012 -4.4819117 -4.2456279 -3.9593928 -3.7227683 -3.4802551][-2.8828142 -3.1725454 -3.2434797 -3.1874895 -3.1438828 -3.2961049 -3.5829983 -3.8383722 -3.9248533 -3.8655553 -3.7427988 -3.5757365 -3.3296361 -3.0502896 -2.7471242][-2.1791539 -2.3720927 -2.4176612 -2.4065735 -2.437098 -2.5425851 -2.6814532 -2.7665384 -2.7483284 -2.6631954 -2.5897508 -2.4983158 -2.3428633 -2.1640229 -1.9737033]]...]
INFO - root - 2017-12-16 09:04:13.225902: step 26310, loss = 0.51, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 14h:21m:53s remains)
INFO - root - 2017-12-16 09:04:14.890887: step 26320, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 14h:14m:08s remains)
INFO - root - 2017-12-16 09:04:16.583156: step 26330, loss = 0.56, batch loss = 0.30 (46.6 examples/sec; 0.172 sec/batch; 14h:36m:07s remains)
INFO - root - 2017-12-16 09:04:18.266822: step 26340, loss = 0.51, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 14h:28m:26s remains)
INFO - root - 2017-12-16 09:04:19.924661: step 26350, loss = 0.63, batch loss = 0.37 (48.3 examples/sec; 0.166 sec/batch; 14h:05m:23s remains)
INFO - root - 2017-12-16 09:04:21.632147: step 26360, loss = 0.57, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 14h:07m:33s remains)
INFO - root - 2017-12-16 09:04:23.321798: step 26370, loss = 0.65, batch loss = 0.39 (47.1 examples/sec; 0.170 sec/batch; 14h:26m:17s remains)
INFO - root - 2017-12-16 09:04:25.016688: step 26380, loss = 0.61, batch loss = 0.35 (48.6 examples/sec; 0.165 sec/batch; 14h:00m:10s remains)
INFO - root - 2017-12-16 09:04:26.669040: step 26390, loss = 0.61, batch loss = 0.35 (48.8 examples/sec; 0.164 sec/batch; 13h:56m:20s remains)
INFO - root - 2017-12-16 09:04:28.358469: step 26400, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:50m:32s remains)
2017-12-16 09:04:28.844513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.54213059 -0.50376058 -0.47853577 -0.46249294 -0.46051097 -0.47362673 -0.49540436 -0.52107763 -0.54137635 -0.41801786 -0.04704833 0.26573372 0.15308547 -0.25660634 -0.88878477][-0.55472195 -0.56573343 -0.58285105 -0.58100438 -0.56861389 -0.56493878 -0.56389487 -0.54494953 -0.53860819 -0.4196583 -0.058612347 0.23911381 0.13989973 -0.19713759 -0.70442176][-1.0066761 -1.1381302 -1.2643 -1.2962062 -1.2547339 -1.1951274 -1.1302433 -1.0181397 -0.95335042 -0.85671258 -0.54877639 -0.30597544 -0.40617764 -0.66590416 -1.0074134][-1.5560714 -1.8849534 -2.1858425 -2.277189 -2.1843693 -2.0339384 -1.8328832 -1.5766826 -1.4369645 -1.3826815 -1.1968373 -1.0915829 -1.2896217 -1.5810142 -1.8219292][-1.8554893 -2.3672276 -2.8230586 -2.9453335 -2.7716942 -2.4729283 -2.0646682 -1.5993047 -1.3270078 -1.3020962 -1.2992601 -1.4151516 -1.8517913 -2.3657546 -2.6913514][-1.7363281 -2.337913 -2.8806663 -3.0252068 -2.7866101 -2.3407838 -1.7161311 -1.0235218 -0.60786128 -0.619799 -0.78174126 -1.1055572 -1.8178045 -2.6540806 -3.1929674][-1.2542267 -1.8180327 -2.3504288 -2.4581633 -2.1591785 -1.6483672 -0.91445959 -0.12303424 0.33724308 0.26892972 -0.035270452 -0.52899587 -1.4439335 -2.5217369 -3.2678506][-0.96069884 -1.4346189 -1.8683848 -1.8763435 -1.4992237 -0.94334257 -0.19341326 0.61642909 1.1093051 1.0033572 0.58187366 0.011283636 -0.95440376 -2.1245399 -2.9910922][-1.2184625 -1.5608485 -1.8698335 -1.8085058 -1.4616323 -0.9745729 -0.33052063 0.37556529 0.79165483 0.67095518 0.28451562 -0.19323611 -1.021263 -2.0683885 -2.8461533][-1.9672554 -2.2024028 -2.4135661 -2.3646345 -2.1284194 -1.7761484 -1.2588413 -0.67502379 -0.34248734 -0.485983 -0.789379 -1.10521 -1.6461452 -2.33049 -2.8226032][-2.7809811 -2.9216926 -3.045845 -3.0036817 -2.8914165 -2.7037334 -2.341867 -1.8333101 -1.5255598 -1.6177819 -1.8144987 -1.9722362 -2.2149358 -2.5065157 -2.6501386][-3.0354636 -3.1506703 -3.227277 -3.1946044 -3.1983476 -3.1808577 -2.9748948 -2.5644386 -2.308341 -2.3458841 -2.4055617 -2.443037 -2.4968889 -2.5292339 -2.4490209][-2.8441491 -2.9541163 -3.0214639 -3.0238352 -3.139451 -3.2772589 -3.2430005 -2.9846401 -2.8372061 -2.8206854 -2.7430475 -2.6699252 -2.6502242 -2.5970044 -2.4229393][-2.6454077 -2.7367563 -2.7607241 -2.7539356 -2.8863544 -3.1104808 -3.2090325 -3.1013589 -3.0028682 -2.9106455 -2.755208 -2.6612558 -2.6540985 -2.6301785 -2.4751413][-2.7108071 -2.7426915 -2.632735 -2.5126917 -2.5568957 -2.7556663 -2.8944097 -2.8434744 -2.7197547 -2.5093036 -2.2462692 -2.096031 -2.1242909 -2.216893 -2.2177765]]...]
INFO - root - 2017-12-16 09:04:30.525001: step 26410, loss = 0.79, batch loss = 0.53 (48.1 examples/sec; 0.166 sec/batch; 14h:09m:00s remains)
INFO - root - 2017-12-16 09:04:32.228995: step 26420, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 14h:29m:05s remains)
INFO - root - 2017-12-16 09:04:33.883528: step 26430, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:09m:34s remains)
INFO - root - 2017-12-16 09:04:35.541499: step 26440, loss = 0.50, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 14h:04m:14s remains)
INFO - root - 2017-12-16 09:04:37.235282: step 26450, loss = 0.56, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 13h:58m:25s remains)
INFO - root - 2017-12-16 09:04:38.953032: step 26460, loss = 0.61, batch loss = 0.35 (47.6 examples/sec; 0.168 sec/batch; 14h:17m:02s remains)
INFO - root - 2017-12-16 09:04:40.650524: step 26470, loss = 0.60, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 14h:09m:24s remains)
INFO - root - 2017-12-16 09:04:42.312704: step 26480, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 14h:09m:12s remains)
INFO - root - 2017-12-16 09:04:43.969474: step 26490, loss = 0.48, batch loss = 0.22 (47.4 examples/sec; 0.169 sec/batch; 14h:21m:24s remains)
INFO - root - 2017-12-16 09:04:45.632596: step 26500, loss = 0.49, batch loss = 0.24 (50.4 examples/sec; 0.159 sec/batch; 13h:28m:43s remains)
2017-12-16 09:04:46.101884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9737232 -2.2272594 -2.4948704 -2.6777608 -2.7172551 -2.6263368 -2.4920559 -2.4006658 -2.3791935 -2.3265319 -2.229785 -2.1856768 -2.1877635 -2.2118919 -2.2557502][-2.2541623 -2.6923239 -3.1283252 -3.4021096 -3.4484639 -3.3457062 -3.1913276 -3.1019962 -3.1464021 -3.1673546 -3.0852976 -2.9959695 -2.9387321 -2.912549 -2.9091537][-2.5757141 -3.1276968 -3.5964289 -3.7910943 -3.7005877 -3.4850984 -3.3439963 -3.410356 -3.7231069 -3.9096928 -3.895875 -3.8084669 -3.6808767 -3.5579619 -3.4657812][-2.8956172 -3.3957405 -3.628922 -3.4394791 -2.8886347 -2.3768632 -2.2525322 -2.6510537 -3.4410102 -3.9956124 -4.2005591 -4.2459049 -4.1599855 -3.9665442 -3.7388277][-3.0906837 -3.3837943 -3.1492276 -2.3022816 -1.1090463 -0.14392352 0.028967381 -0.74255908 -2.07724 -3.1147523 -3.6743646 -3.9657793 -4.0109715 -3.7853038 -3.431602][-3.1606467 -3.1821809 -2.3793981 -0.83084393 1.0911825 2.6590819 2.91264 1.7730165 0.027350187 -1.3314993 -2.2189422 -2.768795 -2.960053 -2.7716932 -2.3984511][-3.0683951 -2.8840208 -1.6705217 0.44875789 3.0623016 5.2908092 5.7366505 4.28382 2.3024011 0.82452965 -0.32075214 -1.0497028 -1.346374 -1.2275423 -0.90310979][-2.8766429 -2.6530704 -1.4632237 0.73869538 3.4870524 6.0242643 7.0135374 5.7138672 3.9020972 2.5328436 1.2515314 0.38873839 -0.0344131 0.010039091 0.26192975][-2.6791265 -2.5886054 -1.7291534 -0.030925751 2.1488433 4.1817808 5.1621256 4.5605221 3.4477243 2.5671067 1.5557809 0.69114017 0.27606559 0.32819033 0.49078965][-2.6155121 -2.7847996 -2.4062994 -1.3379871 0.16227603 1.5445056 2.2530732 1.9790978 1.272773 0.75190973 0.1772511 -0.34843349 -0.56319749 -0.44129968 -0.23779726][-2.6687093 -3.1210289 -3.2365923 -2.748157 -1.7663229 -0.82666922 -0.40175021 -0.63038743 -1.2113845 -1.6644088 -1.9974798 -2.2402399 -2.195677 -1.9565903 -1.6689289][-2.8150265 -3.4657249 -3.8894923 -3.8314786 -3.3017278 -2.6727467 -2.4524345 -2.722512 -3.2774849 -3.7031906 -3.937624 -4.0119648 -3.8413317 -3.526937 -3.2132163][-2.8268292 -3.5071816 -4.0557814 -4.2086759 -3.9508166 -3.5744023 -3.5055382 -3.8430297 -4.3293648 -4.6492419 -4.7963986 -4.8132668 -4.6329947 -4.3622842 -4.0690651][-2.6414425 -3.1835907 -3.6695433 -3.8649845 -3.7400985 -3.561316 -3.59443 -3.8828235 -4.2358389 -4.4673605 -4.5434437 -4.5481262 -4.427371 -4.2000785 -3.9462481][-2.3560855 -2.7259531 -3.0925083 -3.2583289 -3.1974754 -3.1246107 -3.1897449 -3.391212 -3.5832877 -3.6840425 -3.7068305 -3.665705 -3.5574117 -3.3995988 -3.2338667]]...]
INFO - root - 2017-12-16 09:04:47.779670: step 26510, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.163 sec/batch; 13h:53m:42s remains)
INFO - root - 2017-12-16 09:04:49.449387: step 26520, loss = 0.57, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 14h:29m:57s remains)
INFO - root - 2017-12-16 09:04:51.096772: step 26530, loss = 0.61, batch loss = 0.35 (49.4 examples/sec; 0.162 sec/batch; 13h:46m:01s remains)
INFO - root - 2017-12-16 09:04:52.759931: step 26540, loss = 0.70, batch loss = 0.45 (47.5 examples/sec; 0.169 sec/batch; 14h:19m:23s remains)
INFO - root - 2017-12-16 09:04:54.420489: step 26550, loss = 0.49, batch loss = 0.23 (48.6 examples/sec; 0.165 sec/batch; 13h:59m:37s remains)
INFO - root - 2017-12-16 09:04:56.097587: step 26560, loss = 0.49, batch loss = 0.23 (47.5 examples/sec; 0.169 sec/batch; 14h:19m:27s remains)
INFO - root - 2017-12-16 09:04:57.745014: step 26570, loss = 0.57, batch loss = 0.32 (51.2 examples/sec; 0.156 sec/batch; 13h:16m:00s remains)
INFO - root - 2017-12-16 09:04:59.392406: step 26580, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 14h:03m:29s remains)
INFO - root - 2017-12-16 09:05:01.075613: step 26590, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 14h:02m:15s remains)
INFO - root - 2017-12-16 09:05:02.741321: step 26600, loss = 0.64, batch loss = 0.38 (47.6 examples/sec; 0.168 sec/batch; 14h:16m:27s remains)
2017-12-16 09:05:03.216700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7353647 -2.7541475 -2.6939554 -2.6802061 -2.8590708 -3.2157788 -3.5137019 -3.6344039 -3.7087529 -3.8682742 -3.9785609 -3.7417812 -3.092325 -2.3838308 -1.9037441][-2.7319639 -2.7166829 -2.56881 -2.3992562 -2.4052246 -2.6990333 -3.1074426 -3.4560819 -3.7325559 -4.032064 -4.2690878 -4.116014 -3.4420004 -2.6048839 -2.0248113][-2.5569575 -2.5487316 -2.3060274 -1.9108462 -1.6579598 -1.7523915 -2.2145362 -2.8223197 -3.3232183 -3.8275223 -4.2623854 -4.2972789 -3.7422214 -2.8941088 -2.2666085][-2.4515259 -2.5111132 -2.1609414 -1.5617502 -1.0245245 -0.75510347 -1.0667403 -1.8412166 -2.6001267 -3.3251073 -3.993736 -4.2886834 -3.9433174 -3.2063 -2.6119559][-2.4766517 -2.6536188 -2.2833073 -1.4903867 -0.605885 0.18705463 0.27532411 -0.4945122 -1.5183859 -2.5269461 -3.462863 -4.0175 -3.907155 -3.3148811 -2.749135][-2.5052991 -2.7601833 -2.3627815 -1.3977208 -0.10051847 1.2614827 1.8065605 1.1319499 -0.089291573 -1.3820751 -2.6098163 -3.4324412 -3.5905085 -3.1645927 -2.5903997][-2.2396526 -2.55078 -2.1061656 -1.038358 0.50958538 2.1927776 3.1556525 2.6239133 1.3446836 -0.20060706 -1.6560931 -2.6509304 -2.9953375 -2.7132463 -2.2016821][-1.58218 -1.9111379 -1.5411355 -0.54342723 0.95567393 2.5698838 3.4637794 3.0521016 1.8400655 0.28747463 -1.1774782 -2.1367831 -2.5354121 -2.3246331 -1.8998761][-0.84455764 -1.0975116 -0.92623556 -0.25925207 0.95354462 2.0293427 2.4081483 2.101737 1.1720195 -0.10520411 -1.3407446 -2.1375716 -2.4209764 -2.2629225 -1.9225674][-0.48584914 -0.71617138 -0.73537004 -0.40179324 0.39733553 0.90603304 0.89755225 0.63589454 -0.00075888634 -0.95988989 -1.8846712 -2.4339898 -2.564734 -2.4220567 -2.1481919][-0.82772434 -0.98232162 -1.0986512 -0.96087062 -0.49615407 -0.32299471 -0.46244335 -0.67780721 -1.0971446 -1.8049208 -2.4727836 -2.7701366 -2.7870998 -2.6523573 -2.4617295][-1.4684206 -1.5874529 -1.7247224 -1.6950057 -1.480165 -1.4488028 -1.6630683 -1.8768131 -2.1223803 -2.5521383 -2.9359713 -3.0586433 -2.974551 -2.8406684 -2.7026651][-2.182591 -2.2835965 -2.4056773 -2.4667473 -2.4340315 -2.4432006 -2.6014721 -2.7713962 -2.8668242 -3.0388718 -3.1919377 -3.1708031 -3.0295506 -2.8929079 -2.7789669][-2.6436658 -2.7465217 -2.8695827 -2.9387133 -2.9231937 -2.926255 -2.9881034 -3.0689304 -3.0758057 -3.0704157 -3.0651288 -2.9925089 -2.8683071 -2.7529874 -2.6542995][-2.7364349 -2.8119466 -2.8851624 -2.9153035 -2.8824346 -2.8381252 -2.8158789 -2.7882056 -2.7364559 -2.6820846 -2.6370666 -2.5790195 -2.4986324 -2.429497 -2.3798542]]...]
INFO - root - 2017-12-16 09:05:04.878749: step 26610, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:04m:38s remains)
INFO - root - 2017-12-16 09:05:06.570168: step 26620, loss = 0.51, batch loss = 0.26 (46.6 examples/sec; 0.172 sec/batch; 14h:35m:47s remains)
INFO - root - 2017-12-16 09:05:08.304444: step 26630, loss = 0.53, batch loss = 0.27 (46.8 examples/sec; 0.171 sec/batch; 14h:31m:33s remains)
INFO - root - 2017-12-16 09:05:09.974220: step 26640, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 14h:01m:03s remains)
INFO - root - 2017-12-16 09:05:11.651527: step 26650, loss = 0.50, batch loss = 0.25 (48.9 examples/sec; 0.163 sec/batch; 13h:53m:25s remains)
INFO - root - 2017-12-16 09:05:13.288649: step 26660, loss = 0.52, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 13h:46m:16s remains)
INFO - root - 2017-12-16 09:05:14.944860: step 26670, loss = 0.56, batch loss = 0.30 (49.0 examples/sec; 0.163 sec/batch; 13h:51m:43s remains)
INFO - root - 2017-12-16 09:05:16.608751: step 26680, loss = 0.46, batch loss = 0.20 (48.8 examples/sec; 0.164 sec/batch; 13h:55m:06s remains)
INFO - root - 2017-12-16 09:05:18.303977: step 26690, loss = 0.60, batch loss = 0.34 (46.9 examples/sec; 0.171 sec/batch; 14h:29m:24s remains)
INFO - root - 2017-12-16 09:05:19.963895: step 26700, loss = 0.69, batch loss = 0.44 (47.7 examples/sec; 0.168 sec/batch; 14h:15m:30s remains)
2017-12-16 09:05:20.447611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.21954 -0.68395436 -0.3077836 -0.16137481 -0.14816642 -0.24379039 -0.364228 -0.3959527 -0.23424935 0.07388854 0.27667975 0.44060946 0.77270341 1.0868828 1.2642415][-0.97145092 -0.28308487 0.17960382 0.30140734 0.10152006 -0.219872 -0.59155333 -0.85818028 -0.73250771 -0.34334016 -0.029205561 0.098730087 0.4136591 0.63259482 0.76197338][-0.76300609 -0.046803236 0.32844377 0.269881 -0.12052584 -0.61421084 -1.190985 -1.6190425 -1.5653205 -1.2567488 -1.1349372 -1.240518 -1.1592342 -1.0801823 -0.98365784][-0.74679744 -0.1157074 0.012820959 -0.15946269 -0.52167118 -1.008006 -1.6167583 -2.0516658 -2.1830544 -2.179225 -2.3481264 -2.7076244 -2.9058509 -2.8829126 -2.7751193][-0.70705354 -0.24469876 -0.19296861 -0.23552036 -0.27805138 -0.51249135 -0.99956393 -1.5056496 -1.9780886 -2.3958724 -2.8831537 -3.3452168 -3.669307 -3.7395225 -3.648169][-0.82224178 -0.58084726 -0.41515946 -0.10155797 0.37779021 0.68023729 0.40269947 -0.25405669 -1.0777118 -1.8302867 -2.5201154 -3.0828261 -3.4131274 -3.4881945 -3.4098692][-1.0319983 -0.9423331 -0.76744735 -0.22461319 0.69821072 1.4007592 1.3493659 0.69770026 -0.16983318 -0.99635053 -1.7590368 -2.2225342 -2.4382508 -2.4115252 -2.2094393][-1.1684672 -1.2317417 -1.0211198 -0.33859921 0.75230956 1.6081765 1.6433573 1.1722023 0.46611857 -0.24658775 -0.85060441 -1.1877941 -1.2965991 -1.163455 -0.77952182][-1.2776392 -1.310158 -1.0188395 -0.34729123 0.60891223 1.4304273 1.5556936 1.26582 0.81739426 0.27797222 -0.14017129 -0.446751 -0.54260683 -0.48718596 -0.21118498][-1.2604829 -1.1485823 -0.74261045 -0.074605227 0.74539375 1.4263835 1.4844728 1.149199 0.74520135 0.32206297 -0.021400928 -0.38373494 -0.5649581 -0.71024525 -0.71415854][-1.1840407 -0.97888088 -0.47321415 0.26055551 1.0444772 1.5764687 1.4202065 0.86916494 0.34962797 -0.10086226 -0.46203995 -0.83352339 -1.1114018 -1.4141984 -1.6276436][-1.1924808 -0.96341491 -0.44521379 0.26625824 0.8633554 1.0998166 0.7028296 0.06799531 -0.47226012 -0.89841521 -1.2667367 -1.6001234 -1.8883481 -2.1543736 -2.3921981][-1.3200202 -1.1510472 -0.79682708 -0.420447 -0.22066951 -0.33859038 -0.80782139 -1.3378533 -1.7123513 -1.9683472 -2.217248 -2.4386585 -2.6135037 -2.7456529 -2.8671589][-1.8676146 -1.7959845 -1.6600287 -1.5599811 -1.6443925 -1.9153935 -2.3021882 -2.657119 -2.8370788 -2.904902 -3.0148103 -3.121891 -3.1372349 -3.0841558 -2.981714][-2.5949132 -2.5629275 -2.5675843 -2.6456904 -2.8325522 -3.0690346 -3.3161471 -3.4671011 -3.4424596 -3.3410387 -3.2945976 -3.2505641 -3.1651311 -3.0212514 -2.8256063]]...]
INFO - root - 2017-12-16 09:05:22.165138: step 26710, loss = 0.48, batch loss = 0.22 (44.7 examples/sec; 0.179 sec/batch; 15h:12m:09s remains)
INFO - root - 2017-12-16 09:05:23.831554: step 26720, loss = 0.56, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 14h:08m:37s remains)
INFO - root - 2017-12-16 09:05:25.511510: step 26730, loss = 0.63, batch loss = 0.38 (47.8 examples/sec; 0.167 sec/batch; 14h:12m:36s remains)
INFO - root - 2017-12-16 09:05:27.157529: step 26740, loss = 0.51, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 14h:01m:22s remains)
INFO - root - 2017-12-16 09:05:28.821100: step 26750, loss = 0.59, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 14h:09m:54s remains)
INFO - root - 2017-12-16 09:05:30.525474: step 26760, loss = 0.55, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 14h:14m:16s remains)
INFO - root - 2017-12-16 09:05:32.207689: step 26770, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 14h:20m:00s remains)
INFO - root - 2017-12-16 09:05:33.915436: step 26780, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 14h:42m:14s remains)
INFO - root - 2017-12-16 09:05:35.594049: step 26790, loss = 0.70, batch loss = 0.45 (47.0 examples/sec; 0.170 sec/batch; 14h:26m:35s remains)
INFO - root - 2017-12-16 09:05:37.272562: step 26800, loss = 0.56, batch loss = 0.30 (50.0 examples/sec; 0.160 sec/batch; 13h:34m:28s remains)
2017-12-16 09:05:37.781672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8633487 -2.8711286 -2.7924311 -2.6390398 -2.58053 -2.91053 -3.3855126 -3.7423801 -3.87921 -3.7968531 -3.6404662 -3.4646654 -3.377722 -3.3787937 -3.3577125][-2.6526504 -2.7912424 -2.8257525 -2.7117717 -2.6632504 -3.0708752 -3.6201315 -4.065732 -4.3078 -4.3479457 -4.2967067 -4.1475334 -4.0006504 -3.9215074 -3.7995777][-2.2275198 -2.6016295 -2.7915044 -2.66865 -2.5466025 -2.9184647 -3.3771091 -3.7565699 -4.0581465 -4.2375221 -4.2947054 -4.1984253 -4.0495934 -3.9665494 -3.8376226][-1.9902195 -2.5735178 -2.8137648 -2.5895612 -2.3116319 -2.4132125 -2.52557 -2.7328405 -3.0201764 -3.3594673 -3.5950198 -3.6117187 -3.5291076 -3.4673395 -3.3999443][-2.1562085 -2.7469478 -2.8514392 -2.4599628 -1.9545951 -1.5492109 -1.1578414 -1.0777953 -1.4771461 -2.1100037 -2.60826 -2.7563949 -2.7670839 -2.7632406 -2.7605202][-2.4867957 -2.8879452 -2.7778485 -2.1442037 -1.2938635 -0.35190248 0.64376378 1.0016489 0.34104943 -0.75059485 -1.5771099 -1.9254335 -2.0646091 -2.1404638 -2.2253489][-2.7424724 -2.8852298 -2.6079407 -1.8455131 -0.77008855 0.54413223 2.0585673 2.6715662 1.6912172 0.33112311 -0.69422436 -1.2283379 -1.4935074 -1.6462408 -1.819458][-2.7223916 -2.6709404 -2.362293 -1.6998389 -0.73548985 0.45620894 1.9098871 2.5192344 1.6521857 0.47527456 -0.45730519 -0.95890582 -1.2395525 -1.462877 -1.6784921][-2.3597121 -2.1967735 -1.966451 -1.6559529 -1.167848 -0.47073591 0.46649361 0.93050647 0.51611495 -0.22313929 -0.785053 -1.1053338 -1.4169831 -1.7112699 -1.9061958][-1.9729935 -1.7911427 -1.6903532 -1.772117 -1.8792505 -1.7537271 -1.3102027 -0.9805721 -1.0725533 -1.3508279 -1.5305812 -1.6087608 -1.7917368 -2.0677733 -2.17201][-1.5167525 -1.3671696 -1.4426761 -1.9023649 -2.5149724 -2.8839874 -2.8848937 -2.7570448 -2.7345114 -2.7729442 -2.6430306 -2.3680253 -2.2330041 -2.3348954 -2.2677503][-1.0268898 -0.94792616 -1.2885323 -2.0349331 -2.8713591 -3.4845805 -3.7651451 -3.7939656 -3.7344396 -3.6609039 -3.3816686 -2.874207 -2.5019 -2.3757346 -2.1528428][-0.41226125 -0.40806937 -0.94506121 -1.7888906 -2.615685 -3.20318 -3.4866343 -3.5277922 -3.449317 -3.3762233 -3.10936 -2.6045177 -2.1594679 -1.9041114 -1.6498232][0.16897225 0.11571431 -0.47590613 -1.2203058 -1.8059077 -2.176863 -2.3733294 -2.4137256 -2.3801084 -2.3218265 -2.0775566 -1.6566353 -1.2361563 -0.96800041 -0.74957061][0.752336 0.65003538 0.1017797 -0.50836766 -0.85938895 -0.978948 -1.0665694 -1.1170526 -1.1585379 -1.1283206 -0.89989412 -0.56302369 -0.24168205 -0.056591034 0.020833731]]...]
INFO - root - 2017-12-16 09:05:39.444472: step 26810, loss = 0.48, batch loss = 0.22 (48.0 examples/sec; 0.167 sec/batch; 14h:08m:39s remains)
INFO - root - 2017-12-16 09:05:41.127062: step 26820, loss = 0.53, batch loss = 0.28 (45.4 examples/sec; 0.176 sec/batch; 14h:57m:53s remains)
INFO - root - 2017-12-16 09:05:42.793839: step 26830, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 13h:52m:20s remains)
INFO - root - 2017-12-16 09:05:44.475814: step 26840, loss = 0.59, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 14h:16m:47s remains)
INFO - root - 2017-12-16 09:05:46.153051: step 26850, loss = 0.50, batch loss = 0.24 (46.6 examples/sec; 0.172 sec/batch; 14h:35m:28s remains)
INFO - root - 2017-12-16 09:05:47.789743: step 26860, loss = 0.57, batch loss = 0.31 (50.9 examples/sec; 0.157 sec/batch; 13h:21m:03s remains)
INFO - root - 2017-12-16 09:05:49.440382: step 26870, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 14h:07m:44s remains)
INFO - root - 2017-12-16 09:05:51.119365: step 26880, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:14m:07s remains)
INFO - root - 2017-12-16 09:05:52.806899: step 26890, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 14h:11m:54s remains)
INFO - root - 2017-12-16 09:05:54.489066: step 26900, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 14h:21m:47s remains)
2017-12-16 09:05:54.978025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9569827 -2.1079478 -2.2941363 -2.3576629 -2.2668972 -2.1848743 -2.0921233 -1.8174021 -1.4277257 -1.1539644 -1.0908731 -1.1247137 -1.2915884 -1.5620112 -1.8164613][-1.2670101 -1.3094158 -1.46564 -1.5940478 -1.630882 -1.6913344 -1.7116902 -1.5311202 -1.1571182 -0.79566479 -0.59704983 -0.53671575 -0.69398439 -1.0424993 -1.4160175][-0.22959447 -0.21386504 -0.40832436 -0.67929125 -0.88192976 -1.0771598 -1.2194681 -1.2204781 -1.0555162 -0.82577825 -0.57431889 -0.36893535 -0.40516841 -0.67218757 -0.96694946][0.63645864 0.65724087 0.38579011 0.05457139 -0.16980815 -0.41085839 -0.60386384 -0.75594115 -0.91414964 -0.98675025 -0.87134957 -0.6345979 -0.53856885 -0.58047915 -0.63106167][0.90959382 0.9470973 0.66920447 0.46410036 0.41172004 0.34616828 0.23396039 0.017522335 -0.41974294 -0.88685524 -1.1216949 -1.0764754 -0.93262148 -0.76642466 -0.56320679][0.55198288 0.59004569 0.42867446 0.49969435 0.80340767 1.0796285 1.2437987 1.0917957 0.44957328 -0.38611972 -0.98332787 -1.206906 -1.1602459 -0.9885689 -0.807497][-0.18029213 -0.078345776 -0.096681118 0.26043749 0.9388392 1.6688821 2.3339903 2.5101573 1.6977613 0.53056741 -0.41666603 -0.99510014 -1.1803365 -1.1927807 -1.1840477][-1.1148143 -0.91578281 -0.75929666 -0.15723562 0.85479832 1.9929807 3.2418144 3.9414661 2.7901886 1.2877345 0.11173177 -0.68690169 -1.1009556 -1.3871642 -1.5935546][-2.0348289 -1.8165829 -1.5919647 -0.83248079 0.32862496 1.5382447 2.7667706 3.3099573 2.41873 1.1334877 0.040499926 -0.792521 -1.2691971 -1.6206759 -1.8950294][-2.5108459 -2.339292 -2.2109237 -1.5739257 -0.60878909 0.37116838 1.1914077 1.4320245 0.92742968 0.18301797 -0.50748885 -1.1485537 -1.5494972 -1.835866 -2.0303962][-2.3525295 -2.2787862 -2.3150508 -1.9454105 -1.370802 -0.86846817 -0.52860963 -0.44810987 -0.65129757 -0.90316772 -1.1403433 -1.4358044 -1.660825 -1.8387263 -1.9086016][-1.7082748 -1.694999 -1.8000996 -1.6539915 -1.5317369 -1.5566032 -1.6497012 -1.7277855 -1.7986944 -1.7573078 -1.6299266 -1.5683352 -1.5460784 -1.589509 -1.5791622][-0.97985327 -0.9399842 -1.0807425 -1.1227415 -1.3153821 -1.7409408 -2.1781321 -2.5093889 -2.595793 -2.3414886 -1.9426569 -1.5931886 -1.381887 -1.2738566 -1.2484487][-0.41724002 -0.36480021 -0.55404913 -0.70649135 -1.0555222 -1.7118161 -2.382798 -2.8326848 -2.8803892 -2.5271995 -1.9780549 -1.5074711 -1.200012 -1.0108712 -0.94243312][-0.12414598 -0.12896538 -0.36059427 -0.54566884 -0.85354471 -1.4934155 -2.1824098 -2.5642989 -2.576834 -2.2253082 -1.6790566 -1.2239605 -0.92250454 -0.74647415 -0.66707361]]...]
INFO - root - 2017-12-16 09:05:56.648822: step 26910, loss = 0.74, batch loss = 0.48 (49.6 examples/sec; 0.161 sec/batch; 13h:42m:09s remains)
INFO - root - 2017-12-16 09:05:58.289258: step 26920, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.164 sec/batch; 13h:57m:46s remains)
INFO - root - 2017-12-16 09:05:59.968576: step 26930, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:20m:07s remains)
INFO - root - 2017-12-16 09:06:01.639802: step 26940, loss = 0.49, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 14h:08m:00s remains)
INFO - root - 2017-12-16 09:06:03.298096: step 26950, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 13h:53m:58s remains)
INFO - root - 2017-12-16 09:06:04.934915: step 26960, loss = 0.55, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:14m:31s remains)
INFO - root - 2017-12-16 09:06:06.605412: step 26970, loss = 0.64, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 14h:04m:23s remains)
INFO - root - 2017-12-16 09:06:08.289756: step 26980, loss = 0.49, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:04m:24s remains)
INFO - root - 2017-12-16 09:06:09.993974: step 26990, loss = 0.55, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:13m:18s remains)
INFO - root - 2017-12-16 09:06:11.673440: step 27000, loss = 0.58, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:18m:36s remains)
2017-12-16 09:06:12.150998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2023029 -3.0574629 -2.9097123 -2.8664434 -2.5621815 -1.8544184 -1.2774763 -1.0729834 -1.4050105 -2.2127831 -3.2434921 -3.9402502 -3.9769287 -3.5805507 -3.2208414][-2.738415 -2.1682436 -1.8054738 -1.8222758 -1.698972 -1.0810732 -0.46629512 -0.3034358 -0.83993113 -1.9559312 -3.2139783 -3.9570112 -3.9181046 -3.4318287 -2.9477253][-1.7830186 -0.65112174 -0.09241724 -0.31798959 -0.63765419 -0.35099435 0.21140385 0.41812992 -0.19972372 -1.5385983 -2.9588256 -3.7774758 -3.7314348 -3.2089961 -2.5540605][-0.91157246 0.64180064 1.2832246 0.77618694 0.017441034 -0.11085606 0.30525708 0.62210441 0.10654974 -1.2408545 -2.6671219 -3.493484 -3.51124 -2.9981794 -2.2179949][-0.233711 1.3102093 1.7633548 1.031569 0.094764233 -0.15910673 0.25797009 0.72418594 0.39703774 -0.81718063 -2.2370005 -3.1383629 -3.2572596 -2.8357193 -2.0395558][0.083182573 1.3573179 1.5040329 0.67160296 -0.14676237 -0.14029574 0.59281993 1.3451669 1.1603787 -0.093285322 -1.6741612 -2.7598989 -3.0598316 -2.7397113 -2.0139575][-0.30862713 0.54562831 0.48029876 -0.25066614 -0.70237231 -0.1923821 1.0428414 2.1741505 2.1179719 0.66660976 -1.173198 -2.5050116 -2.9620037 -2.7392261 -2.1393049][-0.94375157 -0.48099649 -0.67938507 -1.2127327 -1.2422528 -0.31768322 1.1860003 2.4088273 2.3328772 0.84052873 -1.0094981 -2.385396 -2.9484124 -2.8493166 -2.4494226][-1.7096889 -1.4469862 -1.5011528 -1.6578617 -1.3476728 -0.29782391 1.1067972 2.0592709 1.7698364 0.33843517 -1.2880476 -2.4236243 -2.9290669 -2.9627624 -2.7995377][-2.6372228 -2.4057863 -2.1564975 -1.8394108 -1.2307885 -0.16866302 0.92373586 1.4950228 1.0742095 -0.25066185 -1.6277533 -2.4841056 -2.8725471 -2.9553146 -2.9269726][-3.2102175 -2.9080222 -2.3750095 -1.7749195 -1.1019435 -0.27476931 0.37676978 0.58801746 0.1287303 -0.97477973 -2.0163991 -2.5898123 -2.8173122 -2.8666334 -2.8267214][-3.2877612 -2.833883 -2.1245809 -1.4887491 -0.9609133 -0.48415709 -0.21422338 -0.22128487 -0.61116838 -1.4547279 -2.2320535 -2.6196117 -2.7342436 -2.7183175 -2.5658939][-3.0322585 -2.4735408 -1.7940673 -1.3238192 -1.0279461 -0.80926585 -0.68953586 -0.65732419 -0.798475 -1.2980996 -1.8790114 -2.2370009 -2.4014239 -2.4143441 -2.1969404][-2.7360988 -2.1229229 -1.6131926 -1.4637206 -1.4800208 -1.4203146 -1.1972498 -0.84234488 -0.525097 -0.57850242 -0.92267108 -1.3261958 -1.7259536 -1.9244908 -1.7609692][-2.4826908 -1.9538954 -1.6851915 -1.7579701 -1.9237158 -1.8795953 -1.4990361 -0.78959644 0.014010191 0.41421628 0.30180407 -0.22900128 -0.96595061 -1.4271811 -1.4088442]]...]
INFO - root - 2017-12-16 09:06:13.861084: step 27010, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:13m:24s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:06:15.536853: step 27020, loss = 0.75, batch loss = 0.50 (46.4 examples/sec; 0.173 sec/batch; 14h:38m:15s remains)
INFO - root - 2017-12-16 09:06:17.214565: step 27030, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 13h:57m:31s remains)
INFO - root - 2017-12-16 09:06:18.903566: step 27040, loss = 0.71, batch loss = 0.45 (48.7 examples/sec; 0.164 sec/batch; 13h:56m:15s remains)
INFO - root - 2017-12-16 09:06:20.625041: step 27050, loss = 0.47, batch loss = 0.21 (47.5 examples/sec; 0.168 sec/batch; 14h:17m:47s remains)
INFO - root - 2017-12-16 09:06:22.282203: step 27060, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 14h:05m:44s remains)
INFO - root - 2017-12-16 09:06:23.976757: step 27070, loss = 0.58, batch loss = 0.33 (47.5 examples/sec; 0.169 sec/batch; 14h:17m:56s remains)
INFO - root - 2017-12-16 09:06:25.648822: step 27080, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 14h:21m:07s remains)
INFO - root - 2017-12-16 09:06:27.339340: step 27090, loss = 0.52, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 14h:10m:18s remains)
INFO - root - 2017-12-16 09:06:29.028481: step 27100, loss = 0.70, batch loss = 0.44 (43.1 examples/sec; 0.186 sec/batch; 15h:45m:51s remains)
2017-12-16 09:06:29.598228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5869787 -1.4227304 -1.3703498 -1.4411491 -1.5609668 -1.6065679 -1.4989038 -1.3392019 -1.340596 -1.5755423 -1.9304447 -2.2524137 -2.4213307 -2.3067923 -1.9685814][-1.9089757 -1.7308905 -1.7095981 -1.8629601 -2.0315504 -2.0464091 -1.9006944 -1.7027447 -1.6357534 -1.756972 -2.0170717 -2.3437238 -2.6137998 -2.6178157 -2.3699756][-2.4279623 -2.262531 -2.2730217 -2.4824641 -2.6760607 -2.6513536 -2.477087 -2.3073196 -2.2339022 -2.2624185 -2.4051948 -2.7085574 -3.0122519 -3.0223453 -2.7448168][-2.6947269 -2.5399768 -2.5441544 -2.7619503 -2.9673338 -2.9354119 -2.7354321 -2.5798297 -2.5328426 -2.5731783 -2.71268 -3.0042391 -3.3021932 -3.287827 -2.9469523][-2.4618592 -2.2980058 -2.2937222 -2.5448539 -2.8111868 -2.8314307 -2.5937004 -2.3857775 -2.3178697 -2.3899422 -2.6115532 -2.9900415 -3.3245749 -3.3022745 -2.9582][-1.9147434 -1.7332418 -1.742233 -2.0376577 -2.3344991 -2.3254685 -1.9609625 -1.5452158 -1.2972027 -1.3376389 -1.7227435 -2.3373241 -2.8171051 -2.8572893 -2.5022089][-1.3809032 -1.2053819 -1.2525367 -1.5085385 -1.6961656 -1.5200315 -0.92105663 -0.19650507 0.35174727 0.42822886 -0.11645746 -1.0628191 -1.8053952 -1.9893773 -1.6623397][-1.145216 -1.0324991 -1.0439266 -1.1585789 -1.1633981 -0.82542086 -0.03147769 0.981416 1.8262179 2.0762146 1.4419136 0.15438747 -0.886181 -1.264201 -1.0301869][-1.3122002 -1.3710089 -1.3597976 -1.3145963 -1.1543934 -0.73223376 0.082867384 1.1879866 2.1598818 2.4822714 1.8153727 0.36586118 -0.85168767 -1.3945813 -1.2452084][-1.7245779 -1.9783492 -2.0079432 -1.9029912 -1.6965858 -1.3225083 -0.67973936 0.19425368 0.97318935 1.2298725 0.62586975 -0.59717047 -1.7031209 -2.2373483 -2.0844934][-1.947017 -2.3179374 -2.4057958 -2.3289413 -2.2140021 -2.0219965 -1.6351902 -1.0810144 -0.53379512 -0.30978775 -0.7203176 -1.6173282 -2.5257094 -2.9673285 -2.7699449][-1.8034792 -2.1225104 -2.2268453 -2.2436719 -2.309176 -2.3558462 -2.2451057 -1.9543805 -1.5791999 -1.330331 -1.4995544 -2.06652 -2.7875602 -3.1421578 -2.916805][-1.4665544 -1.612112 -1.6654029 -1.7406355 -1.9562359 -2.207202 -2.3251448 -2.2216063 -1.9072082 -1.5744936 -1.5474553 -1.9088537 -2.5072141 -2.8059013 -2.6071827][-1.3550154 -1.2994065 -1.2375227 -1.2790886 -1.5096827 -1.8188616 -2.0207367 -1.9719687 -1.6586277 -1.3076189 -1.2519174 -1.5469131 -2.0406373 -2.2582386 -2.0705218][-1.5567981 -1.3987274 -1.2640579 -1.2384367 -1.3649045 -1.5728931 -1.696417 -1.6008323 -1.2848692 -1.0044008 -1.0281571 -1.2943652 -1.6351171 -1.6942379 -1.4341965]]...]
INFO - root - 2017-12-16 09:06:31.273735: step 27110, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:55m:00s remains)
INFO - root - 2017-12-16 09:06:32.997456: step 27120, loss = 0.52, batch loss = 0.26 (45.3 examples/sec; 0.176 sec/batch; 14h:58m:07s remains)
INFO - root - 2017-12-16 09:06:34.678003: step 27130, loss = 0.51, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 14h:02m:31s remains)
INFO - root - 2017-12-16 09:06:36.346372: step 27140, loss = 0.59, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 14h:06m:37s remains)
INFO - root - 2017-12-16 09:06:37.994648: step 27150, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 13h:45m:28s remains)
INFO - root - 2017-12-16 09:06:39.679003: step 27160, loss = 0.53, batch loss = 0.27 (46.6 examples/sec; 0.172 sec/batch; 14h:34m:34s remains)
INFO - root - 2017-12-16 09:06:41.355850: step 27170, loss = 0.60, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 14h:19m:50s remains)
INFO - root - 2017-12-16 09:06:43.034880: step 27180, loss = 0.59, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 13h:59m:00s remains)
INFO - root - 2017-12-16 09:06:44.707569: step 27190, loss = 0.54, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 14h:05m:53s remains)
INFO - root - 2017-12-16 09:06:46.373823: step 27200, loss = 0.61, batch loss = 0.35 (47.6 examples/sec; 0.168 sec/batch; 14h:15m:18s remains)
2017-12-16 09:06:46.876466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1602349 -3.1110761 -3.0658271 -3.1730692 -3.3122649 -3.3151839 -3.1892688 -3.0783038 -3.0511253 -3.0125735 -2.9302607 -2.8268626 -2.7509971 -2.6069345 -2.4195678][-3.5027294 -3.3761425 -3.2893128 -3.4712353 -3.7474189 -3.791981 -3.6130667 -3.4915271 -3.4849749 -3.4157014 -3.2597454 -3.203403 -3.19838 -3.062022 -2.8067489][-3.4517844 -3.2418833 -3.1060081 -3.3048334 -3.6311541 -3.6556292 -3.3730683 -3.1923842 -3.2247434 -3.1976166 -3.0443993 -3.0962048 -3.23142 -3.1367075 -2.8852677][-2.8141785 -2.5771985 -2.4302688 -2.6316488 -2.8939981 -2.7327855 -2.1658468 -1.8380954 -1.9878874 -2.1854169 -2.1315498 -2.2637544 -2.5099373 -2.55891 -2.428921][-1.8277631 -1.6785702 -1.6246071 -1.7994851 -1.8423879 -1.286714 -0.24891305 0.30825257 -0.14190245 -0.74148 -0.93774188 -1.1083481 -1.4548814 -1.6710411 -1.6507299][-0.83580303 -0.834568 -0.90929604 -1.0189785 -0.785007 0.3228209 2.0081651 2.9694393 2.0584691 0.74138737 0.14807081 -0.12710929 -0.56764114 -0.91342092 -0.99383664][-0.41311359 -0.45404959 -0.54761243 -0.52374339 0.046793222 1.689266 4.1160488 5.6631918 3.9665358 1.8783348 0.87454128 0.420048 -0.12549996 -0.49101126 -0.58787251][-0.91250896 -0.85503709 -0.85658371 -0.66464436 0.17754817 2.0895259 4.9882069 7.3508615 4.6956282 2.122813 0.93368077 0.36115527 -0.17642498 -0.44054735 -0.41081154][-1.9164381 -1.7876866 -1.6303601 -1.2064748 -0.28762603 1.5029724 3.8162 5.1092749 3.2624724 1.2081721 0.23260617 -0.31554151 -0.73620784 -0.77510035 -0.52361345][-2.590405 -2.3370051 -1.9677576 -1.4285082 -0.54290652 0.777812 2.1445024 2.503865 1.1521716 -0.22711039 -0.88885903 -1.2661694 -1.3939494 -1.1542332 -0.69244766][-2.7160494 -2.3120489 -1.7374315 -1.1156646 -0.36282349 0.37763453 0.83145857 0.55813217 -0.59507918 -1.4897693 -1.7552078 -1.7464957 -1.5650691 -1.1259847 -0.63364041][-2.6553273 -2.1782231 -1.5309678 -0.91773295 -0.39951611 -0.1103301 -0.12081385 -0.67747509 -1.6904912 -2.1819005 -2.0514941 -1.5694308 -1.1057011 -0.70517159 -0.35245681][-2.6106341 -2.1280031 -1.5409329 -1.1159877 -0.85489655 -0.71208525 -0.78344452 -1.2813296 -2.0455143 -2.247211 -1.829721 -1.0379539 -0.37139177 -0.096929073 0.032250881][-2.5843523 -2.1159551 -1.5972075 -1.3758044 -1.2906399 -1.1054915 -1.0425133 -1.3385208 -1.9744704 -2.1269388 -1.5892088 -0.6635704 0.089689732 0.29430032 0.13336658][-2.6060708 -2.163337 -1.6854389 -1.5819414 -1.585615 -1.2840748 -0.98636794 -1.127193 -1.7640221 -1.9216918 -1.3900521 -0.51140451 0.18404222 0.22357273 -0.20736146]]...]
INFO - root - 2017-12-16 09:06:48.543924: step 27210, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 14h:14m:00s remains)
INFO - root - 2017-12-16 09:06:50.232351: step 27220, loss = 0.49, batch loss = 0.23 (48.4 examples/sec; 0.165 sec/batch; 14h:00m:39s remains)
INFO - root - 2017-12-16 09:06:51.914282: step 27230, loss = 0.57, batch loss = 0.31 (45.8 examples/sec; 0.175 sec/batch; 14h:49m:22s remains)
INFO - root - 2017-12-16 09:06:53.619004: step 27240, loss = 0.51, batch loss = 0.25 (46.9 examples/sec; 0.171 sec/batch; 14h:28m:08s remains)
INFO - root - 2017-12-16 09:06:55.290970: step 27250, loss = 0.53, batch loss = 0.28 (44.9 examples/sec; 0.178 sec/batch; 15h:07m:20s remains)
INFO - root - 2017-12-16 09:06:56.969028: step 27260, loss = 0.56, batch loss = 0.30 (47.8 examples/sec; 0.167 sec/batch; 14h:10m:36s remains)
INFO - root - 2017-12-16 09:06:58.649312: step 27270, loss = 0.65, batch loss = 0.39 (48.3 examples/sec; 0.166 sec/batch; 14h:02m:06s remains)
INFO - root - 2017-12-16 09:07:00.303099: step 27280, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 13h:52m:57s remains)
INFO - root - 2017-12-16 09:07:01.979401: step 27290, loss = 0.59, batch loss = 0.34 (46.6 examples/sec; 0.172 sec/batch; 14h:33m:45s remains)
INFO - root - 2017-12-16 09:07:03.678985: step 27300, loss = 0.67, batch loss = 0.41 (49.1 examples/sec; 0.163 sec/batch; 13h:49m:27s remains)
2017-12-16 09:07:04.241023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.067485332 0.31463432 0.25577617 -0.051928759 -0.47797203 -0.94868708 -1.4043107 -1.7618682 -2.0379779 -2.2213094 -2.3174126 -2.3261037 -2.2160795 -2.045378 -1.8929938][-0.6276772 -0.44552839 -0.49114358 -0.76027119 -1.169892 -1.6346554 -2.1026635 -2.4862597 -2.805867 -3.0084243 -3.0704203 -3.0058451 -2.8184042 -2.5613618 -2.3259943][-1.3502897 -1.2940419 -1.3946435 -1.6851678 -2.0691285 -2.4616146 -2.8357179 -3.1223924 -3.3829396 -3.5599322 -3.5715199 -3.4574249 -3.2517655 -2.9396639 -2.5916989][-1.7263547 -1.7456837 -1.8840783 -2.1229696 -2.3887594 -2.6002109 -2.7503445 -2.8742363 -3.0929708 -3.2938809 -3.3571639 -3.3412023 -3.2493255 -3.0235312 -2.5742655][-1.7426069 -1.7529089 -1.8030448 -1.8363156 -1.8024693 -1.659178 -1.4598768 -1.3857441 -1.6233797 -1.981441 -2.3226929 -2.5925565 -2.7911754 -2.7807457 -2.4114335][-1.4228895 -1.3278941 -1.1830854 -0.90191793 -0.43159235 0.16045356 0.732429 0.95876455 0.64140773 0.029467821 -0.69829309 -1.3294604 -1.8714457 -2.156688 -2.0583904][-0.95837045 -0.69760072 -0.3249197 0.25566173 1.0796518 2.0372999 2.840421 3.100672 2.6207812 1.7870929 0.88261986 0.077615976 -0.61077309 -1.0847667 -1.2175026][-0.75858641 -0.38443685 0.071588516 0.68062711 1.4996214 2.4720418 3.2145898 3.4441311 2.9765227 2.2465432 1.5439453 0.98126292 0.56637478 0.25453067 0.033744812][-1.1231854 -0.83241785 -0.49894691 -0.12127972 0.40197206 1.0504484 1.5288844 1.7300556 1.5448446 1.2981806 1.113142 1.1365063 1.2762573 1.3999815 1.2925][-1.9058355 -1.7908449 -1.6507723 -1.5131574 -1.2920561 -0.97318113 -0.70587361 -0.50221145 -0.36863828 -0.14687061 0.22787476 0.8336122 1.5490439 2.1462324 2.2144148][-2.6057632 -2.6496115 -2.6746285 -2.71502 -2.7117968 -2.6425886 -2.5499761 -2.3565607 -2.0107806 -1.4328969 -0.66260815 0.31086874 1.3205352 2.152704 2.3532245][-2.7498019 -2.8825769 -3.054723 -3.2601781 -3.442296 -3.5803833 -3.6170135 -3.4526219 -3.0184114 -2.3164582 -1.4110918 -0.40757322 0.51317525 1.1978848 1.357815][-2.3090529 -2.4977922 -2.7767675 -3.1200798 -3.4256244 -3.6792154 -3.7925923 -3.6901426 -3.3081408 -2.6847763 -1.8846979 -1.091511 -0.44687057 -0.023441553 0.00077986717][-1.724309 -1.935348 -2.2776146 -2.6687608 -3.0166607 -3.3063557 -3.4665971 -3.4501619 -3.2164483 -2.794487 -2.2434916 -1.7234509 -1.3319377 -1.1440184 -1.2239157][-1.3452433 -1.5820216 -1.988596 -2.4017382 -2.7358012 -2.9921107 -3.1722944 -3.2379837 -3.1649129 -2.996736 -2.7294204 -2.4494219 -2.2554088 -2.2188537 -2.311877]]...]
INFO - root - 2017-12-16 09:07:05.891949: step 27310, loss = 0.53, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 14h:00m:53s remains)
INFO - root - 2017-12-16 09:07:07.570697: step 27320, loss = 0.65, batch loss = 0.40 (46.5 examples/sec; 0.172 sec/batch; 14h:34m:29s remains)
INFO - root - 2017-12-16 09:07:09.260212: step 27330, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 14h:00m:45s remains)
INFO - root - 2017-12-16 09:07:10.921093: step 27340, loss = 0.65, batch loss = 0.39 (47.6 examples/sec; 0.168 sec/batch; 14h:14m:46s remains)
INFO - root - 2017-12-16 09:07:12.605522: step 27350, loss = 0.69, batch loss = 0.43 (48.2 examples/sec; 0.166 sec/batch; 14h:03m:53s remains)
INFO - root - 2017-12-16 09:07:14.307578: step 27360, loss = 0.79, batch loss = 0.54 (47.2 examples/sec; 0.169 sec/batch; 14h:21m:44s remains)
INFO - root - 2017-12-16 09:07:15.986506: step 27370, loss = 0.59, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 14h:08m:40s remains)
INFO - root - 2017-12-16 09:07:17.619317: step 27380, loss = 0.51, batch loss = 0.25 (49.7 examples/sec; 0.161 sec/batch; 13h:39m:06s remains)
INFO - root - 2017-12-16 09:07:19.278961: step 27390, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 14h:10m:34s remains)
INFO - root - 2017-12-16 09:07:20.948429: step 27400, loss = 0.47, batch loss = 0.21 (47.5 examples/sec; 0.168 sec/batch; 14h:16m:41s remains)
2017-12-16 09:07:21.474254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6028814 -3.5506616 -3.8363409 -4.2114077 -4.508543 -4.558567 -4.4089766 -4.2656775 -4.2166495 -4.1181993 -3.7403843 -3.2011814 -2.9199262 -3.0072982 -3.2740254][-4.3787265 -4.4160824 -4.6584959 -4.8971395 -4.9691529 -4.827281 -4.6790142 -4.7183819 -4.9732556 -5.1232409 -4.90409 -4.390193 -3.9042125 -3.705533 -3.648422][-4.6572342 -4.783607 -4.8417435 -4.723587 -4.3603354 -3.880177 -3.6152151 -3.8594217 -4.5098953 -5.0194025 -5.0394096 -4.5440931 -3.7972949 -3.2824476 -2.9453666][-4.4588776 -4.6927986 -4.549305 -4.0084133 -3.0974841 -2.1252522 -1.6251794 -2.0716362 -3.2468162 -4.2840767 -4.6389856 -4.1479592 -3.1727149 -2.3819146 -1.7629495][-4.0063925 -4.2620707 -3.8919697 -2.9700625 -1.4949806 0.090705872 0.95471835 0.3388865 -1.3784846 -3.0508008 -3.8648667 -3.5520034 -2.4955888 -1.5695312 -0.76618445][-3.6977472 -3.8396988 -3.1798613 -1.8505347 0.0952909 2.3097022 3.7220442 3.1026666 0.90021849 -1.3886632 -2.7354465 -2.8037841 -2.0095811 -1.1528472 -0.22984672][-3.5687814 -3.5622573 -2.7281868 -1.1342661 1.2330701 4.0752192 6.049674 5.4320993 2.8442204 0.095503092 -1.7438128 -2.2995813 -1.9713769 -1.3255816 -0.39838564][-3.6321931 -3.4454255 -2.5191402 -0.87389636 1.6047444 4.6668024 6.8851614 6.3397646 3.7146752 0.89987373 -1.1290523 -2.1165464 -2.3353596 -1.9855709 -1.1446186][-3.8189225 -3.5207248 -2.6691327 -1.2374246 0.89532065 3.6510265 5.7744112 5.462862 3.2449291 0.79697585 -1.148314 -2.3010602 -2.820003 -2.6827455 -1.9309368][-4.0657191 -3.7752964 -3.1924081 -2.1975124 -0.53192115 1.7272155 3.546211 3.4316862 1.7099683 -0.2280395 -1.8700116 -2.9550943 -3.4229317 -3.2645085 -2.5801947][-4.2643242 -4.08436 -3.7820046 -3.2191818 -2.0844841 -0.39229703 0.99990273 0.92425466 -0.36100411 -1.7574596 -2.9858806 -3.7412496 -3.8866968 -3.5842171 -2.9580345][-4.3604903 -4.3033814 -4.21268 -3.9827366 -3.3647187 -2.3511274 -1.4449831 -1.4655436 -2.2686377 -3.1712704 -3.9163914 -4.2231612 -3.992744 -3.5075638 -2.8869276][-4.3125372 -4.2924528 -4.3162 -4.30412 -4.0969973 -3.5831833 -3.0851481 -3.0890129 -3.5031655 -3.9835715 -4.3363953 -4.2438831 -3.73988 -3.1351326 -2.5357559][-4.1035652 -4.0728135 -4.1127605 -4.1799664 -4.1569891 -3.9432092 -3.7011189 -3.6691785 -3.8343534 -4.0555258 -4.1279216 -3.827373 -3.2438107 -2.6480141 -2.1375363][-3.7421041 -3.7427709 -3.7806032 -3.7887754 -3.7468174 -3.6192017 -3.478838 -3.4055343 -3.3975291 -3.4190183 -3.3322315 -3.0113134 -2.54968 -2.1351738 -1.8421111]]...]
INFO - root - 2017-12-16 09:07:23.112940: step 27410, loss = 0.52, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 13h:45m:17s remains)
INFO - root - 2017-12-16 09:07:24.766476: step 27420, loss = 0.52, batch loss = 0.26 (49.7 examples/sec; 0.161 sec/batch; 13h:39m:13s remains)
INFO - root - 2017-12-16 09:07:26.425257: step 27430, loss = 0.58, batch loss = 0.32 (48.8 examples/sec; 0.164 sec/batch; 13h:53m:14s remains)
INFO - root - 2017-12-16 09:07:28.116181: step 27440, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 14h:07m:10s remains)
INFO - root - 2017-12-16 09:07:29.798861: step 27450, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:17m:46s remains)
INFO - root - 2017-12-16 09:07:31.449442: step 27460, loss = 0.52, batch loss = 0.27 (49.5 examples/sec; 0.162 sec/batch; 13h:41m:33s remains)
INFO - root - 2017-12-16 09:07:33.119863: step 27470, loss = 0.49, batch loss = 0.23 (47.7 examples/sec; 0.168 sec/batch; 14h:13m:12s remains)
INFO - root - 2017-12-16 09:07:34.774929: step 27480, loss = 0.51, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 13h:53m:24s remains)
INFO - root - 2017-12-16 09:07:36.421861: step 27490, loss = 0.48, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 13h:58m:17s remains)
INFO - root - 2017-12-16 09:07:38.081237: step 27500, loss = 0.61, batch loss = 0.35 (48.4 examples/sec; 0.165 sec/batch; 14h:00m:50s remains)
2017-12-16 09:07:38.578347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9238032 -2.0441155 -2.2025576 -2.2457814 -2.01534 -1.4074862 -0.96635413 -0.78327322 -0.63860106 -0.45754743 -0.28720498 -0.32877254 -0.92688394 -1.6571267 -2.1429594][-1.1831869 -1.26958 -1.3725766 -1.5001918 -1.3281994 -0.90188134 -0.68727624 -0.70719874 -0.74967957 -0.77305579 -0.78248084 -1.0171617 -1.7370144 -2.5447295 -3.1460373][-0.87132561 -0.86976206 -0.90337992 -0.90318251 -0.7063514 -0.41082013 -0.42552388 -0.6261673 -0.77732229 -0.907992 -1.0899839 -1.5248582 -2.2709525 -3.0200505 -3.6404114][-0.90322518 -0.83420527 -0.79171765 -0.65289485 -0.36620021 -0.063235283 -0.0861938 -0.32176447 -0.45200574 -0.6638006 -0.99730217 -1.5621413 -2.2971635 -2.903656 -3.4663608][-0.62935257 -0.57647741 -0.48412049 -0.23035526 0.0801692 0.28642607 0.32296109 0.18630219 0.11311865 -0.10905051 -0.52219391 -1.1726257 -1.8456712 -2.2911029 -2.7721825][0.046197891 0.15175748 0.19703388 0.44421005 0.73054552 0.93895674 1.0539806 1.0345221 1.0180199 0.83865356 0.46865797 -0.14508653 -0.77812791 -1.2900175 -1.8429271][0.63901353 0.78427958 0.80527496 1.0922515 1.4038811 1.6454463 1.8614287 1.9503975 2.0399175 2.0067997 1.7121797 1.1624231 0.46509051 -0.18830991 -0.87084484][1.2985222 1.2127917 1.1855366 1.5192213 1.8235707 2.0427485 2.1831012 2.2934828 2.5401802 2.7406201 2.6662526 2.2848315 1.6713176 0.92807961 0.076256275][1.9105725 1.4716914 1.3085911 1.601846 1.8736877 1.9651423 1.8789129 1.8198605 2.0670128 2.5205007 2.8789053 2.8881207 2.5297728 1.8357725 0.9268167][1.7553377 1.0508339 0.74086547 0.99888849 1.283448 1.3635724 1.1890464 1.0465364 1.3058968 2.0118351 2.7542181 3.0570216 2.8318515 2.1480608 1.2285614][0.40407491 -0.32142186 -0.61143792 -0.35621166 -0.036648512 0.016906023 -0.16699266 -0.23489285 0.062238932 0.85354304 1.7200451 2.1214738 1.9583874 1.3808722 0.59829974][-1.2526708 -1.8540378 -2.0614872 -1.8419131 -1.5934141 -1.5870271 -1.7393963 -1.7212186 -1.42944 -0.77877271 -0.028598547 0.34032059 0.22464347 -0.17633677 -0.72705877][-2.6500416 -3.0237646 -3.1071961 -2.935534 -2.7776484 -2.7815366 -2.8713927 -2.7796528 -2.4908793 -2.0396676 -1.5362707 -1.3539057 -1.5455 -1.8827326 -2.2508397][-3.7937799 -3.931102 -3.9045625 -3.7715127 -3.643095 -3.6128528 -3.5990043 -3.4528275 -3.2291348 -2.9647131 -2.736392 -2.7534308 -3.0307925 -3.3314347 -3.5610032][-4.3786712 -4.3565245 -4.2540455 -4.1562171 -4.0926881 -4.0656695 -4.0229964 -3.8586559 -3.6863563 -3.5923755 -3.5881524 -3.7361138 -3.9947419 -4.2020655 -4.3043413]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-27500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:07:40.676739: step 27510, loss = 0.64, batch loss = 0.39 (48.5 examples/sec; 0.165 sec/batch; 13h:58m:26s remains)
INFO - root - 2017-12-16 09:07:42.354764: step 27520, loss = 0.66, batch loss = 0.40 (48.6 examples/sec; 0.165 sec/batch; 13h:57m:12s remains)
INFO - root - 2017-12-16 09:07:43.999470: step 27530, loss = 0.49, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 14h:19m:29s remains)
INFO - root - 2017-12-16 09:07:45.673427: step 27540, loss = 0.56, batch loss = 0.30 (46.7 examples/sec; 0.171 sec/batch; 14h:31m:07s remains)
INFO - root - 2017-12-16 09:07:47.350379: step 27550, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:05m:19s remains)
INFO - root - 2017-12-16 09:07:48.998538: step 27560, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 14h:08m:02s remains)
INFO - root - 2017-12-16 09:07:50.660683: step 27570, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 13h:55m:26s remains)
INFO - root - 2017-12-16 09:07:52.325864: step 27580, loss = 0.53, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 13h:52m:28s remains)
INFO - root - 2017-12-16 09:07:54.006371: step 27590, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.169 sec/batch; 14h:20m:36s remains)
INFO - root - 2017-12-16 09:07:55.690217: step 27600, loss = 0.48, batch loss = 0.23 (46.7 examples/sec; 0.171 sec/batch; 14h:29m:46s remains)
2017-12-16 09:07:56.192349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2437365 -2.3764193 -2.2887816 -2.125792 -2.0392673 -1.9295751 -1.8814535 -1.9927945 -2.1511605 -2.2892795 -2.5417743 -2.7780762 -2.8012781 -2.6218114 -2.3816957][-2.5089359 -2.666292 -2.5889981 -2.341171 -2.0819774 -1.780129 -1.6446617 -1.8141252 -2.0534554 -2.3017991 -2.6634188 -2.99115 -3.0422666 -2.802959 -2.4931278][-2.5769188 -2.7862492 -2.7548993 -2.4584227 -2.0621228 -1.6189656 -1.3987608 -1.5451732 -1.8128886 -2.127656 -2.5951211 -3.0444453 -3.1704214 -2.9200747 -2.5674984][-2.4144158 -2.6328726 -2.6395369 -2.3272755 -1.8435581 -1.3059719 -0.97911394 -1.0745323 -1.3686484 -1.7549417 -2.2883945 -2.790051 -2.9549556 -2.7261229 -2.3875794][-2.1275249 -2.3078773 -2.3002236 -1.9237511 -1.3354211 -0.67409539 -0.18383193 -0.17986655 -0.51124012 -1.0459038 -1.6873827 -2.217288 -2.3634529 -2.16611 -1.887877][-1.822391 -1.9574159 -1.8911359 -1.4064445 -0.64760911 0.28840828 1.0554647 1.1902225 0.71825218 -0.058890581 -0.88839126 -1.4712327 -1.6060083 -1.440892 -1.2057854][-1.5583352 -1.6526608 -1.5280465 -0.96252573 -0.0037939548 1.2718251 2.4177856 2.6866407 1.9756823 0.89687681 -0.14200139 -0.79660726 -0.9711777 -0.87186539 -0.73203778][-1.4520531 -1.5258303 -1.355408 -0.75138581 0.29414058 1.752768 3.0777555 3.2100854 2.2880244 1.0843725 0.018234015 -0.66169155 -0.85453045 -0.80069566 -0.70764947][-1.5852323 -1.6396843 -1.5016321 -1.041364 -0.20373082 0.97735071 1.9169106 1.8871007 1.1819196 0.26412797 -0.56697285 -1.1066973 -1.2400987 -1.1693271 -1.0388713][-1.8207827 -1.8671508 -1.7958915 -1.4785249 -0.83630884 0.05780077 0.68678308 0.63407183 0.10846829 -0.57524395 -1.232195 -1.6784127 -1.7752992 -1.7015311 -1.5458363][-1.9735999 -2.0523062 -2.0514512 -1.8149476 -1.3135301 -0.67105711 -0.2528758 -0.2986505 -0.68626618 -1.2165763 -1.7816246 -2.1846828 -2.3147426 -2.2935429 -2.1776309][-2.1246598 -2.2253342 -2.2586784 -2.052213 -1.6428626 -1.1857079 -0.93368685 -0.9962939 -1.2827749 -1.6639736 -2.0830026 -2.4188952 -2.5857108 -2.6257942 -2.5681674][-2.3105464 -2.4261696 -2.449182 -2.24838 -1.8971194 -1.5654571 -1.4008269 -1.4509023 -1.6324675 -1.8515991 -2.1107695 -2.350009 -2.5225224 -2.6208203 -2.6239033][-2.4291196 -2.5080197 -2.4738955 -2.244303 -1.932544 -1.6948302 -1.6171888 -1.6811554 -1.7714052 -1.8469644 -1.9412968 -2.0457637 -2.1314695 -2.2011976 -2.226155][-2.4585757 -2.4749258 -2.3960061 -2.1532874 -1.8567193 -1.6748807 -1.680231 -1.7809472 -1.8448296 -1.8347354 -1.8057857 -1.7747366 -1.7316759 -1.7071865 -1.6955655]]...]
INFO - root - 2017-12-16 09:07:57.878937: step 27610, loss = 0.53, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 14h:10m:29s remains)
INFO - root - 2017-12-16 09:07:59.543201: step 27620, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:54m:05s remains)
INFO - root - 2017-12-16 09:08:01.206000: step 27630, loss = 0.59, batch loss = 0.33 (46.6 examples/sec; 0.172 sec/batch; 14h:33m:07s remains)
INFO - root - 2017-12-16 09:08:02.881381: step 27640, loss = 0.59, batch loss = 0.33 (46.6 examples/sec; 0.172 sec/batch; 14h:32m:58s remains)
INFO - root - 2017-12-16 09:08:04.602628: step 27650, loss = 0.53, batch loss = 0.27 (47.5 examples/sec; 0.168 sec/batch; 14h:15m:42s remains)
INFO - root - 2017-12-16 09:08:06.325919: step 27660, loss = 0.63, batch loss = 0.37 (47.5 examples/sec; 0.168 sec/batch; 14h:15m:22s remains)
INFO - root - 2017-12-16 09:08:08.003007: step 27670, loss = 0.65, batch loss = 0.39 (48.9 examples/sec; 0.164 sec/batch; 13h:51m:33s remains)
INFO - root - 2017-12-16 09:08:09.672361: step 27680, loss = 0.45, batch loss = 0.20 (46.9 examples/sec; 0.170 sec/batch; 14h:25m:56s remains)
INFO - root - 2017-12-16 09:08:11.343351: step 27690, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 14h:15m:12s remains)
INFO - root - 2017-12-16 09:08:12.995046: step 27700, loss = 0.60, batch loss = 0.34 (48.7 examples/sec; 0.164 sec/batch; 13h:53m:48s remains)
2017-12-16 09:08:13.499064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2407565 -2.9655511 -2.4618597 -2.0565555 -1.8228818 -1.6358891 -1.6937377 -2.1608407 -2.7327144 -3.0802011 -3.1552188 -3.1187773 -3.1000216 -3.0532794 -2.9336724][-2.3749266 -1.9678998 -1.3317673 -0.85435593 -0.69868279 -0.69146681 -0.96567988 -1.6450064 -2.4192238 -2.9530568 -3.2611845 -3.4286966 -3.5463924 -3.5289667 -3.3647647][-1.4604713 -0.93037248 -0.28428531 0.054947376 0.048518896 -0.1969192 -0.6538794 -1.4417535 -2.3173292 -3.0398369 -3.519803 -3.8393812 -4.0739427 -4.0977592 -3.9227605][-0.74210644 -0.20495248 0.38963366 0.60275984 0.47239852 0.10412979 -0.42765749 -1.1436584 -2.0346541 -2.7967904 -3.2680943 -3.6498637 -3.9854035 -4.1155806 -4.0225239][-0.66213322 -0.10186362 0.57082295 0.89564371 0.941463 0.75823307 0.30214763 -0.32614112 -1.091813 -1.7359942 -2.1424611 -2.6275268 -3.1874979 -3.5830069 -3.6489663][-1.0069431 -0.45565736 0.22951937 0.80300021 1.2147803 1.4062047 1.1590762 0.67014456 0.13569546 -0.32496047 -0.67485905 -1.3316503 -2.1716356 -2.868597 -3.1392326][-1.7983845 -1.3804965 -0.72157681 -0.020171642 0.70406508 1.1804812 1.2375247 1.1157959 0.93392253 0.709754 0.34342241 -0.46945965 -1.5025983 -2.3381956 -2.691395][-2.7946174 -2.6040347 -2.0471368 -1.3356009 -0.50582159 0.13809061 0.47930193 0.79076266 1.0106354 0.95739126 0.53115892 -0.35285187 -1.4067175 -2.1794653 -2.4888265][-3.550622 -3.5277529 -3.1437297 -2.5596991 -1.8818893 -1.2911196 -0.84181881 -0.36175036 -0.027797461 -0.050477028 -0.41067433 -1.1211448 -1.9060798 -2.4204509 -2.6242695][-3.7282925 -3.8526902 -3.6803908 -3.4018991 -3.0411859 -2.7093008 -2.4346197 -2.0363703 -1.7810214 -1.8641224 -2.0562534 -2.3757656 -2.702801 -2.8211091 -2.820895][-3.1140532 -3.3017251 -3.3467462 -3.5034561 -3.627979 -3.618094 -3.6253715 -3.4842987 -3.3529992 -3.3778267 -3.3697782 -3.2833807 -3.1612298 -2.9023819 -2.6227586][-1.9726778 -2.119669 -2.2859724 -2.7977452 -3.3137054 -3.6206312 -3.8337598 -3.9521489 -3.9415927 -3.8919525 -3.706048 -3.3678505 -3.0152683 -2.594918 -2.1649501][-0.67064238 -0.72665977 -0.95031786 -1.7040014 -2.5507798 -3.0686929 -3.4335828 -3.7072406 -3.7387273 -3.6349645 -3.3699737 -2.9659948 -2.5924685 -2.2167523 -1.8624392][0.47425747 0.46972179 0.17678332 -0.68976557 -1.6952608 -2.3400106 -2.8079042 -3.1153026 -3.1100807 -2.9561245 -2.718843 -2.45268 -2.2809403 -2.0838039 -1.8378447][1.2015979 1.0936291 0.65093327 -0.2754159 -1.2352992 -1.8109479 -2.130523 -2.26525 -2.1112845 -1.9305294 -1.8226137 -1.8045013 -1.9067955 -1.9248226 -1.7651949]]...]
INFO - root - 2017-12-16 09:08:15.135395: step 27710, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 13h:55m:09s remains)
INFO - root - 2017-12-16 09:08:16.812985: step 27720, loss = 0.74, batch loss = 0.48 (48.1 examples/sec; 0.166 sec/batch; 14h:04m:44s remains)
INFO - root - 2017-12-16 09:08:18.485255: step 27730, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:59m:43s remains)
INFO - root - 2017-12-16 09:08:20.156641: step 27740, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 13h:51m:42s remains)
INFO - root - 2017-12-16 09:08:21.776587: step 27750, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:51m:52s remains)
INFO - root - 2017-12-16 09:08:23.452140: step 27760, loss = 0.66, batch loss = 0.40 (47.4 examples/sec; 0.169 sec/batch; 14h:16m:29s remains)
INFO - root - 2017-12-16 09:08:25.154882: step 27770, loss = 0.54, batch loss = 0.28 (45.2 examples/sec; 0.177 sec/batch; 14h:59m:39s remains)
INFO - root - 2017-12-16 09:08:26.829696: step 27780, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 14h:13m:48s remains)
INFO - root - 2017-12-16 09:08:28.516181: step 27790, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 14h:10m:19s remains)
INFO - root - 2017-12-16 09:08:30.194593: step 27800, loss = 0.68, batch loss = 0.42 (47.7 examples/sec; 0.168 sec/batch; 14h:12m:14s remains)
2017-12-16 09:08:30.681702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8836055 -3.19414 -3.3418779 -3.208874 -2.8241427 -2.4625218 -2.471709 -2.9217086 -3.5695021 -4.1307559 -4.5600376 -4.8733854 -5.0745888 -5.0428839 -4.720006][-3.4234567 -3.8153222 -3.9090943 -3.6059172 -3.019944 -2.5473437 -2.6472123 -3.2324867 -3.9822407 -4.6163125 -5.1964607 -5.7085752 -6.0802231 -6.0629807 -5.6336579][-3.8944659 -4.277606 -4.1674757 -3.5257523 -2.6254218 -1.9602721 -1.9575357 -2.5785789 -3.4161406 -4.2066131 -5.00733 -5.797781 -6.3285913 -6.3696041 -5.811286][-4.1314611 -4.3964105 -3.9942296 -2.9442577 -1.6546887 -0.67349291 -0.42862236 -0.96092534 -1.959924 -3.0376887 -4.0632219 -5.1163945 -5.8387394 -5.9809103 -5.3507957][-4.1677656 -4.3278122 -3.70641 -2.3243613 -0.70327449 0.67790508 1.3866365 1.1635222 0.066568613 -1.3333023 -2.6713893 -4.0043807 -4.970067 -5.2203693 -4.6169548][-4.040462 -4.1013703 -3.339222 -1.8088554 0.0075333118 1.8417118 3.2267363 3.6071837 2.5373428 0.7879 -1.0742977 -2.7431958 -3.9441667 -4.3359108 -3.7740593][-3.814101 -3.776823 -2.88327 -1.2792776 0.7019875 2.9140303 4.9832287 6.2515182 5.3814344 3.168133 0.74177027 -1.378852 -2.8965788 -3.4673331 -2.9346495][-3.5963807 -3.4996898 -2.4881446 -0.87496781 1.0113351 3.2380311 5.6149035 7.4925804 6.7781954 4.510767 1.9167974 -0.42983103 -2.1796994 -2.9121683 -2.4844532][-3.521662 -3.4574714 -2.4680247 -0.91873026 0.77143455 2.5403197 4.4130688 5.7207794 5.4240484 4.022274 2.0596559 -0.043629169 -1.8284748 -2.682518 -2.3781149][-3.5177536 -3.5522742 -2.7392695 -1.3725746 0.016164303 1.213475 2.402271 3.3581769 3.5881073 3.1966431 2.2064927 0.47820234 -1.3486341 -2.3488996 -2.302474][-3.5745275 -3.7297368 -3.19628 -2.1106083 -1.0499849 -0.29569459 0.41074872 1.145571 1.7337034 2.147712 2.0262811 0.83701444 -0.91788232 -2.1109388 -2.3890848][-3.5754073 -3.8754859 -3.6200638 -2.9163373 -2.2322469 -1.8315835 -1.5015228 -1.0347811 -0.28815269 0.67929125 1.2221892 0.567189 -0.93940878 -2.1913872 -2.7468548][-3.3692017 -3.7160735 -3.7033157 -3.3698571 -2.9917448 -2.8420479 -2.7949758 -2.5621898 -1.8180822 -0.67372668 0.16293192 -0.11476088 -1.2233218 -2.3474154 -3.0493858][-2.9718504 -3.2684686 -3.3802543 -3.2807779 -3.118751 -3.1252027 -3.2250268 -3.1218765 -2.5539603 -1.6172638 -0.93454957 -1.0484762 -1.7129798 -2.5184715 -3.1137562][-2.5534122 -2.7531028 -2.8650556 -2.854754 -2.8253093 -2.9207659 -3.0593441 -3.0550585 -2.7392097 -2.202122 -1.8137372 -1.8123287 -2.153991 -2.6144986 -2.9766841]]...]
INFO - root - 2017-12-16 09:08:32.362595: step 27810, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 14h:11m:58s remains)
INFO - root - 2017-12-16 09:08:34.074026: step 27820, loss = 0.47, batch loss = 0.21 (46.3 examples/sec; 0.173 sec/batch; 14h:37m:25s remains)
INFO - root - 2017-12-16 09:08:35.757031: step 27830, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:58m:34s remains)
INFO - root - 2017-12-16 09:08:37.445720: step 27840, loss = 0.59, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 14h:18m:26s remains)
INFO - root - 2017-12-16 09:08:39.153794: step 27850, loss = 0.60, batch loss = 0.34 (47.9 examples/sec; 0.167 sec/batch; 14h:08m:21s remains)
INFO - root - 2017-12-16 09:08:40.858120: step 27860, loss = 0.64, batch loss = 0.38 (46.9 examples/sec; 0.171 sec/batch; 14h:25m:54s remains)
INFO - root - 2017-12-16 09:08:42.549552: step 27870, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:11m:15s remains)
INFO - root - 2017-12-16 09:08:44.252856: step 27880, loss = 0.63, batch loss = 0.37 (48.2 examples/sec; 0.166 sec/batch; 14h:02m:46s remains)
INFO - root - 2017-12-16 09:08:45.960782: step 27890, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 14h:04m:53s remains)
INFO - root - 2017-12-16 09:08:47.655101: step 27900, loss = 0.59, batch loss = 0.33 (46.4 examples/sec; 0.172 sec/batch; 14h:35m:06s remains)
2017-12-16 09:08:48.165783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8675919 -3.2511663 -3.7314143 -4.0952711 -3.8898592 -3.0661674 -1.922044 -1.7988005 -2.5395923 -3.2036371 -3.452352 -3.4297619 -3.1757555 -2.9180295 -2.6531644][-2.8343408 -3.252459 -3.7475073 -4.0116372 -3.6833358 -2.7103558 -1.3708481 -1.2336131 -2.2378492 -3.1816735 -3.6454349 -3.7772613 -3.6408832 -3.4486198 -3.1761284][-2.4737215 -2.843878 -3.340832 -3.493185 -3.0166061 -1.8947356 -0.37930536 -0.2442646 -1.4775115 -2.6915927 -3.3727102 -3.6651735 -3.6758411 -3.5897005 -3.3424089][-1.9134145 -2.2139368 -2.7405624 -2.8161781 -2.1948197 -0.89467657 0.76602888 0.98429441 -0.40463102 -1.8545536 -2.7283089 -3.162683 -3.32045 -3.3738368 -3.1279788][-1.3608606 -1.6277512 -2.1523194 -2.1593189 -1.4352899 -0.056327581 1.7483258 2.0627337 0.57598948 -0.96576178 -1.9290391 -2.4828484 -2.8273144 -3.0338361 -2.7819929][-1.0163368 -1.2664417 -1.6928878 -1.5836116 -0.79090726 0.68688273 2.7626138 3.1971292 1.6806846 0.072370529 -0.99206769 -1.7002113 -2.2645974 -2.585252 -2.3327837][-0.88389289 -1.1080592 -1.3846251 -1.119835 -0.23758268 1.4235613 3.8182745 4.4089422 2.7589078 0.94211459 -0.30620718 -1.1350332 -1.8033342 -2.1374838 -1.8722179][-1.0351614 -1.2244756 -1.3564259 -1.0366579 -0.15453005 1.5751579 4.0262117 4.556416 2.8493729 0.98831987 -0.25661397 -1.0070584 -1.5616162 -1.8222826 -1.5416272][-1.4672229 -1.6656367 -1.717849 -1.3800286 -0.56910145 1.0128729 3.218956 3.5774326 2.0304909 0.36505008 -0.67997539 -1.2351118 -1.6236072 -1.7791471 -1.5508864][-1.9749736 -2.1914783 -2.2441871 -1.9457824 -1.2488691 0.11442518 1.8727813 2.0822644 0.82203555 -0.50222659 -1.3357168 -1.7444575 -1.9925034 -2.0476568 -1.8425345][-2.4764941 -2.7587769 -2.8634725 -2.687531 -2.1359746 -1.0426048 0.26481318 0.33530784 -0.61039281 -1.5939491 -2.1470647 -2.4142482 -2.5600827 -2.517416 -2.3170471][-2.789258 -3.127573 -3.3377628 -3.3140695 -2.9647377 -2.1779308 -1.2841257 -1.2987016 -1.9302424 -2.5372751 -2.8679128 -3.0103629 -3.0544581 -2.9703472 -2.8042443][-2.8524809 -3.1633873 -3.4166837 -3.5064549 -3.3545389 -2.8742261 -2.3370829 -2.3678856 -2.7293267 -3.0289104 -3.1695235 -3.2109418 -3.18999 -3.1093304 -3.0067322][-2.7391653 -2.9771757 -3.2148194 -3.3297319 -3.2702842 -3.0255172 -2.7428477 -2.7601295 -2.9233048 -3.021584 -3.0608211 -3.0563388 -3.016546 -2.9695184 -2.8947153][-2.5594809 -2.7180893 -2.903686 -3.0097432 -3.0070877 -2.8974881 -2.7722301 -2.781075 -2.856051 -2.8889174 -2.8747835 -2.8536437 -2.7993724 -2.7486076 -2.685571]]...]
INFO - root - 2017-12-16 09:08:49.841720: step 27910, loss = 0.51, batch loss = 0.25 (48.9 examples/sec; 0.164 sec/batch; 13h:50m:05s remains)
INFO - root - 2017-12-16 09:08:51.537824: step 27920, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:58m:11s remains)
INFO - root - 2017-12-16 09:08:53.247960: step 27930, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 14h:09m:23s remains)
INFO - root - 2017-12-16 09:08:54.941659: step 27940, loss = 0.83, batch loss = 0.57 (45.8 examples/sec; 0.175 sec/batch; 14h:46m:54s remains)
INFO - root - 2017-12-16 09:08:56.666314: step 27950, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 14h:24m:12s remains)
INFO - root - 2017-12-16 09:08:58.356209: step 27960, loss = 0.50, batch loss = 0.24 (46.5 examples/sec; 0.172 sec/batch; 14h:33m:23s remains)
INFO - root - 2017-12-16 09:09:00.061940: step 27970, loss = 0.67, batch loss = 0.41 (45.7 examples/sec; 0.175 sec/batch; 14h:48m:55s remains)
INFO - root - 2017-12-16 09:09:01.769518: step 27980, loss = 0.55, batch loss = 0.29 (47.1 examples/sec; 0.170 sec/batch; 14h:22m:24s remains)
INFO - root - 2017-12-16 09:09:03.434065: step 27990, loss = 0.55, batch loss = 0.29 (49.6 examples/sec; 0.161 sec/batch; 13h:39m:16s remains)
INFO - root - 2017-12-16 09:09:05.091846: step 28000, loss = 0.61, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 13h:51m:28s remains)
2017-12-16 09:09:05.589366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0459508 -1.0790825 -1.2746694 -1.4874173 -1.8928676 -2.5713251 -3.1004782 -3.3749402 -3.5554783 -3.62518 -3.2659757 -2.555476 -2.113677 -2.2389579 -2.39558][-1.242427 -1.3127006 -1.6795499 -2.1573451 -2.7514424 -3.4506016 -3.8917077 -4.0395679 -4.1376004 -4.1674285 -3.7808843 -2.9911654 -2.4263368 -2.3953514 -2.4127924][-1.7385042 -1.8852133 -2.3967912 -3.0212548 -3.63478 -4.1316504 -4.2594538 -4.1022491 -3.991662 -4.0325346 -3.8225222 -3.2179005 -2.7059686 -2.5799165 -2.4595575][-2.3689716 -2.5689702 -3.0329061 -3.5601864 -3.9172378 -4.0080156 -3.62812 -2.9875975 -2.6410849 -2.82178 -3.1100411 -3.1005874 -2.9812121 -2.9375558 -2.7799094][-2.7582438 -2.9306049 -3.1548245 -3.3147449 -3.2527661 -2.7974725 -1.7979877 -0.66323555 -0.17431831 -0.63737309 -1.6850002 -2.5214419 -2.9489377 -3.1489751 -3.1070764][-2.6775849 -2.7132518 -2.6204076 -2.3698354 -1.7632673 -0.61561644 0.96216011 2.4038699 2.8164504 1.8071358 -0.058511734 -1.5936975 -2.3932195 -2.8079977 -2.952657][-2.2484014 -2.0568523 -1.6763644 -1.1462727 -0.10392451 1.6292951 3.7760642 5.4483948 5.5480394 3.7658284 1.231581 -0.67406 -1.5830456 -2.0609767 -2.3879457][-1.8603799 -1.5543182 -1.0960814 -0.45626032 0.75748706 2.755408 5.1766815 7.044775 6.7342882 4.3671732 1.5408261 -0.42628706 -1.2255353 -1.6180053 -2.0477228][-2.0640686 -1.9034866 -1.55109 -0.98473573 0.011630297 1.662482 3.5721867 4.8658152 4.5099039 2.5698407 0.26708031 -1.3114256 -1.8403515 -2.0921121 -2.4826686][-2.679368 -2.7292962 -2.6161304 -2.1975124 -1.509413 -0.53671312 0.44054151 1.0202816 0.8617425 -0.29153347 -1.7637311 -2.7335396 -2.8821762 -2.9411473 -3.1619921][-3.153089 -3.3753138 -3.4247231 -3.129185 -2.7369576 -2.4433634 -2.2431417 -2.1070247 -2.1263726 -2.6114194 -3.3663988 -3.7736621 -3.6493025 -3.5234985 -3.5297306][-3.2699146 -3.5210795 -3.6144032 -3.4866467 -3.390667 -3.5430429 -3.7768962 -3.8306015 -3.7334354 -3.8057609 -4.0297933 -4.1353784 -3.9559622 -3.7111652 -3.5026336][-3.070811 -3.2487211 -3.3193762 -3.3362305 -3.40797 -3.6886661 -4.02135 -4.1417246 -4.0135641 -3.9152884 -3.9336867 -3.9549978 -3.800529 -3.5093222 -3.2116337][-2.5218332 -2.6509626 -2.7210913 -2.7774677 -2.8705692 -3.0609903 -3.2805915 -3.3675776 -3.2929261 -3.2100046 -3.2013147 -3.2114062 -3.1181259 -2.8797328 -2.6198511][-1.8987982 -1.964278 -2.0054543 -2.03952 -2.087558 -2.1582718 -2.2488132 -2.2984252 -2.2916107 -2.2758555 -2.2743506 -2.2625973 -2.212378 -2.0946026 -1.9589876]]...]
INFO - root - 2017-12-16 09:09:07.313527: step 28010, loss = 0.57, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 14h:12m:25s remains)
INFO - root - 2017-12-16 09:09:08.978198: step 28020, loss = 0.70, batch loss = 0.44 (49.3 examples/sec; 0.162 sec/batch; 13h:43m:45s remains)
INFO - root - 2017-12-16 09:09:10.655409: step 28030, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 14h:18m:11s remains)
INFO - root - 2017-12-16 09:09:12.331332: step 28040, loss = 0.62, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 13h:57m:03s remains)
INFO - root - 2017-12-16 09:09:14.001505: step 28050, loss = 0.64, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 13h:59m:48s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:09:15.691080: step 28060, loss = 0.51, batch loss = 0.25 (46.4 examples/sec; 0.172 sec/batch; 14h:35m:11s remains)
INFO - root - 2017-12-16 09:09:17.382729: step 28070, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 13h:52m:15s remains)
INFO - root - 2017-12-16 09:09:19.055457: step 28080, loss = 0.49, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 13h:49m:37s remains)
INFO - root - 2017-12-16 09:09:20.710272: step 28090, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:01m:43s remains)
INFO - root - 2017-12-16 09:09:22.409425: step 28100, loss = 0.57, batch loss = 0.31 (46.0 examples/sec; 0.174 sec/batch; 14h:41m:40s remains)
2017-12-16 09:09:22.953245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7855246 -2.8299839 -2.8717391 -2.8802071 -2.8857963 -2.899291 -2.8961229 -2.8616493 -2.8196988 -2.774951 -2.7398937 -2.711175 -2.7024236 -2.7071824 -2.7011545][-3.2783267 -3.4029169 -3.4634502 -3.4654925 -3.4575863 -3.456028 -3.4286709 -3.3782215 -3.3461442 -3.3207061 -3.2995167 -3.2792161 -3.2781119 -3.2870483 -3.2675266][-3.6674218 -3.8587518 -3.9680662 -3.9824314 -3.9316244 -3.8396306 -3.7224646 -3.6264629 -3.6209955 -3.6562123 -3.7043574 -3.7505903 -3.8004606 -3.8370404 -3.8066497][-3.6405854 -3.905287 -4.1284828 -4.193327 -4.0620985 -3.8305602 -3.5737109 -3.4065356 -3.4326727 -3.5800819 -3.772984 -3.9665837 -4.1311646 -4.2147622 -4.2018671][-2.8633242 -3.2628517 -3.6785367 -3.8627825 -3.6760302 -3.2634642 -2.799854 -2.500098 -2.4868 -2.7201436 -3.1015291 -3.5098398 -3.8419225 -3.9981611 -4.0291162][-1.5399235 -2.1445253 -2.7777598 -3.0797729 -2.86452 -2.2949212 -1.6387639 -1.18637 -1.0994455 -1.3796595 -1.9012799 -2.4970214 -2.978411 -3.1779108 -3.22401][-0.082553625 -0.99703765 -1.8830521 -2.3560739 -2.2192941 -1.6260372 -0.88497519 -0.30971789 -0.12778711 -0.38582134 -0.93321288 -1.5640252 -2.0097039 -2.1557019 -2.0996225][0.98177838 -0.247221 -1.3573408 -1.9988487 -2.0311882 -1.5892915 -0.9606756 -0.42652142 -0.1607914 -0.30600882 -0.73453188 -1.213401 -1.464716 -1.4184401 -1.1530172][1.2158208 -0.06528163 -1.1823177 -1.9027584 -2.1474264 -2.0078206 -1.6758676 -1.3294538 -1.0694729 -1.0386001 -1.2194151 -1.4139049 -1.352414 -1.0053462 -0.44397151][0.690572 -0.36152911 -1.2969898 -1.9250032 -2.210216 -2.2808719 -2.2819614 -2.2129974 -2.0650036 -1.9374896 -1.8863982 -1.7845998 -1.4137079 -0.7801137 0.020292521][-0.13553691 -0.80069816 -1.4161807 -1.8347405 -2.0630548 -2.270555 -2.5227013 -2.7095327 -2.6959536 -2.5315518 -2.3375146 -2.0187848 -1.4882479 -0.75102925 0.12524104][-0.78141677 -1.0165606 -1.2959647 -1.5129369 -1.6695886 -1.9316728 -2.3158491 -2.6371615 -2.7206485 -2.5790381 -2.3675392 -2.0094047 -1.5192214 -0.87878096 -0.15620804][-1.0057597 -0.95955288 -1.0161334 -1.0525564 -1.1224148 -1.4285388 -1.8729215 -2.2187772 -2.3422287 -2.3135624 -2.2502606 -2.0268052 -1.6884925 -1.265094 -0.8313694][-0.91895461 -0.67725015 -0.59299648 -0.55168366 -0.58921289 -0.889887 -1.3295325 -1.6747594 -1.8651452 -2.0966461 -2.3227794 -2.3945367 -2.3122654 -2.1185946 -1.9304192][-0.64676464 -0.34796119 -0.26991296 -0.27232742 -0.36034179 -0.66639483 -1.0934758 -1.4195826 -1.7111742 -2.2330408 -2.8019586 -3.1689525 -3.3338709 -3.339124 -3.294908]]...]
INFO - root - 2017-12-16 09:09:24.631065: step 28110, loss = 0.62, batch loss = 0.36 (47.2 examples/sec; 0.169 sec/batch; 14h:19m:39s remains)
INFO - root - 2017-12-16 09:09:26.337811: step 28120, loss = 0.53, batch loss = 0.27 (45.1 examples/sec; 0.177 sec/batch; 14h:59m:37s remains)
INFO - root - 2017-12-16 09:09:28.042054: step 28130, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 14h:03m:38s remains)
INFO - root - 2017-12-16 09:09:29.736049: step 28140, loss = 0.46, batch loss = 0.20 (46.4 examples/sec; 0.172 sec/batch; 14h:34m:39s remains)
INFO - root - 2017-12-16 09:09:31.438958: step 28150, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 14h:23m:20s remains)
INFO - root - 2017-12-16 09:09:33.123431: step 28160, loss = 0.60, batch loss = 0.34 (48.4 examples/sec; 0.165 sec/batch; 13h:58m:36s remains)
INFO - root - 2017-12-16 09:09:34.797286: step 28170, loss = 0.48, batch loss = 0.22 (47.5 examples/sec; 0.169 sec/batch; 14h:14m:58s remains)
INFO - root - 2017-12-16 09:09:36.479484: step 28180, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 14h:01m:43s remains)
INFO - root - 2017-12-16 09:09:38.187017: step 28190, loss = 0.66, batch loss = 0.40 (46.9 examples/sec; 0.170 sec/batch; 14h:24m:28s remains)
INFO - root - 2017-12-16 09:09:39.917631: step 28200, loss = 0.51, batch loss = 0.25 (40.8 examples/sec; 0.196 sec/batch; 16h:34m:24s remains)
2017-12-16 09:09:40.429453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.917954 -4.1980033 -4.3053389 -4.1691442 -3.9035854 -3.5610912 -3.2638915 -3.2408547 -3.625967 -4.1910849 -4.7104406 -4.8307209 -4.3370085 -3.4138494 -2.4273167][-3.104255 -3.5140488 -3.6991694 -3.5540028 -3.1804011 -2.6202495 -2.1869662 -2.0728917 -2.5979242 -3.3895898 -4.1599565 -4.5317788 -4.1756897 -3.2036228 -2.1778374][-2.4749 -3.0014009 -3.27291 -3.1118307 -2.5691454 -1.6808453 -0.90760088 -0.66725826 -1.2964554 -2.3734243 -3.4316487 -4.0865831 -3.9744008 -3.0840459 -2.032212][-2.2032902 -2.7599864 -3.0019829 -2.7248809 -1.9202147 -0.60592747 0.606704 1.0699339 0.33120775 -1.1277496 -2.5914888 -3.641403 -3.8706741 -3.1959286 -2.1349745][-2.1073046 -2.6004925 -2.7618635 -2.3286376 -1.2284498 0.53711343 2.2858913 3.0069535 2.1528113 0.25275326 -1.7357893 -3.2340095 -3.8613544 -3.4462266 -2.4780264][-1.8977636 -2.3138885 -2.4322937 -1.9127431 -0.59564912 1.5885251 3.8662384 4.8892384 3.9095109 1.5873237 -0.89608836 -2.8286536 -3.7906494 -3.646472 -2.7629066][-1.3107288 -1.785991 -1.8933777 -1.349135 0.058659315 2.3776209 4.947444 6.3079119 5.3097668 2.6289685 -0.23857927 -2.536551 -3.8104658 -3.8224697 -2.9496434][-0.94695091 -1.5325239 -1.7183143 -1.3298256 -0.056668282 2.1115954 4.6632624 6.3588104 5.5972643 2.9256504 -0.07931757 -2.5585608 -3.955795 -4.0454683 -3.1094813][-1.2799929 -2.0267332 -2.3332071 -2.1467867 -1.1556734 0.59613633 2.7614315 4.4402618 4.1438427 2.061578 -0.54752684 -2.7673934 -4.0885649 -4.1676397 -3.1934452][-2.0761075 -2.8627687 -3.2827897 -3.2822776 -2.5986843 -1.3527378 0.26245475 1.8048689 1.9553349 0.69375396 -1.2204705 -2.9690223 -4.1480508 -4.1633482 -3.2043276][-3.0661843 -3.6932569 -4.0916395 -4.118187 -3.6456549 -2.7945089 -1.6363077 -0.30806756 0.17735887 -0.41112983 -1.6868548 -2.9677372 -3.9372919 -3.9322116 -3.0767527][-4.1391234 -4.514647 -4.7630262 -4.6980171 -4.2829466 -3.6632564 -2.8265314 -1.67209 -1.0193018 -1.1644791 -1.9221169 -2.7999399 -3.5517678 -3.5174041 -2.8121185][-4.853436 -4.952631 -5.018589 -4.8655787 -4.4986906 -4.0805316 -3.5035076 -2.5743484 -1.9300897 -1.8751259 -2.2966971 -2.8969369 -3.3728659 -3.2658644 -2.6664286][-4.9524169 -4.8666162 -4.7719722 -4.6130648 -4.4011745 -4.2442532 -3.9486165 -3.3430707 -2.8574471 -2.7889693 -2.9975095 -3.3851202 -3.5896773 -3.3151193 -2.7225888][-4.5055094 -4.31001 -4.1880465 -4.17317 -4.2173662 -4.335865 -4.3391705 -4.0575457 -3.8218317 -3.7915282 -3.8987355 -4.0931587 -4.0500307 -3.5777719 -2.9125762]]...]
INFO - root - 2017-12-16 09:09:42.121968: step 28210, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 14h:18m:13s remains)
INFO - root - 2017-12-16 09:09:43.809818: step 28220, loss = 0.47, batch loss = 0.21 (47.7 examples/sec; 0.168 sec/batch; 14h:10m:17s remains)
INFO - root - 2017-12-16 09:09:45.514881: step 28230, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 14h:00m:58s remains)
INFO - root - 2017-12-16 09:09:47.183744: step 28240, loss = 0.48, batch loss = 0.23 (48.4 examples/sec; 0.165 sec/batch; 13h:57m:51s remains)
INFO - root - 2017-12-16 09:09:48.872082: step 28250, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:14m:04s remains)
INFO - root - 2017-12-16 09:09:50.538191: step 28260, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 14h:03m:15s remains)
INFO - root - 2017-12-16 09:09:52.203951: step 28270, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 14h:02m:22s remains)
INFO - root - 2017-12-16 09:09:53.885120: step 28280, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 14h:13m:09s remains)
INFO - root - 2017-12-16 09:09:55.602870: step 28290, loss = 0.63, batch loss = 0.37 (46.1 examples/sec; 0.174 sec/batch; 14h:40m:01s remains)
INFO - root - 2017-12-16 09:09:57.306366: step 28300, loss = 0.53, batch loss = 0.27 (46.1 examples/sec; 0.174 sec/batch; 14h:39m:57s remains)
2017-12-16 09:09:57.808606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6769841 -3.6261315 -3.3479428 -2.9058862 -2.3540936 -1.9735699 -2.1116214 -2.6004097 -2.8683541 -2.9006648 -2.7146614 -2.1801293 -1.7667966 -1.8499684 -2.2202692][-4.7170191 -4.7224512 -4.3028932 -3.6040225 -2.7517753 -2.1469426 -2.3040872 -2.9557366 -3.3661423 -3.4816055 -3.2720804 -2.5848553 -2.0821717 -2.239609 -2.7553725][-5.5757933 -5.5335455 -4.894412 -3.8618164 -2.6264024 -1.7668259 -1.8181438 -2.5273442 -3.013288 -3.3024857 -3.1515586 -2.3630424 -1.8093175 -2.0807567 -2.7404377][-5.8156796 -5.5863237 -4.69219 -3.3210187 -1.7074525 -0.5972991 -0.54161716 -1.2856555 -1.9653001 -2.3890848 -2.3240881 -1.4904053 -0.95492792 -1.3572849 -2.210515][-5.3688211 -4.9403191 -3.771271 -2.0935159 -0.096274376 1.3975756 1.6359911 0.87832618 0.087958813 -0.49605763 -0.56288052 0.18480206 0.62546492 0.0077037811 -1.0681943][-4.8349652 -4.1821241 -2.8111877 -0.92326617 1.3978269 3.2731543 3.6575828 2.8672194 1.9361906 1.1492097 0.99910593 1.7135057 2.0656166 1.3122928 0.098116159][-3.8272033 -2.8628795 -1.268687 0.77886176 3.4070826 5.7399607 6.2034092 5.4069495 4.3679185 3.3780122 2.9453669 3.3946714 3.5141521 2.6342368 1.3281147][-2.89817 -1.7771559 -0.0353837 2.1103058 4.8530464 7.6496544 8.2683125 7.5579081 6.5693493 5.3871884 4.6224275 4.63494 4.3901081 3.4367847 2.1380091][-3.2447503 -2.572746 -1.3338386 0.33588934 2.4833674 4.6483092 5.3659391 4.8883648 4.1115594 3.2160859 2.6978498 2.8348365 2.693572 1.9754791 1.0206006][-3.8056993 -3.4359431 -2.5277922 -1.2600734 0.45234561 2.2356038 2.9715495 2.6528854 2.0412483 1.3726602 0.96862364 1.1327162 1.1062553 0.56852293 -0.21049547][-4.0105085 -3.7537935 -3.0256648 -2.0158741 -0.60077524 0.933439 1.6248045 1.3497431 0.876729 0.3795166 0.092234612 0.30197477 0.38735271 -0.0692873 -0.78266621][-4.3002868 -4.2640939 -3.7612138 -2.9894793 -1.8307631 -0.542027 0.072478294 -0.1599102 -0.56738269 -0.94781935 -1.1176537 -0.81256986 -0.59837723 -0.90873432 -1.4767622][-4.5969291 -4.8052988 -4.5465131 -3.9920809 -2.9831004 -1.772385 -1.1508881 -1.3470783 -1.7366942 -2.0065556 -1.9913785 -1.5719438 -1.2399644 -1.3506361 -1.7707009][-4.4726696 -4.8501039 -4.7627974 -4.3382812 -3.4481435 -2.3351877 -1.7089071 -1.8718777 -2.1924729 -2.3447063 -2.1710126 -1.6278222 -1.171042 -1.1247296 -1.4841046][-4.0168576 -4.42585 -4.4708815 -4.1912832 -3.5101068 -2.6281335 -2.1172612 -2.2377963 -2.4744382 -2.5629797 -2.3879924 -1.909006 -1.4539734 -1.3034526 -1.5424545]]...]
INFO - root - 2017-12-16 09:09:59.476530: step 28310, loss = 0.62, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 14h:17m:04s remains)
INFO - root - 2017-12-16 09:10:01.177643: step 28320, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 13h:51m:38s remains)
INFO - root - 2017-12-16 09:10:02.866814: step 28330, loss = 0.63, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 14h:07m:00s remains)
INFO - root - 2017-12-16 09:10:04.527278: step 28340, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:57m:42s remains)
INFO - root - 2017-12-16 09:10:06.200060: step 28350, loss = 0.56, batch loss = 0.30 (49.0 examples/sec; 0.163 sec/batch; 13h:47m:06s remains)
INFO - root - 2017-12-16 09:10:07.900597: step 28360, loss = 0.71, batch loss = 0.45 (48.9 examples/sec; 0.164 sec/batch; 13h:49m:51s remains)
INFO - root - 2017-12-16 09:10:09.596352: step 28370, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 14h:00m:42s remains)
INFO - root - 2017-12-16 09:10:11.254115: step 28380, loss = 0.68, batch loss = 0.42 (47.2 examples/sec; 0.170 sec/batch; 14h:19m:08s remains)
INFO - root - 2017-12-16 09:10:12.921870: step 28390, loss = 0.74, batch loss = 0.48 (47.1 examples/sec; 0.170 sec/batch; 14h:20m:01s remains)
INFO - root - 2017-12-16 09:10:14.590306: step 28400, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 14h:06m:42s remains)
2017-12-16 09:10:15.085769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.91441488 0.40150309 -0.05110836 -0.26987314 -0.43461061 -0.68362355 -0.83697283 -0.86367309 -0.89088356 -0.87825727 -0.98121381 -1.3804748 -2.0003114 -2.6893547 -3.2487767][1.0239892 0.70090127 0.38008666 0.20160031 -0.0037317276 -0.41519034 -0.98973489 -1.576709 -2.0938289 -2.4480829 -2.6872482 -2.9517767 -3.2457826 -3.538353 -3.7261071][0.31248713 0.21874547 0.085517406 0.0059225559 -0.20825124 -0.76932812 -1.6177549 -2.5159752 -3.2547851 -3.7056372 -3.8973439 -3.9506774 -3.9421451 -3.9105992 -3.8227844][-1.1235356 -1.0843694 -1.0960574 -1.1392117 -1.3983662 -1.9613743 -2.6829181 -3.3384876 -3.8152533 -4.0295949 -4.0385971 -3.9230816 -3.7417033 -3.5815067 -3.3912144][-2.6253049 -2.5473316 -2.4937844 -2.5298076 -2.7347283 -3.0422249 -3.225306 -3.1892295 -3.0598483 -2.8877296 -2.7612913 -2.7145329 -2.6681256 -2.6397953 -2.5117128][-3.4464307 -3.3365386 -3.2337766 -3.2335243 -3.2832627 -3.1457875 -2.5754931 -1.6865375 -0.91566575 -0.51599193 -0.573136 -0.93657362 -1.3207912 -1.5745431 -1.5483388][-3.0981596 -2.9579027 -2.8368175 -2.7971461 -2.6562974 -2.056329 -0.77730906 0.84225464 2.022119 2.2769175 1.662528 0.63778758 -0.31373715 -0.91193044 -1.0193181][-1.8646314 -1.6791319 -1.5582675 -1.5486487 -1.3513845 -0.54934 1.1266885 3.1492887 4.4812379 4.2573953 2.8608541 1.199296 -0.1359148 -0.9324075 -1.1370183][-0.70137143 -0.52344084 -0.42438245 -0.47871757 -0.41756463 0.26836753 1.8011279 3.7150946 4.8788881 4.1315184 2.3118253 0.49786329 -0.7967639 -1.5356309 -1.7314174][-0.20949316 -0.096978188 -0.12152791 -0.27479553 -0.33325911 0.040562868 0.97413206 2.1045122 2.6619945 1.942894 0.46069574 -0.92842209 -1.833963 -2.3098524 -2.4103825][-0.23796558 -0.20390558 -0.39365089 -0.64961076 -0.80205178 -0.724179 -0.37018561 0.05228591 0.18017364 -0.35511684 -1.2542319 -2.0800588 -2.5489213 -2.7299762 -2.6658721][-0.5235039 -0.5592016 -0.80499172 -1.0221236 -1.0863845 -1.0059544 -0.87048066 -0.85829628 -1.0859221 -1.5949755 -2.1283207 -2.505409 -2.6053054 -2.4999123 -2.258384][-0.881948 -0.87799871 -0.95018387 -0.89590204 -0.61944926 -0.23324442 -0.0062711239 -0.20994473 -0.83389854 -1.5437601 -2.0247746 -2.1915846 -2.056591 -1.7499292 -1.4269042][-1.2863684 -1.0774829 -0.79064906 -0.31445622 0.34590507 1.0883594 1.4847479 1.1269135 0.14968777 -0.85254908 -1.4340601 -1.5280749 -1.2704647 -0.84192896 -0.55441904][-1.7254484 -1.31695 -0.67315495 0.14292574 1.0374241 1.8632555 2.2367415 1.7985582 0.70439053 -0.38703942 -1.0097238 -1.0577058 -0.76928329 -0.36834502 -0.17100143]]...]
INFO - root - 2017-12-16 09:10:16.769622: step 28410, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 13h:57m:22s remains)
INFO - root - 2017-12-16 09:10:18.417242: step 28420, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:59m:02s remains)
INFO - root - 2017-12-16 09:10:20.083026: step 28430, loss = 0.60, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 14h:05m:14s remains)
INFO - root - 2017-12-16 09:10:21.764585: step 28440, loss = 0.43, batch loss = 0.17 (47.2 examples/sec; 0.169 sec/batch; 14h:18m:50s remains)
INFO - root - 2017-12-16 09:10:23.454465: step 28450, loss = 0.57, batch loss = 0.31 (49.7 examples/sec; 0.161 sec/batch; 13h:34m:53s remains)
INFO - root - 2017-12-16 09:10:25.126830: step 28460, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 13h:54m:33s remains)
INFO - root - 2017-12-16 09:10:26.839679: step 28470, loss = 0.62, batch loss = 0.36 (47.9 examples/sec; 0.167 sec/batch; 14h:06m:18s remains)
INFO - root - 2017-12-16 09:10:28.526756: step 28480, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 14h:08m:01s remains)
INFO - root - 2017-12-16 09:10:30.198141: step 28490, loss = 0.43, batch loss = 0.17 (47.7 examples/sec; 0.168 sec/batch; 14h:10m:36s remains)
INFO - root - 2017-12-16 09:10:31.884796: step 28500, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 14h:08m:16s remains)
2017-12-16 09:10:32.399279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.50827765 -0.92223489 -0.90049481 -0.45430624 0.085999489 0.63091326 0.93188024 0.5825789 -0.33167076 -1.2393323 -1.8619616 -2.2337854 -2.2593226 -2.0868495 -1.9887158][-0.99077666 -1.3096786 -1.2386705 -0.84661937 -0.41338503 -0.045829296 0.12369514 -0.22622943 -1.0213487 -1.8002713 -2.3203633 -2.5544724 -2.461045 -2.2134891 -2.0568309][-1.6814723 -1.8318734 -1.6509533 -1.3008058 -0.954062 -0.710112 -0.63494444 -0.92354059 -1.5546778 -2.2005951 -2.6146092 -2.7293403 -2.5484626 -2.2582202 -2.0759933][-2.5100791 -2.5094314 -2.2615106 -1.8969427 -1.5176876 -1.208995 -1.0435976 -1.2082496 -1.6882088 -2.2249668 -2.5612822 -2.62449 -2.4582694 -2.213958 -2.0550869][-3.1950445 -3.0734882 -2.733681 -2.277159 -1.7543607 -1.2460506 -0.88843691 -0.9018898 -1.2772828 -1.7706903 -2.1068308 -2.2427614 -2.2083824 -2.0821717 -1.9933919][-3.6481767 -3.3957677 -2.9184353 -2.271389 -1.5394218 -0.78995538 -0.24240971 -0.14815378 -0.49175251 -1.0102062 -1.4592915 -1.7692225 -1.906359 -1.913434 -1.9078238][-3.9306588 -3.5783043 -2.9612594 -2.1111236 -1.151737 -0.21792579 0.45165539 0.584738 0.20559192 -0.40841925 -0.99034452 -1.452477 -1.7212069 -1.8189795 -1.8537415][-3.8888094 -3.5108495 -2.8276517 -1.8731093 -0.771981 0.24814343 0.93175411 1.0292022 0.54276872 -0.20088315 -0.90826178 -1.4621649 -1.7611504 -1.8568709 -1.8708467][-3.2316883 -2.928283 -2.3019285 -1.4194362 -0.40064323 0.50918627 1.0873044 1.0703695 0.49896312 -0.3388598 -1.1145847 -1.6651517 -1.9220519 -1.9643078 -1.9253979][-2.1927207 -1.9622605 -1.5028647 -0.89849687 -0.20183015 0.44764137 0.90130091 0.82950306 0.25852203 -0.57541394 -1.3501178 -1.8673282 -2.0784283 -2.0550954 -1.9806192][-1.1431907 -0.87711227 -0.56280231 -0.31379461 -0.028824568 0.35704017 0.75053883 0.70070863 0.13926363 -0.65629232 -1.3942221 -1.9024031 -2.1238427 -2.0989118 -2.0190818][-0.35364842 0.066755533 0.34437966 0.30039191 0.21068382 0.4118278 0.77544022 0.78422475 0.2525382 -0.47914779 -1.2161344 -1.7730554 -2.0769994 -2.1129074 -2.0467093][-0.041678429 0.50531363 0.83774948 0.64416242 0.32000327 0.41421962 0.79039979 0.89707971 0.47397709 -0.16824985 -0.85946226 -1.4782465 -1.9132162 -2.0601709 -2.0517819][-0.40902376 0.13943338 0.49257183 0.26242447 -0.11559153 -0.038621902 0.40656281 0.68520641 0.50530648 0.058441639 -0.51174748 -1.1045598 -1.6381001 -1.9110903 -2.0075896][-1.3582299 -0.97375429 -0.70029914 -0.88653862 -1.2118795 -1.1196824 -0.59871137 -0.11919761 0.041222811 -0.082180977 -0.40613854 -0.83196735 -1.3188357 -1.6885297 -1.8976749]]...]
INFO - root - 2017-12-16 09:10:34.073086: step 28510, loss = 0.53, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 14h:07m:12s remains)
INFO - root - 2017-12-16 09:10:35.773478: step 28520, loss = 0.49, batch loss = 0.23 (46.4 examples/sec; 0.172 sec/batch; 14h:32m:37s remains)
INFO - root - 2017-12-16 09:10:37.502930: step 28530, loss = 0.52, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 13h:53m:41s remains)
INFO - root - 2017-12-16 09:10:39.171379: step 28540, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:54m:04s remains)
INFO - root - 2017-12-16 09:10:40.876737: step 28550, loss = 0.69, batch loss = 0.43 (47.1 examples/sec; 0.170 sec/batch; 14h:21m:18s remains)
INFO - root - 2017-12-16 09:10:42.603801: step 28560, loss = 0.63, batch loss = 0.38 (47.3 examples/sec; 0.169 sec/batch; 14h:17m:00s remains)
INFO - root - 2017-12-16 09:10:44.286968: step 28570, loss = 0.57, batch loss = 0.31 (49.2 examples/sec; 0.163 sec/batch; 13h:44m:12s remains)
INFO - root - 2017-12-16 09:10:45.972854: step 28580, loss = 0.57, batch loss = 0.31 (45.5 examples/sec; 0.176 sec/batch; 14h:50m:07s remains)
INFO - root - 2017-12-16 09:10:47.674170: step 28590, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 14h:08m:03s remains)
INFO - root - 2017-12-16 09:10:49.387618: step 28600, loss = 0.54, batch loss = 0.28 (46.4 examples/sec; 0.172 sec/batch; 14h:32m:24s remains)
2017-12-16 09:10:49.892426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.61033487 -0.79396927 -1.0707725 -1.3207326 -1.4355497 -1.2806624 -0.957127 -0.84278882 -1.1199138 -1.4719248 -1.7092892 -1.8893785 -2.0353355 -2.0917335 -2.0685956][-0.65292549 -0.91045582 -1.2503649 -1.5884306 -1.7917979 -1.588994 -1.0742173 -0.76625323 -0.98176396 -1.37716 -1.6936097 -1.9414688 -2.1157744 -2.1696198 -2.1251938][-1.0358901 -1.2751403 -1.4798424 -1.6377838 -1.6953981 -1.3164011 -0.58890676 -0.16924787 -0.51810813 -1.155447 -1.6851599 -2.0250976 -2.2231197 -2.2791021 -2.2011187][-1.4886084 -1.5925853 -1.5509489 -1.4057786 -1.1600343 -0.50576854 0.44061995 0.89840341 0.31487131 -0.72587311 -1.5577019 -2.0622635 -2.3334842 -2.4018657 -2.278075][-1.8192468 -1.7749507 -1.4825625 -1.088758 -0.55235052 0.36821961 1.5097604 1.9968371 1.2411094 -0.097659826 -1.1949817 -1.9295486 -2.3335242 -2.4319446 -2.2994087][-1.7652067 -1.6258628 -1.2488865 -0.80394471 -0.20049381 0.80356431 1.9944191 2.4891806 1.7279997 0.34153318 -0.87509692 -1.7616477 -2.2715459 -2.3906724 -2.2802954][-1.4161699 -1.2577882 -0.97315049 -0.66690385 -0.2594583 0.52355385 1.5060248 1.9807367 1.4043603 0.26260662 -0.84988141 -1.7551618 -2.27978 -2.3844748 -2.2707338][-0.972854 -0.86197865 -0.71526968 -0.64513671 -0.59266019 -0.18000317 0.52338791 0.94002676 0.59696507 -0.17680645 -1.0439839 -1.8336757 -2.284936 -2.36039 -2.247395][-0.66091239 -0.60492873 -0.57621682 -0.72958457 -1.0157503 -0.89898634 -0.36851048 0.073302984 -0.08742857 -0.60159254 -1.25197 -1.862242 -2.2303758 -2.3088267 -2.2281873][-0.66746116 -0.66007292 -0.71513379 -1.0026519 -1.4271364 -1.4355168 -0.973245 -0.50889504 -0.54750204 -0.93134642 -1.4343216 -1.8823633 -2.1787267 -2.2667897 -2.2189322][-0.88442528 -0.86566257 -0.93951905 -1.2239188 -1.6373963 -1.6763849 -1.2420661 -0.78990066 -0.76866305 -1.1049889 -1.5606797 -1.9781294 -2.2581525 -2.3224781 -2.2523806][-0.91196287 -0.86187458 -0.92733228 -1.1619935 -1.4921095 -1.5074564 -1.1248772 -0.73915756 -0.73614132 -1.105885 -1.5960748 -2.0846133 -2.3867519 -2.4406846 -2.3330462][-0.43442845 -0.39215958 -0.51388347 -0.7838918 -1.0948653 -1.1163108 -0.734674 -0.32885575 -0.34430051 -0.81440461 -1.4597301 -2.0707777 -2.4369125 -2.5257435 -2.3886163][0.28709984 0.2410121 0.02051878 -0.35970712 -0.73059106 -0.73125923 -0.27020693 0.21404839 0.20936012 -0.36841416 -1.1904595 -1.9780226 -2.4439297 -2.5606987 -2.3977625][0.55264282 0.38224697 0.11360955 -0.30391049 -0.67371345 -0.58601642 -0.0012814999 0.573828 0.62943149 0.0022189617 -0.92042124 -1.8412399 -2.4163916 -2.5538607 -2.3830354]]...]
INFO - root - 2017-12-16 09:10:51.606026: step 28610, loss = 0.63, batch loss = 0.37 (46.7 examples/sec; 0.171 sec/batch; 14h:26m:51s remains)
INFO - root - 2017-12-16 09:10:53.309462: step 28620, loss = 0.52, batch loss = 0.26 (47.1 examples/sec; 0.170 sec/batch; 14h:19m:41s remains)
INFO - root - 2017-12-16 09:10:54.994311: step 28630, loss = 0.58, batch loss = 0.32 (48.0 examples/sec; 0.167 sec/batch; 14h:03m:17s remains)
INFO - root - 2017-12-16 09:10:56.681601: step 28640, loss = 0.59, batch loss = 0.33 (46.4 examples/sec; 0.172 sec/batch; 14h:32m:47s remains)
INFO - root - 2017-12-16 09:10:58.373903: step 28650, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 14h:36m:44s remains)
INFO - root - 2017-12-16 09:11:00.040737: step 28660, loss = 0.47, batch loss = 0.21 (48.2 examples/sec; 0.166 sec/batch; 14h:00m:58s remains)
INFO - root - 2017-12-16 09:11:01.734482: step 28670, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 14h:14m:16s remains)
INFO - root - 2017-12-16 09:11:03.411568: step 28680, loss = 0.64, batch loss = 0.38 (48.6 examples/sec; 0.165 sec/batch; 13h:54m:03s remains)
INFO - root - 2017-12-16 09:11:05.063339: step 28690, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 13h:49m:07s remains)
INFO - root - 2017-12-16 09:11:06.766065: step 28700, loss = 0.80, batch loss = 0.54 (45.2 examples/sec; 0.177 sec/batch; 14h:55m:58s remains)
2017-12-16 09:11:07.292266: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.61203527 0.442281 0.27588391 0.069577456 -0.21698117 -0.5566889 -0.93026781 -1.286852 -1.6853359 -2.0374634 -2.1366007 -1.7945437 -1.2968305 -1.1824707 -1.6146028][0.11705852 0.10323501 -0.021517277 -0.30020809 -0.61707222 -0.90382349 -1.2206117 -1.55442 -1.9455731 -2.2105606 -2.2201066 -1.8159037 -1.2812628 -1.1792949 -1.6334414][-1.027542 -0.87136459 -0.92466414 -1.1852224 -1.4100873 -1.5475445 -1.7223628 -1.9783833 -2.2859664 -2.3969262 -2.24446 -1.7022922 -1.0590248 -0.86214685 -1.2922401][-2.1179004 -1.810426 -1.8044715 -2.03011 -2.1210945 -2.0839217 -2.1299119 -2.3668516 -2.6668203 -2.6755466 -2.3536313 -1.5687797 -0.6720742 -0.25596046 -0.66793013][-2.4214308 -1.9096802 -1.8501943 -2.032325 -2.0037479 -1.7982259 -1.7823856 -2.1387522 -2.5906367 -2.7385283 -2.3933382 -1.4619741 -0.31230879 0.33845639 0.03199625][-2.0326807 -1.2150725 -1.0141968 -1.1162534 -0.98275149 -0.65307105 -0.63548005 -1.1008511 -1.7907977 -2.2720089 -2.1559634 -1.2841811 -0.015770197 0.8250556 0.63394809][-1.0221113 -0.03569746 0.29485536 0.27070856 0.43241143 0.774241 0.82351971 0.3864584 -0.485564 -1.4088383 -1.7499008 -1.2204814 -0.085893393 0.82092667 0.82649541][0.033303976 0.930058 1.3065679 1.4575012 1.6885524 2.0508828 2.2077937 1.8543448 0.85415435 -0.48017204 -1.3887966 -1.4143453 -0.68119776 0.16146064 0.42428613][0.35936904 1.0051398 1.3997235 1.7034497 2.0705438 2.525825 2.8081717 2.6016035 1.6454859 0.14222789 -1.1758981 -1.719358 -1.3994274 -0.72273326 -0.33211994][0.25498509 0.46860313 0.67696953 0.94908738 1.3235774 1.8050327 2.1559796 2.1004539 1.3415556 0.040106773 -1.2984447 -2.0717723 -1.9899905 -1.4608417 -1.0963938][0.20835423 -0.077754736 -0.29514885 -0.316679 -0.09873414 0.31923389 0.67203331 0.73084855 0.22155094 -0.69231176 -1.7047673 -2.3178439 -2.2193949 -1.8321737 -1.6027536][-0.31873655 -0.89188159 -1.3978014 -1.5974665 -1.5052516 -1.1806883 -0.87749827 -0.77483439 -1.0185947 -1.4661458 -1.9534276 -2.1915143 -1.9751772 -1.6852053 -1.628808][-1.0781704 -1.7247823 -2.2607267 -2.5197947 -2.5021822 -2.3117137 -2.082689 -1.9232945 -1.9367996 -2.0181768 -2.0676916 -1.9281104 -1.5245776 -1.2314162 -1.2717222][-1.6062703 -2.0395653 -2.4208796 -2.6310883 -2.6890869 -2.6766999 -2.5640955 -2.4214113 -2.3095996 -2.2024336 -2.0250251 -1.6613988 -1.1330593 -0.79477441 -0.87128341][-1.7854962 -1.8746815 -1.9759603 -2.1120856 -2.2397585 -2.3321323 -2.3123829 -2.2204521 -2.0956905 -1.9498498 -1.7485902 -1.3972473 -0.894184 -0.55696893 -0.6608783]]...]
INFO - root - 2017-12-16 09:11:09.000733: step 28710, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 13h:56m:37s remains)
INFO - root - 2017-12-16 09:11:10.653400: step 28720, loss = 0.50, batch loss = 0.24 (49.8 examples/sec; 0.161 sec/batch; 13h:33m:24s remains)
INFO - root - 2017-12-16 09:11:12.304525: step 28730, loss = 0.54, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 13h:50m:53s remains)
INFO - root - 2017-12-16 09:11:13.968022: step 28740, loss = 0.83, batch loss = 0.58 (47.7 examples/sec; 0.168 sec/batch; 14h:09m:15s remains)
INFO - root - 2017-12-16 09:11:15.640437: step 28750, loss = 0.65, batch loss = 0.39 (48.6 examples/sec; 0.165 sec/batch; 13h:53m:33s remains)
INFO - root - 2017-12-16 09:11:17.285805: step 28760, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:54m:51s remains)
INFO - root - 2017-12-16 09:11:18.945951: step 28770, loss = 0.56, batch loss = 0.30 (49.0 examples/sec; 0.163 sec/batch; 13h:45m:55s remains)
INFO - root - 2017-12-16 09:11:20.662050: step 28780, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 14h:11m:58s remains)
INFO - root - 2017-12-16 09:11:22.346184: step 28790, loss = 0.54, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 14h:24m:22s remains)
INFO - root - 2017-12-16 09:11:24.018952: step 28800, loss = 0.61, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 14h:09m:34s remains)
2017-12-16 09:11:24.522413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5877886 -1.6635497 -1.7995439 -1.6502345 -1.3967135 -1.437691 -1.6772054 -1.7209084 -1.3792624 -0.50660658 0.44466758 0.53156829 -0.1451385 -0.90463471 -1.2697589][-2.0016971 -1.9406689 -1.9981273 -1.8779302 -1.7112045 -1.8510439 -2.2049117 -2.3203318 -2.0327587 -1.3172632 -0.61840343 -0.66553938 -1.1366879 -1.5093377 -1.4823828][-2.2737758 -2.1691415 -2.16296 -2.0760138 -1.9870478 -2.1509 -2.5454183 -2.7130668 -2.5257864 -1.9501615 -1.4665287 -1.5695473 -1.880681 -1.9563481 -1.6463953][-1.8775296 -1.6951932 -1.6459237 -1.6444072 -1.6267445 -1.7795463 -2.1292224 -2.2900577 -2.0827549 -1.5811956 -1.2627242 -1.4474572 -1.7843014 -1.8703375 -1.601361][-0.32261896 -0.021951675 0.04069519 -0.084802866 -0.27116632 -0.48527515 -0.80924463 -0.87443149 -0.63174665 -0.27506852 -0.11870909 -0.50387347 -1.0824291 -1.469058 -1.5610886][1.4948928 1.9307745 1.9888947 1.6405962 1.2189956 0.85196447 0.62716818 0.80717182 1.2378039 1.5057108 1.4107184 0.77471805 -0.18912125 -1.0255157 -1.5631474][1.8936646 2.4207847 2.4070013 2.035187 1.6383874 1.4080203 1.5756271 2.2972896 3.0869272 3.338207 2.8775132 1.7752554 0.31866789 -0.90062058 -1.765214][0.26973653 0.7806294 0.850456 0.75938416 0.70086789 0.80617619 1.3758998 2.4231465 3.2185318 3.0923226 2.2500637 0.96328759 -0.45340288 -1.6524382 -2.4966962][-2.1324141 -1.7793952 -1.6492714 -1.4973307 -1.326813 -1.05806 -0.4591105 0.36181903 0.82986569 0.47479081 -0.38453543 -1.4164333 -2.375165 -3.0963128 -3.5558031][-3.7961609 -3.6669888 -3.5665503 -3.3711324 -3.1743298 -2.9509349 -2.5653119 -2.085952 -1.9281533 -2.2835214 -2.9132929 -3.5341659 -3.9758921 -4.1912556 -4.2519693][-4.3502946 -4.3614 -4.3275924 -4.2076006 -4.0682683 -3.9755149 -3.8420081 -3.6938024 -3.6947985 -3.9681134 -4.3154597 -4.5823479 -4.6764889 -4.5788279 -4.3166356][-3.9798255 -4.1023149 -4.1296148 -4.0707703 -4.0132604 -4.01842 -4.047616 -4.117177 -4.2241983 -4.3731551 -4.4929552 -4.4794436 -4.30287 -3.9999256 -3.5426245][-3.0821762 -3.2377586 -3.3140061 -3.3289533 -3.3635733 -3.4400873 -3.5422974 -3.6371875 -3.6920977 -3.697578 -3.6498981 -3.5230744 -3.2877686 -2.9845543 -2.5347519][-2.45639 -2.538914 -2.5962145 -2.6333029 -2.69183 -2.7642117 -2.8356066 -2.8701022 -2.8602519 -2.8103309 -2.7305441 -2.6296701 -2.4934921 -2.3424752 -2.0293849][-2.3275 -2.3397408 -2.3579226 -2.3758378 -2.3972902 -2.4102261 -2.4222012 -2.4214756 -2.4016166 -2.3692768 -2.3381286 -2.3075008 -2.2748964 -2.2333169 -2.0018942]]...]
INFO - root - 2017-12-16 09:11:26.172703: step 28810, loss = 0.54, batch loss = 0.28 (49.4 examples/sec; 0.162 sec/batch; 13h:39m:05s remains)
INFO - root - 2017-12-16 09:11:27.842814: step 28820, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 13h:45m:09s remains)
INFO - root - 2017-12-16 09:11:29.500454: step 28830, loss = 0.58, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 13h:39m:40s remains)
INFO - root - 2017-12-16 09:11:31.180769: step 28840, loss = 0.50, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 14h:10m:04s remains)
INFO - root - 2017-12-16 09:11:32.879382: step 28850, loss = 0.72, batch loss = 0.47 (47.7 examples/sec; 0.168 sec/batch; 14h:09m:20s remains)
INFO - root - 2017-12-16 09:11:34.551102: step 28860, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:05m:19s remains)
INFO - root - 2017-12-16 09:11:36.206493: step 28870, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 14h:04m:59s remains)
INFO - root - 2017-12-16 09:11:37.938141: step 28880, loss = 0.58, batch loss = 0.32 (43.2 examples/sec; 0.185 sec/batch; 15h:36m:49s remains)
INFO - root - 2017-12-16 09:11:39.609297: step 28890, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 13h:48m:16s remains)
INFO - root - 2017-12-16 09:11:41.314151: step 28900, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 14h:12m:23s remains)
2017-12-16 09:11:41.800150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6738415 -2.610847 -2.7133031 -3.049933 -3.4414048 -3.6751218 -3.6718354 -3.5423741 -3.1970742 -2.2584651 -0.92136538 0.021052599 0.42273068 0.67321396 0.48695517][-3.019886 -2.9762092 -2.996937 -3.10728 -3.1888294 -3.2471068 -3.3260894 -3.4151442 -3.3336525 -2.6634893 -1.5698094 -0.78666067 -0.39784944 0.0027339458 0.10346246][-3.2182727 -3.1942382 -3.0994859 -2.87655 -2.4654002 -2.2771916 -2.5025134 -2.9242163 -3.1819015 -2.8734045 -2.1454806 -1.5903122 -1.258612 -0.87837219 -0.67489183][-3.1663144 -3.1618798 -2.9979727 -2.3770103 -1.3977275 -0.97986495 -1.4463574 -2.2677102 -2.8751898 -2.9466176 -2.5984848 -2.249125 -1.989206 -1.704877 -1.5194925][-2.9385598 -2.9829588 -2.8526573 -1.9282488 -0.42457771 0.28880095 -0.35486531 -1.5064714 -2.4067993 -2.8436959 -2.8653471 -2.675612 -2.4646385 -2.2916772 -2.1617651][-2.720907 -2.881249 -2.8832471 -1.9558127 -0.18293428 0.89525414 0.40235615 -0.757869 -1.8103864 -2.5026193 -2.8237019 -2.8050065 -2.6212475 -2.4813042 -2.3669744][-2.6170545 -2.8738859 -3.0420542 -2.3362632 -0.72868788 0.51190686 0.35380554 -0.51164997 -1.4658167 -2.24971 -2.6589725 -2.6889532 -2.4496572 -2.2528458 -2.0828326][-2.7269578 -3.0707211 -3.300441 -2.7923598 -1.4938474 -0.36148286 -0.3283329 -0.86537731 -1.6052796 -2.2585886 -2.5586982 -2.4803877 -2.0901463 -1.7500536 -1.4770319][-2.9810719 -3.3347139 -3.5527959 -3.1601686 -2.0438559 -1.0074024 -0.91588473 -1.3861191 -2.0005167 -2.4422116 -2.5242875 -2.263521 -1.7200899 -1.241775 -0.86733246][-3.1265993 -3.4506512 -3.6412845 -3.3401086 -2.35476 -1.3641808 -1.2968774 -1.9151827 -2.5387974 -2.756433 -2.5878885 -2.1542537 -1.5099504 -1.0015873 -0.60883605][-3.1246927 -3.3158898 -3.4279113 -3.2376072 -2.4619427 -1.5180192 -1.4692465 -2.2966168 -3.0899312 -3.2786679 -2.9796379 -2.4153836 -1.742983 -1.2851275 -0.91121256][-2.9506969 -2.9543822 -2.9812436 -2.9363945 -2.4275587 -1.6038051 -1.528556 -2.4354494 -3.40034 -3.7483768 -3.5578184 -2.9962194 -2.3610547 -1.9527901 -1.5924764][-2.7002256 -2.5551021 -2.4994669 -2.5618789 -2.2843657 -1.6111088 -1.4834988 -2.3024497 -3.34483 -3.8689818 -3.8822527 -3.49578 -3.000608 -2.6522427 -2.29464][-2.473479 -2.2614851 -2.159399 -2.2388873 -2.0229495 -1.4485421 -1.2810917 -1.9354684 -2.8881361 -3.515101 -3.7294152 -3.565866 -3.2672348 -3.0253062 -2.7107747][-2.3060377 -2.0750258 -1.9405918 -1.9499505 -1.6926825 -1.211325 -1.0271882 -1.5156487 -2.2626026 -2.8086667 -3.1079755 -3.1432319 -3.0344102 -2.8909287 -2.6624944]]...]
INFO - root - 2017-12-16 09:11:43.483387: step 28910, loss = 0.46, batch loss = 0.20 (47.0 examples/sec; 0.170 sec/batch; 14h:22m:02s remains)
INFO - root - 2017-12-16 09:11:45.186690: step 28920, loss = 0.50, batch loss = 0.24 (46.0 examples/sec; 0.174 sec/batch; 14h:39m:13s remains)
INFO - root - 2017-12-16 09:11:46.911645: step 28930, loss = 0.49, batch loss = 0.23 (46.2 examples/sec; 0.173 sec/batch; 14h:36m:58s remains)
INFO - root - 2017-12-16 09:11:48.608731: step 28940, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.169 sec/batch; 14h:12m:43s remains)
INFO - root - 2017-12-16 09:11:50.275812: step 28950, loss = 0.55, batch loss = 0.29 (46.9 examples/sec; 0.171 sec/batch; 14h:23m:01s remains)
INFO - root - 2017-12-16 09:11:52.021380: step 28960, loss = 0.58, batch loss = 0.32 (44.4 examples/sec; 0.180 sec/batch; 15h:12m:09s remains)
INFO - root - 2017-12-16 09:11:53.730367: step 28970, loss = 0.54, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 14h:24m:21s remains)
INFO - root - 2017-12-16 09:11:55.406566: step 28980, loss = 0.49, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 14h:14m:21s remains)
INFO - root - 2017-12-16 09:11:57.107405: step 28990, loss = 0.61, batch loss = 0.35 (46.7 examples/sec; 0.171 sec/batch; 14h:27m:15s remains)
INFO - root - 2017-12-16 09:11:58.787784: step 29000, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 14h:21m:04s remains)
2017-12-16 09:11:59.261280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.731801 -2.8384967 -3.0441732 -3.083416 -2.9562201 -2.6790142 -2.2974112 -2.0700631 -2.1653144 -2.2976844 -2.3631876 -2.4266882 -2.4465246 -2.2904205 -2.0545342][-2.3437281 -2.5287962 -2.6079831 -2.5129733 -2.2913806 -2.0069592 -1.6351705 -1.4458015 -1.664048 -1.9503196 -2.1666148 -2.3230693 -2.3978167 -2.1904376 -1.8349557][-1.4889807 -1.8286096 -1.8608587 -1.667979 -1.3755589 -1.1507945 -0.93935061 -0.9668411 -1.3434759 -1.8363408 -2.25951 -2.5602767 -2.6814077 -2.4326668 -2.0108209][-0.72481084 -1.2146657 -1.1846629 -0.8711524 -0.54269242 -0.43158042 -0.4149375 -0.61170256 -1.145671 -1.7960899 -2.4166362 -2.9307003 -3.1503856 -2.9829063 -2.5913363][-0.38164639 -0.94013751 -0.80653405 -0.28163505 0.11957645 0.2092104 0.20387912 0.10291624 -0.34316206 -1.0332062 -1.8380964 -2.6382186 -3.1343746 -3.2805362 -3.0647912][-0.65366197 -1.2217743 -0.9789983 -0.21761584 0.34400511 0.61801982 0.79958725 0.92148447 0.70977855 0.20637727 -0.49147487 -1.3790939 -2.2307527 -2.8345082 -2.9272375][-1.6955082 -2.1831722 -1.819324 -0.89688921 -0.18927073 0.23923326 0.63769221 0.91652703 1.0805247 1.0619583 0.80751395 0.020272255 -1.0989068 -2.0505924 -2.3858912][-3.0125432 -3.3090749 -2.8247173 -1.9183521 -1.1886983 -0.66279912 -0.17145753 0.13624692 0.53776431 1.0380082 1.3225026 0.74929881 -0.45237482 -1.5330902 -1.9613669][-3.6645815 -3.7526088 -3.2155802 -2.3989224 -1.7376533 -1.304324 -0.97371054 -0.83938742 -0.41908753 0.44434595 1.1699383 0.82082462 -0.21836948 -1.1572794 -1.4734681][-3.6813655 -3.5465181 -3.0355468 -2.3637803 -1.8556727 -1.597017 -1.5717652 -1.8137577 -1.5133343 -0.53520858 0.34925795 0.1645062 -0.49020171 -1.0448685 -1.1797493][-3.2204278 -2.9486594 -2.52983 -2.0206761 -1.6541582 -1.5490692 -1.884271 -2.5519361 -2.5004435 -1.7389395 -1.0450143 -1.1530199 -1.4546022 -1.6305338 -1.5983306][-2.4677901 -2.1330111 -1.7490629 -1.2565225 -0.86888921 -0.8484093 -1.551019 -2.6475854 -2.9936113 -2.67745 -2.434648 -2.5986276 -2.6867595 -2.6417706 -2.5823708][-1.7552993 -1.37559 -0.99697649 -0.45000231 0.062981129 0.12871838 -0.6811986 -1.9395421 -2.6311781 -2.8311164 -3.1388535 -3.5377157 -3.6572154 -3.6110928 -3.6083884][-1.2367855 -0.84428394 -0.432783 0.22530985 0.93625426 1.2340591 0.56231427 -0.66671705 -1.602861 -2.3212862 -3.1672304 -3.9416842 -4.2926331 -4.4055424 -4.5296025][-0.90577865 -0.52400279 -0.099709749 0.60710335 1.4512658 1.9424498 1.5040724 0.47191453 -0.51560354 -1.5492312 -2.729353 -3.7873366 -4.3725567 -4.68552 -4.9596786]]...]
INFO - root - 2017-12-16 09:12:00.951394: step 29010, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 14h:13m:30s remains)
INFO - root - 2017-12-16 09:12:02.622045: step 29020, loss = 0.55, batch loss = 0.29 (49.1 examples/sec; 0.163 sec/batch; 13h:43m:36s remains)
INFO - root - 2017-12-16 09:12:04.312987: step 29030, loss = 0.58, batch loss = 0.32 (45.7 examples/sec; 0.175 sec/batch; 14h:45m:56s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:12:05.992404: step 29040, loss = 0.70, batch loss = 0.44 (45.5 examples/sec; 0.176 sec/batch; 14h:48m:33s remains)
INFO - root - 2017-12-16 09:12:07.676749: step 29050, loss = 0.49, batch loss = 0.23 (48.9 examples/sec; 0.163 sec/batch; 13h:46m:38s remains)
INFO - root - 2017-12-16 09:12:09.353148: step 29060, loss = 0.55, batch loss = 0.30 (46.6 examples/sec; 0.172 sec/batch; 14h:27m:22s remains)
INFO - root - 2017-12-16 09:12:11.063927: step 29070, loss = 0.51, batch loss = 0.25 (47.3 examples/sec; 0.169 sec/batch; 14h:15m:23s remains)
INFO - root - 2017-12-16 09:12:12.736087: step 29080, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:10m:10s remains)
INFO - root - 2017-12-16 09:12:14.387662: step 29090, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 13h:47m:55s remains)
INFO - root - 2017-12-16 09:12:16.055098: step 29100, loss = 0.50, batch loss = 0.25 (41.2 examples/sec; 0.194 sec/batch; 16h:21m:43s remains)
2017-12-16 09:12:16.538975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5704784 -2.0208867 -1.1608125 -0.75784218 -0.90459371 -1.4466083 -2.1830103 -2.70491 -2.7245314 -2.4893003 -2.4809148 -2.7739801 -2.9372392 -2.9104064 -2.9485064][-2.5891285 -2.27024 -1.7126808 -1.4368421 -1.5128441 -1.6731822 -1.8603412 -1.9992504 -1.9782043 -1.85923 -2.00305 -2.2850323 -2.3700185 -2.3250647 -2.4960206][-2.5063906 -2.3340337 -2.0118701 -1.8507049 -1.7957666 -1.5507319 -1.1078469 -0.82725656 -0.79240739 -0.91840887 -1.2631091 -1.512351 -1.5216167 -1.5302904 -1.8431573][-2.5248179 -2.3106396 -2.0633974 -1.8969846 -1.6396339 -0.89600658 0.16786242 0.75438714 0.62649441 0.098230124 -0.52703476 -0.80644393 -0.761675 -0.75583136 -1.0707909][-2.7006226 -2.3089895 -1.9726429 -1.6993713 -1.2035452 -0.008327961 1.498112 2.2253375 1.8306971 0.83327627 -0.0770638 -0.40493202 -0.27876854 -0.24646735 -0.44809115][-2.7302752 -2.1809759 -1.7630111 -1.4178398 -0.66224062 0.95146275 2.7978892 3.5475583 2.8966255 1.4150548 0.17605233 -0.31464982 -0.3058784 -0.26030922 -0.24090385][-2.4742429 -1.8643514 -1.4605035 -1.0421069 -0.070353031 1.8737965 3.9000182 4.5988855 3.8405576 2.0073972 0.36210847 -0.54192376 -0.81791031 -0.80969906 -0.65170574][-2.1653275 -1.6638492 -1.3424207 -0.99489248 0.037760496 2.0704722 4.1739306 4.9659238 4.2787781 2.2871737 0.18503976 -1.1826864 -1.768535 -1.8290508 -1.5457144][-2.1827576 -1.8560292 -1.707742 -1.5512717 -0.63579214 1.3798323 3.5594063 4.5509157 4.0853095 2.1176081 -0.2904408 -1.9824407 -2.7841897 -2.8566206 -2.5415938][-2.4840665 -2.365041 -2.4241333 -2.4552526 -1.6842268 0.23655224 2.3103542 3.3338017 3.1260657 1.4764428 -0.9088161 -2.6960788 -3.5833325 -3.6392155 -3.3988218][-2.8191464 -2.9106605 -3.2049522 -3.4340909 -2.85443 -1.1389116 0.68913341 1.669704 1.7554622 0.55393028 -1.5552571 -3.2341442 -4.1079993 -4.2138915 -4.138093][-3.1457133 -3.4759121 -3.9868038 -4.3457813 -3.948081 -2.4878647 -0.96496749 -0.096315861 0.14484644 -0.68586481 -2.3597505 -3.7477565 -4.4164205 -4.4741211 -4.5272484][-3.4683981 -3.8817713 -4.4221373 -4.8326349 -4.6194654 -3.5525627 -2.4405153 -1.812856 -1.5952849 -2.0863645 -3.1863949 -4.058919 -4.3516726 -4.2807856 -4.3874383][-3.41052 -3.7399538 -4.1917119 -4.6116304 -4.6218257 -4.0441566 -3.4164062 -3.0609336 -2.9088175 -3.1381664 -3.6308794 -3.8645923 -3.7649245 -3.6667638 -3.8871362][-2.7534034 -2.9458647 -3.3035541 -3.768765 -4.0384197 -3.8535788 -3.55836 -3.3813663 -3.3189366 -3.3410587 -3.297497 -3.02005 -2.7621121 -2.8425386 -3.2837541]]...]
INFO - root - 2017-12-16 09:12:18.218880: step 29110, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.168 sec/batch; 14h:07m:00s remains)
INFO - root - 2017-12-16 09:12:19.873585: step 29120, loss = 0.47, batch loss = 0.21 (48.6 examples/sec; 0.165 sec/batch; 13h:51m:53s remains)
INFO - root - 2017-12-16 09:12:21.566033: step 29130, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 14h:05m:14s remains)
INFO - root - 2017-12-16 09:12:23.238708: step 29140, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 14h:15m:52s remains)
INFO - root - 2017-12-16 09:12:24.916816: step 29150, loss = 0.53, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 14h:14m:58s remains)
INFO - root - 2017-12-16 09:12:26.593525: step 29160, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 13h:54m:02s remains)
INFO - root - 2017-12-16 09:12:28.268950: step 29170, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:53m:14s remains)
INFO - root - 2017-12-16 09:12:29.946356: step 29180, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:58m:56s remains)
INFO - root - 2017-12-16 09:12:31.611495: step 29190, loss = 0.51, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 13h:39m:58s remains)
INFO - root - 2017-12-16 09:12:33.246232: step 29200, loss = 0.64, batch loss = 0.38 (48.9 examples/sec; 0.163 sec/batch; 13h:46m:22s remains)
2017-12-16 09:12:33.695137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8969398 -2.0072269 -2.1170409 -2.2161777 -2.3675294 -2.5675714 -2.7631114 -2.8737626 -2.9075928 -2.8808844 -2.7801387 -2.6295929 -2.4531696 -2.2859282 -2.1196318][-2.2725973 -2.4758961 -2.6807759 -2.8257079 -3.01467 -3.2888196 -3.5504045 -3.6652255 -3.654552 -3.6227341 -3.5185637 -3.2910409 -2.9938686 -2.7175634 -2.4549923][-2.8502083 -3.1300955 -3.3894663 -3.4873431 -3.5777011 -3.865386 -4.1660547 -4.2599282 -4.2447634 -4.3259211 -4.3414984 -4.1135712 -3.7183218 -3.3240843 -2.9329519][-3.3869662 -3.6624775 -3.8048029 -3.6124926 -3.368371 -3.4856305 -3.7305875 -3.7870593 -3.8607039 -4.2550397 -4.5747762 -4.5131545 -4.2301888 -3.8834548 -3.4098246][-3.72318 -3.8344123 -3.6151257 -2.8996203 -2.1441035 -1.8492069 -1.7843087 -1.7284693 -2.0265179 -2.9485791 -3.7677767 -4.1036048 -4.1845012 -4.0933805 -3.6576023][-3.7441437 -3.6235075 -2.9461756 -1.6364064 -0.23160124 0.68353224 1.3199072 1.7036326 1.0717335 -0.59610677 -1.9958497 -2.7950447 -3.3948877 -3.7600646 -3.5211425][-3.4423223 -3.1459594 -2.1335073 -0.34429002 1.5427811 3.0720441 4.5749569 5.5071011 4.3388691 2.0085347 0.24700689 -0.92680907 -2.0348921 -2.9003429 -2.9635005][-3.1203175 -2.7796285 -1.6659046 0.19302154 2.257385 4.1866827 6.4449587 8.15857 6.322382 3.6632192 1.9143007 0.55782175 -0.884809 -2.0372055 -2.3692973][-3.1157374 -2.9636915 -2.0346065 -0.47819507 1.2518137 2.9537561 4.8332968 6.0178137 5.0327473 3.1413381 1.8998063 0.80701661 -0.59637153 -1.7838423 -2.1970372][-3.3412869 -3.4235373 -2.9458501 -1.9892063 -0.82353592 0.36822224 1.5708673 2.2518094 1.8550184 0.92519927 0.36636686 -0.20181489 -1.2484078 -2.1621692 -2.473572][-3.5308285 -3.8339949 -3.7662196 -3.3634908 -2.7744062 -2.1450071 -1.5237615 -1.1692134 -1.2619854 -1.5570142 -1.6236391 -1.79746 -2.3895431 -2.8618562 -2.940907][-3.5200248 -3.934166 -4.1219106 -4.1094475 -3.9085441 -3.6700265 -3.4733536 -3.3261023 -3.2237754 -3.1577747 -3.0253432 -3.019418 -3.2435229 -3.3675523 -3.2683337][-3.213774 -3.6113629 -3.9072986 -4.0878263 -4.1147838 -4.0960212 -4.1366835 -4.1028914 -3.9299731 -3.7520185 -3.5810556 -3.4961371 -3.5133038 -3.4543533 -3.2557805][-2.7233396 -3.0220945 -3.2839539 -3.4757895 -3.5847917 -3.6612058 -3.7515111 -3.7570162 -3.643189 -3.4869108 -3.3522277 -3.2615 -3.2130885 -3.1021941 -2.9114978][-2.2534149 -2.4235454 -2.5721428 -2.6873875 -2.7751091 -2.8485551 -2.9080219 -2.91496 -2.8551896 -2.7891512 -2.721081 -2.6676285 -2.6246166 -2.5474939 -2.4402955]]...]
INFO - root - 2017-12-16 09:12:35.357437: step 29210, loss = 0.53, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 13h:50m:49s remains)
INFO - root - 2017-12-16 09:12:37.032208: step 29220, loss = 0.52, batch loss = 0.27 (48.5 examples/sec; 0.165 sec/batch; 13h:54m:09s remains)
INFO - root - 2017-12-16 09:12:38.681522: step 29230, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.168 sec/batch; 14h:11m:00s remains)
INFO - root - 2017-12-16 09:12:40.358532: step 29240, loss = 0.52, batch loss = 0.26 (45.0 examples/sec; 0.178 sec/batch; 14h:57m:52s remains)
INFO - root - 2017-12-16 09:12:42.021866: step 29250, loss = 0.67, batch loss = 0.41 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:45s remains)
INFO - root - 2017-12-16 09:12:43.686330: step 29260, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:58m:19s remains)
INFO - root - 2017-12-16 09:12:45.376395: step 29270, loss = 0.50, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:12m:11s remains)
INFO - root - 2017-12-16 09:12:47.014228: step 29280, loss = 0.54, batch loss = 0.29 (50.4 examples/sec; 0.159 sec/batch; 13h:22m:56s remains)
INFO - root - 2017-12-16 09:12:48.670307: step 29290, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 13h:51m:54s remains)
INFO - root - 2017-12-16 09:12:50.314336: step 29300, loss = 0.49, batch loss = 0.24 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:58s remains)
2017-12-16 09:12:50.773641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3232806 -1.2906246 -1.2696617 -1.2495338 -1.2115982 -1.1662676 -1.1622413 -1.2305684 -1.3259554 -1.3949642 -1.4185948 -1.4118979 -1.4026431 -1.4330665 -1.5017149][-0.96355915 -0.97175419 -1.0135572 -1.0445585 -1.0147117 -0.93937945 -0.91964376 -1.0259854 -1.1875933 -1.3032224 -1.3379425 -1.3081063 -1.2624847 -1.2864456 -1.3851457][-0.66116405 -0.74065578 -0.87725878 -0.96932638 -0.92060113 -0.76371288 -0.66016352 -0.75562155 -0.98892152 -1.1990211 -1.2931893 -1.2681764 -1.1835669 -1.1739829 -1.2798264][-0.455235 -0.59512424 -0.8099395 -0.92066944 -0.76336479 -0.40689707 -0.10306597 -0.11209321 -0.45255613 -0.87795711 -1.1677293 -1.2497504 -1.1824572 -1.1496722 -1.2523848][-0.1859901 -0.32435179 -0.57225883 -0.64332211 -0.32324767 0.31783962 0.93286705 1.0915668 0.60235286 -0.17766047 -0.8220911 -1.1380212 -1.1822171 -1.1907613 -1.3036594][0.41582274 0.34267259 0.07167387 0.0044364929 0.43385506 1.3369226 2.3424246 2.764992 2.1043298 0.9147377 -0.17930484 -0.8283155 -1.0877581 -1.2278816 -1.3875035][1.2425942 1.2428949 0.93399143 0.83248973 1.2969637 2.3875272 3.76875 4.5367041 3.7228034 2.1523497 0.66435719 -0.32676578 -0.84334242 -1.1532482 -1.4013612][1.9974973 2.0559332 1.6728981 1.4704154 1.8850253 3.0173228 4.5771503 5.6564884 4.7531052 3.0122411 1.3806882 0.21963263 -0.48399663 -0.95071316 -1.2997186][2.1636093 2.2327092 1.7741611 1.4497349 1.6986701 2.6212194 3.8922489 4.7670212 4.2815323 2.9407561 1.5681314 0.51263142 -0.2325871 -0.79956305 -1.2271677][1.4329855 1.4216881 0.920455 0.524142 0.60558295 1.2255721 2.0926869 2.7435839 2.6485837 1.9063866 1.0013311 0.24663877 -0.35939169 -0.8919915 -1.3198978][0.12505102 0.0044982433 -0.494313 -0.90345466 -0.93914449 -0.60437787 -0.10199976 0.3371377 0.45126987 0.19761825 -0.20983672 -0.575943 -0.89688969 -1.2383511 -1.5547061][-1.2407135 -1.4503531 -1.9156779 -2.3016005 -2.4219987 -2.2951274 -2.0731721 -1.8146502 -1.6265614 -1.5762917 -1.6034722 -1.6212559 -1.6384382 -1.7401476 -1.8755271][-2.2176869 -2.4770575 -2.8692167 -3.2044842 -3.3542724 -3.3621538 -3.314604 -3.1759677 -2.9816539 -2.7830698 -2.5976119 -2.4037893 -2.2238171 -2.1401603 -2.1146629][-2.6044378 -2.8409576 -3.1388452 -3.3989937 -3.5498834 -3.6173944 -3.6376734 -3.5631814 -3.3960814 -3.1789165 -2.9426723 -2.6889925 -2.4494438 -2.2851958 -2.1740005][-2.5479207 -2.7310052 -2.9151993 -3.0701756 -3.1800232 -3.2515669 -3.2890754 -3.2515724 -3.1395843 -2.9719276 -2.7742491 -2.5627553 -2.3607202 -2.2048838 -2.0854177]]...]
INFO - root - 2017-12-16 09:12:52.441788: step 29310, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 14h:02m:48s remains)
INFO - root - 2017-12-16 09:12:54.128707: step 29320, loss = 0.51, batch loss = 0.25 (49.7 examples/sec; 0.161 sec/batch; 13h:33m:32s remains)
INFO - root - 2017-12-16 09:12:55.780466: step 29330, loss = 0.45, batch loss = 0.19 (48.3 examples/sec; 0.166 sec/batch; 13h:56m:29s remains)
INFO - root - 2017-12-16 09:12:57.408840: step 29340, loss = 0.67, batch loss = 0.41 (48.8 examples/sec; 0.164 sec/batch; 13h:48m:10s remains)
INFO - root - 2017-12-16 09:12:59.067812: step 29350, loss = 0.53, batch loss = 0.27 (49.7 examples/sec; 0.161 sec/batch; 13h:33m:52s remains)
INFO - root - 2017-12-16 09:13:00.742321: step 29360, loss = 0.61, batch loss = 0.35 (49.5 examples/sec; 0.162 sec/batch; 13h:36m:39s remains)
INFO - root - 2017-12-16 09:13:02.446124: step 29370, loss = 0.51, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 14h:06m:12s remains)
INFO - root - 2017-12-16 09:13:04.141804: step 29380, loss = 0.52, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:17m:32s remains)
INFO - root - 2017-12-16 09:13:05.781029: step 29390, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 13h:44m:07s remains)
INFO - root - 2017-12-16 09:13:07.445549: step 29400, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 14h:12m:04s remains)
2017-12-16 09:13:07.989282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.523515 -1.5794981 -1.6442024 -1.7030649 -1.7605271 -1.7113581 -1.3442641 -1.0234216 -1.2607635 -1.8493979 -2.3978999 -2.8770671 -3.1148491 -2.9698019 -2.5740201][-2.8238082 -2.9140515 -3.0051584 -3.073328 -3.1162684 -3.0645914 -2.7273674 -2.4153953 -2.5808015 -3.0437732 -3.4876542 -3.8247595 -3.8423364 -3.4465981 -2.8318572][-3.8280482 -3.9424722 -4.0382514 -4.0682325 -4.0334435 -3.9429097 -3.6262155 -3.3241906 -3.4196692 -3.836369 -4.3060818 -4.6267285 -4.5030437 -3.9085221 -3.1386254][-4.2356973 -4.2870712 -4.3043313 -4.2049513 -4.008914 -3.8025377 -3.4788206 -3.1999583 -3.3147449 -3.8198388 -4.4223127 -4.8133717 -4.6758652 -4.0149 -3.2133632][-3.8396778 -3.7779722 -3.6346536 -3.2916894 -2.8517332 -2.4838183 -2.064589 -1.7791408 -1.9920926 -2.6980186 -3.5277128 -4.0962429 -4.0833945 -3.527359 -2.8505542][-2.6096003 -2.4398811 -2.0991054 -1.4598799 -0.77226806 -0.22890472 0.34020066 0.66958451 0.2682929 -0.75982344 -1.8476976 -2.652313 -2.919631 -2.6378279 -2.1948128][-1.0596508 -0.90299392 -0.48266363 0.31510949 1.0973575 1.6619215 2.3008761 2.6503363 2.0927324 0.83086944 -0.41394436 -1.3682157 -1.8905724 -1.9020083 -1.6718755][0.21778274 0.081099987 0.25388217 0.81992054 1.387825 1.7891154 2.3719902 2.6969786 2.0552468 0.74431229 -0.46099377 -1.3433149 -1.8282382 -1.8479964 -1.6343595][0.64530611 0.084418774 -0.19898725 -0.10636997 0.062967062 0.18187881 0.55644965 0.78032994 0.21309471 -0.89057696 -1.8627205 -2.4932461 -2.6819825 -2.4635091 -2.0763724][-0.0070912838 -0.89671862 -1.5566609 -1.8958826 -2.0644505 -2.1563742 -1.9833063 -1.8282969 -2.1636686 -2.8730445 -3.5206707 -3.8542008 -3.7122605 -3.1995983 -2.6114712][-1.1527947 -2.0767403 -2.8711827 -3.4086642 -3.7285013 -3.8718839 -3.7646441 -3.5707304 -3.6461148 -4.0023527 -4.3888216 -4.5266027 -4.1875792 -3.5145731 -2.8381755][-1.8817788 -2.535639 -3.1642969 -3.6440711 -3.9074416 -3.9468987 -3.7857792 -3.542244 -3.4779296 -3.6847749 -4.0064678 -4.1465569 -3.8467708 -3.2321267 -2.6474426][-1.8210714 -2.0902705 -2.3858066 -2.6196122 -2.7073379 -2.6245995 -2.4099784 -2.1845012 -2.1661749 -2.4198871 -2.8445423 -3.1377344 -3.0556502 -2.6708083 -2.3044159][-1.2952963 -1.1336222 -1.0169991 -0.9472214 -0.90227938 -0.79972279 -0.61544538 -0.50431061 -0.66977024 -1.1057975 -1.6854161 -2.1563847 -2.3289723 -2.2068079 -2.0564306][-0.7204318 -0.27937365 0.066387177 0.28622127 0.38430691 0.41618466 0.44298577 0.3721025 0.044197798 -0.52399659 -1.1773914 -1.7360268 -2.0039997 -1.9977069 -1.973382]]...]
INFO - root - 2017-12-16 09:13:09.699963: step 29410, loss = 0.75, batch loss = 0.49 (49.0 examples/sec; 0.163 sec/batch; 13h:44m:31s remains)
INFO - root - 2017-12-16 09:13:11.366824: step 29420, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 14h:10m:47s remains)
INFO - root - 2017-12-16 09:13:13.057879: step 29430, loss = 0.53, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:17m:32s remains)
INFO - root - 2017-12-16 09:13:14.726902: step 29440, loss = 0.46, batch loss = 0.21 (48.5 examples/sec; 0.165 sec/batch; 13h:53m:56s remains)
INFO - root - 2017-12-16 09:13:16.390495: step 29450, loss = 0.63, batch loss = 0.37 (47.0 examples/sec; 0.170 sec/batch; 14h:18m:57s remains)
INFO - root - 2017-12-16 09:13:18.047487: step 29460, loss = 0.48, batch loss = 0.23 (49.7 examples/sec; 0.161 sec/batch; 13h:32m:17s remains)
INFO - root - 2017-12-16 09:13:19.696458: step 29470, loss = 0.61, batch loss = 0.35 (47.5 examples/sec; 0.168 sec/batch; 14h:10m:56s remains)
INFO - root - 2017-12-16 09:13:21.377963: step 29480, loss = 0.57, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 14h:18m:53s remains)
INFO - root - 2017-12-16 09:13:23.090838: step 29490, loss = 0.71, batch loss = 0.45 (46.4 examples/sec; 0.172 sec/batch; 14h:30m:09s remains)
INFO - root - 2017-12-16 09:13:24.772208: step 29500, loss = 0.53, batch loss = 0.27 (46.4 examples/sec; 0.172 sec/batch; 14h:29m:55s remains)
2017-12-16 09:13:25.210685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7803743 -2.0410602 -2.3513432 -2.6004891 -2.654907 -2.5138562 -2.2858357 -2.0995655 -1.9918332 -1.90095 -1.8498827 -1.8673744 -1.8947059 -1.9201405 -1.9459264][-1.9533449 -2.3650784 -2.8739197 -3.3142529 -3.471184 -3.3435521 -3.0922918 -2.8605578 -2.7346776 -2.6418355 -2.5753422 -2.6105251 -2.6147404 -2.5931854 -2.6090159][-2.0789561 -2.5666332 -3.15343 -3.6401162 -3.8087349 -3.6701143 -3.4541998 -3.303458 -3.3360391 -3.3854158 -3.4193153 -3.4940333 -3.4854374 -3.380455 -3.3501794][-2.0924633 -2.4645998 -2.8512518 -3.1171324 -3.0594785 -2.803972 -2.6374695 -2.7491462 -3.1403928 -3.5734375 -3.9082549 -4.1486478 -4.1999173 -4.02634 -3.848443][-1.961427 -2.0588028 -2.0040448 -1.8249898 -1.3844793 -0.84904647 -0.60177255 -0.92284358 -1.7383301 -2.7916827 -3.6524942 -4.1806479 -4.35902 -4.118844 -3.7601714][-1.7639134 -1.5981835 -1.1058613 -0.423285 0.49056697 1.5137165 2.0549176 1.6957877 0.52750444 -1.0360379 -2.4250717 -3.2645032 -3.5222926 -3.2301335 -2.7711329][-1.6805038 -1.3793181 -0.63079584 0.47577095 1.9196165 3.486059 4.5221348 4.4259644 3.1417038 1.1489229 -0.6878047 -1.7686474 -1.9810605 -1.6147825 -1.1614541][-1.7792578 -1.5292535 -0.88340056 0.24998045 1.8928583 3.9180286 5.6557693 6.2064619 5.1396923 2.9272921 0.752604 -0.4737395 -0.64017785 -0.17151666 0.2393291][-2.0198407 -1.9886259 -1.7032484 -0.93211722 0.46988511 2.5660822 4.6181984 5.7056322 4.971818 2.9655364 0.91223884 -0.21669197 -0.22837305 0.31508636 0.6288662][-2.314158 -2.6332788 -2.7604778 -2.4336271 -1.3862374 0.41480637 2.3102276 3.3802278 2.8368251 1.2262261 -0.27652144 -0.98580575 -0.82149494 -0.26615095 -0.014044046][-2.5874493 -3.2052605 -3.7075446 -3.7508392 -3.0420685 -1.6536183 -0.23236442 0.45789194 -0.076711655 -1.269973 -2.1334596 -2.3423111 -1.9867713 -1.4930763 -1.3407564][-2.7712791 -3.5093346 -4.2199612 -4.4871607 -4.0537896 -3.0679152 -2.1592088 -1.9413337 -2.5599754 -3.387383 -3.8219433 -3.6825185 -3.2492924 -2.8625743 -2.7547202][-2.7519999 -3.4538877 -4.16783 -4.4966345 -4.23256 -3.5776997 -3.0826507 -3.2182696 -3.8504515 -4.4597626 -4.6290245 -4.337306 -3.9287372 -3.6691432 -3.6088748][-2.5067723 -3.0589137 -3.6129096 -3.9103706 -3.7705584 -3.3883624 -3.2075415 -3.4579024 -3.9446 -4.3425617 -4.3888903 -4.1408205 -3.8482656 -3.6810164 -3.6510062][-2.1777272 -2.5474677 -2.933744 -3.1665177 -3.1381414 -2.9789968 -2.9516594 -3.1575425 -3.4491434 -3.6243725 -3.5756907 -3.3794231 -3.1805291 -3.0718534 -3.0591438]]...]
INFO - root - 2017-12-16 09:13:26.858958: step 29510, loss = 0.54, batch loss = 0.28 (50.4 examples/sec; 0.159 sec/batch; 13h:21m:29s remains)
INFO - root - 2017-12-16 09:13:28.510674: step 29520, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:53m:45s remains)
INFO - root - 2017-12-16 09:13:30.189177: step 29530, loss = 0.49, batch loss = 0.23 (47.5 examples/sec; 0.168 sec/batch; 14h:09m:55s remains)
INFO - root - 2017-12-16 09:13:31.842687: step 29540, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 13h:53m:00s remains)
INFO - root - 2017-12-16 09:13:33.498036: step 29550, loss = 0.53, batch loss = 0.28 (49.2 examples/sec; 0.163 sec/batch; 13h:40m:40s remains)
INFO - root - 2017-12-16 09:13:35.183691: step 29560, loss = 0.59, batch loss = 0.33 (47.1 examples/sec; 0.170 sec/batch; 14h:17m:34s remains)
INFO - root - 2017-12-16 09:13:36.852133: step 29570, loss = 0.59, batch loss = 0.33 (46.1 examples/sec; 0.173 sec/batch; 14h:35m:41s remains)
INFO - root - 2017-12-16 09:13:38.556807: step 29580, loss = 0.54, batch loss = 0.29 (45.1 examples/sec; 0.177 sec/batch; 14h:55m:55s remains)
INFO - root - 2017-12-16 09:13:40.219978: step 29590, loss = 0.53, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:44m:55s remains)
INFO - root - 2017-12-16 09:13:41.892078: step 29600, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 13h:58m:40s remains)
2017-12-16 09:13:42.368284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3255718 -3.1562109 -3.2585981 -3.4821162 -3.5341969 -3.4904795 -3.4406316 -3.325772 -3.1450014 -2.967963 -2.7262788 -2.3867073 -2.0913751 -1.7833523 -1.4237669][-2.8570194 -2.7582469 -3.0453033 -3.3929634 -3.4700818 -3.4680848 -3.5659523 -3.6301465 -3.5739608 -3.498539 -3.3683057 -3.1423883 -2.9772844 -2.7615008 -2.467972][-1.8114965 -1.7148695 -2.1196048 -2.5225465 -2.5354738 -2.5019438 -2.7432277 -3.0870097 -3.3046207 -3.390738 -3.4102638 -3.4028749 -3.4465339 -3.3807757 -3.2446196][-0.836957 -0.6647867 -0.98962796 -1.2217203 -1.0829234 -0.95029771 -1.2286434 -1.765517 -2.2382519 -2.4866662 -2.7033024 -3.0135043 -3.333436 -3.433322 -3.4384007][-0.24204206 0.073488474 0.031362057 0.17454982 0.61617756 1.0194123 1.0146837 0.49393392 -0.22050023 -0.84968615 -1.3897803 -2.1017911 -2.7995574 -3.1096237 -3.2195773][0.1364677 0.65704012 1.0120184 1.5141771 2.2608941 3.0484359 3.5936744 3.3826225 2.4110425 1.2997096 0.35694742 -0.83302271 -2.08105 -2.7354047 -2.9959755][0.39899683 1.0379119 1.5611579 2.2819312 3.2854311 4.5103445 5.7854862 6.0913858 4.9059019 3.268265 1.8530095 0.24957037 -1.4577297 -2.5003259 -2.9185338][0.29427719 0.8746767 1.277385 2.0829446 3.1688216 4.6018934 6.4023323 7.1171007 5.8656006 4.02404 2.4610617 0.65996647 -1.2451057 -2.461683 -2.8918889][-0.20022392 0.1378057 0.239923 0.87860131 1.9161718 3.2991259 4.9659042 5.5563602 4.6375513 3.217097 1.8901784 0.24703431 -1.4829974 -2.6130493 -2.9826803][-1.1635487 -1.1448539 -1.3272972 -0.92389607 -0.036649942 1.0661833 2.1987522 2.639632 2.1960833 1.4433255 0.55201578 -0.78840148 -2.1533315 -2.9910693 -3.1820939][-2.5484486 -2.7428799 -3.0806053 -2.8615701 -2.2055762 -1.4890113 -0.84609377 -0.55175519 -0.64460182 -0.79761827 -1.1376368 -2.0330045 -2.9934087 -3.4470949 -3.3805022][-3.7407131 -4.0294619 -4.44259 -4.3996916 -4.0348477 -3.6521609 -3.2836726 -3.1094773 -3.1134667 -2.9422498 -2.8475404 -3.2415719 -3.7783284 -3.8740306 -3.5388725][-4.2416644 -4.5619888 -5.0070906 -5.1450233 -5.039144 -4.831501 -4.5761805 -4.5107861 -4.5684981 -4.3868484 -4.1166706 -4.1379123 -4.3242497 -4.1850338 -3.6854577][-4.032321 -4.3445935 -4.7513323 -4.9681253 -4.9568777 -4.795239 -4.6197371 -4.6465158 -4.7789836 -4.6692543 -4.4042182 -4.2605948 -4.2417541 -4.0087962 -3.4979193][-3.3783424 -3.6027765 -3.8719954 -4.0306921 -4.0172505 -3.8883767 -3.7899647 -3.8679957 -4.0143347 -3.9585509 -3.7579427 -3.5880601 -3.5065808 -3.2984209 -2.9168341]]...]
INFO - root - 2017-12-16 09:13:44.012781: step 29610, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:51m:58s remains)
INFO - root - 2017-12-16 09:13:45.662539: step 29620, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 14h:07m:20s remains)
INFO - root - 2017-12-16 09:13:47.321468: step 29630, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 14h:14m:06s remains)
INFO - root - 2017-12-16 09:13:49.004763: step 29640, loss = 0.49, batch loss = 0.24 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:46s remains)
INFO - root - 2017-12-16 09:13:50.657228: step 29650, loss = 0.46, batch loss = 0.20 (48.7 examples/sec; 0.164 sec/batch; 13h:48m:48s remains)
INFO - root - 2017-12-16 09:13:52.344135: step 29660, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:48m:07s remains)
INFO - root - 2017-12-16 09:13:54.009594: step 29670, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 13h:50m:58s remains)
INFO - root - 2017-12-16 09:13:55.662774: step 29680, loss = 0.70, batch loss = 0.44 (49.6 examples/sec; 0.161 sec/batch; 13h:34m:31s remains)
INFO - root - 2017-12-16 09:13:57.289928: step 29690, loss = 0.61, batch loss = 0.35 (50.1 examples/sec; 0.160 sec/batch; 13h:25m:52s remains)
INFO - root - 2017-12-16 09:13:58.959759: step 29700, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:53s remains)
2017-12-16 09:13:59.382251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9764401 -1.9811599 -2.0786741 -2.2489731 -2.4145925 -2.5529609 -2.6696486 -2.765703 -2.8110347 -2.7421119 -2.5999877 -2.4679389 -2.3901949 -2.3726075 -2.4451251][-1.8803394 -1.9375842 -2.1466551 -2.4574759 -2.7223349 -2.8971329 -3.0075231 -3.0740843 -3.0478487 -2.8609953 -2.5904799 -2.4219933 -2.3662186 -2.346487 -2.3921912][-1.9495562 -2.1001129 -2.4394381 -2.8666992 -3.1770134 -3.3087306 -3.3093123 -3.2187281 -3.054018 -2.7348788 -2.3832326 -2.2918453 -2.3663151 -2.4028125 -2.3913248][-2.0968592 -2.322782 -2.7205465 -3.1582603 -3.369545 -3.3191185 -3.0773659 -2.732811 -2.3725917 -1.9647281 -1.6917899 -1.8856936 -2.2584248 -2.4464185 -2.4262481][-2.1938224 -2.3370357 -2.6402938 -2.8811078 -2.8052959 -2.3769441 -1.7584121 -1.1636003 -0.66390836 -0.37387061 -0.44389069 -1.0733556 -1.8125892 -2.2223513 -2.3227475][-2.070013 -2.024472 -2.0263088 -1.8417521 -1.259418 -0.21986365 0.93588114 1.7004416 1.9432137 1.7117932 1.0385067 -0.077780724 -1.1301414 -1.7603271 -2.0421131][-1.5235862 -1.2458125 -0.81404209 -0.10432696 1.1345193 2.8869126 4.6944971 5.3534031 4.5150166 3.2030809 1.868685 0.59267068 -0.49930942 -1.2065518 -1.6567152][-1.0758895 -0.64426923 0.15617061 1.3017561 2.9417102 4.9813461 6.8686581 6.8507967 4.930028 2.9966018 1.4461477 0.36880374 -0.44022882 -0.96481919 -1.411889][-1.3227075 -1.0405017 -0.27507639 0.89014626 2.1954887 3.487633 4.2693996 3.8286707 2.240133 0.68199134 -0.35884714 -0.91592944 -1.2110482 -1.330107 -1.5526747][-2.0069587 -1.9713471 -1.4657352 -0.6351552 0.053463936 0.45708084 0.43951631 -0.12488699 -1.1886768 -2.159266 -2.5447145 -2.4924641 -2.2619305 -1.9708593 -1.9467974][-2.382129 -2.3986862 -2.1500354 -1.7441291 -1.5658734 -1.7896395 -2.2300234 -2.7591214 -3.4466162 -3.8976912 -3.7371864 -3.2122045 -2.6465955 -2.2081428 -2.1124887][-2.1902311 -2.2164536 -2.1646202 -2.1470411 -2.3482907 -2.8878808 -3.502703 -3.9325709 -4.2690406 -4.3159475 -3.8315511 -3.0736549 -2.4156582 -1.9986336 -2.0079815][-1.6164041 -1.6403308 -1.8136928 -2.0539811 -2.3915374 -2.9143937 -3.4024863 -3.6798396 -3.8011565 -3.665554 -3.0747318 -2.3035917 -1.7394702 -1.4981399 -1.6968771][-0.90136206 -0.98999906 -1.3416419 -1.7567115 -2.0682936 -2.4091375 -2.6829476 -2.8149993 -2.8455393 -2.6076317 -2.0011914 -1.3682824 -1.0279908 -1.0201997 -1.3931265][-0.55872834 -0.69064462 -1.0950205 -1.536051 -1.7804602 -1.9454235 -2.0675106 -2.1237853 -2.1106472 -1.8211187 -1.2390306 -0.81753445 -0.78407443 -1.0006355 -1.4413135]]...]
INFO - root - 2017-12-16 09:14:01.025330: step 29710, loss = 0.67, batch loss = 0.41 (48.9 examples/sec; 0.164 sec/batch; 13h:46m:15s remains)
INFO - root - 2017-12-16 09:14:02.710885: step 29720, loss = 0.57, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 14h:19m:41s remains)
INFO - root - 2017-12-16 09:14:04.371042: step 29730, loss = 0.60, batch loss = 0.35 (47.7 examples/sec; 0.168 sec/batch; 14h:06m:02s remains)
INFO - root - 2017-12-16 09:14:06.026403: step 29740, loss = 0.46, batch loss = 0.21 (48.5 examples/sec; 0.165 sec/batch; 13h:52m:33s remains)
INFO - root - 2017-12-16 09:14:07.684973: step 29750, loss = 0.52, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 14h:23m:26s remains)
INFO - root - 2017-12-16 09:14:09.359852: step 29760, loss = 0.70, batch loss = 0.44 (47.0 examples/sec; 0.170 sec/batch; 14h:19m:24s remains)
INFO - root - 2017-12-16 09:14:11.016967: step 29770, loss = 0.56, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:03m:20s remains)
INFO - root - 2017-12-16 09:14:12.642527: step 29780, loss = 0.48, batch loss = 0.22 (48.6 examples/sec; 0.165 sec/batch; 13h:50m:30s remains)
INFO - root - 2017-12-16 09:14:14.323657: step 29790, loss = 0.70, batch loss = 0.44 (46.3 examples/sec; 0.173 sec/batch; 14h:31m:01s remains)
INFO - root - 2017-12-16 09:14:16.044915: step 29800, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 14h:01m:58s remains)
2017-12-16 09:14:16.547983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1595132 -1.7664543 -1.6679839 -1.8723199 -2.0537493 -2.1388803 -2.1730566 -2.2264428 -2.2486179 -2.2339883 -2.2528892 -2.2523685 -2.1936164 -2.0916455 -1.9999393][-1.9203545 -1.6256424 -1.6856349 -2.015377 -2.1913407 -2.1799266 -2.1450903 -2.1689188 -2.2213743 -2.2536397 -2.3101916 -2.3916252 -2.4088597 -2.3832829 -2.3496339][-1.6998508 -1.5087973 -1.7210383 -2.1417286 -2.2992294 -2.1586223 -2.0016656 -1.99298 -2.1332321 -2.2672133 -2.4416838 -2.66138 -2.8002002 -2.8557303 -2.8936043][-1.7227014 -1.5586851 -1.8208826 -2.220247 -2.2752244 -1.9763994 -1.7187445 -1.746775 -2.0015676 -2.2837195 -2.6165144 -3.0112896 -3.3264472 -3.5161676 -3.6311703][-1.8005437 -1.5257781 -1.7325507 -2.0605605 -2.011565 -1.612125 -1.2815771 -1.2965858 -1.6280314 -1.9997281 -2.4577913 -3.0820024 -3.6091208 -4.0038395 -4.2133951][-1.765059 -1.3141826 -1.4516056 -1.6972477 -1.5571349 -1.1333849 -0.786337 -0.7277869 -0.94021773 -1.1621655 -1.6250327 -2.4095545 -3.187197 -3.8220274 -4.1397128][-1.6164887 -1.1647815 -1.2325284 -1.4287922 -1.2589779 -0.84101892 -0.45984483 -0.15290689 0.070816517 0.17485118 -0.27191305 -1.2656132 -2.325403 -3.1552603 -3.5499597][-1.6367278 -1.2229338 -1.21946 -1.3421792 -1.1514521 -0.71809804 -0.20815659 0.43074274 1.009872 1.3249652 0.82004666 -0.35869193 -1.5591328 -2.4809325 -2.8936558][-1.6024821 -1.0772613 -0.89341772 -0.93591154 -0.72835326 -0.30149698 0.23152924 0.9675765 1.6510713 1.8325284 1.1835284 0.084456682 -0.97242272 -1.8585175 -2.3473506][-1.0724179 -0.29770994 0.034764528 0.033034325 0.18889594 0.44944215 0.84547043 1.4668698 2.0375526 2.0814865 1.4640892 0.65402007 -0.19020796 -1.1142777 -1.8222288][-0.3799541 0.55354714 0.82330394 0.69305539 0.72398067 0.79244733 0.98890948 1.4740281 1.9808166 2.124059 1.8122404 1.3599479 0.63854766 -0.40682423 -1.3686812][-0.29883265 0.49726391 0.45148158 0.055990458 -0.042102337 -0.143296 -0.089089394 0.36317515 0.90766764 1.3057139 1.4589443 1.3528578 0.71384144 -0.34265924 -1.3286642][-0.78661191 -0.36918473 -0.72134805 -1.252507 -1.4369 -1.5540462 -1.5514915 -1.1557416 -0.6185621 -0.16703844 0.17058992 0.27256513 -0.21205878 -1.0684224 -1.8137281][-1.4022715 -1.3147665 -1.7934391 -2.2681708 -2.3786619 -2.462676 -2.5135925 -2.2461779 -1.8434529 -1.4992197 -1.2413439 -1.124925 -1.4112089 -1.9238467 -2.352241][-1.7564882 -1.8567131 -2.3827643 -2.8052192 -2.859848 -2.8929608 -2.9445937 -2.7893963 -2.5409725 -2.3259542 -2.1865973 -2.05224 -2.1151149 -2.2948704 -2.4615386]]...]
INFO - root - 2017-12-16 09:14:18.204579: step 29810, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:56m:24s remains)
INFO - root - 2017-12-16 09:14:19.864845: step 29820, loss = 0.48, batch loss = 0.22 (48.8 examples/sec; 0.164 sec/batch; 13h:47m:11s remains)
INFO - root - 2017-12-16 09:14:21.548654: step 29830, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 14h:06m:21s remains)
INFO - root - 2017-12-16 09:14:23.247680: step 29840, loss = 0.63, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 14h:02m:43s remains)
INFO - root - 2017-12-16 09:14:24.907499: step 29850, loss = 0.64, batch loss = 0.39 (48.1 examples/sec; 0.166 sec/batch; 13h:58m:55s remains)
INFO - root - 2017-12-16 09:14:26.561619: step 29860, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 13h:40m:57s remains)
INFO - root - 2017-12-16 09:14:28.192299: step 29870, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 13h:42m:38s remains)
INFO - root - 2017-12-16 09:14:29.835449: step 29880, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 14h:19m:03s remains)
INFO - root - 2017-12-16 09:14:31.493649: step 29890, loss = 0.68, batch loss = 0.42 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:12s remains)
INFO - root - 2017-12-16 09:14:33.198215: step 29900, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 14h:15m:51s remains)
2017-12-16 09:14:33.711041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3045106 -1.3798965 -1.5087571 -1.7151188 -2.0296628 -2.4008925 -2.749181 -3.010596 -3.1688118 -3.2208095 -3.128592 -2.9281268 -2.6799166 -2.4544237 -2.1769087][-1.5642418 -1.8124845 -2.1520822 -2.6040659 -3.153753 -3.6540499 -3.9996862 -4.2094169 -4.3195052 -4.3110328 -4.1695166 -3.934567 -3.6579609 -3.3233385 -2.8250077][-1.9668121 -2.4340074 -2.9718976 -3.5601616 -4.1369925 -4.5189338 -4.6224594 -4.5700617 -4.5725675 -4.5894275 -4.5503092 -4.4494905 -4.3013592 -3.9710107 -3.3213475][-2.4502041 -3.0796661 -3.6071229 -4.015801 -4.289947 -4.2586274 -3.8281007 -3.392343 -3.3210478 -3.5341618 -3.7973566 -4.0409083 -4.217823 -4.0787411 -3.4189582][-2.9102943 -3.5285726 -3.7921448 -3.720058 -3.2966027 -2.5522139 -1.4367359 -0.53368962 -0.46627176 -1.1212437 -1.8861711 -2.6516442 -3.3206408 -3.6248879 -3.1849852][-3.249054 -3.7164249 -3.5336046 -2.7803504 -1.5158448 0.018761635 1.8150854 3.2126927 3.2423859 1.9402118 0.41624022 -0.93844056 -2.1724012 -2.9669809 -2.8473687][-3.3609419 -3.6188872 -2.9692121 -1.5205495 0.41319871 2.50989 4.8716688 6.8137336 6.5424161 4.4008651 2.1463885 0.19401479 -1.5288123 -2.6093562 -2.7175367][-3.2962546 -3.4216549 -2.4874287 -0.59037495 1.7338305 4.0186419 6.6519885 9.0620689 7.5196567 4.6364069 2.1113763 -0.033504963 -1.82851 -2.8637819 -2.8947597][-3.0684948 -3.1477251 -2.2134635 -0.37894464 1.6922531 3.4569969 4.9922252 5.9410024 4.9829421 2.638299 0.45538831 -1.3366554 -2.7392628 -3.44207 -3.1770215][-2.778985 -2.9516039 -2.3572037 -1.0499346 0.33337307 1.3006189 1.9024959 2.2292042 1.5581255 -0.099622011 -1.6794107 -2.8895698 -3.7390814 -3.9088571 -3.3039184][-2.5140138 -2.8166592 -2.6103482 -1.8448234 -1.0854985 -0.81907523 -0.81657863 -0.85427666 -1.3285035 -2.3727586 -3.3547156 -4.0002222 -4.3032961 -4.0223413 -3.1910379][-2.244992 -2.5884337 -2.6438348 -2.3270264 -2.0140309 -2.0712256 -2.3474855 -2.6008837 -2.8970032 -3.4687576 -3.9948773 -4.2551427 -4.1855011 -3.6731448 -2.8494987][-1.8908187 -2.1622524 -2.2783377 -2.1599824 -2.0784192 -2.2584157 -2.6316473 -2.9157472 -3.0575726 -3.3079064 -3.5383832 -3.6045866 -3.4468422 -2.9997389 -2.3534265][-1.5433382 -1.6942681 -1.7758069 -1.700218 -1.6490593 -1.7798121 -2.0401297 -2.2438984 -2.3454025 -2.4678249 -2.5796201 -2.6088297 -2.5313156 -2.2617164 -1.8565778][-1.3047364 -1.3364449 -1.3371247 -1.2542415 -1.1727707 -1.2060046 -1.3412683 -1.4717754 -1.5647352 -1.66219 -1.7546569 -1.8131549 -1.8103652 -1.7034798 -1.5091995]]...]
INFO - root - 2017-12-16 09:14:35.458376: step 29910, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 14h:04m:19s remains)
INFO - root - 2017-12-16 09:14:37.161084: step 29920, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:16s remains)
INFO - root - 2017-12-16 09:14:38.846569: step 29930, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:01m:29s remains)
INFO - root - 2017-12-16 09:14:40.544759: step 29940, loss = 0.63, batch loss = 0.38 (46.4 examples/sec; 0.172 sec/batch; 14h:28m:32s remains)
INFO - root - 2017-12-16 09:14:42.216168: step 29950, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 14h:02m:40s remains)
INFO - root - 2017-12-16 09:14:43.880579: step 29960, loss = 0.51, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 14h:04m:28s remains)
INFO - root - 2017-12-16 09:14:45.548564: step 29970, loss = 0.64, batch loss = 0.38 (48.4 examples/sec; 0.165 sec/batch; 13h:54m:06s remains)
INFO - root - 2017-12-16 09:14:47.211466: step 29980, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 13h:58m:21s remains)
INFO - root - 2017-12-16 09:14:48.866427: step 29990, loss = 0.66, batch loss = 0.41 (49.3 examples/sec; 0.162 sec/batch; 13h:38m:56s remains)
INFO - root - 2017-12-16 09:14:50.500192: step 30000, loss = 0.55, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:57m:55s remains)
2017-12-16 09:14:50.979741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2704506 -2.9187922 -2.579756 -2.4639046 -2.4605987 -2.1329873 -1.5796251 -1.3238608 -1.5222003 -2.1415894 -3.1230121 -3.6975093 -3.3798103 -2.6030443 -2.1265826][-2.6528552 -1.9574277 -1.5714489 -1.7067068 -1.95604 -1.703748 -1.0440717 -0.61889517 -0.73589456 -1.4505886 -2.6106741 -3.2841969 -3.0018263 -2.2152696 -1.6486852][-1.4596715 -0.38657951 -0.11025167 -0.72182178 -1.4513752 -1.4966762 -0.90690947 -0.34690285 -0.32198763 -1.0116205 -2.1799996 -2.8506436 -2.624593 -1.9218743 -1.3151137][-0.22623444 1.1160131 1.2161684 0.023267746 -1.2572905 -1.6153975 -1.0850048 -0.30042076 0.049878597 -0.44939339 -1.5667512 -2.3089643 -2.3191931 -1.8762953 -1.3734598][0.46910357 1.7507365 1.6062846 0.041944504 -1.5195134 -1.9452742 -1.2568551 -0.18140578 0.52892327 0.23120546 -0.78411877 -1.6312578 -1.9502404 -1.8727022 -1.612211][0.3103652 1.2384441 0.89043021 -0.68799686 -2.0973744 -2.2080276 -1.1753398 0.18240523 1.0646796 0.80970788 -0.19699669 -1.1315898 -1.7020391 -1.9537632 -1.9577311][-0.74567544 -0.24915838 -0.51744962 -1.5920233 -2.4105411 -2.0181153 -0.64061558 0.82029581 1.5986059 1.0853267 -0.064924955 -1.0593188 -1.6862104 -2.0525489 -2.2855666][-1.9580982 -1.7831967 -1.849417 -2.3028941 -2.4521127 -1.5776044 -0.023815155 1.3094308 1.698952 0.847656 -0.44907761 -1.3413767 -1.7719787 -2.0339117 -2.3514621][-2.7198057 -2.694509 -2.5863707 -2.6039028 -2.3773606 -1.3993653 -0.040968895 0.8440392 0.77164769 -0.20037603 -1.3392341 -1.8811927 -1.9022818 -1.875361 -2.0818641][-2.9971409 -2.9150362 -2.64042 -2.4698412 -2.2309384 -1.5484841 -0.67796183 -0.28841758 -0.580017 -1.4435693 -2.2936006 -2.4666331 -2.073781 -1.6371487 -1.5907458][-2.6896229 -2.385884 -2.0217161 -1.9419346 -2.0327544 -1.9339978 -1.6277295 -1.5503384 -1.8241966 -2.4206386 -2.955153 -2.8105736 -2.1319404 -1.4375796 -1.088037][-2.2297521 -1.611344 -1.1522256 -1.2321357 -1.7573609 -2.2584922 -2.4202414 -2.5389712 -2.767731 -3.1322012 -3.3226132 -2.9095337 -2.0611892 -1.2218506 -0.63900292][-1.8392241 -0.91002059 -0.36711764 -0.61886132 -1.4820654 -2.3674686 -2.7941051 -2.9504673 -3.0602639 -3.2045884 -3.127841 -2.564543 -1.7410332 -0.93610764 -0.24868131][-1.5300279 -0.35768151 0.26526237 -0.074507952 -1.0655847 -2.0305624 -2.4912329 -2.5708923 -2.4772012 -2.3799572 -2.1714122 -1.726831 -1.1967101 -0.67644966 -0.11670852][-1.328439 -0.16164064 0.4259901 0.11785293 -0.69519174 -1.4291792 -1.7176001 -1.6016605 -1.2262059 -0.89969933 -0.76145673 -0.72832716 -0.71197987 -0.59582996 -0.28757834]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:14:53.329629: step 30010, loss = 0.47, batch loss = 0.21 (48.4 examples/sec; 0.165 sec/batch; 13h:52m:58s remains)
INFO - root - 2017-12-16 09:14:54.975969: step 30020, loss = 0.62, batch loss = 0.36 (49.3 examples/sec; 0.162 sec/batch; 13h:38m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:14:56.632502: step 30030, loss = 0.52, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:45m:33s remains)
INFO - root - 2017-12-16 09:14:58.312406: step 30040, loss = 0.49, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 14h:00m:37s remains)
INFO - root - 2017-12-16 09:14:59.975501: step 30050, loss = 0.56, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 14h:02m:22s remains)
INFO - root - 2017-12-16 09:15:01.625070: step 30060, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:05m:24s remains)
INFO - root - 2017-12-16 09:15:03.309129: step 30070, loss = 0.61, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 13h:57m:00s remains)
INFO - root - 2017-12-16 09:15:04.975721: step 30080, loss = 0.63, batch loss = 0.38 (48.7 examples/sec; 0.164 sec/batch; 13h:48m:00s remains)
INFO - root - 2017-12-16 09:15:06.631931: step 30090, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 13h:58m:29s remains)
INFO - root - 2017-12-16 09:15:08.293839: step 30100, loss = 0.57, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 14h:07m:05s remains)
2017-12-16 09:15:08.773361: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3873651 -2.3339269 -2.2044067 -2.0256908 -1.8302271 -1.66973 -1.59322 -1.6007786 -1.6431997 -1.6430349 -1.6276259 -1.656172 -1.7296166 -1.883023 -2.1583774][-2.2842619 -2.1907656 -2.0085387 -1.7927799 -1.5356933 -1.2896299 -1.1809821 -1.2485255 -1.3466076 -1.3359567 -1.2665375 -1.2277852 -1.2713299 -1.3981725 -1.7327375][-2.2469826 -2.1388049 -1.9566643 -1.7564974 -1.4226925 -1.0560131 -0.90785921 -1.065159 -1.2542571 -1.2355064 -1.0860035 -0.952554 -0.90899074 -0.98118317 -1.2940519][-2.4407165 -2.3737442 -2.2039871 -1.9384013 -1.4454482 -0.8959403 -0.71231008 -0.95161891 -1.2258025 -1.2314411 -1.0954816 -0.93710887 -0.8367039 -0.89203703 -1.1623681][-2.6533651 -2.5697031 -2.2892368 -1.8183053 -1.0612795 -0.3059926 -0.10418725 -0.44992888 -0.84101152 -0.9325943 -0.87450838 -0.735844 -0.67492354 -0.84202516 -1.255007][-2.629982 -2.4012098 -1.8639169 -1.10198 -0.053124189 0.89712596 1.1432121 0.65373111 0.088372469 -0.15694237 -0.2419796 -0.20939779 -0.22590995 -0.5926528 -1.2786106][-2.4146614 -1.9823749 -1.1499395 -0.016962051 1.3273661 2.4988165 2.8535857 2.2860646 1.6249881 1.2981019 1.0503173 0.8125782 0.49084949 -0.13717365 -1.0496367][-2.2157812 -1.6968676 -0.68246138 0.67435908 2.2487564 3.6082244 4.1110907 3.6366582 2.9347486 2.5892415 2.2276821 1.7549157 1.1745379 0.349159 -0.7597425][-2.3521392 -1.9867339 -1.1756494 0.018751621 1.4955058 2.8518 3.3999605 3.0518308 2.4927135 2.2501955 1.9751067 1.5763915 1.0701766 0.33464742 -0.76570857][-2.6724243 -2.56288 -2.0639668 -1.234327 -0.078741789 1.0753036 1.5817015 1.3250501 0.93897247 0.8884933 0.83583593 0.67364359 0.4629333 0.00031137466 -0.8839711][-2.9084821 -3.00406 -2.7886572 -2.2912705 -1.4339932 -0.48932135 -0.067205906 -0.298177 -0.61824417 -0.52848768 -0.34139228 -0.20199299 -0.071863651 -0.16296506 -0.72687232][-2.8895416 -3.1184585 -3.1106365 -2.821059 -2.1682796 -1.4158354 -1.1099998 -1.4029324 -1.7250247 -1.633062 -1.3440988 -0.98600173 -0.616552 -0.42486155 -0.67147577][-2.7707913 -3.0843728 -3.1954808 -3.0174346 -2.5156627 -1.9450243 -1.7417846 -2.0365996 -2.3392348 -2.3129196 -2.0503788 -1.6717157 -1.2684571 -0.98965013 -1.0524749][-2.7397747 -3.1106174 -3.2918403 -3.2044234 -2.86652 -2.4730175 -2.3387413 -2.5525267 -2.7874084 -2.8308053 -2.6880186 -2.3891318 -2.0406237 -1.792639 -1.7567693][-2.7823567 -3.1819243 -3.4321718 -3.4517217 -3.2552752 -3.0087185 -2.9170518 -3.020752 -3.1807685 -3.2667875 -3.2381167 -3.0656493 -2.8360572 -2.6433885 -2.5401669]]...]
INFO - root - 2017-12-16 09:15:10.472839: step 30110, loss = 0.60, batch loss = 0.34 (47.0 examples/sec; 0.170 sec/batch; 14h:17m:20s remains)
INFO - root - 2017-12-16 09:15:12.150219: step 30120, loss = 0.55, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 14h:05m:53s remains)
INFO - root - 2017-12-16 09:15:13.849012: step 30130, loss = 0.47, batch loss = 0.21 (48.3 examples/sec; 0.166 sec/batch; 13h:54m:53s remains)
INFO - root - 2017-12-16 09:15:15.517631: step 30140, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 13h:50m:10s remains)
INFO - root - 2017-12-16 09:15:17.190976: step 30150, loss = 0.63, batch loss = 0.37 (47.2 examples/sec; 0.169 sec/batch; 14h:13m:12s remains)
INFO - root - 2017-12-16 09:15:18.874360: step 30160, loss = 0.55, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 14h:05m:18s remains)
INFO - root - 2017-12-16 09:15:20.541627: step 30170, loss = 0.56, batch loss = 0.30 (46.4 examples/sec; 0.173 sec/batch; 14h:29m:21s remains)
INFO - root - 2017-12-16 09:15:22.237135: step 30180, loss = 0.54, batch loss = 0.28 (46.5 examples/sec; 0.172 sec/batch; 14h:26m:35s remains)
INFO - root - 2017-12-16 09:15:23.937113: step 30190, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 14h:10m:42s remains)
INFO - root - 2017-12-16 09:15:25.625410: step 30200, loss = 0.54, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:56m:13s remains)
2017-12-16 09:15:26.151168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8401957 -3.1451974 -2.3807511 -2.2036953 -2.458638 -2.6883254 -2.7246809 -2.7242765 -2.7574439 -2.6098816 -2.1880443 -1.6338205 -1.2985941 -1.509913 -2.1016109][-4.4773951 -3.7956836 -3.0768178 -2.9211085 -3.1850584 -3.3405969 -3.2657053 -3.2296188 -3.2959332 -3.1643429 -2.6973174 -2.1078842 -1.7208989 -1.8364764 -2.3679407][-4.575346 -3.8742263 -3.2730627 -3.211261 -3.46529 -3.4238992 -3.1111641 -3.0144343 -3.1585696 -3.1515174 -2.8004272 -2.3423178 -2.0197682 -2.02521 -2.4074402][-4.2862582 -3.5009518 -2.9438694 -2.9171357 -3.0564508 -2.7469275 -2.1245213 -1.9726276 -2.1968799 -2.3855851 -2.3003793 -2.1179214 -2.0007918 -1.9660127 -2.2279773][-3.8331985 -2.9395804 -2.3335366 -2.1865108 -2.1224198 -1.4924705 -0.55529892 -0.27902079 -0.68399882 -1.1864967 -1.4053078 -1.5260818 -1.5816312 -1.5370364 -1.7144189][-3.3243303 -2.3613124 -1.6755942 -1.4073727 -1.1116339 -0.16001129 1.1642485 1.6588926 1.0829859 0.22190928 -0.38015676 -0.77871537 -0.98519909 -1.0130252 -1.1250101][-2.6880054 -1.5755773 -0.78338993 -0.45190239 0.07179594 1.4083033 3.1570487 3.7102633 2.7726388 1.4461114 0.4962709 -0.026619673 -0.27499461 -0.36159039 -0.53768623][-2.1497939 -0.88871229 0.085035563 0.53262162 1.1412494 2.6179643 4.517334 4.9305406 3.4851875 1.8579879 0.81355333 0.37920761 0.28074718 0.19921589 -0.10715866][-2.284781 -0.93784261 0.29611278 0.86653495 1.211858 2.1483483 3.3907137 3.5970607 2.3745408 0.995064 0.2255156 0.12849069 0.36409855 0.44153905 0.078857422][-2.8115878 -1.3790181 0.10663176 0.70036411 0.60384774 0.77030063 1.342304 1.5319371 0.8728559 0.02619195 -0.39287579 -0.1789825 0.4541533 0.71783161 0.27067876][-3.6267462 -2.1937516 -0.54005361 0.0829432 -0.28763628 -0.61704493 -0.39166045 -0.19143224 -0.45125234 -0.95163167 -1.0841881 -0.60071635 0.24729896 0.63301444 0.17663574][-4.6488733 -3.4632404 -1.810056 -1.0467098 -1.3679227 -1.800429 -1.7650136 -1.5537128 -1.6567261 -1.9889975 -1.9706793 -1.3692963 -0.43828392 -0.031413078 -0.49569428][-5.4186287 -4.5615373 -3.2131381 -2.4807179 -2.5854754 -2.9137576 -2.9589858 -2.8353877 -2.8660483 -2.9994388 -2.8646369 -2.3119159 -1.5442823 -1.2179583 -1.5951805][-5.2963715 -4.73828 -3.7242956 -3.1340127 -3.1699085 -3.4066942 -3.4852557 -3.42064 -3.3694782 -3.3345075 -3.1452675 -2.73933 -2.2290628 -2.0270867 -2.3255782][-4.4677725 -4.0660157 -3.3515277 -2.9450839 -3.0189731 -3.2306032 -3.318537 -3.2636464 -3.1409924 -3.0201645 -2.8468289 -2.5958931 -2.3075838 -2.1772926 -2.3490365]]...]
INFO - root - 2017-12-16 09:15:27.811286: step 30210, loss = 0.51, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:59m:55s remains)
INFO - root - 2017-12-16 09:15:29.452198: step 30220, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:44m:55s remains)
INFO - root - 2017-12-16 09:15:31.128187: step 30230, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 13h:59m:44s remains)
INFO - root - 2017-12-16 09:15:32.799672: step 30240, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 13h:57m:16s remains)
INFO - root - 2017-12-16 09:15:34.499417: step 30250, loss = 0.55, batch loss = 0.30 (45.9 examples/sec; 0.174 sec/batch; 14h:38m:17s remains)
INFO - root - 2017-12-16 09:15:36.229098: step 30260, loss = 0.54, batch loss = 0.28 (46.1 examples/sec; 0.174 sec/batch; 14h:34m:33s remains)
INFO - root - 2017-12-16 09:15:37.909290: step 30270, loss = 0.46, batch loss = 0.21 (48.1 examples/sec; 0.166 sec/batch; 13h:58m:14s remains)
INFO - root - 2017-12-16 09:15:39.615495: step 30280, loss = 0.46, batch loss = 0.20 (46.0 examples/sec; 0.174 sec/batch; 14h:35m:46s remains)
INFO - root - 2017-12-16 09:15:41.303671: step 30290, loss = 0.55, batch loss = 0.29 (46.7 examples/sec; 0.171 sec/batch; 14h:23m:33s remains)
INFO - root - 2017-12-16 09:15:43.010535: step 30300, loss = 0.66, batch loss = 0.40 (47.1 examples/sec; 0.170 sec/batch; 14h:14m:46s remains)
2017-12-16 09:15:43.502403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.66818929 -1.3915625 -1.83388 -2.0091391 -2.0327532 -1.993067 -1.911124 -1.8909489 -2.0243142 -2.2567406 -2.4219913 -2.4849203 -2.4381294 -2.3619864 -2.2451255][-0.051602125 -0.73953438 -1.1396089 -1.3587917 -1.4940397 -1.5695125 -1.5683858 -1.5962126 -1.762351 -2.045737 -2.265703 -2.3765678 -2.4342039 -2.4548149 -2.4255867][0.33107567 -0.23953271 -0.59782672 -0.8424983 -1.0202427 -1.0800025 -1.068899 -1.1305417 -1.361127 -1.7221355 -1.9595015 -2.0381317 -2.1384571 -2.2632425 -2.3511662][0.519362 0.10992908 -0.16846251 -0.38266587 -0.45222366 -0.37415123 -0.23843145 -0.33710623 -0.75485325 -1.2672296 -1.5474524 -1.5898035 -1.692687 -1.8904238 -2.0667977][0.34895563 0.14375019 0.048815727 0.016266584 0.13572288 0.39882803 0.68310237 0.56611657 -0.050052881 -0.71217084 -1.0447047 -1.0072211 -1.0962932 -1.3427031 -1.6004858][-0.10494971 -0.022629976 0.14762807 0.32590652 0.63566041 1.1173742 1.6336436 1.6060872 0.8342483 0.063237429 -0.26114178 -0.16430688 -0.25397062 -0.56730497 -0.946548][-0.40734184 -0.16976428 0.18325281 0.51129746 1.0157824 1.7898407 2.6870632 2.8923383 1.9906926 1.0808344 0.69088173 0.78910971 0.7027626 0.35968351 -0.13059616][-0.58363771 -0.395064 -0.0865407 0.2720542 0.82684541 1.6766653 2.7513332 3.2334528 2.5328455 1.704484 1.339082 1.4908946 1.4474924 1.1263838 0.60693288][-0.84192836 -0.83114767 -0.6575706 -0.37747979 0.072018385 0.73801184 1.577657 2.043901 1.7693162 1.3311546 1.2035325 1.4964848 1.6259003 1.4045246 0.94775033][-1.0926982 -1.2475821 -1.2126777 -1.069919 -0.78790224 -0.36094952 0.23085332 0.66116047 0.67001057 0.55377817 0.66974378 1.0417287 1.2488418 1.1354847 0.77998281][-1.2233169 -1.4815919 -1.5348239 -1.5124519 -1.4035039 -1.181607 -0.81097031 -0.45560718 -0.30104423 -0.16320825 0.16534042 0.64184356 0.86780214 0.75365758 0.481467][-1.2450578 -1.4622792 -1.5092566 -1.5419035 -1.5751638 -1.5367639 -1.3972666 -1.1959596 -1.0288018 -0.77518678 -0.29345226 0.25421953 0.47590876 0.32826948 0.10318923][-1.2706141 -1.3261514 -1.2443221 -1.2069702 -1.2551426 -1.2993327 -1.3313657 -1.3080794 -1.2497276 -1.0877774 -0.73129904 -0.32750297 -0.18644428 -0.3326385 -0.48647749][-1.4254605 -1.2938536 -1.0336504 -0.87188673 -0.80486786 -0.7723707 -0.80966449 -0.91242671 -1.0279646 -1.0694432 -0.95839536 -0.84348309 -0.88391244 -1.0429775 -1.0759323][-1.6247994 -1.4485406 -1.1366104 -0.86907256 -0.648541 -0.47537911 -0.45115519 -0.61432171 -0.82788444 -1.0008878 -1.100569 -1.2017419 -1.3667481 -1.483856 -1.3788209]]...]
INFO - root - 2017-12-16 09:15:45.174564: step 30310, loss = 0.62, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 14h:10m:19s remains)
INFO - root - 2017-12-16 09:15:46.878191: step 30320, loss = 0.46, batch loss = 0.20 (45.6 examples/sec; 0.175 sec/batch; 14h:42m:39s remains)
INFO - root - 2017-12-16 09:15:48.593066: step 30330, loss = 0.60, batch loss = 0.34 (46.4 examples/sec; 0.172 sec/batch; 14h:27m:22s remains)
INFO - root - 2017-12-16 09:15:50.272781: step 30340, loss = 0.64, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 13h:55m:05s remains)
INFO - root - 2017-12-16 09:15:51.940115: step 30350, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 13h:56m:18s remains)
INFO - root - 2017-12-16 09:15:53.607472: step 30360, loss = 0.53, batch loss = 0.28 (46.6 examples/sec; 0.172 sec/batch; 14h:23m:48s remains)
INFO - root - 2017-12-16 09:15:55.290879: step 30370, loss = 0.60, batch loss = 0.34 (46.6 examples/sec; 0.172 sec/batch; 14h:24m:39s remains)
INFO - root - 2017-12-16 09:15:56.969061: step 30380, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 13h:58m:10s remains)
INFO - root - 2017-12-16 09:15:58.629329: step 30390, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 14h:06m:15s remains)
INFO - root - 2017-12-16 09:16:00.303937: step 30400, loss = 0.47, batch loss = 0.22 (47.5 examples/sec; 0.168 sec/batch; 14h:07m:27s remains)
2017-12-16 09:16:00.794208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1726215 -2.1396103 -2.1684744 -2.284049 -2.4968553 -2.7360139 -2.7695723 -2.5623865 -2.3183265 -2.2459264 -2.2264569 -1.6697822 -0.51908052 0.31288981 0.080400705][-2.2105672 -2.221736 -2.2916439 -2.5258379 -2.888886 -3.213902 -3.2233777 -2.9562531 -2.6598547 -2.5665162 -2.5770431 -2.1400549 -1.2234302 -0.61554646 -0.88340104][-2.217289 -2.1635482 -2.1833241 -2.4639108 -2.9086246 -3.2420793 -3.2204437 -2.9319277 -2.6510098 -2.5696008 -2.7003136 -2.6017516 -2.1203856 -1.7805012 -1.94611][-2.0761638 -1.9339933 -1.8348134 -2.0195551 -2.3513582 -2.50915 -2.3692329 -2.1098661 -1.9599891 -2.0312457 -2.3714032 -2.6383944 -2.5607598 -2.4244521 -2.4431517][-1.7754657 -1.6103497 -1.3652238 -1.2476555 -1.2160097 -1.0721353 -0.832821 -0.68409264 -0.81891847 -1.2061872 -1.7888513 -2.2617252 -2.3808441 -2.3193026 -2.254138][-1.2227978 -1.0274446 -0.62892032 -0.11365128 0.4637301 0.97174573 1.1994307 1.0328693 0.418298 -0.44893515 -1.2879357 -1.7973018 -1.8568771 -1.7442585 -1.6391767][-0.72529888 -0.46370721 0.06989789 0.91735768 1.9747279 2.7655637 2.8510239 2.1195834 0.85384679 -0.4807142 -1.405726 -1.7424208 -1.586062 -1.3783071 -1.2833999][-0.54873037 -0.24171925 0.3021636 1.1542404 2.3062317 3.1873643 3.0686147 1.9107125 0.2775116 -1.2240564 -2.0529594 -2.1665518 -1.8888923 -1.6422248 -1.5779922][-0.69527435 -0.46508229 -0.15115047 0.37191606 1.1430783 1.7415965 1.5231731 0.41608763 -1.0756298 -2.3443606 -2.9065113 -2.8080688 -2.4389141 -2.2062201 -2.2000282][-1.0661579 -1.0083789 -0.92696178 -0.7223649 -0.34613729 -0.071881294 -0.34395051 -1.2526841 -2.3682775 -3.2400298 -3.4753633 -3.1519504 -2.6981339 -2.4784427 -2.6231492][-1.4362432 -1.5417548 -1.5795031 -1.547549 -1.4447842 -1.3486171 -1.5657184 -2.2054803 -2.9443421 -3.4594355 -3.4104655 -2.9503443 -2.5016875 -2.3555436 -2.6421316][-1.765213 -1.9245746 -1.9353439 -1.9438021 -2.0726902 -2.10091 -2.1720793 -2.5285437 -2.9662752 -3.1757305 -2.9415021 -2.5026309 -2.191891 -2.1660352 -2.5387995][-2.1279762 -2.2126777 -2.0892069 -2.0419767 -2.3212628 -2.4743404 -2.466182 -2.6315432 -2.8208783 -2.802834 -2.4643269 -2.0829949 -1.9802651 -2.1501629 -2.5536542][-2.4841866 -2.4520502 -2.190901 -2.1070511 -2.4714167 -2.7597203 -2.8060918 -2.8889356 -2.9175453 -2.8013809 -2.4497991 -2.100409 -2.0801582 -2.3249254 -2.6616249][-2.7311497 -2.5997949 -2.3537879 -2.3830364 -2.8541367 -3.2410851 -3.3484797 -3.394794 -3.3341062 -3.1163573 -2.75473 -2.40221 -2.33587 -2.4831431 -2.6306868]]...]
INFO - root - 2017-12-16 09:16:02.431814: step 30410, loss = 0.58, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 13h:55m:52s remains)
INFO - root - 2017-12-16 09:16:04.111702: step 30420, loss = 0.54, batch loss = 0.29 (45.0 examples/sec; 0.178 sec/batch; 14h:54m:45s remains)
INFO - root - 2017-12-16 09:16:05.838745: step 30430, loss = 0.75, batch loss = 0.50 (47.0 examples/sec; 0.170 sec/batch; 14h:16m:41s remains)
INFO - root - 2017-12-16 09:16:07.518627: step 30440, loss = 0.62, batch loss = 0.36 (48.5 examples/sec; 0.165 sec/batch; 13h:49m:55s remains)
INFO - root - 2017-12-16 09:16:09.223098: step 30450, loss = 0.60, batch loss = 0.34 (46.7 examples/sec; 0.171 sec/batch; 14h:21m:37s remains)
INFO - root - 2017-12-16 09:16:10.899102: step 30460, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:54m:35s remains)
INFO - root - 2017-12-16 09:16:12.586547: step 30470, loss = 0.51, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 13h:45m:39s remains)
INFO - root - 2017-12-16 09:16:14.271761: step 30480, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 14h:01m:06s remains)
INFO - root - 2017-12-16 09:16:15.936903: step 30490, loss = 0.54, batch loss = 0.28 (46.4 examples/sec; 0.172 sec/batch; 14h:27m:21s remains)
INFO - root - 2017-12-16 09:16:17.601842: step 30500, loss = 0.50, batch loss = 0.24 (47.2 examples/sec; 0.169 sec/batch; 14h:12m:56s remains)
2017-12-16 09:16:18.089102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3565793 -2.5724323 -2.5891786 -2.4758487 -2.4040821 -2.29618 -2.0031164 -1.6464932 -1.3919774 -1.1371647 -0.99713981 -1.4372518 -2.3728442 -3.1338565 -3.5755699][-1.5418683 -1.7189722 -1.735465 -1.6809986 -1.7672442 -1.9277742 -1.9401301 -1.7714279 -1.6392379 -1.4909878 -1.4898713 -2.0576191 -3.0910544 -3.9891541 -4.5635576][-1.1807082 -1.2640535 -1.2089976 -1.1690147 -1.328132 -1.6282897 -1.8009013 -1.777087 -1.6291833 -1.4625406 -1.4855266 -2.0461767 -3.0473409 -4.017168 -4.7105665][-0.98525989 -0.95058966 -0.80794013 -0.68219948 -0.74100888 -0.99696445 -1.2133058 -1.2740674 -1.1134002 -0.88443315 -0.91112709 -1.3822045 -2.2724721 -3.2419813 -3.9695435][-0.6275543 -0.42960036 -0.052866936 0.21845841 0.18448806 -0.0044300556 -0.22256255 -0.3660748 -0.23565912 0.040617704 0.10235071 -0.2817297 -1.0859149 -2.0204287 -2.7349148][-0.2736342 0.021037579 0.591254 1.1178508 1.2838953 1.2400038 1.0166194 0.78791571 0.84553909 1.1423011 1.3456717 1.027642 0.29041457 -0.6227057 -1.3603958][0.17574453 0.426975 0.98322892 1.6861858 2.1187382 2.2094412 1.9659634 1.6123981 1.5618451 1.9860125 2.3778663 2.2033286 1.5438538 0.62236953 -0.16866612][0.687613 0.72243643 1.1687975 1.8948398 2.4190779 2.5031037 2.0827045 1.4722826 1.2125289 1.6826911 2.379303 2.5761943 2.2407789 1.508672 0.69813681][1.1034007 0.84920573 1.0127668 1.5512497 1.9987621 2.0041428 1.5177855 0.79015684 0.37450743 0.82195926 1.7441034 2.3283362 2.3429513 1.8484664 1.1006761][0.60654855 0.12196136 0.062005997 0.40900278 0.73147774 0.76909709 0.42519546 -0.1467123 -0.40214825 0.17227244 1.2312396 2.02174 2.1847229 1.7656012 0.99935865][-0.78113055 -1.3718374 -1.5368667 -1.3460385 -1.0795203 -0.94996154 -1.0995224 -1.422267 -1.4468784 -0.8103646 0.19397664 0.98439431 1.2485144 0.90018845 0.19666934][-2.2066817 -2.8074713 -3.0187805 -2.8955646 -2.670274 -2.5079896 -2.5437236 -2.6607711 -2.5184095 -1.9501915 -1.1532692 -0.52355838 -0.33548427 -0.61174369 -1.1865886][-3.27111 -3.7613788 -3.91559 -3.7905545 -3.5677931 -3.4137466 -3.4047961 -3.3749785 -3.1439457 -2.6891475 -2.1746178 -1.8279335 -1.8413649 -2.1621373 -2.6461732][-4.0598888 -4.4041796 -4.4507117 -4.2710748 -4.0550022 -3.9267778 -3.8975816 -3.8329868 -3.6264482 -3.3331146 -3.0917087 -3.0027952 -3.1706271 -3.4986346 -3.8511858][-4.2861924 -4.5063448 -4.4694986 -4.2971163 -4.1375203 -4.0797033 -4.0934858 -4.07257 -3.9573941 -3.8049383 -3.7437212 -3.8226652 -4.025157 -4.2628236 -4.404223]]...]
INFO - root - 2017-12-16 09:16:19.781517: step 30510, loss = 0.60, batch loss = 0.35 (47.4 examples/sec; 0.169 sec/batch; 14h:09m:27s remains)
INFO - root - 2017-12-16 09:16:21.459817: step 30520, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:58m:27s remains)
INFO - root - 2017-12-16 09:16:23.152363: step 30530, loss = 0.64, batch loss = 0.38 (48.2 examples/sec; 0.166 sec/batch; 13h:55m:20s remains)
INFO - root - 2017-12-16 09:16:24.835191: step 30540, loss = 0.55, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 14h:05m:33s remains)
INFO - root - 2017-12-16 09:16:26.497981: step 30550, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 13h:51m:51s remains)
INFO - root - 2017-12-16 09:16:28.162539: step 30560, loss = 0.53, batch loss = 0.27 (49.9 examples/sec; 0.160 sec/batch; 13h:26m:52s remains)
INFO - root - 2017-12-16 09:16:29.802560: step 30570, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 14h:06m:16s remains)
INFO - root - 2017-12-16 09:16:31.442536: step 30580, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 13h:50m:22s remains)
INFO - root - 2017-12-16 09:16:33.092931: step 30590, loss = 0.55, batch loss = 0.29 (48.9 examples/sec; 0.164 sec/batch; 13h:43m:46s remains)
INFO - root - 2017-12-16 09:16:34.758706: step 30600, loss = 0.52, batch loss = 0.26 (46.9 examples/sec; 0.171 sec/batch; 14h:18m:47s remains)
2017-12-16 09:16:35.257140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0360751 -2.331027 -2.4477034 -2.4245346 -2.5551319 -2.8018839 -2.9575741 -2.7197177 -2.0528748 -0.89585066 0.42797804 1.4332078 1.7297413 1.2163606 0.11839509][-1.866518 -2.0048103 -1.9106954 -1.7841223 -1.9389827 -2.3758309 -2.7074585 -2.4454682 -1.5002074 -0.1761055 0.98447633 1.5539684 1.365907 0.42540693 -0.77772832][-2.199579 -1.9873869 -1.6726098 -1.4716187 -1.5687537 -1.9667752 -2.2472513 -1.8111099 -0.56711388 0.87170577 1.6772172 1.6872666 0.9667151 -0.26293445 -1.4457889][-2.4696574 -2.0331073 -1.6392365 -1.4253466 -1.4489741 -1.5899751 -1.5167325 -0.74167657 0.83345175 2.3192441 2.7447064 2.1623337 0.97014666 -0.49389994 -1.7160048][-2.5674324 -2.1007216 -1.7683942 -1.6372671 -1.5062337 -1.1411765 -0.42759919 0.85525227 2.4849684 3.6863229 3.5800879 2.4549396 0.89853787 -0.71690369 -1.9055363][-2.4950316 -2.280323 -2.1147668 -1.8930629 -1.3234515 -0.21019983 1.3629432 3.0690691 4.2395954 4.4987307 3.6294234 1.9866011 0.15758038 -1.3655993 -2.3231709][-2.3243711 -2.3421347 -2.1547017 -1.559135 -0.35494089 1.543972 3.9253137 5.7238932 5.7499475 4.5230122 2.7297237 0.77491045 -0.95120418 -2.1635649 -2.7754536][-1.9769535 -1.9906101 -1.5959318 -0.57698882 1.0463364 3.3191698 5.9575157 7.3751926 5.9281683 3.460633 1.2222097 -0.60802329 -1.9618479 -2.7674184 -3.035965][-1.242376 -1.0517772 -0.43839407 0.785733 2.3750012 4.1289492 5.6487751 5.7455769 3.97212 1.5094931 -0.48366392 -1.8459916 -2.6908431 -3.1144209 -3.1174068][-0.33947945 0.10513735 0.868098 2.0178845 3.0530427 3.6403334 3.6363084 2.8286574 1.2913203 -0.52023768 -1.9164746 -2.7570214 -3.1464643 -3.207865 -2.9162049][0.43233705 1.0280638 1.7015254 2.333823 2.5760386 2.1648057 1.2550895 0.1303556 -1.0332615 -2.1441185 -2.92177 -3.3177497 -3.341161 -3.0589812 -2.4700553][1.0896661 1.5030084 1.7555244 1.7514012 1.4013147 0.50170183 -0.75996125 -1.94406 -2.7212195 -3.2087636 -3.4919791 -3.5292602 -3.2511668 -2.6623912 -1.8817554][1.6788757 1.7320163 1.4913454 0.96891928 0.20195818 -0.92732644 -2.2240126 -3.2159138 -3.6022315 -3.6543703 -3.5573645 -3.2903569 -2.8099172 -2.0613003 -1.2133871][2.2234528 1.772537 0.99207091 0.082423687 -0.92678082 -2.0551448 -3.0494845 -3.5936964 -3.6071172 -3.3550918 -3.0263224 -2.6388137 -2.1247756 -1.4059982 -0.56309986][2.2312953 1.2863946 0.11893511 -0.907202 -1.8702345 -2.6773679 -3.1732843 -3.2384734 -2.9988251 -2.6541088 -2.3162816 -1.9541144 -1.4916127 -0.830315 0.012314558]]...]
INFO - root - 2017-12-16 09:16:36.955294: step 30610, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 14h:00m:21s remains)
INFO - root - 2017-12-16 09:16:38.622806: step 30620, loss = 0.55, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 13h:59m:01s remains)
INFO - root - 2017-12-16 09:16:40.323573: step 30630, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:53m:25s remains)
INFO - root - 2017-12-16 09:16:41.985959: step 30640, loss = 0.60, batch loss = 0.34 (47.5 examples/sec; 0.168 sec/batch; 14h:06m:29s remains)
INFO - root - 2017-12-16 09:16:43.684681: step 30650, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 13h:59m:59s remains)
INFO - root - 2017-12-16 09:16:45.334694: step 30660, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.165 sec/batch; 13h:52m:32s remains)
INFO - root - 2017-12-16 09:16:47.021604: step 30670, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:10m:23s remains)
INFO - root - 2017-12-16 09:16:48.717518: step 30680, loss = 0.48, batch loss = 0.22 (46.4 examples/sec; 0.173 sec/batch; 14h:28m:01s remains)
INFO - root - 2017-12-16 09:16:50.428025: step 30690, loss = 0.54, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:54m:15s remains)
INFO - root - 2017-12-16 09:16:52.124499: step 30700, loss = 0.55, batch loss = 0.29 (46.9 examples/sec; 0.170 sec/batch; 14h:17m:31s remains)
2017-12-16 09:16:52.617358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7884681 -2.1046743 -1.5248775 -0.71355665 -0.64708936 -0.74638796 -0.36247253 0.0049014091 -0.083913088 -0.35688257 -0.76115561 -0.85589004 -0.40905344 0.073226452 0.10259819][-1.7814493 -2.23145 -1.8738288 -1.3480226 -1.4963796 -1.7178167 -1.3479458 -1.0043079 -1.1767823 -1.5757699 -1.9692924 -1.8807125 -1.2116811 -0.57118165 -0.40704203][-1.6350107 -2.1112008 -1.8735754 -1.5652106 -1.8508319 -2.1846669 -1.9854654 -1.8522663 -2.33798 -2.9946518 -3.4263492 -3.2852 -2.5183296 -1.7776251 -1.4875382][-1.281927 -1.6811168 -1.4410827 -1.1836478 -1.4635227 -1.7928374 -1.7003812 -1.7939503 -2.5906076 -3.5573187 -4.1887321 -4.1844029 -3.5277739 -2.7640512 -2.3360937][-1.0656167 -1.2958529 -0.90891016 -0.56827068 -0.72536278 -0.84238207 -0.67354596 -0.92703807 -1.9569103 -3.1573598 -4.0000544 -4.1853728 -3.7022357 -3.021214 -2.5229003][-1.2096465 -1.2323651 -0.63672924 -0.14053202 -0.0054824352 0.30846167 0.68213224 0.34773111 -0.75691819 -1.9895481 -2.9184935 -3.2535851 -2.959749 -2.4939051 -2.1083744][-1.6263266 -1.4626224 -0.67358649 -0.028676987 0.4631567 1.3396449 1.9858053 1.6435463 0.57874751 -0.50362945 -1.3748496 -1.8016087 -1.717422 -1.5046117 -1.3259161][-2.1835637 -1.9297826 -1.1134516 -0.3970021 0.34004784 1.5506568 2.387357 2.0577896 1.2208536 0.50620127 -0.1591835 -0.56073976 -0.57584083 -0.49888802 -0.4937439][-2.5916643 -2.341234 -1.5891566 -0.94909871 -0.24643993 0.92442656 1.7263315 1.4535098 0.88803315 0.55822754 0.17115259 -0.086042166 -0.03519845 -0.0095074177 -0.15141654][-2.4804835 -2.189132 -1.5908451 -1.2469201 -0.91316438 -0.074470282 0.48992658 0.20955706 -0.20512295 -0.28651857 -0.43617809 -0.54711711 -0.38905942 -0.34517312 -0.56942391][-1.9023621 -1.543231 -1.1298904 -1.212095 -1.332056 -0.96333456 -0.749887 -1.1807905 -1.5280225 -1.4941654 -1.4959636 -1.524599 -1.3616828 -1.3182795 -1.5033014][-1.1148877 -0.65089691 -0.30372214 -0.7091192 -1.2618735 -1.3838785 -1.5701237 -2.2275548 -2.6285026 -2.6036756 -2.5449355 -2.5189152 -2.3987811 -2.3557572 -2.4116323][-0.53393114 0.092895985 0.47948647 -0.09094286 -0.91177809 -1.3810487 -1.91925 -2.7475264 -3.1542935 -3.0917218 -2.99688 -2.9461021 -2.876888 -2.8435209 -2.7686634][-0.29402757 0.59328008 1.1169827 0.502604 -0.45508766 -1.1181974 -1.8479773 -2.7414041 -3.1338205 -3.0669618 -2.9596372 -2.9097333 -2.9086008 -2.8434231 -2.6234024][-0.089910746 0.93520355 1.5697315 0.99122286 0.01361084 -0.68416309 -1.473062 -2.4084783 -2.8511934 -2.834754 -2.7276068 -2.7124736 -2.7334008 -2.5852015 -2.2398643]]...]
INFO - root - 2017-12-16 09:16:54.352772: step 30710, loss = 0.56, batch loss = 0.30 (46.9 examples/sec; 0.171 sec/batch; 14h:17m:37s remains)
INFO - root - 2017-12-16 09:16:56.036018: step 30720, loss = 0.54, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 13h:38m:13s remains)
INFO - root - 2017-12-16 09:16:57.731350: step 30730, loss = 0.54, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 14h:21m:31s remains)
INFO - root - 2017-12-16 09:16:59.402560: step 30740, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:45m:34s remains)
INFO - root - 2017-12-16 09:17:01.071225: step 30750, loss = 0.50, batch loss = 0.25 (48.0 examples/sec; 0.166 sec/batch; 13h:57m:19s remains)
INFO - root - 2017-12-16 09:17:02.756247: step 30760, loss = 0.57, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 13h:52m:19s remains)
INFO - root - 2017-12-16 09:17:04.435797: step 30770, loss = 0.60, batch loss = 0.34 (46.3 examples/sec; 0.173 sec/batch; 14h:28m:16s remains)
INFO - root - 2017-12-16 09:17:06.089097: step 30780, loss = 0.57, batch loss = 0.31 (50.0 examples/sec; 0.160 sec/batch; 13h:24m:32s remains)
INFO - root - 2017-12-16 09:17:07.757055: step 30790, loss = 0.71, batch loss = 0.45 (46.6 examples/sec; 0.172 sec/batch; 14h:23m:22s remains)
INFO - root - 2017-12-16 09:17:09.458574: step 30800, loss = 0.79, batch loss = 0.53 (46.7 examples/sec; 0.171 sec/batch; 14h:21m:34s remains)
2017-12-16 09:17:09.957884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0007257 -2.950253 -2.6345551 -2.0548425 -1.3948746 -1.0300181 -0.90852451 -1.0925971 -1.5732661 -2.0977695 -2.3805711 -2.682857 -2.886848 -2.9063087 -2.8160768][-3.0887034 -3.0888844 -2.7983882 -2.1211035 -1.3925463 -1.0140759 -0.87409031 -0.98479962 -1.3106288 -1.6987405 -1.945733 -2.3333814 -2.616472 -2.7189252 -2.7540612][-3.1758456 -3.1128733 -2.7947202 -2.086411 -1.3629276 -0.94463623 -0.759789 -0.72228885 -0.76587641 -0.9850713 -1.2759343 -1.8250132 -2.3235784 -2.5893261 -2.7509313][-3.0990772 -2.851474 -2.4026625 -1.6936126 -1.0073657 -0.58648896 -0.28697014 -0.094757795 0.037528038 -0.0756762 -0.46119881 -1.2258948 -2.0088594 -2.4923964 -2.7585588][-2.7909548 -2.3313785 -1.7725794 -1.0950555 -0.55454147 -0.21734023 0.2473309 0.72227454 1.0465145 0.93585753 0.45741773 -0.5670867 -1.6940217 -2.3885286 -2.6891043][-2.3291738 -1.7661256 -1.1768147 -0.5763092 -0.15306234 0.17433858 0.87968755 1.7555029 2.2796934 2.1171052 1.3794208 0.031409502 -1.4042455 -2.2560453 -2.5969615][-1.7117944 -1.1078513 -0.59756958 -0.052481174 0.34576583 0.83927751 1.8539412 3.0997498 3.7275937 3.4331486 2.2984941 0.58688855 -1.1551018 -2.1231508 -2.4883308][-1.0593826 -0.4355942 0.0063738823 0.48770094 0.86723042 1.4942057 2.6416867 3.9283693 4.5248642 4.0451765 2.65719 0.76586318 -1.0957884 -2.1012471 -2.4472394][-0.66105139 -0.075262785 0.21130157 0.51727915 0.85887337 1.5034852 2.6679533 3.943372 4.3950758 3.8376272 2.4458125 0.51495433 -1.3058455 -2.260056 -2.5094571][-0.65282726 -0.260386 -0.23250079 -0.10256076 0.19102478 0.86335492 2.0078995 3.2994215 3.58687 3.0150578 1.7500265 -0.055165291 -1.686312 -2.4844878 -2.6076202][-0.96810937 -0.85616517 -1.0249445 -0.97339547 -0.66019046 0.018012524 1.0525978 2.1243641 2.2104518 1.6435444 0.58482623 -0.90596604 -2.1761398 -2.7069497 -2.7071853][-1.3951055 -1.4980538 -1.7550318 -1.7388058 -1.4610037 -0.9309988 -0.21297836 0.48107958 0.51813054 0.11438298 -0.64845979 -1.73139 -2.5672083 -2.8313565 -2.7228227][-1.7916343 -1.9965228 -2.2455771 -2.3181832 -2.1941869 -1.9474294 -1.5716081 -1.1454479 -1.0725678 -1.2506121 -1.7100315 -2.35542 -2.7972713 -2.8170786 -2.6207824][-2.0766582 -2.3198581 -2.5491934 -2.7050393 -2.7239964 -2.6631818 -2.4404049 -2.1083126 -1.9726788 -2.0323079 -2.2911198 -2.6490736 -2.8287654 -2.7011333 -2.4583464][-2.1769526 -2.4013255 -2.5859878 -2.7301376 -2.821475 -2.8329253 -2.6687875 -2.3797998 -2.2665591 -2.3099837 -2.4769623 -2.6519282 -2.678998 -2.4923587 -2.257273]]...]
INFO - root - 2017-12-16 09:17:11.703933: step 30810, loss = 0.51, batch loss = 0.25 (46.7 examples/sec; 0.171 sec/batch; 14h:21m:46s remains)
INFO - root - 2017-12-16 09:17:13.447769: step 30820, loss = 0.51, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:58m:12s remains)
INFO - root - 2017-12-16 09:17:15.096257: step 30830, loss = 0.60, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 13h:39m:16s remains)
INFO - root - 2017-12-16 09:17:16.775768: step 30840, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:54m:09s remains)
INFO - root - 2017-12-16 09:17:18.475456: step 30850, loss = 0.47, batch loss = 0.21 (45.5 examples/sec; 0.176 sec/batch; 14h:43m:15s remains)
INFO - root - 2017-12-16 09:17:20.152398: step 30860, loss = 0.66, batch loss = 0.40 (48.5 examples/sec; 0.165 sec/batch; 13h:48m:29s remains)
INFO - root - 2017-12-16 09:17:21.816896: step 30870, loss = 0.55, batch loss = 0.29 (48.9 examples/sec; 0.164 sec/batch; 13h:43m:11s remains)
INFO - root - 2017-12-16 09:17:23.495917: step 30880, loss = 0.58, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:56m:09s remains)
INFO - root - 2017-12-16 09:17:25.162750: step 30890, loss = 0.67, batch loss = 0.41 (49.3 examples/sec; 0.162 sec/batch; 13h:36m:30s remains)
INFO - root - 2017-12-16 09:17:26.807884: step 30900, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 13h:47m:56s remains)
2017-12-16 09:17:27.245831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6320846 -2.6503084 -2.5324092 -2.3680487 -2.3110063 -2.4150784 -2.5664952 -2.6451693 -2.5985057 -2.4749873 -2.3375459 -2.2139311 -2.1301873 -2.1160116 -2.1409302][-3.0809555 -3.1053438 -2.8839798 -2.5628884 -2.4372768 -2.6204545 -2.9139321 -3.088459 -3.0576108 -2.8836913 -2.6899786 -2.5157425 -2.3767366 -2.3342445 -2.3374016][-3.4105659 -3.4100258 -3.0758796 -2.6079996 -2.4011002 -2.6157093 -2.9878442 -3.1990249 -3.1696434 -2.9708223 -2.7729387 -2.6231816 -2.4980936 -2.4709651 -2.4585845][-3.4589844 -3.3998666 -2.980412 -2.3636689 -1.9949023 -2.0701566 -2.3117781 -2.4777374 -2.5037737 -2.4394975 -2.3963675 -2.3973012 -2.3962712 -2.4317222 -2.411026][-3.1727479 -3.0061355 -2.4910884 -1.7340775 -1.0904384 -0.74534881 -0.595613 -0.62897849 -0.85452068 -1.1769698 -1.5394177 -1.8646092 -2.1065991 -2.2437091 -2.2616415][-2.7952268 -2.4702392 -1.7677312 -0.75912035 0.22094488 1.0795419 1.7500772 1.8564916 1.3353519 0.5180068 -0.29150486 -0.9718895 -1.4818757 -1.7991184 -1.9370552][-2.5322461 -2.0277438 -1.0864515 0.21124363 1.4578104 2.637116 3.7500587 4.1024251 3.4116859 2.2513819 1.1319089 0.15553427 -0.62512875 -1.1876867 -1.4881852][-2.4577494 -1.7626358 -0.57036269 0.96025658 2.2573714 3.3683643 4.4377723 4.8795457 4.3021183 3.0932007 1.9071612 0.85401177 -0.063191891 -0.81788361 -1.26918][-2.4303787 -1.646101 -0.41408968 0.96913743 1.9686084 2.673696 3.3104711 3.5885906 3.0950232 2.1649117 1.2735448 0.50808597 -0.26357508 -0.98492932 -1.3617389][-2.3027182 -1.5779169 -0.55017245 0.41096592 0.91537595 1.1634016 1.3950253 1.4826629 1.1295335 0.55190468 0.058468103 -0.31234169 -0.74574256 -1.1915258 -1.378437][-2.0798824 -1.4369159 -0.68300414 -0.17661071 -0.11993098 -0.27705121 -0.36757588 -0.43695915 -0.69318986 -1.0199947 -1.2015958 -1.2160705 -1.2294205 -1.2942809 -1.2961749][-1.9182727 -1.2980705 -0.73811221 -0.65345013 -1.0395522 -1.5184021 -1.8074543 -1.9669428 -2.1893985 -2.3775749 -2.3009787 -1.9747709 -1.6422867 -1.4471565 -1.3717278][-2.0859792 -1.5479757 -1.2168858 -1.3772454 -1.9365941 -2.4941616 -2.7360783 -2.8663983 -3.0163202 -3.0775633 -2.8477848 -2.3825922 -1.9398103 -1.714186 -1.684299][-2.6725268 -2.3352873 -2.163955 -2.3310242 -2.7576787 -3.1322131 -3.1772397 -3.1047125 -3.0974867 -3.0436497 -2.7890058 -2.4110181 -2.1266842 -2.0719068 -2.1750891][-3.1151762 -2.9290581 -2.7856131 -2.8142786 -2.9691391 -3.0616927 -2.9211826 -2.695864 -2.5928044 -2.5052915 -2.3187528 -2.123657 -2.0530837 -2.1887205 -2.432725]]...]
INFO - root - 2017-12-16 09:17:28.891622: step 30910, loss = 0.50, batch loss = 0.25 (47.5 examples/sec; 0.168 sec/batch; 14h:06m:18s remains)
INFO - root - 2017-12-16 09:17:30.585694: step 30920, loss = 0.54, batch loss = 0.29 (46.3 examples/sec; 0.173 sec/batch; 14h:28m:58s remains)
INFO - root - 2017-12-16 09:17:32.295992: step 30930, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:53m:13s remains)
INFO - root - 2017-12-16 09:17:34.004799: step 30940, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:45m:35s remains)
INFO - root - 2017-12-16 09:17:35.678990: step 30950, loss = 0.61, batch loss = 0.35 (48.0 examples/sec; 0.166 sec/batch; 13h:56m:47s remains)
INFO - root - 2017-12-16 09:17:37.340281: step 30960, loss = 0.62, batch loss = 0.36 (49.1 examples/sec; 0.163 sec/batch; 13h:39m:30s remains)
INFO - root - 2017-12-16 09:17:39.044547: step 30970, loss = 0.72, batch loss = 0.46 (46.5 examples/sec; 0.172 sec/batch; 14h:23m:58s remains)
INFO - root - 2017-12-16 09:17:40.764677: step 30980, loss = 0.48, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 14h:10m:08s remains)
INFO - root - 2017-12-16 09:17:42.470015: step 30990, loss = 0.72, batch loss = 0.46 (44.9 examples/sec; 0.178 sec/batch; 14h:56m:12s remains)
INFO - root - 2017-12-16 09:17:44.176712: step 31000, loss = 0.56, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 13h:57m:30s remains)
2017-12-16 09:17:44.660937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0499523 -1.3866756 -1.8992479 -2.5265894 -3.4558609 -4.12395 -4.2755914 -4.2740707 -4.3028011 -4.159502 -3.7232039 -2.909019 -1.9485669 -1.3311498 -1.0826408][-1.204788 -1.5755019 -2.2926319 -3.1199183 -4.0275431 -4.4738646 -4.4044323 -4.3538103 -4.501544 -4.5278921 -4.1994171 -3.4669852 -2.5054703 -1.8190274 -1.467499][-1.6472647 -2.0033922 -2.8472371 -3.8419626 -4.6197958 -4.5755639 -3.9859691 -3.6768551 -3.8806372 -4.1496119 -4.1569691 -3.8168387 -3.1807413 -2.5901291 -2.2085867][-2.3863554 -2.7339649 -3.5375733 -4.5063238 -5.0113678 -4.4281073 -3.1658385 -2.3429134 -2.4344835 -2.9023178 -3.4152093 -3.6819468 -3.6280279 -3.3778687 -3.083751][-3.0509157 -3.3796897 -3.988636 -4.6913824 -4.7534981 -3.6268022 -1.6680325 -0.24044394 -0.19635844 -1.0429088 -2.1506672 -3.0183778 -3.5067077 -3.7005625 -3.6571562][-3.1329193 -3.3527002 -3.6353462 -3.9302719 -3.55188 -1.9486029 0.66560459 2.6656146 2.5402894 1.0265625 -0.69163561 -1.9573834 -2.7158287 -3.151814 -3.3914661][-2.6443245 -2.6351166 -2.6181135 -2.5013881 -1.758004 0.11106396 3.2218041 5.6104159 5.0196342 2.608891 0.40319157 -0.85854018 -1.5057807 -1.990539 -2.5017102][-1.9514596 -1.7057073 -1.4746121 -1.1022705 -0.15617871 1.8257847 5.0453339 7.4096303 6.3204656 3.3680887 1.0261447 0.032890558 -0.35744286 -0.92856848 -1.6984742][-1.6131977 -1.2745824 -1.0321602 -0.61571348 0.30236506 2.0685387 4.661787 6.3795533 5.290102 2.6523929 0.75380445 0.1650455 -0.012505293 -0.66908514 -1.5769436][-1.8355315 -1.7055979 -1.7021527 -1.5160792 -0.81535149 0.41158509 2.0200543 3.0485244 2.1956406 0.35251117 -0.828181 -1.0069942 -0.99366009 -1.5482613 -2.3713119][-2.34303 -2.4830806 -2.7830498 -2.7953823 -2.2834227 -1.5408212 -0.78633487 -0.32929063 -0.99698853 -2.1322412 -2.7362118 -2.7019439 -2.5501995 -2.862916 -3.400955][-2.7542086 -3.0894737 -3.5319963 -3.5890424 -3.2133322 -2.8401945 -2.7272885 -2.729352 -3.1943886 -3.8068857 -4.0596437 -3.9566827 -3.7632537 -3.7945895 -3.9435682][-2.8437924 -3.2325368 -3.6628342 -3.753778 -3.5506186 -3.4710717 -3.6946385 -3.9452434 -4.242836 -4.4709282 -4.4865932 -4.3641481 -4.1677551 -3.9813597 -3.8065848][-2.570796 -2.9135394 -3.2245381 -3.328074 -3.3238163 -3.4762578 -3.786093 -4.0034857 -4.069921 -4.0401497 -3.9716504 -3.8830559 -3.71431 -3.4611745 -3.1586962][-2.0581732 -2.2774444 -2.4434872 -2.5353303 -2.6216457 -2.8080242 -3.0199344 -3.1145008 -3.0679166 -2.9668157 -2.8961773 -2.843538 -2.7380481 -2.5595927 -2.3314152]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:17:46.375004: step 31010, loss = 0.49, batch loss = 0.23 (47.1 examples/sec; 0.170 sec/batch; 14h:13m:50s remains)
INFO - root - 2017-12-16 09:17:48.094697: step 31020, loss = 0.61, batch loss = 0.35 (47.4 examples/sec; 0.169 sec/batch; 14h:07m:45s remains)
INFO - root - 2017-12-16 09:17:49.771199: step 31030, loss = 0.53, batch loss = 0.28 (48.6 examples/sec; 0.165 sec/batch; 13h:46m:40s remains)
INFO - root - 2017-12-16 09:17:51.447929: step 31040, loss = 0.56, batch loss = 0.30 (48.5 examples/sec; 0.165 sec/batch; 13h:49m:32s remains)
INFO - root - 2017-12-16 09:17:53.159881: step 31050, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:09m:04s remains)
INFO - root - 2017-12-16 09:17:54.829621: step 31060, loss = 0.60, batch loss = 0.35 (47.9 examples/sec; 0.167 sec/batch; 13h:59m:35s remains)
INFO - root - 2017-12-16 09:17:56.504901: step 31070, loss = 0.56, batch loss = 0.30 (47.6 examples/sec; 0.168 sec/batch; 14h:04m:03s remains)
INFO - root - 2017-12-16 09:17:58.194700: step 31080, loss = 0.70, batch loss = 0.44 (45.7 examples/sec; 0.175 sec/batch; 14h:39m:49s remains)
INFO - root - 2017-12-16 09:17:59.881413: step 31090, loss = 0.57, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 14h:22m:39s remains)
INFO - root - 2017-12-16 09:18:01.560054: step 31100, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 13h:59m:48s remains)
2017-12-16 09:18:02.080612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091925621 0.27071929 0.3321104 0.0014107227 -0.66362906 -1.3250014 -1.5907247 -1.5757298 -1.6379597 -1.844891 -2.0101116 -2.1032147 -2.1433623 -2.0493035 -1.8875157][0.21891594 0.6279254 0.64268994 0.18041229 -0.60220838 -1.3254024 -1.6019282 -1.5396768 -1.5989521 -1.8263829 -2.0263672 -2.1484518 -2.2024453 -2.0879278 -1.8685148][-0.33425379 0.081120014 0.12132049 -0.26050353 -0.95835054 -1.6197045 -1.8194675 -1.6392565 -1.6068975 -1.7853353 -1.9842995 -2.1338859 -2.1959639 -2.0408602 -1.7639484][-0.97869742 -0.53995037 -0.38940537 -0.57684696 -1.0758048 -1.6397896 -1.8238724 -1.654761 -1.65869 -1.8570721 -2.0581553 -2.1680663 -2.1373327 -1.8856413 -1.5065994][-1.2422017 -0.73409784 -0.45587635 -0.41670609 -0.69222546 -1.1395826 -1.3425087 -1.2956219 -1.4817622 -1.8642398 -2.175966 -2.3170245 -2.2413223 -1.8992293 -1.4072038][-1.1784493 -0.62561178 -0.28519082 -0.10989976 -0.17708135 -0.42443013 -0.56426752 -0.62612283 -1.0662972 -1.7170434 -2.2481318 -2.473557 -2.3662474 -1.9399331 -1.3952014][-1.2884235 -0.8069725 -0.5014205 -0.26614928 -0.11463761 -0.10646963 -0.058086157 -0.069938421 -0.58703268 -1.3800558 -2.094543 -2.4602165 -2.3951957 -1.9536835 -1.4076126][-1.4277701 -1.0743637 -0.92972839 -0.75791895 -0.45929098 -0.25225639 -0.044452429 0.064407825 -0.3215971 -1.0784142 -1.8200834 -2.2363021 -2.2230167 -1.8292964 -1.3625801][-1.3518317 -1.1329889 -1.1764781 -1.1597319 -0.92303479 -0.72060311 -0.48838925 -0.27871084 -0.49374914 -1.0881582 -1.6850439 -2.0197372 -2.0255446 -1.675163 -1.2469176][-0.78671229 -0.74005139 -1.0680331 -1.3784157 -1.4289154 -1.4349792 -1.282609 -1.068417 -1.1454953 -1.5069122 -1.8831685 -2.0513253 -1.9739444 -1.6110487 -1.2025477][-0.1272583 -0.21589017 -0.77560735 -1.4051191 -1.7967513 -2.0886371 -2.0907588 -1.9401404 -1.946023 -2.1225324 -2.2672074 -2.2527802 -2.0893009 -1.750879 -1.4105685][0.18890452 0.057242393 -0.60922539 -1.4267726 -2.074492 -2.588551 -2.7393811 -2.6621208 -2.6773071 -2.7631929 -2.7485 -2.6047626 -2.3939493 -2.1142132 -1.84552][0.13389564 0.02884388 -0.59089839 -1.412608 -2.1682324 -2.804461 -3.0930367 -3.1120095 -3.1367691 -3.186336 -3.1110601 -2.9570708 -2.7977688 -2.6440229 -2.4787948][0.079246759 0.028414965 -0.51432264 -1.2920922 -2.1040742 -2.8313112 -3.2254279 -3.3034115 -3.2764046 -3.2317832 -3.082962 -2.9305902 -2.8365395 -2.7785816 -2.652683][0.14990544 0.055921316 -0.51951754 -1.3083833 -2.1740489 -2.9184771 -3.29419 -3.3037868 -3.0952759 -2.796262 -2.4833033 -2.3049619 -2.2841153 -2.2806165 -2.1391933]]...]
INFO - root - 2017-12-16 09:18:03.808944: step 31110, loss = 0.58, batch loss = 0.32 (47.1 examples/sec; 0.170 sec/batch; 14h:12m:46s remains)
INFO - root - 2017-12-16 09:18:05.493894: step 31120, loss = 0.65, batch loss = 0.39 (47.5 examples/sec; 0.168 sec/batch; 14h:05m:10s remains)
INFO - root - 2017-12-16 09:18:07.162273: step 31130, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.163 sec/batch; 13h:36m:19s remains)
INFO - root - 2017-12-16 09:18:08.872754: step 31140, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 13h:59m:23s remains)
INFO - root - 2017-12-16 09:18:10.558795: step 31150, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 13h:47m:21s remains)
INFO - root - 2017-12-16 09:18:12.266788: step 31160, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 14h:02m:15s remains)
INFO - root - 2017-12-16 09:18:13.948368: step 31170, loss = 0.55, batch loss = 0.29 (46.9 examples/sec; 0.171 sec/batch; 14h:17m:08s remains)
INFO - root - 2017-12-16 09:18:15.631532: step 31180, loss = 0.60, batch loss = 0.34 (48.2 examples/sec; 0.166 sec/batch; 13h:53m:35s remains)
INFO - root - 2017-12-16 09:18:17.324290: step 31190, loss = 0.58, batch loss = 0.32 (46.7 examples/sec; 0.171 sec/batch; 14h:21m:01s remains)
INFO - root - 2017-12-16 09:18:19.024572: step 31200, loss = 0.48, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 14h:00m:54s remains)
2017-12-16 09:18:19.486552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29136181 -0.65262151 -0.89245212 -1.1001161 -1.1128571 -0.86889088 -0.45042861 -0.2966361 -0.6463412 -1.3093677 -2.293427 -3.0327392 -3.0785863 -3.0624204 -3.2583847][-0.28542686 -0.87430835 -1.4168353 -1.7752833 -1.8290801 -1.3173699 -0.29580927 0.22385621 -0.14521003 -1.1217692 -2.3534858 -3.2257221 -3.3107793 -3.233217 -3.3442156][-0.66532743 -1.4497286 -2.1392143 -2.5523796 -2.5398006 -1.6121488 0.042393684 1.037832 0.59766984 -0.76904404 -2.2953384 -3.3353066 -3.5317006 -3.4765923 -3.478301][-1.204044 -2.1718695 -2.8746798 -3.1649916 -2.9020381 -1.3775744 1.0499926 2.5853963 1.9445839 0.0005428791 -1.9871696 -3.3346121 -3.7011993 -3.7037516 -3.6244631][-1.5189182 -2.6437566 -3.3605466 -3.4767647 -2.8871312 -0.77717435 2.4062147 4.499208 3.6190605 0.9726305 -1.5639307 -3.227242 -3.7671714 -3.824842 -3.6636748][-1.7126799 -2.919714 -3.573081 -3.5703678 -2.5706165 0.088475943 3.9036427 6.5695224 5.378304 2.0016074 -1.1278435 -3.0618725 -3.72819 -3.8292732 -3.609268][-1.9554262 -3.0632138 -3.6373563 -3.5039451 -2.1892319 0.95654678 5.5630569 8.967535 7.21637 2.9694533 -0.68007612 -2.8451281 -3.6221535 -3.8084111 -3.5814471][-2.3800437 -3.2844305 -3.7741766 -3.5387712 -2.1190784 1.3397334 6.597858 10.610588 8.1588936 3.3252821 -0.56888008 -2.7671318 -3.5820036 -3.8720794 -3.597621][-2.5948884 -3.3156371 -3.8163595 -3.618309 -2.30523 0.96065855 5.8445106 9.11796 7.2075543 2.7721786 -0.86677158 -2.9097857 -3.6971028 -3.9462438 -3.540508][-2.3384616 -2.957592 -3.557415 -3.5014405 -2.3376582 0.36233497 4.328136 6.9882016 5.7279782 1.9832392 -1.248278 -3.063803 -3.757781 -3.9200311 -3.3627982][-1.657009 -2.278589 -3.0717912 -3.2386131 -2.3427105 -0.33396935 2.7193322 4.9189897 4.048913 0.91593218 -1.8148979 -3.2762661 -3.8196371 -3.8661475 -3.2273338][-1.1971755 -1.7545494 -2.7096729 -3.1566663 -2.6154912 -1.2394531 1.056227 2.8200803 2.119277 -0.38338256 -2.4843452 -3.4660296 -3.8284602 -3.8291905 -3.2069426][-1.3724453 -1.8021476 -2.6746259 -3.2339664 -3.101301 -2.2682786 -0.6069057 0.85446954 0.38307405 -1.5119057 -3.0281935 -3.6099477 -3.8562379 -3.8655167 -3.2942019][-2.0239599 -2.2745483 -2.9022851 -3.4214282 -3.5524755 -3.1500497 -2.0034308 -0.818261 -1.0089396 -2.3713441 -3.5021949 -3.9243314 -4.0969357 -4.0543785 -3.4626784][-2.6471381 -2.7912424 -3.2109225 -3.6215754 -3.8361447 -3.7040691 -2.9835975 -2.0815616 -2.0309131 -2.9236393 -3.7859302 -4.1344609 -4.22748 -4.0648551 -3.4845495]]...]
INFO - root - 2017-12-16 09:18:21.162243: step 31210, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 14h:03m:51s remains)
INFO - root - 2017-12-16 09:18:22.856239: step 31220, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 14h:11m:46s remains)
INFO - root - 2017-12-16 09:18:24.542217: step 31230, loss = 0.59, batch loss = 0.33 (46.3 examples/sec; 0.173 sec/batch; 14h:27m:45s remains)
INFO - root - 2017-12-16 09:18:26.246538: step 31240, loss = 0.58, batch loss = 0.32 (47.0 examples/sec; 0.170 sec/batch; 14h:14m:11s remains)
INFO - root - 2017-12-16 09:18:27.960273: step 31250, loss = 0.68, batch loss = 0.42 (48.4 examples/sec; 0.165 sec/batch; 13h:50m:38s remains)
INFO - root - 2017-12-16 09:18:29.641927: step 31260, loss = 0.60, batch loss = 0.34 (47.2 examples/sec; 0.170 sec/batch; 14h:11m:06s remains)
INFO - root - 2017-12-16 09:18:31.321728: step 31270, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 14h:06m:44s remains)
INFO - root - 2017-12-16 09:18:33.016319: step 31280, loss = 0.59, batch loss = 0.33 (46.9 examples/sec; 0.171 sec/batch; 14h:16m:00s remains)
INFO - root - 2017-12-16 09:18:34.684035: step 31290, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:08m:38s remains)
INFO - root - 2017-12-16 09:18:36.356855: step 31300, loss = 0.58, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 13h:33m:01s remains)
2017-12-16 09:18:36.858399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.50193381 -0.47435391 -0.73806083 -1.170073 -1.5271226 -1.7442648 -1.9531851 -2.2867897 -2.3640471 -1.8984694 -1.3041573 -1.0086772 -1.0659957 -1.4221061 -2.1241524][-0.52332985 -0.35654855 -0.46709263 -0.826334 -1.1972698 -1.4968107 -1.7896156 -2.22285 -2.4901965 -2.1860926 -1.6077734 -1.2288384 -1.1747953 -1.4171017 -2.0060334][-0.3105762 -0.058244228 -0.061179876 -0.35978937 -0.741747 -1.1021905 -1.4243273 -1.9182634 -2.270968 -2.1027203 -1.5572584 -1.0404088 -0.69824886 -0.6900984 -1.1378329][0.16156507 0.40864754 0.41770911 0.053822517 -0.36281133 -0.706982 -0.9919095 -1.4082992 -1.8078134 -1.7671199 -1.2842393 -0.64118481 -0.10832906 0.10132718 -0.098467112][0.53994465 0.84300733 0.81788945 0.44759607 0.0048086643 -0.28322268 -0.4101218 -0.63708055 -1.044762 -1.2511456 -0.97161329 -0.44278884 0.15236974 0.52760935 0.46934223][0.78424478 1.0148067 1.1103425 0.86227989 0.54072618 0.50816536 0.64499021 0.64685106 0.20830178 -0.26135302 -0.37659287 -0.16161323 0.29448342 0.66886234 0.583879][0.80230141 0.84927011 1.0299795 0.95433831 0.94113731 1.2942243 1.793452 2.0111949 1.5923917 0.88895941 0.4250195 0.28091288 0.45848346 0.48504949 0.10302925][0.76024461 0.79332423 0.92405581 0.86969638 0.92751575 1.4871986 2.21861 2.6094072 2.2837079 1.5866392 1.0114381 0.63589931 0.49466252 0.22790337 -0.44895792][0.27060747 0.39076018 0.41862249 0.25026321 0.19442487 0.72557068 1.4746945 1.864126 1.6629503 1.182857 0.73284292 0.34056807 0.085189819 -0.36997485 -1.2836059][-0.67673445 -0.46539819 -0.42644691 -0.59897149 -0.82675493 -0.57505524 -0.089621067 0.22129083 0.149503 -0.086447716 -0.3390255 -0.52873731 -0.73830187 -1.2358512 -2.1528323][-1.7229099 -1.4773418 -1.4728653 -1.601862 -1.8614213 -1.8171029 -1.5740225 -1.4068751 -1.4496949 -1.6194155 -1.7245501 -1.7074597 -1.7419455 -2.0914741 -2.7311718][-2.4947872 -2.3295839 -2.3460562 -2.4514861 -2.6704788 -2.7232316 -2.624455 -2.5847895 -2.6818943 -2.8520954 -2.88277 -2.716228 -2.5404871 -2.6011002 -2.8598747][-2.6578321 -2.6630952 -2.7974031 -2.9349179 -3.1215906 -3.2710404 -3.2907012 -3.3105118 -3.3760724 -3.4496794 -3.3775682 -3.1124206 -2.7815583 -2.6116803 -2.6233554][-2.2301764 -2.3563333 -2.5969558 -2.8330579 -3.09867 -3.3166986 -3.4078252 -3.4299259 -3.3928566 -3.365243 -3.2392871 -2.926168 -2.5663826 -2.3107002 -2.1957831][-1.6875541 -1.9121968 -2.212147 -2.506614 -2.8034885 -3.0383618 -3.1338735 -3.0726218 -2.9366865 -2.8068392 -2.63442 -2.3582921 -2.0497644 -1.8092147 -1.7028277]]...]
INFO - root - 2017-12-16 09:18:38.562352: step 31310, loss = 0.51, batch loss = 0.25 (41.5 examples/sec; 0.193 sec/batch; 16h:06m:30s remains)
INFO - root - 2017-12-16 09:18:40.262427: step 31320, loss = 0.50, batch loss = 0.24 (48.4 examples/sec; 0.165 sec/batch; 13h:49m:33s remains)
INFO - root - 2017-12-16 09:18:41.946677: step 31330, loss = 0.64, batch loss = 0.38 (47.9 examples/sec; 0.167 sec/batch; 13h:58m:20s remains)
INFO - root - 2017-12-16 09:18:43.599994: step 31340, loss = 0.50, batch loss = 0.24 (47.9 examples/sec; 0.167 sec/batch; 13h:57m:39s remains)
INFO - root - 2017-12-16 09:18:45.295860: step 31350, loss = 0.55, batch loss = 0.29 (46.3 examples/sec; 0.173 sec/batch; 14h:26m:27s remains)
INFO - root - 2017-12-16 09:18:47.024417: step 31360, loss = 0.56, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 14h:01m:33s remains)
INFO - root - 2017-12-16 09:18:48.747186: step 31370, loss = 0.55, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 13h:57m:52s remains)
INFO - root - 2017-12-16 09:18:50.464789: step 31380, loss = 0.48, batch loss = 0.22 (46.6 examples/sec; 0.172 sec/batch; 14h:21m:17s remains)
INFO - root - 2017-12-16 09:18:52.143542: step 31390, loss = 0.67, batch loss = 0.41 (48.3 examples/sec; 0.166 sec/batch; 13h:51m:59s remains)
INFO - root - 2017-12-16 09:18:53.878978: step 31400, loss = 0.51, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 14h:13m:19s remains)
2017-12-16 09:18:54.383960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4046521 -1.6140847 -1.7791224 -1.8384027 -1.6923296 -1.3406618 -0.95335364 -0.76182258 -0.81511796 -1.1011226 -1.5275793 -1.8965449 -2.0592005 -2.0209219 -1.9136591][-1.3895495 -1.4883533 -1.634241 -1.7821093 -1.771503 -1.539111 -1.2019368 -0.990337 -0.976804 -1.2254659 -1.7145629 -2.2126906 -2.4694164 -2.4922833 -2.4613998][-1.2593013 -1.2382469 -1.363048 -1.6251473 -1.801311 -1.7595406 -1.5533555 -1.360152 -1.2583482 -1.3936067 -1.8484571 -2.3816888 -2.667433 -2.7103262 -2.718823][-1.1437545 -1.063087 -1.1991618 -1.5601728 -1.9024869 -2.042779 -1.9666262 -1.7754114 -1.5403928 -1.5036469 -1.8315556 -2.3058844 -2.55412 -2.5685413 -2.5507627][-1.2491224 -1.1786656 -1.3239015 -1.6873753 -2.0528054 -2.2374249 -2.1861894 -1.9325414 -1.5503342 -1.3646722 -1.58864 -2.03041 -2.2827373 -2.2536716 -2.1445138][-1.6105385 -1.554302 -1.6281965 -1.8271006 -2.0092306 -2.07048 -1.9186889 -1.5453734 -1.0581489 -0.82450843 -1.0593191 -1.5685318 -1.9385242 -1.9810922 -1.8058612][-1.9442843 -1.8763726 -1.7978098 -1.712203 -1.5721257 -1.3987889 -1.0783819 -0.57035279 -0.022787094 0.18192005 -0.15460753 -0.8225311 -1.4262521 -1.6759957 -1.5523999][-2.0150626 -1.9165932 -1.664957 -1.3045775 -0.87782121 -0.4907223 -0.039853811 0.53306723 1.0684824 1.1975513 0.7548573 -0.027754068 -0.84008825 -1.3404425 -1.382677][-1.8134134 -1.6978409 -1.3721911 -0.91714633 -0.42182851 -0.012080669 0.42111754 0.9485755 1.3986611 1.4812863 1.0901196 0.34940624 -0.5250181 -1.1852901 -1.431838][-1.5679171 -1.4955512 -1.2382853 -0.88906074 -0.56321907 -0.31468248 -0.029654264 0.37486148 0.74270606 0.88991642 0.69881439 0.18293309 -0.52705348 -1.1389464 -1.4740455][-1.4493394 -1.4268105 -1.2968653 -1.1555554 -1.0649503 -1.0208626 -0.91760826 -0.67636383 -0.3837173 -0.15944052 -0.10754514 -0.29131365 -0.69476056 -1.093643 -1.339885][-1.4411371 -1.4726346 -1.4834454 -1.5306139 -1.6163909 -1.7164209 -1.7568718 -1.6632622 -1.4537375 -1.213281 -1.0182152 -0.95109892 -1.0747652 -1.2335082 -1.3299255][-1.5547578 -1.619149 -1.7045102 -1.836364 -1.9924284 -2.143209 -2.2434673 -2.2402265 -2.1167259 -1.9228338 -1.7241943 -1.5943917 -1.596113 -1.6487873 -1.6610394][-1.7843792 -1.8130544 -1.872713 -1.9587578 -2.0668864 -2.1693902 -2.2506189 -2.2920148 -2.2564423 -2.1632991 -2.0699155 -2.0191214 -2.049475 -2.0984418 -2.0896013][-1.963817 -1.9182796 -1.8859367 -1.8628967 -1.8622043 -1.8857461 -1.9431262 -2.0196469 -2.0767708 -2.11293 -2.1523981 -2.2196479 -2.3068612 -2.3723106 -2.3695481]]...]
INFO - root - 2017-12-16 09:18:56.047978: step 31410, loss = 0.50, batch loss = 0.24 (46.3 examples/sec; 0.173 sec/batch; 14h:26m:44s remains)
INFO - root - 2017-12-16 09:18:57.747508: step 31420, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 13h:51m:50s remains)
INFO - root - 2017-12-16 09:18:59.441760: step 31430, loss = 0.52, batch loss = 0.26 (45.2 examples/sec; 0.177 sec/batch; 14h:48m:00s remains)
INFO - root - 2017-12-16 09:19:01.120062: step 31440, loss = 0.62, batch loss = 0.36 (48.4 examples/sec; 0.165 sec/batch; 13h:48m:32s remains)
INFO - root - 2017-12-16 09:19:02.801812: step 31450, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:44m:45s remains)
INFO - root - 2017-12-16 09:19:04.484425: step 31460, loss = 0.63, batch loss = 0.37 (46.6 examples/sec; 0.172 sec/batch; 14h:22m:02s remains)
INFO - root - 2017-12-16 09:19:06.177057: step 31470, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 13h:55m:47s remains)
INFO - root - 2017-12-16 09:19:07.870593: step 31480, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 13h:46m:35s remains)
INFO - root - 2017-12-16 09:19:09.589171: step 31490, loss = 0.61, batch loss = 0.35 (46.1 examples/sec; 0.173 sec/batch; 14h:30m:14s remains)
INFO - root - 2017-12-16 09:19:11.261035: step 31500, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 13h:45m:49s remains)
2017-12-16 09:19:11.751248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6265056 -2.412652 -2.0789959 -1.6307728 -1.0871857 -0.64646924 -0.62980425 -1.3318629 -2.2923951 -3.0448623 -3.6216292 -4.0498915 -3.9203563 -3.2001386 -2.5571742][-2.9630485 -2.9217825 -2.7994916 -2.57999 -2.2598517 -1.9378357 -1.8213694 -2.2718666 -2.9363751 -3.3931942 -3.5997481 -3.6888502 -3.3781881 -2.5291748 -1.8587549][-3.2433836 -3.3587542 -3.4123163 -3.3739505 -3.1986742 -2.909884 -2.6531823 -2.8198404 -3.2501185 -3.4672933 -3.2931752 -2.9850085 -2.4786241 -1.5215483 -0.72765386][-3.443537 -3.5905797 -3.6825557 -3.6143951 -3.3790157 -3.0176206 -2.6393571 -2.6348333 -2.9472156 -3.1010773 -2.7319078 -2.1663194 -1.4927449 -0.534824 0.29986644][-3.4843814 -3.5176754 -3.3758953 -3.0570159 -2.5566406 -1.9640787 -1.3807986 -1.2069149 -1.6191013 -1.9613466 -1.7262578 -1.1426181 -0.49038625 0.18740249 0.89781022][-3.3114724 -3.0948346 -2.523818 -1.7568936 -0.84945858 0.067829609 1.0174577 1.6123669 0.98637724 0.041370392 -0.15033841 0.18003297 0.46366882 0.57655621 0.88279724][-3.0025997 -2.5395911 -1.5052516 -0.21089673 0.9788456 2.1733296 3.7376359 5.231204 4.3393736 2.3837841 1.5352976 1.4636352 1.1857977 0.60435462 0.3264823][-2.7448606 -2.1903839 -0.8900677 0.65309119 1.8221333 3.1130354 5.2622643 7.8826981 6.640419 3.7135193 2.1536567 1.6508629 0.97671509 -0.032148123 -0.66431916][-2.7804794 -2.329813 -1.1862112 0.15891814 1.0770345 2.0836313 3.9385865 5.9399195 5.2037 2.788255 1.172642 0.45540214 -0.29106045 -1.2429962 -1.8401203][-2.9952779 -2.7982435 -2.0780702 -1.2712008 -0.68662751 -0.084468126 1.1055584 2.2751968 2.0273826 0.52618432 -0.68424082 -1.2908951 -1.9687996 -2.5971737 -2.8905032][-3.1092503 -3.2145205 -2.9590743 -2.6813421 -2.4610486 -2.2318642 -1.716651 -1.1625892 -1.1134393 -1.7884609 -2.4246135 -2.8193762 -3.2930076 -3.6400518 -3.6866241][-3.0564389 -3.3548331 -3.431066 -3.505662 -3.5935907 -3.6754413 -3.683008 -3.5321865 -3.4086199 -3.5685134 -3.7908173 -3.945987 -4.1022334 -4.228466 -4.1582174][-2.7717321 -3.13708 -3.3941121 -3.6263208 -3.8669908 -4.1179814 -4.3536963 -4.3909712 -4.2934585 -4.2369952 -4.2479563 -4.2189341 -4.1502604 -4.1351876 -4.0338416][-2.4281077 -2.7150648 -2.983453 -3.2362525 -3.4701419 -3.7377954 -3.9847236 -4.0591512 -3.9771729 -3.8831224 -3.8211515 -3.7122414 -3.5490885 -3.46534 -3.3628426][-2.1563232 -2.3021455 -2.4595368 -2.6199574 -2.7601624 -2.9549978 -3.1317706 -3.1639793 -3.1065459 -3.02385 -2.9537072 -2.8606725 -2.716172 -2.6294045 -2.5595429]]...]
INFO - root - 2017-12-16 09:19:13.388128: step 31510, loss = 0.45, batch loss = 0.19 (47.8 examples/sec; 0.167 sec/batch; 13h:59m:10s remains)
INFO - root - 2017-12-16 09:19:15.072179: step 31520, loss = 0.67, batch loss = 0.41 (46.9 examples/sec; 0.171 sec/batch; 14h:15m:38s remains)
INFO - root - 2017-12-16 09:19:16.757974: step 31530, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 13h:44m:06s remains)
INFO - root - 2017-12-16 09:19:18.412053: step 31540, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 13h:52m:34s remains)
INFO - root - 2017-12-16 09:19:20.081941: step 31550, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:47m:25s remains)
INFO - root - 2017-12-16 09:19:21.743215: step 31560, loss = 0.53, batch loss = 0.27 (46.8 examples/sec; 0.171 sec/batch; 14h:18m:15s remains)
INFO - root - 2017-12-16 09:19:23.426396: step 31570, loss = 0.51, batch loss = 0.25 (48.5 examples/sec; 0.165 sec/batch; 13h:48m:05s remains)
INFO - root - 2017-12-16 09:19:25.118651: step 31580, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:49m:11s remains)
INFO - root - 2017-12-16 09:19:26.816481: step 31590, loss = 0.62, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 14h:06m:45s remains)
INFO - root - 2017-12-16 09:19:28.507033: step 31600, loss = 0.61, batch loss = 0.35 (48.0 examples/sec; 0.167 sec/batch; 13h:55m:46s remains)
2017-12-16 09:19:28.952198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0366023 -1.9407172 -1.8856859 -1.881952 -1.9708221 -2.1343002 -2.3372548 -2.6047182 -2.7992585 -2.8001935 -2.6820238 -2.5597959 -2.515059 -2.3905191 -2.2963738][-2.164907 -2.0070388 -1.9170799 -1.9191778 -2.0750685 -2.3015389 -2.4992869 -2.6926904 -2.8682308 -2.8932114 -2.7603524 -2.6017203 -2.49365 -2.3803701 -2.3061306][-2.3440645 -2.1495218 -2.0176284 -2.0084414 -2.1787994 -2.3607788 -2.4126954 -2.4527826 -2.593627 -2.6633663 -2.5806165 -2.4222655 -2.2893796 -2.15768 -2.1094184][-2.4365134 -2.2159088 -2.0176191 -1.9814916 -2.0712295 -2.077703 -1.8472093 -1.7124724 -1.8322823 -1.9966707 -2.0149798 -1.9104636 -1.8392441 -1.7341019 -1.7120795][-2.0618281 -1.8501782 -1.6534591 -1.5965127 -1.5724959 -1.3697175 -0.89661944 -0.61236644 -0.71633887 -0.96314 -1.1008179 -1.1039171 -1.1179432 -1.116618 -1.131618][-1.1569178 -0.97285676 -0.86762357 -0.84531605 -0.76308525 -0.38117814 0.23075795 0.5876739 0.41393113 0.045770884 -0.20047998 -0.31614614 -0.43420482 -0.56452894 -0.63957036][0.052939892 0.12248468 0.087175846 -0.0096628666 0.052664518 0.55836964 1.2280147 1.546124 1.2095156 0.69111586 0.37923336 0.24661517 0.085788488 -0.1147778 -0.32901859][1.0603805 0.95184374 0.77044487 0.56277204 0.55503368 1.0434849 1.7694404 2.0152481 1.4987373 0.97025013 0.73729348 0.66498303 0.53631926 0.302063 -0.15626097][1.1980641 1.0136707 0.77043867 0.51373887 0.47695351 0.8915329 1.4939971 1.5805829 1.2149689 0.93562961 0.89476657 0.91888523 0.8127954 0.44171286 -0.27755928][0.19236517 0.12103176 -0.0092949867 -0.16354942 -0.14984083 0.22063875 0.66657376 0.6818316 0.53398728 0.57971549 0.79909754 0.9315927 0.83485055 0.37707615 -0.42631328][-1.1811656 -1.1204278 -1.0895306 -1.0689844 -0.9181155 -0.54115164 -0.19939637 -0.12979794 -0.12879443 0.13171363 0.53309369 0.75948858 0.73962688 0.29157424 -0.40332592][-2.266361 -2.1457543 -1.9747239 -1.7857506 -1.498224 -1.1070929 -0.808609 -0.69560826 -0.56517708 -0.20530224 0.22101426 0.4530189 0.46025205 0.12486172 -0.37619233][-2.9343424 -2.8253384 -2.5959444 -2.2899656 -1.9272557 -1.5623908 -1.2735916 -1.1048304 -0.90956175 -0.59100389 -0.27809334 -0.14998436 -0.20466042 -0.39729369 -0.62948859][-3.0831959 -3.0712314 -2.900275 -2.6330159 -2.3111455 -1.9661926 -1.6275265 -1.3870718 -1.2572602 -1.1184801 -0.97540975 -0.97194111 -1.0756232 -1.1526449 -1.1482797][-2.652312 -2.7998219 -2.781781 -2.6457903 -2.4314876 -2.115947 -1.7455361 -1.4672579 -1.3694605 -1.3360828 -1.3389047 -1.4686134 -1.6258683 -1.6323572 -1.539789]]...]
INFO - root - 2017-12-16 09:19:30.613868: step 31610, loss = 0.47, batch loss = 0.21 (47.5 examples/sec; 0.168 sec/batch; 14h:04m:52s remains)
INFO - root - 2017-12-16 09:19:32.295646: step 31620, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 13h:53m:18s remains)
INFO - root - 2017-12-16 09:19:33.979085: step 31630, loss = 0.58, batch loss = 0.32 (46.1 examples/sec; 0.173 sec/batch; 14h:29m:30s remains)
INFO - root - 2017-12-16 09:19:35.664232: step 31640, loss = 0.51, batch loss = 0.25 (46.6 examples/sec; 0.172 sec/batch; 14h:20m:59s remains)
INFO - root - 2017-12-16 09:19:37.337424: step 31650, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 14h:12m:08s remains)
INFO - root - 2017-12-16 09:19:39.040231: step 31660, loss = 0.67, batch loss = 0.41 (48.7 examples/sec; 0.164 sec/batch; 13h:43m:15s remains)
INFO - root - 2017-12-16 09:19:40.703821: step 31670, loss = 0.57, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 13h:37m:58s remains)
INFO - root - 2017-12-16 09:19:42.374689: step 31680, loss = 0.60, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 13h:50m:18s remains)
INFO - root - 2017-12-16 09:19:44.045921: step 31690, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:52m:00s remains)
INFO - root - 2017-12-16 09:19:45.727618: step 31700, loss = 0.50, batch loss = 0.24 (44.4 examples/sec; 0.180 sec/batch; 15h:02m:49s remains)
2017-12-16 09:19:46.244056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1745591 -3.3604722 -2.8910251 -1.6167779 -0.43031037 -0.28795075 -0.73063672 -0.98992395 -1.2671067 -2.1140218 -3.1631241 -3.7728453 -3.6215644 -2.899291 -2.1481423][-3.5726283 -3.9348345 -3.6844738 -2.7390022 -1.8701499 -1.7653146 -1.955186 -1.8848045 -1.9190059 -2.5772371 -3.42877 -3.8802402 -3.6774416 -2.9923058 -2.2595139][-4.0752344 -4.469141 -4.354733 -3.6973143 -3.0619025 -2.8791294 -2.7153535 -2.2936907 -2.1682258 -2.7279689 -3.3937597 -3.6428809 -3.3983393 -2.8222709 -2.194803][-4.5279055 -4.9301119 -4.8732119 -4.3556762 -3.7373862 -3.1992984 -2.5431395 -1.8857446 -1.880806 -2.5765073 -3.1108706 -3.1076517 -2.8529546 -2.4776556 -2.0603533][-4.5969043 -4.98272 -4.9557943 -4.506175 -3.6714473 -2.4884076 -1.1725948 -0.3650825 -0.69284213 -1.757141 -2.4470656 -2.4521732 -2.2987819 -2.1620688 -1.9451489][-4.1815729 -4.5485229 -4.3948689 -3.8094525 -2.6011188 -0.49346519 1.7612052 2.6627703 1.5093482 -0.50693977 -1.8431749 -2.1766133 -2.1470258 -2.0897357 -1.9020159][-3.7976594 -4.0423756 -3.6127639 -2.6903987 -1.0439453 1.9572606 5.293467 6.290453 3.8731823 0.4306705 -1.8247139 -2.5536673 -2.5562749 -2.3338199 -1.9770341][-3.6709957 -3.8187585 -3.1440072 -2.0040908 -0.16560864 3.3356781 7.5213146 8.4649611 4.9508066 0.43213463 -2.4273608 -3.3712184 -3.2766807 -2.7333586 -2.0785315][-3.5090084 -3.727073 -2.9802198 -1.8529582 -0.31565189 2.6048265 6.1097364 7.0469284 3.8815975 -0.55395806 -3.3981981 -4.2377872 -3.8924251 -3.0142345 -2.0782638][-3.3833275 -3.7287297 -3.0374048 -2.0320368 -0.925359 1.0848532 3.7029753 4.5675311 2.0959086 -1.6608367 -4.1509008 -4.7882204 -4.2197809 -3.1026065 -1.9887614][-3.4408829 -3.9052093 -3.3286052 -2.5098486 -1.7845371 -0.49035573 1.3859856 2.1714091 0.47695327 -2.3550673 -4.3117056 -4.7671471 -4.1332111 -2.9790893 -1.8501019][-3.4666138 -4.0311637 -3.6320829 -3.0589583 -2.6487858 -1.840941 -0.40602171 0.38583803 -0.65027 -2.6545014 -4.1367645 -4.4554553 -3.8628075 -2.7965071 -1.7632892][-3.5717435 -4.1134434 -3.8226309 -3.4304438 -3.1655526 -2.5369537 -1.3472327 -0.60933793 -1.2380606 -2.6785662 -3.7791128 -4.0152636 -3.4872668 -2.5526061 -1.6787364][-3.712007 -4.0977149 -3.8156414 -3.4368911 -3.1426454 -2.5214982 -1.5004299 -0.88083792 -1.3424 -2.4445872 -3.2976985 -3.4915235 -3.0253851 -2.2271903 -1.5236032][-3.5800343 -3.7510827 -3.43539 -3.0276728 -2.6500015 -2.0201519 -1.1540171 -0.67336988 -1.1041696 -2.055763 -2.8018556 -2.9838533 -2.5690923 -1.8645581 -1.317943]]...]
INFO - root - 2017-12-16 09:19:47.948788: step 31710, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 14h:28m:18s remains)
INFO - root - 2017-12-16 09:19:49.672763: step 31720, loss = 0.57, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 14h:04m:35s remains)
INFO - root - 2017-12-16 09:19:51.388711: step 31730, loss = 0.59, batch loss = 0.33 (47.9 examples/sec; 0.167 sec/batch; 13h:57m:40s remains)
INFO - root - 2017-12-16 09:19:53.096904: step 31740, loss = 0.65, batch loss = 0.39 (49.5 examples/sec; 0.161 sec/batch; 13h:29m:25s remains)
INFO - root - 2017-12-16 09:19:54.784581: step 31750, loss = 0.53, batch loss = 0.27 (46.7 examples/sec; 0.171 sec/batch; 14h:18m:59s remains)
INFO - root - 2017-12-16 09:19:56.416055: step 31760, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:53m:53s remains)
INFO - root - 2017-12-16 09:19:58.089384: step 31770, loss = 0.64, batch loss = 0.38 (47.4 examples/sec; 0.169 sec/batch; 14h:05m:21s remains)
INFO - root - 2017-12-16 09:19:59.743892: step 31780, loss = 0.59, batch loss = 0.33 (49.0 examples/sec; 0.163 sec/batch; 13h:38m:35s remains)
INFO - root - 2017-12-16 09:20:01.399874: step 31790, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 14h:17m:35s remains)
INFO - root - 2017-12-16 09:20:03.081663: step 31800, loss = 0.56, batch loss = 0.30 (49.6 examples/sec; 0.161 sec/batch; 13h:28m:39s remains)
2017-12-16 09:20:03.588131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0204175 -2.5893078 -2.0033898 -1.5923247 -1.5737872 -1.739193 -1.8387973 -1.8815632 -1.9684992 -2.1162429 -2.3386128 -2.5124178 -2.5651679 -2.4051373 -2.0470903][-3.0581355 -2.6330595 -2.0438468 -1.635005 -1.5947615 -1.73612 -1.8338357 -1.8106159 -1.8184446 -1.8614244 -2.0095713 -2.16064 -2.2644486 -2.2122982 -1.9555664][-2.7329228 -2.3044839 -1.6886166 -1.2387016 -1.1495517 -1.3030397 -1.4514549 -1.454479 -1.4214242 -1.4008279 -1.4589915 -1.57913 -1.733319 -1.7336537 -1.5376803][-2.2724974 -1.7644889 -1.0223442 -0.46525013 -0.35272336 -0.52218843 -0.694592 -0.7202127 -0.73903787 -0.80991375 -0.948689 -1.1160779 -1.242586 -1.2149388 -0.99399149][-1.6807238 -1.0842513 -0.15770531 0.54879284 0.77697778 0.65838861 0.55913162 0.55500317 0.47167325 0.20883369 -0.19406819 -0.48428571 -0.62134922 -0.60407472 -0.36797285][-1.2151279 -0.52984369 0.56508327 1.4214628 1.8600869 1.9916778 2.0818233 2.1491628 2.0144935 1.5582068 0.91321516 0.48802233 0.2773118 0.15714788 0.24666238][-1.2591658 -0.59664249 0.51984072 1.4383714 2.0600762 2.4810104 2.8300576 3.1046515 3.0704021 2.6489568 1.9174228 1.4033926 1.043184 0.72223973 0.63447595][-1.4945636 -1.0409367 -0.11696362 0.76813817 1.4092758 1.9126067 2.4154239 2.8637033 3.0034852 2.8096228 2.2818794 1.825254 1.4082391 0.9899137 0.83637929][-1.7772712 -1.5503633 -0.87132227 -0.2043581 0.19940948 0.57528591 1.1283355 1.5821528 1.7800322 1.8131557 1.7062116 1.5661469 1.2395945 0.85293984 0.76882577][-1.8721502 -1.8085272 -1.3946031 -1.0271714 -0.94741964 -0.84636962 -0.46473908 -0.085493326 0.077526093 0.18991303 0.40661979 0.62635326 0.49864864 0.29848075 0.39486861][-2.1574233 -2.0598319 -1.7305839 -1.5742779 -1.7791787 -1.9652865 -1.8108239 -1.5908556 -1.5160158 -1.3956618 -1.0856934 -0.75136828 -0.69155371 -0.63656914 -0.27388][-2.3746824 -2.1590726 -1.7832626 -1.7078178 -2.0797079 -2.4588819 -2.4921548 -2.4252203 -2.401299 -2.3284712 -2.0931919 -1.8111594 -1.6830099 -1.4582803 -0.90284741][-2.4057896 -2.0550418 -1.5441134 -1.3874476 -1.7666214 -2.2173054 -2.3665202 -2.3780887 -2.4100003 -2.411236 -2.3270206 -2.2074931 -2.1130788 -1.8420887 -1.2323307][-2.3265088 -1.869723 -1.1842022 -0.80430496 -1.0330548 -1.4349444 -1.6086885 -1.6633155 -1.7404128 -1.8170207 -1.8345428 -1.8178797 -1.7837019 -1.5717945 -1.0657597][-2.4528933 -2.118722 -1.4359174 -0.90563142 -0.94011283 -1.2365171 -1.3898095 -1.4403343 -1.5024035 -1.5844095 -1.6341658 -1.6622071 -1.6749833 -1.546962 -1.2020127]]...]
INFO - root - 2017-12-16 09:20:05.299842: step 31810, loss = 0.54, batch loss = 0.29 (44.7 examples/sec; 0.179 sec/batch; 14h:56m:50s remains)
INFO - root - 2017-12-16 09:20:06.979782: step 31820, loss = 0.61, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 14h:08m:08s remains)
INFO - root - 2017-12-16 09:20:08.691347: step 31830, loss = 0.57, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 14h:19m:32s remains)
INFO - root - 2017-12-16 09:20:10.391044: step 31840, loss = 0.51, batch loss = 0.25 (46.7 examples/sec; 0.171 sec/batch; 14h:18m:19s remains)
INFO - root - 2017-12-16 09:20:12.086427: step 31850, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.168 sec/batch; 13h:59m:21s remains)
INFO - root - 2017-12-16 09:20:13.758781: step 31860, loss = 0.65, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 13h:42m:27s remains)
INFO - root - 2017-12-16 09:20:15.445199: step 31870, loss = 0.67, batch loss = 0.41 (45.4 examples/sec; 0.176 sec/batch; 14h:42m:02s remains)
INFO - root - 2017-12-16 09:20:17.100219: step 31880, loss = 0.50, batch loss = 0.24 (48.1 examples/sec; 0.166 sec/batch; 13h:53m:45s remains)
INFO - root - 2017-12-16 09:20:18.765781: step 31890, loss = 0.52, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:00m:20s remains)
INFO - root - 2017-12-16 09:20:20.465150: step 31900, loss = 0.47, batch loss = 0.21 (48.7 examples/sec; 0.164 sec/batch; 13h:42m:52s remains)
2017-12-16 09:20:20.987504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2999134 -3.2914252 -3.23175 -3.0257709 -2.6042991 -2.0657749 -1.568913 -1.0885122 -0.63149393 -0.32847118 -0.25959921 -0.38718724 -0.84501815 -1.5783737 -2.3372173][-3.442215 -3.5596995 -3.5366528 -3.2827492 -2.6753225 -1.870468 -1.1954933 -0.710418 -0.43862319 -0.44624853 -0.60665739 -0.72052193 -0.95377493 -1.4995255 -2.1825931][-3.4721603 -3.6612849 -3.6745329 -3.4322431 -2.714221 -1.6800237 -0.78431165 -0.2202189 -0.080941677 -0.44183588 -0.95847952 -1.1937608 -1.2046136 -1.4794295 -2.0284932][-3.4401865 -3.6613712 -3.7122574 -3.4999938 -2.7586672 -1.549762 -0.35133505 0.40135765 0.49309349 -0.18803525 -1.0873054 -1.5184371 -1.4039128 -1.4437239 -1.8550117][-3.3801086 -3.5540791 -3.6243842 -3.4502788 -2.6632872 -1.2445569 0.33778691 1.4485805 1.5069916 0.45983434 -0.82557082 -1.504283 -1.4449799 -1.3758621 -1.6753867][-3.3509278 -3.4393055 -3.4788439 -3.2913845 -2.4034438 -0.7075659 1.2093956 2.5970232 2.6219628 1.2684095 -0.30012417 -1.2036604 -1.3376954 -1.2689824 -1.4911948][-3.3383713 -3.3502595 -3.3310773 -3.0227933 -1.9465286 0.015130281 2.0772407 3.4013836 3.2284491 1.7131026 0.10430455 -0.88861227 -1.1681604 -1.101101 -1.2758025][-3.3505969 -3.3299007 -3.1928723 -2.6693172 -1.4149864 0.6569891 2.6640723 3.7297328 3.1397016 1.5470541 0.11323524 -0.80291033 -1.0519761 -0.92260003 -1.0412505][-3.4022479 -3.3735857 -3.0945973 -2.3945007 -1.1085234 0.82212806 2.5705302 3.2557881 2.3466113 0.84624863 -0.29949856 -0.96184361 -1.053103 -0.83166206 -0.880257][-3.4495947 -3.4637122 -3.0912344 -2.3200123 -1.2108762 0.34956527 1.7011921 2.0330184 1.0020642 -0.29855537 -1.0066149 -1.3364897 -1.2586464 -0.98184288 -0.93029284][-3.4725904 -3.5474863 -3.2058887 -2.5151913 -1.6421986 -0.487028 0.49116731 0.54723287 -0.42186224 -1.4351465 -1.8646122 -1.9755223 -1.7891631 -1.4601333 -1.259737][-3.452219 -3.6098475 -3.4028378 -2.9007416 -2.3276722 -1.5533464 -0.92624414 -0.93346822 -1.75843 -2.5289695 -2.8371987 -2.8434885 -2.5713019 -2.1412082 -1.7564478][-3.3941727 -3.6389318 -3.6006885 -3.3215132 -3.0206068 -2.6017816 -2.2342381 -2.2418416 -2.8525653 -3.476397 -3.7451117 -3.6955717 -3.3221927 -2.7486167 -2.2139747][-3.3094 -3.611042 -3.7277455 -3.6486096 -3.5409505 -3.3693621 -3.1922982 -3.1857271 -3.5847545 -4.0591226 -4.2605562 -4.1477733 -3.6949697 -3.0549092 -2.4919152][-3.219466 -3.4973457 -3.7120609 -3.7772112 -3.7887728 -3.7897706 -3.7617121 -3.7381628 -3.9470911 -4.1959748 -4.2461662 -4.0211878 -3.5682938 -3.0318851 -2.6051443]]...]
INFO - root - 2017-12-16 09:20:22.660890: step 31910, loss = 0.56, batch loss = 0.30 (44.8 examples/sec; 0.179 sec/batch; 14h:54m:28s remains)
INFO - root - 2017-12-16 09:20:24.361949: step 31920, loss = 0.64, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 13h:54m:23s remains)
INFO - root - 2017-12-16 09:20:26.059988: step 31930, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 13h:58m:07s remains)
INFO - root - 2017-12-16 09:20:27.703651: step 31940, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 14h:00m:27s remains)
INFO - root - 2017-12-16 09:20:29.373077: step 31950, loss = 0.60, batch loss = 0.34 (48.1 examples/sec; 0.166 sec/batch; 13h:52m:37s remains)
INFO - root - 2017-12-16 09:20:31.020952: step 31960, loss = 0.48, batch loss = 0.22 (46.4 examples/sec; 0.172 sec/batch; 14h:23m:42s remains)
INFO - root - 2017-12-16 09:20:32.703142: step 31970, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 13h:45m:07s remains)
INFO - root - 2017-12-16 09:20:34.365435: step 31980, loss = 0.53, batch loss = 0.28 (47.8 examples/sec; 0.168 sec/batch; 13h:58m:57s remains)
INFO - root - 2017-12-16 09:20:36.044317: step 31990, loss = 0.60, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 13h:55m:23s remains)
INFO - root - 2017-12-16 09:20:37.733906: step 32000, loss = 0.57, batch loss = 0.31 (45.5 examples/sec; 0.176 sec/batch; 14h:39m:59s remains)
2017-12-16 09:20:38.224287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2500608 -1.3696846 -1.4173994 -1.3255122 -1.2313479 -1.2286295 -1.2733953 -1.2577795 -1.2035506 -1.262868 -1.4785521 -1.6905003 -1.7791641 -1.7229836 -1.5865996][-1.2711232 -1.4082386 -1.4592148 -1.3620782 -1.2216636 -1.1173476 -1.0162839 -0.84566247 -0.713423 -0.86295474 -1.3379873 -1.8334913 -2.113024 -2.1215544 -1.92409][-1.26745 -1.3879195 -1.4171126 -1.2948983 -1.0948635 -0.8842684 -0.6281606 -0.31867027 -0.16245651 -0.45843554 -1.1891156 -1.9613811 -2.4151 -2.4912488 -2.257678][-1.2614629 -1.3530364 -1.3665737 -1.2273093 -0.97744811 -0.64457536 -0.24240565 0.12294865 0.21627045 -0.24700427 -1.2084827 -2.1941922 -2.7696059 -2.8870807 -2.6309574][-1.2677917 -1.3581654 -1.3986762 -1.3073275 -1.0361515 -0.57095909 0.012334585 0.5083859 0.62409329 0.10003281 -0.99575067 -2.129817 -2.8701389 -3.12886 -2.9333215][-1.2646613 -1.3946465 -1.4971143 -1.4742112 -1.2022505 -0.61450136 0.17527485 0.93992448 1.2577374 0.85171008 -0.26770067 -1.5550869 -2.5153322 -2.9829264 -2.9334514][-1.22577 -1.3785113 -1.5137777 -1.5278547 -1.2711205 -0.5792675 0.51151252 1.6242177 2.235801 1.974858 0.87056828 -0.55919993 -1.7971766 -2.5576692 -2.7295556][-1.1831183 -1.3536831 -1.4884244 -1.4903405 -1.17754 -0.33705258 1.0250144 2.3997104 3.1982882 2.983237 1.8747585 0.35101295 -1.107947 -2.1206632 -2.5081174][-1.1864244 -1.4072645 -1.5207388 -1.4422703 -1.0471911 -0.15368748 1.2585256 2.7073047 3.5853179 3.4331338 2.356981 0.8164444 -0.72080362 -1.8559613 -2.3950069][-1.2035472 -1.4560862 -1.5728004 -1.4413443 -1.0545586 -0.29345846 0.889658 2.1939142 3.0062416 2.8293674 1.7752793 0.31885386 -1.0842385 -2.071301 -2.5080481][-1.1967309 -1.4458451 -1.5961695 -1.5035129 -1.1677532 -0.58397329 0.27773404 1.247113 1.8274667 1.5123365 0.52036047 -0.732918 -1.8085895 -2.4332595 -2.5833356][-1.1621896 -1.3901045 -1.5703251 -1.560766 -1.3192714 -0.91675973 -0.38416088 0.19734097 0.5164957 0.2019701 -0.61810148 -1.5132236 -2.1862783 -2.4380739 -2.3147292][-1.1077521 -1.2836219 -1.4684093 -1.5491505 -1.469855 -1.2914429 -1.0670607 -0.82344341 -0.66962731 -0.83370674 -1.3154087 -1.7898493 -2.0755394 -2.0664952 -1.8135459][-1.0906513 -1.184674 -1.3094397 -1.4193314 -1.4669609 -1.4944041 -1.5118465 -1.4714041 -1.4041831 -1.4382145 -1.6138568 -1.7771235 -1.8077036 -1.6801865 -1.4002421][-1.1258748 -1.1528605 -1.1928216 -1.2407112 -1.3047224 -1.4059079 -1.5191898 -1.5829289 -1.5894301 -1.5885026 -1.6271193 -1.6442585 -1.5681543 -1.396481 -1.1670127]]...]
INFO - root - 2017-12-16 09:20:39.969658: step 32010, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 13h:46m:33s remains)
INFO - root - 2017-12-16 09:20:41.611041: step 32020, loss = 0.54, batch loss = 0.28 (50.0 examples/sec; 0.160 sec/batch; 13h:20m:55s remains)
INFO - root - 2017-12-16 09:20:43.283830: step 32030, loss = 0.56, batch loss = 0.30 (47.5 examples/sec; 0.168 sec/batch; 14h:02m:53s remains)
INFO - root - 2017-12-16 09:20:44.977794: step 32040, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 14h:12m:41s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:20:46.673426: step 32050, loss = 0.51, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 14h:16m:25s remains)
INFO - root - 2017-12-16 09:20:48.372176: step 32060, loss = 0.56, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 14h:01m:18s remains)
INFO - root - 2017-12-16 09:20:50.058109: step 32070, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:58m:14s remains)
INFO - root - 2017-12-16 09:20:51.735320: step 32080, loss = 0.68, batch loss = 0.42 (48.0 examples/sec; 0.167 sec/batch; 13h:54m:26s remains)
INFO - root - 2017-12-16 09:20:53.415789: step 32090, loss = 0.55, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 14h:22m:00s remains)
INFO - root - 2017-12-16 09:20:55.053873: step 32100, loss = 0.48, batch loss = 0.22 (50.7 examples/sec; 0.158 sec/batch; 13h:10m:40s remains)
2017-12-16 09:20:55.518731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3659575 -3.6161118 -3.9149559 -4.0194664 -3.7604508 -3.0792506 -2.2790005 -1.8825854 -2.1804037 -2.9464817 -3.6709437 -4.1562753 -4.3200831 -4.0048223 -3.3480835][-3.881659 -4.0216436 -4.1763234 -4.1180925 -3.5872431 -2.5255063 -1.2341391 -0.43945551 -0.72525263 -1.8477094 -3.0234392 -3.803123 -3.9992361 -3.635911 -2.8951626][-4.283834 -4.3311491 -4.3296714 -4.1043758 -3.3886132 -2.1590285 -0.70112276 0.29118729 0.0891695 -1.0923915 -2.4431927 -3.399359 -3.7080712 -3.3988538 -2.6959808][-4.4767566 -4.4612732 -4.3454103 -3.9468021 -3.1314478 -1.937107 -0.61043549 0.24497867 -0.048714876 -1.2052194 -2.4693694 -3.3002095 -3.5033803 -3.231602 -2.6742721][-4.3308096 -4.2584233 -4.0151329 -3.4512484 -2.4670467 -1.2095151 -0.021622181 0.46222973 -0.21248341 -1.5571198 -2.7093267 -3.2600815 -3.2637377 -3.0359929 -2.6873109][-3.8470864 -3.7099862 -3.3986359 -2.7143757 -1.4762241 0.095192432 1.3763142 1.4477627 0.1046207 -1.7149353 -2.9752047 -3.326818 -3.080863 -2.7607286 -2.4907897][-3.2985516 -3.1112833 -2.7727661 -1.9912062 -0.53708804 1.3988039 2.9665427 2.8346014 0.70080638 -1.7222481 -3.1705809 -3.3725803 -2.866528 -2.3393323 -1.9839977][-3.0242825 -2.8736229 -2.5758152 -1.8483067 -0.3398838 1.7964015 3.5761766 3.4805989 1.0906115 -1.5650132 -3.1079881 -3.2555103 -2.6232383 -1.9573287 -1.5026929][-3.1238732 -3.0273089 -2.8564947 -2.3369765 -1.0738679 0.78321385 2.3237019 2.3247118 0.5006392 -1.6493831 -2.9608331 -3.0903785 -2.4584572 -1.7060249 -1.1382838][-3.2636006 -3.2735915 -3.3282769 -3.0983903 -2.2466843 -0.89844823 0.28020024 0.45865345 -0.6168102 -1.9977421 -2.8700125 -2.9366245 -2.4079394 -1.7085662 -1.1429836][-3.2745981 -3.3511488 -3.5943689 -3.6553922 -3.2172728 -2.3676462 -1.5225433 -1.1401325 -1.5362812 -2.2208343 -2.6913362 -2.7162776 -2.37362 -1.852214 -1.3805153][-3.1733966 -3.2030535 -3.4908938 -3.7371895 -3.6397305 -3.2273769 -2.6655107 -2.1611955 -2.0322292 -2.1832724 -2.4066215 -2.4684374 -2.3267434 -1.9580163 -1.5234197][-3.1848223 -3.0359263 -3.190263 -3.4056268 -3.4576149 -3.313802 -2.977752 -2.4920862 -2.1108332 -1.9675424 -2.0668962 -2.2283783 -2.2502058 -1.9537673 -1.4525225][-3.234436 -2.8914866 -2.8520887 -2.9066284 -2.9243453 -2.909977 -2.7614522 -2.4410243 -2.0669529 -1.8513746 -1.9492291 -2.1992412 -2.3164675 -2.0512152 -1.4580585][-3.3159165 -2.8624623 -2.6209042 -2.4779344 -2.3912725 -2.4109061 -2.4385133 -2.3693721 -2.1614542 -2.009279 -2.1206191 -2.3924098 -2.5578475 -2.3393378 -1.7152]]...]
INFO - root - 2017-12-16 09:20:57.199112: step 32110, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 14h:05m:07s remains)
INFO - root - 2017-12-16 09:20:58.882274: step 32120, loss = 0.53, batch loss = 0.27 (45.1 examples/sec; 0.177 sec/batch; 14h:48m:10s remains)
INFO - root - 2017-12-16 09:21:00.539254: step 32130, loss = 0.60, batch loss = 0.34 (48.8 examples/sec; 0.164 sec/batch; 13h:40m:23s remains)
INFO - root - 2017-12-16 09:21:02.216356: step 32140, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:50m:53s remains)
INFO - root - 2017-12-16 09:21:03.880061: step 32150, loss = 0.49, batch loss = 0.23 (48.3 examples/sec; 0.165 sec/batch; 13h:48m:18s remains)
INFO - root - 2017-12-16 09:21:05.570711: step 32160, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 13h:55m:33s remains)
INFO - root - 2017-12-16 09:21:07.244798: step 32170, loss = 0.58, batch loss = 0.32 (47.4 examples/sec; 0.169 sec/batch; 14h:03m:56s remains)
INFO - root - 2017-12-16 09:21:08.928518: step 32180, loss = 0.49, batch loss = 0.23 (48.4 examples/sec; 0.165 sec/batch; 13h:48m:03s remains)
INFO - root - 2017-12-16 09:21:10.593603: step 32190, loss = 0.50, batch loss = 0.25 (45.2 examples/sec; 0.177 sec/batch; 14h:46m:47s remains)
INFO - root - 2017-12-16 09:21:12.235157: step 32200, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 13h:52m:18s remains)
2017-12-16 09:21:12.737400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8853614 -2.0324264 -2.146399 -2.2396207 -2.3667753 -2.5688241 -2.7940593 -2.8944077 -2.9016879 -2.8414826 -2.7293649 -2.5810645 -2.3930867 -2.1976118 -2.0138288][-2.4106786 -2.6353779 -2.8363774 -2.9761913 -3.1233816 -3.3457417 -3.6300554 -3.7390618 -3.7088056 -3.645505 -3.5616927 -3.3753884 -3.0720775 -2.74357 -2.4480629][-3.115778 -3.4045625 -3.6280198 -3.699059 -3.6825948 -3.8566883 -4.1109238 -4.1952653 -4.1776562 -4.2693119 -4.3876858 -4.2895331 -3.9305153 -3.476974 -3.0603271][-3.7433326 -3.9759462 -4.0514822 -3.783803 -3.3773961 -3.3046627 -3.4475965 -3.4663413 -3.5346918 -3.9820178 -4.4862652 -4.6511922 -4.4765573 -4.1290894 -3.63876][-4.1737275 -4.2130284 -3.8516269 -2.9780953 -1.9757111 -1.4871311 -1.3006781 -1.1502659 -1.3858037 -2.3881764 -3.4510458 -4.0510077 -4.3026714 -4.2922249 -3.8853126][-4.197937 -4.0161529 -3.1389973 -1.5873655 0.042231321 1.121506 1.8706326 2.3947797 1.9315748 0.20084906 -1.4557054 -2.5090926 -3.2837076 -3.8061945 -3.6801834][-3.860132 -3.4968724 -2.2817519 -0.22808385 1.884727 3.5061431 5.105401 6.3139482 5.3679333 2.9278846 0.9079144 -0.48463416 -1.788307 -2.8807073 -3.1238441][-3.4992287 -3.1118455 -1.749136 0.4144454 2.6560359 4.5485835 6.8414817 9.1271286 7.3329654 4.5132594 2.5204458 1.0071788 -0.65754807 -2.0687082 -2.6294448][-3.5087376 -3.2841876 -2.0933483 -0.22072315 1.5863798 3.1717157 4.9425769 6.3418994 5.6601911 3.7143788 2.2876382 1.1143653 -0.48577726 -1.9387653 -2.5652149][-3.7177825 -3.7423596 -3.0293586 -1.8459141 -0.64163995 0.46105814 1.533298 2.316752 2.2044806 1.312881 0.61835074 -0.05476284 -1.236461 -2.3914077 -2.8518608][-3.9315758 -4.2074966 -4.014185 -3.4732852 -2.8242428 -2.2327094 -1.6848965 -1.2507825 -1.1026671 -1.3176926 -1.4812808 -1.745959 -2.4764364 -3.1247427 -3.2912266][-3.9014928 -4.328804 -4.48516 -4.389329 -4.1269331 -3.8778636 -3.6836433 -3.4786897 -3.2176309 -3.0945096 -3.0451977 -3.0986066 -3.4045911 -3.6300211 -3.5707464][-3.5062587 -3.947751 -4.2361083 -4.3989739 -4.3871269 -4.342176 -4.3720207 -4.3052707 -4.0646906 -3.8527784 -3.7431855 -3.6964054 -3.7416468 -3.7160883 -3.5079391][-2.8788211 -3.2231205 -3.5089121 -3.7064862 -3.7930372 -3.8718491 -3.9524627 -3.9509659 -3.8199306 -3.6431861 -3.5344319 -3.4633694 -3.4137769 -3.3003073 -3.0841024][-2.2802832 -2.4824502 -2.6556296 -2.7764764 -2.8572352 -2.9399953 -3.0049179 -3.0156543 -2.9553077 -2.8755593 -2.8095264 -2.7610121 -2.7126822 -2.6300426 -2.4970977]]...]
INFO - root - 2017-12-16 09:21:14.376858: step 32210, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 13h:47m:08s remains)
INFO - root - 2017-12-16 09:21:16.049375: step 32220, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 14h:06m:29s remains)
INFO - root - 2017-12-16 09:21:17.714225: step 32230, loss = 0.47, batch loss = 0.21 (49.9 examples/sec; 0.160 sec/batch; 13h:22m:52s remains)
INFO - root - 2017-12-16 09:21:19.377280: step 32240, loss = 0.71, batch loss = 0.45 (45.6 examples/sec; 0.175 sec/batch; 14h:38m:10s remains)
INFO - root - 2017-12-16 09:21:21.073251: step 32250, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:55m:48s remains)
INFO - root - 2017-12-16 09:21:22.748480: step 32260, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:50m:17s remains)
INFO - root - 2017-12-16 09:21:24.415507: step 32270, loss = 0.48, batch loss = 0.22 (49.5 examples/sec; 0.162 sec/batch; 13h:29m:08s remains)
INFO - root - 2017-12-16 09:21:26.049007: step 32280, loss = 0.58, batch loss = 0.32 (49.6 examples/sec; 0.161 sec/batch; 13h:27m:29s remains)
INFO - root - 2017-12-16 09:21:27.685708: step 32290, loss = 0.65, batch loss = 0.39 (48.5 examples/sec; 0.165 sec/batch; 13h:44m:45s remains)
INFO - root - 2017-12-16 09:21:29.382858: step 32300, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 14h:05m:56s remains)
2017-12-16 09:21:29.881724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.053268 -0.98836696 -0.94332993 -0.91639459 -0.89225245 -0.86900079 -0.87403989 -0.91922247 -0.98323131 -1.0481979 -1.1013139 -1.1347327 -1.16892 -1.2327662 -1.3281608][-0.71755517 -0.69546688 -0.70909357 -0.73038685 -0.719295 -0.68178153 -0.66762269 -0.72435057 -0.82871389 -0.93136823 -1.0027469 -1.0265108 -1.0386139 -1.1043125 -1.2305698][-0.43713522 -0.49713337 -0.59856641 -0.67163265 -0.64759994 -0.541973 -0.44559145 -0.47082341 -0.62380052 -0.81738341 -0.96720791 -1.0124896 -0.99708366 -1.0401434 -1.1699604][-0.24098921 -0.36193323 -0.532295 -0.61441886 -0.5003103 -0.21968675 0.067617893 0.14033484 -0.099293709 -0.49429715 -0.83725834 -1.0148066 -1.0398573 -1.0810199 -1.2036591][0.038250208 -0.086461306 -0.28176737 -0.33013844 -0.086153507 0.45509839 1.0507848 1.2873142 0.9224577 0.20386195 -0.47678912 -0.90060925 -1.0703921 -1.1712328 -1.3037109][0.66835022 0.59541321 0.37956429 0.328264 0.662673 1.4329948 2.3695183 2.8337064 2.336585 1.2651451 0.18006277 -0.56948328 -0.96898568 -1.2068932 -1.3939669][1.4167781 1.4092736 1.175422 1.092603 1.4643207 2.4033852 3.6290712 4.3603134 3.7363434 2.3678727 0.96540117 -0.0818944 -0.71313047 -1.1126181 -1.3880277][1.9445319 1.9521298 1.6370454 1.4805136 1.8368893 2.8019118 4.1060238 4.9760056 4.38019 2.9678593 1.5016253 0.36078334 -0.41297185 -0.93728435 -1.3032631][1.8240652 1.7920103 1.3678772 1.1089203 1.352509 2.1437907 3.185246 3.8831806 3.5843182 2.5724087 1.4330597 0.47358823 -0.27872348 -0.86070609 -1.2759231][0.91217136 0.80010176 0.31858277 0.0057184696 0.13078618 0.68247485 1.3732913 1.8627362 1.8298025 1.3314543 0.66912365 0.052758932 -0.51489115 -1.0323907 -1.4080074][-0.37400889 -0.59815192 -1.0948052 -1.4245751 -1.3886352 -1.0605083 -0.66842365 -0.35538435 -0.24851203 -0.37129927 -0.59166336 -0.82324815 -1.0867628 -1.4042139 -1.652899][-1.5578849 -1.8663571 -2.3324678 -2.6524229 -2.6839375 -2.525063 -2.3594606 -2.1864903 -2.0335739 -1.9353701 -1.8515406 -1.7878029 -1.7812479 -1.8683002 -1.9326587][-2.2854087 -2.6080983 -2.9975917 -3.2808075 -3.367837 -3.3463466 -3.3163235 -3.2188993 -3.0574403 -2.8549786 -2.6307697 -2.4225624 -2.2632461 -2.1872761 -2.1067941][-2.4642262 -2.7399096 -3.0345645 -3.2570398 -3.3606086 -3.4099016 -3.4339759 -3.3721392 -3.2308896 -3.0349765 -2.8087189 -2.5808833 -2.3823481 -2.2397609 -2.0987194][-2.3045056 -2.4934561 -2.6761782 -2.8228211 -2.9141595 -2.9739842 -3.0077243 -2.9714911 -2.8737268 -2.7306557 -2.5658357 -2.3917356 -2.2294571 -2.0990405 -1.9664583]]...]
INFO - root - 2017-12-16 09:21:31.568208: step 32310, loss = 0.61, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 14h:02m:18s remains)
INFO - root - 2017-12-16 09:21:33.232496: step 32320, loss = 0.56, batch loss = 0.30 (47.1 examples/sec; 0.170 sec/batch; 14h:09m:17s remains)
INFO - root - 2017-12-16 09:21:34.905643: step 32330, loss = 0.74, batch loss = 0.49 (46.8 examples/sec; 0.171 sec/batch; 14h:14m:36s remains)
INFO - root - 2017-12-16 09:21:36.566343: step 32340, loss = 0.60, batch loss = 0.34 (46.8 examples/sec; 0.171 sec/batch; 14h:14m:30s remains)
INFO - root - 2017-12-16 09:21:38.271734: step 32350, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 13h:34m:43s remains)
INFO - root - 2017-12-16 09:21:39.921106: step 32360, loss = 0.50, batch loss = 0.24 (47.3 examples/sec; 0.169 sec/batch; 14h:06m:08s remains)
INFO - root - 2017-12-16 09:21:41.571835: step 32370, loss = 0.65, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 13h:56m:13s remains)
INFO - root - 2017-12-16 09:21:43.219483: step 32380, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.167 sec/batch; 13h:56m:53s remains)
INFO - root - 2017-12-16 09:21:44.875376: step 32390, loss = 0.51, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:52m:57s remains)
INFO - root - 2017-12-16 09:21:46.529214: step 32400, loss = 0.48, batch loss = 0.22 (49.0 examples/sec; 0.163 sec/batch; 13h:36m:06s remains)
2017-12-16 09:21:47.047513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5429459 -1.0523254 -0.53200114 -0.34671426 -0.830477 -1.6086879 -2.3697543 -3.0171082 -3.2926035 -3.2528892 -3.1687396 -3.0495446 -2.8729155 -2.6591294 -2.4884415][-1.0862032 -0.61285317 -0.089211226 0.029649496 -0.56891668 -1.5406456 -2.4438996 -3.1499653 -3.5133765 -3.5624757 -3.5302668 -3.3737545 -3.1439095 -2.8184063 -2.5224926][-0.77358043 -0.41647792 0.074551344 0.18981171 -0.55633044 -1.6926689 -2.601609 -3.236115 -3.6570978 -3.8622293 -3.9163818 -3.7939892 -3.5300889 -3.1530545 -2.8203008][-0.57167685 -0.21789861 0.31923318 0.44648886 -0.33260584 -1.4651043 -2.286793 -2.8218377 -3.2967277 -3.6773062 -3.8825369 -3.9078307 -3.7409482 -3.430089 -3.1780348][-0.54461956 -0.14895797 0.53380013 0.8221159 0.1612885 -0.79086411 -1.4597998 -1.8926079 -2.4215977 -2.9712267 -3.3804195 -3.5650797 -3.5265255 -3.348366 -3.2490592][-0.97952306 -0.52985883 0.346797 0.86390543 0.51220179 -0.088421583 -0.50292659 -0.78588092 -1.3082165 -1.981835 -2.5363736 -2.8356218 -2.9219594 -2.9475384 -3.0661612][-1.6532536 -1.1212782 -0.089930534 0.69214153 0.75938392 0.62271357 0.51882792 0.37871695 -0.091778517 -0.83879066 -1.3592153 -1.7294277 -2.004277 -2.2504015 -2.5790277][-2.153862 -1.6429322 -0.64392245 0.3294425 0.84332061 1.2227342 1.4297199 1.4042687 1.0014143 0.48443294 0.10465693 -0.44754052 -0.95713472 -1.3547096 -1.7976239][-2.2439117 -1.8983563 -1.1979413 -0.27436447 0.48657823 1.2827647 1.8420384 1.8815072 1.5608547 1.3932312 1.1847224 0.51557684 -0.097944736 -0.50206161 -0.88525975][-2.1259041 -2.0373063 -1.736688 -1.1464276 -0.44945383 0.56250167 1.2763612 1.358676 1.2328713 1.3761044 1.3533189 0.69994259 0.12092543 -0.10250139 -0.29447055][-1.7885883 -2.0164278 -2.2042885 -2.1216342 -1.7434554 -0.89055669 -0.17406631 0.00014925003 0.034565687 0.39047194 0.4955554 -0.014125109 -0.37742233 -0.30877161 -0.22048283][-1.2788632 -1.7937921 -2.4192555 -2.8613195 -2.9348645 -2.5070219 -2.0224948 -1.8600988 -1.7775879 -1.3770213 -1.0725663 -1.2969284 -1.4016271 -1.0759776 -0.75267851][-0.8170017 -1.5260248 -2.5445554 -3.4166522 -3.7941399 -3.7425454 -3.5327733 -3.4862237 -3.4347849 -3.024935 -2.5356617 -2.4194679 -2.3076282 -1.9006603 -1.4082475][-0.64728165 -1.3811659 -2.5640817 -3.5785646 -4.114727 -4.2353258 -4.1412668 -4.0941648 -4.0262046 -3.669903 -3.1523025 -2.81721 -2.5897856 -2.2027528 -1.6500521][-0.73518157 -1.373437 -2.4407611 -3.3592248 -3.8855782 -4.0095663 -3.8480422 -3.6937151 -3.622716 -3.3780344 -2.9549294 -2.5962598 -2.332967 -1.9600337 -1.4692347]]...]
INFO - root - 2017-12-16 09:21:48.690297: step 32410, loss = 0.52, batch loss = 0.26 (48.6 examples/sec; 0.165 sec/batch; 13h:43m:57s remains)
INFO - root - 2017-12-16 09:21:50.356323: step 32420, loss = 0.65, batch loss = 0.39 (45.8 examples/sec; 0.175 sec/batch; 14h:33m:06s remains)
INFO - root - 2017-12-16 09:21:52.051460: step 32430, loss = 0.66, batch loss = 0.40 (48.7 examples/sec; 0.164 sec/batch; 13h:41m:36s remains)
INFO - root - 2017-12-16 09:21:53.727288: step 32440, loss = 0.48, batch loss = 0.23 (46.2 examples/sec; 0.173 sec/batch; 14h:26m:17s remains)
INFO - root - 2017-12-16 09:21:55.390613: step 32450, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:48m:40s remains)
INFO - root - 2017-12-16 09:21:57.042853: step 32460, loss = 0.60, batch loss = 0.34 (49.4 examples/sec; 0.162 sec/batch; 13h:29m:55s remains)
INFO - root - 2017-12-16 09:21:58.709989: step 32470, loss = 0.65, batch loss = 0.39 (47.6 examples/sec; 0.168 sec/batch; 14h:01m:05s remains)
INFO - root - 2017-12-16 09:22:00.365754: step 32480, loss = 0.58, batch loss = 0.32 (49.0 examples/sec; 0.163 sec/batch; 13h:35m:39s remains)
INFO - root - 2017-12-16 09:22:02.043977: step 32490, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:46m:52s remains)
INFO - root - 2017-12-16 09:22:03.737611: step 32500, loss = 0.66, batch loss = 0.40 (48.2 examples/sec; 0.166 sec/batch; 13h:50m:35s remains)
2017-12-16 09:22:04.227756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1785793 -1.1952466 -1.279068 -1.409948 -1.4399509 -1.2144283 -0.88247561 -0.81994092 -1.1162125 -1.467036 -1.6293216 -1.613554 -1.5236878 -1.432656 -1.3658183][-1.3868499 -1.4596918 -1.6593802 -1.942638 -2.0621824 -1.8210011 -1.3876741 -1.2892928 -1.7072659 -2.1549468 -2.2504656 -2.049226 -1.7783883 -1.5663891 -1.4386771][-1.5903312 -1.7527938 -2.082746 -2.4772639 -2.616926 -2.2611756 -1.6263026 -1.4430147 -2.0200007 -2.6748049 -2.7947028 -2.4821091 -2.0490534 -1.6953027 -1.4980199][-1.7493432 -2.0038438 -2.4014754 -2.780376 -2.7675979 -2.1003094 -1.1083019 -0.81101251 -1.6340083 -2.659503 -3.0271614 -2.7827103 -2.2832341 -1.801266 -1.5315458][-1.8761755 -2.1835103 -2.5448861 -2.7583711 -2.4110861 -1.2597812 0.2187984 0.71139216 -0.39119434 -1.9560702 -2.7874281 -2.8178151 -2.3939486 -1.8633003 -1.5458975][-1.9905995 -2.295964 -2.533155 -2.4702981 -1.7113992 -0.016154766 2.002332 2.7218354 1.3626494 -0.71209395 -2.0957918 -2.5520651 -2.3450708 -1.8799635 -1.558512][-2.0609634 -2.3214505 -2.4112546 -2.1172023 -1.0160179 1.116884 3.5721714 4.4065914 2.8448193 0.38676357 -1.4779199 -2.3016903 -2.2865522 -1.8965353 -1.5807124][-2.0885828 -2.3084133 -2.3231902 -1.9211075 -0.67689085 1.5910704 4.0749626 4.8197803 3.2083242 0.65225244 -1.3552605 -2.3139243 -2.3628995 -1.9757087 -1.6290216][-2.1191239 -2.3198946 -2.3425369 -1.9820856 -0.88525534 1.0579991 3.0668919 3.5895002 2.1931622 -0.00910902 -1.7629981 -2.5715003 -2.5064442 -2.0595615 -1.6758058][-2.2069771 -2.3750296 -2.4043486 -2.1678047 -1.4277189 -0.09141922 1.2876358 1.6321104 0.58316445 -1.0444659 -2.3306515 -2.824852 -2.5786085 -2.0706744 -1.6848311][-2.3436866 -2.4299378 -2.4131505 -2.2850041 -1.9110863 -1.091472 -0.097474813 0.20461869 -0.54809678 -1.7194158 -2.5871289 -2.860003 -2.5182006 -2.0218027 -1.6648966][-2.5007584 -2.5011802 -2.4035378 -2.2867875 -2.0363519 -1.3540277 -0.45100749 -0.12916064 -0.77859211 -1.7763381 -2.518259 -2.7610724 -2.4541354 -1.9864012 -1.6451082][-2.6102915 -2.5802515 -2.4318407 -2.2794573 -1.9161069 -1.0273784 0.028105497 0.37630773 -0.38121176 -1.5050097 -2.3535175 -2.6764307 -2.4276454 -1.9825855 -1.6340797][-2.6056798 -2.6019304 -2.4824493 -2.2478545 -1.5996013 -0.39029944 0.89933634 1.1534145 0.17938614 -1.2066058 -2.209687 -2.6207564 -2.4414206 -2.0128496 -1.6414081][-2.4215853 -2.4330518 -2.3254845 -1.9905527 -1.1436825 0.28265119 1.546876 1.5717709 0.29426908 -1.3340909 -2.3833172 -2.7244313 -2.5139232 -2.0412426 -1.6446631]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-32500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:22:06.407903: step 32510, loss = 0.52, batch loss = 0.26 (46.2 examples/sec; 0.173 sec/batch; 14h:25m:36s remains)
INFO - root - 2017-12-16 09:22:08.055701: step 32520, loss = 0.53, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:52m:26s remains)
INFO - root - 2017-12-16 09:22:09.688864: step 32530, loss = 0.58, batch loss = 0.32 (49.3 examples/sec; 0.162 sec/batch; 13h:31m:20s remains)
INFO - root - 2017-12-16 09:22:11.361850: step 32540, loss = 0.51, batch loss = 0.25 (47.3 examples/sec; 0.169 sec/batch; 14h:06m:16s remains)
INFO - root - 2017-12-16 09:22:13.021649: step 32550, loss = 0.63, batch loss = 0.37 (48.1 examples/sec; 0.166 sec/batch; 13h:50m:59s remains)
INFO - root - 2017-12-16 09:22:14.685490: step 32560, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 13h:53m:58s remains)
INFO - root - 2017-12-16 09:22:16.339365: step 32570, loss = 0.63, batch loss = 0.37 (48.6 examples/sec; 0.164 sec/batch; 13h:42m:01s remains)
INFO - root - 2017-12-16 09:22:18.009381: step 32580, loss = 0.62, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 14h:01m:16s remains)
INFO - root - 2017-12-16 09:22:19.672059: step 32590, loss = 0.60, batch loss = 0.34 (47.2 examples/sec; 0.169 sec/batch; 14h:06m:25s remains)
INFO - root - 2017-12-16 09:22:21.346652: step 32600, loss = 0.62, batch loss = 0.36 (48.7 examples/sec; 0.164 sec/batch; 13h:40m:34s remains)
2017-12-16 09:22:21.821987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.916733 -2.9751434 -3.1277184 -3.3873215 -3.5628862 -3.5389009 -3.3853259 -3.264997 -3.2216232 -3.1379712 -3.0061021 -2.910378 -2.7857366 -2.5704255 -2.273807][-3.0084682 -3.0931959 -3.3263919 -3.6702366 -3.8473203 -3.7512705 -3.5005662 -3.2819924 -3.1315069 -2.9431231 -2.7429903 -2.643362 -2.4927015 -2.2391593 -1.9119381][-2.9528308 -3.0062675 -3.2274883 -3.5547845 -3.7204792 -3.5745749 -3.2771754 -3.0385265 -2.8541665 -2.62129 -2.3647828 -2.2611794 -2.1692724 -1.9473646 -1.6424894][-2.7310178 -2.685004 -2.7180843 -2.8920383 -2.9652989 -2.7788849 -2.5188184 -2.3434868 -2.2170599 -2.0045457 -1.8158271 -1.8229953 -1.8658223 -1.7816198 -1.560914][-2.3470507 -2.1336036 -1.9057058 -1.79052 -1.6634043 -1.332275 -1.0514227 -1.0360889 -1.0285718 -0.89145935 -0.90685904 -1.1281914 -1.3705127 -1.5499063 -1.5295701][-1.9347905 -1.4921609 -0.89889145 -0.37468243 0.21127772 0.82655883 1.1054161 0.87972355 0.64134955 0.55024409 0.16475821 -0.36011672 -0.79057419 -1.1912041 -1.4490271][-1.6282802 -1.0277524 -0.14677453 0.78985596 1.9066701 2.9570413 3.2476826 2.7567325 2.2810249 1.8126945 1.0166025 0.23230553 -0.31842875 -0.86065805 -1.3644439][-1.6031369 -0.96917546 -0.033696651 1.1096623 2.5885439 3.8508792 4.0985732 3.4849577 2.8957849 2.2238092 1.2546039 0.41612196 -0.16814566 -0.72579789 -1.3265092][-1.8976204 -1.3108407 -0.52827787 0.51732707 1.9143476 3.1347008 3.307703 2.770164 2.3121724 1.7517443 0.96367216 0.35853815 -0.1094377 -0.64618266 -1.2022653][-2.4822471 -2.1399229 -1.6866655 -1.0110947 -0.07986784 0.82653117 1.1190014 0.76724291 0.51776838 0.22418261 -0.25269389 -0.58136177 -0.8467052 -1.1572183 -1.5095645][-3.045289 -2.9985979 -2.8953683 -2.7050319 -2.2997398 -1.7811203 -1.5259368 -1.6271863 -1.6108634 -1.6297538 -1.8298869 -1.9148941 -1.9423138 -1.9928639 -2.0969605][-3.2347107 -3.3416538 -3.4492104 -3.5384314 -3.5086594 -3.365581 -3.292552 -3.3146534 -3.2056496 -3.0829971 -3.0944266 -2.9931378 -2.8068783 -2.6359606 -2.537869][-3.1177089 -3.2544441 -3.3940117 -3.5848539 -3.7123222 -3.7478771 -3.7883859 -3.8448355 -3.7374527 -3.6001303 -3.532604 -3.3993287 -3.1823368 -2.9553466 -2.774189][-2.8192906 -2.9442337 -3.0695729 -3.2143645 -3.3121502 -3.3845685 -3.4698782 -3.5259442 -3.4577441 -3.3356273 -3.2647524 -3.1934583 -3.0704839 -2.8968239 -2.7400596][-2.4927649 -2.5693476 -2.6452649 -2.7086952 -2.7466867 -2.7727392 -2.8118911 -2.8386536 -2.8105192 -2.7553513 -2.732302 -2.7131138 -2.6761785 -2.6025665 -2.511795]]...]
INFO - root - 2017-12-16 09:22:23.483778: step 32610, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.168 sec/batch; 13h:57m:22s remains)
INFO - root - 2017-12-16 09:22:25.183327: step 32620, loss = 0.56, batch loss = 0.31 (45.5 examples/sec; 0.176 sec/batch; 14h:38m:59s remains)
INFO - root - 2017-12-16 09:22:26.860822: step 32630, loss = 0.46, batch loss = 0.21 (48.0 examples/sec; 0.167 sec/batch; 13h:53m:29s remains)
INFO - root - 2017-12-16 09:22:28.560958: step 32640, loss = 0.47, batch loss = 0.21 (47.0 examples/sec; 0.170 sec/batch; 14h:11m:18s remains)
INFO - root - 2017-12-16 09:22:30.209320: step 32650, loss = 0.59, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 13h:44m:50s remains)
INFO - root - 2017-12-16 09:22:31.881209: step 32660, loss = 0.68, batch loss = 0.42 (49.1 examples/sec; 0.163 sec/batch; 13h:35m:03s remains)
INFO - root - 2017-12-16 09:22:33.602793: step 32670, loss = 0.53, batch loss = 0.27 (45.7 examples/sec; 0.175 sec/batch; 14h:35m:14s remains)
INFO - root - 2017-12-16 09:22:35.249853: step 32680, loss = 0.66, batch loss = 0.40 (49.1 examples/sec; 0.163 sec/batch; 13h:33m:33s remains)
INFO - root - 2017-12-16 09:22:36.914009: step 32690, loss = 0.58, batch loss = 0.32 (48.9 examples/sec; 0.164 sec/batch; 13h:38m:01s remains)
INFO - root - 2017-12-16 09:22:38.588646: step 32700, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:39m:34s remains)
2017-12-16 09:22:39.089500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8852175 -1.9573063 -2.2281291 -2.688951 -3.0511544 -2.8971982 -2.1214676 -1.1605217 -0.640175 -0.64126194 -0.81973588 -0.79281712 -0.66558826 -0.66470551 -0.76783836][-1.5713871 -1.5735884 -1.834303 -2.3199739 -2.8305714 -2.8622391 -2.1650949 -1.0975586 -0.34059286 -0.13726139 -0.30973506 -0.38543403 -0.33163714 -0.33778739 -0.37911463][-1.6244591 -1.5504515 -1.734446 -2.0775838 -2.5628309 -2.7603865 -2.3190033 -1.4481068 -0.68453813 -0.33353972 -0.36366868 -0.43379509 -0.48206782 -0.5856111 -0.61483419][-2.0187879 -1.9574122 -1.9632633 -1.9370737 -2.0865474 -2.2484944 -2.0779173 -1.5973852 -1.0259932 -0.64076912 -0.47578812 -0.48426116 -0.79106474 -1.1547302 -1.3080815][-2.2877026 -2.2884467 -2.1437976 -1.7627063 -1.3981297 -1.2168345 -1.1290656 -0.99692273 -0.7343241 -0.36802268 -0.078527689 -0.16199875 -0.78864694 -1.5209177 -1.7979678][-2.0852437 -2.2924385 -2.1336374 -1.5127296 -0.66749108 0.0043857098 0.28080153 0.30574441 0.32711172 0.54703116 0.87623215 0.74140453 -0.11432338 -1.050159 -1.422835][-1.8230176 -2.2084634 -2.0755067 -1.3416291 -0.21902251 0.81657195 1.4254682 1.6014271 1.4405849 1.48651 1.8259413 1.8211629 0.99270439 0.054260492 -0.29490352][-1.5795647 -2.0642905 -2.0719459 -1.4608841 -0.38939714 0.6797719 1.5348969 1.8733475 1.5714247 1.4347634 1.7841904 1.9208181 1.4579124 0.9051044 0.710161][-1.4455607 -2.0066791 -2.1213202 -1.7898016 -1.1029164 -0.35731173 0.31999254 0.58386064 0.24441409 -0.024357557 0.3294065 0.75697589 0.84138131 0.83968616 0.94376612][-1.403304 -1.8852236 -2.1008632 -2.0782821 -1.9687184 -1.8104789 -1.6348088 -1.620806 -1.9018421 -2.0385692 -1.6362338 -1.0482711 -0.59911203 -0.23865104 0.065613985][-1.2103796 -1.4393746 -1.5797691 -1.7997417 -2.2161932 -2.6549256 -3.0174105 -3.3123989 -3.5724919 -3.6082249 -3.2144606 -2.638298 -2.0876493 -1.6050991 -1.3285986][-0.84470093 -0.63540971 -0.63207173 -0.98751509 -1.6767168 -2.4655631 -3.1967261 -3.7329676 -4.0498562 -4.1299558 -3.8904581 -3.4752767 -2.9879863 -2.5738876 -2.4544339][-0.46318972 0.072732687 0.24198365 -0.12593818 -0.85732007 -1.7561605 -2.6057866 -3.2170594 -3.577019 -3.7874446 -3.773675 -3.5835562 -3.3267612 -3.1374612 -3.1604829][-0.28672743 0.2560277 0.46591592 0.10698557 -0.52815032 -1.274887 -1.9950409 -2.520833 -2.8658855 -3.1247945 -3.2872527 -3.2799299 -3.1736157 -3.0953977 -3.1527705][-0.68817532 -0.24943447 -0.0855906 -0.39072239 -0.90654838 -1.3993785 -1.8420103 -2.1607831 -2.3796117 -2.5852904 -2.74967 -2.7754884 -2.7076991 -2.6481943 -2.6681967]]...]
INFO - root - 2017-12-16 09:22:40.763345: step 32710, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 13h:51m:33s remains)
INFO - root - 2017-12-16 09:22:42.439204: step 32720, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 13h:58m:29s remains)
INFO - root - 2017-12-16 09:22:44.081620: step 32730, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 13h:39m:11s remains)
INFO - root - 2017-12-16 09:22:45.735158: step 32740, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 13h:51m:34s remains)
INFO - root - 2017-12-16 09:22:47.404501: step 32750, loss = 0.58, batch loss = 0.32 (48.9 examples/sec; 0.164 sec/batch; 13h:37m:02s remains)
INFO - root - 2017-12-16 09:22:49.074458: step 32760, loss = 0.82, batch loss = 0.56 (48.8 examples/sec; 0.164 sec/batch; 13h:39m:03s remains)
INFO - root - 2017-12-16 09:22:50.762614: step 32770, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:49m:21s remains)
INFO - root - 2017-12-16 09:22:52.442757: step 32780, loss = 0.51, batch loss = 0.25 (44.9 examples/sec; 0.178 sec/batch; 14h:50m:46s remains)
INFO - root - 2017-12-16 09:22:54.122286: step 32790, loss = 0.53, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 13h:50m:42s remains)
INFO - root - 2017-12-16 09:22:55.798174: step 32800, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 13h:35m:47s remains)
2017-12-16 09:22:56.261001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4434111 -2.7900295 -2.4776883 -2.6577492 -3.0243485 -3.3581905 -3.6171563 -3.7939692 -3.898654 -4.0331645 -4.2431469 -4.4851065 -4.5652094 -4.3674769 -3.9676733][-3.3793049 -2.6876929 -2.2395654 -2.2325368 -2.4680932 -2.8194752 -3.2314422 -3.5714633 -3.8261862 -4.1531568 -4.5938368 -5.0706096 -5.309659 -5.1483259 -4.6945529][-3.0430336 -2.3211372 -1.7244742 -1.4537766 -1.4265649 -1.6370206 -2.033036 -2.3879728 -2.6936014 -3.1641321 -3.8733864 -4.690197 -5.1942034 -5.1873732 -4.7975931][-2.7656131 -2.0845289 -1.3895789 -0.88380814 -0.55019391 -0.43890369 -0.5236479 -0.62286806 -0.80209851 -1.2676687 -2.1160595 -3.18649 -3.9613044 -4.1945028 -3.9895039][-2.5834739 -1.9998796 -1.2938181 -0.64949143 -0.067157984 0.43151546 0.82970905 1.0986524 1.1003528 0.78421116 0.033759832 -1.0744722 -2.0161576 -2.4431477 -2.4910715][-2.2498128 -1.8410788 -1.2674239 -0.72002614 -0.13675046 0.5895679 1.3255644 1.8731823 2.0537796 1.9939399 1.5556784 0.61373019 -0.34453654 -0.85962594 -1.040784][-1.915888 -1.7031894 -1.3864881 -1.1757436 -0.85182142 -0.11340165 0.77024841 1.3061218 1.4221878 1.5504532 1.4928536 0.8932004 0.17013979 -0.12054276 -0.11171865][-1.5521436 -1.4898969 -1.5296506 -1.8083347 -1.8640969 -1.2656416 -0.41338027 -0.0947001 -0.19746137 -0.0963974 0.07864213 -0.085912466 -0.31884313 -0.13659739 0.2958827][-1.0232139 -0.93389511 -1.2951512 -2.0826781 -2.531059 -2.088002 -1.4188018 -1.3797953 -1.6968142 -1.7256765 -1.5015142 -1.3457823 -1.1216221 -0.47808146 0.41448927][-0.090987206 0.17154169 -0.48116219 -1.7694967 -2.5715945 -2.335233 -1.8873881 -2.0362883 -2.4848688 -2.6343615 -2.4241512 -2.1027653 -1.6423693 -0.76650274 0.43618417][1.0818129 1.6121845 0.83218312 -0.74289846 -1.7642984 -1.7170334 -1.4812469 -1.7596143 -2.2173302 -2.411052 -2.2705765 -1.9181572 -1.4255352 -0.60989618 0.55306411][1.7839413 2.6022677 1.8817225 0.27994776 -0.84447014 -0.93660855 -0.82685959 -1.0794175 -1.4678316 -1.6901884 -1.71527 -1.5170488 -1.1277854 -0.49409139 0.39264703][1.270083 2.3093467 1.9014602 0.60155487 -0.38657022 -0.50819874 -0.41886926 -0.55742013 -0.80981755 -1.0545412 -1.2321616 -1.2381662 -1.0116342 -0.62075818 -0.10412788][-0.19698524 0.861279 0.82047868 -0.087680817 -0.85332704 -0.97108543 -0.88918483 -0.91648889 -1.0439839 -1.2425619 -1.4431603 -1.52555 -1.4485407 -1.2734723 -1.0855446][-1.8007934 -0.91721725 -0.80297542 -1.4019823 -1.9539478 -2.0712121 -2.0268111 -2.0239339 -2.0703745 -2.1784225 -2.3014073 -2.3710585 -2.3551805 -2.3247917 -2.3441494]]...]
INFO - root - 2017-12-16 09:22:57.917464: step 32810, loss = 0.62, batch loss = 0.36 (47.9 examples/sec; 0.167 sec/batch; 13h:53m:20s remains)
INFO - root - 2017-12-16 09:22:59.596497: step 32820, loss = 0.44, batch loss = 0.18 (49.0 examples/sec; 0.163 sec/batch; 13h:34m:59s remains)
INFO - root - 2017-12-16 09:23:01.269495: step 32830, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 13h:54m:27s remains)
INFO - root - 2017-12-16 09:23:02.915495: step 32840, loss = 0.63, batch loss = 0.37 (48.3 examples/sec; 0.166 sec/batch; 13h:47m:42s remains)
INFO - root - 2017-12-16 09:23:04.573829: step 32850, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 13h:45m:15s remains)
INFO - root - 2017-12-16 09:23:06.253119: step 32860, loss = 0.53, batch loss = 0.27 (46.7 examples/sec; 0.171 sec/batch; 14h:16m:03s remains)
INFO - root - 2017-12-16 09:23:07.963925: step 32870, loss = 0.49, batch loss = 0.24 (44.3 examples/sec; 0.181 sec/batch; 15h:02m:49s remains)
INFO - root - 2017-12-16 09:23:09.666616: step 32880, loss = 0.61, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 14h:02m:08s remains)
INFO - root - 2017-12-16 09:23:11.348281: step 32890, loss = 0.64, batch loss = 0.39 (46.7 examples/sec; 0.171 sec/batch; 14h:15m:53s remains)
INFO - root - 2017-12-16 09:23:13.035978: step 32900, loss = 0.59, batch loss = 0.33 (48.8 examples/sec; 0.164 sec/batch; 13h:39m:21s remains)
2017-12-16 09:23:13.540887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6668031 -2.569905 -2.6773689 -2.882205 -3.1189849 -3.3447602 -3.4596019 -3.45195 -3.3967931 -3.3749974 -3.3841004 -3.4272103 -3.502532 -3.303937 -2.8569][-2.0555646 -1.9057425 -2.1713562 -2.7020907 -3.28505 -3.8202162 -4.1327047 -4.1794248 -4.0825286 -3.9529362 -3.8146431 -3.7648883 -3.8102536 -3.5723014 -2.9750493][-1.0561587 -0.73264909 -1.1610547 -2.0017371 -2.9373326 -3.7744391 -4.2357945 -4.3202114 -4.1769586 -3.9633813 -3.6728296 -3.4680033 -3.3669088 -2.9857519 -2.1880317][-0.2269032 0.35820317 -0.17223883 -1.2158635 -2.2875676 -3.1504951 -3.6064239 -3.7056584 -3.5758071 -3.4201345 -3.0704861 -2.7473402 -2.5041094 -1.9233727 -0.86597049][-0.11233687 0.66899991 0.14179373 -0.93021858 -1.8060887 -2.3083336 -2.4360633 -2.3707254 -2.3247325 -2.38559 -2.1305959 -1.8975668 -1.7469913 -1.2187694 -0.17659044][-0.74856377 0.10554457 -0.30656934 -1.199584 -1.6617069 -1.4578955 -0.8820554 -0.4116528 -0.49673712 -0.89395857 -0.93367553 -0.99298847 -1.2133027 -1.0654976 -0.26985097][-1.5818405 -0.75070024 -0.96797144 -1.5467213 -1.5614156 -0.66117966 0.72804546 1.7455509 1.4612956 0.63067436 0.16113043 -0.32034254 -0.92222238 -1.1425772 -0.61084425][-2.2446797 -1.5368652 -1.5777233 -1.851227 -1.5849692 -0.37134123 1.5561366 3.0290363 2.4936993 1.2154417 0.33993173 -0.42003572 -1.1788725 -1.4497739 -1.0031154][-2.7260313 -2.249301 -2.2453685 -2.4081991 -2.1709714 -1.1126016 0.7641089 2.1780179 1.535568 0.21944499 -0.73022342 -1.4733514 -2.1837366 -2.3333888 -1.9435849][-3.0127592 -2.8389084 -2.9521918 -3.1905308 -3.1418047 -2.4596102 -1.0955137 -0.1127193 -0.5781312 -1.5338356 -2.2791271 -2.8150237 -3.2382205 -3.2340245 -2.9162002][-2.9161718 -2.8848705 -3.0654254 -3.4238691 -3.6127834 -3.3522696 -2.5822182 -2.0107703 -2.2347414 -2.7212577 -3.1352265 -3.4231849 -3.5479403 -3.4378405 -3.2317657][-2.4210932 -2.3795335 -2.5490839 -2.9554291 -3.3369703 -3.395864 -3.0875335 -2.8291013 -2.9131114 -3.0538042 -3.1584239 -3.1913846 -3.0898454 -2.9119034 -2.7814212][-1.9006038 -1.8265334 -1.9405377 -2.2916028 -2.7024293 -2.9208095 -2.8954616 -2.8487022 -2.889029 -2.8480499 -2.7384081 -2.5833228 -2.3946383 -2.2244229 -2.1438046][-1.6665748 -1.588218 -1.6244434 -1.8322403 -2.1423054 -2.3735392 -2.4661362 -2.5212779 -2.5488813 -2.455411 -2.2915306 -2.0872669 -1.9269989 -1.814481 -1.7796347][-1.6675751 -1.610306 -1.5945656 -1.6790478 -1.8496501 -2.0110571 -2.0976729 -2.1494498 -2.158751 -2.0892024 -1.9817677 -1.8571007 -1.7719362 -1.7222707 -1.7140995]]...]
INFO - root - 2017-12-16 09:23:15.193891: step 32910, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.166 sec/batch; 13h:46m:41s remains)
INFO - root - 2017-12-16 09:23:16.834671: step 32920, loss = 0.55, batch loss = 0.29 (49.1 examples/sec; 0.163 sec/batch; 13h:33m:49s remains)
INFO - root - 2017-12-16 09:23:18.474236: step 32930, loss = 0.51, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 13h:37m:38s remains)
INFO - root - 2017-12-16 09:23:20.127852: step 32940, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:48m:46s remains)
INFO - root - 2017-12-16 09:23:21.789863: step 32950, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.168 sec/batch; 13h:56m:17s remains)
INFO - root - 2017-12-16 09:23:23.426796: step 32960, loss = 0.53, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:51m:41s remains)
INFO - root - 2017-12-16 09:23:25.118184: step 32970, loss = 0.47, batch loss = 0.21 (47.6 examples/sec; 0.168 sec/batch; 13h:58m:42s remains)
INFO - root - 2017-12-16 09:23:26.783093: step 32980, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 13h:54m:55s remains)
INFO - root - 2017-12-16 09:23:28.419570: step 32990, loss = 0.69, batch loss = 0.43 (48.4 examples/sec; 0.165 sec/batch; 13h:44m:26s remains)
INFO - root - 2017-12-16 09:23:30.078759: step 33000, loss = 0.54, batch loss = 0.29 (46.3 examples/sec; 0.173 sec/batch; 14h:23m:01s remains)
2017-12-16 09:23:30.539928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8985571 -1.7249666 -1.7027981 -1.6706116 -1.6344001 -1.6927239 -1.5296469 -1.368369 -1.6603539 -1.9191053 -1.8816507 -1.9903641 -2.129869 -1.9688814 -1.557178][-2.2506266 -2.2834625 -2.3193409 -2.2496614 -2.2149448 -2.2547567 -1.9785044 -1.6259085 -1.7683315 -1.9734976 -1.9619814 -2.0837677 -2.2289948 -2.0493972 -1.6037953][-2.2803223 -2.6031008 -2.7684119 -2.7487712 -2.7008736 -2.5930469 -2.0933433 -1.4359478 -1.3065801 -1.4350671 -1.5673006 -1.9039929 -2.2103057 -2.1467507 -1.7400119][-2.0264323 -2.6582086 -2.9580352 -2.9281857 -2.7774441 -2.5025549 -1.8104016 -0.89963973 -0.49593318 -0.52081084 -0.80097282 -1.4384545 -2.0467851 -2.2213664 -1.9652655][-1.4243641 -2.1311395 -2.4004614 -2.2917442 -1.9914886 -1.5312588 -0.77964532 0.1318934 0.59977841 0.60152054 0.15264535 -0.80647886 -1.7491215 -2.2258019 -2.2201531][-0.70454407 -1.2572271 -1.2951075 -0.90390265 -0.28491259 0.4347949 1.2094285 1.8369346 1.9009256 1.5344477 0.77325439 -0.48534405 -1.7116857 -2.3903108 -2.5098939][0.082901239 -0.22178912 0.060688972 0.69848967 1.663012 2.8798313 3.7882752 3.8518467 3.0613909 1.8305907 0.50927949 -0.88183868 -2.0663171 -2.669116 -2.7318377][0.57045174 0.62409377 1.1345141 1.8867183 2.9815397 4.6791553 5.8919311 4.9593086 3.12813 1.3105733 -0.20340824 -1.5176642 -2.5159249 -2.9039927 -2.7934587][0.51474023 0.93670607 1.5398984 2.0730996 2.7586269 3.7038908 4.2302814 3.437243 1.7400413 0.14238119 -1.0416588 -2.0830948 -2.7958586 -2.882704 -2.5906458][-0.066071272 0.51798296 1.0911543 1.2243047 1.2822969 1.4920537 1.5186021 0.89844346 -0.30031991 -1.3218327 -1.9794545 -2.5501769 -2.907841 -2.7414386 -2.2758539][-1.2267489 -0.78916907 -0.42391038 -0.50513995 -0.80287731 -1.0255411 -1.2154123 -1.6532063 -2.3381281 -2.7748744 -2.8570642 -2.9263821 -2.9220886 -2.5775602 -2.0677466][-2.3786714 -2.2121382 -2.0885 -2.3320043 -2.789228 -3.2078824 -3.4303393 -3.5577781 -3.7584212 -3.708744 -3.4004712 -3.1298304 -2.9130642 -2.5142608 -2.054863][-2.9228609 -2.9580886 -3.0026741 -3.404212 -3.9737573 -4.3773413 -4.4328885 -4.2154226 -3.9853833 -3.6497765 -3.2266695 -2.8794761 -2.6478231 -2.3896005 -2.1268654][-2.7587342 -2.9041491 -3.1180296 -3.6051295 -4.1216893 -4.3484917 -4.1393452 -3.6021185 -3.0851326 -2.7221549 -2.4775076 -2.3176515 -2.24682 -2.2371862 -2.2042758][-2.5041833 -2.696933 -2.9536591 -3.3384628 -3.60742 -3.59485 -3.1143003 -2.3033581 -1.6814139 -1.5084623 -1.6082602 -1.8044832 -2.0240359 -2.2031481 -2.2696221]]...]
INFO - root - 2017-12-16 09:23:32.235888: step 33010, loss = 0.54, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:09m:07s remains)
INFO - root - 2017-12-16 09:23:33.887589: step 33020, loss = 0.46, batch loss = 0.20 (48.0 examples/sec; 0.167 sec/batch; 13h:51m:59s remains)
INFO - root - 2017-12-16 09:23:35.567956: step 33030, loss = 0.54, batch loss = 0.28 (43.9 examples/sec; 0.182 sec/batch; 15h:08m:35s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:23:37.297264: step 33040, loss = 0.55, batch loss = 0.29 (49.6 examples/sec; 0.161 sec/batch; 13h:24m:40s remains)
INFO - root - 2017-12-16 09:23:38.990176: step 33050, loss = 0.52, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 14h:01m:34s remains)
INFO - root - 2017-12-16 09:23:40.649057: step 33060, loss = 0.54, batch loss = 0.28 (49.7 examples/sec; 0.161 sec/batch; 13h:23m:17s remains)
INFO - root - 2017-12-16 09:23:42.349150: step 33070, loss = 0.60, batch loss = 0.34 (48.2 examples/sec; 0.166 sec/batch; 13h:47m:40s remains)
INFO - root - 2017-12-16 09:23:44.029484: step 33080, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 13h:42m:25s remains)
INFO - root - 2017-12-16 09:23:45.698133: step 33090, loss = 0.73, batch loss = 0.48 (48.5 examples/sec; 0.165 sec/batch; 13h:43m:36s remains)
INFO - root - 2017-12-16 09:23:47.363720: step 33100, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.166 sec/batch; 13h:47m:10s remains)
2017-12-16 09:23:47.805406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7280252 -3.8476963 -3.8149362 -3.6226916 -3.4455185 -3.3054638 -3.1308656 -2.9996011 -2.9689555 -2.9060535 -2.7680414 -2.6257017 -2.6547475 -2.8821456 -3.1270828][-3.7571201 -3.7136874 -3.5873926 -3.373163 -3.21766 -3.0244026 -2.76672 -2.5597556 -2.4635348 -2.2513218 -1.9219457 -1.6242118 -1.6006753 -1.8794472 -2.2220681][-3.1685314 -2.8917272 -2.6409001 -2.4558387 -2.3944125 -2.2600007 -2.0373883 -1.8861768 -1.8875488 -1.6871169 -1.3454196 -1.0288191 -0.98282218 -1.2902818 -1.5784326][-2.1416905 -1.6528486 -1.355112 -1.250819 -1.2960711 -1.2328699 -1.0825454 -1.0947978 -1.3146062 -1.2961612 -1.1280322 -0.96345663 -1.0181519 -1.2354765 -1.3591604][-0.76402581 -0.20001078 0.089856148 0.10732985 -0.091879129 -0.16501856 -0.089243889 -0.22286606 -0.576061 -0.73959744 -0.76883733 -0.84870815 -1.035553 -1.1997695 -1.1653509][0.75903893 1.2177405 1.4138992 1.3180041 0.98959684 0.78796244 0.76169276 0.54669619 0.16425848 -0.099063873 -0.26795173 -0.51386404 -0.83415675 -0.9930886 -0.85481322][1.7217002 2.0181088 2.1067381 1.9744501 1.641963 1.4621069 1.3925464 1.1623497 0.80868721 0.5825367 0.37178755 0.0013520718 -0.43031657 -0.67717624 -0.65257037][1.7017279 1.8532057 1.9437022 1.8203516 1.6015646 1.5369201 1.5346096 1.3384118 1.0529802 0.92102289 0.79967666 0.4208281 -0.052737474 -0.41456664 -0.53068388][0.9888165 0.97653913 0.99816132 0.8988657 0.7900095 0.81745887 0.8608 0.67442727 0.46448445 0.41079402 0.355829 0.075299025 -0.34826064 -0.73243165 -0.95936167][-0.1321249 -0.27214146 -0.31028104 -0.39487171 -0.47445333 -0.45523703 -0.41757584 -0.58288324 -0.7592988 -0.78438032 -0.80119443 -0.98677444 -1.283843 -1.5955381 -1.8363277][-1.3098077 -1.502455 -1.5765598 -1.6276519 -1.6739738 -1.6473198 -1.6355805 -1.7649624 -1.9246137 -2.0008867 -2.0613327 -2.2175803 -2.4156623 -2.5902262 -2.7101984][-2.2013857 -2.4344976 -2.5299335 -2.5589819 -2.5936205 -2.6000628 -2.624881 -2.7593191 -2.8929579 -2.9767227 -3.0557494 -3.1645455 -3.2194886 -3.1928575 -3.1362512][-2.9449818 -3.1733351 -3.2557373 -3.2705269 -3.2978816 -3.3177814 -3.3556991 -3.4335141 -3.4960909 -3.5206661 -3.5315666 -3.5252514 -3.4355922 -3.2694237 -3.1019585][-3.3076906 -3.4285548 -3.4294472 -3.3944039 -3.3888154 -3.3673182 -3.3635867 -3.3876305 -3.3898065 -3.3480992 -3.27767 -3.2003257 -3.077786 -2.9010293 -2.73004][-2.8351729 -2.807873 -2.7232864 -2.6495588 -2.6218035 -2.6011567 -2.6004772 -2.6089246 -2.5886006 -2.511935 -2.3974767 -2.2991991 -2.2012706 -2.085392 -1.9722636]]...]
INFO - root - 2017-12-16 09:23:49.461660: step 33110, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 14h:07m:01s remains)
INFO - root - 2017-12-16 09:23:51.131493: step 33120, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:48m:27s remains)
INFO - root - 2017-12-16 09:23:52.806966: step 33130, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 13h:51m:16s remains)
INFO - root - 2017-12-16 09:23:54.461707: step 33140, loss = 0.58, batch loss = 0.32 (47.8 examples/sec; 0.167 sec/batch; 13h:54m:32s remains)
INFO - root - 2017-12-16 09:23:56.164863: step 33150, loss = 0.57, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 13h:39m:11s remains)
INFO - root - 2017-12-16 09:23:57.826429: step 33160, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:48m:40s remains)
INFO - root - 2017-12-16 09:23:59.489734: step 33170, loss = 0.64, batch loss = 0.38 (46.7 examples/sec; 0.171 sec/batch; 14h:14m:53s remains)
INFO - root - 2017-12-16 09:24:01.165605: step 33180, loss = 0.59, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 14h:04m:31s remains)
INFO - root - 2017-12-16 09:24:02.851604: step 33190, loss = 0.55, batch loss = 0.29 (46.8 examples/sec; 0.171 sec/batch; 14h:12m:56s remains)
INFO - root - 2017-12-16 09:24:04.548757: step 33200, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 14h:24m:21s remains)
2017-12-16 09:24:05.041931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7635741 -1.8285873 -1.9024322 -1.9623032 -1.9761798 -1.9439633 -1.9047546 -1.8924885 -1.9180506 -1.9351211 -1.8871021 -1.7642108 -1.5992769 -1.4327891 -1.2871213][-1.8831372 -1.9756972 -2.0834944 -2.1588395 -2.1445925 -2.0501635 -1.9440798 -1.8953509 -1.9214501 -1.9722528 -1.9547637 -1.8253186 -1.6151552 -1.3905184 -1.2051692][-2.099431 -2.2291381 -2.3643758 -2.4357681 -2.3734174 -2.2156286 -2.0697865 -2.0274949 -2.1057119 -2.23409 -2.2934558 -2.1908815 -1.9529969 -1.6756734 -1.4448667][-2.3291159 -2.4863484 -2.6058397 -2.5879447 -2.381928 -2.0882063 -1.9114081 -1.971452 -2.2212627 -2.5025959 -2.684942 -2.6705809 -2.4543149 -2.1505764 -1.8789425][-2.465054 -2.6388364 -2.6851797 -2.4514446 -1.9297894 -1.3569872 -1.0916145 -1.3057454 -1.8290322 -2.3619318 -2.7494617 -2.9056325 -2.79345 -2.5250719 -2.2436011][-2.5051992 -2.6814785 -2.6286731 -2.1225777 -1.1831408 -0.19512343 0.2668879 -0.044278622 -0.84613681 -1.6929266 -2.3341141 -2.7227244 -2.8058445 -2.6485476 -2.3966942][-2.4894655 -2.6800604 -2.5714874 -1.8571038 -0.52320445 0.95953631 1.8253734 1.6184189 0.60201192 -0.60273659 -1.5947136 -2.2661655 -2.5713608 -2.5590091 -2.3668656][-2.4538887 -2.6983194 -2.658133 -1.945007 -0.41743457 1.506357 2.9231603 3.0957549 2.064945 0.54046273 -0.85098743 -1.8157992 -2.3044612 -2.4026339 -2.263792][-2.3886826 -2.7050989 -2.8299017 -2.3279865 -0.90267813 1.1576102 2.9330413 3.5197074 2.7261245 1.1406281 -0.44765294 -1.5635422 -2.1369464 -2.2811813 -2.1759439][-2.2668087 -2.6251 -2.9069238 -2.684721 -1.5938268 0.21024895 1.9288046 2.7010438 2.2249763 0.89676809 -0.55336642 -1.5971839 -2.1276491 -2.2476292 -2.1373951][-2.1027548 -2.435503 -2.7838483 -2.8152401 -2.1830468 -0.93223166 0.35576391 1.0669298 0.90707088 0.02766943 -1.0450746 -1.8587372 -2.2476873 -2.2790694 -2.1342502][-1.9329047 -2.1785345 -2.5025551 -2.6971662 -2.498153 -1.8516726 -1.082083 -0.55615151 -0.516124 -0.95674169 -1.5983829 -2.1163077 -2.3291733 -2.2726521 -2.1084678][-1.7891841 -1.9243953 -2.1494691 -2.3728819 -2.4290814 -2.229883 -1.8865131 -1.5888078 -1.4942861 -1.6635468 -1.9607127 -2.2003257 -2.2523816 -2.1378691 -1.9922893][-1.7037013 -1.7454245 -1.8444915 -1.989118 -2.1033821 -2.1171064 -2.0356185 -1.9325744 -1.8878365 -1.9364204 -2.0197585 -2.061198 -2.0022275 -1.8908082 -1.799823][-1.7045958 -1.6854342 -1.6826789 -1.7172134 -1.7812988 -1.8368721 -1.8686182 -1.8758593 -1.8823953 -1.8841561 -1.8604535 -1.8044295 -1.7200629 -1.6528933 -1.6276851]]...]
INFO - root - 2017-12-16 09:24:06.716893: step 33210, loss = 0.54, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:08m:27s remains)
INFO - root - 2017-12-16 09:24:08.417941: step 33220, loss = 0.50, batch loss = 0.24 (46.0 examples/sec; 0.174 sec/batch; 14h:26m:37s remains)
INFO - root - 2017-12-16 09:24:10.119843: step 33230, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:53m:04s remains)
INFO - root - 2017-12-16 09:24:11.812716: step 33240, loss = 0.68, batch loss = 0.43 (47.4 examples/sec; 0.169 sec/batch; 14h:01m:18s remains)
INFO - root - 2017-12-16 09:24:13.508733: step 33250, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:54m:18s remains)
INFO - root - 2017-12-16 09:24:15.196670: step 33260, loss = 0.48, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 13h:58m:55s remains)
INFO - root - 2017-12-16 09:24:16.911170: step 33270, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 13h:52m:06s remains)
INFO - root - 2017-12-16 09:24:18.581171: step 33280, loss = 0.66, batch loss = 0.40 (48.2 examples/sec; 0.166 sec/batch; 13h:46m:59s remains)
INFO - root - 2017-12-16 09:24:20.256859: step 33290, loss = 0.58, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:48m:44s remains)
INFO - root - 2017-12-16 09:24:21.926905: step 33300, loss = 0.53, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:34m:51s remains)
2017-12-16 09:24:22.400284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5635548 -1.7338161 -1.7122766 -1.5595039 -1.4057772 -1.3851745 -1.4572554 -1.598351 -1.5755247 -1.37193 -1.1479286 -1.0411144 -1.0895442 -1.196606 -1.2207621][-1.7541335 -1.8114173 -1.6635162 -1.4561315 -1.3589072 -1.3791454 -1.4650793 -1.7023973 -1.8574381 -1.8243148 -1.6951609 -1.5400459 -1.4729834 -1.4186466 -1.3083656][-1.8960855 -1.9033214 -1.7172861 -1.5123631 -1.4114521 -1.382035 -1.4583378 -1.7859411 -2.103642 -2.2421038 -2.2283552 -2.0847034 -1.9537222 -1.7700821 -1.5690062][-2.0350349 -2.0517797 -1.8448896 -1.5760049 -1.4097735 -1.3001589 -1.2660613 -1.5664703 -1.9856174 -2.2313018 -2.3274565 -2.2851117 -2.1712651 -1.9526094 -1.743171][-2.1377635 -2.1702952 -1.8574008 -1.4480118 -1.0948668 -0.83495247 -0.6734333 -0.90617204 -1.3664526 -1.7258611 -1.95051 -2.0433998 -2.0014172 -1.7915151 -1.6169796][-1.8167534 -1.8384392 -1.4024019 -0.77726686 -0.20900416 0.24802518 0.54468727 0.23770428 -0.45044219 -1.075541 -1.5038509 -1.7070997 -1.6346905 -1.3587277 -1.1940185][-1.1814075 -1.3106972 -0.8484472 -0.04752183 0.78888893 1.4595559 1.9002473 1.473213 0.44634008 -0.55263007 -1.2381666 -1.5141647 -1.3148644 -0.8487879 -0.59201038][-0.906386 -1.2869169 -0.95018375 -0.033400297 1.0588851 1.9528029 2.6186979 2.2664144 1.0823419 -0.12198544 -1.009184 -1.3598434 -1.095048 -0.46481979 -0.12588072][-1.2412722 -1.8809992 -1.7556276 -0.92143881 0.1892283 1.1550856 2.0394552 2.134104 1.2288668 0.12480879 -0.80159783 -1.2056171 -0.99714661 -0.35888433 0.068236828][-1.7182916 -2.4672399 -2.576179 -2.0030174 -1.1599201 -0.39194238 0.40285397 0.8798852 0.61409354 -0.07404232 -0.81672192 -1.1905148 -1.095718 -0.53392136 -0.020441294][-1.8237004 -2.6102998 -3.0064051 -2.8104458 -2.2944639 -1.7770565 -1.1570113 -0.55641341 -0.39780343 -0.69339216 -1.1112742 -1.2903183 -1.1376828 -0.64192045 -0.15377188][-1.453356 -2.2290142 -2.8710086 -3.0182204 -2.7936485 -2.4311614 -1.9283797 -1.383249 -1.1583521 -1.2780659 -1.3993456 -1.3003811 -0.9997896 -0.52200079 -0.15628004][-0.69946074 -1.3482255 -2.0602784 -2.3921828 -2.3659072 -2.2077684 -1.8475564 -1.4507279 -1.3756305 -1.5038046 -1.4352686 -1.1137892 -0.75020206 -0.384331 -0.17662144][-0.13978505 -0.56923175 -1.1423551 -1.4228637 -1.3954616 -1.3763357 -1.1878304 -0.95887518 -1.1134605 -1.3148402 -1.1261789 -0.71712697 -0.50479388 -0.33452964 -0.32760167][-0.13613486 -0.44169068 -0.83238292 -0.88354743 -0.6731205 -0.52568579 -0.32323503 -0.20641398 -0.55489445 -0.80487418 -0.56248975 -0.13167691 -0.090936661 -0.16462278 -0.4227984]]...]
INFO - root - 2017-12-16 09:24:24.104177: step 33310, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.170 sec/batch; 14h:05m:41s remains)
INFO - root - 2017-12-16 09:24:25.776561: step 33320, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:43m:35s remains)
INFO - root - 2017-12-16 09:24:27.468242: step 33330, loss = 0.58, batch loss = 0.33 (47.5 examples/sec; 0.168 sec/batch; 13h:59m:30s remains)
INFO - root - 2017-12-16 09:24:29.178410: step 33340, loss = 0.73, batch loss = 0.47 (46.5 examples/sec; 0.172 sec/batch; 14h:18m:28s remains)
INFO - root - 2017-12-16 09:24:30.872444: step 33350, loss = 0.56, batch loss = 0.30 (48.8 examples/sec; 0.164 sec/batch; 13h:37m:45s remains)
INFO - root - 2017-12-16 09:24:32.580933: step 33360, loss = 0.51, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 14h:09m:00s remains)
INFO - root - 2017-12-16 09:24:34.263573: step 33370, loss = 0.57, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:49m:45s remains)
INFO - root - 2017-12-16 09:24:35.916570: step 33380, loss = 0.66, batch loss = 0.41 (47.1 examples/sec; 0.170 sec/batch; 14h:06m:44s remains)
INFO - root - 2017-12-16 09:24:37.590703: step 33390, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:02m:43s remains)
INFO - root - 2017-12-16 09:24:39.291926: step 33400, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 13h:53m:24s remains)
2017-12-16 09:24:39.769275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.49056315 -0.5832566 -0.917179 -1.4152482 -2.0969436 -2.7928038 -3.2205904 -3.2950587 -3.0510778 -2.7310717 -2.6801195 -2.9111695 -3.0706258 -2.8786333 -2.4217794][-0.22205544 -0.16352558 -0.51104903 -1.17013 -2.0953178 -2.9391117 -3.4649417 -3.4972968 -3.1486647 -2.6909461 -2.5038438 -2.7109473 -2.9988813 -3.0236013 -2.7977295][0.24812865 0.14429355 -0.28563309 -1.0874068 -2.1112843 -2.9770944 -3.5068846 -3.5723553 -3.2343216 -2.7400732 -2.5135543 -2.6810334 -2.993062 -3.1350217 -3.1242058][1.198513 0.66594267 -0.10710216 -1.0933484 -2.1236138 -2.9052811 -3.3349614 -3.3977332 -3.1343246 -2.7861207 -2.6603079 -2.8037691 -3.0583909 -3.1895323 -3.2422943][1.8795383 0.83994365 -0.17792654 -1.1623521 -1.9503855 -2.4455156 -2.6175838 -2.575582 -2.4469256 -2.4266341 -2.5440784 -2.7261796 -2.924916 -2.9648368 -2.9586048][2.3531868 0.74550772 -0.60156441 -1.483144 -1.7651243 -1.5582757 -1.1554244 -0.9322952 -1.035519 -1.4823505 -1.9805915 -2.3099561 -2.4578016 -2.3610635 -2.234303][2.5487688 0.46919823 -1.1178381 -1.9332628 -1.6821213 -0.57381952 0.6608963 1.212394 0.77274132 -0.253309 -1.1781999 -1.6884757 -1.7950262 -1.535804 -1.2846925][2.1150873 -0.048131466 -1.6544218 -2.3349881 -1.7757807 0.028399229 2.2150657 3.2285893 2.3314641 0.7384727 -0.59461713 -1.2974366 -1.3077824 -0.87214243 -0.50088513][0.65068269 -1.2372415 -2.5974557 -3.1134648 -2.3544624 -0.24726486 2.4341223 3.676348 2.849983 1.1430068 -0.35223341 -1.0917808 -1.0257605 -0.47177994 -0.076872587][-1.0767399 -2.5288956 -3.5702338 -3.9161308 -3.0681033 -1.0405296 1.4929969 2.9547966 2.55426 1.0826561 -0.36773658 -1.1294762 -1.0883118 -0.57906449 -0.24212599][-2.1769671 -3.3164282 -4.0778008 -4.2270126 -3.4691682 -1.761076 0.49812484 2.0282047 1.9397237 0.78025055 -0.51177263 -1.3890454 -1.5911194 -1.2409846 -0.82780659][-2.5936177 -3.5865128 -4.1783652 -4.2256713 -3.5911651 -2.2496059 -0.33100176 1.1042314 1.2934277 0.51331425 -0.61335909 -1.6067972 -2.0686953 -1.7435815 -1.1880654][-2.5222869 -3.4056582 -3.9313064 -3.9372973 -3.4649062 -2.4034932 -0.89728916 0.37830639 0.804533 0.45582056 -0.44643164 -1.4599113 -2.0447969 -1.7291367 -1.0428911][-2.0068984 -2.82305 -3.3887589 -3.5038581 -3.2518125 -2.5801971 -1.4942241 -0.33299494 0.35918236 0.41548872 -0.16700077 -1.0886832 -1.7222847 -1.329145 -0.4284786][-1.2841995 -1.9503865 -2.5291271 -2.83367 -2.895611 -2.6388674 -1.9886887 -1.0292954 -0.19200397 0.21406078 -0.054605722 -0.83522391 -1.4172232 -0.85748661 0.26134467]]...]
INFO - root - 2017-12-16 09:24:41.460137: step 33410, loss = 0.59, batch loss = 0.33 (49.1 examples/sec; 0.163 sec/batch; 13h:32m:44s remains)
INFO - root - 2017-12-16 09:24:43.154445: step 33420, loss = 0.50, batch loss = 0.24 (46.3 examples/sec; 0.173 sec/batch; 14h:21m:23s remains)
INFO - root - 2017-12-16 09:24:44.850845: step 33430, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:48m:01s remains)
INFO - root - 2017-12-16 09:24:46.567467: step 33440, loss = 0.50, batch loss = 0.24 (46.9 examples/sec; 0.171 sec/batch; 14h:10m:43s remains)
INFO - root - 2017-12-16 09:24:48.268985: step 33450, loss = 0.45, batch loss = 0.19 (48.8 examples/sec; 0.164 sec/batch; 13h:36m:27s remains)
INFO - root - 2017-12-16 09:24:49.953232: step 33460, loss = 0.61, batch loss = 0.35 (46.0 examples/sec; 0.174 sec/batch; 14h:27m:20s remains)
INFO - root - 2017-12-16 09:24:51.691321: step 33470, loss = 0.58, batch loss = 0.33 (47.2 examples/sec; 0.170 sec/batch; 14h:05m:12s remains)
INFO - root - 2017-12-16 09:24:53.356338: step 33480, loss = 0.51, batch loss = 0.26 (49.2 examples/sec; 0.163 sec/batch; 13h:30m:09s remains)
INFO - root - 2017-12-16 09:24:55.058539: step 33490, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 14h:05m:25s remains)
INFO - root - 2017-12-16 09:24:56.742969: step 33500, loss = 0.47, batch loss = 0.21 (47.6 examples/sec; 0.168 sec/batch; 13h:58m:03s remains)
2017-12-16 09:24:57.215585: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5085318 -2.1250927 -2.4266489 -2.5492568 -2.5111976 -2.1573238 -1.77039 -1.5715138 -1.3059415 -0.96091008 -0.84052753 -0.84912455 -0.71447217 -0.44991302 -0.24982619][-1.088504 -1.9582192 -2.5195115 -2.9093213 -3.0851791 -2.8824322 -2.6491871 -2.5933459 -2.393152 -1.9257488 -1.56968 -1.3114682 -0.94684577 -0.51999032 -0.21316719][-1.2578555 -1.9350107 -2.3657222 -2.7391975 -3.0065694 -2.9851208 -2.9687014 -3.1236358 -3.1322424 -2.7882829 -2.4317369 -2.1231365 -1.6178936 -0.95391858 -0.48069453][-1.5107648 -1.5592499 -1.4294181 -1.5046108 -1.7301621 -1.8647109 -2.0644615 -2.494338 -2.8123331 -2.7681391 -2.6659369 -2.5588102 -2.1809452 -1.5645965 -1.0794606][-1.277564 -0.47701478 0.33257675 0.56583619 0.45257044 0.29495931 -0.037441492 -0.67387247 -1.2373646 -1.4783728 -1.6684587 -1.8818415 -1.8691537 -1.6089946 -1.3850362][-0.86721122 0.51216674 1.8670702 2.4464846 2.5612125 2.6223807 2.4332433 1.7156844 1.0528531 0.74758029 0.42371225 -0.016248465 -0.42422187 -0.75249994 -1.0006298][-1.0012063 0.24026775 1.6255708 2.5819235 3.2013326 3.8839788 4.1640825 3.604063 3.077682 3.0036297 2.847578 2.2338114 1.3736095 0.47365642 -0.28534508][-1.9353395 -1.2498457 -0.38084579 0.3811028 1.3505001 2.682478 3.6168151 3.5135283 3.2643123 3.4968085 3.6207156 3.0765142 2.1610751 1.1128674 0.057399511][-2.9723506 -2.8590736 -2.4983585 -2.0828917 -1.2755193 0.025965214 1.0361145 1.1412222 1.0659063 1.412591 1.7077103 1.518337 1.1130645 0.52962947 -0.30646014][-3.4683051 -3.778399 -3.8501325 -3.7274151 -3.2300973 -2.3297417 -1.6798211 -1.6747134 -1.7180028 -1.3960512 -1.0993119 -0.99058688 -0.922948 -1.0007741 -1.3683503][-3.3749545 -3.8654878 -4.2006254 -4.3156857 -4.1216016 -3.6792798 -3.4189327 -3.5543776 -3.6536951 -3.4852014 -3.2323215 -2.9661546 -2.6293337 -2.4069073 -2.3322184][-2.8143728 -3.2587514 -3.6592484 -3.871078 -3.8961606 -3.7998285 -3.8085713 -3.9995041 -4.1931138 -4.186193 -4.0086513 -3.6794844 -3.1899738 -2.7638788 -2.3930621][-2.1961615 -2.4634819 -2.7247596 -2.8831694 -2.966644 -3.0086284 -3.12111 -3.3743598 -3.6384668 -3.7465148 -3.6612625 -3.3769026 -2.9286227 -2.5106561 -2.0244739][-1.7735187 -1.838218 -1.9058316 -1.9563258 -1.995986 -2.0371182 -2.1339898 -2.3117492 -2.4872763 -2.5853314 -2.5773778 -2.4635477 -2.2548594 -2.0203578 -1.618962][-1.7606857 -1.7659454 -1.7477883 -1.7407521 -1.7450188 -1.7580059 -1.7889595 -1.841836 -1.8872564 -1.912776 -1.9153789 -1.9023777 -1.8762779 -1.8260759 -1.5848248]]...]
INFO - root - 2017-12-16 09:24:58.957190: step 33510, loss = 0.56, batch loss = 0.30 (47.6 examples/sec; 0.168 sec/batch; 13h:57m:37s remains)
INFO - root - 2017-12-16 09:25:00.638220: step 33520, loss = 0.57, batch loss = 0.31 (49.5 examples/sec; 0.162 sec/batch; 13h:25m:17s remains)
INFO - root - 2017-12-16 09:25:02.308506: step 33530, loss = 0.65, batch loss = 0.39 (47.6 examples/sec; 0.168 sec/batch; 13h:57m:42s remains)
INFO - root - 2017-12-16 09:25:03.979483: step 33540, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.169 sec/batch; 14h:04m:32s remains)
INFO - root - 2017-12-16 09:25:05.688200: step 33550, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:46m:08s remains)
INFO - root - 2017-12-16 09:25:07.363285: step 33560, loss = 0.54, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 13h:44m:54s remains)
INFO - root - 2017-12-16 09:25:09.089974: step 33570, loss = 0.53, batch loss = 0.27 (47.1 examples/sec; 0.170 sec/batch; 14h:06m:23s remains)
INFO - root - 2017-12-16 09:25:10.750449: step 33580, loss = 0.52, batch loss = 0.26 (49.8 examples/sec; 0.161 sec/batch; 13h:20m:04s remains)
INFO - root - 2017-12-16 09:25:12.419669: step 33590, loss = 0.52, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 14h:08m:23s remains)
INFO - root - 2017-12-16 09:25:14.100109: step 33600, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:52m:37s remains)
2017-12-16 09:25:14.598896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7669295 -1.7846763 -1.8741226 -1.9466336 -1.9323735 -1.8324944 -1.7372293 -1.8396558 -2.0160499 -2.1123147 -2.114809 -2.0487683 -1.9550791 -1.8689597 -1.8040869][-1.85783 -1.9947824 -2.24306 -2.4024644 -2.3419967 -2.1093791 -1.9979455 -2.2977955 -2.7115836 -2.9353042 -2.9576988 -2.8596883 -2.7645617 -2.686609 -2.5922263][-1.9292208 -2.2072022 -2.6169064 -2.8337576 -2.6410577 -2.1962998 -2.0129068 -2.4909348 -3.1883597 -3.5819788 -3.598392 -3.4497328 -3.36017 -3.284977 -3.175154][-1.9911675 -2.3436334 -2.820914 -2.9498041 -2.5755122 -1.9377689 -1.674649 -2.2827275 -3.215708 -3.7639737 -3.7649636 -3.5360913 -3.4050016 -3.3691783 -3.313][-2.0609922 -2.35704 -2.7021644 -2.6194851 -2.0824759 -1.3386685 -1.0631927 -1.7636104 -2.7712874 -3.322242 -3.3364122 -3.0580158 -2.965035 -3.0437851 -3.122858][-2.1103215 -2.2339385 -2.2648067 -1.8979672 -1.2776536 -0.60401964 -0.39382732 -1.0078624 -1.8052558 -2.2483027 -2.2775569 -2.139442 -2.2037387 -2.5251262 -2.8142362][-2.1392703 -2.0930076 -1.8064203 -1.1846116 -0.53602982 -0.00926733 0.16780686 -0.16935802 -0.60597384 -0.83351362 -0.99519 -1.2244011 -1.5851589 -2.0999317 -2.5591021][-2.1642704 -2.0634818 -1.6723747 -0.93054485 -0.18547392 0.34421468 0.56140327 0.45032692 0.29224539 0.10713744 -0.35850978 -0.95185876 -1.4604026 -1.9756672 -2.4167795][-2.2569234 -2.2345886 -1.9011018 -1.1407692 -0.3244102 0.28121996 0.58095932 0.57368636 0.40918922 -0.0061900616 -0.70057905 -1.3843632 -1.7816045 -2.1313543 -2.4085636][-2.4000697 -2.4424276 -2.1761477 -1.4622673 -0.58076513 0.16764331 0.50100827 0.40534067 0.026794672 -0.64871192 -1.3453999 -1.8166913 -1.955698 -2.0033379 -2.1116931][-2.5583508 -2.6090298 -2.3249755 -1.5799341 -0.59787476 0.23318982 0.54782319 0.35839677 -0.17998123 -1.0005357 -1.6689835 -1.8981261 -1.7020425 -1.5084516 -1.5268798][-2.7080343 -2.7691905 -2.4598572 -1.7026393 -0.71814251 0.067330837 0.35508323 0.23363805 -0.21776438 -1.0180247 -1.6282551 -1.6872771 -1.2698045 -0.88060355 -0.84390283][-2.8016353 -2.9533255 -2.7082808 -2.0913839 -1.3207966 -0.71468258 -0.45656502 -0.31560946 -0.43749607 -1.0244366 -1.5269642 -1.5167918 -1.0388012 -0.57046461 -0.41466045][-2.7715392 -3.0484037 -3.0186887 -2.7202404 -2.2958553 -1.8631666 -1.5084903 -1.079638 -0.88253367 -1.2582551 -1.6826754 -1.6930211 -1.2499652 -0.74477303 -0.42043328][-2.5963204 -2.9569716 -3.1463037 -3.1638026 -3.0020792 -2.6583579 -2.2213521 -1.7279677 -1.5229325 -1.8168552 -2.1360166 -2.1567707 -1.7928848 -1.3045452 -0.87063468]]...]
INFO - root - 2017-12-16 09:25:16.281766: step 33610, loss = 0.57, batch loss = 0.31 (47.5 examples/sec; 0.168 sec/batch; 13h:58m:12s remains)
INFO - root - 2017-12-16 09:25:17.984873: step 33620, loss = 0.55, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:39m:47s remains)
INFO - root - 2017-12-16 09:25:19.669894: step 33630, loss = 0.74, batch loss = 0.48 (45.7 examples/sec; 0.175 sec/batch; 14h:32m:16s remains)
INFO - root - 2017-12-16 09:25:21.349700: step 33640, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 13h:55m:21s remains)
INFO - root - 2017-12-16 09:25:23.042058: step 33650, loss = 0.57, batch loss = 0.31 (49.2 examples/sec; 0.163 sec/batch; 13h:30m:39s remains)
INFO - root - 2017-12-16 09:25:24.733538: step 33660, loss = 0.62, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 13h:54m:30s remains)
INFO - root - 2017-12-16 09:25:26.401933: step 33670, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 13h:41m:46s remains)
INFO - root - 2017-12-16 09:25:28.103187: step 33680, loss = 0.53, batch loss = 0.27 (47.0 examples/sec; 0.170 sec/batch; 14h:07m:02s remains)
INFO - root - 2017-12-16 09:25:29.792947: step 33690, loss = 0.60, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 14h:00m:32s remains)
INFO - root - 2017-12-16 09:25:31.505165: step 33700, loss = 0.58, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:44m:55s remains)
2017-12-16 09:25:31.987141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.95393205 -0.81838644 -1.160169 -1.5263958 -1.7396152 -1.9041967 -2.0515363 -2.0644372 -1.8512993 -1.4445632 -1.0595806 -0.64584768 -0.11743689 0.058808565 -0.35183072][-2.2595558 -2.2155187 -2.5045049 -2.7476869 -2.887259 -3.0411518 -3.2289615 -3.3239098 -3.1898084 -2.7906277 -2.3550491 -1.8338997 -1.1815259 -0.92401922 -1.2811358][-3.5611529 -3.6006699 -3.730464 -3.7036724 -3.6211281 -3.6243281 -3.760757 -3.9464002 -4.0249395 -3.845789 -3.5532241 -3.1067398 -2.4711194 -2.2632346 -2.5786052][-4.3447838 -4.3965111 -4.2563968 -3.8098178 -3.3178885 -3.0204012 -2.9475226 -3.1635351 -3.5647449 -3.8366454 -3.9183784 -3.7650938 -3.4203916 -3.3977418 -3.7465925][-4.2613478 -4.1939011 -3.709002 -2.8235686 -1.8729351 -1.1340697 -0.73722422 -0.93478644 -1.6324877 -2.4512722 -3.0629225 -3.3841879 -3.4581866 -3.6726513 -4.1506281][-3.3229871 -2.998673 -2.1957321 -1.0149792 0.2449894 1.3365963 2.1007342 1.9933157 1.0328901 -0.2426827 -1.265857 -2.0057149 -2.4966023 -2.9620252 -3.55366][-1.9063313 -1.3417478 -0.46179223 0.6799624 1.9227424 3.1246557 4.2546611 4.3951735 3.2014403 1.6182718 0.36787629 -0.59739625 -1.3284531 -1.8998613 -2.4882011][-0.855453 -0.18516707 0.56624913 1.3341098 2.2114029 3.3255715 4.7044897 5.1885304 3.9555988 2.3176494 1.1668384 0.23598051 -0.49091649 -1.0164036 -1.5129577][-0.37532234 0.19115162 0.59093714 0.90406823 1.4024541 2.3064399 3.6065631 4.1766753 3.2560863 1.952457 1.0785754 0.3920424 -0.059553862 -0.30672288 -0.64716351][-0.18738508 0.13092327 0.043504238 -0.12197065 0.0074949265 0.61876178 1.6232772 2.1157479 1.5501764 0.7028203 0.11742163 -0.30160451 -0.30849695 -0.07264781 -0.11293507][0.020213842 0.028630495 -0.50333524 -1.1244419 -1.3086891 -1.0168333 -0.38417292 -0.0370059 -0.30096817 -0.72815013 -1.0011299 -1.0967318 -0.64659095 0.066277027 0.32380342][0.40643454 0.30934811 -0.48765814 -1.4123912 -1.8114576 -1.7258844 -1.3872921 -1.1594807 -1.2225378 -1.3852046 -1.4546471 -1.2737182 -0.49512625 0.50306559 0.88303924][0.72910261 0.6888082 -0.093492508 -1.0500134 -1.4713447 -1.4786878 -1.3111953 -1.1529725 -1.1190336 -1.1671343 -1.1558909 -0.83823931 0.10713673 1.1324522 1.4551842][0.71586847 0.77913976 0.16522169 -0.56860793 -0.85575664 -0.80541635 -0.67019773 -0.550586 -0.50241518 -0.53496289 -0.53645849 -0.20342493 0.67246819 1.4780114 1.5648575][-0.15089369 0.031068325 -0.37409639 -0.8492564 -0.9834007 -0.89180636 -0.78014207 -0.7196089 -0.71676528 -0.77075911 -0.7968601 -0.5416733 0.12785101 0.65864468 0.59456849]]...]
INFO - root - 2017-12-16 09:25:33.701192: step 33710, loss = 0.60, batch loss = 0.34 (47.0 examples/sec; 0.170 sec/batch; 14h:07m:51s remains)
INFO - root - 2017-12-16 09:25:35.417972: step 33720, loss = 0.51, batch loss = 0.25 (45.4 examples/sec; 0.176 sec/batch; 14h:37m:00s remains)
INFO - root - 2017-12-16 09:25:37.078715: step 33730, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 13h:45m:22s remains)
INFO - root - 2017-12-16 09:25:38.760304: step 33740, loss = 0.55, batch loss = 0.29 (46.1 examples/sec; 0.174 sec/batch; 14h:24m:56s remains)
INFO - root - 2017-12-16 09:25:40.439101: step 33750, loss = 0.54, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 13h:52m:44s remains)
INFO - root - 2017-12-16 09:25:42.120103: step 33760, loss = 0.50, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 13h:57m:39s remains)
INFO - root - 2017-12-16 09:25:43.806671: step 33770, loss = 0.67, batch loss = 0.41 (47.5 examples/sec; 0.169 sec/batch; 13h:59m:18s remains)
INFO - root - 2017-12-16 09:25:45.490019: step 33780, loss = 0.46, batch loss = 0.20 (48.1 examples/sec; 0.166 sec/batch; 13h:47m:32s remains)
INFO - root - 2017-12-16 09:25:47.131641: step 33790, loss = 0.69, batch loss = 0.43 (49.0 examples/sec; 0.163 sec/batch; 13h:32m:42s remains)
INFO - root - 2017-12-16 09:25:48.813743: step 33800, loss = 0.47, batch loss = 0.21 (46.5 examples/sec; 0.172 sec/batch; 14h:17m:20s remains)
2017-12-16 09:25:49.309660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3451536 -2.1228721 -2.0039532 -2.0316393 -2.1090674 -2.2466073 -2.3526576 -2.4469228 -2.5187225 -2.3899465 -2.1251993 -1.7928151 -1.5749192 -1.5484676 -1.6731052][-2.4976366 -2.2599709 -2.2047637 -2.4154654 -2.7261858 -3.0996346 -3.3650103 -3.4980693 -3.4637156 -3.0882232 -2.5309024 -1.9880391 -1.6375535 -1.4574134 -1.4469063][-2.4361942 -2.1332743 -2.1010847 -2.4510179 -3.0277846 -3.6345272 -4.0439224 -4.2594385 -4.1310282 -3.5866389 -2.9292595 -2.368134 -1.9864867 -1.6370301 -1.4250174][-2.0394595 -1.5783615 -1.4876783 -1.8857031 -2.6163425 -3.3536084 -3.7897797 -3.9622474 -3.7914877 -3.3368425 -2.8781147 -2.5001273 -2.1543527 -1.6634063 -1.2971834][-1.2729923 -0.67597497 -0.595616 -1.0123065 -1.7653764 -2.371531 -2.5221276 -2.505074 -2.3915377 -2.2786062 -2.1716561 -2.1201904 -1.8604069 -1.302709 -0.95191431][-0.30392766 0.43573189 0.5770061 0.2501061 -0.31579089 -0.56802714 -0.26741672 0.055175543 0.025117397 -0.3147943 -0.65094221 -1.0449125 -1.0202804 -0.62570667 -0.44051492][0.26519918 1.1940184 1.5456886 1.5043011 1.3009548 1.5473025 2.3743787 2.9886022 2.8420405 1.9877973 1.2493982 0.46499634 0.11598468 0.10128927 -0.028084993][-0.058755159 0.90140295 1.4233239 1.6262126 1.8199124 2.5124416 3.7263312 4.3948922 4.1464596 3.0762391 2.123796 1.2203245 0.56175303 0.14995527 -0.17486835][-1.2153655 -0.41139305 0.1616168 0.48644543 0.90398026 1.6993003 2.7244163 3.2239895 2.9454622 2.0921979 1.2962761 0.50962949 -0.2102952 -0.66290343 -0.89893746][-2.4488037 -1.8763679 -1.4910388 -1.2751634 -0.99399209 -0.52922416 -0.0039100647 0.19081855 0.041452169 -0.35049868 -0.81736517 -1.3533916 -1.8314396 -1.9991113 -1.9476867][-3.4911218 -3.1135864 -2.9587598 -2.94177 -2.945874 -2.8584626 -2.7183547 -2.7144151 -2.7649884 -2.9135849 -3.160032 -3.4445539 -3.5816357 -3.3947892 -3.0427608][-4.0292606 -3.8238153 -3.785274 -3.8667471 -4.0451384 -4.16033 -4.1711712 -4.2281103 -4.2372317 -4.2551808 -4.36716 -4.49492 -4.4310007 -4.0573363 -3.5993063][-3.7906263 -3.7119367 -3.7105088 -3.7756619 -3.9636359 -4.1359096 -4.1822457 -4.1994486 -4.1846566 -4.109314 -4.081244 -4.0658092 -3.9808116 -3.7191424 -3.4020634][-3.0910249 -3.0424201 -3.0062039 -3.0044336 -3.1529949 -3.3516998 -3.4077225 -3.3963046 -3.3339257 -3.2144828 -3.0820198 -3.0181479 -3.0054126 -2.9553227 -2.842622][-2.5307889 -2.4745002 -2.4008083 -2.3562889 -2.4385567 -2.597832 -2.6438861 -2.6303113 -2.5314615 -2.370398 -2.2168736 -2.1664445 -2.2311709 -2.3444829 -2.3643794]]...]
INFO - root - 2017-12-16 09:25:50.998642: step 33810, loss = 0.64, batch loss = 0.38 (48.0 examples/sec; 0.167 sec/batch; 13h:49m:46s remains)
INFO - root - 2017-12-16 09:25:52.679379: step 33820, loss = 0.53, batch loss = 0.27 (46.2 examples/sec; 0.173 sec/batch; 14h:22m:31s remains)
INFO - root - 2017-12-16 09:25:54.377508: step 33830, loss = 0.65, batch loss = 0.39 (47.4 examples/sec; 0.169 sec/batch; 14h:00m:53s remains)
INFO - root - 2017-12-16 09:25:56.051416: step 33840, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 13h:55m:29s remains)
INFO - root - 2017-12-16 09:25:57.732028: step 33850, loss = 0.63, batch loss = 0.37 (46.7 examples/sec; 0.171 sec/batch; 14h:12m:00s remains)
INFO - root - 2017-12-16 09:25:59.406693: step 33860, loss = 0.51, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 13h:36m:21s remains)
INFO - root - 2017-12-16 09:26:01.114567: step 33870, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:47m:13s remains)
INFO - root - 2017-12-16 09:26:02.807632: step 33880, loss = 0.64, batch loss = 0.38 (46.4 examples/sec; 0.173 sec/batch; 14h:18m:33s remains)
INFO - root - 2017-12-16 09:26:04.485245: step 33890, loss = 0.49, batch loss = 0.24 (48.0 examples/sec; 0.167 sec/batch; 13h:49m:07s remains)
INFO - root - 2017-12-16 09:26:06.189549: step 33900, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 13h:56m:22s remains)
2017-12-16 09:26:06.665342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4237776 -2.561734 -2.7028394 -2.8765888 -3.0197012 -3.086817 -3.1278958 -3.1797705 -3.2477915 -3.2704983 -3.1170187 -2.6628745 -2.1569743 -1.9397008 -1.9259719][-2.6246967 -2.9105394 -3.1474497 -3.3524239 -3.5307937 -3.6431575 -3.5833559 -3.3785787 -3.1498504 -3.0458624 -2.840142 -2.2649732 -1.7634033 -1.6918056 -1.6980524][-2.8383656 -3.2908208 -3.6166468 -3.8281641 -3.9704614 -3.9900904 -3.7032385 -3.1523819 -2.6969984 -2.511591 -2.2386217 -1.5383811 -1.0940095 -1.1848673 -1.2251][-3.0417221 -3.6055253 -3.9224844 -4.0164223 -3.9638071 -3.77768 -3.1890457 -2.3048661 -1.725909 -1.6508517 -1.4538976 -0.82473373 -0.51718307 -0.70680952 -0.73315406][-2.9339867 -3.4692292 -3.5875568 -3.3921099 -3.0993958 -2.6726477 -1.7746375 -0.66852927 -0.21272039 -0.5265677 -0.64833319 -0.26603436 -0.12488151 -0.30855942 -0.275697][-2.3107917 -2.693718 -2.5094161 -1.9846436 -1.4666791 -0.88009036 0.2730546 1.58757 1.7064271 0.74577785 0.11314082 0.14521575 0.075276136 -0.083754063 0.010298491][-1.4501684 -1.6551459 -1.2136852 -0.44749951 0.22556663 0.93312335 2.35823 3.9329319 3.7765718 2.1528306 0.96666527 0.54857135 0.1623075 -0.13672924 -0.14225364][-0.85973608 -0.874215 -0.23495865 0.65402865 1.3299851 2.0607867 3.7035623 5.550241 5.18945 3.1150522 1.5572987 0.76668239 0.073889017 -0.35176921 -0.48075581][-0.81595516 -0.69462693 -0.069860935 0.72566891 1.2590053 1.8015871 3.1529818 4.5386343 4.1275568 2.4870782 1.1588142 0.32926178 -0.33637547 -0.74537075 -0.89795327][-1.119886 -1.0833127 -0.68458807 -0.13750267 0.1947186 0.47217274 1.2887464 2.1003909 1.7867932 0.70146608 -0.21159792 -0.83073938 -1.2943647 -1.5558777 -1.6252649][-1.5257661 -1.5902332 -1.5021684 -1.2218843 -1.0353985 -0.92160666 -0.61003852 -0.27091527 -0.56209207 -1.2531178 -1.798945 -2.1895719 -2.4463987 -2.5314074 -2.4523933][-1.7757251 -1.9056815 -2.0136364 -1.9214016 -1.8545485 -1.9237905 -1.9423974 -1.9639832 -2.2368209 -2.6378605 -2.8996711 -3.0655422 -3.1451194 -3.0724142 -2.8913827][-1.8619945 -2.002769 -2.2053928 -2.2571361 -2.2734475 -2.4562497 -2.6740701 -2.8752439 -3.1315379 -3.3477955 -3.4340758 -3.4182043 -3.3347676 -3.1593206 -2.9354105][-1.8783727 -1.9760252 -2.1716373 -2.2776873 -2.4014442 -2.6594541 -2.9424412 -3.1801138 -3.3629627 -3.4132338 -3.3759351 -3.2850194 -3.1395729 -2.969269 -2.8015606][-1.9979687 -2.0333247 -2.1563959 -2.2704997 -2.4073005 -2.6236365 -2.8480482 -3.0145173 -3.0699689 -3.0185292 -2.9274783 -2.8314893 -2.7185526 -2.6241889 -2.5608137]]...]
INFO - root - 2017-12-16 09:26:08.341073: step 33910, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:46m:53s remains)
INFO - root - 2017-12-16 09:26:10.043505: step 33920, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.163 sec/batch; 13h:33m:27s remains)
INFO - root - 2017-12-16 09:26:11.723964: step 33930, loss = 0.67, batch loss = 0.41 (46.5 examples/sec; 0.172 sec/batch; 14h:15m:32s remains)
INFO - root - 2017-12-16 09:26:13.406903: step 33940, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 13h:33m:05s remains)
INFO - root - 2017-12-16 09:26:15.081639: step 33950, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:43m:58s remains)
INFO - root - 2017-12-16 09:26:16.798421: step 33960, loss = 0.58, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 13h:39m:06s remains)
INFO - root - 2017-12-16 09:26:18.475708: step 33970, loss = 0.49, batch loss = 0.23 (47.6 examples/sec; 0.168 sec/batch; 13h:56m:18s remains)
INFO - root - 2017-12-16 09:26:20.175281: step 33980, loss = 0.57, batch loss = 0.31 (47.5 examples/sec; 0.168 sec/batch; 13h:58m:05s remains)
INFO - root - 2017-12-16 09:26:21.869640: step 33990, loss = 0.65, batch loss = 0.40 (48.2 examples/sec; 0.166 sec/batch; 13h:46m:24s remains)
INFO - root - 2017-12-16 09:26:23.571730: step 34000, loss = 0.60, batch loss = 0.34 (46.9 examples/sec; 0.170 sec/batch; 14h:07m:53s remains)
2017-12-16 09:26:24.050095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3126884 -2.4181089 -2.4822865 -2.5116715 -2.5345085 -2.567687 -2.5985737 -2.6100726 -2.6068981 -2.6086426 -2.6127105 -2.6099708 -2.6220479 -2.6315017 -2.6381242][-2.4671812 -2.6525853 -2.7874334 -2.8713727 -2.945878 -3.03796 -3.120033 -3.1639893 -3.1770556 -3.1895916 -3.1967218 -3.1863971 -3.1839006 -3.1617141 -3.1231997][-2.5533493 -2.7587821 -2.9000695 -2.9792061 -3.0581622 -3.1717591 -3.2847998 -3.3611619 -3.4039969 -3.4529185 -3.4848266 -3.4893248 -3.4875031 -3.4512959 -3.3892379][-2.5819366 -2.7081337 -2.7392201 -2.6987007 -2.6570694 -2.6853938 -2.7479143 -2.8067644 -2.8599973 -2.945399 -3.0188031 -3.063344 -3.0952404 -3.0834317 -3.0561748][-2.5012362 -2.4838336 -2.3432109 -2.1157746 -1.8979131 -1.7859236 -1.7646213 -1.8012748 -1.8738766 -2.0018067 -2.1286793 -2.2337508 -2.3215263 -2.3577569 -2.4098873][-2.2665749 -2.0544116 -1.6915352 -1.2370256 -0.81124067 -0.53370571 -0.39665186 -0.39748585 -0.50132465 -0.69160938 -0.89001954 -1.0610528 -1.2135522 -1.3120135 -1.4419212][-1.9932427 -1.5396702 -0.944028 -0.24670768 0.3932538 0.86731863 1.1271927 1.1396165 0.9693265 0.69148278 0.437047 0.23343587 0.059745312 -0.032847881 -0.18425298][-1.763339 -1.1295085 -0.3960073 0.40726447 1.1683013 1.7726071 2.1033905 2.0831225 1.8504922 1.5354033 1.2918148 1.1191804 1.0033085 1.0275419 0.94815183][-1.6482905 -0.93092108 -0.2031014 0.53822446 1.2723022 1.8653638 2.1588213 2.1039441 1.8933513 1.6601379 1.501204 1.4218559 1.4394555 1.6359255 1.6342046][-1.6328795 -0.88009524 -0.21751142 0.42064881 1.0869691 1.6366093 1.8464406 1.685385 1.4606338 1.3166671 1.2693129 1.3176186 1.5233836 1.8922527 1.9076359][-1.7278516 -0.98795676 -0.36563325 0.20823622 0.83210731 1.332242 1.4886508 1.2656779 1.0175457 0.90439272 0.89778066 0.97575092 1.2577615 1.6843612 1.6369717][-2.0131416 -1.3755926 -0.84747159 -0.38383663 0.12097812 0.52011752 0.61424541 0.39662576 0.19083428 0.10363364 0.10409141 0.17240548 0.42314982 0.80715108 0.74631619][-2.4196057 -1.9583732 -1.5428288 -1.1722683 -0.79607666 -0.52786076 -0.52526379 -0.72114503 -0.86219943 -0.9161191 -0.88884604 -0.79308736 -0.56409311 -0.23753881 -0.24872231][-2.7750587 -2.5326252 -2.2434707 -1.962729 -1.7159274 -1.5819323 -1.6546198 -1.8092124 -1.9047065 -1.9366975 -1.9123204 -1.8301182 -1.6655205 -1.4160913 -1.360849][-2.9992194 -2.9424887 -2.7975357 -2.6203928 -2.464916 -2.388037 -2.4467731 -2.5370593 -2.5827036 -2.6028283 -2.597821 -2.5587676 -2.4857488 -2.3330522 -2.2551298]]...]
INFO - root - 2017-12-16 09:26:25.761878: step 34010, loss = 0.47, batch loss = 0.21 (47.4 examples/sec; 0.169 sec/batch; 14h:00m:25s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:26:27.443929: step 34020, loss = 0.57, batch loss = 0.31 (49.8 examples/sec; 0.161 sec/batch; 13h:18m:50s remains)
INFO - root - 2017-12-16 09:26:29.137484: step 34030, loss = 0.54, batch loss = 0.28 (43.4 examples/sec; 0.184 sec/batch; 15h:16m:53s remains)
INFO - root - 2017-12-16 09:26:30.825588: step 34040, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 14h:01m:05s remains)
INFO - root - 2017-12-16 09:26:32.501231: step 34050, loss = 0.55, batch loss = 0.29 (45.8 examples/sec; 0.174 sec/batch; 14h:27m:58s remains)
INFO - root - 2017-12-16 09:26:34.167051: step 34060, loss = 0.59, batch loss = 0.33 (47.1 examples/sec; 0.170 sec/batch; 14h:04m:31s remains)
INFO - root - 2017-12-16 09:26:35.845915: step 34070, loss = 0.49, batch loss = 0.23 (44.3 examples/sec; 0.180 sec/batch; 14h:57m:39s remains)
INFO - root - 2017-12-16 09:26:37.554718: step 34080, loss = 0.48, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 13h:47m:08s remains)
INFO - root - 2017-12-16 09:26:39.270948: step 34090, loss = 0.63, batch loss = 0.37 (47.5 examples/sec; 0.168 sec/batch; 13h:56m:57s remains)
INFO - root - 2017-12-16 09:26:40.960271: step 34100, loss = 0.54, batch loss = 0.28 (45.6 examples/sec; 0.176 sec/batch; 14h:32m:57s remains)
2017-12-16 09:26:41.447906: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48175073 0.57864833 0.64132786 0.59066129 0.40550232 0.16268444 -0.079201937 -0.26549006 -0.32745624 0.14638853 1.2465124 2.1422062 2.161942 1.485491 0.25632834][-0.24746847 -0.061146259 0.12354565 0.15084004 0.010134935 -0.21877313 -0.48534667 -0.70066595 -0.79262519 -0.29345965 0.90981841 1.9601955 2.1975565 1.7302055 0.63613129][-1.9049095 -1.7365716 -1.4826922 -1.3461387 -1.3844924 -1.5149047 -1.7134688 -1.9330519 -2.0765491 -1.6946199 -0.6309135 0.37859702 0.79924965 0.65979862 -0.0924499][-3.4738264 -3.5023742 -3.3085446 -3.1241879 -3.0147729 -2.9432516 -2.9862907 -3.1641307 -3.355612 -3.1817055 -2.4604301 -1.7086158 -1.2835255 -1.2245526 -1.6128542][-4.2448497 -4.526659 -4.4381566 -4.2106724 -3.8891678 -3.4819231 -3.2282295 -3.276372 -3.4945059 -3.5764637 -3.3378935 -3.0821509 -2.9850984 -2.980722 -3.1611795][-4.0485849 -4.54385 -4.4915237 -4.1482482 -3.5432997 -2.7171676 -2.10106 -1.9486026 -2.1954708 -2.5691659 -2.8825502 -3.2501469 -3.676367 -3.9888964 -4.1862679][-3.3607383 -3.8498547 -3.6188898 -3.0070219 -2.1235223 -1.0236011 -0.066589355 0.35110283 0.069185734 -0.62894905 -1.46757 -2.4253831 -3.3967943 -4.1599746 -4.5022392][-2.7561834 -3.06458 -2.5676811 -1.5912786 -0.42035842 0.89129448 2.0965705 2.7183437 2.3411026 1.3208187 0.12056708 -1.2296817 -2.609993 -3.7064121 -4.2237148][-2.5731182 -2.67255 -2.0472188 -0.95262671 0.22958159 1.5210569 2.7508278 3.3755064 2.9413996 1.8861675 0.67663217 -0.6851809 -2.144366 -3.2642255 -3.7750926][-2.8520579 -2.8190598 -2.2533474 -1.3007601 -0.32435298 0.74345875 1.7958794 2.2866769 1.807404 0.92624855 -0.0322392 -1.0808796 -2.2393484 -3.1022735 -3.4514258][-3.5373597 -3.4400654 -3.0253968 -2.3908722 -1.7410825 -0.99450791 -0.18903565 0.14988589 -0.22296381 -0.80495322 -1.3414505 -1.9057155 -2.5955379 -3.1545508 -3.374526][-4.1055284 -3.9857588 -3.7110262 -3.4178681 -3.1490366 -2.7656016 -2.3043411 -2.1167486 -2.2983425 -2.5133944 -2.6140804 -2.7219691 -2.9906027 -3.3412716 -3.4945414][-4.1217332 -3.9623661 -3.8208652 -3.7922904 -3.8133483 -3.735944 -3.5880308 -3.5280843 -3.5484014 -3.5169013 -3.3145537 -3.0914919 -3.0796943 -3.3205924 -3.4683847][-3.689683 -3.5163989 -3.4117224 -3.4322212 -3.4784923 -3.4767184 -3.494451 -3.527307 -3.5080686 -3.3865135 -3.0227036 -2.6032619 -2.4594505 -2.7293637 -3.0342288][-3.0948722 -2.8985791 -2.7067485 -2.5747752 -2.4259393 -2.3325047 -2.37821 -2.4796538 -2.4917617 -2.3294389 -1.8607085 -1.3731664 -1.2784404 -1.7639651 -2.4077232]]...]
INFO - root - 2017-12-16 09:26:43.180930: step 34110, loss = 0.52, batch loss = 0.27 (47.4 examples/sec; 0.169 sec/batch; 13h:59m:18s remains)
INFO - root - 2017-12-16 09:26:44.860416: step 34120, loss = 0.47, batch loss = 0.21 (47.1 examples/sec; 0.170 sec/batch; 14h:05m:31s remains)
INFO - root - 2017-12-16 09:26:46.557295: step 34130, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 13h:50m:29s remains)
INFO - root - 2017-12-16 09:26:48.236901: step 34140, loss = 0.66, batch loss = 0.40 (50.1 examples/sec; 0.160 sec/batch; 13h:13m:22s remains)
INFO - root - 2017-12-16 09:26:49.938138: step 34150, loss = 0.49, batch loss = 0.23 (47.5 examples/sec; 0.168 sec/batch; 13h:57m:17s remains)
INFO - root - 2017-12-16 09:26:51.623349: step 34160, loss = 0.55, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 14h:01m:44s remains)
INFO - root - 2017-12-16 09:26:53.333830: step 34170, loss = 0.70, batch loss = 0.44 (47.9 examples/sec; 0.167 sec/batch; 13h:49m:34s remains)
INFO - root - 2017-12-16 09:26:55.015437: step 34180, loss = 0.72, batch loss = 0.46 (47.5 examples/sec; 0.169 sec/batch; 13h:58m:09s remains)
INFO - root - 2017-12-16 09:26:56.682662: step 34190, loss = 0.60, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 13h:38m:59s remains)
INFO - root - 2017-12-16 09:26:58.367355: step 34200, loss = 0.61, batch loss = 0.35 (48.8 examples/sec; 0.164 sec/batch; 13h:35m:09s remains)
2017-12-16 09:26:58.822349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8157508 -1.5566491 -1.3364073 -1.0437222 -0.90069282 -1.1634715 -1.7611752 -2.1846061 -1.9953622 -1.38712 -0.578899 0.13722682 0.30379677 -0.41330898 -1.5905994][-1.5761831 -1.494225 -1.4605522 -1.354942 -1.2466387 -1.4555001 -2.0580645 -2.601619 -2.4891195 -1.819649 -1.057912 -0.46049607 -0.25819063 -0.8379761 -1.893092][-1.4157881 -1.5476894 -1.746295 -1.74309 -1.5423231 -1.5798504 -2.1029108 -2.6355469 -2.5869808 -2.0632491 -1.4107513 -0.935146 -0.73458719 -1.2222496 -2.0971088][-1.3731252 -1.5969307 -1.8052769 -1.8258245 -1.5446473 -1.4020026 -1.7845839 -2.1790068 -2.2549477 -2.0652704 -1.6827358 -1.2793206 -1.0515573 -1.3386736 -2.0137293][-1.4064226 -1.532027 -1.6767032 -1.6302444 -1.2332926 -0.92913973 -1.0754777 -1.413649 -1.7384188 -2.0404871 -1.981735 -1.68413 -1.398448 -1.386167 -1.7955644][-1.3272333 -1.2433057 -1.1657301 -0.97633016 -0.47886252 -0.013953209 0.029615641 -0.44608641 -1.2073287 -2.0423949 -2.348068 -2.1319168 -1.7207992 -1.4172015 -1.5859129][-1.1845033 -0.79763329 -0.48338187 -0.16031671 0.4181211 1.069062 1.2961316 0.61128473 -0.70082927 -1.9388127 -2.5592735 -2.4901941 -2.0339422 -1.4989847 -1.4248316][-1.3294311 -0.70158744 -0.18496609 0.3110106 0.94100952 1.7145488 1.9960539 1.1722825 -0.39154625 -1.8430469 -2.6813147 -2.8218515 -2.3792131 -1.6238397 -1.258605][-1.8536165 -1.2361386 -0.58688486 0.040681839 0.67439175 1.2949808 1.3648591 0.57374406 -0.78303981 -2.069803 -2.8774815 -3.128027 -2.7369626 -1.8348436 -1.2315937][-2.6386778 -2.0838923 -1.3864164 -0.75778067 -0.22961903 0.11869264 0.027205944 -0.65859962 -1.7212346 -2.6739731 -3.3670008 -3.5783708 -3.0912929 -2.0490968 -1.291779][-3.46069 -2.980866 -2.4090424 -1.9066746 -1.5176618 -1.3754549 -1.5285752 -2.0476089 -2.7671156 -3.3336389 -3.7756062 -3.8815403 -3.3375704 -2.3276947 -1.5862654][-4.0338964 -3.6687284 -3.2841024 -2.9646153 -2.7517247 -2.7434151 -2.8959334 -3.3313231 -3.7444072 -3.8953161 -3.9873567 -4.0093813 -3.5077252 -2.6835375 -2.0931718][-4.2043276 -3.9790149 -3.7483265 -3.6339703 -3.5316644 -3.6226802 -3.8118584 -4.2291322 -4.4789343 -4.3233147 -4.1137433 -4.0014353 -3.6426024 -3.128082 -2.6767654][-3.9733758 -3.8657231 -3.79052 -3.7773805 -3.8080711 -3.9363308 -4.140337 -4.5071325 -4.7032647 -4.5401669 -4.236773 -4.0365796 -3.852633 -3.5406079 -3.1429219][-3.2279158 -3.2058129 -3.2415624 -3.258461 -3.3750784 -3.5439179 -3.8049741 -4.1408238 -4.3264904 -4.28553 -4.0488939 -3.863498 -3.8305559 -3.668138 -3.3548889]]...]
INFO - root - 2017-12-16 09:27:00.524164: step 34210, loss = 0.51, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:43m:40s remains)
INFO - root - 2017-12-16 09:27:02.192739: step 34220, loss = 0.66, batch loss = 0.40 (48.6 examples/sec; 0.165 sec/batch; 13h:38m:08s remains)
INFO - root - 2017-12-16 09:27:03.875535: step 34230, loss = 0.63, batch loss = 0.38 (47.4 examples/sec; 0.169 sec/batch; 13h:59m:27s remains)
INFO - root - 2017-12-16 09:27:05.571311: step 34240, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 13h:54m:59s remains)
INFO - root - 2017-12-16 09:27:07.251587: step 34250, loss = 0.67, batch loss = 0.41 (47.9 examples/sec; 0.167 sec/batch; 13h:50m:32s remains)
INFO - root - 2017-12-16 09:27:08.928330: step 34260, loss = 0.61, batch loss = 0.35 (45.3 examples/sec; 0.177 sec/batch; 14h:37m:59s remains)
INFO - root - 2017-12-16 09:27:10.598966: step 34270, loss = 0.54, batch loss = 0.28 (45.9 examples/sec; 0.174 sec/batch; 14h:25m:59s remains)
INFO - root - 2017-12-16 09:27:12.297128: step 34280, loss = 0.62, batch loss = 0.37 (44.8 examples/sec; 0.178 sec/batch; 14h:46m:54s remains)
INFO - root - 2017-12-16 09:27:13.964282: step 34290, loss = 0.75, batch loss = 0.49 (47.8 examples/sec; 0.167 sec/batch; 13h:51m:22s remains)
INFO - root - 2017-12-16 09:27:15.651259: step 34300, loss = 0.56, batch loss = 0.31 (47.0 examples/sec; 0.170 sec/batch; 14h:05m:17s remains)
2017-12-16 09:27:16.131247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4550564 -1.5748506 -1.8545675 -2.2451377 -2.6520932 -2.9865565 -3.1910264 -3.2738509 -3.2589855 -3.2174642 -3.1172242 -2.8928831 -2.5494671 -2.2716341 -2.1301768][-1.793869 -2.1016812 -2.5872035 -3.1534724 -3.6401343 -3.9678588 -4.1265211 -4.1752019 -4.1218424 -4.060029 -3.9830141 -3.7466044 -3.3079686 -2.8797567 -2.5975831][-2.3324838 -2.7932844 -3.3375912 -3.8471766 -4.2121544 -4.3334322 -4.2775793 -4.2364173 -4.2280517 -4.3118439 -4.4115391 -4.3459315 -3.97608 -3.5095773 -3.1645389][-2.8417413 -3.3180857 -3.717243 -3.9985013 -4.00661 -3.6454451 -3.193532 -2.983767 -3.0878787 -3.4172788 -3.8513353 -4.1216369 -4.0771937 -3.7812109 -3.5030155][-3.1660764 -3.5243518 -3.6412158 -3.5346727 -3.0165472 -1.9935776 -0.95846677 -0.50637496 -0.72654021 -1.4614689 -2.2858295 -2.9753327 -3.4304721 -3.4573243 -3.3501585][-3.1754763 -3.2415462 -2.9103804 -2.2840707 -1.1419023 0.61908364 2.2059431 2.8803525 2.3535967 1.1295648 -0.11269379 -1.239319 -2.1051614 -2.6048248 -2.8404694][-2.9097462 -2.7227519 -1.9624383 -0.85570633 0.79647517 3.1042295 5.0923243 6.0661545 5.0854645 3.2935696 1.7243161 0.28957725 -0.88809788 -1.8185637 -2.40722][-2.6737835 -2.3735242 -1.4595478 -0.16084456 1.6892443 4.0817432 6.0961237 7.0645847 5.6924171 3.770112 2.4180059 1.1233468 -0.21542525 -1.4615691 -2.298815][-2.6784463 -2.4556735 -1.7194382 -0.58544326 0.99386454 2.940414 4.3670626 4.607502 3.4260511 2.1983314 1.5162389 0.71070623 -0.47912264 -1.759896 -2.6090763][-2.8618269 -2.9168303 -2.6020772 -1.8393075 -0.74836469 0.47051072 1.2286506 1.0585 0.24707341 -0.24702096 -0.339535 -0.72285175 -1.6204703 -2.6951284 -3.2888691][-3.003921 -3.3943338 -3.5361655 -3.2816732 -2.7550893 -2.2035208 -2.0160363 -2.2834013 -2.707449 -2.7329071 -2.4576182 -2.5195847 -3.05157 -3.7161899 -3.9400485][-2.8266816 -3.3745198 -3.7973666 -3.9516406 -3.9051452 -3.8530812 -4.0148044 -4.303473 -4.4455762 -4.217227 -3.8217978 -3.7211003 -3.9276972 -4.1847439 -4.0569258][-2.4021327 -2.8931172 -3.3394217 -3.6475358 -3.8398049 -4.0230761 -4.2833786 -4.538353 -4.6004353 -4.3647914 -4.07633 -3.9445562 -3.957118 -3.916055 -3.6146636][-1.973224 -2.2779064 -2.6025987 -2.8604479 -3.0726559 -3.2914157 -3.5054829 -3.6667557 -3.7031503 -3.6104097 -3.488019 -3.4073076 -3.344873 -3.2119756 -2.9072063][-1.6842978 -1.8141103 -2.0016091 -2.1764517 -2.3152664 -2.452584 -2.5598021 -2.6365581 -2.6484177 -2.6167674 -2.5802398 -2.5420759 -2.4808052 -2.3637631 -2.1620164]]...]
INFO - root - 2017-12-16 09:27:17.789532: step 34310, loss = 0.67, batch loss = 0.41 (48.0 examples/sec; 0.167 sec/batch; 13h:47m:45s remains)
INFO - root - 2017-12-16 09:27:19.469040: step 34320, loss = 0.50, batch loss = 0.24 (49.1 examples/sec; 0.163 sec/batch; 13h:29m:53s remains)
INFO - root - 2017-12-16 09:27:21.106609: step 34330, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:39m:32s remains)
INFO - root - 2017-12-16 09:27:22.785971: step 34340, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:34m:49s remains)
INFO - root - 2017-12-16 09:27:24.467753: step 34350, loss = 0.51, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 13h:53m:24s remains)
INFO - root - 2017-12-16 09:27:26.162580: step 34360, loss = 0.48, batch loss = 0.23 (46.7 examples/sec; 0.171 sec/batch; 14h:11m:03s remains)
INFO - root - 2017-12-16 09:27:27.874240: step 34370, loss = 0.58, batch loss = 0.32 (46.8 examples/sec; 0.171 sec/batch; 14h:09m:09s remains)
INFO - root - 2017-12-16 09:27:29.585189: step 34380, loss = 0.55, batch loss = 0.29 (45.1 examples/sec; 0.177 sec/batch; 14h:40m:25s remains)
INFO - root - 2017-12-16 09:27:31.266658: step 34390, loss = 0.58, batch loss = 0.32 (46.0 examples/sec; 0.174 sec/batch; 14h:24m:49s remains)
INFO - root - 2017-12-16 09:27:32.934130: step 34400, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 13h:33m:10s remains)
2017-12-16 09:27:33.421650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8004037 -1.6810555 -1.6522863 -1.4865108 -1.0545764 -0.41413558 0.05579567 0.034091949 -0.30046558 -0.68550813 -0.95470989 -1.118737 -1.290836 -1.4160991 -1.5210152][-0.70017672 -0.71563971 -0.9497894 -1.1125917 -1.0957468 -0.89154971 -0.729537 -0.91694832 -1.2972636 -1.6299763 -1.835412 -1.9491279 -2.1156757 -2.296237 -2.4237425][0.34520292 0.043359756 -0.52061486 -1.0121295 -1.2692623 -1.2877738 -1.3037428 -1.5808609 -2.0130291 -2.4004202 -2.6468942 -2.7415407 -2.845855 -2.9387445 -2.9234476][1.2099211 0.62405658 -0.18611145 -0.78051746 -1.0414232 -1.0158015 -1.0374174 -1.4016789 -1.9863977 -2.5663528 -2.9515581 -3.1088734 -3.1719353 -3.1468215 -2.9891868][1.7699599 1.1152639 0.31345725 -0.1219511 -0.046236038 0.24334478 0.23628044 -0.33987355 -1.2300327 -2.0693314 -2.618402 -2.8782427 -2.9824893 -2.9572711 -2.7889686][1.9129868 1.5269403 1.0609751 1.0442848 1.6131449 2.2546239 2.2064371 1.2576659 -0.057356358 -1.1749828 -1.8425436 -2.1727021 -2.3713236 -2.4544415 -2.4235966][1.5349343 1.5895305 1.6203594 2.1151361 3.1245737 4.0296507 3.8996339 2.5440674 0.91370821 -0.33738303 -1.014963 -1.3381046 -1.5800054 -1.7905705 -1.8857168][0.47928476 0.85289454 1.2445071 2.036561 3.1797423 4.0675097 3.9176979 2.6017342 1.1174233 0.047664642 -0.50482929 -0.75414383 -0.9096384 -1.0580459 -1.0972311][-0.92625892 -0.53505838 -0.067639828 0.68587375 1.6121278 2.2409744 2.1347513 1.3329148 0.40431023 -0.21736407 -0.50554919 -0.587685 -0.612172 -0.6073097 -0.4647634][-2.1079278 -1.8853388 -1.5447627 -1.0411886 -0.44805336 -0.047343254 -0.0387578 -0.35534787 -0.69701183 -0.86340129 -0.8726989 -0.81481624 -0.74582005 -0.64274013 -0.42314363][-2.6651027 -2.6403232 -2.5012634 -2.2639954 -1.9844124 -1.7780151 -1.6982527 -1.6994076 -1.6709175 -1.5432477 -1.3904483 -1.2785627 -1.1979125 -1.1220548 -0.95165646][-2.57498 -2.6698616 -2.6961374 -2.6742675 -2.6261625 -2.5503149 -2.4603064 -2.3567266 -2.1997924 -2.0065949 -1.8287277 -1.739274 -1.7094423 -1.6915004 -1.6096277][-2.0636888 -2.1632967 -2.2752349 -2.3915915 -2.4724352 -2.4784889 -2.4207168 -2.3329272 -2.22261 -2.1049266 -2.0022733 -1.9733368 -2.0185447 -2.0588262 -2.0478542][-1.3645774 -1.4064503 -1.5429621 -1.7270453 -1.8864086 -1.9613013 -1.9625113 -1.9244769 -1.8632038 -1.8107936 -1.797865 -1.8493447 -1.9495149 -2.0430694 -2.0792465][-0.78720856 -0.74843717 -0.87096465 -1.079641 -1.2976954 -1.4460926 -1.4995236 -1.4657917 -1.3690733 -1.2805107 -1.2730477 -1.3704451 -1.5290264 -1.6719801 -1.7504014]]...]
INFO - root - 2017-12-16 09:27:35.089128: step 34410, loss = 0.52, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 13h:24m:35s remains)
INFO - root - 2017-12-16 09:27:36.764189: step 34420, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:50m:59s remains)
INFO - root - 2017-12-16 09:27:38.454346: step 34430, loss = 0.62, batch loss = 0.36 (46.6 examples/sec; 0.172 sec/batch; 14h:13m:32s remains)
INFO - root - 2017-12-16 09:27:40.154133: step 34440, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 13h:37m:41s remains)
INFO - root - 2017-12-16 09:27:41.821909: step 34450, loss = 0.50, batch loss = 0.24 (49.4 examples/sec; 0.162 sec/batch; 13h:24m:23s remains)
INFO - root - 2017-12-16 09:27:43.510268: step 34460, loss = 0.73, batch loss = 0.47 (48.4 examples/sec; 0.165 sec/batch; 13h:40m:35s remains)
INFO - root - 2017-12-16 09:27:45.195411: step 34470, loss = 0.45, batch loss = 0.20 (46.9 examples/sec; 0.171 sec/batch; 14h:07m:17s remains)
INFO - root - 2017-12-16 09:27:46.871537: step 34480, loss = 0.55, batch loss = 0.29 (48.9 examples/sec; 0.164 sec/batch; 13h:33m:08s remains)
INFO - root - 2017-12-16 09:27:48.531027: step 34490, loss = 0.53, batch loss = 0.27 (48.7 examples/sec; 0.164 sec/batch; 13h:35m:39s remains)
INFO - root - 2017-12-16 09:27:50.192321: step 34500, loss = 0.49, batch loss = 0.23 (48.7 examples/sec; 0.164 sec/batch; 13h:35m:55s remains)
2017-12-16 09:27:50.670277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3349888 -4.1319838 -4.512506 -4.3779507 -3.795167 -2.9666007 -2.3355155 -2.5535321 -3.4150887 -3.7656446 -3.4023557 -2.9839096 -2.8203468 -2.8477678 -2.8464508][-3.7333655 -4.5362763 -4.833807 -4.564971 -3.7926378 -2.6771586 -1.7729471 -2.0194247 -3.174962 -3.7591872 -3.3392749 -2.8357213 -2.687464 -2.7397156 -2.6713777][-3.8972108 -4.5893054 -4.8162327 -4.4371958 -3.5093379 -2.1167107 -0.99584305 -1.3804489 -2.9132671 -3.7833197 -3.3794916 -2.8431239 -2.6874697 -2.7583759 -2.5980132][-3.7780657 -4.2298684 -4.259769 -3.6373916 -2.5780797 -1.0224617 0.16781068 -0.4721787 -2.4000249 -3.6688669 -3.421442 -2.944104 -2.8147945 -2.8592932 -2.647203][-3.5248413 -3.6460953 -3.3489833 -2.3513091 -1.0032446 0.65555906 1.8154197 0.97622514 -1.3685614 -3.1008871 -3.2580948 -3.0040054 -2.9687948 -3.0105243 -2.8246281][-3.4152341 -3.3097529 -2.6616979 -1.2587858 0.44770741 2.2764239 3.4490252 2.6057682 0.13167953 -1.9618735 -2.7009997 -2.8623614 -3.0368953 -3.1613865 -3.0867627][-3.4350646 -3.2493026 -2.4554858 -0.91593456 0.9635129 2.8483152 4.0872936 3.5386558 1.3592486 -0.82341266 -1.9835377 -2.5509245 -2.9145906 -3.1693227 -3.295445][-3.406673 -3.345186 -2.5910633 -1.1072673 0.64534068 2.386786 3.64746 3.4897089 1.9139929 0.05952096 -1.1909728 -1.9532782 -2.4539382 -2.8253756 -3.2156844][-3.2535622 -3.4239492 -2.8438401 -1.5398476 0.0046281815 1.5204039 2.6697884 2.8297191 1.9357457 0.68281317 -0.33985138 -1.1083546 -1.6682634 -2.1594858 -2.7984667][-3.116071 -3.4455547 -2.9638836 -1.7838322 -0.4063623 0.91640949 1.8268175 2.0675178 1.6980577 1.1057527 0.42817545 -0.25337529 -0.85771191 -1.4801848 -2.3044567][-3.0632498 -3.407618 -2.9764037 -1.9069664 -0.7193408 0.33816338 1.0531833 1.3535066 1.3894119 1.28128 0.89997792 0.33020496 -0.270056 -1.0169657 -1.9747189][-3.0485494 -3.1671815 -2.7152905 -1.8746393 -1.0006152 -0.274992 0.14622545 0.466249 0.83657861 1.0759573 0.82422495 0.27258158 -0.29427123 -1.0002337 -1.9035625][-2.8141572 -2.6624963 -2.0985258 -1.4316647 -0.91554761 -0.66221189 -0.65802717 -0.53423667 -0.070076942 0.32299685 0.19286299 -0.29183578 -0.75241065 -1.2237574 -1.8811371][-2.457135 -2.1482902 -1.5483681 -0.98539448 -0.74659157 -0.89419019 -1.2843641 -1.4062309 -1.0417075 -0.6533618 -0.7138499 -1.1004561 -1.3636584 -1.5015575 -1.7829925][-2.0476446 -1.7014749 -1.2338233 -0.85419214 -0.7437427 -0.99000955 -1.5223522 -1.8325806 -1.7051101 -1.5459056 -1.6852949 -1.9776905 -1.9910965 -1.7829149 -1.648716]]...]
INFO - root - 2017-12-16 09:27:52.325965: step 34510, loss = 0.50, batch loss = 0.24 (46.9 examples/sec; 0.170 sec/batch; 14h:06m:32s remains)
INFO - root - 2017-12-16 09:27:54.000136: step 34520, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 13h:45m:38s remains)
INFO - root - 2017-12-16 09:27:55.671431: step 34530, loss = 0.54, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 14h:08m:48s remains)
INFO - root - 2017-12-16 09:27:57.351711: step 34540, loss = 0.64, batch loss = 0.38 (47.6 examples/sec; 0.168 sec/batch; 13h:55m:09s remains)
INFO - root - 2017-12-16 09:27:59.017886: step 34550, loss = 0.45, batch loss = 0.19 (49.4 examples/sec; 0.162 sec/batch; 13h:24m:06s remains)
INFO - root - 2017-12-16 09:28:00.668582: step 34560, loss = 0.62, batch loss = 0.36 (49.3 examples/sec; 0.162 sec/batch; 13h:26m:30s remains)
INFO - root - 2017-12-16 09:28:02.309228: step 34570, loss = 0.55, batch loss = 0.29 (45.9 examples/sec; 0.174 sec/batch; 14h:25m:59s remains)
INFO - root - 2017-12-16 09:28:04.006689: step 34580, loss = 0.59, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 13h:53m:02s remains)
INFO - root - 2017-12-16 09:28:05.713234: step 34590, loss = 0.53, batch loss = 0.28 (46.6 examples/sec; 0.172 sec/batch; 14h:12m:57s remains)
INFO - root - 2017-12-16 09:28:07.450438: step 34600, loss = 0.48, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:45m:49s remains)
2017-12-16 09:28:08.016889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6052482 -2.8547375 -2.8767347 -2.7977512 -2.6746571 -2.4597073 -2.2095535 -2.0033505 -1.8773425 -1.824986 -1.8972734 -1.9335929 -1.872736 -1.7419807 -1.5919924][-2.2336314 -2.3851533 -2.4556649 -2.4899483 -2.4879162 -2.423044 -2.2331603 -1.9184059 -1.6130152 -1.4992356 -1.5925279 -1.6936893 -1.7288344 -1.7734966 -1.8304226][-1.4867985 -1.5966142 -1.7224129 -1.846554 -1.9032526 -1.9381133 -1.8765643 -1.6173052 -1.3081043 -1.1744148 -1.2538846 -1.3908298 -1.5118036 -1.6799778 -1.8808568][-0.44584215 -0.5756309 -0.76149046 -0.95820451 -1.0273794 -1.0746863 -1.1702214 -1.1687063 -1.0600868 -0.95095587 -0.96815062 -1.0771823 -1.2400403 -1.4283643 -1.5794337][0.4842484 0.28783536 0.044241428 -0.11293817 -0.0668087 -0.012669802 -0.17311549 -0.42891276 -0.58761632 -0.61842632 -0.616385 -0.714355 -0.87867689 -1.009145 -0.98850107][0.813149 0.56087685 0.38873911 0.44947433 0.78195 1.0519278 0.94798994 0.54660392 0.10272789 -0.16754556 -0.34634781 -0.505067 -0.65205979 -0.72775495 -0.63609564][0.59698868 0.41227984 0.44130135 0.86671329 1.5396678 2.1032681 2.1905937 1.6784968 0.99119496 0.40848708 -0.037095308 -0.33969426 -0.56939244 -0.725736 -0.73228383][0.12456632 0.23006582 0.59618688 1.3842604 2.368053 3.2361536 3.5482759 2.86092 1.8807869 0.99835634 0.29616785 -0.19715977 -0.60726976 -0.99314797 -1.1985681][-0.79574239 -0.33670926 0.42837739 1.5917728 2.8354778 3.8672943 4.2154479 3.3443389 2.1670027 1.0817845 0.2337935 -0.32325268 -0.88235044 -1.4555748 -1.8399646][-1.7934699 -1.1672505 -0.26057196 1.0380712 2.2831931 3.1015339 3.1353664 2.4030929 1.4522965 0.54519844 -0.2390554 -0.83255613 -1.3529589 -1.9235227 -2.3170676][-2.2197766 -1.6172407 -0.80062938 0.24714637 1.1398113 1.5500791 1.3114257 0.74853635 0.19339371 -0.31628895 -0.84710741 -1.3391016 -1.7665406 -2.2119577 -2.472188][-2.048265 -1.5284359 -0.88834488 -0.21349096 0.14366102 0.076492548 -0.32907748 -0.72044015 -0.95238245 -1.1019244 -1.3042991 -1.5775545 -1.8328562 -2.0878782 -2.2362444][-1.6798217 -1.2704103 -0.84529364 -0.59094715 -0.67144382 -1.0833904 -1.5317564 -1.7773613 -1.7679778 -1.6514802 -1.5627062 -1.5399084 -1.5676601 -1.6745646 -1.7893121][-1.3050109 -1.0918832 -0.94273794 -1.0344565 -1.4283898 -2.0713811 -2.5340171 -2.6164744 -2.3916414 -2.0611937 -1.7151819 -1.434144 -1.2655714 -1.2554753 -1.3176849][-0.92084229 -0.81588674 -0.85637867 -1.1470222 -1.7065067 -2.4332557 -2.900744 -2.8753626 -2.5010812 -2.0419583 -1.5872586 -1.2083598 -0.98824751 -0.94243264 -0.97327971]]...]
INFO - root - 2017-12-16 09:28:09.730254: step 34610, loss = 0.59, batch loss = 0.33 (48.1 examples/sec; 0.166 sec/batch; 13h:45m:06s remains)
INFO - root - 2017-12-16 09:28:11.402170: step 34620, loss = 0.67, batch loss = 0.41 (47.2 examples/sec; 0.169 sec/batch; 14h:01m:19s remains)
INFO - root - 2017-12-16 09:28:13.075816: step 34630, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:51m:46s remains)
INFO - root - 2017-12-16 09:28:14.768674: step 34640, loss = 0.70, batch loss = 0.44 (43.9 examples/sec; 0.182 sec/batch; 15h:05m:40s remains)
INFO - root - 2017-12-16 09:28:16.454513: step 34650, loss = 0.59, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 13h:53m:55s remains)
INFO - root - 2017-12-16 09:28:18.158335: step 34660, loss = 0.52, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 13h:25m:55s remains)
INFO - root - 2017-12-16 09:28:19.842552: step 34670, loss = 0.53, batch loss = 0.27 (45.3 examples/sec; 0.176 sec/batch; 14h:35m:49s remains)
INFO - root - 2017-12-16 09:28:21.520601: step 34680, loss = 0.46, batch loss = 0.20 (46.7 examples/sec; 0.171 sec/batch; 14h:09m:57s remains)
INFO - root - 2017-12-16 09:28:23.187255: step 34690, loss = 0.68, batch loss = 0.42 (47.8 examples/sec; 0.167 sec/batch; 13h:50m:48s remains)
INFO - root - 2017-12-16 09:28:24.844105: step 34700, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 13h:52m:19s remains)
2017-12-16 09:28:25.314690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5135899 -3.3255432 -2.5674462 -1.2279075 -0.39321232 -0.57773781 -1.0084091 -0.94285667 -1.0034968 -1.8583254 -3.0697823 -3.8545642 -3.7992501 -3.1328907 -2.4066195][-3.9161112 -3.9693632 -3.4855831 -2.5308466 -1.9433882 -2.0074296 -2.0876789 -1.656477 -1.4303914 -2.053499 -3.1316357 -3.8756895 -3.8655052 -3.2439928 -2.4999328][-4.3362536 -4.4839444 -4.2244182 -3.5766902 -3.0996757 -2.9996266 -2.6814198 -1.9144406 -1.4278141 -1.8368354 -2.7652049 -3.514976 -3.6211843 -3.1111197 -2.4577329][-4.7500396 -4.89585 -4.694766 -4.0865612 -3.4775264 -3.0432725 -2.3969047 -1.5101101 -1.0299796 -1.3401555 -2.1313689 -2.9193368 -3.2021079 -2.8914 -2.3547981][-4.9154015 -5.0009403 -4.7340512 -3.9182432 -2.8134952 -1.8246081 -0.90570331 -0.11605263 0.023361206 -0.56682456 -1.4817178 -2.4116867 -2.8781166 -2.7283142 -2.2784905][-4.6016574 -4.5221291 -3.9979599 -2.7527497 -0.83018422 0.94197917 2.0467935 2.3233094 1.5367408 0.096547842 -1.3602419 -2.4422259 -2.8870549 -2.7116365 -2.223727][-4.1559839 -3.8182321 -2.8829858 -1.1425744 1.5777891 4.1960144 5.4416189 4.8770213 2.7207031 0.033769608 -2.0150034 -3.059989 -3.2389078 -2.818337 -2.1781533][-3.8881748 -3.4002314 -2.2276189 -0.27246594 2.8543572 6.1440787 7.347116 6.1143022 2.9374895 -0.61573541 -3.0421758 -3.9442034 -3.7484326 -2.998179 -2.1584854][-3.7755642 -3.3408148 -2.2156453 -0.57882035 1.9065866 4.6121635 6.0579 5.1830916 1.9811354 -1.7289972 -4.1131287 -4.7228765 -4.1367674 -3.0844727 -2.081104][-3.7715514 -3.5366452 -2.5789313 -1.3398811 0.31969476 2.3156805 3.7943306 3.3860807 0.69069481 -2.6336446 -4.694561 -5.0566978 -4.260932 -3.0499535 -1.9790856][-3.9412167 -3.8973374 -3.1354966 -2.2445152 -1.2020429 0.15337992 1.5276246 1.6347728 -0.25308084 -2.8741586 -4.562438 -4.8351259 -4.0749354 -2.918189 -1.8905977][-3.9558249 -4.1440544 -3.69599 -3.1537218 -2.5100431 -1.5461707 -0.27374744 0.24823904 -0.85849833 -2.7858183 -4.1535854 -4.4095354 -3.7945271 -2.7852755 -1.8533875][-3.9230022 -4.25518 -4.0451231 -3.6962836 -3.2325513 -2.4179218 -1.2675914 -0.57673872 -1.1804574 -2.5903025 -3.7105594 -3.97241 -3.474962 -2.6063123 -1.8122798][-3.8808227 -4.1868253 -4.0266275 -3.6698749 -3.1762214 -2.449508 -1.4832079 -0.82991707 -1.2055091 -2.3125622 -3.2540026 -3.5072682 -3.0839238 -2.3419328 -1.689335][-3.6375873 -3.8041832 -3.5550482 -3.1178865 -2.5978434 -1.9869083 -1.2124043 -0.691347 -1.0423241 -2.0149198 -2.8355567 -3.0618503 -2.6794336 -2.0152211 -1.5002334]]...]
INFO - root - 2017-12-16 09:28:27.021698: step 34710, loss = 0.66, batch loss = 0.40 (46.4 examples/sec; 0.172 sec/batch; 14h:14m:57s remains)
INFO - root - 2017-12-16 09:28:28.700742: step 34720, loss = 0.49, batch loss = 0.23 (47.0 examples/sec; 0.170 sec/batch; 14h:04m:40s remains)
INFO - root - 2017-12-16 09:28:30.387930: step 34730, loss = 0.55, batch loss = 0.29 (45.8 examples/sec; 0.175 sec/batch; 14h:26m:27s remains)
INFO - root - 2017-12-16 09:28:32.074858: step 34740, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 13h:47m:11s remains)
INFO - root - 2017-12-16 09:28:33.728262: step 34750, loss = 0.65, batch loss = 0.39 (49.0 examples/sec; 0.163 sec/batch; 13h:30m:44s remains)
INFO - root - 2017-12-16 09:28:35.392639: step 34760, loss = 0.68, batch loss = 0.42 (47.4 examples/sec; 0.169 sec/batch; 13h:56m:41s remains)
INFO - root - 2017-12-16 09:28:37.077751: step 34770, loss = 0.59, batch loss = 0.33 (48.2 examples/sec; 0.166 sec/batch; 13h:42m:52s remains)
INFO - root - 2017-12-16 09:28:38.754566: step 34780, loss = 0.56, batch loss = 0.30 (45.7 examples/sec; 0.175 sec/batch; 14h:28m:13s remains)
INFO - root - 2017-12-16 09:28:40.437498: step 34790, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 13h:32m:49s remains)
INFO - root - 2017-12-16 09:28:42.114087: step 34800, loss = 0.47, batch loss = 0.21 (47.9 examples/sec; 0.167 sec/batch; 13h:48m:34s remains)
2017-12-16 09:28:42.587311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6202749 -1.6626853 -1.7143283 -1.7711253 -1.8309939 -1.8938972 -1.9318385 -1.8399713 -1.6013585 -1.4535503 -1.5173672 -1.6902475 -1.8605361 -2.0299411 -2.1856532][-1.7188122 -1.7808436 -1.8511617 -1.9290993 -2.0184152 -2.1119087 -2.1548338 -2.0151968 -1.6855342 -1.4507751 -1.4248066 -1.4896393 -1.5815428 -1.7478513 -1.9524814][-2.351923 -2.383194 -2.4284174 -2.4841537 -2.5549181 -2.6254592 -2.6125126 -2.3734515 -1.9411043 -1.6012709 -1.4656341 -1.4164342 -1.4257886 -1.58932 -1.8309071][-3.2891769 -3.2267706 -3.1694939 -3.1380494 -3.1322322 -3.0991998 -2.9324188 -2.5156095 -1.952348 -1.5510533 -1.3735739 -1.2910564 -1.3039086 -1.5217855 -1.821161][-3.787626 -3.5821743 -3.366616 -3.226917 -3.161953 -3.035121 -2.7320545 -2.1419239 -1.4960301 -1.100342 -0.93224311 -0.871814 -0.99347448 -1.3741685 -1.7909967][-3.5325484 -3.1766193 -2.8306935 -2.6637912 -2.6306455 -2.4625533 -2.0192556 -1.2844492 -0.61025929 -0.29889131 -0.22655344 -0.22441649 -0.51523769 -1.1333653 -1.7077292][-2.6548231 -2.2580798 -1.9382663 -1.8624792 -1.8669224 -1.647047 -1.1165243 -0.31105757 0.39808846 0.6431427 0.57059479 0.38520622 -0.12379146 -0.94323504 -1.6367524][-1.4883254 -1.2031772 -1.0573969 -1.1528205 -1.2314039 -1.0820878 -0.57159972 0.27416563 0.97752452 1.1121881 0.96551657 0.69343758 0.017348289 -0.92115688 -1.6437969][-0.30874586 -0.17838836 -0.2344017 -0.541909 -0.81230462 -0.90064788 -0.60032475 0.12729049 0.69124866 0.67849827 0.53657579 0.28105831 -0.40619588 -1.3006217 -1.8971535][0.65238595 0.67625856 0.46919489 -0.092463732 -0.64319623 -1.0886003 -1.0676105 -0.55633628 -0.19213128 -0.29785872 -0.50580859 -0.81690812 -1.3831902 -2.0321343 -2.3630092][0.93783689 1.0669677 0.78741956 0.084632874 -0.62750411 -1.284072 -1.4731224 -1.1967554 -1.0369111 -1.2098637 -1.4799225 -1.8211597 -2.2948756 -2.7157426 -2.8193314][0.57249117 0.88685679 0.66967344 0.074908018 -0.47579432 -1.1311251 -1.4731351 -1.3898524 -1.3644075 -1.5580701 -1.8392458 -2.1979606 -2.6450162 -2.9585922 -2.9465926][0.16719007 0.7072897 0.51865482 0.072403431 -0.19939017 -0.70804214 -1.1430603 -1.1868021 -1.1853715 -1.3149264 -1.5650868 -1.9790477 -2.4510441 -2.7413771 -2.7277722][-0.045711994 0.57312942 0.35523009 0.033244133 0.056218863 -0.24173141 -0.72978806 -0.89563525 -0.85181367 -0.92780972 -1.1646993 -1.5782229 -2.0016265 -2.2858543 -2.3236501][0.094022989 0.59406233 0.35434294 0.1042695 0.24064088 0.063797 -0.45262122 -0.69689488 -0.58894897 -0.59097636 -0.79015422 -1.1685797 -1.5470632 -1.8070172 -1.9047463]]...]
INFO - root - 2017-12-16 09:28:44.255762: step 34810, loss = 0.52, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 14h:08m:03s remains)
INFO - root - 2017-12-16 09:28:45.940546: step 34820, loss = 0.48, batch loss = 0.23 (47.1 examples/sec; 0.170 sec/batch; 14h:02m:35s remains)
INFO - root - 2017-12-16 09:28:47.596975: step 34830, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 13h:29m:14s remains)
INFO - root - 2017-12-16 09:28:49.288381: step 34840, loss = 0.52, batch loss = 0.26 (46.0 examples/sec; 0.174 sec/batch; 14h:23m:08s remains)
INFO - root - 2017-12-16 09:28:50.988471: step 34850, loss = 0.53, batch loss = 0.27 (49.3 examples/sec; 0.162 sec/batch; 13h:25m:39s remains)
INFO - root - 2017-12-16 09:28:52.739444: step 34860, loss = 0.55, batch loss = 0.29 (46.4 examples/sec; 0.172 sec/batch; 14h:15m:11s remains)
INFO - root - 2017-12-16 09:28:54.427698: step 34870, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 13h:38m:49s remains)
INFO - root - 2017-12-16 09:28:56.102645: step 34880, loss = 0.58, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 13h:52m:12s remains)
INFO - root - 2017-12-16 09:28:57.770627: step 34890, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 13h:41m:33s remains)
INFO - root - 2017-12-16 09:28:59.469123: step 34900, loss = 0.57, batch loss = 0.32 (46.5 examples/sec; 0.172 sec/batch; 14h:12m:34s remains)
2017-12-16 09:28:59.949309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7333238 -2.7143991 -2.7394838 -2.8113136 -2.8795242 -2.93908 -2.9261756 -2.8213842 -2.6450891 -2.2906845 -1.6052418 -0.586081 0.53251147 1.3726931 1.6387181][-2.8036711 -3.0027952 -3.2464619 -3.4683223 -3.5679862 -3.4753304 -3.2016742 -2.890389 -2.6422625 -2.4599042 -2.119036 -1.4331083 -0.48120856 0.42141128 0.95058274][-2.8263731 -3.2154114 -3.5968199 -3.869029 -3.922008 -3.6410613 -3.1417892 -2.6748435 -2.4731114 -2.563168 -2.6532156 -2.4237 -1.7662518 -0.95059824 -0.25128579][-2.8541007 -3.3258 -3.7376189 -3.9593015 -3.8836622 -3.3931694 -2.6599264 -2.0768428 -1.9201756 -2.2934289 -2.8629215 -3.123883 -2.8589983 -2.2735882 -1.5300694][-2.8827703 -3.3211412 -3.6256111 -3.7140179 -3.4927483 -2.7549052 -1.7151636 -0.90427136 -0.77320337 -1.4348601 -2.4742744 -3.1955009 -3.3853045 -3.133903 -2.4738572][-2.8672235 -3.1980844 -3.346102 -3.3070967 -2.8694105 -1.794579 -0.37012935 0.76509643 0.92049122 -0.004121542 -1.5233682 -2.7161949 -3.2997694 -3.3484521 -2.8124545][-2.8181143 -3.0410802 -3.1034577 -2.9221454 -2.1953797 -0.7283597 1.1721818 2.6013923 2.7063203 1.4742775 -0.56044579 -2.1674037 -2.9483163 -3.0940714 -2.6613619][-2.7715311 -2.9356136 -2.9711614 -2.6729717 -1.6370363 0.28380251 2.5973592 4.1055145 4.0058627 2.3559747 -0.10855532 -1.9606082 -2.781749 -2.8407788 -2.3717227][-2.7552125 -2.9413047 -3.0014815 -2.5974507 -1.3100318 0.95870805 3.4324646 4.7601748 4.296783 2.2618937 -0.3894006 -2.268754 -2.9726157 -2.8575995 -2.2805703][-2.7982981 -3.0416431 -3.16748 -2.7640102 -1.4530106 0.82920361 3.136385 4.1231031 3.3095412 1.1314359 -1.3282803 -2.9923182 -3.4532766 -3.1537755 -2.4786344][-2.867578 -3.1740696 -3.3263876 -2.9977269 -1.8948929 -0.058619976 1.7263331 2.3804035 1.5023382 -0.42169726 -2.4391434 -3.7376804 -3.9773121 -3.5702715 -2.8260314][-2.9268177 -3.2750874 -3.432888 -3.1857433 -2.4483311 -1.2612696 -0.13938665 0.19755387 -0.53507757 -1.9364352 -3.3536241 -4.16854 -4.2180672 -3.7802668 -3.0562758][-2.9198534 -3.2786553 -3.4366684 -3.291826 -2.9258106 -2.3032281 -1.7751603 -1.728549 -2.2653651 -3.1317825 -3.9159527 -4.2346163 -4.0913777 -3.6644192 -3.0540175][-2.834718 -3.1730294 -3.3624802 -3.3528216 -3.2521431 -3.0412042 -2.8644938 -2.9668813 -3.3157949 -3.7535129 -4.0128989 -3.9727013 -3.6944809 -3.2910371 -2.77842][-2.7042327 -2.9981637 -3.21548 -3.324717 -3.4023566 -3.4297 -3.4283848 -3.5172665 -3.6465955 -3.73065 -3.6879973 -3.4874821 -3.1759238 -2.7939949 -2.3272789]]...]
INFO - root - 2017-12-16 09:29:01.692333: step 34910, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 14h:00m:09s remains)
INFO - root - 2017-12-16 09:29:03.396179: step 34920, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 13h:57m:52s remains)
INFO - root - 2017-12-16 09:29:05.097979: step 34930, loss = 0.58, batch loss = 0.32 (48.3 examples/sec; 0.165 sec/batch; 13h:40m:40s remains)
INFO - root - 2017-12-16 09:29:06.766453: step 34940, loss = 0.55, batch loss = 0.29 (49.3 examples/sec; 0.162 sec/batch; 13h:24m:19s remains)
INFO - root - 2017-12-16 09:29:08.440605: step 34950, loss = 0.63, batch loss = 0.37 (47.4 examples/sec; 0.169 sec/batch; 13h:57m:36s remains)
INFO - root - 2017-12-16 09:29:10.118474: step 34960, loss = 0.50, batch loss = 0.25 (47.8 examples/sec; 0.167 sec/batch; 13h:49m:20s remains)
INFO - root - 2017-12-16 09:29:11.809239: step 34970, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 13h:49m:07s remains)
INFO - root - 2017-12-16 09:29:13.481977: step 34980, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 13h:40m:43s remains)
INFO - root - 2017-12-16 09:29:15.145898: step 34990, loss = 0.62, batch loss = 0.36 (48.6 examples/sec; 0.165 sec/batch; 13h:35m:59s remains)
INFO - root - 2017-12-16 09:29:16.834334: step 35000, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 13h:44m:03s remains)
2017-12-16 09:29:17.287280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75979793 -1.0115045 -1.4553096 -1.858996 -2.1740229 -2.3240557 -2.5131075 -2.8537538 -3.1300645 -3.1610074 -2.9033818 -2.5429878 -2.271565 -2.1196208 -2.0121236][-0.28992891 -0.25110388 -0.58387363 -0.92534149 -1.2523282 -1.5424316 -1.8590858 -2.3133297 -2.7079926 -2.8971844 -2.7855577 -2.5780904 -2.4011092 -2.2337363 -2.041657][-0.068755388 0.22198987 0.051909924 -0.15128207 -0.35794163 -0.67280531 -1.0287509 -1.5526078 -2.0676112 -2.3914909 -2.4808753 -2.4482496 -2.3292098 -2.1003 -1.8505112][-0.12564969 0.15266919 0.16352034 0.16031218 0.21465063 0.11357617 -0.17218256 -0.72628236 -1.3901601 -1.8639667 -2.1332784 -2.1919479 -2.0622356 -1.7691563 -1.4620864][-0.030960321 0.085802317 0.15822673 0.21903539 0.43374825 0.65668321 0.59441018 0.11710119 -0.685405 -1.3665786 -1.8224449 -1.9690759 -1.8158829 -1.4618082 -1.1186343][0.46368909 0.53416109 0.60431457 0.6298039 0.8265779 1.1818929 1.3670113 1.0138721 0.10468268 -0.85928237 -1.563897 -1.8372149 -1.6948011 -1.3189262 -0.92401505][1.0203738 1.2596414 1.4332538 1.4245808 1.6229749 1.9159169 2.2287321 1.9257822 0.89861894 -0.31987929 -1.2666475 -1.6902764 -1.6499236 -1.3122836 -0.86712611][0.94803786 1.2707491 1.501821 1.7043147 2.1544781 2.5586996 2.9893913 2.7817945 1.6352372 0.20339084 -0.99851382 -1.6787477 -1.7916752 -1.4638019 -0.97223794][0.22141171 0.47341752 0.63097191 1.048151 1.8050179 2.5599823 3.2218766 3.1687841 1.9949312 0.43705392 -0.9295125 -1.7550764 -1.9788656 -1.7125504 -1.2433641][-0.94622135 -0.75287473 -0.53872383 -0.0013782978 0.89055324 1.84094 2.5916972 2.586205 1.6119518 0.21841669 -1.0203558 -1.7998086 -2.0561717 -1.8647705 -1.5234746][-1.9899995 -1.7983561 -1.4962802 -0.84910345 0.093699455 1.0712945 1.6819344 1.5199997 0.65069342 -0.49225819 -1.4033949 -1.9407893 -2.137748 -2.014781 -1.7871478][-2.710248 -2.5187595 -2.1328857 -1.3749574 -0.37864923 0.53248167 0.88460755 0.56955552 -0.2328434 -1.1476798 -1.7343415 -2.0315287 -2.1229978 -2.0648553 -1.933377][-2.8947053 -2.7725854 -2.3988464 -1.684607 -0.71655011 0.11070561 0.31189227 -0.084903 -0.79723144 -1.4901707 -1.7902572 -1.8118045 -1.7403239 -1.7224894 -1.7249436][-2.3953519 -2.4363801 -2.2628579 -1.7668732 -0.9831593 -0.26739192 -0.12938857 -0.55578172 -1.2245686 -1.71101 -1.7789857 -1.5428936 -1.2911651 -1.2701966 -1.4288232][-1.4854553 -1.6782818 -1.7970171 -1.6486268 -1.1536027 -0.58262277 -0.49829972 -0.99830246 -1.7018442 -2.0708122 -1.9135157 -1.4292245 -0.98912311 -0.93909574 -1.2317054]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:29:19.669127: step 35010, loss = 0.52, batch loss = 0.26 (44.4 examples/sec; 0.180 sec/batch; 14h:53m:09s remains)
INFO - root - 2017-12-16 09:29:21.336836: step 35020, loss = 0.53, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 13h:53m:01s remains)
INFO - root - 2017-12-16 09:29:23.025000: step 35030, loss = 0.79, batch loss = 0.53 (47.1 examples/sec; 0.170 sec/batch; 14h:02m:18s remains)
INFO - root - 2017-12-16 09:29:24.742277: step 35040, loss = 0.49, batch loss = 0.23 (46.8 examples/sec; 0.171 sec/batch; 14h:06m:39s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:29:26.415921: step 35050, loss = 0.60, batch loss = 0.34 (47.4 examples/sec; 0.169 sec/batch; 13h:56m:04s remains)
INFO - root - 2017-12-16 09:29:28.080178: step 35060, loss = 0.54, batch loss = 0.29 (48.8 examples/sec; 0.164 sec/batch; 13h:32m:47s remains)
INFO - root - 2017-12-16 09:29:29.757302: step 35070, loss = 0.62, batch loss = 0.36 (48.2 examples/sec; 0.166 sec/batch; 13h:43m:20s remains)
INFO - root - 2017-12-16 09:29:31.417782: step 35080, loss = 0.55, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:43m:52s remains)
INFO - root - 2017-12-16 09:29:33.101378: step 35090, loss = 0.51, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 13h:55m:53s remains)
INFO - root - 2017-12-16 09:29:34.757672: step 35100, loss = 0.49, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 13h:35m:41s remains)
2017-12-16 09:29:35.237402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7675343 -3.4987235 -2.9609423 -2.5401812 -2.4859395 -2.6437604 -2.9053485 -3.1751983 -3.3194361 -3.2114177 -2.7718036 -1.836005 -0.37390924 0.72977114 0.58263755][-3.2411172 -2.3848474 -1.1512536 -0.26162529 -0.24533534 -0.89057887 -1.749964 -2.3735676 -2.6719368 -2.641264 -2.2084739 -1.0925279 0.68832088 2.0628574 1.9117897][-2.3120759 -0.88862419 1.0214221 2.336478 2.1756709 0.87053585 -0.63244212 -1.6176143 -2.0731773 -2.1972752 -1.8705368 -0.800776 0.93155193 2.3139737 2.1671536][-1.6517473 0.1343317 2.433253 3.9627244 3.6465509 2.0223119 0.34778929 -0.754547 -1.4466449 -1.9228371 -1.8590062 -1.0469854 0.32988644 1.4044995 1.1478264][-1.693967 0.14447522 2.4318421 3.9656274 3.7732112 2.5687568 1.5028462 0.72454882 -0.22910047 -1.2579643 -1.741466 -1.4310863 -0.67257261 -0.1223402 -0.5019213][-2.2434976 -0.6858815 1.2596955 2.7330606 3.0442331 2.8352115 2.9247034 2.7813995 1.5676296 -0.26691103 -1.5763338 -1.9068366 -1.6370039 -1.4173727 -1.7712002][-2.8041801 -1.7204881 -0.21878719 1.229723 2.1987774 3.1816947 4.4430723 4.9667768 3.4809988 0.84311366 -1.2709086 -2.104166 -2.1685827 -2.0719664 -2.2267234][-3.0639234 -2.469054 -1.4361224 -0.17110968 1.1668205 3.0358145 5.1040335 6.057251 4.4725971 1.4267869 -1.1454564 -2.3177323 -2.4716921 -2.3468831 -2.1922107][-3.0893245 -2.9529891 -2.4250932 -1.5141581 -0.24299741 1.7973654 3.9964612 4.9873257 3.7069094 1.0118353 -1.4686387 -2.6831055 -2.8339567 -2.5419016 -2.0576427][-3.0118232 -3.2080479 -3.1694291 -2.7117205 -1.7452533 0.041648149 1.983556 2.8898032 2.0684936 0.092259169 -1.8871021 -2.9529247 -3.1092229 -2.7578931 -2.1666143][-2.7579753 -3.0822375 -3.34542 -3.2651877 -2.6418879 -1.2763686 0.23315072 0.93209815 0.47969913 -0.80361927 -2.1880658 -3.0066638 -3.1303098 -2.8361468 -2.3510256][-2.4200892 -2.7484705 -3.1565652 -3.2502692 -2.8570635 -1.8743787 -0.76237142 -0.28354764 -0.58833683 -1.3829153 -2.2172475 -2.6566355 -2.6464012 -2.3699732 -2.069602][-1.9734926 -2.2754712 -2.7207735 -2.85714 -2.5759096 -1.820361 -0.96041417 -0.60904777 -0.8567313 -1.4304686 -1.9346089 -2.0532341 -1.7963271 -1.3930974 -1.1936027][-1.4803128 -1.7217088 -2.1257837 -2.2788732 -2.0396187 -1.3894455 -0.70291841 -0.45740306 -0.74673569 -1.3028924 -1.694662 -1.6630659 -1.1607002 -0.51716781 -0.20537329][-1.1427716 -1.2973188 -1.6311512 -1.7664018 -1.575738 -1.0610304 -0.55004406 -0.42074668 -0.752197 -1.3321401 -1.7811624 -1.7704883 -1.1906643 -0.35977125 0.20092297]]...]
INFO - root - 2017-12-16 09:29:36.902429: step 35110, loss = 0.61, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 13h:43m:24s remains)
INFO - root - 2017-12-16 09:29:38.598774: step 35120, loss = 0.49, batch loss = 0.23 (45.6 examples/sec; 0.175 sec/batch; 14h:29m:13s remains)
INFO - root - 2017-12-16 09:29:40.284917: step 35130, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 13h:40m:56s remains)
INFO - root - 2017-12-16 09:29:41.948571: step 35140, loss = 0.64, batch loss = 0.38 (48.5 examples/sec; 0.165 sec/batch; 13h:37m:58s remains)
INFO - root - 2017-12-16 09:29:43.625149: step 35150, loss = 0.55, batch loss = 0.29 (48.9 examples/sec; 0.164 sec/batch; 13h:30m:41s remains)
INFO - root - 2017-12-16 09:29:45.261879: step 35160, loss = 0.57, batch loss = 0.31 (47.4 examples/sec; 0.169 sec/batch; 13h:56m:09s remains)
INFO - root - 2017-12-16 09:29:46.926177: step 35170, loss = 0.50, batch loss = 0.24 (48.7 examples/sec; 0.164 sec/batch; 13h:33m:52s remains)
INFO - root - 2017-12-16 09:29:48.617972: step 35180, loss = 0.69, batch loss = 0.44 (47.7 examples/sec; 0.168 sec/batch; 13h:50m:29s remains)
INFO - root - 2017-12-16 09:29:50.281064: step 35190, loss = 0.52, batch loss = 0.26 (46.2 examples/sec; 0.173 sec/batch; 14h:18m:16s remains)
INFO - root - 2017-12-16 09:29:51.959069: step 35200, loss = 0.48, batch loss = 0.22 (48.0 examples/sec; 0.166 sec/batch; 13h:45m:00s remains)
2017-12-16 09:29:52.454657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8436289 -1.4273412 -1.2657058 -1.3839079 -1.2918948 -0.95391893 -0.82290161 -1.1669539 -1.6258533 -1.8763421 -1.8201061 -1.7013699 -1.3926911 -0.96921694 -0.93416548][-1.6972526 -1.3615887 -1.3197397 -1.4734321 -1.3603356 -1.0459198 -1.1107366 -1.6783038 -2.2983654 -2.6303415 -2.6191511 -2.5303855 -2.0973036 -1.4492875 -1.1768909][-1.3608545 -1.175584 -1.2854565 -1.5529848 -1.4892324 -1.2428964 -1.4804933 -2.1454248 -2.7357013 -3.0485432 -3.063606 -3.066803 -2.5843787 -1.7130064 -1.2601084][-0.90198267 -0.89292622 -1.1567559 -1.5119313 -1.4212241 -1.1777172 -1.5049022 -2.1599145 -2.6326144 -2.8353558 -2.89002 -2.9745646 -2.4672191 -1.4375467 -0.91143489][-0.71233225 -0.79420137 -1.01757 -1.1978942 -0.9103955 -0.5238713 -0.79033446 -1.3639301 -1.7046402 -1.8652778 -2.0480576 -2.2113905 -1.7547315 -0.72185791 -0.26971483][-0.45265377 -0.58676219 -0.62364638 -0.50482416 -0.0077013969 0.51078439 0.404469 0.014858484 -0.20351577 -0.40246427 -0.74068236 -0.99479127 -0.60299313 0.3268671 0.61695433][-0.071033 -0.088698387 0.1935761 0.66905022 1.4025078 2.0214951 2.1001589 1.8569334 1.656523 1.2696576 0.68730307 0.21671677 0.41554976 1.1831605 1.2925677][0.28929567 0.41276598 0.97292948 1.7780974 2.7876532 3.5302365 3.7057731 3.5524971 3.2885506 2.7037861 1.8520143 1.1067441 1.0220227 1.5327673 1.4991794][0.55945563 0.71576405 1.2721932 2.1145151 3.0933864 3.8271749 3.9895236 3.8334472 3.5003765 2.8409598 1.969517 1.1572554 0.941607 1.2470717 1.1296005][0.25638366 0.4400394 0.89399791 1.4963086 2.1787498 2.6884215 2.7377131 2.5459697 2.2233341 1.7186244 1.0649035 0.35982561 0.12152076 0.39439869 0.33916974][-0.64586568 -0.50903559 -0.26131272 0.088351965 0.5573895 0.86186266 0.81232929 0.609992 0.33709645 -0.033396244 -0.49337244 -1.0490329 -1.224687 -0.92515934 -0.82197249][-1.811306 -1.776217 -1.6950904 -1.5194829 -1.2248393 -1.0737412 -1.1775261 -1.3606639 -1.5919052 -1.8973427 -2.2035615 -2.5434918 -2.5319867 -2.1255705 -1.843735][-2.7120104 -2.7403078 -2.7964971 -2.7713256 -2.6373005 -2.5869355 -2.7017174 -2.874146 -3.0864284 -3.3412619 -3.5704422 -3.7407575 -3.535548 -2.9492683 -2.4134569][-3.2989762 -3.3696334 -3.4976363 -3.5952 -3.5740604 -3.5379617 -3.5683613 -3.6224875 -3.7323308 -3.9106691 -4.1109886 -4.2075653 -3.9120593 -3.2057147 -2.4489431][-3.4866209 -3.6124887 -3.7938316 -3.94175 -3.945735 -3.87085 -3.7945154 -3.7168522 -3.7086263 -3.8094597 -3.9763436 -4.0116262 -3.6372728 -2.864316 -2.0465496]]...]
INFO - root - 2017-12-16 09:29:54.138442: step 35210, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:45m:23s remains)
INFO - root - 2017-12-16 09:29:55.837121: step 35220, loss = 0.54, batch loss = 0.28 (45.5 examples/sec; 0.176 sec/batch; 14h:31m:57s remains)
INFO - root - 2017-12-16 09:29:57.525062: step 35230, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 13h:38m:11s remains)
INFO - root - 2017-12-16 09:29:59.212121: step 35240, loss = 0.49, batch loss = 0.24 (47.2 examples/sec; 0.170 sec/batch; 14h:00m:21s remains)
INFO - root - 2017-12-16 09:30:00.893780: step 35250, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.169 sec/batch; 13h:59m:18s remains)
INFO - root - 2017-12-16 09:30:02.595343: step 35260, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 13h:45m:02s remains)
INFO - root - 2017-12-16 09:30:04.300571: step 35270, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 13h:45m:26s remains)
INFO - root - 2017-12-16 09:30:05.984192: step 35280, loss = 0.51, batch loss = 0.25 (49.8 examples/sec; 0.161 sec/batch; 13h:16m:23s remains)
INFO - root - 2017-12-16 09:30:07.662720: step 35290, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 13h:55m:34s remains)
INFO - root - 2017-12-16 09:30:09.357503: step 35300, loss = 0.60, batch loss = 0.34 (49.2 examples/sec; 0.163 sec/batch; 13h:26m:03s remains)
2017-12-16 09:30:09.872008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1096725 -2.9471519 -2.8739219 -2.9142013 -2.9623346 -2.7069983 -1.9781225 -1.2752887 -1.0100379 -1.1469327 -1.3857276 -1.6676121 -1.7443711 -1.5648987 -1.3178972][-2.8262312 -2.6969869 -2.6405454 -2.7367113 -2.9189303 -2.8069582 -2.2009811 -1.5469736 -1.200762 -1.2016488 -1.363385 -1.6540345 -1.7852454 -1.6033094 -1.3858433][-2.739501 -2.6585293 -2.600971 -2.7362971 -3.039149 -3.0600917 -2.5073164 -1.8211403 -1.4536459 -1.4754854 -1.7161317 -2.0479629 -2.1183863 -1.8632183 -1.5821364][-2.7722087 -2.6494076 -2.52644 -2.66343 -2.9800134 -2.9438095 -2.2771811 -1.5440156 -1.3298873 -1.6048057 -2.0844703 -2.4896739 -2.5020335 -2.1819112 -1.8300507][-2.7928641 -2.5546699 -2.3126109 -2.3614774 -2.515898 -2.1910276 -1.2753235 -0.53667212 -0.65415108 -1.4376459 -2.2664847 -2.7872956 -2.7833862 -2.3862393 -1.9872121][-2.6475966 -2.272234 -1.880849 -1.7112939 -1.5175211 -0.73492241 0.49354339 1.1759043 0.58224607 -0.81632113 -2.0959482 -2.8245585 -2.8694553 -2.4587598 -2.0369232][-2.2020943 -1.7682182 -1.2410597 -0.77105474 -0.04113245 1.2831414 2.7526565 3.2517219 2.1693573 0.21687341 -1.5242491 -2.5524344 -2.7602382 -2.4294128 -2.0238535][-1.756321 -1.3424903 -0.77291751 -0.053417444 1.1018989 2.8999367 4.6303725 5.0120516 3.6333566 1.292814 -0.81042588 -2.1533866 -2.6041179 -2.3982568 -2.0192184][-1.8093672 -1.4138261 -0.84959435 -0.018674374 1.2807388 3.1583958 4.9145637 5.3548832 4.0644269 1.7369051 -0.42916405 -1.9203051 -2.5483539 -2.450352 -2.0932477][-2.2374978 -1.9125522 -1.3675926 -0.57190967 0.5592227 2.0925832 3.5497451 4.061461 3.0967183 1.188956 -0.7130692 -2.0910378 -2.679116 -2.5726702 -2.2015746][-2.837328 -2.6214211 -2.1558318 -1.4219981 -0.4573611 0.7743516 1.9923868 2.4492507 1.6764164 0.14238763 -1.4113193 -2.5051866 -2.9053092 -2.6959736 -2.2787244][-3.0070479 -2.9635878 -2.6561337 -2.0859642 -1.2674476 -0.18258691 0.95579052 1.4340501 0.81596518 -0.49137187 -1.8308926 -2.7494173 -3.0155544 -2.7530808 -2.3238139][-2.6513386 -2.788883 -2.7416532 -2.4349871 -1.8040192 -0.77981663 0.43822265 1.1139092 0.70358634 -0.45641065 -1.7108057 -2.6044219 -2.8883047 -2.6735682 -2.3025863][-1.9867635 -2.2553618 -2.4838972 -2.4545004 -2.0420606 -1.1088545 0.21322465 1.0950336 0.89003062 -0.12466502 -1.3299055 -2.2800808 -2.660886 -2.5428102 -2.247402][-1.6259743 -1.9646494 -2.2930472 -2.3995376 -2.1308537 -1.227555 0.24063468 1.3415048 1.2896323 0.35873389 -0.8666122 -1.9417943 -2.4849937 -2.4793494 -2.228507]]...]
INFO - root - 2017-12-16 09:30:11.593348: step 35310, loss = 0.49, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 13h:58m:26s remains)
INFO - root - 2017-12-16 09:30:13.274670: step 35320, loss = 0.62, batch loss = 0.36 (46.2 examples/sec; 0.173 sec/batch; 14h:16m:56s remains)
INFO - root - 2017-12-16 09:30:14.932355: step 35330, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 13h:44m:30s remains)
INFO - root - 2017-12-16 09:30:16.581820: step 35340, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 13h:40m:03s remains)
INFO - root - 2017-12-16 09:30:18.286050: step 35350, loss = 0.57, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 13h:57m:21s remains)
INFO - root - 2017-12-16 09:30:19.965450: step 35360, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 13h:39m:43s remains)
INFO - root - 2017-12-16 09:30:21.612205: step 35370, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:42m:01s remains)
INFO - root - 2017-12-16 09:30:23.302155: step 35380, loss = 0.60, batch loss = 0.34 (47.3 examples/sec; 0.169 sec/batch; 13h:57m:43s remains)
INFO - root - 2017-12-16 09:30:24.998390: step 35390, loss = 0.47, batch loss = 0.21 (47.2 examples/sec; 0.169 sec/batch; 13h:59m:12s remains)
INFO - root - 2017-12-16 09:30:26.676290: step 35400, loss = 0.55, batch loss = 0.30 (47.5 examples/sec; 0.169 sec/batch; 13h:54m:34s remains)
2017-12-16 09:30:27.147203: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8492422 -1.7352486 -1.6340753 -1.5857534 -1.5665112 -1.5151399 -1.4355354 -1.3381578 -1.2256488 -1.1122665 -1.0014646 -0.87387121 -0.75321817 -0.65809131 -0.60243714][-2.7109349 -2.7150111 -2.7067897 -2.7679663 -2.8202543 -2.7678776 -2.6425564 -2.4745507 -2.3139117 -2.1777492 -2.0360281 -1.8429836 -1.6336482 -1.4446858 -1.3244092][-3.2273574 -3.4077663 -3.5504539 -3.8037865 -3.9967399 -3.959543 -3.794198 -3.6275735 -3.5149922 -3.4572062 -3.3971419 -3.2586923 -3.0217957 -2.7684824 -2.5895305][-3.0396235 -3.3053513 -3.5390639 -3.9889469 -4.3029666 -4.2206378 -4.007184 -3.9557366 -4.0112672 -4.10942 -4.264905 -4.32902 -4.1750031 -3.9264293 -3.7461925][-2.1261296 -2.2876725 -2.4741011 -3.0003548 -3.3224113 -3.078455 -2.7628989 -2.8887405 -3.2263842 -3.5594406 -4.0152268 -4.4175205 -4.4776754 -4.3527417 -4.2688522][-0.99202418 -0.820685 -0.73018968 -1.1698896 -1.3493743 -0.79377985 -0.3817718 -0.80155373 -1.4287653 -1.9366097 -2.6569972 -3.3932021 -3.7095032 -3.7609801 -3.84874][-0.035145998 0.534322 0.94599748 0.71411109 0.80446529 1.7477064 2.242835 1.4931946 0.62034297 0.069762707 -0.74924469 -1.6750251 -2.1709421 -2.4050539 -2.6644635][0.37762928 1.1601961 1.7728634 1.6669645 2.0294104 3.3137465 3.783443 2.7784419 1.8981214 1.524261 0.80742645 -0.060159922 -0.54657781 -0.84354734 -1.1924964][-0.10748601 0.54718041 1.0303943 1.0333803 1.4056437 2.4926639 2.8691106 2.0897183 1.476377 1.333833 1.0002537 0.47718287 0.16604543 -0.065683842 -0.36149859][-1.1486961 -0.933311 -0.76816916 -0.80063331 -0.60764945 0.023875237 0.2838881 -0.1563766 -0.42753422 -0.32958937 -0.28083062 -0.38900077 -0.473359 -0.557199 -0.68837321][-1.9313692 -2.2724912 -2.4734938 -2.7017517 -2.8116255 -2.6074183 -2.4687343 -2.6522214 -2.7365835 -2.5678356 -2.3314672 -2.127737 -1.9615753 -1.8836902 -1.8802645][-2.0627761 -2.9001997 -3.4879794 -3.8843281 -4.1765809 -4.2559052 -4.2167449 -4.2618246 -4.2854748 -4.1705861 -3.9312835 -3.627914 -3.3296971 -3.1348515 -3.0356324][-1.5032389 -2.6157045 -3.4193752 -3.8963439 -4.2630053 -4.5038819 -4.5821695 -4.6014233 -4.6465693 -4.6390634 -4.4888058 -4.2165489 -3.9767203 -3.7947569 -3.6569452][-0.64435375 -1.7630478 -2.6180456 -3.0755746 -3.412497 -3.6988752 -3.8236861 -3.8176854 -3.852644 -3.9345791 -3.9328918 -3.8364496 -3.7495251 -3.6305151 -3.4792318][0.060488224 -0.8012588 -1.5800197 -1.9620875 -2.1697259 -2.3733547 -2.483119 -2.4496574 -2.4789402 -2.6437232 -2.8045304 -2.912106 -2.9643078 -2.8833461 -2.7232025]]...]
INFO - root - 2017-12-16 09:30:28.807547: step 35410, loss = 0.64, batch loss = 0.38 (48.5 examples/sec; 0.165 sec/batch; 13h:37m:33s remains)
INFO - root - 2017-12-16 09:30:30.489712: step 35420, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 13h:56m:07s remains)
INFO - root - 2017-12-16 09:30:32.160875: step 35430, loss = 0.65, batch loss = 0.39 (46.6 examples/sec; 0.172 sec/batch; 14h:09m:55s remains)
INFO - root - 2017-12-16 09:30:33.826292: step 35440, loss = 0.62, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 13h:54m:51s remains)
INFO - root - 2017-12-16 09:30:35.529271: step 35450, loss = 0.54, batch loss = 0.28 (45.5 examples/sec; 0.176 sec/batch; 14h:30m:36s remains)
INFO - root - 2017-12-16 09:30:37.221551: step 35460, loss = 0.47, batch loss = 0.21 (48.6 examples/sec; 0.165 sec/batch; 13h:35m:10s remains)
INFO - root - 2017-12-16 09:30:38.922142: step 35470, loss = 0.45, batch loss = 0.19 (46.9 examples/sec; 0.171 sec/batch; 14h:04m:09s remains)
INFO - root - 2017-12-16 09:30:40.593357: step 35480, loss = 0.66, batch loss = 0.40 (49.5 examples/sec; 0.162 sec/batch; 13h:19m:48s remains)
INFO - root - 2017-12-16 09:30:42.251346: step 35490, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 13h:39m:58s remains)
INFO - root - 2017-12-16 09:30:43.943950: step 35500, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 13h:52m:48s remains)
2017-12-16 09:30:44.482578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4925845 -1.5135851 -1.6092136 -1.8137827 -2.0459781 -2.2259181 -2.2843256 -2.2590568 -2.2995985 -2.50165 -2.6632721 -2.684469 -2.5655456 -2.2699962 -1.9700634][-1.7204473 -1.7788498 -1.9810063 -2.3206758 -2.5994146 -2.6830351 -2.4970994 -2.2251778 -2.2369931 -2.5780692 -2.8739181 -3.0028315 -2.9271052 -2.5629666 -2.1558332][-1.9353157 -2.041276 -2.3828347 -2.8572972 -3.135915 -2.9325261 -2.2551591 -1.6131861 -1.5008652 -1.9801499 -2.5472114 -2.9811015 -3.1336153 -2.838922 -2.3559487][-2.0249333 -2.2095685 -2.6767032 -3.2377377 -3.4573202 -2.8612638 -1.4581052 -0.1606648 0.12525797 -0.67013156 -1.6945562 -2.6393688 -3.1639094 -3.013881 -2.5316861][-2.0814486 -2.344136 -2.8593054 -3.4274886 -3.534575 -2.49359 -0.26779819 1.7654736 2.2078731 0.93313313 -0.79464567 -2.3648779 -3.2485518 -3.224993 -2.7086885][-2.1947379 -2.5060246 -2.9322479 -3.338294 -3.2026038 -1.762213 1.0944128 3.6080253 4.048975 2.1969492 -0.38164783 -2.4285538 -3.4663382 -3.4554241 -2.87647][-2.3212233 -2.5802336 -2.8764451 -3.033886 -2.6295221 -0.85539937 2.3009193 4.9466248 5.104929 2.676841 -0.42512059 -2.6184936 -3.5846968 -3.5042424 -2.9234409][-2.4658105 -2.621475 -2.7550364 -2.7362838 -2.1901321 -0.32947636 2.7301314 5.1000347 4.9146042 2.2919562 -0.64695013 -2.6558893 -3.4431992 -3.2967327 -2.8036752][-2.5946276 -2.6389184 -2.7093556 -2.6620915 -2.1230485 -0.5182302 2.1388419 4.0698833 3.8895295 1.5997009 -0.85783255 -2.5011394 -3.1508312 -3.0125685 -2.6409652][-2.6168694 -2.588635 -2.6400869 -2.656188 -2.3186316 -1.0974514 1.1429172 2.9088929 2.8900363 1.0449963 -1.010462 -2.3540459 -2.8941143 -2.8199606 -2.5616286][-2.6037188 -2.5105565 -2.5257678 -2.6021261 -2.4835913 -1.548945 0.49362755 2.2999866 2.3864172 0.73681068 -1.1721287 -2.3792975 -2.8507037 -2.7851119 -2.5776463][-2.4372909 -2.1516297 -2.031523 -2.0852983 -2.1198997 -1.4590836 0.36564064 2.0210264 1.9498518 0.29471588 -1.4926984 -2.5737429 -2.99325 -2.8813629 -2.6259654][-2.2071795 -1.6702628 -1.2853643 -1.2092403 -1.3733 -0.97852707 0.4833715 1.7805879 1.4368033 -0.17140818 -1.7663457 -2.6831913 -3.0677154 -2.9365172 -2.6455007][-1.9692035 -1.2147319 -0.6204406 -0.520568 -0.91368544 -0.7891196 0.37733793 1.4676721 1.039381 -0.48277819 -1.9045 -2.6794667 -3.0238197 -2.9038305 -2.6268249][-1.8399374 -0.98106456 -0.39772594 -0.50882804 -1.1394167 -1.1500036 -0.044199705 1.0099263 0.57233119 -0.89948332 -2.1670606 -2.773015 -3.0313826 -2.8875935 -2.6383953]]...]
INFO - root - 2017-12-16 09:30:46.249262: step 35510, loss = 0.64, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 13h:33m:18s remains)
INFO - root - 2017-12-16 09:30:47.946784: step 35520, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 13h:53m:18s remains)
INFO - root - 2017-12-16 09:30:49.663868: step 35530, loss = 0.72, batch loss = 0.47 (43.8 examples/sec; 0.183 sec/batch; 15h:03m:41s remains)
INFO - root - 2017-12-16 09:30:51.362689: step 35540, loss = 0.67, batch loss = 0.42 (46.4 examples/sec; 0.172 sec/batch; 14h:13m:40s remains)
INFO - root - 2017-12-16 09:30:53.052956: step 35550, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 13h:54m:32s remains)
INFO - root - 2017-12-16 09:30:54.723374: step 35560, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 13h:42m:59s remains)
INFO - root - 2017-12-16 09:30:56.416905: step 35570, loss = 0.52, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 14h:05m:41s remains)
INFO - root - 2017-12-16 09:30:58.112515: step 35580, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 13h:29m:30s remains)
INFO - root - 2017-12-16 09:30:59.808285: step 35590, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 13h:46m:54s remains)
INFO - root - 2017-12-16 09:31:01.460327: step 35600, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 13h:51m:39s remains)
2017-12-16 09:31:01.965359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4500451 -2.9469128 -2.8600175 -3.5369172 -4.4185486 -4.6787853 -4.0313215 -2.7054558 -1.2829694 -0.90452993 -1.9436939 -2.9253607 -3.3420513 -3.2127368 -3.0710015][-2.9999857 -2.3210208 -2.0868587 -2.753788 -3.6442957 -3.8634558 -3.1306121 -1.7941551 -0.25601935 0.18243527 -1.0551423 -2.2324 -2.8331401 -2.8404565 -2.8657563][-2.3247144 -1.5124731 -1.1421717 -1.7796957 -2.6340387 -2.8183575 -2.0634286 -0.75758564 0.80056334 1.3520668 0.060688496 -1.2000433 -1.934714 -2.1492648 -2.3614318][-1.7984509 -0.87330544 -0.43739092 -1.0655869 -1.8488576 -1.8708365 -1.0506456 0.2741704 1.900213 2.5738714 1.3173499 -0.030582428 -0.9156251 -1.301154 -1.6959467][-1.5247313 -0.57398546 -0.1583879 -0.76910365 -1.4794974 -1.3677543 -0.43163955 0.99562669 2.7105854 3.5306704 2.4164259 1.014349 0.029415131 -0.5288167 -1.158574][-1.578833 -0.68859029 -0.33930659 -0.868701 -1.4304541 -1.1396526 -0.035414696 1.49874 3.3167603 4.2807283 3.2309134 1.8244073 0.79951596 0.05429697 -0.84469318][-1.8736928 -1.1007936 -0.80393457 -1.2018589 -1.5273721 -1.0658197 0.14516354 1.7959688 3.7375982 4.7577791 3.6194255 2.1666443 1.0766077 0.25029993 -0.77808714][-2.2187049 -1.5568316 -1.3556371 -1.5689423 -1.6559951 -1.1003288 0.1429739 1.8390672 3.778147 4.5977764 3.36048 1.9797032 0.98877621 0.23469305 -0.7347281][-2.4791903 -1.9262395 -1.7750907 -1.8978578 -1.8885887 -1.3827906 -0.23779821 1.4350884 3.2738683 3.9133322 2.7337024 1.5244319 0.70992517 0.064549446 -0.73130178][-2.7042348 -2.2823865 -2.18195 -2.354748 -2.3937285 -1.9799652 -0.97766793 0.51827359 2.1133521 2.57467 1.5078115 0.45969892 -0.17460585 -0.67968416 -1.2508376][-2.9888735 -2.762748 -2.7822487 -3.0539486 -3.1910179 -2.8845172 -2.0523014 -0.79678822 0.42921519 0.68400908 -0.18332982 -0.96879995 -1.4343197 -1.7611578 -2.0874953][-3.1100602 -3.1049325 -3.2846577 -3.6468923 -3.8768952 -3.7136717 -3.1788177 -2.3284364 -1.4578788 -1.2322125 -1.7501121 -2.2447479 -2.5575416 -2.7507639 -2.9100039][-2.9720798 -3.094579 -3.3419523 -3.7545891 -4.082448 -4.0620475 -3.7805276 -3.2835371 -2.7181773 -2.4851241 -2.6912017 -2.9338336 -3.1179686 -3.1857998 -3.2226219][-2.6549134 -2.7606149 -2.9674048 -3.3515997 -3.6799619 -3.7087986 -3.5528615 -3.2737651 -2.9578781 -2.7869282 -2.8024652 -2.8580935 -2.923614 -2.9282751 -2.8876464][-2.3483417 -2.4013636 -2.5359614 -2.7816577 -3.014298 -3.0616198 -2.9732304 -2.8337417 -2.6886792 -2.5658906 -2.4824402 -2.4402158 -2.435823 -2.4193962 -2.3661425]]...]
INFO - root - 2017-12-16 09:31:03.650369: step 35610, loss = 0.51, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:35m:26s remains)
INFO - root - 2017-12-16 09:31:05.332915: step 35620, loss = 0.54, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 14h:01m:49s remains)
INFO - root - 2017-12-16 09:31:07.014390: step 35630, loss = 0.51, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 13h:24m:32s remains)
INFO - root - 2017-12-16 09:31:08.715719: step 35640, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 13h:36m:18s remains)
INFO - root - 2017-12-16 09:31:10.370195: step 35650, loss = 0.64, batch loss = 0.38 (48.7 examples/sec; 0.164 sec/batch; 13h:32m:12s remains)
INFO - root - 2017-12-16 09:31:12.034948: step 35660, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:41m:48s remains)
INFO - root - 2017-12-16 09:31:13.724988: step 35670, loss = 0.52, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 13h:49m:15s remains)
INFO - root - 2017-12-16 09:31:15.394968: step 35680, loss = 0.64, batch loss = 0.38 (48.4 examples/sec; 0.165 sec/batch; 13h:37m:25s remains)
INFO - root - 2017-12-16 09:31:17.041938: step 35690, loss = 0.66, batch loss = 0.40 (47.3 examples/sec; 0.169 sec/batch; 13h:57m:07s remains)
INFO - root - 2017-12-16 09:31:18.703431: step 35700, loss = 0.60, batch loss = 0.34 (48.8 examples/sec; 0.164 sec/batch; 13h:30m:09s remains)
2017-12-16 09:31:19.218370: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0026591 0.72713256 0.18688703 -0.42309904 -1.0744072 -1.6745236 -2.0748398 -2.300905 -2.4836416 -2.6801765 -2.797914 -2.8206756 -2.8125257 -2.8523045 -2.958446][0.96656036 0.723279 0.3442235 -0.14797902 -0.75115728 -1.3338699 -1.7390501 -1.9493432 -2.0963516 -2.2951655 -2.4806223 -2.6561379 -2.8357384 -3.0396631 -3.2927852][0.80032158 0.69356632 0.46755934 0.094194651 -0.46740973 -1.0460298 -1.4485916 -1.6326027 -1.6805665 -1.7772536 -1.9610851 -2.2482946 -2.6024382 -2.9714324 -3.341109][0.70049715 0.790895 0.721472 0.43755698 -0.046570063 -0.56723213 -0.9346 -1.1002473 -1.1071746 -1.1650478 -1.3517083 -1.705276 -2.16654 -2.6263747 -3.0327873][0.50662684 0.83889437 0.97682858 0.85000086 0.5240438 0.14413238 -0.16696167 -0.37508321 -0.44989765 -0.54221106 -0.70643747 -1.0181677 -1.4339416 -1.8608968 -2.2599359][0.0052981377 0.5978 1.0406799 1.1979256 1.1176937 0.91115046 0.674104 0.40562224 0.22419763 0.11708403 0.013286114 -0.16727376 -0.46953976 -0.86656237 -1.3412348][-0.74190462 -0.0097255707 0.66509414 1.100327 1.2954793 1.2967606 1.1715846 0.93297148 0.75786066 0.73639131 0.77773857 0.77455235 0.61085844 0.17662764 -0.45938814][-1.5859144 -0.87490964 -0.13699365 0.4379499 0.79220271 0.96766591 0.98108339 0.8657589 0.82138944 0.98791003 1.2620192 1.5011802 1.5167377 1.1274893 0.45946741][-2.3110077 -1.758312 -1.1141949 -0.5433315 -0.15573955 0.077623129 0.17347479 0.17729855 0.25922441 0.56753016 1.0094376 1.4551575 1.6992335 1.5713181 1.1792927][-2.8221071 -2.5094426 -2.0792148 -1.6549952 -1.3412675 -1.138447 -1.0131167 -0.91614747 -0.74401236 -0.40894902 0.039228678 0.53220558 0.97888565 1.2525227 1.3316422][-2.9846604 -2.9147687 -2.7485237 -2.5594478 -2.417192 -2.3219283 -2.2403116 -2.126318 -1.9372683 -1.6435668 -1.2862234 -0.85451925 -0.31165457 0.26881433 0.74001][-2.7528002 -2.8507397 -2.900157 -2.9293258 -2.9628932 -2.99811 -3.0083151 -2.9550138 -2.8298023 -2.6386371 -2.4267387 -2.142386 -1.6859037 -1.0708851 -0.44425595][-2.2935932 -2.4598734 -2.6202564 -2.7724156 -2.9166994 -3.04938 -3.1486976 -3.1870894 -3.1600459 -3.0940714 -3.0313344 -2.9302728 -2.6855958 -2.262116 -1.7569575][-1.8395314 -1.9960928 -2.1744721 -2.3592196 -2.5170729 -2.6593904 -2.7861812 -2.8800368 -2.9240022 -2.9592938 -3.030405 -3.1111951 -3.1095347 -2.9755626 -2.7462852][-1.4316962 -1.5575198 -1.7419415 -1.9315645 -2.0673566 -2.1677442 -2.2502849 -2.3036768 -2.3246 -2.3682077 -2.49584 -2.6850104 -2.8625867 -2.9770374 -3.0339613]]...]
INFO - root - 2017-12-16 09:31:20.878989: step 35710, loss = 0.48, batch loss = 0.22 (48.7 examples/sec; 0.164 sec/batch; 13h:31m:47s remains)
INFO - root - 2017-12-16 09:31:22.558432: step 35720, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.168 sec/batch; 13h:48m:42s remains)
INFO - root - 2017-12-16 09:31:24.235654: step 35730, loss = 0.62, batch loss = 0.36 (48.9 examples/sec; 0.164 sec/batch; 13h:28m:46s remains)
INFO - root - 2017-12-16 09:31:25.888913: step 35740, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 13h:35m:18s remains)
INFO - root - 2017-12-16 09:31:27.574170: step 35750, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 13h:57m:21s remains)
INFO - root - 2017-12-16 09:31:29.243135: step 35760, loss = 0.48, batch loss = 0.23 (48.3 examples/sec; 0.166 sec/batch; 13h:39m:05s remains)
INFO - root - 2017-12-16 09:31:30.957345: step 35770, loss = 0.58, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 13h:41m:15s remains)
INFO - root - 2017-12-16 09:31:32.627650: step 35780, loss = 0.50, batch loss = 0.24 (46.4 examples/sec; 0.172 sec/batch; 14h:12m:45s remains)
INFO - root - 2017-12-16 09:31:34.305470: step 35790, loss = 0.60, batch loss = 0.34 (48.5 examples/sec; 0.165 sec/batch; 13h:35m:06s remains)
INFO - root - 2017-12-16 09:31:35.975812: step 35800, loss = 0.58, batch loss = 0.32 (45.8 examples/sec; 0.175 sec/batch; 14h:24m:16s remains)
2017-12-16 09:31:36.540846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4252782 -1.3365695 -1.2580041 -1.1881276 -1.1414093 -1.1547345 -1.2095709 -1.2264649 -1.1105738 -0.86911094 -0.619122 -0.53937936 -0.65657651 -0.80458033 -0.7261802][-2.2651455 -2.3345232 -2.3810678 -2.4028096 -2.4246051 -2.4958541 -2.5975366 -2.6378696 -2.5117192 -2.2504125 -1.9646893 -1.8363756 -1.8551099 -1.8741055 -1.6846582][-3.0741692 -3.2871363 -3.4417534 -3.5343628 -3.5851722 -3.6388125 -3.6946235 -3.6884513 -3.5854702 -3.4252956 -3.2814238 -3.2680404 -3.3001311 -3.2324264 -2.939126][-3.6177278 -3.9484234 -4.139122 -4.2078848 -4.1469245 -4.0278826 -3.8657613 -3.6931374 -3.6265194 -3.6958313 -3.8900151 -4.2088447 -4.480144 -4.4812565 -4.1569228][-3.6856236 -4.1046557 -4.2454176 -4.10463 -3.6915741 -3.1298614 -2.4963167 -2.0308549 -1.9648007 -2.3508391 -3.0730333 -4.0016017 -4.7658496 -5.0767627 -4.8762264][-3.3919573 -3.786912 -3.7583179 -3.2619286 -2.3551486 -1.1850113 0.041146278 0.87618041 0.89848709 0.19102025 -1.0197785 -2.5555029 -3.9341273 -4.655005 -4.6614037][-3.0244112 -3.3850746 -3.2455323 -2.4542851 -1.1121169 0.63052559 2.4347613 3.5912235 3.5025222 2.539819 1.1239369 -0.687358 -2.4172578 -3.4213769 -3.6082478][-2.7603741 -3.1837986 -3.0642095 -2.169682 -0.65420663 1.3760574 3.5567658 4.9200697 4.68688 3.5881298 2.3333614 0.669157 -1.0981544 -2.2438478 -2.56789][-2.7364624 -3.2840731 -3.3631165 -2.6364615 -1.2019447 0.74319243 2.8071921 4.0593414 3.7671583 2.8882186 2.1713822 1.0331235 -0.43267608 -1.4921665 -1.9023547][-2.6911683 -3.3893614 -3.7527142 -3.4015241 -2.2993805 -0.7700609 0.83019829 1.7349255 1.4875131 1.0181615 0.88880253 0.41299152 -0.5006814 -1.2844068 -1.6888056][-2.6230013 -3.4916608 -4.098731 -4.0638771 -3.3939905 -2.427552 -1.4546485 -0.98098588 -1.182557 -1.3074282 -1.0150388 -0.9218365 -1.1949588 -1.516216 -1.7302057][-2.0318515 -3.0227325 -3.7368703 -3.9386404 -3.6946077 -3.3360763 -3.0611362 -3.0795586 -3.3340325 -3.3023963 -2.8675234 -2.4486964 -2.213815 -2.0655243 -1.9965869][-0.96333933 -1.9002699 -2.5376973 -2.8914201 -3.0079637 -3.1127126 -3.3721 -3.7899547 -4.1303763 -4.0245595 -3.5631061 -3.0556834 -2.58891 -2.2147136 -1.9468536][0.40916014 -0.33560371 -0.81741047 -1.1561943 -1.449173 -1.7542689 -2.2738686 -2.9697013 -3.4597764 -3.4146087 -3.0447865 -2.6746831 -2.2878165 -1.9039918 -1.5611368][1.3415017 1.0008039 0.84904575 0.58771896 0.21721411 -0.12436509 -0.67745841 -1.4680284 -2.0608327 -2.1379423 -1.9578326 -1.8422649 -1.7081046 -1.4741299 -1.1154009]]...]
INFO - root - 2017-12-16 09:31:38.244031: step 35810, loss = 0.66, batch loss = 0.40 (46.2 examples/sec; 0.173 sec/batch; 14h:16m:59s remains)
INFO - root - 2017-12-16 09:31:39.946221: step 35820, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:37m:02s remains)
INFO - root - 2017-12-16 09:31:41.635325: step 35830, loss = 0.58, batch loss = 0.33 (47.7 examples/sec; 0.168 sec/batch; 13h:49m:25s remains)
INFO - root - 2017-12-16 09:31:43.298913: step 35840, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:44m:46s remains)
INFO - root - 2017-12-16 09:31:45.000622: step 35850, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:40m:06s remains)
INFO - root - 2017-12-16 09:31:46.672586: step 35860, loss = 0.60, batch loss = 0.34 (46.7 examples/sec; 0.171 sec/batch; 14h:07m:22s remains)
INFO - root - 2017-12-16 09:31:48.317286: step 35870, loss = 0.59, batch loss = 0.33 (50.4 examples/sec; 0.159 sec/batch; 13h:04m:03s remains)
INFO - root - 2017-12-16 09:31:50.012085: step 35880, loss = 0.57, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 13h:51m:34s remains)
INFO - root - 2017-12-16 09:31:51.695498: step 35890, loss = 0.61, batch loss = 0.36 (49.1 examples/sec; 0.163 sec/batch; 13h:25m:28s remains)
INFO - root - 2017-12-16 09:31:53.383103: step 35900, loss = 0.67, batch loss = 0.41 (47.2 examples/sec; 0.169 sec/batch; 13h:57m:51s remains)
2017-12-16 09:31:53.887701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8465154 -3.003201 -3.003983 -2.6305285 -1.9737511 -1.1783674 -0.54794586 -0.41066563 -0.8635689 -1.6564753 -2.454041 -2.9804957 -3.1449542 -3.2130473 -3.1538539][-3.2408741 -3.3822539 -3.2356167 -2.568475 -1.5280833 -0.48807561 0.20782208 0.2504971 -0.34278011 -1.2297434 -2.1623425 -2.8056483 -3.0454881 -3.1100814 -2.9668808][-3.3610222 -3.5089824 -3.2488978 -2.3636978 -1.0758804 0.095502377 0.73753166 0.70754623 -0.0044913292 -0.93169546 -1.8500304 -2.637701 -2.9954357 -3.0707941 -2.909869][-3.0804152 -3.3639603 -3.1613283 -2.1852283 -0.74348474 0.49701262 1.173995 1.0934603 0.29918551 -0.66627741 -1.5616498 -2.4585569 -3.0035517 -3.1698887 -3.0538416][-2.5534694 -2.9741526 -2.8577867 -1.8612709 -0.27987242 1.0587728 1.8497908 1.7258742 0.75123191 -0.3185308 -1.2311018 -2.1668458 -2.8367333 -3.146019 -3.1073265][-1.7842078 -2.1612184 -2.0226629 -1.085348 0.47662306 1.888377 2.820081 2.6974881 1.4662755 0.22068596 -0.74668038 -1.6290228 -2.2771108 -2.6307414 -2.6770015][-0.92555463 -1.205516 -1.0337719 -0.21488571 1.1407666 2.5172369 3.5667346 3.444597 2.0704086 0.70867324 -0.31204653 -1.1160538 -1.6360104 -1.9090716 -1.9637351][-0.36196518 -0.5867492 -0.49037898 0.056763649 1.0648403 2.3028166 3.3826106 3.3475955 2.0713866 0.73305511 -0.28111196 -0.99075866 -1.3824577 -1.5830178 -1.6197047][-0.5579865 -0.71618843 -0.71645641 -0.42017257 0.23104477 1.1957562 2.1740329 2.283674 1.2893765 0.13548875 -0.76260245 -1.3496152 -1.6459126 -1.7719262 -1.7627392][-1.2144825 -1.2780222 -1.3269924 -1.2070682 -0.82954264 -0.19758439 0.5505805 0.71509433 0.028560162 -0.78963435 -1.4593098 -1.8744664 -2.0754402 -2.1419995 -2.1059649][-2.0906138 -2.1804795 -2.2967393 -2.2963891 -2.1170516 -1.7492568 -1.2069733 -1.0237546 -1.4269795 -1.9308292 -2.3500149 -2.5937963 -2.7093453 -2.7226803 -2.6506839][-2.8900557 -3.0508819 -3.2458849 -3.3581276 -3.3385761 -3.1357327 -2.7668478 -2.5836043 -2.7900562 -3.0631542 -3.2908549 -3.4159813 -3.4514859 -3.4032824 -3.2944236][-3.19977 -3.3829107 -3.5788589 -3.7272182 -3.772438 -3.6757908 -3.4521556 -3.3290851 -3.4218323 -3.5443289 -3.6530366 -3.6983614 -3.6653409 -3.5793509 -3.4689717][-2.9903409 -3.110292 -3.2282417 -3.3249283 -3.3621709 -3.313426 -3.2075374 -3.1528852 -3.1978939 -3.239907 -3.271183 -3.2742963 -3.2253973 -3.1486721 -3.0707631][-2.4850166 -2.5435352 -2.5840735 -2.6266875 -2.6548586 -2.6414456 -2.6019912 -2.5796962 -2.5973594 -2.6082144 -2.6061611 -2.5893209 -2.5552649 -2.5094709 -2.4648292]]...]
INFO - root - 2017-12-16 09:31:55.547170: step 35910, loss = 0.52, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 13h:28m:42s remains)
INFO - root - 2017-12-16 09:31:57.226909: step 35920, loss = 0.55, batch loss = 0.29 (49.2 examples/sec; 0.162 sec/batch; 13h:22m:58s remains)
INFO - root - 2017-12-16 09:31:58.909736: step 35930, loss = 0.67, batch loss = 0.42 (48.7 examples/sec; 0.164 sec/batch; 13h:31m:18s remains)
INFO - root - 2017-12-16 09:32:00.561808: step 35940, loss = 0.64, batch loss = 0.38 (48.3 examples/sec; 0.166 sec/batch; 13h:39m:04s remains)
INFO - root - 2017-12-16 09:32:02.286846: step 35950, loss = 0.51, batch loss = 0.26 (46.4 examples/sec; 0.172 sec/batch; 14h:11m:49s remains)
INFO - root - 2017-12-16 09:32:03.967946: step 35960, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:24m:42s remains)
INFO - root - 2017-12-16 09:32:05.632321: step 35970, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 13h:58m:01s remains)
INFO - root - 2017-12-16 09:32:07.311472: step 35980, loss = 0.48, batch loss = 0.23 (49.6 examples/sec; 0.161 sec/batch; 13h:16m:53s remains)
INFO - root - 2017-12-16 09:32:08.988722: step 35990, loss = 0.55, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:33m:00s remains)
INFO - root - 2017-12-16 09:32:10.665424: step 36000, loss = 0.78, batch loss = 0.52 (45.8 examples/sec; 0.175 sec/batch; 14h:23m:06s remains)
2017-12-16 09:32:11.157282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6611494 -1.8455114 -2.2200766 -2.5658641 -2.6899371 -2.536463 -2.2822738 -2.3945742 -2.5885577 -2.4407251 -2.110409 -1.9851769 -2.063448 -1.9568554 -1.6573395][-1.5803462 -1.681236 -2.0035226 -2.3476276 -2.482053 -2.3754003 -2.2147615 -2.3805487 -2.5641031 -2.3173552 -1.8972799 -1.767308 -1.9787519 -2.1130388 -2.0504611][-1.7541463 -1.8323225 -2.0643005 -2.2972658 -2.3146017 -2.1773999 -2.0625303 -2.2629826 -2.3693635 -2.0118148 -1.5488684 -1.4675639 -1.85323 -2.2014785 -2.3107886][-2.030612 -2.1055064 -2.2162676 -2.2653847 -2.1184411 -1.8774805 -1.7788711 -1.9567777 -1.9869671 -1.5577563 -1.0341638 -0.99675846 -1.5548768 -2.0906622 -2.3373585][-2.2185357 -2.2727151 -2.246835 -2.099684 -1.7685435 -1.4122293 -1.2993939 -1.4948794 -1.4661953 -0.99062085 -0.47417164 -0.51123166 -1.2297567 -1.9414181 -2.1772776][-2.2556977 -2.2432539 -2.058286 -1.7227321 -1.266335 -0.78902245 -0.63159037 -0.812737 -0.77263212 -0.3261838 0.19601107 0.058358669 -0.88353205 -1.7287376 -1.8389518][-2.1218143 -2.016712 -1.7065873 -1.2675158 -0.79711843 -0.30943418 -0.15844703 -0.3315649 -0.26651025 0.17370296 0.68960571 0.442847 -0.647822 -1.4907486 -1.4045715][-1.9564056 -1.813628 -1.4616082 -1.0509825 -0.72635341 -0.36059141 -0.21359634 -0.41146123 -0.3942982 0.052909374 0.58787251 0.34308434 -0.72417092 -1.4887819 -1.2629689][-1.8897325 -1.8056765 -1.50467 -1.1972003 -1.0471849 -0.85628939 -0.79527295 -1.0691355 -1.162168 -0.72869122 -0.14092255 -0.27771306 -1.1911207 -1.856266 -1.6844342][-2.0118408 -2.0669503 -1.95079 -1.8092926 -1.8061392 -1.7250482 -1.7083148 -2.0688357 -2.2744341 -1.9138461 -1.3696805 -1.3308842 -1.9831192 -2.5011506 -2.4345074][-2.2909923 -2.5275481 -2.6083186 -2.6247289 -2.6410325 -2.5312445 -2.4908261 -2.8614416 -3.1283159 -2.8981743 -2.4697483 -2.3405633 -2.7406878 -3.1135249 -3.1535208][-2.5242531 -2.906709 -3.1168425 -3.2077312 -3.1526451 -2.9360182 -2.8123748 -3.0974066 -3.3714628 -3.2663577 -2.9731965 -2.8580396 -3.0897746 -3.3870363 -3.5044377][-2.5416031 -2.9654272 -3.2083058 -3.2735353 -3.133595 -2.8419104 -2.6579361 -2.8226945 -3.0786481 -3.0851443 -2.9195266 -2.8428893 -2.9776764 -3.1925287 -3.3530653][-2.3526211 -2.7068002 -2.9020848 -2.9044096 -2.7225072 -2.4342554 -2.2595983 -2.3315625 -2.53494 -2.5944271 -2.5267236 -2.4735389 -2.5417695 -2.698529 -2.8623884][-2.0826457 -2.3165739 -2.4487052 -2.4156671 -2.2349663 -1.9950342 -1.8675349 -1.8871557 -2.009161 -2.0761898 -2.0599821 -2.0366058 -2.0657258 -2.1578648 -2.2879369]]...]
INFO - root - 2017-12-16 09:32:12.841273: step 36010, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:33m:14s remains)
INFO - root - 2017-12-16 09:32:14.518629: step 36020, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 14h:00m:41s remains)
INFO - root - 2017-12-16 09:32:16.209025: step 36030, loss = 0.61, batch loss = 0.35 (46.2 examples/sec; 0.173 sec/batch; 14h:16m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:32:17.877309: step 36040, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:42m:57s remains)
INFO - root - 2017-12-16 09:32:19.545829: step 36050, loss = 0.57, batch loss = 0.31 (48.7 examples/sec; 0.164 sec/batch; 13h:30m:56s remains)
INFO - root - 2017-12-16 09:32:21.191482: step 36060, loss = 0.72, batch loss = 0.46 (48.2 examples/sec; 0.166 sec/batch; 13h:40m:52s remains)
INFO - root - 2017-12-16 09:32:22.866783: step 36070, loss = 0.58, batch loss = 0.32 (47.9 examples/sec; 0.167 sec/batch; 13h:45m:08s remains)
INFO - root - 2017-12-16 09:32:24.534882: step 36080, loss = 0.48, batch loss = 0.22 (48.9 examples/sec; 0.164 sec/batch; 13h:28m:29s remains)
INFO - root - 2017-12-16 09:32:26.197026: step 36090, loss = 0.51, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 13h:23m:55s remains)
INFO - root - 2017-12-16 09:32:27.886680: step 36100, loss = 0.54, batch loss = 0.28 (47.7 examples/sec; 0.168 sec/batch; 13h:48m:50s remains)
2017-12-16 09:32:28.371871: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2877493 0.85965562 -0.018561602 -1.1515418 -2.1663179 -2.8557019 -3.264533 -3.5456769 -3.8419716 -4.1365094 -4.2478065 -4.060123 -3.6400261 -3.1507008 -2.7109361][0.78244638 0.16978312 -0.95314777 -2.307137 -3.4892941 -4.2636137 -4.6101456 -4.6707139 -4.5972776 -4.4838839 -4.2677469 -3.9193575 -3.4766941 -3.0342228 -2.6361976][-0.7710892 -1.3396008 -2.3727052 -3.5844371 -4.5813551 -5.08006 -5.0391026 -4.6108904 -4.0525255 -3.5783834 -3.2052441 -2.9214506 -2.7085943 -2.5265615 -2.3118808][-2.3129961 -2.7123771 -3.4835837 -4.3337665 -4.883687 -4.8798957 -4.2488508 -3.18366 -2.0923216 -1.3431244 -1.0219676 -1.0743353 -1.3495432 -1.626374 -1.7236475][-2.9893939 -3.2152159 -3.6623802 -4.0528226 -4.0775566 -3.5198245 -2.2646971 -0.57304597 0.96194625 1.7811558 1.7424695 1.0529368 0.095242739 -0.71549809 -1.1226757][-2.801321 -2.8848059 -3.0059676 -2.9572418 -2.5435946 -1.5662084 0.11909676 2.2513239 4.0293865 4.5824184 3.8176978 2.3724673 0.8755877 -0.2603271 -0.81495953][-2.3461854 -2.3259344 -2.1284657 -1.635347 -0.81513882 0.38888049 2.1290615 4.2281694 5.7907658 5.6355934 4.0685606 2.1056197 0.4763608 -0.60727358 -1.0760849][-2.079191 -1.9601038 -1.4493243 -0.54304516 0.54338074 1.6496499 2.8238537 3.9908106 4.542222 3.8296325 2.153105 0.39347148 -0.84932649 -1.5249484 -1.716516][-1.9390392 -1.7060974 -0.96699727 0.10810184 1.145505 1.7922676 2.0373771 2.0579498 1.7013643 0.81109 -0.44430625 -1.5654644 -2.1733687 -2.3174558 -2.1710496][-1.480952 -1.1247302 -0.36881518 0.53123212 1.1309671 1.119266 0.55213618 -0.22408009 -1.0287186 -1.7330824 -2.2822154 -2.5739095 -2.5297492 -2.2572997 -1.9324416][-0.46901643 -0.052182198 0.44778323 0.89261246 0.87640905 0.20158744 -0.86681306 -1.9001832 -2.5991337 -2.7829819 -2.5741949 -2.1532774 -1.6989267 -1.3017615 -1.0729495][0.80101585 1.2238204 1.3245249 1.1026418 0.44238019 -0.62516856 -1.7536385 -2.5711133 -2.8166878 -2.3945818 -1.5786903 -0.77986479 -0.29340768 -0.1236465 -0.21135783][1.9060533 2.2758548 1.9667366 1.1016626 -0.035196066 -1.2005109 -2.0876236 -2.4872739 -2.2633853 -1.4649246 -0.431535 0.34928966 0.5449717 0.30979776 -0.10925913][2.6459057 2.8838732 2.2624056 1.0507805 -0.28397465 -1.4234509 -2.0899653 -2.2004583 -1.7693057 -0.98201609 -0.16595507 0.32379031 0.22186208 -0.27370906 -0.78002071][2.7457931 2.849962 2.1603754 0.95245171 -0.33834195 -1.3980583 -1.9691925 -2.0225115 -1.6722806 -1.1825613 -0.7849226 -0.6439302 -0.87635338 -1.3188841 -1.5860478]]...]
INFO - root - 2017-12-16 09:32:30.035983: step 36110, loss = 0.50, batch loss = 0.24 (47.2 examples/sec; 0.169 sec/batch; 13h:57m:16s remains)
INFO - root - 2017-12-16 09:32:31.688461: step 36120, loss = 0.54, batch loss = 0.29 (48.0 examples/sec; 0.167 sec/batch; 13h:42m:38s remains)
INFO - root - 2017-12-16 09:32:33.343132: step 36130, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:37m:50s remains)
INFO - root - 2017-12-16 09:32:35.011466: step 36140, loss = 0.59, batch loss = 0.33 (50.0 examples/sec; 0.160 sec/batch; 13h:10m:25s remains)
INFO - root - 2017-12-16 09:32:36.667283: step 36150, loss = 0.53, batch loss = 0.27 (46.9 examples/sec; 0.171 sec/batch; 14h:02m:44s remains)
INFO - root - 2017-12-16 09:32:38.350458: step 36160, loss = 0.47, batch loss = 0.21 (49.0 examples/sec; 0.163 sec/batch; 13h:25m:52s remains)
INFO - root - 2017-12-16 09:32:40.037366: step 36170, loss = 0.61, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 13h:38m:45s remains)
INFO - root - 2017-12-16 09:32:41.699693: step 36180, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.165 sec/batch; 13h:37m:12s remains)
INFO - root - 2017-12-16 09:32:43.348119: step 36190, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.165 sec/batch; 13h:32m:29s remains)
INFO - root - 2017-12-16 09:32:45.006518: step 36200, loss = 0.53, batch loss = 0.27 (46.8 examples/sec; 0.171 sec/batch; 14h:04m:44s remains)
2017-12-16 09:32:45.523096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.36555028 0.087448835 0.36755252 0.3266151 0.22590017 0.55744386 1.0250456 0.98205209 0.35712862 -0.51138759 -1.4231331 -2.1106725 -2.3448198 -2.2200885 -2.0420361][0.34875703 0.71602559 0.76990342 0.41123557 -0.018843889 0.0053503513 0.33223796 0.21677542 -0.39941931 -1.1994725 -1.9977756 -2.5541825 -2.6435771 -2.389343 -2.1062446][0.5254612 0.72199774 0.57045412 -0.037283421 -0.73915243 -0.91059375 -0.68830383 -0.74514461 -1.2591656 -1.9843268 -2.6701109 -3.0868859 -2.9936433 -2.5897813 -2.194145][0.075445652 0.10117483 -0.13087916 -0.7677089 -1.4517534 -1.5951624 -1.3258852 -1.2732689 -1.7072434 -2.3947387 -3.0486665 -3.39661 -3.2051597 -2.7104247 -2.2495584][-0.65730774 -0.69857657 -0.85235 -1.2782047 -1.6095686 -1.3952615 -0.86390269 -0.65331137 -1.0856414 -1.9289269 -2.769815 -3.2564447 -3.1775193 -2.7097168 -2.260093][-1.4542665 -1.4300127 -1.3447727 -1.35511 -1.1423944 -0.37366033 0.68672562 1.221967 0.74493051 -0.5197655 -1.804631 -2.6739092 -2.9053807 -2.6029713 -2.2344139][-2.0335815 -1.8770355 -1.4788617 -1.0521485 -0.29880214 1.1055381 2.71542 3.5570924 2.9145133 1.1637042 -0.57999611 -1.8677871 -2.45527 -2.3964691 -2.1629956][-2.3342271 -2.0226226 -1.466136 -0.786638 0.34786582 2.1743481 4.200696 4.9802122 4.1297932 2.2768414 0.39522791 -1.1394105 -1.9809992 -2.1390746 -2.0487852][-2.7677891 -2.4246302 -1.8978293 -1.1707523 -0.0055627823 1.8513973 3.7910082 4.42025 3.7707226 2.3094265 0.66176224 -0.85819995 -1.7522844 -1.9701006 -1.95656][-3.2580535 -3.0246906 -2.682524 -2.1752663 -1.2473017 0.3482244 1.9684937 2.4221823 2.0872757 1.2164421 0.076266289 -1.1964248 -1.900929 -1.9830161 -1.9361609][-3.6566877 -3.6454744 -3.6184902 -3.4997966 -2.9159179 -1.5774715 -0.25939631 0.047230721 -0.1146965 -0.56146884 -1.2284054 -2.0535369 -2.4016926 -2.2224627 -2.0227935][-4.1196952 -4.2940083 -4.5309534 -4.7360973 -4.4656315 -3.4774718 -2.4666641 -2.1689651 -2.2188587 -2.4042766 -2.6509955 -2.9714646 -2.9091706 -2.4800749 -2.1126351][-4.5214558 -4.71506 -5.04809 -5.387476 -5.3480396 -4.6962748 -3.9636321 -3.6830132 -3.6046534 -3.5971632 -3.564467 -3.5197482 -3.1487761 -2.5583842 -2.1200237][-4.419209 -4.5409312 -4.8042336 -5.0899634 -5.1540289 -4.8051872 -4.3523664 -4.120513 -4.0282426 -3.9174676 -3.7442274 -3.4907365 -2.9972496 -2.4094141 -2.0173919][-3.6695657 -3.7091212 -3.8428669 -4.0116382 -4.0932183 -3.9554796 -3.7337668 -3.6139579 -3.5316739 -3.4331994 -3.2650127 -2.9828982 -2.5550828 -2.1163867 -1.8494283]]...]
INFO - root - 2017-12-16 09:32:47.208006: step 36210, loss = 0.55, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 13h:44m:21s remains)
INFO - root - 2017-12-16 09:32:48.888868: step 36220, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 13h:37m:21s remains)
INFO - root - 2017-12-16 09:32:50.570818: step 36230, loss = 0.66, batch loss = 0.40 (48.8 examples/sec; 0.164 sec/batch; 13h:29m:42s remains)
INFO - root - 2017-12-16 09:32:52.239855: step 36240, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 13h:44m:50s remains)
INFO - root - 2017-12-16 09:32:53.927471: step 36250, loss = 0.62, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 13h:55m:41s remains)
INFO - root - 2017-12-16 09:32:55.567206: step 36260, loss = 0.61, batch loss = 0.35 (48.1 examples/sec; 0.166 sec/batch; 13h:41m:04s remains)
INFO - root - 2017-12-16 09:32:57.214163: step 36270, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:47m:13s remains)
INFO - root - 2017-12-16 09:32:58.875435: step 36280, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:39m:16s remains)
INFO - root - 2017-12-16 09:33:00.534922: step 36290, loss = 0.57, batch loss = 0.32 (45.4 examples/sec; 0.176 sec/batch; 14h:30m:16s remains)
INFO - root - 2017-12-16 09:33:02.207125: step 36300, loss = 0.54, batch loss = 0.29 (46.8 examples/sec; 0.171 sec/batch; 14h:04m:05s remains)
2017-12-16 09:33:02.665235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0469064 -1.3189416 -1.6034014 -1.8632197 -1.9004411 -1.7082788 -1.3952913 -1.1203014 -1.1650968 -1.4625944 -1.7698109 -2.0374787 -2.1701107 -2.1763463 -2.0906625][-1.1767366 -1.4108169 -1.6441588 -1.8479515 -1.8443494 -1.547621 -1.0910115 -0.76563096 -0.84141457 -1.2247801 -1.6751074 -2.0509458 -2.2502706 -2.2704146 -2.1689448][-1.4231772 -1.5199461 -1.5657341 -1.5564536 -1.3369238 -0.83179855 -0.21980739 0.10722399 -0.19765019 -0.8950628 -1.6055691 -2.1297364 -2.3923771 -2.4115622 -2.2664011][-1.6592219 -1.5731933 -1.3613697 -1.0457472 -0.52253127 0.23740697 1.0117567 1.3288693 0.759413 -0.3362143 -1.3896663 -2.14101 -2.5191917 -2.5535653 -2.3474796][-1.8049279 -1.5802393 -1.1825552 -0.6454041 0.12935615 1.1312914 2.1043196 2.4761686 1.7467813 0.34608316 -0.9723897 -1.942683 -2.4723747 -2.5626051 -2.3484187][-1.7142267 -1.5011506 -1.1071434 -0.55857921 0.24361539 1.2504807 2.276525 2.7313428 2.1209688 0.72513533 -0.69010258 -1.804688 -2.4116781 -2.5189247 -2.3159175][-1.5185363 -1.4045268 -1.145745 -0.77002203 -0.21487832 0.52005434 1.321044 1.7921147 1.4550014 0.4043107 -0.83729148 -1.9039361 -2.4685249 -2.5328658 -2.3151541][-1.2811207 -1.277271 -1.1700045 -1.0290345 -0.83722878 -0.45681012 0.069599867 0.52053142 0.39806032 -0.31119609 -1.2433382 -2.1076338 -2.5480483 -2.5456722 -2.3151512][-1.0816699 -1.1399961 -1.1405816 -1.1919061 -1.3106407 -1.2122869 -0.8414979 -0.42289186 -0.40632033 -0.86962557 -1.5723503 -2.2239773 -2.5384312 -2.5175502 -2.3057485][-1.0790037 -1.1414564 -1.1922256 -1.3686703 -1.6307204 -1.6756859 -1.3658872 -0.924307 -0.81462 -1.151595 -1.708094 -2.2345734 -2.4882448 -2.4788928 -2.2962456][-1.1578424 -1.1896151 -1.2472769 -1.4401898 -1.7570693 -1.8624592 -1.5588577 -1.099295 -0.94010019 -1.2545426 -1.7641405 -2.2640383 -2.5367532 -2.5269055 -2.3302095][-1.0725405 -1.034826 -1.1022515 -1.2979951 -1.612656 -1.716627 -1.4087934 -0.96577621 -0.82585132 -1.1781876 -1.7385695 -2.3223059 -2.6376112 -2.6304111 -2.398998][-0.56305504 -0.55262387 -0.69605327 -0.94438112 -1.2411398 -1.3013785 -0.96384287 -0.5101645 -0.41540504 -0.85989535 -1.5729861 -2.2844937 -2.6605453 -2.6801825 -2.4403296][0.24832058 0.10006118 -0.21695089 -0.5933913 -0.900897 -0.86079323 -0.39942336 0.12051344 0.17043734 -0.43249583 -1.3462548 -2.2058058 -2.6563411 -2.6977515 -2.4453189][0.58696651 0.31119561 -0.11995339 -0.59058464 -0.86228836 -0.66479349 -0.052144289 0.56697631 0.63894916 -0.075907946 -1.1218519 -2.0968199 -2.645113 -2.7010787 -2.4412153]]...]
INFO - root - 2017-12-16 09:33:04.317641: step 36310, loss = 0.54, batch loss = 0.28 (49.1 examples/sec; 0.163 sec/batch; 13h:24m:26s remains)
INFO - root - 2017-12-16 09:33:05.993703: step 36320, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 13h:37m:42s remains)
INFO - root - 2017-12-16 09:33:07.647831: step 36330, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 13h:58m:57s remains)
INFO - root - 2017-12-16 09:33:09.334514: step 36340, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:39m:25s remains)
INFO - root - 2017-12-16 09:33:11.005594: step 36350, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 13h:50m:57s remains)
INFO - root - 2017-12-16 09:33:12.677815: step 36360, loss = 0.55, batch loss = 0.30 (45.3 examples/sec; 0.177 sec/batch; 14h:31m:45s remains)
INFO - root - 2017-12-16 09:33:14.326656: step 36370, loss = 0.57, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 13h:18m:33s remains)
INFO - root - 2017-12-16 09:33:15.980393: step 36380, loss = 0.60, batch loss = 0.35 (48.3 examples/sec; 0.166 sec/batch; 13h:37m:50s remains)
INFO - root - 2017-12-16 09:33:17.630386: step 36390, loss = 0.62, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 13h:47m:38s remains)
INFO - root - 2017-12-16 09:33:19.294202: step 36400, loss = 0.49, batch loss = 0.23 (46.0 examples/sec; 0.174 sec/batch; 14h:17m:47s remains)
2017-12-16 09:33:19.772049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18820119 -0.91379344 -1.2924632 -1.4892488 -1.8414694 -2.3474245 -2.7054284 -2.82351 -2.6746776 -2.4321582 -2.1845322 -1.9220786 -1.7937899 -1.9433768 -2.3011098][-0.62487447 -1.3323926 -1.6914897 -1.8944256 -2.2481587 -2.7389662 -3.0763655 -3.1428065 -2.9221168 -2.5865047 -2.237901 -1.9369541 -1.8584335 -2.0943348 -2.5417931][-1.5813468 -2.2082715 -2.5217662 -2.6954486 -2.9921076 -3.341289 -3.4886291 -3.3958898 -3.0791051 -2.6860559 -2.2556095 -1.8853496 -1.7368275 -1.9173567 -2.3879063][-2.4673643 -2.9592776 -3.2324479 -3.387253 -3.614604 -3.7963271 -3.6922622 -3.3936849 -3.0276716 -2.677922 -2.2616076 -1.848299 -1.5735462 -1.6039448 -1.9715474][-2.6557176 -3.0004463 -3.2616532 -3.4463825 -3.6096756 -3.5812373 -3.2321544 -2.7732754 -2.4343755 -2.2872887 -2.071537 -1.7067735 -1.330397 -1.1685166 -1.3901846][-2.0169058 -2.1878586 -2.4552417 -2.7264891 -2.8215897 -2.56747 -2.03072 -1.5683913 -1.4278626 -1.6069976 -1.6800821 -1.4178982 -0.98960066 -0.68008316 -0.79685211][-0.72192264 -0.78616846 -1.1207254 -1.5509427 -1.6260428 -1.1610117 -0.46382654 -0.0218122 -0.12789702 -0.69339645 -1.1048858 -1.0684929 -0.70963323 -0.36326456 -0.41562247][0.58583331 0.453274 0.034100056 -0.51659274 -0.55087209 0.14146185 1.0300841 1.4461651 1.0399854 0.13546729 -0.60165668 -0.84970772 -0.63801074 -0.25660181 -0.27295756][1.2833734 0.88212061 0.38275552 -0.10441351 -0.014438868 0.8491919 1.9267406 2.3386402 1.6792445 0.53245664 -0.38835597 -0.80614793 -0.64919949 -0.22847939 -0.24272513][1.3480489 0.56339264 -0.020869017 -0.36871743 -0.16271424 0.71294785 1.787323 2.1565981 1.3635869 0.13430285 -0.78350782 -1.1360238 -0.91107249 -0.45666015 -0.46672928][0.94869113 -0.21930909 -0.84545124 -1.0246637 -0.72820604 0.03342247 0.923753 1.1509113 0.31779695 -0.85001111 -1.6169026 -1.7583452 -1.4091333 -0.97070277 -0.95403445][0.025576591 -1.4625733 -2.0811048 -2.0085554 -1.5100187 -0.74203014 0.0021870136 0.11016226 -0.64645064 -1.6426036 -2.2078097 -2.1816351 -1.8406277 -1.5292498 -1.5141656][-1.2244619 -2.794591 -3.31465 -2.9968269 -2.2612092 -1.393351 -0.6858418 -0.61371505 -1.2512076 -2.0479882 -2.4236164 -2.2933977 -2.0164976 -1.8598588 -1.9033622][-2.3925653 -3.7721605 -4.0248456 -3.4210641 -2.4544451 -1.4905505 -0.83818674 -0.870322 -1.4691119 -2.1049395 -2.3184907 -2.1493998 -1.9467247 -1.9315532 -2.0959792][-2.7929261 -3.7348785 -3.6825507 -2.8886547 -1.8462666 -0.91805148 -0.47775424 -0.71615958 -1.3401939 -1.8988538 -2.0525846 -1.8985751 -1.7781639 -1.8724098 -2.1745315]]...]
INFO - root - 2017-12-16 09:33:21.426403: step 36410, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:31m:12s remains)
INFO - root - 2017-12-16 09:33:23.116700: step 36420, loss = 0.63, batch loss = 0.37 (45.2 examples/sec; 0.177 sec/batch; 14h:34m:01s remains)
INFO - root - 2017-12-16 09:33:24.780717: step 36430, loss = 0.57, batch loss = 0.31 (48.1 examples/sec; 0.166 sec/batch; 13h:41m:31s remains)
INFO - root - 2017-12-16 09:33:26.448056: step 36440, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 13h:34m:32s remains)
INFO - root - 2017-12-16 09:33:28.109412: step 36450, loss = 0.76, batch loss = 0.50 (48.5 examples/sec; 0.165 sec/batch; 13h:34m:32s remains)
INFO - root - 2017-12-16 09:33:29.740562: step 36460, loss = 0.49, batch loss = 0.23 (48.7 examples/sec; 0.164 sec/batch; 13h:31m:06s remains)
INFO - root - 2017-12-16 09:33:31.415580: step 36470, loss = 0.49, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:34m:39s remains)
INFO - root - 2017-12-16 09:33:33.116918: step 36480, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 13h:55m:21s remains)
INFO - root - 2017-12-16 09:33:34.785259: step 36490, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 13h:49m:58s remains)
INFO - root - 2017-12-16 09:33:36.439141: step 36500, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 13h:38m:04s remains)
2017-12-16 09:33:36.892330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9344172 -1.6840005 -1.357524 -1.119833 -0.91596556 -0.69560623 -0.61602271 -0.78841662 -1.1911137 -1.7561667 -2.2686362 -2.5233443 -2.5164046 -2.367964 -2.1989536][-2.1322913 -1.9886183 -1.7596257 -1.5896592 -1.4476838 -1.2506942 -1.0728285 -1.0896245 -1.3322169 -1.7451389 -2.1343088 -2.2688427 -2.1388309 -1.8750591 -1.6096561][-1.992099 -1.9732985 -1.8533645 -1.7822824 -1.6904128 -1.4581633 -1.1964518 -1.153863 -1.3597264 -1.7363658 -2.0777712 -2.1530161 -1.8675318 -1.3891146 -0.93354189][-1.558758 -1.6282709 -1.5578855 -1.53971 -1.4925958 -1.2507099 -0.89404309 -0.79652667 -1.0494379 -1.5127722 -1.9213045 -2.021064 -1.6447456 -0.9454447 -0.25046706][-1.0224894 -1.1008859 -1.0065503 -0.96879113 -0.97622383 -0.77854931 -0.34927034 -0.11990929 -0.33844471 -0.90731609 -1.5079572 -1.7886517 -1.4974995 -0.715505 0.16169715][-0.68031478 -0.73786283 -0.57620037 -0.47103357 -0.53740144 -0.4533565 -0.001922369 0.44582605 0.42944288 -0.11061192 -0.86497569 -1.3952236 -1.3400978 -0.67166042 0.24548149][-0.61225986 -0.56911683 -0.27748322 -0.10242987 -0.24050593 -0.34633875 -0.012641191 0.57411671 0.87968016 0.65692425 -0.064074278 -0.78932536 -1.0608923 -0.66927528 0.11420584][-0.66296911 -0.44504941 0.048583746 0.33073211 0.1235137 -0.23937416 -0.20125628 0.33966923 0.96009541 1.2163966 0.73770094 -0.076981068 -0.67412245 -0.68582094 -0.22459555][-0.86818194 -0.43454278 0.29611421 0.76957917 0.584265 -0.0086574554 -0.39132822 -0.16866136 0.51858974 1.1867344 1.1411798 0.47209406 -0.26213741 -0.63469517 -0.59661436][-1.3383569 -0.71191418 0.24832368 0.96592069 0.90110373 0.23750782 -0.46596062 -0.64213383 -0.10079718 0.78257442 1.1542509 0.79688883 0.11095047 -0.4644649 -0.79383314][-1.9010648 -1.2223204 -0.15891409 0.74558234 0.89622664 0.35510588 -0.41642761 -0.850191 -0.54722011 0.31384134 0.94323444 0.91585445 0.41759372 -0.19175792 -0.71844983][-2.297394 -1.7647012 -0.81700218 0.12438798 0.48070335 0.17802143 -0.45499766 -0.93875229 -0.81540048 -0.088761806 0.64208245 0.89330029 0.64907813 0.14861512 -0.44034111][-2.5242307 -2.2733712 -1.6315728 -0.86185932 -0.41231287 -0.45744634 -0.83680093 -1.176836 -1.1038193 -0.53899062 0.166919 0.65316558 0.74662757 0.49475837 -0.024494648][-2.6953061 -2.7078948 -2.4142032 -1.9281877 -1.5157927 -1.388162 -1.5287722 -1.6901255 -1.5804675 -1.1013666 -0.42444766 0.2093823 0.60613513 0.65389276 0.32403326][-2.7572393 -2.9312427 -2.8945346 -2.6717041 -2.4019418 -2.2374444 -2.2335806 -2.2647152 -2.1295075 -1.7240136 -1.1161945 -0.46544814 0.083492994 0.39210773 0.3279984]]...]
INFO - root - 2017-12-16 09:33:38.562157: step 36510, loss = 0.48, batch loss = 0.22 (45.3 examples/sec; 0.176 sec/batch; 14h:30m:36s remains)
INFO - root - 2017-12-16 09:33:40.205248: step 36520, loss = 0.58, batch loss = 0.32 (50.3 examples/sec; 0.159 sec/batch; 13h:04m:38s remains)
INFO - root - 2017-12-16 09:33:41.864060: step 36530, loss = 0.61, batch loss = 0.35 (49.4 examples/sec; 0.162 sec/batch; 13h:19m:33s remains)
INFO - root - 2017-12-16 09:33:43.558536: step 36540, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:47m:06s remains)
INFO - root - 2017-12-16 09:33:45.260579: step 36550, loss = 0.53, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 14h:02m:23s remains)
INFO - root - 2017-12-16 09:33:46.970987: step 36560, loss = 0.57, batch loss = 0.31 (45.8 examples/sec; 0.175 sec/batch; 14h:22m:12s remains)
INFO - root - 2017-12-16 09:33:48.707801: step 36570, loss = 0.63, batch loss = 0.38 (46.7 examples/sec; 0.171 sec/batch; 14h:04m:20s remains)
INFO - root - 2017-12-16 09:33:50.429354: step 36580, loss = 0.58, batch loss = 0.32 (46.9 examples/sec; 0.171 sec/batch; 14h:01m:03s remains)
INFO - root - 2017-12-16 09:33:52.098377: step 36590, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 13h:48m:11s remains)
INFO - root - 2017-12-16 09:33:53.815338: step 36600, loss = 0.50, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 14h:04m:34s remains)
2017-12-16 09:33:54.332510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6180146 -3.0925941 -2.3927665 -2.2518086 -2.6440248 -3.0941808 -3.376189 -3.5345368 -3.5948486 -3.4516535 -2.9450812 -2.2432449 -1.7112304 -1.7423025 -2.42607][-4.1478868 -3.4964991 -2.7083478 -2.5587459 -2.9731307 -3.4248011 -3.7149982 -3.9843121 -4.1709423 -4.0948572 -3.6013029 -2.8881161 -2.3517146 -2.2621434 -2.7349565][-4.1920156 -3.3651247 -2.492569 -2.2831495 -2.5830283 -2.9447069 -3.2137337 -3.5276167 -3.8251567 -3.8923085 -3.5614429 -2.9567297 -2.4706123 -2.3101072 -2.5910408][-3.6006799 -2.5792127 -1.6216714 -1.3744053 -1.5005565 -1.5993972 -1.728101 -2.0533433 -2.5374563 -2.841764 -2.7162471 -2.3357668 -1.9959698 -1.8474557 -2.0157433][-2.8436284 -1.728435 -0.75484788 -0.41188717 -0.32531834 -0.025386095 0.24423504 0.036627054 -0.69831324 -1.3446932 -1.5943542 -1.5104666 -1.3539393 -1.2264099 -1.3298987][-1.9882685 -0.7883631 0.11220455 0.4468708 0.80520678 1.5615354 2.3870506 2.3573179 1.297327 0.15556598 -0.57918417 -0.80926406 -0.73755026 -0.54981053 -0.59279621][-1.036956 0.29628539 1.2020741 1.6656647 2.2526464 3.2952242 4.5348387 4.7015324 3.2896309 1.7233377 0.6594739 0.17826891 0.17993212 0.32249856 0.19222999][-0.37590885 1.0301516 2.078053 2.5722742 3.1268454 4.2110472 5.5864239 5.9083319 4.551878 2.8609872 1.6415548 1.0910897 1.1093674 1.2056637 0.877964][-0.78554904 0.48500752 1.5557888 1.9975915 2.2637067 3.002986 4.0086141 4.2392292 3.3810887 2.067822 1.0571012 0.7367394 0.96240973 1.1269534 0.62861514][-1.705888 -0.60832608 0.44080353 0.83249235 0.91117144 1.2629349 1.8373885 1.9858451 1.561002 0.74748397 0.11214232 0.087823391 0.48810673 0.73442245 0.1541636][-2.8092105 -1.9348143 -0.89820194 -0.45819545 -0.54144788 -0.50785291 -0.23555946 -0.11609602 -0.30634856 -0.66705215 -0.889977 -0.61293769 -0.056036472 0.1454072 -0.49815834][-3.8969584 -3.2948842 -2.3819673 -1.9929986 -2.2329078 -2.4357169 -2.3696415 -2.2707105 -2.2506006 -2.293144 -2.2099535 -1.7470094 -1.1270864 -0.97844994 -1.6117949][-4.67825 -4.2849331 -3.5712671 -3.330554 -3.6322277 -3.9480131 -4.0240221 -3.9476519 -3.8278563 -3.7097178 -3.4642293 -2.9724321 -2.4083877 -2.2678747 -2.760663][-4.7747755 -4.4981947 -3.95812 -3.8016925 -4.0858979 -4.3867817 -4.4820175 -4.4394083 -4.3504457 -4.2302485 -3.9936924 -3.5721393 -3.1309581 -2.9740345 -3.2441995][-4.1094837 -3.8577809 -3.4440217 -3.3569655 -3.5989451 -3.8379016 -3.9179666 -3.8966722 -3.8486273 -3.7649717 -3.6001225 -3.3170874 -3.0101097 -2.8342469 -2.8996429]]...]
INFO - root - 2017-12-16 09:33:55.998469: step 36610, loss = 0.63, batch loss = 0.37 (48.6 examples/sec; 0.164 sec/batch; 13h:31m:04s remains)
INFO - root - 2017-12-16 09:33:57.707754: step 36620, loss = 0.56, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 14h:08m:01s remains)
INFO - root - 2017-12-16 09:33:59.497182: step 36630, loss = 0.57, batch loss = 0.31 (32.4 examples/sec; 0.247 sec/batch; 20h:17m:47s remains)
INFO - root - 2017-12-16 09:34:01.172291: step 36640, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 13h:30m:22s remains)
INFO - root - 2017-12-16 09:34:02.853462: step 36650, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:37m:34s remains)
INFO - root - 2017-12-16 09:34:04.520433: step 36660, loss = 0.53, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:37m:42s remains)
INFO - root - 2017-12-16 09:34:06.207626: step 36670, loss = 0.49, batch loss = 0.23 (46.4 examples/sec; 0.172 sec/batch; 14h:09m:16s remains)
INFO - root - 2017-12-16 09:34:07.935946: step 36680, loss = 0.51, batch loss = 0.25 (47.2 examples/sec; 0.170 sec/batch; 13h:56m:09s remains)
INFO - root - 2017-12-16 09:34:09.659654: step 36690, loss = 0.59, batch loss = 0.34 (46.8 examples/sec; 0.171 sec/batch; 14h:02m:54s remains)
INFO - root - 2017-12-16 09:34:11.343674: step 36700, loss = 0.50, batch loss = 0.24 (47.5 examples/sec; 0.169 sec/batch; 13h:51m:04s remains)
2017-12-16 09:34:11.811465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5231936 -2.6909683 -2.8254619 -2.8565567 -2.7850502 -2.7720761 -2.8644352 -2.9378126 -2.9013722 -2.7522981 -2.6645534 -2.7303281 -2.8822112 -2.9860132 -3.0531764][-2.784416 -3.110122 -3.4505873 -3.691185 -3.7556286 -3.7691064 -3.81205 -3.780534 -3.6138098 -3.4389849 -3.4559169 -3.707499 -4.0593748 -4.2963276 -4.4134455][-2.3131988 -2.776072 -3.3461998 -3.8298807 -4.0744772 -4.1649442 -4.13974 -3.9269133 -3.5356274 -3.2636161 -3.3993964 -3.8953354 -4.5202661 -4.9855318 -5.2508597][-1.4017483 -1.8367012 -2.4911766 -3.1078525 -3.4722204 -3.6477222 -3.5891623 -3.2270367 -2.6015325 -2.120821 -2.3326848 -3.0864539 -4.049314 -4.7934942 -5.1752853][-0.61687064 -0.82216048 -1.3517206 -1.8800414 -2.1727433 -2.3479598 -2.2994308 -1.9207706 -1.2147474 -0.58276594 -0.71456206 -1.5881624 -2.8219814 -3.811347 -4.2933846][-0.39313698 -0.35199356 -0.63452804 -0.87152123 -0.9254483 -0.9968034 -0.9473207 -0.60293841 0.0051648617 0.61843467 0.72414732 -0.02421999 -1.3231825 -2.3903003 -2.8962984][-1.030648 -0.89132559 -0.88595366 -0.75319445 -0.56681406 -0.528788 -0.45661592 -0.21446633 0.23598766 0.78491807 1.1264765 0.75777483 -0.24197602 -1.0684181 -1.411231][-1.9850669 -1.913537 -1.850904 -1.5773457 -1.2369342 -1.072787 -0.95954454 -0.80699813 -0.60744107 -0.21683359 0.29740024 0.34827042 -0.16377878 -0.55705988 -0.62383652][-2.5695682 -2.6312006 -2.7284482 -2.6162481 -2.3770046 -2.1838162 -2.0394278 -1.934057 -1.9384276 -1.7818096 -1.3346314 -1.0690118 -1.1298685 -1.0501817 -0.7558918][-2.4917622 -2.7979152 -3.2012992 -3.4211831 -3.4204555 -3.3133569 -3.1899383 -3.1173229 -3.2560341 -3.2320328 -2.897861 -2.5710781 -2.3714736 -1.985034 -1.4469955][-1.9052646 -2.5576043 -3.2708552 -3.7574611 -3.9454675 -3.9351969 -3.8330393 -3.7924705 -3.9517269 -3.9484484 -3.5943561 -3.1798894 -2.8218915 -2.3312256 -1.7437196][-1.2421058 -2.057796 -2.9626813 -3.5931079 -3.7881155 -3.7444165 -3.6345859 -3.6517296 -3.8174987 -3.836772 -3.4474659 -2.9157498 -2.4122646 -1.9809493 -1.5176671][-0.79580581 -1.4924726 -2.3271658 -2.9220481 -3.0476167 -2.9249797 -2.7780976 -2.8141813 -3.0107806 -3.071419 -2.7261086 -2.1728339 -1.6995091 -1.3877139 -1.194744][-0.39652514 -0.76142991 -1.3248258 -1.7354476 -1.7813029 -1.6070628 -1.4569402 -1.5134816 -1.732704 -1.8118538 -1.5486624 -1.1129495 -0.75127327 -0.58583951 -0.64766681][0.068879128 -0.043271542 -0.30058241 -0.46278965 -0.40827096 -0.22994876 -0.11925554 -0.19418001 -0.3888514 -0.49354625 -0.3689487 -0.12462854 0.072276831 0.11204219 -0.095083952]]...]
INFO - root - 2017-12-16 09:34:13.489242: step 36710, loss = 0.57, batch loss = 0.31 (48.1 examples/sec; 0.166 sec/batch; 13h:40m:25s remains)
INFO - root - 2017-12-16 09:34:15.160679: step 36720, loss = 0.48, batch loss = 0.22 (48.2 examples/sec; 0.166 sec/batch; 13h:37m:45s remains)
INFO - root - 2017-12-16 09:34:16.858864: step 36730, loss = 0.59, batch loss = 0.33 (47.6 examples/sec; 0.168 sec/batch; 13h:48m:36s remains)
INFO - root - 2017-12-16 09:34:18.534744: step 36740, loss = 0.47, batch loss = 0.21 (47.1 examples/sec; 0.170 sec/batch; 13h:56m:52s remains)
INFO - root - 2017-12-16 09:34:20.194351: step 36750, loss = 0.48, batch loss = 0.23 (48.3 examples/sec; 0.166 sec/batch; 13h:36m:42s remains)
INFO - root - 2017-12-16 09:34:21.875185: step 36760, loss = 0.62, batch loss = 0.37 (47.8 examples/sec; 0.167 sec/batch; 13h:44m:24s remains)
INFO - root - 2017-12-16 09:34:23.593860: step 36770, loss = 0.64, batch loss = 0.38 (43.8 examples/sec; 0.183 sec/batch; 15h:00m:51s remains)
INFO - root - 2017-12-16 09:34:25.293374: step 36780, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 13h:52m:23s remains)
INFO - root - 2017-12-16 09:34:26.984622: step 36790, loss = 0.59, batch loss = 0.33 (47.0 examples/sec; 0.170 sec/batch; 13h:58m:40s remains)
INFO - root - 2017-12-16 09:34:28.675832: step 36800, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 13h:38m:35s remains)
2017-12-16 09:34:29.232002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7513056 -3.5617423 -3.5925927 -3.8027334 -3.9358575 -3.9279177 -3.8586581 -3.7391486 -3.3665671 -3.0576138 -3.2553492 -3.4212794 -3.0561738 -2.2716563 -1.2802082][-3.1783752 -2.9833925 -3.0934248 -3.3840675 -3.5034528 -3.4053309 -3.2625124 -3.1260095 -2.7471964 -2.4890566 -2.9045167 -3.3026128 -2.9967198 -2.1125543 -0.90720534][-2.465836 -2.4338925 -2.7254543 -3.0919847 -3.132884 -2.8775368 -2.6255133 -2.4056654 -1.9469416 -1.6721879 -2.3787508 -3.2206545 -3.2451985 -2.4238005 -1.1125987][-1.5406996 -1.772486 -2.3133893 -2.7812996 -2.7468793 -2.3347206 -1.9204334 -1.5091059 -0.80769813 -0.40327537 -1.366379 -2.7407336 -3.3141618 -2.8043144 -1.5512972][-0.59962296 -0.95496857 -1.6189235 -2.2009647 -2.2823088 -1.8682145 -1.2336506 -0.46115494 0.63546252 1.2670255 0.045839071 -1.8875126 -3.0691535 -2.9863343 -1.8554832][0.076457262 -0.19835901 -0.84036636 -1.5536206 -1.7918441 -1.3654063 -0.36489177 0.95956373 2.4631374 3.1751912 1.5292783 -0.95815229 -2.64189 -2.8664088 -1.8567863][0.37907767 0.30557489 -0.22308993 -0.99964643 -1.3421457 -0.78242779 0.67979407 2.6093166 4.4560146 5.0071678 2.7845738 -0.17460489 -2.1648757 -2.5310853 -1.5816481][0.35638 0.55062556 0.18204951 -0.67137814 -1.14362 -0.45266891 1.3353868 3.6314185 5.6789 5.86557 3.2450683 0.13968921 -1.8471023 -2.1627982 -1.2497333][-0.0084583759 0.36654019 0.055344582 -0.92402458 -1.5474335 -0.90537977 0.83031607 3.0925357 5.07248 5.0469656 2.5444267 -0.25952983 -1.9114652 -2.0214725 -1.0669237][-0.30465841 -0.010937452 -0.41341352 -1.492979 -2.3050859 -1.9637603 -0.6132195 1.3579383 3.1392376 3.1953957 1.1685817 -1.0570546 -2.2236824 -2.0588331 -1.0259622][-0.47874713 -0.40452278 -0.91672277 -2.0107713 -2.9090564 -2.9256442 -2.0820434 -0.53049231 0.96201634 1.1550107 -0.2769711 -1.8565402 -2.5373225 -2.0680437 -0.94487214][-0.74910212 -0.99902141 -1.5042673 -2.3634086 -3.1256332 -3.3628793 -2.9083209 -1.7439909 -0.53166294 -0.28270411 -1.2764665 -2.3370929 -2.5651219 -1.7751828 -0.50563455][-1.0636694 -1.6121635 -2.1256318 -2.6766317 -3.2105503 -3.4638405 -3.1778703 -2.2555745 -1.2541922 -1.0184511 -1.7369938 -2.4137282 -2.2714183 -1.1749115 0.23931217][-1.5186961 -2.2321188 -2.7057474 -3.0082638 -3.3474061 -3.5299778 -3.2636635 -2.5175033 -1.7127919 -1.5476383 -2.1144562 -2.5560994 -2.1622307 -0.92297149 0.53574371][-2.2443595 -2.9193115 -3.249269 -3.3778157 -3.5580106 -3.5942569 -3.3431945 -2.7467818 -2.1553776 -2.1315162 -2.6662641 -3.0305357 -2.6153498 -1.4693651 -0.15003848]]...]
INFO - root - 2017-12-16 09:34:30.899259: step 36810, loss = 0.65, batch loss = 0.40 (47.0 examples/sec; 0.170 sec/batch; 13h:58m:15s remains)
INFO - root - 2017-12-16 09:34:32.594976: step 36820, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 13h:43m:39s remains)
INFO - root - 2017-12-16 09:34:34.292348: step 36830, loss = 0.57, batch loss = 0.31 (46.5 examples/sec; 0.172 sec/batch; 14h:08m:40s remains)
INFO - root - 2017-12-16 09:34:36.003169: step 36840, loss = 0.49, batch loss = 0.23 (47.1 examples/sec; 0.170 sec/batch; 13h:56m:27s remains)
INFO - root - 2017-12-16 09:34:37.708323: step 36850, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 13h:56m:17s remains)
INFO - root - 2017-12-16 09:34:39.460721: step 36860, loss = 0.51, batch loss = 0.25 (45.4 examples/sec; 0.176 sec/batch; 14h:28m:46s remains)
INFO - root - 2017-12-16 09:34:41.146552: step 36870, loss = 0.47, batch loss = 0.21 (47.8 examples/sec; 0.167 sec/batch; 13h:44m:35s remains)
INFO - root - 2017-12-16 09:34:42.833898: step 36880, loss = 0.54, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 13h:49m:51s remains)
INFO - root - 2017-12-16 09:34:44.490611: step 36890, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 13h:32m:54s remains)
INFO - root - 2017-12-16 09:34:46.179563: step 36900, loss = 0.49, batch loss = 0.23 (46.7 examples/sec; 0.171 sec/batch; 14h:03m:55s remains)
2017-12-16 09:34:46.670495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.96863556 -1.1490077 -1.2209046 -1.1487609 -0.94093239 -0.61159945 -0.32406402 -0.42884207 -0.9595412 -1.4598961 -1.5210898 -1.1615441 -0.70703256 -0.49041605 -0.58835435][-1.7212201 -1.9806727 -2.0511303 -1.8331335 -1.3120838 -0.5851469 0.062042236 0.042303324 -0.70502532 -1.5200942 -1.8592854 -1.5985382 -1.0068959 -0.58774054 -0.56779456][-2.8969526 -3.1981959 -3.2353768 -2.809587 -1.915341 -0.75069249 0.1732285 0.15011477 -0.81602645 -1.9976897 -2.7526891 -2.7609274 -2.1714792 -1.5293477 -1.3154196][-3.911221 -4.2564678 -4.2905688 -3.7579298 -2.6214688 -1.1964881 -0.034654856 -0.016815662 -1.1311367 -2.5726638 -3.6862283 -4.0706367 -3.6360142 -2.8836403 -2.4906888][-4.3208065 -4.7297273 -4.8361826 -4.3088636 -3.0869956 -1.515799 -0.23303819 -0.16292977 -1.3395575 -2.9200954 -4.278995 -4.9528427 -4.6863055 -3.910028 -3.401732][-4.2505665 -4.6589251 -4.779532 -4.2747622 -2.982183 -1.2175233 0.22897935 0.34123921 -0.9496336 -2.7634299 -4.3727913 -5.2044754 -4.9902954 -4.2043805 -3.6963992][-3.9185903 -4.232161 -4.2620783 -3.6659317 -2.2030385 -0.20247364 1.4352427 1.4809141 -0.13127017 -2.304384 -4.08359 -4.8377132 -4.5074782 -3.7052097 -3.2974033][-3.4260736 -3.6190028 -3.4861407 -2.7304 -1.0905269 1.1439927 2.9179814 2.675781 0.57636213 -1.8794303 -3.5935788 -4.0303316 -3.4057078 -2.6189926 -2.4618347][-2.8561084 -2.9393468 -2.6703992 -1.7923794 -0.099767208 2.1178038 3.7219975 3.0915139 0.70229006 -1.7414403 -3.0711336 -2.9906838 -2.0604746 -1.35046 -1.5632228][-2.157269 -2.1852684 -1.8861706 -1.0726837 0.48641062 2.4384429 3.6086853 2.7130377 0.32382154 -1.8391023 -2.67946 -2.0942841 -0.91578674 -0.34387898 -0.94643664][-1.2368441 -1.3510685 -1.2516165 -0.72108328 0.49741411 2.0626724 2.8136828 1.7571752 -0.45977724 -2.2563 -2.6373281 -1.7203177 -0.44989455 -0.060542345 -0.91421711][-0.25410271 -0.56776249 -0.84467793 -0.720024 0.036743879 1.1175144 1.4337041 0.255769 -1.6926799 -3.0490439 -3.0921278 -2.0677266 -0.85428989 -0.57309747 -1.3701825][0.66526675 0.13300323 -0.5478344 -0.90276051 -0.63334894 -0.080517769 -0.12595272 -1.3333867 -2.9525635 -3.8969295 -3.7169874 -2.7054949 -1.5985744 -1.3198701 -1.9668752][1.2421074 0.53825569 -0.42493796 -1.159337 -1.3197684 -1.1701741 -1.4418498 -2.5381083 -3.7548747 -4.3201766 -3.984571 -3.0601382 -2.1514113 -1.8918415 -2.3836229][1.1111653 0.37933588 -0.65103257 -1.5639846 -1.9729004 -2.0810132 -2.4168811 -3.1937776 -3.9621882 -4.1842842 -3.7921767 -3.1092134 -2.4784107 -2.2884538 -2.5879192]]...]
INFO - root - 2017-12-16 09:34:48.359714: step 36910, loss = 0.51, batch loss = 0.25 (48.0 examples/sec; 0.167 sec/batch; 13h:40m:25s remains)
INFO - root - 2017-12-16 09:34:50.036647: step 36920, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 13h:43m:47s remains)
INFO - root - 2017-12-16 09:34:51.688283: step 36930, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 13h:38m:34s remains)
INFO - root - 2017-12-16 09:34:53.388862: step 36940, loss = 0.51, batch loss = 0.26 (44.7 examples/sec; 0.179 sec/batch; 14h:41m:58s remains)
INFO - root - 2017-12-16 09:34:55.077552: step 36950, loss = 0.61, batch loss = 0.35 (47.6 examples/sec; 0.168 sec/batch; 13h:47m:01s remains)
INFO - root - 2017-12-16 09:34:56.772852: step 36960, loss = 0.51, batch loss = 0.25 (45.2 examples/sec; 0.177 sec/batch; 14h:30m:54s remains)
INFO - root - 2017-12-16 09:34:58.455114: step 36970, loss = 0.55, batch loss = 0.29 (47.7 examples/sec; 0.168 sec/batch; 13h:46m:55s remains)
INFO - root - 2017-12-16 09:35:00.156579: step 36980, loss = 0.49, batch loss = 0.23 (46.9 examples/sec; 0.171 sec/batch; 13h:59m:54s remains)
INFO - root - 2017-12-16 09:35:01.859955: step 36990, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 13h:55m:10s remains)
INFO - root - 2017-12-16 09:35:03.547287: step 37000, loss = 0.45, batch loss = 0.19 (47.1 examples/sec; 0.170 sec/batch; 13h:56m:11s remains)
2017-12-16 09:35:04.015538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.928194 -1.8697481 -1.7939235 -1.7257868 -1.6891038 -1.6935757 -1.7098694 -1.7179205 -1.7201674 -1.7234412 -1.7270451 -1.7518275 -1.7909449 -1.8195163 -1.9022975][-1.9792433 -1.9694067 -1.9282897 -1.8895442 -1.8869789 -1.9345742 -1.9916776 -2.0270796 -2.0443521 -2.0527394 -2.0492272 -2.0624993 -2.0733845 -2.0481133 -2.1187987][-2.1112077 -2.1558502 -2.146368 -2.1257958 -2.1532929 -2.2408421 -2.3447151 -2.4306872 -2.490629 -2.5310707 -2.5414736 -2.5501847 -2.5306258 -2.4482186 -2.5024848][-2.405628 -2.476522 -2.4524555 -2.3880148 -2.3692024 -2.424655 -2.5109043 -2.6039681 -2.6922524 -2.7723467 -2.8169069 -2.8469775 -2.8301079 -2.7497246 -2.8472123][-2.75188 -2.8124256 -2.7319396 -2.5657485 -2.4443922 -2.409658 -2.4309466 -2.500545 -2.6005867 -2.7269959 -2.8300552 -2.9105814 -2.937809 -2.9230471 -3.1098897][-2.9732623 -2.9925663 -2.8387368 -2.5594397 -2.3137503 -2.1748614 -2.1281877 -2.1609805 -2.2710063 -2.4551556 -2.632391 -2.7914927 -2.8859725 -2.9600902 -3.254261][-2.8687861 -2.7602379 -2.4951069 -2.1018384 -1.73593 -1.5005459 -1.3871852 -1.3829515 -1.5070603 -1.7335826 -1.958584 -2.156765 -2.2902372 -2.4419074 -2.8662438][-2.3666852 -2.053442 -1.6487367 -1.1629983 -0.70375419 -0.40639699 -0.28656459 -0.28462672 -0.44847262 -0.69959831 -0.93715465 -1.0875069 -1.1246974 -1.2668356 -1.8191769][-1.7260988 -1.1801471 -0.64672315 -0.0927484 0.41779661 0.71677709 0.78067589 0.74000359 0.5730021 0.35915804 0.17022538 0.15181923 0.3307991 0.29294991 -0.37427282][-1.255137 -0.514405 0.115448 0.7226975 1.2744803 1.541368 1.5178411 1.3872151 1.2080445 1.0504181 0.95939374 1.146199 1.611598 1.6977212 0.94494534][-1.0650154 -0.19541359 0.50496197 1.1767883 1.7893569 2.0707524 2.0057814 1.7991421 1.6101239 1.5067191 1.505199 1.8484924 2.5061347 2.640413 1.7998888][-1.3305247 -0.47751808 0.21820784 0.87491679 1.4794061 1.7639754 1.6902874 1.4667265 1.3003783 1.2208889 1.2172503 1.5410881 2.1828306 2.3109276 1.495342][-2.1238098 -1.4642789 -0.88183928 -0.32305288 0.18130255 0.42500186 0.36487126 0.16333532 0.023578644 -0.035129309 -0.025304794 0.20659494 0.69122386 0.7840066 0.13829613][-2.9252167 -2.5484638 -2.1281948 -1.7051811 -1.332359 -1.1541524 -1.2057583 -1.3555709 -1.4613576 -1.4985046 -1.4818966 -1.3128252 -0.971946 -0.88954544 -1.3113652][-3.3242168 -3.1872666 -2.9444776 -2.6605711 -2.4123013 -2.3003697 -2.3404226 -2.428998 -2.492451 -2.5152245 -2.5084767 -2.4218054 -2.2424097 -2.1844585 -2.3988957]]...]
INFO - root - 2017-12-16 09:35:05.744647: step 37010, loss = 0.65, batch loss = 0.39 (47.9 examples/sec; 0.167 sec/batch; 13h:42m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:35:07.433373: step 37020, loss = 0.62, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 13h:44m:48s remains)
INFO - root - 2017-12-16 09:35:09.126615: step 37030, loss = 0.55, batch loss = 0.29 (47.5 examples/sec; 0.168 sec/batch; 13h:49m:26s remains)
INFO - root - 2017-12-16 09:35:10.822168: step 37040, loss = 0.50, batch loss = 0.24 (44.3 examples/sec; 0.181 sec/batch; 14h:49m:57s remains)
INFO - root - 2017-12-16 09:35:12.481761: step 37050, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 13h:21m:30s remains)
INFO - root - 2017-12-16 09:35:14.174711: step 37060, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:39m:26s remains)
INFO - root - 2017-12-16 09:35:15.862925: step 37070, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 13h:26m:49s remains)
INFO - root - 2017-12-16 09:35:17.552654: step 37080, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 13h:37m:58s remains)
INFO - root - 2017-12-16 09:35:19.261559: step 37090, loss = 0.51, batch loss = 0.25 (47.2 examples/sec; 0.170 sec/batch; 13h:54m:44s remains)
INFO - root - 2017-12-16 09:35:20.944759: step 37100, loss = 0.62, batch loss = 0.36 (49.8 examples/sec; 0.161 sec/batch; 13h:10m:45s remains)
2017-12-16 09:35:21.428498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1747205 -3.0587342 -3.0064945 -2.9235053 -2.7782311 -2.6559415 -2.7686887 -3.0440083 -3.2384217 -3.2469926 -3.0569353 -2.8721805 -2.9365015 -3.2427878 -3.4567008][-3.2829952 -3.1760073 -3.0978436 -2.8916674 -2.5416257 -2.26825 -2.3946192 -2.795733 -3.067867 -3.0584552 -2.7306621 -2.3864784 -2.3494432 -2.6534555 -2.9626448][-3.1788075 -3.1399107 -3.0597935 -2.8485479 -2.3928173 -1.9610387 -2.039058 -2.4905427 -2.8101754 -2.8208177 -2.4346752 -1.9138441 -1.6727653 -1.8605151 -2.1705387][-2.6366842 -2.6840956 -2.63642 -2.5320532 -2.149467 -1.6504147 -1.6807106 -2.2200608 -2.6869328 -2.7893043 -2.5024233 -1.9481125 -1.5491192 -1.6159973 -1.9219704][-1.7831289 -1.8035483 -1.7138195 -1.7246857 -1.5315056 -1.1042377 -1.1222254 -1.6476426 -2.2056334 -2.4584424 -2.4791071 -2.2440209 -1.9641639 -2.0699773 -2.4086361][-0.98170912 -0.91275394 -0.765779 -0.90618432 -0.94181943 -0.65331817 -0.55447054 -0.8493886 -1.2546458 -1.5858676 -1.9449544 -2.181596 -2.2763062 -2.5975542 -3.0019553][-0.39937186 -0.24599695 -0.089732885 -0.4053005 -0.70410264 -0.51649547 -0.099938393 0.044793606 -0.059684038 -0.32740498 -0.8773011 -1.5407212 -2.1285605 -2.7048805 -3.1781023][-0.37283802 -0.17054081 -0.076491594 -0.5177232 -0.91183639 -0.54685462 0.27943277 0.9672606 1.2825539 1.2126517 0.6330862 -0.34014821 -1.3534815 -2.2114227 -2.7816923][-0.87684727 -0.71104062 -0.74274027 -1.2351383 -1.4692768 -0.84204376 0.3599987 1.4746037 2.108263 2.2995553 1.8865762 0.89459658 -0.31004596 -1.3201098 -1.9476805][-1.6378586 -1.5684056 -1.8287821 -2.3482809 -2.4145644 -1.6900171 -0.42879534 0.77224088 1.5163743 1.9012489 1.8835058 1.2747982 0.25645328 -0.65998781 -1.2410731][-2.1340227 -2.1867871 -2.6701856 -3.2412748 -3.2946515 -2.7322218 -1.7325199 -0.72749472 -0.052210331 0.43864822 0.76633239 0.61735606 -0.053748369 -0.71113074 -1.1787915][-1.9304729 -2.0732789 -2.7399971 -3.4326582 -3.6423762 -3.4055698 -2.8955293 -2.3115067 -1.8306499 -1.3940556 -0.92970955 -0.77218866 -1.0779455 -1.4369891 -1.7458372][-1.4624104 -1.6340873 -2.3196881 -2.9857447 -3.3351433 -3.5101855 -3.5556781 -3.46677 -3.2747068 -3.0132227 -2.627003 -2.3595026 -2.3698111 -2.4249642 -2.488848][-1.1794207 -1.2949295 -1.8466868 -2.2861102 -2.6213639 -3.1409297 -3.7041411 -4.0484848 -4.1534209 -4.1299391 -3.933789 -3.6996126 -3.5370822 -3.2896147 -3.0600822][-1.1121641 -1.191538 -1.5627433 -1.6787516 -1.8019695 -2.4577458 -3.3552759 -3.9738572 -4.306879 -4.5158453 -4.5042629 -4.3269968 -4.054872 -3.5528612 -3.10524]]...]
INFO - root - 2017-12-16 09:35:23.114274: step 37110, loss = 0.62, batch loss = 0.36 (47.2 examples/sec; 0.169 sec/batch; 13h:53m:51s remains)
INFO - root - 2017-12-16 09:35:24.792256: step 37120, loss = 0.49, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 13h:51m:30s remains)
INFO - root - 2017-12-16 09:35:26.472328: step 37130, loss = 0.72, batch loss = 0.46 (48.6 examples/sec; 0.165 sec/batch; 13h:30m:58s remains)
INFO - root - 2017-12-16 09:35:28.143175: step 37140, loss = 0.53, batch loss = 0.27 (46.9 examples/sec; 0.171 sec/batch; 13h:59m:44s remains)
INFO - root - 2017-12-16 09:35:29.836145: step 37150, loss = 0.59, batch loss = 0.34 (48.0 examples/sec; 0.167 sec/batch; 13h:41m:13s remains)
INFO - root - 2017-12-16 09:35:31.549730: step 37160, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 14h:12m:00s remains)
INFO - root - 2017-12-16 09:35:33.219139: step 37170, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 13h:47m:49s remains)
INFO - root - 2017-12-16 09:35:34.923520: step 37180, loss = 0.59, batch loss = 0.33 (46.8 examples/sec; 0.171 sec/batch; 14h:01m:48s remains)
INFO - root - 2017-12-16 09:35:36.614109: step 37190, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 14h:11m:25s remains)
INFO - root - 2017-12-16 09:35:38.290097: step 37200, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 13h:42m:59s remains)
2017-12-16 09:35:38.792457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8052561 -1.8201791 -1.8524671 -1.8748143 -1.8140082 -1.6948349 -1.6046988 -1.6172605 -1.8021179 -2.0492506 -2.1711783 -2.1337712 -2.0246568 -1.940619 -1.9027858][-1.5247154 -1.5641325 -1.635576 -1.677763 -1.55822 -1.3309027 -1.1339457 -1.138476 -1.4639974 -1.9057075 -2.15317 -2.1430678 -2.0118406 -1.883441 -1.8095827][-1.304108 -1.3995351 -1.5226812 -1.546036 -1.3188157 -0.86232328 -0.46824837 -0.45774281 -0.95231748 -1.6744525 -2.1475294 -2.2317843 -2.0892193 -1.8986211 -1.773064][-1.33964 -1.4875305 -1.6237739 -1.5754964 -1.1851293 -0.42397547 0.32402563 0.44377804 -0.29213715 -1.3634806 -2.1703219 -2.4412177 -2.32012 -2.0670111 -1.8640965][-1.523294 -1.6688316 -1.7707297 -1.636777 -1.0455235 0.034471273 1.2815442 1.7008379 0.79191041 -0.74671149 -2.0046661 -2.5961895 -2.6154661 -2.3720927 -2.1042485][-1.7009277 -1.8261464 -1.8672007 -1.6659795 -0.910282 0.51100707 2.3253424 3.2413123 2.1892817 0.1599524 -1.6155505 -2.6217701 -2.9029751 -2.7427125 -2.4402158][-1.7991283 -1.9005771 -1.8842466 -1.6541872 -0.78758609 0.87555265 3.2630012 4.8013725 3.3865087 0.93741584 -1.1859218 -2.5474656 -3.0824249 -3.0348611 -2.7698395][-1.8420498 -1.9333436 -1.8920659 -1.6558568 -0.77841568 0.93083668 3.5172513 5.2824993 3.7197821 1.2414386 -0.88486087 -2.3401749 -3.0342512 -3.15448 -3.0013814][-1.8708223 -1.9715903 -1.9563626 -1.7670288 -1.0266434 0.49475098 2.6971686 4.0539646 2.9023945 0.96820974 -0.68047917 -1.9179313 -2.665483 -2.9776807 -3.0191545][-1.8844323 -2.0267644 -2.1115394 -2.0198398 -1.4756514 -0.3277607 1.2901404 2.3357956 1.6119659 0.45901918 -0.45590425 -1.3028728 -2.0275991 -2.5191288 -2.7996271][-1.9189384 -2.15465 -2.363405 -2.365366 -2.0013568 -1.2256019 -0.12762523 0.59332728 0.2713182 -0.097164392 -0.22544909 -0.5848155 -1.2290062 -1.8960798 -2.424901][-2.021028 -2.3545432 -2.6381545 -2.6744089 -2.4044976 -1.8591278 -1.1842582 -0.80362606 -0.84151888 -0.54911041 -0.0078315735 0.087322235 -0.53074694 -1.4054489 -2.1582966][-2.2054458 -2.5138702 -2.7627935 -2.771 -2.5307696 -2.0944295 -1.6580935 -1.5011399 -1.4226958 -0.7769413 0.12526202 0.44559979 -0.2242918 -1.3217018 -2.2453492][-2.4021864 -2.5208082 -2.5536222 -2.4850883 -2.255183 -1.8479321 -1.5147498 -1.5828745 -1.5984323 -0.942974 -0.024774313 0.2879529 -0.45393026 -1.5764387 -2.5262582][-2.4754808 -2.3172207 -2.1169348 -1.901908 -1.6254582 -1.2431761 -1.0619404 -1.418715 -1.6497056 -1.0984352 -0.21987963 0.10722375 -0.60757363 -1.7214642 -2.6677203]]...]
INFO - root - 2017-12-16 09:35:40.527847: step 37210, loss = 0.53, batch loss = 0.27 (45.7 examples/sec; 0.175 sec/batch; 14h:21m:40s remains)
INFO - root - 2017-12-16 09:35:42.265830: step 37220, loss = 0.62, batch loss = 0.36 (47.2 examples/sec; 0.169 sec/batch; 13h:53m:21s remains)
INFO - root - 2017-12-16 09:35:43.933771: step 37230, loss = 0.59, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 13h:52m:44s remains)
INFO - root - 2017-12-16 09:35:45.621889: step 37240, loss = 0.53, batch loss = 0.27 (45.3 examples/sec; 0.177 sec/batch; 14h:29m:53s remains)
INFO - root - 2017-12-16 09:35:47.338600: step 37250, loss = 0.58, batch loss = 0.33 (45.2 examples/sec; 0.177 sec/batch; 14h:30m:23s remains)
INFO - root - 2017-12-16 09:35:49.028414: step 37260, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 14h:11m:19s remains)
INFO - root - 2017-12-16 09:35:50.701813: step 37270, loss = 0.51, batch loss = 0.25 (47.3 examples/sec; 0.169 sec/batch; 13h:51m:24s remains)
INFO - root - 2017-12-16 09:35:52.362510: step 37280, loss = 0.52, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:25m:08s remains)
INFO - root - 2017-12-16 09:35:54.033245: step 37290, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 13h:41m:11s remains)
INFO - root - 2017-12-16 09:35:55.685944: step 37300, loss = 0.58, batch loss = 0.32 (49.7 examples/sec; 0.161 sec/batch; 13h:11m:51s remains)
2017-12-16 09:35:56.200841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9473281 -3.126076 -3.2003486 -3.1345153 -2.9779196 -2.9248633 -3.0933995 -3.3890896 -3.6846857 -3.9183187 -4.0328765 -3.9549618 -3.5984201 -3.0434225 -2.443172][-2.8090353 -3.0406895 -3.0761619 -2.8455522 -2.4813623 -2.2930284 -2.4783924 -2.9535034 -3.4772785 -3.8757131 -4.1019335 -4.1262059 -3.7965021 -3.1547523 -2.3683686][-2.4402118 -2.7327287 -2.7060022 -2.3061602 -1.7221556 -1.2923036 -1.4181026 -2.1286309 -2.9365277 -3.5313258 -3.9174635 -4.1122503 -3.9186587 -3.3027306 -2.4479508][-2.0542469 -2.4419687 -2.411088 -1.8553193 -0.95703495 -0.1949873 -0.15557432 -1.0149428 -2.1013846 -2.973702 -3.5876606 -3.9983382 -3.95642 -3.4236226 -2.6124828][-1.8137658 -2.290494 -2.2618163 -1.5657884 -0.32597327 0.90000677 1.266504 0.38838053 -0.99676991 -2.2395539 -3.1571767 -3.7563162 -3.8636427 -3.4378681 -2.7026649][-1.7036738 -2.2212558 -2.1724472 -1.297457 0.26726341 1.9155419 2.5593107 1.7258866 0.14030075 -1.4304097 -2.6580482 -3.4466689 -3.6714394 -3.3141537 -2.680675][-1.5944543 -2.089644 -1.9693443 -1.0030327 0.67913342 2.5354903 3.3440344 2.596904 0.98478842 -0.75820196 -2.2159479 -3.1307178 -3.4008291 -3.1063664 -2.5201011][-1.3321738 -1.7142057 -1.5699658 -0.68612063 0.83689308 2.5020607 3.1984494 2.490505 1.0637779 -0.59414005 -2.0805335 -2.9582229 -3.1774571 -2.9057443 -2.3582008][-0.9492718 -1.2009813 -1.1830606 -0.56169975 0.61314559 1.6458709 1.9627821 1.4390953 0.44786835 -0.91037476 -2.1886771 -2.9232485 -3.0816703 -2.8217661 -2.3027244][-0.83003831 -0.99185061 -1.1367052 -0.85421705 -0.17020702 0.27040219 0.34252715 0.10486698 -0.51612878 -1.4878585 -2.4110913 -2.9380116 -3.0343604 -2.8260841 -2.4005883][-1.0564412 -1.1862161 -1.3995435 -1.3729117 -1.0940799 -0.982862 -0.99678254 -1.0785519 -1.4013486 -2.0335109 -2.6504414 -2.9912777 -3.0236602 -2.8740702 -2.5573959][-1.5273873 -1.6363866 -1.862684 -1.9916549 -1.9825746 -2.0491157 -2.1204176 -2.127219 -2.2241607 -2.5184584 -2.827692 -2.9698784 -2.9593825 -2.8666873 -2.6452649][-2.0542061 -2.1304767 -2.3881385 -2.6233957 -2.7265103 -2.7890363 -2.8468494 -2.8066788 -2.7639873 -2.8284566 -2.8876972 -2.877774 -2.8353224 -2.7532465 -2.6069353][-2.415832 -2.4850523 -2.6967187 -2.8842568 -2.9570146 -2.9585824 -2.9438438 -2.8684142 -2.7858231 -2.7340984 -2.6747625 -2.6187625 -2.5763128 -2.50868 -2.3851645][-2.4102414 -2.4400253 -2.5259395 -2.5893288 -2.5939593 -2.5467272 -2.5030861 -2.4486988 -2.3868551 -2.3396978 -2.2912529 -2.2490511 -2.2094097 -2.1545036 -2.0755553]]...]
INFO - root - 2017-12-16 09:35:57.889695: step 37310, loss = 0.63, batch loss = 0.37 (45.7 examples/sec; 0.175 sec/batch; 14h:20m:34s remains)
INFO - root - 2017-12-16 09:35:59.599787: step 37320, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 13h:27m:54s remains)
INFO - root - 2017-12-16 09:36:01.296051: step 37330, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 13h:42m:02s remains)
INFO - root - 2017-12-16 09:36:02.984159: step 37340, loss = 0.54, batch loss = 0.28 (46.7 examples/sec; 0.171 sec/batch; 14h:01m:56s remains)
INFO - root - 2017-12-16 09:36:04.701003: step 37350, loss = 0.65, batch loss = 0.39 (46.6 examples/sec; 0.172 sec/batch; 14h:04m:45s remains)
INFO - root - 2017-12-16 09:36:06.399954: step 37360, loss = 0.57, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 13h:30m:20s remains)
INFO - root - 2017-12-16 09:36:08.060560: step 37370, loss = 0.60, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 13h:25m:24s remains)
INFO - root - 2017-12-16 09:36:09.750772: step 37380, loss = 0.71, batch loss = 0.45 (47.0 examples/sec; 0.170 sec/batch; 13h:57m:11s remains)
INFO - root - 2017-12-16 09:36:11.413028: step 37390, loss = 0.50, batch loss = 0.24 (49.2 examples/sec; 0.163 sec/batch; 13h:20m:02s remains)
INFO - root - 2017-12-16 09:36:13.097519: step 37400, loss = 0.76, batch loss = 0.50 (47.5 examples/sec; 0.168 sec/batch; 13h:47m:41s remains)
2017-12-16 09:36:13.572660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2031517 -0.70748138 -0.37511587 -0.32652235 -0.45202482 -0.67107248 -0.84533906 -0.83175874 -0.61046064 -0.33878183 -0.16108108 0.09128809 0.45572424 0.85636973 1.5142717][-0.80718923 -0.06256938 0.36896896 0.26571631 -0.098376513 -0.5490073 -0.90448 -0.94122541 -0.60802817 -0.24603868 -0.10210228 0.091869593 0.45936918 0.82698584 1.3953137][-0.55354369 0.40165997 0.81215 0.54892039 0.02294755 -0.584038 -1.0974526 -1.2531186 -0.98401952 -0.7335273 -0.82469237 -0.86168206 -0.70431507 -0.50241423 -0.12766743][-0.708691 0.14306688 0.42083621 0.19172215 -0.31157804 -0.85166097 -1.3038619 -1.4928743 -1.4367166 -1.4685709 -1.8225803 -2.2140503 -2.2437901 -2.1436336 -2.0128832][-0.93150771 -0.28127384 -0.024307966 -0.01655221 -0.37935042 -0.75114989 -0.96755219 -1.2001145 -1.4695125 -1.7970836 -2.2631927 -2.7596412 -2.9615822 -2.9800248 -2.9888139][-1.1339856 -0.74705279 -0.47644818 -0.23412538 -0.22491479 -0.16911554 -0.09056282 -0.3619287 -0.92829466 -1.5131955 -2.0365727 -2.5253386 -2.7446268 -2.7557619 -2.7974842][-1.1861335 -1.0404398 -0.74054253 -0.2947793 0.081350565 0.51188278 0.78281307 0.44742727 -0.32500005 -1.0375779 -1.6065207 -1.965518 -2.0664265 -1.9518545 -1.827003][-0.48760962 -0.48435223 -0.274678 0.18245959 0.65034819 1.105756 1.2978988 0.98604512 0.22976136 -0.4984318 -1.0379229 -1.3361423 -1.3046194 -1.0211946 -0.68818855][0.59363294 0.67105579 0.7148459 0.82589626 0.99916768 1.2064319 1.2897332 1.0322096 0.47909379 -0.20719528 -0.7935344 -1.1069013 -1.0649621 -0.74925065 -0.444484][1.3820152 1.5616705 1.5131111 1.3007946 1.0192995 0.92773604 0.978549 0.76448846 0.25031447 -0.42730141 -1.0499794 -1.4091177 -1.495875 -1.3651893 -1.3015474][1.4231448 1.7563279 1.7157819 1.2739038 0.6904633 0.38733459 0.34160066 0.13568902 -0.34840012 -0.935249 -1.4791977 -1.8571217 -2.0814698 -2.1790876 -2.2793431][0.72351432 1.0526323 0.922297 0.357697 -0.27694869 -0.62190104 -0.73439956 -0.9560349 -1.2850651 -1.6232497 -1.94365 -2.2136178 -2.451416 -2.6785746 -2.8411961][-0.51352882 -0.40697742 -0.64375043 -1.1784558 -1.7080212 -1.9541428 -1.9611708 -2.0417378 -2.1695151 -2.2591035 -2.335269 -2.4484217 -2.5947301 -2.779798 -2.8874869][-1.7111197 -1.8217701 -2.1381567 -2.5657985 -2.9355834 -3.0278778 -2.8904495 -2.8351824 -2.8129573 -2.76596 -2.72307 -2.6896822 -2.6769497 -2.7034078 -2.6688671][-2.5861816 -2.8090856 -3.1239786 -3.4150178 -3.5803492 -3.5231957 -3.3067112 -3.1272578 -2.9918146 -2.8885293 -2.8088932 -2.734797 -2.6430655 -2.558434 -2.4252989]]...]
INFO - root - 2017-12-16 09:36:15.241398: step 37410, loss = 0.56, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 13h:45m:24s remains)
INFO - root - 2017-12-16 09:36:16.945356: step 37420, loss = 0.56, batch loss = 0.30 (46.9 examples/sec; 0.171 sec/batch; 13h:58m:39s remains)
INFO - root - 2017-12-16 09:36:18.604649: step 37430, loss = 0.53, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:42m:28s remains)
INFO - root - 2017-12-16 09:36:20.259342: step 37440, loss = 0.55, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 13h:43m:31s remains)
INFO - root - 2017-12-16 09:36:21.932566: step 37450, loss = 0.50, batch loss = 0.25 (51.1 examples/sec; 0.156 sec/batch; 12h:49m:14s remains)
INFO - root - 2017-12-16 09:36:23.601675: step 37460, loss = 0.53, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:22m:46s remains)
INFO - root - 2017-12-16 09:36:25.297530: step 37470, loss = 0.50, batch loss = 0.24 (44.8 examples/sec; 0.179 sec/batch; 14h:37m:50s remains)
INFO - root - 2017-12-16 09:36:26.991441: step 37480, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 13h:34m:27s remains)
INFO - root - 2017-12-16 09:36:28.638021: step 37490, loss = 0.47, batch loss = 0.21 (48.2 examples/sec; 0.166 sec/batch; 13h:36m:29s remains)
INFO - root - 2017-12-16 09:36:30.300794: step 37500, loss = 0.54, batch loss = 0.29 (45.9 examples/sec; 0.174 sec/batch; 14h:16m:23s remains)
2017-12-16 09:36:30.771551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.394804 -2.55713 -2.5271592 -2.28098 -1.8602519 -1.4137317 -1.2847315 -1.4907818 -1.7169338 -1.7326596 -1.6794313 -1.708748 -1.7717507 -1.8070608 -1.7590064][-2.7613811 -3.0486522 -3.1206994 -2.9969203 -2.6148796 -2.166425 -2.0151036 -2.261723 -2.4916475 -2.4674571 -2.3421307 -2.3574266 -2.4045358 -2.4181614 -2.3049998][-3.1192174 -3.5442052 -3.6951239 -3.6624236 -3.2972994 -2.8114295 -2.6015871 -2.9000845 -3.1778464 -3.2095037 -3.1315727 -3.2219129 -3.3241134 -3.3521428 -3.186595][-3.2505414 -3.7657597 -3.9556284 -3.9178312 -3.5486703 -3.0178421 -2.7090778 -3.0186415 -3.4409761 -3.6510234 -3.7296662 -3.9930696 -4.2548671 -4.2926497 -4.1372118][-3.0606363 -3.5447233 -3.6777129 -3.5708528 -3.1439719 -2.5678544 -2.2246749 -2.5425494 -3.0544882 -3.4737349 -3.8235974 -4.3048487 -4.6984773 -4.8020821 -4.6992035][-2.6285698 -2.9506435 -2.9876077 -2.7535899 -2.2162023 -1.4884839 -1.0475202 -1.1899257 -1.7046471 -2.3893116 -3.1174831 -3.7897911 -4.2934618 -4.5206804 -4.4869728][-2.1646955 -2.3099523 -2.3007548 -1.9570543 -1.2132778 -0.23672676 0.44734645 0.59753013 0.27053738 -0.6234858 -1.6681244 -2.4686358 -3.0313921 -3.4523931 -3.4928732][-1.7801533 -1.8601521 -1.8877029 -1.4844649 -0.65101123 0.49551964 1.3440356 1.7540467 1.7215235 0.7548933 -0.44779789 -1.2409647 -1.7699461 -2.3247108 -2.4246492][-1.6917943 -1.8085518 -1.8965039 -1.5582918 -0.84014142 0.1517489 0.93832088 1.4858651 1.7192452 0.96778965 -0.057131052 -0.633188 -1.0273122 -1.6015468 -1.73396][-1.8393819 -2.1080756 -2.3484547 -2.260287 -1.7745538 -1.0605242 -0.48920667 0.037322521 0.39620638 0.072074413 -0.45021737 -0.6086601 -0.78359294 -1.2841052 -1.3938884][-2.0524211 -2.4836254 -2.8755655 -2.9976571 -2.7997935 -2.4652209 -2.191262 -1.8158606 -1.4689016 -1.3855836 -1.3498763 -1.0814013 -0.96624625 -1.2249707 -1.1666598][-2.1416507 -2.6805124 -3.0974126 -3.2816572 -3.2664363 -3.1855054 -3.1415694 -3.0070207 -2.8148396 -2.5570545 -2.1874709 -1.7367818 -1.4637094 -1.3912983 -1.0992287][-2.0056696 -2.5452518 -2.8750207 -2.9713063 -2.9186072 -2.8567102 -2.8865023 -2.9402285 -2.9722297 -2.7193961 -2.3088737 -1.9483173 -1.7442762 -1.5221729 -1.0650284][-1.8384281 -2.2759919 -2.4578235 -2.3370512 -2.0590966 -1.7871236 -1.768067 -1.9697856 -2.201973 -2.0661368 -1.7643082 -1.6457212 -1.6312654 -1.448519 -0.9814055][-1.8114312 -2.1534863 -2.214241 -1.8879838 -1.3414985 -0.7431674 -0.50032318 -0.73453081 -1.056892 -0.97704244 -0.87886012 -1.065208 -1.234668 -1.1535687 -0.75535762]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-37500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:36:32.947026: step 37510, loss = 0.61, batch loss = 0.36 (47.3 examples/sec; 0.169 sec/batch; 13h:51m:07s remains)
INFO - root - 2017-12-16 09:36:34.627823: step 37520, loss = 0.52, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 13h:17m:37s remains)
INFO - root - 2017-12-16 09:36:36.297601: step 37530, loss = 0.48, batch loss = 0.22 (47.5 examples/sec; 0.168 sec/batch; 13h:48m:18s remains)
INFO - root - 2017-12-16 09:36:37.967661: step 37540, loss = 0.58, batch loss = 0.33 (46.4 examples/sec; 0.172 sec/batch; 14h:07m:01s remains)
INFO - root - 2017-12-16 09:36:39.624898: step 37550, loss = 0.50, batch loss = 0.24 (49.2 examples/sec; 0.163 sec/batch; 13h:18m:59s remains)
INFO - root - 2017-12-16 09:36:41.280743: step 37560, loss = 0.50, batch loss = 0.24 (46.3 examples/sec; 0.173 sec/batch; 14h:08m:38s remains)
INFO - root - 2017-12-16 09:36:42.945755: step 37570, loss = 0.54, batch loss = 0.28 (47.9 examples/sec; 0.167 sec/batch; 13h:40m:44s remains)
INFO - root - 2017-12-16 09:36:44.610158: step 37580, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 13h:39m:21s remains)
INFO - root - 2017-12-16 09:36:46.295433: step 37590, loss = 0.50, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 13h:21m:46s remains)
INFO - root - 2017-12-16 09:36:47.948475: step 37600, loss = 0.56, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 13h:21m:38s remains)
2017-12-16 09:36:48.461842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4812479 -1.6060703 -1.7045956 -1.7734952 -1.8305721 -1.9254682 -1.9689153 -1.9330364 -1.8976245 -1.8737773 -1.7866538 -1.6352289 -1.6132469 -1.7162161 -1.8019297][-0.632105 -0.84528708 -1.0805507 -1.2304163 -1.2880279 -1.3666748 -1.5012237 -1.5930171 -1.7251296 -1.7982535 -1.6568222 -1.3651946 -1.1941983 -1.2479795 -1.3486578][0.09569335 -0.30919886 -0.66666734 -0.835776 -0.81570232 -0.72672796 -0.81820405 -1.0518938 -1.3628254 -1.5365512 -1.3856161 -1.0348319 -0.855736 -0.92158914 -1.0483382][0.43782711 -0.14862108 -0.61103785 -0.74368763 -0.49626172 -0.12077856 -0.0057814121 -0.260432 -0.66243017 -0.90079904 -0.808408 -0.58657086 -0.54676723 -0.67025852 -0.79870021][0.29760575 -0.31967068 -0.68927443 -0.67695606 -0.21816158 0.43970585 0.79377532 0.59760332 0.16772723 -0.13993335 -0.2468195 -0.27601194 -0.37006402 -0.48947227 -0.67955792][-0.24930048 -0.66731179 -0.76372123 -0.49836934 0.15615106 1.0089667 1.5948598 1.4695358 0.91773725 0.4721756 0.18830681 -0.052085161 -0.2332561 -0.36416173 -0.68587041][-0.91995239 -1.0165768 -0.75796139 -0.18912983 0.65750957 1.7375522 2.5627851 2.3834605 1.5377531 0.82477379 0.40231967 0.1309607 0.0054631233 -0.16029835 -0.6631248][-1.3464735 -1.1307175 -0.65812206 0.13857222 1.1855159 2.4740763 3.4990864 3.0608788 1.8572273 0.86277771 0.28586054 0.063189268 0.063477516 -0.18154168 -0.86570621][-1.5455481 -1.2698746 -0.7520647 0.16562486 1.2421322 2.3849277 3.1602855 2.6054807 1.4228373 0.39442372 -0.21976542 -0.40819263 -0.32616138 -0.54103792 -1.259169][-1.6298417 -1.4342048 -0.95849156 -0.092976332 0.72854567 1.3730605 1.5673325 0.98995876 0.10748839 -0.63257825 -1.0486292 -1.1272361 -0.97766948 -1.0611931 -1.6181555][-1.756995 -1.5569099 -1.134514 -0.4988935 -0.089962721 0.054334402 -0.1671226 -0.74699557 -1.3318456 -1.7162 -1.8281354 -1.715279 -1.4544374 -1.3597826 -1.6795642][-1.7730649 -1.582637 -1.2864387 -0.94496322 -0.88455415 -1.0351371 -1.4197054 -1.9409317 -2.355392 -2.4695532 -2.318548 -2.0066922 -1.6091874 -1.3817894 -1.4872699][-1.5391512 -1.4913875 -1.3636113 -1.2737808 -1.4114957 -1.6548291 -2.0569952 -2.5215955 -2.8686819 -2.8414359 -2.530081 -2.0746732 -1.6045393 -1.2656934 -1.2141738][-1.3643591 -1.4709523 -1.4763733 -1.4986482 -1.6800984 -1.9694872 -2.3620522 -2.7835793 -3.0157111 -2.871851 -2.4605768 -1.9446414 -1.4722192 -1.1096072 -1.0068825][-1.3032279 -1.5058036 -1.5087993 -1.4711442 -1.6176021 -1.9683892 -2.4017842 -2.6934488 -2.7175381 -2.4971659 -2.1269569 -1.6979913 -1.3160087 -1.0450538 -0.95970213]]...]
INFO - root - 2017-12-16 09:36:50.165958: step 37610, loss = 0.47, batch loss = 0.21 (48.1 examples/sec; 0.166 sec/batch; 13h:37m:47s remains)
INFO - root - 2017-12-16 09:36:51.869247: step 37620, loss = 0.58, batch loss = 0.32 (44.0 examples/sec; 0.182 sec/batch; 14h:53m:43s remains)
INFO - root - 2017-12-16 09:36:53.556164: step 37630, loss = 0.52, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 13h:56m:50s remains)
INFO - root - 2017-12-16 09:36:55.263193: step 37640, loss = 0.51, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 13h:53m:49s remains)
INFO - root - 2017-12-16 09:36:56.939487: step 37650, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:34m:55s remains)
INFO - root - 2017-12-16 09:36:58.639282: step 37660, loss = 0.52, batch loss = 0.26 (47.2 examples/sec; 0.170 sec/batch; 13h:53m:18s remains)
INFO - root - 2017-12-16 09:37:00.331836: step 37670, loss = 0.54, batch loss = 0.28 (46.6 examples/sec; 0.172 sec/batch; 14h:02m:46s remains)
INFO - root - 2017-12-16 09:37:02.019051: step 37680, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:43m:59s remains)
INFO - root - 2017-12-16 09:37:03.747188: step 37690, loss = 0.66, batch loss = 0.40 (45.6 examples/sec; 0.176 sec/batch; 14h:22m:39s remains)
INFO - root - 2017-12-16 09:37:05.419177: step 37700, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 13h:36m:05s remains)
2017-12-16 09:37:05.911760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5814323 -3.8953967 -3.9966004 -3.9944224 -3.9307346 -3.6967649 -3.0212331 -2.0728042 -1.5162623 -1.7921124 -2.6217654 -3.4261737 -3.8980637 -3.8545771 -3.0304141][-2.2558944 -3.1322415 -3.7362447 -3.9816446 -3.8907471 -3.3557372 -2.2631848 -0.90813279 -0.12511373 -0.43861043 -1.4813427 -2.6448462 -3.40523 -3.4359312 -2.4734995][-0.50849724 -2.1098695 -3.2744322 -3.7062347 -3.4538536 -2.5849714 -1.1529729 0.42753458 1.233839 0.79868722 -0.53084826 -2.0420389 -3.056735 -3.0888429 -1.96516][0.84751153 -1.3279756 -2.8625891 -3.317997 -2.8209925 -1.5731087 0.17007518 1.8010774 2.2921362 1.4390941 -0.26188231 -1.9587593 -3.0031049 -2.8797491 -1.6655699][0.86511779 -1.3738621 -2.8302941 -3.046087 -2.2037086 -0.52511 1.5983863 3.1733341 3.1574264 1.6768088 -0.39004838 -2.0979109 -2.9494886 -2.6939445 -1.5537326][-0.20191741 -2.017724 -3.0232766 -2.8663507 -1.6359673 0.5437901 2.9390388 4.1518631 3.3341527 1.2172923 -1.0015411 -2.4717758 -3.012888 -2.6363707 -1.6804199][-1.6224096 -2.8002176 -3.247695 -2.70995 -1.1740236 1.3420031 3.8813925 4.6824532 2.9845867 0.26573348 -1.9270821 -2.994029 -3.1936665 -2.7377694 -1.9698361][-2.744638 -3.3402352 -3.3230596 -2.6025467 -1.1450028 1.1101236 3.3366475 3.7976503 1.8049617 -0.97475159 -2.860713 -3.4397926 -3.3094702 -2.7673662 -2.1607788][-3.2198579 -3.3544662 -3.0418839 -2.3738434 -1.3935857 0.12800574 1.6465535 1.8014889 0.057516336 -2.1929572 -3.539247 -3.7028041 -3.2885015 -2.6735728 -2.1011486][-3.0359151 -2.7345109 -2.2466969 -1.8174989 -1.4675696 -0.84041762 -0.08528161 -0.10530281 -1.3786765 -2.89348 -3.711412 -3.6581149 -3.1168923 -2.4533072 -1.832191][-2.4706903 -1.7973719 -1.2000424 -1.0474935 -1.3056085 -1.5304019 -1.4728477 -1.635902 -2.3934669 -3.226994 -3.5408373 -3.2406244 -2.5962319 -1.9283013 -1.32332][-2.0353742 -1.0890181 -0.38391745 -0.38432252 -1.0389431 -1.8236396 -2.2433996 -2.4682519 -2.7853272 -3.0599821 -2.9070575 -2.2933214 -1.5602243 -1.018566 -0.67062426][-1.8898025 -0.78083754 -0.016039848 -0.13503194 -0.94589567 -1.858022 -2.3530991 -2.484807 -2.4847393 -2.3496594 -1.8165309 -0.93451774 -0.23180866 -0.022768021 -0.16364074][-1.953822 -0.88744271 -0.16107059 -0.27205491 -0.92386746 -1.5652417 -1.8426437 -1.8413994 -1.7237387 -1.4326112 -0.74051297 0.20270872 0.70263338 0.42819691 -0.26454639][-2.1562037 -1.2936596 -0.68752158 -0.65426433 -0.9339416 -1.1465122 -1.1724629 -1.1352245 -1.099067 -0.87717247 -0.22165608 0.60815 0.87155557 0.25360703 -0.76287961]]...]
INFO - root - 2017-12-16 09:37:07.598404: step 37710, loss = 0.63, batch loss = 0.37 (47.8 examples/sec; 0.167 sec/batch; 13h:41m:47s remains)
INFO - root - 2017-12-16 09:37:09.302024: step 37720, loss = 0.62, batch loss = 0.36 (45.1 examples/sec; 0.177 sec/batch; 14h:31m:11s remains)
INFO - root - 2017-12-16 09:37:11.015128: step 37730, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:55s remains)
INFO - root - 2017-12-16 09:37:12.703392: step 37740, loss = 0.53, batch loss = 0.27 (46.0 examples/sec; 0.174 sec/batch; 14h:14m:32s remains)
INFO - root - 2017-12-16 09:37:14.356850: step 37750, loss = 0.48, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:36m:14s remains)
INFO - root - 2017-12-16 09:37:16.026611: step 37760, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 13h:50m:44s remains)
INFO - root - 2017-12-16 09:37:17.725800: step 37770, loss = 0.50, batch loss = 0.24 (45.3 examples/sec; 0.176 sec/batch; 14h:26m:46s remains)
INFO - root - 2017-12-16 09:37:19.426356: step 37780, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 13h:53m:31s remains)
INFO - root - 2017-12-16 09:37:21.126405: step 37790, loss = 0.49, batch loss = 0.23 (45.8 examples/sec; 0.175 sec/batch; 14h:17m:33s remains)
INFO - root - 2017-12-16 09:37:22.828777: step 37800, loss = 0.60, batch loss = 0.34 (47.7 examples/sec; 0.168 sec/batch; 13h:44m:10s remains)
2017-12-16 09:37:23.294350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7257427 -1.5825069 -1.5959 -1.7706021 -1.9595482 -2.0538268 -2.0782602 -2.1155906 -2.2093723 -2.3537896 -2.5069454 -2.5891237 -2.506006 -2.2288284 -1.8683374][-2.2112272 -2.1280806 -2.1913531 -2.375792 -2.5432379 -2.6166344 -2.6441233 -2.6557517 -2.6787281 -2.7522447 -2.8469687 -2.8810978 -2.7332928 -2.4062505 -2.1111746][-2.6848793 -2.6865196 -2.799722 -2.9467795 -3.0304661 -3.0342269 -3.0093431 -3.0071223 -3.05142 -3.1545281 -3.2877789 -3.2690828 -2.995796 -2.5608611 -2.238456][-2.6224961 -2.7183664 -2.8869176 -2.9977689 -2.979239 -2.8524671 -2.7061262 -2.6863225 -2.8300104 -3.1085703 -3.3698616 -3.3588119 -3.0086844 -2.4900558 -2.1006434][-1.9389485 -2.1180816 -2.3341045 -2.4195726 -2.3082674 -2.0016725 -1.6680145 -1.5338233 -1.7585094 -2.3151019 -2.8447609 -2.968977 -2.6683841 -2.1273103 -1.6561488][-1.0239969 -1.2445495 -1.4921026 -1.5534916 -1.3076935 -0.73048568 -0.037009239 0.36631536 0.10394931 -0.779861 -1.6765267 -2.0836437 -1.9256692 -1.4325769 -0.925099][-0.4834727 -0.70161259 -0.92515433 -0.89775574 -0.45680606 0.45245218 1.5772092 2.3839738 2.2325513 1.0269737 -0.30767298 -1.1050147 -1.1910759 -0.79405391 -0.33643126][-0.69281042 -0.85873544 -0.97564745 -0.83987391 -0.21946764 0.91405559 2.3490465 3.5086458 3.4785087 2.0684955 0.44237781 -0.6716454 -0.98222804 -0.67950857 -0.29711175][-1.3892982 -1.5115175 -1.5359625 -1.3504868 -0.72238588 0.34863162 1.6341989 2.6685598 2.6777346 1.4955821 -0.013019323 -1.191444 -1.5999024 -1.3445939 -0.98945332][-1.9023395 -2.0442681 -2.0469403 -1.8937783 -1.4480348 -0.70887637 0.13626671 0.82620049 0.85410714 0.029283047 -1.1999068 -2.2273223 -2.5639932 -2.2673109 -1.8057311][-1.8182309 -1.9257755 -1.9697001 -1.9644188 -1.8183147 -1.5017898 -1.0661478 -0.66472018 -0.6687597 -1.2417592 -2.1600766 -2.9503992 -3.1679478 -2.7750478 -2.1569529][-1.3039324 -1.3087682 -1.3816545 -1.551039 -1.7171724 -1.778132 -1.6782265 -1.4910729 -1.5137043 -1.8812733 -2.4983132 -3.05091 -3.1542892 -2.6919985 -2.0017378][-0.82962048 -0.68135178 -0.71262753 -0.984588 -1.3721302 -1.6817677 -1.7912071 -1.6817846 -1.6671625 -1.8881688 -2.2706552 -2.635711 -2.6399448 -2.1964993 -1.5716019][-0.75065315 -0.52888083 -0.52762318 -0.79840422 -1.2105368 -1.5467987 -1.6538768 -1.524708 -1.4699665 -1.6212559 -1.8621087 -2.0517535 -1.9460925 -1.5458486 -1.0335914][-1.0735872 -0.92442584 -0.94862461 -1.1443808 -1.4156251 -1.5948148 -1.5648166 -1.3740708 -1.2953749 -1.4171865 -1.5760322 -1.5970187 -1.3361281 -0.87673318 -0.41851676]]...]
INFO - root - 2017-12-16 09:37:24.998062: step 37810, loss = 0.46, batch loss = 0.20 (47.4 examples/sec; 0.169 sec/batch; 13h:49m:15s remains)
INFO - root - 2017-12-16 09:37:26.692465: step 37820, loss = 0.52, batch loss = 0.26 (47.5 examples/sec; 0.168 sec/batch; 13h:47m:10s remains)
INFO - root - 2017-12-16 09:37:28.375995: step 37830, loss = 0.59, batch loss = 0.33 (44.3 examples/sec; 0.180 sec/batch; 14h:46m:18s remains)
INFO - root - 2017-12-16 09:37:30.062470: step 37840, loss = 0.51, batch loss = 0.25 (48.2 examples/sec; 0.166 sec/batch; 13h:35m:28s remains)
INFO - root - 2017-12-16 09:37:31.741038: step 37850, loss = 0.67, batch loss = 0.41 (46.6 examples/sec; 0.172 sec/batch; 14h:02m:23s remains)
INFO - root - 2017-12-16 09:37:33.386371: step 37860, loss = 0.48, batch loss = 0.22 (47.4 examples/sec; 0.169 sec/batch; 13h:49m:29s remains)
INFO - root - 2017-12-16 09:37:35.106624: step 37870, loss = 0.54, batch loss = 0.28 (46.2 examples/sec; 0.173 sec/batch; 14h:10m:44s remains)
INFO - root - 2017-12-16 09:37:36.786347: step 37880, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 13h:22m:14s remains)
INFO - root - 2017-12-16 09:37:38.435710: step 37890, loss = 0.60, batch loss = 0.35 (48.9 examples/sec; 0.163 sec/batch; 13h:22m:43s remains)
INFO - root - 2017-12-16 09:37:40.090073: step 37900, loss = 0.51, batch loss = 0.25 (48.9 examples/sec; 0.163 sec/batch; 13h:22m:34s remains)
2017-12-16 09:37:40.583536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1198926 -3.0271301 -2.9338503 -2.9917352 -3.1086929 -3.1214917 -3.0458915 -2.9844105 -2.9892626 -2.95192 -2.8437304 -2.7186966 -2.6327684 -2.4931006 -2.3239725][-3.3290894 -3.1563897 -3.03169 -3.1503615 -3.3869429 -3.4788156 -3.4284582 -3.3584006 -3.3356731 -3.2201324 -3.0202682 -2.9124086 -2.8932252 -2.7787824 -2.563287][-3.0904021 -2.8638399 -2.6829391 -2.826339 -3.1362782 -3.2675412 -3.1831958 -3.0444524 -3.0007737 -2.8862016 -2.6871204 -2.675122 -2.7931414 -2.7413654 -2.5514636][-2.409395 -2.2001543 -2.0475368 -2.2001984 -2.4629824 -2.4489002 -2.0717249 -1.7544751 -1.778265 -1.8720727 -1.7855976 -1.8519673 -2.0788534 -2.1787858 -2.1086059][-1.5678596 -1.4868394 -1.4656634 -1.6082473 -1.6323769 -1.2015204 -0.33591986 0.23123479 -0.08297801 -0.59684753 -0.75141013 -0.8407867 -1.1741897 -1.4508383 -1.4935663][-0.75556219 -0.80171525 -0.93068135 -1.043164 -0.80114865 0.22125864 1.7138259 2.6545146 1.8749492 0.63815475 0.11051893 -0.070463419 -0.49468958 -0.87744045 -0.955642][-0.39190173 -0.48241532 -0.66941905 -0.669873 -0.13133764 1.4141092 3.6317785 5.0967646 3.5683949 1.5820754 0.62875128 0.24408054 -0.23801875 -0.55033684 -0.58982372][-0.861611 -0.893015 -1.0126208 -0.84153259 -0.033781767 1.7682264 4.443223 6.5922127 4.1754179 1.7743661 0.62849522 0.10745883 -0.33669758 -0.51083839 -0.40360737][-1.8509086 -1.80252 -1.7356428 -1.3537612 -0.43755627 1.2637739 3.4365284 4.644639 2.939533 1.0281839 0.045153141 -0.4607296 -0.79142916 -0.74972904 -0.45888638][-2.5648363 -2.3859792 -2.0738783 -1.5444636 -0.66065741 0.59899187 1.8722684 2.228821 0.97301555 -0.31491208 -0.93182743 -1.2487751 -1.3507437 -1.0595756 -0.58658922][-2.7472618 -2.4327426 -1.923373 -1.3027724 -0.57114029 0.14649892 0.55278349 0.29776525 -0.76711714 -1.5844977 -1.7966928 -1.7685373 -1.5782448 -1.1339157 -0.61956465][-2.7071319 -2.3008258 -1.6805297 -1.0383841 -0.4918592 -0.21764159 -0.31519794 -0.88653827 -1.8626167 -2.3193798 -2.2015049 -1.771129 -1.3450336 -0.94444287 -0.52432513][-2.629503 -2.1679862 -1.5434859 -1.0007885 -0.68532956 -0.63447809 -0.86151314 -1.463429 -2.2510204 -2.4697022 -2.0985718 -1.4242303 -0.87664318 -0.58157706 -0.36272383][-2.5983176 -2.1357133 -1.5435483 -1.1902754 -1.0694542 -1.023083 -1.1528344 -1.6162553 -2.2705538 -2.3739562 -1.8646832 -1.1196245 -0.52484369 -0.27314997 -0.26895905][-2.625196 -2.2054307 -1.6709073 -1.4764966 -1.4505417 -1.2819335 -1.1486326 -1.3840266 -2.0036709 -2.0963511 -1.5876307 -0.87905335 -0.36538386 -0.279747 -0.50593543]]...]
INFO - root - 2017-12-16 09:37:42.246830: step 37910, loss = 0.57, batch loss = 0.31 (50.4 examples/sec; 0.159 sec/batch; 12h:59m:12s remains)
INFO - root - 2017-12-16 09:37:43.911093: step 37920, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 13h:49m:47s remains)
INFO - root - 2017-12-16 09:37:45.576593: step 37930, loss = 0.76, batch loss = 0.50 (48.4 examples/sec; 0.165 sec/batch; 13h:30m:49s remains)
INFO - root - 2017-12-16 09:37:47.255142: step 37940, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:40m:56s remains)
INFO - root - 2017-12-16 09:37:48.936350: step 37950, loss = 0.54, batch loss = 0.28 (47.4 examples/sec; 0.169 sec/batch; 13h:48m:31s remains)
INFO - root - 2017-12-16 09:37:50.625793: step 37960, loss = 0.59, batch loss = 0.33 (47.8 examples/sec; 0.167 sec/batch; 13h:41m:32s remains)
INFO - root - 2017-12-16 09:37:52.311143: step 37970, loss = 0.49, batch loss = 0.23 (45.1 examples/sec; 0.177 sec/batch; 14h:30m:45s remains)
INFO - root - 2017-12-16 09:37:54.017612: step 37980, loss = 0.60, batch loss = 0.35 (45.0 examples/sec; 0.178 sec/batch; 14h:32m:23s remains)
INFO - root - 2017-12-16 09:37:55.671831: step 37990, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:04s remains)
INFO - root - 2017-12-16 09:37:57.344735: step 38000, loss = 0.50, batch loss = 0.24 (46.8 examples/sec; 0.171 sec/batch; 13h:59m:49s remains)
2017-12-16 09:37:57.863891: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048897028 0.21691108 0.18962789 -0.076280355 -0.5496558 -1.0853221 -1.5110338 -1.7620901 -1.8769851 -1.8824954 -1.8363197 -1.8871331 -1.8972296 -1.69971 -1.3932576][-0.49753535 -0.30484772 -0.31357646 -0.54339147 -0.95589042 -1.433605 -1.8604882 -2.2184088 -2.5116253 -2.69069 -2.7293222 -2.7178833 -2.5881224 -2.2569382 -1.8918828][-1.170006 -0.999941 -1.06973 -1.3274443 -1.7265151 -2.1511059 -2.551501 -2.9220541 -3.2633176 -3.4950323 -3.5451448 -3.4284368 -3.1487596 -2.7104607 -2.3153846][-1.6412171 -1.4915359 -1.6003625 -1.880796 -2.2339234 -2.5753965 -2.85045 -3.0549865 -3.3114233 -3.5794733 -3.69209 -3.5965047 -3.3103402 -2.8936572 -2.5057147][-1.7760658 -1.6080875 -1.6807134 -1.8338196 -1.9755379 -2.067549 -2.0514133 -2.0016596 -2.1603343 -2.5458763 -2.8897305 -3.0564861 -3.0270259 -2.8038509 -2.4703839][-1.5247825 -1.3084681 -1.2385235 -1.1323524 -0.92069387 -0.60531461 -0.21097016 0.12287307 0.04426527 -0.54541552 -1.3018326 -1.9190412 -2.2971675 -2.4069164 -2.2095933][-0.980862 -0.73362708 -0.47414434 -0.068691254 0.51084232 1.1939805 1.9274998 2.5235081 2.41782 1.531955 0.39501524 -0.56573439 -1.2076772 -1.556193 -1.5645596][-0.60988915 -0.354182 0.014049053 0.54522943 1.2617052 2.0877147 2.9588761 3.6710973 3.4934025 2.4703288 1.3183985 0.41392827 -0.12223244 -0.43179047 -0.55866432][-0.81836081 -0.67407 -0.37935567 0.02042532 0.57245946 1.1869473 1.8146243 2.2540092 2.1657596 1.5842323 0.97755122 0.63973737 0.58096147 0.56585264 0.43739462][-1.5861353 -1.6445746 -1.5303615 -1.3166827 -1.0285032 -0.72121978 -0.3547368 -0.075110435 -0.02414465 -0.07552433 0.030210733 0.39882517 0.88969541 1.2665987 1.2496171][-2.4514468 -2.667794 -2.7110939 -2.6790285 -2.5852132 -2.4789681 -2.2829063 -2.0613885 -1.8677568 -1.5222541 -0.87961626 -0.029981852 0.78004026 1.3510342 1.3689864][-2.8976662 -3.1262727 -3.2924452 -3.4343672 -3.5243688 -3.5467958 -3.4401472 -3.2289188 -2.9124765 -2.3709748 -1.5427986 -0.61856151 0.14850998 0.62943125 0.58392477][-2.6228445 -2.8073938 -3.0754528 -3.3788929 -3.6111455 -3.7202339 -3.6562507 -3.4570193 -3.1490765 -2.6413209 -1.9027362 -1.1888175 -0.69464338 -0.45884418 -0.62241948][-2.0678337 -2.1834149 -2.4850101 -2.8800504 -3.1808763 -3.345247 -3.3187296 -3.1654253 -2.9643793 -2.6324153 -2.1383679 -1.7136345 -1.4895155 -1.4643385 -1.7005941][-1.7746085 -1.8173561 -2.1155958 -2.5368247 -2.8774464 -3.0843258 -3.1248972 -3.0446689 -2.957648 -2.8144569 -2.5733261 -2.3872347 -2.3126864 -2.3727379 -2.5635684]]...]
INFO - root - 2017-12-16 09:37:59.597896: step 38010, loss = 0.53, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 13h:33m:48s remains)
INFO - root - 2017-12-16 09:38:01.283790: step 38020, loss = 0.65, batch loss = 0.39 (46.7 examples/sec; 0.171 sec/batch; 14h:01m:38s remains)
INFO - root - 2017-12-16 09:38:02.928684: step 38030, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:30m:01s remains)
INFO - root - 2017-12-16 09:38:04.603267: step 38040, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 13h:30m:51s remains)
INFO - root - 2017-12-16 09:38:06.273036: step 38050, loss = 0.59, batch loss = 0.33 (47.5 examples/sec; 0.169 sec/batch; 13h:46m:57s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:38:07.914406: step 38060, loss = 0.61, batch loss = 0.35 (47.7 examples/sec; 0.168 sec/batch; 13h:43m:43s remains)
INFO - root - 2017-12-16 09:38:09.608559: step 38070, loss = 0.55, batch loss = 0.29 (46.3 examples/sec; 0.173 sec/batch; 14h:08m:37s remains)
INFO - root - 2017-12-16 09:38:11.275393: step 38080, loss = 0.52, batch loss = 0.27 (48.4 examples/sec; 0.165 sec/batch; 13h:31m:26s remains)
INFO - root - 2017-12-16 09:38:12.925802: step 38090, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 13h:47m:22s remains)
INFO - root - 2017-12-16 09:38:14.609855: step 38100, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.166 sec/batch; 13h:33m:03s remains)
2017-12-16 09:38:15.095276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5831318 -3.0848238 -3.5241172 -3.5403948 -3.1896439 -2.7225223 -2.3448505 -2.0005858 -1.7267741 -1.7976482 -2.3924835 -3.1519442 -3.3891497 -3.0543194 -2.5102668][-3.9808149 -4.4078283 -4.538651 -4.244812 -3.701654 -3.2165532 -2.964381 -2.8098519 -2.7574158 -3.0359271 -3.6147804 -4.108798 -4.0095367 -3.5165951 -3.0564218][-5.1017966 -5.4354248 -5.2120142 -4.44849 -3.4623828 -2.7692802 -2.5760834 -2.6751416 -2.954355 -3.5064888 -4.1218624 -4.4537015 -4.0710478 -3.4418492 -3.0774894][-5.5120745 -5.7660065 -5.1993818 -3.9144182 -2.4781234 -1.4412241 -1.1028671 -1.4071156 -2.1045265 -2.9569936 -3.715071 -4.011405 -3.5800362 -2.9790356 -2.6664426][-5.109561 -5.344173 -4.5336018 -2.8478816 -0.96817279 0.57663822 1.3077519 0.86784005 -0.3551178 -1.7013958 -2.7412345 -3.1573997 -2.8772514 -2.4235241 -2.1844125][-4.2414637 -4.3836188 -3.4194241 -1.4676938 0.71829891 2.8235171 4.1181183 3.720021 2.0327151 0.20827198 -1.2381914 -2.0025988 -2.054297 -1.8837118 -1.8213751][-3.4005792 -3.470221 -2.5096045 -0.46016502 1.99276 4.5652895 6.4528742 6.2285261 4.16551 1.8941844 0.053420544 -1.0876687 -1.4683664 -1.5575312 -1.6166511][-2.96588 -2.9170651 -2.1022358 -0.29495621 2.0386755 4.7346239 7.0420551 7.2073593 5.0442753 2.5671446 0.48626709 -0.91007388 -1.5732721 -1.8650712 -1.9572482][-3.0457578 -2.9194331 -2.3826332 -1.0681076 0.79887033 3.1657398 5.367568 5.7965622 4.1275749 1.9660246 0.066853523 -1.2667284 -2.1100719 -2.5835211 -2.6125691][-3.4107008 -3.3490505 -3.1857443 -2.3555074 -0.97819877 0.84210634 2.6185734 3.1129138 2.0003326 0.40118384 -1.0805119 -2.1570935 -2.8805537 -3.3401322 -3.2962685][-3.6732821 -3.7934151 -3.9222808 -3.5558619 -2.7111087 -1.511898 -0.27844667 0.20386529 -0.39304018 -1.4375867 -2.4767337 -3.2416763 -3.6747859 -3.8782854 -3.6980596][-3.8239975 -4.1167765 -4.41905 -4.4344187 -4.0821066 -3.4050937 -2.59077 -2.1666253 -2.4356332 -3.0639486 -3.7546792 -4.1776586 -4.2265763 -4.0837774 -3.7362933][-3.7799325 -4.1058307 -4.3865309 -4.5587091 -4.5215335 -4.2175078 -3.7215862 -3.387305 -3.500926 -3.8976493 -4.3417521 -4.4983325 -4.24442 -3.8048387 -3.3152366][-3.593955 -3.7317023 -3.798764 -3.8859627 -3.9653492 -3.8792913 -3.6113935 -3.3764994 -3.414772 -3.6578767 -3.9382379 -3.973645 -3.6198969 -3.1386247 -2.6936355][-3.4557469 -3.2339573 -2.9838154 -2.8594928 -2.8715508 -2.843349 -2.702246 -2.5604019 -2.5755272 -2.7705798 -3.0181928 -3.0705218 -2.7927792 -2.4156187 -2.1162689]]...]
INFO - root - 2017-12-16 09:38:16.781128: step 38110, loss = 0.47, batch loss = 0.21 (46.9 examples/sec; 0.171 sec/batch; 13h:57m:18s remains)
INFO - root - 2017-12-16 09:38:18.458407: step 38120, loss = 0.52, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 13h:44m:56s remains)
INFO - root - 2017-12-16 09:38:20.126555: step 38130, loss = 0.62, batch loss = 0.36 (48.3 examples/sec; 0.165 sec/batch; 13h:31m:57s remains)
INFO - root - 2017-12-16 09:38:21.802207: step 38140, loss = 0.51, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 13h:18m:25s remains)
INFO - root - 2017-12-16 09:38:23.457020: step 38150, loss = 0.58, batch loss = 0.33 (46.2 examples/sec; 0.173 sec/batch; 14h:08m:47s remains)
INFO - root - 2017-12-16 09:38:25.158884: step 38160, loss = 0.68, batch loss = 0.42 (48.9 examples/sec; 0.164 sec/batch; 13h:22m:58s remains)
INFO - root - 2017-12-16 09:38:26.800604: step 38170, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:34m:58s remains)
INFO - root - 2017-12-16 09:38:28.462548: step 38180, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 13h:45m:11s remains)
INFO - root - 2017-12-16 09:38:30.133905: step 38190, loss = 0.49, batch loss = 0.23 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:34s remains)
INFO - root - 2017-12-16 09:38:31.806078: step 38200, loss = 0.56, batch loss = 0.30 (43.7 examples/sec; 0.183 sec/batch; 14h:58m:05s remains)
2017-12-16 09:38:32.303247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0663416 -2.2106104 -2.2208772 -2.1118245 -1.9978398 -1.8775289 -1.564899 -1.1159333 -0.78663611 -0.59314096 -0.55629718 -0.9945488 -1.8458304 -2.5757906 -3.0245605][-1.4393201 -1.5981629 -1.6136156 -1.5593826 -1.5950441 -1.7187525 -1.6366942 -1.3746692 -1.1381854 -1.0380147 -1.1208092 -1.6919296 -2.6267638 -3.5177069 -4.1392713][-1.0231119 -1.131119 -1.0988435 -1.059261 -1.1713629 -1.4390163 -1.5468907 -1.4398243 -1.285717 -1.1881055 -1.2700652 -1.8120255 -2.7569177 -3.7685592 -4.5424271][-0.73500228 -0.75930774 -0.62611747 -0.47662604 -0.50125575 -0.7304529 -0.93486452 -0.99873281 -0.91950357 -0.76597345 -0.786005 -1.2524024 -2.146383 -3.2047076 -4.0346575][-0.26189661 -0.19512987 0.07792592 0.38893867 0.4869194 0.33661985 0.084386826 -0.12414098 -0.11578798 0.10142922 0.206182 -0.17223239 -0.98531866 -2.0190952 -2.8714504][0.21708751 0.25716829 0.6105876 1.1684942 1.470933 1.4868524 1.250674 0.91294694 0.8469553 1.1649895 1.497421 1.2717378 0.53919053 -0.46228671 -1.3609513][0.76057744 0.666379 0.93729997 1.5655813 2.0827928 2.3091822 2.1214366 1.696384 1.5242867 1.9802871 2.604846 2.6668282 2.0867219 1.086849 0.079812527][1.2797647 0.97896814 1.0886083 1.6327195 2.1959066 2.4680743 2.2175794 1.6499057 1.2554483 1.7035079 2.6369958 3.165071 2.955842 2.147191 1.1462188][1.6242766 1.1673446 1.017555 1.3652921 1.8206143 2.0272608 1.7704377 1.1518214 0.6292026 0.97076392 1.9900966 2.8247452 2.9953218 2.4688263 1.5527825][1.0524616 0.49944115 0.22852635 0.39748859 0.75666952 0.96704245 0.82357335 0.30967832 -0.054607868 0.33856773 1.356575 2.2833147 2.6183586 2.2131348 1.3746281][-0.27681971 -0.88284731 -1.1742257 -1.1117396 -0.84762585 -0.60330844 -0.63874972 -0.95515656 -1.1191609 -0.68086588 0.22545195 1.0995183 1.5159662 1.2555993 0.54976726][-1.6278632 -2.2683272 -2.5876594 -2.5704865 -2.3527679 -2.1364937 -2.1237321 -2.2679906 -2.2434065 -1.8101742 -1.0793225 -0.39446473 -0.089579821 -0.28302145 -0.8503629][-2.6692703 -3.275413 -3.5310848 -3.4678543 -3.2434583 -3.0721774 -3.056803 -3.0812178 -2.9371114 -2.543999 -2.0314574 -1.6325424 -1.5680134 -1.8623648 -2.3807554][-3.4545572 -3.9564552 -4.0999522 -3.9572845 -3.7356043 -3.5976059 -3.5847692 -3.5792487 -3.4252629 -3.1440125 -2.8603592 -2.7282746 -2.8522274 -3.1961877 -3.6084604][-3.8292251 -4.18143 -4.215682 -4.0422888 -3.8640852 -3.7922206 -3.8272815 -3.8390241 -3.7367258 -3.5615888 -3.4545255 -3.4978626 -3.6949036 -3.9690757 -4.1785631]]...]
INFO - root - 2017-12-16 09:38:33.974394: step 38210, loss = 0.50, batch loss = 0.24 (48.4 examples/sec; 0.165 sec/batch; 13h:30m:51s remains)
INFO - root - 2017-12-16 09:38:35.653295: step 38220, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:18s remains)
INFO - root - 2017-12-16 09:38:37.314846: step 38230, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 13h:28m:13s remains)
INFO - root - 2017-12-16 09:38:38.980124: step 38240, loss = 0.49, batch loss = 0.23 (46.9 examples/sec; 0.171 sec/batch; 13h:56m:30s remains)
INFO - root - 2017-12-16 09:38:40.627498: step 38250, loss = 0.72, batch loss = 0.46 (49.1 examples/sec; 0.163 sec/batch; 13h:19m:51s remains)
INFO - root - 2017-12-16 09:38:42.293784: step 38260, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 13h:40m:05s remains)
INFO - root - 2017-12-16 09:38:43.945100: step 38270, loss = 0.59, batch loss = 0.34 (50.0 examples/sec; 0.160 sec/batch; 13h:04m:02s remains)
INFO - root - 2017-12-16 09:38:45.575588: step 38280, loss = 0.53, batch loss = 0.27 (49.9 examples/sec; 0.160 sec/batch; 13h:06m:53s remains)
INFO - root - 2017-12-16 09:38:47.276685: step 38290, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.163 sec/batch; 13h:21m:42s remains)
INFO - root - 2017-12-16 09:38:48.919329: step 38300, loss = 0.55, batch loss = 0.30 (49.2 examples/sec; 0.163 sec/batch; 13h:18m:00s remains)
2017-12-16 09:38:49.372499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3749592 -2.2073205 -1.6725879 -1.1473382 -0.9365952 -1.0949333 -1.5016117 -1.8029224 -1.7647314 -1.248497 -0.73843741 -0.91207719 -1.88096 -3.0794742 -3.978936][-2.9408708 -2.5723729 -1.6933073 -0.72794116 -0.30306745 -0.60313356 -1.2846072 -1.7595744 -1.8540916 -1.417608 -0.8907696 -0.97568631 -1.8001928 -2.7874687 -3.5393319][-3.0904775 -2.3160925 -1.0901053 0.27218175 0.93334746 0.44496012 -0.55604839 -1.2407447 -1.5194261 -1.2476135 -0.75867569 -0.83416414 -1.4783148 -2.1358638 -2.6571615][-2.6954615 -1.46883 -0.019402504 1.5375471 2.3331888 1.7242949 0.4870553 -0.42892647 -0.89589214 -0.80598581 -0.3427434 -0.37151241 -0.83651292 -1.2927581 -1.7225883][-2.1564815 -0.63827562 0.95335031 2.5267522 3.2978427 2.5917561 1.2337952 0.20375395 -0.33864546 -0.27174664 0.14902139 0.11724043 -0.35374236 -0.88259256 -1.4520595][-1.627423 -0.058015347 1.4986017 2.8426878 3.4161222 2.8518746 1.848222 1.0845439 0.62373352 0.64264107 0.97934723 0.76640248 -0.031453133 -0.99304342 -1.9631439][-1.1430403 0.13285422 1.4064283 2.42923 2.9731028 2.9986465 2.7893999 2.5890429 2.3032753 2.2078497 2.2690814 1.7274678 0.47602844 -1.0192401 -2.4517164][-0.85237348 -0.13414645 0.67546749 1.5467043 2.2747948 2.938957 3.4607842 3.7563193 3.6033151 3.4071419 3.2484391 2.4620168 0.96195269 -0.82619429 -2.5192621][-1.0480721 -0.97554934 -0.63753438 0.036748171 0.85371423 1.7396743 2.5370538 3.0139425 3.0198495 2.9398968 2.864526 2.2208831 0.80265355 -0.99629354 -2.707438][-1.304158 -1.7091637 -1.844569 -1.5175736 -0.84874082 -0.081377029 0.59646225 1.0321178 1.1681147 1.3301239 1.5621622 1.2535646 0.073949814 -1.5367526 -3.0854912][-1.6312401 -2.2243898 -2.5817804 -2.5324779 -2.1418419 -1.6288855 -1.1910698 -0.88478637 -0.67533267 -0.29216075 0.23704052 0.23983741 -0.58638382 -1.9108531 -3.3298473][-2.271951 -2.7946186 -3.1456831 -3.2135134 -3.0245035 -2.7288997 -2.4755857 -2.296453 -2.1198242 -1.6318749 -0.87770557 -0.555763 -1.0790291 -2.1835096 -3.4301724][-2.9184933 -3.2404213 -3.5067277 -3.5963345 -3.5359335 -3.4039664 -3.2770581 -3.1868942 -3.06565 -2.5895379 -1.7210158 -1.1708291 -1.5165974 -2.4723368 -3.5347733][-3.1610742 -3.3189209 -3.4716096 -3.5406244 -3.5419641 -3.5014119 -3.4651103 -3.4455452 -3.3795638 -2.9751914 -2.1687553 -1.6027606 -1.8345957 -2.6373112 -3.4535453][-2.8485267 -2.9143507 -2.9903965 -3.0390563 -3.0611355 -3.050782 -3.03405 -3.0322466 -3.0093877 -2.7569342 -2.2132132 -1.8089485 -1.9413944 -2.4894416 -3.0149338]]...]
INFO - root - 2017-12-16 09:38:51.010384: step 38310, loss = 0.75, batch loss = 0.49 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:40s remains)
INFO - root - 2017-12-16 09:38:52.652864: step 38320, loss = 0.45, batch loss = 0.19 (47.8 examples/sec; 0.167 sec/batch; 13h:41m:08s remains)
INFO - root - 2017-12-16 09:38:54.337069: step 38330, loss = 0.60, batch loss = 0.35 (47.9 examples/sec; 0.167 sec/batch; 13h:39m:24s remains)
INFO - root - 2017-12-16 09:38:56.034518: step 38340, loss = 0.56, batch loss = 0.31 (48.3 examples/sec; 0.166 sec/batch; 13h:32m:20s remains)
INFO - root - 2017-12-16 09:38:57.704703: step 38350, loss = 0.51, batch loss = 0.25 (48.1 examples/sec; 0.166 sec/batch; 13h:36m:07s remains)
INFO - root - 2017-12-16 09:38:59.343348: step 38360, loss = 0.47, batch loss = 0.21 (49.5 examples/sec; 0.162 sec/batch; 13h:12m:08s remains)
INFO - root - 2017-12-16 09:39:00.992462: step 38370, loss = 0.53, batch loss = 0.28 (46.8 examples/sec; 0.171 sec/batch; 13h:58m:46s remains)
INFO - root - 2017-12-16 09:39:02.653885: step 38380, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.166 sec/batch; 13h:32m:04s remains)
INFO - root - 2017-12-16 09:39:04.321527: step 38390, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 13h:20m:14s remains)
INFO - root - 2017-12-16 09:39:05.976419: step 38400, loss = 0.47, batch loss = 0.21 (46.7 examples/sec; 0.171 sec/batch; 14h:00m:14s remains)
2017-12-16 09:39:06.434010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6684823 -2.5544837 -2.5749338 -2.6038704 -2.5168309 -2.2898173 -2.0994956 -2.0494576 -2.0194459 -1.9782002 -1.8481324 -1.6554686 -1.5190694 -1.3677518 -1.1746341][-2.7011857 -2.6410244 -2.70756 -2.7772703 -2.828511 -2.695385 -2.5665903 -2.5723042 -2.6015506 -2.5856471 -2.5001605 -2.3846593 -2.3379037 -2.2445149 -2.0732117][-2.1973963 -2.366823 -2.5281982 -2.7517469 -3.1164839 -3.2494104 -3.216501 -3.298429 -3.395443 -3.4532037 -3.4728215 -3.5296407 -3.6212697 -3.6053824 -3.4433103][-1.1922307 -1.6500387 -1.9702253 -2.3608356 -3.069788 -3.4908869 -3.4888012 -3.5993447 -3.8474245 -4.038413 -4.1775084 -4.4403973 -4.7353635 -4.8056445 -4.6094837][-0.10951376 -0.74451029 -1.0891477 -1.5332575 -2.4009595 -2.8912525 -2.7758131 -2.8308003 -3.2141194 -3.5278137 -3.7780423 -4.2545485 -4.8302889 -5.0666571 -4.9032755][0.67463636 0.035906315 -0.10303926 -0.33999443 -1.1050034 -1.3961017 -0.91933608 -0.84443808 -1.4668419 -1.9723792 -2.2855372 -2.94977 -3.8261795 -4.2278843 -4.1411686][0.748868 0.32949615 0.53246379 0.6847024 0.31455851 0.44672871 1.3895559 1.6058972 0.64222431 -0.092554569 -0.32538414 -1.0657313 -2.1737335 -2.6880748 -2.666923][0.037621737 -0.13954973 0.4015615 0.97699308 1.0332522 1.6126235 3.027056 3.3847983 2.0468481 1.1403573 1.1072533 0.47467613 -0.67492974 -1.2061908 -1.2049733][-0.94898963 -1.0605406 -0.41976047 0.23432755 0.44056392 1.2406647 2.8409827 3.1861327 1.7669504 0.9239974 1.0418572 0.63528442 -0.28865409 -0.62895393 -0.5559119][-1.5645781 -1.9032769 -1.4785031 -1.026842 -1.0032029 -0.44732249 0.80604315 0.99963951 -0.1071105 -0.67585027 -0.4999491 -0.72485256 -1.2779515 -1.3427114 -1.1378886][-1.7297637 -2.2814555 -2.2458544 -2.1737704 -2.5090361 -2.3946705 -1.7093139 -1.6573478 -2.4027486 -2.7042878 -2.5140047 -2.5656748 -2.7750649 -2.6222873 -2.3191969][-1.465591 -2.1921053 -2.6018741 -2.9627438 -3.5365534 -3.7761416 -3.5738511 -3.5998812 -3.9769235 -4.066144 -3.8681848 -3.7996211 -3.8094087 -3.5488882 -3.2624891][-1.0085535 -1.839726 -2.6161165 -3.2699506 -3.8447766 -4.1524906 -4.1376181 -4.1236668 -4.1936932 -4.113266 -3.9383509 -3.8437674 -3.8085093 -3.5838647 -3.3729734][-0.56684792 -1.430337 -2.3582337 -3.0203419 -3.4150767 -3.5960412 -3.5919862 -3.4323163 -3.3065224 -3.1888766 -3.0916004 -3.035584 -2.9900291 -2.8387563 -2.7392993][-0.43914127 -1.2059574 -2.0134008 -2.4894743 -2.6155014 -2.6208882 -2.5514383 -2.3141835 -2.1266758 -2.0630505 -2.1001821 -2.1324811 -2.1136913 -2.0330379 -2.0210991]]...]
INFO - root - 2017-12-16 09:39:08.099392: step 38410, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 13h:25m:36s remains)
INFO - root - 2017-12-16 09:39:09.745749: step 38420, loss = 0.52, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 13h:33m:10s remains)
INFO - root - 2017-12-16 09:39:11.423917: step 38430, loss = 0.52, batch loss = 0.26 (49.0 examples/sec; 0.163 sec/batch; 13h:20m:42s remains)
INFO - root - 2017-12-16 09:39:13.079270: step 38440, loss = 0.48, batch loss = 0.22 (46.2 examples/sec; 0.173 sec/batch; 14h:07m:49s remains)
INFO - root - 2017-12-16 09:39:14.748926: step 38450, loss = 0.49, batch loss = 0.23 (45.6 examples/sec; 0.176 sec/batch; 14h:20m:36s remains)
INFO - root - 2017-12-16 09:39:16.392353: step 38460, loss = 0.65, batch loss = 0.39 (48.4 examples/sec; 0.165 sec/batch; 13h:29m:56s remains)
INFO - root - 2017-12-16 09:39:18.106123: step 38470, loss = 0.51, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 13h:52m:50s remains)
INFO - root - 2017-12-16 09:39:19.781296: step 38480, loss = 0.60, batch loss = 0.34 (46.5 examples/sec; 0.172 sec/batch; 14h:03m:18s remains)
INFO - root - 2017-12-16 09:39:21.477499: step 38490, loss = 0.51, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 13h:57m:51s remains)
INFO - root - 2017-12-16 09:39:23.160034: step 38500, loss = 0.59, batch loss = 0.33 (48.9 examples/sec; 0.164 sec/batch; 13h:21m:09s remains)
2017-12-16 09:39:23.646291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7641289 -3.0954638 -3.3548861 -3.3463271 -3.1647644 -2.877759 -2.7195656 -2.9001713 -3.3849854 -3.9290214 -4.3065357 -4.5358963 -4.6154289 -4.6060944 -4.4797325][-3.1937287 -3.6727839 -3.956243 -3.8021779 -3.3738031 -2.8759701 -2.6880796 -2.9304829 -3.5451829 -4.2324643 -4.7070065 -5.0877666 -5.3361549 -5.4205112 -5.2592545][-3.5993724 -4.1747212 -4.3754449 -3.9508207 -3.2095807 -2.5158195 -2.1681702 -2.3409715 -3.0293508 -3.8939579 -4.540534 -5.1071444 -5.5627317 -5.7949581 -5.6122804][-3.7784629 -4.3292131 -4.3414817 -3.5988581 -2.4916346 -1.4189208 -0.69974923 -0.71000361 -1.5824368 -2.7928529 -3.6865754 -4.4719992 -5.1983948 -5.5126963 -5.2515135][-3.7948949 -4.3251514 -4.2321386 -3.1998608 -1.7158059 -0.11789465 1.1260226 1.3232679 0.31946063 -1.2585491 -2.5277112 -3.5753264 -4.5101147 -4.8850632 -4.5350494][-3.6892462 -4.1934409 -4.0699134 -3.0144749 -1.2650318 0.83220696 2.7843616 3.5190132 2.4686296 0.4986918 -1.1583472 -2.4750793 -3.546526 -4.0148611 -3.6846061][-3.4454155 -3.844862 -3.6546474 -2.6251018 -0.79511619 1.7281234 4.5001621 6.3607292 5.2280636 2.5767968 0.28982544 -1.3613217 -2.5188947 -3.0033641 -2.7166505][-3.1424775 -3.4252162 -3.1152022 -2.0597193 -0.29680109 2.3334033 5.8229837 8.8913116 7.4406872 4.0343046 1.2952366 -0.56141543 -1.7678659 -2.2550359 -1.9627798][-2.9579391 -3.2022688 -2.9125085 -1.9500027 -0.36269212 2.0529053 5.1228533 7.2526197 6.304492 3.6765211 1.3140175 -0.37945151 -1.5254898 -1.9964243 -1.7550464][-2.9151397 -3.2148061 -3.0134151 -2.1413329 -0.71883035 1.1635315 3.3174965 4.5511303 4.0671749 2.4370282 0.861228 -0.39872527 -1.3369739 -1.7702007 -1.7242157][-2.9635324 -3.3323376 -3.2142098 -2.4954126 -1.3098276 0.10243464 1.553081 2.3113472 2.0001657 1.1894519 0.4414289 -0.27405095 -0.98520088 -1.5180185 -1.765224][-3.0415642 -3.4648507 -3.5186605 -3.058362 -2.2012959 -1.2103287 -0.28701329 0.14649415 0.022611618 -0.21811104 -0.30474973 -0.55275106 -1.0962582 -1.7557682 -2.1651785][-3.0228438 -3.4814873 -3.7201672 -3.5747035 -2.9912174 -2.3029215 -1.7550775 -1.5278344 -1.4777013 -1.3938999 -1.1337476 -1.1230268 -1.5338314 -2.1533637 -2.5960526][-2.8487566 -3.2737939 -3.617763 -3.690021 -3.3253791 -2.9180036 -2.6177108 -2.4295046 -2.3206768 -2.1200233 -1.8115921 -1.6931412 -1.9052811 -2.3478475 -2.734997][-2.5713897 -2.9074082 -3.2250822 -3.3668323 -3.2170167 -3.0033121 -2.8303852 -2.6912036 -2.6298296 -2.535778 -2.3176339 -2.1465847 -2.1944773 -2.4397542 -2.6975031]]...]
INFO - root - 2017-12-16 09:39:25.326861: step 38510, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:30m:40s remains)
INFO - root - 2017-12-16 09:39:26.986702: step 38520, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:33m:18s remains)
INFO - root - 2017-12-16 09:39:28.621037: step 38530, loss = 0.62, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 13h:36m:43s remains)
INFO - root - 2017-12-16 09:39:30.289991: step 38540, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 13h:44m:04s remains)
INFO - root - 2017-12-16 09:39:31.960639: step 38550, loss = 0.49, batch loss = 0.24 (48.0 examples/sec; 0.167 sec/batch; 13h:35m:57s remains)
INFO - root - 2017-12-16 09:39:33.627881: step 38560, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 13h:42m:50s remains)
INFO - root - 2017-12-16 09:39:35.261084: step 38570, loss = 0.56, batch loss = 0.30 (49.2 examples/sec; 0.163 sec/batch; 13h:16m:19s remains)
INFO - root - 2017-12-16 09:39:36.946550: step 38580, loss = 0.61, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 13h:39m:48s remains)
INFO - root - 2017-12-16 09:39:38.636024: step 38590, loss = 0.62, batch loss = 0.36 (47.4 examples/sec; 0.169 sec/batch; 13h:47m:10s remains)
INFO - root - 2017-12-16 09:39:40.270774: step 38600, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:42m:09s remains)
2017-12-16 09:39:40.721331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5227628 -3.8287768 -2.9974623 -2.1056337 -1.3476743 -2.0624053 -3.172545 -3.7807057 -3.8069344 -3.4143062 -3.0648232 -2.936166 -3.1273882 -3.4741082 -3.5008812][-3.91224 -3.070503 -2.0917165 -1.1632353 -0.44614542 -1.3447853 -2.6504304 -3.4243302 -3.4726324 -3.0709476 -2.759052 -2.7793622 -3.080179 -3.3881416 -3.3139489][-3.0388036 -2.0902011 -1.0203853 -0.071476221 0.67200255 -0.37922764 -1.8247268 -2.6847396 -2.7639444 -2.3201826 -2.0835798 -2.309876 -2.7842355 -3.0585444 -2.9008083][-2.1735275 -1.1734965 -0.040287018 1.0508604 1.925441 0.81250143 -0.71213245 -1.657012 -1.8353767 -1.3864653 -1.2341701 -1.6864941 -2.2554765 -2.5578477 -2.3687651][-1.7318711 -0.68055606 0.57349229 1.9476659 3.0448315 1.984535 0.35225987 -0.72868431 -0.99822676 -0.55545509 -0.47273111 -1.1073332 -1.7960556 -2.098146 -1.8604262][-1.6509948 -0.57304597 0.83107972 2.4978426 3.9275768 2.957989 1.222239 0.003557682 -0.33084536 0.050667524 0.036703825 -0.74277472 -1.581444 -1.8891389 -1.6116065][-1.6890152 -0.56693935 0.95160031 2.933223 4.72439 3.7217972 1.7526615 0.36007833 -0.057088137 0.2127223 0.033910036 -0.8524586 -1.7578031 -2.0693984 -1.7435098][-1.8050687 -0.59319425 1.0687785 3.27296 5.1259041 3.9117529 1.7465761 0.27727509 -0.17575598 -0.074209929 -0.31933284 -1.1394203 -1.9203117 -2.113378 -1.7047172][-1.8295144 -0.589903 1.1478224 3.3284314 4.9158239 3.5606945 1.4383838 0.13874722 -0.27104282 -0.27104187 -0.55202532 -1.1907372 -1.7236822 -1.7156284 -1.2107165][-1.8391004 -0.65405571 1.0210471 3.0256593 4.2389154 2.8625829 0.93475795 -0.18276143 -0.53473818 -0.5601759 -0.83719754 -1.2633507 -1.4721905 -1.206513 -0.64829814][-2.1305165 -1.112015 0.38408709 2.0862515 3.0268476 1.8637841 0.2682755 -0.6955719 -1.0239545 -1.1170924 -1.3584558 -1.5937786 -1.5330081 -1.0780183 -0.557142][-2.7141967 -1.8922987 -0.66120279 0.73320246 1.4877844 0.62927437 -0.632251 -1.4364287 -1.7079561 -1.8295734 -2.0361223 -2.1559472 -1.9535458 -1.4592309 -1.0326269][-3.4015598 -2.8254533 -1.9316778 -0.90246212 -0.34040141 -0.89914036 -1.7836525 -2.3524325 -2.5281308 -2.6095767 -2.7336345 -2.7599721 -2.5364017 -2.1453 -1.9023312][-3.9853497 -3.7281685 -3.2237704 -2.5862842 -2.1767771 -2.4569037 -2.9620228 -3.2824223 -3.3846228 -3.407371 -3.468056 -3.4861498 -3.3451107 -3.1086953 -2.9858394][-4.1540589 -4.1666994 -3.9665437 -3.5933065 -3.3115578 -3.3932776 -3.5955915 -3.7289073 -3.7678983 -3.7763233 -3.8197441 -3.8511932 -3.8177958 -3.7116566 -3.6636953]]...]
INFO - root - 2017-12-16 09:39:42.370085: step 38610, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:22m:56s remains)
INFO - root - 2017-12-16 09:39:44.027251: step 38620, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 13h:24m:40s remains)
INFO - root - 2017-12-16 09:39:45.699882: step 38630, loss = 0.55, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 13h:16m:43s remains)
INFO - root - 2017-12-16 09:39:47.362741: step 38640, loss = 0.52, batch loss = 0.26 (47.1 examples/sec; 0.170 sec/batch; 13h:51m:18s remains)
INFO - root - 2017-12-16 09:39:49.066995: step 38650, loss = 0.49, batch loss = 0.24 (46.4 examples/sec; 0.172 sec/batch; 14h:04m:23s remains)
INFO - root - 2017-12-16 09:39:50.736156: step 38660, loss = 0.64, batch loss = 0.39 (46.9 examples/sec; 0.170 sec/batch; 13h:54m:43s remains)
INFO - root - 2017-12-16 09:39:52.457601: step 38670, loss = 0.55, batch loss = 0.29 (46.5 examples/sec; 0.172 sec/batch; 14h:01m:37s remains)
INFO - root - 2017-12-16 09:39:54.160388: step 38680, loss = 0.55, batch loss = 0.29 (47.2 examples/sec; 0.170 sec/batch; 13h:50m:40s remains)
INFO - root - 2017-12-16 09:39:55.811883: step 38690, loss = 0.68, batch loss = 0.42 (48.2 examples/sec; 0.166 sec/batch; 13h:32m:55s remains)
INFO - root - 2017-12-16 09:39:57.483053: step 38700, loss = 0.56, batch loss = 0.30 (45.0 examples/sec; 0.178 sec/batch; 14h:30m:39s remains)
2017-12-16 09:39:57.953730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.65382373 -0.62786973 -0.81265056 -1.3628548 -2.0745754 -2.65689 -2.9589062 -2.9520113 -2.7214885 -2.2244592 -1.4572568 -0.64693689 -0.080692291 0.0377872 -0.26552367][-0.92967641 -0.79888 -0.80031073 -1.1963284 -1.8020273 -2.3884299 -2.7774742 -2.8775625 -2.793555 -2.4101932 -1.649823 -0.70989752 0.079913855 0.3813591 0.13113046][-1.5038794 -1.3602251 -1.2150934 -1.3836516 -1.7825737 -2.1875732 -2.4110625 -2.4261858 -2.3755858 -2.2297621 -1.6914598 -0.90980864 -0.12806225 0.3804934 0.33527493][-2.1522555 -2.06538 -1.9623257 -1.9758083 -2.1577039 -2.3567033 -2.3026893 -1.9514792 -1.7639612 -1.7448695 -1.5054781 -0.99891984 -0.35848331 0.14670515 0.27139568][-2.4177756 -2.4519956 -2.4479659 -2.4091151 -2.4385126 -2.3959801 -2.0490952 -1.4112794 -1.0038871 -1.0474017 -1.0597458 -0.83126128 -0.42436337 -0.036727905 0.19869304][-2.0232096 -2.0944448 -2.1952536 -2.1045952 -1.9420314 -1.6830337 -1.0673913 -0.22230411 0.38782096 0.30879617 0.0065813065 -0.12308574 -0.10754132 0.010134935 0.22920728][-0.98513615 -1.0749032 -1.1954323 -0.98545682 -0.61263931 -0.17320395 0.43810034 1.2490106 1.8100092 1.6886647 1.249505 0.78832626 0.40942168 0.18732 0.14206219][0.36621404 0.32714176 0.27850366 0.60158706 1.0874786 1.5550568 1.8853176 2.1717455 2.3298461 2.0919731 1.6265414 1.1396232 0.67977023 0.29069543 -0.018788815][1.1912971 1.2155905 1.2244227 1.5844393 2.0862277 2.501435 2.5332835 2.3555567 2.1139357 1.6421583 1.1294134 0.67969704 0.35351872 0.089624405 -0.22016001][0.62704277 0.77698255 0.88670731 1.1157525 1.4347444 1.757241 1.8149836 1.4831977 1.0144889 0.42460942 -0.00932312 -0.25580931 -0.31890583 -0.27145767 -0.36723042][-0.94960093 -0.6573118 -0.46257508 -0.41471767 -0.33090234 -0.15486884 0.037858009 -0.10633278 -0.42914462 -0.90323722 -1.1926991 -1.3222128 -1.1949937 -0.88797283 -0.72662067][-2.2597914 -1.7600093 -1.325484 -1.2122567 -1.2719877 -1.2272536 -1.0474155 -1.0997554 -1.2445738 -1.5072596 -1.6561193 -1.7956281 -1.7026269 -1.3392224 -1.0920628][-2.3214645 -1.6348314 -0.90064168 -0.47146368 -0.48015821 -0.56594515 -0.58487 -0.72299922 -0.94512153 -1.2295475 -1.5073862 -1.8677642 -1.9761399 -1.7594789 -1.5887926][-1.2738612 -0.52668858 0.37968969 1.0142121 1.120059 0.88613009 0.55166912 0.18085957 -0.33664632 -0.92844045 -1.5580366 -2.1952789 -2.5100293 -2.469758 -2.4117172][-0.35718322 0.12093735 0.94658709 1.6601985 1.8912199 1.663707 1.148901 0.44170117 -0.44645488 -1.2907653 -2.0731606 -2.751379 -3.0770693 -3.0793147 -3.0639052]]...]
INFO - root - 2017-12-16 09:39:59.663920: step 38710, loss = 0.47, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 13h:41m:33s remains)
INFO - root - 2017-12-16 09:40:01.345051: step 38720, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 13h:18m:47s remains)
INFO - root - 2017-12-16 09:40:03.022924: step 38730, loss = 0.58, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:35m:01s remains)
INFO - root - 2017-12-16 09:40:04.717149: step 38740, loss = 0.68, batch loss = 0.42 (48.5 examples/sec; 0.165 sec/batch; 13h:26m:54s remains)
INFO - root - 2017-12-16 09:40:06.393703: step 38750, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 13h:24m:21s remains)
INFO - root - 2017-12-16 09:40:08.101733: step 38760, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 13h:33m:59s remains)
INFO - root - 2017-12-16 09:40:09.793373: step 38770, loss = 0.63, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 13h:27m:17s remains)
INFO - root - 2017-12-16 09:40:11.501221: step 38780, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:22m:23s remains)
INFO - root - 2017-12-16 09:40:13.185232: step 38790, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 13h:33m:25s remains)
INFO - root - 2017-12-16 09:40:14.885541: step 38800, loss = 0.49, batch loss = 0.24 (46.7 examples/sec; 0.171 sec/batch; 13h:57m:51s remains)
2017-12-16 09:40:15.372478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8278794 -1.278965 -0.979247 -0.97031915 -1.3555524 -2.1115682 -3.054944 -3.8559237 -4.2523713 -4.1308217 -3.6512885 -3.2348387 -3.0606337 -3.1719987 -3.456887][-1.3608134 -0.80156434 -0.43462956 -0.38615167 -0.7573334 -1.515833 -2.5093343 -3.3804936 -3.8527184 -3.7461207 -3.2303405 -2.8319097 -2.7920783 -3.1036139 -3.5537698][-1.4608929 -0.88459408 -0.37455034 -0.13930082 -0.35813284 -0.97802162 -1.824481 -2.5994194 -3.0454659 -2.9916089 -2.5980966 -2.3479319 -2.4382622 -2.824116 -3.2885683][-1.9543252 -1.3514324 -0.6153605 -0.126086 -0.13344717 -0.55947912 -1.1506962 -1.7366254 -2.1522927 -2.2603073 -2.1367331 -2.1520076 -2.4066856 -2.7839124 -3.0706348][-2.1383562 -1.5225925 -0.59132528 0.14437819 0.3351388 0.092363596 -0.26113486 -0.65699065 -1.0409828 -1.3796049 -1.6755409 -2.1016531 -2.6157963 -3.0308325 -3.1441722][-1.9120784 -1.3976855 -0.44706845 0.4728384 0.89260149 0.91636992 0.88065481 0.78509426 0.50782394 -0.038926363 -0.77286506 -1.7007456 -2.6365497 -3.2681179 -3.4031234][-1.7823921 -1.4964167 -0.70224285 0.29335332 0.9290638 1.240751 1.5614457 1.8280213 1.7413647 1.083324 0.038448095 -1.2404238 -2.4589934 -3.2422266 -3.4475646][-2.1260016 -2.1035645 -1.5984178 -0.68904769 0.08021307 0.58596587 1.109061 1.57009 1.6231205 1.018539 -0.069566727 -1.3859454 -2.5492425 -3.1945302 -3.2754793][-2.869241 -2.9780207 -2.6761765 -1.9326209 -1.1828971 -0.66647184 -0.18301535 0.23548675 0.3048842 -0.17698479 -1.0881288 -2.1337962 -2.9346724 -3.2020082 -2.9767628][-3.3314803 -3.4025323 -3.1677358 -2.6032977 -2.0715032 -1.7867436 -1.5758432 -1.3882775 -1.4035387 -1.71668 -2.3018966 -2.9224329 -3.207516 -2.9775624 -2.3949358][-3.1047707 -3.0520365 -2.7839117 -2.3463018 -2.0875585 -2.1936784 -2.4236479 -2.6001744 -2.7614455 -2.9566998 -3.2037187 -3.3673348 -3.1672051 -2.5460496 -1.693886][-2.482935 -2.3326588 -1.9969128 -1.5922897 -1.5425324 -1.9953853 -2.6677251 -3.2425218 -3.6205411 -3.7798996 -3.777225 -3.5796094 -3.0762074 -2.2901278 -1.3717942][-1.8541139 -1.6729217 -1.2196767 -0.72338641 -0.7387706 -1.4174621 -2.4078457 -3.3218255 -3.9848762 -4.28212 -4.2349491 -3.9154048 -3.3707931 -2.6868618 -1.9682211][-1.6359779 -1.4731588 -0.93848705 -0.31071806 -0.2441988 -0.92299056 -1.9819963 -3.044652 -3.9365692 -4.4809437 -4.6030097 -4.3929987 -4.0283446 -3.5970564 -3.1602228][-1.6754916 -1.4818096 -0.84990728 -0.097566366 0.056972504 -0.5788101 -1.6115799 -2.7000864 -3.7067409 -4.4077435 -4.6588488 -4.5870771 -4.4335275 -4.2442336 -4.0056648]]...]
INFO - root - 2017-12-16 09:40:17.047563: step 38810, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:23m:01s remains)
INFO - root - 2017-12-16 09:40:18.717908: step 38820, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.163 sec/batch; 13h:19m:57s remains)
INFO - root - 2017-12-16 09:40:20.433541: step 38830, loss = 0.51, batch loss = 0.25 (50.2 examples/sec; 0.159 sec/batch; 12h:59m:45s remains)
INFO - root - 2017-12-16 09:40:22.123756: step 38840, loss = 0.52, batch loss = 0.27 (44.7 examples/sec; 0.179 sec/batch; 14h:35m:10s remains)
INFO - root - 2017-12-16 09:40:23.837722: step 38850, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.168 sec/batch; 13h:44m:09s remains)
INFO - root - 2017-12-16 09:40:25.508668: step 38860, loss = 0.71, batch loss = 0.45 (45.3 examples/sec; 0.177 sec/batch; 14h:24m:18s remains)
INFO - root - 2017-12-16 09:40:27.181629: step 38870, loss = 0.53, batch loss = 0.27 (48.0 examples/sec; 0.167 sec/batch; 13h:36m:07s remains)
INFO - root - 2017-12-16 09:40:28.880226: step 38880, loss = 0.49, batch loss = 0.23 (46.4 examples/sec; 0.173 sec/batch; 14h:04m:17s remains)
INFO - root - 2017-12-16 09:40:30.596678: step 38890, loss = 0.73, batch loss = 0.47 (46.4 examples/sec; 0.173 sec/batch; 14h:04m:18s remains)
INFO - root - 2017-12-16 09:40:32.286738: step 38900, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 13h:39m:34s remains)
2017-12-16 09:40:32.804122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5842619 -3.8827734 -3.9096003 -3.8863244 -3.8134184 -3.4402559 -3.0455041 -2.8596578 -2.7586994 -2.55516 -2.6604247 -3.1864793 -3.6546004 -3.6976304 -3.2596085][-2.9445114 -3.2257752 -3.3744855 -3.3870926 -3.2119935 -2.6960092 -2.2040658 -1.9540968 -1.7575729 -1.511694 -1.7302842 -2.5121155 -3.1941113 -3.4052851 -3.0375619][-2.4983859 -2.796644 -3.0443835 -3.0845129 -2.7039552 -1.8675157 -1.1577195 -0.82364488 -0.55090833 -0.39558303 -0.85914588 -1.9391332 -2.91843 -3.3513098 -3.0780351][-2.2329521 -2.5202613 -2.835382 -2.8396943 -2.2042456 -0.95575285 0.10686088 0.61524796 0.97074461 0.91902494 0.11824059 -1.3105791 -2.611587 -3.3335934 -3.2919657][-2.0412076 -2.3467615 -2.6793432 -2.6260958 -1.7146738 -0.059013367 1.4228008 2.2203779 2.6213722 2.2436323 1.1218886 -0.65022075 -2.2543452 -3.2853074 -3.5171309][-1.6851504 -2.0308251 -2.32705 -2.145889 -0.92330265 1.1190863 2.9775338 3.9958663 4.2814078 3.4777446 1.9297071 -0.11610317 -1.8962719 -3.0954025 -3.5381575][-1.1972208 -1.5399325 -1.7015224 -1.2804126 0.192209 2.4882889 4.6306047 5.6047659 5.6257792 4.3522329 2.4262891 0.20741701 -1.6873796 -2.8647435 -3.3342423][-0.87070811 -1.2679938 -1.3169186 -0.87051082 0.54980445 2.8009534 5.0458865 6.003768 5.7639971 4.284534 2.3216586 0.18596816 -1.6601106 -2.7355688 -3.1352901][-1.2130973 -1.6988508 -1.7875153 -1.4274656 -0.2732296 1.5491934 3.4758425 4.5133739 4.468751 3.3540211 1.7873669 -0.085155249 -1.7976348 -2.8036144 -3.1518631][-2.0402932 -2.5346065 -2.6726356 -2.443701 -1.5282371 -0.1379137 1.2798529 2.3552866 2.6399426 2.0527945 1.0280993 -0.50911963 -2.0553985 -3.0475283 -3.400888][-3.1060729 -3.4013934 -3.4688988 -3.273253 -2.575803 -1.5961524 -0.5521307 0.5558176 1.2215345 1.1613481 0.48587108 -0.8595376 -2.2412882 -3.2802477 -3.7084162][-4.1040454 -4.2057309 -4.1046424 -3.8518078 -3.2985816 -2.6039889 -1.7932009 -0.6337496 0.43649411 0.91423512 0.45796084 -0.73220921 -1.9869603 -3.0538576 -3.64642][-4.6091986 -4.5693226 -4.2786922 -3.8987584 -3.4960546 -2.9897597 -2.3332653 -1.1718941 0.15384793 0.96097016 0.63811326 -0.38765252 -1.4754099 -2.5487373 -3.2308917][-4.5717998 -4.435998 -4.0628085 -3.6708746 -3.4204319 -3.2114573 -2.8190989 -1.8298727 -0.49004841 0.36426425 0.12364101 -0.70238268 -1.5110519 -2.4314363 -3.041374][-3.9966149 -3.8751531 -3.5993981 -3.4390717 -3.4280658 -3.5063083 -3.4289284 -2.7818186 -1.6278892 -0.7942425 -0.98564315 -1.645098 -2.1587522 -2.7955418 -3.198565]]...]
INFO - root - 2017-12-16 09:40:34.510029: step 38910, loss = 0.70, batch loss = 0.44 (48.6 examples/sec; 0.165 sec/batch; 13h:26m:16s remains)
INFO - root - 2017-12-16 09:40:36.207261: step 38920, loss = 0.67, batch loss = 0.41 (46.5 examples/sec; 0.172 sec/batch; 14h:02m:41s remains)
INFO - root - 2017-12-16 09:40:37.887138: step 38930, loss = 0.65, batch loss = 0.40 (46.3 examples/sec; 0.173 sec/batch; 14h:04m:56s remains)
INFO - root - 2017-12-16 09:40:39.560405: step 38940, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:31m:46s remains)
INFO - root - 2017-12-16 09:40:41.231026: step 38950, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 13h:38m:30s remains)
INFO - root - 2017-12-16 09:40:42.920567: step 38960, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 13h:38m:35s remains)
INFO - root - 2017-12-16 09:40:44.601119: step 38970, loss = 0.67, batch loss = 0.41 (48.7 examples/sec; 0.164 sec/batch; 13h:24m:12s remains)
INFO - root - 2017-12-16 09:40:46.276721: step 38980, loss = 0.66, batch loss = 0.40 (48.8 examples/sec; 0.164 sec/batch; 13h:22m:46s remains)
INFO - root - 2017-12-16 09:40:47.949262: step 38990, loss = 0.68, batch loss = 0.42 (47.2 examples/sec; 0.169 sec/batch; 13h:48m:59s remains)
INFO - root - 2017-12-16 09:40:49.649564: step 39000, loss = 0.50, batch loss = 0.25 (46.8 examples/sec; 0.171 sec/batch; 13h:55m:51s remains)
2017-12-16 09:40:50.131695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1497619 -1.9293829 -1.8156304 -1.9920537 -2.5852108 -3.1614866 -3.4860778 -3.7681375 -4.1024647 -4.2903452 -4.3004074 -4.1397038 -3.89262 -3.6336498 -3.3928032][-1.117743 -0.96530795 -0.96422184 -1.3338444 -2.1449637 -2.9182916 -3.3881395 -3.7534108 -4.08063 -4.2807622 -4.2856355 -4.1307945 -3.8646233 -3.545187 -3.2490621][-0.14092803 -0.1571126 -0.37254262 -0.91202819 -1.7713979 -2.5787621 -3.1054034 -3.4300976 -3.719039 -3.9067123 -3.9250832 -3.7808197 -3.5337458 -3.2235842 -2.9184594][0.70631123 0.57591414 0.24585986 -0.3117063 -1.0906296 -1.7957743 -2.1987326 -2.3556123 -2.509311 -2.619442 -2.6393256 -2.595659 -2.4916723 -2.3162577 -2.1014][1.342675 1.2153881 0.96942949 0.58754992 0.081629753 -0.37179613 -0.53045619 -0.48147428 -0.53707528 -0.69493711 -0.79275739 -0.899789 -1.0374526 -1.0860924 -1.0329545][1.1997156 1.2146099 1.1599269 1.1028728 0.98482537 0.89921141 1.0458636 1.2883759 1.2601311 1.0013571 0.78824162 0.52643919 0.15195227 -0.14596558 -0.27035189][0.64582396 0.82776713 0.99125671 1.3265319 1.6483963 1.9724009 2.4292872 2.7921069 2.7640479 2.3538058 2.0348089 1.6107311 1.0288188 0.49469066 0.18972731][-0.063781977 0.2209549 0.65409803 1.376276 2.1574767 2.9477961 3.7211907 4.2771187 4.2972946 3.6673138 3.1317675 2.4585969 1.6076612 0.76431942 0.22852945][-0.87342846 -0.60094011 -0.14480782 0.56942248 1.4748025 2.3628461 3.14556 3.6128685 3.5047181 2.8344047 2.1775687 1.4866922 0.63889313 -0.14913368 -0.62645042][-1.7173913 -1.5529191 -1.2599657 -0.74739337 -0.03332448 0.6683166 1.2218449 1.436924 1.1997244 0.63127804 0.079871655 -0.44168961 -1.0537914 -1.5381359 -1.7755678][-2.5237281 -2.5400286 -2.4647198 -2.2110932 -1.7664979 -1.3125834 -0.96403921 -0.87392807 -1.10209 -1.4844639 -1.8422208 -2.1801751 -2.5283797 -2.7481425 -2.7452188][-3.0931613 -3.2236881 -3.3029718 -3.2478628 -3.0557759 -2.8350685 -2.6643474 -2.6523118 -2.8308024 -3.0776176 -3.315742 -3.5377421 -3.697453 -3.6968718 -3.4905975][-3.3295488 -3.5274253 -3.6598921 -3.6878495 -3.6404119 -3.5698757 -3.5269952 -3.5752268 -3.7189159 -3.8962812 -4.0479116 -4.1502709 -4.16382 -4.0369778 -3.6871357][-3.1425254 -3.4086797 -3.5989347 -3.70114 -3.7413521 -3.761961 -3.7898006 -3.8539138 -3.9578094 -4.0487032 -4.1011987 -4.108139 -4.0213332 -3.8250318 -3.4536781][-2.734802 -3.0168686 -3.2310412 -3.3641958 -3.4374371 -3.4884362 -3.5460477 -3.6239147 -3.7088704 -3.7729495 -3.789696 -3.7465007 -3.5999784 -3.375412 -3.0803237]]...]
INFO - root - 2017-12-16 09:40:51.830247: step 39010, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.169 sec/batch; 13h:48m:56s remains)
INFO - root - 2017-12-16 09:40:53.519834: step 39020, loss = 0.49, batch loss = 0.23 (45.2 examples/sec; 0.177 sec/batch; 14h:24m:59s remains)
INFO - root - 2017-12-16 09:40:55.202558: step 39030, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 13h:46m:22s remains)
INFO - root - 2017-12-16 09:40:56.873068: step 39040, loss = 0.48, batch loss = 0.22 (48.5 examples/sec; 0.165 sec/batch; 13h:26m:38s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:40:58.571723: step 39050, loss = 0.52, batch loss = 0.26 (43.3 examples/sec; 0.185 sec/batch; 15h:03m:12s remains)
INFO - root - 2017-12-16 09:41:00.249808: step 39060, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 13h:33m:50s remains)
INFO - root - 2017-12-16 09:41:01.924619: step 39070, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:28m:29s remains)
INFO - root - 2017-12-16 09:41:03.599560: step 39080, loss = 0.61, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 13h:47m:11s remains)
INFO - root - 2017-12-16 09:41:05.323134: step 39090, loss = 0.47, batch loss = 0.21 (47.2 examples/sec; 0.170 sec/batch; 13h:49m:00s remains)
INFO - root - 2017-12-16 09:41:07.082503: step 39100, loss = 0.46, batch loss = 0.20 (46.7 examples/sec; 0.171 sec/batch; 13h:57m:22s remains)
2017-12-16 09:41:07.583931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.228026 -1.4313805 -1.5960755 -1.6629443 -1.5262231 -1.183373 -0.80691469 -0.62467587 -0.68322861 -0.97126973 -1.3940616 -1.7578137 -1.91215 -1.8700101 -1.7759407][-1.2232758 -1.3226099 -1.4703891 -1.6241179 -1.6164833 -1.3820751 -1.0431247 -0.82959092 -0.8133266 -1.0620673 -1.5500157 -2.0478945 -2.3040497 -2.3292274 -2.3165104][-1.1062076 -1.0916903 -1.220634 -1.4846003 -1.6541228 -1.5987221 -1.3757889 -1.1669697 -1.0527459 -1.185639 -1.6458387 -2.1907218 -2.4879696 -2.5445383 -2.5795326][-0.99278927 -0.91756558 -1.0536996 -1.4092062 -1.7378168 -1.8558521 -1.7535489 -1.5366619 -1.2798481 -1.2380018 -1.5761769 -2.0745823 -2.347445 -2.3855522 -2.3966036][-1.0752935 -1.003399 -1.1420755 -1.4937586 -1.8435042 -2.0073206 -1.931653 -1.6535659 -1.2457582 -1.0485058 -1.2813654 -1.7461303 -2.0260286 -2.025321 -1.9425991][-1.4010186 -1.3391327 -1.404163 -1.5922816 -1.7642498 -1.8134413 -1.6526563 -1.2694732 -0.7630316 -0.50856268 -0.74347723 -1.266993 -1.6617184 -1.7259822 -1.5730512][-1.7166555 -1.6433853 -1.5614349 -1.4749897 -1.3391191 -1.1677167 -0.85646546 -0.35435653 0.20099759 0.4263382 0.10297203 -0.56197131 -1.1737561 -1.4322281 -1.3244495][-1.7921932 -1.6899493 -1.4407859 -1.0864408 -0.67110634 -0.29691911 0.13745213 0.69912148 1.2381694 1.3897026 0.96982574 0.19968295 -0.61119545 -1.1177068 -1.1730812][-1.6047764 -1.4847298 -1.1595818 -0.70965159 -0.22212863 0.17946172 0.60516858 1.1271651 1.5832725 1.6853278 1.3148384 0.588145 -0.2820363 -0.9477787 -1.2072865][-1.3618991 -1.2800804 -1.017759 -0.66680753 -0.3371737 -0.084414244 0.20211554 0.60439253 0.97532797 1.134712 0.9590683 0.45621371 -0.24990678 -0.86738169 -1.2171022][-1.2233663 -1.1906077 -1.0558643 -0.91084027 -0.8140924 -0.76280022 -0.6589781 -0.42820203 -0.14223313 0.085683107 0.15008569 -0.018860102 -0.41296077 -0.81875694 -1.0797329][-1.1944631 -1.2191983 -1.2255931 -1.2697015 -1.3500431 -1.44449 -1.4919274 -1.4171507 -1.2202585 -0.97884858 -0.77462435 -0.69423234 -0.808133 -0.97297561 -1.0856287][-1.2985114 -1.3590213 -1.4397988 -1.5672826 -1.7162199 -1.8605847 -1.9682287 -1.9842052 -1.8754593 -1.6834338 -1.480659 -1.3452953 -1.3423761 -1.4031515 -1.4340103][-1.5206552 -1.5433571 -1.5968778 -1.6771501 -1.7797346 -1.8797926 -1.9695183 -2.0283327 -2.0086672 -1.9235117 -1.8330425 -1.7830844 -1.8157945 -1.8748441 -1.8852674][-1.6980567 -1.644766 -1.605994 -1.5800266 -1.5790904 -1.60695 -1.6746318 -1.7668507 -1.8405644 -1.8895767 -1.9375811 -2.0134604 -2.1100039 -2.1871951 -2.1977029]]...]
INFO - root - 2017-12-16 09:41:09.258641: step 39110, loss = 0.48, batch loss = 0.22 (49.2 examples/sec; 0.162 sec/batch; 13h:14m:32s remains)
INFO - root - 2017-12-16 09:41:10.911530: step 39120, loss = 0.62, batch loss = 0.36 (47.1 examples/sec; 0.170 sec/batch; 13h:50m:20s remains)
INFO - root - 2017-12-16 09:41:12.607690: step 39130, loss = 0.45, batch loss = 0.19 (47.1 examples/sec; 0.170 sec/batch; 13h:50m:40s remains)
INFO - root - 2017-12-16 09:41:14.340701: step 39140, loss = 0.55, batch loss = 0.29 (46.4 examples/sec; 0.172 sec/batch; 14h:03m:23s remains)
INFO - root - 2017-12-16 09:41:16.026581: step 39150, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.169 sec/batch; 13h:47m:57s remains)
INFO - root - 2017-12-16 09:41:17.711645: step 39160, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:28m:36s remains)
INFO - root - 2017-12-16 09:41:19.389017: step 39170, loss = 0.53, batch loss = 0.27 (46.1 examples/sec; 0.173 sec/batch; 14h:08m:12s remains)
INFO - root - 2017-12-16 09:41:21.084550: step 39180, loss = 0.52, batch loss = 0.26 (47.9 examples/sec; 0.167 sec/batch; 13h:36m:26s remains)
INFO - root - 2017-12-16 09:41:22.753469: step 39190, loss = 0.57, batch loss = 0.31 (46.9 examples/sec; 0.171 sec/batch; 13h:54m:41s remains)
INFO - root - 2017-12-16 09:41:24.425750: step 39200, loss = 0.57, batch loss = 0.31 (48.8 examples/sec; 0.164 sec/batch; 13h:21m:45s remains)
2017-12-16 09:41:24.900192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12631369 0.58019924 0.69851184 0.38682032 0.16729641 0.47941375 1.1853859 1.480922 0.95500588 -0.090262175 -1.065909 -1.848935 -2.3710775 -2.4695408 -2.2759333][0.08917284 0.66361189 0.90138054 0.63796735 0.26526237 0.24416637 0.60897231 0.76529455 0.262321 -0.64313495 -1.4945441 -2.1695561 -2.5633066 -2.5752563 -2.3305273][-0.32587409 0.30822611 0.6698699 0.51648164 0.077120781 -0.30096889 -0.31553888 -0.2942946 -0.62959146 -1.3112109 -2.0122607 -2.5839238 -2.8566604 -2.7390902 -2.402884][-0.98951364 -0.38434196 0.034430504 0.027629614 -0.36886239 -0.86133397 -1.1606072 -1.2942917 -1.5245662 -2.0112975 -2.5436187 -2.9583836 -3.0611939 -2.8140218 -2.4193864][-1.7674257 -1.2407748 -0.81297505 -0.62913346 -0.84375978 -1.2686265 -1.6593769 -1.9034972 -2.1688452 -2.5190747 -2.85853 -3.0969198 -3.0622835 -2.7550733 -2.3556168][-2.6164179 -2.1708691 -1.7485497 -1.4094884 -1.3226047 -1.4748751 -1.7406932 -1.9843001 -2.2626569 -2.5410609 -2.7625551 -2.8871434 -2.8104157 -2.5386906 -2.2147598][-3.4029694 -3.0216162 -2.5647612 -2.02494 -1.562899 -1.3129056 -1.2191135 -1.2853718 -1.5382721 -1.8520565 -2.0947118 -2.2811093 -2.3172603 -2.2113867 -2.0417354][-3.8379178 -3.5271575 -3.0164678 -2.2443004 -1.3964996 -0.72151506 -0.2002821 0.0097296238 -0.236583 -0.65858364 -1.0686007 -1.4947213 -1.8042519 -1.9479516 -1.9447803][-3.8204961 -3.6153202 -3.0806806 -2.1410747 -1.0444862 -0.051555634 0.74437714 1.1088591 0.92781544 0.42157078 -0.20887518 -0.90649581 -1.4978663 -1.8727765 -1.9806721][-3.2721312 -3.2410955 -2.8277652 -1.9218583 -0.79339683 0.29005718 1.2130036 1.6901333 1.5639715 1.00068 0.18697309 -0.69070423 -1.4486761 -1.9347124 -2.0838327][-2.3336694 -2.5594478 -2.40278 -1.7862611 -0.89614415 0.043929338 0.95219088 1.503758 1.4644401 0.96662068 0.16431212 -0.72072208 -1.4976468 -2.0120327 -2.1620426][-1.3320014 -1.9401639 -2.1065695 -1.9272606 -1.4286312 -0.73874652 0.045779228 0.69513488 0.86958122 0.59840369 -0.018428802 -0.78125107 -1.4497707 -1.9316573 -2.1279798][-0.81810844 -1.8073853 -2.2745667 -2.4916241 -2.3760509 -1.9314659 -1.2196982 -0.44164944 0.044347763 0.1204977 -0.19028234 -0.70453286 -1.193639 -1.6287714 -1.9485161][-0.855698 -2.0764079 -2.773525 -3.257273 -3.4254346 -3.1594102 -2.4438894 -1.469851 -0.6050458 -0.14381814 -0.12183332 -0.36470246 -0.6818794 -1.1331604 -1.6638644][-1.3672361 -2.5714166 -3.3134785 -3.8564425 -4.1056633 -3.9212747 -3.1510091 -1.9885234 -0.84531474 -0.13198566 0.13825703 0.089210272 -0.15150332 -0.67704749 -1.3918586]]...]
INFO - root - 2017-12-16 09:41:26.603981: step 39210, loss = 0.53, batch loss = 0.27 (47.3 examples/sec; 0.169 sec/batch; 13h:47m:21s remains)
INFO - root - 2017-12-16 09:41:28.299323: step 39220, loss = 0.57, batch loss = 0.31 (47.6 examples/sec; 0.168 sec/batch; 13h:41m:14s remains)
INFO - root - 2017-12-16 09:41:29.987193: step 39230, loss = 0.50, batch loss = 0.24 (47.0 examples/sec; 0.170 sec/batch; 13h:51m:21s remains)
INFO - root - 2017-12-16 09:41:31.679850: step 39240, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 13h:26m:56s remains)
INFO - root - 2017-12-16 09:41:33.368560: step 39250, loss = 0.60, batch loss = 0.34 (46.7 examples/sec; 0.171 sec/batch; 13h:58m:07s remains)
INFO - root - 2017-12-16 09:41:35.073975: step 39260, loss = 0.67, batch loss = 0.41 (46.8 examples/sec; 0.171 sec/batch; 13h:54m:38s remains)
INFO - root - 2017-12-16 09:41:36.747328: step 39270, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:35m:32s remains)
INFO - root - 2017-12-16 09:41:38.397220: step 39280, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 13h:24m:32s remains)
INFO - root - 2017-12-16 09:41:40.040679: step 39290, loss = 0.51, batch loss = 0.25 (46.7 examples/sec; 0.171 sec/batch; 13h:57m:29s remains)
INFO - root - 2017-12-16 09:41:41.717152: step 39300, loss = 0.52, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:29m:30s remains)
2017-12-16 09:41:42.232040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.8597647 -1.0886234 -1.4700911 -1.6754079 -1.5728354 -1.2032206 -0.73795319 -0.59259772 -0.98459625 -1.5660191 -1.997776 -2.2110786 -2.2083845 -2.1152117 -2.0316954][-0.76312411 -1.0915689 -1.5788186 -1.9765877 -2.0439441 -1.6861523 -1.0842452 -0.7161895 -0.98010528 -1.5866942 -2.0620787 -2.28746 -2.2789414 -2.1705751 -2.0695705][-1.0807644 -1.3605683 -1.7401934 -2.0631704 -2.0887847 -1.6673362 -0.93625104 -0.4605664 -0.70359826 -1.4499074 -2.0838084 -2.3726635 -2.3777065 -2.2664318 -2.1413052][-1.5155327 -1.7027912 -1.8810167 -1.9348418 -1.704847 -1.0655662 -0.21310091 0.30391192 -0.060564041 -1.0683609 -1.9827586 -2.4438112 -2.5177982 -2.4154449 -2.2465403][-1.9195244 -1.9611378 -1.8830795 -1.5963554 -1.0238125 -0.13790083 0.81424165 1.3430612 0.79257822 -0.48785138 -1.6996521 -2.3887966 -2.6034496 -2.5436509 -2.3379712][-2.0826402 -2.0216968 -1.7781769 -1.2644467 -0.41520524 0.67005515 1.6735115 2.1179585 1.4532907 0.034609795 -1.3360372 -2.2267814 -2.5825438 -2.5668969 -2.3544059][-1.9091578 -1.8781075 -1.6339984 -1.0790286 -0.21099448 0.87544537 1.8303151 2.2436666 1.6468992 0.31781006 -1.0620853 -2.097698 -2.5547779 -2.5582547 -2.3352718][-1.4903126 -1.5603414 -1.4275845 -1.0501021 -0.46916521 0.3500011 1.1543648 1.5982912 1.2471755 0.22116733 -1.0174679 -2.0827279 -2.5785241 -2.5578876 -2.317441][-0.90450025 -1.0960189 -1.1415213 -1.0515553 -0.91630328 -0.51290476 0.13256788 0.69003558 0.62360835 -0.1154952 -1.1572428 -2.1219091 -2.5842957 -2.5435569 -2.3116872][-0.60539412 -0.83649325 -0.99161339 -1.1176151 -1.3054127 -1.2323965 -0.673982 0.0078063011 0.14065003 -0.40111637 -1.3085 -2.1762519 -2.5814962 -2.5177166 -2.3059616][-0.79011738 -0.93375719 -1.1256632 -1.3732609 -1.6758275 -1.6623952 -1.0704204 -0.28080273 -0.048330307 -0.53701222 -1.4084537 -2.2458615 -2.6240072 -2.5605683 -2.3349276][-1.0717838 -1.0782684 -1.2533959 -1.5646079 -1.8858814 -1.8294823 -1.18484 -0.36115861 -0.10384512 -0.65173578 -1.5834517 -2.4302278 -2.7926354 -2.7083387 -2.4217696][-1.0073143 -0.96181333 -1.154387 -1.5065155 -1.823263 -1.7019863 -1.0241683 -0.24605083 -0.057676792 -0.72224641 -1.7255505 -2.5985136 -2.9600363 -2.8615885 -2.5205958][-0.45733488 -0.48767757 -0.79136264 -1.1882286 -1.4934851 -1.3327228 -0.61231387 0.15163326 0.21392226 -0.59608793 -1.7375598 -2.6895058 -3.054399 -2.9446924 -2.5733447][0.15125155 -0.075782776 -0.50931954 -0.93881774 -1.1872779 -0.94625151 -0.15644979 0.625582 0.6281178 -0.31048536 -1.5940185 -2.630651 -3.0416863 -2.9516664 -2.579886]]...]
INFO - root - 2017-12-16 09:41:43.928817: step 39310, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 13h:39m:49s remains)
INFO - root - 2017-12-16 09:41:45.629856: step 39320, loss = 0.55, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 13h:45m:57s remains)
INFO - root - 2017-12-16 09:41:47.304074: step 39330, loss = 0.61, batch loss = 0.35 (47.9 examples/sec; 0.167 sec/batch; 13h:35m:16s remains)
INFO - root - 2017-12-16 09:41:49.025684: step 39340, loss = 0.58, batch loss = 0.32 (47.3 examples/sec; 0.169 sec/batch; 13h:46m:10s remains)
INFO - root - 2017-12-16 09:41:50.723353: step 39350, loss = 0.54, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 13h:27m:49s remains)
INFO - root - 2017-12-16 09:41:52.390749: step 39360, loss = 0.67, batch loss = 0.41 (47.3 examples/sec; 0.169 sec/batch; 13h:46m:59s remains)
INFO - root - 2017-12-16 09:41:54.102026: step 39370, loss = 0.50, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:38m:56s remains)
INFO - root - 2017-12-16 09:41:55.766356: step 39380, loss = 0.62, batch loss = 0.36 (47.5 examples/sec; 0.168 sec/batch; 13h:42m:30s remains)
INFO - root - 2017-12-16 09:41:57.414449: step 39390, loss = 0.51, batch loss = 0.26 (48.3 examples/sec; 0.166 sec/batch; 13h:29m:47s remains)
INFO - root - 2017-12-16 09:41:59.086570: step 39400, loss = 0.55, batch loss = 0.29 (47.3 examples/sec; 0.169 sec/batch; 13h:47m:04s remains)
2017-12-16 09:41:59.544826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9555061 -2.9500074 -2.4657857 -1.8698068 -1.3687173 -1.2540752 -1.5146765 -1.7586763 -1.8266766 -2.0431685 -2.4372172 -2.6728911 -2.594111 -2.4029391 -2.3495979][-2.9439113 -3.0494614 -2.7499874 -2.24727 -1.7586696 -1.6518693 -1.954174 -2.2079694 -2.2846766 -2.5617695 -3.002254 -3.2099998 -3.1258824 -3.0766473 -3.2797537][-3.1371806 -3.3720489 -3.2654963 -2.857281 -2.3211141 -2.1370442 -2.360817 -2.5490084 -2.6308954 -2.941772 -3.3874111 -3.5815847 -3.5504224 -3.683136 -4.0651832][-3.4353592 -3.7523203 -3.7259066 -3.2790887 -2.5726731 -2.1539879 -2.1060262 -2.1190495 -2.2316415 -2.6395376 -3.1671855 -3.4406371 -3.5105693 -3.7262652 -4.152987][-3.7499094 -4.0894251 -3.935214 -3.1775725 -2.0030286 -1.0087721 -0.42954278 -0.25988698 -0.69735932 -1.5625184 -2.3682945 -2.8409638 -3.0047681 -3.157212 -3.4539423][-4.010148 -4.2751584 -3.8939474 -2.6410997 -0.75846422 1.1316326 2.5093749 2.7734663 1.6852663 0.0017595291 -1.3314922 -2.0127296 -2.1689229 -2.1593406 -2.2659366][-4.0784712 -4.2081871 -3.5763631 -1.8952413 0.65145516 3.5154588 5.8455572 6.0747232 4.0817614 1.4929237 -0.4329952 -1.3292304 -1.4067161 -1.2131028 -1.1371884][-3.9546647 -3.9380736 -3.1538162 -1.2481617 1.6244376 5.1705256 8.265934 8.2192612 5.3816004 2.2166708 -0.030219078 -1.0545615 -1.0224909 -0.67769957 -0.50067914][-3.706444 -3.6357894 -2.8457627 -1.1950346 1.1974201 4.1381874 6.4436131 6.2915382 3.9999235 1.3502183 -0.5638876 -1.392453 -1.2145978 -0.83349669 -0.62817109][-3.4007492 -3.3782742 -2.8436556 -1.7958232 -0.24158716 1.7381914 3.2705061 3.1985934 1.6844676 -0.18876863 -1.5811208 -2.1272132 -1.8632996 -1.5286238 -1.3695438][-3.1841185 -3.3355432 -3.2087486 -2.8010743 -2.0381832 -0.96916366 -0.14269686 -0.16276145 -0.97504067 -2.0312469 -2.7835631 -2.8818243 -2.4761007 -2.1951575 -2.1182027][-3.0009825 -3.2556543 -3.4036975 -3.4645972 -3.3135338 -2.9388869 -2.6216438 -2.5841162 -2.8321331 -3.2003109 -3.3672652 -3.0815415 -2.5831738 -2.4002914 -2.4690604][-2.8638439 -3.0684991 -3.2790391 -3.4774714 -3.5842433 -3.5351715 -3.4286976 -3.2924924 -3.2225728 -3.2392149 -3.1006744 -2.62059 -2.1178057 -2.07553 -2.3216588][-2.6926975 -2.6830614 -2.6945555 -2.78886 -2.9017177 -2.906033 -2.8081331 -2.6258574 -2.479744 -2.3859141 -2.1511271 -1.655468 -1.2643374 -1.428462 -1.9402573][-2.4635971 -2.1447141 -1.8283896 -1.6911327 -1.6983045 -1.6924436 -1.6290464 -1.4961605 -1.3994836 -1.3445528 -1.1525239 -0.77199113 -0.56328726 -0.92866182 -1.6779997]]...]
INFO - root - 2017-12-16 09:42:01.213441: step 39410, loss = 0.56, batch loss = 0.30 (47.9 examples/sec; 0.167 sec/batch; 13h:35m:22s remains)
INFO - root - 2017-12-16 09:42:02.912720: step 39420, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 13h:43m:40s remains)
INFO - root - 2017-12-16 09:42:04.600737: step 39430, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:34m:33s remains)
INFO - root - 2017-12-16 09:42:06.244901: step 39440, loss = 0.70, batch loss = 0.44 (49.3 examples/sec; 0.162 sec/batch; 13h:13m:07s remains)
INFO - root - 2017-12-16 09:42:07.922251: step 39450, loss = 0.61, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 13h:46m:20s remains)
INFO - root - 2017-12-16 09:42:09.615124: step 39460, loss = 0.59, batch loss = 0.33 (45.7 examples/sec; 0.175 sec/batch; 14h:14m:25s remains)
INFO - root - 2017-12-16 09:42:11.311485: step 39470, loss = 0.60, batch loss = 0.34 (48.5 examples/sec; 0.165 sec/batch; 13h:26m:04s remains)
INFO - root - 2017-12-16 09:42:12.979068: step 39480, loss = 0.51, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 13h:38m:15s remains)
INFO - root - 2017-12-16 09:42:14.634757: step 39490, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 13h:22m:28s remains)
INFO - root - 2017-12-16 09:42:16.271616: step 39500, loss = 0.53, batch loss = 0.27 (49.1 examples/sec; 0.163 sec/batch; 13h:15m:05s remains)
2017-12-16 09:42:16.791941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6287324 -1.7280755 -1.6072412 -1.2888144 -0.9535985 -0.60278714 -0.35976219 -0.40855074 -0.76206362 -1.3026289 -1.9185466 -2.3907373 -2.6151872 -2.587863 -2.4712219][-1.8954178 -1.9938687 -1.922792 -1.6831348 -1.4125762 -1.1383673 -0.92783 -0.86468089 -0.99882388 -1.3518876 -1.8298599 -2.2040994 -2.3371124 -2.2090163 -1.9740409][-1.7872844 -1.9129393 -1.9120616 -1.7950082 -1.6396254 -1.4255557 -1.2285364 -1.1210457 -1.1660138 -1.4121652 -1.7746637 -2.0214679 -2.0054669 -1.6833228 -1.2445699][-1.4660609 -1.5881258 -1.6365975 -1.5836623 -1.4945025 -1.3395624 -1.1445022 -1.0296313 -1.0875024 -1.3355727 -1.6448298 -1.800117 -1.6467078 -1.1201237 -0.40952647][-1.0616465 -1.1542125 -1.2031436 -1.1399338 -1.0186059 -0.87481189 -0.69037712 -0.58215153 -0.66835594 -0.95674109 -1.2670993 -1.4150972 -1.2469369 -0.65215719 0.20931101][-0.83910716 -0.88889623 -0.88790405 -0.71519589 -0.51236224 -0.38007474 -0.24327207 -0.14370465 -0.20659971 -0.47970819 -0.75622678 -0.88436651 -0.78171992 -0.29495025 0.50341487][-0.88315773 -0.88329089 -0.77679312 -0.42330754 -0.10965347 -0.017134666 0.0083053112 -0.00085806847 -0.06806612 -0.22776437 -0.27044249 -0.1936574 -0.088875055 0.18664026 0.69204473][-1.0766765 -1.015229 -0.77870595 -0.18959594 0.30783153 0.37514544 0.21824694 0.0019710064 -0.15657187 -0.18984437 0.11429477 0.56399751 0.74500728 0.74822044 0.78021073][-1.3820174 -1.2865729 -0.93258107 -0.13177967 0.62342811 0.79460645 0.51358604 0.055488348 -0.28332686 -0.31318045 0.19296384 0.94389009 1.2419834 1.0571125 0.679332][-1.8273697 -1.7719139 -1.3604279 -0.41615129 0.59515071 0.98387551 0.73017168 0.15387774 -0.3382113 -0.44374561 0.06588316 0.87594366 1.3026376 1.0940888 0.53000093][-2.1446726 -2.2098577 -1.8786867 -0.95878375 0.11443114 0.69103837 0.60898829 0.14099002 -0.35197496 -0.55252731 -0.17712998 0.58156395 1.1037269 1.022428 0.52313137][-2.162508 -2.3734336 -2.2352371 -1.5447044 -0.61675215 0.038762093 0.15090132 -0.095347643 -0.43969226 -0.66284704 -0.43956709 0.18622303 0.75533581 0.882478 0.6225996][-2.0321178 -2.3621814 -2.4612629 -2.1258833 -1.5087829 -0.96039379 -0.72333694 -0.7389015 -0.870054 -0.99954784 -0.86566055 -0.40185511 0.16058517 0.52363181 0.59095478][-1.981263 -2.3751278 -2.6423483 -2.6263316 -2.3754947 -2.0453382 -1.7798181 -1.6291285 -1.5868022 -1.6085688 -1.4973783 -1.1280444 -0.58065796 -0.061977386 0.2881422][-2.0291266 -2.3971343 -2.724525 -2.8898625 -2.8905296 -2.7706838 -2.5871561 -2.3961513 -2.2563832 -2.2087402 -2.1212451 -1.8443763 -1.3574921 -0.80733192 -0.31481743]]...]
INFO - root - 2017-12-16 09:42:18.478462: step 39510, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 13h:27m:56s remains)
INFO - root - 2017-12-16 09:42:20.152411: step 39520, loss = 0.50, batch loss = 0.24 (48.0 examples/sec; 0.167 sec/batch; 13h:34m:21s remains)
INFO - root - 2017-12-16 09:42:21.814753: step 39530, loss = 0.57, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 13h:41m:13s remains)
INFO - root - 2017-12-16 09:42:23.468284: step 39540, loss = 0.77, batch loss = 0.51 (48.4 examples/sec; 0.165 sec/batch; 13h:26m:58s remains)
INFO - root - 2017-12-16 09:42:25.125584: step 39550, loss = 0.54, batch loss = 0.28 (49.9 examples/sec; 0.160 sec/batch; 13h:02m:57s remains)
INFO - root - 2017-12-16 09:42:26.784086: step 39560, loss = 0.55, batch loss = 0.29 (49.7 examples/sec; 0.161 sec/batch; 13h:06m:00s remains)
INFO - root - 2017-12-16 09:42:28.458975: step 39570, loss = 0.66, batch loss = 0.40 (49.0 examples/sec; 0.163 sec/batch; 13h:16m:46s remains)
INFO - root - 2017-12-16 09:42:30.124042: step 39580, loss = 0.53, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:30m:46s remains)
INFO - root - 2017-12-16 09:42:31.795511: step 39590, loss = 0.69, batch loss = 0.43 (48.8 examples/sec; 0.164 sec/batch; 13h:20m:26s remains)
INFO - root - 2017-12-16 09:42:33.471682: step 39600, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 13h:30m:45s remains)
2017-12-16 09:42:33.955022: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9922621 -2.8962455 -3.0196586 -3.228344 -3.3395686 -3.3089101 -3.2624836 -3.2879584 -3.2201364 -3.1224146 -3.2095926 -3.4838581 -3.8133378 -3.9825921 -3.8062112][-3.1363945 -3.1874561 -3.5083861 -3.8425188 -3.944186 -3.8175259 -3.6765423 -3.6222177 -3.53929 -3.4685769 -3.606437 -3.9008727 -4.2252369 -4.3897858 -4.1761103][-2.894218 -3.0540695 -3.5251317 -4.0099268 -4.1714449 -3.9761307 -3.6845131 -3.5089049 -3.3755431 -3.3675675 -3.5383883 -3.7972174 -4.090035 -4.2601118 -4.0957994][-2.2946007 -2.5173326 -3.090982 -3.6845963 -3.910033 -3.6733112 -3.2163658 -2.9441314 -2.7725241 -2.784066 -2.9183204 -3.1136971 -3.3483486 -3.5689511 -3.5607076][-1.3810539 -1.6109022 -2.2429428 -2.917275 -3.1948717 -2.9364142 -2.4472146 -2.0922942 -1.9296587 -1.9937301 -2.0836694 -2.1733167 -2.2890368 -2.5440314 -2.7163212][-0.34500575 -0.51764905 -1.0807154 -1.6927567 -1.9197364 -1.6570133 -1.1632169 -0.84394789 -0.83688533 -1.0169368 -1.0889165 -1.0934093 -1.217423 -1.5461675 -1.8930132][0.89899874 1.0212667 0.59822416 0.11145091 -0.034338474 0.24939466 0.69981408 0.90457988 0.68966341 0.36553144 0.2471931 0.20817256 -0.13163733 -0.72524369 -1.2481861][1.7266433 2.23894 2.0944264 1.8383906 1.8937447 2.3289382 2.6843541 2.6300614 2.1151578 1.5818818 1.3334415 1.0672081 0.45284986 -0.35558033 -0.98300731][0.86910105 1.6136966 1.7661479 1.8465621 2.1563609 2.6933591 3.0129969 2.8130095 2.1363213 1.4680963 0.94786477 0.42724633 -0.31620693 -1.129127 -1.5840735][-0.65278506 0.087123632 0.3769176 0.59884 1.0293694 1.5342028 1.8279798 1.6745012 1.1204467 0.46062374 -0.21425414 -0.91626227 -1.7364528 -2.3925655 -2.606653][-1.9465923 -1.3737431 -1.1529584 -0.97421372 -0.55807233 -0.13579655 0.1070056 0.04226923 -0.2452035 -0.728637 -1.3905083 -2.1544807 -2.9539452 -3.451797 -3.4029632][-3.0363307 -2.7197454 -2.7084963 -2.7115355 -2.4582245 -2.1083493 -1.9149567 -1.8483262 -1.904726 -2.1742046 -2.71739 -3.3441522 -3.9142854 -4.2267461 -3.9947357][-3.6723485 -3.6902106 -3.9076214 -4.0984178 -4.0266089 -3.7992187 -3.6249933 -3.4659247 -3.3712459 -3.4443016 -3.7586205 -4.0807018 -4.371911 -4.4870186 -4.1383691][-3.3611236 -3.5459352 -3.8764493 -4.1222825 -4.124074 -4.0057707 -3.9040856 -3.7498968 -3.5854735 -3.5939364 -3.7339535 -3.8070526 -3.90134 -3.9545753 -3.705317][-2.4048839 -2.5907674 -2.8898439 -3.1108987 -3.1606481 -3.121978 -3.0636778 -2.95784 -2.8412175 -2.8444095 -2.8943477 -2.8261616 -2.7955785 -2.8679333 -2.8378432]]...]
INFO - root - 2017-12-16 09:42:35.635506: step 39610, loss = 0.53, batch loss = 0.27 (47.2 examples/sec; 0.170 sec/batch; 13h:47m:42s remains)
INFO - root - 2017-12-16 09:42:37.313600: step 39620, loss = 0.50, batch loss = 0.24 (45.2 examples/sec; 0.177 sec/batch; 14h:24m:30s remains)
INFO - root - 2017-12-16 09:42:39.006406: step 39630, loss = 0.63, batch loss = 0.38 (47.3 examples/sec; 0.169 sec/batch; 13h:46m:18s remains)
INFO - root - 2017-12-16 09:42:40.679997: step 39640, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 13h:15m:27s remains)
INFO - root - 2017-12-16 09:42:42.325286: step 39650, loss = 0.54, batch loss = 0.28 (49.6 examples/sec; 0.161 sec/batch; 13h:07m:12s remains)
INFO - root - 2017-12-16 09:42:44.004444: step 39660, loss = 0.63, batch loss = 0.37 (47.0 examples/sec; 0.170 sec/batch; 13h:50m:59s remains)
INFO - root - 2017-12-16 09:42:45.665202: step 39670, loss = 0.56, batch loss = 0.30 (49.9 examples/sec; 0.160 sec/batch; 13h:02m:26s remains)
INFO - root - 2017-12-16 09:42:47.328574: step 39680, loss = 0.54, batch loss = 0.28 (47.6 examples/sec; 0.168 sec/batch; 13h:40m:24s remains)
INFO - root - 2017-12-16 09:42:49.009394: step 39690, loss = 0.67, batch loss = 0.41 (47.7 examples/sec; 0.168 sec/batch; 13h:38m:21s remains)
INFO - root - 2017-12-16 09:42:50.651896: step 39700, loss = 0.57, batch loss = 0.31 (48.0 examples/sec; 0.167 sec/batch; 13h:33m:35s remains)
2017-12-16 09:42:51.113574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0007055 -0.91095674 -0.84002578 -0.79000747 -0.78760469 -0.8294884 -0.89051116 -0.897493 -0.80216622 -0.46173036 0.22280097 0.796633 0.68097234 0.14652276 -0.60100532][-2.0676816 -2.0178823 -1.9546429 -1.8983147 -1.8979902 -1.9769439 -2.0761743 -2.1031828 -2.025805 -1.701247 -1.0562718 -0.52375007 -0.55093622 -0.87412012 -1.3357375][-3.1835155 -3.2189126 -3.1707232 -3.0917583 -3.0604806 -3.1205578 -3.1722522 -3.1767201 -3.1248996 -2.9128773 -2.4598176 -2.1212611 -2.1415327 -2.2833533 -2.4374733][-4.0367584 -4.1386857 -4.0341287 -3.8499079 -3.6792779 -3.5558999 -3.4023032 -3.2815065 -3.2654841 -3.2829225 -3.1886878 -3.1792321 -3.3530717 -3.5029826 -3.4591198][-4.3205347 -4.3981037 -4.1429973 -3.7308354 -3.2406888 -2.6853292 -2.0707331 -1.6714988 -1.7480302 -2.1552129 -2.6016657 -3.11931 -3.7015023 -4.0408244 -3.9499583][-3.9338145 -3.9525881 -3.5282421 -2.816607 -1.8918846 -0.72942746 0.60686636 1.4811795 1.2000687 0.23121834 -0.80258393 -1.9281077 -3.0237751 -3.6832733 -3.73572][-2.9921477 -2.9953432 -2.5475845 -1.6777945 -0.43985808 1.2872438 3.4101386 4.7799268 4.2006917 2.6615906 1.2642491 -0.22368169 -1.7039415 -2.6858661 -2.9545059][-1.9452848 -1.9947484 -1.6837404 -0.91521037 0.28542733 2.1273961 4.6489124 6.3062921 5.5058951 3.7662654 2.3876171 0.94861746 -0.5978744 -1.7328057 -2.1610436][-1.2203407 -1.3788092 -1.3374885 -0.9935919 -0.3097 0.99079466 3.0056043 4.3467293 3.8505278 2.6487136 1.8482308 0.92018771 -0.29575658 -1.2885969 -1.7523973][-0.75463462 -0.87402296 -1.0882452 -1.2752029 -1.2947508 -0.88291204 0.2422657 1.1391335 0.99987459 0.46491313 0.28676271 -0.021908283 -0.64903092 -1.3089919 -1.705838][-0.019845486 0.0052518845 -0.39386415 -1.0618534 -1.7468562 -2.2011566 -2.0174987 -1.6379926 -1.6051135 -1.6707106 -1.4914929 -1.3570342 -1.4176407 -1.6691315 -1.8988998][0.805372 0.899889 0.37302542 -0.50835443 -1.5434778 -2.5280926 -2.9897835 -3.0535645 -3.0549216 -2.9507005 -2.6446905 -2.3163266 -2.0804558 -2.1010678 -2.2288303][1.239321 1.3169215 0.78704405 -0.016748428 -1.0733151 -2.1712697 -2.8735476 -3.1722589 -3.2634332 -3.1528718 -2.8824465 -2.5730884 -2.2569351 -2.1988108 -2.3578761][1.0000606 1.0075717 0.6163137 0.099601746 -0.71740031 -1.6157587 -2.1486089 -2.4108438 -2.4741127 -2.3554473 -2.1050897 -1.8392987 -1.6313477 -1.7288092 -2.0406673][0.033636808 0.04949379 -0.111871 -0.37834978 -0.91347456 -1.3966353 -1.503227 -1.5064745 -1.4071832 -1.211038 -0.89785147 -0.6589458 -0.65864396 -0.97818995 -1.494959]]...]
INFO - root - 2017-12-16 09:42:52.786697: step 39710, loss = 0.48, batch loss = 0.22 (46.0 examples/sec; 0.174 sec/batch; 14h:07m:46s remains)
INFO - root - 2017-12-16 09:42:54.450913: step 39720, loss = 0.51, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:21m:08s remains)
INFO - root - 2017-12-16 09:42:56.121733: step 39730, loss = 0.60, batch loss = 0.34 (47.2 examples/sec; 0.170 sec/batch; 13h:47m:23s remains)
INFO - root - 2017-12-16 09:42:57.836788: step 39740, loss = 0.57, batch loss = 0.31 (45.4 examples/sec; 0.176 sec/batch; 14h:20m:23s remains)
INFO - root - 2017-12-16 09:42:59.538603: step 39750, loss = 0.62, batch loss = 0.36 (46.8 examples/sec; 0.171 sec/batch; 13h:54m:04s remains)
INFO - root - 2017-12-16 09:43:01.234369: step 39760, loss = 0.49, batch loss = 0.23 (46.9 examples/sec; 0.171 sec/batch; 13h:52m:12s remains)
INFO - root - 2017-12-16 09:43:02.907373: step 39770, loss = 0.56, batch loss = 0.30 (47.1 examples/sec; 0.170 sec/batch; 13h:48m:38s remains)
INFO - root - 2017-12-16 09:43:04.578604: step 39780, loss = 0.62, batch loss = 0.36 (48.5 examples/sec; 0.165 sec/batch; 13h:24m:12s remains)
INFO - root - 2017-12-16 09:43:06.254735: step 39790, loss = 0.58, batch loss = 0.32 (46.8 examples/sec; 0.171 sec/batch; 13h:53m:09s remains)
INFO - root - 2017-12-16 09:43:07.937349: step 39800, loss = 0.60, batch loss = 0.34 (47.2 examples/sec; 0.169 sec/batch; 13h:46m:01s remains)
2017-12-16 09:43:08.414004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5187223 -3.5805104 -3.5740747 -3.4024065 -3.1330819 -2.9945238 -3.1693492 -3.471447 -3.6112442 -3.5945487 -3.4810581 -3.3437066 -3.2535608 -3.1844859 -3.124943][-3.7922206 -3.8303356 -3.8033602 -3.6348763 -3.3149233 -3.1443808 -3.3907044 -3.7790155 -3.8644786 -3.7156956 -3.5261173 -3.4003758 -3.3540924 -3.3024364 -3.2286377][-3.253711 -3.504921 -3.694478 -3.6773677 -3.4261565 -3.171144 -3.292434 -3.6457253 -3.5960951 -3.2840743 -3.0346594 -2.9500413 -2.9536245 -2.9264293 -2.9280527][-2.2516656 -2.7809148 -3.2700427 -3.4202642 -3.17349 -2.7344632 -2.6554477 -2.8568084 -2.7244251 -2.3228724 -2.0385177 -2.0332749 -2.1107059 -2.159688 -2.2798991][-1.1512529 -1.9288545 -2.5322957 -2.7120965 -2.4381225 -1.8270495 -1.4884372 -1.4640839 -1.2715604 -0.89116311 -0.76399994 -0.985294 -1.2183665 -1.3922503 -1.643337][-0.093316317 -0.99752986 -1.6344464 -1.8010863 -1.3987919 -0.60925066 0.029679298 0.36230087 0.61585617 0.82204986 0.63975787 0.030160427 -0.45984244 -0.7622813 -1.1461903][1.0282438 0.14143443 -0.48712683 -0.62834239 -0.20267582 0.79198146 1.6734722 2.3654311 2.6749661 2.4918215 1.9086201 0.88998652 0.097024918 -0.32232165 -0.74741554][1.5223396 0.76411724 0.33000827 0.27842808 0.5912776 1.65009 2.6480539 3.4433959 3.6264975 3.1096156 2.3061669 1.1101832 0.19276285 -0.28297591 -0.67484105][1.1152756 0.60637808 0.34942007 0.29715633 0.47077751 1.321507 2.2039306 2.6585157 2.6362517 2.1889989 1.4847012 0.43763566 -0.36811185 -0.73581481 -0.98437405][0.13071084 -0.16371608 -0.29709411 -0.44099331 -0.5770781 -0.10439038 0.45577502 0.67459035 0.60286736 0.38643456 -0.018363237 -0.78218162 -1.3915589 -1.5214747 -1.5329609][-1.2057263 -1.2699528 -1.351508 -1.7189615 -2.1502702 -1.9810874 -1.7011244 -1.6033523 -1.6008554 -1.5971794 -1.7968534 -2.3321447 -2.698252 -2.5443275 -2.2753859][-2.8525002 -2.6807747 -2.6071966 -3.0828834 -3.7256575 -3.8231187 -3.7501512 -3.6941452 -3.6760998 -3.5952868 -3.6740446 -3.9539692 -4.0457954 -3.686769 -3.1712418][-3.9932647 -3.6736829 -3.476944 -3.9146388 -4.625598 -4.8288651 -4.8940887 -4.8790331 -4.8777266 -4.8287592 -4.8218842 -4.9212093 -4.8612509 -4.3694868 -3.7109718][-4.0697813 -3.6909504 -3.4504447 -3.7826543 -4.4002934 -4.6523657 -4.7709017 -4.8131685 -4.8314953 -4.8293829 -4.8363976 -4.9236407 -4.81068 -4.3082981 -3.675561][-3.3510582 -2.9611552 -2.736526 -2.9991388 -3.5018754 -3.745791 -3.8568678 -3.9074447 -3.9392414 -3.9870822 -4.105341 -4.2244215 -4.1104813 -3.7233796 -3.2310214]]...]
INFO - root - 2017-12-16 09:43:10.066805: step 39810, loss = 0.58, batch loss = 0.32 (46.9 examples/sec; 0.170 sec/batch; 13h:51m:13s remains)
INFO - root - 2017-12-16 09:43:11.756591: step 39820, loss = 0.74, batch loss = 0.48 (46.1 examples/sec; 0.173 sec/batch; 14h:05m:38s remains)
INFO - root - 2017-12-16 09:43:13.435873: step 39830, loss = 0.55, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 13h:40m:04s remains)
INFO - root - 2017-12-16 09:43:15.100257: step 39840, loss = 0.54, batch loss = 0.28 (47.8 examples/sec; 0.167 sec/batch; 13h:36m:41s remains)
INFO - root - 2017-12-16 09:43:16.768025: step 39850, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:21m:27s remains)
INFO - root - 2017-12-16 09:43:18.430260: step 39860, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:31m:56s remains)
INFO - root - 2017-12-16 09:43:20.087863: step 39870, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:29m:13s remains)
INFO - root - 2017-12-16 09:43:21.753435: step 39880, loss = 0.51, batch loss = 0.25 (46.9 examples/sec; 0.171 sec/batch; 13h:51m:36s remains)
INFO - root - 2017-12-16 09:43:23.430501: step 39890, loss = 0.63, batch loss = 0.38 (47.0 examples/sec; 0.170 sec/batch; 13h:50m:54s remains)
INFO - root - 2017-12-16 09:43:25.125180: step 39900, loss = 0.53, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 13h:44m:48s remains)
2017-12-16 09:43:25.627370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3268986 -1.4986312 -1.7183771 -1.9576297 -2.1645238 -2.2361455 -2.1879272 -2.1174464 -2.149755 -2.30013 -2.4490583 -2.4970639 -2.4439855 -2.3396277 -2.1860225][-1.3246734 -1.5592971 -1.8657186 -2.1987383 -2.4795022 -2.526624 -2.4026604 -2.2430527 -2.2642369 -2.4878321 -2.739892 -2.8395069 -2.7995708 -2.6663411 -2.4066348][-1.2706457 -1.5291097 -1.8729298 -2.25793 -2.5218489 -2.4765174 -2.1962061 -1.9006044 -1.8879033 -2.2292094 -2.6592748 -2.9110851 -2.932791 -2.7325892 -2.3185925][-1.1851746 -1.4300096 -1.7763728 -2.148968 -2.2882161 -1.992305 -1.4092214 -0.85288775 -0.80009186 -1.3803196 -2.1842575 -2.755837 -2.9128103 -2.6691504 -2.0322022][-1.1203194 -1.3169523 -1.6297823 -1.9292789 -1.8227508 -1.1618364 -0.13984704 0.75733256 0.75330162 -0.28393292 -1.6409259 -2.5908706 -2.9410675 -2.6052442 -1.7607107][-1.1427981 -1.3045348 -1.571305 -1.7120585 -1.2952917 -0.21365595 1.3066235 2.5100555 2.1748462 0.41803074 -1.423813 -2.6818087 -3.0999217 -2.6398075 -1.6462278][-1.2652732 -1.4353669 -1.6467326 -1.6038687 -0.86312413 0.65847588 2.6844692 4.0497966 2.9356112 0.62719822 -1.5001631 -2.9142909 -3.2762988 -2.6534069 -1.5881929][-1.3888175 -1.5546851 -1.6591253 -1.438019 -0.47178674 1.3600316 3.6980567 4.625174 2.828783 0.42517042 -1.7084588 -3.046423 -3.2040296 -2.4019268 -1.3188453][-1.4808688 -1.5876608 -1.5707624 -1.2199439 -0.15111589 1.6804347 3.4972329 3.5394244 1.8616753 -0.24042559 -2.0572491 -3.0487258 -2.9047184 -2.0217443 -1.0829796][-1.5981445 -1.63158 -1.5251102 -1.1181029 -0.10362291 1.3043404 2.2320461 1.8944907 0.53235483 -1.0727929 -2.410917 -2.9632013 -2.5847847 -1.7851717 -1.0927696][-1.7302204 -1.719631 -1.5451401 -1.1718602 -0.39933336 0.45584702 0.88013124 0.46799302 -0.53487706 -1.7095251 -2.5919771 -2.7880197 -2.2924092 -1.6328771 -1.169046][-1.7911787 -1.7215102 -1.5548438 -1.3212415 -0.8463254 -0.35384226 -0.14550591 -0.49974895 -1.2418151 -2.0894277 -2.6281 -2.6190827 -2.1366956 -1.6436893 -1.3351015][-1.8046769 -1.7282102 -1.6708229 -1.6295345 -1.3672189 -1.0349414 -0.92217934 -1.2669126 -1.8492353 -2.43932 -2.7469902 -2.6564116 -2.2862582 -1.9453444 -1.7272032][-1.8064357 -1.8297682 -1.9556826 -2.0665259 -1.9038024 -1.6223905 -1.5572159 -1.8527566 -2.3121324 -2.7182045 -2.9151936 -2.8280683 -2.557312 -2.3196135 -2.1234043][-1.7817132 -1.9355884 -2.2000103 -2.3649404 -2.2563438 -2.0269475 -1.9599967 -2.1673474 -2.4869881 -2.7696347 -2.9021685 -2.8411129 -2.6671667 -2.4857147 -2.3055084]]...]
INFO - root - 2017-12-16 09:43:27.366364: step 39910, loss = 0.52, batch loss = 0.26 (45.4 examples/sec; 0.176 sec/batch; 14h:19m:05s remains)
INFO - root - 2017-12-16 09:43:29.071767: step 39920, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 13h:18m:03s remains)
INFO - root - 2017-12-16 09:43:30.750856: step 39930, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:30m:02s remains)
INFO - root - 2017-12-16 09:43:32.420789: step 39940, loss = 0.72, batch loss = 0.46 (48.8 examples/sec; 0.164 sec/batch; 13h:19m:03s remains)
INFO - root - 2017-12-16 09:43:34.100818: step 39950, loss = 0.61, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 13h:36m:33s remains)
INFO - root - 2017-12-16 09:43:35.772701: step 39960, loss = 0.58, batch loss = 0.32 (46.9 examples/sec; 0.170 sec/batch; 13h:50m:56s remains)
INFO - root - 2017-12-16 09:43:37.474772: step 39970, loss = 0.51, batch loss = 0.25 (47.2 examples/sec; 0.169 sec/batch; 13h:46m:07s remains)
INFO - root - 2017-12-16 09:43:39.175987: step 39980, loss = 0.79, batch loss = 0.53 (48.8 examples/sec; 0.164 sec/batch; 13h:18m:34s remains)
INFO - root - 2017-12-16 09:43:40.847839: step 39990, loss = 0.61, batch loss = 0.35 (47.3 examples/sec; 0.169 sec/batch; 13h:43m:46s remains)
INFO - root - 2017-12-16 09:43:42.585122: step 40000, loss = 0.55, batch loss = 0.29 (45.2 examples/sec; 0.177 sec/batch; 14h:21m:55s remains)
2017-12-16 09:43:43.063136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7927382 -2.0738797 -2.774157 -3.4122703 -3.7298813 -3.8807664 -3.8068981 -3.560147 -3.2524571 -2.9571507 -2.7556999 -2.7422864 -3.0450797 -3.4788721 -3.7599607][-2.0668917 -2.4819694 -3.1891794 -3.7124426 -3.8160691 -3.7187767 -3.4543366 -3.0865226 -2.6842456 -2.2535505 -2.0557837 -2.1987317 -2.7808232 -3.5057502 -4.0292754][-2.3672433 -2.84133 -3.4876087 -3.8540759 -3.7377317 -3.3656096 -2.8484848 -2.2131336 -1.5942161 -1.0824327 -0.98895931 -1.49368 -2.5518081 -3.6024818 -4.2954426][-2.6576018 -3.0807738 -3.6619821 -3.9111204 -3.5289149 -2.7733254 -1.8546281 -0.89515746 -0.095171928 0.18702078 -0.15873814 -1.1674625 -2.6446919 -3.8837876 -4.5494466][-2.6933951 -3.0833559 -3.6397328 -3.8492699 -3.3237281 -2.2368937 -0.9491334 0.3672123 1.2570591 1.2152495 0.274884 -1.2235243 -2.9236293 -4.2014761 -4.7003193][-2.5057478 -2.890501 -3.460428 -3.682198 -3.0943687 -1.779716 -0.1213336 1.5378723 2.4737184 2.0080402 0.60014153 -1.2693391 -3.0831304 -4.2646565 -4.5516939][-2.2950187 -2.6503465 -3.1924763 -3.3264115 -2.5615232 -1.0236984 0.89104247 2.6807344 3.3668907 2.372494 0.52010679 -1.532884 -3.2805154 -4.2401552 -4.2807355][-2.1974981 -2.4151268 -2.734014 -2.6099277 -1.6599824 -0.010635614 1.8688128 3.4508798 3.4749629 1.8749707 -0.20983028 -2.1418252 -3.5702481 -4.156805 -3.9380577][-2.1537206 -2.2011337 -2.2371244 -1.886983 -0.89470792 0.61698151 2.1139057 3.1000187 2.5119236 0.73302317 -1.1779753 -2.7230048 -3.6734395 -3.8809118 -3.5368776][-2.111588 -2.0824294 -2.0797 -1.7468319 -0.89168537 0.32363605 1.3974209 1.8457367 1.1229086 -0.35386586 -1.8588517 -2.94985 -3.4521871 -3.4346657 -3.1820402][-2.1065872 -2.126662 -2.1629143 -1.9097627 -1.1773268 -0.1809907 0.63063908 0.87484288 0.22257209 -0.973861 -2.1681993 -2.9551392 -3.2137771 -3.1087127 -2.9602687][-2.040235 -2.0809727 -2.1049562 -1.8592551 -1.2597795 -0.44955754 0.20828795 0.35839605 -0.28481603 -1.3953073 -2.3595831 -2.9290721 -3.0674756 -2.9162886 -2.7383761][-1.8216376 -1.7872405 -1.8376129 -1.6534445 -1.2296904 -0.65305173 -0.21113372 -0.1720469 -0.73496962 -1.6094882 -2.2698374 -2.5958776 -2.629127 -2.4761181 -2.2487159][-1.689111 -1.5668007 -1.6955742 -1.6600521 -1.40269 -0.95715964 -0.584965 -0.46741807 -0.71864271 -1.1755223 -1.5561236 -1.7823505 -1.883029 -1.8195362 -1.5971431][-1.764717 -1.6368597 -1.7431442 -1.7397709 -1.574119 -1.1615829 -0.70369124 -0.43012702 -0.37039948 -0.47700131 -0.70256925 -1.0348117 -1.2833446 -1.3054491 -1.0568956]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:43:45.453771: step 40010, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:30m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:43:47.158993: step 40020, loss = 0.49, batch loss = 0.24 (46.6 examples/sec; 0.172 sec/batch; 13h:57m:32s remains)
INFO - root - 2017-12-16 09:43:48.860127: step 40030, loss = 0.54, batch loss = 0.28 (46.1 examples/sec; 0.173 sec/batch; 14h:05m:06s remains)
INFO - root - 2017-12-16 09:43:50.571769: step 40040, loss = 0.62, batch loss = 0.36 (45.8 examples/sec; 0.175 sec/batch; 14h:11m:33s remains)
INFO - root - 2017-12-16 09:43:52.307985: step 40050, loss = 0.46, batch loss = 0.20 (45.1 examples/sec; 0.177 sec/batch; 14h:25m:06s remains)
INFO - root - 2017-12-16 09:43:54.010459: step 40060, loss = 0.60, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 13h:21m:17s remains)
INFO - root - 2017-12-16 09:43:55.679077: step 40070, loss = 0.52, batch loss = 0.26 (46.3 examples/sec; 0.173 sec/batch; 14h:02m:07s remains)
INFO - root - 2017-12-16 09:43:57.386753: step 40080, loss = 0.58, batch loss = 0.32 (44.6 examples/sec; 0.180 sec/batch; 14h:34m:57s remains)
INFO - root - 2017-12-16 09:43:59.130648: step 40090, loss = 0.49, batch loss = 0.23 (46.6 examples/sec; 0.171 sec/batch; 13h:55m:45s remains)
INFO - root - 2017-12-16 09:44:00.869221: step 40100, loss = 0.53, batch loss = 0.27 (45.8 examples/sec; 0.175 sec/batch; 14h:10m:41s remains)
2017-12-16 09:44:01.424665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.78746724 -0.74199724 -0.7061919 -0.67829859 -0.663767 -0.66836464 -0.67600882 -0.54962361 -0.11850262 0.35955381 0.27742577 -0.28932118 -1.0463573 -1.9604023 -2.7889369][-0.65052223 -0.61535513 -0.58174336 -0.55260873 -0.54503334 -0.554108 -0.53590739 -0.34081054 0.24155021 0.85021782 0.73493552 0.083975554 -0.71606433 -1.7436979 -2.7080572][-1.3605428 -1.417063 -1.4254606 -1.4235814 -1.4242977 -1.4361486 -1.3843533 -1.1400278 -0.5519135 0.01591444 -0.080376625 -0.60542178 -1.2396047 -2.1399286 -2.9892328][-2.3693378 -2.4946849 -2.5215342 -2.5011866 -2.4821353 -2.4541957 -2.326479 -2.058639 -1.6084893 -1.2809479 -1.4099108 -1.8002565 -2.2948089 -2.9839127 -3.5110626][-2.9357426 -3.0469842 -3.0035505 -2.8220325 -2.6359415 -2.4280887 -2.1325722 -1.7802231 -1.5901095 -1.7046006 -2.0718205 -2.5317154 -3.0691824 -3.6442511 -3.8855915][-2.8425429 -2.8757992 -2.6706471 -2.2122121 -1.7341321 -1.2122487 -0.53983605 0.074866295 -0.033012867 -0.77445519 -1.5784309 -2.3207264 -3.0887704 -3.7107472 -3.8256021][-2.2563944 -2.193779 -1.8260351 -1.1289824 -0.33096886 0.55904841 1.7366464 2.7304451 2.3662684 1.0167758 -0.23094296 -1.3153739 -2.3556454 -3.1122758 -3.277174][-1.575572 -1.5370827 -1.1877162 -0.46782291 0.44772792 1.5096176 3.0659611 4.4400492 3.9797933 2.3839982 0.9813571 -0.24311352 -1.4418497 -2.3075461 -2.5880942][-1.2923778 -1.501775 -1.422766 -0.98582017 -0.30913448 0.53029346 1.8596494 3.0855906 2.9268329 1.8352296 0.78427052 -0.23428559 -1.2581478 -1.9877107 -2.2691734][-1.6390822 -2.1533012 -2.4271507 -2.3357658 -1.9874809 -1.5000937 -0.61230946 0.27564597 0.4131813 -0.039734364 -0.59622574 -1.2154013 -1.8547807 -2.281652 -2.4330432][-2.1016521 -2.8380392 -3.4215808 -3.6417494 -3.5598552 -3.2977316 -2.7001882 -2.0461631 -1.6726058 -1.6288404 -1.7993515 -2.1057267 -2.4220097 -2.6018748 -2.60586][-2.3791707 -3.2017694 -3.8910317 -4.221118 -4.1909609 -3.9305995 -3.4466672 -2.8917792 -2.4425874 -2.1673577 -2.1212337 -2.2520912 -2.4380622 -2.5502846 -2.4992945][-2.5273509 -3.220078 -3.7574184 -3.9726911 -3.811429 -3.4288316 -2.9230156 -2.4151797 -2.0198269 -1.749007 -1.7046297 -1.8866088 -2.1823707 -2.3947318 -2.3639202][-2.6966436 -3.0928822 -3.3223584 -3.2969587 -2.9945526 -2.5348198 -2.0345852 -1.6198833 -1.3667244 -1.2147971 -1.2541759 -1.5669601 -2.0605018 -2.4542646 -2.5521538][-2.4348674 -2.5173047 -2.5517814 -2.4497347 -2.1511774 -1.7360775 -1.346417 -1.0925254 -0.94373178 -0.8524127 -0.96543527 -1.4068823 -2.0716572 -2.6731944 -2.9578786]]...]
INFO - root - 2017-12-16 09:44:03.121048: step 40110, loss = 0.59, batch loss = 0.33 (49.0 examples/sec; 0.163 sec/batch; 13h:16m:03s remains)
INFO - root - 2017-12-16 09:44:04.793927: step 40120, loss = 0.53, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 13h:27m:05s remains)
INFO - root - 2017-12-16 09:44:06.482580: step 40130, loss = 0.62, batch loss = 0.37 (47.7 examples/sec; 0.168 sec/batch; 13h:37m:02s remains)
INFO - root - 2017-12-16 09:44:08.150570: step 40140, loss = 0.50, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 13h:12m:59s remains)
INFO - root - 2017-12-16 09:44:09.840934: step 40150, loss = 0.55, batch loss = 0.30 (47.8 examples/sec; 0.167 sec/batch; 13h:34m:46s remains)
INFO - root - 2017-12-16 09:44:11.495567: step 40160, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 13h:35m:10s remains)
INFO - root - 2017-12-16 09:44:13.199995: step 40170, loss = 0.51, batch loss = 0.25 (47.9 examples/sec; 0.167 sec/batch; 13h:33m:42s remains)
INFO - root - 2017-12-16 09:44:14.861567: step 40180, loss = 0.78, batch loss = 0.52 (46.6 examples/sec; 0.172 sec/batch; 13h:56m:37s remains)
INFO - root - 2017-12-16 09:44:16.540454: step 40190, loss = 0.65, batch loss = 0.39 (47.3 examples/sec; 0.169 sec/batch; 13h:44m:42s remains)
INFO - root - 2017-12-16 09:44:18.222450: step 40200, loss = 0.49, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:23m:02s remains)
2017-12-16 09:44:18.731717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.902464 -2.1810725 -2.4780958 -2.6538053 -2.6308675 -2.4430919 -2.2148752 -2.0673788 -2.0095983 -1.9207112 -1.8664749 -1.8717341 -1.9015381 -1.9415805 -1.9971554][-2.2507095 -2.7524133 -3.2753305 -3.5685472 -3.5777025 -3.3412149 -3.0195041 -2.8259585 -2.798003 -2.7510459 -2.6657226 -2.6495309 -2.664098 -2.6770616 -2.7241812][-2.5869689 -3.2061834 -3.8064873 -4.0802183 -3.9872828 -3.668241 -3.3872845 -3.3539281 -3.5777249 -3.7086744 -3.6526265 -3.6175411 -3.5809989 -3.5289912 -3.5190623][-2.716588 -3.2255249 -3.5518088 -3.4762402 -3.0950308 -2.6848993 -2.5788271 -2.9209201 -3.6222589 -4.1065068 -4.2954235 -4.3703051 -4.336781 -4.1545329 -3.9815259][-2.561846 -2.7891412 -2.5914345 -1.8833158 -1.079316 -0.58131719 -0.60649526 -1.2566195 -2.4394176 -3.428896 -3.9988449 -4.3284335 -4.4179387 -4.1084714 -3.7903845][-2.3247206 -2.229614 -1.4495218 -0.037543774 1.256501 2.0056961 2.104378 1.288548 -0.20514464 -1.5839304 -2.5363905 -3.1983333 -3.371978 -2.9969354 -2.5870333][-2.2412844 -1.8957297 -0.72689474 1.2439723 3.0609095 4.2785063 4.7997847 4.157917 2.5562394 0.89516282 -0.37032294 -1.3303505 -1.5540962 -1.166256 -0.76760507][-2.37947 -2.0501342 -0.93102121 1.007041 3.121423 4.9089546 6.1311617 6.1083622 4.6910019 2.9310529 1.4737599 0.26855612 -0.043205738 0.36177087 0.65826321][-2.6426697 -2.5800266 -1.8778679 -0.43931735 1.4225531 3.4034755 5.1508322 5.857008 4.8816261 3.2489393 1.7870233 0.58472824 0.30875659 0.68903255 0.87547326][-2.8471844 -3.120008 -2.929934 -2.1013165 -0.69109643 1.1322024 2.7909462 3.5429533 2.8692882 1.5875666 0.48348427 -0.3477428 -0.48535585 -0.13854599 -0.020011187][-2.983994 -3.5549026 -3.8661973 -3.5037951 -2.4945285 -1.0581559 0.17903805 0.55581164 -0.1066339 -1.1185653 -1.7507713 -2.0661898 -1.9970503 -1.6752625 -1.5380611][-3.0382051 -3.7742233 -4.3713942 -4.3697586 -3.6875374 -2.6625051 -1.9294858 -1.9822075 -2.7360151 -3.5228684 -3.832828 -3.7828419 -3.5266268 -3.19002 -2.9973516][-2.9356143 -3.640934 -4.2802444 -4.4457407 -4.0274811 -3.3872623 -3.0600414 -3.4330225 -4.1946745 -4.7871809 -4.8955069 -4.6629725 -4.3160853 -3.9971664 -3.7915883][-2.6540704 -3.17732 -3.6909542 -3.88903 -3.6816239 -3.369904 -3.3595662 -3.8000612 -4.3613176 -4.7325692 -4.7480545 -4.5184994 -4.2097268 -3.9007862 -3.7308369][-2.3111138 -2.6441314 -2.9878249 -3.1650324 -3.1000922 -3.013176 -3.1175916 -3.4427373 -3.7622147 -3.9103537 -3.8486481 -3.64461 -3.4054911 -3.1727369 -3.0455382]]...]
INFO - root - 2017-12-16 09:44:20.412500: step 40210, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:25m:32s remains)
INFO - root - 2017-12-16 09:44:22.090998: step 40220, loss = 0.60, batch loss = 0.34 (47.8 examples/sec; 0.167 sec/batch; 13h:35m:39s remains)
INFO - root - 2017-12-16 09:44:23.808569: step 40230, loss = 0.55, batch loss = 0.29 (46.7 examples/sec; 0.171 sec/batch; 13h:54m:23s remains)
INFO - root - 2017-12-16 09:44:25.489389: step 40240, loss = 0.58, batch loss = 0.32 (48.4 examples/sec; 0.165 sec/batch; 13h:24m:20s remains)
INFO - root - 2017-12-16 09:44:27.183230: step 40250, loss = 0.46, batch loss = 0.21 (45.6 examples/sec; 0.176 sec/batch; 14h:14m:54s remains)
INFO - root - 2017-12-16 09:44:28.872384: step 40260, loss = 0.48, batch loss = 0.22 (46.4 examples/sec; 0.172 sec/batch; 13h:59m:41s remains)
INFO - root - 2017-12-16 09:44:30.549405: step 40270, loss = 0.65, batch loss = 0.40 (45.5 examples/sec; 0.176 sec/batch; 14h:16m:35s remains)
INFO - root - 2017-12-16 09:44:32.237292: step 40280, loss = 0.45, batch loss = 0.20 (48.1 examples/sec; 0.166 sec/batch; 13h:30m:35s remains)
INFO - root - 2017-12-16 09:44:33.931577: step 40290, loss = 0.62, batch loss = 0.37 (47.6 examples/sec; 0.168 sec/batch; 13h:38m:19s remains)
INFO - root - 2017-12-16 09:44:35.610023: step 40300, loss = 0.65, batch loss = 0.39 (47.2 examples/sec; 0.169 sec/batch; 13h:45m:10s remains)
2017-12-16 09:44:36.144043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0907362 -1.0715203 -1.0215577 -0.962286 -0.92347276 -0.89967191 -0.91355968 -0.98558676 -1.0809306 -1.1239616 -1.0897425 -0.98619592 -0.84547603 -0.68142843 -0.51940835][-1.4521449 -1.5826507 -1.6660309 -1.7128643 -1.7519209 -1.7698245 -1.8381333 -2.0197396 -2.2307606 -2.3312492 -2.2717714 -2.0750442 -1.780333 -1.4184115 -1.0659919][-1.8836943 -2.1703305 -2.3758736 -2.4709811 -2.4993393 -2.4831641 -2.567879 -2.877723 -3.264277 -3.4718266 -3.4727316 -3.3036983 -2.9812376 -2.5368769 -2.0429413][-2.1614881 -2.544991 -2.7679145 -2.7611625 -2.6138871 -2.4027641 -2.3723569 -2.772052 -3.3329577 -3.7442236 -3.9644334 -4.0186477 -3.8885446 -3.5602469 -3.0792739][-2.1583989 -2.4888322 -2.5488803 -2.2425091 -1.7548673 -1.1648182 -0.79689658 -1.0980582 -1.8385761 -2.6304398 -3.2776513 -3.733484 -4.007916 -4.0317068 -3.732923][-1.9276445 -2.0600553 -1.8162497 -1.0712941 -0.039321661 1.1304107 2.0495656 2.1151426 1.2559903 -0.046347857 -1.3007027 -2.3213282 -3.1788354 -3.7521875 -3.789598][-1.7057767 -1.5983653 -1.0463803 0.10776591 1.7164042 3.5994666 5.2422781 5.8809881 5.0494766 3.2995174 1.4676557 -0.15596104 -1.6691992 -2.8853369 -3.3340869][-1.72855 -1.5057893 -0.83548737 0.47213268 2.42399 4.7820845 7.0157738 8.1599159 7.4795179 5.7239752 3.7148545 1.7700384 -0.22298265 -1.9364785 -2.7360671][-2.0452571 -1.9602515 -1.5173497 -0.48933542 1.2635405 3.4409921 5.533349 6.65368 6.4521637 5.418458 3.9070303 2.1084926 0.053264141 -1.7401841 -2.6894157][-2.4108994 -2.6812387 -2.7339334 -2.2963431 -1.1797681 0.34441113 1.8485048 2.695801 2.8705823 2.6014068 1.8979566 0.61663771 -1.0710052 -2.5907102 -3.4698763][-2.5866258 -3.2295871 -3.7886906 -3.9769926 -3.5414462 -2.665668 -1.7043318 -1.113917 -0.85393608 -0.69902408 -0.81422496 -1.5185502 -2.5811555 -3.6740847 -4.3800936][-2.4773359 -3.2795405 -4.1305075 -4.7442822 -4.8276567 -4.414094 -3.811738 -3.4204574 -3.2455511 -3.0020518 -2.8202722 -3.0468159 -3.5002356 -4.1150618 -4.5922494][-2.1504192 -2.8413215 -3.674305 -4.4196157 -4.7639914 -4.6590023 -4.29706 -4.0882297 -4.0429826 -3.8131943 -3.4754691 -3.3235533 -3.3688979 -3.6136932 -3.8833139][-1.7277292 -2.1830239 -2.8057044 -3.4491849 -3.8456159 -3.9137082 -3.7622995 -3.7040098 -3.7389836 -3.4826789 -3.0216703 -2.6458385 -2.4432564 -2.4619949 -2.6075447][-1.3954768 -1.6530514 -2.0346212 -2.4567461 -2.7591431 -2.8945835 -2.8964734 -2.9223452 -2.951997 -2.7201111 -2.2819972 -1.8703201 -1.5942554 -1.5122516 -1.5882422]]...]
INFO - root - 2017-12-16 09:44:37.821429: step 40310, loss = 0.64, batch loss = 0.38 (44.8 examples/sec; 0.179 sec/batch; 14h:30m:02s remains)
INFO - root - 2017-12-16 09:44:39.535775: step 40320, loss = 0.55, batch loss = 0.29 (45.9 examples/sec; 0.174 sec/batch; 14h:09m:38s remains)
INFO - root - 2017-12-16 09:44:41.204587: step 40330, loss = 0.51, batch loss = 0.25 (46.6 examples/sec; 0.172 sec/batch; 13h:55m:17s remains)
INFO - root - 2017-12-16 09:44:42.902297: step 40340, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 13h:38m:39s remains)
INFO - root - 2017-12-16 09:44:44.598713: step 40350, loss = 0.53, batch loss = 0.28 (49.0 examples/sec; 0.163 sec/batch; 13h:15m:33s remains)
INFO - root - 2017-12-16 09:44:46.252682: step 40360, loss = 0.79, batch loss = 0.53 (47.5 examples/sec; 0.168 sec/batch; 13h:40m:14s remains)
INFO - root - 2017-12-16 09:44:47.946729: step 40370, loss = 0.54, batch loss = 0.28 (47.5 examples/sec; 0.169 sec/batch; 13h:40m:30s remains)
INFO - root - 2017-12-16 09:44:49.660608: step 40380, loss = 0.50, batch loss = 0.24 (43.8 examples/sec; 0.183 sec/batch; 14h:49m:52s remains)
INFO - root - 2017-12-16 09:44:51.384575: step 40390, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.164 sec/batch; 13h:20m:46s remains)
INFO - root - 2017-12-16 09:44:53.067180: step 40400, loss = 0.52, batch loss = 0.26 (46.7 examples/sec; 0.171 sec/batch; 13h:53m:26s remains)
2017-12-16 09:44:53.559587: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062960863 -0.50532043 -1.1646589 -1.7720835 -2.2251577 -2.5969486 -2.8728907 -3.0503616 -3.219136 -3.3754954 -3.4968631 -3.5877738 -3.6791961 -3.7685773 -3.7809312][0.26773977 -0.20124936 -0.82492423 -1.3853714 -1.7634923 -2.0521598 -2.2741911 -2.4583032 -2.6930919 -2.9883075 -3.3029847 -3.5855379 -3.8345432 -3.9980159 -3.9590447][0.44824576 0.13995647 -0.35998297 -0.82414126 -1.1546515 -1.40675 -1.5755093 -1.7167296 -1.9358484 -2.2767 -2.6811018 -3.0600767 -3.36291 -3.538635 -3.5184][0.7374742 0.62878489 0.3240242 0.0024175644 -0.30469918 -0.60123158 -0.7973 -0.92357183 -1.1130046 -1.4277232 -1.7873814 -2.1118338 -2.3563528 -2.500227 -2.5913584][0.95921469 1.142082 1.1015174 0.935128 0.62077165 0.22607803 -0.061323881 -0.21920896 -0.34106731 -0.52261484 -0.7477355 -0.936123 -1.1080763 -1.3028293 -1.5730404][0.85768104 1.3755047 1.6465919 1.6378415 1.3421361 0.91379571 0.59880662 0.4843545 0.49470782 0.47415686 0.36442447 0.22045469 0.0083303452 -0.33460164 -0.83174908][0.28954768 1.1017854 1.6676614 1.8753335 1.694916 1.3223336 1.0959737 1.1183634 1.2884841 1.3764238 1.2981927 1.1223636 0.8147409 0.32961893 -0.31411076][-0.58423972 0.37069678 1.076055 1.4175751 1.3784959 1.1581557 1.0884821 1.2802353 1.61992 1.8371465 1.7967293 1.6097867 1.3052127 0.86691 0.27799249][-1.5121971 -0.65961111 -0.0044198036 0.36130285 0.42370152 0.36371422 0.45913863 0.75441456 1.146137 1.4244046 1.4751146 1.3895936 1.255105 1.0532339 0.74747372][-2.3674304 -1.8048234 -1.3410797 -1.0537889 -0.94889545 -0.88257384 -0.68417859 -0.35291934 0.0064313412 0.28029156 0.40037918 0.44795847 0.56076813 0.71523 0.83781385][-2.9364328 -2.7149541 -2.5140443 -2.3752973 -2.3086724 -2.2129984 -2.0047488 -1.715402 -1.4415681 -1.210874 -1.0494537 -0.88986027 -0.56646621 -0.094275951 0.37859654][-3.0210791 -3.0831256 -3.1204722 -3.1376767 -3.1518123 -3.1069663 -2.9840999 -2.8278584 -2.6823831 -2.5342433 -2.3853974 -2.1853838 -1.8047736 -1.2253363 -0.60398817][-2.7111921 -2.9392748 -3.1108689 -3.2226615 -3.3009512 -3.3206191 -3.3024278 -3.2939868 -3.3052239 -3.2937212 -3.2485824 -3.1228774 -2.8377223 -2.3593495 -1.81929][-2.2028346 -2.5260255 -2.7453821 -2.8674054 -2.9148421 -2.9147353 -2.924665 -2.9983141 -3.1368446 -3.2773519 -3.396728 -3.4556913 -3.3843124 -3.1485868 -2.8190231][-1.6611785 -2.0539031 -2.3274202 -2.4286919 -2.3862064 -2.2809374 -2.2142754 -2.2586343 -2.4264951 -2.6557298 -2.8902712 -3.10563 -3.2618151 -3.3028884 -3.2412066]]...]
INFO - root - 2017-12-16 09:44:55.253502: step 40410, loss = 0.48, batch loss = 0.22 (47.8 examples/sec; 0.167 sec/batch; 13h:34m:49s remains)
INFO - root - 2017-12-16 09:44:56.952642: step 40420, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 13h:29m:01s remains)
INFO - root - 2017-12-16 09:44:58.648055: step 40430, loss = 0.58, batch loss = 0.32 (48.0 examples/sec; 0.167 sec/batch; 13h:31m:17s remains)
INFO - root - 2017-12-16 09:45:00.306037: step 40440, loss = 0.53, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 13h:28m:32s remains)
INFO - root - 2017-12-16 09:45:01.999804: step 40450, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:16m:58s remains)
INFO - root - 2017-12-16 09:45:03.678859: step 40460, loss = 0.65, batch loss = 0.39 (46.9 examples/sec; 0.171 sec/batch; 13h:50m:06s remains)
INFO - root - 2017-12-16 09:45:05.352142: step 40470, loss = 0.57, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 13h:19m:36s remains)
INFO - root - 2017-12-16 09:45:07.020513: step 40480, loss = 0.56, batch loss = 0.30 (45.7 examples/sec; 0.175 sec/batch; 14h:12m:18s remains)
INFO - root - 2017-12-16 09:45:08.774157: step 40490, loss = 0.48, batch loss = 0.23 (45.9 examples/sec; 0.174 sec/batch; 14h:08m:01s remains)
INFO - root - 2017-12-16 09:45:10.498279: step 40500, loss = 0.50, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 13h:14m:43s remains)
2017-12-16 09:45:11.016512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.850889 -1.9150037 -2.021282 -2.1273532 -2.2238491 -2.3038626 -2.3423567 -2.3619897 -2.3791997 -2.3767326 -2.3569465 -2.3225937 -2.2710638 -2.2110932 -2.1798165][-2.5924807 -2.8069668 -3.0351813 -3.2417743 -3.4285011 -3.546972 -3.5634108 -3.5418789 -3.5154719 -3.457047 -3.3642411 -3.2615516 -3.1742051 -3.088362 -3.0293145][-3.3328242 -3.6765642 -3.9449587 -4.1454277 -4.3085251 -4.3421903 -4.2345352 -4.0720978 -3.9540632 -3.8225069 -3.6307282 -3.4380493 -3.3099089 -3.2219024 -3.1350102][-3.9668012 -4.2797265 -4.3798213 -4.36784 -4.3225231 -4.18163 -3.8874426 -3.5726476 -3.392138 -3.219161 -2.9484787 -2.6845076 -2.5410385 -2.484971 -2.4127676][-4.1426377 -4.2045217 -3.9499714 -3.5687971 -3.2522295 -2.9236717 -2.486867 -2.0646102 -1.8689098 -1.7633719 -1.5273112 -1.2710043 -1.1737953 -1.2067075 -1.2272284][-3.548481 -3.3532257 -2.831521 -2.2108705 -1.7560545 -1.3780288 -0.90092111 -0.38420248 -0.16205096 -0.22509432 -0.17410398 -0.088602543 -0.10354304 -0.18297577 -0.24762893][-2.4811666 -2.2127123 -1.7319659 -1.208735 -0.84466982 -0.56352532 -0.12905431 0.46488619 0.73118734 0.45345736 0.2167654 0.052554607 -0.15646815 -0.26053524 -0.28411341][-1.539762 -1.4401703 -1.2187968 -0.9940666 -0.84369516 -0.72184658 -0.38638735 0.15635252 0.36199474 -0.061469555 -0.47677541 -0.80093634 -1.0959579 -1.2024096 -1.1155941][-1.1725247 -1.3684826 -1.4997644 -1.6476467 -1.7424467 -1.7455171 -1.4979485 -1.0666384 -0.95116746 -1.4035428 -1.8587573 -2.1974046 -2.4196534 -2.4299867 -2.2542675][-1.4413345 -1.8795559 -2.2769766 -2.7213163 -3.0277371 -3.1094744 -2.9515853 -2.6526704 -2.5785429 -2.9169879 -3.2401805 -3.4745774 -3.5870249 -3.4957304 -3.3036387][-2.0145097 -2.5239787 -2.971549 -3.5450253 -3.9552417 -4.0802951 -3.9941268 -3.7787619 -3.6324358 -3.7374656 -3.8614359 -3.9538143 -3.9998417 -3.9126911 -3.7627521][-2.3715456 -2.7345865 -3.0742919 -3.6015134 -4.0273466 -4.1944127 -4.1256127 -3.9355931 -3.7087932 -3.6257172 -3.6042123 -3.5996227 -3.6088967 -3.5657811 -3.5183685][-2.1630979 -2.2730932 -2.4409561 -2.8191495 -3.188185 -3.3631606 -3.2893364 -3.0872667 -2.8065655 -2.6351044 -2.596498 -2.6341856 -2.719213 -2.7305984 -2.7169611][-1.4975353 -1.379594 -1.3786181 -1.6254587 -1.9710462 -2.1597476 -2.0427239 -1.7582953 -1.4450948 -1.3354332 -1.4252319 -1.5936899 -1.7323965 -1.7599452 -1.7261479][-0.69937539 -0.4731369 -0.44028497 -0.66916132 -0.99106896 -1.1420546 -0.96778977 -0.6293391 -0.36192203 -0.35489702 -0.57735467 -0.83332109 -0.97453809 -0.97235835 -0.874089]]...]
INFO - root - 2017-12-16 09:45:12.716425: step 40510, loss = 0.55, batch loss = 0.29 (47.4 examples/sec; 0.169 sec/batch; 13h:41m:55s remains)
INFO - root - 2017-12-16 09:45:14.377632: step 40520, loss = 0.51, batch loss = 0.26 (48.9 examples/sec; 0.164 sec/batch; 13h:16m:38s remains)
INFO - root - 2017-12-16 09:45:16.036957: step 40530, loss = 0.54, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 13h:26m:14s remains)
INFO - root - 2017-12-16 09:45:17.705185: step 40540, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 13h:43m:07s remains)
INFO - root - 2017-12-16 09:45:19.378574: step 40550, loss = 0.55, batch loss = 0.29 (47.6 examples/sec; 0.168 sec/batch; 13h:36m:56s remains)
INFO - root - 2017-12-16 09:45:21.028197: step 40560, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:21m:25s remains)
INFO - root - 2017-12-16 09:45:22.682259: step 40570, loss = 0.62, batch loss = 0.36 (46.9 examples/sec; 0.171 sec/batch; 13h:49m:44s remains)
INFO - root - 2017-12-16 09:45:24.393855: step 40580, loss = 0.54, batch loss = 0.28 (47.1 examples/sec; 0.170 sec/batch; 13h:46m:40s remains)
INFO - root - 2017-12-16 09:45:26.103518: step 40590, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 13h:42m:55s remains)
INFO - root - 2017-12-16 09:45:27.775201: step 40600, loss = 0.47, batch loss = 0.22 (47.5 examples/sec; 0.168 sec/batch; 13h:38m:34s remains)
2017-12-16 09:45:28.255942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1985642 -1.3051898 -1.4884996 -1.7594144 -2.1384892 -2.588809 -3.040658 -3.4501152 -3.6941838 -3.7694597 -3.7029777 -3.4794989 -3.1900077 -2.9120972 -2.5771284][-1.4232655 -1.718599 -2.1336262 -2.6512246 -3.2554517 -3.7922006 -4.1782675 -4.5094805 -4.6827664 -4.6535444 -4.5234375 -4.2838626 -3.9699612 -3.630393 -3.1499157][-1.764544 -2.2909145 -2.9209476 -3.5767727 -4.1856918 -4.5167837 -4.5255966 -4.5045428 -4.4837961 -4.4309134 -4.3791084 -4.2973642 -4.1317196 -3.9170909 -3.4150584][-2.1745372 -2.8915558 -3.5802178 -4.1088791 -4.3779368 -4.1710291 -3.5531073 -3.0533576 -2.8615205 -2.9332781 -3.1656077 -3.425189 -3.6346655 -3.7392516 -3.3778768][-2.6011095 -3.387629 -3.8919096 -3.9780812 -3.5562742 -2.5750792 -1.2420858 -0.20701861 0.068474531 -0.37198591 -1.1214355 -1.8921154 -2.6068764 -3.2154717 -3.1451659][-2.9408813 -3.6507754 -3.79172 -3.2408261 -2.0084298 -0.24678326 1.766773 3.2867048 3.516607 2.4824035 1.0235434 -0.32289076 -1.5926743 -2.6876733 -2.9270656][-3.1288273 -3.6874356 -3.4252253 -2.2306437 -0.31711078 2.041929 4.6031809 6.5722027 6.515337 4.5831623 2.3387578 0.47134662 -1.2189733 -2.571604 -2.9648829][-3.1869955 -3.6322291 -3.1142478 -1.4769462 0.88517427 3.4718654 6.2253704 8.5029259 7.2815609 4.5690784 1.9964817 -0.065738916 -1.791685 -3.0218482 -3.2605321][-3.1205928 -3.5101774 -2.9559531 -1.3172617 0.87553167 3.0224621 4.7869663 5.6568871 4.6861954 2.4947355 0.26918006 -1.4995437 -2.8456783 -3.652385 -3.5357909][-2.9726002 -3.41172 -3.1018362 -1.8418592 -0.25417209 1.0791528 1.8962748 2.1200969 1.3517103 -0.21521115 -1.8114617 -3.0452962 -3.8521998 -4.099257 -3.6247938][-2.7539978 -3.2424459 -3.2210143 -2.4595563 -1.4966269 -0.88257575 -0.69780433 -0.83316255 -1.4191118 -2.3913581 -3.3806372 -4.0694861 -4.3852863 -4.2084088 -3.4810588][-2.4000635 -2.8651354 -3.0414803 -2.7267997 -2.2562358 -2.0532618 -2.1656368 -2.4680076 -2.8778191 -3.3919249 -3.8869398 -4.1747956 -4.1774487 -3.8013465 -3.0618191][-1.9123842 -2.2675617 -2.4652109 -2.3692136 -2.1892853 -2.1875081 -2.4065969 -2.7078514 -2.9453061 -3.145 -3.3428006 -3.4288912 -3.3547082 -3.0377898 -2.4752476][-1.4940668 -1.679672 -1.794323 -1.7484994 -1.6667081 -1.6929387 -1.8332489 -2.0318274 -2.1791177 -2.2749341 -2.3605561 -2.400491 -2.3817146 -2.2269132 -1.9039714][-1.2266538 -1.2722335 -1.2797611 -1.2246301 -1.1336445 -1.1190016 -1.1917241 -1.3189793 -1.4147103 -1.4946196 -1.5788957 -1.639425 -1.6742826 -1.6466141 -1.5099554]]...]
INFO - root - 2017-12-16 09:45:29.910052: step 40610, loss = 0.46, batch loss = 0.20 (47.2 examples/sec; 0.169 sec/batch; 13h:43m:44s remains)
INFO - root - 2017-12-16 09:45:31.554448: step 40620, loss = 0.65, batch loss = 0.39 (47.8 examples/sec; 0.167 sec/batch; 13h:34m:25s remains)
INFO - root - 2017-12-16 09:45:33.232898: step 40630, loss = 0.63, batch loss = 0.37 (48.3 examples/sec; 0.166 sec/batch; 13h:25m:21s remains)
INFO - root - 2017-12-16 09:45:34.893836: step 40640, loss = 0.54, batch loss = 0.28 (48.3 examples/sec; 0.166 sec/batch; 13h:25m:48s remains)
INFO - root - 2017-12-16 09:45:36.568701: step 40650, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 13h:26m:39s remains)
INFO - root - 2017-12-16 09:45:38.251573: step 40660, loss = 0.66, batch loss = 0.40 (47.4 examples/sec; 0.169 sec/batch; 13h:41m:40s remains)
INFO - root - 2017-12-16 09:45:39.915653: step 40670, loss = 0.55, batch loss = 0.29 (49.4 examples/sec; 0.162 sec/batch; 13h:08m:22s remains)
INFO - root - 2017-12-16 09:45:41.548294: step 40680, loss = 0.52, batch loss = 0.26 (49.7 examples/sec; 0.161 sec/batch; 13h:02m:57s remains)
INFO - root - 2017-12-16 09:45:43.198015: step 40690, loss = 0.51, batch loss = 0.25 (48.3 examples/sec; 0.166 sec/batch; 13h:25m:54s remains)
INFO - root - 2017-12-16 09:45:44.842190: step 40700, loss = 0.57, batch loss = 0.31 (48.6 examples/sec; 0.165 sec/batch; 13h:21m:21s remains)
2017-12-16 09:45:45.285762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8808997 -1.6809703 -1.6279161 -1.576005 -1.5318459 -1.5806005 -1.4020276 -1.2325971 -1.5531211 -1.8543555 -1.8320029 -1.9518476 -2.1230826 -1.9702109 -1.5583255][-2.2228742 -2.2288041 -2.2377567 -2.1494756 -2.1093593 -2.1471 -1.8588879 -1.5038409 -1.6746434 -1.9199812 -1.9225413 -2.052577 -2.2225449 -2.0482638 -1.5986547][-2.2401054 -2.5409329 -2.68732 -2.652071 -2.6024346 -2.5002842 -1.9943482 -1.3419667 -1.2440345 -1.4087942 -1.5503912 -1.8861498 -2.2071218 -2.1471856 -1.7325546][-1.9494357 -2.562696 -2.8590119 -2.8274326 -2.6884422 -2.4310308 -1.739181 -0.83032775 -0.46190834 -0.5278132 -0.82132781 -1.4483253 -2.0567105 -2.2210398 -1.9476248][-1.3073856 -1.9976156 -2.2910919 -2.2047048 -1.9257138 -1.4861808 -0.74325967 0.16426849 0.59582233 0.5527401 0.094439507 -0.842904 -1.7673581 -2.215744 -2.1845315][-0.56492126 -1.1085186 -1.1711289 -0.82959855 -0.24956703 0.44009757 1.2044976 1.8297622 1.8672516 1.4548025 0.68846583 -0.53652275 -1.7233284 -2.3661375 -2.4641795][0.22974038 -0.079389334 0.16768742 0.748785 1.6632259 2.8473098 3.7413547 3.8071153 2.9995344 1.7422645 0.43060064 -0.92748547 -2.0761859 -2.6471572 -2.6888647][0.68884063 0.73252082 1.2022603 1.8838212 2.9180362 4.5738535 5.762536 4.8895512 3.0798194 1.2570238 -0.24654508 -1.5360118 -2.5159302 -2.8779824 -2.749218][0.57341838 0.9810257 1.5414755 2.0109723 2.6560447 3.6064312 4.1604795 3.4279187 1.7389562 0.13600564 -1.0456491 -2.0641627 -2.7684798 -2.851356 -2.5425057][-0.08359766 0.48818636 1.0425067 1.1427357 1.1937063 1.4354451 1.5131612 0.94229436 -0.25936151 -1.2902589 -1.949182 -2.511508 -2.8604183 -2.6915417 -2.2186785][-1.2883257 -0.84636462 -0.47851312 -0.56915 -0.86065209 -1.039655 -1.175439 -1.570776 -2.2657218 -2.7344642 -2.8271258 -2.8913045 -2.8798661 -2.5241389 -2.0054717][-2.4445062 -2.2555549 -2.1187158 -2.3671327 -2.8169587 -3.2013524 -3.3743207 -3.4747205 -3.688122 -3.6699927 -3.3759146 -3.1086884 -2.8835616 -2.4694357 -2.0002968][-2.9594021 -2.9696398 -2.9938798 -3.3958874 -3.9606707 -4.3398023 -4.3664885 -4.1339564 -3.9270391 -3.623935 -3.2117805 -2.8666041 -2.6240938 -2.3510163 -2.0777428][-2.752785 -2.8725708 -3.0626962 -3.5416036 -4.0609775 -4.2842569 -4.0717206 -3.541585 -3.0620365 -2.7291472 -2.4857204 -2.3102589 -2.2245402 -2.1968153 -2.1496885][-2.4421179 -2.6092844 -2.8492441 -3.2399306 -3.5274305 -3.5330579 -3.0699372 -2.2834306 -1.7032461 -1.5470752 -1.632862 -1.7966149 -1.9922715 -2.1539817 -2.2106457]]...]
INFO - root - 2017-12-16 09:45:46.974308: step 40710, loss = 0.49, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 13h:28m:21s remains)
INFO - root - 2017-12-16 09:45:48.645033: step 40720, loss = 0.52, batch loss = 0.26 (48.1 examples/sec; 0.166 sec/batch; 13h:28m:13s remains)
INFO - root - 2017-12-16 09:45:50.330492: step 40730, loss = 0.72, batch loss = 0.47 (47.8 examples/sec; 0.168 sec/batch; 13h:34m:34s remains)
INFO - root - 2017-12-16 09:45:51.980999: step 40740, loss = 0.54, batch loss = 0.28 (49.7 examples/sec; 0.161 sec/batch; 13h:02m:13s remains)
INFO - root - 2017-12-16 09:45:53.694610: step 40750, loss = 0.49, batch loss = 0.23 (45.6 examples/sec; 0.176 sec/batch; 14h:13m:49s remains)
INFO - root - 2017-12-16 09:45:55.374563: step 40760, loss = 0.59, batch loss = 0.33 (47.0 examples/sec; 0.170 sec/batch; 13h:47m:53s remains)
INFO - root - 2017-12-16 09:45:57.063838: step 40770, loss = 0.54, batch loss = 0.28 (49.0 examples/sec; 0.163 sec/batch; 13h:13m:43s remains)
INFO - root - 2017-12-16 09:45:58.744141: step 40780, loss = 0.54, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 13h:33m:26s remains)
INFO - root - 2017-12-16 09:46:00.431647: step 40790, loss = 0.58, batch loss = 0.32 (44.3 examples/sec; 0.181 sec/batch; 14h:38m:18s remains)
INFO - root - 2017-12-16 09:46:02.151118: step 40800, loss = 0.54, batch loss = 0.28 (45.1 examples/sec; 0.177 sec/batch; 14h:21m:42s remains)
2017-12-16 09:46:02.650327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2275636 -2.0931067 -2.0583243 -2.0833161 -2.2093813 -2.3659611 -2.4020584 -2.3256938 -2.1365173 -1.9277874 -1.9578452 -2.2514596 -2.6251588 -2.8967862 -3.161217][-2.3396723 -2.1670642 -2.1185734 -2.1456907 -2.4111133 -2.7374079 -2.8271964 -2.7330694 -2.5073218 -2.2593458 -2.2573409 -2.5260322 -2.8678312 -3.0339451 -3.246304][-2.6129472 -2.3873563 -2.251456 -2.2093575 -2.5518355 -2.9502316 -3.0338912 -2.8725817 -2.6605225 -2.5207896 -2.5949132 -2.8832951 -3.1737151 -3.2775543 -3.4076178][-2.7142673 -2.3782773 -2.018611 -1.7742375 -1.9611094 -2.2496386 -2.2738981 -2.0784113 -2.0133677 -2.108777 -2.3782988 -2.820781 -3.1685319 -3.2711158 -3.3803022][-2.3951585 -1.8469003 -1.235782 -0.64559686 -0.43813026 -0.46980846 -0.33993244 -0.16563582 -0.37750983 -0.82755482 -1.399816 -2.1695607 -2.748353 -2.94046 -3.0678983][-1.6405874 -0.87893176 0.034389019 0.98092484 1.5398865 1.8472326 2.2181542 2.3581798 1.8048637 0.94321728 0.040256262 -1.0497924 -1.9611391 -2.3873515 -2.7017367][-0.77554762 0.089854 1.1934409 2.3896267 3.2056386 3.7865975 4.5381651 4.753109 3.8034084 2.5585072 1.291352 -0.0068614483 -1.1540778 -1.88445 -2.3934743][-0.20320225 0.73303938 1.9588563 3.3085635 4.1497536 4.8155146 5.8105564 5.8889332 4.6017857 3.1878784 1.8752615 0.64582586 -0.52618241 -1.3646563 -2.0318151][-0.013386488 0.93607283 2.14806 3.2625244 3.638891 3.7720559 4.1889725 4.1395206 3.2777002 2.27333 1.3207533 0.40807414 -0.47355866 -1.2016852 -1.9546652][-0.44222474 0.44749951 1.47946 2.2000868 2.0339754 1.5970056 1.5109799 1.4636216 1.0947845 0.60304427 0.044867039 -0.59636796 -1.2297589 -1.7420523 -2.4391186][-0.97786224 -0.17883086 0.62687922 1.0020013 0.51778483 -0.22546506 -0.53057241 -0.58505523 -0.676824 -0.793182 -1.0615412 -1.4873317 -1.9103997 -2.3203411 -2.9312954][-1.3291568 -0.6107223 -0.00690794 0.091272116 -0.47443104 -1.208069 -1.5176221 -1.5509827 -1.4922622 -1.4038594 -1.5477822 -1.9061141 -2.2430875 -2.5709023 -3.1055622][-1.7606442 -1.1598778 -0.73267174 -0.64144742 -1.0254388 -1.5507233 -1.7523115 -1.7524643 -1.6116693 -1.452893 -1.5716268 -1.9155364 -2.2100039 -2.4901628 -2.9685163][-2.4453075 -2.0035772 -1.6408734 -1.5099913 -1.6743968 -1.9537184 -2.0069695 -1.9369025 -1.7898297 -1.6839678 -1.8130746 -2.1292036 -2.3905752 -2.6181583 -3.0129764][-3.2599022 -3.0526955 -2.8082697 -2.6180806 -2.5903084 -2.6725211 -2.6477804 -2.5549245 -2.4427624 -2.3993869 -2.5271075 -2.7482054 -2.9202871 -3.0299735 -3.2585921]]...]
INFO - root - 2017-12-16 09:46:04.363165: step 40810, loss = 0.50, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 13h:40m:52s remains)
INFO - root - 2017-12-16 09:46:06.045077: step 40820, loss = 0.49, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 13h:29m:25s remains)
INFO - root - 2017-12-16 09:46:07.758016: step 40830, loss = 0.64, batch loss = 0.38 (46.3 examples/sec; 0.173 sec/batch; 14h:00m:06s remains)
INFO - root - 2017-12-16 09:46:09.448098: step 40840, loss = 0.51, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 13h:23m:36s remains)
INFO - root - 2017-12-16 09:46:11.121094: step 40850, loss = 0.68, batch loss = 0.43 (48.1 examples/sec; 0.166 sec/batch; 13h:28m:57s remains)
INFO - root - 2017-12-16 09:46:12.771329: step 40860, loss = 0.58, batch loss = 0.32 (48.8 examples/sec; 0.164 sec/batch; 13h:16m:13s remains)
INFO - root - 2017-12-16 09:46:14.440239: step 40870, loss = 0.53, batch loss = 0.27 (46.8 examples/sec; 0.171 sec/batch; 13h:51m:26s remains)
INFO - root - 2017-12-16 09:46:16.117825: step 40880, loss = 0.51, batch loss = 0.25 (47.7 examples/sec; 0.168 sec/batch; 13h:35m:10s remains)
INFO - root - 2017-12-16 09:46:17.815545: step 40890, loss = 0.56, batch loss = 0.30 (46.3 examples/sec; 0.173 sec/batch; 13h:59m:39s remains)
INFO - root - 2017-12-16 09:46:19.521133: step 40900, loss = 0.50, batch loss = 0.24 (45.3 examples/sec; 0.176 sec/batch; 14h:17m:31s remains)
2017-12-16 09:46:20.001725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0343888 -1.8206892 -1.5603265 -1.4190779 -1.6986498 -2.1131921 -2.1896002 -2.091804 -2.1496797 -2.410635 -2.3472943 -1.9053372 -1.384104 -1.0163398 -0.89437366][-2.02688 -1.8321202 -1.5366719 -1.3739526 -1.7054783 -2.3673835 -2.745575 -2.9070234 -3.1140692 -3.332376 -3.1900659 -2.6831977 -2.253468 -1.9143355 -1.6855544][-2.2277217 -2.2605686 -2.0810063 -1.8816156 -2.1338806 -2.7780173 -3.2741017 -3.5941572 -3.8467362 -3.9774089 -3.7745838 -3.2955709 -2.8967133 -2.5898759 -2.3189869][-2.4165196 -2.6427627 -2.6070995 -2.513979 -2.7473826 -3.2699614 -3.727715 -3.983613 -4.1145887 -4.0716281 -3.8244655 -3.3952389 -3.0394921 -2.806268 -2.5430632][-2.236798 -2.5726388 -2.6972013 -2.8126922 -3.077611 -3.4277754 -3.6850452 -3.7646146 -3.6909533 -3.5423951 -3.3242712 -3.0006025 -2.7326078 -2.5851583 -2.3489761][-1.7218956 -2.0752656 -2.3313267 -2.5730755 -2.8017387 -2.9986126 -3.0035467 -2.913554 -2.77561 -2.6632872 -2.5986865 -2.4467344 -2.3304563 -2.2628884 -2.1022565][-0.96354163 -1.3225256 -1.6042955 -1.8431568 -2.0195456 -2.0503809 -1.987 -1.8991511 -1.8561556 -1.8767064 -1.9486288 -1.9748528 -2.0017476 -1.9798892 -1.8522462][-0.11882162 -0.41195738 -0.6519078 -0.8437103 -0.941295 -0.94388139 -0.98537123 -1.0866225 -1.1525519 -1.2583789 -1.463093 -1.6697907 -1.7679596 -1.7126348 -1.5601593][0.4767251 0.34677434 0.13648057 -0.070616245 -0.12683797 -0.16081166 -0.38259315 -0.67998135 -0.88052833 -1.1215268 -1.426151 -1.6865699 -1.7594481 -1.5957652 -1.3708674][0.50134921 0.50405979 0.22706962 -0.020440817 -0.0025165081 -0.017194033 -0.28465486 -0.69398475 -1.02535 -1.38799 -1.732583 -1.9308642 -1.8517679 -1.5788692 -1.3451895][0.11805701 0.099594116 -0.26778698 -0.52682352 -0.4317131 -0.36015248 -0.58228254 -1.0050213 -1.404429 -1.8161216 -2.0961685 -2.1442857 -1.9136026 -1.5944237 -1.4278961][-0.48019743 -0.59451091 -0.99274433 -1.1634594 -0.96147609 -0.75767922 -0.86724317 -1.2653579 -1.7041688 -2.0671139 -2.2264657 -2.1145604 -1.8370354 -1.6281738 -1.6076396][-1.0250171 -1.1728925 -1.4536986 -1.4505759 -1.1868261 -0.89396477 -0.91678119 -1.2741731 -1.6868827 -1.9975594 -2.0375845 -1.8909969 -1.725808 -1.699537 -1.8521432][-1.3890977 -1.4603443 -1.5321774 -1.3463537 -1.0234083 -0.72346795 -0.73874843 -1.0565885 -1.4237194 -1.6931225 -1.7039597 -1.621577 -1.6409941 -1.8272748 -2.1308703][-1.4367361 -1.400124 -1.2741764 -0.921592 -0.55905044 -0.37585592 -0.47538936 -0.83680892 -1.25738 -1.5249653 -1.4998811 -1.438954 -1.5606496 -1.8793876 -2.2538595]]...]
INFO - root - 2017-12-16 09:46:21.743649: step 40910, loss = 0.46, batch loss = 0.20 (48.6 examples/sec; 0.165 sec/batch; 13h:19m:58s remains)
INFO - root - 2017-12-16 09:46:23.417815: step 40920, loss = 0.57, batch loss = 0.31 (48.8 examples/sec; 0.164 sec/batch; 13h:17m:15s remains)
INFO - root - 2017-12-16 09:46:25.078152: step 40930, loss = 0.58, batch loss = 0.32 (48.2 examples/sec; 0.166 sec/batch; 13h:26m:30s remains)
INFO - root - 2017-12-16 09:46:26.713221: step 40940, loss = 0.54, batch loss = 0.28 (48.7 examples/sec; 0.164 sec/batch; 13h:18m:01s remains)
INFO - root - 2017-12-16 09:46:28.393553: step 40950, loss = 0.52, batch loss = 0.26 (46.5 examples/sec; 0.172 sec/batch; 13h:56m:26s remains)
INFO - root - 2017-12-16 09:46:30.055509: step 40960, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:21m:48s remains)
INFO - root - 2017-12-16 09:46:31.746897: step 40970, loss = 0.58, batch loss = 0.32 (48.0 examples/sec; 0.167 sec/batch; 13h:29m:27s remains)
INFO - root - 2017-12-16 09:46:33.431268: step 40980, loss = 0.55, batch loss = 0.29 (48.4 examples/sec; 0.165 sec/batch; 13h:22m:54s remains)
INFO - root - 2017-12-16 09:46:35.119937: step 40990, loss = 0.53, batch loss = 0.27 (46.3 examples/sec; 0.173 sec/batch; 13h:58m:55s remains)
INFO - root - 2017-12-16 09:46:36.818172: step 41000, loss = 0.60, batch loss = 0.34 (47.5 examples/sec; 0.168 sec/batch; 13h:38m:35s remains)
2017-12-16 09:46:37.315319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1294692 -1.4927003 -1.7783519 -1.903597 -1.9098854 -1.8229413 -1.7340826 -1.6260668 -1.6035479 -1.7664132 -1.8771706 -1.7478518 -1.4694366 -1.335641 -1.4812272][-0.94298518 -1.4673061 -1.8925326 -2.0672245 -1.9715791 -1.7039101 -1.468544 -1.2642915 -1.209199 -1.3967839 -1.5006592 -1.3475722 -1.1006415 -1.102991 -1.4295676][-1.0302757 -1.5548397 -1.9485205 -2.0611298 -1.842574 -1.4934373 -1.2187936 -1.0180821 -0.96200848 -1.1230884 -1.1948007 -1.0160744 -0.83151579 -0.93795824 -1.3177587][-1.1558447 -1.5794468 -1.8720634 -1.8793653 -1.5791484 -1.2685561 -1.1376904 -1.1036131 -1.1550565 -1.2983634 -1.3521107 -1.1818612 -1.0059131 -1.0532004 -1.2806773][-1.135641 -1.5020657 -1.7796072 -1.7474437 -1.4089653 -1.1493095 -1.220021 -1.4076147 -1.6376888 -1.8516768 -1.959058 -1.8327796 -1.5974731 -1.4415886 -1.3874135][-0.95689762 -1.3816968 -1.7811551 -1.8304822 -1.5426973 -1.352899 -1.4128751 -1.6385453 -1.9782634 -2.3335745 -2.6218054 -2.6280689 -2.3801694 -2.0247252 -1.6898038][-0.71063364 -1.2770083 -1.8407023 -1.9346725 -1.6083009 -1.265965 -1.1062936 -1.23507 -1.6771898 -2.2698567 -2.8410053 -3.0886838 -2.9661348 -2.5504396 -2.0716746][-0.58146226 -1.2699795 -1.938769 -2.0088503 -1.5334086 -0.82547355 -0.24402213 -0.16274381 -0.74131572 -1.5313488 -2.348784 -2.8887947 -3.0272472 -2.7233462 -2.2910924][-0.54513097 -1.2993801 -1.962137 -1.934355 -1.2713162 -0.17374778 0.90275717 1.2198744 0.51490617 -0.34666324 -1.2999452 -2.0808697 -2.5326889 -2.5173247 -2.3086257][-0.41838193 -1.2077714 -1.8444338 -1.8626275 -1.2046276 0.051674128 1.4862151 2.047106 1.4043145 0.67213559 -0.16744208 -0.95773745 -1.635859 -2.0112672 -2.1926372][-0.2045145 -0.95836496 -1.645126 -1.9369268 -1.670148 -0.76776648 0.51240277 1.2438042 1.0720625 0.81997061 0.43699741 -0.12728763 -0.84722137 -1.4953536 -2.004113][-0.14651871 -0.77994645 -1.4401321 -2.00723 -2.1761005 -1.744478 -0.86996329 -0.19670558 0.016960382 0.24591255 0.3470006 0.061980247 -0.53447986 -1.2639837 -1.9462837][0.027267933 -0.45150232 -1.0713507 -1.8659005 -2.3634109 -2.2566333 -1.6791449 -1.1071922 -0.76416266 -0.39991832 -0.098443031 -0.13811159 -0.60588419 -1.3274436 -2.01252][0.23319983 -0.08323741 -0.60836124 -1.4440832 -2.0106275 -1.9385865 -1.4237869 -0.91668177 -0.76522112 -0.5677948 -0.32622123 -0.33413363 -0.7383039 -1.3840897 -1.9616643][0.30589938 0.051013231 -0.36462879 -1.037991 -1.4483014 -1.2144439 -0.60778224 -0.1381762 -0.19155312 -0.23898554 -0.14106154 -0.20384622 -0.61802411 -1.1505892 -1.6167469]]...]
INFO - root - 2017-12-16 09:46:39.050128: step 41010, loss = 0.53, batch loss = 0.27 (46.1 examples/sec; 0.173 sec/batch; 14h:02m:15s remains)
INFO - root - 2017-12-16 09:46:40.722082: step 41020, loss = 0.49, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 13h:21m:28s remains)
INFO - root - 2017-12-16 09:46:42.371840: step 41030, loss = 0.46, batch loss = 0.20 (49.9 examples/sec; 0.160 sec/batch; 12h:58m:34s remains)
INFO - root - 2017-12-16 09:46:44.061331: step 41040, loss = 0.60, batch loss = 0.34 (47.6 examples/sec; 0.168 sec/batch; 13h:36m:42s remains)
INFO - root - 2017-12-16 09:46:45.763581: step 41050, loss = 0.64, batch loss = 0.38 (46.4 examples/sec; 0.173 sec/batch; 13h:57m:58s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:46:47.433641: step 41060, loss = 0.52, batch loss = 0.26 (47.3 examples/sec; 0.169 sec/batch; 13h:41m:48s remains)
INFO - root - 2017-12-16 09:46:49.109925: step 41070, loss = 0.66, batch loss = 0.40 (48.9 examples/sec; 0.164 sec/batch; 13h:14m:20s remains)
INFO - root - 2017-12-16 09:46:50.796232: step 41080, loss = 0.61, batch loss = 0.36 (48.2 examples/sec; 0.166 sec/batch; 13h:26m:56s remains)
INFO - root - 2017-12-16 09:46:52.448079: step 41090, loss = 0.53, batch loss = 0.27 (48.1 examples/sec; 0.166 sec/batch; 13h:28m:14s remains)
INFO - root - 2017-12-16 09:46:54.146679: step 41100, loss = 0.48, batch loss = 0.22 (47.7 examples/sec; 0.168 sec/batch; 13h:34m:40s remains)
2017-12-16 09:46:54.608664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8357782 -3.7663383 -3.7647085 -3.9388433 -4.0866222 -4.047976 -3.9524612 -3.8775158 -3.7531304 -3.4146986 -2.7895784 -1.9649825 -1.4749793 -1.9146268 -2.9380682][-4.0058894 -4.0879817 -4.2295237 -4.4958086 -4.7451353 -4.78403 -4.7405152 -4.6859303 -4.559145 -4.2563629 -3.5466952 -2.6285253 -2.1439245 -2.65536 -3.6029015][-3.7022834 -3.93036 -4.190856 -4.4075832 -4.523231 -4.4703503 -4.3829556 -4.3062639 -4.2245283 -4.0449829 -3.4055405 -2.5151179 -2.149961 -2.7806466 -3.6905222][-3.1861334 -3.5670316 -3.9428976 -4.0285935 -3.7252231 -3.2952495 -2.9470158 -2.8454201 -3.0115716 -3.1586311 -2.7445209 -1.9677396 -1.7382486 -2.3874166 -3.2118418][-2.8261387 -3.3370457 -3.6943192 -3.4524946 -2.542624 -1.429497 -0.56478703 -0.41617417 -1.0083693 -1.6675682 -1.6862085 -1.1835726 -1.1019872 -1.6767848 -2.3299251][-2.7448027 -3.2775106 -3.4783514 -2.8503089 -1.3603517 0.62208891 2.2892029 2.6062858 1.5040197 0.16298819 -0.45581746 -0.4413271 -0.56199992 -1.0589678 -1.4559605][-2.8154497 -3.2439237 -3.3266325 -2.4476538 -0.43518865 2.403044 5.0553837 5.581049 3.8576014 1.7862685 0.51938939 0.013044119 -0.37812734 -0.749962 -0.87324917][-3.0774574 -3.3609214 -3.3951921 -2.4355192 -0.19565105 3.1483715 6.5212479 7.2791748 5.1328 2.5820463 0.773911 -0.22195029 -0.85944867 -1.1125413 -0.96631384][-3.2941079 -3.4338803 -3.4884551 -2.7032814 -0.74489808 2.3611295 5.4575624 6.2393265 4.3371735 1.9081552 0.0068542957 -1.1854819 -1.8699967 -1.9776924 -1.6312568][-3.589767 -3.7095976 -3.8384883 -3.3451803 -1.8900363 0.54079819 3.0315382 3.7224357 2.3041232 0.36290479 -1.3024879 -2.4475551 -2.9955413 -2.9408629 -2.4704547][-3.7793045 -3.9294329 -4.1602097 -3.9971423 -3.0981758 -1.4049789 0.42639256 0.9478178 -0.043188095 -1.4509943 -2.7380667 -3.6274285 -3.9191656 -3.6892455 -3.1329887][-3.8348079 -3.980104 -4.2870374 -4.4184818 -3.996129 -2.9394772 -1.7365047 -1.3860908 -2.0080848 -2.8922319 -3.7135267 -4.212419 -4.1750355 -3.8359389 -3.2654495][-3.6914587 -3.8155446 -4.1390538 -4.4274993 -4.3507032 -3.8107188 -3.1245866 -2.9077117 -3.2081137 -3.6392674 -4.0230756 -4.14411 -3.8817921 -3.4737291 -2.97977][-3.3558545 -3.5184603 -3.8357363 -4.137619 -4.2211537 -3.9962494 -3.63382 -3.4658146 -3.5506926 -3.7058358 -3.7976933 -3.6529403 -3.2777367 -2.8900788 -2.5424306][-2.962749 -3.2391324 -3.5151424 -3.6979802 -3.7405858 -3.6082456 -3.3993559 -3.2512453 -3.2293072 -3.2450542 -3.187675 -2.9588418 -2.6222441 -2.3725381 -2.2379181]]...]
INFO - root - 2017-12-16 09:46:56.289519: step 41110, loss = 0.53, batch loss = 0.28 (47.5 examples/sec; 0.168 sec/batch; 13h:38m:05s remains)
INFO - root - 2017-12-16 09:46:57.951733: step 41120, loss = 0.55, batch loss = 0.29 (48.8 examples/sec; 0.164 sec/batch; 13h:16m:19s remains)
INFO - root - 2017-12-16 09:46:59.615990: step 41130, loss = 0.61, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 13h:25m:13s remains)
INFO - root - 2017-12-16 09:47:01.322873: step 41140, loss = 0.56, batch loss = 0.30 (48.1 examples/sec; 0.166 sec/batch; 13h:28m:21s remains)
INFO - root - 2017-12-16 09:47:02.993084: step 41150, loss = 0.60, batch loss = 0.34 (49.1 examples/sec; 0.163 sec/batch; 13h:10m:30s remains)
INFO - root - 2017-12-16 09:47:04.641018: step 41160, loss = 0.50, batch loss = 0.24 (45.6 examples/sec; 0.175 sec/batch; 14h:11m:00s remains)
INFO - root - 2017-12-16 09:47:06.299803: step 41170, loss = 0.63, batch loss = 0.37 (49.7 examples/sec; 0.161 sec/batch; 13h:01m:47s remains)
INFO - root - 2017-12-16 09:47:07.981779: step 41180, loss = 0.58, batch loss = 0.32 (48.9 examples/sec; 0.164 sec/batch; 13h:14m:36s remains)
INFO - root - 2017-12-16 09:47:09.669444: step 41190, loss = 0.49, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:25m:22s remains)
INFO - root - 2017-12-16 09:47:11.371479: step 41200, loss = 0.80, batch loss = 0.54 (48.7 examples/sec; 0.164 sec/batch; 13h:17m:28s remains)
2017-12-16 09:47:11.853582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.3502872 -0.44858825 -0.87452877 -1.0369904 -0.853943 -0.60041738 -0.30331635 -0.029423714 -0.024941444 -0.39026964 -1.318904 -2.3721967 -2.9461634 -2.7584693 -1.9089341][-1.2183369 -1.2702506 -1.6250501 -1.730428 -1.4348181 -1.0476805 -0.69787169 -0.36183262 -0.25773287 -0.523525 -1.3419513 -2.2128196 -2.6432879 -2.3760018 -1.5297921][-1.937229 -1.979825 -2.3133535 -2.422354 -2.1160524 -1.6684918 -1.265021 -0.94695044 -0.70692277 -0.76560569 -1.3302687 -1.943011 -2.2069705 -1.834154 -0.97710955][-1.975805 -2.0297494 -2.3399534 -2.4410446 -2.096791 -1.6753418 -1.3219858 -1.0173454 -0.75316417 -0.68173039 -1.0820619 -1.5050151 -1.6130471 -1.1272919 -0.28171825][-1.4568726 -1.3824245 -1.6137105 -1.7297513 -1.4642979 -1.1027199 -0.80518663 -0.51579785 -0.29098868 -0.25868845 -0.60169232 -0.9374274 -0.96258271 -0.47732532 0.23400378][-0.84032416 -0.51242435 -0.54151738 -0.58824337 -0.3212347 0.023974419 0.26931238 0.50672412 0.70297122 0.60885692 0.176929 -0.25193429 -0.40430748 -0.18498945 0.23913288][-0.33738112 0.24126625 0.45076156 0.55505466 0.90360832 1.2745669 1.5331616 1.7763593 1.9051473 1.6402042 1.0160034 0.43931842 0.012362242 -0.18597937 -0.12301803][-0.038892031 0.70445871 1.108933 1.2706172 1.6179163 1.9744842 2.28212 2.5877907 2.6895978 2.3445184 1.6914451 0.96961212 0.26544881 -0.35577941 -0.67235255][-0.1246748 0.62359667 1.0305343 1.1298366 1.2925019 1.5332015 1.8232019 2.0931876 2.1332943 1.8160245 1.2933047 0.59845686 -0.23252606 -0.98869228 -1.4177303][-0.59398425 -0.10145521 0.13168049 0.16656947 0.24373102 0.39396882 0.6343236 0.84995127 0.87306 0.66261721 0.30506825 -0.28701615 -1.0783759 -1.7453833 -2.0316362][-1.4166224 -1.1721379 -1.0445497 -1.03169 -1.0040032 -0.95286512 -0.81991923 -0.68435168 -0.670046 -0.7922821 -0.97780466 -1.3456435 -1.8551431 -2.2327371 -2.2855008][-2.14666 -2.1296175 -2.099699 -2.0925584 -2.1180847 -2.1309316 -2.0884407 -2.0315897 -2.031368 -2.047426 -2.0557973 -2.132556 -2.2619152 -2.3148453 -2.1558826][-2.6369381 -2.6613185 -2.6447082 -2.6402905 -2.6758029 -2.7173908 -2.720772 -2.6947711 -2.6757777 -2.6282892 -2.5302386 -2.4273252 -2.3190827 -2.1703603 -1.9203932][-2.4793346 -2.46957 -2.4176145 -2.3737912 -2.3621609 -2.3655989 -2.358227 -2.3590956 -2.3543427 -2.3146126 -2.2281721 -2.1022882 -1.9690143 -1.8379543 -1.660646][-1.8490068 -1.8155738 -1.7724115 -1.734822 -1.7070401 -1.6965935 -1.6990838 -1.727464 -1.7691855 -1.7876723 -1.7850394 -1.7568159 -1.7054718 -1.6444882 -1.5991697]]...]
INFO - root - 2017-12-16 09:47:13.493262: step 41210, loss = 0.60, batch loss = 0.34 (48.5 examples/sec; 0.165 sec/batch; 13h:21m:02s remains)
INFO - root - 2017-12-16 09:47:15.178712: step 41220, loss = 0.68, batch loss = 0.42 (46.1 examples/sec; 0.173 sec/batch; 14h:01m:57s remains)
INFO - root - 2017-12-16 09:47:16.862923: step 41230, loss = 0.60, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 13h:26m:18s remains)
INFO - root - 2017-12-16 09:47:18.577146: step 41240, loss = 0.63, batch loss = 0.37 (46.8 examples/sec; 0.171 sec/batch; 13h:50m:20s remains)
INFO - root - 2017-12-16 09:47:20.282603: step 41250, loss = 0.52, batch loss = 0.26 (46.8 examples/sec; 0.171 sec/batch; 13h:49m:05s remains)
INFO - root - 2017-12-16 09:47:21.995242: step 41260, loss = 0.73, batch loss = 0.47 (47.1 examples/sec; 0.170 sec/batch; 13h:43m:42s remains)
INFO - root - 2017-12-16 09:47:23.671130: step 41270, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:22m:32s remains)
INFO - root - 2017-12-16 09:47:25.348082: step 41280, loss = 0.46, batch loss = 0.20 (47.9 examples/sec; 0.167 sec/batch; 13h:30m:46s remains)
INFO - root - 2017-12-16 09:47:27.016102: step 41290, loss = 0.53, batch loss = 0.27 (47.6 examples/sec; 0.168 sec/batch; 13h:35m:34s remains)
INFO - root - 2017-12-16 09:47:28.703699: step 41300, loss = 0.50, batch loss = 0.24 (46.5 examples/sec; 0.172 sec/batch; 13h:55m:51s remains)
2017-12-16 09:47:29.184580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4287262 -2.537123 -2.3565183 -1.9420109 -1.6136005 -1.4257553 -1.2648312 -1.204107 -1.4205755 -1.9338009 -2.4492846 -2.6879141 -2.6651943 -2.5756373 -2.3108802][-2.8921409 -3.0237145 -2.7377005 -2.1140254 -1.5083543 -1.0290616 -0.74932051 -0.836009 -1.2838429 -1.8633542 -2.2802007 -2.5064151 -2.6203206 -2.5331762 -1.9831976][-3.1607218 -3.2806489 -2.9355578 -2.1783657 -1.3661178 -0.728987 -0.44857228 -0.6900636 -1.2639546 -1.7919532 -2.1575072 -2.4339733 -2.6192446 -2.3575006 -1.5099521][-3.0715854 -3.1615865 -2.8517556 -2.12497 -1.2903807 -0.67391622 -0.47922015 -0.7769109 -1.2987019 -1.7307038 -2.0870171 -2.4554141 -2.5896025 -2.0823829 -1.1159452][-2.6854088 -2.7300739 -2.5055988 -1.8970537 -1.1541435 -0.63279235 -0.47905838 -0.67810678 -1.0308629 -1.4668291 -1.942631 -2.3667231 -2.3915024 -1.7805972 -0.99888241][-2.1254086 -2.1316445 -1.9893599 -1.4614639 -0.80376387 -0.34768629 -0.15007949 -0.18943477 -0.442616 -0.99727356 -1.6645601 -2.1186194 -2.0705643 -1.6066718 -1.2258257][-1.5874101 -1.5749297 -1.4584267 -0.95577824 -0.33339214 0.13583159 0.41907287 0.52062392 0.21449161 -0.56112731 -1.3849437 -1.8233581 -1.8017843 -1.6279167 -1.5259461][-1.2941298 -1.2619054 -1.1167868 -0.59434247 0.011006832 0.45346761 0.76938128 0.90188718 0.45857954 -0.43333721 -1.2938848 -1.7380451 -1.8258839 -1.7665343 -1.605727][-1.37504 -1.344295 -1.1879047 -0.70210564 -0.2082839 0.1305356 0.38684821 0.43966722 0.041623831 -0.69092035 -1.3979516 -1.8377503 -1.9740267 -1.8414485 -1.476805][-1.6486342 -1.6064446 -1.4715698 -1.1269311 -0.77928185 -0.51199865 -0.34360862 -0.34794569 -0.62139559 -1.1133249 -1.6298828 -2.0231295 -2.1115103 -1.8410888 -1.3261889][-1.8746943 -1.7854564 -1.7127812 -1.5230038 -1.2850097 -1.0724148 -0.99840236 -1.0765004 -1.3148346 -1.6447041 -1.9882554 -2.2538683 -2.2114713 -1.8437157 -1.3603139][-2.02863 -1.9210094 -1.9048158 -1.8023939 -1.6120927 -1.4834815 -1.5506963 -1.7381499 -1.9688879 -2.1733508 -2.3500319 -2.4294555 -2.2790828 -1.9412056 -1.6212859][-2.1994009 -2.092427 -2.0885019 -1.9984398 -1.8383143 -1.8261428 -2.0234871 -2.2747149 -2.4461956 -2.5376451 -2.5773993 -2.5340371 -2.3689859 -2.1312838 -1.9690142][-2.3780179 -2.3179085 -2.3131139 -2.2004833 -2.066184 -2.1325819 -2.364274 -2.551698 -2.6271646 -2.628757 -2.6008768 -2.5271375 -2.388772 -2.2422013 -2.1684322][-2.4534 -2.4635522 -2.4765289 -2.3636134 -2.253073 -2.3167737 -2.4744339 -2.5661108 -2.5661995 -2.5137441 -2.4604852 -2.396131 -2.2936718 -2.1725683 -2.0715823]]...]
INFO - root - 2017-12-16 09:47:30.832357: step 41310, loss = 0.51, batch loss = 0.25 (49.3 examples/sec; 0.162 sec/batch; 13h:07m:25s remains)
INFO - root - 2017-12-16 09:47:32.516290: step 41320, loss = 0.67, batch loss = 0.41 (47.7 examples/sec; 0.168 sec/batch; 13h:34m:17s remains)
INFO - root - 2017-12-16 09:47:34.212799: step 41330, loss = 0.51, batch loss = 0.25 (48.6 examples/sec; 0.165 sec/batch; 13h:18m:45s remains)
INFO - root - 2017-12-16 09:47:35.880375: step 41340, loss = 0.60, batch loss = 0.35 (47.8 examples/sec; 0.167 sec/batch; 13h:31m:42s remains)
INFO - root - 2017-12-16 09:47:37.579638: step 41350, loss = 0.50, batch loss = 0.24 (46.0 examples/sec; 0.174 sec/batch; 14h:03m:15s remains)
INFO - root - 2017-12-16 09:47:39.306812: step 41360, loss = 0.50, batch loss = 0.24 (46.6 examples/sec; 0.172 sec/batch; 13h:52m:54s remains)
INFO - root - 2017-12-16 09:47:41.042691: step 41370, loss = 0.49, batch loss = 0.23 (46.0 examples/sec; 0.174 sec/batch; 14h:03m:37s remains)
INFO - root - 2017-12-16 09:47:42.745326: step 41380, loss = 0.55, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 13h:16m:53s remains)
INFO - root - 2017-12-16 09:47:44.431427: step 41390, loss = 0.58, batch loss = 0.32 (47.2 examples/sec; 0.169 sec/batch; 13h:42m:05s remains)
INFO - root - 2017-12-16 09:47:46.082357: step 41400, loss = 0.48, batch loss = 0.22 (47.3 examples/sec; 0.169 sec/batch; 13h:40m:35s remains)
2017-12-16 09:47:46.530226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.95260739 -0.32773948 -0.50850916 -0.67734563 -0.53943694 -0.62354624 -1.1287948 -1.5951251 -1.5482011 -1.1685517 -1.1449689 -1.2380171 -0.64176095 0.10251951 -0.15651679][-0.81706941 -0.2172792 -0.46528208 -0.73806787 -0.65182889 -0.77787971 -1.4003327 -2.0029345 -1.8568622 -1.2564226 -1.0609428 -1.0157239 -0.24473262 0.61338878 0.38923264][-0.77495134 -0.26654887 -0.57627904 -0.99715984 -1.0879611 -1.3816426 -2.1870735 -2.8197045 -2.6066933 -1.9179864 -1.684545 -1.6020081 -0.86657238 -0.032586813 -0.097610474][-0.43584514 0.0075023174 -0.41827655 -1.0741061 -1.3401859 -1.7600945 -2.6970172 -3.3376348 -3.0818686 -2.4902091 -2.4296336 -2.5221319 -2.037077 -1.3310632 -1.1843824][-0.028763294 0.47396874 0.022489071 -0.78567743 -1.0786791 -1.40411 -2.2437668 -2.8577085 -2.6666965 -2.3155673 -2.6003458 -3.10061 -3.0559957 -2.5373411 -2.1892972][0.019706964 0.62835932 0.33548641 -0.35488081 -0.44473898 -0.39689004 -0.82309747 -1.2677494 -1.2349547 -1.1619797 -1.8135676 -2.7568097 -3.1463413 -2.8790083 -2.4833381][-0.19431782 0.51496434 0.43883872 0.035110235 0.27965546 0.85301924 1.0090282 0.76270151 0.59350967 0.39841008 -0.48556387 -1.7284484 -2.4125686 -2.3346813 -2.0375459][-0.54865587 0.19978213 0.30228114 0.21496534 0.81438661 1.8611548 2.5790265 2.4173543 1.983237 1.6479218 0.68988276 -0.666142 -1.5156418 -1.6207896 -1.5095228][-0.797596 -0.19953918 -0.054795742 0.012802601 0.73493838 1.9428852 2.8543003 2.7038839 2.2018507 1.8914802 1.0472457 -0.17079997 -0.99448669 -1.2336504 -1.3416979][-0.62406266 -0.38048387 -0.43953454 -0.49899817 -0.00084781647 1.0282788 1.8324835 1.691776 1.212141 0.95079184 0.34081435 -0.61597097 -1.2388446 -1.4927187 -1.7561215][0.038587809 -0.1425128 -0.54596436 -0.91952407 -0.88774383 -0.297781 0.23885393 0.033988237 -0.45211828 -0.7116791 -1.1070154 -1.7503059 -2.1339884 -2.3467331 -2.643153][0.73922396 0.14354587 -0.58524203 -1.300509 -1.7782824 -1.7435327 -1.6016095 -1.9138057 -2.4124861 -2.6156344 -2.8368955 -3.1839163 -3.3078475 -3.38722 -3.5865364][1.1526761 0.28804755 -0.64569914 -1.5605274 -2.4671919 -2.9317679 -3.1217046 -3.5098042 -3.9011226 -4.0258856 -4.1165051 -4.2111483 -4.0738034 -3.9527049 -3.9927096][1.3254087 0.31508565 -0.67782855 -1.6761973 -2.7610178 -3.5189776 -3.9146128 -4.261054 -4.484262 -4.5030355 -4.5546374 -4.4942884 -4.1345406 -3.8021362 -3.6801763][1.4496362 0.33853364 -0.65343356 -1.6829002 -2.8453028 -3.7035096 -4.1488318 -4.338604 -4.346179 -4.2812996 -4.3363218 -4.2144165 -3.6096036 -3.0154805 -2.7845242]]...]
INFO - root - 2017-12-16 09:47:48.196579: step 41410, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 13h:23m:41s remains)
INFO - root - 2017-12-16 09:47:49.876029: step 41420, loss = 0.74, batch loss = 0.49 (46.3 examples/sec; 0.173 sec/batch; 13h:57m:33s remains)
INFO - root - 2017-12-16 09:47:51.513443: step 41430, loss = 0.53, batch loss = 0.27 (49.7 examples/sec; 0.161 sec/batch; 13h:00m:13s remains)
INFO - root - 2017-12-16 09:47:53.153547: step 41440, loss = 0.58, batch loss = 0.32 (48.0 examples/sec; 0.167 sec/batch; 13h:28m:56s remains)
INFO - root - 2017-12-16 09:47:54.828714: step 41450, loss = 0.58, batch loss = 0.32 (45.9 examples/sec; 0.174 sec/batch; 14h:05m:26s remains)
INFO - root - 2017-12-16 09:47:56.462144: step 41460, loss = 0.53, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 13h:05m:19s remains)
INFO - root - 2017-12-16 09:47:58.090919: step 41470, loss = 0.55, batch loss = 0.30 (50.9 examples/sec; 0.157 sec/batch; 12h:43m:03s remains)
INFO - root - 2017-12-16 09:47:59.772699: step 41480, loss = 0.60, batch loss = 0.35 (49.3 examples/sec; 0.162 sec/batch; 13h:07m:03s remains)
INFO - root - 2017-12-16 09:48:01.416842: step 41490, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:21m:37s remains)
INFO - root - 2017-12-16 09:48:03.047475: step 41500, loss = 0.58, batch loss = 0.32 (48.5 examples/sec; 0.165 sec/batch; 13h:19m:56s remains)
2017-12-16 09:48:03.472316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.88553 -2.9183898 -2.5864882 -1.9090852 -0.975827 -0.32558608 -0.21565723 -0.49571145 -1.0366113 -1.7686732 -2.3606381 -2.8016253 -2.9253037 -2.8751833 -2.7902539][-3.1330612 -3.124357 -2.706476 -1.9222648 -0.9502846 -0.27188945 -0.050062656 -0.1339283 -0.575788 -1.3258047 -2.0214229 -2.5696535 -2.822643 -2.8722842 -2.8328619][-3.2469792 -3.1814322 -2.6809306 -1.8291669 -0.78774893 -0.027021885 0.40828848 0.532058 0.18489122 -0.62144232 -1.5031459 -2.2750964 -2.7297549 -2.8919795 -2.8661921][-3.1172032 -2.8976464 -2.2607794 -1.3184487 -0.25616026 0.558074 1.1908882 1.4701464 1.1206577 0.15819311 -0.89390624 -1.8709862 -2.5395596 -2.8092179 -2.8043594][-2.695775 -2.3010325 -1.5696045 -0.61088562 0.38161135 1.2137358 2.035763 2.5024784 2.1939094 1.1155722 -0.10469651 -1.3132602 -2.2724578 -2.6950212 -2.7079294][-2.1018975 -1.5944681 -0.89601016 -0.024020672 0.90588021 1.8602674 2.9444911 3.7804182 3.547425 2.2599318 0.73923182 -0.80298269 -2.0394368 -2.5876775 -2.5894046][-1.4009573 -0.87149262 -0.28629041 0.54297686 1.5247653 2.694135 4.1860771 5.4595919 5.1252165 3.4991109 1.5444989 -0.38877428 -1.8435962 -2.4824827 -2.4620018][-0.782292 -0.29130316 0.19288206 0.9569149 1.9184649 3.1333778 4.7453356 6.15226 5.7488041 4.0329704 1.8858683 -0.23141026 -1.7771358 -2.4448998 -2.3851984][-0.56606448 -0.1752944 0.21259069 0.81130648 1.5483382 2.569463 3.9115012 5.1489334 4.9732056 3.6347063 1.5918162 -0.43869162 -1.9234365 -2.539649 -2.4245152][-0.78192151 -0.56550968 -0.305614 0.11125565 0.63258505 1.4281216 2.4952347 3.5218828 3.540242 2.5799425 0.81124544 -0.97813034 -2.2547271 -2.7038891 -2.510406][-1.253041 -1.2319953 -1.1253155 -0.847415 -0.42610097 0.19993067 1.0438633 1.8301804 1.8710353 1.0527627 -0.38037682 -1.7833593 -2.697053 -2.8666942 -2.5891018][-1.716743 -1.8782318 -1.9312187 -1.7881968 -1.4853616 -1.0789386 -0.48231089 0.047078848 0.037252665 -0.63344157 -1.6639347 -2.5564373 -3.0158465 -2.9103982 -2.5692964][-2.04919 -2.3156743 -2.4582384 -2.480489 -2.3643727 -2.2024238 -1.8618801 -1.4815514 -1.463851 -1.9245123 -2.5286632 -2.9589024 -3.0553622 -2.7691097 -2.4056926][-2.1983726 -2.4620252 -2.6409853 -2.751935 -2.7635157 -2.7172723 -2.506583 -2.2006085 -2.13049 -2.4175239 -2.7561071 -2.9183865 -2.8225617 -2.4980776 -2.1701617][-2.1369848 -2.3655646 -2.5479574 -2.6771712 -2.712862 -2.6781445 -2.5240211 -2.2877071 -2.2231479 -2.4184997 -2.6247902 -2.6587703 -2.4797482 -2.1912541 -1.9326341]]...]
INFO - root - 2017-12-16 09:48:05.131062: step 41510, loss = 0.62, batch loss = 0.36 (49.3 examples/sec; 0.162 sec/batch; 13h:06m:25s remains)
INFO - root - 2017-12-16 09:48:06.761785: step 41520, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:16m:44s remains)
INFO - root - 2017-12-16 09:48:08.404000: step 41530, loss = 0.48, batch loss = 0.22 (48.4 examples/sec; 0.165 sec/batch; 13h:22m:22s remains)
INFO - root - 2017-12-16 09:48:10.044144: step 41540, loss = 0.49, batch loss = 0.23 (49.8 examples/sec; 0.161 sec/batch; 12h:59m:42s remains)
INFO - root - 2017-12-16 09:48:11.679015: step 41550, loss = 0.59, batch loss = 0.33 (48.3 examples/sec; 0.166 sec/batch; 13h:23m:28s remains)
INFO - root - 2017-12-16 09:48:13.331570: step 41560, loss = 0.53, batch loss = 0.27 (47.8 examples/sec; 0.167 sec/batch; 13h:30m:59s remains)
INFO - root - 2017-12-16 09:48:14.960956: step 41570, loss = 0.76, batch loss = 0.50 (50.3 examples/sec; 0.159 sec/batch; 12h:50m:53s remains)
INFO - root - 2017-12-16 09:48:16.609378: step 41580, loss = 0.57, batch loss = 0.31 (47.7 examples/sec; 0.168 sec/batch; 13h:33m:11s remains)
INFO - root - 2017-12-16 09:48:18.265929: step 41590, loss = 0.68, batch loss = 0.42 (48.9 examples/sec; 0.164 sec/batch; 13h:13m:25s remains)
INFO - root - 2017-12-16 09:48:19.882869: step 41600, loss = 0.50, batch loss = 0.24 (49.7 examples/sec; 0.161 sec/batch; 13h:00m:27s remains)
2017-12-16 09:48:20.320216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8967302 -1.9240704 -1.9597483 -2.0067222 -2.0641725 -2.1348112 -2.1904423 -2.2420621 -2.2904718 -2.3365269 -2.3591583 -2.348526 -2.2994897 -2.2194805 -2.16197][-2.05099 -2.1121457 -2.2042222 -2.3102586 -2.4325194 -2.5580885 -2.6263633 -2.6623225 -2.7075527 -2.7602363 -2.7889907 -2.7849162 -2.720242 -2.6144793 -2.551754][-2.3292198 -2.4345727 -2.5610368 -2.6979432 -2.8609631 -3.0078559 -3.0427742 -3.0011892 -3.0072293 -3.0808096 -3.1548676 -3.17635 -3.1191669 -3.0276237 -2.987253][-2.6004508 -2.7467082 -2.8810692 -2.9660134 -3.0484412 -3.0818543 -2.9490747 -2.754499 -2.7457118 -2.9389215 -3.1174784 -3.2038248 -3.1935544 -3.1523752 -3.2247782][-2.6374507 -2.8530316 -2.9265416 -2.7998347 -2.6043112 -2.3234928 -1.8546193 -1.4048125 -1.4360404 -1.8772119 -2.3016148 -2.5755191 -2.6774843 -2.767684 -3.0212033][-2.4586322 -2.689857 -2.5524385 -2.0696986 -1.461426 -0.72967207 0.21808505 0.98525977 0.82649589 0.035310507 -0.75342524 -1.302802 -1.5888994 -1.840081 -2.3056288][-2.2297833 -2.3217366 -1.8496459 -0.96082687 0.061830044 1.2166107 2.6490557 3.7529933 3.3570263 2.0732143 0.91692448 0.12460399 -0.37460494 -0.76827049 -1.3939338][-2.0795188 -1.9171536 -1.0895253 0.087203026 1.3390582 2.787792 4.595089 5.9703903 5.061904 3.2131789 1.8187635 0.98744917 0.50074816 0.072705507 -0.61585867][-2.0256498 -1.5953476 -0.5768671 0.51924825 1.5096259 2.6735637 4.0817881 4.9986429 4.1355715 2.5520284 1.4428422 0.90747833 0.6900003 0.45850992 -0.19400978][-2.0495052 -1.500921 -0.64686275 0.017493963 0.43048716 1.0184989 1.8579471 2.298681 1.7046206 0.80650759 0.34445119 0.36329317 0.52304721 0.53279614 0.0098352432][-1.9935019 -1.5279562 -0.99672139 -0.829792 -0.92127013 -0.81939614 -0.37002039 -0.159765 -0.55238235 -0.91967297 -0.74664104 -0.19130945 0.357661 0.57570195 0.151021][-2.0642896 -1.734621 -1.4532118 -1.5564984 -1.9005624 -2.0403795 -1.8589723 -1.7899342 -2.0099268 -2.0035455 -1.3628987 -0.42784715 0.36603618 0.67238617 0.35504627][-2.4541817 -2.3052363 -2.1215265 -2.1898553 -2.4799223 -2.7255304 -2.7415864 -2.7719367 -2.8630571 -2.5880504 -1.6639842 -0.55477762 0.33068275 0.67650771 0.41622663][-3.0714467 -3.0712812 -2.8851857 -2.8002126 -2.9429362 -3.1465487 -3.2276993 -3.281024 -3.2668009 -2.8823371 -1.9565564 -0.95169663 -0.21078706 0.055997372 -0.16338873][-3.4991603 -3.499203 -3.2292557 -3.0030284 -3.0183234 -3.1452045 -3.212553 -3.2180264 -3.1418266 -2.8154459 -2.1354468 -1.4648944 -1.0355077 -0.89762831 -1.0797155]]...]
INFO - root - 2017-12-16 09:48:21.953019: step 41610, loss = 0.46, batch loss = 0.20 (48.1 examples/sec; 0.166 sec/batch; 13h:25m:59s remains)
INFO - root - 2017-12-16 09:48:23.602515: step 41620, loss = 0.59, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 13h:19m:41s remains)
INFO - root - 2017-12-16 09:48:25.257177: step 41630, loss = 0.60, batch loss = 0.34 (48.3 examples/sec; 0.166 sec/batch; 13h:22m:45s remains)
INFO - root - 2017-12-16 09:48:26.890897: step 41640, loss = 0.60, batch loss = 0.34 (49.0 examples/sec; 0.163 sec/batch; 13h:11m:00s remains)
INFO - root - 2017-12-16 09:48:28.551799: step 41650, loss = 0.49, batch loss = 0.23 (46.4 examples/sec; 0.172 sec/batch; 13h:56m:09s remains)
INFO - root - 2017-12-16 09:48:30.205217: step 41660, loss = 0.54, batch loss = 0.29 (47.8 examples/sec; 0.167 sec/batch; 13h:30m:53s remains)
INFO - root - 2017-12-16 09:48:31.828757: step 41670, loss = 0.60, batch loss = 0.34 (50.7 examples/sec; 0.158 sec/batch; 12h:44m:59s remains)
INFO - root - 2017-12-16 09:48:33.491223: step 41680, loss = 0.54, batch loss = 0.29 (47.0 examples/sec; 0.170 sec/batch; 13h:45m:04s remains)
INFO - root - 2017-12-16 09:48:35.176099: step 41690, loss = 0.57, batch loss = 0.31 (46.0 examples/sec; 0.174 sec/batch; 14h:02m:34s remains)
INFO - root - 2017-12-16 09:48:36.804932: step 41700, loss = 0.69, batch loss = 0.43 (49.1 examples/sec; 0.163 sec/batch; 13h:09m:51s remains)
2017-12-16 09:48:37.253567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0774198 -1.121491 -1.3657649 -1.4952056 -1.4808326 -1.3966019 -1.2804927 -1.1628613 -1.0687177 -1.0203012 -0.99134636 -0.90591955 -0.74993885 -0.63082981 -0.80502307][-1.6203272 -1.6371214 -1.9143567 -2.050694 -2.0014882 -1.8762118 -1.688764 -1.4830501 -1.3100345 -1.2479851 -1.2653733 -1.1965268 -1.0116243 -0.85661125 -1.0420913][-2.2246702 -2.2691762 -2.6247158 -2.8147254 -2.7990849 -2.6656175 -2.395674 -2.0642867 -1.7995603 -1.7425227 -1.8477398 -1.8747115 -1.7359976 -1.5524437 -1.6350843][-2.4132047 -2.4893377 -2.9132249 -3.1709232 -3.2369645 -3.0981066 -2.7096889 -2.2464085 -1.89475 -1.8383284 -2.0299368 -2.1849136 -2.1167474 -1.9110787 -1.887416][-1.8861611 -2.0125916 -2.5087211 -2.7914832 -2.878787 -2.7056878 -2.1675839 -1.5121808 -1.0658261 -1.0773681 -1.48715 -1.8634719 -2.000602 -1.8920803 -1.8122363][-0.868644 -1.0458046 -1.6386445 -1.9958012 -2.0787444 -1.8676981 -1.1732019 -0.22420955 0.41369414 0.37112236 -0.25238657 -0.92249167 -1.3485254 -1.4326576 -1.40681][0.12680316 -0.059725523 -0.72081494 -1.1116462 -1.1808854 -0.9008 -0.13244963 0.9735496 1.7896349 1.829448 1.1248467 0.27122402 -0.39401972 -0.73635578 -0.90318751][0.69047689 0.58583283 0.029085398 -0.35736084 -0.48909354 -0.23355985 0.46755672 1.5513406 2.4085925 2.5360525 1.8657186 0.9322679 0.16191101 -0.35364342 -0.71627259][0.30121017 0.26076722 -0.20845151 -0.60129821 -0.77141666 -0.58216131 0.0068256855 0.95442295 1.7683089 1.9808056 1.4710863 0.65669513 -0.113693 -0.71323717 -1.1204376][-0.56116366 -0.70670915 -1.2081264 -1.67623 -1.9051814 -1.7659044 -1.2799648 -0.53218246 0.07547164 0.30447316 -0.027643681 -0.61130142 -1.165318 -1.5952706 -1.8682839][-1.1187272 -1.4280703 -2.0135179 -2.5107579 -2.7791681 -2.6800275 -2.3482375 -1.8796849 -1.4965897 -1.2950342 -1.4220314 -1.6896973 -1.8768196 -1.9745649 -2.0459809][-0.87822652 -1.3026359 -1.9364378 -2.3883309 -2.6393886 -2.6015828 -2.4188795 -2.1605098 -1.9400499 -1.8082414 -1.7938218 -1.7899809 -1.7122886 -1.586687 -1.5860282][-0.31479788 -0.79348505 -1.4741474 -1.9039688 -2.0949545 -2.0899968 -1.9906038 -1.8466086 -1.7049452 -1.5965673 -1.5290729 -1.3738669 -1.1376371 -0.91347718 -0.92433536][0.28767729 -0.18258309 -0.86421394 -1.2547358 -1.407228 -1.4125748 -1.3596108 -1.2909445 -1.2251966 -1.1761706 -1.0979629 -0.85607028 -0.46981919 -0.1713655 -0.25375319][0.49642706 0.11236382 -0.51857841 -0.82413769 -0.8992523 -0.90026546 -0.87720358 -0.849756 -0.83032954 -0.81843281 -0.72340882 -0.37718248 0.16797113 0.55166435 0.43713403]]...]
INFO - root - 2017-12-16 09:48:38.936981: step 41710, loss = 0.56, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 13h:53m:00s remains)
INFO - root - 2017-12-16 09:48:40.601102: step 41720, loss = 0.55, batch loss = 0.29 (49.9 examples/sec; 0.160 sec/batch; 12h:57m:34s remains)
INFO - root - 2017-12-16 09:48:42.227842: step 41730, loss = 0.62, batch loss = 0.36 (48.9 examples/sec; 0.164 sec/batch; 13h:13m:30s remains)
INFO - root - 2017-12-16 09:48:43.860118: step 41740, loss = 0.45, batch loss = 0.19 (48.5 examples/sec; 0.165 sec/batch; 13h:19m:10s remains)
INFO - root - 2017-12-16 09:48:45.527873: step 41750, loss = 0.57, batch loss = 0.31 (47.8 examples/sec; 0.167 sec/batch; 13h:31m:08s remains)
INFO - root - 2017-12-16 09:48:47.149283: step 41760, loss = 0.49, batch loss = 0.24 (49.4 examples/sec; 0.162 sec/batch; 13h:05m:19s remains)
INFO - root - 2017-12-16 09:48:48.802020: step 41770, loss = 0.57, batch loss = 0.31 (49.0 examples/sec; 0.163 sec/batch; 13h:10m:32s remains)
INFO - root - 2017-12-16 09:48:50.470402: step 41780, loss = 0.52, batch loss = 0.26 (47.4 examples/sec; 0.169 sec/batch; 13h:37m:02s remains)
INFO - root - 2017-12-16 09:48:52.096763: step 41790, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 13h:11m:03s remains)
INFO - root - 2017-12-16 09:48:53.729942: step 41800, loss = 0.56, batch loss = 0.30 (49.2 examples/sec; 0.162 sec/batch; 13h:07m:07s remains)
2017-12-16 09:48:54.144759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9183118 -2.5381098 -2.2872379 -2.2128439 -2.2623911 -2.4738102 -2.8329296 -3.150727 -3.2575455 -3.1401901 -2.9340827 -2.7839489 -2.8786566 -3.0666974 -3.1484113][-3.0787449 -2.7075131 -2.4915209 -2.3704407 -2.3537359 -2.5523274 -2.951113 -3.3255742 -3.4048605 -3.1593888 -2.7583859 -2.4120238 -2.4180198 -2.6565182 -2.8761][-2.9255402 -2.6598988 -2.583143 -2.5572653 -2.5824528 -2.7553158 -3.0996354 -3.4479671 -3.5212846 -3.2098827 -2.6566234 -2.1414838 -2.0442648 -2.27015 -2.5514667][-2.302774 -2.1887953 -2.238703 -2.3842208 -2.5627532 -2.7493095 -3.0318775 -3.3196464 -3.4118152 -3.1695473 -2.7171335 -2.290045 -2.2358654 -2.478853 -2.7546268][-1.5273342 -1.5056452 -1.5656271 -1.8262303 -2.0849047 -2.1917675 -2.263056 -2.441798 -2.5988624 -2.5674262 -2.5265675 -2.5464704 -2.7550848 -3.0618484 -3.2795224][-0.86438811 -0.83686173 -0.84003472 -1.076611 -1.2976632 -1.1722964 -0.92167819 -0.87758613 -1.016306 -1.274471 -1.7833774 -2.402812 -3.0131974 -3.4827602 -3.6896143][-0.5722456 -0.528177 -0.48117733 -0.63348866 -0.67899048 -0.19496322 0.52396464 0.94601727 0.93773246 0.46048808 -0.50524986 -1.6871927 -2.7117343 -3.386389 -3.667721][-0.81455529 -0.80302954 -0.77301705 -0.87148237 -0.66919494 0.21409988 1.4459372 2.3530252 2.5819461 2.0537417 0.88461995 -0.58036149 -1.8783917 -2.7283356 -3.1088974][-1.4955317 -1.5574906 -1.6297565 -1.7333431 -1.3714193 -0.30096149 1.2025955 2.4702284 2.9359176 2.53426 1.5050414 0.15277147 -1.121538 -1.9547529 -2.3347831][-2.19804 -2.3180108 -2.6209769 -2.832376 -2.5338941 -1.6113429 -0.20531893 1.1666715 1.8339832 1.6947277 1.0034919 0.0020678043 -1.0193254 -1.7071285 -2.0242944][-2.3069363 -2.4346352 -2.9410021 -3.3434014 -3.32492 -2.8393955 -1.8548456 -0.71399307 -0.025512695 0.06927371 -0.25113869 -0.8861593 -1.6046534 -2.1504745 -2.4751527][-1.6772672 -1.7962621 -2.4382875 -3.060693 -3.4519522 -3.5728555 -3.2181606 -2.5418196 -1.9977889 -1.8117234 -1.88048 -2.1980605 -2.6579795 -3.1249168 -3.4640896][-0.72990346 -0.80386519 -1.5327759 -2.3295019 -3.0945234 -3.8148117 -4.1160488 -3.9337711 -3.6128573 -3.4332304 -3.3433833 -3.4057956 -3.711237 -4.1154666 -4.3690453][0.15515471 0.11550522 -0.63983154 -1.509214 -2.496556 -3.6442966 -4.4258547 -4.6169147 -4.5118852 -4.3705573 -4.1708212 -4.0482173 -4.2323666 -4.5355244 -4.6413412][0.6542592 0.68355393 0.035098076 -0.74088073 -1.7554967 -3.0934069 -4.1478472 -4.5486841 -4.5689888 -4.41327 -4.0549068 -3.7534223 -3.8515472 -4.1343346 -4.2211475]]...]
INFO - root - 2017-12-16 09:48:55.797662: step 41810, loss = 0.54, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 13h:41m:16s remains)
INFO - root - 2017-12-16 09:48:57.471165: step 41820, loss = 0.71, batch loss = 0.45 (47.3 examples/sec; 0.169 sec/batch; 13h:38m:33s remains)
INFO - root - 2017-12-16 09:48:59.114373: step 41830, loss = 0.52, batch loss = 0.26 (47.8 examples/sec; 0.167 sec/batch; 13h:31m:15s remains)
INFO - root - 2017-12-16 09:49:00.728786: step 41840, loss = 0.56, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:17m:12s remains)
INFO - root - 2017-12-16 09:49:02.362666: step 41850, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:21m:05s remains)
INFO - root - 2017-12-16 09:49:03.987610: step 41860, loss = 0.50, batch loss = 0.24 (49.9 examples/sec; 0.160 sec/batch; 12h:56m:14s remains)
INFO - root - 2017-12-16 09:49:05.620081: step 41870, loss = 0.57, batch loss = 0.31 (48.5 examples/sec; 0.165 sec/batch; 13h:19m:35s remains)
INFO - root - 2017-12-16 09:49:07.238791: step 41880, loss = 0.52, batch loss = 0.26 (49.6 examples/sec; 0.161 sec/batch; 13h:00m:31s remains)
INFO - root - 2017-12-16 09:49:08.908810: step 41890, loss = 0.53, batch loss = 0.27 (50.3 examples/sec; 0.159 sec/batch; 12h:50m:31s remains)
INFO - root - 2017-12-16 09:49:10.572526: step 41900, loss = 0.64, batch loss = 0.38 (48.5 examples/sec; 0.165 sec/batch; 13h:18m:52s remains)
2017-12-16 09:49:11.000300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1206559 -0.98925436 -0.90911341 -0.85501742 -1.3776392 -2.374573 -3.2388039 -3.4100747 -3.2941709 -2.9123745 -1.8951017 -0.49063528 0.37630391 0.51422477 -0.32048845][-1.4607936 -1.7480524 -2.0299098 -2.2310598 -2.7442255 -3.5121031 -4.0437303 -4.06155 -3.8582604 -3.5774877 -2.8041384 -1.6984022 -0.9101032 -0.65504277 -1.2362795][-1.5250437 -2.0379457 -2.5100627 -2.8073175 -3.1616526 -3.5286908 -3.7190785 -3.6194963 -3.5797412 -3.6118727 -3.1883802 -2.3165512 -1.5953327 -1.2686104 -1.6162577][-1.5166461 -1.9677407 -2.3734066 -2.654336 -2.7430701 -2.6786165 -2.6645536 -2.6748331 -2.8752716 -3.2381992 -3.1431758 -2.5466111 -1.8949438 -1.4376353 -1.5096326][-1.5292571 -1.8159547 -2.0212719 -2.1824026 -1.9719369 -1.5946455 -1.5177212 -1.7621655 -2.2328517 -2.8908415 -3.0529785 -2.6647711 -2.0843406 -1.4869663 -1.345643][-1.6973271 -1.6716672 -1.5897809 -1.452312 -0.8975482 -0.25700569 -0.26085424 -0.83335686 -1.6493118 -2.5798602 -3.0638816 -2.8585978 -2.2216964 -1.5425711 -1.3037575][-1.5116316 -1.1814616 -0.7762537 -0.18083215 0.86129689 1.7972987 1.5664721 0.41608286 -0.88435638 -2.1730802 -2.9810526 -2.9961495 -2.319828 -1.522137 -1.2298005][-1.0441967 -0.45903087 0.26011658 1.309582 2.8370049 4.016221 3.3359425 1.6362021 0.016763449 -1.519166 -2.615298 -2.848314 -2.2110913 -1.3305376 -1.0220387][-0.81654692 -0.10755038 0.7162869 1.940551 3.7228258 4.9094572 3.9061234 2.1472833 0.63245392 -0.85434628 -2.0168033 -2.4083025 -1.9494362 -1.1508374 -0.93955004][-1.1926576 -0.50211334 0.25666881 1.3271198 2.8367417 3.7274921 2.9474394 1.6476071 0.40124202 -0.86422455 -1.8451105 -2.2108259 -1.8220836 -1.1357045 -1.0433224][-1.7790825 -1.1980231 -0.65233755 0.066137791 0.9965167 1.4322715 0.95433736 0.20974851 -0.60315287 -1.4156964 -2.0112109 -2.0271497 -1.4744875 -0.90092444 -0.95932412][-2.5406187 -2.1277215 -1.7375458 -1.3555568 -0.980736 -0.88829434 -1.1938491 -1.5593305 -1.81157 -2.0687261 -2.1020589 -1.7074524 -0.9946152 -0.54197645 -0.78366959][-3.6248205 -3.3186686 -2.9587548 -2.749361 -2.7265337 -2.8621178 -3.1630902 -3.2934518 -3.1036654 -2.7690833 -2.2592163 -1.5868943 -0.95075607 -0.68782794 -1.0496808][-4.5576892 -4.2592993 -3.9627984 -3.9106433 -4.114831 -4.3582854 -4.7054987 -4.8134117 -4.4260635 -3.6770513 -2.7156196 -1.9474528 -1.529346 -1.5368785 -1.8652363][-4.9542947 -4.6824989 -4.4530239 -4.4890203 -4.7415571 -5.0401964 -5.4673767 -5.758769 -5.5308881 -4.7381616 -3.6766739 -2.90745 -2.6932096 -2.772646 -2.9654961]]...]
INFO - root - 2017-12-16 09:49:12.645334: step 41910, loss = 0.51, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 13h:37m:53s remains)
INFO - root - 2017-12-16 09:49:14.327005: step 41920, loss = 0.55, batch loss = 0.29 (49.9 examples/sec; 0.160 sec/batch; 12h:56m:13s remains)
INFO - root - 2017-12-16 09:49:15.973009: step 41930, loss = 0.59, batch loss = 0.33 (49.8 examples/sec; 0.161 sec/batch; 12h:58m:31s remains)
INFO - root - 2017-12-16 09:49:17.599564: step 41940, loss = 0.52, batch loss = 0.26 (51.2 examples/sec; 0.156 sec/batch; 12h:36m:55s remains)
INFO - root - 2017-12-16 09:49:19.237822: step 41950, loss = 0.56, batch loss = 0.30 (49.2 examples/sec; 0.162 sec/batch; 13h:06m:52s remains)
INFO - root - 2017-12-16 09:49:20.870357: step 41960, loss = 0.65, batch loss = 0.39 (50.0 examples/sec; 0.160 sec/batch; 12h:54m:09s remains)
INFO - root - 2017-12-16 09:49:22.525974: step 41970, loss = 0.61, batch loss = 0.35 (46.9 examples/sec; 0.171 sec/batch; 13h:46m:14s remains)
INFO - root - 2017-12-16 09:49:24.172387: step 41980, loss = 0.44, batch loss = 0.18 (49.5 examples/sec; 0.162 sec/batch; 13h:02m:33s remains)
INFO - root - 2017-12-16 09:49:25.796535: step 41990, loss = 0.50, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 13h:13m:46s remains)
INFO - root - 2017-12-16 09:49:27.460796: step 42000, loss = 0.58, batch loss = 0.32 (47.6 examples/sec; 0.168 sec/batch; 13h:33m:59s remains)
2017-12-16 09:49:27.897864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.374414 -3.16696 -2.4320698 -1.1481879 -0.38451791 -0.61039734 -1.0574734 -1.0011369 -1.0752902 -1.908836 -3.0867846 -3.8517342 -3.779882 -3.0857847 -2.3126857][-3.7827542 -3.8109105 -3.3428636 -2.4220304 -1.8690674 -1.9708443 -2.0760334 -1.6734226 -1.4849429 -2.1147115 -3.1657391 -3.8758478 -3.8402526 -3.185132 -2.4050002][-4.22768 -4.3583803 -4.1055155 -3.4696641 -3.0095541 -2.9293206 -2.6383383 -1.9156302 -1.4800205 -1.9156437 -2.8147261 -3.5175319 -3.5882378 -3.0487251 -2.3605902][-4.6734047 -4.8117733 -4.6214409 -4.0145864 -3.3942776 -2.969151 -2.3632848 -1.5310725 -1.106205 -1.4321216 -2.1823239 -2.9095223 -3.155983 -2.8176498 -2.2514696][-4.85012 -4.9456463 -4.6967511 -3.8908794 -2.7767789 -1.788923 -0.89755642 -0.15966988 -0.055002451 -0.63537693 -1.5034276 -2.3765714 -2.8121641 -2.643996 -2.1654322][-4.5414758 -4.4727035 -3.9685283 -2.738055 -0.81687176 0.95621467 2.0332019 2.2807686 1.4856997 0.072559357 -1.3260554 -2.3632996 -2.7999928 -2.6167161 -2.1070347][-4.0852747 -3.76319 -2.8466408 -1.1106296 1.5996935 4.2104435 5.4358721 4.8500576 2.696336 0.053556442 -1.9346713 -2.9571557 -3.1490886 -2.7296739 -2.06415][-3.8024402 -3.3305702 -2.1772091 -0.23217607 2.8646104 6.1320047 7.309844 6.0526114 2.8861406 -0.60204816 -2.9517629 -3.8349218 -3.6659243 -2.9179845 -2.045326][-3.6761565 -3.250339 -2.1405971 -0.52168345 1.9250901 4.5771179 5.964633 5.043314 1.8499806 -1.7684779 -4.0527248 -4.6320186 -4.0688367 -3.0092053 -1.9632406][-3.6767881 -3.4362888 -2.4811106 -1.2563152 0.35926056 2.2899511 3.680578 3.1907985 0.47997642 -2.7414908 -4.6849813 -5.0016909 -4.2128448 -2.9871204 -1.8603756][-3.8732729 -3.8053641 -3.0485029 -2.1670074 -1.1574341 0.14380193 1.4253883 1.4326084 -0.49301159 -3.0305128 -4.5986738 -4.8082666 -4.0414143 -2.8566332 -1.7649527][-3.9653606 -4.1104665 -3.6548715 -3.119175 -2.5025005 -1.5702372 -0.35833526 0.072635889 -1.086926 -2.9507334 -4.2027621 -4.3927474 -3.770098 -2.7217498 -1.7157472][-3.9859571 -4.2725358 -4.0515165 -3.7195508 -3.26975 -2.4594302 -1.3334978 -0.71741939 -1.3843536 -2.7562261 -3.7748044 -3.973208 -3.4572589 -2.5336325 -1.6609962][-3.9454017 -4.2099442 -4.0519819 -3.7198415 -3.238049 -2.4905589 -1.5241362 -0.92428994 -1.3561989 -2.4462698 -3.3157732 -3.5209243 -3.0683956 -2.2567239 -1.5264555][-3.6775 -3.8121274 -3.5684898 -3.1621597 -2.6531794 -2.0111113 -1.2187409 -0.73308337 -1.1337463 -2.1113777 -2.8898981 -3.0815878 -2.659574 -1.9146466 -1.3233603]]...]
INFO - root - 2017-12-16 09:49:29.557788: step 42010, loss = 0.53, batch loss = 0.27 (50.0 examples/sec; 0.160 sec/batch; 12h:53m:53s remains)
INFO - root - 2017-12-16 09:49:31.189819: step 42020, loss = 0.48, batch loss = 0.22 (49.8 examples/sec; 0.161 sec/batch; 12h:58m:01s remains)
INFO - root - 2017-12-16 09:49:32.809400: step 42030, loss = 0.55, batch loss = 0.29 (49.8 examples/sec; 0.161 sec/batch; 12h:57m:45s remains)
INFO - root - 2017-12-16 09:49:34.442994: step 42040, loss = 0.73, batch loss = 0.47 (48.9 examples/sec; 0.164 sec/batch; 13h:12m:30s remains)
INFO - root - 2017-12-16 09:49:36.110284: step 42050, loss = 0.48, batch loss = 0.22 (47.2 examples/sec; 0.169 sec/batch; 13h:40m:04s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:49:37.742549: step 42060, loss = 0.62, batch loss = 0.36 (50.3 examples/sec; 0.159 sec/batch; 12h:49m:53s remains)
INFO - root - 2017-12-16 09:49:39.384837: step 42070, loss = 0.48, batch loss = 0.22 (46.8 examples/sec; 0.171 sec/batch; 13h:48m:12s remains)
INFO - root - 2017-12-16 09:49:41.016572: step 42080, loss = 0.53, batch loss = 0.27 (49.3 examples/sec; 0.162 sec/batch; 13h:04m:47s remains)
INFO - root - 2017-12-16 09:49:42.650295: step 42090, loss = 0.59, batch loss = 0.33 (50.1 examples/sec; 0.160 sec/batch; 12h:52m:49s remains)
INFO - root - 2017-12-16 09:49:44.309319: step 42100, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 13h:08m:51s remains)
2017-12-16 09:49:44.732276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9492862 -2.8504467 -2.7510097 -2.6699762 -2.6705918 -2.6829417 -2.5794983 -2.5306542 -2.591609 -2.693053 -2.848336 -3.0288477 -3.1839395 -3.1869786 -3.0036619][-3.6287355 -3.6464214 -3.6077762 -3.5651445 -3.555747 -3.5273945 -3.3280835 -3.1619194 -3.0905728 -3.0841637 -3.2101851 -3.3790903 -3.5042627 -3.4294782 -3.1594739][-3.8402781 -3.9969659 -4.0315742 -4.0892782 -4.1340017 -4.041585 -3.6835971 -3.3636103 -3.156683 -3.04502 -3.0887003 -3.2759302 -3.5295763 -3.5432041 -3.2783771][-3.6019092 -3.8101959 -3.8926685 -4.0191617 -4.0721703 -3.8704052 -3.3614888 -2.8906841 -2.6327944 -2.4980955 -2.5016143 -2.7879992 -3.2627096 -3.462292 -3.2611427][-3.1789606 -3.3108375 -3.3607795 -3.4239581 -3.3744745 -3.0601256 -2.4487624 -1.8881272 -1.6601939 -1.5935023 -1.6539081 -2.0614233 -2.7712393 -3.1968372 -3.1253715][-2.4166749 -2.4419191 -2.37519 -2.2767611 -2.0800018 -1.6727921 -0.99978673 -0.43513811 -0.18418288 -0.19942594 -0.42468488 -1.0026525 -1.9507341 -2.6688812 -2.835115][-1.0788981 -1.0920321 -1.0326008 -0.84715128 -0.58198178 -0.19078922 0.37107611 0.85072255 1.0284867 0.836931 0.40461969 -0.3357079 -1.3914514 -2.2454536 -2.5902414][0.47027087 0.24147558 0.12714243 0.21525526 0.37581205 0.59670472 0.93399334 1.2363687 1.2835243 0.93000865 0.34998775 -0.419299 -1.3557235 -2.1293736 -2.5023937][1.4133482 0.94159603 0.56102324 0.36088753 0.27884412 0.30596423 0.51956487 0.70520782 0.589726 0.11928368 -0.52713895 -1.2132576 -1.892921 -2.4189174 -2.6269839][1.3878248 0.71183872 0.13153791 -0.29725742 -0.56673574 -0.64909315 -0.46719086 -0.25444293 -0.36642265 -0.89684439 -1.5881757 -2.201623 -2.6711631 -2.9125469 -2.8683074][0.77664876 0.061517477 -0.56830907 -1.0657252 -1.4245472 -1.4995818 -1.3128791 -1.0816965 -1.1502849 -1.638935 -2.2828836 -2.8393781 -3.2216296 -3.2955346 -3.0888302][0.19448733 -0.36797428 -0.92246664 -1.3692008 -1.6679225 -1.7175095 -1.5037092 -1.2582822 -1.2887439 -1.6810565 -2.2361996 -2.7762866 -3.2048106 -3.3225253 -3.1283877][-0.18745041 -0.48956454 -0.857726 -1.1532115 -1.3132733 -1.309087 -1.0300194 -0.7519809 -0.78672504 -1.1412877 -1.6231008 -2.1833248 -2.7547102 -3.0515041 -2.9769554][-0.29755592 -0.40052962 -0.54368913 -0.646847 -0.67622006 -0.58263218 -0.22861671 0.036591291 -0.044961691 -0.40051579 -0.86779845 -1.468469 -2.184082 -2.6352425 -2.6941571][-0.46079826 -0.51631534 -0.5758903 -0.578889 -0.53234386 -0.39262307 -0.022845745 0.275558 0.24159217 -0.048568964 -0.50934851 -1.1416303 -1.8918579 -2.3923962 -2.503418]]...]
INFO - root - 2017-12-16 09:49:46.359098: step 42110, loss = 0.53, batch loss = 0.27 (49.2 examples/sec; 0.162 sec/batch; 13h:06m:13s remains)
INFO - root - 2017-12-16 09:49:48.004793: step 42120, loss = 0.57, batch loss = 0.31 (50.2 examples/sec; 0.159 sec/batch; 12h:51m:25s remains)
INFO - root - 2017-12-16 09:49:49.613203: step 42130, loss = 0.72, batch loss = 0.46 (50.6 examples/sec; 0.158 sec/batch; 12h:45m:30s remains)
INFO - root - 2017-12-16 09:49:51.273702: step 42140, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:19m:05s remains)
INFO - root - 2017-12-16 09:49:52.927512: step 42150, loss = 0.51, batch loss = 0.25 (49.4 examples/sec; 0.162 sec/batch; 13h:03m:27s remains)
INFO - root - 2017-12-16 09:49:54.552359: step 42160, loss = 0.66, batch loss = 0.40 (51.0 examples/sec; 0.157 sec/batch; 12h:39m:31s remains)
INFO - root - 2017-12-16 09:49:56.179121: step 42170, loss = 0.74, batch loss = 0.48 (48.3 examples/sec; 0.166 sec/batch; 13h:21m:52s remains)
INFO - root - 2017-12-16 09:49:57.786760: step 42180, loss = 0.69, batch loss = 0.43 (51.1 examples/sec; 0.157 sec/batch; 12h:37m:46s remains)
INFO - root - 2017-12-16 09:49:59.430405: step 42190, loss = 0.53, batch loss = 0.28 (48.4 examples/sec; 0.165 sec/batch; 13h:20m:03s remains)
INFO - root - 2017-12-16 09:50:01.057754: step 42200, loss = 0.59, batch loss = 0.33 (50.1 examples/sec; 0.160 sec/batch; 12h:52m:26s remains)
2017-12-16 09:50:01.474837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4879656 -2.1939704 -1.8049541 -1.4169384 -1.1089519 -0.9455682 -1.0849391 -1.6985497 -2.4696927 -3.1229932 -3.537498 -3.7061524 -3.4958854 -2.8202503 -2.2797766][-2.88553 -2.8164468 -2.6401963 -2.4646113 -2.3436224 -2.2839842 -2.3264995 -2.712678 -3.2201643 -3.5634093 -3.617331 -3.4395289 -2.9693937 -2.1699574 -1.6163127][-3.2395444 -3.3009591 -3.2579114 -3.2128172 -3.1526263 -3.0756001 -2.9594746 -3.0498476 -3.3559325 -3.5184426 -3.2750657 -2.8148036 -2.1814063 -1.291069 -0.57343912][-3.4671178 -3.5146403 -3.4570549 -3.342648 -3.1362936 -2.9039855 -2.6181202 -2.4848516 -2.6916456 -2.8320665 -2.5396149 -1.9790889 -1.2516961 -0.40936363 0.33152175][-3.4536498 -3.3622165 -3.082216 -2.6411419 -2.1239543 -1.6043677 -1.0595869 -0.67929924 -0.92516863 -1.3440324 -1.2962897 -0.86067307 -0.234097 0.362931 0.90047145][-3.2166939 -2.8858318 -2.2157698 -1.3290602 -0.40014756 0.47097993 1.3984921 2.2253525 1.875797 0.85853267 0.40435529 0.5321548 0.74655795 0.803246 0.92699313][-2.8722126 -2.3677986 -1.3120835 0.036230564 1.2366829 2.357208 3.8593075 5.574832 5.0783176 3.0806749 1.9603274 1.7483876 1.4913397 0.84858966 0.45425916][-2.6681123 -2.1252785 -0.86549556 0.678493 1.8974493 3.0674303 4.9560194 7.5653181 6.9418182 4.0324755 2.2614295 1.7077291 1.080328 0.04603529 -0.673507][-2.7923474 -2.3496511 -1.2737693 0.0614233 1.0602455 1.9654906 3.5088599 5.4100847 5.0214357 2.7421443 1.0423167 0.32505727 -0.37932777 -1.3367214 -1.9863997][-3.0165091 -2.7784209 -2.1335297 -1.3204675 -0.66727972 -0.11008358 0.82875776 1.9019687 1.7266905 0.32281995 -0.92281806 -1.5232316 -2.1392643 -2.8134551 -3.163312][-3.1511838 -3.1918211 -2.9303718 -2.6068437 -2.3490691 -2.1691735 -1.7942517 -1.2924418 -1.2857906 -1.9932839 -2.6561711 -3.0719638 -3.491024 -3.8819537 -3.9687834][-3.0634189 -3.3249228 -3.3384135 -3.3490877 -3.3956094 -3.5005441 -3.5432084 -3.4040275 -3.3635225 -3.585815 -3.8466821 -4.0094166 -4.1461496 -4.2753458 -4.2116895][-2.7249374 -3.0644584 -3.2524743 -3.4191282 -3.6009221 -3.8453989 -4.08978 -4.1168823 -4.0960846 -4.09972 -4.1331115 -4.1267328 -4.0656548 -4.0268116 -3.9212174][-2.3505375 -2.6048994 -2.8238435 -3.0104289 -3.2079272 -3.4617758 -3.7093275 -3.7801692 -3.745491 -3.6619802 -3.605454 -3.5355005 -3.3995876 -3.2922993 -3.1740048][-2.0559025 -2.2023613 -2.3417931 -2.4797528 -2.6097488 -2.7992373 -2.9743309 -3.0377848 -3.0159376 -2.9449599 -2.8916471 -2.8253288 -2.6915274 -2.5955937 -2.51363]]...]
INFO - root - 2017-12-16 09:50:03.107011: step 42210, loss = 0.60, batch loss = 0.34 (48.6 examples/sec; 0.165 sec/batch; 13h:17m:04s remains)
INFO - root - 2017-12-16 09:50:04.751059: step 42220, loss = 0.49, batch loss = 0.23 (47.8 examples/sec; 0.167 sec/batch; 13h:29m:13s remains)
INFO - root - 2017-12-16 09:50:06.389007: step 42230, loss = 0.58, batch loss = 0.32 (49.3 examples/sec; 0.162 sec/batch; 13h:05m:19s remains)
INFO - root - 2017-12-16 09:50:08.007412: step 42240, loss = 0.47, batch loss = 0.21 (48.4 examples/sec; 0.165 sec/batch; 13h:20m:24s remains)
INFO - root - 2017-12-16 09:50:09.663836: step 42250, loss = 0.62, batch loss = 0.36 (48.1 examples/sec; 0.166 sec/batch; 13h:25m:19s remains)
INFO - root - 2017-12-16 09:50:11.286681: step 42260, loss = 0.59, batch loss = 0.33 (48.7 examples/sec; 0.164 sec/batch; 13h:14m:11s remains)
INFO - root - 2017-12-16 09:50:12.930312: step 42270, loss = 0.55, batch loss = 0.29 (46.2 examples/sec; 0.173 sec/batch; 13h:57m:36s remains)
INFO - root - 2017-12-16 09:50:14.559124: step 42280, loss = 0.49, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 13h:35m:43s remains)
INFO - root - 2017-12-16 09:50:16.182136: step 42290, loss = 0.52, batch loss = 0.26 (49.4 examples/sec; 0.162 sec/batch; 13h:03m:38s remains)
INFO - root - 2017-12-16 09:50:17.827863: step 42300, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:22m:32s remains)
2017-12-16 09:50:18.249470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9997939 -1.8386898 -1.7763193 -1.8032302 -1.8587554 -2.0180037 -2.2701838 -2.60653 -2.8076985 -2.6489832 -2.3294709 -2.0911124 -2.0168595 -1.9514261 -2.0050905][-2.3754439 -2.1395941 -2.0139523 -1.9968047 -2.0622861 -2.2653542 -2.5328991 -2.8087184 -2.9555862 -2.8249087 -2.5739338 -2.3700643 -2.311295 -2.3064234 -2.3763161][-2.7398965 -2.455713 -2.2916212 -2.2301149 -2.2385867 -2.3649409 -2.5089426 -2.658206 -2.7313595 -2.6548018 -2.5240602 -2.3962309 -2.2754695 -2.2663245 -2.3807149][-2.8215129 -2.5401108 -2.3300478 -2.224803 -2.1486382 -2.0843756 -2.0040388 -1.9772038 -1.999223 -2.0199957 -2.0253847 -1.9900482 -1.8635948 -1.86866 -2.0181885][-2.3378901 -2.0843749 -1.8647219 -1.753883 -1.6077905 -1.3290082 -0.97881424 -0.80726552 -0.86494911 -1.0100182 -1.1258869 -1.1822348 -1.0708685 -1.0822182 -1.2770038][-1.3352048 -1.0695407 -0.8959105 -0.84690905 -0.66218543 -0.22880435 0.31291056 0.55349445 0.396286 0.080927134 -0.17341328 -0.2645843 -0.1523838 -0.1952033 -0.45206547][0.055136681 0.27694154 0.34233046 0.27199817 0.40317631 0.88906145 1.5688708 1.8577969 1.5005558 1.0022304 0.69221234 0.61784816 0.68323779 0.55070543 0.17729878][1.1883063 1.2858949 1.1542127 0.90837932 0.9398427 1.3688796 2.1525147 2.497957 1.9694784 1.470062 1.321665 1.3676753 1.3687682 1.0651534 0.51446986][1.3476784 1.3112235 0.96685004 0.54238939 0.48670053 0.85120082 1.4943445 1.8099115 1.5431411 1.3496339 1.4854736 1.6530511 1.5536289 1.0983434 0.41349602][0.39790535 0.29861307 -0.042778969 -0.5011425 -0.58984482 -0.20769453 0.34667468 0.67169333 0.71475029 0.90319705 1.2555907 1.460392 1.3104901 0.7845614 0.081056595][-1.0202622 -1.0943362 -1.3354489 -1.6480157 -1.611081 -1.1697056 -0.62306046 -0.24313784 0.036608219 0.4996407 0.96007991 1.1724453 1.0150571 0.47917366 -0.28674054][-2.2352445 -2.3028429 -2.3857231 -2.4960773 -2.2850947 -1.7513874 -1.2020246 -0.81266832 -0.43875182 0.10946918 0.553849 0.716856 0.50296521 -0.097613096 -0.84842062][-2.8740065 -2.9090376 -2.8731952 -2.8295367 -2.5146925 -1.9477048 -1.4158552 -1.0762277 -0.78998661 -0.31168222 0.026524782 0.065648794 -0.33391166 -0.97556686 -1.5981163][-2.74213 -2.833024 -2.7957356 -2.6808422 -2.3357189 -1.7955995 -1.2725898 -0.959934 -0.76000381 -0.47463691 -0.35894847 -0.54465687 -1.0943617 -1.6980603 -2.1428311][-2.2537627 -2.4256959 -2.4138467 -2.2756672 -1.9532402 -1.4535472 -0.95770371 -0.66389871 -0.53232145 -0.43857098 -0.61120594 -0.99171937 -1.4805137 -1.9532986 -2.2205751]]...]
INFO - root - 2017-12-16 09:50:19.901729: step 42310, loss = 0.49, batch loss = 0.23 (46.9 examples/sec; 0.170 sec/batch; 13h:44m:33s remains)
INFO - root - 2017-12-16 09:50:21.538890: step 42320, loss = 0.58, batch loss = 0.32 (47.5 examples/sec; 0.169 sec/batch; 13h:34m:57s remains)
INFO - root - 2017-12-16 09:50:23.186283: step 42330, loss = 0.54, batch loss = 0.28 (49.4 examples/sec; 0.162 sec/batch; 13h:03m:01s remains)
INFO - root - 2017-12-16 09:50:24.822715: step 42340, loss = 0.55, batch loss = 0.29 (50.7 examples/sec; 0.158 sec/batch; 12h:43m:38s remains)
INFO - root - 2017-12-16 09:50:26.448647: step 42350, loss = 0.58, batch loss = 0.32 (49.5 examples/sec; 0.162 sec/batch; 13h:01m:55s remains)
INFO - root - 2017-12-16 09:50:28.085743: step 42360, loss = 0.43, batch loss = 0.18 (46.9 examples/sec; 0.171 sec/batch; 13h:44m:59s remains)
INFO - root - 2017-12-16 09:50:29.740698: step 42370, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 13h:20m:21s remains)
INFO - root - 2017-12-16 09:50:31.381110: step 42380, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:18m:52s remains)
INFO - root - 2017-12-16 09:50:33.012604: step 42390, loss = 0.49, batch loss = 0.23 (50.3 examples/sec; 0.159 sec/batch; 12h:48m:43s remains)
INFO - root - 2017-12-16 09:50:34.636276: step 42400, loss = 0.50, batch loss = 0.24 (49.4 examples/sec; 0.162 sec/batch; 13h:03m:18s remains)
2017-12-16 09:50:35.078519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9379148 -1.9279537 -2.0823493 -2.1298046 -1.9457362 -1.8843017 -2.2183235 -2.7409208 -3.1434658 -3.2860625 -3.3366394 -3.3561084 -3.3655472 -3.3472574 -3.3769569][-1.9499218 -2.0077164 -2.2472024 -2.3900466 -2.2949162 -2.2463386 -2.4731622 -2.8170996 -3.0760121 -3.2109408 -3.2514541 -3.261023 -3.2600343 -3.1991272 -3.1594265][-2.1398094 -2.3006473 -2.5925226 -2.7670317 -2.7003341 -2.5874383 -2.63621 -2.7491632 -2.8375404 -2.892292 -2.9148924 -2.9044006 -2.7752287 -2.4507072 -2.11715][-2.45344 -2.6209691 -2.7976635 -2.7958174 -2.5412221 -2.2054193 -2.0475209 -2.0499783 -2.0844834 -2.0798426 -2.1259522 -2.1636884 -1.8931847 -1.117328 -0.27233338][-2.6006005 -2.6936648 -2.6469913 -2.2531071 -1.4907739 -0.72719359 -0.39655042 -0.44893694 -0.66211581 -0.83975506 -1.0449942 -1.2745991 -1.0008103 0.086410046 1.375123][-2.3488636 -2.2941456 -1.9873927 -1.1089056 0.32059979 1.6614697 2.0707605 1.6402333 0.88448524 0.20789742 -0.364388 -0.85735106 -0.75054038 0.22309875 1.5214419][-1.6141858 -1.4151304 -0.90298009 0.38533306 2.5320442 4.5721264 4.9397869 3.7572882 2.1159718 0.61429 -0.48988116 -1.175607 -1.1832626 -0.49860561 0.51286077][-0.74932718 -0.50338542 0.014616013 1.4385831 4.0328503 6.6378136 6.8174582 4.8765278 2.5173252 0.28575134 -1.3104393 -2.1161501 -2.2149172 -1.7751315 -1.0919884][-0.18020129 -0.066035986 0.27230525 1.4149208 3.5081723 5.4118462 5.4268084 3.7894375 1.5952218 -0.68833613 -2.2934127 -3.0152507 -3.091414 -2.802413 -2.3600874][-0.21858931 -0.32842851 -0.22826815 0.4426322 1.7207129 2.88014 2.9164479 1.874459 0.23691559 -1.5934579 -2.8602839 -3.4469452 -3.5856414 -3.4558175 -3.1892965][-0.82411695 -1.131156 -1.256934 -1.0397397 -0.48081744 0.081011057 0.16365719 -0.25934434 -1.1337603 -2.2655137 -3.0019135 -3.323885 -3.4560227 -3.4510708 -3.4146991][-1.5875663 -1.985884 -2.2570102 -2.3502228 -2.2727456 -2.1192555 -2.0184765 -1.9719937 -2.1514316 -2.5634522 -2.7678161 -2.7665846 -2.7762289 -2.8284733 -2.9661229][-2.1564629 -2.5537319 -2.8637707 -3.0905404 -3.20325 -3.190382 -3.0277121 -2.674222 -2.4300892 -2.4453161 -2.391542 -2.1937268 -2.1002164 -2.1294365 -2.2796414][-2.3373938 -2.6166408 -2.8620582 -3.0674667 -3.2090974 -3.2254171 -3.0165148 -2.5541921 -2.195744 -2.1404095 -2.1194665 -1.957553 -1.8470922 -1.8809898 -2.0091248][-2.2126262 -2.3309078 -2.44725 -2.5564232 -2.6353772 -2.6336372 -2.4303756 -2.0358238 -1.821945 -1.9297103 -2.0705361 -2.0452757 -2.0028036 -2.0686178 -2.2017145]]...]
INFO - root - 2017-12-16 09:50:36.746856: step 42410, loss = 0.59, batch loss = 0.33 (50.1 examples/sec; 0.160 sec/batch; 12h:52m:15s remains)
INFO - root - 2017-12-16 09:50:38.377188: step 42420, loss = 0.52, batch loss = 0.26 (50.4 examples/sec; 0.159 sec/batch; 12h:46m:44s remains)
INFO - root - 2017-12-16 09:50:40.027249: step 42430, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 13h:23m:45s remains)
INFO - root - 2017-12-16 09:50:41.686769: step 42440, loss = 0.58, batch loss = 0.32 (47.7 examples/sec; 0.168 sec/batch; 13h:30m:57s remains)
INFO - root - 2017-12-16 09:50:43.313980: step 42450, loss = 0.62, batch loss = 0.36 (48.8 examples/sec; 0.164 sec/batch; 13h:12m:40s remains)
INFO - root - 2017-12-16 09:50:44.932019: step 42460, loss = 0.57, batch loss = 0.31 (48.9 examples/sec; 0.164 sec/batch; 13h:11m:09s remains)
INFO - root - 2017-12-16 09:50:46.577790: step 42470, loss = 0.48, batch loss = 0.23 (48.0 examples/sec; 0.167 sec/batch; 13h:24m:51s remains)
INFO - root - 2017-12-16 09:50:48.205528: step 42480, loss = 0.58, batch loss = 0.32 (50.3 examples/sec; 0.159 sec/batch; 12h:49m:25s remains)
INFO - root - 2017-12-16 09:50:49.862957: step 42490, loss = 0.54, batch loss = 0.28 (47.3 examples/sec; 0.169 sec/batch; 13h:37m:01s remains)
INFO - root - 2017-12-16 09:50:51.499533: step 42500, loss = 0.53, batch loss = 0.27 (49.5 examples/sec; 0.161 sec/batch; 13h:00m:30s remains)
2017-12-16 09:50:51.944751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9006221 -1.9287395 -1.966692 -2.0146089 -2.0680523 -2.1254346 -2.1842039 -2.2428422 -2.3055708 -2.3276811 -2.2573817 -2.1593637 -2.1561849 -2.2648938 -2.4065745][-2.1320238 -2.166929 -2.217423 -2.2787838 -2.3451383 -2.4326348 -2.5340016 -2.6354668 -2.7309282 -2.7448649 -2.5905533 -2.3766749 -2.2848866 -2.3179 -2.3834476][-2.7015233 -2.7335711 -2.7719705 -2.8043158 -2.8323565 -2.8835309 -2.9643641 -3.0369852 -3.0784826 -2.9944959 -2.6829381 -2.317749 -2.1024871 -2.013242 -1.9908829][-3.3147736 -3.3540368 -3.3825305 -3.3543692 -3.2738552 -3.1579709 -3.090354 -3.0505459 -2.9478076 -2.6490984 -2.1566014 -1.7172329 -1.4704831 -1.3469739 -1.3052343][-3.5984459 -3.5598414 -3.5189905 -3.4087236 -3.1771507 -2.8535109 -2.6496158 -2.5584466 -2.372659 -1.8951192 -1.2603055 -0.82379985 -0.67155957 -0.68983829 -0.7972852][-3.4092472 -3.2216363 -3.059778 -2.8682137 -2.545963 -2.101733 -1.8463848 -1.7704413 -1.5353589 -0.95013082 -0.31672668 0.0025691986 -0.0080394745 -0.27340174 -0.64110005][-2.7119672 -2.4007788 -2.1540256 -1.899838 -1.5294012 -1.0582623 -0.84122694 -0.82657719 -0.60510516 -0.035094023 0.39953065 0.50712109 0.32358336 -0.21856236 -0.83454823][-1.4150449 -1.0638988 -0.76776564 -0.46801627 -0.11201429 0.22420716 0.27410603 0.095868111 0.16182208 0.47170997 0.57122111 0.42592072 0.086978912 -0.59701812 -1.3595803][0.22346902 0.4973104 0.72139788 1.0295014 1.2809989 1.4241762 1.1815102 0.6808877 0.34139633 0.20910597 0.044276714 -0.20019722 -0.60702419 -1.2850865 -1.9894476][1.2654867 1.4884701 1.6773379 1.9909327 2.0649931 1.8979867 1.4104249 0.69632745 -0.0074751377 -0.46724892 -0.7493459 -0.96299076 -1.2557334 -1.7825605 -2.3621602][1.2468135 1.3878949 1.6189725 1.9880731 2.0238392 1.6741254 1.1829066 0.49223423 -0.3594923 -1.0253382 -1.3668684 -1.4536538 -1.5448233 -1.8782558 -2.3094559][0.95791841 0.88011265 1.0807953 1.5614157 1.69506 1.3323476 0.90119481 0.39125729 -0.44284928 -1.1749157 -1.4821113 -1.4630699 -1.4038043 -1.6065876 -1.8940928][1.0785575 0.8014648 0.99696493 1.6284115 1.8404963 1.5353944 1.1305132 0.74776578 -0.078863382 -0.84463978 -1.178835 -1.1424592 -1.0002491 -1.1199567 -1.3332099][1.2481136 0.95128322 1.258976 2.0367043 2.3608 2.0026309 1.4111307 0.86977816 0.08907938 -0.660835 -0.97396696 -0.90204966 -0.73894584 -0.84779608 -1.0870879][1.1182616 0.94422317 1.3753178 2.3125422 2.7072346 2.2125504 1.3793139 0.62360787 -0.21334291 -0.91093469 -1.180477 -1.1487911 -0.9651413 -1.0350143 -1.2644719]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-42500 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:50:53.972703: step 42510, loss = 0.48, batch loss = 0.22 (51.1 examples/sec; 0.156 sec/batch; 12h:35m:55s remains)
INFO - root - 2017-12-16 09:50:55.649706: step 42520, loss = 0.59, batch loss = 0.33 (48.5 examples/sec; 0.165 sec/batch; 13h:17m:14s remains)
INFO - root - 2017-12-16 09:50:57.302332: step 42530, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 13h:16m:22s remains)
INFO - root - 2017-12-16 09:50:58.933102: step 42540, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:18m:24s remains)
INFO - root - 2017-12-16 09:51:00.582131: step 42550, loss = 0.50, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:14m:37s remains)
INFO - root - 2017-12-16 09:51:02.200807: step 42560, loss = 0.58, batch loss = 0.32 (50.0 examples/sec; 0.160 sec/batch; 12h:53m:30s remains)
INFO - root - 2017-12-16 09:51:03.826831: step 42570, loss = 0.54, batch loss = 0.28 (49.0 examples/sec; 0.163 sec/batch; 13h:08m:53s remains)
INFO - root - 2017-12-16 09:51:05.495670: step 42580, loss = 0.53, batch loss = 0.27 (48.5 examples/sec; 0.165 sec/batch; 13h:17m:04s remains)
INFO - root - 2017-12-16 09:51:07.140164: step 42590, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 13h:14m:55s remains)
INFO - root - 2017-12-16 09:51:08.784621: step 42600, loss = 0.52, batch loss = 0.26 (44.7 examples/sec; 0.179 sec/batch; 14h:23m:56s remains)
2017-12-16 09:51:09.198257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3602912 -3.5994918 -3.7183867 -3.6433303 -3.4210379 -3.1497269 -2.9900913 -2.9595537 -3.031019 -2.9268904 -2.47228 -1.7658859 -0.89403212 0.0021748543 0.611248][-3.8509507 -4.0848045 -4.1818676 -4.1094012 -3.8800869 -3.4775991 -3.1189554 -2.9491124 -2.997155 -3.0161386 -2.7571721 -2.27618 -1.6028485 -0.84093654 -0.24658751][-3.9364371 -4.0980825 -4.1014767 -3.9504442 -3.6483588 -3.0803237 -2.5212862 -2.3040304 -2.4755101 -2.7749147 -2.8715625 -2.7485 -2.4091496 -1.8981743 -1.3981562][-3.7457287 -3.820498 -3.6789005 -3.405828 -3.0020947 -2.2421911 -1.5040486 -1.2963809 -1.6382816 -2.2318072 -2.7180495 -2.962487 -2.9844363 -2.7479451 -2.3903518][-3.5241423 -3.5341196 -3.2884221 -2.9161677 -2.3724151 -1.4294672 -0.59392083 -0.46809053 -0.95846939 -1.6952683 -2.3789771 -2.8578134 -3.1073878 -3.1087184 -2.9203911][-3.4337957 -3.3968496 -3.0778074 -2.5949931 -1.8479249 -0.73011136 0.067891836 0.01589179 -0.66368496 -1.4582579 -2.1359181 -2.5995886 -2.8787498 -2.993217 -2.9917355][-3.4302673 -3.3507853 -2.9882507 -2.3573556 -1.3052373 0.041015387 0.78692627 0.49131346 -0.40967166 -1.3140671 -2.0034478 -2.3642895 -2.5281467 -2.6269023 -2.7439721][-3.4284451 -3.3067622 -2.9435639 -2.2129903 -0.91949534 0.62883639 1.3725164 0.91552019 -0.14264703 -1.2142372 -1.978081 -2.2555082 -2.2874827 -2.3177903 -2.4796743][-3.3833003 -3.2319565 -2.9377651 -2.2320843 -0.88018394 0.74543786 1.4822657 0.98602915 -0.13317299 -1.3207155 -2.1591773 -2.413903 -2.3467386 -2.2747889 -2.38945][-3.3211946 -3.167398 -2.9831355 -2.4068911 -1.12633 0.45688987 1.1528771 0.66579628 -0.44267571 -1.6639242 -2.5315104 -2.7902386 -2.6769795 -2.477685 -2.4482832][-3.2722733 -3.175415 -3.1015348 -2.69227 -1.5934963 -0.16791677 0.5001986 0.077625275 -0.91226268 -2.0512366 -2.8823619 -3.1615193 -3.0341716 -2.7435112 -2.5561998][-3.2579458 -3.2579513 -3.3008111 -3.0898244 -2.2599277 -1.1250809 -0.54306948 -0.78502965 -1.5228825 -2.4340384 -3.0959325 -3.3355794 -3.2117994 -2.920805 -2.6721673][-3.2443824 -3.3328671 -3.4595447 -3.434659 -2.9425173 -2.1881974 -1.7514749 -1.8328378 -2.2333121 -2.8101497 -3.1994207 -3.3405504 -3.2452173 -3.022198 -2.7993889][-3.2170687 -3.3733768 -3.5588202 -3.6615019 -3.4381557 -3.0232909 -2.728862 -2.6873183 -2.8065019 -3.0520415 -3.2119553 -3.2550976 -3.1849523 -3.055783 -2.9139493][-3.1539063 -3.3380921 -3.5557437 -3.7393727 -3.72246 -3.5636921 -3.3851385 -3.258764 -3.1878364 -3.2028708 -3.1986082 -3.1653762 -3.1203272 -3.0585885 -2.9902887]]...]
INFO - root - 2017-12-16 09:51:10.822356: step 42610, loss = 0.62, batch loss = 0.36 (49.3 examples/sec; 0.162 sec/batch; 13h:03m:41s remains)
INFO - root - 2017-12-16 09:51:12.509268: step 42620, loss = 0.45, batch loss = 0.19 (46.3 examples/sec; 0.173 sec/batch; 13h:54m:28s remains)
INFO - root - 2017-12-16 09:51:14.176386: step 42630, loss = 0.49, batch loss = 0.23 (50.7 examples/sec; 0.158 sec/batch; 12h:42m:55s remains)
INFO - root - 2017-12-16 09:51:15.807759: step 42640, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:19m:12s remains)
INFO - root - 2017-12-16 09:51:17.459550: step 42650, loss = 0.60, batch loss = 0.34 (46.7 examples/sec; 0.171 sec/batch; 13h:47m:33s remains)
INFO - root - 2017-12-16 09:51:19.121421: step 42660, loss = 0.56, batch loss = 0.30 (50.9 examples/sec; 0.157 sec/batch; 12h:39m:53s remains)
INFO - root - 2017-12-16 09:51:20.736008: step 42670, loss = 0.53, batch loss = 0.27 (51.0 examples/sec; 0.157 sec/batch; 12h:37m:38s remains)
INFO - root - 2017-12-16 09:51:22.350802: step 42680, loss = 0.50, batch loss = 0.25 (49.1 examples/sec; 0.163 sec/batch; 13h:06m:39s remains)
INFO - root - 2017-12-16 09:51:23.979038: step 42690, loss = 0.49, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 13h:10m:23s remains)
INFO - root - 2017-12-16 09:51:25.624553: step 42700, loss = 0.72, batch loss = 0.46 (48.1 examples/sec; 0.166 sec/batch; 13h:23m:34s remains)
2017-12-16 09:51:26.058529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1052176 -1.1823893 -1.2641762 -1.2992924 -1.2890165 -1.2556039 -1.1710094 -1.0808917 -1.0709106 -1.1781524 -1.3759748 -1.5692642 -1.6301064 -1.518827 -1.296995][-1.1771482 -1.2981679 -1.3984519 -1.4265227 -1.3767787 -1.2902924 -1.1410556 -0.96977568 -0.92740476 -1.1135429 -1.498353 -1.8870262 -2.0821977 -1.9932877 -1.683146][-1.2173545 -1.3587571 -1.450747 -1.4469256 -1.3401791 -1.1751295 -0.95188081 -0.70618689 -0.66094434 -0.97549093 -1.5756913 -2.1836176 -2.5002737 -2.4380183 -2.0627322][-1.2257197 -1.3592285 -1.4201262 -1.3751931 -1.2126609 -0.94451928 -0.6027633 -0.28409624 -0.26851678 -0.70706606 -1.5215423 -2.3370709 -2.7963824 -2.8026898 -2.4093037][-1.2206056 -1.3472495 -1.3713425 -1.2698677 -1.0409179 -0.6705935 -0.17612696 0.25544524 0.32056236 -0.13325644 -1.0716115 -2.0896142 -2.7668529 -2.9227765 -2.618386][-1.2219099 -1.3518159 -1.332415 -1.1893777 -0.92386007 -0.46514177 0.23500204 0.937876 1.2525947 0.921551 -0.059939861 -1.3147299 -2.2779281 -2.6734047 -2.5542107][-1.2215558 -1.359684 -1.326244 -1.1605984 -0.83204603 -0.16991019 0.82562757 1.8400414 2.4147394 2.1248362 1.0411503 -0.42617726 -1.6468091 -2.293011 -2.3997276][-1.2644728 -1.4206879 -1.377604 -1.1286889 -0.65048814 0.22632408 1.4194152 2.5876029 3.2873948 3.0041535 1.8162801 0.2452364 -1.1043667 -1.932837 -2.2432775][-1.3171182 -1.4876703 -1.4173303 -1.0962752 -0.541944 0.349164 1.5231686 2.6669996 3.3836725 3.1296875 1.9605548 0.45036769 -0.8728559 -1.759892 -2.1829681][-1.3354893 -1.5184721 -1.4795988 -1.1766667 -0.67493546 0.046909571 1.0076902 2.0012367 2.610924 2.3238976 1.2229824 -0.11873198 -1.2608482 -2.0202007 -2.3162384][-1.3264565 -1.521059 -1.5404272 -1.3455825 -0.98723662 -0.47713315 0.20619178 0.93232036 1.3459249 0.98347664 -0.013994932 -1.1019031 -1.9309587 -2.3473761 -2.3630819][-1.2772636 -1.4784727 -1.5644529 -1.4928784 -1.2813319 -0.99216604 -0.64967883 -0.27684665 -0.074563026 -0.35885477 -1.0383643 -1.7198657 -2.1553149 -2.2407057 -2.0573754][-1.1952413 -1.3578079 -1.4729493 -1.4876618 -1.4103322 -1.3445299 -1.3015382 -1.2177256 -1.1437001 -1.2561059 -1.5301673 -1.7817469 -1.8828706 -1.7836865 -1.5503707][-1.1334478 -1.2204472 -1.3095063 -1.3631197 -1.4022157 -1.4848063 -1.5766641 -1.598074 -1.5587385 -1.5410519 -1.5762573 -1.5863317 -1.5222254 -1.378456 -1.1858034][-1.1273994 -1.1550444 -1.1833029 -1.2104324 -1.2620302 -1.3642846 -1.4783121 -1.5268722 -1.516757 -1.4795301 -1.4519238 -1.4024053 -1.308695 -1.1774685 -1.0378615]]...]
INFO - root - 2017-12-16 09:51:27.689398: step 42710, loss = 0.50, batch loss = 0.24 (50.0 examples/sec; 0.160 sec/batch; 12h:52m:00s remains)
INFO - root - 2017-12-16 09:51:29.355888: step 42720, loss = 0.55, batch loss = 0.29 (48.1 examples/sec; 0.166 sec/batch; 13h:22m:34s remains)
INFO - root - 2017-12-16 09:51:30.996564: step 42730, loss = 0.51, batch loss = 0.25 (49.0 examples/sec; 0.163 sec/batch; 13h:08m:45s remains)
INFO - root - 2017-12-16 09:51:32.629485: step 42740, loss = 0.48, batch loss = 0.22 (49.3 examples/sec; 0.162 sec/batch; 13h:04m:09s remains)
INFO - root - 2017-12-16 09:51:34.299402: step 42750, loss = 0.49, batch loss = 0.23 (50.0 examples/sec; 0.160 sec/batch; 12h:52m:42s remains)
INFO - root - 2017-12-16 09:51:35.945099: step 42760, loss = 0.63, batch loss = 0.37 (49.5 examples/sec; 0.162 sec/batch; 13h:00m:01s remains)
INFO - root - 2017-12-16 09:51:37.571849: step 42770, loss = 0.71, batch loss = 0.45 (48.9 examples/sec; 0.164 sec/batch; 13h:10m:16s remains)
INFO - root - 2017-12-16 09:51:39.257810: step 42780, loss = 0.50, batch loss = 0.25 (48.7 examples/sec; 0.164 sec/batch; 13h:13m:04s remains)
INFO - root - 2017-12-16 09:51:40.876546: step 42790, loss = 0.57, batch loss = 0.31 (49.5 examples/sec; 0.162 sec/batch; 13h:00m:32s remains)
INFO - root - 2017-12-16 09:51:42.490104: step 42800, loss = 0.65, batch loss = 0.40 (48.9 examples/sec; 0.164 sec/batch; 13h:09m:26s remains)
2017-12-16 09:51:42.943102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1630321 -1.832322 -1.2583656 -0.74640131 -1.0855024 -2.0406818 -2.942986 -3.6354394 -4.2112713 -4.3221016 -3.5585086 -2.4603069 -1.9211909 -1.9620684 -2.2108009][-3.095823 -2.7269771 -2.0304227 -1.3798506 -1.601902 -2.3947105 -3.1082683 -3.7853284 -4.4498343 -4.6126008 -3.8628902 -2.715138 -1.9431362 -1.7158122 -1.9402853][-3.608705 -3.3230891 -2.6914098 -2.090405 -2.1753852 -2.6395996 -2.9997962 -3.495626 -4.0712547 -4.157156 -3.4208341 -2.2422771 -1.1646852 -0.50075889 -0.58455896][-3.6236882 -3.4980459 -3.0426762 -2.5475125 -2.477339 -2.5050111 -2.340883 -2.5334325 -3.0190537 -3.1307561 -2.5076082 -1.3524415 -0.047776937 0.99526024 1.1317139][-3.2339706 -3.2899284 -3.0429087 -2.6459894 -2.3701422 -1.8760719 -1.134392 -0.96040964 -1.4051226 -1.827116 -1.6584135 -0.88985264 0.32131577 1.5406008 1.993252][-2.7164583 -2.8859725 -2.8081212 -2.5178003 -2.0453246 -1.1020212 0.15160036 0.80015731 0.44275069 -0.46580672 -1.0220895 -0.90316153 -0.083203793 1.0917833 1.7846367][-2.0789149 -2.2923808 -2.375762 -2.2065933 -1.6470747 -0.45078921 1.2804563 2.4777343 2.1861074 0.68259573 -0.65066326 -1.1640561 -0.78258288 0.1973567 0.92456007][-1.5706787 -1.7961149 -2.0369754 -2.0619414 -1.5943055 -0.40379691 1.5044765 2.9923604 2.7565076 1.0226967 -0.6825372 -1.6150568 -1.6428828 -0.88842666 -0.1559267][-1.3900909 -1.6451483 -2.0475631 -2.2987995 -2.0739369 -1.0844991 0.61723185 2.0096271 1.8849189 0.44992852 -1.1700807 -2.2123289 -2.3820119 -1.7642176 -1.063526][-1.281163 -1.5572026 -2.0809388 -2.5710564 -2.6922791 -2.1118464 -0.83609223 0.31947517 0.39782906 -0.49721956 -1.6235074 -2.4312265 -2.6226516 -2.1062388 -1.4449308][-0.92386055 -1.2317785 -1.8473392 -2.5253937 -2.9858983 -2.8244071 -2.0018394 -1.1456273 -0.88682568 -1.1802052 -1.6734595 -2.0975983 -2.1714458 -1.7354796 -1.1521697][-0.4224242 -0.78672683 -1.4442084 -2.1714199 -2.7236171 -2.7572627 -2.3358574 -1.8651989 -1.6678255 -1.6025381 -1.6242005 -1.6704326 -1.5819852 -1.2042642 -0.71334863][-0.057848692 -0.46800816 -1.0710336 -1.6429557 -2.0277159 -2.0216846 -1.8031186 -1.6660199 -1.721805 -1.7198156 -1.6157143 -1.4853108 -1.2875873 -0.88823092 -0.43596661][-0.048701048 -0.41571641 -0.82603681 -1.0735512 -1.0984477 -0.86035538 -0.63246381 -0.72778761 -1.0783439 -1.3578193 -1.4335601 -1.3456298 -1.1510543 -0.80601168 -0.4516623][-0.45313632 -0.66627073 -0.82600355 -0.74779952 -0.43063366 0.078166962 0.45298195 0.35490084 -0.16714668 -0.71189833 -0.99964738 -1.0490906 -0.95270216 -0.75842607 -0.57101023]]...]
INFO - root - 2017-12-16 09:51:44.578523: step 42810, loss = 0.53, batch loss = 0.27 (48.5 examples/sec; 0.165 sec/batch; 13h:16m:02s remains)
INFO - root - 2017-12-16 09:51:46.211463: step 42820, loss = 0.59, batch loss = 0.33 (47.3 examples/sec; 0.169 sec/batch; 13h:36m:43s remains)
INFO - root - 2017-12-16 09:51:47.860367: step 42830, loss = 0.52, batch loss = 0.26 (49.0 examples/sec; 0.163 sec/batch; 13h:08m:15s remains)
INFO - root - 2017-12-16 09:51:49.485552: step 42840, loss = 0.50, batch loss = 0.24 (50.7 examples/sec; 0.158 sec/batch; 12h:42m:17s remains)
INFO - root - 2017-12-16 09:51:51.127677: step 42850, loss = 0.55, batch loss = 0.29 (49.0 examples/sec; 0.163 sec/batch; 13h:07m:25s remains)
INFO - root - 2017-12-16 09:51:52.766203: step 42860, loss = 0.49, batch loss = 0.23 (48.4 examples/sec; 0.165 sec/batch; 13h:17m:46s remains)
INFO - root - 2017-12-16 09:51:54.402553: step 42870, loss = 0.64, batch loss = 0.38 (48.1 examples/sec; 0.166 sec/batch; 13h:22m:10s remains)
INFO - root - 2017-12-16 09:51:56.047831: step 42880, loss = 0.50, batch loss = 0.25 (48.8 examples/sec; 0.164 sec/batch; 13h:11m:30s remains)
INFO - root - 2017-12-16 09:51:57.669229: step 42890, loss = 0.56, batch loss = 0.30 (49.0 examples/sec; 0.163 sec/batch; 13h:07m:56s remains)
INFO - root - 2017-12-16 09:51:59.288672: step 42900, loss = 0.50, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 13h:07m:20s remains)
2017-12-16 09:51:59.738560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.387898 -2.3644488 -2.3291633 -2.3153098 -2.2677908 -2.1669235 -2.037781 -1.9163746 -1.8336139 -1.7921486 -1.7778211 -1.7741324 -1.7728741 -1.7731154 -1.7870085][-2.8280649 -2.8541174 -2.8641703 -2.9291847 -2.9486501 -2.8384705 -2.6485829 -2.400913 -2.2124455 -2.1018424 -2.0528052 -2.0239334 -1.993935 -1.9589709 -1.9445939][-3.2219124 -3.262177 -3.2857981 -3.4184482 -3.5137284 -3.4202762 -3.2285728 -2.9583547 -2.7224452 -2.5833352 -2.4952896 -2.4332716 -2.3598714 -2.2727039 -2.1920476][-3.3465052 -3.383378 -3.3685868 -3.4604716 -3.5251131 -3.42142 -3.2760007 -3.1141825 -2.999424 -2.912863 -2.79055 -2.6919246 -2.6129313 -2.4915423 -2.3401122][-3.0070634 -3.0013485 -2.8951473 -2.9410093 -2.9326172 -2.7541175 -2.6449456 -2.613049 -2.6708961 -2.7745426 -2.7523196 -2.6206975 -2.4917302 -2.3484709 -2.1914344][-2.4336433 -2.252322 -2.03048 -1.9862765 -1.9450098 -1.700163 -1.5828443 -1.6161046 -1.8581108 -2.2066526 -2.3151913 -2.2480121 -2.1613228 -2.0363755 -1.8658798][-2.1476703 -1.6290107 -1.0940992 -0.8480463 -0.66746056 -0.3970288 -0.24218416 -0.23719478 -0.58038604 -1.1418766 -1.4639888 -1.6289682 -1.7381951 -1.7323372 -1.5838382][-2.3283985 -1.5649452 -0.55990791 0.058659077 0.46060467 0.76204538 0.95014381 1.0088222 0.73414683 0.16029143 -0.37502789 -0.87713933 -1.2843349 -1.4219183 -1.3109547][-2.7240641 -1.9735919 -0.79263842 0.229954 0.96327424 1.4735022 1.6874549 1.6840951 1.3916478 0.87227726 0.25344443 -0.49452293 -1.1674875 -1.4190826 -1.3383695][-2.9010139 -2.3097889 -1.3073186 -0.33346748 0.54498458 1.1931734 1.4889777 1.5025644 1.1784487 0.65241766 0.0078244209 -0.83253384 -1.6018772 -1.8832101 -1.7444732][-2.861083 -2.4162207 -1.7173252 -1.1558489 -0.64563131 -0.087408781 0.33212686 0.44314337 0.18414283 -0.31307769 -0.93808317 -1.6815981 -2.243016 -2.3616915 -2.1573229][-2.6828175 -2.3297303 -1.8496419 -1.7289486 -1.6739187 -1.3673764 -1.0445087 -0.988075 -1.2237575 -1.5877649 -1.9912428 -2.4023917 -2.5947444 -2.5422318 -2.3355184][-2.5261407 -2.1355729 -1.7672997 -1.9446604 -2.3030307 -2.2799439 -2.1101122 -2.0788841 -2.2423315 -2.493427 -2.6658902 -2.7134185 -2.5654974 -2.3121459 -2.1334009][-2.4352396 -1.9635973 -1.6206176 -1.9583483 -2.5689905 -2.7146893 -2.5317519 -2.4361448 -2.4632998 -2.52149 -2.4193296 -2.1426766 -1.793027 -1.4595104 -1.3287338][-2.2770481 -1.7170779 -1.3412929 -1.6950779 -2.3795469 -2.5611413 -2.3155658 -2.0594213 -1.8527367 -1.6230443 -1.323444 -0.96959794 -0.64803028 -0.40550721 -0.39786708]]...]
INFO - root - 2017-12-16 09:52:01.382939: step 42910, loss = 0.59, batch loss = 0.33 (46.4 examples/sec; 0.172 sec/batch; 13h:51m:53s remains)
INFO - root - 2017-12-16 09:52:03.084298: step 42920, loss = 0.52, batch loss = 0.26 (48.2 examples/sec; 0.166 sec/batch; 13h:20m:45s remains)
INFO - root - 2017-12-16 09:52:04.766278: step 42930, loss = 0.55, batch loss = 0.30 (47.7 examples/sec; 0.168 sec/batch; 13h:28m:50s remains)
INFO - root - 2017-12-16 09:52:06.397022: step 42940, loss = 0.63, batch loss = 0.38 (49.5 examples/sec; 0.162 sec/batch; 12h:59m:29s remains)
INFO - root - 2017-12-16 09:52:08.034706: step 42950, loss = 0.59, batch loss = 0.33 (48.0 examples/sec; 0.167 sec/batch; 13h:23m:32s remains)
INFO - root - 2017-12-16 09:52:09.708816: step 42960, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 13h:02m:31s remains)
INFO - root - 2017-12-16 09:52:11.345797: step 42970, loss = 0.48, batch loss = 0.22 (49.6 examples/sec; 0.161 sec/batch; 12h:57m:46s remains)
INFO - root - 2017-12-16 09:52:12.974644: step 42980, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 13h:07m:55s remains)
INFO - root - 2017-12-16 09:52:14.658605: step 42990, loss = 0.51, batch loss = 0.25 (44.2 examples/sec; 0.181 sec/batch; 14h:33m:16s remains)
INFO - root - 2017-12-16 09:52:16.354614: step 43000, loss = 0.56, batch loss = 0.30 (47.3 examples/sec; 0.169 sec/batch; 13h:35m:42s remains)
2017-12-16 09:52:16.774260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3416737 -1.2547431 -1.2205988 -1.2217255 -1.2088873 -1.1405271 -1.0450629 -0.99899971 -1.0209391 -1.0704182 -1.134557 -1.1862919 -1.2198325 -1.2621371 -1.3417366][-1.0736132 -0.96913946 -0.97609341 -1.0327978 -1.022319 -0.88702095 -0.70461512 -0.62359226 -0.70177794 -0.85509288 -1.0103437 -1.09316 -1.1114745 -1.1323889 -1.2099864][-0.86488473 -0.77210927 -0.85051548 -0.96338 -0.91801822 -0.65235507 -0.3087554 -0.14602399 -0.31220865 -0.64836657 -0.955932 -1.104612 -1.1155421 -1.0988581 -1.1435857][-0.73146927 -0.62971032 -0.7491889 -0.87218761 -0.7498945 -0.2909801 0.30209517 0.59597993 0.32440662 -0.25861335 -0.80240667 -1.1010822 -1.1715224 -1.1565301 -1.1742691][-0.5519129 -0.34401488 -0.40801215 -0.48713028 -0.26162267 0.40226698 1.2540512 1.7358034 1.3846958 0.52010417 -0.3446207 -0.89444363 -1.1276221 -1.1966476 -1.2476275][-0.19077039 0.23800445 0.30954432 0.30689335 0.60138559 1.390758 2.4539406 3.1653898 2.7872703 1.6987526 0.50580668 -0.35272145 -0.83827651 -1.0988348 -1.2705503][0.1729548 0.84332013 1.0640666 1.1285582 1.4542525 2.286216 3.4916346 4.4054022 4.0674086 2.8658049 1.4785833 0.37152958 -0.36010885 -0.82867539 -1.1597885][0.36021471 1.2139821 1.5508027 1.5896211 1.8066003 2.5068409 3.6453092 4.6089039 4.4831171 3.4235127 2.1017654 0.95272422 0.0988574 -0.5087744 -0.97038245][0.093100071 0.92896509 1.293479 1.304739 1.3567119 1.7735083 2.5511715 3.2947853 3.4264114 2.8476474 1.9398224 1.0317178 0.27062464 -0.35053992 -0.88249445][-0.53930724 0.081090212 0.33017969 0.26043391 0.17016983 0.34179473 0.76826024 1.2589092 1.4950063 1.358459 0.95868754 0.43311214 -0.090101719 -0.58660388 -1.0613054][-1.2957711 -0.98591816 -0.94722986 -1.1444101 -1.3475287 -1.3782877 -1.2251825 -0.95029533 -0.70284963 -0.5636034 -0.57751656 -0.71320355 -0.92976749 -1.1810315 -1.4674767][-1.9169667 -1.9037458 -2.0557287 -2.3485363 -2.6433079 -2.8063316 -2.8116839 -2.6903238 -2.4872024 -2.2492006 -2.0368679 -1.9200313 -1.8930056 -1.9046698 -1.9511982][-2.2506902 -2.40845 -2.6573365 -2.9750562 -3.2885535 -3.5160069 -3.6070094 -3.5770655 -3.4376636 -3.1920521 -2.909292 -2.682431 -2.52295 -2.3859556 -2.2659791][-2.3009536 -2.5085552 -2.7647946 -3.0494428 -3.324219 -3.5282359 -3.6246121 -3.6215529 -3.5263591 -3.3253965 -3.0759518 -2.8552067 -2.667469 -2.4833961 -2.312078][-2.164628 -2.3431444 -2.5466857 -2.7558827 -2.9430389 -3.0795295 -3.1482031 -3.1527247 -3.0894272 -2.9535346 -2.781451 -2.6194403 -2.4703624 -2.3163884 -2.1653829]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:52:18.399206: step 43010, loss = 0.55, batch loss = 0.29 (49.2 examples/sec; 0.163 sec/batch; 13h:04m:59s remains)
INFO - root - 2017-12-16 09:52:20.089479: step 43020, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:17m:22s remains)
INFO - root - 2017-12-16 09:52:21.721692: step 43030, loss = 0.55, batch loss = 0.29 (50.5 examples/sec; 0.158 sec/batch; 12h:43m:40s remains)
INFO - root - 2017-12-16 09:52:23.374112: step 43040, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:09m:27s remains)
INFO - root - 2017-12-16 09:52:25.041020: step 43050, loss = 0.49, batch loss = 0.23 (48.2 examples/sec; 0.166 sec/batch; 13h:20m:35s remains)
INFO - root - 2017-12-16 09:52:26.718384: step 43060, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 13h:19m:05s remains)
INFO - root - 2017-12-16 09:52:28.357496: step 43070, loss = 0.56, batch loss = 0.30 (49.6 examples/sec; 0.161 sec/batch; 12h:58m:44s remains)
INFO - root - 2017-12-16 09:52:29.993178: step 43080, loss = 0.59, batch loss = 0.33 (47.4 examples/sec; 0.169 sec/batch; 13h:33m:17s remains)
INFO - root - 2017-12-16 09:52:31.625913: step 43090, loss = 0.56, batch loss = 0.30 (49.7 examples/sec; 0.161 sec/batch; 12h:55m:41s remains)
INFO - root - 2017-12-16 09:52:33.253704: step 43100, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:09m:15s remains)
2017-12-16 09:52:33.665668: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.68896735 -0.4620868 -0.2341609 0.045575142 0.059340715 -0.24272299 -0.32795453 0.038091183 0.52539134 0.9918685 1.3173418 1.0623128 0.40623713 -0.34048271 -0.98402464][-0.88150084 -0.71149516 -0.47152698 -0.17150998 -0.21386933 -0.56139565 -0.62969768 -0.37212133 0.043075085 0.46673918 0.73320889 0.52199841 -0.013293982 -0.6492151 -1.289839][-0.9572171 -0.85672963 -0.56398821 -0.19319749 -0.32435989 -0.82706273 -1.0897313 -1.1035686 -0.9198364 -0.6373173 -0.45291603 -0.63851523 -1.1255897 -1.6448247 -2.1843781][-0.681474 -0.62824976 -0.33536053 0.096406937 -0.068950176 -0.66045964 -1.0605073 -1.1896172 -1.257072 -1.2429452 -1.2828288 -1.5751781 -2.0583408 -2.504447 -2.9277296][-0.15625978 0.0030987263 0.28531456 0.62412405 0.48040271 -0.059494257 -0.404284 -0.45268679 -0.67352474 -0.99404991 -1.3636415 -1.7655818 -2.19129 -2.6157646 -3.0118923][0.26824737 0.49766588 0.72975826 0.9635551 0.89109349 0.6468308 0.68404675 0.90149689 0.50806141 -0.18890095 -0.83513343 -1.290017 -1.6286848 -2.0010991 -2.4584656][0.14201307 0.42012358 0.63156605 0.93695545 1.1555436 1.4355979 1.998204 2.5982568 1.9173162 0.79723668 -0.014642239 -0.41058981 -0.52940404 -0.76315749 -1.2333852][-0.17941451 0.0906055 0.32032704 0.76492763 1.2986 1.9501989 2.8117383 3.5135763 2.7254355 1.5196636 0.85712147 0.801332 1.0815911 1.0205009 0.58198476][-0.62892973 -0.41697156 -0.088335037 0.41744375 1.0127993 1.8175137 2.5382273 2.8345902 2.3267763 1.6278574 1.4915879 1.8783867 2.50647 2.566292 2.0967324][-0.94181895 -0.86674476 -0.57088423 -0.20871234 0.20546508 0.855042 1.3110459 1.4561465 1.3524635 1.3089638 1.6853507 2.3596632 2.9919713 3.0253398 2.6422784][-0.82284462 -0.99298894 -0.94900095 -0.94693422 -0.92877042 -0.62019813 -0.37084532 -0.18789363 0.074523211 0.57825136 1.3507836 2.0786955 2.254288 2.0880353 1.9506471][-0.53882444 -0.98496187 -1.2423527 -1.669662 -2.0804992 -2.1663153 -2.1082668 -1.9536501 -1.5493035 -0.86123455 -0.027712584 0.47328854 0.3152473 0.071474075 0.10496593][-0.28156662 -0.98700285 -1.5721889 -2.2501333 -2.8942239 -3.2278366 -3.3541894 -3.381459 -3.1084552 -2.6256983 -2.0433838 -1.7598341 -1.9076362 -2.0223525 -1.8622591][-0.25087333 -1.066685 -1.7379115 -2.3908768 -2.9639845 -3.3606341 -3.6338763 -3.9475303 -4.0508828 -4.0428638 -3.8690476 -3.7586184 -3.7901433 -3.7276809 -3.450963][-0.30786824 -1.0562247 -1.5402575 -1.9287612 -2.2091489 -2.5176361 -2.9281394 -3.5303571 -4.1009426 -4.566443 -4.808764 -4.7991877 -4.6728678 -4.4346571 -4.0665617]]...]
INFO - root - 2017-12-16 09:52:35.290873: step 43110, loss = 0.53, batch loss = 0.27 (49.3 examples/sec; 0.162 sec/batch; 13h:02m:56s remains)
INFO - root - 2017-12-16 09:52:36.960602: step 43120, loss = 0.48, batch loss = 0.23 (47.4 examples/sec; 0.169 sec/batch; 13h:34m:43s remains)
INFO - root - 2017-12-16 09:52:38.657357: step 43130, loss = 0.47, batch loss = 0.21 (47.9 examples/sec; 0.167 sec/batch; 13h:25m:38s remains)
INFO - root - 2017-12-16 09:52:40.293343: step 43140, loss = 0.51, batch loss = 0.25 (49.8 examples/sec; 0.161 sec/batch; 12h:54m:59s remains)
INFO - root - 2017-12-16 09:52:41.923501: step 43150, loss = 0.53, batch loss = 0.27 (47.9 examples/sec; 0.167 sec/batch; 13h:25m:30s remains)
INFO - root - 2017-12-16 09:52:43.554602: step 43160, loss = 0.57, batch loss = 0.31 (49.4 examples/sec; 0.162 sec/batch; 13h:01m:07s remains)
INFO - root - 2017-12-16 09:52:45.187376: step 43170, loss = 0.59, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 13h:09m:30s remains)
INFO - root - 2017-12-16 09:52:46.802290: step 43180, loss = 0.53, batch loss = 0.27 (50.6 examples/sec; 0.158 sec/batch; 12h:42m:58s remains)
INFO - root - 2017-12-16 09:52:48.436967: step 43190, loss = 0.47, batch loss = 0.21 (49.9 examples/sec; 0.160 sec/batch; 12h:52m:46s remains)
INFO - root - 2017-12-16 09:52:50.078625: step 43200, loss = 0.59, batch loss = 0.33 (45.7 examples/sec; 0.175 sec/batch; 14h:04m:31s remains)
2017-12-16 09:52:50.513491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8142459 -1.8701637 -2.0375853 -2.2706184 -2.3759873 -2.1304605 -1.7369552 -1.6582414 -1.9962026 -2.3736591 -2.5218718 -2.4254115 -2.2015929 -1.9758897 -1.8302187][-1.9763422 -2.1040781 -2.3637137 -2.6556766 -2.6752179 -2.1209652 -1.3655223 -1.2013019 -1.8056219 -2.5409026 -2.8742025 -2.7476542 -2.4263911 -2.0958316 -1.8765552][-2.1000338 -2.3078606 -2.6261408 -2.8991997 -2.7167826 -1.7023921 -0.40169573 0.0020756721 -0.85131919 -2.0548944 -2.7865045 -2.8509483 -2.5415347 -2.1602974 -1.8975933][-2.1591859 -2.426897 -2.7427354 -2.9172378 -2.4580452 -0.90397263 1.0172391 1.7599514 0.78730893 -0.90867424 -2.2079749 -2.6712694 -2.5342383 -2.1816661 -1.9157696][-2.196291 -2.4834433 -2.7437453 -2.7989225 -2.095937 -0.091253281 2.4472539 3.643213 2.6079161 0.44982696 -1.4620241 -2.387748 -2.4979813 -2.2101367 -1.9502404][-2.2229493 -2.482311 -2.6823411 -2.6688273 -1.8502409 0.33791566 3.1812098 4.6173792 3.5637853 1.1541767 -1.0767626 -2.2792888 -2.5241506 -2.2708914 -2.0088658][-2.2085073 -2.4330888 -2.6080747 -2.5964146 -1.8539059 0.10744047 2.6555502 3.9132183 2.9470222 0.72128105 -1.3583784 -2.4937935 -2.680526 -2.373476 -2.0586503][-2.1746707 -2.3668447 -2.548161 -2.6117055 -2.0760341 -0.5685724 1.3905687 2.2953498 1.4722538 -0.34676814 -2.0148315 -2.8797092 -2.8904526 -2.4834085 -2.094584][-2.1520574 -2.3076589 -2.4814484 -2.590651 -2.2580578 -1.1605725 0.24441433 0.8107655 0.050219774 -1.4201996 -2.6691008 -3.1922121 -3.0032907 -2.5046675 -2.0818281][-2.2055674 -2.2638555 -2.3235579 -2.3964329 -2.1944389 -1.3847864 -0.29950356 0.04928875 -0.71549726 -1.9917624 -2.9937358 -3.3071947 -2.9830818 -2.4325025 -2.0164185][-2.3496935 -2.2517076 -2.129 -2.0647941 -1.8472848 -1.1334248 -0.217587 0.0087094307 -0.72368729 -1.9372268 -2.8999839 -3.1853247 -2.8661187 -2.3253241 -1.949141][-2.5200419 -2.3262932 -2.031853 -1.7814567 -1.3596562 -0.40297079 0.57808018 0.62791991 -0.2649157 -1.6176938 -2.6970026 -3.0570064 -2.7998142 -2.2712381 -1.9121972][-2.5839105 -2.4269125 -2.1018653 -1.6577873 -0.87491715 0.49350619 1.6335733 1.5200515 0.27734685 -1.3677632 -2.5835571 -3.0094848 -2.7805984 -2.2732854 -1.9232888][-2.3810637 -2.3419695 -2.1137798 -1.5740798 -0.45765603 1.1957157 2.3560255 2.0141494 0.45752239 -1.3782793 -2.6176703 -3.0193977 -2.7997124 -2.3052292 -1.9545894][-1.7596818 -1.8524401 -1.8305652 -1.3793935 -0.2800405 1.3282835 2.3665674 1.8914893 0.21979094 -1.6277673 -2.8048556 -3.1589041 -2.8984585 -2.3648958 -1.9803813]]...]
INFO - root - 2017-12-16 09:52:52.117583: step 43210, loss = 0.67, batch loss = 0.41 (49.9 examples/sec; 0.160 sec/batch; 12h:52m:31s remains)
INFO - root - 2017-12-16 09:52:53.790899: step 43220, loss = 0.52, batch loss = 0.26 (48.4 examples/sec; 0.165 sec/batch; 13h:16m:46s remains)
INFO - root - 2017-12-16 09:52:55.413946: step 43230, loss = 0.52, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 12h:49m:10s remains)
INFO - root - 2017-12-16 09:52:57.023232: step 43240, loss = 0.58, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:21m:24s remains)
INFO - root - 2017-12-16 09:52:58.655097: step 43250, loss = 0.58, batch loss = 0.32 (49.8 examples/sec; 0.161 sec/batch; 12h:54m:09s remains)
INFO - root - 2017-12-16 09:53:00.285744: step 43260, loss = 0.63, batch loss = 0.37 (49.4 examples/sec; 0.162 sec/batch; 13h:00m:45s remains)
INFO - root - 2017-12-16 09:53:01.937431: step 43270, loss = 0.50, batch loss = 0.24 (48.9 examples/sec; 0.164 sec/batch; 13h:08m:44s remains)
INFO - root - 2017-12-16 09:53:03.578497: step 43280, loss = 0.58, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 13h:13m:39s remains)
INFO - root - 2017-12-16 09:53:05.219812: step 43290, loss = 0.52, batch loss = 0.26 (49.2 examples/sec; 0.162 sec/batch; 13h:03m:16s remains)
INFO - root - 2017-12-16 09:53:06.897748: step 43300, loss = 0.73, batch loss = 0.47 (47.7 examples/sec; 0.168 sec/batch; 13h:28m:17s remains)
2017-12-16 09:53:07.337646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5440581 -1.5592597 -1.5701621 -1.5739524 -1.5662439 -1.5135279 -1.4265774 -1.3800366 -1.3696716 -1.3171473 -1.2207377 -1.119408 -1.0359735 -0.95520794 -0.85035038][-2.0678027 -2.2267849 -2.3375149 -2.4104514 -2.4663455 -2.4354892 -2.4032917 -2.4882236 -2.5777261 -2.4935825 -2.3589859 -2.25585 -2.1455789 -1.949834 -1.6621801][-2.5802703 -2.8575497 -2.9720774 -2.9857812 -2.9803965 -2.8655155 -2.7884636 -3.0150549 -3.309979 -3.3099654 -3.2471762 -3.2499387 -3.1636105 -2.8720155 -2.4013815][-2.7327671 -2.9578466 -2.9020634 -2.731796 -2.5817776 -2.2690339 -2.0218391 -2.3447394 -2.9811287 -3.3040709 -3.522248 -3.7868361 -3.8452215 -3.5430827 -2.9736233][-2.3546367 -2.2565072 -1.7389693 -1.1657389 -0.73466432 -0.073903322 0.49228406 0.069834948 -1.1257945 -2.132127 -2.9571605 -3.6719613 -4.024395 -3.8959935 -3.4266171][-1.7923534 -1.2891628 -0.23230314 0.918787 1.9014132 3.2090538 4.3684921 4.0196533 2.201133 0.27102518 -1.3590429 -2.7072978 -3.4961543 -3.7222044 -3.5407696][-1.5957705 -0.96028209 0.32521176 1.8490388 3.3914425 5.4122982 7.3108263 7.3676443 5.3401728 2.8021691 0.48006845 -1.424894 -2.6279528 -3.196739 -3.3520589][-1.9587395 -1.5778973 -0.5603714 0.8417213 2.4728849 4.6695204 6.9037437 7.4363518 5.7956705 3.3976209 0.96854043 -1.0849001 -2.3324955 -2.8729248 -3.1492181][-2.6287434 -2.7798944 -2.3741095 -1.5048277 -0.27133131 1.4857986 3.336602 4.05219 3.1346591 1.553859 -0.28522229 -1.9356651 -2.8425558 -3.1007066 -3.3048649][-3.0435293 -3.6862092 -3.9233513 -3.727119 -3.07433 -1.9588853 -0.73990095 -0.17837572 -0.54880404 -1.2604933 -2.2091057 -3.1115978 -3.5239339 -3.5202644 -3.5465126][-2.9216063 -3.7763469 -4.4202437 -4.7320881 -4.5943265 -4.0576544 -3.4362004 -3.1515474 -3.2835164 -3.4068289 -3.5462484 -3.7250557 -3.7048388 -3.5423369 -3.3592746][-2.3886743 -3.1547081 -3.8938079 -4.4108839 -4.5585661 -4.3693867 -4.1189489 -4.063622 -4.1772041 -4.080791 -3.7933216 -3.4910865 -3.1989713 -2.981679 -2.7109079][-1.811555 -2.3459885 -2.9612155 -3.4484792 -3.6265123 -3.5394831 -3.4360993 -3.5051804 -3.6540961 -3.5598702 -3.1526127 -2.6945517 -2.3894691 -2.2519515 -2.0210304][-1.4239628 -1.72176 -2.1119933 -2.4564703 -2.5973136 -2.5510135 -2.4963741 -2.5894411 -2.7515454 -2.7102246 -2.4014661 -2.0372777 -1.8493354 -1.7896127 -1.614059][-1.2448435 -1.3746214 -1.5748211 -1.7641547 -1.8594892 -1.8557539 -1.8585265 -1.9583454 -2.080044 -2.0991545 -1.9763452 -1.8156588 -1.7591238 -1.7294898 -1.5895196]]...]
INFO - root - 2017-12-16 09:53:08.982145: step 43310, loss = 0.77, batch loss = 0.51 (47.7 examples/sec; 0.168 sec/batch; 13h:28m:37s remains)
INFO - root - 2017-12-16 09:53:10.642071: step 43320, loss = 0.61, batch loss = 0.36 (49.9 examples/sec; 0.160 sec/batch; 12h:53m:15s remains)
INFO - root - 2017-12-16 09:53:12.268938: step 43330, loss = 0.60, batch loss = 0.34 (48.5 examples/sec; 0.165 sec/batch; 13h:14m:24s remains)
INFO - root - 2017-12-16 09:53:13.896430: step 43340, loss = 0.48, batch loss = 0.23 (49.8 examples/sec; 0.161 sec/batch; 12h:54m:45s remains)
INFO - root - 2017-12-16 09:53:15.559455: step 43350, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 13h:15m:40s remains)
INFO - root - 2017-12-16 09:53:17.182082: step 43360, loss = 0.52, batch loss = 0.26 (49.9 examples/sec; 0.160 sec/batch; 12h:52m:04s remains)
INFO - root - 2017-12-16 09:53:18.823668: step 43370, loss = 0.66, batch loss = 0.40 (48.0 examples/sec; 0.167 sec/batch; 13h:23m:52s remains)
INFO - root - 2017-12-16 09:53:20.470533: step 43380, loss = 0.61, batch loss = 0.35 (49.1 examples/sec; 0.163 sec/batch; 13h:04m:57s remains)
INFO - root - 2017-12-16 09:53:22.078481: step 43390, loss = 0.58, batch loss = 0.32 (51.6 examples/sec; 0.155 sec/batch; 12h:26m:32s remains)
INFO - root - 2017-12-16 09:53:23.731434: step 43400, loss = 0.52, batch loss = 0.26 (47.0 examples/sec; 0.170 sec/batch; 13h:39m:51s remains)
2017-12-16 09:53:24.153312: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.78238368 0.47972584 -0.057041168 -0.73557711 -1.2913933 -1.6788495 -1.9333423 -2.1472492 -2.3195517 -2.3984869 -2.4298735 -2.5085993 -2.6589348 -2.8349926 -2.9960456][0.89550424 0.60041094 0.12972569 -0.48737824 -1.0457499 -1.4431486 -1.6616275 -1.8163288 -1.9730393 -2.1516728 -2.3824573 -2.66974 -2.9941111 -3.303688 -3.5007989][0.70954776 0.57599449 0.29054189 -0.20957494 -0.75949419 -1.1929989 -1.4384918 -1.5319366 -1.5986114 -1.7914405 -2.1504841 -2.5691843 -3.0049939 -3.3601432 -3.5330505][0.6328392 0.72698236 0.61473632 0.22686529 -0.29641366 -0.76861084 -1.0894406 -1.1905556 -1.1910996 -1.3393332 -1.7047539 -2.1416209 -2.561307 -2.8545105 -2.9382429][0.70245957 0.99903774 1.000772 0.69380069 0.24539971 -0.20614409 -0.54592967 -0.66589928 -0.63848579 -0.69487011 -0.96347237 -1.3396968 -1.6710186 -1.8698505 -1.9010055][0.54890561 0.99701357 1.1224093 0.95275879 0.63942409 0.28986812 0.048219204 -0.013176918 0.10375309 0.1925559 0.047294855 -0.2634027 -0.53209972 -0.68808925 -0.75848639][-0.079527617 0.4319253 0.68864393 0.70658422 0.5805819 0.43745685 0.39731765 0.50711918 0.73466086 0.94047213 0.94677544 0.7642324 0.5605247 0.43132162 0.26045227][-0.99308884 -0.47967279 -0.1226759 0.062791824 0.11135173 0.15632248 0.29209208 0.54152489 0.855968 1.18185 1.3897595 1.3937261 1.2820826 1.1964869 0.97623134][-1.8071086 -1.3595409 -0.97627306 -0.7225275 -0.58985674 -0.43598449 -0.18721056 0.11993241 0.45092583 0.79443264 1.0919082 1.250351 1.2697704 1.2507343 1.1167579][-2.411155 -2.1022754 -1.8212912 -1.6158623 -1.4627984 -1.2582253 -0.95855653 -0.63327408 -0.35008 -0.10214806 0.14340496 0.35600591 0.50597858 0.59405661 0.65372753][-2.785805 -2.6688728 -2.5473933 -2.4521604 -2.367496 -2.2028835 -1.9487009 -1.6709379 -1.4563038 -1.2854847 -1.0845014 -0.85280359 -0.62351525 -0.400782 -0.12711072][-2.8366642 -2.883209 -2.9062867 -2.9196031 -2.9140251 -2.833497 -2.6924391 -2.5274947 -2.4006445 -2.2976975 -2.145431 -1.9246182 -1.664866 -1.3737236 -0.98906434][-2.6389489 -2.7446697 -2.8259089 -2.8876677 -2.9432867 -2.959435 -2.9354649 -2.9017782 -2.8991389 -2.9014878 -2.840013 -2.6857412 -2.4584732 -2.1681886 -1.8012861][-2.2527328 -2.3635979 -2.4650776 -2.5533969 -2.640949 -2.7004585 -2.7266853 -2.7605953 -2.8458557 -2.9459229 -2.9994435 -2.9685645 -2.8595619 -2.6835911 -2.4508886][-1.6983113 -1.8144913 -1.9987056 -2.1780694 -2.326108 -2.3975904 -2.3974197 -2.3816981 -2.4360712 -2.5454521 -2.6384511 -2.693135 -2.7196529 -2.7234051 -2.6972461]]...]
INFO - root - 2017-12-16 09:53:25.801498: step 43410, loss = 0.50, batch loss = 0.25 (47.1 examples/sec; 0.170 sec/batch; 13h:37m:58s remains)
INFO - root - 2017-12-16 09:53:27.460590: step 43420, loss = 0.47, batch loss = 0.21 (49.6 examples/sec; 0.161 sec/batch; 12h:57m:49s remains)
INFO - root - 2017-12-16 09:53:29.084005: step 43430, loss = 0.58, batch loss = 0.32 (48.1 examples/sec; 0.166 sec/batch; 13h:21m:04s remains)
INFO - root - 2017-12-16 09:53:30.747058: step 43440, loss = 0.57, batch loss = 0.31 (49.1 examples/sec; 0.163 sec/batch; 13h:04m:12s remains)
INFO - root - 2017-12-16 09:53:32.391008: step 43450, loss = 0.52, batch loss = 0.26 (48.5 examples/sec; 0.165 sec/batch; 13h:15m:21s remains)
INFO - root - 2017-12-16 09:53:34.017865: step 43460, loss = 0.57, batch loss = 0.31 (49.6 examples/sec; 0.161 sec/batch; 12h:57m:21s remains)
INFO - root - 2017-12-16 09:53:35.683170: step 43470, loss = 0.48, batch loss = 0.22 (48.0 examples/sec; 0.167 sec/batch; 13h:22m:16s remains)
INFO - root - 2017-12-16 09:53:37.340025: step 43480, loss = 0.56, batch loss = 0.30 (49.3 examples/sec; 0.162 sec/batch; 13h:02m:26s remains)
INFO - root - 2017-12-16 09:53:38.985296: step 43490, loss = 0.51, batch loss = 0.25 (49.9 examples/sec; 0.160 sec/batch; 12h:52m:02s remains)
INFO - root - 2017-12-16 09:53:40.615557: step 43500, loss = 0.58, batch loss = 0.32 (48.7 examples/sec; 0.164 sec/batch; 13h:10m:29s remains)
2017-12-16 09:53:41.056273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2665856 -2.2174344 -2.1877253 -2.1535068 -2.1554582 -2.1907132 -2.211498 -2.1927087 -2.1506317 -2.1035948 -2.0503597 -1.9860919 -1.9277951 -1.8795996 -1.8494728][-2.7724679 -2.7598286 -2.7685554 -2.7728262 -2.8279612 -2.9308193 -2.9954724 -2.9880471 -2.9496112 -2.9053202 -2.8601983 -2.817353 -2.7810266 -2.7604017 -2.74305][-2.7775602 -2.80752 -2.8561552 -2.8908436 -2.9727254 -3.0980592 -3.170362 -3.1629143 -3.1458108 -3.1395993 -3.1323485 -3.1440063 -3.1811235 -3.2373943 -3.2700057][-2.3782713 -2.3742194 -2.3998041 -2.3946607 -2.3978329 -2.4324453 -2.4046659 -2.3501463 -2.3654513 -2.4384255 -2.5513716 -2.6890132 -2.8668666 -3.0392051 -3.1654143][-1.9381667 -1.9080712 -1.8951895 -1.7711502 -1.5751219 -1.3477278 -1.0757447 -0.86972427 -0.82339787 -0.94020545 -1.1824018 -1.4874806 -1.8461754 -2.1537902 -2.3714051][-1.657388 -1.6839244 -1.661101 -1.4155014 -0.98985136 -0.47206891 0.069126129 0.43700576 0.55222058 0.44888949 0.12100816 -0.27320957 -0.67593443 -0.98365676 -1.194399][-1.4187633 -1.5897195 -1.6526674 -1.4835904 -1.0649782 -0.47114587 0.1698308 0.58754659 0.71009731 0.635592 0.35608053 0.013631105 -0.24473333 -0.36741066 -0.45561874][-1.2644304 -1.6454787 -1.8628168 -1.8765311 -1.6307628 -1.1406391 -0.52839172 -0.14552259 -0.11602116 -0.25007272 -0.4639765 -0.60475457 -0.61917567 -0.46874416 -0.32207775][-1.1633244 -1.6210216 -1.8810024 -2.0420239 -2.0111783 -1.7680919 -1.3647105 -1.1331173 -1.2629598 -1.451403 -1.6227788 -1.6266013 -1.4060878 -0.96982741 -0.56859243][-1.2328626 -1.5159591 -1.6650971 -1.8407797 -1.9924638 -2.0036459 -1.8672221 -1.8642509 -2.1492326 -2.395324 -2.5338666 -2.4898815 -2.1660502 -1.5701642 -0.99163091][-1.3445225 -1.3829203 -1.3272566 -1.4625349 -1.7466792 -1.94661 -2.0196033 -2.1976073 -2.5475738 -2.775054 -2.9090581 -2.8690827 -2.5170696 -1.8758551 -1.2569983][-1.4116864 -1.3254139 -1.1277968 -1.2639186 -1.6782408 -1.9471481 -2.0024221 -2.1295397 -2.3328671 -2.4716702 -2.6108544 -2.6338434 -2.3408458 -1.747736 -1.176834][-1.5420306 -1.4579217 -1.3681946 -1.6179129 -2.0305488 -2.1183617 -1.8798604 -1.6408026 -1.5845859 -1.7281785 -2.0341654 -2.1994479 -2.0181775 -1.5537183 -1.0743072][-1.764293 -1.7622744 -1.8296795 -2.1628077 -2.4336667 -2.2020075 -1.5619502 -0.91209412 -0.64277112 -0.89398217 -1.5056188 -1.9306818 -1.9254608 -1.5946227 -1.2299324][-1.9493234 -1.995258 -2.131207 -2.4270148 -2.5556705 -2.1002941 -1.2389462 -0.44530725 -0.10576868 -0.43977022 -1.2817068 -1.9160767 -2.0704205 -1.86358 -1.5888276]]...]
INFO - root - 2017-12-16 09:53:42.683564: step 43510, loss = 0.69, batch loss = 0.43 (47.4 examples/sec; 0.169 sec/batch; 13h:33m:08s remains)
INFO - root - 2017-12-16 09:53:44.320634: step 43520, loss = 0.61, batch loss = 0.35 (48.7 examples/sec; 0.164 sec/batch; 13h:10m:53s remains)
INFO - root - 2017-12-16 09:53:45.959909: step 43530, loss = 0.57, batch loss = 0.31 (49.3 examples/sec; 0.162 sec/batch; 13h:01m:36s remains)
INFO - root - 2017-12-16 09:53:47.577300: step 43540, loss = 0.52, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 12h:48m:47s remains)
INFO - root - 2017-12-16 09:53:49.217818: step 43550, loss = 0.62, batch loss = 0.36 (46.2 examples/sec; 0.173 sec/batch; 13h:53m:53s remains)
INFO - root - 2017-12-16 09:53:50.844033: step 43560, loss = 0.49, batch loss = 0.24 (46.3 examples/sec; 0.173 sec/batch; 13h:51m:12s remains)
INFO - root - 2017-12-16 09:53:52.513217: step 43570, loss = 0.49, batch loss = 0.23 (48.8 examples/sec; 0.164 sec/batch; 13h:10m:11s remains)
INFO - root - 2017-12-16 09:53:54.141142: step 43580, loss = 0.64, batch loss = 0.38 (49.6 examples/sec; 0.161 sec/batch; 12h:57m:16s remains)
INFO - root - 2017-12-16 09:53:55.784456: step 43590, loss = 0.64, batch loss = 0.38 (46.0 examples/sec; 0.174 sec/batch; 13h:57m:20s remains)
INFO - root - 2017-12-16 09:53:57.429828: step 43600, loss = 0.52, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 13h:21m:52s remains)
2017-12-16 09:53:57.854176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1113307 -3.141449 -2.9526005 -2.541908 -2.0065134 -1.6128381 -1.327988 -1.0559223 -0.87464166 -1.1594716 -2.0024927 -2.8712008 -3.3772449 -3.5651779 -3.5318236][-3.1260355 -3.1316807 -2.8874562 -2.3600345 -1.6933088 -1.3112555 -1.0641811 -0.8059206 -0.63959217 -0.87976384 -1.7845428 -2.7855706 -3.3894162 -3.6441453 -3.6058087][-3.3504767 -3.3014188 -3.0278716 -2.4368725 -1.7429161 -1.3520238 -1.1055532 -0.7145927 -0.50077915 -0.71359968 -1.6360258 -2.7150016 -3.4489937 -3.7540741 -3.7161264][-3.6685016 -3.5746357 -3.2497196 -2.5888746 -1.7436386 -1.1682497 -0.83299327 -0.40128708 -0.29185081 -0.65866256 -1.599067 -2.7047832 -3.5134482 -3.8424482 -3.7661672][-3.8388305 -3.741667 -3.3797932 -2.6419384 -1.5828124 -0.73738325 -0.23582053 0.13675976 -0.10328245 -0.75324726 -1.7245939 -2.8362949 -3.6231632 -3.9259026 -3.7858272][-3.7615614 -3.7622714 -3.4617925 -2.6430507 -1.3647777 -0.14135909 0.6875596 1.0000668 0.36880851 -0.6930238 -1.8695494 -2.9543896 -3.7108831 -3.9453158 -3.7436633][-3.2758164 -3.5073929 -3.400167 -2.628767 -1.1900705 0.38585353 1.6194665 2.0585177 1.1034877 -0.36624265 -1.7519517 -2.8782341 -3.5720525 -3.7644668 -3.5642319][-2.4355934 -2.8976426 -2.9997754 -2.4348333 -1.0934268 0.58200192 2.0314734 2.6559794 1.6668007 0.00022053719 -1.4867907 -2.626863 -3.2764497 -3.4622107 -3.2799754][-1.5594027 -2.1749849 -2.4435148 -2.126873 -1.0770501 0.34620547 1.7086031 2.4472063 1.7481844 0.18448019 -1.2798911 -2.2958438 -2.8610644 -3.0259104 -2.8586268][-0.97038531 -1.5457321 -1.8760397 -1.7481589 -1.0245324 -0.0089194775 1.0307486 1.688266 1.2573237 -0.030275583 -1.2411947 -1.9799023 -2.3839948 -2.4937406 -2.3379283][-0.77234161 -1.2229294 -1.5088077 -1.4371679 -0.93412828 -0.21044302 0.46700072 0.85818887 0.59739494 -0.32997131 -1.2148916 -1.702054 -1.9472287 -2.0074241 -1.8876878][-0.90862048 -1.3148949 -1.5103259 -1.3268746 -0.80744421 -0.18232632 0.27421951 0.43367672 0.20861626 -0.44423616 -1.0888073 -1.4319184 -1.6261472 -1.6906873 -1.612841][-1.267 -1.5974462 -1.6963589 -1.3966687 -0.7461164 -0.08802247 0.24241829 0.17748761 -0.13933706 -0.68248725 -1.1499939 -1.4280838 -1.5747391 -1.6007659 -1.539641][-1.6251676 -1.9655726 -2.0437224 -1.7200857 -1.0805439 -0.47214139 -0.26647139 -0.46682155 -0.85550117 -1.3103361 -1.6351386 -1.8354673 -1.9093971 -1.887218 -1.803354][-1.9274371 -2.3073232 -2.4755101 -2.3140688 -1.8682101 -1.3986341 -1.2513283 -1.4342926 -1.7779915 -2.1140456 -2.3190372 -2.4442582 -2.4468627 -2.3744886 -2.2615931]]...]
INFO - root - 2017-12-16 09:53:59.523355: step 43610, loss = 0.53, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:06m:39s remains)
INFO - root - 2017-12-16 09:54:01.181832: step 43620, loss = 0.58, batch loss = 0.32 (49.1 examples/sec; 0.163 sec/batch; 13h:03m:53s remains)
INFO - root - 2017-12-16 09:54:02.833670: step 43630, loss = 0.56, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 13h:33m:52s remains)
INFO - root - 2017-12-16 09:54:04.459890: step 43640, loss = 0.64, batch loss = 0.38 (50.0 examples/sec; 0.160 sec/batch; 12h:51m:01s remains)
INFO - root - 2017-12-16 09:54:06.139726: step 43650, loss = 0.50, batch loss = 0.24 (48.2 examples/sec; 0.166 sec/batch; 13h:19m:21s remains)
INFO - root - 2017-12-16 09:54:07.792506: step 43660, loss = 0.56, batch loss = 0.30 (48.4 examples/sec; 0.165 sec/batch; 13h:15m:02s remains)
INFO - root - 2017-12-16 09:54:09.424121: step 43670, loss = 0.53, batch loss = 0.27 (48.5 examples/sec; 0.165 sec/batch; 13h:14m:49s remains)
INFO - root - 2017-12-16 09:54:11.063465: step 43680, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:15m:18s remains)
INFO - root - 2017-12-16 09:54:12.710587: step 43690, loss = 0.54, batch loss = 0.29 (49.9 examples/sec; 0.160 sec/batch; 12h:51m:36s remains)
INFO - root - 2017-12-16 09:54:14.343629: step 43700, loss = 0.46, batch loss = 0.20 (47.7 examples/sec; 0.168 sec/batch; 13h:27m:22s remains)
2017-12-16 09:54:14.783300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5646973 -2.123868 -1.765588 -1.6171913 -1.5620368 -1.5089208 -1.3372302 -1.2535831 -1.517459 -1.7148935 -1.5981264 -1.7504932 -1.9894767 -1.766068 -1.3091538][-2.9758618 -2.7599382 -2.6058946 -2.5235891 -2.4106851 -2.3154733 -2.0311024 -1.6823747 -1.6940792 -1.7864074 -1.6394498 -1.7978876 -2.0542152 -1.8576686 -1.3897576][-2.86239 -2.9583466 -3.168045 -3.3308153 -3.2963834 -3.1817145 -2.7366507 -2.0140426 -1.6235747 -1.5396628 -1.4240458 -1.6813729 -2.0560973 -1.9820486 -1.5590365][-2.1592097 -2.6124728 -3.2127283 -3.6390772 -3.7197366 -3.5699189 -2.9695454 -1.9689394 -1.2616789 -1.0122609 -0.93187106 -1.3453346 -1.9193013 -2.0796716 -1.7884369][-0.92928481 -1.6837049 -2.5184362 -3.0708666 -3.1546843 -2.9212437 -2.2221379 -1.1351894 -0.37650561 -0.10160899 -0.13630581 -0.74587274 -1.6067615 -2.0837419 -2.0258][0.40404344 -0.51796305 -1.3040915 -1.67959 -1.5386698 -1.0928023 -0.276201 0.75974894 1.2346988 1.0840986 0.70222712 -0.17882586 -1.3491868 -2.1474288 -2.295984][1.4076786 0.4769187 -0.034555674 -0.0044481754 0.46624708 1.252176 2.402869 3.3908656 3.2685659 2.3100832 1.2271118 -0.058003187 -1.4317667 -2.3233793 -2.5301645][1.7092731 1.0967808 0.99147606 1.3517883 2.0683124 3.1625526 4.8429394 6.0667048 4.9537668 2.8697565 1.0994272 -0.41345954 -1.742166 -2.5428433 -2.6642621][1.3306363 1.1070197 1.3939438 1.917866 2.5057786 3.3666737 4.8671694 5.8491383 4.4148445 2.1519272 0.43664145 -0.90992141 -2.053616 -2.6392465 -2.6151698][0.33899403 0.4099195 0.85382676 1.3328991 1.5698206 1.8985741 2.6701076 3.15475 2.1579559 0.45840764 -0.75562811 -1.6644946 -2.3929088 -2.6387646 -2.4156654][-1.1570153 -0.99578607 -0.54753029 -0.22221136 -0.33854651 -0.40189695 -0.055904627 0.145432 -0.51288569 -1.5141736 -2.1273413 -2.484328 -2.6846037 -2.5795095 -2.2198274][-2.325357 -2.30662 -2.1061945 -1.9988115 -2.2465923 -2.5576146 -2.5257964 -2.4250691 -2.7126913 -3.1376195 -3.2538195 -3.1674979 -2.9887574 -2.6353476 -2.2190986][-2.7009714 -2.8624544 -2.876008 -2.9454319 -3.3343 -3.8144941 -3.93762 -3.8413353 -3.8260982 -3.8484497 -3.6796527 -3.362211 -3.0293736 -2.6245267 -2.26185][-2.4972143 -2.721122 -2.8540175 -3.0405192 -3.4941392 -4.0096989 -4.1321497 -3.9577227 -3.7188106 -3.5258591 -3.3145432 -2.9966116 -2.7117372 -2.4633965 -2.2799816][-2.1400926 -2.3535848 -2.5226436 -2.7525606 -3.1484368 -3.5343287 -3.5642815 -3.2399502 -2.84171 -2.596873 -2.4868014 -2.3858788 -2.3273735 -2.3144929 -2.3008633]]...]
INFO - root - 2017-12-16 09:54:16.416797: step 43710, loss = 0.62, batch loss = 0.36 (48.6 examples/sec; 0.165 sec/batch; 13h:13m:06s remains)
INFO - root - 2017-12-16 09:54:18.078945: step 43720, loss = 0.57, batch loss = 0.31 (46.6 examples/sec; 0.172 sec/batch; 13h:46m:59s remains)
INFO - root - 2017-12-16 09:54:19.741370: step 43730, loss = 0.61, batch loss = 0.35 (48.8 examples/sec; 0.164 sec/batch; 13h:09m:06s remains)
INFO - root - 2017-12-16 09:54:21.359896: step 43740, loss = 0.55, batch loss = 0.29 (49.3 examples/sec; 0.162 sec/batch; 13h:00m:55s remains)
INFO - root - 2017-12-16 09:54:23.000155: step 43750, loss = 0.54, batch loss = 0.28 (49.4 examples/sec; 0.162 sec/batch; 12h:59m:33s remains)
INFO - root - 2017-12-16 09:54:24.641982: step 43760, loss = 0.53, batch loss = 0.28 (49.3 examples/sec; 0.162 sec/batch; 13h:01m:07s remains)
INFO - root - 2017-12-16 09:54:26.275252: step 43770, loss = 0.52, batch loss = 0.26 (49.8 examples/sec; 0.161 sec/batch; 12h:53m:45s remains)
INFO - root - 2017-12-16 09:54:27.946461: step 43780, loss = 0.54, batch loss = 0.28 (48.1 examples/sec; 0.166 sec/batch; 13h:19m:55s remains)
INFO - root - 2017-12-16 09:54:29.604467: step 43790, loss = 0.62, batch loss = 0.36 (49.3 examples/sec; 0.162 sec/batch; 13h:00m:49s remains)
INFO - root - 2017-12-16 09:54:31.216714: step 43800, loss = 0.60, batch loss = 0.35 (49.2 examples/sec; 0.163 sec/batch; 13h:03m:05s remains)
2017-12-16 09:54:31.636156: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3245561 1.0987685 0.6870594 0.43087435 0.36628175 0.13621044 -0.32798195 -0.77347088 -1.1777263 -1.4997755 -1.7383537 -2.1086833 -2.6985137 -3.3612337 -3.9176469][0.78039384 0.68346691 0.40986276 0.25462437 0.19504285 -0.12872386 -0.75958288 -1.478579 -2.1839983 -2.7734818 -3.1765022 -3.538425 -3.9369326 -4.2796168 -4.439971][-0.57452989 -0.60183609 -0.75110316 -0.82994258 -0.90845978 -1.2760223 -1.9142712 -2.6482067 -3.291503 -3.7380741 -3.999887 -4.1843963 -4.3142648 -4.370791 -4.2539892][-2.2431402 -2.2642932 -2.374115 -2.4574723 -2.5515423 -2.8350954 -3.2126195 -3.5484838 -3.7340982 -3.7321007 -3.6501358 -3.5802097 -3.5523753 -3.5327506 -3.36274][-3.4955468 -3.5820427 -3.6953201 -3.7638221 -3.794683 -3.7931032 -3.6248379 -3.26382 -2.7470386 -2.26879 -1.9569498 -1.8695292 -1.9712923 -2.1106215 -2.0798507][-3.856179 -3.9922254 -4.0888414 -4.1029119 -3.9486814 -3.5054197 -2.6921611 -1.6112478 -0.5768441 0.11766839 0.28851891 0.039941788 -0.45836687 -0.89158475 -1.0376828][-3.1401577 -3.2647538 -3.336725 -3.2775121 -2.9010215 -2.0033996 -0.58833778 1.014787 2.1618145 2.502043 2.0484335 1.1819472 0.26413226 -0.43641722 -0.68273795][-1.8239417 -1.8954433 -1.8352027 -1.6499087 -1.1299176 -0.013658285 1.6477644 3.3280041 4.1117 3.6402338 2.3743708 1.0079434 -0.078505039 -0.79914129 -1.0018111][-0.88131571 -0.90114665 -0.68960738 -0.3479054 0.18306494 1.1749566 2.4901683 3.6446621 3.8000743 2.7269123 1.1116688 -0.34427595 -1.3033071 -1.7912896 -1.8081952][-0.70614469 -0.690153 -0.52096772 -0.23673606 0.21329689 0.86634994 1.5307934 1.8703086 1.6502268 0.67471862 -0.70860136 -1.9181814 -2.6292126 -2.8708169 -2.6772676][-0.80099523 -0.81115472 -0.88392496 -0.84349561 -0.57293522 -0.13956404 0.18155169 0.13396525 -0.17650628 -0.87703288 -1.8071127 -2.6567397 -3.1378083 -3.2391336 -2.9853625][-0.82878637 -0.91110826 -1.2365534 -1.4477526 -1.2653075 -0.84846449 -0.53658259 -0.48907781 -0.66576922 -1.150761 -1.8194699 -2.4230633 -2.7507322 -2.8309057 -2.6091344][-0.86857748 -0.94146526 -1.3130904 -1.5047877 -1.2311357 -0.65845418 -0.10888624 0.23699832 0.210109 -0.25529909 -0.93951416 -1.5112712 -1.8449309 -2.0064363 -1.8945235][-1.1270406 -1.1193478 -1.2461342 -1.1200951 -0.58234417 0.16229439 0.893626 1.5045865 1.6145282 1.061182 0.23485398 -0.34946918 -0.64999759 -0.94543147 -1.0391454][-1.5637769 -1.4816835 -1.2665343 -0.72624552 0.063985825 0.84571075 1.5080497 2.0652206 2.1577423 1.6103153 0.79671717 0.28799248 0.13249636 -0.076620817 -0.26784706]]...]
INFO - root - 2017-12-16 09:54:33.251092: step 43810, loss = 0.54, batch loss = 0.28 (50.3 examples/sec; 0.159 sec/batch; 12h:45m:36s remains)
INFO - root - 2017-12-16 09:54:34.877520: step 43820, loss = 0.57, batch loss = 0.31 (48.2 examples/sec; 0.166 sec/batch; 13h:18m:24s remains)
INFO - root - 2017-12-16 09:54:36.504653: step 43830, loss = 0.71, batch loss = 0.45 (50.5 examples/sec; 0.158 sec/batch; 12h:41m:58s remains)
INFO - root - 2017-12-16 09:54:38.154990: step 43840, loss = 0.62, batch loss = 0.36 (47.6 examples/sec; 0.168 sec/batch; 13h:28m:16s remains)
INFO - root - 2017-12-16 09:54:39.798479: step 43850, loss = 0.62, batch loss = 0.36 (49.1 examples/sec; 0.163 sec/batch; 13h:04m:13s remains)
INFO - root - 2017-12-16 09:54:41.427328: step 43860, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 13h:21m:31s remains)
INFO - root - 2017-12-16 09:54:43.072406: step 43870, loss = 0.61, batch loss = 0.35 (48.6 examples/sec; 0.164 sec/batch; 13h:11m:13s remains)
INFO - root - 2017-12-16 09:54:44.689364: step 43880, loss = 0.65, batch loss = 0.40 (49.7 examples/sec; 0.161 sec/batch; 12h:54m:16s remains)
INFO - root - 2017-12-16 09:54:46.349645: step 43890, loss = 0.51, batch loss = 0.25 (45.4 examples/sec; 0.176 sec/batch; 14h:07m:29s remains)
INFO - root - 2017-12-16 09:54:47.989006: step 43900, loss = 0.53, batch loss = 0.27 (49.2 examples/sec; 0.162 sec/batch; 13h:01m:32s remains)
2017-12-16 09:54:48.427157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3629937 -2.1571558 -1.7820473 -1.4037213 -1.0376501 -0.87397742 -0.98125374 -1.1898369 -1.2713597 -1.1524554 -1.274006 -1.5012866 -1.5298852 -1.4483061 -1.3237653][-2.0963128 -1.7326421 -1.3196172 -1.035996 -0.79105461 -0.67684591 -0.99952912 -1.5282137 -1.9676267 -2.1762798 -2.4781487 -2.6234775 -2.4095552 -2.1096323 -1.8464541][-1.7263991 -1.3072703 -0.98954403 -1.0132208 -1.080385 -1.1216407 -1.4940646 -2.0520658 -2.53628 -2.8805857 -3.2800956 -3.4079089 -3.135524 -2.7442753 -2.3830838][-1.3211377 -0.97515738 -0.92632246 -1.2688975 -1.5937164 -1.7513783 -1.9791605 -2.2631087 -2.4853077 -2.7796576 -3.2463336 -3.5170255 -3.3900852 -3.0456278 -2.6597602][-1.0155795 -0.83280337 -1.0350492 -1.530744 -1.9439321 -2.0470607 -1.8715912 -1.5451739 -1.2735554 -1.4094002 -2.0203588 -2.5743804 -2.8032773 -2.7745523 -2.5541553][-1.0076513 -0.94448376 -1.2091095 -1.6776459 -2.0037801 -1.8317487 -1.0423201 0.040563822 0.85746288 0.80908942 -0.095327854 -1.0761098 -1.7538977 -2.1704926 -2.2188332][-1.1994729 -1.1220096 -1.2775903 -1.5861629 -1.7137926 -1.1882569 0.20030832 2.0067537 3.2589328 2.9614599 1.5506446 0.16058874 -0.92295647 -1.6789087 -1.9674811][-1.3156011 -1.1358258 -1.1092029 -1.1950666 -1.1025442 -0.31953526 1.3007324 3.4038665 4.6938477 3.800915 2.0479805 0.49158144 -0.73060918 -1.5623732 -1.8859751][-1.3754218 -1.0795822 -0.89973962 -0.85805905 -0.69419527 -0.020699263 1.28197 2.8574436 3.5330465 2.63394 1.1218452 -0.19718814 -1.2021047 -1.8662145 -2.0547001][-1.3476102 -1.090059 -0.9131515 -0.84399617 -0.79346228 -0.48490787 0.18292618 0.98038888 1.2328284 0.66125083 -0.3647778 -1.2770417 -1.946888 -2.3316495 -2.3478539][-1.34258 -1.1799717 -1.0889143 -1.1188581 -1.2463158 -1.3118361 -1.1755449 -0.89645422 -0.83776259 -1.1452363 -1.7437307 -2.3093894 -2.6782773 -2.7672117 -2.6210306][-1.2740328 -1.2773439 -1.2975751 -1.4063985 -1.6355118 -1.8534886 -1.9254316 -1.8780558 -1.9013412 -2.1327429 -2.481998 -2.812058 -2.9515471 -2.846935 -2.6234474][-1.1154702 -1.2342494 -1.3464173 -1.4577322 -1.6113536 -1.7259305 -1.7897711 -1.7975748 -1.858763 -2.0923376 -2.3840404 -2.6153498 -2.6429789 -2.5020347 -2.3254397][-0.91763794 -1.0480311 -1.1637772 -1.2075422 -1.1870227 -1.1048107 -1.0722779 -1.0974425 -1.1955241 -1.4734497 -1.7865354 -2.0304275 -2.0881958 -1.9909445 -1.8969827][-0.81186378 -0.87811685 -0.89126384 -0.81187785 -0.6205864 -0.39494872 -0.32120657 -0.37975836 -0.55112469 -0.93176627 -1.342073 -1.6392417 -1.6925147 -1.5759647 -1.5062876]]...]
INFO - root - 2017-12-16 09:54:50.088588: step 43910, loss = 0.56, batch loss = 0.30 (48.2 examples/sec; 0.166 sec/batch; 13h:19m:01s remains)
INFO - root - 2017-12-16 09:54:51.769284: step 43920, loss = 0.55, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 13h:18m:55s remains)
INFO - root - 2017-12-16 09:54:53.407406: step 43930, loss = 0.63, batch loss = 0.37 (48.2 examples/sec; 0.166 sec/batch; 13h:18m:43s remains)
INFO - root - 2017-12-16 09:54:55.035045: step 43940, loss = 0.50, batch loss = 0.24 (49.8 examples/sec; 0.161 sec/batch; 12h:51m:57s remains)
INFO - root - 2017-12-16 09:54:56.692861: step 43950, loss = 0.59, batch loss = 0.34 (49.7 examples/sec; 0.161 sec/batch; 12h:53m:32s remains)
INFO - root - 2017-12-16 09:54:58.323995: step 43960, loss = 0.53, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 12h:58m:00s remains)
INFO - root - 2017-12-16 09:54:59.972460: step 43970, loss = 0.62, batch loss = 0.36 (47.7 examples/sec; 0.168 sec/batch; 13h:26m:11s remains)
INFO - root - 2017-12-16 09:55:01.622171: step 43980, loss = 0.58, batch loss = 0.32 (49.5 examples/sec; 0.162 sec/batch; 12h:57m:42s remains)
INFO - root - 2017-12-16 09:55:03.265753: step 43990, loss = 0.50, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 13h:24m:22s remains)
INFO - root - 2017-12-16 09:55:04.907330: step 44000, loss = 0.66, batch loss = 0.40 (48.2 examples/sec; 0.166 sec/batch; 13h:18m:21s remains)
2017-12-16 09:55:05.341680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7999625 -1.8332462 -1.7983232 -1.7136188 -1.5625539 -1.570586 -1.7253482 -1.6577218 -1.2830009 -1.0058684 -1.1490116 -1.513504 -1.7159981 -1.5900478 -1.3465675][-2.0528054 -1.9758476 -1.796257 -1.6252177 -1.4834007 -1.5231874 -1.6978159 -1.6167593 -1.1880714 -0.85150647 -1.0271112 -1.4769144 -1.7647138 -1.6741196 -1.4515421][-2.3032506 -2.1150239 -1.7217908 -1.3929832 -1.162683 -1.1289123 -1.2149569 -1.097913 -0.64034414 -0.34671664 -0.62260509 -1.1835126 -1.5727072 -1.5757936 -1.4294354][-2.4215603 -2.0676849 -1.3599491 -0.69621623 -0.32480741 -0.22782946 -0.29251981 -0.1768136 0.22433472 0.35539865 -0.090127945 -0.74369717 -1.2084612 -1.2974565 -1.2291523][-2.1960695 -1.7064688 -0.72338974 0.239146 0.76499581 0.92350292 0.79277158 0.71965528 0.87101984 0.78944659 0.20917583 -0.49541831 -0.92537296 -1.0500717 -1.0075296][-1.7238995 -1.2013466 -0.093528509 1.0020008 1.5943477 1.8000457 1.570416 1.2220428 1.0016298 0.65599012 0.0082416534 -0.61667812 -0.92396867 -0.9828707 -0.92626476][-1.5692601 -1.1720644 -0.0819478 1.0555611 1.6801527 1.9511359 1.6530812 1.144213 0.64065313 0.046147346 -0.61860216 -1.1101011 -1.2346849 -1.1657569 -1.0552481][-2.0283411 -1.8312516 -0.908954 0.14927387 0.81771469 1.2074325 1.0142529 0.55314493 0.066297531 -0.53880596 -1.1528959 -1.5271583 -1.5709944 -1.4336331 -1.2301075][-2.703398 -2.7747869 -2.134568 -1.2606465 -0.58317447 -0.09519434 -0.055315971 -0.2407887 -0.53067958 -0.97923815 -1.3979247 -1.6323495 -1.6607033 -1.5458314 -1.2622609][-2.9426854 -3.3397079 -3.0718231 -2.463969 -1.8520693 -1.3107371 -1.0574626 -1.0181061 -1.1067578 -1.2910112 -1.43185 -1.4825017 -1.5005207 -1.3979864 -1.0655463][-2.5355942 -3.1557879 -3.1848688 -2.8411777 -2.386549 -1.9601331 -1.6730227 -1.5520598 -1.4806232 -1.3910012 -1.186516 -1.0030996 -1.0180602 -0.99012589 -0.72072244][-1.6103356 -2.2580733 -2.4094112 -2.2699316 -2.0503352 -1.8011558 -1.575048 -1.5021522 -1.4088128 -1.1315998 -0.69137561 -0.38556874 -0.48086953 -0.59533036 -0.50549042][-0.70654178 -1.2190453 -1.2462566 -1.1828843 -1.1838168 -1.0987668 -0.88936007 -0.89870358 -0.93889832 -0.69111109 -0.22518253 0.10424709 -0.036352634 -0.30946732 -0.4824363][-0.44581056 -0.70812464 -0.51650071 -0.35359979 -0.42959547 -0.36739755 -0.1080792 -0.17888403 -0.36199474 -0.27636671 0.078626394 0.46148229 0.3567524 0.0019056797 -0.40375483][-1.0201833 -0.98352265 -0.54295909 -0.22759128 -0.25754595 -0.17749524 0.18382907 0.12276888 -0.15309978 -0.21624398 -0.0033810139 0.51253653 0.55621624 0.25787902 -0.18533874]]...]
INFO - root - 2017-12-16 09:55:06.967564: step 44010, loss = 0.63, batch loss = 0.37 (51.0 examples/sec; 0.157 sec/batch; 12h:34m:49s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:55:08.634867: step 44020, loss = 0.51, batch loss = 0.25 (49.1 examples/sec; 0.163 sec/batch; 13h:03m:13s remains)
INFO - root - 2017-12-16 09:55:10.255156: step 44030, loss = 0.53, batch loss = 0.27 (50.5 examples/sec; 0.158 sec/batch; 12h:41m:40s remains)
INFO - root - 2017-12-16 09:55:11.889262: step 44040, loss = 0.53, batch loss = 0.28 (46.6 examples/sec; 0.172 sec/batch; 13h:46m:07s remains)
INFO - root - 2017-12-16 09:55:13.573893: step 44050, loss = 0.54, batch loss = 0.28 (48.8 examples/sec; 0.164 sec/batch; 13h:07m:28s remains)
INFO - root - 2017-12-16 09:55:15.209548: step 44060, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:10m:29s remains)
INFO - root - 2017-12-16 09:55:16.824242: step 44070, loss = 0.70, batch loss = 0.45 (49.7 examples/sec; 0.161 sec/batch; 12h:54m:15s remains)
INFO - root - 2017-12-16 09:55:18.441647: step 44080, loss = 0.55, batch loss = 0.29 (49.8 examples/sec; 0.161 sec/batch; 12h:52m:59s remains)
INFO - root - 2017-12-16 09:55:20.071464: step 44090, loss = 0.51, batch loss = 0.26 (50.8 examples/sec; 0.157 sec/batch; 12h:36m:56s remains)
INFO - root - 2017-12-16 09:55:21.739339: step 44100, loss = 0.55, batch loss = 0.29 (49.9 examples/sec; 0.160 sec/batch; 12h:50m:36s remains)
2017-12-16 09:55:22.188609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1772792 -2.0312378 -1.7128754 -1.5354974 -1.6532981 -1.9060601 -2.2045245 -2.4550514 -2.6955798 -2.6848869 -1.96812 -0.69873476 0.45280123 0.881685 0.3221035][-2.3601756 -2.3870058 -2.2117465 -2.1727028 -2.3990169 -2.7118254 -2.9332533 -2.9710159 -2.8542407 -2.4675603 -1.5544018 -0.43050921 0.28764629 0.2792697 -0.52207613][-2.8638728 -3.0544095 -3.04515 -3.1214182 -3.3771076 -3.594152 -3.6417789 -3.3673196 -2.783246 -2.0166781 -1.0826037 -0.38599622 -0.34833503 -0.8551873 -1.7050128][-3.3300748 -3.6155682 -3.7427447 -3.849159 -3.9557986 -3.9011168 -3.6303887 -2.9662933 -1.955088 -0.96912706 -0.2199297 -0.14673448 -0.8135469 -1.7072407 -2.483285][-3.4896357 -3.829273 -3.9748738 -3.9579511 -3.7358546 -3.2860835 -2.6043949 -1.5840033 -0.37211847 0.5578301 0.91022563 0.34925365 -0.82316554 -1.9313705 -2.6382277][-3.2067585 -3.5657122 -3.6740484 -3.4691067 -2.8906674 -1.9572302 -0.82485127 0.40075779 1.4183693 1.810127 1.5721118 0.63482237 -0.65403235 -1.7034171 -2.2472332][-2.4036665 -2.7484372 -2.826092 -2.4579537 -1.5422211 -0.20197511 1.2117829 2.3102381 2.6048009 2.0275266 1.0539551 -0.032172918 -0.98375392 -1.4464478 -1.4868171][-1.3585485 -1.6317565 -1.6172522 -1.1675786 -0.16282082 1.2482302 2.5650966 3.1251895 2.4221971 0.99414253 -0.263839 -0.9929601 -1.265318 -1.1038995 -0.6550374][-0.50557721 -0.64395678 -0.56703043 -0.18897796 0.5283215 1.4373093 2.1111224 1.9604914 0.74338722 -0.75072455 -1.6478581 -1.7017648 -1.2653652 -0.63431859 0.0786829][-0.31193948 -0.25031757 -0.092544079 0.12946177 0.40193629 0.626492 0.54903126 -0.083778381 -1.2398632 -2.2972779 -2.4881752 -1.8478111 -0.99031496 -0.20807266 0.47053218][-0.71471334 -0.502632 -0.24111629 -0.077538729 -0.094035387 -0.32083941 -0.81824851 -1.5656914 -2.4062643 -2.9136338 -2.482569 -1.4632826 -0.58849144 -0.065458059 0.32794595][-1.7587149 -1.4414678 -1.0446867 -0.8142302 -0.89495683 -1.2226914 -1.7883954 -2.3671072 -2.7253292 -2.6065762 -1.7563162 -0.77503276 -0.30375934 -0.31107235 -0.33155727][-2.5044901 -2.1810534 -1.7911241 -1.5510776 -1.5142791 -1.6779387 -2.1109147 -2.4618263 -2.4374087 -1.8842387 -0.92863131 -0.24038196 -0.29133511 -0.872916 -1.2774773][-2.5721502 -2.4057655 -2.2097766 -2.0373356 -1.8557181 -1.7554389 -1.9782801 -2.2318041 -2.1644638 -1.5773004 -0.73999596 -0.30817008 -0.6657331 -1.5349505 -2.1945856][-2.2645586 -2.3265264 -2.3800173 -2.2889822 -1.9796408 -1.6957195 -1.8270634 -2.1560225 -2.321511 -1.8761871 -1.1364378 -0.73778605 -1.1130323 -1.9944401 -2.7014008]]...]
INFO - root - 2017-12-16 09:55:23.816071: step 44110, loss = 0.59, batch loss = 0.34 (49.9 examples/sec; 0.160 sec/batch; 12h:50m:09s remains)
INFO - root - 2017-12-16 09:55:25.448201: step 44120, loss = 0.59, batch loss = 0.33 (49.5 examples/sec; 0.162 sec/batch; 12h:56m:49s remains)
INFO - root - 2017-12-16 09:55:27.086385: step 44130, loss = 0.61, batch loss = 0.36 (47.2 examples/sec; 0.169 sec/batch; 13h:33m:51s remains)
INFO - root - 2017-12-16 09:55:28.723344: step 44140, loss = 0.52, batch loss = 0.27 (50.0 examples/sec; 0.160 sec/batch; 12h:49m:41s remains)
INFO - root - 2017-12-16 09:55:30.361643: step 44150, loss = 0.51, batch loss = 0.25 (48.9 examples/sec; 0.164 sec/batch; 13h:06m:26s remains)
INFO - root - 2017-12-16 09:55:32.008925: step 44160, loss = 0.74, batch loss = 0.48 (49.3 examples/sec; 0.162 sec/batch; 12h:59m:03s remains)
INFO - root - 2017-12-16 09:55:33.634922: step 44170, loss = 0.52, batch loss = 0.26 (49.9 examples/sec; 0.160 sec/batch; 12h:50m:39s remains)
INFO - root - 2017-12-16 09:55:35.261312: step 44180, loss = 0.52, batch loss = 0.27 (48.2 examples/sec; 0.166 sec/batch; 13h:16m:54s remains)
INFO - root - 2017-12-16 09:55:36.914878: step 44190, loss = 0.55, batch loss = 0.30 (46.9 examples/sec; 0.170 sec/batch; 13h:38m:55s remains)
INFO - root - 2017-12-16 09:55:38.579927: step 44200, loss = 0.45, batch loss = 0.20 (47.4 examples/sec; 0.169 sec/batch; 13h:30m:53s remains)
2017-12-16 09:55:38.989660: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28969765 0.30864525 0.17365956 0.21617985 0.572129 1.1378131 1.5501294 1.4527202 1.1175244 0.96182108 1.1115301 1.037719 0.52424073 -0.05106163 -0.27403426][-0.4614948 -0.50984693 -0.65247989 -0.71797252 -0.55921376 -0.16982961 0.17541242 0.10716343 -0.088959217 -0.042552471 0.24462771 0.23010159 -0.28400993 -0.88963521 -1.2202196][-1.6305687 -1.7138624 -1.8848932 -2.079767 -2.1811953 -2.1092384 -2.007781 -2.0278938 -2.0257516 -1.7501749 -1.2789214 -1.1074336 -1.4142532 -1.9338489 -2.3116102][-2.7338006 -2.7750623 -2.8597372 -3.0574203 -3.2831597 -3.4802713 -3.5914273 -3.5916028 -3.4267519 -2.9953673 -2.453723 -2.1562102 -2.2728515 -2.6558094 -2.9984028][-3.2370002 -3.1204214 -2.9660382 -2.9322116 -3.0599082 -3.3753107 -3.6601596 -3.6550245 -3.3508768 -2.8978128 -2.4665897 -2.2633181 -2.3635025 -2.6568322 -2.9388037][-2.8236794 -2.4443524 -1.8447765 -1.3837142 -1.3204082 -1.6867242 -2.122931 -2.2121835 -1.9552468 -1.6120685 -1.3688065 -1.3556042 -1.6008351 -1.9752586 -2.3386922][-1.5118043 -0.81595039 0.27184224 1.1851099 1.4099829 0.89429 0.23601627 -0.065643072 -0.013266563 0.10028601 0.13079453 -0.076602936 -0.55879164 -1.1603132 -1.7049406][0.22948718 1.100908 2.5725467 3.839886 4.0295677 3.1101997 2.0815556 1.4695215 1.224088 1.0581264 0.88500524 0.53702092 -0.085390329 -0.83246624 -1.4784074][1.3644903 2.1261828 3.5995176 4.856554 4.8289108 3.6419375 2.3177593 1.4458261 0.95742965 0.64515972 0.39229679 0.03291297 -0.56427264 -1.2490497 -1.8107457][1.1872363 1.6101153 2.5622885 3.3435404 3.1788585 2.0693963 0.76036334 -0.19694281 -0.70208657 -0.95483291 -1.0944785 -1.2882476 -1.6631693 -2.112972 -2.4477365][-0.13829803 -0.040740013 0.38110876 0.74587417 0.51185822 -0.42850602 -1.5201352 -2.29575 -2.6214488 -2.6748252 -2.6337659 -2.5937946 -2.685806 -2.8672996 -2.936053][-1.8612561 -1.9803773 -1.882951 -1.7605221 -1.9610736 -2.5856717 -3.3265517 -3.7968209 -3.8817277 -3.7347655 -3.5097475 -3.3113773 -3.2056684 -3.1884058 -3.0837884][-3.0813322 -3.2893226 -3.3116698 -3.2653418 -3.3449173 -3.6467061 -4.0290942 -4.2282352 -4.1452641 -3.8770781 -3.5648646 -3.3112264 -3.1720033 -3.135525 -3.0226941][-3.3267338 -3.5134597 -3.5463245 -3.520999 -3.5245008 -3.6227355 -3.7483888 -3.7601776 -3.5947123 -3.3122725 -3.0257258 -2.8388948 -2.8142893 -2.9124894 -2.9359512][-2.8627603 -3.0037196 -3.016458 -2.9837179 -2.9494686 -2.9481797 -2.9406056 -2.8724835 -2.7344034 -2.552305 -2.38698 -2.32426 -2.4441147 -2.7042685 -2.9162028]]...]
INFO - root - 2017-12-16 09:55:40.620202: step 44210, loss = 0.55, batch loss = 0.29 (48.3 examples/sec; 0.166 sec/batch; 13h:16m:22s remains)
INFO - root - 2017-12-16 09:55:42.249628: step 44220, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 13h:00m:09s remains)
INFO - root - 2017-12-16 09:55:43.890019: step 44230, loss = 0.56, batch loss = 0.30 (48.7 examples/sec; 0.164 sec/batch; 13h:08m:51s remains)
INFO - root - 2017-12-16 09:55:45.522469: step 44240, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:06m:14s remains)
INFO - root - 2017-12-16 09:55:47.137763: step 44250, loss = 0.56, batch loss = 0.30 (46.0 examples/sec; 0.174 sec/batch; 13h:54m:59s remains)
INFO - root - 2017-12-16 09:55:48.826539: step 44260, loss = 0.56, batch loss = 0.31 (46.2 examples/sec; 0.173 sec/batch; 13h:52m:23s remains)
INFO - root - 2017-12-16 09:55:50.475323: step 44270, loss = 0.48, batch loss = 0.22 (48.3 examples/sec; 0.165 sec/batch; 13h:15m:02s remains)
INFO - root - 2017-12-16 09:55:52.111992: step 44280, loss = 0.59, batch loss = 0.33 (48.6 examples/sec; 0.165 sec/batch; 13h:10m:41s remains)
INFO - root - 2017-12-16 09:55:53.755336: step 44290, loss = 0.68, batch loss = 0.42 (47.8 examples/sec; 0.167 sec/batch; 13h:24m:08s remains)
INFO - root - 2017-12-16 09:55:55.403572: step 44300, loss = 0.50, batch loss = 0.24 (49.7 examples/sec; 0.161 sec/batch; 12h:53m:06s remains)
2017-12-16 09:55:55.841678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7170432 -1.9444027 -1.9519264 -1.7964149 -1.7995064 -2.1029112 -2.4592168 -2.5951622 -2.1174624 -1.0191882 0.40591908 1.5890555 1.9653347 1.4952271 0.38959026][-1.8513732 -1.8059824 -1.5287321 -1.1703225 -1.2680838 -1.799896 -2.361048 -2.3455167 -1.4807775 -0.16558361 1.1074564 1.7364461 1.6129136 0.76374793 -0.44256628][-2.2680798 -1.9285369 -1.4381835 -1.0026164 -1.0546703 -1.5866574 -2.064738 -1.6785722 -0.34263563 1.1741412 2.0673473 2.0382049 1.3492119 0.20203018 -0.95736217][-2.5861149 -2.0261989 -1.4526885 -1.1124765 -1.1314731 -1.4549066 -1.5140312 -0.682861 1.07335 2.7152822 3.1506288 2.5137255 1.3601985 -0.026149988 -1.2335658][-2.6942449 -2.1645036 -1.7292893 -1.5700889 -1.4419105 -1.1878505 -0.54309058 0.82171822 2.5939381 3.9033968 3.8237293 2.6776707 1.1141648 -0.42348254 -1.5608306][-2.563226 -2.2835987 -2.0448892 -1.8840997 -1.4714441 -0.49454582 1.0219254 2.8704941 4.1843691 4.5142717 3.7162793 2.1104472 0.27744365 -1.2237107 -2.073065][-2.2215428 -2.2067866 -2.0878751 -1.6800654 -0.75837052 0.98016 3.3490098 5.5611858 5.7501345 4.5247421 2.8005202 0.86637473 -0.89661431 -2.1230361 -2.6620975][-1.7400441 -1.7911338 -1.5544386 -0.81569767 0.5243237 2.6718051 5.5044231 7.7101631 6.2890825 3.6258967 1.2952998 -0.5819962 -1.9580216 -2.7588911 -3.0204237][-0.98422873 -0.95096838 -0.52216911 0.41998005 1.8389251 3.6710088 5.6576557 6.3596077 4.5384674 1.8462875 -0.35588622 -1.8329321 -2.6965215 -3.1479573 -3.1931522][-0.29876804 -0.05944109 0.57560253 1.5802352 2.6667979 3.6731875 4.2072611 3.5635946 1.8059328 -0.22659922 -1.8064139 -2.7347836 -3.1549287 -3.2765908 -3.0867424][0.0982275 0.58985734 1.3305554 2.088836 2.6131232 2.673032 2.07011 0.785938 -0.66598427 -1.9267521 -2.8236837 -3.2709503 -3.3419321 -3.1617136 -2.7244072][0.43679333 0.97042131 1.5427501 1.8012393 1.698272 1.0509281 -0.12084413 -1.5453632 -2.5972254 -3.1415365 -3.3810124 -3.4067335 -3.2421706 -2.8429718 -2.2128654][0.88103485 1.1529138 1.3010399 1.0364678 0.3580761 -0.69937503 -1.9846306 -3.1353686 -3.6740537 -3.6410842 -3.4054618 -3.1295552 -2.8123567 -2.2956235 -1.5978472][1.3961244 1.2315536 0.82718253 0.088958979 -0.93957973 -2.1062846 -3.1268184 -3.7304478 -3.7066207 -3.3244858 -2.8817947 -2.5232179 -2.1695588 -1.6483772 -0.945048][1.6657331 1.0454936 0.14535141 -0.8875519 -1.9297266 -2.8118222 -3.3370049 -3.3814945 -3.024101 -2.542582 -2.1456814 -1.8665266 -1.5413 -1.0340359 -0.32232952]]...]
INFO - root - 2017-12-16 09:55:57.467716: step 44310, loss = 0.52, batch loss = 0.26 (47.6 examples/sec; 0.168 sec/batch; 13h:26m:50s remains)
INFO - root - 2017-12-16 09:55:59.084199: step 44320, loss = 0.52, batch loss = 0.27 (50.6 examples/sec; 0.158 sec/batch; 12h:39m:32s remains)
INFO - root - 2017-12-16 09:56:00.733667: step 44330, loss = 0.55, batch loss = 0.30 (48.6 examples/sec; 0.165 sec/batch; 13h:11m:02s remains)
INFO - root - 2017-12-16 09:56:02.406245: step 44340, loss = 0.50, batch loss = 0.24 (48.3 examples/sec; 0.165 sec/batch; 13h:14m:44s remains)
INFO - root - 2017-12-16 09:56:04.038157: step 44350, loss = 0.55, batch loss = 0.29 (49.9 examples/sec; 0.160 sec/batch; 12h:50m:21s remains)
INFO - root - 2017-12-16 09:56:05.661679: step 44360, loss = 0.48, batch loss = 0.22 (48.0 examples/sec; 0.167 sec/batch; 13h:19m:36s remains)
INFO - root - 2017-12-16 09:56:07.288635: step 44370, loss = 0.54, batch loss = 0.28 (50.2 examples/sec; 0.159 sec/batch; 12h:45m:54s remains)
INFO - root - 2017-12-16 09:56:08.932781: step 44380, loss = 0.61, batch loss = 0.36 (51.0 examples/sec; 0.157 sec/batch; 12h:32m:51s remains)
INFO - root - 2017-12-16 09:56:10.549373: step 44390, loss = 0.52, batch loss = 0.26 (48.8 examples/sec; 0.164 sec/batch; 13h:07m:28s remains)
INFO - root - 2017-12-16 09:56:12.177367: step 44400, loss = 0.51, batch loss = 0.25 (49.3 examples/sec; 0.162 sec/batch; 12h:58m:54s remains)
2017-12-16 09:56:12.613569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8621109 -2.9214764 -2.9426937 -2.9616222 -2.9783671 -2.9686508 -2.9305692 -2.8552868 -2.7759209 -2.7076507 -2.6626396 -2.6509831 -2.6127934 -2.5381796 -2.4596384][-3.6474075 -3.7204247 -3.7402954 -3.7390232 -3.7195482 -3.6466753 -3.5676961 -3.4780397 -3.396297 -3.3487391 -3.3301015 -3.3660667 -3.3736038 -3.306788 -3.2143641][-4.2831278 -4.3205023 -4.3013062 -4.231986 -4.1105471 -3.9073675 -3.7299297 -3.6158338 -3.5514188 -3.5506668 -3.5987339 -3.7416229 -3.8761954 -3.9041247 -3.8440118][-4.47767 -4.4224834 -4.2768035 -4.0523224 -3.7424173 -3.35393 -3.0039315 -2.8056993 -2.7695494 -2.8617816 -3.0572395 -3.4160745 -3.8059642 -4.0293379 -4.0923371][-4.0052252 -3.7911253 -3.4608603 -3.0616152 -2.562335 -1.9547145 -1.3830268 -1.0338287 -1.0297005 -1.2319009 -1.5976117 -2.2378376 -2.9602115 -3.4755583 -3.7067618][-2.9885628 -2.6232855 -2.0895994 -1.5406564 -0.93516111 -0.13077736 0.72666836 1.2695398 1.2368743 0.91741586 0.3685112 -0.53424096 -1.545545 -2.3017492 -2.6931393][-1.8201575 -1.3418019 -0.63206828 0.073075056 0.69030023 1.5549335 2.6825116 3.4260657 3.275285 2.7697637 2.0498593 1.0282896 -0.099575281 -1.0144116 -1.5462966][-0.8386811 -0.38311028 0.44604397 1.1923513 1.7217915 2.4814484 3.6833026 4.5338945 4.1267729 3.428334 2.6957324 1.7121294 0.62041354 -0.3038013 -0.84556806][-0.20213294 0.19117713 0.96065307 1.603646 1.986129 2.5022657 3.1684091 3.5910885 3.2639258 2.6213534 1.911123 1.1112733 0.23868799 -0.53756809 -1.0115272][0.22569394 0.49773693 1.126029 1.6252544 1.8703806 2.0370247 2.1268442 2.139075 1.9255164 1.330596 0.57219458 -0.16604018 -0.80143917 -1.3708117 -1.703594][0.15743446 0.2852993 0.70849681 1.0830977 1.2617285 1.220829 0.99744534 0.85271454 0.70218754 0.13950515 -0.69199181 -1.4184344 -1.8663867 -2.1955259 -2.3495688][-0.62211657 -0.73535478 -0.57763731 -0.2062037 0.082226038 0.066488743 -0.24227595 -0.45538247 -0.597473 -1.071578 -1.8290391 -2.3722854 -2.5455945 -2.5971754 -2.5371397][-2.013634 -2.2715762 -2.2058692 -1.7861071 -1.3228874 -1.1679789 -1.3621037 -1.5492449 -1.6871866 -2.0994251 -2.6109037 -2.8459251 -2.8102744 -2.6572561 -2.4376915][-3.5993395 -3.8320458 -3.7644875 -3.2844541 -2.6971374 -2.3731205 -2.3467169 -2.3939254 -2.4837224 -2.76761 -2.999197 -2.9695067 -2.8308721 -2.6174912 -2.329551][-4.755538 -4.8557687 -4.7366352 -4.3146105 -3.7979484 -3.4328105 -3.2587738 -3.1998756 -3.2418261 -3.3665428 -3.3498206 -3.1431124 -2.9147327 -2.6669285 -2.298871]]...]
INFO - root - 2017-12-16 09:56:14.219941: step 44410, loss = 0.52, batch loss = 0.27 (51.3 examples/sec; 0.156 sec/batch; 12h:29m:19s remains)
INFO - root - 2017-12-16 09:56:15.852248: step 44420, loss = 0.58, batch loss = 0.33 (49.8 examples/sec; 0.161 sec/batch; 12h:51m:41s remains)
INFO - root - 2017-12-16 09:56:17.484755: step 44430, loss = 0.57, batch loss = 0.31 (49.2 examples/sec; 0.163 sec/batch; 13h:00m:30s remains)
INFO - root - 2017-12-16 09:56:19.122476: step 44440, loss = 0.59, batch loss = 0.33 (46.7 examples/sec; 0.171 sec/batch; 13h:43m:03s remains)
INFO - root - 2017-12-16 09:56:20.787112: step 44450, loss = 0.54, batch loss = 0.28 (49.2 examples/sec; 0.163 sec/batch; 13h:00m:37s remains)
INFO - root - 2017-12-16 09:56:22.411580: step 44460, loss = 0.65, batch loss = 0.39 (50.5 examples/sec; 0.159 sec/batch; 12h:41m:10s remains)
INFO - root - 2017-12-16 09:56:24.080953: step 44470, loss = 0.48, batch loss = 0.22 (42.2 examples/sec; 0.190 sec/batch; 15h:10m:36s remains)
INFO - root - 2017-12-16 09:56:25.709727: step 44480, loss = 0.53, batch loss = 0.27 (48.3 examples/sec; 0.166 sec/batch; 13h:15m:36s remains)
INFO - root - 2017-12-16 09:56:27.343433: step 44490, loss = 0.54, batch loss = 0.28 (49.4 examples/sec; 0.162 sec/batch; 12h:57m:51s remains)
INFO - root - 2017-12-16 09:56:28.958097: step 44500, loss = 0.58, batch loss = 0.32 (48.6 examples/sec; 0.165 sec/batch; 13h:10m:13s remains)
2017-12-16 09:56:29.382144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2199464 -3.3446398 -3.3746312 -3.3644786 -3.3852425 -3.4851248 -3.6703181 -3.8667767 -3.9044037 -3.8772202 -3.8855319 -3.8457108 -3.6480811 -3.2136681 -2.8071189][-3.316963 -3.4961331 -3.533627 -3.4891624 -3.4420462 -3.5494509 -3.848619 -4.2253275 -4.3761158 -4.34165 -4.3219633 -4.2739167 -4.0629244 -3.5475345 -3.1023657][-3.010412 -3.1145191 -3.0765371 -2.9328158 -2.8783286 -3.0550196 -3.4625492 -3.9852443 -4.2771664 -4.2510486 -4.1728635 -4.1361318 -3.9151697 -3.4451542 -3.0588906][-2.6036255 -2.5763612 -2.4458084 -2.2666578 -2.2279673 -2.3951547 -2.7847121 -3.3198822 -3.7078381 -3.6770513 -3.5279284 -3.5033898 -3.3856976 -3.117029 -2.8810618][-2.2113662 -2.0469394 -1.8217617 -1.5939834 -1.5491378 -1.5948341 -1.7414567 -2.0161715 -2.308655 -2.2828655 -2.211128 -2.329349 -2.43711 -2.4446528 -2.3305826][-1.8672234 -1.4198751 -0.97743261 -0.59934342 -0.39007437 -0.24479938 -0.0060944557 0.17495441 0.10408688 -0.033469439 -0.2878015 -0.78492904 -1.1145368 -1.3035043 -1.2561018][-1.7633231 -1.1022043 -0.35743165 0.35351014 0.93425727 1.4546216 2.1997082 3.0263445 3.2253253 2.7632082 2.0989363 1.0759525 0.33068705 -0.089328766 -0.19386244][-2.0952075 -1.4347491 -0.54925156 0.4367795 1.385745 2.3336952 3.5576046 4.7718668 5.248311 4.3681297 3.3813407 2.0412705 0.97946382 0.30736804 0.017789364][-2.7169945 -2.2891042 -1.5079823 -0.52025664 0.59926009 1.7377145 2.9570568 3.8637989 4.1582146 3.2873852 2.3531215 1.1330073 0.069724083 -0.6427145 -0.9626528][-3.3826866 -3.2199919 -2.675379 -1.856308 -0.86426616 0.09808135 0.8592217 1.1346266 1.14884 0.6911087 0.12234759 -0.7458961 -1.7072474 -2.3730352 -2.6021218][-4.0072351 -4.1349316 -3.8584018 -3.2897267 -2.59653 -1.949733 -1.5991492 -1.6767917 -1.7974157 -1.9770424 -2.255414 -2.7984104 -3.4861917 -3.984622 -4.0972438][-4.2300591 -4.5783048 -4.546442 -4.2316141 -3.8555446 -3.5449944 -3.5063014 -3.7110934 -3.8015625 -3.758359 -3.7217011 -3.9052253 -4.2379951 -4.5440631 -4.6485004][-3.8385544 -4.187171 -4.218864 -4.0986018 -3.963038 -3.8541317 -3.943763 -4.1250467 -4.1401258 -3.9823601 -3.7460284 -3.6664271 -3.7485855 -3.9742236 -4.1193385][-3.1706581 -3.3891888 -3.4213111 -3.3995862 -3.3860865 -3.3637443 -3.4251618 -3.5081525 -3.4839778 -3.2912631 -3.0369711 -2.867758 -2.8655167 -3.0627961 -3.2459033][-2.6479728 -2.7768483 -2.7871523 -2.7958484 -2.822032 -2.8176656 -2.8240774 -2.8161077 -2.7660172 -2.5942881 -2.3476706 -2.1552565 -2.1453454 -2.3367085 -2.5492167]]...]
INFO - root - 2017-12-16 09:56:30.996296: step 44510, loss = 0.59, batch loss = 0.33 (48.6 examples/sec; 0.164 sec/batch; 13h:09m:22s remains)
INFO - root - 2017-12-16 09:56:32.625051: step 44520, loss = 0.62, batch loss = 0.36 (47.8 examples/sec; 0.167 sec/batch; 13h:22m:45s remains)
INFO - root - 2017-12-16 09:56:34.242446: step 44530, loss = 0.52, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:04m:50s remains)
INFO - root - 2017-12-16 09:56:35.862120: step 44540, loss = 0.67, batch loss = 0.41 (50.1 examples/sec; 0.160 sec/batch; 12h:45m:42s remains)
INFO - root - 2017-12-16 09:56:37.488687: step 44550, loss = 0.52, batch loss = 0.26 (50.4 examples/sec; 0.159 sec/batch; 12h:42m:23s remains)
INFO - root - 2017-12-16 09:56:39.116277: step 44560, loss = 0.67, batch loss = 0.41 (49.8 examples/sec; 0.161 sec/batch; 12h:51m:40s remains)
INFO - root - 2017-12-16 09:56:40.750920: step 44570, loss = 0.56, batch loss = 0.30 (47.0 examples/sec; 0.170 sec/batch; 13h:37m:18s remains)
INFO - root - 2017-12-16 09:56:42.407911: step 44580, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:07m:37s remains)
INFO - root - 2017-12-16 09:56:44.030253: step 44590, loss = 0.53, batch loss = 0.27 (49.5 examples/sec; 0.162 sec/batch; 12h:56m:08s remains)
INFO - root - 2017-12-16 09:56:45.645761: step 44600, loss = 0.66, batch loss = 0.40 (49.3 examples/sec; 0.162 sec/batch; 12h:59m:24s remains)
2017-12-16 09:56:46.074511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2753515 -2.3472977 -2.5063076 -2.6961136 -2.8002772 -2.8608646 -2.8960352 -2.8710022 -2.8431337 -2.7526619 -2.3428283 -1.821255 -1.6333885 -1.6650015 -1.7232394][-2.5130367 -2.7328935 -2.9835424 -3.2548976 -3.4536126 -3.4698288 -3.2398229 -2.8464031 -2.6133633 -2.4533346 -1.9484825 -1.3949161 -1.3275974 -1.4184486 -1.3797784][-2.8247793 -3.2158628 -3.5247035 -3.8122182 -3.9880919 -3.8635082 -3.2920754 -2.5105996 -2.1209404 -1.9710379 -1.4567649 -0.92171967 -0.945953 -1.0855336 -1.0028412][-3.0831742 -3.5864263 -3.8568387 -4.0185986 -4.0223041 -3.6456962 -2.6729009 -1.5657613 -1.1894279 -1.2010351 -0.83853424 -0.44250643 -0.54717529 -0.70914781 -0.62436533][-2.9700649 -3.4278288 -3.4732089 -3.3630054 -3.1391811 -2.484715 -1.1182142 0.15843272 0.22909141 -0.21546555 -0.1888864 0.019772291 -0.15457106 -0.29737806 -0.26674867][-2.2839599 -2.5246854 -2.2412231 -1.8416876 -1.4424081 -0.57458627 1.1440625 2.525604 2.0822136 0.9492414 0.50228119 0.4567287 0.22603559 0.047097921 -0.092353821][-1.3523601 -1.343493 -0.75601065 -0.09828043 0.38324165 1.400095 3.5235965 5.1157026 4.039669 2.1223524 1.1366508 0.75327039 0.41117907 0.13950419 -0.17201328][-0.76429415 -0.520869 0.31727362 1.0909591 1.5084193 2.5455625 4.9046831 6.8266811 5.112998 2.6898448 1.3712544 0.68715906 0.26626611 0.0060431957 -0.32386303][-0.73651135 -0.37188435 0.50354671 1.2049735 1.3828545 2.0321653 3.692548 4.7929096 3.5914142 1.7399099 0.60729122 -0.080625057 -0.51383245 -0.6472764 -0.7906574][-1.0733212 -0.77950609 -0.1439631 0.35809851 0.33177376 0.51632833 1.3557942 1.8480852 0.99891472 -0.22879982 -1.0134771 -1.4717667 -1.6906 -1.6443099 -1.569927][-1.3911612 -1.289777 -1.0488808 -0.855705 -0.98799706 -1.0280249 -0.71740079 -0.62376308 -1.2802639 -2.0891256 -2.556931 -2.7231171 -2.6930423 -2.5274436 -2.3210206][-1.5678403 -1.5847626 -1.6410228 -1.6514648 -1.829432 -2.0517659 -2.1010232 -2.249378 -2.7633309 -3.2606111 -3.4666858 -3.3722744 -3.1681702 -2.9430652 -2.726469][-1.6196803 -1.6593052 -1.8052094 -1.9378147 -2.1827829 -2.5190814 -2.7706256 -3.0214281 -3.3691707 -3.6109238 -3.6267469 -3.4266357 -3.1893275 -3.0016537 -2.8471718][-1.6333401 -1.6483095 -1.7718202 -1.9523371 -2.2476377 -2.6381981 -2.9833961 -3.2161455 -3.3554344 -3.3842359 -3.2739558 -3.0847225 -2.9281921 -2.8419924 -2.8054674][-1.7252097 -1.7305806 -1.8164115 -1.9793956 -2.2358806 -2.5366435 -2.775435 -2.8883109 -2.8757243 -2.7596817 -2.6084881 -2.475872 -2.4147887 -2.4389899 -2.5018744]]...]
INFO - root - 2017-12-16 09:56:47.684435: step 44610, loss = 0.51, batch loss = 0.26 (49.3 examples/sec; 0.162 sec/batch; 12h:57m:55s remains)
INFO - root - 2017-12-16 09:56:49.323012: step 44620, loss = 0.54, batch loss = 0.28 (48.0 examples/sec; 0.167 sec/batch; 13h:19m:52s remains)
INFO - root - 2017-12-16 09:56:50.930769: step 44630, loss = 0.47, batch loss = 0.21 (51.3 examples/sec; 0.156 sec/batch; 12h:27m:57s remains)
INFO - root - 2017-12-16 09:56:52.556945: step 44640, loss = 0.54, batch loss = 0.29 (50.0 examples/sec; 0.160 sec/batch; 12h:47m:31s remains)
INFO - root - 2017-12-16 09:56:54.206889: step 44650, loss = 0.54, batch loss = 0.28 (48.2 examples/sec; 0.166 sec/batch; 13h:15m:55s remains)
INFO - root - 2017-12-16 09:56:55.858707: step 44660, loss = 0.51, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 13h:25m:11s remains)
INFO - root - 2017-12-16 09:56:57.479342: step 44670, loss = 0.55, batch loss = 0.29 (48.7 examples/sec; 0.164 sec/batch; 13h:07m:18s remains)
INFO - root - 2017-12-16 09:56:59.124218: step 44680, loss = 0.54, batch loss = 0.28 (49.2 examples/sec; 0.163 sec/batch; 13h:00m:23s remains)
INFO - root - 2017-12-16 09:57:00.779347: step 44690, loss = 0.69, batch loss = 0.43 (48.8 examples/sec; 0.164 sec/batch; 13h:06m:32s remains)
INFO - root - 2017-12-16 09:57:02.391016: step 44700, loss = 0.55, batch loss = 0.29 (51.8 examples/sec; 0.154 sec/batch; 12h:20m:55s remains)
2017-12-16 09:57:02.841119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3894222 -2.4820573 -2.530782 -2.5526536 -2.5772371 -2.59913 -2.6181479 -2.6426075 -2.665807 -2.7157059 -2.8164341 -2.8623111 -2.8204741 -2.7110181 -2.4995406][-2.5953479 -2.7712812 -2.8801551 -2.9508815 -3.0285137 -3.1045418 -3.173209 -3.2263212 -3.2395802 -3.3079898 -3.469557 -3.5267217 -3.440074 -3.2283466 -2.8551838][-2.786834 -2.9823349 -3.0876853 -3.1501424 -3.2247417 -3.3138919 -3.4125752 -3.4936688 -3.5155249 -3.6369381 -3.89849 -4.0383081 -3.9852448 -3.7647696 -3.2994585][-2.8753738 -2.9560642 -2.9243274 -2.8575261 -2.8163211 -2.8315597 -2.9108346 -2.9839938 -3.0079997 -3.2083254 -3.6091375 -3.9163449 -4.0206342 -3.944521 -3.5559211][-2.6116209 -2.4327061 -2.1605809 -1.8825822 -1.66623 -1.5698605 -1.6159461 -1.6605678 -1.6583576 -1.9571016 -2.5672963 -3.1139717 -3.4678018 -3.6677954 -3.5223625][-1.9791039 -1.4386001 -0.83282781 -0.28612447 0.15825677 0.39105749 0.38412046 0.38871074 0.41722703 -0.052950621 -0.96573257 -1.8102 -2.4871407 -3.052012 -3.2663298][-1.2887238 -0.36588764 0.57125258 1.39799 2.0912111 2.5159776 2.5907667 2.6559012 2.5988219 1.8425477 0.561851 -0.60723162 -1.6072336 -2.494077 -2.9882507][-1.096174 0.049951077 1.1648042 2.1476495 3.0435283 3.6729872 3.91102 4.1263885 3.9097278 2.8718407 1.3244975 -0.010596037 -1.1931821 -2.2534826 -2.8625865][-1.5898726 -0.57256114 0.3739922 1.1808715 1.9594901 2.4766228 2.5869782 2.7039526 2.600415 1.7466857 0.42458105 -0.69594371 -1.7102308 -2.5986738 -3.0065818][-2.2341306 -1.4817832 -0.76539993 -0.1438489 0.46146655 0.82480693 0.82118893 0.83512115 0.76483893 0.12968588 -0.85528767 -1.6910145 -2.459085 -3.0702915 -3.1929731][-2.7402544 -2.2913017 -1.8053045 -1.3384017 -0.89982033 -0.68081 -0.75207114 -0.77945459 -0.78802681 -1.159181 -1.8094349 -2.41407 -3.0110765 -3.4004631 -3.3268156][-2.970516 -2.8059123 -2.5423675 -2.2441921 -1.9873825 -1.9048265 -2.0131464 -2.0585749 -2.0237191 -2.1655653 -2.5031202 -2.8900099 -3.2936668 -3.4952364 -3.3187833][-2.8916702 -2.8805223 -2.7650762 -2.6105473 -2.4966559 -2.5108809 -2.6251273 -2.6741891 -2.6464624 -2.682514 -2.8449364 -3.0566511 -3.2699862 -3.3324313 -3.1348178][-2.6300416 -2.6557674 -2.612982 -2.5476229 -2.5102952 -2.5405302 -2.626936 -2.6786349 -2.6781602 -2.6994176 -2.7965722 -2.912626 -3.0045719 -2.994997 -2.8344502][-2.3626976 -2.3832977 -2.362227 -2.3212137 -2.2894425 -2.2950268 -2.3264315 -2.3524261 -2.3685675 -2.3993049 -2.4687257 -2.5509524 -2.6050906 -2.6032655 -2.5248537]]...]
INFO - root - 2017-12-16 09:57:04.468484: step 44710, loss = 0.58, batch loss = 0.32 (49.8 examples/sec; 0.161 sec/batch; 12h:50m:19s remains)
INFO - root - 2017-12-16 09:57:06.075072: step 44720, loss = 0.70, batch loss = 0.44 (48.4 examples/sec; 0.165 sec/batch; 13h:13m:12s remains)
INFO - root - 2017-12-16 09:57:07.708371: step 44730, loss = 0.65, batch loss = 0.39 (46.4 examples/sec; 0.172 sec/batch; 13h:46m:12s remains)
INFO - root - 2017-12-16 09:57:09.337359: step 44740, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 13h:04m:49s remains)
INFO - root - 2017-12-16 09:57:10.962238: step 44750, loss = 0.63, batch loss = 0.37 (48.4 examples/sec; 0.165 sec/batch; 13h:12m:59s remains)
INFO - root - 2017-12-16 09:57:12.587011: step 44760, loss = 0.59, batch loss = 0.33 (49.5 examples/sec; 0.162 sec/batch; 12h:54m:33s remains)
INFO - root - 2017-12-16 09:57:14.244581: step 44770, loss = 0.50, batch loss = 0.24 (48.6 examples/sec; 0.165 sec/batch; 13h:10m:11s remains)
INFO - root - 2017-12-16 09:57:15.881652: step 44780, loss = 0.47, batch loss = 0.22 (49.3 examples/sec; 0.162 sec/batch; 12h:58m:12s remains)
INFO - root - 2017-12-16 09:57:17.492620: step 44790, loss = 0.52, batch loss = 0.26 (49.6 examples/sec; 0.161 sec/batch; 12h:53m:14s remains)
INFO - root - 2017-12-16 09:57:19.124932: step 44800, loss = 0.52, batch loss = 0.26 (49.1 examples/sec; 0.163 sec/batch; 13h:00m:55s remains)
2017-12-16 09:57:19.579858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7154689 -2.8283598 -2.9803209 -3.0784035 -2.9081161 -2.566669 -2.2058673 -1.9703754 -1.8455677 -1.5759187 -1.4793162 -1.9908129 -2.72002 -3.2815 -3.6432354][-3.8530443 -3.9698198 -4.0782123 -4.0956779 -3.8226 -3.3560624 -2.8953962 -2.6547797 -2.6412632 -2.5671284 -2.6259499 -3.1245537 -3.73708 -4.081152 -4.1630774][-4.6064906 -4.7555647 -4.8018789 -4.7442846 -4.4021769 -3.8598542 -3.3356564 -3.0548723 -3.0856736 -3.1241684 -3.317488 -3.8674202 -4.4430118 -4.6115446 -4.4298296][-4.8301964 -4.9816437 -4.953723 -4.7924385 -4.36279 -3.7395825 -3.111877 -2.7630441 -2.7966185 -2.9159694 -3.2613263 -3.9209876 -4.5428619 -4.6761856 -4.3510933][-4.3971643 -4.516818 -4.35043 -4.0785913 -3.5729661 -2.8460009 -2.062891 -1.5973053 -1.6104635 -1.8147191 -2.3116903 -3.1388738 -3.9732361 -4.2651758 -3.9525247][-3.3488986 -3.3935518 -3.0564694 -2.6452754 -2.1534085 -1.4294233 -0.53395748 0.10713506 0.11294866 -0.22073174 -0.86731923 -1.8531827 -2.8906794 -3.449821 -3.3119802][-1.8858851 -1.7997233 -1.3418884 -0.80995846 -0.33591127 0.26314354 1.1130135 1.8065069 1.8127735 1.2564375 0.43684149 -0.66058195 -1.812721 -2.5937543 -2.6436064][-0.5209657 -0.33356714 0.16498923 0.69915414 1.0081444 1.3209984 1.9615424 2.5697172 2.4959323 1.7867362 0.87913704 -0.20173788 -1.3348541 -2.1383002 -2.2602985][0.16685963 0.29537153 0.64959812 0.97551179 1.0332706 1.0428984 1.3129027 1.6131742 1.4739487 0.88267708 0.11218405 -0.83086491 -1.6997182 -2.2505755 -2.2744408][0.035967112 0.00339365 0.13303375 0.2118423 0.069084406 -0.12459707 -0.12116909 0.0040831566 -0.098286152 -0.46141636 -1.0137233 -1.8101583 -2.4412951 -2.6669216 -2.4577518][-0.54739439 -0.68723619 -0.78185093 -0.92768478 -1.2224523 -1.5059794 -1.5254009 -1.3826429 -1.4056058 -1.6239614 -2.0243645 -2.712441 -3.1791067 -3.1173768 -2.6137803][-1.416984 -1.6036365 -1.8625207 -2.1687691 -2.4628432 -2.6174202 -2.4264097 -2.1832936 -2.2183738 -2.3743827 -2.6817174 -3.3082891 -3.6793575 -3.4024181 -2.6654713][-2.2833209 -2.5286047 -2.8865633 -3.1790071 -3.2607863 -3.0739968 -2.6215246 -2.3084037 -2.3925281 -2.5172877 -2.7230308 -3.3048854 -3.6763825 -3.3285122 -2.524627][-2.97406 -3.1660042 -3.4373062 -3.54818 -3.360121 -2.8727617 -2.2296319 -1.9074645 -2.0356364 -2.1386395 -2.2489228 -2.8320384 -3.2731359 -2.9854264 -2.2647903][-3.2046764 -3.2305534 -3.2971988 -3.2182257 -2.8755352 -2.3070092 -1.687997 -1.4603086 -1.6422791 -1.7224233 -1.7598763 -2.3369019 -2.8788939 -2.6867304 -2.0547597]]...]
INFO - root - 2017-12-16 09:57:21.198427: step 44810, loss = 0.50, batch loss = 0.24 (49.2 examples/sec; 0.163 sec/batch; 12h:59m:37s remains)
INFO - root - 2017-12-16 09:57:22.834492: step 44820, loss = 0.49, batch loss = 0.24 (46.2 examples/sec; 0.173 sec/batch; 13h:49m:58s remains)
INFO - root - 2017-12-16 09:57:24.464779: step 44830, loss = 0.47, batch loss = 0.21 (51.5 examples/sec; 0.155 sec/batch; 12h:24m:45s remains)
INFO - root - 2017-12-16 09:57:26.095030: step 44840, loss = 0.56, batch loss = 0.30 (49.7 examples/sec; 0.161 sec/batch; 12h:52m:16s remains)
INFO - root - 2017-12-16 09:57:27.714213: step 44850, loss = 0.56, batch loss = 0.30 (48.0 examples/sec; 0.167 sec/batch; 13h:19m:16s remains)
INFO - root - 2017-12-16 09:57:29.363700: step 44860, loss = 0.50, batch loss = 0.24 (48.5 examples/sec; 0.165 sec/batch; 13h:11m:02s remains)
INFO - root - 2017-12-16 09:57:30.979893: step 44870, loss = 0.54, batch loss = 0.28 (51.5 examples/sec; 0.155 sec/batch; 12h:25m:07s remains)
INFO - root - 2017-12-16 09:57:32.613863: step 44880, loss = 0.62, batch loss = 0.37 (48.0 examples/sec; 0.167 sec/batch; 13h:19m:32s remains)
INFO - root - 2017-12-16 09:57:34.217539: step 44890, loss = 0.57, batch loss = 0.31 (51.2 examples/sec; 0.156 sec/batch; 12h:28m:48s remains)
INFO - root - 2017-12-16 09:57:35.854382: step 44900, loss = 0.50, batch loss = 0.24 (48.8 examples/sec; 0.164 sec/batch; 13h:06m:33s remains)
2017-12-16 09:57:36.268839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.601624 -1.5116205 -1.4392414 -1.2835513 -1.4253719 -2.1358118 -3.0274115 -3.4124079 -3.1855311 -2.8424778 -2.3849583 -1.7515261 -1.1816982 -1.0348868 -1.6437157][-1.4655275 -1.5858591 -1.6493388 -1.7050476 -1.8347995 -2.3062758 -2.9057555 -3.063412 -2.8346837 -2.6755259 -2.4560959 -2.1825511 -1.9360958 -1.8571515 -2.2874315][-1.3592438 -1.606725 -1.7689061 -1.8921508 -1.8856913 -1.9728227 -2.1174691 -2.0978467 -2.016005 -2.1200244 -2.2133 -2.1887383 -2.1620092 -2.103636 -2.3803368][-1.2705021 -1.5338678 -1.7665818 -1.8435042 -1.6200254 -1.2084162 -0.84261537 -0.69797993 -0.83402085 -1.292502 -1.7278931 -1.9587057 -2.1177559 -1.9907147 -2.1071825][-1.3059877 -1.5186913 -1.6607704 -1.5523138 -1.0788585 -0.23519063 0.4999969 0.65334177 0.28946972 -0.52837288 -1.3254138 -1.8212409 -2.0384142 -1.8311155 -1.7523692][-1.4417484 -1.4344134 -1.4767156 -1.2186724 -0.43661141 0.74525928 1.6511629 1.660805 0.98502779 -0.19893122 -1.2694887 -1.9766891 -2.1764438 -1.8673396 -1.61766][-1.4257998 -1.2286922 -1.1903144 -0.71949315 0.303658 1.6975586 2.5421264 2.2178767 1.1333463 -0.34543943 -1.5928878 -2.3896952 -2.5353677 -2.0936093 -1.6425867][-1.6204932 -1.268528 -0.98117721 -0.2603116 0.85084867 2.213238 2.696274 1.9385173 0.541399 -0.88918746 -2.0487702 -2.7396154 -2.7470241 -2.1349607 -1.5201079][-2.1157427 -1.5294193 -0.94150114 -0.15446091 0.75443649 1.7208245 1.6973193 0.73893404 -0.60113955 -1.747967 -2.6037998 -2.9973507 -2.7529795 -2.0022924 -1.3415854][-2.6334045 -1.8479418 -1.1263019 -0.53943205 -0.05696702 0.41672993 0.11939692 -0.77167761 -1.93123 -2.7934475 -3.24481 -3.1541674 -2.56147 -1.6852772 -1.1578336][-3.0722933 -2.3407967 -1.7925277 -1.4849386 -1.3748429 -1.2921447 -1.5850399 -2.2947614 -3.2449782 -3.7291014 -3.7575221 -3.2260547 -2.2922575 -1.4586096 -1.2122605][-3.507206 -3.0724394 -2.7312248 -2.5318213 -2.5973933 -2.739275 -3.0192952 -3.6156449 -4.2666969 -4.3586721 -4.0424428 -3.2874205 -2.3248262 -1.6780113 -1.7034984][-3.9121275 -3.7409058 -3.518353 -3.3145487 -3.4113903 -3.6555419 -3.9262917 -4.4325733 -4.8538151 -4.6495962 -4.0427465 -3.2177417 -2.4774966 -2.1492722 -2.4564817][-4.0614424 -4.0622139 -3.8839245 -3.7286568 -3.8006144 -4.0150275 -4.2516947 -4.7152438 -5.0302577 -4.7580028 -4.0817165 -3.3216295 -2.843148 -2.8615713 -3.2493987][-3.79178 -3.8764787 -3.7415185 -3.6313882 -3.6671591 -3.7769594 -3.9804907 -4.4017258 -4.683856 -4.5455184 -4.0166292 -3.4610319 -3.2438655 -3.4507349 -3.7902997]]...]
INFO - root - 2017-12-16 09:57:37.911136: step 44910, loss = 0.64, batch loss = 0.38 (48.7 examples/sec; 0.164 sec/batch; 13h:07m:00s remains)
INFO - root - 2017-12-16 09:57:39.548708: step 44920, loss = 0.63, batch loss = 0.37 (49.0 examples/sec; 0.163 sec/batch; 13h:02m:27s remains)
INFO - root - 2017-12-16 09:57:41.189286: step 44930, loss = 0.53, batch loss = 0.28 (47.2 examples/sec; 0.170 sec/batch; 13h:32m:48s remains)
INFO - root - 2017-12-16 09:57:42.823289: step 44940, loss = 0.47, batch loss = 0.21 (50.0 examples/sec; 0.160 sec/batch; 12h:47m:26s remains)
INFO - root - 2017-12-16 09:57:44.449785: step 44950, loss = 0.56, batch loss = 0.30 (49.0 examples/sec; 0.163 sec/batch; 13h:02m:30s remains)
INFO - root - 2017-12-16 09:57:46.106339: step 44960, loss = 0.51, batch loss = 0.25 (46.8 examples/sec; 0.171 sec/batch; 13h:38m:29s remains)
INFO - root - 2017-12-16 09:57:47.738687: step 44970, loss = 0.51, batch loss = 0.25 (47.6 examples/sec; 0.168 sec/batch; 13h:24m:50s remains)
INFO - root - 2017-12-16 09:57:49.366369: step 44980, loss = 0.56, batch loss = 0.30 (47.4 examples/sec; 0.169 sec/batch; 13h:28m:02s remains)
INFO - root - 2017-12-16 09:57:51.007297: step 44990, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:12m:18s remains)
INFO - root - 2017-12-16 09:57:52.636902: step 45000, loss = 0.61, batch loss = 0.35 (49.2 examples/sec; 0.163 sec/batch; 12h:59m:54s remains)
2017-12-16 09:57:53.096657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2957382 -1.386085 -1.6711497 -2.0885048 -2.4907372 -2.806262 -2.9660773 -2.9847915 -2.9454715 -2.94145 -2.9180026 -2.701827 -2.3827622 -2.146538 -2.0364242][-1.63788 -1.8751595 -2.3693509 -3.0199237 -3.6142476 -4.0045171 -4.1288662 -4.0888319 -3.9507017 -3.9139547 -3.8649139 -3.596561 -3.1696975 -2.831244 -2.6338868][-2.1558642 -2.5161402 -3.0939381 -3.7873859 -4.3637242 -4.6542282 -4.6421061 -4.4510145 -4.2879272 -4.3094568 -4.3213677 -4.1659408 -3.8041329 -3.5037775 -3.3060567][-2.5692191 -2.9387884 -3.4058681 -3.9151855 -4.2598853 -4.2124147 -3.8817148 -3.4899678 -3.4018412 -3.5730453 -3.8388398 -3.9958205 -3.9184859 -3.8020744 -3.7544637][-2.6959143 -2.9439402 -3.2015948 -3.4443536 -3.3411679 -2.7593324 -1.9528588 -1.2904929 -1.2681667 -1.7901742 -2.5034461 -3.0549562 -3.307256 -3.4369678 -3.5977592][-2.5524909 -2.5798118 -2.589668 -2.44078 -1.7937009 -0.63170624 0.71955538 1.6834495 1.5773876 0.50834441 -0.69562316 -1.5707414 -2.1246605 -2.5632262 -3.0548935][-2.2144206 -2.0300889 -1.7585052 -1.2713451 -0.24079204 1.4496982 3.3522565 4.6833172 4.2051277 2.5350573 0.93162441 -0.13990593 -0.91409111 -1.7392569 -2.6141844][-1.8848073 -1.5341507 -1.0825453 -0.41397822 0.81870031 2.7151105 4.8829203 6.3471012 5.3583832 3.4513333 1.8482034 0.7915082 -0.12766862 -1.2794183 -2.4054108][-1.8296289 -1.4308975 -0.96021032 -0.30976057 0.86517763 2.509825 4.1196432 4.9733067 4.1452103 2.7139356 1.6154916 0.88097954 -0.039234161 -1.3208127 -2.5185757][-1.9994687 -1.7411962 -1.4649435 -1.0317038 -0.25302649 0.77649641 1.6941478 1.9857657 1.4601767 0.66510105 0.27262926 -0.035159826 -0.77920234 -1.9308401 -2.8850288][-2.2373824 -2.2882733 -2.3731503 -2.2802622 -1.9896244 -1.5037452 -1.0967492 -1.0754644 -1.382633 -1.6379986 -1.5539073 -1.5196768 -1.9627575 -2.7492073 -3.2756751][-2.3293173 -2.6350958 -2.9798865 -3.2042291 -3.2940059 -3.2008259 -3.1571014 -3.2777305 -3.421926 -3.3494363 -3.0346594 -2.8515592 -2.9910264 -3.3501794 -3.4557061][-2.2478807 -2.6208718 -3.0524197 -3.4312751 -3.693243 -3.8052645 -3.9113038 -4.0360804 -4.0861473 -3.9209633 -3.6307788 -3.4179506 -3.3774815 -3.417696 -3.285893][-1.9836502 -2.2942588 -2.6755643 -3.0227063 -3.2632442 -3.4099088 -3.5138016 -3.6177306 -3.6433794 -3.555131 -3.3918395 -3.2357435 -3.1507149 -3.0655932 -2.8758862][-1.7316837 -1.9271727 -2.1766157 -2.4095762 -2.5645103 -2.65012 -2.7083321 -2.7673731 -2.7902451 -2.7596667 -2.6844974 -2.6118023 -2.5503194 -2.463408 -2.3026237]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 09:57:55.311840: step 45010, loss = 0.70, batch loss = 0.44 (47.5 examples/sec; 0.168 sec/batch; 13h:27m:07s remains)
INFO - root - 2017-12-16 09:57:56.966185: step 45020, loss = 0.55, batch loss = 0.29 (48.8 examples/sec; 0.164 sec/batch; 13h:05m:59s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 09:57:58.594420: step 45030, loss = 0.67, batch loss = 0.41 (49.3 examples/sec; 0.162 sec/batch; 12h:57m:17s remains)
INFO - root - 2017-12-16 09:58:00.239061: step 45040, loss = 0.56, batch loss = 0.30 (48.9 examples/sec; 0.164 sec/batch; 13h:03m:37s remains)
INFO - root - 2017-12-16 09:58:01.867500: step 45050, loss = 0.66, batch loss = 0.40 (50.6 examples/sec; 0.158 sec/batch; 12h:37m:52s remains)
INFO - root - 2017-12-16 09:58:03.485704: step 45060, loss = 0.55, batch loss = 0.29 (47.9 examples/sec; 0.167 sec/batch; 13h:20m:53s remains)
INFO - root - 2017-12-16 09:58:05.136262: step 45070, loss = 0.53, batch loss = 0.27 (48.6 examples/sec; 0.165 sec/batch; 13h:08m:03s remains)
INFO - root - 2017-12-16 09:58:06.758903: step 45080, loss = 0.58, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 12h:55m:58s remains)
INFO - root - 2017-12-16 09:58:08.417963: step 45090, loss = 0.54, batch loss = 0.28 (47.0 examples/sec; 0.170 sec/batch; 13h:35m:24s remains)
INFO - root - 2017-12-16 09:58:10.044625: step 45100, loss = 0.52, batch loss = 0.26 (50.7 examples/sec; 0.158 sec/batch; 12h:35m:32s remains)
2017-12-16 09:58:10.505787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2061782 -2.0242903 -2.1443822 -2.5207789 -2.8648729 -2.9553137 -2.8302784 -2.6474574 -2.5224607 -2.5229545 -2.4364927 -2.1867039 -1.9698846 -1.9283404 -1.9121398][-3.1451867 -2.9244764 -2.9662802 -3.2292237 -3.3637409 -3.2153394 -2.8935022 -2.5636373 -2.3729136 -2.4288683 -2.3601084 -1.9970132 -1.6494756 -1.5560495 -1.4515166][-3.6733885 -3.3698342 -3.3153284 -3.3830972 -3.2785189 -2.9421616 -2.4721012 -2.0493255 -1.8522732 -1.9867485 -2.0691376 -1.7849827 -1.4445603 -1.289686 -1.1111541][-3.5828357 -3.1953073 -3.075038 -2.9481413 -2.6331744 -2.1080365 -1.4621409 -0.94770622 -0.7268002 -0.98431396 -1.3099666 -1.2746671 -1.1245296 -0.97897112 -0.77215981][-3.16688 -2.7133031 -2.431473 -2.0918126 -1.5960674 -0.94976759 -0.16849136 0.44380307 0.60832357 0.2418623 -0.29979849 -0.63586247 -0.83673751 -0.85682356 -0.77524507][-2.7398789 -2.1981897 -1.6465135 -1.0671844 -0.43239379 0.33820796 1.2493906 1.9709842 2.131005 1.7102029 0.94505811 0.22049785 -0.36141205 -0.6940546 -0.9185673][-2.5230713 -1.7614832 -0.92082882 -0.095359564 0.74632907 1.6957934 2.8245246 3.718961 3.9125693 3.3912952 2.4502351 1.4121501 0.53116131 -0.053905487 -0.48866642][-2.5416226 -1.4884095 -0.3316617 0.712451 1.6363552 2.5837762 3.7728679 4.7499304 4.9433432 4.3511791 3.2908547 2.1609647 1.2440016 0.65985894 0.2459085][-2.7667322 -1.5806488 -0.35969162 0.62333679 1.3752198 2.0712841 2.8452685 3.3705924 3.3704374 2.8643091 2.12338 1.3312635 0.73409128 0.4391315 0.31438756][-2.7783337 -1.7071941 -0.72447574 -0.082200289 0.16599607 0.38232708 0.71150613 0.89914703 0.77639675 0.452116 0.081775188 -0.32653093 -0.59622586 -0.59098446 -0.47822714][-2.2537968 -1.3896813 -0.78530431 -0.65108621 -0.97411454 -1.2332548 -1.2066491 -1.2013295 -1.3421817 -1.5104078 -1.672904 -1.8400906 -1.9198734 -1.8139904 -1.6144015][-1.3212156 -0.59784532 -0.32948995 -0.63756752 -1.3807788 -1.9428549 -2.1191251 -2.1896448 -2.283838 -2.3429327 -2.3926013 -2.4657888 -2.4974322 -2.42271 -2.2647185][-0.31310749 0.31528544 0.44400263 -0.054280996 -1.0079713 -1.6975667 -1.9491066 -2.0165706 -2.0772552 -2.1245382 -2.1727319 -2.1936898 -2.1877518 -2.1520407 -2.0335739][0.29252815 0.95004821 1.1432009 0.65522337 -0.30602551 -0.9814893 -1.214233 -1.2656729 -1.3204302 -1.3689042 -1.3939016 -1.394978 -1.3977821 -1.4005995 -1.2712196][0.11506748 0.78410006 1.0922697 0.77438807 0.027737141 -0.48759389 -0.65234125 -0.67460322 -0.68995285 -0.71236193 -0.71263909 -0.706619 -0.72108054 -0.74551725 -0.59720111]]...]
INFO - root - 2017-12-16 09:58:12.145771: step 45110, loss = 0.48, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 13h:09m:23s remains)
INFO - root - 2017-12-16 09:58:13.779642: step 45120, loss = 0.54, batch loss = 0.28 (50.1 examples/sec; 0.160 sec/batch; 12h:45m:33s remains)
INFO - root - 2017-12-16 09:58:15.406801: step 45130, loss = 0.63, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 13h:20m:44s remains)
INFO - root - 2017-12-16 09:58:17.052736: step 45140, loss = 0.53, batch loss = 0.27 (49.0 examples/sec; 0.163 sec/batch; 13h:01m:57s remains)
INFO - root - 2017-12-16 09:58:18.672018: step 45150, loss = 0.64, batch loss = 0.38 (49.3 examples/sec; 0.162 sec/batch; 12h:57m:34s remains)
INFO - root - 2017-12-16 09:58:20.302184: step 45160, loss = 0.53, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:05m:49s remains)
INFO - root - 2017-12-16 09:58:21.921680: step 45170, loss = 0.49, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 12h:56m:29s remains)
INFO - root - 2017-12-16 09:58:23.591709: step 45180, loss = 0.49, batch loss = 0.23 (47.2 examples/sec; 0.170 sec/batch; 13h:31m:44s remains)
INFO - root - 2017-12-16 09:58:25.261656: step 45190, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:10m:50s remains)
INFO - root - 2017-12-16 09:58:26.893062: step 45200, loss = 0.55, batch loss = 0.29 (48.6 examples/sec; 0.165 sec/batch; 13h:08m:17s remains)
2017-12-16 09:58:27.330613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0255399 -3.2935817 -3.3510013 -3.0619314 -2.5916748 -2.2882197 -2.4350457 -3.0402272 -3.8307338 -4.5659084 -4.7195196 -4.26024 -3.7417321 -3.3681324 -3.1093981][-3.5928111 -3.717063 -3.5360227 -2.8349507 -2.0036981 -1.352926 -1.2931342 -1.9215329 -2.9591293 -4.1785216 -4.6696091 -4.2623782 -3.5885816 -3.0253735 -2.6758389][-4.07946 -3.9372125 -3.4520257 -2.2430308 -1.023999 -0.0786984 0.25449491 -0.36112356 -1.7589707 -3.4687843 -4.3290639 -4.0777946 -3.3491092 -2.6502743 -2.1762094][-4.2140126 -3.8077607 -3.0088222 -1.4249988 0.23081279 1.5402431 2.0818136 1.3545134 -0.43676257 -2.5507622 -3.7616282 -3.7665873 -3.2053683 -2.5442 -1.9867512][-3.9932587 -3.5344806 -2.5588913 -0.75460148 1.3420508 3.1335003 4.0505028 3.2427366 1.0520332 -1.4086554 -2.9903517 -3.4464145 -3.2177896 -2.7585156 -2.2433751][-3.4960523 -3.2273474 -2.3564367 -0.61072433 1.7805369 4.0941963 5.5832586 4.996418 2.6587021 -0.039027452 -1.9908834 -2.9996538 -3.2328539 -3.0734053 -2.7048478][-2.5982344 -2.751018 -2.263736 -0.91278851 1.2767761 3.7542078 5.8235035 5.9053526 3.9367883 1.3019018 -0.96127164 -2.4320428 -3.107162 -3.2446365 -3.0964859][-1.3206733 -1.8971314 -1.8956101 -1.0332433 0.71557212 2.7980788 4.6857615 5.5519409 4.4854746 2.2234123 -0.10299635 -1.8293365 -2.7760599 -3.1430042 -3.1937973][-0.16944456 -1.0491661 -1.4428923 -0.86925709 0.46832991 2.0983555 3.5606239 4.50268 4.2852249 2.7113354 0.63159633 -1.2008625 -2.3655429 -2.900636 -3.0793009][0.21636796 -0.78224361 -1.3175763 -0.89977229 0.1707871 1.4782362 2.6639774 3.4244807 3.4922764 2.5397818 0.9725492 -0.79387534 -2.1161757 -2.6961353 -2.8617239][-0.1682775 -1.1328591 -1.4881531 -1.0790991 -0.19733357 0.93124413 2.0252054 2.6222498 2.5574815 2.045963 0.97738814 -0.53819466 -1.8068299 -2.3928988 -2.5420859][-0.89865637 -1.6409848 -1.7030587 -1.2828212 -0.5603224 0.41864824 1.423399 1.9194181 1.876714 1.5228119 0.74346566 -0.48247814 -1.5234232 -1.9838939 -2.1220827][-1.4659631 -1.9801629 -1.8384002 -1.4252436 -0.85637593 -0.11465001 0.6920898 1.2180521 1.3706357 1.0961382 0.34517717 -0.69477212 -1.4258889 -1.6491909 -1.7417235][-1.5711889 -2.0088155 -2.0023329 -1.6868293 -1.2546984 -0.73500896 -0.15769362 0.42346096 0.82811189 0.65623617 -0.17373347 -1.1118867 -1.5856407 -1.6205387 -1.6758225][-1.7528775 -2.1136892 -2.2719703 -2.131109 -1.7751939 -1.3758663 -0.94538081 -0.43566382 0.05079484 -0.075142384 -0.88559353 -1.7020257 -1.9756311 -1.8640172 -1.8480523]]...]
INFO - root - 2017-12-16 09:58:28.950495: step 45210, loss = 0.58, batch loss = 0.32 (49.4 examples/sec; 0.162 sec/batch; 12h:54m:39s remains)
INFO - root - 2017-12-16 09:58:30.571161: step 45220, loss = 0.51, batch loss = 0.25 (49.9 examples/sec; 0.160 sec/batch; 12h:47m:14s remains)
INFO - root - 2017-12-16 09:58:32.200266: step 45230, loss = 0.57, batch loss = 0.31 (48.8 examples/sec; 0.164 sec/batch; 13h:04m:47s remains)
INFO - root - 2017-12-16 09:58:33.823870: step 45240, loss = 0.62, batch loss = 0.37 (50.4 examples/sec; 0.159 sec/batch; 12h:39m:51s remains)
INFO - root - 2017-12-16 09:58:35.451923: step 45250, loss = 0.49, batch loss = 0.23 (49.0 examples/sec; 0.163 sec/batch; 13h:02m:17s remains)
INFO - root - 2017-12-16 09:58:37.096071: step 45260, loss = 0.54, batch loss = 0.28 (48.9 examples/sec; 0.164 sec/batch; 13h:03m:40s remains)
INFO - root - 2017-12-16 09:58:38.738300: step 45270, loss = 0.57, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:12m:05s remains)
INFO - root - 2017-12-16 09:58:40.361974: step 45280, loss = 0.56, batch loss = 0.30 (49.3 examples/sec; 0.162 sec/batch; 12h:56m:47s remains)
INFO - root - 2017-12-16 09:58:41.983671: step 45290, loss = 0.49, batch loss = 0.23 (50.8 examples/sec; 0.158 sec/batch; 12h:33m:58s remains)
INFO - root - 2017-12-16 09:58:43.673922: step 45300, loss = 0.50, batch loss = 0.25 (49.2 examples/sec; 0.163 sec/batch; 12h:58m:52s remains)
2017-12-16 09:58:44.111536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1638634 -2.3610034 -2.5650506 -2.6800921 -2.6983163 -2.6126025 -2.4289105 -2.22527 -2.0253694 -1.8937165 -1.8640743 -1.8423152 -1.8065891 -1.7770269 -1.7659467][-1.7771428 -1.7055306 -1.7284956 -1.8505597 -1.9560006 -2.0098858 -2.0464134 -2.1108665 -1.9762211 -1.7219993 -1.5757943 -1.496004 -1.4553924 -1.4889598 -1.6557527][-0.851612 -0.64515984 -0.62315881 -0.76632929 -0.94178069 -1.0351567 -1.1525046 -1.3630028 -1.4375117 -1.287891 -1.1709201 -1.1147434 -1.1194732 -1.1766886 -1.3199575][0.15181184 0.33873487 0.28461027 0.055483341 -0.11444259 -0.1318469 -0.16032124 -0.30588841 -0.49812579 -0.61414146 -0.69089389 -0.73868132 -0.84323311 -0.92655146 -0.90534711][0.71812177 0.83571935 0.70684195 0.43567896 0.34913278 0.56143737 0.77959323 0.85075617 0.63690615 0.27069473 -0.1851089 -0.59781444 -0.89872158 -0.94419205 -0.75583577][0.68945813 0.80109572 0.65817857 0.47359204 0.63742709 1.2164574 1.8678424 2.2266362 1.9585454 1.2112215 0.31758475 -0.44027936 -0.92269421 -1.0566423 -0.92086232][0.13518977 0.30032468 0.29885387 0.34008574 0.82504106 1.8543808 3.0400007 3.868048 3.4391015 2.231055 0.97208047 -0.040949583 -0.70356321 -1.081497 -1.1693326][-0.85728073 -0.64213288 -0.48536503 -0.22608328 0.59901261 1.9462326 3.4981568 4.6799183 3.9810293 2.4460919 1.0122411 -0.02391386 -0.72218168 -1.2558634 -1.5288794][-1.9317939 -1.6844984 -1.4737217 -1.088884 -0.0919013 1.2391489 2.504288 3.1079438 2.5576437 1.3932173 0.22414756 -0.621786 -1.1855334 -1.6001503 -1.8465323][-2.504611 -2.1985338 -1.9660103 -1.5565007 -0.62400448 0.34391785 1.0097377 1.0863945 0.660872 -0.028131008 -0.720001 -1.26237 -1.6193178 -1.8217769 -1.9107372][-2.4099746 -2.0464516 -1.7262496 -1.293545 -0.61232913 -0.16011095 -0.10957265 -0.39932895 -0.77412164 -1.0967033 -1.4012055 -1.6656654 -1.7884283 -1.7724991 -1.7098403][-1.9365709 -1.5052688 -1.1712099 -0.83825946 -0.49625576 -0.59077263 -0.96658874 -1.4252197 -1.7802429 -1.8920538 -1.8972465 -1.8291298 -1.6595087 -1.4576354 -1.348194][-1.3454355 -0.94390178 -0.69971573 -0.5771333 -0.56465888 -0.96977258 -1.5654187 -2.134656 -2.4540303 -2.4185393 -2.1822274 -1.8528388 -1.4879766 -1.1683248 -1.0457038][-0.82239962 -0.50474846 -0.3804729 -0.43001568 -0.61459291 -1.0917912 -1.735213 -2.300997 -2.5617747 -2.4084287 -2.0160398 -1.5641663 -1.1234304 -0.83786952 -0.76475704][-0.60873652 -0.4633677 -0.4122448 -0.49712837 -0.73046827 -1.1654562 -1.6989648 -2.1590557 -2.29398 -2.0175884 -1.5578361 -1.1239836 -0.76894343 -0.57472992 -0.5585742]]...]
INFO - root - 2017-12-16 09:58:45.743150: step 45310, loss = 0.56, batch loss = 0.30 (49.9 examples/sec; 0.160 sec/batch; 12h:47m:45s remains)
INFO - root - 2017-12-16 09:58:47.378492: step 45320, loss = 0.59, batch loss = 0.33 (50.0 examples/sec; 0.160 sec/batch; 12h:45m:15s remains)
INFO - root - 2017-12-16 09:58:49.006950: step 45330, loss = 0.57, batch loss = 0.31 (49.8 examples/sec; 0.161 sec/batch; 12h:48m:26s remains)
INFO - root - 2017-12-16 09:58:50.629187: step 45340, loss = 0.52, batch loss = 0.27 (49.3 examples/sec; 0.162 sec/batch; 12h:56m:37s remains)
INFO - root - 2017-12-16 09:58:52.252598: step 45350, loss = 0.52, batch loss = 0.26 (50.1 examples/sec; 0.160 sec/batch; 12h:44m:50s remains)
INFO - root - 2017-12-16 09:58:53.914310: step 45360, loss = 0.59, batch loss = 0.33 (46.9 examples/sec; 0.171 sec/batch; 13h:36m:10s remains)
INFO - root - 2017-12-16 09:58:55.576378: step 45370, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 12h:57m:51s remains)
INFO - root - 2017-12-16 09:58:57.227093: step 45380, loss = 0.51, batch loss = 0.25 (48.4 examples/sec; 0.165 sec/batch; 13h:10m:16s remains)
INFO - root - 2017-12-16 09:58:58.851261: step 45390, loss = 0.49, batch loss = 0.24 (49.0 examples/sec; 0.163 sec/batch; 13h:01m:48s remains)
INFO - root - 2017-12-16 09:59:00.519473: step 45400, loss = 0.48, batch loss = 0.22 (48.1 examples/sec; 0.166 sec/batch; 13h:15m:26s remains)
2017-12-16 09:59:00.938166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4400492 -3.2876339 -2.5844624 -1.324035 -0.56587279 -0.76621139 -1.2009989 -1.1606303 -1.2431638 -2.0552535 -3.1895609 -3.9176912 -3.8303504 -3.1434116 -2.3962386][-3.8391266 -3.9116819 -3.4657669 -2.5519116 -2.0042889 -2.0981255 -2.2111154 -1.829106 -1.6443707 -2.2468941 -3.2573843 -3.9375553 -3.8835578 -3.2267778 -2.4729695][-4.279695 -4.431922 -4.1844263 -3.5406651 -3.0882502 -3.0244346 -2.7585881 -2.0666516 -1.6436909 -2.053539 -2.9045303 -3.5736105 -3.624536 -3.0824764 -2.421653][-4.7208905 -4.860323 -4.6582341 -4.042973 -3.4453645 -3.0563798 -2.4849331 -1.6890914 -1.2800448 -1.5757209 -2.2756259 -2.96815 -3.1947529 -2.8582664 -2.3173883][-4.8949018 -4.9815688 -4.7189484 -3.9097455 -2.8161366 -1.8667601 -1.0185983 -0.32295036 -0.23497295 -0.78609574 -1.5966344 -2.4340565 -2.8517983 -2.6896398 -2.2386079][-4.5885286 -4.5173292 -4.0160079 -2.780266 -0.86816108 0.86950016 1.9073594 2.1148636 1.3132029 -0.063548326 -1.4101998 -2.412077 -2.8360569 -2.6668186 -2.1858079][-4.143425 -3.8377573 -2.9374971 -1.2043141 1.500623 4.0908833 5.2865458 4.6729012 2.5273983 -0.068460464 -1.9989069 -2.9859092 -3.1730154 -2.7797985 -2.1509748][-3.8775358 -3.4275875 -2.2947896 -0.35093021 2.7455127 5.9979734 7.1491413 5.8813581 2.745156 -0.68692732 -2.9801941 -3.8347521 -3.6716743 -2.965379 -2.1429541][-3.7719226 -3.369338 -2.2730465 -0.64629436 1.8091605 4.4533081 5.8190546 4.904314 1.7634952 -1.79339 -4.030962 -4.5942526 -4.057703 -3.0598295 -2.0731535][-3.7755232 -3.5569041 -2.614506 -1.3812891 0.24312758 2.1639698 3.5467479 3.0794394 0.44054389 -2.7192323 -4.6252441 -4.9374871 -4.1881685 -3.0347276 -1.9750274][-3.9458113 -3.9042063 -3.1547027 -2.2734473 -1.2634618 0.017451048 1.285218 1.3179166 -0.52663219 -2.9934354 -4.5267811 -4.739665 -4.0147581 -2.9091766 -1.8896194][-3.9997487 -4.1613226 -3.7224865 -3.1997671 -2.5925932 -1.6899281 -0.50364983 -0.047638416 -1.1254867 -2.9216595 -4.1410704 -4.3347025 -3.7516029 -2.7816558 -1.8501552][-3.9984715 -4.2925968 -4.0850821 -3.7609653 -3.326797 -2.5523617 -1.4558873 -0.81847334 -1.419655 -2.7329557 -3.7286005 -3.9329453 -3.4559278 -2.607404 -1.8052691][-3.9485674 -4.2190437 -4.0625272 -3.732461 -3.2645755 -2.5509055 -1.6093612 -0.98904657 -1.3706928 -2.4222612 -3.2820935 -3.5006757 -3.0905352 -2.3495669 -1.6829188][-3.6771793 -3.8115349 -3.5682168 -3.1642234 -2.6654911 -2.0473154 -1.2716337 -0.76460338 -1.1254193 -2.0764272 -2.8567681 -3.074096 -2.6999314 -2.0253694 -1.4932]]...]
INFO - root - 2017-12-16 09:59:02.577285: step 45410, loss = 0.63, batch loss = 0.37 (47.9 examples/sec; 0.167 sec/batch; 13h:19m:55s remains)
INFO - root - 2017-12-16 09:59:04.210034: step 45420, loss = 0.72, batch loss = 0.46 (49.3 examples/sec; 0.162 sec/batch; 12h:56m:30s remains)
INFO - root - 2017-12-16 09:59:05.829702: step 45430, loss = 0.50, batch loss = 0.24 (49.1 examples/sec; 0.163 sec/batch; 12h:59m:02s remains)
INFO - root - 2017-12-16 09:59:07.510195: step 45440, loss = 0.49, batch loss = 0.23 (46.2 examples/sec; 0.173 sec/batch; 13h:48m:12s remains)
INFO - root - 2017-12-16 09:59:09.203896: step 45450, loss = 0.50, batch loss = 0.24 (46.6 examples/sec; 0.172 sec/batch; 13h:40m:50s remains)
INFO - root - 2017-12-16 09:59:10.826997: step 45460, loss = 0.61, batch loss = 0.35 (49.5 examples/sec; 0.162 sec/batch; 12h:53m:52s remains)
INFO - root - 2017-12-16 09:59:12.432416: step 45470, loss = 0.53, batch loss = 0.27 (49.4 examples/sec; 0.162 sec/batch; 12h:55m:02s remains)
INFO - root - 2017-12-16 09:59:14.060194: step 45480, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:19m:20s remains)
INFO - root - 2017-12-16 09:59:15.683057: step 45490, loss = 0.46, batch loss = 0.20 (49.8 examples/sec; 0.161 sec/batch; 12h:48m:06s remains)
INFO - root - 2017-12-16 09:59:17.332895: step 45500, loss = 0.59, batch loss = 0.33 (49.2 examples/sec; 0.163 sec/batch; 12h:57m:42s remains)
2017-12-16 09:59:17.756178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9792347 -2.0148544 -2.0423422 -2.0673828 -2.0896301 -2.10832 -2.1281679 -2.1562364 -2.1892707 -2.2291174 -2.2712786 -2.3056428 -2.3177116 -2.3066423 -2.2737143][-2.1220276 -2.187892 -2.2301748 -2.2639327 -2.2986145 -2.3332872 -2.3780508 -2.4432819 -2.5264995 -2.5986078 -2.6537683 -2.6967337 -2.6927915 -2.6400151 -2.541085][-2.4844816 -2.5579412 -2.6042957 -2.6426601 -2.6824636 -2.7163641 -2.7684233 -2.8482649 -2.9388604 -2.9857605 -2.9741859 -2.9400711 -2.8665242 -2.740273 -2.5725977][-2.9528923 -2.9862404 -2.9999492 -3.0388541 -3.0958371 -3.1162138 -3.1457059 -3.202491 -3.2419128 -3.1556635 -2.953023 -2.7612126 -2.5885184 -2.4076335 -2.2202249][-3.2669895 -3.1573336 -3.0859227 -3.0889935 -3.1027238 -3.0307105 -2.9815974 -3.0505753 -3.1193392 -2.9276848 -2.5436656 -2.2331073 -2.0084138 -1.8553417 -1.7387683][-3.3672724 -3.0381958 -2.7399611 -2.5741286 -2.4585071 -2.2203732 -2.1139016 -2.2978766 -2.498126 -2.31438 -1.8838439 -1.5984795 -1.447099 -1.3672158 -1.3909465][-3.1312885 -2.6169641 -2.0251937 -1.6148342 -1.3225229 -0.95121121 -0.807765 -1.1465218 -1.5422448 -1.4926255 -1.2220805 -1.1831726 -1.1334299 -1.1439229 -1.3465668][-2.3472784 -1.7453307 -0.9318949 -0.33445311 0.029457331 0.43482256 0.58041763 0.12590837 -0.44707143 -0.55507576 -0.64347064 -0.90764582 -1.0322206 -1.1586863 -1.5349092][-1.2060239 -0.56842542 0.32915711 0.98261619 1.2763898 1.5134766 1.6568286 1.2542617 0.58638835 0.1840229 -0.36221433 -0.99042642 -1.3011934 -1.5212376 -1.8968205][-0.18835187 0.42797112 1.2211409 1.7035463 1.8032358 1.9711835 2.2217715 1.9383504 1.1931558 0.4716928 -0.38138366 -1.2481289 -1.6593164 -1.8536987 -2.0810614][0.28858781 0.86711264 1.3858128 1.5377676 1.4369302 1.5573838 1.9699419 1.8425257 1.1061645 0.24884129 -0.61576438 -1.3847866 -1.7654098 -1.8932518 -1.9611466][0.55656552 1.1632762 1.2995284 1.0360503 0.76198769 0.79155874 1.2222509 1.2916219 0.75715232 -0.021970272 -0.71858931 -1.2779106 -1.60901 -1.678968 -1.6666651][0.75969863 1.4080107 1.2692378 0.7717948 0.3509264 0.25760627 0.73067975 0.97270131 0.602813 -0.056635141 -0.51818907 -0.97373331 -1.3436259 -1.3937244 -1.3273027][0.8716538 1.5483062 1.4140639 0.84124947 0.37613654 0.25886989 0.66938329 0.999707 0.67866278 0.071424246 -0.28261423 -0.64730513 -1.0857257 -1.1673191 -1.1024922][0.92009473 1.6337411 1.6443083 1.1634455 0.69540644 0.59498525 1.030524 1.4007859 1.123198 0.65621877 0.30790997 -0.25602746 -0.91268349 -1.0699902 -1.0039676]]...]
INFO - root - 2017-12-16 09:59:19.393560: step 45510, loss = 0.61, batch loss = 0.35 (48.2 examples/sec; 0.166 sec/batch; 13h:14m:17s remains)
INFO - root - 2017-12-16 09:59:21.031019: step 45520, loss = 0.53, batch loss = 0.27 (50.0 examples/sec; 0.160 sec/batch; 12h:45m:39s remains)
INFO - root - 2017-12-16 09:59:22.665281: step 45530, loss = 0.60, batch loss = 0.35 (46.7 examples/sec; 0.171 sec/batch; 13h:39m:13s remains)
INFO - root - 2017-12-16 09:59:24.316884: step 45540, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:03m:11s remains)
INFO - root - 2017-12-16 09:59:25.925370: step 45550, loss = 0.54, batch loss = 0.28 (49.9 examples/sec; 0.160 sec/batch; 12h:46m:45s remains)
INFO - root - 2017-12-16 09:59:27.544454: step 45560, loss = 0.53, batch loss = 0.27 (47.7 examples/sec; 0.168 sec/batch; 13h:22m:09s remains)
INFO - root - 2017-12-16 09:59:29.172723: step 45570, loss = 0.48, batch loss = 0.22 (49.9 examples/sec; 0.160 sec/batch; 12h:46m:22s remains)
INFO - root - 2017-12-16 09:59:30.823704: step 45580, loss = 0.52, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:05m:34s remains)
INFO - root - 2017-12-16 09:59:32.499528: step 45590, loss = 0.63, batch loss = 0.37 (49.0 examples/sec; 0.163 sec/batch; 13h:00m:29s remains)
INFO - root - 2017-12-16 09:59:34.112120: step 45600, loss = 0.51, batch loss = 0.26 (49.7 examples/sec; 0.161 sec/batch; 12h:50m:14s remains)
2017-12-16 09:59:34.538189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4734445 -4.4795103 -4.5223184 -4.5414972 -4.5090637 -4.4525366 -4.3610787 -4.2630882 -4.2271943 -4.2519445 -4.3102775 -4.405488 -4.5252733 -4.5748129 -4.4059982][-4.2277546 -4.2023659 -4.24659 -4.3026352 -4.3096714 -4.2632647 -4.0948448 -3.9584637 -3.9487019 -4.0216179 -4.0722661 -4.1267543 -4.2066293 -4.2067032 -3.9407587][-3.9285574 -3.817075 -3.8020439 -3.8934436 -3.9526963 -3.8553429 -3.5379148 -3.317497 -3.3934531 -3.6208129 -3.7677855 -3.808563 -3.7960763 -3.6518087 -3.1931949][-3.807436 -3.6113355 -3.5037434 -3.5091939 -3.4371498 -3.0757623 -2.5064132 -2.1527858 -2.3227403 -2.8186569 -3.2567892 -3.4403694 -3.3466074 -2.96182 -2.1386909][-3.7972541 -3.50206 -3.2240636 -3.0336425 -2.6739969 -1.870801 -0.79741049 -0.17529464 -0.44042253 -1.3171965 -2.212606 -2.7589517 -2.8011737 -2.2563496 -1.078688][-3.5868 -3.1542084 -2.5998876 -2.1037621 -1.3856533 0.024846554 1.8073547 2.7935708 2.3105104 0.794019 -0.75431633 -1.7998596 -2.2032175 -1.7805082 -0.44999349][-3.0750494 -2.3879917 -1.5170124 -0.72234142 0.43913817 2.6046493 5.3102884 6.7048264 5.6497507 3.114136 0.71884036 -0.89782608 -1.7127545 -1.4881624 -0.19273376][-2.5941198 -1.6044065 -0.51010096 0.37429976 1.6464474 4.06993 7.1719332 8.8167343 7.0678387 3.8676026 1.186312 -0.56573892 -1.4655665 -1.3659092 -0.25225663][-2.3358309 -1.2488644 -0.25730395 0.29279971 1.0814102 2.8569663 5.0253668 5.8260555 4.3649206 1.9041326 -0.1151607 -1.3161477 -1.8865463 -1.7186265 -0.89949036][-2.4391046 -1.413965 -0.73246288 -0.60927474 -0.47663355 0.405226 1.5605381 1.7518537 0.6611445 -0.93108284 -2.0825298 -2.6014676 -2.7911139 -2.5576241 -1.9451501][-2.9533226 -2.1269572 -1.6534107 -1.8332852 -2.0729702 -1.7454555 -1.16542 -1.2060759 -2.0450759 -3.0149283 -3.5050106 -3.5566864 -3.5256662 -3.2936337 -2.8058305][-3.3983991 -2.797405 -2.5543034 -2.9000154 -3.3395433 -3.2838013 -2.8970408 -2.9241314 -3.5054531 -4.0290804 -4.0874553 -3.8822789 -3.7503831 -3.5657907 -3.1944175][-3.3654852 -3.0414386 -2.9704664 -3.3417206 -3.7955587 -3.8019454 -3.4594316 -3.4129958 -3.7439151 -3.9918377 -3.8965631 -3.624269 -3.4393888 -3.2332847 -2.9352579][-2.9439738 -2.7937727 -2.8187902 -3.1169043 -3.4397476 -3.4609318 -3.222337 -3.1422224 -3.3119097 -3.4249356 -3.3191113 -3.0598154 -2.840395 -2.6551542 -2.4519734][-2.4521263 -2.4121544 -2.4749651 -2.6719851 -2.8456233 -2.8689399 -2.7247028 -2.6618729 -2.7480397 -2.8268716 -2.7643003 -2.5807488 -2.3626041 -2.185806 -2.0330989]]...]
INFO - root - 2017-12-16 09:59:36.145244: step 45610, loss = 0.53, batch loss = 0.27 (50.2 examples/sec; 0.159 sec/batch; 12h:41m:21s remains)
INFO - root - 2017-12-16 09:59:37.812184: step 45620, loss = 0.62, batch loss = 0.36 (46.6 examples/sec; 0.172 sec/batch; 13h:40m:50s remains)
INFO - root - 2017-12-16 09:59:39.479159: step 45630, loss = 0.54, batch loss = 0.29 (49.0 examples/sec; 0.163 sec/batch; 13h:00m:09s remains)
INFO - root - 2017-12-16 09:59:41.093908: step 45640, loss = 0.59, batch loss = 0.34 (48.9 examples/sec; 0.164 sec/batch; 13h:02m:37s remains)
INFO - root - 2017-12-16 09:59:42.757123: step 45650, loss = 0.57, batch loss = 0.31 (47.3 examples/sec; 0.169 sec/batch; 13h:29m:22s remains)
INFO - root - 2017-12-16 09:59:44.402462: step 45660, loss = 0.47, batch loss = 0.21 (49.3 examples/sec; 0.162 sec/batch; 12h:55m:24s remains)
INFO - root - 2017-12-16 09:59:46.009393: step 45670, loss = 0.63, batch loss = 0.37 (51.4 examples/sec; 0.156 sec/batch; 12h:23m:59s remains)
INFO - root - 2017-12-16 09:59:47.678463: step 45680, loss = 0.67, batch loss = 0.41 (48.7 examples/sec; 0.164 sec/batch; 13h:05m:29s remains)
INFO - root - 2017-12-16 09:59:49.347961: step 45690, loss = 0.80, batch loss = 0.54 (46.7 examples/sec; 0.171 sec/batch; 13h:38m:02s remains)
INFO - root - 2017-12-16 09:59:50.967574: step 45700, loss = 0.58, batch loss = 0.32 (49.7 examples/sec; 0.161 sec/batch; 12h:49m:41s remains)
2017-12-16 09:59:51.413397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75277746 -0.58444726 -0.74201274 -1.0757091 -1.4605398 -1.8069062 -2.1809559 -2.6631932 -2.9941294 -2.8580961 -2.4743359 -2.1148975 -1.9130611 -1.819921 -1.7263067][-0.33685756 0.0023818016 -0.015006065 -0.29461217 -0.67435467 -1.1251469 -1.6708643 -2.3409815 -2.8295836 -2.88646 -2.6829734 -2.4419422 -2.2582564 -2.0202591 -1.7267724][0.17040372 0.57363415 0.69347715 0.48473811 0.1937139 -0.31237316 -0.9937228 -1.7901137 -2.4366617 -2.7265456 -2.7269015 -2.6251991 -2.4284284 -1.9733982 -1.4148436][0.36300588 0.618593 0.75846481 0.63286996 0.52577877 0.16636634 -0.43377483 -1.1902615 -1.8920945 -2.3504157 -2.5802217 -2.608798 -2.3557072 -1.6870458 -0.85892689][0.49941993 0.500201 0.49572396 0.43295121 0.45809293 0.34279966 0.019902229 -0.60548604 -1.3151891 -1.9337182 -2.3579955 -2.4933655 -2.1878018 -1.4071193 -0.40321934][0.86515069 0.72725344 0.64038467 0.58010292 0.75927091 0.90597844 0.7321043 0.13383818 -0.70391464 -1.5453768 -2.1846504 -2.4345434 -2.1305757 -1.3668516 -0.34941816][1.0181344 0.94851613 0.98929238 1.0893183 1.5055754 1.9486353 1.7986224 1.1432812 0.13054752 -0.98240936 -1.8682479 -2.3004925 -2.0984731 -1.4396162 -0.56694674][0.40654898 0.4665184 0.73461056 1.1326489 2.024452 2.9165547 2.875212 2.1698129 0.99195409 -0.39352453 -1.4910408 -2.0999753 -2.0437307 -1.5370512 -0.8601917][-0.62359262 -0.51622248 -0.13306594 0.581347 1.796488 2.9522955 3.1561081 2.5180309 1.2450418 -0.23267293 -1.3706931 -2.043107 -2.0948672 -1.7182014 -1.2102875][-1.6672362 -1.5305908 -1.1105229 -0.21218967 1.0347993 2.0646303 2.4143131 1.9215891 0.73525882 -0.60795975 -1.6214182 -2.1923971 -2.269753 -1.9855313 -1.58326][-2.5310574 -2.3508742 -1.895033 -0.9851799 0.11684895 0.92114592 1.1982539 0.72226214 -0.29355526 -1.315707 -2.0089116 -2.3244987 -2.3197191 -2.0704811 -1.7489871][-3.0530081 -2.8793752 -2.4041228 -1.5408502 -0.60188055 0.011581659 0.11846209 -0.40885735 -1.2203157 -1.8567622 -2.1695437 -2.1878438 -2.0418305 -1.8445172 -1.6498181][-2.8301625 -2.7459791 -2.3586528 -1.6576698 -0.95095026 -0.54005063 -0.6098994 -1.1640668 -1.7284983 -2.0092034 -2.063386 -1.9368098 -1.7663326 -1.6574543 -1.5777262][-1.9939514 -1.9560972 -1.7000451 -1.2614286 -0.87138736 -0.75262153 -0.98470271 -1.4859223 -1.8241916 -1.8610885 -1.7671747 -1.6476202 -1.6126497 -1.6934179 -1.7486923][-1.111587 -1.1060014 -0.93514442 -0.70038688 -0.600217 -0.70637393 -1.0149606 -1.4413869 -1.6634504 -1.5798991 -1.4505525 -1.4279664 -1.5443418 -1.748732 -1.9094985]]...]
INFO - root - 2017-12-16 09:59:53.032573: step 45710, loss = 0.56, batch loss = 0.30 (49.5 examples/sec; 0.161 sec/batch; 12h:51m:50s remains)
INFO - root - 2017-12-16 09:59:54.661873: step 45720, loss = 0.70, batch loss = 0.44 (50.2 examples/sec; 0.159 sec/batch; 12h:41m:34s remains)
INFO - root - 2017-12-16 09:59:56.293127: step 45730, loss = 0.56, batch loss = 0.31 (49.2 examples/sec; 0.163 sec/batch; 12h:57m:25s remains)
INFO - root - 2017-12-16 09:59:57.908946: step 45740, loss = 0.52, batch loss = 0.27 (48.8 examples/sec; 0.164 sec/batch; 13h:04m:06s remains)
INFO - root - 2017-12-16 09:59:59.538448: step 45750, loss = 0.50, batch loss = 0.24 (47.6 examples/sec; 0.168 sec/batch; 13h:23m:02s remains)
INFO - root - 2017-12-16 10:00:01.163499: step 45760, loss = 0.64, batch loss = 0.39 (48.7 examples/sec; 0.164 sec/batch; 13h:05m:29s remains)
INFO - root - 2017-12-16 10:00:02.824434: step 45770, loss = 0.64, batch loss = 0.39 (48.3 examples/sec; 0.166 sec/batch; 13h:11m:00s remains)
INFO - root - 2017-12-16 10:00:04.453037: step 45780, loss = 0.63, batch loss = 0.37 (50.0 examples/sec; 0.160 sec/batch; 12h:44m:22s remains)
INFO - root - 2017-12-16 10:00:06.081092: step 45790, loss = 0.44, batch loss = 0.18 (49.1 examples/sec; 0.163 sec/batch; 12h:58m:14s remains)
INFO - root - 2017-12-16 10:00:07.753303: step 45800, loss = 0.54, batch loss = 0.28 (48.5 examples/sec; 0.165 sec/batch; 13h:07m:40s remains)
2017-12-16 10:00:08.231478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1224494 -3.5326521 -3.5875454 -3.464817 -3.4202657 -3.5754004 -3.8180242 -3.9300361 -3.8317041 -3.5840454 -3.2878497 -2.9954236 -2.9214759 -3.1322711 -3.4174891][-3.3506951 -3.6833439 -3.7025557 -3.5741019 -3.489912 -3.5912757 -3.7792087 -3.8571353 -3.7661982 -3.5272255 -3.1663554 -2.7578218 -2.5855842 -2.7572637 -3.0246673][-2.6656351 -2.9953532 -3.1326964 -3.1473227 -3.1615088 -3.2567661 -3.3765688 -3.4298551 -3.3914161 -3.2331188 -2.9256248 -2.5329466 -2.350189 -2.5285671 -2.8055925][-1.1666613 -1.7737467 -2.2352538 -2.5164382 -2.6914928 -2.7851143 -2.8181291 -2.8270218 -2.8022308 -2.7593231 -2.5595107 -2.2529018 -2.1566367 -2.4258277 -2.7895122][0.69167113 -0.30993223 -1.2526144 -1.9108888 -2.2791295 -2.3338904 -2.2126243 -2.0705605 -2.0355194 -2.0597954 -1.9183658 -1.7090378 -1.7990799 -2.2971704 -2.8620851][2.1988909 0.77407861 -0.66212 -1.6630907 -2.0989335 -2.0061545 -1.6513584 -1.3597825 -1.2994562 -1.3086979 -1.1250607 -0.91691267 -1.147786 -1.8862417 -2.73874][2.4082372 0.87818456 -0.75145888 -1.7757814 -2.1008644 -1.7959242 -1.2299289 -0.89251828 -0.85687661 -0.817703 -0.46398818 -0.12733102 -0.3823123 -1.2859119 -2.3707216][1.2435069 -0.0093646049 -1.3489406 -2.1100059 -2.1793797 -1.6322875 -0.93481457 -0.62893796 -0.704052 -0.63967419 -0.10221744 0.41021371 0.18132997 -0.76718485 -1.9490664][-0.083611012 -0.80105221 -1.6562377 -2.1445682 -2.0264025 -1.3509825 -0.62170088 -0.43161881 -0.70363712 -0.72088528 -0.080069542 0.61076736 0.48084426 -0.40977788 -1.6067576][-0.6559844 -0.8010031 -1.2927407 -1.6223836 -1.4634838 -0.87795234 -0.3233645 -0.35295224 -0.84920669 -0.95830584 -0.18909216 0.71664977 0.728811 -0.081424952 -1.2692041][-0.30205941 -0.050397635 -0.36198139 -0.75340807 -0.84499478 -0.565053 -0.28297758 -0.49281216 -1.090163 -1.2015293 -0.34063745 0.73190045 0.88045835 0.13661456 -1.0334127][0.43654442 0.92342496 0.60518551 -0.060747147 -0.63349664 -0.777197 -0.75657129 -0.96401954 -1.363011 -1.3615439 -0.45033658 0.69186711 0.9115 0.19972634 -0.96582043][1.1920238 1.7651842 1.2605269 0.27846026 -0.69237828 -1.1442657 -1.2565651 -1.2835444 -1.3245307 -1.123131 -0.26339746 0.7743597 0.93515182 0.15879893 -1.047383][1.4909823 2.0676882 1.54251 0.48167086 -0.62248576 -1.1930578 -1.2968622 -1.1370659 -0.853384 -0.50857413 0.1939373 0.9623394 0.95682263 0.074795246 -1.2268361][1.0806117 1.8183029 1.5607715 0.6151123 -0.44297624 -1.0015805 -1.0113657 -0.66763592 -0.15935612 0.31191492 0.87661767 1.3961782 1.2447758 0.29034042 -1.1580371]]...]
INFO - root - 2017-12-16 10:00:09.876751: step 45810, loss = 0.50, batch loss = 0.24 (49.4 examples/sec; 0.162 sec/batch; 12h:53m:48s remains)
INFO - root - 2017-12-16 10:00:11.522860: step 45820, loss = 0.49, batch loss = 0.23 (47.3 examples/sec; 0.169 sec/batch; 13h:27m:18s remains)
INFO - root - 2017-12-16 10:00:13.159215: step 45830, loss = 0.61, batch loss = 0.36 (49.0 examples/sec; 0.163 sec/batch; 13h:00m:28s remains)
INFO - root - 2017-12-16 10:00:14.780480: step 45840, loss = 0.50, batch loss = 0.24 (50.0 examples/sec; 0.160 sec/batch; 12h:45m:04s remains)
INFO - root - 2017-12-16 10:00:16.460724: step 45850, loss = 0.55, batch loss = 0.29 (49.3 examples/sec; 0.162 sec/batch; 12h:54m:58s remains)
INFO - root - 2017-12-16 10:00:18.130230: step 45860, loss = 0.61, batch loss = 0.35 (49.2 examples/sec; 0.163 sec/batch; 12h:56m:44s remains)
INFO - root - 2017-12-16 10:00:19.751298: step 45870, loss = 0.51, batch loss = 0.26 (49.8 examples/sec; 0.161 sec/batch; 12h:47m:12s remains)
INFO - root - 2017-12-16 10:00:21.369565: step 45880, loss = 0.46, batch loss = 0.21 (51.2 examples/sec; 0.156 sec/batch; 12h:25m:51s remains)
INFO - root - 2017-12-16 10:00:22.988826: step 45890, loss = 0.59, batch loss = 0.33 (50.1 examples/sec; 0.160 sec/batch; 12h:42m:22s remains)
INFO - root - 2017-12-16 10:00:24.632622: step 45900, loss = 0.61, batch loss = 0.35 (48.5 examples/sec; 0.165 sec/batch; 13h:08m:33s remains)
2017-12-16 10:00:25.072964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.62726331 -0.75631785 -1.0265594 -1.4596553 -1.8718 -2.207444 -2.475915 -2.6037271 -2.5379124 -2.4772687 -2.5803478 -2.5930483 -2.4235282 -2.2076249 -2.0271659][-0.73232174 -0.90107882 -1.1699165 -1.5459597 -1.8267937 -2.0069442 -2.1222737 -2.2576656 -2.2369769 -2.2567372 -2.4152782 -2.5059168 -2.3584893 -2.0780463 -1.8077183][-0.87401831 -1.0961351 -1.423491 -1.7202178 -1.8009988 -1.7234099 -1.6089377 -1.6478949 -1.6817701 -1.7980983 -2.0192087 -2.1962152 -2.1185403 -1.8428237 -1.5792546][-1.0275022 -1.2956829 -1.6320517 -1.850364 -1.6825416 -1.1768615 -0.73247063 -0.58111715 -0.70135581 -0.99871993 -1.3865019 -1.7261229 -1.7808607 -1.6266094 -1.4470731][-1.1398221 -1.4179235 -1.729866 -1.8782371 -1.5033281 -0.62604094 0.10100293 0.39069772 0.2489481 -0.23207903 -0.87965274 -1.3768657 -1.5349095 -1.5050162 -1.4320154][-1.1967375 -1.5069044 -1.7812777 -1.8400452 -1.3340853 -0.35386205 0.43030548 0.75664139 0.64824486 0.065584183 -0.73506117 -1.3214625 -1.546046 -1.6487751 -1.7205584][-1.2152654 -1.5631025 -1.7960277 -1.7296666 -1.1686159 -0.20246243 0.53577447 0.798383 0.67807317 -0.011094332 -0.9355917 -1.5449347 -1.7591498 -1.8753569 -2.0097227][-1.2181127 -1.5536079 -1.7509296 -1.5672832 -0.90392613 0.014834642 0.7316947 0.942225 0.72412157 -0.065843821 -1.075531 -1.6585259 -1.7686503 -1.7913373 -1.8573158][-1.2246161 -1.536194 -1.6692365 -1.4449559 -0.74399889 0.22681975 1.0409665 1.238142 0.91824532 0.066836834 -0.90452266 -1.4116592 -1.4496746 -1.39385 -1.3542204][-1.2257841 -1.5083995 -1.5787512 -1.3468927 -0.72234249 0.26873446 1.1530972 1.3508921 0.9847765 0.20711708 -0.62063158 -1.1011758 -1.1931087 -1.1139199 -0.96888471][-1.2037787 -1.467497 -1.5328631 -1.3534565 -0.8160001 0.093448162 0.84972453 0.93118453 0.60133791 0.00931263 -0.68418193 -1.1794305 -1.3487345 -1.2502857 -0.96379471][-1.1594946 -1.4300141 -1.5737348 -1.53985 -1.1657646 -0.4831773 -0.010075808 -0.014035225 -0.17616916 -0.4924835 -1.0318245 -1.5535359 -1.7493224 -1.629694 -1.2553711][-1.121379 -1.4283211 -1.7118459 -1.8854957 -1.7818898 -1.4285223 -1.2318606 -1.1923479 -1.0712609 -1.1000495 -1.493475 -1.9663887 -2.1732762 -2.0686624 -1.7610842][-1.1035903 -1.4623985 -1.8741963 -2.2552397 -2.408412 -2.3951466 -2.390655 -2.2887585 -1.9781958 -1.8079032 -2.0578279 -2.4263086 -2.6136148 -2.6086512 -2.4424322][-1.0825058 -1.4711773 -1.9693398 -2.4664209 -2.8054547 -3.05599 -3.1495054 -2.9975977 -2.6269898 -2.3842082 -2.5016031 -2.7519174 -2.9046721 -2.9636559 -2.9131794]]...]
INFO - root - 2017-12-16 10:00:26.688610: step 45910, loss = 0.50, batch loss = 0.24 (50.0 examples/sec; 0.160 sec/batch; 12h:43m:49s remains)
INFO - root - 2017-12-16 10:00:28.336185: step 45920, loss = 0.55, batch loss = 0.29 (48.5 examples/sec; 0.165 sec/batch; 13h:07m:12s remains)
INFO - root - 2017-12-16 10:00:29.965536: step 45930, loss = 0.56, batch loss = 0.30 (48.3 examples/sec; 0.166 sec/batch; 13h:10m:45s remains)
INFO - root - 2017-12-16 10:00:31.603613: step 45940, loss = 0.87, batch loss = 0.61 (49.4 examples/sec; 0.162 sec/batch; 12h:52m:40s remains)
INFO - root - 2017-12-16 10:00:33.251440: step 45950, loss = 0.56, batch loss = 0.31 (50.8 examples/sec; 0.157 sec/batch; 12h:31m:58s remains)
INFO - root - 2017-12-16 10:00:34.857784: step 45960, loss = 0.62, batch loss = 0.37 (49.2 examples/sec; 0.163 sec/batch; 12h:56m:35s remains)
INFO - root - 2017-12-16 10:00:36.520026: step 45970, loss = 0.58, batch loss = 0.32 (49.2 examples/sec; 0.163 sec/batch; 12h:56m:38s remains)
INFO - root - 2017-12-16 10:00:38.129735: step 45980, loss = 0.44, batch loss = 0.18 (50.3 examples/sec; 0.159 sec/batch; 12h:39m:28s remains)
INFO - root - 2017-12-16 10:00:39.761367: step 45990, loss = 0.54, batch loss = 0.28 (51.3 examples/sec; 0.156 sec/batch; 12h:24m:28s remains)
INFO - root - 2017-12-16 10:00:41.412524: step 46000, loss = 0.48, batch loss = 0.22 (49.5 examples/sec; 0.162 sec/batch; 12h:51m:48s remains)
2017-12-16 10:00:41.839205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1581295 -3.0103559 -2.9560652 -3.0187848 -3.0732286 -2.8143592 -2.0802782 -1.3699278 -1.0936694 -1.2389424 -1.4717195 -1.7257338 -1.7495723 -1.5190256 -1.2295959][-2.9037836 -2.7972925 -2.7617037 -2.8653591 -3.0406854 -2.9121752 -2.2921338 -1.6238954 -1.2717528 -1.2930138 -1.4501625 -1.6934614 -1.7630455 -1.5365546 -1.2836453][-2.8574593 -2.7974966 -2.7511072 -2.8807042 -3.1656969 -3.1644466 -2.5918047 -1.8991109 -1.5300834 -1.5660093 -1.7841769 -2.0562925 -2.0661011 -1.7761936 -1.4703381][-2.9286475 -2.82037 -2.6992574 -2.8173738 -3.10194 -3.0397115 -2.35301 -1.6168113 -1.4005988 -1.6882536 -2.1402969 -2.4830043 -2.4388027 -2.0851972 -1.7127191][-2.9624579 -2.7368486 -2.4874625 -2.5069897 -2.6259167 -2.2784092 -1.347123 -0.61305094 -0.72980654 -1.5154446 -2.3123705 -2.7778134 -2.7203045 -2.2909081 -1.8723465][-2.8041608 -2.4477103 -2.0526245 -1.8551445 -1.6233468 -0.82136416 0.41087317 1.085068 0.50164461 -0.88122523 -2.1199217 -2.7947609 -2.7943149 -2.3587115 -1.9205289][-2.3301978 -1.9144555 -1.3990366 -0.90939808 -0.15136838 1.1721485 2.64201 3.1419795 2.0866454 0.16783404 -1.5276523 -2.5060887 -2.6740041 -2.3224528 -1.9033339][-1.8494468 -1.455194 -0.90186226 -0.17928386 0.97547507 2.7574942 4.4816103 4.8735876 3.5269806 1.2356131 -0.81647015 -2.1076794 -2.5186243 -2.2922804 -1.8981084][-1.8979871 -1.5155151 -0.959442 -0.13151836 1.1603124 3.0160391 4.7538729 5.1975756 3.931843 1.6534512 -0.45651233 -1.8935006 -2.4784439 -2.3543534 -1.9773763][-2.3460031 -2.0349584 -1.4852734 -0.6860919 0.45164704 1.9789045 3.4201066 3.9232547 2.9683225 1.0934491 -0.7545259 -2.0765092 -2.6198268 -2.4872167 -2.0942976][-2.9567807 -2.751956 -2.2812328 -1.5384178 -0.55686152 0.67928028 1.885843 2.3353035 1.5678871 0.066015005 -1.4371619 -2.4791002 -2.8447714 -2.6149957 -2.176369][-3.1511686 -3.1102533 -2.7911003 -2.2062433 -1.3743032 -0.28131914 0.84989738 1.3195429 0.70471978 -0.56966889 -1.8481264 -2.7118688 -2.9482613 -2.6735859 -2.2256866][-2.803417 -2.9466953 -2.8904209 -2.5720551 -1.9372934 -0.90422618 0.30906296 0.97221637 0.56420016 -0.55516982 -1.7432165 -2.5799313 -2.8322179 -2.6029654 -2.2098987][-2.124331 -2.4059834 -2.6326983 -2.6049309 -2.1919029 -1.2493671 0.063925982 0.93781972 0.745841 -0.23214865 -1.37697 -2.273577 -2.6215174 -2.4863675 -2.162683][-1.7327899 -2.0794408 -2.4093418 -2.52625 -2.2568672 -1.3477019 0.11554575 1.2053707 1.1620469 0.25783896 -0.91762936 -1.9461932 -2.4564137 -2.4255493 -2.1452179]]...]
INFO - root - 2017-12-16 10:00:43.500023: step 46010, loss = 0.65, batch loss = 0.39 (49.1 examples/sec; 0.163 sec/batch; 12h:58m:45s remains)
INFO - root - 2017-12-16 10:00:45.132325: step 46020, loss = 0.67, batch loss = 0.41 (49.5 examples/sec; 0.162 sec/batch; 12h:52m:16s remains)
INFO - root - 2017-12-16 10:00:46.739311: step 46030, loss = 0.50, batch loss = 0.25 (51.1 examples/sec; 0.157 sec/batch; 12h:27m:45s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 10:00:48.377824: step 46040, loss = 0.56, batch loss = 0.31 (48.4 examples/sec; 0.165 sec/batch; 13h:09m:17s remains)
INFO - root - 2017-12-16 10:00:50.021741: step 46050, loss = 0.51, batch loss = 0.25 (47.0 examples/sec; 0.170 sec/batch; 13h:33m:05s remains)
INFO - root - 2017-12-16 10:00:51.688590: step 46060, loss = 0.62, batch loss = 0.36 (48.0 examples/sec; 0.167 sec/batch; 13h:15m:44s remains)
INFO - root - 2017-12-16 10:00:53.323543: step 46070, loss = 0.56, batch loss = 0.31 (47.1 examples/sec; 0.170 sec/batch; 13h:30m:47s remains)
INFO - root - 2017-12-16 10:00:54.943542: step 46080, loss = 0.57, batch loss = 0.32 (49.7 examples/sec; 0.161 sec/batch; 12h:48m:04s remains)
INFO - root - 2017-12-16 10:00:56.593021: step 46090, loss = 0.54, batch loss = 0.28 (49.2 examples/sec; 0.162 sec/batch; 12h:55m:29s remains)
INFO - root - 2017-12-16 10:00:58.220148: step 46100, loss = 0.49, batch loss = 0.24 (47.7 examples/sec; 0.168 sec/batch; 13h:20m:57s remains)
2017-12-16 10:00:58.638278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.37313 -2.4391775 -1.949717 -1.3678688 -1.0677841 -0.93811333 -0.9388566 -0.93467009 -0.92351484 -0.69534945 -0.29390025 -0.21287131 -0.17477012 0.011454582 0.24981737][-1.9032733 -2.1293578 -1.8196594 -1.2808681 -0.84488 -0.62505448 -0.58338416 -0.65848744 -0.70687485 -0.50944221 -0.19392633 -0.20782495 -0.18996143 0.10586262 0.50803566][-1.539276 -1.8804421 -1.7855687 -1.4089714 -0.95156252 -0.59649086 -0.45345402 -0.53966033 -0.72802222 -0.74499285 -0.6798172 -0.83370233 -0.87512589 -0.5935694 -0.048990965][-1.28646 -1.6800617 -1.698307 -1.4803567 -1.0613484 -0.55464959 -0.24719405 -0.31189132 -0.71514177 -1.1130786 -1.3418342 -1.6083062 -1.7495621 -1.5639758 -1.0584409][-1.032228 -1.4071877 -1.4949644 -1.2652196 -0.786515 -0.15346551 0.35409069 0.30853009 -0.36976838 -1.1408252 -1.6755347 -2.0650415 -2.3146768 -2.2962554 -1.9879988][-0.80907047 -1.2172018 -1.334985 -0.89971232 -0.18896151 0.69166207 1.4829514 1.4034715 0.43664598 -0.62321866 -1.4283948 -2.0105486 -2.3527584 -2.4938438 -2.47548][-0.85564613 -1.3023659 -1.3246863 -0.6549257 0.31352806 1.532733 2.6159623 2.432421 1.3361225 0.12748861 -0.83608413 -1.5402753 -1.9245973 -2.1876814 -2.4399078][-1.1633197 -1.5625648 -1.5483452 -0.93823719 0.1178329 1.6541431 2.9370668 2.6507833 1.5983417 0.50805569 -0.45112836 -1.2349516 -1.6106975 -1.8531742 -2.1915591][-1.5135343 -1.8105574 -1.8213321 -1.4095114 -0.4867872 0.944396 2.0233848 1.7828405 0.93010187 0.085220814 -0.74474895 -1.4450395 -1.7087015 -1.8230369 -2.1506248][-1.6184068 -1.7542514 -1.7690144 -1.6221304 -1.040799 -0.031218767 0.63686848 0.34959459 -0.33822227 -0.99274027 -1.5904896 -2.0234904 -2.1141431 -2.1201637 -2.3408158][-1.3660048 -1.2805396 -1.3497785 -1.4875036 -1.3042936 -0.79460537 -0.59106147 -1.0194147 -1.6732428 -2.2341912 -2.6177902 -2.7417979 -2.6541653 -2.566494 -2.6393566][-1.0288159 -0.6882267 -0.71950448 -1.0127062 -1.1816773 -1.2281029 -1.5156161 -2.1357446 -2.7814867 -3.255739 -3.4529195 -3.3503973 -3.1108494 -2.9091864 -2.7905564][-0.72895622 -0.16633463 -0.088041544 -0.4463104 -0.83073378 -1.297766 -2.0109816 -2.7289085 -3.281069 -3.6450751 -3.7470796 -3.556653 -3.2023902 -2.8099654 -2.50403][-0.48294425 0.20338416 0.34830904 0.019503832 -0.50564539 -1.2418479 -2.1197593 -2.7465005 -3.13026 -3.3591862 -3.437726 -3.2749753 -2.8842623 -2.3084474 -1.7968793][-0.383893 0.28129649 0.49665785 0.20769954 -0.36350632 -1.1979079 -2.1032295 -2.5563152 -2.7117724 -2.7205355 -2.728951 -2.5847273 -2.2058132 -1.5825418 -1.0180376]]...]
INFO - root - 2017-12-16 10:01:00.263798: step 46110, loss = 0.62, batch loss = 0.36 (50.9 examples/sec; 0.157 sec/batch; 12h:30m:17s remains)
INFO - root - 2017-12-16 10:01:01.935569: step 46120, loss = 0.55, batch loss = 0.30 (46.5 examples/sec; 0.172 sec/batch; 13h:40m:19s remains)
INFO - root - 2017-12-16 10:01:03.556458: step 46130, loss = 0.54, batch loss = 0.28 (49.0 examples/sec; 0.163 sec/batch; 12h:59m:31s remains)
INFO - root - 2017-12-16 10:01:05.198119: step 46140, loss = 0.57, batch loss = 0.31 (47.9 examples/sec; 0.167 sec/batch; 13h:17m:00s remains)
INFO - root - 2017-12-16 10:01:06.858653: step 46150, loss = 0.48, batch loss = 0.22 (47.6 examples/sec; 0.168 sec/batch; 13h:21m:25s remains)
INFO - root - 2017-12-16 10:01:08.485425: step 46160, loss = 0.58, batch loss = 0.33 (49.8 examples/sec; 0.161 sec/batch; 12h:47m:23s remains)
INFO - root - 2017-12-16 10:01:10.102416: step 46170, loss = 0.77, batch loss = 0.52 (49.7 examples/sec; 0.161 sec/batch; 12h:47m:42s remains)
INFO - root - 2017-12-16 10:01:11.733306: step 46180, loss = 0.52, batch loss = 0.26 (49.4 examples/sec; 0.162 sec/batch; 12h:52m:14s remains)
INFO - root - 2017-12-16 10:01:13.358013: step 46190, loss = 0.60, batch loss = 0.34 (49.7 examples/sec; 0.161 sec/batch; 12h:48m:13s remains)
INFO - root - 2017-12-16 10:01:15.020997: step 46200, loss = 0.48, batch loss = 0.22 (50.0 examples/sec; 0.160 sec/batch; 12h:43m:27s remains)
2017-12-16 10:01:15.470226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7382 -3.0497482 -3.2424841 -3.2545602 -2.9917488 -2.5713015 -2.3410509 -2.5743594 -3.1639805 -3.7898502 -4.2899427 -4.6275616 -4.8209524 -4.76888 -4.5157804][-3.2647505 -3.7129998 -3.8984618 -3.8043077 -3.3454442 -2.7267511 -2.4653239 -2.8002498 -3.5272574 -4.2333679 -4.8406763 -5.4111075 -5.8102388 -5.8056593 -5.4601016][-3.7307434 -4.2043595 -4.2586975 -3.9014592 -3.1306908 -2.3044236 -1.9041295 -2.1901252 -2.9973381 -3.8250346 -4.6358433 -5.4834423 -6.1027751 -6.1727629 -5.7534075][-4.0579386 -4.4328537 -4.2947674 -3.5770626 -2.3576264 -1.0775707 -0.40382826 -0.5834614 -1.488174 -2.5688112 -3.6359131 -4.7726946 -5.6562133 -5.8188477 -5.3013411][-4.1562009 -4.4413061 -4.1781917 -3.1308239 -1.5192876 0.20495415 1.2963994 1.421423 0.51198816 -0.86569738 -2.2635202 -3.6572409 -4.7921133 -5.1333489 -4.6396122][-3.9639726 -4.1285734 -3.7401533 -2.5453568 -0.75677323 1.3097365 3.0180566 3.7001445 2.8766434 1.310339 -0.54141688 -2.2997115 -3.7094378 -4.2299304 -3.8202488][-3.6206186 -3.5797758 -2.9761209 -1.7149339 0.11379671 2.472966 4.8028355 6.3460331 5.8414164 3.8583033 1.4724946 -0.76791394 -2.5024712 -3.2572744 -2.9809031][-3.3104668 -3.131566 -2.3534079 -0.96471834 0.76927161 3.0429056 5.6594162 7.7787361 7.4722204 5.1494074 2.5649893 0.16698217 -1.7567594 -2.716208 -2.5820272][-3.2069423 -3.0371468 -2.207844 -0.79397893 0.74698305 2.477124 4.4128046 5.8171167 5.7386131 4.2974768 2.38927 0.3399334 -1.5322827 -2.5666547 -2.5804412][-3.2771406 -3.2309558 -2.4560032 -1.1043825 0.16436028 1.2089949 2.2829669 3.2051194 3.4730537 3.0712531 2.1321065 0.56368494 -1.1938311 -2.2736568 -2.520298][-3.4502182 -3.5937471 -3.041816 -1.9146701 -0.91765738 -0.31953335 0.27646041 0.90766478 1.4091382 1.7819965 1.7111433 0.7366457 -0.81774127 -2.0449617 -2.6091073][-3.5406396 -3.844826 -3.5519619 -2.7900844 -2.1667502 -1.8651874 -1.604857 -1.2295583 -0.61748278 0.22268438 0.81959963 0.38907003 -0.85740769 -2.1156316 -2.867435][-3.3476298 -3.7088685 -3.6451774 -3.2837024 -2.9762185 -2.8431029 -2.7993052 -2.6613977 -2.0680265 -1.0719112 -0.21377301 -0.26943922 -1.177608 -2.2462871 -3.0357246][-2.9367406 -3.2419281 -3.2991009 -3.2164607 -3.1171365 -3.1110134 -3.1853538 -3.1799455 -2.7492208 -1.9261837 -1.2112662 -1.1789095 -1.6738644 -2.3668635 -2.9715841][-2.4855459 -2.6922948 -2.7764843 -2.8204684 -2.84268 -2.9306753 -3.0439706 -3.0789442 -2.8385413 -2.3645065 -1.9775407 -1.9150231 -2.1491783 -2.5103085 -2.8549902]]...]
INFO - root - 2017-12-16 10:01:17.088367: step 46210, loss = 0.63, batch loss = 0.37 (48.5 examples/sec; 0.165 sec/batch; 13h:07m:33s remains)
INFO - root - 2017-12-16 10:01:18.746449: step 46220, loss = 0.51, batch loss = 0.26 (48.7 examples/sec; 0.164 sec/batch; 13h:03m:18s remains)
INFO - root - 2017-12-16 10:01:20.375798: step 46230, loss = 0.53, batch loss = 0.27 (48.9 examples/sec; 0.164 sec/batch; 13h:01m:13s remains)
INFO - root - 2017-12-16 10:01:21.994799: step 46240, loss = 0.68, batch loss = 0.42 (49.1 examples/sec; 0.163 sec/batch; 12h:57m:48s remains)
Exception in thread Thread-8:
Traceback (most recent call last):
  File "/home/v-chaoqw/anaconda2/envs/hdrnet/lib/python2.7/threading.py", line 801, in __bootstrap_inner
    self.run()
  File "/home/v-chaoqw/anaconda2/envs/hdrnet/lib/python2.7/threading.py", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File "/home/v-chaoqw/MYSFC-ORI/datasets/dataloader.py", line 106, in thread_main
    video = self.dataset[video_id]
  File "/home/v-chaoqw/MYSFC-ORI/datasets/vid.py", line 85, in __getitem__
    image = Image.open(img_path).convert("RGB")
  File "/home/v-chaoqw/anaconda2/envs/hdrnet/lib/python2.7/site-packages/PIL/Image.py", line 2530, in open
    fp = builtins.open(filename, "rb")
IOError: [Errno 2] No such file or directory: '/home/v-chaoqw/MYSFC-ORI/ILSVRC2015-curated/Data/VID/train/b/ILSVRC2015_train_00181008/000091.00.crop.x.jpg'

