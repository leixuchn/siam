INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "243"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-12 08:01:43.367733: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 08:01:43.367767: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 08:01:43.367773: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 08:01:43.367777: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 08:01:43.367782: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 08:01:43.710462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-12 08:01:43.710500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-12 08:01:43.710507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-12 08:01:43.710515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-decay7000-nosplit-clip50-shortcut1/model.ckpt-40000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-decay7000-nosplit-clip50-shortcut1/model.ckpt-40000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-12 08:01:46.633917: step 0, loss = 0.62, batch loss = 0.56 (3.6 examples/sec; 2.214 sec/batch; 204h:28m:13s remains)
2017-12-12 08:01:47.008910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8954782 -2.9482167 -3.1533229 -3.3822718 -3.5054867 -3.3875184 -2.9220383 -2.271821 -1.8580647 -1.9909363 -2.5305085 -3.127779 -3.5431106 -3.7207081 -3.6791356][-2.8461437 -2.9002628 -3.1440842 -3.3915184 -3.4599624 -3.2108359 -2.5328326 -1.621979 -1.0637925 -1.2604117 -1.9979309 -2.7879608 -3.3003409 -3.5234673 -3.5554669][-2.7407494 -2.8303547 -3.113507 -3.3134439 -3.214447 -2.6990218 -1.6810746 -0.48218417 0.11121178 -0.29598331 -1.322993 -2.3406684 -2.9528012 -3.2095199 -3.3188879][-2.5659158 -2.7179976 -3.008693 -3.0495093 -2.632143 -1.705637 -0.25290895 1.1854162 1.6377106 0.8653841 -0.5233407 -1.7947049 -2.5276604 -2.8365297 -3.0273306][-2.2894645 -2.4988763 -2.7472744 -2.5399652 -1.680485 -0.2151463 1.7403593 3.3345885 3.4868436 2.2506905 0.44971228 -1.1154671 -2.0206196 -2.4428434 -2.7354934][-1.9389254 -2.1932037 -2.3707864 -1.9004128 -0.58462954 1.4508781 3.8780165 5.4561605 5.1291313 3.3622832 1.1571198 -0.64769149 -1.6757526 -2.1946464 -2.5569859][-1.6366243 -1.9285686 -2.0135751 -1.2756665 0.46311712 2.9949908 5.6988325 6.9586039 5.9538641 3.6383696 1.1483822 -0.69773555 -1.6799173 -2.1935439 -2.5547762][-1.5416367 -1.8435459 -1.8088565 -0.82641625 1.1764221 3.8800545 6.4252157 7.0493956 5.3883181 2.765192 0.31783724 -1.2561822 -1.9892063 -2.3794699 -2.6782641][-1.6087248 -1.8608155 -1.6989453 -0.57851291 1.4077067 3.8330359 5.7519283 5.60872 3.5839386 1.0807381 -0.94472051 -2.0202727 -2.4162345 -2.637625 -2.85402][-1.6453228 -1.7915504 -1.5289931 -0.41980147 1.2967839 3.1301074 4.2073565 3.4271398 1.3881841 -0.66771793 -2.0739985 -2.6322775 -2.7611403 -2.8678269 -3.0349236][-1.7100751 -1.7230835 -1.3901427 -0.42329359 0.85938692 2.0031862 2.3332496 1.2397661 -0.51025534 -1.9602321 -2.7519045 -2.9348497 -2.9581048 -3.0541856 -3.2170744][-1.9266367 -1.8618538 -1.5163488 -0.76789379 0.038101912 0.57590294 0.45250845 -0.55949664 -1.7818189 -2.616621 -2.9577138 -2.9701407 -3.0242722 -3.18241 -3.3703995][-2.2923234 -2.2399986 -1.9252762 -1.3930321 -1.0089488 -0.90964127 -1.1773639 -1.816052 -2.4074686 -2.7351077 -2.833925 -2.8410828 -2.9920714 -3.235853 -3.4628904][-2.6866434 -2.7090604 -2.4736547 -2.1599879 -2.1144814 -2.258333 -2.4358809 -2.5817103 -2.5800147 -2.550513 -2.578927 -2.6860213 -2.954371 -3.2668362 -3.5149257][-2.9052391 -3.0274396 -2.925271 -2.85044 -3.0554831 -3.2527025 -3.1627903 -2.7889676 -2.3106275 -2.0960016 -2.2083981 -2.5110812 -2.928834 -3.2912095 -3.5326676]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-12 08:01:49.487042: step 10, loss = 0.44, batch loss = 0.38 (37.5 examples/sec; 0.214 sec/batch; 19h:43m:20s remains)
INFO - root - 2017-12-12 08:01:51.652991: step 20, loss = 0.28, batch loss = 0.22 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:49s remains)
(239, 239, 3)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest
INFO - root - 2017-12-12 08:01:53.806175: step 30, loss = 0.40, batch loss = 0.34 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:18s remains)
INFO - root - 2017-12-12 08:01:55.963289: step 40, loss = 0.48, batch loss = 0.43 (36.8 examples/sec; 0.218 sec/batch; 20h:05m:25s remains)
INFO - root - 2017-12-12 08:01:58.122392: step 50, loss = 0.41, batch loss = 0.35 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:13s remains)
INFO - root - 2017-12-12 08:02:00.294611: step 60, loss = 0.65, batch loss = 0.59 (37.1 examples/sec; 0.215 sec/batch; 19h:53m:28s remains)
INFO - root - 2017-12-12 08:02:02.466007: step 70, loss = 0.29, batch loss = 0.23 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-12 08:02:04.663579: step 80, loss = 0.34, batch loss = 0.28 (37.0 examples/sec; 0.216 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-12 08:02:06.827750: step 90, loss = 0.44, batch loss = 0.38 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:39s remains)
INFO - root - 2017-12-12 08:02:08.993547: step 100, loss = 0.33, batch loss = 0.27 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:00s remains)
2017-12-12 08:02:09.273304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7573562 -1.8979025 -0.80094504 0.11666536 0.36202765 0.025913477 -0.87187123 -2.0428123 -3.1165886 -3.9099474 -4.1171107 -3.8133707 -2.9741163 -1.7801692 -0.93693137][-2.3731441 -1.2955496 -0.075582266 0.88373756 1.1631207 0.85746956 0.021483183 -1.087013 -2.1500854 -2.9710994 -3.2672329 -3.0897098 -2.4296694 -1.4826303 -0.87507343][-2.2723553 -1.1731479 0.021739721 0.96227551 1.2898955 1.0893631 0.40643358 -0.57159853 -1.5731275 -2.3702862 -2.7306523 -2.6895895 -2.2254617 -1.5050693 -1.0261772][-2.3271265 -1.3423123 -0.25938511 0.62886381 1.0165009 0.9545536 0.46952629 -0.30780506 -1.1702607 -1.8935592 -2.311379 -2.4398711 -2.2114482 -1.7327437 -1.3278787][-2.4186301 -1.5557988 -0.56199121 0.30921412 0.79304123 0.8969636 0.648448 0.14524007 -0.51746941 -1.1750939 -1.6863713 -2.0084705 -2.022603 -1.7769208 -1.4555862][-2.4676208 -1.6357114 -0.62817669 0.30566049 0.93767834 1.2313542 1.2279897 1.0156431 0.53047037 -0.15590286 -0.84370351 -1.3759611 -1.5812387 -1.4841416 -1.2424848][-2.5238414 -1.6922426 -0.65987396 0.31833363 1.0683346 1.5241032 1.727613 1.756 1.377768 0.58143425 -0.31193185 -1.0100863 -1.2876918 -1.1797235 -0.93598175][-2.5965846 -1.7878232 -0.776222 0.18171549 0.96742964 1.4945879 1.788878 1.9305563 1.5803542 0.69909954 -0.27938366 -0.99582791 -1.2070115 -0.98229337 -0.68599248][-2.5976512 -1.8423164 -0.89741039 -0.007735014 0.73651838 1.2360315 1.4857941 1.5827837 1.1874685 0.29786015 -0.60576534 -1.1927123 -1.2590277 -0.92105532 -0.60621953][-2.5069313 -1.839224 -1.0095544 -0.22959256 0.42078495 0.8482461 0.98716116 0.9275794 0.43502331 -0.40477562 -1.117521 -1.4704785 -1.3671062 -0.9773972 -0.71514606][-2.3952227 -1.8401561 -1.1536191 -0.5016799 0.035144091 0.3901639 0.41966105 0.18521953 -0.38588023 -1.117552 -1.579397 -1.6715031 -1.426935 -1.0524864 -0.90588546][-2.3469496 -1.932502 -1.4271631 -0.94542861 -0.5589211 -0.30128717 -0.36337185 -0.70732927 -1.2754545 -1.8525069 -2.0859454 -1.9831076 -1.6629169 -1.3402472 -1.312993][-2.3268118 -2.0204103 -1.6752312 -1.3700407 -1.1674411 -1.0785913 -1.2689571 -1.6679878 -2.1502409 -2.5370898 -2.5907979 -2.3865643 -2.0595641 -1.8128566 -1.8804573][-2.3302851 -2.0748124 -1.827402 -1.6543586 -1.6061249 -1.6895037 -1.9973314 -2.3978434 -2.7517509 -2.9600437 -2.9069524 -2.687789 -2.4123807 -2.2516029 -2.3654337][-2.3778543 -2.1648626 -1.9830862 -1.8927119 -1.9309859 -2.1004004 -2.4236841 -2.7464387 -2.9589133 -3.0318298 -2.937582 -2.7606754 -2.5708776 -2.4817448 -2.5772862]]...]
INFO - root - 2017-12-12 08:02:11.441867: step 110, loss = 0.46, batch loss = 0.41 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:39s remains)
(239, 239, 3)
INFO - root - 2017-12-12 08:02:13.615180: step 120, loss = 0.57, batch loss = 0.52 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:46s remains)
INFO - root - 2017-12-12 08:02:15.791903: step 130, loss = 0.35, batch loss = 0.30 (35.7 examples/sec; 0.224 sec/batch; 20h:42m:11s remains)
INFO - root - 2017-12-12 08:02:17.997193: step 140, loss = 0.29, batch loss = 0.23 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:18s remains)
INFO - root - 2017-12-12 08:02:20.165597: step 150, loss = 0.48, batch loss = 0.42 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:46s remains)
INFO - root - 2017-12-12 08:02:22.377518: step 160, loss = 0.38, batch loss = 0.32 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:34s remains)
INFO - root - 2017-12-12 08:02:24.570696: step 170, loss = 0.32, batch loss = 0.27 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:58s remains)
INFO - root - 2017-12-12 08:02:26.769806: step 180, loss = 0.36, batch loss = 0.30 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:03s remains)
INFO - root - 2017-12-12 08:02:28.986561: step 190, loss = 0.42, batch loss = 0.36 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-12 08:02:31.158148: step 200, loss = 0.29, batch loss = 0.24 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:59s remains)
2017-12-12 08:02:31.428456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2313752 -4.098609 -3.9905279 -4.041316 -4.259119 -4.50248 -4.576858 -4.4906988 -4.4475813 -4.6070585 -4.9279 -5.1541777 -5.0810928 -4.7225161 -4.2468095][-4.3846493 -4.0736933 -3.9278908 -4.1274395 -4.554543 -4.8834262 -4.8391247 -4.5707288 -4.49591 -4.8296947 -5.4250712 -5.8469782 -5.782361 -5.2694116 -4.5852628][-4.4419179 -4.039413 -3.9959581 -4.4186606 -5.0028648 -5.27642 -4.9564567 -4.4059658 -4.2755 -4.8005872 -5.692976 -6.3335762 -6.3062496 -5.6598215 -4.7841773][-4.5000663 -4.1241074 -4.2036428 -4.7194304 -5.2209306 -5.2176008 -4.4894886 -3.6104069 -3.4376917 -4.2001123 -5.4330931 -6.3395386 -6.4166937 -5.7170515 -4.7390709][-4.6349974 -4.332056 -4.4271383 -4.7801976 -4.9223318 -4.4441538 -3.2393093 -2.0744338 -1.96532 -3.0770917 -4.7160339 -5.9280891 -6.1711397 -5.5280457 -4.5651565][-4.5888515 -4.2881265 -4.2524018 -4.2711725 -3.9173019 -2.8760731 -1.2153997 0.090733051 -0.056988716 -1.6287435 -3.6853833 -5.1898403 -5.6348877 -5.1551476 -4.3221331][-3.9755852 -3.5687478 -3.3266187 -2.9952948 -2.1382775 -0.53612232 1.4870641 2.7279847 2.1295054 0.038753033 -2.4082735 -4.1834807 -4.86439 -4.633914 -4.029057][-3.0048826 -2.5004811 -2.097343 -1.5088842 -0.24403644 1.8111546 4.0300922 5.015748 3.8697608 1.3224318 -1.3956649 -3.3473783 -4.2174168 -4.2314982 -3.8557003][-1.9404122 -1.4876645 -1.156147 -0.59835005 0.76591563 2.9893425 5.1655874 5.7861414 4.1898909 1.4377534 -1.2685511 -3.1540363 -4.0296717 -4.1538582 -3.9070392][-1.1384175 -0.94996381 -1.029685 -0.88069415 0.20114756 2.2424614 4.1350136 4.3783226 2.6077192 0.040314674 -2.2683506 -3.7592511 -4.3863916 -4.4103689 -4.1373615][-1.1205673 -1.3670294 -2.0419278 -2.4493172 -1.8062899 -0.12985778 1.3793948 1.3828743 -0.20695043 -2.2385173 -3.8612957 -4.742506 -4.9685678 -4.7722092 -4.3895936][-1.7184885 -2.3554463 -3.5228984 -4.3558726 -4.0787826 -2.7890596 -1.6549817 -1.7288702 -2.9076905 -4.2677684 -5.1723914 -5.4686389 -5.3309693 -4.945477 -4.4784718][-2.6678815 -3.4498992 -4.7242737 -5.6409907 -5.5402021 -4.5941844 -3.8080261 -3.9012928 -4.6634951 -5.4285321 -5.7588367 -5.638154 -5.2758121 -4.8085032 -4.3376408][-3.4143529 -4.0806746 -5.1155553 -5.8264756 -5.7438669 -5.073112 -4.5772576 -4.6738868 -5.1228743 -5.473917 -5.4649649 -5.1688728 -4.7711372 -4.3575354 -3.9781008][-3.764308 -4.1846395 -4.8334284 -5.231308 -5.1161819 -4.6746058 -4.3955836 -4.4740195 -4.708786 -4.8206882 -4.68626 -4.4086528 -4.1171751 -3.8383284 -3.5903854]]...]
(239, 239, 3)
INFO - root - 2017-12-12 08:02:33.631414: step 210, loss = 0.31, batch loss = 0.25 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:28s remains)
INFO - root - 2017-12-12 08:02:35.813213: step 220, loss = 0.42, batch loss = 0.36 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:34s remains)
INFO - root - 2017-12-12 08:02:38.006099: step 230, loss = 0.31, batch loss = 0.25 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:56s remains)
INFO - root - 2017-12-12 08:02:40.185233: step 240, loss = 0.30, batch loss = 0.25 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:36s remains)
INFO - root - 2017-12-12 08:02:42.400347: step 250, loss = 0.34, batch loss = 0.28 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:54s remains)
INFO - root - 2017-12-12 08:02:44.597313: step 260, loss = 0.31, batch loss = 0.25 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:57s remains)
INFO - root - 2017-12-12 08:02:46.800367: step 270, loss = 0.39, batch loss = 0.33 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:53s remains)
INFO - root - 2017-12-12 08:02:49.016947: step 280, loss = 0.27, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:00s remains)
INFO - root - 2017-12-12 08:02:51.237908: step 290, loss = 0.33, batch loss = 0.27 (32.7 examples/sec; 0.245 sec/batch; 22h:33m:48s remains)
INFO - root - 2017-12-12 08:02:53.459927: step 300, loss = 0.42, batch loss = 0.36 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:22s remains)
2017-12-12 08:02:53.738223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1161613 -1.1236432 -1.4944422 -2.0661166 -2.7233326 -3.0806341 -3.0097659 -2.5410593 -1.8492534 -1.2141407 -0.829304 -0.78935361 -0.95930958 -1.158874 -1.3754478][-2.28658 -2.093976 -2.0823805 -2.266417 -2.7209561 -3.0558844 -3.0847871 -2.7913651 -2.3689203 -2.0376022 -1.8986452 -1.9264859 -1.8934952 -1.7895576 -1.7858009][-3.3733544 -3.0468576 -2.6597772 -2.3468008 -2.3787212 -2.5224693 -2.6062691 -2.5535793 -2.4965014 -2.5500469 -2.6565797 -2.7293103 -2.5282736 -2.215158 -2.0992537][-3.8066716 -3.4097002 -2.7393837 -1.9953861 -1.6271844 -1.5977445 -1.7279686 -1.8704635 -2.0575025 -2.2965288 -2.4705539 -2.5058594 -2.2237411 -1.895605 -1.8911085][-3.6106064 -3.1065853 -2.2059805 -1.1390021 -0.46396542 -0.30962658 -0.48520541 -0.77423739 -1.1183569 -1.3888805 -1.4806242 -1.3987756 -1.1063716 -0.95024371 -1.2559235][-3.367245 -2.7367945 -1.6222988 -0.34524131 0.53285027 0.82685614 0.69299889 0.33507276 -0.12293458 -0.38688922 -0.38528538 -0.21550155 0.026479483 -0.020265102 -0.61488485][-3.3917987 -2.627224 -1.3629615 -0.017602444 0.88888788 1.2585244 1.1859646 0.81098843 0.3068552 0.10373425 0.19469047 0.42364478 0.64655352 0.53979635 -0.13899422][-3.5750492 -2.6942108 -1.4246118 -0.21908879 0.50532913 0.82032108 0.76350975 0.41012 -0.010457993 -0.015372038 0.25001621 0.54307842 0.73488569 0.61826754 -0.0321095][-3.8750813 -3.0374923 -1.9523605 -1.0366993 -0.56522465 -0.32917762 -0.34011889 -0.57591438 -0.78545022 -0.53630328 -0.11831832 0.17068744 0.24926281 0.083375454 -0.49753523][-4.1256418 -3.5565429 -2.8480794 -2.341567 -2.1568308 -1.9771836 -1.880046 -1.9117186 -1.8799881 -1.4679434 -0.98805237 -0.7740128 -0.83662081 -1.0746558 -1.5912728][-4.019361 -3.8852718 -3.6641655 -3.6452391 -3.8435206 -3.7977388 -3.6341271 -3.4638164 -3.2257576 -2.7161078 -2.1851661 -2.0017142 -2.1217356 -2.3978786 -2.8668191][-3.5064788 -3.7584567 -3.9141998 -4.2721014 -4.8240094 -4.9798732 -4.8317723 -4.5354366 -4.16307 -3.5873041 -3.0020561 -2.8195441 -2.9492548 -3.2203653 -3.6119556][-2.9632425 -3.3748257 -3.666306 -4.170023 -4.9166179 -5.2206 -5.1306596 -4.7858458 -4.3287134 -3.7071748 -3.1257198 -2.9996548 -3.1748507 -3.4566286 -3.7850995][-3.0516028 -3.3817265 -3.5846548 -4.0308661 -4.7664986 -5.1147647 -5.0952978 -4.7736983 -4.2603264 -3.6167367 -3.0942709 -3.030062 -3.2150705 -3.4686353 -3.7209063][-3.4876051 -3.6193893 -3.647279 -3.9297056 -4.5239596 -4.8626261 -4.9421067 -4.7142296 -4.2203493 -3.6362171 -3.2250242 -3.17487 -3.276473 -3.4010527 -3.5090752]]...]
INFO - root - 2017-12-12 08:02:55.945396: step 310, loss = 0.32, batch loss = 0.26 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:25s remains)
INFO - root - 2017-12-12 08:02:58.173626: step 320, loss = 0.23, batch loss = 0.17 (35.1 examples/sec; 0.228 sec/batch; 21h:03m:26s remains)
INFO - root - 2017-12-12 08:03:00.383639: step 330, loss = 0.31, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:02s remains)
INFO - root - 2017-12-12 08:03:02.579407: step 340, loss = 0.30, batch loss = 0.24 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:09s remains)
(239, 239, 3)
INFO - root - 2017-12-12 08:03:04.787144: step 350, loss = 0.30, batch loss = 0.24 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:18s remains)
INFO - root - 2017-12-12 08:03:06.991949: step 360, loss = 0.37, batch loss = 0.32 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:58s remains)
INFO - root - 2017-12-12 08:03:09.201190: step 370, loss = 0.31, batch loss = 0.25 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:11s remains)
INFO - root - 2017-12-12 08:03:11.414893: step 380, loss = 0.34, batch loss = 0.28 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:10s remains)
INFO - root - 2017-12-12 08:03:13.609582: step 390, loss = 0.26, batch loss = 0.20 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:40s remains)
INFO - root - 2017-12-12 08:03:15.814468: step 400, loss = 0.26, batch loss = 0.21 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:51s remains)
2017-12-12 08:03:16.111903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7512097 -4.9947071 -5.1077566 -5.1482625 -5.1709857 -5.0631413 -4.6851759 -4.4044242 -4.64795 -5.1936212 -5.5751667 -5.6989183 -5.740715 -5.6785564 -5.2512903][-4.94645 -5.0429139 -4.9645233 -4.90536 -4.8543873 -4.60789 -4.0895715 -3.8013809 -4.1077318 -4.6830773 -5.006588 -5.034462 -5.0927877 -5.20613 -5.0044641][-4.6377516 -4.5165329 -4.1726341 -3.9031937 -3.6485102 -3.220401 -2.6174369 -2.3659601 -2.7430186 -3.374064 -3.7044053 -3.7302089 -3.8933477 -4.2480774 -4.314188][-3.8109982 -3.4863989 -2.96668 -2.5478263 -2.1673589 -1.6799872 -1.0653169 -0.75910091 -1.0531886 -1.6535938 -1.9983472 -2.0780878 -2.4294968 -3.1149094 -3.4903624][-2.7429292 -2.1386178 -1.3808496 -0.74463892 -0.28097892 0.034608126 0.47287178 0.83852649 0.73125911 0.27242017 -0.03942728 -0.19170165 -0.79018545 -1.8889529 -2.6670947][-2.0857515 -1.245774 -0.18000245 0.84071279 1.503804 1.6871822 1.9164445 2.2571423 2.2271183 1.8139198 1.4803832 1.2252405 0.38510823 -1.0592284 -2.1824183][-1.9248343 -1.0566783 0.20363021 1.491081 2.2242081 2.3035529 2.407244 2.7292287 2.6400344 2.1306331 1.6902797 1.30916 0.34864664 -1.1694369 -2.3388648][-1.7918885 -0.98001337 0.23350525 1.3634455 1.8415568 1.7217214 1.7767022 2.1771638 2.0596888 1.4101274 0.79892278 0.23472095 -0.80891109 -2.2084062 -3.1421609][-1.2639861 -0.5114913 0.46210289 1.0385945 0.86694741 0.31137228 0.27009034 0.79239154 0.76295114 0.10150766 -0.55666614 -1.1879873 -2.2042284 -3.3808343 -4.0010796][-0.41513276 0.34686065 1.0226896 0.92904019 -0.068924665 -1.2374711 -1.5015697 -0.9462471 -0.86395025 -1.4399459 -2.0414152 -2.5964887 -3.4090791 -4.2450476 -4.5461087][-0.055184364 0.74038053 1.1698968 0.56093144 -1.088649 -2.7336996 -3.1789019 -2.6443672 -2.4612503 -2.9118207 -3.4408126 -3.8956671 -4.4371486 -4.8850536 -4.8896542][-0.36140943 0.47572875 0.81609559 -0.017619371 -1.9207661 -3.7221549 -4.2625875 -3.8264642 -3.6163497 -3.9215655 -4.3558016 -4.7150846 -5.0301404 -5.169208 -4.990757][-1.0360899 -0.12588787 0.3108511 -0.44827509 -2.1380317 -3.7338593 -4.2815623 -4.0551939 -3.9470503 -4.1935229 -4.5397754 -4.82946 -5.0138288 -5.0124884 -4.81746][-1.7793223 -0.90117812 -0.39807916 -0.91333628 -2.1368263 -3.2804816 -3.6950636 -3.6116033 -3.6061292 -3.8360875 -4.1178455 -4.3409309 -4.4768858 -4.4669003 -4.368144][-2.0914907 -1.3759038 -0.954273 -1.2990367 -2.0761814 -2.7499309 -2.9711494 -2.9374037 -2.9758534 -3.1728384 -3.4100528 -3.6161401 -3.75627 -3.7887504 -3.7894289]]...]
INFO - root - 2017-12-12 08:03:18.304611: step 410, loss = 0.37, batch loss = 0.31 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:48s remains)
INFO - root - 2017-12-12 08:03:20.518111: step 420, loss = 0.45, batch loss = 0.39 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:56s remains)
INFO - root - 2017-12-12 08:03:22.758389: step 430, loss = 0.34, batch loss = 0.28 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:38s remains)
(239, 239, 3)
INFO - root - 2017-12-12 08:03:24.956086: step 440, loss = 0.26, batch loss = 0.20 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:29s remains)
INFO - root - 2017-12-12 08:03:27.187847: step 450, loss = 0.31, batch loss = 0.25 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:56s remains)
INFO - root - 2017-12-12 08:03:29.418911: step 460, loss = 0.27, batch loss = 0.21 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:46s remains)
INFO - root - 2017-12-12 08:03:31.641896: step 470, loss = 0.25, batch loss = 0.19 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:12s remains)
INFO - root - 2017-12-12 08:03:33.855854: step 480, loss = 0.28, batch loss = 0.22 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:37s remains)
INFO - root - 2017-12-12 08:03:36.084595: step 490, loss = 0.32, batch loss = 0.26 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:07s remains)
INFO - root - 2017-12-12 08:03:38.299876: step 500, loss = 0.39, batch loss = 0.33 (36.8 examples/sec; 0.218 sec/batch; 20h:03m:48s remains)
2017-12-12 08:03:38.591214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2436051 -4.5530391 -4.8938813 -4.9793043 -4.8343492 -4.7059731 -4.6758518 -4.7454047 -4.77098 -4.6252341 -4.2784538 -3.7336378 -3.1111321 -2.599997 -2.2448411][-4.0286975 -4.3300076 -4.6054511 -4.52935 -4.2598085 -4.1388373 -4.2210875 -4.42089 -4.4747524 -4.2025042 -3.6623092 -3.0310326 -2.4564905 -2.0991414 -1.8778658][-4.1869011 -4.3053613 -4.3621039 -4.1164451 -3.8363683 -3.7651093 -3.8832884 -4.0290189 -3.9106479 -3.3842559 -2.6009395 -1.8801144 -1.4719994 -1.5127702 -1.7786382][-4.2188673 -4.0043049 -3.759392 -3.3957913 -3.1732273 -3.1668315 -3.2624593 -3.2458127 -2.8366261 -2.0013938 -0.92705917 -0.11725497 -0.0027098656 -0.66656423 -1.678913][-3.9481421 -3.4010341 -2.8934698 -2.3933628 -2.1196213 -2.0528846 -2.0251088 -1.827071 -1.2096295 -0.18857765 1.0273709 1.7472005 1.4169807 0.096752405 -1.5593013][-3.3661025 -2.6573863 -2.04491 -1.4379787 -0.97301316 -0.599427 -0.27425694 0.092658758 0.66405535 1.4986153 2.5106816 2.9360957 2.22058 0.57977152 -1.3046241][-2.582727 -1.8683612 -1.2947068 -0.67608023 -0.022343397 0.73815632 1.3981142 1.7710156 1.9510574 2.2312198 2.748167 2.8363981 2.009048 0.520638 -1.1025786][-1.7964331 -1.0871148 -0.61279106 -0.15601635 0.47094369 1.364336 2.0838976 2.2194161 1.8984809 1.64469 1.7111931 1.5984745 0.95086336 -0.077871323 -1.170439][-1.1525605 -0.44556475 -0.18018079 -0.077962637 0.20739484 0.82071304 1.2459164 1.0866618 0.55992126 0.18158388 0.0751462 -0.064057827 -0.44151425 -0.99956942 -1.5637894][-0.68059921 -0.1006887 -0.15221834 -0.50273895 -0.67312145 -0.49271202 -0.40401769 -0.67221141 -1.0344784 -1.1997125 -1.2706161 -1.3876114 -1.5956886 -1.8622502 -2.0933943][-0.59207153 -0.27896118 -0.60993814 -1.2455208 -1.6918799 -1.7784667 -1.8228867 -1.9691619 -1.9960812 -1.8664017 -1.863791 -1.9968462 -2.1760945 -2.3724322 -2.5495739][-1.2266703 -1.1595261 -1.5500255 -2.1329949 -2.5544448 -2.6675839 -2.6425636 -2.5585458 -2.3016982 -2.0101361 -2.0138471 -2.2250223 -2.4840925 -2.7611964 -3.0591345][-2.1882706 -2.1844063 -2.3975937 -2.7084222 -2.9645882 -3.0279558 -2.9131513 -2.68578 -2.3434963 -2.0806584 -2.160708 -2.4320354 -2.7706308 -3.1241658 -3.4804957][-2.7429423 -2.6737361 -2.6419616 -2.6843376 -2.8104177 -2.8690171 -2.7444928 -2.524199 -2.2929199 -2.1999338 -2.3779621 -2.6567075 -2.9911716 -3.2925537 -3.5298896][-2.7276468 -2.5649533 -2.3977163 -2.3235137 -2.4224162 -2.5335851 -2.4824705 -2.3722546 -2.3059831 -2.3421674 -2.5131664 -2.6877351 -2.8991702 -3.0305815 -3.044378]]...]
INFO - root - 2017-12-12 08:03:40.818933: step 510, loss = 0.39, batch loss = 0.34 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:29s remains)
INFO - root - 2017-12-12 08:03:43.074135: step 520, loss = 0.25, batch loss = 0.19 (34.6 examples/sec; 0.231 sec/batch; 21h:18m:52s remains)
(239, 239, 3)
INFO - root - 2017-12-12 08:03:45.318636: step 530, loss = 0.31, batch loss = 0.25 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:48s remains)
INFO - root - 2017-12-12 08:03:47.552472: step 540, loss = 0.30, batch loss = 0.24 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:33s remains)
INFO - root - 2017-12-12 08:03:49.766036: step 550, loss = 0.32, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:30s remains)
INFO - root - 2017-12-12 08:03:52.006738: step 560, loss = 0.31, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:00s remains)
INFO - root - 2017-12-12 08:03:54.232832: step 570, loss = 0.29, batch loss = 0.23 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:42s remains)
INFO - root - 2017-12-12 08:03:56.447401: step 580, loss = 0.28, batch loss = 0.22 (37.0 examples/sec; 0.217 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-12 08:03:58.673166: step 590, loss = 0.30, batch loss = 0.24 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:32s remains)
INFO - root - 2017-12-12 08:04:00.918065: step 600, loss = 0.27, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:18s remains)
2017-12-12 08:04:01.192317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9551091 -3.8469436 -4.0283604 -4.35359 -4.669055 -4.73595 -4.891727 -5.4284997 -5.961504 -6.1287394 -5.7384472 -4.3824515 -2.3818352 -1.0783608 -1.0669649][-4.140162 -4.1814957 -4.5815043 -4.9064922 -4.9263968 -4.6041179 -4.5259252 -5.0827532 -5.6920738 -5.8197155 -5.3278961 -4.0341573 -2.3676476 -1.4248219 -1.518222][-3.8183136 -3.9573133 -4.5132 -4.781323 -4.4381585 -3.7259474 -3.4979322 -4.1447115 -4.8661389 -5.0274096 -4.5845346 -3.5219476 -2.3178902 -1.8320855 -2.1367779][-3.0831532 -3.2658186 -3.8520768 -4.0016537 -3.3020673 -2.2281508 -1.9219419 -2.6738117 -3.5375905 -3.8898406 -3.6913023 -2.9447784 -2.1376307 -1.9776887 -2.4169512][-2.288043 -2.528049 -3.1098208 -3.0960505 -1.9997425 -0.56231213 -0.21994448 -1.1351881 -2.2232065 -2.8718958 -2.979723 -2.4992244 -1.8485259 -1.7483261 -2.2063699][-1.815135 -2.0982423 -2.5385785 -2.2198734 -0.73515725 0.96783471 1.2931559 0.20479703 -1.1251445 -2.0776775 -2.457098 -2.1394176 -1.4142988 -1.1357803 -1.5945511][-1.8292115 -2.147963 -2.377202 -1.6744446 0.18417549 2.0939929 2.4370339 1.2668164 -0.21294522 -1.344074 -1.9121345 -1.6870238 -0.85348225 -0.33485937 -0.92990327][-2.2736726 -2.6200435 -2.6851039 -1.6899123 0.40506744 2.4187682 2.8299382 1.703079 0.2671814 -0.80209327 -1.3959112 -1.1910644 -0.35445261 0.19782901 -0.68990111][-3.0064447 -3.4406424 -3.4971709 -2.4441543 -0.36192703 1.5848501 2.0490458 1.0909741 -0.11681914 -0.95398045 -1.4219911 -1.1894486 -0.42600393 -0.039859772 -1.1509721][-3.737699 -4.3670068 -4.5831108 -3.7092659 -1.8766328 -0.15833211 0.33029604 -0.37918854 -1.2399166 -1.7345974 -1.975567 -1.6880327 -1.0533471 -0.85181093 -2.0003934][-4.3730545 -5.1101432 -5.4740314 -4.8669229 -3.4642429 -2.1636565 -1.7821541 -2.2521229 -2.7423136 -2.8949389 -2.881417 -2.5332417 -2.0783949 -2.0647967 -3.1272151][-4.9931602 -5.6808243 -6.0375109 -5.6498642 -4.7217135 -3.9682357 -3.86656 -4.2447782 -4.4933248 -4.44415 -4.2864804 -3.9196057 -3.5686724 -3.5676417 -4.349113][-5.6226883 -6.0774736 -6.2519689 -5.9260187 -5.382112 -5.1228294 -5.3363538 -5.738059 -5.9032183 -5.7853565 -5.5343766 -5.1449509 -4.8163619 -4.8290687 -5.2856045][-6.1802216 -6.24596 -6.0377975 -5.616662 -5.299078 -5.3441124 -5.7357883 -6.1151276 -6.2370744 -6.1247463 -5.8870964 -5.5696096 -5.2708793 -5.2375317 -5.4393182][-6.50848 -6.28244 -5.7357554 -5.1804976 -4.9312568 -5.0749874 -5.4808064 -5.8117933 -5.9238586 -5.850296 -5.6714821 -5.4039607 -5.076273 -4.909296 -4.9282646]]...]
(239, 239, 3)
INFO - root - 2017-12-12 08:04:03.436503: step 610, loss = 0.30, batch loss = 0.24 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:08s remains)
INFO - root - 2017-12-12 08:04:05.657130: step 620, loss = 0.20, batch loss = 0.14 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:32s remains)
INFO - root - 2017-12-12 08:04:07.891287: step 630, loss = 0.25, batch loss = 0.19 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:59s remains)
INFO - root - 2017-12-12 08:04:10.117127: step 640, loss = 0.30, batch loss = 0.24 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:17s remains)
INFO - root - 2017-12-12 08:04:12.385999: step 650, loss = 0.34, batch loss = 0.28 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-12 08:04:14.651864: step 660, loss = 0.23, batch loss = 0.17 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-12 08:04:16.872946: step 670, loss = 0.39, batch loss = 0.34 (34.8 examples/sec; 0.230 sec/batch; 21h:09m:37s remains)
INFO - root - 2017-12-12 08:04:19.111750: step 680, loss = 0.41, batch loss = 0.36 (36.1 examples/sec; 0.221 sec/batch; 20h:23m:55s remains)
INFO - root - 2017-12-12 08:04:21.353700: step 690, loss = 0.36, batch loss = 0.30 (33.8 examples/sec; 0.237 sec/batch; 21h:48m:26s remains)
INFO - root - 2017-12-12 08:04:23.601504: step 700, loss = 0.30, batch loss = 0.24 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:28s remains)
2017-12-12 08:04:23.906248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2716608 -5.0287766 -4.7460222 -4.4415612 -4.1597509 -3.8868804 -3.5870144 -3.2956867 -3.0737112 -2.9202104 -2.7645178 -2.4793167 -1.9616873 -1.2061207 -1.0940683][-5.7732983 -5.4905443 -5.1604939 -4.7917137 -4.3934069 -3.9782 -3.5008802 -3.0602283 -2.78271 -2.6566222 -2.5694962 -2.3917141 -2.021028 -1.4148579 -1.2675016][-5.7989936 -5.476841 -5.1446257 -4.7766128 -4.3194704 -3.8056104 -3.1899371 -2.6465068 -2.3703587 -2.3625135 -2.4390447 -2.4672315 -2.3587012 -1.9760417 -1.8169125][-5.0219951 -4.6100445 -4.2806091 -3.963129 -3.531352 -2.999285 -2.3129463 -1.7475071 -1.5695728 -1.7904742 -2.1661644 -2.5307534 -2.7302508 -2.5861311 -2.4259286][-4.02963 -3.4781256 -3.1147501 -2.8318875 -2.4344637 -1.9096777 -1.179143 -0.60609674 -0.552598 -1.0492692 -1.8322668 -2.6633918 -3.2718253 -3.377821 -3.2196889][-3.205265 -2.4840438 -2.0167058 -1.6913471 -1.2535834 -0.65473413 0.16980076 0.77865005 0.73595977 -0.0071239471 -1.1945567 -2.5058365 -3.51068 -3.8277278 -3.6895962][-2.7351446 -1.924531 -1.3788157 -1.0115724 -0.51140857 0.20698905 1.1707094 1.8556745 1.7843778 0.91493487 -0.52048922 -2.1420224 -3.3856637 -3.8304904 -3.7145967][-2.8295557 -2.0559783 -1.4804251 -1.0553436 -0.45499873 0.39002085 1.469239 2.2030027 2.1400502 1.2674077 -0.19872952 -1.8753816 -3.1573772 -3.5819013 -3.4543486][-3.2543728 -2.5744889 -1.9934505 -1.5135081 -0.86289692 -0.027056932 0.97824836 1.6058443 1.5049722 0.71289945 -0.56898546 -2.012758 -3.1042595 -3.3474152 -3.1330621][-3.8115296 -3.1942616 -2.6168709 -2.1124673 -1.5684026 -0.93791318 -0.23281336 0.12437391 -0.081854582 -0.76148582 -1.7204986 -2.7129154 -3.4080794 -3.3112733 -2.9585161][-4.2447782 -3.5877624 -2.9582934 -2.4150176 -1.9795452 -1.6174814 -1.3016479 -1.2883162 -1.6543572 -2.2737322 -2.9550977 -3.5207615 -3.8099282 -3.3673608 -2.9023061][-4.271102 -3.4944263 -2.7938457 -2.2315137 -1.8897334 -1.8107903 -1.8779799 -2.180563 -2.6964431 -3.277607 -3.7709029 -4.0794258 -4.0727978 -3.4066746 -2.923661][-4.2012858 -3.3217537 -2.5926926 -2.042105 -1.7992276 -1.8865494 -2.1874933 -2.6632931 -3.2255576 -3.7478664 -4.1224165 -4.345716 -4.2515 -3.521615 -3.1434476][-4.2453284 -3.4034686 -2.7593591 -2.2830145 -2.1375604 -2.3082049 -2.687995 -3.1852126 -3.6831825 -4.1130252 -4.3867111 -4.5557332 -4.40641 -3.662442 -3.4358716][-4.717123 -4.083477 -3.6437843 -3.3437042 -3.307996 -3.4904127 -3.8256493 -4.2080369 -4.5410109 -4.7943096 -4.9280682 -5.0415306 -4.8466349 -4.19075 -4.1121325]]...]
INFO - root - 2017-12-12 08:04:26.149849: step 710, loss = 0.39, batch loss = 0.34 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:55s remains)
INFO - root - 2017-12-12 08:04:28.379736: step 720, loss = 0.26, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:23s remains)
INFO - root - 2017-12-12 08:04:30.617999: step 730, loss = 0.37, batch loss = 0.31 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:02s remains)
INFO - root - 2017-12-12 08:04:32.850250: step 740, loss = 0.43, batch loss = 0.37 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:39s remains)
(239, 239, 3)
INFO - root - 2017-12-12 08:04:35.097657: step 750, loss = 0.33, batch loss = 0.27 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:33s remains)
INFO - root - 2017-12-12 08:04:37.342083: step 760, loss = 0.26, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:36s remains)
INFO - root - 2017-12-12 08:04:39.569294: step 770, loss = 0.29, batch loss = 0.24 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:57s remains)
INFO - root - 2017-12-12 08:04:41.808716: step 780, loss = 0.52, batch loss = 0.46 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:12s remains)
