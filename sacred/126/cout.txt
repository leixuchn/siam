INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "126"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv2/split:0", shape=(8, 29, 29, 48), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_1:0", shape=(8, 200, 29, 29), dtype=float32)
Tensor("siamese_fc/conv2/def/transpose_2:0", shape=(8, 48, 29, 29), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc/conv2/def/transpose_3:0", shape=(8, 200, 29, 29), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv2/split:0", shape=(8, 57, 57, 48), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b1/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_1:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("siamese_fc_1/conv2/def/transpose_2:0", shape=(8, 48, 57, 57), dtype=float32) <tf.Variable 'siamese_fc/conv2/def/b2/weights:0' shape=(128, 48, 5, 5) dtype=float32_ref> Tensor("siamese_fc_1/conv2/def/transpose_3:0", shape=(8, 200, 57, 57), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 02:21:19.701692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:21:19.701728: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:21:19.701734: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:21:19.701743: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:21:19.701747: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:21:20.360537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 8.74GiB
2017-12-07 02:21:20.360572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 02:21:20.360578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 02:21:20.360585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-07 02:21:26.527366: step 0, loss = 0.61, batch loss = 0.54 (1.8 examples/sec; 4.517 sec/batch; 417h:13m:50s remains)
2017-12-07 02:21:27.066354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8539305 -2.875421 -2.9033837 -3.0206006 -3.1845951 -3.3184364 -3.4404635 -3.5292559 -3.4991171 -3.4125805 -3.2567682 -3.1136034 -2.9864731 -2.820272 -2.6913471][-2.9323459 -2.911891 -2.9333096 -3.1104116 -3.2762122 -3.3578238 -3.4720581 -3.5586767 -3.4378064 -3.2689774 -3.0882342 -2.9436874 -2.8522658 -2.7350841 -2.637259][-2.95151 -2.9010913 -2.9952102 -3.277719 -3.3941636 -3.3135653 -3.287231 -3.1855569 -2.8720984 -2.6528263 -2.5463841 -2.500771 -2.5720534 -2.5841639 -2.5458083][-2.9330573 -2.9651136 -3.2612038 -3.702574 -3.7592797 -3.4701834 -3.1954641 -2.7336614 -2.235769 -2.0840926 -2.1249089 -2.1873395 -2.4094105 -2.5037236 -2.4681573][-3.0737243 -3.2484629 -3.7062085 -4.14289 -4.0109692 -3.47545 -2.8791382 -1.977458 -1.3763976 -1.4943972 -1.8053732 -2.0157933 -2.3618891 -2.4936292 -2.4196887][-3.3451614 -3.548043 -3.9789906 -4.2103195 -3.794446 -2.9805851 -1.9634159 -0.54145122 0.14424419 -0.38415003 -1.1261284 -1.642581 -2.1786964 -2.4085641 -2.3698261][-3.4792247 -3.5681124 -3.8727915 -3.9111443 -3.3369842 -2.3906748 -1.0571368 0.71614265 1.3843808 0.46036196 -0.65178895 -1.4587624 -2.1008511 -2.3744571 -2.3912191][-3.4442382 -3.4143524 -3.6259847 -3.5720556 -3.0313091 -2.2007539 -0.91799831 0.68445253 1.0737147 0.033298492 -1.0727067 -1.8939896 -2.3976505 -2.5172007 -2.5146961][-3.4192319 -3.4113338 -3.6028719 -3.4411178 -2.9056458 -2.1952598 -1.1623349 -0.090954304 -0.12332535 -1.0276918 -1.8328183 -2.4878006 -2.7890546 -2.7149312 -2.6496019][-3.4731929 -3.554 -3.6860385 -3.3640838 -2.8167872 -2.2111833 -1.4742873 -0.95506191 -1.2754614 -1.9221733 -2.3438919 -2.7925444 -3.0088818 -2.8452935 -2.7307589][-3.4906709 -3.5951211 -3.6076407 -3.1958518 -2.7694578 -2.3858533 -2.0127015 -1.9121044 -2.2803977 -2.6049809 -2.6923723 -2.9373617 -3.1067672 -2.923625 -2.7790489][-3.4746051 -3.5161824 -3.431623 -3.0933685 -2.9003942 -2.7478127 -2.6613648 -2.774929 -3.0467248 -3.1305962 -3.0288758 -3.113833 -3.2128568 -3.0072818 -2.8436623][-3.5566058 -3.5631683 -3.4794755 -3.317884 -3.3013163 -3.2222557 -3.2048182 -3.3117425 -3.4047337 -3.3500764 -3.1979871 -3.2405548 -3.3054519 -3.0873032 -2.9110928][-3.7527916 -3.7550364 -3.6841192 -3.6328692 -3.6694403 -3.5831876 -3.543653 -3.591063 -3.5376172 -3.3886282 -3.229188 -3.2853394 -3.333601 -3.1094975 -2.9300289][-4.0458097 -4.050386 -3.9871142 -3.9678762 -3.9906108 -3.8961272 -3.8595524 -3.8776438 -3.7486789 -3.5140021 -3.343246 -3.3753712 -3.356132 -3.0903964 -2.9007301]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b1/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(3, 3, 192, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/b2/biases:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 02:21:34.233573: step 10, loss = 0.78, batch loss = 0.71 (11.7 examples/sec; 0.682 sec/batch; 63h:01m:16s remains)
INFO - root - 2017-12-07 02:21:41.167948: step 20, loss = 0.64, batch loss = 0.56 (11.7 examples/sec; 0.684 sec/batch; 63h:10m:25s remains)
INFO - root - 2017-12-07 02:21:48.089485: step 30, loss = 0.62, batch loss = 0.55 (11.3 examples/sec; 0.708 sec/batch; 65h:23m:15s remains)
INFO - root - 2017-12-07 02:21:55.033545: step 40, loss = 0.72, batch loss = 0.65 (11.6 examples/sec; 0.691 sec/batch; 63h:50m:44s remains)
INFO - root - 2017-12-07 02:22:01.972215: step 50, loss = 0.79, batch loss = 0.72 (11.6 examples/sec; 0.688 sec/batch; 63h:33m:29s remains)
INFO - root - 2017-12-07 02:22:08.896697: step 60, loss = 0.92, batch loss = 0.85 (11.5 examples/sec; 0.698 sec/batch; 64h:29m:44s remains)
INFO - root - 2017-12-07 02:22:15.863788: step 70, loss = 0.81, batch loss = 0.74 (11.1 examples/sec; 0.724 sec/batch; 66h:48m:37s remains)
INFO - root - 2017-12-07 02:22:22.801317: step 80, loss = 0.67, batch loss = 0.60 (11.6 examples/sec; 0.691 sec/batch; 63h:47m:50s remains)
INFO - root - 2017-12-07 02:22:29.693654: step 90, loss = 0.89, batch loss = 0.82 (12.0 examples/sec; 0.665 sec/batch; 61h:21m:49s remains)
INFO - root - 2017-12-07 02:22:36.638766: step 100, loss = 0.82, batch loss = 0.75 (11.6 examples/sec; 0.691 sec/batch; 63h:50m:33s remains)
2017-12-07 02:22:37.132372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5360196 -2.4659748 -2.5191069 -2.7184787 -2.8290942 -2.8598981 -2.9021478 -2.9558063 -2.8698223 -2.7008333 -2.6388516 -2.6625586 -2.6453609 -2.578177 -2.5372114][-2.5482275 -2.4776163 -2.4623492 -2.5078125 -2.5038424 -2.5109572 -2.5994134 -2.7037382 -2.631526 -2.5195541 -2.5339241 -2.6187816 -2.6835835 -2.6907296 -2.6678889][-2.5581083 -2.5132146 -2.459476 -2.4231489 -2.4015923 -2.4424372 -2.5642841 -2.6890056 -2.6278396 -2.558593 -2.602078 -2.6848421 -2.7975812 -2.8689146 -2.862916][-2.5953665 -2.5919054 -2.5361636 -2.4843855 -2.4980643 -2.5596826 -2.6400733 -2.7168889 -2.6572032 -2.6292589 -2.685245 -2.7428551 -2.8864603 -3.0270419 -3.0369914][-2.6520069 -2.6686544 -2.6005464 -2.5349884 -2.5724113 -2.63373 -2.6473913 -2.6525366 -2.593586 -2.6027985 -2.6629581 -2.7033246 -2.8804822 -3.0962524 -3.1373086][-2.6752124 -2.680723 -2.569191 -2.4697351 -2.520309 -2.6019106 -2.5903764 -2.5285511 -2.4440231 -2.45455 -2.511559 -2.565557 -2.7807848 -3.0608644 -3.157557][-2.66294 -2.6791382 -2.5608745 -2.4418368 -2.4914689 -2.5781353 -2.5412359 -2.4015131 -2.272537 -2.2890692 -2.3616867 -2.4345946 -2.6384058 -2.9124379 -3.0442405][-2.6151242 -2.6524849 -2.5648303 -2.467926 -2.5181403 -2.5878639 -2.5109947 -2.3091719 -2.1468184 -2.1499949 -2.2282293 -2.3187742 -2.4804969 -2.7072 -2.8571014][-2.5718412 -2.6315546 -2.6026402 -2.5518794 -2.5995233 -2.6450481 -2.5388377 -2.3258245 -2.1651785 -2.1424842 -2.1968503 -2.2736695 -2.3763011 -2.554225 -2.7115593][-2.5808935 -2.6586058 -2.6867294 -2.6848893 -2.7380552 -2.7696121 -2.6410797 -2.4404917 -2.3029299 -2.2495704 -2.2563939 -2.299118 -2.3370578 -2.4808884 -2.6487336][-2.6356263 -2.7030408 -2.7529979 -2.7735763 -2.8239162 -2.8477921 -2.6975894 -2.4900861 -2.3664305 -2.2998371 -2.2860472 -2.3258467 -2.333287 -2.4424348 -2.5944903][-2.6717744 -2.6948271 -2.7155471 -2.7265437 -2.7756767 -2.8031025 -2.6530161 -2.4565248 -2.3534372 -2.29333 -2.2717986 -2.3010731 -2.2788315 -2.3483479 -2.4842348][-2.6919334 -2.6809671 -2.6552117 -2.6254911 -2.6509547 -2.682688 -2.5714362 -2.4393888 -2.3843498 -2.3387356 -2.2973464 -2.283685 -2.2268889 -2.2678635 -2.396112][-2.6938102 -2.6746597 -2.6335225 -2.5828512 -2.5827031 -2.6104574 -2.560945 -2.5167241 -2.5149937 -2.4816928 -2.4200592 -2.3685851 -2.2963781 -2.3254969 -2.4431391][-2.6751471 -2.6694536 -2.6473892 -2.6187391 -2.6231792 -2.6538439 -2.6487198 -2.645457 -2.6537442 -2.617671 -2.5517006 -2.503727 -2.46092 -2.485476 -2.5719252]]...]
INFO - root - 2017-12-07 02:22:44.050844: step 110, loss = 1.05, batch loss = 0.98 (11.3 examples/sec; 0.707 sec/batch; 65h:15m:57s remains)
INFO - root - 2017-12-07 02:22:50.677907: step 120, loss = 0.71, batch loss = 0.64 (11.4 examples/sec; 0.703 sec/batch; 64h:55m:26s remains)
INFO - root - 2017-12-07 02:22:57.625885: step 130, loss = 0.89, batch loss = 0.82 (11.5 examples/sec; 0.696 sec/batch; 64h:17m:01s remains)
INFO - root - 2017-12-07 02:23:04.608802: step 140, loss = 0.78, batch loss = 0.71 (11.6 examples/sec; 0.691 sec/batch; 63h:47m:03s remains)
INFO - root - 2017-12-07 02:23:11.471487: step 150, loss = 0.98, batch loss = 0.91 (11.6 examples/sec; 0.691 sec/batch; 63h:49m:30s remains)
INFO - root - 2017-12-07 02:23:18.423611: step 160, loss = 0.71, batch loss = 0.64 (11.2 examples/sec; 0.717 sec/batch; 66h:09m:12s remains)
INFO - root - 2017-12-07 02:23:25.376136: step 170, loss = 0.86, batch loss = 0.79 (11.5 examples/sec; 0.697 sec/batch; 64h:19m:16s remains)
INFO - root - 2017-12-07 02:23:32.333953: step 180, loss = 0.62, batch loss = 0.55 (11.5 examples/sec; 0.698 sec/batch; 64h:24m:08s remains)
INFO - root - 2017-12-07 02:23:39.235052: step 190, loss = 0.95, batch loss = 0.88 (11.7 examples/sec; 0.685 sec/batch; 63h:12m:14s remains)
INFO - root - 2017-12-07 02:23:46.235092: step 200, loss = 0.83, batch loss = 0.76 (11.2 examples/sec; 0.714 sec/batch; 65h:55m:45s remains)
2017-12-07 02:23:46.816030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9646215 -2.5613027 -1.8499475 -0.72964358 -0.40513086 -1.5396335 -2.582762 -3.0112498 -3.0062342 -2.1007674 -1.3741665 -1.7919939 -1.9463944 -1.3017974 -1.3493035][-2.7224312 -2.2792115 -1.7442241 -0.70231318 -0.37912989 -1.463716 -2.4750819 -2.8073034 -2.6286931 -1.6484969 -0.95525551 -1.2728634 -1.2602663 -0.63733673 -0.8081243][-2.2939136 -1.8675346 -1.5946193 -0.80939865 -0.60208178 -1.6658649 -2.6851268 -2.9796786 -2.6252089 -1.55143 -0.89508915 -1.1686847 -1.0837877 -0.51593661 -0.73104572][-2.0545936 -1.6113348 -1.5446515 -1.1428006 -1.1986058 -2.2784562 -3.2325778 -3.3892455 -2.8379974 -1.7943578 -1.3538783 -1.7653401 -1.7531984 -1.1950934 -1.3101349][-2.2644508 -1.7222087 -1.6749125 -1.5732737 -1.8797929 -2.8060532 -3.3601179 -3.1765518 -2.5754242 -1.9692218 -2.139116 -2.8866267 -2.9647939 -2.2805016 -2.1802428][-2.7376747 -2.0857868 -1.8861575 -1.7948921 -1.9484048 -2.1764255 -1.8500681 -1.228662 -1.0236926 -1.4881482 -2.717772 -4.0389833 -4.2864985 -3.4300179 -3.093653][-3.2392633 -2.5470266 -2.1743381 -1.8835111 -1.4888096 -0.508075 1.1013527 2.1703963 1.5099316 -0.45634246 -2.8077126 -4.6530981 -5.0759807 -4.1408343 -3.6731749][-3.5813422 -3.0143213 -2.6219583 -2.1941028 -1.2494562 0.87144232 3.5980387 4.9695883 3.4009409 0.28479624 -2.5873928 -4.4845223 -4.8983259 -3.9690242 -3.5329583][-3.533555 -3.2901833 -3.0928569 -2.7273884 -1.6224704 0.86544752 3.7884121 4.9767618 2.9913602 -0.29630756 -2.7482498 -4.0355239 -4.112504 -3.1514304 -2.7821071][-3.1469288 -3.3091114 -3.4197545 -3.2192802 -2.3250277 -0.29235935 1.9903297 2.7433162 0.9125061 -1.7697043 -3.2885251 -3.6814961 -3.2823572 -2.268791 -1.9459386][-2.7150893 -3.1211472 -3.4347067 -3.4015894 -2.9091325 -1.6789007 -0.31854486 -0.025914192 -1.507798 -3.3321729 -3.8703043 -3.4625463 -2.6399994 -1.6033931 -1.3530858][-2.4710276 -2.8327916 -3.0892944 -3.1099975 -3.0135808 -2.6394029 -2.2031569 -2.3440926 -3.4615922 -4.5152187 -4.341898 -3.4725115 -2.4303675 -1.391825 -1.1200862][-2.3474178 -2.4492278 -2.4279165 -2.3847053 -2.5679417 -2.8704891 -3.1800096 -3.6585844 -4.4830437 -5.01194 -4.5656753 -3.6560726 -2.6596885 -1.672739 -1.2326715][-2.5214407 -2.3630147 -2.045918 -1.8647237 -2.100019 -2.7107387 -3.4420691 -4.0923357 -4.6868811 -4.9140968 -4.5076032 -3.8567929 -3.1297185 -2.2678475 -1.6285241][-2.7693305 -2.5387449 -2.1078515 -1.8308597 -1.9441521 -2.466146 -3.1748958 -3.7605186 -4.1618872 -4.2689395 -4.0338435 -3.7036269 -3.3012025 -2.6400862 -1.9363639]]...]
INFO - root - 2017-12-07 02:23:53.759040: step 210, loss = 0.70, batch loss = 0.62 (11.4 examples/sec; 0.699 sec/batch; 64h:30m:26s remains)
INFO - root - 2017-12-07 02:24:00.730375: step 220, loss = 0.79, batch loss = 0.72 (11.4 examples/sec; 0.704 sec/batch; 64h:56m:24s remains)
INFO - root - 2017-12-07 02:24:07.472155: step 230, loss = 0.63, batch loss = 0.55 (11.5 examples/sec; 0.695 sec/batch; 64h:06m:02s remains)
INFO - root - 2017-12-07 02:24:14.406822: step 240, loss = 0.56, batch loss = 0.49 (11.6 examples/sec; 0.687 sec/batch; 63h:24m:55s remains)
INFO - root - 2017-12-07 02:24:21.363760: step 250, loss = 0.91, batch loss = 0.83 (11.3 examples/sec; 0.708 sec/batch; 65h:23m:01s remains)
INFO - root - 2017-12-07 02:24:28.347810: step 260, loss = 0.67, batch loss = 0.60 (11.6 examples/sec; 0.687 sec/batch; 63h:25m:55s remains)
INFO - root - 2017-12-07 02:24:35.305329: step 270, loss = 0.85, batch loss = 0.78 (11.6 examples/sec; 0.691 sec/batch; 63h:44m:00s remains)
INFO - root - 2017-12-07 02:24:42.282470: step 280, loss = 0.56, batch loss = 0.49 (11.5 examples/sec; 0.697 sec/batch; 64h:20m:15s remains)
INFO - root - 2017-12-07 02:24:49.244207: step 290, loss = 0.66, batch loss = 0.59 (11.1 examples/sec; 0.724 sec/batch; 66h:48m:09s remains)
INFO - root - 2017-12-07 02:24:56.255110: step 300, loss = 1.00, batch loss = 0.92 (11.3 examples/sec; 0.708 sec/batch; 65h:18m:39s remains)
2017-12-07 02:24:56.808514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9290619 -3.6894004 -3.414413 -3.2963052 -3.2593212 -3.1333222 -2.964474 -2.6877875 -2.2650871 -1.8804352 -1.76984 -1.9650512 -2.1658134 -2.3145163 -2.3401048][-4.3829689 -4.095088 -3.7727866 -3.6108632 -3.4482737 -3.1899829 -2.9141607 -2.6101675 -2.2096531 -1.9115984 -1.9012711 -2.1305139 -2.3672633 -2.5809765 -2.7080736][-4.9828248 -4.7154064 -4.3831453 -4.1343112 -3.8407 -3.5607929 -3.3982215 -3.2626503 -3.009897 -2.823061 -2.8439159 -2.9368057 -3.0070934 -3.0740535 -3.0813017][-4.9763589 -4.8206649 -4.6207376 -4.4150062 -4.1294508 -3.9494526 -3.9707041 -4.0209308 -3.9424777 -3.8631697 -3.8845277 -3.8752987 -3.826926 -3.7778106 -3.7127559][-3.5630863 -3.4832311 -3.4068685 -3.2369909 -2.9826844 -2.9072552 -3.0517173 -3.2542784 -3.3795292 -3.5011821 -3.6649568 -3.7851646 -3.8817816 -3.9778275 -4.104012][-2.9298544 -2.9035106 -2.8721023 -2.6529455 -2.3346417 -2.2140491 -2.3651419 -2.6007605 -2.7620082 -2.8932228 -2.99644 -3.0634165 -3.1139674 -3.1821632 -3.3659418][-2.0743825 -2.1300969 -2.1844304 -2.0116692 -1.6736016 -1.4845853 -1.6439657 -2.0257096 -2.3679516 -2.60258 -2.7388682 -2.8507266 -2.9079504 -2.9132552 -2.9969959][-0.39581156 -0.49906898 -0.55524182 -0.29748583 0.25299597 0.72379827 0.69326878 0.21517611 -0.34341288 -0.78115535 -1.0485442 -1.2858396 -1.4738848 -1.5959647 -1.792546][-0.76864243 -0.81068468 -0.78965354 -0.466254 0.18422031 0.8431654 1.0176768 0.67439985 0.2114768 -0.051328182 -0.047904015 -0.025885582 0.0015878677 0.050710678 0.019729137][-1.8738024 -1.8227541 -1.741852 -1.4921784 -1.0456526 -0.55409408 -0.41237307 -0.72013378 -1.140964 -1.338237 -1.2450624 -1.1330388 -1.0242097 -0.83379006 -0.59654522][-1.1076808 -1.3263652 -1.5174642 -1.5567577 -1.4052017 -1.1343079 -1.0331917 -1.2820582 -1.6657968 -1.8990359 -1.8799381 -1.8517489 -1.8353822 -1.7118754 -1.4523156][-1.0269973 -1.373667 -1.7154927 -1.9584346 -2.0487053 -1.9706149 -1.8965802 -2.047369 -2.3372185 -2.5440533 -2.5611582 -2.5565696 -2.5695667 -2.4749503 -2.2091844][-2.432483 -2.5246043 -2.6054578 -2.6596603 -2.6505697 -2.5572557 -2.4736028 -2.5661218 -2.8005252 -3.0374725 -3.1674185 -3.2654095 -3.3609776 -3.3638725 -3.1918309][-3.0616198 -3.1031487 -3.1161873 -3.111237 -3.0703053 -2.9854097 -2.9103682 -2.9393685 -3.0465283 -3.1572576 -3.2085695 -3.2357175 -3.28193 -3.2829895 -3.1523972][-3.1732545 -3.190515 -3.2079318 -3.2293034 -3.228394 -3.1944039 -3.1629517 -3.1721098 -3.2064159 -3.2396841 -3.2464762 -3.2434139 -3.2521548 -3.214376 -3.0578489]]...]
INFO - root - 2017-12-07 02:25:03.805061: step 310, loss = 0.84, batch loss = 0.76 (11.5 examples/sec; 0.696 sec/batch; 64h:14m:03s remains)
INFO - root - 2017-12-07 02:25:10.760006: step 320, loss = 0.63, batch loss = 0.56 (11.8 examples/sec; 0.680 sec/batch; 62h:44m:25s remains)
INFO - root - 2017-12-07 02:25:17.678443: step 330, loss = 0.74, batch loss = 0.67 (11.7 examples/sec; 0.685 sec/batch; 63h:14m:14s remains)
INFO - root - 2017-12-07 02:25:24.648183: step 340, loss = 0.83, batch loss = 0.75 (11.0 examples/sec; 0.730 sec/batch; 67h:19m:01s remains)
INFO - root - 2017-12-07 02:25:31.358237: step 350, loss = 0.74, batch loss = 0.67 (11.8 examples/sec; 0.681 sec/batch; 62h:48m:20s remains)
INFO - root - 2017-12-07 02:25:38.293069: step 360, loss = 0.82, batch loss = 0.74 (11.0 examples/sec; 0.726 sec/batch; 67h:01m:33s remains)
INFO - root - 2017-12-07 02:25:45.233235: step 370, loss = 0.87, batch loss = 0.80 (11.8 examples/sec; 0.680 sec/batch; 62h:44m:21s remains)
INFO - root - 2017-12-07 02:25:52.194474: step 380, loss = 0.72, batch loss = 0.65 (11.3 examples/sec; 0.708 sec/batch; 65h:21m:46s remains)
INFO - root - 2017-12-07 02:25:59.107979: step 390, loss = 0.73, batch loss = 0.66 (11.6 examples/sec; 0.690 sec/batch; 63h:39m:17s remains)
INFO - root - 2017-12-07 02:26:06.157809: step 400, loss = 0.70, batch loss = 0.63 (11.3 examples/sec; 0.708 sec/batch; 65h:19m:32s remains)
2017-12-07 02:26:06.713111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5338931 -2.6447754 -2.792661 -2.9135008 -2.9242167 -2.87465 -2.8408585 -2.8935533 -3.136445 -3.4990406 -3.7896898 -3.9224782 -3.9325771 -3.7032261 -3.3146515][-3.1618972 -3.4302237 -3.6653945 -3.8130445 -3.8103352 -3.6810994 -3.5370235 -3.4821024 -3.6215808 -3.8667552 -4.019187 -4.0098243 -3.8955564 -3.5884504 -3.2017579][-2.9633727 -3.3362944 -3.6553798 -3.8934999 -4.00534 -3.9618051 -3.8528752 -3.7854381 -3.8641419 -4.0287347 -4.1210847 -4.0529671 -3.8315992 -3.4258409 -3.0369186][-2.1122365 -2.4018071 -2.6505551 -2.9203358 -3.1886249 -3.3942585 -3.5301967 -3.6255898 -3.7331572 -3.8228834 -3.8670282 -3.7975118 -3.5524592 -3.1354797 -2.8021798][-1.6134665 -1.7031581 -1.6743073 -1.6980953 -1.8364871 -2.0831981 -2.4037294 -2.7581615 -3.0718083 -3.2501864 -3.3204362 -3.2710121 -3.0595741 -2.7245278 -2.5240903][-1.890419 -1.8724024 -1.6720495 -1.496244 -1.4453511 -1.5473919 -1.8110209 -2.2284865 -2.6652474 -2.9357176 -3.0195737 -2.945076 -2.7348046 -2.4584105 -2.361809][-2.4003379 -2.3617904 -2.1558383 -1.9796572 -1.9247811 -1.9650183 -2.108871 -2.4002113 -2.731988 -2.9299169 -2.9575329 -2.8359771 -2.6317208 -2.4097726 -2.3880363][-2.706939 -2.7546535 -2.623446 -2.4932203 -2.4761431 -2.5121765 -2.5586371 -2.6531036 -2.7330997 -2.7413464 -2.6943302 -2.5790787 -2.4662781 -2.3703492 -2.4558768][-2.5680528 -2.7702584 -2.8045259 -2.7999682 -2.8659048 -2.9482832 -2.9770937 -2.9635389 -2.8536954 -2.6913404 -2.5498304 -2.4161389 -2.3526015 -2.35347 -2.5170326][-2.4022202 -2.6752138 -2.8324447 -2.9457512 -3.0978446 -3.2626014 -3.3765192 -3.4225152 -3.3284185 -3.1633749 -2.9875422 -2.7881973 -2.633728 -2.5721247 -2.6766248][-2.8249707 -3.0649116 -3.2428746 -3.3909485 -3.547245 -3.7076435 -3.8422337 -3.9373097 -3.9162402 -3.8238685 -3.6798825 -3.4388242 -3.1646037 -2.9704289 -2.9404089][-3.5231526 -3.6632876 -3.765435 -3.8597848 -3.9642251 -4.0514269 -4.1211419 -4.1643682 -4.13524 -4.072361 -3.9723897 -3.7576773 -3.4712491 -3.2348228 -3.1323984][-3.8695345 -3.9161289 -3.9195755 -3.9332931 -3.9699037 -3.9948084 -4.0220265 -4.0362926 -4.004168 -3.9484029 -3.8638227 -3.6903412 -3.4572792 -3.26259 -3.1678193][-3.6147037 -3.6425912 -3.6117268 -3.5820541 -3.5758777 -3.5787327 -3.617105 -3.6589243 -3.6649122 -3.6406865 -3.5817239 -3.4548957 -3.2910962 -3.1646917 -3.10718][-3.1727405 -3.2002549 -3.1715086 -3.127378 -3.1047249 -3.1076338 -3.1601498 -3.2303448 -3.277956 -3.2986922 -3.285243 -3.2192948 -3.1253285 -3.0577307 -3.0321693]]...]
INFO - root - 2017-12-07 02:26:13.727652: step 410, loss = 0.78, batch loss = 0.71 (11.6 examples/sec; 0.692 sec/batch; 63h:49m:09s remains)
INFO - root - 2017-12-07 02:26:20.722041: step 420, loss = 0.73, batch loss = 0.66 (11.5 examples/sec; 0.695 sec/batch; 64h:07m:38s remains)
INFO - root - 2017-12-07 02:26:27.675966: step 430, loss = 0.77, batch loss = 0.70 (11.6 examples/sec; 0.692 sec/batch; 63h:52m:17s remains)
INFO - root - 2017-12-07 02:26:34.603397: step 440, loss = 0.91, batch loss = 0.84 (11.6 examples/sec; 0.688 sec/batch; 63h:28m:33s remains)
INFO - root - 2017-12-07 02:26:41.556810: step 450, loss = 0.83, batch loss = 0.76 (11.4 examples/sec; 0.700 sec/batch; 64h:33m:28s remains)
INFO - root - 2017-12-07 02:26:48.270229: step 460, loss = 0.68, batch loss = 0.60 (11.8 examples/sec; 0.679 sec/batch; 62h:36m:45s remains)
INFO - root - 2017-12-07 02:26:55.254165: step 470, loss = 0.74, batch loss = 0.67 (11.3 examples/sec; 0.707 sec/batch; 65h:13m:25s remains)
INFO - root - 2017-12-07 02:27:02.175272: step 480, loss = 0.66, batch loss = 0.59 (11.5 examples/sec; 0.695 sec/batch; 64h:03m:09s remains)
INFO - root - 2017-12-07 02:27:09.159613: step 490, loss = 0.90, batch loss = 0.83 (11.3 examples/sec; 0.708 sec/batch; 65h:16m:03s remains)
INFO - root - 2017-12-07 02:27:16.062459: step 500, loss = 0.75, batch loss = 0.68 (12.4 examples/sec; 0.648 sec/batch; 59h:43m:26s remains)
2017-12-07 02:27:16.595069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5529222 -3.5062392 -3.3842933 -3.2410593 -3.1644235 -3.1052012 -2.9951348 -2.8282681 -2.6896875 -2.725059 -2.771245 -2.8388259 -2.6598825 -2.2914548 -2.0552008][-3.3576345 -3.2876983 -3.0730209 -2.861578 -2.7746537 -2.7941427 -2.8250895 -2.837353 -2.9227977 -3.0714276 -3.1212127 -2.9729953 -2.4310174 -1.7191985 -1.3644357][-2.9966655 -2.8939118 -2.6149352 -2.4293921 -2.4994454 -2.7141907 -2.8830664 -2.9461184 -3.00031 -2.9920521 -2.8530638 -2.5108843 -1.8484666 -1.1202693 -0.98693943][-2.5539274 -2.4374461 -2.1399932 -1.9794524 -2.1304779 -2.46673 -2.7075424 -2.7519441 -2.6545753 -2.4523973 -2.2614169 -2.0617576 -1.7002327 -1.3435364 -1.5526836][-2.1997669 -2.1176014 -1.8449965 -1.6290224 -1.5963967 -1.7646537 -1.9542735 -2.0255861 -1.8685205 -1.5914052 -1.4896774 -1.6140435 -1.6769006 -1.7096074 -2.1001182][-1.8981264 -1.8684847 -1.7271178 -1.4720228 -1.1037009 -0.86417842 -0.91468763 -1.1270378 -1.0588191 -0.74837685 -0.71384263 -1.0282016 -1.3268187 -1.6039915 -2.0827849][-1.6617851 -1.6142502 -1.5896878 -1.3346629 -0.66168618 0.037629604 0.15130854 -0.31512547 -0.5105598 -0.28870153 -0.22590637 -0.50704718 -0.81246281 -1.1873734 -1.6762049][-1.3352208 -1.1320198 -1.1253934 -0.98281765 -0.30294609 0.563169 0.78787374 0.19572735 -0.18614101 -0.066748142 0.0092425346 -0.20711613 -0.49564266 -0.8724246 -1.3212059][-0.75259352 -0.41768932 -0.467232 -0.61381316 -0.27696562 0.33526373 0.45183182 -0.0025835037 -0.30877256 -0.26754522 -0.22728062 -0.38428402 -0.61899137 -0.90500164 -1.2618411][-0.33914089 -0.16978168 -0.37049484 -0.73958659 -0.68499732 -0.3426857 -0.32417583 -0.58342052 -0.79137969 -0.85329533 -0.84764528 -0.90253258 -0.99461389 -1.0956233 -1.3260722][-0.57465506 -0.6888299 -0.93659377 -1.2545643 -1.2431374 -1.0183685 -0.98855042 -1.0325251 -1.1107326 -1.2229795 -1.2148669 -1.1784947 -1.1470494 -1.1913707 -1.4305606][-1.1690881 -1.4358449 -1.6011117 -1.7566156 -1.7043552 -1.579119 -1.5381141 -1.4300873 -1.3700917 -1.4365048 -1.3884377 -1.3175592 -1.2695119 -1.4042063 -1.7156038][-1.7659035 -1.9433889 -1.9666643 -1.9660671 -1.9033599 -1.8966501 -1.8701341 -1.6947572 -1.567513 -1.5838029 -1.5655599 -1.4985516 -1.4761195 -1.68802 -1.9819834][-2.1951709 -2.1949914 -2.1190624 -2.0181787 -1.9220622 -1.9321027 -1.858649 -1.6834424 -1.5711482 -1.6410408 -1.7208221 -1.6983581 -1.7199538 -1.9266787 -2.1065781][-2.3104784 -2.2298837 -2.1591561 -2.0446687 -1.9607115 -1.9747508 -1.8861599 -1.7488103 -1.6832132 -1.7958655 -1.9366019 -1.9395845 -1.9663572 -2.1335523 -2.2053075]]...]
INFO - root - 2017-12-07 02:27:23.571635: step 510, loss = 0.66, batch loss = 0.59 (11.4 examples/sec; 0.702 sec/batch; 64h:43m:12s remains)
INFO - root - 2017-12-07 02:27:30.615340: step 520, loss = 0.57, batch loss = 0.50 (11.4 examples/sec; 0.699 sec/batch; 64h:26m:21s remains)
INFO - root - 2017-12-07 02:27:37.544696: step 530, loss = 0.58, batch loss = 0.51 (11.7 examples/sec; 0.684 sec/batch; 63h:04m:43s remains)
INFO - root - 2017-12-07 02:27:44.431132: step 540, loss = 0.71, batch loss = 0.64 (11.2 examples/sec; 0.714 sec/batch; 65h:48m:29s remains)
INFO - root - 2017-12-07 02:27:51.383522: step 550, loss = 0.70, batch loss = 0.63 (11.5 examples/sec; 0.694 sec/batch; 64h:01m:42s remains)
INFO - root - 2017-12-07 02:27:58.308726: step 560, loss = 0.62, batch loss = 0.55 (11.7 examples/sec; 0.681 sec/batch; 62h:47m:27s remains)
INFO - root - 2017-12-07 02:28:05.145111: step 570, loss = 0.78, batch loss = 0.71 (13.2 examples/sec; 0.605 sec/batch; 55h:48m:11s remains)
INFO - root - 2017-12-07 02:28:11.919251: step 580, loss = 0.73, batch loss = 0.66 (11.5 examples/sec; 0.698 sec/batch; 64h:22m:48s remains)
INFO - root - 2017-12-07 02:28:18.880121: step 590, loss = 0.92, batch loss = 0.85 (11.6 examples/sec; 0.692 sec/batch; 63h:48m:22s remains)
INFO - root - 2017-12-07 02:28:25.833044: step 600, loss = 0.71, batch loss = 0.63 (11.1 examples/sec; 0.721 sec/batch; 66h:26m:40s remains)
2017-12-07 02:28:26.353490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5279777 -3.6385558 -3.6742697 -3.7482748 -3.83456 -3.9249411 -4.0473566 -4.2216091 -4.2999468 -4.1596022 -3.9568274 -3.7142575 -3.5093331 -3.3472366 -3.1984067][-3.7152245 -3.8818054 -3.9343863 -4.0181031 -4.109941 -4.2101502 -4.3206573 -4.4428577 -4.4247222 -4.196002 -3.9187021 -3.6356497 -3.4243543 -3.2675261 -3.111155][-3.6792414 -3.8959634 -4.0032735 -4.10252 -4.2098408 -4.3561192 -4.4809494 -4.5287213 -4.37332 -4.0453825 -3.7078321 -3.3995309 -3.2052875 -3.1082749 -3.0205965][-3.476084 -3.644259 -3.7325873 -3.8206074 -3.9581337 -4.1387935 -4.2429967 -4.1885986 -3.9033771 -3.5532062 -3.3148556 -3.1874835 -3.1889167 -3.2428083 -3.2610748][-3.2691505 -3.3306255 -3.3705368 -3.4393227 -3.5784225 -3.7196298 -3.7410786 -3.5983849 -3.2529221 -2.9603987 -2.9456503 -3.1502023 -3.4567461 -3.6700535 -3.7074304][-2.9868128 -2.9711754 -3.0066516 -3.0784039 -3.2016711 -3.2679543 -3.2231493 -3.101558 -2.8379312 -2.6726139 -2.8486471 -3.2596679 -3.720053 -3.9705944 -3.9292812][-2.8199933 -2.793303 -2.8234739 -2.8396959 -2.8828497 -2.8164845 -2.6525354 -2.5403135 -2.4099696 -2.4153621 -2.7559366 -3.2271767 -3.6856561 -3.8601041 -3.7160351][-3.0358105 -2.983016 -2.9352124 -2.8077574 -2.7030704 -2.4766984 -2.2122838 -2.0976374 -2.0126941 -2.1102772 -2.5392842 -2.9963944 -3.3715994 -3.4303496 -3.230865][-3.2250264 -3.1002645 -2.983161 -2.7982702 -2.6395998 -2.3671994 -2.1229053 -2.0762165 -2.0191066 -2.0831478 -2.4031355 -2.74853 -3.0074673 -3.0022454 -2.8297238][-3.2034445 -3.1029935 -3.035248 -2.9585404 -2.8974857 -2.7135377 -2.5388732 -2.5121059 -2.4272406 -2.3848002 -2.5086393 -2.6792612 -2.8255053 -2.7999716 -2.6599069][-3.1327353 -3.1802487 -3.2309408 -3.2615557 -3.2809925 -3.1883974 -3.1015849 -3.0707464 -2.9767215 -2.8850396 -2.91073 -3.0011268 -3.0625439 -2.9860053 -2.8084266][-3.0042043 -3.1637144 -3.2698698 -3.2958465 -3.2866907 -3.2447913 -3.2695289 -3.3130081 -3.3091345 -3.3008304 -3.3665776 -3.4741898 -3.511894 -3.4053583 -3.1996398][-2.7713385 -2.9416182 -3.0264239 -3.0133572 -2.9605923 -2.952872 -3.0627947 -3.1816602 -3.2552915 -3.3282027 -3.4228685 -3.5258608 -3.5437136 -3.4400344 -3.2885265][-2.7599268 -2.7710843 -2.7235765 -2.6650062 -2.6409187 -2.6833453 -2.8024027 -2.9008365 -2.9563403 -3.0173321 -3.091243 -3.1710973 -3.1947541 -3.1263428 -3.0489025][-3.13508 -2.8987217 -2.6684654 -2.5236421 -2.5196016 -2.5726669 -2.6443415 -2.6796336 -2.6813502 -2.7058868 -2.745075 -2.8010173 -2.8366737 -2.807806 -2.7769985]]...]
INFO - root - 2017-12-07 02:28:33.237515: step 610, loss = 0.79, batch loss = 0.71 (11.8 examples/sec; 0.677 sec/batch; 62h:25m:06s remains)
INFO - root - 2017-12-07 02:28:40.161998: step 620, loss = 0.58, batch loss = 0.50 (11.9 examples/sec; 0.674 sec/batch; 62h:07m:52s remains)
INFO - root - 2017-12-07 02:28:47.070833: step 630, loss = 0.68, batch loss = 0.61 (11.7 examples/sec; 0.681 sec/batch; 62h:48m:34s remains)
INFO - root - 2017-12-07 02:28:54.044891: step 640, loss = 0.77, batch loss = 0.70 (11.5 examples/sec; 0.696 sec/batch; 64h:09m:28s remains)
INFO - root - 2017-12-07 02:29:00.984774: step 650, loss = 0.74, batch loss = 0.66 (11.5 examples/sec; 0.696 sec/batch; 64h:07m:25s remains)
INFO - root - 2017-12-07 02:29:07.915492: step 660, loss = 0.73, batch loss = 0.66 (11.4 examples/sec; 0.699 sec/batch; 64h:24m:40s remains)
INFO - root - 2017-12-07 02:29:14.891564: step 670, loss = 0.85, batch loss = 0.78 (11.6 examples/sec; 0.689 sec/batch; 63h:31m:35s remains)
INFO - root - 2017-12-07 02:29:21.791391: step 680, loss = 1.01, batch loss = 0.94 (11.2 examples/sec; 0.717 sec/batch; 66h:04m:19s remains)
INFO - root - 2017-12-07 02:29:28.537830: step 690, loss = 0.84, batch loss = 0.77 (11.6 examples/sec; 0.689 sec/batch; 63h:29m:53s remains)
INFO - root - 2017-12-07 02:29:35.514577: step 700, loss = 0.93, batch loss = 0.86 (11.4 examples/sec; 0.701 sec/batch; 64h:38m:59s remains)
2017-12-07 02:29:36.076142: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3569221 1.3873096 1.3213143 1.1413388 0.87780094 0.64704084 0.56274986 0.67882204 0.83338308 0.90670633 0.99876738 1.1036625 1.1762781 1.1662574 1.0669713][1.3884258 1.5066009 1.364284 1.0550132 0.66865396 0.22822142 -0.07877779 -0.055915833 0.14725971 0.38983154 0.6758132 0.91661406 1.0920534 1.1479468 1.0724039][1.363234 1.5968122 1.410532 1.0302968 0.5325346 -0.15259218 -0.69950366 -0.81340146 -0.61848426 -0.23121214 0.21322775 0.55678797 0.83966255 1.0135584 1.0362306][1.3127036 1.6740284 1.5645604 1.2097335 0.58556318 -0.3888998 -1.1466954 -1.3921423 -1.2747746 -0.82910609 -0.33095026 0.021318913 0.37500525 0.70928812 0.91479778][1.2516499 1.7223387 1.7994108 1.5561523 0.78460121 -0.50185537 -1.4284408 -1.7667873 -1.7625592 -1.3460155 -0.91660929 -0.64208221 -0.25447416 0.26473093 0.71463251][1.1669512 1.666513 2.0064578 1.9841905 1.0871854 -0.51822686 -1.5932522 -1.9568586 -2.0414238 -1.7325778 -1.4714971 -1.3268468 -0.90751004 -0.19732809 0.49840403][1.0207963 1.4206824 1.9831004 2.2248955 1.3149767 -0.45651531 -1.5662425 -1.8660407 -1.9745572 -1.8374248 -1.8394935 -1.9113417 -1.5268211 -0.64588594 0.2810092][0.7797637 0.978281 1.6432567 2.0874925 1.3140349 -0.35286093 -1.3723121 -1.5902522 -1.6880398 -1.6809375 -1.9528248 -2.3170443 -2.0429964 -1.022851 0.09133482][0.55015564 0.50707293 1.1161075 1.654911 1.1541176 -0.17212963 -1.0523772 -1.2958055 -1.433075 -1.5354943 -1.9716578 -2.5309923 -2.3549891 -1.2640009 -0.050753117][0.42239094 0.13790989 0.50940132 0.98246574 0.78270769 -0.041148663 -0.69849634 -0.997519 -1.1948979 -1.3301258 -1.7982395 -2.4311934 -2.3544257 -1.3138671 -0.11885166][0.44992781 0.01227808 0.10943127 0.41714144 0.388134 -0.022433758 -0.44101858 -0.75292158 -0.92543292 -0.97170973 -1.3402867 -1.9533648 -2.0004299 -1.161083 -0.10132027][0.545135 0.10088158 0.0033121109 0.11870098 0.13032341 0.0098142624 -0.17351389 -0.45044923 -0.57954407 -0.512758 -0.75346351 -1.290678 -1.4492786 -0.87258673 -0.017370224][0.60794735 0.26643276 0.11149359 0.08092308 0.035225868 -0.0010585785 -0.080318451 -0.31776762 -0.41172028 -0.26279497 -0.3374424 -0.7067647 -0.87590408 -0.52436471 0.10023355][0.63198519 0.42406034 0.31265259 0.25600576 0.17581701 0.1036911 -0.011237621 -0.22785139 -0.30290174 -0.12694836 -0.038038731 -0.19452238 -0.32564878 -0.16708851 0.21488237][0.61402416 0.50084782 0.46646452 0.48489237 0.46638775 0.39010525 0.23224497 0.015774727 -0.057347775 0.11795187 0.31839943 0.30852509 0.17668486 0.16277027 0.33428049]]...]
INFO - root - 2017-12-07 02:29:43.031944: step 710, loss = 0.66, batch loss = 0.58 (11.6 examples/sec; 0.690 sec/batch; 63h:36m:56s remains)
INFO - root - 2017-12-07 02:29:49.923099: step 720, loss = 0.66, batch loss = 0.59 (11.8 examples/sec; 0.679 sec/batch; 62h:33m:38s remains)
INFO - root - 2017-12-07 02:29:56.815487: step 730, loss = 0.73, batch loss = 0.65 (11.6 examples/sec; 0.687 sec/batch; 63h:17m:54s remains)
INFO - root - 2017-12-07 02:30:03.796488: step 740, loss = 0.70, batch loss = 0.63 (11.5 examples/sec; 0.693 sec/batch; 63h:50m:14s remains)
INFO - root - 2017-12-07 02:30:10.746539: step 750, loss = 0.51, batch loss = 0.44 (11.6 examples/sec; 0.690 sec/batch; 63h:36m:45s remains)
INFO - root - 2017-12-07 02:30:17.674724: step 760, loss = 0.87, batch loss = 0.80 (11.3 examples/sec; 0.705 sec/batch; 64h:59m:24s remains)
INFO - root - 2017-12-07 02:30:24.599135: step 770, loss = 0.76, batch loss = 0.69 (11.5 examples/sec; 0.699 sec/batch; 64h:22m:39s remains)
INFO - root - 2017-12-07 02:30:31.552084: step 780, loss = 0.84, batch loss = 0.77 (11.5 examples/sec; 0.697 sec/batch; 64h:16m:07s remains)
INFO - root - 2017-12-07 02:30:38.536496: step 790, loss = 0.81, batch loss = 0.74 (11.6 examples/sec; 0.689 sec/batch; 63h:29m:37s remains)
INFO - root - 2017-12-07 02:30:45.273284: step 800, loss = 1.01, batch loss = 0.94 (12.8 examples/sec; 0.623 sec/batch; 57h:22m:59s remains)
2017-12-07 02:30:45.803819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3750405 -3.2833428 -3.2950015 -3.3220274 -3.0580063 -2.710336 -2.5447135 -2.5137837 -2.6537154 -2.6610289 -2.6019859 -2.5562344 -2.2178273 -2.0202224 -2.0454314][-3.4008658 -3.3035998 -3.3230715 -3.3422651 -3.13627 -2.8822651 -2.74617 -2.7082272 -2.817169 -2.7503018 -2.5967672 -2.4659252 -2.043838 -1.8308952 -1.90418][-3.3922744 -3.2712543 -3.2215133 -3.1448519 -2.8860602 -2.5703359 -2.3153837 -2.1683967 -2.26679 -2.2529485 -2.2167275 -2.2202854 -1.8749752 -1.7223504 -1.8486722][-3.4415007 -3.3361073 -3.2385786 -3.1009443 -2.8338614 -2.4992256 -2.1596985 -1.8958187 -1.9455583 -1.9333334 -1.964334 -2.0714803 -1.7821276 -1.6565132 -1.7709744][-3.3656716 -3.2854683 -3.229485 -3.1946185 -3.0894847 -2.88486 -2.5651007 -2.20865 -2.1681135 -2.0609019 -2.0080798 -2.1124635 -1.8401794 -1.7729523 -1.9352849][-3.1608829 -2.9140096 -2.7378538 -2.7336054 -2.7870822 -2.817944 -2.7008414 -2.462785 -2.5139437 -2.4061546 -2.2472546 -2.2528193 -1.8980212 -1.8269989 -2.0689642][-3.0753932 -2.7214904 -2.3799975 -2.1935732 -2.0659149 -1.9791286 -1.8436244 -1.7035906 -1.9911454 -2.1160007 -2.0890596 -2.1285827 -1.689822 -1.5091326 -1.6881571][-3.0569291 -2.7802739 -2.4655395 -2.1716352 -1.8173494 -1.4819131 -1.0934134 -0.79900575 -1.1069362 -1.3821282 -1.5988874 -1.8684344 -1.5635307 -1.4055176 -1.5216732][-3.1973386 -3.0874724 -2.9696195 -2.7877176 -2.5134988 -2.244225 -1.8221784 -1.4227397 -1.526382 -1.6078527 -1.7748382 -2.0377033 -1.7730207 -1.6288221 -1.6930394][-3.185926 -3.1774077 -3.2283258 -3.2006986 -3.14853 -3.0942392 -2.7978334 -2.4601064 -2.4571891 -2.362828 -2.4177096 -2.5721116 -2.2977185 -2.1316023 -2.0865438][-3.266263 -3.2058334 -3.2186153 -3.1591032 -3.1932712 -3.2760887 -3.1396117 -3.0000334 -3.0581014 -2.9684887 -3.0255666 -3.1597629 -2.984786 -2.8995428 -2.8052077][-3.4718831 -3.4133115 -3.3701918 -3.1825657 -3.0903463 -3.0607028 -2.9081602 -2.8748522 -2.9945526 -2.9568834 -3.0382781 -3.1525264 -3.073292 -3.0901952 -3.0317168][-3.2620406 -3.2836392 -3.3022902 -3.1437035 -3.0268543 -2.9136312 -2.6923134 -2.6219716 -2.6710763 -2.6107917 -2.6689537 -2.7196219 -2.6503589 -2.667377 -2.6047337][-2.8923097 -2.9589324 -3.0258758 -2.9547117 -2.9088058 -2.8337817 -2.6608789 -2.6032338 -2.593894 -2.5155704 -2.5403881 -2.5203276 -2.4250178 -2.3952544 -2.2764065][-2.7922721 -2.852406 -2.9098308 -2.8823254 -2.8772917 -2.8394704 -2.7441778 -2.7304876 -2.7214298 -2.6725986 -2.6931362 -2.6480808 -2.5507798 -2.5150931 -2.3710713]]...]
INFO - root - 2017-12-07 02:30:52.663075: step 810, loss = 0.76, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:58m:23s remains)
INFO - root - 2017-12-07 02:30:59.651639: step 820, loss = 0.56, batch loss = 0.49 (11.8 examples/sec; 0.680 sec/batch; 62h:38m:23s remains)
INFO - root - 2017-12-07 02:31:06.663701: step 830, loss = 0.63, batch loss = 0.55 (11.8 examples/sec; 0.675 sec/batch; 62h:12m:11s remains)
INFO - root - 2017-12-07 02:31:13.562922: step 840, loss = 0.74, batch loss = 0.66 (11.5 examples/sec; 0.693 sec/batch; 63h:49m:51s remains)
INFO - root - 2017-12-07 02:31:20.548027: step 850, loss = 0.62, batch loss = 0.55 (11.4 examples/sec; 0.699 sec/batch; 64h:25m:40s remains)
INFO - root - 2017-12-07 02:31:27.536688: step 860, loss = 0.71, batch loss = 0.63 (11.8 examples/sec; 0.675 sec/batch; 62h:13m:39s remains)
INFO - root - 2017-12-07 02:31:34.417956: step 870, loss = 0.72, batch loss = 0.65 (11.5 examples/sec; 0.696 sec/batch; 64h:07m:54s remains)
INFO - root - 2017-12-07 02:31:41.397540: step 880, loss = 0.88, batch loss = 0.80 (11.1 examples/sec; 0.720 sec/batch; 66h:20m:48s remains)
INFO - root - 2017-12-07 02:31:48.455407: step 890, loss = 0.85, batch loss = 0.78 (11.2 examples/sec; 0.713 sec/batch; 65h:40m:38s remains)
INFO - root - 2017-12-07 02:31:55.443255: step 900, loss = 0.62, batch loss = 0.55 (11.2 examples/sec; 0.715 sec/batch; 65h:52m:54s remains)
2017-12-07 02:31:55.983421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3532159 -2.2112384 -2.2798562 -2.5359149 -3.0003242 -3.3291581 -3.2172461 -2.84049 -2.4050827 -2.2863362 -2.6639853 -3.2547579 -3.7104092 -4.0329719 -4.3384838][-2.8486893 -2.571012 -2.3892632 -2.3768759 -2.6497924 -2.9416721 -2.7975612 -2.4647851 -2.2731154 -2.3515251 -2.7372742 -3.2316782 -3.5304766 -3.6544461 -3.7449493][-2.7476571 -2.7140236 -2.638351 -2.6619859 -2.845506 -2.9773035 -2.6251683 -2.0737772 -1.9316156 -2.1095195 -2.4851503 -2.8839822 -3.1083431 -3.2335122 -3.3043492][-2.0414085 -2.5098076 -2.7622228 -2.9667442 -3.1208615 -3.1556232 -2.7728941 -2.148237 -1.8887563 -1.9157038 -2.0907815 -2.3348074 -2.5271592 -2.8000917 -3.0383258][-1.62147 -2.0643058 -2.4124453 -2.7172315 -2.9382179 -3.0587339 -2.9150343 -2.4628019 -2.1600437 -2.0691781 -2.1001718 -2.2618265 -2.4328263 -2.6885638 -2.9430246][-2.5048425 -2.4228413 -2.3644514 -2.3916786 -2.4927926 -2.580049 -2.5452051 -2.212023 -1.998944 -2.0545683 -2.2683513 -2.6082029 -2.885498 -2.9909616 -2.9144621][-3.2453346 -3.015897 -2.7311957 -2.4612374 -2.2345946 -1.9581704 -1.6115985 -1.1273713 -0.94679546 -1.2141023 -1.7266634 -2.3531117 -2.809047 -2.8749933 -2.5832925][-2.7895856 -2.7936602 -2.7127662 -2.5278161 -2.2514963 -1.750396 -1.0376818 -0.25710917 0.10704374 -0.15357733 -0.81496978 -1.65013 -2.3178728 -2.65815 -2.7218471][-2.3316031 -2.329556 -2.3005464 -2.2417777 -2.1720829 -1.9122009 -1.3807409 -0.70053363 -0.27452755 -0.42245436 -1.0520217 -1.8868773 -2.6334376 -3.1189637 -3.3973377][-2.4419332 -2.2815449 -2.0615382 -1.8860643 -1.8582973 -1.8427382 -1.7371721 -1.5127168 -1.3934491 -1.6046174 -2.1579509 -2.8234019 -3.3635397 -3.6612263 -3.7706339][-2.7358198 -2.4925761 -2.1431756 -1.8289962 -1.6853464 -1.6244378 -1.5971749 -1.5932395 -1.7764144 -2.1925051 -2.7802248 -3.3438554 -3.6920006 -3.8082602 -3.7485738][-2.9739447 -2.7274256 -2.3857203 -2.0879509 -1.9236445 -1.7960219 -1.6542578 -1.5974987 -1.806807 -2.2653568 -2.8461819 -3.3334177 -3.5824838 -3.6238244 -3.5013511][-3.0753732 -2.9049478 -2.6633139 -2.4930143 -2.4324343 -2.3649988 -2.1915407 -2.0731883 -2.1856754 -2.5056248 -2.920166 -3.2489085 -3.4011908 -3.4118071 -3.3235483][-3.217361 -3.1206379 -2.9462981 -2.8312457 -2.8395653 -2.865458 -2.751225 -2.6158323 -2.6375279 -2.8320551 -3.1093254 -3.320981 -3.4104972 -3.39937 -3.336905][-3.2624846 -3.1833172 -3.0188732 -2.9008727 -2.9173012 -2.9803116 -2.9176426 -2.7908497 -2.7939339 -2.9458661 -3.1301157 -3.2314296 -3.2453408 -3.2083879 -3.1663871]]...]
INFO - root - 2017-12-07 02:32:03.001589: step 910, loss = 0.73, batch loss = 0.66 (11.3 examples/sec; 0.709 sec/batch; 65h:15m:33s remains)
INFO - root - 2017-12-07 02:32:09.672083: step 920, loss = 0.93, batch loss = 0.86 (11.6 examples/sec; 0.689 sec/batch; 63h:29m:14s remains)
INFO - root - 2017-12-07 02:32:16.672109: step 930, loss = 0.88, batch loss = 0.81 (11.6 examples/sec; 0.687 sec/batch; 63h:16m:57s remains)
INFO - root - 2017-12-07 02:32:23.615771: step 940, loss = 0.63, batch loss = 0.56 (11.5 examples/sec; 0.698 sec/batch; 64h:18m:46s remains)
INFO - root - 2017-12-07 02:32:30.636350: step 950, loss = 0.80, batch loss = 0.72 (11.8 examples/sec; 0.679 sec/batch; 62h:34m:24s remains)
INFO - root - 2017-12-07 02:32:37.570386: step 960, loss = 0.78, batch loss = 0.71 (11.9 examples/sec; 0.670 sec/batch; 61h:40m:23s remains)
INFO - root - 2017-12-07 02:32:44.486106: step 970, loss = 0.80, batch loss = 0.73 (11.9 examples/sec; 0.671 sec/batch; 61h:46m:56s remains)
INFO - root - 2017-12-07 02:32:51.453697: step 980, loss = 0.63, batch loss = 0.56 (11.7 examples/sec; 0.683 sec/batch; 62h:55m:00s remains)
INFO - root - 2017-12-07 02:32:58.436654: step 990, loss = 0.67, batch loss = 0.59 (11.6 examples/sec; 0.692 sec/batch; 63h:42m:33s remains)
INFO - root - 2017-12-07 02:33:05.401345: step 1000, loss = 0.68, batch loss = 0.61 (11.0 examples/sec; 0.726 sec/batch; 66h:51m:07s remains)
2017-12-07 02:33:05.883006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7341793 -2.7802699 -2.8749804 -2.9550519 -2.8850918 -2.7770209 -2.7995648 -2.876555 -2.8941822 -2.8166027 -2.7378552 -2.6573839 -2.5787036 -2.5309186 -2.4764743][-2.4804735 -2.611867 -2.8124769 -2.9877987 -2.9419875 -2.7366185 -2.6097884 -2.5566487 -2.5002115 -2.3488059 -2.2443504 -2.1824129 -2.1410651 -2.1540623 -2.1238215][-2.3672566 -2.4667568 -2.6859193 -2.9141917 -2.9456263 -2.7737784 -2.6067352 -2.4930477 -2.4211264 -2.2138698 -2.0449152 -1.9380007 -1.8784251 -1.9458055 -1.9455767][-2.1884727 -2.146389 -2.29927 -2.4922757 -2.5297062 -2.4188492 -2.2649486 -2.1460459 -2.2266457 -2.1566195 -1.9889607 -1.8296759 -1.7344158 -1.8376384 -1.8442643][-1.948199 -1.9097662 -2.1459246 -2.3413963 -2.2412429 -1.9248362 -1.4965794 -1.1755524 -1.4455886 -1.7417097 -1.7595756 -1.6492577 -1.5957048 -1.7798681 -1.8018284][-1.8580554 -2.0131009 -2.5128462 -2.8646724 -2.6388619 -1.9750755 -1.0897415 -0.34780693 -0.59093404 -1.170754 -1.3839359 -1.2970173 -1.2808161 -1.6188514 -1.7746379][-1.984159 -2.20079 -2.7913065 -3.2621875 -3.0954173 -2.4542389 -1.5381434 -0.58368039 -0.58182073 -1.1014984 -1.3024671 -1.1137247 -1.0066235 -1.4357302 -1.7634826][-2.249743 -2.4217699 -2.9404039 -3.3894382 -3.3358097 -2.9973264 -2.4372804 -1.6588757 -1.3584168 -1.5189981 -1.514149 -1.2017777 -0.98028708 -1.3736448 -1.7914941][-1.8642204 -1.9178247 -2.3228528 -2.6905327 -2.7391255 -2.652097 -2.3761823 -1.8319659 -1.4341154 -1.3217125 -1.2234969 -0.94470143 -0.68132067 -0.97414017 -1.4465213][-1.0401511 -0.93454957 -1.2186327 -1.5263884 -1.6755605 -1.7497776 -1.6137197 -1.2406101 -0.88584471 -0.65778232 -0.53932881 -0.37829685 -0.18327713 -0.40273619 -0.90390849][-1.1406858 -0.89305925 -0.98719454 -1.1639192 -1.3272092 -1.4784279 -1.4219909 -1.1919944 -0.94384503 -0.69974589 -0.52813768 -0.44486618 -0.35526896 -0.52142739 -0.940922][-2.4653289 -2.1423025 -2.0213993 -2.0111463 -2.1198096 -2.289058 -2.2650907 -2.1319132 -1.9907918 -1.8204234 -1.6672297 -1.6027219 -1.525718 -1.5427732 -1.7166364][-3.8034775 -3.4961638 -3.3138824 -3.2474847 -3.3365302 -3.4626477 -3.4233117 -3.3137746 -3.2101738 -3.0996413 -2.9934015 -2.92877 -2.8368318 -2.7708306 -2.7814808][-4.1929169 -3.9702761 -3.8337464 -3.8003936 -3.8912022 -3.9659295 -3.9180315 -3.8316951 -3.7562385 -3.6855757 -3.629498 -3.5850563 -3.5272927 -3.4907613 -3.4802918][-4.1469464 -4.020843 -3.9225678 -3.9059935 -3.9783041 -4.0298524 -3.996098 -3.9477375 -3.9123311 -3.8789864 -3.8474689 -3.8161023 -3.7875528 -3.7686841 -3.7442095]]...]
INFO - root - 2017-12-07 02:33:12.843703: step 1010, loss = 0.69, batch loss = 0.61 (11.3 examples/sec; 0.707 sec/batch; 65h:03m:42s remains)
INFO - root - 2017-12-07 02:33:19.772525: step 1020, loss = 0.60, batch loss = 0.53 (11.6 examples/sec; 0.691 sec/batch; 63h:36m:13s remains)
INFO - root - 2017-12-07 02:33:26.614279: step 1030, loss = 0.68, batch loss = 0.61 (11.5 examples/sec; 0.695 sec/batch; 64h:01m:14s remains)
INFO - root - 2017-12-07 02:33:33.570439: step 1040, loss = 0.81, batch loss = 0.74 (11.8 examples/sec; 0.679 sec/batch; 62h:28m:40s remains)
INFO - root - 2017-12-07 02:33:40.547634: step 1050, loss = 0.80, batch loss = 0.72 (11.3 examples/sec; 0.709 sec/batch; 65h:15m:50s remains)
INFO - root - 2017-12-07 02:33:47.448090: step 1060, loss = 0.67, batch loss = 0.60 (12.0 examples/sec; 0.666 sec/batch; 61h:20m:10s remains)
INFO - root - 2017-12-07 02:33:54.349426: step 1070, loss = 0.62, batch loss = 0.54 (11.2 examples/sec; 0.714 sec/batch; 65h:44m:49s remains)
INFO - root - 2017-12-07 02:34:01.285066: step 1080, loss = 0.68, batch loss = 0.61 (11.6 examples/sec; 0.688 sec/batch; 63h:18m:59s remains)
INFO - root - 2017-12-07 02:34:08.178923: step 1090, loss = 0.72, batch loss = 0.65 (11.3 examples/sec; 0.705 sec/batch; 64h:55m:37s remains)
INFO - root - 2017-12-07 02:34:15.106733: step 1100, loss = 0.86, batch loss = 0.79 (11.5 examples/sec; 0.695 sec/batch; 64h:00m:46s remains)
2017-12-07 02:34:15.656150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1226459 -2.1355982 -2.0935619 -2.1622512 -2.40995 -2.7309537 -3.1077788 -3.345048 -3.5411456 -3.9172091 -4.334074 -4.5732851 -4.4144115 -3.7721479 -3.1898854][-2.12205 -2.1275744 -2.0267704 -1.8136334 -1.6933563 -1.8280096 -2.2222269 -2.5553861 -2.9239709 -3.5708036 -4.1674266 -4.4826708 -4.3626227 -3.7647016 -3.2494416][-2.0252931 -2.1344624 -2.1815557 -1.9920385 -1.7624474 -1.8479309 -2.3000281 -2.6368611 -2.9652979 -3.5364885 -3.9957707 -4.2507749 -4.1714692 -3.6455302 -3.2165203][-2.0476036 -2.2515755 -2.4330969 -2.382648 -2.1800568 -2.257288 -2.657177 -2.9053159 -3.1701446 -3.5892377 -3.8424895 -4.050395 -4.0866289 -3.6632295 -3.2530537][-2.249948 -2.5543575 -2.8555896 -2.9080224 -2.7102032 -2.6585515 -2.7976632 -2.8086009 -2.9849129 -3.2846432 -3.4368832 -3.7570133 -4.0572815 -3.8240833 -3.4176996][-2.0880482 -2.3581457 -2.6703029 -2.7496424 -2.5509493 -2.4281664 -2.4178364 -2.3267217 -2.5303493 -2.8110883 -2.9381781 -3.389025 -3.9317303 -3.9126794 -3.5637205][-1.7837005 -1.829438 -2.025861 -2.055011 -1.8726039 -1.7726712 -1.7366252 -1.704185 -2.0391717 -2.3838491 -2.5533757 -3.0723438 -3.7061355 -3.84129 -3.5963895][-2.0041595 -1.9078345 -2.0020826 -1.9351408 -1.7190564 -1.5977561 -1.5053639 -1.5119014 -1.9000466 -2.2553182 -2.4473577 -2.9336963 -3.4763269 -3.64988 -3.5160668][-2.317807 -2.2429473 -2.2977204 -2.1647823 -1.9154098 -1.7547777 -1.617265 -1.6680272 -2.0372362 -2.3243396 -2.504787 -2.9023898 -3.2835641 -3.4343219 -3.412926][-2.3151095 -2.2698469 -2.2602298 -2.0570838 -1.8071921 -1.6256857 -1.4890511 -1.6068854 -1.9446683 -2.1801057 -2.3807211 -2.723855 -3.0092986 -3.1940846 -3.3200283][-2.1754198 -2.1917412 -2.1810498 -1.9960251 -1.7873025 -1.5987635 -1.456738 -1.6043768 -1.8831692 -2.0960472 -2.3234153 -2.6098433 -2.8217034 -2.9979496 -3.1944661][-2.1668377 -2.2118573 -2.2614617 -2.2024729 -2.1339855 -2.0302417 -1.9434788 -2.0596254 -2.202697 -2.3692551 -2.580795 -2.7919335 -2.9567075 -3.0642667 -3.1982126][-2.6119971 -2.6247067 -2.6700435 -2.6451898 -2.6325192 -2.575491 -2.5201998 -2.519824 -2.4641888 -2.5425096 -2.6879036 -2.8609195 -3.0844765 -3.1896811 -3.2663434][-3.0638924 -3.0750637 -3.0423408 -2.8955328 -2.766005 -2.6316562 -2.5420346 -2.430254 -2.2598295 -2.2867928 -2.3678641 -2.5425253 -2.8545694 -3.0071182 -3.0843191][-2.9690666 -3.0040598 -2.9469042 -2.7475076 -2.565352 -2.4190743 -2.3330531 -2.1623783 -1.9781599 -2.0075505 -2.0972788 -2.3432257 -2.7007728 -2.8317573 -2.855125]]...]
INFO - root - 2017-12-07 02:34:22.600799: step 1110, loss = 0.87, batch loss = 0.80 (11.6 examples/sec; 0.691 sec/batch; 63h:37m:05s remains)
INFO - root - 2017-12-07 02:34:29.614080: step 1120, loss = 0.94, batch loss = 0.87 (11.8 examples/sec; 0.676 sec/batch; 62h:12m:36s remains)
INFO - root - 2017-12-07 02:34:36.527316: step 1130, loss = 0.69, batch loss = 0.62 (11.6 examples/sec; 0.691 sec/batch; 63h:35m:31s remains)
INFO - root - 2017-12-07 02:34:43.511323: step 1140, loss = 0.69, batch loss = 0.62 (11.5 examples/sec; 0.694 sec/batch; 63h:51m:12s remains)
INFO - root - 2017-12-07 02:34:50.177689: step 1150, loss = 0.74, batch loss = 0.67 (11.6 examples/sec; 0.691 sec/batch; 63h:36m:26s remains)
INFO - root - 2017-12-07 02:34:57.121191: step 1160, loss = 0.65, batch loss = 0.58 (11.5 examples/sec; 0.693 sec/batch; 63h:47m:39s remains)
INFO - root - 2017-12-07 02:35:04.035697: step 1170, loss = 0.60, batch loss = 0.53 (11.8 examples/sec; 0.680 sec/batch; 62h:34m:32s remains)
INFO - root - 2017-12-07 02:35:10.977587: step 1180, loss = 0.76, batch loss = 0.69 (11.6 examples/sec; 0.691 sec/batch; 63h:34m:10s remains)
INFO - root - 2017-12-07 02:35:17.893180: step 1190, loss = 0.72, batch loss = 0.64 (11.6 examples/sec; 0.689 sec/batch; 63h:26m:41s remains)
INFO - root - 2017-12-07 02:35:24.847275: step 1200, loss = 0.70, batch loss = 0.63 (11.4 examples/sec; 0.699 sec/batch; 64h:20m:17s remains)
2017-12-07 02:35:25.374143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0408015 -4.59016 -4.3270078 -4.00842 -3.6759768 -3.607784 -3.8393836 -4.3179464 -4.8650584 -5.078135 -4.8719549 -4.4198155 -3.5360827 -2.455708 -1.985507][-4.5001721 -4.3180804 -4.257514 -4.0455317 -3.7073357 -3.6053205 -3.8111033 -4.3329782 -4.9351635 -5.3051438 -5.2726631 -4.7371011 -3.6117742 -2.3685949 -1.7760096][-3.5165932 -3.6452746 -3.7458739 -3.6454725 -3.2761416 -2.9757791 -3.0041664 -3.5429974 -4.2554812 -4.9053617 -5.2117662 -4.7523313 -3.5745621 -2.3300729 -1.7137439][-2.651731 -3.024076 -3.2396455 -3.222301 -2.8254528 -2.2525692 -1.9672596 -2.4171243 -3.3677778 -4.42633 -5.1078949 -4.9188414 -3.9278791 -2.6780591 -1.882376][-1.803555 -2.411653 -2.7491102 -2.8296494 -2.4653258 -1.7213967 -1.0078566 -1.0995877 -2.2779462 -3.7791762 -4.8210487 -5.0939331 -4.5740204 -3.4482985 -2.3569713][-1.2297533 -2.0716045 -2.4949284 -2.5163732 -2.0120206 -0.99881339 0.30292082 0.75246286 -0.64354467 -2.6748075 -4.1376262 -4.9426126 -5.0445094 -4.332962 -3.1499629][-1.2963648 -2.449924 -2.997045 -2.8096757 -1.9727702 -0.47550797 1.6190987 2.7310781 1.244163 -1.1886115 -3.0711451 -4.322145 -4.9675059 -4.8715544 -3.9945536][-1.9392495 -3.5150261 -4.3631344 -4.0884638 -3.0521214 -1.2418792 1.3110313 2.9028416 1.9168644 -0.12455082 -1.9497337 -3.3173251 -4.2459545 -4.7189856 -4.3911819][-2.2986465 -4.12175 -5.2987556 -5.2417011 -4.3569431 -2.6060834 -0.27879953 1.1346684 0.86256838 -0.1364274 -1.2834964 -2.3458512 -3.19706 -3.9507792 -4.1626191][-2.2367473 -3.9191771 -5.1551476 -5.4340944 -4.9004641 -3.4532671 -1.714325 -0.79145765 -0.63142705 -0.70140147 -1.030582 -1.5403016 -2.0798714 -2.7874117 -3.2670877][-2.1916292 -3.3843515 -4.3391042 -4.7921338 -4.5744605 -3.5699191 -2.5099063 -2.048389 -1.6981356 -1.2885227 -1.1117959 -1.1511195 -1.3924131 -1.9219248 -2.3587725][-2.2282927 -2.782804 -3.3522525 -3.87921 -3.9347126 -3.4178367 -2.9613552 -2.7737925 -2.3781037 -1.8835092 -1.6033435 -1.4704368 -1.6750081 -2.1410823 -2.4190855][-2.8150985 -2.9421573 -3.2763271 -3.8551557 -4.1062379 -3.8789992 -3.6537969 -3.4565995 -3.0430446 -2.6444724 -2.4437144 -2.3193276 -2.601954 -3.1111872 -3.3727651][-3.6396286 -3.6176467 -3.8694408 -4.4182787 -4.69339 -4.5150018 -4.2295146 -3.8798196 -3.4717853 -3.2088113 -3.1031737 -3.0282316 -3.2946129 -3.74801 -4.0674257][-3.6711054 -3.6096911 -3.7909486 -4.1672349 -4.3485546 -4.2194738 -3.9572749 -3.6258945 -3.3596857 -3.2243323 -3.1829691 -3.2039576 -3.4221237 -3.7320101 -4.0122662]]...]
INFO - root - 2017-12-07 02:35:32.247221: step 1210, loss = 0.76, batch loss = 0.68 (12.0 examples/sec; 0.666 sec/batch; 61h:18m:33s remains)
INFO - root - 2017-12-07 02:35:39.243355: step 1220, loss = 0.79, batch loss = 0.72 (11.3 examples/sec; 0.709 sec/batch; 65h:13m:38s remains)
INFO - root - 2017-12-07 02:35:46.165031: step 1230, loss = 0.68, batch loss = 0.61 (11.5 examples/sec; 0.698 sec/batch; 64h:11m:41s remains)
INFO - root - 2017-12-07 02:35:53.139629: step 1240, loss = 0.80, batch loss = 0.73 (11.1 examples/sec; 0.722 sec/batch; 66h:27m:29s remains)
INFO - root - 2017-12-07 02:36:00.197812: step 1250, loss = 0.72, batch loss = 0.65 (11.4 examples/sec; 0.699 sec/batch; 64h:20m:35s remains)
INFO - root - 2017-12-07 02:36:06.860961: step 1260, loss = 0.57, batch loss = 0.49 (11.9 examples/sec; 0.674 sec/batch; 61h:59m:28s remains)
INFO - root - 2017-12-07 02:36:13.863378: step 1270, loss = 0.68, batch loss = 0.61 (11.6 examples/sec; 0.690 sec/batch; 63h:29m:02s remains)
INFO - root - 2017-12-07 02:36:20.816030: step 1280, loss = 0.87, batch loss = 0.80 (12.0 examples/sec; 0.665 sec/batch; 61h:08m:53s remains)
INFO - root - 2017-12-07 02:36:27.750593: step 1290, loss = 0.77, batch loss = 0.70 (11.6 examples/sec; 0.690 sec/batch; 63h:26m:52s remains)
INFO - root - 2017-12-07 02:36:34.674773: step 1300, loss = 0.86, batch loss = 0.79 (11.7 examples/sec; 0.683 sec/batch; 62h:49m:08s remains)
2017-12-07 02:36:35.201938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3058329 -2.3343225 -2.374537 -2.4463139 -2.4411893 -2.3709471 -2.4403958 -2.3847272 -2.1315064 -2.3087549 -2.5204091 -2.4286082 -2.3795652 -2.4235241 -2.304585][-2.3386717 -2.3344746 -2.3408797 -2.3974915 -2.4140944 -2.3798113 -2.4966078 -2.4860077 -2.2437806 -2.4447563 -2.6776471 -2.5335531 -2.4179437 -2.4396162 -2.3084652][-2.3416216 -2.3247039 -2.2948716 -2.316453 -2.3779252 -2.4267011 -2.6067085 -2.6293125 -2.3811405 -2.6034088 -2.8674309 -2.6873548 -2.518044 -2.5079618 -2.3416119][-2.3249478 -2.3286431 -2.2659059 -2.2151613 -2.3216605 -2.4865575 -2.7272894 -2.741864 -2.4567397 -2.68956 -2.9873035 -2.8012753 -2.6128135 -2.5843287 -2.3842037][-2.34053 -2.3835964 -2.292625 -2.136812 -2.2553024 -2.5331993 -2.8198285 -2.7848046 -2.439126 -2.6750526 -3.0072918 -2.8500347 -2.6760526 -2.6401944 -2.4170694][-2.36047 -2.4385359 -2.3312881 -2.0784359 -2.1639898 -2.5132713 -2.8359723 -2.7385457 -2.3232474 -2.5603197 -2.9290643 -2.8313348 -2.7076535 -2.6777463 -2.4414606][-2.3841324 -2.4712014 -2.3650985 -2.0498161 -2.0554335 -2.3977759 -2.7373528 -2.587851 -2.1277566 -2.3855844 -2.7871292 -2.7553539 -2.6989019 -2.6908259 -2.4520359][-2.5122426 -2.5805147 -2.4793091 -2.1408887 -2.0338216 -2.2886233 -2.6043439 -2.4079442 -1.9197254 -2.203805 -2.6256804 -2.6423645 -2.6606283 -2.6893814 -2.45153][-2.7204792 -2.7861254 -2.7168028 -2.3980238 -2.1937156 -2.3392673 -2.6030016 -2.3604743 -1.8338506 -2.1039426 -2.4967623 -2.5145035 -2.5922165 -2.6611471 -2.4280887][-2.877305 -2.9416137 -2.9307356 -2.7009244 -2.4725039 -2.5338697 -2.751941 -2.495374 -1.9386313 -2.1706781 -2.5022497 -2.4792044 -2.5706861 -2.6530578 -2.4094818][-2.9892664 -3.0371394 -3.0782537 -2.9688025 -2.7772982 -2.7939558 -2.9861083 -2.7487936 -2.2040427 -2.4051642 -2.6731496 -2.5950119 -2.6570508 -2.7136407 -2.4396367][-3.04419 -3.0639582 -3.1241226 -3.1066766 -2.9666815 -2.9628835 -3.1382372 -2.9417377 -2.4519587 -2.6364613 -2.8605685 -2.743278 -2.7533681 -2.7614913 -2.4699497][-2.9715385 -2.9831052 -3.0542464 -3.0984919 -3.0076823 -3.0143707 -3.1793303 -3.0227113 -2.6108947 -2.7802787 -2.9652634 -2.8322971 -2.7982991 -2.745893 -2.4507298][-2.8525758 -2.8787975 -2.9580965 -3.0392542 -2.9993763 -3.0300431 -3.1883855 -3.0716197 -2.7456341 -2.8903494 -3.0284529 -2.8920774 -2.8284361 -2.7178049 -2.4242244][-2.6913953 -2.7204401 -2.7914588 -2.8866343 -2.8841772 -2.9250102 -3.0655367 -2.9899611 -2.7544432 -2.8746407 -2.9727826 -2.8578377 -2.7971649 -2.6669831 -2.4016974]]...]
INFO - root - 2017-12-07 02:36:42.197114: step 1310, loss = 0.77, batch loss = 0.70 (11.5 examples/sec; 0.693 sec/batch; 63h:45m:45s remains)
INFO - root - 2017-12-07 02:36:49.131686: step 1320, loss = 0.64, batch loss = 0.57 (11.5 examples/sec; 0.696 sec/batch; 64h:04m:00s remains)
INFO - root - 2017-12-07 02:36:56.050449: step 1330, loss = 0.75, batch loss = 0.68 (11.5 examples/sec; 0.699 sec/batch; 64h:15m:44s remains)
INFO - root - 2017-12-07 02:37:03.028124: step 1340, loss = 0.74, batch loss = 0.66 (11.3 examples/sec; 0.708 sec/batch; 65h:07m:44s remains)
INFO - root - 2017-12-07 02:37:09.994804: step 1350, loss = 0.64, batch loss = 0.56 (11.5 examples/sec; 0.694 sec/batch; 63h:51m:44s remains)
INFO - root - 2017-12-07 02:37:16.942963: step 1360, loss = 0.75, batch loss = 0.67 (11.7 examples/sec; 0.685 sec/batch; 63h:01m:11s remains)
INFO - root - 2017-12-07 02:37:23.735024: step 1370, loss = 0.69, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 51h:41m:24s remains)
INFO - root - 2017-12-07 02:37:30.585202: step 1380, loss = 0.70, batch loss = 0.63 (11.4 examples/sec; 0.701 sec/batch; 64h:26m:08s remains)
INFO - root - 2017-12-07 02:37:37.585030: step 1390, loss = 0.72, batch loss = 0.65 (11.4 examples/sec; 0.703 sec/batch; 64h:42m:14s remains)
INFO - root - 2017-12-07 02:37:44.506428: step 1400, loss = 0.50, batch loss = 0.43 (11.7 examples/sec; 0.686 sec/batch; 63h:08m:02s remains)
2017-12-07 02:37:45.083869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3673735 -3.6085882 -3.7374692 -3.815742 -3.8580322 -3.8026085 -3.7492504 -3.8413591 -3.9898462 -3.894074 -3.7419693 -3.6840422 -3.632941 -3.5939841 -3.5816655][-3.0527382 -3.3880424 -3.6013246 -3.7620749 -3.7982516 -3.6883264 -3.5616031 -3.5817318 -3.6954663 -3.5599484 -3.4187603 -3.4151993 -3.3902235 -3.4258361 -3.5076993][-2.6182041 -3.0440016 -3.3237865 -3.5611699 -3.5989656 -3.4167886 -3.1882935 -3.0979908 -3.1471872 -2.989387 -2.8655415 -2.8931322 -2.8809295 -2.97503 -3.1338253][-2.1641507 -2.6354632 -2.9621329 -3.2807264 -3.3737831 -3.18684 -2.9434702 -2.7914162 -2.7669957 -2.5435624 -2.3517811 -2.325 -2.2697103 -2.3577015 -2.4962382][-1.8785467 -2.2675004 -2.5371737 -2.8895097 -3.0864117 -3.0248594 -2.8953354 -2.7787876 -2.7210021 -2.386023 -2.0261283 -1.8581398 -1.7190285 -1.7417762 -1.7638669][-2.1446 -2.3436995 -2.4618087 -2.7305732 -2.9343452 -2.9472175 -2.8810215 -2.8051138 -2.7916555 -2.4499247 -2.0098734 -1.7665553 -1.6041226 -1.5498581 -1.4179783][-2.808542 -2.7709136 -2.7114182 -2.7901096 -2.8571553 -2.7989645 -2.661622 -2.5744662 -2.6735239 -2.5324717 -2.2872903 -2.232945 -2.2630115 -2.267837 -2.0804272][-3.2406545 -2.970005 -2.7636747 -2.701611 -2.6931977 -2.6429436 -2.5174923 -2.4338443 -2.5548582 -2.5446248 -2.5280313 -2.7375004 -3.1012998 -3.3554604 -3.3149979][-3.3108437 -2.8827486 -2.5917044 -2.4769075 -2.5136123 -2.6694193 -2.790782 -2.859252 -2.915278 -2.8154855 -2.7702503 -2.9722362 -3.4662197 -3.9103432 -4.0752096][-3.4525476 -2.9805012 -2.6137061 -2.4196026 -2.4558432 -2.7680454 -3.1454601 -3.4151721 -3.4622428 -3.2685573 -3.0750613 -3.0386181 -3.3652775 -3.7615423 -4.0014887][-3.7999976 -3.4171705 -3.016614 -2.7029958 -2.6026795 -2.8206697 -3.2075961 -3.506695 -3.5029867 -3.25527 -2.9794655 -2.7786126 -2.9113135 -3.1951108 -3.4350948][-4.13828 -3.988559 -3.7101409 -3.3822036 -3.1506331 -3.1550331 -3.3229125 -3.4357762 -3.3033571 -2.99535 -2.6707764 -2.4141912 -2.4328685 -2.6411538 -2.9137883][-4.282506 -4.3910408 -4.3297439 -4.1105919 -3.8654046 -3.7270093 -3.6752498 -3.564918 -3.3130832 -2.99886 -2.7153153 -2.526691 -2.5069766 -2.6523545 -2.9355154][-4.2615271 -4.53865 -4.6498466 -4.5385952 -4.36101 -4.2180157 -4.0570135 -3.78183 -3.4436946 -3.1724205 -3.0082712 -2.980247 -3.0002451 -3.1000843 -3.3361897][-3.9401236 -4.3131452 -4.5501256 -4.5037141 -4.4480691 -4.4392133 -4.3157253 -3.9579115 -3.5320051 -3.2525609 -3.1603866 -3.2283635 -3.2798543 -3.3409843 -3.5041056]]...]
INFO - root - 2017-12-07 02:37:52.058727: step 1410, loss = 0.72, batch loss = 0.65 (11.0 examples/sec; 0.725 sec/batch; 66h:41m:28s remains)
INFO - root - 2017-12-07 02:37:59.075748: step 1420, loss = 0.73, batch loss = 0.66 (11.1 examples/sec; 0.722 sec/batch; 66h:23m:40s remains)
INFO - root - 2017-12-07 02:38:06.083888: step 1430, loss = 0.82, batch loss = 0.75 (11.5 examples/sec; 0.696 sec/batch; 64h:01m:19s remains)
INFO - root - 2017-12-07 02:38:13.025227: step 1440, loss = 0.75, batch loss = 0.68 (11.4 examples/sec; 0.701 sec/batch; 64h:26m:07s remains)
INFO - root - 2017-12-07 02:38:19.942853: step 1450, loss = 0.74, batch loss = 0.67 (11.7 examples/sec; 0.686 sec/batch; 63h:04m:08s remains)
INFO - root - 2017-12-07 02:38:26.892660: step 1460, loss = 0.95, batch loss = 0.88 (11.3 examples/sec; 0.707 sec/batch; 64h:59m:00s remains)
INFO - root - 2017-12-07 02:38:33.826352: step 1470, loss = 0.76, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:20m:09s remains)
INFO - root - 2017-12-07 02:38:40.814148: step 1480, loss = 0.89, batch loss = 0.82 (11.5 examples/sec; 0.694 sec/batch; 63h:47m:55s remains)
INFO - root - 2017-12-07 02:38:47.558044: step 1490, loss = 0.87, batch loss = 0.80 (11.7 examples/sec; 0.684 sec/batch; 62h:55m:42s remains)
INFO - root - 2017-12-07 02:38:54.519431: step 1500, loss = 0.87, batch loss = 0.80 (11.7 examples/sec; 0.682 sec/batch; 62h:41m:12s remains)
2017-12-07 02:38:55.077899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0688596 -2.5848799 -2.4616952 -2.7232685 -3.2051611 -3.7132487 -4.1203308 -4.1397166 -3.68401 -2.9097774 -2.3980949 -2.6995926 -3.6684349 -4.3033757 -3.8681953][-2.5503011 -1.9947782 -2.0057657 -2.628706 -3.4829686 -4.15579 -4.4773602 -4.2428408 -3.4751098 -2.4245114 -1.917115 -2.5369377 -3.9036996 -4.7527547 -4.1769786][-2.1498098 -1.5516586 -1.7139482 -2.6298418 -3.7477584 -4.5004849 -4.6694088 -4.1936407 -3.1755152 -1.9240346 -1.4550014 -2.331666 -3.9845109 -5.0018234 -4.3864107][-1.7654147 -1.1724794 -1.4707308 -2.5020285 -3.70611 -4.4793992 -4.5541964 -3.9673777 -2.8322339 -1.4830127 -1.043299 -2.0518904 -3.8302956 -4.9107385 -4.3294053][-1.3623021 -0.89543343 -1.3378088 -2.3639874 -3.5008876 -4.2542844 -4.3663721 -3.8415027 -2.7835832 -1.5340874 -1.1196065 -2.1141677 -3.8175468 -4.8291736 -4.2572613][-1.1994772 -1.0078654 -1.6257279 -2.6050396 -3.5212295 -4.1103072 -4.1941195 -3.7407084 -2.8878105 -1.9333708 -1.6637349 -2.6054606 -4.0988736 -4.9423442 -4.3251858][-1.4605262 -1.5754666 -2.29558 -3.1770821 -3.8021932 -4.1320281 -4.0967755 -3.6638033 -2.98809 -2.3325324 -2.3062272 -3.2812443 -4.5914941 -5.2294893 -4.5201163][-1.7701445 -2.0260339 -2.7053292 -3.4459543 -3.893671 -4.08115 -3.9645784 -3.5222023 -2.8707056 -2.2835281 -2.4100556 -3.51022 -4.8070679 -5.3497653 -4.6142707][-1.9022729 -2.1686902 -2.6527691 -3.1024978 -3.4077663 -3.6026177 -3.6167531 -3.3612652 -2.7957537 -2.1116796 -2.1129415 -3.1544123 -4.4081745 -4.9100585 -4.2691889][-2.2699306 -2.5615382 -2.8286769 -2.9237251 -3.0112967 -3.2525244 -3.5051842 -3.5310998 -3.1949182 -2.543462 -2.34696 -3.1434288 -4.1446733 -4.4338369 -3.7907274][-2.8918734 -3.1078751 -3.2035766 -3.1197958 -3.1270986 -3.478987 -3.9273031 -4.1043115 -3.8839936 -3.3005841 -2.9554205 -3.4492612 -4.1433973 -4.1854067 -3.4761434][-3.5809503 -3.6666613 -3.6438422 -3.4970217 -3.4740577 -3.8523943 -4.3896203 -4.6730127 -4.536 -4.0113482 -3.552876 -3.7378576 -4.1374574 -4.0199027 -3.3070936][-3.5855227 -3.61666 -3.5802875 -3.4735584 -3.4380481 -3.7089353 -4.1729083 -4.5068278 -4.4744554 -4.0668168 -3.6465437 -3.6489608 -3.8715596 -3.7615895 -3.1996827][-3.0935936 -3.1343985 -3.1391997 -3.1081233 -3.1172273 -3.2839379 -3.6229398 -3.9370606 -3.9685771 -3.6686416 -3.3532858 -3.2874789 -3.4340763 -3.4451287 -3.1244912][-2.9373102 -2.9739456 -2.9723997 -2.9467666 -2.9686894 -3.0709653 -3.3008409 -3.5558879 -3.6049736 -3.4005315 -3.1838348 -3.112977 -3.20472 -3.2719882 -3.1193256]]...]
INFO - root - 2017-12-07 02:39:02.012181: step 1510, loss = 0.81, batch loss = 0.74 (11.7 examples/sec; 0.686 sec/batch; 63h:03m:47s remains)
INFO - root - 2017-12-07 02:39:08.978882: step 1520, loss = 0.63, batch loss = 0.55 (11.2 examples/sec; 0.717 sec/batch; 65h:52m:48s remains)
INFO - root - 2017-12-07 02:39:15.867084: step 1530, loss = 0.74, batch loss = 0.67 (11.9 examples/sec; 0.670 sec/batch; 61h:34m:27s remains)
INFO - root - 2017-12-07 02:39:22.771017: step 1540, loss = 0.84, batch loss = 0.76 (11.6 examples/sec; 0.689 sec/batch; 63h:21m:24s remains)
INFO - root - 2017-12-07 02:39:29.682388: step 1550, loss = 0.69, batch loss = 0.62 (11.4 examples/sec; 0.703 sec/batch; 64h:37m:15s remains)
INFO - root - 2017-12-07 02:39:36.655872: step 1560, loss = 0.76, batch loss = 0.68 (11.6 examples/sec; 0.690 sec/batch; 63h:27m:04s remains)
INFO - root - 2017-12-07 02:39:43.593121: step 1570, loss = 0.75, batch loss = 0.68 (11.7 examples/sec; 0.683 sec/batch; 62h:44m:40s remains)
INFO - root - 2017-12-07 02:39:50.558737: step 1580, loss = 0.65, batch loss = 0.58 (11.4 examples/sec; 0.704 sec/batch; 64h:44m:06s remains)
INFO - root - 2017-12-07 02:39:57.454683: step 1590, loss = 0.92, batch loss = 0.85 (11.5 examples/sec; 0.699 sec/batch; 64h:12m:59s remains)
INFO - root - 2017-12-07 02:40:04.180223: step 1600, loss = 0.76, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 61h:56m:40s remains)
2017-12-07 02:40:04.762096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1293612 -4.1992636 -4.2594848 -4.3219471 -4.39615 -4.4470582 -4.46682 -4.4707561 -4.4491448 -4.403873 -4.3162389 -4.1920385 -4.0684047 -3.937037 -3.8236809][-4.3500862 -4.4611726 -4.5117512 -4.526679 -4.5803947 -4.6632776 -4.7227731 -4.7582326 -4.7628655 -4.7468095 -4.7007761 -4.6349745 -4.5665927 -4.4427066 -4.2816682][-4.3221135 -4.4370427 -4.4659452 -4.4346876 -4.4384923 -4.4698596 -4.468595 -4.4380713 -4.3742189 -4.3025322 -4.2546477 -4.2604785 -4.2990041 -4.274508 -4.1935873][-4.08174 -4.2857852 -4.5112534 -4.781796 -5.1022153 -5.3522596 -5.4461603 -5.4119954 -5.2922945 -5.1993933 -5.1960554 -5.2647867 -5.2832937 -5.1237807 -4.8249154][-3.3157477 -3.2496924 -3.3170979 -3.6199861 -4.071516 -4.44124 -4.62921 -4.6601086 -4.6206479 -4.6592827 -4.8264194 -5.0655246 -5.2066846 -5.1100674 -4.8146896][-2.6414487 -2.2521498 -2.0587575 -2.2096388 -2.5553951 -2.8192878 -2.9171238 -2.8838577 -2.9090486 -3.1055677 -3.4477205 -3.8391709 -4.1098747 -4.1095576 -3.9322999][-2.3899782 -1.7540359 -1.3006468 -1.2202649 -1.3768308 -1.5193455 -1.5166607 -1.4248817 -1.5526462 -1.9757116 -2.5784066 -3.2249744 -3.7085843 -3.8276019 -3.6981335][-2.5360732 -1.751786 -1.0286844 -0.53304958 -0.18848085 0.12861395 0.50809574 0.80427313 0.59288931 -0.065934658 -0.94127536 -1.8779593 -2.662446 -3.0706244 -3.183682][-3.1050749 -2.5822673 -2.053436 -1.5711653 -1.1021311 -0.55425954 0.12822247 0.68852329 0.60930347 -0.001103878 -0.8419342 -1.6957693 -2.3624368 -2.7182341 -2.8652782][-3.5762737 -3.3597827 -3.1335616 -2.8765378 -2.6256192 -2.2827361 -1.7364378 -1.225112 -1.1410329 -1.4402819 -1.9245696 -2.4425173 -2.88444 -3.1425843 -3.2638905][-3.7400014 -3.7198286 -3.6535845 -3.4906831 -3.3236189 -3.069097 -2.6418905 -2.2545695 -2.1017358 -2.147465 -2.3300917 -2.6475592 -3.056319 -3.3888087 -3.6183887][-3.5259087 -3.6132998 -3.6666617 -3.6495376 -3.6411958 -3.5094275 -3.1854637 -2.8720679 -2.6748481 -2.5631261 -2.5868483 -2.854393 -3.2925079 -3.6825981 -3.9650002][-3.0795081 -3.0363643 -3.0005741 -3.0260358 -3.1650867 -3.2195797 -3.1050711 -2.9769731 -2.8880963 -2.775286 -2.7570295 -3.0299697 -3.4439349 -3.7764378 -4.00913][-2.7796564 -2.4804764 -2.2240002 -2.1606364 -2.316987 -2.4677143 -2.5091424 -2.51852 -2.518178 -2.4413295 -2.417697 -2.6263204 -2.9166317 -3.1259165 -3.331917][-2.9578419 -2.5818963 -2.3063772 -2.2952831 -2.5045161 -2.7339561 -2.884716 -2.9688249 -3.017437 -2.9808927 -2.9717517 -3.0809746 -3.1749187 -3.1549633 -3.1700072]]...]
INFO - root - 2017-12-07 02:40:11.682863: step 1610, loss = 0.85, batch loss = 0.77 (11.4 examples/sec; 0.703 sec/batch; 64h:36m:11s remains)
INFO - root - 2017-12-07 02:40:18.685248: step 1620, loss = 0.61, batch loss = 0.54 (11.1 examples/sec; 0.722 sec/batch; 66h:19m:10s remains)
INFO - root - 2017-12-07 02:40:25.618457: step 1630, loss = 0.81, batch loss = 0.74 (11.2 examples/sec; 0.711 sec/batch; 65h:22m:42s remains)
INFO - root - 2017-12-07 02:40:32.560794: step 1640, loss = 0.69, batch loss = 0.62 (11.4 examples/sec; 0.704 sec/batch; 64h:41m:33s remains)
INFO - root - 2017-12-07 02:40:39.600716: step 1650, loss = 0.70, batch loss = 0.63 (11.1 examples/sec; 0.718 sec/batch; 65h:56m:41s remains)
INFO - root - 2017-12-07 02:40:46.533266: step 1660, loss = 0.75, batch loss = 0.68 (11.8 examples/sec; 0.678 sec/batch; 62h:16m:00s remains)
INFO - root - 2017-12-07 02:40:53.535216: step 1670, loss = 0.71, batch loss = 0.63 (11.2 examples/sec; 0.714 sec/batch; 65h:34m:45s remains)
INFO - root - 2017-12-07 02:41:00.478118: step 1680, loss = 0.86, batch loss = 0.78 (11.6 examples/sec; 0.687 sec/batch; 63h:10m:24s remains)
INFO - root - 2017-12-07 02:41:07.394731: step 1690, loss = 0.70, batch loss = 0.63 (11.6 examples/sec; 0.690 sec/batch; 63h:26m:17s remains)
INFO - root - 2017-12-07 02:41:14.304860: step 1700, loss = 0.81, batch loss = 0.73 (11.3 examples/sec; 0.708 sec/batch; 65h:05m:39s remains)
2017-12-07 02:41:14.846028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7791927 -1.0501804 -0.915174 -1.4713562 -2.1361785 -2.7026687 -2.8448017 -2.6740506 -2.5807467 -2.4674215 -2.4434447 -2.634181 -2.9869857 -2.9376056 -2.2260914][-1.1289899 -0.27688837 -0.18377256 -0.86889529 -1.789206 -2.6502867 -3.0076761 -2.8720922 -2.6191797 -2.4947147 -2.5881863 -2.8069108 -3.0692232 -2.8037415 -1.9283154][-1.0581443 -0.08087635 0.075740814 -0.54612947 -1.4659944 -2.3265653 -2.6829634 -2.4731417 -2.0549514 -1.94505 -2.2093263 -2.5311449 -2.7347233 -2.3920424 -1.6101546][-1.7855518 -0.89800119 -0.69282556 -1.0871077 -1.7229998 -2.2570364 -2.3019309 -1.8369203 -1.2933998 -1.298804 -1.7328522 -2.1351106 -2.3130159 -2.0651922 -1.5949206][-2.5370536 -1.9355016 -1.7561908 -1.9037774 -2.130904 -2.1554787 -1.7667985 -1.0630705 -0.59146261 -0.89813828 -1.5040762 -1.9257653 -2.0556791 -1.9037898 -1.7084703][-2.808718 -2.438756 -2.3424385 -2.3095732 -2.1147804 -1.5337973 -0.60694981 0.34197712 0.61650658 -0.13889742 -1.0706236 -1.6776679 -1.9335418 -1.9457688 -1.9182725][-2.8537378 -2.532258 -2.4387131 -2.3065016 -1.8413939 -0.78845978 0.62236786 1.8445177 2.0345635 1.0210524 -0.14014959 -1.0110614 -1.590694 -1.9345732 -2.0453198][-3.0526853 -2.8104818 -2.6941051 -2.5231977 -2.0178509 -0.90248418 0.53057909 1.7177124 1.9395976 1.1015644 0.13443756 -0.6166544 -1.2161305 -1.7330453 -1.9359312][-3.3963752 -3.3898773 -3.4038563 -3.2751527 -2.8532414 -2.0087726 -1.0865192 -0.44565034 -0.33777237 -0.74580693 -1.1715827 -1.4295948 -1.6379392 -2.0004504 -2.1518016][-3.7783418 -3.8728642 -3.9491653 -3.8491435 -3.5418015 -3.0256217 -2.6277885 -2.4941704 -2.5670574 -2.7810645 -2.9114547 -2.7968369 -2.5948119 -2.7067876 -2.7691731][-3.8066502 -3.8341699 -3.8367841 -3.743088 -3.5709505 -3.2772741 -3.1139832 -3.0816574 -3.1135683 -3.221303 -3.3363562 -3.1983693 -2.8573394 -2.8569124 -2.8902149][-3.3853359 -3.3908381 -3.4259593 -3.4483237 -3.39846 -3.2236028 -3.0997429 -2.9455266 -2.7656343 -2.7467656 -2.8988492 -2.8880591 -2.6202102 -2.5966229 -2.5780058][-2.8969388 -2.8464324 -2.928333 -3.0666933 -3.1120415 -3.0515256 -3.0248165 -2.8692236 -2.598166 -2.4919534 -2.5708094 -2.5783544 -2.397325 -2.380574 -2.3281877][-2.7479117 -2.5705047 -2.5830531 -2.7041235 -2.7399921 -2.7021277 -2.7344089 -2.6727843 -2.5054564 -2.4465475 -2.4560215 -2.414721 -2.2612386 -2.2344522 -2.2003136][-2.8995302 -2.7240758 -2.6886816 -2.7560506 -2.7719364 -2.7397244 -2.7555773 -2.7267394 -2.6477809 -2.6566863 -2.6617112 -2.6131568 -2.4847088 -2.4223979 -2.3776746]]...]
INFO - root - 2017-12-07 02:41:21.746936: step 1710, loss = 1.19, batch loss = 1.12 (11.5 examples/sec; 0.698 sec/batch; 64h:07m:42s remains)
INFO - root - 2017-12-07 02:41:28.576825: step 1720, loss = 0.60, batch loss = 0.53 (11.5 examples/sec; 0.694 sec/batch; 63h:43m:54s remains)
INFO - root - 2017-12-07 02:41:35.577377: step 1730, loss = 0.74, batch loss = 0.67 (11.7 examples/sec; 0.685 sec/batch; 62h:53m:42s remains)
INFO - root - 2017-12-07 02:41:42.538029: step 1740, loss = 0.80, batch loss = 0.73 (11.5 examples/sec; 0.695 sec/batch; 63h:51m:36s remains)
INFO - root - 2017-12-07 02:41:49.449693: step 1750, loss = 0.78, batch loss = 0.70 (12.0 examples/sec; 0.668 sec/batch; 61h:20m:36s remains)
INFO - root - 2017-12-07 02:41:56.393515: step 1760, loss = 0.93, batch loss = 0.85 (11.3 examples/sec; 0.705 sec/batch; 64h:48m:27s remains)
INFO - root - 2017-12-07 02:42:03.388061: step 1770, loss = 0.80, batch loss = 0.73 (11.4 examples/sec; 0.701 sec/batch; 64h:23m:07s remains)
INFO - root - 2017-12-07 02:42:10.228854: step 1780, loss = 0.86, batch loss = 0.79 (11.6 examples/sec; 0.688 sec/batch; 63h:13m:28s remains)
INFO - root - 2017-12-07 02:42:17.081680: step 1790, loss = 0.66, batch loss = 0.59 (11.9 examples/sec; 0.674 sec/batch; 61h:54m:35s remains)
INFO - root - 2017-12-07 02:42:24.067958: step 1800, loss = 0.76, batch loss = 0.69 (11.7 examples/sec; 0.683 sec/batch; 62h:42m:35s remains)
2017-12-07 02:42:24.598388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5961494 -3.0205812 -3.0244734 -2.6229825 -2.3056862 -2.6482389 -3.3047118 -3.7285266 -3.9325383 -4.1516767 -4.3539186 -4.3973641 -4.351378 -4.2630582 -4.2373481][-2.152492 -2.4604712 -2.5009203 -2.2321706 -2.0965171 -2.5489221 -3.1453552 -3.5123994 -3.7617106 -4.0502028 -4.2942314 -4.3423176 -4.2700105 -4.1396036 -4.0833788][-1.4391408 -1.6516941 -1.8119524 -1.7590656 -1.8401685 -2.2567253 -2.5897713 -2.7903552 -3.028059 -3.3246336 -3.5215173 -3.4781177 -3.3957705 -3.370775 -3.461807][-1.4313283 -1.6838791 -1.8862016 -1.8405306 -1.8885789 -2.1509933 -2.2814243 -2.3689134 -2.5194643 -2.7354827 -2.857024 -2.7154841 -2.6502767 -2.7715926 -3.0158927][-1.7766452 -2.0215518 -2.1927235 -2.0846739 -2.003634 -2.1815019 -2.3007467 -2.3224459 -2.3041701 -2.3509004 -2.3379891 -2.2091064 -2.3141398 -2.6011422 -2.9141541][-1.7888186 -1.9371781 -2.0578706 -1.8393745 -1.6075296 -1.8136601 -2.2104881 -2.4461699 -2.4513168 -2.2974265 -1.9631481 -1.757385 -2.0081556 -2.3383961 -2.6065984][-1.7932062 -1.9655421 -2.0633125 -1.6989148 -1.2151325 -1.3083854 -1.8809783 -2.4754767 -2.8914909 -2.8446875 -2.2628341 -1.8059883 -1.8473802 -1.8057904 -1.7973132][-1.8502495 -2.1726863 -2.1545091 -1.57637 -0.77948213 -0.44697618 -0.63237309 -1.2869689 -2.2551663 -2.8402987 -2.5842481 -2.1677206 -1.8348961 -1.0779934 -0.50709319][-2.0500569 -2.5314181 -2.2853639 -1.4352653 -0.37262821 0.54019928 1.1289887 0.83000755 -0.39805126 -1.6706307 -2.1855354 -2.2840261 -1.8570452 -0.57482505 0.51629448][-1.988662 -2.5045462 -2.0859065 -1.2273068 -0.30628538 0.71603489 1.724978 1.8554754 0.9434638 -0.25484991 -1.074096 -1.6398997 -1.4822774 -0.33004808 0.78004885][-1.3709259 -1.6554286 -1.2109046 -0.60617161 -0.11882496 0.51352549 1.1691885 1.2826581 0.8161006 0.24752665 -0.30098629 -1.0011187 -1.2397716 -0.70024037 0.074371338][-1.1939108 -1.1117866 -0.68739033 -0.32427979 -0.17405128 -0.068758965 -0.12809372 -0.34638071 -0.50507736 -0.39125919 -0.4377141 -0.93467116 -1.3868759 -1.379616 -0.95708418][-1.736603 -1.4684129 -1.0968499 -0.8310535 -0.74085331 -0.90308475 -1.4354377 -1.9416394 -2.0278597 -1.6246655 -1.3184733 -1.4118812 -1.6728315 -1.8153756 -1.5754616][-2.7259824 -2.4917774 -2.2153747 -1.936358 -1.707011 -1.7925944 -2.312789 -2.8339832 -3.0106583 -2.7935696 -2.5486724 -2.4206035 -2.386431 -2.4128485 -2.2314277][-3.3587275 -3.295929 -3.128686 -2.8949895 -2.6513624 -2.6458924 -2.9440002 -3.2872708 -3.4972949 -3.5323334 -3.5120387 -3.433677 -3.3366289 -3.2753196 -3.1023054]]...]
INFO - root - 2017-12-07 02:42:31.542973: step 1810, loss = 0.72, batch loss = 0.65 (11.8 examples/sec; 0.677 sec/batch; 62h:10m:10s remains)
INFO - root - 2017-12-07 02:42:38.508381: step 1820, loss = 0.69, batch loss = 0.62 (11.3 examples/sec; 0.710 sec/batch; 65h:12m:18s remains)
INFO - root - 2017-12-07 02:42:45.230979: step 1830, loss = 0.65, batch loss = 0.57 (11.1 examples/sec; 0.723 sec/batch; 66h:23m:13s remains)
INFO - root - 2017-12-07 02:42:52.127645: step 1840, loss = 0.62, batch loss = 0.55 (11.8 examples/sec; 0.677 sec/batch; 62h:11m:11s remains)
INFO - root - 2017-12-07 02:42:59.101711: step 1850, loss = 0.62, batch loss = 0.55 (11.6 examples/sec; 0.687 sec/batch; 63h:07m:35s remains)
INFO - root - 2017-12-07 02:43:05.976123: step 1860, loss = 0.75, batch loss = 0.68 (11.8 examples/sec; 0.680 sec/batch; 62h:25m:36s remains)
INFO - root - 2017-12-07 02:43:12.965599: step 1870, loss = 0.78, batch loss = 0.71 (11.4 examples/sec; 0.699 sec/batch; 64h:10m:35s remains)
INFO - root - 2017-12-07 02:43:19.961140: step 1880, loss = 0.73, batch loss = 0.65 (11.3 examples/sec; 0.710 sec/batch; 65h:13m:06s remains)
INFO - root - 2017-12-07 02:43:26.907280: step 1890, loss = 0.78, batch loss = 0.71 (11.3 examples/sec; 0.706 sec/batch; 64h:52m:08s remains)
INFO - root - 2017-12-07 02:43:33.899645: step 1900, loss = 0.71, batch loss = 0.64 (11.3 examples/sec; 0.708 sec/batch; 64h:59m:37s remains)
2017-12-07 02:43:34.447959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5526175 -1.1032548 -0.47860408 -0.86407542 -1.4428577 -2.1068006 -2.9047794 -3.2967515 -3.2262387 -2.8311205 -2.1430066 -1.664753 -1.865617 -2.3932948 -2.3504593][-1.7895033 -0.043802261 0.63330269 0.15821028 -0.44591475 -1.232002 -2.1666863 -2.5962296 -2.5195379 -2.1940134 -1.5784056 -1.1334193 -1.3667483 -1.8640413 -1.6369858][-0.75777125 1.0242591 1.5882235 0.98462677 0.33451366 -0.51091361 -1.4862733 -1.8451121 -1.6886404 -1.446548 -1.0261583 -0.77713847 -1.1035345 -1.531678 -1.1270854][0.45105553 2.1072783 2.4656839 1.7019687 1.0154395 0.2170105 -0.67599893 -0.97869349 -0.8985064 -0.85933733 -0.61067438 -0.47781038 -0.80141973 -1.140501 -0.6286087][1.0504231 2.3694787 2.45724 1.6746449 1.1389894 0.63293648 0.063055038 -0.19795513 -0.40020704 -0.71468186 -0.64970231 -0.45476437 -0.58248353 -0.73401284 -0.15047026][0.76035261 1.65414 1.5288482 0.92006588 0.63975763 0.47337866 0.2019639 -0.047585011 -0.44965887 -0.90406895 -0.83354712 -0.47934747 -0.44246244 -0.52150154 0.018931866][0.086514473 0.53190184 0.31216335 -0.025558949 -0.065916061 -0.028865814 -0.15177822 -0.4354198 -0.92970562 -1.4159417 -1.3391247 -0.87150192 -0.65641189 -0.66359591 -0.20685673][-0.12647486 -0.056149483 -0.39042377 -0.64647913 -0.62116981 -0.47535419 -0.40696621 -0.55220628 -0.99143052 -1.5635583 -1.7270544 -1.4137654 -1.1410983 -1.0769768 -0.65636539][-0.31364536 -0.39208698 -0.6970582 -0.85289717 -0.799736 -0.63613653 -0.46310878 -0.4817214 -0.77062964 -1.2778912 -1.6207137 -1.557862 -1.3664865 -1.3236809 -0.92998552][-0.72641182 -0.83015108 -0.99236035 -0.98840308 -0.86719847 -0.684175 -0.53291631 -0.53786588 -0.71674037 -1.1143093 -1.509387 -1.5879564 -1.4387596 -1.3391318 -0.94325137][-1.291604 -1.3517356 -1.3753123 -1.2483184 -1.0712183 -0.88431311 -0.80709457 -0.80470729 -0.79859114 -0.95304465 -1.2558973 -1.4327846 -1.385927 -1.2723019 -0.90786195][-1.8227584 -1.8080399 -1.7548425 -1.6393116 -1.5298805 -1.4284363 -1.4643395 -1.5037313 -1.3843813 -1.3130028 -1.4218702 -1.5536268 -1.5633011 -1.4866989 -1.198226][-2.241123 -2.1407156 -2.0571935 -2.0167196 -2.0067821 -2.0056975 -2.1326518 -2.2507904 -2.1866848 -2.1109192 -2.1697309 -2.27263 -2.2749403 -2.1254792 -1.7611296][-2.4429255 -2.3431051 -2.2914267 -2.3233428 -2.3694377 -2.3994606 -2.504024 -2.5938134 -2.5527825 -2.5265756 -2.6117258 -2.7460067 -2.7829604 -2.592391 -2.1933205][-2.739193 -2.6572547 -2.6055553 -2.6257823 -2.6490746 -2.6469154 -2.6922505 -2.7247133 -2.6611443 -2.5903316 -2.5983696 -2.6757126 -2.7099714 -2.5867362 -2.3479135]]...]
INFO - root - 2017-12-07 02:43:41.383586: step 1910, loss = 0.55, batch loss = 0.48 (11.5 examples/sec; 0.697 sec/batch; 63h:58m:02s remains)
INFO - root - 2017-12-07 02:43:48.341394: step 1920, loss = 0.83, batch loss = 0.75 (11.2 examples/sec; 0.717 sec/batch; 65h:49m:20s remains)
INFO - root - 2017-12-07 02:43:55.325751: step 1930, loss = 0.90, batch loss = 0.82 (11.2 examples/sec; 0.712 sec/batch; 65h:23m:21s remains)
INFO - root - 2017-12-07 02:44:02.079157: step 1940, loss = 0.60, batch loss = 0.52 (15.9 examples/sec; 0.504 sec/batch; 46h:16m:26s remains)
INFO - root - 2017-12-07 02:44:09.074902: step 1950, loss = 0.88, batch loss = 0.81 (11.4 examples/sec; 0.699 sec/batch; 64h:11m:13s remains)
INFO - root - 2017-12-07 02:44:16.021220: step 1960, loss = 0.91, batch loss = 0.84 (11.4 examples/sec; 0.702 sec/batch; 64h:29m:25s remains)
INFO - root - 2017-12-07 02:44:23.052167: step 1970, loss = 0.84, batch loss = 0.77 (10.9 examples/sec; 0.731 sec/batch; 67h:06m:29s remains)
INFO - root - 2017-12-07 02:44:30.005493: step 1980, loss = 0.76, batch loss = 0.69 (11.5 examples/sec; 0.694 sec/batch; 63h:41m:39s remains)
INFO - root - 2017-12-07 02:44:36.906332: step 1990, loss = 0.89, batch loss = 0.82 (11.6 examples/sec; 0.690 sec/batch; 63h:23m:24s remains)
INFO - root - 2017-12-07 02:44:43.872579: step 2000, loss = 0.69, batch loss = 0.62 (11.3 examples/sec; 0.705 sec/batch; 64h:42m:55s remains)
2017-12-07 02:44:44.421528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4161353 -3.4898217 -3.5567331 -3.5804906 -3.5888352 -3.6513858 -3.780688 -3.926023 -4.0469284 -4.0741982 -4.0186324 -3.9008489 -3.7344484 -3.5862787 -3.4179888][-3.5827127 -3.6960211 -3.7750719 -3.7722597 -3.7707777 -3.8560307 -4.0231075 -4.1706018 -4.2412467 -4.1571789 -3.9926755 -3.8738048 -3.7435226 -3.6461625 -3.4377322][-3.7186236 -3.8349566 -3.8689654 -3.812695 -3.7956016 -3.8890028 -4.0484786 -4.109726 -4.0339332 -3.7402937 -3.432559 -3.3845515 -3.4324181 -3.5456445 -3.4134951][-3.8434834 -3.866231 -3.7732959 -3.64713 -3.6321483 -3.743422 -3.8555636 -3.7562339 -3.4348509 -2.8174143 -2.2912087 -2.3055656 -2.6355333 -3.104337 -3.2120202][-4.0616765 -3.9881103 -3.8452685 -3.7687817 -3.7577724 -3.7819855 -3.6944914 -3.2991076 -2.6255827 -1.6836271 -1.0114062 -1.167522 -1.8184295 -2.5807724 -2.9047058][-4.054287 -3.9010143 -3.7757113 -3.7735245 -3.7467172 -3.699198 -3.4530718 -2.7717381 -1.8640506 -0.89079165 -0.42349005 -0.86563587 -1.7289948 -2.4939506 -2.7682271][-3.5844104 -3.1531553 -2.7624569 -2.5964034 -2.6138191 -2.8263638 -2.8215413 -2.225445 -1.4721127 -0.82415056 -0.75270128 -1.3913078 -2.2139459 -2.7628369 -2.820085][-2.8894372 -1.8850949 -0.81156182 -0.24591732 -0.50006104 -1.4150667 -2.0716219 -1.9432948 -1.6146789 -1.3151722 -1.460988 -2.0627267 -2.665803 -2.9794555 -2.8900902][-2.3812652 -0.82941866 0.82568645 1.5906568 0.86955023 -0.75652838 -1.891767 -2.10136 -2.0725365 -1.9610155 -2.1063333 -2.5196819 -2.8601444 -3.0003872 -2.905695][-2.5589683 -1.0709128 0.39800739 0.91691971 0.016793728 -1.57722 -2.4982498 -2.543982 -2.4892306 -2.4100394 -2.5172348 -2.7790103 -2.9391944 -2.9810781 -2.9401722][-3.303905 -2.3949854 -1.4985266 -1.2098241 -1.7879653 -2.7604027 -3.1707082 -2.9493985 -2.7896395 -2.6921306 -2.7629008 -2.9395342 -3.0175185 -3.0233784 -3.0145247][-3.8837085 -3.5287609 -3.0831928 -2.8588657 -3.0189338 -3.3910823 -3.4544144 -3.1801896 -3.0266008 -2.9473786 -3.0070872 -3.142328 -3.1826682 -3.1559117 -3.1171455][-4.0104051 -3.907922 -3.6666176 -3.4792256 -3.4411752 -3.5069916 -3.4524183 -3.2838182 -3.2323241 -3.206182 -3.2528021 -3.3312516 -3.3338242 -3.2762685 -3.2030234][-3.8686123 -3.8231237 -3.6657639 -3.5313914 -3.4698863 -3.4557114 -3.4040184 -3.3510048 -3.3667297 -3.3593707 -3.3620439 -3.3763404 -3.3546348 -3.2985156 -3.2312064][-3.7467511 -3.7576277 -3.6854072 -3.5956256 -3.5224044 -3.4597433 -3.3923321 -3.3551736 -3.3562472 -3.3425777 -3.3262279 -3.3168647 -3.2969787 -3.2599726 -3.2145922]]...]
INFO - root - 2017-12-07 02:44:51.461045: step 2010, loss = 0.79, batch loss = 0.72 (11.2 examples/sec; 0.713 sec/batch; 65h:29m:16s remains)
INFO - root - 2017-12-07 02:44:58.361805: step 2020, loss = 0.87, batch loss = 0.80 (11.4 examples/sec; 0.702 sec/batch; 64h:28m:37s remains)
INFO - root - 2017-12-07 02:45:05.339808: step 2030, loss = 0.58, batch loss = 0.51 (11.7 examples/sec; 0.686 sec/batch; 62h:57m:18s remains)
INFO - root - 2017-12-07 02:45:12.302555: step 2040, loss = 0.79, batch loss = 0.72 (11.4 examples/sec; 0.703 sec/batch; 64h:29m:55s remains)
INFO - root - 2017-12-07 02:45:19.245944: step 2050, loss = 0.86, batch loss = 0.78 (11.6 examples/sec; 0.688 sec/batch; 63h:09m:01s remains)
INFO - root - 2017-12-07 02:45:25.902168: step 2060, loss = 0.71, batch loss = 0.64 (11.6 examples/sec; 0.693 sec/batch; 63h:34m:14s remains)
INFO - root - 2017-12-07 02:45:32.942486: step 2070, loss = 0.70, batch loss = 0.63 (11.3 examples/sec; 0.706 sec/batch; 64h:47m:04s remains)
INFO - root - 2017-12-07 02:45:39.962528: step 2080, loss = 0.74, batch loss = 0.67 (11.4 examples/sec; 0.701 sec/batch; 64h:18m:49s remains)
INFO - root - 2017-12-07 02:45:46.904517: step 2090, loss = 0.61, batch loss = 0.54 (11.5 examples/sec; 0.698 sec/batch; 64h:01m:03s remains)
INFO - root - 2017-12-07 02:45:53.827879: step 2100, loss = 0.82, batch loss = 0.74 (11.8 examples/sec; 0.676 sec/batch; 62h:00m:01s remains)
2017-12-07 02:45:54.380089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7505927 -2.5848184 -2.2256165 -1.6544595 -1.1236026 -1.1034417 -1.3887568 -1.636167 -2.01423 -2.6720719 -3.3034036 -3.5138438 -3.3736272 -3.1127205 -2.8449366][-2.8244526 -2.6063151 -2.151824 -1.4593129 -0.71217442 -0.58076239 -0.959712 -1.2099881 -1.5067742 -2.2236261 -3.1442099 -3.7061245 -3.7338428 -3.4531486 -3.0850887][-2.8613472 -2.5853999 -2.0491996 -1.2422786 -0.23001194 0.13196945 -0.28439093 -0.60997486 -0.8433733 -1.5207942 -2.6270685 -3.5982583 -3.92881 -3.7422593 -3.31533][-2.8773212 -2.5329313 -1.9482002 -1.069087 0.20184374 0.89633656 0.52616549 0.012357712 -0.26083803 -0.84041476 -1.9954431 -3.2725241 -3.9255245 -3.9240108 -3.521498][-2.9566829 -2.5462956 -1.9268255 -1.0084693 0.42437506 1.4561467 1.2038274 0.43816519 0.036377907 -0.39182615 -1.4726243 -2.8564992 -3.7259274 -3.927691 -3.6395369][-3.1048543 -2.6581445 -2.0419455 -1.1482787 0.29567528 1.6356816 1.65938 0.739182 0.13171768 -0.15562582 -1.1036832 -2.4655871 -3.442157 -3.8027875 -3.6634493][-3.2908084 -2.8895094 -2.3459806 -1.556994 -0.20350122 1.3665233 1.8603296 1.0033603 0.18824196 -0.10687447 -0.96256185 -2.2715116 -3.2769675 -3.7215397 -3.6629658][-3.4574356 -3.1779616 -2.7579839 -2.0876803 -0.9040134 0.69168663 1.6890707 1.1350455 0.24876308 -0.14827538 -0.9711566 -2.2442939 -3.2492239 -3.732867 -3.6843626][-3.4953923 -3.3796194 -3.1164489 -2.5764308 -1.6038127 -0.13396549 1.1662588 1.0645361 0.33050394 -0.13663816 -0.97865033 -2.2097287 -3.2071748 -3.7254298 -3.6824782][-3.4734764 -3.5211477 -3.4197593 -3.0159249 -2.2681253 -1.0246491 0.3429389 0.69434166 0.295959 -0.12132263 -0.96824336 -2.1440387 -3.1449449 -3.7072558 -3.6964681][-3.4092836 -3.5639915 -3.6075916 -3.3589106 -2.8263266 -1.8602183 -0.6187408 0.036695004 0.028258324 -0.24232531 -1.0286744 -2.1312072 -3.147778 -3.7361879 -3.7404439][-3.2768579 -3.46466 -3.6300218 -3.5590901 -3.2395868 -2.5601106 -1.5477419 -0.79389024 -0.48848009 -0.55272174 -1.1902435 -2.2052372 -3.2064724 -3.7937198 -3.7897556][-3.0992336 -3.2560129 -3.491977 -3.5749481 -3.4494369 -3.0536273 -2.3413651 -1.6556044 -1.1621637 -1.0151544 -1.459409 -2.3381903 -3.2099705 -3.7122655 -3.6822119][-2.6866264 -2.7865975 -3.0307243 -3.2040625 -3.2021894 -3.0160801 -2.6146073 -2.1322284 -1.6248002 -1.3493454 -1.606606 -2.2683108 -2.884892 -3.2240648 -3.1762602][-2.2256224 -2.2858047 -2.4953957 -2.6955719 -2.7529972 -2.6783576 -2.5036001 -2.2430511 -1.8607273 -1.5514524 -1.6471035 -2.0467396 -2.3956406 -2.5707793 -2.5189605]]...]
INFO - root - 2017-12-07 02:46:01.277194: step 2110, loss = 0.78, batch loss = 0.71 (11.7 examples/sec; 0.682 sec/batch; 62h:38m:03s remains)
INFO - root - 2017-12-07 02:46:08.157356: step 2120, loss = 0.81, batch loss = 0.74 (11.7 examples/sec; 0.682 sec/batch; 62h:35m:27s remains)
INFO - root - 2017-12-07 02:46:15.095075: step 2130, loss = 0.54, batch loss = 0.46 (11.6 examples/sec; 0.691 sec/batch; 63h:24m:40s remains)
INFO - root - 2017-12-07 02:46:22.087362: step 2140, loss = 0.77, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 61h:52m:02s remains)
INFO - root - 2017-12-07 02:46:29.053683: step 2150, loss = 0.66, batch loss = 0.58 (11.7 examples/sec; 0.682 sec/batch; 62h:36m:58s remains)
INFO - root - 2017-12-07 02:46:35.978045: step 2160, loss = 0.81, batch loss = 0.73 (11.5 examples/sec; 0.695 sec/batch; 63h:45m:50s remains)
INFO - root - 2017-12-07 02:46:42.675423: step 2170, loss = 0.78, batch loss = 0.71 (11.4 examples/sec; 0.699 sec/batch; 64h:08m:31s remains)
INFO - root - 2017-12-07 02:46:49.569722: step 2180, loss = 0.78, batch loss = 0.71 (11.9 examples/sec; 0.671 sec/batch; 61h:36m:31s remains)
INFO - root - 2017-12-07 02:46:56.527328: step 2190, loss = 0.81, batch loss = 0.73 (11.3 examples/sec; 0.707 sec/batch; 64h:52m:16s remains)
INFO - root - 2017-12-07 02:47:03.486975: step 2200, loss = 0.58, batch loss = 0.51 (11.6 examples/sec; 0.691 sec/batch; 63h:24m:26s remains)
2017-12-07 02:47:04.039741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3975523 -2.8352365 -3.4712057 -3.7133117 -3.6319134 -3.3980658 -2.3319471 -0.62390256 0.53833866 0.32770491 -0.74494195 -1.4354699 -1.8010614 -2.7326407 -3.9108315][-2.7089067 -3.0869789 -3.6282732 -3.7844038 -3.5165834 -3.1271591 -1.9789984 -0.10843182 1.1559319 0.837543 -0.57153511 -1.8016379 -2.5578356 -3.4181771 -4.1846976][-2.9577756 -3.3064854 -3.6514394 -3.5442922 -2.9333735 -2.2959774 -1.2019312 0.61521673 1.8567529 1.4135985 -0.24218464 -1.942219 -3.0391669 -3.8534324 -4.2635517][-3.0968828 -3.4567118 -3.5710437 -3.1469188 -2.2268481 -1.4172659 -0.48686481 1.1200047 2.1837296 1.5155525 -0.2896719 -2.2104545 -3.4566169 -4.0899625 -4.0602093][-3.0987945 -3.4244778 -3.3529761 -2.6929965 -1.6190143 -0.76205039 0.064079285 1.5398917 2.4560027 1.5402017 -0.36424494 -2.351721 -3.6666546 -4.0789046 -3.647903][-3.0535178 -3.3244963 -3.1145544 -2.2780633 -1.0914142 -0.1208272 0.75570536 2.1831727 2.993227 1.9830298 0.073410988 -2.0146894 -3.5660088 -3.9840298 -3.4108934][-3.1843033 -3.4904752 -3.2536979 -2.2570956 -0.84827161 0.38730335 1.4057832 2.7259011 3.4591923 2.5916629 0.82264614 -1.4046249 -3.3223281 -3.9706314 -3.4473286][-3.5677052 -3.9888263 -3.8121283 -2.6978316 -1.0549865 0.41511822 1.5221577 2.6490507 3.3554831 2.842278 1.3825517 -0.82661748 -2.9972181 -3.9674039 -3.6212142][-3.9635124 -4.4561186 -4.3828311 -3.3302641 -1.7123053 -0.26541471 0.79223061 1.8011456 2.6521387 2.594243 1.5193181 -0.44616628 -2.5752263 -3.7680676 -3.6462023][-4.13413 -4.5808997 -4.599637 -3.7862267 -2.4621439 -1.3256748 -0.46945858 0.5484395 1.6607084 2.0253801 1.3313332 -0.3061614 -2.2534113 -3.5381773 -3.602597][-4.0943856 -4.4147696 -4.48886 -3.9360902 -2.9582968 -2.1605494 -1.5484326 -0.62685132 0.48160458 1.0574236 0.75656939 -0.41894627 -2.0456576 -3.2817354 -3.4671464][-3.9498386 -4.1806517 -4.2990217 -3.9447722 -3.2416823 -2.6939695 -2.2467411 -1.5061915 -0.65597463 -0.15932703 -0.21274805 -0.94379163 -2.1725442 -3.1778934 -3.3253303][-3.8514884 -4.033042 -4.1616478 -3.9566438 -3.4965599 -3.132308 -2.8106894 -2.2645032 -1.6921599 -1.3483465 -1.3108137 -1.7527411 -2.6439006 -3.3792696 -3.4519286][-3.8506148 -3.9692204 -4.0598445 -3.9624302 -3.7150016 -3.469296 -3.2238107 -2.8193502 -2.4145143 -2.1934805 -2.1935647 -2.5249758 -3.1485224 -3.6248751 -3.64886][-3.7211707 -3.7696939 -3.7920907 -3.7338898 -3.6320329 -3.484679 -3.2974505 -2.9960086 -2.6707361 -2.497252 -2.5213099 -2.7856178 -3.2248132 -3.5262041 -3.5430608]]...]
INFO - root - 2017-12-07 02:47:11.039686: step 2210, loss = 0.93, batch loss = 0.86 (11.8 examples/sec; 0.680 sec/batch; 62h:21m:36s remains)
INFO - root - 2017-12-07 02:47:17.948270: step 2220, loss = 0.79, batch loss = 0.72 (11.7 examples/sec; 0.686 sec/batch; 62h:57m:16s remains)
INFO - root - 2017-12-07 02:47:24.906065: step 2230, loss = 0.92, batch loss = 0.85 (11.5 examples/sec; 0.694 sec/batch; 63h:38m:51s remains)
INFO - root - 2017-12-07 02:47:33.311044: step 2240, loss = 0.73, batch loss = 0.66 (7.5 examples/sec; 1.072 sec/batch; 98h:21m:28s remains)
INFO - root - 2017-12-07 02:47:47.236182: step 2250, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.087 sec/batch; 99h:45m:09s remains)
INFO - root - 2017-12-07 02:47:58.597332: step 2260, loss = 0.61, batch loss = 0.54 (6.6 examples/sec; 1.208 sec/batch; 110h:46m:13s remains)
INFO - root - 2017-12-07 02:48:09.945120: step 2270, loss = 0.82, batch loss = 0.75 (6.8 examples/sec; 1.179 sec/batch; 108h:07m:48s remains)
INFO - root - 2017-12-07 02:48:21.233785: step 2280, loss = 0.77, batch loss = 0.70 (7.4 examples/sec; 1.082 sec/batch; 99h:17m:03s remains)
INFO - root - 2017-12-07 02:48:32.550950: step 2290, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 1.107 sec/batch; 101h:33m:21s remains)
INFO - root - 2017-12-07 02:48:43.927006: step 2300, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 1.107 sec/batch; 101h:31m:25s remains)
2017-12-07 02:48:44.773146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2374098 -2.361603 -2.4672256 -2.5067747 -2.5456362 -2.6408639 -2.769264 -2.8509738 -2.8142283 -2.7684188 -2.7675395 -2.7539372 -2.6989434 -2.5622678 -2.3948479][-2.2384307 -2.4297786 -2.5710623 -2.5596228 -2.4864075 -2.4814272 -2.5613117 -2.5997939 -2.5087845 -2.4383035 -2.4607952 -2.5351491 -2.5854635 -2.4718537 -2.3425088][-2.3523927 -2.5763984 -2.7026272 -2.573997 -2.3335326 -2.1458807 -2.0886629 -2.0435779 -1.9091053 -1.8546121 -1.9567258 -2.2005177 -2.4054034 -2.3451657 -2.3157432][-2.5062795 -2.6793907 -2.6694651 -2.2954555 -1.8235667 -1.3864563 -1.1157625 -0.9463582 -0.79704571 -0.83205676 -1.1081946 -1.5631433 -1.9561074 -2.0610809 -2.2537663][-2.5979939 -2.6747193 -2.4612131 -1.7788661 -1.069612 -0.42004251 0.088242054 0.44360828 0.6128583 0.45181417 -0.016225815 -0.62831116 -1.1929479 -1.5938494 -2.087832][-2.5932937 -2.5941901 -2.2256258 -1.2955818 -0.42927027 0.2838583 0.89787769 1.3989921 1.6017332 1.3739638 0.85104656 0.20505714 -0.48987126 -1.2164731 -1.9466965][-2.4690661 -2.48033 -2.1195259 -1.1310294 -0.25476074 0.35268545 0.88277531 1.3616982 1.5985417 1.4848814 1.1086297 0.5476017 -0.2130723 -1.1258209 -1.9040163][-2.258287 -2.3688042 -2.1797445 -1.3250289 -0.58423066 -0.13184214 0.30295563 0.73013735 0.97256184 0.971365 0.77977991 0.34812593 -0.37523556 -1.2453926 -1.8601887][-1.9906979 -2.1635466 -2.1050627 -1.4016063 -0.80072379 -0.46389627 -0.10447264 0.23412943 0.36321354 0.2953763 0.16492558 -0.15250015 -0.76168633 -1.4334815 -1.7693605][-1.6109517 -1.7389376 -1.6914454 -1.0725894 -0.59512019 -0.35007143 -0.10827303 0.039699078 -0.078745365 -0.34586 -0.54690576 -0.80357432 -1.2425468 -1.6179595 -1.6313462][-1.2653198 -1.2812879 -1.1626766 -0.6214149 -0.30581236 -0.1880703 -0.036762714 -0.055959225 -0.37701988 -0.81134748 -1.0880139 -1.3087375 -1.5538352 -1.6430032 -1.40168][-1.1555903 -1.094347 -0.8971777 -0.45776415 -0.32491016 -0.35300303 -0.26125526 -0.32531261 -0.62158179 -0.97010803 -1.2169831 -1.39851 -1.5125227 -1.4205844 -1.1324053][-1.1390431 -1.0870368 -0.90538526 -0.64663815 -0.66073585 -0.7612443 -0.69124246 -0.67716312 -0.75050688 -0.85265708 -0.9871738 -1.1011593 -1.1528034 -1.0380685 -0.88562322][-1.1046071 -1.0639274 -0.9452467 -0.86508179 -0.94300413 -1.0267856 -0.9488616 -0.80566573 -0.62108946 -0.5124743 -0.55660868 -0.63772082 -0.69996405 -0.66109252 -0.72722769][-0.98816204 -0.93387461 -0.857703 -0.87757349 -0.95875263 -1.0392292 -1.0010715 -0.76004744 -0.39996147 -0.23465157 -0.28339481 -0.38674259 -0.48988295 -0.51961207 -0.72932386]]...]
INFO - root - 2017-12-07 02:48:56.200922: step 2310, loss = 0.85, batch loss = 0.78 (6.6 examples/sec; 1.215 sec/batch; 111h:23m:55s remains)
INFO - root - 2017-12-07 02:49:07.428994: step 2320, loss = 0.81, batch loss = 0.74 (7.3 examples/sec; 1.100 sec/batch; 100h:55m:16s remains)
INFO - root - 2017-12-07 02:49:21.623225: step 2330, loss = 1.01, batch loss = 0.94 (7.4 examples/sec; 1.082 sec/batch; 99h:16m:21s remains)
INFO - root - 2017-12-07 02:49:32.900128: step 2340, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.164 sec/batch; 106h:45m:35s remains)
INFO - root - 2017-12-07 02:49:44.318199: step 2350, loss = 0.80, batch loss = 0.72 (7.1 examples/sec; 1.124 sec/batch; 103h:05m:06s remains)
INFO - root - 2017-12-07 02:49:55.722311: step 2360, loss = 0.64, batch loss = 0.56 (6.8 examples/sec; 1.180 sec/batch; 108h:13m:47s remains)
INFO - root - 2017-12-07 02:50:07.088127: step 2370, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.164 sec/batch; 106h:45m:40s remains)
INFO - root - 2017-12-07 02:50:18.512069: step 2380, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.153 sec/batch; 105h:41m:54s remains)
INFO - root - 2017-12-07 02:50:29.876836: step 2390, loss = 0.60, batch loss = 0.53 (7.2 examples/sec; 1.107 sec/batch; 101h:31m:08s remains)
INFO - root - 2017-12-07 02:50:41.164926: step 2400, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.083 sec/batch; 99h:16m:13s remains)
2017-12-07 02:50:42.011544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6558862 -2.8365812 -2.8052988 -2.4498787 -1.9820294 -1.6653659 -1.5734725 -1.7421155 -2.0254331 -2.4116352 -2.9511054 -3.4892352 -3.5992713 -3.340271 -3.1276999][-2.5885823 -2.928153 -2.9896848 -2.6339312 -2.1869929 -1.9599085 -2.0034313 -2.1503742 -2.2430608 -2.4212716 -2.770066 -3.2585921 -3.4397595 -3.2552056 -3.0580897][-2.4034011 -2.8289118 -3.0208449 -2.7978659 -2.5391994 -2.4906201 -2.6067119 -2.6031168 -2.4621868 -2.4176111 -2.5544181 -2.9598165 -3.2431731 -3.1799932 -3.005599][-2.5262737 -2.8293242 -3.0423155 -2.9320807 -2.8205657 -2.8381958 -2.8783934 -2.7454426 -2.5492587 -2.4545345 -2.4702168 -2.7956362 -3.1478314 -3.1466403 -2.9589505][-2.5855851 -2.7294698 -2.9116414 -2.8638554 -2.8101745 -2.8034935 -2.7774987 -2.6777048 -2.6134377 -2.5711522 -2.5297804 -2.8011198 -3.1602197 -3.1505477 -2.9326267][-2.7964566 -2.8964281 -3.0054059 -2.9078751 -2.7617693 -2.5644255 -2.3871825 -2.3509011 -2.441668 -2.4891534 -2.5423698 -2.9142227 -3.3133268 -3.2969954 -3.0716572][-2.9464517 -3.0177448 -2.9876285 -2.7401462 -2.3927982 -1.8632586 -1.4343951 -1.4011242 -1.6293392 -1.8602884 -2.2201478 -2.8850503 -3.4364238 -3.4741426 -3.2783756][-2.6987619 -2.7867537 -2.6673331 -2.3656549 -1.9219692 -1.1847024 -0.57139421 -0.48339128 -0.7682209 -1.1951249 -1.8891819 -2.8182971 -3.4799132 -3.572278 -3.4230113][-2.3579249 -2.4510064 -2.3113046 -2.1231318 -1.8369186 -1.2144742 -0.6475246 -0.49422431 -0.66278434 -1.0634055 -1.8007443 -2.7418392 -3.4210649 -3.5675635 -3.4811764][-2.0904527 -2.1671574 -2.0806837 -2.0297279 -1.9686713 -1.6234505 -1.2763133 -1.1567686 -1.2179329 -1.5148537 -2.1228735 -2.9017153 -3.4738386 -3.5762119 -3.4774511][-2.1692848 -2.2502282 -2.2865291 -2.357877 -2.4951105 -2.4728782 -2.3973618 -2.3651798 -2.372056 -2.550864 -2.9192777 -3.398591 -3.7309384 -3.6722562 -3.4728615][-2.4578943 -2.5169413 -2.60738 -2.7151113 -2.9833567 -3.2212558 -3.3743458 -3.4823055 -3.5423479 -3.6377485 -3.7594032 -3.9485488 -4.063746 -3.863297 -3.5868227][-2.8779802 -2.9353118 -3.0211439 -3.0902205 -3.3224559 -3.5789456 -3.7584941 -3.9138403 -4.0347033 -4.1241064 -4.1337008 -4.1740718 -4.1894975 -3.9804075 -3.7460926][-3.4153395 -3.4732904 -3.5266056 -3.5455253 -3.6758268 -3.8346863 -3.9464271 -4.0727568 -4.1952019 -4.2815213 -4.2792292 -4.2896643 -4.2836847 -4.1341577 -3.963675][-3.8634224 -3.8849046 -3.892611 -3.8786714 -3.9292829 -3.9957178 -4.0555358 -4.1424289 -4.2423744 -4.3336873 -4.3669982 -4.378675 -4.3554077 -4.2413969 -4.0984416]]...]
INFO - root - 2017-12-07 02:50:53.344820: step 2410, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 1.150 sec/batch; 105h:24m:14s remains)
INFO - root - 2017-12-07 02:51:04.731765: step 2420, loss = 0.85, batch loss = 0.78 (7.4 examples/sec; 1.083 sec/batch; 99h:18m:04s remains)
INFO - root - 2017-12-07 02:51:16.118413: step 2430, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 1.140 sec/batch; 104h:32m:56s remains)
INFO - root - 2017-12-07 02:51:27.483713: step 2440, loss = 0.61, batch loss = 0.54 (7.3 examples/sec; 1.092 sec/batch; 100h:08m:00s remains)
INFO - root - 2017-12-07 02:51:38.729679: step 2450, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 1.149 sec/batch; 105h:21m:07s remains)
INFO - root - 2017-12-07 02:51:50.165622: step 2460, loss = 0.63, batch loss = 0.56 (6.6 examples/sec; 1.215 sec/batch; 111h:23m:29s remains)
INFO - root - 2017-12-07 02:52:01.415786: step 2470, loss = 0.65, batch loss = 0.58 (7.3 examples/sec; 1.096 sec/batch; 100h:26m:34s remains)
INFO - root - 2017-12-07 02:52:12.876817: step 2480, loss = 0.70, batch loss = 0.63 (7.2 examples/sec; 1.113 sec/batch; 102h:03m:20s remains)
INFO - root - 2017-12-07 02:52:24.256348: step 2490, loss = 0.78, batch loss = 0.71 (7.5 examples/sec; 1.067 sec/batch; 97h:46m:03s remains)
INFO - root - 2017-12-07 02:52:35.692619: step 2500, loss = 0.63, batch loss = 0.56 (7.0 examples/sec; 1.150 sec/batch; 105h:24m:49s remains)
2017-12-07 02:52:36.509952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4380269 -3.2150409 -3.2850637 -3.6458011 -3.9241245 -3.87462 -3.7042174 -3.68228 -3.7649393 -3.7601531 -3.7052662 -3.7011626 -3.664362 -3.5109148 -3.3396168][-3.2200804 -2.6409025 -2.571245 -3.1442671 -3.7454939 -3.7895117 -3.5594068 -3.4940112 -3.634454 -3.7401481 -3.7133152 -3.6352544 -3.505959 -3.2396479 -2.9646749][-2.9608731 -1.8234203 -1.4425335 -2.1401019 -3.0125895 -3.1495142 -2.9211178 -2.8916535 -3.2141757 -3.5680029 -3.6004753 -3.3820934 -3.1083322 -2.776341 -2.4229777][-2.4910603 -0.92384362 -0.30720615 -1.0993307 -2.1446877 -2.3661222 -2.1317458 -2.0889204 -2.5415254 -3.0851715 -3.1167836 -2.71689 -2.2941098 -1.9108906 -1.5084751][-2.4055548 -0.72429228 0.0047917366 -0.69900656 -1.6366551 -1.7628226 -1.3540447 -1.0852804 -1.4774523 -2.1173029 -2.1971824 -1.7812526 -1.36131 -0.9969089 -0.63039923][-3.003118 -1.4814823 -0.71910191 -1.0663917 -1.5732658 -1.3171909 -0.47201657 0.22062302 -0.03980732 -0.88148642 -1.2550521 -1.0782113 -0.81191516 -0.55746531 -0.37038994][-3.5289509 -2.2418866 -1.5281081 -1.5213873 -1.5853674 -0.95646882 0.24327612 1.2203054 0.9069252 -0.2771945 -0.98565459 -1.0350323 -0.86568618 -0.69260812 -0.68527842][-3.5171559 -2.3342147 -1.6494782 -1.3915701 -1.2314298 -0.59062219 0.48264647 1.318882 0.75854063 -0.5662868 -1.2630832 -1.2412233 -1.0146966 -0.88979864 -1.0122259][-3.1294503 -1.9564705 -1.2901731 -0.89694858 -0.703233 -0.331573 0.27946091 0.63975525 -0.0034484863 -0.96224236 -1.2922962 -1.0784423 -0.81203032 -0.74304795 -0.913677][-2.5428557 -1.3961251 -0.84641361 -0.51929855 -0.48179412 -0.4493072 -0.18619967 -0.060925961 -0.52674389 -1.0078571 -1.0147419 -0.79489946 -0.63322043 -0.62871981 -0.75510311][-2.1355076 -1.1431985 -0.81168222 -0.72356749 -0.87977958 -0.96536064 -0.67768812 -0.44726872 -0.61695862 -0.80534387 -0.78226733 -0.78627753 -0.8340652 -0.89995289 -0.92773604][-2.4214246 -1.7673178 -1.6687365 -1.698869 -1.8571391 -1.8590531 -1.3923118 -1.0002425 -0.99237347 -1.1169713 -1.1890998 -1.3437614 -1.5015349 -1.5779216 -1.4895012][-3.0652637 -2.7377 -2.7236598 -2.6722465 -2.6705141 -2.5764747 -2.0743196 -1.6495202 -1.5700514 -1.6972933 -1.8116148 -1.9647458 -2.1180618 -2.1293597 -1.9493601][-3.5971377 -3.460448 -3.427063 -3.2771769 -3.1567068 -3.0490642 -2.6581326 -2.328182 -2.2526751 -2.3539991 -2.43471 -2.5282941 -2.645328 -2.5815105 -2.3784876][-3.9406395 -3.9192958 -3.8661971 -3.7000213 -3.5519795 -3.4690595 -3.2437277 -3.0760825 -3.0628867 -3.0997858 -3.0864496 -3.0940032 -3.1558344 -3.0862322 -2.9625463]]...]
INFO - root - 2017-12-07 02:52:47.708629: step 2510, loss = 0.65, batch loss = 0.58 (7.3 examples/sec; 1.093 sec/batch; 100h:09m:01s remains)
INFO - root - 2017-12-07 02:52:59.087547: step 2520, loss = 0.74, batch loss = 0.66 (7.3 examples/sec; 1.095 sec/batch; 100h:21m:03s remains)
INFO - root - 2017-12-07 02:53:10.387462: step 2530, loss = 0.83, batch loss = 0.75 (7.2 examples/sec; 1.108 sec/batch; 101h:31m:01s remains)
INFO - root - 2017-12-07 02:53:21.932848: step 2540, loss = 0.91, batch loss = 0.84 (6.9 examples/sec; 1.159 sec/batch; 106h:13m:19s remains)
INFO - root - 2017-12-07 02:53:33.327824: step 2550, loss = 0.88, batch loss = 0.81 (6.7 examples/sec; 1.186 sec/batch; 108h:39m:58s remains)
INFO - root - 2017-12-07 02:53:44.747759: step 2560, loss = 0.67, batch loss = 0.60 (6.8 examples/sec; 1.175 sec/batch; 107h:40m:08s remains)
INFO - root - 2017-12-07 02:53:55.888647: step 2570, loss = 0.68, batch loss = 0.61 (7.2 examples/sec; 1.107 sec/batch; 101h:27m:34s remains)
INFO - root - 2017-12-07 02:54:07.292735: step 2580, loss = 0.74, batch loss = 0.67 (6.8 examples/sec; 1.175 sec/batch; 107h:39m:07s remains)
INFO - root - 2017-12-07 02:54:18.719049: step 2590, loss = 0.89, batch loss = 0.81 (6.7 examples/sec; 1.201 sec/batch; 110h:04m:56s remains)
INFO - root - 2017-12-07 02:54:29.901586: step 2600, loss = 0.96, batch loss = 0.89 (7.0 examples/sec; 1.148 sec/batch; 105h:13m:48s remains)
2017-12-07 02:54:30.700442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9503973 -2.9345336 -2.8507886 -2.7575417 -2.8038397 -2.8703656 -2.863265 -2.7743011 -2.7135205 -2.8432932 -3.0893202 -3.2616377 -3.3346968 -3.3655005 -3.3073883][-2.7407932 -2.7009759 -2.5269532 -2.3251243 -2.3208177 -2.3705544 -2.3879762 -2.3223455 -2.2705438 -2.498076 -2.9349422 -3.2160549 -3.3135061 -3.369236 -3.2836757][-2.599699 -2.5555291 -2.3162181 -2.0213318 -1.9404001 -1.9455237 -1.9862063 -1.9562469 -1.8881335 -2.1386805 -2.7034771 -3.0454271 -3.1231828 -3.1842532 -3.0433643][-2.5952933 -2.5549989 -2.3160346 -2.01109 -1.8859632 -1.8537078 -1.8999162 -1.896081 -1.8051875 -2.0151429 -2.5834739 -2.8822765 -2.8676918 -2.8787556 -2.6378264][-2.6187749 -2.5370722 -2.3152533 -2.060621 -1.9221389 -1.8385127 -1.8422546 -1.8574786 -1.8320336 -2.0759668 -2.6209497 -2.8532453 -2.7379408 -2.6708329 -2.2853422][-2.6116185 -2.4392552 -2.1893616 -1.9597354 -1.7856696 -1.6198313 -1.5284436 -1.5442979 -1.667366 -2.0653446 -2.6522005 -2.878037 -2.7316749 -2.617883 -2.1434784][-2.5267954 -2.2576654 -1.924093 -1.6552901 -1.4136295 -1.160356 -0.95659375 -0.94037724 -1.2004914 -1.7533734 -2.4223201 -2.7532513 -2.7039142 -2.6254902 -2.1566362][-2.4198661 -2.0953097 -1.6633031 -1.3164437 -1.0140598 -0.71656609 -0.45533514 -0.40924644 -0.74725938 -1.3855832 -2.1272035 -2.6099322 -2.7029419 -2.6654592 -2.2358625][-2.4385076 -2.1189251 -1.6460116 -1.2717254 -0.99560523 -0.79820633 -0.65843272 -0.64545274 -0.95064878 -1.5216756 -2.1997414 -2.7155075 -2.881753 -2.8502045 -2.4730611][-2.6407144 -2.377032 -1.9432883 -1.6136668 -1.4293802 -1.4042249 -1.4319611 -1.4747469 -1.7017572 -2.0835371 -2.5185261 -2.8680849 -2.9868836 -2.9563198 -2.6728687][-2.9594693 -2.771234 -2.441359 -2.2013631 -2.0902014 -2.1590052 -2.2543416 -2.2955902 -2.4097383 -2.5754027 -2.7395935 -2.856504 -2.8665142 -2.8510923 -2.7122648][-3.2980075 -3.1846952 -2.9655838 -2.8108239 -2.7228003 -2.777751 -2.8003533 -2.7221103 -2.6944003 -2.6846893 -2.6670952 -2.6237206 -2.5661907 -2.6010931 -2.6350121][-3.54516 -3.490684 -3.3643949 -3.2842104 -3.203337 -3.2073829 -3.0869606 -2.8017592 -2.6207309 -2.516396 -2.4805472 -2.4480672 -2.4250057 -2.5268803 -2.67246][-3.5078311 -3.4369857 -3.3573682 -3.3338342 -3.268878 -3.2647023 -3.058847 -2.6136556 -2.3178232 -2.1945145 -2.2787313 -2.4151781 -2.5254383 -2.6797323 -2.8365641][-3.1846764 -3.0271366 -2.9579482 -2.9982183 -2.9852519 -3.0138264 -2.7956126 -2.2899644 -1.9478273 -1.8521473 -2.0839427 -2.4092362 -2.6604116 -2.8541265 -2.9831076]]...]
INFO - root - 2017-12-07 02:54:42.042096: step 2610, loss = 0.86, batch loss = 0.79 (7.3 examples/sec; 1.103 sec/batch; 101h:03m:46s remains)
INFO - root - 2017-12-07 02:54:53.219551: step 2620, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 1.154 sec/batch; 105h:46m:52s remains)
INFO - root - 2017-12-07 02:55:04.487125: step 2630, loss = 0.67, batch loss = 0.60 (6.7 examples/sec; 1.200 sec/batch; 109h:54m:42s remains)
INFO - root - 2017-12-07 02:55:15.847094: step 2640, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 1.152 sec/batch; 105h:33m:47s remains)
INFO - root - 2017-12-07 02:55:27.219962: step 2650, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 1.135 sec/batch; 104h:01m:07s remains)
INFO - root - 2017-12-07 02:55:38.415709: step 2660, loss = 0.92, batch loss = 0.84 (7.5 examples/sec; 1.061 sec/batch; 97h:13m:59s remains)
INFO - root - 2017-12-07 02:55:49.814184: step 2670, loss = 0.83, batch loss = 0.76 (7.5 examples/sec; 1.062 sec/batch; 97h:16m:18s remains)
INFO - root - 2017-12-07 02:56:01.111944: step 2680, loss = 0.79, batch loss = 0.72 (6.7 examples/sec; 1.187 sec/batch; 108h:42m:42s remains)
INFO - root - 2017-12-07 02:56:12.433091: step 2690, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.182 sec/batch; 108h:19m:27s remains)
INFO - root - 2017-12-07 02:56:26.730662: step 2700, loss = 0.75, batch loss = 0.68 (6.5 examples/sec; 1.235 sec/batch; 113h:07m:09s remains)
2017-12-07 02:56:27.513803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1002355 -2.1792345 -2.1872044 -2.2569609 -2.2792392 -2.2480075 -2.2151334 -2.1754422 -2.1737859 -2.1472626 -2.146409 -2.171211 -2.1784089 -2.2014053 -2.2137284][-2.0396335 -1.950213 -1.8638668 -1.8960769 -1.9251325 -1.9223301 -1.8895168 -1.7972465 -1.7400677 -1.6656618 -1.6660354 -1.7266815 -1.7688634 -1.8181615 -1.8222363][-1.6933069 -1.6683915 -1.7042048 -1.8611467 -1.9895158 -2.072742 -2.0891583 -2.0025065 -1.9226253 -1.7685025 -1.6856589 -1.6727319 -1.669513 -1.6790593 -1.6137011][-1.5188141 -1.5942419 -1.767102 -2.0439837 -2.2505913 -2.3778079 -2.3891792 -2.301872 -2.2427487 -2.0763357 -1.941076 -1.8788242 -1.9047067 -1.9556444 -1.8764911][-1.8205326 -1.8622966 -1.9853942 -2.18951 -2.3300562 -2.4032454 -2.3645194 -2.2835321 -2.3361313 -2.3413179 -2.2926712 -2.2353649 -2.2821028 -2.3119929 -2.1762781][-1.516757 -1.4495056 -1.4627478 -1.5738494 -1.6933024 -1.7814691 -1.8098788 -1.8235359 -2.0310135 -2.2538655 -2.3397176 -2.3367896 -2.4214771 -2.4300539 -2.2898774][-0.18149424 -0.047698021 0.054374218 0.10149097 0.10245705 0.087536812 0.012811184 -0.087272644 -0.38817358 -0.7450664 -0.96499848 -1.1528847 -1.4924152 -1.7293849 -1.8429296][0.65746689 0.76307964 0.83516312 0.89877462 1.0058856 1.1560774 1.2108445 1.1634922 0.84095621 0.4156394 0.11458683 -0.16927338 -0.55890155 -0.77925134 -0.92843151][-0.03478384 -0.011280537 -0.04266119 -0.13069296 -0.15105438 -0.146626 -0.209836 -0.3414216 -0.69979453 -1.1111047 -1.335109 -1.4461267 -1.5435705 -1.4372854 -1.3154032][-0.98869729 -1.0638185 -1.1453001 -1.2718132 -1.3213468 -1.4263251 -1.5455725 -1.6317375 -1.8630838 -2.12353 -2.2357984 -2.2413802 -2.21419 -2.0286098 -1.8735855][-0.99066448 -1.1065373 -1.2062371 -1.3057909 -1.3099401 -1.3999407 -1.4898703 -1.5419283 -1.7118752 -1.8725741 -1.8901703 -1.783052 -1.652775 -1.4685333 -1.3734124][-0.84682631 -0.9331758 -1.0712559 -1.2083001 -1.3027828 -1.514364 -1.6627138 -1.7376108 -1.841696 -1.8675306 -1.7708836 -1.5589337 -1.3773439 -1.2359488 -1.2128708][-0.98359632 -1.0079622 -1.2013078 -1.4137762 -1.6252918 -1.9486156 -2.1065347 -2.1308532 -2.122236 -2.0153019 -1.8474641 -1.5980017 -1.4251549 -1.4014084 -1.5558455][-1.1008039 -1.0973308 -1.3054056 -1.4812608 -1.6298659 -1.8537648 -1.9040041 -1.8915412 -1.9057374 -1.8877468 -1.880954 -1.7821224 -1.6774106 -1.7035494 -1.8911762][-1.698276 -1.624944 -1.7530611 -1.8752792 -1.9956863 -2.1678088 -2.1670325 -2.1201482 -2.0988445 -2.069474 -2.1267331 -2.1347318 -2.0892985 -2.1237991 -2.2339513]]...]
INFO - root - 2017-12-07 02:56:38.813373: step 2710, loss = 1.00, batch loss = 0.93 (7.2 examples/sec; 1.116 sec/batch; 102h:11m:34s remains)
INFO - root - 2017-12-07 02:56:50.148771: step 2720, loss = 0.88, batch loss = 0.81 (7.2 examples/sec; 1.111 sec/batch; 101h:45m:28s remains)
INFO - root - 2017-12-07 02:57:01.452570: step 2730, loss = 0.60, batch loss = 0.53 (6.7 examples/sec; 1.199 sec/batch; 109h:51m:08s remains)
INFO - root - 2017-12-07 02:57:12.711739: step 2740, loss = 1.02, batch loss = 0.95 (7.2 examples/sec; 1.118 sec/batch; 102h:25m:47s remains)
INFO - root - 2017-12-07 02:57:24.028917: step 2750, loss = 0.64, batch loss = 0.57 (7.2 examples/sec; 1.107 sec/batch; 101h:25m:27s remains)
INFO - root - 2017-12-07 02:57:35.480946: step 2760, loss = 0.77, batch loss = 0.70 (7.3 examples/sec; 1.091 sec/batch; 99h:56m:33s remains)
INFO - root - 2017-12-07 02:57:46.819012: step 2770, loss = 0.84, batch loss = 0.77 (7.6 examples/sec; 1.052 sec/batch; 96h:18m:33s remains)
INFO - root - 2017-12-07 02:57:58.269709: step 2780, loss = 0.65, batch loss = 0.58 (6.8 examples/sec; 1.181 sec/batch; 108h:09m:05s remains)
INFO - root - 2017-12-07 02:58:09.723006: step 2790, loss = 0.82, batch loss = 0.75 (6.6 examples/sec; 1.218 sec/batch; 111h:31m:53s remains)
INFO - root - 2017-12-07 02:58:21.087869: step 2800, loss = 0.61, batch loss = 0.53 (7.2 examples/sec; 1.111 sec/batch; 101h:44m:55s remains)
2017-12-07 02:58:21.901177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0475421 -4.0862408 -4.0933127 -4.0662818 -4.0113277 -3.9421329 -3.8668661 -3.8026288 -3.75673 -3.7454224 -3.7433684 -3.7266078 -3.6955986 -3.6517124 -3.601737][-4.0491605 -4.0140772 -3.9653571 -3.9135072 -3.8564181 -3.7898443 -3.7149029 -3.6569262 -3.6226482 -3.6438322 -3.6817873 -3.7071083 -3.7140744 -3.688411 -3.6315415][-3.6106105 -3.4641573 -3.3498964 -3.2836118 -3.2414753 -3.214442 -3.1952662 -3.201401 -3.2214243 -3.2837739 -3.3662109 -3.4478543 -3.5229604 -3.5577202 -3.5381241][-2.8576913 -2.6257987 -2.4848609 -2.4497266 -2.452522 -2.4717236 -2.5031433 -2.5666378 -2.6552391 -2.7845449 -2.9322038 -3.0787067 -3.2135644 -3.3014946 -3.3238292][-2.0897331 -1.829354 -1.6880028 -1.6700912 -1.6612263 -1.6518807 -1.6517646 -1.7173908 -1.8391833 -2.0114436 -2.2205548 -2.46282 -2.7126427 -2.9062729 -3.0033135][-1.3210826 -1.0885544 -0.97193527 -0.94299054 -0.86978054 -0.74706078 -0.6544416 -0.70281291 -0.86530375 -1.0939772 -1.3601716 -1.6967516 -2.0794704 -2.3900406 -2.5774045][-0.75502133 -0.49527025 -0.30642891 -0.157969 0.062941551 0.36845016 0.60152674 0.59526825 0.35156822 -0.0084934235 -0.40095139 -0.89183736 -1.4561534 -1.9045894 -2.1800008][-0.60962772 -0.30732059 -0.022770405 0.26856041 0.64000607 1.0751748 1.432488 1.4971271 1.2319398 0.83832312 0.42561388 -0.1432972 -0.85992694 -1.4561388 -1.8368094][-1.2788908 -1.0789232 -0.86884189 -0.59633327 -0.24091625 0.13040113 0.41303253 0.4342823 0.21166706 -0.072533607 -0.33314419 -0.74171829 -1.3257155 -1.7887411 -2.030236][-2.3390524 -2.3151848 -2.2844775 -2.140485 -1.8916278 -1.6643479 -1.511972 -1.5487037 -1.7330115 -1.9128652 -2.0643158 -2.3120236 -2.6683846 -2.8666642 -2.8338027][-2.9456906 -3.0175982 -3.1071205 -3.0823874 -2.9765787 -2.9123871 -2.8964803 -2.9806132 -3.1370451 -3.2617412 -3.3473229 -3.4770472 -3.6488988 -3.6586173 -3.4627931][-3.4935865 -3.5594733 -3.6145754 -3.5620477 -3.4675386 -3.4353137 -3.4516284 -3.5365407 -3.6925085 -3.8289747 -3.9187722 -4.014853 -4.0970364 -4.0101976 -3.7500973][-4.1468763 -4.1652355 -4.1434369 -4.0265651 -3.9001729 -3.8319349 -3.8088257 -3.8359256 -3.9286475 -4.0317721 -4.1067553 -4.1748862 -4.2041955 -4.0827065 -3.8291056][-4.7167525 -4.7011623 -4.6408434 -4.5191154 -4.416995 -4.3591304 -4.3306422 -4.3319483 -4.3647418 -4.3870769 -4.3651915 -4.3225737 -4.2503881 -4.073019 -3.8189015][-4.81668 -4.8122044 -4.7609997 -4.6650777 -4.585042 -4.5372477 -4.516583 -4.5196123 -4.529922 -4.5121083 -4.4411378 -4.33664 -4.1974821 -3.9972949 -3.7697849]]...]
INFO - root - 2017-12-07 02:58:33.235286: step 2810, loss = 0.83, batch loss = 0.75 (7.5 examples/sec; 1.070 sec/batch; 97h:59m:13s remains)
INFO - root - 2017-12-07 02:58:44.641657: step 2820, loss = 0.67, batch loss = 0.60 (6.9 examples/sec; 1.160 sec/batch; 106h:15m:59s remains)
INFO - root - 2017-12-07 02:58:56.068460: step 2830, loss = 0.82, batch loss = 0.74 (6.5 examples/sec; 1.240 sec/batch; 113h:33m:11s remains)
INFO - root - 2017-12-07 02:59:07.298006: step 2840, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 1.166 sec/batch; 106h:46m:15s remains)
INFO - root - 2017-12-07 02:59:18.567358: step 2850, loss = 0.86, batch loss = 0.79 (7.3 examples/sec; 1.103 sec/batch; 101h:01m:05s remains)
INFO - root - 2017-12-07 02:59:29.872562: step 2860, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 1.128 sec/batch; 103h:18m:21s remains)
INFO - root - 2017-12-07 02:59:41.281000: step 2870, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 1.125 sec/batch; 103h:01m:45s remains)
INFO - root - 2017-12-07 02:59:52.588160: step 2880, loss = 0.91, batch loss = 0.84 (7.1 examples/sec; 1.134 sec/batch; 103h:48m:31s remains)
INFO - root - 2017-12-07 03:00:03.975840: step 2890, loss = 0.57, batch loss = 0.50 (7.2 examples/sec; 1.113 sec/batch; 101h:55m:41s remains)
INFO - root - 2017-12-07 03:00:12.867009: step 2900, loss = 0.93, batch loss = 0.86 (10.4 examples/sec; 0.766 sec/batch; 70h:09m:39s remains)
2017-12-07 03:00:13.501922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.192204 -4.0788226 -3.9376044 -3.7444844 -3.5627093 -3.5003877 -3.5150752 -3.7075839 -3.8789062 -3.9254532 -3.8278172 -3.7076817 -3.7366366 -3.7020421 -3.698782][-4.1922636 -4.1346626 -4.0908942 -3.9738975 -3.8268037 -3.7347939 -3.6593733 -3.800808 -3.9644601 -3.9195988 -3.6859419 -3.52869 -3.6530116 -3.7295492 -3.788209][-4.2315636 -4.2362704 -4.3148222 -4.3345342 -4.2616196 -4.1565948 -3.9951749 -4.0434289 -4.1487031 -4.0087819 -3.6849837 -3.518656 -3.6946607 -3.8204417 -3.8820996][-4.1585712 -4.2080822 -4.3834853 -4.541431 -4.5593052 -4.4729748 -4.2635846 -4.2065587 -4.1894097 -3.9355621 -3.5489559 -3.3814988 -3.5924807 -3.7593417 -3.8370361][-4.2055106 -4.3058586 -4.552516 -4.8425417 -5.0001583 -5.0080662 -4.8164339 -4.6333752 -4.3958845 -3.935559 -3.4338489 -3.2206023 -3.4365036 -3.6638107 -3.803452][-4.46235 -4.5490088 -4.7465143 -5.0372324 -5.271873 -5.3751564 -5.2512627 -5.0005493 -4.5996237 -4.0063214 -3.4460745 -3.1862142 -3.3664241 -3.6105351 -3.8100739][-4.833137 -4.8762674 -4.9330759 -5.0852475 -5.2422466 -5.3093243 -5.1917953 -4.9194145 -4.5095854 -3.9704673 -3.5192833 -3.3212953 -3.4895802 -3.7126851 -3.8989041][-5.1414356 -5.1588287 -5.0992222 -5.1210256 -5.1795821 -5.1853852 -5.0635481 -4.8023615 -4.4656253 -4.0589175 -3.7652924 -3.6698425 -3.8614192 -4.0941195 -4.2506404][-5.1588087 -5.1828494 -5.0701613 -5.0230346 -5.0243449 -5.0239592 -4.9591551 -4.7889209 -4.5883412 -4.3014288 -4.0961843 -4.0746436 -4.3156881 -4.5843296 -4.7140956][-5.13843 -5.1778207 -5.0701866 -5.0025711 -4.9677525 -4.9714713 -4.9455752 -4.8335958 -4.7174821 -4.4639044 -4.2622747 -4.288908 -4.6018419 -4.9459209 -5.0790143][-5.1411638 -5.1852875 -5.077765 -4.9417853 -4.8013077 -4.7477984 -4.6845465 -4.5771551 -4.4996438 -4.2761965 -4.1088166 -4.1987863 -4.5871959 -4.9948988 -5.1767964][-5.1883817 -5.2359776 -5.1113949 -4.8375521 -4.5275483 -4.3898444 -4.2973318 -4.2402725 -4.2320185 -4.0676022 -3.9656622 -4.0919514 -4.4701214 -4.841671 -5.039422][-5.2530918 -5.3098903 -5.1702256 -4.7933083 -4.3853059 -4.2472162 -4.2152572 -4.2770896 -4.3302245 -4.1431632 -3.9916396 -4.0219064 -4.24237 -4.4773507 -4.6702075][-5.2405362 -5.2889762 -5.1466336 -4.7523375 -4.3533564 -4.2842646 -4.3122735 -4.4065022 -4.4066243 -4.1134567 -3.8634021 -3.7650361 -3.841326 -4.0103488 -4.26722][-5.2243738 -5.2957458 -5.16799 -4.7918425 -4.4378695 -4.4177389 -4.4409804 -4.4703035 -4.3844957 -4.0622482 -3.7920914 -3.6080151 -3.5744243 -3.702956 -4.009913]]...]
INFO - root - 2017-12-07 03:00:21.569961: step 2910, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.816 sec/batch; 74h:43m:17s remains)
INFO - root - 2017-12-07 03:00:29.646635: step 2920, loss = 0.80, batch loss = 0.73 (10.0 examples/sec; 0.800 sec/batch; 73h:15m:29s remains)
INFO - root - 2017-12-07 03:00:37.715807: step 2930, loss = 0.93, batch loss = 0.85 (10.4 examples/sec; 0.768 sec/batch; 70h:19m:54s remains)
INFO - root - 2017-12-07 03:00:45.699866: step 2940, loss = 0.61, batch loss = 0.53 (10.3 examples/sec; 0.777 sec/batch; 71h:05m:13s remains)
INFO - root - 2017-12-07 03:00:53.762973: step 2950, loss = 0.94, batch loss = 0.86 (9.8 examples/sec; 0.814 sec/batch; 74h:31m:09s remains)
INFO - root - 2017-12-07 03:01:01.884612: step 2960, loss = 0.91, batch loss = 0.83 (9.2 examples/sec; 0.870 sec/batch; 79h:37m:11s remains)
INFO - root - 2017-12-07 03:01:09.871524: step 2970, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.795 sec/batch; 72h:48m:52s remains)
INFO - root - 2017-12-07 03:01:18.078101: step 2980, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.808 sec/batch; 73h:56m:18s remains)
INFO - root - 2017-12-07 03:01:26.171015: step 2990, loss = 0.63, batch loss = 0.56 (10.0 examples/sec; 0.803 sec/batch; 73h:29m:59s remains)
INFO - root - 2017-12-07 03:01:34.163864: step 3000, loss = 0.92, batch loss = 0.85 (10.1 examples/sec; 0.791 sec/batch; 72h:25m:27s remains)
2017-12-07 03:01:34.761212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3018978 -3.3636475 -3.4651039 -3.6004615 -3.7184348 -3.7603889 -3.6729479 -3.5351093 -3.3750074 -3.3399091 -3.4077587 -3.5147665 -3.628155 -3.8462873 -4.1426544][-3.0334733 -3.0471573 -3.0683737 -3.1317368 -3.203464 -3.2327275 -3.1172366 -2.9053779 -2.7703362 -2.8414874 -2.9654264 -3.0660067 -3.1947227 -3.4853709 -3.7641416][-2.6431832 -2.5490875 -2.4634342 -2.50944 -2.654429 -2.7679753 -2.6415555 -2.3281755 -2.2023225 -2.3491507 -2.5112557 -2.6851416 -2.956069 -3.3162246 -3.4389076][-2.5173292 -2.4013019 -2.3231454 -2.4545319 -2.7004571 -2.8526123 -2.6843133 -2.3095438 -2.1926363 -2.304306 -2.3781283 -2.586812 -3.0060816 -3.3931618 -3.4148068][-2.4772372 -2.3655868 -2.3647728 -2.6116185 -2.9405637 -3.0562167 -2.8350351 -2.5153804 -2.4480479 -2.4403248 -2.3390524 -2.4828596 -2.8925438 -3.1934876 -3.1479762][-2.3171403 -2.1013777 -2.060818 -2.30799 -2.6147857 -2.6901808 -2.5273914 -2.3815565 -2.3754787 -2.2745926 -2.0841057 -2.2005053 -2.5308597 -2.7599661 -2.7676945][-2.2401235 -2.0021369 -1.9314313 -2.1143429 -2.3184114 -2.3537438 -2.2927756 -2.3005505 -2.2727456 -2.0584459 -1.8372598 -1.901762 -2.0810945 -2.2205048 -2.3304377][-2.1225924 -1.9534981 -1.941843 -2.1078134 -2.2049196 -2.1779919 -2.1520047 -2.19462 -2.1233573 -1.8836665 -1.7082183 -1.7202516 -1.7385495 -1.7553158 -1.868068][-2.2138405 -2.1044483 -2.146405 -2.2936771 -2.31262 -2.2113295 -2.102488 -2.0325129 -1.843956 -1.5858231 -1.4716268 -1.5085506 -1.5184758 -1.552593 -1.7107201][-2.3915031 -2.3648586 -2.4355059 -2.5345392 -2.5177319 -2.4063303 -2.2627873 -2.1347919 -1.9226961 -1.6825931 -1.5904069 -1.6306372 -1.6588595 -1.7505474 -1.9810867][-2.52778 -2.5698566 -2.6672163 -2.7523341 -2.7590313 -2.7107284 -2.634161 -2.5607955 -2.4376493 -2.2811801 -2.1965311 -2.1810577 -2.1655703 -2.22245 -2.4246197][-2.7523565 -2.8258557 -2.933198 -3.0298791 -3.0858128 -3.0909572 -3.0513589 -2.989361 -2.8984725 -2.7823224 -2.6970358 -2.6413388 -2.6119227 -2.6636016 -2.8309793][-2.9320908 -2.9944751 -3.0888786 -3.177887 -3.2514718 -3.2808151 -3.2616887 -3.206418 -3.13375 -3.051085 -2.9844115 -2.9392815 -2.9379559 -3.0127406 -3.1563625][-3.0967028 -3.1487103 -3.2249355 -3.2992692 -3.3656936 -3.3965869 -3.3915145 -3.3576574 -3.3138018 -3.2729774 -3.2430789 -3.2271762 -3.2413347 -3.2912798 -3.3583326][-3.1964874 -3.2386785 -3.2861938 -3.3265705 -3.3613715 -3.3834169 -3.3928928 -3.3875413 -3.3695793 -3.3440945 -3.3187134 -3.300535 -3.2973471 -3.3004282 -3.2999282]]...]
INFO - root - 2017-12-07 03:01:42.787900: step 3010, loss = 0.84, batch loss = 0.77 (9.4 examples/sec; 0.855 sec/batch; 78h:12m:50s remains)
INFO - root - 2017-12-07 03:01:50.863981: step 3020, loss = 0.91, batch loss = 0.83 (10.2 examples/sec; 0.781 sec/batch; 71h:28m:40s remains)
INFO - root - 2017-12-07 03:01:58.982706: step 3030, loss = 0.71, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 74h:33m:18s remains)
INFO - root - 2017-12-07 03:02:07.012863: step 3040, loss = 0.72, batch loss = 0.64 (10.3 examples/sec; 0.780 sec/batch; 71h:23m:06s remains)
INFO - root - 2017-12-07 03:02:14.941249: step 3050, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.755 sec/batch; 69h:06m:06s remains)
INFO - root - 2017-12-07 03:02:23.067688: step 3060, loss = 0.84, batch loss = 0.76 (9.6 examples/sec; 0.832 sec/batch; 76h:07m:28s remains)
INFO - root - 2017-12-07 03:02:31.045948: step 3070, loss = 0.65, batch loss = 0.58 (9.4 examples/sec; 0.849 sec/batch; 77h:43m:25s remains)
INFO - root - 2017-12-07 03:02:41.775792: step 3080, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.827 sec/batch; 75h:42m:14s remains)
INFO - root - 2017-12-07 03:02:49.764898: step 3090, loss = 0.81, batch loss = 0.73 (10.3 examples/sec; 0.778 sec/batch; 71h:13m:04s remains)
INFO - root - 2017-12-07 03:02:57.729952: step 3100, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.787 sec/batch; 72h:00m:19s remains)
2017-12-07 03:02:58.290378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9058406 -1.8678772 -1.9239008 -2.0348947 -2.197711 -2.4243555 -2.6179004 -2.8060532 -2.8653233 -2.7492971 -2.7567341 -3.0176654 -3.3190508 -3.5783696 -3.8006864][-1.8233254 -1.7724373 -1.8136997 -1.9062078 -2.0721359 -2.3648708 -2.6669674 -2.9853 -3.0650918 -2.7850766 -2.6245451 -2.7910745 -2.9985166 -3.1506994 -3.3731716][-1.6576102 -1.620909 -1.6872003 -1.8055007 -2.0034432 -2.3473802 -2.7363048 -3.1701939 -3.29709 -2.8992348 -2.5527186 -2.5285172 -2.4981709 -2.3861423 -2.471941][-1.8336875 -1.7803149 -1.8648019 -2.0057182 -2.177665 -2.4437051 -2.7782454 -3.1877747 -3.353333 -2.9584646 -2.5065227 -2.2875953 -2.0090892 -1.589735 -1.4375563][-2.3027306 -2.1681702 -2.1936872 -2.2663925 -2.2956784 -2.3542516 -2.4847112 -2.6983871 -2.8492723 -2.5788918 -2.1936619 -1.9573586 -1.5872483 -0.99133396 -0.6533556][-2.5435519 -2.316705 -2.2578287 -2.2305593 -2.1020615 -1.9145856 -1.7001085 -1.5747027 -1.6867723 -1.6200638 -1.3827662 -1.2394178 -1.004842 -0.53286934 -0.25286627][-2.3512826 -1.9599705 -1.7233553 -1.524302 -1.2681258 -0.86309981 -0.24761152 0.25986814 0.10452509 -0.1222024 -0.091946125 -0.12368727 -0.23548222 -0.22558594 -0.31360769][-1.7103865 -1.2301505 -0.84187675 -0.46942067 -0.15924501 0.34121323 1.2169509 2.0200105 1.8035097 1.3006902 1.0555496 0.7531085 0.23928499 -0.26456547 -0.75831556][-1.0674124 -0.65229225 -0.22903347 0.20196486 0.461411 0.87282467 1.6752315 2.4930019 2.3746014 1.7962995 1.3274288 0.87028074 0.19902611 -0.561471 -1.2778492][-0.65039992 -0.34768772 0.03190136 0.41841078 0.61507607 0.88352442 1.3783698 1.9386015 1.9431591 1.4490857 0.90117311 0.45935106 -0.12146902 -0.900161 -1.6798601][-0.46894097 -0.29425144 -0.037302971 0.19797611 0.29837179 0.39595604 0.55333471 0.83205175 0.90664625 0.57738352 0.13598061 -0.1572051 -0.52086782 -1.1658461 -1.8585689][-0.65703177 -0.56105042 -0.41818142 -0.33112621 -0.33772802 -0.37974739 -0.43346691 -0.33051872 -0.24389982 -0.40092802 -0.61492896 -0.69384313 -0.82907748 -1.3344026 -1.9031992][-1.0550756 -0.97324038 -0.88154984 -0.89271474 -0.96673727 -1.0662041 -1.1606586 -1.1266582 -1.0811412 -1.1205723 -1.1339622 -1.0478485 -1.0506899 -1.5108061 -2.0336161][-1.4679344 -1.382139 -1.2891462 -1.326304 -1.4080348 -1.4974051 -1.544925 -1.5249293 -1.5296905 -1.5262184 -1.4442723 -1.3103151 -1.2655222 -1.720953 -2.2260823][-1.769361 -1.6716537 -1.5544016 -1.5860569 -1.6592665 -1.7203138 -1.7250504 -1.7122157 -1.760741 -1.7760706 -1.6966712 -1.5868351 -1.5339477 -1.9840505 -2.4545012]]...]
INFO - root - 2017-12-07 03:03:06.416593: step 3110, loss = 0.55, batch loss = 0.48 (9.5 examples/sec; 0.845 sec/batch; 77h:17m:18s remains)
INFO - root - 2017-12-07 03:03:14.468091: step 3120, loss = 0.80, batch loss = 0.73 (9.0 examples/sec; 0.885 sec/batch; 80h:57m:46s remains)
INFO - root - 2017-12-07 03:03:22.504694: step 3130, loss = 0.93, batch loss = 0.85 (10.0 examples/sec; 0.800 sec/batch; 73h:11m:39s remains)
INFO - root - 2017-12-07 03:03:30.457707: step 3140, loss = 0.90, batch loss = 0.82 (10.3 examples/sec; 0.775 sec/batch; 70h:53m:47s remains)
INFO - root - 2017-12-07 03:03:38.581760: step 3150, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.796 sec/batch; 72h:50m:33s remains)
INFO - root - 2017-12-07 03:03:46.577434: step 3160, loss = 0.64, batch loss = 0.56 (10.0 examples/sec; 0.797 sec/batch; 72h:57m:11s remains)
INFO - root - 2017-12-07 03:03:54.733958: step 3170, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.840 sec/batch; 76h:49m:23s remains)
INFO - root - 2017-12-07 03:04:02.754324: step 3180, loss = 0.84, batch loss = 0.77 (9.4 examples/sec; 0.855 sec/batch; 78h:12m:59s remains)
INFO - root - 2017-12-07 03:04:10.838883: step 3190, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.787 sec/batch; 72h:00m:34s remains)
INFO - root - 2017-12-07 03:04:18.615677: step 3200, loss = 0.77, batch loss = 0.70 (10.0 examples/sec; 0.803 sec/batch; 73h:26m:24s remains)
2017-12-07 03:04:19.225733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2639856 -4.2448025 -4.2576623 -4.2951584 -4.3404474 -4.367034 -4.3898454 -4.3904634 -4.3893929 -4.3996868 -4.3626451 -4.2511225 -4.0798149 -3.8870339 -3.7439795][-4.3651052 -4.313334 -4.3143587 -4.3519039 -4.4340768 -4.5108352 -4.585783 -4.6548266 -4.7212105 -4.7724829 -4.7003956 -4.4961228 -4.2276721 -3.956933 -3.7728312][-4.3854952 -4.3353271 -4.372366 -4.4396811 -4.5825987 -4.7328181 -4.8668666 -5.0168314 -5.1448112 -5.2061348 -5.0179167 -4.6322594 -4.2249217 -3.8887012 -3.7133751][-4.2207489 -4.1807318 -4.2714062 -4.3892927 -4.59 -4.804636 -4.9676037 -5.1636209 -5.315608 -5.3524508 -5.0129814 -4.4069386 -3.8515792 -3.4993627 -3.4331238][-3.9611652 -3.923049 -4.0473781 -4.1835952 -4.3894153 -4.6163411 -4.7166924 -4.8330774 -4.9021325 -4.8491712 -4.3662019 -3.6111205 -3.0269828 -2.7919016 -2.9386177][-3.6134377 -3.5394859 -3.5958986 -3.6508043 -3.7706416 -3.924895 -3.877986 -3.7929845 -3.6901119 -3.5036569 -2.9803457 -2.3043513 -1.9318507 -1.9933977 -2.4211283][-3.3385973 -3.1455681 -3.0000391 -2.8511782 -2.7922626 -2.7942398 -2.5484309 -2.2429595 -1.99809 -1.7707508 -1.3800988 -1.0081646 -1.0410471 -1.4714773 -2.1473932][-3.2278607 -2.8431821 -2.4272838 -2.071424 -1.8672554 -1.7535565 -1.4083741 -1.030484 -0.78339815 -0.65111923 -0.47667694 -0.40886259 -0.7324717 -1.3517935 -2.1092129][-2.9921544 -2.4479561 -1.8971813 -1.517586 -1.3132434 -1.1620252 -0.81867504 -0.53609276 -0.49453378 -0.61269093 -0.71435285 -0.84035754 -1.1736791 -1.6533096 -2.261771][-2.5811772 -1.9904215 -1.5106385 -1.3012545 -1.2769861 -1.2547669 -1.0408094 -0.94943619 -1.1533065 -1.4425437 -1.6344872 -1.7138977 -1.8537049 -2.0826859 -2.4962468][-2.1455863 -1.5821579 -1.2375586 -1.2413657 -1.472975 -1.7271633 -1.7470036 -1.8087385 -2.0766687 -2.311486 -2.3831885 -2.3047624 -2.2653754 -2.3578463 -2.6759834][-1.9441755 -1.4046855 -1.1711037 -1.3338933 -1.7267809 -2.1322014 -2.2576702 -2.2862215 -2.4159753 -2.4885287 -2.4490118 -2.3469055 -2.3090136 -2.4363062 -2.7784357][-2.0866868 -1.6729741 -1.5406551 -1.7542019 -2.1176262 -2.4268222 -2.4754639 -2.382086 -2.3702652 -2.3648276 -2.3351772 -2.3307643 -2.397423 -2.5951271 -2.9410348][-2.6575389 -2.4631753 -2.4029129 -2.5664022 -2.8056176 -2.9503059 -2.9033022 -2.7600334 -2.6966212 -2.6969378 -2.7182045 -2.7839158 -2.8772097 -3.0266681 -3.2575207][-3.477921 -3.417738 -3.3433237 -3.3799877 -3.4644108 -3.5020733 -3.4491482 -3.3579888 -3.3308196 -3.3529327 -3.3803561 -3.4151134 -3.43682 -3.47118 -3.5469236]]...]
INFO - root - 2017-12-07 03:04:27.397172: step 3210, loss = 0.79, batch loss = 0.71 (8.9 examples/sec; 0.900 sec/batch; 82h:17m:18s remains)
INFO - root - 2017-12-07 03:04:35.428684: step 3220, loss = 0.88, batch loss = 0.81 (10.3 examples/sec; 0.776 sec/batch; 71h:00m:26s remains)
INFO - root - 2017-12-07 03:04:43.508019: step 3230, loss = 0.83, batch loss = 0.76 (10.1 examples/sec; 0.793 sec/batch; 72h:29m:55s remains)
INFO - root - 2017-12-07 03:04:51.549240: step 3240, loss = 0.75, batch loss = 0.67 (9.8 examples/sec; 0.819 sec/batch; 74h:55m:52s remains)
INFO - root - 2017-12-07 03:04:59.527234: step 3250, loss = 0.92, batch loss = 0.85 (10.6 examples/sec; 0.753 sec/batch; 68h:52m:48s remains)
INFO - root - 2017-12-07 03:05:07.566748: step 3260, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.824 sec/batch; 75h:21m:27s remains)
INFO - root - 2017-12-07 03:05:15.592106: step 3270, loss = 0.90, batch loss = 0.83 (9.5 examples/sec; 0.840 sec/batch; 76h:49m:02s remains)
INFO - root - 2017-12-07 03:05:23.583748: step 3280, loss = 0.66, batch loss = 0.59 (10.2 examples/sec; 0.781 sec/batch; 71h:25m:14s remains)
INFO - root - 2017-12-07 03:05:31.580976: step 3290, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.792 sec/batch; 72h:27m:25s remains)
INFO - root - 2017-12-07 03:05:39.746767: step 3300, loss = 0.55, batch loss = 0.48 (9.5 examples/sec; 0.839 sec/batch; 76h:44m:29s remains)
2017-12-07 03:05:40.372472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6500785 -3.4511006 -3.368248 -3.5109525 -3.7811663 -4.2006631 -4.5413585 -4.5518923 -4.2034531 -3.5538318 -3.0108604 -2.9077854 -3.1600571 -3.5190144 -3.6188259][-3.8453894 -3.6800058 -3.5490129 -3.6581736 -3.9371622 -4.3720312 -4.7351074 -4.7600193 -4.390903 -3.5881553 -2.8357186 -2.6713934 -3.0867186 -3.66404 -3.7975292][-3.750421 -3.596518 -3.3442106 -3.3586226 -3.6567342 -4.0977082 -4.4525447 -4.4788527 -4.1510305 -3.3723631 -2.5050335 -2.2365286 -2.7395277 -3.5191746 -3.7367771][-3.6600082 -3.6529279 -3.2938766 -3.1185091 -3.3237429 -3.6168504 -3.8024523 -3.7613275 -3.5192616 -2.9310157 -2.0702078 -1.6055067 -1.9821692 -2.7819138 -3.074872][-3.7806387 -3.989671 -3.5490339 -3.152473 -3.123982 -3.0667229 -2.9148011 -2.7354174 -2.6417532 -2.421032 -1.7986057 -1.2030745 -1.2681656 -1.8222308 -2.0777137][-3.8235338 -4.0569143 -3.5338793 -3.0416439 -2.8732452 -2.5258946 -2.0077899 -1.5344865 -1.4715831 -1.6939421 -1.5750201 -1.1501303 -1.0260837 -1.2933192 -1.461324][-3.8837574 -4.0294833 -3.44621 -2.937099 -2.606708 -1.9433889 -0.92609143 0.088229656 0.34299088 -0.26029682 -0.77532482 -0.79647422 -0.7478137 -0.89567995 -1.0639594][-3.7019844 -3.7061667 -3.1261344 -2.6794031 -2.2714686 -1.409678 0.011375904 1.6057558 2.1894035 1.3395481 0.32777691 -0.13118649 -0.26243925 -0.40351629 -0.60431719][-3.39246 -3.2174582 -2.6890521 -2.3924317 -2.1008182 -1.3486836 0.025673866 1.6857409 2.4193163 1.6582761 0.56939363 -0.0805068 -0.35783625 -0.5365262 -0.6823473][-3.3759255 -3.073894 -2.5905724 -2.3653095 -2.1425905 -1.5873942 -0.58901453 0.62523794 1.2024226 0.71771193 -0.05702734 -0.5910418 -0.88028574 -1.078243 -1.1149006][-3.4131866 -3.0846162 -2.6718321 -2.4384484 -2.1809394 -1.7476249 -1.1539197 -0.51854944 -0.27360058 -0.53840327 -0.87060022 -1.1164086 -1.2885935 -1.4192386 -1.357749][-3.1566765 -2.8148746 -2.4966879 -2.3121834 -2.1360295 -1.9179869 -1.6864378 -1.4853115 -1.4776921 -1.5373392 -1.4795442 -1.4387205 -1.4779944 -1.5540709 -1.5106931][-2.9618423 -2.6179786 -2.3897152 -2.3138812 -2.2803006 -2.2533934 -2.2309716 -2.237514 -2.2841806 -2.1969724 -1.9648738 -1.791405 -1.7476425 -1.8051107 -1.8366921][-3.0227954 -2.7681932 -2.6359932 -2.6436894 -2.6777158 -2.6974788 -2.6902366 -2.702538 -2.6996405 -2.5730515 -2.3535995 -2.1851337 -2.11121 -2.1402652 -2.2114573][-3.1555562 -3.0197592 -2.9670308 -3.0123961 -3.0612803 -3.0644865 -3.0234184 -2.984756 -2.9421115 -2.854136 -2.7422223 -2.6605763 -2.6178207 -2.6261916 -2.6733668]]...]
INFO - root - 2017-12-07 03:05:48.389951: step 3310, loss = 0.63, batch loss = 0.56 (9.5 examples/sec; 0.840 sec/batch; 76h:47m:47s remains)
INFO - root - 2017-12-07 03:05:56.457079: step 3320, loss = 1.04, batch loss = 0.96 (9.8 examples/sec; 0.817 sec/batch; 74h:40m:10s remains)
INFO - root - 2017-12-07 03:06:04.531978: step 3330, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.785 sec/batch; 71h:47m:29s remains)
INFO - root - 2017-12-07 03:06:12.534792: step 3340, loss = 0.72, batch loss = 0.65 (10.2 examples/sec; 0.784 sec/batch; 71h:42m:12s remains)
INFO - root - 2017-12-07 03:06:20.464384: step 3350, loss = 0.94, batch loss = 0.87 (10.1 examples/sec; 0.791 sec/batch; 72h:19m:23s remains)
INFO - root - 2017-12-07 03:06:28.502645: step 3360, loss = 0.73, batch loss = 0.66 (10.6 examples/sec; 0.752 sec/batch; 68h:43m:43s remains)
INFO - root - 2017-12-07 03:06:36.622973: step 3370, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.807 sec/batch; 73h:45m:02s remains)
INFO - root - 2017-12-07 03:06:44.696033: step 3380, loss = 0.85, batch loss = 0.78 (8.9 examples/sec; 0.895 sec/batch; 81h:50m:39s remains)
INFO - root - 2017-12-07 03:06:52.675354: step 3390, loss = 0.79, batch loss = 0.72 (9.7 examples/sec; 0.827 sec/batch; 75h:38m:50s remains)
INFO - root - 2017-12-07 03:07:00.803929: step 3400, loss = 0.63, batch loss = 0.56 (10.6 examples/sec; 0.757 sec/batch; 69h:13m:38s remains)
2017-12-07 03:07:01.398156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3503497 -3.7467866 -4.0387573 -4.1194339 -4.1075344 -4.0956435 -3.9587858 -3.7489324 -3.63544 -3.5796654 -3.5226898 -3.4807935 -3.4155526 -3.4260173 -3.28655][-3.4796429 -3.778465 -3.8906565 -3.8382382 -3.7690449 -3.7964659 -3.6868017 -3.3880258 -3.1809311 -3.051343 -2.9290459 -2.9642797 -3.0258384 -3.0727751 -2.8150754][-3.1157241 -3.1930881 -3.1103022 -3.0381188 -3.0567503 -3.3261867 -3.3822756 -2.9610243 -2.5582337 -2.2572441 -2.0585322 -2.2813749 -2.5829375 -2.6789296 -2.2514329][-2.4922221 -2.3599594 -2.225549 -2.2829387 -2.479125 -3.017386 -3.1732492 -2.5281525 -1.8955448 -1.4459524 -1.24259 -1.7197933 -2.2827241 -2.4419298 -1.9171362][-1.7800136 -1.5542293 -1.5288978 -1.7875133 -2.1402435 -2.7795596 -2.8361351 -1.9010611 -1.1445856 -0.76564813 -0.79141235 -1.5523212 -2.2950785 -2.4836276 -1.9496365][-1.1059604 -0.87667537 -0.99940658 -1.3873534 -1.8174911 -2.3852108 -2.1212943 -0.90720868 -0.24985838 -0.25522089 -0.715256 -1.6851416 -2.4477367 -2.6340165 -2.200285][-0.88702655 -0.75349045 -0.91426921 -1.2186124 -1.5605712 -1.8161576 -1.1004596 0.25780249 0.55623722 -0.023836613 -0.93229413 -1.87516 -2.4353619 -2.6017466 -2.3944578][-1.4118986 -1.4288123 -1.4700806 -1.4728587 -1.4766235 -1.1618896 -0.017698288 1.1907334 0.94189596 -0.19039583 -1.3427072 -2.0235198 -2.2407582 -2.4383063 -2.5525618][-2.2452903 -2.3859625 -2.2847264 -1.9948235 -1.5940213 -0.71180081 0.64888239 1.4662428 0.77946329 -0.56254673 -1.61448 -1.9175031 -1.8556468 -2.2079923 -2.67167][-3.0597596 -3.2382331 -3.0494418 -2.6067433 -1.9528036 -0.79406381 0.45738745 0.78310013 -0.066222668 -1.1856325 -1.8807645 -1.9199965 -1.8538525 -2.4679523 -3.0888352][-3.7491934 -3.9287071 -3.7830236 -3.3643343 -2.6953716 -1.6041369 -0.65449166 -0.652899 -1.3789401 -2.0441217 -2.3261061 -2.2590637 -2.3708868 -3.1622133 -3.7176652][-4.0524011 -4.2742343 -4.2845216 -4.0313559 -3.5085182 -2.6764765 -2.0139918 -2.0918758 -2.5648541 -2.8377161 -2.8573875 -2.7782555 -3.0417247 -3.8066149 -4.1468973][-4.0268369 -4.2195134 -4.32404 -4.228539 -3.9119258 -3.3877578 -2.9286985 -2.9591331 -3.2175477 -3.3185258 -3.282805 -3.2523699 -3.5214944 -4.0691338 -4.17587][-3.9132671 -4.000453 -4.0902739 -4.0731649 -3.9149575 -3.6522026 -3.3737612 -3.3836169 -3.5323436 -3.5919387 -3.5771399 -3.578371 -3.7605252 -4.0387788 -4.0010171][-3.6970425 -3.7182317 -3.7898002 -3.8234515 -3.7912192 -3.7070391 -3.5906277 -3.6170034 -3.7044706 -3.7280264 -3.7120681 -3.7045317 -3.7715795 -3.858125 -3.77531]]...]
INFO - root - 2017-12-07 03:07:09.605590: step 3410, loss = 0.69, batch loss = 0.61 (9.8 examples/sec; 0.820 sec/batch; 74h:59m:33s remains)
INFO - root - 2017-12-07 03:07:17.603728: step 3420, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.796 sec/batch; 72h:44m:36s remains)
INFO - root - 2017-12-07 03:07:25.654184: step 3430, loss = 0.75, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 76h:54m:49s remains)
INFO - root - 2017-12-07 03:07:33.693581: step 3440, loss = 0.64, batch loss = 0.57 (10.3 examples/sec; 0.779 sec/batch; 71h:12m:03s remains)
INFO - root - 2017-12-07 03:07:41.699202: step 3450, loss = 0.93, batch loss = 0.85 (10.4 examples/sec; 0.771 sec/batch; 70h:25m:54s remains)
INFO - root - 2017-12-07 03:07:49.736137: step 3460, loss = 1.00, batch loss = 0.92 (10.0 examples/sec; 0.800 sec/batch; 73h:08m:21s remains)
INFO - root - 2017-12-07 03:07:57.772609: step 3470, loss = 0.70, batch loss = 0.63 (10.6 examples/sec; 0.754 sec/batch; 68h:56m:56s remains)
INFO - root - 2017-12-07 03:08:05.841409: step 3480, loss = 0.88, batch loss = 0.81 (9.6 examples/sec; 0.832 sec/batch; 76h:00m:13s remains)
INFO - root - 2017-12-07 03:08:16.740403: step 3490, loss = 0.74, batch loss = 0.66 (9.2 examples/sec; 0.870 sec/batch; 79h:28m:50s remains)
INFO - root - 2017-12-07 03:08:24.805905: step 3500, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 70h:48m:23s remains)
2017-12-07 03:08:25.414984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2813795 -3.9060409 -3.7634811 -2.9893632 -2.8586683 -3.2147245 -3.670871 -3.811156 -3.4197824 -3.0253286 -2.9705334 -3.1820312 -3.3871994 -3.3974781 -2.8425572][-2.6210208 -3.3656828 -3.0751984 -2.0360575 -1.8641582 -2.3614759 -2.9171 -3.1383018 -2.7946682 -2.4049103 -2.3127503 -2.426074 -2.5206251 -2.427361 -1.7834988][-1.7601178 -2.6366892 -2.2196157 -0.98269463 -0.77370644 -1.3690331 -1.9958959 -2.3611221 -2.1959004 -1.8892937 -1.7199149 -1.5729275 -1.4130387 -1.2114143 -0.62973428][-1.3437574 -2.3965635 -2.0165682 -0.74747229 -0.55667686 -1.1761587 -1.771255 -2.2265017 -2.2962639 -2.13552 -1.8524055 -1.4024644 -0.93611312 -0.60324192 -0.13759899][-1.1451037 -2.3765433 -2.2331789 -1.1955342 -1.0788009 -1.615937 -2.0662293 -2.5143032 -2.818958 -2.843848 -2.483891 -1.7902296 -1.070416 -0.62720776 -0.27165842][-0.9880321 -2.2701712 -2.4236999 -1.8188317 -1.7914424 -2.1839447 -2.4615662 -2.8365498 -3.278151 -3.4442282 -3.07793 -2.2524607 -1.3657556 -0.79974341 -0.43132448][-1.0959871 -2.2465191 -2.5599174 -2.329417 -2.3395846 -2.5172162 -2.5681195 -2.7637014 -3.1637869 -3.3360052 -3.042629 -2.3099697 -1.4416821 -0.80712581 -0.35968637][-1.3057027 -2.1917009 -2.4996228 -2.5135007 -2.5715437 -2.5914574 -2.4277663 -2.395998 -2.6022272 -2.6623919 -2.4326482 -1.8803825 -1.2117434 -0.7560935 -0.42408991][-1.1704979 -1.7567232 -1.9528294 -2.0720339 -2.2711203 -2.3515451 -2.1673622 -2.0994139 -2.2189178 -2.1871312 -1.9355106 -1.4267292 -0.88559008 -0.63626981 -0.49948549][-0.93425751 -1.2427723 -1.2804227 -1.4166081 -1.8142879 -2.129447 -2.1347034 -2.1932294 -2.2954743 -2.2230139 -1.9763069 -1.4529016 -0.93376184 -0.7809391 -0.84151673][-1.1465199 -1.28811 -1.1946113 -1.2853181 -1.811491 -2.3237033 -2.5526972 -2.7737231 -2.9338131 -2.9232843 -2.7388546 -2.2020714 -1.6299608 -1.471029 -1.6341984][-1.5122621 -1.6602554 -1.573477 -1.6179194 -2.162411 -2.7600145 -3.1511357 -3.4693115 -3.6327658 -3.6596782 -3.5062037 -3.0153131 -2.4833462 -2.3088596 -2.4927363][-2.0355003 -2.252579 -2.1882303 -2.1523468 -2.5687194 -3.0839224 -3.4930017 -3.7386432 -3.7409441 -3.6420207 -3.4095368 -2.9884827 -2.6201582 -2.5881562 -2.9075966][-2.7239914 -2.9800415 -2.9358377 -2.8600485 -3.1095636 -3.4235144 -3.7102506 -3.7883568 -3.6026793 -3.3596864 -3.0592065 -2.7427831 -2.5470805 -2.6314135 -3.0700374][-2.9480791 -3.1848922 -3.166137 -3.125165 -3.3012352 -3.4742057 -3.6777961 -3.6852579 -3.440691 -3.135407 -2.8257084 -2.6367373 -2.5665345 -2.6715055 -3.0808775]]...]
INFO - root - 2017-12-07 03:08:33.183983: step 3510, loss = 0.80, batch loss = 0.73 (9.4 examples/sec; 0.855 sec/batch; 78h:05m:49s remains)
INFO - root - 2017-12-07 03:08:41.196450: step 3520, loss = 0.73, batch loss = 0.66 (8.9 examples/sec; 0.900 sec/batch; 82h:16m:23s remains)
INFO - root - 2017-12-07 03:08:49.355109: step 3530, loss = 0.71, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 74h:19m:40s remains)
INFO - root - 2017-12-07 03:08:57.440576: step 3540, loss = 0.93, batch loss = 0.86 (9.8 examples/sec; 0.813 sec/batch; 74h:17m:19s remains)
INFO - root - 2017-12-07 03:09:05.508807: step 3550, loss = 0.61, batch loss = 0.53 (10.0 examples/sec; 0.802 sec/batch; 73h:17m:43s remains)
INFO - root - 2017-12-07 03:09:13.538912: step 3560, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.776 sec/batch; 70h:52m:25s remains)
INFO - root - 2017-12-07 03:09:21.669876: step 3570, loss = 0.81, batch loss = 0.73 (9.6 examples/sec; 0.836 sec/batch; 76h:24m:46s remains)
INFO - root - 2017-12-07 03:09:29.726269: step 3580, loss = 0.75, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 78h:12m:10s remains)
INFO - root - 2017-12-07 03:09:37.685018: step 3590, loss = 0.84, batch loss = 0.77 (10.0 examples/sec; 0.797 sec/batch; 72h:49m:03s remains)
INFO - root - 2017-12-07 03:09:45.672039: step 3600, loss = 0.75, batch loss = 0.68 (10.6 examples/sec; 0.756 sec/batch; 69h:03m:12s remains)
2017-12-07 03:09:46.263697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.549468 -1.5269196 -1.6399508 -1.7103391 -1.7525179 -1.9879203 -2.5444508 -3.2735229 -3.8103216 -3.7436323 -3.2964828 -3.0140264 -2.718122 -2.2448559 -2.1160879][-1.4308112 -1.5770321 -1.8324215 -1.9773109 -2.095155 -2.3581271 -2.8507452 -3.4680698 -3.9600775 -3.9390657 -3.5459454 -3.30976 -2.9559925 -2.4408278 -2.2981808][-1.6097462 -1.8872406 -2.2718694 -2.4557314 -2.5876207 -2.7666225 -3.0616519 -3.4756539 -3.8801219 -3.94324 -3.6864338 -3.5999265 -3.3514359 -2.965672 -2.8904924][-2.0110953 -2.2479448 -2.6526511 -2.8621545 -2.9685667 -3.000191 -2.9926877 -3.1245232 -3.4877234 -3.7436051 -3.7339258 -3.8560386 -3.7777143 -3.5333009 -3.4648404][-2.0113375 -2.0575895 -2.4233444 -2.7615075 -2.9102297 -2.8423815 -2.5342751 -2.3714273 -2.7924316 -3.3474169 -3.5881841 -3.8495083 -3.9118998 -3.7889118 -3.65552][-1.6865141 -1.6306403 -2.0205128 -2.525219 -2.6784582 -2.4681454 -1.8360999 -1.3261828 -1.8246164 -2.7569268 -3.2495687 -3.5542445 -3.6933715 -3.677362 -3.5399737][-1.3493831 -1.2616529 -1.7305393 -2.3729012 -2.4379013 -2.0039737 -1.066247 -0.22051668 -0.68777609 -1.9310651 -2.7035801 -3.0428293 -3.2089267 -3.296875 -3.2768888][-1.2021086 -1.1498857 -1.7189355 -2.4531045 -2.4524519 -1.8599176 -0.82280254 0.14231539 -0.13632822 -1.3954866 -2.3028779 -2.6137767 -2.6902289 -2.7806416 -2.8573756][-1.7297347 -1.775563 -2.3460746 -3.019105 -3.0110154 -2.4368875 -1.611619 -0.82259154 -0.81939483 -1.6741495 -2.3948925 -2.5372167 -2.3964314 -2.4012721 -2.531306][-2.2667563 -2.4382234 -2.95322 -3.5117803 -3.6003246 -3.1839869 -2.6702824 -2.1331184 -1.8866999 -2.1903448 -2.5619929 -2.5083671 -2.205802 -2.2140956 -2.4314895][-2.2068028 -2.4404044 -2.891428 -3.3551445 -3.56226 -3.3483491 -3.1264732 -2.8713174 -2.5784345 -2.5022445 -2.6135292 -2.5147486 -2.2352493 -2.3500431 -2.6193409][-1.9298084 -2.1110463 -2.4373689 -2.7785726 -3.0455837 -3.0219996 -3.0450778 -3.0576684 -2.8665495 -2.6152487 -2.5828104 -2.5281506 -2.3556316 -2.5468702 -2.7517362][-1.7260034 -1.7894244 -1.9754984 -2.196265 -2.4615984 -2.550406 -2.6801291 -2.8115542 -2.7049358 -2.3777122 -2.2644901 -2.2510035 -2.1896307 -2.4393804 -2.5896554][-1.7123399 -1.6980648 -1.761409 -1.8538876 -2.0608709 -2.1884177 -2.3133428 -2.4069395 -2.2772009 -1.9090202 -1.7403731 -1.7371898 -1.7663636 -2.0728858 -2.1661119][-1.7997098 -1.7095244 -1.6714361 -1.6802585 -1.8295774 -1.9599359 -2.066052 -2.1099429 -1.959482 -1.641299 -1.4822662 -1.4572425 -1.50875 -1.7796247 -1.7553606]]...]
INFO - root - 2017-12-07 03:09:54.296897: step 3610, loss = 0.90, batch loss = 0.83 (9.7 examples/sec; 0.822 sec/batch; 75h:03m:41s remains)
INFO - root - 2017-12-07 03:10:02.361992: step 3620, loss = 0.55, batch loss = 0.48 (9.5 examples/sec; 0.840 sec/batch; 76h:46m:04s remains)
INFO - root - 2017-12-07 03:10:10.404388: step 3630, loss = 0.64, batch loss = 0.57 (9.2 examples/sec; 0.868 sec/batch; 79h:19m:07s remains)
INFO - root - 2017-12-07 03:10:18.403419: step 3640, loss = 0.62, batch loss = 0.55 (10.3 examples/sec; 0.776 sec/batch; 70h:55m:58s remains)
INFO - root - 2017-12-07 03:10:26.604387: step 3650, loss = 0.98, batch loss = 0.91 (10.1 examples/sec; 0.789 sec/batch; 72h:05m:56s remains)
INFO - root - 2017-12-07 03:10:37.457895: step 3660, loss = 0.86, batch loss = 0.79 (8.9 examples/sec; 0.903 sec/batch; 82h:31m:01s remains)
INFO - root - 2017-12-07 03:10:45.386245: step 3670, loss = 0.72, batch loss = 0.65 (10.8 examples/sec; 0.742 sec/batch; 67h:48m:23s remains)
INFO - root - 2017-12-07 03:10:53.494597: step 3680, loss = 0.61, batch loss = 0.54 (9.5 examples/sec; 0.840 sec/batch; 76h:41m:08s remains)
INFO - root - 2017-12-07 03:11:01.432947: step 3690, loss = 0.82, batch loss = 0.75 (9.0 examples/sec; 0.885 sec/batch; 80h:48m:40s remains)
INFO - root - 2017-12-07 03:11:09.585653: step 3700, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.802 sec/batch; 73h:12m:47s remains)
2017-12-07 03:11:10.168903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4630957 -2.3008914 -2.106349 -2.3391201 -2.6424966 -2.8219666 -2.854569 -2.6177297 -2.4231467 -2.4888945 -2.7443871 -3.0546288 -3.3474936 -3.5365391 -3.7148073][-2.1925085 -2.028163 -1.775991 -2.1220691 -2.5264578 -2.6980724 -2.6748166 -2.1421659 -1.639971 -1.6970227 -2.1206086 -2.5869966 -2.996516 -3.3749762 -3.714309][-2.7066724 -2.5693836 -2.2554569 -2.6171556 -2.9393249 -2.9064336 -2.6612103 -1.7644875 -0.96562862 -1.1099386 -1.740319 -2.3637769 -2.8330188 -3.2951064 -3.7031121][-3.1255808 -3.035151 -2.6985602 -3.0554037 -3.2618227 -3.0251265 -2.5491147 -1.3296092 -0.32638788 -0.6922965 -1.6735432 -2.5726855 -3.1528454 -3.5872989 -3.86851][-3.4296367 -3.3772893 -3.076324 -3.4241295 -3.4855723 -3.0248661 -2.256129 -0.65585542 0.48596764 -0.26057911 -1.7775049 -3.1071 -3.8609176 -4.1780958 -4.1905403][-3.5753975 -3.4395752 -3.1638868 -3.4552405 -3.2545629 -2.4539838 -1.3161895 0.64360571 1.7941728 0.61416578 -1.3951769 -3.1735423 -4.2164598 -4.5289927 -4.4046764][-3.4911439 -3.220845 -2.8955736 -3.0512271 -2.5736485 -1.4758158 -0.027346611 2.2001524 3.3672843 1.9617791 -0.35252237 -2.5545568 -3.9736924 -4.4424829 -4.3628268][-3.1952713 -2.8810306 -2.5454907 -2.607914 -2.103241 -1.0806923 0.44007111 2.7359662 3.9171619 2.6485782 0.44797802 -1.8875053 -3.4677417 -4.0265684 -4.0629144][-2.8042655 -2.485929 -2.1083326 -1.9643869 -1.5106888 -0.93738246 0.070067883 1.794209 2.6740346 1.8600354 0.1901474 -1.8360314 -3.1625819 -3.6243126 -3.7352686][-2.5335541 -2.3216026 -1.89747 -1.4283245 -0.8901403 -0.71850252 -0.35141993 0.57314348 1.014327 0.475502 -0.743922 -2.3425028 -3.2247581 -3.4557877 -3.5104637][-2.6206555 -2.5792482 -2.1808145 -1.4718916 -0.80155396 -0.80375957 -0.86903763 -0.56831932 -0.43144321 -0.85884047 -1.8025575 -3.0131016 -3.5439792 -3.5594945 -3.4705303][-2.9147608 -2.9470959 -2.5842643 -1.8556242 -1.2655325 -1.3727503 -1.6223087 -1.6508982 -1.6055207 -1.9797943 -2.7929945 -3.7401772 -4.0929165 -3.9503989 -3.7015071][-3.1026752 -3.027185 -2.6408515 -2.0272598 -1.7187169 -2.0246327 -2.4067907 -2.5451527 -2.4197474 -2.719698 -3.441617 -4.219141 -4.4876142 -4.2726235 -3.9327769][-3.0434406 -2.7751224 -2.3048766 -1.8088596 -1.7595963 -2.2560062 -2.7492979 -2.8735919 -2.6557717 -2.9649992 -3.6765003 -4.36903 -4.6266813 -4.4202161 -4.0449619][-2.9038539 -2.6051302 -2.2342572 -1.9148118 -2.0356541 -2.6232333 -3.0960417 -3.0506096 -2.715523 -3.0300286 -3.7471144 -4.4093051 -4.67833 -4.498004 -4.1019611]]...]
INFO - root - 2017-12-07 03:11:18.109969: step 3710, loss = 0.57, batch loss = 0.50 (10.5 examples/sec; 0.759 sec/batch; 69h:18m:43s remains)
INFO - root - 2017-12-07 03:11:26.201407: step 3720, loss = 0.80, batch loss = 0.72 (9.9 examples/sec; 0.808 sec/batch; 73h:46m:35s remains)
INFO - root - 2017-12-07 03:11:34.267017: step 3730, loss = 0.62, batch loss = 0.55 (9.4 examples/sec; 0.854 sec/batch; 77h:57m:20s remains)
INFO - root - 2017-12-07 03:11:44.581378: step 3740, loss = 0.77, batch loss = 0.69 (8.0 examples/sec; 1.004 sec/batch; 91h:42m:34s remains)
INFO - root - 2017-12-07 03:11:56.530675: step 3750, loss = 1.05, batch loss = 0.98 (7.0 examples/sec; 1.143 sec/batch; 104h:24m:23s remains)
INFO - root - 2017-12-07 03:12:11.309358: step 3760, loss = 0.76, batch loss = 0.69 (1.9 examples/sec; 4.160 sec/batch; 379h:53m:04s remains)
INFO - root - 2017-12-07 03:12:23.541813: step 3770, loss = 0.79, batch loss = 0.72 (5.7 examples/sec; 1.402 sec/batch; 128h:00m:46s remains)
INFO - root - 2017-12-07 03:12:39.596086: step 3780, loss = 0.72, batch loss = 0.65 (4.9 examples/sec; 1.629 sec/batch; 148h:45m:02s remains)
INFO - root - 2017-12-07 03:12:55.099766: step 3790, loss = 0.64, batch loss = 0.57 (4.9 examples/sec; 1.622 sec/batch; 148h:07m:35s remains)
INFO - root - 2017-12-07 03:13:13.373472: step 3800, loss = 0.67, batch loss = 0.60 (5.4 examples/sec; 1.494 sec/batch; 136h:26m:35s remains)
2017-12-07 03:13:14.481321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.242609 -4.4357657 -4.4137459 -4.2039371 -3.9733779 -3.8354883 -3.7975841 -3.7380819 -3.6281006 -3.5362997 -3.5319452 -3.6450613 -3.7826962 -3.6899793 -3.2620091][-4.633729 -5.0265417 -5.1029277 -4.9473839 -4.7654419 -4.6764622 -4.6315713 -4.4641933 -4.2202806 -4.0467672 -4.0044022 -4.1483736 -4.3535933 -4.1608977 -3.378253][-4.9024849 -5.3546057 -5.4149027 -5.2804346 -5.1553912 -5.1702948 -5.1650724 -4.9642768 -4.7136178 -4.571825 -4.5482187 -4.7531533 -5.0196643 -4.7020903 -3.6050863][-4.5012212 -4.8636088 -4.8306665 -4.6598482 -4.510344 -4.525126 -4.4606853 -4.1406484 -3.8895383 -3.8784757 -4.0310283 -4.4774318 -4.9405842 -4.6056256 -3.35071][-3.8456652 -4.0357089 -3.8343666 -3.5509896 -3.3863025 -3.4844825 -3.4554348 -3.0347946 -2.723949 -2.8034942 -3.179801 -3.9892442 -4.7767124 -4.5244188 -3.1765351][-3.6095581 -3.5974092 -3.1525698 -2.6329079 -2.3190725 -2.3653636 -2.3141463 -1.8725078 -1.7169807 -2.121736 -2.8484643 -3.9894278 -4.9865165 -4.7762709 -3.3462219][-3.701942 -3.6026115 -3.0401192 -2.3034039 -1.5890458 -1.0942848 -0.53589964 0.19400358 0.18307543 -0.8612926 -2.3052337 -3.9816945 -5.2971668 -5.2335477 -3.8152325][-4.2658515 -4.3761144 -3.9920907 -3.2537355 -2.2356625 -1.1358733 0.25926828 1.8970065 2.4525003 1.3468351 -0.53304005 -2.7407773 -4.5467949 -4.8616643 -3.7635283][-4.6497707 -5.01404 -4.9068108 -4.555923 -3.9171851 -3.0423052 -1.619719 0.35119915 1.4889884 1.0668244 -0.30250502 -2.1954494 -3.824522 -4.0695281 -3.1505191][-4.4388628 -4.873332 -4.8945775 -4.8175206 -4.6050653 -4.2013636 -3.3285656 -1.9351044 -1.1098435 -1.246424 -1.9529343 -3.0979545 -3.9805524 -3.7486718 -2.7866936][-3.8123124 -4.1711392 -4.2187066 -4.3024073 -4.3537354 -4.1940851 -3.616102 -2.6797178 -2.2553766 -2.4141972 -2.8111477 -3.4736693 -3.8155091 -3.3607423 -2.5733688][-3.2773154 -3.3274379 -3.1828289 -3.2787139 -3.5787017 -3.7495134 -3.5132077 -2.9916422 -2.8698528 -3.0238452 -3.24256 -3.6157854 -3.6537242 -3.207057 -2.699887][-3.5957441 -3.5245295 -3.2830262 -3.279182 -3.4875877 -3.654021 -3.5806477 -3.4007654 -3.5256824 -3.6747291 -3.7646194 -3.8989127 -3.7365367 -3.3472004 -3.0475516][-3.815315 -3.8507724 -3.8025987 -3.842953 -3.8643432 -3.8292143 -3.7022843 -3.6483011 -3.8485303 -3.983639 -3.98836 -3.9645705 -3.716332 -3.4172764 -3.2617497][-3.8438447 -3.821033 -3.8133197 -3.8716435 -3.8528485 -3.7832174 -3.653899 -3.6424835 -3.8217978 -3.9471288 -3.9576941 -3.910881 -3.7037621 -3.5051486 -3.4170651]]...]
INFO - root - 2017-12-07 03:13:29.970019: step 3810, loss = 0.69, batch loss = 0.62 (5.3 examples/sec; 1.502 sec/batch; 137h:06m:26s remains)
INFO - root - 2017-12-07 03:13:50.028662: step 3820, loss = 0.79, batch loss = 0.72 (4.9 examples/sec; 1.635 sec/batch; 149h:15m:28s remains)
INFO - root - 2017-12-07 03:14:08.290151: step 3830, loss = 0.89, batch loss = 0.81 (5.3 examples/sec; 1.516 sec/batch; 138h:27m:02s remains)
INFO - root - 2017-12-07 03:18:59.171352: step 3840, loss = 0.79, batch loss = 0.72 (5.6 examples/sec; 1.432 sec/batch; 130h:46m:25s remains)
INFO - root - 2017-12-07 03:19:16.196319: step 3850, loss = 0.84, batch loss = 0.77 (4.4 examples/sec; 1.808 sec/batch; 165h:01m:24s remains)
INFO - root - 2017-12-07 03:19:31.460975: step 3860, loss = 0.74, batch loss = 0.66 (4.9 examples/sec; 1.620 sec/batch; 147h:55m:04s remains)
INFO - root - 2017-12-07 03:19:49.539161: step 3870, loss = 0.94, batch loss = 0.87 (5.4 examples/sec; 1.469 sec/batch; 134h:07m:13s remains)
INFO - root - 2017-12-07 03:20:07.902143: step 3880, loss = 0.79, batch loss = 0.71 (2.4 examples/sec; 3.279 sec/batch; 299h:19m:21s remains)
INFO - root - 2017-12-07 03:20:25.828884: step 3890, loss = 0.87, batch loss = 0.79 (7.0 examples/sec; 1.149 sec/batch; 104h:52m:21s remains)
INFO - root - 2017-12-07 03:20:37.391509: step 3900, loss = 0.69, batch loss = 0.61 (7.0 examples/sec; 1.150 sec/batch; 104h:58m:21s remains)
2017-12-07 03:20:38.305334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5448446 -3.7751908 -3.8139317 -3.4587495 -2.8774545 -2.4866607 -2.620801 -3.0976012 -3.5130892 -3.715086 -3.654388 -3.5160196 -3.2885771 -2.6934166 -1.8645556][-3.7889712 -3.9508004 -3.8570428 -3.3194995 -2.5258617 -2.0537055 -2.3027413 -2.9211016 -3.384201 -3.6218436 -3.6746483 -3.6309831 -3.4524314 -2.9063363 -2.0943577][-4.0764389 -4.1312943 -3.9321544 -3.299943 -2.3406522 -1.7551463 -2.0440993 -2.7653222 -3.2956872 -3.5126514 -3.5968204 -3.5833404 -3.3746042 -2.7867594 -1.9387267][-4.1962504 -4.1344438 -3.903615 -3.2808113 -2.2182007 -1.5134118 -1.815695 -2.686563 -3.3525758 -3.5118964 -3.4914787 -3.43159 -3.2208877 -2.6670165 -1.8751311][-4.0285678 -3.872983 -3.6613009 -3.0802708 -1.9173143 -1.041024 -1.312062 -2.4112678 -3.3546925 -3.56738 -3.3722117 -3.1926484 -3.0316558 -2.6079988 -2.0414104][-3.9133441 -3.7113037 -3.5033436 -2.8908787 -1.5559785 -0.40554523 -0.55663061 -1.8541408 -3.1520705 -3.6028826 -3.2933826 -2.95895 -2.8169279 -2.489028 -2.1289849][-3.7871675 -3.5101411 -3.2206731 -2.5142202 -1.0160534 0.41995 0.46179104 -0.93685865 -2.5491621 -3.3511691 -3.0801744 -2.6765306 -2.5594602 -2.244009 -1.9207497][-3.1881123 -2.7591429 -2.3627515 -1.6836765 -0.27788925 1.2228074 1.4254489 0.049815178 -1.6956639 -2.7574086 -2.6169498 -2.229568 -2.1436374 -1.8327839 -1.4676659][-2.5633905 -2.1001706 -1.7292972 -1.2363522 -0.12321758 1.2889552 1.6813822 0.52687359 -1.0983541 -2.1881177 -2.1521981 -1.7863426 -1.6805348 -1.3653209 -0.97439623][-2.634011 -2.3600676 -2.1726789 -1.9304676 -1.1663435 0.0451746 0.55543327 -0.28205538 -1.5865607 -2.5050287 -2.4699068 -2.0437365 -1.8140976 -1.4213533 -0.97794533][-2.9558647 -2.7573848 -2.6164351 -2.4615431 -1.9786432 -1.0626955 -0.60472465 -1.1766281 -2.1079445 -2.7654953 -2.6731448 -2.2129693 -1.9461994 -1.5761971 -1.193305][-2.9065719 -2.6270931 -2.4253869 -2.2611246 -1.9778581 -1.3816419 -1.0585046 -1.3864598 -1.9349298 -2.3377721 -2.2563872 -1.9357502 -1.805949 -1.5851877 -1.3543293][-2.4830186 -2.1843457 -1.9670916 -1.8062708 -1.685746 -1.3924594 -1.2056761 -1.309473 -1.5357146 -1.7550452 -1.7742169 -1.7540452 -1.8791218 -1.8188868 -1.6677494][-1.8664269 -1.7007987 -1.5705602 -1.4391329 -1.3772616 -1.2457433 -1.1046939 -1.0380299 -1.0533679 -1.1339045 -1.2328026 -1.4921935 -1.8922224 -2.0329752 -1.9476709][-1.266901 -1.14129 -1.1088142 -1.0501637 -1.0173855 -0.94515252 -0.78132629 -0.56751156 -0.40353203 -0.28801346 -0.35095119 -0.78252411 -1.4881861 -1.9850602 -2.1467643]]...]
INFO - root - 2017-12-07 03:20:49.946223: step 3910, loss = 0.92, batch loss = 0.85 (7.2 examples/sec; 1.119 sec/batch; 102h:05m:48s remains)
INFO - root - 2017-12-07 03:21:01.696977: step 3920, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.181 sec/batch; 107h:49m:24s remains)
INFO - root - 2017-12-07 03:21:13.304460: step 3930, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 1.138 sec/batch; 103h:51m:15s remains)
INFO - root - 2017-12-07 03:21:25.179766: step 3940, loss = 0.85, batch loss = 0.78 (6.5 examples/sec; 1.226 sec/batch; 111h:55m:01s remains)
INFO - root - 2017-12-07 03:21:36.881114: step 3950, loss = 1.00, batch loss = 0.93 (6.5 examples/sec; 1.238 sec/batch; 112h:56m:27s remains)
INFO - root - 2017-12-07 03:21:48.713126: step 3960, loss = 0.63, batch loss = 0.55 (6.6 examples/sec; 1.209 sec/batch; 110h:19m:25s remains)
INFO - root - 2017-12-07 03:22:00.292707: step 3970, loss = 0.82, batch loss = 0.74 (8.4 examples/sec; 0.958 sec/batch; 87h:22m:53s remains)
INFO - root - 2017-12-07 03:22:12.030189: step 3980, loss = 0.92, batch loss = 0.85 (7.0 examples/sec; 1.143 sec/batch; 104h:19m:03s remains)
INFO - root - 2017-12-07 03:22:23.658393: step 3990, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 1.119 sec/batch; 102h:07m:08s remains)
INFO - root - 2017-12-07 03:22:33.241078: step 4000, loss = 0.95, batch loss = 0.88 (9.6 examples/sec; 0.834 sec/batch; 76h:05m:22s remains)
2017-12-07 03:22:33.863521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7339764 -2.0046644 -2.2861497 -2.5935845 -2.7749691 -2.1742058 -1.30986 -0.56427717 -0.14894867 -0.06088829 0.10750961 0.18007278 0.026149273 0.19446802 0.38485146][-1.463542 -1.8677413 -2.4617777 -2.9069605 -3.0389414 -2.2405365 -1.0190198 -0.14031172 0.05670929 -0.24806118 -0.17189646 0.19763851 0.2665596 0.36005545 0.32706833][-1.3662658 -1.8257627 -2.6007395 -3.1504447 -3.2023838 -2.2264307 -0.67533278 0.38823509 0.42126179 -0.30655766 -0.50667238 0.055175781 0.4371314 0.63581848 0.43933296][-1.3109694 -1.7494199 -2.56191 -3.1930075 -3.1662288 -2.0620222 -0.25952768 1.0360026 0.96530724 -0.20255423 -0.829813 -0.21991873 0.5248251 0.9912 0.75439215][-1.3995335 -1.8346388 -2.6139827 -3.2284808 -3.0627515 -1.8404851 0.13446712 1.7253842 1.6250601 0.081525326 -0.9842813 -0.48438573 0.5340004 1.3241291 1.1281753][-1.6310437 -2.1009555 -2.8042412 -3.2460055 -2.7853465 -1.3587244 0.75843334 2.6033692 2.4653821 0.58296585 -0.87279868 -0.54496884 0.62697363 1.6571059 1.4709883][-1.7869055 -2.2979958 -2.8511634 -2.9594038 -2.1237228 -0.42243671 1.768105 3.6353798 3.2883964 1.1248016 -0.56597567 -0.43525982 0.64310837 1.6895113 1.4325957][-1.6583395 -2.2812731 -2.7061565 -2.4640365 -1.2850838 0.62981033 2.7489452 4.3597383 3.7143421 1.401505 -0.37058115 -0.524389 0.17970562 0.963954 0.60421324][-1.7280662 -2.4760184 -2.8559308 -2.3238204 -0.84670162 1.1853232 3.1202211 4.3275509 3.594286 1.3772945 -0.37237978 -0.81535292 -0.61770129 -0.26311874 -0.73695922][-2.2028737 -2.9171782 -3.2010717 -2.4310744 -0.75492549 1.2971182 3.0357041 3.8870783 3.1342373 1.0747342 -0.649734 -1.3022225 -1.4637358 -1.4599113 -2.0048189][-2.7017517 -3.2491903 -3.4066927 -2.4702764 -0.67469406 1.3531137 2.8762784 3.3860769 2.5032477 0.48309469 -1.2261281 -1.9783661 -2.2729979 -2.4360914 -2.9287252][-2.9529891 -3.3298225 -3.3681426 -2.4118505 -0.6694746 1.2573338 2.5800247 2.7750726 1.6871777 -0.32576466 -1.9752493 -2.7017109 -2.9558811 -3.1429007 -3.489789][-2.9020314 -3.1914859 -3.2040987 -2.3787949 -0.81637287 0.93549252 2.007782 1.8775048 0.58850574 -1.3111577 -2.7255924 -3.2664568 -3.365994 -3.4719887 -3.6700382][-2.8162599 -3.0528898 -3.0785623 -2.4295328 -1.0927291 0.43124676 1.1930695 0.73116875 -0.6542902 -2.2645535 -3.2842302 -3.5750248 -3.5178747 -3.5150652 -3.5831013][-2.7483284 -2.9515471 -2.9982917 -2.5290928 -1.4591923 -0.23371506 0.22150564 -0.45693421 -1.7915857 -3.0363579 -3.6399679 -3.714323 -3.547905 -3.4381936 -3.3882074]]...]
INFO - root - 2017-12-07 03:22:41.978554: step 4010, loss = 0.72, batch loss = 0.65 (10.4 examples/sec; 0.770 sec/batch; 70h:13m:29s remains)
INFO - root - 2017-12-07 03:22:50.136831: step 4020, loss = 0.79, batch loss = 0.71 (10.0 examples/sec; 0.802 sec/batch; 73h:11m:46s remains)
INFO - root - 2017-12-07 03:22:58.494832: step 4030, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 75h:14m:41s remains)
INFO - root - 2017-12-07 03:23:06.764114: step 4040, loss = 0.55, batch loss = 0.48 (9.4 examples/sec; 0.852 sec/batch; 77h:45m:37s remains)
INFO - root - 2017-12-07 03:23:15.031039: step 4050, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.773 sec/batch; 70h:31m:59s remains)
INFO - root - 2017-12-07 03:23:23.245324: step 4060, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 73h:14m:52s remains)
INFO - root - 2017-12-07 03:23:31.371648: step 4070, loss = 0.85, batch loss = 0.78 (9.5 examples/sec; 0.840 sec/batch; 76h:37m:47s remains)
INFO - root - 2017-12-07 03:23:39.474747: step 4080, loss = 0.72, batch loss = 0.64 (10.2 examples/sec; 0.782 sec/batch; 71h:18m:19s remains)
INFO - root - 2017-12-07 03:23:47.670491: step 4090, loss = 0.61, batch loss = 0.53 (9.4 examples/sec; 0.854 sec/batch; 77h:53m:32s remains)
INFO - root - 2017-12-07 03:23:55.875349: step 4100, loss = 0.91, batch loss = 0.84 (9.4 examples/sec; 0.852 sec/batch; 77h:42m:24s remains)
2017-12-07 03:23:56.530242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4587035 -3.4195814 -3.4414678 -3.5027254 -3.6177342 -3.6823659 -3.5792975 -3.5280595 -3.6226816 -3.7803526 -3.8688507 -3.7976089 -3.5814397 -3.4466937 -3.5114689][-3.1523647 -3.089684 -3.1046071 -3.1722507 -3.3160129 -3.3828068 -3.253716 -3.1935844 -3.3004646 -3.4837365 -3.5349593 -3.3944821 -3.1150513 -3.0637798 -3.2959154][-2.6291552 -2.4656355 -2.3770926 -2.3666589 -2.4528942 -2.4682441 -2.3395305 -2.3181117 -2.4575477 -2.6836562 -2.7351542 -2.6249104 -2.3717651 -2.4164274 -2.7546585][-1.8201451 -1.4798388 -1.2537262 -1.1752262 -1.2082176 -1.2551234 -1.2531605 -1.3309846 -1.5326312 -1.8155224 -1.8613708 -1.7910864 -1.6165524 -1.7284713 -2.0949292][-0.9151113 -0.43247771 -0.15331459 -0.055810928 -0.012708187 -0.03730011 -0.073310852 -0.15714264 -0.41579628 -0.75543594 -0.7630446 -0.75211811 -0.78248739 -1.0067477 -1.3754885][-0.21554804 0.3276186 0.57327223 0.68346071 0.83997536 0.93610334 1.0202823 0.99716568 0.55182314 0.00939846 -0.12857246 -0.32091904 -0.61378551 -0.84638095 -1.0320489][-0.12053537 0.4904623 0.77846766 1.0617628 1.4763727 1.8461533 2.2278142 2.299789 1.6052685 0.79110193 0.37269878 -0.14342785 -0.70708561 -0.91764522 -0.8924818][-0.631279 -0.017360687 0.40932894 0.98752546 1.7249503 2.395227 3.0141573 3.1004982 2.1350722 1.1033025 0.48438454 -0.18340731 -0.79412055 -0.94430685 -0.87225556][-1.3906753 -0.86251187 -0.3352375 0.41313219 1.2621875 2.03796 2.6687269 2.6834459 1.7101612 0.74074459 0.13477612 -0.48484397 -0.99454045 -1.0960629 -1.0776536][-2.285176 -1.9198854 -1.2935259 -0.43262386 0.34943533 1.012455 1.4565821 1.3813868 0.63376856 0.019423485 -0.26606083 -0.65330982 -0.96427941 -1.014823 -1.2094042][-2.7958028 -2.5991051 -1.9438255 -1.0800087 -0.51869035 -0.12958956 0.0047125816 -0.18857145 -0.709213 -0.9817555 -0.911278 -0.94841981 -0.8969059 -0.83745527 -1.2903533][-2.7097492 -2.5988708 -2.0334635 -1.3686149 -1.1765008 -1.1644759 -1.3309445 -1.6057067 -1.9038529 -1.910198 -1.5997107 -1.3628435 -1.0307603 -0.974787 -1.6763415][-2.1607406 -2.0649109 -1.6047895 -1.2540739 -1.4621687 -1.7627604 -2.1194315 -2.443258 -2.6416812 -2.5492721 -2.1356256 -1.6588416 -1.1005085 -1.0604434 -1.8449752][-1.4093132 -1.2975276 -0.94015455 -0.85582495 -1.2999163 -1.6980705 -2.0344865 -2.3547258 -2.5954595 -2.6228981 -2.3492746 -1.8974996 -1.3028932 -1.2250769 -1.9070356][-0.53508115 -0.37843609 -0.17681789 -0.36704969 -0.95503783 -1.3492239 -1.617281 -1.8621502 -2.0292156 -2.1137786 -2.010987 -1.6773093 -1.1139884 -0.96143961 -1.4951932]]...]
INFO - root - 2017-12-07 03:24:04.783901: step 4110, loss = 0.80, batch loss = 0.73 (9.6 examples/sec; 0.837 sec/batch; 76h:22m:35s remains)
INFO - root - 2017-12-07 03:24:12.979832: step 4120, loss = 0.85, batch loss = 0.78 (10.0 examples/sec; 0.800 sec/batch; 72h:57m:16s remains)
INFO - root - 2017-12-07 03:24:21.308670: step 4130, loss = 0.79, batch loss = 0.72 (9.4 examples/sec; 0.848 sec/batch; 77h:18m:30s remains)
INFO - root - 2017-12-07 03:24:29.308512: step 4140, loss = 0.74, batch loss = 0.67 (10.1 examples/sec; 0.790 sec/batch; 72h:04m:34s remains)
INFO - root - 2017-12-07 03:24:37.608191: step 4150, loss = 0.64, batch loss = 0.57 (9.1 examples/sec; 0.878 sec/batch; 80h:03m:17s remains)
INFO - root - 2017-12-07 03:24:45.997143: step 4160, loss = 1.01, batch loss = 0.94 (9.1 examples/sec; 0.881 sec/batch; 80h:19m:06s remains)
INFO - root - 2017-12-07 03:24:54.303744: step 4170, loss = 1.21, batch loss = 1.14 (9.6 examples/sec; 0.834 sec/batch; 76h:04m:14s remains)
INFO - root - 2017-12-07 03:25:02.621769: step 4180, loss = 0.90, batch loss = 0.83 (9.4 examples/sec; 0.852 sec/batch; 77h:42m:10s remains)
INFO - root - 2017-12-07 03:25:10.976782: step 4190, loss = 1.07, batch loss = 1.00 (9.8 examples/sec; 0.818 sec/batch; 74h:34m:58s remains)
INFO - root - 2017-12-07 03:25:19.357804: step 4200, loss = 0.74, batch loss = 0.66 (9.9 examples/sec; 0.810 sec/batch; 73h:53m:31s remains)
2017-12-07 03:25:19.997125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.302398 -2.2103639 -2.2263539 -2.3329427 -2.4565096 -2.552937 -2.6184754 -2.6684129 -2.71377 -2.6949182 -2.5968156 -2.5197246 -2.5631838 -2.7127638 -2.905206][-2.4617007 -2.4265144 -2.4705117 -2.5256295 -2.5171115 -2.461854 -2.4051716 -2.3808486 -2.3829858 -2.3291974 -2.1783776 -2.063314 -2.1097088 -2.3257477 -2.6514738][-2.8768115 -2.8974972 -2.9073019 -2.8455212 -2.6665854 -2.4433362 -2.2780282 -2.2174368 -2.2406509 -2.2347665 -2.1222923 -2.0263836 -2.0749066 -2.2909827 -2.6326041][-3.0254502 -3.015754 -2.9353294 -2.7645583 -2.4875932 -2.2103286 -2.0585594 -2.0757565 -2.2137654 -2.3347595 -2.3437891 -2.3258069 -2.3780344 -2.5201113 -2.732569][-2.812525 -2.7230427 -2.5675933 -2.344264 -2.0594623 -1.8480363 -1.8326867 -2.0165784 -2.2642274 -2.4496295 -2.5275722 -2.5483267 -2.5764737 -2.6208572 -2.663645][-2.6650648 -2.5325751 -2.3483827 -2.1127937 -1.8470802 -1.7221746 -1.8476825 -2.1580138 -2.4052904 -2.5187831 -2.5495834 -2.5449052 -2.5286694 -2.5017834 -2.4496369][-2.7095027 -2.5941854 -2.3955364 -2.1182756 -1.8419423 -1.7731652 -1.9747639 -2.3076429 -2.4840162 -2.4858756 -2.460494 -2.4606862 -2.4663014 -2.4584451 -2.4282413][-2.7473533 -2.7227445 -2.5350657 -2.2164719 -1.9141638 -1.8524215 -2.0255568 -2.2692528 -2.3750834 -2.3584359 -2.3961449 -2.523509 -2.6702619 -2.7610183 -2.7933078][-2.5991254 -2.6669178 -2.5518997 -2.2836766 -2.0242882 -1.9819963 -2.1046104 -2.2403703 -2.2944498 -2.2825997 -2.3528719 -2.5658293 -2.8178582 -2.9653759 -3.0230923][-2.4517319 -2.5348625 -2.4955907 -2.3211858 -2.1633904 -2.1997311 -2.3460112 -2.437 -2.4221005 -2.3108351 -2.2279744 -2.3031378 -2.4731722 -2.5915613 -2.6858883][-2.5182052 -2.5634789 -2.527338 -2.4090002 -2.3340032 -2.4266589 -2.5927911 -2.6482611 -2.5471151 -2.3068626 -2.0328248 -1.9028277 -1.9299119 -2.0213416 -2.1924813][-2.7790926 -2.7759569 -2.7066612 -2.6043549 -2.5482678 -2.6097977 -2.7193937 -2.7207422 -2.5840178 -2.3385103 -2.0418606 -1.8520441 -1.8345339 -1.9347498 -2.1346269][-2.9520755 -2.9392881 -2.8665869 -2.7847152 -2.7169685 -2.7032995 -2.7329166 -2.7111859 -2.6287427 -2.499783 -2.3209136 -2.1848421 -2.1647806 -2.2291439 -2.3389318][-2.7701306 -2.8028393 -2.7831247 -2.7479239 -2.6943903 -2.6511354 -2.6463861 -2.6393418 -2.6362677 -2.6282735 -2.5669236 -2.4904103 -2.4721451 -2.4773481 -2.4567409][-2.6477075 -2.692281 -2.6870196 -2.6591475 -2.6039672 -2.5449157 -2.5188904 -2.5132384 -2.5410652 -2.596004 -2.6053152 -2.5766811 -2.5867128 -2.5633831 -2.4232895]]...]
INFO - root - 2017-12-07 03:25:28.331005: step 4210, loss = 0.86, batch loss = 0.79 (9.4 examples/sec; 0.855 sec/batch; 77h:57m:37s remains)
INFO - root - 2017-12-07 03:25:36.844878: step 4220, loss = 0.94, batch loss = 0.87 (9.0 examples/sec; 0.887 sec/batch; 80h:53m:07s remains)
INFO - root - 2017-12-07 03:25:45.196423: step 4230, loss = 0.63, batch loss = 0.56 (10.0 examples/sec; 0.798 sec/batch; 72h:47m:39s remains)
INFO - root - 2017-12-07 03:25:53.434556: step 4240, loss = 0.65, batch loss = 0.58 (9.8 examples/sec; 0.815 sec/batch; 74h:18m:17s remains)
INFO - root - 2017-12-07 03:26:01.689327: step 4250, loss = 0.81, batch loss = 0.73 (9.8 examples/sec; 0.814 sec/batch; 74h:12m:05s remains)
INFO - root - 2017-12-07 03:26:09.973578: step 4260, loss = 0.79, batch loss = 0.72 (9.7 examples/sec; 0.822 sec/batch; 74h:59m:30s remains)
INFO - root - 2017-12-07 03:26:18.313055: step 4270, loss = 0.65, batch loss = 0.58 (9.3 examples/sec; 0.858 sec/batch; 78h:16m:11s remains)
INFO - root - 2017-12-07 03:26:26.585061: step 4280, loss = 0.69, batch loss = 0.61 (9.3 examples/sec; 0.862 sec/batch; 78h:36m:18s remains)
INFO - root - 2017-12-07 03:26:34.858851: step 4290, loss = 0.68, batch loss = 0.61 (10.0 examples/sec; 0.800 sec/batch; 72h:53m:59s remains)
INFO - root - 2017-12-07 03:26:43.246291: step 4300, loss = 0.70, batch loss = 0.62 (9.9 examples/sec; 0.812 sec/batch; 73h:59m:36s remains)
2017-12-07 03:26:43.850609: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.757247 0.68159485 0.65814543 0.68963623 0.78810358 0.9647913 1.1269236 1.1663656 1.1155643 0.97127867 0.82966948 0.747118 0.70125532 0.70698404 0.71978045][0.922493 0.85121393 0.82905006 0.827621 0.91617489 1.1888494 1.4355145 1.4415922 1.3245516 1.1099949 0.92166758 0.83847141 0.80658531 0.82941961 0.84027767][1.096417 1.0492764 1.0574169 1.0482106 1.1318936 1.4770212 1.7499976 1.6699624 1.4708571 1.2134914 0.9963789 0.909997 0.889657 0.93529797 0.96495676][1.276896 1.2534456 1.2967086 1.3029795 1.38731 1.7430019 1.9920831 1.8396788 1.5999694 1.3532152 1.1295905 1.0054512 0.9635849 1.0170293 1.0717764][1.4426284 1.470192 1.5582194 1.5841231 1.6378365 1.9034324 2.0838275 1.9009776 1.6505642 1.4468212 1.2455468 1.0973887 1.0438442 1.0946341 1.157052][1.5262666 1.6301122 1.7683234 1.8227892 1.8680038 2.0424776 2.1641474 1.9624972 1.6506033 1.4215693 1.2140155 1.0710521 1.0804543 1.1869926 1.2795496][1.5585575 1.723331 1.875782 1.936758 1.9959455 2.138463 2.263762 2.067956 1.657115 1.3344812 1.0819373 0.96198034 1.0760608 1.2665429 1.4190855][1.5858912 1.7807722 1.8858204 1.888998 1.9339442 2.0763516 2.2579861 2.1270905 1.6888437 1.315196 1.0171084 0.88271904 1.0354958 1.2541986 1.4516692][1.6773214 1.8673906 1.8885145 1.8016739 1.7801819 1.8631172 2.0164962 1.9164009 1.5448437 1.2597976 1.0187864 0.90629721 1.0493984 1.2188935 1.384583][1.783946 1.9375672 1.8869667 1.7453842 1.6734581 1.6772294 1.7195077 1.5405135 1.2017159 1.0298095 0.93081713 0.93638945 1.101244 1.2316151 1.3546591][1.8368816 1.9260612 1.823205 1.6700215 1.5991259 1.5690641 1.5229325 1.2855506 0.97994947 0.87791395 0.85666847 0.91754293 1.0622211 1.1686039 1.3127341][1.8906522 1.9156847 1.7799621 1.5840659 1.4561009 1.3552275 1.2631817 1.1147704 0.975821 0.99615431 1.0248737 1.0441771 1.0752811 1.0984573 1.2343111][1.8942041 1.9004879 1.7887039 1.5616479 1.364706 1.1695623 1.0193901 0.94594574 0.91856432 1.023962 1.1223488 1.147368 1.1400414 1.132277 1.2405972][1.9004006 1.8850164 1.8086882 1.5906572 1.4010296 1.1954522 1.0214491 0.96604156 0.88694859 0.93240118 1.0401678 1.0909657 1.1334314 1.1953168 1.3055][1.9673963 1.9116912 1.8419247 1.6391931 1.5036445 1.34905 1.2046957 1.1794257 1.0159883 0.96292686 1.0478268 1.082531 1.1367846 1.2459874 1.3523507]]...]
INFO - root - 2017-12-07 03:26:52.287142: step 4310, loss = 1.01, batch loss = 0.94 (9.2 examples/sec; 0.871 sec/batch; 79h:23m:38s remains)
INFO - root - 2017-12-07 03:27:00.717572: step 4320, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.860 sec/batch; 78h:25m:28s remains)
INFO - root - 2017-12-07 03:27:09.019943: step 4330, loss = 0.68, batch loss = 0.61 (10.5 examples/sec; 0.765 sec/batch; 69h:46m:35s remains)
INFO - root - 2017-12-07 03:27:17.317889: step 4340, loss = 0.73, batch loss = 0.65 (9.4 examples/sec; 0.851 sec/batch; 77h:33m:47s remains)
INFO - root - 2017-12-07 03:27:25.599713: step 4350, loss = 0.78, batch loss = 0.71 (9.9 examples/sec; 0.810 sec/batch; 73h:47m:28s remains)
INFO - root - 2017-12-07 03:27:33.911752: step 4360, loss = 0.98, batch loss = 0.91 (9.5 examples/sec; 0.842 sec/batch; 76h:43m:07s remains)
INFO - root - 2017-12-07 03:27:42.211229: step 4370, loss = 0.77, batch loss = 0.70 (9.6 examples/sec; 0.829 sec/batch; 75h:34m:37s remains)
INFO - root - 2017-12-07 03:27:50.472830: step 4380, loss = 0.77, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 79h:24m:17s remains)
INFO - root - 2017-12-07 03:27:58.799423: step 4390, loss = 0.73, batch loss = 0.66 (10.0 examples/sec; 0.800 sec/batch; 72h:53m:03s remains)
INFO - root - 2017-12-07 03:28:07.034096: step 4400, loss = 0.63, batch loss = 0.56 (9.2 examples/sec; 0.873 sec/batch; 79h:33m:49s remains)
2017-12-07 03:28:07.670983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5420725 -1.7408733 -2.0200799 -2.1582336 -1.9869006 -1.7029192 -1.4765871 -1.7264404 -2.1303349 -2.2698524 -2.4023273 -2.4413514 -2.4107571 -2.6073246 -2.768574][-1.9324026 -2.0156536 -2.2107666 -2.2473319 -2.0278163 -1.7339923 -1.5502336 -1.8222108 -2.1640832 -2.2536345 -2.3849232 -2.4764173 -2.4225018 -2.5367012 -2.583482][-1.9398963 -1.8445499 -1.9413347 -1.9571464 -1.8194075 -1.535692 -1.314131 -1.5178504 -1.813653 -1.9298122 -2.119838 -2.2298295 -2.0777755 -2.0480685 -1.9477694][-1.5012393 -1.3434663 -1.4404612 -1.5652988 -1.6152141 -1.4224873 -1.1794615 -1.2063007 -1.3018663 -1.2875504 -1.4843876 -1.661139 -1.5635052 -1.5342679 -1.377414][-0.99692655 -0.97243595 -1.113008 -1.2835712 -1.4503264 -1.3019123 -1.0979598 -1.001543 -0.931551 -0.76997375 -0.84850287 -1.0204928 -1.0658231 -1.1488931 -1.0484176][-0.44564319 -0.59588027 -0.68752146 -0.84562325 -1.1015115 -0.98284388 -0.76049447 -0.52969694 -0.30436897 -0.15692616 -0.21040154 -0.37212515 -0.48761845 -0.59434628 -0.53565311][0.029860973 -0.30456495 -0.35448027 -0.46472311 -0.6356945 -0.41459322 -0.041154385 0.40234756 0.78576136 0.77535009 0.54302835 0.29707718 0.17988253 0.19876194 0.30211306][0.1792388 -0.24424219 -0.30087805 -0.34910822 -0.31633568 0.091031551 0.65495396 1.3310742 1.8405643 1.6748309 1.2651691 0.9333992 0.80721855 0.91971731 1.0311451][-0.12954664 -0.35068512 -0.31303644 -0.3333683 -0.25242996 0.14972067 0.71253967 1.3933434 1.8804917 1.753118 1.4195065 1.1415596 1.0199108 1.10602 1.1302071][-0.59516 -0.58460855 -0.47285867 -0.50898361 -0.48344755 -0.23067045 0.1321454 0.59133768 0.93005562 0.95969677 0.85023451 0.71438837 0.68908024 0.727685 0.67581987][-0.9372797 -0.83847094 -0.74505138 -0.78672242 -0.76802373 -0.599741 -0.40660763 -0.22505856 -0.10970402 -0.035609722 -0.051046371 -0.10399199 -0.015545845 0.029066086 -0.0088243484][-1.3235528 -1.2167201 -1.1277208 -1.1091642 -1.0469759 -0.9251492 -0.854089 -0.86689138 -0.90417433 -0.84939957 -0.90068865 -0.98323274 -0.86671948 -0.805022 -0.77514982][-1.7328923 -1.6425002 -1.5358505 -1.4555368 -1.3903825 -1.3474734 -1.356848 -1.4213781 -1.4891829 -1.467499 -1.5684958 -1.6875267 -1.6162159 -1.5738473 -1.4996672][-2.136935 -2.0674284 -1.9958446 -1.9293709 -1.888952 -1.8796904 -1.8879113 -1.9246809 -1.975004 -1.9972467 -2.0937123 -2.1534212 -2.0608175 -1.9759209 -1.8746741][-2.4350095 -2.3791251 -2.3447986 -2.3273895 -2.3239729 -2.3313985 -2.3328204 -2.3525624 -2.3918626 -2.4213514 -2.4539058 -2.4295206 -2.312932 -2.1663444 -1.9939446]]...]
INFO - root - 2017-12-07 03:28:16.036923: step 4410, loss = 0.82, batch loss = 0.74 (9.4 examples/sec; 0.853 sec/batch; 77h:44m:09s remains)
INFO - root - 2017-12-07 03:28:24.288317: step 4420, loss = 0.67, batch loss = 0.60 (8.8 examples/sec; 0.906 sec/batch; 82h:32m:04s remains)
INFO - root - 2017-12-07 03:28:32.617920: step 4430, loss = 0.67, batch loss = 0.59 (9.2 examples/sec; 0.872 sec/batch; 79h:29m:51s remains)
INFO - root - 2017-12-07 03:28:41.015069: step 4440, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.809 sec/batch; 73h:41m:22s remains)
INFO - root - 2017-12-07 03:28:48.984371: step 4450, loss = 0.68, batch loss = 0.61 (13.1 examples/sec; 0.611 sec/batch; 55h:40m:33s remains)
INFO - root - 2017-12-07 03:28:57.240200: step 4460, loss = 0.71, batch loss = 0.63 (9.2 examples/sec; 0.866 sec/batch; 78h:56m:58s remains)
INFO - root - 2017-12-07 03:29:05.596757: step 4470, loss = 0.77, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 76h:25m:58s remains)
INFO - root - 2017-12-07 03:29:13.798532: step 4480, loss = 0.67, batch loss = 0.59 (9.8 examples/sec; 0.816 sec/batch; 74h:23m:15s remains)
INFO - root - 2017-12-07 03:29:22.057031: step 4490, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.805 sec/batch; 73h:19m:55s remains)
INFO - root - 2017-12-07 03:29:30.323142: step 4500, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.796 sec/batch; 72h:30m:01s remains)
2017-12-07 03:29:30.979768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8525057 -2.0149541 -1.830606 -2.3698373 -2.9057364 -2.9063387 -2.4824753 -2.1065907 -2.0665939 -2.2599816 -2.6736057 -2.937145 -2.6596828 -2.2252939 -2.1404757][-2.6009805 -1.639827 -1.4786146 -2.1818254 -2.8503838 -2.8016868 -2.1192679 -1.497715 -1.4919505 -1.9461763 -2.6048493 -2.8716354 -2.3876255 -1.7192388 -1.5516653][-2.5806427 -1.4780712 -1.2840891 -2.0921054 -2.8911719 -2.8797462 -1.9764338 -1.0654562 -1.1474056 -1.9683971 -2.8619385 -3.1073589 -2.4889693 -1.6343665 -1.3484766][-2.5112503 -1.3193743 -1.1068883 -1.9567053 -2.8471706 -2.8222756 -1.6113279 -0.48642755 -0.91418028 -2.2294724 -3.2304842 -3.3487988 -2.6727786 -1.7617695 -1.3961065][-2.2937367 -1.0843318 -0.873446 -1.7167962 -2.6180282 -2.4506598 -0.75337934 0.52183294 -0.54839277 -2.537432 -3.5840976 -3.4566517 -2.7417831 -1.8639522 -1.4492958][-2.07981 -0.91997552 -0.69332409 -1.4176805 -2.2114968 -1.8239505 0.49953508 1.9287791 0.094291687 -2.6336591 -3.7231355 -3.3418343 -2.6791153 -1.9438162 -1.5294015][-1.7296116 -0.76009274 -0.62142992 -1.1770799 -1.7710271 -1.0859923 1.9019504 3.4689002 0.91970778 -2.5023289 -3.6978848 -3.1080236 -2.5217633 -1.9429021 -1.5505593][-1.4239416 -0.76716781 -0.88890052 -1.3975916 -1.935065 -1.1703794 2.1854472 3.9426918 1.2452321 -2.3489652 -3.5491338 -2.816206 -2.2490675 -1.7070079 -1.2687895][-1.1272733 -0.753422 -1.1675305 -1.8578594 -2.7004223 -2.3030272 0.77190542 2.5874267 0.54603577 -2.3824005 -3.2622511 -2.3845804 -1.8463092 -1.3694091 -0.96918726][-0.74757051 -0.4696734 -0.97248745 -1.7999816 -2.8810678 -2.9408598 -0.61628628 0.91122866 -0.41118288 -2.4466023 -2.8864651 -1.917994 -1.4650266 -1.1394391 -0.88781166][-0.5251224 -0.13869 -0.46894598 -1.2391543 -2.3816326 -2.8510985 -1.522294 -0.56640124 -1.339421 -2.5245125 -2.5644326 -1.6962676 -1.4512043 -1.3139977 -1.2412443][-0.4568181 0.11482668 0.080638409 -0.45363474 -1.5042908 -2.2635808 -1.9428415 -1.7187903 -2.227536 -2.6902251 -2.3232284 -1.6210763 -1.6050808 -1.6388972 -1.7553413][-0.706903 0.041604519 0.32544994 0.18687916 -0.53540921 -1.3326633 -1.67642 -2.01747 -2.416213 -2.4010255 -1.826926 -1.3691459 -1.573936 -1.7598634 -2.0377622][-1.4217043 -0.67508721 -0.19019651 0.076590538 -0.19788313 -0.78935623 -1.3244739 -1.8365471 -2.08791 -1.8062532 -1.2607415 -1.1563709 -1.5928102 -1.8955693 -2.2748957][-2.3349707 -1.7632294 -1.2293191 -0.686815 -0.56701922 -0.87968135 -1.3094094 -1.7450941 -1.8232272 -1.4012868 -0.9909668 -1.1873717 -1.7657194 -2.0774627 -2.4263144]]...]
INFO - root - 2017-12-07 03:29:39.318318: step 4510, loss = 0.68, batch loss = 0.61 (9.3 examples/sec; 0.856 sec/batch; 78h:00m:28s remains)
INFO - root - 2017-12-07 03:29:47.573253: step 4520, loss = 0.72, batch loss = 0.64 (9.7 examples/sec; 0.821 sec/batch; 74h:48m:40s remains)
INFO - root - 2017-12-07 03:29:55.878337: step 4530, loss = 0.73, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 73h:43m:59s remains)
INFO - root - 2017-12-07 03:30:04.214446: step 4540, loss = 0.62, batch loss = 0.55 (9.2 examples/sec; 0.869 sec/batch; 79h:08m:55s remains)
INFO - root - 2017-12-07 03:30:12.563220: step 4550, loss = 0.81, batch loss = 0.73 (9.8 examples/sec; 0.819 sec/batch; 74h:35m:12s remains)
INFO - root - 2017-12-07 03:30:20.767663: step 4560, loss = 0.87, batch loss = 0.80 (9.5 examples/sec; 0.845 sec/batch; 77h:00m:25s remains)
INFO - root - 2017-12-07 03:30:28.994561: step 4570, loss = 0.60, batch loss = 0.53 (9.8 examples/sec; 0.816 sec/batch; 74h:18m:31s remains)
INFO - root - 2017-12-07 03:30:37.271581: step 4580, loss = 0.85, batch loss = 0.77 (10.8 examples/sec; 0.742 sec/batch; 67h:34m:55s remains)
INFO - root - 2017-12-07 03:30:45.509195: step 4590, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 72h:22m:23s remains)
INFO - root - 2017-12-07 03:30:53.931544: step 4600, loss = 0.73, batch loss = 0.66 (9.5 examples/sec; 0.838 sec/batch; 76h:21m:43s remains)
2017-12-07 03:30:54.595600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8888476 -1.9386077 -1.9381483 -1.8999162 -1.8556473 -1.822505 -1.7947626 -1.7710304 -1.7435374 -1.7291906 -1.7791433 -1.885402 -1.9974258 -2.0257924 -1.9865131][-1.7904935 -1.8481777 -1.8879452 -1.8973758 -1.8945954 -1.8718581 -1.84009 -1.8195076 -1.8111587 -1.8249633 -1.881866 -1.9416835 -1.94311 -1.8249342 -1.6435962][-1.9370561 -2.0415561 -2.1620204 -2.2393675 -2.266118 -2.2253485 -2.1569819 -2.1065183 -2.0698636 -2.0541308 -2.0437274 -1.9708719 -1.8061233 -1.5230963 -1.2171693][-2.8135505 -2.9769421 -3.1365736 -3.2173715 -3.2080166 -3.1360841 -3.0448904 -2.9458103 -2.8246441 -2.7249508 -2.6358504 -2.4593658 -2.1654713 -1.7834473 -1.430197][-3.3234768 -3.3379917 -3.3377986 -3.3003917 -3.2096877 -3.1046605 -3.0324473 -2.9645514 -2.8885236 -2.8585784 -2.8635566 -2.7746296 -2.5197806 -2.1901329 -1.898159][-2.7505958 -2.570864 -2.3834093 -2.2179654 -2.0283649 -1.8659704 -1.7914875 -1.8038406 -1.8873386 -2.0583167 -2.2610788 -2.3869023 -2.3229547 -2.140342 -1.9536302][-0.92237306 -0.61622119 -0.31792784 -0.0819006 0.15754747 0.37471962 0.55137444 0.62031126 0.56301785 0.36882019 0.1435976 -0.099966049 -0.27685022 -0.41494274 -0.60611939][-0.45461583 -0.10988426 0.21897125 0.45765018 0.64831352 0.81572247 1.0209103 1.1976247 1.2840886 1.2587714 1.2527828 1.1703076 1.0339665 0.80666113 0.39866543][-1.4475863 -1.3866575 -1.3358021 -1.3355892 -1.3352964 -1.3125272 -1.1806521 -0.99133468 -0.85238695 -0.77496958 -0.63451242 -0.55656362 -0.47878623 -0.41137838 -0.489882][-1.8123915 -1.9234922 -2.0644774 -2.2361195 -2.3870425 -2.5228002 -2.5071831 -2.2854435 -2.0202103 -1.8149459 -1.6371305 -1.5647454 -1.4488351 -1.2569456 -1.146085][-2.2124712 -2.1121497 -2.0166833 -1.9879515 -2.0113835 -2.1412728 -2.220948 -2.1013942 -1.9335697 -1.7496967 -1.6021028 -1.5312498 -1.402585 -1.1997263 -1.0650718][-2.3436875 -2.3017466 -2.1761355 -2.0791223 -2.0277531 -2.0790074 -2.08392 -1.999758 -1.995676 -1.9978802 -2.002218 -2.0111425 -1.910831 -1.6777205 -1.4366829][-2.5332971 -2.6362779 -2.609148 -2.5504479 -2.4868476 -2.4869409 -2.3971581 -2.2355616 -2.20664 -2.2395432 -2.3142967 -2.3837032 -2.3290358 -2.1311033 -1.8679674][-2.9011049 -2.9590085 -2.893343 -2.7735105 -2.673996 -2.6895804 -2.6553726 -2.5647964 -2.5517812 -2.524085 -2.5164957 -2.5170345 -2.4238799 -2.2195683 -1.9565828][-2.717988 -2.8263831 -2.8465083 -2.8112369 -2.7957077 -2.8929348 -2.9407949 -2.9341578 -2.93046 -2.8299654 -2.6962559 -2.5470452 -2.3356295 -2.0410814 -1.727453]]...]
INFO - root - 2017-12-07 03:31:03.005058: step 4610, loss = 0.61, batch loss = 0.54 (10.4 examples/sec; 0.772 sec/batch; 70h:21m:02s remains)
INFO - root - 2017-12-07 03:31:11.456895: step 4620, loss = 0.81, batch loss = 0.74 (9.1 examples/sec; 0.876 sec/batch; 79h:46m:49s remains)
INFO - root - 2017-12-07 03:31:19.820312: step 4630, loss = 0.66, batch loss = 0.59 (9.4 examples/sec; 0.853 sec/batch; 77h:39m:20s remains)
INFO - root - 2017-12-07 03:31:28.121448: step 4640, loss = 0.69, batch loss = 0.62 (9.2 examples/sec; 0.867 sec/batch; 78h:59m:17s remains)
INFO - root - 2017-12-07 03:31:36.667192: step 4650, loss = 0.62, batch loss = 0.55 (9.3 examples/sec; 0.858 sec/batch; 78h:05m:48s remains)
INFO - root - 2017-12-07 03:31:45.075284: step 4660, loss = 0.88, batch loss = 0.80 (9.1 examples/sec; 0.876 sec/batch; 79h:46m:15s remains)
INFO - root - 2017-12-07 03:31:53.426844: step 4670, loss = 0.55, batch loss = 0.47 (9.7 examples/sec; 0.824 sec/batch; 75h:02m:10s remains)
INFO - root - 2017-12-07 03:32:01.866132: step 4680, loss = 0.87, batch loss = 0.80 (9.5 examples/sec; 0.846 sec/batch; 77h:01m:07s remains)
INFO - root - 2017-12-07 03:32:10.227119: step 4690, loss = 0.65, batch loss = 0.58 (9.8 examples/sec; 0.817 sec/batch; 74h:21m:35s remains)
INFO - root - 2017-12-07 03:32:18.588532: step 4700, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.833 sec/batch; 75h:51m:08s remains)
2017-12-07 03:32:19.282686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.81378126 -0.83629584 -1.2076595 -1.7062755 -2.1850266 -2.7404828 -3.1999855 -3.590095 -3.8925266 -3.9918613 -3.8136117 -3.4323187 -3.3432188 -3.6832502 -4.1061692][-1.2264163 -1.2315702 -1.5270116 -1.8211727 -2.1510518 -2.6754062 -3.2398102 -3.7751141 -4.0570941 -4.0234575 -3.6465921 -3.174063 -3.1703784 -3.610009 -4.1186461][-0.98069739 -1.0225935 -1.3619652 -1.496923 -1.5321312 -1.8674757 -2.6071851 -3.4469366 -3.798214 -3.6227994 -3.0925407 -2.6033132 -2.725719 -3.252368 -3.8472524][-0.35980844 -0.62088847 -1.1745543 -1.2096472 -0.79376483 -0.63186789 -1.3446732 -2.5475147 -3.189362 -3.0815198 -2.5494897 -2.070488 -2.2606025 -2.8321443 -3.5097787][-0.11206245 -0.46854115 -1.1323974 -0.96769547 0.10073757 1.0256162 0.57090616 -0.98573565 -2.1043549 -2.2725286 -1.8373423 -1.3661575 -1.6107912 -2.3255661 -3.1663613][0.2482028 -0.17938423 -0.82311368 -0.39977598 1.2475748 2.8507938 2.6692977 0.7551384 -0.88116145 -1.2938969 -0.77295423 -0.10985374 -0.35143995 -1.2822063 -2.2854784][0.60792542 5.1498413e-05 -0.61753345 0.044278622 2.0303559 3.9131937 3.7478685 1.4491706 -0.60456467 -1.1019742 -0.24807739 0.78864527 0.5429306 -0.72221112 -1.9823263][0.16089201 -0.46497011 -0.99307108 -0.19182014 1.8744979 3.7423468 3.4800692 0.96117163 -1.2920451 -1.6964197 -0.43014336 0.95133829 0.66311741 -0.94870186 -2.5895395][-0.87627864 -1.3293245 -1.7163484 -0.86459517 1.2311535 3.1421146 2.9617982 0.48932123 -1.7968478 -2.0970676 -0.55683684 1.0511689 0.82164526 -0.98181224 -2.9315085][-1.6330791 -1.9363387 -2.2012544 -1.4313204 0.5276618 2.4195538 2.4382305 0.32561779 -1.7978561 -2.0461452 -0.43407679 1.2664809 1.1584148 -0.72955608 -2.7805827][-1.8288682 -2.0834708 -2.2789783 -1.6818218 -0.08714962 1.4920011 1.6438231 0.165133 -1.5377061 -1.7293811 -0.17748594 1.517632 1.5730987 -0.25247097 -2.2404325][-1.4952157 -1.7139308 -1.9025447 -1.6246424 -0.63991761 0.4102273 0.64149189 -0.15647459 -1.3246629 -1.4120634 0.0031552315 1.592164 1.8016176 0.24724865 -1.4981022][-0.9802444 -1.1550388 -1.3724322 -1.4706497 -1.1710904 -0.70816684 -0.494622 -0.8194983 -1.5060945 -1.4586635 -0.27910852 1.0059719 1.2305741 0.10544395 -1.11444][-0.5945735 -0.73057342 -0.95958424 -1.2941623 -1.47261 -1.485028 -1.4477513 -1.6111405 -1.9436586 -1.7975361 -0.94619823 -0.099774837 0.075035572 -0.52502108 -1.0727305][-0.38659668 -0.47296286 -0.6658864 -1.0347013 -1.4093964 -1.6990724 -1.8552158 -1.9911482 -2.0960827 -1.9326906 -1.428822 -0.93495822 -0.769151 -0.9099791 -0.93135524]]...]
INFO - root - 2017-12-07 03:32:27.737341: step 4710, loss = 0.62, batch loss = 0.55 (9.8 examples/sec; 0.815 sec/batch; 74h:12m:47s remains)
INFO - root - 2017-12-07 03:32:36.086504: step 4720, loss = 0.58, batch loss = 0.51 (9.1 examples/sec; 0.877 sec/batch; 79h:53m:13s remains)
INFO - root - 2017-12-07 03:32:44.406403: step 4730, loss = 0.76, batch loss = 0.68 (10.0 examples/sec; 0.803 sec/batch; 73h:05m:37s remains)
INFO - root - 2017-12-07 03:32:52.885928: step 4740, loss = 0.71, batch loss = 0.64 (8.8 examples/sec; 0.912 sec/batch; 82h:59m:32s remains)
INFO - root - 2017-12-07 03:33:01.336102: step 4750, loss = 0.58, batch loss = 0.51 (9.1 examples/sec; 0.875 sec/batch; 79h:39m:58s remains)
INFO - root - 2017-12-07 03:33:09.673158: step 4760, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.784 sec/batch; 71h:20m:51s remains)
INFO - root - 2017-12-07 03:33:17.763473: step 4770, loss = 0.85, batch loss = 0.78 (10.0 examples/sec; 0.802 sec/batch; 72h:59m:55s remains)
INFO - root - 2017-12-07 03:33:26.180745: step 4780, loss = 0.88, batch loss = 0.81 (9.2 examples/sec; 0.872 sec/batch; 79h:24m:36s remains)
INFO - root - 2017-12-07 03:33:34.590328: step 4790, loss = 0.65, batch loss = 0.57 (8.7 examples/sec; 0.921 sec/batch; 83h:52m:26s remains)
INFO - root - 2017-12-07 03:33:43.006594: step 4800, loss = 0.56, batch loss = 0.49 (10.1 examples/sec; 0.791 sec/batch; 72h:00m:31s remains)
2017-12-07 03:33:43.651678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2834609 -2.4933953 -2.6789122 -2.7147596 -2.6471767 -2.5576358 -2.3433607 -2.2022622 -2.3257251 -2.5163679 -2.5644228 -2.3781049 -2.0230784 -1.7234247 -1.6069918][-1.98402 -2.403553 -2.782825 -2.9119742 -2.8177485 -2.6946452 -2.4008381 -2.1004453 -2.1824071 -2.4691262 -2.6596746 -2.6327581 -2.2843943 -1.8162131 -1.5839353][-1.5693963 -2.322804 -2.9074247 -3.0661209 -2.8899455 -2.6616693 -2.2611282 -1.7709091 -1.7937694 -2.2431438 -2.701674 -2.9366612 -2.7049713 -2.1394529 -1.7717624][-1.1705117 -2.1275952 -2.8374867 -3.0482607 -2.8774028 -2.558351 -2.0084333 -1.2900662 -1.2029383 -1.836138 -2.6366572 -3.1760643 -3.1616781 -2.6087008 -2.1195765][-1.1144321 -1.9879022 -2.6218381 -2.8962879 -2.8748178 -2.5441051 -1.8086879 -0.75381231 -0.35826206 -1.064575 -2.2289486 -3.1321518 -3.4283977 -3.0233402 -2.4945316][-1.3606977 -2.0769017 -2.5044365 -2.7178664 -2.7664564 -2.4190197 -1.4934413 -0.10226774 0.67913294 -0.017506123 -1.5301578 -2.8045106 -3.4107046 -3.201715 -2.6705253][-1.770442 -2.423712 -2.6755769 -2.7369189 -2.6863251 -2.2191274 -1.1149642 0.47462893 1.5323653 0.88295078 -0.83883452 -2.3542757 -3.1728864 -3.1539106 -2.6227131][-2.1305354 -2.7238698 -2.8565164 -2.7852149 -2.6316032 -2.0761306 -0.90621066 0.73767471 1.9360003 1.4451404 -0.20862532 -1.7720311 -2.7380466 -2.9635801 -2.5857925][-2.4893069 -2.9127111 -2.9155288 -2.7566552 -2.5389709 -1.9899881 -0.91350269 0.59048986 1.7386379 1.4672556 0.097612381 -1.3145573 -2.2846253 -2.6768045 -2.5431743][-2.9018762 -3.1967969 -3.1100574 -2.8400748 -2.5542121 -2.0759759 -1.2306814 -0.072977543 0.85399103 0.74308395 -0.24531364 -1.3559215 -2.1527689 -2.5384629 -2.5822654][-3.1336765 -3.3537617 -3.2809858 -2.9489069 -2.5551732 -2.1181469 -1.5734441 -0.93560767 -0.40026236 -0.44722724 -1.0560164 -1.8232465 -2.3855746 -2.6980085 -2.8329229][-3.0288048 -3.1297536 -3.1392522 -2.8839798 -2.4553061 -2.0218391 -1.6989934 -1.531626 -1.4504724 -1.5695097 -1.934535 -2.4169729 -2.7620783 -3.006691 -3.1762755][-2.6923337 -2.6687012 -2.7752895 -2.7140393 -2.3763115 -1.9631922 -1.734889 -1.8330617 -2.0706701 -2.2967126 -2.5269761 -2.806886 -3.0473344 -3.349299 -3.6180484][-2.2957444 -2.1620948 -2.2935812 -2.3981721 -2.1811717 -1.7748201 -1.5109255 -1.688406 -2.1170266 -2.4639556 -2.602767 -2.680558 -2.8489504 -3.2945442 -3.8084774][-1.9675281 -1.7005689 -1.7469685 -1.9158633 -1.8187561 -1.4330754 -1.0822108 -1.1989021 -1.7172472 -2.2080877 -2.3333457 -2.214411 -2.2452171 -2.7291949 -3.4482946]]...]
INFO - root - 2017-12-07 03:33:51.942417: step 4810, loss = 0.56, batch loss = 0.49 (9.8 examples/sec; 0.813 sec/batch; 74h:01m:14s remains)
INFO - root - 2017-12-07 03:34:00.360077: step 4820, loss = 0.80, batch loss = 0.73 (9.5 examples/sec; 0.845 sec/batch; 76h:54m:39s remains)
INFO - root - 2017-12-07 03:34:08.751200: step 4830, loss = 0.66, batch loss = 0.58 (9.3 examples/sec; 0.864 sec/batch; 78h:40m:35s remains)
INFO - root - 2017-12-07 03:34:17.075053: step 4840, loss = 0.89, batch loss = 0.82 (10.3 examples/sec; 0.779 sec/batch; 70h:56m:44s remains)
INFO - root - 2017-12-07 03:34:25.459672: step 4850, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 74h:26m:45s remains)
INFO - root - 2017-12-07 03:34:33.936748: step 4860, loss = 0.74, batch loss = 0.67 (9.3 examples/sec; 0.862 sec/batch; 78h:28m:03s remains)
INFO - root - 2017-12-07 03:34:42.486512: step 4870, loss = 0.71, batch loss = 0.64 (9.3 examples/sec; 0.857 sec/batch; 78h:00m:15s remains)
INFO - root - 2017-12-07 03:34:50.874985: step 4880, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.856 sec/batch; 77h:51m:22s remains)
INFO - root - 2017-12-07 03:34:59.235615: step 4890, loss = 0.76, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 73h:30m:01s remains)
INFO - root - 2017-12-07 03:35:07.615677: step 4900, loss = 0.67, batch loss = 0.60 (9.9 examples/sec; 0.811 sec/batch; 73h:49m:45s remains)
2017-12-07 03:35:08.320054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4559958 -3.7273638 -3.6756904 -3.3494997 -2.5829124 -1.95457 -1.6615901 -1.8275266 -2.8620033 -3.8791213 -4.309495 -4.4548049 -4.2621078 -3.974345 -3.9683042][-3.4238434 -3.7104115 -3.7084932 -3.3832958 -2.7554851 -2.3017828 -1.9508083 -1.908675 -2.8075795 -3.7599771 -4.1934018 -4.3688378 -4.180851 -3.8999395 -3.7923145][-3.3704593 -3.7659078 -3.8242733 -3.4333444 -2.81143 -2.4188273 -1.9952147 -1.8376765 -2.7139919 -3.6253443 -3.9891179 -4.2097626 -4.0942545 -3.8264408 -3.6934042][-3.2871194 -3.8201535 -3.938972 -3.4228415 -2.668292 -2.2221031 -1.7163451 -1.4870694 -2.3820891 -3.2459931 -3.5075057 -3.874532 -3.9697137 -3.7677214 -3.703048][-3.1524396 -3.8136353 -3.9821191 -3.32157 -2.3593102 -1.8116326 -1.2126136 -0.85652041 -1.7212467 -2.5073445 -2.660367 -3.2390127 -3.5536525 -3.3387175 -3.3406134][-3.0737581 -3.7703094 -3.9212375 -3.1575413 -2.1008997 -1.5027816 -0.78218341 -0.25181198 -1.0974631 -1.8844383 -2.0277224 -2.7855458 -3.1738114 -2.7706332 -2.7642765][-3.1410027 -3.7309041 -3.750931 -2.9663675 -2.0481851 -1.5597532 -0.78301406 -0.094633579 -0.91682005 -1.8017354 -2.0788231 -2.9507306 -3.2682741 -2.6561542 -2.6325481][-3.2194977 -3.6126895 -3.4455819 -2.7317786 -2.0734224 -1.7633865 -1.0549819 -0.43402696 -1.2978852 -2.2915378 -2.6813951 -3.4630363 -3.5850646 -2.8694921 -2.8268569][-3.1953263 -3.4019651 -3.0731654 -2.468318 -2.0097747 -1.8019352 -1.2854407 -1.030277 -2.0520513 -3.1018088 -3.5371602 -4.0743604 -3.9110413 -3.1659369 -3.0737209][-3.1722722 -3.2842851 -2.8916924 -2.3517308 -1.9049222 -1.6377003 -1.3479626 -1.5663793 -2.7562895 -3.765132 -4.1784596 -4.4729085 -4.1091342 -3.4642475 -3.3675609][-3.2174344 -3.2754655 -2.9646831 -2.5463076 -2.073741 -1.7254221 -1.5957928 -2.1090877 -3.2985544 -4.1264381 -4.4111538 -4.4630289 -3.9597957 -3.4404457 -3.3411121][-3.2952669 -3.2692389 -3.1028562 -2.8765004 -2.4646811 -2.1457329 -2.1208005 -2.6855023 -3.7119009 -4.3299866 -4.5105557 -4.4115152 -3.8295977 -3.3896089 -3.2552314][-3.3182752 -3.1877828 -3.1093159 -3.0432482 -2.7650039 -2.5768857 -2.6123433 -3.0459058 -3.7795548 -4.1742253 -4.2808404 -4.149065 -3.634655 -3.3187196 -3.233253][-3.1338992 -2.9291718 -2.8554356 -2.8499866 -2.6839423 -2.6170669 -2.6908569 -2.9658136 -3.397191 -3.5781851 -3.5998731 -3.4825428 -3.1153927 -2.9557414 -2.986063][-2.6483068 -2.4646935 -2.4006102 -2.4034064 -2.2872272 -2.2579763 -2.3199377 -2.4842989 -2.7262123 -2.7893984 -2.7580771 -2.6482263 -2.3899841 -2.3196483 -2.4271035]]...]
INFO - root - 2017-12-07 03:35:16.835240: step 4910, loss = 0.98, batch loss = 0.90 (8.6 examples/sec; 0.926 sec/batch; 84h:15m:40s remains)
INFO - root - 2017-12-07 03:35:25.187766: step 4920, loss = 0.62, batch loss = 0.55 (9.9 examples/sec; 0.809 sec/batch; 73h:36m:21s remains)
INFO - root - 2017-12-07 03:35:33.591126: step 4930, loss = 0.67, batch loss = 0.60 (9.6 examples/sec; 0.832 sec/batch; 75h:40m:12s remains)
INFO - root - 2017-12-07 03:35:41.948677: step 4940, loss = 0.81, batch loss = 0.74 (9.7 examples/sec; 0.822 sec/batch; 74h:47m:57s remains)
INFO - root - 2017-12-07 03:35:50.221957: step 4950, loss = 0.80, batch loss = 0.73 (9.7 examples/sec; 0.822 sec/batch; 74h:45m:10s remains)
INFO - root - 2017-12-07 03:35:58.560939: step 4960, loss = 0.99, batch loss = 0.92 (9.3 examples/sec; 0.859 sec/batch; 78h:10m:58s remains)
INFO - root - 2017-12-07 03:36:06.944773: step 4970, loss = 0.68, batch loss = 0.61 (9.6 examples/sec; 0.832 sec/batch; 75h:42m:07s remains)
INFO - root - 2017-12-07 03:36:15.305634: step 4980, loss = 0.87, batch loss = 0.79 (9.9 examples/sec; 0.806 sec/batch; 73h:18m:55s remains)
INFO - root - 2017-12-07 03:36:23.669721: step 4990, loss = 0.63, batch loss = 0.56 (9.3 examples/sec; 0.862 sec/batch; 78h:23m:39s remains)
INFO - root - 2017-12-07 03:36:31.992350: step 5000, loss = 0.65, batch loss = 0.58 (9.8 examples/sec; 0.818 sec/batch; 74h:23m:55s remains)
2017-12-07 03:36:32.669912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3190939 -2.2616785 -2.2669094 -2.2740531 -2.2178028 -2.2849157 -2.2414033 -2.2224953 -2.5352244 -2.7291121 -2.7469606 -2.7326565 -2.1491046 -1.6380386 -1.520174][-2.3328612 -2.3186288 -2.3648632 -2.4304414 -2.4186659 -2.4914083 -2.4221959 -2.3095636 -2.5048163 -2.6820488 -2.6831033 -2.698709 -2.1389933 -1.5863402 -1.5649993][-2.3678398 -2.4324267 -2.5574446 -2.6942589 -2.7444906 -2.8271632 -2.7049043 -2.5182338 -2.6830072 -2.9497952 -3.0060103 -3.0656614 -2.5483313 -1.9483294 -1.9276769][-2.5252662 -2.69556 -2.8851707 -3.0415871 -3.0615692 -3.0591927 -2.8188145 -2.5668225 -2.7977743 -3.1759415 -3.243345 -3.3578575 -2.9103808 -2.2724783 -2.2215846][-2.8249147 -2.9887912 -3.1372218 -3.254847 -3.1815705 -3.0282607 -2.5759749 -2.175529 -2.508857 -3.0769298 -3.2693834 -3.5060802 -3.2165823 -2.6540895 -2.5905681][-3.0764418 -3.0781045 -3.1037331 -3.1503327 -2.9371185 -2.5471687 -1.8007698 -1.256695 -1.7540696 -2.5630236 -3.0003171 -3.5087676 -3.5143747 -3.1689308 -3.1333885][-3.0124745 -2.7984638 -2.780587 -2.8227122 -2.4688885 -1.7790225 -0.66859555 -0.048021793 -0.88079166 -1.9910085 -2.6727428 -3.363183 -3.558527 -3.4199305 -3.4215481][-2.6167581 -2.2511158 -2.3049028 -2.4208713 -2.0451648 -1.2596862 0.021404743 0.60791063 -0.55002546 -1.8491273 -2.6617079 -3.3760209 -3.5631626 -3.486536 -3.3959465][-2.1587949 -1.7641337 -1.940325 -2.2013624 -2.0866747 -1.6360795 -0.58825111 -0.11877537 -1.1959782 -2.2301924 -2.8611348 -3.4068763 -3.5102136 -3.4788227 -3.3371265][-1.7867811 -1.4986286 -1.8595195 -2.3160641 -2.5801015 -2.5929425 -1.8140326 -1.349016 -2.137059 -2.7140522 -2.9676609 -3.2117302 -3.1871686 -3.2129111 -3.098506][-1.7812097 -1.6772213 -2.1683338 -2.6725512 -3.1307349 -3.385746 -2.7557938 -2.21418 -2.7037029 -2.9276986 -2.9044232 -2.9479058 -2.8432171 -2.9329309 -2.8983817][-2.1937931 -2.25325 -2.7303338 -3.073277 -3.4313698 -3.6723282 -3.121753 -2.5787578 -2.8473449 -2.8723083 -2.7499022 -2.7394176 -2.6127796 -2.7545586 -2.8220816][-2.7463517 -2.845365 -3.1220331 -3.1737845 -3.281786 -3.3704457 -2.897727 -2.4522066 -2.6148291 -2.5891209 -2.5140524 -2.538079 -2.3879428 -2.4757309 -2.5766578][-3.1789222 -3.2150867 -3.2693572 -3.0968857 -2.9647737 -2.8763804 -2.4748688 -2.1523151 -2.256748 -2.2485349 -2.279912 -2.3839695 -2.2519588 -2.2950954 -2.4209642][-3.2009125 -3.1906085 -3.1768832 -2.9992037 -2.8254955 -2.7087817 -2.4418788 -2.2445855 -2.3215673 -2.33808 -2.4117465 -2.5046873 -2.381073 -2.4009795 -2.5346844]]...]
INFO - root - 2017-12-07 03:36:40.998773: step 5010, loss = 0.88, batch loss = 0.81 (10.2 examples/sec; 0.785 sec/batch; 71h:25m:48s remains)
INFO - root - 2017-12-07 03:36:49.291009: step 5020, loss = 0.84, batch loss = 0.77 (9.6 examples/sec; 0.836 sec/batch; 76h:01m:18s remains)
INFO - root - 2017-12-07 03:36:57.654648: step 5030, loss = 0.80, batch loss = 0.72 (9.5 examples/sec; 0.845 sec/batch; 76h:54m:15s remains)
INFO - root - 2017-12-07 03:37:06.095933: step 5040, loss = 0.86, batch loss = 0.79 (9.4 examples/sec; 0.850 sec/batch; 77h:16m:29s remains)
INFO - root - 2017-12-07 03:37:14.496422: step 5050, loss = 0.80, batch loss = 0.73 (8.6 examples/sec; 0.928 sec/batch; 84h:24m:07s remains)
INFO - root - 2017-12-07 03:37:22.844994: step 5060, loss = 0.64, batch loss = 0.57 (9.7 examples/sec; 0.826 sec/batch; 75h:10m:02s remains)
INFO - root - 2017-12-07 03:37:31.087814: step 5070, loss = 0.93, batch loss = 0.85 (10.0 examples/sec; 0.802 sec/batch; 72h:54m:31s remains)
INFO - root - 2017-12-07 03:37:39.214323: step 5080, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 77h:21m:51s remains)
INFO - root - 2017-12-07 03:37:47.648468: step 5090, loss = 0.76, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:06m:26s remains)
INFO - root - 2017-12-07 03:37:55.934244: step 5100, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.798 sec/batch; 72h:36m:04s remains)
2017-12-07 03:37:56.531360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4917035 -0.75526667 -0.77748966 -1.1883855 -1.6536942 -1.8616784 -1.8541796 -1.7898946 -1.450259 -1.6505511 -1.9450705 -1.5749693 -1.2606559 -0.88263011 -0.64572787][-1.9773118 -1.0556562 -0.93423033 -1.2074578 -1.5694118 -1.7644217 -1.8758039 -1.9386647 -1.6727059 -1.8996909 -2.1775036 -1.8319774 -1.6185071 -1.2285447 -0.86077237][-2.11253 -1.1396348 -0.9004724 -0.965883 -1.2017992 -1.4605887 -1.8308332 -2.2235212 -2.124578 -2.3043537 -2.4582586 -2.0891087 -1.9528158 -1.5837135 -1.1280344][-1.8548298 -1.0903597 -0.97887731 -0.95403147 -1.0461485 -1.3116584 -1.9181716 -2.610575 -2.678865 -2.7561202 -2.7329741 -2.2989426 -2.2041657 -1.8942113 -1.369632][-1.6905563 -1.3331013 -1.4972098 -1.474215 -1.4880438 -1.73263 -2.3720906 -3.1259732 -3.2560358 -3.1387711 -2.9295809 -2.4284768 -2.352479 -2.1271262 -1.5501132][-1.7558591 -1.7865551 -2.203696 -2.3079824 -2.4041076 -2.643384 -2.9824803 -3.4305122 -3.4969802 -3.213284 -2.935128 -2.4490519 -2.3617108 -2.1704574 -1.5393147][-1.7426577 -2.1890266 -2.82166 -3.04538 -3.1333523 -3.1424255 -2.8983169 -2.8741531 -2.9173536 -2.6843314 -2.6180754 -2.3811862 -2.3013294 -2.0672557 -1.3550026][-1.8788757 -2.6542771 -3.3256893 -3.4967296 -3.2769575 -2.7151427 -1.7142806 -1.152509 -1.2308729 -1.3287549 -1.7226489 -1.9246469 -1.9243383 -1.6690519 -0.91820145][-2.3432965 -3.1991813 -3.6835191 -3.6182277 -2.8883662 -1.5976253 0.083223343 1.1446743 1.0096421 0.45901346 -0.4123621 -1.0686512 -1.1991146 -0.94424152 -0.2306962][-2.6048865 -3.3231666 -3.6392651 -3.3998618 -2.2796223 -0.46397758 1.5022106 2.7551961 2.518456 1.6933856 0.68546867 -0.138134 -0.37867022 -0.17560196 0.39864302][-2.8033352 -3.3138719 -3.5710926 -3.2795944 -1.9080651 0.22176933 2.135582 3.2399716 2.9033585 2.0820241 1.3401465 0.65354109 0.34726143 0.37734556 0.65374804][-3.0592678 -3.3800707 -3.6082065 -3.2664943 -1.7195301 0.47240829 2.1023564 2.9335399 2.5638185 1.8656769 1.4427123 0.92934227 0.5956912 0.49599218 0.53524303][-3.3792729 -3.5255146 -3.665607 -3.1357608 -1.3726315 0.73460245 1.9602165 2.4890957 2.0950923 1.5253944 1.3250489 0.917789 0.60123634 0.47528172 0.46358013][-3.4598031 -3.543056 -3.5410979 -2.6924806 -0.69914412 1.2288737 2.0107265 2.2115164 1.7837667 1.3528566 1.2980018 0.96389866 0.68984175 0.58219242 0.56515074][-3.1358032 -3.1274195 -2.9262824 -1.7946172 0.20003128 1.7138 1.9700398 1.8325925 1.4448528 1.2119179 1.248415 0.98831511 0.782835 0.69920158 0.66077709]]...]
INFO - root - 2017-12-07 03:38:04.939162: step 5110, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.829 sec/batch; 75h:21m:59s remains)
INFO - root - 2017-12-07 03:38:13.328507: step 5120, loss = 0.70, batch loss = 0.62 (9.1 examples/sec; 0.883 sec/batch; 80h:19m:28s remains)
INFO - root - 2017-12-07 03:38:21.599760: step 5130, loss = 0.76, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 72h:37m:01s remains)
INFO - root - 2017-12-07 03:38:29.941008: step 5140, loss = 0.68, batch loss = 0.61 (9.6 examples/sec; 0.837 sec/batch; 76h:06m:47s remains)
INFO - root - 2017-12-07 03:38:38.304156: step 5150, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.793 sec/batch; 72h:08m:27s remains)
INFO - root - 2017-12-07 03:38:46.792425: step 5160, loss = 0.92, batch loss = 0.85 (8.9 examples/sec; 0.895 sec/batch; 81h:21m:14s remains)
INFO - root - 2017-12-07 03:38:55.045329: step 5170, loss = 0.83, batch loss = 0.76 (8.8 examples/sec; 0.909 sec/batch; 82h:38m:32s remains)
INFO - root - 2017-12-07 03:39:03.277034: step 5180, loss = 0.64, batch loss = 0.56 (10.2 examples/sec; 0.783 sec/batch; 71h:11m:01s remains)
INFO - root - 2017-12-07 03:39:11.605667: step 5190, loss = 0.65, batch loss = 0.58 (10.0 examples/sec; 0.799 sec/batch; 72h:37m:44s remains)
INFO - root - 2017-12-07 03:39:19.913046: step 5200, loss = 0.77, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 78h:34m:25s remains)
2017-12-07 03:39:20.566115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7392864 -2.8106997 -2.8738091 -2.9096253 -2.9180281 -2.9011216 -2.8817444 -2.9042392 -2.9321344 -2.9536307 -2.9533684 -2.9303017 -2.8983796 -2.8544154 -2.7828937][-2.8647451 -2.9522531 -3.0234623 -3.0565581 -3.0724287 -3.0662584 -3.0308337 -3.0315156 -3.0258231 -3.0052109 -2.9473464 -2.8782063 -2.8329334 -2.8066239 -2.7510672][-2.7228861 -2.7697392 -2.8062348 -2.7859383 -2.7600818 -2.7188964 -2.636672 -2.6047864 -2.6089997 -2.629128 -2.5918634 -2.5333741 -2.528419 -2.5769033 -2.5798659][-2.4224474 -2.4855475 -2.5677276 -2.5707612 -2.577827 -2.5503216 -2.4626949 -2.4275875 -2.4442291 -2.4609127 -2.3985384 -2.3321514 -2.3872142 -2.5363898 -2.6046395][-2.3093731 -2.483994 -2.6478596 -2.6585107 -2.6810398 -2.665596 -2.6078086 -2.6243215 -2.6443508 -2.5827904 -2.4191077 -2.2981291 -2.3821948 -2.5896585 -2.6901767][-2.1118879 -2.2330325 -2.3227804 -2.2251918 -2.1394408 -1.9846265 -1.8217435 -1.8276935 -1.8380282 -1.749146 -1.5592811 -1.4604123 -1.657764 -1.996525 -2.2249765][-1.5778439 -1.4529784 -1.2954748 -0.97460365 -0.63705826 -0.1791153 0.15346289 0.064649105 -0.16882324 -0.32401896 -0.32144165 -0.35213661 -0.71229887 -1.1926486 -1.5663676][-0.91371942 -0.64898896 -0.40141153 -0.010848045 0.51203585 1.2334256 1.7142615 1.4660311 0.84307814 0.21368265 -0.18515158 -0.48017979 -0.95441365 -1.4122987 -1.6902683][-0.95804787 -0.93701911 -1.0038371 -0.96529222 -0.78176427 -0.41242886 -0.14736843 -0.29857635 -0.72736812 -1.1878297 -1.4506969 -1.6306536 -1.9454362 -2.2011638 -2.2599335][-1.5017629 -1.7507741 -2.027555 -2.2225451 -2.3447864 -2.3262849 -2.2282023 -2.2156751 -2.3112783 -2.4014657 -2.387321 -2.3879509 -2.5386047 -2.6305313 -2.5652242][-1.7191005 -1.9267743 -2.1080015 -2.2577381 -2.3875735 -2.4011207 -2.2595787 -2.0736194 -1.9513929 -1.895134 -1.9138899 -2.0504827 -2.2985756 -2.459712 -2.4973967][-1.5963118 -1.5693626 -1.5206468 -1.5128236 -1.5699108 -1.5804727 -1.5019426 -1.3783362 -1.3117895 -1.3878708 -1.6114457 -1.9188449 -2.2233849 -2.4042211 -2.5014834][-1.7502224 -1.6552436 -1.5362921 -1.4755063 -1.4994831 -1.5226042 -1.5539241 -1.6065149 -1.7106946 -1.9276552 -2.231595 -2.528631 -2.7025905 -2.725832 -2.6945519][-2.53009 -2.6198273 -2.6531305 -2.6820307 -2.7224503 -2.7371979 -2.7618003 -2.8254871 -2.919837 -3.0491731 -3.1733184 -3.2366438 -3.1812487 -3.0199203 -2.839793][-3.1123838 -3.2960966 -3.4093132 -3.4898543 -3.5504429 -3.5673246 -3.551265 -3.5373924 -3.5412102 -3.5362945 -3.4824328 -3.3749719 -3.2222536 -3.0313814 -2.8352075]]...]
INFO - root - 2017-12-07 03:39:28.962548: step 5210, loss = 0.70, batch loss = 0.62 (9.3 examples/sec; 0.861 sec/batch; 78h:14m:53s remains)
INFO - root - 2017-12-07 03:39:37.443504: step 5220, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 71h:37m:46s remains)
INFO - root - 2017-12-07 03:39:45.971585: step 5230, loss = 0.95, batch loss = 0.88 (10.0 examples/sec; 0.804 sec/batch; 73h:04m:59s remains)
INFO - root - 2017-12-07 03:39:54.285576: step 5240, loss = 0.82, batch loss = 0.74 (9.8 examples/sec; 0.818 sec/batch; 74h:20m:43s remains)
INFO - root - 2017-12-07 03:40:02.699964: step 5250, loss = 0.63, batch loss = 0.56 (8.9 examples/sec; 0.899 sec/batch; 81h:41m:55s remains)
INFO - root - 2017-12-07 03:40:11.038809: step 5260, loss = 0.91, batch loss = 0.83 (9.2 examples/sec; 0.865 sec/batch; 78h:38m:46s remains)
INFO - root - 2017-12-07 03:40:19.342152: step 5270, loss = 0.84, batch loss = 0.77 (9.6 examples/sec; 0.830 sec/batch; 75h:26m:33s remains)
INFO - root - 2017-12-07 03:40:27.598441: step 5280, loss = 0.88, batch loss = 0.81 (9.7 examples/sec; 0.824 sec/batch; 74h:55m:39s remains)
INFO - root - 2017-12-07 03:40:35.894854: step 5290, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.860 sec/batch; 78h:10m:38s remains)
INFO - root - 2017-12-07 03:40:44.135225: step 5300, loss = 0.68, batch loss = 0.60 (10.1 examples/sec; 0.788 sec/batch; 71h:38m:38s remains)
2017-12-07 03:40:44.784198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5124621 -2.616698 -2.6785078 -2.7092719 -2.7491121 -2.7162859 -2.6570964 -2.6021647 -2.6059237 -2.6167881 -2.5496185 -2.4886708 -2.4754548 -2.4627137 -2.4602277][-2.4448743 -2.6701984 -2.7790325 -2.7929275 -2.8337781 -2.8050616 -2.723443 -2.6148851 -2.5663617 -2.5500317 -2.457159 -2.3802595 -2.3621576 -2.352268 -2.3671188][-2.3360949 -2.6626854 -2.7750509 -2.7108042 -2.6965294 -2.6422672 -2.538569 -2.4160843 -2.3599138 -2.34553 -2.2568967 -2.1849971 -2.1741295 -2.1564946 -2.1694691][-2.0689421 -2.4319015 -2.4886532 -2.31255 -2.2367663 -2.1572282 -2.0621531 -1.9875364 -1.9675245 -1.9702997 -1.880302 -1.8207884 -1.8159831 -1.7733059 -1.753593][-1.7627861 -2.0797646 -2.0250895 -1.7549188 -1.6337361 -1.5352015 -1.4584494 -1.454299 -1.4986057 -1.5323815 -1.4523211 -1.415993 -1.4179223 -1.3538625 -1.3085055][-1.5424826 -1.7866812 -1.6437719 -1.3415172 -1.193804 -1.0402315 -0.91634536 -0.9331286 -1.0417151 -1.1606033 -1.1583469 -1.1851897 -1.1947887 -1.107923 -1.051929][-1.5194342 -1.7077303 -1.5308444 -1.2564373 -1.0860696 -0.83862853 -0.59455657 -0.58546591 -0.7788465 -1.0358531 -1.1668811 -1.2646854 -1.2796297 -1.1717138 -1.1022937][-1.6378171 -1.7780402 -1.6001799 -1.3621745 -1.1710505 -0.82710767 -0.46108556 -0.43172193 -0.7178092 -1.0943329 -1.3257687 -1.4481316 -1.4733131 -1.4007592 -1.3714929][-1.60099 -1.6934304 -1.5228238 -1.3198643 -1.1366808 -0.77018046 -0.40595961 -0.43591 -0.79899263 -1.2017751 -1.421139 -1.4976907 -1.5289969 -1.5433741 -1.6059237][-1.3922434 -1.4814785 -1.3678317 -1.2407072 -1.1070495 -0.79114819 -0.50802469 -0.61358428 -0.98510361 -1.3148708 -1.4533861 -1.4650304 -1.4880891 -1.5584812 -1.6636522][-1.2037201 -1.3468804 -1.3379636 -1.2956738 -1.2142153 -0.9468751 -0.71015406 -0.83439279 -1.1562181 -1.3888218 -1.4546943 -1.4350779 -1.4505346 -1.5313911 -1.634578][-1.1767271 -1.3726511 -1.4349582 -1.4371998 -1.3792634 -1.1348393 -0.9212842 -1.0380673 -1.3162541 -1.4738526 -1.4833307 -1.4571989 -1.4902611 -1.5833039 -1.7019176][-1.2345471 -1.4578786 -1.541065 -1.5574386 -1.5129089 -1.2941735 -1.09657 -1.1899772 -1.4182928 -1.5079553 -1.476542 -1.477021 -1.5814886 -1.7414665 -1.912235][-1.2639189 -1.4825723 -1.5569842 -1.5704949 -1.5496037 -1.3852737 -1.230932 -1.2989826 -1.4762962 -1.5463929 -1.5426466 -1.6233616 -1.8133476 -2.023699 -2.2004788][-1.30148 -1.4801145 -1.5387249 -1.5524721 -1.5567582 -1.4489291 -1.349462 -1.4091175 -1.5539639 -1.6519129 -1.7302401 -1.8892746 -2.1090484 -2.293565 -2.3964283]]...]
INFO - root - 2017-12-07 03:40:53.244729: step 5310, loss = 0.98, batch loss = 0.91 (10.0 examples/sec; 0.801 sec/batch; 72h:45m:42s remains)
INFO - root - 2017-12-07 03:41:01.674574: step 5320, loss = 0.58, batch loss = 0.50 (9.5 examples/sec; 0.843 sec/batch; 76h:35m:55s remains)
INFO - root - 2017-12-07 03:41:10.088347: step 5330, loss = 0.81, batch loss = 0.74 (9.5 examples/sec; 0.845 sec/batch; 76h:45m:51s remains)
INFO - root - 2017-12-07 03:41:18.370181: step 5340, loss = 0.62, batch loss = 0.54 (9.5 examples/sec; 0.844 sec/batch; 76h:43m:13s remains)
INFO - root - 2017-12-07 03:41:26.879885: step 5350, loss = 0.65, batch loss = 0.58 (9.0 examples/sec; 0.890 sec/batch; 80h:53m:39s remains)
INFO - root - 2017-12-07 03:41:35.268648: step 5360, loss = 0.77, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 75h:09m:13s remains)
INFO - root - 2017-12-07 03:41:43.519888: step 5370, loss = 0.60, batch loss = 0.53 (10.0 examples/sec; 0.801 sec/batch; 72h:46m:04s remains)
INFO - root - 2017-12-07 03:41:51.867994: step 5380, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 76h:00m:46s remains)
INFO - root - 2017-12-07 03:41:59.831183: step 5390, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.780 sec/batch; 70h:50m:45s remains)
INFO - root - 2017-12-07 03:42:08.176487: step 5400, loss = 0.70, batch loss = 0.62 (9.7 examples/sec; 0.824 sec/batch; 74h:52m:36s remains)
2017-12-07 03:42:08.841290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9322276 -2.9592266 -2.985178 -3.0298014 -3.0712991 -3.0776067 -3.0936885 -3.118367 -3.138835 -3.1926651 -3.2378914 -3.2237959 -3.1915145 -3.1643372 -3.1328268][-3.1370473 -3.1670213 -3.1697602 -3.174051 -3.1708722 -3.132987 -3.1377382 -3.1708875 -3.2016187 -3.2946963 -3.378108 -3.3601806 -3.3197637 -3.2810068 -3.2020729][-3.1986904 -3.1855731 -3.1044259 -3.0327983 -2.9871943 -2.9318821 -2.919991 -2.9205306 -2.9268477 -3.0588086 -3.2317734 -3.2925885 -3.3236988 -3.3489466 -3.2747493][-3.0748193 -2.9585705 -2.7553864 -2.607996 -2.5345602 -2.4817853 -2.4390712 -2.3797145 -2.3249485 -2.4497304 -2.7425337 -2.9778306 -3.1695948 -3.3270783 -3.3158917][-2.8857555 -2.6633224 -2.3310151 -2.0999167 -1.9848268 -1.9167845 -1.8415606 -1.7025228 -1.5486436 -1.6177335 -2.0305285 -2.4812186 -2.8535724 -3.1507812 -3.226347][-2.8586903 -2.5845423 -2.2126691 -1.9370036 -1.756222 -1.6379983 -1.5082562 -1.2851434 -1.0385923 -1.0737472 -1.5891335 -2.20555 -2.6867638 -3.0478439 -3.1647635][-3.0840614 -2.8573024 -2.59757 -2.393445 -2.1875486 -1.9945438 -1.7674773 -1.4404202 -1.1559 -1.2151692 -1.8030088 -2.4892774 -2.9642708 -3.3067989 -3.430594][-3.1728408 -3.0440693 -2.9787979 -2.9457793 -2.8006694 -2.6250491 -2.3785658 -2.0059927 -1.6965528 -1.7401321 -2.2730191 -2.877552 -3.2513411 -3.5257106 -3.6611135][-2.893868 -2.8235078 -2.9232011 -3.0794182 -3.0953948 -3.1219 -3.0746117 -2.7970695 -2.4719372 -2.3836534 -2.7031252 -3.0957329 -3.2849705 -3.4364729 -3.5421786][-2.3438613 -2.1827366 -2.3457141 -2.6977777 -2.9869382 -3.3451507 -3.6383765 -3.583286 -3.2864814 -3.0130835 -3.0483394 -3.2048328 -3.2228034 -3.2402239 -3.2987905][-1.6254888 -1.2694242 -1.4141326 -1.9945135 -2.6148179 -3.3112817 -3.9628742 -4.1758475 -3.9597652 -3.5463321 -3.3176274 -3.2618566 -3.1683192 -3.1011448 -3.1321478][-0.94931507 -0.40950394 -0.50418115 -1.2891302 -2.1819849 -3.1055436 -4.0068293 -4.4429951 -4.3368807 -3.9047039 -3.5336773 -3.3014998 -3.1083026 -2.9713438 -2.9596529][-0.50940323 0.074927807 -0.020895481 -0.92495942 -1.9383657 -2.9073236 -3.8758008 -4.4249172 -4.4133859 -4.0555639 -3.6831934 -3.3698058 -3.1112094 -2.9002051 -2.7976685][-0.47604036 0.0827384 -0.052138329 -1.0004029 -2.0010257 -2.8235977 -3.6701221 -4.2385454 -4.3246832 -4.1458192 -3.9112048 -3.6253419 -3.3512528 -3.0783892 -2.8597589][-0.90512323 -0.32279778 -0.48644996 -1.4012296 -2.2776141 -2.8065438 -3.4080606 -3.95101 -4.189465 -4.2889843 -4.275372 -4.0910616 -3.8670557 -3.5669522 -3.2524891]]...]
INFO - root - 2017-12-07 03:42:17.154411: step 5410, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.811 sec/batch; 73h:41m:51s remains)
INFO - root - 2017-12-07 03:42:25.517209: step 5420, loss = 0.65, batch loss = 0.57 (9.4 examples/sec; 0.849 sec/batch; 77h:07m:30s remains)
INFO - root - 2017-12-07 03:42:33.821075: step 5430, loss = 0.65, batch loss = 0.57 (9.1 examples/sec; 0.882 sec/batch; 80h:06m:40s remains)
INFO - root - 2017-12-07 03:42:42.139245: step 5440, loss = 0.56, batch loss = 0.49 (10.2 examples/sec; 0.787 sec/batch; 71h:32m:17s remains)
INFO - root - 2017-12-07 03:42:50.451019: step 5450, loss = 0.78, batch loss = 0.71 (8.9 examples/sec; 0.897 sec/batch; 81h:30m:14s remains)
INFO - root - 2017-12-07 03:42:58.868806: step 5460, loss = 0.87, batch loss = 0.80 (9.9 examples/sec; 0.806 sec/batch; 73h:15m:16s remains)
INFO - root - 2017-12-07 03:43:07.243930: step 5470, loss = 0.77, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:25m:14s remains)
INFO - root - 2017-12-07 03:43:15.544821: step 5480, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.825 sec/batch; 74h:55m:14s remains)
INFO - root - 2017-12-07 03:43:23.936264: step 5490, loss = 0.62, batch loss = 0.55 (10.0 examples/sec; 0.800 sec/batch; 72h:38m:41s remains)
INFO - root - 2017-12-07 03:43:32.278645: step 5500, loss = 0.82, batch loss = 0.74 (10.0 examples/sec; 0.796 sec/batch; 72h:19m:05s remains)
2017-12-07 03:43:32.905102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0072844 -2.1135271 -2.0873766 -2.0952206 -2.0048602 -1.9922798 -1.943722 -1.798301 -1.7163186 -1.6658592 -1.6296258 -1.5729685 -1.5110204 -1.4565246 -1.4154665][-2.0842979 -2.2249386 -2.1737118 -2.2198949 -2.1689391 -2.2337778 -2.2966034 -2.1538136 -2.0221095 -1.9860501 -1.9052088 -1.8474007 -1.7588046 -1.5986052 -1.4770164][-1.7130682 -1.7023094 -1.5179632 -1.6385138 -1.7964377 -2.1008472 -2.4510865 -2.4402289 -2.3085647 -2.2811298 -2.1523836 -2.1386781 -2.1512885 -1.9290102 -1.6942925][-1.1278772 -0.80723429 -0.22867107 -0.091218948 -0.23640347 -0.59575129 -1.1668103 -1.4678314 -1.5778625 -1.841538 -1.8664742 -2.0127163 -2.3059125 -2.2122903 -1.9729548][-0.49653602 0.043655872 1.0440168 1.58782 1.6589999 1.4829774 0.98713875 0.68599319 0.44127893 -0.26335382 -0.70938253 -1.1259143 -1.8270528 -2.1267288 -2.1217127][-0.13457251 0.52153015 1.8349838 2.7565722 3.0961514 3.195004 2.9781075 3.0520926 2.9546838 1.911231 1.0445428 0.4743371 -0.48695135 -1.2913005 -1.7485454][-0.38964128 0.13689709 1.5156612 2.7751074 3.5339355 4.1060514 4.4653244 5.3296051 5.7224026 4.4424782 3.0869026 2.3210759 1.2030721 -0.042909145 -1.0471554][-0.86979485 -0.6621902 0.33982372 1.4894295 2.4323153 3.3988943 4.3884315 6.1362686 7.2924633 6.1598692 4.5843964 3.5768757 2.3258491 0.85262918 -0.46806169][-1.1743894 -1.2311623 -0.67123294 0.2373004 1.1516972 2.022491 2.7367883 4.3273458 5.68612 5.1376467 4.0123091 3.2355633 2.2617421 1.0257497 -0.22968435][-1.4219842 -1.5626106 -1.2223256 -0.41923761 0.46913671 1.1487899 1.4234805 2.4573169 3.4140086 3.1027207 2.3804684 1.9576144 1.4395924 0.63218069 -0.36734867][-1.6960571 -1.8346882 -1.6013503 -0.93226337 -0.18438482 0.29239035 0.2607584 0.78206348 1.3126559 1.1338859 0.75407887 0.59640455 0.39312172 -0.068398 -0.8297801][-2.0271535 -2.1554143 -1.9386103 -1.3886597 -0.898334 -0.72434878 -1.0240531 -0.91534877 -0.66524816 -0.69831753 -0.81653833 -0.79003739 -0.7431066 -0.86002946 -1.3657663][-2.2986946 -2.4270573 -2.1936758 -1.691334 -1.28633 -1.2082634 -1.5765715 -1.7112112 -1.6660576 -1.7065976 -1.8180554 -1.8102026 -1.6445625 -1.517029 -1.7874022][-2.528738 -2.8142021 -2.7663584 -2.4763117 -2.1612735 -2.0586815 -2.2931833 -2.407016 -2.3726413 -2.3186369 -2.3114593 -2.2797806 -2.0970323 -1.9328833 -2.0726931][-2.6200218 -3.0359752 -3.273159 -3.3211958 -3.1879351 -3.0684378 -3.0950937 -3.0794117 -2.9682889 -2.7651856 -2.5962548 -2.4714868 -2.3038561 -2.2238054 -2.3082964]]...]
INFO - root - 2017-12-07 03:43:41.210206: step 5510, loss = 0.76, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 75h:50m:42s remains)
INFO - root - 2017-12-07 03:43:49.433110: step 5520, loss = 0.64, batch loss = 0.56 (9.0 examples/sec; 0.886 sec/batch; 80h:30m:36s remains)
INFO - root - 2017-12-07 03:43:57.764238: step 5530, loss = 0.73, batch loss = 0.66 (9.8 examples/sec; 0.814 sec/batch; 73h:53m:41s remains)
INFO - root - 2017-12-07 03:44:06.116330: step 5540, loss = 0.85, batch loss = 0.78 (10.0 examples/sec; 0.802 sec/batch; 72h:52m:54s remains)
INFO - root - 2017-12-07 03:44:14.475271: step 5550, loss = 0.78, batch loss = 0.71 (9.8 examples/sec; 0.816 sec/batch; 74h:05m:58s remains)
INFO - root - 2017-12-07 03:44:22.629674: step 5560, loss = 1.03, batch loss = 0.96 (10.0 examples/sec; 0.800 sec/batch; 72h:39m:08s remains)
INFO - root - 2017-12-07 03:44:31.093241: step 5570, loss = 0.69, batch loss = 0.61 (9.2 examples/sec; 0.869 sec/batch; 78h:56m:05s remains)
INFO - root - 2017-12-07 03:44:39.304403: step 5580, loss = 0.80, batch loss = 0.73 (9.9 examples/sec; 0.807 sec/batch; 73h:17m:39s remains)
INFO - root - 2017-12-07 03:44:47.531249: step 5590, loss = 0.97, batch loss = 0.90 (10.0 examples/sec; 0.797 sec/batch; 72h:19m:50s remains)
INFO - root - 2017-12-07 03:44:55.916069: step 5600, loss = 0.72, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 73h:52m:31s remains)
2017-12-07 03:44:56.568039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9095368 -2.8763595 -2.674525 -2.4478912 -2.4445357 -2.5314026 -2.6462021 -2.8314686 -3.07487 -3.3405056 -3.5590098 -3.6370835 -3.5015788 -3.2767863 -3.0985789][-3.0075831 -2.9637737 -2.6906366 -2.3620288 -2.2667718 -2.2117786 -2.1494875 -2.2393107 -2.5403085 -2.9893675 -3.5234177 -3.8772955 -3.8375964 -3.5804815 -3.2783208][-3.0885725 -3.0646536 -2.8207517 -2.4498115 -2.1863804 -1.822197 -1.3837483 -1.250967 -1.5381665 -2.1251404 -3.0572524 -3.8474991 -4.0548673 -3.8673151 -3.4708893][-3.0127254 -3.0294743 -2.8979447 -2.546366 -2.1552775 -1.5662601 -0.815583 -0.47551513 -0.61700368 -1.1090934 -2.3047853 -3.543591 -4.0935564 -4.083868 -3.6678996][-2.7751887 -2.8536878 -2.8758078 -2.5809324 -2.1748722 -1.5533128 -0.67384887 -0.19707918 -0.014365196 -0.14213276 -1.4096379 -3.014183 -3.9244459 -4.1576018 -3.8087969][-2.496284 -2.6166134 -2.750308 -2.4601092 -2.0483947 -1.4618154 -0.58809233 0.0037703514 0.65740633 0.96657562 -0.36949062 -2.2858176 -3.5356343 -4.0339108 -3.8472102][-2.2008398 -2.3182013 -2.4693532 -2.1003683 -1.654995 -1.0940423 -0.26995659 0.42089653 1.5269899 2.1819744 0.74494123 -1.3902586 -2.9031024 -3.6411872 -3.6528742][-2.0046935 -2.0706222 -2.126986 -1.6337497 -1.2014134 -0.73051524 -0.031450272 0.65701532 1.995924 2.7712412 1.3320479 -0.72633052 -2.2570581 -3.1129751 -3.3006473][-2.0268867 -2.0179582 -1.8859122 -1.2875476 -0.937742 -0.63539171 -0.19200325 0.3197422 1.5742631 2.3509774 1.2379298 -0.3969717 -1.7783217 -2.7107863 -3.0427756][-2.3430536 -2.2902515 -1.9890575 -1.343323 -1.071058 -0.93134403 -0.79301882 -0.51974487 0.5501914 1.3873243 0.82962036 -0.25211191 -1.4658225 -2.476141 -2.9171152][-2.7741213 -2.716048 -2.3655896 -1.7676215 -1.530654 -1.5066562 -1.6046431 -1.4620249 -0.51704979 0.41908598 0.44558 -0.19079304 -1.3181691 -2.4028907 -2.9082594][-3.0615427 -3.0529659 -2.7839887 -2.3016121 -2.0680697 -2.0860379 -2.3093305 -2.2212629 -1.3989656 -0.43166804 0.0056004524 -0.31923103 -1.3500125 -2.4355161 -2.9692173][-3.2044344 -3.2575202 -3.08912 -2.7051067 -2.4581838 -2.4785876 -2.7297969 -2.6835096 -2.0847654 -1.2774794 -0.70830822 -0.82612967 -1.6594839 -2.6162379 -3.1279035][-3.2528429 -3.3571486 -3.2828693 -2.9865682 -2.761517 -2.8105164 -3.0698786 -3.0805311 -2.7251205 -2.1435711 -1.6081541 -1.5976975 -2.1703565 -2.89426 -3.2948282][-3.2132332 -3.339139 -3.3471296 -3.1773465 -3.0396962 -3.1391442 -3.387234 -3.4300582 -3.2469897 -2.8736966 -2.4465976 -2.3429317 -2.6380343 -3.0742466 -3.3164454]]...]
INFO - root - 2017-12-07 03:45:04.864281: step 5610, loss = 0.97, batch loss = 0.90 (9.4 examples/sec; 0.852 sec/batch; 77h:21m:49s remains)
INFO - root - 2017-12-07 03:45:13.312505: step 5620, loss = 0.68, batch loss = 0.60 (10.1 examples/sec; 0.790 sec/batch; 71h:41m:59s remains)
INFO - root - 2017-12-07 03:45:21.671191: step 5630, loss = 0.98, batch loss = 0.91 (9.7 examples/sec; 0.824 sec/batch; 74h:49m:18s remains)
INFO - root - 2017-12-07 03:45:29.968903: step 5640, loss = 0.77, batch loss = 0.70 (9.5 examples/sec; 0.844 sec/batch; 76h:39m:20s remains)
INFO - root - 2017-12-07 03:45:38.452678: step 5650, loss = 0.70, batch loss = 0.62 (9.3 examples/sec; 0.859 sec/batch; 77h:57m:17s remains)
INFO - root - 2017-12-07 03:45:46.846736: step 5660, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 0.873 sec/batch; 79h:17m:01s remains)
INFO - root - 2017-12-07 03:45:55.113472: step 5670, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.799 sec/batch; 72h:32m:02s remains)
INFO - root - 2017-12-07 03:46:03.507249: step 5680, loss = 0.71, batch loss = 0.64 (9.7 examples/sec; 0.824 sec/batch; 74h:48m:20s remains)
INFO - root - 2017-12-07 03:46:11.799136: step 5690, loss = 0.60, batch loss = 0.53 (9.6 examples/sec; 0.834 sec/batch; 75h:41m:00s remains)
INFO - root - 2017-12-07 03:46:20.230186: step 5700, loss = 0.68, batch loss = 0.61 (9.5 examples/sec; 0.844 sec/batch; 76h:35m:53s remains)
2017-12-07 03:46:20.891060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5916071 -1.9665658 -1.2671576 -0.6869607 -0.48810887 -0.699564 -1.0163949 -1.170471 -1.2056346 -1.1322691 -1.1395252 -1.4287112 -1.5808601 -1.4912722 -1.1824734][-2.7636902 -2.1794386 -1.4250052 -0.69541264 -0.30423641 -0.42230797 -0.77183104 -0.884784 -0.79191947 -0.64762855 -0.65202856 -0.93147779 -1.0505471 -0.98248887 -0.78044581][-2.9374743 -2.3231776 -1.4750936 -0.63133883 -0.15596008 -0.27129889 -0.67577624 -0.75374675 -0.62525153 -0.54424572 -0.55269122 -0.72557235 -0.7486012 -0.664407 -0.5408721][-3.0332706 -2.4639764 -1.6630657 -0.88162971 -0.47487521 -0.61000991 -0.999563 -1.0383079 -0.96432829 -1.0323577 -1.0658553 -1.1231394 -1.0501721 -0.92061305 -0.82716322][-3.0556583 -2.6436043 -2.0417883 -1.4640741 -1.1979752 -1.3254085 -1.6541715 -1.7024064 -1.7236307 -1.9011521 -1.922708 -1.8739929 -1.7123065 -1.5081546 -1.3982463][-3.0612898 -2.8016157 -2.3954515 -2.0261726 -1.9138339 -2.0576987 -2.3633509 -2.4590468 -2.540791 -2.7317486 -2.7016735 -2.5781481 -2.3550663 -2.1049025 -1.9765537][-3.0460844 -2.8547282 -2.5222669 -2.2453568 -2.2326536 -2.4361539 -2.773632 -2.8991008 -2.981411 -3.1225471 -3.0786505 -2.9681985 -2.7613897 -2.5423903 -2.4466038][-3.0198879 -2.7976902 -2.3908029 -2.0590622 -2.0563726 -2.327961 -2.7370729 -2.8890414 -2.9401684 -3.0336876 -3.0335298 -3.015166 -2.894021 -2.7706008 -2.7422814][-2.9938345 -2.664171 -2.0862694 -1.6026905 -1.566437 -1.9190705 -2.4169159 -2.5948029 -2.6315637 -2.7145233 -2.7856722 -2.8728838 -2.8603866 -2.8388946 -2.8718338][-2.905802 -2.4457552 -1.6949344 -1.0687649 -0.99530935 -1.4183469 -1.9945545 -2.2308767 -2.3097937 -2.428339 -2.5649958 -2.7350054 -2.8198516 -2.8757043 -2.943347][-2.7865448 -2.2655306 -1.4456277 -0.75817752 -0.6595962 -1.0894396 -1.6786578 -1.9691212 -2.0988605 -2.2429872 -2.4179821 -2.6404209 -2.7976451 -2.898066 -2.982749][-2.7618506 -2.288204 -1.5094521 -0.82606769 -0.68306732 -1.041368 -1.5750422 -1.8793592 -2.0190198 -2.1578372 -2.3488846 -2.5883226 -2.7753272 -2.8895411 -2.9753113][-2.8545814 -2.4893475 -1.8114557 -1.1852317 -0.99468088 -1.2558732 -1.6912317 -1.968951 -2.0925064 -2.2202713 -2.4106996 -2.6269555 -2.7994242 -2.9013448 -2.970582][-2.9552939 -2.7060046 -2.143759 -1.621321 -1.4189413 -1.6045399 -1.9394236 -2.160799 -2.2494323 -2.3630993 -2.5409157 -2.7192492 -2.8624969 -2.945085 -2.9913945][-2.9812665 -2.8165255 -2.3438864 -1.9203501 -1.7380247 -1.8931859 -2.1755359 -2.3509874 -2.3959765 -2.4799652 -2.6347256 -2.7802458 -2.8997445 -2.9667559 -3.0015166]]...]
INFO - root - 2017-12-07 03:46:28.889519: step 5710, loss = 0.56, batch loss = 0.49 (9.5 examples/sec; 0.844 sec/batch; 76h:37m:37s remains)
INFO - root - 2017-12-07 03:46:37.215920: step 5720, loss = 0.71, batch loss = 0.64 (9.3 examples/sec; 0.864 sec/batch; 78h:23m:03s remains)
INFO - root - 2017-12-07 03:46:45.552749: step 5730, loss = 0.74, batch loss = 0.67 (9.2 examples/sec; 0.869 sec/batch; 78h:53m:20s remains)
INFO - root - 2017-12-07 03:46:53.838489: step 5740, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.787 sec/batch; 71h:25m:55s remains)
INFO - root - 2017-12-07 03:47:02.136929: step 5750, loss = 0.91, batch loss = 0.84 (9.5 examples/sec; 0.841 sec/batch; 76h:19m:35s remains)
INFO - root - 2017-12-07 03:47:10.416969: step 5760, loss = 0.97, batch loss = 0.90 (10.0 examples/sec; 0.801 sec/batch; 72h:42m:39s remains)
INFO - root - 2017-12-07 03:47:18.641179: step 5770, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:46m:51s remains)
INFO - root - 2017-12-07 03:47:26.851303: step 5780, loss = 0.88, batch loss = 0.81 (9.6 examples/sec; 0.835 sec/batch; 75h:46m:48s remains)
INFO - root - 2017-12-07 03:47:35.136329: step 5790, loss = 0.89, batch loss = 0.82 (9.2 examples/sec; 0.871 sec/batch; 79h:00m:56s remains)
INFO - root - 2017-12-07 03:47:43.460880: step 5800, loss = 1.05, batch loss = 0.98 (10.0 examples/sec; 0.801 sec/batch; 72h:39m:36s remains)
2017-12-07 03:47:44.117558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0009489 -3.9845254 -3.9210329 -3.7563577 -3.4129729 -3.2605953 -3.3415546 -3.4231243 -3.376699 -2.9392877 -2.3254368 -1.9703469 -1.8597684 -1.9636936 -2.0714128][-3.5338364 -3.5583045 -3.5489991 -3.4621871 -3.1783128 -3.0230374 -3.0521545 -3.153491 -3.2760634 -3.1534328 -2.8799417 -2.7351298 -2.6831694 -2.7591012 -2.8873124][-3.1968603 -3.1950188 -3.1649668 -3.1152694 -2.9247015 -2.8484526 -2.8531232 -2.8788064 -2.9816413 -2.983861 -2.964251 -3.0373521 -3.0666132 -3.1253877 -3.2284431][-3.0910094 -2.9925916 -2.7719183 -2.5333114 -2.2445569 -2.1379726 -2.1443281 -2.1880271 -2.3186674 -2.4025288 -2.5776405 -2.8645704 -3.0054045 -3.0923796 -3.1834447][-3.1245818 -2.9197967 -2.478519 -1.9821179 -1.436892 -1.1073198 -1.03355 -1.1692917 -1.4507618 -1.6945109 -2.0902834 -2.6234369 -2.9117324 -2.9941897 -3.0082645][-2.8220835 -2.6727059 -2.2352295 -1.6888463 -0.99782443 -0.4134593 -0.12439251 -0.16278648 -0.46853352 -0.76786065 -1.3017952 -2.1253235 -2.7601657 -3.0164838 -2.9871814][-2.2713892 -2.2003744 -1.8744943 -1.4727409 -0.93587327 -0.32987595 0.14853811 0.3242135 0.11011696 -0.11424923 -0.52765441 -1.4023259 -2.3830652 -3.0585821 -3.2691877][-2.2745023 -2.131129 -1.7571423 -1.4297233 -1.1249835 -0.68697834 -0.16436338 0.2176919 0.13510513 0.030097008 -0.04570961 -0.61343431 -1.6206341 -2.5665591 -3.01092][-2.7626178 -2.5462499 -2.0786848 -1.6928396 -1.4880321 -1.2242708 -0.83654857 -0.40409374 -0.26616335 -0.19588709 -0.0094447136 -0.2624259 -1.0586009 -1.9285033 -2.3778429][-3.3217475 -3.1372559 -2.7047799 -2.3232453 -2.1350296 -1.9803913 -1.79894 -1.4647949 -1.204761 -1.0290811 -0.73483634 -0.787693 -1.3324113 -2.0191395 -2.4015336][-3.6360383 -3.5614085 -3.305532 -3.0507226 -2.87631 -2.7505579 -2.7196963 -2.5848837 -2.370821 -2.193156 -1.922868 -1.8605897 -2.1791878 -2.6666822 -2.9315097][-3.7853827 -3.7797592 -3.6894481 -3.5718107 -3.4292433 -3.2870727 -3.2983086 -3.3380923 -3.30837 -3.2449524 -3.0802491 -2.9622135 -3.1367521 -3.4956295 -3.6604867][-3.8425288 -3.8826342 -3.8880413 -3.8761632 -3.7965775 -3.6349614 -3.5599833 -3.5967488 -3.6276212 -3.6268911 -3.5578344 -3.4581361 -3.5855522 -3.8768845 -4.0083256][-3.7510438 -3.839103 -3.9375687 -4.0357194 -4.0652809 -3.9783552 -3.8794322 -3.8787756 -3.865653 -3.8042288 -3.7143259 -3.6001577 -3.6481912 -3.7971616 -3.8444502][-3.5926991 -3.6716022 -3.7753572 -3.898211 -3.9984572 -4.033083 -4.0463319 -4.120285 -4.1494451 -4.1062531 -4.0163527 -3.8791161 -3.8123298 -3.7892706 -3.7298732]]...]
INFO - root - 2017-12-07 03:47:52.349841: step 5810, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 72h:16m:49s remains)
INFO - root - 2017-12-07 03:48:00.575064: step 5820, loss = 0.82, batch loss = 0.75 (9.5 examples/sec; 0.846 sec/batch; 76h:46m:56s remains)
INFO - root - 2017-12-07 03:48:08.861745: step 5830, loss = 0.56, batch loss = 0.49 (9.4 examples/sec; 0.854 sec/batch; 77h:31m:26s remains)
INFO - root - 2017-12-07 03:48:17.165739: step 5840, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 72h:50m:19s remains)
INFO - root - 2017-12-07 03:48:25.478208: step 5850, loss = 0.67, batch loss = 0.60 (10.3 examples/sec; 0.777 sec/batch; 70h:32m:12s remains)
INFO - root - 2017-12-07 03:48:33.768973: step 5860, loss = 0.84, batch loss = 0.77 (9.6 examples/sec; 0.832 sec/batch; 75h:28m:04s remains)
INFO - root - 2017-12-07 03:48:42.101652: step 5870, loss = 0.72, batch loss = 0.64 (10.1 examples/sec; 0.792 sec/batch; 71h:52m:39s remains)
INFO - root - 2017-12-07 03:48:50.496027: step 5880, loss = 0.68, batch loss = 0.60 (9.1 examples/sec; 0.876 sec/batch; 79h:31m:10s remains)
INFO - root - 2017-12-07 03:48:58.809971: step 5890, loss = 0.57, batch loss = 0.49 (9.6 examples/sec; 0.831 sec/batch; 75h:21m:22s remains)
INFO - root - 2017-12-07 03:49:07.074053: step 5900, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.779 sec/batch; 70h:39m:27s remains)
2017-12-07 03:49:07.705149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0962868 -2.3712966 -2.2758908 -2.039196 -1.8264465 -1.6362731 -1.5774593 -1.7635813 -1.9586864 -2.03165 -2.0633831 -2.0980818 -2.0179229 -1.8341537 -1.6846223][-3.0106955 -3.3832245 -3.3168745 -3.0455053 -2.7320013 -2.3946166 -2.2377844 -2.4458556 -2.7084858 -2.8188584 -2.8948157 -3.0178659 -3.0304315 -2.8878603 -2.7098947][-3.4570308 -3.7948675 -3.7164133 -3.4514718 -3.1083479 -2.7208736 -2.564594 -2.8626177 -3.2411895 -3.4348054 -3.5816331 -3.7782702 -3.8531621 -3.7191091 -3.4744205][-3.5469086 -3.8600647 -3.8084671 -3.5384879 -3.0854876 -2.5560861 -2.3606102 -2.7336249 -3.2446063 -3.6006789 -3.9253719 -4.2775879 -4.4607821 -4.3848948 -4.1264114][-3.3715246 -3.7148554 -3.7487822 -3.4608605 -2.8672688 -2.2088034 -2.0188942 -2.4745717 -3.102809 -3.6037898 -4.0828171 -4.5671673 -4.8522058 -4.8329515 -4.5581303][-2.8510211 -3.2127018 -3.406177 -3.2064123 -2.6113915 -1.9654305 -1.8235481 -2.2648332 -2.8351903 -3.2986627 -3.764843 -4.2442017 -4.5264244 -4.5049977 -4.2133613][-2.0028472 -2.2506595 -2.5303903 -2.4264867 -1.9320135 -1.4552145 -1.47228 -1.920027 -2.371912 -2.7119853 -3.0564852 -3.3936152 -3.5358877 -3.4263659 -3.106832][-1.4249132 -1.6516242 -2.0446308 -2.0351503 -1.6394577 -1.3528018 -1.5563776 -2.0631082 -2.4704542 -2.7493832 -3.0111017 -3.1717031 -3.0915992 -2.828609 -2.4503708][-1.1562309 -1.5159085 -2.0729468 -2.1144955 -1.7122262 -1.4415047 -1.6473792 -2.0943182 -2.4187939 -2.6590765 -2.9227114 -3.0634212 -2.9426975 -2.6714244 -2.3134894][-0.847626 -1.229212 -1.832305 -1.9219558 -1.627284 -1.5195994 -1.8295624 -2.2790217 -2.5297837 -2.6640091 -2.845 -2.9535723 -2.8400488 -2.5873883 -2.223289][-1.1065001 -1.4827957 -2.0823164 -2.240099 -2.127887 -2.2197216 -2.6397772 -3.1006131 -3.2945852 -3.3148711 -3.3590622 -3.3622437 -3.1854148 -2.9033065 -2.5401704][-1.5854552 -2.0147762 -2.5686884 -2.6942139 -2.5929508 -2.6469002 -2.9729123 -3.3364024 -3.4763403 -3.4725266 -3.5036306 -3.5223255 -3.3844755 -3.159009 -2.8764734][-1.7651868 -2.1627347 -2.6092908 -2.7098932 -2.6540642 -2.7025018 -2.9648509 -3.2616448 -3.3905854 -3.4139762 -3.4649076 -3.5071907 -3.4102244 -3.2276766 -2.9868121][-1.9647968 -2.241158 -2.5255563 -2.588155 -2.5903659 -2.6732311 -2.9132195 -3.1712198 -3.3036518 -3.3503075 -3.4067349 -3.4565217 -3.3941875 -3.2472482 -3.0331235][-2.0915232 -2.2609227 -2.414176 -2.4489708 -2.4879789 -2.587975 -2.7725439 -2.9450245 -3.0201106 -3.0323148 -3.0478592 -3.0690715 -3.0248549 -2.9181454 -2.7631633]]...]
INFO - root - 2017-12-07 03:49:16.147922: step 5910, loss = 0.62, batch loss = 0.55 (9.0 examples/sec; 0.885 sec/batch; 80h:18m:00s remains)
INFO - root - 2017-12-07 03:49:24.364838: step 5920, loss = 0.81, batch loss = 0.74 (9.3 examples/sec; 0.861 sec/batch; 78h:05m:15s remains)
INFO - root - 2017-12-07 03:49:32.628916: step 5930, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.818 sec/batch; 74h:14m:47s remains)
INFO - root - 2017-12-07 03:49:40.961163: step 5940, loss = 0.65, batch loss = 0.58 (9.4 examples/sec; 0.848 sec/batch; 76h:54m:30s remains)
INFO - root - 2017-12-07 03:49:49.297463: step 5950, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.816 sec/batch; 74h:01m:37s remains)
INFO - root - 2017-12-07 03:49:57.619363: step 5960, loss = 0.54, batch loss = 0.47 (9.6 examples/sec; 0.830 sec/batch; 75h:16m:16s remains)
INFO - root - 2017-12-07 03:50:05.946913: step 5970, loss = 0.63, batch loss = 0.56 (9.4 examples/sec; 0.853 sec/batch; 77h:20m:36s remains)
INFO - root - 2017-12-07 03:50:14.222852: step 5980, loss = 0.79, batch loss = 0.72 (9.0 examples/sec; 0.886 sec/batch; 80h:22m:22s remains)
INFO - root - 2017-12-07 03:50:22.412741: step 5990, loss = 0.84, batch loss = 0.77 (9.9 examples/sec; 0.808 sec/batch; 73h:17m:19s remains)
INFO - root - 2017-12-07 03:50:30.665370: step 6000, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.806 sec/batch; 73h:08m:25s remains)
2017-12-07 03:50:31.250593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5897028 -2.6211381 -2.6481364 -2.6459346 -2.6387033 -2.6498561 -2.6766338 -2.7440224 -2.8308997 -2.8624978 -2.8693178 -2.8543079 -2.7881188 -2.7199934 -2.6632528][-2.5759773 -2.625679 -2.6794233 -2.7099793 -2.731992 -2.7410598 -2.7159705 -2.6765659 -2.6623881 -2.658051 -2.7095752 -2.7737784 -2.7672553 -2.724031 -2.6724777][-2.5895963 -2.6592298 -2.7315078 -2.7801523 -2.790421 -2.7154872 -2.5096583 -2.2301319 -2.0432913 -2.0518322 -2.2698219 -2.5427756 -2.6884012 -2.7064929 -2.677573][-2.614383 -2.67882 -2.7122107 -2.7159929 -2.6629629 -2.4407368 -2.0027506 -1.4808025 -1.1943552 -1.3440208 -1.8745017 -2.4367516 -2.7219706 -2.7593775 -2.7120066][-2.6834307 -2.6795824 -2.5685287 -2.44486 -2.288275 -1.9176638 -1.3126218 -0.70261526 -0.49363351 -0.92578888 -1.8378229 -2.6289327 -2.9174933 -2.8647394 -2.7422581][-2.8355136 -2.7082748 -2.3703055 -2.0578249 -1.7807169 -1.2927523 -0.6051507 -0.078498363 -0.12039757 -0.9124 -2.1573765 -3.0256581 -3.1764436 -2.9605436 -2.7475452][-2.9759533 -2.7551851 -2.2282763 -1.7519641 -1.370353 -0.77589655 -0.075656414 0.23871565 -0.13821125 -1.2052531 -2.5621068 -3.3332138 -3.3097897 -2.9756458 -2.7194753][-2.9840305 -2.7465065 -2.1293375 -1.5573802 -1.0902798 -0.39142275 0.28424978 0.31765604 -0.40795422 -1.6083734 -2.8423371 -3.4088898 -3.2651253 -2.9101977 -2.6697125][-2.7131577 -2.5637774 -1.9366477 -1.3124738 -0.81056261 -0.098399639 0.46670675 0.19310713 -0.79772973 -2.0199554 -3.0130062 -3.3496656 -3.1556895 -2.847964 -2.6461821][-2.340385 -2.348491 -1.7211986 -1.012006 -0.45984674 0.18727922 0.55742645 0.04788208 -1.0744441 -2.2682211 -3.0386147 -3.1962435 -3.0079155 -2.7843213 -2.6393533][-2.2886484 -2.4130328 -1.7465138 -0.9093504 -0.31377125 0.16344118 0.29532337 -0.31177378 -1.3494647 -2.4233294 -3.0366287 -3.1042013 -2.9307733 -2.7526212 -2.6353683][-2.6601386 -2.8077171 -2.0787637 -1.1816349 -0.65452385 -0.44092059 -0.56979108 -1.1554968 -1.9157975 -2.6954584 -3.1247902 -3.1243997 -2.9398532 -2.7566276 -2.6391306][-3.0889235 -3.2286844 -2.5365498 -1.7355793 -1.3490691 -1.3268995 -1.6097591 -2.1118405 -2.5733008 -3.0022922 -3.2237208 -3.1741476 -2.9884465 -2.7967625 -2.6619048][-3.3179812 -3.4534988 -2.9421878 -2.3992324 -2.1742768 -2.1663134 -2.3495417 -2.632093 -2.8584146 -3.0793571 -3.2083154 -3.1709273 -3.0182014 -2.8435249 -2.6995811][-3.3309956 -3.4278526 -3.1133866 -2.8336256 -2.6933484 -2.5268626 -2.3906617 -2.3362541 -2.4154773 -2.6601236 -2.9162555 -3.0203307 -2.9627171 -2.8501537 -2.7235479]]...]
INFO - root - 2017-12-07 03:50:39.633662: step 6010, loss = 0.88, batch loss = 0.81 (9.5 examples/sec; 0.842 sec/batch; 76h:21m:23s remains)
INFO - root - 2017-12-07 03:50:47.494677: step 6020, loss = 0.81, batch loss = 0.74 (10.1 examples/sec; 0.795 sec/batch; 72h:07m:25s remains)
INFO - root - 2017-12-07 03:50:55.863326: step 6030, loss = 0.56, batch loss = 0.49 (9.8 examples/sec; 0.820 sec/batch; 74h:22m:22s remains)
INFO - root - 2017-12-07 03:51:04.161126: step 6040, loss = 0.66, batch loss = 0.58 (9.7 examples/sec; 0.823 sec/batch; 74h:37m:34s remains)
INFO - root - 2017-12-07 03:51:12.572927: step 6050, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 75h:54m:48s remains)
INFO - root - 2017-12-07 03:51:20.935721: step 6060, loss = 0.92, batch loss = 0.85 (9.0 examples/sec; 0.885 sec/batch; 80h:13m:27s remains)
INFO - root - 2017-12-07 03:51:29.216795: step 6070, loss = 0.66, batch loss = 0.58 (9.7 examples/sec; 0.821 sec/batch; 74h:24m:52s remains)
INFO - root - 2017-12-07 03:51:37.485502: step 6080, loss = 0.62, batch loss = 0.54 (9.8 examples/sec; 0.816 sec/batch; 73h:57m:45s remains)
INFO - root - 2017-12-07 03:51:45.746891: step 6090, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.827 sec/batch; 75h:00m:22s remains)
INFO - root - 2017-12-07 03:51:54.137695: step 6100, loss = 0.77, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 75h:49m:08s remains)
2017-12-07 03:51:54.854338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5758822 -3.4730179 -3.2853222 -3.0339389 -2.7989008 -2.6559153 -2.7341843 -2.8372922 -2.9073775 -3.0632932 -3.1612082 -3.1613984 -3.1212544 -3.1614234 -3.298059][-3.3689952 -3.1142073 -2.7657781 -2.3663812 -2.0288947 -1.8403304 -1.9388585 -2.075336 -2.1683109 -2.3360846 -2.4515829 -2.46735 -2.4397457 -2.5172489 -2.6541369][-3.1251335 -2.7755091 -2.3311579 -1.8457839 -1.4560158 -1.2328188 -1.3179624 -1.4554265 -1.566221 -1.7109666 -1.8230047 -1.8603013 -1.8758292 -1.993381 -2.0938337][-2.953223 -2.5918219 -2.1048539 -1.5233278 -1.0269992 -0.71071219 -0.72669792 -0.84153533 -0.96104264 -1.0804245 -1.1813762 -1.2458084 -1.341764 -1.5781341 -1.7421532][-2.8101885 -2.499423 -2.070051 -1.4846213 -0.964237 -0.64952135 -0.68658233 -0.85492158 -1.0546749 -1.2081745 -1.2648957 -1.2003229 -1.1791315 -1.3700578 -1.4797335][-2.7305408 -2.4414978 -2.0639238 -1.5287616 -1.0966227 -0.89664912 -0.95774388 -1.0835159 -1.2230496 -1.3043182 -1.2743185 -1.1209528 -1.0198605 -1.1321511 -1.1437829][-2.8200769 -2.545413 -2.1693795 -1.6351571 -1.2509708 -1.1480086 -1.2357473 -1.3098907 -1.3259308 -1.2616384 -1.0945051 -0.82370329 -0.59674644 -0.58296943 -0.53104067][-2.973325 -2.7703481 -2.4001606 -1.8423464 -1.4417286 -1.3881223 -1.5224056 -1.6115675 -1.6276686 -1.5556087 -1.3550236 -1.0408463 -0.72811055 -0.54130721 -0.33811092][-3.1001315 -3.0063281 -2.6665442 -2.0768485 -1.5830469 -1.3995032 -1.3921943 -1.4257433 -1.5391922 -1.6944973 -1.7453835 -1.6572087 -1.5415409 -1.4384017 -1.2408829][-3.2176068 -3.2275844 -2.9057636 -2.2495949 -1.6428499 -1.2964094 -1.1434891 -1.1765912 -1.4194074 -1.7730954 -2.0205753 -2.0978367 -2.1226144 -2.082201 -1.9437389][-3.233005 -3.3058391 -3.0196049 -2.4057994 -1.8872836 -1.5912063 -1.4737682 -1.6016886 -1.9032013 -2.251972 -2.5199983 -2.670788 -2.7773485 -2.8291936 -2.8073595][-3.237325 -3.3168755 -3.0785327 -2.5698509 -2.2008471 -2.0383062 -2.0320842 -2.2483351 -2.5316169 -2.7689369 -2.9718859 -3.1521535 -3.2944779 -3.3895698 -3.4502418][-3.2996159 -3.3584728 -3.1425967 -2.710284 -2.427954 -2.3516865 -2.4628038 -2.7250729 -2.9562705 -3.0420909 -3.079087 -3.1207466 -3.14052 -3.1784897 -3.2732253][-3.4401264 -3.4953194 -3.338562 -3.0583124 -2.9110775 -2.954335 -3.1652465 -3.4002485 -3.518687 -3.4583523 -3.3501 -3.2960873 -3.2851236 -3.3389637 -3.4653172][-3.6608944 -3.6943173 -3.5480871 -3.3428519 -3.2481537 -3.3362594 -3.5808995 -3.7773414 -3.8203151 -3.7105489 -3.5564778 -3.4548447 -3.4188881 -3.4516559 -3.5362239]]...]
INFO - root - 2017-12-07 03:52:03.312635: step 6110, loss = 0.59, batch loss = 0.52 (10.0 examples/sec; 0.798 sec/batch; 72h:23m:41s remains)
INFO - root - 2017-12-07 03:52:11.675370: step 6120, loss = 0.74, batch loss = 0.67 (9.7 examples/sec; 0.826 sec/batch; 74h:55m:26s remains)
INFO - root - 2017-12-07 03:52:20.119505: step 6130, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.821 sec/batch; 74h:25m:03s remains)
INFO - root - 2017-12-07 03:52:28.622445: step 6140, loss = 0.71, batch loss = 0.64 (9.0 examples/sec; 0.886 sec/batch; 80h:21m:22s remains)
INFO - root - 2017-12-07 03:52:36.940964: step 6150, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.828 sec/batch; 75h:04m:49s remains)
INFO - root - 2017-12-07 03:52:45.209784: step 6160, loss = 0.69, batch loss = 0.61 (9.8 examples/sec; 0.816 sec/batch; 73h:58m:05s remains)
INFO - root - 2017-12-07 03:52:53.649688: step 6170, loss = 0.75, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 78h:25m:16s remains)
INFO - root - 2017-12-07 03:53:01.963997: step 6180, loss = 0.83, batch loss = 0.76 (9.7 examples/sec; 0.823 sec/batch; 74h:35m:12s remains)
INFO - root - 2017-12-07 03:53:10.378888: step 6190, loss = 0.92, batch loss = 0.85 (9.0 examples/sec; 0.884 sec/batch; 80h:10m:10s remains)
INFO - root - 2017-12-07 03:53:18.890335: step 6200, loss = 0.62, batch loss = 0.55 (9.5 examples/sec; 0.839 sec/batch; 76h:02m:54s remains)
2017-12-07 03:53:19.543751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0074844 -1.0361788 -1.297111 -1.3561985 -1.162255 -0.855376 -0.53500152 -0.62902737 -0.676651 -0.47872186 -0.32292318 -0.3854537 -0.62804365 -0.94462371 -1.4453306][-1.6603489 -1.4351213 -1.3036149 -1.1575398 -0.87162089 -0.52033973 -0.35279608 -0.76594067 -1.0544856 -0.88571906 -0.69411254 -0.75664258 -0.99819112 -1.2903905 -1.6384027][-1.9070463 -1.4889891 -1.1898258 -1.032232 -0.7339561 -0.37190151 -0.36108541 -1.0976708 -1.6587832 -1.5217361 -1.2203879 -1.0219035 -1.0185249 -1.1676018 -1.290339][-1.887599 -1.6755056 -1.5788269 -1.4741914 -1.0174317 -0.52561092 -0.632987 -1.6373684 -2.3831275 -2.1240568 -1.5202391 -0.88461757 -0.58873987 -0.74782586 -0.88380218][-1.7962017 -1.8718605 -2.1126022 -2.1102142 -1.5511789 -0.943784 -1.011652 -2.0579627 -2.8112 -2.4192481 -1.6851447 -0.86785412 -0.51073265 -0.83321786 -1.0708497][-1.6904161 -1.798929 -2.0331278 -1.9578354 -1.4069865 -0.89169836 -1.0024514 -2.0022409 -2.6955183 -2.3587043 -1.8424704 -1.2442274 -1.0765173 -1.6435864 -1.9646299][-1.7564254 -1.8096879 -1.8509984 -1.6452441 -1.2190382 -0.92303824 -1.1241772 -2.0128331 -2.6107426 -2.44285 -2.2131205 -1.8334236 -1.7800231 -2.4441419 -2.8300574][-2.247659 -2.1945887 -2.1191447 -1.9740729 -1.7555652 -1.5862937 -1.6074772 -2.0924695 -2.4416254 -2.370326 -2.3723505 -2.1802692 -2.1667209 -2.7497442 -3.0991149][-3.1716557 -3.09272 -3.0290205 -3.0909021 -3.1123166 -3.0224302 -2.8099294 -2.7707305 -2.7060435 -2.521306 -2.5472479 -2.4321303 -2.3697288 -2.7620559 -2.9881167][-4.1559477 -4.2114906 -4.2547531 -4.4669132 -4.5776134 -4.4746656 -4.1037374 -3.7307744 -3.388268 -3.1036735 -3.0592666 -2.8297853 -2.5589075 -2.6977515 -2.816823][-4.3162241 -4.4802308 -4.5910387 -4.8194566 -4.9078746 -4.78032 -4.4073892 -4.0268307 -3.7713687 -3.7191205 -3.831171 -3.5682213 -3.087184 -2.9061322 -2.8637638][-3.4788477 -3.6729136 -3.8653338 -4.1379032 -4.2898917 -4.2860665 -4.0916576 -3.9220123 -3.9368689 -4.2003012 -4.53919 -4.3886971 -3.9157104 -3.6248729 -3.5105767][-2.7232685 -2.8323932 -2.9269872 -3.0763719 -3.184772 -3.2716947 -3.269187 -3.3161576 -3.5359349 -3.9220724 -4.3026519 -4.2491312 -3.94725 -3.8095305 -3.8748052][-2.48976 -2.5762548 -2.6362336 -2.6900184 -2.7039623 -2.7531009 -2.7758975 -2.8272128 -2.9852118 -3.2549813 -3.5202649 -3.4986439 -3.3345807 -3.3319316 -3.5234423][-2.1038382 -2.2692149 -2.4613125 -2.5955534 -2.6601388 -2.7688708 -2.822135 -2.8203511 -2.8160043 -2.8761747 -2.9952652 -2.9989758 -2.9688778 -3.0412109 -3.1816831]]...]
INFO - root - 2017-12-07 03:53:27.881252: step 6210, loss = 0.74, batch loss = 0.66 (9.3 examples/sec; 0.863 sec/batch; 78h:11m:27s remains)
INFO - root - 2017-12-07 03:53:36.113959: step 6220, loss = 0.83, batch loss = 0.76 (10.3 examples/sec; 0.773 sec/batch; 70h:04m:00s remains)
INFO - root - 2017-12-07 03:53:44.443179: step 6230, loss = 0.80, batch loss = 0.73 (9.6 examples/sec; 0.832 sec/batch; 75h:24m:17s remains)
INFO - root - 2017-12-07 03:53:52.706900: step 6240, loss = 0.65, batch loss = 0.58 (8.8 examples/sec; 0.908 sec/batch; 82h:18m:06s remains)
INFO - root - 2017-12-07 03:54:00.981859: step 6250, loss = 0.66, batch loss = 0.58 (9.6 examples/sec; 0.835 sec/batch; 75h:41m:34s remains)
INFO - root - 2017-12-07 03:54:09.337621: step 6260, loss = 0.90, batch loss = 0.83 (9.7 examples/sec; 0.823 sec/batch; 74h:37m:01s remains)
INFO - root - 2017-12-07 03:54:17.656369: step 6270, loss = 0.68, batch loss = 0.61 (9.3 examples/sec; 0.856 sec/batch; 77h:32m:56s remains)
INFO - root - 2017-12-07 03:54:25.972237: step 6280, loss = 0.77, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 72h:44m:55s remains)
INFO - root - 2017-12-07 03:54:34.446833: step 6290, loss = 0.77, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 80h:49m:30s remains)
INFO - root - 2017-12-07 03:54:42.858219: step 6300, loss = 0.96, batch loss = 0.88 (9.6 examples/sec; 0.836 sec/batch; 75h:46m:43s remains)
2017-12-07 03:54:43.494264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9767942 -4.1990833 -3.8768094 -3.6283238 -3.6705074 -3.6957531 -3.4487355 -3.1046641 -2.7598205 -2.5955644 -2.6266408 -2.6924586 -2.6279671 -2.4139109 -2.2131779][-3.9843004 -4.2840157 -4.13122 -4.0298529 -4.1212764 -4.1653452 -3.9834456 -3.6945829 -3.3723736 -3.2609928 -3.3459015 -3.3953242 -3.24326 -2.8464735 -2.387522][-3.6909094 -3.9432511 -3.8808305 -3.9006865 -4.0389619 -4.1271939 -4.01639 -3.7188163 -3.3687959 -3.3134649 -3.5195775 -3.6496196 -3.4667997 -2.9294047 -2.3145022][-3.1909797 -3.3760324 -3.3814354 -3.5100241 -3.7379012 -3.9309351 -3.9059017 -3.6008093 -3.2484689 -3.2491374 -3.5255618 -3.6635618 -3.4517598 -2.89791 -2.3102877][-2.9517851 -3.1184545 -3.1474414 -3.2517896 -3.4142761 -3.5404813 -3.4662819 -3.1018567 -2.7582231 -2.8467798 -3.1859679 -3.3428943 -3.1865954 -2.755722 -2.3128467][-2.6673093 -2.8731368 -2.9050055 -2.8939404 -2.8776879 -2.8087764 -2.5619733 -2.0574508 -1.6414435 -1.7391164 -2.1075437 -2.2943139 -2.2540517 -2.029114 -1.8193543][-1.531806 -1.7457855 -1.7754891 -1.6744254 -1.5234601 -1.3565373 -1.1009908 -0.64805532 -0.30009127 -0.43687224 -0.78475165 -0.9379077 -0.92144513 -0.81857872 -0.84650064][-0.027689457 -0.15855169 -0.12628174 0.044807911 0.19976139 0.34563351 0.49234009 0.71912384 0.85656452 0.62128782 0.25945187 0.087790966 0.085049152 0.12699413 -0.03747654][0.62554932 0.51709557 0.51485872 0.63511515 0.68089056 0.73649788 0.82303953 0.97758722 1.0899343 0.92338085 0.67982817 0.52076006 0.45138216 0.43471527 0.2739172][0.10162544 -0.065737247 -0.17680883 -0.18982601 -0.2784853 -0.31952477 -0.28302526 -0.11728144 0.084332466 0.11579275 0.1355691 0.11883688 0.10329294 0.1788168 0.17004347][-1.0389075 -1.2460721 -1.4060578 -1.4919505 -1.6314645 -1.7035265 -1.6831028 -1.4986789 -1.2774706 -1.1847243 -0.98122692 -0.800555 -0.67135072 -0.43368125 -0.24092197][-2.0068996 -2.1872704 -2.3353305 -2.4423313 -2.5877995 -2.6515737 -2.6010761 -2.3928757 -2.2570035 -2.2704597 -2.0175719 -1.6853449 -1.447618 -1.1558201 -0.8629365][-2.5240207 -2.6676779 -2.8010187 -2.9149852 -3.0528131 -3.1027532 -3.0290313 -2.8371193 -2.8169646 -2.9511175 -2.6882463 -2.2739606 -1.9657035 -1.6692235 -1.3367703][-2.9226153 -3.0528879 -3.201823 -3.3361752 -3.4549396 -3.4771557 -3.3776965 -3.18331 -3.1643405 -3.2831402 -3.0036047 -2.5152843 -2.0695755 -1.6998131 -1.3580217][-3.4777834 -3.5653791 -3.6829739 -3.7982213 -3.8791337 -3.8643367 -3.7435186 -3.5281734 -3.4173262 -3.3867002 -3.0679812 -2.4954858 -1.9065716 -1.4720993 -1.2029839]]...]
INFO - root - 2017-12-07 03:54:51.908012: step 6310, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.815 sec/batch; 73h:53m:13s remains)
INFO - root - 2017-12-07 03:55:00.396737: step 6320, loss = 0.76, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 75h:35m:22s remains)
INFO - root - 2017-12-07 03:55:08.438361: step 6330, loss = 0.60, batch loss = 0.53 (9.6 examples/sec; 0.829 sec/batch; 75h:08m:05s remains)
INFO - root - 2017-12-07 03:55:16.960144: step 6340, loss = 0.97, batch loss = 0.90 (9.1 examples/sec; 0.882 sec/batch; 79h:55m:03s remains)
INFO - root - 2017-12-07 03:55:25.305599: step 6350, loss = 0.67, batch loss = 0.60 (10.0 examples/sec; 0.797 sec/batch; 72h:09m:44s remains)
INFO - root - 2017-12-07 03:55:33.770422: step 6360, loss = 0.69, batch loss = 0.62 (9.3 examples/sec; 0.861 sec/batch; 77h:57m:52s remains)
INFO - root - 2017-12-07 03:55:42.127211: step 6370, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 73h:48m:10s remains)
INFO - root - 2017-12-07 03:55:50.431062: step 6380, loss = 0.69, batch loss = 0.62 (9.9 examples/sec; 0.807 sec/batch; 73h:05m:14s remains)
INFO - root - 2017-12-07 03:55:58.836074: step 6390, loss = 0.76, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:38m:29s remains)
INFO - root - 2017-12-07 03:56:07.154193: step 6400, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.817 sec/batch; 74h:00m:33s remains)
2017-12-07 03:56:07.853005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2701898 -2.4709911 -2.7009492 -2.9352541 -3.1353555 -3.293885 -3.3355222 -3.402297 -3.6355672 -3.87428 -3.9652069 -3.9774616 -4.0431008 -4.1139455 -4.0540748][-2.1617498 -2.2899833 -2.5302711 -2.8375058 -3.1224632 -3.3846643 -3.5119603 -3.5963693 -3.8141003 -4.0198207 -4.0504375 -3.9946346 -4.0042028 -4.0687022 -4.0309734][-2.362402 -2.44279 -2.6366632 -2.8847961 -3.09055 -3.2956724 -3.39808 -3.4513297 -3.6521165 -3.8932054 -3.9859934 -3.9816277 -4.005137 -4.0923667 -4.0881481][-2.7416139 -2.7921402 -2.9211526 -3.0600996 -3.1153634 -3.1860425 -3.1905687 -3.1661427 -3.3314061 -3.6494696 -3.880363 -3.9571774 -3.9794359 -4.0562229 -4.0610776][-3.1022511 -3.133945 -3.2297277 -3.2907715 -3.2432451 -3.2240777 -3.14991 -3.0420966 -3.1525097 -3.4963384 -3.794313 -3.8765416 -3.8419936 -3.8814516 -3.9045961][-3.1717148 -3.218472 -3.3328214 -3.3619671 -3.2573822 -3.1945777 -3.1142826 -3.00497 -3.0944614 -3.4198794 -3.7166705 -3.7498636 -3.6303749 -3.6387291 -3.7209716][-2.8270311 -2.9381204 -3.1214278 -3.1308284 -2.9532945 -2.8479218 -2.7848849 -2.7281909 -2.826189 -3.1427557 -3.4365714 -3.4792976 -3.3650782 -3.3895073 -3.533983][-2.3908632 -2.5711832 -2.8182521 -2.8127751 -2.5334353 -2.3281763 -2.2443626 -2.2491732 -2.3912685 -2.7154055 -3.0195503 -3.130239 -3.0939322 -3.1628528 -3.3368573][-2.3963583 -2.5821581 -2.8228054 -2.8142476 -2.4948072 -2.2078538 -2.0564682 -2.0425489 -2.180819 -2.4671538 -2.7404308 -2.8954077 -2.9140952 -2.9934645 -3.1474][-2.6878204 -2.8142962 -3.0011616 -3.0284991 -2.7927437 -2.5472021 -2.4004083 -2.3601394 -2.4573081 -2.6668925 -2.8773699 -3.0090063 -2.9988344 -3.0096517 -3.0826316][-2.5878499 -2.6377192 -2.7703071 -2.8411584 -2.7249289 -2.5773349 -2.5051622 -2.5017738 -2.59614 -2.7509503 -2.90231 -2.9916306 -2.9343424 -2.8533103 -2.8395143][-2.16516 -2.1629262 -2.2416096 -2.2994621 -2.22833 -2.1350682 -2.1061056 -2.1399789 -2.2303362 -2.3496323 -2.4551153 -2.5166268 -2.4558349 -2.3394628 -2.2629776][-1.9027464 -1.8620741 -1.8605464 -1.8519902 -1.7734153 -1.7015789 -1.6959176 -1.750365 -1.8226237 -1.8922982 -1.9601676 -2.0089033 -1.9713938 -1.866473 -1.7629225][-1.9023972 -1.8409519 -1.7964151 -1.7495739 -1.6853125 -1.6473167 -1.6540992 -1.7020602 -1.7423029 -1.76688 -1.8014119 -1.8379557 -1.835289 -1.7929504 -1.7395515][-1.9638796 -1.9066114 -1.8706689 -1.8357124 -1.7928021 -1.7691023 -1.7675512 -1.7863967 -1.7922771 -1.7935817 -1.8138211 -1.8449128 -1.8664684 -1.8743694 -1.8786249]]...]
INFO - root - 2017-12-07 03:56:16.283468: step 6410, loss = 0.64, batch loss = 0.56 (9.8 examples/sec; 0.814 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-07 03:56:24.738570: step 6420, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.812 sec/batch; 73h:35m:39s remains)
INFO - root - 2017-12-07 03:56:33.149409: step 6430, loss = 0.86, batch loss = 0.79 (9.5 examples/sec; 0.839 sec/batch; 75h:58m:11s remains)
INFO - root - 2017-12-07 03:56:41.368469: step 6440, loss = 0.75, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 71h:02m:16s remains)
INFO - root - 2017-12-07 03:56:49.909114: step 6450, loss = 0.63, batch loss = 0.56 (9.1 examples/sec; 0.880 sec/batch; 79h:43m:08s remains)
INFO - root - 2017-12-07 03:56:58.281774: step 6460, loss = 0.94, batch loss = 0.86 (8.7 examples/sec; 0.916 sec/batch; 82h:58m:59s remains)
INFO - root - 2017-12-07 03:57:06.621547: step 6470, loss = 0.86, batch loss = 0.79 (10.3 examples/sec; 0.777 sec/batch; 70h:22m:15s remains)
INFO - root - 2017-12-07 03:57:14.877490: step 6480, loss = 0.80, batch loss = 0.72 (9.9 examples/sec; 0.810 sec/batch; 73h:22m:05s remains)
INFO - root - 2017-12-07 03:57:23.253411: step 6490, loss = 0.78, batch loss = 0.70 (8.8 examples/sec; 0.909 sec/batch; 82h:17m:14s remains)
INFO - root - 2017-12-07 03:57:31.666345: step 6500, loss = 0.86, batch loss = 0.79 (9.6 examples/sec; 0.833 sec/batch; 75h:26m:29s remains)
2017-12-07 03:57:32.414980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1805596 -3.4378428 -3.613502 -3.4088235 -2.7244785 -2.2447453 -2.2331107 -2.3635013 -2.3452954 -2.1591594 -2.1056166 -2.145947 -2.624495 -3.2770576 -3.5220802][-3.3307393 -3.6216357 -3.7260122 -3.2862642 -2.2609479 -1.6531556 -1.8833699 -2.3058622 -2.2665467 -1.8816941 -1.7665701 -1.8334064 -2.4436989 -3.3180556 -3.6148157][-3.4397545 -3.6735611 -3.6220212 -3.0669112 -1.9212263 -1.2183192 -1.5674078 -2.1322646 -1.97878 -1.3594499 -1.2666166 -1.5054474 -2.3190091 -3.3189735 -3.6047218][-3.5694129 -3.6043217 -3.2686238 -2.7367702 -1.7633297 -1.0714798 -1.3913209 -1.8327332 -1.4601402 -0.73627853 -0.75432873 -1.2882595 -2.3008325 -3.3236375 -3.6032887][-3.7484593 -3.5172398 -2.8612847 -2.3988619 -1.6657197 -0.99260283 -1.2171266 -1.396837 -0.8388052 -0.15412045 -0.29395247 -1.0781248 -2.2309241 -3.2642112 -3.5939505][-3.9198256 -3.5288315 -2.6569042 -2.231616 -1.5826125 -0.90596223 -1.0780959 -0.97394466 -0.26736069 0.25666904 -0.024868488 -0.98370433 -2.2193696 -3.1740713 -3.5129378][-3.9119215 -3.493834 -2.6737418 -2.2660244 -1.5556076 -0.86181927 -0.96748447 -0.5765245 0.24736309 0.58404827 0.077381134 -1.118818 -2.3710165 -3.1149459 -3.3827443][-3.7394867 -3.3740797 -2.7750521 -2.3468065 -1.5460095 -0.85351777 -0.85343695 -0.29671192 0.53063393 0.73818064 0.038094997 -1.3438518 -2.4988883 -2.9496825 -3.1269636][-3.5751572 -3.2404714 -2.7676973 -2.1382205 -1.1902363 -0.53600931 -0.48060369 -0.0086584091 0.63888741 0.68731546 -0.20238304 -1.7131987 -2.6891119 -2.8524876 -2.9452147][-3.444953 -3.1017072 -2.6049924 -1.6994355 -0.67259812 -0.13128805 -0.063387871 0.2527194 0.60505867 0.39912319 -0.68749428 -2.1834381 -2.9307785 -2.8880372 -2.9476261][-3.4257565 -3.0171032 -2.3325026 -1.167021 -0.176579 0.15754461 0.17786312 0.3357234 0.42820406 0.011666775 -1.1163855 -2.4535489 -3.0426269 -2.9394608 -3.0175328][-3.4156771 -2.9090166 -1.9446776 -0.64692926 0.1880331 0.30350876 0.20054436 0.19087076 0.072831154 -0.46231365 -1.5148835 -2.6646309 -3.1888103 -3.1292036 -3.2428379][-3.3385458 -2.7677445 -1.6089687 -0.37283325 0.20103025 0.12915611 -0.082330227 -0.19004917 -0.44080281 -1.0627902 -2.0364947 -3.0123594 -3.4648631 -3.4459004 -3.571825][-3.1839933 -2.5557826 -1.3988557 -0.46504974 -0.19642687 -0.38559914 -0.63818145 -0.80022097 -1.1342027 -1.8003147 -2.6432817 -3.408534 -3.7560716 -3.7304072 -3.8126619][-3.0644121 -2.4785786 -1.5562341 -0.95080924 -0.82778978 -1.0205705 -1.2820439 -1.4561043 -1.8157327 -2.4428179 -3.1244385 -3.6718428 -3.9150047 -3.85662 -3.8515048]]...]
INFO - root - 2017-12-07 03:57:40.820491: step 6510, loss = 0.62, batch loss = 0.54 (9.6 examples/sec; 0.836 sec/batch; 75h:44m:01s remains)
INFO - root - 2017-12-07 03:57:49.303456: step 6520, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.836 sec/batch; 75h:42m:14s remains)
INFO - root - 2017-12-07 03:57:57.712498: step 6530, loss = 0.64, batch loss = 0.57 (9.6 examples/sec; 0.829 sec/batch; 75h:05m:06s remains)
INFO - root - 2017-12-07 03:58:06.165741: step 6540, loss = 0.67, batch loss = 0.60 (8.9 examples/sec; 0.902 sec/batch; 81h:39m:07s remains)
INFO - root - 2017-12-07 03:58:14.604574: step 6550, loss = 0.92, batch loss = 0.84 (8.9 examples/sec; 0.897 sec/batch; 81h:12m:50s remains)
INFO - root - 2017-12-07 03:58:22.924757: step 6560, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.816 sec/batch; 73h:54m:59s remains)
INFO - root - 2017-12-07 03:58:31.431759: step 6570, loss = 0.74, batch loss = 0.66 (9.5 examples/sec; 0.844 sec/batch; 76h:24m:48s remains)
INFO - root - 2017-12-07 03:58:39.842477: step 6580, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 72h:42m:08s remains)
INFO - root - 2017-12-07 03:58:48.319325: step 6590, loss = 0.77, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:50m:48s remains)
INFO - root - 2017-12-07 03:58:56.660946: step 6600, loss = 0.71, batch loss = 0.64 (9.7 examples/sec; 0.826 sec/batch; 74h:48m:26s remains)
2017-12-07 03:58:57.298290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2808719 -5.2663951 -4.5865293 -3.2540808 -2.0167332 -1.2894115 -0.84253478 -0.52429843 -0.00253582 0.63659286 0.84906006 0.93180466 1.2300067 1.0127935 -0.026817322][-5.0404053 -4.9917231 -4.2907882 -2.89426 -1.5712576 -0.95821953 -0.81326818 -0.79020858 -0.43609548 0.28339815 0.72157955 0.98490334 1.1981564 0.77023506 -0.34027576][-4.8153605 -4.7211151 -4.073637 -2.7884188 -1.5535412 -1.1342039 -1.275964 -1.5009115 -1.258508 -0.51096249 -0.00046825409 0.27228737 0.37042809 0.057842731 -0.61851382][-4.7350969 -4.6059828 -4.0504708 -2.9341421 -1.8499045 -1.6049206 -2.0017273 -2.4173057 -2.3072946 -1.779355 -1.4779065 -1.330138 -1.1224723 -0.88719511 -0.77384877][-4.7124062 -4.4700718 -3.9154496 -2.9398611 -2.0045125 -1.7984164 -2.1410394 -2.5005951 -2.586369 -2.5885134 -2.7793295 -2.8535042 -2.5199926 -1.8523438 -1.1890392][-4.7043419 -4.2280622 -3.513314 -2.630944 -1.8523004 -1.5639927 -1.53073 -1.5285652 -1.740598 -2.3149211 -3.0213838 -3.355391 -3.1740317 -2.6744368 -2.0996046][-4.6669621 -3.986757 -3.121418 -2.3851988 -1.8252809 -1.4554372 -1.0155308 -0.62703109 -0.83508277 -1.7595582 -2.8177161 -3.3968344 -3.4904392 -3.34128 -3.0468502][-4.5219502 -3.7392771 -2.8123999 -2.2989998 -2.0579536 -1.6683307 -0.95275474 -0.18665266 -0.1735878 -1.1827369 -2.4471552 -3.1938913 -3.4205418 -3.3976917 -3.249332][-4.42674 -3.5860982 -2.5720191 -2.2053854 -2.2929685 -2.0332131 -1.2264338 -0.089961052 0.31645584 -0.5691154 -1.947681 -2.8221605 -3.0445108 -2.9088483 -2.7461629][-4.49136 -3.6197734 -2.4474792 -2.0211217 -2.3020935 -2.3325179 -1.7066917 -0.44329262 0.22595358 -0.5175395 -1.8680503 -2.731056 -2.8472829 -2.4583807 -2.0916803][-4.7381058 -3.9146309 -2.6601639 -2.0794482 -2.3762288 -2.6536803 -2.328824 -1.2636948 -0.63021231 -1.2864242 -2.5297043 -3.2953773 -3.2073507 -2.4005287 -1.4979823][-5.071331 -4.4159508 -3.2252975 -2.4702 -2.5325923 -2.8289402 -2.7398593 -2.0204504 -1.5713353 -2.15725 -3.2510338 -3.8596411 -3.5223641 -2.2191238 -0.67611194][-5.2121878 -4.7399735 -3.6941953 -2.7553782 -2.3782122 -2.4703808 -2.5587046 -2.2305882 -2.0097482 -2.5285087 -3.437942 -3.883419 -3.4187169 -1.9210868 -0.137784][-5.0070109 -4.6816611 -3.8534646 -2.8784924 -2.1982181 -2.1457274 -2.4229426 -2.4730208 -2.45598 -2.871562 -3.5516961 -3.8920312 -3.5183735 -2.2891104 -0.80753732][-4.6634617 -4.4775004 -3.9687877 -3.2590585 -2.6818326 -2.6997776 -3.0794826 -3.3077431 -3.3494582 -3.5617762 -3.9373925 -4.1719365 -3.9748995 -3.2062743 -2.2219479]]...]
INFO - root - 2017-12-07 03:59:05.868208: step 6610, loss = 0.65, batch loss = 0.58 (9.6 examples/sec; 0.836 sec/batch; 75h:42m:58s remains)
INFO - root - 2017-12-07 03:59:14.190306: step 6620, loss = 0.74, batch loss = 0.66 (9.6 examples/sec; 0.837 sec/batch; 75h:45m:47s remains)
INFO - root - 2017-12-07 03:59:22.708263: step 6630, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 0.866 sec/batch; 78h:26m:05s remains)
INFO - root - 2017-12-07 03:59:30.957516: step 6640, loss = 0.78, batch loss = 0.71 (10.5 examples/sec; 0.759 sec/batch; 68h:43m:59s remains)
INFO - root - 2017-12-07 03:59:39.103263: step 6650, loss = 0.96, batch loss = 0.89 (9.5 examples/sec; 0.841 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-07 03:59:47.523154: step 6660, loss = 0.66, batch loss = 0.59 (9.2 examples/sec; 0.872 sec/batch; 78h:57m:32s remains)
INFO - root - 2017-12-07 03:59:55.845135: step 6670, loss = 0.83, batch loss = 0.76 (9.1 examples/sec; 0.882 sec/batch; 79h:51m:03s remains)
INFO - root - 2017-12-07 04:00:04.126827: step 6680, loss = 0.77, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 70h:06m:16s remains)
INFO - root - 2017-12-07 04:00:12.459394: step 6690, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.830 sec/batch; 75h:07m:11s remains)
INFO - root - 2017-12-07 04:00:20.633487: step 6700, loss = 0.86, batch loss = 0.79 (9.7 examples/sec; 0.825 sec/batch; 74h:39m:09s remains)
2017-12-07 04:00:21.339833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5863314 -3.5559869 -3.51915 -3.4935417 -3.5032296 -3.5179834 -3.6544859 -3.7502167 -3.7069266 -3.5144515 -3.4777131 -3.7261651 -3.9041123 -3.6331079 -3.0441442][-3.6164961 -3.5680482 -3.5216129 -3.5236325 -3.5943463 -3.6972704 -3.8948426 -4.039094 -3.9690967 -3.63411 -3.3985984 -3.5414467 -3.7237375 -3.5047863 -2.8696256][-3.6171823 -3.5644536 -3.5593758 -3.6492422 -3.83467 -4.058526 -4.2913756 -4.4455905 -4.3324323 -3.8579397 -3.3183544 -3.1752646 -3.2689071 -3.1990178 -2.74826][-3.62402 -3.5919585 -3.6191416 -3.7180891 -3.8906453 -4.0941477 -4.2370658 -4.3187051 -4.1909456 -3.7328305 -3.0801969 -2.724556 -2.7385848 -2.8390908 -2.6774][-3.6198769 -3.5799499 -3.5628624 -3.5345712 -3.5114312 -3.4999766 -3.4024754 -3.3576078 -3.3581681 -3.2232206 -2.8157132 -2.506237 -2.4698868 -2.5981519 -2.5917974][-3.5641294 -3.4797521 -3.383961 -3.1880307 -2.896821 -2.5404623 -2.1188858 -1.9633217 -2.196635 -2.5060315 -2.4695895 -2.3361607 -2.268491 -2.3266239 -2.3571291][-3.4656901 -3.3267145 -3.1455488 -2.749568 -2.1417682 -1.4296381 -0.75916481 -0.61370182 -1.1610699 -1.8926649 -2.1666293 -2.1679821 -2.0523334 -1.9830616 -2.0220325][-3.3255079 -3.1234074 -2.8408973 -2.2169969 -1.3119977 -0.34354496 0.39721823 0.37441778 -0.51452565 -1.5392127 -1.9906924 -2.06945 -1.9435623 -1.8123112 -1.904067][-3.0907364 -2.8362391 -2.4807773 -1.7152412 -0.65178537 0.38451529 0.97379684 0.62979984 -0.51114416 -1.5724206 -1.9844987 -2.0204451 -1.8552704 -1.7369702 -1.9484231][-2.8525438 -2.5972209 -2.28243 -1.6016798 -0.66769838 0.16634655 0.46126175 -0.0901618 -1.1252086 -1.8861721 -2.1179426 -2.0988576 -1.8639121 -1.7136858 -2.0338387][-2.7827539 -2.5413392 -2.3015404 -1.8318317 -1.2175188 -0.67968559 -0.62167692 -1.1346962 -1.7650778 -2.0906918 -2.1775389 -2.2175426 -2.0070779 -1.8749144 -2.2792318][-2.8724873 -2.6134522 -2.3959205 -2.0813506 -1.7284541 -1.4119093 -1.4627068 -1.7948966 -2.0197749 -2.0427692 -2.1047859 -2.2823856 -2.2398119 -2.2470026 -2.695581][-3.0410216 -2.768621 -2.5329099 -2.2802212 -2.0518243 -1.8670847 -1.9859869 -2.1813645 -2.1908991 -2.1092238 -2.1701388 -2.4034328 -2.5222688 -2.6763396 -3.1254802][-3.233089 -2.9971571 -2.7648785 -2.5099885 -2.3204157 -2.2173796 -2.3983963 -2.5644467 -2.5534315 -2.4871712 -2.5192165 -2.6969829 -2.8512852 -3.0566046 -3.4455729][-3.4146194 -3.2511468 -3.0447443 -2.7803607 -2.5907049 -2.5244389 -2.7286012 -2.9067268 -2.9583263 -2.9891927 -3.0396483 -3.1282978 -3.2242818 -3.3983269 -3.6790535]]...]
INFO - root - 2017-12-07 04:00:29.632923: step 6710, loss = 0.66, batch loss = 0.58 (9.7 examples/sec; 0.823 sec/batch; 74h:26m:36s remains)
INFO - root - 2017-12-07 04:00:37.981990: step 6720, loss = 0.72, batch loss = 0.64 (9.8 examples/sec; 0.818 sec/batch; 74h:03m:24s remains)
INFO - root - 2017-12-07 04:00:46.253771: step 6730, loss = 0.54, batch loss = 0.46 (9.7 examples/sec; 0.828 sec/batch; 74h:58m:06s remains)
INFO - root - 2017-12-07 04:00:54.693615: step 6740, loss = 0.67, batch loss = 0.59 (9.3 examples/sec; 0.861 sec/batch; 77h:54m:04s remains)
INFO - root - 2017-12-07 04:01:02.983144: step 6750, loss = 1.02, batch loss = 0.95 (9.9 examples/sec; 0.807 sec/batch; 73h:02m:09s remains)
INFO - root - 2017-12-07 04:01:11.304173: step 6760, loss = 0.77, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 76h:35m:33s remains)
INFO - root - 2017-12-07 04:01:19.690622: step 6770, loss = 0.68, batch loss = 0.61 (9.2 examples/sec; 0.869 sec/batch; 78h:35m:17s remains)
INFO - root - 2017-12-07 04:01:27.933968: step 6780, loss = 0.78, batch loss = 0.70 (10.3 examples/sec; 0.774 sec/batch; 70h:01m:19s remains)
INFO - root - 2017-12-07 04:01:36.257913: step 6790, loss = 0.92, batch loss = 0.85 (9.6 examples/sec; 0.838 sec/batch; 75h:46m:34s remains)
INFO - root - 2017-12-07 04:01:44.620090: step 6800, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.835 sec/batch; 75h:32m:12s remains)
2017-12-07 04:01:45.288598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2305067 -3.103725 -2.8904772 -2.6035039 -2.3759596 -2.2832131 -2.3816786 -2.6620302 -2.9697266 -3.1583738 -3.3321629 -3.3692403 -3.2691131 -3.2188845 -3.2932894][-3.2895837 -3.3280687 -3.2054257 -2.86193 -2.4424021 -2.0156224 -1.7985802 -1.830754 -1.9892776 -2.153832 -2.4279828 -2.6397781 -2.7572885 -2.8861945 -3.0373838][-3.2064657 -3.4154394 -3.4739332 -3.2002511 -2.7082014 -2.0913596 -1.6907327 -1.4893537 -1.4097757 -1.5334866 -1.8455384 -2.0926983 -2.2480516 -2.3845084 -2.5159233][-3.1102481 -3.4731956 -3.7019529 -3.5033956 -2.9808393 -2.3183391 -1.89657 -1.563637 -1.2585638 -1.3038263 -1.5238183 -1.6530645 -1.7144189 -1.7948699 -1.9130409][-2.887285 -3.2590902 -3.5525327 -3.4088693 -2.9878213 -2.5447135 -2.3482766 -2.0454364 -1.6637781 -1.614073 -1.6138296 -1.5340395 -1.5049906 -1.5051081 -1.5237546][-2.7485502 -3.0096588 -3.2332013 -3.0756001 -2.7697968 -2.5458407 -2.5564561 -2.3697841 -2.1046023 -2.0678072 -1.9657915 -1.8464181 -1.8505647 -1.7521946 -1.5966072][-2.7442441 -2.9177845 -3.09685 -2.956748 -2.7704191 -2.6306992 -2.5979304 -2.3242261 -2.0449541 -1.9376786 -1.8418026 -1.8889222 -2.0412161 -1.9340277 -1.709924][-2.6842384 -2.7333262 -2.7607174 -2.5642281 -2.45497 -2.3325281 -2.1516552 -1.7533524 -1.4445934 -1.2922513 -1.3084705 -1.6049676 -1.9162087 -1.8900135 -1.7444694][-2.6914697 -2.6590428 -2.4178724 -1.9824905 -1.7683666 -1.5740387 -1.2634783 -0.83964467 -0.60006714 -0.50685763 -0.66753912 -1.1097462 -1.511436 -1.6316502 -1.679183][-2.6417019 -2.5290654 -2.0680733 -1.4977353 -1.2361505 -1.0597639 -0.82129169 -0.52079272 -0.36678839 -0.29565954 -0.45443082 -0.81463528 -1.1797283 -1.3725221 -1.50382][-2.612361 -2.4707603 -2.0432234 -1.6327424 -1.4865096 -1.4412506 -1.3976045 -1.3114948 -1.2302492 -1.1552629 -1.2149491 -1.379379 -1.5736063 -1.6716197 -1.7265327][-2.7175157 -2.6174736 -2.3793929 -2.2299218 -2.2361021 -2.2966068 -2.4011309 -2.4466162 -2.4067054 -2.3287711 -2.2521076 -2.1870611 -2.1612487 -2.1565697 -2.1892226][-2.8248496 -2.7689123 -2.6889963 -2.7314639 -2.8781557 -3.0492978 -3.2415676 -3.3136852 -3.2607889 -3.1378906 -2.9702568 -2.805964 -2.714035 -2.7064648 -2.7646904][-2.9319811 -2.9257972 -2.9366627 -3.0431557 -3.2096329 -3.374584 -3.5214174 -3.5426702 -3.4836373 -3.3945417 -3.2942653 -3.2050641 -3.1613798 -3.1613717 -3.1803384][-2.9391258 -2.9626973 -3.0110033 -3.1046894 -3.2197061 -3.3141322 -3.3802037 -3.3723359 -3.3418779 -3.3213391 -3.3111541 -3.3010716 -3.2853613 -3.252969 -3.2125401]]...]
INFO - root - 2017-12-07 04:01:53.627820: step 6810, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 0.867 sec/batch; 78h:26m:39s remains)
INFO - root - 2017-12-07 04:02:01.885255: step 6820, loss = 0.79, batch loss = 0.72 (9.6 examples/sec; 0.837 sec/batch; 75h:43m:15s remains)
INFO - root - 2017-12-07 04:02:10.225468: step 6830, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.826 sec/batch; 74h:41m:14s remains)
INFO - root - 2017-12-07 04:02:18.483526: step 6840, loss = 0.64, batch loss = 0.56 (9.5 examples/sec; 0.842 sec/batch; 76h:08m:49s remains)
INFO - root - 2017-12-07 04:02:26.988050: step 6850, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.809 sec/batch; 73h:10m:37s remains)
INFO - root - 2017-12-07 04:02:35.314013: step 6860, loss = 0.78, batch loss = 0.71 (9.3 examples/sec; 0.858 sec/batch; 77h:35m:04s remains)
INFO - root - 2017-12-07 04:02:43.640039: step 6870, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.826 sec/batch; 74h:44m:46s remains)
INFO - root - 2017-12-07 04:02:52.031208: step 6880, loss = 0.81, batch loss = 0.74 (9.7 examples/sec; 0.823 sec/batch; 74h:28m:16s remains)
INFO - root - 2017-12-07 04:03:00.484749: step 6890, loss = 0.68, batch loss = 0.60 (9.3 examples/sec; 0.860 sec/batch; 77h:45m:34s remains)
INFO - root - 2017-12-07 04:03:08.806461: step 6900, loss = 0.84, batch loss = 0.76 (9.9 examples/sec; 0.812 sec/batch; 73h:24m:00s remains)
2017-12-07 04:03:09.443122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6505244 -2.3851542 -2.5305076 -2.7272518 -2.6748433 -2.5180354 -2.532311 -2.9165816 -3.3510554 -3.4454381 -3.1378045 -2.8510847 -2.8985188 -2.8749733 -2.5765145][-2.5158563 -2.3019583 -2.6707251 -3.0937638 -3.0866323 -2.8895798 -2.8999033 -3.3319716 -3.793798 -3.862977 -3.5131783 -3.2492886 -3.2920396 -3.1347511 -2.6236181][-2.7142153 -2.6319613 -3.2223277 -3.8445997 -3.8330035 -3.4115081 -3.1332154 -3.3250365 -3.6340535 -3.6890662 -3.57416 -3.6069667 -3.7206228 -3.4026291 -2.6426196][-3.1933722 -3.2059972 -3.8138936 -4.4280367 -4.2758994 -3.4757037 -2.6969929 -2.5186093 -2.7111073 -2.9021506 -3.2121267 -3.67352 -3.9052196 -3.4863267 -2.5525532][-3.6564882 -3.6643782 -4.1356339 -4.5744886 -4.1963921 -2.9400854 -1.5387585 -1.08674 -1.5284288 -2.183722 -2.9860578 -3.8273947 -4.1854491 -3.8239684 -2.9456706][-3.922368 -3.7831168 -4.0343943 -4.2640967 -3.642467 -1.8782184 0.15988588 0.67147684 -0.46602178 -1.8619838 -3.0951371 -4.1169624 -4.5882545 -4.4138045 -3.7208407][-3.9019449 -3.5473742 -3.5425107 -3.6140425 -2.8251686 -0.70134449 1.7543378 2.1124229 0.21148157 -1.8585677 -3.2881002 -4.190814 -4.6831536 -4.7878761 -4.3664422][-3.7128952 -3.1542263 -2.9884906 -3.0959024 -2.4311426 -0.47190523 1.8080292 1.9788904 -0.11360312 -2.2840843 -3.5095944 -4.0008307 -4.4212284 -4.7959113 -4.6466689][-3.4535789 -2.7934327 -2.5631828 -2.7742767 -2.4556437 -1.1942115 0.33553505 0.32526064 -1.3459842 -2.9659867 -3.630121 -3.5766506 -3.7508831 -4.1942892 -4.2475972][-3.3429871 -2.7261491 -2.4387226 -2.5839591 -2.5091105 -1.9913623 -1.2760499 -1.5003788 -2.6612983 -3.5456321 -3.6298885 -3.100028 -2.9076867 -3.1895008 -3.2856257][-3.457761 -3.0670543 -2.7509491 -2.685308 -2.6463227 -2.5362191 -2.344065 -2.7265611 -3.479068 -3.7665257 -3.5406463 -2.8967824 -2.5288846 -2.6392884 -2.6637082][-3.6176493 -3.5659962 -3.3593812 -3.1297932 -2.9856935 -2.9596362 -2.9996176 -3.4018464 -3.7782266 -3.6739011 -3.4090211 -2.9664841 -2.7181175 -2.8687925 -2.8807397][-3.6796308 -3.9219871 -3.9185164 -3.6879311 -3.528585 -3.5084443 -3.5242047 -3.6773214 -3.6456056 -3.2810259 -3.0865974 -2.9555504 -2.9810424 -3.3331709 -3.4642191][-3.7337217 -4.1464415 -4.3412189 -4.2585235 -4.1835985 -4.1457987 -4.0415144 -3.8764668 -3.5309262 -3.11064 -3.0856209 -3.198173 -3.3703618 -3.7710257 -3.9294314][-3.8196156 -4.2508368 -4.5246453 -4.5998425 -4.6269875 -4.5543456 -4.3497238 -4.0583553 -3.6667356 -3.3730178 -3.5050087 -3.7344797 -3.9093935 -4.2237015 -4.3296447]]...]
INFO - root - 2017-12-07 04:03:17.894694: step 6910, loss = 0.71, batch loss = 0.64 (9.8 examples/sec; 0.813 sec/batch; 73h:34m:15s remains)
INFO - root - 2017-12-07 04:03:26.273902: step 6920, loss = 0.62, batch loss = 0.55 (9.6 examples/sec; 0.832 sec/batch; 75h:16m:08s remains)
INFO - root - 2017-12-07 04:03:34.587356: step 6930, loss = 0.63, batch loss = 0.55 (9.2 examples/sec; 0.867 sec/batch; 78h:25m:55s remains)
INFO - root - 2017-12-07 04:03:42.813874: step 6940, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.791 sec/batch; 71h:29m:46s remains)
INFO - root - 2017-12-07 04:03:51.227505: step 6950, loss = 0.84, batch loss = 0.77 (9.1 examples/sec; 0.882 sec/batch; 79h:44m:25s remains)
INFO - root - 2017-12-07 04:03:59.282073: step 6960, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 72h:38m:24s remains)
INFO - root - 2017-12-07 04:04:07.659216: step 6970, loss = 0.86, batch loss = 0.79 (9.6 examples/sec; 0.833 sec/batch; 75h:17m:27s remains)
INFO - root - 2017-12-07 04:04:15.914828: step 6980, loss = 0.74, batch loss = 0.67 (10.3 examples/sec; 0.780 sec/batch; 70h:30m:08s remains)
INFO - root - 2017-12-07 04:04:24.191578: step 6990, loss = 0.59, batch loss = 0.52 (9.5 examples/sec; 0.843 sec/batch; 76h:14m:23s remains)
INFO - root - 2017-12-07 04:04:32.548769: step 7000, loss = 0.71, batch loss = 0.64 (9.1 examples/sec; 0.877 sec/batch; 79h:16m:23s remains)
2017-12-07 04:04:33.194741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14548779 -0.45174146 -0.49325109 -0.47134638 -0.55284762 -0.73766923 -1.3732898 -2.0371654 -2.2212248 -2.1455686 -1.7550852 -1.4907913 -1.3530533 -1.218302 -1.2584314][-0.40511608 -0.65270782 -0.82265115 -0.94482541 -1.0570304 -1.1180871 -1.4692364 -1.7461443 -1.662883 -1.6661074 -1.6068239 -1.6053264 -1.5397658 -1.4223404 -1.5714293][-0.60000682 -0.78548837 -1.0598562 -1.2657571 -1.3162286 -1.1501305 -1.0579884 -0.87871695 -0.70923758 -1.0128624 -1.4032776 -1.572854 -1.4056296 -1.1907313 -1.3733702][-0.76371455 -0.90279508 -1.2794871 -1.5696814 -1.5976317 -1.2389095 -0.64780641 0.056052208 0.26035976 -0.40016842 -1.1763291 -1.3903108 -1.0157351 -0.69142962 -0.89138055][-0.90447736 -1.0443487 -1.4810524 -1.8303602 -1.9046862 -1.5284562 -0.58078623 0.59971762 0.83212376 -0.14892912 -1.1986785 -1.43309 -0.90893483 -0.46583319 -0.54224825][-0.9362309 -1.07619 -1.5639784 -2.0007932 -2.1996062 -1.8624959 -0.65107584 0.87412453 1.1007791 -0.12243605 -1.3498371 -1.675276 -1.1829901 -0.687526 -0.51467419][-0.66568732 -0.74832392 -1.2862756 -1.9059446 -2.3690724 -2.1676154 -0.79634237 0.9329648 1.10004 -0.3041563 -1.6190534 -2.0660295 -1.6878667 -1.1299131 -0.69679523][-0.29762554 -0.33854675 -0.9532001 -1.7369602 -2.3885431 -2.2584178 -0.82044744 0.90081453 0.90641117 -0.56754088 -1.8227932 -2.2958899 -1.9825647 -1.3553596 -0.82007742][-0.062730312 -0.237751 -1.0032372 -1.85815 -2.5298736 -2.3367009 -0.93019462 0.54194164 0.42057467 -0.8089118 -1.730809 -2.1071272 -1.8757987 -1.2857053 -0.75692248][0.021076679 -0.34427071 -1.1655207 -1.9611542 -2.5451245 -2.34385 -1.1699734 -0.031820297 -0.083086491 -0.849406 -1.3771939 -1.6914735 -1.5960832 -1.07318 -0.51184297][-0.017879009 -0.55935431 -1.2716627 -1.8591068 -2.3213499 -2.2126713 -1.3964565 -0.57510352 -0.47768974 -0.81755829 -1.1085045 -1.416882 -1.4098129 -0.90819168 -0.282269][-0.3793745 -0.92538214 -1.3521428 -1.6483939 -1.960779 -1.963094 -1.5292099 -0.98361158 -0.77377796 -0.84178925 -1.0582004 -1.3556688 -1.3419259 -0.83741236 -0.24056482][-1.0288615 -1.5725832 -1.6484015 -1.602632 -1.7393129 -1.8267801 -1.6815157 -1.3083398 -0.99705338 -0.92358327 -1.0898645 -1.2928708 -1.1408558 -0.5952599 -0.13647556][-1.5794303 -2.1262684 -1.9205999 -1.5813792 -1.5309832 -1.668371 -1.7063019 -1.3992736 -1.0432508 -0.95125389 -1.0812254 -1.157553 -0.86467314 -0.30023909 0.034156322][-1.8301971 -2.2640612 -1.9208868 -1.4256315 -1.2504723 -1.3985882 -1.5094144 -1.2121696 -0.90040088 -0.92584777 -1.0586479 -1.0060236 -0.56772971 0.016629219 0.30030155]]...]
INFO - root - 2017-12-07 04:04:41.444752: step 7010, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.813 sec/batch; 73h:30m:39s remains)
INFO - root - 2017-12-07 04:04:49.808942: step 7020, loss = 0.81, batch loss = 0.74 (9.5 examples/sec; 0.844 sec/batch; 76h:20m:10s remains)
INFO - root - 2017-12-07 04:04:58.185395: step 7030, loss = 0.66, batch loss = 0.59 (9.4 examples/sec; 0.850 sec/batch; 76h:50m:20s remains)
INFO - root - 2017-12-07 04:05:06.416457: step 7040, loss = 0.66, batch loss = 0.59 (9.5 examples/sec; 0.839 sec/batch; 75h:50m:17s remains)
INFO - root - 2017-12-07 04:05:14.829384: step 7050, loss = 0.61, batch loss = 0.54 (9.7 examples/sec; 0.824 sec/batch; 74h:28m:04s remains)
INFO - root - 2017-12-07 04:05:23.309661: step 7060, loss = 0.79, batch loss = 0.72 (9.4 examples/sec; 0.853 sec/batch; 77h:07m:21s remains)
INFO - root - 2017-12-07 04:05:31.743728: step 7070, loss = 0.82, batch loss = 0.75 (9.4 examples/sec; 0.849 sec/batch; 76h:42m:25s remains)
INFO - root - 2017-12-07 04:05:39.869562: step 7080, loss = 0.64, batch loss = 0.57 (10.2 examples/sec; 0.786 sec/batch; 71h:05m:38s remains)
INFO - root - 2017-12-07 04:05:48.265894: step 7090, loss = 0.95, batch loss = 0.87 (9.0 examples/sec; 0.887 sec/batch; 80h:11m:53s remains)
INFO - root - 2017-12-07 04:05:56.482915: step 7100, loss = 0.99, batch loss = 0.91 (9.2 examples/sec; 0.868 sec/batch; 78h:25m:49s remains)
2017-12-07 04:05:57.139731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1679928 -2.364759 -2.4280543 -1.9629607 -1.2701437 -1.1325784 -1.2702723 -1.4380755 -1.676403 -1.7671907 -1.9015803 -2.3144221 -2.5910153 -2.5297754 -2.3833098][-1.8410192 -2.1565828 -2.4197598 -2.0373495 -1.3768463 -1.1749475 -1.2003465 -1.355269 -1.6112339 -1.7199156 -1.8188367 -2.0984561 -2.2831407 -2.1845586 -2.0577953][-1.68117 -2.0066745 -2.356956 -2.0259011 -1.4309137 -1.2059343 -1.2478712 -1.5125329 -1.8073714 -2.018847 -2.2389107 -2.4743681 -2.5604186 -2.3516278 -2.1367791][-1.5568483 -1.7767708 -2.110553 -1.87005 -1.4395211 -1.3044035 -1.4810281 -1.8049619 -1.9972396 -2.2598352 -2.673306 -2.977071 -3.0508792 -2.8117075 -2.5233161][-1.5539744 -1.6572065 -1.8964975 -1.7857497 -1.5379705 -1.4203873 -1.5198731 -1.5664825 -1.3833845 -1.5633888 -2.1758821 -2.7109346 -2.9845479 -2.9188943 -2.718276][-1.5742054 -1.6755109 -1.7966115 -1.6771219 -1.4034982 -1.1760631 -1.0461071 -0.58155656 0.13325357 0.16371727 -0.65385294 -1.6683416 -2.3560562 -2.6498027 -2.8158603][-1.5744114 -1.8006804 -1.8072731 -1.5796008 -1.2574914 -1.1006043 -0.91404414 -0.035742283 1.193902 1.695992 0.94199705 -0.50424457 -1.6063621 -2.2832217 -2.91611][-1.4373703 -1.9570758 -2.0034072 -1.6461313 -1.2980289 -1.3959494 -1.4347327 -0.61488843 0.69848967 1.7152147 1.4588094 0.1229825 -0.97742057 -1.7787802 -2.696063][-1.1014504 -1.8615382 -2.0973763 -1.7775364 -1.4733276 -1.7845731 -2.0964739 -1.7338119 -0.76635122 0.50735331 0.85145378 0.16668653 -0.39942884 -0.98133516 -1.8778586][-0.73482275 -1.5202718 -1.8666394 -1.6677234 -1.485256 -1.9145458 -2.5380595 -2.7926214 -2.2016265 -0.85355306 -0.041125298 0.090987206 0.2831192 0.091813087 -0.60796976][-0.73707485 -1.1790416 -1.4243801 -1.3042455 -1.1479394 -1.4512625 -2.1907904 -2.9382563 -2.8555746 -1.9366009 -1.1617005 -0.48789573 0.35323334 0.68990946 0.44421291][-1.0382073 -0.81351519 -0.697701 -0.57328677 -0.4362514 -0.56943965 -1.1797056 -2.0202408 -2.3824875 -2.2729065 -2.0809379 -1.4062972 -0.30576181 0.40099716 0.59780359][-1.2619061 -0.58750892 -0.16588306 -0.02128315 0.0629611 -0.064884663 -0.52710986 -1.1536565 -1.6187801 -2.0650334 -2.3875847 -1.925451 -0.90189719 -0.095153332 0.30218792][-1.5146601 -0.8470118 -0.4814074 -0.42161655 -0.36687565 -0.45821309 -0.68459249 -0.85336161 -1.0062521 -1.4915648 -2.0577316 -1.890558 -1.1742883 -0.47803855 -0.049118042][-2.1493654 -1.7014141 -1.5262558 -1.5067816 -1.3780236 -1.3421385 -1.3038311 -0.98946452 -0.64620805 -0.76338673 -1.245698 -1.3319416 -1.1270339 -0.86313844 -0.64322281]]...]
INFO - root - 2017-12-07 04:06:05.443910: step 7110, loss = 1.03, batch loss = 0.96 (9.4 examples/sec; 0.847 sec/batch; 76h:31m:36s remains)
INFO - root - 2017-12-07 04:06:13.670183: step 7120, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.774 sec/batch; 69h:59m:57s remains)
INFO - root - 2017-12-07 04:06:22.024098: step 7130, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.845 sec/batch; 76h:21m:55s remains)
INFO - root - 2017-12-07 04:06:30.266304: step 7140, loss = 0.70, batch loss = 0.62 (9.1 examples/sec; 0.875 sec/batch; 79h:05m:11s remains)
INFO - root - 2017-12-07 04:06:38.542371: step 7150, loss = 0.72, batch loss = 0.65 (9.9 examples/sec; 0.812 sec/batch; 73h:23m:15s remains)
INFO - root - 2017-12-07 04:06:46.910217: step 7160, loss = 0.86, batch loss = 0.78 (9.4 examples/sec; 0.847 sec/batch; 76h:35m:06s remains)
INFO - root - 2017-12-07 04:06:55.208312: step 7170, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.826 sec/batch; 74h:38m:29s remains)
INFO - root - 2017-12-07 04:07:03.462605: step 7180, loss = 0.63, batch loss = 0.56 (10.5 examples/sec; 0.759 sec/batch; 68h:34m:40s remains)
INFO - root - 2017-12-07 04:07:11.920628: step 7190, loss = 0.76, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 77h:17m:21s remains)
INFO - root - 2017-12-07 04:07:20.223423: step 7200, loss = 0.69, batch loss = 0.61 (9.2 examples/sec; 0.867 sec/batch; 78h:21m:55s remains)
2017-12-07 04:07:20.853149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9644113 -3.4017098 -3.756299 -3.6914945 -3.4774561 -3.27358 -3.1485856 -3.1677821 -3.2582471 -3.2068009 -3.0841918 -2.8532734 -2.8520789 -3.2689571 -3.6930356][-2.7886019 -3.5052242 -4.0196352 -3.8997078 -3.5017567 -3.1071587 -2.8179798 -2.7508273 -2.7963345 -2.8498049 -3.0487437 -3.163259 -3.4255195 -3.9335771 -4.2766571][-2.7317028 -3.3510964 -3.6061018 -3.2230399 -2.6918118 -2.2211301 -1.8626375 -1.7776046 -1.7864802 -1.9343147 -2.4505124 -2.9406118 -3.4622469 -3.8884254 -3.913487][-2.6375647 -3.010428 -2.9796109 -2.487308 -2.0160437 -1.5495033 -1.0835202 -0.86680365 -0.75318527 -0.85250807 -1.5070217 -2.2518084 -2.9765244 -3.3186975 -3.0409474][-2.5569437 -2.8093433 -2.7973528 -2.5115762 -2.2345579 -1.7902763 -1.2322316 -0.9778533 -0.95237613 -1.0518756 -1.6155818 -2.3246841 -3.0031698 -3.2673116 -2.8694792][-2.6294053 -2.9203374 -3.0815926 -3.0820704 -2.8400183 -2.1624832 -1.3144977 -0.99038839 -1.4386911 -2.1103778 -2.8985019 -3.5717967 -3.975471 -3.96194 -3.4573853][-2.9756844 -3.2504086 -3.3915935 -3.3928666 -2.8472471 -1.6016881 -0.13152933 0.577302 -0.34707594 -1.9623556 -3.395319 -4.3456631 -4.5850248 -4.2746239 -3.6863909][-3.4895749 -3.7361147 -3.7735558 -3.6252923 -2.647974 -0.77289367 1.3773994 2.6570897 1.6093035 -0.74042678 -2.8562737 -4.2334409 -4.4713092 -3.8807886 -3.1007285][-3.4835052 -3.8617239 -3.9330389 -3.8079481 -2.8038127 -0.89111257 1.294179 2.7310133 1.9722757 -0.37304497 -2.6314945 -4.1025124 -4.2395124 -3.3581278 -2.3089521][-2.8109498 -3.3829138 -3.6056161 -3.6914992 -3.1859217 -2.0001135 -0.55152369 0.47848892 0.092396259 -1.6186857 -3.4689617 -4.6152315 -4.5113297 -3.38474 -2.1459572][-2.3223758 -2.9149523 -3.1463571 -3.2743416 -3.2258916 -2.8993645 -2.4150229 -2.072911 -2.2933915 -3.3029962 -4.5164838 -5.1246872 -4.764442 -3.6338446 -2.5107331][-2.5556197 -2.9540157 -3.04113 -3.00179 -3.042558 -3.1402972 -3.2345762 -3.4173245 -3.6750994 -4.2889743 -5.0351362 -5.2303019 -4.8146043 -4.0251703 -3.3138156][-3.3894658 -3.5843065 -3.5711102 -3.4099615 -3.38448 -3.5491834 -3.7526267 -4.0453773 -4.2870913 -4.6929431 -5.1353559 -5.0521531 -4.6459827 -4.224997 -3.9840167][-3.9868565 -4.1446838 -4.1864767 -4.0228777 -3.8628182 -3.8370585 -3.8497331 -4.0337 -4.2732797 -4.6066809 -4.872139 -4.6246877 -4.14167 -3.784837 -3.6547565][-3.9776521 -4.1767035 -4.33298 -4.261909 -4.03115 -3.8122649 -3.638551 -3.7082186 -3.9590905 -4.279912 -4.4494958 -4.1503205 -3.6040611 -3.159013 -2.9272413]]...]
INFO - root - 2017-12-07 04:07:29.268744: step 7210, loss = 0.76, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:03m:15s remains)
INFO - root - 2017-12-07 04:07:37.684230: step 7220, loss = 0.86, batch loss = 0.79 (9.6 examples/sec; 0.831 sec/batch; 75h:06m:35s remains)
INFO - root - 2017-12-07 04:07:46.040087: step 7230, loss = 0.83, batch loss = 0.75 (9.2 examples/sec; 0.865 sec/batch; 78h:09m:43s remains)
INFO - root - 2017-12-07 04:07:54.412199: step 7240, loss = 0.58, batch loss = 0.50 (10.0 examples/sec; 0.800 sec/batch; 72h:19m:03s remains)
INFO - root - 2017-12-07 04:08:02.782604: step 7250, loss = 0.70, batch loss = 0.63 (9.6 examples/sec; 0.830 sec/batch; 75h:01m:32s remains)
INFO - root - 2017-12-07 04:08:11.070826: step 7260, loss = 0.63, batch loss = 0.56 (9.2 examples/sec; 0.867 sec/batch; 78h:19m:11s remains)
INFO - root - 2017-12-07 04:08:19.066137: step 7270, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 70h:51m:04s remains)
INFO - root - 2017-12-07 04:08:27.430165: step 7280, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 76h:38m:56s remains)
INFO - root - 2017-12-07 04:08:35.731521: step 7290, loss = 0.92, batch loss = 0.85 (10.0 examples/sec; 0.802 sec/batch; 72h:24m:28s remains)
INFO - root - 2017-12-07 04:08:44.090169: step 7300, loss = 0.79, batch loss = 0.72 (9.0 examples/sec; 0.884 sec/batch; 79h:52m:36s remains)
2017-12-07 04:08:44.776498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393689 -4.4605861 -3.6414433 -2.5039907 -1.2127786 0.887589 2.3249044 1.7991424 0.67817163 -0.49992251 -1.3777113 -1.4515193 -1.0103679 -0.2859025 0.10778904][-4.2346 -4.4759641 -3.5990334 -2.35847 -1.125798 0.88261604 2.3745089 1.9849167 0.93296003 -0.19243193 -1.2280262 -1.4657726 -0.92650104 -0.015285015 0.47772408][-4.2404742 -4.5234585 -3.7338037 -2.4929986 -1.3635232 0.52590561 2.1256094 1.9127245 0.89196587 -0.10453224 -1.1258061 -1.455693 -0.8855803 0.065496922 0.55922985][-4.293282 -4.6495748 -4.106142 -2.9875798 -1.9450848 -0.14003134 1.661736 1.705821 0.69180965 -0.25125647 -1.2070251 -1.4995227 -0.87618375 -0.0052628517 0.37030554][-4.2209449 -4.5899124 -4.3428264 -3.3630476 -2.3476462 -0.61205411 1.3421149 1.5990582 0.46717453 -0.64638257 -1.6223917 -1.836359 -1.0960855 -0.1783576 0.26394653][-3.790956 -4.1221776 -4.0307341 -3.0687475 -2.0019894 -0.26968861 1.9181123 2.5248356 1.2780395 -0.25976562 -1.5459607 -1.9182048 -1.2656705 -0.38402128 0.24653673][-3.4159598 -3.5784202 -3.334115 -2.2000384 -1.0298979 0.76089764 3.1968312 4.2359772 3.0640965 1.1622 -0.48992109 -1.1849198 -0.95376849 -0.53043032 -0.0753665][-3.5916119 -3.6652865 -3.1784551 -1.8954422 -0.72939205 0.91522074 3.1504993 4.2049274 3.3047752 1.5734816 -0.0042004585 -0.70824432 -0.73620677 -0.71103978 -0.47416282][-3.8996744 -4.0350246 -3.5506148 -2.4752941 -1.5580201 -0.26102686 1.3290758 1.9208298 1.3032427 0.17176008 -0.87653446 -1.2000978 -1.1005573 -1.0641158 -0.68379068][-4.034771 -4.1454988 -3.7194927 -3.0359483 -2.4361615 -1.3548357 -0.32978725 -0.24146223 -0.64392495 -1.2113001 -1.8260527 -1.8173239 -1.5239129 -1.4307327 -0.94388509][-3.9912424 -3.9991896 -3.567049 -3.2064614 -2.8443117 -1.8578537 -1.209851 -1.4137893 -1.5528474 -1.7739685 -2.252996 -2.0869548 -1.647378 -1.6200202 -1.3670087][-3.8937273 -3.7648861 -3.3060479 -3.0846431 -2.7922 -1.9000983 -1.5893281 -2.0108974 -2.0044665 -2.1315444 -2.6393995 -2.4249535 -1.9549756 -2.0046308 -1.9380114][-4.0540271 -3.9817264 -3.6255403 -3.4429162 -3.0568929 -2.159126 -1.9683232 -2.3944058 -2.2748752 -2.3383064 -2.7602406 -2.4778531 -2.0937767 -2.2359178 -2.304426][-4.2862811 -4.4393888 -4.3701539 -4.326601 -3.8841803 -2.8904409 -2.514462 -2.6609249 -2.3824036 -2.3432331 -2.5794673 -2.2284381 -1.9169037 -2.064676 -2.2174344][-4.4056005 -4.5508833 -4.5612531 -4.661993 -4.3538961 -3.4496236 -2.9695568 -2.910831 -2.5583878 -2.4320097 -2.5064697 -2.1793025 -1.9465842 -2.0091357 -2.0790558]]...]
INFO - root - 2017-12-07 04:08:53.117421: step 7310, loss = 0.85, batch loss = 0.78 (9.5 examples/sec; 0.841 sec/batch; 75h:55m:40s remains)
INFO - root - 2017-12-07 04:09:01.492437: step 7320, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.824 sec/batch; 74h:23m:17s remains)
INFO - root - 2017-12-07 04:09:09.928682: step 7330, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 72h:28m:18s remains)
INFO - root - 2017-12-07 04:09:18.299626: step 7340, loss = 1.00, batch loss = 0.93 (9.2 examples/sec; 0.866 sec/batch; 78h:14m:23s remains)
INFO - root - 2017-12-07 04:09:26.703385: step 7350, loss = 0.66, batch loss = 0.59 (9.4 examples/sec; 0.852 sec/batch; 76h:59m:05s remains)
INFO - root - 2017-12-07 04:09:35.052402: step 7360, loss = 0.83, batch loss = 0.76 (9.3 examples/sec; 0.865 sec/batch; 78h:05m:21s remains)
INFO - root - 2017-12-07 04:09:43.410764: step 7370, loss = 0.56, batch loss = 0.49 (9.9 examples/sec; 0.806 sec/batch; 72h:45m:15s remains)
INFO - root - 2017-12-07 04:09:51.678491: step 7380, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.830 sec/batch; 74h:59m:28s remains)
INFO - root - 2017-12-07 04:09:59.996085: step 7390, loss = 1.04, batch loss = 0.97 (9.8 examples/sec; 0.815 sec/batch; 73h:34m:04s remains)
INFO - root - 2017-12-07 04:10:08.300134: step 7400, loss = 0.94, batch loss = 0.86 (9.7 examples/sec; 0.826 sec/batch; 74h:33m:26s remains)
2017-12-07 04:10:08.952972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5026779 -3.4707603 -3.4768045 -3.4370904 -3.4078517 -3.3186579 -3.1261911 -2.895144 -2.7982326 -2.917923 -3.0318861 -3.2019131 -3.3451648 -3.4244161 -3.4799218][-3.5466571 -3.505393 -3.5374746 -3.5128741 -3.4291065 -3.2127972 -2.8388932 -2.4509306 -2.3311949 -2.5262246 -2.7597446 -3.0429287 -3.2626021 -3.4089351 -3.5189469][-3.3127983 -3.2621474 -3.3645587 -3.4239078 -3.30235 -2.972569 -2.4227138 -1.9070783 -1.8074327 -2.0897455 -2.4208202 -2.7641902 -3.0260563 -3.2541537 -3.4789615][-2.8426056 -2.7986529 -2.9899263 -3.1360221 -2.9639802 -2.537457 -1.8804982 -1.3455796 -1.3517771 -1.7450693 -2.1210377 -2.4416502 -2.6939721 -2.987092 -3.3521404][-2.485405 -2.5194936 -2.7794559 -2.8874707 -2.531445 -1.9378128 -1.220849 -0.80652285 -1.0624166 -1.6394124 -2.043366 -2.2961092 -2.518666 -2.8552861 -3.3344405][-2.3325915 -2.4425874 -2.7114758 -2.6892774 -2.0648179 -1.2133429 -0.39779377 -0.13302469 -0.70682836 -1.4968305 -1.9016197 -2.0034168 -2.179589 -2.6251659 -3.2772169][-2.3241696 -2.4406104 -2.625241 -2.4691849 -1.6983273 -0.7180531 0.15382385 0.31444502 -0.47291136 -1.3444786 -1.6422877 -1.5451863 -1.7064078 -2.3008153 -3.1330876][-2.5116248 -2.670073 -2.7796614 -2.5548046 -1.835716 -1.0103893 -0.34231806 -0.30766726 -0.97470427 -1.6377301 -1.7427766 -1.5324366 -1.6963947 -2.3585637 -3.213428][-2.9745204 -3.2265487 -3.3442197 -3.1350498 -2.5609128 -1.9386196 -1.4671209 -1.4708393 -1.9084675 -2.2689826 -2.1846356 -1.9349568 -2.1072798 -2.7634258 -3.5269482][-3.4481325 -3.693223 -3.7675402 -3.56816 -3.1333771 -2.6612427 -2.3235555 -2.3497195 -2.6670303 -2.8597174 -2.7215445 -2.4898894 -2.62993 -3.1877842 -3.8012831][-3.7058878 -3.860944 -3.8220155 -3.5751343 -3.2308502 -2.9128027 -2.7524045 -2.8523455 -3.1108873 -3.2328813 -3.152988 -3.0144291 -3.1192555 -3.50914 -3.9183683][-3.7135184 -3.7725663 -3.6865668 -3.4683485 -3.2268276 -3.0526738 -3.0312424 -3.1812577 -3.4005995 -3.4909821 -3.452219 -3.396275 -3.48742 -3.7240193 -3.930846][-3.6367326 -3.6586039 -3.6206105 -3.5272472 -3.4398692 -3.391546 -3.4155283 -3.5143733 -3.653203 -3.7156143 -3.6969218 -3.6685138 -3.713649 -3.8175063 -3.8724747][-3.6161191 -3.6424265 -3.6546831 -3.6465864 -3.6487253 -3.6636868 -3.6798658 -3.6964505 -3.7388854 -3.7687814 -3.7591038 -3.7323303 -3.7264709 -3.7454774 -3.7326682][-3.5856168 -3.6112757 -3.6396933 -3.6632538 -3.6879861 -3.711437 -3.7162213 -3.7061486 -3.7080317 -3.7123632 -3.6960118 -3.6650321 -3.6420853 -3.6334679 -3.6117051]]...]
INFO - root - 2017-12-07 04:10:17.293775: step 7410, loss = 0.90, batch loss = 0.83 (9.4 examples/sec; 0.855 sec/batch; 77h:13m:47s remains)
INFO - root - 2017-12-07 04:10:25.644510: step 7420, loss = 0.58, batch loss = 0.51 (10.2 examples/sec; 0.781 sec/batch; 70h:31m:03s remains)
INFO - root - 2017-12-07 04:10:33.885391: step 7430, loss = 0.83, batch loss = 0.76 (9.5 examples/sec; 0.841 sec/batch; 75h:54m:50s remains)
INFO - root - 2017-12-07 04:10:42.160919: step 7440, loss = 0.66, batch loss = 0.58 (9.8 examples/sec; 0.819 sec/batch; 73h:59m:10s remains)
INFO - root - 2017-12-07 04:10:50.601454: step 7450, loss = 0.89, batch loss = 0.81 (9.6 examples/sec; 0.830 sec/batch; 74h:54m:44s remains)
INFO - root - 2017-12-07 04:10:58.919131: step 7460, loss = 0.74, batch loss = 0.67 (9.3 examples/sec; 0.865 sec/batch; 78h:04m:32s remains)
INFO - root - 2017-12-07 04:11:07.219961: step 7470, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 75h:10m:36s remains)
INFO - root - 2017-12-07 04:11:15.563107: step 7480, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 75h:12m:15s remains)
INFO - root - 2017-12-07 04:11:23.893422: step 7490, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.814 sec/batch; 73h:28m:54s remains)
INFO - root - 2017-12-07 04:11:32.124346: step 7500, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.835 sec/batch; 75h:21m:22s remains)
2017-12-07 04:11:32.775718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9729223 -3.1603348 -3.4855397 -3.8187239 -4.04189 -4.1552029 -4.2603498 -4.3092971 -4.2769084 -4.1562295 -3.9534695 -3.6801677 -3.4886677 -3.4999354 -3.5933747][-3.2430477 -3.3885756 -3.6575804 -3.8837185 -3.9222708 -3.8318725 -3.8220193 -3.8477983 -3.8773441 -3.872776 -3.81273 -3.6888225 -3.5394022 -3.5168619 -3.4957535][-3.4113936 -3.4789143 -3.6282852 -3.6481416 -3.410604 -3.0851829 -2.938571 -2.9283504 -3.0356004 -3.1444473 -3.2524786 -3.3653643 -3.4013147 -3.4591343 -3.3852863][-3.4063826 -3.5075808 -3.6652052 -3.5729594 -3.0724576 -2.5110412 -2.2135682 -2.1548522 -2.3285408 -2.5852509 -2.8565862 -3.1522963 -3.3588886 -3.4920928 -3.4072125][-3.3168197 -3.528712 -3.7863686 -3.7346451 -3.1693335 -2.5011215 -2.0897133 -1.8953161 -2.000448 -2.3529308 -2.7566452 -3.1619565 -3.522475 -3.7196739 -3.6323619][-3.2389834 -3.4362533 -3.638356 -3.5281124 -2.9771943 -2.3895497 -2.0595853 -1.881865 -2.0142367 -2.3596966 -2.6575603 -2.9796813 -3.3936255 -3.6357183 -3.5892148][-2.9607577 -2.995126 -2.9548404 -2.5643287 -1.7819743 -1.0527067 -0.69600797 -0.65065455 -1.0643198 -1.6089044 -1.9282084 -2.175972 -2.5291884 -2.7746694 -2.8566551][-2.3277652 -2.3227575 -2.2182446 -1.6979365 -0.7111578 0.31328773 1.1054897 1.3903117 0.90341377 0.18035984 -0.36851883 -0.80438733 -1.2272463 -1.579097 -1.8789048][-1.4776585 -1.5962205 -1.7188303 -1.5078638 -0.87053251 -0.053832054 0.868237 1.4659948 1.332202 0.92200041 0.48994493 0.067876816 -0.36188269 -0.78654504 -1.299701][-1.2117376 -1.4425671 -1.7363787 -1.843802 -1.6283276 -1.2454741 -0.66565084 -0.20494127 -0.08440876 -0.058138847 -0.077881336 -0.17408514 -0.3588624 -0.59204078 -1.0928323][-1.4212291 -1.6980395 -1.9440086 -2.0202212 -1.9112659 -1.8118465 -1.6625733 -1.6315949 -1.6433225 -1.5654693 -1.3475735 -1.0782437 -0.82672286 -0.54059649 -0.63215733][-1.6873002 -1.8770983 -1.9140306 -1.7335691 -1.5283113 -1.6033905 -1.8751395 -2.29427 -2.6570725 -2.8415027 -2.705586 -2.2668362 -1.6716311 -0.852052 -0.33304882][-2.1782029 -2.2237248 -2.0719728 -1.7143621 -1.4248126 -1.4574511 -1.7861271 -2.317394 -2.9097052 -3.4488811 -3.7163725 -3.5296164 -3.047051 -2.1413584 -1.2593045][-2.9173133 -2.9013486 -2.7649457 -2.5021005 -2.2788754 -2.1733756 -2.2343397 -2.5135067 -3.0063987 -3.5765729 -4.0330019 -4.1830792 -4.1091309 -3.6152411 -2.9322324][-3.5314431 -3.5730724 -3.5727077 -3.5133123 -3.4263353 -3.3087273 -3.2259395 -3.2673595 -3.4655111 -3.6770704 -3.8721712 -4.0500159 -4.2006655 -4.1256142 -3.846302]]...]
INFO - root - 2017-12-07 04:11:41.231124: step 7510, loss = 0.76, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:10m:03s remains)
INFO - root - 2017-12-07 04:11:49.502345: step 7520, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.833 sec/batch; 75h:10m:21s remains)
INFO - root - 2017-12-07 04:11:57.816762: step 7530, loss = 0.79, batch loss = 0.72 (9.6 examples/sec; 0.834 sec/batch; 75h:16m:00s remains)
INFO - root - 2017-12-07 04:12:06.217922: step 7540, loss = 0.86, batch loss = 0.79 (9.3 examples/sec; 0.859 sec/batch; 77h:35m:00s remains)
INFO - root - 2017-12-07 04:12:14.567533: step 7550, loss = 0.85, batch loss = 0.78 (9.6 examples/sec; 0.836 sec/batch; 75h:25m:03s remains)
INFO - root - 2017-12-07 04:12:22.908101: step 7560, loss = 0.70, batch loss = 0.63 (9.2 examples/sec; 0.874 sec/batch; 78h:54m:07s remains)
INFO - root - 2017-12-07 04:12:31.171534: step 7570, loss = 0.83, batch loss = 0.76 (10.4 examples/sec; 0.769 sec/batch; 69h:24m:02s remains)
INFO - root - 2017-12-07 04:12:39.461715: step 7580, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.832 sec/batch; 75h:07m:49s remains)
INFO - root - 2017-12-07 04:12:47.569209: step 7590, loss = 0.63, batch loss = 0.55 (9.1 examples/sec; 0.876 sec/batch; 79h:02m:27s remains)
INFO - root - 2017-12-07 04:12:55.808918: step 7600, loss = 0.61, batch loss = 0.53 (8.9 examples/sec; 0.895 sec/batch; 80h:46m:09s remains)
2017-12-07 04:12:56.429598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2249861 -3.1764178 -3.269016 -3.3870063 -3.24669 -3.0017929 -3.0078802 -3.3187861 -3.6110811 -3.563103 -3.3779197 -3.337081 -3.3930616 -3.3498244 -3.2053471][-3.0879774 -2.9819493 -3.0817871 -3.2458143 -3.0241446 -2.6182742 -2.5273757 -2.816998 -3.1590304 -3.2552354 -3.2885642 -3.4979038 -3.6573343 -3.4888904 -3.106487][-2.9410686 -2.755424 -2.8635445 -3.1227715 -2.8790767 -2.2974622 -2.0285313 -2.1261151 -2.3428333 -2.5612736 -2.94543 -3.5759366 -3.975116 -3.816911 -3.3422253][-2.8219731 -2.5550275 -2.6672611 -3.0623837 -2.9225187 -2.3141253 -1.9285269 -1.8053207 -1.8274434 -2.0739655 -2.6596663 -3.4384065 -3.8544962 -3.705209 -3.3523753][-2.7506225 -2.4365563 -2.5379114 -3.0059862 -2.986424 -2.4180532 -1.9905818 -1.8138204 -1.8050818 -2.03013 -2.5573113 -3.1028187 -3.241595 -3.0281978 -2.9088645][-2.6614511 -2.3343816 -2.4117019 -2.8470132 -2.7896416 -2.0938909 -1.5382323 -1.48542 -1.7136717 -2.0480151 -2.4750733 -2.7708592 -2.6940515 -2.5142651 -2.6202898][-2.5387032 -2.2475333 -2.3715169 -2.7599361 -2.5373979 -1.5729864 -0.82087612 -0.9340837 -1.5177875 -2.0457664 -2.3982937 -2.5759611 -2.4911613 -2.4330945 -2.6378574][-2.4490714 -2.2325637 -2.5085621 -2.9271977 -2.5944397 -1.5140853 -0.72330928 -0.88580632 -1.5496888 -2.017705 -2.196923 -2.3526433 -2.4317958 -2.4941049 -2.6458302][-2.3864548 -2.2525382 -2.6703565 -3.119967 -2.7788062 -1.8491275 -1.272846 -1.4562688 -1.9266157 -2.0714774 -2.0344319 -2.2277668 -2.4992161 -2.5956469 -2.6055312][-2.3214655 -2.2488565 -2.6792564 -3.0190377 -2.6182427 -1.8818107 -1.6071057 -1.933656 -2.2383838 -2.080231 -1.9783933 -2.3102374 -2.7200665 -2.7961149 -2.6604247][-2.2276506 -2.2152419 -2.6118758 -2.8125348 -2.333477 -1.7198493 -1.613914 -2.0652723 -2.3127162 -2.0329728 -2.0237191 -2.5113823 -2.9817374 -3.0086086 -2.778729][-2.1541362 -2.2119169 -2.6040583 -2.7279119 -2.2692614 -1.7931991 -1.7304044 -2.1236289 -2.2862785 -2.004446 -2.0913031 -2.6124969 -3.0078516 -2.9567425 -2.7065175][-2.1720665 -2.3048825 -2.7141368 -2.7797523 -2.3841708 -2.1047633 -2.1276069 -2.4140174 -2.5173235 -2.2853308 -2.3549614 -2.7288139 -2.9383802 -2.8096721 -2.5778151][-2.2098494 -2.4032252 -2.821697 -2.8505821 -2.4846244 -2.3585954 -2.4719582 -2.708904 -2.8394179 -2.7015929 -2.7118263 -2.8919907 -2.9411154 -2.8077075 -2.598299][-2.2297091 -2.4053776 -2.788265 -2.8291814 -2.5213513 -2.4827604 -2.5869803 -2.7585235 -2.9073782 -2.8384748 -2.8206675 -2.8889277 -2.9067867 -2.8689518 -2.7197843]]...]
INFO - root - 2017-12-07 04:13:04.793592: step 7610, loss = 0.54, batch loss = 0.47 (9.2 examples/sec; 0.874 sec/batch; 78h:52m:35s remains)
INFO - root - 2017-12-07 04:13:13.100730: step 7620, loss = 0.78, batch loss = 0.71 (10.1 examples/sec; 0.794 sec/batch; 71h:39m:46s remains)
INFO - root - 2017-12-07 04:13:21.499725: step 7630, loss = 0.80, batch loss = 0.73 (9.3 examples/sec; 0.862 sec/batch; 77h:46m:07s remains)
INFO - root - 2017-12-07 04:13:29.766720: step 7640, loss = 0.67, batch loss = 0.60 (9.3 examples/sec; 0.858 sec/batch; 77h:27m:50s remains)
INFO - root - 2017-12-07 04:13:38.023542: step 7650, loss = 0.80, batch loss = 0.72 (10.0 examples/sec; 0.802 sec/batch; 72h:20m:49s remains)
INFO - root - 2017-12-07 04:13:46.342724: step 7660, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.819 sec/batch; 73h:53m:32s remains)
INFO - root - 2017-12-07 04:13:54.570491: step 7670, loss = 0.82, batch loss = 0.75 (9.6 examples/sec; 0.830 sec/batch; 74h:52m:40s remains)
INFO - root - 2017-12-07 04:14:02.801861: step 7680, loss = 0.67, batch loss = 0.59 (10.0 examples/sec; 0.797 sec/batch; 71h:55m:31s remains)
INFO - root - 2017-12-07 04:14:11.249725: step 7690, loss = 0.71, batch loss = 0.64 (9.1 examples/sec; 0.881 sec/batch; 79h:31m:04s remains)
INFO - root - 2017-12-07 04:14:19.484975: step 7700, loss = 0.78, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 78h:51m:30s remains)
2017-12-07 04:14:20.159932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0169892 -3.1059499 -2.8538797 -2.3681834 -1.7656364 -1.4484694 -1.6231577 -1.9848909 -2.5951223 -2.908669 -2.6206012 -2.2948844 -2.1364493 -1.9464657 -1.8700709][-2.8965454 -3.0703323 -2.6928334 -1.9688382 -1.1284719 -0.77972817 -1.0739241 -1.5702434 -2.392308 -2.9035892 -2.7277102 -2.4714227 -2.3509655 -2.1717751 -2.1058705][-2.6371999 -2.759294 -2.2706501 -1.4856751 -0.7395978 -0.63024831 -1.1095765 -1.6928265 -2.5787938 -3.1442361 -3.0402269 -2.8969781 -2.8453991 -2.7165232 -2.7415056][-2.6884098 -2.65484 -2.1115165 -1.4558392 -1.0122638 -1.1107156 -1.4951496 -1.8250208 -2.4566603 -2.8613348 -2.80416 -2.88517 -3.01689 -2.9213254 -2.9111006][-2.7752235 -2.5699968 -2.0040755 -1.5786092 -1.4942 -1.6837142 -1.7257655 -1.5464733 -1.7778068 -1.9412611 -1.8660116 -2.1778324 -2.5935946 -2.6536584 -2.671365][-2.4268751 -2.1297321 -1.5727165 -1.364074 -1.5295324 -1.7414963 -1.4698484 -0.93121314 -0.98012376 -1.0303519 -0.81888103 -1.0803385 -1.4905384 -1.6468217 -1.8534884][-2.3078098 -1.9839947 -1.4494383 -1.2267528 -1.2865174 -1.2850058 -0.71931767 0.0057907104 -0.039704323 0.026012897 0.48670673 0.39462566 0.080822945 -0.24305391 -0.84430861][-2.836947 -2.6618383 -2.1453264 -1.6231189 -1.2765915 -0.98204947 -0.28891563 0.33706617 0.16915131 0.49179459 1.3880739 1.5856619 1.2936687 0.60733509 -0.53682613][-3.440412 -3.4772043 -2.8814039 -1.964731 -1.2667062 -0.92326427 -0.47984886 -0.24859333 -0.53312206 0.043016911 1.2531562 1.7339864 1.5766664 0.70856524 -0.71023679][-3.7637186 -3.8515701 -3.0983925 -1.9408991 -1.1955495 -1.1366587 -1.2154343 -1.4462645 -1.7944319 -1.2952151 -0.37784481 0.02396822 0.10059786 -0.34999228 -1.2481389][-3.7320068 -3.8001914 -3.0647106 -2.0150006 -1.4855323 -1.6949298 -2.1148226 -2.5443068 -2.8106298 -2.4741209 -2.0768981 -1.9825604 -1.8098731 -1.7743866 -1.9483054][-3.4514773 -3.6057062 -3.1935964 -2.5833385 -2.3072746 -2.5195081 -2.8879745 -3.2088666 -3.2956207 -3.0244234 -2.8461847 -2.8049824 -2.5396118 -2.2999456 -2.1524506][-3.0356908 -3.4051018 -3.4873276 -3.4078064 -3.2853093 -3.2541661 -3.308912 -3.3564608 -3.2467725 -2.9658327 -2.7839575 -2.6082044 -2.2370119 -1.9429297 -1.7346501][-2.3924332 -2.9850247 -3.4491274 -3.7238922 -3.648737 -3.4101458 -3.229337 -3.0731905 -2.8293004 -2.5616851 -2.3788416 -2.1464231 -1.771781 -1.4627516 -1.2252667][-1.7362702 -2.4542229 -3.0913997 -3.4767575 -3.3862715 -3.0707965 -2.8274274 -2.5942578 -2.2788517 -2.0210423 -1.8403141 -1.6350365 -1.3524554 -1.089325 -0.87357926]]...]
INFO - root - 2017-12-07 04:14:28.500277: step 7710, loss = 0.82, batch loss = 0.75 (9.4 examples/sec; 0.849 sec/batch; 76h:36m:10s remains)
INFO - root - 2017-12-07 04:14:36.817976: step 7720, loss = 0.64, batch loss = 0.57 (10.0 examples/sec; 0.803 sec/batch; 72h:27m:06s remains)
INFO - root - 2017-12-07 04:14:45.179838: step 7730, loss = 0.77, batch loss = 0.70 (9.5 examples/sec; 0.843 sec/batch; 76h:02m:10s remains)
INFO - root - 2017-12-07 04:14:53.496567: step 7740, loss = 1.04, batch loss = 0.97 (9.9 examples/sec; 0.811 sec/batch; 73h:07m:25s remains)
INFO - root - 2017-12-07 04:15:01.715760: step 7750, loss = 0.85, batch loss = 0.78 (10.0 examples/sec; 0.797 sec/batch; 71h:56m:17s remains)
INFO - root - 2017-12-07 04:15:10.012692: step 7760, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.799 sec/batch; 72h:03m:34s remains)
INFO - root - 2017-12-07 04:15:18.294658: step 7770, loss = 0.91, batch loss = 0.84 (9.3 examples/sec; 0.856 sec/batch; 77h:11m:20s remains)
INFO - root - 2017-12-07 04:15:26.519497: step 7780, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.791 sec/batch; 71h:20m:20s remains)
INFO - root - 2017-12-07 04:15:34.886199: step 7790, loss = 0.76, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 80h:40m:32s remains)
INFO - root - 2017-12-07 04:15:43.146168: step 7800, loss = 0.77, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 76h:59m:57s remains)
2017-12-07 04:15:43.796598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9798856 -3.0298643 -2.9500279 -2.7747135 -2.5479631 -2.2978523 -2.2528391 -2.3428855 -2.4285276 -2.5099471 -2.5781698 -2.6463914 -2.6841574 -2.6353822 -2.5148077][-2.3701885 -2.4475417 -2.4475641 -2.4055696 -2.2421942 -1.9797757 -1.9377556 -2.0366232 -2.1821883 -2.3294008 -2.4184005 -2.4869909 -2.4934251 -2.3999531 -2.1846511][-1.8086543 -1.9252052 -2.048574 -2.1910765 -2.16291 -1.9749017 -1.9629974 -2.0109038 -2.1320434 -2.3293052 -2.4364142 -2.4510534 -2.3533051 -2.1811228 -1.872324][-1.5535192 -1.7756219 -2.0027385 -2.173311 -2.1048281 -1.9242094 -1.9289389 -1.8938429 -2.005908 -2.3266246 -2.552371 -2.6098351 -2.5180674 -2.3681364 -2.0165291][-1.5956228 -1.839292 -2.054606 -2.1512353 -1.9800684 -1.7546661 -1.7009549 -1.5871823 -1.7734413 -2.304388 -2.6711295 -2.7706761 -2.7718749 -2.802361 -2.625237][-1.6506209 -1.74595 -1.8719223 -1.9032316 -1.7005939 -1.4455976 -1.258678 -0.98710227 -1.1777916 -1.8744943 -2.3774574 -2.4809375 -2.5272484 -2.7179713 -2.8350773][-1.5580621 -1.6315091 -1.7720442 -1.7346888 -1.377023 -0.88275695 -0.25090933 0.41226625 0.31641293 -0.56352019 -1.2884765 -1.5254178 -1.7023776 -2.0535216 -2.4230053][-1.3229988 -1.5270207 -1.7567105 -1.6013415 -1.0704799 -0.31999493 0.76008558 1.8384981 1.8743172 0.9183917 0.080883026 -0.29324722 -0.6113131 -1.0864255 -1.5761459][-1.3005359 -1.5058382 -1.6841657 -1.4194715 -0.94087124 -0.37949133 0.52271986 1.3245068 1.2125759 0.47394133 -0.049180984 -0.22015333 -0.40995979 -0.76300359 -1.1418958][-1.8222189 -1.9304566 -1.9858177 -1.7005396 -1.4267991 -1.1971087 -0.74010396 -0.48097157 -0.82395911 -1.321274 -1.468293 -1.3524113 -1.2986193 -1.4270499 -1.6495812][-2.6880426 -2.7926102 -2.7884431 -2.5571136 -2.4357128 -2.316741 -2.0603013 -2.0862298 -2.4927301 -2.8334875 -2.8556428 -2.7297564 -2.6433744 -2.7006583 -2.8796329][-3.4268804 -3.6500175 -3.7383106 -3.6344922 -3.5452213 -3.3443849 -3.06385 -3.0897241 -3.3712192 -3.555897 -3.5727377 -3.5505376 -3.5439975 -3.6166677 -3.7900097][-3.7090232 -3.9841325 -4.1285696 -4.1323218 -4.06448 -3.8168011 -3.521297 -3.517756 -3.7024512 -3.8014667 -3.8150749 -3.8103912 -3.8041208 -3.8400836 -3.9467576][-3.7089229 -3.8498092 -3.8995953 -3.8791838 -3.7942166 -3.5732679 -3.3519421 -3.3541894 -3.5071173 -3.6519308 -3.7599366 -3.8104153 -3.8090436 -3.7963591 -3.8033004][-3.7762914 -3.7661295 -3.6780522 -3.5767927 -3.4595292 -3.3104153 -3.2075539 -3.2165504 -3.2968903 -3.4009345 -3.5138824 -3.5953541 -3.6292098 -3.6238127 -3.5938067]]...]
INFO - root - 2017-12-07 04:15:52.125380: step 7810, loss = 0.63, batch loss = 0.55 (9.6 examples/sec; 0.831 sec/batch; 74h:55m:23s remains)
INFO - root - 2017-12-07 04:16:00.382356: step 7820, loss = 0.69, batch loss = 0.62 (9.8 examples/sec; 0.813 sec/batch; 73h:17m:01s remains)
INFO - root - 2017-12-07 04:16:08.657870: step 7830, loss = 0.72, batch loss = 0.65 (9.4 examples/sec; 0.850 sec/batch; 76h:39m:52s remains)
INFO - root - 2017-12-07 04:16:16.996547: step 7840, loss = 0.59, batch loss = 0.52 (9.7 examples/sec; 0.825 sec/batch; 74h:22m:48s remains)
INFO - root - 2017-12-07 04:16:25.300379: step 7850, loss = 0.62, batch loss = 0.55 (10.0 examples/sec; 0.799 sec/batch; 72h:02m:27s remains)
INFO - root - 2017-12-07 04:16:33.544145: step 7860, loss = 0.69, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 73h:58m:24s remains)
INFO - root - 2017-12-07 04:16:41.812413: step 7870, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.828 sec/batch; 74h:41m:03s remains)
INFO - root - 2017-12-07 04:16:50.054289: step 7880, loss = 0.60, batch loss = 0.52 (9.8 examples/sec; 0.817 sec/batch; 73h:39m:50s remains)
INFO - root - 2017-12-07 04:16:58.357056: step 7890, loss = 0.84, batch loss = 0.76 (9.2 examples/sec; 0.869 sec/batch; 78h:21m:06s remains)
INFO - root - 2017-12-07 04:17:06.431216: step 7900, loss = 0.72, batch loss = 0.65 (9.6 examples/sec; 0.830 sec/batch; 74h:51m:40s remains)
2017-12-07 04:17:07.216103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4418397 -3.3737617 -3.3638198 -3.3569164 -3.4052691 -3.5160103 -3.6277156 -3.7036047 -3.7399247 -3.7362611 -3.7395272 -3.7478282 -3.7281983 -3.7092986 -3.7162912][-3.4196887 -3.3393064 -3.3320894 -3.3406358 -3.4167109 -3.5642009 -3.702009 -3.794445 -3.8452988 -3.8507698 -3.8572404 -3.8548915 -3.8080239 -3.7655914 -3.7661991][-3.3072324 -3.242805 -3.2421169 -3.2748923 -3.3874264 -3.5736487 -3.7429862 -3.8556166 -3.9177921 -3.9305434 -3.9304469 -3.88739 -3.7736206 -3.6788044 -3.6766617][-3.2651811 -3.2160344 -3.2077403 -3.2684414 -3.4247422 -3.6407008 -3.8273675 -3.9457686 -3.991065 -3.9870675 -3.957571 -3.8558929 -3.6839509 -3.5480902 -3.5405779][-3.2844105 -3.2553084 -3.2346425 -3.3102436 -3.5054345 -3.7565656 -3.9867547 -4.14986 -4.2107515 -4.21672 -4.1755724 -4.0314932 -3.8359554 -3.6893086 -3.6607409][-3.146539 -3.1577396 -3.1322 -3.2030282 -3.4071932 -3.6515896 -3.8870258 -4.0701194 -4.1520152 -4.1997538 -4.1867657 -4.03995 -3.8642938 -3.723877 -3.6581669][-2.9561062 -3.0044887 -3.0088365 -3.1138299 -3.3517454 -3.5981762 -3.8127246 -3.9676683 -4.0223312 -4.0368886 -3.9613693 -3.7481356 -3.5764499 -3.4564059 -3.3696351][-2.7448027 -2.8328853 -2.9188201 -3.1120377 -3.4188032 -3.6896124 -3.8840942 -4.0171361 -4.0546107 -4.0256267 -3.8603611 -3.5571449 -3.3752851 -3.2812386 -3.1957309][-2.5722177 -2.7200508 -2.9087114 -3.1823416 -3.5223017 -3.79371 -3.9548109 -4.0575275 -4.0843172 -4.0185056 -3.7832851 -3.4196563 -3.223875 -3.1213596 -3.023685][-2.4528165 -2.6291809 -2.8754556 -3.1859956 -3.53576 -3.8217316 -3.9965768 -4.1430411 -4.2211256 -4.138628 -3.8544559 -3.475703 -3.2698131 -3.13264 -2.9977279][-2.3779504 -2.571105 -2.8606424 -3.212007 -3.5757165 -3.8685877 -4.0509238 -4.2404714 -4.3832464 -4.3069305 -4.0052261 -3.6454885 -3.4139104 -3.1998177 -2.9987621][-2.3880126 -2.618948 -2.9589963 -3.3468177 -3.7189872 -4.0053077 -4.1825385 -4.3966179 -4.5935407 -4.5625272 -4.2968683 -3.9693961 -3.6765394 -3.3413153 -3.042408][-2.429472 -2.6892104 -3.0502841 -3.4401231 -3.7811072 -4.0431118 -4.22919 -4.471971 -4.6989174 -4.7173476 -4.5241795 -4.2489672 -3.92031 -3.5085638 -3.1381555][-2.4635329 -2.7054453 -3.0300105 -3.3749819 -3.6514831 -3.8738792 -4.0640926 -4.301507 -4.5036726 -4.5335879 -4.3861375 -4.1574092 -3.8391242 -3.4622548 -3.1178761][-2.5561845 -2.7567062 -3.0258653 -3.3039865 -3.5135117 -3.7084274 -3.9096603 -4.1286469 -4.3137007 -4.361382 -4.2482939 -4.0197268 -3.7088428 -3.4084332 -3.1221776]]...]
INFO - root - 2017-12-07 04:17:15.687388: step 7910, loss = 0.74, batch loss = 0.66 (8.7 examples/sec; 0.923 sec/batch; 83h:12m:41s remains)
INFO - root - 2017-12-07 04:17:24.056284: step 7920, loss = 0.78, batch loss = 0.70 (9.7 examples/sec; 0.824 sec/batch; 74h:15m:08s remains)
INFO - root - 2017-12-07 04:17:32.361771: step 7930, loss = 0.96, batch loss = 0.89 (9.9 examples/sec; 0.806 sec/batch; 72h:37m:36s remains)
INFO - root - 2017-12-07 04:17:40.850517: step 7940, loss = 0.70, batch loss = 0.63 (9.4 examples/sec; 0.854 sec/batch; 76h:57m:25s remains)
INFO - root - 2017-12-07 04:17:49.302471: step 7950, loss = 0.73, batch loss = 0.66 (9.5 examples/sec; 0.844 sec/batch; 76h:07m:33s remains)
INFO - root - 2017-12-07 04:17:57.701016: step 7960, loss = 0.75, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 78h:58m:46s remains)
INFO - root - 2017-12-07 04:18:06.134810: step 7970, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.796 sec/batch; 71h:43m:58s remains)
INFO - root - 2017-12-07 04:18:14.675947: step 7980, loss = 0.68, batch loss = 0.61 (9.4 examples/sec; 0.853 sec/batch; 76h:51m:10s remains)
INFO - root - 2017-12-07 04:18:23.179773: step 7990, loss = 0.88, batch loss = 0.81 (9.0 examples/sec; 0.885 sec/batch; 79h:45m:10s remains)
INFO - root - 2017-12-07 04:18:31.685913: step 8000, loss = 0.60, batch loss = 0.53 (9.4 examples/sec; 0.853 sec/batch; 76h:55m:41s remains)
2017-12-07 04:18:32.345038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3043661 -1.3338616 -1.6022818 -1.7146077 -1.60643 -1.4560192 -1.3125784 -1.2540722 -1.3913083 -1.5793283 -1.7559977 -1.8050256 -1.7909172 -1.902086 -1.9515419][-1.2640119 -1.3780646 -1.5521886 -1.4870319 -1.3306789 -1.2694039 -1.2254653 -1.2302256 -1.3869925 -1.55685 -1.7577116 -1.8796785 -1.8652513 -1.8701897 -1.7633116][-1.2689593 -1.3967469 -1.3927135 -1.1462233 -0.96297884 -0.938972 -0.92710662 -0.98948884 -1.1838293 -1.3784516 -1.6636941 -1.8525202 -1.8617411 -1.8315997 -1.6422048][-1.3520944 -1.457437 -1.3359153 -0.98326826 -0.79705286 -0.73206306 -0.63602781 -0.63879824 -0.81705046 -1.0847521 -1.4771686 -1.6730494 -1.6829367 -1.7186055 -1.5753469][-1.4676259 -1.5459738 -1.3366463 -0.89390993 -0.67227769 -0.52447581 -0.30218649 -0.22683334 -0.43652749 -0.85601974 -1.3491087 -1.5085778 -1.4839091 -1.5846589 -1.5186005][-1.4509647 -1.4627514 -1.1384771 -0.59158278 -0.29190826 -0.052614212 0.32700682 0.46958828 0.041810513 -0.68219471 -1.2742264 -1.4013846 -1.392592 -1.5917864 -1.6103444][-1.3106461 -1.2153933 -0.73813748 -0.074530125 0.32443237 0.66636133 1.2082701 1.3964672 0.66584253 -0.41157532 -1.134594 -1.3491726 -1.4817202 -1.796735 -1.8680236][-1.1378508 -0.96039438 -0.41720009 0.25223732 0.64489031 0.99375677 1.5598359 1.7137275 0.81655121 -0.38409996 -1.1241932 -1.4353955 -1.7046533 -2.070049 -2.1888881][-0.97853637 -0.74090004 -0.25499821 0.24147463 0.44789982 0.63463116 1.0020819 1.0037098 0.24356937 -0.73237848 -1.3271594 -1.6580286 -2.0132577 -2.395175 -2.5355835][-0.84074473 -0.55342126 -0.14832449 0.16952324 0.17597342 0.14810228 0.22347927 0.038191319 -0.49717259 -1.1513913 -1.5944724 -1.9180875 -2.3274693 -2.7269969 -2.8235817][-0.74024463 -0.41561413 -0.051769257 0.16145897 0.061139107 -0.15816069 -0.39460897 -0.75539231 -1.0972898 -1.4815068 -1.7750871 -2.0074894 -2.3853521 -2.8263674 -2.908498][-0.73316431 -0.41778564 -0.1489253 -0.057991028 -0.20148373 -0.497082 -0.85819674 -1.1748068 -1.3139558 -1.5900486 -1.8757761 -2.0633986 -2.3805811 -2.8293726 -2.944479][-0.84505725 -0.63901353 -0.55193734 -0.58122945 -0.69826341 -0.91004753 -1.1471372 -1.2364075 -1.2318482 -1.5152903 -1.903573 -2.1692441 -2.5399323 -3.0486474 -3.2305236][-1.2302063 -1.1180415 -1.1306159 -1.1683545 -1.2271428 -1.3254533 -1.3614578 -1.1992869 -1.0716753 -1.2882023 -1.6209593 -1.8938882 -2.3934798 -3.0700474 -3.4339979][-1.8493624 -1.7332566 -1.6305623 -1.5229306 -1.5572555 -1.6413708 -1.5293548 -1.2242069 -1.0390661 -1.1854181 -1.4320507 -1.70438 -2.2994587 -3.017467 -3.4108791]]...]
INFO - root - 2017-12-07 04:18:40.766187: step 8010, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.790 sec/batch; 71h:13m:57s remains)
INFO - root - 2017-12-07 04:18:49.041473: step 8020, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.821 sec/batch; 73h:58m:51s remains)
INFO - root - 2017-12-07 04:18:57.374838: step 8030, loss = 0.79, batch loss = 0.71 (9.5 examples/sec; 0.842 sec/batch; 75h:52m:04s remains)
INFO - root - 2017-12-07 04:19:05.869556: step 8040, loss = 0.81, batch loss = 0.73 (9.5 examples/sec; 0.846 sec/batch; 76h:15m:20s remains)
INFO - root - 2017-12-07 04:19:14.220676: step 8050, loss = 0.71, batch loss = 0.64 (9.2 examples/sec; 0.873 sec/batch; 78h:41m:14s remains)
INFO - root - 2017-12-07 04:19:22.645224: step 8060, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.811 sec/batch; 73h:07m:08s remains)
INFO - root - 2017-12-07 04:19:30.993952: step 8070, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.817 sec/batch; 73h:39m:10s remains)
INFO - root - 2017-12-07 04:19:39.183954: step 8080, loss = 0.68, batch loss = 0.61 (9.8 examples/sec; 0.818 sec/batch; 73h:43m:09s remains)
INFO - root - 2017-12-07 04:19:47.492204: step 8090, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.820 sec/batch; 73h:52m:14s remains)
INFO - root - 2017-12-07 04:19:55.957553: step 8100, loss = 0.89, batch loss = 0.82 (9.5 examples/sec; 0.840 sec/batch; 75h:39m:37s remains)
2017-12-07 04:19:56.642687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.65294838 -0.85551238 -1.0278926 -0.95372486 -0.86533904 -0.71106529 -0.66069126 -0.60885572 -0.27724743 0.087377548 0.24838018 0.28041124 -0.096874714 -0.88597155 -1.3197856][-0.82346773 -0.98070168 -1.1123431 -1.0361068 -0.98228097 -0.81597972 -0.7437191 -0.67926168 -0.30593538 -0.034387112 -0.012016773 0.034359932 -0.20743084 -0.7896955 -1.132576][-1.0868165 -1.1988795 -1.1945035 -1.0254455 -0.89332294 -0.6537075 -0.55935049 -0.54684234 -0.24924755 -0.04916811 -0.10303593 -0.10911751 -0.31182861 -0.76556015 -0.975291][-1.4552166 -1.5420525 -1.3626401 -1.023242 -0.73380661 -0.39443016 -0.26273394 -0.39785385 -0.27380085 -0.061303139 -0.066408157 -0.16135311 -0.47294784 -0.92513752 -1.069994][-1.5906353 -1.692111 -1.4416637 -0.99878883 -0.56454515 -0.11350012 0.018037796 -0.3243618 -0.42976022 -0.20951223 -0.12589645 -0.25934029 -0.60592389 -1.0648065 -1.2419698][-1.3208816 -1.5173345 -1.3423235 -0.89047861 -0.39966679 0.12402725 0.21711588 -0.25473928 -0.48899031 -0.306561 -0.20350885 -0.30973864 -0.553283 -0.93060136 -1.1621139][-1.0351901 -1.2766805 -1.1039011 -0.636287 -0.1356163 0.36782026 0.33379269 -0.3013792 -0.65070868 -0.48903489 -0.3006897 -0.28881073 -0.37287092 -0.62120366 -0.90748191][-0.84096289 -1.0763938 -0.83569121 -0.28197479 0.22176027 0.65112972 0.38977003 -0.52166486 -1.0595212 -0.91886926 -0.52923608 -0.2160058 -0.034805298 -0.13964653 -0.48516774][-0.71020031 -0.93299747 -0.61111832 0.052598476 0.549685 0.90833473 0.55811548 -0.46908522 -1.1915867 -1.2068172 -0.73635483 -0.17392397 0.20921326 0.26835585 0.025568485][-0.65290141 -0.94458652 -0.56852746 0.22910833 0.77707624 1.1278396 0.85578012 -0.11050415 -1.0202723 -1.3195493 -0.98117614 -0.33536577 0.1802268 0.4231534 0.37042475][-0.65116858 -1.0745363 -0.71954608 0.17583466 0.81395388 1.2367206 1.1296005 0.32054758 -0.68777466 -1.2696621 -1.2095811 -0.63902187 0.025535107 0.50060177 0.62467957][-0.69189095 -1.2202318 -0.95939517 -0.076631069 0.59020376 1.0863209 1.1863894 0.5900569 -0.3549242 -1.0710993 -1.3173273 -0.97359395 -0.24731159 0.40395641 0.71632576][-0.66649628 -1.2391775 -1.1600819 -0.41061544 0.18914938 0.71341896 0.99273252 0.60077429 -0.13521719 -0.77817512 -1.2659445 -1.2452235 -0.630914 0.10816574 0.60236025][-0.47501111 -1.1185868 -1.2903962 -0.73036194 -0.21820927 0.30248308 0.70553684 0.504313 0.019262314 -0.45376778 -1.0624862 -1.3461301 -0.9285953 -0.2004652 0.41359997][-0.063194275 -0.79446888 -1.2887161 -1.0234976 -0.64790726 -0.16766024 0.3223362 0.31506205 0.041221619 -0.34097528 -0.94276333 -1.3607645 -1.0881996 -0.43616366 0.23500252]]...]
INFO - root - 2017-12-07 04:20:04.916138: step 8110, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.807 sec/batch; 72h:42m:49s remains)
INFO - root - 2017-12-07 04:20:13.243989: step 8120, loss = 0.79, batch loss = 0.72 (9.7 examples/sec; 0.822 sec/batch; 74h:04m:33s remains)
INFO - root - 2017-12-07 04:20:21.601878: step 8130, loss = 0.92, batch loss = 0.84 (9.3 examples/sec; 0.863 sec/batch; 77h:47m:03s remains)
INFO - root - 2017-12-07 04:20:29.914692: step 8140, loss = 0.53, batch loss = 0.46 (8.9 examples/sec; 0.901 sec/batch; 81h:13m:18s remains)
INFO - root - 2017-12-07 04:20:38.356939: step 8150, loss = 0.86, batch loss = 0.78 (10.3 examples/sec; 0.780 sec/batch; 70h:17m:19s remains)
INFO - root - 2017-12-07 04:20:46.826074: step 8160, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.815 sec/batch; 73h:23m:09s remains)
INFO - root - 2017-12-07 04:20:55.212851: step 8170, loss = 0.66, batch loss = 0.58 (9.3 examples/sec; 0.863 sec/batch; 77h:42m:42s remains)
INFO - root - 2017-12-07 04:21:03.558886: step 8180, loss = 0.94, batch loss = 0.86 (9.4 examples/sec; 0.848 sec/batch; 76h:25m:16s remains)
INFO - root - 2017-12-07 04:21:11.950518: step 8190, loss = 0.67, batch loss = 0.60 (9.1 examples/sec; 0.877 sec/batch; 79h:02m:36s remains)
INFO - root - 2017-12-07 04:21:20.387580: step 8200, loss = 0.90, batch loss = 0.82 (9.7 examples/sec; 0.825 sec/batch; 74h:19m:15s remains)
2017-12-07 04:21:21.039875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7361913 -3.5816641 -3.3554668 -3.1609986 -3.1609085 -3.2633526 -3.3280578 -3.5778155 -3.8104489 -3.9406409 -3.8640978 -3.6479659 -3.4934118 -3.5313797 -3.7943254][-3.9268618 -3.6985745 -3.3731861 -3.1419516 -3.1599617 -3.2798867 -3.2216072 -3.3085542 -3.5062175 -3.6804221 -3.6754594 -3.4665499 -3.2655959 -3.316103 -3.6474051][-3.9983454 -3.87101 -3.4825501 -3.0647869 -2.8350596 -2.7139654 -2.4685373 -2.4395728 -2.7092376 -3.0874372 -3.2763777 -3.0923793 -2.7412949 -2.690753 -3.018302][-3.8453426 -3.84007 -3.3185611 -2.5980308 -2.0425441 -1.7039104 -1.4117842 -1.3975189 -1.8415489 -2.5439496 -2.995821 -2.8279223 -2.266114 -2.0148318 -2.2829201][-3.1439457 -3.2513354 -2.6585436 -1.7101579 -0.96570277 -0.58820343 -0.31809092 -0.3368783 -0.9644115 -2.0585549 -2.8346863 -2.7601953 -2.0888543 -1.6493001 -1.7862096][-2.2115457 -2.3291049 -1.7391858 -0.72548938 0.0070643425 0.33782816 0.73627138 0.85439348 0.12192106 -1.4021425 -2.5921302 -2.7346711 -2.1351302 -1.5880129 -1.5779841][-1.7191756 -1.6644919 -1.0867507 -0.15102673 0.53955269 0.98317575 1.6619344 1.9334679 1.0762429 -0.80363107 -2.2891517 -2.5792809 -2.1002402 -1.537878 -1.4377229][-1.6591499 -1.3903112 -0.86496234 -0.098752975 0.51825047 1.024478 1.8066826 2.1357098 1.2262855 -0.75580454 -2.2253525 -2.4719863 -2.0488913 -1.4872208 -1.3155398][-1.7297871 -1.4085872 -1.0421076 -0.43914413 0.11972332 0.52353 1.1019483 1.4126468 0.64665413 -1.0353057 -2.2320137 -2.4307868 -2.0980349 -1.6153753 -1.4695783][-1.7518373 -1.6511357 -1.5883467 -1.1303527 -0.56715131 -0.24945211 0.13577414 0.43019772 -0.0629344 -1.2405958 -2.1191471 -2.4088168 -2.3159254 -2.0736747 -2.0652845][-1.8913937 -2.1283455 -2.3893661 -2.0230896 -1.4002955 -1.0232258 -0.6953969 -0.43478823 -0.67291236 -1.3149607 -1.9054191 -2.3538215 -2.5583172 -2.5704048 -2.6718597][-2.143491 -2.686264 -3.2060297 -2.9086165 -2.2019725 -1.6869612 -1.3817167 -1.2769415 -1.370512 -1.5520697 -1.8337512 -2.3343847 -2.7703319 -2.9704647 -3.0874348][-2.3980572 -3.0809717 -3.6968079 -3.4589229 -2.7896614 -2.3070517 -2.1546423 -2.2475562 -2.2608793 -2.1217949 -2.1007133 -2.4743071 -2.9598446 -3.2752552 -3.3960371][-2.6644502 -3.2973337 -3.8114474 -3.6135874 -3.0453014 -2.6856859 -2.6785522 -2.8431692 -2.7919145 -2.5095403 -2.3374953 -2.562305 -3.0248737 -3.4019501 -3.5681982][-2.9403083 -3.372961 -3.6437769 -3.4541633 -3.0247662 -2.7741809 -2.7993329 -2.91696 -2.8484044 -2.6171498 -2.5016632 -2.7169459 -3.1571078 -3.5101852 -3.6475782]]...]
INFO - root - 2017-12-07 04:21:29.066205: step 8210, loss = 0.83, batch loss = 0.75 (10.2 examples/sec; 0.788 sec/batch; 70h:59m:20s remains)
INFO - root - 2017-12-07 04:21:37.507123: step 8220, loss = 0.64, batch loss = 0.56 (9.7 examples/sec; 0.828 sec/batch; 74h:32m:33s remains)
INFO - root - 2017-12-07 04:21:45.879977: step 8230, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.792 sec/batch; 71h:22m:46s remains)
INFO - root - 2017-12-07 04:21:54.194204: step 8240, loss = 0.74, batch loss = 0.67 (9.6 examples/sec; 0.831 sec/batch; 74h:50m:07s remains)
INFO - root - 2017-12-07 04:22:02.589651: step 8250, loss = 0.76, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 71h:29m:31s remains)
INFO - root - 2017-12-07 04:22:10.846565: step 8260, loss = 0.70, batch loss = 0.63 (9.4 examples/sec; 0.847 sec/batch; 76h:17m:28s remains)
INFO - root - 2017-12-07 04:22:19.244536: step 8270, loss = 0.70, batch loss = 0.63 (8.8 examples/sec; 0.904 sec/batch; 81h:25m:39s remains)
INFO - root - 2017-12-07 04:22:27.569174: step 8280, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.833 sec/batch; 75h:03m:34s remains)
INFO - root - 2017-12-07 04:22:35.858140: step 8290, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.856 sec/batch; 77h:07m:45s remains)
INFO - root - 2017-12-07 04:22:44.202951: step 8300, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 0.866 sec/batch; 78h:01m:56s remains)
2017-12-07 04:22:44.854843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8159163 -2.8559837 -2.8266439 -2.7434225 -2.6931314 -2.6698422 -2.6837273 -2.74336 -2.7969353 -2.8574243 -2.8607831 -2.78972 -2.6960385 -2.6066704 -2.5684021][-3.060091 -3.1203938 -3.0877504 -2.9994283 -2.9579668 -2.9453132 -2.9639244 -3.0164709 -3.0526693 -3.1154447 -3.0960321 -2.9336796 -2.7202234 -2.5161808 -2.4348292][-3.3240175 -3.3725672 -3.3085346 -3.1984193 -3.1634884 -3.1796014 -3.2275147 -3.2751172 -3.3156829 -3.3933134 -3.3521509 -3.1124668 -2.7804945 -2.4579077 -2.3321867][-3.4088969 -3.3768277 -3.2490954 -3.113091 -3.094135 -3.1305451 -3.1474349 -3.1047015 -3.1208787 -3.2544861 -3.2813139 -3.1045592 -2.7731109 -2.3948746 -2.2340126][-3.2344639 -3.101263 -2.9187746 -2.7786105 -2.7824581 -2.7907653 -2.6553111 -2.3875411 -2.3317454 -2.5703785 -2.7785792 -2.8170109 -2.6206748 -2.2673001 -2.0973792][-2.9996781 -2.8590565 -2.6958935 -2.5755529 -2.5332279 -2.3985944 -2.0155547 -1.5171561 -1.4937487 -1.9875872 -2.5034661 -2.7854304 -2.6841834 -2.2976971 -2.0598137][-2.7591662 -2.67155 -2.5156381 -2.3592241 -2.1974638 -1.8610356 -1.2725291 -0.66671848 -0.80138445 -1.6282985 -2.4724536 -2.9645543 -2.8964758 -2.4406936 -2.1313493][-2.8348975 -2.8338976 -2.6263084 -2.3513453 -2.0597715 -1.6183891 -1.0179472 -0.50248265 -0.84018731 -1.8620119 -2.8069706 -3.3248816 -3.198143 -2.6914587 -2.3211257][-3.0783646 -3.1176643 -2.8485599 -2.5385473 -2.3159983 -2.016716 -1.6363535 -1.3735363 -1.809325 -2.71859 -3.4387307 -3.7645521 -3.5310345 -2.9979396 -2.55345][-3.1563473 -3.1750569 -2.883009 -2.6417849 -2.6306581 -2.5680165 -2.3871746 -2.2713 -2.615623 -3.223927 -3.6467872 -3.8213434 -3.5821514 -3.098846 -2.6297617][-3.1688404 -3.2434998 -3.0669529 -2.9835043 -3.1361661 -3.1915016 -3.0599942 -2.9415989 -3.0901542 -3.4124188 -3.6624455 -3.7971559 -3.6292644 -3.2037868 -2.6930752][-3.0852895 -3.2559059 -3.2610438 -3.3548646 -3.5846057 -3.6489797 -3.5009279 -3.3462586 -3.3467422 -3.5150702 -3.7307286 -3.8932641 -3.8195288 -3.462038 -2.9337034][-3.0860295 -3.3077626 -3.4037151 -3.5414531 -3.7053761 -3.6701717 -3.4531441 -3.2380931 -3.1515892 -3.2515047 -3.4625108 -3.643806 -3.6560688 -3.4228702 -3.026572][-3.0419731 -3.2267952 -3.3093238 -3.3970749 -3.4678619 -3.3807573 -3.1749868 -2.9902744 -2.8998947 -2.9631639 -3.1480012 -3.3241229 -3.3781207 -3.2273169 -2.9751713][-2.8777623 -2.9609442 -2.9943397 -3.0352869 -3.0632029 -3.0073361 -2.902215 -2.8364849 -2.8311443 -2.9189801 -3.0973263 -3.2731476 -3.3545523 -3.2676647 -3.0973268]]...]
INFO - root - 2017-12-07 04:22:53.206989: step 8310, loss = 0.60, batch loss = 0.52 (10.1 examples/sec; 0.794 sec/batch; 71h:30m:15s remains)
INFO - root - 2017-12-07 04:23:01.651643: step 8320, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.800 sec/batch; 72h:01m:24s remains)
INFO - root - 2017-12-07 04:23:09.929856: step 8330, loss = 0.80, batch loss = 0.73 (9.1 examples/sec; 0.875 sec/batch; 78h:44m:58s remains)
INFO - root - 2017-12-07 04:23:18.174077: step 8340, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.799 sec/batch; 71h:54m:11s remains)
INFO - root - 2017-12-07 04:23:26.535277: step 8350, loss = 0.89, batch loss = 0.82 (9.3 examples/sec; 0.863 sec/batch; 77h:42m:23s remains)
INFO - root - 2017-12-07 04:23:34.921650: step 8360, loss = 0.86, batch loss = 0.79 (8.9 examples/sec; 0.901 sec/batch; 81h:08m:32s remains)
INFO - root - 2017-12-07 04:23:43.261420: step 8370, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.802 sec/batch; 72h:10m:09s remains)
INFO - root - 2017-12-07 04:23:51.607700: step 8380, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.805 sec/batch; 72h:30m:41s remains)
INFO - root - 2017-12-07 04:23:59.919980: step 8390, loss = 0.67, batch loss = 0.60 (9.4 examples/sec; 0.848 sec/batch; 76h:20m:59s remains)
INFO - root - 2017-12-07 04:24:08.269526: step 8400, loss = 0.89, batch loss = 0.82 (9.5 examples/sec; 0.838 sec/batch; 75h:28m:57s remains)
2017-12-07 04:24:08.997372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2652888 -3.2025118 -3.2921295 -3.3173721 -3.3787508 -3.4314976 -3.08268 -2.4613645 -2.1453035 -2.6188173 -3.5891042 -4.3178649 -4.5099273 -4.1284394 -3.3818254][-3.0774918 -2.9026604 -2.9603558 -2.9898026 -3.1151924 -3.1338053 -2.5176983 -1.6927266 -1.4723535 -2.2636762 -3.5182672 -4.4012475 -4.691524 -4.2841911 -3.4428253][-2.8738115 -2.5558696 -2.5671897 -2.5936179 -2.6843381 -2.5456791 -1.7613966 -1.0024049 -0.9634831 -1.7975461 -2.9342947 -3.8111 -4.3358312 -4.1401739 -3.3891463][-2.7042449 -2.2653599 -2.1890969 -2.034822 -1.8235433 -1.5291553 -1.0612078 -0.92084289 -1.1894982 -1.7822351 -2.3671825 -2.8751149 -3.4452024 -3.525743 -3.0621285][-2.6006413 -2.0780926 -1.8787136 -1.4337821 -0.85881805 -0.56562829 -0.73048139 -1.3491931 -1.8712835 -2.1141295 -2.01109 -1.8910236 -2.238611 -2.5416021 -2.5396247][-2.6077833 -2.0839186 -1.7301242 -0.9495461 -0.089852333 0.13385391 -0.42929769 -1.4523308 -2.1791389 -2.4254167 -2.0398939 -1.4164283 -1.3392878 -1.6101184 -1.9345057][-2.6710265 -2.1006784 -1.4377086 -0.34658289 0.47140694 0.45896959 -0.16985607 -1.1171949 -1.95519 -2.5543342 -2.4210641 -1.620965 -1.1120534 -1.1747112 -1.568876][-2.7464693 -1.9556251 -0.85050511 0.26515865 0.42477703 -0.11312532 -0.54888296 -1.0209723 -1.6780465 -2.4923811 -2.720438 -2.04461 -1.4303455 -1.3886638 -1.6684675][-2.813889 -1.68734 -0.23360157 0.5648427 -0.18878984 -1.2323811 -1.3955212 -1.3396745 -1.5851159 -2.255614 -2.644743 -2.291141 -1.910526 -1.8690424 -1.863807][-2.8171935 -1.389555 0.19828129 0.66485596 -0.67256474 -1.9910779 -2.12559 -1.8748226 -1.764075 -2.0044045 -2.238308 -2.144429 -2.095345 -2.1000681 -1.7772007][-2.7146366 -1.1812532 0.23111725 0.49051428 -0.80456996 -2.0013278 -2.2666759 -2.1821365 -2.0257108 -1.9053082 -1.8816783 -1.9594929 -2.1590788 -2.1693871 -1.6123025][-2.6297517 -1.2480099 -0.18313551 0.068751812 -0.75053549 -1.5593426 -1.883656 -2.0715866 -2.2005234 -2.070296 -1.9151981 -2.0648961 -2.3904369 -2.4579191 -1.8848348][-2.6011291 -1.5417209 -0.78770494 -0.41822386 -0.69191289 -1.0953009 -1.316215 -1.6265459 -2.1518743 -2.2709625 -2.0897875 -2.1775527 -2.5529957 -2.7508144 -2.386467][-2.6184912 -1.83391 -1.2691653 -0.91871 -0.94694471 -1.0953312 -1.0705869 -1.2615461 -1.9094191 -2.1441448 -1.9554214 -2.0528028 -2.5165195 -2.8920436 -2.8161955][-2.7271295 -2.1073537 -1.6679835 -1.4322848 -1.4121735 -1.3744962 -1.1776843 -1.20607 -1.6641543 -1.7306912 -1.4824085 -1.6320388 -2.139924 -2.6691494 -2.8966143]]...]
INFO - root - 2017-12-07 04:24:17.279776: step 8410, loss = 0.69, batch loss = 0.61 (10.2 examples/sec; 0.782 sec/batch; 70h:25m:25s remains)
INFO - root - 2017-12-07 04:24:25.690433: step 8420, loss = 0.69, batch loss = 0.61 (9.2 examples/sec; 0.873 sec/batch; 78h:34m:04s remains)
INFO - root - 2017-12-07 04:24:33.972260: step 8430, loss = 0.85, batch loss = 0.78 (9.7 examples/sec; 0.825 sec/batch; 74h:16m:19s remains)
INFO - root - 2017-12-07 04:24:42.430573: step 8440, loss = 0.85, batch loss = 0.77 (9.5 examples/sec; 0.840 sec/batch; 75h:35m:27s remains)
INFO - root - 2017-12-07 04:24:50.642475: step 8450, loss = 0.83, batch loss = 0.75 (9.6 examples/sec; 0.833 sec/batch; 75h:00m:16s remains)
INFO - root - 2017-12-07 04:24:59.015502: step 8460, loss = 0.97, batch loss = 0.89 (9.5 examples/sec; 0.842 sec/batch; 75h:46m:11s remains)
INFO - root - 2017-12-07 04:25:07.322112: step 8470, loss = 0.57, batch loss = 0.50 (10.1 examples/sec; 0.793 sec/batch; 71h:23m:40s remains)
INFO - root - 2017-12-07 04:25:15.656168: step 8480, loss = 0.70, batch loss = 0.63 (9.4 examples/sec; 0.851 sec/batch; 76h:32m:59s remains)
INFO - root - 2017-12-07 04:25:24.023271: step 8490, loss = 0.77, batch loss = 0.70 (9.6 examples/sec; 0.833 sec/batch; 74h:59m:33s remains)
INFO - root - 2017-12-07 04:25:32.374473: step 8500, loss = 0.70, batch loss = 0.62 (9.4 examples/sec; 0.848 sec/batch; 76h:20m:00s remains)
2017-12-07 04:25:33.022148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.47827697 -0.50082445 -0.523746 -0.58160377 -0.65439272 -0.72032332 -0.769598 -0.80138731 -0.820364 -0.80953074 -0.80264807 -0.81499028 -0.82800388 -0.83546829 -0.82175207][-0.36396551 -0.40968657 -0.45343041 -0.50986719 -0.55921936 -0.57604027 -0.56582952 -0.54476571 -0.50927258 -0.45151782 -0.41166306 -0.40982246 -0.4364543 -0.46161771 -0.46663332][-0.9661417 -1.0460961 -1.0497701 -1.0118392 -0.94335771 -0.82742834 -0.7000196 -0.60665011 -0.54341483 -0.48356366 -0.45637274 -0.48700619 -0.55830193 -0.61424255 -0.62969279][-1.6810362 -1.8136435 -1.8071063 -1.7231901 -1.585928 -1.3802013 -1.1643591 -0.99921513 -0.88347292 -0.78408027 -0.74783611 -0.8028245 -0.90621543 -0.97776747 -0.98478007][-1.8715591 -2.023802 -2.0547523 -2.0418184 -1.9845974 -1.8468792 -1.700984 -1.5869374 -1.4845192 -1.3773265 -1.3444211 -1.414551 -1.5170245 -1.5705826 -1.5581527][-1.8069465 -1.9013903 -1.8951409 -1.8764076 -1.8378005 -1.752377 -1.6955497 -1.6650882 -1.616219 -1.5560796 -1.5814104 -1.7039042 -1.8340962 -1.909179 -1.9245555][-2.1736305 -2.2211049 -2.1094329 -1.9207034 -1.6696291 -1.3879743 -1.1902087 -1.0606999 -0.96363449 -0.90961051 -0.96809793 -1.1341536 -1.3049777 -1.42594 -1.4783485][-2.6516032 -2.7702036 -2.6267138 -2.2895565 -1.803076 -1.2271442 -0.71929216 -0.31278515 -0.058767796 0.035007954 -0.041725159 -0.2614336 -0.50188828 -0.69073415 -0.773494][-2.5738564 -2.7804124 -2.7106621 -2.4434021 -2.0449078 -1.5626056 -1.1041594 -0.68085027 -0.37262821 -0.21038771 -0.21848631 -0.38992882 -0.60040975 -0.77367854 -0.84524274][-2.0956209 -2.2401247 -2.1386423 -1.9207041 -1.6793869 -1.4468083 -1.2999454 -1.1969855 -1.1429524 -1.13427 -1.1964192 -1.3564577 -1.5137239 -1.6305475 -1.6704147][-2.2090936 -2.3290496 -2.2315509 -2.0189888 -1.7491782 -1.4790018 -1.3199892 -1.251647 -1.2393982 -1.2789922 -1.3876879 -1.5861273 -1.7640843 -1.8981292 -1.9637754][-2.4965694 -2.6303902 -2.5985067 -2.4681525 -2.2525642 -1.9818566 -1.7534602 -1.5548255 -1.3641727 -1.2267892 -1.1839886 -1.2558694 -1.3546901 -1.4658597 -1.5491006][-2.1641505 -2.2044661 -2.0915203 -1.9070168 -1.7020493 -1.5161531 -1.3882723 -1.2815526 -1.1723762 -1.0961101 -1.0714464 -1.0918753 -1.1359088 -1.2189353 -1.30709][-1.748656 -1.7672858 -1.6560831 -1.4577563 -1.2257497 -1.0459793 -0.93591046 -0.87903714 -0.85773611 -0.87969065 -0.93212867 -0.97022247 -1.0047898 -1.0638692 -1.131783][-1.2866695 -1.3060164 -1.2931364 -1.2422259 -1.1582 -1.096061 -1.0502121 -1.0122352 -0.98095608 -0.97121954 -0.98216558 -0.97126579 -0.95726728 -0.96704412 -0.99056363]]...]
INFO - root - 2017-12-07 04:25:41.380607: step 8510, loss = 1.00, batch loss = 0.93 (9.9 examples/sec; 0.812 sec/batch; 73h:02m:57s remains)
INFO - root - 2017-12-07 04:25:49.717817: step 8520, loss = 0.65, batch loss = 0.58 (9.0 examples/sec; 0.888 sec/batch; 79h:54m:40s remains)
INFO - root - 2017-12-07 04:25:57.879508: step 8530, loss = 0.86, batch loss = 0.79 (8.9 examples/sec; 0.899 sec/batch; 80h:53m:23s remains)
INFO - root - 2017-12-07 04:26:06.204861: step 8540, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.817 sec/batch; 73h:29m:30s remains)
INFO - root - 2017-12-07 04:26:14.456884: step 8550, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.847 sec/batch; 76h:11m:32s remains)
INFO - root - 2017-12-07 04:26:22.808065: step 8560, loss = 0.75, batch loss = 0.68 (9.5 examples/sec; 0.844 sec/batch; 75h:55m:43s remains)
INFO - root - 2017-12-07 04:26:31.073708: step 8570, loss = 0.72, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 72h:25m:28s remains)
INFO - root - 2017-12-07 04:26:39.449327: step 8580, loss = 0.87, batch loss = 0.80 (9.6 examples/sec; 0.836 sec/batch; 75h:11m:39s remains)
INFO - root - 2017-12-07 04:26:47.777291: step 8590, loss = 0.78, batch loss = 0.70 (8.8 examples/sec; 0.906 sec/batch; 81h:31m:09s remains)
INFO - root - 2017-12-07 04:26:55.972872: step 8600, loss = 0.55, batch loss = 0.47 (9.9 examples/sec; 0.806 sec/batch; 72h:30m:02s remains)
2017-12-07 04:26:56.617926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2851787 -3.2785835 -3.2573643 -3.2587943 -3.2788606 -3.2862563 -3.2696157 -3.2466309 -3.2592013 -3.2852378 -3.2868371 -3.2698798 -3.2481656 -3.230113 -3.2073896][-4.1532512 -4.0790119 -3.9951372 -3.9831543 -4.0208092 -4.0197005 -3.9709756 -3.9144766 -3.9591432 -4.0498505 -4.0787873 -4.0637741 -4.0282016 -3.9924207 -3.9660075][-4.53883 -4.4180546 -4.3226509 -4.3434095 -4.4161844 -4.4073133 -4.3147411 -4.2089872 -4.2794461 -4.442584 -4.4856977 -4.4568939 -4.3884974 -4.3229389 -4.2932711][-4.1022191 -4.0334005 -4.0264235 -4.1529064 -4.2788668 -4.2636166 -4.1193442 -3.9505777 -4.05771 -4.3349743 -4.4253178 -4.40379 -4.3433042 -4.3368182 -4.4396858][-3.1376486 -2.9513354 -2.8981366 -3.0410657 -3.1845341 -3.147836 -2.9457862 -2.7459784 -3.0153885 -3.5950727 -3.8794754 -3.9070621 -3.8903928 -4.021008 -4.335032][-1.8437443 -1.6083457 -1.6325965 -1.9528303 -2.1652248 -1.9780574 -1.4384623 -0.94372392 -1.2363298 -2.094965 -2.6159983 -2.7340825 -2.8039439 -3.1223869 -3.6811166][-1.0156603 -0.71109676 -0.79088879 -1.2554495 -1.5427845 -1.2252779 -0.3319211 0.54987764 0.38887405 -0.56647658 -1.2391644 -1.4769628 -1.6655078 -2.1557565 -2.9156113][-1.2794111 -0.866853 -0.72603631 -0.9936955 -1.2576649 -1.0807738 -0.35462141 0.41887569 0.32905626 -0.42625475 -0.96902013 -1.163466 -1.2994111 -1.7562547 -2.5555859][-1.927639 -1.6877019 -1.4752529 -1.4563158 -1.5445344 -1.536109 -1.2685769 -0.90953183 -0.99318671 -1.4010949 -1.6096506 -1.5330374 -1.3421772 -1.4370971 -1.9813802][-2.4504209 -2.5761621 -2.5214119 -2.3984759 -2.3643382 -2.3727171 -2.2492688 -2.0411897 -2.0916569 -2.2902112 -2.3457215 -2.1338918 -1.67153 -1.3167746 -1.3801644][-2.5743542 -2.8799672 -2.9876056 -2.9677453 -2.9971163 -2.9944544 -2.8112597 -2.5352292 -2.5051062 -2.6451 -2.7225883 -2.5785666 -2.1183538 -1.6733923 -1.5703351][-2.6129317 -2.7978964 -2.9071186 -2.9968042 -3.1170864 -3.0414882 -2.6570432 -2.1917367 -2.0556927 -2.1951931 -2.375879 -2.418571 -2.2176518 -2.0465238 -2.1620564][-2.6085763 -2.7551398 -2.9002488 -3.1312194 -3.3931756 -3.3278809 -2.8404045 -2.2782624 -2.0618255 -2.1502512 -2.3308964 -2.4436896 -2.4014704 -2.4252889 -2.7115221][-2.3532877 -2.5867782 -2.7729588 -3.0409565 -3.4008629 -3.4707556 -3.1605873 -2.7802024 -2.6206899 -2.6659517 -2.7790933 -2.856976 -2.859587 -2.934279 -3.1956308][-2.5183117 -2.7612081 -2.9079099 -3.0740108 -3.3133631 -3.3568783 -3.1854315 -3.0540571 -3.0837736 -3.1972909 -3.2806726 -3.2786524 -3.2444727 -3.2952962 -3.4569573]]...]
INFO - root - 2017-12-07 04:27:04.918735: step 8610, loss = 0.71, batch loss = 0.63 (9.7 examples/sec; 0.826 sec/batch; 74h:21m:25s remains)
INFO - root - 2017-12-07 04:27:13.236403: step 8620, loss = 0.74, batch loss = 0.66 (9.6 examples/sec; 0.832 sec/batch; 74h:53m:38s remains)
INFO - root - 2017-12-07 04:27:21.460699: step 8630, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 74h:54m:30s remains)
INFO - root - 2017-12-07 04:27:29.863252: step 8640, loss = 0.63, batch loss = 0.56 (10.0 examples/sec; 0.802 sec/batch; 72h:11m:35s remains)
INFO - root - 2017-12-07 04:27:38.134273: step 8650, loss = 0.55, batch loss = 0.47 (10.3 examples/sec; 0.779 sec/batch; 70h:02m:26s remains)
INFO - root - 2017-12-07 04:27:46.464215: step 8660, loss = 0.80, batch loss = 0.73 (9.7 examples/sec; 0.824 sec/batch; 74h:06m:34s remains)
INFO - root - 2017-12-07 04:27:54.795397: step 8670, loss = 0.85, batch loss = 0.78 (10.4 examples/sec; 0.773 sec/batch; 69h:30m:35s remains)
INFO - root - 2017-12-07 04:28:03.096620: step 8680, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 75h:14m:25s remains)
INFO - root - 2017-12-07 04:28:11.325447: step 8690, loss = 0.98, batch loss = 0.91 (9.7 examples/sec; 0.828 sec/batch; 74h:30m:40s remains)
INFO - root - 2017-12-07 04:28:19.747923: step 8700, loss = 0.68, batch loss = 0.61 (9.5 examples/sec; 0.844 sec/batch; 75h:54m:10s remains)
2017-12-07 04:28:20.432743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9659512 -2.6826429 -2.4558494 -2.4556553 -2.6958251 -2.8758702 -2.9145422 -2.7283926 -2.6443081 -2.6873107 -2.6153102 -2.6011128 -2.5835047 -2.7048202 -2.8616118][-2.6399617 -2.1949236 -1.8684728 -1.9428821 -2.3534002 -2.5842929 -2.56027 -2.1804845 -2.0446174 -2.1793633 -2.1536579 -2.2272394 -2.2171872 -2.3742869 -2.5108569][-2.3383574 -1.8669493 -1.6340721 -2.0016472 -2.6289415 -2.82036 -2.5232086 -1.6965768 -1.3755255 -1.6191616 -1.8039865 -2.132076 -2.2278979 -2.4403324 -2.56854][-1.992275 -1.7341757 -1.8903463 -2.7445879 -3.5306079 -3.5347912 -2.674159 -1.1212165 -0.54495573 -1.0149198 -1.6484632 -2.2795603 -2.3081794 -2.449517 -2.5006962][-1.7600021 -1.8593822 -2.4517 -3.5616522 -4.1586728 -3.672101 -1.9211199 0.41888666 0.9267149 -0.25000381 -1.6855843 -2.6895432 -2.56249 -2.4429913 -2.1410377][-1.5958416 -2.0589776 -2.9060378 -3.8670757 -3.8987274 -2.6813765 0.0085730553 2.8418221 2.8342061 0.7818923 -1.304239 -2.542733 -2.4918268 -2.4103086 -1.8430955][-1.5346916 -2.2564034 -3.133471 -3.6529224 -2.8454854 -0.83354855 2.4916649 5.1658497 4.2070351 1.4350781 -0.76416326 -1.8003967 -1.8191996 -2.0699461 -1.6566548][-1.6627364 -2.4555962 -3.1889462 -3.2000065 -1.6681895 0.90012836 4.3466129 6.4197245 4.7205381 1.7592087 -0.26246262 -1.0701661 -1.1729767 -1.7334671 -1.5710146][-2.1659744 -2.9038758 -3.3530409 -2.9642482 -1.171247 1.2592363 4.0739059 5.5006609 3.9602985 1.7117901 0.15641785 -0.56038713 -0.90202427 -1.7187848 -1.7459898][-2.8530359 -3.6152582 -3.8361 -3.2109008 -1.5364273 0.25899792 2.0210075 2.8958249 2.0300598 0.91137552 0.06055975 -0.49066424 -1.0155075 -1.9960799 -2.1360292][-3.3752556 -4.2943845 -4.5208549 -3.9023354 -2.5775433 -1.3879123 -0.41606855 0.16756153 -0.14084578 -0.55996728 -0.82123256 -1.0892713 -1.5674634 -2.4867728 -2.6058159][-3.7421274 -4.6037717 -4.8121276 -4.3462172 -3.5967159 -3.0298753 -2.5986152 -2.1159852 -1.9986711 -2.0180807 -1.8368502 -1.8508396 -2.2501571 -2.9898362 -2.9769325][-4.0236907 -4.5665755 -4.6070094 -4.2520952 -4.0187535 -3.9509978 -3.9007664 -3.5724201 -3.2152729 -2.9277813 -2.5083847 -2.4375143 -2.836154 -3.3506746 -3.1752439][-4.0801816 -4.3163414 -4.1913142 -3.9056239 -3.9478738 -4.120904 -4.2511568 -4.0734043 -3.6273272 -3.2002268 -2.7879081 -2.7944469 -3.2177804 -3.5436208 -3.2810667][-3.9247146 -3.9836166 -3.7596974 -3.5327356 -3.6854992 -3.9783835 -4.1773648 -4.069612 -3.6215293 -3.1970162 -2.9398677 -3.0585871 -3.4286489 -3.6090577 -3.3503852]]...]
INFO - root - 2017-12-07 04:28:28.751221: step 8710, loss = 0.90, batch loss = 0.83 (10.3 examples/sec; 0.775 sec/batch; 69h:42m:07s remains)
INFO - root - 2017-12-07 04:28:37.163728: step 8720, loss = 0.83, batch loss = 0.76 (9.3 examples/sec; 0.862 sec/batch; 77h:33m:03s remains)
INFO - root - 2017-12-07 04:28:45.509442: step 8730, loss = 0.88, batch loss = 0.80 (9.3 examples/sec; 0.861 sec/batch; 77h:25m:59s remains)
INFO - root - 2017-12-07 04:28:53.854975: step 8740, loss = 0.77, batch loss = 0.70 (9.7 examples/sec; 0.821 sec/batch; 73h:48m:48s remains)
INFO - root - 2017-12-07 04:29:02.146212: step 8750, loss = 0.96, batch loss = 0.89 (9.6 examples/sec; 0.831 sec/batch; 74h:42m:27s remains)
INFO - root - 2017-12-07 04:29:10.564832: step 8760, loss = 0.79, batch loss = 0.72 (9.4 examples/sec; 0.849 sec/batch; 76h:22m:57s remains)
INFO - root - 2017-12-07 04:29:18.899419: step 8770, loss = 0.70, batch loss = 0.63 (9.5 examples/sec; 0.844 sec/batch; 75h:51m:12s remains)
INFO - root - 2017-12-07 04:29:27.111963: step 8780, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 74h:00m:52s remains)
INFO - root - 2017-12-07 04:29:35.527821: step 8790, loss = 0.58, batch loss = 0.51 (9.8 examples/sec; 0.819 sec/batch; 73h:40m:22s remains)
INFO - root - 2017-12-07 04:29:43.768310: step 8800, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.788 sec/batch; 70h:51m:28s remains)
2017-12-07 04:29:44.462045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9086957 -3.0205002 -3.0751352 -3.1013002 -3.0487344 -2.9021955 -2.7312756 -2.6241486 -2.6109724 -2.67492 -2.7918954 -2.8972592 -2.9413617 -2.9275126 -2.9192457][-2.78996 -2.8735676 -2.9457898 -3.0364254 -3.0652194 -3.0034676 -2.8904083 -2.765501 -2.6637492 -2.6048734 -2.6482265 -2.7582994 -2.836607 -2.8319185 -2.8376751][-2.3471482 -2.16358 -2.0791614 -2.1784592 -2.373204 -2.6312385 -2.8696475 -2.934185 -2.75835 -2.4088564 -2.1199276 -2.009692 -2.033447 -2.1073341 -2.2351196][-2.3937442 -2.1304972 -1.8907466 -1.8112209 -1.8723602 -2.1636736 -2.5897591 -2.8729477 -2.8270025 -2.5066772 -2.1253431 -1.8305387 -1.635709 -1.5911472 -1.701344][-2.7154498 -2.5624943 -2.329252 -2.0959659 -1.8596027 -1.8205733 -2.0809124 -2.4513087 -2.6499557 -2.7190115 -2.6966958 -2.5504756 -2.2405303 -1.9457264 -1.850863][-2.9048996 -2.6345415 -2.395447 -2.1890178 -1.8527093 -1.5304391 -1.544302 -1.8372726 -2.153517 -2.4498482 -2.6619811 -2.7625499 -2.7005105 -2.5622969 -2.4540339][-3.0528493 -2.5783901 -2.2528744 -2.1680408 -1.9764323 -1.6516578 -1.5204666 -1.5847819 -1.7341735 -2.0112531 -2.2859108 -2.5399151 -2.7265863 -2.8253474 -2.893014][-3.0804188 -2.5452766 -2.2261915 -2.3071051 -2.3792312 -2.2645388 -2.1181653 -1.933151 -1.8235774 -1.9644268 -2.219764 -2.5324364 -2.7709134 -2.9111991 -3.0830646][-3.1248004 -2.7476554 -2.5297384 -2.6221137 -2.7311015 -2.7207236 -2.6483493 -2.4512143 -2.2345588 -2.1297634 -2.1465428 -2.3827713 -2.6718946 -2.9148808 -3.1420388][-3.0922346 -3.0365176 -3.0211251 -3.0286062 -2.9129219 -2.7645545 -2.7181745 -2.7014666 -2.6402121 -2.4886982 -2.2912011 -2.3135257 -2.5234299 -2.8310452 -3.1473868][-2.828743 -3.0301509 -3.3406475 -3.4809048 -3.2907348 -2.9498014 -2.744534 -2.767576 -2.9174247 -2.9652848 -2.8074365 -2.6953807 -2.6814694 -2.8246884 -3.1669402][-2.8244791 -2.9662716 -3.3654847 -3.6943274 -3.6713929 -3.321418 -2.9452124 -2.8308935 -3.0407085 -3.2897973 -3.3502655 -3.3242846 -3.2095258 -3.1024389 -3.2483881][-3.0482526 -3.0447536 -3.2883024 -3.6100578 -3.7810993 -3.6386955 -3.3160973 -3.0813723 -3.1146536 -3.3077996 -3.5019219 -3.6848488 -3.6955802 -3.5025353 -3.4582202][-2.9755259 -2.8914318 -3.0086775 -3.2863536 -3.6120906 -3.7792478 -3.7228475 -3.5498571 -3.407156 -3.3917956 -3.5288582 -3.776782 -3.8815496 -3.6976185 -3.6091354][-2.9169512 -2.7559896 -2.7413979 -2.9091752 -3.243155 -3.5874186 -3.8136454 -3.8842247 -3.8163819 -3.7336771 -3.7451622 -3.8367648 -3.8249903 -3.6202946 -3.5513875]]...]
INFO - root - 2017-12-07 04:29:52.865898: step 8810, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 76h:43m:26s remains)
INFO - root - 2017-12-07 04:30:01.211803: step 8820, loss = 0.64, batch loss = 0.57 (9.0 examples/sec; 0.893 sec/batch; 80h:19m:21s remains)
INFO - root - 2017-12-07 04:30:09.519984: step 8830, loss = 0.70, batch loss = 0.63 (9.9 examples/sec; 0.810 sec/batch; 72h:52m:02s remains)
INFO - root - 2017-12-07 04:30:17.490526: step 8840, loss = 0.75, batch loss = 0.67 (9.5 examples/sec; 0.845 sec/batch; 75h:56m:14s remains)
INFO - root - 2017-12-07 04:30:25.887115: step 8850, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.827 sec/batch; 74h:21m:31s remains)
INFO - root - 2017-12-07 04:30:34.235207: step 8860, loss = 0.68, batch loss = 0.61 (9.1 examples/sec; 0.878 sec/batch; 78h:56m:23s remains)
INFO - root - 2017-12-07 04:30:42.532167: step 8870, loss = 0.83, batch loss = 0.75 (10.0 examples/sec; 0.803 sec/batch; 72h:11m:29s remains)
INFO - root - 2017-12-07 04:30:50.862851: step 8880, loss = 0.71, batch loss = 0.64 (9.5 examples/sec; 0.839 sec/batch; 75h:24m:51s remains)
INFO - root - 2017-12-07 04:30:59.156981: step 8890, loss = 0.87, batch loss = 0.79 (9.7 examples/sec; 0.825 sec/batch; 74h:09m:07s remains)
INFO - root - 2017-12-07 04:31:07.562262: step 8900, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 75h:16m:32s remains)
2017-12-07 04:31:08.315566: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7848163 -3.6700025 -3.5434973 -3.5007119 -3.5243425 -3.6443686 -3.8172097 -3.926621 -3.9456129 -3.9178529 -3.8561621 -3.7781246 -3.6921048 -3.7662852 -4.1508331][-3.7334828 -3.544373 -3.2316933 -3.0030174 -2.9225297 -3.0571177 -3.3494592 -3.6450057 -3.8585479 -3.9700298 -3.9786093 -3.8902516 -3.7161348 -3.6700051 -3.9774182][-3.5598495 -3.29607 -2.7585764 -2.2725141 -2.0330036 -2.1466305 -2.5314329 -3.0158219 -3.4542937 -3.7676618 -3.9191461 -3.8885589 -3.6691594 -3.5345039 -3.7816124][-3.383008 -3.0790505 -2.3969562 -1.7027757 -1.276345 -1.2888336 -1.6403499 -2.1892092 -2.8027124 -3.3302007 -3.7080462 -3.8672347 -3.7191238 -3.5652494 -3.768245][-3.2980151 -2.9898868 -2.2463326 -1.4240291 -0.78332233 -0.50185227 -0.64337754 -1.0875974 -1.7543352 -2.4725661 -3.1656904 -3.6786575 -3.7959635 -3.7440639 -3.9295008][-3.319159 -3.0339074 -2.2803607 -1.3966949 -0.56924629 0.14978409 0.45987368 0.259377 -0.40085268 -1.3218699 -2.3857396 -3.3269603 -3.7885573 -3.9038725 -4.1292129][-3.4116678 -3.1641531 -2.4079115 -1.4772015 -0.52530432 0.55589676 1.4210987 1.5854025 0.88382673 -0.31106138 -1.7520559 -3.0600896 -3.8022115 -4.0392618 -4.32101][-3.5667 -3.3382707 -2.5980167 -1.6761258 -0.71208119 0.47544193 1.6470714 2.1668534 1.5005603 0.23320484 -1.305989 -2.771945 -3.7161894 -4.1122031 -4.508863][-3.8275087 -3.6226254 -2.9613438 -2.1405525 -1.3343229 -0.34896898 0.67981863 1.301672 1.094759 0.25556135 -0.95678544 -2.3336976 -3.3996191 -4.0261269 -4.6098337][-4.035049 -3.9287186 -3.4088709 -2.7760091 -2.2827601 -1.6289313 -0.80670929 -0.03429985 0.338964 0.1089983 -0.75001192 -2.0146296 -3.1301036 -3.895463 -4.5792284][-4.2311563 -4.2599578 -3.9008112 -3.4675093 -3.2674348 -2.8888385 -2.1776974 -1.2892308 -0.58215809 -0.41433811 -0.98391151 -2.0406315 -3.0326161 -3.7675147 -4.4527068][-4.2791886 -4.4962215 -4.273622 -3.9439929 -3.8587992 -3.6247313 -3.0548685 -2.2848568 -1.6464324 -1.3610601 -1.5942011 -2.22857 -2.9333889 -3.5861928 -4.2968407][-3.7041919 -4.2690296 -4.3268042 -4.1793213 -4.1476383 -3.9521725 -3.5339255 -3.0162199 -2.5894647 -2.3096581 -2.2775829 -2.5090365 -2.9285398 -3.486259 -4.1916924][-2.864377 -3.7283416 -4.1057787 -4.2114997 -4.2452583 -4.0502839 -3.7512879 -3.485255 -3.2465785 -2.9968626 -2.8402047 -2.8575377 -3.0555055 -3.470531 -4.0921307][-2.485858 -3.3806057 -3.8831658 -4.1052933 -4.1304917 -3.9236345 -3.7149811 -3.6506331 -3.5722198 -3.3780866 -3.2195346 -3.1698496 -3.2333283 -3.4935877 -4.0013413]]...]
INFO - root - 2017-12-07 04:31:16.651524: step 8910, loss = 0.71, batch loss = 0.64 (10.0 examples/sec; 0.797 sec/batch; 71h:36m:30s remains)
INFO - root - 2017-12-07 04:31:24.987542: step 8920, loss = 0.73, batch loss = 0.66 (9.8 examples/sec; 0.815 sec/batch; 73h:16m:40s remains)
INFO - root - 2017-12-07 04:31:33.268462: step 8930, loss = 0.82, batch loss = 0.74 (9.4 examples/sec; 0.849 sec/batch; 76h:19m:19s remains)
INFO - root - 2017-12-07 04:31:41.465116: step 8940, loss = 0.63, batch loss = 0.55 (10.3 examples/sec; 0.775 sec/batch; 69h:37m:36s remains)
INFO - root - 2017-12-07 04:31:49.779853: step 8950, loss = 0.72, batch loss = 0.65 (9.1 examples/sec; 0.874 sec/batch; 78h:35m:25s remains)
INFO - root - 2017-12-07 04:31:58.067183: step 8960, loss = 0.89, batch loss = 0.82 (9.6 examples/sec; 0.835 sec/batch; 75h:02m:28s remains)
INFO - root - 2017-12-07 04:32:06.511578: step 8970, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.778 sec/batch; 69h:55m:13s remains)
INFO - root - 2017-12-07 04:32:14.835629: step 8980, loss = 0.78, batch loss = 0.71 (9.8 examples/sec; 0.815 sec/batch; 73h:13m:14s remains)
INFO - root - 2017-12-07 04:32:23.224785: step 8990, loss = 0.71, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 73h:56m:49s remains)
INFO - root - 2017-12-07 04:32:31.466756: step 9000, loss = 0.62, batch loss = 0.55 (9.4 examples/sec; 0.847 sec/batch; 76h:09m:06s remains)
2017-12-07 04:32:32.125878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5949333 -1.7737591 -1.4816215 -1.1113942 -0.953197 -0.92485666 -0.81799912 -0.6593883 -0.58573842 -0.61488938 -0.80490875 -1.1066697 -1.3221238 -1.3252404 -1.2368238][-1.5272238 -1.6268044 -1.3480194 -0.99299192 -0.80482626 -0.72076154 -0.56467104 -0.31108332 -0.13773823 -0.16300631 -0.41014338 -0.82551622 -1.1427035 -1.1920891 -1.1043494][-1.611135 -1.6820238 -1.4920285 -1.2396789 -1.0842474 -0.97725797 -0.79528856 -0.48565745 -0.25139189 -0.27635956 -0.54774141 -1.024003 -1.4271057 -1.5631833 -1.5282784][-1.6738923 -1.7686038 -1.6753037 -1.5075424 -1.3565056 -1.2062693 -1.0010104 -0.68778253 -0.45438433 -0.490268 -0.76947093 -1.2716615 -1.7753046 -2.0915818 -2.2539632][-1.7758999 -1.897325 -1.8746731 -1.7534144 -1.5927677 -1.4207952 -1.2297752 -0.97526336 -0.81692648 -0.85625362 -1.0509925 -1.4367623 -1.8986959 -2.28818 -2.5678675][-1.3893385 -1.5939462 -1.7003293 -1.6715539 -1.5651507 -1.3916676 -1.1862171 -0.98570323 -0.98949265 -1.1405246 -1.3006175 -1.5646405 -1.956466 -2.3758528 -2.7278328][-1.1710477 -1.4014509 -1.5594592 -1.5629745 -1.4747078 -1.276659 -0.9825685 -0.73960328 -0.83291912 -1.0711346 -1.1765921 -1.2738552 -1.5126977 -1.8922048 -2.2851472][-1.7723591 -1.9956257 -2.1278834 -2.0910537 -1.987278 -1.8130641 -1.5127079 -1.2152381 -1.2544646 -1.421241 -1.3583317 -1.1897767 -1.1418347 -1.3261702 -1.6022234][-2.1240299 -2.3155229 -2.44929 -2.4626827 -2.479537 -2.4933729 -2.3980043 -2.2328446 -2.2773013 -2.3557744 -2.1442032 -1.7963104 -1.5382493 -1.5287266 -1.6173475][-2.3086421 -2.4187994 -2.5166712 -2.5642219 -2.6369777 -2.7429976 -2.788568 -2.7354527 -2.78751 -2.855722 -2.7139487 -2.4757938 -2.2929304 -2.2824802 -2.3029525][-2.8306885 -2.8804379 -2.9312747 -2.9558725 -2.9982133 -3.0942016 -3.1790409 -3.1520391 -3.1483684 -3.1646929 -3.076654 -2.943897 -2.8396416 -2.8217864 -2.7886636][-3.2533717 -3.2687588 -3.3133068 -3.3397009 -3.3620868 -3.4420681 -3.5403578 -3.5357413 -3.4943252 -3.4598286 -3.3690829 -3.2716298 -3.2056489 -3.1899612 -3.1435802][-3.3286643 -3.371269 -3.4649405 -3.5456204 -3.5940127 -3.6621172 -3.7358508 -3.7359645 -3.6848617 -3.630187 -3.562242 -3.51259 -3.508791 -3.5282612 -3.4961305][-3.1511555 -3.2460136 -3.3873897 -3.5029035 -3.5818737 -3.6631236 -3.7450769 -3.7846122 -3.785223 -3.7748921 -3.7717271 -3.7877774 -3.833719 -3.8688359 -3.828814][-3.055932 -3.1477489 -3.2879395 -3.3806415 -3.4296169 -3.4785891 -3.5358782 -3.5875638 -3.6136618 -3.6296787 -3.6665986 -3.7306349 -3.8237233 -3.9054146 -3.9305732]]...]
INFO - root - 2017-12-07 04:32:40.424653: step 9010, loss = 0.87, batch loss = 0.80 (10.0 examples/sec; 0.802 sec/batch; 72h:03m:29s remains)
INFO - root - 2017-12-07 04:32:48.826140: step 9020, loss = 0.77, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:16m:14s remains)
INFO - root - 2017-12-07 04:32:57.106729: step 9030, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 72h:22m:16s remains)
INFO - root - 2017-12-07 04:33:05.536853: step 9040, loss = 0.64, batch loss = 0.56 (9.5 examples/sec; 0.845 sec/batch; 75h:55m:03s remains)
INFO - root - 2017-12-07 04:33:13.932010: step 9050, loss = 0.69, batch loss = 0.62 (9.2 examples/sec; 0.872 sec/batch; 78h:21m:59s remains)
INFO - root - 2017-12-07 04:33:22.146739: step 9060, loss = 0.92, batch loss = 0.85 (10.0 examples/sec; 0.800 sec/batch; 71h:51m:21s remains)
INFO - root - 2017-12-07 04:33:30.523393: step 9070, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 71h:57m:52s remains)
INFO - root - 2017-12-07 04:33:38.984169: step 9080, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 76h:12m:20s remains)
INFO - root - 2017-12-07 04:33:47.372349: step 9090, loss = 0.78, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 76h:29m:21s remains)
INFO - root - 2017-12-07 04:33:55.776930: step 9100, loss = 0.71, batch loss = 0.64 (8.8 examples/sec; 0.914 sec/batch; 82h:05m:56s remains)
2017-12-07 04:33:56.425440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2630081 -4.2085309 -4.1591282 -4.1363525 -4.1398344 -4.1801534 -4.2028761 -4.1921158 -4.1610088 -4.1225314 -4.0954337 -4.0581326 -4.033443 -4.027082 -3.9923353][-4.224113 -4.2299881 -4.2123675 -4.235919 -4.2992568 -4.3965158 -4.43515 -4.3747153 -4.2842412 -4.2037039 -4.1346221 -4.0292554 -3.9480131 -3.8947308 -3.7991011][-3.4990983 -3.6073291 -3.6642752 -3.7710314 -3.907994 -4.0517592 -4.09578 -3.9754579 -3.8269248 -3.7084849 -3.5869036 -3.382648 -3.207963 -3.0845547 -2.8991175][-2.2526715 -2.4873519 -2.6673374 -2.8802433 -3.0677729 -3.2359953 -3.2722564 -3.0971344 -2.9217033 -2.7821465 -2.5993085 -2.2873611 -2.0044806 -1.7997591 -1.5052803][-0.98001719 -1.2844362 -1.5623581 -1.8626516 -2.1361544 -2.4075146 -2.4705486 -2.258806 -2.1048648 -1.9791815 -1.7866623 -1.433979 -1.0997102 -0.85293293 -0.474262][0.5398984 0.31592655 0.092741966 -0.16451502 -0.46646428 -0.81474233 -0.87434936 -0.68458962 -0.67942357 -0.70684195 -0.639719 -0.38346243 -0.1530509 -0.026320457 0.24491215][0.9357276 0.93429089 0.90881634 0.83323812 0.65085745 0.40543032 0.44843912 0.55772829 0.3184762 0.083247185 -0.015332699 0.082600594 0.10607481 -0.0022468567 0.023625374][0.34261703 0.61109495 0.81126547 0.95960426 1.0169277 1.0214567 1.160284 1.1043115 0.65533257 0.26075077 0.023813725 -0.037064552 -0.22272062 -0.56238675 -0.76591921][-0.10666227 0.26618624 0.5309267 0.78507376 1.016696 1.2182178 1.3983412 1.267518 0.79911613 0.38594246 0.080348492 -0.0977149 -0.38666248 -0.8143723 -1.1065369][-0.44303775 -0.14648914 0.013620853 0.20094633 0.43002129 0.65677834 0.8196311 0.74772835 0.42026997 0.10988379 -0.16621017 -0.37070227 -0.62366247 -0.97982788 -1.2082][-0.65014172 -0.46147442 -0.42052031 -0.35452604 -0.24531984 -0.12907839 -0.021849632 0.019785881 -0.13896799 -0.3201499 -0.51006794 -0.67741036 -0.84182262 -1.1076615 -1.2686222][-0.79771352 -0.68158126 -0.68202209 -0.67671108 -0.68681622 -0.68323708 -0.60442853 -0.47585011 -0.51424193 -0.62619352 -0.74794006 -0.85414028 -0.92946172 -1.116488 -1.2340398][-0.90813065 -0.80118728 -0.76032877 -0.73954535 -0.79891276 -0.88048148 -0.84467721 -0.70620871 -0.7144897 -0.82290888 -0.93074656 -1.0039093 -1.0336843 -1.1630018 -1.239022][-1.0640197 -0.98327518 -0.91921067 -0.86690354 -0.902838 -1.0058177 -1.0075738 -0.9102366 -0.92244554 -1.008508 -1.0937262 -1.1493216 -1.1673632 -1.2629168 -1.3086555][-1.2217188 -1.1833169 -1.1210146 -1.0547543 -1.0581784 -1.1298969 -1.1337776 -1.0770135 -1.0914025 -1.1327755 -1.1828802 -1.2224755 -1.2413232 -1.3219948 -1.3587861]]...]
INFO - root - 2017-12-07 04:34:04.714173: step 9110, loss = 0.77, batch loss = 0.70 (9.6 examples/sec; 0.832 sec/batch; 74h:45m:11s remains)
INFO - root - 2017-12-07 04:34:13.051067: step 9120, loss = 0.64, batch loss = 0.57 (9.6 examples/sec; 0.830 sec/batch; 74h:31m:01s remains)
INFO - root - 2017-12-07 04:34:21.378749: step 9130, loss = 0.64, batch loss = 0.57 (9.3 examples/sec; 0.861 sec/batch; 77h:18m:23s remains)
INFO - root - 2017-12-07 04:34:29.700011: step 9140, loss = 0.84, batch loss = 0.77 (9.5 examples/sec; 0.846 sec/batch; 75h:59m:01s remains)
INFO - root - 2017-12-07 04:34:37.986146: step 9150, loss = 0.61, batch loss = 0.54 (11.2 examples/sec; 0.717 sec/batch; 64h:22m:46s remains)
INFO - root - 2017-12-07 04:34:45.975580: step 9160, loss = 0.60, batch loss = 0.53 (10.0 examples/sec; 0.798 sec/batch; 71h:42m:46s remains)
INFO - root - 2017-12-07 04:34:54.314164: step 9170, loss = 0.73, batch loss = 0.66 (9.5 examples/sec; 0.841 sec/batch; 75h:31m:57s remains)
INFO - root - 2017-12-07 04:35:02.551241: step 9180, loss = 0.81, batch loss = 0.74 (9.0 examples/sec; 0.885 sec/batch; 79h:29m:19s remains)
INFO - root - 2017-12-07 04:35:10.955238: step 9190, loss = 0.86, batch loss = 0.78 (10.0 examples/sec; 0.799 sec/batch; 71h:43m:58s remains)
INFO - root - 2017-12-07 04:35:19.201464: step 9200, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.805 sec/batch; 72h:18m:04s remains)
2017-12-07 04:35:19.770981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2842464 -4.3260965 -4.2704587 -4.2638054 -4.3500772 -4.4167438 -4.3993993 -4.2683067 -4.1545291 -4.1736183 -4.2644916 -4.3185697 -4.3147211 -4.2820745 -4.2743459][-4.47441 -4.54322 -4.3997593 -4.3290114 -4.2642508 -4.0740767 -3.8550663 -3.6175687 -3.4717908 -3.5812941 -3.8064241 -3.9460168 -4.0006628 -3.992238 -3.9988451][-4.7211909 -4.7500796 -4.4016395 -4.1426592 -3.7921276 -3.2709885 -2.9294634 -2.6846147 -2.5505695 -2.7636232 -3.0840414 -3.2735338 -3.4034963 -3.442872 -3.4869969][-4.9065309 -4.7093115 -3.9761608 -3.4476225 -2.8039215 -2.0500011 -1.719945 -1.462595 -1.2376442 -1.5057502 -1.87502 -2.0645692 -2.2999656 -2.4708042 -2.627883][-5.05235 -4.6730113 -3.7097628 -3.0996962 -2.3251071 -1.4123724 -0.93746209 -0.35346603 0.14855766 -0.22173214 -0.68019772 -0.83214784 -1.1151373 -1.4277146 -1.7048795][-5.2254372 -4.9094396 -4.0068107 -3.4561019 -2.5666628 -1.3456862 -0.42445898 0.79679871 1.5917821 0.89821959 0.13217449 -0.074761391 -0.35438538 -0.72562265 -1.03319][-5.4258523 -5.2114115 -4.3271508 -3.6459384 -2.46307 -0.82232451 0.64906836 2.5195918 3.4852371 2.2679253 0.96380186 0.48377228 0.13330555 -0.25271988 -0.57348943][-5.4620304 -5.2701135 -4.3320293 -3.4544177 -2.035893 -0.17674828 1.5055175 3.5593338 4.4025259 2.7685642 1.2322307 0.60762262 0.24402571 -0.072565556 -0.34682417][-5.28642 -5.0873542 -4.170723 -3.2346435 -1.8409207 -0.18951797 1.1371746 2.6273613 3.0455604 1.6227775 0.41605282 -0.0042009354 -0.12177849 -0.14518833 -0.25376272][-5.1090331 -4.9158278 -4.0684648 -3.1702654 -1.9260983 -0.51942968 0.45970726 1.3745155 1.5114698 0.4569869 -0.31433773 -0.5006671 -0.34836817 -0.06309557 -0.039963245][-5.0165749 -4.8019538 -3.9934988 -3.1449876 -2.02854 -0.76253104 0.0411582 0.56287193 0.50012589 -0.30773926 -0.82513452 -0.93921328 -0.6922462 -0.28154993 -0.21580648][-5.0299578 -4.7804656 -3.9826784 -3.1806304 -2.2662268 -1.2516387 -0.66006804 -0.41095734 -0.57116818 -1.1879785 -1.6039314 -1.7200174 -1.4224944 -0.95995283 -0.85610414][-4.98299 -4.7059741 -3.9278898 -3.1849008 -2.5023921 -1.8362684 -1.4712472 -1.36027 -1.5552106 -2.0379572 -2.3409173 -2.3743773 -1.9989483 -1.5097122 -1.3728206][-4.8246589 -4.6634388 -4.1069536 -3.5462327 -3.0740762 -2.6504817 -2.4114463 -2.3845994 -2.6319144 -3.0668492 -3.331069 -3.3151758 -2.9173865 -2.4253194 -2.2142472][-4.5520225 -4.5270205 -4.2385764 -3.9022796 -3.6009746 -3.358922 -3.2465906 -3.2822194 -3.5256488 -3.880686 -4.0754495 -4.0227985 -3.6634197 -3.2318439 -2.9993851]]...]
INFO - root - 2017-12-07 04:35:28.198161: step 9210, loss = 0.71, batch loss = 0.64 (9.6 examples/sec; 0.834 sec/batch; 74h:53m:03s remains)
INFO - root - 2017-12-07 04:35:36.460173: step 9220, loss = 0.59, batch loss = 0.52 (9.2 examples/sec; 0.867 sec/batch; 77h:49m:32s remains)
INFO - root - 2017-12-07 04:35:44.784377: step 9230, loss = 0.82, batch loss = 0.74 (10.3 examples/sec; 0.779 sec/batch; 69h:57m:44s remains)
INFO - root - 2017-12-07 04:35:53.043162: step 9240, loss = 0.85, batch loss = 0.78 (9.6 examples/sec; 0.833 sec/batch; 74h:47m:14s remains)
INFO - root - 2017-12-07 04:36:01.417069: step 9250, loss = 0.65, batch loss = 0.58 (9.4 examples/sec; 0.848 sec/batch; 76h:05m:58s remains)
INFO - root - 2017-12-07 04:36:09.730177: step 9260, loss = 0.69, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 72h:35m:24s remains)
INFO - root - 2017-12-07 04:36:18.046712: step 9270, loss = 0.86, batch loss = 0.78 (9.5 examples/sec; 0.844 sec/batch; 75h:45m:54s remains)
INFO - root - 2017-12-07 04:36:26.408091: step 9280, loss = 0.87, batch loss = 0.80 (8.8 examples/sec; 0.906 sec/batch; 81h:19m:59s remains)
INFO - root - 2017-12-07 04:36:34.625147: step 9290, loss = 0.86, batch loss = 0.79 (9.9 examples/sec; 0.804 sec/batch; 72h:12m:19s remains)
INFO - root - 2017-12-07 04:36:42.890105: step 9300, loss = 0.83, batch loss = 0.76 (10.0 examples/sec; 0.802 sec/batch; 71h:59m:35s remains)
2017-12-07 04:36:43.477712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0624361 -3.1550214 -3.1839077 -3.105927 -2.9033465 -2.653666 -2.4849019 -2.440834 -2.5649533 -2.8950114 -3.2024872 -3.2307143 -2.9571018 -2.4809184 -2.0212989][-3.1391573 -3.1383162 -3.0552144 -2.8968892 -2.6543665 -2.386472 -2.2552636 -2.303911 -2.5288963 -2.9087453 -3.1910367 -3.135777 -2.7508528 -2.2152553 -1.7820232][-3.1669006 -3.077672 -2.8922567 -2.6728306 -2.3913736 -2.1176653 -2.0322859 -2.1692302 -2.46831 -2.883126 -3.1619487 -3.06635 -2.6083395 -2.0294392 -1.6230736][-2.8773494 -2.7329009 -2.5103667 -2.289314 -2.0057304 -1.7722096 -1.7583156 -1.9653358 -2.3029528 -2.7034621 -2.9632161 -2.8646922 -2.4102349 -1.8651712 -1.5181639][-2.7637472 -2.5827777 -2.3383591 -2.1082165 -1.8368771 -1.6464846 -1.6659715 -1.8776886 -2.2099738 -2.5574179 -2.769155 -2.6501734 -2.2134714 -1.7281277 -1.4348085][-2.7217221 -2.587992 -2.3814085 -2.173383 -1.9376791 -1.7676783 -1.7358105 -1.8281541 -2.0384119 -2.2684548 -2.4154429 -2.2933016 -1.9172077 -1.5382161 -1.3255529][-2.7074394 -2.65747 -2.5080705 -2.2968488 -2.0272729 -1.8026056 -1.6647577 -1.6364231 -1.7397795 -1.8992338 -2.031616 -1.9366286 -1.6432991 -1.3746719 -1.2477517][-2.7153406 -2.7470417 -2.6307931 -2.3735003 -1.9868391 -1.6195469 -1.351887 -1.2818601 -1.3977365 -1.5887792 -1.7552938 -1.7128005 -1.5100892 -1.3389411 -1.2822697][-2.5064116 -2.5912673 -2.5181947 -2.298398 -1.9137266 -1.4897437 -1.1651735 -1.1265206 -1.311075 -1.5428021 -1.7088771 -1.679467 -1.5217412 -1.4135973 -1.4076259][-2.1963012 -2.2921381 -2.2487965 -2.0977423 -1.8005528 -1.4476359 -1.200388 -1.2385197 -1.4623933 -1.681354 -1.8211043 -1.7919621 -1.646646 -1.5381799 -1.5391533][-2.0716705 -2.2055335 -2.1716771 -1.9988444 -1.6816125 -1.3549328 -1.1856835 -1.2819641 -1.4936299 -1.6836424 -1.8463976 -1.8926563 -1.8096774 -1.7034109 -1.6853175][-2.0008144 -2.1843088 -2.1821575 -1.9988885 -1.6651776 -1.3751452 -1.2894497 -1.4235611 -1.6165838 -1.8139579 -2.0273578 -2.1263254 -2.0618758 -1.9212832 -1.8490918][-1.7331622 -1.940769 -1.9844587 -1.8598473 -1.6296916 -1.4773507 -1.4976408 -1.6494417 -1.8377278 -2.0762122 -2.3339565 -2.4411104 -2.3495262 -2.1477971 -1.9924445][-1.4855714 -1.65872 -1.7074728 -1.6461577 -1.5588849 -1.5564666 -1.6468322 -1.7895365 -1.961201 -2.1998639 -2.4454622 -2.5301032 -2.439455 -2.2354436 -2.0445628][-1.6105654 -1.6820633 -1.6731708 -1.6260295 -1.612648 -1.673532 -1.7757814 -1.8990948 -2.0499814 -2.243191 -2.4118061 -2.4336615 -2.3361583 -2.1538956 -1.9700444]]...]
INFO - root - 2017-12-07 04:36:51.774353: step 9310, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.825 sec/batch; 74h:04m:32s remains)
INFO - root - 2017-12-07 04:36:59.942598: step 9320, loss = 0.71, batch loss = 0.64 (9.3 examples/sec; 0.859 sec/batch; 77h:07m:14s remains)
INFO - root - 2017-12-07 04:37:08.310124: step 9330, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.790 sec/batch; 70h:53m:59s remains)
INFO - root - 2017-12-07 04:37:16.542822: step 9340, loss = 0.83, batch loss = 0.75 (9.6 examples/sec; 0.831 sec/batch; 74h:34m:48s remains)
INFO - root - 2017-12-07 04:37:24.848846: step 9350, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.827 sec/batch; 74h:16m:24s remains)
INFO - root - 2017-12-07 04:37:33.027660: step 9360, loss = 0.81, batch loss = 0.74 (10.2 examples/sec; 0.781 sec/batch; 70h:03m:35s remains)
INFO - root - 2017-12-07 04:37:41.254107: step 9370, loss = 1.19, batch loss = 1.12 (9.7 examples/sec; 0.822 sec/batch; 73h:45m:22s remains)
INFO - root - 2017-12-07 04:37:49.597485: step 9380, loss = 0.64, batch loss = 0.57 (9.2 examples/sec; 0.868 sec/batch; 77h:52m:02s remains)
INFO - root - 2017-12-07 04:37:57.869190: step 9390, loss = 0.70, batch loss = 0.63 (10.0 examples/sec; 0.798 sec/batch; 71h:37m:37s remains)
INFO - root - 2017-12-07 04:38:06.148727: step 9400, loss = 0.91, batch loss = 0.84 (9.9 examples/sec; 0.812 sec/batch; 72h:52m:51s remains)
2017-12-07 04:38:06.844230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0838008 -0.92421126 -0.93673372 -1.1107376 -1.1950908 -1.0550134 -0.92192507 -0.92277551 -1.0581431 -1.2199485 -1.1964891 -1.137754 -1.8446362 -2.8317695 -3.4355741][-1.3850956 -1.3284538 -1.4943883 -1.8185525 -2.0678937 -2.0083308 -1.8214915 -1.7749515 -1.9874089 -2.3221262 -2.2288623 -1.8677912 -2.3920362 -3.2853508 -3.7647431][-1.7340271 -1.8570046 -2.1650076 -2.5606995 -2.8530664 -2.8442101 -2.6506577 -2.6331801 -2.9419112 -3.3279114 -3.0092275 -2.3010688 -2.6991725 -3.5759583 -3.9643276][-1.8863962 -2.0367312 -2.3193679 -2.6917455 -2.9921489 -3.0482078 -2.8382051 -2.7567198 -3.0353673 -3.3836405 -2.9558954 -2.2236238 -2.7816579 -3.794378 -4.2040191][-0.81146312 -0.85491824 -1.0308371 -1.4158573 -1.9094982 -2.2270043 -2.1506784 -2.0234461 -2.1500247 -2.2531769 -1.6043627 -0.92083311 -1.7169113 -2.9633045 -3.5676115][0.55112839 0.63625383 0.6006794 0.13524199 -0.64494252 -1.277245 -1.4025247 -1.2176051 -1.0600216 -0.7885778 0.063487053 0.61761713 -0.36608076 -1.79339 -2.6328709][0.57509947 0.76400471 0.92885971 0.54351854 -0.13167858 -0.587332 -0.51022983 0.048805714 0.62176418 1.0260782 1.5069566 1.3962607 -0.026132107 -1.6779642 -2.629931][0.29573727 0.606657 1.0169444 0.92535734 0.6488142 0.56094217 0.89460707 1.8377585 2.7896256 3.1804729 3.0978632 2.2389441 0.41623783 -1.399807 -2.4754658][0.34027529 0.635623 1.0515122 1.0804338 1.0381036 0.99543428 1.1405644 2.0406818 3.1742501 3.6080608 3.2442608 2.155715 0.48189735 -1.0795748 -2.0720129][0.23296642 0.48820019 0.81726646 0.81873035 0.76108789 0.52394247 0.28082514 0.83374166 1.7666016 1.9598498 1.350924 0.27861118 -0.98321247 -1.9713881 -2.5476568][-0.59519768 -0.25891495 0.12739277 0.14400578 0.056215763 -0.27390146 -0.67320824 -0.37154055 0.26075268 0.17662859 -0.58225012 -1.5296152 -2.3804452 -2.8829169 -3.1096265][-1.8588338 -1.4806242 -1.0800071 -1.0523183 -1.1620896 -1.5122781 -1.9170768 -1.7590001 -1.4248898 -1.6605687 -2.2434196 -2.8300364 -3.2586577 -3.3734393 -3.3159018][-2.660449 -2.2848885 -2.0200496 -2.0999084 -2.2521799 -2.5901499 -2.9536524 -2.919188 -2.8787942 -3.1852818 -3.5031366 -3.7384744 -3.8443873 -3.699729 -3.459697][-3.1109118 -2.6544235 -2.5773063 -2.8460102 -3.0981951 -3.4013126 -3.65152 -3.6584966 -3.7312384 -3.8783202 -3.8693795 -3.8424783 -3.8192484 -3.6238539 -3.3799491][-3.3109422 -2.8717732 -2.9061618 -3.2677736 -3.6168432 -3.912159 -4.087316 -4.1122751 -4.1673131 -4.0464282 -3.6564305 -3.3994937 -3.3271098 -3.1956534 -3.029887]]...]
INFO - root - 2017-12-07 04:38:15.178175: step 9410, loss = 0.84, batch loss = 0.77 (9.3 examples/sec; 0.860 sec/batch; 77h:09m:11s remains)
INFO - root - 2017-12-07 04:38:23.514379: step 9420, loss = 0.72, batch loss = 0.64 (9.8 examples/sec; 0.819 sec/batch; 73h:32m:07s remains)
INFO - root - 2017-12-07 04:38:31.854201: step 9430, loss = 0.79, batch loss = 0.72 (10.2 examples/sec; 0.784 sec/batch; 70h:22m:13s remains)
INFO - root - 2017-12-07 04:38:40.149844: step 9440, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 0.833 sec/batch; 74h:44m:49s remains)
INFO - root - 2017-12-07 04:38:48.380212: step 9450, loss = 0.82, batch loss = 0.75 (9.9 examples/sec; 0.807 sec/batch; 72h:22m:40s remains)
INFO - root - 2017-12-07 04:38:56.750650: step 9460, loss = 0.90, batch loss = 0.83 (9.5 examples/sec; 0.843 sec/batch; 75h:36m:35s remains)
INFO - root - 2017-12-07 04:39:04.770340: step 9470, loss = 0.67, batch loss = 0.60 (10.1 examples/sec; 0.796 sec/batch; 71h:24m:37s remains)
INFO - root - 2017-12-07 04:39:13.037009: step 9480, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.783 sec/batch; 70h:16m:02s remains)
INFO - root - 2017-12-07 04:39:21.291337: step 9490, loss = 0.58, batch loss = 0.51 (9.6 examples/sec; 0.831 sec/batch; 74h:32m:12s remains)
INFO - root - 2017-12-07 04:39:29.537074: step 9500, loss = 0.80, batch loss = 0.73 (10.0 examples/sec; 0.800 sec/batch; 71h:46m:48s remains)
2017-12-07 04:39:30.226835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4980433 -4.221333 -3.8466182 -2.9968166 -1.8570473 -0.46775365 0.24042463 0.22016191 -0.12390327 -0.85532784 -2.06453 -3.2820208 -4.514492 -5.56037 -5.420001][-3.4767151 -4.2019067 -3.7341604 -2.7928715 -1.5759015 -0.034313679 0.75474405 0.79506254 0.43213272 -0.36169243 -1.54228 -2.7331634 -3.9822671 -5.1316838 -5.0785961][-3.3272169 -3.915936 -3.4069562 -2.5094619 -1.3799729 0.10615921 0.99983215 1.242528 0.97353745 0.10881519 -1.0919552 -2.2324049 -3.525975 -4.8354573 -4.9250412][-3.2301302 -3.611362 -3.1314611 -2.3124361 -1.2832389 0.11843443 1.1065135 1.5923576 1.4293346 0.52004004 -0.72154713 -1.9212244 -3.3760633 -4.8608909 -5.1127481][-3.0240679 -3.1427727 -2.6532803 -1.928154 -1.0675075 0.082709312 1.0040298 1.6124568 1.489121 0.57648611 -0.55894113 -1.7296903 -3.18687 -4.6408854 -4.9172707][-2.6786008 -2.595057 -2.025492 -1.2660849 -0.54949379 0.25631475 1.012641 1.7302351 1.7437282 0.9128027 -0.11573172 -1.3736262 -2.9426432 -4.4005737 -4.69086][-2.670022 -2.5069163 -1.7277865 -0.5929234 0.3130703 0.85702229 1.3006196 1.9510121 2.1428409 1.5264482 0.65933371 -0.67926383 -2.3760321 -3.9113696 -4.3804264][-3.11027 -2.8429012 -1.9118786 -0.41591454 0.82472277 1.2747703 1.4142404 1.8411555 2.0823255 1.7183719 0.98830748 -0.33065462 -2.0132878 -3.553956 -4.2064943][-3.42825 -3.0255427 -2.0771878 -0.49979115 0.85345411 1.2433028 1.0691462 1.1076698 1.220417 1.0852304 0.55244637 -0.604568 -2.1395824 -3.5352795 -4.1819911][-3.1988564 -2.8690171 -2.082716 -0.76005769 0.28577757 0.511395 0.15062475 -0.16684151 -0.17162991 -0.083228111 -0.33543777 -1.1883936 -2.3844628 -3.5008931 -4.0350013][-2.5981338 -2.5168927 -1.9754202 -0.9873836 -0.34092617 -0.28371 -0.65646172 -1.0811863 -1.1037018 -0.90486813 -1.0112436 -1.5852361 -2.34987 -3.0612276 -3.3972287][-2.1686676 -2.2448723 -1.8203995 -1.0518363 -0.66693354 -0.75554609 -1.1074853 -1.4634337 -1.490953 -1.3727434 -1.5064118 -1.8404951 -2.1191659 -2.4287977 -2.6007171][-1.9797156 -2.1487861 -1.8893182 -1.4154139 -1.2017851 -1.3144195 -1.5866213 -1.8432212 -1.8195477 -1.7102633 -1.8284461 -1.9475625 -1.8736928 -1.857192 -1.8661981][-1.76775 -1.964463 -1.9053869 -1.7354045 -1.6247947 -1.6740963 -1.8797772 -2.109175 -2.1097536 -1.9826779 -1.9761662 -1.9161868 -1.6838796 -1.497575 -1.3951809][-1.6150846 -1.7461295 -1.8040533 -1.8348217 -1.809257 -1.8096688 -1.9103336 -2.0612884 -2.0766816 -1.9742086 -1.9123037 -1.8320215 -1.6654382 -1.5279109 -1.4669518]]...]
INFO - root - 2017-12-07 04:39:38.384624: step 9510, loss = 0.61, batch loss = 0.54 (10.1 examples/sec; 0.793 sec/batch; 71h:08m:44s remains)
INFO - root - 2017-12-07 04:39:46.694031: step 9520, loss = 0.85, batch loss = 0.77 (10.0 examples/sec; 0.797 sec/batch; 71h:32m:22s remains)
INFO - root - 2017-12-07 04:39:54.963700: step 9530, loss = 0.87, batch loss = 0.80 (9.9 examples/sec; 0.808 sec/batch; 72h:30m:08s remains)
INFO - root - 2017-12-07 04:40:03.298024: step 9540, loss = 0.83, batch loss = 0.76 (9.7 examples/sec; 0.825 sec/batch; 74h:01m:54s remains)
INFO - root - 2017-12-07 04:40:11.606712: step 9550, loss = 0.80, batch loss = 0.73 (9.1 examples/sec; 0.875 sec/batch; 78h:30m:01s remains)
INFO - root - 2017-12-07 04:40:19.897537: step 9560, loss = 0.87, batch loss = 0.80 (9.6 examples/sec; 0.837 sec/batch; 75h:03m:26s remains)
INFO - root - 2017-12-07 04:40:28.153154: step 9570, loss = 0.76, batch loss = 0.68 (9.8 examples/sec; 0.817 sec/batch; 73h:18m:56s remains)
INFO - root - 2017-12-07 04:40:36.456505: step 9580, loss = 0.57, batch loss = 0.50 (9.7 examples/sec; 0.829 sec/batch; 74h:19m:42s remains)
INFO - root - 2017-12-07 04:40:44.733269: step 9590, loss = 0.71, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 74h:45m:37s remains)
INFO - root - 2017-12-07 04:40:52.959818: step 9600, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.802 sec/batch; 71h:54m:30s remains)
2017-12-07 04:40:53.663684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0475593 -4.0731058 -4.0560284 -3.9466093 -3.6532063 -3.1009798 -2.5380456 -2.3283489 -2.3353162 -2.3229432 -2.0589778 -1.6908 -1.2761016 -0.752893 -0.50870419][-3.340147 -3.4696662 -3.663938 -3.8417807 -3.6170115 -2.9698656 -2.3964665 -2.3536272 -2.7299387 -3.0944338 -3.0128551 -2.5012467 -1.7503934 -0.84511065 -0.26970863][-2.7100677 -2.9292867 -3.3109479 -3.7234869 -3.4960878 -2.7542491 -2.2096248 -2.2158601 -2.6900725 -3.277627 -3.4244077 -2.9515409 -2.1679623 -1.2481294 -0.64485097][-2.4169312 -2.6866813 -3.1404591 -3.6741557 -3.4501514 -2.6916294 -2.25793 -2.23843 -2.59203 -3.1857979 -3.4855216 -3.1866198 -2.5940297 -1.9058537 -1.5478423][-1.4793181 -1.7786021 -2.3359733 -3.11754 -3.1505706 -2.5750496 -2.2821457 -2.2196043 -2.4775229 -3.0508041 -3.4878905 -3.5018311 -3.2028441 -2.7461276 -2.6523902][-0.096508026 -0.26205683 -0.91712236 -2.0200841 -2.5269811 -2.2674727 -2.1144536 -2.07909 -2.4036849 -3.1049681 -3.7812262 -4.1818581 -4.111639 -3.7153249 -3.707855][0.37915802 0.39949226 -0.20697594 -1.3674974 -2.1168048 -1.9530368 -1.723238 -1.66253 -2.0549684 -2.8536196 -3.734556 -4.5115886 -4.6334944 -4.2851238 -4.3350935][-0.049452305 0.046129704 -0.43980956 -1.3221574 -1.9360507 -1.6487992 -1.3214738 -1.2631512 -1.6021907 -2.2868531 -3.1900954 -4.1289845 -4.2935266 -4.0358148 -4.23914][-0.74171877 -0.63243437 -1.0046484 -1.4949188 -1.8195379 -1.4547904 -1.2493436 -1.3225262 -1.5546389 -1.9815888 -2.6592932 -3.4769731 -3.4914036 -3.2753127 -3.5883541][-1.2719314 -1.1077578 -1.3178642 -1.4652383 -1.5613623 -1.2676227 -1.291301 -1.4503167 -1.4982512 -1.6651762 -2.0601966 -2.6577656 -2.5654709 -2.4544625 -2.8349802][-1.886821 -1.5711136 -1.4835005 -1.3246355 -1.2523611 -1.146874 -1.4090447 -1.6312621 -1.5956159 -1.5963459 -1.7421207 -2.1256666 -2.0181007 -2.0444753 -2.4449439][-2.7050054 -2.297034 -1.9699266 -1.6206706 -1.4312127 -1.5408545 -2.0382781 -2.319865 -2.2921185 -2.218704 -2.1870944 -2.2852178 -2.0771489 -2.1130507 -2.4271][-3.4359608 -3.1021233 -2.7458806 -2.3578603 -2.0829 -2.3182409 -2.8382893 -3.0099242 -2.982038 -2.8733006 -2.7296045 -2.62294 -2.3792188 -2.4172318 -2.5839605][-3.5892873 -3.4624684 -3.2429881 -2.9353156 -2.6739953 -2.9158161 -3.2660007 -3.2087798 -3.1459935 -3.0737009 -2.9088755 -2.7167408 -2.4814391 -2.4966826 -2.5163968][-2.8936603 -2.9928598 -3.0170302 -2.9155617 -2.8470397 -3.0909262 -3.2341394 -2.9996698 -2.9154937 -2.9109459 -2.7728205 -2.5996745 -2.4271679 -2.3671813 -2.2659681]]...]
INFO - root - 2017-12-07 04:41:02.029075: step 9610, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.810 sec/batch; 72h:38m:46s remains)
INFO - root - 2017-12-07 04:41:10.247477: step 9620, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.832 sec/batch; 74h:39m:30s remains)
INFO - root - 2017-12-07 04:41:18.459563: step 9630, loss = 0.64, batch loss = 0.57 (9.9 examples/sec; 0.810 sec/batch; 72h:39m:11s remains)
INFO - root - 2017-12-07 04:41:26.705748: step 9640, loss = 0.82, batch loss = 0.74 (9.8 examples/sec; 0.815 sec/batch; 73h:04m:22s remains)
INFO - root - 2017-12-07 04:41:34.949443: step 9650, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.822 sec/batch; 73h:44m:09s remains)
INFO - root - 2017-12-07 04:41:43.317401: step 9660, loss = 0.77, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:55m:55s remains)
INFO - root - 2017-12-07 04:41:51.583486: step 9670, loss = 0.72, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 72h:20m:26s remains)
INFO - root - 2017-12-07 04:41:59.886065: step 9680, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.816 sec/batch; 73h:12m:48s remains)
INFO - root - 2017-12-07 04:42:08.182086: step 9690, loss = 1.00, batch loss = 0.93 (9.5 examples/sec; 0.846 sec/batch; 75h:52m:59s remains)
INFO - root - 2017-12-07 04:42:16.458581: step 9700, loss = 0.64, batch loss = 0.57 (9.7 examples/sec; 0.823 sec/batch; 73h:47m:56s remains)
2017-12-07 04:42:17.102950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8115923 -3.6412604 -3.5050089 -3.6174111 -3.8550093 -4.0812726 -4.1213193 -3.9855506 -3.8126025 -3.8036466 -3.9753213 -4.1214523 -4.0554709 -3.8448353 -3.6291821][-3.1874585 -3.0720179 -2.9256845 -3.14349 -3.5660965 -3.9556644 -4.0599 -3.910718 -3.7271128 -3.8274999 -4.1665592 -4.4720197 -4.464191 -4.192534 -3.864419][-2.5104957 -2.3694167 -2.18743 -2.4815149 -2.9896507 -3.407481 -3.4806464 -3.2322838 -2.9562922 -3.154202 -3.7622201 -4.3993244 -4.624033 -4.3996625 -4.0401855][-1.9815314 -1.6807308 -1.4903011 -1.8826807 -2.3526294 -2.5935533 -2.4512749 -1.9428039 -1.5593328 -1.9849691 -3.0498729 -4.1331706 -4.6348081 -4.4723344 -4.1265588][-1.6130023 -1.0102656 -0.77516127 -1.2581463 -1.7059097 -1.7960129 -1.4058118 -0.64826465 -0.25615072 -0.95223117 -2.4709814 -3.9219031 -4.6531787 -4.5468054 -4.195745][-1.9836748 -1.1244593 -0.61130905 -0.81208849 -0.96953893 -0.85227656 -0.23053789 0.56882334 0.71243668 -0.27780294 -2.0722945 -3.7034631 -4.5704026 -4.5451617 -4.1956253][-3.027638 -2.1059494 -1.3187203 -0.956933 -0.458174 0.1903162 1.2082577 1.9449444 1.5684881 0.13144922 -1.8816381 -3.5377948 -4.3937392 -4.4222741 -4.1094232][-3.691103 -2.9098659 -2.1797183 -1.5738785 -0.65919065 0.45954227 1.8892393 2.6849284 1.9392819 0.18698502 -1.9400647 -3.5408776 -4.3236756 -4.3635345 -4.0866818][-3.6544943 -3.060257 -2.5835555 -2.1654255 -1.445255 -0.4360342 0.939991 1.8136129 1.3912888 -0.077868462 -1.960808 -3.3984623 -4.12082 -4.2439694 -4.1117568][-3.2916522 -2.833591 -2.5699778 -2.4459293 -2.1380589 -1.6069272 -0.61854744 0.26677418 0.35978603 -0.51894665 -1.9482121 -3.1328888 -3.7722208 -3.983222 -4.0190439][-2.8743391 -2.6160212 -2.5383139 -2.5557189 -2.4352553 -2.2299254 -1.6341901 -0.90554166 -0.65292335 -1.1754017 -2.1685622 -3.064039 -3.5764489 -3.7842457 -3.8711536][-2.5807834 -2.52149 -2.6177957 -2.6794381 -2.6006339 -2.517592 -2.1028388 -1.5547698 -1.4500358 -1.8958714 -2.6125283 -3.2455626 -3.5972431 -3.7327998 -3.7977161][-2.8959963 -2.95795 -3.1380279 -3.2047184 -3.1242366 -3.0221829 -2.5585051 -2.1008108 -2.2086022 -2.7499795 -3.373255 -3.7900434 -3.8821166 -3.7839646 -3.7495084][-3.4089775 -3.5011744 -3.7188358 -3.8296356 -3.8354254 -3.7960925 -3.3476894 -2.9221044 -3.0969877 -3.6612957 -4.2248845 -4.453826 -4.2435908 -3.8678374 -3.691576][-3.8156507 -3.8737445 -4.0556355 -4.1608829 -4.2161484 -4.2618976 -3.933661 -3.5916276 -3.770957 -4.2656569 -4.7211452 -4.8277268 -4.4733024 -3.9807925 -3.7072868]]...]
INFO - root - 2017-12-07 04:42:25.403154: step 9710, loss = 0.62, batch loss = 0.55 (9.6 examples/sec; 0.832 sec/batch; 74h:34m:22s remains)
INFO - root - 2017-12-07 04:42:33.829633: step 9720, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.802 sec/batch; 71h:52m:39s remains)
INFO - root - 2017-12-07 04:42:42.112792: step 9730, loss = 0.89, batch loss = 0.82 (9.7 examples/sec; 0.828 sec/batch; 74h:15m:34s remains)
INFO - root - 2017-12-07 04:42:50.364965: step 9740, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.794 sec/batch; 71h:09m:40s remains)
INFO - root - 2017-12-07 04:42:58.663071: step 9750, loss = 0.86, batch loss = 0.79 (9.5 examples/sec; 0.841 sec/batch; 75h:23m:45s remains)
INFO - root - 2017-12-07 04:43:06.900327: step 9760, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.845 sec/batch; 75h:47m:27s remains)
INFO - root - 2017-12-07 04:43:15.324581: step 9770, loss = 0.68, batch loss = 0.60 (9.6 examples/sec; 0.837 sec/batch; 75h:01m:20s remains)
INFO - root - 2017-12-07 04:43:23.467843: step 9780, loss = 0.72, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:43m:47s remains)
INFO - root - 2017-12-07 04:43:31.687800: step 9790, loss = 0.81, batch loss = 0.73 (9.7 examples/sec; 0.822 sec/batch; 73h:43m:31s remains)
INFO - root - 2017-12-07 04:43:40.042086: step 9800, loss = 0.76, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:55m:27s remains)
2017-12-07 04:43:40.670287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8570008 -2.8636901 -2.8764882 -2.8836904 -2.8792765 -2.8210812 -2.7487946 -2.6819868 -2.6128008 -2.5491314 -2.4664538 -2.4300671 -2.2169826 -1.8503788 -1.6315165][-3.7032547 -3.7002807 -3.7123804 -3.7337329 -3.7775421 -3.7615502 -3.7182534 -3.6610625 -3.6084812 -3.5958123 -3.5807078 -3.5564167 -3.2015924 -2.5346689 -1.9908757][-3.1765804 -3.1342144 -3.1080978 -3.1273119 -3.2476757 -3.3220875 -3.3453188 -3.3327122 -3.3594594 -3.5003827 -3.6473966 -3.7359378 -3.3966889 -2.5932088 -1.8370643][-1.7262039 -1.6249394 -1.4866257 -1.4011118 -1.5159266 -1.6639395 -1.7712116 -1.8280013 -1.9750075 -2.3572261 -2.7589087 -3.0547905 -2.9017162 -2.1916504 -1.4410665][-0.025266171 0.16356802 0.51957512 0.87335014 0.94699717 0.86211014 0.73234844 0.61821175 0.32289934 -0.39514685 -1.1346982 -1.7193346 -1.9207549 -1.4942937 -0.916512][0.838254 1.0671182 1.6200771 2.2878823 2.6858344 2.857626 2.9114585 2.9119954 2.572114 1.4861708 0.31233978 -0.66374135 -1.2943575 -1.2202218 -0.86676812][0.5906601 0.80901194 1.4129181 2.2032733 2.7965479 3.2371349 3.59408 3.8922958 3.75393 2.5738282 1.1912303 -0.0041370392 -0.96080852 -1.204576 -1.0935965][-0.014897823 0.13162613 0.56560659 1.138423 1.5480857 1.980597 2.4805574 3.0404344 3.2642188 2.37147 1.1902828 0.1196928 -0.87659645 -1.2487907 -1.3091252][-0.12335682 -0.11926699 0.020842075 0.20309258 0.2132206 0.40114975 0.80409288 1.4202871 1.8991361 1.3609405 0.53250074 -0.19581842 -0.95196247 -1.1789763 -1.2568836][0.40426302 0.28900385 0.17396259 0.019004345 -0.32588673 -0.41836405 -0.2326808 0.22176647 0.69013405 0.29720879 -0.27094746 -0.6387682 -1.0350559 -0.9599297 -0.92460275][0.99437284 0.93324709 0.79115391 0.51172352 -0.022322655 -0.35054541 -0.4145689 -0.24375105 0.020152092 -0.41456842 -0.87703204 -0.99934649 -1.0941131 -0.7253952 -0.54613996][1.1816792 1.3059072 1.358994 1.2175117 0.73853779 0.32971764 0.094799519 -0.00024652481 -0.0082960129 -0.564796 -0.99681377 -1.0154798 -0.97255445 -0.44757652 -0.17979097][1.0330486 1.3372169 1.6798859 1.8776016 1.708714 1.4590888 1.2593145 1.0550203 0.831213 0.12582445 -0.35786247 -0.42974615 -0.43832588 0.062416553 0.28175688][0.72597837 1.1464233 1.7032375 2.2217231 2.4407558 2.4992781 2.4932098 2.3571286 2.0571179 1.2374077 0.6389389 0.42828512 0.26505518 0.59481573 0.66949129][0.23471498 0.73584986 1.3906455 2.0704141 2.5569086 2.8871684 3.0996213 3.1552873 2.9569659 2.1433773 1.4266582 1.014195 0.63427114 0.7262702 0.68884945]]...]
INFO - root - 2017-12-07 04:43:48.971100: step 9810, loss = 0.92, batch loss = 0.85 (9.2 examples/sec; 0.865 sec/batch; 77h:34m:44s remains)
INFO - root - 2017-12-07 04:43:57.270138: step 9820, loss = 0.59, batch loss = 0.51 (9.7 examples/sec; 0.826 sec/batch; 73h:59m:36s remains)
INFO - root - 2017-12-07 04:44:05.594368: step 9830, loss = 0.60, batch loss = 0.53 (9.2 examples/sec; 0.873 sec/batch; 78h:17m:20s remains)
INFO - root - 2017-12-07 04:44:13.927483: step 9840, loss = 0.87, batch loss = 0.79 (9.6 examples/sec; 0.837 sec/batch; 75h:03m:17s remains)
INFO - root - 2017-12-07 04:44:22.196659: step 9850, loss = 0.54, batch loss = 0.47 (9.8 examples/sec; 0.815 sec/batch; 73h:02m:01s remains)
INFO - root - 2017-12-07 04:44:30.412487: step 9860, loss = 0.65, batch loss = 0.58 (10.2 examples/sec; 0.786 sec/batch; 70h:27m:36s remains)
INFO - root - 2017-12-07 04:44:38.648471: step 9870, loss = 0.95, batch loss = 0.88 (9.6 examples/sec; 0.835 sec/batch; 74h:51m:18s remains)
INFO - root - 2017-12-07 04:44:46.948596: step 9880, loss = 0.84, batch loss = 0.76 (9.5 examples/sec; 0.840 sec/batch; 75h:14m:32s remains)
INFO - root - 2017-12-07 04:44:55.277919: step 9890, loss = 0.67, batch loss = 0.60 (9.2 examples/sec; 0.867 sec/batch; 77h:43m:29s remains)
INFO - root - 2017-12-07 04:45:03.649446: step 9900, loss = 0.74, batch loss = 0.67 (9.7 examples/sec; 0.821 sec/batch; 73h:35m:55s remains)
2017-12-07 04:45:04.280684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7677608 -1.9242167 -2.06482 -2.0447834 -1.9234979 -1.8497195 -1.8878446 -2.0044973 -2.1343009 -2.2252853 -2.2770736 -2.2969797 -2.2838812 -2.2699263 -2.2740881][-1.5027571 -1.593477 -1.7239037 -1.7052917 -1.5858493 -1.5308998 -1.6221759 -1.7804675 -1.9675448 -2.116257 -2.1837835 -2.1768932 -2.1013937 -2.0424533 -2.0286071][-1.8010793 -1.9347026 -2.0817475 -2.0689645 -1.9524808 -1.9043944 -2.0102034 -2.1766813 -2.4137731 -2.6463928 -2.709703 -2.5861406 -2.3547616 -2.1976256 -2.1371112][-2.6495116 -2.7659855 -2.8353996 -2.7477109 -2.5987847 -2.5379748 -2.6499767 -2.843647 -3.1360373 -3.4338696 -3.4583824 -3.17096 -2.7940557 -2.6170123 -2.596426][-3.4969015 -3.4349179 -3.2953277 -3.0760612 -2.8703251 -2.7313585 -2.7562881 -2.8976891 -3.1218038 -3.3813806 -3.3513145 -2.9723454 -2.6193295 -2.6395917 -2.7973442][-3.8772812 -3.5805044 -3.1906071 -2.7756453 -2.4264169 -2.2049844 -2.2004595 -2.3247869 -2.4066093 -2.5059657 -2.386291 -2.0029173 -1.8160174 -2.1122727 -2.4396222][-3.693814 -3.1635659 -2.5369287 -1.9331448 -1.4488015 -1.1748362 -1.2019961 -1.3702655 -1.4547067 -1.5559249 -1.4962327 -1.2073326 -1.1639006 -1.5561702 -1.8360901][-3.2570167 -2.6527042 -2.0301065 -1.5347798 -1.2127573 -1.0348997 -1.0230129 -1.0261767 -1.0410011 -1.2290764 -1.3660092 -1.3141589 -1.4150965 -1.7936711 -1.9051404][-2.9463496 -2.4434657 -2.027247 -1.8047168 -1.7384412 -1.7135956 -1.6948538 -1.5785568 -1.6096129 -1.9591982 -2.3053825 -2.44738 -2.5998921 -2.8496256 -2.7605104][-2.8499174 -2.6600611 -2.5565853 -2.568804 -2.599957 -2.5736208 -2.5011966 -2.362324 -2.516155 -3.0295305 -3.502799 -3.7346156 -3.8343115 -3.9010892 -3.64405][-3.0229559 -3.0337162 -3.1485243 -3.3097582 -3.3422289 -3.2477276 -3.0936849 -2.9409938 -3.0757446 -3.4904246 -3.8772151 -4.1414585 -4.2610421 -4.2765226 -3.9882021][-3.6321261 -3.6552203 -3.7562141 -3.8913031 -3.8812723 -3.7746625 -3.6499763 -3.5197344 -3.447932 -3.4178915 -3.4494405 -3.6669161 -3.9028578 -4.03847 -3.8730578][-3.8935535 -3.8129232 -3.8228207 -3.920578 -3.8838029 -3.7193928 -3.5595372 -3.404135 -3.1436486 -2.73944 -2.548017 -2.8272214 -3.2732093 -3.5593956 -3.4627693][-3.8739867 -3.6998963 -3.6515751 -3.7594995 -3.7278357 -3.4584222 -3.1596203 -2.9272842 -2.5974369 -2.0813665 -1.8570158 -2.2316921 -2.8452635 -3.1957407 -3.0899529][-3.723834 -3.553221 -3.4704196 -3.5144887 -3.4346685 -3.1048865 -2.7427952 -2.5254669 -2.3072991 -1.9498661 -1.8350394 -2.2262149 -2.8062773 -3.1001496 -2.9807048]]...]
INFO - root - 2017-12-07 04:45:12.654151: step 9910, loss = 0.89, batch loss = 0.82 (9.3 examples/sec; 0.860 sec/batch; 77h:04m:33s remains)
INFO - root - 2017-12-07 04:45:21.246748: step 9920, loss = 0.75, batch loss = 0.67 (9.3 examples/sec; 0.861 sec/batch; 77h:08m:41s remains)
INFO - root - 2017-12-07 04:45:29.682967: step 9930, loss = 0.74, batch loss = 0.67 (8.9 examples/sec; 0.897 sec/batch; 80h:19m:52s remains)
INFO - root - 2017-12-07 04:45:38.026700: step 9940, loss = 0.64, batch loss = 0.57 (9.8 examples/sec; 0.817 sec/batch; 73h:11m:03s remains)
INFO - root - 2017-12-07 04:45:46.598396: step 9950, loss = 1.06, batch loss = 0.98 (9.3 examples/sec; 0.860 sec/batch; 77h:01m:57s remains)
INFO - root - 2017-12-07 04:45:55.014182: step 9960, loss = 0.74, batch loss = 0.67 (10.1 examples/sec; 0.795 sec/batch; 71h:13m:14s remains)
INFO - root - 2017-12-07 04:46:03.392886: step 9970, loss = 0.93, batch loss = 0.85 (9.7 examples/sec; 0.821 sec/batch; 73h:34m:01s remains)
INFO - root - 2017-12-07 04:46:11.799478: step 9980, loss = 0.60, batch loss = 0.52 (9.2 examples/sec; 0.867 sec/batch; 77h:41m:07s remains)
INFO - root - 2017-12-07 04:46:20.219264: step 9990, loss = 0.73, batch loss = 0.65 (9.8 examples/sec; 0.820 sec/batch; 73h:29m:04s remains)
INFO - root - 2017-12-07 04:46:28.574016: step 10000, loss = 0.83, batch loss = 0.76 (9.7 examples/sec; 0.826 sec/batch; 73h:59m:06s remains)
2017-12-07 04:46:29.182999: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1313562 -4.2093959 -4.2423768 -4.0876122 -3.8752129 -3.744698 -3.6931441 -3.736125 -3.8437409 -3.958322 -4.02961 -3.9751077 -3.6653476 -3.4123704 -3.6263313][-3.9308717 -4.1409745 -4.266036 -4.1082077 -3.8176641 -3.5730178 -3.3982816 -3.3416393 -3.4484649 -3.6778347 -3.8232651 -3.7519717 -3.417419 -3.1751804 -3.3796759][-3.7450547 -4.011405 -4.116209 -3.9136469 -3.5728798 -3.2325506 -2.8962698 -2.6557493 -2.7142782 -3.0874572 -3.362318 -3.3798783 -3.166693 -3.0311158 -3.2123654][-3.6195595 -3.7794542 -3.6944244 -3.3550096 -3.0054684 -2.710381 -2.3140857 -1.8916452 -1.8888905 -2.3697922 -2.7836418 -2.9649384 -2.9919729 -2.9594486 -3.008925][-3.6354198 -3.586442 -3.2747588 -2.8370986 -2.5079279 -2.2084992 -1.6147578 -0.93038106 -0.93113828 -1.5679595 -2.1789162 -2.6175547 -2.9456134 -2.950603 -2.7566996][-3.4193635 -3.2026608 -2.7904067 -2.3948851 -1.9855208 -1.3653357 -0.27485609 0.69810057 0.46446943 -0.60335803 -1.5601437 -2.233449 -2.739171 -2.7516279 -2.4273605][-2.7379456 -2.4979393 -2.1814406 -1.9326792 -1.3732517 -0.23122692 1.5039959 2.7112699 2.0134773 0.33334446 -1.0464811 -1.9297907 -2.4545896 -2.4335742 -2.1633806][-1.8565729 -1.7244773 -1.598244 -1.5279729 -0.9990592 0.29836941 2.2216983 3.4256487 2.4784503 0.70010996 -0.71438169 -1.6190763 -2.1256821 -2.1799114 -2.0713668][-1.1843128 -1.2353473 -1.3254547 -1.4164641 -1.0564246 -0.066274166 1.3297448 2.0941143 1.3811603 0.21616697 -0.63632107 -1.254782 -1.7135506 -1.9156835 -1.9994416][-0.77526736 -1.0694306 -1.3457601 -1.5149856 -1.2741132 -0.58439445 0.25599623 0.63127327 0.15419817 -0.47293091 -0.83311009 -1.1643744 -1.5152321 -1.7125568 -1.7764463][-0.60945439 -1.1488831 -1.5674613 -1.8011913 -1.7141051 -1.2896881 -0.81868863 -0.67418647 -0.92171359 -1.1601737 -1.2315996 -1.4317071 -1.7602499 -1.9431243 -1.9333065][-0.82774806 -1.4980276 -2.0138371 -2.2588387 -2.2335651 -1.9530194 -1.621042 -1.4852662 -1.4943223 -1.4736178 -1.4393754 -1.6474092 -2.0987957 -2.463697 -2.5835352][-1.3262603 -1.9042695 -2.4089994 -2.5985146 -2.4838643 -2.159905 -1.828717 -1.6573105 -1.5265906 -1.3620245 -1.2948811 -1.4950032 -1.9895804 -2.5239992 -2.8352389][-1.7865534 -2.1216898 -2.4572775 -2.51856 -2.3000207 -2.0196018 -1.8552289 -1.8311696 -1.7157567 -1.4646018 -1.3106859 -1.3551614 -1.6862111 -2.1778564 -2.5816226][-1.9984102 -2.3038816 -2.6373656 -2.5968571 -2.1665485 -1.8047287 -1.7644792 -1.9470518 -2.03068 -1.9558485 -1.8608153 -1.7598503 -1.7835033 -1.9863904 -2.264282]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 04:46:38.094703: step 10010, loss = 0.62, batch loss = 0.55 (9.7 examples/sec; 0.822 sec/batch; 73h:39m:36s remains)
INFO - root - 2017-12-07 04:46:46.504245: step 10020, loss = 1.15, batch loss = 1.08 (10.0 examples/sec; 0.802 sec/batch; 71h:49m:19s remains)
INFO - root - 2017-12-07 04:46:54.816305: step 10030, loss = 0.87, batch loss = 0.80 (9.5 examples/sec; 0.844 sec/batch; 75h:34m:44s remains)
INFO - root - 2017-12-07 04:47:03.097407: step 10040, loss = 0.89, batch loss = 0.82 (10.1 examples/sec; 0.794 sec/batch; 71h:07m:13s remains)
INFO - root - 2017-12-07 04:47:11.552826: step 10050, loss = 0.89, batch loss = 0.82 (9.5 examples/sec; 0.838 sec/batch; 75h:05m:53s remains)
INFO - root - 2017-12-07 04:47:19.934989: step 10060, loss = 0.80, batch loss = 0.72 (9.6 examples/sec; 0.836 sec/batch; 74h:54m:46s remains)
INFO - root - 2017-12-07 04:47:28.271155: step 10070, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.818 sec/batch; 73h:14m:31s remains)
INFO - root - 2017-12-07 04:47:36.712173: step 10080, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.819 sec/batch; 73h:18m:22s remains)
INFO - root - 2017-12-07 04:47:45.090074: step 10090, loss = 0.78, batch loss = 0.71 (9.6 examples/sec; 0.833 sec/batch; 74h:34m:22s remains)
INFO - root - 2017-12-07 04:47:53.230581: step 10100, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.861 sec/batch; 77h:04m:30s remains)
2017-12-07 04:47:53.956662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9936628 -3.283087 -3.4074798 -3.3381829 -3.1047673 -2.9954529 -3.0154796 -3.0900993 -3.1112559 -3.0783582 -3.0835319 -3.1369293 -3.1106524 -3.1202927 -3.1284339][-2.9324305 -3.1801767 -3.2873731 -3.2310524 -3.0038867 -2.9188671 -2.9961827 -3.0806689 -3.0568407 -2.9753873 -2.9267917 -2.9470682 -2.9508457 -2.9712894 -2.9548578][-2.8942876 -3.0911553 -3.1486559 -3.070435 -2.8536987 -2.8378897 -3.0290747 -3.1748509 -3.1567428 -3.0588703 -2.944057 -2.87887 -2.8610463 -2.8572533 -2.8070474][-2.7584474 -2.8712289 -2.8318989 -2.683291 -2.4771361 -2.5441222 -2.8696163 -3.1477003 -3.2252572 -3.1465306 -2.9216175 -2.6832285 -2.58956 -2.5996373 -2.6613085][-2.5370612 -2.543128 -2.3898349 -2.156867 -1.9824543 -2.130604 -2.5537009 -2.9880805 -3.2022169 -3.1275716 -2.7563529 -2.3233202 -2.1252882 -2.1205444 -2.3094356][-2.4039228 -2.2664244 -1.9495287 -1.5434213 -1.2974823 -1.3955975 -1.8140998 -2.411535 -2.848501 -2.9040277 -2.5679932 -2.1418376 -1.9423387 -1.8726068 -2.0058219][-2.4232461 -2.1421647 -1.6314065 -0.94413638 -0.4322865 -0.25813627 -0.50793147 -1.1695266 -1.8708947 -2.2362108 -2.2187264 -2.114593 -2.1320965 -2.0633428 -2.0662837][-2.534411 -2.2599728 -1.6791458 -0.75903773 0.11268234 0.69030571 0.75571585 0.16545773 -0.75761294 -1.4809611 -1.8407245 -2.1196582 -2.3984494 -2.4103265 -2.3448761][-2.5600193 -2.4750869 -2.0570364 -1.1787038 -0.17949152 0.573534 0.81008291 0.39204073 -0.49780154 -1.2956409 -1.7339964 -2.153137 -2.5511994 -2.6694803 -2.6423793][-2.4389853 -2.6187048 -2.5402315 -2.018321 -1.2542238 -0.6858778 -0.4789381 -0.66884971 -1.233258 -1.7043855 -1.8354666 -2.0959671 -2.4742227 -2.7089686 -2.7617497][-2.252732 -2.5789981 -2.7809792 -2.6677709 -2.2828932 -2.0466788 -1.9844973 -2.0532694 -2.2456789 -2.2457948 -1.9242682 -1.8513896 -2.0703924 -2.3626788 -2.4359362][-1.9883666 -2.313978 -2.6458147 -2.7989292 -2.6918008 -2.7235498 -2.8444042 -2.95776 -2.9757133 -2.6843104 -2.0614805 -1.6535738 -1.5982149 -1.7942271 -1.7924135][-1.9176021 -2.2141614 -2.5470853 -2.7169764 -2.6312432 -2.7171638 -2.9382625 -3.1666129 -3.2022562 -2.8580475 -2.1988494 -1.6368151 -1.361928 -1.4031413 -1.2687204][-2.2010882 -2.5136108 -2.779357 -2.7922196 -2.5436134 -2.4949927 -2.6432953 -2.8640943 -2.9077868 -2.6274233 -2.1007996 -1.584487 -1.2579002 -1.2253468 -1.0422463][-2.6104884 -2.971566 -3.2033665 -3.1072865 -2.771122 -2.5926814 -2.553906 -2.5888302 -2.5651064 -2.4071999 -2.112669 -1.7715905 -1.5115142 -1.4857554 -1.3622665]]...]
INFO - root - 2017-12-07 04:48:02.369122: step 10110, loss = 0.65, batch loss = 0.57 (9.4 examples/sec; 0.848 sec/batch; 75h:55m:43s remains)
INFO - root - 2017-12-07 04:48:10.732683: step 10120, loss = 0.89, batch loss = 0.82 (9.5 examples/sec; 0.842 sec/batch; 75h:25m:51s remains)
INFO - root - 2017-12-07 04:48:19.039062: step 10130, loss = 0.79, batch loss = 0.72 (9.2 examples/sec; 0.870 sec/batch; 77h:52m:46s remains)
INFO - root - 2017-12-07 04:48:27.509834: step 10140, loss = 0.76, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:27m:52s remains)
INFO - root - 2017-12-07 04:48:35.870096: step 10150, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.797 sec/batch; 71h:21m:20s remains)
INFO - root - 2017-12-07 04:48:44.216830: step 10160, loss = 0.55, batch loss = 0.48 (9.4 examples/sec; 0.852 sec/batch; 76h:16m:29s remains)
INFO - root - 2017-12-07 04:48:52.454809: step 10170, loss = 0.79, batch loss = 0.72 (10.1 examples/sec; 0.788 sec/batch; 70h:34m:52s remains)
INFO - root - 2017-12-07 04:49:00.714913: step 10180, loss = 0.71, batch loss = 0.64 (9.4 examples/sec; 0.848 sec/batch; 75h:54m:26s remains)
INFO - root - 2017-12-07 04:49:09.171377: step 10190, loss = 0.70, batch loss = 0.63 (9.2 examples/sec; 0.872 sec/batch; 78h:04m:44s remains)
INFO - root - 2017-12-07 04:49:17.418042: step 10200, loss = 0.76, batch loss = 0.68 (10.4 examples/sec; 0.772 sec/batch; 69h:07m:22s remains)
2017-12-07 04:49:18.051015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8730974 -2.7760925 -2.8252969 -2.9112267 -2.9898973 -3.0219114 -2.949858 -2.7623558 -2.6843491 -2.7511256 -2.8039894 -2.8725884 -2.9880791 -3.0731187 -3.1062546][-2.8482115 -2.7928586 -2.8459964 -2.9008827 -2.9973087 -3.1200843 -3.1358812 -2.984031 -2.887249 -2.9480433 -3.0291314 -3.1105819 -3.1770124 -3.1674085 -3.1374831][-2.8248138 -2.8354826 -2.9068837 -2.959703 -3.081934 -3.2684851 -3.3311141 -3.1572552 -3.0018497 -3.0406618 -3.1411633 -3.2208457 -3.2296834 -3.14571 -3.0833156][-2.8479321 -2.9056156 -2.9819105 -3.0507092 -3.1936083 -3.4105546 -3.4509945 -3.1722631 -2.9104233 -2.9070048 -3.0300326 -3.1302848 -3.128664 -3.0487289 -3.0167589][-2.8687134 -2.9352474 -3.0319738 -3.1338849 -3.2769825 -3.4745915 -3.4335232 -3.0138097 -2.6355619 -2.6116967 -2.81048 -2.9927933 -3.0304143 -2.9985271 -3.0121655][-2.9061031 -3.0006671 -3.1393118 -3.2599506 -3.3355165 -3.4076304 -3.1809392 -2.5681465 -2.0869412 -2.1212616 -2.4983358 -2.8414869 -2.967032 -2.9883301 -3.0152605][-2.977654 -3.1182003 -3.2878275 -3.3720307 -3.3049715 -3.1544037 -2.6573367 -1.8484352 -1.3741443 -1.6097665 -2.2486598 -2.7732239 -2.9952371 -3.0427732 -3.0240951][-3.0800619 -3.2071004 -3.3243322 -3.2847214 -3.0315435 -2.6397107 -1.9157004 -1.0599089 -0.78670692 -1.3542697 -2.2546687 -2.890924 -3.1395464 -3.1628263 -3.0614734][-3.2669005 -3.2599659 -3.1892562 -2.9566693 -2.5685031 -2.0853469 -1.3821657 -0.75810766 -0.81207585 -1.6222644 -2.5752852 -3.1207511 -3.2665641 -3.2109475 -3.0400989][-3.4595695 -3.2492943 -2.9657168 -2.5912809 -2.2074592 -1.8818767 -1.5119357 -1.3030431 -1.5675828 -2.3024349 -3.0146184 -3.3053594 -3.2855277 -3.1280046 -2.917798][-3.5626323 -3.2289004 -2.8826504 -2.5166359 -2.2696009 -2.231792 -2.2342377 -2.2703514 -2.4564147 -2.8501773 -3.1760449 -3.2257335 -3.1238608 -2.9287415 -2.7554982][-3.5142894 -3.2037287 -2.9730716 -2.7495108 -2.6955695 -2.8956289 -3.1015086 -3.1416209 -3.0947266 -3.1181209 -3.1242492 -3.0604911 -2.9964771 -2.8473864 -2.7658277][-3.3375442 -3.1626883 -3.0863183 -2.991236 -3.0671883 -3.3755198 -3.6349051 -3.6183286 -3.4358528 -3.2792141 -3.1544955 -3.105269 -3.1176114 -3.017864 -2.9971566][-3.1383073 -3.1087441 -3.1242061 -3.0961447 -3.224997 -3.5435722 -3.7754047 -3.7431607 -3.5550671 -3.396009 -3.279757 -3.2857056 -3.3639371 -3.3003876 -3.2886453][-2.9724703 -3.0416403 -3.0640128 -3.0372496 -3.1588526 -3.4316359 -3.6129122 -3.586257 -3.4269133 -3.3066206 -3.2357426 -3.2920895 -3.4220757 -3.4018326 -3.3584285]]...]
INFO - root - 2017-12-07 04:49:26.519793: step 10210, loss = 0.70, batch loss = 0.63 (9.3 examples/sec; 0.858 sec/batch; 76h:50m:36s remains)
INFO - root - 2017-12-07 04:49:34.944143: step 10220, loss = 0.71, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 74h:35m:04s remains)
INFO - root - 2017-12-07 04:49:43.345692: step 10230, loss = 0.87, batch loss = 0.80 (8.9 examples/sec; 0.900 sec/batch; 80h:33m:26s remains)
INFO - root - 2017-12-07 04:49:51.680908: step 10240, loss = 0.78, batch loss = 0.71 (9.9 examples/sec; 0.806 sec/batch; 72h:10m:15s remains)
INFO - root - 2017-12-07 04:50:00.044447: step 10250, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 74h:55m:48s remains)
INFO - root - 2017-12-07 04:50:08.350020: step 10260, loss = 0.75, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 76h:48m:33s remains)
INFO - root - 2017-12-07 04:50:16.684304: step 10270, loss = 0.92, batch loss = 0.85 (10.0 examples/sec; 0.802 sec/batch; 71h:49m:19s remains)
INFO - root - 2017-12-07 04:50:25.026075: step 10280, loss = 0.87, batch loss = 0.80 (9.5 examples/sec; 0.844 sec/batch; 75h:35m:11s remains)
INFO - root - 2017-12-07 04:50:33.339178: step 10290, loss = 0.82, batch loss = 0.74 (9.7 examples/sec; 0.825 sec/batch; 73h:52m:36s remains)
INFO - root - 2017-12-07 04:50:41.713909: step 10300, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 73h:11m:20s remains)
2017-12-07 04:50:42.383066: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0975304 1.753685 1.6509638 1.6965137 1.8100839 1.9676695 2.0748158 2.0751777 2.0247493 1.9183407 1.6625152 0.9542284 -0.079712391 -0.83821559 -1.0466037][1.0194864 0.5904026 0.40664864 0.34752798 0.35295105 0.44130754 0.5183053 0.51334143 0.57746077 0.66772652 0.59052229 0.03908968 -0.81976223 -1.4248195 -1.5331395][-0.010873795 -0.47261214 -0.76096439 -0.96512413 -1.0693703 -1.0649619 -1.0622346 -1.1497571 -1.0220597 -0.76920915 -0.70184255 -1.0623536 -1.691555 -2.1550403 -2.1813934][-0.57135344 -0.97361374 -1.2658651 -1.5338161 -1.6979587 -1.7192557 -1.7199795 -1.8142855 -1.6318307 -1.3733146 -1.3847282 -1.74811 -2.3270397 -2.7717 -2.7405849][-0.74500704 -1.086009 -1.2371266 -1.3775008 -1.502094 -1.5367467 -1.5260773 -1.5576229 -1.3078625 -1.0966442 -1.1956809 -1.6099267 -2.2635989 -2.8290794 -2.8805923][-0.41009712 -0.74293947 -0.83462381 -0.86276674 -0.88090777 -0.81337237 -0.71181154 -0.706651 -0.59331083 -0.65324378 -0.93194366 -1.3693264 -2.0317225 -2.6271377 -2.7196178][-0.0087294579 -0.22242689 -0.337533 -0.42164135 -0.45170283 -0.27869272 -0.011053085 0.11942482 0.16642904 -0.08911705 -0.58065176 -1.1381269 -1.8754745 -2.4957151 -2.6288147][-0.17998934 -0.41744041 -0.72752643 -1.0595479 -1.2741275 -1.1825163 -0.924989 -0.72194552 -0.49987125 -0.51104093 -0.84307051 -1.3243573 -2.0050011 -2.5840898 -2.7952724][-0.67593241 -0.99862766 -1.4103484 -1.788954 -2.0326629 -2.0424809 -1.9844041 -1.9428937 -1.7574582 -1.6633573 -1.7750368 -2.0139425 -2.4428935 -2.8782444 -3.1131182][-1.7174239 -2.0424774 -2.2570691 -2.3812048 -2.4403338 -2.3638718 -2.2773902 -2.1946642 -2.0726452 -2.0065496 -1.956022 -2.0313249 -2.313724 -2.7318506 -3.0658145][-2.5856705 -3.0526552 -3.2145491 -3.219377 -3.2540803 -3.2010319 -2.9885645 -2.6320548 -2.3623006 -2.1753008 -1.8214612 -1.6864758 -1.8884161 -2.3648181 -2.7658][-2.6348403 -3.1818223 -3.4720726 -3.5276034 -3.6550975 -3.6684947 -3.3698375 -2.8719771 -2.5802383 -2.3427258 -1.8191848 -1.5450704 -1.6648724 -2.0800517 -2.4033842][-2.2942939 -2.8773489 -3.4580996 -3.744031 -3.9678988 -4.0195265 -3.671937 -3.0815833 -2.7463694 -2.4903259 -1.9805841 -1.68906 -1.6988626 -1.9115844 -2.0934844][-1.9338801 -2.5889878 -3.3774667 -3.7923694 -4.0789342 -4.2485657 -4.0446858 -3.5263453 -3.1275458 -2.8145406 -2.3409944 -2.0568013 -1.9335325 -1.9330196 -1.9972196][-1.8144958 -2.5146203 -3.2981257 -3.664037 -3.9398057 -4.1657748 -4.1121097 -3.7254868 -3.3716192 -3.1309857 -2.7995639 -2.5898504 -2.3857956 -2.2514713 -2.2883728]]...]
INFO - root - 2017-12-07 04:50:50.725664: step 10310, loss = 0.76, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:05m:35s remains)
INFO - root - 2017-12-07 04:50:59.084109: step 10320, loss = 0.82, batch loss = 0.75 (9.3 examples/sec; 0.864 sec/batch; 77h:19m:00s remains)
INFO - root - 2017-12-07 04:51:07.344390: step 10330, loss = 0.72, batch loss = 0.65 (9.6 examples/sec; 0.835 sec/batch; 74h:42m:13s remains)
INFO - root - 2017-12-07 04:51:15.672587: step 10340, loss = 0.91, batch loss = 0.84 (9.7 examples/sec; 0.827 sec/batch; 73h:59m:47s remains)
INFO - root - 2017-12-07 04:51:24.003381: step 10350, loss = 0.90, batch loss = 0.83 (9.6 examples/sec; 0.830 sec/batch; 74h:18m:05s remains)
INFO - root - 2017-12-07 04:51:32.386585: step 10360, loss = 0.76, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:17m:37s remains)
INFO - root - 2017-12-07 04:51:40.749806: step 10370, loss = 0.61, batch loss = 0.54 (9.7 examples/sec; 0.829 sec/batch; 74h:09m:16s remains)
INFO - root - 2017-12-07 04:51:49.112986: step 10380, loss = 0.74, batch loss = 0.67 (9.1 examples/sec; 0.883 sec/batch; 78h:58m:52s remains)
INFO - root - 2017-12-07 04:51:57.389462: step 10390, loss = 0.88, batch loss = 0.81 (9.9 examples/sec; 0.804 sec/batch; 71h:57m:35s remains)
INFO - root - 2017-12-07 04:52:05.662219: step 10400, loss = 0.95, batch loss = 0.88 (9.6 examples/sec; 0.829 sec/batch; 74h:12m:51s remains)
2017-12-07 04:52:06.344992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9256396 -2.8901222 -2.861424 -2.8348255 -2.8371024 -2.9211936 -3.0449955 -3.1690068 -3.2739372 -3.316467 -3.2725918 -3.1739612 -3.0707655 -3.0062451 -2.9863863][-2.9090161 -2.8723118 -2.8542769 -2.8281951 -2.8416462 -2.9274712 -3.0524945 -3.1972775 -3.3151484 -3.3633981 -3.31153 -3.1906257 -3.0765588 -3.0097947 -2.9874322][-2.8908587 -2.8445439 -2.8285761 -2.7908816 -2.8072195 -2.886868 -3.0159535 -3.1995614 -3.3550909 -3.4210887 -3.3664234 -3.2245219 -3.095943 -3.018645 -2.9821172][-2.8685431 -2.8068748 -2.7816186 -2.7120616 -2.7010925 -2.7628117 -2.8978474 -3.1287 -3.3386202 -3.4466157 -3.4243979 -3.2983954 -3.1722074 -3.079932 -3.012495][-2.8522429 -2.7946486 -2.7812696 -2.6877537 -2.6406918 -2.6983585 -2.831677 -3.0670576 -3.2976828 -3.4421165 -3.477788 -3.4099035 -3.3074214 -3.1928177 -3.0821922][-2.8319435 -2.7843287 -2.7959237 -2.7006512 -2.6387246 -2.7062726 -2.8126059 -2.9953089 -3.2002215 -3.3564568 -3.462446 -3.490967 -3.441524 -3.3150516 -3.1593456][-2.8182364 -2.7644918 -2.7970476 -2.717783 -2.659163 -2.7166514 -2.7496173 -2.8448241 -3.0010653 -3.1663189 -3.3713107 -3.5433893 -3.5879354 -3.466501 -3.256216][-2.9196341 -2.8867373 -2.950779 -2.9165564 -2.8641338 -2.8638875 -2.777137 -2.7604058 -2.8565893 -3.0134029 -3.3121512 -3.6274445 -3.7681067 -3.6636 -3.3936663][-3.0834789 -3.1044161 -3.2109079 -3.2463155 -3.2231054 -3.1695051 -2.9745688 -2.8610404 -2.8937101 -3.0178537 -3.3674111 -3.7749946 -3.9753458 -3.884212 -3.5732977][-3.237226 -3.3186579 -3.4478924 -3.5339551 -3.5376959 -3.4604051 -3.2092509 -3.0447967 -3.0398331 -3.1283512 -3.4857373 -3.9232121 -4.1303697 -4.026391 -3.6931567][-3.3232732 -3.4308348 -3.5376532 -3.6242547 -3.6474221 -3.5894802 -3.3445539 -3.1623569 -3.1318326 -3.1872683 -3.5179324 -3.9349902 -4.1183968 -4.0007529 -3.6737971][-3.3210497 -3.4181533 -3.4907706 -3.5622497 -3.6092687 -3.6014819 -3.4179244 -3.2724209 -3.2443109 -3.263226 -3.5214059 -3.8647788 -4.0017338 -3.8716512 -3.5644734][-3.2952631 -3.3622208 -3.4016135 -3.4536383 -3.5079107 -3.5344737 -3.4223766 -3.3324409 -3.3313742 -3.3323126 -3.5009184 -3.7397327 -3.8193285 -3.6856704 -3.4172981][-3.2653823 -3.314518 -3.3341587 -3.3683386 -3.4117527 -3.4419627 -3.3869843 -3.3470829 -3.3621771 -3.3577695 -3.4507258 -3.5943704 -3.626384 -3.5019341 -3.2886419][-3.1953659 -3.2317357 -3.2432055 -3.2668021 -3.2965047 -3.3134909 -3.2916942 -3.28618 -3.3077297 -3.30343 -3.3446212 -3.4213717 -3.4265294 -3.3245473 -3.1702859]]...]
INFO - root - 2017-12-07 04:52:14.533167: step 10410, loss = 0.74, batch loss = 0.67 (9.6 examples/sec; 0.831 sec/batch; 74h:19m:14s remains)
INFO - root - 2017-12-07 04:52:22.884014: step 10420, loss = 0.87, batch loss = 0.80 (9.7 examples/sec; 0.821 sec/batch; 73h:24m:33s remains)
INFO - root - 2017-12-07 04:52:31.274415: step 10430, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.858 sec/batch; 76h:46m:24s remains)
INFO - root - 2017-12-07 04:52:39.517615: step 10440, loss = 0.83, batch loss = 0.75 (10.0 examples/sec; 0.802 sec/batch; 71h:43m:12s remains)
INFO - root - 2017-12-07 04:52:47.906169: step 10450, loss = 0.86, batch loss = 0.79 (9.4 examples/sec; 0.847 sec/batch; 75h:47m:15s remains)
INFO - root - 2017-12-07 04:52:56.213574: step 10460, loss = 0.64, batch loss = 0.57 (9.4 examples/sec; 0.850 sec/batch; 76h:03m:50s remains)
INFO - root - 2017-12-07 04:53:04.495751: step 10470, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.797 sec/batch; 71h:17m:53s remains)
INFO - root - 2017-12-07 04:53:12.883707: step 10480, loss = 0.69, batch loss = 0.61 (9.7 examples/sec; 0.822 sec/batch; 73h:31m:43s remains)
INFO - root - 2017-12-07 04:53:21.119609: step 10490, loss = 0.71, batch loss = 0.64 (9.7 examples/sec; 0.822 sec/batch; 73h:29m:53s remains)
INFO - root - 2017-12-07 04:53:29.514238: step 10500, loss = 0.72, batch loss = 0.65 (9.5 examples/sec; 0.839 sec/batch; 75h:02m:37s remains)
2017-12-07 04:53:30.225345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9650688 -3.0872068 -3.1743045 -3.1460438 -3.0033345 -2.7567222 -2.6702604 -2.7185369 -2.742754 -2.9851243 -3.3900294 -3.6043661 -3.4565654 -2.7509134 -1.6527839][-2.8394849 -3.0718188 -3.2840686 -3.2962611 -3.0881119 -2.8106148 -2.8183005 -2.9711666 -3.0373478 -3.2480228 -3.5537927 -3.5905466 -3.2573569 -2.4735882 -1.4498069][-2.74306 -3.1581693 -3.5481551 -3.6386173 -3.3652267 -3.056689 -3.0826714 -3.1711624 -3.1154723 -3.1844759 -3.3243051 -3.2319465 -2.8453054 -2.1636963 -1.383733][-2.7718549 -3.3283045 -3.8451071 -3.972461 -3.6224508 -3.271522 -3.2271161 -3.1163859 -2.852793 -2.7637467 -2.790987 -2.705514 -2.4232898 -1.9595673 -1.4873009][-2.6920922 -3.3687062 -3.98363 -4.1067915 -3.6682456 -3.2409801 -3.028481 -2.6470246 -2.206701 -2.0672879 -2.12394 -2.1770005 -2.1119392 -1.9119174 -1.7395153][-2.5580139 -3.2610657 -3.9206204 -4.0711007 -3.6327963 -3.1319127 -2.6526318 -1.9383059 -1.3735285 -1.3112414 -1.5428932 -1.8200409 -2.0116243 -2.0586343 -2.0972235][-2.4687185 -3.0036423 -3.5682323 -3.7517169 -3.4150586 -2.8696685 -2.1200805 -1.1389647 -0.57194567 -0.76482296 -1.299017 -1.8098743 -2.16979 -2.3403075 -2.4459763][-2.6531 -2.9069815 -3.2232056 -3.3347349 -3.0781424 -2.481818 -1.5459564 -0.46950698 -0.08517313 -0.68100405 -1.5190372 -2.1103063 -2.4369645 -2.5848637 -2.7006445][-2.9903972 -3.0646148 -3.1479793 -3.1272924 -2.8860874 -2.2530649 -1.3155925 -0.3809967 -0.26893806 -1.0955732 -1.9379103 -2.3436797 -2.507874 -2.645083 -2.8309817][-3.0011594 -2.9732461 -2.8993516 -2.8048503 -2.6217654 -2.1390481 -1.4726579 -0.90983462 -1.0443215 -1.8449035 -2.41002 -2.4499004 -2.3746116 -2.4740772 -2.7546477][-2.8183489 -2.6333964 -2.4492006 -2.3665833 -2.3051462 -2.0467246 -1.676986 -1.434572 -1.697859 -2.4103248 -2.7545364 -2.5457957 -2.3195364 -2.348352 -2.6375513][-2.7800596 -2.444159 -2.2008057 -2.1560769 -2.1774268 -2.0782855 -1.9106989 -1.8257434 -2.052537 -2.578548 -2.747623 -2.4753084 -2.3231535 -2.4455404 -2.787158][-2.8554959 -2.4884777 -2.2214713 -2.1635041 -2.2026184 -2.24327 -2.2746878 -2.2880754 -2.4032111 -2.6656983 -2.6153393 -2.3166857 -2.3067679 -2.5844212 -2.9992533][-2.8736236 -2.5334969 -2.241508 -2.1407886 -2.1713922 -2.34429 -2.5690863 -2.6932788 -2.7260098 -2.7893836 -2.6222131 -2.3509014 -2.3866684 -2.6351786 -2.9589567][-2.8279173 -2.5307369 -2.2628641 -2.1761949 -2.219656 -2.4278367 -2.7209082 -2.8977456 -2.897074 -2.8662291 -2.6867285 -2.4670398 -2.4841747 -2.6304293 -2.7829413]]...]
INFO - root - 2017-12-07 04:53:38.629251: step 10510, loss = 0.91, batch loss = 0.84 (10.1 examples/sec; 0.792 sec/batch; 70h:47m:56s remains)
INFO - root - 2017-12-07 04:53:46.982950: step 10520, loss = 0.61, batch loss = 0.53 (9.4 examples/sec; 0.853 sec/batch; 76h:19m:14s remains)
INFO - root - 2017-12-07 04:53:55.218846: step 10530, loss = 0.66, batch loss = 0.59 (10.1 examples/sec; 0.794 sec/batch; 71h:01m:39s remains)
INFO - root - 2017-12-07 04:54:03.586605: step 10540, loss = 0.79, batch loss = 0.71 (9.6 examples/sec; 0.835 sec/batch; 74h:43m:17s remains)
INFO - root - 2017-12-07 04:54:11.837109: step 10550, loss = 0.74, batch loss = 0.67 (9.3 examples/sec; 0.862 sec/batch; 77h:05m:21s remains)
INFO - root - 2017-12-07 04:54:20.203621: step 10560, loss = 0.79, batch loss = 0.72 (10.0 examples/sec; 0.799 sec/batch; 71h:27m:27s remains)
INFO - root - 2017-12-07 04:54:28.547541: step 10570, loss = 0.77, batch loss = 0.70 (9.7 examples/sec; 0.829 sec/batch; 74h:05m:33s remains)
INFO - root - 2017-12-07 04:54:36.849047: step 10580, loss = 0.76, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 78h:01m:12s remains)
INFO - root - 2017-12-07 04:54:45.200269: step 10590, loss = 0.69, batch loss = 0.61 (9.5 examples/sec; 0.840 sec/batch; 75h:05m:56s remains)
INFO - root - 2017-12-07 04:54:53.539329: step 10600, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.852 sec/batch; 76h:10m:05s remains)
2017-12-07 04:54:54.216322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8416185 -1.7785516 -0.6264801 -0.067953587 -0.51966524 -1.1636906 -1.4165289 -1.5541136 -1.6665065 -1.5081887 -1.0015788 -0.48253083 -0.47400808 -0.96680593 -1.4966829][-2.7449455 -1.7285647 -0.72887564 -0.41247749 -0.9221971 -1.3963401 -1.4883893 -1.5346344 -1.525141 -1.2812858 -0.82670879 -0.48457694 -0.6964376 -1.3109758 -1.8074977][-2.8481202 -1.9832473 -1.2615535 -1.1609721 -1.5392776 -1.6770978 -1.5447006 -1.4975955 -1.459131 -1.2805934 -1.0167267 -0.90972877 -1.263011 -1.8382068 -2.2241445][-3.0671036 -2.3924029 -1.9734616 -2.0100992 -2.1584597 -1.9777782 -1.7041554 -1.6427805 -1.6869485 -1.6982663 -1.6364095 -1.6122243 -1.8664403 -2.2044494 -2.4343524][-3.2401304 -2.7145538 -2.532423 -2.6465912 -2.6072378 -2.2132258 -1.8754113 -1.8607864 -2.0814788 -2.3861487 -2.4503899 -2.290225 -2.2330954 -2.2711995 -2.38407][-3.3455687 -2.8939838 -2.8194964 -2.9525642 -2.8076472 -2.3354819 -1.992579 -2.0200543 -2.4081304 -2.919898 -2.9519024 -2.5528984 -2.216718 -2.1030428 -2.2721877][-3.4121833 -2.9336827 -2.7838519 -2.8253157 -2.6553214 -2.2254446 -1.9566185 -2.0870864 -2.6185369 -3.1112828 -2.910569 -2.2885411 -1.86571 -1.8470085 -2.2513952][-3.306047 -2.6531775 -2.2882056 -2.2184498 -2.1083889 -1.7578142 -1.5525379 -1.8004041 -2.4307404 -2.7343109 -2.2560034 -1.6025381 -1.3638008 -1.6831746 -2.3979688][-2.9389467 -2.0380547 -1.4369774 -1.284672 -1.2612119 -0.96645308 -0.79774737 -1.1599531 -1.8175986 -1.9065819 -1.3225629 -0.90738487 -1.0582185 -1.7603083 -2.6463275][-2.5499048 -1.5017102 -0.8090415 -0.64223552 -0.7052083 -0.52218485 -0.4538908 -0.88189578 -1.4445162 -1.3513086 -0.81970334 -0.71040606 -1.1581047 -2.0150375 -2.8273785][-2.3950593 -1.3922005 -0.78391433 -0.68799567 -0.86973047 -0.92524242 -1.0421281 -1.4382195 -1.8140402 -1.6161418 -1.2095923 -1.2816076 -1.7545204 -2.4477043 -2.9672952][-2.4119816 -1.5443902 -1.0994847 -1.1222262 -1.4553235 -1.7770603 -2.0341382 -2.2770946 -2.3880434 -2.1536162 -1.9280834 -2.1170654 -2.5007153 -2.9261448 -3.0745838][-2.4531083 -1.6972818 -1.4029469 -1.5690536 -2.0220613 -2.4766488 -2.7281427 -2.7488213 -2.5848212 -2.3549578 -2.3569493 -2.6554556 -2.9786775 -3.2017298 -3.094708][-2.5122838 -1.8237126 -1.6311159 -1.9185784 -2.4104578 -2.8583117 -3.0547848 -2.9518046 -2.6659341 -2.4777365 -2.6300163 -2.9491482 -3.1752687 -3.2588029 -3.047492][-2.6195617 -1.985801 -1.8653736 -2.224674 -2.6854153 -3.0696321 -3.2542682 -3.148716 -2.8267431 -2.6386156 -2.8104529 -3.0914989 -3.2247119 -3.220891 -2.9978771]]...]
INFO - root - 2017-12-07 04:55:02.529091: step 10610, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.824 sec/batch; 73h:39m:38s remains)
INFO - root - 2017-12-07 04:55:10.796674: step 10620, loss = 0.67, batch loss = 0.60 (9.8 examples/sec; 0.814 sec/batch; 72h:47m:19s remains)
INFO - root - 2017-12-07 04:55:19.180728: step 10630, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 76h:06m:56s remains)
INFO - root - 2017-12-07 04:55:27.566058: step 10640, loss = 0.76, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:45m:22s remains)
INFO - root - 2017-12-07 04:55:35.885352: step 10650, loss = 0.84, batch loss = 0.77 (9.9 examples/sec; 0.811 sec/batch; 72h:29m:38s remains)
INFO - root - 2017-12-07 04:55:44.312792: step 10660, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.819 sec/batch; 73h:11m:39s remains)
INFO - root - 2017-12-07 04:55:52.647887: step 10670, loss = 0.60, batch loss = 0.53 (9.7 examples/sec; 0.821 sec/batch; 73h:22m:34s remains)
INFO - root - 2017-12-07 04:56:00.981465: step 10680, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.827 sec/batch; 73h:53m:38s remains)
INFO - root - 2017-12-07 04:56:09.448349: step 10690, loss = 0.64, batch loss = 0.57 (8.4 examples/sec; 0.949 sec/batch; 84h:51m:52s remains)
INFO - root - 2017-12-07 04:56:17.667916: step 10700, loss = 0.70, batch loss = 0.63 (9.6 examples/sec; 0.830 sec/batch; 74h:09m:03s remains)
2017-12-07 04:56:18.312313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2900274 -2.3181434 -2.2291291 -2.4605255 -3.1377592 -3.5320683 -3.2997897 -2.7993588 -2.5656002 -2.6957655 -2.8647916 -3.0722239 -3.4741819 -3.4642417 -3.1518879][-2.3793993 -2.4293776 -2.3778758 -2.7056854 -3.3071508 -3.4933381 -3.1026959 -2.4400907 -2.1388993 -2.4088228 -2.7311063 -3.0659456 -3.5196064 -3.6001 -3.2663579][-2.5044672 -2.5727317 -2.5873137 -3.0158606 -3.531517 -3.6141024 -3.2106156 -2.4996481 -2.1976736 -2.6211839 -2.9881141 -3.2949405 -3.74193 -3.9799433 -3.6168284][-2.6041489 -2.6295645 -2.5938828 -2.9196553 -3.1341839 -3.0253122 -2.6278751 -2.0211842 -1.8639421 -2.4831719 -2.8916149 -3.1833906 -3.6966677 -4.115489 -3.7354743][-2.7064147 -2.6979718 -2.5604696 -2.6502151 -2.4163253 -1.9402297 -1.3775716 -0.84829187 -0.87648106 -1.6945236 -2.286931 -2.749464 -3.400116 -3.8388855 -3.2410221][-2.7662187 -2.7780595 -2.6327507 -2.5900636 -1.996423 -1.1210573 -0.27398348 0.314157 0.23178196 -0.64148211 -1.4927156 -2.2585635 -2.9989507 -3.2938924 -2.4268646][-2.7929425 -2.8668766 -2.8044615 -2.8012123 -2.1412549 -1.1277194 -0.10856247 0.66530418 0.80201674 0.085414886 -0.93729854 -1.949543 -2.6562243 -2.8208425 -1.8496351][-2.8114667 -2.9255488 -2.8956947 -2.8998241 -2.3282676 -1.5069311 -0.62482786 0.30592918 0.83033276 0.47933292 -0.530617 -1.6616521 -2.3763864 -2.5254254 -1.6057179][-2.764992 -2.8733044 -2.7977083 -2.6941147 -2.1831615 -1.6238816 -0.899534 0.21069908 1.1274385 1.2076592 0.34829664 -0.89468789 -1.8349097 -2.2575481 -1.6333208][-2.671479 -2.7482452 -2.6165071 -2.4350867 -2.0763967 -1.8401012 -1.3104181 -0.12295246 1.0089765 1.4511104 0.97998571 -0.11234665 -1.1623509 -1.8268709 -1.4967875][-2.5497599 -2.5594172 -2.3466828 -2.0877013 -1.9093051 -2.0224397 -1.8524241 -0.96530223 0.033155441 0.70590591 0.89638662 0.39941406 -0.33239174 -0.96515942 -0.86366773][-2.397681 -2.2786641 -1.9103656 -1.5132151 -1.3957424 -1.6927528 -1.8487396 -1.4355145 -0.85054064 -0.2588973 0.37077618 0.51499319 0.32679367 0.020391464 0.083643436][-2.3347561 -2.1294961 -1.6233106 -1.0820642 -0.90002179 -1.1410637 -1.4116795 -1.3326547 -1.1316605 -0.783782 -0.15885878 0.16463852 0.20500278 0.12420845 0.29180288][-2.3279929 -2.1192799 -1.6086116 -1.0502634 -0.81447911 -0.93239784 -1.2026765 -1.33302 -1.4011621 -1.295758 -0.87493968 -0.65138197 -0.66247725 -0.73579264 -0.57542014][-2.3621531 -2.2398422 -1.8746023 -1.4577472 -1.2760456 -1.3382323 -1.5924842 -1.8394754 -2.0325413 -2.052197 -1.781033 -1.5953104 -1.5718381 -1.5979724 -1.4759188]]...]
INFO - root - 2017-12-07 04:56:26.679516: step 10710, loss = 0.78, batch loss = 0.71 (9.6 examples/sec; 0.831 sec/batch; 74h:17m:32s remains)
INFO - root - 2017-12-07 04:56:34.729126: step 10720, loss = 0.74, batch loss = 0.67 (9.6 examples/sec; 0.835 sec/batch; 74h:36m:57s remains)
INFO - root - 2017-12-07 04:56:43.140705: step 10730, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.804 sec/batch; 71h:49m:03s remains)
INFO - root - 2017-12-07 04:56:51.310784: step 10740, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 70h:49m:56s remains)
INFO - root - 2017-12-07 04:56:59.742744: step 10750, loss = 0.61, batch loss = 0.54 (9.5 examples/sec; 0.841 sec/batch; 75h:08m:21s remains)
INFO - root - 2017-12-07 04:57:08.095721: step 10760, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.828 sec/batch; 74h:01m:07s remains)
INFO - root - 2017-12-07 04:57:16.455537: step 10770, loss = 0.63, batch loss = 0.56 (8.9 examples/sec; 0.903 sec/batch; 80h:42m:50s remains)
INFO - root - 2017-12-07 04:57:24.775553: step 10780, loss = 0.69, batch loss = 0.61 (10.2 examples/sec; 0.787 sec/batch; 70h:17m:34s remains)
INFO - root - 2017-12-07 04:57:33.086108: step 10790, loss = 0.78, batch loss = 0.71 (9.8 examples/sec; 0.818 sec/batch; 73h:04m:08s remains)
INFO - root - 2017-12-07 04:57:41.507404: step 10800, loss = 0.65, batch loss = 0.58 (9.6 examples/sec; 0.832 sec/batch; 74h:19m:10s remains)
2017-12-07 04:57:42.178355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6044564 -2.7459178 -2.7810156 -2.6978335 -2.397099 -2.1133509 -2.240653 -2.7551155 -2.996788 -2.5788541 -1.8644564 -1.3051217 -1.155736 -1.2890716 -1.1472683][-2.3762326 -2.5316038 -2.5989623 -2.5586495 -2.3030291 -2.0316656 -2.1207118 -2.4803329 -2.5234756 -2.1052153 -1.4959769 -1.0089567 -0.92201352 -1.0903101 -0.91271996][-1.931138 -2.0822635 -2.1350093 -2.1174083 -1.8943617 -1.561686 -1.5497732 -1.7778983 -1.7444403 -1.4382601 -1.0144939 -0.615294 -0.5737431 -0.75558329 -0.60173345][-1.6895716 -1.9062164 -1.9506269 -1.9221632 -1.7234719 -1.3602936 -1.2845671 -1.4745128 -1.4825163 -1.3015325 -0.97323465 -0.54403591 -0.44709158 -0.63443232 -0.62969494][-1.621103 -1.9436269 -2.0177002 -1.9623923 -1.7447722 -1.3601282 -1.2390313 -1.3821967 -1.3793771 -1.2899103 -1.1029432 -0.70321894 -0.45808125 -0.47774386 -0.4987402][-1.7277248 -2.0517557 -2.117702 -2.1017864 -1.9835598 -1.7474914 -1.7397728 -1.8374181 -1.6959291 -1.5502305 -1.4687324 -1.1747351 -0.77604365 -0.46994209 -0.25664473][-1.8099878 -1.9526019 -1.8895178 -1.9140482 -1.9421542 -1.8906472 -1.9826086 -1.9567938 -1.5800238 -1.3035834 -1.3529181 -1.3050003 -0.97099781 -0.5105443 -0.07379818][-1.614572 -1.5243387 -1.1805925 -1.0074351 -0.96336055 -0.95750165 -1.1035089 -0.98985052 -0.46975517 -0.14197588 -0.39751577 -0.72519183 -0.73439932 -0.48931479 -0.14692307][-1.5302517 -1.3213739 -0.77372408 -0.31657934 -0.040445328 0.0021181107 -0.28896523 -0.33017063 0.085010529 0.282835 -0.14290142 -0.67799425 -0.92152214 -0.9179225 -0.753675][-1.4401121 -1.2486854 -0.82357931 -0.48116136 -0.25458717 -0.35308313 -0.86277413 -1.1082795 -0.80267644 -0.66943192 -1.0678065 -1.4683523 -1.6034613 -1.5908904 -1.4176681][-1.7228508 -1.6704078 -1.3990912 -1.1814907 -1.0056515 -1.1374884 -1.6435258 -1.9105706 -1.6379144 -1.5004773 -1.775125 -1.9066858 -1.8128288 -1.7132115 -1.4537208][-2.5090013 -2.5862541 -2.4066319 -2.153887 -1.7839246 -1.653868 -1.8686261 -1.9338176 -1.6619487 -1.6157606 -1.8492036 -1.7223535 -1.4129434 -1.2435071 -0.92461348][-2.7410994 -2.7720284 -2.5674253 -2.2671094 -1.8113108 -1.5822449 -1.6786954 -1.638088 -1.3804789 -1.4733171 -1.7400846 -1.5092654 -1.1513329 -1.0394566 -0.746541][-2.2144027 -2.2224233 -2.0513356 -1.7938502 -1.4804528 -1.4078338 -1.5997391 -1.5801604 -1.3359308 -1.4767568 -1.7179358 -1.4644961 -1.1529114 -1.1427133 -0.91393924][-2.3156722 -2.3467126 -2.2735355 -2.1005964 -1.9338641 -1.9108903 -2.0022814 -1.8244607 -1.4371848 -1.4790137 -1.6886914 -1.5011392 -1.2770011 -1.307179 -1.0384502]]...]
INFO - root - 2017-12-07 04:57:50.510290: step 10810, loss = 0.79, batch loss = 0.72 (9.5 examples/sec; 0.842 sec/batch; 75h:12m:44s remains)
INFO - root - 2017-12-07 04:57:58.803454: step 10820, loss = 0.82, batch loss = 0.74 (10.0 examples/sec; 0.802 sec/batch; 71h:40m:47s remains)
INFO - root - 2017-12-07 04:58:07.138240: step 10830, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.821 sec/batch; 73h:20m:25s remains)
INFO - root - 2017-12-07 04:58:15.407263: step 10840, loss = 0.67, batch loss = 0.60 (9.5 examples/sec; 0.841 sec/batch; 75h:08m:41s remains)
INFO - root - 2017-12-07 04:58:23.770183: step 10850, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.834 sec/batch; 74h:28m:17s remains)
INFO - root - 2017-12-07 04:58:32.056940: step 10860, loss = 0.64, batch loss = 0.57 (9.4 examples/sec; 0.852 sec/batch; 76h:07m:45s remains)
INFO - root - 2017-12-07 04:58:40.325229: step 10870, loss = 0.76, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 74h:18m:23s remains)
INFO - root - 2017-12-07 04:58:48.514168: step 10880, loss = 0.74, batch loss = 0.66 (10.2 examples/sec; 0.782 sec/batch; 69h:50m:41s remains)
INFO - root - 2017-12-07 04:58:56.870332: step 10890, loss = 0.92, batch loss = 0.85 (9.7 examples/sec; 0.821 sec/batch; 73h:19m:49s remains)
INFO - root - 2017-12-07 04:59:05.292481: step 10900, loss = 0.87, batch loss = 0.79 (9.1 examples/sec; 0.877 sec/batch; 78h:22m:40s remains)
2017-12-07 04:59:05.967787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6075311 -2.1517906 -2.0312927 -2.2658875 -2.4635539 -2.6443191 -3.0544372 -3.2773576 -3.3160839 -3.1600347 -2.6850414 -2.0887752 -1.396234 -1.1680715 -1.6628478][-2.2789085 -1.9091699 -1.8895736 -2.0982223 -2.082108 -2.1195285 -2.5560675 -2.7275629 -2.684516 -2.591419 -2.205056 -1.6973786 -1.1593678 -1.0622463 -1.6176586][-1.8548031 -1.6527731 -1.8314753 -2.0626812 -1.9261346 -1.9437468 -2.3566566 -2.3411281 -2.106005 -2.0377448 -1.8015993 -1.4545105 -1.1479518 -1.2664664 -1.8530805][-1.3659899 -1.3106792 -1.6671896 -1.9562809 -1.8714359 -2.0379481 -2.5085907 -2.3780956 -1.9489443 -1.7733216 -1.5493312 -1.2956038 -1.2149153 -1.5511851 -2.161361][-1.1254816 -1.2501907 -1.7315469 -2.0326707 -1.9852781 -2.1664524 -2.5469625 -2.2855673 -1.6669593 -1.3199852 -1.036267 -0.88483214 -1.0343654 -1.5675137 -2.2241516][-1.1797686 -1.4568858 -1.9520767 -2.2101541 -2.1461315 -2.1958466 -2.347007 -1.8542109 -0.96200085 -0.39823866 -0.14168215 -0.23312712 -0.66772485 -1.4154181 -2.1853337][-1.1999025 -1.4442692 -1.7734504 -1.9813566 -2.003845 -2.0030954 -1.9716616 -1.3149281 -0.22925186 0.47717857 0.58909893 0.20623636 -0.42539978 -1.2911317 -2.1157002][-1.3740079 -1.5732751 -1.6947145 -1.8353846 -1.9577956 -1.9612844 -1.8095033 -1.1081727 -0.0087895393 0.67347622 0.60097313 0.087981224 -0.49637747 -1.3068767 -2.0764911][-1.6535759 -1.9601934 -1.9578638 -2.0140636 -2.1634834 -2.161773 -1.9237511 -1.2902489 -0.38505507 0.10692072 -0.14974689 -0.63890529 -0.99201536 -1.5991554 -2.1841028][-1.9348996 -2.3909912 -2.3214297 -2.2692122 -2.362076 -2.3399818 -2.0915041 -1.6399553 -1.0231361 -0.82393456 -1.3075831 -1.7401893 -1.8142455 -2.1120396 -2.4291792][-2.1194794 -2.6671443 -2.5495315 -2.3832836 -2.3964839 -2.3564372 -2.1394091 -1.8609409 -1.4715686 -1.5130558 -2.1909547 -2.5908725 -2.4511547 -2.4483604 -2.519248][-2.2040515 -2.8655381 -2.7572055 -2.4877269 -2.4442582 -2.4203703 -2.2314019 -1.9526775 -1.5489168 -1.6469994 -2.3709662 -2.6793952 -2.3703427 -2.2018933 -2.2306371][-2.0575204 -2.8271291 -2.8079448 -2.5485387 -2.5504246 -2.5871165 -2.4178858 -2.0432122 -1.5026493 -1.5389872 -2.1757283 -2.309274 -1.8057435 -1.5549231 -1.7052197][-2.1993365 -3.0030971 -3.0364022 -2.8189654 -2.8473787 -2.8903847 -2.7162416 -2.2430902 -1.6043816 -1.5845647 -2.0900671 -2.0152624 -1.2962625 -0.98353744 -1.2934349][-2.725338 -3.4681382 -3.4851382 -3.3120828 -3.3316195 -3.3005648 -3.0767286 -2.557929 -1.950484 -1.894721 -2.2480075 -2.0186491 -1.1863611 -0.868711 -1.2783158]]...]
INFO - root - 2017-12-07 04:59:14.373059: step 10910, loss = 0.62, batch loss = 0.55 (9.9 examples/sec; 0.805 sec/batch; 71h:52m:07s remains)
INFO - root - 2017-12-07 04:59:22.679833: step 10920, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.828 sec/batch; 73h:57m:32s remains)
INFO - root - 2017-12-07 04:59:31.078242: step 10930, loss = 0.72, batch loss = 0.65 (9.5 examples/sec; 0.845 sec/batch; 75h:29m:58s remains)
INFO - root - 2017-12-07 04:59:39.330059: step 10940, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.820 sec/batch; 73h:13m:18s remains)
INFO - root - 2017-12-07 04:59:47.631733: step 10950, loss = 0.80, batch loss = 0.73 (9.6 examples/sec; 0.830 sec/batch; 74h:09m:41s remains)
INFO - root - 2017-12-07 04:59:55.927827: step 10960, loss = 0.85, batch loss = 0.78 (9.3 examples/sec; 0.865 sec/batch; 77h:13m:03s remains)
INFO - root - 2017-12-07 05:00:04.202751: step 10970, loss = 0.61, batch loss = 0.54 (9.9 examples/sec; 0.812 sec/batch; 72h:29m:09s remains)
INFO - root - 2017-12-07 05:00:12.543812: step 10980, loss = 0.65, batch loss = 0.58 (9.8 examples/sec; 0.819 sec/batch; 73h:10m:50s remains)
INFO - root - 2017-12-07 05:00:20.870222: step 10990, loss = 0.78, batch loss = 0.71 (9.5 examples/sec; 0.841 sec/batch; 75h:07m:10s remains)
INFO - root - 2017-12-07 05:00:29.062165: step 11000, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.783 sec/batch; 69h:57m:27s remains)
2017-12-07 05:00:29.725099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4628415 -3.5874746 -3.6321321 -3.62833 -3.6691771 -3.7025185 -3.6785834 -3.6319246 -3.635397 -3.7171586 -3.7694173 -3.7194366 -3.6428671 -3.6404738 -3.7593367][-3.3068151 -3.3494596 -3.2444687 -3.1247096 -3.1790247 -3.2632706 -3.2525475 -3.2431695 -3.3640933 -3.5591903 -3.5763891 -3.3948932 -3.1953802 -3.178091 -3.3881149][-3.0174575 -2.9548893 -2.683744 -2.4634843 -2.5572352 -2.7012191 -2.7018929 -2.7358313 -3.0613546 -3.4850235 -3.5278339 -3.2009041 -2.7783937 -2.614316 -2.7971401][-2.67566 -2.5325482 -2.1093206 -1.8103943 -1.9274631 -2.0438459 -1.9480193 -1.9591405 -2.4831491 -3.186718 -3.3821349 -3.0713749 -2.5133481 -2.1983249 -2.2667921][-2.3912582 -2.2015519 -1.6373422 -1.2529974 -1.3464127 -1.2783132 -0.85798955 -0.66686535 -1.2565598 -2.2075212 -2.7571168 -2.7895274 -2.4764004 -2.2130485 -2.187572][-2.1997991 -1.9834738 -1.3358738 -0.89209056 -0.925421 -0.566679 0.33826828 0.84517908 0.19892931 -1.0264635 -2.0353861 -2.5649137 -2.7397385 -2.7038393 -2.6428728][-1.956547 -1.6960227 -1.0528958 -0.6150434 -0.63401031 -0.12212133 1.0767517 1.7492256 0.97028685 -0.48936415 -1.779881 -2.6693945 -3.2574522 -3.4215808 -3.3237567][-1.7068207 -1.3688293 -0.79737282 -0.4260931 -0.50215793 -0.10653067 1.0004611 1.5460815 0.63662004 -0.79502487 -2.0634065 -3.0269237 -3.7460325 -3.931462 -3.7520797][-1.5939491 -1.206398 -0.73150992 -0.43733072 -0.59929013 -0.48239756 0.32317019 0.67723227 -0.17114973 -1.3846438 -2.447711 -3.2052054 -3.7733569 -3.8845158 -3.68569][-1.5816712 -1.1564612 -0.79876184 -0.63611627 -0.87763476 -0.97129631 -0.39435577 -0.14207983 -0.83910322 -1.8334017 -2.6186152 -3.0062327 -3.258461 -3.2876396 -3.1635149][-1.5677283 -1.1135681 -0.86918187 -0.87287521 -1.1780677 -1.3804371 -1.0022116 -0.87080932 -1.4675994 -2.2377958 -2.673003 -2.6400931 -2.5697827 -2.5349703 -2.488626][-1.4741361 -1.0099289 -0.86548448 -1.0573037 -1.4898937 -1.8743224 -1.8206999 -1.8844266 -2.3996098 -2.8698654 -2.9476433 -2.6082523 -2.3226349 -2.2142684 -2.1884665][-1.6672318 -1.2627199 -1.2003219 -1.5076754 -2.042347 -2.5602136 -2.770932 -3.0046461 -3.4375749 -3.6793532 -3.5525708 -3.094244 -2.7251585 -2.5389938 -2.4703946][-2.3256559 -2.0902407 -2.0999134 -2.4083335 -2.8808751 -3.3130255 -3.5369954 -3.7515259 -4.0279303 -4.1261139 -3.9728143 -3.6049898 -3.286123 -3.0630329 -2.9347987][-3.0369864 -2.9507113 -2.9965539 -3.2327185 -3.5605154 -3.8287296 -3.9916172 -4.1405563 -4.2698121 -4.2586632 -4.1218767 -3.8947158 -3.660603 -3.4344811 -3.2622077]]...]
INFO - root - 2017-12-07 05:00:38.083584: step 11010, loss = 0.70, batch loss = 0.63 (9.5 examples/sec; 0.846 sec/batch; 75h:31m:03s remains)
INFO - root - 2017-12-07 05:00:46.333359: step 11020, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.795 sec/batch; 70h:58m:20s remains)
INFO - root - 2017-12-07 05:00:54.640261: step 11030, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.800 sec/batch; 71h:25m:59s remains)
INFO - root - 2017-12-07 05:01:02.689378: step 11040, loss = 0.72, batch loss = 0.65 (9.2 examples/sec; 0.865 sec/batch; 77h:14m:42s remains)
INFO - root - 2017-12-07 05:01:10.940107: step 11050, loss = 0.66, batch loss = 0.59 (10.0 examples/sec; 0.803 sec/batch; 71h:42m:23s remains)
INFO - root - 2017-12-07 05:01:19.213152: step 11060, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.804 sec/batch; 71h:46m:57s remains)
INFO - root - 2017-12-07 05:01:27.674279: step 11070, loss = 0.68, batch loss = 0.61 (10.0 examples/sec; 0.798 sec/batch; 71h:14m:09s remains)
INFO - root - 2017-12-07 05:01:36.027847: step 11080, loss = 1.04, batch loss = 0.96 (9.9 examples/sec; 0.808 sec/batch; 72h:07m:37s remains)
INFO - root - 2017-12-07 05:01:44.391875: step 11090, loss = 0.90, batch loss = 0.83 (9.5 examples/sec; 0.838 sec/batch; 74h:48m:35s remains)
INFO - root - 2017-12-07 05:01:52.695536: step 11100, loss = 0.81, batch loss = 0.73 (9.2 examples/sec; 0.870 sec/batch; 77h:41m:38s remains)
2017-12-07 05:01:53.413893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7808502 -2.9839253 -3.0852845 -3.1292148 -3.1737549 -3.2618394 -3.4204769 -3.50215 -3.5210135 -3.5213218 -3.483398 -3.3884442 -3.2471 -3.0294409 -2.7834697][-2.2833796 -2.5450702 -2.5578496 -2.5047991 -2.5681677 -2.7191658 -2.8821552 -2.8742723 -2.7995939 -2.8092771 -2.7840905 -2.656601 -2.4946766 -2.2821329 -2.0565579][-2.0168488 -2.2261529 -2.1326771 -2.0232756 -2.1663926 -2.4749918 -2.6909821 -2.6660724 -2.6082702 -2.566047 -2.3906736 -2.1209838 -1.9426174 -1.8787754 -1.8413758][-1.8580005 -1.9054234 -1.7432942 -1.5652893 -1.6877985 -2.1107712 -2.5019145 -2.7144332 -2.8573558 -2.7269049 -2.2501616 -1.7325912 -1.4885466 -1.6125021 -1.805352][-1.6105099 -1.5059206 -1.3606091 -1.103976 -1.065403 -1.4242835 -1.8469176 -2.2760749 -2.7561321 -2.7560205 -2.2071264 -1.602586 -1.248595 -1.3462644 -1.6466346][-1.3962262 -1.2529886 -1.170388 -0.85598493 -0.63044524 -0.69086933 -0.73493028 -0.99211907 -1.636651 -1.9659133 -1.6314816 -1.1785662 -0.87207985 -0.93260264 -1.3240187][-1.327014 -1.2408452 -1.217912 -0.92991519 -0.58519268 -0.19476271 0.35645008 0.44868517 -0.22354031 -0.83763885 -0.728127 -0.50049329 -0.44785762 -0.62547827 -1.1231995][-1.4407108 -1.4066644 -1.4201031 -1.2122638 -0.82420969 -0.17330313 0.63790321 0.86808777 0.33859491 -0.20157433 -0.12891722 -0.10395527 -0.3995347 -0.709018 -1.2038248][-1.6253374 -1.6057751 -1.6481969 -1.5044925 -1.1455929 -0.59176135 -0.021072865 0.15463066 -0.039988995 -0.28597069 -0.1154213 -0.0995636 -0.48448539 -0.72624969 -1.0820727][-1.7863517 -1.715147 -1.7551057 -1.6667607 -1.4551826 -1.1692004 -0.8718791 -0.69800758 -0.65784431 -0.79117942 -0.6014657 -0.48496127 -0.68785691 -0.71581221 -0.84111738][-2.0901778 -1.9257772 -1.9286375 -1.9088309 -1.8947928 -1.8526382 -1.6912389 -1.4716976 -1.3869109 -1.5538402 -1.413377 -1.2882395 -1.4122303 -1.3411067 -1.2135594][-2.5318584 -2.3093066 -2.2781076 -2.3325887 -2.4403348 -2.4881451 -2.3751667 -2.1664691 -2.1063251 -2.1925004 -2.0525231 -2.097286 -2.3965633 -2.4239664 -2.1817369][-3.1813364 -2.99184 -2.9110818 -2.952189 -3.0605965 -3.098376 -2.9910462 -2.8167682 -2.7354469 -2.6637807 -2.5322366 -2.7603745 -3.2207675 -3.3784933 -3.2080212][-3.74281 -3.6309905 -3.5129089 -3.5008276 -3.553319 -3.5349429 -3.4387879 -3.3096747 -3.2101789 -3.053813 -2.9394445 -3.1605511 -3.5182612 -3.6719027 -3.6327081][-3.8825712 -3.8593571 -3.7795703 -3.7457058 -3.760227 -3.7282314 -3.664176 -3.5801105 -3.4999833 -3.3556578 -3.2350762 -3.2829845 -3.3963308 -3.4827008 -3.5416105]]...]
INFO - root - 2017-12-07 05:02:01.731919: step 11110, loss = 1.03, batch loss = 0.96 (9.5 examples/sec; 0.840 sec/batch; 74h:57m:28s remains)
INFO - root - 2017-12-07 05:02:10.017156: step 11120, loss = 0.59, batch loss = 0.52 (9.6 examples/sec; 0.835 sec/batch; 74h:34m:12s remains)
INFO - root - 2017-12-07 05:02:18.365837: step 11130, loss = 0.81, batch loss = 0.74 (9.5 examples/sec; 0.840 sec/batch; 74h:58m:23s remains)
INFO - root - 2017-12-07 05:02:26.763607: step 11140, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 0.895 sec/batch; 79h:54m:34s remains)
INFO - root - 2017-12-07 05:02:35.020291: step 11150, loss = 0.85, batch loss = 0.78 (9.7 examples/sec; 0.827 sec/batch; 73h:49m:26s remains)
INFO - root - 2017-12-07 05:02:43.293295: step 11160, loss = 0.95, batch loss = 0.88 (9.5 examples/sec; 0.840 sec/batch; 75h:00m:25s remains)
INFO - root - 2017-12-07 05:02:51.603541: step 11170, loss = 0.71, batch loss = 0.63 (9.5 examples/sec; 0.839 sec/batch; 74h:52m:22s remains)
INFO - root - 2017-12-07 05:02:59.885763: step 11180, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.789 sec/batch; 70h:26m:41s remains)
INFO - root - 2017-12-07 05:03:08.314504: step 11190, loss = 0.66, batch loss = 0.59 (9.3 examples/sec; 0.858 sec/batch; 76h:35m:35s remains)
INFO - root - 2017-12-07 05:03:16.533805: step 11200, loss = 0.71, batch loss = 0.64 (8.9 examples/sec; 0.896 sec/batch; 79h:56m:01s remains)
2017-12-07 05:03:17.210664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2256398 -1.7652791 -1.425703 -1.357574 -1.5936952 -2.0249007 -2.4915891 -2.8645926 -3.0473721 -3.0175319 -2.8088927 -2.4739709 -2.1062024 -1.8097508 -1.7993934][-1.7553039 -1.4564211 -1.4020503 -1.6393003 -2.0783889 -2.5528672 -2.9292428 -3.1357429 -3.1379986 -2.9474416 -2.6302309 -2.2707951 -1.9626875 -1.7909579 -1.9300282][-1.5411897 -1.5295174 -1.7264807 -2.0565436 -2.4149985 -2.695694 -2.8452892 -2.8624654 -2.7457888 -2.5128443 -2.2358782 -2.0029094 -1.8837636 -1.9179242 -2.1904616][-1.5878391 -1.8098018 -2.0889826 -2.3471651 -2.5388296 -2.6173594 -2.5713987 -2.4438145 -2.27545 -2.0756061 -1.9167805 -1.8722119 -1.9724212 -2.1833241 -2.5031219][-1.6973941 -1.9957695 -2.2359495 -2.4013841 -2.4867253 -2.4605141 -2.3197083 -2.154098 -2.026696 -1.9427433 -1.9554958 -2.0986595 -2.3196881 -2.5573163 -2.8000836][-1.8439834 -2.0893862 -2.2249422 -2.2714491 -2.2546003 -2.1643991 -2.0007238 -1.8655598 -1.800657 -1.8100545 -1.9502649 -2.213239 -2.4935436 -2.715349 -2.888576][-2.1370595 -2.2074606 -2.1610157 -2.0299935 -1.8635957 -1.6783204 -1.4854763 -1.4185395 -1.4859178 -1.6279082 -1.8644154 -2.1760983 -2.4619112 -2.6546955 -2.793354][-2.4172039 -2.2375863 -1.9660456 -1.6663587 -1.3919201 -1.1347835 -0.91628981 -0.93768239 -1.1811764 -1.4915783 -1.8494554 -2.1947484 -2.4581525 -2.5904922 -2.6834674][-2.4650922 -2.1304274 -1.7184651 -1.3455195 -1.1091671 -1.0293932 -1.07025 -1.2997735 -1.6489046 -1.9673951 -2.2596159 -2.48832 -2.6301384 -2.6607199 -2.7095122][-2.5187581 -2.2552354 -1.93926 -1.6802056 -1.5964642 -1.7366242 -2.0067172 -2.3567176 -2.6841683 -2.8785224 -2.993479 -3.0221643 -2.9862556 -2.8926587 -2.8762393][-2.725018 -2.6782875 -2.6123731 -2.5813911 -2.6560295 -2.8473434 -3.0715995 -3.2661219 -3.3855705 -3.4056423 -3.3808799 -3.3011909 -3.1832747 -3.0394683 -3.0026622][-3.0264153 -3.0723877 -3.1099782 -3.1444635 -3.2279825 -3.3621261 -3.4872351 -3.5659068 -3.5813475 -3.5343585 -3.4631956 -3.3570161 -3.2239628 -3.0734429 -3.0513613][-3.2451971 -3.2394822 -3.2303848 -3.198555 -3.1856208 -3.2031596 -3.2407689 -3.2946715 -3.3435397 -3.3561723 -3.336019 -3.2696788 -3.1768632 -3.0708404 -3.1013298][-3.2875991 -3.2415719 -3.2081463 -3.1685658 -3.1422234 -3.1326113 -3.1349964 -3.1576266 -3.1866517 -3.198437 -3.1885462 -3.1507661 -3.1182384 -3.0920315 -3.2006259][-3.2685297 -3.2134118 -3.1802063 -3.1514759 -3.1414876 -3.1502404 -3.1660612 -3.1869073 -3.1992428 -3.196692 -3.1811059 -3.1556935 -3.1562533 -3.1772189 -3.3238094]]...]
INFO - root - 2017-12-07 05:03:25.644323: step 11210, loss = 0.73, batch loss = 0.65 (9.5 examples/sec; 0.846 sec/batch; 75h:28m:55s remains)
INFO - root - 2017-12-07 05:03:34.039870: step 11220, loss = 0.96, batch loss = 0.89 (9.2 examples/sec; 0.873 sec/batch; 77h:52m:21s remains)
INFO - root - 2017-12-07 05:03:42.563358: step 11230, loss = 0.74, batch loss = 0.67 (8.7 examples/sec; 0.924 sec/batch; 82h:28m:32s remains)
INFO - root - 2017-12-07 05:03:51.003029: step 11240, loss = 0.64, batch loss = 0.57 (10.0 examples/sec; 0.800 sec/batch; 71h:24m:38s remains)
INFO - root - 2017-12-07 05:03:59.448949: step 11250, loss = 0.72, batch loss = 0.64 (9.4 examples/sec; 0.850 sec/batch; 75h:52m:10s remains)
INFO - root - 2017-12-07 05:04:07.738622: step 11260, loss = 0.88, batch loss = 0.81 (9.8 examples/sec; 0.817 sec/batch; 72h:52m:40s remains)
INFO - root - 2017-12-07 05:04:15.952208: step 11270, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.821 sec/batch; 73h:13m:57s remains)
INFO - root - 2017-12-07 05:04:24.256022: step 11280, loss = 0.70, batch loss = 0.63 (9.5 examples/sec; 0.838 sec/batch; 74h:44m:48s remains)
INFO - root - 2017-12-07 05:04:32.688004: step 11290, loss = 0.81, batch loss = 0.73 (9.9 examples/sec; 0.808 sec/batch; 72h:08m:06s remains)
INFO - root - 2017-12-07 05:04:41.103113: step 11300, loss = 0.61, batch loss = 0.54 (9.9 examples/sec; 0.804 sec/batch; 71h:45m:46s remains)
2017-12-07 05:04:41.701212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.293741 -3.3329005 -3.4295278 -3.629427 -3.6016974 -3.27202 -2.8286848 -2.5910912 -2.7034357 -2.975368 -3.2802441 -3.6394815 -3.932343 -3.8309209 -3.4109979][-3.1278009 -3.1628284 -3.2853189 -3.5531604 -3.5756168 -3.2826612 -2.8975515 -2.6765714 -2.6729832 -2.7934465 -3.0733528 -3.4860034 -3.7730141 -3.6007934 -3.0788274][-3.0106096 -3.0591669 -3.145854 -3.3263192 -3.263938 -2.9845505 -2.6847944 -2.5102742 -2.4611688 -2.4849205 -2.7512956 -3.1349287 -3.326838 -3.1179056 -2.6646013][-2.6532993 -2.6636858 -2.6174469 -2.6534548 -2.5740194 -2.4295874 -2.3144262 -2.2702835 -2.3302214 -2.4463587 -2.7711825 -3.1025696 -3.2073569 -3.0557225 -2.7656693][-1.9762738 -1.9507215 -1.9636009 -2.1123738 -2.2473235 -2.3319213 -2.3283441 -2.2765703 -2.3200657 -2.4141979 -2.6073842 -2.7737694 -2.8728848 -2.9089308 -2.8142409][-1.3967178 -1.438009 -1.7030292 -2.1567655 -2.5409629 -2.7119417 -2.6024477 -2.3149617 -2.1167114 -1.9677184 -1.8200226 -1.7257016 -1.8938563 -2.2175088 -2.3243365][-1.4136045 -1.5901875 -1.9959795 -2.4973073 -2.7906833 -2.7624493 -2.4126894 -1.9116809 -1.5001206 -1.1403937 -0.7703886 -0.56103015 -0.84168768 -1.3373559 -1.5384245][-1.8225913 -2.0760555 -2.410522 -2.705833 -2.7402284 -2.4717188 -2.0284002 -1.5829687 -1.1619709 -0.73087955 -0.38221121 -0.30749035 -0.71370912 -1.1685169 -1.23751][-2.4218397 -2.6276765 -2.8598709 -2.9648497 -2.8103514 -2.4320107 -2.0910594 -1.8967576 -1.6257539 -1.268189 -1.1570456 -1.4250922 -1.9597325 -2.1994104 -1.9626281][-2.8604374 -3.0238705 -3.2265635 -3.2459555 -3.0109978 -2.6885915 -2.5741224 -2.6593013 -2.5217786 -2.2155886 -2.2441227 -2.6717319 -3.1809371 -3.2096596 -2.7786694][-3.0719457 -3.2462773 -3.4323173 -3.3943317 -3.1791356 -3.0300922 -3.1512423 -3.3827963 -3.2594032 -2.927393 -2.8994112 -3.2314215 -3.5672264 -3.4176993 -2.8856091][-3.206203 -3.2757754 -3.3202267 -3.1870322 -3.0726786 -3.1874914 -3.4950469 -3.7061257 -3.5099611 -3.1659641 -3.0827115 -3.2440143 -3.3697376 -3.153425 -2.6536894][-2.8527443 -2.8313763 -2.7566614 -2.6440737 -2.766746 -3.1595612 -3.5292206 -3.5735819 -3.2758553 -3.0163105 -3.0097485 -3.1323609 -3.1936879 -3.0218878 -2.622664][-2.6187749 -2.6492743 -2.49125 -2.4210424 -2.7309127 -3.2126269 -3.425607 -3.234818 -2.877512 -2.7540646 -2.9002805 -3.0738735 -3.1812801 -3.1032619 -2.8138845][-2.9784842 -3.0376997 -2.7740502 -2.6685047 -2.9748259 -3.3138871 -3.2913122 -2.9832897 -2.68791 -2.6844921 -2.8930562 -3.0588222 -3.1840396 -3.1535721 -2.9195571]]...]
INFO - root - 2017-12-07 05:04:50.013549: step 11310, loss = 0.55, batch loss = 0.47 (9.5 examples/sec; 0.839 sec/batch; 74h:50m:15s remains)
INFO - root - 2017-12-07 05:04:58.273929: step 11320, loss = 0.75, batch loss = 0.67 (9.5 examples/sec; 0.847 sec/batch; 75h:31m:28s remains)
INFO - root - 2017-12-07 05:05:06.581645: step 11330, loss = 0.55, batch loss = 0.48 (9.8 examples/sec; 0.815 sec/batch; 72h:43m:39s remains)
INFO - root - 2017-12-07 05:05:14.911108: step 11340, loss = 0.63, batch loss = 0.56 (10.1 examples/sec; 0.793 sec/batch; 70h:45m:33s remains)
INFO - root - 2017-12-07 05:05:23.000995: step 11350, loss = 0.90, batch loss = 0.83 (9.4 examples/sec; 0.848 sec/batch; 75h:38m:24s remains)
INFO - root - 2017-12-07 05:05:31.211302: step 11360, loss = 0.73, batch loss = 0.66 (9.8 examples/sec; 0.817 sec/batch; 72h:50m:15s remains)
INFO - root - 2017-12-07 05:05:39.557934: step 11370, loss = 0.97, batch loss = 0.89 (9.5 examples/sec; 0.844 sec/batch; 75h:19m:42s remains)
INFO - root - 2017-12-07 05:05:47.863896: step 11380, loss = 0.88, batch loss = 0.80 (10.1 examples/sec; 0.790 sec/batch; 70h:27m:07s remains)
INFO - root - 2017-12-07 05:05:56.125815: step 11390, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.790 sec/batch; 70h:29m:46s remains)
INFO - root - 2017-12-07 05:06:04.465552: step 11400, loss = 0.89, batch loss = 0.82 (9.9 examples/sec; 0.809 sec/batch; 72h:11m:44s remains)
2017-12-07 05:06:05.131993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6739278 -1.604816 -1.5837913 -1.5453937 -1.4580154 -1.4584279 -1.4991086 -1.4566669 -1.3762708 -1.2930169 -1.2689691 -1.3820035 -1.5190229 -1.5567815 -1.5565739][-1.6207175 -1.5490551 -1.4879639 -1.3754344 -1.148618 -0.99095011 -0.8597908 -0.65139937 -0.552984 -0.60828829 -0.82097578 -1.1949005 -1.4842384 -1.4620619 -1.2962317][-1.8067722 -1.77618 -1.7663038 -1.6558216 -1.3376443 -1.0223293 -0.68124151 -0.29186916 -0.22263908 -0.52804017 -1.0753396 -1.6864324 -2.0080037 -1.7900198 -1.3413236][-1.6696746 -1.6503017 -1.7994933 -1.8275223 -1.6017385 -1.2838557 -0.83367443 -0.33987522 -0.26313353 -0.74930978 -1.5396471 -2.2472212 -2.4456315 -1.9696269 -1.2725644][-1.2208571 -1.1655691 -1.4619174 -1.6290092 -1.5388105 -1.2458191 -0.69158363 -0.12530375 -0.032999992 -0.64238119 -1.5701673 -2.2629485 -2.2746274 -1.6121631 -0.82531667][-0.83571982 -0.66620326 -0.96762514 -1.1036813 -0.93013096 -0.43005466 0.4087491 1.1015816 1.1030664 0.20427656 -1.0205688 -1.8005767 -1.7361429 -1.0455227 -0.3049202][-0.34752989 -0.14541912 -0.51068878 -0.67157364 -0.43213844 0.29413557 1.4041963 2.2572565 2.1307354 0.90785265 -0.61825657 -1.4922204 -1.4136007 -0.77015233 -0.12679672][-0.52826285 -0.561424 -1.1518862 -1.4703367 -1.3430746 -0.73863292 0.18817854 0.93442297 0.88604927 -0.14328527 -1.3937976 -1.9842589 -1.716774 -1.0274725 -0.40606833][-1.8136098 -2.1372058 -2.8644938 -3.2028222 -3.1070406 -2.6946759 -2.1915793 -1.7892649 -1.6771472 -2.1854036 -2.8527536 -2.9499459 -2.3803325 -1.5523014 -0.87558866][-2.945256 -3.2877445 -3.8755388 -4.1132421 -3.9894373 -3.6753244 -3.4335074 -3.3294778 -3.1985512 -3.3968792 -3.7071226 -3.482873 -2.7624836 -1.9361353 -1.2994902][-3.5050735 -3.6315129 -3.9574568 -4.1451197 -4.0906239 -3.8564765 -3.6705322 -3.6472621 -3.51756 -3.6079211 -3.8322997 -3.5447273 -2.8678055 -2.1768391 -1.6614847][-3.7864666 -3.645498 -3.726084 -3.9044452 -3.9656291 -3.8133821 -3.6272371 -3.5570843 -3.4299526 -3.511807 -3.7368169 -3.5163488 -2.9530814 -2.3770232 -1.9509909][-4.0669765 -3.7006638 -3.5672896 -3.7209187 -3.8876343 -3.8609972 -3.7741587 -3.6976891 -3.5776813 -3.6486928 -3.8195367 -3.651217 -3.1809387 -2.6248407 -2.1980925][-4.623713 -4.2652087 -4.0725656 -4.192842 -4.3808589 -4.3849125 -4.317606 -4.1589847 -3.9844263 -4.011632 -4.057723 -3.8753891 -3.444617 -2.8823919 -2.4276557][-4.7970238 -4.5339561 -4.3487411 -4.4029088 -4.5165577 -4.5084853 -4.4455981 -4.2499642 -4.0591006 -4.0222225 -3.9493117 -3.7512028 -3.3854122 -2.9119749 -2.5143037]]...]
INFO - root - 2017-12-07 05:06:13.464488: step 11410, loss = 0.92, batch loss = 0.84 (10.0 examples/sec; 0.799 sec/batch; 71h:16m:14s remains)
INFO - root - 2017-12-07 05:06:21.795421: step 11420, loss = 0.75, batch loss = 0.67 (10.2 examples/sec; 0.785 sec/batch; 70h:00m:53s remains)
INFO - root - 2017-12-07 05:06:30.134101: step 11430, loss = 0.79, batch loss = 0.71 (9.7 examples/sec; 0.826 sec/batch; 73h:37m:50s remains)
INFO - root - 2017-12-07 05:06:38.444787: step 11440, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.822 sec/batch; 73h:18m:04s remains)
INFO - root - 2017-12-07 05:06:46.924174: step 11450, loss = 0.57, batch loss = 0.50 (8.7 examples/sec; 0.920 sec/batch; 82h:04m:00s remains)
INFO - root - 2017-12-07 05:06:55.319849: step 11460, loss = 0.62, batch loss = 0.54 (9.5 examples/sec; 0.839 sec/batch; 74h:50m:38s remains)
INFO - root - 2017-12-07 05:07:03.577175: step 11470, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.813 sec/batch; 72h:27m:42s remains)
INFO - root - 2017-12-07 05:07:11.931291: step 11480, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 75h:31m:34s remains)
INFO - root - 2017-12-07 05:07:20.216367: step 11490, loss = 0.92, batch loss = 0.85 (9.9 examples/sec; 0.805 sec/batch; 71h:47m:35s remains)
INFO - root - 2017-12-07 05:07:28.562482: step 11500, loss = 1.07, batch loss = 1.00 (9.7 examples/sec; 0.827 sec/batch; 73h:44m:26s remains)
2017-12-07 05:07:29.243287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7849655 -2.7350531 -2.6446295 -2.6452131 -2.7895222 -3.07143 -3.3679144 -3.5330822 -3.5257242 -3.3508458 -3.0578125 -2.7615042 -2.6123395 -2.4894633 -2.3321035][-2.5297904 -2.4186199 -2.3237414 -2.4266591 -2.7390525 -3.1338072 -3.4210258 -3.5087926 -3.4996605 -3.4080682 -3.1384957 -2.7618668 -2.5928042 -2.4939151 -2.2824304][-2.1159587 -2.0834894 -2.1496282 -2.4370186 -2.8551297 -3.1826811 -3.269347 -3.1465378 -3.1498663 -3.3037639 -3.2336025 -2.9028378 -2.7623825 -2.7066119 -2.4535556][-1.9848695 -2.2492998 -2.5921984 -2.9871304 -3.2651141 -3.2672968 -3.0031896 -2.6471846 -2.7311804 -3.2120955 -3.3902171 -3.1485062 -3.0229874 -3.0109482 -2.7707698][-2.2887869 -2.7117009 -3.1049628 -3.4152327 -3.4496355 -3.1542895 -2.6506572 -2.2360325 -2.5248487 -3.2757897 -3.6089003 -3.423445 -3.2731643 -3.253159 -3.0337081][-2.709002 -2.9114168 -3.0653462 -3.2488935 -3.1922457 -2.7587101 -2.1087956 -1.7409163 -2.3247943 -3.3140221 -3.7384651 -3.5721142 -3.3490233 -3.2598629 -3.0618973][-2.6628237 -2.4454594 -2.3361437 -2.5108798 -2.4842863 -1.8895032 -0.97763062 -0.69050741 -1.7133293 -3.0375011 -3.5855296 -3.456306 -3.1700971 -2.965585 -2.7502739][-2.0462403 -1.5761867 -1.358798 -1.5572281 -1.5144277 -0.72038651 0.49794626 0.71820164 -0.71724176 -2.3228786 -2.9496989 -2.8363698 -2.5486221 -2.2747815 -2.0473394][-1.2808342 -0.88780737 -0.70493937 -0.85933995 -0.7635231 -0.019967079 1.1487589 1.3558974 0.019761562 -1.3551056 -1.8573797 -1.8321111 -1.7530582 -1.7066145 -1.66064][-1.2183444 -1.0043745 -0.87003064 -0.92261457 -0.85832953 -0.52477479 0.14539242 0.32952023 -0.3947978 -1.071672 -1.2788262 -1.3706727 -1.6258068 -1.9422219 -2.0891466][-1.8930087 -1.6004937 -1.3413775 -1.2777653 -1.3244693 -1.3831825 -1.0945592 -0.897377 -1.1170061 -1.2503464 -1.2732837 -1.4651949 -1.9302092 -2.4239416 -2.6068079][-2.4623361 -1.953486 -1.5334349 -1.4418256 -1.6193221 -1.7935603 -1.5631716 -1.3227761 -1.2901838 -1.2260492 -1.2629528 -1.5625255 -2.089936 -2.5872259 -2.7585788][-2.6373434 -2.0976925 -1.702791 -1.6897602 -1.9402547 -2.0700941 -1.8005836 -1.5561647 -1.4676316 -1.402678 -1.4810426 -1.7680445 -2.1729145 -2.5150423 -2.619019][-2.6022568 -2.2568319 -2.0128095 -2.043946 -2.2536607 -2.3297579 -2.1431015 -1.9869397 -1.8877432 -1.7827637 -1.7891288 -1.9384549 -2.1510262 -2.3075833 -2.3272202][-2.3597658 -2.2552257 -2.1798012 -2.2292979 -2.3737381 -2.4475145 -2.3805139 -2.2965329 -2.1915302 -2.0562484 -1.9870551 -2.0100343 -2.0866323 -2.1479664 -2.1442947]]...]
INFO - root - 2017-12-07 05:07:37.645848: step 11510, loss = 0.79, batch loss = 0.71 (9.4 examples/sec; 0.848 sec/batch; 75h:38m:20s remains)
INFO - root - 2017-12-07 05:07:45.985320: step 11520, loss = 0.61, batch loss = 0.54 (9.3 examples/sec; 0.860 sec/batch; 76h:42m:41s remains)
INFO - root - 2017-12-07 05:07:54.207637: step 11530, loss = 0.69, batch loss = 0.61 (10.0 examples/sec; 0.800 sec/batch; 71h:21m:09s remains)
INFO - root - 2017-12-07 05:08:02.558457: step 11540, loss = 0.72, batch loss = 0.64 (9.6 examples/sec; 0.833 sec/batch; 74h:18m:38s remains)
INFO - root - 2017-12-07 05:08:10.817480: step 11550, loss = 0.78, batch loss = 0.70 (9.6 examples/sec; 0.835 sec/batch; 74h:29m:08s remains)
INFO - root - 2017-12-07 05:08:19.165503: step 11560, loss = 0.70, batch loss = 0.62 (10.3 examples/sec; 0.778 sec/batch; 69h:21m:18s remains)
INFO - root - 2017-12-07 05:08:27.461971: step 11570, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.803 sec/batch; 71h:33m:27s remains)
INFO - root - 2017-12-07 05:08:35.851552: step 11580, loss = 1.01, batch loss = 0.94 (9.5 examples/sec; 0.839 sec/batch; 74h:48m:34s remains)
INFO - root - 2017-12-07 05:08:44.066441: step 11590, loss = 0.82, batch loss = 0.75 (9.6 examples/sec; 0.833 sec/batch; 74h:17m:00s remains)
INFO - root - 2017-12-07 05:08:52.489662: step 11600, loss = 0.70, batch loss = 0.62 (9.3 examples/sec; 0.861 sec/batch; 76h:46m:43s remains)
2017-12-07 05:08:53.176175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7334018 -3.1612732 -3.554404 -3.8077557 -3.8221169 -3.6607294 -3.4922907 -3.3587241 -3.3097653 -3.3425403 -3.3836455 -3.4401884 -3.5065763 -3.5483971 -3.5467048][-2.7362576 -3.4306939 -4.0996714 -4.561841 -4.6437683 -4.4328928 -4.1950574 -3.9892483 -3.9037895 -3.9687777 -4.0809031 -4.1948757 -4.2897897 -4.3194189 -4.2921376][-2.6684184 -3.551198 -4.4673252 -5.1528125 -5.326581 -5.077539 -4.7773352 -4.5539517 -4.4934025 -4.6206985 -4.835515 -5.0373864 -5.1973968 -5.2283292 -5.1645803][-2.4249117 -3.3081357 -4.293726 -5.0863853 -5.3129396 -4.999476 -4.6236172 -4.4514384 -4.514338 -4.7850938 -5.1568327 -5.4748836 -5.7040548 -5.7338638 -5.6336794][-2.1323283 -2.8627253 -3.6862783 -4.3603296 -4.4969854 -4.0577793 -3.5870843 -3.4743259 -3.7338982 -4.2562118 -4.8670907 -5.31114 -5.5529637 -5.5223985 -5.37853][-1.9713497 -2.5195446 -3.0415764 -3.3898826 -3.2900167 -2.7202206 -2.172076 -2.0615952 -2.4907434 -3.2746506 -4.1501212 -4.7211308 -4.9542561 -4.85939 -4.6808634][-1.9714389 -2.3907216 -2.6348684 -2.6603513 -2.3408136 -1.7150996 -1.0679722 -0.87585139 -1.4011908 -2.37941 -3.4307718 -4.0632534 -4.2856317 -4.1948056 -4.0629988][-2.0475457 -2.2954514 -2.3287458 -2.2029276 -1.8177776 -1.2092421 -0.42417717 -0.078968048 -0.65595365 -1.7670538 -2.9037991 -3.5518196 -3.750917 -3.7159469 -3.7199957][-2.0659454 -2.1296027 -2.0392084 -1.9243851 -1.6042011 -1.0477874 -0.19269371 0.22681952 -0.36338377 -1.5003235 -2.6037455 -3.18642 -3.3054831 -3.2619882 -3.3710504][-2.0958309 -2.0831459 -1.9601429 -1.8841662 -1.5982451 -1.1046569 -0.35057545 0.00064516068 -0.54887295 -1.5847106 -2.5076151 -2.9412317 -2.9621885 -2.8671002 -3.0265162][-2.2544875 -2.2747121 -2.1855145 -2.1287796 -1.8284395 -1.3916838 -0.80588913 -0.5272727 -0.97277355 -1.8492768 -2.5548444 -2.8525944 -2.829016 -2.7045503 -2.8496256][-2.55095 -2.6682711 -2.6370401 -2.5887728 -2.3000815 -1.942467 -1.4688079 -1.171006 -1.459445 -2.1753356 -2.7080538 -2.89854 -2.8695154 -2.7175868 -2.8154917][-2.6744056 -2.8676438 -2.8875985 -2.8633461 -2.6470563 -2.3836734 -2.0008347 -1.7102468 -1.8733151 -2.4277778 -2.8346438 -2.986227 -2.9801922 -2.8118556 -2.8326302][-2.5621402 -2.7486076 -2.7752638 -2.7689967 -2.6517603 -2.4709015 -2.1615026 -1.9119012 -1.9990926 -2.3783519 -2.6623988 -2.8037505 -2.8508599 -2.7261634 -2.7273626][-2.515995 -2.631155 -2.6108179 -2.5862851 -2.535244 -2.4093013 -2.1577621 -1.945981 -1.9721184 -2.180618 -2.3489771 -2.49057 -2.5937312 -2.5486598 -2.5627975]]...]
INFO - root - 2017-12-07 05:09:01.542262: step 11610, loss = 0.61, batch loss = 0.54 (10.2 examples/sec; 0.787 sec/batch; 70h:07m:40s remains)
INFO - root - 2017-12-07 05:09:09.908343: step 11620, loss = 0.71, batch loss = 0.64 (9.3 examples/sec; 0.861 sec/batch; 76h:45m:23s remains)
INFO - root - 2017-12-07 05:09:18.392002: step 11630, loss = 0.67, batch loss = 0.60 (9.3 examples/sec; 0.858 sec/batch; 76h:30m:08s remains)
INFO - root - 2017-12-07 05:09:26.854365: step 11640, loss = 0.71, batch loss = 0.64 (9.5 examples/sec; 0.841 sec/batch; 74h:57m:28s remains)
INFO - root - 2017-12-07 05:09:35.344132: step 11650, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.798 sec/batch; 71h:05m:01s remains)
INFO - root - 2017-12-07 05:09:43.346839: step 11660, loss = 0.74, batch loss = 0.67 (13.8 examples/sec; 0.581 sec/batch; 51h:49m:00s remains)
INFO - root - 2017-12-07 05:09:51.732736: step 11670, loss = 0.97, batch loss = 0.90 (9.8 examples/sec; 0.819 sec/batch; 73h:00m:31s remains)
INFO - root - 2017-12-07 05:10:00.031437: step 11680, loss = 0.97, batch loss = 0.89 (9.2 examples/sec; 0.870 sec/batch; 77h:34m:27s remains)
INFO - root - 2017-12-07 05:10:08.349369: step 11690, loss = 0.87, batch loss = 0.80 (9.7 examples/sec; 0.826 sec/batch; 73h:34m:06s remains)
INFO - root - 2017-12-07 05:10:16.751964: step 11700, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.821 sec/batch; 73h:11m:21s remains)
2017-12-07 05:10:17.475835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2338557 -2.4167984 -2.5911865 -2.8016868 -3.3081236 -3.63977 -3.2479441 -2.6532192 -2.5337718 -2.4497023 -1.8907895 -1.5269935 -1.9713945 -2.3539267 -2.1254196][-2.0488181 -2.0981 -2.2367094 -2.5659928 -3.2245235 -3.7024167 -3.5179558 -3.1334207 -2.9693294 -2.6010523 -1.8413041 -1.4840944 -2.0541263 -2.5325289 -2.3413279][-1.8025281 -1.8133411 -2.1424346 -2.7230773 -3.4686658 -3.9404228 -3.8552423 -3.5783796 -3.3129904 -2.71031 -1.8249934 -1.5373123 -2.2845309 -2.9258332 -2.8067708][-1.6222222 -1.7553842 -2.3803964 -3.1448855 -3.7552047 -3.9345226 -3.6569071 -3.2821293 -2.9640491 -2.3789458 -1.6459038 -1.567554 -2.502677 -3.3033807 -3.2752094][-1.5647795 -1.8233905 -2.6391072 -3.4019675 -3.6722195 -3.388355 -2.8646741 -2.5482483 -2.46366 -2.0955822 -1.5397234 -1.5958569 -2.6018744 -3.5086198 -3.6028657][-1.6064384 -1.9103472 -2.6789856 -3.1805429 -2.9675899 -2.2090185 -1.5918913 -1.6596315 -2.0875583 -1.9912763 -1.5148547 -1.6065178 -2.5925047 -3.5178032 -3.7175176][-1.595777 -1.8658309 -2.4518507 -2.628114 -2.0100744 -0.94903708 -0.39122581 -0.9142735 -1.7320867 -1.7755456 -1.3656721 -1.504473 -2.4021549 -3.1820574 -3.4024808][-2.0331187 -2.1013129 -2.4127729 -2.3740082 -1.6756506 -0.65395927 -0.30293751 -1.0482945 -1.8876631 -1.8363249 -1.4314742 -1.5829308 -2.2730803 -2.7528191 -2.8694482][-2.8221495 -2.785109 -2.889919 -2.762465 -2.1860323 -1.3952057 -1.2462335 -1.9426522 -2.5396187 -2.3303211 -1.9928403 -2.128366 -2.5336151 -2.6729202 -2.6818752][-3.0692568 -3.0751548 -3.1613634 -3.0701008 -2.6037369 -1.9857397 -1.8897414 -2.3764582 -2.6877582 -2.486443 -2.4286511 -2.64892 -2.7912583 -2.6642289 -2.6367567][-3.0180774 -2.9903178 -3.1511242 -3.1965485 -2.8287177 -2.323071 -2.2446625 -2.5263915 -2.5714254 -2.420398 -2.6560178 -2.95794 -2.8588705 -2.5321507 -2.4796164][-3.1154923 -3.0802593 -3.3816657 -3.5828881 -3.2293665 -2.7106833 -2.5849051 -2.6734061 -2.4721313 -2.3117108 -2.7520561 -3.1241865 -2.8865061 -2.4558029 -2.4025292][-3.0876594 -3.02868 -3.4045532 -3.6932528 -3.3310802 -2.7841415 -2.5607364 -2.4375594 -2.0963647 -2.0320907 -2.7173696 -3.2271674 -2.9840052 -2.5940862 -2.5914431][-2.9645875 -2.9179976 -3.2741318 -3.4982026 -3.1135411 -2.6072083 -2.3034241 -1.9826999 -1.58072 -1.6716614 -2.5811019 -3.2662578 -3.1762679 -2.9614244 -2.9730835][-2.9887676 -3.0323262 -3.3140204 -3.410902 -3.0766673 -2.7600632 -2.4511304 -1.9267638 -1.3853664 -1.5413713 -2.5781226 -3.39423 -3.4306474 -3.2161694 -2.9851913]]...]
INFO - root - 2017-12-07 05:10:25.838737: step 11710, loss = 0.68, batch loss = 0.61 (9.5 examples/sec; 0.846 sec/batch; 75h:24m:58s remains)
INFO - root - 2017-12-07 05:10:34.195225: step 11720, loss = 0.88, batch loss = 0.81 (8.5 examples/sec; 0.941 sec/batch; 83h:49m:05s remains)
INFO - root - 2017-12-07 05:10:42.542064: step 11730, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.811 sec/batch; 72h:17m:05s remains)
INFO - root - 2017-12-07 05:10:51.086171: step 11740, loss = 0.61, batch loss = 0.54 (9.7 examples/sec; 0.824 sec/batch; 73h:25m:17s remains)
INFO - root - 2017-12-07 05:10:59.519486: step 11750, loss = 0.87, batch loss = 0.80 (9.1 examples/sec; 0.877 sec/batch; 78h:08m:57s remains)
INFO - root - 2017-12-07 05:11:07.948472: step 11760, loss = 0.77, batch loss = 0.70 (9.7 examples/sec; 0.823 sec/batch; 73h:16m:50s remains)
INFO - root - 2017-12-07 05:11:16.246440: step 11770, loss = 0.76, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:35m:44s remains)
INFO - root - 2017-12-07 05:11:24.573900: step 11780, loss = 0.86, batch loss = 0.79 (10.0 examples/sec; 0.799 sec/batch; 71h:12m:45s remains)
INFO - root - 2017-12-07 05:11:32.899047: step 11790, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.803 sec/batch; 71h:32m:01s remains)
INFO - root - 2017-12-07 05:11:41.274981: step 11800, loss = 0.90, batch loss = 0.82 (9.5 examples/sec; 0.845 sec/batch; 75h:18m:06s remains)
2017-12-07 05:11:41.919889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2915931 -3.2798088 -3.2789521 -3.27317 -3.3204141 -3.3235023 -3.2649612 -3.1836038 -3.0971913 -3.037353 -2.9561191 -2.8263912 -2.8201647 -2.93289 -2.9714146][-2.5497789 -2.4800587 -2.480823 -2.4664478 -2.4665484 -2.4353986 -2.3608797 -2.2471344 -2.1326199 -2.0669041 -2.0299551 -1.9860213 -2.1083395 -2.3519754 -2.494945][-2.7802825 -2.7268581 -2.7711 -2.7749817 -2.7250051 -2.6279936 -2.5040731 -2.3637805 -2.2312038 -2.1929328 -2.21826 -2.2191052 -2.3179734 -2.4752436 -2.5301228][-2.8313298 -2.7700493 -2.7821288 -2.7942324 -2.7529271 -2.6676064 -2.5899136 -2.5501194 -2.521126 -2.5480247 -2.5828645 -2.5542178 -2.5566025 -2.5748761 -2.5212917][-2.4276125 -2.305685 -2.2312496 -2.272584 -2.3272529 -2.3137698 -2.3036504 -2.3919113 -2.5233159 -2.6447182 -2.7352133 -2.768826 -2.8143163 -2.790277 -2.6373479][-2.21693 -2.1451762 -2.0955639 -2.1911948 -2.275624 -2.190356 -2.0563376 -2.0595751 -2.2181921 -2.4220073 -2.6355972 -2.8195939 -3.0252616 -3.1121349 -2.9872527][-1.7724147 -1.8445148 -1.9101477 -2.0179594 -2.0348465 -1.8271186 -1.5335929 -1.3911772 -1.4704885 -1.6985984 -1.9725587 -2.2021074 -2.4954491 -2.7333877 -2.8048263][-0.95870781 -0.99752164 -1.0878699 -1.1983879 -1.243988 -1.1340964 -0.99694824 -0.97382307 -1.0495389 -1.2229664 -1.3797789 -1.4548469 -1.6814647 -1.9941785 -2.2589195][-1.0170856 -0.95580482 -0.99796128 -1.0731506 -1.1567371 -1.171741 -1.2491648 -1.3977985 -1.469182 -1.5621467 -1.5977073 -1.5390501 -1.6458888 -1.8757792 -2.0876317][-1.7292862 -1.7174127 -1.728596 -1.7261214 -1.744343 -1.7619548 -1.9017572 -2.09515 -2.1559699 -2.2375276 -2.2725403 -2.2172217 -2.2887211 -2.4148965 -2.4382749][-2.0252297 -2.1629169 -2.258039 -2.2641919 -2.2124026 -2.1438911 -2.1848657 -2.2898231 -2.3342161 -2.4428139 -2.4967055 -2.4538741 -2.5255685 -2.6425791 -2.6413488][-1.9493968 -2.1217844 -2.2872472 -2.3750393 -2.3509305 -2.2582409 -2.1848152 -2.1355803 -2.0918324 -2.1642375 -2.210218 -2.1929324 -2.3335342 -2.5524025 -2.6493051][-2.2281716 -2.3061893 -2.4285855 -2.533653 -2.5654833 -2.5380483 -2.5011704 -2.4538889 -2.4090621 -2.4383025 -2.4312911 -2.3936338 -2.4986973 -2.6693974 -2.7466085][-2.78751 -2.8215485 -2.8837242 -2.9479289 -2.9950066 -3.0274215 -3.058579 -3.073247 -3.0836606 -3.1295872 -3.1384661 -3.0961242 -3.1074893 -3.1180229 -3.0570655][-3.1076572 -3.1337879 -3.1506016 -3.1633739 -3.1795692 -3.2054906 -3.2386668 -3.2605617 -3.2790494 -3.323118 -3.3594694 -3.3648734 -3.3786721 -3.356811 -3.2701087]]...]
INFO - root - 2017-12-07 05:11:50.199671: step 11810, loss = 0.79, batch loss = 0.71 (9.1 examples/sec; 0.880 sec/batch; 78h:26m:06s remains)
INFO - root - 2017-12-07 05:11:58.565367: step 11820, loss = 0.68, batch loss = 0.61 (10.0 examples/sec; 0.802 sec/batch; 71h:26m:15s remains)
INFO - root - 2017-12-07 05:12:06.914684: step 11830, loss = 0.64, batch loss = 0.57 (9.8 examples/sec; 0.814 sec/batch; 72h:30m:54s remains)
INFO - root - 2017-12-07 05:12:15.166834: step 11840, loss = 1.01, batch loss = 0.94 (9.7 examples/sec; 0.824 sec/batch; 73h:24m:42s remains)
INFO - root - 2017-12-07 05:12:23.431100: step 11850, loss = 0.72, batch loss = 0.65 (9.8 examples/sec; 0.816 sec/batch; 72h:41m:49s remains)
INFO - root - 2017-12-07 05:12:31.758863: step 11860, loss = 0.77, batch loss = 0.70 (9.6 examples/sec; 0.831 sec/batch; 73h:59m:20s remains)
INFO - root - 2017-12-07 05:12:40.032937: step 11870, loss = 0.63, batch loss = 0.55 (9.3 examples/sec; 0.864 sec/batch; 76h:57m:28s remains)
INFO - root - 2017-12-07 05:12:48.411350: step 11880, loss = 0.67, batch loss = 0.60 (10.6 examples/sec; 0.752 sec/batch; 66h:56m:40s remains)
INFO - root - 2017-12-07 05:12:56.762577: step 11890, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.807 sec/batch; 71h:50m:10s remains)
INFO - root - 2017-12-07 05:13:05.076611: step 11900, loss = 0.87, batch loss = 0.79 (9.2 examples/sec; 0.871 sec/batch; 77h:32m:19s remains)
2017-12-07 05:13:05.720139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8008897 -2.4675436 -2.5117526 -2.5515122 -2.6204729 -2.820549 -3.0839512 -3.2405431 -3.2224431 -3.2408466 -3.3024573 -3.3109765 -3.1917338 -3.0463896 -3.0559258][-2.6029444 -2.302196 -2.4642129 -2.6179752 -2.7166743 -2.8938131 -3.1349015 -3.2538197 -3.2365685 -3.2796445 -3.3627548 -3.3855882 -3.26678 -3.157666 -3.2623429][-2.3703167 -2.1190658 -2.3523932 -2.5529935 -2.6327171 -2.7830899 -2.9738336 -2.9795179 -2.9228048 -2.9786971 -3.1016579 -3.1987228 -3.176054 -3.1702201 -3.36243][-2.1077316 -1.8371031 -2.0242949 -2.2158055 -2.2973118 -2.458545 -2.5682259 -2.4146886 -2.3170428 -2.3743446 -2.4983168 -2.6496761 -2.7645385 -2.9350145 -3.2428641][-1.9329054 -1.6310568 -1.6968603 -1.8150706 -1.8661523 -1.963496 -1.8831136 -1.5804803 -1.5362351 -1.678555 -1.8392055 -2.0600755 -2.2868152 -2.5849917 -2.9634471][-1.942492 -1.7190857 -1.728466 -1.7362702 -1.6038415 -1.3839025 -0.89949083 -0.43321753 -0.60194921 -1.0231788 -1.366612 -1.6978438 -1.9825478 -2.2953606 -2.6126552][-2.140799 -2.0948474 -2.1143787 -1.9821405 -1.5784552 -0.9334054 0.018434525 0.63560152 0.19252014 -0.56335235 -1.1150842 -1.5211315 -1.7699192 -1.9743807 -2.1687407][-2.5236411 -2.6521504 -2.6961732 -2.4844732 -1.9517434 -1.1400602 -0.067212105 0.49354362 -0.021070004 -0.77541518 -1.2124987 -1.4598172 -1.5089648 -1.5149245 -1.5927083][-2.8653169 -3.0473232 -3.0795007 -2.8823116 -2.4564209 -1.8463211 -1.0860791 -0.81145716 -1.2436693 -1.6897168 -1.7599666 -1.6534421 -1.3823442 -1.1486795 -1.1600773][-2.980732 -3.0905676 -3.0966997 -3.0216646 -2.8692789 -2.6152611 -2.2694457 -2.2750087 -2.5916924 -2.7454052 -2.5047965 -2.0866666 -1.5341053 -1.1211531 -1.0871184][-2.9305613 -2.8798566 -2.8609605 -2.9680104 -3.1081111 -3.135282 -3.0915601 -3.2641044 -3.5037885 -3.5589905 -3.2620249 -2.7375424 -2.0611444 -1.5830355 -1.5192275][-2.8724818 -2.6865826 -2.6690283 -2.9083428 -3.1779029 -3.2449751 -3.2597911 -3.4664903 -3.6840122 -3.8073971 -3.6996412 -3.3365107 -2.7480903 -2.3021619 -2.1601985][-2.8632338 -2.6003618 -2.5815134 -2.8307703 -3.0516033 -3.0147548 -2.975678 -3.1550722 -3.3888922 -3.5804689 -3.6498227 -3.5385873 -3.1974673 -2.8762693 -2.6690359][-2.8485801 -2.5391564 -2.487411 -2.6823099 -2.820734 -2.7161641 -2.6370759 -2.762552 -2.9682112 -3.106792 -3.1929698 -3.2526221 -3.18085 -3.0028195 -2.784256][-2.7888741 -2.4647508 -2.3833501 -2.556313 -2.6860075 -2.6281056 -2.5491004 -2.6043463 -2.713747 -2.6891894 -2.6606088 -2.7724428 -2.89334 -2.778348 -2.5446603]]...]
INFO - root - 2017-12-07 05:13:14.011738: step 11910, loss = 0.77, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 80h:59m:40s remains)
INFO - root - 2017-12-07 05:13:22.185420: step 11920, loss = 0.95, batch loss = 0.88 (9.9 examples/sec; 0.812 sec/batch; 72h:16m:37s remains)
INFO - root - 2017-12-07 05:13:30.533798: step 11930, loss = 0.73, batch loss = 0.66 (10.2 examples/sec; 0.784 sec/batch; 69h:46m:47s remains)
INFO - root - 2017-12-07 05:13:38.799077: step 11940, loss = 0.66, batch loss = 0.58 (9.9 examples/sec; 0.809 sec/batch; 72h:04m:22s remains)
INFO - root - 2017-12-07 05:13:47.125723: step 11950, loss = 0.63, batch loss = 0.56 (10.1 examples/sec; 0.794 sec/batch; 70h:41m:58s remains)
INFO - root - 2017-12-07 05:13:55.434922: step 11960, loss = 0.70, batch loss = 0.62 (9.6 examples/sec; 0.834 sec/batch; 74h:17m:50s remains)
INFO - root - 2017-12-07 05:14:03.801544: step 11970, loss = 0.69, batch loss = 0.62 (9.1 examples/sec; 0.881 sec/batch; 78h:27m:17s remains)
INFO - root - 2017-12-07 05:14:11.987658: step 11980, loss = 0.85, batch loss = 0.78 (9.2 examples/sec; 0.865 sec/batch; 77h:01m:40s remains)
INFO - root - 2017-12-07 05:14:21.921163: step 11990, loss = 0.78, batch loss = 0.71 (4.3 examples/sec; 1.865 sec/batch; 166h:03m:58s remains)
INFO - root - 2017-12-07 05:14:32.773466: step 12000, loss = 0.69, batch loss = 0.62 (6.6 examples/sec; 1.213 sec/batch; 107h:58m:53s remains)
2017-12-07 05:14:33.570561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4645152 -2.454628 -2.7059054 -2.9131935 -3.0107229 -2.9779637 -2.7496653 -2.5136628 -2.2948158 -2.2412908 -2.3712935 -2.5981052 -2.826057 -2.8482625 -2.7911043][-2.0641272 -2.001034 -2.3076019 -2.5477295 -2.6609268 -2.6024323 -2.375422 -2.101913 -1.7410336 -1.5607893 -1.6746669 -2.0091844 -2.377135 -2.3860238 -2.2216394][-1.4237001 -1.2891262 -1.6176009 -1.8964648 -2.0017419 -1.908489 -1.7182181 -1.4474452 -0.9776535 -0.624213 -0.72891426 -1.2433875 -1.7568719 -1.73978 -1.462929][-0.80352139 -0.60900855 -0.94443727 -1.2592976 -1.3616724 -1.2783966 -1.1914535 -0.93785167 -0.34615707 0.26484871 0.20239639 -0.52694225 -1.2116487 -1.2229085 -0.93953228][-0.5067203 -0.2410984 -0.50172925 -0.794235 -0.88091612 -0.91499782 -0.97757006 -0.75696588 -0.025690079 0.87811613 0.93876457 0.06205368 -0.763402 -0.86410332 -0.76561642][-0.71897006 -0.4257884 -0.53253436 -0.66821742 -0.68637824 -0.82303095 -0.98538804 -0.74769211 0.066273212 1.1192803 1.2704501 0.32011032 -0.55601239 -0.7208972 -0.86427283][-1.2366366 -0.93543029 -0.87645197 -0.77990651 -0.70288491 -0.89676094 -1.0522146 -0.72817588 0.12285519 1.1131854 1.1787658 0.20261908 -0.5330193 -0.65655255 -0.98264289][-1.7642739 -1.4687462 -1.2790124 -0.9606874 -0.77588773 -0.9597795 -1.1033742 -0.72153234 0.13312197 0.9375205 0.78770494 -0.15078831 -0.6555109 -0.66257691 -1.0788043][-2.195442 -1.8945735 -1.6100738 -1.0748367 -0.73081946 -0.83142185 -0.9716742 -0.65949869 0.12739754 0.73114157 0.42218447 -0.33803225 -0.63172436 -0.62224412 -1.1394556][-2.508091 -2.2360959 -1.8623483 -1.096632 -0.51207972 -0.42066288 -0.53120947 -0.40957594 0.14325762 0.45366383 0.10642576 -0.35657167 -0.47739196 -0.59393525 -1.2216907][-2.48271 -2.3067284 -1.8975148 -0.93956637 -0.12649584 0.18248653 0.10964155 0.0047836304 0.20738554 0.16340017 -0.16201115 -0.342793 -0.39792919 -0.72249651 -1.3886213][-2.2703872 -2.1917648 -1.7504494 -0.67996311 0.27961349 0.76774406 0.76715326 0.52132893 0.35355139 -0.0033841133 -0.29279423 -0.32575417 -0.4525342 -0.9507761 -1.5125983][-2.1075816 -2.1335828 -1.6928241 -0.57310605 0.42815495 1.0165877 1.1478987 0.88941479 0.514236 0.048485756 -0.15954399 -0.170712 -0.44310069 -0.98243451 -1.2893395][-2.0449193 -2.1480772 -1.7398405 -0.66733122 0.26573324 0.85633278 1.1310468 0.94854307 0.55124521 0.20527601 0.11719322 0.060314178 -0.33323765 -0.834666 -0.90250969][-1.9135303 -2.0473301 -1.7181571 -0.8436842 -0.093780518 0.44702339 0.81521749 0.69347954 0.34369898 0.18206882 0.16976261 0.050671577 -0.34411478 -0.72093844 -0.59855819]]...]
INFO - root - 2017-12-07 05:14:45.199039: step 12010, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 1.146 sec/batch; 102h:00m:57s remains)
INFO - root - 2017-12-07 05:14:56.945182: step 12020, loss = 0.73, batch loss = 0.65 (7.1 examples/sec; 1.132 sec/batch; 100h:45m:26s remains)
INFO - root - 2017-12-07 05:15:08.560681: step 12030, loss = 0.57, batch loss = 0.49 (7.0 examples/sec; 1.145 sec/batch; 101h:55m:40s remains)
INFO - root - 2017-12-07 05:15:20.264686: step 12040, loss = 0.74, batch loss = 0.67 (6.7 examples/sec; 1.195 sec/batch; 106h:21m:12s remains)
INFO - root - 2017-12-07 05:15:31.822786: step 12050, loss = 0.87, batch loss = 0.80 (6.8 examples/sec; 1.177 sec/batch; 104h:44m:30s remains)
INFO - root - 2017-12-07 05:15:43.359690: step 12060, loss = 0.86, batch loss = 0.78 (6.9 examples/sec; 1.162 sec/batch; 103h:25m:29s remains)
INFO - root - 2017-12-07 05:15:54.983694: step 12070, loss = 0.57, batch loss = 0.49 (6.7 examples/sec; 1.188 sec/batch; 105h:43m:11s remains)
INFO - root - 2017-12-07 05:16:06.630255: step 12080, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.175 sec/batch; 104h:36m:52s remains)
INFO - root - 2017-12-07 05:16:18.290709: step 12090, loss = 0.92, batch loss = 0.85 (6.9 examples/sec; 1.152 sec/batch; 102h:29m:46s remains)
INFO - root - 2017-12-07 05:16:29.884984: step 12100, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 1.159 sec/batch; 103h:10m:15s remains)
2017-12-07 05:16:30.744262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2685237 -1.2110119 -1.0499239 -0.91888595 -0.73570037 -0.55894995 -0.57826567 -0.41513824 0.061929226 0.065633774 -0.4542017 -0.83956552 -0.85454416 -0.77698612 -0.90261436][-1.4250445 -1.223453 -0.89870143 -0.74063849 -0.71389866 -0.72334242 -0.81178594 -0.60967946 -0.095732212 0.0025444031 -0.42075348 -0.73933911 -0.70248747 -0.61610866 -0.84467649][-1.5049949 -1.196348 -0.791034 -0.68974209 -0.86780047 -1.0585585 -1.1567824 -0.84514141 -0.28755426 -0.17913008 -0.56145716 -0.87438989 -0.81852937 -0.72581625 -0.92465568][-1.6019959 -1.3223403 -0.92026973 -0.8698225 -1.2258146 -1.5244033 -1.5021441 -0.98877454 -0.41048527 -0.41490698 -0.88638163 -1.2580283 -1.2197669 -1.1496239 -1.2552097][-1.9691374 -1.7584794 -1.304812 -1.179508 -1.5944481 -1.9203107 -1.6766458 -0.92958689 -0.40552235 -0.66212583 -1.3036442 -1.710743 -1.6588874 -1.6357176 -1.710326][-2.333636 -2.1887181 -1.6381729 -1.3565137 -1.7894106 -2.1780338 -1.8367248 -0.96359372 -0.54054427 -1.0253811 -1.7324097 -2.0021837 -1.8190939 -1.8108356 -1.9440665][-2.1813836 -2.1545651 -1.6244383 -1.324491 -1.8167257 -2.325536 -2.0439568 -1.2069032 -0.88531065 -1.4371035 -2.045012 -2.0544868 -1.6764297 -1.6426063 -1.8765728][-1.4611638 -1.606688 -1.3648226 -1.2806647 -1.7944727 -2.2640035 -2.0412698 -1.3878016 -1.2050374 -1.7452269 -2.255827 -2.099072 -1.5914991 -1.4775746 -1.7151589][-0.64123368 -0.926687 -1.0815251 -1.2925389 -1.7154236 -1.938926 -1.7160239 -1.325861 -1.3440409 -1.8961658 -2.3893392 -2.2347448 -1.7219057 -1.4594181 -1.4729531][-0.0790534 -0.41562796 -0.83674622 -1.2505212 -1.5155997 -1.4402838 -1.259815 -1.1771131 -1.4094217 -1.9297874 -2.3674061 -2.3065808 -1.9159145 -1.5234067 -1.2384319][0.05138731 -0.22976971 -0.74030614 -1.2595372 -1.4045362 -1.1422892 -1.045114 -1.2165124 -1.5298936 -1.8928947 -2.1540165 -2.1456161 -1.8782437 -1.4743712 -1.1223855][-0.23509359 -0.38233137 -0.800936 -1.3056233 -1.3911755 -1.1319404 -1.1902218 -1.4422772 -1.6147695 -1.6423476 -1.5718029 -1.5127227 -1.4119279 -1.2388914 -1.1821952][-0.62928319 -0.65129757 -0.95480895 -1.4338546 -1.5481462 -1.4645221 -1.6682737 -1.8230951 -1.6892207 -1.3062415 -0.89685321 -0.78756309 -0.86831379 -1.0202093 -1.4021091][-0.88289118 -0.92981029 -1.2203467 -1.6585729 -1.8107352 -1.9546659 -2.247252 -2.2649939 -1.9184346 -1.3195066 -0.79756212 -0.73814106 -0.93302631 -1.2011199 -1.765806][-0.80836058 -0.97527504 -1.3225315 -1.7641895 -2.0224481 -2.3794148 -2.7001395 -2.5772867 -2.1457474 -1.5446167 -1.1347666 -1.2258911 -1.5108168 -1.7471564 -2.2038291]]...]
INFO - root - 2017-12-07 05:16:42.430553: step 12110, loss = 0.84, batch loss = 0.77 (7.2 examples/sec; 1.106 sec/batch; 98h:27m:47s remains)
INFO - root - 2017-12-07 05:16:54.006885: step 12120, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.167 sec/batch; 103h:50m:45s remains)
INFO - root - 2017-12-07 05:17:05.618901: step 12130, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.137 sec/batch; 101h:08m:22s remains)
INFO - root - 2017-12-07 05:17:17.215010: step 12140, loss = 0.79, batch loss = 0.71 (6.8 examples/sec; 1.181 sec/batch; 105h:07m:47s remains)
INFO - root - 2017-12-07 05:17:28.785829: step 12150, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 1.165 sec/batch; 103h:41m:33s remains)
INFO - root - 2017-12-07 05:17:40.468223: step 12160, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 1.180 sec/batch; 104h:58m:43s remains)
INFO - root - 2017-12-07 05:17:52.280457: step 12170, loss = 0.59, batch loss = 0.51 (6.9 examples/sec; 1.152 sec/batch; 102h:30m:20s remains)
INFO - root - 2017-12-07 05:18:03.881394: step 12180, loss = 0.67, batch loss = 0.59 (7.1 examples/sec; 1.131 sec/batch; 100h:39m:55s remains)
INFO - root - 2017-12-07 05:18:15.771983: step 12190, loss = 0.58, batch loss = 0.51 (6.6 examples/sec; 1.220 sec/batch; 108h:35m:14s remains)
INFO - root - 2017-12-07 05:18:27.615161: step 12200, loss = 0.67, batch loss = 0.60 (6.7 examples/sec; 1.189 sec/batch; 105h:45m:22s remains)
2017-12-07 05:18:28.476161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0657911 -3.1974771 -3.31176 -3.4019938 -3.4597895 -3.4874134 -3.50379 -3.5124207 -3.4924393 -3.4336925 -3.4035206 -3.3884962 -3.353435 -3.2949734 -3.1900826][-3.3086009 -3.4588511 -3.5413299 -3.6115963 -3.6629794 -3.6679707 -3.6360109 -3.60607 -3.576396 -3.5146897 -3.5055394 -3.5188832 -3.4946091 -3.4392152 -3.3153496][-3.4266462 -3.5551577 -3.5289698 -3.5144565 -3.5514479 -3.5567942 -3.4999807 -3.4710989 -3.5003824 -3.5005331 -3.5573187 -3.6390796 -3.6448956 -3.5805736 -3.401592][-3.3733764 -3.4806511 -3.3227644 -3.1739872 -3.1616902 -3.1493914 -3.0574827 -3.0488968 -3.1903796 -3.2791467 -3.3926921 -3.5747387 -3.6817346 -3.6506941 -3.4267654][-3.1987841 -3.3141084 -3.0738988 -2.7830291 -2.6746459 -2.5623269 -2.3187354 -2.2191598 -2.4356766 -2.6132708 -2.7556009 -3.0274014 -3.266119 -3.3020129 -3.068377][-2.9609213 -3.053206 -2.7385969 -2.2873111 -2.0304945 -1.7564368 -1.2696764 -0.99727464 -1.30776 -1.7006507 -1.9753067 -2.3814743 -2.7642651 -2.8695476 -2.6058326][-2.6815555 -2.6917453 -2.2693384 -1.624613 -1.2105579 -0.8286736 -0.14382458 0.31085634 -0.096874714 -0.76381993 -1.2271149 -1.7458453 -2.2486517 -2.455874 -2.2351356][-2.4385395 -2.3260882 -1.8057058 -1.027838 -0.5691433 -0.27034092 0.37655163 0.90374184 0.5128417 -0.23440266 -0.7433362 -1.2074242 -1.6866634 -1.9401116 -1.8155961][-2.2940972 -2.0915008 -1.5497601 -0.76559258 -0.38955736 -0.35137844 0.0036435127 0.45876551 0.27578449 -0.26353836 -0.63273215 -0.93531084 -1.2760153 -1.4741924 -1.4392698][-2.2360721 -2.0268433 -1.5708187 -0.89060473 -0.61355495 -0.79374981 -0.76770186 -0.47153592 -0.44538403 -0.66401196 -0.79342508 -0.88448405 -1.0377047 -1.1509202 -1.207608][-2.2189598 -2.0481861 -1.7582285 -1.2744467 -1.0865495 -1.3357747 -1.5144949 -1.4057007 -1.3202269 -1.3333488 -1.2886527 -1.2224059 -1.1639843 -1.1252484 -1.182574][-2.2235131 -2.1059253 -1.9849014 -1.728178 -1.6161427 -1.8052161 -2.0057142 -2.0238607 -1.9919248 -1.975605 -1.915266 -1.8253129 -1.6685798 -1.5055254 -1.4768198][-2.3217323 -2.2617028 -2.2448814 -2.1421514 -2.0736566 -2.1630716 -2.2907307 -2.340766 -2.3592002 -2.3696258 -2.3714106 -2.3479378 -2.2377288 -2.0814767 -1.9952381][-2.4628816 -2.4416761 -2.4589677 -2.4322691 -2.3946207 -2.4206924 -2.4848189 -2.5346303 -2.5865552 -2.63717 -2.6853223 -2.7048225 -2.6530046 -2.5504436 -2.4571562][-2.5409684 -2.5331192 -2.5549598 -2.5579829 -2.5429676 -2.5506587 -2.5840888 -2.631999 -2.693069 -2.7535708 -2.8043511 -2.8221402 -2.7912998 -2.7292337 -2.6603162]]...]
INFO - root - 2017-12-07 05:18:40.209752: step 12210, loss = 0.67, batch loss = 0.59 (6.9 examples/sec; 1.166 sec/batch; 103h:45m:16s remains)
INFO - root - 2017-12-07 05:18:51.856969: step 12220, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 1.136 sec/batch; 101h:05m:59s remains)
INFO - root - 2017-12-07 05:19:03.868531: step 12230, loss = 0.77, batch loss = 0.70 (6.6 examples/sec; 1.221 sec/batch; 108h:35m:41s remains)
INFO - root - 2017-12-07 05:19:15.372539: step 12240, loss = 0.55, batch loss = 0.48 (6.6 examples/sec; 1.208 sec/batch; 107h:26m:36s remains)
INFO - root - 2017-12-07 05:19:27.167965: step 12250, loss = 0.92, batch loss = 0.85 (6.8 examples/sec; 1.173 sec/batch; 104h:23m:26s remains)
INFO - root - 2017-12-07 05:19:38.969651: step 12260, loss = 0.76, batch loss = 0.68 (7.2 examples/sec; 1.113 sec/batch; 98h:58m:12s remains)
INFO - root - 2017-12-07 05:19:50.670996: step 12270, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.186 sec/batch; 105h:32m:25s remains)
INFO - root - 2017-12-07 05:20:02.459468: step 12280, loss = 0.94, batch loss = 0.87 (6.8 examples/sec; 1.181 sec/batch; 105h:01m:30s remains)
INFO - root - 2017-12-07 05:20:14.047570: step 12290, loss = 0.66, batch loss = 0.58 (7.1 examples/sec; 1.126 sec/batch; 100h:08m:54s remains)
INFO - root - 2017-12-07 05:20:25.408428: step 12300, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 1.158 sec/batch; 102h:57m:57s remains)
2017-12-07 05:20:26.338405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.877697 -4.7656054 -4.4352703 -3.578722 -2.3338692 -1.5697625 -2.1743681 -3.9089384 -5.5641794 -5.8915734 -4.7483039 -3.1520967 -2.1270483 -2.3453422 -3.4507422][-4.969923 -4.600709 -3.9951708 -2.7367964 -1.2259002 -0.58291578 -1.4118943 -3.2516122 -4.9230042 -5.2217016 -4.0944862 -2.8088403 -2.2754719 -2.7320175 -3.6958213][-5.0022244 -4.3608975 -3.3766413 -1.6456463 0.025074959 0.39257145 -0.662426 -2.5385761 -4.124548 -4.3126664 -3.2230308 -2.3223436 -2.3595986 -3.1336315 -4.0374489][-5.1582508 -4.2795129 -2.9149637 -0.86489677 0.72964239 0.78435326 -0.35912371 -2.1559722 -3.5497754 -3.4709151 -2.3316669 -1.7725608 -2.3267403 -3.4218445 -4.3271532][-5.1566038 -4.1161962 -2.531621 -0.49921346 0.78644419 0.63285542 -0.36128569 -1.892591 -2.9413755 -2.5465145 -1.4410803 -1.246567 -2.2043304 -3.5654533 -4.5252175][-5.137167 -4.0614991 -2.3721726 -0.48412156 0.57688093 0.49163151 -0.12009907 -1.1682074 -1.7178679 -1.1004231 -0.33606052 -0.70698214 -2.0335925 -3.6007929 -4.5952072][-5.3356123 -4.3910289 -2.6745315 -0.81296611 0.42319965 0.75527334 0.54202127 -0.04238081 -0.22805214 0.34795284 0.51849127 -0.45119047 -2.0275283 -3.64076 -4.5994277][-5.4591408 -4.8021712 -3.1831923 -1.2428031 0.4319129 1.377809 1.5002937 1.0794454 0.71802235 0.74129391 0.25848341 -1.0013523 -2.4810414 -3.8857186 -4.663229][-5.6143713 -5.3209147 -3.9778636 -2.0543849 0.050338745 1.7420692 2.4289308 2.0695291 1.0827808 0.11463594 -1.093262 -2.4565518 -3.5739844 -4.4577708 -4.8144455][-5.7745075 -5.8449039 -4.8732719 -3.1026316 -0.76949978 1.579452 2.9313593 2.6945968 1.1244469 -0.81931472 -2.73134 -4.1894684 -4.9174719 -5.1805339 -5.0469179][-5.7271852 -5.9835243 -5.3480291 -3.7929065 -1.4660597 1.2606883 3.0245857 2.7880154 0.92331886 -1.5676816 -3.8940585 -5.376369 -5.8070774 -5.6406984 -5.2255254][-5.5347548 -5.8002024 -5.3786345 -4.0657825 -1.93869 0.73688459 2.4179778 2.0020146 0.14581776 -2.3280659 -4.5869308 -5.8157921 -5.9359941 -5.546361 -5.1091824][-5.3362923 -5.5035205 -5.1495028 -4.0226521 -2.208396 0.018766403 1.2531829 0.64686394 -1.0239823 -3.2128005 -5.0669417 -5.7905407 -5.5781031 -5.1200457 -4.8477755][-5.0509486 -5.1263795 -4.8013563 -3.8500271 -2.3810601 -0.71942639 0.051505566 -0.53702068 -1.9076188 -3.7139637 -5.1018543 -5.3991041 -5.0744305 -4.727582 -4.6668234][-4.4794149 -4.5406337 -4.3471475 -3.7176251 -2.7393854 -1.6943052 -1.2784767 -1.7173939 -2.6699486 -3.9347382 -4.7499809 -4.7110162 -4.4223456 -4.2882862 -4.5135555]]...]
INFO - root - 2017-12-07 05:20:37.943887: step 12310, loss = 0.99, batch loss = 0.92 (6.8 examples/sec; 1.173 sec/batch; 104h:21m:58s remains)
INFO - root - 2017-12-07 05:20:49.635860: step 12320, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 1.161 sec/batch; 103h:13m:30s remains)
INFO - root - 2017-12-07 05:21:01.381092: step 12330, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 1.147 sec/batch; 102h:03m:14s remains)
INFO - root - 2017-12-07 05:21:13.157952: step 12340, loss = 0.92, batch loss = 0.85 (6.7 examples/sec; 1.192 sec/batch; 105h:58m:32s remains)
INFO - root - 2017-12-07 05:21:24.765840: step 12350, loss = 0.88, batch loss = 0.81 (6.9 examples/sec; 1.153 sec/batch; 102h:29m:42s remains)
INFO - root - 2017-12-07 05:21:36.385542: step 12360, loss = 0.64, batch loss = 0.57 (6.5 examples/sec; 1.228 sec/batch; 109h:11m:55s remains)
INFO - root - 2017-12-07 05:21:47.930850: step 12370, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 1.154 sec/batch; 102h:39m:29s remains)
INFO - root - 2017-12-07 05:21:59.664365: step 12380, loss = 0.62, batch loss = 0.55 (6.8 examples/sec; 1.178 sec/batch; 104h:44m:32s remains)
INFO - root - 2017-12-07 05:22:11.405564: step 12390, loss = 0.97, batch loss = 0.90 (7.0 examples/sec; 1.149 sec/batch; 102h:10m:30s remains)
INFO - root - 2017-12-07 05:22:23.052798: step 12400, loss = 0.68, batch loss = 0.60 (7.1 examples/sec; 1.134 sec/batch; 100h:50m:18s remains)
2017-12-07 05:22:24.012029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5593059 -3.3115497 -3.1487992 -3.0627046 -3.0669808 -3.1626663 -3.2358246 -3.304637 -3.4448094 -3.606055 -3.7022474 -3.748713 -3.7498994 -3.6435449 -3.5162375][-2.9857736 -2.8042073 -2.7135944 -2.7014818 -2.7681561 -2.9375439 -3.0586891 -3.1294155 -3.2347832 -3.3488321 -3.3635118 -3.3616977 -3.3706169 -3.3192868 -3.2749767][-3.040066 -2.941637 -2.9127007 -2.9287746 -3.0002809 -3.1685886 -3.257153 -3.2566264 -3.2507293 -3.233088 -3.0804105 -2.9261341 -2.85458 -2.8412669 -2.9155421][-3.6159379 -3.5334997 -3.4934793 -3.4745677 -3.501636 -3.5945978 -3.6151869 -3.5432787 -3.452961 -3.3701327 -3.16246 -2.9434056 -2.8412628 -2.868252 -3.020596][-3.9099572 -3.7982824 -3.708441 -3.6335 -3.6283174 -3.6630764 -3.6176567 -3.4880466 -3.357914 -3.3126378 -3.2566621 -3.1940727 -3.2279062 -3.3745933 -3.5715551][-3.4859509 -3.3281658 -3.2050076 -3.1166625 -3.181994 -3.2785988 -3.2108955 -2.9713655 -2.7348876 -2.7029929 -2.8210309 -2.9779325 -3.1896386 -3.4687283 -3.678519][-2.4178154 -2.1636047 -1.9654176 -1.9083633 -2.111865 -2.3500273 -2.3103666 -1.9682555 -1.6185772 -1.549166 -1.7537494 -2.057194 -2.3797197 -2.721144 -2.8864508][-1.9867308 -1.6732678 -1.4183822 -1.3846369 -1.6406112 -1.8777707 -1.7676432 -1.2631681 -0.72255731 -0.47311544 -0.56459451 -0.80933022 -1.0693767 -1.3556964 -1.4654188][-2.8893883 -2.7603619 -2.6490347 -2.7041385 -2.9108219 -3.0081863 -2.8028231 -2.23814 -1.596169 -1.10977 -0.84921694 -0.7520237 -0.730983 -0.79925752 -0.81512666][-3.7338181 -3.8939998 -4.041338 -4.2472572 -4.4477677 -4.4768887 -4.3109326 -3.901865 -3.4154406 -2.9284639 -2.4296722 -2.0070684 -1.671592 -1.4419031 -1.3176999][-3.6531463 -3.9631882 -4.2591448 -4.5319958 -4.7349453 -4.804214 -4.7869177 -4.5644264 -4.2741075 -3.9652154 -3.5003357 -3.0253408 -2.6033318 -2.2237659 -1.9793532][-3.4537954 -3.7387028 -4.0401912 -4.3092709 -4.4830918 -4.5830121 -4.6587467 -4.5083413 -4.2429371 -3.9699221 -3.5862489 -3.2330737 -2.9246573 -2.55941 -2.2599852][-3.603869 -3.8445895 -4.1392493 -4.4181666 -4.6065731 -4.74329 -4.8542614 -4.7123203 -4.3655696 -3.9563379 -3.535439 -3.2028494 -2.9111946 -2.5423381 -2.2018178][-3.9665394 -4.1461182 -4.3591304 -4.5307159 -4.6377258 -4.7446737 -4.8857679 -4.8934541 -4.6969666 -4.3319869 -3.9606001 -3.6067433 -3.2559826 -2.82552 -2.370744][-4.3390374 -4.4665456 -4.5152183 -4.3972416 -4.2325416 -4.1184068 -4.1308393 -4.228025 -4.290134 -4.2448306 -4.2179618 -4.1045265 -3.9059103 -3.5648139 -3.0669465]]...]
INFO - root - 2017-12-07 05:22:35.662615: step 12410, loss = 0.89, batch loss = 0.82 (7.0 examples/sec; 1.139 sec/batch; 101h:15m:12s remains)
INFO - root - 2017-12-07 05:22:47.129933: step 12420, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 1.131 sec/batch; 100h:30m:52s remains)
INFO - root - 2017-12-07 05:22:58.763109: step 12430, loss = 0.71, batch loss = 0.64 (6.8 examples/sec; 1.174 sec/batch; 104h:20m:17s remains)
INFO - root - 2017-12-07 05:23:10.377156: step 12440, loss = 0.55, batch loss = 0.48 (7.1 examples/sec; 1.131 sec/batch; 100h:33m:24s remains)
INFO - root - 2017-12-07 05:23:21.974367: step 12450, loss = 0.91, batch loss = 0.83 (6.9 examples/sec; 1.167 sec/batch; 103h:42m:53s remains)
INFO - root - 2017-12-07 05:23:33.717940: step 12460, loss = 0.68, batch loss = 0.61 (6.4 examples/sec; 1.256 sec/batch; 111h:41m:24s remains)
INFO - root - 2017-12-07 05:23:45.407200: step 12470, loss = 0.58, batch loss = 0.51 (7.2 examples/sec; 1.118 sec/batch; 99h:21m:18s remains)
INFO - root - 2017-12-07 05:23:56.901164: step 12480, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.122 sec/batch; 99h:45m:39s remains)
INFO - root - 2017-12-07 05:24:08.666168: step 12490, loss = 0.78, batch loss = 0.70 (7.0 examples/sec; 1.143 sec/batch; 101h:34m:04s remains)
INFO - root - 2017-12-07 05:24:20.378086: step 12500, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.198 sec/batch; 106h:29m:07s remains)
2017-12-07 05:24:21.225184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.47626781 -0.2621851 -0.15093184 -0.32330513 -0.964664 -1.482883 -1.5673909 -1.1778119 -0.54038692 -0.36395836 -0.64985657 -1.3636389 -1.7962859 -1.7248156 -1.4204309][-0.055585384 0.14513254 0.22076178 -0.018361568 -0.6584177 -1.0578427 -1.0174592 -0.6261909 -0.087872982 -0.028039932 -0.45161748 -1.3242111 -1.8742986 -1.8134954 -1.4685328][0.22674322 0.55826759 0.73032856 0.534009 -0.038706779 -0.35209322 -0.30498457 -0.030381203 0.37019539 0.2971139 -0.32551956 -1.3330998 -1.9670894 -1.9136648 -1.5827787][0.47936106 0.96049356 1.2025847 1.0300288 0.47404051 0.1179657 0.081348419 0.22784662 0.52327347 0.33376265 -0.42192793 -1.4018064 -2.0548475 -2.0591264 -1.8524582][0.41443491 0.89451933 1.1518526 1.0627117 0.63315821 0.21818542 0.067840576 0.14798403 0.42658281 0.20626545 -0.56793857 -1.4480369 -2.1549468 -2.3342173 -2.2987759][-0.22048759 0.13131809 0.42840624 0.55154562 0.43309259 0.083772182 -0.074417114 0.084160805 0.47273636 0.3216939 -0.39976692 -1.2682266 -2.1539447 -2.5950997 -2.712657][-1.0485573 -0.80703378 -0.46637344 -0.12788439 0.10100651 -0.068351746 -0.090633869 0.29675388 0.92565918 0.94592857 0.27346134 -0.77950764 -1.9857337 -2.7034218 -2.9195995][-1.3738904 -1.2276986 -0.87471032 -0.49055815 -0.12939882 -0.17636681 -0.040154457 0.6112237 1.4525342 1.6374674 0.99934769 -0.27957964 -1.7802732 -2.7011008 -2.9517398][-0.97538877 -0.96838641 -0.70406675 -0.45208287 -0.18448925 -0.19909096 0.060403347 0.86393261 1.6642489 1.8033228 1.1909456 -0.16099644 -1.7225256 -2.6925471 -2.9142256][-0.37879467 -0.54753685 -0.40802383 -0.24177361 0.04299736 0.16380167 0.48840141 1.1698489 1.5363698 1.3075624 0.67695332 -0.54855967 -1.8805356 -2.7169094 -2.8742933][0.19481659 -0.11318207 -0.078792095 0.13407326 0.608881 0.91737843 1.1036639 1.3403349 1.0615654 0.37777328 -0.25469208 -1.2112358 -2.1700673 -2.7804403 -2.8743591][0.87478209 0.49951935 0.42702532 0.66476822 1.2848206 1.6254864 1.4104543 1.0649071 0.33377314 -0.50870919 -1.0501056 -1.7269795 -2.3334465 -2.7284832 -2.8185744][1.3506289 1.0155597 0.87417078 1.0157952 1.5340886 1.6621346 0.98989105 0.22540712 -0.60698915 -1.3135662 -1.6700292 -2.0546212 -2.3128977 -2.4919229 -2.603518][1.0567031 0.85188437 0.71754169 0.68157721 0.93366289 0.83797121 -0.045039177 -0.87766719 -1.5318687 -2.0095294 -2.1721141 -2.3089492 -2.2632763 -2.2027361 -2.2968972][-0.18197632 -0.22147274 -0.2823391 -0.41396952 -0.330873 -0.43775797 -1.0913587 -1.6521897 -2.0578089 -2.3719232 -2.4241428 -2.4673781 -2.340976 -2.1966634 -2.2670453]]...]
INFO - root - 2017-12-07 05:24:32.917634: step 12510, loss = 0.76, batch loss = 0.68 (7.2 examples/sec; 1.112 sec/batch; 98h:52m:18s remains)
INFO - root - 2017-12-07 05:24:44.736516: step 12520, loss = 0.78, batch loss = 0.71 (6.7 examples/sec; 1.190 sec/batch; 105h:46m:47s remains)
INFO - root - 2017-12-07 05:24:56.467733: step 12530, loss = 0.67, batch loss = 0.60 (6.7 examples/sec; 1.203 sec/batch; 106h:55m:23s remains)
INFO - root - 2017-12-07 05:25:08.057512: step 12540, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 1.130 sec/batch; 100h:26m:13s remains)
INFO - root - 2017-12-07 05:25:19.869179: step 12550, loss = 0.91, batch loss = 0.84 (6.5 examples/sec; 1.236 sec/batch; 109h:51m:24s remains)
INFO - root - 2017-12-07 05:25:31.601978: step 12560, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 1.148 sec/batch; 102h:02m:59s remains)
INFO - root - 2017-12-07 05:25:43.286670: step 12570, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 1.143 sec/batch; 101h:35m:37s remains)
INFO - root - 2017-12-07 05:25:54.927235: step 12580, loss = 0.73, batch loss = 0.66 (7.3 examples/sec; 1.103 sec/batch; 97h:59m:06s remains)
INFO - root - 2017-12-07 05:26:06.625454: step 12590, loss = 0.70, batch loss = 0.63 (7.2 examples/sec; 1.113 sec/batch; 98h:51m:44s remains)
INFO - root - 2017-12-07 05:26:18.190160: step 12600, loss = 0.71, batch loss = 0.63 (6.8 examples/sec; 1.180 sec/batch; 104h:53m:24s remains)
2017-12-07 05:26:19.079952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5600586 -2.1961887 -1.7325423 -1.4716175 -1.6508665 -2.1115592 -2.6085603 -2.8701587 -2.6079881 -1.6796687 -0.75723314 -0.86539912 -1.7627208 -2.4001889 -2.6185703][-3.0853133 -2.5754819 -1.9087229 -1.5394955 -1.656812 -2.0554113 -2.5604076 -2.8969238 -2.6548233 -1.6990335 -0.7535603 -0.87639475 -1.7832336 -2.38818 -2.6470063][-3.072547 -2.7813029 -2.1395237 -1.6465344 -1.4405758 -1.384867 -1.6231375 -2.0169806 -1.9747977 -1.2645094 -0.59335446 -0.88057923 -1.7858467 -2.2935705 -2.5374784][-2.987819 -3.0951962 -2.6335764 -2.00634 -1.3086112 -0.66310477 -0.56539369 -1.0096917 -1.2429659 -0.94173884 -0.7252295 -1.2215354 -2.0395346 -2.3375733 -2.4685977][-3.2226381 -3.5767744 -3.1913486 -2.4388628 -1.4585495 -0.60498643 -0.44120073 -0.9045825 -1.2996008 -1.3294473 -1.4389467 -1.9707441 -2.5805717 -2.6359816 -2.6148181][-3.4599793 -3.8357127 -3.3280954 -2.3897195 -1.4074085 -0.73091745 -0.70697761 -1.1608942 -1.610287 -1.7960954 -2.0105667 -2.4632313 -2.9461913 -2.9250293 -2.7942214][-3.1850564 -3.5076146 -2.8763013 -1.8812532 -1.0809762 -0.60210371 -0.59295225 -0.91606951 -1.3160262 -1.6021168 -1.8988545 -2.4014707 -2.9990454 -3.1084867 -2.9300971][-2.6800447 -2.9515939 -2.382664 -1.5618963 -0.9657023 -0.53296995 -0.35799932 -0.49438286 -0.817219 -1.1721227 -1.5037529 -2.0895193 -2.8828974 -3.1846137 -3.0084715][-2.7470174 -2.9362001 -2.5040131 -1.9349406 -1.4741793 -1.0551283 -0.83051777 -0.90937805 -1.1306684 -1.3011262 -1.3851171 -1.8300731 -2.685472 -3.1477659 -3.0501609][-3.1290698 -3.2874629 -2.9807992 -2.5830007 -2.1937418 -1.8222439 -1.6691518 -1.7418258 -1.8109059 -1.6966782 -1.4859211 -1.7531655 -2.6068392 -3.2028604 -3.2346611][-3.2780831 -3.4615974 -3.2818596 -2.9922585 -2.6225319 -2.2665803 -2.1721282 -2.2268081 -2.196177 -1.9537127 -1.6709728 -1.9022384 -2.7420158 -3.3933072 -3.5298564][-3.5091612 -3.6384077 -3.5036323 -3.2571545 -2.9155102 -2.6020477 -2.5527291 -2.6240878 -2.6059384 -2.4053349 -2.2059476 -2.455795 -3.1635416 -3.6734827 -3.7804716][-3.6838934 -3.7460959 -3.6221995 -3.3988447 -3.1280513 -2.9215271 -2.9131947 -2.9670379 -2.9596839 -2.86032 -2.7815084 -3.0004473 -3.4876547 -3.7949028 -3.8391905][-3.529083 -3.5412321 -3.456008 -3.3267808 -3.1804442 -3.0897026 -3.0893428 -3.0833955 -3.0338461 -2.9717677 -2.9742596 -3.1569533 -3.4698429 -3.6567612 -3.6966119][-3.5310142 -3.5413237 -3.514586 -3.4672508 -3.4116468 -3.3843818 -3.3892286 -3.373702 -3.3330543 -3.2925134 -3.3023646 -3.4004617 -3.5475597 -3.6143277 -3.5920844]]...]
INFO - root - 2017-12-07 05:26:30.657581: step 12610, loss = 0.66, batch loss = 0.59 (7.1 examples/sec; 1.130 sec/batch; 100h:22m:08s remains)
INFO - root - 2017-12-07 05:26:42.479733: step 12620, loss = 0.63, batch loss = 0.56 (6.7 examples/sec; 1.200 sec/batch; 106h:37m:58s remains)
INFO - root - 2017-12-07 05:26:54.271212: step 12630, loss = 0.66, batch loss = 0.59 (6.5 examples/sec; 1.224 sec/batch; 108h:43m:07s remains)
INFO - root - 2017-12-07 05:27:05.978643: step 12640, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 1.137 sec/batch; 100h:59m:09s remains)
INFO - root - 2017-12-07 05:27:17.695715: step 12650, loss = 0.80, batch loss = 0.72 (7.1 examples/sec; 1.134 sec/batch; 100h:46m:55s remains)
INFO - root - 2017-12-07 05:27:29.308957: step 12660, loss = 0.83, batch loss = 0.76 (7.0 examples/sec; 1.142 sec/batch; 101h:27m:21s remains)
INFO - root - 2017-12-07 05:27:41.069292: step 12670, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.195 sec/batch; 106h:09m:29s remains)
INFO - root - 2017-12-07 05:27:52.660680: step 12680, loss = 0.92, batch loss = 0.85 (6.4 examples/sec; 1.244 sec/batch; 110h:30m:11s remains)
INFO - root - 2017-12-07 05:28:04.343977: step 12690, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 1.155 sec/batch; 102h:39m:00s remains)
INFO - root - 2017-12-07 05:28:16.056023: step 12700, loss = 0.90, batch loss = 0.82 (7.1 examples/sec; 1.134 sec/batch; 100h:46m:20s remains)
2017-12-07 05:28:16.957232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2876496 -4.3758192 -4.45776 -4.5858788 -4.6292014 -4.6272049 -4.6309681 -4.63285 -4.6652803 -4.4623017 -3.9245558 -3.1995239 -2.5661714 -2.4468973 -2.6347518][-4.46873 -4.4918966 -4.5499616 -4.7130213 -4.8387651 -4.9019127 -4.9307756 -4.9158459 -4.841218 -4.4681468 -3.7686989 -3.0192888 -2.4760222 -2.4305143 -2.6652334][-4.3614025 -4.2859478 -4.3279157 -4.5399122 -4.74241 -4.8288016 -4.8354411 -4.74673 -4.5679793 -4.1254382 -3.4501398 -2.8920107 -2.587543 -2.6288505 -2.85163][-4.341403 -4.2355685 -4.2449622 -4.39756 -4.5154471 -4.4843712 -4.4273343 -4.3058429 -4.1193652 -3.7092621 -3.1381145 -2.7515872 -2.6259155 -2.7179632 -2.9324636][-4.2826405 -4.1396613 -4.0360947 -4.0361872 -3.9961634 -3.8455429 -3.7928545 -3.7239642 -3.5677176 -3.1663878 -2.6060357 -2.2175376 -2.1308415 -2.2359695 -2.5020883][-3.8986363 -3.6984603 -3.4995952 -3.4250851 -3.3053732 -3.0535052 -2.8868437 -2.749604 -2.5573242 -2.1765711 -1.7065797 -1.3802359 -1.3386452 -1.4602776 -1.7623305][-3.2276921 -2.9267888 -2.7074118 -2.5755939 -2.3101673 -1.8056123 -1.2866943 -1.0213561 -0.96755004 -0.85820794 -0.75460768 -0.68558407 -0.747406 -0.84841275 -1.0503097][-2.3384657 -1.9492064 -1.6813271 -1.3839123 -0.86677432 -0.10414743 0.67552233 0.75248337 0.23994255 -0.19759893 -0.54248214 -0.68514872 -0.77176142 -0.76462173 -0.7388761][-1.4792516 -1.0441101 -0.75259542 -0.3427434 0.23255634 0.88163948 1.3951564 0.94395781 -0.095153332 -0.89337277 -1.3810475 -1.4921019 -1.48564 -1.3738141 -1.2111671][-1.1160276 -0.78715897 -0.67646146 -0.48070574 -0.25077677 -0.065018654 0.0017137527 -0.60319066 -1.6363952 -2.4014034 -2.7762539 -2.7974405 -2.6992564 -2.5401034 -2.4099612][-1.6310234 -1.58201 -1.7371628 -1.7885325 -1.8354037 -1.8843827 -1.9356062 -2.3798513 -3.1531377 -3.7535834 -4.0079536 -4.0114031 -3.8698525 -3.6808619 -3.6004994][-2.6011152 -2.7861114 -3.0063257 -3.0444121 -3.0702248 -3.095119 -3.1021028 -3.3570428 -3.8890443 -4.3463907 -4.521894 -4.5025411 -4.2960978 -4.0722475 -4.0238495][-3.2370002 -3.473542 -3.5813482 -3.472765 -3.3958945 -3.3669944 -3.3546267 -3.46868 -3.7750287 -4.0553031 -4.1213765 -4.0219374 -3.7856486 -3.6154113 -3.6728094][-3.2330544 -3.3481951 -3.2963862 -3.119668 -3.0207608 -2.9751 -2.9131961 -2.8656845 -2.940609 -3.0487463 -3.0578942 -2.9607997 -2.8188076 -2.8068237 -2.9841354][-3.1891012 -3.2208896 -3.114717 -2.9620738 -2.8872204 -2.8310742 -2.7142615 -2.5626912 -2.4970851 -2.4999979 -2.5077481 -2.4925895 -2.4764984 -2.5574646 -2.7405567]]...]
INFO - root - 2017-12-07 05:28:28.511317: step 12710, loss = 0.59, batch loss = 0.52 (7.1 examples/sec; 1.129 sec/batch; 100h:18m:09s remains)
INFO - root - 2017-12-07 05:28:39.961266: step 12720, loss = 0.87, batch loss = 0.80 (7.8 examples/sec; 1.028 sec/batch; 91h:18m:26s remains)
INFO - root - 2017-12-07 05:28:51.666431: step 12730, loss = 0.68, batch loss = 0.61 (6.6 examples/sec; 1.216 sec/batch; 108h:02m:44s remains)
INFO - root - 2017-12-07 05:29:03.351769: step 12740, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.122 sec/batch; 99h:38m:25s remains)
INFO - root - 2017-12-07 05:29:15.047830: step 12750, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 1.152 sec/batch; 102h:21m:39s remains)
INFO - root - 2017-12-07 05:29:26.608017: step 12760, loss = 1.14, batch loss = 1.07 (6.9 examples/sec; 1.160 sec/batch; 103h:02m:53s remains)
INFO - root - 2017-12-07 05:29:38.396753: step 12770, loss = 0.68, batch loss = 0.61 (6.6 examples/sec; 1.209 sec/batch; 107h:24m:37s remains)
INFO - root - 2017-12-07 05:29:50.017782: step 12780, loss = 0.63, batch loss = 0.56 (6.7 examples/sec; 1.194 sec/batch; 106h:04m:37s remains)
INFO - root - 2017-12-07 05:30:01.694911: step 12790, loss = 1.04, batch loss = 0.96 (6.9 examples/sec; 1.154 sec/batch; 102h:27m:39s remains)
INFO - root - 2017-12-07 05:30:13.393229: step 12800, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 1.168 sec/batch; 103h:42m:24s remains)
2017-12-07 05:30:14.201466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4449582 -2.8387399 -3.1171494 -3.2554564 -3.080497 -2.916255 -2.7011 -2.3862457 -2.1265984 -2.0381384 -2.4853673 -2.8709655 -2.514205 -2.1884243 -2.4640236][-2.2924321 -2.585928 -2.8632135 -3.0890579 -2.895329 -2.714736 -2.4968364 -2.0967591 -1.7723777 -1.7699661 -2.3760343 -2.6932139 -2.0442457 -1.5757515 -1.9891829][-1.9412436 -2.0888045 -2.4478579 -2.8898449 -2.7985733 -2.6695848 -2.4098258 -1.7904904 -1.3208184 -1.3882258 -2.076725 -2.2679915 -1.3254888 -0.75957012 -1.416513][-1.5948484 -1.5728033 -2.0736287 -2.8080692 -2.9425521 -2.9087248 -2.4853115 -1.5266786 -0.97005105 -1.1968708 -1.9188337 -1.980449 -0.78574204 -0.12982464 -1.0691373][-1.5049846 -1.2821312 -1.8728962 -2.8378925 -3.1808357 -3.1920424 -2.5017314 -1.187721 -0.68846893 -1.1793792 -1.9128397 -1.8405285 -0.44044876 0.28397894 -0.89616394][-1.5777612 -1.1300292 -1.6954191 -2.717196 -3.1825795 -3.2494807 -2.3235857 -0.77592778 -0.51338267 -1.3161671 -2.0033062 -1.7841804 -0.2504611 0.5207 -0.81025267][-1.6368382 -1.0106511 -1.4734159 -2.4191351 -2.9241228 -3.0169654 -1.9562452 -0.41169262 -0.49105048 -1.5057185 -2.0520484 -1.7268341 -0.16883135 0.61855125 -0.80490589][-1.8241704 -1.1305406 -1.440273 -2.2266681 -2.7228532 -2.8040991 -1.7228415 -0.44496131 -0.89582014 -1.9029558 -2.1522355 -1.7092416 -0.22086525 0.54885244 -0.95485377][-2.2039242 -1.5329206 -1.6571629 -2.2068911 -2.647923 -2.6942902 -1.7281151 -0.86231112 -1.5839827 -2.4250257 -2.3896208 -1.8746862 -0.52559495 0.17032623 -1.3292427][-2.5294595 -1.9614699 -1.9238853 -2.2321451 -2.5533676 -2.5562098 -1.818234 -1.3373973 -2.058573 -2.6002235 -2.3978977 -1.9104226 -0.80259204 -0.29691172 -1.6797657][-2.7052703 -2.2702465 -2.1607058 -2.28676 -2.4728084 -2.4472265 -1.9670775 -1.7552552 -2.2567918 -2.4801395 -2.2366989 -1.8526669 -1.0300832 -0.75302744 -1.9264605][-2.7996616 -2.5102623 -2.4045053 -2.4526055 -2.5637434 -2.5303173 -2.276346 -2.2139895 -2.4383318 -2.4368064 -2.2593632 -2.0136595 -1.449199 -1.326211 -2.2070837][-2.8433709 -2.6744814 -2.6173491 -2.6476605 -2.7295008 -2.7031841 -2.5979574 -2.6200261 -2.6569657 -2.5458088 -2.4470222 -2.3134215 -1.961906 -1.9170213 -2.4967546][-2.8059502 -2.7290761 -2.7232494 -2.7529516 -2.8074818 -2.7783804 -2.7509298 -2.8018663 -2.753511 -2.6272962 -2.5815353 -2.5282333 -2.354569 -2.3622506 -2.6924274][-2.7423904 -2.7182293 -2.7350864 -2.7582998 -2.7827325 -2.7470312 -2.7400928 -2.7842994 -2.7294898 -2.6486018 -2.62988 -2.6117887 -2.548924 -2.5762699 -2.7448618]]...]
INFO - root - 2017-12-07 05:30:25.808320: step 12810, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.193 sec/batch; 105h:56m:51s remains)
INFO - root - 2017-12-07 05:30:37.619760: step 12820, loss = 0.79, batch loss = 0.72 (6.7 examples/sec; 1.190 sec/batch; 105h:39m:09s remains)
INFO - root - 2017-12-07 05:30:49.396716: step 12830, loss = 0.71, batch loss = 0.64 (6.8 examples/sec; 1.182 sec/batch; 104h:56m:19s remains)
INFO - root - 2017-12-07 05:31:01.034697: step 12840, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 1.152 sec/batch; 102h:19m:47s remains)
INFO - root - 2017-12-07 05:31:12.625897: step 12850, loss = 0.96, batch loss = 0.89 (7.1 examples/sec; 1.133 sec/batch; 100h:38m:34s remains)
INFO - root - 2017-12-07 05:31:24.297754: step 12860, loss = 0.55, batch loss = 0.48 (7.2 examples/sec; 1.113 sec/batch; 98h:50m:37s remains)
INFO - root - 2017-12-07 05:31:36.037682: step 12870, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.178 sec/batch; 104h:36m:11s remains)
INFO - root - 2017-12-07 05:31:47.716121: step 12880, loss = 0.77, batch loss = 0.70 (6.5 examples/sec; 1.236 sec/batch; 109h:43m:34s remains)
INFO - root - 2017-12-07 05:31:59.364965: step 12890, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.139 sec/batch; 101h:07m:15s remains)
INFO - root - 2017-12-07 05:32:11.059525: step 12900, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.131 sec/batch; 100h:26m:21s remains)
2017-12-07 05:32:11.951814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3132575 -2.46378 -2.7530422 -3.0370717 -3.2047291 -3.3381939 -3.4276729 -3.4819145 -3.4066744 -3.1843598 -2.9706764 -2.7647858 -2.694005 -2.812727 -2.9444828][-2.2218316 -2.4471874 -2.9234223 -3.3825731 -3.6404786 -3.7617366 -3.7717583 -3.7634418 -3.6851115 -3.4901352 -3.3422065 -3.226748 -3.2822127 -3.5313423 -3.7702067][-1.9361234 -2.2060697 -2.9498432 -3.6321123 -3.9688251 -4.029686 -3.9286427 -3.8883557 -3.9219568 -3.9146512 -3.9310315 -3.9150975 -4.0310926 -4.3149543 -4.5272546][-1.617759 -1.8663657 -2.9166508 -3.8780694 -4.2659869 -4.1811271 -3.9026489 -3.8260489 -4.0179248 -4.2441206 -4.3893104 -4.3616762 -4.4189839 -4.6337776 -4.7356133][-1.3624346 -1.4353805 -2.6994958 -3.9485004 -4.3273849 -3.9596279 -3.3626642 -3.21138 -3.6219645 -4.1137342 -4.3993616 -4.3851271 -4.406044 -4.5321131 -4.515625][-0.96377945 -0.82625484 -2.2033124 -3.6553609 -3.9282892 -3.1357715 -2.070349 -1.8415251 -2.5944118 -3.4782948 -4.0162086 -4.1436014 -4.2336593 -4.2909188 -4.121521][-0.67933178 -0.48493838 -1.9611444 -3.5041649 -3.5723937 -2.2795265 -0.673095 -0.37816429 -1.6127181 -2.9964697 -3.8238804 -4.08399 -4.2189736 -4.1395278 -3.7216136][-0.76437759 -0.671931 -2.1815364 -3.6516254 -3.4840398 -1.7630181 0.29256058 0.65162277 -1.006511 -2.811152 -3.8147137 -4.1036558 -4.2355909 -4.0927873 -3.563879][-1.0300105 -1.1156907 -2.5617223 -3.8703089 -3.5782886 -1.6879871 0.536788 0.95648146 -0.7935729 -2.6810811 -3.6437724 -3.8562233 -3.995626 -4.008472 -3.6845071][-1.5988188 -1.8603435 -3.08668 -4.09869 -3.753716 -2.00079 -0.0080332756 0.37567091 -1.1246555 -2.7799644 -3.5721323 -3.6808829 -3.77607 -3.893795 -3.7918916][-2.4757791 -2.8276851 -3.6831048 -4.2422028 -3.7971191 -2.3458006 -0.85531878 -0.61688542 -1.7236373 -2.9690931 -3.5658145 -3.6677194 -3.7576132 -3.874764 -3.8630083][-2.9833431 -3.2765326 -3.7954466 -3.9853251 -3.5007467 -2.4595118 -1.5977545 -1.5765371 -2.2840889 -2.9901106 -3.2721007 -3.2990787 -3.3253703 -3.3733444 -3.4148159][-3.0006287 -3.1685176 -3.5063376 -3.5444093 -3.1246426 -2.5008802 -2.1920629 -2.3663747 -2.749433 -3.002764 -3.061182 -3.0978689 -3.1386223 -3.1478696 -3.16192][-3.0027137 -3.0706289 -3.3680282 -3.4145555 -3.1070991 -2.7504392 -2.6842871 -2.845881 -2.9342415 -2.8907123 -2.8557882 -2.9870579 -3.1745424 -3.3041551 -3.3506][-3.098702 -3.1074104 -3.4703953 -3.6362586 -3.4882565 -3.2890463 -3.2331605 -3.2408092 -3.0984795 -2.9086709 -2.8470216 -2.9955287 -3.2108514 -3.3717523 -3.4397392]]...]
INFO - root - 2017-12-07 05:32:23.458309: step 12910, loss = 0.99, batch loss = 0.92 (6.6 examples/sec; 1.216 sec/batch; 107h:54m:53s remains)
INFO - root - 2017-12-07 05:32:34.844605: step 12920, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 1.135 sec/batch; 100h:43m:51s remains)
INFO - root - 2017-12-07 05:32:46.633361: step 12930, loss = 0.93, batch loss = 0.86 (6.8 examples/sec; 1.180 sec/batch; 104h:43m:00s remains)
INFO - root - 2017-12-07 05:32:58.282506: step 12940, loss = 0.65, batch loss = 0.58 (6.9 examples/sec; 1.165 sec/batch; 103h:27m:12s remains)
INFO - root - 2017-12-07 05:33:09.962627: step 12950, loss = 0.83, batch loss = 0.76 (6.6 examples/sec; 1.216 sec/batch; 107h:55m:07s remains)
INFO - root - 2017-12-07 05:33:21.687857: step 12960, loss = 0.99, batch loss = 0.92 (6.8 examples/sec; 1.179 sec/batch; 104h:37m:14s remains)
INFO - root - 2017-12-07 05:33:33.141763: step 12970, loss = 0.69, batch loss = 0.62 (7.2 examples/sec; 1.115 sec/batch; 98h:58m:02s remains)
INFO - root - 2017-12-07 05:33:44.825724: step 12980, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 1.157 sec/batch; 102h:42m:27s remains)
INFO - root - 2017-12-07 05:33:56.278354: step 12990, loss = 0.93, batch loss = 0.86 (7.3 examples/sec; 1.099 sec/batch; 97h:32m:34s remains)
INFO - root - 2017-12-07 05:34:08.065167: step 13000, loss = 0.93, batch loss = 0.86 (6.8 examples/sec; 1.168 sec/batch; 103h:40m:24s remains)
2017-12-07 05:34:09.008671: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46273518 0.40615225 0.35443115 0.32196665 0.29613209 0.30665302 0.33591461 0.36637688 0.42700148 0.49870729 0.52978516 0.53917122 0.51781845 0.47745085 0.47258759][0.52026224 0.47451019 0.41727829 0.36822319 0.31590509 0.29765415 0.30204773 0.29901743 0.32601166 0.38272142 0.40041018 0.41299486 0.41364861 0.39261198 0.42842531][0.37666655 0.36589813 0.32517767 0.27786303 0.20988655 0.1661272 0.14520693 0.098663807 0.071016312 0.10238266 0.10556746 0.13344049 0.1870389 0.21507978 0.32300758][0.17894506 0.21135664 0.195827 0.15554667 0.079346657 0.012191772 -0.025919914 -0.10040808 -0.16757631 -0.14337397 -0.13748837 -0.077572346 0.053880215 0.13802242 0.30120277][-0.13805628 -0.09294939 -0.077867031 -0.066905975 -0.076994896 -0.077788353 -0.044931889 -0.084229469 -0.173841 -0.18988085 -0.23631954 -0.19864082 -0.0189085 0.099978447 0.28837347][-0.41314983 -0.33375788 -0.24142313 -0.095824242 0.0719285 0.2651186 0.49056816 0.56932974 0.45670891 0.31021261 0.05909729 -0.094637871 -0.018155575 0.03963089 0.19875813][-0.86619544 -0.65681481 -0.41434097 -0.090738773 0.2476368 0.59200764 0.97060061 1.1536036 1.0433779 0.79440689 0.33676529 -0.034837723 -0.10013914 -0.10477734 0.041112423][-1.5330808 -1.2828467 -0.99759221 -0.61999035 -0.27450323 0.016149521 0.3380146 0.48708677 0.40565491 0.22099781 -0.18767691 -0.51714873 -0.530241 -0.44610476 -0.19654322][-1.867373 -1.7404556 -1.6362352 -1.4201844 -1.1975315 -0.9953649 -0.72822976 -0.57180834 -0.54557872 -0.55373311 -0.77805042 -0.95332122 -0.86105371 -0.6899035 -0.36930275][-2.5820189 -2.4677346 -2.4227369 -2.2607522 -2.0091989 -1.6810143 -1.2575142 -0.931968 -0.72678638 -0.57520509 -0.6582849 -0.76046443 -0.67422462 -0.5412364 -0.2818985][-3.3193207 -3.1117818 -2.9942615 -2.7735941 -2.4176304 -1.942569 -1.3581133 -0.8714447 -0.53626418 -0.30566406 -0.32123375 -0.41996241 -0.39146376 -0.32007647 -0.15039158][-2.9847717 -2.7917733 -2.7828245 -2.698174 -2.472075 -2.1144862 -1.5745368 -1.0384328 -0.62342334 -0.32815456 -0.25410271 -0.318254 -0.31228828 -0.26628637 -0.1611495][-2.5586233 -2.4591942 -2.5864408 -2.6208565 -2.5383348 -2.3739731 -1.9350061 -1.3733649 -0.87829685 -0.49972963 -0.31844664 -0.329597 -0.33291054 -0.30017614 -0.21372938][-2.9481981 -2.9060144 -3.0141582 -2.946161 -2.8061101 -2.6611516 -2.2229569 -1.626317 -1.1001594 -0.689306 -0.46341825 -0.45574403 -0.46845484 -0.4540503 -0.3465004][-3.0298109 -2.9986656 -3.0932369 -2.9449868 -2.739913 -2.5515876 -2.1042123 -1.5341136 -1.0331805 -0.61947083 -0.38011503 -0.37249041 -0.41225195 -0.47120476 -0.39880562]]...]
INFO - root - 2017-12-07 05:34:20.749485: step 13010, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 1.147 sec/batch; 101h:46m:29s remains)
INFO - root - 2017-12-07 05:34:32.421194: step 13020, loss = 0.67, batch loss = 0.59 (6.7 examples/sec; 1.188 sec/batch; 105h:23m:28s remains)
INFO - root - 2017-12-07 05:34:43.917251: step 13030, loss = 0.68, batch loss = 0.61 (6.5 examples/sec; 1.226 sec/batch; 108h:48m:30s remains)
INFO - root - 2017-12-07 05:34:55.574079: step 13040, loss = 0.93, batch loss = 0.86 (6.5 examples/sec; 1.229 sec/batch; 109h:05m:45s remains)
INFO - root - 2017-12-07 05:35:07.225574: step 13050, loss = 0.75, batch loss = 0.67 (6.5 examples/sec; 1.235 sec/batch; 109h:36m:01s remains)
INFO - root - 2017-12-07 05:35:18.804920: step 13060, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 1.125 sec/batch; 99h:48m:06s remains)
INFO - root - 2017-12-07 05:35:30.550361: step 13070, loss = 0.63, batch loss = 0.56 (7.0 examples/sec; 1.138 sec/batch; 100h:59m:11s remains)
INFO - root - 2017-12-07 05:35:42.260256: step 13080, loss = 0.60, batch loss = 0.52 (6.9 examples/sec; 1.165 sec/batch; 103h:21m:07s remains)
INFO - root - 2017-12-07 05:35:53.756017: step 13090, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.165 sec/batch; 103h:24m:22s remains)
INFO - root - 2017-12-07 05:36:05.392143: step 13100, loss = 0.87, batch loss = 0.79 (6.6 examples/sec; 1.213 sec/batch; 107h:38m:53s remains)
2017-12-07 05:36:06.261488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5963306 -3.5489817 -3.5188091 -3.51087 -3.6216052 -3.7600331 -3.7654657 -3.8045073 -3.8854787 -3.9005585 -3.9870863 -4.1243062 -4.1482973 -4.1162577 -4.001008][-3.5928705 -3.5515633 -3.521172 -3.4664044 -3.567625 -3.7761359 -3.8435173 -3.9142318 -3.9591794 -3.8474436 -3.8784056 -4.110055 -4.2096863 -4.1802807 -3.9191666][-3.5592124 -3.5509336 -3.497997 -3.2983832 -3.2743177 -3.4431078 -3.5650158 -3.6954708 -3.750504 -3.5722511 -3.5751207 -3.8156216 -3.9093642 -3.8657105 -3.4949603][-3.434557 -3.4273129 -3.2756848 -2.8654466 -2.6470366 -2.7565796 -3.0213704 -3.4035029 -3.6128678 -3.3926821 -3.2083569 -3.210058 -3.1629152 -3.1471081 -2.9379559][-3.1892636 -3.1063452 -2.7962537 -2.1928973 -1.8171203 -1.8734028 -2.2592826 -2.8745551 -3.2832575 -3.1517487 -2.8824511 -2.6706421 -2.5386229 -2.6074014 -2.6641803][-2.9004006 -2.7181206 -2.3004019 -1.6007011 -1.1284502 -1.0897446 -1.4619403 -2.1772118 -2.7683794 -2.9307923 -2.9079537 -2.7859015 -2.7522287 -2.8683271 -2.9821348][-2.6517394 -2.4369311 -1.9974704 -1.2622936 -0.6434958 -0.36810589 -0.54008532 -1.2339969 -2.0397935 -2.6464403 -3.0172939 -3.0707858 -3.1200786 -3.1833115 -3.1637688][-2.5631013 -2.3771448 -1.889519 -1.0551927 -0.24150515 0.29694414 0.33810043 -0.3823905 -1.4759812 -2.5521107 -3.2771392 -3.438324 -3.4894443 -3.3778415 -3.0999994][-2.6182098 -2.4260004 -1.8858924 -1.0904007 -0.26889658 0.35207272 0.46657133 -0.22937965 -1.3499436 -2.5467443 -3.4019334 -3.69633 -3.8130023 -3.6348743 -3.2025881][-2.4687648 -2.2726302 -1.8023949 -1.2235568 -0.57616472 -0.014292717 0.13008642 -0.33304977 -1.1045539 -2.0226855 -2.8150129 -3.3050671 -3.6285377 -3.58569 -3.2838283][-2.2240963 -2.0967064 -1.8392639 -1.5301585 -1.0804925 -0.5676434 -0.34259272 -0.48652911 -0.87502789 -1.5168445 -2.276751 -2.937829 -3.4222643 -3.5084648 -3.3782225][-2.2139921 -2.1628752 -2.0898504 -2.0115552 -1.7972689 -1.4720159 -1.3207591 -1.3311276 -1.5630236 -2.0416541 -2.6412013 -3.1834841 -3.5713091 -3.6621273 -3.6248939][-2.3585155 -2.2728255 -2.2530081 -2.3544066 -2.4182153 -2.3892145 -2.411644 -2.4621763 -2.6290412 -2.8937874 -3.1476824 -3.3408537 -3.5022125 -3.5720475 -3.6496563][-2.4959071 -2.3131361 -2.3008864 -2.4971995 -2.7237928 -2.8844943 -2.971066 -2.9905674 -3.0490022 -3.1073093 -3.1153035 -3.1293285 -3.2022 -3.3040733 -3.4683437][-2.8424354 -2.6616154 -2.6487184 -2.8261037 -3.045629 -3.2159481 -3.287467 -3.2690148 -3.2635713 -3.2645459 -3.2911122 -3.3524828 -3.4181924 -3.4875293 -3.5763962]]...]
INFO - root - 2017-12-07 05:36:17.820363: step 13110, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 1.129 sec/batch; 100h:12m:01s remains)
INFO - root - 2017-12-07 05:36:29.480961: step 13120, loss = 0.70, batch loss = 0.62 (6.9 examples/sec; 1.153 sec/batch; 102h:17m:03s remains)
INFO - root - 2017-12-07 05:36:41.219630: step 13130, loss = 0.70, batch loss = 0.63 (6.6 examples/sec; 1.221 sec/batch; 108h:18m:08s remains)
INFO - root - 2017-12-07 05:36:52.876072: step 13140, loss = 0.93, batch loss = 0.86 (6.6 examples/sec; 1.205 sec/batch; 106h:55m:44s remains)
INFO - root - 2017-12-07 05:37:04.405580: step 13150, loss = 0.90, batch loss = 0.82 (7.0 examples/sec; 1.137 sec/batch; 100h:51m:35s remains)
INFO - root - 2017-12-07 05:37:16.064128: step 13160, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 1.131 sec/batch; 100h:21m:56s remains)
INFO - root - 2017-12-07 05:37:27.718042: step 13170, loss = 0.98, batch loss = 0.90 (6.9 examples/sec; 1.161 sec/batch; 103h:01m:20s remains)
INFO - root - 2017-12-07 05:37:39.359742: step 13180, loss = 0.91, batch loss = 0.84 (6.9 examples/sec; 1.166 sec/batch; 103h:27m:41s remains)
INFO - root - 2017-12-07 05:37:51.196421: step 13190, loss = 0.68, batch loss = 0.61 (6.7 examples/sec; 1.200 sec/batch; 106h:25m:35s remains)
INFO - root - 2017-12-07 05:38:02.998210: step 13200, loss = 0.81, batch loss = 0.74 (6.8 examples/sec; 1.182 sec/batch; 104h:51m:52s remains)
2017-12-07 05:38:03.821015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1651058 -3.8508539 -3.4831338 -3.237618 -3.2295222 -3.1109416 -2.586158 -2.1345963 -2.1462455 -2.4721518 -2.7689185 -2.9789882 -3.1888037 -3.4559889 -3.7331483][-3.8888748 -3.4235787 -2.9948304 -2.8211451 -2.8776207 -2.5900416 -1.7655168 -1.1146486 -1.0638523 -1.5216966 -2.0359271 -2.4083591 -2.7444053 -3.0391443 -3.2779055][-3.5597231 -3.0299423 -2.6442754 -2.5947289 -2.6334672 -2.0702527 -0.94839811 -0.13009787 -0.013631821 -0.6305542 -1.3616271 -1.8876143 -2.3586075 -2.6712465 -2.8883443][-3.3419013 -2.8400812 -2.5546489 -2.6290913 -2.5740967 -1.6662164 -0.19998884 0.87574816 0.99292278 0.00881815 -0.98776484 -1.5914993 -2.0885627 -2.3632326 -2.5693212][-3.2507973 -2.8772125 -2.7210402 -2.8601928 -2.6556771 -1.4274094 0.48128796 2.0152197 1.9844546 0.36724997 -0.90657353 -1.4984777 -1.9673419 -2.230906 -2.4852245][-3.3261845 -3.1161458 -3.0466251 -3.1473002 -2.7588019 -1.2872462 1.1428056 3.2094054 2.7231522 0.31839228 -1.1042104 -1.5265739 -1.9393108 -2.2517838 -2.6343522][-3.6304946 -3.543335 -3.4833899 -3.4928565 -2.9996514 -1.3902431 1.6347423 4.1731234 2.8688741 -0.32936144 -1.6871886 -1.7394462 -1.9827688 -2.3281789 -2.8840871][-4.102951 -4.0472269 -3.9648857 -3.9394195 -3.4485335 -1.7705159 1.7682185 4.4670649 2.3313932 -1.3126643 -2.4342732 -2.0190988 -1.9924049 -2.3261852 -3.058639][-4.4538484 -4.3599658 -4.2617841 -4.2832227 -3.8757052 -2.1885598 1.5749483 4.0649786 1.5189462 -2.0769451 -2.8810964 -2.064409 -1.8005388 -2.1159935 -2.9922972][-4.50741 -4.3714194 -4.2827444 -4.4035187 -4.1412687 -2.5142245 1.1200638 3.1574893 0.75763607 -2.30888 -2.7347195 -1.7220008 -1.4112058 -1.7733266 -2.7236958][-4.2460294 -4.113328 -4.0612755 -4.2612557 -4.1693907 -2.7836981 0.31729221 1.7877097 -0.1576252 -2.3682981 -2.3543973 -1.3367693 -1.1088173 -1.5230625 -2.4865315][-3.8474398 -3.7707753 -3.7913222 -4.0254431 -4.0857615 -3.1473198 -0.94805717 -0.098910332 -1.4509234 -2.6536932 -2.2497318 -1.3450902 -1.1846042 -1.6149495 -2.5949564][-3.5096803 -3.4837937 -3.5676718 -3.8279874 -4.0649643 -3.7004251 -2.4501367 -2.0459328 -2.7664924 -3.0920248 -2.4576552 -1.7244875 -1.6001937 -1.9871962 -2.8972011][-3.3370996 -3.3378034 -3.4308598 -3.6423345 -3.9739623 -4.0998669 -3.6056361 -3.4301572 -3.6471782 -3.4158249 -2.7057166 -2.1454093 -2.0378466 -2.3656814 -3.1151628][-3.3103468 -3.3033903 -3.347857 -3.4106545 -3.6434636 -3.9888382 -4.0005517 -4.0110331 -4.0468488 -3.6796942 -3.0236845 -2.5770793 -2.4760451 -2.7354932 -3.265563]]...]
INFO - root - 2017-12-07 05:38:15.346838: step 13210, loss = 0.67, batch loss = 0.59 (7.1 examples/sec; 1.125 sec/batch; 99h:46m:05s remains)
INFO - root - 2017-12-07 05:38:27.030656: step 13220, loss = 0.72, batch loss = 0.65 (6.6 examples/sec; 1.215 sec/batch; 107h:45m:07s remains)
INFO - root - 2017-12-07 05:38:38.628129: step 13230, loss = 0.57, batch loss = 0.50 (7.1 examples/sec; 1.134 sec/batch; 100h:36m:18s remains)
INFO - root - 2017-12-07 05:38:50.390392: step 13240, loss = 0.82, batch loss = 0.74 (6.4 examples/sec; 1.252 sec/batch; 111h:03m:28s remains)
INFO - root - 2017-12-07 05:39:02.064484: step 13250, loss = 0.87, batch loss = 0.79 (6.9 examples/sec; 1.167 sec/batch; 103h:30m:35s remains)
INFO - root - 2017-12-07 05:39:13.802235: step 13260, loss = 0.93, batch loss = 0.86 (6.5 examples/sec; 1.224 sec/batch; 108h:34m:37s remains)
INFO - root - 2017-12-07 05:39:25.318418: step 13270, loss = 1.00, batch loss = 0.92 (6.7 examples/sec; 1.191 sec/batch; 105h:38m:11s remains)
INFO - root - 2017-12-07 05:39:37.021067: step 13280, loss = 0.77, batch loss = 0.69 (6.9 examples/sec; 1.158 sec/batch; 102h:39m:11s remains)
INFO - root - 2017-12-07 05:39:48.775603: step 13290, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 1.139 sec/batch; 101h:01m:57s remains)
INFO - root - 2017-12-07 05:40:00.537659: step 13300, loss = 0.87, batch loss = 0.80 (7.2 examples/sec; 1.106 sec/batch; 98h:04m:16s remains)
2017-12-07 05:40:01.420631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7351451 -2.7137971 -2.8098493 -2.9980385 -3.1010051 -2.968492 -2.716336 -2.3063216 -1.8730564 -1.6355584 -1.7187219 -2.1926105 -2.71483 -2.8498325 -2.5584145][-3.2501478 -3.3446095 -3.5177729 -3.6931734 -3.6825123 -3.4552698 -3.147742 -2.7689543 -2.4733346 -2.4306788 -2.6490579 -3.0741832 -3.4141757 -3.336853 -2.89709][-3.8442914 -4.0327682 -4.22223 -4.2802563 -4.0912061 -3.7321446 -3.3427458 -2.9851544 -2.8055739 -2.8753431 -3.2041483 -3.6824055 -3.9910328 -3.840704 -3.4088128][-4.0561824 -4.2043886 -4.2978053 -4.1784196 -3.8079181 -3.3484159 -3.007545 -2.8069241 -2.7368836 -2.8338823 -3.197165 -3.6616874 -3.8814235 -3.6264505 -3.2122526][-4.2071748 -4.2099209 -4.1605949 -3.9171786 -3.4410763 -2.8847675 -2.5762928 -2.46669 -2.3772631 -2.3909276 -2.7889194 -3.3402233 -3.5870314 -3.3620219 -3.0186234][-4.2432876 -4.1207814 -4.00814 -3.7554955 -3.2758703 -2.7135653 -2.4487343 -2.3516903 -2.108927 -1.9502275 -2.3483083 -3.0527365 -3.4925323 -3.405385 -3.1454952][-4.0212989 -3.8747482 -3.7314296 -3.507761 -3.1077156 -2.6638272 -2.4573185 -2.2553005 -1.7269473 -1.3580852 -1.8250992 -2.8590393 -3.7163656 -3.9028506 -3.7279952][-3.8942811 -3.7766318 -3.6052341 -3.3895547 -3.1446292 -2.961359 -2.8983698 -2.5425715 -1.6675875 -1.0643632 -1.5567374 -2.8568211 -4.0450163 -4.3709645 -4.0651383][-3.8777728 -3.7967458 -3.6018353 -3.3988447 -3.36552 -3.4918144 -3.5576015 -3.0576835 -1.9205272 -1.1059949 -1.4875643 -2.7478616 -3.9646358 -4.2285504 -3.6885281][-3.6770694 -3.6286678 -3.3663278 -3.1373687 -3.2754731 -3.6349733 -3.8095629 -3.3358564 -2.2603667 -1.443157 -1.57657 -2.508328 -3.5158198 -3.6950502 -3.0380902][-3.3300145 -3.2305255 -2.841836 -2.5433915 -2.6784868 -3.0181122 -3.1665039 -2.8497612 -2.1482394 -1.6517003 -1.7556221 -2.4506841 -3.2767553 -3.4114301 -2.7378576][-2.8157172 -2.6057997 -2.1105437 -1.7325642 -1.7624528 -1.9271765 -1.9778576 -1.8529289 -1.5926609 -1.4673789 -1.6386666 -2.210361 -2.9053388 -3.1137848 -2.66763][-2.49492 -2.2621267 -1.850991 -1.5071723 -1.3735759 -1.2650466 -1.1714876 -1.1948142 -1.2742164 -1.3988898 -1.6055734 -2.0621676 -2.6676874 -3.0198634 -2.9136539][-2.5144472 -2.3968866 -2.2354498 -2.064508 -1.8724229 -1.6140194 -1.4331069 -1.5041232 -1.6865056 -1.8708322 -2.1037288 -2.4663935 -2.8996582 -3.1567326 -3.1620219][-2.6723778 -2.6962454 -2.7859862 -2.8440366 -2.7662978 -2.511713 -2.2505631 -2.2069376 -2.3452382 -2.5491478 -2.8022013 -3.0835171 -3.3014789 -3.3604622 -3.3175867]]...]
INFO - root - 2017-12-07 05:40:13.227865: step 13310, loss = 1.00, batch loss = 0.92 (6.5 examples/sec; 1.225 sec/batch; 108h:35m:51s remains)
INFO - root - 2017-12-07 05:40:24.859102: step 13320, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 1.131 sec/batch; 100h:15m:03s remains)
INFO - root - 2017-12-07 05:40:36.419994: step 13330, loss = 0.90, batch loss = 0.83 (6.9 examples/sec; 1.151 sec/batch; 102h:03m:13s remains)
INFO - root - 2017-12-07 05:40:48.098997: step 13340, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.160 sec/batch; 102h:51m:52s remains)
INFO - root - 2017-12-07 05:40:59.888827: step 13350, loss = 0.80, batch loss = 0.73 (6.7 examples/sec; 1.189 sec/batch; 105h:21m:52s remains)
INFO - root - 2017-12-07 05:41:11.619082: step 13360, loss = 1.10, batch loss = 1.03 (6.7 examples/sec; 1.198 sec/batch; 106h:11m:42s remains)
INFO - root - 2017-12-07 05:41:23.252329: step 13370, loss = 0.80, batch loss = 0.72 (6.9 examples/sec; 1.161 sec/batch; 102h:54m:53s remains)
INFO - root - 2017-12-07 05:41:34.969679: step 13380, loss = 1.03, batch loss = 0.96 (7.0 examples/sec; 1.145 sec/batch; 101h:28m:23s remains)
INFO - root - 2017-12-07 05:41:46.525669: step 13390, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 1.140 sec/batch; 101h:00m:54s remains)
INFO - root - 2017-12-07 05:41:58.028646: step 13400, loss = 0.65, batch loss = 0.58 (7.0 examples/sec; 1.136 sec/batch; 100h:42m:36s remains)
2017-12-07 05:41:58.881822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8315692 -2.5905185 -2.8296218 -2.2550049 -1.2367213 -0.74299431 -0.95692611 -1.2683175 -1.3012335 -0.14871883 1.4239802 1.9282393 1.3614969 -0.36112022 -2.2302403][-2.0983069 -2.866168 -3.0084395 -2.2879233 -1.12289 -0.48389912 -0.34256554 -0.21225786 -0.050045013 0.961184 1.9814744 1.8955436 0.97518015 -0.771312 -2.5061479][-2.3870678 -3.0820246 -3.0350688 -2.1263065 -0.88741136 -0.18599272 0.24483776 0.63184643 0.89730549 1.7536712 2.1970611 1.5930095 0.41707087 -1.2589855 -2.7546475][-2.6293988 -3.1703763 -2.9151342 -1.8561356 -0.64651656 0.10200882 0.73849964 1.236402 1.6293573 2.3191342 2.2356396 1.2194161 -0.15454721 -1.7490346 -2.9435549][-2.8349867 -3.150502 -2.675087 -1.5562634 -0.46631455 0.34789038 1.1324501 1.7073569 2.2025671 2.5887885 1.9764833 0.690393 -0.84679031 -2.3404129 -3.2239838][-3.0613861 -3.1225364 -2.436563 -1.3076766 -0.30348921 0.7343359 1.7309227 2.4036698 2.9591045 2.8979907 1.8455 0.36548328 -1.3600779 -2.781034 -3.468648][-3.2839522 -3.1382985 -2.3027375 -1.2369738 -0.29722404 0.97962332 2.079412 2.7353973 3.2297158 2.7542176 1.4545093 -0.11461782 -1.887635 -3.0992236 -3.6201336][-3.4258771 -3.2273164 -2.3765144 -1.3770416 -0.37700891 1.1149979 2.1604838 2.7232146 3.1591196 2.4158797 1.0494051 -0.53906059 -2.2298274 -3.1819291 -3.6177623][-3.4682379 -3.4079916 -2.6960361 -1.7272956 -0.57981038 1.0613685 2.0140405 2.6132216 3.1528139 2.3536744 1.0506678 -0.4896183 -2.0480523 -2.8789248 -3.3867388][-3.4084449 -3.6254849 -3.1835613 -2.3119309 -1.0528266 0.63916206 1.4857154 2.086338 2.6051211 1.7956905 0.69080544 -0.62014914 -1.8666751 -2.5517569 -3.0586462][-3.1876342 -3.6730642 -3.5596445 -2.9016671 -1.6801078 -0.057030678 0.70178795 1.2161307 1.5178776 0.71300459 -0.11676693 -1.0488436 -1.84092 -2.2906649 -2.6921508][-2.77323 -3.4057031 -3.5874591 -3.236897 -2.2100661 -0.79564762 -0.17322206 0.15646935 0.17050791 -0.58182 -1.2030098 -1.8067391 -2.1688554 -2.3208554 -2.4868751][-2.2320411 -2.8630366 -3.2607205 -3.2410865 -2.5518703 -1.4832904 -0.9890821 -0.74813056 -0.82874441 -1.3931835 -1.8371623 -2.2235115 -2.3276179 -2.2839019 -2.2702198][-1.8062632 -2.3327494 -2.8071849 -3.0416899 -2.7343464 -2.0869443 -1.7080715 -1.4676487 -1.4861119 -1.7967122 -2.071867 -2.3035877 -2.2858973 -2.1563125 -2.0520017][-1.524297 -1.8677909 -2.2557352 -2.5671306 -2.5400004 -2.2657452 -2.0431473 -1.866632 -1.8330491 -1.9204752 -2.0350978 -2.14547 -2.0776358 -1.924268 -1.797184]]...]
INFO - root - 2017-12-07 05:42:10.631951: step 13410, loss = 0.89, batch loss = 0.81 (6.6 examples/sec; 1.206 sec/batch; 106h:51m:07s remains)
INFO - root - 2017-12-07 05:42:22.209737: step 13420, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 1.146 sec/batch; 101h:35m:39s remains)
INFO - root - 2017-12-07 05:42:33.808006: step 13430, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 1.148 sec/batch; 101h:45m:16s remains)
INFO - root - 2017-12-07 05:42:45.530322: step 13440, loss = 0.59, batch loss = 0.51 (6.7 examples/sec; 1.196 sec/batch; 105h:57m:18s remains)
INFO - root - 2017-12-07 05:42:57.102080: step 13450, loss = 0.65, batch loss = 0.58 (7.0 examples/sec; 1.145 sec/batch; 101h:30m:57s remains)
INFO - root - 2017-12-07 05:43:08.857629: step 13460, loss = 0.66, batch loss = 0.59 (6.8 examples/sec; 1.182 sec/batch; 104h:43m:05s remains)
INFO - root - 2017-12-07 05:43:20.626985: step 13470, loss = 0.92, batch loss = 0.85 (6.7 examples/sec; 1.198 sec/batch; 106h:11m:12s remains)
INFO - root - 2017-12-07 05:43:32.242480: step 13480, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 1.124 sec/batch; 99h:35m:20s remains)
INFO - root - 2017-12-07 05:43:43.966469: step 13490, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 1.159 sec/batch; 102h:44m:09s remains)
INFO - root - 2017-12-07 05:43:55.780639: step 13500, loss = 0.98, batch loss = 0.90 (6.9 examples/sec; 1.154 sec/batch; 102h:14m:55s remains)
2017-12-07 05:43:56.637507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5529523 -1.5690045 -1.5120764 -1.4173923 -1.6032605 -1.813518 -1.8604245 -1.9373868 -2.0176072 -2.2246044 -2.6128983 -2.8907917 -3.0326576 -3.2686126 -3.2867866][-1.5916321 -1.6523039 -1.725343 -1.7331452 -1.9276624 -2.0398426 -1.9694357 -2.0423436 -2.1400871 -2.3882365 -2.89167 -3.1734662 -3.2512476 -3.4898825 -3.4655938][-1.8926244 -2.0139437 -2.2064059 -2.3018959 -2.4674196 -2.4341629 -2.156925 -2.0698681 -2.0464196 -2.2208202 -2.7743702 -3.0611901 -3.1263266 -3.3444698 -3.2699835][-2.2889462 -2.4697473 -2.7060039 -2.7974477 -2.8468535 -2.6539321 -2.2283864 -1.9960821 -1.8446813 -1.9235935 -2.4230645 -2.677331 -2.7887154 -2.9686868 -2.8281283][-2.5980368 -2.7825086 -2.9804711 -3.011198 -2.906271 -2.5445423 -1.9704964 -1.5800958 -1.3701513 -1.44133 -1.8730774 -2.1346612 -2.3818433 -2.5485725 -2.3320661][-2.6824725 -2.8323717 -2.9581161 -2.9086947 -2.6935194 -2.2135129 -1.5034447 -0.91939139 -0.67664909 -0.81952739 -1.2299774 -1.5771761 -2.0224881 -2.2549455 -2.0223179][-2.6258926 -2.679903 -2.6572287 -2.4895494 -2.2148356 -1.6829767 -0.90650964 -0.27132368 -0.068320274 -0.23470783 -0.58447075 -0.99668956 -1.5499012 -1.9142208 -1.811933][-2.625531 -2.58291 -2.377533 -2.0550115 -1.730763 -1.1675436 -0.40763664 0.14625978 0.31159067 0.22461605 -0.03771019 -0.504822 -1.1094651 -1.5978839 -1.6742311][-2.7930167 -2.7003989 -2.3706477 -1.8989027 -1.4627032 -0.7636292 0.020726681 0.50224495 0.69338751 0.676209 0.34742641 -0.32211304 -1.0624189 -1.6584129 -1.7986748][-2.9213502 -2.8639021 -2.5185766 -1.949568 -1.4052153 -0.58500409 0.2077198 0.634933 0.87364721 0.88908958 0.41251659 -0.49364519 -1.428153 -2.0684695 -2.0876467][-3.0044901 -3.0031214 -2.6659093 -2.0115328 -1.3656662 -0.52614951 0.17318535 0.44786406 0.59330177 0.54576874 -0.01513195 -0.97487378 -1.895824 -2.4156768 -2.2494636][-3.2002053 -3.2933726 -2.9359024 -2.1518862 -1.4356921 -0.70794249 -0.15155792 -0.034240723 -0.079886913 -0.28456068 -0.87813234 -1.6974201 -2.315655 -2.4832017 -2.1320155][-3.3760488 -3.707006 -3.464294 -2.5979531 -1.8124378 -1.2522089 -0.85595584 -0.88449788 -1.127732 -1.4401774 -1.9645324 -2.483376 -2.6296859 -2.3910768 -1.9242756][-3.3888698 -3.9738927 -3.9264226 -3.03334 -2.193172 -1.7729063 -1.5666707 -1.8126073 -2.2575617 -2.6145592 -3.0025768 -3.2023258 -2.904741 -2.3713918 -1.8672478][-3.2860627 -3.8993115 -3.9105914 -3.0108635 -2.1457269 -1.8116369 -1.7987092 -2.3019881 -2.9785466 -3.41993 -3.696974 -3.6369262 -3.0162897 -2.3080955 -1.7873042]]...]
INFO - root - 2017-12-07 05:44:08.218347: step 13510, loss = 0.75, batch loss = 0.67 (6.6 examples/sec; 1.206 sec/batch; 106h:50m:49s remains)
INFO - root - 2017-12-07 05:44:19.934227: step 13520, loss = 0.67, batch loss = 0.59 (7.0 examples/sec; 1.140 sec/batch; 101h:00m:34s remains)
INFO - root - 2017-12-07 05:44:31.521772: step 13530, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 1.140 sec/batch; 101h:00m:45s remains)
INFO - root - 2017-12-07 05:44:43.220620: step 13540, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 1.147 sec/batch; 101h:36m:48s remains)
INFO - root - 2017-12-07 05:44:54.628017: step 13550, loss = 0.82, batch loss = 0.75 (7.4 examples/sec; 1.084 sec/batch; 96h:02m:54s remains)
INFO - root - 2017-12-07 05:45:06.293099: step 13560, loss = 0.62, batch loss = 0.55 (7.0 examples/sec; 1.136 sec/batch; 100h:39m:09s remains)
INFO - root - 2017-12-07 05:45:17.854812: step 13570, loss = 0.58, batch loss = 0.51 (7.1 examples/sec; 1.134 sec/batch; 100h:29m:24s remains)
INFO - root - 2017-12-07 05:45:29.552016: step 13580, loss = 0.67, batch loss = 0.59 (6.7 examples/sec; 1.195 sec/batch; 105h:52m:53s remains)
INFO - root - 2017-12-07 05:45:41.266870: step 13590, loss = 0.78, batch loss = 0.70 (6.7 examples/sec; 1.191 sec/batch; 105h:28m:48s remains)
INFO - root - 2017-12-07 05:45:52.994866: step 13600, loss = 0.74, batch loss = 0.66 (6.9 examples/sec; 1.156 sec/batch; 102h:21m:41s remains)
2017-12-07 05:45:53.866540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8848455 -1.908114 -1.9428675 -1.7916076 -1.6069283 -1.4790311 -1.1698704 -0.98315144 -0.89929605 -0.98724031 -1.3336906 -1.8681777 -2.0751071 -2.1053038 -2.4126182][-1.7998765 -1.7014773 -1.8400271 -1.9270573 -1.9577191 -1.9815903 -1.5790832 -1.1871574 -0.91564727 -1.0499315 -1.47451 -1.8108284 -1.7032342 -1.5663381 -1.9456711][-1.7094226 -1.3761573 -1.4346747 -1.6119485 -1.8920043 -2.1884995 -1.8523655 -1.3123562 -0.88040948 -1.0182009 -1.5006526 -1.7146442 -1.3659997 -0.94186258 -1.2519135][-1.5674567 -0.98791432 -0.88008189 -1.0900371 -1.5636196 -2.0916693 -1.944505 -1.5144083 -1.1668112 -1.318784 -1.6516278 -1.599894 -1.0334194 -0.42519474 -0.71542358][-1.4639611 -0.70994568 -0.39701462 -0.47905421 -0.88533807 -1.3312833 -1.2973657 -1.2395415 -1.3552864 -1.7702899 -2.0003278 -1.5769637 -0.67620468 0.12108803 -0.16258717][-1.391686 -0.60184383 -0.2035532 -0.13163424 -0.23281765 -0.26923275 -0.11881351 -0.36325407 -1.0398884 -1.9046092 -2.2474289 -1.6200466 -0.36477947 0.69757318 0.56057072][-1.1643281 -0.42522955 -0.12763786 -0.054170609 0.21142101 0.68825436 1.0791302 0.66264105 -0.53066039 -1.849977 -2.4673014 -1.8537965 -0.31034517 1.0587978 1.1811042][-0.83038211 -0.13506842 -0.037813663 -0.034154415 0.60126352 1.5831895 2.2927628 1.8033743 0.12483788 -1.6448152 -2.6812487 -2.326745 -0.69587374 0.85366678 1.2270789][-0.55296135 -0.033124447 -0.17293978 -0.15585852 0.80496407 2.1730909 3.1433263 2.6569891 0.84434271 -1.0547283 -2.4509425 -2.63452 -1.3512425 0.031400204 0.51774836][-0.60842204 -0.31014729 -0.58056426 -0.48135185 0.60100651 1.9676366 2.8601747 2.5360656 1.1429119 -0.43537569 -1.9331081 -2.6397619 -1.8868675 -0.72880149 -0.12519884][-0.75740314 -0.63382006 -0.99847484 -0.97613645 -0.16211605 0.87772655 1.6356902 1.5811124 0.80768442 -0.21378326 -1.5340853 -2.4660728 -2.1287496 -1.1344812 -0.36003113][-0.80918407 -0.79925084 -1.2951672 -1.4835551 -1.1302254 -0.49058151 0.15449 0.2981081 0.051868916 -0.42336512 -1.3504877 -2.1847394 -2.1454022 -1.374465 -0.5443337][-0.74244857 -0.930491 -1.6022344 -1.9453757 -1.8697956 -1.4850769 -1.0332248 -0.9522469 -0.96763277 -1.0445201 -1.4496589 -1.8957851 -1.9336524 -1.3560812 -0.59867215][-0.61266327 -0.87877488 -1.5764275 -1.948282 -2.0085807 -1.8413692 -1.6778455 -1.801146 -1.8230038 -1.7086182 -1.7242329 -1.8465371 -1.9027696 -1.4855511 -0.75782919][-0.70346427 -0.87938738 -1.3738368 -1.6212404 -1.6413283 -1.5432448 -1.5810587 -1.9375691 -2.1772416 -2.1448293 -2.0439711 -2.0491285 -2.1030788 -1.7548864 -1.0098791]]...]
INFO - root - 2017-12-07 05:46:05.550542: step 13610, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 1.155 sec/batch; 102h:18m:04s remains)
INFO - root - 2017-12-07 05:46:17.171900: step 13620, loss = 0.80, batch loss = 0.73 (6.8 examples/sec; 1.176 sec/batch; 104h:09m:00s remains)
INFO - root - 2017-12-07 05:46:28.664315: step 13630, loss = 0.84, batch loss = 0.77 (7.0 examples/sec; 1.150 sec/batch; 101h:51m:55s remains)
INFO - root - 2017-12-07 05:46:40.308614: step 13640, loss = 0.78, batch loss = 0.71 (6.6 examples/sec; 1.204 sec/batch; 106h:36m:20s remains)
INFO - root - 2017-12-07 05:46:51.894675: step 13650, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 1.169 sec/batch; 103h:30m:41s remains)
INFO - root - 2017-12-07 05:47:03.430597: step 13660, loss = 0.87, batch loss = 0.80 (6.8 examples/sec; 1.169 sec/batch; 103h:31m:04s remains)
INFO - root - 2017-12-07 05:47:15.142134: step 13670, loss = 0.67, batch loss = 0.60 (6.8 examples/sec; 1.175 sec/batch; 104h:02m:07s remains)
INFO - root - 2017-12-07 05:47:26.858071: step 13680, loss = 0.67, batch loss = 0.60 (6.5 examples/sec; 1.231 sec/batch; 109h:02m:51s remains)
INFO - root - 2017-12-07 05:47:38.436092: step 13690, loss = 0.86, batch loss = 0.79 (6.7 examples/sec; 1.199 sec/batch; 106h:10m:06s remains)
INFO - root - 2017-12-07 05:47:49.879084: step 13700, loss = 0.97, batch loss = 0.90 (7.3 examples/sec; 1.090 sec/batch; 96h:31m:17s remains)
2017-12-07 05:47:50.743822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5130782 -2.5051599 -2.7234716 -2.8357174 -2.7187 -2.5602427 -2.3944912 -2.2681625 -2.1071961 -1.92939 -1.8702459 -2.0391405 -2.22959 -2.3293717 -2.3344769][-2.9314544 -2.9483941 -3.2091532 -3.1747539 -2.8862314 -2.7129896 -2.5560684 -2.4310787 -2.2796228 -2.089052 -2.0231476 -2.2532248 -2.476892 -2.5751894 -2.5804849][-3.0275326 -3.10562 -3.4089246 -3.2718966 -2.9431086 -2.8407063 -2.689477 -2.5236917 -2.413394 -2.2977219 -2.2730298 -2.5349376 -2.7035928 -2.7159991 -2.6532187][-2.8662665 -3.0241864 -3.3375514 -3.1362834 -2.8772666 -2.8894711 -2.7428048 -2.5360918 -2.4859853 -2.5124216 -2.5599084 -2.8166013 -2.8895435 -2.769196 -2.6014915][-2.5852704 -2.8461227 -3.1287005 -2.878715 -2.7191603 -2.8203969 -2.701437 -2.5130532 -2.5512969 -2.67201 -2.7315211 -2.9568977 -2.9572432 -2.7323289 -2.4919028][-2.3851757 -2.7691226 -2.9905124 -2.7225411 -2.6484258 -2.8013475 -2.7074986 -2.5419831 -2.639503 -2.760618 -2.7654905 -3.0178776 -3.0109053 -2.736979 -2.4567235][-2.4210777 -2.8988066 -3.0837421 -2.8106766 -2.7448297 -2.9024348 -2.8488855 -2.7446141 -2.8629427 -2.8841281 -2.7904541 -3.0990355 -3.1121325 -2.7914624 -2.4876323][-2.5952387 -3.0780728 -3.2477241 -2.993669 -2.9113879 -3.0901489 -3.1091418 -3.0547881 -3.1612148 -3.0490007 -2.872052 -3.232832 -3.271317 -2.8927672 -2.5815916][-2.6440997 -3.0127988 -3.1851971 -3.0646358 -3.1105533 -3.4096272 -3.5339191 -3.4556668 -3.46737 -3.2443562 -3.0573053 -3.4893296 -3.5502982 -3.0966415 -2.7410672][-2.5364933 -2.7497916 -2.9464998 -3.0527074 -3.2942309 -3.7377155 -3.9453988 -3.7732849 -3.6297097 -3.3851943 -3.294374 -3.8080788 -3.8935835 -3.3755708 -2.9466767][-2.2492363 -2.3782609 -2.6353147 -2.9355378 -3.3056896 -3.8174539 -4.0666156 -3.8535931 -3.6714249 -3.5397701 -3.595237 -4.1267786 -4.170536 -3.5881305 -3.0784922][-2.0320559 -2.1821482 -2.4726996 -2.8283625 -3.2081032 -3.701385 -3.9803514 -3.7878258 -3.6300609 -3.632977 -3.7932353 -4.3068213 -4.273459 -3.6478848 -3.0909023][-2.2497489 -2.4513347 -2.672492 -2.9230423 -3.1898599 -3.5879631 -3.8549175 -3.660759 -3.4900045 -3.5243571 -3.6940529 -4.2172117 -4.1544437 -3.5473351 -3.01438][-2.9103847 -3.1472178 -3.2862935 -3.3944097 -3.4955661 -3.6660254 -3.7624869 -3.4414506 -3.2082829 -3.2083292 -3.3618331 -3.9168067 -3.8708043 -3.3133407 -2.8554077][-3.6856141 -3.8850982 -3.9636514 -3.9811976 -3.9393466 -3.8626075 -3.7299094 -3.2653933 -2.968009 -2.9187865 -3.0287535 -3.6054034 -3.580085 -3.0684457 -2.6898143]]...]
INFO - root - 2017-12-07 05:48:02.476885: step 13710, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.168 sec/batch; 103h:24m:34s remains)
INFO - root - 2017-12-07 05:48:14.153294: step 13720, loss = 0.70, batch loss = 0.63 (6.8 examples/sec; 1.182 sec/batch; 104h:39m:57s remains)
INFO - root - 2017-12-07 05:48:25.744602: step 13730, loss = 0.86, batch loss = 0.79 (6.7 examples/sec; 1.200 sec/batch; 106h:17m:12s remains)
INFO - root - 2017-12-07 05:48:37.251026: step 13740, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 1.138 sec/batch; 100h:47m:42s remains)
INFO - root - 2017-12-07 05:48:48.908283: step 13750, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 1.127 sec/batch; 99h:47m:22s remains)
INFO - root - 2017-12-07 05:49:00.340162: step 13760, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 1.163 sec/batch; 102h:57m:55s remains)
INFO - root - 2017-12-07 05:49:12.014432: step 13770, loss = 0.80, batch loss = 0.72 (6.6 examples/sec; 1.203 sec/batch; 106h:31m:47s remains)
INFO - root - 2017-12-07 05:49:23.693392: step 13780, loss = 0.78, batch loss = 0.71 (6.7 examples/sec; 1.191 sec/batch; 105h:28m:04s remains)
INFO - root - 2017-12-07 05:49:35.272282: step 13790, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 1.183 sec/batch; 104h:42m:28s remains)
INFO - root - 2017-12-07 05:49:46.845032: step 13800, loss = 0.82, batch loss = 0.75 (6.8 examples/sec; 1.179 sec/batch; 104h:23m:41s remains)
2017-12-07 05:49:47.769788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.67658 -3.7288933 -3.7730079 -3.7565229 -3.7548282 -3.772192 -3.8317468 -3.8554516 -3.7608464 -3.6813538 -3.6365919 -3.5398107 -3.5052471 -3.5891354 -3.7205734][-3.7975366 -3.9130862 -4.0303397 -4.0722604 -4.1161923 -4.083334 -4.052083 -3.9468102 -3.7238748 -3.5746422 -3.4833381 -3.3156886 -3.2857838 -3.4526179 -3.6753039][-3.6488483 -3.8694437 -4.100915 -4.2403235 -4.3393893 -4.2404008 -4.0926304 -3.8292227 -3.4704349 -3.2567663 -3.131876 -2.9128835 -2.9099145 -3.1754742 -3.5037694][-3.0422325 -3.4064279 -3.7966847 -4.03166 -4.136776 -3.9681172 -3.7392263 -3.4086914 -3.0100584 -2.7659345 -2.6019406 -2.3651965 -2.4279401 -2.8177457 -3.2552447][-2.1999707 -2.6932535 -3.2724659 -3.5861917 -3.6163294 -3.3120871 -3.0017524 -2.7419331 -2.5062509 -2.309284 -2.0809147 -1.8249097 -1.9634838 -2.4918394 -3.0307434][-1.4242969 -1.8987093 -2.595993 -2.9766932 -2.9106793 -2.4213846 -1.9544721 -1.8734138 -2.0145361 -1.9997249 -1.7396376 -1.4315822 -1.6023483 -2.2522287 -2.9015913][-1.0009005 -1.3078566 -1.9853466 -2.3763483 -2.2225862 -1.5293984 -0.80536914 -0.86854744 -1.4605587 -1.7269301 -1.5266457 -1.1798918 -1.3414471 -2.0801287 -2.8574786][-1.0777268 -1.2288237 -1.7899714 -2.123585 -1.8857565 -1.0225725 -0.025175095 -0.099332333 -0.96409488 -1.4288292 -1.3530612 -1.0439227 -1.1652918 -1.9235399 -2.8190541][-1.441344 -1.5426989 -1.9865384 -2.2741928 -2.056967 -1.2289665 -0.25348663 -0.26344442 -1.0365188 -1.4477708 -1.3868518 -1.1166074 -1.1616344 -1.8418849 -2.77893][-1.8853645 -1.9976962 -2.3382542 -2.565197 -2.414151 -1.7259758 -0.99687862 -1.0040505 -1.5307386 -1.7483366 -1.6112809 -1.3675447 -1.3788097 -1.9426098 -2.8264568][-2.228574 -2.326206 -2.5328789 -2.650291 -2.4980574 -1.9223607 -1.4685428 -1.5512595 -1.9092946 -1.9981174 -1.8124015 -1.5956814 -1.6577005 -2.1733868 -2.9616456][-2.3326037 -2.3677716 -2.4763577 -2.5257998 -2.3594248 -1.9083822 -1.7234786 -1.9180086 -2.1731761 -2.2029407 -1.9854691 -1.7596345 -1.8656914 -2.3732297 -3.0800765][-2.2206297 -2.2185366 -2.2861116 -2.324362 -2.1710949 -1.8401148 -1.8326309 -2.0992737 -2.2879362 -2.3172424 -2.1212664 -1.872854 -1.9737713 -2.4665046 -3.1232095][-2.0199358 -1.9487722 -1.9574375 -1.9644587 -1.8226047 -1.5994382 -1.6746647 -1.9335637 -2.0707202 -2.1590159 -2.0997472 -1.943635 -2.0369558 -2.4859498 -3.1005521][-2.0181322 -1.880697 -1.8223372 -1.7563922 -1.6284854 -1.5253942 -1.6455066 -1.8784347 -1.9973788 -2.1355197 -2.2083857 -2.1687746 -2.2603142 -2.6286478 -3.1491628]]...]
INFO - root - 2017-12-07 05:49:59.479424: step 13810, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 1.179 sec/batch; 104h:21m:41s remains)
INFO - root - 2017-12-07 05:50:11.068682: step 13820, loss = 0.75, batch loss = 0.67 (6.6 examples/sec; 1.209 sec/batch; 106h:59m:17s remains)
INFO - root - 2017-12-07 05:50:22.680532: step 13830, loss = 0.68, batch loss = 0.61 (6.7 examples/sec; 1.190 sec/batch; 105h:21m:01s remains)
INFO - root - 2017-12-07 05:50:34.353749: step 13840, loss = 0.76, batch loss = 0.69 (6.9 examples/sec; 1.160 sec/batch; 102h:38m:12s remains)
INFO - root - 2017-12-07 05:50:46.116147: step 13850, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 1.154 sec/batch; 102h:09m:58s remains)
INFO - root - 2017-12-07 05:50:57.462073: step 13860, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 1.146 sec/batch; 101h:24m:22s remains)
INFO - root - 2017-12-07 05:51:09.136652: step 13870, loss = 0.80, batch loss = 0.72 (7.2 examples/sec; 1.109 sec/batch; 98h:08m:09s remains)
INFO - root - 2017-12-07 05:51:20.606496: step 13880, loss = 0.71, batch loss = 0.64 (6.8 examples/sec; 1.180 sec/batch; 104h:27m:28s remains)
INFO - root - 2017-12-07 05:51:32.379328: step 13890, loss = 0.69, batch loss = 0.62 (6.6 examples/sec; 1.204 sec/batch; 106h:36m:05s remains)
INFO - root - 2017-12-07 05:51:44.137380: step 13900, loss = 0.53, batch loss = 0.46 (6.8 examples/sec; 1.181 sec/batch; 104h:29m:57s remains)
2017-12-07 05:51:44.979583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0039682388 0.64185762 1.093966 1.3597393 1.4006085 1.4315515 1.3152795 1.3781443 1.4730468 1.3299546 1.4117022 1.4749289 1.4253454 1.3699613 1.3681183][0.17778635 0.88995314 1.3077364 1.5227594 1.6249394 1.7194767 1.5582681 1.5091348 1.5333462 1.4024024 1.4981608 1.5784931 1.5851202 1.5511866 1.5556417][0.33286428 1.0169187 1.309772 1.4706259 1.6714044 1.8083653 1.5744023 1.3759084 1.3331871 1.2421317 1.3347831 1.4412556 1.5247693 1.5239825 1.5388799][0.38401461 1.0035791 1.2092295 1.3919835 1.6840048 1.8214569 1.5674601 1.3074708 1.2675138 1.2522964 1.2672157 1.3111606 1.4429793 1.5020518 1.5337219][0.40522432 0.98511553 1.1556664 1.3657155 1.6887822 1.8316317 1.6510262 1.4083591 1.4232078 1.4845872 1.3448186 1.2271767 1.3483281 1.4592695 1.47016][0.34616613 0.91446686 1.0989203 1.3212109 1.6396904 1.8476281 1.7867169 1.5742221 1.6053615 1.6870475 1.416461 1.1417723 1.2131567 1.3470888 1.343112][0.10031033 0.5931735 0.78198528 0.98065042 1.2806025 1.5599303 1.5982432 1.41258 1.4566784 1.5751152 1.2689891 0.90561914 0.921566 1.0645838 1.0954432][-0.17663145 0.21485138 0.38611603 0.54487276 0.7842536 1.0259981 1.0555215 0.88249826 0.95056581 1.1182752 0.87125397 0.56286049 0.6057272 0.77084541 0.83849716][-0.53312731 -0.26346397 -0.13295364 -0.013110638 0.15348816 0.3146863 0.298656 0.13399506 0.19471073 0.37523365 0.22197628 0.031074047 0.10907316 0.26929188 0.36342573][-0.94671512 -0.87811828 -0.87928057 -0.83276391 -0.72440052 -0.61695528 -0.63437319 -0.76571131 -0.75848413 -0.63683486 -0.73749542 -0.86613154 -0.81756115 -0.705637 -0.601629][-1.1831546 -1.2517488 -1.3610699 -1.4252369 -1.4147527 -1.3685441 -1.3823395 -1.47861 -1.5367975 -1.4880116 -1.5421286 -1.6070962 -1.5838952 -1.5297718 -1.448652][-1.4656434 -1.578805 -1.7581849 -1.9466124 -2.0552626 -2.0930221 -2.1206167 -2.1947467 -2.2808881 -2.2808905 -2.3087702 -2.3499777 -2.3566592 -2.3457975 -2.2743256][-1.6616042 -1.751534 -1.9621642 -2.2254467 -2.4090965 -2.5096743 -2.5649476 -2.6356592 -2.7108579 -2.7281299 -2.7746921 -2.8351383 -2.8659019 -2.8668969 -2.7852309][-1.6227069 -1.6166492 -1.8078563 -2.0786653 -2.2820995 -2.4010017 -2.4491272 -2.4876723 -2.5265126 -2.5444183 -2.5965259 -2.6551828 -2.6907396 -2.6976728 -2.6320844][-1.7325101 -1.6965971 -1.8808997 -2.1388397 -2.3398206 -2.4605277 -2.4922292 -2.5025072 -2.5163896 -2.529099 -2.5665903 -2.6019683 -2.6238222 -2.638972 -2.6174502]]...]
INFO - root - 2017-12-07 05:51:56.535515: step 13910, loss = 0.57, batch loss = 0.49 (7.2 examples/sec; 1.115 sec/batch; 98h:41m:48s remains)
INFO - root - 2017-12-07 05:52:08.189510: step 13920, loss = 0.62, batch loss = 0.55 (7.1 examples/sec; 1.131 sec/batch; 100h:03m:39s remains)
INFO - root - 2017-12-07 05:52:19.735834: step 13930, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 1.158 sec/batch; 102h:30m:08s remains)
INFO - root - 2017-12-07 05:52:31.319985: step 13940, loss = 0.57, batch loss = 0.50 (6.8 examples/sec; 1.181 sec/batch; 104h:31m:22s remains)
INFO - root - 2017-12-07 05:52:42.894493: step 13950, loss = 0.61, batch loss = 0.54 (6.6 examples/sec; 1.209 sec/batch; 106h:57m:57s remains)
INFO - root - 2017-12-07 05:52:54.450362: step 13960, loss = 0.79, batch loss = 0.71 (6.7 examples/sec; 1.196 sec/batch; 105h:47m:10s remains)
INFO - root - 2017-12-07 05:53:06.022927: step 13970, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 1.117 sec/batch; 98h:50m:19s remains)
INFO - root - 2017-12-07 05:53:17.896419: step 13980, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 1.133 sec/batch; 100h:14m:46s remains)
INFO - root - 2017-12-07 05:53:30.810797: step 13990, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 1.145 sec/batch; 101h:18m:45s remains)
INFO - root - 2017-12-07 05:53:42.357371: step 14000, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.177 sec/batch; 104h:07m:47s remains)
2017-12-07 05:53:43.212309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0400519 -2.961247 -3.3503509 -3.9094431 -3.9604921 -3.342885 -2.5461609 -2.2586355 -2.5741358 -2.9675746 -2.6752033 -1.7966835 -1.2731562 -1.3522444 -1.6724181][-2.703949 -2.7995677 -3.3633223 -3.7691805 -3.4619057 -2.6049478 -1.737344 -1.5548937 -2.1557202 -2.6137595 -1.9291186 -0.64170337 -0.11115503 -0.45326352 -1.0938699][-2.5450644 -2.8341632 -3.4289107 -3.4977887 -2.8124683 -1.8555129 -1.2132871 -1.3717542 -2.1239998 -2.2817049 -1.019043 0.55154943 0.80078745 -0.019756317 -0.97625661][-2.7429433 -3.082418 -3.4551997 -3.1350574 -2.3188465 -1.638809 -1.5002072 -1.9529045 -2.4129362 -1.798341 0.14621353 1.6850562 1.284605 -0.12583351 -1.2617197][-3.1182871 -3.423871 -3.4786873 -2.8421979 -2.1303732 -1.9063089 -2.1257205 -2.345274 -2.0092187 -0.67676377 1.3219776 2.06992 0.81813478 -0.8232131 -1.6839368][-3.4063346 -3.538445 -3.3399243 -2.6227336 -2.1064422 -2.0811102 -2.0646014 -1.5009081 -0.36641788 1.02777 2.2075515 1.7295346 -0.13322878 -1.57971 -1.9161763][-3.4459593 -3.27977 -2.9194946 -2.2817202 -1.9167397 -1.8672879 -1.3545961 0.025100708 1.5110602 2.2257109 2.1683359 0.8196249 -0.99930286 -1.9499459 -1.8152277][-3.4088802 -3.0569801 -2.7726643 -2.3191488 -1.9459774 -1.6441982 -0.66661096 1.0228553 2.0478616 1.6520467 0.72217846 -0.65258574 -1.8453529 -2.1617334 -1.6262825][-3.3919957 -3.0601478 -2.9857616 -2.7134862 -2.3133562 -1.8040907 -0.59990072 0.82647276 0.94410849 -0.21642208 -1.2255666 -2.0076702 -2.4427476 -2.338578 -1.7057936][-3.3319802 -3.1208153 -3.2759538 -3.1940765 -2.8415787 -2.2556486 -1.0769806 -0.15372705 -0.72704816 -2.0607378 -2.727807 -2.8398123 -2.6765339 -2.3669574 -1.947336][-3.1145289 -3.0430908 -3.4424362 -3.5563529 -3.2569654 -2.6215336 -1.5678074 -1.0538952 -1.8660448 -2.9789348 -3.2744029 -2.94667 -2.4782128 -2.1690166 -2.0784168][-2.8030293 -2.7436378 -3.1593056 -3.3111434 -3.0599346 -2.4710269 -1.5987127 -1.3578813 -2.1703153 -2.9994566 -3.0503128 -2.5930991 -2.1316888 -1.9737425 -2.1435962][-2.7357988 -2.6927977 -2.986011 -3.0864315 -2.9352832 -2.4952831 -1.8330281 -1.7277493 -2.3823125 -2.9276333 -2.8384528 -2.450042 -2.1810968 -2.1801634 -2.41495][-2.9912353 -2.983809 -3.1429768 -3.1993308 -3.1625497 -2.8981361 -2.464973 -2.400661 -2.7595241 -2.9740579 -2.7977774 -2.5587051 -2.5076246 -2.605453 -2.775425][-3.1561062 -3.1683679 -3.1929231 -3.1419051 -3.1238689 -3.0175436 -2.787128 -2.7413409 -2.8780627 -2.9180865 -2.7728868 -2.6855614 -2.7793603 -2.8939295 -2.9708147]]...]
INFO - root - 2017-12-07 05:53:54.823653: step 14010, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 1.164 sec/batch; 103h:00m:31s remains)
INFO - root - 2017-12-07 05:54:06.414164: step 14020, loss = 0.82, batch loss = 0.75 (6.6 examples/sec; 1.213 sec/batch; 107h:18m:00s remains)
INFO - root - 2017-12-07 05:54:18.011525: step 14030, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 1.124 sec/batch; 99h:27m:51s remains)
INFO - root - 2017-12-07 05:54:29.740553: step 14040, loss = 0.85, batch loss = 0.78 (6.9 examples/sec; 1.167 sec/batch; 103h:14m:26s remains)
INFO - root - 2017-12-07 05:54:41.467618: step 14050, loss = 0.83, batch loss = 0.76 (6.6 examples/sec; 1.219 sec/batch; 107h:48m:35s remains)
INFO - root - 2017-12-07 05:54:52.953476: step 14060, loss = 0.67, batch loss = 0.60 (6.3 examples/sec; 1.266 sec/batch; 111h:59m:13s remains)
INFO - root - 2017-12-07 05:55:04.575450: step 14070, loss = 0.71, batch loss = 0.64 (7.2 examples/sec; 1.107 sec/batch; 97h:53m:03s remains)
INFO - root - 2017-12-07 05:55:16.364040: step 14080, loss = 0.68, batch loss = 0.61 (7.2 examples/sec; 1.117 sec/batch; 98h:49m:53s remains)
INFO - root - 2017-12-07 05:55:27.982158: step 14090, loss = 0.64, batch loss = 0.56 (7.3 examples/sec; 1.103 sec/batch; 97h:34m:18s remains)
INFO - root - 2017-12-07 05:55:39.548347: step 14100, loss = 0.73, batch loss = 0.65 (6.8 examples/sec; 1.177 sec/batch; 104h:04m:40s remains)
2017-12-07 05:55:40.471338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7139783 -1.7054543 -1.6727147 -1.598563 -1.5420835 -1.5425589 -1.6154819 -1.6841977 -1.7302768 -1.7163661 -1.6653814 -1.5928097 -1.4799898 -1.4104953 -1.4922969][-1.4609032 -1.4993126 -1.5059314 -1.4379387 -1.3531268 -1.3314557 -1.4234741 -1.5341649 -1.5910084 -1.5116677 -1.4021473 -1.3492517 -1.2829657 -1.2742121 -1.4269226][-0.97454357 -0.97947288 -0.964221 -0.88318396 -0.79451823 -0.78898597 -0.951653 -1.1288836 -1.1999853 -1.0687196 -0.92620087 -0.90814281 -0.88945293 -0.9358201 -1.1631465][-0.47442031 -0.36954117 -0.26813078 -0.15197659 -0.06959486 -0.054970264 -0.22199774 -0.40854502 -0.52015591 -0.48536992 -0.45300579 -0.50915337 -0.47441363 -0.48919463 -0.7338922][-0.13411808 0.058837891 0.23347235 0.32267904 0.33205938 0.43413639 0.49133587 0.56413269 0.51460934 0.320992 0.082082272 -0.10627604 -0.069012165 -0.04740572 -0.32417059][-0.041121483 0.17756605 0.37381077 0.39543295 0.36976051 0.71470881 1.2464776 1.7874503 1.829463 1.2463136 0.56512403 0.14305353 0.10877371 0.14116287 -0.15824747][-0.24712658 -0.13449955 0.016246319 0.01761055 0.044034958 0.64433289 1.6023388 2.5472627 2.6270223 1.7193146 0.78757811 0.2755537 0.14636278 0.1208086 -0.20344305][-0.44834018 -0.47458053 -0.45874214 -0.50975204 -0.50194168 0.01249218 0.84050083 1.642828 1.7067647 1.0050273 0.45615578 0.24790144 0.13337088 -0.0089797974 -0.35978127][-0.64336205 -0.77124262 -0.85681295 -0.92252231 -0.98683691 -0.76956773 -0.30036354 0.20752525 0.23698521 -0.043948174 0.010845661 0.23756075 0.22280169 -0.0035886765 -0.36498451][-0.9111352 -1.1680415 -1.3400674 -1.3748069 -1.4572287 -1.4983907 -1.2823977 -0.96880722 -0.92585754 -0.84214091 -0.35242939 0.17643833 0.30378389 0.059648514 -0.31538391][-1.3297532 -1.5986025 -1.7222598 -1.6313727 -1.6627419 -1.8775179 -1.8565381 -1.7135928 -1.7223253 -1.5108762 -0.94247556 -0.3526783 -0.11768007 -0.25621605 -0.5346272][-1.7379527 -1.8842571 -1.8980744 -1.7383361 -1.7166495 -1.9155891 -1.9157202 -1.8781533 -1.9828732 -1.8398001 -1.4566126 -1.0647206 -0.87633872 -0.91648674 -1.0214186][-1.9742551 -1.9755151 -1.9386954 -1.8723035 -1.8987904 -2.030344 -1.9794176 -1.967485 -2.1098609 -2.0375679 -1.8367879 -1.6566999 -1.5714855 -1.5717375 -1.5136659][-2.2734506 -2.1943841 -2.1606803 -2.2084565 -2.3048012 -2.3883555 -2.3310378 -2.3161147 -2.4012008 -2.3121567 -2.1615579 -2.0514514 -1.974983 -1.9161267 -1.7648954][-2.5666051 -2.4841247 -2.487092 -2.5919383 -2.7143903 -2.759161 -2.6861982 -2.6332173 -2.6280718 -2.5309324 -2.4052334 -2.2795053 -2.1466186 -2.014046 -1.8502214]]...]
INFO - root - 2017-12-07 05:55:52.063171: step 14110, loss = 0.85, batch loss = 0.77 (7.4 examples/sec; 1.087 sec/batch; 96h:06m:17s remains)
INFO - root - 2017-12-07 05:56:03.534866: step 14120, loss = 0.86, batch loss = 0.79 (7.0 examples/sec; 1.147 sec/batch; 101h:24m:30s remains)
INFO - root - 2017-12-07 05:56:15.083622: step 14130, loss = 0.65, batch loss = 0.58 (7.2 examples/sec; 1.104 sec/batch; 97h:35m:55s remains)
INFO - root - 2017-12-07 05:56:26.796813: step 14140, loss = 0.74, batch loss = 0.67 (6.7 examples/sec; 1.191 sec/batch; 105h:21m:31s remains)
INFO - root - 2017-12-07 05:56:38.544768: step 14150, loss = 0.81, batch loss = 0.73 (6.8 examples/sec; 1.181 sec/batch; 104h:26m:21s remains)
INFO - root - 2017-12-07 05:56:50.207720: step 14160, loss = 0.80, batch loss = 0.72 (6.7 examples/sec; 1.192 sec/batch; 105h:21m:47s remains)
INFO - root - 2017-12-07 05:57:01.652740: step 14170, loss = 0.92, batch loss = 0.84 (7.7 examples/sec; 1.044 sec/batch; 92h:20m:30s remains)
INFO - root - 2017-12-07 05:57:13.098099: step 14180, loss = 0.76, batch loss = 0.69 (6.6 examples/sec; 1.215 sec/batch; 107h:26m:55s remains)
INFO - root - 2017-12-07 05:57:24.699936: step 14190, loss = 0.73, batch loss = 0.66 (6.6 examples/sec; 1.220 sec/batch; 107h:51m:16s remains)
INFO - root - 2017-12-07 05:57:36.324631: step 14200, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 1.130 sec/batch; 99h:55m:04s remains)
2017-12-07 05:57:37.157187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.655982 -2.8111682 -3.127125 -3.2874579 -3.1720376 -2.9650438 -2.8850756 -2.9376111 -2.9793921 -2.9587922 -2.9421029 -2.997911 -3.0866404 -3.0898652 -2.9941959][-2.5614607 -2.7227707 -3.0411568 -3.1521611 -2.9222031 -2.5873921 -2.4719744 -2.547611 -2.6653681 -2.7665167 -2.8227828 -2.8701091 -2.9310875 -2.9166384 -2.8109238][-2.6873131 -2.8314996 -3.1029961 -3.1362591 -2.7979429 -2.3897262 -2.2797222 -2.3496664 -2.4682641 -2.6724594 -2.8585966 -2.9513979 -3.0023766 -2.9668152 -2.8744891][-2.9869256 -3.1592836 -3.3580117 -3.2320809 -2.6681466 -2.1028509 -1.984622 -2.100184 -2.2555764 -2.5350938 -2.8074672 -2.9459362 -3.0164273 -3.0072861 -2.9773617][-3.3307047 -3.6218181 -3.8374004 -3.6210237 -2.8523979 -2.0343053 -1.7028732 -1.7378647 -1.9658527 -2.3784885 -2.7126112 -2.8408928 -2.9296484 -2.9885881 -3.050266][-3.5367694 -3.9530284 -4.2313733 -4.0549054 -3.3015485 -2.3505726 -1.654808 -1.311456 -1.4200873 -1.9678404 -2.4594378 -2.6075261 -2.6758158 -2.7584729 -2.8596535][-3.592339 -4.0943718 -4.4515572 -4.3444881 -3.6960235 -2.8147092 -1.899811 -1.0652161 -0.75670838 -1.2279265 -1.8491826 -2.0922675 -2.1542356 -2.2118533 -2.2990882][-3.5071845 -4.0079212 -4.414021 -4.3665442 -3.8297987 -3.1847668 -2.4677625 -1.5710034 -0.90562248 -0.95243335 -1.3811638 -1.6174562 -1.6970334 -1.7586391 -1.8164251][-3.3681087 -3.8458667 -4.3069234 -4.3174596 -3.8337841 -3.3369637 -2.8953967 -2.2394538 -1.6593571 -1.4752171 -1.5688574 -1.6331844 -1.6176405 -1.6137228 -1.5843396][-3.2531075 -3.7491641 -4.3213139 -4.461566 -4.0236807 -3.5035272 -3.116168 -2.6442144 -2.3054283 -2.1788743 -2.1520474 -2.0752282 -1.9790123 -1.9218366 -1.7668777][-3.1103442 -3.5725017 -4.1998677 -4.5103159 -4.20772 -3.6662002 -3.2015319 -2.8009624 -2.6491849 -2.60366 -2.54394 -2.395319 -2.2465549 -2.2353098 -2.1538856][-2.9603815 -3.323617 -3.9035895 -4.3260789 -4.2234163 -3.7856998 -3.2446156 -2.8178282 -2.7361674 -2.7379661 -2.7351222 -2.584686 -2.3656809 -2.3071454 -2.2620246][-2.908505 -3.1544666 -3.6237996 -4.0158086 -4.0357771 -3.7861085 -3.3316016 -2.9416513 -2.8682258 -2.859437 -2.9102738 -2.8120484 -2.5667448 -2.3911157 -2.2385235][-2.9849663 -3.1573949 -3.5316772 -3.8073022 -3.8412449 -3.7644012 -3.5124497 -3.313375 -3.3309271 -3.32131 -3.3560596 -3.2870557 -3.0585542 -2.8163075 -2.5491803][-3.2192392 -3.3768997 -3.6884706 -3.8810377 -3.9383562 -3.9801178 -3.8663144 -3.7878771 -3.8687406 -3.9086287 -3.9681766 -3.9374421 -3.7551336 -3.5139265 -3.2000692]]...]
INFO - root - 2017-12-07 05:57:48.822349: step 14210, loss = 0.92, batch loss = 0.84 (6.8 examples/sec; 1.176 sec/batch; 103h:57m:38s remains)
INFO - root - 2017-12-07 05:58:00.506634: step 14220, loss = 0.66, batch loss = 0.58 (6.8 examples/sec; 1.184 sec/batch; 104h:38m:52s remains)
INFO - root - 2017-12-07 05:58:12.050330: step 14230, loss = 1.06, batch loss = 0.98 (6.8 examples/sec; 1.180 sec/batch; 104h:18m:22s remains)
INFO - root - 2017-12-07 05:58:23.589655: step 14240, loss = 0.64, batch loss = 0.57 (7.0 examples/sec; 1.146 sec/batch; 101h:16m:50s remains)
INFO - root - 2017-12-07 05:58:35.088137: step 14250, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 1.121 sec/batch; 99h:03m:54s remains)
INFO - root - 2017-12-07 05:58:46.699699: step 14260, loss = 0.72, batch loss = 0.65 (6.7 examples/sec; 1.194 sec/batch; 105h:31m:12s remains)
INFO - root - 2017-12-07 05:58:58.358832: step 14270, loss = 0.88, batch loss = 0.81 (6.8 examples/sec; 1.184 sec/batch; 104h:42m:21s remains)
INFO - root - 2017-12-07 05:59:10.124370: step 14280, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.170 sec/batch; 103h:27m:23s remains)
INFO - root - 2017-12-07 05:59:21.699644: step 14290, loss = 0.84, batch loss = 0.77 (7.1 examples/sec; 1.132 sec/batch; 100h:01m:15s remains)
INFO - root - 2017-12-07 05:59:33.215680: step 14300, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 1.146 sec/batch; 101h:17m:18s remains)
2017-12-07 05:59:34.082309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9913218 -0.80878758 -0.62043953 -0.49393058 -0.45419168 -0.6934669 -1.1577296 -1.5085161 -1.6277852 -1.6140501 -1.4660778 -1.2606354 -1.1297929 -1.0971849 -1.1917892][-0.93845892 -0.90832376 -0.85445333 -0.85440922 -0.944525 -1.2517092 -1.6987531 -2.0353189 -2.1494341 -2.1118422 -1.9084446 -1.624927 -1.4287934 -1.3502939 -1.3669162][-0.87379193 -1.0299773 -1.1266613 -1.2778146 -1.5008609 -1.8293543 -2.1887414 -2.4595213 -2.5622759 -2.5174327 -2.2995837 -1.9477456 -1.6334236 -1.4082036 -1.2672653][-0.67860413 -1.0073762 -1.2572892 -1.5490091 -1.8653221 -2.1661952 -2.4075034 -2.5908811 -2.6932144 -2.6938934 -2.5319614 -2.1862652 -1.8105261 -1.4727502 -1.1937313][-0.42997241 -0.87302494 -1.2443192 -1.6240222 -1.9537239 -2.1521897 -2.2421513 -2.3337805 -2.4693861 -2.6042769 -2.6043549 -2.4110837 -2.150826 -1.8594368 -1.5714562][-0.27381277 -0.70280695 -1.064034 -1.3900137 -1.5998602 -1.5966504 -1.4818635 -1.4600983 -1.6720395 -2.0434783 -2.3128757 -2.3804967 -2.341073 -2.200515 -2.0266192][-0.19645309 -0.50018859 -0.75377655 -0.95066428 -0.98334527 -0.7536459 -0.41737795 -0.264266 -0.53558135 -1.1343536 -1.6446288 -1.9202785 -2.0471714 -2.06199 -2.079632][-0.12448835 -0.32228279 -0.49790072 -0.62392116 -0.56716967 -0.24019623 0.19540834 0.42884493 0.15747738 -0.50818634 -1.0545032 -1.3335819 -1.4807191 -1.6141543 -1.8480542][-0.1140132 -0.28408813 -0.46351981 -0.606987 -0.57270861 -0.27735329 0.11523628 0.33828497 0.12788343 -0.41869831 -0.79076886 -0.87952161 -0.92622185 -1.1538248 -1.5737927][-0.22335958 -0.41339731 -0.61805654 -0.78083706 -0.77532411 -0.55189872 -0.2502861 -0.04994154 -0.14330387 -0.47238064 -0.63053894 -0.54555655 -0.53055286 -0.838295 -1.3729422][-0.32491541 -0.51876783 -0.69401836 -0.8170166 -0.79815459 -0.63510752 -0.42704821 -0.26133823 -0.23185682 -0.34205008 -0.36026096 -0.22881222 -0.24180079 -0.63118458 -1.2306294][-0.33325434 -0.51297832 -0.63908744 -0.71780586 -0.70325589 -0.59243345 -0.45586658 -0.32413626 -0.21801805 -0.1737771 -0.13132715 -0.048051357 -0.12724352 -0.57516551 -1.2315297][-0.29089022 -0.49048328 -0.6243186 -0.71618748 -0.74195242 -0.700685 -0.63470125 -0.54330969 -0.40544462 -0.26801014 -0.21306753 -0.20667505 -0.31917286 -0.76041007 -1.4559095][-0.22873211 -0.45656466 -0.63105536 -0.76894069 -0.8607049 -0.9121871 -0.94499445 -0.93356204 -0.82016325 -0.63443232 -0.56736255 -0.60984445 -0.72652555 -1.1204109 -1.7963457][-0.12500906 -0.34384823 -0.5540731 -0.75014043 -0.9257834 -1.0882332 -1.2345738 -1.3153942 -1.2591195 -1.0867205 -1.0415568 -1.1544197 -1.321053 -1.65693 -2.1929173]]...]
INFO - root - 2017-12-07 05:59:45.713958: step 14310, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 1.156 sec/batch; 102h:10m:08s remains)
INFO - root - 2017-12-07 05:59:57.518849: step 14320, loss = 0.69, batch loss = 0.61 (6.7 examples/sec; 1.195 sec/batch; 105h:38m:58s remains)
INFO - root - 2017-12-07 06:00:09.268238: step 14330, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.174 sec/batch; 103h:46m:55s remains)
INFO - root - 2017-12-07 06:00:21.053884: step 14340, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 1.144 sec/batch; 101h:06m:16s remains)
INFO - root - 2017-12-07 06:00:32.732221: step 14350, loss = 0.70, batch loss = 0.62 (6.8 examples/sec; 1.183 sec/batch; 104h:34m:46s remains)
INFO - root - 2017-12-07 06:00:44.133249: step 14360, loss = 0.82, batch loss = 0.75 (7.2 examples/sec; 1.111 sec/batch; 98h:10m:50s remains)
INFO - root - 2017-12-07 06:00:55.778071: step 14370, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.160 sec/batch; 102h:32m:22s remains)
INFO - root - 2017-12-07 06:01:07.494520: step 14380, loss = 0.87, batch loss = 0.80 (6.8 examples/sec; 1.179 sec/batch; 104h:12m:35s remains)
INFO - root - 2017-12-07 06:01:19.186219: step 14390, loss = 0.76, batch loss = 0.68 (6.6 examples/sec; 1.211 sec/batch; 106h:59m:44s remains)
INFO - root - 2017-12-07 06:01:30.885929: step 14400, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 1.149 sec/batch; 101h:34m:01s remains)
2017-12-07 06:01:31.794702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8977437 -2.6433959 -1.6760418 -1.3704925 -2.0324163 -2.8360083 -3.3739257 -3.6747274 -3.5718153 -3.179616 -2.7917204 -2.566031 -2.455111 -2.5158582 -2.8350196][-2.9356866 -2.6933596 -1.7063746 -1.2511013 -1.6679704 -2.2030151 -2.5483131 -2.9445872 -3.0967021 -2.9047134 -2.6237416 -2.4552479 -2.4133904 -2.5594816 -2.9344783][-3.2798021 -3.2415218 -2.4073884 -1.7551141 -1.6554227 -1.6413765 -1.5489662 -1.9244084 -2.4294531 -2.5251899 -2.3416467 -2.217407 -2.2349875 -2.4637539 -2.9041717][-3.0151172 -3.3312118 -2.8944588 -2.3369973 -2.00383 -1.5718429 -0.9697969 -1.2387326 -2.131202 -2.5557525 -2.4515314 -2.2945354 -2.2299404 -2.3980424 -2.8280892][-2.3323364 -2.889781 -2.8308082 -2.6178107 -2.4892926 -1.9719105 -0.9702332 -1.0842967 -2.2960715 -2.9772716 -2.8959403 -2.6882844 -2.4959493 -2.506279 -2.8429251][-1.908644 -2.5035594 -2.54728 -2.5705466 -2.7043529 -2.1620073 -0.84547281 -0.92147326 -2.4560945 -3.2526336 -3.1352198 -2.9502797 -2.7399564 -2.6773052 -2.9429619][-1.7846808 -2.2429624 -2.0888636 -2.0929947 -2.3094456 -1.5626669 0.14549112 0.027026653 -1.8890221 -2.8466461 -2.8389983 -2.8511653 -2.729423 -2.6160841 -2.8187957][-1.9200585 -2.1490409 -1.6670225 -1.4554176 -1.5622361 -0.52058816 1.5737557 1.4344988 -0.686012 -1.7354069 -1.9902859 -2.3817708 -2.4537549 -2.3343542 -2.5196064][-2.1186554 -2.21915 -1.6137996 -1.3201077 -1.3829288 -0.23282623 1.9494982 1.8552489 -0.042172909 -0.88538456 -1.2375975 -1.8739655 -2.042968 -1.8986866 -2.1080451][-2.3190777 -2.5139098 -2.0792313 -1.9434841 -2.1171448 -1.2516613 0.46742249 0.38777924 -0.86073852 -1.0921378 -1.1275003 -1.5490994 -1.5634446 -1.3472059 -1.5853817][-2.3345203 -2.6529634 -2.475409 -2.5098395 -2.7122488 -2.1643317 -1.1273491 -1.331533 -2.0718074 -1.8754888 -1.6307766 -1.705873 -1.4885726 -1.1964407 -1.3799787][-2.1215224 -2.5398402 -2.5857503 -2.642241 -2.6743817 -2.2909524 -1.7815237 -2.0936635 -2.5781422 -2.3399072 -2.0921013 -1.9771426 -1.670882 -1.4623165 -1.5972583][-1.9407167 -2.3597848 -2.4781036 -2.3537953 -2.0735235 -1.771801 -1.6747534 -2.114922 -2.5205755 -2.3948882 -2.2310779 -1.9862194 -1.6174972 -1.5392549 -1.7850301][-2.1045892 -2.4593568 -2.5163147 -2.1218996 -1.6059816 -1.4462667 -1.6967196 -2.2100821 -2.5418391 -2.4226499 -2.2461076 -1.8850915 -1.3839006 -1.3382986 -1.7844064][-2.5199652 -2.6787333 -2.5117817 -1.8892047 -1.3422229 -1.4545414 -1.9738595 -2.414988 -2.6390815 -2.4799128 -2.2474287 -1.8673098 -1.2606192 -1.1379869 -1.7027743]]...]
INFO - root - 2017-12-07 06:01:43.565954: step 14410, loss = 0.81, batch loss = 0.73 (6.7 examples/sec; 1.194 sec/batch; 105h:30m:58s remains)
INFO - root - 2017-12-07 06:01:55.201093: step 14420, loss = 0.89, batch loss = 0.81 (6.5 examples/sec; 1.240 sec/batch; 109h:34m:25s remains)
INFO - root - 2017-12-07 06:02:06.961436: step 14430, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.182 sec/batch; 104h:24m:27s remains)
INFO - root - 2017-12-07 06:02:18.700191: step 14440, loss = 0.69, batch loss = 0.62 (6.8 examples/sec; 1.173 sec/batch; 103h:39m:54s remains)
INFO - root - 2017-12-07 06:02:30.433338: step 14450, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.191 sec/batch; 105h:11m:46s remains)
INFO - root - 2017-12-07 06:02:42.229439: step 14460, loss = 0.58, batch loss = 0.51 (6.9 examples/sec; 1.152 sec/batch; 101h:44m:28s remains)
INFO - root - 2017-12-07 06:02:53.928129: step 14470, loss = 0.78, batch loss = 0.71 (6.6 examples/sec; 1.206 sec/batch; 106h:31m:50s remains)
INFO - root - 2017-12-07 06:03:05.420500: step 14480, loss = 0.89, batch loss = 0.82 (7.1 examples/sec; 1.124 sec/batch; 99h:17m:27s remains)
INFO - root - 2017-12-07 06:03:16.963468: step 14490, loss = 0.87, batch loss = 0.79 (7.0 examples/sec; 1.144 sec/batch; 101h:03m:23s remains)
INFO - root - 2017-12-07 06:03:28.677793: step 14500, loss = 0.88, batch loss = 0.81 (6.7 examples/sec; 1.189 sec/batch; 104h:59m:33s remains)
2017-12-07 06:03:29.580652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0471668 -3.7781689 -3.5986571 -3.3973334 -3.0149002 -2.4720037 -2.2676218 -2.21672 -2.27928 -2.1870961 -1.845346 -1.6990342 -1.799875 -2.0812986 -2.4022853][-4.560617 -4.2105117 -4.0358109 -3.872628 -3.3189106 -2.4571037 -2.0382371 -1.9302835 -2.1100447 -2.1092925 -1.7448971 -1.6316843 -1.7165871 -1.9252386 -2.3067238][-4.7988229 -4.5741081 -4.6592531 -4.6829729 -3.9714248 -2.8096251 -2.24634 -2.1041331 -2.3837702 -2.3823006 -1.8436458 -1.549629 -1.4451613 -1.5577395 -2.0658095][-4.4377074 -4.2976046 -4.6026416 -4.7999706 -4.1312208 -3.0289454 -2.4150541 -2.0968015 -2.3583102 -2.3296995 -1.6137857 -1.1301639 -0.93999863 -1.0880928 -1.7211928][-3.9267275 -3.7915254 -4.0890646 -4.1625204 -3.3653612 -2.256299 -1.485853 -1.0274944 -1.5248525 -1.7963269 -1.2222013 -0.77349925 -0.67889071 -0.92221069 -1.5549963][-3.6674216 -3.673631 -3.9999745 -3.8996596 -2.8917665 -1.5607684 -0.29043674 0.44695377 -0.4046278 -1.1894958 -0.97297144 -0.73611593 -0.93836474 -1.3886206 -1.9606018][-3.6495602 -3.6682868 -3.9053202 -3.6793346 -2.6845059 -1.2431858 0.53630257 1.5997257 0.46765327 -0.8363049 -1.0591421 -1.1029439 -1.6354353 -2.2954142 -2.7528362][-3.6141777 -3.4903166 -3.5407271 -3.188179 -2.2719984 -0.83185768 1.1400456 2.1770144 0.71209145 -0.97062445 -1.4480639 -1.5598278 -2.1135454 -2.7096481 -2.9824047][-3.4875853 -3.2865288 -3.2357783 -2.8978128 -2.1914651 -0.88895583 0.99293995 1.858305 0.42311764 -1.136976 -1.5785613 -1.6544499 -2.0620446 -2.4182422 -2.5032182][-3.4665515 -3.2375879 -3.1544037 -2.9570897 -2.5679922 -1.5659416 -0.1408844 0.42325783 -0.58187222 -1.5304246 -1.6866555 -1.598284 -1.6621411 -1.6835525 -1.6778169][-3.6497729 -3.3436122 -3.1696773 -3.0998902 -2.9993093 -2.3562679 -1.5068777 -1.2449987 -1.8340306 -2.2611721 -2.1777594 -1.8663039 -1.499886 -1.2270553 -1.2515719][-3.7865777 -3.3374581 -3.0451136 -3.0618024 -3.1256335 -2.7420318 -2.305505 -2.1644082 -2.405453 -2.5769997 -2.5696595 -2.4111118 -2.0184655 -1.7683196 -1.8752999][-4.083231 -3.6909783 -3.5055146 -3.607821 -3.5955594 -3.1283631 -2.6981959 -2.42313 -2.4003808 -2.45088 -2.5644217 -2.6886365 -2.5355771 -2.4802608 -2.6404543][-4.4162135 -4.2007661 -4.2413445 -4.4332385 -4.3513913 -3.8559794 -3.3590932 -2.874577 -2.6085353 -2.4701257 -2.5300012 -2.8278434 -2.9131303 -2.9684858 -3.0698876][-4.4563012 -4.4543757 -4.6878595 -4.9723725 -4.9802566 -4.6678219 -4.2282486 -3.723489 -3.446723 -3.2279758 -3.24181 -3.6432519 -3.8660338 -3.8529456 -3.7053216]]...]
INFO - root - 2017-12-07 06:03:41.332665: step 14510, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 1.167 sec/batch; 103h:04m:02s remains)
INFO - root - 2017-12-07 06:03:52.978977: step 14520, loss = 0.63, batch loss = 0.56 (6.9 examples/sec; 1.151 sec/batch; 101h:42m:03s remains)
INFO - root - 2017-12-07 06:04:04.680029: step 14530, loss = 0.89, batch loss = 0.81 (7.1 examples/sec; 1.131 sec/batch; 99h:53m:59s remains)
INFO - root - 2017-12-07 06:04:16.336343: step 14540, loss = 0.63, batch loss = 0.55 (7.5 examples/sec; 1.064 sec/batch; 93h:58m:58s remains)
INFO - root - 2017-12-07 06:04:28.147567: step 14550, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.193 sec/batch; 105h:23m:15s remains)
INFO - root - 2017-12-07 06:04:39.923529: step 14560, loss = 0.71, batch loss = 0.64 (7.0 examples/sec; 1.138 sec/batch; 100h:31m:56s remains)
INFO - root - 2017-12-07 06:04:51.516884: step 14570, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.124 sec/batch; 99h:15m:45s remains)
INFO - root - 2017-12-07 06:05:03.225336: step 14580, loss = 0.97, batch loss = 0.90 (7.2 examples/sec; 1.112 sec/batch; 98h:10m:58s remains)
INFO - root - 2017-12-07 06:05:15.006622: step 14590, loss = 0.74, batch loss = 0.67 (6.7 examples/sec; 1.194 sec/batch; 105h:24m:58s remains)
INFO - root - 2017-12-07 06:05:26.736361: step 14600, loss = 0.61, batch loss = 0.54 (6.6 examples/sec; 1.217 sec/batch; 107h:26m:14s remains)
2017-12-07 06:05:27.500662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3280954 -2.2749734 -1.9568946 -0.96182537 0.054185867 0.4656291 0.20407581 -0.29658604 -0.9015851 -1.7282205 -2.303154 -2.2756476 -2.102716 -1.9309969 -1.6288195][-2.27379 -2.27686 -1.9872251 -1.1294177 -0.24511719 0.20494223 0.083696365 -0.054415226 -0.2901926 -0.88757968 -1.3203595 -1.4207613 -1.5236959 -1.559999 -1.456115][-2.0883045 -2.0647 -1.7458372 -0.98178506 -0.21151161 0.12857199 -0.044007778 -0.042456627 -0.18406582 -0.57737708 -0.68748617 -0.723649 -0.950948 -1.0932846 -1.152868][-2.0551322 -1.9443843 -1.5196531 -0.715466 0.0708437 0.29999304 0.0014638901 -0.060228825 -0.34663963 -0.68219161 -0.50387573 -0.47935438 -0.76217675 -0.91374326 -0.9883647][-1.9467468 -1.7476964 -1.2426939 -0.37339973 0.43333435 0.54921865 0.13663054 -0.061594486 -0.58002424 -0.96298981 -0.66570282 -0.68130016 -0.97800159 -1.0431039 -0.99556708][-1.7069795 -1.4116261 -0.91422033 -0.063990593 0.73056412 0.86995888 0.534019 0.29997063 -0.4306066 -0.96422005 -0.76248574 -0.92022657 -1.2374499 -1.2128568 -1.1012774][-1.6691678 -1.2754738 -0.81007719 -0.0072646141 0.79866314 1.143239 1.1470885 1.0909739 0.2480197 -0.46798849 -0.48134971 -0.87861013 -1.3606234 -1.4150188 -1.4115164][-1.4788826 -1.0363455 -0.66572618 -0.0034542084 0.6899724 1.0820723 1.358325 1.5317802 0.7003684 -0.087922096 -0.22800207 -0.80482745 -1.4793386 -1.6736684 -1.7983918][-1.0502355 -0.62655234 -0.3925724 0.073968887 0.51958895 0.69158506 0.97551537 1.1894922 0.47982407 -0.19981909 -0.35482788 -0.97960567 -1.6789439 -1.85411 -1.949518][-0.56352139 -0.30095911 -0.22833729 0.086446285 0.32506037 0.2568655 0.44658566 0.57528114 -0.080499172 -0.5963769 -0.71837711 -1.2773962 -1.8484266 -1.8893266 -1.8610544][-0.36899948 -0.41812086 -0.51429939 -0.30050898 -0.19686747 -0.36184216 -0.19834042 -0.14911652 -0.75797868 -1.1140993 -1.1863549 -1.6040261 -1.97422 -1.8945844 -1.7983897][-0.65808034 -0.96601772 -1.1468725 -1.0069528 -0.99531674 -1.1555367 -0.9475925 -0.92254853 -1.4309211 -1.6412318 -1.6726449 -1.9441047 -2.1393971 -1.9949059 -1.8660195][-1.221031 -1.641598 -1.819334 -1.7635236 -1.8389997 -1.9395783 -1.667613 -1.6021652 -1.9295557 -2.0357568 -2.0746312 -2.2650697 -2.3752985 -2.2330575 -2.1073263][-1.9190259 -2.2363317 -2.3412611 -2.3459294 -2.4482355 -2.4725966 -2.1845973 -2.0541737 -2.1577654 -2.1722825 -2.2406819 -2.4042645 -2.5207534 -2.4724846 -2.412827][-2.4721208 -2.6208687 -2.64494 -2.669857 -2.7555881 -2.7567997 -2.5675826 -2.4606104 -2.4422536 -2.4272318 -2.5218668 -2.6636796 -2.7849138 -2.8077049 -2.7789259]]...]
INFO - root - 2017-12-07 06:05:39.248095: step 14610, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.131 sec/batch; 99h:52m:26s remains)
INFO - root - 2017-12-07 06:05:50.936838: step 14620, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 1.170 sec/batch; 103h:19m:48s remains)
INFO - root - 2017-12-07 06:06:02.713489: step 14630, loss = 0.69, batch loss = 0.61 (6.5 examples/sec; 1.230 sec/batch; 108h:36m:50s remains)
INFO - root - 2017-12-07 06:06:14.368015: step 14640, loss = 0.78, batch loss = 0.71 (6.7 examples/sec; 1.199 sec/batch; 105h:50m:53s remains)
INFO - root - 2017-12-07 06:06:25.964255: step 14650, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 1.137 sec/batch; 100h:24m:43s remains)
INFO - root - 2017-12-07 06:06:37.556498: step 14660, loss = 0.88, batch loss = 0.81 (7.2 examples/sec; 1.117 sec/batch; 98h:35m:00s remains)
INFO - root - 2017-12-07 06:06:49.138037: step 14670, loss = 0.69, batch loss = 0.62 (7.0 examples/sec; 1.137 sec/batch; 100h:22m:59s remains)
INFO - root - 2017-12-07 06:07:00.850406: step 14680, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 1.176 sec/batch; 103h:49m:26s remains)
INFO - root - 2017-12-07 06:07:12.703304: step 14690, loss = 0.75, batch loss = 0.68 (6.5 examples/sec; 1.222 sec/batch; 107h:54m:32s remains)
INFO - root - 2017-12-07 06:07:24.315580: step 14700, loss = 0.63, batch loss = 0.55 (7.1 examples/sec; 1.132 sec/batch; 99h:57m:21s remains)
2017-12-07 06:07:25.157550: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5077052 -4.5482969 -4.6176586 -4.5204859 -4.2093983 -3.9174476 -3.7750952 -3.6899343 -3.610651 -3.6109154 -3.6988454 -3.7743673 -3.8037176 -3.7659092 -3.7393718][-4.3382406 -4.3328004 -4.406795 -4.3994265 -4.215066 -4.0159817 -3.9241786 -3.8332357 -3.7427194 -3.7346466 -3.8045194 -3.8652444 -3.8710983 -3.8603454 -3.8485658][-3.6103895 -3.4412012 -3.4830294 -3.6054726 -3.6722229 -3.6731029 -3.6702459 -3.5772419 -3.4798491 -3.4277337 -3.4195719 -3.4277894 -3.3836522 -3.3530033 -3.3234515][-2.7227502 -2.4817038 -2.5537333 -2.8032041 -3.0862021 -3.249227 -3.337461 -3.2679586 -3.1588449 -2.9989839 -2.8103149 -2.6748085 -2.4892206 -2.3690188 -2.2856755][-1.7970881 -1.561058 -1.6710889 -1.9545176 -2.2381976 -2.361223 -2.4141717 -2.3493075 -2.2382777 -1.9753535 -1.6112881 -1.3242366 -1.0249128 -0.86870146 -0.80873203][-1.3253493 -1.1022282 -1.1894293 -1.3352289 -1.4129763 -1.3538888 -1.2972078 -1.2195654 -1.1916051 -1.0177884 -0.72193861 -0.46590304 -0.16792202 -0.0065383911 0.022421837][-1.5928559 -1.3522205 -1.3686409 -1.3372965 -1.1612015 -0.91903925 -0.7806921 -0.73053 -0.8935535 -1.0258377 -1.0685399 -1.0762956 -0.89488077 -0.69703078 -0.57136774][-1.9412847 -1.5992615 -1.544395 -1.4819915 -1.2478077 -0.97670889 -0.8311367 -0.83228111 -1.142535 -1.5216944 -1.873203 -2.1713552 -2.1938968 -2.0164354 -1.7943947][-1.9279652 -1.4987366 -1.429219 -1.4840515 -1.4312437 -1.3447287 -1.3270614 -1.3922822 -1.6797464 -1.9864388 -2.2938948 -2.5849266 -2.6721983 -2.5389462 -2.3519967][-1.6910262 -1.2849188 -1.2815146 -1.4650676 -1.6304719 -1.7756042 -1.8769431 -1.9156606 -2.0252976 -2.0895495 -2.1562405 -2.2660489 -2.325846 -2.2454367 -2.1671882][-1.755115 -1.4126632 -1.4132953 -1.5609789 -1.7733123 -2.0082414 -2.1303432 -2.0971153 -2.0676887 -1.9628136 -1.858319 -1.8599865 -1.9287498 -1.9239988 -1.9462745][-2.2779031 -1.9963338 -1.9392538 -1.9300857 -2.0369525 -2.1912987 -2.2623749 -2.1731181 -2.0750115 -1.8792493 -1.6876876 -1.6387911 -1.6813421 -1.7252746 -1.8542137][-2.8231473 -2.5956159 -2.5128107 -2.4068308 -2.4417224 -2.5189204 -2.5577652 -2.4637017 -2.3389139 -2.0869095 -1.83763 -1.725683 -1.6667039 -1.6824749 -1.8595233][-2.9976606 -2.7773008 -2.6905031 -2.5844722 -2.6149063 -2.6612701 -2.6768923 -2.5883751 -2.46488 -2.2060378 -1.9720531 -1.8841941 -1.8091648 -1.811954 -1.9894052][-2.8324368 -2.5771592 -2.488698 -2.41712 -2.4588027 -2.4762526 -2.44902 -2.3617597 -2.284091 -2.1123788 -1.9817309 -2.0162113 -2.0514603 -2.1145723 -2.303652]]...]
INFO - root - 2017-12-07 06:07:36.773194: step 14710, loss = 0.77, batch loss = 0.70 (7.2 examples/sec; 1.114 sec/batch; 98h:20m:49s remains)
INFO - root - 2017-12-07 06:07:48.250012: step 14720, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.169 sec/batch; 103h:09m:28s remains)
INFO - root - 2017-12-07 06:07:59.754674: step 14730, loss = 0.91, batch loss = 0.84 (6.8 examples/sec; 1.171 sec/batch; 103h:24m:05s remains)
INFO - root - 2017-12-07 06:08:11.319969: step 14740, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.132 sec/batch; 99h:52m:42s remains)
INFO - root - 2017-12-07 06:08:22.864924: step 14750, loss = 0.63, batch loss = 0.56 (7.3 examples/sec; 1.091 sec/batch; 96h:18m:13s remains)
INFO - root - 2017-12-07 06:08:34.504535: step 14760, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 1.139 sec/batch; 100h:33m:09s remains)
INFO - root - 2017-12-07 06:08:46.180112: step 14770, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.193 sec/batch; 105h:17m:08s remains)
INFO - root - 2017-12-07 06:08:57.866112: step 14780, loss = 0.80, batch loss = 0.73 (6.7 examples/sec; 1.201 sec/batch; 106h:01m:47s remains)
INFO - root - 2017-12-07 06:09:09.303882: step 14790, loss = 0.88, batch loss = 0.80 (6.8 examples/sec; 1.171 sec/batch; 103h:21m:33s remains)
INFO - root - 2017-12-07 06:09:20.823974: step 14800, loss = 0.74, batch loss = 0.67 (6.8 examples/sec; 1.176 sec/batch; 103h:49m:02s remains)
2017-12-07 06:09:21.760340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.96558213 -1.5174458 -2.1618991 -2.7070651 -3.0960689 -3.2985718 -3.3201132 -3.3303924 -3.3184624 -3.2413244 -3.130403 -3.1164517 -3.2171655 -3.3393369 -3.4521294][-1.1494927 -1.6722586 -2.3464763 -2.88173 -3.2301087 -3.3415966 -3.3346772 -3.44977 -3.6073668 -3.6779461 -3.6799374 -3.6997118 -3.728889 -3.7388883 -3.7444162][-1.4774144 -2.0486734 -2.7621479 -3.2027802 -3.4440222 -3.4858186 -3.476424 -3.5919657 -3.7644019 -3.840775 -3.8416429 -3.8424911 -3.7791002 -3.6899912 -3.6393619][-1.7741971 -2.4182177 -3.0834832 -3.3664556 -3.5146673 -3.5807705 -3.6117382 -3.651931 -3.7414525 -3.7689114 -3.742039 -3.7157376 -3.5806997 -3.4241843 -3.3476028][-1.9481237 -2.4785833 -2.9213133 -3.0659845 -3.194382 -3.3408906 -3.4019985 -3.3152866 -3.320334 -3.3390136 -3.3320708 -3.3475971 -3.2746015 -3.20231 -3.1704085][-1.9077775 -2.084326 -2.1901219 -2.2561424 -2.4073224 -2.5703444 -2.5344183 -2.2826524 -2.2541854 -2.3821664 -2.5499177 -2.7975607 -2.9618931 -3.094476 -3.1670566][-1.556792 -1.3527658 -1.1701624 -1.2064793 -1.3411491 -1.3769929 -1.1310596 -0.77244353 -0.86211276 -1.2173648 -1.6393845 -2.1893098 -2.5963163 -2.8989062 -3.0957515][-0.99826384 -0.68934083 -0.46811628 -0.45665717 -0.46720958 -0.25751972 0.24506235 0.56158257 0.15264893 -0.46480298 -1.0043049 -1.6462843 -2.0756292 -2.3859689 -2.7125988][-0.7646687 -0.60615587 -0.42928243 -0.2844739 -0.10046911 0.21942139 0.69717884 0.77125788 0.10842037 -0.48562479 -0.72862029 -1.0932972 -1.3409784 -1.5845282 -2.0623157][-0.9589684 -0.977468 -0.75901484 -0.4638195 -0.23482704 -0.10104609 0.0216403 -0.22770309 -0.85260129 -1.046916 -0.72316122 -0.64333344 -0.662133 -0.86493373 -1.4623919][-1.4929266 -1.6254466 -1.3848186 -1.060657 -0.87174678 -0.89661765 -0.96197557 -1.237469 -1.6214592 -1.4654286 -0.91621447 -0.69890046 -0.666651 -0.83876944 -1.3887696][-2.2647069 -2.4379191 -2.2379565 -1.9961531 -1.8693459 -1.8280492 -1.7046261 -1.6833074 -1.7797551 -1.5716045 -1.327158 -1.3895769 -1.4946032 -1.634588 -1.9651434][-2.9561892 -3.1681144 -3.0717547 -2.9242361 -2.8326325 -2.641161 -2.2629602 -2.0191214 -2.0383227 -1.98839 -2.1003852 -2.3806849 -2.5478683 -2.6110721 -2.6854534][-3.465219 -3.7179868 -3.7516537 -3.6706576 -3.5568156 -3.2366414 -2.7749414 -2.5604615 -2.6757028 -2.8021228 -3.0912154 -3.4061134 -3.5311399 -3.48066 -3.3209844][-3.7386856 -3.954267 -4.0488205 -4.0029235 -3.9129231 -3.6345801 -3.2916203 -3.2250757 -3.3984804 -3.5723805 -3.8562057 -4.0751324 -4.1011863 -3.970866 -3.6933818]]...]
INFO - root - 2017-12-07 06:09:33.358675: step 14810, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 1.177 sec/batch; 103h:52m:58s remains)
INFO - root - 2017-12-07 06:09:44.850886: step 14820, loss = 0.70, batch loss = 0.63 (7.0 examples/sec; 1.139 sec/batch; 100h:31m:48s remains)
INFO - root - 2017-12-07 06:09:56.377284: step 14830, loss = 0.99, batch loss = 0.91 (6.8 examples/sec; 1.180 sec/batch; 104h:05m:41s remains)
INFO - root - 2017-12-07 06:10:08.035392: step 14840, loss = 0.83, batch loss = 0.75 (7.1 examples/sec; 1.129 sec/batch; 99h:37m:49s remains)
INFO - root - 2017-12-07 06:10:19.544342: step 14850, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 1.129 sec/batch; 99h:35m:46s remains)
INFO - root - 2017-12-07 06:10:31.396034: step 14860, loss = 0.70, batch loss = 0.63 (6.4 examples/sec; 1.253 sec/batch; 110h:34m:46s remains)
INFO - root - 2017-12-07 06:10:43.169254: step 14870, loss = 0.63, batch loss = 0.56 (6.9 examples/sec; 1.167 sec/batch; 102h:58m:46s remains)
INFO - root - 2017-12-07 06:10:54.692985: step 14880, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 1.135 sec/batch; 100h:08m:10s remains)
INFO - root - 2017-12-07 06:11:06.279712: step 14890, loss = 0.85, batch loss = 0.77 (7.3 examples/sec; 1.092 sec/batch; 96h:19m:58s remains)
INFO - root - 2017-12-07 06:11:17.877363: step 14900, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 1.148 sec/batch; 101h:18m:07s remains)
2017-12-07 06:11:18.747531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3764338 -3.4657042 -3.0543098 -2.1088033 -0.27112293 1.5602002 2.4060359 2.139823 1.047472 -0.44444203 -2.0056849 -3.2639375 -4.0280576 -4.2925882 -4.1295676][-3.7116604 -3.8666954 -3.6473567 -3.1763725 -1.9391742 -0.26859713 1.1811109 2.1416597 2.3805594 1.5086503 -0.4268198 -2.4205847 -3.6322155 -4.0526733 -4.0352168][-3.5749192 -3.6538939 -3.5484917 -3.5895026 -3.2175708 -2.3126252 -1.0747116 0.59310246 2.3293133 2.7394042 1.117619 -1.2784398 -2.9693441 -3.5880907 -3.5816612][-2.6076202 -2.5918679 -2.5572753 -3.0220575 -3.5158539 -3.6582541 -3.2711387 -1.8181653 0.6617403 2.4791484 2.0015335 -0.084105492 -2.008508 -2.8512125 -2.8389258][-1.2898386 -1.1429613 -1.0503664 -1.6068459 -2.5481739 -3.5535662 -4.2355833 -3.7873931 -1.6611276 0.98772097 1.9881287 0.85991287 -0.99994969 -2.0961 -2.0857468][-0.61754918 -0.36900711 -0.012734413 -0.08307457 -0.7287941 -1.9719803 -3.4858801 -4.4314189 -3.609365 -1.070811 1.1042156 1.1954904 -0.30435705 -1.6603451 -1.7007766][-0.71232176 -0.39049435 0.328362 1.1120319 1.4780273 0.80437088 -1.1064401 -3.4605403 -4.4523578 -2.984978 -0.49136496 0.63577747 -0.20459175 -1.5806394 -1.7382884][-1.2294309 -1.10443 -0.30767393 1.1739383 2.8319187 3.5328541 2.0209699 -1.2435262 -4.0537372 -4.2878122 -2.3996694 -0.82095861 -0.91131473 -1.9895349 -2.2273929][-2.3850904 -2.6828389 -2.2060437 -0.64007187 1.899334 4.2528124 4.1684809 1.2203407 -2.5618272 -4.3841357 -3.7216988 -2.4735124 -2.2079966 -2.8858721 -3.0799155][-3.8946576 -4.5772495 -4.6664977 -3.6072381 -0.94847941 2.5408425 4.2628336 2.8100047 -0.68739915 -3.4634647 -4.0614171 -3.541976 -3.2576432 -3.5937848 -3.6351314][-3.958576 -4.8119488 -5.4636106 -5.2770438 -3.3850007 0.050201893 2.8252673 3.1026034 0.77598763 -2.1452911 -3.6320035 -3.8260534 -3.6828759 -3.76563 -3.542017][-2.7722142 -3.4112628 -4.2986965 -4.8999963 -4.2148118 -1.8355813 0.94196129 2.4716711 1.564292 -0.88993764 -2.7974114 -3.5399246 -3.6738923 -3.7086937 -3.3415308][-2.0333655 -2.3521981 -3.1491742 -4.0637832 -4.2854376 -3.0051408 -0.6789465 1.386395 1.4855866 -0.30338192 -2.1523128 -3.1141171 -3.4099548 -3.4888024 -3.1707311][-1.9725115 -1.9993896 -2.4390934 -3.1498175 -3.5966554 -2.9519222 -1.2235065 0.62060213 0.93857861 -0.35362434 -1.8470776 -2.572814 -2.5959187 -2.5692244 -2.5347128][-1.913197 -1.6280982 -1.6028647 -1.9856298 -2.4127159 -2.1317313 -0.8347795 0.6628623 0.89949417 -0.15784788 -1.3908868 -1.818449 -1.3518176 -1.042841 -1.3581643]]...]
INFO - root - 2017-12-07 06:11:30.262308: step 14910, loss = 0.75, batch loss = 0.68 (6.4 examples/sec; 1.241 sec/batch; 109h:26m:39s remains)
INFO - root - 2017-12-07 06:11:41.916135: step 14920, loss = 0.66, batch loss = 0.58 (6.7 examples/sec; 1.191 sec/batch; 105h:03m:57s remains)
INFO - root - 2017-12-07 06:11:53.558366: step 14930, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 1.150 sec/batch; 101h:26m:48s remains)
INFO - root - 2017-12-07 06:12:05.265161: step 14940, loss = 0.55, batch loss = 0.48 (7.0 examples/sec; 1.146 sec/batch; 101h:05m:15s remains)
INFO - root - 2017-12-07 06:12:16.795839: step 14950, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.070 sec/batch; 94h:20m:38s remains)
INFO - root - 2017-12-07 06:12:28.485552: step 14960, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 1.172 sec/batch; 103h:23m:10s remains)
INFO - root - 2017-12-07 06:12:40.046697: step 14970, loss = 0.88, batch loss = 0.81 (6.5 examples/sec; 1.231 sec/batch; 108h:33m:52s remains)
INFO - root - 2017-12-07 06:12:51.647710: step 14980, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 1.135 sec/batch; 100h:08m:54s remains)
INFO - root - 2017-12-07 06:13:03.533632: step 14990, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.170 sec/batch; 103h:10m:51s remains)
INFO - root - 2017-12-07 06:13:15.214606: step 15000, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.200 sec/batch; 105h:50m:39s remains)
2017-12-07 06:13:16.084337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.229706 -3.2612228 -3.2862649 -3.234863 -3.173707 -3.1785605 -3.3327632 -3.7740817 -4.2671018 -4.5499873 -4.6037049 -4.3491883 -3.9567568 -3.5394444 -3.2235928][-3.2484918 -3.3192716 -3.3590384 -3.2735407 -3.1061184 -2.9098854 -2.8542235 -3.2937946 -3.997273 -4.5575962 -4.877151 -4.7163014 -4.2390003 -3.6550333 -3.1711268][-3.3164127 -3.4063535 -3.4393086 -3.3504844 -3.1002223 -2.6082649 -2.1744616 -2.4631248 -3.3902678 -4.3917375 -5.1377769 -5.1583543 -4.5938306 -3.8295357 -3.1766124][-3.3858602 -3.4761469 -3.5051367 -3.4472518 -3.1322498 -2.3468695 -1.4783764 -1.5070624 -2.598175 -4.0986452 -5.3432617 -5.6220202 -5.015758 -4.0844259 -3.2622359][-3.3934519 -3.4507701 -3.4728632 -3.4102464 -3.0149803 -2.0672088 -0.9879086 -0.85045218 -2.0025337 -3.8565965 -5.493372 -6.0510077 -5.5351381 -4.4989142 -3.4953187][-3.2820563 -3.227762 -3.1379995 -2.8980303 -2.3727019 -1.5122764 -0.59702063 -0.55124092 -1.7886364 -3.8063765 -5.5941954 -6.3453331 -6.0189095 -4.991344 -3.8482158][-2.9381773 -2.7161965 -2.4283049 -1.933732 -1.340008 -0.72549629 -0.088147163 -0.1290803 -1.3757043 -3.4221218 -5.258286 -6.182333 -6.1176376 -5.2427473 -4.0899925][-2.2242463 -1.7307551 -1.211422 -0.58009291 -0.10896587 0.20552349 0.6792841 0.7054534 -0.38960218 -2.308646 -4.1312118 -5.3195033 -5.6651645 -5.08058 -4.0761533][-1.5033908 -0.67307925 0.070919991 0.64116669 0.77371979 0.78569984 1.1844568 1.3296738 0.5404191 -0.999017 -2.6691203 -4.1352358 -4.9324965 -4.7219281 -3.9706924][-1.2991836 -0.30725622 0.48032379 0.80558395 0.56522417 0.40440416 0.79252386 1.066359 0.67394876 -0.32853842 -1.7183235 -3.2937646 -4.3560624 -4.4417906 -3.9515681][-1.6445217 -0.78660178 -0.18762779 -0.17467213 -0.67280245 -0.92798471 -0.61069775 -0.32075405 -0.3873477 -0.86525369 -1.8841116 -3.2583075 -4.218328 -4.3801632 -4.04875][-2.2739587 -1.7120128 -1.3892882 -1.5678484 -2.1099217 -2.3734734 -2.1877263 -1.9822888 -1.9115441 -2.0837386 -2.7513852 -3.7195568 -4.3647733 -4.4474893 -4.156292][-2.9597442 -2.7065654 -2.6062331 -2.8082342 -3.2244508 -3.4423707 -3.3791673 -3.2896457 -3.2283254 -3.2827797 -3.6671951 -4.2245808 -4.53583 -4.4729667 -4.1453629][-3.3674581 -3.314563 -3.3444266 -3.5143249 -3.785064 -3.942091 -3.9514337 -3.9200926 -3.8647926 -3.8712573 -4.0737052 -4.3451047 -4.4385586 -4.2875876 -3.9612415][-3.4689364 -3.5136635 -3.5928805 -3.711951 -3.8568447 -3.9438686 -3.9584091 -3.9326708 -3.8913655 -3.8859732 -3.961395 -4.0518484 -4.0519123 -3.9117644 -3.6747909]]...]
INFO - root - 2017-12-07 06:13:27.806240: step 15010, loss = 0.78, batch loss = 0.71 (6.5 examples/sec; 1.237 sec/batch; 109h:05m:50s remains)
INFO - root - 2017-12-07 06:13:39.484654: step 15020, loss = 0.70, batch loss = 0.62 (7.1 examples/sec; 1.123 sec/batch; 99h:04m:46s remains)
INFO - root - 2017-12-07 06:13:51.180375: step 15030, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.125 sec/batch; 99h:11m:01s remains)
INFO - root - 2017-12-07 06:14:02.883762: step 15040, loss = 0.75, batch loss = 0.68 (7.0 examples/sec; 1.143 sec/batch; 100h:48m:00s remains)
INFO - root - 2017-12-07 06:14:14.715248: step 15050, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.202 sec/batch; 105h:58m:14s remains)
INFO - root - 2017-12-07 06:14:26.431077: step 15060, loss = 0.60, batch loss = 0.53 (6.5 examples/sec; 1.240 sec/batch; 109h:20m:40s remains)
INFO - root - 2017-12-07 06:14:38.096143: step 15070, loss = 0.91, batch loss = 0.84 (7.2 examples/sec; 1.111 sec/batch; 97h:59m:06s remains)
INFO - root - 2017-12-07 06:14:49.857360: step 15080, loss = 0.61, batch loss = 0.53 (7.0 examples/sec; 1.151 sec/batch; 101h:27m:46s remains)
INFO - root - 2017-12-07 06:15:01.424280: step 15090, loss = 0.91, batch loss = 0.84 (6.9 examples/sec; 1.158 sec/batch; 102h:04m:22s remains)
INFO - root - 2017-12-07 06:15:13.103193: step 15100, loss = 0.75, batch loss = 0.67 (7.0 examples/sec; 1.147 sec/batch; 101h:07m:21s remains)
2017-12-07 06:15:14.043622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0410709 -4.3176732 -4.1777883 -3.7630365 -3.1814404 -2.1779411 -1.3458922 -1.3587856 -1.9209604 -2.850246 -3.8901715 -4.4886456 -4.6362872 -4.487041 -4.2186747][-4.3318534 -4.7483273 -4.6280231 -4.2469125 -3.5838907 -2.1534677 -0.72493672 -0.44722676 -1.1813776 -2.4606855 -3.7232852 -4.3748274 -4.6150985 -4.6283555 -4.4132628][-4.51233 -4.975131 -4.8576131 -4.5457883 -3.8407021 -2.0004275 0.0881629 0.85213614 -0.013231754 -1.775197 -3.4132261 -4.2102666 -4.5691576 -4.7497849 -4.6378536][-4.5968466 -5.0287614 -4.9002585 -4.6402874 -3.867666 -1.7572973 0.81693172 2.1321907 1.3080916 -0.92255735 -3.0293863 -4.1621737 -4.7388043 -4.9678779 -4.8278227][-4.6542277 -4.9947915 -4.8112473 -4.5398369 -3.7249565 -1.5759881 1.1540594 3.0106893 2.5675197 0.2285862 -2.1941633 -3.8314357 -4.8732648 -5.15153 -4.8588514][-4.715683 -4.95036 -4.6251965 -4.1585021 -3.3083596 -1.3334215 1.2401443 3.3721924 3.4610319 1.4202185 -1.0615563 -3.2583385 -4.9156165 -5.2781787 -4.7823668][-4.8566632 -5.0942717 -4.5630069 -3.7066643 -2.7087898 -1.0082996 1.1534357 3.1820598 3.7126083 2.1852307 -0.16023445 -2.7886024 -4.9143586 -5.3242435 -4.6306458][-5.0582557 -5.4233637 -4.726151 -3.4642396 -2.2585099 -0.78521609 0.86133289 2.4199367 3.0793419 2.0860429 0.033297062 -2.7346306 -5.0199881 -5.377594 -4.5323963][-5.2108359 -5.7418051 -5.0411983 -3.5459957 -2.132025 -0.77536178 0.41102982 1.3242774 1.7850809 1.0851445 -0.68328691 -3.3205695 -5.4330025 -5.5970206 -4.6377373][-5.1656179 -5.7682366 -5.255167 -3.7780313 -2.186883 -0.80228281 0.035973549 0.24924612 0.29682302 -0.34575367 -1.8744748 -4.1120534 -5.7534871 -5.6877346 -4.7120633][-4.9008145 -5.4625916 -5.2565613 -4.0017719 -2.3696663 -0.95730257 -0.41013765 -0.785007 -1.1394889 -1.8313131 -3.0683579 -4.6148481 -5.5900507 -5.3926764 -4.588707][-4.6031609 -5.0544677 -5.149497 -4.2679596 -2.7862303 -1.4825623 -1.0723403 -1.6418982 -2.2134802 -3.0477512 -4.0493469 -4.8294578 -5.1358685 -4.9395523 -4.4599938][-4.3124552 -4.6184845 -4.8500814 -4.3705368 -3.2246354 -2.0983636 -1.6112428 -1.9995856 -2.7039404 -3.7476897 -4.5906959 -4.7925496 -4.6619325 -4.5541744 -4.390512][-4.0380969 -4.2078223 -4.4173665 -4.2784195 -3.5940418 -2.7304749 -2.1323411 -2.2772009 -3.0287342 -4.074152 -4.691165 -4.5865974 -4.3166184 -4.3123555 -4.3224154][-3.8427315 -3.9382811 -4.0944929 -4.2046537 -3.9601772 -3.4322207 -2.8934059 -2.8845916 -3.4610028 -4.1423206 -4.4483571 -4.2833042 -4.0807295 -4.1252079 -4.1637516]]...]
INFO - root - 2017-12-07 06:15:25.628211: step 15110, loss = 0.56, batch loss = 0.49 (7.4 examples/sec; 1.076 sec/batch; 94h:51m:06s remains)
INFO - root - 2017-12-07 06:15:37.387694: step 15120, loss = 0.81, batch loss = 0.74 (6.5 examples/sec; 1.238 sec/batch; 109h:09m:16s remains)
INFO - root - 2017-12-07 06:15:49.143532: step 15130, loss = 0.91, batch loss = 0.84 (6.8 examples/sec; 1.180 sec/batch; 103h:59m:40s remains)
INFO - root - 2017-12-07 06:16:00.936507: step 15140, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.130 sec/batch; 99h:37m:54s remains)
INFO - root - 2017-12-07 06:16:12.524560: step 15150, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 1.141 sec/batch; 100h:33m:36s remains)
INFO - root - 2017-12-07 06:16:24.165074: step 15160, loss = 0.79, batch loss = 0.71 (6.7 examples/sec; 1.193 sec/batch; 105h:07m:50s remains)
INFO - root - 2017-12-07 06:16:35.975561: step 15170, loss = 0.89, batch loss = 0.82 (6.8 examples/sec; 1.178 sec/batch; 103h:51m:03s remains)
INFO - root - 2017-12-07 06:16:47.690298: step 15180, loss = 0.71, batch loss = 0.63 (6.7 examples/sec; 1.190 sec/batch; 104h:51m:44s remains)
INFO - root - 2017-12-07 06:16:59.340239: step 15190, loss = 0.67, batch loss = 0.60 (6.8 examples/sec; 1.179 sec/batch; 103h:57m:33s remains)
INFO - root - 2017-12-07 06:17:11.057750: step 15200, loss = 0.60, batch loss = 0.53 (7.2 examples/sec; 1.111 sec/batch; 97h:54m:14s remains)
2017-12-07 06:17:11.961622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7468781 -2.793689 -2.721278 -2.6502843 -2.7156432 -3.0178294 -3.3443689 -3.5105882 -3.8397012 -3.9855187 -3.9734771 -3.9149327 -3.6933217 -3.452384 -3.26195][-2.6144168 -2.6270735 -2.5817132 -2.5192528 -2.6095128 -2.9569452 -3.3241441 -3.5534198 -3.8502302 -3.8082175 -3.6341498 -3.6152921 -3.5365081 -3.4058218 -3.3005924][-2.6164875 -2.6020155 -2.5820851 -2.552628 -2.6232462 -2.9178553 -3.2229824 -3.3939571 -3.5531847 -3.3334894 -3.1131871 -3.226181 -3.3353715 -3.3419447 -3.3230546][-2.6914091 -2.63033 -2.6588778 -2.7305889 -2.8114493 -3.0794616 -3.3076627 -3.3252273 -3.3013959 -2.9977298 -2.8135896 -2.9911675 -3.1708417 -3.257442 -3.2809181][-2.6917624 -2.51304 -2.5856118 -2.7892683 -2.8971872 -3.1361399 -3.2955151 -3.1519482 -3.0186062 -2.7899456 -2.6841786 -2.8612187 -3.0637059 -3.1865745 -3.1954577][-2.661417 -2.3192472 -2.394738 -2.6676068 -2.7633142 -2.951961 -3.054147 -2.7886772 -2.651545 -2.6541376 -2.7050047 -2.909174 -3.1285806 -3.2118039 -3.1345665][-2.6044168 -2.1197107 -2.1793368 -2.469775 -2.5483589 -2.6877706 -2.7355845 -2.3721766 -2.27468 -2.5372045 -2.823122 -3.1584861 -3.3964415 -3.3702493 -3.1350074][-2.5956075 -2.0599113 -2.0741513 -2.3442695 -2.4543688 -2.6079655 -2.594733 -2.1610043 -2.0720451 -2.4952507 -3.0250969 -3.50205 -3.6917319 -3.522047 -3.1632285][-2.496681 -2.033659 -2.048867 -2.26391 -2.3846328 -2.4969058 -2.3539593 -1.8734956 -1.8780775 -2.4928994 -3.2763276 -3.8363974 -3.9128351 -3.6143675 -3.2136664][-2.4913402 -2.18624 -2.2410386 -2.3634167 -2.4187059 -2.4106772 -2.0974197 -1.6874139 -1.9137511 -2.7021213 -3.5603695 -4.0609827 -4.00247 -3.6487777 -3.2876091][-2.9442673 -2.8324122 -2.8972883 -2.8488114 -2.7316384 -2.5761886 -2.2228723 -1.9990616 -2.4343934 -3.230181 -3.9291625 -4.2583833 -4.0972061 -3.7246146 -3.3945498][-3.6415427 -3.75527 -3.8055363 -3.5583653 -3.264451 -3.0654984 -2.8367047 -2.7950783 -3.2315731 -3.8169928 -4.2239985 -4.3707695 -4.1523256 -3.7855129 -3.4718516][-3.8279285 -4.0104027 -4.0218844 -3.7333033 -3.5022583 -3.4899678 -3.5011973 -3.5501776 -3.8258038 -4.1325207 -4.272943 -4.288372 -4.0794063 -3.7552063 -3.4785848][-3.5684543 -3.5426266 -3.4013791 -3.1803308 -3.1950872 -3.459228 -3.666687 -3.7498331 -3.8564956 -3.9436052 -3.9428234 -3.9500117 -3.8391032 -3.62792 -3.4333906][-3.3145428 -3.0401649 -2.7540531 -2.6332808 -2.8450608 -3.2477658 -3.5024502 -3.5655227 -3.5786386 -3.5436921 -3.5121295 -3.5750139 -3.5788884 -3.4831898 -3.3631368]]...]
INFO - root - 2017-12-07 06:17:23.607428: step 15210, loss = 0.76, batch loss = 0.68 (6.7 examples/sec; 1.201 sec/batch; 105h:49m:33s remains)
INFO - root - 2017-12-07 06:17:35.187194: step 15220, loss = 0.68, batch loss = 0.61 (6.7 examples/sec; 1.200 sec/batch; 105h:47m:32s remains)
INFO - root - 2017-12-07 06:17:46.925112: step 15230, loss = 0.72, batch loss = 0.64 (6.8 examples/sec; 1.177 sec/batch; 103h:43m:58s remains)
INFO - root - 2017-12-07 06:17:58.584577: step 15240, loss = 0.70, batch loss = 0.63 (6.8 examples/sec; 1.174 sec/batch; 103h:27m:32s remains)
INFO - root - 2017-12-07 06:18:10.093184: step 15250, loss = 0.76, batch loss = 0.68 (7.3 examples/sec; 1.096 sec/batch; 96h:36m:40s remains)
INFO - root - 2017-12-07 06:18:21.637728: step 15260, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 1.142 sec/batch; 100h:38m:05s remains)
INFO - root - 2017-12-07 06:18:33.310706: step 15270, loss = 0.80, batch loss = 0.73 (6.6 examples/sec; 1.212 sec/batch; 106h:49m:54s remains)
INFO - root - 2017-12-07 06:18:45.014144: step 15280, loss = 0.80, batch loss = 0.73 (6.6 examples/sec; 1.210 sec/batch; 106h:36m:12s remains)
INFO - root - 2017-12-07 06:18:56.582137: step 15290, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 1.146 sec/batch; 101h:00m:20s remains)
INFO - root - 2017-12-07 06:19:08.227347: step 15300, loss = 0.72, batch loss = 0.65 (6.6 examples/sec; 1.212 sec/batch; 106h:44m:57s remains)
2017-12-07 06:19:09.129201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1715457 -2.7967772 -3.0989213 -2.775501 -2.1416278 -1.4063158 -1.2272575 -1.5643659 -1.8128471 -1.6819851 -1.1250124 -0.59844255 -1.0658445 -1.9293396 -1.6596532][-2.301944 -2.997571 -3.1435781 -2.7553747 -2.2144172 -1.6765783 -1.6455917 -1.8729475 -1.8505471 -1.435308 -0.74635434 -0.30337477 -0.96885157 -1.9888093 -1.9422224][-2.1149204 -2.7201281 -2.7861362 -2.4928842 -2.1090319 -1.7619669 -1.8647435 -2.0302112 -1.8336017 -1.2781131 -0.53668904 -0.1237855 -0.90778589 -2.0451617 -2.2004831][-1.7713697 -2.2234867 -2.2664862 -2.1287186 -1.8120811 -1.459024 -1.5204871 -1.6166639 -1.438098 -1.0362029 -0.44953275 -0.075609684 -0.9110465 -2.1444073 -2.4841661][-1.4236703 -1.7823195 -1.8567522 -1.8201478 -1.4807169 -1.0193381 -0.94220877 -0.90624475 -0.84190392 -0.77892113 -0.40828323 -0.060843945 -0.90916228 -2.2114744 -2.7368236][-1.1553264 -1.4691951 -1.5779345 -1.6427691 -1.3635225 -0.93846154 -0.73627257 -0.48410606 -0.44023848 -0.62343812 -0.37363672 -0.027284145 -0.87643981 -2.2567966 -2.9634664][-1.0158229 -1.2614255 -1.3721697 -1.4931586 -1.3644235 -1.1347332 -0.85760617 -0.38144398 -0.21666908 -0.43185902 -0.18698263 0.10243607 -0.79714656 -2.2414742 -3.07169][-0.88058257 -1.0200496 -1.0513659 -1.1847374 -1.2786033 -1.3201363 -1.0497317 -0.45545268 -0.16007233 -0.24989939 0.019604206 0.20122957 -0.71934223 -2.0969367 -2.8979158][-0.99218535 -0.99091721 -0.88356519 -1.001776 -1.2894959 -1.5513024 -1.3435633 -0.73829389 -0.40312767 -0.34977388 -0.029411793 0.11233473 -0.68926167 -1.8418455 -2.4720047][-1.5074968 -1.4182811 -1.2271352 -1.2875736 -1.600342 -1.876889 -1.6897566 -1.1636994 -0.88906431 -0.79058719 -0.46179318 -0.31034756 -0.87276053 -1.6713738 -2.0744357][-2.1729007 -2.0317922 -1.7769713 -1.7475948 -1.9586647 -2.148314 -1.9773216 -1.6008332 -1.4513161 -1.4070106 -1.1501491 -0.98191929 -1.3061779 -1.793653 -2.0300426][-2.7274749 -2.5221627 -2.2294691 -2.1453142 -2.2826712 -2.4328985 -2.3462248 -2.1403215 -2.090399 -2.0851943 -1.9225602 -1.7935417 -1.9723482 -2.2397938 -2.3721838][-3.0830026 -2.8956382 -2.64532 -2.5592012 -2.6504025 -2.7864745 -2.7949934 -2.70926 -2.6885409 -2.6838517 -2.6027734 -2.540081 -2.6352148 -2.7613106 -2.8368368][-2.9575105 -2.8436494 -2.699861 -2.6379681 -2.6743727 -2.7701216 -2.8287911 -2.8140335 -2.7963281 -2.7905059 -2.76398 -2.742733 -2.7844944 -2.8364935 -2.8915308][-2.5243773 -2.4547396 -2.3766742 -2.3186677 -2.2878261 -2.3345652 -2.401623 -2.4183857 -2.3981092 -2.4005995 -2.4158466 -2.4245212 -2.4601755 -2.5020766 -2.5727234]]...]
INFO - root - 2017-12-07 06:19:20.970950: step 15310, loss = 0.81, batch loss = 0.74 (6.7 examples/sec; 1.185 sec/batch; 104h:26m:53s remains)
INFO - root - 2017-12-07 06:19:32.716282: step 15320, loss = 0.91, batch loss = 0.84 (6.4 examples/sec; 1.240 sec/batch; 109h:17m:39s remains)
INFO - root - 2017-12-07 06:19:44.239889: step 15330, loss = 0.60, batch loss = 0.53 (7.1 examples/sec; 1.128 sec/batch; 99h:21m:19s remains)
INFO - root - 2017-12-07 06:19:55.813183: step 15340, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 1.117 sec/batch; 98h:26m:49s remains)
INFO - root - 2017-12-07 06:20:07.442761: step 15350, loss = 0.79, batch loss = 0.72 (6.8 examples/sec; 1.170 sec/batch; 103h:06m:33s remains)
INFO - root - 2017-12-07 06:20:19.172824: step 15360, loss = 0.70, batch loss = 0.62 (6.5 examples/sec; 1.234 sec/batch; 108h:41m:18s remains)
INFO - root - 2017-12-07 06:20:30.872598: step 15370, loss = 0.72, batch loss = 0.65 (6.6 examples/sec; 1.218 sec/batch; 107h:19m:30s remains)
INFO - root - 2017-12-07 06:20:42.576171: step 15380, loss = 0.56, batch loss = 0.49 (6.8 examples/sec; 1.184 sec/batch; 104h:16m:52s remains)
INFO - root - 2017-12-07 06:20:53.985698: step 15390, loss = 0.71, batch loss = 0.63 (7.8 examples/sec; 1.031 sec/batch; 90h:49m:17s remains)
INFO - root - 2017-12-07 06:21:05.767948: step 15400, loss = 0.78, batch loss = 0.71 (7.1 examples/sec; 1.131 sec/batch; 99h:36m:27s remains)
2017-12-07 06:21:06.635775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4973402 -3.5370533 -3.5603895 -3.5443411 -3.4692478 -3.3528602 -3.2606606 -3.2653103 -3.4299502 -3.6202512 -3.7057209 -3.6881719 -3.5951078 -3.3558209 -2.986495][-3.4896557 -3.5430069 -3.5952859 -3.5999017 -3.4946125 -3.2767212 -3.0474796 -2.9407973 -3.120913 -3.4012904 -3.5352192 -3.4762187 -3.2680764 -2.9034057 -2.4675417][-3.4634886 -3.4907889 -3.5289431 -3.5248818 -3.3733308 -3.0577168 -2.68997 -2.4562931 -2.6697083 -3.0920253 -3.3247275 -3.2446427 -2.9128726 -2.4343958 -1.9326549][-3.4238544 -3.3764296 -3.3256407 -3.2459893 -3.0209234 -2.6238122 -2.1641853 -1.8810015 -2.172569 -2.7285724 -3.043972 -2.9293737 -2.4814186 -1.8796728 -1.2392254][-3.4325809 -3.2779312 -3.0616128 -2.8220141 -2.466903 -1.978225 -1.4497166 -1.168437 -1.6229458 -2.3998497 -2.9072003 -2.867816 -2.3661816 -1.6350935 -0.80947232][-3.5280061 -3.2608421 -2.8429446 -2.3791366 -1.874589 -1.3176117 -0.72608137 -0.46297264 -1.120924 -2.16984 -2.9374332 -3.1060741 -2.7109993 -2.0297658 -1.2173133][-3.5994928 -3.2228131 -2.5989523 -1.8869789 -1.1948109 -0.51591611 0.18846941 0.43335295 -0.45183659 -1.7177351 -2.6332664 -2.9683099 -2.7913766 -2.3756685 -1.7436097][-3.7057772 -3.3224416 -2.6200626 -1.7629421 -0.90734243 -0.0720849 0.83833027 1.1464972 0.16058588 -1.2015197 -2.1931784 -2.6146989 -2.5829308 -2.3906424 -1.9748425][-3.8309054 -3.574635 -2.9741197 -2.1783047 -1.3856227 -0.58509207 0.30246162 0.6182251 -0.13157558 -1.2938066 -2.2388387 -2.6985548 -2.7102041 -2.5132222 -2.1192462][-3.7818954 -3.6157489 -3.1454372 -2.5225968 -1.9075027 -1.2293625 -0.51716208 -0.33755541 -0.88097763 -1.7301824 -2.5709822 -3.0637169 -3.0727324 -2.728056 -2.125608][-3.6978502 -3.5211098 -3.0954328 -2.5927007 -2.1434891 -1.6380804 -1.2067351 -1.2797728 -1.7099922 -2.1913791 -2.7689776 -3.190062 -3.17673 -2.6981359 -1.9273612][-3.7280264 -3.5250692 -3.0925708 -2.6030064 -2.2144339 -1.8757792 -1.7943974 -2.2032497 -2.6569855 -2.8547683 -3.0359526 -3.1019902 -2.8585553 -2.2735264 -1.4963865][-3.7185695 -3.4764142 -3.0008531 -2.443532 -1.999526 -1.6556647 -1.712225 -2.335433 -2.9727769 -3.2335629 -3.2543917 -2.978704 -2.4023683 -1.6017816 -0.76465988][-3.551914 -3.2406695 -2.6994052 -2.0901957 -1.6048381 -1.2412834 -1.3642626 -2.0981894 -2.8890758 -3.2768531 -3.2445321 -2.7643602 -2.0205784 -1.0062311 0.058558464][-3.2510166 -2.8559766 -2.2504265 -1.615176 -1.0896647 -0.715816 -0.97994852 -1.879348 -2.8277283 -3.3590612 -3.3087854 -2.733202 -1.9560573 -0.89909649 0.25840664]]...]
INFO - root - 2017-12-07 06:21:18.388791: step 15410, loss = 0.74, batch loss = 0.67 (6.6 examples/sec; 1.213 sec/batch; 106h:50m:03s remains)
INFO - root - 2017-12-07 06:21:30.004809: step 15420, loss = 0.73, batch loss = 0.65 (7.0 examples/sec; 1.136 sec/batch; 100h:04m:34s remains)
INFO - root - 2017-12-07 06:21:41.402976: step 15430, loss = 0.86, batch loss = 0.79 (6.9 examples/sec; 1.159 sec/batch; 102h:05m:27s remains)
INFO - root - 2017-12-07 06:21:53.152395: step 15440, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.158 sec/batch; 101h:57m:42s remains)
INFO - root - 2017-12-07 06:22:04.611391: step 15450, loss = 0.64, batch loss = 0.57 (7.3 examples/sec; 1.100 sec/batch; 96h:50m:47s remains)
INFO - root - 2017-12-07 06:22:16.251550: step 15460, loss = 0.96, batch loss = 0.89 (6.9 examples/sec; 1.160 sec/batch; 102h:07m:56s remains)
INFO - root - 2017-12-07 06:22:27.799440: step 15470, loss = 0.70, batch loss = 0.63 (7.4 examples/sec; 1.076 sec/batch; 94h:47m:08s remains)
INFO - root - 2017-12-07 06:22:39.460069: step 15480, loss = 0.89, batch loss = 0.82 (6.7 examples/sec; 1.198 sec/batch; 105h:28m:03s remains)
INFO - root - 2017-12-07 06:22:51.112597: step 15490, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.143 sec/batch; 100h:39m:23s remains)
INFO - root - 2017-12-07 06:23:02.838561: step 15500, loss = 0.64, batch loss = 0.57 (6.6 examples/sec; 1.214 sec/batch; 106h:53m:51s remains)
2017-12-07 06:23:03.660157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6543531 -1.8535016 -2.0946164 -2.2347162 -2.3842938 -2.5045986 -2.512434 -2.5455966 -2.5686684 -2.5695181 -2.3984959 -2.2284627 -2.3206823 -2.3931184 -2.2557814][-1.3412864 -1.6174357 -1.9050863 -2.0573039 -2.1777062 -2.28423 -2.2925179 -2.3111858 -2.3165767 -2.2901258 -2.122174 -2.0162323 -2.1576898 -2.1491892 -1.9099095][-1.2149723 -1.5438404 -1.7981606 -1.8700306 -1.8707149 -1.892535 -1.9034908 -1.9391463 -1.9359529 -1.8664353 -1.7497215 -1.7843866 -1.9563928 -1.8510125 -1.5764751][-1.2813406 -1.5716629 -1.7051125 -1.6537378 -1.5076532 -1.4121187 -1.4418366 -1.495383 -1.4235022 -1.2801549 -1.288301 -1.5253265 -1.7039351 -1.5132506 -1.2085812][-1.5978987 -1.7514174 -1.7136838 -1.5360363 -1.24336 -1.060714 -1.1210947 -1.1309397 -0.85789013 -0.55949783 -0.68343067 -1.1168435 -1.3229821 -1.0960383 -0.73090243][-1.9952595 -2.0163949 -1.8341482 -1.5232985 -1.0912783 -0.88496065 -0.99626923 -0.9490025 -0.48627734 -0.069782257 -0.28654909 -0.77444887 -0.94835281 -0.72236872 -0.37785912][-2.269304 -2.1729686 -1.9132133 -1.5458283 -1.1101651 -0.98703265 -1.138654 -1.0333087 -0.48771691 -0.10789442 -0.43203259 -0.88806319 -0.9644773 -0.7628634 -0.50191092][-2.4559255 -2.2790227 -1.9687624 -1.6141801 -1.3131511 -1.3223543 -1.4738021 -1.3273239 -0.77931952 -0.49502468 -0.89582682 -1.2613592 -1.2509093 -1.1562898 -1.0215895][-2.5256047 -2.3090322 -1.9632683 -1.6510041 -1.5087252 -1.5963786 -1.7449675 -1.6699181 -1.1892023 -0.947217 -1.275964 -1.4660256 -1.4222736 -1.4788048 -1.4383228][-2.6668143 -2.4414968 -2.0761042 -1.8030345 -1.7235222 -1.7637072 -1.8846767 -1.9144928 -1.5270114 -1.2680559 -1.4285898 -1.4487114 -1.4444423 -1.59991 -1.5649903][-2.9988723 -2.7744045 -2.3758919 -2.0657518 -1.8787498 -1.7818861 -1.8548768 -1.9343362 -1.6238527 -1.3944991 -1.4787171 -1.4474304 -1.5069568 -1.6861892 -1.6389282][-3.2916746 -3.1087489 -2.7064023 -2.3123162 -1.9627249 -1.7497728 -1.7643478 -1.8015931 -1.516155 -1.3656566 -1.4646916 -1.4599478 -1.5578408 -1.7250605 -1.711992][-3.3362579 -3.232677 -2.8693836 -2.4081562 -1.9375765 -1.6445153 -1.5771384 -1.5238013 -1.2973776 -1.2716076 -1.3843963 -1.3713224 -1.4453509 -1.5840933 -1.6063974][-3.1848493 -3.1549716 -2.8611121 -2.4111717 -1.9245896 -1.5910323 -1.4399319 -1.3355584 -1.2236447 -1.3040485 -1.3672879 -1.2854178 -1.2646947 -1.3042541 -1.303077][-3.0586631 -3.0823457 -2.8838935 -2.5636635 -2.2224157 -1.9504173 -1.7625732 -1.6342394 -1.603554 -1.6980758 -1.6981268 -1.5791605 -1.4904389 -1.4320099 -1.3700364]]...]
INFO - root - 2017-12-07 06:23:15.256234: step 15510, loss = 0.64, batch loss = 0.57 (7.6 examples/sec; 1.051 sec/batch; 92h:30m:07s remains)
INFO - root - 2017-12-07 06:23:26.989536: step 15520, loss = 0.70, batch loss = 0.63 (6.8 examples/sec; 1.181 sec/batch; 104h:00m:45s remains)
INFO - root - 2017-12-07 06:23:38.518005: step 15530, loss = 0.87, batch loss = 0.80 (7.2 examples/sec; 1.105 sec/batch; 97h:16m:14s remains)
INFO - root - 2017-12-07 06:23:50.111289: step 15540, loss = 0.74, batch loss = 0.66 (6.7 examples/sec; 1.187 sec/batch; 104h:29m:01s remains)
INFO - root - 2017-12-07 06:24:01.731817: step 15550, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 1.153 sec/batch; 101h:30m:17s remains)
INFO - root - 2017-12-07 06:24:13.373693: step 15560, loss = 0.64, batch loss = 0.57 (7.2 examples/sec; 1.117 sec/batch; 98h:20m:51s remains)
INFO - root - 2017-12-07 06:24:25.107759: step 15570, loss = 0.87, batch loss = 0.80 (6.8 examples/sec; 1.174 sec/batch; 103h:21m:24s remains)
INFO - root - 2017-12-07 06:24:36.810526: step 15580, loss = 0.61, batch loss = 0.54 (6.6 examples/sec; 1.214 sec/batch; 106h:54m:01s remains)
INFO - root - 2017-12-07 06:24:48.403437: step 15590, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.152 sec/batch; 101h:25m:47s remains)
INFO - root - 2017-12-07 06:25:00.118779: step 15600, loss = 0.66, batch loss = 0.59 (6.4 examples/sec; 1.244 sec/batch; 109h:28m:38s remains)
2017-12-07 06:25:01.007654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3284824 -2.4951274 -2.6126971 -2.523571 -2.2458005 -2.2155921 -2.7262869 -3.4536748 -3.9410594 -3.9890509 -3.6432023 -3.0884111 -2.3961706 -1.4009483 -0.67856073][-2.3950031 -2.5500002 -2.6141195 -2.4575021 -2.1017368 -1.9889896 -2.4197922 -3.0975149 -3.5815427 -3.70295 -3.4699605 -3.028038 -2.4828229 -1.5430949 -0.73859382][-2.5268373 -2.6454892 -2.6414332 -2.4244754 -2.0315983 -1.8853712 -2.2658837 -2.8856387 -3.3214741 -3.4449103 -3.2710333 -2.9138162 -2.4619069 -1.5968356 -0.81441689][-2.5887556 -2.7430277 -2.7259977 -2.4590173 -2.0260732 -1.8755262 -2.2717495 -2.9024944 -3.31466 -3.3793423 -3.1602852 -2.7984657 -2.3841372 -1.6192498 -0.94633031][-2.593102 -2.8765454 -2.9289279 -2.6307354 -2.1016293 -1.8289177 -2.0788944 -2.5743237 -2.9140334 -3.0028665 -2.8596377 -2.5629396 -2.2181525 -1.5762672 -1.0450761][-2.780689 -3.1779168 -3.2704697 -2.8788571 -2.1379728 -1.5446599 -1.3950078 -1.5648279 -1.8182695 -2.1053123 -2.2488396 -2.1643567 -1.9647298 -1.4666619 -1.0821829][-3.0952284 -3.5763881 -3.7341571 -3.3407562 -2.4846087 -1.5411961 -0.85677361 -0.57384729 -0.73947048 -1.2691774 -1.6982629 -1.7733231 -1.6683321 -1.3132808 -1.0864608][-3.1630211 -3.7308316 -4.1178684 -4.0487723 -3.5076628 -2.5914464 -1.5814626 -0.90240979 -0.87017655 -1.3721454 -1.7789891 -1.7779841 -1.6048927 -1.2918425 -1.1396868][-2.9486721 -3.5825746 -4.2203422 -4.5504174 -4.5075164 -3.956856 -2.9995399 -2.2180009 -2.0469069 -2.3315375 -2.4722266 -2.2295623 -1.8908908 -1.5074263 -1.2967396][-2.6269655 -3.2657967 -4.0214558 -4.5202627 -4.7002482 -4.36433 -3.4924378 -2.8301082 -2.8133359 -3.1152709 -3.1113105 -2.6340022 -2.1084988 -1.626488 -1.3384602][-2.3079245 -2.8788309 -3.5895846 -4.0498767 -4.2080288 -3.8649487 -2.9383187 -2.3514876 -2.6091371 -3.1404233 -3.2358692 -2.7456446 -2.2115393 -1.7074823 -1.3362086][-2.1674776 -2.6013572 -3.1087978 -3.3824272 -3.4164367 -3.0352716 -2.0954759 -1.5976384 -2.0863447 -2.7931542 -3.0058975 -2.6128325 -2.2389858 -1.8437419 -1.4101462][-2.254245 -2.5898571 -2.9559727 -3.0706947 -2.9706459 -2.5206103 -1.5597358 -1.1166236 -1.7788439 -2.6286244 -2.9250579 -2.5701492 -2.2647355 -1.9323249 -1.4374015][-2.3466675 -2.586277 -2.9220304 -3.0190234 -2.8509884 -2.3018739 -1.2594626 -0.74458885 -1.4411521 -2.4277148 -2.8830066 -2.6036582 -2.3247895 -2.0033917 -1.4585674][-2.2064431 -2.3522673 -2.6994622 -2.8579054 -2.6821222 -2.0604305 -0.94731688 -0.29197645 -0.87669206 -1.9151325 -2.5455379 -2.4037077 -2.2157338 -1.98598 -1.4823816]]...]
INFO - root - 2017-12-07 06:25:12.659065: step 15610, loss = 1.03, batch loss = 0.96 (7.2 examples/sec; 1.112 sec/batch; 97h:54m:48s remains)
INFO - root - 2017-12-07 06:25:24.233736: step 15620, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 1.124 sec/batch; 98h:56m:16s remains)
INFO - root - 2017-12-07 06:25:36.028347: step 15630, loss = 0.97, batch loss = 0.90 (6.8 examples/sec; 1.180 sec/batch; 103h:53m:34s remains)
INFO - root - 2017-12-07 06:25:47.449653: step 15640, loss = 0.88, batch loss = 0.80 (6.6 examples/sec; 1.218 sec/batch; 107h:10m:58s remains)
INFO - root - 2017-12-07 06:25:59.002106: step 15650, loss = 0.91, batch loss = 0.83 (6.7 examples/sec; 1.191 sec/batch; 104h:47m:44s remains)
INFO - root - 2017-12-07 06:26:10.658768: step 15660, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 1.106 sec/batch; 97h:19m:47s remains)
INFO - root - 2017-12-07 06:26:22.161184: step 15670, loss = 0.69, batch loss = 0.61 (7.0 examples/sec; 1.141 sec/batch; 100h:24m:04s remains)
INFO - root - 2017-12-07 06:26:33.768901: step 15680, loss = 0.91, batch loss = 0.84 (7.3 examples/sec; 1.099 sec/batch; 96h:45m:29s remains)
INFO - root - 2017-12-07 06:26:45.528519: step 15690, loss = 0.76, batch loss = 0.68 (6.7 examples/sec; 1.191 sec/batch; 104h:46m:54s remains)
INFO - root - 2017-12-07 06:26:57.026490: step 15700, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 1.162 sec/batch; 102h:16m:12s remains)
2017-12-07 06:26:57.849903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7974706 -3.85037 -3.9408598 -4.0378981 -4.1158442 -4.2183218 -4.3068652 -4.3226724 -4.3106542 -4.2637296 -4.2369928 -4.23395 -4.161314 -4.0632257 -3.9611754][-3.415534 -3.3721175 -3.4311905 -3.5676818 -3.7056959 -3.8898492 -4.0552235 -4.1139402 -4.1440344 -4.085649 -4.03395 -4.0354958 -3.9534578 -3.8668711 -3.8351545][-3.0157371 -2.8203263 -2.8123441 -2.9617867 -3.1262636 -3.3356247 -3.4904993 -3.4964056 -3.500874 -3.4212351 -3.360528 -3.40423 -3.3732109 -3.3286729 -3.4118934][-2.7208536 -2.4796023 -2.4685936 -2.631273 -2.7826841 -2.9418302 -2.9820383 -2.853179 -2.8193903 -2.7759414 -2.7373872 -2.8378465 -2.8611293 -2.8213909 -2.944859][-2.6006634 -2.4323685 -2.4655504 -2.6031833 -2.641819 -2.6330981 -2.4372478 -2.1255953 -2.159323 -2.3651237 -2.5150931 -2.7250369 -2.7887664 -2.6883659 -2.7224827][-2.6109571 -2.5340796 -2.5877576 -2.6694729 -2.575774 -2.3079255 -1.7192664 -1.0837848 -1.1858933 -1.8627474 -2.480267 -3.0039916 -3.21277 -3.05973 -2.879606][-2.8372784 -2.8217669 -2.8913066 -2.9588575 -2.7632878 -2.2172205 -1.1119368 0.057174683 0.065947533 -1.012512 -2.116046 -3.0170445 -3.4458909 -3.3650908 -3.0794089][-2.9654517 -2.8807814 -2.8954597 -2.9667602 -2.7648125 -2.0953791 -0.7315135 0.78797579 0.94709826 -0.22226095 -1.4946258 -2.5677555 -3.1910324 -3.2662435 -3.046649][-3.0517969 -2.8661561 -2.8101952 -2.9050107 -2.8125272 -2.3807755 -1.4406462 -0.30988741 -0.15603304 -0.92670894 -1.829143 -2.6407323 -3.1356456 -3.2162824 -3.0518241][-3.2003231 -2.9836469 -2.9357934 -3.0990586 -3.1726515 -3.1116953 -2.8326759 -2.3101747 -2.2144256 -2.5065022 -2.8824363 -3.2716522 -3.4532342 -3.3688412 -3.174706][-3.5341852 -3.3298931 -3.2682426 -3.384789 -3.4834735 -3.6372683 -3.7999024 -3.7335947 -3.7062335 -3.7226405 -3.8207819 -3.9676647 -3.9173155 -3.6498487 -3.3800707][-4.0401177 -3.9210563 -3.8233447 -3.7999935 -3.7815032 -3.9153068 -4.2064447 -4.3833623 -4.4822564 -4.471489 -4.4774203 -4.4648933 -4.2555046 -3.8661323 -3.5406923][-4.36726 -4.3579721 -4.2681746 -4.174068 -4.0916362 -4.1333132 -4.315258 -4.4752178 -4.6147437 -4.6725945 -4.6683426 -4.5371342 -4.2287421 -3.8264341 -3.5464926][-4.3904486 -4.4651175 -4.4463563 -4.3992953 -4.3577728 -4.3603077 -4.4108524 -4.4623265 -4.5458293 -4.6156754 -4.5982738 -4.4202728 -4.1036272 -3.7671824 -3.5765421][-4.1801991 -4.2694244 -4.3214378 -4.350029 -4.3693042 -4.3852429 -4.3813806 -4.3686342 -4.3669372 -4.3662457 -4.3160391 -4.16713 -3.9574952 -3.769475 -3.6742005]]...]
INFO - root - 2017-12-07 06:27:09.503864: step 15710, loss = 0.57, batch loss = 0.50 (6.8 examples/sec; 1.171 sec/batch; 103h:04m:26s remains)
INFO - root - 2017-12-07 06:27:21.119165: step 15720, loss = 0.96, batch loss = 0.89 (7.1 examples/sec; 1.128 sec/batch; 99h:14m:36s remains)
INFO - root - 2017-12-07 06:27:32.755638: step 15730, loss = 0.79, batch loss = 0.72 (6.7 examples/sec; 1.191 sec/batch; 104h:46m:49s remains)
INFO - root - 2017-12-07 06:27:44.389276: step 15740, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 1.155 sec/batch; 101h:35m:43s remains)
INFO - root - 2017-12-07 06:27:56.101214: step 15750, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.177 sec/batch; 103h:35m:28s remains)
INFO - root - 2017-12-07 06:28:07.628605: step 15760, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 1.140 sec/batch; 100h:17m:09s remains)
INFO - root - 2017-12-07 06:28:19.430488: step 15770, loss = 0.67, batch loss = 0.60 (6.5 examples/sec; 1.225 sec/batch; 107h:45m:35s remains)
INFO - root - 2017-12-07 06:28:31.014518: step 15780, loss = 0.85, batch loss = 0.77 (6.6 examples/sec; 1.206 sec/batch; 106h:08m:23s remains)
INFO - root - 2017-12-07 06:28:42.678207: step 15790, loss = 0.82, batch loss = 0.75 (7.0 examples/sec; 1.147 sec/batch; 100h:56m:11s remains)
INFO - root - 2017-12-07 06:28:54.380110: step 15800, loss = 1.11, batch loss = 1.04 (7.1 examples/sec; 1.119 sec/batch; 98h:26m:27s remains)
2017-12-07 06:28:55.245839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6325271 -1.5032442 -1.6417975 -1.8108513 -1.8837612 -2.0306404 -2.3427014 -2.6613727 -2.8094068 -2.7221918 -2.6595392 -2.8007593 -2.7803359 -2.7051928 -2.7457986][-1.4682467 -1.3813841 -1.569195 -1.7818255 -1.8876805 -2.0380502 -2.3979855 -2.8228998 -3.0358512 -2.9654863 -2.8851156 -2.9776545 -2.9496636 -2.8980875 -2.8838725][-1.3332751 -1.2905006 -1.4917834 -1.7008843 -1.8295875 -2.0334914 -2.4358926 -2.8719845 -3.0870743 -3.0517735 -2.9888015 -3.0230563 -3.0388 -3.0854208 -3.0790544][-1.4336588 -1.4204199 -1.5989084 -1.8062909 -1.9848766 -2.2332416 -2.5714793 -2.8691037 -3.0059714 -3.037189 -3.0384896 -3.0520806 -3.1311846 -3.2982342 -3.3672035][-1.6800466 -1.7462163 -1.9211447 -2.1298733 -2.3131702 -2.4993792 -2.6922936 -2.8635397 -3.0023317 -3.1348176 -3.215426 -3.2349458 -3.2954216 -3.4363868 -3.4919538][-1.8541486 -2.0090764 -2.2216117 -2.408736 -2.5132689 -2.5614045 -2.6017962 -2.753089 -3.0175326 -3.2941086 -3.4600668 -3.519655 -3.5399151 -3.5761862 -3.5508106][-1.8260007 -1.9366612 -2.1260755 -2.2617078 -2.3293452 -2.3526621 -2.3821688 -2.6176062 -3.0413723 -3.4213669 -3.6532054 -3.7753441 -3.7745051 -3.7244263 -3.6708798][-1.5292139 -1.4960721 -1.6377976 -1.7671776 -1.8815687 -1.9970291 -2.1357586 -2.4886413 -2.9965281 -3.4059191 -3.6981792 -3.8882079 -3.8880384 -3.7987814 -3.7356236][-1.1575878 -0.96450806 -1.0611217 -1.2148943 -1.390718 -1.612396 -1.865099 -2.26195 -2.708847 -3.0696573 -3.3842463 -3.6221817 -3.6473873 -3.5941353 -3.5997581][-0.8432653 -0.56318331 -0.663208 -0.859256 -1.0580106 -1.2565942 -1.4583578 -1.7542856 -2.0989373 -2.4217901 -2.7385929 -3.0042844 -3.0844831 -3.114049 -3.2069604][-0.83380508 -0.64184308 -0.84417677 -1.1178138 -1.2809613 -1.3302746 -1.3397262 -1.4467585 -1.6598513 -1.8835921 -2.1172981 -2.3324373 -2.4220054 -2.4739451 -2.5899212][-1.2936518 -1.348918 -1.6438046 -1.938457 -2.0620592 -2.0203376 -1.9210758 -1.9022951 -1.9681122 -2.0112712 -2.0552697 -2.1407661 -2.1654656 -2.128859 -2.1140106][-2.0263355 -2.2631009 -2.5549 -2.790045 -2.8546152 -2.7959332 -2.7230296 -2.7023215 -2.6909862 -2.6260557 -2.5351715 -2.4885883 -2.3934224 -2.2069829 -1.9936862][-2.6302938 -2.8356459 -3.0241542 -3.1778283 -3.2202764 -3.1876364 -3.1845977 -3.2322297 -3.263046 -3.2217245 -3.082397 -2.9343178 -2.7556558 -2.4875362 -2.1793044][-2.8645933 -2.9475906 -3.0250223 -3.1188359 -3.1710434 -3.1800685 -3.2267854 -3.3027263 -3.3411989 -3.3220654 -3.2046652 -3.0658243 -2.9193954 -2.6905718 -2.4193301]]...]
INFO - root - 2017-12-07 06:29:06.839453: step 15810, loss = 0.96, batch loss = 0.89 (6.8 examples/sec; 1.183 sec/batch; 104h:04m:28s remains)
INFO - root - 2017-12-07 06:29:18.467241: step 15820, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 1.191 sec/batch; 104h:47m:59s remains)
INFO - root - 2017-12-07 06:29:30.078172: step 15830, loss = 0.70, batch loss = 0.62 (7.0 examples/sec; 1.140 sec/batch; 100h:16m:53s remains)
INFO - root - 2017-12-07 06:29:41.923427: step 15840, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 1.152 sec/batch; 101h:20m:05s remains)
INFO - root - 2017-12-07 06:29:53.714045: step 15850, loss = 0.60, batch loss = 0.53 (7.1 examples/sec; 1.133 sec/batch; 99h:37m:36s remains)
INFO - root - 2017-12-07 06:30:05.596083: step 15860, loss = 0.66, batch loss = 0.59 (6.8 examples/sec; 1.176 sec/batch; 103h:26m:25s remains)
INFO - root - 2017-12-07 06:30:17.375470: step 15870, loss = 0.93, batch loss = 0.86 (6.6 examples/sec; 1.203 sec/batch; 105h:50m:53s remains)
INFO - root - 2017-12-07 06:30:28.756409: step 15880, loss = 0.87, batch loss = 0.80 (6.9 examples/sec; 1.162 sec/batch; 102h:10m:13s remains)
INFO - root - 2017-12-07 06:30:40.297278: step 15890, loss = 0.88, batch loss = 0.81 (7.3 examples/sec; 1.091 sec/batch; 95h:56m:50s remains)
INFO - root - 2017-12-07 06:30:52.043346: step 15900, loss = 0.86, batch loss = 0.78 (6.7 examples/sec; 1.194 sec/batch; 104h:57m:50s remains)
2017-12-07 06:30:52.952315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9212825 -2.912641 -2.8699443 -2.8153195 -2.8181319 -2.7133782 -2.7671475 -2.9779563 -3.1844239 -3.5083632 -3.7868366 -3.8874612 -3.7733412 -3.4274914 -2.9987035][-3.3861685 -3.4784102 -3.5056186 -3.5342166 -3.6043527 -3.5188344 -3.5651214 -3.74546 -3.9408216 -4.2322674 -4.4445119 -4.5052881 -4.3756156 -4.0401592 -3.5990124][-3.5791721 -3.696804 -3.7581854 -3.868453 -3.9991021 -3.94685 -3.9523618 -4.0097337 -4.1252894 -4.3771238 -4.5919638 -4.7198062 -4.6571269 -4.3954554 -3.9991305][-3.529671 -3.6431658 -3.7241826 -3.8477924 -3.90561 -3.8051274 -3.7323458 -3.6402354 -3.6695786 -3.8919575 -4.170857 -4.4505472 -4.5075216 -4.34555 -4.016449][-3.3630772 -3.4562657 -3.5318 -3.5887172 -3.4757073 -3.2786872 -3.1085625 -2.8395214 -2.6767802 -2.7468104 -3.0200622 -3.3748014 -3.5111563 -3.4467897 -3.2299972][-3.0065942 -3.0192919 -3.0201015 -2.9596171 -2.6754208 -2.3651619 -2.0251353 -1.4988234 -1.0371008 -0.86939359 -1.1207278 -1.5603077 -1.8409219 -1.9830134 -1.991827][-2.4471152 -2.3267136 -2.2229643 -2.080754 -1.7044187 -1.2601585 -0.67285943 0.060895443 0.65687084 0.87142086 0.50318718 -0.12094212 -0.58104873 -0.92079711 -1.1344714][-1.8883796 -1.6112504 -1.4493573 -1.3135319 -0.99589705 -0.53725123 0.13232374 0.80072021 1.2344217 1.2936521 0.769701 0.049144745 -0.40624237 -0.69294858 -0.8689518][-1.731324 -1.3049433 -1.0419743 -0.817384 -0.52581954 -0.19473076 0.24642086 0.49247169 0.4836297 0.29054976 -0.24858093 -0.77919006 -0.97831511 -1.0028572 -0.97829127][-2.2037418 -1.7395818 -1.40293 -1.0881796 -0.83591866 -0.67504072 -0.53596354 -0.66091871 -0.91247582 -1.1710277 -1.5587487 -1.8406932 -1.8524997 -1.6989758 -1.531508][-3.1523309 -2.8530171 -2.6204481 -2.3947337 -2.2722659 -2.2476943 -2.2635217 -2.4256766 -2.5641646 -2.6554904 -2.7984238 -2.8746533 -2.8345695 -2.7002711 -2.5582924][-4.0824265 -4.0406456 -3.9991746 -3.9325004 -3.9295154 -3.960665 -3.9838712 -4.0342236 -4.0194407 -3.9588451 -3.9253469 -3.8871138 -3.838861 -3.7372117 -3.6183946][-4.459825 -4.6036582 -4.7062039 -4.7590961 -4.8074393 -4.8273048 -4.7890763 -4.7294722 -4.6241388 -4.5180292 -4.4371686 -4.3867707 -4.3542943 -4.2785988 -4.175652][-4.3072896 -4.5245938 -4.687624 -4.7917395 -4.8348012 -4.8064637 -4.7151494 -4.610167 -4.5096579 -4.4441652 -4.398169 -4.36032 -4.3203716 -4.2580109 -4.1886311][-3.9597156 -4.1616092 -4.3066 -4.389358 -4.4059911 -4.3655734 -4.2878051 -4.2102785 -4.1606331 -4.1432257 -4.1265373 -4.0988121 -4.0617137 -4.0212579 -3.98792]]...]
INFO - root - 2017-12-07 06:31:04.604331: step 15910, loss = 0.85, batch loss = 0.78 (6.6 examples/sec; 1.211 sec/batch; 106h:27m:20s remains)
INFO - root - 2017-12-07 06:31:16.358334: step 15920, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 1.166 sec/batch; 102h:30m:17s remains)
INFO - root - 2017-12-07 06:31:28.031134: step 15930, loss = 0.65, batch loss = 0.58 (6.8 examples/sec; 1.171 sec/batch; 102h:56m:53s remains)
INFO - root - 2017-12-07 06:31:39.625205: step 15940, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 1.160 sec/batch; 102h:02m:45s remains)
INFO - root - 2017-12-07 06:31:51.368618: step 15950, loss = 0.81, batch loss = 0.74 (7.2 examples/sec; 1.118 sec/batch; 98h:19m:42s remains)
INFO - root - 2017-12-07 06:32:03.036317: step 15960, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.190 sec/batch; 104h:39m:19s remains)
INFO - root - 2017-12-07 06:32:14.647700: step 15970, loss = 0.80, batch loss = 0.73 (6.4 examples/sec; 1.245 sec/batch; 109h:29m:56s remains)
INFO - root - 2017-12-07 06:32:26.152997: step 15980, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 1.117 sec/batch; 98h:10m:59s remains)
INFO - root - 2017-12-07 06:32:38.000415: step 15990, loss = 0.92, batch loss = 0.84 (7.2 examples/sec; 1.112 sec/batch; 97h:47m:50s remains)
INFO - root - 2017-12-07 06:32:49.557343: step 16000, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 1.141 sec/batch; 100h:18m:05s remains)
2017-12-07 06:32:50.446225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8055398 -3.7575083 -3.6496627 -3.5252733 -3.4461098 -3.4646356 -3.5618777 -3.6733248 -3.7606905 -3.8190992 -3.8162966 -3.7470355 -3.6261828 -3.4554105 -3.2763205][-4.1669192 -4.1874857 -4.1168232 -3.9653275 -3.8096046 -3.7591286 -3.8459198 -3.9867573 -4.1218095 -4.2306409 -4.2465348 -4.1447515 -3.9454386 -3.6643419 -3.3798618][-4.0171442 -4.2149897 -4.3193989 -4.24035 -4.0676584 -3.9830792 -4.0893178 -4.2688136 -4.4457245 -4.602201 -4.6720023 -4.6130972 -4.407083 -4.0367556 -3.6065714][-3.2492747 -3.5739233 -3.790061 -3.7590847 -3.6176894 -3.574285 -3.6819932 -3.8107374 -3.8814719 -3.9835715 -4.1364465 -4.3015566 -4.3452439 -4.1380315 -3.7214131][-2.4954534 -2.7688181 -2.8749762 -2.7524803 -2.6399255 -2.6522386 -2.6636462 -2.5482731 -2.3183832 -2.2235966 -2.4346993 -2.9024248 -3.3405209 -3.5172415 -3.3683157][-2.4709835 -2.5310826 -2.3675783 -2.1244545 -2.1377244 -2.2548511 -2.1109102 -1.6472352 -1.0518064 -0.69894648 -0.89534831 -1.5215414 -2.2068937 -2.6883202 -2.8219237][-3.0097716 -2.8760376 -2.5159202 -2.2647607 -2.4793243 -2.7280977 -2.5605249 -1.9952748 -1.2785037 -0.81646466 -0.92656517 -1.434015 -2.0092115 -2.4470544 -2.6057124][-3.3644714 -3.075583 -2.6881661 -2.6471426 -3.111289 -3.456357 -3.3632398 -2.9323006 -2.3428602 -1.9680722 -2.0399725 -2.3392162 -2.6692 -2.8523424 -2.812202][-3.3066778 -2.9571829 -2.6806591 -2.8794713 -3.4939337 -3.9136412 -3.9755461 -3.756516 -3.3269882 -3.0361521 -3.0447435 -3.1560974 -3.2810159 -3.2437904 -3.0205507][-3.0881639 -2.8395181 -2.7384973 -3.0463505 -3.5787678 -3.9440351 -4.1228027 -4.0813527 -3.7883444 -3.5504591 -3.4915118 -3.5026102 -3.5155036 -3.3607264 -3.0449646][-2.9493423 -2.8853707 -2.9279244 -3.1822228 -3.4482241 -3.6268883 -3.7801132 -3.7972064 -3.5971804 -3.4065962 -3.3088708 -3.3012695 -3.2930634 -3.1362126 -2.8752804][-2.9049032 -2.9841344 -3.0876245 -3.2277226 -3.248785 -3.2373178 -3.2868042 -3.273427 -3.131449 -3.0111971 -2.9359837 -2.9523611 -2.9475689 -2.8191633 -2.6688843][-3.1227057 -3.2185407 -3.2876229 -3.3108065 -3.2143092 -3.1306784 -3.1230998 -3.0542049 -2.9215236 -2.8469691 -2.8341165 -2.89276 -2.8902183 -2.7713099 -2.66998][-3.481256 -3.5439191 -3.5529461 -3.5155728 -3.4260457 -3.3894567 -3.4101329 -3.3518181 -3.239969 -3.16642 -3.1529646 -3.182646 -3.1346314 -2.9847665 -2.8582697][-3.6626496 -3.7150161 -3.7269139 -3.7009053 -3.6478472 -3.6436315 -3.6694314 -3.6422582 -3.5751309 -3.4968057 -3.4453883 -3.408227 -3.313683 -3.1564703 -3.0168762]]...]
INFO - root - 2017-12-07 06:33:02.207553: step 16010, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 1.165 sec/batch; 102h:22m:34s remains)
INFO - root - 2017-12-07 06:33:13.827988: step 16020, loss = 0.74, batch loss = 0.66 (7.0 examples/sec; 1.138 sec/batch; 100h:01m:28s remains)
INFO - root - 2017-12-07 06:33:25.640190: step 16030, loss = 0.68, batch loss = 0.61 (6.9 examples/sec; 1.153 sec/batch; 101h:23m:51s remains)
INFO - root - 2017-12-07 06:33:37.324508: step 16040, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 1.146 sec/batch; 100h:46m:24s remains)
INFO - root - 2017-12-07 06:33:49.130610: step 16050, loss = 0.69, batch loss = 0.62 (6.7 examples/sec; 1.193 sec/batch; 104h:51m:39s remains)
INFO - root - 2017-12-07 06:34:00.278922: step 16060, loss = 0.74, batch loss = 0.66 (7.4 examples/sec; 1.088 sec/batch; 95h:38m:05s remains)
INFO - root - 2017-12-07 06:34:12.006591: step 16070, loss = 0.61, batch loss = 0.53 (7.1 examples/sec; 1.134 sec/batch; 99h:41m:50s remains)
INFO - root - 2017-12-07 06:34:23.757381: step 16080, loss = 0.76, batch loss = 0.69 (7.0 examples/sec; 1.143 sec/batch; 100h:27m:53s remains)
INFO - root - 2017-12-07 06:34:35.563602: step 16090, loss = 0.97, batch loss = 0.90 (6.8 examples/sec; 1.174 sec/batch; 103h:12m:38s remains)
INFO - root - 2017-12-07 06:34:47.202170: step 16100, loss = 0.64, batch loss = 0.57 (6.5 examples/sec; 1.232 sec/batch; 108h:15m:37s remains)
2017-12-07 06:34:48.077873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3002141 -2.3587925 -2.3424697 -2.3416903 -2.3517015 -2.3816075 -2.4052057 -2.3784037 -2.3417356 -2.3056023 -2.2621455 -2.1965663 -2.1706672 -2.2878721 -2.4575429][-2.236511 -2.3496671 -2.3822277 -2.457705 -2.5397165 -2.6109157 -2.6481416 -2.6241775 -2.5685911 -2.4966774 -2.39817 -2.2244647 -2.0895913 -2.1838472 -2.3904102][-2.3955159 -2.5099616 -2.5182688 -2.6015944 -2.6664319 -2.6579018 -2.6094818 -2.5768909 -2.5744154 -2.5700612 -2.5340564 -2.3479195 -2.1430392 -2.1949158 -2.3887529][-2.5419338 -2.585959 -2.520556 -2.6228352 -2.7191343 -2.6539164 -2.5259523 -2.4953 -2.5639431 -2.6065378 -2.6165771 -2.4600677 -2.2420444 -2.2944236 -2.51053][-2.2977848 -2.1838429 -2.01401 -2.1636634 -2.3828018 -2.3877659 -2.2739544 -2.331182 -2.5627904 -2.6828709 -2.6452141 -2.3724976 -2.0011921 -1.9421177 -2.1763923][-1.5088863 -1.1797273 -0.87366128 -1.0013316 -1.2860143 -1.3444021 -1.2662976 -1.5122788 -2.1595652 -2.6751451 -2.8472443 -2.6019411 -2.0877128 -1.7616746 -1.7914107][-0.78875947 -0.25971937 0.18071556 0.15003204 -0.11914158 -0.16247177 0.039331913 -0.11974144 -0.94425821 -1.7651944 -2.2609386 -2.3215518 -2.0294228 -1.7298572 -1.6618476][-0.72175 -0.10703087 0.38682127 0.51698208 0.45537376 0.61902523 1.10674 1.2595568 0.5503788 -0.34148741 -1.0259895 -1.4034023 -1.4909735 -1.477073 -1.5441179][-1.3234453 -0.8948586 -0.55510235 -0.38009119 -0.21669292 0.1631422 0.81437969 1.1967916 0.81344032 0.12191486 -0.5067687 -0.98407578 -1.2942679 -1.5122793 -1.7441311][-2.0745776 -1.9502745 -1.8646731 -1.8164084 -1.6754251 -1.349417 -0.82601666 -0.42944098 -0.49017978 -0.75496888 -0.97663927 -1.1409914 -1.3050475 -1.5055511 -1.8174336][-2.1896942 -2.3414307 -2.5134959 -2.6851497 -2.7329555 -2.6145399 -2.3801322 -2.1752787 -2.1687255 -2.2082243 -2.12674 -1.9582522 -1.8270071 -1.806325 -1.9806242][-1.3906293 -1.7657454 -2.1942542 -2.6241815 -2.9398227 -3.0938759 -3.1701517 -3.1964812 -3.1962864 -3.0712204 -2.7453222 -2.3306572 -2.0110378 -1.8731399 -1.9940336][-0.46789169 -0.90030837 -1.4325831 -1.9851317 -2.4751811 -2.841959 -3.1483383 -3.3394995 -3.3389468 -3.1137109 -2.6768928 -2.1729541 -1.8004084 -1.6469393 -1.7802913][-0.29918051 -0.56311536 -0.95767593 -1.4155874 -1.8840837 -2.2751377 -2.6342573 -2.8726811 -2.9053929 -2.7497323 -2.4389858 -2.0671577 -1.773591 -1.6367581 -1.7438579][-1.1177812 -1.1620398 -1.3271477 -1.5763917 -1.8613915 -2.0954506 -2.3255706 -2.4841123 -2.5181422 -2.4644153 -2.3423977 -2.1701434 -2.0110838 -1.9168158 -1.971611]]...]
INFO - root - 2017-12-07 06:34:59.696510: step 16110, loss = 0.80, batch loss = 0.72 (7.0 examples/sec; 1.149 sec/batch; 101h:00m:32s remains)
INFO - root - 2017-12-07 06:35:11.213553: step 16120, loss = 0.64, batch loss = 0.57 (6.9 examples/sec; 1.157 sec/batch; 101h:43m:03s remains)
INFO - root - 2017-12-07 06:35:22.780963: step 16130, loss = 0.89, batch loss = 0.82 (6.8 examples/sec; 1.172 sec/batch; 102h:57m:56s remains)
INFO - root - 2017-12-07 06:35:34.577692: step 16140, loss = 0.84, batch loss = 0.77 (6.3 examples/sec; 1.267 sec/batch; 111h:22m:15s remains)
INFO - root - 2017-12-07 06:35:46.156373: step 16150, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.132 sec/batch; 99h:30m:52s remains)
INFO - root - 2017-12-07 06:35:57.889502: step 16160, loss = 0.59, batch loss = 0.52 (7.0 examples/sec; 1.142 sec/batch; 100h:19m:08s remains)
INFO - root - 2017-12-07 06:36:09.632404: step 16170, loss = 0.74, batch loss = 0.66 (6.7 examples/sec; 1.186 sec/batch; 104h:14m:35s remains)
INFO - root - 2017-12-07 06:36:21.312096: step 16180, loss = 0.64, batch loss = 0.57 (6.7 examples/sec; 1.197 sec/batch; 105h:13m:13s remains)
INFO - root - 2017-12-07 06:36:33.009936: step 16190, loss = 1.08, batch loss = 1.01 (6.5 examples/sec; 1.228 sec/batch; 107h:51m:35s remains)
INFO - root - 2017-12-07 06:36:44.674938: step 16200, loss = 0.65, batch loss = 0.58 (6.9 examples/sec; 1.165 sec/batch; 102h:23m:16s remains)
2017-12-07 06:36:45.572899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5177879 -2.5919738 -2.6563795 -2.5682979 -2.2890983 -1.9765584 -1.7406251 -1.6989238 -1.6865451 -1.6439574 -1.7157454 -1.9359879 -2.2727933 -2.5764174 -2.4804473][-2.7192769 -2.7970943 -2.8507156 -2.7281065 -2.3290043 -1.8482349 -1.490849 -1.401943 -1.4031317 -1.4060264 -1.5911803 -2.002322 -2.5363622 -2.9871502 -2.9999595][-2.8775821 -2.8966222 -2.86063 -2.5991278 -1.9894466 -1.2471285 -0.71858716 -0.63232327 -0.81945777 -1.0641282 -1.493715 -2.1192322 -2.8623841 -3.4644198 -3.5979712][-3.3298936 -3.2596827 -3.0867734 -2.6559463 -1.8391385 -0.85165262 -0.1111927 0.074387074 -0.18523359 -0.61795378 -1.2867899 -2.1463625 -3.1307459 -3.9292526 -4.2074833][-3.7693598 -3.5916057 -3.2378192 -2.6777411 -1.8712859 -0.96742058 -0.20881033 0.14156675 -0.033551216 -0.44767284 -1.0861185 -1.9791512 -3.1125212 -4.1219649 -4.5753932][-3.8683553 -3.5532725 -2.9914508 -2.342947 -1.6814616 -1.0524452 -0.46178341 -0.12495661 -0.37634611 -0.811404 -1.2114401 -1.8039289 -2.7541397 -3.7975371 -4.3751926][-3.874805 -3.4625678 -2.7470987 -2.0766571 -1.558264 -1.1114922 -0.60748482 -0.3607502 -0.81596351 -1.3089457 -1.43118 -1.5862489 -2.1699541 -3.056304 -3.6597586][-3.8046026 -3.3525319 -2.6210372 -2.0278924 -1.5867786 -1.2195232 -0.82747245 -0.75464821 -1.3177087 -1.70876 -1.5191522 -1.2976196 -1.5686932 -2.2634842 -2.8292634][-3.5905976 -3.1478567 -2.4989409 -2.0072944 -1.5866468 -1.2693493 -1.0947988 -1.2797394 -1.8897409 -2.104259 -1.6669044 -1.2023692 -1.276401 -1.8322258 -2.3093498][-3.3389602 -2.9115419 -2.3382854 -1.8842404 -1.4345052 -1.1389139 -1.1623192 -1.4929631 -2.0373 -2.1433299 -1.6823044 -1.2358816 -1.2997909 -1.7757993 -2.0766208][-3.2472713 -2.8227067 -2.3177092 -1.9181635 -1.5348079 -1.3413415 -1.4645326 -1.6890788 -1.9504826 -1.9109886 -1.5507038 -1.3080568 -1.4506812 -1.8218727 -1.913307][-3.3300219 -2.931077 -2.4929385 -2.1675632 -1.8987994 -1.8041365 -1.9099295 -1.9340401 -1.9335496 -1.8050611 -1.6057315 -1.5824714 -1.7908657 -2.0132556 -1.8867524][-3.6133075 -3.3148527 -2.989994 -2.7536001 -2.5458903 -2.4151046 -2.3873081 -2.2888792 -2.2244196 -2.1675403 -2.144907 -2.2484221 -2.4304056 -2.4561076 -2.148783][-3.979295 -3.8388872 -3.7023442 -3.6318951 -3.5133257 -3.3433478 -3.189255 -3.05893 -3.0433879 -3.1045647 -3.1911054 -3.282692 -3.3253031 -3.1536236 -2.7077389][-4.0678949 -4.105113 -4.1605558 -4.2527905 -4.2287469 -4.0784307 -3.9231517 -3.8658147 -3.9401166 -4.0685668 -4.134038 -4.1057258 -3.9959354 -3.7140231 -3.2011471]]...]
INFO - root - 2017-12-07 06:36:57.087348: step 16210, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 1.161 sec/batch; 102h:02m:21s remains)
INFO - root - 2017-12-07 06:37:08.745565: step 16220, loss = 0.72, batch loss = 0.65 (6.8 examples/sec; 1.171 sec/batch; 102h:51m:53s remains)
INFO - root - 2017-12-07 06:37:20.455284: step 16230, loss = 0.82, batch loss = 0.75 (6.6 examples/sec; 1.218 sec/batch; 107h:02m:54s remains)
INFO - root - 2017-12-07 06:37:32.037553: step 16240, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 1.154 sec/batch; 101h:22m:44s remains)
INFO - root - 2017-12-07 06:37:43.701690: step 16250, loss = 0.73, batch loss = 0.65 (6.9 examples/sec; 1.157 sec/batch; 101h:37m:15s remains)
INFO - root - 2017-12-07 06:37:55.280951: step 16260, loss = 0.61, batch loss = 0.53 (6.8 examples/sec; 1.173 sec/batch; 103h:03m:19s remains)
INFO - root - 2017-12-07 06:38:06.910685: step 16270, loss = 0.67, batch loss = 0.60 (7.1 examples/sec; 1.122 sec/batch; 98h:34m:39s remains)
INFO - root - 2017-12-07 06:38:18.561499: step 16280, loss = 0.63, batch loss = 0.56 (6.6 examples/sec; 1.208 sec/batch; 106h:06m:17s remains)
INFO - root - 2017-12-07 06:38:30.303939: step 16290, loss = 0.60, batch loss = 0.52 (6.9 examples/sec; 1.156 sec/batch; 101h:34m:01s remains)
INFO - root - 2017-12-07 06:38:41.884260: step 16300, loss = 0.56, batch loss = 0.49 (6.7 examples/sec; 1.188 sec/batch; 104h:22m:33s remains)
2017-12-07 06:38:42.705212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3600438 -2.2567651 -2.2217638 -2.4534101 -2.8622613 -3.0701528 -2.7451952 -2.209409 -1.8800945 -1.8590581 -2.4170852 -3.5493486 -4.5996246 -5.1558142 -4.7535248][-2.3681805 -2.2327573 -2.154505 -2.2815688 -2.5664787 -2.6535349 -2.1908596 -1.5758283 -1.2818947 -1.4402208 -2.2758985 -3.6061683 -4.6913428 -5.1831169 -4.681469][-2.46417 -2.3623044 -2.3194299 -2.3968029 -2.5472879 -2.5044107 -1.9349885 -1.2432592 -0.93857551 -1.2011232 -2.1917791 -3.6032095 -4.6541424 -5.0421786 -4.4236994][-2.5659068 -2.5229921 -2.5787807 -2.7294738 -2.91829 -2.9542608 -2.4977756 -1.8872743 -1.5891719 -1.8195267 -2.7363791 -4.0430832 -4.9649038 -5.1934395 -4.395709][-2.5791552 -2.5210798 -2.5928252 -2.7806282 -2.9851909 -3.0843863 -2.7995744 -2.4210989 -2.337821 -2.7156835 -3.67582 -4.9295511 -5.7168946 -5.7412882 -4.7461157][-2.4746623 -2.3175769 -2.2477083 -2.2505453 -2.226783 -2.1547234 -1.9017975 -1.7467229 -2.0186512 -2.7518091 -3.9721251 -5.40425 -6.3168664 -6.3466067 -5.2703977][-2.3906827 -2.1617432 -1.8720458 -1.5039098 -1.0017419 -0.47598505 -0.012042046 0.099461555 -0.45453739 -1.5781705 -3.1126013 -4.8061943 -5.9574213 -6.1236572 -5.090529][-2.3770335 -2.1949081 -1.8073051 -1.1574683 -0.21867609 0.82214308 1.6583033 1.9636054 1.4049454 0.059888363 -1.7164254 -3.620894 -4.9552193 -5.1902962 -4.1435103][-2.4303231 -2.4147267 -2.1407435 -1.4926708 -0.47075057 0.7050519 1.6346378 2.0468817 1.6884394 0.46918154 -1.2452843 -3.1399665 -4.4540215 -4.4926982 -3.2018716][-2.5157087 -2.6999521 -2.6293983 -2.1923645 -1.4127183 -0.53666878 0.035438061 0.20602989 -0.055534363 -0.93747973 -2.2986042 -3.8987789 -4.9483871 -4.5721273 -2.9141297][-2.579833 -2.9074001 -2.9620895 -2.6695442 -2.100234 -1.5188317 -1.2853847 -1.263263 -1.4364285 -2.0844882 -3.25984 -4.8068209 -5.8570004 -5.2875948 -3.4163246][-2.6413035 -3.0294986 -3.0513721 -2.6613092 -1.9828727 -1.387625 -1.2917013 -1.3267782 -1.4707263 -2.0478876 -3.1940842 -4.898941 -6.251492 -5.8829937 -4.0962548][-2.8377857 -3.1889892 -3.0717368 -2.4573772 -1.5059731 -0.725482 -0.59371567 -0.49208808 -0.45460606 -0.93507695 -2.1413004 -4.158185 -5.9840431 -5.9803514 -4.3799644][-3.0817506 -3.4323711 -3.2424664 -2.4926655 -1.3575897 -0.4718039 -0.2891922 0.072010994 0.43678188 0.14200401 -1.100296 -3.4084787 -5.6398087 -5.889327 -4.3435121][-3.323772 -3.69627 -3.5431204 -2.8537831 -1.7993348 -1.1117723 -1.0529232 -0.47328877 0.26470613 0.28097296 -0.79841113 -3.09978 -5.4583077 -5.8154044 -4.2605596]]...]
INFO - root - 2017-12-07 06:38:54.356192: step 16310, loss = 0.94, batch loss = 0.87 (7.0 examples/sec; 1.146 sec/batch; 100h:40m:20s remains)
INFO - root - 2017-12-07 06:39:06.076613: step 16320, loss = 0.73, batch loss = 0.65 (6.7 examples/sec; 1.201 sec/batch; 105h:28m:17s remains)
INFO - root - 2017-12-07 06:39:17.746896: step 16330, loss = 0.78, batch loss = 0.70 (7.1 examples/sec; 1.132 sec/batch; 99h:26m:32s remains)
INFO - root - 2017-12-07 06:39:29.245885: step 16340, loss = 0.80, batch loss = 0.73 (7.3 examples/sec; 1.102 sec/batch; 96h:46m:49s remains)
INFO - root - 2017-12-07 06:39:40.976632: step 16350, loss = 0.71, batch loss = 0.63 (6.8 examples/sec; 1.184 sec/batch; 103h:59m:16s remains)
INFO - root - 2017-12-07 06:39:52.313265: step 16360, loss = 0.80, batch loss = 0.72 (7.2 examples/sec; 1.118 sec/batch; 98h:10m:27s remains)
INFO - root - 2017-12-07 06:40:03.670733: step 16370, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.187 sec/batch; 104h:13m:51s remains)
INFO - root - 2017-12-07 06:40:15.205033: step 16380, loss = 1.07, batch loss = 0.99 (7.2 examples/sec; 1.117 sec/batch; 98h:06m:34s remains)
INFO - root - 2017-12-07 06:40:26.835629: step 16390, loss = 0.89, batch loss = 0.82 (7.1 examples/sec; 1.122 sec/batch; 98h:32m:27s remains)
INFO - root - 2017-12-07 06:40:38.475203: step 16400, loss = 0.72, batch loss = 0.64 (6.9 examples/sec; 1.161 sec/batch; 101h:55m:07s remains)
2017-12-07 06:40:39.387659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7652891 -1.5588269 -1.4456575 -1.2736671 -1.3243454 -1.2309875 -0.92264032 -0.54566193 -0.02166605 0.27116537 -0.017456055 -0.47678781 -0.95483327 -1.3109977 -1.5248821][-1.5635674 -1.3377345 -1.329469 -1.1881709 -1.221137 -1.0841575 -0.74805117 -0.31287432 0.22704268 0.3084569 -0.16590977 -0.66334462 -1.0712421 -1.1866088 -1.247792][-1.7616332 -1.6639619 -1.7475269 -1.6161125 -1.6570427 -1.6044123 -1.3923247 -0.96421123 -0.40634298 -0.45507669 -0.95395684 -1.4131095 -1.745542 -1.5056531 -1.2435429][-1.9521165 -1.9210579 -1.9871161 -1.837821 -1.9060926 -2.019763 -1.9707022 -1.4451432 -0.70535016 -0.75581431 -1.1893351 -1.6986523 -2.2060525 -1.8602757 -1.3830819][-1.7506125 -1.7223094 -1.6892552 -1.4806595 -1.4956691 -1.5433805 -1.3009706 -0.43124723 0.43329239 0.090324879 -0.4985652 -1.2078245 -2.0784743 -1.9334731 -1.4926007][-1.4520476 -1.4282818 -1.2823579 -1.0089254 -0.86488676 -0.55039811 0.22519255 1.5617123 2.3257294 1.3545055 0.4631629 -0.46095037 -1.6313362 -1.7109454 -1.4432912][-1.1018648 -1.2268851 -1.0901995 -0.82890463 -0.57534575 0.098777294 1.301414 2.9512715 3.4374552 1.9050183 0.9140234 -0.0314703 -1.2404153 -1.3552399 -1.2440112][-0.94489455 -1.2516813 -1.170948 -0.98488522 -0.80072594 -0.17541933 0.90469646 2.3798933 2.6230803 1.2333436 0.61762476 -0.094614506 -1.1120477 -1.0980222 -1.0558162][-0.87082839 -1.2317808 -1.146688 -1.0413723 -1.0638034 -0.8403821 -0.301661 0.68205214 0.79388189 -0.043040276 -0.068768978 -0.49340844 -1.2958357 -1.1249828 -1.0595837][-0.68529844 -0.95697331 -0.77354956 -0.67161679 -0.85075021 -0.96522188 -0.81745291 -0.23634291 -0.24116325 -0.6800158 -0.35017776 -0.69471622 -1.4785049 -1.2829669 -1.2010438][-0.59280467 -0.69405007 -0.38914919 -0.21785736 -0.34098005 -0.4774437 -0.3786459 0.052185535 -0.039897919 -0.30369806 0.0808754 -0.38098478 -1.3076687 -1.2541933 -1.2644825][-0.65803289 -0.5981524 -0.20409107 0.046522141 0.061603546 0.077657223 0.24959946 0.65031672 0.55892658 0.38265991 0.65261555 0.023449421 -0.99317312 -1.1438029 -1.2844417][-0.64365149 -0.40678072 2.5749207e-05 0.15806246 0.13288927 0.20966864 0.41233492 0.81138611 0.82699013 0.73386669 0.77961588 -0.017681122 -1.0087082 -1.2544899 -1.432688][-0.16605759 0.1596508 0.44680977 0.35230064 0.12532091 0.093215466 0.18699789 0.48097754 0.61706352 0.6174674 0.45018148 -0.49023533 -1.3580995 -1.5631676 -1.6789153][0.29463911 0.5964489 0.78579807 0.58996391 0.27353191 0.099957943 0.0049977303 0.18734646 0.41508007 0.52397585 0.24911451 -0.75606704 -1.5564802 -1.7667577 -1.8623405]]...]
INFO - root - 2017-12-07 06:40:51.116936: step 16410, loss = 0.64, batch loss = 0.56 (6.4 examples/sec; 1.243 sec/batch; 109h:07m:35s remains)
INFO - root - 2017-12-07 06:41:02.559331: step 16420, loss = 0.79, batch loss = 0.71 (8.2 examples/sec; 0.982 sec/batch; 86h:10m:35s remains)
INFO - root - 2017-12-07 06:41:14.237496: step 16430, loss = 0.66, batch loss = 0.59 (7.1 examples/sec; 1.130 sec/batch; 99h:11m:04s remains)
INFO - root - 2017-12-07 06:41:26.064505: step 16440, loss = 0.73, batch loss = 0.66 (6.6 examples/sec; 1.214 sec/batch; 106h:34m:25s remains)
INFO - root - 2017-12-07 06:41:37.815517: step 16450, loss = 0.72, batch loss = 0.65 (6.7 examples/sec; 1.202 sec/batch; 105h:33m:35s remains)
INFO - root - 2017-12-07 06:41:49.600697: step 16460, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.170 sec/batch; 102h:44m:59s remains)
INFO - root - 2017-12-07 06:42:01.417549: step 16470, loss = 0.61, batch loss = 0.54 (6.9 examples/sec; 1.166 sec/batch; 102h:21m:23s remains)
INFO - root - 2017-12-07 06:42:13.204667: step 16480, loss = 1.03, batch loss = 0.95 (6.9 examples/sec; 1.165 sec/batch; 102h:16m:40s remains)
INFO - root - 2017-12-07 06:42:24.777936: step 16490, loss = 0.85, batch loss = 0.78 (6.8 examples/sec; 1.179 sec/batch; 103h:31m:52s remains)
INFO - root - 2017-12-07 06:42:36.520598: step 16500, loss = 1.01, batch loss = 0.94 (7.0 examples/sec; 1.147 sec/batch; 100h:39m:26s remains)
2017-12-07 06:42:37.424390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2683644 -3.2204928 -3.18683 -3.1448832 -3.0975642 -3.0801938 -3.1084473 -3.1782408 -3.2480807 -3.2934203 -3.3204932 -3.3354838 -3.3350444 -3.3188782 -3.2752724][-3.2474451 -3.2015347 -3.1378045 -3.0525069 -2.9660287 -2.8861237 -2.8723578 -2.989634 -3.1369534 -3.2384276 -3.3039465 -3.3342624 -3.3320355 -3.3066075 -3.2581432][-2.9521735 -2.916553 -2.822104 -2.690062 -2.5314202 -2.3386641 -2.2274475 -2.3270609 -2.5004 -2.6223125 -2.7203124 -2.7857695 -2.8211026 -2.8268666 -2.7887626][-2.4268117 -2.3575757 -2.1958842 -2.0169978 -1.8363581 -1.6046591 -1.4298542 -1.4724798 -1.6295891 -1.7853987 -1.9289446 -2.0182991 -2.0860174 -2.100853 -2.0436287][-1.7626257 -1.7007966 -1.5142889 -1.3224635 -1.1761091 -1.0048997 -0.84825683 -0.88080049 -1.0641003 -1.2656331 -1.3949406 -1.4140925 -1.4229443 -1.4077046 -1.3781211][-1.1311615 -1.1480899 -1.0674202 -0.98004365 -0.92938209 -0.8157692 -0.6360662 -0.6046195 -0.724571 -0.88392258 -0.93876433 -0.87390018 -0.830863 -0.80210161 -0.85449529][-0.92318559 -1.0409226 -1.0884211 -1.0954342 -1.1094837 -1.0386534 -0.863255 -0.74412179 -0.70166373 -0.73941135 -0.67046261 -0.47707152 -0.33444786 -0.29863405 -0.47915435][-1.1460369 -1.3027687 -1.3698235 -1.3317161 -1.3064129 -1.3038621 -1.2665353 -1.165 -0.9648149 -0.86200476 -0.71684384 -0.40948915 -0.14195156 -0.092137337 -0.39926386][-1.3178606 -1.4846911 -1.5611982 -1.4911263 -1.4050071 -1.4464686 -1.5819407 -1.5622847 -1.2955399 -1.1710379 -1.1722047 -1.0455816 -0.91424537 -0.9428823 -1.1633611][-1.4632671 -1.543813 -1.5501759 -1.3814588 -1.1523538 -1.1520286 -1.4399481 -1.5808599 -1.4337759 -1.4206228 -1.5592113 -1.5598228 -1.5263085 -1.6065841 -1.7027349][-1.9264934 -1.935837 -1.9005365 -1.6796157 -1.3688858 -1.3045597 -1.5830867 -1.7088196 -1.559525 -1.5203359 -1.6016016 -1.5464368 -1.4396639 -1.4262815 -1.414768][-2.515017 -2.6796279 -2.7867975 -2.6584134 -2.3927453 -2.263284 -2.3745651 -2.2855198 -1.993618 -1.7666204 -1.6523626 -1.5434499 -1.40534 -1.2567348 -1.126559][-2.6418929 -2.914151 -3.0771055 -2.9890449 -2.7642372 -2.6320562 -2.6641178 -2.5103035 -2.2311144 -1.9165008 -1.6594713 -1.5340042 -1.3997233 -1.2567542 -1.1933801][-2.6525767 -2.7707791 -2.7435646 -2.5319648 -2.314656 -2.2222772 -2.2660482 -2.1929853 -2.0814989 -1.8823991 -1.6834331 -1.6124079 -1.4696236 -1.3465526 -1.4106839][-2.9649215 -2.82116 -2.4977517 -2.0700259 -1.8291228 -1.7887387 -1.8490705 -1.8189445 -1.7987735 -1.7087722 -1.6291184 -1.6873446 -1.6057212 -1.4742262 -1.5155568]]...]
INFO - root - 2017-12-07 06:42:49.202608: step 16510, loss = 0.80, batch loss = 0.73 (7.0 examples/sec; 1.139 sec/batch; 99h:56m:44s remains)
INFO - root - 2017-12-07 06:43:00.849157: step 16520, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.181 sec/batch; 103h:38m:42s remains)
INFO - root - 2017-12-07 06:43:12.538084: step 16530, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 1.162 sec/batch; 101h:57m:37s remains)
INFO - root - 2017-12-07 06:43:24.342995: step 16540, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.155 sec/batch; 101h:19m:36s remains)
INFO - root - 2017-12-07 06:43:35.853660: step 16550, loss = 0.71, batch loss = 0.63 (6.4 examples/sec; 1.242 sec/batch; 109h:01m:21s remains)
INFO - root - 2017-12-07 06:43:47.586195: step 16560, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.172 sec/batch; 102h:53m:15s remains)
INFO - root - 2017-12-07 06:43:59.241622: step 16570, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 1.118 sec/batch; 98h:07m:28s remains)
INFO - root - 2017-12-07 06:44:10.848430: step 16580, loss = 0.94, batch loss = 0.86 (7.2 examples/sec; 1.112 sec/batch; 97h:37m:28s remains)
INFO - root - 2017-12-07 06:44:22.548229: step 16590, loss = 0.74, batch loss = 0.66 (7.1 examples/sec; 1.126 sec/batch; 98h:47m:48s remains)
INFO - root - 2017-12-07 06:44:34.221534: step 16600, loss = 0.82, batch loss = 0.74 (6.6 examples/sec; 1.203 sec/batch; 105h:35m:21s remains)
2017-12-07 06:44:35.195105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5445116 -1.4990647 -1.4597242 -1.4156804 -1.4033313 -1.5107918 -1.7035193 -2.0232658 -2.4111845 -2.7534993 -3.0681348 -3.2847373 -3.3194189 -3.1165304 -2.7424026][-1.0409677 -0.97464156 -0.97129083 -1.0187302 -1.131161 -1.3676131 -1.655412 -2.004312 -2.3957937 -2.7553034 -3.1438284 -3.4323914 -3.5243404 -3.3551421 -2.9771783][-0.56879973 -0.55004048 -0.6761744 -0.92082644 -1.2323532 -1.6190934 -1.9365675 -2.1707582 -2.3981802 -2.628783 -3.0132842 -3.3527315 -3.5350564 -3.4709587 -3.1755183][-0.34636164 -0.41444492 -0.66721916 -1.1186733 -1.6036587 -2.0779264 -2.4013073 -2.505667 -2.5575705 -2.6358707 -2.9555793 -3.2757356 -3.5143962 -3.5468445 -3.3557315][-0.4916265 -0.61531138 -0.87747145 -1.3686337 -1.84199 -2.2680521 -2.5618958 -2.6296148 -2.6416113 -2.6577835 -2.9082417 -3.1681023 -3.4336028 -3.5572171 -3.4926987][-0.932837 -1.0512683 -1.1767535 -1.5205874 -1.7900915 -2.0432961 -2.2890005 -2.4022591 -2.4982653 -2.5250478 -2.6962523 -2.9120693 -3.2593465 -3.5243843 -3.6040239][-1.465821 -1.4936795 -1.4350533 -1.5349593 -1.5040569 -1.5916891 -1.8550954 -2.0857844 -2.3145328 -2.3524079 -2.3645306 -2.5420299 -3.0228305 -3.441128 -3.6408968][-1.8778381 -1.7436604 -1.5233204 -1.3816144 -1.0468986 -0.97505641 -1.3257804 -1.7491848 -2.1139739 -2.15853 -2.006475 -2.1650836 -2.7775016 -3.3130398 -3.5996079][-1.8172038 -1.5679796 -1.3084054 -0.99154425 -0.42860079 -0.28871202 -0.76456451 -1.3570881 -1.8065491 -1.8225658 -1.4852731 -1.6024172 -2.3274963 -2.9942374 -3.3850594][-1.5446043 -1.2438626 -1.0286136 -0.62260079 0.053169727 0.18217564 -0.36179543 -0.96019363 -1.3663511 -1.3526418 -0.92477703 -0.9871459 -1.7162778 -2.4588871 -2.9440022][-1.4369001 -1.2029285 -1.0592599 -0.63065529 0.033377647 0.078416824 -0.47165322 -0.98557591 -1.2962253 -1.2547727 -0.82928634 -0.81293392 -1.3554997 -1.9839752 -2.4629216][-1.4709055 -1.3597221 -1.2680943 -0.87974882 -0.35346222 -0.39251566 -0.90439415 -1.3109164 -1.5015423 -1.3864298 -0.98655105 -0.9353416 -1.3074272 -1.7627056 -2.1607437][-1.4412448 -1.4107766 -1.3471498 -1.0599308 -0.76113105 -0.89944649 -1.3594675 -1.6603942 -1.7376442 -1.526226 -1.1196926 -1.0618076 -1.3686893 -1.7251267 -2.0469344][-1.5092216 -1.5212834 -1.4846716 -1.3318644 -1.2358479 -1.423084 -1.7850749 -1.9874098 -1.9833446 -1.73986 -1.3945489 -1.3578467 -1.5955057 -1.8340371 -2.0445654][-1.7170081 -1.7393541 -1.7277608 -1.6803148 -1.6761723 -1.7895086 -1.980242 -2.0847404 -2.0681736 -1.9218893 -1.7507372 -1.7643683 -1.8999114 -1.993468 -2.0604444]]...]
INFO - root - 2017-12-07 06:44:46.696775: step 16610, loss = 0.63, batch loss = 0.56 (7.0 examples/sec; 1.149 sec/batch; 100h:48m:02s remains)
INFO - root - 2017-12-07 06:44:58.421413: step 16620, loss = 0.64, batch loss = 0.57 (6.9 examples/sec; 1.167 sec/batch; 102h:23m:48s remains)
INFO - root - 2017-12-07 06:45:10.116510: step 16630, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.125 sec/batch; 98h:44m:22s remains)
INFO - root - 2017-12-07 06:45:21.825778: step 16640, loss = 0.81, batch loss = 0.74 (6.7 examples/sec; 1.192 sec/batch; 104h:37m:33s remains)
INFO - root - 2017-12-07 06:45:33.489945: step 16650, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.201 sec/batch; 105h:23m:35s remains)
INFO - root - 2017-12-07 06:45:45.149973: step 16660, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 1.108 sec/batch; 97h:10m:31s remains)
INFO - root - 2017-12-07 06:45:56.763042: step 16670, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 1.157 sec/batch; 101h:29m:42s remains)
INFO - root - 2017-12-07 06:46:08.310967: step 16680, loss = 0.79, batch loss = 0.72 (6.9 examples/sec; 1.167 sec/batch; 102h:24m:31s remains)
INFO - root - 2017-12-07 06:46:20.138789: step 16690, loss = 0.56, batch loss = 0.49 (6.7 examples/sec; 1.186 sec/batch; 104h:01m:59s remains)
INFO - root - 2017-12-07 06:46:31.782440: step 16700, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 1.184 sec/batch; 103h:50m:05s remains)
2017-12-07 06:46:32.651542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5234227 -4.5550814 -4.5719266 -4.5542932 -4.5200067 -4.4869227 -4.5166645 -4.5693288 -4.6099291 -4.5794029 -4.45615 -4.2981043 -4.1204529 -3.9952953 -3.9703841][-4.640522 -4.5846229 -4.4964962 -4.3217793 -4.1269631 -3.9933286 -4.0798759 -4.2864761 -4.4424458 -4.39131 -4.1753025 -3.9519548 -3.7440186 -3.6388111 -3.6760809][-4.6770658 -4.5078888 -4.287015 -3.945544 -3.5790138 -3.3669877 -3.5158057 -3.8437235 -4.0178838 -3.8544216 -3.4973519 -3.2565258 -3.1286349 -3.1483464 -3.3048921][-4.6469541 -4.3923473 -4.0859442 -3.6623962 -3.1995959 -2.9717391 -3.1491284 -3.4629061 -3.5147092 -3.1752102 -2.6874876 -2.5044844 -2.60666 -2.89386 -3.2297323][-4.602457 -4.2887964 -3.9464855 -3.5226479 -3.0576115 -2.8374627 -2.9612136 -3.1291528 -2.9935131 -2.507597 -2.001071 -1.9719787 -2.3597066 -2.9086776 -3.4041138][-4.535285 -4.1515965 -3.7568781 -3.3206515 -2.910965 -2.7455573 -2.7945709 -2.7958598 -2.5566335 -2.0914974 -1.6813016 -1.8024786 -2.3698418 -3.0416417 -3.6019835][-4.3938808 -3.9114301 -3.4232817 -2.9465325 -2.6267364 -2.5837779 -2.5799158 -2.4615531 -2.2245045 -1.9003479 -1.6233993 -1.8192508 -2.4433217 -3.1083884 -3.6731937][-4.1545439 -3.5949264 -3.0452743 -2.5458627 -2.2928631 -2.3456423 -2.3126478 -2.1608758 -2.0068088 -1.8298874 -1.6232207 -1.7564797 -2.3255291 -2.9705496 -3.5558348][-3.8934495 -3.320899 -2.740314 -2.2136812 -1.9440989 -1.9828839 -1.9478142 -1.9038112 -1.9248171 -1.8684089 -1.6332619 -1.5924327 -2.0319822 -2.6916242 -3.3543241][-3.7082202 -3.1544757 -2.5714705 -2.0379865 -1.7437048 -1.7152436 -1.7097251 -1.8285818 -1.9732645 -1.9293585 -1.6150029 -1.3860681 -1.6993191 -2.3779318 -3.1371393][-3.6549475 -3.1586697 -2.6065543 -2.0923538 -1.7739739 -1.6414783 -1.5802011 -1.7688644 -1.931879 -1.8206813 -1.4478948 -1.1585302 -1.4293201 -2.1317477 -2.9674041][-3.697535 -3.2282379 -2.6772325 -2.189069 -1.8744273 -1.6694331 -1.5649436 -1.7553949 -1.8976452 -1.7323499 -1.3542423 -1.0833697 -1.3414304 -2.0412245 -2.9122694][-3.7883704 -3.3129249 -2.7167487 -2.2449729 -1.9727142 -1.747215 -1.6262991 -1.8102448 -1.9889562 -1.8663919 -1.536989 -1.2853251 -1.4815657 -2.0968029 -2.9102333][-3.7998798 -3.2756824 -2.6290877 -2.1941628 -1.9861085 -1.8045089 -1.7095158 -1.9251971 -2.1848164 -2.1692128 -1.8996038 -1.6312068 -1.7195814 -2.2051032 -2.897285][-3.6708364 -3.0353558 -2.3168495 -1.8951101 -1.7368655 -1.6185524 -1.5830891 -1.8628087 -2.2314234 -2.3262908 -2.1162903 -1.8406854 -1.8541443 -2.2642434 -2.8875606]]...]
INFO - root - 2017-12-07 06:46:44.218220: step 16710, loss = 0.85, batch loss = 0.78 (7.3 examples/sec; 1.102 sec/batch; 96h:39m:56s remains)
INFO - root - 2017-12-07 06:46:55.841268: step 16720, loss = 0.76, batch loss = 0.68 (6.8 examples/sec; 1.184 sec/batch; 103h:52m:48s remains)
INFO - root - 2017-12-07 06:47:07.508561: step 16730, loss = 0.69, batch loss = 0.62 (6.5 examples/sec; 1.224 sec/batch; 107h:21m:59s remains)
INFO - root - 2017-12-07 06:47:18.984554: step 16740, loss = 0.78, batch loss = 0.71 (7.2 examples/sec; 1.113 sec/batch; 97h:38m:31s remains)
INFO - root - 2017-12-07 06:47:30.617776: step 16750, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.143 sec/batch; 100h:16m:27s remains)
INFO - root - 2017-12-07 06:47:42.213405: step 16760, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 1.173 sec/batch; 102h:52m:31s remains)
INFO - root - 2017-12-07 06:47:53.830022: step 16770, loss = 0.79, batch loss = 0.71 (7.0 examples/sec; 1.149 sec/batch; 100h:45m:54s remains)
INFO - root - 2017-12-07 06:48:05.538703: step 16780, loss = 0.74, batch loss = 0.66 (6.5 examples/sec; 1.225 sec/batch; 107h:25m:58s remains)
INFO - root - 2017-12-07 06:48:17.218914: step 16790, loss = 0.97, batch loss = 0.90 (6.8 examples/sec; 1.179 sec/batch; 103h:22m:58s remains)
INFO - root - 2017-12-07 06:48:28.823718: step 16800, loss = 0.55, batch loss = 0.47 (7.2 examples/sec; 1.105 sec/batch; 96h:55m:19s remains)
2017-12-07 06:48:29.696963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5129404 -3.6442919 -3.7002373 -3.5864925 -3.3469925 -3.1544814 -3.1098762 -3.186038 -3.290659 -3.3884788 -3.4705825 -3.4973633 -3.4800582 -3.4189517 -3.35635][-3.7046757 -3.8740685 -3.9014177 -3.7082119 -3.3720822 -3.1077204 -3.0209889 -3.0695744 -3.203886 -3.3445392 -3.4224834 -3.3572347 -3.2225819 -3.0868189 -3.0012636][-3.8503242 -3.9925637 -3.9299567 -3.6341395 -3.2150474 -2.907536 -2.7822556 -2.7994442 -2.9769368 -3.1586437 -3.1986449 -3.0192153 -2.8025203 -2.6936655 -2.7085557][-3.7736092 -3.9659185 -3.8865304 -3.5655403 -3.1804934 -2.9737022 -2.8806524 -2.8512993 -2.972805 -3.0974517 -3.1186156 -2.9595773 -2.7824602 -2.7534032 -2.82583][-3.2670648 -3.5503001 -3.6300268 -3.5218313 -3.3439531 -3.287632 -3.1996059 -3.021471 -2.9285171 -2.8704994 -2.838582 -2.7323766 -2.6260328 -2.6595864 -2.7668352][-2.3558407 -2.6501789 -2.9412839 -3.1863074 -3.2948885 -3.3108802 -3.0694623 -2.6377447 -2.318645 -2.1123142 -2.0652721 -2.0301335 -2.0072074 -2.0465157 -2.149842][-1.1739602 -1.3713365 -1.7520158 -2.1729739 -2.4380207 -2.5429971 -2.3616276 -2.0197349 -1.7208283 -1.4640436 -1.2880964 -1.1475348 -1.1336462 -1.2103798 -1.3649759][-0.18826151 -0.098886013 -0.2939024 -0.72197318 -1.16834 -1.5637484 -1.7264156 -1.7009263 -1.5305882 -1.2472205 -0.9202311 -0.66833997 -0.65815544 -0.77065229 -0.96187973][-0.24370909 0.23327303 0.3668952 0.04562664 -0.63285208 -1.448683 -2.0314736 -2.2651107 -2.1852226 -1.949244 -1.6986687 -1.5700519 -1.6314387 -1.69629 -1.780072][-0.81987143 -0.21165323 0.1400075 0.049001217 -0.54656243 -1.4547362 -2.2793977 -2.80485 -3.023839 -3.062377 -2.9800313 -2.9267545 -2.9476542 -2.9252665 -2.9381635][-1.1197433 -0.7032485 -0.39079857 -0.35558653 -0.70714378 -1.3845642 -2.1829114 -2.849967 -3.3386517 -3.6074827 -3.6246779 -3.5822017 -3.5458293 -3.4991436 -3.5160348][-1.5317595 -1.2608109 -0.999089 -0.96239519 -1.2896209 -1.8931077 -2.6677837 -3.3240571 -3.8051383 -4.0375886 -3.9997694 -3.9180532 -3.8685346 -3.8357742 -3.8609498][-1.6551824 -1.3409944 -1.1549544 -1.2369206 -1.6352761 -2.2154872 -2.9503636 -3.5969014 -4.0804811 -4.3071709 -4.24749 -4.1176534 -4.0374689 -3.998405 -4.034544][-1.2743328 -0.82967567 -0.73748541 -1.0284894 -1.6068015 -2.2510614 -2.9361682 -3.5518091 -4.028636 -4.3110046 -4.3057408 -4.1930661 -4.126513 -4.1081896 -4.1529307][-0.88124919 -0.35994196 -0.30119467 -0.670897 -1.3369992 -2.0834405 -2.8040013 -3.4690971 -3.9642732 -4.2700772 -4.2660217 -4.1361175 -4.05895 -4.0295882 -4.03817]]...]
INFO - root - 2017-12-07 06:48:41.195713: step 16810, loss = 0.72, batch loss = 0.65 (7.5 examples/sec; 1.073 sec/batch; 94h:07m:29s remains)
INFO - root - 2017-12-07 06:48:52.909604: step 16820, loss = 0.69, batch loss = 0.62 (6.6 examples/sec; 1.203 sec/batch; 105h:31m:42s remains)
INFO - root - 2017-12-07 06:49:04.635088: step 16830, loss = 0.73, batch loss = 0.66 (6.7 examples/sec; 1.195 sec/batch; 104h:48m:49s remains)
INFO - root - 2017-12-07 06:49:16.335206: step 16840, loss = 0.92, batch loss = 0.85 (6.9 examples/sec; 1.163 sec/batch; 101h:59m:00s remains)
INFO - root - 2017-12-07 06:49:27.867447: step 16850, loss = 0.76, batch loss = 0.69 (7.5 examples/sec; 1.073 sec/batch; 94h:04m:29s remains)
INFO - root - 2017-12-07 06:49:39.527296: step 16860, loss = 0.83, batch loss = 0.76 (6.7 examples/sec; 1.193 sec/batch; 104h:38m:00s remains)
INFO - root - 2017-12-07 06:49:51.245258: step 16870, loss = 0.73, batch loss = 0.66 (6.7 examples/sec; 1.190 sec/batch; 104h:21m:09s remains)
INFO - root - 2017-12-07 06:50:02.859867: step 16880, loss = 1.02, batch loss = 0.95 (6.6 examples/sec; 1.204 sec/batch; 105h:31m:53s remains)
INFO - root - 2017-12-07 06:50:14.502357: step 16890, loss = 0.88, batch loss = 0.81 (7.0 examples/sec; 1.148 sec/batch; 100h:38m:22s remains)
INFO - root - 2017-12-07 06:50:26.058389: step 16900, loss = 0.89, batch loss = 0.82 (6.9 examples/sec; 1.158 sec/batch; 101h:32m:17s remains)
2017-12-07 06:50:26.966906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.90467262 -1.4542534 -1.5966649 -1.347131 -0.69491744 -0.058184624 0.0357852 -0.024038792 0.078545094 0.42847824 0.81816721 0.66212368 0.13564253 -0.29272795 -0.51034021][-0.12844944 -0.62930727 -0.84439683 -0.80352569 -0.41752529 0.10569143 0.095999718 -0.14067268 -0.11356831 0.22654104 0.57214165 0.31627989 -0.36515236 -0.9449563 -1.2639656][0.11977148 -0.10465908 -0.10855627 -0.06485796 0.1544857 0.67275429 0.73001957 0.32146931 0.047155857 0.095741749 0.31019402 0.070194244 -0.6444602 -1.3408098 -1.7196159][0.12969637 0.13598442 0.28367186 0.3567996 0.45319271 0.96924973 1.2051601 0.7377491 0.10730314 -0.22937012 -0.18885946 -0.29305887 -0.839587 -1.4636009 -1.743881][0.10931873 0.11082602 0.14727736 0.18096876 0.26902628 0.79520369 1.2111664 0.79277229 -0.084888935 -0.75910211 -0.90333462 -0.847435 -1.1320407 -1.5936651 -1.6795619][0.22004747 0.091126919 -0.17205954 -0.20238638 0.03437233 0.72603464 1.4866805 1.3129306 0.25955248 -0.85076523 -1.3934171 -1.3753879 -1.3702233 -1.5446608 -1.3815844][0.48829842 0.26151228 -0.21707535 -0.28336716 0.12875795 1.0941992 2.4245076 2.7313385 1.5157347 -0.14050913 -1.2038815 -1.2899582 -1.0048237 -0.92877483 -0.75045371][0.58681917 0.28833389 -0.17841148 -0.23624516 0.28285456 1.4872861 3.42388 4.374897 3.2031798 1.1962752 -0.33297491 -0.73534083 -0.47866058 -0.39679527 -0.38269854][0.34003782 0.065781116 -0.29719019 -0.40757132 0.023257732 1.1820068 3.1721263 4.4294062 3.6015282 1.7928338 0.2904706 -0.3556695 -0.41444683 -0.59148908 -0.83696938][-0.30684614 -0.44297719 -0.72461796 -0.97109938 -0.77505541 0.000767231 1.4131699 2.4493494 2.0383639 0.82647133 -0.15374804 -0.58713484 -0.69826627 -1.0547576 -1.5951409][-1.151962 -1.1884403 -1.433028 -1.7497039 -1.7696743 -1.4522653 -0.74885869 -0.046433926 -0.0645051 -0.69209003 -1.0958107 -1.1873753 -1.24417 -1.6025653 -2.1370773][-1.813904 -1.8059938 -1.9834235 -2.2436621 -2.3795218 -2.392061 -2.1476862 -1.6364455 -1.3648036 -1.5869691 -1.570591 -1.3678782 -1.3664055 -1.7003915 -2.1887624][-2.3481007 -2.2720041 -2.2747514 -2.3473527 -2.4952316 -2.6106749 -2.4277546 -1.9161716 -1.5557477 -1.6208532 -1.4697697 -1.2000461 -1.2491415 -1.6009841 -2.1056869][-2.981132 -2.8042345 -2.6222658 -2.5027347 -2.5472054 -2.5214791 -2.167449 -1.6720178 -1.4459875 -1.5486438 -1.4306681 -1.1456275 -1.0983059 -1.2818074 -1.6536219][-3.6190596 -3.4837205 -3.2927976 -3.0797148 -2.9428129 -2.6803074 -2.1545892 -1.7120504 -1.6358747 -1.7525446 -1.6458428 -1.3544421 -1.1357601 -1.0403631 -1.1498318]]...]
INFO - root - 2017-12-07 06:50:38.582004: step 16910, loss = 0.60, batch loss = 0.53 (6.8 examples/sec; 1.180 sec/batch; 103h:25m:11s remains)
INFO - root - 2017-12-07 06:50:50.255236: step 16920, loss = 0.83, batch loss = 0.76 (6.6 examples/sec; 1.207 sec/batch; 105h:46m:29s remains)
INFO - root - 2017-12-07 06:51:01.852203: step 16930, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 1.189 sec/batch; 104h:10m:56s remains)
INFO - root - 2017-12-07 06:51:13.383319: step 16940, loss = 0.72, batch loss = 0.64 (7.0 examples/sec; 1.137 sec/batch; 99h:38m:50s remains)
INFO - root - 2017-12-07 06:51:25.060515: step 16950, loss = 0.84, batch loss = 0.77 (6.8 examples/sec; 1.184 sec/batch; 103h:46m:38s remains)
INFO - root - 2017-12-07 06:51:36.801003: step 16960, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 1.126 sec/batch; 98h:43m:48s remains)
INFO - root - 2017-12-07 06:51:48.291757: step 16970, loss = 0.62, batch loss = 0.54 (6.6 examples/sec; 1.213 sec/batch; 106h:20m:01s remains)
INFO - root - 2017-12-07 06:51:59.956497: step 16980, loss = 0.74, batch loss = 0.67 (6.7 examples/sec; 1.188 sec/batch; 104h:09m:01s remains)
INFO - root - 2017-12-07 06:52:11.455459: step 16990, loss = 0.68, batch loss = 0.60 (7.1 examples/sec; 1.131 sec/batch; 99h:09m:09s remains)
INFO - root - 2017-12-07 06:52:23.000071: step 17000, loss = 0.59, batch loss = 0.52 (6.8 examples/sec; 1.182 sec/batch; 103h:34m:11s remains)
2017-12-07 06:52:23.937488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9240615 -1.944155 -2.0191112 -2.0527406 -2.0764804 -2.2054491 -2.3584256 -2.3952322 -2.4464293 -2.5208864 -2.5269976 -2.3203764 -1.8796792 -1.5797746 -1.5359631][-1.9554291 -2.0831685 -2.217469 -2.2237027 -2.1214495 -2.0577049 -2.0194852 -1.976088 -2.0581021 -2.1509621 -2.2046316 -2.1527708 -1.8975828 -1.674505 -1.5808933][-2.0788665 -2.2752943 -2.4511228 -2.4221747 -2.235306 -2.0300813 -1.8487096 -1.7471178 -1.7996149 -1.8141155 -1.8679101 -1.9618347 -1.8893166 -1.7816014 -1.7189965][-2.1758924 -2.4570494 -2.6784043 -2.6597776 -2.5052543 -2.3073394 -2.1267526 -2.0684719 -2.1196322 -2.0341222 -2.0010941 -2.059217 -1.9547591 -1.8446665 -1.795054][-2.0527604 -2.4135959 -2.6255493 -2.6469083 -2.6112647 -2.5150471 -2.4133086 -2.4379981 -2.5176671 -2.3717375 -2.2613933 -2.2427418 -2.0582364 -1.9300056 -1.8719118][-1.6475821 -2.0144441 -2.1503394 -2.1980758 -2.2649682 -2.2240567 -2.1750655 -2.2941482 -2.4561231 -2.3536034 -2.2653346 -2.2514369 -2.0757165 -1.9601228 -1.8944888][-1.3984673 -1.6312234 -1.6160288 -1.6449625 -1.707613 -1.6036668 -1.5231915 -1.6828945 -1.9195287 -1.9058411 -1.9285715 -2.0005219 -1.9119976 -1.8241796 -1.7531989][-1.6184893 -1.5669801 -1.3497772 -1.3056195 -1.2762966 -1.0622492 -0.91211343 -1.0258377 -1.2259977 -1.2303841 -1.3761885 -1.567874 -1.6097345 -1.5919914 -1.5876756][-2.0795569 -1.7991056 -1.4580798 -1.3469419 -1.2333093 -1.0093575 -0.85832238 -0.92496657 -1.0215597 -0.96332192 -1.1396334 -1.3563468 -1.464668 -1.5180824 -1.6061575][-2.5061536 -2.1732378 -1.8598359 -1.7329738 -1.5699525 -1.3964458 -1.3119574 -1.3829217 -1.4250555 -1.322216 -1.4487743 -1.5992017 -1.6945827 -1.7823484 -1.8922408][-2.7354329 -2.4482169 -2.2227476 -2.1348996 -2.0032475 -1.925992 -1.943898 -2.0755858 -2.1257839 -2.0191195 -2.0702815 -2.1251483 -2.165246 -2.2534075 -2.3395522][-2.9494777 -2.7041149 -2.5063949 -2.4493489 -2.3949494 -2.4132936 -2.5193684 -2.6991284 -2.7614684 -2.6691232 -2.6502991 -2.6001904 -2.5492027 -2.5896022 -2.6164594][-3.1730511 -2.9503608 -2.7367835 -2.6924987 -2.7143159 -2.7878296 -2.922286 -3.0692611 -3.0847507 -3.0025039 -2.94495 -2.8263674 -2.7133312 -2.6998391 -2.6707411][-3.1843529 -3.0095503 -2.8251648 -2.7951126 -2.8476124 -2.9258385 -3.0326214 -3.1061764 -3.0776596 -3.0080113 -2.9296527 -2.7784457 -2.6414824 -2.5909381 -2.5390534][-3.026288 -2.8873849 -2.7357492 -2.6916082 -2.7081788 -2.7549253 -2.8324366 -2.8704948 -2.8483033 -2.8106909 -2.73628 -2.583992 -2.4397125 -2.3638797 -2.3099856]]...]
INFO - root - 2017-12-07 06:52:35.595027: step 17010, loss = 0.71, batch loss = 0.64 (7.3 examples/sec; 1.102 sec/batch; 96h:33m:26s remains)
INFO - root - 2017-12-07 06:52:47.358054: step 17020, loss = 1.04, batch loss = 0.97 (7.0 examples/sec; 1.139 sec/batch; 99h:49m:34s remains)
INFO - root - 2017-12-07 06:52:58.948767: step 17030, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 1.136 sec/batch; 99h:32m:33s remains)
INFO - root - 2017-12-07 06:53:10.978928: step 17040, loss = 0.80, batch loss = 0.73 (6.5 examples/sec; 1.240 sec/batch; 108h:38m:35s remains)
INFO - root - 2017-12-07 06:53:22.795972: step 17050, loss = 0.90, batch loss = 0.83 (6.8 examples/sec; 1.179 sec/batch; 103h:17m:50s remains)
INFO - root - 2017-12-07 06:53:34.426706: step 17060, loss = 0.65, batch loss = 0.57 (7.3 examples/sec; 1.103 sec/batch; 96h:36m:22s remains)
INFO - root - 2017-12-07 06:53:46.097750: step 17070, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 1.156 sec/batch; 101h:15m:47s remains)
INFO - root - 2017-12-07 06:53:57.823843: step 17080, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 1.131 sec/batch; 99h:05m:56s remains)
INFO - root - 2017-12-07 06:54:09.431604: step 17090, loss = 0.91, batch loss = 0.84 (6.8 examples/sec; 1.180 sec/batch; 103h:21m:49s remains)
INFO - root - 2017-12-07 06:54:21.167679: step 17100, loss = 0.73, batch loss = 0.66 (6.3 examples/sec; 1.263 sec/batch; 110h:38m:25s remains)
2017-12-07 06:54:22.073027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7339859 -3.671279 -3.3703117 -3.1456792 -3.0984695 -3.2279634 -3.5060115 -3.7454348 -3.827579 -3.7063298 -3.4691663 -3.2841573 -3.2143912 -3.1859627 -3.1504903][-3.5502 -3.3599679 -2.8124468 -2.436409 -2.3219159 -2.4868052 -2.8794193 -3.3109431 -3.6380424 -3.7174203 -3.5763085 -3.3669448 -3.1548438 -2.9523797 -2.8053107][-3.2986221 -3.0741715 -2.3785608 -1.8120894 -1.5192149 -1.6206117 -2.0791533 -2.6804242 -3.1964571 -3.4522538 -3.464519 -3.3531349 -3.1173482 -2.8056974 -2.506438][-3.2251556 -3.1581838 -2.6336088 -1.9886429 -1.3271379 -0.94100118 -1.0032649 -1.4636052 -2.0398226 -2.5048985 -2.862056 -3.1065028 -3.0603852 -2.7531457 -2.3022943][-3.2540097 -3.466485 -3.2711749 -2.6701648 -1.7753794 -0.87101483 -0.2365427 -0.19002438 -0.55336356 -1.0778141 -1.8431883 -2.561168 -2.8162203 -2.6020365 -2.1500762][-2.9763091 -3.5126681 -3.5707722 -2.9971375 -2.0420334 -0.94976425 0.048722267 0.39099884 0.12155962 -0.43864059 -1.3641105 -2.2725375 -2.7244735 -2.7424455 -2.590863][-2.6258307 -3.2291775 -3.269449 -2.638494 -1.6551571 -0.5139122 0.54334593 0.70939255 0.010887623 -0.85591769 -1.6671855 -2.1516027 -2.325877 -2.4656143 -2.8245292][-2.6512384 -3.1836805 -3.1990662 -2.66782 -1.8261051 -0.68954062 0.38857126 0.39116859 -0.800401 -1.9598544 -2.4405231 -2.1559865 -1.6789672 -1.6599817 -2.3606968][-2.7506163 -3.1938224 -3.3513908 -3.148675 -2.724436 -1.8135154 -0.80294847 -0.75647879 -1.9050086 -2.9217529 -3.0270784 -2.2728548 -1.4507048 -1.2898278 -1.972177][-2.8237727 -2.9518909 -3.1440282 -3.2492514 -3.2992542 -2.7801147 -2.0334325 -1.9531553 -2.7284343 -3.3128326 -3.0699883 -2.20567 -1.4190147 -1.3255289 -1.8872259][-3.0097144 -2.77553 -2.8193283 -3.0361915 -3.3479643 -3.1923637 -2.7635937 -2.7163434 -3.1838741 -3.4154048 -3.065233 -2.3773055 -1.8171601 -1.830976 -2.2083616][-2.9059467 -2.5494347 -2.5365148 -2.8483918 -3.2982674 -3.4189405 -3.2526219 -3.2485225 -3.4660714 -3.4532733 -3.1842356 -2.7901659 -2.5126026 -2.5873623 -2.7031934][-2.6169498 -2.2819335 -2.3059335 -2.6842499 -3.2024126 -3.5112348 -3.54099 -3.5679145 -3.5826211 -3.4411335 -3.3076568 -3.2024069 -3.2072322 -3.3872585 -3.3921156][-2.7494478 -2.5705056 -2.647213 -2.95197 -3.38689 -3.7513494 -3.8547814 -3.8276887 -3.7001314 -3.5369759 -3.5329013 -3.6076155 -3.7510302 -3.9336281 -3.9094138][-3.3115878 -3.280292 -3.3525159 -3.45333 -3.6565988 -3.9535429 -4.087141 -4.0850072 -3.9554336 -3.8341846 -3.8881738 -3.9761899 -4.0548673 -4.1173654 -4.0317497]]...]
INFO - root - 2017-12-07 06:54:33.579674: step 17110, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 1.142 sec/batch; 100h:03m:43s remains)
INFO - root - 2017-12-07 06:54:45.246270: step 17120, loss = 0.83, batch loss = 0.76 (6.9 examples/sec; 1.162 sec/batch; 101h:49m:32s remains)
INFO - root - 2017-12-07 06:54:56.916106: step 17130, loss = 0.75, batch loss = 0.67 (6.9 examples/sec; 1.155 sec/batch; 101h:13m:12s remains)
INFO - root - 2017-12-07 06:55:08.475218: step 17140, loss = 0.77, batch loss = 0.70 (6.9 examples/sec; 1.157 sec/batch; 101h:22m:51s remains)
INFO - root - 2017-12-07 06:55:20.210409: step 17150, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 1.118 sec/batch; 97h:57m:44s remains)
INFO - root - 2017-12-07 06:55:31.833503: step 17160, loss = 0.76, batch loss = 0.68 (7.2 examples/sec; 1.116 sec/batch; 97h:43m:00s remains)
INFO - root - 2017-12-07 06:55:43.616695: step 17170, loss = 0.96, batch loss = 0.89 (7.1 examples/sec; 1.131 sec/batch; 99h:03m:36s remains)
INFO - root - 2017-12-07 06:55:55.389240: step 17180, loss = 0.68, batch loss = 0.61 (6.3 examples/sec; 1.267 sec/batch; 110h:56m:22s remains)
INFO - root - 2017-12-07 06:56:07.257846: step 17190, loss = 0.71, batch loss = 0.64 (6.5 examples/sec; 1.232 sec/batch; 107h:55m:24s remains)
INFO - root - 2017-12-07 06:56:18.851316: step 17200, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 1.151 sec/batch; 100h:46m:01s remains)
2017-12-07 06:56:19.727200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2395015 -1.2537928 -1.2452903 -1.2458923 -1.2295516 -1.1996696 -1.2040575 -1.2585342 -1.3128903 -1.3194425 -1.3044171 -1.3151121 -1.3593974 -1.4049795 -1.4333582][-1.2300718 -1.2370994 -1.221267 -1.215694 -1.1806405 -1.1226487 -1.1090949 -1.1543555 -1.1959791 -1.1966925 -1.1929519 -1.2208054 -1.284132 -1.3542571 -1.409157][-1.2249651 -1.2274222 -1.204865 -1.1916692 -1.1375325 -1.043237 -0.98804164 -0.99899817 -1.0127575 -1.0084031 -1.0256126 -1.0852387 -1.187 -1.290976 -1.3750703][-1.2217252 -1.22714 -1.2105231 -1.2013786 -1.1470542 -1.0393198 -0.9613049 -0.95552707 -0.94860768 -0.94253922 -0.97375059 -1.0478747 -1.1631029 -1.2664456 -1.3534932][-1.2317107 -1.245615 -1.2421939 -1.2401736 -1.1905189 -1.068707 -0.9659946 -0.93904328 -0.91430736 -0.91976595 -0.99429226 -1.1157484 -1.2502794 -1.3434684 -1.4063823][-1.1972649 -1.229347 -1.2421203 -1.2480536 -1.2042713 -1.0850046 -0.96908855 -0.913733 -0.85000443 -0.8449626 -0.9601965 -1.13005 -1.2907228 -1.3839822 -1.4387219][-1.1096082 -1.169523 -1.2050171 -1.2227185 -1.1791334 -1.072947 -0.96696258 -0.89904213 -0.78703594 -0.74097848 -0.86892915 -1.0669205 -1.2433009 -1.3353214 -1.3958988][-1.1858809 -1.214556 -1.2247112 -1.2288144 -1.1867299 -1.1068957 -1.034755 -0.97725368 -0.8355453 -0.74312639 -0.85534358 -1.0671785 -1.2493415 -1.3283079 -1.3689423][-1.315779 -1.3118293 -1.2985251 -1.2828166 -1.2372768 -1.1872449 -1.1798391 -1.1825676 -1.0699074 -0.9809494 -1.095758 -1.3134041 -1.4655428 -1.4820876 -1.4464841][-1.2971342 -1.2987354 -1.3021159 -1.2978454 -1.2637956 -1.2323327 -1.27157 -1.3300638 -1.2621737 -1.1923027 -1.3121846 -1.5324302 -1.6699839 -1.663794 -1.5886948][-1.2058578 -1.2391171 -1.277699 -1.3178332 -1.3237116 -1.3218834 -1.3868132 -1.4688709 -1.4171028 -1.3267605 -1.399493 -1.5770564 -1.7054675 -1.7240231 -1.6733651][-1.1688135 -1.2369041 -1.3080926 -1.3935521 -1.4537311 -1.5065448 -1.6098201 -1.7130637 -1.6615648 -1.5212336 -1.491529 -1.5601873 -1.6360602 -1.6610281 -1.6370583][-1.2409558 -1.2942791 -1.3500824 -1.4383395 -1.5325792 -1.6425567 -1.7963235 -1.939692 -1.9190214 -1.7712448 -1.6621428 -1.6199336 -1.6200957 -1.6183889 -1.5960181][-1.3086243 -1.3344035 -1.3547342 -1.4091146 -1.5015116 -1.6371582 -1.822294 -1.9855802 -1.9982162 -1.8865821 -1.7606804 -1.6703954 -1.6251264 -1.6035404 -1.5817289][-1.3418121 -1.3489993 -1.339823 -1.3629465 -1.4318595 -1.5514333 -1.7188671 -1.8641174 -1.8930116 -1.8321373 -1.7429826 -1.6661537 -1.619133 -1.5945811 -1.5797091]]...]
INFO - root - 2017-12-07 06:56:31.378342: step 17210, loss = 0.60, batch loss = 0.53 (7.0 examples/sec; 1.138 sec/batch; 99h:41m:07s remains)
INFO - root - 2017-12-07 06:56:43.186251: step 17220, loss = 0.75, batch loss = 0.68 (6.7 examples/sec; 1.188 sec/batch; 104h:03m:30s remains)
INFO - root - 2017-12-07 06:56:54.941660: step 17230, loss = 0.85, batch loss = 0.77 (6.7 examples/sec; 1.201 sec/batch; 105h:08m:52s remains)
INFO - root - 2017-12-07 06:57:06.611534: step 17240, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 1.127 sec/batch; 98h:40m:12s remains)
INFO - root - 2017-12-07 06:57:18.396176: step 17250, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 1.125 sec/batch; 98h:32m:11s remains)
INFO - root - 2017-12-07 06:57:30.096582: step 17260, loss = 0.72, batch loss = 0.65 (7.1 examples/sec; 1.122 sec/batch; 98h:13m:42s remains)
INFO - root - 2017-12-07 06:57:41.697300: step 17270, loss = 0.79, batch loss = 0.71 (6.8 examples/sec; 1.168 sec/batch; 102h:18m:29s remains)
INFO - root - 2017-12-07 06:57:53.276049: step 17280, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.199 sec/batch; 104h:59m:05s remains)
INFO - root - 2017-12-07 06:58:05.034364: step 17290, loss = 1.05, batch loss = 0.98 (7.1 examples/sec; 1.122 sec/batch; 98h:13m:13s remains)
INFO - root - 2017-12-07 06:58:16.826030: step 17300, loss = 0.79, batch loss = 0.72 (7.1 examples/sec; 1.128 sec/batch; 98h:47m:15s remains)
2017-12-07 06:58:17.756078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1827369 -2.874033 -2.54277 -2.2109685 -2.1100607 -2.3038514 -2.5623593 -2.7156138 -2.791132 -2.9791903 -3.2185657 -3.2979879 -3.2879858 -3.172828 -2.8299406][-3.2626152 -2.8697658 -2.4825788 -2.0954931 -1.9358165 -2.0983508 -2.3473232 -2.5025854 -2.5976861 -2.8068273 -2.979413 -2.9737234 -2.9125395 -2.7757487 -2.4222724][-3.4609876 -2.974721 -2.5039458 -2.0679014 -1.8660152 -1.9853852 -2.2216592 -2.3738556 -2.4670172 -2.6426206 -2.7176175 -2.6237226 -2.5477138 -2.4437375 -2.1313529][-3.7561727 -3.2461889 -2.7392812 -2.269958 -1.9719858 -1.9348321 -2.0616238 -2.1669917 -2.2456632 -2.3366747 -2.2942445 -2.1228411 -2.0657361 -2.0267527 -1.7893443][-4.0814681 -3.6474888 -3.1671491 -2.6736693 -2.2346408 -1.950613 -1.8397408 -1.7921863 -1.8077428 -1.8188112 -1.7340097 -1.6083362 -1.6466277 -1.7152417 -1.5786755][-4.2481365 -3.8510194 -3.3779297 -2.9393234 -2.5213726 -2.15559 -1.8862927 -1.6543727 -1.5306261 -1.4376309 -1.3605762 -1.3718324 -1.5668156 -1.7810011 -1.798393][-4.0799432 -3.6357164 -3.1401196 -2.8700118 -2.695111 -2.479975 -2.2530901 -1.9246926 -1.6681502 -1.4543154 -1.3385496 -1.4235539 -1.6830513 -1.9869359 -2.1584158][-3.8732409 -3.350359 -2.8244708 -2.755374 -2.8266723 -2.7476449 -2.556742 -2.1484218 -1.7868116 -1.5361919 -1.4674807 -1.6171131 -1.8926494 -2.219079 -2.4975824][-3.887578 -3.3217862 -2.7561102 -2.7581758 -2.8886657 -2.7798493 -2.50252 -2.0050023 -1.6295569 -1.5229323 -1.6768098 -1.964005 -2.2731111 -2.586463 -2.8715923][-4.0874929 -3.5504975 -2.9348526 -2.825655 -2.8375003 -2.6421123 -2.3085916 -1.8424954 -1.5738325 -1.6737752 -2.0174398 -2.3745637 -2.6368122 -2.8759179 -3.1299963][-4.2019711 -3.7422249 -3.1603022 -2.936801 -2.811291 -2.5506902 -2.2316942 -1.9374444 -1.8387215 -2.021811 -2.3495858 -2.6354251 -2.798898 -2.9529264 -3.1637387][-4.3497815 -4.01649 -3.5160232 -3.2155514 -2.9353931 -2.5101283 -2.0847905 -1.8820086 -1.8657877 -1.9767873 -2.1439071 -2.3033631 -2.3752122 -2.4574773 -2.6443026][-4.350369 -4.1645436 -3.686486 -3.2621756 -2.8732748 -2.3819566 -1.9486961 -1.8682678 -1.8973 -1.8896813 -1.8692019 -1.8928607 -1.8725164 -1.8616617 -1.9842658][-4.1251707 -4.034966 -3.5307202 -3.0325134 -2.6921926 -2.3794336 -2.1886511 -2.3711154 -2.5505748 -2.5110669 -2.3615103 -2.2874367 -2.1843333 -2.0428543 -2.0310576][-3.5581818 -3.4873924 -2.9835634 -2.5471845 -2.4061174 -2.3869746 -2.4741187 -2.8385921 -3.1440701 -3.1420307 -2.9678426 -2.8508687 -2.6959572 -2.4754839 -2.3924718]]...]
INFO - root - 2017-12-07 06:58:29.295000: step 17310, loss = 0.79, batch loss = 0.72 (7.3 examples/sec; 1.098 sec/batch; 96h:09m:35s remains)
INFO - root - 2017-12-07 06:58:40.994215: step 17320, loss = 0.68, batch loss = 0.61 (6.8 examples/sec; 1.175 sec/batch; 102h:54m:35s remains)
INFO - root - 2017-12-07 06:58:52.429130: step 17330, loss = 0.74, batch loss = 0.67 (7.5 examples/sec; 1.061 sec/batch; 92h:53m:35s remains)
INFO - root - 2017-12-07 06:59:04.187359: step 17340, loss = 0.72, batch loss = 0.65 (6.7 examples/sec; 1.201 sec/batch; 105h:07m:06s remains)
INFO - root - 2017-12-07 06:59:15.921510: step 17350, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.187 sec/batch; 103h:57m:10s remains)
INFO - root - 2017-12-07 06:59:27.547990: step 17360, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 1.120 sec/batch; 98h:04m:08s remains)
INFO - root - 2017-12-07 06:59:39.357282: step 17370, loss = 0.61, batch loss = 0.54 (6.9 examples/sec; 1.159 sec/batch; 101h:28m:37s remains)
INFO - root - 2017-12-07 06:59:51.086583: step 17380, loss = 0.89, batch loss = 0.82 (7.0 examples/sec; 1.135 sec/batch; 99h:23m:15s remains)
INFO - root - 2017-12-07 07:00:02.656563: step 17390, loss = 0.74, batch loss = 0.67 (7.3 examples/sec; 1.099 sec/batch; 96h:13m:58s remains)
INFO - root - 2017-12-07 07:00:14.430562: step 17400, loss = 0.91, batch loss = 0.84 (6.5 examples/sec; 1.240 sec/batch; 108h:31m:10s remains)
2017-12-07 07:00:15.347842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.92256594 -0.81625414 -0.61189008 -0.62823319 -0.87825418 -1.1014216 -1.3098717 -1.8264399 -2.5113521 -3.0598814 -3.4499698 -3.4949305 -3.14077 -2.9348762 -3.2982502][-1.9094622 -1.6867881 -1.4462812 -1.3798537 -1.4768641 -1.7843552 -2.2826238 -2.9535887 -3.6040382 -4.0347276 -4.1768584 -3.9311938 -3.3975902 -3.1151638 -3.4944279][-2.5217247 -2.4319181 -2.4159913 -2.5236821 -2.6292148 -2.9139462 -3.4736843 -4.1168408 -4.706121 -5.0876112 -5.1531887 -4.7971134 -4.1526742 -3.7610464 -3.9869463][-2.2099197 -2.1329403 -2.3447478 -2.6235571 -2.7570338 -2.9655495 -3.3133435 -3.6013875 -3.9660866 -4.4231277 -4.747014 -4.5087013 -3.8000689 -3.3003325 -3.4512012][-1.4184437 -1.3673904 -1.8667221 -2.2398479 -2.283551 -2.2770529 -2.1714661 -1.8406131 -1.9058342 -2.7296524 -3.8012459 -3.9989076 -3.2579365 -2.4766176 -2.421571][-0.6260283 -0.66095352 -1.4267068 -1.8851438 -1.8148699 -1.4396818 -0.54910946 0.55273294 0.48925257 -1.2519684 -3.2608287 -3.7577934 -2.907865 -1.7733493 -1.4546721][-0.54777265 -0.55370712 -1.3009448 -1.6858983 -1.5441074 -0.73299956 1.1441541 3.071485 2.7694502 -0.039221764 -2.7407911 -3.3620255 -2.3596823 -0.97879958 -0.59821582][-0.82892942 -0.78701234 -1.3422985 -1.5924499 -1.4867089 -0.49460864 1.9085112 4.1742811 3.5805359 0.31758785 -2.3838856 -2.8964162 -1.9097049 -0.64335632 -0.3952322][-1.2690966 -1.2887325 -1.7192175 -1.982614 -2.0628843 -1.1917586 1.0348587 2.8705311 2.0668402 -0.65127468 -2.5453343 -2.7674656 -1.8792996 -0.83927894 -0.68202472][-1.8003304 -1.7900872 -2.0530238 -2.3550854 -2.6243765 -2.058125 -0.33222866 0.97347832 0.24120808 -1.5611327 -2.6008251 -2.6552534 -1.9156482 -1.0584893 -0.91544604][-1.9535508 -1.8189154 -1.8444026 -2.0755358 -2.4812949 -2.3108792 -1.167244 -0.26376867 -0.65284252 -1.5049694 -1.9968967 -2.1743302 -1.7227633 -1.0803757 -0.9705379][-1.8700237 -1.6298664 -1.4150984 -1.4328673 -1.7998958 -1.9266038 -1.3407393 -0.84072042 -1.0183141 -1.2024333 -1.3917067 -1.747659 -1.6583698 -1.3725393 -1.3811238][-1.6425946 -1.5132258 -1.2785316 -1.1062717 -1.4101119 -1.6994553 -1.5669754 -1.4203012 -1.5214632 -1.4570005 -1.6442065 -2.0793252 -2.0943089 -2.051681 -2.2349403][-1.4739656 -1.6234791 -1.5130041 -1.3117785 -1.5979612 -1.9031224 -2.0686805 -2.2568684 -2.3387043 -2.1328621 -2.3429871 -2.800786 -2.8246393 -2.8476009 -3.0630782][-1.8792591 -2.1690137 -2.0870867 -1.891645 -2.0710917 -2.1958988 -2.4250145 -2.7493987 -2.724097 -2.4256749 -2.6777573 -3.1453786 -3.1440666 -3.1589017 -3.351532]]...]
INFO - root - 2017-12-07 07:00:26.900661: step 17410, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 1.112 sec/batch; 97h:19m:08s remains)
INFO - root - 2017-12-07 07:00:38.512909: step 17420, loss = 0.69, batch loss = 0.62 (7.3 examples/sec; 1.098 sec/batch; 96h:04m:34s remains)
INFO - root - 2017-12-07 07:00:50.169518: step 17430, loss = 0.83, batch loss = 0.76 (6.7 examples/sec; 1.186 sec/batch; 103h:47m:13s remains)
INFO - root - 2017-12-07 07:01:02.091786: step 17440, loss = 0.82, batch loss = 0.74 (6.4 examples/sec; 1.248 sec/batch; 109h:11m:57s remains)
INFO - root - 2017-12-07 07:01:13.617996: step 17450, loss = 0.79, batch loss = 0.72 (7.6 examples/sec; 1.051 sec/batch; 91h:56m:18s remains)
INFO - root - 2017-12-07 07:01:25.235714: step 17460, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 1.154 sec/batch; 100h:56m:43s remains)
INFO - root - 2017-12-07 07:01:36.933636: step 17470, loss = 0.80, batch loss = 0.73 (6.9 examples/sec; 1.162 sec/batch; 101h:39m:57s remains)
INFO - root - 2017-12-07 07:01:48.647687: step 17480, loss = 0.72, batch loss = 0.64 (6.7 examples/sec; 1.195 sec/batch; 104h:33m:08s remains)
INFO - root - 2017-12-07 07:02:00.228770: step 17490, loss = 0.80, batch loss = 0.73 (6.7 examples/sec; 1.189 sec/batch; 104h:02m:25s remains)
INFO - root - 2017-12-07 07:02:11.909487: step 17500, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.139 sec/batch; 99h:41m:49s remains)
2017-12-07 07:02:12.717851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7947252 -2.7149148 -2.6283779 -2.593349 -2.6418509 -2.7571735 -2.8971109 -3.0460088 -3.1740358 -3.2658958 -3.3120279 -3.2977004 -3.2783523 -3.28471 -3.2987263][-2.2048786 -2.0084078 -1.8054287 -1.7226512 -1.8161042 -1.9839621 -2.1767952 -2.4148211 -2.6680236 -2.913795 -3.1046307 -3.1821404 -3.2193632 -3.2598505 -3.2836947][-1.5884495 -1.3372517 -1.0694909 -0.97883415 -1.1610489 -1.3847589 -1.5609605 -1.7330074 -1.941447 -2.2265198 -2.5006943 -2.6577547 -2.7824273 -2.9103749 -2.9926424][-1.0636325 -0.80529308 -0.53943682 -0.48995519 -0.79429007 -1.1091609 -1.2728217 -1.3451593 -1.457057 -1.7086155 -1.9671273 -2.1023152 -2.2452233 -2.4315515 -2.5632653][-0.47534871 -0.31223249 -0.14925909 -0.17646933 -0.54379225 -0.88035822 -0.97948408 -0.92138648 -0.94953966 -1.2256498 -1.5173492 -1.6541932 -1.7973647 -2.0144677 -2.1904781][0.13500547 0.009291172 -0.037128925 -0.13592148 -0.44712138 -0.62009883 -0.4641695 -0.13806677 -0.080498695 -0.45365858 -0.85497427 -1.0503685 -1.2047653 -1.4026198 -1.572783][0.47255564 0.0044550896 -0.24660063 -0.38238716 -0.56432891 -0.43498778 0.1342802 0.836946 0.97452641 0.45512104 -0.094460964 -0.35819817 -0.49730968 -0.62150836 -0.70062375][0.24404955 -0.36115646 -0.59840345 -0.63553834 -0.68205261 -0.35910416 0.46105576 1.3910017 1.5437369 0.92176628 0.32068014 0.091113567 0.015416145 -0.096179962 -0.13775778][-0.56466579 -1.0382268 -1.0508623 -0.89703035 -0.8680625 -0.62338638 0.035548687 0.76928616 0.85954475 0.31756115 -0.097644806 -0.1129837 -0.056063652 -0.21610451 -0.3424468][-1.4788756 -1.6421583 -1.3993747 -1.1598537 -1.1790335 -1.1682961 -0.87091875 -0.50001717 -0.525676 -0.91652918 -1.1052146 -0.92814827 -0.76181173 -0.95005012 -1.1375716][-2.0250273 -1.900934 -1.5359948 -1.3486946 -1.4916432 -1.6844394 -1.6677005 -1.5667722 -1.7005899 -1.9802938 -2.0316291 -1.7930486 -1.6240427 -1.8201432 -1.9829266][-1.9528606 -1.7033441 -1.3804088 -1.3140745 -1.5589442 -1.8426521 -1.9538851 -1.985395 -2.1729374 -2.3821306 -2.3612225 -2.1478684 -2.0320837 -2.2123723 -2.3159204][-1.5405424 -1.3003249 -1.0944018 -1.1234572 -1.386209 -1.6621945 -1.8084698 -1.8992579 -2.0879002 -2.2397609 -2.1887386 -2.0144742 -1.9304433 -2.0549526 -2.1116228][-1.6529589 -1.4851203 -1.3865056 -1.4466565 -1.6259758 -1.7894509 -1.8775549 -1.9590614 -2.1150439 -2.23626 -2.2100251 -2.1071141 -2.0522063 -2.1074357 -2.1138124][-2.3330948 -2.2692766 -2.2671771 -2.3354385 -2.4328926 -2.4982533 -2.5228271 -2.5642295 -2.6564569 -2.7413359 -2.7545991 -2.7235041 -2.6963959 -2.6961751 -2.6616316]]...]
INFO - root - 2017-12-07 07:02:24.125738: step 17510, loss = 0.86, batch loss = 0.79 (7.5 examples/sec; 1.068 sec/batch; 93h:26m:37s remains)
INFO - root - 2017-12-07 07:02:35.787504: step 17520, loss = 0.62, batch loss = 0.54 (6.5 examples/sec; 1.240 sec/batch; 108h:30m:07s remains)
INFO - root - 2017-12-07 07:02:47.330382: step 17530, loss = 0.73, batch loss = 0.66 (6.6 examples/sec; 1.221 sec/batch; 106h:47m:10s remains)
INFO - root - 2017-12-07 07:02:59.003998: step 17540, loss = 0.68, batch loss = 0.60 (6.6 examples/sec; 1.203 sec/batch; 105h:17m:06s remains)
INFO - root - 2017-12-07 07:03:10.623662: step 17550, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 1.124 sec/batch; 98h:22m:16s remains)
INFO - root - 2017-12-07 07:03:22.347942: step 17560, loss = 0.73, batch loss = 0.66 (7.0 examples/sec; 1.141 sec/batch; 99h:48m:13s remains)
INFO - root - 2017-12-07 07:03:34.144229: step 17570, loss = 1.00, batch loss = 0.93 (7.2 examples/sec; 1.107 sec/batch; 96h:52m:12s remains)
INFO - root - 2017-12-07 07:03:45.704637: step 17580, loss = 0.73, batch loss = 0.65 (6.7 examples/sec; 1.191 sec/batch; 104h:12m:56s remains)
INFO - root - 2017-12-07 07:03:57.315729: step 17590, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.187 sec/batch; 103h:52m:05s remains)
INFO - root - 2017-12-07 07:04:09.027986: step 17600, loss = 0.92, batch loss = 0.85 (7.0 examples/sec; 1.142 sec/batch; 99h:53m:17s remains)
2017-12-07 07:04:09.900421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4843831 -3.6396124 -3.7475631 -3.8822279 -3.9795146 -3.9787288 -3.8101275 -3.530211 -3.3502591 -3.3192594 -3.3278756 -3.3661139 -3.3654704 -3.3642206 -3.4377313][-3.7237978 -3.9349823 -4.0174265 -4.086472 -4.0929961 -3.9440057 -3.6025229 -3.2378674 -3.0583725 -3.0985332 -3.198143 -3.3116388 -3.297061 -3.2838218 -3.3752058][-3.7592425 -3.9561415 -3.9421957 -3.8801868 -3.7477322 -3.4626455 -3.0670981 -2.7571135 -2.6223192 -2.6786616 -2.8229537 -3.0744624 -3.1780553 -3.1569223 -3.1523776][-3.8894689 -4.0059834 -3.843111 -3.6029787 -3.3413472 -3.0059328 -2.7305267 -2.6187162 -2.5644097 -2.6091709 -2.712882 -3.0131114 -3.2547212 -3.283675 -3.2222385][-3.9609668 -3.9596128 -3.643537 -3.2126744 -2.778749 -2.3886256 -2.2141032 -2.28137 -2.4218092 -2.6336794 -2.7812276 -3.0357375 -3.2962625 -3.402678 -3.4416561][-3.6720462 -3.6163552 -3.2888172 -2.8142653 -2.267055 -1.7496238 -1.5054042 -1.6604397 -2.0127447 -2.3739016 -2.5958061 -2.806869 -3.0864716 -3.3414941 -3.6321847][-3.1562815 -3.0975158 -2.8798513 -2.4861813 -1.8915267 -1.2478852 -0.84443712 -0.9852705 -1.4265037 -1.7973719 -2.0095644 -2.1989071 -2.5569718 -3.0108175 -3.6158547][-2.7611341 -2.6307635 -2.4250534 -2.0729437 -1.5198147 -0.89173985 -0.405159 -0.4950366 -0.883234 -1.1579828 -1.317883 -1.5143929 -1.9248543 -2.4856281 -3.2539816][-2.9220142 -2.6567369 -2.40159 -2.0416133 -1.5922434 -1.1751721 -0.77033186 -0.657501 -0.75127697 -0.88393283 -1.1098104 -1.398072 -1.8202834 -2.376991 -3.0990896][-3.3354514 -3.0260515 -2.8304632 -2.5590358 -2.3017488 -2.1238561 -1.7956264 -1.4783895 -1.2611623 -1.2711642 -1.5757799 -1.9333742 -2.2968092 -2.7572813 -3.3041563][-3.6293442 -3.436506 -3.3904102 -3.2525735 -3.2076077 -3.2299778 -3.00411 -2.6440253 -2.3000681 -2.20805 -2.4310508 -2.7113566 -2.9798491 -3.3573289 -3.7913678][-3.5538194 -3.4171915 -3.3881207 -3.2500596 -3.3012 -3.4462633 -3.3617797 -3.1471784 -2.9035184 -2.7698293 -2.8600349 -3.0663648 -3.293642 -3.6172926 -3.9560006][-3.1694164 -3.0138922 -2.9558225 -2.8052871 -2.8987193 -3.075417 -3.0881085 -3.0538588 -2.959589 -2.8468766 -2.8657551 -2.9980254 -3.1235409 -3.2922649 -3.4763417][-2.7716255 -2.5634995 -2.4736192 -2.4185488 -2.5581403 -2.6884322 -2.7271943 -2.8150949 -2.8652127 -2.7809815 -2.7319765 -2.7215936 -2.6865938 -2.6603451 -2.6477695][-2.4251387 -2.2469575 -2.151345 -2.1486204 -2.2465837 -2.2540128 -2.2748435 -2.4231088 -2.5414143 -2.454083 -2.3792093 -2.3262708 -2.2851827 -2.2132552 -2.0394757]]...]
INFO - root - 2017-12-07 07:04:21.485044: step 17610, loss = 0.88, batch loss = 0.81 (6.8 examples/sec; 1.169 sec/batch; 102h:15m:41s remains)
INFO - root - 2017-12-07 07:04:33.068632: step 17620, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 1.148 sec/batch; 100h:23m:18s remains)
INFO - root - 2017-12-07 07:04:44.715390: step 17630, loss = 0.79, batch loss = 0.72 (7.2 examples/sec; 1.109 sec/batch; 97h:00m:25s remains)
INFO - root - 2017-12-07 07:04:56.257822: step 17640, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.157 sec/batch; 101h:09m:31s remains)
INFO - root - 2017-12-07 07:05:07.968084: step 17650, loss = 0.67, batch loss = 0.60 (7.3 examples/sec; 1.095 sec/batch; 95h:43m:37s remains)
INFO - root - 2017-12-07 07:05:19.831279: step 17660, loss = 0.90, batch loss = 0.83 (6.5 examples/sec; 1.232 sec/batch; 107h:46m:34s remains)
INFO - root - 2017-12-07 07:05:31.327590: step 17670, loss = 0.66, batch loss = 0.59 (6.4 examples/sec; 1.246 sec/batch; 109h:00m:27s remains)
INFO - root - 2017-12-07 07:05:43.033091: step 17680, loss = 0.72, batch loss = 0.65 (7.2 examples/sec; 1.110 sec/batch; 97h:06m:31s remains)
INFO - root - 2017-12-07 07:05:54.649486: step 17690, loss = 0.76, batch loss = 0.69 (7.1 examples/sec; 1.133 sec/batch; 99h:05m:22s remains)
INFO - root - 2017-12-07 07:06:06.305627: step 17700, loss = 1.14, batch loss = 1.07 (6.8 examples/sec; 1.180 sec/batch; 103h:11m:16s remains)
2017-12-07 07:06:07.161239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2809491 -3.172874 -2.868886 -2.7015672 -2.8124952 -2.9165025 -2.8042765 -2.7228751 -2.6313853 -2.3989697 -2.2308083 -2.237293 -2.4116106 -2.7502284 -2.7298932][-2.8490355 -2.6366835 -2.3580632 -2.3948646 -2.6508658 -2.7424805 -2.4564288 -2.12853 -1.8761683 -1.6258714 -1.5766068 -1.6303575 -1.8191426 -2.2779388 -2.4036012][-2.3387845 -2.0213623 -1.82376 -2.0574694 -2.3892407 -2.3916974 -1.9821537 -1.4900858 -1.137033 -0.87819839 -0.88550234 -0.965935 -1.2990553 -2.020731 -2.4028831][-1.828372 -1.4292493 -1.3209782 -1.6508205 -1.8976409 -1.7128377 -1.2602272 -0.78546429 -0.44450212 -0.25147533 -0.32518387 -0.46691608 -0.89831519 -1.683826 -2.1550739][-1.5018914 -1.2513978 -1.3099484 -1.58676 -1.5447176 -1.125037 -0.65718865 -0.2602129 0.0401659 0.1383667 -0.14251089 -0.55503774 -1.0967088 -1.798399 -2.1674221][-1.2441549 -1.2643762 -1.5128376 -1.6886463 -1.3367751 -0.87598848 -0.60090446 -0.32408905 0.019215584 0.15386677 -0.28945637 -0.96326232 -1.5929456 -2.240273 -2.5664129][-1.0016375 -1.1603982 -1.4560077 -1.5056682 -1.0192394 -0.70187163 -0.74660707 -0.66590405 -0.31830692 -0.13142204 -0.57938194 -1.2188914 -1.651257 -2.1561279 -2.5684566][-0.7480135 -0.98435283 -1.2089617 -1.0808356 -0.60971546 -0.51415372 -0.79109645 -0.793674 -0.44826722 -0.31365633 -0.77663279 -1.321486 -1.5693984 -1.92343 -2.2674403][-0.42983961 -0.67814517 -0.886889 -0.68766475 -0.32076883 -0.37415218 -0.60787106 -0.48261762 -0.096543312 0.011843681 -0.37662077 -0.91009092 -1.3278174 -1.8536267 -2.1184859][-0.030294895 -0.20273256 -0.4614079 -0.44062853 -0.35188627 -0.48334384 -0.503747 -0.21038103 0.11127615 0.16008472 0.0031924248 -0.29234886 -0.76587152 -1.3724599 -1.5759022][0.44933987 0.29633379 -0.032178402 -0.18035364 -0.30485964 -0.4896419 -0.42236853 -0.1940732 -0.099831581 -0.20530272 -0.20153332 -0.24214745 -0.53344727 -0.83536077 -0.73676085][0.8423624 0.61313486 0.30950403 0.17228603 0.0028691292 -0.21101475 -0.23098421 -0.1785326 -0.22800732 -0.38758326 -0.33154058 -0.2827363 -0.45785737 -0.467108 -0.076061726][0.85100365 0.51997232 0.35878325 0.34336615 0.197402 -0.017388821 -0.13496685 -0.18205738 -0.22576857 -0.33314276 -0.26084423 -0.17291975 -0.19970751 -0.012721062 0.41079473][0.52165127 0.18521976 0.26839542 0.40597248 0.33437109 0.12854624 -0.12354803 -0.27781487 -0.30482817 -0.3004365 -0.21591711 -0.12753916 -0.0055646896 0.2776947 0.6280117][0.0048055649 -0.16873312 0.16730452 0.41831684 0.44394398 0.28176737 0.00060510635 -0.19102383 -0.21537113 -0.18526602 -0.20272064 -0.22721815 -0.076322556 0.14796066 0.37343979]]...]
INFO - root - 2017-12-07 07:06:18.897746: step 17710, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.198 sec/batch; 104h:47m:13s remains)
INFO - root - 2017-12-07 07:06:30.709472: step 17720, loss = 0.79, batch loss = 0.72 (6.8 examples/sec; 1.171 sec/batch; 102h:22m:54s remains)
INFO - root - 2017-12-07 07:06:42.328094: step 17730, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 1.111 sec/batch; 97h:10m:24s remains)
INFO - root - 2017-12-07 07:06:54.024819: step 17740, loss = 0.82, batch loss = 0.75 (6.6 examples/sec; 1.205 sec/batch; 105h:22m:24s remains)
INFO - root - 2017-12-07 07:07:05.842994: step 17750, loss = 0.78, batch loss = 0.70 (6.9 examples/sec; 1.157 sec/batch; 101h:07m:39s remains)
INFO - root - 2017-12-07 07:07:17.477766: step 17760, loss = 0.86, batch loss = 0.79 (6.6 examples/sec; 1.212 sec/batch; 105h:56m:19s remains)
INFO - root - 2017-12-07 07:07:29.019159: step 17770, loss = 0.78, batch loss = 0.71 (6.9 examples/sec; 1.158 sec/batch; 101h:13m:52s remains)
INFO - root - 2017-12-07 07:07:40.704242: step 17780, loss = 0.63, batch loss = 0.56 (7.2 examples/sec; 1.118 sec/batch; 97h:42m:41s remains)
INFO - root - 2017-12-07 07:07:52.364535: step 17790, loss = 0.84, batch loss = 0.77 (6.6 examples/sec; 1.214 sec/batch; 106h:06m:20s remains)
INFO - root - 2017-12-07 07:08:04.161956: step 17800, loss = 0.68, batch loss = 0.61 (6.8 examples/sec; 1.174 sec/batch; 102h:35m:12s remains)
2017-12-07 07:08:05.063078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9592903 -2.7931855 -2.8362679 -3.0424089 -3.2724938 -3.3483577 -3.283555 -3.2855465 -3.3977976 -3.4709466 -3.3731689 -3.1217823 -2.8333116 -2.7059174 -2.7874393][-2.6789877 -2.4932318 -2.6314621 -2.9887278 -3.3276615 -3.3957984 -3.2495995 -3.1907318 -3.2505648 -3.2741156 -3.1301317 -2.8286908 -2.5290956 -2.4429979 -2.5820026][-2.3750031 -2.2863798 -2.5628633 -3.0234299 -3.3676858 -3.3322458 -3.0375714 -2.8679154 -2.8705988 -2.8862579 -2.7419286 -2.4318638 -2.1833322 -2.1833043 -2.3706129][-2.1915627 -2.2877247 -2.6717167 -3.1036239 -3.3036492 -3.0769112 -2.5938063 -2.3053968 -2.3439226 -2.5092359 -2.5063856 -2.263797 -2.0607636 -2.0501747 -2.1828239][-2.2005651 -2.4876373 -2.9339237 -3.2597265 -3.2396238 -2.7682028 -2.067678 -1.6656249 -1.805867 -2.1949987 -2.4090807 -2.2872903 -2.0958786 -2.0115659 -2.0760412][-2.4046309 -2.822448 -3.23834 -3.3566959 -3.0290065 -2.2721922 -1.400286 -0.98681474 -1.3103342 -1.9161594 -2.305656 -2.2759531 -2.0849185 -1.9974604 -2.1257946][-2.5721638 -3.0537558 -3.3571661 -3.1983905 -2.553905 -1.6026857 -0.75788093 -0.55347037 -1.1343901 -1.8816183 -2.3464084 -2.3507559 -2.1317856 -2.0636766 -2.3059509][-2.6460633 -3.1310613 -3.3034892 -2.9435725 -2.2129889 -1.4154689 -0.95309162 -1.0732024 -1.6720197 -2.2705095 -2.6016397 -2.524878 -2.2305663 -2.198617 -2.5478969][-2.8367052 -3.2081213 -3.1949906 -2.6877508 -1.9964406 -1.4994907 -1.4633288 -1.7713568 -2.1886556 -2.5019083 -2.6337292 -2.4512606 -2.1291142 -2.2100143 -2.7190387][-3.2532868 -3.4237676 -3.1682394 -2.5277486 -1.891654 -1.6271603 -1.8319407 -2.181416 -2.4384484 -2.5847073 -2.5942693 -2.3469718 -2.0321765 -2.2031922 -2.8018322][-3.651464 -3.5847158 -3.0568573 -2.3374636 -1.8658156 -1.8694358 -2.246798 -2.5929339 -2.7441392 -2.7977848 -2.7055068 -2.4085946 -2.0999737 -2.2334993 -2.7458568][-3.9018679 -3.6391091 -2.852036 -1.9991834 -1.6042449 -1.774199 -2.2583132 -2.603025 -2.6974921 -2.7021008 -2.5717921 -2.317802 -2.0781946 -2.1509154 -2.5342886][-3.9901609 -3.6106734 -2.7159343 -1.83324 -1.4614344 -1.6245279 -2.0618773 -2.3836725 -2.4809978 -2.4903729 -2.3636718 -2.1823752 -2.0174041 -2.0463555 -2.3459992][-4.0028548 -3.512568 -2.6323075 -1.9001424 -1.6755025 -1.8537307 -2.1822517 -2.4189394 -2.5171432 -2.5454268 -2.4233644 -2.2686467 -2.1279309 -2.1071281 -2.3178225][-3.8445067 -3.3217616 -2.5825129 -2.097322 -2.0547762 -2.2287261 -2.4070716 -2.5022955 -2.5580244 -2.6005325 -2.5284212 -2.4360754 -2.3502049 -2.3209765 -2.44302]]...]
INFO - root - 2017-12-07 07:08:16.608572: step 17810, loss = 0.82, batch loss = 0.75 (6.8 examples/sec; 1.183 sec/batch; 103h:24m:02s remains)
INFO - root - 2017-12-07 07:08:28.150027: step 17820, loss = 0.57, batch loss = 0.50 (6.9 examples/sec; 1.166 sec/batch; 101h:57m:47s remains)
INFO - root - 2017-12-07 07:08:39.855389: step 17830, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 1.202 sec/batch; 105h:04m:07s remains)
INFO - root - 2017-12-07 07:08:51.616009: step 17840, loss = 0.84, batch loss = 0.77 (7.1 examples/sec; 1.121 sec/batch; 98h:00m:23s remains)
INFO - root - 2017-12-07 07:09:03.339825: step 17850, loss = 0.65, batch loss = 0.58 (6.8 examples/sec; 1.184 sec/batch; 103h:29m:15s remains)
INFO - root - 2017-12-07 07:09:15.048040: step 17860, loss = 0.67, batch loss = 0.59 (6.5 examples/sec; 1.230 sec/batch; 107h:28m:48s remains)
INFO - root - 2017-12-07 07:09:26.595353: step 17870, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.158 sec/batch; 101h:09m:48s remains)
INFO - root - 2017-12-07 07:09:38.193575: step 17880, loss = 0.68, batch loss = 0.61 (7.1 examples/sec; 1.133 sec/batch; 99h:01m:26s remains)
INFO - root - 2017-12-07 07:09:49.891442: step 17890, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.129 sec/batch; 98h:40m:03s remains)
INFO - root - 2017-12-07 07:10:01.507438: step 17900, loss = 0.82, batch loss = 0.75 (6.6 examples/sec; 1.212 sec/batch; 105h:53m:35s remains)
2017-12-07 07:10:02.405664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.973434 -3.1938825 -3.5252452 -3.6925051 -3.4970706 -3.1759458 -3.2670708 -3.4631889 -3.2660782 -2.9356751 -2.5141773 -2.0008719 -1.6549549 -1.9061387 -2.5032046][-2.796319 -3.0737152 -3.5021996 -3.8275645 -3.6206975 -3.1233609 -3.0125942 -3.0942523 -2.8733091 -2.6013975 -2.3611596 -2.1222689 -2.0020876 -2.3446908 -2.8955312][-2.3988359 -2.6431062 -3.0492125 -3.5635839 -3.503746 -3.0063882 -2.7704182 -2.7362881 -2.4544442 -2.2732332 -2.3276095 -2.4614272 -2.5305514 -2.7986655 -3.1931114][-2.2123458 -2.2941711 -2.5852947 -3.210397 -3.3542891 -2.9773219 -2.6658039 -2.49544 -2.1760764 -2.1113839 -2.4536798 -2.8993487 -3.0914207 -3.2172318 -3.3978477][-2.445066 -2.453229 -2.6661346 -3.2098141 -3.3399582 -2.9704537 -2.5333467 -2.1958034 -1.891825 -1.9676647 -2.5160437 -3.087081 -3.3080249 -3.3450902 -3.4191816][-2.8794775 -2.8933206 -3.0430088 -3.3840132 -3.3227513 -2.8420453 -2.2205555 -1.7264719 -1.5024292 -1.7466769 -2.4425898 -3.0565214 -3.2719426 -3.3038812 -3.3589849][-3.1064768 -3.0646679 -3.1308551 -3.2734003 -3.020937 -2.4248598 -1.6452296 -1.0195346 -0.85586834 -1.2412436 -2.04705 -2.6931586 -2.982105 -3.1476583 -3.2881703][-3.1383166 -2.9534068 -2.8595285 -2.82898 -2.4593034 -1.7871058 -0.96205091 -0.31431913 -0.11052561 -0.44168925 -1.1867354 -1.8242898 -2.2890418 -2.6978528 -2.9930959][-2.9152436 -2.6545448 -2.4182534 -2.2967381 -1.9799345 -1.3668401 -0.7098732 -0.23314953 0.040321827 -0.039171696 -0.50878 -1.0300477 -1.6034408 -2.1740534 -2.5533371][-2.7182271 -2.4911094 -2.1985009 -2.089817 -1.963928 -1.5097644 -1.0964284 -0.85090518 -0.54066968 -0.37958908 -0.58746195 -0.97650361 -1.5527747 -2.0973163 -2.4317887][-2.9153676 -2.7472801 -2.4504833 -2.3648314 -2.35883 -2.0440705 -1.8316326 -1.778821 -1.4832108 -1.2186437 -1.3141682 -1.6578074 -2.1929185 -2.6468067 -2.9217219][-3.3111382 -3.1835706 -2.9205403 -2.8137596 -2.8222909 -2.620338 -2.5619476 -2.6039581 -2.3902409 -2.1935422 -2.3001697 -2.6038871 -3.0607014 -3.4182355 -3.6567113][-3.6082661 -3.5320766 -3.3297567 -3.183979 -3.1422994 -3.0312848 -3.0458622 -3.1180825 -3.0169086 -2.939352 -3.0351622 -3.2348742 -3.5783923 -3.8602872 -4.0764132][-3.7359529 -3.6982455 -3.5664663 -3.4142437 -3.3433752 -3.2921536 -3.3045402 -3.3593776 -3.3604999 -3.3527098 -3.3776593 -3.4504061 -3.6465573 -3.8387 -4.0089231][-3.7100983 -3.6822546 -3.615797 -3.5389061 -3.501102 -3.4880345 -3.490149 -3.5178287 -3.5396373 -3.5369849 -3.5027885 -3.4923391 -3.5639451 -3.6654596 -3.7698047]]...]
INFO - root - 2017-12-07 07:10:14.194351: step 17910, loss = 0.81, batch loss = 0.74 (6.8 examples/sec; 1.172 sec/batch; 102h:24m:03s remains)
INFO - root - 2017-12-07 07:10:25.895470: step 17920, loss = 0.88, batch loss = 0.81 (6.9 examples/sec; 1.156 sec/batch; 101h:02m:31s remains)
INFO - root - 2017-12-07 07:10:37.485454: step 17930, loss = 0.75, batch loss = 0.68 (7.1 examples/sec; 1.125 sec/batch; 98h:19m:35s remains)
INFO - root - 2017-12-07 07:10:48.748873: step 17940, loss = 0.79, batch loss = 0.72 (7.0 examples/sec; 1.144 sec/batch; 99h:58m:35s remains)
INFO - root - 2017-12-07 07:11:00.486224: step 17950, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 1.141 sec/batch; 99h:41m:21s remains)
INFO - root - 2017-12-07 07:11:12.165335: step 17960, loss = 0.61, batch loss = 0.53 (6.7 examples/sec; 1.199 sec/batch; 104h:43m:48s remains)
INFO - root - 2017-12-07 07:11:23.645555: step 17970, loss = 0.76, batch loss = 0.69 (7.2 examples/sec; 1.114 sec/batch; 97h:18m:03s remains)
INFO - root - 2017-12-07 07:11:35.375847: step 17980, loss = 0.67, batch loss = 0.59 (6.5 examples/sec; 1.229 sec/batch; 107h:24m:18s remains)
INFO - root - 2017-12-07 07:11:47.016948: step 17990, loss = 0.79, batch loss = 0.72 (6.7 examples/sec; 1.193 sec/batch; 104h:12m:07s remains)
INFO - root - 2017-12-07 07:11:58.489389: step 18000, loss = 0.66, batch loss = 0.59 (7.4 examples/sec; 1.080 sec/batch; 94h:20m:28s remains)
2017-12-07 07:11:59.370311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0057912 -1.912781 -1.8308573 -1.8972123 -2.0876663 -2.2144659 -2.3162987 -2.4832761 -2.6649542 -2.6516585 -2.3501272 -1.9037943 -1.7823753 -2.3462019 -3.1482031][-1.3211744 -1.1452265 -1.0470383 -1.2295587 -1.558778 -1.7551188 -1.9420838 -2.1908052 -2.4369807 -2.4176049 -1.9861498 -1.303597 -1.0365241 -1.7700183 -2.9048195][-1.2876697 -0.96597552 -0.75004363 -0.92308211 -1.2452629 -1.3829181 -1.5371013 -1.7457893 -2.0380075 -2.1516292 -1.7580116 -0.8940444 -0.39172697 -1.0982316 -2.3981757][-1.6293714 -1.3947613 -1.1808343 -1.2817786 -1.4048262 -1.2177625 -1.0543637 -1.0500994 -1.3780129 -1.7607913 -1.5714097 -0.67451882 -0.0020332336 -0.568768 -1.8157291][-2.0538626 -1.9527841 -1.8277833 -1.869617 -1.78722 -1.2092888 -0.56148505 -0.16719437 -0.4628191 -1.1005983 -1.1642678 -0.4009285 0.25403833 -0.22967482 -1.3582144][-2.9933591 -2.929256 -2.7627559 -2.5676398 -2.17828 -1.200628 -0.0042572021 0.87078285 0.59524155 -0.30672359 -0.65383959 -0.12434053 0.38782024 -0.068490505 -1.0931776][-3.5021424 -3.3607683 -3.1204348 -2.7213011 -2.0420511 -0.7096529 1.0632949 2.3955464 1.9657469 0.69617176 0.0047039986 0.20486736 0.55955362 0.17249775 -0.78435755][-3.2866163 -2.9902804 -2.6801274 -2.1810803 -1.2778645 0.32457113 2.510066 4.0268593 3.1960583 1.4889903 0.52194834 0.45289469 0.77101278 0.56824684 -0.28964186][-3.0039735 -2.7852092 -2.6668429 -2.1994369 -1.115597 0.71369219 3.0639167 4.4647484 3.3202047 1.4112949 0.34461498 0.15337992 0.55407333 0.62164974 -0.060801983][-2.5305071 -2.6748066 -3.0604982 -2.8140702 -1.7482464 0.041225433 2.1576004 3.3319669 2.3871188 0.71909237 -0.23821592 -0.44683671 0.022185802 0.2933197 -0.2479043][-1.6187081 -2.2556119 -3.231935 -3.2646575 -2.2372665 -0.56813979 1.1808767 2.0906444 1.3786044 -0.020376205 -0.93577433 -1.1385114 -0.64540696 -0.27287769 -0.70415831][-0.86607409 -1.78865 -3.1157742 -3.3651681 -2.3744965 -0.81951785 0.64265156 1.2715292 0.62255859 -0.6536653 -1.5867376 -1.804806 -1.3718917 -1.0150287 -1.3230875][-0.79668522 -1.7169924 -2.8957329 -3.0343027 -2.029484 -0.72014093 0.29094362 0.4923768 -0.16811228 -1.2782149 -2.1015744 -2.3015413 -2.0299432 -1.8201911 -2.0224473][-1.0859993 -1.8804142 -2.5404787 -2.2342639 -1.1768353 -0.25268745 0.10609674 -0.19498062 -0.826952 -1.6980631 -2.3248549 -2.5341635 -2.5547333 -2.5827093 -2.6697636][-1.696444 -2.293776 -2.4799428 -1.8768294 -0.8770597 -0.22900534 -0.29547977 -0.79240084 -1.2199159 -1.8529298 -2.3153398 -2.5366139 -2.8606129 -3.0712109 -3.0341244]]...]
INFO - root - 2017-12-07 07:12:11.142277: step 18010, loss = 0.82, batch loss = 0.75 (6.8 examples/sec; 1.182 sec/batch; 103h:17m:43s remains)
INFO - root - 2017-12-07 07:12:22.845540: step 18020, loss = 0.85, batch loss = 0.77 (6.7 examples/sec; 1.186 sec/batch; 103h:37m:02s remains)
INFO - root - 2017-12-07 07:12:34.478094: step 18030, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 1.177 sec/batch; 102h:46m:20s remains)
INFO - root - 2017-12-07 07:12:46.076494: step 18040, loss = 0.99, batch loss = 0.92 (7.3 examples/sec; 1.103 sec/batch; 96h:22m:18s remains)
INFO - root - 2017-12-07 07:12:57.718934: step 18050, loss = 0.71, batch loss = 0.63 (7.3 examples/sec; 1.095 sec/batch; 95h:37m:47s remains)
INFO - root - 2017-12-07 07:13:08.901420: step 18060, loss = 0.85, batch loss = 0.78 (6.9 examples/sec; 1.154 sec/batch; 100h:45m:07s remains)
INFO - root - 2017-12-07 07:13:20.506460: step 18070, loss = 0.84, batch loss = 0.77 (6.9 examples/sec; 1.162 sec/batch; 101h:28m:07s remains)
INFO - root - 2017-12-07 07:13:32.121196: step 18080, loss = 0.74, batch loss = 0.67 (6.6 examples/sec; 1.221 sec/batch; 106h:37m:35s remains)
INFO - root - 2017-12-07 07:13:43.810460: step 18090, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 1.189 sec/batch; 103h:50m:32s remains)
INFO - root - 2017-12-07 07:13:55.508459: step 18100, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.129 sec/batch; 98h:35m:04s remains)
2017-12-07 07:13:56.381495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.45209 -3.4412858 -3.383719 -3.3084576 -3.2370973 -3.1772461 -3.0952532 -3.0592887 -3.0713975 -3.0821843 -3.1061683 -3.1757624 -3.2675023 -3.3235586 -3.2764812][-3.4795241 -3.50492 -3.4753611 -3.3990059 -3.2862465 -3.2249889 -3.2184286 -3.3264976 -3.51418 -3.6200013 -3.6100769 -3.5934916 -3.5933409 -3.52019 -3.3197234][-3.4871976 -3.5250192 -3.5930984 -3.6907854 -3.7457981 -3.8137794 -3.8746357 -3.9554248 -4.0479817 -3.9805617 -3.7850728 -3.7255461 -3.7951241 -3.7321734 -3.4627237][-3.3711476 -3.4040856 -3.5788407 -3.8738682 -4.1030359 -4.2130251 -4.18366 -4.0174541 -3.7552156 -3.3377471 -2.9596519 -3.0197766 -3.4130559 -3.5873511 -3.4267521][-3.1606371 -3.1325178 -3.2065079 -3.3443389 -3.3715191 -3.3261256 -3.2669396 -3.1351511 -2.8484898 -2.4133842 -2.06648 -2.2346263 -2.8023124 -3.1351342 -3.1339912][-2.870491 -2.6574082 -2.3826888 -2.0093911 -1.5105641 -1.2455959 -1.4334464 -1.8874342 -2.1880164 -2.2288475 -2.1016061 -2.1705184 -2.4887733 -2.6348376 -2.6212025][-2.5700517 -2.2729731 -1.8598449 -1.2555842 -0.48818278 -0.096565723 -0.3748641 -1.0510356 -1.622659 -2.0204914 -2.2013435 -2.2940276 -2.3677742 -2.2677586 -2.1627116][-2.50322 -2.3045125 -2.1001761 -1.7783906 -1.2903664 -0.99417853 -1.051137 -1.2521038 -1.3749881 -1.6359303 -2.0193284 -2.3660736 -2.5029016 -2.3543806 -2.1762948][-2.506773 -2.3388855 -2.2469265 -2.1363378 -1.9371829 -1.8488111 -1.8231502 -1.6659632 -1.3981256 -1.4139285 -1.8273065 -2.3847816 -2.7261236 -2.7158356 -2.5428712][-2.5006428 -2.2736135 -2.1534929 -2.0876715 -2.0601375 -2.2209382 -2.3876271 -2.2807424 -1.9473557 -1.7756691 -2.003185 -2.4977398 -2.9207273 -3.0114741 -2.8574209][-2.797024 -2.5663886 -2.4081097 -2.326236 -2.3245966 -2.5399489 -2.7832146 -2.8222575 -2.6193852 -2.437917 -2.4789271 -2.7401919 -3.0346718 -3.0823357 -2.9156332][-3.3017147 -3.2469864 -3.152348 -3.0286179 -2.9071364 -2.9607911 -3.0778255 -3.1300216 -3.0537808 -3.0034344 -3.0357509 -3.1474662 -3.2263613 -3.0940521 -2.8443112][-3.3243232 -3.3800569 -3.3372006 -3.1739595 -2.9671597 -2.9093447 -2.9444046 -2.9762566 -2.9305921 -2.929389 -2.9857969 -3.0836339 -3.1131544 -2.9655111 -2.765012][-3.028827 -3.0563073 -3.0079021 -2.842216 -2.6648719 -2.638958 -2.6970773 -2.7268264 -2.6756039 -2.6591067 -2.7070332 -2.8071673 -2.8758214 -2.8514128 -2.8132007][-3.0051234 -2.9874287 -2.9592886 -2.8689613 -2.7859235 -2.8117208 -2.8823853 -2.8985796 -2.8369765 -2.8010745 -2.8210974 -2.8888049 -2.9426322 -2.9600034 -2.9924054]]...]
INFO - root - 2017-12-07 07:14:08.037364: step 18110, loss = 0.70, batch loss = 0.63 (6.7 examples/sec; 1.191 sec/batch; 104h:01m:26s remains)
INFO - root - 2017-12-07 07:14:19.580354: step 18120, loss = 1.04, batch loss = 0.96 (6.5 examples/sec; 1.233 sec/batch; 107h:38m:37s remains)
INFO - root - 2017-12-07 07:14:31.326773: step 18130, loss = 0.59, batch loss = 0.52 (6.8 examples/sec; 1.175 sec/batch; 102h:38m:40s remains)
INFO - root - 2017-12-07 07:14:43.040585: step 18140, loss = 0.82, batch loss = 0.74 (6.8 examples/sec; 1.180 sec/batch; 103h:02m:22s remains)
INFO - root - 2017-12-07 07:14:54.585186: step 18150, loss = 0.56, batch loss = 0.48 (7.0 examples/sec; 1.139 sec/batch; 99h:29m:27s remains)
INFO - root - 2017-12-07 07:15:06.212523: step 18160, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.130 sec/batch; 98h:41m:59s remains)
INFO - root - 2017-12-07 07:15:17.971096: step 18170, loss = 0.83, batch loss = 0.75 (6.7 examples/sec; 1.187 sec/batch; 103h:40m:33s remains)
INFO - root - 2017-12-07 07:15:29.653585: step 18180, loss = 0.65, batch loss = 0.58 (6.7 examples/sec; 1.202 sec/batch; 104h:54m:29s remains)
INFO - root - 2017-12-07 07:15:41.358177: step 18190, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.121 sec/batch; 97h:51m:28s remains)
INFO - root - 2017-12-07 07:15:52.958190: step 18200, loss = 0.81, batch loss = 0.74 (7.0 examples/sec; 1.138 sec/batch; 99h:21m:45s remains)
2017-12-07 07:15:53.910518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1303277 -3.7682784 -4.0887361 -3.8827415 -3.1138875 -2.6905313 -2.6344624 -2.6197681 -2.5404844 -2.2433805 -1.7989256 -1.5194421 -2.0109062 -2.5617352 -2.4944017][-2.8565154 -3.6125574 -4.0488944 -3.7539039 -3.0252781 -2.7472725 -2.5901902 -2.4730229 -2.2852845 -1.9166722 -1.6242502 -1.5585754 -2.1189995 -2.6131039 -2.4235828][-2.8441024 -3.6124651 -3.9558258 -3.5192535 -2.9761646 -3.0114391 -2.9043493 -2.7079175 -2.3434455 -1.75492 -1.5168087 -1.6175854 -2.1931834 -2.6125085 -2.3250492][-2.8432131 -3.4417734 -3.4839025 -2.9493039 -2.7560194 -3.2204733 -3.2510843 -2.9995418 -2.47766 -1.6624274 -1.4233317 -1.5735688 -2.1050861 -2.5108376 -2.2461271][-2.7409372 -3.107913 -2.8760548 -2.3792825 -2.4898875 -3.1673079 -3.1844506 -2.8560002 -2.2959387 -1.4279714 -1.2986476 -1.4862254 -1.9167204 -2.3810251 -2.2102244][-2.5493202 -2.6991241 -2.3639605 -2.0247984 -2.2704017 -2.829144 -2.6065385 -2.1521916 -1.6351483 -0.89814138 -1.0369642 -1.3135185 -1.6829247 -2.2418485 -2.1464293][-2.399893 -2.3495705 -2.019582 -1.8304553 -1.991745 -2.2055261 -1.7202914 -1.2618797 -0.86598611 -0.35725546 -0.819865 -1.1682169 -1.4864593 -2.1013875 -1.9939375][-2.3375115 -2.105798 -1.7927492 -1.6757741 -1.6475437 -1.5468626 -0.96921229 -0.6513865 -0.41059923 -0.22168207 -0.98205662 -1.3303318 -1.5385818 -2.1049132 -1.8937645][-2.2557309 -1.873086 -1.5890236 -1.5641472 -1.4803967 -1.2745001 -0.80613804 -0.658715 -0.49623895 -0.56791925 -1.4676125 -1.7594562 -1.9245021 -2.4015405 -2.0257673][-2.1384606 -1.7055726 -1.5221086 -1.6594865 -1.6951418 -1.5171688 -1.1950207 -1.0637493 -0.83571339 -1.0262668 -1.8885534 -2.0913339 -2.2859936 -2.6504126 -2.1151197][-2.1615872 -1.8425002 -1.8478146 -2.1570418 -2.2991588 -2.0589402 -1.6925597 -1.4266546 -1.0699227 -1.272438 -1.9901564 -2.1471987 -2.427758 -2.6972437 -2.1447294][-2.4022551 -2.2566197 -2.4239438 -2.834322 -2.9843922 -2.5993962 -2.0806859 -1.6659646 -1.2327762 -1.4070697 -2.0043373 -2.2265911 -2.5811605 -2.7636886 -2.2787592][-2.6016316 -2.6190817 -2.8784227 -3.3128433 -3.4269192 -2.9609656 -2.4120393 -2.0123339 -1.675741 -1.8646495 -2.4262788 -2.8024993 -3.1779759 -3.2504594 -2.8368626][-2.7212324 -2.8474166 -3.03124 -3.3395419 -3.3611956 -2.9058094 -2.5093107 -2.327682 -2.2550409 -2.512517 -3.0696349 -3.5410128 -3.8541956 -3.8241765 -3.5023985][-2.8467011 -2.9667497 -2.9687984 -3.0816472 -2.9835734 -2.5693402 -2.350949 -2.3805945 -2.4916687 -2.7493315 -3.235625 -3.6810911 -3.8753119 -3.8003342 -3.5962477]]...]
INFO - root - 2017-12-07 07:16:05.497702: step 18210, loss = 0.78, batch loss = 0.70 (6.6 examples/sec; 1.204 sec/batch; 105h:07m:20s remains)
INFO - root - 2017-12-07 07:16:17.143976: step 18220, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.161 sec/batch; 101h:20m:02s remains)
INFO - root - 2017-12-07 07:16:28.853654: step 18230, loss = 0.69, batch loss = 0.62 (6.8 examples/sec; 1.179 sec/batch; 102h:54m:48s remains)
INFO - root - 2017-12-07 07:16:38.188633: step 18240, loss = 0.81, batch loss = 0.73 (8.9 examples/sec; 0.900 sec/batch; 78h:32m:55s remains)
INFO - root - 2017-12-07 07:16:46.225291: step 18250, loss = 0.80, batch loss = 0.73 (9.2 examples/sec; 0.872 sec/batch; 76h:04m:33s remains)
INFO - root - 2017-12-07 07:16:54.647959: step 18260, loss = 0.79, batch loss = 0.72 (9.9 examples/sec; 0.809 sec/batch; 70h:36m:40s remains)
INFO - root - 2017-12-07 07:17:02.963731: step 18270, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 0.832 sec/batch; 72h:36m:13s remains)
INFO - root - 2017-12-07 07:17:11.301478: step 18280, loss = 0.53, batch loss = 0.46 (9.6 examples/sec; 0.832 sec/batch; 72h:39m:42s remains)
INFO - root - 2017-12-07 07:17:19.492099: step 18290, loss = 0.79, batch loss = 0.71 (10.2 examples/sec; 0.783 sec/batch; 68h:22m:50s remains)
INFO - root - 2017-12-07 07:17:27.771507: step 18300, loss = 0.99, batch loss = 0.92 (9.7 examples/sec; 0.827 sec/batch; 72h:08m:40s remains)
2017-12-07 07:17:28.405102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19868851 -0.38343096 -0.57954073 -0.92140555 -0.812258 -0.22910786 0.37729597 0.84580469 1.1659861 1.3200431 1.3878183 1.4497237 1.3067598 0.77851534 0.029521465][-0.15390396 -0.3711834 -0.52832985 -0.82657123 -0.65549874 -0.012781143 0.59662676 1.0072236 1.254077 1.3329644 1.34167 1.3455849 1.1946807 0.72929 0.049505234][-0.45586014 -0.64952183 -0.7093761 -0.89875078 -0.64049745 0.074409008 0.72723627 1.1158347 1.3008885 1.3177009 1.2716861 1.2174935 1.0573802 0.62833977 -0.053953171][-0.81687284 -1.0220273 -1.0454476 -1.17332 -0.87495923 -0.10807657 0.61299562 1.0139422 1.1366777 1.0954542 1.0152178 0.92141771 0.75856876 0.36132336 -0.35263157][-1.1327496 -1.4239228 -1.5059111 -1.6227303 -1.2918172 -0.42620659 0.44741249 0.91867685 0.94472313 0.774487 0.5897665 0.42605639 0.25252581 -0.10604715 -0.8031826][-1.4287341 -1.8470755 -2.0422912 -2.1380713 -1.6990366 -0.65441871 0.44698095 1.0409708 0.96779394 0.61872435 0.24319315 -0.073523521 -0.31093836 -0.65164471 -1.2724123][-1.5724041 -2.1143415 -2.440259 -2.4709995 -1.8422146 -0.58829713 0.75399876 1.4750724 1.2662764 0.68228292 0.048460007 -0.48263931 -0.8456409 -1.1952796 -1.698271][-1.7379763 -2.3696351 -2.8240392 -2.7653897 -1.9113538 -0.46352124 1.0534363 1.8650098 1.5905156 0.84819078 0.021589756 -0.71037269 -1.2528749 -1.6899045 -2.1551902][-2.2119851 -2.8637428 -3.4059567 -3.2971883 -2.3259046 -0.85448122 0.60499477 1.3881359 1.230823 0.59657431 -0.14978027 -0.87798 -1.5088437 -2.0335305 -2.520021][-2.7934446 -3.4475851 -4.0621023 -3.9932392 -3.101907 -1.8166637 -0.59405088 0.11581326 0.1859169 -0.15658045 -0.65455818 -1.2465954 -1.8535657 -2.3717849 -2.8338614][-3.4250422 -4.0374746 -4.6395717 -4.6633949 -4.0244846 -3.0896025 -2.2380393 -1.6710134 -1.4388361 -1.4892809 -1.6695566 -1.9878612 -2.4104438 -2.8192742 -3.21811][-3.4537272 -3.93326 -4.4177604 -4.5211782 -4.179297 -3.651782 -3.2342129 -2.9204793 -2.71826 -2.6574998 -2.6136758 -2.6411836 -2.8206644 -3.0923741 -3.4034681][-2.8509755 -3.1204863 -3.42589 -3.5434864 -3.4022791 -3.1817698 -3.100028 -3.0187235 -2.9346485 -2.9451113 -2.9012685 -2.8056374 -2.8726611 -3.1021218 -3.3296986][-2.5020447 -2.573087 -2.6786709 -2.732492 -2.6914787 -2.6576 -2.7577052 -2.8015313 -2.7997441 -2.8826661 -2.8535972 -2.6611936 -2.7018766 -2.9957294 -3.17684][-2.6474547 -2.6471949 -2.6598 -2.6791136 -2.6826315 -2.7129827 -2.8357484 -2.8926289 -2.8872919 -2.94007 -2.85822 -2.5772734 -2.6125107 -2.9413886 -3.0411129]]...]
INFO - root - 2017-12-07 07:17:36.752559: step 18310, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.821 sec/batch; 71h:36m:40s remains)
INFO - root - 2017-12-07 07:17:44.994966: step 18320, loss = 0.96, batch loss = 0.89 (9.7 examples/sec; 0.825 sec/batch; 71h:59m:29s remains)
INFO - root - 2017-12-07 07:17:53.199125: step 18330, loss = 0.65, batch loss = 0.58 (9.9 examples/sec; 0.806 sec/batch; 70h:19m:15s remains)
INFO - root - 2017-12-07 07:18:01.549256: step 18340, loss = 0.83, batch loss = 0.76 (9.5 examples/sec; 0.846 sec/batch; 73h:47m:09s remains)
INFO - root - 2017-12-07 07:18:09.817316: step 18350, loss = 0.80, batch loss = 0.72 (9.6 examples/sec; 0.831 sec/batch; 72h:28m:35s remains)
INFO - root - 2017-12-07 07:18:18.136190: step 18360, loss = 0.66, batch loss = 0.58 (9.7 examples/sec; 0.821 sec/batch; 71h:39m:11s remains)
INFO - root - 2017-12-07 07:18:26.569864: step 18370, loss = 0.79, batch loss = 0.72 (9.6 examples/sec; 0.830 sec/batch; 72h:24m:34s remains)
INFO - root - 2017-12-07 07:18:34.828937: step 18380, loss = 0.77, batch loss = 0.69 (10.0 examples/sec; 0.798 sec/batch; 69h:38m:26s remains)
INFO - root - 2017-12-07 07:18:43.151429: step 18390, loss = 0.95, batch loss = 0.88 (9.6 examples/sec; 0.835 sec/batch; 72h:50m:51s remains)
INFO - root - 2017-12-07 07:18:51.470056: step 18400, loss = 0.89, batch loss = 0.82 (8.8 examples/sec; 0.905 sec/batch; 78h:59m:00s remains)
2017-12-07 07:18:52.095369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8972225 -1.8903992 -1.9854217 -1.9305301 -1.776454 -1.6319613 -1.5135906 -1.5368886 -1.7221677 -1.9345872 -2.0703895 -2.1826987 -2.3779688 -2.546953 -2.4928875][-1.9372287 -1.9036474 -1.8461082 -1.6931572 -1.572093 -1.4963164 -1.4857693 -1.6063306 -1.8773718 -2.09441 -2.1507006 -2.1515658 -2.2922308 -2.4649036 -2.4344954][-1.8032815 -1.7721238 -1.7402673 -1.7345107 -1.7912889 -1.7719941 -1.7419648 -1.8043454 -2.011297 -2.1828904 -2.1812062 -2.1128206 -2.1923976 -2.3307128 -2.3197205][-1.7438426 -1.7541447 -1.846163 -2.0618584 -2.2908607 -2.3256094 -2.2897668 -2.2727947 -2.2785342 -2.193882 -1.9877663 -1.7906868 -1.8078673 -1.9505603 -2.0500753][-1.525497 -1.6487021 -1.9337976 -2.3480957 -2.6829343 -2.7759676 -2.7654576 -2.6794953 -2.4765294 -2.1144471 -1.6938825 -1.3909295 -1.394248 -1.6405613 -1.8859723][-1.4123156 -1.5903683 -1.958292 -2.3667378 -2.6572223 -2.8301694 -2.9224958 -2.8236682 -2.4896135 -1.9748878 -1.5428874 -1.3386755 -1.451616 -1.7650344 -2.0029874][-1.6578796 -1.7777565 -2.0297327 -2.1908531 -2.2441642 -2.4299991 -2.6769979 -2.6398034 -2.3008926 -1.8009593 -1.4459827 -1.3783922 -1.5998731 -1.9394102 -2.0914369][-1.5886021 -1.5534151 -1.6470201 -1.6760468 -1.6404095 -1.8343475 -2.1230361 -2.0964694 -1.8214231 -1.4641891 -1.1629405 -1.0573635 -1.2401657 -1.6056781 -1.8331702][-1.1244762 -1.0145814 -1.0792859 -1.1640027 -1.2128637 -1.4051459 -1.6054802 -1.532028 -1.3842022 -1.2786062 -1.0369771 -0.79471612 -0.87682676 -1.2754707 -1.6531258][-1.0235527 -0.97757292 -1.0699432 -1.1532786 -1.17009 -1.2706413 -1.4394155 -1.488445 -1.5566313 -1.6421826 -1.4142234 -1.1092234 -1.17436 -1.5938823 -1.9857688][-1.6386278 -1.591785 -1.6128075 -1.5963323 -1.5171204 -1.5351079 -1.7086253 -1.9175682 -2.165555 -2.3273661 -2.065937 -1.7274313 -1.7775171 -2.1571221 -2.4462039][-2.1959593 -2.1170375 -2.0949626 -2.083467 -2.0546741 -2.0967717 -2.2221723 -2.3748655 -2.5547347 -2.6395128 -2.3708491 -2.07583 -2.1682723 -2.53431 -2.7257328][-2.4704809 -2.373908 -2.3502362 -2.3757627 -2.4124 -2.4881334 -2.6004264 -2.721736 -2.8626773 -2.9077814 -2.6695242 -2.4192193 -2.5370166 -2.8671677 -2.9556458][-2.7488809 -2.7013564 -2.7040586 -2.7546337 -2.8246436 -2.9128389 -3.0300021 -3.1537614 -3.3161445 -3.390924 -3.1799436 -2.9112158 -2.9557273 -3.1858177 -3.1727011][-3.0476508 -3.064784 -3.1109266 -3.1804738 -3.2526133 -3.3238029 -3.4067445 -3.4844074 -3.6144793 -3.6735036 -3.45498 -3.1674218 -3.1677914 -3.3510802 -3.310811]]...]
INFO - root - 2017-12-07 07:19:00.368614: step 18410, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.817 sec/batch; 71h:15m:31s remains)
INFO - root - 2017-12-07 07:19:08.613932: step 18420, loss = 0.77, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 73h:12m:50s remains)
INFO - root - 2017-12-07 07:19:16.923225: step 18430, loss = 0.89, batch loss = 0.81 (9.5 examples/sec; 0.842 sec/batch; 73h:24m:51s remains)
INFO - root - 2017-12-07 07:19:25.282200: step 18440, loss = 0.72, batch loss = 0.65 (9.0 examples/sec; 0.889 sec/batch; 77h:33m:03s remains)
INFO - root - 2017-12-07 07:19:33.592108: step 18450, loss = 0.71, batch loss = 0.63 (10.0 examples/sec; 0.803 sec/batch; 70h:04m:39s remains)
INFO - root - 2017-12-07 07:19:41.833592: step 18460, loss = 0.70, batch loss = 0.63 (9.8 examples/sec; 0.814 sec/batch; 70h:58m:11s remains)
INFO - root - 2017-12-07 07:19:50.128829: step 18470, loss = 0.76, batch loss = 0.68 (9.7 examples/sec; 0.824 sec/batch; 71h:53m:35s remains)
INFO - root - 2017-12-07 07:19:58.311403: step 18480, loss = 0.81, batch loss = 0.74 (10.5 examples/sec; 0.760 sec/batch; 66h:16m:09s remains)
INFO - root - 2017-12-07 07:20:06.782944: step 18490, loss = 0.80, batch loss = 0.73 (9.2 examples/sec; 0.874 sec/batch; 76h:11m:32s remains)
INFO - root - 2017-12-07 07:20:15.108565: step 18500, loss = 0.55, batch loss = 0.47 (9.9 examples/sec; 0.811 sec/batch; 70h:43m:48s remains)
2017-12-07 07:20:15.759836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6881323 -2.7171478 -2.7472115 -2.7724319 -2.7835977 -2.7796528 -2.7639818 -2.7427011 -2.720602 -2.7013502 -2.6826572 -2.6735361 -2.6639628 -2.7043424 -2.855448][-2.7454777 -2.7979379 -2.8438182 -2.8775871 -2.8890505 -2.8765235 -2.8483982 -2.8138289 -2.7793682 -2.7480001 -2.7129602 -2.687573 -2.6654873 -2.6961262 -2.8423052][-2.8110042 -2.884655 -2.941138 -2.9770126 -2.9822173 -2.9566128 -2.9189053 -2.8786089 -2.8413191 -2.8055739 -2.7540967 -2.7083278 -2.6698675 -2.685534 -2.8226271][-2.8592012 -2.9547195 -3.0299146 -3.0751815 -3.0757911 -3.0383 -2.9982259 -2.9564579 -2.9158759 -2.8754821 -2.8012285 -2.7282867 -2.6688561 -2.6688292 -2.7983651][-2.8936558 -3.0143123 -3.1137161 -3.1703649 -3.159904 -3.0999732 -3.0474398 -2.9972563 -2.9525094 -2.9126792 -2.8245215 -2.7307687 -2.6568875 -2.6465037 -2.773222][-2.9236333 -3.0718527 -3.1944766 -3.258121 -3.2318387 -3.1372638 -3.0544207 -2.9801228 -2.9289398 -2.8970413 -2.807894 -2.708005 -2.6320288 -2.6214252 -2.7511268][-2.94761 -3.1095035 -3.2406726 -3.3033257 -3.267416 -3.1463497 -3.0274391 -2.9237931 -2.8686783 -2.8491874 -2.7663949 -2.6698203 -2.6039472 -2.6042304 -2.741251][-2.9633431 -3.1165495 -3.2375619 -3.2932436 -3.2623014 -3.1487126 -3.0182962 -2.8955867 -2.8355806 -2.8213782 -2.7438135 -2.6493871 -2.5919557 -2.6060154 -2.7531352][-2.9402099 -3.0747614 -3.1804821 -3.2321565 -3.227787 -3.1630588 -3.0619106 -2.9386804 -2.86551 -2.8377528 -2.7560167 -2.6566334 -2.6003797 -2.6265974 -2.7872107][-2.8581789 -2.9737439 -3.066292 -3.1177862 -3.1433218 -3.1358447 -3.07869 -2.9680176 -2.8814909 -2.8343444 -2.7488811 -2.6488883 -2.5963054 -2.6385438 -2.8189263][-2.7522523 -2.8451741 -2.9263942 -2.9783583 -3.0231953 -3.0553832 -3.0341077 -2.9401534 -2.8444142 -2.778429 -2.6910062 -2.6007509 -2.5615172 -2.6303186 -2.8422561][-2.6775832 -2.7416065 -2.8063147 -2.8526392 -2.8998752 -2.9456375 -2.9419045 -2.8640141 -2.7673154 -2.6886539 -2.6052094 -2.5378113 -2.5253534 -2.6292086 -2.8798118][-2.6581903 -2.6864767 -2.7203915 -2.7450695 -2.7724428 -2.8052993 -2.8030276 -2.7410557 -2.6579323 -2.5835109 -2.5147398 -2.4805231 -2.4978127 -2.6335459 -2.9172425][-2.6528177 -2.6475937 -2.6500998 -2.6487784 -2.6481931 -2.655098 -2.6428103 -2.5940466 -2.5311041 -2.4707065 -2.4187338 -2.416862 -2.4636774 -2.6206932 -2.9213254][-2.6687942 -2.6324701 -2.6069126 -2.5805159 -2.548872 -2.5224051 -2.49115 -2.4479804 -2.4018486 -2.3574519 -2.3246849 -2.3556697 -2.4379625 -2.6122527 -2.9210427]]...]
INFO - root - 2017-12-07 07:20:24.061026: step 18510, loss = 0.67, batch loss = 0.60 (9.5 examples/sec; 0.841 sec/batch; 73h:20m:40s remains)
INFO - root - 2017-12-07 07:20:32.345699: step 18520, loss = 0.71, batch loss = 0.64 (9.2 examples/sec; 0.872 sec/batch; 76h:03m:30s remains)
INFO - root - 2017-12-07 07:20:40.541295: step 18530, loss = 0.83, batch loss = 0.76 (9.4 examples/sec; 0.847 sec/batch; 73h:52m:59s remains)
INFO - root - 2017-12-07 07:20:48.898711: step 18540, loss = 0.91, batch loss = 0.84 (9.9 examples/sec; 0.810 sec/batch; 70h:38m:31s remains)
INFO - root - 2017-12-07 07:20:57.247421: step 18550, loss = 0.63, batch loss = 0.56 (9.5 examples/sec; 0.844 sec/batch; 73h:34m:52s remains)
INFO - root - 2017-12-07 07:21:05.177219: step 18560, loss = 0.66, batch loss = 0.59 (14.7 examples/sec; 0.543 sec/batch; 47h:18m:33s remains)
INFO - root - 2017-12-07 07:21:13.522078: step 18570, loss = 0.64, batch loss = 0.57 (9.5 examples/sec; 0.839 sec/batch; 73h:10m:04s remains)
INFO - root - 2017-12-07 07:21:21.844096: step 18580, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.813 sec/batch; 70h:56m:03s remains)
INFO - root - 2017-12-07 07:21:30.176150: step 18590, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.812 sec/batch; 70h:48m:42s remains)
INFO - root - 2017-12-07 07:21:38.537656: step 18600, loss = 0.70, batch loss = 0.63 (9.3 examples/sec; 0.858 sec/batch; 74h:47m:22s remains)
2017-12-07 07:21:39.278150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5434859 -3.5315008 -3.3741288 -3.1339531 -2.891643 -2.7887397 -2.91996 -2.9516516 -3.0008194 -3.2923694 -3.5297742 -3.6262269 -3.7112267 -3.7828906 -3.8743398][-3.4077358 -3.3878675 -3.1130919 -2.652179 -2.2203016 -2.0479529 -2.2873659 -2.4173217 -2.4838815 -2.8439128 -3.1481028 -3.2220311 -3.258615 -3.3417778 -3.4856207][-3.1638498 -3.1251931 -2.7318563 -2.0921872 -1.5377803 -1.3307228 -1.6797175 -1.8921115 -1.9141951 -2.2606974 -2.5665493 -2.6061716 -2.6389611 -2.7934105 -3.0104504][-2.8989515 -2.8342905 -2.3661878 -1.6489172 -1.0652719 -0.84396267 -1.2584195 -1.4964595 -1.3935392 -1.6521502 -1.9114318 -1.9176414 -2.0131068 -2.3082352 -2.6216764][-2.9299321 -2.7957313 -2.2636056 -1.5312376 -1.0039053 -0.81198907 -1.2592785 -1.480443 -1.2112722 -1.3820744 -1.6414104 -1.7094855 -1.9643509 -2.4258747 -2.7842455][-3.0826535 -2.8744054 -2.3266182 -1.6364911 -1.191066 -1.0232508 -1.4316428 -1.5841095 -1.1647565 -1.2669282 -1.5792301 -1.7871082 -2.2759886 -2.9199753 -3.3078525][-2.9637079 -2.7392681 -2.2531381 -1.6333215 -1.2416818 -1.032367 -1.2875481 -1.3297703 -0.85314775 -0.99268484 -1.4973361 -1.9648736 -2.7146482 -3.4502525 -3.7608726][-2.3910313 -2.2000721 -1.8464346 -1.3737547 -1.0867698 -0.836323 -0.84166908 -0.76577353 -0.38885498 -0.696512 -1.4355984 -2.1388588 -3.005764 -3.6802449 -3.8185711][-1.8724716 -1.7190659 -1.4657977 -1.0969288 -0.88194203 -0.5620594 -0.28116989 -0.073366642 0.1360693 -0.29358816 -1.1031609 -1.9029782 -2.7609031 -3.3272927 -3.3808804][-1.6290731 -1.5336275 -1.3649435 -1.0811307 -0.95924067 -0.64131236 -0.1923337 0.059264183 0.00445652 -0.51988149 -1.2111027 -1.9078097 -2.5694594 -2.9566402 -2.9674954][-1.608182 -1.6710429 -1.6860902 -1.5266774 -1.4946706 -1.223053 -0.74544764 -0.54741764 -0.79789662 -1.2964723 -1.7083602 -2.1659617 -2.5525293 -2.7485175 -2.7606373][-1.8305418 -2.0758531 -2.2524753 -2.1480157 -2.1290698 -1.8868389 -1.4618208 -1.3821819 -1.7754061 -2.2078824 -2.3774471 -2.6628718 -2.8672514 -2.9549031 -3.0287037][-2.152844 -2.5223162 -2.8133421 -2.7271032 -2.7235761 -2.5703645 -2.2939494 -2.3915346 -2.918509 -3.3393574 -3.4378893 -3.6670504 -3.7417374 -3.7253263 -3.7714055][-2.6381655 -3.0859642 -3.4402132 -3.3797152 -3.4075294 -3.3920498 -3.2948463 -3.5005009 -4.0570927 -4.408917 -4.4613476 -4.6176867 -4.5993285 -4.5138869 -4.5173874][-3.2219758 -3.6573265 -4.023243 -4.0285139 -4.0903444 -4.2017126 -4.2369547 -4.4438429 -4.9287391 -5.1789665 -5.1950669 -5.2978444 -5.2489877 -5.1807904 -5.2162533]]...]
INFO - root - 2017-12-07 07:21:47.753830: step 18610, loss = 0.84, batch loss = 0.77 (8.8 examples/sec; 0.906 sec/batch; 78h:59m:18s remains)
INFO - root - 2017-12-07 07:21:56.071790: step 18620, loss = 0.83, batch loss = 0.75 (9.9 examples/sec; 0.808 sec/batch; 70h:28m:47s remains)
INFO - root - 2017-12-07 07:22:04.300078: step 18630, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.818 sec/batch; 71h:18m:47s remains)
INFO - root - 2017-12-07 07:22:12.656846: step 18640, loss = 0.78, batch loss = 0.71 (9.6 examples/sec; 0.835 sec/batch; 72h:46m:46s remains)
INFO - root - 2017-12-07 07:22:20.966008: step 18650, loss = 0.90, batch loss = 0.83 (9.7 examples/sec; 0.824 sec/batch; 71h:50m:54s remains)
INFO - root - 2017-12-07 07:22:29.351290: step 18660, loss = 0.78, batch loss = 0.71 (9.2 examples/sec; 0.865 sec/batch; 75h:23m:51s remains)
INFO - root - 2017-12-07 07:22:37.710236: step 18670, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.819 sec/batch; 71h:21m:44s remains)
INFO - root - 2017-12-07 07:22:46.007075: step 18680, loss = 0.58, batch loss = 0.51 (10.1 examples/sec; 0.795 sec/batch; 69h:18m:35s remains)
INFO - root - 2017-12-07 07:22:54.351267: step 18690, loss = 0.64, batch loss = 0.57 (9.6 examples/sec; 0.832 sec/batch; 72h:31m:01s remains)
INFO - root - 2017-12-07 07:23:02.655811: step 18700, loss = 0.66, batch loss = 0.59 (9.9 examples/sec; 0.805 sec/batch; 70h:08m:14s remains)
2017-12-07 07:23:03.330309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8977079 -2.9959154 -2.7777805 -2.0125053 -1.0096788 -0.56984639 -1.0331478 -1.8657947 -2.4770117 -2.7945538 -2.8694997 -2.9430995 -3.0493894 -2.8293266 -2.4865861][-2.7427642 -2.9542425 -2.820509 -2.0424652 -0.91766095 -0.29549837 -0.517509 -1.1146789 -1.6940203 -2.2827029 -2.679589 -2.9009547 -3.040174 -2.8093233 -2.5198333][-2.9874477 -3.2323122 -3.0711627 -2.2023199 -0.99231744 -0.19902754 -0.066496849 -0.25494576 -0.64374161 -1.3971322 -2.145715 -2.6215074 -2.8757715 -2.7329104 -2.6001863][-3.510056 -3.7841077 -3.6151233 -2.7911255 -1.6166773 -0.58545256 0.062764645 0.39819956 0.24444342 -0.59199786 -1.5203376 -2.1388803 -2.4896483 -2.4735093 -2.4859219][-3.8011584 -4.0457063 -3.8309696 -3.180912 -2.3008595 -1.2300687 -0.13704205 0.69606733 0.68323326 -0.21848249 -1.170754 -1.7473645 -2.0992079 -2.1869829 -2.3011603][-3.7084665 -3.8846722 -3.538337 -2.9634163 -2.3272753 -1.3505461 -0.1585269 0.783319 0.76657009 -0.065321445 -0.8218708 -1.2330813 -1.5951438 -1.8979702 -2.1766937][-3.5173516 -3.6110954 -3.1792083 -2.6549652 -2.0706403 -1.0901463 0.071867943 0.81323528 0.6262598 -0.050481319 -0.44859481 -0.56481624 -0.90209985 -1.4553845 -1.9480064][-3.4046838 -3.4395275 -3.0619035 -2.7038333 -2.2247167 -1.2629671 -0.099034309 0.56955433 0.35886621 -0.10797787 -0.14342594 0.080672741 -0.16392565 -0.93831468 -1.6634991][-3.3075962 -3.3604169 -3.122385 -2.9233487 -2.5808067 -1.76488 -0.754555 -0.20694733 -0.38323736 -0.57711911 -0.23881245 0.25717878 0.11736298 -0.726043 -1.5349846][-3.2378743 -3.3008883 -3.1801238 -3.0751853 -2.7856688 -2.0825703 -1.2957411 -1.0134089 -1.329582 -1.5029428 -1.0749109 -0.50719428 -0.53882 -1.1611481 -1.7600591][-3.0842109 -3.1600132 -3.1047082 -3.0765235 -2.8771565 -2.3290396 -1.737047 -1.6483288 -2.0585985 -2.310689 -2.0094025 -1.5467942 -1.5268815 -1.8776052 -2.1982887][-2.9185257 -3.0172467 -2.9728312 -2.9016802 -2.7275267 -2.3334489 -1.9805524 -2.052613 -2.5029125 -2.8318779 -2.7116041 -2.4070354 -2.3580692 -2.4989085 -2.6344838][-3.0090647 -3.1573181 -3.0994463 -2.9580381 -2.7460315 -2.4057379 -2.1763585 -2.3228836 -2.7546005 -3.1029572 -3.1449325 -3.0169935 -2.95594 -2.975512 -3.0151944][-3.3919733 -3.4946992 -3.4381361 -3.3164124 -3.1681371 -2.9593716 -2.853827 -3.0100572 -3.3190923 -3.5567145 -3.6085496 -3.5263743 -3.4321289 -3.3949971 -3.4024272][-3.9017036 -3.9059193 -3.8452406 -3.7572742 -3.671186 -3.5824461 -3.5698471 -3.7004039 -3.8789327 -3.9927468 -3.9987028 -3.9197133 -3.8024471 -3.726445 -3.6923351]]...]
INFO - root - 2017-12-07 07:23:11.547798: step 18710, loss = 0.90, batch loss = 0.83 (9.7 examples/sec; 0.821 sec/batch; 71h:32m:35s remains)
INFO - root - 2017-12-07 07:23:19.885122: step 18720, loss = 0.64, batch loss = 0.57 (9.5 examples/sec; 0.839 sec/batch; 73h:09m:18s remains)
INFO - root - 2017-12-07 07:23:28.169566: step 18730, loss = 0.59, batch loss = 0.52 (9.3 examples/sec; 0.857 sec/batch; 74h:44m:05s remains)
INFO - root - 2017-12-07 07:23:36.599997: step 18740, loss = 0.67, batch loss = 0.60 (9.1 examples/sec; 0.883 sec/batch; 76h:59m:05s remains)
INFO - root - 2017-12-07 07:23:44.849445: step 18750, loss = 0.79, batch loss = 0.72 (9.9 examples/sec; 0.810 sec/batch; 70h:33m:02s remains)
INFO - root - 2017-12-07 07:23:53.189530: step 18760, loss = 0.76, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 74h:23m:35s remains)
INFO - root - 2017-12-07 07:24:01.487773: step 18770, loss = 0.84, batch loss = 0.77 (10.0 examples/sec; 0.799 sec/batch; 69h:36m:57s remains)
INFO - root - 2017-12-07 07:24:09.763507: step 18780, loss = 0.89, batch loss = 0.82 (9.5 examples/sec; 0.846 sec/batch; 73h:41m:23s remains)
INFO - root - 2017-12-07 07:24:18.059082: step 18790, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.824 sec/batch; 71h:47m:45s remains)
INFO - root - 2017-12-07 07:24:26.369075: step 18800, loss = 0.75, batch loss = 0.68 (10.1 examples/sec; 0.794 sec/batch; 69h:12m:31s remains)
2017-12-07 07:24:27.029523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6140802 -2.8944418 -3.0628657 -3.1501119 -3.1923518 -3.1810536 -3.125123 -2.909286 -2.5929623 -2.2031827 -1.7772546 -1.5607996 -1.4861171 -1.5584362 -1.7734926][-2.3333113 -2.6415896 -2.8094432 -2.8822603 -2.9280939 -2.9389241 -2.9466014 -2.7432837 -2.3863037 -1.9669149 -1.4950876 -1.2459421 -1.1479778 -1.1608577 -1.3664684][-1.899688 -2.2374861 -2.4236832 -2.4621551 -2.4584866 -2.4641526 -2.5313365 -2.400063 -2.1027231 -1.8061273 -1.4379308 -1.2149377 -1.0760133 -0.93657255 -1.0038962][-1.2283366 -1.5648737 -1.8319457 -1.9227433 -1.9127488 -1.9361742 -2.0412381 -1.9319084 -1.6262844 -1.432653 -1.2329307 -1.1466124 -1.1059842 -0.93211412 -0.87004566][-0.7766571 -0.99562287 -1.2697694 -1.4023941 -1.3888826 -1.4182665 -1.5297365 -1.3953438 -1.0598147 -0.93745041 -0.92306805 -1.0413132 -1.2160866 -1.1180384 -1.0179183][-0.88626361 -0.91453981 -1.0476902 -1.1029305 -1.0136032 -0.93657088 -0.93901086 -0.71472907 -0.37033606 -0.46850443 -0.78688884 -1.1762722 -1.5495074 -1.5622063 -1.5022757][-1.2396276 -1.14733 -1.1443942 -1.1080804 -0.893739 -0.555532 -0.2537384 0.20587873 0.56088448 0.090753555 -0.69524288 -1.3706412 -1.8814077 -2.0127349 -2.012017][-1.2870674 -1.1867552 -1.1685109 -1.1347818 -0.833657 -0.24611568 0.36082792 1.0184579 1.3803787 0.60367727 -0.45918274 -1.247031 -1.8061175 -2.0187132 -2.0576794][-1.062139 -1.0139594 -0.99790788 -0.94287133 -0.61082315 0.0066099167 0.59369421 1.1030769 1.3386884 0.546041 -0.38744736 -1.0332789 -1.5169261 -1.7194705 -1.7363021][-1.0488455 -1.0687463 -1.0148795 -0.87522674 -0.57225418 -0.12750816 0.18974972 0.3556571 0.44096947 -0.053007126 -0.58593988 -0.93886995 -1.2752268 -1.3855186 -1.3268163][-1.4385042 -1.5537541 -1.492769 -1.2907598 -1.0522985 -0.80373645 -0.6777916 -0.68434286 -0.591887 -0.7234807 -0.83001184 -0.8883884 -1.0451994 -1.0338781 -0.90205193][-1.795135 -2.0171266 -2.0236912 -1.8715038 -1.7505333 -1.6356986 -1.5487502 -1.5086849 -1.2666609 -1.0686572 -0.86612129 -0.69282794 -0.67264414 -0.53124523 -0.3825078][-1.882745 -2.1650321 -2.2582352 -2.2050319 -2.1931977 -2.1284182 -1.9737179 -1.7964108 -1.3995969 -0.97242785 -0.63646126 -0.39398909 -0.27910423 -0.055544376 0.037575245][-1.7894719 -2.1187875 -2.3093636 -2.3803716 -2.4160712 -2.2969017 -2.0160902 -1.7373223 -1.2799504 -0.80039287 -0.50135136 -0.311471 -0.18407393 0.049953938 0.043259621][-1.7219687 -2.0379517 -2.2649891 -2.4118578 -2.4617593 -2.3055036 -1.9760778 -1.7063196 -1.3308609 -0.94292212 -0.78508854 -0.73907232 -0.68491912 -0.5255661 -0.61528349]]...]
INFO - root - 2017-12-07 07:24:35.321568: step 18810, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.820 sec/batch; 71h:25m:04s remains)
INFO - root - 2017-12-07 07:24:43.533392: step 18820, loss = 0.87, batch loss = 0.80 (10.1 examples/sec; 0.793 sec/batch; 69h:04m:59s remains)
INFO - root - 2017-12-07 07:24:51.935487: step 18830, loss = 0.76, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:56m:52s remains)
INFO - root - 2017-12-07 07:25:00.305718: step 18840, loss = 0.94, batch loss = 0.87 (9.5 examples/sec; 0.840 sec/batch; 73h:11m:16s remains)
INFO - root - 2017-12-07 07:25:08.552302: step 18850, loss = 0.86, batch loss = 0.79 (9.7 examples/sec; 0.825 sec/batch; 71h:52m:16s remains)
INFO - root - 2017-12-07 07:25:16.863482: step 18860, loss = 0.70, batch loss = 0.63 (9.6 examples/sec; 0.833 sec/batch; 72h:32m:13s remains)
INFO - root - 2017-12-07 07:25:25.210061: step 18870, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 71h:59m:36s remains)
INFO - root - 2017-12-07 07:25:33.158678: step 18880, loss = 0.76, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:17m:48s remains)
INFO - root - 2017-12-07 07:25:41.530933: step 18890, loss = 0.69, batch loss = 0.62 (9.7 examples/sec; 0.827 sec/batch; 72h:03m:33s remains)
INFO - root - 2017-12-07 07:25:49.872857: step 18900, loss = 0.93, batch loss = 0.85 (9.3 examples/sec; 0.859 sec/batch; 74h:50m:17s remains)
2017-12-07 07:25:50.590468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2677293 -3.3453629 -3.4442177 -3.5056956 -3.5277455 -3.5405934 -3.5560532 -3.5632935 -3.524817 -3.4483566 -3.3718512 -3.3504381 -3.3430631 -3.3103075 -3.3104434][-3.2172928 -3.3568537 -3.5506158 -3.676434 -3.7224295 -3.7322936 -3.7221048 -3.6954746 -3.6183729 -3.5195451 -3.4328384 -3.3869972 -3.3316159 -3.232625 -3.1615195][-3.1004519 -3.2906713 -3.5783312 -3.7765169 -3.8564665 -3.8792276 -3.8737981 -3.8238559 -3.7261598 -3.6423402 -3.5816815 -3.5084691 -3.3797462 -3.2353935 -3.1608727][-2.7471545 -2.9287724 -3.2694001 -3.5384471 -3.6568809 -3.6478667 -3.5635252 -3.4132802 -3.2618518 -3.2009821 -3.1968844 -3.1551614 -3.0545082 -2.9794185 -2.9896126][-2.3445327 -2.4347422 -2.6915812 -2.8698883 -2.8640611 -2.6802769 -2.410717 -2.1021981 -1.9051497 -1.9127641 -2.0075486 -2.0783904 -2.093168 -2.0935962 -2.0561898][-1.870657 -1.7086208 -1.6798687 -1.5834754 -1.3616858 -0.938529 -0.40965748 0.027233124 0.057677269 -0.25317478 -0.53792524 -0.72183943 -0.8072989 -0.79387808 -0.573576][-1.4571433 -1.1275716 -0.96577072 -0.80991817 -0.51478744 0.074788094 0.8939395 1.5001054 1.2957511 0.64958715 0.25985432 0.11980438 0.060055733 0.12306833 0.46313477][-1.6461105 -1.4745421 -1.5582795 -1.6855936 -1.5532362 -0.98294187 -0.10667086 0.49889374 0.25495529 -0.32016993 -0.50316381 -0.39741707 -0.30687571 -0.18019247 0.15904951][-2.0848753 -2.0640197 -2.3873599 -2.7782698 -2.8285251 -2.4121108 -1.758034 -1.3430617 -1.5380046 -1.8656301 -1.8297243 -1.5284362 -1.3009913 -1.1492021 -0.89482522][-2.2674949 -2.2378407 -2.5976615 -3.0371614 -3.1396472 -2.8583069 -2.4436624 -2.2036438 -2.3385611 -2.5138507 -2.4597661 -2.2249315 -2.0657933 -2.0151529 -1.8680892][-2.5496216 -2.5013649 -2.8107219 -3.1702905 -3.2208581 -2.9470143 -2.5651889 -2.3434515 -2.4271145 -2.5768218 -2.5945683 -2.515547 -2.5119681 -2.5710158 -2.4511042][-2.9303875 -2.8744755 -3.0802956 -3.3283129 -3.3498635 -3.1040215 -2.7932072 -2.6316242 -2.6721065 -2.7506223 -2.7195587 -2.6500211 -2.646616 -2.6319964 -2.4475899][-3.2019708 -3.1763468 -3.3241746 -3.5219052 -3.5792162 -3.4313822 -3.2323067 -3.1354504 -3.1452923 -3.156034 -3.095437 -3.0374188 -3.0332263 -2.9659114 -2.7683606][-3.4710698 -3.5151522 -3.6574745 -3.8369973 -3.9320021 -3.8678265 -3.7769344 -3.7664421 -3.8204532 -3.8674307 -3.8644955 -3.8696113 -3.9120574 -3.9007726 -3.8044305][-3.6008434 -3.6650684 -3.7777984 -3.9074059 -3.9897926 -3.9810138 -3.9785655 -4.0421343 -4.1431928 -4.2331433 -4.2814512 -4.3270197 -4.3812218 -4.3952084 -4.3520122]]...]
INFO - root - 2017-12-07 07:25:58.961952: step 18910, loss = 0.91, batch loss = 0.84 (9.4 examples/sec; 0.849 sec/batch; 73h:54m:52s remains)
INFO - root - 2017-12-07 07:26:07.370336: step 18920, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.813 sec/batch; 70h:46m:34s remains)
INFO - root - 2017-12-07 07:26:15.656653: step 18930, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.848 sec/batch; 73h:53m:17s remains)
INFO - root - 2017-12-07 07:26:23.903964: step 18940, loss = 0.77, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 75h:21m:04s remains)
INFO - root - 2017-12-07 07:26:32.145481: step 18950, loss = 0.52, batch loss = 0.45 (9.8 examples/sec; 0.812 sec/batch; 70h:44m:49s remains)
INFO - root - 2017-12-07 07:26:40.604705: step 18960, loss = 0.74, batch loss = 0.67 (9.7 examples/sec; 0.821 sec/batch; 71h:31m:40s remains)
INFO - root - 2017-12-07 07:26:49.050355: step 18970, loss = 0.63, batch loss = 0.56 (9.9 examples/sec; 0.804 sec/batch; 70h:02m:00s remains)
INFO - root - 2017-12-07 07:26:57.645262: step 18980, loss = 0.69, batch loss = 0.61 (9.3 examples/sec; 0.862 sec/batch; 75h:05m:02s remains)
INFO - root - 2017-12-07 07:27:05.875875: step 18990, loss = 0.79, batch loss = 0.72 (9.2 examples/sec; 0.870 sec/batch; 75h:45m:48s remains)
INFO - root - 2017-12-07 07:27:14.231101: step 19000, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.799 sec/batch; 69h:34m:09s remains)
2017-12-07 07:27:14.872738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7978888 -3.5027285 -2.7131515 -1.4680252 -0.74825621 -1.0073161 -1.6055939 -2.0632839 -2.3858993 -2.6921659 -2.9104834 -2.8967018 -2.8368907 -2.9153986 -3.0634408][-4.1707854 -3.8467765 -3.0998118 -2.063843 -1.5871022 -1.9444439 -2.3930795 -2.5969205 -2.6892266 -2.8386889 -2.965483 -2.8949254 -2.7964487 -2.8531594 -3.0279891][-4.5728579 -4.2761326 -3.6985581 -2.9597592 -2.6450648 -2.81035 -2.8628972 -2.6803019 -2.5988212 -2.7678611 -2.9304876 -2.8330648 -2.6875443 -2.7160792 -2.9290504][-4.624342 -4.454051 -4.1065159 -3.6034961 -3.2463264 -2.9502275 -2.4850793 -2.0515075 -2.0679076 -2.4794095 -2.7394028 -2.586807 -2.394171 -2.4509144 -2.7610154][-4.400115 -4.39638 -4.2030349 -3.683938 -2.9647384 -2.0022047 -1.079134 -0.74932647 -1.2265759 -2.0029128 -2.2987232 -2.0552351 -1.8753176 -2.0729175 -2.5578661][-4.2159829 -4.2309952 -3.9087672 -3.0485449 -1.7574432 -0.16555214 0.96002626 0.809865 -0.30866146 -1.3206038 -1.5665476 -1.3443882 -1.3400793 -1.7778554 -2.4452915][-4.1417913 -4.0449567 -3.4658027 -2.199918 -0.46833134 1.265358 2.0736241 1.2870059 -0.0016527176 -0.77470088 -0.85381579 -0.76436353 -1.0161042 -1.689362 -2.4753475][-4.0609541 -3.8726876 -3.1192322 -1.5949776 0.18108177 1.3533583 1.3178396 0.31713676 -0.52255249 -0.74527884 -0.63699365 -0.66467595 -1.0557277 -1.8124261 -2.5841961][-3.8916855 -3.641115 -2.8110287 -1.2647598 0.13907194 0.44760036 -0.19162798 -1.0193799 -1.3516409 -1.2640908 -1.1589684 -1.2335069 -1.5502155 -2.1652887 -2.7620387][-3.5999274 -3.2678354 -2.4475327 -1.1431873 -0.27859497 -0.50398946 -1.2291756 -1.7549696 -1.8750525 -1.8743 -1.9424362 -2.0627115 -2.2403607 -2.625947 -2.9909127][-3.5031233 -3.0970654 -2.3358662 -1.3390238 -0.84412026 -1.156606 -1.6846929 -2.0208166 -2.2406945 -2.433167 -2.5916717 -2.6952977 -2.7516851 -2.9426987 -3.1280096][-3.5134063 -3.0527906 -2.3634148 -1.6277587 -1.3180578 -1.5596194 -1.9444172 -2.2820768 -2.6236191 -2.8479929 -2.9283853 -2.9687903 -2.9555764 -3.0415649 -3.1567714][-3.4197824 -2.9266167 -2.3288178 -1.832655 -1.6982 -1.9522815 -2.3360193 -2.69823 -3.0217571 -3.1413853 -3.0991428 -3.0783446 -3.0243456 -3.0534453 -3.1583047][-3.3147202 -2.8558173 -2.3740292 -2.0750647 -2.0808342 -2.3363779 -2.6833262 -2.9775662 -3.1863499 -3.202652 -3.1013374 -3.0699637 -3.0231283 -3.0500963 -3.1797588][-3.2742383 -2.8956628 -2.5222325 -2.3532367 -2.4163458 -2.6025691 -2.8410006 -3.0194626 -3.1176405 -3.0915816 -3.0193233 -3.0301769 -3.0344322 -3.0989177 -3.2475052]]...]
INFO - root - 2017-12-07 07:27:23.090314: step 19010, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 71h:38m:44s remains)
INFO - root - 2017-12-07 07:27:31.461929: step 19020, loss = 0.73, batch loss = 0.66 (9.7 examples/sec; 0.828 sec/batch; 72h:03m:38s remains)
INFO - root - 2017-12-07 07:27:39.790363: step 19030, loss = 0.73, batch loss = 0.66 (9.1 examples/sec; 0.883 sec/batch; 76h:52m:31s remains)
INFO - root - 2017-12-07 07:27:48.148375: step 19040, loss = 0.75, batch loss = 0.67 (10.5 examples/sec; 0.765 sec/batch; 66h:36m:01s remains)
INFO - root - 2017-12-07 07:27:56.372761: step 19050, loss = 0.77, batch loss = 0.70 (9.7 examples/sec; 0.823 sec/batch; 71h:38m:12s remains)
INFO - root - 2017-12-07 07:28:04.665494: step 19060, loss = 0.81, batch loss = 0.74 (9.1 examples/sec; 0.882 sec/batch; 76h:48m:28s remains)
INFO - root - 2017-12-07 07:28:13.196932: step 19070, loss = 0.72, batch loss = 0.65 (10.1 examples/sec; 0.793 sec/batch; 69h:00m:35s remains)
INFO - root - 2017-12-07 07:28:21.546235: step 19080, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 0.900 sec/batch; 78h:20m:36s remains)
INFO - root - 2017-12-07 07:28:29.959924: step 19090, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.825 sec/batch; 71h:50m:57s remains)
INFO - root - 2017-12-07 07:28:38.304912: step 19100, loss = 0.80, batch loss = 0.73 (9.7 examples/sec; 0.825 sec/batch; 71h:48m:18s remains)
2017-12-07 07:28:38.943914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1500485 -1.166925 -0.46256161 -0.70361233 -1.5499716 -2.4174957 -2.8831756 -2.7173905 -2.2483308 -2.0007384 -2.5056195 -3.4344728 -3.5303543 -2.4875512 -1.6389678][-2.266705 -1.2893045 -0.35383844 -0.27829409 -0.85349393 -1.695029 -2.190578 -1.9423652 -1.3561897 -1.1563673 -1.9106739 -3.0926285 -3.2806072 -2.1516249 -1.2134817][-1.8416245 -1.1999681 -0.37169123 -0.134696 -0.37069464 -1.027652 -1.5427575 -1.3961172 -0.85992551 -0.59471011 -1.2397287 -2.2849071 -2.4095502 -1.3877232 -0.71112204][-1.2965138 -1.131603 -0.629946 -0.39398956 -0.24920416 -0.48310375 -0.8446784 -0.86978269 -0.71567607 -0.66505885 -1.1893625 -1.8099387 -1.5511854 -0.57208133 -0.26811886][-0.85956454 -1.1900895 -1.0294447 -0.78877521 -0.17054224 0.23451233 0.28326607 0.1000967 -0.34624243 -0.80678105 -1.4656253 -1.7312727 -0.989434 -0.0019431114 -0.0067801476][-0.57948852 -1.2518048 -1.2714329 -0.90536237 0.1960144 1.3374262 1.9584441 1.5830374 0.36433792 -0.81502843 -1.7222049 -1.7457073 -0.58634138 0.50456047 0.28346682][-0.42945361 -1.1800766 -1.1883259 -0.63228488 0.69007587 2.4049578 3.8099213 3.5194578 1.5605903 -0.46762657 -1.8395755 -1.8268983 -0.40842676 0.861228 0.62008286][-0.47118092 -1.1971457 -1.1426041 -0.51665473 0.57389164 2.2733855 4.3336487 4.7314377 2.7006669 0.15112638 -1.670737 -1.8539734 -0.53555584 0.72061586 0.52317858][-0.78144312 -1.5639107 -1.5387504 -0.88175273 -0.042924881 1.2358875 3.222425 4.1975317 2.76933 0.33546829 -1.549845 -1.8790364 -0.81868172 0.23393965 -0.016712189][-1.0802391 -1.9623377 -2.2314873 -1.777446 -1.0012434 0.13461399 1.8324113 2.8954287 2.128325 0.17752695 -1.491169 -1.8815303 -1.1001277 -0.33114767 -0.66283011][-1.1025794 -1.9192345 -2.4708176 -2.4397664 -1.8027487 -0.63188148 0.80186224 1.6928806 1.3351526 -0.018758774 -1.3170424 -1.7034278 -1.316201 -0.97937512 -1.4938006][-1.101572 -1.7242002 -2.2433157 -2.4158053 -2.038449 -1.1316717 -0.17817974 0.32028437 0.12090397 -0.6668601 -1.4698958 -1.7194374 -1.6475222 -1.748194 -2.4973941][-1.959456 -2.3379574 -2.6521764 -2.7057185 -2.4380326 -1.891592 -1.4516294 -1.3649211 -1.5608087 -1.9761107 -2.3395495 -2.3888433 -2.3809309 -2.6294549 -3.3874102][-3.1211691 -3.3575389 -3.5321684 -3.4593253 -3.1878977 -2.8436713 -2.6705174 -2.7803023 -2.9525895 -3.1147189 -3.1766305 -3.1053524 -3.0766797 -3.2779188 -3.821413][-3.3054945 -3.4882665 -3.6454628 -3.6172426 -3.4361794 -3.2339892 -3.1552978 -3.2729042 -3.3834131 -3.4097104 -3.3597221 -3.2840075 -3.2671428 -3.4173028 -3.7500308]]...]
INFO - root - 2017-12-07 07:28:47.357907: step 19110, loss = 0.69, batch loss = 0.61 (9.1 examples/sec; 0.878 sec/batch; 76h:24m:57s remains)
INFO - root - 2017-12-07 07:28:55.700922: step 19120, loss = 0.79, batch loss = 0.72 (9.5 examples/sec; 0.841 sec/batch; 73h:10m:31s remains)
INFO - root - 2017-12-07 07:29:04.027503: step 19130, loss = 0.54, batch loss = 0.47 (9.7 examples/sec; 0.822 sec/batch; 71h:35m:32s remains)
INFO - root - 2017-12-07 07:29:12.185250: step 19140, loss = 0.85, batch loss = 0.78 (10.3 examples/sec; 0.778 sec/batch; 67h:45m:33s remains)
INFO - root - 2017-12-07 07:29:20.608415: step 19150, loss = 0.74, batch loss = 0.67 (9.6 examples/sec; 0.829 sec/batch; 72h:10m:31s remains)
INFO - root - 2017-12-07 07:29:28.909008: step 19160, loss = 0.57, batch loss = 0.50 (10.1 examples/sec; 0.791 sec/batch; 68h:49m:36s remains)
INFO - root - 2017-12-07 07:29:37.244027: step 19170, loss = 0.56, batch loss = 0.49 (9.6 examples/sec; 0.829 sec/batch; 72h:10m:38s remains)
INFO - root - 2017-12-07 07:29:45.527183: step 19180, loss = 0.86, batch loss = 0.79 (9.3 examples/sec; 0.860 sec/batch; 74h:52m:24s remains)
INFO - root - 2017-12-07 07:29:53.558746: step 19190, loss = 0.71, batch loss = 0.64 (9.9 examples/sec; 0.810 sec/batch; 70h:31m:30s remains)
INFO - root - 2017-12-07 07:30:01.770637: step 19200, loss = 0.64, batch loss = 0.57 (9.9 examples/sec; 0.807 sec/batch; 70h:15m:37s remains)
2017-12-07 07:30:02.497825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9134436 -2.9578352 -3.0399256 -3.0827079 -3.0753255 -3.0522993 -3.0161061 -2.980973 -2.9764462 -3.0296192 -3.0870159 -3.0980885 -3.0385747 -2.959208 -2.8893614][-2.4882941 -2.5734062 -2.7308173 -2.7886751 -2.7592728 -2.7326093 -2.7017941 -2.6853356 -2.7257895 -2.8734446 -3.0414882 -3.1431983 -3.1323242 -3.0611587 -2.9656978][-2.2602968 -2.4518301 -2.7303882 -2.8216357 -2.7985559 -2.8219798 -2.8412523 -2.8500152 -2.8971484 -3.0699556 -3.2881713 -3.4395213 -3.4697146 -3.3915815 -3.2146075][-2.3467605 -2.697845 -3.1750402 -3.3866329 -3.4355862 -3.5106692 -3.5432763 -3.5204029 -3.4804316 -3.5697012 -3.7497435 -3.9164422 -3.9853189 -3.8896151 -3.604959][-2.5943613 -3.0104649 -3.6181993 -3.9137583 -3.9753907 -3.9745128 -3.8611569 -3.657115 -3.4362788 -3.3908241 -3.5601044 -3.8576269 -4.1123509 -4.1293058 -3.8634236][-2.7970753 -3.2096658 -3.8046691 -4.0565534 -4.0180426 -3.837014 -3.5297818 -3.1759458 -2.8293757 -2.641212 -2.7789302 -3.2167854 -3.706439 -3.9243326 -3.7980413][-2.4119434 -2.6806469 -3.1394942 -3.2968411 -3.1353326 -2.8096585 -2.4331088 -2.1471879 -1.9624269 -1.878495 -2.1342077 -2.7112117 -3.3226097 -3.6196318 -3.5526638][-1.6221507 -1.5461321 -1.7211144 -1.7547493 -1.5466316 -1.2191565 -0.94996381 -0.90729666 -1.0066199 -1.1371472 -1.622061 -2.4371543 -3.2352204 -3.6125185 -3.5398571][-0.91790462 -0.63499141 -0.67259145 -0.68516636 -0.52062607 -0.285213 -0.14570141 -0.26429844 -0.46317863 -0.59229112 -1.0083308 -1.8356652 -2.7716765 -3.3685112 -3.479985][-0.95409632 -0.70112967 -0.8036325 -0.9422574 -0.90827703 -0.76414037 -0.66934586 -0.7879405 -0.97627854 -1.0702524 -1.2680583 -1.8397424 -2.6845298 -3.3315909 -3.5140629][-1.7711627 -1.6755998 -1.8859422 -2.1284885 -2.1716273 -2.0209422 -1.8278 -1.7631857 -1.8145738 -1.8678977 -1.9443293 -2.324332 -3.0597372 -3.6472988 -3.7551558][-2.457016 -2.5492163 -2.8789263 -3.1640339 -3.1897931 -2.9691331 -2.6494727 -2.3908107 -2.2567334 -2.2322445 -2.2845569 -2.5936913 -3.2452991 -3.7549129 -3.7922361][-2.7005246 -2.8491635 -3.15551 -3.3567371 -3.3071532 -3.0360711 -2.7129486 -2.4891381 -2.40864 -2.4304919 -2.4806151 -2.6659331 -3.0820127 -3.4323759 -3.4607346][-2.7453938 -2.859601 -3.0596471 -3.1475048 -3.0309258 -2.763279 -2.5189738 -2.4189577 -2.4537587 -2.5427666 -2.6055796 -2.7051229 -2.9147367 -3.1177936 -3.1619983][-2.7964511 -2.8893666 -3.0131674 -3.0377088 -2.9261212 -2.7467718 -2.6254416 -2.611105 -2.6543245 -2.7052844 -2.7365215 -2.798964 -2.9193835 -3.0417945 -3.081471]]...]
INFO - root - 2017-12-07 07:30:10.913832: step 19210, loss = 0.72, batch loss = 0.65 (9.4 examples/sec; 0.853 sec/batch; 74h:13m:44s remains)
INFO - root - 2017-12-07 07:30:19.223604: step 19220, loss = 0.71, batch loss = 0.64 (9.8 examples/sec; 0.820 sec/batch; 71h:21m:14s remains)
INFO - root - 2017-12-07 07:30:27.519276: step 19230, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.820 sec/batch; 71h:18m:55s remains)
INFO - root - 2017-12-07 07:30:35.790064: step 19240, loss = 0.89, batch loss = 0.82 (10.2 examples/sec; 0.785 sec/batch; 68h:20m:47s remains)
INFO - root - 2017-12-07 07:30:44.100124: step 19250, loss = 0.88, batch loss = 0.81 (9.6 examples/sec; 0.830 sec/batch; 72h:12m:10s remains)
INFO - root - 2017-12-07 07:30:52.313660: step 19260, loss = 0.63, batch loss = 0.56 (9.4 examples/sec; 0.854 sec/batch; 74h:17m:13s remains)
INFO - root - 2017-12-07 07:31:00.617639: step 19270, loss = 0.71, batch loss = 0.63 (9.9 examples/sec; 0.807 sec/batch; 70h:13m:11s remains)
INFO - root - 2017-12-07 07:31:09.005689: step 19280, loss = 0.78, batch loss = 0.71 (9.8 examples/sec; 0.817 sec/batch; 71h:03m:12s remains)
INFO - root - 2017-12-07 07:31:17.353768: step 19290, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.826 sec/batch; 71h:51m:25s remains)
INFO - root - 2017-12-07 07:31:25.773483: step 19300, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 73h:46m:24s remains)
2017-12-07 07:31:26.471265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5408683 -3.3654246 -3.3118048 -3.4285569 -3.4862046 -3.4079754 -3.454627 -3.6707821 -3.7952385 -3.6020563 -3.4138126 -3.5724161 -3.7245815 -3.5507021 -3.3121781][-3.2763786 -3.0764618 -3.0274167 -3.1867552 -3.2152963 -3.0279565 -3.0800045 -3.4439945 -3.7884021 -3.77619 -3.6485982 -3.7017827 -3.6327643 -3.2105265 -2.7951775][-2.988924 -2.6940625 -2.5789123 -2.7358804 -2.7154174 -2.4303913 -2.4908047 -2.9567842 -3.5064833 -3.7954862 -3.8877637 -3.9301906 -3.7233715 -3.1460547 -2.5479827][-2.632406 -2.1668167 -1.9524841 -2.0803685 -2.0436697 -1.7626257 -1.8795164 -2.3558674 -2.9794464 -3.5130315 -3.8507652 -3.9604821 -3.7352638 -3.1617963 -2.4728575][-2.3300908 -1.7516668 -1.5514026 -1.7241333 -1.7072685 -1.5182312 -1.7180831 -2.0900676 -2.559432 -3.1201029 -3.5625396 -3.7404735 -3.6303778 -3.2644205 -2.7166209][-2.219053 -1.6685402 -1.5738099 -1.7407956 -1.6894743 -1.6187239 -1.8936858 -2.0926847 -2.330214 -2.8505027 -3.3520217 -3.6317041 -3.7652001 -3.8063469 -3.6827445][-2.2256746 -1.8111546 -1.888809 -2.0194135 -1.8851211 -1.8625288 -2.1518342 -2.2303474 -2.3204849 -2.7848792 -3.2511945 -3.4984393 -3.6973972 -3.9931002 -4.2335439][-2.105093 -1.7687538 -1.9580698 -2.0687673 -1.9881532 -2.1411755 -2.5705628 -2.7234449 -2.7951293 -3.1695216 -3.4696577 -3.4889789 -3.4937735 -3.6834197 -3.9061372][-2.0115173 -1.7748854 -2.0239651 -2.1182482 -2.0973392 -2.3143718 -2.7911077 -3.1140456 -3.3544936 -3.7206619 -3.91057 -3.7912476 -3.6048145 -3.5452151 -3.5320954][-2.2230945 -2.2681854 -2.6458983 -2.7428663 -2.6412258 -2.6300101 -2.8329325 -3.1228809 -3.4309406 -3.7317622 -3.9004676 -3.8660977 -3.6899781 -3.4887567 -3.2552404][-2.49051 -2.8301816 -3.3481102 -3.4977655 -3.3294885 -3.07767 -3.0081077 -3.2342229 -3.5193772 -3.7108388 -3.8625293 -3.9178374 -3.8129094 -3.6077065 -3.2626011][-2.6112843 -3.049953 -3.6160874 -3.8870933 -3.7995911 -3.4601803 -3.2500181 -3.4036934 -3.6226358 -3.7721698 -3.9136355 -3.9552214 -3.8947277 -3.82087 -3.5627508][-2.6974759 -3.0636964 -3.5807643 -3.947885 -4.0015249 -3.6852472 -3.4028728 -3.4183955 -3.5209765 -3.7057788 -3.8950899 -3.9202657 -3.9050088 -3.9807673 -3.8786612][-2.9356823 -3.1295176 -3.4868546 -3.8287342 -3.9559038 -3.7392533 -3.51792 -3.5121868 -3.6069832 -3.8542752 -4.099535 -4.1397371 -4.1556487 -4.2565842 -4.1687975][-3.3997889 -3.4114857 -3.5523887 -3.7435148 -3.8409042 -3.7230525 -3.6057591 -3.6393056 -3.7547174 -4.0020165 -4.2427568 -4.2832947 -4.2725124 -4.3062377 -4.1931071]]...]
INFO - root - 2017-12-07 07:31:34.781091: step 19310, loss = 0.65, batch loss = 0.58 (10.0 examples/sec; 0.800 sec/batch; 69h:38m:23s remains)
INFO - root - 2017-12-07 07:31:43.143785: step 19320, loss = 0.75, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 73h:30m:54s remains)
INFO - root - 2017-12-07 07:31:51.544310: step 19330, loss = 0.87, batch loss = 0.80 (9.1 examples/sec; 0.878 sec/batch; 76h:24m:14s remains)
INFO - root - 2017-12-07 07:32:00.057267: step 19340, loss = 0.86, batch loss = 0.79 (8.8 examples/sec; 0.906 sec/batch; 78h:51m:14s remains)
INFO - root - 2017-12-07 07:32:08.380851: step 19350, loss = 0.64, batch loss = 0.56 (8.8 examples/sec; 0.905 sec/batch; 78h:41m:14s remains)
INFO - root - 2017-12-07 07:32:17.013639: step 19360, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.800 sec/batch; 69h:35m:57s remains)
INFO - root - 2017-12-07 07:32:25.244809: step 19370, loss = 0.85, batch loss = 0.78 (9.7 examples/sec; 0.828 sec/batch; 71h:58m:56s remains)
INFO - root - 2017-12-07 07:32:33.591567: step 19380, loss = 0.78, batch loss = 0.70 (9.9 examples/sec; 0.805 sec/batch; 70h:03m:20s remains)
INFO - root - 2017-12-07 07:32:41.908313: step 19390, loss = 0.74, batch loss = 0.67 (9.4 examples/sec; 0.855 sec/batch; 74h:21m:18s remains)
INFO - root - 2017-12-07 07:32:50.225865: step 19400, loss = 0.65, batch loss = 0.58 (9.5 examples/sec; 0.846 sec/batch; 73h:36m:07s remains)
2017-12-07 07:32:50.868257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0971832 -1.4023345 -1.8439305 -1.9459133 -1.8674519 -1.884877 -1.7981875 -1.6545186 -1.5785477 -1.5193753 -1.4709432 -1.5895398 -1.823339 -2.0030098 -2.022929][-1.1498976 -1.4108975 -1.8157573 -1.9514453 -2.0170844 -2.095016 -1.8940091 -1.6227765 -1.462069 -1.323071 -1.2550588 -1.4267907 -1.7119324 -1.969033 -2.0345776][-1.2551961 -1.5688927 -2.0063837 -2.2269886 -2.33758 -2.3475902 -2.0217152 -1.6761146 -1.4601107 -1.3091917 -1.3611355 -1.6219797 -1.8606291 -2.0341742 -2.0162051][-1.3365545 -1.7039504 -2.1814051 -2.4803452 -2.5915241 -2.5384793 -2.21563 -1.8641305 -1.5079355 -1.1964254 -1.2860622 -1.678426 -2.0135906 -2.1825595 -2.0660448][-1.6278257 -1.9567146 -2.3829534 -2.5935421 -2.5401869 -2.3893456 -2.225132 -2.1338761 -1.7936866 -1.2522736 -1.1846399 -1.5813823 -2.0124838 -2.2287102 -2.1269753][-2.0628288 -2.3289084 -2.7594543 -2.9318266 -2.6576996 -2.2286079 -2.0050457 -2.1176302 -1.9998629 -1.505805 -1.4023151 -1.7639203 -2.1772966 -2.3440089 -2.244787][-2.3377481 -2.5248475 -3.0568662 -3.3957663 -3.0943441 -2.4078767 -1.8639021 -1.8331897 -1.835676 -1.5625589 -1.6203625 -2.0551183 -2.4712112 -2.6079271 -2.5133209][-2.5570753 -2.5990674 -3.1253204 -3.5594578 -3.2845898 -2.4584458 -1.6521614 -1.436609 -1.5450695 -1.5687482 -1.8374703 -2.3389344 -2.7520831 -2.8719835 -2.7931695][-2.7845173 -2.692898 -3.0657897 -3.4195323 -3.1376221 -2.2571623 -1.2659199 -0.88631821 -1.1197147 -1.5112815 -2.0463679 -2.6028695 -2.9382136 -2.967268 -2.8467832][-2.7679148 -2.6399498 -2.897922 -3.2449346 -3.1639733 -2.5106249 -1.5296311 -0.93697619 -1.0061867 -1.4580722 -2.0896277 -2.6426907 -2.9146183 -2.891634 -2.754972][-2.8993964 -2.7972612 -2.9505985 -3.2604034 -3.3749156 -3.0646036 -2.3764796 -1.8111608 -1.695195 -1.9459379 -2.399344 -2.7852128 -2.9484651 -2.9089661 -2.8218958][-3.4170761 -3.4005544 -3.449244 -3.5677757 -3.6163094 -3.4532504 -3.0377862 -2.6413872 -2.503082 -2.6181426 -2.8997588 -3.1313062 -3.2301531 -3.2551279 -3.2963259][-3.8342297 -3.9002242 -3.9402363 -3.9341931 -3.8735588 -3.7283955 -3.4615359 -3.2190962 -3.1341338 -3.2086601 -3.3865981 -3.5306175 -3.6072786 -3.6639712 -3.7527578][-3.8681524 -3.9465 -4.0063038 -4.0021687 -3.9545424 -3.8715045 -3.7458878 -3.6533444 -3.6448016 -3.675282 -3.7196472 -3.7289662 -3.7074213 -3.6936827 -3.7190986][-3.5790093 -3.6854308 -3.7777243 -3.8202789 -3.8097711 -3.7570615 -3.6761608 -3.6205812 -3.6067357 -3.5909715 -3.5590689 -3.5013468 -3.4381578 -3.396704 -3.3865333]]...]
INFO - root - 2017-12-07 07:32:59.265348: step 19410, loss = 0.83, batch loss = 0.75 (9.0 examples/sec; 0.892 sec/batch; 77h:32m:25s remains)
INFO - root - 2017-12-07 07:33:07.707865: step 19420, loss = 0.75, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 74h:43m:48s remains)
INFO - root - 2017-12-07 07:33:16.073596: step 19430, loss = 0.81, batch loss = 0.74 (9.7 examples/sec; 0.829 sec/batch; 72h:03m:13s remains)
INFO - root - 2017-12-07 07:33:24.411730: step 19440, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.799 sec/batch; 69h:27m:15s remains)
INFO - root - 2017-12-07 07:33:32.883243: step 19450, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.823 sec/batch; 71h:34m:15s remains)
INFO - root - 2017-12-07 07:33:41.403173: step 19460, loss = 0.68, batch loss = 0.61 (9.6 examples/sec; 0.835 sec/batch; 72h:38m:46s remains)
INFO - root - 2017-12-07 07:33:49.750639: step 19470, loss = 0.79, batch loss = 0.72 (10.4 examples/sec; 0.772 sec/batch; 67h:10m:02s remains)
INFO - root - 2017-12-07 07:33:58.244304: step 19480, loss = 0.71, batch loss = 0.64 (9.3 examples/sec; 0.858 sec/batch; 74h:34m:42s remains)
INFO - root - 2017-12-07 07:34:06.547717: step 19490, loss = 0.68, batch loss = 0.61 (9.2 examples/sec; 0.872 sec/batch; 75h:48m:21s remains)
INFO - root - 2017-12-07 07:34:14.741178: step 19500, loss = 0.83, batch loss = 0.76 (9.9 examples/sec; 0.810 sec/batch; 70h:24m:44s remains)
2017-12-07 07:34:15.413637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9049881 -2.8878088 -2.7902379 -2.754456 -2.8102322 -2.8549056 -2.9107189 -3.0710797 -3.4025316 -3.8041022 -4.0955973 -4.2237267 -4.2761807 -4.3074827 -3.9958262][-3.0038671 -2.9760618 -2.8708749 -2.8184912 -2.8628488 -2.853395 -2.8498676 -2.9904451 -3.276073 -3.5900736 -3.8205183 -3.9700537 -4.049911 -4.0289826 -3.6222897][-2.9214773 -2.9145005 -2.8375428 -2.8129575 -2.9155955 -2.9399958 -2.9256158 -3.0057733 -3.1544242 -3.3425608 -3.5612388 -3.8007681 -3.9643054 -3.9356747 -3.4591458][-2.8536203 -2.9169288 -2.9033542 -2.9147153 -3.0749567 -3.1458595 -3.1144886 -3.0804691 -3.019177 -3.0425534 -3.2569633 -3.625176 -3.9034829 -3.8995814 -3.389343][-2.8060429 -2.9658432 -3.0467508 -3.0868025 -3.2352576 -3.2848115 -3.1853395 -3.0226555 -2.7842655 -2.6989937 -2.9586196 -3.4691782 -3.8485467 -3.8720281 -3.393764][-2.8489296 -3.0815709 -3.2310333 -3.2663207 -3.3476527 -3.326489 -3.1326318 -2.8591194 -2.5132532 -2.372236 -2.6514072 -3.2180233 -3.6379552 -3.7204914 -3.4136386][-2.9441085 -3.1424284 -3.2696462 -3.2685902 -3.2827272 -3.2104139 -2.9568295 -2.6155009 -2.2413743 -2.10614 -2.3882527 -2.911052 -3.2655077 -3.3869789 -3.3283553][-3.0123291 -3.05034 -3.0722215 -3.030683 -3.0267835 -2.9667351 -2.7343073 -2.3911769 -2.0750978 -2.0260348 -2.3146122 -2.74176 -2.9745893 -3.0739136 -3.1922889][-3.0366447 -2.8880887 -2.7819459 -2.7262027 -2.7702549 -2.7998381 -2.6656632 -2.3694766 -2.1045547 -2.0946777 -2.3489857 -2.6771588 -2.8119493 -2.8493948 -3.007122][-2.9575491 -2.7076216 -2.5255284 -2.4717307 -2.5574546 -2.66189 -2.6222272 -2.3852239 -2.1677206 -2.1685781 -2.370446 -2.619504 -2.7123625 -2.7137475 -2.8278103][-2.8274851 -2.5641282 -2.3285341 -2.2398162 -2.3024178 -2.4102385 -2.4127393 -2.2493672 -2.1235209 -2.1771951 -2.3782771 -2.6059868 -2.7186761 -2.7461171 -2.8179][-2.8204894 -2.6086025 -2.3324802 -2.177633 -2.1645217 -2.1961348 -2.1912951 -2.1136487 -2.1341853 -2.2993693 -2.5372818 -2.7544537 -2.8673368 -2.9325738 -2.9914579][-2.9109197 -2.7893114 -2.5237532 -2.3571718 -2.3075781 -2.2752149 -2.2541986 -2.2464051 -2.3958604 -2.6542597 -2.8846865 -3.0262563 -3.0676315 -3.1271546 -3.1996045][-3.0288434 -2.979753 -2.7200222 -2.5629339 -2.5486207 -2.5617509 -2.5757632 -2.5643559 -2.6821024 -2.8992548 -3.073329 -3.1571479 -3.166297 -3.2232628 -3.3155456][-2.9613695 -2.9415531 -2.6734829 -2.5311818 -2.5958066 -2.7486396 -2.8619199 -2.8135691 -2.800703 -2.9033952 -3.0269969 -3.1271181 -3.1792049 -3.2508712 -3.3350081]]...]
INFO - root - 2017-12-07 07:34:23.824407: step 19510, loss = 0.92, batch loss = 0.85 (8.9 examples/sec; 0.900 sec/batch; 78h:12m:41s remains)
INFO - root - 2017-12-07 07:34:32.301418: step 19520, loss = 0.83, batch loss = 0.76 (9.7 examples/sec; 0.821 sec/batch; 71h:24m:27s remains)
INFO - root - 2017-12-07 07:34:40.769947: step 19530, loss = 0.81, batch loss = 0.74 (9.9 examples/sec; 0.812 sec/batch; 70h:34m:01s remains)
INFO - root - 2017-12-07 07:34:49.314032: step 19540, loss = 0.63, batch loss = 0.56 (9.2 examples/sec; 0.868 sec/batch; 75h:28m:10s remains)
INFO - root - 2017-12-07 07:34:57.829530: step 19550, loss = 0.70, batch loss = 0.62 (9.3 examples/sec; 0.861 sec/batch; 74h:52m:10s remains)
INFO - root - 2017-12-07 07:35:06.256152: step 19560, loss = 0.66, batch loss = 0.58 (8.9 examples/sec; 0.900 sec/batch; 78h:14m:17s remains)
INFO - root - 2017-12-07 07:35:14.601751: step 19570, loss = 0.75, batch loss = 0.68 (10.0 examples/sec; 0.796 sec/batch; 69h:13m:08s remains)
INFO - root - 2017-12-07 07:35:23.031932: step 19580, loss = 0.90, batch loss = 0.83 (9.6 examples/sec; 0.830 sec/batch; 72h:10m:02s remains)
INFO - root - 2017-12-07 07:35:31.262411: step 19590, loss = 0.63, batch loss = 0.56 (9.5 examples/sec; 0.845 sec/batch; 73h:27m:35s remains)
INFO - root - 2017-12-07 07:35:39.592681: step 19600, loss = 1.03, batch loss = 0.96 (10.0 examples/sec; 0.799 sec/batch; 69h:24m:49s remains)
2017-12-07 07:35:40.208817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6658611 -3.7699246 -3.696815 -3.5795975 -3.3235688 -3.0321703 -2.9950528 -3.1088719 -3.1533446 -3.2253628 -3.3292646 -3.3754783 -3.4292159 -3.5376928 -3.5677433][-3.7913222 -3.8956239 -3.7714505 -3.5169005 -3.0400581 -2.6069164 -2.5771241 -2.7465448 -2.8362975 -2.9859447 -3.1530731 -3.1974709 -3.2532203 -3.4322588 -3.4974132][-3.7374666 -3.7960148 -3.5989196 -3.1996746 -2.520335 -2.0214798 -2.0292616 -2.193382 -2.3204436 -2.5578046 -2.7591615 -2.7622709 -2.822228 -3.0869269 -3.1939263][-3.579421 -3.5778773 -3.3483443 -2.8798046 -2.1232593 -1.6207728 -1.6177998 -1.6630256 -1.7806911 -2.1327121 -2.3678885 -2.3033476 -2.3368304 -2.6195419 -2.7352951][-3.5605922 -3.5232716 -3.2950773 -2.8483245 -2.1228776 -1.6455612 -1.4905345 -1.2614784 -1.3476834 -1.8475456 -2.0826147 -1.9319189 -1.9537697 -2.3078494 -2.4881248][-3.5837626 -3.5744338 -3.3745542 -2.9692554 -2.2861154 -1.7951384 -1.3494503 -0.75571537 -0.8643117 -1.4990363 -1.6518219 -1.4508421 -1.5422552 -2.0375216 -2.3654308][-3.6388688 -3.640404 -3.3872449 -2.9611166 -2.3136561 -1.7671013 -0.94897389 0.086102009 -0.10675907 -0.86549282 -0.92493534 -0.80132937 -1.0810909 -1.7354054 -2.2399971][-3.7625442 -3.7696681 -3.4397807 -2.9426842 -2.2481892 -1.5681291 -0.36218119 1.0768356 0.72268486 -0.17928839 -0.19173861 -0.26814842 -0.78174281 -1.502655 -2.0857813][-3.8956857 -3.9272294 -3.5875573 -3.038394 -2.247571 -1.4634271 -0.074810028 1.4423094 0.85924625 -0.12086916 -0.039188862 -0.22396278 -0.81312966 -1.3918493 -1.8198559][-3.9618669 -4.0505247 -3.7625031 -3.1647604 -2.2165198 -1.4058175 -0.21713829 0.89278984 0.1210866 -0.75634289 -0.4943645 -0.58126807 -1.0513883 -1.4124064 -1.6420193][-3.9018538 -4.0958195 -3.9225564 -3.2560253 -2.1516693 -1.2702935 -0.39219332 0.23702049 -0.54604268 -1.2156048 -0.80907726 -0.79514527 -1.1117072 -1.3200197 -1.4887283][-3.8244505 -4.1040339 -4.0505557 -3.3975909 -2.2596014 -1.3662782 -0.86558914 -0.59787655 -1.1366799 -1.4607475 -1.0075235 -1.0109227 -1.2579775 -1.4727409 -1.6537383][-3.8191657 -4.1469555 -4.2081027 -3.6784477 -2.6476765 -1.8549342 -1.6914647 -1.5951476 -1.6968763 -1.6470101 -1.2605002 -1.3705199 -1.6188791 -1.9158304 -2.1584678][-3.9027107 -4.3163548 -4.50942 -4.1465082 -3.2647114 -2.5989583 -2.6135068 -2.4710708 -2.1896574 -1.926532 -1.7084715 -1.9617777 -2.2340634 -2.5524516 -2.7449512][-4.0073442 -4.4777 -4.69451 -4.415 -3.7193897 -3.2976158 -3.4657948 -3.2977614 -2.8547416 -2.4964213 -2.3466551 -2.6158295 -2.8709126 -3.1173878 -3.1083446]]...]
INFO - root - 2017-12-07 07:35:48.563120: step 19610, loss = 0.78, batch loss = 0.70 (9.9 examples/sec; 0.810 sec/batch; 70h:21m:47s remains)
INFO - root - 2017-12-07 07:35:56.713023: step 19620, loss = 0.80, batch loss = 0.73 (10.0 examples/sec; 0.803 sec/batch; 69h:46m:00s remains)
INFO - root - 2017-12-07 07:36:05.186797: step 19630, loss = 0.76, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:53m:16s remains)
INFO - root - 2017-12-07 07:36:13.499797: step 19640, loss = 0.72, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 71h:33m:52s remains)
INFO - root - 2017-12-07 07:36:21.824432: step 19650, loss = 0.60, batch loss = 0.53 (10.3 examples/sec; 0.778 sec/batch; 67h:35m:08s remains)
INFO - root - 2017-12-07 07:36:30.243146: step 19660, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 0.896 sec/batch; 77h:50m:52s remains)
INFO - root - 2017-12-07 07:36:38.593644: step 19670, loss = 0.83, batch loss = 0.76 (9.0 examples/sec; 0.887 sec/batch; 77h:06m:54s remains)
INFO - root - 2017-12-07 07:36:46.834658: step 19680, loss = 0.58, batch loss = 0.51 (10.5 examples/sec; 0.765 sec/batch; 66h:26m:10s remains)
INFO - root - 2017-12-07 07:36:55.239580: step 19690, loss = 0.69, batch loss = 0.62 (10.0 examples/sec; 0.802 sec/batch; 69h:41m:35s remains)
INFO - root - 2017-12-07 07:37:03.711725: step 19700, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.864 sec/batch; 75h:06m:49s remains)
2017-12-07 07:37:04.381884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8802979 -2.9667687 -3.0430355 -2.9332366 -2.7776542 -3.0201454 -3.4847848 -3.4631467 -3.000663 -2.5883765 -2.3580124 -2.1127186 -2.0093639 -2.307343 -2.7743134][-3.1511264 -3.2785888 -3.3976016 -3.3329687 -3.086762 -3.0124071 -3.1255667 -2.9583764 -2.6004024 -2.3334568 -2.1339889 -1.9159946 -1.9200983 -2.2477744 -2.7068114][-3.191658 -3.406496 -3.5862255 -3.5495925 -3.2250147 -2.8681111 -2.6708205 -2.4675548 -2.280544 -2.1090636 -1.8889418 -1.7445395 -1.9008386 -2.2320101 -2.6486843][-3.1430738 -3.4156134 -3.6311455 -3.6156554 -3.2646265 -2.7691114 -2.4003446 -2.2131875 -2.1476839 -2.0143068 -1.8185353 -1.8058732 -2.0545588 -2.3110993 -2.6820927][-3.0200441 -3.2573009 -3.4550433 -3.4807675 -3.2408333 -2.8587196 -2.5382693 -2.3853421 -2.3281307 -2.1854603 -2.0226374 -2.0790484 -2.2926512 -2.4772375 -2.8652637][-2.841989 -2.9389422 -3.0729356 -3.1391258 -3.1068749 -2.9823639 -2.8113661 -2.6549625 -2.523941 -2.3619392 -2.1803246 -2.2024915 -2.3854704 -2.635721 -3.1233149][-2.7080045 -2.6160245 -2.6399837 -2.7042284 -2.8521857 -2.9973068 -2.981504 -2.7954178 -2.4998498 -2.2105656 -1.9736149 -2.010777 -2.3192837 -2.724406 -3.2882094][-2.6386788 -2.3946314 -2.2520535 -2.1911056 -2.4074354 -2.7577863 -2.8862836 -2.6795659 -2.1993546 -1.7552488 -1.5381641 -1.7252221 -2.2421782 -2.773627 -3.3471954][-2.5917566 -2.1879246 -1.7943304 -1.4969261 -1.6731913 -2.144623 -2.380223 -2.1829081 -1.6213889 -1.2098835 -1.2636487 -1.753237 -2.4432576 -2.9950004 -3.51717][-2.5691326 -2.0102825 -1.3696451 -0.87436843 -0.94383979 -1.357389 -1.5699754 -1.4029965 -1.030216 -0.9765861 -1.4390659 -2.1634715 -2.8822947 -3.3946326 -3.8440475][-2.6200089 -2.0073304 -1.2922382 -0.73616672 -0.60713983 -0.68085194 -0.65327716 -0.53754354 -0.52367353 -0.88755989 -1.5812292 -2.3324778 -3.0325341 -3.5864353 -4.0316982][-2.7510917 -2.2509105 -1.6710775 -1.1945467 -0.87519169 -0.54191375 -0.27012634 -0.25309324 -0.54493618 -1.1201746 -1.821378 -2.48561 -3.1121588 -3.62364 -3.9731269][-2.9235678 -2.6445942 -2.3074386 -1.9649055 -1.5608053 -1.0516012 -0.72567344 -0.78345275 -1.1388938 -1.6611862 -2.2149861 -2.7395487 -3.2287984 -3.5826972 -3.7753277][-3.1035049 -2.988874 -2.8188579 -2.6038327 -2.2949882 -1.9205575 -1.7260258 -1.7972322 -1.9940929 -2.2786717 -2.5937696 -2.9117417 -3.200017 -3.4051824 -3.5449841][-3.2448897 -3.1919122 -3.1088567 -3.0170286 -2.9057708 -2.7650926 -2.7026985 -2.7233453 -2.7348456 -2.7870421 -2.8341804 -2.8700805 -2.9301558 -3.0443473 -3.2317643]]...]
INFO - root - 2017-12-07 07:37:12.763359: step 19710, loss = 0.70, batch loss = 0.62 (9.5 examples/sec; 0.843 sec/batch; 73h:12m:13s remains)
INFO - root - 2017-12-07 07:37:21.107416: step 19720, loss = 0.90, batch loss = 0.82 (9.8 examples/sec; 0.816 sec/batch; 70h:55m:15s remains)
INFO - root - 2017-12-07 07:37:29.444620: step 19730, loss = 0.68, batch loss = 0.61 (9.2 examples/sec; 0.867 sec/batch; 75h:20m:58s remains)
INFO - root - 2017-12-07 07:37:37.807830: step 19740, loss = 0.78, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 73h:06m:51s remains)
INFO - root - 2017-12-07 07:37:46.300194: step 19750, loss = 0.82, batch loss = 0.74 (9.1 examples/sec; 0.882 sec/batch; 76h:37m:30s remains)
INFO - root - 2017-12-07 07:37:54.641439: step 19760, loss = 0.68, batch loss = 0.60 (9.1 examples/sec; 0.882 sec/batch; 76h:37m:22s remains)
INFO - root - 2017-12-07 07:38:02.910502: step 19770, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.778 sec/batch; 67h:36m:32s remains)
INFO - root - 2017-12-07 07:38:11.282193: step 19780, loss = 0.94, batch loss = 0.86 (9.6 examples/sec; 0.833 sec/batch; 72h:19m:19s remains)
INFO - root - 2017-12-07 07:38:19.593442: step 19790, loss = 0.65, batch loss = 0.58 (9.3 examples/sec; 0.856 sec/batch; 74h:23m:51s remains)
INFO - root - 2017-12-07 07:38:27.885754: step 19800, loss = 0.95, batch loss = 0.88 (9.8 examples/sec; 0.816 sec/batch; 70h:52m:19s remains)
2017-12-07 07:38:28.524260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7544451 -2.1147919 -2.2866032 -2.1855583 -2.1613431 -2.4645312 -3.0528073 -3.51717 -3.7727079 -4.0141258 -3.9208279 -3.2258296 -2.0903671 -1.0986104 -0.94806623][-1.7242196 -1.9908652 -2.1625555 -2.1475067 -2.2090745 -2.5312042 -2.9543386 -3.2204866 -3.5099061 -3.8986664 -3.8606119 -3.0717168 -1.756067 -0.58762956 -0.35462284][-2.1667082 -2.3123422 -2.4267859 -2.4201105 -2.4363801 -2.6617935 -2.8525069 -2.85562 -3.0471561 -3.5061467 -3.595433 -2.8844266 -1.5603654 -0.38651657 -0.1630497][-2.9135277 -2.9072924 -2.8806071 -2.7984939 -2.7309437 -2.8271141 -2.7931397 -2.5593615 -2.5973949 -3.0871058 -3.3850086 -2.8982987 -1.76543 -0.75569534 -0.51134157][-3.4919357 -3.2896702 -3.08082 -2.9345827 -2.8630075 -2.9127316 -2.7381458 -2.3149173 -2.1260736 -2.5508609 -3.0843883 -3.0075433 -2.2497311 -1.4249108 -1.0012453][-3.7346511 -3.3040304 -2.8507085 -2.5975852 -2.5266347 -2.6109939 -2.4332695 -1.9510672 -1.5894444 -1.8535221 -2.4978967 -2.8911877 -2.6219554 -1.9306023 -1.2643104][-3.7466216 -3.2229509 -2.619565 -2.2637475 -2.0931859 -2.0920496 -1.9187965 -1.5035844 -1.1594436 -1.2501829 -1.797823 -2.5183187 -2.7883482 -2.2752249 -1.4515867][-3.6621644 -3.1942158 -2.6615348 -2.336262 -2.0657723 -1.8270178 -1.5359793 -1.126406 -0.84632206 -0.783221 -1.0914495 -1.9115012 -2.6469057 -2.4238465 -1.6001875][-3.5445557 -3.2367105 -2.8998528 -2.6959825 -2.3922679 -1.8598945 -1.3561981 -0.8432641 -0.53253388 -0.385437 -0.53208375 -1.3209939 -2.3464396 -2.4167197 -1.6668465][-3.4348729 -3.2535026 -3.0525875 -2.9243932 -2.5666142 -1.7597997 -1.0683873 -0.49047494 -0.19886303 -0.19704914 -0.37367916 -1.0749421 -2.1436408 -2.3963785 -1.7052252][-3.3410242 -3.25672 -3.0972426 -2.9439631 -2.4897974 -1.4974802 -0.71612144 -0.1598568 0.0020289421 -0.34960842 -0.73827505 -1.309581 -2.1929905 -2.4450703 -1.7532852][-2.9902191 -3.0392966 -2.9581041 -2.8306966 -2.4000103 -1.4785354 -0.78885269 -0.35534573 -0.35203648 -0.98706603 -1.5537913 -1.9583139 -2.5138249 -2.5937128 -1.8467546][-2.3964825 -2.6086044 -2.65301 -2.6327872 -2.3795848 -1.7566819 -1.3002043 -1.0826862 -1.1947446 -1.8403997 -2.3822351 -2.586525 -2.7935948 -2.6583586 -1.9060602][-1.9692488 -2.3006675 -2.4555485 -2.5498734 -2.5040884 -2.1796403 -1.9085395 -1.8668664 -2.0225179 -2.4586892 -2.7869306 -2.8006392 -2.7869935 -2.5686359 -1.9372475][-1.7575958 -2.10795 -2.306756 -2.4603248 -2.5885689 -2.5442052 -2.3749685 -2.3742585 -2.4653349 -2.6282618 -2.7345269 -2.636734 -2.52701 -2.3880668 -1.9639363]]...]
INFO - root - 2017-12-07 07:38:36.785167: step 19810, loss = 0.79, batch loss = 0.72 (9.7 examples/sec; 0.825 sec/batch; 71h:38m:36s remains)
INFO - root - 2017-12-07 07:38:44.825636: step 19820, loss = 0.61, batch loss = 0.54 (9.3 examples/sec; 0.865 sec/batch; 75h:06m:52s remains)
INFO - root - 2017-12-07 07:38:53.276597: step 19830, loss = 0.65, batch loss = 0.58 (9.4 examples/sec; 0.854 sec/batch; 74h:12m:41s remains)
INFO - root - 2017-12-07 07:39:01.585765: step 19840, loss = 0.80, batch loss = 0.73 (9.2 examples/sec; 0.868 sec/batch; 75h:24m:29s remains)
INFO - root - 2017-12-07 07:39:09.919926: step 19850, loss = 0.89, batch loss = 0.82 (10.0 examples/sec; 0.800 sec/batch; 69h:30m:07s remains)
INFO - root - 2017-12-07 07:39:18.219947: step 19860, loss = 0.79, batch loss = 0.72 (10.3 examples/sec; 0.777 sec/batch; 67h:29m:17s remains)
INFO - root - 2017-12-07 07:39:26.564755: step 19870, loss = 0.84, batch loss = 0.76 (9.6 examples/sec; 0.832 sec/batch; 72h:16m:20s remains)
INFO - root - 2017-12-07 07:39:34.863104: step 19880, loss = 0.76, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 75h:11m:19s remains)
INFO - root - 2017-12-07 07:39:43.457380: step 19890, loss = 0.96, batch loss = 0.89 (8.8 examples/sec; 0.914 sec/batch; 79h:19m:57s remains)
INFO - root - 2017-12-07 07:39:51.805828: step 19900, loss = 0.82, batch loss = 0.75 (9.2 examples/sec; 0.866 sec/batch; 75h:11m:29s remains)
2017-12-07 07:39:52.512556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.70819 -1.4709127 -1.3644769 -1.5020313 -1.6533132 -1.793334 -1.9550426 -2.0475547 -2.0274832 -1.8949306 -1.8300796 -1.74122 -1.4381177 -1.0064266 -0.73548222][-1.6417789 -1.5878606 -1.5311706 -1.5709827 -1.6522946 -1.822031 -2.0499666 -2.1784933 -2.1704364 -2.0389354 -1.9175205 -1.7668247 -1.5123672 -1.1526432 -0.88650894][-1.8472021 -1.8705757 -1.7254238 -1.6898129 -1.8030345 -2.0267248 -2.2587438 -2.3699636 -2.3637054 -2.2532067 -2.0420492 -1.8540132 -1.7080965 -1.5221224 -1.3516755][-2.2623932 -2.2479184 -2.0412607 -2.0089693 -2.1058717 -2.2179585 -2.3498149 -2.4325514 -2.4353547 -2.3030586 -1.9690518 -1.6970923 -1.5967462 -1.5535662 -1.5017233][-2.4322934 -2.4192152 -2.3455145 -2.415478 -2.4024684 -2.2632756 -2.2456508 -2.349844 -2.3547254 -2.1225476 -1.6671302 -1.3328896 -1.2422283 -1.2486367 -1.2543249][-2.2798278 -2.3949354 -2.5321705 -2.6295009 -2.4349205 -2.0758588 -1.9632576 -2.0747883 -2.0374677 -1.742367 -1.3187459 -1.0951715 -1.0560279 -1.0137975 -0.9511385][-2.26514 -2.5790884 -2.7435982 -2.6196718 -2.1550658 -1.6492853 -1.4670758 -1.5134344 -1.4616184 -1.2443821 -1.043262 -1.0889826 -1.169317 -1.0546896 -0.86088157][-2.5310483 -2.9279842 -2.976356 -2.6236243 -1.9865735 -1.4078822 -1.1057298 -1.0485363 -1.0440683 -0.99786758 -1.0089817 -1.235574 -1.3862145 -1.257858 -0.99397182][-2.5329232 -2.8058281 -2.7771707 -2.4220672 -1.8502591 -1.3385296 -1.0677264 -1.0183904 -1.0434699 -1.0833023 -1.1047525 -1.2370706 -1.3064034 -1.1922069 -0.98512268][-2.1492965 -2.2561448 -2.2309415 -2.0049562 -1.6178422 -1.2739191 -1.1669726 -1.1958005 -1.1538882 -1.0816877 -0.9563098 -0.93610668 -0.91556239 -0.82487273 -0.73769283][-1.8173554 -1.8286884 -1.8389914 -1.7148106 -1.4854896 -1.2638328 -1.2816887 -1.346024 -1.1947787 -0.95544696 -0.70727396 -0.61041045 -0.58067656 -0.55736732 -0.57376528][-1.9016023 -1.8312511 -1.8295934 -1.760386 -1.6508508 -1.5322604 -1.5658739 -1.5914719 -1.385164 -1.0843863 -0.79011536 -0.6599679 -0.64249754 -0.67184615 -0.72863865][-2.3143504 -2.2280855 -2.1799388 -2.1010804 -2.0654843 -2.0571659 -2.0806389 -2.0607507 -1.8612173 -1.5667546 -1.2924581 -1.1610491 -1.1344588 -1.1431339 -1.1759081][-2.6565814 -2.5466487 -2.4374843 -2.3349724 -2.3247089 -2.3805029 -2.4140887 -2.4073372 -2.2867515 -2.0765138 -1.8784988 -1.7980134 -1.777992 -1.7489491 -1.7286348][-2.8192444 -2.6755376 -2.529006 -2.4178617 -2.4025517 -2.4563074 -2.4691687 -2.4651775 -2.4163287 -2.3032181 -2.2091215 -2.2098932 -2.237391 -2.2317388 -2.2106264]]...]
INFO - root - 2017-12-07 07:40:00.819194: step 19910, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.836 sec/batch; 72h:33m:04s remains)
INFO - root - 2017-12-07 07:40:09.097959: step 19920, loss = 0.68, batch loss = 0.61 (9.7 examples/sec; 0.824 sec/batch; 71h:32m:17s remains)
INFO - root - 2017-12-07 07:40:17.435059: step 19930, loss = 0.82, batch loss = 0.75 (9.4 examples/sec; 0.854 sec/batch; 74h:06m:42s remains)
INFO - root - 2017-12-07 07:40:25.745613: step 19940, loss = 0.84, batch loss = 0.76 (9.0 examples/sec; 0.885 sec/batch; 76h:50m:15s remains)
INFO - root - 2017-12-07 07:40:34.087028: step 19950, loss = 0.65, batch loss = 0.57 (9.8 examples/sec; 0.817 sec/batch; 70h:54m:52s remains)
INFO - root - 2017-12-07 07:40:42.367688: step 19960, loss = 0.73, batch loss = 0.66 (10.1 examples/sec; 0.795 sec/batch; 69h:00m:03s remains)
INFO - root - 2017-12-07 07:40:50.761105: step 19970, loss = 0.76, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 72h:22m:55s remains)
INFO - root - 2017-12-07 07:40:59.104426: step 19980, loss = 0.55, batch loss = 0.48 (9.2 examples/sec; 0.872 sec/batch; 75h:43m:17s remains)
INFO - root - 2017-12-07 07:41:07.478115: step 19990, loss = 0.72, batch loss = 0.65 (9.8 examples/sec; 0.819 sec/batch; 71h:06m:08s remains)
INFO - root - 2017-12-07 07:41:15.891839: step 20000, loss = 0.59, batch loss = 0.52 (9.6 examples/sec; 0.836 sec/batch; 72h:33m:00s remains)
2017-12-07 07:41:16.562767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3293312 -2.5063219 -2.6500981 -2.8001151 -2.9640322 -3.0787339 -3.0664053 -2.9376144 -2.8038964 -2.7342267 -2.6480536 -2.580132 -2.6913636 -2.9035606 -2.987879][-2.291615 -2.5946491 -2.6970811 -2.7178283 -2.7367208 -2.7371738 -2.6730189 -2.4801521 -2.3408773 -2.3256116 -2.2748041 -2.2238493 -2.4550185 -2.8681316 -3.0602732][-2.5236282 -2.6939917 -2.5230844 -2.2926242 -2.1327498 -2.0707023 -2.0959072 -2.0004022 -1.9750607 -2.1663718 -2.2714393 -2.2611256 -2.5988431 -3.1506381 -3.3585141][-2.7083135 -2.5108871 -2.039906 -1.7234538 -1.5482752 -1.5057025 -1.6087532 -1.4922292 -1.4540408 -1.8940721 -2.2345843 -2.3459079 -2.8668079 -3.5239217 -3.6652303][-2.6226721 -2.1633706 -1.6144788 -1.3297844 -1.0264084 -0.72905207 -0.52257586 0.056408882 0.3353529 -0.42705774 -1.1825714 -1.6276522 -2.5779254 -3.4592822 -3.589005][-2.3783503 -1.967263 -1.540138 -1.149297 -0.40620375 0.42826891 1.2652578 2.6250496 3.2259035 1.9546022 0.638041 -0.20809507 -1.711417 -2.9437108 -3.1380606][-2.3481011 -2.2092195 -1.8954835 -1.232018 0.035692215 1.300458 2.5455809 4.3545685 5.029254 3.2425256 1.5687537 0.60759592 -1.1131301 -2.4814329 -2.6685634][-2.4458756 -2.6213105 -2.5004716 -1.877882 -0.60374 0.58091927 1.6564245 3.2117014 3.6643133 1.9132104 0.44339466 -0.17390299 -1.490591 -2.524744 -2.508538][-2.4637289 -2.7561059 -2.7767444 -2.4497223 -1.6316957 -0.93254972 -0.44749904 0.37078333 0.54577017 -0.70479846 -1.6219387 -1.7824416 -2.4101877 -2.8519039 -2.5756078][-2.5387068 -2.6664956 -2.5729041 -2.3888686 -1.9730656 -1.7303247 -1.8241632 -1.7230515 -1.8176112 -2.5782514 -3.0571554 -3.0202641 -3.1738439 -3.2098746 -2.8271713][-2.8694265 -2.7915835 -2.5412035 -2.3716605 -2.2024245 -2.258368 -2.6663933 -2.919961 -3.1032181 -3.5080769 -3.7846384 -3.8147278 -3.8560019 -3.7888203 -3.4283][-3.09646 -3.0162687 -2.8310935 -2.7596211 -2.7485073 -2.9217663 -3.3574023 -3.5865831 -3.5886059 -3.6820962 -3.8820243 -4.0683241 -4.2106948 -4.2565622 -3.9751437][-2.8933647 -2.9632072 -3.0432608 -3.1963565 -3.300549 -3.4981542 -3.8299735 -3.8975363 -3.6509924 -3.4727933 -3.548399 -3.7482917 -3.9865093 -4.17589 -4.0231066][-2.7261767 -2.8899646 -3.1611853 -3.4769163 -3.6754093 -3.8768511 -4.1086578 -4.071435 -3.707334 -3.3993807 -3.3577118 -3.4754839 -3.6859281 -3.8968048 -3.8104715][-2.8428233 -3.0042548 -3.2932863 -3.6138389 -3.8087084 -3.9609911 -4.0899525 -4.0169683 -3.7268412 -3.4928703 -3.4378364 -3.4774432 -3.5870836 -3.6948667 -3.5869074]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-conv2/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 07:41:25.461094: step 20010, loss = 0.69, batch loss = 0.62 (9.0 examples/sec; 0.888 sec/batch; 77h:04m:47s remains)
INFO - root - 2017-12-07 07:41:33.829767: step 20020, loss = 0.57, batch loss = 0.50 (9.6 examples/sec; 0.829 sec/batch; 71h:59m:00s remains)
INFO - root - 2017-12-07 07:41:42.336261: step 20030, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.840 sec/batch; 72h:54m:31s remains)
INFO - root - 2017-12-07 07:41:50.666012: step 20040, loss = 0.82, batch loss = 0.75 (9.4 examples/sec; 0.852 sec/batch; 73h:56m:21s remains)
INFO - root - 2017-12-07 07:41:59.055968: step 20050, loss = 0.72, batch loss = 0.65 (9.7 examples/sec; 0.827 sec/batch; 71h:45m:28s remains)
INFO - root - 2017-12-07 07:42:07.422459: step 20060, loss = 0.80, batch loss = 0.72 (9.2 examples/sec; 0.868 sec/batch; 75h:21m:49s remains)
INFO - root - 2017-12-07 07:42:15.676995: step 20070, loss = 0.97, batch loss = 0.90 (9.6 examples/sec; 0.837 sec/batch; 72h:36m:04s remains)
INFO - root - 2017-12-07 07:42:24.033186: step 20080, loss = 0.81, batch loss = 0.73 (9.8 examples/sec; 0.818 sec/batch; 70h:57m:09s remains)
INFO - root - 2017-12-07 07:42:32.474052: step 20090, loss = 0.65, batch loss = 0.58 (9.6 examples/sec; 0.833 sec/batch; 72h:15m:22s remains)
INFO - root - 2017-12-07 07:42:40.834742: step 20100, loss = 0.85, batch loss = 0.77 (9.7 examples/sec; 0.824 sec/batch; 71h:32m:36s remains)
2017-12-07 07:42:41.509528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9380541 -2.1067863 -2.2556925 -2.3675888 -2.3768234 -2.3372946 -2.3170106 -2.1769621 -1.8291082 -1.6466897 -1.8478835 -2.177846 -2.473752 -2.8089442 -3.09312][-1.5010359 -1.7010696 -1.9169228 -2.0812228 -2.1658351 -2.2482228 -2.3840661 -2.3448968 -1.958905 -1.6985371 -1.7906356 -1.8874695 -1.9564338 -2.2521985 -2.6856837][-1.4935789 -1.4656215 -1.5717833 -1.7309649 -1.8800955 -2.0021443 -2.1507921 -2.210721 -2.0247552 -2.0266848 -2.3211648 -2.2882137 -1.9846222 -1.9729891 -2.3425705][-1.7033889 -1.5434206 -1.4942238 -1.5601978 -1.6915588 -1.8547778 -2.1077626 -2.4183011 -2.4782012 -2.7408762 -3.2639098 -3.1662667 -2.50916 -2.0954306 -2.2287686][-1.8230114 -1.8035257 -1.873517 -1.9853196 -2.0071378 -1.9032931 -1.960717 -2.306777 -2.4931798 -2.9148233 -3.5467384 -3.4459882 -2.7096391 -2.1965594 -2.2452188][-1.9157531 -1.799866 -1.7486446 -1.7788601 -1.7267425 -1.3240273 -1.016196 -1.2252951 -1.4866648 -2.0762389 -2.9015899 -2.994643 -2.4842739 -2.1743543 -2.3497376][-1.8504524 -1.7916262 -1.6184564 -1.4358103 -1.1214607 -0.374259 0.15256548 -0.074154854 -0.50216365 -1.2985456 -2.3146286 -2.5241947 -2.1888688 -2.0789828 -2.4321883][-1.3357902 -1.4750974 -1.5394359 -1.5413525 -1.2549031 -0.43955612 0.1240654 -0.072929859 -0.44906926 -1.2739797 -2.3435283 -2.5197673 -2.1291292 -1.9804885 -2.3909013][-0.98245525 -1.1205542 -1.3466873 -1.5725608 -1.4497581 -0.89369893 -0.61481571 -0.81741166 -0.97450066 -1.6656966 -2.677254 -2.8269134 -2.3669591 -2.02247 -2.3202848][-0.769526 -0.90248108 -1.2516341 -1.6156971 -1.6365407 -1.3271115 -1.3128345 -1.5340366 -1.5418553 -2.0116105 -2.841152 -3.0063934 -2.6268384 -2.1774306 -2.3239865][-1.0702221 -1.0485818 -1.1727016 -1.4051809 -1.4668059 -1.3143044 -1.4734559 -1.7560756 -1.8180237 -2.2770889 -2.9843712 -3.1707115 -2.897305 -2.4214978 -2.4270935][-2.0962894 -1.9758015 -1.7169533 -1.5433309 -1.3770468 -1.1735644 -1.335386 -1.6486175 -1.9102736 -2.5222206 -3.1736634 -3.3638444 -3.1548934 -2.7207119 -2.6405597][-2.7233243 -2.742619 -2.5040917 -2.265501 -2.0414011 -1.7128136 -1.6557965 -1.7570894 -2.0755312 -2.8060045 -3.3904593 -3.5156648 -3.3040185 -2.9431953 -2.841805][-3.1203589 -3.3711171 -3.3415523 -3.1911778 -2.9928341 -2.6379566 -2.482975 -2.3616033 -2.5776415 -3.2598147 -3.7101669 -3.7398939 -3.4818211 -3.159831 -3.0397189][-3.4555814 -3.7533548 -3.8290262 -3.7747316 -3.6852343 -3.5375381 -3.502728 -3.3154469 -3.3408253 -3.7711434 -4.0031319 -3.9392271 -3.6560926 -3.3605576 -3.2217388]]...]
INFO - root - 2017-12-07 07:42:49.930874: step 20110, loss = 0.84, batch loss = 0.77 (10.1 examples/sec; 0.796 sec/batch; 69h:03m:51s remains)
INFO - root - 2017-12-07 07:42:58.395930: step 20120, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 71h:13m:01s remains)
INFO - root - 2017-12-07 07:43:06.424407: step 20130, loss = 0.70, batch loss = 0.62 (9.6 examples/sec; 0.834 sec/batch; 72h:20m:34s remains)
INFO - root - 2017-12-07 07:43:14.843332: step 20140, loss = 0.89, batch loss = 0.82 (9.2 examples/sec; 0.869 sec/batch; 75h:22m:00s remains)
INFO - root - 2017-12-07 07:43:23.191404: step 20150, loss = 0.94, batch loss = 0.87 (10.5 examples/sec; 0.762 sec/batch; 66h:05m:09s remains)
INFO - root - 2017-12-07 07:43:31.505887: step 20160, loss = 0.80, batch loss = 0.73 (10.1 examples/sec; 0.792 sec/batch; 68h:40m:23s remains)
INFO - root - 2017-12-07 07:43:40.009247: step 20170, loss = 0.76, batch loss = 0.68 (9.4 examples/sec; 0.848 sec/batch; 73h:32m:53s remains)
INFO - root - 2017-12-07 07:43:48.335931: step 20180, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 69h:30m:28s remains)
INFO - root - 2017-12-07 07:43:56.787984: step 20190, loss = 0.82, batch loss = 0.75 (9.3 examples/sec; 0.860 sec/batch; 74h:34m:22s remains)
INFO - root - 2017-12-07 07:44:05.135224: step 20200, loss = 0.68, batch loss = 0.61 (9.3 examples/sec; 0.863 sec/batch; 74h:49m:40s remains)
2017-12-07 07:44:05.811034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.53456 -1.3296635 -1.3881099 -1.6127687 -1.5443122 -1.172091 -0.6244669 0.36746693 0.91153 0.77765703 0.10880852 -0.90996265 -1.6353226 -1.6951292 -1.4807384][-2.3812194 -2.3199675 -2.4619126 -2.6891446 -2.6581984 -2.316735 -1.7760086 -0.88013792 -0.2446332 0.097198009 0.18858957 -0.27145576 -0.967824 -1.4690757 -1.7510145][-2.8732319 -2.7137723 -2.6540289 -2.6107175 -2.4171228 -2.1414657 -1.8963397 -1.6145792 -1.4363387 -1.079447 -0.35259724 0.057144165 -0.075616837 -0.5419867 -1.1222014][-3.3424616 -3.0436203 -2.6717396 -2.2455778 -1.6350164 -1.028091 -0.69418859 -0.8007257 -1.3393016 -1.698101 -1.2527566 -0.44902968 0.062983036 0.14228201 -0.16103745][-3.5168982 -3.0711527 -2.4508786 -1.8454051 -1.0149119 -0.094011784 0.53421545 0.51867151 -0.39966011 -1.5738058 -1.9733715 -1.4483216 -0.65546441 -0.070481777 0.08743][-3.3896384 -2.7250819 -1.8320529 -1.0783639 -0.093551159 1.010406 1.959568 2.3028455 1.251029 -0.59025073 -1.9275572 -2.0953879 -1.5144036 -0.87653351 -0.56261826][-3.078207 -2.3809357 -1.5474927 -0.82967949 0.26346159 1.5736523 3.033567 4.0727148 3.2153897 1.0723686 -0.90202665 -1.7601483 -1.6046827 -1.2487786 -1.1321852][-2.3236859 -1.7217617 -1.2708197 -0.86567068 0.053770542 1.2171459 2.904458 4.5780172 4.3005953 2.5712266 0.60467291 -0.67216039 -0.95137572 -0.96265888 -1.0986941][-1.3011861 -0.80545521 -0.80003619 -0.72362614 -0.073568344 0.72713232 2.1826167 3.8547249 4.0774689 3.0254631 1.5499296 0.41577148 -0.059356689 -0.31767702 -0.68320322][-0.35905409 -0.07347393 -0.50307703 -0.73001719 -0.36017036 0.14002132 1.1202507 2.331378 2.7649689 2.1987724 1.4478936 1.0340214 0.93330479 0.76931858 0.28826284][0.49315166 0.68792343 0.036462307 -0.4745729 -0.51335025 -0.33450317 0.1344533 0.82471132 1.2561297 0.90633106 0.61939907 0.83281708 1.1704626 1.237699 0.81646681][0.65230942 0.934587 0.41360235 -0.16318178 -0.4708364 -0.50275946 -0.4205637 -0.16435862 0.13137865 -0.076043606 -0.14016247 0.3406167 0.88311338 0.98680735 0.48555851][0.33713102 0.80469704 0.69266939 0.35069418 0.015187263 -0.21339655 -0.45721793 -0.6013341 -0.61812997 -0.8001833 -0.71862078 -0.069532394 0.63374853 0.705811 0.11045456][-0.10080671 0.50572681 0.83062696 0.8955698 0.80631351 0.55987644 0.1837821 -0.24542093 -0.65928459 -0.98410821 -1.0300272 -0.51823545 0.17714643 0.29749441 -0.183002][-0.76064515 -0.24240685 0.24136782 0.63331127 0.88957691 0.79954481 0.52574921 0.084314346 -0.47467971 -0.81787515 -0.94167066 -0.66143608 -0.17543888 -0.1239357 -0.54025126]]...]
INFO - root - 2017-12-07 07:44:14.229223: step 20210, loss = 0.93, batch loss = 0.86 (9.5 examples/sec; 0.839 sec/batch; 72h:48m:29s remains)
INFO - root - 2017-12-07 07:44:22.655195: step 20220, loss = 0.80, batch loss = 0.73 (9.4 examples/sec; 0.853 sec/batch; 74h:01m:46s remains)
INFO - root - 2017-12-07 07:44:30.969545: step 20230, loss = 0.70, batch loss = 0.63 (9.5 examples/sec; 0.843 sec/batch; 73h:07m:07s remains)
INFO - root - 2017-12-07 07:44:39.326458: step 20240, loss = 0.78, batch loss = 0.71 (10.0 examples/sec; 0.802 sec/batch; 69h:36m:08s remains)
INFO - root - 2017-12-07 07:44:47.776992: step 20250, loss = 0.69, batch loss = 0.62 (10.1 examples/sec; 0.794 sec/batch; 68h:52m:29s remains)
INFO - root - 2017-12-07 07:44:56.229890: step 20260, loss = 0.88, batch loss = 0.80 (8.9 examples/sec; 0.897 sec/batch; 77h:49m:53s remains)
INFO - root - 2017-12-07 07:45:04.558506: step 20270, loss = 0.81, batch loss = 0.74 (9.7 examples/sec; 0.827 sec/batch; 71h:42m:23s remains)
INFO - root - 2017-12-07 07:45:12.943627: step 20280, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 0.837 sec/batch; 72h:35m:13s remains)
INFO - root - 2017-12-07 07:45:21.273181: step 20290, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.810 sec/batch; 70h:15m:56s remains)
INFO - root - 2017-12-07 07:45:29.645005: step 20300, loss = 0.96, batch loss = 0.89 (9.4 examples/sec; 0.853 sec/batch; 73h:56m:09s remains)
2017-12-07 07:45:30.342389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9711466 -2.8559465 -2.7447419 -2.800395 -2.9255013 -2.8808045 -2.76327 -2.7138591 -2.7424459 -2.8700857 -2.964622 -2.8925381 -2.6448438 -2.2532063 -2.003603][-2.9749374 -2.878674 -2.7932944 -2.8751063 -3.0810409 -3.2193928 -3.2909544 -3.3261938 -3.3638153 -3.439992 -3.4526865 -3.3383713 -3.0510113 -2.5618711 -2.2088516][-2.8979616 -2.8459973 -2.7749093 -2.8292336 -3.0388336 -3.2505932 -3.41157 -3.487731 -3.52391 -3.572711 -3.5851886 -3.526989 -3.2951186 -2.8069644 -2.4106328][-2.9529705 -2.9909158 -2.9563866 -2.9920321 -3.1768625 -3.3688617 -3.4853125 -3.4741948 -3.4073069 -3.3760343 -3.4028726 -3.4334142 -3.3107743 -2.9131646 -2.5398002][-2.9468222 -3.0734255 -3.0721798 -3.078311 -3.2187791 -3.3706684 -3.4388554 -3.3423915 -3.1697872 -3.0710211 -3.1249654 -3.2326145 -3.2023449 -2.8912349 -2.5293112][-2.9262881 -3.1668608 -3.1686082 -3.0636907 -3.0152545 -2.9888365 -2.9171915 -2.7002487 -2.4397979 -2.3395443 -2.5029707 -2.773634 -2.9162021 -2.77691 -2.4965935][-2.9451365 -3.2415662 -3.216383 -2.9705744 -2.6739635 -2.4257503 -2.1909895 -1.834033 -1.424099 -1.2163382 -1.4016888 -1.8315701 -2.2302904 -2.4246125 -2.4086475][-2.8319466 -3.079968 -3.034019 -2.7367425 -2.3308167 -2.0348783 -1.8033791 -1.4404492 -0.94225788 -0.57388639 -0.63495922 -1.0255654 -1.5174124 -1.9411848 -2.1535945][-2.9437027 -3.102232 -3.0618615 -2.8079443 -2.4326315 -2.2061913 -2.0584784 -1.7946091 -1.3466802 -0.94449997 -0.90708089 -1.1265347 -1.4839141 -1.8940849 -2.1598685][-3.1690843 -3.2340798 -3.2022357 -3.0088599 -2.7246132 -2.5921235 -2.5036314 -2.331181 -1.980531 -1.6386428 -1.5603449 -1.6142478 -1.7949033 -2.0990679 -2.3413308][-3.0127718 -3.0225716 -3.0195389 -2.9138155 -2.78469 -2.8050337 -2.8368416 -2.7786169 -2.5240917 -2.2361009 -2.1203449 -2.0589993 -2.0958989 -2.2567425 -2.4169574][-2.7466168 -2.7462533 -2.7573857 -2.6952672 -2.6776462 -2.8096416 -2.9563568 -3.0287557 -2.9018869 -2.7215891 -2.6437011 -2.5840173 -2.5883169 -2.64575 -2.6965971][-2.6847353 -2.673646 -2.6660624 -2.6081793 -2.6305461 -2.774431 -2.9454937 -3.0881481 -3.0810652 -3.0238619 -3.0148115 -3.0140927 -3.049763 -3.0840812 -3.0875573][-2.7645273 -2.7535214 -2.7422419 -2.7002535 -2.7417884 -2.8568964 -2.9969106 -3.1552863 -3.2348723 -3.2714872 -3.2917681 -3.2947176 -3.3169508 -3.3271067 -3.2950091][-2.7699919 -2.7638912 -2.7549973 -2.7285242 -2.7741446 -2.862668 -2.9776382 -3.1341565 -3.270947 -3.3913672 -3.4676206 -3.5056388 -3.5352144 -3.5473092 -3.4891953]]...]
INFO - root - 2017-12-07 07:45:38.800496: step 20310, loss = 0.91, batch loss = 0.84 (9.3 examples/sec; 0.861 sec/batch; 74h:37m:53s remains)
INFO - root - 2017-12-07 07:45:47.104226: step 20320, loss = 0.73, batch loss = 0.66 (9.3 examples/sec; 0.865 sec/batch; 74h:57m:59s remains)
INFO - root - 2017-12-07 07:45:55.531408: step 20330, loss = 0.54, batch loss = 0.47 (9.4 examples/sec; 0.847 sec/batch; 73h:28m:27s remains)
INFO - root - 2017-12-07 07:46:03.952682: step 20340, loss = 0.70, batch loss = 0.63 (9.3 examples/sec; 0.861 sec/batch; 74h:38m:05s remains)
INFO - root - 2017-12-07 07:46:12.168488: step 20350, loss = 0.66, batch loss = 0.59 (10.3 examples/sec; 0.780 sec/batch; 67h:37m:44s remains)
INFO - root - 2017-12-07 07:46:20.433618: step 20360, loss = 0.79, batch loss = 0.72 (9.3 examples/sec; 0.863 sec/batch; 74h:49m:01s remains)
INFO - root - 2017-12-07 07:46:28.866216: step 20370, loss = 0.66, batch loss = 0.59 (8.9 examples/sec; 0.894 sec/batch; 77h:32m:40s remains)
INFO - root - 2017-12-07 07:46:37.187603: step 20380, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.818 sec/batch; 70h:54m:27s remains)
INFO - root - 2017-12-07 07:46:45.565746: step 20390, loss = 0.62, batch loss = 0.54 (9.8 examples/sec; 0.815 sec/batch; 70h:41m:31s remains)
INFO - root - 2017-12-07 07:46:53.914765: step 20400, loss = 0.79, batch loss = 0.71 (9.4 examples/sec; 0.849 sec/batch; 73h:35m:22s remains)
2017-12-07 07:46:54.594492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6662197 -1.8546546 -2.1449683 -2.5428061 -3.1346402 -3.7543766 -4.1928535 -4.32303 -4.2634988 -4.1035056 -3.9092398 -3.7969267 -3.6884613 -3.6090326 -3.65437][-1.6360209 -1.8103757 -2.0543098 -2.4034877 -2.9946966 -3.6385164 -4.1230707 -4.3235803 -4.3481932 -4.2739048 -4.1149158 -3.9551873 -3.7378294 -3.5386307 -3.5175512][-1.6641517 -1.8384948 -2.0482337 -2.3442962 -2.8913634 -3.5003672 -3.9755411 -4.205296 -4.3078933 -4.3107009 -4.169704 -3.9697318 -3.6507578 -3.3217587 -3.2326617][-1.7416527 -1.9154911 -2.1102912 -2.3573127 -2.8172312 -3.3426578 -3.7658145 -3.9690573 -4.0679579 -4.0517612 -3.8953576 -3.7247646 -3.4442353 -3.1206846 -3.0078487][-2.0446429 -2.233686 -2.4075475 -2.5876193 -2.8890491 -3.2354703 -3.5219834 -3.6479549 -3.69989 -3.6273828 -3.4465852 -3.3344595 -3.1725202 -2.9445772 -2.8475237][-2.3561413 -2.55227 -2.7018557 -2.8381419 -3.0065804 -3.1655483 -3.2701416 -3.2748084 -3.2612281 -3.152389 -3.0045919 -2.9857271 -2.9314246 -2.7829127 -2.6766884][-2.3406746 -2.5430398 -2.7019854 -2.8524716 -2.9795098 -3.0312228 -2.9928205 -2.8917341 -2.8393664 -2.7709832 -2.737963 -2.8449807 -2.8738456 -2.7507653 -2.5810213][-2.02824 -2.2169402 -2.3989913 -2.5869684 -2.7366006 -2.7992277 -2.7619758 -2.6752603 -2.6251159 -2.5602732 -2.5639095 -2.6876798 -2.7101221 -2.5670435 -2.3552098][-1.6165431 -1.7561436 -1.9207702 -2.0994234 -2.2674212 -2.4052722 -2.4981322 -2.558877 -2.5890996 -2.5250964 -2.4686375 -2.4661679 -2.3828738 -2.1933143 -1.9769146][-1.2964847 -1.3750188 -1.5028143 -1.6316371 -1.7490423 -1.8794007 -2.0256481 -2.1798062 -2.295387 -2.3072731 -2.2787743 -2.2193992 -2.0599515 -1.8335807 -1.6136751][-1.2306154 -1.2323427 -1.3010137 -1.3626897 -1.3995974 -1.4578674 -1.5476518 -1.6613367 -1.7633171 -1.8250592 -1.8694482 -1.8519256 -1.7200763 -1.5194831 -1.3191121][-1.1617587 -1.0996554 -1.1050198 -1.1040893 -1.089262 -1.1050518 -1.1449642 -1.1923554 -1.2495012 -1.3174417 -1.3918481 -1.4030926 -1.3115332 -1.161942 -1.0013602][-0.97308826 -0.87703657 -0.83537722 -0.79096937 -0.74934387 -0.74700713 -0.76758575 -0.78259015 -0.81662035 -0.87966681 -0.95179248 -0.96549344 -0.89069247 -0.77472234 -0.65424442][-0.89851689 -0.78189874 -0.70185137 -0.62690258 -0.56790066 -0.54982591 -0.54644752 -0.53007579 -0.53852391 -0.58048105 -0.62461376 -0.61814427 -0.54714346 -0.45967674 -0.38696432][-0.86562586 -0.74978733 -0.6545639 -0.570415 -0.50740886 -0.47263622 -0.43412328 -0.37491417 -0.34056807 -0.34571218 -0.35792542 -0.33577776 -0.27121735 -0.2080884 -0.179667]]...]
INFO - root - 2017-12-07 07:47:03.126619: step 20410, loss = 0.74, batch loss = 0.67 (9.0 examples/sec; 0.890 sec/batch; 77h:11m:37s remains)
INFO - root - 2017-12-07 07:47:11.533278: step 20420, loss = 0.95, batch loss = 0.87 (9.8 examples/sec; 0.815 sec/batch; 70h:40m:26s remains)
INFO - root - 2017-12-07 07:47:20.066549: step 20430, loss = 0.72, batch loss = 0.65 (9.4 examples/sec; 0.855 sec/batch; 74h:04m:57s remains)
INFO - root - 2017-12-07 07:47:28.208862: step 20440, loss = 0.59, batch loss = 0.52 (10.7 examples/sec; 0.746 sec/batch; 64h:41m:53s remains)
INFO - root - 2017-12-07 07:47:36.519466: step 20450, loss = 0.56, batch loss = 0.48 (9.1 examples/sec; 0.880 sec/batch; 76h:18m:48s remains)
INFO - root - 2017-12-07 07:47:44.827711: step 20460, loss = 0.67, batch loss = 0.59 (10.3 examples/sec; 0.779 sec/batch; 67h:31m:48s remains)
INFO - root - 2017-12-07 07:47:53.246096: step 20470, loss = 0.68, batch loss = 0.60 (9.5 examples/sec; 0.840 sec/batch; 72h:49m:20s remains)
INFO - root - 2017-12-07 07:48:01.753149: step 20480, loss = 0.89, batch loss = 0.82 (9.1 examples/sec; 0.882 sec/batch; 76h:24m:06s remains)
INFO - root - 2017-12-07 07:48:10.175148: step 20490, loss = 0.86, batch loss = 0.78 (9.7 examples/sec; 0.822 sec/batch; 71h:12m:35s remains)
INFO - root - 2017-12-07 07:48:18.609644: step 20500, loss = 0.79, batch loss = 0.72 (8.9 examples/sec; 0.897 sec/batch; 77h:42m:42s remains)
2017-12-07 07:48:19.310829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4396367 -1.6625388 -1.9880779 -1.7783873 -1.1935592 -0.44164944 0.041936874 -0.61692452 -1.5436709 -1.9873078 -2.1224439 -1.8794975 -1.5489383 -1.4790037 -1.3396823][-1.3955612 -1.6063929 -2.0777633 -2.0881083 -1.6793566 -0.99429417 -0.23564482 -0.46945667 -1.2165315 -1.6197019 -1.8373444 -1.6827424 -1.3705096 -1.1861916 -0.87215662][-0.94502091 -1.11728 -1.730706 -2.0458972 -1.9677863 -1.5353127 -0.66298437 -0.53659987 -1.0172677 -1.2784648 -1.4816363 -1.45347 -1.2768888 -1.1184018 -0.75329041][-0.66939425 -0.60004139 -1.2563109 -1.8557196 -2.1646698 -2.1313944 -1.5017459 -1.396765 -1.7550843 -1.8450789 -1.9617093 -1.9678056 -1.8792293 -1.7125332 -1.3259358][-0.38228893 -0.16888571 -0.87394643 -1.7445114 -2.4397206 -2.8458903 -2.6501343 -2.7680001 -3.0688782 -2.9732404 -2.9177649 -2.8318141 -2.7430449 -2.5602746 -2.1769159][-0.15544033 -0.12326002 -0.84280157 -1.7336144 -2.5320768 -3.1377592 -3.18875 -3.3755379 -3.5456431 -3.3240061 -3.1710086 -3.0376124 -3.0328057 -2.9149044 -2.6131258][-0.46302938 -0.68528152 -1.1976213 -1.669811 -2.0353725 -2.2932751 -2.1589344 -2.111721 -2.0610657 -1.8813212 -1.8670876 -1.9600832 -2.2251503 -2.2689826 -2.135385][-1.2580061 -1.3765507 -1.2838585 -1.0006287 -0.65096021 -0.24696779 0.22997332 0.39537191 0.415452 0.34777546 -0.02596283 -0.59843683 -1.1943371 -1.3528328 -1.3352525][-1.6607149 -1.3900244 -0.78151751 -0.22637701 0.26357603 0.83055353 1.2273154 1.1697507 1.0038471 0.74394083 0.098983288 -0.71329355 -1.2532771 -1.1483436 -1.0123465][-1.4580724 -0.99541521 -0.36589336 -0.23741817 -0.33683014 -0.23279619 -0.25275612 -0.57621241 -0.71824288 -0.775857 -1.1658273 -1.6485276 -1.7673082 -1.366529 -1.2156076][-1.2852542 -0.89282537 -0.46984696 -0.83071446 -1.405813 -1.6066146 -1.8132646 -2.1846054 -2.2176716 -2.0014772 -1.9653783 -2.0217469 -1.9017489 -1.5683675 -1.6590366][-1.3318627 -1.0119534 -0.67578053 -1.2104168 -1.9125686 -2.1340158 -2.3386965 -2.671309 -2.6140323 -2.2001908 -1.8224626 -1.6520114 -1.5647972 -1.4044435 -1.6769891][-1.3046362 -1.0240834 -0.7200861 -1.1999638 -1.8041658 -1.8932152 -1.9059198 -2.0570989 -1.9176254 -1.4981322 -1.0857534 -1.015027 -1.1631942 -1.102052 -1.3759682][-1.4824636 -1.1826105 -0.77522683 -1.0243733 -1.3796737 -1.2683637 -1.1383688 -1.2577348 -1.288909 -1.1594529 -0.95700765 -1.060787 -1.3260155 -1.2417848 -1.4750845][-1.8545337 -1.5396228 -1.034847 -1.0963407 -1.2869983 -1.0738533 -0.93825078 -1.2011461 -1.5216296 -1.7094378 -1.6130593 -1.5851667 -1.6157007 -1.3423529 -1.5596359]]...]
INFO - root - 2017-12-07 07:48:27.741819: step 20510, loss = 0.84, batch loss = 0.77 (9.6 examples/sec; 0.835 sec/batch; 72h:21m:22s remains)
INFO - root - 2017-12-07 07:48:35.964766: step 20520, loss = 0.69, batch loss = 0.62 (10.3 examples/sec; 0.775 sec/batch; 67h:08m:12s remains)
INFO - root - 2017-12-07 07:48:44.476080: step 20530, loss = 0.80, batch loss = 0.73 (9.2 examples/sec; 0.873 sec/batch; 75h:38m:23s remains)
INFO - root - 2017-12-07 07:48:52.846354: step 20540, loss = 0.77, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 74h:03m:22s remains)
INFO - root - 2017-12-07 07:49:01.302274: step 20550, loss = 0.58, batch loss = 0.51 (10.0 examples/sec; 0.803 sec/batch; 69h:37m:01s remains)
INFO - root - 2017-12-07 07:49:09.646380: step 20560, loss = 0.91, batch loss = 0.84 (9.8 examples/sec; 0.815 sec/batch; 70h:39m:04s remains)
INFO - root - 2017-12-07 07:49:18.039953: step 20570, loss = 0.70, batch loss = 0.63 (9.5 examples/sec; 0.842 sec/batch; 72h:59m:20s remains)
INFO - root - 2017-12-07 07:49:26.528854: step 20580, loss = 0.98, batch loss = 0.90 (9.3 examples/sec; 0.861 sec/batch; 74h:34m:43s remains)
INFO - root - 2017-12-07 07:49:34.858742: step 20590, loss = 0.82, batch loss = 0.75 (9.3 examples/sec; 0.856 sec/batch; 74h:07m:56s remains)
INFO - root - 2017-12-07 07:49:43.207904: step 20600, loss = 0.80, batch loss = 0.73 (9.4 examples/sec; 0.848 sec/batch; 73h:26m:37s remains)
2017-12-07 07:49:43.833767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2928329 -4.4405479 -4.3727264 -4.2019367 -3.9800642 -3.7520232 -3.7627234 -3.9204473 -4.014101 -4.124063 -4.1434083 -3.9495375 -3.6904819 -3.6191344 -3.7407737][-4.8202558 -4.8678551 -4.7603216 -4.6155691 -4.4807372 -4.2905111 -4.1890106 -4.1637487 -4.1039491 -4.1257243 -4.1605124 -4.071394 -3.9198232 -3.8126838 -3.8862152][-4.7556067 -4.7701025 -4.7791557 -4.8675122 -5.0211291 -5.0505476 -4.9014874 -4.6944256 -4.5039296 -4.4305348 -4.4519625 -4.3858886 -4.2013063 -3.9339318 -3.8774362][-3.8798196 -3.8847966 -4.0822477 -4.4646583 -4.894289 -5.1344275 -4.9943271 -4.7001772 -4.4953442 -4.440917 -4.5146031 -4.4840493 -4.2418203 -3.8477597 -3.7257633][-2.4040811 -2.3968987 -2.7444634 -3.312731 -3.8515129 -4.164607 -4.0582566 -3.8193574 -3.7534192 -3.878288 -4.0951023 -4.1541271 -3.9462752 -3.5757275 -3.4798908][-1.1854858 -1.2832897 -1.7622764 -2.3148153 -2.632216 -2.6964695 -2.4796982 -2.2975173 -2.4410899 -2.8624079 -3.3051004 -3.5106316 -3.4260178 -3.233139 -3.2551069][-1.034972 -1.3827062 -1.9275548 -2.2388406 -2.129494 -1.7836649 -1.3636448 -1.1638508 -1.4207423 -2.0224085 -2.6032364 -2.9220853 -2.9713359 -2.9758615 -3.0997267][-1.8468463 -2.2645128 -2.6588695 -2.6583624 -2.2995942 -1.8735404 -1.5450885 -1.4046004 -1.5698407 -1.9887531 -2.4225724 -2.7048826 -2.7981696 -2.8707335 -2.9563427][-2.6170921 -2.9233694 -3.1309085 -2.9726872 -2.6912413 -2.5434551 -2.493784 -2.379452 -2.2951298 -2.3445568 -2.4753923 -2.6410809 -2.7515392 -2.8175468 -2.7857885][-2.682653 -2.8248994 -2.8936291 -2.7545185 -2.7347674 -2.9348257 -3.1418819 -3.0740643 -2.840301 -2.6535225 -2.5344303 -2.5357275 -2.6021667 -2.6313186 -2.5313058][-2.1681585 -2.202734 -2.1781349 -2.1283956 -2.3550622 -2.7769325 -3.1367271 -3.1087317 -2.8360624 -2.5928316 -2.3720496 -2.2344599 -2.191196 -2.1448755 -2.015918][-1.6758304 -1.700983 -1.6520817 -1.6764243 -1.9931853 -2.3985572 -2.6935527 -2.6024895 -2.3371873 -2.1485562 -1.9642508 -1.7878115 -1.6776104 -1.6136153 -1.5123641][-1.4783893 -1.5606551 -1.5463769 -1.6239085 -1.9065368 -2.1460695 -2.2236574 -1.9871614 -1.7129962 -1.5816419 -1.5073264 -1.4246163 -1.3672855 -1.3732712 -1.3285589][-1.4354582 -1.5227146 -1.5371268 -1.6273661 -1.8388791 -1.9606578 -1.9078259 -1.6349719 -1.3934703 -1.2930217 -1.3148382 -1.349735 -1.4017322 -1.5337801 -1.552983][-1.4352689 -1.4547849 -1.4525414 -1.4965785 -1.6079667 -1.6636984 -1.5950422 -1.4328032 -1.3262806 -1.3021936 -1.3834748 -1.4547119 -1.5756936 -1.798352 -1.8351309]]...]
INFO - root - 2017-12-07 07:49:52.209488: step 20610, loss = 0.92, batch loss = 0.85 (9.1 examples/sec; 0.878 sec/batch; 76h:05m:48s remains)
INFO - root - 2017-12-07 07:50:00.599106: step 20620, loss = 0.91, batch loss = 0.83 (9.5 examples/sec; 0.838 sec/batch; 72h:37m:41s remains)
INFO - root - 2017-12-07 07:50:09.037647: step 20630, loss = 0.86, batch loss = 0.79 (9.4 examples/sec; 0.848 sec/batch; 73h:29m:50s remains)
INFO - root - 2017-12-07 07:50:17.361783: step 20640, loss = 0.65, batch loss = 0.58 (9.5 examples/sec; 0.838 sec/batch; 72h:38m:05s remains)
INFO - root - 2017-12-07 07:50:25.713210: step 20650, loss = 0.88, batch loss = 0.81 (9.9 examples/sec; 0.812 sec/batch; 70h:19m:05s remains)
INFO - root - 2017-12-07 07:50:34.092930: step 20660, loss = 0.99, batch loss = 0.91 (9.6 examples/sec; 0.836 sec/batch; 72h:25m:50s remains)
INFO - root - 2017-12-07 07:50:42.359640: step 20670, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.795 sec/batch; 68h:53m:03s remains)
INFO - root - 2017-12-07 07:50:50.729052: step 20680, loss = 0.83, batch loss = 0.76 (9.3 examples/sec; 0.857 sec/batch; 74h:14m:02s remains)
INFO - root - 2017-12-07 07:50:59.074153: step 20690, loss = 0.67, batch loss = 0.60 (9.5 examples/sec; 0.846 sec/batch; 73h:15m:42s remains)
INFO - root - 2017-12-07 07:51:07.470675: step 20700, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.814 sec/batch; 70h:31m:40s remains)
2017-12-07 07:51:08.150589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9041345 -2.7359586 -2.7019153 -2.7279634 -2.8641496 -3.1106195 -3.3541636 -3.5359566 -3.5824373 -3.4492128 -3.2704873 -3.2078547 -3.3164 -3.4646542 -3.3832831][-2.8288727 -2.7111392 -2.76016 -2.8355846 -3.0126202 -3.3163707 -3.6215487 -3.8562689 -3.9340789 -3.8052671 -3.6161308 -3.5178313 -3.5379431 -3.6398573 -3.6313529][-2.9700561 -2.8139343 -2.8243089 -2.8514779 -2.9940419 -3.3193665 -3.6952431 -3.9976561 -4.1481671 -4.0840778 -3.9126058 -3.7901688 -3.718482 -3.7291381 -3.7222064][-3.2459641 -3.0125427 -2.9046335 -2.7534542 -2.7147946 -2.9498854 -3.310807 -3.6344192 -3.9083798 -4.0064039 -3.9455905 -3.8598218 -3.7364583 -3.6476007 -3.5684452][-3.4246178 -3.0376847 -2.701983 -2.2349715 -1.9300973 -2.0609879 -2.3911276 -2.6953709 -3.085875 -3.396409 -3.5504162 -3.6092184 -3.5552936 -3.4656129 -3.3475008][-3.4119263 -2.7984056 -2.1314983 -1.2577629 -0.66535187 -0.73188877 -1.0439734 -1.3262672 -1.8339503 -2.3833025 -2.8591142 -3.1792066 -3.3156142 -3.3641319 -3.272965][-3.2484469 -2.4926915 -1.5884035 -0.40677261 0.39628839 0.4195137 0.16466951 -0.137609 -0.75742483 -1.5317087 -2.3376639 -2.9032602 -3.224359 -3.4457936 -3.4058836][-3.1373625 -2.4105322 -1.4821887 -0.23075008 0.61637211 0.67330551 0.46075869 0.1003561 -0.57888913 -1.4123907 -2.3186052 -2.9282455 -3.2771707 -3.5810235 -3.5723982][-3.1475716 -2.5595541 -1.7956696 -0.74820566 -0.096885681 -0.14601135 -0.4074645 -0.79340148 -1.3251741 -1.8771365 -2.5312009 -2.9568186 -3.1958456 -3.4965742 -3.5354259][-3.2280207 -2.7816432 -2.2693288 -1.5626571 -1.2036271 -1.4483297 -1.8118587 -2.1323466 -2.3512552 -2.4676478 -2.7392325 -2.9260883 -3.0312514 -3.2907915 -3.3715887][-3.3620574 -3.0596538 -2.7757583 -2.3453908 -2.1512136 -2.4501259 -2.7772455 -2.957895 -2.9635105 -2.8735228 -2.9451833 -3.00392 -3.0497515 -3.286727 -3.3931208][-3.4984398 -3.3658657 -3.2747569 -3.0103202 -2.8392806 -3.0491362 -3.2014694 -3.2267408 -3.212621 -3.1919522 -3.2529635 -3.2540488 -3.2494648 -3.4092894 -3.4864635][-3.6132011 -3.6233015 -3.703284 -3.571398 -3.3987827 -3.4882092 -3.468488 -3.4247932 -3.5283494 -3.6770234 -3.7640033 -3.7266774 -3.6476655 -3.6214333 -3.5304909][-3.6576462 -3.7293069 -3.9112978 -3.879421 -3.7097349 -3.7057028 -3.5889528 -3.5353878 -3.7540312 -4.0267863 -4.1319833 -4.076323 -3.9522347 -3.7765148 -3.5614655][-3.6004913 -3.6596105 -3.8854194 -3.9297831 -3.7786107 -3.7282143 -3.5725622 -3.5015516 -3.7156219 -3.9506748 -3.9664044 -3.8291626 -3.6428163 -3.3818824 -3.160542]]...]
INFO - root - 2017-12-07 07:51:16.416875: step 20710, loss = 0.87, batch loss = 0.79 (9.5 examples/sec; 0.841 sec/batch; 72h:48m:45s remains)
INFO - root - 2017-12-07 07:51:24.784696: step 20720, loss = 0.59, batch loss = 0.52 (9.5 examples/sec; 0.839 sec/batch; 72h:40m:36s remains)
INFO - root - 2017-12-07 07:51:33.156914: step 20730, loss = 0.76, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:42m:28s remains)
INFO - root - 2017-12-07 07:51:41.432387: step 20740, loss = 0.83, batch loss = 0.76 (9.8 examples/sec; 0.818 sec/batch; 70h:51m:21s remains)
INFO - root - 2017-12-07 07:51:49.852724: step 20750, loss = 0.56, batch loss = 0.49 (9.6 examples/sec; 0.835 sec/batch; 72h:17m:13s remains)
INFO - root - 2017-12-07 07:51:57.698117: step 20760, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.795 sec/batch; 68h:50m:56s remains)
INFO - root - 2017-12-07 07:52:06.157130: step 20770, loss = 0.80, batch loss = 0.73 (9.0 examples/sec; 0.887 sec/batch; 76h:50m:31s remains)
INFO - root - 2017-12-07 07:52:14.477182: step 20780, loss = 0.76, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:28m:40s remains)
INFO - root - 2017-12-07 07:52:22.746379: step 20790, loss = 0.85, batch loss = 0.78 (9.7 examples/sec; 0.821 sec/batch; 71h:07m:44s remains)
INFO - root - 2017-12-07 07:52:31.187980: step 20800, loss = 0.66, batch loss = 0.58 (9.3 examples/sec; 0.856 sec/batch; 74h:07m:52s remains)
2017-12-07 07:52:31.801992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.6775558 -1.1812971 -1.2278547 -0.46915412 0.32759666 0.47498322 0.21580362 0.024013519 -0.28195095 -0.68185854 -0.85629439 -0.87362385 -0.56193209 0.031420708 0.45323133][-1.9251466 -2.1096971 -1.7276809 -0.65077257 0.28208828 0.38511324 0.11162376 -0.0027532578 -0.26019955 -0.6830759 -1.0066745 -1.2330422 -0.99539566 -0.2189846 0.4435358][-2.9765768 -2.9793186 -2.4069765 -1.182075 -0.13674545 -0.063225269 -0.32027769 -0.27116871 -0.36464787 -0.70426154 -1.0883312 -1.5129747 -1.4296114 -0.66214371 0.09361887][-3.1452127 -3.1325507 -2.6665134 -1.5329933 -0.49951673 -0.49020052 -0.74968553 -0.54753041 -0.438843 -0.60593057 -0.95338535 -1.5497615 -1.6865849 -1.0883541 -0.41467237][-2.5225883 -2.5832505 -2.3938785 -1.5572629 -0.62931037 -0.59162211 -0.80837035 -0.54831743 -0.34781122 -0.43642879 -0.80761456 -1.5332565 -1.8111732 -1.3948917 -0.936959][-1.5218687 -1.7065809 -1.8517358 -1.4044073 -0.57737708 -0.32001638 -0.36381149 -0.092060566 0.10075808 -0.055634022 -0.5591116 -1.3809736 -1.7277191 -1.4731081 -1.276969][-0.669436 -0.93679547 -1.4066927 -1.3338764 -0.56654596 0.052423477 0.32444239 0.68946171 0.91738081 0.65960121 -0.022444248 -0.90255594 -1.2829289 -1.1392097 -1.1293087][-0.23704338 -0.52503991 -1.191278 -1.3570647 -0.64681911 0.22654819 0.74704552 1.2488441 1.629498 1.3710427 0.53693151 -0.39330864 -0.81351995 -0.7593503 -0.8692975][0.036567688 -0.21471882 -0.910445 -1.2163477 -0.6913991 0.16972733 0.763783 1.3430157 1.8865819 1.7169452 0.84485817 -0.010340691 -0.37217045 -0.38821173 -0.64221215][0.22667027 0.046794415 -0.59043312 -0.96579218 -0.67565393 -0.019034386 0.51178312 1.065732 1.6544209 1.6156898 0.88980436 0.34583092 0.26821804 0.28211737 -0.10510063][0.27183676 0.096318245 -0.49387026 -0.9026897 -0.77501345 -0.34688044 0.062451363 0.47888184 0.99700165 1.1318564 0.69079304 0.58236361 0.8657012 0.97088766 0.49610806][-0.017797947 -0.24919844 -0.81280684 -1.1949112 -1.0912337 -0.82834148 -0.56640887 -0.34612751 0.10007524 0.47138357 0.36095667 0.55997753 1.0092177 1.1039138 0.51221514][-0.696774 -0.9249444 -1.3220754 -1.5133622 -1.3437059 -1.229882 -1.1246066 -1.0358942 -0.64481807 -0.14208651 -0.07901907 0.18291664 0.56996822 0.579823 -0.062176704][-1.465786 -1.6074033 -1.7729816 -1.7249241 -1.494246 -1.4497497 -1.3808162 -1.3165524 -1.0162258 -0.58321953 -0.5272541 -0.39987993 -0.18938351 -0.22438335 -0.80830431][-2.2164795 -2.273042 -2.2443514 -2.00197 -1.7251711 -1.7257795 -1.6734831 -1.5968664 -1.344636 -1.002573 -0.9911983 -1.0465631 -1.0400944 -1.0955012 -1.5558934]]...]
INFO - root - 2017-12-07 07:52:40.223718: step 20810, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 0.901 sec/batch; 77h:59m:38s remains)
INFO - root - 2017-12-07 07:52:48.496342: step 20820, loss = 0.64, batch loss = 0.57 (9.3 examples/sec; 0.859 sec/batch; 74h:21m:21s remains)
INFO - root - 2017-12-07 07:52:56.803275: step 20830, loss = 0.50, batch loss = 0.43 (9.9 examples/sec; 0.810 sec/batch; 70h:08m:51s remains)
INFO - root - 2017-12-07 07:53:05.096678: step 20840, loss = 0.63, batch loss = 0.55 (9.6 examples/sec; 0.830 sec/batch; 71h:52m:03s remains)
INFO - root - 2017-12-07 07:53:13.387852: step 20850, loss = 0.52, batch loss = 0.44 (9.5 examples/sec; 0.839 sec/batch; 72h:38m:12s remains)
INFO - root - 2017-12-07 07:53:21.752684: step 20860, loss = 0.81, batch loss = 0.74 (9.2 examples/sec; 0.867 sec/batch; 75h:00m:59s remains)
INFO - root - 2017-12-07 07:53:30.180950: step 20870, loss = 0.53, batch loss = 0.45 (9.3 examples/sec; 0.863 sec/batch; 74h:43m:23s remains)
INFO - root - 2017-12-07 07:53:38.509711: step 20880, loss = 0.57, batch loss = 0.50 (9.8 examples/sec; 0.819 sec/batch; 70h:52m:41s remains)
INFO - root - 2017-12-07 07:53:46.870912: step 20890, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.826 sec/batch; 71h:29m:32s remains)
INFO - root - 2017-12-07 07:53:55.132543: step 20900, loss = 0.77, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:14m:55s remains)
2017-12-07 07:53:55.780822: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27844667 0.30008221 0.28593159 0.24466276 0.18486595 0.11976719 0.039243221 -0.088680744 -0.22769499 -0.35450411 -0.46210194 -0.55669188 -0.64909768 -0.72373891 -0.76720524][0.30162382 0.32095194 0.306077 0.26167059 0.197855 0.12737846 0.038280964 -0.099896908 -0.24608421 -0.37126637 -0.47212958 -0.56038 -0.64364123 -0.71050954 -0.758337][0.3348875 0.339818 0.30977631 0.24619722 0.1601758 0.066281319 -0.040365219 -0.18161726 -0.31256866 -0.4107213 -0.48556709 -0.55245876 -0.61933827 -0.68678546 -0.75859547][0.34405231 0.33160639 0.28228712 0.19468641 0.081666946 -0.043452263 -0.17789173 -0.32448483 -0.42833185 -0.48047686 -0.51433849 -0.55176759 -0.59913135 -0.67172766 -0.77323985][0.31006432 0.28163576 0.21252489 0.10027552 -0.03627634 -0.19685984 -0.36465025 -0.51227903 -0.57417846 -0.56101584 -0.54619789 -0.55284047 -0.58863735 -0.67295885 -0.79860497][0.24834728 0.20228004 0.11144209 -0.029730797 -0.20052528 -0.41555262 -0.62949634 -0.766731 -0.76226687 -0.66922283 -0.60255289 -0.58140874 -0.61068058 -0.70397496 -0.84416604][0.21036196 0.13866472 0.022254467 -0.15559483 -0.37329483 -0.65627956 -0.92261004 -1.0349615 -0.95474052 -0.79422808 -0.70194745 -0.67825389 -0.70721292 -0.795558 -0.93825126][0.18145704 0.076223373 -0.071706295 -0.2904048 -0.55992937 -0.90313196 -1.2053695 -1.2762213 -1.1303833 -0.95043635 -0.87153482 -0.8665216 -0.88667679 -0.949502 -1.0825729][0.14975166 0.019301891 -0.15051222 -0.39475679 -0.69634628 -1.0673575 -1.3649709 -1.3841383 -1.2087219 -1.0783858 -1.0533874 -1.0676689 -1.0743775 -1.113281 -1.2404206][0.11711121 -0.024452209 -0.19477415 -0.43383861 -0.72630882 -1.0748374 -1.3227077 -1.300863 -1.1606691 -1.1350851 -1.1827488 -1.2046847 -1.2061968 -1.2469966 -1.3799436][0.088546276 -0.052148342 -0.22016287 -0.44975853 -0.71707106 -1.01105 -1.2062287 -1.1961026 -1.142792 -1.2089748 -1.2959137 -1.3200493 -1.3312306 -1.3879912 -1.5242956][0.085128784 -0.0500803 -0.21975994 -0.44575191 -0.67941022 -0.91057205 -1.072495 -1.1166511 -1.1586096 -1.2577236 -1.3414049 -1.3819778 -1.4268913 -1.5076296 -1.6406775][0.094358444 -0.035859108 -0.20755243 -0.42543888 -0.62858391 -0.81897473 -0.96614742 -1.0527129 -1.1416013 -1.2347364 -1.3029695 -1.3697517 -1.4557188 -1.5592175 -1.684833][0.1124258 0.00028848648 -0.15563774 -0.35068798 -0.52563214 -0.69763064 -0.83911967 -0.9376328 -1.0421484 -1.1467376 -1.2364709 -1.3386955 -1.461005 -1.580689 -1.6861498][0.13949203 0.059080124 -0.067090034 -0.23662806 -0.38915396 -0.54608917 -0.68334484 -0.7872057 -0.9087 -1.0467203 -1.1819398 -1.3209639 -1.4677417 -1.5900402 -1.6639676]]...]
INFO - root - 2017-12-07 07:54:04.072006: step 20910, loss = 0.60, batch loss = 0.52 (9.1 examples/sec; 0.880 sec/batch; 76h:10m:15s remains)
INFO - root - 2017-12-07 07:54:12.365009: step 20920, loss = 0.98, batch loss = 0.91 (9.8 examples/sec; 0.816 sec/batch; 70h:37m:15s remains)
INFO - root - 2017-12-07 07:54:20.701490: step 20930, loss = 0.78, batch loss = 0.71 (9.7 examples/sec; 0.823 sec/batch; 71h:15m:50s remains)
INFO - root - 2017-12-07 07:54:29.102826: step 20940, loss = 0.77, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 73h:35m:18s remains)
INFO - root - 2017-12-07 07:54:37.524510: step 20950, loss = 0.81, batch loss = 0.74 (9.3 examples/sec; 0.856 sec/batch; 74h:05m:08s remains)
INFO - root - 2017-12-07 07:54:45.986196: step 20960, loss = 0.92, batch loss = 0.85 (9.0 examples/sec; 0.893 sec/batch; 77h:19m:07s remains)
INFO - root - 2017-12-07 07:54:54.345128: step 20970, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.797 sec/batch; 68h:58m:22s remains)
INFO - root - 2017-12-07 07:55:02.727943: step 20980, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.815 sec/batch; 70h:30m:01s remains)
INFO - root - 2017-12-07 07:55:11.034571: step 20990, loss = 0.89, batch loss = 0.82 (9.4 examples/sec; 0.855 sec/batch; 73h:56m:51s remains)
INFO - root - 2017-12-07 07:55:19.499527: step 21000, loss = 0.88, batch loss = 0.81 (9.7 examples/sec; 0.826 sec/batch; 71h:29m:01s remains)
2017-12-07 07:55:20.206436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2332244 -3.3285482 -3.4524503 -3.5748305 -3.5488012 -3.2158132 -2.857337 -2.746206 -2.9525838 -3.4750075 -3.9507644 -3.9882276 -3.6602335 -3.3463006 -3.4337761][-3.4206018 -3.4919317 -3.5734425 -3.6660213 -3.4492002 -2.794518 -2.4181278 -2.6890852 -3.3129206 -4.0678611 -4.4756293 -4.199532 -3.5769677 -3.1483674 -3.2998347][-3.6922688 -3.7728033 -3.9016323 -3.9727917 -3.4298062 -2.2560055 -1.6674099 -2.2231612 -3.4054909 -4.56837 -4.9528575 -4.4045548 -3.5690761 -3.0728681 -3.2108994][-3.6934919 -3.8645611 -4.1641717 -4.2138605 -3.274307 -1.5147469 -0.5229516 -1.1954665 -3.075021 -4.8650947 -5.3633847 -4.6674151 -3.6922796 -3.0940962 -3.0977077][-3.3763847 -3.6982124 -4.2250991 -4.2549624 -2.9863148 -0.80837345 0.60599422 -0.089086056 -2.705224 -5.1457243 -5.7801685 -4.97633 -3.8560176 -3.1252871 -2.9557753][-2.8325467 -3.3026416 -4.0163908 -4.0618782 -2.7519617 -0.4217124 1.4475408 0.94802237 -2.1350527 -5.0429754 -5.8819218 -5.16562 -4.0225563 -3.2212424 -2.9251292][-2.1407659 -2.6690807 -3.4047456 -3.4515951 -2.3045628 -0.0010323524 2.3001084 2.1304522 -1.0712023 -4.2369919 -5.3773732 -4.9834104 -4.0416374 -3.3340194 -2.9996371][-1.7104988 -2.1918211 -2.7599363 -2.7906783 -1.9342635 0.16454792 2.68004 2.8005323 -0.24197674 -3.3963766 -4.7593336 -4.6490054 -3.9293528 -3.3831959 -3.0846992][-1.81443 -2.2170281 -2.5424695 -2.5338676 -1.9883151 -0.33565235 1.8694472 2.074338 -0.46057582 -3.2228351 -4.5763359 -4.607162 -4.0700622 -3.6837015 -3.4311066][-2.4038682 -2.7181275 -2.8056571 -2.656795 -2.2612376 -1.1562428 0.32041502 0.43536282 -1.4004729 -3.5346992 -4.7191105 -4.8180585 -4.4230127 -4.1580248 -3.9520638][-3.1724362 -3.3612461 -3.2792072 -2.9949796 -2.6199174 -1.947578 -1.1516492 -1.131428 -2.3198726 -3.8544009 -4.8614068 -5.0196471 -4.73812 -4.5571504 -4.3892546][-3.8242667 -3.9002852 -3.7719719 -3.4562888 -3.0534716 -2.5777087 -2.1540236 -2.0980086 -2.7218192 -3.7699144 -4.6648293 -5.0000668 -4.95355 -4.8768921 -4.6806588][-4.2769971 -4.2281151 -4.0306473 -3.6473372 -3.1327133 -2.6643147 -2.41183 -2.3892925 -2.7344394 -3.4650092 -4.2568059 -4.8151007 -5.1156149 -5.1499023 -4.7951593][-4.39173 -4.1552396 -3.7385154 -3.1829252 -2.5516651 -2.0799153 -1.979955 -2.167336 -2.5430341 -3.1246657 -3.8091259 -4.5575757 -5.1728554 -5.2569036 -4.6608887][-4.3633838 -4.0240431 -3.4590659 -2.7985744 -2.1462259 -1.7291732 -1.7539856 -2.1102254 -2.4859192 -2.8385167 -3.3042166 -4.0815654 -4.85926 -5.005928 -4.3104935]]...]
INFO - root - 2017-12-07 07:55:28.569804: step 21010, loss = 0.67, batch loss = 0.60 (9.9 examples/sec; 0.810 sec/batch; 70h:04m:46s remains)
INFO - root - 2017-12-07 07:55:36.849018: step 21020, loss = 0.94, batch loss = 0.86 (9.7 examples/sec; 0.829 sec/batch; 71h:43m:18s remains)
INFO - root - 2017-12-07 07:55:45.274636: step 21030, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.823 sec/batch; 71h:11m:53s remains)
INFO - root - 2017-12-07 07:55:53.613336: step 21040, loss = 0.93, batch loss = 0.86 (9.5 examples/sec; 0.845 sec/batch; 73h:08m:02s remains)
INFO - root - 2017-12-07 07:56:01.898395: step 21050, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.848 sec/batch; 73h:19m:59s remains)
INFO - root - 2017-12-07 07:56:10.345201: step 21060, loss = 0.88, batch loss = 0.81 (9.4 examples/sec; 0.847 sec/batch; 73h:14m:39s remains)
INFO - root - 2017-12-07 07:56:18.447658: step 21070, loss = 0.73, batch loss = 0.66 (9.9 examples/sec; 0.805 sec/batch; 69h:40m:33s remains)
INFO - root - 2017-12-07 07:56:26.897096: step 21080, loss = 0.78, batch loss = 0.71 (9.5 examples/sec; 0.842 sec/batch; 72h:52m:03s remains)
INFO - root - 2017-12-07 07:56:35.204218: step 21090, loss = 0.80, batch loss = 0.73 (8.8 examples/sec; 0.910 sec/batch; 78h:41m:40s remains)
INFO - root - 2017-12-07 07:56:43.551768: step 21100, loss = 0.73, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 70h:23m:41s remains)
2017-12-07 07:56:44.168066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.681396 -2.7415638 -2.7694058 -2.7235355 -2.6135535 -2.4805737 -2.3429344 -2.2313151 -2.1809821 -2.2210047 -2.2953336 -2.3396688 -2.3925567 -2.4063544 -2.3441219][-2.418195 -2.468261 -2.4901779 -2.4247017 -2.2730289 -2.117698 -1.9516745 -1.8019369 -1.72142 -1.7522635 -1.8183668 -1.8191884 -1.8351386 -1.8520997 -1.8495111][-2.1920638 -2.2184289 -2.2491639 -2.2134383 -2.0878954 -1.9886594 -1.863807 -1.7213342 -1.627785 -1.6279616 -1.6296268 -1.5110927 -1.4177897 -1.4083748 -1.4842265][-2.0745306 -2.0745044 -2.1469398 -2.1859124 -2.1238203 -2.0744905 -1.9656887 -1.8183267 -1.7225413 -1.7011929 -1.6337116 -1.3873255 -1.1660054 -1.1288815 -1.2827024][-2.0392771 -2.0386369 -2.1693282 -2.2712419 -2.2190144 -2.114141 -1.918438 -1.7127829 -1.646692 -1.6733539 -1.6148241 -1.3415418 -1.0706847 -1.0359523 -1.2419536][-1.9796307 -1.9878082 -2.1540666 -2.2538216 -2.1246307 -1.8493602 -1.4541028 -1.1461189 -1.1667452 -1.3507135 -1.3992996 -1.2257376 -1.0170016 -1.0257845 -1.2610011][-1.8443961 -1.8301175 -1.9566643 -1.9773524 -1.7207 -1.2584264 -0.64920521 -0.22211647 -0.36949873 -0.76481318 -0.96888304 -0.9662044 -0.90146852 -1.0023761 -1.286449][-1.6464367 -1.5875499 -1.6276524 -1.5554333 -1.2355037 -0.71384859 -0.033051968 0.42472267 0.12770891 -0.45218611 -0.799628 -0.95401978 -1.0216053 -1.2014108 -1.5160294][-1.358011 -1.3076243 -1.3192956 -1.2410672 -1.0007229 -0.6169703 -0.084181786 0.25233269 -0.12229872 -0.75007844 -1.14082 -1.3607762 -1.4739764 -1.6547217 -1.9265568][-1.2209098 -1.2274528 -1.2741401 -1.2406912 -1.1163738 -0.91905189 -0.63754916 -0.48380637 -0.83440065 -1.3762357 -1.7186701 -1.9280574 -2.0112317 -2.0910859 -2.1820562][-1.4286296 -1.4917777 -1.5824835 -1.5914204 -1.5318034 -1.4512093 -1.3616388 -1.320673 -1.5655818 -1.9409754 -2.1891532 -2.3656442 -2.417856 -2.3813534 -2.2761841][-1.9219019 -2.0175629 -2.1314692 -2.1554556 -2.0918124 -2.0316653 -2.0203412 -2.020705 -2.1662972 -2.3849347 -2.5452752 -2.6886981 -2.7323337 -2.6524475 -2.4637823][-2.3851104 -2.4834576 -2.5997062 -2.6253939 -2.5464516 -2.4742475 -2.4798732 -2.5003414 -2.6099572 -2.7634997 -2.9008489 -3.0384688 -3.0742786 -3.0009575 -2.8366394][-2.4186218 -2.4910257 -2.5881526 -2.6149609 -2.5499334 -2.4785471 -2.4771509 -2.5066552 -2.6102905 -2.7546873 -2.9020634 -3.0369453 -3.0653095 -3.0105224 -2.9011478][-2.0278668 -2.0653381 -2.1267004 -2.1453211 -2.1077385 -2.0472033 -2.0320706 -2.0526609 -2.1302345 -2.2403748 -2.3556652 -2.4455054 -2.4469678 -2.3922586 -2.3140178]]...]
INFO - root - 2017-12-07 07:56:52.391047: step 21110, loss = 0.66, batch loss = 0.59 (10.0 examples/sec; 0.801 sec/batch; 69h:15m:54s remains)
INFO - root - 2017-12-07 07:57:00.794429: step 21120, loss = 0.82, batch loss = 0.74 (9.6 examples/sec; 0.837 sec/batch; 72h:25m:25s remains)
INFO - root - 2017-12-07 07:57:09.210001: step 21130, loss = 0.83, batch loss = 0.76 (9.0 examples/sec; 0.887 sec/batch; 76h:43m:01s remains)
INFO - root - 2017-12-07 07:57:17.473399: step 21140, loss = 0.70, batch loss = 0.63 (10.2 examples/sec; 0.788 sec/batch; 68h:07m:03s remains)
INFO - root - 2017-12-07 07:57:25.791473: step 21150, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.816 sec/batch; 70h:33m:27s remains)
INFO - root - 2017-12-07 07:57:34.284123: step 21160, loss = 0.73, batch loss = 0.65 (9.1 examples/sec; 0.880 sec/batch; 76h:04m:22s remains)
INFO - root - 2017-12-07 07:57:42.583008: step 21170, loss = 0.80, batch loss = 0.73 (9.8 examples/sec; 0.814 sec/batch; 70h:26m:16s remains)
INFO - root - 2017-12-07 07:57:51.005857: step 21180, loss = 0.89, batch loss = 0.82 (9.3 examples/sec; 0.858 sec/batch; 74h:11m:28s remains)
INFO - root - 2017-12-07 07:57:59.365789: step 21190, loss = 0.87, batch loss = 0.80 (9.5 examples/sec; 0.839 sec/batch; 72h:35m:40s remains)
INFO - root - 2017-12-07 07:58:07.732399: step 21200, loss = 0.71, batch loss = 0.64 (9.1 examples/sec; 0.877 sec/batch; 75h:49m:44s remains)
2017-12-07 07:58:08.364055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8777313 -1.6741812 -1.4409709 -1.4388449 -1.6277552 -1.8840885 -1.9911096 -1.8704059 -1.7055662 -1.6588733 -1.8748307 -2.290988 -2.6488509 -2.8992519 -3.0937061][-1.2454276 -1.1051648 -0.99600697 -1.052609 -1.3184767 -1.7069342 -1.9438469 -1.8637259 -1.6505291 -1.6122444 -1.777674 -2.0116537 -2.1588342 -2.2472839 -2.3835115][-0.76969576 -0.84586048 -0.94000483 -1.0962076 -1.4886253 -2.0240331 -2.4357347 -2.4240437 -2.1705451 -2.1109924 -2.1712844 -2.212939 -2.127486 -2.0439358 -2.0634918][-0.68385339 -1.0715129 -1.2844889 -1.3571267 -1.6269357 -2.1165853 -2.6392007 -2.7353234 -2.5531652 -2.5577164 -2.5349815 -2.4215593 -2.2400489 -2.2298591 -2.2948146][-0.89456463 -1.492985 -1.6880033 -1.5046692 -1.3932996 -1.6035082 -2.079463 -2.2465253 -2.232286 -2.3787141 -2.3652866 -2.1374402 -1.9621613 -2.150763 -2.4263744][-1.4707606 -2.0717235 -2.0367889 -1.4705632 -0.85873222 -0.632262 -0.87351346 -1.0135188 -1.1824329 -1.4743223 -1.5100374 -1.22451 -1.163594 -1.611449 -2.1825867][-2.33556 -2.8387218 -2.5130119 -1.6039636 -0.63976336 -0.11721611 -0.17809105 -0.34058094 -0.64389491 -0.96284103 -0.86905146 -0.43455052 -0.42540693 -1.009356 -1.739728][-3.2914968 -3.6482592 -3.1242666 -2.0540197 -0.94678092 -0.34820175 -0.46673059 -0.90702891 -1.3870881 -1.5921929 -1.1789503 -0.5035603 -0.4178071 -0.98708606 -1.6518939][-3.8712375 -4.0708184 -3.5005939 -2.4312513 -1.3169134 -0.70995378 -0.98077416 -1.8070412 -2.4913604 -2.5575635 -1.8492553 -0.98967052 -0.79356146 -1.2590759 -1.770174][-3.8566265 -3.9603381 -3.4633994 -2.4627128 -1.3432052 -0.71479273 -1.0931194 -2.2092948 -2.9961851 -2.9035382 -1.985738 -0.99507642 -0.70094967 -1.0487299 -1.4694841][-3.5770195 -3.6108701 -3.2338729 -2.3860188 -1.3457954 -0.74419808 -1.092427 -2.2596967 -3.0151734 -2.798974 -1.8243392 -0.78588128 -0.43349338 -0.7060535 -1.141525][-3.3830371 -3.4004493 -3.2019076 -2.6266422 -1.824362 -1.3364587 -1.6022103 -2.6242695 -3.2284899 -2.894033 -1.904464 -0.81724334 -0.47137856 -0.82065129 -1.4084375][-3.3404164 -3.3887467 -3.3590715 -3.0746059 -2.56257 -2.2151222 -2.4024711 -3.1943707 -3.6366587 -3.2235248 -2.2120428 -0.99642992 -0.50523949 -0.82455015 -1.5498211][-3.3492086 -3.4212022 -3.4797206 -3.4131036 -3.1477442 -2.911588 -2.9950595 -3.5323915 -3.8993945 -3.555155 -2.5972242 -1.2135873 -0.3567152 -0.39265633 -1.1069849][-3.3744807 -3.4506238 -3.5077541 -3.5254879 -3.3908947 -3.2130346 -3.2083068 -3.538322 -3.92095 -3.7681873 -2.9961233 -1.6142962 -0.40721464 -0.072329044 -0.64006329]]...]
INFO - root - 2017-12-07 07:58:16.822879: step 21210, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 0.906 sec/batch; 78h:18m:43s remains)
INFO - root - 2017-12-07 07:58:25.247876: step 21220, loss = 0.66, batch loss = 0.59 (8.6 examples/sec; 0.925 sec/batch; 79h:58m:58s remains)
INFO - root - 2017-12-07 07:58:33.577528: step 21230, loss = 0.63, batch loss = 0.56 (9.7 examples/sec; 0.825 sec/batch; 71h:19m:52s remains)
INFO - root - 2017-12-07 07:58:42.023282: step 21240, loss = 0.80, batch loss = 0.73 (9.4 examples/sec; 0.853 sec/batch; 73h:46m:17s remains)
INFO - root - 2017-12-07 07:58:50.337697: step 21250, loss = 0.87, batch loss = 0.79 (9.0 examples/sec; 0.891 sec/batch; 76h:59m:31s remains)
INFO - root - 2017-12-07 07:58:58.899870: step 21260, loss = 0.72, batch loss = 0.65 (9.3 examples/sec; 0.857 sec/batch; 74h:07m:06s remains)
INFO - root - 2017-12-07 07:59:07.211681: step 21270, loss = 0.84, batch loss = 0.77 (9.1 examples/sec; 0.881 sec/batch; 76h:09m:35s remains)
INFO - root - 2017-12-07 07:59:15.472417: step 21280, loss = 0.82, batch loss = 0.75 (9.8 examples/sec; 0.813 sec/batch; 70h:17m:47s remains)
INFO - root - 2017-12-07 07:59:23.878879: step 21290, loss = 0.84, batch loss = 0.76 (9.8 examples/sec; 0.814 sec/batch; 70h:24m:33s remains)
INFO - root - 2017-12-07 07:59:32.277933: step 21300, loss = 0.74, batch loss = 0.67 (9.6 examples/sec; 0.836 sec/batch; 72h:17m:19s remains)
2017-12-07 07:59:32.902001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6636741 -2.4713387 -2.4686146 -2.6601725 -3.1636906 -3.4773657 -3.1844726 -2.6641083 -2.5489259 -2.4852762 -1.7992494 -1.0673835 -0.85893273 -1.4207118 -2.1480012][-1.9173675 -1.6225259 -1.6835232 -2.1831837 -2.9383411 -3.1940951 -2.5871851 -1.8614509 -1.8287477 -2.0786746 -1.6669676 -1.1691637 -1.0902417 -1.5813067 -2.2152209][-1.2233264 -0.91057253 -1.0605686 -1.7532411 -2.5250344 -2.666132 -1.8697071 -1.0513141 -1.1008308 -1.6622791 -1.615448 -1.4206653 -1.4598141 -1.8273489 -2.3171372][-1.0323009 -0.78549719 -0.93621945 -1.6063225 -2.2391491 -2.2927761 -1.4511361 -0.51715159 -0.5232749 -1.2592773 -1.5472541 -1.6303802 -1.7598591 -2.0359924 -2.4000328][-1.3777409 -1.2539165 -1.3688431 -1.8851955 -2.313591 -2.282506 -1.4200969 -0.33832693 -0.18713522 -0.94659591 -1.4790881 -1.7747104 -1.9650991 -2.1831229 -2.4617493][-1.7938151 -1.8059359 -1.761759 -2.0054214 -2.2805266 -2.289304 -1.5659862 -0.46424294 -0.16876984 -0.84797192 -1.5019207 -1.8907304 -2.0870562 -2.2662661 -2.5039945][-1.7476118 -1.9060392 -1.7589989 -1.7682376 -2.0194108 -2.2247512 -1.7556896 -0.78057432 -0.37322378 -0.85227036 -1.4229538 -1.7729442 -1.9940779 -2.2270362 -2.5013502][-1.2532346 -1.6296849 -1.5329015 -1.4488604 -1.7384934 -2.2074714 -2.0692935 -1.3463197 -0.89667892 -1.0967395 -1.3912022 -1.56672 -1.7980103 -2.1214039 -2.4615717][-0.82056165 -1.4697685 -1.6399763 -1.5763283 -1.7687547 -2.2889276 -2.4645612 -2.0794837 -1.6803322 -1.5859871 -1.4854603 -1.3765075 -1.5783753 -1.9996619 -2.4036562][-0.47175932 -1.349246 -1.8869381 -1.9141212 -1.917558 -2.325948 -2.7204149 -2.6626191 -2.3589089 -2.026258 -1.5541003 -1.1843328 -1.3854675 -1.9100392 -2.3601053][-0.40689564 -1.2859006 -2.0314572 -2.0962498 -1.9015071 -2.1719904 -2.75639 -3.0071006 -2.82888 -2.3547359 -1.6142371 -1.073699 -1.2871628 -1.890383 -2.3520174][-0.70499325 -1.2092295 -1.8899388 -1.9198573 -1.6626823 -1.9816029 -2.7704763 -3.2398148 -3.14834 -2.6046281 -1.7420974 -1.1382644 -1.3393338 -1.9339302 -2.3657746][-1.0727155 -1.1013639 -1.5370011 -1.4795859 -1.275315 -1.7620633 -2.7582698 -3.3832753 -3.3426585 -2.7386651 -1.8614221 -1.2876792 -1.4395158 -1.9581246 -2.3661513][-1.410367 -1.0967638 -1.2391107 -1.056361 -0.91515112 -1.5215623 -2.5813208 -3.2976036 -3.3701363 -2.8045924 -1.9689107 -1.4159417 -1.4571736 -1.8958533 -2.3295002][-1.8728218 -1.4767911 -1.3825769 -1.0852485 -0.92985415 -1.4165831 -2.3067806 -3.0879431 -3.3945994 -2.9727609 -2.189158 -1.5795429 -1.4559534 -1.8253872 -2.2925589]]...]
INFO - root - 2017-12-07 07:59:41.332135: step 21310, loss = 0.69, batch loss = 0.62 (8.9 examples/sec; 0.901 sec/batch; 77h:52m:15s remains)
INFO - root - 2017-12-07 07:59:49.615457: step 21320, loss = 0.80, batch loss = 0.72 (9.8 examples/sec; 0.813 sec/batch; 70h:16m:54s remains)
INFO - root - 2017-12-07 07:59:57.991825: step 21330, loss = 0.50, batch loss = 0.43 (9.6 examples/sec; 0.837 sec/batch; 72h:21m:10s remains)
INFO - root - 2017-12-07 08:00:06.333444: step 21340, loss = 0.77, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 69h:21m:22s remains)
INFO - root - 2017-12-07 08:00:14.676237: step 21350, loss = 0.74, batch loss = 0.66 (9.7 examples/sec; 0.827 sec/batch; 71h:26m:12s remains)
INFO - root - 2017-12-07 08:00:22.962499: step 21360, loss = 0.76, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 73h:44m:09s remains)
INFO - root - 2017-12-07 08:00:31.389175: step 21370, loss = 0.67, batch loss = 0.60 (9.8 examples/sec; 0.813 sec/batch; 70h:13m:53s remains)
INFO - root - 2017-12-07 08:00:39.457362: step 21380, loss = 0.85, batch loss = 0.77 (12.2 examples/sec; 0.653 sec/batch; 56h:26m:23s remains)
INFO - root - 2017-12-07 08:00:47.586984: step 21390, loss = 1.00, batch loss = 0.93 (10.0 examples/sec; 0.797 sec/batch; 68h:53m:55s remains)
INFO - root - 2017-12-07 08:00:55.945153: step 21400, loss = 1.00, batch loss = 0.93 (9.5 examples/sec; 0.838 sec/batch; 72h:23m:39s remains)
2017-12-07 08:00:56.569116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5996208 -3.6813188 -3.7941377 -3.9293633 -4.0961494 -4.2025695 -4.1893024 -4.1266589 -4.1196227 -4.1466055 -4.1521111 -4.15637 -4.1151385 -4.0298595 -3.9613037][-3.6274936 -3.7400455 -3.8874221 -4.0447168 -4.2308807 -4.3479404 -4.32293 -4.2382855 -4.2169333 -4.2199736 -4.1417265 -4.0261083 -3.8535604 -3.6694524 -3.6094534][-3.6750846 -3.7964149 -3.9418459 -4.0458136 -4.1474915 -4.2009177 -4.1465764 -4.0316753 -3.9930108 -3.988245 -3.8451953 -3.6055732 -3.2710021 -2.9492738 -2.8978753][-3.7411025 -3.8402276 -3.9195275 -3.8919837 -3.8188217 -3.7405972 -3.6107554 -3.4033346 -3.3113852 -3.308435 -3.1503739 -2.8767209 -2.4710689 -2.0713127 -2.0645697][-3.8948622 -3.931818 -3.8768337 -3.6691592 -3.3880279 -3.1497157 -2.8962326 -2.5268495 -2.3309143 -2.3478279 -2.273721 -2.1374784 -1.8516891 -1.5142391 -1.5958991][-4.1193728 -4.0495954 -3.841095 -3.4792194 -3.0015225 -2.583643 -2.1600747 -1.6198492 -1.326571 -1.4158361 -1.5609961 -1.7336121 -1.7569611 -1.6173086 -1.7716584][-4.111805 -3.9564054 -3.6864367 -3.2852507 -2.6787438 -2.0672045 -1.4361873 -0.74773788 -0.44472718 -0.73444867 -1.2024295 -1.7129886 -2.0655985 -2.1295316 -2.2860231][-3.8422568 -3.6220102 -3.355237 -2.9885936 -2.3578048 -1.6441474 -0.911324 -0.23717594 -0.087325096 -0.6398592 -1.3549314 -2.0175006 -2.4862175 -2.6146679 -2.699142][-3.6159811 -3.3371196 -3.0627561 -2.7476878 -2.1943893 -1.5777833 -0.99063587 -0.58511686 -0.68892884 -1.2719519 -1.9085615 -2.4079161 -2.7608361 -2.8550024 -2.8860326][-3.5854273 -3.2160807 -2.8767405 -2.6019998 -2.2337136 -1.928339 -1.6878581 -1.6168659 -1.8360934 -2.1941767 -2.4746885 -2.6208005 -2.7442365 -2.7908332 -2.8543744][-3.465333 -3.0571904 -2.633697 -2.3890805 -2.2875788 -2.4026189 -2.5215988 -2.6622307 -2.8513136 -2.9104106 -2.8092871 -2.6400495 -2.573276 -2.6068974 -2.744432][-3.1934762 -2.8562889 -2.3901165 -2.1615632 -2.274035 -2.7062507 -3.0593457 -3.253695 -3.3700776 -3.2497122 -2.9530511 -2.6630745 -2.5470233 -2.593647 -2.7628026][-3.0609832 -2.7935812 -2.3198473 -2.0844421 -2.2535417 -2.7481873 -3.1191318 -3.2816203 -3.3663445 -3.2499766 -2.9805295 -2.7541883 -2.6758256 -2.7092576 -2.8487005][-3.2759233 -2.9623895 -2.5066962 -2.2883227 -2.3871365 -2.7331781 -2.9600587 -3.0429854 -3.1179996 -3.0998249 -2.9806077 -2.8855639 -2.855412 -2.8541446 -2.9396217][-3.5829213 -3.2185698 -2.8013048 -2.6017091 -2.5943828 -2.7237551 -2.7844489 -2.8000824 -2.87993 -2.9619451 -2.985611 -2.9929562 -2.9971993 -2.9745657 -3.0166326]]...]
INFO - root - 2017-12-07 08:01:04.948236: step 21410, loss = 0.78, batch loss = 0.71 (9.5 examples/sec; 0.845 sec/batch; 73h:03m:10s remains)
INFO - root - 2017-12-07 08:01:13.343060: step 21420, loss = 0.74, batch loss = 0.67 (9.8 examples/sec; 0.814 sec/batch; 70h:21m:37s remains)
INFO - root - 2017-12-07 08:01:21.568244: step 21430, loss = 0.74, batch loss = 0.67 (10.0 examples/sec; 0.799 sec/batch; 69h:04m:33s remains)
INFO - root - 2017-12-07 08:01:29.979479: step 21440, loss = 0.89, batch loss = 0.82 (9.4 examples/sec; 0.854 sec/batch; 73h:46m:51s remains)
INFO - root - 2017-12-07 08:01:38.313300: step 21450, loss = 0.76, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 69h:42m:21s remains)
INFO - root - 2017-12-07 08:01:46.672161: step 21460, loss = 0.69, batch loss = 0.62 (9.6 examples/sec; 0.836 sec/batch; 72h:15m:09s remains)
INFO - root - 2017-12-07 08:01:54.994938: step 21470, loss = 0.95, batch loss = 0.87 (9.5 examples/sec; 0.841 sec/batch; 72h:38m:28s remains)
INFO - root - 2017-12-07 08:02:03.159337: step 21480, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.833 sec/batch; 71h:59m:26s remains)
INFO - root - 2017-12-07 08:02:11.505664: step 21490, loss = 0.63, batch loss = 0.56 (9.6 examples/sec; 0.833 sec/batch; 71h:57m:03s remains)
INFO - root - 2017-12-07 08:02:19.884524: step 21500, loss = 0.94, batch loss = 0.87 (9.0 examples/sec; 0.890 sec/batch; 76h:51m:19s remains)
2017-12-07 08:02:20.490586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0523939 -2.9897659 -2.9657586 -3.0335326 -3.1510711 -3.1250625 -3.1027589 -3.1943524 -3.2605314 -3.1421716 -2.9686518 -2.8880317 -3.0372529 -3.3032138 -3.5350926][-2.7705445 -2.7510462 -2.9147537 -3.2140183 -3.5805545 -3.8534465 -4.0288615 -4.1253023 -4.0266147 -3.663835 -3.2290494 -2.8673334 -2.7962542 -3.0046871 -3.3003192][-2.8430443 -2.7075033 -2.759861 -2.9783099 -3.3038716 -3.6567349 -3.8919408 -3.9532743 -3.7768352 -3.3761039 -2.9009151 -2.3929086 -2.085789 -2.1286566 -2.466373][-3.1591234 -2.8945572 -2.83506 -2.9916315 -3.2216105 -3.4909 -3.6354468 -3.6038089 -3.4576335 -3.1956396 -2.7707319 -2.1725779 -1.643492 -1.4769313 -1.8369362][-3.4847722 -3.1954212 -3.0809593 -3.2301452 -3.3879337 -3.5394189 -3.5508003 -3.4492254 -3.4375372 -3.3717051 -2.9945493 -2.331147 -1.6449847 -1.3562737 -1.7570817][-3.7566733 -3.3928761 -3.1482344 -3.2563906 -3.383801 -3.4320939 -3.3540115 -3.2413354 -3.370759 -3.4595792 -3.1637163 -2.5629604 -1.8441916 -1.4677951 -1.8090143][-3.5838735 -3.0227671 -2.6159391 -2.654767 -2.7013216 -2.5857468 -2.410856 -2.3566926 -2.6785932 -2.9663086 -2.8771825 -2.471487 -1.8195002 -1.3745446 -1.5814989][-2.7826707 -2.0617709 -1.5335093 -1.4365609 -1.294946 -0.98852992 -0.73652172 -0.81426191 -1.4245236 -2.0381994 -2.2913568 -2.1427524 -1.6584461 -1.23331 -1.2681329][-1.8959217 -1.1715007 -0.71507382 -0.57087421 -0.20787239 0.35241556 0.79939747 0.74157524 -0.0915823 -1.0890915 -1.7635286 -1.9485018 -1.7166445 -1.3444905 -1.1119435][-1.7340658 -1.3155866 -1.0479422 -0.7511034 0.010499954 0.91983652 1.5462279 1.5125847 0.56031942 -0.66824579 -1.601661 -2.0373967 -2.05503 -1.7571347 -1.2854362][-2.0166821 -1.915683 -1.7024355 -1.1875956 -0.18288183 0.77186775 1.2763624 1.1972041 0.29826355 -0.90226436 -1.8221116 -2.2988493 -2.4461472 -2.2415278 -1.6671975][-1.8484201 -2.0174682 -1.9538782 -1.5028703 -0.70396996 -0.10501575 0.095807552 -0.029532909 -0.7222414 -1.6364801 -2.2467313 -2.5714672 -2.7039087 -2.5859194 -2.0496657][-1.3442712 -1.8562922 -2.1776123 -2.0788083 -1.6957483 -1.4764104 -1.5395007 -1.6756921 -2.0373356 -2.4873734 -2.6743569 -2.7710381 -2.8132982 -2.7664404 -2.3617253][-1.1748796 -1.9170349 -2.5829923 -2.8357625 -2.7928381 -2.7910128 -2.9244664 -2.9799728 -3.0179896 -3.0280695 -2.9202464 -2.9061728 -2.9163108 -2.971386 -2.7519703][-1.3920383 -2.0884132 -2.8474154 -3.2995586 -3.45127 -3.4800525 -3.4978433 -3.4390545 -3.3008099 -3.1128116 -2.9177854 -2.9105821 -3.008302 -3.228555 -3.2144866]]...]
INFO - root - 2017-12-07 08:02:28.859576: step 21510, loss = 0.76, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 72h:11m:24s remains)
INFO - root - 2017-12-07 08:02:37.282334: step 21520, loss = 0.74, batch loss = 0.67 (9.2 examples/sec; 0.871 sec/batch; 75h:11m:51s remains)
INFO - root - 2017-12-07 08:02:45.523899: step 21530, loss = 0.68, batch loss = 0.61 (10.1 examples/sec; 0.791 sec/batch; 68h:18m:51s remains)
INFO - root - 2017-12-07 08:02:53.949865: step 21540, loss = 0.57, batch loss = 0.50 (9.3 examples/sec; 0.861 sec/batch; 74h:24m:16s remains)
INFO - root - 2017-12-07 08:03:02.210945: step 21550, loss = 1.00, batch loss = 0.92 (9.7 examples/sec; 0.826 sec/batch; 71h:22m:21s remains)
INFO - root - 2017-12-07 08:03:10.420663: step 21560, loss = 0.98, batch loss = 0.91 (10.0 examples/sec; 0.803 sec/batch; 69h:20m:03s remains)
INFO - root - 2017-12-07 08:03:18.636897: step 21570, loss = 1.08, batch loss = 1.01 (9.8 examples/sec; 0.820 sec/batch; 70h:48m:58s remains)
INFO - root - 2017-12-07 08:03:26.961454: step 21580, loss = 0.69, batch loss = 0.62 (9.3 examples/sec; 0.858 sec/batch; 74h:08m:01s remains)
INFO - root - 2017-12-07 08:03:35.477140: step 21590, loss = 0.91, batch loss = 0.84 (8.9 examples/sec; 0.900 sec/batch; 77h:42m:43s remains)
INFO - root - 2017-12-07 08:03:43.986384: step 21600, loss = 0.68, batch loss = 0.61 (9.1 examples/sec; 0.877 sec/batch; 75h:42m:58s remains)
2017-12-07 08:03:44.631399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2063882 -3.2288246 -3.2306473 -3.2233446 -3.2181706 -3.2299767 -3.1910968 -3.0600448 -2.8404093 -2.6202862 -2.5502021 -2.649312 -2.6917696 -2.6799617 -2.7683308][-3.0441306 -3.0586348 -3.0359588 -3.0174692 -3.0121341 -3.0093262 -2.9756904 -2.8643947 -2.6587605 -2.4434638 -2.4014621 -2.5620961 -2.648468 -2.6400142 -2.715508][-2.7346697 -2.7290144 -2.6871443 -2.6607094 -2.6484344 -2.6043079 -2.5558023 -2.4946265 -2.3611214 -2.1920853 -2.1934788 -2.4313717 -2.5946844 -2.6168821 -2.683794][-2.3721006 -2.3630164 -2.3239565 -2.2932234 -2.2744734 -2.2024362 -2.1600895 -2.1768737 -2.1388171 -2.0165484 -2.0320978 -2.2956774 -2.5050116 -2.5657096 -2.6476591][-2.056303 -2.0696244 -2.0655417 -2.0421238 -2.0233071 -1.9484017 -1.9241393 -1.9957404 -2.0127177 -1.9096634 -1.9216232 -2.1724901 -2.3859861 -2.476964 -2.5973034][-1.7977109 -1.8138645 -1.8457575 -1.8355334 -1.8097048 -1.732939 -1.7111204 -1.8051207 -1.8492229 -1.775806 -1.8085234 -2.0549138 -2.2719047 -2.3918548 -2.5572343][-1.7156377 -1.7031419 -1.7497277 -1.7632844 -1.7193973 -1.6137862 -1.5575955 -1.6389129 -1.6877668 -1.6453104 -1.7114911 -1.959527 -2.1791849 -2.318469 -2.521234][-1.6784196 -1.6339157 -1.6607809 -1.6752052 -1.6143594 -1.4777768 -1.3717809 -1.4196062 -1.4785376 -1.4868498 -1.6127431 -1.8845634 -2.114207 -2.2670898 -2.4883759][-1.5650649 -1.5133772 -1.4981916 -1.487911 -1.4264569 -1.3165202 -1.2287312 -1.2908368 -1.3712463 -1.4235928 -1.6014318 -1.8986032 -2.1355999 -2.2831144 -2.4926095][-1.6329269 -1.6022367 -1.5711248 -1.5336487 -1.4748371 -1.427702 -1.3953786 -1.4746199 -1.5555665 -1.6042023 -1.7598617 -2.0149968 -2.2243059 -2.3509452 -2.5304475][-1.9659336 -1.9842441 -1.9801753 -1.9224255 -1.8377748 -1.8088744 -1.7946844 -1.8442743 -1.8934395 -1.9154775 -2.0200458 -2.2065642 -2.3692651 -2.4641709 -2.6005039][-2.3109114 -2.3721943 -2.4039702 -2.3484337 -2.2419138 -2.1987743 -2.1812396 -2.1943369 -2.2054393 -2.1992638 -2.2703958 -2.4167058 -2.5442986 -2.6076064 -2.6933103][-2.502562 -2.5712633 -2.6275973 -2.5994687 -2.5140734 -2.4718022 -2.4465232 -2.4338541 -2.4153001 -2.3918157 -2.4453204 -2.5679908 -2.6738558 -2.7181196 -2.7706468][-2.5828366 -2.6187325 -2.6708534 -2.6656084 -2.6059284 -2.5716209 -2.5617988 -2.5617652 -2.5383949 -2.5093796 -2.5570209 -2.6691942 -2.7591386 -2.7884717 -2.8218961][-2.6707087 -2.6717219 -2.6923866 -2.6800423 -2.6227787 -2.5871506 -2.5952642 -2.6195631 -2.5990877 -2.5608628 -2.600122 -2.7087798 -2.7882953 -2.8086212 -2.8362765]]...]
INFO - root - 2017-12-07 08:03:52.892563: step 21610, loss = 0.62, batch loss = 0.55 (9.5 examples/sec; 0.839 sec/batch; 72h:29m:21s remains)
INFO - root - 2017-12-07 08:04:01.203952: step 21620, loss = 1.01, batch loss = 0.94 (10.1 examples/sec; 0.791 sec/batch; 68h:17m:47s remains)
INFO - root - 2017-12-07 08:04:09.647263: step 21630, loss = 0.84, batch loss = 0.77 (9.2 examples/sec; 0.868 sec/batch; 74h:55m:57s remains)
INFO - root - 2017-12-07 08:04:18.006586: step 21640, loss = 0.93, batch loss = 0.85 (9.5 examples/sec; 0.844 sec/batch; 72h:52m:54s remains)
INFO - root - 2017-12-07 08:04:26.325326: step 21650, loss = 0.63, batch loss = 0.56 (9.8 examples/sec; 0.816 sec/batch; 70h:27m:05s remains)
INFO - root - 2017-12-07 08:04:34.596232: step 21660, loss = 0.63, batch loss = 0.56 (9.9 examples/sec; 0.810 sec/batch; 69h:55m:35s remains)
INFO - root - 2017-12-07 08:04:42.997426: step 21670, loss = 0.63, batch loss = 0.55 (9.6 examples/sec; 0.835 sec/batch; 72h:05m:40s remains)
INFO - root - 2017-12-07 08:04:51.418742: step 21680, loss = 0.56, batch loss = 0.49 (9.6 examples/sec; 0.834 sec/batch; 72h:00m:10s remains)
INFO - root - 2017-12-07 08:04:59.813241: step 21690, loss = 0.78, batch loss = 0.71 (8.8 examples/sec; 0.911 sec/batch; 78h:41m:11s remains)
INFO - root - 2017-12-07 08:05:07.897526: step 21700, loss = 0.96, batch loss = 0.89 (9.6 examples/sec; 0.830 sec/batch; 71h:37m:22s remains)
2017-12-07 08:05:08.639343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.59630775 -1.0748928 -2.1214271 -2.9310153 -3.4462233 -3.0873122 -2.2458928 -1.4663787 -1.0201304 -1.2591159 -1.7140472 -1.9607658 -2.2751751 -2.4064617 -2.1364818][-0.5427134 -1.2565804 -2.1519725 -2.6887937 -3.004684 -2.4222834 -1.4855583 -0.85213733 -0.56086159 -0.90862131 -1.412369 -1.5823991 -1.8313928 -1.9873919 -1.6956034][-0.70252538 -1.7170589 -2.5421772 -2.8913217 -2.8897178 -1.9605196 -0.95831418 -0.50458908 -0.27068853 -0.6002512 -1.2775569 -1.6855028 -1.9867749 -1.9429765 -1.3589692][-0.62553382 -1.8740187 -2.9254477 -3.4768124 -3.3249092 -2.1168938 -1.065058 -0.54192066 0.080674648 -0.042872429 -1.1913972 -2.2569361 -2.7246754 -2.3755524 -1.4127338][-0.388299 -1.6647143 -2.9915991 -3.7788122 -3.5609393 -2.4051921 -1.4886234 -0.59217834 0.79303217 0.81846952 -1.008574 -2.7989163 -3.4487715 -2.8830581 -1.6104655][-0.079870224 -1.1209791 -2.4630589 -3.2252197 -3.0228882 -2.2752526 -1.6606877 -0.27786589 1.8306656 1.7598104 -0.71260118 -2.9702549 -3.748775 -3.0538411 -1.455739][-0.033951283 -0.88183331 -2.0061965 -2.4135008 -2.1809437 -1.9443376 -1.6696167 0.096390724 2.6115303 2.3371091 -0.45866919 -2.8172879 -3.6286864 -2.8260837 -0.90167522][-0.30190754 -1.2930593 -2.2493784 -2.2028913 -1.7246342 -1.6227303 -1.3638082 0.50063038 2.7404323 2.1273184 -0.64887285 -2.7753248 -3.4383917 -2.4854608 -0.41462135][-0.61057377 -1.9866853 -3.0233853 -2.8194642 -2.1266878 -1.7108376 -1.0174007 0.77928305 2.3437247 1.3830147 -1.1658843 -2.9371467 -3.4559274 -2.4468131 -0.50531507][-0.46662331 -2.3401353 -3.737143 -3.7967186 -3.1185081 -2.2829719 -1.07356 0.53472567 1.4214835 0.28180742 -1.9022171 -3.3229761 -3.7215667 -2.7703121 -1.1667292][0.10866833 -2.0241346 -3.8598659 -4.4750471 -4.0137973 -2.8808658 -1.4254744 -0.16350555 0.24895763 -0.8570621 -2.6437635 -3.7728093 -4.0694561 -3.253489 -1.9841194][0.34167957 -1.6778567 -3.7788262 -4.9093304 -4.7194819 -3.5097287 -2.1300685 -1.1988528 -0.94072127 -1.7857957 -3.1787667 -4.1202121 -4.3966193 -3.790612 -2.845437][0.25092649 -1.4601033 -3.653966 -5.1150594 -5.1748261 -4.1360464 -3.0655398 -2.38375 -2.0536208 -2.5326238 -3.5552495 -4.33948 -4.6398339 -4.2838554 -3.6423979][0.032429695 -1.3345258 -3.5295608 -5.0880003 -5.27302 -4.4876719 -3.7919469 -3.3475313 -2.9703326 -3.1465468 -3.7887197 -4.3474517 -4.6085687 -4.4228477 -4.044117][-0.0049386024 -1.2281828 -3.4358892 -4.9519739 -5.1241679 -4.518683 -4.1386719 -3.9149144 -3.6092854 -3.6427183 -3.9672725 -4.27051 -4.4176488 -4.33297 -4.1538939]]...]
INFO - root - 2017-12-07 08:05:16.939652: step 21710, loss = 0.73, batch loss = 0.66 (9.5 examples/sec; 0.845 sec/batch; 72h:57m:29s remains)
INFO - root - 2017-12-07 08:05:25.168596: step 21720, loss = 0.77, batch loss = 0.70 (9.9 examples/sec; 0.810 sec/batch; 69h:56m:22s remains)
INFO - root - 2017-12-07 08:05:33.563914: step 21730, loss = 0.79, batch loss = 0.72 (10.5 examples/sec; 0.762 sec/batch; 65h:45m:46s remains)
INFO - root - 2017-12-07 08:05:41.795058: step 21740, loss = 1.07, batch loss = 1.00 (9.7 examples/sec; 0.824 sec/batch; 71h:07m:05s remains)
INFO - root - 2017-12-07 08:05:50.041557: step 21750, loss = 0.77, batch loss = 0.70 (9.6 examples/sec; 0.830 sec/batch; 71h:39m:06s remains)
INFO - root - 2017-12-07 08:05:58.326767: step 21760, loss = 0.82, batch loss = 0.75 (9.7 examples/sec; 0.823 sec/batch; 71h:03m:31s remains)
INFO - root - 2017-12-07 08:06:06.607417: step 21770, loss = 0.96, batch loss = 0.88 (9.4 examples/sec; 0.855 sec/batch; 73h:48m:02s remains)
INFO - root - 2017-12-07 08:06:14.860554: step 21780, loss = 0.65, batch loss = 0.58 (10.0 examples/sec; 0.801 sec/batch; 69h:08m:01s remains)
INFO - root - 2017-12-07 08:06:23.103459: step 21790, loss = 1.07, batch loss = 1.00 (9.7 examples/sec; 0.821 sec/batch; 70h:50m:40s remains)
INFO - root - 2017-12-07 08:06:31.483198: step 21800, loss = 0.67, batch loss = 0.60 (9.8 examples/sec; 0.816 sec/batch; 70h:26m:34s remains)
2017-12-07 08:06:32.210284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.51295185 -0.91285229 -1.291472 -1.1225352 -0.62241077 -0.45808244 -0.62939286 -0.72624874 -0.874393 -1.2667072 -1.686738 -1.9524193 -2.1532645 -2.2723665 -2.3674192][-0.50692296 -0.84976983 -1.1304376 -0.86345005 -0.47981858 -0.49471498 -0.74010658 -0.92213035 -1.1853566 -1.6087844 -1.96001 -2.1652491 -2.3816884 -2.534266 -2.6007876][-0.46766925 -0.69185638 -0.90147519 -0.73290968 -0.57811356 -0.66555786 -0.85944176 -1.1327703 -1.5591388 -2.0429246 -2.3425989 -2.4656134 -2.6419282 -2.7482402 -2.7344811][-0.34246635 -0.41060829 -0.55161095 -0.53488016 -0.60869527 -0.74709988 -0.90790224 -1.2707489 -1.8349237 -2.4093494 -2.7349372 -2.8354855 -2.9461458 -2.914062 -2.7886505][-0.3352356 -0.3762598 -0.44660068 -0.41327 -0.46986651 -0.57482004 -0.79300141 -1.3090785 -2.0385857 -2.7260523 -3.1259954 -3.2214808 -3.21407 -3.0144169 -2.8066401][-0.48981023 -0.64748478 -0.69762969 -0.55173254 -0.425344 -0.39883852 -0.65332437 -1.2960076 -2.1602831 -2.9214597 -3.3599553 -3.4382143 -3.2967148 -2.9864597 -2.7583346][-0.85589767 -1.1141982 -1.1469448 -0.87326074 -0.55153966 -0.38083982 -0.61188149 -1.2912941 -2.1751409 -2.9049816 -3.3392439 -3.4000735 -3.1685822 -2.8339179 -2.6486535][-1.1677997 -1.4028153 -1.4159968 -1.080106 -0.67059779 -0.4012351 -0.55863881 -1.1814053 -1.9684744 -2.5981202 -3.0203779 -3.0838408 -2.8153133 -2.5211535 -2.4392037][-1.1593411 -1.2034338 -1.1639435 -0.86048126 -0.54032278 -0.34780931 -0.51948142 -1.0595529 -1.6824768 -2.2139993 -2.6400857 -2.7181988 -2.4800203 -2.2766464 -2.3037038][-1.0459204 -0.81338048 -0.62790155 -0.37547302 -0.26078033 -0.30069208 -0.57289529 -1.0340269 -1.5171096 -1.9887047 -2.389734 -2.467056 -2.3039763 -2.2119594 -2.2939119][-0.973922 -0.60645676 -0.29873466 -0.051826 -0.09751749 -0.36265945 -0.72999787 -1.1237199 -1.5275047 -1.9738231 -2.3204417 -2.3666213 -2.2702107 -2.2554257 -2.3340118][-0.91219568 -0.66401291 -0.42747211 -0.20723867 -0.26973438 -0.59726524 -0.97363877 -1.3042076 -1.6786437 -2.1386027 -2.4818187 -2.5173047 -2.4499176 -2.4484921 -2.4712276][-0.78957105 -0.77153087 -0.72504783 -0.62878776 -0.71031642 -1.01384 -1.3397357 -1.626173 -1.982609 -2.4169738 -2.7443027 -2.7758093 -2.7232118 -2.7219505 -2.6904311][-0.76485753 -0.88503695 -0.95631027 -1.0026746 -1.1742077 -1.4983327 -1.8133564 -2.090014 -2.4057722 -2.72075 -2.9535465 -2.94987 -2.8845468 -2.8712862 -2.8169267][-0.95130634 -1.0900686 -1.1905134 -1.3583684 -1.6380208 -1.9906783 -2.2660975 -2.4865341 -2.7151256 -2.8810625 -3.0029666 -2.9705153 -2.889559 -2.84999 -2.7829149]]...]
INFO - root - 2017-12-07 08:06:40.492126: step 21810, loss = 0.99, batch loss = 0.92 (9.5 examples/sec; 0.840 sec/batch; 72h:27m:37s remains)
INFO - root - 2017-12-07 08:06:48.851887: step 21820, loss = 0.93, batch loss = 0.86 (9.5 examples/sec; 0.843 sec/batch; 72h:44m:38s remains)
INFO - root - 2017-12-07 08:06:57.145180: step 21830, loss = 0.78, batch loss = 0.71 (10.3 examples/sec; 0.779 sec/batch; 67h:12m:46s remains)
INFO - root - 2017-12-07 08:07:05.502020: step 21840, loss = 0.74, batch loss = 0.67 (9.5 examples/sec; 0.838 sec/batch; 72h:17m:54s remains)
INFO - root - 2017-12-07 08:07:13.793781: step 21850, loss = 0.72, batch loss = 0.65 (9.8 examples/sec; 0.813 sec/batch; 70h:09m:26s remains)
INFO - root - 2017-12-07 08:07:22.066947: step 21860, loss = 0.79, batch loss = 0.72 (9.5 examples/sec; 0.840 sec/batch; 72h:31m:25s remains)
INFO - root - 2017-12-07 08:07:30.428351: step 21870, loss = 0.67, batch loss = 0.60 (9.3 examples/sec; 0.862 sec/batch; 74h:20m:45s remains)
INFO - root - 2017-12-07 08:07:38.760264: step 21880, loss = 1.00, batch loss = 0.92 (10.2 examples/sec; 0.786 sec/batch; 67h:47m:47s remains)
INFO - root - 2017-12-07 08:07:47.049911: step 21890, loss = 0.74, batch loss = 0.67 (10.2 examples/sec; 0.784 sec/batch; 67h:37m:36s remains)
INFO - root - 2017-12-07 08:07:55.450780: step 21900, loss = 0.76, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:36m:57s remains)
2017-12-07 08:07:56.126632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3880036 -2.1406448 -2.0716348 -2.1946537 -2.2133472 -2.0003021 -1.6817467 -1.5988257 -1.6651545 -1.5181515 -1.0590675 -0.78971696 -0.98146439 -1.375304 -1.5528069][-1.3986032 -1.2728455 -1.1905451 -1.1697879 -1.1031351 -0.95762467 -0.80120182 -0.85129905 -0.98702121 -0.90994954 -0.53288841 -0.31505728 -0.53643584 -0.926491 -1.0454528][-1.3312895 -1.404675 -1.3044794 -1.1450934 -1.032656 -0.9438355 -0.90337062 -1.0262344 -1.1020374 -0.99534106 -0.71276879 -0.53615689 -0.68645167 -0.86543155 -0.6852839][-1.8448513 -2.1181126 -1.9588253 -1.6827624 -1.5983615 -1.5238829 -1.4587693 -1.5279279 -1.4912028 -1.279264 -1.0067928 -0.83841515 -0.91899705 -0.9210403 -0.48759985][-2.2304635 -2.5658963 -2.2659178 -1.8152332 -1.7875783 -1.8224723 -1.7621799 -1.7825072 -1.677825 -1.4210598 -1.1977098 -1.1003039 -1.1847165 -1.1264846 -0.60710454][-1.9755013 -2.1518817 -1.6914339 -1.1014156 -1.095299 -1.3029754 -1.3701713 -1.4202819 -1.292865 -0.95416 -0.73153758 -0.74861383 -1.0005164 -1.0867014 -0.68024683][-1.1260369 -1.0343034 -0.43094015 0.22080517 0.15615988 -0.29669285 -0.569674 -0.74455023 -0.793571 -0.60708 -0.43631268 -0.50467396 -0.8285656 -0.99732041 -0.66348314][-0.74504137 -0.55805826 -0.00094270706 0.51687241 0.33366585 -0.26887798 -0.60237718 -0.73677778 -0.83782268 -0.7652483 -0.68035817 -0.792505 -1.0438118 -1.0622878 -0.60664749][-1.161819 -0.98387384 -0.57296944 -0.21402121 -0.31040144 -0.77062297 -1.0060301 -0.96980929 -0.90221214 -0.75245142 -0.71955347 -0.92809772 -1.1883562 -1.0657659 -0.54524803][-1.0426702 -0.7767477 -0.46424913 -0.22893238 -0.16455793 -0.41786957 -0.64106011 -0.62344456 -0.5310874 -0.41643238 -0.50400448 -0.84010172 -1.1727479 -1.0824087 -0.65772748][-0.24984455 0.080776215 0.27868128 0.37468433 0.47115421 0.32498932 0.10163164 0.096356392 0.14469147 0.14617586 -0.10276365 -0.51048851 -0.8525784 -0.83010721 -0.49763107][-0.048544884 0.25340128 0.42765522 0.57135248 0.78701448 0.84298277 0.77739954 0.78137016 0.70092154 0.46794462 0.070344448 -0.33294439 -0.61760712 -0.57815289 -0.19678259][-0.20186234 0.097661495 0.29491758 0.46956444 0.64631605 0.73669243 0.7512579 0.73579884 0.56495285 0.24891376 -0.080294609 -0.33527803 -0.52671552 -0.5237329 -0.14292574][-0.55438757 -0.35825109 -0.24475527 -0.21270418 -0.21790934 -0.20596266 -0.15591145 -0.18837976 -0.35243368 -0.57889366 -0.70213747 -0.77959847 -0.86927629 -0.88167 -0.52076507][-1.3280256 -1.2071912 -1.1394172 -1.1724935 -1.2517481 -1.2339983 -1.1297414 -1.1593597 -1.3148816 -1.481792 -1.5390344 -1.6105409 -1.6956997 -1.6904349 -1.2955325]]...]
INFO - root - 2017-12-07 08:08:04.502740: step 21910, loss = 0.79, batch loss = 0.72 (9.1 examples/sec; 0.881 sec/batch; 76h:00m:45s remains)
INFO - root - 2017-12-07 08:08:12.867792: step 21920, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.821 sec/batch; 70h:51m:21s remains)
INFO - root - 2017-12-07 08:08:21.251970: step 21930, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 73h:05m:13s remains)
INFO - root - 2017-12-07 08:08:29.527875: step 21940, loss = 0.65, batch loss = 0.57 (9.3 examples/sec; 0.863 sec/batch; 74h:25m:45s remains)
INFO - root - 2017-12-07 08:08:37.699344: step 21950, loss = 0.75, batch loss = 0.68 (9.8 examples/sec; 0.817 sec/batch; 70h:30m:23s remains)
INFO - root - 2017-12-07 08:08:46.151892: step 21960, loss = 0.81, batch loss = 0.74 (9.6 examples/sec; 0.835 sec/batch; 72h:01m:30s remains)
INFO - root - 2017-12-07 08:08:54.554779: step 21970, loss = 0.84, batch loss = 0.77 (8.7 examples/sec; 0.919 sec/batch; 79h:16m:29s remains)
INFO - root - 2017-12-07 08:09:02.830109: step 21980, loss = 0.85, batch loss = 0.78 (9.8 examples/sec; 0.813 sec/batch; 70h:05m:24s remains)
INFO - root - 2017-12-07 08:09:11.286940: step 21990, loss = 0.77, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 74h:12m:15s remains)
INFO - root - 2017-12-07 08:09:19.501259: step 22000, loss = 0.61, batch loss = 0.54 (10.1 examples/sec; 0.790 sec/batch; 68h:10m:07s remains)
2017-12-07 08:09:20.157335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0593734 -2.0246506 -2.5572042 -3.0041761 -2.594 -1.6321926 -0.84356213 -0.77287555 -1.5949724 -2.7900829 -3.4881842 -3.0894952 -1.9019275 -0.95321321 -1.0115681][-2.3001182 -2.2767205 -2.719696 -2.985553 -2.4313369 -1.4270573 -0.74726057 -0.91068006 -1.8751273 -2.9545248 -3.4225547 -2.8636003 -1.6964738 -0.91935921 -1.0795133][-2.5560045 -2.4081786 -2.6414142 -2.7270813 -2.1890392 -1.3740158 -0.9846487 -1.3966031 -2.3283396 -3.0825789 -3.249928 -2.6755266 -1.7881486 -1.3728812 -1.6605518][-2.6897473 -2.3492155 -2.3424928 -2.3225896 -1.9822724 -1.552599 -1.4843118 -1.978617 -2.6525953 -2.984653 -2.8696537 -2.3303928 -1.807972 -1.7780027 -2.15386][-2.622129 -2.0849576 -1.9172766 -1.8621719 -1.6536839 -1.3937962 -1.422385 -1.9095573 -2.4584165 -2.6233909 -2.4436002 -2.0217726 -1.7841282 -1.93943 -2.2779336][-2.4819279 -1.8567953 -1.6159194 -1.449569 -1.0781155 -0.59657097 -0.48606372 -1.0712957 -1.8973999 -2.3469656 -2.3704627 -2.0516644 -1.9046052 -2.0458231 -2.2752903][-2.5126836 -1.89796 -1.5302997 -1.1084881 -0.4200778 0.40095186 0.67436171 -0.17604113 -1.5372655 -2.4972117 -2.7932417 -2.5098829 -2.2999864 -2.3027406 -2.3705587][-2.6752219 -2.0819881 -1.5074508 -0.80352139 -0.0056123734 0.72205257 0.740819 -0.3611412 -1.8708117 -2.8822703 -3.1111028 -2.7362194 -2.4540019 -2.4028656 -2.4257817][-2.8308063 -2.2262502 -1.4305348 -0.54498506 0.052009583 0.22408962 -0.28202868 -1.4094224 -2.5938103 -3.2312911 -3.1212888 -2.500963 -2.1135454 -2.0522158 -2.1119049][-2.9570341 -2.3661239 -1.501034 -0.64286494 -0.34656906 -0.63926888 -1.417836 -2.36124 -3.1102905 -3.407032 -3.0597305 -2.2587786 -1.7152064 -1.542835 -1.5818202][-2.9946127 -2.4670453 -1.7318709 -1.0908318 -0.99487877 -1.4161165 -2.1423202 -2.801908 -3.2077258 -3.3238521 -2.9394512 -2.1293323 -1.533221 -1.3031907 -1.3324046][-3.0177295 -2.5399873 -1.9537065 -1.5179586 -1.5088849 -1.8611414 -2.3814609 -2.7951751 -3.0557661 -3.1976438 -2.9663386 -2.3235414 -1.7915199 -1.5766923 -1.6459653][-3.0370603 -2.5575094 -2.0483692 -1.7464359 -1.7967355 -2.0978615 -2.4540954 -2.665802 -2.8037887 -2.9431958 -2.8547039 -2.4271998 -2.0165584 -1.8483329 -1.928097][-3.21634 -2.8237939 -2.402478 -2.1630654 -2.1976602 -2.4075465 -2.6196933 -2.7161248 -2.7692776 -2.8484335 -2.8029501 -2.5301847 -2.248533 -2.1273885 -2.1901965][-3.4876468 -3.3130262 -3.0827374 -2.9402666 -2.953629 -3.0541182 -3.1323919 -3.1517684 -3.1572022 -3.1891553 -3.1686077 -3.0179505 -2.8521245 -2.7864871 -2.8355796]]...]
INFO - root - 2017-12-07 08:09:28.215666: step 22010, loss = 0.69, batch loss = 0.62 (9.3 examples/sec; 0.857 sec/batch; 73h:56m:40s remains)
INFO - root - 2017-12-07 08:09:36.642001: step 22020, loss = 0.76, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:22m:35s remains)
INFO - root - 2017-12-07 08:09:44.897975: step 22030, loss = 0.69, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 70h:41m:03s remains)
INFO - root - 2017-12-07 08:09:53.326428: step 22040, loss = 0.68, batch loss = 0.61 (9.6 examples/sec; 0.833 sec/batch; 71h:48m:48s remains)
INFO - root - 2017-12-07 08:10:01.683017: step 22050, loss = 0.61, batch loss = 0.53 (10.0 examples/sec; 0.799 sec/batch; 68h:55m:44s remains)
INFO - root - 2017-12-07 08:10:09.995298: step 22060, loss = 0.72, batch loss = 0.65 (9.9 examples/sec; 0.809 sec/batch; 69h:44m:07s remains)
INFO - root - 2017-12-07 08:10:18.298707: step 22070, loss = 0.81, batch loss = 0.74 (9.8 examples/sec; 0.820 sec/batch; 70h:42m:23s remains)
INFO - root - 2017-12-07 08:10:26.627482: step 22080, loss = 0.77, batch loss = 0.70 (9.9 examples/sec; 0.807 sec/batch; 69h:33m:06s remains)
INFO - root - 2017-12-07 08:10:35.076811: step 22090, loss = 0.80, batch loss = 0.73 (9.3 examples/sec; 0.862 sec/batch; 74h:17m:40s remains)
INFO - root - 2017-12-07 08:10:43.368612: step 22100, loss = 1.01, batch loss = 0.94 (9.3 examples/sec; 0.861 sec/batch; 74h:13m:17s remains)
2017-12-07 08:10:44.049324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2410185 -2.6396055 -2.0703897 -1.8303356 -2.0363629 -2.6380482 -3.049237 -2.7986264 -1.9578571 -1.0398486 -0.49886847 -0.41902208 -0.7988615 -1.6824052 -2.649251][-3.223877 -2.6103258 -2.1328475 -2.0164909 -2.3233538 -2.9318581 -3.2621331 -2.8814011 -1.9554913 -0.92157269 -0.12927675 0.17035198 -0.22011137 -1.3687077 -2.6483011][-2.8966017 -2.4197512 -2.0836515 -1.9927859 -2.2400506 -2.7467976 -2.9806097 -2.6170926 -1.8545032 -0.9392221 -0.077699184 0.41663027 0.11305332 -1.1479423 -2.6206322][-2.363718 -2.0898263 -1.8328836 -1.6699557 -1.8437767 -2.3009799 -2.5029941 -2.2364061 -1.6254432 -0.81216717 0.030380726 0.60949707 0.36071062 -0.91560626 -2.5118854][-1.8073807 -1.6020801 -1.274416 -0.99323583 -1.146699 -1.619288 -1.822825 -1.631654 -1.088886 -0.35754108 0.30729771 0.72896147 0.38252115 -0.87839985 -2.4983306][-1.1587958 -0.840137 -0.41661644 -0.090126038 -0.20583916 -0.5606916 -0.6866467 -0.55950451 -0.16403103 0.32555962 0.68196297 0.78683758 0.25899124 -0.98743033 -2.5397234][-0.6384747 -0.24811316 0.072407246 0.33255196 0.39876127 0.35585737 0.38697052 0.44586897 0.58793545 0.72888088 0.75833845 0.597033 -0.012774467 -1.2012663 -2.6297112][-0.25691986 0.081007004 0.17093229 0.26522923 0.46837282 0.64776182 0.79214334 0.84777737 0.86861134 0.82437849 0.67529964 0.36743164 -0.25942421 -1.4290147 -2.7451708][0.22260571 0.35940075 0.19794416 0.017951012 0.060462952 0.14738607 0.22080231 0.24796581 0.24431467 0.2615509 0.21226501 -0.0088887215 -0.53087425 -1.6332092 -2.8246307][0.58073711 0.53585768 0.16883659 -0.32905817 -0.63030291 -0.7741642 -0.85623646 -0.91668558 -0.91784048 -0.809448 -0.75312304 -0.84148383 -1.1949329 -2.0964022 -3.0222182][0.079331875 0.0026531219 -0.32371902 -0.89384747 -1.3611681 -1.5672858 -1.6841652 -1.7904062 -1.7966905 -1.7196295 -1.7133069 -1.7520335 -1.9434271 -2.5798211 -3.1873193][-0.76979017 -0.94559336 -1.2063746 -1.6867511 -2.0933743 -2.1637728 -2.1438875 -2.18287 -2.1801956 -2.2345281 -2.3742566 -2.4106314 -2.4853303 -2.8562703 -3.1809525][-1.3894641 -1.6090567 -1.8546157 -2.2708025 -2.6266227 -2.6360054 -2.5572128 -2.4734178 -2.3389862 -2.3764489 -2.5231919 -2.5452733 -2.5788555 -2.8086839 -3.002115][-2.0612726 -2.1856115 -2.3168247 -2.6017003 -2.8876767 -2.9268851 -2.9385791 -2.8601859 -2.6746135 -2.6288481 -2.6643958 -2.6202319 -2.6019444 -2.7219577 -2.8263659][-2.6167974 -2.7143548 -2.7892685 -2.936738 -3.0787826 -3.080277 -3.0951409 -3.0180228 -2.8600249 -2.8157663 -2.8088503 -2.751142 -2.6995995 -2.7223294 -2.7415314]]...]
INFO - root - 2017-12-07 08:10:52.523713: step 22110, loss = 0.80, batch loss = 0.73 (9.6 examples/sec; 0.834 sec/batch; 71h:56m:23s remains)
INFO - root - 2017-12-07 08:11:00.908039: step 22120, loss = 0.75, batch loss = 0.68 (9.5 examples/sec; 0.843 sec/batch; 72h:42m:06s remains)
INFO - root - 2017-12-07 08:11:09.293150: step 22130, loss = 0.77, batch loss = 0.70 (8.9 examples/sec; 0.895 sec/batch; 77h:09m:55s remains)
INFO - root - 2017-12-07 08:11:17.602728: step 22140, loss = 0.70, batch loss = 0.63 (9.3 examples/sec; 0.864 sec/batch; 74h:28m:57s remains)
INFO - root - 2017-12-07 08:11:25.976592: step 22150, loss = 0.84, batch loss = 0.77 (9.8 examples/sec; 0.816 sec/batch; 70h:20m:54s remains)
INFO - root - 2017-12-07 08:11:34.259293: step 22160, loss = 0.70, batch loss = 0.63 (9.7 examples/sec; 0.822 sec/batch; 70h:49m:24s remains)
INFO - root - 2017-12-07 08:11:42.746148: step 22170, loss = 0.63, batch loss = 0.56 (9.3 examples/sec; 0.865 sec/batch; 74h:31m:35s remains)
INFO - root - 2017-12-07 08:11:51.069244: step 22180, loss = 0.82, batch loss = 0.75 (9.6 examples/sec; 0.834 sec/batch; 71h:53m:19s remains)
INFO - root - 2017-12-07 08:11:59.559932: step 22190, loss = 0.72, batch loss = 0.65 (8.8 examples/sec; 0.908 sec/batch; 78h:15m:17s remains)
INFO - root - 2017-12-07 08:12:07.930849: step 22200, loss = 0.67, batch loss = 0.60 (9.7 examples/sec; 0.823 sec/batch; 70h:53m:47s remains)
2017-12-07 08:12:08.600534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6892164 -3.3339493 -3.2559309 -2.5356717 -2.0849783 -2.1280816 -2.3663414 -2.8539391 -2.9456735 -2.7821374 -2.4651589 -2.0359666 -1.9864569 -2.2248013 -2.3312333][-2.2853041 -2.8866625 -2.8380148 -2.2643156 -1.8806705 -1.8524783 -1.9403801 -2.2469485 -2.2578053 -2.1289027 -2.0023801 -1.810647 -1.940336 -2.2898993 -2.4873757][-2.3485906 -2.7750888 -2.6859221 -2.1860797 -1.7563431 -1.4885159 -1.285145 -1.3378811 -1.3828077 -1.5439973 -1.8221865 -1.9214334 -2.064564 -2.2962611 -2.404542][-2.609617 -3.0011387 -3.1114786 -2.8905263 -2.5702977 -2.1485007 -1.6409528 -1.3437006 -1.3364983 -1.7729437 -2.3714266 -2.5394018 -2.3993208 -2.3213277 -2.3259978][-2.1737878 -2.6409407 -3.073945 -3.2520337 -3.2017806 -2.871717 -2.351428 -1.9405687 -1.836472 -2.3103831 -2.9638531 -3.0538557 -2.5881011 -2.175411 -2.0822198][-1.4242623 -1.8153093 -2.2445803 -2.4412603 -2.4022884 -2.1291816 -1.7159076 -1.377646 -1.2688644 -1.6995633 -2.3461111 -2.5348606 -2.1073363 -1.5728476 -1.285342][-0.87632632 -0.947073 -1.0485907 -0.9168725 -0.6329565 -0.33473015 -0.13528109 -0.10892248 -0.1832366 -0.59496117 -1.1098776 -1.2891066 -0.9420085 -0.34215689 0.058974266][-0.43031406 -0.36358833 -0.41361046 -0.28663635 0.0035533905 0.22748041 0.25437307 0.088270664 -0.11428261 -0.50805664 -0.93952513 -1.0432105 -0.56147385 0.32336855 0.905468][-0.56543088 -0.68266034 -0.95392656 -1.1201227 -1.0582259 -0.924906 -0.87201095 -0.91507459 -0.99353743 -1.2030764 -1.4861507 -1.565784 -1.0740764 -0.098110676 0.52882814][-0.71190786 -1.1002145 -1.6375017 -2.0869074 -2.2094572 -2.0863776 -1.9155209 -1.7992382 -1.7950606 -1.8758636 -2.023402 -2.0648446 -1.6854393 -0.95801663 -0.55568576][-0.71197963 -0.94074726 -1.3949318 -1.8816931 -2.071425 -1.9176912 -1.6204264 -1.4389572 -1.5208952 -1.7153735 -1.9438057 -2.0614495 -1.8481209 -1.4252737 -1.218348][-1.4333444 -1.2638206 -1.353034 -1.5372081 -1.5451663 -1.3164749 -1.0118361 -0.92572045 -1.1589477 -1.4863489 -1.7743914 -1.9051483 -1.763068 -1.5063319 -1.3917522][-2.5220313 -2.3072073 -2.31491 -2.3326695 -2.1796665 -1.8920546 -1.6349914 -1.6492944 -1.9140615 -2.1894238 -2.3634188 -2.3560979 -2.2001119 -2.024425 -1.9297168][-3.108243 -3.0008202 -3.0646248 -3.1251953 -3.0629444 -2.902235 -2.7284913 -2.72639 -2.88194 -3.0289211 -3.068836 -2.9400845 -2.7841198 -2.7054181 -2.6552377][-3.1361122 -3.05579 -3.0481086 -3.0610657 -3.0849679 -3.1017356 -3.0669155 -3.0782461 -3.1474352 -3.2168539 -3.23272 -3.1365151 -3.1028516 -3.1814091 -3.2600036]]...]
INFO - root - 2017-12-07 08:12:16.929601: step 22210, loss = 0.76, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 70h:24m:03s remains)
INFO - root - 2017-12-07 08:12:25.313754: step 22220, loss = 0.78, batch loss = 0.71 (9.0 examples/sec; 0.887 sec/batch; 76h:28m:16s remains)
INFO - root - 2017-12-07 08:12:33.784467: step 22230, loss = 0.85, batch loss = 0.78 (9.5 examples/sec; 0.843 sec/batch; 72h:38m:57s remains)
INFO - root - 2017-12-07 08:12:42.137703: step 22240, loss = 0.74, batch loss = 0.67 (9.9 examples/sec; 0.808 sec/batch; 69h:36m:09s remains)
INFO - root - 2017-12-07 08:12:50.448283: step 22250, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.835 sec/batch; 71h:55m:54s remains)
INFO - root - 2017-12-07 08:12:58.752294: step 22260, loss = 0.59, batch loss = 0.52 (9.8 examples/sec; 0.814 sec/batch; 70h:09m:51s remains)
INFO - root - 2017-12-07 08:13:07.320827: step 22270, loss = 0.58, batch loss = 0.51 (9.2 examples/sec; 0.874 sec/batch; 75h:20m:28s remains)
INFO - root - 2017-12-07 08:13:15.607119: step 22280, loss = 0.68, batch loss = 0.61 (8.7 examples/sec; 0.918 sec/batch; 79h:05m:11s remains)
INFO - root - 2017-12-07 08:13:23.961058: step 22290, loss = 0.73, batch loss = 0.66 (9.6 examples/sec; 0.830 sec/batch; 71h:31m:22s remains)
INFO - root - 2017-12-07 08:13:32.316045: step 22300, loss = 0.69, batch loss = 0.62 (9.4 examples/sec; 0.848 sec/batch; 73h:03m:30s remains)
2017-12-07 08:13:33.028076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3538709 -4.3748584 -4.4686112 -4.5379167 -4.4884815 -4.1637969 -3.504396 -2.8845 -2.5730338 -2.6692948 -3.1826205 -3.711658 -4.0525346 -3.9956193 -3.7326431][-4.7544012 -4.8914175 -5.1092634 -5.233716 -5.0705013 -4.4849572 -3.4966984 -2.5754566 -2.0468714 -2.1312282 -2.8345356 -3.5412683 -3.9764063 -3.8833284 -3.5438898][-4.9455581 -5.1645026 -5.4240117 -5.5412269 -5.3027935 -4.5830069 -3.342371 -2.0712829 -1.2520554 -1.3883483 -2.3424745 -3.2174764 -3.7111988 -3.6045854 -3.2729206][-4.7676668 -4.954793 -5.1188474 -5.1776204 -4.9374781 -4.156137 -2.6997294 -1.0814183 -0.080860138 -0.52734661 -1.8550186 -2.89963 -3.3825092 -3.2100015 -2.938839][-4.3229923 -4.4064169 -4.406981 -4.3865724 -4.0895019 -3.1108465 -1.3536608 0.53616428 1.3600597 0.27695513 -1.4964969 -2.687717 -3.0686278 -2.7643938 -2.533658][-3.9274168 -3.9670563 -3.9437356 -3.9538877 -3.5045929 -2.0888534 0.13129473 2.2599196 2.5685911 0.67726564 -1.4535446 -2.6682248 -2.9189546 -2.5186033 -2.2984517][-3.7760184 -3.8302574 -3.8913546 -3.9914522 -3.3708291 -1.572659 0.99399662 3.2130775 2.9293671 0.49734831 -1.6698401 -2.7155838 -2.8088188 -2.38115 -2.2090979][-3.8275251 -3.9353888 -4.1148348 -4.2743778 -3.5376961 -1.6119909 0.96945095 2.9481435 2.3460264 -0.0045022964 -1.9156849 -2.7837591 -2.7946477 -2.3565669 -2.1824532][-3.8492742 -4.0050254 -4.286788 -4.4442329 -3.650579 -1.8493457 0.37344408 1.8396835 1.2556052 -0.55713344 -2.0844305 -2.8442447 -2.8612823 -2.4122138 -2.191726][-3.6410849 -3.7809114 -4.0925651 -4.2193408 -3.4331512 -1.8926914 -0.15524912 0.79926157 0.30103779 -1.0065651 -2.2168381 -2.8795714 -2.880543 -2.4735563 -2.2496555][-3.2694366 -3.347652 -3.6439233 -3.7641506 -3.0758629 -1.7947154 -0.41132832 0.2398572 -0.26229334 -1.3936086 -2.4476042 -2.9429545 -2.847934 -2.525094 -2.4000823][-2.934504 -2.8987741 -3.1070147 -3.2140894 -2.6941574 -1.7010555 -0.59330344 -0.077826023 -0.56811643 -1.6776555 -2.7057784 -3.1295261 -2.9943624 -2.7590735 -2.7070584][-2.6953042 -2.5272822 -2.6090102 -2.6884484 -2.3504505 -1.649411 -0.81309986 -0.42229986 -0.87718749 -1.9288628 -2.9038525 -3.3152444 -3.1964893 -3.0284128 -2.9807777][-2.49087 -2.2103164 -2.2160048 -2.2863917 -2.0696061 -1.5877266 -1.0302541 -0.84828329 -1.2842937 -2.1402895 -2.9500897 -3.3077097 -3.2470107 -3.1484551 -3.097455][-2.4158537 -2.0697155 -2.055675 -2.1499739 -2.0198879 -1.6861408 -1.3521543 -1.3561354 -1.7582176 -2.3588231 -2.93225 -3.2123518 -3.2254677 -3.2021279 -3.169795]]...]
INFO - root - 2017-12-07 08:13:41.229920: step 22310, loss = 0.80, batch loss = 0.73 (9.5 examples/sec; 0.842 sec/batch; 72h:32m:09s remains)
INFO - root - 2017-12-07 08:13:49.650447: step 22320, loss = 0.68, batch loss = 0.61 (9.4 examples/sec; 0.847 sec/batch; 73h:00m:49s remains)
INFO - root - 2017-12-07 08:13:57.689765: step 22330, loss = 0.99, batch loss = 0.92 (9.4 examples/sec; 0.848 sec/batch; 73h:03m:50s remains)
INFO - root - 2017-12-07 08:14:06.001855: step 22340, loss = 0.76, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 69h:01m:50s remains)
INFO - root - 2017-12-07 08:14:14.457255: step 22350, loss = 0.78, batch loss = 0.71 (9.4 examples/sec; 0.849 sec/batch; 73h:07m:07s remains)
INFO - root - 2017-12-07 08:14:22.881795: step 22360, loss = 0.75, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 76h:19m:20s remains)
INFO - root - 2017-12-07 08:14:31.189316: step 22370, loss = 0.83, batch loss = 0.76 (9.7 examples/sec; 0.821 sec/batch; 70h:45m:27s remains)
INFO - root - 2017-12-07 08:14:39.533579: step 22380, loss = 0.71, batch loss = 0.63 (9.6 examples/sec; 0.829 sec/batch; 71h:26m:39s remains)
INFO - root - 2017-12-07 08:14:47.945091: step 22390, loss = 0.63, batch loss = 0.56 (9.9 examples/sec; 0.811 sec/batch; 69h:50m:07s remains)
INFO - root - 2017-12-07 08:14:56.347635: step 22400, loss = 0.67, batch loss = 0.60 (9.5 examples/sec; 0.843 sec/batch; 72h:38m:40s remains)
2017-12-07 08:14:57.040434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.105566 -3.3921838 -3.6759052 -3.8233953 -3.860611 -3.8248084 -3.8084626 -3.8047786 -3.7748516 -3.7236538 -3.670032 -3.6065469 -3.5458803 -3.4823649 -3.42447][-3.0714967 -3.3626249 -3.6133251 -3.6863306 -3.6450191 -3.5407948 -3.5165746 -3.5500703 -3.5705538 -3.53536 -3.477581 -3.4042406 -3.3276556 -3.2596078 -3.2326143][-2.9134235 -3.1327629 -3.2628527 -3.2025928 -3.053015 -2.8743863 -2.8452888 -2.9395742 -3.0534458 -3.0502315 -2.966964 -2.8696458 -2.7787769 -2.7382517 -2.7987795][-2.6976089 -2.8203759 -2.8289876 -2.6727209 -2.4892311 -2.3131628 -2.3118124 -2.4629369 -2.6324878 -2.6291957 -2.4920416 -2.3559055 -2.2410307 -2.2171173 -2.3714564][-2.5166328 -2.5717406 -2.5245557 -2.3778603 -2.2859094 -2.2339792 -2.337111 -2.5339069 -2.6836958 -2.5967894 -2.3403206 -2.0979905 -1.9023809 -1.8290312 -2.0257528][-2.3891923 -2.4125788 -2.35189 -2.2370081 -2.2206483 -2.269388 -2.46143 -2.7081032 -2.8496852 -2.6855822 -2.3193555 -1.968174 -1.6900725 -1.5748103 -1.8039129][-2.334722 -2.3705692 -2.3360195 -2.2546532 -2.2336593 -2.2777605 -2.4651146 -2.7560196 -2.9595294 -2.8213725 -2.4426236 -2.0380867 -1.6890457 -1.5187502 -1.740236][-2.3535676 -2.4010246 -2.4077859 -2.3501441 -2.2704759 -2.2338817 -2.354634 -2.689044 -3.0177436 -3.0639577 -2.8646674 -2.534091 -2.1524434 -1.8832843 -2.0001698][-2.3256431 -2.341275 -2.3948298 -2.3929422 -2.2935798 -2.1941872 -2.2530339 -2.5807118 -2.9729123 -3.1892309 -3.197547 -2.9820681 -2.6343451 -2.3424509 -2.3892086][-2.2680213 -2.2485237 -2.3644457 -2.4708655 -2.4202302 -2.3081355 -2.3061326 -2.5374799 -2.860142 -3.1123986 -3.2183433 -3.0863819 -2.8107195 -2.5753553 -2.6419206][-2.2681053 -2.2500596 -2.4267728 -2.6208062 -2.6067805 -2.4786291 -2.3638635 -2.3893301 -2.5245528 -2.7192907 -2.9013119 -2.9281693 -2.8205495 -2.6905832 -2.7675805][-2.2805023 -2.2695944 -2.4633679 -2.6732845 -2.6696105 -2.5520964 -2.3995674 -2.296325 -2.2952566 -2.4103692 -2.6100502 -2.7525716 -2.7992043 -2.7749822 -2.8548248][-2.2856457 -2.265367 -2.430974 -2.611721 -2.6111765 -2.5262504 -2.4220417 -2.3516927 -2.3897183 -2.5131292 -2.6788404 -2.7975278 -2.8550491 -2.8465629 -2.8812222][-2.3061738 -2.26253 -2.376924 -2.5260463 -2.5592856 -2.5333121 -2.5025711 -2.4931426 -2.5800478 -2.7205069 -2.835835 -2.8535645 -2.8005605 -2.6779449 -2.57054][-2.3950815 -2.3500664 -2.4148734 -2.523603 -2.5741704 -2.5948191 -2.6166582 -2.6419294 -2.7107773 -2.7922177 -2.8054807 -2.6893563 -2.5167303 -2.3122478 -2.1482494]]...]
INFO - root - 2017-12-07 08:15:05.513142: step 22410, loss = 0.71, batch loss = 0.64 (9.2 examples/sec; 0.869 sec/batch; 74h:51m:03s remains)
INFO - root - 2017-12-07 08:15:13.982838: step 22420, loss = 0.85, batch loss = 0.78 (9.7 examples/sec; 0.827 sec/batch; 71h:15m:09s remains)
INFO - root - 2017-12-07 08:15:22.283522: step 22430, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.812 sec/batch; 69h:55m:07s remains)
INFO - root - 2017-12-07 08:15:30.667934: step 22440, loss = 0.79, batch loss = 0.72 (9.6 examples/sec; 0.835 sec/batch; 71h:52m:42s remains)
INFO - root - 2017-12-07 08:15:38.971063: step 22450, loss = 0.60, batch loss = 0.52 (9.0 examples/sec; 0.886 sec/batch; 76h:16m:53s remains)
INFO - root - 2017-12-07 08:15:47.303440: step 22460, loss = 0.71, batch loss = 0.63 (9.8 examples/sec; 0.819 sec/batch; 70h:31m:36s remains)
INFO - root - 2017-12-07 08:15:55.688075: step 22470, loss = 0.76, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 70h:59m:04s remains)
INFO - root - 2017-12-07 08:16:03.979507: step 22480, loss = 1.21, batch loss = 1.13 (9.5 examples/sec; 0.846 sec/batch; 72h:50m:22s remains)
INFO - root - 2017-12-07 08:16:12.249062: step 22490, loss = 0.75, batch loss = 0.68 (9.9 examples/sec; 0.805 sec/batch; 69h:18m:23s remains)
INFO - root - 2017-12-07 08:16:20.577396: step 22500, loss = 0.65, batch loss = 0.58 (9.5 examples/sec; 0.842 sec/batch; 72h:28m:46s remains)
2017-12-07 08:16:21.198642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.36804056 -0.34698677 -0.28277969 -0.23771381 -0.26646423 -0.27224207 -0.3182354 -0.42079926 -0.572572 -0.85019183 -1.0556824 -1.1738386 -1.4515924 -1.8015513 -2.152329][-0.42282915 -0.42995095 -0.37710953 -0.33641148 -0.34566784 -0.30092144 -0.35719776 -0.547009 -0.76170683 -0.9934504 -1.0333385 -0.95168757 -1.1101882 -1.4599981 -1.9348257][-0.012567997 -0.018399715 0.067666054 0.13428974 0.148108 0.21317053 0.095535755 -0.24183607 -0.64028215 -0.98922777 -1.0359948 -0.89072204 -0.92246652 -1.1401889 -1.5362022][0.47302914 0.46512938 0.58817863 0.67377377 0.70118093 0.75615025 0.57185507 0.12048435 -0.43604159 -0.91932487 -1.0453725 -0.9198122 -0.8690114 -0.95616627 -1.2358253][0.58190346 0.58252 0.72837114 0.78920078 0.78553009 0.79907417 0.58763075 0.1379509 -0.45670128 -0.97630429 -1.1334078 -1.0396605 -0.95465684 -0.9753089 -1.1466057][0.52025747 0.51170254 0.60958433 0.54432917 0.41976261 0.31916142 0.10979891 -0.2099328 -0.6725862 -1.0657833 -1.1642268 -1.1057756 -1.0697787 -1.119802 -1.231611][0.44479752 0.41683674 0.42141104 0.18514061 -0.077298164 -0.29760647 -0.47505593 -0.60226274 -0.82974911 -1.0046008 -1.0037119 -1.0204601 -1.1051488 -1.2362151 -1.3281755][0.34862566 0.28897524 0.20172167 -0.15145922 -0.47711325 -0.7524159 -0.86351252 -0.80505323 -0.85303235 -0.89702725 -0.87349367 -1.0402994 -1.2767613 -1.4482219 -1.4824729][0.31048107 0.22367477 0.10636711 -0.290514 -0.66348243 -1.0637565 -1.2475755 -1.1770861 -1.2200139 -1.2359664 -1.1734011 -1.3569038 -1.6095626 -1.6967096 -1.6513767][0.30917931 0.21431446 0.13081646 -0.24912596 -0.6728797 -1.2678757 -1.6959095 -1.8469348 -2.0892816 -2.163667 -2.0513902 -2.1314309 -2.2467995 -2.1201174 -1.9858654][0.22365808 0.11766195 0.059372425 -0.27086067 -0.67402411 -1.3675282 -1.9851336 -2.3635013 -2.768007 -2.8955717 -2.7888579 -2.7898431 -2.7962739 -2.5267277 -2.4091249][-0.12213087 -0.24183512 -0.27523279 -0.5064919 -0.80227113 -1.4573078 -2.1521175 -2.6731911 -3.1110668 -3.2388015 -3.1212289 -2.9674621 -2.8171034 -2.4932404 -2.5129919][-0.65185285 -0.81363463 -0.81623077 -0.86039233 -0.94068551 -1.4171596 -2.0546403 -2.6137118 -3.1017728 -3.3393807 -3.2431238 -2.866569 -2.5007486 -2.1704695 -2.3693182][-0.7801466 -0.98085856 -0.99431658 -0.96422005 -0.93847823 -1.2663958 -1.8041897 -2.3546093 -2.9210114 -3.2644162 -3.119308 -2.5142851 -2.0047827 -1.760802 -2.1600339][-0.43303919 -0.56886196 -0.58014536 -0.57718778 -0.58018279 -0.83374166 -1.2431922 -1.6960123 -2.2851121 -2.7109463 -2.627949 -2.1141138 -1.7432239 -1.7131228 -2.2957573]]...]
INFO - root - 2017-12-07 08:16:29.581880: step 22510, loss = 0.77, batch loss = 0.70 (9.8 examples/sec; 0.819 sec/batch; 70h:32m:08s remains)
INFO - root - 2017-12-07 08:16:37.885545: step 22520, loss = 0.96, batch loss = 0.88 (9.3 examples/sec; 0.863 sec/batch; 74h:19m:51s remains)
INFO - root - 2017-12-07 08:16:46.251846: step 22530, loss = 0.70, batch loss = 0.63 (9.4 examples/sec; 0.853 sec/batch; 73h:28m:45s remains)
INFO - root - 2017-12-07 08:16:54.619972: step 22540, loss = 0.91, batch loss = 0.83 (9.3 examples/sec; 0.861 sec/batch; 74h:08m:34s remains)
INFO - root - 2017-12-07 08:17:03.002580: step 22550, loss = 0.70, batch loss = 0.63 (10.1 examples/sec; 0.793 sec/batch; 68h:18m:10s remains)
INFO - root - 2017-12-07 08:17:11.356364: step 22560, loss = 1.00, batch loss = 0.93 (9.6 examples/sec; 0.833 sec/batch; 71h:42m:04s remains)
INFO - root - 2017-12-07 08:17:19.687966: step 22570, loss = 0.76, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 72h:22m:04s remains)
INFO - root - 2017-12-07 08:17:28.055013: step 22580, loss = 0.91, batch loss = 0.83 (10.0 examples/sec; 0.802 sec/batch; 69h:00m:23s remains)
INFO - root - 2017-12-07 08:17:36.426637: step 22590, loss = 0.82, batch loss = 0.75 (9.6 examples/sec; 0.835 sec/batch; 71h:53m:44s remains)
INFO - root - 2017-12-07 08:17:44.712505: step 22600, loss = 0.99, batch loss = 0.92 (10.2 examples/sec; 0.787 sec/batch; 67h:45m:09s remains)
2017-12-07 08:17:45.350749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.80833268 -0.892442 -0.75423837 -0.33819437 0.20533848 0.78983831 1.2678905 1.489121 1.3601599 0.91122437 0.36158752 -0.16061068 -0.62249541 -1.0561955 -1.2944574][-0.64965248 -0.55991793 -0.38568592 -0.23853064 -0.18480396 -0.083904266 0.16148472 0.53675175 0.80246782 0.77802992 0.45941782 -0.042266369 -0.54721785 -1.018127 -1.2940109][-0.46793628 -0.1666255 0.057518482 -0.018779278 -0.33800173 -0.62221313 -0.6929822 -0.47562623 -0.21163321 -0.0778985 -0.15179539 -0.38025856 -0.6874547 -1.0524848 -1.312983][-0.38910723 -0.00889492 0.22505379 0.11186218 -0.26323366 -0.63143826 -0.85808325 -0.90210462 -0.85151291 -0.78768325 -0.77689528 -0.81800151 -0.94001651 -1.1588447 -1.3392999][-0.26669693 -0.025047302 0.12297153 0.16683388 0.09970808 -0.014525414 -0.18196583 -0.44176674 -0.68620777 -0.8423934 -0.9495461 -1.026283 -1.1450188 -1.3136284 -1.4288483][-0.22801161 -0.22849035 -0.24855518 -0.066889763 0.25700569 0.62380648 0.79783487 0.598217 0.161026 -0.27991915 -0.66220379 -0.93198204 -1.1353369 -1.3168085 -1.4569788][-0.379889 -0.52406859 -0.64177179 -0.49020529 -0.036476135 0.693768 1.3852816 1.6358089 1.31078 0.63418007 -0.089215279 -0.60039473 -0.87442136 -1.0810325 -1.3248928][-0.61616468 -0.74092364 -0.8500154 -0.76367259 -0.37655258 0.40291309 1.353786 2.0800943 2.135087 1.5181646 0.68110657 -0.029941559 -0.48763013 -0.83893085 -1.2132361][-0.88867712 -0.90161777 -0.90773106 -0.8153441 -0.55819607 -0.059956074 0.56101465 1.1725254 1.50424 1.4897842 1.1391897 0.55789328 -0.097962379 -0.71300268 -1.2114294][-1.0437729 -0.9484694 -0.78083825 -0.58050895 -0.429399 -0.33279896 -0.26553535 -0.10063982 0.15961885 0.53275776 0.74381828 0.57426882 0.0023727417 -0.69665456 -1.2322512][-1.136735 -0.97085953 -0.61524916 -0.24116039 -0.13057947 -0.3292408 -0.65356755 -0.89900041 -0.9252131 -0.59614396 -0.24081087 -0.10753059 -0.36775589 -0.88523412 -1.32586][-1.2752917 -1.0278049 -0.55390239 -0.07378912 0.061038017 -0.22983694 -0.71227646 -1.2227907 -1.5153825 -1.4077263 -1.1461048 -0.91992784 -0.94397163 -1.2117946 -1.4822609][-1.482017 -1.1990178 -0.78330088 -0.29707479 0.054597378 0.0961442 -0.11486244 -0.5496695 -0.97311282 -1.1708612 -1.2381561 -1.2365475 -1.3117921 -1.4696047 -1.6240165][-1.8127561 -1.5812955 -1.3444829 -0.94833255 -0.36307859 0.22995186 0.60315895 0.546731 0.13480186 -0.34213638 -0.76165533 -1.0668459 -1.2894154 -1.4862673 -1.6650982][-2.1643147 -1.9930379 -1.8716171 -1.5541182 -0.89134336 0.030677795 0.94462204 1.439539 1.2602901 0.65411425 -0.007174015 -0.54104948 -0.8886323 -1.1867864 -1.5046873]]...]
INFO - root - 2017-12-07 08:17:53.707728: step 22610, loss = 0.74, batch loss = 0.67 (9.4 examples/sec; 0.854 sec/batch; 73h:30m:04s remains)
INFO - root - 2017-12-07 08:18:02.056563: step 22620, loss = 0.87, batch loss = 0.79 (10.0 examples/sec; 0.796 sec/batch; 68h:31m:13s remains)
INFO - root - 2017-12-07 08:18:10.518705: step 22630, loss = 0.73, batch loss = 0.66 (9.4 examples/sec; 0.853 sec/batch; 73h:26m:05s remains)
INFO - root - 2017-12-07 08:18:18.561529: step 22640, loss = 0.86, batch loss = 0.79 (9.6 examples/sec; 0.836 sec/batch; 71h:57m:56s remains)
INFO - root - 2017-12-07 08:18:26.855778: step 22650, loss = 0.61, batch loss = 0.54 (9.7 examples/sec; 0.826 sec/batch; 71h:07m:39s remains)
INFO - root - 2017-12-07 08:18:35.277784: step 22660, loss = 0.81, batch loss = 0.73 (9.3 examples/sec; 0.860 sec/batch; 74h:02m:58s remains)
INFO - root - 2017-12-07 08:18:43.602712: step 22670, loss = 0.68, batch loss = 0.61 (9.0 examples/sec; 0.889 sec/batch; 76h:30m:51s remains)
INFO - root - 2017-12-07 08:18:51.945711: step 22680, loss = 0.80, batch loss = 0.73 (9.5 examples/sec; 0.844 sec/batch; 72h:39m:14s remains)
INFO - root - 2017-12-07 08:19:00.310395: step 22690, loss = 0.92, batch loss = 0.85 (9.8 examples/sec; 0.820 sec/batch; 70h:36m:02s remains)
INFO - root - 2017-12-07 08:19:08.689933: step 22700, loss = 0.84, batch loss = 0.76 (9.2 examples/sec; 0.868 sec/batch; 74h:42m:53s remains)
2017-12-07 08:19:09.364342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7800903 -3.8387165 -3.8663268 -3.879039 -3.7718637 -3.6239474 -3.5034428 -3.3473787 -3.2868538 -3.2415452 -2.9246712 -2.4978325 -2.3434594 -2.5135779 -2.8196969][-3.7944565 -3.7960672 -3.7291815 -3.6543946 -3.4152815 -3.1768832 -3.0344291 -2.8085451 -2.6278384 -2.4504945 -2.0356708 -1.5972502 -1.5548255 -1.9427288 -2.3070002][-3.7763977 -3.6921494 -3.5457397 -3.4132607 -3.0348439 -2.6787915 -2.530086 -2.2113342 -1.8012824 -1.5195432 -1.176301 -0.88772631 -1.0811896 -1.6257708 -1.8115819][-3.7368393 -3.5466173 -3.3541443 -3.1679697 -2.5694985 -1.9609821 -1.7231266 -1.2818761 -0.72657323 -0.60073757 -0.5776329 -0.56051421 -1.0534182 -1.6494613 -1.4704149][-3.7425423 -3.445518 -3.2310357 -2.9809632 -2.1159232 -1.204042 -0.81050324 -0.26559782 0.15999031 -0.19635582 -0.55999422 -0.6678555 -1.303952 -1.8093176 -1.2140653][-3.8365431 -3.4666224 -3.2288136 -2.8609352 -1.7262409 -0.60081363 -0.016510487 0.73970604 0.9231329 -0.058950424 -0.70898604 -0.73496938 -1.2595675 -1.5226574 -0.64739394][-3.9606917 -3.553767 -3.2607994 -2.6868212 -1.3228142 -0.12463808 0.65055132 1.7100673 1.7666254 0.31551838 -0.50315213 -0.4696002 -0.81015444 -0.75763774 0.16984177][-4.0527129 -3.6542537 -3.2711458 -2.4801764 -1.0651741 -0.022977829 0.71350384 1.8063474 1.8396044 0.42701721 -0.30130625 -0.28026485 -0.47164178 -0.13808298 0.60782194][-4.1183987 -3.8118386 -3.3707266 -2.4568348 -1.1581125 -0.3508625 0.19053745 0.94820929 0.94278622 0.029378891 -0.4201684 -0.45327783 -0.53775096 -0.084794044 0.43284369][-4.1639986 -4.0075378 -3.5777693 -2.6681571 -1.5359104 -0.80317211 -0.34773493 0.043017387 -0.0026102066 -0.42693233 -0.6477859 -0.73163056 -0.71923852 -0.28785849 0.016798973][-4.1218557 -4.0986714 -3.7694709 -3.0316424 -2.1237524 -1.3675957 -0.91758728 -0.7470305 -0.815155 -0.95324755 -1.0685065 -1.1110694 -0.95028758 -0.63386917 -0.57095218][-3.9485953 -3.9444473 -3.7081563 -3.2456589 -2.666723 -2.0637388 -1.7753844 -1.819654 -1.9296343 -1.9837873 -2.0409162 -1.9613986 -1.659071 -1.4792924 -1.6183538][-3.7020774 -3.6328514 -3.4356284 -3.2147963 -2.9724216 -2.6616724 -2.6189008 -2.8169298 -2.9653339 -3.015692 -3.0603333 -2.9308777 -2.6384268 -2.5976429 -2.8332789][-3.5498257 -3.4485919 -3.3052297 -3.2443626 -3.2252455 -3.1322742 -3.1802247 -3.336607 -3.3869817 -3.3936732 -3.4288423 -3.3770394 -3.2496986 -3.3114312 -3.5034313][-3.5603795 -3.5144944 -3.4709592 -3.5052469 -3.5823843 -3.6002653 -3.638787 -3.6633852 -3.5870633 -3.5059006 -3.4882393 -3.4878621 -3.4784856 -3.5532627 -3.6358056]]...]
INFO - root - 2017-12-07 08:19:17.677261: step 22710, loss = 0.64, batch loss = 0.57 (9.1 examples/sec; 0.878 sec/batch; 75h:34m:12s remains)
INFO - root - 2017-12-07 08:19:26.154362: step 22720, loss = 0.69, batch loss = 0.61 (10.0 examples/sec; 0.800 sec/batch; 68h:51m:00s remains)
INFO - root - 2017-12-07 08:19:34.495461: step 22730, loss = 0.72, batch loss = 0.65 (9.5 examples/sec; 0.840 sec/batch; 72h:15m:09s remains)
INFO - root - 2017-12-07 08:19:42.919931: step 22740, loss = 0.96, batch loss = 0.89 (9.5 examples/sec; 0.838 sec/batch; 72h:07m:35s remains)
INFO - root - 2017-12-07 08:19:51.175524: step 22750, loss = 0.85, batch loss = 0.77 (9.6 examples/sec; 0.835 sec/batch; 71h:51m:02s remains)
INFO - root - 2017-12-07 08:19:59.521123: step 22760, loss = 0.68, batch loss = 0.61 (9.3 examples/sec; 0.858 sec/batch; 73h:47m:48s remains)
INFO - root - 2017-12-07 08:20:07.786619: step 22770, loss = 0.96, batch loss = 0.89 (9.7 examples/sec; 0.822 sec/batch; 70h:43m:57s remains)
INFO - root - 2017-12-07 08:20:16.130186: step 22780, loss = 0.82, batch loss = 0.74 (9.8 examples/sec; 0.817 sec/batch; 70h:17m:11s remains)
INFO - root - 2017-12-07 08:20:24.469447: step 22790, loss = 0.67, batch loss = 0.60 (9.9 examples/sec; 0.809 sec/batch; 69h:34m:57s remains)
INFO - root - 2017-12-07 08:20:32.680257: step 22800, loss = 1.02, batch loss = 0.95 (9.5 examples/sec; 0.839 sec/batch; 72h:12m:57s remains)
2017-12-07 08:20:33.348812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1306109 -1.0623469 -1.0072069 -1.1092257 -0.89194846 -0.55837965 -0.47989917 -0.64877486 -0.8196671 -0.55444217 -0.29125309 -0.4340241 -0.64851928 -0.83255243 -1.1201668][-1.2265139 -1.1982443 -1.1648948 -1.2169857 -1.0195551 -0.7517488 -0.70731592 -0.83889174 -0.91359377 -0.63988638 -0.36595201 -0.40800714 -0.6062026 -0.81604195 -1.0975125][-1.2377021 -1.2642994 -1.1989229 -1.1823888 -1.1062186 -0.92605758 -0.87341666 -0.97792077 -1.0907254 -0.91742516 -0.61239433 -0.53655005 -0.71101594 -0.90087175 -1.1451194][-1.2168686 -1.2338104 -1.0198536 -0.92350721 -1.0089543 -0.84835744 -0.68243933 -0.804255 -1.1076195 -1.0967643 -0.74916434 -0.59485626 -0.75893044 -0.90010524 -1.1114364][-1.2584476 -1.1994658 -0.8081243 -0.68931508 -0.86655283 -0.54800892 -0.18936729 -0.45074534 -1.0279696 -1.1331217 -0.711195 -0.53697205 -0.66601491 -0.68264651 -0.85609341][-1.31851 -1.1522365 -0.65434551 -0.59113193 -0.73912215 -0.18620825 0.24109268 -0.29363966 -1.0718703 -1.1186101 -0.58559251 -0.40212727 -0.42529821 -0.28309822 -0.46161938][-1.3224795 -1.0677404 -0.58087063 -0.62471056 -0.66037726 0.079282284 0.37274408 -0.45903134 -1.2465963 -1.1410198 -0.65757251 -0.53515267 -0.34333515 0.042097092 -0.1253171][-1.2916055 -1.0858128 -0.75030351 -0.89498591 -0.83235812 -0.060694218 0.026887894 -0.8437705 -1.3416815 -1.1253068 -0.95119596 -0.99650788 -0.5533874 0.086772442 0.04569912][-1.2117031 -1.209511 -1.0835836 -1.2999284 -1.197669 -0.50931787 -0.561965 -1.1750236 -1.2436471 -1.0424509 -1.2607667 -1.4676461 -0.87782741 -0.19730759 -0.11567211][-1.1528001 -1.3319433 -1.3128548 -1.5861921 -1.5653358 -1.0670257 -1.1840096 -1.4657302 -1.2555451 -1.1913569 -1.6391351 -1.7717824 -1.0587187 -0.56432176 -0.50960922][-1.0716619 -1.3148692 -1.3103626 -1.6330001 -1.7651765 -1.5176833 -1.6836736 -1.7669024 -1.4764078 -1.5287578 -1.9734592 -1.8896971 -1.1071076 -0.82790732 -0.85284829][-0.989027 -1.2049673 -1.2123542 -1.5480797 -1.7401385 -1.6423883 -1.8494816 -1.9362655 -1.6986306 -1.7311003 -2.02051 -1.7744679 -1.0641479 -0.9696095 -1.0107043][-0.92282581 -1.1085842 -1.1496618 -1.4175882 -1.5515275 -1.5036647 -1.7288978 -1.9080515 -1.7768466 -1.7361462 -1.8643084 -1.5604608 -1.0208216 -1.0088079 -0.98976469][-0.86284971 -1.0008631 -1.0339744 -1.1431859 -1.1967511 -1.2070081 -1.4923368 -1.7849705 -1.7404695 -1.6467605 -1.6510665 -1.3506248 -0.97212577 -0.96122432 -0.91613841][-0.74901032 -0.80735469 -0.78692007 -0.72556448 -0.77953053 -0.91915345 -1.3045347 -1.6766722 -1.6594744 -1.5208924 -1.4439685 -1.1473558 -0.84335566 -0.79780793 -0.801574]]...]
INFO - root - 2017-12-07 08:20:41.552417: step 22810, loss = 0.67, batch loss = 0.60 (9.2 examples/sec; 0.865 sec/batch; 74h:26m:39s remains)
INFO - root - 2017-12-07 08:20:49.829565: step 22820, loss = 0.77, batch loss = 0.70 (10.3 examples/sec; 0.780 sec/batch; 67h:04m:39s remains)
INFO - root - 2017-12-07 08:20:58.227374: step 22830, loss = 0.94, batch loss = 0.87 (10.0 examples/sec; 0.798 sec/batch; 68h:36m:17s remains)
INFO - root - 2017-12-07 08:21:06.564448: step 22840, loss = 0.63, batch loss = 0.56 (9.4 examples/sec; 0.854 sec/batch; 73h:28m:19s remains)
INFO - root - 2017-12-07 08:21:14.936547: step 22850, loss = 0.66, batch loss = 0.59 (9.4 examples/sec; 0.854 sec/batch; 73h:29m:48s remains)
INFO - root - 2017-12-07 08:21:23.093183: step 22860, loss = 0.97, batch loss = 0.90 (9.5 examples/sec; 0.843 sec/batch; 72h:32m:49s remains)
INFO - root - 2017-12-07 08:21:31.397034: step 22870, loss = 0.59, batch loss = 0.52 (9.9 examples/sec; 0.805 sec/batch; 69h:14m:05s remains)
INFO - root - 2017-12-07 08:21:39.610886: step 22880, loss = 0.98, batch loss = 0.90 (10.1 examples/sec; 0.792 sec/batch; 68h:05m:02s remains)
INFO - root - 2017-12-07 08:21:47.833577: step 22890, loss = 1.08, batch loss = 1.01 (10.0 examples/sec; 0.803 sec/batch; 69h:05m:21s remains)
INFO - root - 2017-12-07 08:21:56.065229: step 22900, loss = 0.55, batch loss = 0.48 (9.6 examples/sec; 0.833 sec/batch; 71h:38m:19s remains)
2017-12-07 08:21:56.766611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9998357 -2.0853581 -2.1193676 -2.0932851 -2.0250182 -1.9969313 -1.9958723 -2.0185056 -2.0595818 -2.0652206 -2.0377727 -1.9391906 -1.8240161 -1.7464323 -1.7065301][-2.2914145 -2.3719242 -2.3943582 -2.3510203 -2.2937005 -2.3097425 -2.3195729 -2.3503392 -2.3795476 -2.3664823 -2.3650796 -2.2682018 -2.1001675 -1.9406269 -1.8209047][-2.5802703 -2.6949191 -2.7560215 -2.7094722 -2.6520832 -2.6835165 -2.6992421 -2.7568917 -2.763643 -2.6711318 -2.6506963 -2.572875 -2.4519453 -2.3354294 -2.1872919][-2.5352769 -2.7425327 -2.9372351 -2.9648633 -2.8704576 -2.7632961 -2.655406 -2.7221837 -2.7827377 -2.7354112 -2.7818961 -2.7572207 -2.7196846 -2.6895394 -2.5173402][-2.2036715 -2.4695117 -2.7844296 -2.8440642 -2.6528945 -2.398895 -2.1917946 -2.2924745 -2.379251 -2.3916948 -2.5735712 -2.6760602 -2.7822623 -2.8373678 -2.5934319][-1.692826 -1.9201462 -2.2311287 -2.1651571 -1.7070682 -1.2125537 -0.99403095 -1.3268065 -1.6415119 -1.8115833 -2.0907502 -2.2403114 -2.4698486 -2.6797929 -2.5156574][-0.94218183 -1.0118747 -1.2566242 -1.1990526 -0.72483635 -0.19635773 -0.084439754 -0.72830296 -1.3323789 -1.6815636 -1.9681995 -1.9673269 -2.0983455 -2.2994547 -2.2130682][-0.43592834 -0.47366381 -0.81243324 -1.0860825 -1.0763645 -0.97196317 -1.1579974 -1.8034275 -2.1908543 -2.2756026 -2.2651243 -2.019233 -2.0068576 -2.0914943 -2.0231092][-0.87481546 -1.0178494 -1.4851859 -2.0329881 -2.3707209 -2.4920468 -2.7250538 -3.1750333 -3.2643402 -3.0327945 -2.7255032 -2.3365066 -2.3029118 -2.3867738 -2.3980699][-2.1208954 -2.1943982 -2.5415802 -3.0165668 -3.3337595 -3.359329 -3.4182506 -3.6605186 -3.6395526 -3.4051182 -3.0987473 -2.7900743 -2.8397043 -2.9732761 -3.045594][-3.2342799 -3.1945519 -3.3223312 -3.5238135 -3.6050146 -3.4570944 -3.3446755 -3.3905582 -3.3139493 -3.1925468 -3.0818474 -2.9967232 -3.181046 -3.4017339 -3.527986][-3.4299998 -3.4179516 -3.4664989 -3.4896169 -3.40937 -3.2007895 -3.0746319 -3.0830362 -3.06938 -3.084538 -3.1174276 -3.1491518 -3.3174365 -3.4887867 -3.5839691][-2.814496 -2.8400917 -2.887682 -2.8978477 -2.8511186 -2.7646782 -2.7694302 -2.871356 -2.9585397 -3.029283 -3.040787 -3.0018177 -3.016757 -3.0100055 -2.9908679][-1.9647717 -1.990648 -2.0229175 -2.0415089 -2.0532684 -2.0815978 -2.1660311 -2.2700346 -2.3268907 -2.3407369 -2.2827892 -2.2012107 -2.160094 -2.101594 -2.065274][-2.0038207 -2.06007 -2.126574 -2.1844969 -2.2285218 -2.2772484 -2.3431692 -2.3897326 -2.3912225 -2.3602686 -2.2916973 -2.251348 -2.2634456 -2.256948 -2.2514546]]...]
INFO - root - 2017-12-07 08:22:05.080801: step 22910, loss = 0.96, batch loss = 0.89 (8.9 examples/sec; 0.896 sec/batch; 77h:04m:47s remains)
INFO - root - 2017-12-07 08:22:13.352315: step 22920, loss = 0.61, batch loss = 0.54 (10.3 examples/sec; 0.780 sec/batch; 67h:06m:26s remains)
INFO - root - 2017-12-07 08:22:21.663492: step 22930, loss = 0.64, batch loss = 0.57 (10.2 examples/sec; 0.787 sec/batch; 67h:39m:05s remains)
INFO - root - 2017-12-07 08:22:30.109989: step 22940, loss = 0.80, batch loss = 0.73 (9.1 examples/sec; 0.881 sec/batch; 75h:45m:42s remains)
INFO - root - 2017-12-07 08:22:38.235603: step 22950, loss = 0.77, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 67h:16m:25s remains)
INFO - root - 2017-12-07 08:22:46.540610: step 22960, loss = 0.72, batch loss = 0.65 (10.0 examples/sec; 0.797 sec/batch; 68h:29m:40s remains)
INFO - root - 2017-12-07 08:22:54.768292: step 22970, loss = 0.76, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 69h:44m:00s remains)
INFO - root - 2017-12-07 08:23:03.075272: step 22980, loss = 0.64, batch loss = 0.56 (9.3 examples/sec; 0.860 sec/batch; 73h:54m:24s remains)
INFO - root - 2017-12-07 08:23:11.410582: step 22990, loss = 1.12, batch loss = 1.05 (9.4 examples/sec; 0.853 sec/batch; 73h:21m:06s remains)
INFO - root - 2017-12-07 08:23:19.827682: step 23000, loss = 0.53, batch loss = 0.45 (9.3 examples/sec; 0.860 sec/batch; 73h:55m:49s remains)
2017-12-07 08:23:20.498654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.38273 -1.6467836 -1.8237407 -1.6339533 -1.0988913 -0.697428 -0.66251969 -0.79694796 -1.0911229 -1.3224087 -1.5075405 -1.4100373 -0.76615834 -0.44922686 -0.47531533][-1.427074 -1.6782005 -1.6786056 -1.3708117 -0.92617345 -0.69031215 -0.80702019 -1.1556897 -1.6928048 -2.0785589 -2.264575 -2.0906177 -1.4991429 -1.2631507 -1.3474793][-1.7112062 -2.0304077 -1.9644549 -1.6987123 -1.4549835 -1.3217738 -1.4636357 -1.8368607 -2.306829 -2.651557 -2.8720632 -2.8044169 -2.3710117 -2.1452429 -2.2384403][-1.1280985 -1.5045807 -1.5875788 -1.5505857 -1.5289803 -1.4289894 -1.4395192 -1.6305661 -1.8833396 -2.281431 -2.8177471 -3.0923803 -2.84839 -2.4501719 -2.3491478][-0.12748003 -0.5457623 -0.89463496 -1.1468105 -1.358242 -1.2994285 -1.0912874 -0.91684842 -0.811383 -1.1916752 -1.9879513 -2.659107 -2.7451506 -2.3922341 -2.1388803][0.12428904 -0.28725338 -0.83057594 -1.24297 -1.5771351 -1.560425 -1.1207702 -0.56443048 -0.14437008 -0.34130764 -0.99950838 -1.6024125 -1.8536837 -1.8606038 -1.8733554][-0.061233997 -0.38903141 -0.95001864 -1.3826931 -1.7987082 -1.9217341 -1.4709973 -0.7995553 -0.33269548 -0.37509489 -0.646677 -0.79709649 -0.88862824 -1.2236397 -1.6854844][-0.62917542 -0.85487175 -1.3495622 -1.704783 -2.0559397 -2.2069757 -1.8581698 -1.3325274 -1.1039226 -1.2300003 -1.2750778 -1.036092 -0.95554042 -1.3853819 -2.0395525][-1.1025057 -1.3303084 -1.7456717 -1.9348459 -2.1052094 -2.1477616 -1.8716023 -1.5882318 -1.6310484 -1.9965081 -2.2350111 -2.1403983 -2.1606719 -2.5318346 -2.9923339][-1.0509393 -1.3652942 -1.7198181 -1.7559061 -1.8066716 -1.7753839 -1.5731688 -1.4279652 -1.4831209 -1.8825257 -2.350534 -2.6563382 -3.0246341 -3.4228797 -3.677042][-1.2773809 -1.6191754 -1.8495185 -1.7060187 -1.6453304 -1.4948306 -1.16816 -0.89224935 -0.71381712 -1.0004616 -1.6380992 -2.3315978 -3.0672009 -3.6451316 -3.9353445][-1.3354163 -1.7098215 -1.8620639 -1.6259384 -1.5954428 -1.5158997 -1.1493239 -0.72168732 -0.30151796 -0.40665579 -1.0183675 -1.8055432 -2.5582728 -3.1416276 -3.4881094][-1.1883543 -1.4265192 -1.4089587 -1.0733354 -1.1655793 -1.2963259 -1.1013458 -0.78418374 -0.4683044 -0.65760159 -1.2738271 -1.9395964 -2.3817723 -2.7012053 -3.0060587][-1.3350976 -1.2879012 -0.95185566 -0.464118 -0.65601873 -0.94963789 -0.83255529 -0.60595679 -0.52292156 -0.95543027 -1.6484864 -2.210398 -2.3770187 -2.4281297 -2.5919926][-1.9141521 -1.5682845 -0.9410491 -0.40551186 -0.69479227 -0.9976027 -0.82109594 -0.52622747 -0.48404121 -0.93161535 -1.4101429 -1.7108471 -1.704318 -1.6639819 -1.8256705]]...]
INFO - root - 2017-12-07 08:23:28.785225: step 23010, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.840 sec/batch; 72h:14m:06s remains)
INFO - root - 2017-12-07 08:23:37.022993: step 23020, loss = 0.71, batch loss = 0.64 (10.1 examples/sec; 0.793 sec/batch; 68h:10m:47s remains)
INFO - root - 2017-12-07 08:23:45.536760: step 23030, loss = 0.96, batch loss = 0.89 (9.5 examples/sec; 0.841 sec/batch; 72h:17m:56s remains)
INFO - root - 2017-12-07 08:23:53.787000: step 23040, loss = 0.55, batch loss = 0.47 (9.3 examples/sec; 0.857 sec/batch; 73h:39m:03s remains)
INFO - root - 2017-12-07 08:24:02.200083: step 23050, loss = 0.92, batch loss = 0.85 (10.0 examples/sec; 0.803 sec/batch; 69h:00m:27s remains)
INFO - root - 2017-12-07 08:24:10.487305: step 23060, loss = 0.69, batch loss = 0.62 (9.9 examples/sec; 0.808 sec/batch; 69h:29m:09s remains)
INFO - root - 2017-12-07 08:24:18.848177: step 23070, loss = 0.81, batch loss = 0.74 (10.0 examples/sec; 0.797 sec/batch; 68h:28m:35s remains)
INFO - root - 2017-12-07 08:24:27.229740: step 23080, loss = 0.66, batch loss = 0.58 (9.4 examples/sec; 0.854 sec/batch; 73h:26m:30s remains)
INFO - root - 2017-12-07 08:24:35.570953: step 23090, loss = 0.79, batch loss = 0.72 (9.4 examples/sec; 0.849 sec/batch; 72h:55m:58s remains)
INFO - root - 2017-12-07 08:24:43.946575: step 23100, loss = 0.76, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 67h:14m:57s remains)
2017-12-07 08:24:44.567780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.01241 -1.8126438 -2.0224609 -2.5003486 -2.9508104 -3.1144137 -2.8358705 -2.3390243 -2.2161574 -2.5174689 -2.7809792 -2.8106248 -2.7786167 -2.7660463 -2.7258313][-2.6121125 -2.435854 -2.5827169 -2.7915816 -2.9286242 -2.8479753 -2.3125618 -1.7331672 -1.8549707 -2.4873304 -2.9031906 -2.9932752 -3.0376482 -3.070015 -3.0544047][-3.0519238 -2.8468716 -2.9668953 -3.0096064 -2.9822032 -2.782177 -2.0947933 -1.5875552 -1.9850483 -2.770803 -3.1861913 -3.2975435 -3.4653845 -3.5869279 -3.62014][-2.6341124 -2.3796263 -2.6278079 -2.8625422 -3.1235135 -3.1233575 -2.3791616 -1.8937302 -2.3300414 -2.9590378 -3.1118202 -3.0546837 -3.2486978 -3.4375212 -3.5285053][-1.6402445 -1.508008 -2.0370677 -2.5469809 -3.0857477 -3.150445 -2.2526882 -1.7262819 -2.1168249 -2.5126081 -2.4112911 -2.2340991 -2.40363 -2.5132523 -2.5174797][-0.96463251 -1.0872247 -1.8330984 -2.3085408 -2.5848477 -2.2060814 -0.98925972 -0.53052306 -1.0799162 -1.5027883 -1.5075798 -1.4865768 -1.6991022 -1.6662772 -1.473114][-1.1362224 -1.4190621 -2.1406245 -2.2783337 -1.9034002 -0.75800514 0.77735233 0.85393333 -0.16279316 -0.85294414 -1.1448977 -1.3708105 -1.6390109 -1.5252275 -1.2305551][-1.3517015 -1.7557464 -2.4479043 -2.3198938 -1.4086356 0.22502136 1.685792 1.1354933 -0.27334785 -1.0748439 -1.4579964 -1.7057185 -1.8644128 -1.656383 -1.3629584][-1.0593379 -1.6012814 -2.3551476 -2.2212307 -1.2826097 0.12789536 0.915586 -0.012885571 -1.2257819 -1.7397616 -1.9422212 -1.9391584 -1.7664359 -1.3597078 -1.0455654][-0.6776228 -1.2864511 -2.1133144 -2.1263459 -1.4569023 -0.60022616 -0.47492003 -1.4474363 -2.1654937 -2.2875481 -2.3625991 -2.2403715 -1.9243581 -1.4886394 -1.1954334][-0.64101481 -1.2257202 -2.0365474 -2.1353571 -1.6419387 -1.1244907 -1.3185716 -2.1532257 -2.4786005 -2.4561963 -2.6273344 -2.6083341 -2.448415 -2.2288449 -2.0231309][-1.1187527 -1.6457653 -2.3726444 -2.4143987 -1.8304152 -1.3413029 -1.5643117 -2.1569877 -2.2810783 -2.2988222 -2.4953728 -2.4939218 -2.4471421 -2.4654303 -2.4431715][-1.5581081 -2.1931176 -2.8727121 -2.7825565 -1.9285951 -1.2742083 -1.4025228 -1.7820795 -1.8418689 -1.9468937 -2.0665188 -1.9454489 -1.9325695 -2.1648784 -2.41544][-1.571358 -2.3988724 -3.1399713 -3.0102072 -2.0006456 -1.1741595 -1.1897097 -1.3874733 -1.3944824 -1.5291209 -1.5773685 -1.4890449 -1.6756904 -2.1774812 -2.6930978][-1.4575012 -2.2207489 -2.9809341 -2.90746 -2.0691531 -1.4172919 -1.4301169 -1.4374778 -1.214987 -1.1542153 -1.1616573 -1.3726413 -2.0792253 -2.9469547 -3.5887446]]...]
INFO - root - 2017-12-07 08:24:52.883898: step 23110, loss = 0.70, batch loss = 0.62 (9.5 examples/sec; 0.838 sec/batch; 72h:03m:22s remains)
INFO - root - 2017-12-07 08:25:01.228225: step 23120, loss = 0.63, batch loss = 0.56 (9.7 examples/sec; 0.828 sec/batch; 71h:10m:26s remains)
INFO - root - 2017-12-07 08:25:09.607327: step 23130, loss = 0.99, batch loss = 0.92 (9.1 examples/sec; 0.874 sec/batch; 75h:08m:57s remains)
INFO - root - 2017-12-07 08:25:17.940202: step 23140, loss = 0.83, batch loss = 0.76 (9.6 examples/sec; 0.832 sec/batch; 71h:30m:07s remains)
INFO - root - 2017-12-07 08:25:26.249258: step 23150, loss = 0.76, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 71h:22m:27s remains)
INFO - root - 2017-12-07 08:25:34.654591: step 23160, loss = 0.86, batch loss = 0.79 (9.5 examples/sec; 0.841 sec/batch; 72h:16m:18s remains)
INFO - root - 2017-12-07 08:25:42.893830: step 23170, loss = 0.61, batch loss = 0.54 (9.9 examples/sec; 0.805 sec/batch; 69h:11m:05s remains)
INFO - root - 2017-12-07 08:25:51.265027: step 23180, loss = 0.75, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 73h:11m:49s remains)
INFO - root - 2017-12-07 08:25:59.543730: step 23190, loss = 0.67, batch loss = 0.60 (9.2 examples/sec; 0.866 sec/batch; 74h:24m:58s remains)
INFO - root - 2017-12-07 08:26:07.845714: step 23200, loss = 0.60, batch loss = 0.53 (9.8 examples/sec; 0.818 sec/batch; 70h:14m:21s remains)
2017-12-07 08:26:08.473978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.97437 -2.944273 -2.922368 -2.9512122 -2.9878409 -3.0210869 -3.0662386 -3.1011033 -3.1381578 -3.1463833 -3.0949559 -3.0442977 -3.0227633 -3.0183887 -3.0128775][-2.9495618 -2.9380445 -2.9436998 -3.0095868 -3.0620031 -3.0909925 -3.1340179 -3.1698341 -3.2185798 -3.226398 -3.1445436 -3.0609541 -3.0257916 -3.0270257 -3.0434175][-2.9020138 -2.880651 -2.8981566 -2.9971056 -3.0613034 -3.0790949 -3.1133468 -3.1775076 -3.3029 -3.3782172 -3.3015308 -3.1723428 -3.082932 -3.0493436 -3.06371][-2.8806338 -2.8342452 -2.8430133 -2.9313924 -2.9358935 -2.8445144 -2.7751622 -2.8348818 -3.0843184 -3.3345165 -3.3849747 -3.287457 -3.1660056 -3.0904136 -3.081816][-2.8433166 -2.7886019 -2.791822 -2.8250592 -2.6706541 -2.3188348 -1.985074 -1.9806242 -2.423162 -2.9975629 -3.3276286 -3.340065 -3.1966972 -3.0760775 -3.0510788][-2.722249 -2.6943355 -2.7163959 -2.6522009 -2.2153015 -1.4150338 -0.65983844 -0.57471633 -1.3391697 -2.4169309 -3.1833224 -3.3962197 -3.246583 -3.0615819 -3.0065613][-2.5391989 -2.5761328 -2.6509738 -2.4759464 -1.7107141 -0.40498972 0.79335546 0.90180922 -0.26176691 -1.8737347 -3.0708218 -3.4883327 -3.3484602 -3.1024907 -3.0077677][-2.4300809 -2.5497828 -2.7129521 -2.5066798 -1.5600703 0.046203136 1.5092487 1.5898113 0.18071938 -1.6756864 -3.0283041 -3.5055079 -3.371613 -3.1109695 -3.0065911][-2.431752 -2.6106448 -2.87289 -2.7752416 -1.9717653 -0.51529884 0.79102612 0.80561495 -0.47717381 -2.0877771 -3.1945684 -3.5195336 -3.3498869 -3.102828 -3.0014248][-2.5644417 -2.7220743 -3.0040898 -3.0422206 -2.5590677 -1.5164254 -0.57363319 -0.59745288 -1.5149953 -2.6203995 -3.323308 -3.4675167 -3.3097773 -3.1252398 -3.0267184][-2.7885709 -2.8651137 -3.0806589 -3.1990972 -3.0161593 -2.3984694 -1.7825489 -1.7458539 -2.2058051 -2.7789059 -3.1408784 -3.194346 -3.1227102 -3.0561681 -2.9889774][-2.9108028 -2.8764172 -2.9720187 -3.0893204 -3.096941 -2.8156469 -2.4874144 -2.4298358 -2.5589795 -2.7556214 -2.9105139 -2.9428737 -2.9646535 -2.9938443 -2.9445133][-2.9509616 -2.80905 -2.7880445 -2.8561025 -2.9148653 -2.8058805 -2.6463456 -2.5884678 -2.5490196 -2.5665317 -2.6664135 -2.7571461 -2.8852057 -2.9880838 -2.9466267][-2.9782739 -2.7581768 -2.6598647 -2.6722522 -2.7109444 -2.6579838 -2.5564618 -2.4829311 -2.3738918 -2.3323369 -2.4448657 -2.608675 -2.8173842 -2.9657726 -2.9419484][-3.0194705 -2.7677503 -2.6145782 -2.5440977 -2.5079792 -2.4478951 -2.3562922 -2.2898848 -2.2172565 -2.228236 -2.4045262 -2.6298475 -2.8647866 -3.0034513 -2.97662]]...]
INFO - root - 2017-12-07 08:26:16.784547: step 23210, loss = 0.72, batch loss = 0.64 (9.8 examples/sec; 0.814 sec/batch; 69h:56m:42s remains)
INFO - root - 2017-12-07 08:26:25.123475: step 23220, loss = 0.65, batch loss = 0.58 (9.7 examples/sec; 0.825 sec/batch; 70h:53m:55s remains)
INFO - root - 2017-12-07 08:26:33.420247: step 23230, loss = 0.97, batch loss = 0.89 (9.2 examples/sec; 0.869 sec/batch; 74h:40m:39s remains)
INFO - root - 2017-12-07 08:26:41.778887: step 23240, loss = 0.64, batch loss = 0.57 (9.9 examples/sec; 0.805 sec/batch; 69h:09m:18s remains)
INFO - root - 2017-12-07 08:26:50.178072: step 23250, loss = 0.68, batch loss = 0.60 (9.8 examples/sec; 0.813 sec/batch; 69h:52m:33s remains)
INFO - root - 2017-12-07 08:26:58.480010: step 23260, loss = 0.77, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 72h:16m:55s remains)
INFO - root - 2017-12-07 08:27:06.586448: step 23270, loss = 0.66, batch loss = 0.58 (9.0 examples/sec; 0.888 sec/batch; 76h:15m:22s remains)
INFO - root - 2017-12-07 08:27:14.782820: step 23280, loss = 0.82, batch loss = 0.74 (10.1 examples/sec; 0.792 sec/batch; 67h:59m:37s remains)
INFO - root - 2017-12-07 08:27:23.263454: step 23290, loss = 0.75, batch loss = 0.68 (9.6 examples/sec; 0.831 sec/batch; 71h:21m:56s remains)
INFO - root - 2017-12-07 08:27:31.775923: step 23300, loss = 0.78, batch loss = 0.71 (9.3 examples/sec; 0.864 sec/batch; 74h:12m:27s remains)
2017-12-07 08:27:32.411558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9582582 -4.0480561 -4.2803707 -4.3112731 -3.9649072 -3.6947944 -4.0619569 -4.68014 -4.6996579 -4.0555353 -3.3861389 -2.9925191 -3.4672124 -4.9182887 -5.8036294][-2.9921756 -3.172749 -3.4508557 -3.4274311 -3.0577612 -2.7930913 -3.1484818 -3.8545771 -4.0143256 -3.4800148 -2.9421048 -2.6689107 -3.2465022 -4.736124 -5.6335268][-2.6310387 -3.1408277 -3.6083128 -3.6322455 -3.344739 -3.1145217 -3.3758163 -3.9682434 -4.0784049 -3.5470533 -2.9764194 -2.583919 -3.0067978 -4.3164597 -5.0916862][-3.3117433 -4.0600233 -4.5814495 -4.6283212 -4.4178448 -4.1417627 -4.2740221 -4.7366772 -4.8062387 -4.3516378 -3.7516243 -3.1785789 -3.3205905 -4.3089528 -4.8848004][-4.381999 -4.9862094 -5.2066317 -5.0377259 -4.6890264 -4.2227635 -4.2669182 -4.787149 -5.0253539 -4.8712735 -4.44877 -3.9139357 -3.935776 -4.713604 -5.1811585][-4.8519626 -5.0168967 -4.754838 -4.265974 -3.6694729 -3.0367496 -3.1080172 -3.7687809 -4.2283072 -4.3532624 -4.1160645 -3.7240605 -3.814445 -4.674386 -5.3274245][-3.9383631 -3.5251288 -2.7269945 -1.8460939 -0.9557507 -0.32145119 -0.6474576 -1.5778604 -2.279578 -2.5861607 -2.4911568 -2.3094316 -2.6654835 -3.860281 -4.8983388][-2.7417188 -1.784688 -0.40799809 0.94320965 2.1123195 2.6938772 2.1091318 0.99688768 0.16794777 -0.23076391 -0.28737593 -0.38953447 -1.1560509 -2.7921543 -4.2326837][-3.1190286 -2.0716927 -0.54292727 0.88679028 1.9621434 2.36027 1.823123 0.95110035 0.31702042 0.036153316 -0.041115761 -0.25855398 -1.151511 -2.8080666 -4.2428746][-4.1284547 -3.4822862 -2.4863944 -1.633486 -1.1238196 -1.016422 -1.3874485 -1.9165301 -2.2424769 -2.2239704 -2.0267537 -1.9781103 -2.5516171 -3.744199 -4.7172918][-4.0457554 -3.7192192 -3.2934322 -3.0484145 -3.0388544 -3.1744933 -3.4900498 -3.86636 -4.0609884 -3.8702221 -3.4174194 -3.0576587 -3.2534022 -4.004818 -4.5807133][-3.4416807 -3.1720433 -2.9593945 -2.9080954 -3.037519 -3.2479677 -3.5716386 -3.9434516 -4.1753769 -4.0553293 -3.6056423 -3.1180744 -3.0931027 -3.6153848 -4.0545368][-3.7252979 -3.5686374 -3.499975 -3.5312023 -3.6936214 -3.9495339 -4.2659965 -4.5636296 -4.6899652 -4.4937506 -3.9772069 -3.40346 -3.2914934 -3.7587609 -4.2163482][-4.3425422 -4.4632745 -4.624732 -4.8068814 -5.025135 -5.2246485 -5.3387284 -5.32354 -5.1141362 -4.6862497 -4.0840507 -3.5754037 -3.5915012 -4.1971989 -4.772284][-4.6808944 -4.9447613 -5.1555076 -5.30257 -5.3921037 -5.3709059 -5.2221031 -4.9538088 -4.5605745 -4.09298 -3.622247 -3.3655856 -3.6106758 -4.3455582 -4.9780922]]...]
INFO - root - 2017-12-07 08:27:40.831046: step 23310, loss = 0.74, batch loss = 0.66 (9.2 examples/sec; 0.869 sec/batch; 74h:35m:32s remains)
INFO - root - 2017-12-07 08:27:49.186611: step 23320, loss = 0.56, batch loss = 0.48 (9.9 examples/sec; 0.808 sec/batch; 69h:22m:26s remains)
INFO - root - 2017-12-07 08:27:57.583796: step 23330, loss = 0.80, batch loss = 0.73 (9.7 examples/sec; 0.826 sec/batch; 70h:56m:49s remains)
INFO - root - 2017-12-07 08:28:05.934921: step 23340, loss = 0.55, batch loss = 0.48 (10.0 examples/sec; 0.801 sec/batch; 68h:47m:53s remains)
INFO - root - 2017-12-07 08:28:14.417781: step 23350, loss = 0.69, batch loss = 0.62 (9.5 examples/sec; 0.845 sec/batch; 72h:32m:19s remains)
INFO - root - 2017-12-07 08:28:22.838995: step 23360, loss = 0.78, batch loss = 0.71 (9.1 examples/sec; 0.881 sec/batch; 75h:41m:44s remains)
INFO - root - 2017-12-07 08:28:31.148391: step 23370, loss = 0.68, batch loss = 0.61 (9.8 examples/sec; 0.815 sec/batch; 70h:01m:31s remains)
INFO - root - 2017-12-07 08:28:39.528646: step 23380, loss = 0.76, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 68h:06m:26s remains)
INFO - root - 2017-12-07 08:28:47.956647: step 23390, loss = 0.81, batch loss = 0.74 (9.3 examples/sec; 0.861 sec/batch; 73h:53m:35s remains)
INFO - root - 2017-12-07 08:28:56.534199: step 23400, loss = 0.80, batch loss = 0.73 (9.6 examples/sec; 0.836 sec/batch; 71h:45m:30s remains)
2017-12-07 08:28:57.201737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7537248 -1.6553545 -1.6084893 -1.8425109 -2.3131208 -2.2212415 -1.4170647 -0.8203733 -0.64365554 -0.85904217 -1.3973479 -1.6521749 -1.4883142 -1.1180513 -1.0116217][-1.5314138 -1.4826791 -1.5257547 -1.8295753 -2.286175 -2.1157432 -1.2128537 -0.66016674 -0.62616777 -0.87399912 -1.4601977 -1.8083801 -1.6666851 -1.4140649 -1.4175696][-1.1300128 -1.2307386 -1.4362116 -1.8101473 -2.1895349 -1.9770002 -1.0575392 -0.53494263 -0.58724284 -0.83201885 -1.3775101 -1.7582521 -1.7016921 -1.6148498 -1.733192][-0.805789 -1.0138731 -1.277221 -1.6979959 -1.9975295 -1.7683177 -0.94471073 -0.47110224 -0.58092403 -0.85233736 -1.3833728 -1.8638766 -1.8997808 -1.8772409 -2.0205886][-0.55915189 -0.83335948 -1.1103718 -1.5774171 -1.8492763 -1.6111887 -0.93265557 -0.49230933 -0.54763746 -0.81487489 -1.3831551 -2.0105233 -2.1675828 -2.1553521 -2.2969954][-0.46938062 -0.89695716 -1.1680934 -1.5864229 -1.8180766 -1.5479765 -0.94400597 -0.47253251 -0.39589739 -0.625427 -1.2119236 -1.9214337 -2.2154515 -2.2318149 -2.3686857][-0.58623505 -1.1338959 -1.327554 -1.5697973 -1.7359457 -1.4366057 -0.82576489 -0.32320642 -0.21321535 -0.48391843 -1.0788987 -1.7868958 -2.1771259 -2.2632751 -2.4050043][-0.8846488 -1.3645785 -1.3912048 -1.4212961 -1.5184422 -1.2179146 -0.60602927 -0.14968777 -0.1423564 -0.46014404 -1.0431023 -1.7365706 -2.21592 -2.4172311 -2.5563488][-1.1642249 -1.4812005 -1.4003122 -1.3028088 -1.3194983 -1.0244913 -0.45647 -0.1185813 -0.19031763 -0.43842387 -0.91813612 -1.591507 -2.1633997 -2.4724574 -2.6278725][-1.4770143 -1.6175377 -1.4391341 -1.2362628 -1.1783073 -0.9255271 -0.47181463 -0.2312355 -0.27279234 -0.37981129 -0.68994331 -1.2907434 -1.8843992 -2.2652717 -2.4897141][-1.910903 -1.9105048 -1.5709708 -1.1783381 -1.0305815 -0.85312963 -0.52827597 -0.33349562 -0.30481672 -0.28114986 -0.43116808 -0.94205832 -1.5336051 -1.9692793 -2.2656205][-2.3686526 -2.2607567 -1.7217474 -1.1433787 -0.95526838 -0.87507081 -0.66809583 -0.50264764 -0.42366886 -0.29386044 -0.36499214 -0.81422424 -1.3720608 -1.8287327 -2.1570051][-2.7434425 -2.6217024 -2.0406885 -1.4497073 -1.3224485 -1.3210824 -1.1143668 -0.89551592 -0.7491889 -0.50790334 -0.52227807 -0.87120485 -1.3458664 -1.7684963 -2.077008][-3.0553222 -2.9413695 -2.4359055 -1.938473 -1.8753381 -1.9074118 -1.6501682 -1.3800247 -1.1807775 -0.88591766 -0.81625867 -0.94024134 -1.1701729 -1.4642277 -1.7479651][-3.3854814 -3.2472649 -2.8500483 -2.4517922 -2.3678706 -2.3235807 -1.9906757 -1.6927111 -1.48599 -1.1814029 -0.97692275 -0.85920691 -0.8301878 -1.0016623 -1.2930081]]...]
INFO - root - 2017-12-07 08:29:08.185463: step 23410, loss = 0.61, batch loss = 0.54 (7.0 examples/sec; 1.139 sec/batch; 97h:48m:56s remains)
INFO - root - 2017-12-07 08:29:19.791336: step 23420, loss = 0.78, batch loss = 0.71 (7.0 examples/sec; 1.148 sec/batch; 98h:33m:44s remains)
INFO - root - 2017-12-07 08:29:31.519645: step 23430, loss = 0.96, batch loss = 0.88 (6.7 examples/sec; 1.186 sec/batch; 101h:50m:16s remains)
INFO - root - 2017-12-07 08:29:43.340919: step 23440, loss = 0.82, batch loss = 0.75 (6.4 examples/sec; 1.258 sec/batch; 107h:58m:58s remains)
INFO - root - 2017-12-07 08:29:54.925961: step 23450, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 1.139 sec/batch; 97h:45m:18s remains)
INFO - root - 2017-12-07 08:30:06.611190: step 23460, loss = 0.76, batch loss = 0.69 (7.3 examples/sec; 1.102 sec/batch; 94h:34m:13s remains)
INFO - root - 2017-12-07 08:30:18.205192: step 23470, loss = 0.71, batch loss = 0.64 (7.1 examples/sec; 1.127 sec/batch; 96h:44m:17s remains)
INFO - root - 2017-12-07 08:30:29.955406: step 23480, loss = 0.91, batch loss = 0.83 (6.4 examples/sec; 1.251 sec/batch; 107h:22m:44s remains)
INFO - root - 2017-12-07 08:30:41.710277: step 23490, loss = 0.70, batch loss = 0.63 (6.6 examples/sec; 1.207 sec/batch; 103h:35m:21s remains)
INFO - root - 2017-12-07 08:30:53.306266: step 23500, loss = 0.65, batch loss = 0.58 (7.2 examples/sec; 1.116 sec/batch; 95h:47m:52s remains)
2017-12-07 08:30:54.184780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2746344 -3.5571077 -3.506038 -3.1704376 -2.7797289 -2.5955849 -2.85888 -3.1044796 -3.1297908 -2.9213266 -2.739877 -2.9671183 -3.0890732 -2.7227879 -2.3293614][-3.3394172 -3.6950645 -3.62137 -3.2446809 -2.7921352 -2.5595074 -2.8724017 -3.132113 -3.1879091 -3.0527139 -2.9517093 -3.2363813 -3.2162952 -2.5446935 -1.8982117][-3.4072261 -3.5871019 -3.3497057 -2.9628029 -2.5954144 -2.4922924 -2.9000554 -3.1104388 -3.2004743 -3.2388079 -3.280992 -3.5809908 -3.3180361 -2.302588 -1.427417][-3.4804971 -3.3064449 -2.8370371 -2.4604311 -2.2661614 -2.3430386 -2.8075862 -2.9390397 -3.13792 -3.4408011 -3.6695862 -3.9587903 -3.4415722 -2.1328752 -1.0946476][-3.6060646 -3.1262603 -2.5580282 -2.226104 -2.12043 -2.1405251 -2.3389933 -2.2075922 -2.5518122 -3.2572274 -3.7719781 -4.06432 -3.3622241 -1.9196613 -0.86584377][-3.7278042 -3.2049952 -2.8124869 -2.6674273 -2.5712204 -2.2604241 -1.7450833 -0.9736793 -1.3537736 -2.6453826 -3.6725373 -4.0550041 -3.2401357 -1.7530963 -0.7452898][-3.5004809 -3.218497 -3.2009506 -3.3626881 -3.3309226 -2.6843762 -1.3111336 0.34683228 0.12697935 -1.7823815 -3.4730511 -4.0674424 -3.2678993 -1.7674325 -0.76266694][-3.1263466 -3.179688 -3.5586259 -4.0029879 -4.0498066 -3.2478352 -1.3208382 1.0631375 1.1893358 -1.0046041 -3.1358447 -3.9441626 -3.2687826 -1.8443611 -0.85558319][-2.5095696 -2.8409696 -3.5710878 -4.2662148 -4.4735837 -3.8194554 -1.9480276 0.4946599 0.9185977 -0.95658755 -2.8961477 -3.6500106 -3.1250863 -1.9267986 -1.0559533][-1.8064573 -2.2085989 -3.0446136 -3.8074369 -4.1797419 -3.92299 -2.586524 -0.65764427 -0.20767355 -1.5319366 -2.8616333 -3.3254604 -2.9431553 -2.1010807 -1.4617858][-1.6199167 -1.9612739 -2.6182294 -3.1539226 -3.5252357 -3.6623847 -2.9437931 -1.6815836 -1.3653111 -2.2054489 -2.884732 -3.0144651 -2.8143246 -2.4304447 -2.0697119][-1.9234018 -2.2254004 -2.6747155 -2.9130244 -3.1551857 -3.4715533 -3.174041 -2.4569898 -2.3092408 -2.8080134 -3.0150461 -2.8839006 -2.82091 -2.7943158 -2.6605663][-2.49268 -2.7931588 -3.0797758 -3.0979118 -3.1609635 -3.4105895 -3.2572689 -2.8719337 -2.858057 -3.1637411 -3.1017523 -2.8191016 -2.8063145 -2.9175389 -2.8766377][-2.7970603 -3.0277624 -3.2139475 -3.1829371 -3.15075 -3.254209 -3.101341 -2.8966613 -2.9936528 -3.2138481 -3.0496154 -2.7471802 -2.75635 -2.8500271 -2.8049791][-2.848268 -2.9264064 -3.005399 -2.9974608 -2.9953032 -3.0501447 -2.9353466 -2.8353853 -2.9464712 -3.0994344 -2.9469013 -2.7459345 -2.8054295 -2.8486118 -2.7833388]]...]
INFO - root - 2017-12-07 08:31:05.718340: step 23510, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 1.184 sec/batch; 101h:34m:54s remains)
INFO - root - 2017-12-07 08:31:17.466831: step 23520, loss = 0.83, batch loss = 0.76 (6.2 examples/sec; 1.285 sec/batch; 110h:16m:03s remains)
INFO - root - 2017-12-07 08:31:29.086413: step 23530, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 1.122 sec/batch; 96h:18m:37s remains)
INFO - root - 2017-12-07 08:31:40.448269: step 23540, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 1.122 sec/batch; 96h:19m:32s remains)
INFO - root - 2017-12-07 08:31:52.114475: step 23550, loss = 0.67, batch loss = 0.60 (6.8 examples/sec; 1.169 sec/batch; 100h:17m:07s remains)
INFO - root - 2017-12-07 08:32:03.805118: step 23560, loss = 0.69, batch loss = 0.62 (6.9 examples/sec; 1.154 sec/batch; 99h:04m:29s remains)
INFO - root - 2017-12-07 08:32:15.446186: step 23570, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 1.171 sec/batch; 100h:27m:44s remains)
INFO - root - 2017-12-07 08:32:26.868452: step 23580, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 1.120 sec/batch; 96h:07m:43s remains)
INFO - root - 2017-12-07 08:32:38.562794: step 23590, loss = 0.73, batch loss = 0.66 (6.8 examples/sec; 1.182 sec/batch; 101h:24m:22s remains)
INFO - root - 2017-12-07 08:32:50.093466: step 23600, loss = 0.67, batch loss = 0.59 (6.5 examples/sec; 1.239 sec/batch; 106h:16m:38s remains)
2017-12-07 08:32:50.929395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.949744 -2.9630132 -2.9389663 -2.9378636 -3.0060153 -3.1297762 -3.2428474 -3.2795663 -3.2996039 -3.337501 -3.3084774 -3.2020473 -3.0735667 -2.8658328 -2.619019][-3.0615401 -3.0847521 -3.0330539 -2.9663811 -3.006598 -3.1467924 -3.284874 -3.3555679 -3.4122553 -3.481946 -3.4577732 -3.3233356 -3.1929116 -2.9733682 -2.6759028][-3.3165503 -3.3251925 -3.213913 -3.0392604 -2.9899721 -3.0797858 -3.2066898 -3.3124843 -3.4376972 -3.5498006 -3.5385065 -3.4281278 -3.3738713 -3.2367253 -2.9658556][-3.5508811 -3.564096 -3.4187851 -3.1515689 -2.9980857 -2.9933434 -3.0375485 -3.1098657 -3.274601 -3.425359 -3.4527705 -3.4312859 -3.5052471 -3.4622626 -3.2249346][-3.7075934 -3.8023026 -3.7018456 -3.4411037 -3.2250495 -3.1153603 -3.0196011 -2.9560957 -3.0416799 -3.1672211 -3.2057657 -3.2699254 -3.4359787 -3.4209733 -3.1905065][-3.8120203 -3.9764123 -3.9016616 -3.6384521 -3.3330317 -3.065625 -2.7479939 -2.4540405 -2.4071009 -2.505692 -2.5937524 -2.7834716 -3.0617571 -3.09487 -2.9155972][-3.9533229 -4.1088195 -3.9757307 -3.6475194 -3.2138424 -2.7335587 -2.1689382 -1.6446409 -1.4810877 -1.5591092 -1.7154922 -2.0682588 -2.5011315 -2.648138 -2.5961967][-4.0997348 -4.2119293 -4.0147805 -3.6400099 -3.100606 -2.4058609 -1.6013422 -0.91317463 -0.70794725 -0.84132528 -1.0958056 -1.5977995 -2.1515362 -2.3886523 -2.4347744][-4.0397096 -4.1289415 -3.9242859 -3.5785315 -3.05513 -2.2766082 -1.3727398 -0.66906357 -0.49262428 -0.69266009 -1.0081606 -1.5542121 -2.119154 -2.3563948 -2.4194453][-3.7610197 -3.8580785 -3.6924365 -3.4354396 -3.0661044 -2.4248266 -1.6515765 -1.0814645 -0.93270731 -1.0834186 -1.3340864 -1.787159 -2.2654915 -2.4430192 -2.4750571][-3.4730258 -3.599112 -3.502367 -3.3409667 -3.1278253 -2.6900938 -2.1246314 -1.7142379 -1.5888274 -1.6660962 -1.8043303 -2.139797 -2.5190039 -2.6353436 -2.6277125][-3.4365475 -3.5886154 -3.5355306 -3.4135184 -3.2537436 -2.9120965 -2.474246 -2.1532712 -2.0488765 -2.0905442 -2.1707673 -2.4510751 -2.7891941 -2.8688114 -2.826612][-3.6661391 -3.8677557 -3.8497491 -3.7142539 -3.5140889 -3.1906977 -2.8033361 -2.4859076 -2.347959 -2.3489127 -2.3942869 -2.6409254 -2.9384995 -2.9937415 -2.9398088][-3.5864272 -3.7657979 -3.7446246 -3.5970259 -3.3897448 -3.1413817 -2.87164 -2.6141119 -2.4800467 -2.4625506 -2.5009007 -2.7255812 -2.982996 -3.0274138 -2.9833002][-3.2548571 -3.3475041 -3.3045807 -3.1825442 -3.0430269 -2.9320006 -2.8450115 -2.7406142 -2.685431 -2.6822453 -2.7179427 -2.9058044 -3.1070387 -3.1398335 -3.1132782]]...]
INFO - root - 2017-12-07 08:33:02.638735: step 23610, loss = 0.81, batch loss = 0.74 (6.7 examples/sec; 1.196 sec/batch; 102h:39m:43s remains)
INFO - root - 2017-12-07 08:33:14.484965: step 23620, loss = 0.75, batch loss = 0.68 (6.5 examples/sec; 1.229 sec/batch; 105h:27m:19s remains)
INFO - root - 2017-12-07 08:33:26.105238: step 23630, loss = 0.71, batch loss = 0.64 (6.4 examples/sec; 1.251 sec/batch; 107h:22m:02s remains)
INFO - root - 2017-12-07 08:33:37.555080: step 23640, loss = 0.66, batch loss = 0.59 (7.2 examples/sec; 1.114 sec/batch; 95h:34m:33s remains)
INFO - root - 2017-12-07 08:33:49.074705: step 23650, loss = 1.01, batch loss = 0.94 (7.1 examples/sec; 1.130 sec/batch; 96h:57m:25s remains)
INFO - root - 2017-12-07 08:34:00.572427: step 23660, loss = 0.76, batch loss = 0.69 (8.0 examples/sec; 1.005 sec/batch; 86h:11m:23s remains)
INFO - root - 2017-12-07 08:34:12.281441: step 23670, loss = 0.81, batch loss = 0.74 (6.6 examples/sec; 1.216 sec/batch; 104h:17m:13s remains)
INFO - root - 2017-12-07 08:34:24.132004: step 23680, loss = 0.62, batch loss = 0.55 (6.6 examples/sec; 1.217 sec/batch; 104h:23m:26s remains)
INFO - root - 2017-12-07 08:34:35.927160: step 23690, loss = 0.72, batch loss = 0.65 (7.2 examples/sec; 1.113 sec/batch; 95h:28m:20s remains)
INFO - root - 2017-12-07 08:34:47.509090: step 23700, loss = 0.72, batch loss = 0.65 (7.0 examples/sec; 1.139 sec/batch; 97h:42m:42s remains)
2017-12-07 08:34:48.371748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.343868 -0.87702465 -0.52903581 -0.65762329 -1.0399489 -1.4272411 -1.9512208 -2.8910785 -3.8757162 -4.4025273 -4.2388678 -3.5669177 -2.5373969 -1.6807151 -1.6275747][-1.1400037 -0.63755369 -0.21296406 -0.30691624 -0.73691869 -1.2038801 -1.7087283 -2.4635119 -3.2261271 -3.6283922 -3.5417526 -3.1169453 -2.4280887 -1.8223958 -1.7921453][-1.1271696 -0.6646626 -0.18920422 -0.15563536 -0.51845217 -0.995898 -1.4882567 -2.0766661 -2.5711453 -2.7622647 -2.6639252 -2.4403203 -2.1490104 -1.922472 -1.9838297][-1.3454354 -0.87065005 -0.26203489 0.024160862 -0.13899612 -0.57481956 -1.1744685 -1.8301895 -2.292624 -2.4462733 -2.4379635 -2.4478817 -2.5362518 -2.67486 -2.8208621][-1.6376672 -1.1046202 -0.31225967 0.26593971 0.35820961 0.0059695244 -0.70815539 -1.5067115 -1.9973364 -2.12697 -2.2070315 -2.4231133 -2.8357286 -3.3342116 -3.6191254][-1.8132877 -1.1459019 -0.13308907 0.74248123 1.0684934 0.75833893 -0.067462921 -0.98087478 -1.4571424 -1.5075696 -1.5993662 -1.9121842 -2.5167923 -3.3184414 -3.7983956][-1.9137845 -1.0884485 0.15536976 1.3461542 1.9446936 1.76862 0.99318266 0.13348818 -0.26659775 -0.25694847 -0.4763844 -1.0237122 -1.8531785 -2.9123607 -3.5931737][-1.850266 -0.94644547 0.39910078 1.736918 2.5071483 2.5239496 2.0633535 1.535738 1.3976064 1.4402871 0.91158581 -0.10271025 -1.2371707 -2.4820702 -3.2906818][-1.8223321 -0.89442348 0.50801373 1.8737512 2.6603208 2.7700033 2.6191626 2.5142937 2.7167058 2.7335691 1.7549129 0.23062897 -1.15484 -2.4191272 -3.1789784][-2.1246815 -1.2443264 0.16301346 1.5087323 2.2587118 2.391386 2.4383607 2.6711707 3.0975137 2.967906 1.6636677 -0.12074423 -1.5436532 -2.6159787 -3.2028308][-2.62145 -1.8418398 -0.54361796 0.66177273 1.2819433 1.3792686 1.5591574 2.0132957 2.5179453 2.2186642 0.74106312 -1.0583894 -2.2740541 -2.9008818 -3.1141467][-2.864053 -2.1700995 -1.0206103 0.0032296181 0.5138464 0.64205217 1.0083389 1.6997576 2.2657366 1.7885427 0.10112572 -1.8291833 -3.0558181 -3.4157624 -3.2426865][-2.5914469 -1.9816904 -1.0046329 -0.1692853 0.28933477 0.54163742 1.1324744 2.0831137 2.7432852 2.1794567 0.32688189 -1.7577488 -3.0859675 -3.3728976 -2.9309587][-2.305717 -1.7895422 -1.0057681 -0.36974812 0.0081644058 0.32805538 1.0463681 2.16049 2.9366617 2.5151234 0.8537569 -1.0532322 -2.2799153 -2.4427407 -1.8149934][-2.1745663 -1.7422571 -1.1239777 -0.67285848 -0.4153204 -0.13687277 0.57321215 1.7237067 2.6160512 2.5185614 1.3492341 -0.15172863 -1.1997936 -1.3406658 -0.77704024]]...]
INFO - root - 2017-12-07 08:35:00.079027: step 23710, loss = 0.62, batch loss = 0.55 (6.4 examples/sec; 1.256 sec/batch; 107h:42m:05s remains)
INFO - root - 2017-12-07 08:35:11.698082: step 23720, loss = 0.67, batch loss = 0.60 (7.0 examples/sec; 1.137 sec/batch; 97h:30m:56s remains)
INFO - root - 2017-12-07 08:35:23.345783: step 23730, loss = 0.73, batch loss = 0.65 (6.7 examples/sec; 1.189 sec/batch; 102h:00m:39s remains)
INFO - root - 2017-12-07 08:35:34.979148: step 23740, loss = 0.84, batch loss = 0.77 (7.2 examples/sec; 1.104 sec/batch; 94h:42m:39s remains)
INFO - root - 2017-12-07 08:35:46.763044: step 23750, loss = 0.79, batch loss = 0.71 (6.6 examples/sec; 1.210 sec/batch; 103h:47m:11s remains)
INFO - root - 2017-12-07 08:35:58.578045: step 23760, loss = 0.76, batch loss = 0.69 (6.8 examples/sec; 1.181 sec/batch; 101h:19m:14s remains)
INFO - root - 2017-12-07 08:36:10.286160: step 23770, loss = 0.61, batch loss = 0.54 (7.2 examples/sec; 1.114 sec/batch; 95h:29m:56s remains)
INFO - root - 2017-12-07 08:36:22.003519: step 23780, loss = 1.03, batch loss = 0.95 (6.9 examples/sec; 1.154 sec/batch; 98h:57m:58s remains)
INFO - root - 2017-12-07 08:36:33.665004: step 23790, loss = 0.68, batch loss = 0.61 (6.6 examples/sec; 1.207 sec/batch; 103h:31m:35s remains)
INFO - root - 2017-12-07 08:36:45.362640: step 23800, loss = 0.84, batch loss = 0.76 (6.8 examples/sec; 1.177 sec/batch; 100h:54m:31s remains)
2017-12-07 08:36:46.230689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7012875 -2.7147131 -2.6935148 -2.6397882 -2.5900426 -2.5886216 -2.6029267 -2.5849028 -2.5604563 -2.512259 -2.37866 -2.202836 -2.1592824 -2.2391171 -2.3461461][-2.7394533 -2.8049831 -2.7398033 -2.5668163 -2.3456302 -2.1693695 -2.1195731 -2.1718094 -2.3288572 -2.479835 -2.4850516 -2.3819366 -2.3959579 -2.5379591 -2.6665063][-2.8853445 -2.9431925 -2.8105001 -2.5258415 -2.172199 -1.8766439 -1.8039792 -1.908674 -2.2025759 -2.5703373 -2.7930248 -2.8424845 -2.9229479 -3.089726 -3.2111769][-3.0802498 -3.1290107 -2.930099 -2.552669 -2.1169906 -1.797766 -1.752367 -1.8570764 -2.1505349 -2.5967903 -3.0036416 -3.2428522 -3.411643 -3.56304 -3.6166785][-2.9991937 -3.0738106 -2.9082026 -2.5794847 -2.204632 -1.9524322 -1.9335492 -1.9639993 -2.1081948 -2.4816208 -2.9421616 -3.3273964 -3.6337686 -3.8171635 -3.7790513][-2.4080911 -2.5168767 -2.5063362 -2.4151917 -2.2163279 -2.0435259 -2.0185182 -1.9515893 -1.9263508 -2.1325662 -2.4678221 -2.7847319 -3.1437008 -3.4330921 -3.4509404][-1.7620769 -1.9075677 -2.0476115 -2.1973889 -2.146775 -1.9873662 -1.9276552 -1.8397269 -1.7767386 -1.8847971 -2.0077586 -2.0582798 -2.2781482 -2.6047282 -2.7606282][-1.2976351 -1.4787679 -1.7088523 -1.9860749 -2.0103273 -1.8640685 -1.8087668 -1.7692077 -1.7645001 -1.8671908 -1.8189421 -1.5919764 -1.6045935 -1.8914316 -2.1958895][-1.145092 -1.2932801 -1.5326214 -1.8358676 -1.8939435 -1.8065925 -1.8238654 -1.860322 -1.954216 -2.105906 -1.9963543 -1.6754708 -1.5584266 -1.7209325 -2.0207148][-1.6123161 -1.6471603 -1.8003671 -2.0205402 -2.0417595 -1.9820471 -2.0409517 -2.1474755 -2.3886645 -2.670588 -2.6492271 -2.4384134 -2.31549 -2.3084695 -2.3865969][-2.6342316 -2.6632485 -2.7264428 -2.7930171 -2.7170711 -2.6160746 -2.623631 -2.7031827 -2.9899101 -3.3503866 -3.461504 -3.4345019 -3.3318238 -3.1642275 -2.9684854][-3.5216174 -3.6216681 -3.6687281 -3.6404881 -3.49667 -3.3556116 -3.284492 -3.2694654 -3.448678 -3.7092805 -3.848973 -3.924149 -3.8448691 -3.5858181 -3.1604624][-3.9026747 -4.0188303 -4.0599051 -4.00575 -3.8704879 -3.7413697 -3.6677477 -3.637054 -3.7150614 -3.7898719 -3.8031321 -3.81292 -3.7458253 -3.5156231 -3.0515857][-3.8520377 -3.9387212 -3.9863038 -4.0013905 -3.9726095 -3.8984833 -3.8365703 -3.8078704 -3.8340306 -3.8092186 -3.7176809 -3.6120143 -3.5310135 -3.3814261 -3.0179224][-3.4495931 -3.5196137 -3.5858397 -3.6906259 -3.7962878 -3.8085814 -3.7566402 -3.6775117 -3.6056507 -3.50102 -3.3756595 -3.2492762 -3.2127516 -3.1350145 -2.8523288]]...]
INFO - root - 2017-12-07 08:36:57.966278: step 23810, loss = 0.64, batch loss = 0.57 (6.8 examples/sec; 1.178 sec/batch; 101h:01m:20s remains)
INFO - root - 2017-12-07 08:37:09.681696: step 23820, loss = 0.90, batch loss = 0.82 (6.7 examples/sec; 1.199 sec/batch; 102h:49m:25s remains)
INFO - root - 2017-12-07 08:37:21.333610: step 23830, loss = 0.75, batch loss = 0.68 (6.3 examples/sec; 1.266 sec/batch; 108h:33m:32s remains)
INFO - root - 2017-12-07 08:37:33.012487: step 23840, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 1.143 sec/batch; 97h:58m:12s remains)
INFO - root - 2017-12-07 08:37:44.392950: step 23850, loss = 0.64, batch loss = 0.57 (7.3 examples/sec; 1.101 sec/batch; 94h:25m:34s remains)
INFO - root - 2017-12-07 08:37:55.999256: step 23860, loss = 0.79, batch loss = 0.72 (6.8 examples/sec; 1.170 sec/batch; 100h:20m:55s remains)
INFO - root - 2017-12-07 08:38:07.678279: step 23870, loss = 0.88, batch loss = 0.81 (6.8 examples/sec; 1.182 sec/batch; 101h:17m:35s remains)
INFO - root - 2017-12-07 08:38:19.392881: step 23880, loss = 0.63, batch loss = 0.56 (6.3 examples/sec; 1.276 sec/batch; 109h:21m:31s remains)
INFO - root - 2017-12-07 08:38:30.849612: step 23890, loss = 0.76, batch loss = 0.68 (7.0 examples/sec; 1.141 sec/batch; 97h:48m:51s remains)
INFO - root - 2017-12-07 08:38:42.506907: step 23900, loss = 1.00, batch loss = 0.93 (6.8 examples/sec; 1.175 sec/batch; 100h:45m:27s remains)
2017-12-07 08:38:43.439040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2726097 -2.9740169 -2.5425344 -2.1290443 -1.7446032 -1.4978719 -1.2642102 -1.0279744 -0.84982634 -0.70963836 -0.7746079 -0.90315938 -0.9135437 -0.96144414 -1.1693523][-3.728694 -3.3709056 -2.8829594 -2.4846034 -2.1900396 -2.1374257 -2.1703892 -2.2395456 -2.3329465 -2.3065095 -2.3508289 -2.2952261 -1.9480975 -1.5687239 -1.3597119][-4.0873585 -3.5054348 -2.7304018 -2.1508465 -1.8206196 -1.8923988 -2.1829739 -2.6336942 -3.1474986 -3.3991923 -3.572149 -3.5216186 -3.0047622 -2.3073032 -1.7133553][-4.1498713 -3.5504303 -2.7156181 -2.0390038 -1.6096921 -1.6421208 -2.0054364 -2.6231437 -3.2925544 -3.5619974 -3.6448636 -3.4643931 -2.7768669 -1.8900106 -1.1655498][-3.6022277 -3.0097299 -2.2064185 -1.472687 -0.98331571 -1.025588 -1.4659588 -2.2253659 -3.016048 -3.3204608 -3.3585386 -3.0534096 -2.215976 -1.1252146 -0.30346155][-2.7850366 -2.1884446 -1.3595357 -0.49138236 0.17627573 0.2627058 -0.10091543 -0.83727813 -1.6361816 -1.9749601 -2.1034598 -1.9387503 -1.2642105 -0.27788162 0.42037821][-1.8953235 -1.3304527 -0.53786349 0.39465332 1.2423344 1.535543 1.354249 0.76883078 0.1468544 -0.032012939 -0.079217434 0.0057673454 0.47030973 1.2089839 1.5784597][-1.3114445 -0.72655869 0.025724411 0.90332413 1.7728829 2.1425076 2.0031128 1.377727 0.74174738 0.6517539 0.79215145 1.008317 1.5140128 2.2810307 2.6165504][-1.4439743 -1.0268114 -0.53775239 0.031852722 0.7104454 1.0995154 1.0619559 0.50142527 -0.15971231 -0.36826038 -0.34344625 -0.12902117 0.43151665 1.3405576 1.9740634][-1.8483758 -1.4269528 -1.1023355 -0.85708809 -0.54323339 -0.29782343 -0.23233414 -0.50346136 -0.923754 -1.1198132 -1.1787701 -0.97119117 -0.40555 0.47853327 1.1683278][-2.3303547 -1.7043502 -1.2373288 -1.026463 -0.88220525 -0.79485321 -0.79138684 -0.97243237 -1.2166862 -1.3356934 -1.330905 -1.0444865 -0.43988109 0.38822079 1.0113974][-3.0715563 -2.510638 -2.1180358 -1.9165804 -1.7168248 -1.5800145 -1.5801277 -1.7381809 -1.9283984 -2.0118854 -1.9580054 -1.6595633 -1.0734603 -0.28326941 0.35952711][-3.3267729 -2.9468224 -2.7832718 -2.7242651 -2.5817823 -2.4825716 -2.4698002 -2.537401 -2.5991116 -2.6422048 -2.6414609 -2.4248955 -1.952117 -1.3271832 -0.82604218][-3.2037897 -2.8215075 -2.6510391 -2.5701246 -2.486002 -2.4805331 -2.5141397 -2.553437 -2.555783 -2.587275 -2.642458 -2.516808 -2.149821 -1.6966424 -1.3981483][-3.0122738 -2.6592531 -2.4608121 -2.3329132 -2.2634764 -2.2843921 -2.3245809 -2.3474271 -2.3467593 -2.4204984 -2.5539298 -2.5696187 -2.3734655 -2.0823216 -1.8689311]]...]
INFO - root - 2017-12-07 08:38:55.251624: step 23910, loss = 0.84, batch loss = 0.77 (6.7 examples/sec; 1.195 sec/batch; 102h:28m:24s remains)
INFO - root - 2017-12-07 08:39:06.909356: step 23920, loss = 0.64, batch loss = 0.57 (7.1 examples/sec; 1.126 sec/batch; 96h:28m:56s remains)
INFO - root - 2017-12-07 08:39:18.659766: step 23930, loss = 0.53, batch loss = 0.45 (6.7 examples/sec; 1.187 sec/batch; 101h:42m:43s remains)
INFO - root - 2017-12-07 08:39:30.389318: step 23940, loss = 0.65, batch loss = 0.58 (6.5 examples/sec; 1.222 sec/batch; 104h:41m:52s remains)
INFO - root - 2017-12-07 08:39:41.988344: step 23950, loss = 0.92, batch loss = 0.85 (7.2 examples/sec; 1.118 sec/batch; 95h:47m:17s remains)
INFO - root - 2017-12-07 08:39:53.629040: step 23960, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 1.128 sec/batch; 96h:38m:49s remains)
INFO - root - 2017-12-07 08:40:05.364570: step 23970, loss = 0.88, batch loss = 0.81 (7.1 examples/sec; 1.120 sec/batch; 95h:57m:28s remains)
INFO - root - 2017-12-07 08:40:16.978360: step 23980, loss = 0.58, batch loss = 0.51 (6.5 examples/sec; 1.233 sec/batch; 105h:39m:16s remains)
INFO - root - 2017-12-07 08:40:28.559582: step 23990, loss = 0.72, batch loss = 0.65 (6.7 examples/sec; 1.190 sec/batch; 102h:00m:04s remains)
INFO - root - 2017-12-07 08:40:40.098836: step 24000, loss = 0.92, batch loss = 0.85 (7.2 examples/sec; 1.104 sec/batch; 94h:34m:32s remains)
2017-12-07 08:40:40.975558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.644558 -2.7419057 -2.7126098 -2.5118771 -2.2081757 -2.0467751 -2.2168729 -2.0786493 -1.5167727 -1.1792028 -1.3704803 -1.7189367 -2.0969763 -2.7526398 -3.2729042][-3.0299582 -2.9016151 -2.787868 -2.6210327 -2.3591521 -2.31184 -2.4968436 -2.237746 -1.5650666 -1.2214205 -1.4451106 -1.8295836 -2.2085357 -2.838587 -3.3150175][-3.1683083 -2.7885628 -2.6448898 -2.5154333 -2.2416651 -2.3072188 -2.5190194 -2.1651728 -1.4772882 -1.2753716 -1.6736274 -2.1454821 -2.5253384 -3.0917718 -3.471056][-3.300662 -2.7513552 -2.6641889 -2.5802293 -2.2298107 -2.2976398 -2.4270329 -1.9434793 -1.3414836 -1.405468 -2.0400712 -2.5550489 -2.8406777 -3.2419896 -3.4963176][-3.0041957 -2.3558807 -2.432549 -2.479315 -2.1514168 -2.1693423 -2.088243 -1.4062247 -0.91982579 -1.2918844 -2.1334879 -2.6547194 -2.8547878 -3.1212597 -3.3351834][-2.3450992 -1.7809169 -2.0978312 -2.2702153 -1.9437726 -1.7751672 -1.3262448 -0.45055342 -0.16376972 -0.86691165 -1.8181624 -2.3049541 -2.4626298 -2.7023373 -3.0327654][-1.8311536 -1.5996885 -2.1791842 -2.3672166 -1.9782023 -1.5770426 -0.74718761 0.26434755 0.30370426 -0.67526031 -1.669692 -2.1186004 -2.2695398 -2.4794674 -2.8498745][-1.4501941 -1.6749861 -2.4301381 -2.5845966 -2.1787984 -1.6988382 -0.70570278 0.31711149 0.23241282 -0.745893 -1.5998454 -1.9594414 -2.1256847 -2.3207846 -2.664376][-1.5138044 -2.0866413 -2.7999063 -2.8356571 -2.482306 -2.0293484 -1.0973787 -0.23806524 -0.35490656 -1.0808463 -1.6246843 -1.8027401 -1.9208422 -2.0895495 -2.3715365][-1.9455352 -2.6211755 -3.1195331 -3.0245891 -2.7448616 -2.3468587 -1.6049309 -1.0148606 -1.1655862 -1.6487327 -1.9426219 -2.0472977 -2.1495874 -2.2828112 -2.4495168][-2.5436366 -3.1771364 -3.4330907 -3.2877879 -3.09591 -2.7629087 -2.227644 -1.8484092 -1.9549251 -2.1801751 -2.2581902 -2.3024468 -2.37095 -2.490871 -2.6335948][-3.0380301 -3.5858066 -3.6752961 -3.5543249 -3.4436481 -3.1771841 -2.7887881 -2.4936161 -2.470196 -2.4215484 -2.2825258 -2.2372303 -2.2364149 -2.3267217 -2.5314524][-3.0806484 -3.4744785 -3.4380956 -3.3364732 -3.259161 -3.0757239 -2.8342662 -2.6223552 -2.5220191 -2.3057323 -2.0442874 -1.9821687 -2.0071807 -2.1434934 -2.4318285][-2.7609837 -2.895793 -2.7585766 -2.6280618 -2.5246267 -2.4121151 -2.3249502 -2.2417731 -2.1900139 -2.0101554 -1.804883 -1.8056195 -1.9282458 -2.1800907 -2.5074139][-2.5700176 -2.4207897 -2.1718593 -1.9745638 -1.8253012 -1.7656076 -1.801846 -1.8618128 -1.954881 -1.9795959 -1.9750583 -2.1018991 -2.3167331 -2.6245108 -2.8559422]]...]
INFO - root - 2017-12-07 08:40:52.756426: step 24010, loss = 0.66, batch loss = 0.58 (6.6 examples/sec; 1.211 sec/batch; 103h:48m:21s remains)
INFO - root - 2017-12-07 08:41:04.419297: step 24020, loss = 0.67, batch loss = 0.59 (6.5 examples/sec; 1.233 sec/batch; 105h:39m:46s remains)
INFO - root - 2017-12-07 08:41:16.024023: step 24030, loss = 0.68, batch loss = 0.60 (7.1 examples/sec; 1.125 sec/batch; 96h:25m:17s remains)
INFO - root - 2017-12-07 08:41:27.523216: step 24040, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 1.112 sec/batch; 95h:14m:58s remains)
INFO - root - 2017-12-07 08:41:39.100148: step 24050, loss = 0.56, batch loss = 0.48 (7.2 examples/sec; 1.118 sec/batch; 95h:48m:22s remains)
INFO - root - 2017-12-07 08:41:50.789226: step 24060, loss = 0.87, batch loss = 0.80 (6.7 examples/sec; 1.198 sec/batch; 102h:39m:26s remains)
INFO - root - 2017-12-07 08:42:02.382128: step 24070, loss = 0.60, batch loss = 0.52 (6.6 examples/sec; 1.212 sec/batch; 103h:48m:35s remains)
INFO - root - 2017-12-07 08:42:13.992013: step 24080, loss = 0.66, batch loss = 0.59 (7.0 examples/sec; 1.142 sec/batch; 97h:49m:12s remains)
INFO - root - 2017-12-07 08:42:25.721137: step 24090, loss = 0.71, batch loss = 0.64 (6.9 examples/sec; 1.156 sec/batch; 99h:00m:38s remains)
INFO - root - 2017-12-07 08:42:37.393055: step 24100, loss = 0.86, batch loss = 0.78 (7.2 examples/sec; 1.117 sec/batch; 95h:41m:08s remains)
2017-12-07 08:42:38.278733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6966772 -3.2530982 -2.7985544 -2.6126845 -2.647305 -2.5347219 -2.3719788 -2.2879202 -2.2616277 -2.4648314 -2.739105 -2.8966174 -2.985193 -2.9039576 -2.709116][-3.6898379 -3.2599673 -2.8605428 -2.739944 -2.8496592 -2.7437508 -2.383249 -2.1394851 -2.0248137 -2.1460361 -2.466224 -2.7441685 -3.0154409 -3.0953984 -2.9003918][-3.7238955 -3.2861252 -2.9164605 -2.8075278 -2.9095082 -2.758939 -2.1282973 -1.6868129 -1.5204685 -1.586674 -2.0078294 -2.5087075 -3.040452 -3.3527932 -3.1803937][-3.8252697 -3.3323863 -2.9226811 -2.7115197 -2.703826 -2.4375942 -1.5418279 -1.0068848 -0.89949036 -0.976737 -1.5111003 -2.2525985 -3.0277109 -3.5490603 -3.4358876][-4.0601854 -3.5225108 -3.0427921 -2.6351447 -2.3930418 -1.9192908 -0.79610085 -0.29370213 -0.42154503 -0.67136788 -1.2665801 -2.0824285 -2.929111 -3.5431347 -3.5294759][-4.4357538 -3.90341 -3.3108883 -2.566607 -1.9109709 -1.1292498 0.16549206 0.52132654 -0.031340122 -0.63323832 -1.2767963 -1.9963825 -2.7562268 -3.4045167 -3.5499711][-4.766428 -4.2767053 -3.5340192 -2.394629 -1.2233713 -0.07904911 1.3415928 1.5089884 0.48294592 -0.49647784 -1.1503427 -1.711472 -2.3939025 -3.1450167 -3.5413136][-4.927072 -4.579731 -3.8480263 -2.5742111 -1.1069176 0.25512266 1.6932216 1.7610145 0.51520586 -0.5797224 -1.0936146 -1.4569974 -2.08707 -2.928772 -3.4812374][-4.9204292 -4.7774692 -4.2167816 -3.112184 -1.7720368 -0.51934195 0.77471256 1.0034146 0.083480358 -0.75357008 -1.100143 -1.4255185 -2.1426575 -3.0260091 -3.5308912][-4.7381954 -4.6399727 -4.2156577 -3.4142313 -2.4611113 -1.5189307 -0.38595486 0.19931602 -0.10785675 -0.52690244 -0.80340266 -1.316674 -2.3049831 -3.2813549 -3.6727328][-4.483767 -4.3239193 -3.9858854 -3.5308981 -3.0511713 -2.5195816 -1.6411903 -0.85903454 -0.68725061 -0.76043868 -0.99345231 -1.6035001 -2.6430714 -3.5166712 -3.7220731][-4.229208 -4.0050836 -3.7581887 -3.6182947 -3.5555794 -3.3920882 -2.7830758 -2.0118184 -1.6501386 -1.6345282 -1.8816917 -2.4368598 -3.2119789 -3.7238388 -3.6563749][-4.0134916 -3.6729894 -3.4124866 -3.4091744 -3.5555484 -3.5773511 -3.1401529 -2.4523106 -2.0872962 -2.1249852 -2.4296675 -2.8940096 -3.3810198 -3.5960908 -3.4013445][-3.9125843 -3.4686399 -3.1346927 -3.1129632 -3.2634213 -3.3160954 -3.0272262 -2.5265126 -2.2747624 -2.3690784 -2.6710968 -3.0202832 -3.290998 -3.3502429 -3.1592858][-3.9985449 -3.5482061 -3.1626348 -3.0861411 -3.1831043 -3.23216 -3.1200013 -2.8757889 -2.7822621 -2.9024234 -3.1367216 -3.3584366 -3.4586258 -3.4148865 -3.2346]]...]
INFO - root - 2017-12-07 08:42:49.718509: step 24110, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 1.170 sec/batch; 100h:15m:36s remains)
INFO - root - 2017-12-07 08:43:01.474959: step 24120, loss = 0.63, batch loss = 0.56 (6.9 examples/sec; 1.165 sec/batch; 99h:46m:58s remains)
INFO - root - 2017-12-07 08:43:13.050814: step 24130, loss = 0.75, batch loss = 0.67 (6.8 examples/sec; 1.168 sec/batch; 100h:02m:32s remains)
INFO - root - 2017-12-07 08:43:24.786471: step 24140, loss = 0.71, batch loss = 0.64 (6.6 examples/sec; 1.220 sec/batch; 104h:27m:50s remains)
INFO - root - 2017-12-07 08:43:36.396241: step 24150, loss = 0.84, batch loss = 0.77 (6.7 examples/sec; 1.196 sec/batch; 102h:27m:12s remains)
INFO - root - 2017-12-07 08:43:48.002169: step 24160, loss = 0.83, batch loss = 0.76 (7.3 examples/sec; 1.101 sec/batch; 94h:17m:21s remains)
INFO - root - 2017-12-07 08:43:59.435601: step 24170, loss = 0.64, batch loss = 0.57 (7.0 examples/sec; 1.146 sec/batch; 98h:08m:35s remains)
INFO - root - 2017-12-07 08:44:11.064532: step 24180, loss = 0.69, batch loss = 0.62 (6.8 examples/sec; 1.170 sec/batch; 100h:10m:04s remains)
INFO - root - 2017-12-07 08:44:22.755472: step 24190, loss = 0.64, batch loss = 0.57 (6.6 examples/sec; 1.204 sec/batch; 103h:04m:59s remains)
INFO - root - 2017-12-07 08:44:34.383467: step 24200, loss = 0.77, batch loss = 0.69 (7.0 examples/sec; 1.151 sec/batch; 98h:34m:25s remains)
2017-12-07 08:44:35.329792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9150567 -1.5635912 -1.5768099 -2.1956701 -2.7411826 -2.6203032 -2.3241224 -2.4476981 -2.7805676 -2.9875743 -2.8615117 -2.5264897 -2.2363398 -2.0415838 -2.1133859][-1.1380763 -0.517184 -0.63177776 -1.5305285 -2.0593626 -1.69034 -1.2784119 -1.4106276 -1.8673291 -2.2352412 -2.0878317 -1.6364403 -1.3749373 -1.3623343 -1.5364659][-0.83113313 -0.12451267 -0.37920141 -1.2601247 -1.5547125 -1.0370002 -0.74017477 -0.98813772 -1.5556984 -2.046176 -1.909097 -1.4972334 -1.4110072 -1.5147178 -1.4984176][-0.46976662 0.14276791 -0.27245188 -0.94704747 -0.97865748 -0.58068228 -0.51638341 -0.79006076 -1.3943167 -1.9397159 -1.8096404 -1.5126405 -1.5594232 -1.5233884 -1.0891335][-0.15877104 0.3747263 -0.10074615 -0.62018609 -0.60648847 -0.32949114 -0.11989546 -0.050219536 -0.53387332 -1.0511169 -0.91244435 -0.67191172 -0.73919845 -0.5258255 0.017591476][-0.052237988 0.4379468 0.038487911 -0.35855865 -0.35792208 0.019727707 0.69650364 1.0641356 0.43220711 -0.26910067 -0.14678144 0.22798014 0.36416578 0.7227211 1.0380497][0.17497253 0.65274382 0.43977213 0.215693 0.29434109 0.92997646 2.0587273 2.3699207 1.2515168 0.22542953 0.25544357 0.67404604 0.92805338 1.241786 1.2048292][0.22896862 0.72889853 0.76837206 0.77724981 0.98298407 1.7895203 2.9850049 2.9426289 1.4490075 0.25861549 0.094588757 0.34226465 0.48730564 0.63098097 0.41039896][0.028062344 0.54175282 0.6718154 0.70900393 0.82356834 1.4414258 2.2501678 1.9491472 0.80161953 0.088145256 0.028311253 0.16937876 0.10142565 0.020782471 -0.27831078][-0.26210165 0.24949408 0.3818717 0.26426125 0.13924599 0.43948889 0.9277997 0.69811058 0.21418571 0.1016326 0.24056959 0.3160882 0.077764988 -0.2292161 -0.60647631][-0.604614 -0.16423082 -0.032164574 -0.25506449 -0.52477717 -0.47359228 -0.18495941 -0.27158594 -0.36533165 -0.33274126 -0.3186965 -0.30649185 -0.45909762 -0.64812565 -0.91647077][-1.3380854 -1.1573963 -1.0191922 -1.1377287 -1.3260288 -1.2983427 -1.0314171 -0.97040844 -0.97616339 -1.0549304 -1.2343962 -1.2101629 -1.0951419 -1.009908 -1.1936717][-2.08279 -2.1486094 -1.9780741 -1.9225414 -1.9547865 -1.8916428 -1.6283021 -1.502295 -1.5988266 -1.8288898 -2.0440252 -1.8732061 -1.5868299 -1.4274616 -1.6801112][-2.4842148 -2.433876 -2.1573596 -2.0226214 -2.0327065 -2.0187447 -1.8368733 -1.7569096 -1.9230359 -2.1842904 -2.3160961 -2.0618494 -1.7776797 -1.7269177 -2.0505304][-2.5329709 -2.2804472 -1.9719129 -1.8932879 -2.0004492 -2.0856185 -2.0262718 -2.0189514 -2.1545129 -2.3032303 -2.3305209 -2.1284683 -1.9683769 -2.0170782 -2.2795417]]...]
INFO - root - 2017-12-07 08:44:47.007855: step 24210, loss = 0.75, batch loss = 0.68 (6.6 examples/sec; 1.211 sec/batch; 103h:43m:05s remains)
INFO - root - 2017-12-07 08:44:58.720574: step 24220, loss = 0.84, batch loss = 0.76 (6.8 examples/sec; 1.180 sec/batch; 101h:03m:20s remains)
INFO - root - 2017-12-07 08:45:10.366930: step 24230, loss = 0.64, batch loss = 0.56 (6.9 examples/sec; 1.154 sec/batch; 98h:49m:58s remains)
INFO - root - 2017-12-07 08:45:22.205393: step 24240, loss = 0.77, batch loss = 0.70 (6.7 examples/sec; 1.198 sec/batch; 102h:33m:13s remains)
INFO - root - 2017-12-07 08:45:33.954387: step 24250, loss = 0.66, batch loss = 0.59 (6.8 examples/sec; 1.180 sec/batch; 101h:02m:27s remains)
INFO - root - 2017-12-07 08:45:45.732312: step 24260, loss = 0.80, batch loss = 0.73 (6.6 examples/sec; 1.210 sec/batch; 103h:34m:29s remains)
INFO - root - 2017-12-07 08:45:57.384689: step 24270, loss = 0.85, batch loss = 0.78 (7.1 examples/sec; 1.133 sec/batch; 97h:01m:55s remains)
INFO - root - 2017-12-07 08:46:09.040324: step 24280, loss = 0.77, batch loss = 0.70 (7.0 examples/sec; 1.148 sec/batch; 98h:15m:28s remains)
INFO - root - 2017-12-07 08:46:20.763582: step 24290, loss = 0.75, batch loss = 0.68 (7.3 examples/sec; 1.101 sec/batch; 94h:15m:26s remains)
INFO - root - 2017-12-07 08:46:32.309903: step 24300, loss = 0.75, batch loss = 0.68 (6.6 examples/sec; 1.216 sec/batch; 104h:08m:33s remains)
2017-12-07 08:46:33.140875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4163606 -2.0163031 -2.02248 -2.0987968 -1.7186482 -0.86266327 -0.15860462 -0.10192442 -0.69964862 -1.6450667 -2.4382854 -2.7295952 -2.3154459 -1.5729837 -1.2411625][-2.3802483 -1.8514862 -1.799715 -1.9795632 -1.7907214 -1.0277293 -0.34813881 -0.1355505 -0.43374729 -1.2513642 -2.1820061 -2.7591052 -2.6075926 -1.9146473 -1.42766][-2.4305248 -1.7916803 -1.6626897 -1.9057093 -1.9624441 -1.5638418 -1.2821753 -1.2200496 -1.2560675 -1.6655688 -2.2955413 -2.8382831 -2.9321985 -2.5041456 -2.0036144][-2.3960783 -1.6901665 -1.5507798 -1.8640246 -2.0769575 -2.0021863 -2.106993 -2.2866204 -2.1850893 -2.1264341 -2.1995497 -2.3270154 -2.4905143 -2.5133026 -2.360966][-2.134537 -1.2579074 -1.0687203 -1.3775373 -1.6609406 -1.7774479 -2.0758185 -2.4211223 -2.339968 -2.0246086 -1.6453519 -1.3058126 -1.3589869 -1.7671967 -2.1430326][-1.8409052 -0.80601621 -0.48129606 -0.5593493 -0.6319561 -0.65978622 -0.9661355 -1.5378203 -1.7307751 -1.4842196 -0.9585681 -0.41753292 -0.28350782 -0.6347568 -1.0085139][-1.775779 -0.75026727 -0.35836935 -0.2452631 -0.072613239 0.12464046 -0.022400379 -0.67059064 -1.2016857 -1.2465417 -0.89389086 -0.33302832 0.096499443 0.13383913 0.10474539][-1.9579377 -1.1049275 -0.76811385 -0.63677216 -0.38864374 0.0030827522 0.20606613 -0.090266228 -0.49070334 -0.6504209 -0.60468006 -0.26455975 0.18970871 0.47859383 0.76867676][-2.1568246 -1.4089346 -1.1356087 -1.1029251 -0.8941195 -0.28836489 0.3390975 0.5704608 0.49965239 0.36430359 0.15307617 0.026024342 0.0653739 0.079404354 0.29665375][-2.1419334 -1.3741603 -1.1518917 -1.3111916 -1.3285809 -0.82514763 -0.08253336 0.47295189 0.68167353 0.64243174 0.34957218 -0.069213867 -0.40903234 -0.755301 -0.90316224][-2.0546303 -1.2743304 -1.1094427 -1.5094342 -1.874299 -1.796844 -1.3723509 -0.8845911 -0.60068727 -0.4685266 -0.53075004 -0.76992655 -1.0152557 -1.2636693 -1.4458723][-2.1565254 -1.3885412 -1.1891718 -1.6411071 -2.1580839 -2.3342776 -2.15405 -1.7689285 -1.5369537 -1.4434242 -1.4092531 -1.3949084 -1.3673685 -1.3731558 -1.4269533][-2.5000668 -1.8156645 -1.5179203 -1.816361 -2.2570066 -2.4257531 -2.2564964 -1.8740242 -1.6503103 -1.6454778 -1.6529679 -1.6186709 -1.5920241 -1.6414084 -1.7592928][-2.8598289 -2.3707871 -2.0974963 -2.3234978 -2.7101755 -2.8449154 -2.6497335 -2.2431467 -2.0115018 -2.0461862 -2.1201024 -2.1450307 -2.2048688 -2.3382781 -2.5001445][-2.9990931 -2.7105627 -2.5682652 -2.850738 -3.2417159 -3.3939772 -3.236836 -2.8783858 -2.6651595 -2.7094293 -2.7874532 -2.7822723 -2.7920041 -2.8401175 -2.9052908]]...]
INFO - root - 2017-12-07 08:46:44.844849: step 24310, loss = 0.82, batch loss = 0.75 (7.1 examples/sec; 1.131 sec/batch; 96h:51m:29s remains)
INFO - root - 2017-12-07 08:46:56.425644: step 24320, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 1.150 sec/batch; 98h:25m:36s remains)
INFO - root - 2017-12-07 08:47:08.191725: step 24330, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.178 sec/batch; 100h:50m:17s remains)
INFO - root - 2017-12-07 08:47:19.920654: step 24340, loss = 0.85, batch loss = 0.77 (6.6 examples/sec; 1.211 sec/batch; 103h:40m:10s remains)
INFO - root - 2017-12-07 08:47:31.606910: step 24350, loss = 0.70, batch loss = 0.63 (6.9 examples/sec; 1.166 sec/batch; 99h:49m:39s remains)
INFO - root - 2017-12-07 08:47:43.253626: step 24360, loss = 0.76, batch loss = 0.68 (6.9 examples/sec; 1.160 sec/batch; 99h:18m:39s remains)
INFO - root - 2017-12-07 08:47:54.925578: step 24370, loss = 0.68, batch loss = 0.61 (7.5 examples/sec; 1.073 sec/batch; 91h:50m:15s remains)
INFO - root - 2017-12-07 08:48:06.679648: step 24380, loss = 0.81, batch loss = 0.73 (6.6 examples/sec; 1.213 sec/batch; 103h:48m:38s remains)
INFO - root - 2017-12-07 08:48:18.392974: step 24390, loss = 0.96, batch loss = 0.89 (7.1 examples/sec; 1.132 sec/batch; 96h:52m:06s remains)
INFO - root - 2017-12-07 08:48:29.909823: step 24400, loss = 0.81, batch loss = 0.73 (7.1 examples/sec; 1.131 sec/batch; 96h:49m:26s remains)
2017-12-07 08:48:30.777772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7366054 -1.5091858 -1.0161953 -0.67093778 -0.95687222 -1.4362319 -1.9289846 -1.8613636 -1.29846 -1.0983269 -1.9096425 -3.2267246 -3.5164473 -2.3705266 -1.3026855][-1.8796098 -1.6401756 -0.88369751 -0.23475027 -0.28783035 -0.66184831 -1.2670002 -1.3488131 -0.80943322 -0.61071253 -1.5219409 -2.9148226 -3.1806154 -2.0563552 -1.1294591][-0.85243583 -1.0449445 -0.45972848 0.22130632 0.41330051 0.45430517 -0.024908543 -0.39467239 -0.284019 -0.46674442 -1.447773 -2.5894005 -2.5110517 -1.3699298 -0.72784376][0.02752924 -0.78815937 -0.55577636 0.045586109 0.46553612 1.0843673 1.1240292 0.75863647 0.35376263 -0.46963906 -1.6562912 -2.5188887 -1.9841998 -0.7185843 -0.3438735][0.44794464 -1.1153114 -1.2863986 -0.7003901 0.040054321 1.3143668 2.0856738 1.9574847 1.0712376 -0.43264914 -1.7841897 -2.3413486 -1.3642161 0.057333946 0.16247082][0.90352058 -1.1916947 -1.6835461 -1.0406389 0.14232016 2.1411228 3.7671652 4.0298548 2.4576464 -0.057724953 -1.8403049 -2.2184715 -0.86136484 0.81788921 0.85614109][1.0067501 -1.22172 -1.8682692 -1.203022 0.23681641 2.6495471 5.1035681 6.3352928 4.6658869 1.3341866 -1.1864915 -1.9395802 -0.69723153 1.003386 1.1334352][0.44543648 -1.6655896 -2.4798508 -2.0250497 -0.66611004 1.5278387 3.898221 5.707572 5.0475349 2.1958852 -0.3645277 -1.3838644 -0.53952169 0.67094088 0.66535425][-0.31644869 -2.1268284 -3.1065559 -2.9438722 -1.7552519 0.0072917938 1.5791101 2.91461 2.9157462 1.2174897 -0.503942 -1.0965068 -0.408257 0.16846371 -0.17337132][-0.9495697 -2.2994988 -3.3199463 -3.3952494 -2.3405561 -0.88398409 -0.018837929 0.62663889 0.74370623 -0.11176395 -0.99887562 -1.0502181 -0.37726736 -0.32806253 -1.06282][-1.4615495 -2.2845895 -3.1580243 -3.3324561 -2.497041 -1.4184587 -1.0896306 -1.0349934 -0.98922729 -1.3052537 -1.5627174 -1.3451686 -0.92156649 -1.2969265 -2.2993486][-2.268461 -2.6001732 -3.153048 -3.2040989 -2.5408533 -1.8364375 -1.8808072 -2.2802038 -2.4695952 -2.5229571 -2.3425136 -2.0158617 -1.9120786 -2.5495791 -3.6693873][-3.0693707 -3.1964405 -3.5064156 -3.418488 -2.834044 -2.2973816 -2.4191682 -2.9213386 -3.2676497 -3.2446704 -2.878509 -2.52571 -2.5344658 -3.1088212 -4.1069813][-3.1728721 -3.238097 -3.4369111 -3.3708045 -2.973264 -2.6346579 -2.73895 -3.1257272 -3.3948979 -3.2857833 -2.9093609 -2.6409378 -2.6630006 -3.0172286 -3.6112695][-3.0081067 -2.9845362 -3.0556729 -3.0275784 -2.8255692 -2.6855655 -2.809366 -3.1027718 -3.2981861 -3.2188578 -2.9691429 -2.8142769 -2.8375368 -3.0254259 -3.2678423]]...]
INFO - root - 2017-12-07 08:48:42.737004: step 24410, loss = 0.56, batch loss = 0.49 (6.5 examples/sec; 1.235 sec/batch; 105h:41m:12s remains)
INFO - root - 2017-12-07 08:48:54.228897: step 24420, loss = 0.82, batch loss = 0.75 (6.4 examples/sec; 1.249 sec/batch; 106h:51m:17s remains)
INFO - root - 2017-12-07 08:49:05.827511: step 24430, loss = 0.89, batch loss = 0.82 (7.2 examples/sec; 1.116 sec/batch; 95h:29m:45s remains)
INFO - root - 2017-12-07 08:49:17.599364: step 24440, loss = 0.81, batch loss = 0.74 (6.9 examples/sec; 1.159 sec/batch; 99h:08m:30s remains)
INFO - root - 2017-12-07 08:49:29.472025: step 24450, loss = 0.77, batch loss = 0.70 (6.7 examples/sec; 1.202 sec/batch; 102h:51m:14s remains)
INFO - root - 2017-12-07 08:49:41.093896: step 24460, loss = 0.74, batch loss = 0.67 (6.6 examples/sec; 1.220 sec/batch; 104h:23m:10s remains)
INFO - root - 2017-12-07 08:49:52.854982: step 24470, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 1.177 sec/batch; 100h:44m:36s remains)
INFO - root - 2017-12-07 08:50:04.626790: step 24480, loss = 0.86, batch loss = 0.78 (7.0 examples/sec; 1.137 sec/batch; 97h:17m:13s remains)
INFO - root - 2017-12-07 08:50:16.188347: step 24490, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 1.129 sec/batch; 96h:34m:45s remains)
INFO - root - 2017-12-07 08:50:27.779762: step 24500, loss = 1.03, batch loss = 0.95 (6.7 examples/sec; 1.192 sec/batch; 101h:56m:48s remains)
2017-12-07 08:50:28.700472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6083047 -3.769876 -3.8802893 -3.8638594 -3.6875644 -3.6453874 -3.7943563 -4.0766125 -4.42015 -4.5526047 -4.5829887 -4.3435225 -3.4813194 -2.2908626 -1.8029747][-3.4177856 -3.534071 -3.7147326 -3.7575307 -3.5719032 -3.5377526 -3.7323499 -4.1713781 -4.7890463 -5.111279 -5.1522527 -4.8465252 -3.8459651 -2.4386616 -1.7154291][-3.268868 -3.3231642 -3.4832752 -3.4147453 -3.0287607 -2.7900391 -2.7980587 -3.3144975 -4.3083935 -5.0354834 -5.2882338 -5.0488358 -4.0643082 -2.6297815 -1.7858245][-2.8255489 -2.9210861 -3.1003242 -2.8412194 -2.1229651 -1.5150433 -1.2138479 -1.7983701 -3.2309239 -4.510952 -5.1753836 -5.1824965 -4.3539052 -2.9901009 -2.048866][-2.1833787 -2.4961519 -2.832514 -2.5320179 -1.5544591 -0.54062057 0.16729879 -0.35826397 -1.9769719 -3.6098964 -4.7192626 -5.11052 -4.6186876 -3.4895468 -2.5123208][-1.783716 -2.4274611 -2.9434526 -2.6311822 -1.4407897 0.042901993 1.1319785 0.64533663 -0.97894239 -2.7187347 -4.1381454 -4.8186603 -4.6880865 -3.9425728 -3.1166186][-1.8581994 -2.7736714 -3.3125257 -2.8659847 -1.3809357 0.67753029 2.132165 1.6265564 0.018684864 -1.7403588 -3.3337798 -4.2491493 -4.5267982 -4.3354917 -3.9035618][-2.5449083 -3.6210854 -4.0167141 -3.3589044 -1.6480715 0.82895088 2.4956431 2.0897713 0.72532129 -0.82107353 -2.3370275 -3.3661246 -4.0153084 -4.39309 -4.4739947][-3.3350687 -4.5447755 -4.781693 -4.0177803 -2.2876132 0.13396549 1.5715694 1.3201489 0.44045496 -0.590714 -1.6950133 -2.5664077 -3.324492 -4.0264106 -4.525198][-3.7548323 -4.9459739 -5.0520911 -4.3414912 -2.7404838 -0.76486254 0.1303463 -0.13946676 -0.61875749 -1.0888915 -1.5617743 -1.9854808 -2.568172 -3.3037028 -4.0044422][-3.8046725 -4.8169889 -4.888402 -4.3615994 -3.0261993 -1.664937 -1.3061204 -1.5519898 -1.723659 -1.7632904 -1.6944559 -1.6204596 -1.863323 -2.3962493 -3.0321052][-3.7126896 -4.4956555 -4.60764 -4.2979622 -3.295126 -2.5102782 -2.5338452 -2.6869073 -2.6097856 -2.356477 -1.9750853 -1.5851169 -1.5453353 -1.8850822 -2.3721695][-3.4434958 -3.9552512 -4.0924058 -3.9664748 -3.3065176 -2.9667468 -3.2243209 -3.3431559 -3.2158237 -2.9037604 -2.4778891 -1.9840524 -1.813406 -2.1152761 -2.5652657][-3.1692045 -3.4182072 -3.6003222 -3.6505809 -3.2808142 -3.1646392 -3.427855 -3.4764438 -3.3917146 -3.2181263 -2.8810945 -2.3472738 -2.1306033 -2.5184412 -3.0884113][-3.1379395 -3.1674371 -3.4447818 -3.6862173 -3.5700884 -3.5242774 -3.6473517 -3.5213883 -3.3905289 -3.3256891 -3.0531325 -2.5226238 -2.3693221 -2.8676763 -3.5375891]]...]
INFO - root - 2017-12-07 08:50:40.173688: step 24510, loss = 0.68, batch loss = 0.61 (7.2 examples/sec; 1.113 sec/batch; 95h:15m:00s remains)
INFO - root - 2017-12-07 08:50:51.999192: step 24520, loss = 0.77, batch loss = 0.70 (6.6 examples/sec; 1.211 sec/batch; 103h:33m:42s remains)
INFO - root - 2017-12-07 08:51:03.642391: step 24530, loss = 0.86, batch loss = 0.79 (6.9 examples/sec; 1.158 sec/batch; 99h:01m:34s remains)
INFO - root - 2017-12-07 08:51:15.322548: step 24540, loss = 0.59, batch loss = 0.52 (6.9 examples/sec; 1.155 sec/batch; 98h:49m:03s remains)
INFO - root - 2017-12-07 08:51:26.994492: step 24550, loss = 0.74, batch loss = 0.67 (6.6 examples/sec; 1.219 sec/batch; 104h:15m:37s remains)
INFO - root - 2017-12-07 08:51:38.718843: step 24560, loss = 0.69, batch loss = 0.61 (6.6 examples/sec; 1.217 sec/batch; 104h:04m:23s remains)
INFO - root - 2017-12-07 08:51:50.494218: step 24570, loss = 0.77, batch loss = 0.70 (6.4 examples/sec; 1.258 sec/batch; 107h:37m:38s remains)
INFO - root - 2017-12-07 08:52:02.098993: step 24580, loss = 0.61, batch loss = 0.54 (7.0 examples/sec; 1.135 sec/batch; 97h:06m:24s remains)
INFO - root - 2017-12-07 08:52:13.795174: step 24590, loss = 0.81, batch loss = 0.74 (7.1 examples/sec; 1.122 sec/batch; 95h:56m:34s remains)
INFO - root - 2017-12-07 08:52:25.421863: step 24600, loss = 0.84, batch loss = 0.76 (7.1 examples/sec; 1.125 sec/batch; 96h:13m:31s remains)
2017-12-07 08:52:26.302893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1400676 -3.3166018 -3.3520074 -3.250397 -3.4029267 -3.8054128 -4.0301614 -4.2107878 -4.485044 -4.5838504 -4.3714986 -3.9853888 -3.5228682 -3.1778529 -3.0728021][-3.149121 -3.2765493 -3.06567 -2.6658077 -2.8112216 -3.3723078 -3.6730542 -3.9548297 -4.5073433 -4.8809009 -4.8207712 -4.4824123 -3.9394379 -3.39669 -3.1099639][-3.0983086 -3.06093 -2.4877815 -1.7695396 -1.9607673 -2.65026 -2.9081545 -3.1440473 -3.8510323 -4.4488134 -4.6010318 -4.5142889 -4.1408815 -3.5634766 -3.0975885][-2.9637303 -2.6502762 -1.69941 -0.83552408 -1.2476301 -2.0784445 -2.2070365 -2.2735662 -2.954123 -3.5825102 -3.7990847 -3.9191215 -3.8782372 -3.52628 -3.0890095][-2.7994256 -2.2006211 -1.0186808 -0.29387665 -1.0044832 -1.7990594 -1.6222677 -1.4151993 -1.9904196 -2.5457964 -2.6357751 -2.7434483 -3.0068567 -3.0806007 -2.9812031][-2.6973472 -1.9204397 -0.75803041 -0.3639369 -1.1655345 -1.5700448 -0.88026237 -0.39021444 -0.96032977 -1.5618181 -1.492851 -1.4272437 -1.8627708 -2.3107982 -2.5831859][-2.640646 -1.7902019 -0.8209641 -0.74339628 -1.389744 -1.2491779 -0.12569237 0.48896742 -0.17907238 -0.89701962 -0.70159721 -0.48218703 -1.044714 -1.7073872 -2.0901768][-2.5390019 -1.6808896 -0.95304608 -1.0739949 -1.5191188 -1.0692286 0.076694489 0.531281 -0.19394255 -0.8259995 -0.41031933 -0.026767731 -0.61383462 -1.3091013 -1.609916][-2.4497132 -1.6752243 -1.1993 -1.3886046 -1.6663206 -1.2667713 -0.48338795 -0.30988598 -0.90527749 -1.2043881 -0.56195164 -0.10874128 -0.64495325 -1.1956477 -1.3356564][-2.4560723 -1.8306866 -1.5348101 -1.6568089 -1.7895155 -1.5946667 -1.2404752 -1.2672591 -1.714401 -1.7653353 -1.1213703 -0.82169151 -1.2969244 -1.6115081 -1.5813835][-2.5174646 -2.0127335 -1.7476652 -1.6909163 -1.7166116 -1.7180374 -1.6315191 -1.7280681 -2.0640144 -2.0462594 -1.5879798 -1.5045435 -1.8903751 -1.9766321 -1.8446043][-2.6396961 -2.1683977 -1.7752128 -1.5064113 -1.5184753 -1.6909988 -1.7155495 -1.7758656 -2.005363 -1.9857461 -1.6901591 -1.7077713 -1.917479 -1.8018928 -1.6546199][-2.7873187 -2.2963178 -1.7279556 -1.3054688 -1.3501687 -1.5961294 -1.62203 -1.6643944 -1.8637214 -1.8701634 -1.6785274 -1.70805 -1.7741151 -1.58624 -1.4941959][-2.8529015 -2.3753414 -1.7574863 -1.3624923 -1.4864318 -1.710619 -1.6659408 -1.6836624 -1.8876989 -1.9141145 -1.7995882 -1.848418 -1.8811169 -1.7541347 -1.7005293][-2.8335056 -2.4065187 -1.8750374 -1.617923 -1.8057103 -1.9604974 -1.8518381 -1.8439944 -2.034059 -2.0698805 -2.013864 -2.0978138 -2.1749415 -2.1545382 -2.1292913]]...]
INFO - root - 2017-12-07 08:52:37.882890: step 24610, loss = 0.73, batch loss = 0.66 (7.2 examples/sec; 1.112 sec/batch; 95h:04m:24s remains)
INFO - root - 2017-12-07 08:52:49.705187: step 24620, loss = 0.75, batch loss = 0.68 (6.8 examples/sec; 1.175 sec/batch; 100h:30m:13s remains)
INFO - root - 2017-12-07 08:53:01.330932: step 24630, loss = 0.56, batch loss = 0.49 (7.2 examples/sec; 1.107 sec/batch; 94h:41m:42s remains)
INFO - root - 2017-12-07 08:53:13.219229: step 24640, loss = 0.79, batch loss = 0.71 (6.4 examples/sec; 1.246 sec/batch; 106h:31m:43s remains)
INFO - root - 2017-12-07 08:53:24.851289: step 24650, loss = 0.88, batch loss = 0.81 (6.8 examples/sec; 1.178 sec/batch; 100h:44m:26s remains)
INFO - root - 2017-12-07 08:53:36.334308: step 24660, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 1.134 sec/batch; 96h:57m:53s remains)
INFO - root - 2017-12-07 08:53:48.033144: step 24670, loss = 0.70, batch loss = 0.63 (7.6 examples/sec; 1.047 sec/batch; 89h:31m:43s remains)
INFO - root - 2017-12-07 08:53:59.861184: step 24680, loss = 0.81, batch loss = 0.73 (6.7 examples/sec; 1.189 sec/batch; 101h:39m:04s remains)
INFO - root - 2017-12-07 08:54:11.514033: step 24690, loss = 0.51, batch loss = 0.44 (6.7 examples/sec; 1.188 sec/batch; 101h:35m:31s remains)
INFO - root - 2017-12-07 08:54:23.275745: step 24700, loss = 0.77, batch loss = 0.70 (7.1 examples/sec; 1.130 sec/batch; 96h:35m:15s remains)
2017-12-07 08:54:24.183205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3875694 -4.4383965 -4.48806 -4.5359187 -4.5768337 -4.6089845 -4.6398416 -4.6698637 -4.7030163 -4.7308593 -4.7228622 -4.6853218 -4.6443024 -4.61037 -4.5841646][-4.3881826 -4.4572716 -4.5302272 -4.6065626 -4.681531 -4.7527094 -4.8337722 -4.9056258 -4.959003 -4.9892673 -4.9638286 -4.8922825 -4.8163948 -4.7547688 -4.7050858][-4.2523394 -4.3512468 -4.4627767 -4.5637269 -4.64052 -4.7058878 -4.7892909 -4.8484478 -4.8713727 -4.8750844 -4.8294673 -4.7402272 -4.6574349 -4.6025109 -4.5694804][-4.0144148 -4.1511269 -4.3065019 -4.4230342 -4.4665632 -4.4593835 -4.4462113 -4.3851523 -4.2954988 -4.22754 -4.1502948 -4.06893 -4.028718 -4.0249758 -4.042141][-3.6512547 -3.7688632 -3.8935454 -3.9499795 -3.8930671 -3.7477887 -3.586062 -3.40662 -3.2548983 -3.1811228 -3.1108603 -3.0587437 -3.0750186 -3.1258054 -3.1824226][-3.0934954 -3.0940022 -3.0708685 -2.9811721 -2.7984009 -2.5210619 -2.2556188 -2.0756598 -2.0441132 -2.1043332 -2.0958843 -2.0490031 -2.0526571 -2.0847738 -2.077981][-2.3129468 -2.0775323 -1.7917686 -1.5128815 -1.2360539 -0.92662692 -0.70521092 -0.72863364 -1.0033035 -1.3129988 -1.4180095 -1.3621666 -1.2853956 -1.1963437 -1.0405047][-1.7128718 -1.2929564 -0.80895615 -0.41422462 -0.13208485 0.077748775 0.1069479 -0.21881437 -0.79960275 -1.3247359 -1.552134 -1.5249763 -1.4325321 -1.3297238 -1.1930547][-1.9271357 -1.5701222 -1.1640527 -0.87664294 -0.71731567 -0.64275527 -0.73105 -1.1330867 -1.710736 -2.2079363 -2.4378033 -2.4219584 -2.3482144 -2.3122287 -2.3362114][-2.7653122 -2.6399012 -2.4736848 -2.3610823 -2.279547 -2.2137036 -2.2421107 -2.476727 -2.8487895 -3.2241302 -3.4612751 -3.499361 -3.4606102 -3.4691281 -3.5679264][-3.6100268 -3.7089984 -3.7576783 -3.7713485 -3.7280848 -3.6537151 -3.6270626 -3.7070994 -3.8659494 -4.068809 -4.2493148 -4.3162909 -4.32423 -4.3670969 -4.48313][-4.0872612 -4.3175635 -4.4896345 -4.5807314 -4.5888605 -4.5481586 -4.521718 -4.5272279 -4.5549259 -4.581665 -4.6068153 -4.5937223 -4.5798135 -4.6083074 -4.6843333][-4.123683 -4.3691363 -4.5754833 -4.7190332 -4.7938681 -4.8049808 -4.7949214 -4.7855783 -4.7831507 -4.7499108 -4.682457 -4.5970936 -4.5254664 -4.4844456 -4.4475713][-3.911752 -4.0983396 -4.2848549 -4.4576073 -4.5920944 -4.6667047 -4.7019839 -4.7143488 -4.7124338 -4.6604476 -4.566834 -4.4673533 -4.3605366 -4.2477436 -4.1118774][-3.6727595 -3.8016613 -3.9419596 -4.0864444 -4.2069249 -4.2861414 -4.3264489 -4.336257 -4.3236651 -4.25998 -4.1623673 -4.0637403 -3.9528255 -3.8332267 -3.7123425]]...]
INFO - root - 2017-12-07 08:54:35.945841: step 24710, loss = 0.78, batch loss = 0.70 (6.6 examples/sec; 1.218 sec/batch; 104h:07m:45s remains)
INFO - root - 2017-12-07 08:54:47.818589: step 24720, loss = 0.77, batch loss = 0.70 (6.4 examples/sec; 1.255 sec/batch; 107h:15m:18s remains)
INFO - root - 2017-12-07 08:54:59.378815: step 24730, loss = 0.54, batch loss = 0.47 (7.1 examples/sec; 1.125 sec/batch; 96h:11m:09s remains)
INFO - root - 2017-12-07 08:55:10.851337: step 24740, loss = 0.85, batch loss = 0.77 (7.1 examples/sec; 1.125 sec/batch; 96h:10m:40s remains)
INFO - root - 2017-12-07 08:55:22.496780: step 24750, loss = 0.66, batch loss = 0.58 (6.9 examples/sec; 1.155 sec/batch; 98h:41m:50s remains)
INFO - root - 2017-12-07 08:55:34.123497: step 24760, loss = 1.18, batch loss = 1.10 (6.6 examples/sec; 1.204 sec/batch; 102h:53m:25s remains)
INFO - root - 2017-12-07 08:55:45.730330: step 24770, loss = 0.82, batch loss = 0.74 (6.9 examples/sec; 1.152 sec/batch; 98h:26m:29s remains)
INFO - root - 2017-12-07 08:55:57.418056: step 24780, loss = 0.79, batch loss = 0.72 (7.2 examples/sec; 1.104 sec/batch; 94h:21m:43s remains)
INFO - root - 2017-12-07 08:56:09.114119: step 24790, loss = 0.78, batch loss = 0.71 (6.8 examples/sec; 1.185 sec/batch; 101h:18m:10s remains)
INFO - root - 2017-12-07 08:56:20.578958: step 24800, loss = 0.82, batch loss = 0.75 (6.9 examples/sec; 1.167 sec/batch; 99h:44m:38s remains)
2017-12-07 08:56:21.465227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7920725 -2.2468288 -2.9929147 -3.8303156 -4.000288 -3.8202097 -3.82953 -3.7891493 -3.639385 -3.6199076 -3.6218848 -3.5206695 -3.2858291 -3.1147811 -3.07988][-1.5756564 -1.9107077 -2.5380507 -3.3200345 -3.5828838 -3.5450947 -3.6417592 -3.6259754 -3.4535093 -3.3944294 -3.4677286 -3.5385723 -3.4712365 -3.3941855 -3.4099984][-1.8040018 -1.8991723 -2.2313323 -2.8006406 -3.054765 -3.1201661 -3.3029513 -3.3417513 -3.2283034 -3.2469742 -3.5069103 -3.8765793 -4.0993686 -4.1469784 -4.1282234][-2.7662892 -2.6022916 -2.6127472 -2.9853139 -3.2658572 -3.456203 -3.7276495 -3.8435249 -3.8610055 -4.0349946 -4.42156 -4.92518 -5.2002769 -5.1173496 -4.8475156][-3.7505705 -3.5062737 -3.4006526 -3.7049322 -4.0122371 -4.203846 -4.3764997 -4.3762922 -4.3718233 -4.5924163 -5.0136003 -5.5464163 -5.8235531 -5.6492414 -5.2008348][-3.9975324 -3.8493605 -3.7972062 -4.0871658 -4.3216329 -4.2713966 -4.0547237 -3.6651921 -3.4056096 -3.5385199 -3.9759262 -4.5943818 -5.0548353 -5.1093884 -4.8716159][-3.9598749 -3.7900729 -3.6371517 -3.7528381 -3.7702832 -3.3903062 -2.7908459 -2.0915558 -1.6920602 -1.8868368 -2.4242177 -3.1029224 -3.6552913 -3.8816681 -3.9128079][-3.8876364 -3.4708614 -2.8902411 -2.5573268 -2.2739801 -1.6988332 -1.0056257 -0.30785275 -0.018649101 -0.46751618 -1.2133152 -1.9555986 -2.4501123 -2.6605299 -2.8102789][-3.2519197 -2.6253448 -1.7767081 -1.2174056 -0.90314722 -0.46956134 0.018749714 0.55778694 0.74190331 0.13434696 -0.70501924 -1.4107118 -1.7385187 -1.8517702 -2.0247188][-2.338253 -1.7047606 -0.98394012 -0.60780597 -0.56376982 -0.551908 -0.52365732 -0.32413769 -0.27304125 -0.78061342 -1.4293098 -1.8701706 -1.8675842 -1.7316 -1.7760243][-1.7360141 -1.3292718 -0.93628049 -0.83388925 -0.97994494 -1.2465334 -1.597615 -1.7420847 -1.8302095 -2.1887708 -2.5980277 -2.7948298 -2.5502384 -2.1507275 -1.9864573][-1.6108093 -1.4318955 -1.2644324 -1.2697198 -1.3564408 -1.6340029 -2.1196976 -2.4463315 -2.5860925 -2.7919807 -3.0795927 -3.2903762 -3.1268055 -2.6936564 -2.3413689][-1.9926219 -1.8222771 -1.6018338 -1.4845045 -1.3726544 -1.5267951 -1.9993994 -2.3149388 -2.4071076 -2.4876285 -2.7304156 -3.1059115 -3.2426162 -2.9466715 -2.4501667][-2.2378769 -2.0256276 -1.6749339 -1.3829618 -1.1691272 -1.2954717 -1.7537055 -2.0017772 -1.9965146 -1.9859254 -2.1358089 -2.5390804 -2.8590689 -2.7073998 -2.1793053][-2.2239132 -2.0617743 -1.7224669 -1.3959107 -1.2052765 -1.3763454 -1.8468723 -2.0877762 -2.0454369 -1.9617767 -1.9661038 -2.2202756 -2.5458717 -2.5167994 -2.1334794]]...]
INFO - root - 2017-12-07 08:56:33.036596: step 24810, loss = 0.66, batch loss = 0.58 (7.4 examples/sec; 1.077 sec/batch; 92h:02m:40s remains)
INFO - root - 2017-12-07 08:56:44.611764: step 24820, loss = 0.69, batch loss = 0.61 (6.7 examples/sec; 1.201 sec/batch; 102h:41m:04s remains)
INFO - root - 2017-12-07 08:56:56.373545: step 24830, loss = 0.53, batch loss = 0.46 (6.6 examples/sec; 1.219 sec/batch; 104h:11m:29s remains)
INFO - root - 2017-12-07 08:57:08.007957: step 24840, loss = 0.52, batch loss = 0.45 (6.9 examples/sec; 1.152 sec/batch; 98h:29m:03s remains)
INFO - root - 2017-12-07 08:57:19.602317: step 24850, loss = 0.81, batch loss = 0.73 (7.0 examples/sec; 1.144 sec/batch; 97h:46m:30s remains)
INFO - root - 2017-12-07 08:57:31.189058: step 24860, loss = 0.71, batch loss = 0.64 (6.7 examples/sec; 1.198 sec/batch; 102h:22m:23s remains)
INFO - root - 2017-12-07 08:57:42.858861: step 24870, loss = 0.79, batch loss = 0.72 (6.5 examples/sec; 1.234 sec/batch; 105h:25m:39s remains)
INFO - root - 2017-12-07 08:57:54.563240: step 24880, loss = 0.64, batch loss = 0.57 (6.8 examples/sec; 1.178 sec/batch; 100h:40m:22s remains)
INFO - root - 2017-12-07 08:58:06.305127: step 24890, loss = 0.72, batch loss = 0.65 (7.2 examples/sec; 1.106 sec/batch; 94h:28m:16s remains)
INFO - root - 2017-12-07 08:58:17.974650: step 24900, loss = 0.73, batch loss = 0.66 (6.9 examples/sec; 1.153 sec/batch; 98h:31m:26s remains)
2017-12-07 08:58:18.845174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3215466 -1.4123008 -1.1123383 -0.8282187 -0.77155161 -0.80250263 -0.87196493 -1.0663979 -1.3924406 -1.6199744 -1.4620881 -0.83566809 -0.058451176 0.44397831 0.59329271][-1.0661776 -1.2102273 -1.0629923 -0.89195347 -0.83725619 -0.807276 -0.78871155 -0.85654044 -1.0286167 -1.0864735 -0.83647537 -0.29764223 0.26770163 0.55522394 0.51253128][-0.73689675 -1.0192146 -1.1619382 -1.2104394 -1.1932781 -1.0497918 -0.85718632 -0.74950123 -0.73655128 -0.64030385 -0.37874079 -0.025256157 0.24586964 0.28338289 0.10257912][-0.49782825 -1.0151165 -1.5482426 -1.8170192 -1.723314 -1.2884197 -0.75226283 -0.40205145 -0.31304502 -0.29132509 -0.21254206 -0.046871185 0.083328724 0.050863743 -0.084674358][-0.57722831 -1.3540506 -2.2694492 -2.7521536 -2.4868731 -1.5623116 -0.39903498 0.38803959 0.473279 0.17987061 -0.14028549 -0.22523785 -0.15269852 -0.12657881 -0.079323292][-0.56425381 -1.3996921 -2.444366 -2.9777923 -2.4836638 -1.0932112 0.54762983 1.4763799 1.1789398 0.29856062 -0.40625763 -0.56969357 -0.33742666 -0.048262119 0.28156281][-0.35135078 -1.0679054 -1.9825854 -2.3547025 -1.5495055 0.2350955 2.1684666 3.0549045 2.2702136 0.80762291 -0.23737335 -0.41604567 0.029817581 0.62813377 1.1383271][-0.22393417 -0.8348484 -1.5730925 -1.7803781 -0.897758 0.82544947 2.5667458 3.2768197 2.3862958 0.86776495 -0.14467525 -0.23395729 0.337142 1.0472822 1.4675331][-0.35134077 -0.85344338 -1.3167105 -1.277302 -0.43228197 0.87117195 1.9100399 2.0722637 1.2386308 0.15847826 -0.37226582 -0.21830416 0.3272624 0.90074444 1.1074762][-0.56413841 -0.9843266 -1.1946282 -0.94795394 -0.19729424 0.65927315 1.0512662 0.7622714 0.0386014 -0.50663257 -0.49094343 -0.10562754 0.29733086 0.57485867 0.57221889][-0.55198574 -0.8800385 -0.9181428 -0.58713841 -0.054142475 0.35141706 0.32700872 -0.12248468 -0.66990066 -0.8691256 -0.58040929 -0.14442348 0.076355457 0.076151371 -0.069906235][-0.50205684 -0.69133496 -0.56603742 -0.16462803 0.22358751 0.32375383 0.086753368 -0.32333565 -0.65219069 -0.6435504 -0.32622528 -0.00048923492 0.053767681 -0.11819267 -0.32966757][-0.42366123 -0.50426817 -0.34563684 -0.021091938 0.20632219 0.14605379 -0.11821938 -0.36746264 -0.45860863 -0.30817223 -0.057517529 0.086745739 0.0078978539 -0.2260704 -0.4260602][-0.47324753 -0.46158147 -0.3077569 -0.083333015 0.050596237 -0.0084114075 -0.17869949 -0.28356171 -0.25372982 -0.1058588 0.028285503 0.031130791 -0.0965004 -0.29638052 -0.44332743][-0.56918716 -0.57439065 -0.46472478 -0.29553938 -0.17573881 -0.16716146 -0.23222971 -0.24590254 -0.16535044 -0.038820267 0.045352459 0.033856392 -0.052142143 -0.18198061 -0.29527283]]...]
INFO - root - 2017-12-07 08:58:30.798823: step 24910, loss = 0.81, batch loss = 0.73 (6.5 examples/sec; 1.231 sec/batch; 105h:11m:43s remains)
INFO - root - 2017-12-07 08:58:42.417427: step 24920, loss = 0.89, batch loss = 0.82 (7.4 examples/sec; 1.088 sec/batch; 92h:59m:36s remains)
INFO - root - 2017-12-07 08:58:54.017412: step 24930, loss = 0.81, batch loss = 0.74 (6.7 examples/sec; 1.197 sec/batch; 102h:17m:34s remains)
INFO - root - 2017-12-07 08:59:05.706015: step 24940, loss = 0.85, batch loss = 0.77 (7.0 examples/sec; 1.147 sec/batch; 97h:57m:43s remains)
INFO - root - 2017-12-07 08:59:17.322570: step 24950, loss = 0.74, batch loss = 0.67 (6.7 examples/sec; 1.201 sec/batch; 102h:34m:00s remains)
INFO - root - 2017-12-07 08:59:29.049209: step 24960, loss = 0.66, batch loss = 0.59 (7.1 examples/sec; 1.131 sec/batch; 96h:39m:18s remains)
INFO - root - 2017-12-07 08:59:40.757195: step 24970, loss = 0.87, batch loss = 0.80 (7.1 examples/sec; 1.127 sec/batch; 96h:17m:25s remains)
INFO - root - 2017-12-07 08:59:52.346195: step 24980, loss = 0.60, batch loss = 0.53 (7.0 examples/sec; 1.147 sec/batch; 97h:58m:33s remains)
INFO - root - 2017-12-07 09:00:03.978575: step 24990, loss = 0.78, batch loss = 0.71 (6.6 examples/sec; 1.204 sec/batch; 102h:51m:44s remains)
INFO - root - 2017-12-07 09:00:15.563069: step 25000, loss = 0.83, batch loss = 0.76 (6.8 examples/sec; 1.182 sec/batch; 100h:56m:40s remains)
2017-12-07 09:00:16.493929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2168858 -2.296768 -2.1505218 -1.8895626 -1.7524197 -1.8173878 -1.9071774 -1.9128947 -1.977834 -2.0306716 -1.9022844 -1.5715873 -1.2463913 -0.75991821 -0.10557842][-2.4233356 -2.3894548 -2.1010404 -1.7645798 -1.6307197 -1.7461286 -1.8913627 -1.9125779 -1.9173789 -1.8120346 -1.533138 -1.1383264 -0.906564 -0.66764092 -0.25352478][-2.4568624 -2.2294409 -1.8377056 -1.5418937 -1.4843421 -1.6366971 -1.8076265 -1.8338611 -1.7400148 -1.4352889 -0.9826889 -0.55747557 -0.42322206 -0.46550012 -0.43298554][-2.34587 -1.9754553 -1.6058078 -1.4349058 -1.3961847 -1.4323323 -1.5126891 -1.5020485 -1.3219295 -0.91897821 -0.47265577 -0.19215631 -0.17452669 -0.37455225 -0.63841128][-1.9154568 -1.5667009 -1.3354948 -1.3363636 -1.3490264 -1.2944593 -1.2133396 -1.0177617 -0.65486407 -0.29037714 -0.13354588 -0.18456745 -0.28342104 -0.49153066 -0.84227538][-1.2368572 -1.0464783 -0.89188409 -0.94114089 -1.0611453 -1.0749338 -0.90838456 -0.43885064 0.053015232 0.097450733 -0.24911356 -0.57693315 -0.61266613 -0.65471363 -0.87610483][-0.70843863 -0.70657468 -0.44201827 -0.2637558 -0.39431858 -0.47919202 -0.13026571 0.77199888 1.1949506 0.48653793 -0.54863191 -1.0619466 -1.016279 -0.9621985 -1.031523][-0.26390314 -0.40238667 0.030339718 0.42713356 0.23523331 0.099135876 0.72464132 2.0521102 2.1881738 0.65738583 -0.80874085 -1.3330665 -1.2768621 -1.2491462 -1.1963437][0.032554626 -0.21731949 0.24230433 0.60400772 0.24557114 0.02400589 0.68668365 1.9591818 1.7246966 -0.057374 -1.4032657 -1.7447681 -1.6518304 -1.5494192 -1.3481922][-0.13012123 -0.4497366 -0.073131561 0.1408639 -0.28923798 -0.4877491 -0.0086278915 0.83504581 0.3509326 -1.1706183 -2.1363461 -2.3189929 -2.1782565 -1.955725 -1.658808][-0.49349928 -0.82790947 -0.60391665 -0.56261253 -0.96103716 -1.0868196 -0.8669138 -0.45777655 -0.98213243 -2.1399393 -2.7808766 -2.8360457 -2.687772 -2.5465846 -2.3956046][-0.95190334 -1.2490349 -1.1753423 -1.2916076 -1.6844234 -1.789597 -1.8039253 -1.7668104 -2.2935593 -3.1801791 -3.5550666 -3.4527709 -3.315877 -3.3361316 -3.40991][-1.9914334 -2.1916618 -2.1025887 -2.1647372 -2.4341779 -2.511281 -2.6610346 -2.8781831 -3.4529402 -4.191545 -4.4042454 -4.2264161 -4.1085892 -4.1541777 -4.2412043][-3.456234 -3.537605 -3.3501632 -3.2428298 -3.3355894 -3.3449047 -3.4806805 -3.7740321 -4.2776318 -4.7808847 -4.9132881 -4.8293705 -4.7942367 -4.798697 -4.7998843][-4.442853 -4.4411535 -4.2282758 -4.0746241 -4.113934 -4.1450329 -4.2422848 -4.4353933 -4.7203174 -4.9131746 -4.9476542 -4.9521122 -4.9731765 -4.9502606 -4.92727]]...]
INFO - root - 2017-12-07 09:00:28.392759: step 25010, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.155 sec/batch; 98h:38m:22s remains)
INFO - root - 2017-12-07 09:00:40.209659: step 25020, loss = 0.84, batch loss = 0.77 (6.5 examples/sec; 1.230 sec/batch; 105h:04m:29s remains)
INFO - root - 2017-12-07 09:00:51.719219: step 25030, loss = 0.87, batch loss = 0.80 (6.6 examples/sec; 1.208 sec/batch; 103h:12m:25s remains)
INFO - root - 2017-12-07 09:01:03.415795: step 25040, loss = 0.75, batch loss = 0.67 (7.1 examples/sec; 1.125 sec/batch; 96h:05m:41s remains)
INFO - root - 2017-12-07 09:01:15.044022: step 25050, loss = 0.78, batch loss = 0.70 (6.8 examples/sec; 1.172 sec/batch; 100h:05m:22s remains)
INFO - root - 2017-12-07 09:01:26.771789: step 25060, loss = 0.74, batch loss = 0.67 (6.9 examples/sec; 1.159 sec/batch; 99h:01m:00s remains)
INFO - root - 2017-12-07 09:01:38.674134: step 25070, loss = 0.50, batch loss = 0.43 (6.8 examples/sec; 1.173 sec/batch; 100h:12m:36s remains)
INFO - root - 2017-12-07 09:01:50.250717: step 25080, loss = 0.61, batch loss = 0.54 (6.6 examples/sec; 1.212 sec/batch; 103h:27m:38s remains)
INFO - root - 2017-12-07 09:02:01.829766: step 25090, loss = 0.61, batch loss = 0.54 (7.1 examples/sec; 1.122 sec/batch; 95h:46m:05s remains)
INFO - root - 2017-12-07 09:02:13.558977: step 25100, loss = 0.99, batch loss = 0.92 (6.9 examples/sec; 1.158 sec/batch; 98h:51m:06s remains)
2017-12-07 09:02:14.483634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8406038 -2.801182 -2.7264829 -2.6632075 -2.6578016 -2.7097511 -2.7538371 -2.7642159 -2.7364867 -2.7032857 -2.7186711 -2.7487895 -2.7502689 -2.7808864 -2.828285][-3.0312634 -2.970088 -2.8189297 -2.6677804 -2.6113062 -2.6584511 -2.7305143 -2.8159039 -2.8778939 -2.9013236 -2.9662848 -2.9928918 -2.9031522 -2.8065825 -2.718082][-3.4848366 -3.4785724 -3.391479 -3.2937014 -3.256959 -3.2863746 -3.3412275 -3.4596934 -3.5463576 -3.4551837 -3.3519502 -3.24866 -3.065011 -2.8820577 -2.7056274][-3.744864 -3.7734456 -3.7561083 -3.7003269 -3.6553991 -3.6832263 -3.8638554 -4.2234349 -4.5473871 -4.4818568 -4.19191 -3.7944093 -3.3496871 -2.9354897 -2.6274967][-3.4573927 -3.5406492 -3.6289442 -3.5737379 -3.3430498 -3.1138134 -3.1662171 -3.6488886 -4.3360448 -4.7289186 -4.7658229 -4.427681 -3.7871108 -3.0192385 -2.4230363][-2.6335654 -2.7746925 -3.001399 -3.041832 -2.7211328 -2.2637379 -1.9979458 -2.2326992 -2.9897819 -3.8717861 -4.5757513 -4.7299237 -4.1998663 -3.1929314 -2.2945576][-1.8777692 -1.9944623 -2.1561098 -2.0829353 -1.6348963 -1.0341711 -0.48605752 -0.37534809 -1.0888321 -2.4139447 -3.8278227 -4.6033034 -4.3725986 -3.3589008 -2.3233593][-1.9154325 -2.0764647 -2.0797944 -1.7082829 -0.96998596 -0.080010891 0.91179276 1.4504991 0.75273514 -1.0079556 -2.9581273 -4.1890197 -4.2934446 -3.4541345 -2.4468436][-2.548959 -2.7577512 -2.6184506 -2.0299759 -1.185591 -0.33373022 0.56921339 1.1103373 0.55267 -0.99658513 -2.680613 -3.7477775 -3.9686356 -3.3702979 -2.5135274][-3.1447034 -3.3805199 -3.2073398 -2.6515145 -2.0144687 -1.5136545 -1.0788157 -0.81704855 -1.1765182 -2.1591911 -3.1753612 -3.7626064 -3.8921912 -3.4137566 -2.7045441][-3.2414517 -3.540911 -3.5272212 -3.2517562 -2.9790912 -2.8009043 -2.6659834 -2.5297627 -2.66144 -3.154583 -3.6709189 -3.9802549 -4.0819244 -3.7336287 -3.1598577][-2.8379745 -3.0959926 -3.2513452 -3.3113925 -3.395915 -3.4452207 -3.4256096 -3.3018951 -3.20271 -3.2513723 -3.3824787 -3.6231134 -3.8905814 -3.8320673 -3.4894176][-2.535135 -2.6252985 -2.7491446 -2.9129226 -3.1299117 -3.2610302 -3.2273955 -3.0252156 -2.7802458 -2.6639113 -2.7294359 -3.0299661 -3.4245543 -3.5710666 -3.417558][-2.6669176 -2.6075411 -2.6206193 -2.721853 -2.8466296 -2.8729644 -2.7473655 -2.5213442 -2.3270204 -2.2656052 -2.3613088 -2.621233 -2.9289336 -3.0928149 -3.0567389][-3.0153146 -2.873559 -2.7877979 -2.8329387 -2.9359879 -2.9947402 -2.9512653 -2.8095734 -2.6739993 -2.6029341 -2.6017039 -2.6802912 -2.8062172 -2.9022613 -2.9028926]]...]
INFO - root - 2017-12-07 09:02:26.302999: step 25110, loss = 0.86, batch loss = 0.79 (6.4 examples/sec; 1.256 sec/batch; 107h:15m:08s remains)
INFO - root - 2017-12-07 09:02:37.709008: step 25120, loss = 0.74, batch loss = 0.67 (7.0 examples/sec; 1.139 sec/batch; 97h:16m:41s remains)
INFO - root - 2017-12-07 09:02:49.101982: step 25130, loss = 0.83, batch loss = 0.76 (7.2 examples/sec; 1.114 sec/batch; 95h:08m:59s remains)
INFO - root - 2017-12-07 09:03:00.731099: step 25140, loss = 0.56, batch loss = 0.48 (6.7 examples/sec; 1.195 sec/batch; 102h:00m:20s remains)
INFO - root - 2017-12-07 09:03:12.485417: step 25150, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.184 sec/batch; 101h:06m:07s remains)
INFO - root - 2017-12-07 09:03:24.076549: step 25160, loss = 0.88, batch loss = 0.80 (7.0 examples/sec; 1.140 sec/batch; 97h:21m:33s remains)
INFO - root - 2017-12-07 09:03:35.681575: step 25170, loss = 0.70, batch loss = 0.62 (6.9 examples/sec; 1.154 sec/batch; 98h:30m:11s remains)
INFO - root - 2017-12-07 09:03:47.098782: step 25180, loss = 0.68, batch loss = 0.61 (7.0 examples/sec; 1.144 sec/batch; 97h:37m:46s remains)
INFO - root - 2017-12-07 09:03:58.930504: step 25190, loss = 0.77, batch loss = 0.70 (6.6 examples/sec; 1.211 sec/batch; 103h:20m:18s remains)
INFO - root - 2017-12-07 09:04:10.620515: step 25200, loss = 0.77, batch loss = 0.70 (6.8 examples/sec; 1.179 sec/batch; 100h:40m:20s remains)
2017-12-07 09:04:11.488889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6597822 -2.7236676 -2.9099503 -2.6975923 -2.0018015 -1.720557 -2.1072907 -2.7217321 -3.2356815 -3.5696664 -3.7046206 -3.4899068 -2.8467555 -2.2220917 -1.7432091][-2.585305 -2.647398 -2.8094063 -2.4681468 -1.6219571 -1.2346804 -1.7038176 -2.4571171 -3.0449898 -3.4428329 -3.6810679 -3.5388358 -2.8898482 -2.2634456 -1.90535][-2.5130525 -2.3919015 -2.3507087 -1.8193884 -0.91475058 -0.50160742 -1.0042374 -1.7279725 -2.1293585 -2.3814082 -2.5825467 -2.5372791 -2.0747361 -1.6878982 -1.5824597][-2.381772 -2.0687754 -1.8207879 -1.1945391 -0.45083022 -0.22613096 -0.70409989 -1.2032132 -1.3483596 -1.5763385 -1.931597 -2.1095264 -1.8505287 -1.6340625 -1.5988908][-2.2728453 -1.9393346 -1.6540926 -1.1057863 -0.63007617 -0.51986 -0.64832926 -0.62886596 -0.60975432 -1.1572263 -1.9171443 -2.4074168 -2.3541822 -2.2372947 -2.1849144][-2.1997225 -1.9345846 -1.6422219 -1.1390471 -0.78568363 -0.51322126 -0.016884804 0.58841848 0.5716877 -0.52022862 -1.8095925 -2.6436338 -2.8367786 -2.8034246 -2.7414036][-2.2821438 -2.0459929 -1.5987415 -0.91668177 -0.35885572 0.26985836 1.2541146 2.158535 1.8940926 0.36462593 -1.1624959 -2.1282318 -2.4741774 -2.4818945 -2.3866222][-2.4714923 -2.2622311 -1.6801975 -0.80239916 0.0024247169 0.83662987 1.8269062 2.4828768 1.9802852 0.67563581 -0.37899971 -1.0143909 -1.299736 -1.2601836 -1.0802951][-2.6192431 -2.4554629 -1.9352655 -1.0624158 -0.19732428 0.52103329 1.0503006 1.1521111 0.64584827 0.066479206 -0.062965393 -0.12394476 -0.31689835 -0.35263634 -0.16836548][-2.7085547 -2.6315775 -2.2718658 -1.4553552 -0.57462358 0.027098656 0.21970654 0.0023574829 -0.38225126 -0.42745543 0.030820847 0.27478218 0.02549839 -0.2937665 -0.35430241][-2.6925967 -2.7133107 -2.5211673 -1.7180965 -0.73977017 -0.095666409 0.0304389 -0.23450994 -0.48859835 -0.38644266 0.10241842 0.28371191 -0.082094669 -0.56016922 -0.794193][-2.6486487 -2.7253485 -2.6113176 -1.7683728 -0.644758 0.14892769 0.3425684 0.10102224 -0.10774136 -0.12137985 0.063309669 0.032605171 -0.27930498 -0.61278892 -0.82661009][-2.6378436 -2.7611768 -2.6992478 -1.817672 -0.57407951 0.31500006 0.52950716 0.29494238 0.14715481 0.042321682 -0.067855835 -0.21072006 -0.27495384 -0.23190928 -0.27286625][-2.6332817 -2.7897713 -2.7890668 -1.9513416 -0.70894575 0.12205935 0.24385118 -0.0068645477 -0.061349869 -0.091757774 -0.271173 -0.38392305 -0.25313854 0.023522377 0.15319204][-2.6241541 -2.75262 -2.7756662 -2.0157983 -0.82861543 -0.093019485 -0.077901363 -0.34087181 -0.35246229 -0.30373478 -0.471802 -0.61714721 -0.58302712 -0.4298141 -0.26519871]]...]
INFO - root - 2017-12-07 09:04:23.235953: step 25210, loss = 0.64, batch loss = 0.57 (7.1 examples/sec; 1.121 sec/batch; 95h:40m:39s remains)
INFO - root - 2017-12-07 09:04:34.870518: step 25220, loss = 0.63, batch loss = 0.56 (6.6 examples/sec; 1.216 sec/batch; 103h:46m:09s remains)
INFO - root - 2017-12-07 09:04:46.398691: step 25230, loss = 0.62, batch loss = 0.55 (7.1 examples/sec; 1.129 sec/batch; 96h:20m:15s remains)
INFO - root - 2017-12-07 09:04:57.870292: step 25240, loss = 0.75, batch loss = 0.67 (7.3 examples/sec; 1.089 sec/batch; 92h:55m:44s remains)
INFO - root - 2017-12-07 09:05:09.558870: step 25250, loss = 0.74, batch loss = 0.66 (6.8 examples/sec; 1.168 sec/batch; 99h:41m:20s remains)
INFO - root - 2017-12-07 09:05:21.146124: step 25260, loss = 0.69, batch loss = 0.62 (7.3 examples/sec; 1.095 sec/batch; 93h:25m:58s remains)
INFO - root - 2017-12-07 09:05:32.865743: step 25270, loss = 0.66, batch loss = 0.59 (6.6 examples/sec; 1.211 sec/batch; 103h:21m:32s remains)
INFO - root - 2017-12-07 09:05:44.444428: step 25280, loss = 0.63, batch loss = 0.56 (6.8 examples/sec; 1.180 sec/batch; 100h:39m:45s remains)
INFO - root - 2017-12-07 09:05:56.147359: step 25290, loss = 0.61, batch loss = 0.54 (7.2 examples/sec; 1.105 sec/batch; 94h:19m:36s remains)
INFO - root - 2017-12-07 09:06:07.889553: step 25300, loss = 0.77, batch loss = 0.70 (6.4 examples/sec; 1.254 sec/batch; 107h:02m:58s remains)
2017-12-07 09:06:08.700217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1375232 -4.0218639 -3.8488655 -3.6284964 -3.4636981 -3.3160787 -3.271945 -3.3572347 -3.4658651 -3.4984994 -3.4003525 -3.2870598 -3.149843 -3.017314 -2.9243133][-3.6496031 -3.4365118 -3.2859015 -3.1808038 -3.1912906 -3.2217836 -3.308311 -3.4441521 -3.5008943 -3.4137661 -3.1882935 -3.0050306 -2.8556006 -2.7566638 -2.7362201][-3.2585201 -2.9408779 -2.7785859 -2.7486162 -2.8884237 -3.0841274 -3.2657666 -3.3907094 -3.3194413 -3.0788052 -2.7465789 -2.5290279 -2.4173262 -2.4007444 -2.5032291][-3.2003508 -2.862736 -2.6878304 -2.6541762 -2.7465916 -2.8622406 -2.9404616 -2.9620149 -2.8121839 -2.5458348 -2.2347307 -2.073698 -2.0356612 -2.1036084 -2.3106515][-2.9372647 -2.6297262 -2.4556804 -2.4167683 -2.470696 -2.4998889 -2.4540348 -2.3622468 -2.1904368 -1.9762151 -1.7125974 -1.5739934 -1.581352 -1.7259638 -2.0321727][-2.6734381 -2.3825972 -2.1479425 -2.0427287 -2.040971 -2.0362139 -1.9182694 -1.7423544 -1.5480492 -1.3647938 -1.1221769 -0.9846437 -1.0363159 -1.2616138 -1.6841207][-2.6611238 -2.2992029 -1.8755457 -1.5513332 -1.3211186 -1.1494153 -0.8999908 -0.64809585 -0.49886441 -0.45402455 -0.38302994 -0.3816762 -0.56939435 -0.92625546 -1.464334][-2.4306962 -2.0041306 -1.4682662 -0.99365354 -0.570488 -0.20999193 0.16763687 0.44723749 0.48060513 0.32589436 0.17051601 -0.019948959 -0.37041092 -0.85145569 -1.4496868][-2.231137 -1.7964704 -1.2845879 -0.8593545 -0.49117064 -0.16943741 0.15680408 0.36733866 0.31812191 0.085538387 -0.12281752 -0.32362127 -0.65218782 -1.0887282 -1.614789][-2.3535082 -1.9922881 -1.6018152 -1.3109446 -1.0557296 -0.83211017 -0.62505221 -0.53554678 -0.65035081 -0.87831831 -1.0230384 -1.1130385 -1.295722 -1.5709264 -1.9376798][-2.4114127 -2.1471312 -1.905216 -1.7529104 -1.6094642 -1.4876666 -1.4100912 -1.4377282 -1.5976317 -1.7940912 -1.8665607 -1.8587968 -1.9054759 -2.0318034 -2.2502334][-2.37592 -2.2018573 -2.0815222 -2.046617 -2.0279598 -2.0192766 -2.0433135 -2.1203015 -2.2449348 -2.3518503 -2.350703 -2.2928431 -2.2709496 -2.3144598 -2.43461][-2.6563277 -2.5806165 -2.5538669 -2.5804009 -2.607229 -2.6126459 -2.6230092 -2.6603251 -2.7126203 -2.7419116 -2.6879892 -2.5963535 -2.5266585 -2.5093818 -2.5578861][-3.015614 -3.0142062 -3.0303349 -3.0686584 -3.0906677 -3.0777588 -3.0537574 -3.0501974 -3.0556011 -3.0344808 -2.9472871 -2.8377037 -2.7510684 -2.7046037 -2.7017281][-3.0789425 -3.091897 -3.1023476 -3.1247087 -3.1392932 -3.1315446 -3.1124721 -3.0998473 -3.0907145 -3.0616724 -2.9945126 -2.9190073 -2.8633871 -2.8312621 -2.8146544]]...]
INFO - root - 2017-12-07 09:06:20.478767: step 25310, loss = 0.84, batch loss = 0.77 (6.6 examples/sec; 1.209 sec/batch; 103h:10m:46s remains)
INFO - root - 2017-12-07 09:06:32.192552: step 25320, loss = 0.75, batch loss = 0.68 (6.9 examples/sec; 1.162 sec/batch; 99h:11m:01s remains)
INFO - root - 2017-12-07 09:06:43.808568: step 25330, loss = 0.69, batch loss = 0.61 (7.0 examples/sec; 1.145 sec/batch; 97h:42m:31s remains)
INFO - root - 2017-12-07 09:06:55.443492: step 25340, loss = 0.92, batch loss = 0.85 (6.8 examples/sec; 1.179 sec/batch; 100h:34m:19s remains)
INFO - root - 2017-12-07 09:07:07.093000: step 25350, loss = 0.63, batch loss = 0.55 (6.5 examples/sec; 1.237 sec/batch; 105h:34m:03s remains)
INFO - root - 2017-12-07 09:07:18.655337: step 25360, loss = 0.65, batch loss = 0.58 (6.8 examples/sec; 1.181 sec/batch; 100h:47m:34s remains)
INFO - root - 2017-12-07 09:07:30.159530: step 25370, loss = 0.84, batch loss = 0.77 (7.4 examples/sec; 1.076 sec/batch; 91h:48m:11s remains)
INFO - root - 2017-12-07 09:07:41.784709: step 25380, loss = 0.74, batch loss = 0.67 (7.4 examples/sec; 1.079 sec/batch; 92h:02m:34s remains)
INFO - root - 2017-12-07 09:07:53.472923: step 25390, loss = 0.94, batch loss = 0.87 (6.7 examples/sec; 1.192 sec/batch; 101h:41m:34s remains)
INFO - root - 2017-12-07 09:08:05.315932: step 25400, loss = 0.76, batch loss = 0.68 (6.5 examples/sec; 1.237 sec/batch; 105h:31m:51s remains)
2017-12-07 09:08:06.260492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6558323 -1.358705 -0.9315753 -0.61062932 -0.24004412 0.041218758 0.015088558 -0.4677918 -1.1030128 -1.2906771 -0.73018479 -0.30889559 -0.66270185 -1.6117046 -2.29354][-1.2189763 -0.582263 -0.0068926811 0.35191488 0.50777817 0.45362139 0.11309862 -0.50492644 -1.1340754 -1.188592 -0.32548523 0.49716902 0.46812773 -0.40630865 -1.3706625][-1.0174112 -0.04890728 0.77903652 1.3437061 1.4189472 1.1090655 0.53872061 -0.17778826 -0.84985471 -0.90093565 -0.051758766 0.84747839 1.1054521 0.46170712 -0.71702504][-0.70572853 0.30729103 0.95313311 1.2672977 1.1118689 0.826869 0.4753027 0.058304787 -0.32753277 -0.12749624 0.78882408 1.5987382 1.8369989 1.0970497 -0.37359][0.2481761 0.94352818 0.9405303 0.53387737 0.070244312 0.21551657 0.56594229 0.74560165 0.76644611 1.1924939 2.1465201 2.7779503 2.8638549 1.8587108 0.17442036][1.5970225 2.1064405 1.7352285 0.91491604 0.39140892 0.94055367 1.7740841 2.4233189 2.7956748 3.2541823 3.9931278 4.296442 3.9817266 2.6630416 1.0770612][2.2877016 3.0166101 2.9546242 2.3011475 1.8884163 2.3715277 3.1157765 3.8528366 4.2917442 4.5936012 5.0238571 5.1587324 4.7452345 3.4169121 2.1510119][1.8688116 2.6784296 2.918119 2.4515896 2.0197821 2.0905375 2.4324737 3.0076795 3.4426756 3.7092857 3.9872274 4.1848927 4.0747108 3.1547537 2.3658924][0.54085684 1.049036 1.1248727 0.57871962 0.1227479 0.072675705 0.32039928 0.77224493 1.1424637 1.4442935 1.7184138 1.9638329 2.0120039 1.3926945 0.92349672][-0.8908484 -0.62485623 -0.78172326 -1.4946916 -1.9517186 -1.9022484 -1.61499 -1.2386599 -0.96117759 -0.691087 -0.37319279 -0.12650156 -0.17968845 -0.84020686 -1.2265069][-1.9117646 -1.6928225 -1.8596215 -2.501327 -2.8966372 -2.8715472 -2.7187047 -2.4856853 -2.3098719 -2.0916512 -1.7394016 -1.4159498 -1.5146878 -2.1459327 -2.3685524][-2.1180367 -1.9142668 -1.9715464 -2.362988 -2.6368198 -2.6950681 -2.7396646 -2.7099314 -2.629662 -2.4228022 -2.071867 -1.7085607 -1.8103299 -2.3424656 -2.3581669][-2.0332313 -1.885299 -1.8799663 -2.0506182 -2.1886315 -2.2789009 -2.4420242 -2.565449 -2.4529681 -2.0814755 -1.5872564 -1.1746051 -1.3593962 -1.9171877 -1.9211681][-2.167305 -2.069375 -2.1118188 -2.2399163 -2.3523159 -2.4743142 -2.7405734 -2.9784234 -2.7870891 -2.1946442 -1.4865947 -0.96550465 -1.1907208 -1.8186324 -1.905946][-2.7119691 -2.5939813 -2.6807919 -2.845192 -3.0163538 -3.2240033 -3.5498462 -3.7461681 -3.3978624 -2.6432657 -1.87693 -1.3912497 -1.6488719 -2.2583764 -2.337852]]...]
INFO - root - 2017-12-07 09:08:17.868792: step 25410, loss = 0.74, batch loss = 0.67 (7.2 examples/sec; 1.108 sec/batch; 94h:29m:23s remains)
INFO - root - 2017-12-07 09:08:29.720933: step 25420, loss = 0.63, batch loss = 0.55 (6.7 examples/sec; 1.198 sec/batch; 102h:13m:23s remains)
INFO - root - 2017-12-07 09:08:41.226194: step 25430, loss = 0.54, batch loss = 0.47 (7.1 examples/sec; 1.130 sec/batch; 96h:25m:13s remains)
INFO - root - 2017-12-07 09:08:52.455609: step 25440, loss = 0.83, batch loss = 0.76 (7.1 examples/sec; 1.130 sec/batch; 96h:25m:21s remains)
INFO - root - 2017-12-07 09:09:04.232273: step 25450, loss = 0.62, batch loss = 0.54 (6.5 examples/sec; 1.228 sec/batch; 104h:45m:25s remains)
INFO - root - 2017-12-07 09:09:15.895094: step 25460, loss = 0.72, batch loss = 0.65 (6.9 examples/sec; 1.152 sec/batch; 98h:13m:53s remains)
INFO - root - 2017-12-07 09:09:27.305481: step 25470, loss = 0.88, batch loss = 0.81 (7.1 examples/sec; 1.128 sec/batch; 96h:12m:58s remains)
INFO - root - 2017-12-07 09:09:38.912693: step 25480, loss = 0.80, batch loss = 0.73 (7.1 examples/sec; 1.134 sec/batch; 96h:42m:04s remains)
INFO - root - 2017-12-07 09:09:50.703304: step 25490, loss = 0.70, batch loss = 0.63 (7.1 examples/sec; 1.121 sec/batch; 95h:34m:43s remains)
INFO - root - 2017-12-07 09:10:02.171898: step 25500, loss = 0.73, batch loss = 0.66 (6.6 examples/sec; 1.203 sec/batch; 102h:36m:10s remains)
2017-12-07 09:10:03.011890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.4184263 -3.6877527 -4.0299258 -4.1681442 -4.2479205 -4.4695926 -4.6166372 -4.58158 -4.5278039 -4.4842963 -4.4382892 -4.3316226 -4.2403803 -4.2285075 -4.1884522][-2.3452802 -2.5034921 -2.7350707 -2.8300662 -3.0436749 -3.5620053 -3.9870362 -4.1380839 -4.2486453 -4.3854518 -4.488471 -4.4833326 -4.4213343 -4.3591337 -4.2433248][-1.617033 -1.4277341 -1.3518703 -1.3383129 -1.6445236 -2.2851684 -2.7522156 -2.894155 -3.1754174 -3.5629137 -3.8375871 -3.9569888 -3.9946592 -4.0009689 -3.9642279][-1.7749424 -1.3502398 -1.1206725 -1.0635877 -1.2306452 -1.5983899 -1.7582355 -1.7438982 -2.2153435 -2.931628 -3.3675091 -3.5822287 -3.6744509 -3.7214575 -3.7910323][-2.1463871 -1.6493514 -1.4470413 -1.4042768 -1.3157496 -1.2514918 -0.97140169 -0.73681235 -1.4126902 -2.5246542 -3.0954845 -3.318737 -3.328516 -3.2943163 -3.457819][-2.5341706 -2.0626917 -1.8806341 -1.740201 -1.2838359 -0.65004945 0.33196259 1.1385732 0.49326563 -1.0248656 -1.8372157 -2.2058244 -2.295655 -2.3130062 -2.6677942][-2.815798 -2.51186 -2.4218719 -2.2249591 -1.5429983 -0.63032341 0.82638979 2.2443371 1.7672176 0.0364213 -0.88968229 -1.2880075 -1.4410028 -1.4455378 -1.7611902][-3.1044025 -2.9357457 -2.9083111 -2.8112478 -2.3456566 -1.8118048 -0.676425 0.70566511 0.44276142 -0.87465453 -1.3392847 -1.2714405 -1.1670344 -0.88871574 -0.89738083][-3.5725927 -3.4848137 -3.4976959 -3.4614584 -3.1388421 -2.8570857 -2.1102681 -0.96823859 -0.98893046 -1.6940427 -1.5986383 -1.1520963 -0.94825697 -0.5153873 -0.3005929][-3.7952552 -3.7440195 -3.7623022 -3.6885979 -3.3453579 -3.1039195 -2.609556 -1.7697463 -1.7381253 -2.1229587 -1.7830958 -1.2463992 -1.0583506 -0.47999787 -0.0095491409][-3.7658756 -3.7627158 -3.7757764 -3.6679666 -3.4010634 -3.2680473 -2.9331958 -2.3112254 -2.3421273 -2.6823931 -2.3848531 -1.883847 -1.685276 -1.0366666 -0.3889308][-3.8268385 -3.8599265 -3.8499541 -3.7867649 -3.7597263 -3.7221329 -3.2548056 -2.5545719 -2.5822697 -2.8966923 -2.6613266 -2.3802435 -2.3762598 -1.8985593 -1.2344923][-3.8459568 -3.8037715 -3.6308281 -3.5174322 -3.7367353 -3.8103158 -3.200892 -2.4572406 -2.5483255 -2.7863808 -2.4503284 -2.3135617 -2.4867458 -2.2147477 -1.5873675][-3.75752 -3.6191945 -3.3134718 -3.1638675 -3.5097015 -3.6397209 -3.0345082 -2.4346805 -2.6189454 -2.7456865 -2.3063941 -2.2775862 -2.5360527 -2.3634326 -1.8384275][-3.6410813 -3.4714894 -3.1751266 -3.0915546 -3.5284953 -3.701489 -3.1112857 -2.5174389 -2.50559 -2.4008274 -1.9371178 -1.9677737 -2.1751223 -2.0309386 -1.8122034]]...]
INFO - root - 2017-12-07 09:10:14.735863: step 25510, loss = 0.69, batch loss = 0.62 (7.1 examples/sec; 1.129 sec/batch; 96h:18m:44s remains)
INFO - root - 2017-12-07 09:10:26.421770: step 25520, loss = 0.91, batch loss = 0.83 (7.1 examples/sec; 1.134 sec/batch; 96h:39m:59s remains)
INFO - root - 2017-12-07 09:10:38.079413: step 25530, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.199 sec/batch; 102h:16m:39s remains)
INFO - root - 2017-12-07 09:10:49.709542: step 25540, loss = 0.83, batch loss = 0.75 (6.8 examples/sec; 1.171 sec/batch; 99h:50m:29s remains)
INFO - root - 2017-12-07 09:11:01.260069: step 25550, loss = 0.83, batch loss = 0.76 (7.4 examples/sec; 1.077 sec/batch; 91h:50m:42s remains)
INFO - root - 2017-12-07 09:11:12.717309: step 25560, loss = 0.83, batch loss = 0.75 (6.8 examples/sec; 1.177 sec/batch; 100h:19m:40s remains)
INFO - root - 2017-12-07 09:11:24.464298: step 25570, loss = 0.67, batch loss = 0.60 (6.9 examples/sec; 1.156 sec/batch; 98h:33m:59s remains)
INFO - root - 2017-12-07 09:11:36.167721: step 25580, loss = 0.76, batch loss = 0.69 (6.7 examples/sec; 1.203 sec/batch; 102h:31m:19s remains)
INFO - root - 2017-12-07 09:11:47.864258: step 25590, loss = 0.91, batch loss = 0.84 (7.0 examples/sec; 1.137 sec/batch; 96h:55m:57s remains)
INFO - root - 2017-12-07 09:11:59.402816: step 25600, loss = 0.73, batch loss = 0.66 (7.1 examples/sec; 1.121 sec/batch; 95h:31m:38s remains)
2017-12-07 09:12:00.198741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1078324 -2.5738463 -2.8167267 -2.6994879 -2.5234954 -2.648335 -2.4728029 -2.2796042 -2.7426505 -2.8483582 -2.7442439 -2.924562 -2.8449166 -2.8831611 -3.1731596][-1.8046091 -2.1612847 -2.4550929 -2.4541631 -2.362375 -2.4441772 -2.2344763 -2.0659685 -2.3361356 -2.248426 -2.0163507 -2.1666267 -2.2200847 -2.451123 -2.9686594][-1.52878 -1.897464 -2.2575817 -2.3014386 -2.2256029 -2.2051284 -1.9314876 -1.832119 -1.970104 -1.7817385 -1.4962883 -1.6004214 -1.7983625 -2.1650372 -2.8654096][-1.135505 -1.5576084 -1.9099016 -1.8672922 -1.7514827 -1.6235886 -1.3416176 -1.3482959 -1.5039079 -1.3994703 -1.2377608 -1.4834442 -1.8754656 -2.2693021 -2.9231877][-0.95764065 -1.4435532 -1.779741 -1.6356492 -1.4109924 -1.1094692 -0.66666174 -0.652997 -1.0109963 -1.2244246 -1.2600167 -1.6149495 -2.0869083 -2.4104369 -2.9035983][-1.0548723 -1.5989156 -1.9511051 -1.7637317 -1.4086006 -0.91018081 -0.21694803 -0.092820644 -0.75536084 -1.3461154 -1.4534168 -1.6943567 -2.102282 -2.3726268 -2.7680454][-1.0485613 -1.4827735 -1.8179116 -1.7039068 -1.2673163 -0.58997345 0.22460842 0.35117579 -0.69151783 -1.6393514 -1.7886221 -1.8957293 -2.2281818 -2.4802623 -2.7922406][-1.050318 -1.1449881 -1.3856893 -1.3621283 -0.90336895 -0.14568472 0.61555243 0.62193918 -0.73525167 -1.9380851 -2.1817279 -2.2438259 -2.4312129 -2.5227404 -2.6549807][-1.3473046 -1.1025298 -1.211715 -1.2510891 -0.80966616 -0.11099052 0.55473757 0.5445118 -0.79371858 -1.9135973 -2.1134346 -2.1315784 -2.2067771 -2.2615163 -2.3699961][-1.514431 -1.0971892 -1.1373303 -1.219171 -0.88221359 -0.3732295 0.11819029 0.10249853 -1.0296121 -1.8425896 -1.8793044 -1.7887321 -1.802207 -1.9982245 -2.2834618][-1.441937 -1.010366 -1.0371168 -1.15482 -0.95572376 -0.65936112 -0.33493805 -0.35206079 -1.2871554 -1.8538516 -1.8061457 -1.6799183 -1.674655 -1.9918525 -2.3985312][-1.6322398 -1.2455482 -1.1914973 -1.269623 -1.1701839 -1.0603468 -0.83742905 -0.77234888 -1.4658499 -1.8525543 -1.8209937 -1.7571721 -1.7657824 -2.0988278 -2.4837055][-1.854367 -1.5390413 -1.4214206 -1.4670002 -1.4469392 -1.4466856 -1.3089201 -1.2063136 -1.7244394 -1.9787531 -1.8968987 -1.8254714 -1.8744881 -2.2247417 -2.5915082][-1.9381278 -1.6765625 -1.5385013 -1.5956328 -1.6793864 -1.7363081 -1.6850867 -1.6335347 -2.0656862 -2.2541723 -2.1094694 -2.0140753 -2.0626609 -2.3669498 -2.7270942][-1.8509378 -1.6437395 -1.5316668 -1.6076467 -1.7570505 -1.8703699 -1.903461 -1.8902054 -2.1526763 -2.2481341 -2.1213498 -2.0936306 -2.1823108 -2.3962829 -2.6826303]]...]
INFO - root - 2017-12-07 09:12:11.976103: step 25610, loss = 0.93, batch loss = 0.85 (6.7 examples/sec; 1.201 sec/batch; 102h:23m:36s remains)
INFO - root - 2017-12-07 09:12:23.530996: step 25620, loss = 0.61, batch loss = 0.54 (7.1 examples/sec; 1.133 sec/batch; 96h:33m:39s remains)
INFO - root - 2017-12-07 09:12:35.159048: step 25630, loss = 0.93, batch loss = 0.85 (7.4 examples/sec; 1.080 sec/batch; 92h:01m:21s remains)
INFO - root - 2017-12-07 09:12:46.696246: step 25640, loss = 0.74, batch loss = 0.67 (7.1 examples/sec; 1.132 sec/batch; 96h:28m:13s remains)
INFO - root - 2017-12-07 09:12:58.220680: step 25650, loss = 0.69, batch loss = 0.62 (7.2 examples/sec; 1.114 sec/batch; 94h:57m:07s remains)
INFO - root - 2017-12-07 09:13:09.890493: step 25660, loss = 0.92, batch loss = 0.84 (6.8 examples/sec; 1.174 sec/batch; 100h:06m:04s remains)
INFO - root - 2017-12-07 09:13:21.745635: step 25670, loss = 1.07, batch loss = 1.00 (6.7 examples/sec; 1.196 sec/batch; 101h:55m:47s remains)
INFO - root - 2017-12-07 09:13:33.587796: step 25680, loss = 0.72, batch loss = 0.65 (7.6 examples/sec; 1.053 sec/batch; 89h:44m:24s remains)
INFO - root - 2017-12-07 09:13:47.740262: step 25690, loss = 0.68, batch loss = 0.60 (5.6 examples/sec; 1.428 sec/batch; 121h:41m:28s remains)
INFO - root - 2017-12-07 09:14:02.948863: step 25700, loss = 0.67, batch loss = 0.60 (5.1 examples/sec; 1.561 sec/batch; 133h:03m:24s remains)
2017-12-07 09:14:04.049441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8730602 -3.038692 -2.1909549 -1.9705591 -2.2972181 -2.45817 -2.5201392 -2.6683388 -2.6838975 -2.5634711 -2.3585589 -2.1868718 -2.2026281 -2.2759628 -2.4370933][-3.8581026 -3.0335836 -2.19028 -1.9365003 -2.2323618 -2.4165947 -2.567569 -2.8072422 -2.7927847 -2.537497 -2.2088449 -2.0798054 -2.2715995 -2.5078695 -2.7446568][-3.8505549 -3.1084437 -2.3070443 -2.0020466 -2.2452633 -2.4402063 -2.7321041 -3.1678843 -3.2315083 -2.9606359 -2.697494 -2.7680309 -3.1434984 -3.3906927 -3.4939535][-3.9481516 -3.3610392 -2.6255455 -2.261302 -2.4106839 -2.5346522 -2.8945312 -3.5328116 -3.7297909 -3.5102413 -3.3724833 -3.6185267 -4.063067 -4.1857014 -4.0281596][-4.1304054 -3.7357194 -3.0766866 -2.6205637 -2.5223491 -2.3536255 -2.5864305 -3.342926 -3.6430972 -3.4781089 -3.4169664 -3.6882844 -4.0741463 -4.0968385 -3.8139827][-4.2270637 -3.9905696 -3.4212437 -2.8639009 -2.4288518 -1.8198133 -1.8104417 -2.6832185 -3.1929457 -3.2326734 -3.3603511 -3.6816564 -4.0241928 -4.0141873 -3.636014][-4.1468306 -3.9759669 -3.4228852 -2.7221537 -1.9215291 -0.84903979 -0.61266971 -1.6778622 -2.5262811 -2.8881445 -3.2895956 -3.7194655 -4.0593519 -4.030695 -3.4911349][-3.96592 -3.7721128 -3.1673622 -2.3093755 -1.1558702 0.34277153 0.79072285 -0.43445683 -1.5436566 -2.1276286 -2.7602882 -3.3061793 -3.6736023 -3.7043097 -3.1458325][-3.8313141 -3.595643 -3.0537038 -2.2984986 -1.1382272 0.40260315 0.88272 -0.30147457 -1.3264563 -1.8540061 -2.533906 -3.096559 -3.4103215 -3.4382172 -2.9341764][-3.779856 -3.5150075 -3.1234698 -2.6570873 -1.8148987 -0.60302043 -0.235322 -1.1639664 -1.8549304 -2.1590941 -2.72499 -3.1512632 -3.2924671 -3.2414906 -2.7640724][-3.778163 -3.4265184 -3.0732932 -2.7855508 -2.2639711 -1.3962977 -1.0933707 -1.7114053 -2.0992961 -2.2584908 -2.7468109 -3.0516093 -3.0896168 -3.0409567 -2.6612251][-3.8599606 -3.3742049 -2.9071681 -2.6595464 -2.403398 -1.8751888 -1.6551273 -2.0091572 -2.1541507 -2.2463787 -2.7377639 -3.0109482 -3.0350909 -2.9905109 -2.6599035][-4.0131454 -3.4277396 -2.7804275 -2.4492302 -2.3903251 -2.2039244 -2.1406243 -2.3038306 -2.1900885 -2.179774 -2.6894655 -3.0344923 -3.1732583 -3.1477413 -2.7852762][-4.1192865 -3.5195847 -2.7594204 -2.3276126 -2.3742929 -2.4506009 -2.543565 -2.6139288 -2.2871678 -2.1419842 -2.5936272 -2.9541268 -3.2092369 -3.2786112 -2.9968915][-4.0974412 -3.5105073 -2.7069507 -2.2168858 -2.3165681 -2.54398 -2.7687411 -2.8851798 -2.4994421 -2.2483795 -2.5530179 -2.76143 -2.950789 -3.0554008 -2.9168315]]...]
INFO - root - 2017-12-07 09:14:18.951722: step 25710, loss = 0.57, batch loss = 0.50 (5.7 examples/sec; 1.410 sec/batch; 120h:07m:05s remains)
INFO - root - 2017-12-07 09:14:34.005309: step 25720, loss = 0.93, batch loss = 0.86 (5.3 examples/sec; 1.522 sec/batch; 129h:40m:11s remains)
INFO - root - 2017-12-07 09:14:48.932762: step 25730, loss = 0.83, batch loss = 0.75 (5.3 examples/sec; 1.501 sec/batch; 127h:56m:20s remains)
INFO - root - 2017-12-07 09:15:03.853108: step 25740, loss = 0.87, batch loss = 0.79 (5.2 examples/sec; 1.531 sec/batch; 130h:29m:48s remains)
INFO - root - 2017-12-07 09:15:18.589820: step 25750, loss = 0.73, batch loss = 0.66 (5.4 examples/sec; 1.483 sec/batch; 126h:19m:36s remains)
INFO - root - 2017-12-07 09:15:33.581585: step 25760, loss = 0.85, batch loss = 0.78 (5.1 examples/sec; 1.565 sec/batch; 133h:21m:49s remains)
INFO - root - 2017-12-07 09:15:48.498119: step 25770, loss = 0.78, batch loss = 0.71 (5.2 examples/sec; 1.540 sec/batch; 131h:12m:27s remains)
INFO - root - 2017-12-07 09:16:03.431432: step 25780, loss = 0.86, batch loss = 0.78 (5.6 examples/sec; 1.435 sec/batch; 122h:17m:11s remains)
INFO - root - 2017-12-07 09:16:18.368342: step 25790, loss = 0.76, batch loss = 0.69 (5.4 examples/sec; 1.475 sec/batch; 125h:41m:00s remains)
INFO - root - 2017-12-07 09:16:33.270975: step 25800, loss = 0.83, batch loss = 0.75 (5.5 examples/sec; 1.451 sec/batch; 123h:35m:26s remains)
2017-12-07 09:16:34.341583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.27147 -2.7383761 -2.1132505 -1.8808155 -2.0576077 -2.6857829 -3.2887435 -3.526958 -3.4993615 -3.3780015 -3.2255003 -3.2318416 -3.254199 -3.0814478 -2.6845911][-3.3148198 -2.7051806 -2.0547879 -1.8759921 -2.1323059 -2.8453045 -3.4023087 -3.4895566 -3.330761 -3.1221473 -2.9550903 -2.9706597 -2.9471583 -2.6838779 -2.1298389][-3.233747 -2.5865731 -1.9668148 -1.8703783 -2.1992383 -2.9038353 -3.2655168 -3.0576053 -2.7149186 -2.4578016 -2.3584442 -2.4304233 -2.3680511 -2.0119641 -1.3427823][-2.7769787 -2.129895 -1.6359339 -1.7078319 -2.1399252 -2.7653852 -2.8350558 -2.2670705 -1.7572525 -1.5276594 -1.5428159 -1.6700139 -1.5494733 -1.1028905 -0.39776087][-1.8388343 -1.3166068 -1.1111197 -1.4479191 -1.9661999 -2.3913255 -2.087486 -1.183027 -0.60830879 -0.56243205 -0.81606865 -1.0958078 -1.0322897 -0.64438605 -0.038268566][-0.79483032 -0.56963491 -0.76462865 -1.3037703 -1.7427199 -1.7697015 -1.0050988 0.20631218 0.77219439 0.53986883 -0.018906116 -0.51319885 -0.67939758 -0.58667946 -0.2808795][-0.13375378 -0.26579523 -0.77217913 -1.3339412 -1.5584705 -1.053571 0.28271484 1.7763 2.226048 1.6416817 0.78533792 0.049412727 -0.46782589 -0.79224968 -0.81559062][0.033935547 -0.37582922 -0.98673511 -1.4347136 -1.4602373 -0.50774288 1.2562985 2.8350773 2.954587 2.0194688 1.0238466 0.14972973 -0.6599071 -1.2684743 -1.4679592][-0.17265987 -0.76933932 -1.3283193 -1.611619 -1.4919541 -0.37518787 1.2958789 2.4362354 2.1960506 1.2354641 0.44504309 -0.30625534 -1.1766448 -1.8588092 -2.077266][-0.78034568 -1.3712692 -1.7843299 -1.9182782 -1.7529578 -0.73361754 0.51999331 1.0982943 0.74789667 0.077361107 -0.35988951 -0.8917942 -1.6515741 -2.241514 -2.4087365][-1.5546846 -2.00824 -2.2226126 -2.2249017 -2.0484126 -1.2296262 -0.3990221 -0.20780897 -0.50903463 -0.88622212 -1.0843987 -1.4222872 -1.9443882 -2.3697329 -2.5142019][-2.2237935 -2.5238905 -2.5332203 -2.3593063 -2.1371458 -1.538285 -1.0445442 -1.0661399 -1.3135478 -1.6069636 -1.7892332 -2.0053487 -2.2905357 -2.5600462 -2.6731775][-2.9046893 -3.1100867 -2.9920928 -2.6652856 -2.3795295 -1.9268131 -1.5975876 -1.6672523 -1.8854923 -2.2007277 -2.4266558 -2.5660343 -2.7263646 -2.9406397 -3.0533662][-3.4921098 -3.7044282 -3.6013637 -3.2589908 -2.9627671 -2.5933409 -2.330786 -2.3781948 -2.5475907 -2.7924011 -2.9632156 -3.0411916 -3.1612639 -3.368017 -3.4920638][-3.7451503 -3.9442933 -3.92105 -3.7047195 -3.5015574 -3.2250049 -3.0090761 -3.0513473 -3.1676614 -3.3150244 -3.4508815 -3.5243289 -3.6125367 -3.768774 -3.84825]]...]
INFO - root - 2017-12-07 09:16:49.196474: step 25810, loss = 0.98, batch loss = 0.91 (5.1 examples/sec; 1.571 sec/batch; 133h:49m:13s remains)
INFO - root - 2017-12-07 09:17:04.145835: step 25820, loss = 0.87, batch loss = 0.79 (5.3 examples/sec; 1.508 sec/batch; 128h:26m:51s remains)
INFO - root - 2017-12-07 09:17:19.082830: step 25830, loss = 0.79, batch loss = 0.71 (5.4 examples/sec; 1.485 sec/batch; 126h:32m:36s remains)
INFO - root - 2017-12-07 09:17:34.009795: step 25840, loss = 0.72, batch loss = 0.65 (5.4 examples/sec; 1.482 sec/batch; 126h:12m:57s remains)
INFO - root - 2017-12-07 09:17:48.957436: step 25850, loss = 0.57, batch loss = 0.50 (5.0 examples/sec; 1.594 sec/batch; 135h:47m:48s remains)
INFO - root - 2017-12-07 09:18:03.808601: step 25860, loss = 0.98, batch loss = 0.91 (5.3 examples/sec; 1.501 sec/batch; 127h:48m:47s remains)
INFO - root - 2017-12-07 09:18:18.559396: step 25870, loss = 0.83, batch loss = 0.75 (5.6 examples/sec; 1.440 sec/batch; 122h:40m:47s remains)
INFO - root - 2017-12-07 09:18:33.575735: step 25880, loss = 0.64, batch loss = 0.57 (5.5 examples/sec; 1.453 sec/batch; 123h:46m:08s remains)
INFO - root - 2017-12-07 09:18:48.349426: step 25890, loss = 0.83, batch loss = 0.76 (5.6 examples/sec; 1.431 sec/batch; 121h:53m:50s remains)
INFO - root - 2017-12-07 09:19:03.296688: step 25900, loss = 0.76, batch loss = 0.69 (5.3 examples/sec; 1.511 sec/batch; 128h:40m:59s remains)
2017-12-07 09:19:04.360459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.5177121 -3.683835 -3.9028523 -4.08226 -4.1009874 -3.9951506 -3.8981245 -3.8380356 -3.7808475 -3.7311962 -3.7241642 -3.8059802 -3.9142649 -3.9073844 -3.7963452][-3.3824749 -3.4847517 -3.6501994 -3.7573423 -3.658047 -3.43484 -3.3032784 -3.2930324 -3.334105 -3.4235582 -3.5664687 -3.7893977 -4.0042176 -3.9990723 -3.8132][-3.1649199 -3.1056061 -3.0953946 -3.0224767 -2.77653 -2.4896164 -2.4221361 -2.5557294 -2.746109 -2.9952507 -3.2620523 -3.562243 -3.8235447 -3.8096805 -3.580215][-3.0494628 -2.8679657 -2.7018051 -2.4435923 -2.0873082 -1.8189802 -1.8669009 -2.1119702 -2.3644807 -2.636632 -2.8836763 -3.1504898 -3.4024949 -3.4005413 -3.1913705][-3.1536169 -3.0104361 -2.8369155 -2.5059967 -2.1032786 -1.8748055 -1.9592147 -2.15422 -2.2763734 -2.3841498 -2.4765794 -2.6420059 -2.8439648 -2.8255105 -2.6439104][-3.3683009 -3.3527777 -3.2493262 -2.9521298 -2.5834858 -2.3729806 -2.3896017 -2.426302 -2.3492131 -2.2186475 -2.0780818 -2.0641112 -2.1140335 -2.0437009 -1.9442868][-3.4179153 -3.4378798 -3.3154602 -3.0378351 -2.7266176 -2.4991884 -2.4016273 -2.3053129 -2.1280484 -1.9438083 -1.7618794 -1.7189558 -1.6753111 -1.5635073 -1.5470109][-3.0560093 -2.9545367 -2.7205524 -2.4433379 -2.2197597 -2.0475149 -1.9615946 -1.9303308 -1.8487117 -1.7284851 -1.5897658 -1.5591111 -1.4713402 -1.3674982 -1.4309635][-2.4583087 -2.1118464 -1.7388732 -1.4504409 -1.2639904 -1.1175277 -1.074384 -1.1573246 -1.2124529 -1.1619313 -1.0333903 -0.96368861 -0.90649676 -0.99054337 -1.2818267][-2.0266359 -1.4249 -0.95934963 -0.70285273 -0.59637427 -0.55482745 -0.59650135 -0.74122429 -0.81298923 -0.74934149 -0.62509584 -0.56446695 -0.60290813 -0.85176754 -1.2845736][-2.1221414 -1.4638958 -1.036196 -0.85190511 -0.81067014 -0.85296416 -0.94306755 -1.0549421 -1.0669513 -0.98965883 -0.89476728 -0.85899687 -0.92111993 -1.1791389 -1.585294][-2.73327 -2.2783959 -1.9623499 -1.7608674 -1.6525154 -1.6453571 -1.7135525 -1.7956378 -1.8123133 -1.7725422 -1.7032278 -1.6751795 -1.7111058 -1.8775065 -2.1776984][-3.3764658 -3.1923594 -3.0250835 -2.8583317 -2.748096 -2.7276845 -2.782454 -2.8440766 -2.8692031 -2.855448 -2.8004246 -2.7692585 -2.7685661 -2.8200893 -2.9572325][-3.6834738 -3.6756163 -3.634423 -3.573468 -3.5479832 -3.5693743 -3.6128225 -3.6325521 -3.6295977 -3.6014056 -3.5546331 -3.5172162 -3.4900372 -3.4654851 -3.469784][-3.711246 -3.7813375 -3.82201 -3.8433247 -3.8765497 -3.9235272 -3.9701767 -3.992909 -3.9916382 -3.9660401 -3.9231596 -3.8618495 -3.794245 -3.7222619 -3.6544051]]...]
INFO - root - 2017-12-07 09:19:19.305719: step 25910, loss = 0.77, batch loss = 0.70 (5.3 examples/sec; 1.496 sec/batch; 127h:22m:35s remains)
INFO - root - 2017-12-07 09:19:34.183265: step 25920, loss = 0.59, batch loss = 0.52 (5.2 examples/sec; 1.543 sec/batch; 131h:21m:50s remains)
INFO - root - 2017-12-07 09:19:49.106870: step 25930, loss = 0.65, batch loss = 0.58 (5.5 examples/sec; 1.465 sec/batch; 124h:44m:56s remains)
INFO - root - 2017-12-07 09:20:03.867330: step 25940, loss = 0.77, batch loss = 0.70 (5.2 examples/sec; 1.545 sec/batch; 131h:31m:25s remains)
INFO - root - 2017-12-07 09:20:18.741929: step 25950, loss = 0.86, batch loss = 0.79 (5.9 examples/sec; 1.361 sec/batch; 115h:55m:36s remains)
INFO - root - 2017-12-07 09:20:33.500741: step 25960, loss = 0.82, batch loss = 0.75 (5.5 examples/sec; 1.456 sec/batch; 123h:59m:02s remains)
INFO - root - 2017-12-07 09:20:48.391327: step 25970, loss = 0.81, batch loss = 0.74 (5.4 examples/sec; 1.495 sec/batch; 127h:17m:31s remains)
INFO - root - 2017-12-07 09:21:03.166042: step 25980, loss = 0.61, batch loss = 0.54 (5.7 examples/sec; 1.397 sec/batch; 118h:58m:01s remains)
INFO - root - 2017-12-07 09:21:17.982530: step 25990, loss = 0.67, batch loss = 0.59 (5.1 examples/sec; 1.555 sec/batch; 132h:24m:56s remains)
INFO - root - 2017-12-07 09:21:32.504550: step 26000, loss = 0.83, batch loss = 0.76 (5.4 examples/sec; 1.493 sec/batch; 127h:04m:51s remains)
2017-12-07 09:21:33.554507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1917224 -3.4857624 -3.5438008 -3.3773587 -3.1546369 -2.9588594 -2.9111476 -2.9511614 -2.9415498 -2.8954825 -2.9022751 -2.9110126 -2.9656632 -3.0328717 -2.9327774][-3.2837117 -3.5787807 -3.6608534 -3.5429645 -3.3718634 -3.147449 -3.0071864 -2.9239774 -2.7520006 -2.5876155 -2.5570338 -2.5316486 -2.5888584 -2.7164092 -2.6695595][-2.9791353 -3.2413418 -3.351917 -3.3247652 -3.2845709 -3.1790066 -3.1226807 -3.0548787 -2.8133342 -2.558485 -2.4837923 -2.4809933 -2.6155462 -2.8434806 -2.821075][-2.6687489 -2.8833454 -2.9955328 -3.0113544 -3.0184946 -3.0001655 -3.0892048 -3.135509 -2.9397411 -2.6577494 -2.5215 -2.5539863 -2.8173816 -3.1729672 -3.2223053][-2.645288 -2.824749 -2.88511 -2.8540554 -2.8260078 -2.8030243 -2.9573052 -3.0701842 -2.9565291 -2.7382333 -2.5693874 -2.6062903 -2.9053555 -3.2733181 -3.3577251][-2.9518015 -3.1398673 -3.0754714 -2.8695827 -2.7201953 -2.5940027 -2.6681395 -2.7575216 -2.7478442 -2.6867535 -2.5693054 -2.5841768 -2.8234472 -3.0839493 -3.1412082][-3.4316525 -3.6694741 -3.4318116 -3.0351193 -2.7650676 -2.4851532 -2.2874281 -2.1613865 -2.1839533 -2.332535 -2.3940108 -2.4364607 -2.6087723 -2.8122132 -2.9053662][-3.8067443 -3.9618344 -3.5136535 -3.009047 -2.76678 -2.4709296 -2.0606771 -1.6753263 -1.6057346 -1.8161459 -1.9964867 -2.0994134 -2.2969241 -2.6299372 -2.903749][-3.9840827 -3.9436014 -3.2758417 -2.7025752 -2.5224247 -2.3293226 -1.9739749 -1.600477 -1.5115855 -1.6515338 -1.7644434 -1.8578506 -2.1035891 -2.5985274 -3.0303509][-3.9997199 -3.7684813 -2.9849372 -2.4408684 -2.302892 -2.2391891 -2.1118627 -2.0540321 -2.1497517 -2.1796076 -2.016834 -1.9127321 -2.0999422 -2.6384864 -3.1008866][-3.8703883 -3.532676 -2.8188462 -2.4498415 -2.4190586 -2.4793284 -2.5148649 -2.6986508 -2.9438043 -2.8808365 -2.4915166 -2.2498224 -2.357904 -2.7713065 -3.0989628][-3.8000243 -3.4765184 -2.9778707 -2.8388667 -2.9048181 -3.0466628 -3.1780233 -3.4836359 -3.7433817 -3.5462098 -3.0014946 -2.7260194 -2.8147793 -3.0611968 -3.2039828][-4.049861 -3.813859 -3.5280175 -3.5194724 -3.5769997 -3.6975219 -3.8328722 -4.1719437 -4.4304752 -4.1735115 -3.5715127 -3.2533388 -3.2841988 -3.3747165 -3.4158642][-4.4976177 -4.3760543 -4.2328606 -4.259717 -4.2885895 -4.3468571 -4.3875847 -4.6097713 -4.7970848 -4.5845609 -4.0962563 -3.8142617 -3.7940998 -3.7790771 -3.7937512][-4.7227855 -4.6894188 -4.6085463 -4.6217461 -4.66748 -4.7295 -4.7139583 -4.8015113 -4.9194794 -4.814558 -4.5084414 -4.2982373 -4.2472 -4.1602039 -4.0940084]]...]
INFO - root - 2017-12-07 09:21:48.236624: step 26010, loss = 0.78, batch loss = 0.71 (5.6 examples/sec; 1.432 sec/batch; 121h:56m:06s remains)
INFO - root - 2017-12-07 09:22:02.729994: step 26020, loss = 0.75, batch loss = 0.68 (5.5 examples/sec; 1.461 sec/batch; 124h:23m:12s remains)
INFO - root - 2017-12-07 09:22:17.938649: step 26030, loss = 0.84, batch loss = 0.77 (5.0 examples/sec; 1.597 sec/batch; 135h:54m:46s remains)
INFO - root - 2017-12-07 09:22:32.975677: step 26040, loss = 0.85, batch loss = 0.78 (5.2 examples/sec; 1.545 sec/batch; 131h:28m:51s remains)
INFO - root - 2017-12-07 09:22:47.922178: step 26050, loss = 0.77, batch loss = 0.70 (5.5 examples/sec; 1.447 sec/batch; 123h:08m:55s remains)
INFO - root - 2017-12-07 09:23:02.865303: step 26060, loss = 0.75, batch loss = 0.68 (5.1 examples/sec; 1.561 sec/batch; 132h:52m:12s remains)
INFO - root - 2017-12-07 09:23:17.826226: step 26070, loss = 0.72, batch loss = 0.65 (5.2 examples/sec; 1.538 sec/batch; 130h:53m:49s remains)
INFO - root - 2017-12-07 09:23:32.691894: step 26080, loss = 0.63, batch loss = 0.56 (5.5 examples/sec; 1.461 sec/batch; 124h:19m:31s remains)
INFO - root - 2017-12-07 09:23:47.647213: step 26090, loss = 0.76, batch loss = 0.69 (5.4 examples/sec; 1.491 sec/batch; 126h:56m:32s remains)
INFO - root - 2017-12-07 09:24:02.558233: step 26100, loss = 0.93, batch loss = 0.86 (5.4 examples/sec; 1.483 sec/batch; 126h:12m:15s remains)
2017-12-07 09:24:03.630311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4571109 -2.3595998 -2.2766302 -2.1323979 -2.0388806 -2.0884821 -2.077014 -1.9213548 -1.8062651 -1.7215152 -1.5640464 -1.4156027 -1.242636 -0.94308424 -0.59891558][-1.808708 -2.0562015 -2.1561663 -2.1604445 -2.2358303 -2.4499965 -2.593924 -2.5602412 -2.4890928 -2.3876698 -2.2106097 -1.9991815 -1.8308995 -1.6450555 -1.3559911][-1.2145193 -1.5587122 -1.6914978 -1.8433981 -2.0833752 -2.3301945 -2.4602675 -2.5022912 -2.5324757 -2.5005362 -2.4280639 -2.3160365 -2.3015835 -2.3881562 -2.2947254][-1.3953674 -1.4326124 -1.3312612 -1.5223589 -1.8245895 -1.9297454 -1.9394948 -1.9983664 -2.0625172 -2.0922186 -2.2011433 -2.3123667 -2.4313681 -2.6682127 -2.7305527][-2.2261913 -1.9111607 -1.480098 -1.4450715 -1.4766216 -1.2660246 -1.2078986 -1.3668947 -1.447665 -1.509573 -1.8430676 -2.2379129 -2.457046 -2.6630783 -2.7647252][-3.3263731 -2.8833132 -2.2141111 -1.7511048 -1.2261984 -0.62376881 -0.5887599 -0.91362524 -0.92310405 -0.89395714 -1.3008778 -1.8172522 -2.041744 -2.1642811 -2.2676027][-3.4805479 -3.1873088 -2.6377625 -1.9450305 -1.0120292 -0.20881176 -0.22500706 -0.51504135 -0.35056114 -0.30986357 -0.79876232 -1.3107615 -1.4619155 -1.5810997 -1.7365539][-2.9868135 -2.9202247 -2.6413903 -1.9754975 -0.99634337 -0.28885221 -0.37836504 -0.604342 -0.475286 -0.62077332 -1.1982694 -1.6150949 -1.6156855 -1.6640251 -1.755717][-2.6459315 -2.8419433 -2.808392 -2.3384967 -1.6465642 -1.2802658 -1.4740937 -1.6404307 -1.561748 -1.754993 -2.2320373 -2.5366321 -2.5192196 -2.5404468 -2.5714226][-2.9111381 -3.2469726 -3.2646372 -2.8847203 -2.4291298 -2.2881691 -2.4600205 -2.4800558 -2.356158 -2.4204304 -2.6618481 -2.9358284 -3.0789831 -3.2177241 -3.3202534][-3.0676212 -3.4383821 -3.4969559 -3.2828908 -3.0877943 -3.1244552 -3.2491412 -3.1477079 -2.897876 -2.7188635 -2.6531255 -2.7518008 -2.8531737 -2.9429722 -3.0915358][-2.7987871 -3.1292267 -3.1955609 -3.1406491 -3.2293031 -3.4334135 -3.5370686 -3.3400848 -2.9301171 -2.5579791 -2.3201625 -2.2418792 -2.137696 -2.0419436 -2.1847045][-3.0109277 -3.1139922 -2.9065132 -2.674057 -2.7199197 -2.9283175 -3.0569897 -2.9107745 -2.5329959 -2.2046897 -2.0427332 -2.0131145 -1.9114494 -1.8018758 -1.9553337][-3.6411884 -3.5014772 -3.0313349 -2.584661 -2.4825501 -2.6360087 -2.8136683 -2.7992578 -2.6107411 -2.4449511 -2.3889439 -2.4257069 -2.4479866 -2.4947524 -2.7108142][-4.2390409 -4.0597882 -3.604939 -3.1643252 -3.0294294 -3.15618 -3.3306627 -3.3620124 -3.2810698 -3.2072744 -3.1887794 -3.220758 -3.2882919 -3.4176025 -3.6543145]]...]
INFO - root - 2017-12-07 09:24:18.567359: step 26110, loss = 0.65, batch loss = 0.57 (5.2 examples/sec; 1.542 sec/batch; 131h:14m:08s remains)
INFO - root - 2017-12-07 09:24:33.418388: step 26120, loss = 0.76, batch loss = 0.68 (5.5 examples/sec; 1.454 sec/batch; 123h:46m:58s remains)
INFO - root - 2017-12-07 09:24:48.242205: step 26130, loss = 0.58, batch loss = 0.51 (5.6 examples/sec; 1.433 sec/batch; 121h:55m:06s remains)
INFO - root - 2017-12-07 09:25:03.202201: step 26140, loss = 0.67, batch loss = 0.60 (5.5 examples/sec; 1.452 sec/batch; 123h:31m:43s remains)
INFO - root - 2017-12-07 09:25:18.089330: step 26150, loss = 0.67, batch loss = 0.60 (5.2 examples/sec; 1.543 sec/batch; 131h:19m:30s remains)
INFO - root - 2017-12-07 09:25:33.335335: step 26160, loss = 0.61, batch loss = 0.54 (5.4 examples/sec; 1.484 sec/batch; 126h:15m:27s remains)
INFO - root - 2017-12-07 09:25:48.287506: step 26170, loss = 0.86, batch loss = 0.79 (5.4 examples/sec; 1.488 sec/batch; 126h:38m:30s remains)
INFO - root - 2017-12-07 09:26:03.051425: step 26180, loss = 0.70, batch loss = 0.63 (5.4 examples/sec; 1.478 sec/batch; 125h:43m:11s remains)
INFO - root - 2017-12-07 09:26:18.115235: step 26190, loss = 0.66, batch loss = 0.59 (5.2 examples/sec; 1.551 sec/batch; 131h:59m:54s remains)
INFO - root - 2017-12-07 09:26:33.102529: step 26200, loss = 0.70, batch loss = 0.63 (5.2 examples/sec; 1.530 sec/batch; 130h:10m:06s remains)
2017-12-07 09:26:34.210595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7810588 -2.8244338 -2.8740473 -2.9903197 -3.1518867 -3.2730072 -3.3176026 -3.3511927 -3.3854949 -3.3675196 -3.3419118 -3.344785 -3.3886154 -3.4153228 -3.3542047][-2.8247056 -2.8264222 -2.8081419 -2.8895621 -3.073252 -3.2312515 -3.3078284 -3.4304848 -3.5878432 -3.6487255 -3.6928196 -3.6818819 -3.6579139 -3.6352606 -3.5660717][-2.7438805 -2.6569805 -2.5170026 -2.5085139 -2.6993835 -2.9035132 -3.0184665 -3.2181838 -3.4979613 -3.6683609 -3.8154063 -3.8578124 -3.8282509 -3.8022065 -3.7473707][-2.444108 -2.1823831 -1.8445637 -1.6774952 -1.8156252 -2.076241 -2.2684522 -2.5858152 -3.03606 -3.3473547 -3.5748549 -3.6744962 -3.7115626 -3.7652583 -3.8011191][-2.1168702 -1.6835291 -1.2024217 -0.8829422 -0.86121821 -1.0151525 -1.1458287 -1.4438848 -1.9323246 -2.3475819 -2.6699114 -2.9505963 -3.2530708 -3.5147526 -3.7072139][-1.8460965 -1.3637376 -0.92881536 -0.57080269 -0.34076357 -0.20655012 -0.10519409 -0.18613195 -0.53439093 -0.93401361 -1.3091373 -1.8458729 -2.5124931 -3.0655174 -3.4298623][-1.7791455 -1.2992887 -0.93734 -0.58076596 -0.16298151 0.26247168 0.64045906 0.8422184 0.65528011 0.24041891 -0.20040369 -0.937721 -1.8692975 -2.594873 -3.0327461][-1.9056497 -1.4220998 -1.0464616 -0.69119239 -0.27238894 0.20618725 0.74665451 1.2120905 1.2275553 0.81428671 0.29372168 -0.53921175 -1.4875705 -2.1465302 -2.555305][-2.1885648 -1.6757894 -1.1784689 -0.75750732 -0.39225483 -0.058538437 0.3403554 0.70850897 0.73748016 0.43835545 -0.0033445358 -0.70031238 -1.3804519 -1.7860591 -2.1033323][-2.5830712 -2.1251647 -1.5942633 -1.1544168 -0.8163147 -0.54439712 -0.31898832 -0.21299934 -0.34821987 -0.62317085 -0.9503274 -1.3686361 -1.6698422 -1.7802391 -1.9580066][-2.8947282 -2.6301138 -2.2813489 -2.0004807 -1.7621472 -1.5393059 -1.4239938 -1.4762251 -1.6912634 -1.9286044 -2.1385164 -2.2894154 -2.2544377 -2.1373823 -2.1810191][-3.1441424 -3.1110582 -3.0234628 -2.956588 -2.8229332 -2.6152213 -2.4971135 -2.5507946 -2.7273974 -2.8664598 -2.9260874 -2.869251 -2.6518209 -2.4815619 -2.5160599][-3.2897499 -3.4261637 -3.5420282 -3.6264763 -3.5692348 -3.3526716 -3.1432428 -3.0811014 -3.1635432 -3.2546687 -3.2932544 -3.2053895 -2.9632821 -2.78929 -2.8118048][-3.2620828 -3.4404695 -3.6095972 -3.7602117 -3.8231375 -3.7281151 -3.5728569 -3.4794586 -3.4717433 -3.4828284 -3.4798009 -3.3849859 -3.1539998 -2.9604197 -2.9222343][-3.1280055 -3.2856257 -3.4274442 -3.5467758 -3.6285706 -3.600476 -3.5008407 -3.4178765 -3.3737512 -3.3410084 -3.3213868 -3.2454333 -3.0633955 -2.8899405 -2.8272541]]...]
INFO - root - 2017-12-07 09:26:49.210511: step 26210, loss = 0.69, batch loss = 0.62 (5.4 examples/sec; 1.492 sec/batch; 126h:58m:55s remains)
INFO - root - 2017-12-07 09:27:03.919952: step 26220, loss = 0.68, batch loss = 0.61 (5.6 examples/sec; 1.434 sec/batch; 121h:58m:36s remains)
INFO - root - 2017-12-07 09:27:19.020524: step 26230, loss = 0.78, batch loss = 0.71 (5.0 examples/sec; 1.590 sec/batch; 135h:16m:44s remains)
INFO - root - 2017-12-07 09:27:34.003726: step 26240, loss = 0.80, batch loss = 0.73 (5.0 examples/sec; 1.609 sec/batch; 136h:50m:47s remains)
INFO - root - 2017-12-07 09:27:48.786723: step 26250, loss = 0.65, batch loss = 0.57 (5.4 examples/sec; 1.481 sec/batch; 126h:00m:54s remains)
INFO - root - 2017-12-07 09:28:03.823840: step 26260, loss = 0.90, batch loss = 0.83 (5.3 examples/sec; 1.521 sec/batch; 129h:23m:55s remains)
INFO - root - 2017-12-07 09:28:18.815049: step 26270, loss = 0.92, batch loss = 0.84 (5.5 examples/sec; 1.445 sec/batch; 122h:56m:30s remains)
INFO - root - 2017-12-07 09:28:33.657332: step 26280, loss = 0.73, batch loss = 0.66 (5.4 examples/sec; 1.469 sec/batch; 124h:57m:16s remains)
INFO - root - 2017-12-07 09:28:48.480937: step 26290, loss = 0.93, batch loss = 0.86 (5.5 examples/sec; 1.458 sec/batch; 124h:00m:45s remains)
INFO - root - 2017-12-07 09:29:03.352810: step 26300, loss = 0.78, batch loss = 0.71 (5.5 examples/sec; 1.464 sec/batch; 124h:32m:22s remains)
2017-12-07 09:29:04.445949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1716194 -2.9283094 -3.6415808 -3.9197302 -3.7899857 -3.1234624 -2.2248943 -1.7497835 -1.8566272 -2.1787 -2.6360807 -2.8384111 -2.5617776 -1.9359448 -1.2296691][-1.6338515 -2.4297309 -3.1856318 -3.5920496 -3.5458634 -2.8591166 -1.8719466 -1.3343863 -1.4396567 -1.8857212 -2.61382 -2.986258 -2.696362 -2.0783291 -1.392405][-1.6993151 -2.4018645 -3.0077529 -3.3570626 -3.2962847 -2.6296911 -1.6876101 -1.2122164 -1.3271604 -1.8138192 -2.7560034 -3.2727981 -2.9996271 -2.4540849 -1.8468547][-2.6681685 -3.1878357 -3.5157022 -3.5475278 -3.2404037 -2.451793 -1.4697063 -0.9556632 -0.95741105 -1.4778624 -2.6839209 -3.484942 -3.4557426 -3.1160097 -2.6483004][-3.8194337 -4.1952381 -4.1746993 -3.6608002 -2.7777629 -1.6279142 -0.5216701 0.086174488 0.10917282 -0.67259932 -2.2800279 -3.5471742 -4.0408797 -4.0109129 -3.6111579][-4.3530874 -4.7003665 -4.4244103 -3.4247475 -2.0018511 -0.52776051 0.75683308 1.5393071 1.5193605 0.37441826 -1.4528098 -2.9720416 -3.9386559 -4.2107091 -3.889889][-4.0764713 -4.5342317 -4.2658639 -3.1629047 -1.6600907 -0.20727873 1.157114 2.1769748 2.1730371 0.84781933 -0.75828552 -2.1193106 -3.2188978 -3.5880046 -3.3643179][-3.7834873 -4.1791978 -3.8176272 -2.7349491 -1.3927 -0.15850687 1.1512752 2.3194628 2.2076783 0.74033594 -0.56252909 -1.6779659 -2.7037129 -2.9576058 -2.8019338][-3.6355417 -3.7600918 -3.09132 -2.0236685 -0.97539973 -0.0018029213 1.2505684 2.4630613 2.1201539 0.532835 -0.55491996 -1.5321846 -2.3863704 -2.3746786 -2.2216249][-3.1642089 -3.0909719 -2.2625017 -1.4358752 -0.82156587 -0.076862335 1.1741114 2.3596907 1.9025908 0.44010067 -0.40101385 -1.2931814 -1.9543695 -1.6697738 -1.4871304][-2.3709633 -2.1943345 -1.401305 -0.90893531 -0.74628615 -0.26550388 0.80608559 1.7698383 1.3453679 0.21112013 -0.44291902 -1.3147492 -1.8391204 -1.4328547 -1.2653122][-1.7825167 -1.6600108 -1.1172581 -0.92649817 -0.94436884 -0.51753926 0.33234358 1.0635953 0.90396023 0.28022385 -0.18261147 -1.0686932 -1.6605511 -1.4547133 -1.4289606][-1.291342 -1.2830358 -1.1269929 -1.2099416 -1.1523292 -0.58911252 0.076088428 0.57218027 0.63211823 0.37246466 0.038302422 -0.80706716 -1.5582609 -1.688045 -1.7737901][-1.0692351 -1.215632 -1.4352214 -1.6974187 -1.4526961 -0.6403389 -0.014904499 0.28545141 0.23697376 -0.099741459 -0.42702198 -1.1304743 -1.9532926 -2.2928813 -2.3288][-1.6913998 -1.9254532 -2.2599397 -2.4529865 -2.0002174 -0.9812305 -0.2605648 -0.035271645 -0.39155436 -1.1052983 -1.4485235 -1.870033 -2.54217 -2.7943494 -2.6057172]]...]
INFO - root - 2017-12-07 09:29:19.356565: step 26310, loss = 0.89, batch loss = 0.82 (5.6 examples/sec; 1.439 sec/batch; 122h:24m:52s remains)
INFO - root - 2017-12-07 09:29:34.164161: step 26320, loss = 0.81, batch loss = 0.74 (5.2 examples/sec; 1.532 sec/batch; 130h:18m:16s remains)
INFO - root - 2017-12-07 09:29:49.044407: step 26330, loss = 0.78, batch loss = 0.70 (5.4 examples/sec; 1.487 sec/batch; 126h:27m:12s remains)
INFO - root - 2017-12-07 09:30:03.979701: step 26340, loss = 0.85, batch loss = 0.78 (5.6 examples/sec; 1.428 sec/batch; 121h:25m:58s remains)
INFO - root - 2017-12-07 09:30:18.916029: step 26350, loss = 0.59, batch loss = 0.52 (5.4 examples/sec; 1.486 sec/batch; 126h:20m:13s remains)
INFO - root - 2017-12-07 09:30:33.921555: step 26360, loss = 0.84, batch loss = 0.76 (5.1 examples/sec; 1.570 sec/batch; 133h:28m:15s remains)
INFO - root - 2017-12-07 09:30:48.680806: step 26370, loss = 0.73, batch loss = 0.66 (5.6 examples/sec; 1.434 sec/batch; 121h:55m:15s remains)
INFO - root - 2017-12-07 09:31:03.308034: step 26380, loss = 0.65, batch loss = 0.57 (5.6 examples/sec; 1.432 sec/batch; 121h:47m:17s remains)
INFO - root - 2017-12-07 09:31:18.278449: step 26390, loss = 0.74, batch loss = 0.67 (5.2 examples/sec; 1.532 sec/batch; 130h:13m:50s remains)
INFO - root - 2017-12-07 09:31:33.166937: step 26400, loss = 0.72, batch loss = 0.64 (5.1 examples/sec; 1.572 sec/batch; 133h:37m:57s remains)
2017-12-07 09:31:34.207812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0452352 -3.1956418 -3.3651888 -3.4439917 -3.4246686 -3.38992 -3.4540958 -3.583111 -3.626627 -3.5940368 -3.5363975 -3.4739995 -3.3802094 -3.3015337 -3.2755694][-3.0941796 -3.2335703 -3.3574209 -3.4167333 -3.4256845 -3.4268537 -3.4885039 -3.563313 -3.5541644 -3.5185223 -3.4671688 -3.3913217 -3.26056 -3.1215639 -3.0749578][-3.2749753 -3.2927649 -3.270895 -3.2300482 -3.1948225 -3.1868796 -3.241312 -3.2947369 -3.2965255 -3.3075101 -3.3128407 -3.2724862 -3.1538353 -2.9984632 -2.920476][-3.4890294 -3.384306 -3.2201114 -3.0580158 -2.9212251 -2.878056 -2.9533267 -3.0490453 -3.1075587 -3.1916432 -3.2677186 -3.2642655 -3.14005 -2.9500113 -2.8251808][-3.6230495 -3.4060116 -3.1240358 -2.8403716 -2.5830336 -2.4992557 -2.6267838 -2.8002949 -2.930768 -3.0744526 -3.19865 -3.2044187 -3.0444469 -2.8146122 -2.6844625][-3.6313996 -3.3742104 -3.0503097 -2.710099 -2.393266 -2.3131521 -2.5164833 -2.7677326 -2.9519591 -3.1123977 -3.2398038 -3.2264349 -3.025511 -2.7654762 -2.6427937][-3.5472226 -3.30379 -2.9987686 -2.6793895 -2.4050007 -2.3865194 -2.6384323 -2.9091554 -3.0938387 -3.2272315 -3.3351569 -3.3271208 -3.1456609 -2.9088438 -2.8076282][-3.4494188 -3.2626266 -3.0104973 -2.7398589 -2.5525384 -2.6032529 -2.8612928 -3.0971727 -3.2332196 -3.3220749 -3.4092793 -3.4415374 -3.3303807 -3.1573422 -3.0785551][-3.4480119 -3.3351564 -3.1482718 -2.9169133 -2.7984219 -2.8964229 -3.1129966 -3.2560289 -3.3121257 -3.3608432 -3.4482684 -3.5241337 -3.484715 -3.3765397 -3.3120503][-3.5862679 -3.5200973 -3.3921323 -3.1919346 -3.1176522 -3.227767 -3.3661189 -3.3885493 -3.3629475 -3.3846607 -3.4840159 -3.5850329 -3.5904655 -3.5409913 -3.5126843][-3.7085519 -3.6775219 -3.6020012 -3.4426689 -3.4033084 -3.5194588 -3.5954626 -3.533149 -3.4559388 -3.4526143 -3.5378637 -3.6144297 -3.6097484 -3.584753 -3.5824323][-3.731477 -3.741914 -3.7115479 -3.5821536 -3.5487204 -3.6421039 -3.6689641 -3.5738711 -3.4808707 -3.4635284 -3.5131433 -3.5532093 -3.5345857 -3.5117936 -3.5079536][-3.63601 -3.6987929 -3.7057447 -3.5949349 -3.5398831 -3.5693808 -3.546315 -3.4453878 -3.3583446 -3.333312 -3.3433857 -3.3558054 -3.356113 -3.3633208 -3.3701425][-3.5702858 -3.6625264 -3.6954124 -3.6042933 -3.5234284 -3.4753721 -3.4034371 -3.3148782 -3.2488813 -3.2099371 -3.1794982 -3.1427176 -3.1439242 -3.1769402 -3.196007][-3.6102252 -3.6865044 -3.7121878 -3.6365354 -3.5508902 -3.4615176 -3.3640041 -3.2981825 -3.257163 -3.2085919 -3.1522508 -3.0623612 -3.0446172 -3.0857522 -3.1034203]]...]
INFO - root - 2017-12-07 09:31:49.059112: step 26410, loss = 0.76, batch loss = 0.69 (5.4 examples/sec; 1.472 sec/batch; 125h:10m:49s remains)
INFO - root - 2017-12-07 09:32:03.951039: step 26420, loss = 0.73, batch loss = 0.66 (5.4 examples/sec; 1.495 sec/batch; 127h:07m:36s remains)
INFO - root - 2017-12-07 09:32:18.963697: step 26430, loss = 1.04, batch loss = 0.96 (5.1 examples/sec; 1.561 sec/batch; 132h:45m:24s remains)
INFO - root - 2017-12-07 09:32:33.857306: step 26440, loss = 0.69, batch loss = 0.62 (5.0 examples/sec; 1.589 sec/batch; 135h:07m:20s remains)
INFO - root - 2017-12-07 09:32:48.618864: step 26450, loss = 0.62, batch loss = 0.55 (5.7 examples/sec; 1.402 sec/batch; 119h:12m:41s remains)
INFO - root - 2017-12-07 09:33:03.539278: step 26460, loss = 0.75, batch loss = 0.68 (5.6 examples/sec; 1.439 sec/batch; 122h:18m:33s remains)
INFO - root - 2017-12-07 09:33:18.422346: step 26470, loss = 0.61, batch loss = 0.54 (5.6 examples/sec; 1.419 sec/batch; 120h:36m:33s remains)
INFO - root - 2017-12-07 09:33:33.570049: step 26480, loss = 0.79, batch loss = 0.72 (5.1 examples/sec; 1.555 sec/batch; 132h:11m:12s remains)
INFO - root - 2017-12-07 09:33:48.575832: step 26490, loss = 0.60, batch loss = 0.53 (5.2 examples/sec; 1.546 sec/batch; 131h:22m:43s remains)
INFO - root - 2017-12-07 09:34:03.252270: step 26500, loss = 0.63, batch loss = 0.56 (6.0 examples/sec; 1.331 sec/batch; 113h:09m:27s remains)
2017-12-07 09:34:04.352796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7795064 -2.7716815 -2.7502909 -2.7448597 -2.7590346 -2.7848449 -2.8138931 -2.831743 -2.8345852 -2.8311038 -2.8306389 -2.8282909 -2.8089833 -2.7731214 -2.738749][-2.7757339 -2.7861371 -2.7881122 -2.8046668 -2.8452241 -2.8997865 -2.9566875 -2.9902067 -2.9935873 -2.9817131 -2.9659228 -2.944577 -2.8946702 -2.8181691 -2.746191][-2.7729163 -2.8106766 -2.84542 -2.8840356 -2.9477844 -3.0347333 -3.12649 -3.1702218 -3.1488736 -3.1125784 -3.0834184 -3.0456557 -2.9597292 -2.8381412 -2.7304192][-2.744597 -2.8000455 -2.8573155 -2.8973153 -2.9535351 -3.0453584 -3.162106 -3.2182765 -3.1747465 -3.126441 -3.1186712 -3.1017041 -3.0038261 -2.8549805 -2.7204819][-2.6910152 -2.7346711 -2.7729087 -2.7761593 -2.7793403 -2.8390641 -2.9591482 -3.0261297 -2.962795 -2.9025536 -2.9488409 -3.0278764 -3.0005574 -2.8683176 -2.7112834][-2.6595545 -2.6835248 -2.6810818 -2.6050699 -2.4921279 -2.45965 -2.5711133 -2.6631746 -2.5915215 -2.5028698 -2.6049142 -2.8250647 -2.9371126 -2.8703496 -2.7077074][-2.7167928 -2.777673 -2.7959402 -2.6623535 -2.4012935 -2.2056069 -2.2077377 -2.2270985 -2.1000621 -1.9976742 -2.1915412 -2.5671716 -2.8296647 -2.8638935 -2.7373338][-2.8675804 -3.0044479 -3.0827322 -2.9455147 -2.6074071 -2.3183236 -2.2066076 -2.1079397 -1.9041731 -1.8069816 -2.0989387 -2.5827355 -2.910686 -2.960691 -2.8193324][-3.0147097 -3.1907127 -3.2842607 -3.1788018 -2.8916698 -2.6464903 -2.5199614 -2.3815928 -2.1665485 -2.0317619 -2.2466331 -2.6703489 -2.9742074 -3.017303 -2.8597071][-3.0984497 -3.2523651 -3.28636 -3.1813917 -2.9800413 -2.8040261 -2.6802573 -2.5568302 -2.4021974 -2.2497401 -2.3159308 -2.6151066 -2.8941183 -2.9713287 -2.8685682][-3.1030467 -3.1759331 -3.1036067 -2.954741 -2.8032348 -2.6623931 -2.5665112 -2.5452461 -2.5296805 -2.4576962 -2.4930415 -2.7586265 -3.0093153 -3.0491982 -2.9377868][-3.0161207 -2.9730189 -2.7985344 -2.6216893 -2.5320828 -2.483809 -2.5068789 -2.6672888 -2.8062027 -2.8134956 -2.8311758 -3.0304103 -3.2202349 -3.1861925 -3.0122972][-2.8442941 -2.7119145 -2.5143077 -2.4033465 -2.4290831 -2.5115962 -2.6673951 -2.9257231 -3.1324553 -3.1896291 -3.1965446 -3.2888312 -3.3573236 -3.2372317 -3.0116873][-2.5707228 -2.4343367 -2.3391638 -2.3833621 -2.5433798 -2.7168131 -2.9035959 -3.1247988 -3.2966013 -3.3694184 -3.3973348 -3.4490845 -3.4303632 -3.2537115 -3.0026011][-2.2651811 -2.2346518 -2.2847865 -2.4409716 -2.6440206 -2.8245656 -3.0029316 -3.1616039 -3.2488372 -3.2715876 -3.3009703 -3.3647604 -3.3481498 -3.1916142 -2.9835749]]...]
INFO - root - 2017-12-07 09:34:19.242176: step 26510, loss = 0.73, batch loss = 0.66 (5.6 examples/sec; 1.432 sec/batch; 121h:42m:27s remains)
INFO - root - 2017-12-07 09:34:34.204449: step 26520, loss = 0.70, batch loss = 0.63 (5.2 examples/sec; 1.529 sec/batch; 129h:59m:13s remains)
INFO - root - 2017-12-07 09:34:49.246566: step 26530, loss = 0.76, batch loss = 0.69 (5.1 examples/sec; 1.562 sec/batch; 132h:44m:25s remains)
INFO - root - 2017-12-07 09:35:04.158207: step 26540, loss = 0.69, batch loss = 0.62 (5.6 examples/sec; 1.427 sec/batch; 121h:16m:09s remains)
INFO - root - 2017-12-07 09:35:18.923291: step 26550, loss = 0.68, batch loss = 0.60 (5.4 examples/sec; 1.495 sec/batch; 127h:03m:41s remains)
INFO - root - 2017-12-07 09:35:33.973086: step 26560, loss = 0.62, batch loss = 0.54 (5.0 examples/sec; 1.587 sec/batch; 134h:53m:28s remains)
INFO - root - 2017-12-07 09:35:48.862592: step 26570, loss = 0.84, batch loss = 0.77 (5.1 examples/sec; 1.559 sec/batch; 132h:30m:38s remains)
INFO - root - 2017-12-07 09:36:03.742982: step 26580, loss = 0.78, batch loss = 0.70 (5.1 examples/sec; 1.561 sec/batch; 132h:37m:43s remains)
INFO - root - 2017-12-07 09:36:18.775642: step 26590, loss = 0.79, batch loss = 0.72 (5.4 examples/sec; 1.489 sec/batch; 126h:30m:52s remains)
INFO - root - 2017-12-07 09:36:33.784879: step 26600, loss = 0.81, batch loss = 0.74 (5.2 examples/sec; 1.536 sec/batch; 130h:28m:48s remains)
2017-12-07 09:36:34.831157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1785314 -3.2527971 -3.3587241 -3.4586039 -3.5256937 -3.5309856 -3.457891 -3.3493083 -3.2925324 -3.278703 -3.2790265 -3.2765408 -3.2524285 -3.1800179 -3.1022303][-3.7119203 -3.7600417 -3.8548265 -3.953073 -4.0238876 -4.0275683 -3.9255738 -3.7844322 -3.721065 -3.7280941 -3.736901 -3.6981282 -3.6012111 -3.4628692 -3.3787909][-3.858067 -3.8746283 -3.944006 -3.997498 -4.0394769 -4.0485134 -3.9359565 -3.7818027 -3.7339478 -3.7706935 -3.8062439 -3.7799563 -3.6684124 -3.5126073 -3.4564271][-3.6648538 -3.6709325 -3.7220173 -3.7124681 -3.7120733 -3.7392993 -3.6301427 -3.4358788 -3.3624654 -3.3949857 -3.4863303 -3.5615878 -3.5320759 -3.4177346 -3.4130592][-3.3049865 -3.3289175 -3.34655 -3.2494173 -3.2001719 -3.2812324 -3.256505 -3.078176 -2.9520802 -2.9553652 -3.1107292 -3.3355343 -3.4448628 -3.4203062 -3.4950855][-2.9995685 -3.0468228 -3.0224197 -2.8211761 -2.689836 -2.7808237 -2.8620892 -2.7838879 -2.6547234 -2.585453 -2.7166963 -2.9848657 -3.166539 -3.2202764 -3.356709][-2.7703643 -2.8672915 -2.8478131 -2.5835662 -2.33624 -2.277009 -2.2983613 -2.320673 -2.3227077 -2.2484412 -2.2684655 -2.4151196 -2.5525534 -2.6914563 -2.9392388][-2.668458 -2.7641249 -2.7446778 -2.4902353 -2.1969309 -1.9424002 -1.7327678 -1.7868366 -2.0135133 -2.1129425 -2.1288207 -2.1298544 -2.1198816 -2.2706735 -2.5783119][-2.6378055 -2.6908457 -2.6564741 -2.4667716 -2.2860079 -2.0409577 -1.7171228 -1.6952417 -1.9633152 -2.1530595 -2.2434127 -2.220907 -2.0928357 -2.0901625 -2.24639][-2.41354 -2.5102992 -2.5519342 -2.5105379 -2.5356452 -2.4795136 -2.3125281 -2.3265681 -2.5142379 -2.6082759 -2.6706347 -2.6424584 -2.5187278 -2.4468884 -2.4674835][-1.9823039 -2.2302294 -2.432128 -2.5659103 -2.7995052 -2.9684162 -3.0566795 -3.2130761 -3.365397 -3.3715906 -3.3603284 -3.3179932 -3.235157 -3.1621561 -3.1068482][-1.7375684 -2.1438043 -2.4739447 -2.7343781 -3.0951288 -3.4055104 -3.6328082 -3.8361037 -3.9268317 -3.881671 -3.8304725 -3.7872462 -3.7437844 -3.6937351 -3.6115298][-1.9561806 -2.3968303 -2.6810594 -2.9135265 -3.2363644 -3.527102 -3.7406564 -3.9102407 -3.9738259 -3.9352641 -3.8888958 -3.8517997 -3.84785 -3.8940868 -3.9393156][-2.5003135 -2.8426027 -2.977386 -3.0900145 -3.2773461 -3.4505925 -3.5779352 -3.6858644 -3.7421422 -3.7478366 -3.7581673 -3.7819111 -3.8350585 -3.929065 -4.036798][-3.0162425 -3.1562889 -3.1378078 -3.1455116 -3.2350452 -3.3222115 -3.3867869 -3.4619389 -3.5317857 -3.5897346 -3.6639845 -3.7624583 -3.8717508 -3.9718692 -4.0512877]]...]
INFO - root - 2017-12-07 09:36:49.573242: step 26610, loss = 0.56, batch loss = 0.49 (4.9 examples/sec; 1.619 sec/batch; 137h:35m:41s remains)
INFO - root - 2017-12-07 09:37:04.477149: step 26620, loss = 0.71, batch loss = 0.63 (5.2 examples/sec; 1.529 sec/batch; 129h:53m:22s remains)
INFO - root - 2017-12-07 09:37:19.402718: step 26630, loss = 0.79, batch loss = 0.72 (5.8 examples/sec; 1.380 sec/batch; 117h:15m:54s remains)
INFO - root - 2017-12-07 09:37:34.352608: step 26640, loss = 0.70, batch loss = 0.63 (5.6 examples/sec; 1.424 sec/batch; 121h:01m:12s remains)
INFO - root - 2017-12-07 09:37:49.397086: step 26650, loss = 0.81, batch loss = 0.74 (5.4 examples/sec; 1.489 sec/batch; 126h:30m:40s remains)
INFO - root - 2017-12-07 09:38:04.439639: step 26660, loss = 0.71, batch loss = 0.64 (5.3 examples/sec; 1.498 sec/batch; 127h:13m:54s remains)
INFO - root - 2017-12-07 09:38:19.529785: step 26670, loss = 0.80, batch loss = 0.72 (5.6 examples/sec; 1.419 sec/batch; 120h:31m:55s remains)
INFO - root - 2017-12-07 09:38:34.156478: step 26680, loss = 0.93, batch loss = 0.86 (5.7 examples/sec; 1.411 sec/batch; 119h:54m:18s remains)
INFO - root - 2017-12-07 09:38:48.922392: step 26690, loss = 0.97, batch loss = 0.90 (5.4 examples/sec; 1.475 sec/batch; 125h:16m:10s remains)
INFO - root - 2017-12-07 09:39:03.968422: step 26700, loss = 0.62, batch loss = 0.55 (5.3 examples/sec; 1.510 sec/batch; 128h:14m:31s remains)
2017-12-07 09:39:05.046360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2903938 -2.368921 -2.3764794 -2.3642304 -2.3575261 -2.3947253 -2.4258389 -2.3598514 -2.273567 -2.336632 -2.4001398 -2.3428843 -2.2740018 -2.1969161 -2.1876237][-2.3293064 -2.5191298 -2.5902977 -2.5790317 -2.5457711 -2.5731843 -2.6123857 -2.5197823 -2.3764534 -2.4724326 -2.6278095 -2.5906525 -2.4867039 -2.339968 -2.2377944][-2.1948743 -2.4423161 -2.5608726 -2.5548444 -2.4732163 -2.4405627 -2.4818869 -2.4129658 -2.2671051 -2.3860295 -2.6043801 -2.5698967 -2.415838 -2.1981032 -2.012753][-1.9123604 -2.1523552 -2.3006506 -2.3149705 -2.1536741 -1.9804153 -1.9626765 -1.9717982 -1.9445775 -2.1324735 -2.3923173 -2.3528807 -2.141736 -1.8282673 -1.5485005][-1.5500906 -1.7715967 -1.91008 -1.8896296 -1.6114016 -1.2252662 -1.0672331 -1.2195222 -1.4544134 -1.7537997 -2.0510862 -2.0550492 -1.8456066 -1.4753773 -1.1463108][-1.189822 -1.4406738 -1.5380955 -1.4150264 -1.0045831 -0.361269 0.030090809 -0.31439352 -0.90344954 -1.3617547 -1.7089584 -1.7940116 -1.6330278 -1.3053014 -1.0089166][-0.85508752 -1.1427975 -1.1893742 -0.98020387 -0.51238203 0.3000493 0.87673521 0.35327482 -0.53611374 -1.1469803 -1.5635965 -1.707927 -1.5650473 -1.2588983 -0.97345805][-0.547837 -0.85423112 -0.88426614 -0.67612267 -0.32894707 0.36163759 0.89373016 0.33204603 -0.65797067 -1.3881569 -1.8497655 -1.9338162 -1.6467175 -1.204783 -0.8186357][-0.317585 -0.67381525 -0.80191135 -0.74415731 -0.61378932 -0.2167573 0.13813353 -0.27761555 -1.1315866 -1.9035642 -2.3449528 -2.3004382 -1.8593283 -1.2576785 -0.76807952][-0.47637916 -0.87764955 -1.0998094 -1.1584435 -1.1531427 -0.96056032 -0.7380383 -0.98216724 -1.6112306 -2.3187385 -2.7249374 -2.6005259 -2.1177356 -1.4868484 -1.0049076][-1.0776706 -1.5065281 -1.7330463 -1.7857482 -1.7687147 -1.6239386 -1.4433293 -1.5778267 -1.9905515 -2.5316324 -2.9356048 -2.8527761 -2.4002795 -1.7985563 -1.3986921][-1.6617038 -2.0971439 -2.2971797 -2.2651091 -2.1855583 -2.0152879 -1.8151376 -1.8821473 -2.1421249 -2.5049419 -2.8786745 -2.9070704 -2.5696325 -2.0398214 -1.7057137][-1.8912857 -2.2722945 -2.4242661 -2.3077688 -2.2287214 -2.1185327 -1.9392431 -1.9128623 -2.0437899 -2.2801239 -2.6299138 -2.7593598 -2.5488214 -2.1043749 -1.8061838][-1.8161769 -2.1022229 -2.2386286 -2.139858 -2.1528454 -2.1685784 -2.0337491 -1.8901505 -1.8802185 -2.0423298 -2.3776491 -2.574748 -2.4437585 -2.068825 -1.7872577][-1.8127027 -1.9680843 -2.0728221 -2.0356815 -2.1242712 -2.2111709 -2.1203792 -1.9164398 -1.8279147 -1.9667053 -2.3098524 -2.5418248 -2.4822979 -2.1927752 -1.9439178]]...]
INFO - root - 2017-12-07 09:39:20.094757: step 26710, loss = 0.80, batch loss = 0.73 (5.1 examples/sec; 1.573 sec/batch; 133h:36m:03s remains)
INFO - root - 2017-12-07 09:39:35.043887: step 26720, loss = 0.59, batch loss = 0.52 (5.2 examples/sec; 1.539 sec/batch; 130h:45m:29s remains)
INFO - root - 2017-12-07 09:39:49.897803: step 26730, loss = 0.65, batch loss = 0.58 (5.3 examples/sec; 1.511 sec/batch; 128h:22m:33s remains)
INFO - root - 2017-12-07 09:40:04.803941: step 26740, loss = 0.93, batch loss = 0.86 (5.5 examples/sec; 1.467 sec/batch; 124h:37m:38s remains)
INFO - root - 2017-12-07 09:40:19.643780: step 26750, loss = 0.85, batch loss = 0.78 (5.5 examples/sec; 1.450 sec/batch; 123h:08m:05s remains)
INFO - root - 2017-12-07 09:40:34.542707: step 26760, loss = 0.93, batch loss = 0.86 (5.3 examples/sec; 1.513 sec/batch; 128h:29m:49s remains)
INFO - root - 2017-12-07 09:40:49.545664: step 26770, loss = 0.78, batch loss = 0.70 (5.1 examples/sec; 1.567 sec/batch; 133h:05m:24s remains)
INFO - root - 2017-12-07 09:41:04.437737: step 26780, loss = 0.91, batch loss = 0.83 (5.3 examples/sec; 1.500 sec/batch; 127h:24m:51s remains)
INFO - root - 2017-12-07 09:41:19.411347: step 26790, loss = 0.68, batch loss = 0.61 (5.4 examples/sec; 1.476 sec/batch; 125h:18m:46s remains)
INFO - root - 2017-12-07 09:41:34.640924: step 26800, loss = 0.85, batch loss = 0.78 (5.3 examples/sec; 1.497 sec/batch; 127h:06m:28s remains)
2017-12-07 09:41:35.814347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0029039 -1.9293923 -1.7922051 -1.7507324 -1.7209082 -1.6431177 -1.6668973 -1.9539175 -2.4942274 -2.8990464 -2.823622 -2.4434898 -2.1467378 -2.259222 -2.7166486][-1.5827944 -1.5250456 -1.3976605 -1.3523543 -1.2970071 -1.1658723 -1.1381469 -1.4054124 -1.9175105 -2.2976048 -2.2820055 -2.0655231 -1.9617937 -2.1978035 -2.6945868][-1.9920032 -2.0155482 -1.9879632 -1.9903972 -1.9478924 -1.7942293 -1.6900418 -1.8113015 -2.146657 -2.3992913 -2.3574924 -2.2850597 -2.4525633 -2.92628 -3.44637][-2.1315289 -2.2345729 -2.3003085 -2.3495178 -2.331306 -2.1836202 -2.0126171 -1.9759569 -2.1428454 -2.288228 -2.1842515 -2.1328549 -2.4801228 -3.126133 -3.6338737][-1.9825168 -2.0736473 -2.2092283 -2.3399823 -2.3751559 -2.2492609 -2.0686231 -1.998524 -2.090822 -2.1301379 -1.866405 -1.6861355 -2.0598636 -2.7986128 -3.3998094][-2.149569 -2.144067 -2.2941468 -2.5188575 -2.6159587 -2.5176454 -2.3717523 -2.3399148 -2.4181387 -2.4035835 -2.0344083 -1.7078407 -1.9248922 -2.5698314 -3.1490693][-2.804101 -2.6366792 -2.67945 -2.8980687 -2.9969711 -2.8792865 -2.6582069 -2.4871387 -2.3963685 -2.2949955 -1.9119949 -1.5434222 -1.6018748 -2.0871253 -2.57982][-2.7956018 -2.4869068 -2.3923364 -2.5475302 -2.6939325 -2.7129636 -2.5588746 -2.2564588 -1.9658167 -1.8264985 -1.6225779 -1.4555428 -1.5613751 -2.0045581 -2.465821][-2.2710004 -1.9421885 -1.7844093 -1.9135628 -2.1616137 -2.3879035 -2.3508279 -1.9296393 -1.4740472 -1.3920836 -1.5090833 -1.6631043 -1.8769557 -2.1850071 -2.4928141][-2.1371598 -1.8563533 -1.7489207 -1.9277802 -2.278409 -2.6002049 -2.5307403 -1.9565811 -1.3656199 -1.3072746 -1.5870881 -1.8947909 -2.1146166 -2.261482 -2.4170356][-2.0635886 -1.833467 -1.7722156 -1.9889247 -2.3911102 -2.7322745 -2.6418619 -2.0911877 -1.5692112 -1.5595684 -1.8491189 -2.1289568 -2.2634022 -2.2727649 -2.3023832][-2.3743742 -2.2794762 -2.2943327 -2.4842525 -2.8053088 -3.0514369 -2.9429231 -2.4681654 -2.0323973 -1.9909027 -2.1595969 -2.2762973 -2.2676468 -2.1786184 -2.108314][-2.7165182 -2.6463382 -2.6685052 -2.79636 -3.0026565 -3.1296778 -3.0178714 -2.6729746 -2.3733311 -2.3239675 -2.3861647 -2.3907084 -2.2961731 -2.1858642 -2.078804][-2.5731285 -2.5384719 -2.5657468 -2.6556654 -2.7840214 -2.8626883 -2.80542 -2.6319237 -2.5081244 -2.5149093 -2.5565464 -2.5052495 -2.3555703 -2.2057171 -2.0470321][-2.4614394 -2.4757316 -2.5274765 -2.5834312 -2.6390035 -2.6716628 -2.6527848 -2.5931525 -2.5727415 -2.5998559 -2.6211357 -2.556735 -2.3949885 -2.2149391 -2.0173912]]...]
INFO - root - 2017-12-07 09:41:50.569927: step 26810, loss = 0.84, batch loss = 0.77 (5.3 examples/sec; 1.501 sec/batch; 127h:26m:17s remains)
INFO - root - 2017-12-07 09:42:05.440435: step 26820, loss = 0.67, batch loss = 0.60 (5.5 examples/sec; 1.461 sec/batch; 124h:04m:55s remains)
INFO - root - 2017-12-07 09:42:20.226977: step 26830, loss = 0.79, batch loss = 0.72 (5.4 examples/sec; 1.472 sec/batch; 124h:56m:46s remains)
INFO - root - 2017-12-07 09:42:35.407011: step 26840, loss = 0.67, batch loss = 0.59 (5.1 examples/sec; 1.560 sec/batch; 132h:26m:42s remains)
INFO - root - 2017-12-07 09:42:50.271086: step 26850, loss = 0.74, batch loss = 0.67 (5.1 examples/sec; 1.580 sec/batch; 134h:08m:21s remains)
INFO - root - 2017-12-07 09:43:05.254638: step 26860, loss = 0.74, batch loss = 0.66 (5.4 examples/sec; 1.494 sec/batch; 126h:52m:33s remains)
INFO - root - 2017-12-07 09:43:20.114172: step 26870, loss = 0.75, batch loss = 0.68 (5.5 examples/sec; 1.456 sec/batch; 123h:36m:11s remains)
INFO - root - 2017-12-07 09:43:34.692250: step 26880, loss = 0.88, batch loss = 0.81 (5.4 examples/sec; 1.470 sec/batch; 124h:46m:51s remains)
INFO - root - 2017-12-07 09:43:49.591037: step 26890, loss = 0.75, batch loss = 0.67 (5.2 examples/sec; 1.552 sec/batch; 131h:46m:52s remains)
INFO - root - 2017-12-07 09:44:04.482479: step 26900, loss = 0.71, batch loss = 0.63 (5.1 examples/sec; 1.570 sec/batch; 133h:17m:20s remains)
2017-12-07 09:44:05.533463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7198625 -3.769788 -3.8012919 -3.8380167 -3.8515811 -3.8368602 -3.7850435 -3.7460208 -3.7311869 -3.7224529 -3.7406111 -3.7645183 -3.7414944 -3.6747558 -3.5864453][-3.7494178 -3.7245636 -3.6856546 -3.6755726 -3.65972 -3.619956 -3.4963973 -3.3810658 -3.3339744 -3.3337259 -3.4154415 -3.5459912 -3.6324892 -3.6512067 -3.6273861][-3.2986465 -3.2215261 -3.1697407 -3.1628909 -3.1438541 -3.0912843 -2.9120746 -2.7184877 -2.6369264 -2.6309297 -2.7614903 -2.9785957 -3.1404042 -3.2130108 -3.2301011][-2.7635157 -2.7354732 -2.8129411 -2.9598382 -3.0545964 -3.0481586 -2.8835793 -2.6734433 -2.5991261 -2.6101875 -2.7905908 -3.0819881 -3.2216446 -3.2073953 -3.1279612][-2.6887586 -2.7583311 -2.9761362 -3.2791705 -3.4653964 -3.4548085 -3.2508101 -3.0080576 -2.9259791 -2.9519253 -3.1971319 -3.5724633 -3.6463618 -3.4811459 -3.2507052][-2.9768519 -3.2151051 -3.5382175 -3.8854959 -4.0060387 -3.7858479 -3.3406806 -3.0080659 -3.0141692 -3.1762381 -3.513412 -3.8879905 -3.7682929 -3.3195055 -2.8539181][-2.563231 -2.896946 -3.2043419 -3.399816 -3.2109261 -2.5554633 -1.6604726 -1.169714 -1.4677725 -2.0956268 -2.8226237 -3.4131551 -3.3121984 -2.7511411 -2.1930497][-2.0360813 -2.3206794 -2.5235114 -2.5326779 -2.0581985 -1.1026158 0.0759325 0.70045376 0.1428299 -0.87802553 -1.9188855 -2.7090623 -2.7564182 -2.3329382 -1.967684][-2.4051721 -2.6521835 -2.8169365 -2.7964654 -2.3560877 -1.5832605 -0.66839123 -0.21695995 -0.76787782 -1.6767657 -2.4569824 -2.9681897 -2.8465829 -2.3954732 -2.1663187][-3.1975892 -3.4286177 -3.6428385 -3.7555857 -3.525254 -3.1073322 -2.6190329 -2.4059515 -2.8518033 -3.4637456 -3.8176751 -3.9096429 -3.5035272 -2.8882957 -2.5920742][-3.663672 -3.8757961 -4.0960569 -4.3089628 -4.2705665 -4.1273942 -3.9362862 -3.8219805 -4.0875463 -4.4280753 -4.4922342 -4.3552375 -3.8626451 -3.1963685 -2.80871][-3.5138042 -3.6415863 -3.7490618 -3.8637309 -3.8116927 -3.728235 -3.6747856 -3.5850742 -3.717298 -3.9397845 -3.9155531 -3.7161469 -3.3018348 -2.7932849 -2.504899][-2.8722885 -2.8993044 -2.862107 -2.8186455 -2.6712379 -2.5243378 -2.4922013 -2.4022815 -2.4637215 -2.6778893 -2.7263691 -2.6461978 -2.4998736 -2.3368111 -2.29944][-2.242933 -2.2188859 -2.1004143 -1.9984245 -1.8419099 -1.7030878 -1.7221024 -1.6973693 -1.7618911 -1.9471669 -2.0446482 -2.0876153 -2.1966152 -2.3567116 -2.5305047][-2.0667279 -2.0620883 -1.9539192 -1.8832572 -1.8376155 -1.8450105 -2.0042317 -2.1122956 -2.2429891 -2.421581 -2.5010145 -2.5227745 -2.6389174 -2.8051562 -2.9294512]]...]
INFO - root - 2017-12-07 09:44:20.363894: step 26910, loss = 0.66, batch loss = 0.58 (5.4 examples/sec; 1.470 sec/batch; 124h:48m:18s remains)
INFO - root - 2017-12-07 09:44:35.278864: step 26920, loss = 0.79, batch loss = 0.72 (5.6 examples/sec; 1.439 sec/batch; 122h:06m:40s remains)
INFO - root - 2017-12-07 09:44:50.268473: step 26930, loss = 1.02, batch loss = 0.94 (5.1 examples/sec; 1.568 sec/batch; 133h:06m:21s remains)
INFO - root - 2017-12-07 09:45:04.988451: step 26940, loss = 0.77, batch loss = 0.69 (5.3 examples/sec; 1.502 sec/batch; 127h:28m:35s remains)
INFO - root - 2017-12-07 09:45:19.737234: step 26950, loss = 0.76, batch loss = 0.69 (5.6 examples/sec; 1.426 sec/batch; 120h:59m:31s remains)
INFO - root - 2017-12-07 09:45:34.549459: step 26960, loss = 0.86, batch loss = 0.79 (5.5 examples/sec; 1.460 sec/batch; 123h:56m:37s remains)
INFO - root - 2017-12-07 09:45:49.484937: step 26970, loss = 0.83, batch loss = 0.76 (5.4 examples/sec; 1.485 sec/batch; 126h:03m:00s remains)
INFO - root - 2017-12-07 09:46:04.324592: step 26980, loss = 0.77, batch loss = 0.69 (5.1 examples/sec; 1.557 sec/batch; 132h:08m:21s remains)
INFO - root - 2017-12-07 09:46:19.143377: step 26990, loss = 0.81, batch loss = 0.74 (5.5 examples/sec; 1.454 sec/batch; 123h:24m:46s remains)
INFO - root - 2017-12-07 09:46:34.165608: step 27000, loss = 0.80, batch loss = 0.73 (5.1 examples/sec; 1.557 sec/batch; 132h:06m:35s remains)
2017-12-07 09:46:35.283169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.34045 -3.4549108 -3.5372596 -3.5492206 -3.4635935 -3.2286229 -2.9067669 -2.5769556 -2.3075614 -2.2540231 -2.2193973 -2.0784204 -1.9098318 -1.7547755 -1.7998092][-4.4533448 -4.9279833 -5.1760769 -5.1378808 -4.8623648 -4.3567157 -3.8327365 -3.4485984 -3.1752439 -3.1527853 -3.0767081 -2.7938771 -2.4483311 -2.0708277 -1.9208255][-5.2286205 -5.8561859 -6.1686668 -6.1009707 -5.7737484 -5.1366873 -4.5236163 -4.1949029 -3.9899869 -3.987992 -3.833648 -3.3600931 -2.8058062 -2.184057 -1.8413374][-4.8913751 -5.4276652 -5.7247491 -5.6972885 -5.4147468 -4.731617 -4.0062633 -3.7060471 -3.6121554 -3.7158108 -3.6416545 -3.196332 -2.6561782 -1.9970095 -1.6129479][-3.8758073 -4.2349539 -4.4643784 -4.3204026 -3.8647976 -2.9888425 -2.0921233 -1.8053019 -1.8951921 -2.2115705 -2.3489149 -2.1079409 -1.80562 -1.414835 -1.1984839][-3.1433659 -3.3586502 -3.4401128 -2.9128163 -1.9387953 -0.5822258 0.69706059 0.97219706 0.4249258 -0.36656713 -0.87468529 -0.99085975 -1.0511422 -1.0337296 -0.95636797][-3.2181077 -3.257452 -3.0252671 -2.0165749 -0.51301765 1.3386836 3.0274558 3.3061829 2.2402039 0.91252279 -0.0061502457 -0.533932 -0.96373153 -1.2550182 -1.1840785][-3.8710475 -3.7684586 -3.2859564 -2.0558572 -0.3860817 1.5189886 3.1470323 3.3176632 2.1608143 0.84173536 -0.13876343 -0.77091694 -1.2842917 -1.6453991 -1.4994318][-4.3391795 -4.4145904 -4.09204 -3.0518882 -1.5595963 0.017202854 1.2061615 1.2945547 0.6097765 -0.15135574 -0.83278823 -1.2772689 -1.6103485 -1.8198617 -1.5880194][-4.3212218 -4.8304658 -4.8886361 -4.1494188 -2.90971 -1.6994681 -0.8842268 -0.73099828 -0.76705337 -0.77461457 -0.98616147 -1.1880977 -1.3688502 -1.4876125 -1.3249393][-4.0756259 -4.9019313 -5.2102814 -4.6577425 -3.5801373 -2.5530825 -1.8969696 -1.5838695 -1.1233933 -0.65056086 -0.53190207 -0.58313942 -0.7344656 -0.89359593 -0.96195674][-3.6874297 -4.4735708 -4.7598839 -4.2812 -3.3832452 -2.508296 -1.8774087 -1.3566616 -0.69816637 -0.18376398 0.00056314468 -0.063489914 -0.33235693 -0.64377 -0.94743323][-3.0323606 -3.5766292 -3.8184605 -3.4995034 -2.7956131 -2.026099 -1.4235551 -0.84900951 -0.29642582 0.023481846 0.083581924 -0.12129688 -0.53345823 -0.97991014 -1.3825243][-2.4514532 -2.7385101 -2.9782567 -2.8687773 -2.3450084 -1.7154076 -1.2396617 -0.80194259 -0.49988389 -0.41773987 -0.53702831 -0.87035131 -1.3169606 -1.7305441 -2.0441332][-2.2657902 -2.3055956 -2.4136782 -2.3241687 -1.9178133 -1.5211861 -1.3240893 -1.2123802 -1.2233102 -1.3042526 -1.4871423 -1.8116896 -2.1899714 -2.4902077 -2.6233697]]...]
INFO - root - 2017-12-07 09:46:49.938786: step 27010, loss = 0.73, batch loss = 0.65 (5.5 examples/sec; 1.446 sec/batch; 122h:42m:13s remains)
INFO - root - 2017-12-07 09:47:04.818910: step 27020, loss = 0.80, batch loss = 0.73 (5.6 examples/sec; 1.440 sec/batch; 122h:12m:54s remains)
INFO - root - 2017-12-07 09:47:19.869889: step 27030, loss = 0.74, batch loss = 0.67 (5.4 examples/sec; 1.495 sec/batch; 126h:51m:30s remains)
INFO - root - 2017-12-07 09:47:34.809030: step 27040, loss = 0.91, batch loss = 0.83 (5.2 examples/sec; 1.542 sec/batch; 130h:51m:08s remains)
INFO - root - 2017-12-07 09:47:49.746428: step 27050, loss = 0.57, batch loss = 0.50 (5.0 examples/sec; 1.596 sec/batch; 135h:24m:23s remains)
INFO - root - 2017-12-07 09:48:04.805284: step 27060, loss = 0.71, batch loss = 0.64 (5.4 examples/sec; 1.474 sec/batch; 125h:05m:34s remains)
INFO - root - 2017-12-07 09:48:19.563474: step 27070, loss = 0.85, batch loss = 0.77 (5.6 examples/sec; 1.427 sec/batch; 121h:03m:56s remains)
INFO - root - 2017-12-07 09:48:34.407007: step 27080, loss = 0.88, batch loss = 0.81 (5.5 examples/sec; 1.452 sec/batch; 123h:13m:30s remains)
INFO - root - 2017-12-07 09:48:49.384407: step 27090, loss = 0.85, batch loss = 0.78 (5.1 examples/sec; 1.570 sec/batch; 133h:12m:22s remains)
INFO - root - 2017-12-07 09:49:04.638689: step 27100, loss = 0.84, batch loss = 0.77 (4.9 examples/sec; 1.620 sec/batch; 137h:26m:23s remains)
2017-12-07 09:49:05.735170: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.7377882 -5.0835891 -5.2841249 -5.2835727 -5.1614184 -5.0034161 -4.9199891 -4.899128 -4.89387 -4.899261 -4.8676291 -4.7969408 -4.6252613 -4.3377624 -4.0171323][-4.6977186 -4.9978709 -5.1479826 -5.1157556 -4.987134 -4.8400865 -4.8270617 -4.9030433 -4.9637356 -4.9878249 -4.9373193 -4.8400488 -4.6283574 -4.2920942 -3.9284928][-4.1906652 -4.3868403 -4.4387851 -4.3278623 -4.0828195 -3.7926385 -3.7275772 -3.8422778 -3.9795427 -4.0689049 -4.0984077 -4.1045218 -3.9893012 -3.7488356 -3.4788404][-3.510325 -3.6655474 -3.6831381 -3.510457 -3.1067162 -2.6066198 -2.4312367 -2.6048892 -2.9076562 -3.1544776 -3.3125408 -3.4100251 -3.3580503 -3.1905928 -3.0029402][-3.0738745 -3.2740643 -3.2938974 -3.0086925 -2.3336375 -1.5095723 -1.1271243 -1.3774874 -1.9839358 -2.5655851 -2.9708538 -3.2001686 -3.1837652 -3.0036287 -2.771822][-2.8453643 -3.0333285 -2.9977365 -2.5504785 -1.5679381 -0.39436674 0.2604351 -0.01341629 -0.88449192 -1.8087215 -2.546628 -3.0123219 -3.1584811 -3.0287886 -2.7587605][-2.5402718 -2.6142302 -2.4771924 -1.9027419 -0.79636359 0.46185541 1.1932106 0.87778091 -0.12615824 -1.25312 -2.2633524 -2.9490926 -3.2613378 -3.1635475 -2.8014507][-2.1164939 -2.0033212 -1.7719512 -1.1997476 -0.24812841 0.75685263 1.3275747 0.94834375 -0.072064877 -1.2885671 -2.5024874 -3.3273895 -3.7074456 -3.533644 -2.9848619][-1.6620817 -1.4172845 -1.2170255 -0.88098407 -0.35868549 0.13156033 0.36668539 -0.012406349 -0.81348372 -1.8493869 -2.9800539 -3.7306104 -4.0547905 -3.8124843 -3.1394815][-1.4767423 -1.3124762 -1.2819958 -1.2688131 -1.203187 -1.169888 -1.2245102 -1.4979427 -1.8824501 -2.4344761 -3.1249783 -3.550899 -3.7154455 -3.4764524 -2.8813033][-1.7800813 -1.7890787 -1.8733599 -1.9890151 -2.074966 -2.151613 -2.2614195 -2.3839991 -2.4187887 -2.561007 -2.8501463 -3.0042663 -3.0829806 -2.9472237 -2.5389833][-2.3947957 -2.4722219 -2.5338452 -2.5947769 -2.6263647 -2.62576 -2.651468 -2.6355464 -2.50385 -2.48604 -2.6257243 -2.7311544 -2.8400953 -2.8143377 -2.5484235][-2.9811032 -3.0175908 -3.013073 -3.0267739 -3.0479541 -3.0260875 -3.004323 -2.9244342 -2.762861 -2.7023954 -2.7744234 -2.8762732 -3.0117965 -3.0316586 -2.8415236][-3.2814186 -3.2934513 -3.2704785 -3.2717133 -3.2865162 -3.2656755 -3.2208772 -3.1338191 -3.0179174 -2.9952736 -3.0577481 -3.1469049 -3.2525568 -3.2639456 -3.1198833][-3.2715726 -3.2609513 -3.2238877 -3.2071702 -3.2164235 -3.2189479 -3.2049031 -3.1716647 -3.1403537 -3.1734066 -3.2451763 -3.32085 -3.3804102 -3.3675315 -3.2503443]]...]
INFO - root - 2017-12-07 09:49:20.857217: step 27110, loss = 0.63, batch loss = 0.55 (5.3 examples/sec; 1.507 sec/batch; 127h:49m:12s remains)
INFO - root - 2017-12-07 09:49:35.687337: step 27120, loss = 0.77, batch loss = 0.70 (5.7 examples/sec; 1.392 sec/batch; 118h:02m:43s remains)
INFO - root - 2017-12-07 09:49:50.618813: step 27130, loss = 0.95, batch loss = 0.88 (5.4 examples/sec; 1.487 sec/batch; 126h:08m:02s remains)
INFO - root - 2017-12-07 09:50:05.585365: step 27140, loss = 0.69, batch loss = 0.61 (5.3 examples/sec; 1.507 sec/batch; 127h:47m:44s remains)
INFO - root - 2017-12-07 09:50:20.726902: step 27150, loss = 0.81, batch loss = 0.74 (5.5 examples/sec; 1.465 sec/batch; 124h:17m:39s remains)
INFO - root - 2017-12-07 09:50:35.729513: step 27160, loss = 0.53, batch loss = 0.46 (5.3 examples/sec; 1.510 sec/batch; 128h:04m:26s remains)
INFO - root - 2017-12-07 09:50:50.731142: step 27170, loss = 0.74, batch loss = 0.67 (5.0 examples/sec; 1.609 sec/batch; 136h:25m:57s remains)
INFO - root - 2017-12-07 09:51:05.718965: step 27180, loss = 0.58, batch loss = 0.51 (5.1 examples/sec; 1.566 sec/batch; 132h:48m:48s remains)
INFO - root - 2017-12-07 09:51:20.435155: step 27190, loss = 0.82, batch loss = 0.75 (5.4 examples/sec; 1.471 sec/batch; 124h:47m:30s remains)
INFO - root - 2017-12-07 09:51:35.298483: step 27200, loss = 0.64, batch loss = 0.57 (5.4 examples/sec; 1.469 sec/batch; 124h:37m:01s remains)
2017-12-07 09:51:36.326701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.826062 -2.8381822 -2.8644674 -2.8897848 -2.9386029 -2.9906123 -3.0395942 -3.0961788 -3.1866522 -3.2870569 -3.3025827 -3.2456856 -3.1916466 -3.1453824 -3.118191][-2.6954527 -2.7458954 -2.780664 -2.7748718 -2.777123 -2.810235 -2.8641405 -2.9326935 -3.0740309 -3.2678647 -3.3575618 -3.3235407 -3.2602448 -3.190124 -3.168335][-2.6626942 -2.776392 -2.77844 -2.6544333 -2.5013766 -2.4295368 -2.3888583 -2.3642879 -2.4938519 -2.807014 -3.0518398 -3.1056824 -3.0662065 -2.9820628 -2.9852307][-2.7449169 -2.8956413 -2.8335044 -2.5793164 -2.2429023 -2.0052674 -1.7499893 -1.4768498 -1.526221 -2.0097456 -2.5333891 -2.8156998 -2.8844366 -2.810061 -2.824158][-2.8411858 -2.9933624 -2.8490086 -2.4884562 -1.9820726 -1.5218937 -0.93604946 -0.30297565 -0.23779631 -0.9472394 -1.8745494 -2.5291338 -2.8040366 -2.7647204 -2.7813816][-2.8905034 -3.0590084 -2.8805282 -2.4740944 -1.8449953 -1.1338038 -0.14837313 0.90631151 1.1286354 0.19677448 -1.1190543 -2.1322715 -2.601459 -2.5999761 -2.6309814][-2.8139577 -2.983314 -2.8173454 -2.4473906 -1.8279183 -1.0058191 0.23957825 1.5991397 1.9611459 0.92929077 -0.53591013 -1.704581 -2.2634907 -2.3004398 -2.3477483][-2.6489563 -2.7752843 -2.6172347 -2.3615506 -1.9377983 -1.2766848 -0.16646814 1.1366682 1.5309639 0.63798714 -0.61016083 -1.639653 -2.1260314 -2.0959997 -2.0312459][-2.5506752 -2.6580229 -2.5097346 -2.3337331 -2.1273293 -1.8235979 -1.2047391 -0.3364687 0.0087146759 -0.43647432 -1.103374 -1.7324309 -2.0318205 -1.8722453 -1.6176803][-2.4611495 -2.6064796 -2.4876418 -2.3289914 -2.2220504 -2.2108986 -2.1129456 -1.7861419 -1.5388317 -1.5180111 -1.5462375 -1.6643586 -1.6981263 -1.4016902 -1.027276][-2.190738 -2.4717445 -2.5034733 -2.4119155 -2.3600779 -2.5152049 -2.7536063 -2.7929287 -2.6284578 -2.3338602 -1.9706502 -1.6991053 -1.4397888 -0.92294073 -0.39910698][-1.9906056 -2.4628365 -2.6848285 -2.6559193 -2.6012096 -2.817929 -3.1729531 -3.3049376 -3.1155844 -2.6786275 -2.2065263 -1.8222821 -1.4237256 -0.7660799 -0.10813236][-2.0526891 -2.6121335 -2.8962498 -2.8216991 -2.6838307 -2.8863153 -3.2346177 -3.3139582 -3.0582619 -2.5682642 -2.1956131 -1.9232345 -1.5909607 -1.0028977 -0.35242987][-2.0883014 -2.5776525 -2.84203 -2.7476287 -2.5869985 -2.75648 -3.0063448 -2.9628875 -2.6894579 -2.3420486 -2.2537315 -2.1735663 -1.9699199 -1.5423455 -0.93051338][-2.1757474 -2.5932243 -2.8747377 -2.8468366 -2.7084727 -2.7702327 -2.8250542 -2.6495333 -2.4456329 -2.4082425 -2.6783466 -2.752439 -2.5605898 -2.2373188 -1.6795597]]...]
INFO - root - 2017-12-07 09:51:51.266191: step 27210, loss = 0.68, batch loss = 0.61 (5.2 examples/sec; 1.535 sec/batch; 130h:10m:36s remains)
INFO - root - 2017-12-07 09:52:06.338498: step 27220, loss = 0.88, batch loss = 0.80 (5.2 examples/sec; 1.550 sec/batch; 131h:28m:13s remains)
INFO - root - 2017-12-07 09:52:21.321874: step 27230, loss = 0.68, batch loss = 0.61 (5.6 examples/sec; 1.419 sec/batch; 120h:18m:15s remains)
INFO - root - 2017-12-07 09:52:36.277588: step 27240, loss = 0.99, batch loss = 0.92 (5.3 examples/sec; 1.508 sec/batch; 127h:53m:43s remains)
INFO - root - 2017-12-07 09:52:51.139299: step 27250, loss = 0.81, batch loss = 0.73 (5.5 examples/sec; 1.456 sec/batch; 123h:25m:03s remains)
INFO - root - 2017-12-07 09:53:06.022439: step 27260, loss = 0.78, batch loss = 0.71 (5.1 examples/sec; 1.554 sec/batch; 131h:46m:09s remains)
INFO - root - 2017-12-07 09:53:20.797590: step 27270, loss = 0.74, batch loss = 0.67 (5.3 examples/sec; 1.513 sec/batch; 128h:19m:21s remains)
INFO - root - 2017-12-07 09:53:35.935179: step 27280, loss = 0.81, batch loss = 0.73 (5.6 examples/sec; 1.436 sec/batch; 121h:44m:54s remains)
INFO - root - 2017-12-07 09:53:50.888013: step 27290, loss = 0.83, batch loss = 0.76 (5.4 examples/sec; 1.468 sec/batch; 124h:28m:04s remains)
INFO - root - 2017-12-07 09:54:05.888158: step 27300, loss = 0.72, batch loss = 0.65 (5.2 examples/sec; 1.535 sec/batch; 130h:10m:31s remains)
2017-12-07 09:54:07.055421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3756838 -3.4123709 -3.3925862 -3.3494086 -3.3349757 -3.287663 -3.1370621 -2.9947643 -2.9346092 -2.9302719 -2.9071209 -2.8399677 -2.7560573 -2.7223752 -2.9739389][-3.4830177 -3.5377214 -3.4498179 -3.3400049 -3.2941298 -3.2824745 -3.2327151 -3.1935644 -3.2010453 -3.2486222 -3.2588141 -3.161721 -3.02909 -2.9259329 -3.1344957][-4.0184917 -3.9947309 -3.7836766 -3.5628569 -3.4679933 -3.4796915 -3.5504434 -3.6302524 -3.6353979 -3.6293397 -3.6244438 -3.5756097 -3.5623059 -3.5074987 -3.6424918][-4.6748061 -4.622838 -4.4153156 -4.1738167 -4.0813303 -4.1065359 -4.1891885 -4.2810426 -4.2276058 -4.1549387 -4.1602941 -4.2356143 -4.4329424 -4.4753332 -4.5111227][-4.5305939 -4.5484319 -4.4316564 -4.1973243 -4.0909462 -4.1059327 -4.1464396 -4.2703514 -4.2761855 -4.238831 -4.2791209 -4.3947134 -4.6621289 -4.7117434 -4.688695][-3.4044905 -3.4456506 -3.3223743 -3.0042989 -2.7980063 -2.7223964 -2.6575422 -2.7917674 -2.929198 -2.9860253 -3.0962653 -3.2877469 -3.6271334 -3.7319555 -3.7348919][-1.6697793 -1.7227669 -1.615638 -1.329905 -1.1082232 -0.86157775 -0.47453356 -0.45004964 -0.7296555 -1.043746 -1.3881128 -1.7793403 -2.2520952 -2.4410028 -2.4564123][-0.1503644 -0.2528739 -0.051584244 0.35524225 0.70170164 1.1737342 1.8901114 2.0495048 1.4950838 0.78338623 0.15407419 -0.37141562 -0.87444758 -1.0863907 -1.1049325][-0.24411345 -0.29058266 0.19331264 0.85062218 1.3406005 1.8834815 2.6567941 2.8004522 2.1048517 1.3411489 0.77498484 0.31490326 -0.12113094 -0.27611732 -0.2890296][-1.5465257 -1.485374 -0.99785352 -0.51989269 -0.2561326 0.075349331 0.66371918 0.80196667 0.38515043 0.0676465 -0.09627533 -0.3577919 -0.65724087 -0.6068542 -0.41059685][-2.2035253 -2.1289089 -1.9049115 -1.803308 -1.8110895 -1.6564672 -1.2171962 -1.083339 -1.3550885 -1.4136159 -1.337111 -1.4434054 -1.5966384 -1.3593857 -0.99162626][-1.7502472 -1.8138518 -1.8465607 -1.9258454 -2.0417354 -1.9775319 -1.655427 -1.5857856 -1.9100056 -2.0163264 -1.9431038 -2.041887 -2.2086194 -2.0470331 -1.8390393][-1.1200757 -1.2273738 -1.3496063 -1.4503057 -1.6162505 -1.6093419 -1.3222847 -1.2615201 -1.6610184 -1.9181283 -1.9760454 -2.1565788 -2.4014385 -2.4355001 -2.4842665][-1.4080145 -1.3826475 -1.3723483 -1.3452756 -1.4814246 -1.4914939 -1.1993692 -1.1170659 -1.5373766 -1.8922634 -2.0421355 -2.2279968 -2.3940506 -2.3847017 -2.4215059][-2.4135878 -2.3026807 -2.1322014 -1.940115 -2.01881 -2.0677106 -1.8478246 -1.8183753 -2.2324436 -2.5705724 -2.6840842 -2.7691994 -2.8029118 -2.692934 -2.6283174]]...]
INFO - root - 2017-12-07 09:54:21.861484: step 27310, loss = 0.76, batch loss = 0.69 (5.5 examples/sec; 1.447 sec/batch; 122h:42m:14s remains)
INFO - root - 2017-12-07 09:54:36.680385: step 27320, loss = 0.93, batch loss = 0.85 (5.3 examples/sec; 1.495 sec/batch; 126h:46m:06s remains)
INFO - root - 2017-12-07 09:54:51.652457: step 27330, loss = 0.83, batch loss = 0.76 (5.1 examples/sec; 1.572 sec/batch; 133h:14m:28s remains)
INFO - root - 2017-12-07 09:55:06.377520: step 27340, loss = 0.75, batch loss = 0.68 (5.5 examples/sec; 1.453 sec/batch; 123h:09m:52s remains)
INFO - root - 2017-12-07 09:55:21.371467: step 27350, loss = 0.79, batch loss = 0.72 (5.5 examples/sec; 1.455 sec/batch; 123h:22m:03s remains)
INFO - root - 2017-12-07 09:55:36.170631: step 27360, loss = 0.66, batch loss = 0.59 (5.6 examples/sec; 1.426 sec/batch; 120h:52m:44s remains)
INFO - root - 2017-12-07 09:55:51.131241: step 27370, loss = 0.75, batch loss = 0.68 (5.3 examples/sec; 1.503 sec/batch; 127h:22m:18s remains)
INFO - root - 2017-12-07 09:56:06.082768: step 27380, loss = 0.74, batch loss = 0.67 (5.3 examples/sec; 1.507 sec/batch; 127h:45m:08s remains)
INFO - root - 2017-12-07 09:56:20.786794: step 27390, loss = 0.64, batch loss = 0.57 (5.6 examples/sec; 1.438 sec/batch; 121h:53m:20s remains)
INFO - root - 2017-12-07 09:56:35.732525: step 27400, loss = 0.79, batch loss = 0.71 (5.5 examples/sec; 1.455 sec/batch; 123h:20m:03s remains)
2017-12-07 09:56:36.811239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.0945 -4.9894342 -2.9267967 0.17240191 1.8598123 1.4129872 0.25551271 -1.45628 -3.2491925 -4.1251297 -3.9126303 -2.1274786 0.29189634 0.998549 0.046103477][-5.0123119 -4.9550705 -3.1534684 -0.4904089 0.89883423 0.51904964 -0.33609676 -1.596962 -3.045279 -3.797276 -3.5800529 -2.0390446 0.011310101 0.71889114 0.038474083][-4.7202258 -4.8710279 -3.4718912 -1.3406003 -0.26153326 -0.49622869 -0.83804321 -1.3896248 -2.3047061 -2.8662434 -2.8011651 -1.8210568 -0.4592433 0.012038708 -0.4789722][-4.4943194 -4.7741694 -3.6127617 -1.7958024 -0.86006904 -0.93638062 -0.80707622 -0.79433417 -1.4478366 -2.0163093 -2.2272904 -1.8962486 -1.3111989 -1.153445 -1.4325881][-4.4633522 -4.6084623 -3.41284 -1.743782 -0.95205712 -1.000994 -0.61193109 -0.37292194 -1.0766506 -1.7697232 -2.2141786 -2.3503392 -2.2335875 -2.2056038 -2.2740686][-4.6201673 -4.3384795 -2.9407845 -1.4157922 -0.89284682 -0.97438741 -0.35538387 -0.018518925 -0.9775691 -1.9692774 -2.6371708 -2.9157653 -2.7767386 -2.5799363 -2.457705][-4.6884656 -3.8490965 -2.2523475 -0.9285233 -0.63711405 -0.56160235 0.42768145 0.77543926 -0.70006704 -2.2870393 -3.3174963 -3.600467 -3.1984568 -2.7102528 -2.5465407][-4.4072189 -3.029604 -1.4243259 -0.40621281 -0.26836634 0.077221394 1.4367929 1.7069616 -0.32139158 -2.5050189 -3.8594623 -4.1639237 -3.58567 -2.9783998 -3.0426121][-3.7366579 -2.0739951 -0.78623223 -0.30148745 -0.418859 0.043626785 1.4706225 1.5766749 -0.61611247 -2.8680618 -4.1090226 -4.2535062 -3.5175548 -2.9330487 -3.3589325][-2.8150282 -1.1953528 -0.50649881 -0.74424338 -1.2294328 -0.8549602 0.37856531 0.42321539 -1.4239056 -3.2261543 -3.996841 -3.8328066 -3.0459547 -2.6865892 -3.4250159][-2.0403924 -0.69920945 -0.65643311 -1.51213 -2.2687333 -2.0016327 -1.0185163 -0.91891909 -2.1369321 -3.251555 -3.4844656 -3.0757556 -2.4117904 -2.4544783 -3.4410098][-1.6320333 -0.78335118 -1.2672942 -2.3192813 -3.0388074 -2.8692529 -2.1777868 -2.0224733 -2.5944247 -3.0561028 -2.9042532 -2.3312478 -1.8484313 -2.2445822 -3.3162889][-1.4012032 -1.1952262 -2.0473578 -2.9311106 -3.3735142 -3.3085902 -2.9230571 -2.7499769 -2.82645 -2.8434534 -2.5349789 -1.9662411 -1.7071028 -2.2801096 -3.2022214][-1.4030685 -1.8769603 -2.9198914 -3.4605818 -3.563746 -3.5708406 -3.4025002 -3.2500505 -3.1142852 -2.9680166 -2.6945639 -2.24044 -2.1205842 -2.5709198 -3.1432414][-1.5315235 -2.4929519 -3.6336496 -3.9257407 -3.7856932 -3.7135072 -3.5957992 -3.4813912 -3.3548927 -3.26823 -3.1107116 -2.7399497 -2.5512593 -2.6551719 -2.8627739]]...]
INFO - root - 2017-12-07 09:56:51.630581: step 27410, loss = 0.73, batch loss = 0.66 (5.2 examples/sec; 1.553 sec/batch; 131h:37m:02s remains)
INFO - root - 2017-12-07 09:57:06.516179: step 27420, loss = 0.65, batch loss = 0.57 (5.5 examples/sec; 1.444 sec/batch; 122h:20m:25s remains)
INFO - root - 2017-12-07 09:57:21.168262: step 27430, loss = 0.74, batch loss = 0.67 (5.7 examples/sec; 1.402 sec/batch; 118h:50m:13s remains)
INFO - root - 2017-12-07 09:57:36.053178: step 27440, loss = 0.93, batch loss = 0.86 (5.5 examples/sec; 1.461 sec/batch; 123h:49m:38s remains)
INFO - root - 2017-12-07 09:57:50.921306: step 27450, loss = 0.64, batch loss = 0.57 (5.2 examples/sec; 1.525 sec/batch; 129h:13m:36s remains)
INFO - root - 2017-12-07 09:58:05.841819: step 27460, loss = 0.62, batch loss = 0.54 (5.0 examples/sec; 1.585 sec/batch; 134h:15m:42s remains)
INFO - root - 2017-12-07 09:58:20.719635: step 27470, loss = 0.65, batch loss = 0.58 (5.2 examples/sec; 1.540 sec/batch; 130h:27m:42s remains)
INFO - root - 2017-12-07 09:58:35.587144: step 27480, loss = 0.77, batch loss = 0.70 (5.5 examples/sec; 1.448 sec/batch; 122h:41m:30s remains)
INFO - root - 2017-12-07 09:58:50.421916: step 27490, loss = 0.66, batch loss = 0.59 (5.4 examples/sec; 1.485 sec/batch; 125h:47m:30s remains)
INFO - root - 2017-12-07 09:59:05.505913: step 27500, loss = 0.75, batch loss = 0.68 (5.2 examples/sec; 1.548 sec/batch; 131h:10m:32s remains)
2017-12-07 09:59:06.641848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3385489 -2.371594 -2.3719931 -2.2760377 -2.2390203 -2.3623552 -2.6314621 -2.9935603 -3.3219702 -3.4871998 -3.46314 -3.2391596 -2.8354349 -2.5951672 -2.7289572][-2.5769649 -2.5848932 -2.5570827 -2.3740547 -2.1654632 -2.13607 -2.3810871 -2.8225605 -3.3127205 -3.5996923 -3.5216861 -3.1853533 -2.7687914 -2.5112405 -2.556808][-2.9198751 -2.8538268 -2.7762997 -2.5669508 -2.2037647 -1.9081419 -2.0338302 -2.5216742 -3.154 -3.5321808 -3.4213381 -3.0792489 -2.8030684 -2.6155589 -2.5277762][-3.0100632 -2.8756466 -2.7263651 -2.5123024 -2.0660369 -1.541471 -1.5009842 -2.0372355 -2.8212085 -3.3079662 -3.2785406 -3.0591636 -2.9752727 -2.8289742 -2.6143723][-2.8543591 -2.7574496 -2.599556 -2.3786473 -1.9248655 -1.294239 -0.9677484 -1.3028574 -2.0685797 -2.6936359 -2.9533968 -3.0259969 -3.1139925 -2.9535336 -2.63103][-2.5562057 -2.5264735 -2.4126544 -2.1582334 -1.7310319 -1.1574435 -0.55868292 -0.43878579 -1.0145104 -1.8439577 -2.5191183 -2.89994 -3.0758381 -2.9388676 -2.6277404][-2.4513698 -2.4474921 -2.3753276 -2.1243289 -1.7662854 -1.3035195 -0.52258158 0.057621956 -0.27407646 -1.2996979 -2.2683997 -2.7744541 -2.9376874 -2.8563886 -2.6189427][-2.8386223 -2.7280211 -2.6498408 -2.4881649 -2.269475 -1.9413919 -1.161721 -0.3681283 -0.47028637 -1.4882107 -2.4985194 -2.945817 -3.0292118 -2.946384 -2.7222972][-3.4750073 -3.2297554 -3.1145625 -3.0542591 -2.9913855 -2.7812784 -2.1802657 -1.46403 -1.3901768 -2.1786819 -3.0136938 -3.3038423 -3.2334661 -3.0165374 -2.6886954][-3.8814719 -3.6478868 -3.58959 -3.5993774 -3.584173 -3.4126835 -2.9520717 -2.3504403 -2.1372762 -2.61489 -3.2299623 -3.3918772 -3.1522737 -2.7639449 -2.3252184][-3.7749763 -3.610168 -3.6129014 -3.6114588 -3.5714331 -3.4094758 -3.0414848 -2.5042968 -2.1637537 -2.3449609 -2.7454758 -2.8393092 -2.507056 -2.0470793 -1.629781][-3.1142149 -3.0089273 -3.0266998 -2.9928486 -2.9276762 -2.8115635 -2.5337749 -2.0641873 -1.659595 -1.5951047 -1.7593532 -1.7606874 -1.4358561 -1.0525389 -0.79453993][-2.1249802 -2.0706532 -2.085479 -2.0630007 -2.021343 -1.9553788 -1.7541049 -1.3859863 -1.0206966 -0.85419512 -0.86948657 -0.83399558 -0.61211276 -0.37792635 -0.26301718][-1.0335741 -1.0088024 -1.0129302 -1.0382972 -1.0588162 -1.0390007 -0.90992355 -0.66070175 -0.41111755 -0.26996946 -0.24484873 -0.23662233 -0.1518693 -0.050561905 -0.0069885254][-0.2455616 -0.24259901 -0.23968744 -0.28377676 -0.33798027 -0.34312534 -0.26407051 -0.12316704 0.0026688576 0.079070091 0.10194016 0.096151829 0.10275507 0.12464952 0.13606119]]...]
INFO - root - 2017-12-07 09:59:21.262631: step 27510, loss = 0.66, batch loss = 0.58 (5.5 examples/sec; 1.446 sec/batch; 122h:28m:04s remains)
INFO - root - 2017-12-07 09:59:36.004084: step 27520, loss = 0.82, batch loss = 0.75 (5.4 examples/sec; 1.468 sec/batch; 124h:23m:33s remains)
INFO - root - 2017-12-07 09:59:50.868989: step 27530, loss = 1.06, batch loss = 0.98 (5.5 examples/sec; 1.451 sec/batch; 122h:55m:33s remains)
INFO - root - 2017-12-07 10:00:05.839475: step 27540, loss = 0.88, batch loss = 0.81 (5.1 examples/sec; 1.562 sec/batch; 132h:16m:57s remains)
INFO - root - 2017-12-07 10:00:20.659674: step 27550, loss = 0.63, batch loss = 0.55 (5.3 examples/sec; 1.509 sec/batch; 127h:51m:22s remains)
INFO - root - 2017-12-07 10:00:35.467710: step 27560, loss = 0.66, batch loss = 0.59 (5.7 examples/sec; 1.401 sec/batch; 118h:38m:31s remains)
INFO - root - 2017-12-07 10:00:50.220688: step 27570, loss = 0.84, batch loss = 0.77 (5.8 examples/sec; 1.388 sec/batch; 117h:36m:33s remains)
INFO - root - 2017-12-07 10:01:04.842487: step 27580, loss = 0.89, batch loss = 0.81 (5.5 examples/sec; 1.458 sec/batch; 123h:27m:53s remains)
INFO - root - 2017-12-07 10:01:19.795412: step 27590, loss = 0.79, batch loss = 0.72 (5.3 examples/sec; 1.523 sec/batch; 129h:00m:05s remains)
INFO - root - 2017-12-07 10:01:34.726304: step 27600, loss = 0.90, batch loss = 0.83 (5.4 examples/sec; 1.471 sec/batch; 124h:37m:20s remains)
2017-12-07 10:01:35.838519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1344213 -2.261759 -2.356354 -2.3653924 -2.1790764 -1.7918103 -1.4002137 -1.236172 -1.2225721 -1.4561596 -2.0138857 -2.5589414 -2.6137626 -2.1757963 -1.5969193][-2.0250502 -2.0783968 -2.0735464 -2.0561733 -1.9561365 -1.5190673 -1.0335672 -0.86194372 -0.75360322 -0.83464265 -1.3192878 -1.8553634 -2.0097849 -1.7878954 -1.4162591][-2.0324612 -1.8416638 -1.6822796 -1.6269162 -1.5822759 -1.1693072 -0.7291553 -0.620193 -0.50338292 -0.428545 -0.70419693 -1.1333752 -1.2870691 -1.2538333 -1.2106125][-2.4410868 -1.9420099 -1.557194 -1.2753675 -1.0905335 -0.68031192 -0.3131361 -0.26485062 -0.27307129 -0.12407064 -0.16766214 -0.49959326 -0.59524727 -0.6860826 -0.98116851][-3.4229217 -2.7953634 -2.0461309 -1.2413073 -0.62238 -0.059440136 0.36527061 0.36301851 0.11027718 0.16911125 0.22986507 -0.069526672 -0.0676589 -0.20094395 -0.79783463][-4.4148884 -3.8794236 -2.8947244 -1.6327467 -0.47665405 0.47360945 1.1476989 1.1512618 0.6060915 0.41933489 0.39876795 0.073717594 0.18707705 0.12102509 -0.62156773][-4.7656941 -4.3638887 -3.4400024 -2.1545696 -0.74954748 0.559875 1.5679331 1.6714587 0.93503714 0.50219154 0.32680988 -0.058257103 0.12559366 0.18791676 -0.53871155][-4.5980992 -4.2295089 -3.5089726 -2.5096664 -1.2503176 0.011455536 1.0093913 1.2465792 0.66647148 0.2227459 -0.051249027 -0.41054535 -0.083828926 0.20128584 -0.38022137][-4.0350442 -3.6521776 -3.1542919 -2.4613209 -1.5461869 -0.71039033 0.019794941 0.38932943 0.12374878 -0.16806316 -0.48179746 -0.77801228 -0.27693129 0.30730438 -0.028935432][-3.3769419 -2.9823837 -2.6553624 -2.2033591 -1.6723197 -1.3214431 -0.86474133 -0.3840065 -0.36412048 -0.54241347 -0.91344142 -1.1890988 -0.60218692 0.24917555 0.25038815][-2.7324882 -2.3205085 -2.1542969 -1.9781866 -1.8092263 -1.8470681 -1.6056516 -1.0484071 -0.77996397 -0.84957647 -1.2605944 -1.6190877 -1.1505527 -0.19599628 0.15035534][-2.2931483 -1.851362 -1.7727497 -1.8008144 -1.8600559 -2.1245353 -2.108603 -1.6299591 -1.238344 -1.1639333 -1.4973524 -1.9502835 -1.7466836 -0.94519377 -0.44108963][-2.1219554 -1.720901 -1.6752436 -1.7685845 -1.8960564 -2.2129164 -2.4274561 -2.1947677 -1.852612 -1.6837327 -1.8374977 -2.1879089 -2.0961945 -1.5341644 -1.143733][-2.0829127 -1.7978342 -1.7854753 -1.8921652 -1.9981523 -2.2183239 -2.4791651 -2.441946 -2.23535 -2.0897214 -2.1210339 -2.2901728 -2.1515913 -1.7832625 -1.6265378][-2.031642 -1.8371866 -1.8463581 -1.9616516 -2.0656765 -2.1777313 -2.3274467 -2.3566828 -2.2844546 -2.2430971 -2.2754049 -2.3476036 -2.1814651 -1.9485793 -1.9415705]]...]
INFO - root - 2017-12-07 10:01:50.656376: step 27610, loss = 0.65, batch loss = 0.58 (5.4 examples/sec; 1.483 sec/batch; 125h:36m:02s remains)
INFO - root - 2017-12-07 10:02:05.463088: step 27620, loss = 0.85, batch loss = 0.78 (5.4 examples/sec; 1.490 sec/batch; 126h:11m:59s remains)
INFO - root - 2017-12-07 10:02:20.429853: step 27630, loss = 0.86, batch loss = 0.79 (5.3 examples/sec; 1.522 sec/batch; 128h:55m:35s remains)
INFO - root - 2017-12-07 10:02:35.110154: step 27640, loss = 0.73, batch loss = 0.65 (5.3 examples/sec; 1.505 sec/batch; 127h:28m:39s remains)
INFO - root - 2017-12-07 10:02:50.149823: step 27650, loss = 0.63, batch loss = 0.56 (5.1 examples/sec; 1.582 sec/batch; 133h:56m:33s remains)
INFO - root - 2017-12-07 10:03:05.137007: step 27660, loss = 0.71, batch loss = 0.64 (5.0 examples/sec; 1.598 sec/batch; 135h:18m:37s remains)
INFO - root - 2017-12-07 10:03:19.752563: step 27670, loss = 0.88, batch loss = 0.80 (5.3 examples/sec; 1.520 sec/batch; 128h:42m:01s remains)
INFO - root - 2017-12-07 10:03:34.738469: step 27680, loss = 0.81, batch loss = 0.73 (5.4 examples/sec; 1.490 sec/batch; 126h:07m:18s remains)
INFO - root - 2017-12-07 10:03:49.795017: step 27690, loss = 0.69, batch loss = 0.62 (5.5 examples/sec; 1.455 sec/batch; 123h:11m:57s remains)
INFO - root - 2017-12-07 10:04:04.600018: step 27700, loss = 0.76, batch loss = 0.68 (5.3 examples/sec; 1.519 sec/batch; 128h:38m:22s remains)
2017-12-07 10:04:05.708017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1961911 -3.2036729 -3.2537293 -3.2992506 -3.3196406 -3.3137188 -3.2871687 -3.2437022 -3.1902604 -3.1684523 -3.2024422 -3.2723699 -3.2605562 -3.0989218 -2.8475888][-3.4159479 -3.4657075 -3.5480757 -3.6049056 -3.6164403 -3.5983992 -3.5647764 -3.5198553 -3.4579287 -3.4203267 -3.4404142 -3.5184762 -3.4981971 -3.2868209 -2.9567454][-3.4670067 -3.5688007 -3.6805372 -3.7671914 -3.7902341 -3.7687616 -3.7280085 -3.6860776 -3.6408455 -3.6089625 -3.6158471 -3.6837351 -3.6358232 -3.3946776 -3.0200517][-2.7799666 -2.866375 -3.0105994 -3.2109685 -3.335464 -3.3844404 -3.404644 -3.4049451 -3.4013729 -3.3605714 -3.3091152 -3.3345375 -3.2766929 -3.0857027 -2.7692137][-2.2261145 -2.179426 -2.1816645 -2.305805 -2.3955712 -2.4724872 -2.6099153 -2.7765338 -3.00686 -3.1150827 -3.0633287 -3.0427389 -2.9860849 -2.9113684 -2.7393248][-2.2105668 -2.1979387 -2.1478326 -2.1760836 -2.1623812 -2.1551163 -2.1992831 -2.272517 -2.516151 -2.6701438 -2.6218348 -2.5876389 -2.5781038 -2.6971021 -2.7660897][-2.0511951 -2.0206978 -1.9801893 -2.0436285 -2.0972898 -2.1679573 -2.175035 -2.0860684 -2.1565247 -2.1951292 -2.1215174 -2.1370347 -2.2290189 -2.5142269 -2.7477887][-1.7933943 -1.6607845 -1.4889979 -1.388401 -1.3265719 -1.3703177 -1.3577199 -1.2880056 -1.422646 -1.5941534 -1.6844971 -1.8297224 -2.0087214 -2.3520997 -2.6353319][-1.511184 -1.5238593 -1.4450862 -1.2982967 -1.1014886 -0.94199824 -0.69813538 -0.4879899 -0.56730151 -0.77270246 -0.93589973 -1.1056893 -1.2541194 -1.5518172 -1.870496][-1.8669372 -2.0145979 -2.1141372 -2.1516085 -2.1276181 -2.0609598 -1.8320708 -1.5505559 -1.4291992 -1.3771899 -1.3335392 -1.3365271 -1.3411727 -1.4796894 -1.6928203][-2.5527539 -2.6665773 -2.7387366 -2.8196731 -2.9294071 -3.0668244 -3.0996222 -3.0504494 -3.0064588 -2.9199367 -2.7979009 -2.6950755 -2.6169086 -2.6105921 -2.6229892][-2.6780744 -2.820034 -2.9166613 -3.0170636 -3.1369233 -3.2984362 -3.3906837 -3.4137473 -3.4210534 -3.4028082 -3.3515594 -3.2775834 -3.2119057 -3.1485786 -3.027838][-2.8537912 -3.0363421 -3.1669779 -3.2767358 -3.3710952 -3.4917994 -3.5527639 -3.5320268 -3.4915798 -3.4408679 -3.3797941 -3.3101833 -3.2812483 -3.2143373 -3.0156376][-3.0067456 -3.1508338 -3.2253888 -3.2624354 -3.2800655 -3.3358426 -3.3823757 -3.3827515 -3.3716178 -3.3423538 -3.2969725 -3.2457325 -3.2504921 -3.2045302 -3.0052869][-2.7020445 -2.8079267 -2.8507314 -2.8448238 -2.805723 -2.7806504 -2.7668476 -2.7560205 -2.7776499 -2.81281 -2.8390503 -2.8452458 -2.8646026 -2.8287702 -2.6785324]]...]
INFO - root - 2017-12-07 10:04:20.653002: step 27710, loss = 0.64, batch loss = 0.57 (5.5 examples/sec; 1.445 sec/batch; 122h:22m:10s remains)
INFO - root - 2017-12-07 10:04:35.519680: step 27720, loss = 0.83, batch loss = 0.76 (5.6 examples/sec; 1.427 sec/batch; 120h:50m:38s remains)
INFO - root - 2017-12-07 10:04:50.612302: step 27730, loss = 0.65, batch loss = 0.58 (5.4 examples/sec; 1.474 sec/batch; 124h:47m:34s remains)
INFO - root - 2017-12-07 10:05:05.579860: step 27740, loss = 0.69, batch loss = 0.62 (5.0 examples/sec; 1.585 sec/batch; 134h:09m:29s remains)
INFO - root - 2017-12-07 10:05:20.420182: step 27750, loss = 0.76, batch loss = 0.69 (5.3 examples/sec; 1.499 sec/batch; 126h:51m:53s remains)
INFO - root - 2017-12-07 10:05:35.300477: step 27760, loss = 0.74, batch loss = 0.67 (5.6 examples/sec; 1.418 sec/batch; 120h:00m:50s remains)
INFO - root - 2017-12-07 10:05:50.141848: step 27770, loss = 0.70, batch loss = 0.63 (5.3 examples/sec; 1.507 sec/batch; 127h:36m:07s remains)
INFO - root - 2017-12-07 10:06:05.138568: step 27780, loss = 0.76, batch loss = 0.68 (5.2 examples/sec; 1.553 sec/batch; 131h:28m:35s remains)
INFO - root - 2017-12-07 10:06:20.228437: step 27790, loss = 0.79, batch loss = 0.71 (4.9 examples/sec; 1.642 sec/batch; 138h:57m:17s remains)
INFO - root - 2017-12-07 10:06:35.051342: step 27800, loss = 0.75, batch loss = 0.68 (5.3 examples/sec; 1.499 sec/batch; 126h:52m:30s remains)
2017-12-07 10:06:36.112982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.389334 -2.4167638 -2.4394658 -2.4603984 -2.4778676 -2.5126619 -2.584317 -2.6650414 -2.7077107 -2.7211528 -2.7169003 -2.6812425 -2.6091738 -2.5115325 -2.441812][-2.4145577 -2.4458146 -2.4856002 -2.5473971 -2.6025639 -2.6484158 -2.7244451 -2.8008802 -2.8189545 -2.810987 -2.7930207 -2.7285354 -2.6329765 -2.5278435 -2.4649355][-2.4918242 -2.5255814 -2.5859733 -2.6842513 -2.7338538 -2.7106547 -2.6977239 -2.6958954 -2.6536984 -2.6233954 -2.63523 -2.6057687 -2.5585802 -2.5040517 -2.4722011][-2.5740967 -2.5963426 -2.6613078 -2.7489 -2.7105141 -2.5480442 -2.4062524 -2.3300056 -2.2558413 -2.2538483 -2.3578291 -2.428467 -2.4685874 -2.469285 -2.4485328][-2.5457683 -2.4974606 -2.4983549 -2.4764843 -2.2556081 -1.9531531 -1.7773986 -1.7568538 -1.780359 -1.9245067 -2.2170584 -2.441761 -2.5385289 -2.5253007 -2.4469008][-2.3380959 -2.1378686 -1.9895704 -1.7384489 -1.2620647 -0.87390542 -0.81509781 -0.99656439 -1.2521586 -1.6439762 -2.1626437 -2.5249639 -2.6328096 -2.5780392 -2.4497406][-2.0093307 -1.6430061 -1.3509762 -0.8871274 -0.24003124 0.10081768 -0.044540405 -0.41839027 -0.87344575 -1.4477477 -2.0718875 -2.4689631 -2.5656533 -2.5240679 -2.4299335][-1.7189367 -1.3394818 -1.1080437 -0.74535918 -0.29656267 -0.21527863 -0.47488785 -0.82178926 -1.2529039 -1.7669284 -2.2436404 -2.510922 -2.5601344 -2.5314283 -2.4760127][-1.701097 -1.5337775 -1.5997405 -1.6231027 -1.5928497 -1.7387106 -1.9294138 -2.09316 -2.3491123 -2.6236358 -2.8116717 -2.8829136 -2.8407447 -2.7422919 -2.6487811][-1.9907441 -2.0713282 -2.389617 -2.700768 -2.9230728 -3.1207266 -3.2074895 -3.2642131 -3.393012 -3.4702489 -3.4408474 -3.3705177 -3.2274036 -3.0210409 -2.8572893][-2.3381596 -2.5422087 -2.9083455 -3.2488089 -3.4848495 -3.630322 -3.6628766 -3.6742551 -3.710696 -3.690228 -3.6069465 -3.50801 -3.3208258 -3.0801716 -2.9096041][-2.5304661 -2.7248292 -2.9946356 -3.221288 -3.3775394 -3.4727168 -3.5045748 -3.5191355 -3.5117714 -3.4900358 -3.4382827 -3.35155 -3.1698234 -2.9607577 -2.8284507][-2.5565405 -2.6893651 -2.8527319 -2.9875474 -3.1038823 -3.2058153 -3.2793968 -3.3242142 -3.3203595 -3.3031044 -3.2482538 -3.1391339 -2.968297 -2.8010957 -2.6988559][-2.5193548 -2.6112356 -2.7232494 -2.8296456 -2.9346097 -3.027257 -3.0953784 -3.1279578 -3.1041121 -3.0558438 -2.978323 -2.8659773 -2.7337499 -2.6224914 -2.5621364][-2.4863355 -2.549283 -2.6249948 -2.6984832 -2.7614412 -2.7999263 -2.81888 -2.81485 -2.7724314 -2.7114346 -2.6419516 -2.5662022 -2.5018594 -2.4628339 -2.4551673]]...]
INFO - root - 2017-12-07 10:06:51.124301: step 27810, loss = 0.72, batch loss = 0.64 (5.7 examples/sec; 1.410 sec/batch; 119h:21m:59s remains)
INFO - root - 2017-12-07 10:07:06.184365: step 27820, loss = 0.84, batch loss = 0.77 (5.1 examples/sec; 1.556 sec/batch; 131h:41m:58s remains)
INFO - root - 2017-12-07 10:07:21.113487: step 27830, loss = 0.67, batch loss = 0.60 (5.1 examples/sec; 1.576 sec/batch; 133h:25m:10s remains)
INFO - root - 2017-12-07 10:07:36.032406: step 27840, loss = 0.78, batch loss = 0.71 (5.6 examples/sec; 1.432 sec/batch; 121h:11m:01s remains)
INFO - root - 2017-12-07 10:07:51.108387: step 27850, loss = 0.92, batch loss = 0.85 (5.4 examples/sec; 1.484 sec/batch; 125h:33m:14s remains)
INFO - root - 2017-12-07 10:08:06.045133: step 27860, loss = 0.86, batch loss = 0.79 (5.5 examples/sec; 1.448 sec/batch; 122h:31m:53s remains)
