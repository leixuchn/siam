INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "296"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
inputs Tensor("batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
inputs Tensor("batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-16 07:45:51.352269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:45:51.352302: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:45:51.352308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:45:51.352313: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:45:51.352317: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 07:45:52.375445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 07:45:52.375487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 07:45:52.375494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 07:45:52.375502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 07:45:57.369344: step 0, loss = 0.25, batch loss = 0.16 (2.9 examples/sec; 2.720 sec/batch; 251h:11m:49s remains)
inputs Tensor("batch_1:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_2/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
inputs Tensor("batch_1:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("siamese_fc_3/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
Tensor("detection_1/add:0", shape=(8, 15, 15), dtype=float32)
[u'siamese_fc/conv1/weights:0',
 u'siamese_fc/conv1/BatchNorm/beta:0',
 u'siamese_fc/conv1/BatchNorm/gamma:0',
 u'siamese_fc/conv1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b1/weights:0',
 u'siamese_fc/conv2/b1/BatchNorm/beta:0',
 u'siamese_fc/conv2/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv2/b2/weights:0',
 u'siamese_fc/conv2/b2/BatchNorm/beta:0',
 u'siamese_fc/conv2/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv2/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv3/weights:0',
 u'siamese_fc/conv3/BatchNorm/beta:0',
 u'siamese_fc/conv3/BatchNorm/gamma:0',
 u'siamese_fc/conv3/BatchNorm/moving_mean:0',
 u'siamese_fc/conv3/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b1/weights:0',
 u'siamese_fc/conv4/b1/BatchNorm/beta:0',
 u'siamese_fc/conv4/b1/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b1/BatchNorm/moving_variance:0',
 u'siamese_fc/conv4/b2/weights:0',
 u'siamese_fc/conv4/b2/BatchNorm/beta:0',
 u'siamese_fc/conv4/b2/BatchNorm/gamma:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_mean:0',
 u'siamese_fc/conv4/b2/BatchNorm/moving_variance:0',
 u'siamese_fc/conv5/b1/weights:0',
 u'siamese_fc/conv5/b1/biases:0',
 u'siamese_fc/conv5/b2/weights:0',
 u'siamese_fc/conv5/b2/biases:0',
 u'detection/biases:0',
 u'global_step:0',
 u'OptimizeLoss/siamese_fc/conv1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv2/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv3/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b1/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/beta/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv4/b2/BatchNorm/gamma/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b1/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b1/biases/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b2/weights/Momentum:0',
 u'OptimizeLoss/siamese_fc/conv5/b2/biases/Momentum:0',
 u'OptimizeLoss/detection/biases/Momentum:0']
2017-12-16 07:45:58.219436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.1044183 -2.2278945 -2.4850826 -2.502851 -2.6099062 -2.8659377 -2.8147981 -2.3079941 -1.5425675 -0.45301771 0.71310568 1.1984344 0.16158676 -1.7105749 -3.4023662][-2.8241515 -2.9237099 -3.278203 -3.346302 -3.3496659 -3.5143037 -3.5034742 -3.2519965 -2.8715625 -2.2202985 -1.3873475 -0.95263696 -1.6185188 -2.9159868 -4.003581][-3.0883727 -3.0606375 -3.3096924 -3.2555392 -3.0732942 -3.1213439 -3.1899452 -3.277837 -3.3896656 -3.2851722 -2.9206572 -2.7023549 -3.1118646 -3.8886 -4.3016315][-3.1008985 -2.8332496 -2.8203197 -2.5138528 -2.0458751 -1.8806267 -1.9381683 -2.2934868 -2.8574996 -3.2236023 -3.270077 -3.3023415 -3.6637704 -4.1870737 -4.0586462][-2.4979031 -1.9441633 -1.5120857 -0.82204795 -0.067089081 0.17974377 -0.059920788 -0.8055203 -1.8244109 -2.5041547 -2.6733761 -2.6921177 -2.9756007 -3.4165263 -2.9415312][-1.3324099 -0.50047922 0.43998623 1.5464854 2.4550734 2.4887705 1.7766666 0.59314203 -0.74171662 -1.5542321 -1.6597056 -1.5527894 -1.8320367 -2.3684728 -1.8391693][-0.48602462 0.5052352 1.8927784 3.2437396 4.0343676 3.669548 2.5103889 1.1327333 -0.23806715 -1.0483131 -1.1535726 -1.1116009 -1.5897868 -2.3764818 -2.1031728][-1.0324442 -0.19947481 1.3454599 2.7261295 3.4070029 2.9356942 1.7426915 0.50312471 -0.65455866 -1.3666079 -1.5175946 -1.6064148 -2.2330494 -3.1572251 -3.1635568][-2.4866791 -1.98557 -0.65589285 0.55568266 1.2621756 1.0770264 0.20717287 -0.6789701 -1.429132 -1.8353999 -1.8654308 -1.9280043 -2.5113578 -3.4132795 -3.6965961][-3.3760834 -3.0327768 -2.0992603 -1.2831907 -0.74706745 -0.78491926 -1.2963634 -1.657032 -1.7643816 -1.701077 -1.5811503 -1.6605349 -2.2152798 -3.0194678 -3.5000432][-3.2982154 -3.0590448 -2.5675948 -2.1829226 -1.9401662 -2.0426311 -2.3121595 -2.2034533 -1.7797914 -1.4279196 -1.3501678 -1.6101952 -2.2076805 -2.8716583 -3.3509891][-2.9167762 -2.7093005 -2.4301205 -2.2156827 -2.1090782 -2.2299919 -2.3395329 -1.9788225 -1.3874137 -1.1010339 -1.3015685 -1.8981125 -2.6453753 -3.251698 -3.637938][-2.6866245 -2.3219857 -1.9740853 -1.7482209 -1.6802535 -1.7402472 -1.705512 -1.2429514 -0.752728 -0.77210021 -1.3547175 -2.2460494 -3.0512185 -3.528456 -3.7189434][-2.5217409 -2.0563552 -1.728555 -1.6277092 -1.6203411 -1.5350015 -1.2539949 -0.70449281 -0.4113512 -0.82223368 -1.7402174 -2.6818461 -3.2517509 -3.3543777 -3.2158122][-2.2221239 -1.8491311 -1.729454 -1.9014769 -2.0392678 -1.8493578 -1.3370941 -0.70534086 -0.535053 -1.1658914 -2.1799462 -2.9596586 -3.1922598 -2.9323134 -2.5450907]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 07:46:00.680225: step 10, loss = 0.31, batch loss = 0.23 (49.3 examples/sec; 0.162 sec/batch; 14h:58m:38s remains)
INFO - root - 2017-12-16 07:46:02.310747: step 20, loss = 0.25, batch loss = 0.17 (48.4 examples/sec; 0.165 sec/batch; 15h:16m:12s remains)
INFO - root - 2017-12-16 07:46:03.931090: step 30, loss = 0.26, batch loss = 0.17 (49.2 examples/sec; 0.163 sec/batch; 15h:01m:33s remains)
INFO - root - 2017-12-16 07:46:05.583856: step 40, loss = 0.33, batch loss = 0.25 (47.4 examples/sec; 0.169 sec/batch; 15h:35m:26s remains)
INFO - root - 2017-12-16 07:46:07.260855: step 50, loss = 0.31, batch loss = 0.22 (45.3 examples/sec; 0.177 sec/batch; 16h:18m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-testbug
INFO - root - 2017-12-16 07:46:08.953032: step 60, loss = 0.61, batch loss = 0.53 (46.8 examples/sec; 0.171 sec/batch; 15h:46m:37s remains)
INFO - root - 2017-12-16 07:46:10.602918: step 70, loss = 0.24, batch loss = 0.16 (49.0 examples/sec; 0.163 sec/batch; 15h:04m:20s remains)
INFO - root - 2017-12-16 07:46:12.265043: step 80, loss = 0.34, batch loss = 0.26 (44.1 examples/sec; 0.181 sec/batch; 16h:44m:21s remains)
INFO - root - 2017-12-16 07:46:13.941193: step 90, loss = 0.34, batch loss = 0.26 (49.6 examples/sec; 0.161 sec/batch; 14h:52m:52s remains)
INFO - root - 2017-12-16 07:46:15.602208: step 100, loss = 0.24, batch loss = 0.16 (46.5 examples/sec; 0.172 sec/batch; 15h:54m:08s remains)
2017-12-16 07:46:16.136187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.3507118 -5.3652415 -5.1167173 -4.7977171 -4.5851 -3.9691837 -3.1187243 -2.5201554 -2.3829217 -3.0318923 -4.1547246 -5.0540223 -5.5615716 -5.9816451 -6.0332913][-5.3221755 -5.2655382 -4.9475522 -4.6318469 -4.4762931 -3.9305596 -3.194417 -2.7379627 -2.5740814 -3.1151221 -4.2720017 -5.1661048 -5.3958216 -5.4777713 -5.6197467][-4.742857 -4.6578007 -4.290463 -4.0280218 -3.9825978 -3.5483413 -2.9636927 -2.6625628 -2.4987483 -2.8892505 -3.9268079 -4.7564692 -4.730166 -4.4262381 -4.45588][-3.7492712 -3.5127597 -2.9165688 -2.5412586 -2.5328743 -2.2090003 -1.781213 -1.6415668 -1.5973697 -1.9567595 -2.8591831 -3.6726322 -3.582068 -3.065351 -2.9539664][-2.8046012 -2.4257753 -1.532321 -0.821383 -0.5659616 -0.09581089 0.39589643 0.48540592 0.27478218 -0.29618788 -1.2141311 -2.1241426 -2.2336307 -1.8950193 -1.9082308][-2.1604915 -1.7928438 -0.71098351 0.42328358 1.1819768 2.1189981 3.0070772 3.2681894 2.7840366 1.7976108 0.68867588 -0.37653017 -0.82321024 -0.90103936 -1.2414298][-1.6636658 -1.2241042 -0.012009144 1.4508924 2.6971445 4.1292305 5.4810982 6.0226517 5.3949022 3.9932613 2.5468516 1.1886983 0.2921505 -0.27731705 -0.98853087][-1.7389777 -1.2393532 -0.074488163 1.3516288 2.6674676 4.15201 5.5816021 6.2661219 5.7630215 4.4138117 2.9964995 1.5796809 0.43471956 -0.42949986 -1.3781528][-2.2418783 -1.8588879 -0.96011209 0.10293198 1.0818949 2.2174144 3.375339 4.0510368 3.8678827 3.0135927 2.0088692 0.79485178 -0.33904123 -1.211659 -2.1155853][-2.484138 -2.2968237 -1.7555878 -1.1489363 -0.61291957 0.10185719 0.94294643 1.5318952 1.5732527 1.1419835 0.53495026 -0.39105129 -1.341269 -2.0448294 -2.7109194][-2.8576734 -2.9305515 -2.7594285 -2.540895 -2.3446145 -1.9314604 -1.3537643 -0.92771649 -0.85222173 -1.0528307 -1.3499899 -1.9017489 -2.469218 -2.8310833 -3.1214638][-3.2730536 -3.6002557 -3.7436059 -3.8024371 -3.8483682 -3.6908674 -3.3715563 -3.1315746 -3.1119864 -3.2182379 -3.3195839 -3.509243 -3.6386633 -3.6046081 -3.5021467][-3.3907681 -3.80663 -4.1031284 -4.2973723 -4.46279 -4.4868464 -4.390419 -4.3056121 -4.3404675 -4.4041114 -4.3877835 -4.3229985 -4.154633 -3.8963962 -3.598917][-3.4706557 -3.8081837 -4.0831084 -4.2590766 -4.3802152 -4.4159508 -4.3926859 -4.3537621 -4.38951 -4.4193268 -4.347198 -4.16631 -3.8870733 -3.616344 -3.3778133][-3.2712994 -3.463964 -3.6553631 -3.8032467 -3.86166 -3.8271837 -3.7684917 -3.7066507 -3.7159379 -3.7179663 -3.6498418 -3.5003362 -3.2761021 -3.1245389 -3.0374424]]...]
INFO - root - 2017-12-16 07:46:17.789491: step 110, loss = 0.29, batch loss = 0.21 (48.5 examples/sec; 0.165 sec/batch; 15h:13m:25s remains)
INFO - root - 2017-12-16 07:46:19.479954: step 120, loss = 0.35, batch loss = 0.26 (48.0 examples/sec; 0.167 sec/batch; 15h:22m:53s remains)
INFO - root - 2017-12-16 07:46:21.144537: step 130, loss = 0.32, batch loss = 0.24 (47.4 examples/sec; 0.169 sec/batch; 15h:34m:42s remains)
INFO - root - 2017-12-16 07:46:22.796250: step 140, loss = 0.24, batch loss = 0.16 (49.6 examples/sec; 0.161 sec/batch; 14h:53m:49s remains)
INFO - root - 2017-12-16 07:46:24.444025: step 150, loss = 0.31, batch loss = 0.23 (49.7 examples/sec; 0.161 sec/batch; 14h:52m:16s remains)
INFO - root - 2017-12-16 07:46:26.090162: step 160, loss = 0.27, batch loss = 0.18 (46.8 examples/sec; 0.171 sec/batch; 15h:47m:15s remains)
INFO - root - 2017-12-16 07:46:27.763184: step 170, loss = 0.40, batch loss = 0.32 (45.2 examples/sec; 0.177 sec/batch; 16h:19m:21s remains)
INFO - root - 2017-12-16 07:46:29.421454: step 180, loss = 0.27, batch loss = 0.19 (48.0 examples/sec; 0.167 sec/batch; 15h:23m:08s remains)
INFO - root - 2017-12-16 07:46:31.056835: step 190, loss = 0.46, batch loss = 0.37 (49.1 examples/sec; 0.163 sec/batch; 15h:02m:27s remains)
INFO - root - 2017-12-16 07:46:32.724320: step 200, loss = 0.29, batch loss = 0.21 (47.9 examples/sec; 0.167 sec/batch; 15h:25m:30s remains)
2017-12-16 07:46:33.255743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.7982273 -5.3949552 -4.7493443 -4.030776 -3.2085638 -2.2170954 -1.2053869 -0.32980013 0.039031982 -0.021960258 -0.47016621 -1.4734783 -2.4284322 -2.8706968 -2.680867][-5.0684433 -4.6775317 -4.0832744 -3.3160729 -2.4101727 -1.4779735 -0.70731521 -0.19158983 -0.0733428 -0.22901869 -0.61257148 -1.4470758 -2.1921155 -2.4844637 -2.2981346][-4.4096451 -4.2060905 -3.7969096 -3.0592108 -2.081634 -1.182328 -0.63950992 -0.52669287 -0.76887369 -1.0908318 -1.3604045 -1.8900516 -2.3333807 -2.4527948 -2.246547][-3.7929902 -3.9507577 -3.8302724 -3.1576805 -2.046699 -0.99833059 -0.45773268 -0.58892608 -1.2481637 -1.9462926 -2.3133478 -2.6779466 -2.9083152 -2.86908 -2.5482118][-3.3189662 -3.8246541 -3.9310751 -3.2548785 -1.9066842 -0.53960061 0.16839457 -0.098838329 -1.1889541 -2.3998244 -3.1097789 -3.5414863 -3.7008297 -3.5214369 -3.0020046][-2.8724484 -3.520901 -3.6669912 -2.8497915 -1.143647 0.692832 1.6951113 1.3368826 -0.22509432 -2.0857971 -3.3750639 -4.1321516 -4.3741255 -4.0870471 -3.3617263][-2.4747841 -3.0757766 -3.0757513 -1.9758728 0.18963957 2.621501 4.0664454 3.7319775 1.7735028 -0.76392365 -2.7969499 -4.09518 -4.6030369 -4.333405 -3.5062864][-2.2702079 -2.6986961 -2.534071 -1.2612209 1.1610818 4.0172129 5.9068232 5.8173513 3.834096 0.9410615 -1.67273 -3.5129871 -4.3973947 -4.2711945 -3.4722562][-2.430697 -2.6981797 -2.5111928 -1.3569684 0.91150951 3.711133 5.7307148 5.959815 4.3612337 1.6406384 -1.0687439 -3.0881689 -4.1441793 -4.1039681 -3.37261][-3.0240128 -3.2292368 -3.1823878 -2.3687184 -0.56953573 1.7572174 3.5649271 4.0043888 2.9651418 0.81499434 -1.5099883 -3.2865858 -4.2121506 -4.1089144 -3.3834214][-3.7338967 -3.9326344 -4.0760202 -3.6533103 -2.4102139 -0.68429971 0.80489492 1.3768215 0.81510162 -0.72221494 -2.5118797 -3.8916419 -4.5461292 -4.3101 -3.5434229][-3.9508078 -4.092114 -4.3442645 -4.2015147 -3.4045796 -2.2036765 -0.98663139 -0.31807089 -0.56342435 -1.6526761 -3.0492353 -4.189106 -4.6991286 -4.4412336 -3.6931262][-3.3495936 -3.5233004 -3.8806472 -3.9121065 -3.3365126 -2.3850925 -1.2626936 -0.4940443 -0.63281322 -1.5790484 -2.8849063 -4.0294967 -4.5673327 -4.39411 -3.721015][-2.3292117 -2.6449895 -3.2000508 -3.4584296 -2.9980342 -2.0156806 -0.70555639 0.30966663 0.23112011 -0.77085662 -2.2296665 -3.5881658 -4.306108 -4.2804084 -3.6823549][-1.48845 -1.8891442 -2.6014364 -3.083729 -2.760745 -1.7273645 -0.20901299 1.0658145 1.1019816 0.072724342 -1.4980164 -3.03671 -3.9567571 -4.1035662 -3.5965207]]...]
INFO - root - 2017-12-16 07:46:34.907368: step 210, loss = 0.28, batch loss = 0.19 (47.3 examples/sec; 0.169 sec/batch; 15h:37m:30s remains)
INFO - root - 2017-12-16 07:46:36.585032: step 220, loss = 0.34, batch loss = 0.25 (48.5 examples/sec; 0.165 sec/batch; 15h:13m:49s remains)
INFO - root - 2017-12-16 07:46:38.227731: step 230, loss = 0.29, batch loss = 0.21 (46.9 examples/sec; 0.170 sec/batch; 15h:44m:04s remains)
INFO - root - 2017-12-16 07:46:39.897715: step 240, loss = 0.31, batch loss = 0.23 (48.1 examples/sec; 0.166 sec/batch; 15h:21m:16s remains)
INFO - root - 2017-12-16 07:46:41.555928: step 250, loss = 0.48, batch loss = 0.40 (45.6 examples/sec; 0.176 sec/batch; 16h:11m:58s remains)
INFO - root - 2017-12-16 07:46:43.204413: step 260, loss = 0.24, batch loss = 0.16 (49.4 examples/sec; 0.162 sec/batch; 14h:56m:27s remains)
INFO - root - 2017-12-16 07:46:44.864016: step 270, loss = 0.38, batch loss = 0.29 (48.2 examples/sec; 0.166 sec/batch; 15h:18m:55s remains)
INFO - root - 2017-12-16 07:46:46.537961: step 280, loss = 0.26, batch loss = 0.18 (46.0 examples/sec; 0.174 sec/batch; 16h:02m:11s remains)
INFO - root - 2017-12-16 07:46:48.219212: step 290, loss = 0.36, batch loss = 0.27 (45.8 examples/sec; 0.175 sec/batch; 16h:06m:16s remains)
INFO - root - 2017-12-16 07:46:49.897567: step 300, loss = 0.30, batch loss = 0.22 (47.4 examples/sec; 0.169 sec/batch; 15h:34m:43s remains)
2017-12-16 07:46:50.386993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.665395 -1.7649744 -1.4683816 -1.4195197 -1.9609673 -2.3612425 -1.8187234 -1.3270094 -1.6891305 -2.5143642 -3.085948 -2.9286048 -2.1928983 -1.4848266 -1.1116111][-1.4647224 -1.6105933 -1.4403069 -1.5713933 -2.1751966 -2.5607033 -1.8781047 -1.3821373 -1.9476366 -2.9556708 -3.6689548 -3.5868554 -2.8314276 -2.0411422 -1.4911704][-0.94816089 -1.0458455 -1.0090032 -1.3516922 -2.0462489 -2.5381074 -2.0238395 -1.7841635 -2.603744 -3.7279835 -4.4693856 -4.4207196 -3.7266011 -2.980329 -2.3526182][-0.091176987 -0.073153496 -0.15847301 -0.67321014 -1.4104285 -2.0536866 -1.9402552 -2.0754304 -3.0065773 -4.0568886 -4.7113519 -4.6742992 -4.1384168 -3.6087446 -3.0499673][0.73286104 0.91660118 0.82435322 0.40482473 -0.14822006 -0.85847044 -1.1983385 -1.7026916 -2.6215477 -3.5199068 -4.108233 -4.1363964 -3.8060884 -3.5546751 -3.1289277][1.1415586 1.404448 1.392436 1.3146925 1.2237587 0.77259636 0.27994108 -0.40216589 -1.205219 -1.9395378 -2.585319 -2.8141112 -2.7242332 -2.7281246 -2.4257605][0.85661507 1.032712 1.0350556 1.2771306 1.7360339 1.8745766 1.7464929 1.3145394 0.8422513 0.33658314 -0.44343042 -0.99486876 -1.1864495 -1.4327734 -1.301424][0.23912811 0.18833494 0.10096169 0.43244696 1.1904354 1.876533 2.3767796 2.5373807 2.5896282 2.4239454 1.5657377 0.59041309 0.019094944 -0.5384357 -0.71770239][-0.27847958 -0.69474196 -0.95942354 -0.72585654 0.037617207 0.97140551 1.9673777 2.8866749 3.6729746 4.0621815 3.3872623 2.1136332 1.2206192 0.37308884 -0.22031069][-0.88098645 -1.6013806 -1.9938591 -1.9082804 -1.3717277 -0.59521604 0.45191765 1.8812742 3.3378868 4.40178 4.2077217 2.8859248 1.8086643 0.7950263 -0.10477686][-1.4436035 -2.2852893 -2.7282896 -2.783319 -2.5152411 -2.0654974 -1.3069036 0.1691308 1.9188838 3.4579601 3.7458677 2.523252 1.2835526 0.12306452 -0.93957591][-2.0915003 -2.8527765 -3.2845185 -3.3845739 -3.2077932 -2.9429479 -2.4744184 -1.2456038 0.4216814 2.0736051 2.5838928 1.4771748 0.055612087 -1.2685955 -2.3737204][-2.782198 -3.3428 -3.692205 -3.7967117 -3.6858611 -3.5811386 -3.3684626 -2.543479 -1.2529387 0.13359642 0.60536814 -0.28925991 -1.6688664 -2.9322913 -3.857173][-3.3553867 -3.5765038 -3.7311487 -3.8364892 -3.8674951 -3.9404471 -3.917686 -3.4596074 -2.5901973 -1.6068783 -1.2935104 -1.9311535 -3.0204487 -3.9688184 -4.5691352][-3.7111371 -3.6527653 -3.583693 -3.6493778 -3.8047283 -3.974798 -4.023921 -3.7386761 -3.1391389 -2.4719052 -2.2662146 -2.678618 -3.4392669 -4.0682521 -4.4081774]]...]
INFO - root - 2017-12-16 07:46:52.053462: step 310, loss = 0.28, batch loss = 0.19 (47.8 examples/sec; 0.167 sec/batch; 15h:26m:22s remains)
INFO - root - 2017-12-16 07:46:53.767206: step 320, loss = 0.22, batch loss = 0.14 (45.4 examples/sec; 0.176 sec/batch; 16h:15m:36s remains)
INFO - root - 2017-12-16 07:46:55.457214: step 330, loss = 0.32, batch loss = 0.23 (48.5 examples/sec; 0.165 sec/batch; 15h:12m:45s remains)
INFO - root - 2017-12-16 07:46:57.098465: step 340, loss = 0.26, batch loss = 0.18 (48.5 examples/sec; 0.165 sec/batch; 15h:13m:27s remains)
INFO - root - 2017-12-16 07:46:58.748515: step 350, loss = 0.27, batch loss = 0.19 (48.0 examples/sec; 0.167 sec/batch; 15h:22m:56s remains)
INFO - root - 2017-12-16 07:47:00.410155: step 360, loss = 0.34, batch loss = 0.26 (47.7 examples/sec; 0.168 sec/batch; 15h:28m:48s remains)
INFO - root - 2017-12-16 07:47:02.099308: step 370, loss = 0.32, batch loss = 0.24 (47.8 examples/sec; 0.167 sec/batch; 15h:26m:47s remains)
INFO - root - 2017-12-16 07:47:03.775841: step 380, loss = 0.30, batch loss = 0.21 (46.9 examples/sec; 0.170 sec/batch; 15h:43m:15s remains)
INFO - root - 2017-12-16 07:47:05.499896: step 390, loss = 0.39, batch loss = 0.30 (49.3 examples/sec; 0.162 sec/batch; 14h:57m:52s remains)
INFO - root - 2017-12-16 07:47:07.210107: step 400, loss = 0.23, batch loss = 0.14 (49.5 examples/sec; 0.162 sec/batch; 14h:55m:18s remains)
