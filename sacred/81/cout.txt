INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "81"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-06 04:28:24.114845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:28:24.114883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:28:24.114890: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:28:24.114895: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:28:24.114899: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-06 04:28:24.649150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 8.75GiB
2017-12-06 04:28:24.649186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-06 04:28:24.649193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-06 04:28:24.649200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-06 04:28:30.070474: step 0, loss = 0.88, batch loss = 0.68 (2.0 examples/sec; 4.091 sec/batch; 377h:48m:19s remains)
2017-12-06 04:28:30.695903: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071459979 0.071804248 0.073462248 0.077004738 0.080064021 0.08135777 0.081899136 0.085848719 0.088759005 0.085583068 0.082691386 0.082235329 0.081531532 0.08145608 0.080367386][0.074151427 0.076087587 0.077938177 0.080483079 0.082269624 0.084912181 0.091440022 0.099887595 0.10715874 0.10760646 0.1065912 0.10477998 0.0962224 0.092105575 0.0859167][0.076297238 0.078272834 0.0802754 0.083762504 0.089194693 0.0979724 0.11500105 0.13538772 0.15385546 0.15302601 0.14989789 0.14143434 0.11895742 0.10368751 0.090980522][0.0779896 0.07765311 0.078016788 0.085430458 0.10389675 0.13326602 0.16873261 0.20421602 0.22189555 0.21751602 0.2125169 0.18736871 0.15051037 0.12363781 0.10239083][0.081205286 0.079829924 0.084317788 0.098591112 0.13227826 0.17927802 0.23387939 0.28861856 0.31945091 0.30560872 0.27090791 0.22690989 0.17769082 0.13897762 0.11072475][0.086053111 0.085139968 0.092359118 0.11608708 0.16351159 0.23584008 0.31799841 0.41201234 0.45863354 0.40613714 0.33349553 0.26219043 0.19371651 0.14555171 0.11401431][0.089550659 0.087983832 0.099125773 0.12621379 0.18653958 0.27061358 0.37561136 0.51232278 0.56604993 0.44844154 0.34835416 0.26262829 0.18449859 0.13766564 0.10918558][0.092497304 0.093352653 0.10157824 0.1262296 0.1834567 0.25269625 0.33014306 0.41966522 0.42581761 0.34408021 0.27640373 0.20961389 0.1490905 0.11830296 0.1026938][0.09566009 0.09912347 0.10633396 0.12909546 0.17147578 0.21662685 0.26093602 0.28188449 0.26812151 0.22499281 0.19400151 0.15652223 0.11980967 0.10469695 0.097116694][0.092170268 0.096061125 0.10617625 0.12668324 0.15593506 0.18142404 0.19524218 0.19063604 0.17482956 0.15474628 0.14305584 0.12563312 0.10860723 0.10257605 0.098268412][0.088502921 0.091905668 0.10358649 0.11933712 0.13738665 0.14655212 0.14610815 0.14021145 0.1261685 0.11538362 0.1135896 0.11096642 0.10297804 0.099785611 0.096545696][0.087681927 0.090966105 0.09928868 0.10794333 0.11210511 0.11402112 0.11455479 0.11511616 0.10851235 0.099925414 0.09940815 0.099078804 0.0951181 0.095207937 0.092875771][0.088839635 0.088817917 0.092921078 0.097810954 0.097684123 0.098188341 0.098478682 0.10098695 0.10118297 0.096342996 0.09345147 0.093674816 0.092005804 0.092023596 0.090842739][0.090478741 0.0899425 0.08962208 0.090473033 0.091076195 0.092863731 0.094241127 0.097263955 0.097914428 0.093604609 0.090628393 0.092592515 0.092379436 0.092512175 0.089175165][0.094910093 0.094024129 0.091659755 0.0888494 0.090162218 0.0920382 0.09288346 0.097489789 0.099035472 0.0949034 0.092747748 0.093287379 0.092753872 0.092807 0.088799357]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 04:28:35.990862: step 10, loss = 0.88, batch loss = 0.67 (18.1 examples/sec; 0.442 sec/batch; 40h:50m:47s remains)
INFO - root - 2017-12-06 04:28:40.360541: step 20, loss = 0.87, batch loss = 0.66 (18.5 examples/sec; 0.433 sec/batch; 39h:57m:08s remains)
INFO - root - 2017-12-06 04:28:44.734495: step 30, loss = 0.88, batch loss = 0.67 (18.2 examples/sec; 0.438 sec/batch; 40h:29m:41s remains)
INFO - root - 2017-12-06 04:28:49.031589: step 40, loss = 0.88, batch loss = 0.67 (18.3 examples/sec; 0.437 sec/batch; 40h:23m:01s remains)
INFO - root - 2017-12-06 04:28:52.334115: step 50, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 21h:22m:20s remains)
INFO - root - 2017-12-06 04:28:54.645803: step 60, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.229 sec/batch; 21h:07m:23s remains)
INFO - root - 2017-12-06 04:28:56.902602: step 70, loss = 0.88, batch loss = 0.67 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:50s remains)
INFO - root - 2017-12-06 04:28:59.220508: step 80, loss = 0.87, batch loss = 0.66 (34.1 examples/sec; 0.235 sec/batch; 21h:40m:51s remains)
INFO - root - 2017-12-06 04:29:01.522317: step 90, loss = 0.87, batch loss = 0.66 (35.2 examples/sec; 0.227 sec/batch; 21h:00m:06s remains)
INFO - root - 2017-12-06 04:29:03.813638: step 100, loss = 0.89, batch loss = 0.68 (33.8 examples/sec; 0.237 sec/batch; 21h:51m:56s remains)
2017-12-06 04:29:04.132926: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.055492662 0.05518702 0.053569864 0.052047778 0.051305629 0.050249275 0.048150957 0.048273418 0.048880626 0.050816849 0.052315049 0.052924857 0.052318033 0.051274903 0.051492173][0.056781232 0.056268305 0.055184763 0.054258883 0.053530429 0.052080933 0.049334764 0.04921189 0.050359186 0.052576024 0.053619966 0.053983502 0.053537272 0.052657317 0.053212505][0.056788914 0.05591803 0.054760985 0.053468648 0.0528072 0.05175139 0.049753137 0.050099757 0.051572081 0.052694626 0.05300203 0.053660762 0.053318396 0.052363764 0.0532107][0.05620268 0.054743055 0.052872278 0.050776061 0.050033145 0.050218258 0.049301736 0.050481264 0.050472721 0.04950542 0.049132396 0.050943296 0.051288608 0.0501081 0.051031336][0.05499595 0.053984333 0.052227426 0.05034804 0.049073078 0.048943851 0.048473936 0.048606027 0.048548914 0.047419842 0.046446595 0.048752066 0.049608417 0.047797192 0.048024729][0.053314842 0.052726716 0.052219559 0.051079463 0.050177719 0.049255695 0.048476823 0.048794869 0.048992023 0.048014708 0.046190783 0.046314821 0.0469931 0.045645434 0.045392793][0.052679859 0.052583471 0.052862354 0.053351093 0.052886918 0.051242329 0.049241014 0.049881738 0.051846579 0.051657923 0.048042718 0.045846414 0.045065578 0.044107724 0.042843986][0.053066734 0.053297564 0.05365837 0.055092573 0.055425674 0.05333624 0.050726235 0.051228244 0.054070208 0.053869016 0.049670588 0.045993343 0.043649722 0.041823626 0.040241562][0.05335398 0.054446809 0.055064075 0.055998415 0.055746727 0.054052051 0.052486002 0.052908435 0.055259548 0.0556953 0.051553763 0.046938349 0.044205584 0.042325534 0.039784674][0.054383893 0.055676233 0.055835467 0.056148402 0.055676058 0.054743815 0.053956226 0.05359685 0.05512264 0.055291895 0.052684959 0.049826123 0.047521647 0.044221528 0.040780533][0.055153873 0.056458987 0.055994812 0.056080982 0.056155048 0.055613421 0.055239931 0.054962605 0.05630099 0.056217764 0.055084486 0.053321552 0.049839724 0.046540253 0.04409742][0.054472823 0.056135837 0.056207165 0.05596878 0.055847012 0.05636137 0.057008237 0.056557667 0.057047941 0.057925582 0.057127938 0.054765709 0.052174173 0.049592208 0.048832018][0.053838305 0.055279095 0.05543964 0.055150587 0.054680385 0.0553967 0.057097882 0.0571262 0.056859136 0.057362042 0.05675431 0.054233287 0.052449137 0.0511745 0.05129889][0.053386644 0.054384235 0.054096147 0.054739546 0.055034548 0.055018529 0.055642158 0.055362903 0.055037793 0.055215288 0.05489704 0.053971015 0.054024704 0.053220291 0.053322505][0.052875295 0.05391537 0.053479977 0.053808507 0.054421008 0.0544846 0.054077942 0.05378329 0.053881936 0.053994872 0.054324854 0.054314438 0.054497194 0.053859614 0.053500462]]...]
INFO - root - 2017-12-06 04:29:06.445032: step 110, loss = 0.89, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:39s remains)
INFO - root - 2017-12-06 04:29:08.772093: step 120, loss = 0.88, batch loss = 0.67 (35.3 examples/sec; 0.227 sec/batch; 20h:57m:07s remains)
INFO - root - 2017-12-06 04:29:11.054787: step 130, loss = 0.86, batch loss = 0.65 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:45s remains)
INFO - root - 2017-12-06 04:29:13.364539: step 140, loss = 0.87, batch loss = 0.66 (32.4 examples/sec; 0.247 sec/batch; 22h:47m:04s remains)
INFO - root - 2017-12-06 04:29:15.683803: step 150, loss = 0.90, batch loss = 0.69 (34.7 examples/sec; 0.230 sec/batch; 21h:15m:25s remains)
INFO - root - 2017-12-06 04:29:17.961162: step 160, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 20h:45m:56s remains)
INFO - root - 2017-12-06 04:29:20.266400: step 170, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:19s remains)
INFO - root - 2017-12-06 04:29:22.559650: step 180, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:59s remains)
INFO - root - 2017-12-06 04:29:24.899035: step 190, loss = 0.90, batch loss = 0.69 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:36s remains)
INFO - root - 2017-12-06 04:29:27.181192: step 200, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 21h:20m:06s remains)
2017-12-06 04:29:27.488629: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039359987 0.042559236 0.046672389 0.053366512 0.053306796 0.054255538 0.05354882 0.047751542 0.041321259 0.035289362 0.031761553 0.027907129 0.024453707 0.02427949 0.029430322][0.042738885 0.048503064 0.054268278 0.062308021 0.064180635 0.061631538 0.058271416 0.051705725 0.044400629 0.037321188 0.034382049 0.032564443 0.029061679 0.029965553 0.039444853][0.041569702 0.049856529 0.052924082 0.056843132 0.058828667 0.055260032 0.048418548 0.042660493 0.038654596 0.036742415 0.03819767 0.036915869 0.033158172 0.03414366 0.039985545][0.04796271 0.054399297 0.048635162 0.041570589 0.040123262 0.039213035 0.037786257 0.038826548 0.037440822 0.038042314 0.038070794 0.037628632 0.035164457 0.03468889 0.039340105][0.057873569 0.065391794 0.055751562 0.042751078 0.037522286 0.040473275 0.046892792 0.049818583 0.050191134 0.049435686 0.042766251 0.039446186 0.039655335 0.039391119 0.043275725][0.075583562 0.086746283 0.076051138 0.058214352 0.050241098 0.05940704 0.071735084 0.0740095 0.071409233 0.064380266 0.052640006 0.044409677 0.040226046 0.040037524 0.044634502][0.0872891 0.10639382 0.09567596 0.076705948 0.073101968 0.091638923 0.11574624 0.12667876 0.11373492 0.092606179 0.072119825 0.058415093 0.049156923 0.046473783 0.050234631][0.10684714 0.13356413 0.12286349 0.10346531 0.09930037 0.1266436 0.17163226 0.2032682 0.1736421 0.13134103 0.099711254 0.079365 0.071989618 0.067612343 0.065968364][0.11898764 0.1518317 0.14058158 0.11688075 0.10652971 0.1327212 0.1813913 0.2184093 0.18300052 0.13252752 0.099477254 0.08187826 0.075068325 0.072455712 0.070380673][0.097583152 0.12234258 0.11318672 0.088844389 0.0754683 0.088077419 0.11221116 0.13035756 0.12289526 0.097793825 0.078449629 0.067614511 0.057434216 0.054201216 0.053227119][0.070774391 0.090618722 0.087258257 0.067012154 0.056172863 0.059366412 0.070384264 0.078970283 0.082790643 0.075142808 0.067135751 0.062380992 0.055407725 0.049423724 0.045309152][0.0511282 0.061414838 0.059312925 0.048430964 0.04415923 0.047820512 0.055049874 0.057943664 0.060773417 0.061377823 0.058304332 0.054542616 0.04573011 0.039395839 0.037845332][0.040936258 0.050142609 0.055013813 0.051189594 0.048019785 0.04986985 0.051781587 0.049256485 0.055023171 0.058574915 0.053211287 0.042874698 0.032788958 0.02678341 0.027614281][0.05861187 0.06589748 0.07117857 0.069516078 0.066488765 0.065117083 0.060569227 0.054582894 0.05761046 0.062219709 0.056227624 0.042963274 0.031657841 0.025371566 0.031250995][0.071275048 0.075523324 0.0764217 0.0755088 0.069346458 0.065015428 0.060663722 0.05393362 0.056311995 0.057389282 0.05221542 0.04270485 0.034481212 0.032645777 0.039961018]]...]
INFO - root - 2017-12-06 04:29:29.755568: step 210, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.228 sec/batch; 21h:04m:21s remains)
INFO - root - 2017-12-06 04:29:32.028771: step 220, loss = 0.88, batch loss = 0.68 (34.1 examples/sec; 0.235 sec/batch; 21h:38m:56s remains)
INFO - root - 2017-12-06 04:29:34.316877: step 230, loss = 0.87, batch loss = 0.66 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:05s remains)
INFO - root - 2017-12-06 04:29:36.618013: step 240, loss = 0.89, batch loss = 0.68 (34.0 examples/sec; 0.236 sec/batch; 21h:44m:37s remains)
INFO - root - 2017-12-06 04:29:38.925062: step 250, loss = 0.90, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:14m:35s remains)
INFO - root - 2017-12-06 04:29:41.213966: step 260, loss = 0.90, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:45m:32s remains)
INFO - root - 2017-12-06 04:29:43.463044: step 270, loss = 0.88, batch loss = 0.67 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:55s remains)
INFO - root - 2017-12-06 04:29:45.739655: step 280, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 21h:07m:35s remains)
INFO - root - 2017-12-06 04:29:47.991634: step 290, loss = 0.88, batch loss = 0.67 (33.5 examples/sec; 0.238 sec/batch; 22h:00m:21s remains)
INFO - root - 2017-12-06 04:29:50.245378: step 300, loss = 0.89, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:02s remains)
2017-12-06 04:29:50.533411: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041765057 0.049916077 0.054602202 0.055851962 0.054080915 0.048477147 0.040480439 0.035579484 0.036596537 0.042077363 0.048111547 0.050215747 0.049543973 0.046717297 0.046238441][0.030678183 0.041014098 0.046648387 0.04816791 0.047481146 0.044415824 0.037886117 0.03328092 0.033027928 0.038196329 0.04294743 0.04576223 0.046040308 0.04630224 0.04788642][0.022396937 0.029147793 0.035772122 0.041356396 0.047361884 0.047949251 0.044462655 0.041961886 0.039728541 0.041945934 0.044221967 0.046696659 0.048946295 0.052527133 0.057097349][0.019387811 0.023363248 0.032080248 0.043217305 0.054649573 0.057292882 0.057699237 0.05682585 0.054994855 0.053305548 0.051184941 0.051568452 0.053439338 0.058047187 0.063687637][0.02135729 0.023124445 0.027602885 0.038152531 0.049109291 0.054318655 0.056947779 0.057261642 0.055391412 0.052916262 0.052356374 0.052657556 0.053566907 0.056436393 0.0585561][0.021375839 0.019763768 0.023542006 0.032877591 0.04377906 0.049307164 0.051539216 0.049829435 0.046761002 0.043082461 0.039585032 0.038910426 0.040016584 0.043495454 0.045772504][0.026939638 0.0262805 0.031579621 0.039537564 0.045977157 0.048082519 0.049253475 0.046210829 0.042270932 0.038513854 0.035593484 0.033809993 0.035057865 0.040041629 0.044789813][0.031828098 0.035662249 0.043710854 0.052754235 0.056770917 0.059061754 0.061400246 0.060961198 0.059681814 0.058550451 0.055196475 0.051946525 0.052150298 0.055683631 0.060116414][0.022826336 0.026659351 0.03127129 0.040172979 0.049897905 0.057336006 0.06455043 0.069548666 0.073650032 0.07679303 0.073267192 0.066863224 0.062791377 0.063580841 0.065618381][0.012697516 0.011886822 0.0081395209 0.0090517793 0.017216161 0.027314402 0.03767876 0.046765935 0.054854561 0.060726065 0.060124587 0.055191647 0.0495557 0.047857154 0.046985429][0.020865537 0.017013162 0.0085508358 0.0014763735 0.003135873 0.0092924684 0.01672576 0.023146331 0.029961158 0.033501014 0.035697345 0.034835167 0.032828435 0.031006292 0.027965453][0.040748019 0.03907451 0.026465442 0.015336756 0.011087172 0.011096483 0.013356334 0.016911335 0.021369539 0.026210599 0.031098597 0.029448032 0.027881868 0.026119087 0.023364093][0.047194842 0.049235042 0.043616921 0.035598956 0.031270519 0.03067217 0.029519353 0.02960854 0.031770237 0.033994492 0.034953624 0.034098517 0.033022586 0.031251319 0.0308403][0.038646791 0.044672664 0.044005748 0.042712215 0.041710053 0.041414853 0.03968985 0.037998386 0.038217138 0.039525777 0.041853193 0.04186533 0.040261511 0.03864393 0.038214885][0.03683478 0.043146271 0.046080891 0.047829684 0.049942594 0.050169561 0.050582174 0.05066004 0.051104676 0.051422764 0.052342791 0.051844824 0.05058584 0.047743704 0.046222065]]...]
INFO - root - 2017-12-06 04:29:52.794973: step 310, loss = 0.88, batch loss = 0.67 (35.6 examples/sec; 0.225 sec/batch; 20h:43m:58s remains)
INFO - root - 2017-12-06 04:29:55.082551: step 320, loss = 0.87, batch loss = 0.67 (35.5 examples/sec; 0.225 sec/batch; 20h:48m:12s remains)
INFO - root - 2017-12-06 04:29:57.350161: step 330, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:07s remains)
INFO - root - 2017-12-06 04:29:59.618197: step 340, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:54s remains)
INFO - root - 2017-12-06 04:30:01.909243: step 350, loss = 0.88, batch loss = 0.67 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:42s remains)
INFO - root - 2017-12-06 04:30:04.183238: step 360, loss = 0.89, batch loss = 0.68 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:48s remains)
INFO - root - 2017-12-06 04:30:06.477243: step 370, loss = 0.89, batch loss = 0.68 (35.6 examples/sec; 0.225 sec/batch; 20h:45m:26s remains)
INFO - root - 2017-12-06 04:30:08.768314: step 380, loss = 0.88, batch loss = 0.67 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:44s remains)
INFO - root - 2017-12-06 04:30:11.048237: step 390, loss = 0.88, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:48s remains)
INFO - root - 2017-12-06 04:30:13.351432: step 400, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:39s remains)
2017-12-06 04:30:13.644672: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050435849 0.049007997 0.047292978 0.04594662 0.044827148 0.044370137 0.044583105 0.045712128 0.047575794 0.050580978 0.054627202 0.059109233 0.062798224 0.065056212 0.065931477][0.052290171 0.050756805 0.049228854 0.048038386 0.0471861 0.047198154 0.047847986 0.049324349 0.051609106 0.055105619 0.059803575 0.064660512 0.068128787 0.069740526 0.070010275][0.05392427 0.052249551 0.050875537 0.0501964 0.050107285 0.050935566 0.052041143 0.053817905 0.05651205 0.060341783 0.065606967 0.070626743 0.07333535 0.074014395 0.073535][0.05827114 0.056975931 0.056167349 0.056201175 0.056993321 0.058390275 0.059850261 0.061816245 0.064913206 0.068930872 0.073923782 0.07823886 0.079976395 0.079532295 0.078046091][0.067419931 0.067220092 0.067358442 0.068141922 0.069077626 0.070217215 0.071512915 0.073366314 0.07630191 0.080303118 0.08425118 0.086946845 0.0873876 0.085669652 0.0827635][0.076373965 0.077765353 0.079410806 0.081056766 0.082046621 0.082572386 0.082915254 0.083719529 0.085507289 0.088833943 0.091769189 0.093273178 0.093006946 0.090364762 0.086423606][0.081353635 0.084053248 0.086603492 0.08861576 0.08990369 0.090741158 0.09148977 0.092037112 0.092621386 0.094000429 0.0954715 0.096283287 0.095785558 0.09296421 0.08844202][0.085900076 0.089401893 0.091681816 0.092378326 0.092354469 0.0926701 0.09395168 0.095141008 0.095697634 0.096127421 0.096656837 0.097102761 0.096460871 0.093271717 0.088740908][0.087438636 0.091187648 0.0936952 0.093627654 0.092446946 0.091541909 0.092459522 0.093335412 0.093674287 0.09427236 0.095197223 0.09603864 0.09554676 0.0924369 0.0880497][0.085735619 0.088760637 0.090507194 0.09011557 0.08871036 0.08735729 0.087557085 0.0875821 0.08758454 0.08823745 0.089653082 0.091000646 0.091073968 0.088801488 0.085235372][0.081997544 0.0844507 0.086240016 0.0859749 0.084853217 0.083472811 0.082940124 0.082319126 0.08163581 0.081616096 0.082528383 0.08380729 0.084392071 0.083432712 0.081331961][0.077760793 0.080435053 0.082792945 0.082687393 0.08135666 0.079768755 0.078707822 0.07759957 0.076769762 0.076268941 0.076646127 0.077491216 0.078160316 0.078048415 0.076945573][0.073017254 0.075241767 0.07762029 0.077661574 0.076713018 0.075278521 0.074000053 0.07275337 0.071824193 0.07143075 0.071762107 0.072504245 0.073111817 0.073242038 0.072624929][0.068705976 0.06988135 0.071231157 0.070909984 0.070282429 0.069432378 0.068662778 0.06782984 0.067038529 0.066545196 0.066820346 0.067631312 0.068467811 0.068869986 0.06854362][0.066107683 0.066270009 0.06661135 0.066046007 0.065660208 0.065187618 0.064704925 0.0641075 0.063533448 0.06307213 0.063073553 0.063682728 0.064466447 0.065090716 0.065186031]]...]
INFO - root - 2017-12-06 04:30:15.910129: step 410, loss = 0.87, batch loss = 0.67 (35.0 examples/sec; 0.229 sec/batch; 21h:05m:10s remains)
INFO - root - 2017-12-06 04:30:18.231503: step 420, loss = 0.88, batch loss = 0.67 (33.9 examples/sec; 0.236 sec/batch; 21h:45m:31s remains)
INFO - root - 2017-12-06 04:30:20.521598: step 430, loss = 0.89, batch loss = 0.68 (35.4 examples/sec; 0.226 sec/batch; 20h:51m:19s remains)
INFO - root - 2017-12-06 04:30:22.813771: step 440, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:58m:00s remains)
INFO - root - 2017-12-06 04:30:25.196681: step 450, loss = 0.89, batch loss = 0.68 (34.6 examples/sec; 0.231 sec/batch; 21h:20m:57s remains)
INFO - root - 2017-12-06 04:30:27.507009: step 460, loss = 0.88, batch loss = 0.68 (34.4 examples/sec; 0.232 sec/batch; 21h:26m:35s remains)
INFO - root - 2017-12-06 04:30:29.841959: step 470, loss = 0.88, batch loss = 0.67 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:50s remains)
INFO - root - 2017-12-06 04:30:32.112485: step 480, loss = 0.90, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:23s remains)
INFO - root - 2017-12-06 04:30:34.401093: step 490, loss = 0.88, batch loss = 0.68 (34.0 examples/sec; 0.235 sec/batch; 21h:43m:01s remains)
INFO - root - 2017-12-06 04:30:36.717982: step 500, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:52s remains)
2017-12-06 04:30:37.017851: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18321991 0.13678022 0.1362354 0.13069709 0.13145278 0.12999745 0.1321654 0.14174478 0.15601917 0.16784647 0.17556754 0.18078402 0.17749786 0.17762235 0.17522989][0.15548758 0.11328327 0.11137839 0.10627205 0.11067734 0.10911847 0.11420526 0.11823883 0.13111143 0.14019108 0.14216039 0.13725494 0.12980324 0.12808493 0.12431786][0.15760911 0.11877226 0.12246022 0.12925629 0.13960589 0.13399343 0.138107 0.13483572 0.13943623 0.13900623 0.13748984 0.14465295 0.14328417 0.14382012 0.14883335][0.15902835 0.12730774 0.14518924 0.15497299 0.15798578 0.15481491 0.15858413 0.14678656 0.14678748 0.14159919 0.14278007 0.14849916 0.13829276 0.13682315 0.13888754][0.17167957 0.1387272 0.16556333 0.1855343 0.17638935 0.16193442 0.15895894 0.15451969 0.17315811 0.18119356 0.19596475 0.1959978 0.17707701 0.14651044 0.10511222][0.21510708 0.17802435 0.21023369 0.23111744 0.21629749 0.19749948 0.17770788 0.16655815 0.18260284 0.20337053 0.22891819 0.22636175 0.19043708 0.12831628 0.069968492][0.2503081 0.20203717 0.21450709 0.22567032 0.22733048 0.21914962 0.20825276 0.20853873 0.20930463 0.21261805 0.22363025 0.21635368 0.16906852 0.10719212 0.060877815][0.2384759 0.17615442 0.174394 0.17895174 0.19436353 0.20056033 0.21888822 0.23446839 0.20411734 0.17231667 0.16189426 0.16373619 0.13890725 0.11120725 0.090052105][0.19928977 0.13648593 0.12389461 0.12808722 0.13857733 0.14531906 0.17221405 0.19498642 0.1667732 0.13278139 0.11858145 0.12080262 0.11318367 0.11363398 0.10783482][0.17078164 0.12134897 0.11733287 0.11497725 0.10666018 0.10774013 0.12365904 0.12779272 0.11165296 0.10924762 0.10945564 0.10762469 0.098669328 0.096856378 0.083989218][0.12512927 0.073467314 0.059327565 0.052924156 0.04182335 0.040738523 0.055472322 0.069914453 0.074790604 0.07939323 0.077855676 0.070962816 0.059634209 0.049402595 0.029819937][0.080056652 0.029506518 0.013985639 0.0022030305 0.0029765554 0.009917222 0.024077712 0.043907836 0.047765784 0.059508927 0.051579237 0.034827836 0.019793479 0.0091076065 -0.0030696727][0.013844131 -0.017628316 -0.02150126 -0.022624152 -0.01472804 -0.0085847387 -0.0041042082 0.0018357951 0.0072784517 0.010443745 0.0070038084 -0.0030364897 -0.0096584847 -0.0087176263 -0.013103342][-0.02756873 -0.031080469 -0.025006467 -0.021236967 -0.01564995 -0.015035836 -0.018535793 -0.019299319 -0.017853839 -0.016674973 -0.011292571 -0.010666427 -0.011884203 -0.0057694297 -0.0093782246][-0.024928298 -0.019226508 -0.013442178 -0.011341287 -0.0087790331 -0.0070368974 -0.00951826 -0.013826231 -0.017518668 -0.016665475 -0.010217065 -0.0031566117 -0.0010761637 0.0051357243 -0.0069290688]]...]
INFO - root - 2017-12-06 04:30:39.304909: step 510, loss = 0.89, batch loss = 0.68 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:31s remains)
INFO - root - 2017-12-06 04:30:41.576273: step 520, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:10s remains)
INFO - root - 2017-12-06 04:30:43.903634: step 530, loss = 0.89, batch loss = 0.68 (34.5 examples/sec; 0.232 sec/batch; 21h:22m:36s remains)
INFO - root - 2017-12-06 04:30:46.159950: step 540, loss = 0.90, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:33s remains)
INFO - root - 2017-12-06 04:30:48.427418: step 550, loss = 0.88, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:16s remains)
INFO - root - 2017-12-06 04:30:50.711051: step 560, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 21h:19m:58s remains)
INFO - root - 2017-12-06 04:30:52.997636: step 570, loss = 0.89, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:45s remains)
INFO - root - 2017-12-06 04:30:55.374102: step 580, loss = 0.89, batch loss = 0.68 (34.6 examples/sec; 0.231 sec/batch; 21h:18m:28s remains)
INFO - root - 2017-12-06 04:30:57.673026: step 590, loss = 0.90, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:46m:58s remains)
INFO - root - 2017-12-06 04:30:59.975852: step 600, loss = 0.89, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:14s remains)
2017-12-06 04:31:00.281323: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.021427352 0.021472793 0.02137414 0.020575907 0.019059304 0.016874932 0.014382813 0.012162477 0.010266803 0.0086037368 0.0083035938 0.0095072091 0.011976294 0.015770182 0.019547254][0.02098614 0.021145545 0.021149132 0.0206221 0.019438058 0.01777913 0.015796144 0.013830345 0.012047157 0.010493491 0.010370448 0.011582527 0.01391504 0.017364815 0.020458564][0.020214807 0.020533051 0.020869881 0.020695359 0.020088639 0.018948808 0.01744128 0.015781838 0.014119651 0.012916192 0.012950029 0.013996489 0.016060155 0.01869522 0.020499375][0.019136291 0.019539721 0.019943964 0.019857705 0.019384556 0.018626787 0.017552499 0.016607918 0.015932169 0.015836038 0.016399235 0.017092761 0.018480919 0.020076036 0.020499893][0.019471124 0.020178676 0.020588636 0.020548638 0.020115778 0.019455444 0.018601272 0.018374179 0.018925764 0.019684404 0.020187333 0.020231467 0.020433541 0.020954546 0.02025529][0.021440864 0.02268324 0.023197152 0.023057215 0.0228212 0.022404939 0.021976274 0.022449274 0.023530304 0.024242356 0.023823515 0.02245193 0.021296419 0.020831209 0.019452009][0.022836365 0.024142709 0.024536643 0.024713442 0.024918523 0.02493326 0.02498835 0.025705073 0.026215993 0.025797013 0.024035279 0.021854416 0.020619735 0.020156477 0.018962909][0.020871583 0.021866765 0.022570521 0.023633204 0.024799235 0.025720529 0.026057437 0.026258666 0.02575304 0.0243668 0.021944709 0.019684684 0.019034419 0.018877108 0.018224195][0.017902169 0.018370833 0.01893422 0.020372614 0.022214524 0.023694504 0.024476562 0.024533998 0.023668572 0.021707717 0.018981073 0.01684555 0.015969507 0.01561664 0.015291397][0.016594779 0.016214211 0.015485048 0.01562069 0.016470972 0.017219018 0.018031809 0.018600069 0.018394541 0.017249927 0.015379798 0.013752826 0.012989119 0.012770191 0.012722693][0.017077483 0.015095923 0.012491692 0.011079002 0.01093293 0.011183858 0.01171061 0.012452461 0.012709506 0.012385003 0.011735633 0.010952886 0.010883965 0.011215385 0.01148681][0.017984323 0.014762513 0.011177324 0.0088333488 0.0081245229 0.0080718677 0.0081655607 0.00855151 0.0087278448 0.0087921545 0.0088462718 0.008709684 0.0088383816 0.0092669837 0.009604048][0.018041041 0.014766466 0.011273298 0.0086306855 0.0073608197 0.0067124106 0.0062670242 0.0062309839 0.0062865987 0.0065574721 0.0068999026 0.0070903674 0.00730576 0.0076463073 0.0079840329][0.017885134 0.015237287 0.012278363 0.0097799934 0.007964097 0.0066120643 0.0058124736 0.0054903645 0.0055933725 0.0059328005 0.0063468628 0.0065986905 0.0068143234 0.0069979709 0.0070899744][0.017071787 0.015129331 0.012645829 0.010157645 0.0080283582 0.0062895622 0.0054587014 0.005234221 0.0054777861 0.0058802161 0.0063367691 0.0067141224 0.0070159156 0.0072311908 0.0072509572]]...]
INFO - root - 2017-12-06 04:31:02.588176: step 610, loss = 0.88, batch loss = 0.67 (34.1 examples/sec; 0.235 sec/batch; 21h:38m:25s remains)
INFO - root - 2017-12-06 04:31:04.846745: step 620, loss = 0.89, batch loss = 0.68 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:04s remains)
INFO - root - 2017-12-06 04:31:07.134312: step 630, loss = 0.89, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:04s remains)
INFO - root - 2017-12-06 04:31:09.450028: step 640, loss = 0.90, batch loss = 0.69 (35.5 examples/sec; 0.226 sec/batch; 20h:47m:38s remains)
INFO - root - 2017-12-06 04:31:11.751264: step 650, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:43s remains)
INFO - root - 2017-12-06 04:31:14.077023: step 660, loss = 0.88, batch loss = 0.67 (34.3 examples/sec; 0.234 sec/batch; 21h:31m:34s remains)
INFO - root - 2017-12-06 04:31:16.400472: step 670, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:50s remains)
INFO - root - 2017-12-06 04:31:18.710446: step 680, loss = 0.90, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 21h:16m:15s remains)
INFO - root - 2017-12-06 04:31:21.030391: step 690, loss = 0.90, batch loss = 0.69 (34.2 examples/sec; 0.234 sec/batch; 21h:34m:57s remains)
INFO - root - 2017-12-06 04:31:23.316535: step 700, loss = 0.89, batch loss = 0.68 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:05s remains)
2017-12-06 04:31:23.633543: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039300125 0.039327484 0.038908284 0.038614485 0.038453039 0.038272239 0.038162928 0.038195074 0.037993882 0.037429154 0.036846593 0.036246497 0.035425175 0.0344658 0.034023371][0.046817023 0.046549719 0.045583639 0.044691402 0.043985132 0.043621209 0.04352079 0.043600384 0.043495815 0.043119442 0.042763498 0.042610783 0.042309817 0.041385274 0.040274937][0.049423244 0.048687432 0.047320146 0.046014544 0.044899527 0.044356745 0.044403803 0.044407833 0.044285592 0.0442329 0.04446603 0.044950988 0.045360897 0.044969831 0.043881889][0.049637217 0.049320992 0.048367653 0.047199506 0.045935761 0.044834007 0.044238839 0.044036504 0.044236589 0.045191903 0.046533931 0.047900584 0.048877057 0.04888789 0.047837961][0.049446333 0.050199587 0.050311666 0.049946707 0.049173485 0.047826175 0.04626606 0.045308035 0.045513187 0.046902474 0.048745114 0.0506148 0.051555302 0.051165137 0.050299797][0.049592163 0.050765183 0.051732671 0.052390251 0.052662846 0.051996212 0.05087154 0.049768668 0.048878025 0.049213786 0.050407443 0.051713925 0.052352954 0.052241687 0.051891383][0.049750585 0.050507624 0.051164594 0.052153256 0.053242024 0.053989697 0.054846462 0.054917019 0.053381998 0.051858153 0.051086012 0.051150393 0.051319804 0.051602717 0.051770266][0.050028328 0.05008382 0.050257269 0.050885517 0.051852647 0.053119283 0.054816443 0.055886943 0.054835442 0.052762751 0.051249232 0.050593216 0.050282609 0.050637905 0.051006604][0.049726654 0.049983058 0.050137203 0.050205681 0.050467905 0.050977584 0.05149563 0.05169357 0.051788952 0.051668178 0.051440623 0.051064145 0.050485585 0.050351437 0.050582554][0.048370685 0.049248759 0.049829591 0.04978824 0.04953948 0.049192388 0.04870772 0.048566367 0.049333792 0.050394315 0.051529672 0.052026477 0.051758673 0.05132607 0.05117603][0.046309415 0.047493968 0.048053633 0.047813136 0.047335062 0.046953674 0.046936776 0.04715148 0.047914471 0.049203675 0.051041026 0.0522225 0.052294035 0.051670972 0.051241208][0.044244114 0.045118283 0.045396928 0.045151722 0.045051035 0.0453065 0.045698356 0.046103556 0.046747241 0.047755804 0.049394678 0.050574806 0.050848108 0.05028506 0.049828757][0.0426639 0.042994913 0.04323243 0.043098386 0.043280561 0.044025872 0.044725914 0.045212273 0.045706328 0.046394024 0.047674213 0.048642967 0.048865858 0.048479941 0.048235159][0.042320479 0.042353082 0.042481359 0.042524558 0.042923216 0.04381777 0.044605825 0.045236837 0.045826707 0.046455074 0.047389012 0.04796103 0.048087139 0.047749531 0.047386166][0.043195594 0.043146048 0.043189753 0.043395925 0.043922577 0.044766974 0.045634564 0.046486784 0.047323171 0.047911007 0.048343141 0.048314985 0.048031997 0.047688816 0.047450442]]...]
INFO - root - 2017-12-06 04:31:25.911865: step 710, loss = 0.89, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:39s remains)
INFO - root - 2017-12-06 04:31:28.184997: step 720, loss = 0.90, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:02s remains)
INFO - root - 2017-12-06 04:31:30.506188: step 730, loss = 0.89, batch loss = 0.68 (32.7 examples/sec; 0.245 sec/batch; 22h:32m:12s remains)
INFO - root - 2017-12-06 04:31:32.771782: step 740, loss = 0.89, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:42s remains)
INFO - root - 2017-12-06 04:31:35.093594: step 750, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:16s remains)
INFO - root - 2017-12-06 04:31:37.416454: step 760, loss = 0.90, batch loss = 0.69 (34.6 examples/sec; 0.231 sec/batch; 21h:19m:55s remains)
INFO - root - 2017-12-06 04:31:39.689629: step 770, loss = 0.90, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:29s remains)
INFO - root - 2017-12-06 04:31:41.972088: step 780, loss = 0.90, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:11s remains)
INFO - root - 2017-12-06 04:31:44.267487: step 790, loss = 0.88, batch loss = 0.67 (34.4 examples/sec; 0.232 sec/batch; 21h:24m:59s remains)
INFO - root - 2017-12-06 04:31:46.554364: step 800, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 21h:19m:26s remains)
2017-12-06 04:31:46.837322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015742496 -0.015885387 -0.016533142 -0.017804494 -0.017430723 -0.013744591 -0.011048934 -0.01013652 -0.012789645 -0.014258496 -0.014637494 -0.015954757 -0.017492661 -0.018656574 -0.018003752][-0.014709919 -0.015045274 -0.016106892 -0.017869553 -0.01842586 -0.015793361 -0.013207303 -0.012645832 -0.014430756 -0.015490594 -0.014312408 -0.014101557 -0.014531177 -0.015148834 -0.015327998][-0.01312333 -0.014020994 -0.015524963 -0.017756211 -0.01876968 -0.017645113 -0.014781389 -0.013389681 -0.014485729 -0.014917306 -0.012576374 -0.011620023 -0.011541258 -0.012275443 -0.01293606][-0.0096996948 -0.012015596 -0.014013889 -0.016156189 -0.016661454 -0.015505834 -0.013387878 -0.011366599 -0.01158329 -0.01219898 -0.010499625 -0.0096963719 -0.0096839406 -0.010557792 -0.011651503][-0.0073862169 -0.010622228 -0.011976076 -0.013253833 -0.013625861 -0.013322401 -0.011812228 -0.010097788 -0.010030031 -0.010723642 -0.0094006378 -0.0086157992 -0.0086531993 -0.00967391 -0.011402157][-0.0076192692 -0.010464526 -0.011250183 -0.01224267 -0.012338276 -0.011970205 -0.011141234 -0.01020471 -0.01015494 -0.010189112 -0.0091408491 -0.0082407687 -0.0084610861 -0.0096526761 -0.011242475][-0.0088981055 -0.011691929 -0.011428979 -0.011782146 -0.011422317 -0.010693082 -0.01031292 -0.0094835926 -0.0092772748 -0.0090240017 -0.0078766812 -0.0076368377 -0.0079427641 -0.0093939826 -0.011202137][-0.0098088393 -0.011935104 -0.010827309 -0.010348621 -0.009363357 -0.0086419843 -0.008253891 -0.0066036861 -0.0070943665 -0.0076292232 -0.0066435337 -0.0070797596 -0.0076889265 -0.0091429744 -0.010659264][-0.0097507294 -0.011470987 -0.0101968 -0.009314999 -0.0080018472 -0.0076157115 -0.0076603163 -0.0057936516 -0.0060460716 -0.0069179479 -0.0070108715 -0.00759021 -0.0079374686 -0.0092027131 -0.010363121][-0.0096185692 -0.010970806 -0.010127929 -0.0096672419 -0.0090580676 -0.0091439243 -0.0099294614 -0.0094055813 -0.0089006908 -0.0090053156 -0.0093515087 -0.0095767844 -0.0090887789 -0.0098535931 -0.010806952][-0.010225452 -0.011295089 -0.01102484 -0.011054325 -0.010776529 -0.011217798 -0.012266817 -0.012087583 -0.011819723 -0.011712715 -0.011752094 -0.011920294 -0.010811705 -0.01068082 -0.010926322][-0.01076954 -0.011615505 -0.011627813 -0.011782288 -0.011475834 -0.01180312 -0.012341144 -0.012513439 -0.012445863 -0.012188939 -0.012047319 -0.012048068 -0.01161204 -0.011504671 -0.011352714][-0.011163861 -0.011619117 -0.011703623 -0.011819207 -0.011693413 -0.011741127 -0.011774314 -0.011984057 -0.012381664 -0.012347324 -0.011975657 -0.011472437 -0.011268512 -0.01158607 -0.011964618][-0.011536784 -0.011623411 -0.011715156 -0.011710877 -0.011693819 -0.011670858 -0.011483699 -0.011531133 -0.012078245 -0.012510049 -0.012391848 -0.011941031 -0.011565008 -0.011780731 -0.012390628][-0.011599086 -0.011679048 -0.011757955 -0.011657433 -0.011677361 -0.011707249 -0.011533908 -0.011646189 -0.011942596 -0.012426042 -0.01269124 -0.012736613 -0.012441345 -0.012330503 -0.012844196]]...]
INFO - root - 2017-12-06 04:31:49.125627: step 810, loss = 0.89, batch loss = 0.69 (35.5 examples/sec; 0.226 sec/batch; 20h:47m:16s remains)
INFO - root - 2017-12-06 04:31:51.387473: step 820, loss = 0.90, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:26s remains)
INFO - root - 2017-12-06 04:31:53.687560: step 830, loss = 0.90, batch loss = 0.69 (34.3 examples/sec; 0.233 sec/batch; 21h:29m:40s remains)
INFO - root - 2017-12-06 04:31:55.981635: step 840, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:24s remains)
INFO - root - 2017-12-06 04:31:58.253389: step 850, loss = 0.89, batch loss = 0.68 (35.8 examples/sec; 0.224 sec/batch; 20h:35m:31s remains)
INFO - root - 2017-12-06 04:32:00.563613: step 860, loss = 0.88, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:06s remains)
INFO - root - 2017-12-06 04:32:02.872438: step 870, loss = 0.89, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:52m:30s remains)
INFO - root - 2017-12-06 04:32:05.162175: step 880, loss = 0.87, batch loss = 0.66 (33.1 examples/sec; 0.242 sec/batch; 22h:17m:37s remains)
INFO - root - 2017-12-06 04:32:07.504733: step 890, loss = 0.89, batch loss = 0.69 (32.9 examples/sec; 0.243 sec/batch; 22h:23m:53s remains)
INFO - root - 2017-12-06 04:32:09.809369: step 900, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 21h:23m:47s remains)
2017-12-06 04:32:10.137320: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11373216 0.11343749 0.11378612 0.11426665 0.11467098 0.11495219 0.11509091 0.11514369 0.11486381 0.11452505 0.11422297 0.11386487 0.11353761 0.11329032 0.11251311][0.11380702 0.1137176 0.11438327 0.11539464 0.11638226 0.11711037 0.11754024 0.11752188 0.11692016 0.11631948 0.11589473 0.11540996 0.1150329 0.11477794 0.11415063][0.11276047 0.11270912 0.11362557 0.1148877 0.1164858 0.11772969 0.11841097 0.11852144 0.11779334 0.11692435 0.1162086 0.1156126 0.11529599 0.11503795 0.11444043][0.1117328 0.11119109 0.11183019 0.11295621 0.11495306 0.11649841 0.11730151 0.11796553 0.11765344 0.11713655 0.11634703 0.11560054 0.11530623 0.11512059 0.11454183][0.11075161 0.11007315 0.1106353 0.11147748 0.11353491 0.11488023 0.11552411 0.1165251 0.11655812 0.11661492 0.11606036 0.11536683 0.11514589 0.11496128 0.11448804][0.10995784 0.10939828 0.11076869 0.1124019 0.11471102 0.11597744 0.1158231 0.11608014 0.11556688 0.11564925 0.11562398 0.11555763 0.11542474 0.11523254 0.1145096][0.10924418 0.1084111 0.11001474 0.11312829 0.11698593 0.11936882 0.11951552 0.1189494 0.11675663 0.11563525 0.11564247 0.11583577 0.1159582 0.11571178 0.11455277][0.10895899 0.10721217 0.10759881 0.11049737 0.115821 0.12146547 0.12474276 0.125282 0.12178724 0.11900984 0.1178043 0.11720307 0.11692998 0.11640407 0.11468749][0.11036493 0.10758324 0.1054433 0.10576098 0.10981545 0.11691771 0.1240367 0.12904967 0.12766738 0.12481412 0.12204152 0.1193518 0.1177523 0.11670443 0.11458597][0.11367606 0.11088471 0.10699269 0.10460368 0.10569075 0.11120375 0.11852506 0.12563488 0.12838829 0.12865505 0.12635745 0.12233959 0.11899817 0.11700228 0.11425962][0.11663863 0.11527708 0.11192453 0.10862534 0.10711424 0.10936449 0.11400628 0.11958309 0.12379386 0.12703063 0.12752712 0.12426461 0.12076829 0.118016 0.114355][0.1175683 0.1176092 0.11588331 0.11352822 0.11148418 0.11128975 0.1130072 0.11604314 0.11945403 0.1229528 0.12489879 0.1233182 0.12085842 0.11836791 0.11460114][0.11698635 0.11786287 0.1174528 0.1164517 0.11522043 0.11458254 0.11485489 0.11584592 0.11767642 0.11996503 0.12122881 0.12073275 0.1193132 0.11767708 0.11446919][0.11570078 0.11680605 0.116981 0.11684979 0.11658825 0.11650118 0.11644374 0.11651574 0.11696145 0.11768325 0.11802083 0.11759098 0.11702822 0.11633922 0.11403452][0.11389847 0.11473583 0.11506319 0.11527038 0.1154321 0.11557764 0.11554956 0.11535043 0.11530952 0.11549266 0.11552782 0.11537629 0.11527504 0.11501092 0.1134087]]...]
INFO - root - 2017-12-06 04:32:12.434817: step 910, loss = 0.88, batch loss = 0.67 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:50s remains)
INFO - root - 2017-12-06 04:32:14.735473: step 920, loss = 0.89, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 21h:27m:56s remains)
INFO - root - 2017-12-06 04:32:17.033720: step 930, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 20h:53m:39s remains)
INFO - root - 2017-12-06 04:32:19.312332: step 940, loss = 0.90, batch loss = 0.69 (35.0 examples/sec; 0.229 sec/batch; 21h:03m:47s remains)
INFO - root - 2017-12-06 04:32:21.565590: step 950, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:30s remains)
INFO - root - 2017-12-06 04:32:23.853551: step 960, loss = 0.87, batch loss = 0.66 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:18s remains)
INFO - root - 2017-12-06 04:32:26.120286: step 970, loss = 0.87, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 21h:14m:35s remains)
INFO - root - 2017-12-06 04:32:28.436653: step 980, loss = 0.87, batch loss = 0.66 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-06 04:32:30.685001: step 990, loss = 0.88, batch loss = 0.67 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:07s remains)
INFO - root - 2017-12-06 04:32:32.992203: step 1000, loss = 0.89, batch loss = 0.68 (33.0 examples/sec; 0.242 sec/batch; 22h:18m:34s remains)
2017-12-06 04:32:33.266212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0044944584 -0.0088818632 -0.017055735 -0.025504444 -0.031230493 -0.035660654 -0.0354034 -0.03330376 -0.032244008 -0.030929413 -0.031237764 -0.032154266 -0.032936927 -0.033642791 -0.034502141][0.022574024 0.014491094 0.0031120311 -0.0067855678 -0.013039643 -0.014531676 -0.012317011 -0.0085538533 -0.0066588372 -0.00981399 -0.016183116 -0.020878803 -0.025043651 -0.026900671 -0.028183939][0.054593451 0.039665 0.029904803 0.02448043 0.023207957 0.028073693 0.035926148 0.040610537 0.037211522 0.023934824 0.006697787 -0.007938778 -0.017113164 -0.021042917 -0.02330986][0.084371515 0.064812 0.064805411 0.072433136 0.083403267 0.099151559 0.11237071 0.1180219 0.10566596 0.074592412 0.037333518 0.0078147221 -0.010116126 -0.017584058 -0.019788671][0.1000528 0.07896959 0.094839968 0.12476789 0.15382195 0.18480745 0.20599961 0.21355253 0.19177476 0.14213 0.08052095 0.029225854 -0.003357742 -0.016560011 -0.018174393][0.10234023 0.08169625 0.1101855 0.15557076 0.20108734 0.2474848 0.27937818 0.29223043 0.26843482 0.20449063 0.12084376 0.0496739 0.0013312344 -0.01842613 -0.01917397][0.10039646 0.082870379 0.11481469 0.16638201 0.22209918 0.27920252 0.31600627 0.33429 0.31000027 0.23913631 0.1444378 0.061075017 0.0016379245 -0.022596093 -0.023046711][0.10940631 0.08864031 0.11750566 0.1672354 0.22554091 0.28806439 0.33147609 0.35072529 0.32416335 0.25421828 0.15494545 0.065071993 -0.0015235525 -0.029778231 -0.029132009][0.13660608 0.11375449 0.13358985 0.16877848 0.2182657 0.27672052 0.31939012 0.33850279 0.31612059 0.25460619 0.1598943 0.070320919 0.00070954859 -0.03075319 -0.03263307][0.17660545 0.14630997 0.15856779 0.18228361 0.21472529 0.253258 0.28618494 0.30558717 0.29352942 0.24682873 0.16583484 0.080716833 0.011203857 -0.023353256 -0.03005092][0.21643183 0.17931709 0.18523557 0.19801772 0.21805705 0.24070057 0.25878242 0.27295253 0.26867563 0.23559523 0.16647296 0.086608544 0.020891638 -0.013261253 -0.021296181][0.24766791 0.20209673 0.20197706 0.20769866 0.21946779 0.23143467 0.2408995 0.25054759 0.25044826 0.2264429 0.16648585 0.088777907 0.025618954 -0.008169679 -0.015710596][0.25697443 0.20952974 0.20854539 0.20935653 0.2089946 0.21082267 0.21583043 0.22599569 0.2316075 0.2136163 0.16071323 0.090077966 0.029741986 -0.0063593145 -0.020225549][0.23181421 0.1897956 0.18688126 0.18283591 0.177893 0.17271428 0.1711742 0.17907883 0.18857642 0.17866933 0.13898639 0.079443291 0.029456003 -0.00204066 -0.018328607][0.16033164 0.12683231 0.12393468 0.11932718 0.11444663 0.10966524 0.10713794 0.11235153 0.11783548 0.11388972 0.091892205 0.052916609 0.019023618 -0.0046114866 -0.017628979]]...]
INFO - root - 2017-12-06 04:32:35.563617: step 1010, loss = 0.89, batch loss = 0.68 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:36s remains)
INFO - root - 2017-12-06 04:32:37.829787: step 1020, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 21h:10m:03s remains)
INFO - root - 2017-12-06 04:32:40.142157: step 1030, loss = 0.88, batch loss = 0.67 (33.8 examples/sec; 0.237 sec/batch; 21h:49m:02s remains)
INFO - root - 2017-12-06 04:32:42.451359: step 1040, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.230 sec/batch; 21h:08m:04s remains)
INFO - root - 2017-12-06 04:32:44.744347: step 1050, loss = 0.88, batch loss = 0.67 (33.7 examples/sec; 0.238 sec/batch; 21h:52m:50s remains)
INFO - root - 2017-12-06 04:32:47.069315: step 1060, loss = 0.89, batch loss = 0.68 (34.0 examples/sec; 0.235 sec/batch; 21h:39m:12s remains)
INFO - root - 2017-12-06 04:32:49.352934: step 1070, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.228 sec/batch; 20h:56m:55s remains)
INFO - root - 2017-12-06 04:32:51.648992: step 1080, loss = 0.85, batch loss = 0.64 (34.6 examples/sec; 0.231 sec/batch; 21h:16m:57s remains)
INFO - root - 2017-12-06 04:32:53.985915: step 1090, loss = 0.88, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 21h:59m:48s remains)
INFO - root - 2017-12-06 04:32:56.308678: step 1100, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 21h:04m:47s remains)
2017-12-06 04:32:56.579503: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050881434 0.051399168 0.051744629 0.052134018 0.052534375 0.052926768 0.053284314 0.053540591 0.053667579 0.053653169 0.053515833 0.053315874 0.053101953 0.052924704 0.05244701][0.051772054 0.052568544 0.052904021 0.053304303 0.053722922 0.05414008 0.054541763 0.054847304 0.0550221 0.055068631 0.055014592 0.054905545 0.0547777 0.054689508 0.054371592][0.051906329 0.053011287 0.053350691 0.053769354 0.0542009 0.05463982 0.055077162 0.055422593 0.055640582 0.055743013 0.05576488 0.0557469 0.055716414 0.055705216 0.055514548][0.052533973 0.053896364 0.05427314 0.05470873 0.055141125 0.055582587 0.056025494 0.056382094 0.056627218 0.056795176 0.056919973 0.057002429 0.057054359 0.057103332 0.056936126][0.053454842 0.055063155 0.0555163 0.055974457 0.056394581 0.056839842 0.057294328 0.057659578 0.057935026 0.058181029 0.058417361 0.05856714 0.058638681 0.058666337 0.058452357][0.054371718 0.0564111 0.056925323 0.0573793 0.057740126 0.058160964 0.058660965 0.059079167 0.0593833 0.059710849 0.060066056 0.060254242 0.060299482 0.060283769 0.059982453][0.055615205 0.057803269 0.058353405 0.058796961 0.059097406 0.0594675 0.059992719 0.060461093 0.060786348 0.061135802 0.061561998 0.061811525 0.061862897 0.061833452 0.061371204][0.056946512 0.059273053 0.059815232 0.060189623 0.060364056 0.060615111 0.061111446 0.061624344 0.061943781 0.062297959 0.062791169 0.063114539 0.063208818 0.063175514 0.062563017][0.058343489 0.060620379 0.061092261 0.061339486 0.061335448 0.061400387 0.061748777 0.062222231 0.062526211 0.062885776 0.063447967 0.063872874 0.064020619 0.064026982 0.063362673][0.059323635 0.061568957 0.061825912 0.061907012 0.061745744 0.061624382 0.061790954 0.062196229 0.062469985 0.062819764 0.06339623 0.063921466 0.064175069 0.064252779 0.063644081][0.059895765 0.062092174 0.062114563 0.061996151 0.061704647 0.061399456 0.061386812 0.061706286 0.061948661 0.062255125 0.062818289 0.063431337 0.063816234 0.063998193 0.063440949][0.060281772 0.062352184 0.06218968 0.061910737 0.061513167 0.061071578 0.060856473 0.061005693 0.061193127 0.061454233 0.061966803 0.062649 0.063188329 0.063465267 0.06296806][0.060253885 0.062420342 0.06218024 0.061767075 0.061297234 0.060790773 0.060451638 0.060422536 0.060550552 0.060773697 0.061216589 0.061857779 0.062432315 0.062758788 0.062397163][0.059959043 0.062232625 0.061995085 0.061544556 0.061060656 0.060572784 0.060210582 0.060076807 0.060152847 0.060331952 0.060655389 0.061151091 0.061641086 0.061930563 0.061645884][0.059549097 0.061779264 0.061609421 0.061146174 0.060648162 0.060162846 0.059791241 0.05955955 0.059502434 0.059676442 0.059951205 0.060337279 0.060742337 0.061012533 0.060753066]]...]
INFO - root - 2017-12-06 04:32:58.866200: step 1110, loss = 0.89, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:14s remains)
INFO - root - 2017-12-06 04:33:01.177773: step 1120, loss = 0.90, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:43s remains)
INFO - root - 2017-12-06 04:33:03.433415: step 1130, loss = 0.88, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:22s remains)
INFO - root - 2017-12-06 04:33:05.687579: step 1140, loss = 0.89, batch loss = 0.68 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:26s remains)
INFO - root - 2017-12-06 04:33:07.977812: step 1150, loss = 0.87, batch loss = 0.66 (33.8 examples/sec; 0.237 sec/batch; 21h:47m:21s remains)
INFO - root - 2017-12-06 04:33:10.279349: step 1160, loss = 0.85, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:06s remains)
INFO - root - 2017-12-06 04:33:12.560070: step 1170, loss = 0.86, batch loss = 0.65 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:40s remains)
INFO - root - 2017-12-06 04:33:14.838613: step 1180, loss = 0.86, batch loss = 0.65 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:05s remains)
INFO - root - 2017-12-06 04:33:17.164503: step 1190, loss = 0.86, batch loss = 0.65 (34.7 examples/sec; 0.231 sec/batch; 21h:12m:53s remains)
INFO - root - 2017-12-06 04:33:19.462608: step 1200, loss = 0.88, batch loss = 0.67 (34.3 examples/sec; 0.233 sec/batch; 21h:28m:00s remains)
2017-12-06 04:33:19.748541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.099656045 -0.14059788 -0.18700193 -0.25892532 -0.32380277 -0.35468328 -0.36397862 -0.36497921 -0.35213876 -0.31223732 -0.22279149 -0.097518943 0.041558497 0.15758549 0.23755984][-0.065114506 -0.060979255 -0.089975737 -0.18433091 -0.30256128 -0.39969307 -0.45913649 -0.48728985 -0.50477886 -0.51797229 -0.47780716 -0.36772597 -0.20562597 -0.037469327 0.085723273][0.13794632 0.18492362 0.1993404 0.12160066 -0.023694385 -0.17928357 -0.29140526 -0.36122042 -0.41585875 -0.47758746 -0.50203043 -0.45839745 -0.33073515 -0.14548528 0.029218182][0.31558353 0.37400514 0.44167548 0.42401236 0.30789632 0.14132358 -0.0058341585 -0.12035266 -0.2168095 -0.32106143 -0.40929216 -0.44755948 -0.38452649 -0.22443599 -0.018510129][0.4102847 0.43433762 0.52334875 0.57746571 0.54851478 0.43825161 0.29880971 0.16026361 0.025635894 -0.11403324 -0.25813437 -0.37848741 -0.40577406 -0.3096652 -0.10088643][0.40308517 0.37926668 0.44407684 0.53698021 0.60701978 0.62099683 0.57516265 0.45857054 0.29116029 0.10692835 -0.083269194 -0.276856 -0.40844303 -0.40476209 -0.25208044][0.32505703 0.26269472 0.28170443 0.33490795 0.43071944 0.54412252 0.63867 0.64754236 0.53602928 0.34476715 0.11713903 -0.12134528 -0.32906705 -0.41892552 -0.35490352][0.2311977 0.15766314 0.13768797 0.13373093 0.1845924 0.30279559 0.46333408 0.59366864 0.62586671 0.51776749 0.31055856 0.049137831 -0.21413442 -0.37921476 -0.407238][0.14902155 0.08380799 0.05099225 0.010957573 0.0055705607 0.0674957 0.19848078 0.35175955 0.4795903 0.51382542 0.40826637 0.19744529 -0.058974814 -0.2679286 -0.37351263][0.054628745 0.011129912 -0.011376385 -0.051999882 -0.081588611 -0.071315564 -0.0053917728 0.10353997 0.24280135 0.35946667 0.36959928 0.2699942 0.088963427 -0.10365491 -0.24874993][-0.067832634 -0.083852217 -0.084598407 -0.099383652 -0.11918785 -0.13263293 -0.11794545 -0.066131413 0.040199108 0.17070881 0.25270665 0.25420994 0.1746427 0.049048431 -0.0816389][-0.20231958 -0.19923505 -0.17956884 -0.16696811 -0.16347727 -0.1758537 -0.19255343 -0.17369621 -0.10294212 -0.00018357486 0.093854636 0.15643474 0.16772234 0.11807716 0.034184918][-0.30062658 -0.29113215 -0.26651281 -0.24055032 -0.21820463 -0.21472044 -0.22432116 -0.21734442 -0.17182459 -0.1074515 -0.038775593 0.016655583 0.051722221 0.051828265 0.020416144][-0.32917494 -0.32578677 -0.31248772 -0.29676139 -0.27855933 -0.26555645 -0.25484058 -0.23283146 -0.19257188 -0.145076 -0.10205968 -0.069154829 -0.048760965 -0.0473253 -0.063851207][-0.27681094 -0.28331196 -0.28006005 -0.27760607 -0.28361279 -0.28654754 -0.27904955 -0.25028381 -0.21292131 -0.16754016 -0.12880482 -0.1094866 -0.10881621 -0.10841242 -0.11976893]]...]
INFO - root - 2017-12-06 04:33:22.049903: step 1210, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 21h:18m:36s remains)
INFO - root - 2017-12-06 04:33:24.359217: step 1220, loss = 0.86, batch loss = 0.65 (31.9 examples/sec; 0.251 sec/batch; 23h:05m:28s remains)
INFO - root - 2017-12-06 04:33:26.687052: step 1230, loss = 0.84, batch loss = 0.63 (32.8 examples/sec; 0.244 sec/batch; 22h:25m:15s remains)
INFO - root - 2017-12-06 04:33:28.971429: step 1240, loss = 0.86, batch loss = 0.65 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:59s remains)
INFO - root - 2017-12-06 04:33:31.286821: step 1250, loss = 0.88, batch loss = 0.67 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:01s remains)
INFO - root - 2017-12-06 04:33:33.592599: step 1260, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:08s remains)
INFO - root - 2017-12-06 04:33:35.884298: step 1270, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:11s remains)
INFO - root - 2017-12-06 04:33:38.172083: step 1280, loss = 0.89, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:13s remains)
INFO - root - 2017-12-06 04:33:40.531097: step 1290, loss = 0.89, batch loss = 0.68 (35.0 examples/sec; 0.228 sec/batch; 21h:00m:38s remains)
INFO - root - 2017-12-06 04:33:42.815104: step 1300, loss = 0.88, batch loss = 0.67 (33.3 examples/sec; 0.241 sec/batch; 22h:07m:49s remains)
2017-12-06 04:33:43.128989: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016622949 0.017199229 0.01695507 0.016841106 0.017155752 0.017880104 0.018763974 0.019735567 0.020315163 0.020537071 0.020477992 0.020386025 0.020435527 0.020462278 0.019906][0.020064089 0.021152925 0.021032762 0.021006066 0.021201525 0.021517899 0.021872502 0.022260547 0.022540707 0.022595454 0.022493269 0.022413608 0.022384379 0.022371583 0.021929462][0.021484636 0.022738021 0.022695605 0.022638217 0.022651061 0.0226481 0.022598062 0.02259833 0.022640973 0.0226251 0.022526659 0.022528466 0.02262209 0.022723857 0.022513594][0.022150774 0.023318887 0.02318868 0.022919782 0.022590548 0.022222321 0.021787964 0.021371551 0.021128062 0.020994257 0.021037936 0.021362048 0.021818668 0.022230621 0.022393756][0.022559397 0.023504596 0.02320708 0.022711534 0.022039086 0.021276224 0.02039282 0.01939689 0.018602282 0.01829987 0.018588539 0.01937085 0.020362865 0.021304697 0.02188265][0.022050437 0.022895861 0.022562388 0.022059288 0.021481667 0.020703789 0.019509241 0.017925452 0.016437285 0.015605278 0.015832964 0.017018527 0.018565159 0.02004873 0.020966101][0.02043806 0.021138616 0.020749398 0.020176847 0.019620351 0.018924106 0.017717265 0.015969347 0.014355607 0.013460822 0.013732046 0.015132319 0.017029159 0.018880419 0.019981604][0.01787398 0.018442381 0.017922223 0.017145623 0.016334575 0.015445169 0.014198344 0.012689967 0.011522774 0.011047911 0.01175604 0.01354827 0.01579709 0.01794361 0.018999808][0.015111368 0.015500795 0.014886431 0.014077432 0.013161745 0.012124162 0.010755707 0.0094470829 0.0087752417 0.0088787377 0.010053959 0.012164913 0.014662325 0.016930226 0.017781723][0.012400575 0.0127979 0.012396634 0.011930916 0.011288356 0.010347582 0.0089211427 0.00769265 0.0071325675 0.0072206892 0.0082234107 0.010099992 0.012431331 0.014550209 0.015133478][0.0092557594 0.010010075 0.010235269 0.010515127 0.010588344 0.010144558 0.0089144148 0.007511463 0.0063790977 0.0055920072 0.0056790635 0.0066324919 0.0081631318 0.0096322633 0.0099292248][0.0060823895 0.0074418634 0.0083438046 0.0094190948 0.01023984 0.010384183 0.0094246306 0.007537052 0.0051892176 0.0028933659 0.0017094724 0.0016206913 0.0022772327 0.0028043538 0.0023473576][0.0039060898 0.0059925467 0.0074061714 0.0089362971 0.010450698 0.011090424 0.0099947676 0.0068521723 0.0024544299 -0.0019917898 -0.0052365754 -0.0068891477 -0.0071083307 -0.0071646553 -0.0081058852][0.0033975132 0.00619955 0.007906884 0.0093568675 0.01100513 0.011884578 0.010506045 0.0058544315 -0.0013541058 -0.0087905042 -0.014736999 -0.018343236 -0.019459262 -0.019388521 -0.020259183][0.002953805 0.00640944 0.008210212 0.009335421 0.010690909 0.011360925 0.0098460652 0.0045042671 -0.0045850351 -0.014956631 -0.023622766 -0.029085422 -0.031038087 -0.031300303 -0.032118797]]...]
INFO - root - 2017-12-06 04:33:45.453710: step 1310, loss = 0.89, batch loss = 0.68 (34.2 examples/sec; 0.234 sec/batch; 21h:33m:03s remains)
INFO - root - 2017-12-06 04:33:47.746643: step 1320, loss = 0.86, batch loss = 0.65 (34.2 examples/sec; 0.234 sec/batch; 21h:31m:11s remains)
INFO - root - 2017-12-06 04:33:50.051265: step 1330, loss = 0.87, batch loss = 0.66 (34.1 examples/sec; 0.234 sec/batch; 21h:33m:06s remains)
INFO - root - 2017-12-06 04:33:52.357416: step 1340, loss = 0.85, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:54m:44s remains)
INFO - root - 2017-12-06 04:33:54.643851: step 1350, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:54s remains)
INFO - root - 2017-12-06 04:33:56.937539: step 1360, loss = 0.89, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:30s remains)
INFO - root - 2017-12-06 04:33:59.235499: step 1370, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:58s remains)
INFO - root - 2017-12-06 04:34:01.541871: step 1380, loss = 0.88, batch loss = 0.67 (34.0 examples/sec; 0.235 sec/batch; 21h:37m:23s remains)
INFO - root - 2017-12-06 04:34:03.803739: step 1390, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:15s remains)
INFO - root - 2017-12-06 04:34:06.098680: step 1400, loss = 0.86, batch loss = 0.65 (33.5 examples/sec; 0.239 sec/batch; 21h:56m:41s remains)
2017-12-06 04:34:06.382541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011778856 -0.01836334 -0.023968201 -0.030344807 -0.038469467 -0.045744877 -0.048930507 -0.0465671 -0.040228415 -0.034188114 -0.027879907 -0.020111857 -0.0085572135 -0.002233889 -0.0049966536][0.00011593476 -0.0066377148 -0.011689832 -0.016181914 -0.02194982 -0.027993735 -0.032257084 -0.030664874 -0.023605889 -0.017254885 -0.013480222 -0.008101007 0.0016491376 0.0068953969 0.003325507][0.0061852336 0.00082046911 -0.002942156 -0.0060707629 -0.0097915512 -0.014062487 -0.01819684 -0.016757896 -0.010675184 -0.0050546676 -0.0029987581 -0.00050442666 0.0061542913 0.0095443055 0.0058063865][0.0087970048 0.0056566857 0.0032477491 0.0013132691 -0.001307793 -0.0048862696 -0.0068856105 -0.0049569197 -0.0020116754 0.00096192211 0.0026094802 0.0032227635 0.0066440143 0.0073216073 0.0037000775][0.0095224567 0.0085560717 0.0069219135 0.0059165657 0.0034065321 0.0017809682 0.0036197454 0.0046674684 0.0032309815 0.0045227259 0.006902203 0.0063312761 0.0067965761 0.0059240274 0.0017674714][0.010529913 0.010639518 0.0093765631 0.0081661232 0.0051039346 0.0059314221 0.012330934 0.010958262 0.0050006844 0.0057443529 0.0095035471 0.0073892809 0.0061180778 0.0042551123 0.00028084964][0.011513382 0.012415167 0.012131125 0.010900211 0.00655533 0.0078348853 0.015498001 0.012452189 0.0059631355 0.0068803318 0.010377914 0.0084249638 0.0067513213 0.0042007752 9.0669841e-05][0.011933014 0.013230842 0.013808083 0.012931243 0.0092541426 0.0094207153 0.014493857 0.011672959 0.0080407262 0.008249253 0.010079253 0.0092591494 0.0078693293 0.0052417517 0.0015007742][0.011762433 0.013377 0.014225677 0.013637818 0.011177663 0.010210861 0.012739141 0.011671104 0.0096765719 0.0087383538 0.0092321634 0.009545166 0.0088910982 0.0069441758 0.004181724][0.011296023 0.012818906 0.013724305 0.013721641 0.012148529 0.010451991 0.010893676 0.010918539 0.0099912509 0.008676175 0.0087680109 0.0093583055 0.0091889352 0.008063063 0.0068247691][0.010875605 0.012032226 0.012611933 0.012979686 0.012073036 0.010482609 0.010050207 0.010326356 0.0097981654 0.00878302 0.0087877139 0.0091622323 0.00917048 0.0084082037 0.00790919][0.010235794 0.011028197 0.011264611 0.011803132 0.01125852 0.010117028 0.0095657371 0.0098699182 0.0094302818 0.0087101981 0.0084840842 0.0086277835 0.0086502992 0.0081237145 0.0075586848][0.0091862865 0.0098357573 0.0099369884 0.010080397 0.010054037 0.0099036992 0.009174902 0.00908497 0.0088848583 0.0084953643 0.0083092339 0.0083593652 0.0082565136 0.0076615363 0.0068127587][0.0078965686 0.0086215176 0.0086957477 0.0089930855 0.0092837662 0.0091025531 0.0085949078 0.0087405629 0.0088442676 0.0084649436 0.0081687532 0.0081342012 0.0080782995 0.0076438859 0.0065026879][0.0061978437 0.0070550218 0.0072176494 0.007489644 0.0076250695 0.0075699016 0.0074310787 0.0073001496 0.0074405782 0.0075744577 0.0075233765 0.0074119903 0.0073998608 0.0068865716 0.0054557137]]...]
INFO - root - 2017-12-06 04:34:08.668570: step 1410, loss = 0.88, batch loss = 0.67 (34.1 examples/sec; 0.234 sec/batch; 21h:33m:39s remains)
INFO - root - 2017-12-06 04:34:10.955430: step 1420, loss = 0.87, batch loss = 0.66 (34.4 examples/sec; 0.232 sec/batch; 21h:21m:53s remains)
INFO - root - 2017-12-06 04:34:13.237206: step 1430, loss = 0.88, batch loss = 0.67 (34.7 examples/sec; 0.231 sec/batch; 21h:11m:56s remains)
INFO - root - 2017-12-06 04:34:15.476508: step 1440, loss = 0.86, batch loss = 0.65 (34.7 examples/sec; 0.231 sec/batch; 21h:12m:28s remains)
INFO - root - 2017-12-06 04:34:17.810560: step 1450, loss = 0.88, batch loss = 0.68 (34.0 examples/sec; 0.235 sec/batch; 21h:37m:06s remains)
INFO - root - 2017-12-06 04:34:20.116130: step 1460, loss = 0.87, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 21h:19m:34s remains)
INFO - root - 2017-12-06 04:34:22.394205: step 1470, loss = 0.86, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 21h:38m:17s remains)
INFO - root - 2017-12-06 04:34:24.701436: step 1480, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:03s remains)
INFO - root - 2017-12-06 04:34:27.018005: step 1490, loss = 0.86, batch loss = 0.65 (32.4 examples/sec; 0.247 sec/batch; 22h:43m:59s remains)
INFO - root - 2017-12-06 04:34:29.308112: step 1500, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:09s remains)
2017-12-06 04:34:29.633267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0046121106 -0.0026021637 0.00017657131 0.002624616 0.0036362335 0.0021208897 -0.000630077 -0.0034873709 -0.0055475682 -0.005692061 -0.0045969523 -0.0026357062 -0.00036086515 0.0011404678 0.0013875514][-0.004977081 -0.0023197569 0.0010357387 0.0039733686 0.0043266341 0.00053655356 -0.005080685 -0.010047231 -0.013528006 -0.014064005 -0.012180328 -0.008339826 -0.0037781298 -0.00036388263 0.00092515722][-0.004821308 -0.0015183538 0.0024492145 0.0057118945 0.0050292984 -0.0019062534 -0.011968207 -0.020971652 -0.026866041 -0.027900582 -0.024435598 -0.01754489 -0.0091154 -0.0026061572 0.0004257448][-0.003962718 -0.00055775791 0.0039436966 0.0075671487 0.0057266429 -0.0041992404 -0.019149421 -0.033682533 -0.043451697 -0.045698844 -0.040685043 -0.03058314 -0.017591476 -0.0068797842 -0.0010833666][-0.0026829317 0.0002200827 0.0041814856 0.0073371083 0.0043325983 -0.0076928698 -0.025891457 -0.045577995 -0.060374 -0.065109968 -0.059282377 -0.046075605 -0.029065438 -0.013997976 -0.00488957][-0.0010944121 0.00078506768 0.0031437017 0.0041366331 -0.00089700148 -0.013854586 -0.033020314 -0.055025328 -0.07372874 -0.081526071 -0.076174468 -0.061283972 -0.042085804 -0.023848502 -0.011406491][-0.00045589358 0.00014661625 0.00071698055 -0.0008091256 -0.0081577525 -0.021714408 -0.040820938 -0.062524915 -0.08170329 -0.091121674 -0.08724381 -0.073092312 -0.054363474 -0.03524439 -0.020323189][-0.00092346594 -0.001164753 -0.0022752695 -0.0065626167 -0.015391096 -0.02843615 -0.045896158 -0.065220729 -0.082170844 -0.091435805 -0.089869641 -0.078225113 -0.06179589 -0.04364723 -0.028012689][-0.0024793521 -0.0038345642 -0.0060727634 -0.01142776 -0.019645881 -0.030715704 -0.045049816 -0.061187483 -0.076195292 -0.084993839 -0.08533404 -0.076464541 -0.063080177 -0.047276262 -0.032189704][-0.0038309693 -0.0060952343 -0.0093179736 -0.014392808 -0.020949902 -0.029490571 -0.040012956 -0.052231573 -0.0646373 -0.07257992 -0.07394807 -0.067844212 -0.057345219 -0.044643085 -0.031952418][-0.0034338571 -0.0060888119 -0.0097025391 -0.013995191 -0.019089157 -0.025210863 -0.032839946 -0.041652955 -0.051293317 -0.05825796 -0.060209371 -0.056123484 -0.04795916 -0.03775556 -0.027531847][-0.0013442449 -0.0037470683 -0.0072216727 -0.010729548 -0.014479907 -0.018664667 -0.023890425 -0.029961858 -0.036788147 -0.04208206 -0.044106863 -0.041844629 -0.035707921 -0.027622465 -0.019812167][-9.137392e-05 -0.001686573 -0.0043844171 -0.0067710541 -0.008903699 -0.011088293 -0.013919011 -0.017507974 -0.02177725 -0.025626827 -0.027669933 -0.026593357 -0.022645496 -0.016958194 -0.011404663][0.000621099 0.00039928034 -0.00099140033 -0.00242557 -0.0038333945 -0.0050959587 -0.0064128526 -0.0079391152 -0.010050287 -0.012475232 -0.013896279 -0.013478525 -0.011340333 -0.007971406 -0.0046303123][0.0017188266 0.0018612593 0.0010805987 0.00022580475 -0.00077259913 -0.00187172 -0.0029345378 -0.0038128942 -0.0046222657 -0.0056181997 -0.0060983561 -0.0055821873 -0.0044094697 -0.0024531521 -0.00058815628]]...]
INFO - root - 2017-12-06 04:34:31.951778: step 1510, loss = 0.88, batch loss = 0.67 (34.0 examples/sec; 0.235 sec/batch; 21h:36m:47s remains)
INFO - root - 2017-12-06 04:34:34.220255: step 1520, loss = 0.85, batch loss = 0.65 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:38s remains)
INFO - root - 2017-12-06 04:34:36.512858: step 1530, loss = 0.89, batch loss = 0.68 (33.7 examples/sec; 0.237 sec/batch; 21h:47m:49s remains)
INFO - root - 2017-12-06 04:34:38.810352: step 1540, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:26s remains)
INFO - root - 2017-12-06 04:34:41.114590: step 1550, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:46s remains)
INFO - root - 2017-12-06 04:34:43.397830: step 1560, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 21h:25m:23s remains)
INFO - root - 2017-12-06 04:34:45.707669: step 1570, loss = 0.85, batch loss = 0.64 (34.6 examples/sec; 0.231 sec/batch; 21h:14m:36s remains)
INFO - root - 2017-12-06 04:34:48.010932: step 1580, loss = 0.89, batch loss = 0.68 (35.6 examples/sec; 0.225 sec/batch; 20h:39m:10s remains)
INFO - root - 2017-12-06 04:34:50.326894: step 1590, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:07s remains)
INFO - root - 2017-12-06 04:34:52.606313: step 1600, loss = 0.87, batch loss = 0.66 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:37s remains)
2017-12-06 04:34:52.906780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12608424 0.12874125 0.15265028 0.17038414 0.18012419 0.18501386 0.18616167 0.18247326 0.17906347 0.1781721 0.17786653 0.17475761 0.16248578 0.14190522 0.10853584][0.036744069 0.0060632192 0.00048620254 -0.009085048 -0.022199135 -0.032909974 -0.045933522 -0.057600874 -0.061284393 -0.051852804 -0.031534359 -0.0022589229 0.023682501 0.037924368 0.037518982][-0.045136824 -0.097136386 -0.12132876 -0.15020721 -0.17935228 -0.19868526 -0.22280183 -0.24958527 -0.26428729 -0.25012109 -0.21043195 -0.15395291 -0.093145952 -0.045499675 -0.016502727][-0.087106377 -0.133392 -0.14813061 -0.17395316 -0.20370288 -0.22455186 -0.25273013 -0.28612596 -0.31155372 -0.30403164 -0.25867805 -0.18788604 -0.11609526 -0.066174492 -0.035957143][-0.092340738 -0.12040855 -0.12059473 -0.13824102 -0.16293512 -0.1793966 -0.20852281 -0.25326893 -0.29124981 -0.29539773 -0.25818717 -0.19253887 -0.12506601 -0.082036495 -0.05708573][-0.091288909 -0.10843779 -0.098782942 -0.1047007 -0.11053172 -0.10175784 -0.10981554 -0.14998826 -0.20051901 -0.22488782 -0.20714791 -0.16087413 -0.1124064 -0.085708573 -0.067572705][-0.089230955 -0.10060798 -0.07940571 -0.059937023 -0.025152989 0.035642084 0.072880343 0.058055323 0.0024173781 -0.052013461 -0.070983909 -0.055546053 -0.036294706 -0.035678573 -0.034762181][-0.097062021 -0.10124643 -0.0641064 -0.01241898 0.072787717 0.19086492 0.27575526 0.2864697 0.22616021 0.14946023 0.098980293 0.081359193 0.074597016 0.051461648 0.032755341][-0.13615529 -0.145643 -0.10746761 -0.044945013 0.060146246 0.20148028 0.3047829 0.32749745 0.26713857 0.18749234 0.1302013 0.10355186 0.087925673 0.060429525 0.04384483][-0.21067512 -0.24840879 -0.23605582 -0.19781223 -0.11520667 0.0062351413 0.096083388 0.116142 0.072155833 0.017133512 -0.021660183 -0.035760615 -0.040754266 -0.050043195 -0.040780887][-0.28300649 -0.35317 -0.3771612 -0.37482315 -0.32710913 -0.23360167 -0.15813705 -0.13322172 -0.15503556 -0.18298833 -0.20146999 -0.20482147 -0.19790234 -0.18575534 -0.15015335][-0.31280792 -0.39954802 -0.44476816 -0.46693709 -0.44370052 -0.37127084 -0.30617458 -0.28132254 -0.29431325 -0.30981204 -0.31801665 -0.31493056 -0.29900777 -0.27245575 -0.22556165][-0.30604413 -0.39707518 -0.45286191 -0.49374032 -0.49349192 -0.44675866 -0.3964189 -0.37466532 -0.38530207 -0.3968741 -0.40063044 -0.39382395 -0.37054807 -0.33502397 -0.27990875][-0.2895596 -0.38124642 -0.44152933 -0.49034739 -0.50691289 -0.47947091 -0.44131804 -0.42377567 -0.42934778 -0.43679231 -0.4372313 -0.42238486 -0.39240408 -0.34935445 -0.28741014][-0.27466014 -0.36524203 -0.42380312 -0.46757078 -0.48446736 -0.47076711 -0.44526815 -0.42638874 -0.41919675 -0.41718575 -0.40699783 -0.38422367 -0.34873807 -0.30011064 -0.23825495]]...]
INFO - root - 2017-12-06 04:34:55.183348: step 1610, loss = 0.84, batch loss = 0.64 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:20s remains)
INFO - root - 2017-12-06 04:34:57.451544: step 1620, loss = 0.85, batch loss = 0.64 (35.6 examples/sec; 0.225 sec/batch; 20h:38m:42s remains)
INFO - root - 2017-12-06 04:34:59.746396: step 1630, loss = 0.86, batch loss = 0.65 (35.3 examples/sec; 0.226 sec/batch; 20h:48m:39s remains)
INFO - root - 2017-12-06 04:35:02.038780: step 1640, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:40m:07s remains)
INFO - root - 2017-12-06 04:35:04.358191: step 1650, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.234 sec/batch; 21h:31m:54s remains)
INFO - root - 2017-12-06 04:35:06.665693: step 1660, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:14s remains)
INFO - root - 2017-12-06 04:35:08.954948: step 1670, loss = 0.87, batch loss = 0.66 (33.6 examples/sec; 0.238 sec/batch; 21h:54m:18s remains)
INFO - root - 2017-12-06 04:35:11.281272: step 1680, loss = 0.89, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 21h:24m:35s remains)
INFO - root - 2017-12-06 04:35:13.606394: step 1690, loss = 0.89, batch loss = 0.68 (34.1 examples/sec; 0.234 sec/batch; 21h:32m:20s remains)
INFO - root - 2017-12-06 04:35:15.891167: step 1700, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 21h:01m:23s remains)
2017-12-06 04:35:16.170177: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10214593 0.082663879 0.014563251 -0.11079884 -0.23657987 -0.29869056 -0.28141105 -0.22977115 -0.18211816 -0.14178112 -0.10067588 -0.039525129 0.032995485 0.086731583 0.11837138][0.47383982 0.45310879 0.35282087 0.15785427 -0.09376429 -0.27567625 -0.33803481 -0.32755315 -0.28313887 -0.21882072 -0.15740731 -0.096852541 -0.027364792 0.033724152 0.0811193][0.83918768 0.84497821 0.75410694 0.53720689 0.22587334 -0.037011728 -0.23478265 -0.32493967 -0.34151173 -0.28806123 -0.21008453 -0.14151448 -0.077728748 -0.023113251 0.026337996][1.0038823 1.036231 1.0396199 0.90375346 0.62556344 0.35992843 0.094343618 -0.10168812 -0.24519645 -0.26795277 -0.229198 -0.17426755 -0.11491218 -0.064346395 -0.011966906][1.091825 1.1236995 1.2173059 1.2299469 1.0724287 0.8852247 0.60749871 0.34455019 0.09801 -0.018081989 -0.075944327 -0.0864317 -0.081126064 -0.064435787 -0.029537968][1.1656588 1.1771773 1.339123 1.4762996 1.4708412 1.422855 1.22547 0.92078334 0.5500437 0.3065263 0.13338891 0.04851561 -0.015387287 -0.03809014 -0.039053287][1.1832125 1.1899774 1.3618939 1.5562619 1.6861676 1.7822111 1.68306 1.4609005 1.0844359 0.74528438 0.42259377 0.20497836 0.042965867 -0.042527057 -0.04991857][1.1272999 1.1342123 1.2945724 1.4667965 1.6026915 1.7521594 1.7995611 1.7211002 1.437719 1.1302708 0.75477374 0.43336546 0.1632777 -0.0035146661 -0.075867295][1.0469085 1.0052387 1.0935315 1.2129611 1.3321921 1.4864906 1.56109 1.5644774 1.4330359 1.2427475 0.922481 0.60482472 0.31287104 0.095824689 -0.023598256][0.79627353 0.77947593 0.8552267 0.88852948 0.94348747 1.05815 1.1646695 1.2365234 1.1812927 1.0623987 0.81720465 0.55047011 0.28881854 0.095030665 0.0096833631][0.45447236 0.44791442 0.51990324 0.57950652 0.61015958 0.64770383 0.73024833 0.82052743 0.82997012 0.77377135 0.57745427 0.37257141 0.1797273 0.0173674 -0.058955498][0.19006911 0.12566698 0.13842559 0.17074862 0.21035196 0.27139866 0.31920612 0.38447976 0.44429225 0.45999479 0.35825062 0.21037512 0.054040544 -0.05014969 -0.0809608][-0.033897951 -0.089098707 -0.12103518 -0.14915851 -0.17599103 -0.14373419 -0.054234907 0.013586368 0.059987113 0.096198842 0.079844989 0.035264947 -0.031531386 -0.080675349 -0.091559023][-0.17934982 -0.23444672 -0.21018997 -0.21785489 -0.26759049 -0.30114633 -0.29159492 -0.24308087 -0.16870566 -0.1033432 -0.084017232 -0.083665855 -0.10055166 -0.094792172 -0.089957461][-0.17810814 -0.18621372 -0.1745059 -0.16203284 -0.14813626 -0.18427382 -0.21305798 -0.236015 -0.23346567 -0.19291516 -0.14472246 -0.11848365 -0.097239621 -0.091172785 -0.081221588]]...]
INFO - root - 2017-12-06 04:35:18.465568: step 1710, loss = 0.89, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 21h:27m:01s remains)
INFO - root - 2017-12-06 04:35:20.787650: step 1720, loss = 0.86, batch loss = 0.65 (34.7 examples/sec; 0.230 sec/batch; 21h:10m:40s remains)
INFO - root - 2017-12-06 04:35:23.049178: step 1730, loss = 0.87, batch loss = 0.66 (35.1 examples/sec; 0.228 sec/batch; 20h:57m:10s remains)
INFO - root - 2017-12-06 04:35:25.411220: step 1740, loss = 0.88, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:03s remains)
INFO - root - 2017-12-06 04:35:27.669461: step 1750, loss = 0.82, batch loss = 0.61 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:59s remains)
INFO - root - 2017-12-06 04:35:29.930225: step 1760, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.232 sec/batch; 21h:20m:42s remains)
INFO - root - 2017-12-06 04:35:32.231992: step 1770, loss = 0.86, batch loss = 0.65 (34.2 examples/sec; 0.234 sec/batch; 21h:30m:45s remains)
INFO - root - 2017-12-06 04:35:34.511709: step 1780, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 21h:01m:36s remains)
INFO - root - 2017-12-06 04:35:36.784313: step 1790, loss = 0.85, batch loss = 0.64 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:50s remains)
INFO - root - 2017-12-06 04:35:39.108247: step 1800, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 21h:09m:15s remains)
2017-12-06 04:35:39.401524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028345764 -0.0027076006 -0.035018213 -0.059739389 -0.073437966 -0.080593333 -0.077902742 -0.08340092 -0.086777978 -0.086819619 -0.0895357 -0.081095293 -0.079487488 -0.10084707 -0.12541488][-9.5043331e-05 -0.037505489 -0.072327763 -0.094237059 -0.10379516 -0.10380198 -0.093875229 -0.087377444 -0.084738359 -0.081997879 -0.08102981 -0.078511864 -0.077085033 -0.093806863 -0.12255438][-0.030205205 -0.064567819 -0.094460696 -0.11557759 -0.12224723 -0.11214804 -0.092318073 -0.080523938 -0.07530304 -0.071117416 -0.068084314 -0.064370841 -0.060889989 -0.073734514 -0.095153868][-0.057975635 -0.085241817 -0.10237158 -0.11163945 -0.10652983 -0.093121558 -0.074526355 -0.060750313 -0.055558637 -0.054256812 -0.050950572 -0.042337097 -0.033021528 -0.039298821 -0.059367977][-0.060441814 -0.0825735 -0.092221193 -0.094072305 -0.085728094 -0.067228265 -0.043640207 -0.029362053 -0.026982291 -0.029088732 -0.026095666 -0.015750449 -0.0032268986 -0.0043177307 -0.023987386][-0.036734611 -0.053879816 -0.058962446 -0.057891387 -0.047920942 -0.031589925 -0.0092678368 0.0051830262 0.0053363554 0.0020857677 0.0037749521 0.0147698 0.028830767 0.032242991 0.012492787][0.0085777789 -0.0014479868 -0.00051644072 0.0022744052 0.0081596561 0.020420015 0.039241076 0.046595544 0.038852483 0.031500846 0.033511132 0.04542499 0.061010517 0.073440351 0.057314537][0.042022683 0.034358986 0.039075531 0.051341593 0.060550444 0.071820341 0.081303023 0.083288305 0.073401526 0.05718226 0.050918445 0.05649703 0.073406823 0.093482964 0.0912746][0.049598068 0.043803103 0.048521705 0.061072744 0.071079239 0.083927415 0.093082286 0.0873859 0.073221147 0.057044119 0.050634317 0.053375922 0.068915658 0.092794009 0.10482972][0.040031135 0.034444995 0.034847982 0.044422768 0.050157495 0.056169443 0.059344314 0.054982148 0.04276254 0.024602532 0.014685757 0.018151078 0.031620428 0.0499259 0.06688197][0.013460919 0.0076601058 0.0080767609 0.015428409 0.021338224 0.024513841 0.02716817 0.023379602 0.015997063 0.0099836625 0.0058577396 0.0057009906 0.0091962554 0.016690675 0.031431489][-0.01594488 -0.0201377 -0.020432327 -0.017415382 -0.015958531 -0.015124058 -0.015826989 -0.017695069 -0.019321179 -0.019152371 -0.021180566 -0.01470132 -0.00789357 -0.0042279474 0.004513897][-0.0337761 -0.035768051 -0.035670768 -0.03436479 -0.033148736 -0.030845029 -0.029527139 -0.032782957 -0.035559773 -0.033690341 -0.036206827 -0.027872043 -0.022190157 -0.024719188 -0.024284467][-0.069680624 -0.069073476 -0.067919962 -0.0675472 -0.066887736 -0.0679449 -0.06931679 -0.0669004 -0.066419132 -0.064312093 -0.070849463 -0.073090717 -0.072509438 -0.072936654 -0.070732921][-0.13377972 -0.1346164 -0.13417235 -0.13317734 -0.1323819 -0.13167968 -0.12994146 -0.12905276 -0.12887558 -0.11876287 -0.12442241 -0.1280001 -0.12799236 -0.12909679 -0.12932923]]...]
INFO - root - 2017-12-06 04:35:41.723677: step 1810, loss = 0.87, batch loss = 0.66 (34.4 examples/sec; 0.232 sec/batch; 21h:20m:27s remains)
INFO - root - 2017-12-06 04:35:44.033551: step 1820, loss = 0.89, batch loss = 0.68 (31.0 examples/sec; 0.258 sec/batch; 23h:44m:25s remains)
INFO - root - 2017-12-06 04:35:46.321895: step 1830, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 20h:30m:53s remains)
INFO - root - 2017-12-06 04:35:48.595051: step 1840, loss = 0.90, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:51m:20s remains)
INFO - root - 2017-12-06 04:35:50.895108: step 1850, loss = 0.87, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 21h:18m:51s remains)
INFO - root - 2017-12-06 04:35:53.203765: step 1860, loss = 0.87, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:59s remains)
INFO - root - 2017-12-06 04:35:55.548357: step 1870, loss = 0.79, batch loss = 0.58 (32.4 examples/sec; 0.247 sec/batch; 22h:39m:03s remains)
INFO - root - 2017-12-06 04:35:57.842918: step 1880, loss = 0.85, batch loss = 0.64 (33.9 examples/sec; 0.236 sec/batch; 21h:41m:01s remains)
INFO - root - 2017-12-06 04:36:00.183697: step 1890, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 21h:18m:26s remains)
INFO - root - 2017-12-06 04:36:02.480540: step 1900, loss = 0.81, batch loss = 0.60 (33.8 examples/sec; 0.237 sec/batch; 21h:45m:10s remains)
2017-12-06 04:36:02.767821: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0065496154 0.0070524588 0.006981384 0.0068462193 0.0066225864 0.0065148361 0.006520886 0.0064713843 0.0064394362 0.0064562336 0.0065424107 0.0065771155 0.0065718368 0.0065841563 0.0061878413][0.0068742223 0.007482823 0.0073987246 0.0072515793 0.0070502609 0.0068850182 0.0068041272 0.0067010634 0.0066275522 0.0065895878 0.0066852309 0.0068356209 0.0069650337 0.0070340224 0.0066593252][0.0065118447 0.007333301 0.0073477514 0.0072526895 0.0070876218 0.0069412068 0.0067751892 0.006547939 0.0062858686 0.0060521998 0.0059870072 0.006066937 0.0062400103 0.0063648149 0.0061312579][0.005964946 0.007007014 0.00707623 0.0069728605 0.0067176968 0.0063958466 0.0060283765 0.00557597 0.0050245859 0.0044850484 0.0041425042 0.0040967613 0.0042692982 0.0045106038 0.0045810416][0.005512692 0.006600868 0.0066279322 0.0064656734 0.0060845837 0.0055523105 0.0048818476 0.004055921 0.0031564608 0.0023655742 0.0018785372 0.0018525645 0.0021976531 0.002790112 0.003364116][0.0050749406 0.0063299164 0.0064380467 0.0063482039 0.0060018115 0.0053914189 0.0045239143 0.0033312105 0.0020152554 0.00090586394 0.00032641366 0.00051785633 0.0011776164 0.0020995215 0.0029189587][0.00489486 0.0062893331 0.0066997372 0.006935399 0.0069809854 0.006718073 0.0060213208 0.004714895 0.0030067936 0.0014949888 0.00063319132 0.00073277578 0.0013590865 0.0023406856 0.0032623447][0.004711803 0.0062957518 0.00683444 0.007281132 0.0075931437 0.0075985715 0.0070621967 0.0058483779 0.0040975064 0.0023166537 0.00099752843 0.00067658722 0.0011852756 0.0023373514 0.0033798888][0.0046185516 0.0059301592 0.0060658529 0.006073717 0.0058901086 0.0052412637 0.0041802377 0.0026261881 0.00090385973 -0.00041542947 -0.000977587 -0.00049086288 0.00066004694 0.0022305734 0.0034027435][0.0045167282 0.0055602975 0.0053662248 0.0049513653 0.004365623 0.0034812056 0.00243349 0.0012010224 0.00012693554 -0.00047520548 -0.00042100996 0.00034829229 0.00154946 0.0030306205 0.0038614683][0.0044165254 0.0055219792 0.0055184178 0.0053365156 0.0050044246 0.0044191033 0.0036639757 0.0027842298 0.0020587295 0.0016451143 0.0016324557 0.002220802 0.0031073131 0.0041742213 0.0045230612][0.0043792538 0.0055994876 0.00583601 0.0059572347 0.0059796609 0.0058247708 0.0055210814 0.0050360262 0.0045861639 0.0042847097 0.0041945726 0.0045159757 0.0050074197 0.0055969208 0.0054256953][0.0042757057 0.0055673681 0.0058775432 0.0062013753 0.0064651668 0.0066363364 0.0066389441 0.0064816289 0.0062663481 0.0060870647 0.0060004406 0.006102711 0.0062968843 0.0065173469 0.0059995428][0.0043197535 0.00557572 0.0057833828 0.0060216412 0.0062884949 0.006481763 0.0065952577 0.0066169947 0.0066029467 0.0065427758 0.0065425597 0.0066034384 0.0066689216 0.0067607537 0.006166134][0.0040647537 0.0052311122 0.005343426 0.0054761693 0.0056661479 0.0058087558 0.0058905929 0.0059482306 0.0060408562 0.0061439388 0.0062395819 0.0063550659 0.0064880475 0.0065871254 0.0061130673]]...]
INFO - root - 2017-12-06 04:36:05.076087: step 1910, loss = 0.87, batch loss = 0.66 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:21s remains)
INFO - root - 2017-12-06 04:36:07.339448: step 1920, loss = 0.88, batch loss = 0.67 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-06 04:36:09.646542: step 1930, loss = 0.86, batch loss = 0.66 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:04s remains)
INFO - root - 2017-12-06 04:36:11.918355: step 1940, loss = 0.85, batch loss = 0.64 (33.7 examples/sec; 0.237 sec/batch; 21h:47m:03s remains)
INFO - root - 2017-12-06 04:36:14.233908: step 1950, loss = 0.84, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:55s remains)
INFO - root - 2017-12-06 04:36:16.527809: step 1960, loss = 0.86, batch loss = 0.65 (34.0 examples/sec; 0.236 sec/batch; 21h:37m:26s remains)
INFO - root - 2017-12-06 04:36:18.843064: step 1970, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:54s remains)
INFO - root - 2017-12-06 04:36:21.158458: step 1980, loss = 0.81, batch loss = 0.61 (33.7 examples/sec; 0.238 sec/batch; 21h:48m:40s remains)
INFO - root - 2017-12-06 04:36:23.434479: step 1990, loss = 0.88, batch loss = 0.67 (35.8 examples/sec; 0.223 sec/batch; 20h:29m:23s remains)
INFO - root - 2017-12-06 04:36:25.737783: step 2000, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 20h:58m:16s remains)
2017-12-06 04:36:26.032819: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.6621 0.44002876 0.21564141 0.075866833 0.02710228 0.0045003965 -0.01142896 -0.014642458 -0.021816509 -0.026783336 -0.021987483 -0.029775577 -0.013203684 0.044024941 0.17316981][0.43769914 0.19124091 -0.0086890757 -0.11712511 -0.12179101 -0.12747738 -0.13464245 -0.15477723 -0.17974351 -0.19963567 -0.20824447 -0.23146759 -0.23243557 -0.18357579 -0.064765327][0.22743365 -0.040975917 -0.25703144 -0.33590037 -0.32420543 -0.31052423 -0.31096205 -0.32568789 -0.34776145 -0.37546945 -0.39269516 -0.40457737 -0.38776016 -0.31388506 -0.21462813][0.18426731 -0.0874443 -0.282167 -0.3381919 -0.33470032 -0.3468287 -0.37080154 -0.39405909 -0.41983378 -0.44334856 -0.46308342 -0.48403263 -0.48359117 -0.42010862 -0.34396061][0.089327678 -0.17115428 -0.31368962 -0.32577702 -0.31544626 -0.31313175 -0.32307255 -0.33025584 -0.3378571 -0.34965372 -0.36831373 -0.39537415 -0.43179846 -0.40179923 -0.36956698][0.031260516 -0.25164303 -0.36376858 -0.34152812 -0.3214834 -0.30360308 -0.29101503 -0.2749345 -0.26296762 -0.26193762 -0.27902466 -0.31262493 -0.36033675 -0.34904686 -0.3392548][0.020925034 -0.25676134 -0.37435919 -0.34793258 -0.32219526 -0.29881835 -0.27898863 -0.25261039 -0.22815478 -0.21842666 -0.23825982 -0.28483748 -0.34129989 -0.34470668 -0.33270463][0.00639572 -0.2560946 -0.36513084 -0.3368755 -0.31241 -0.292026 -0.27946946 -0.25903338 -0.23837064 -0.23043373 -0.25541651 -0.31169018 -0.36629787 -0.3746711 -0.33989319][-0.01537424 -0.26937559 -0.33960515 -0.31462938 -0.2876831 -0.27738634 -0.2888523 -0.30126691 -0.30546153 -0.31250003 -0.34218076 -0.39736465 -0.43638793 -0.43597233 -0.40666071][-0.0044765808 -0.23565786 -0.30865702 -0.29482752 -0.26945713 -0.28010026 -0.32578713 -0.37338543 -0.4047637 -0.42770439 -0.4589608 -0.51076 -0.54230648 -0.52975947 -0.48907033][0.014257934 -0.21191142 -0.33164847 -0.34396136 -0.33216584 -0.35481784 -0.40878451 -0.46789226 -0.50821412 -0.53778505 -0.57054287 -0.6149267 -0.63687891 -0.61262077 -0.52923745][-0.041700855 -0.24487698 -0.39294428 -0.44418997 -0.44408077 -0.46666044 -0.51041251 -0.558975 -0.59213924 -0.61662865 -0.64121324 -0.6738798 -0.68489516 -0.65260649 -0.53963774][-0.090312749 -0.29704213 -0.46117795 -0.54150051 -0.54800463 -0.5690738 -0.59979659 -0.63037854 -0.64794689 -0.66079766 -0.67322266 -0.69230729 -0.69149107 -0.63960034 -0.52043152][0.076556474 -0.14649665 -0.32237819 -0.42094627 -0.43470597 -0.47239059 -0.51094341 -0.52594388 -0.52940786 -0.54673231 -0.55611151 -0.56504762 -0.56137258 -0.501618 -0.42017257][0.34720165 0.11040959 -0.045707416 -0.12461954 -0.13464493 -0.16160728 -0.19303526 -0.20635103 -0.20889574 -0.2164157 -0.21842122 -0.24848379 -0.2733787 -0.21400388 -0.15721232]]...]
INFO - root - 2017-12-06 04:36:28.307133: step 2010, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.229 sec/batch; 20h:59m:51s remains)
INFO - root - 2017-12-06 04:36:30.616347: step 2020, loss = 0.86, batch loss = 0.65 (35.3 examples/sec; 0.226 sec/batch; 20h:47m:06s remains)
INFO - root - 2017-12-06 04:36:32.892857: step 2030, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.234 sec/batch; 21h:31m:23s remains)
INFO - root - 2017-12-06 04:36:35.200849: step 2040, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 21h:20m:36s remains)
INFO - root - 2017-12-06 04:36:37.497176: step 2050, loss = 0.86, batch loss = 0.65 (35.3 examples/sec; 0.226 sec/batch; 20h:46m:41s remains)
INFO - root - 2017-12-06 04:36:39.808534: step 2060, loss = 0.77, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:16s remains)
INFO - root - 2017-12-06 04:36:42.042398: step 2070, loss = 0.84, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:20s remains)
INFO - root - 2017-12-06 04:36:44.316759: step 2080, loss = 0.90, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:03s remains)
INFO - root - 2017-12-06 04:36:46.582770: step 2090, loss = 0.83, batch loss = 0.62 (35.6 examples/sec; 0.225 sec/batch; 20h:37m:00s remains)
INFO - root - 2017-12-06 04:36:48.888414: step 2100, loss = 0.85, batch loss = 0.64 (33.4 examples/sec; 0.239 sec/batch; 21h:57m:55s remains)
2017-12-06 04:36:49.187235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035247911 -0.048376847 -0.072474688 -0.10708205 -0.15132701 -0.18973133 -0.22545035 -0.26993996 -0.30146146 -0.31937474 -0.32297289 -0.31784409 -0.29612204 -0.25723135 -0.21338013][-0.013644274 -0.019265935 -0.032985996 -0.048455372 -0.064084075 -0.087635033 -0.11896887 -0.16608128 -0.21498393 -0.26867074 -0.31527269 -0.34946096 -0.35988754 -0.34823549 -0.32765341][0.0086190253 0.020941734 0.024032138 0.020813704 0.013984986 -0.0030283779 -0.034968212 -0.078614496 -0.13390227 -0.19702171 -0.26569468 -0.32880467 -0.37178737 -0.3948378 -0.39798772][0.014173917 0.040895209 0.061219394 0.0739254 0.081718639 0.074975394 0.04809317 0.0010631792 -0.061295338 -0.13145161 -0.20019014 -0.26623273 -0.32002592 -0.36183167 -0.38342893][-0.0041936524 0.037747867 0.075238258 0.10340293 0.12377332 0.12827964 0.11160673 0.072431155 0.015382051 -0.051078163 -0.12263822 -0.19095328 -0.2483729 -0.30050325 -0.33303779][-0.060259145 -0.0050994828 0.04868301 0.089767069 0.12122163 0.13666621 0.13335106 0.10878611 0.064671189 0.0095604211 -0.052844312 -0.113804 -0.16524777 -0.20881532 -0.23872821][-0.135123 -0.072404794 -0.0079871714 0.045712024 0.088457808 0.11384268 0.12208951 0.10786207 0.075592965 0.0318823 -0.019441644 -0.068810053 -0.10960354 -0.14342451 -0.16241311][-0.18835026 -0.13089621 -0.064378537 -0.0028828345 0.050599717 0.091072306 0.11551347 0.11531425 0.093981609 0.06137681 0.01965978 -0.022641022 -0.057635672 -0.089955807 -0.11399846][-0.19438671 -0.15416631 -0.10382658 -0.051620007 -0.001370851 0.04626818 0.085009038 0.10533015 0.10207145 0.084972873 0.055040047 0.020362586 -0.010624927 -0.036738638 -0.062119327][-0.17385396 -0.15236533 -0.11954023 -0.084130794 -0.046988651 -0.0038961507 0.042141594 0.080933571 0.10236125 0.10403681 0.090435535 0.066572204 0.039144389 0.017349645 -0.0087609254][-0.18455501 -0.1757416 -0.1540361 -0.12651299 -0.098018244 -0.063053019 -0.016395152 0.034108803 0.076452076 0.10321061 0.11302173 0.10839106 0.094591722 0.072032973 0.045483373][-0.22255 -0.22603679 -0.21499313 -0.19210029 -0.16504225 -0.13669726 -0.092108615 -0.03815715 0.018379204 0.066719517 0.10146984 0.12103745 0.12174393 0.11243117 0.093052149][-0.23674256 -0.26921791 -0.28295571 -0.27387762 -0.25597388 -0.2344552 -0.19332336 -0.14015536 -0.075658 -0.012209579 0.046722524 0.092502818 0.11550474 0.12324458 0.11086217][-0.20156875 -0.25873816 -0.29784435 -0.31789339 -0.32594955 -0.31846714 -0.29310063 -0.2553308 -0.19865066 -0.1351029 -0.06813281 -0.009043742 0.032829702 0.054485902 0.056457922][-0.13662346 -0.20489177 -0.25936991 -0.30068994 -0.33188778 -0.34905386 -0.35114688 -0.3464601 -0.31746531 -0.2742686 -0.21363765 -0.15058312 -0.096661985 -0.057949357 -0.035222404]]...]
INFO - root - 2017-12-06 04:36:51.478846: step 2110, loss = 0.90, batch loss = 0.69 (33.0 examples/sec; 0.243 sec/batch; 22h:15m:51s remains)
INFO - root - 2017-12-06 04:36:53.788798: step 2120, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:23s remains)
INFO - root - 2017-12-06 04:36:56.056853: step 2130, loss = 0.84, batch loss = 0.63 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:40s remains)
INFO - root - 2017-12-06 04:36:58.317845: step 2140, loss = 0.88, batch loss = 0.67 (34.7 examples/sec; 0.231 sec/batch; 21h:09m:18s remains)
INFO - root - 2017-12-06 04:37:00.601349: step 2150, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.224 sec/batch; 20h:32m:00s remains)
INFO - root - 2017-12-06 04:37:02.904518: step 2160, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:26s remains)
INFO - root - 2017-12-06 04:37:05.228750: step 2170, loss = 0.82, batch loss = 0.61 (33.1 examples/sec; 0.242 sec/batch; 22h:12m:18s remains)
INFO - root - 2017-12-06 04:37:07.540071: step 2180, loss = 0.89, batch loss = 0.68 (34.1 examples/sec; 0.234 sec/batch; 21h:30m:11s remains)
INFO - root - 2017-12-06 04:37:09.829611: step 2190, loss = 0.90, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-06 04:37:12.123418: step 2200, loss = 0.87, batch loss = 0.66 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:28s remains)
2017-12-06 04:37:12.412504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40255341 -0.39107782 -0.30663183 -0.17936921 -0.0786045 -0.0404017 -0.053319212 -0.08823546 -0.11286974 -0.12828758 -0.13686301 -0.13385873 -0.11189726 -0.10149193 -0.11203799][-0.37593162 -0.32945597 -0.22702724 -0.10077305 -0.010864366 0.025906302 0.016910844 -0.00988115 -0.0358254 -0.035501048 -0.026104441 -0.0055657476 0.029231064 0.052924454 0.063012034][-0.24713847 -0.1741527 -0.069400676 0.0368338 0.10595884 0.13906696 0.13141191 0.11515433 0.098568089 0.094386376 0.090023659 0.10657028 0.15803504 0.20240831 0.224412][-0.063181683 0.023129337 0.11418176 0.19222134 0.24207002 0.27883425 0.28852931 0.28879359 0.28143007 0.27279994 0.2506623 0.23762086 0.2471512 0.27394161 0.30941832][0.12264308 0.22956699 0.32091823 0.37574202 0.40398183 0.42887571 0.43060738 0.4180336 0.39629465 0.36808255 0.33127078 0.30467495 0.30526778 0.33267909 0.38182816][0.24665093 0.363997 0.45587602 0.51106107 0.54163265 0.56694221 0.57330096 0.56324607 0.53489262 0.48628423 0.42264664 0.37200069 0.352954 0.371834 0.43220913][0.2704913 0.38954404 0.48542973 0.54109055 0.57278681 0.60161829 0.62167424 0.62125611 0.5926789 0.53012258 0.43716466 0.35779169 0.30833963 0.3107259 0.38877553][0.18842876 0.31100595 0.42807502 0.51693708 0.56851971 0.60107976 0.60311776 0.58393621 0.53798115 0.46483675 0.3636719 0.26230302 0.19408929 0.17957243 0.26063651][0.075754322 0.18221948 0.29735491 0.39401814 0.45700845 0.50318474 0.50746733 0.47541651 0.41460532 0.33683908 0.23973855 0.12897989 0.060477182 0.02347596 0.087561779][-0.027751686 0.044807516 0.13732663 0.22399881 0.2909987 0.31973457 0.30231398 0.25147355 0.1867038 0.11579099 0.063826367 0.00039390847 -0.03521074 -0.060062151 0.0016478673][-0.074279249 -0.044091165 0.0029255003 0.050342567 0.089436136 0.10397746 0.08431565 0.037490681 -0.017311972 -0.071254984 -0.096867725 -0.1180402 -0.11793423 -0.12866828 -0.094743185][-0.078564517 -0.073426276 -0.058080498 -0.044582486 -0.035843767 -0.039255697 -0.05236489 -0.072405577 -0.094131157 -0.11013804 -0.10896122 -0.10417762 -0.095246509 -0.10051464 -0.098311454][-0.075654 -0.068967164 -0.067904122 -0.064715713 -0.071489654 -0.083978511 -0.10123847 -0.11381124 -0.12136107 -0.12047693 -0.1142932 -0.10234122 -0.088975221 -0.084308408 -0.0825575][-0.072351441 -0.07162337 -0.067475975 -0.057224777 -0.057427008 -0.065352216 -0.077062838 -0.088104047 -0.094816834 -0.092684284 -0.0885725 -0.082331225 -0.074635483 -0.069114916 -0.066682845][-0.070297547 -0.070415847 -0.070841596 -0.068275094 -0.068661794 -0.068441108 -0.067726195 -0.067400321 -0.067003176 -0.0679468 -0.064766504 -0.0632803 -0.061726093 -0.058106348 -0.050481629]]...]
INFO - root - 2017-12-06 04:37:14.802168: step 2210, loss = 0.89, batch loss = 0.68 (33.8 examples/sec; 0.237 sec/batch; 21h:42m:01s remains)
INFO - root - 2017-12-06 04:37:17.106757: step 2220, loss = 0.91, batch loss = 0.70 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:45s remains)
INFO - root - 2017-12-06 04:37:19.398154: step 2230, loss = 0.82, batch loss = 0.61 (33.7 examples/sec; 0.237 sec/batch; 21h:45m:25s remains)
INFO - root - 2017-12-06 04:37:21.714201: step 2240, loss = 0.82, batch loss = 0.61 (35.8 examples/sec; 0.223 sec/batch; 20h:30m:00s remains)
INFO - root - 2017-12-06 04:37:23.994063: step 2250, loss = 0.80, batch loss = 0.59 (33.0 examples/sec; 0.242 sec/batch; 22h:13m:56s remains)
INFO - root - 2017-12-06 04:37:26.283958: step 2260, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:19s remains)
INFO - root - 2017-12-06 04:37:28.541942: step 2270, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:26s remains)
INFO - root - 2017-12-06 04:37:30.837127: step 2280, loss = 0.81, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 21h:23m:34s remains)
INFO - root - 2017-12-06 04:37:33.086539: step 2290, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.224 sec/batch; 20h:35m:10s remains)
INFO - root - 2017-12-06 04:37:35.346411: step 2300, loss = 0.83, batch loss = 0.62 (36.2 examples/sec; 0.221 sec/batch; 20h:15m:52s remains)
2017-12-06 04:37:35.648143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.089118674 -0.14567697 -0.22211216 -0.2975879 -0.36229855 -0.41163522 -0.44600382 -0.4655726 -0.46897259 -0.46328756 -0.46433628 -0.44484645 -0.42632234 -0.36851716 -0.28180897][-0.11201133 -0.20070063 -0.31649506 -0.42369363 -0.512446 -0.57314461 -0.61839485 -0.64342582 -0.64464104 -0.63100696 -0.62062943 -0.59049362 -0.55267179 -0.46846092 -0.33192074][-0.10689504 -0.20559996 -0.32196322 -0.42068431 -0.48820806 -0.52796435 -0.56406766 -0.57959205 -0.57188892 -0.54385638 -0.5201683 -0.47376493 -0.40858665 -0.27992639 -0.08985012][-0.057830542 -0.12053689 -0.19012289 -0.22310877 -0.21596698 -0.19695616 -0.1925575 -0.17990689 -0.16177382 -0.14274234 -0.12836771 -0.091815658 -0.038083844 0.067869291 0.24760252][-0.0080786012 -0.017512195 -0.026131688 0.016463403 0.099303052 0.16607046 0.2017976 0.23947302 0.25391924 0.2643103 0.27049261 0.28831813 0.31994948 0.39378342 0.52146709][0.024924267 0.070838973 0.1237607 0.22145671 0.3368955 0.41535193 0.43782598 0.44932765 0.44049329 0.42979804 0.4128041 0.42167914 0.45628327 0.52495795 0.62560308][0.033261385 0.10787371 0.19448119 0.31921613 0.4431119 0.52640432 0.53770661 0.534794 0.50816184 0.48549718 0.462676 0.44382086 0.45444477 0.49558818 0.58058095][-0.012876559 0.057501454 0.14381133 0.25505206 0.36033753 0.43927893 0.44742996 0.43763763 0.41365689 0.38464564 0.36547542 0.34416217 0.34770727 0.36719757 0.4186919][-0.068412378 -0.03739417 0.01562373 0.083232224 0.14133176 0.1944671 0.20513427 0.20355821 0.18758474 0.16429389 0.15782382 0.1235137 0.11126019 0.11291546 0.12213735][-0.085605584 -0.090390615 -0.083367981 -0.063062906 -0.047565609 -0.025873348 -0.023983218 -0.028910017 -0.031637639 -0.044609047 -0.048320182 -0.065924652 -0.068498172 -0.071420535 -0.078337133][-0.074982464 -0.08366029 -0.093396962 -0.098216273 -0.1011424 -0.096258745 -0.0953691 -0.089017831 -0.0767317 -0.073337846 -0.065562233 -0.070658423 -0.081081711 -0.082031347 -0.12757562][-0.067935675 -0.069864579 -0.072359949 -0.075705752 -0.078727 -0.077571243 -0.079535224 -0.073754042 -0.063525386 -0.058659736 -0.045132272 -0.028934972 -0.024960015 -0.010701578 -0.063025393][-0.068439543 -0.069943428 -0.071374469 -0.072634019 -0.073449552 -0.0735534 -0.0686051 -0.05737054 -0.046207219 -0.034781545 -0.022367137 -0.0066729 0.0090860315 0.0383335 0.011528883][-0.068685472 -0.069772653 -0.070585072 -0.071070671 -0.0712957 -0.070598125 -0.068750523 -0.065751128 -0.058373943 -0.050581541 -0.037504166 -0.0081922635 0.0117728 0.058492202 0.065286607][-0.069189489 -0.069677055 -0.069864154 -0.070084006 -0.070249662 -0.069642857 -0.073395342 -0.070918083 -0.07212612 -0.068323761 -0.060162976 -0.034672767 0.00094585866 0.055877324 0.10291433]]...]
INFO - root - 2017-12-06 04:37:37.914408: step 2310, loss = 0.81, batch loss = 0.60 (34.9 examples/sec; 0.229 sec/batch; 21h:02m:58s remains)
INFO - root - 2017-12-06 04:37:40.213952: step 2320, loss = 0.84, batch loss = 0.63 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:45s remains)
INFO - root - 2017-12-06 04:37:42.532286: step 2330, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:59m:31s remains)
INFO - root - 2017-12-06 04:37:44.833142: step 2340, loss = 0.86, batch loss = 0.65 (34.7 examples/sec; 0.230 sec/batch; 21h:07m:48s remains)
INFO - root - 2017-12-06 04:37:47.138390: step 2350, loss = 0.90, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:06m:40s remains)
INFO - root - 2017-12-06 04:37:49.397300: step 2360, loss = 0.79, batch loss = 0.58 (34.6 examples/sec; 0.232 sec/batch; 21h:13m:59s remains)
INFO - root - 2017-12-06 04:37:51.677652: step 2370, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:01s remains)
INFO - root - 2017-12-06 04:37:53.987643: step 2380, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:34s remains)
INFO - root - 2017-12-06 04:37:56.251640: step 2390, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 21h:22m:17s remains)
INFO - root - 2017-12-06 04:37:58.536598: step 2400, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:40s remains)
2017-12-06 04:37:58.828889: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.69550616 0.90348136 0.94334728 0.77397442 0.50158805 0.19327672 -0.067500092 -0.18318225 -0.2429845 -0.25146577 -0.17571299 -0.13074234 -0.1227622 -0.20714994 -0.36316583][0.4360778 0.54485559 0.46611106 0.23619126 -0.091398813 -0.42260325 -0.66938972 -0.75792563 -0.76847935 -0.71591437 -0.60335767 -0.54649293 -0.53086358 -0.58057547 -0.63796347][0.20479725 0.19953944 0.026051085 -0.27242863 -0.595317 -0.85197449 -0.97933382 -0.99672997 -0.94645983 -0.88475072 -0.81688833 -0.77188182 -0.78607196 -0.77864486 -0.76531774][0.019850884 -0.015273418 -0.17856978 -0.4740954 -0.78087693 -0.98851895 -1.0542585 -1.0367309 -0.97327983 -0.89259446 -0.83266538 -0.81041127 -0.84549093 -0.76977456 -0.70232373][0.026319701 -0.0083821975 -0.13876911 -0.35272363 -0.575817 -0.70477009 -0.73653758 -0.72801304 -0.71428424 -0.7134577 -0.74009722 -0.77313668 -0.7880103 -0.6687035 -0.5116874][0.31066117 0.25489816 0.15107542 -0.012577452 -0.18238662 -0.2786507 -0.30158433 -0.30098078 -0.32537737 -0.39208591 -0.485918 -0.57119524 -0.58222139 -0.4197585 -0.26355392][0.65781784 0.58396029 0.48432714 0.34963083 0.20409499 0.10263389 0.049863134 0.0028958544 -0.082658619 -0.19969928 -0.33268374 -0.44006738 -0.4495863 -0.29923466 -0.11147241][0.90322125 0.7760486 0.6438356 0.49298996 0.34614384 0.226375 0.14443304 0.063482851 -0.055043578 -0.20409235 -0.35059744 -0.4535915 -0.4452906 -0.31914523 -0.10347737][0.98210108 0.7940858 0.59570503 0.40030673 0.22158913 0.082228959 -0.022264689 -0.1261961 -0.24676423 -0.38717407 -0.52246481 -0.6037451 -0.58564764 -0.44383326 -0.29727164][0.85003209 0.59031641 0.34991038 0.11306652 -0.10837553 -0.26685637 -0.38282922 -0.48544002 -0.59123105 -0.70339924 -0.79777694 -0.82981986 -0.80442047 -0.65305442 -0.50425446][0.53907889 0.25950003 0.0031833388 -0.25436014 -0.49750289 -0.66662616 -0.78860652 -0.8828969 -0.96260035 -1.0320079 -1.0767947 -1.0646225 -1.0031583 -0.84246832 -0.700274][0.12717319 -0.15636513 -0.39741981 -0.6307894 -0.86362338 -1.0145884 -1.1162952 -1.1807897 -1.2231244 -1.2603465 -1.2773134 -1.2362642 -1.1349144 -0.95599276 -0.80609035][-0.26177227 -0.55026591 -0.76253819 -0.96624488 -1.1678402 -1.2841299 -1.3538836 -1.3802179 -1.3875458 -1.3970776 -1.3913033 -1.3379911 -1.2277792 -1.0475861 -0.89306355][-0.47734392 -0.80034679 -1.0047479 -1.1605177 -1.309827 -1.379317 -1.4071589 -1.4109719 -1.4096895 -1.4147252 -1.4118605 -1.3634319 -1.2580185 -1.1009542 -0.96591][-0.41208628 -0.7297467 -0.96865362 -1.1057063 -1.1745046 -1.1716551 -1.1398501 -1.0902286 -1.0580792 -1.0532299 -1.060258 -1.0614133 -1.0172054 -0.93950838 -0.86285973]]...]
INFO - root - 2017-12-06 04:38:01.108703: step 2410, loss = 0.87, batch loss = 0.66 (32.6 examples/sec; 0.246 sec/batch; 22h:32m:07s remains)
INFO - root - 2017-12-06 04:38:03.425187: step 2420, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:04s remains)
INFO - root - 2017-12-06 04:38:05.708451: step 2430, loss = 0.86, batch loss = 0.65 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:37s remains)
INFO - root - 2017-12-06 04:38:08.037321: step 2440, loss = 0.85, batch loss = 0.65 (35.3 examples/sec; 0.227 sec/batch; 20h:46m:14s remains)
INFO - root - 2017-12-06 04:38:10.340918: step 2450, loss = 0.83, batch loss = 0.62 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:34s remains)
INFO - root - 2017-12-06 04:38:12.626688: step 2460, loss = 0.89, batch loss = 0.68 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:25s remains)
INFO - root - 2017-12-06 04:38:14.938211: step 2470, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:58s remains)
INFO - root - 2017-12-06 04:38:17.207350: step 2480, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:05s remains)
INFO - root - 2017-12-06 04:38:19.528997: step 2490, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 20h:56m:09s remains)
INFO - root - 2017-12-06 04:38:21.826067: step 2500, loss = 0.79, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 21h:08m:13s remains)
2017-12-06 04:38:22.102708: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018414784 0.019034881 0.018929351 0.018760744 0.018577565 0.018415879 0.018324111 0.018286351 0.018300835 0.018345144 0.018404428 0.018441763 0.018423814 0.01824249 0.017218504][0.019170996 0.0199585 0.019805375 0.019549858 0.019280795 0.019033644 0.018899079 0.018863905 0.018901672 0.018975254 0.019049291 0.019084092 0.019015554 0.018815454 0.017718498][0.019781273 0.02090947 0.020744864 0.020434808 0.020115521 0.019807097 0.019625638 0.019592952 0.019673932 0.019757535 0.01982433 0.019826394 0.019689482 0.01943114 0.018250879][0.020708945 0.021993224 0.021889482 0.021578815 0.021242362 0.02089173 0.020674352 0.020616714 0.02070548 0.020766895 0.020792391 0.02073494 0.02049135 0.020076338 0.018775601][0.021591563 0.023037996 0.02308533 0.022870522 0.022576284 0.022209246 0.021934312 0.021831352 0.021887314 0.021883208 0.021821294 0.021658111 0.021299 0.020728316 0.019175556][0.02234159 0.024017725 0.024239454 0.024125736 0.023897719 0.02353644 0.023178574 0.023005966 0.022993732 0.022929091 0.022758227 0.022457313 0.021982413 0.021263119 0.019495][0.022997122 0.024750311 0.025125328 0.025123108 0.024993587 0.024660792 0.024231736 0.023932878 0.023817975 0.023668576 0.023384828 0.022974882 0.022388097 0.021563765 0.01955327][0.023443561 0.025171969 0.025649991 0.025744576 0.025645819 0.025313582 0.024835389 0.024391968 0.024143513 0.023892719 0.023536455 0.023076985 0.022461262 0.021620866 0.019435596][0.023351673 0.025033493 0.025520835 0.025647726 0.025507007 0.025213551 0.024756547 0.024252843 0.023889285 0.02354994 0.023165438 0.022711184 0.022134516 0.021368917 0.019234959][0.022694122 0.02449334 0.024937455 0.025035847 0.024831761 0.024513368 0.024064418 0.023572143 0.023154516 0.022785086 0.02241208 0.022001665 0.021514755 0.02087098 0.018811125][0.021783423 0.023666006 0.024012122 0.024058823 0.023809981 0.023502562 0.023069169 0.022589583 0.022186641 0.02186374 0.021526355 0.021181736 0.020776439 0.020274166 0.018387992][0.020804983 0.022577647 0.022837024 0.022866149 0.022628885 0.022344973 0.021956872 0.021522906 0.02114537 0.020865116 0.020606842 0.02033845 0.020036016 0.01961508 0.017890852][0.019615952 0.021317188 0.021475818 0.02150498 0.021304596 0.021084059 0.020794068 0.020476107 0.020202871 0.019990128 0.019798141 0.019604538 0.019393619 0.019038837 0.017514635][0.018555012 0.020137969 0.020222079 0.020237166 0.02009837 0.019945305 0.019764487 0.019565497 0.019384596 0.019236621 0.019100588 0.018973757 0.018848229 0.018592726 0.017155174][0.01631793 0.017828386 0.01792087 0.018044535 0.018080708 0.018115889 0.01810756 0.017985869 0.01786634 0.017767314 0.017686877 0.017608661 0.017603915 0.017474856 0.016177144]]...]
INFO - root - 2017-12-06 04:38:24.390015: step 2510, loss = 0.91, batch loss = 0.70 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:15s remains)
INFO - root - 2017-12-06 04:38:26.692508: step 2520, loss = 0.85, batch loss = 0.65 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:58s remains)
INFO - root - 2017-12-06 04:38:29.016076: step 2530, loss = 0.83, batch loss = 0.62 (33.8 examples/sec; 0.237 sec/batch; 21h:42m:13s remains)
INFO - root - 2017-12-06 04:38:31.298548: step 2540, loss = 0.83, batch loss = 0.62 (34.3 examples/sec; 0.233 sec/batch; 21h:21m:45s remains)
INFO - root - 2017-12-06 04:38:33.647466: step 2550, loss = 0.83, batch loss = 0.62 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:50s remains)
INFO - root - 2017-12-06 04:38:35.936097: step 2560, loss = 0.86, batch loss = 0.65 (33.6 examples/sec; 0.238 sec/batch; 21h:48m:23s remains)
INFO - root - 2017-12-06 04:38:38.235023: step 2570, loss = 0.82, batch loss = 0.61 (35.7 examples/sec; 0.224 sec/batch; 20h:30m:31s remains)
INFO - root - 2017-12-06 04:38:40.582651: step 2580, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:17s remains)
INFO - root - 2017-12-06 04:38:42.860053: step 2590, loss = 0.82, batch loss = 0.61 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:36s remains)
INFO - root - 2017-12-06 04:38:45.172531: step 2600, loss = 0.89, batch loss = 0.68 (34.0 examples/sec; 0.235 sec/batch; 21h:32m:51s remains)
2017-12-06 04:38:45.474395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.27340993 -0.26003623 -0.26173362 -0.24866831 -0.2350376 -0.22663048 -0.20817 -0.19817063 -0.17170632 -0.1488204 -0.13734302 -0.12477063 -0.13952771 -0.15357673 -0.16066271][-0.26899269 -0.2404584 -0.23636681 -0.22025844 -0.20347381 -0.183665 -0.15671639 -0.13569456 -0.10487343 -0.082858413 -0.074877352 -0.081085995 -0.098227441 -0.11299314 -0.11668523][-0.24136192 -0.20775902 -0.19987273 -0.17989781 -0.1605183 -0.14083259 -0.1132376 -0.080237061 -0.043056265 -0.023581643 -0.025129694 -0.0382715 -0.056129239 -0.081019089 -0.085827641][-0.19000638 -0.13489936 -0.12133199 -0.10021396 -0.083831355 -0.068006776 -0.039825812 -0.0014808439 0.037066497 0.058428824 0.045235828 0.00066247955 -0.03613003 -0.068950564 -0.069177561][-0.15740287 -0.086804196 -0.0720132 -0.048212506 -0.034738906 -0.023520168 0.0033772029 0.042275481 0.089296453 0.11782805 0.10566344 0.053143054 0.0037031546 -0.040176354 -0.044984438][-0.130907 -0.055882603 -0.027468249 -0.00066163391 0.013584144 0.02281487 0.047840469 0.086395688 0.1324057 0.1607776 0.14631534 0.086247467 0.021792419 -0.031765968 -0.041999564][-0.11693542 -0.043413192 -0.011352085 0.018288992 0.036538713 0.048818879 0.072113596 0.10894649 0.15073806 0.17598107 0.16350761 0.11387401 0.049470849 -0.011311311 -0.035121627][-0.11657276 -0.057727959 -0.01695542 0.014722213 0.039789289 0.061040439 0.090535067 0.12791494 0.16014197 0.17186189 0.15278211 0.10824846 0.05418361 -0.00562435 -0.038371019][-0.10595708 -0.066301681 -0.034488898 -0.0052626282 0.020579003 0.048351474 0.085993536 0.12631798 0.14703754 0.14189339 0.11522413 0.075860582 0.032791056 -0.011140682 -0.041410409][-0.089444995 -0.070633769 -0.046612434 -0.02332167 0.0010253973 0.0369791 0.083328657 0.12218023 0.13305598 0.11564795 0.082980894 0.050018966 0.0230074 -0.003479816 -0.02715886][-0.066235565 -0.071702391 -0.06488616 -0.048021816 -0.023824036 0.012484983 0.057693608 0.097075589 0.1028602 0.081741013 0.051404767 0.025414541 0.006855391 -0.005626414 -0.015197683][-0.043222647 -0.061786793 -0.06669718 -0.059703689 -0.04340478 -0.0092099495 0.03168314 0.059198089 0.060419768 0.043836884 0.022229083 0.005044505 -0.0077962205 -0.015953034 -0.018645663][-0.029234257 -0.045034051 -0.055054665 -0.054485548 -0.042260677 -0.02100325 0.0028403141 0.020764038 0.023817167 0.013764113 0.0005401969 -0.0095717013 -0.015868962 -0.018849079 -0.020049851][-0.024277683 -0.031969931 -0.038506221 -0.040242802 -0.035353966 -0.024228763 -0.012161326 -0.0044408478 -0.0029468946 -0.0081154518 -0.014361452 -0.018702891 -0.021875832 -0.023062851 -0.024150912][-0.024963852 -0.027246505 -0.028997 -0.030016482 -0.028989214 -0.026275316 -0.022597708 -0.020392213 -0.017125413 -0.019260291 -0.019586097 -0.02185883 -0.025636885 -0.026486438 -0.027680602]]...]
INFO - root - 2017-12-06 04:38:47.791845: step 2610, loss = 0.84, batch loss = 0.63 (33.7 examples/sec; 0.238 sec/batch; 21h:46m:01s remains)
INFO - root - 2017-12-06 04:38:50.097960: step 2620, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 21h:02m:05s remains)
INFO - root - 2017-12-06 04:38:52.372622: step 2630, loss = 0.83, batch loss = 0.62 (34.9 examples/sec; 0.230 sec/batch; 21h:01m:58s remains)
INFO - root - 2017-12-06 04:38:54.674451: step 2640, loss = 0.85, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 21h:27m:52s remains)
INFO - root - 2017-12-06 04:38:56.966524: step 2650, loss = 0.85, batch loss = 0.64 (33.0 examples/sec; 0.243 sec/batch; 22h:14m:22s remains)
INFO - root - 2017-12-06 04:38:59.253935: step 2660, loss = 0.88, batch loss = 0.67 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:03s remains)
INFO - root - 2017-12-06 04:39:01.527489: step 2670, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 20h:55m:28s remains)
INFO - root - 2017-12-06 04:39:03.817206: step 2680, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:19s remains)
INFO - root - 2017-12-06 04:39:06.122733: step 2690, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:22s remains)
INFO - root - 2017-12-06 04:39:08.438502: step 2700, loss = 0.83, batch loss = 0.62 (34.6 examples/sec; 0.231 sec/batch; 21h:10m:30s remains)
2017-12-06 04:39:08.757490: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0756537 1.2939059 1.2528154 1.0513507 0.85140431 0.6893822 0.59463376 0.52249205 0.46560216 0.50055224 0.55568051 0.56352168 0.4843418 0.36812431 0.24375898][1.6197716 1.7024268 1.4965432 1.0603044 0.61967027 0.30003166 0.1163559 0.025678813 0.04358314 0.17844017 0.38420749 0.58094829 0.67212266 0.67207766 0.59681827][1.8720236 1.8031914 1.3884596 0.73542863 0.032280713 -0.53488857 -0.86069453 -0.96754968 -0.86236244 -0.57827264 -0.17387062 0.23335409 0.52471471 0.678435 0.70347655][1.8281856 1.6658683 1.1275785 0.34166756 -0.47321111 -1.1227033 -1.4661245 -1.5545876 -1.3940672 -1.036411 -0.54939884 -0.086299963 0.26491505 0.45539671 0.51172787][1.3344818 1.0614884 0.5098756 -0.1939183 -0.88998342 -1.3632793 -1.5520996 -1.4889842 -1.2370186 -0.89037663 -0.48764992 -0.14135668 0.080431953 0.13307278 0.082419366][0.77003127 0.47857136 0.047742553 -0.40926594 -0.75074524 -0.88202959 -0.79076272 -0.58926731 -0.34288421 -0.13807672 0.00071332976 0.059461951 0.030430503 -0.092035271 -0.24911551][0.19302475 -0.01336446 -0.22453886 -0.33224937 -0.21988015 0.099585652 0.509081 0.87973452 1.0704032 1.045818 0.80477917 0.46878493 0.10984087 -0.22634917 -0.47190315][-0.31955275 -0.40395793 -0.39231947 -0.1987066 0.25922459 0.91516209 1.5901774 2.0501356 2.1724081 1.928018 1.4059914 0.775375 0.19028668 -0.25622898 -0.53567195][-0.78323638 -0.78317183 -0.64311224 -0.28803128 0.33024949 1.112151 1.8605294 2.3407543 2.4053757 2.0553737 1.4093899 0.68793064 0.0845032 -0.3191987 -0.51726949][-1.1217964 -1.0875633 -0.91645771 -0.52622664 0.087677374 0.824635 1.4928569 1.9066156 1.9123701 1.5638009 1.0088353 0.42407504 -0.047112692 -0.32845896 -0.43098086][-1.1840075 -1.1578207 -1.0345707 -0.74322426 -0.27725187 0.27404764 0.78500587 1.0632677 1.0212514 0.72219515 0.30813769 -0.055560011 -0.28966504 -0.36720049 -0.34409019][-0.97278422 -0.98467416 -0.98496336 -0.89845788 -0.65973765 -0.3207466 0.01760111 0.20582759 0.17913608 0.010522246 -0.21545033 -0.35224447 -0.42969605 -0.38760227 -0.28554505][-0.52001572 -0.56594396 -0.69930196 -0.82287043 -0.83135355 -0.68741047 -0.49255595 -0.36398533 -0.3515262 -0.39117903 -0.45778668 -0.42940289 -0.37994879 -0.28569028 -0.19176291][0.13808171 0.037973896 -0.24022229 -0.5630933 -0.78216058 -0.81035972 -0.69513386 -0.58119029 -0.53576308 -0.51011539 -0.48177278 -0.36541277 -0.28520203 -0.1814201 -0.11309505][0.77090549 0.68849736 0.34088764 -0.13218378 -0.54810971 -0.716155 -0.67689216 -0.56468582 -0.46864247 -0.41211653 -0.35637653 -0.24712726 -0.15568849 -0.0695877 -0.025690969]]...]
INFO - root - 2017-12-06 04:39:11.037797: step 2710, loss = 0.88, batch loss = 0.67 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:46s remains)
INFO - root - 2017-12-06 04:39:13.354667: step 2720, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:33s remains)
INFO - root - 2017-12-06 04:39:15.639391: step 2730, loss = 0.78, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 21h:15m:31s remains)
INFO - root - 2017-12-06 04:39:17.900351: step 2740, loss = 0.90, batch loss = 0.69 (34.6 examples/sec; 0.231 sec/batch; 21h:10m:24s remains)
INFO - root - 2017-12-06 04:39:20.175798: step 2750, loss = 0.87, batch loss = 0.66 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:13s remains)
INFO - root - 2017-12-06 04:39:22.495194: step 2760, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:52s remains)
INFO - root - 2017-12-06 04:39:24.841186: step 2770, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 21h:11m:26s remains)
INFO - root - 2017-12-06 04:39:27.127167: step 2780, loss = 0.85, batch loss = 0.64 (34.0 examples/sec; 0.235 sec/batch; 21h:32m:56s remains)
INFO - root - 2017-12-06 04:39:29.388243: step 2790, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.224 sec/batch; 20h:28m:53s remains)
INFO - root - 2017-12-06 04:39:31.710691: step 2800, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 21h:30m:30s remains)
2017-12-06 04:39:32.012075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.26245072 -0.33968151 -0.32943514 -0.26663747 -0.16005044 -0.046141908 0.045595836 0.1157418 0.14497194 0.1404314 0.11887172 0.080633804 0.039321173 0.012217913 -0.030148229][-0.32665691 -0.41417304 -0.40798262 -0.33974481 -0.22558552 -0.1131182 -0.034887157 -0.0081524588 -0.035253152 -0.088934153 -0.15307967 -0.21688148 -0.26177612 -0.27696118 -0.2945042][-0.30499765 -0.40418318 -0.41090006 -0.34993812 -0.24249592 -0.13518205 -0.065018624 -0.06241513 -0.1198716 -0.20463111 -0.2971788 -0.37628862 -0.42414209 -0.43361521 -0.430295][-0.21294795 -0.31112772 -0.32118636 -0.26481029 -0.16335699 -0.056061409 0.017585818 0.018299755 -0.042587634 -0.14404516 -0.2491632 -0.33364776 -0.38075659 -0.39090359 -0.37746575][-0.070477575 -0.154163 -0.16540219 -0.12511937 -0.050456155 0.028810341 0.086011052 0.090537667 0.044912081 -0.043096602 -0.1319903 -0.19577704 -0.21845286 -0.21248023 -0.19185479][0.077621251 0.0048550107 -0.019109778 -0.014024634 0.0067600571 0.02872264 0.040881794 0.032352779 -0.0048558526 -0.069778815 -0.12939009 -0.15803711 -0.14438692 -0.10694869 -0.0654828][0.18124308 0.11103664 0.065660357 0.028749611 -0.013567321 -0.060950991 -0.11257182 -0.15726587 -0.20431441 -0.24975358 -0.27837503 -0.27730694 -0.2370345 -0.17445542 -0.11368153][0.18566628 0.13166375 0.085303351 0.029038917 -0.053593524 -0.15300092 -0.256607 -0.33913955 -0.40102124 -0.43231264 -0.43560022 -0.41475073 -0.37074572 -0.31623563 -0.25923255][0.10936372 0.091398358 0.078912944 0.045497958 -0.027801961 -0.13141952 -0.2432801 -0.335974 -0.40021139 -0.42594159 -0.4236441 -0.40360168 -0.37765536 -0.35614637 -0.3396143][-0.014366526 0.0049348362 0.039799143 0.05637851 0.027574029 -0.0335863 -0.10572587 -0.16656785 -0.20544991 -0.21872821 -0.21754505 -0.21081889 -0.21309464 -0.23731741 -0.2795282][-0.12716101 -0.081168287 -0.0068814084 0.049929719 0.063064113 0.050410915 0.034445364 0.034084391 0.051048685 0.072716191 0.088295504 0.086048007 0.054266613 -0.014501516 -0.11487102][-0.10754001 -0.071985036 -0.0017227568 0.056604255 0.0821756 0.092624858 0.1144602 0.16756551 0.24051644 0.31278378 0.36582968 0.3842341 0.35631785 0.27444029 0.14444397][0.057731126 0.045010332 0.066923246 0.088711396 0.095943585 0.10820259 0.15090032 0.23973812 0.35442775 0.47878003 0.57834816 0.63161212 0.62997174 0.56229687 0.4326852][0.28628927 0.20871918 0.17868736 0.15644617 0.14108539 0.15717931 0.2157049 0.31813082 0.43873051 0.57134575 0.69149363 0.755703 0.76532423 0.69526017 0.57237][0.4337877 0.31385529 0.23860164 0.18233116 0.14588983 0.15584852 0.22432862 0.34257755 0.47311985 0.58396482 0.68554908 0.75308007 0.76050997 0.71794468 0.61889577]]...]
INFO - root - 2017-12-06 04:39:34.276837: step 2810, loss = 0.81, batch loss = 0.60 (35.2 examples/sec; 0.227 sec/batch; 20h:49m:45s remains)
INFO - root - 2017-12-06 04:39:36.607096: step 2820, loss = 0.77, batch loss = 0.56 (33.6 examples/sec; 0.238 sec/batch; 21h:48m:29s remains)
INFO - root - 2017-12-06 04:39:38.945552: step 2830, loss = 0.84, batch loss = 0.63 (33.1 examples/sec; 0.242 sec/batch; 22h:07m:24s remains)
INFO - root - 2017-12-06 04:39:41.238527: step 2840, loss = 0.87, batch loss = 0.66 (33.9 examples/sec; 0.236 sec/batch; 21h:36m:48s remains)
INFO - root - 2017-12-06 04:39:43.495080: step 2850, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:38s remains)
INFO - root - 2017-12-06 04:39:45.773924: step 2860, loss = 0.80, batch loss = 0.59 (35.9 examples/sec; 0.223 sec/batch; 20h:25m:22s remains)
INFO - root - 2017-12-06 04:39:48.097402: step 2870, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:56m:28s remains)
INFO - root - 2017-12-06 04:39:50.359031: step 2880, loss = 0.77, batch loss = 0.56 (36.0 examples/sec; 0.222 sec/batch; 20h:20m:56s remains)
INFO - root - 2017-12-06 04:39:52.672033: step 2890, loss = 0.78, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:09s remains)
INFO - root - 2017-12-06 04:39:54.983704: step 2900, loss = 0.85, batch loss = 0.64 (35.2 examples/sec; 0.228 sec/batch; 20h:50m:02s remains)
2017-12-06 04:39:55.284570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091099635 -0.090982907 -0.091530353 -0.093007639 -0.096012212 -0.10099217 -0.10687038 -0.11155778 -0.11279432 -0.10992586 -0.10432042 -0.098479144 -0.094303 -0.091866583 -0.090216115][-0.092070676 -0.092283219 -0.093888834 -0.098866522 -0.10992137 -0.12739179 -0.14689231 -0.1622764 -0.16754742 -0.15974608 -0.14140075 -0.11997736 -0.10371373 -0.09538722 -0.092090413][-0.092561178 -0.093115851 -0.096458569 -0.10748076 -0.12864146 -0.15288773 -0.17280775 -0.18968248 -0.20547977 -0.21111965 -0.19444284 -0.15890047 -0.12340879 -0.10211606 -0.094273642][-0.092857264 -0.093463406 -0.098364137 -0.11402848 -0.13435465 -0.13783038 -0.12256614 -0.11759026 -0.1478107 -0.19798902 -0.22100347 -0.19583827 -0.14793938 -0.11093938 -0.096018128][-0.093036532 -0.093374215 -0.098919481 -0.11498596 -0.11968693 -0.076870993 0.003372997 0.055335194 0.018487222 -0.092311218 -0.18817988 -0.20608672 -0.16567403 -0.11868259 -0.097155914][-0.093011856 -0.09281899 -0.098006718 -0.10987583 -0.089954942 0.014760219 0.18431044 0.31210482 0.28622273 0.10929806 -0.082803443 -0.17393556 -0.16530241 -0.12118972 -0.096953034][-0.093326777 -0.092631996 -0.097641885 -0.10708562 -0.070514284 0.080229186 0.32376692 0.5255264 0.52121097 0.29590878 0.026552305 -0.1286011 -0.15323108 -0.11688934 -0.092967466][-0.095384836 -0.094973668 -0.10194694 -0.1175002 -0.091479078 0.04837428 0.29080603 0.50824296 0.52351224 0.31145144 0.048968382 -0.10919649 -0.14005437 -0.10857135 -0.087159537][-0.10998622 -0.11688748 -0.13005327 -0.15148714 -0.14417832 -0.047808796 0.13722572 0.31201652 0.34112066 0.19922656 0.0081635565 -0.10873441 -0.1278573 -0.10009541 -0.081805214][-0.13847522 -0.15905179 -0.1808812 -0.20409447 -0.20561644 -0.14453124 -0.01894987 0.10563339 0.1400643 0.065164335 -0.04685469 -0.11456197 -0.11894119 -0.094568484 -0.080044538][-0.1464529 -0.16254035 -0.18503244 -0.21823677 -0.24185959 -0.21951106 -0.14357859 -0.059070256 -0.023050133 -0.04767222 -0.094646655 -0.11863276 -0.11214042 -0.093917511 -0.084194653][-0.10938588 -0.094881445 -0.1042336 -0.15272868 -0.21111488 -0.22832629 -0.19110575 -0.13379718 -0.099816874 -0.10054812 -0.11303055 -0.1143496 -0.10718454 -0.097800136 -0.092540488][-0.04055186 0.021514051 0.03871879 -0.023485385 -0.12461274 -0.1866183 -0.18150494 -0.13640912 -0.10038422 -0.092124283 -0.096105367 -0.096825615 -0.097265944 -0.095422447 -0.094009958][0.042699948 0.17043412 0.23528099 0.16515666 0.0084587559 -0.115098 -0.14993161 -0.11686774 -0.078698508 -0.069914952 -0.078328826 -0.085759208 -0.0903017 -0.09126462 -0.092676058][0.075468384 0.2466262 0.35266945 0.28477973 0.092605717 -0.07476002 -0.14134103 -0.12060173 -0.08060997 -0.067980945 -0.0750851 -0.083810218 -0.088938043 -0.090759762 -0.093251914]]...]
INFO - root - 2017-12-06 04:39:57.589809: step 2910, loss = 0.78, batch loss = 0.57 (35.0 examples/sec; 0.228 sec/batch; 20h:53m:57s remains)
INFO - root - 2017-12-06 04:39:59.864401: step 2920, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 20h:45m:03s remains)
INFO - root - 2017-12-06 04:40:02.186372: step 2930, loss = 0.86, batch loss = 0.65 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:00s remains)
INFO - root - 2017-12-06 04:40:04.466771: step 2940, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:01s remains)
INFO - root - 2017-12-06 04:40:06.756089: step 2950, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.230 sec/batch; 21h:00m:40s remains)
INFO - root - 2017-12-06 04:40:09.035659: step 2960, loss = 0.82, batch loss = 0.61 (34.4 examples/sec; 0.232 sec/batch; 21h:16m:54s remains)
INFO - root - 2017-12-06 04:40:11.330044: step 2970, loss = 0.89, batch loss = 0.68 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:29s remains)
INFO - root - 2017-12-06 04:40:13.584093: step 2980, loss = 0.90, batch loss = 0.69 (34.4 examples/sec; 0.232 sec/batch; 21h:16m:01s remains)
INFO - root - 2017-12-06 04:40:15.889234: step 2990, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 20h:44m:12s remains)
INFO - root - 2017-12-06 04:40:18.169964: step 3000, loss = 0.87, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 21h:06m:35s remains)
2017-12-06 04:40:18.441229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039586544 -0.039575472 -0.039900742 -0.039834678 -0.039012559 -0.038045838 -0.037394598 -0.036929317 -0.036360931 -0.035181731 -0.03429269 -0.034811635 -0.037366368 -0.041054949 -0.045025848][-0.038139358 -0.038429912 -0.039030533 -0.039337661 -0.039388329 -0.039548047 -0.039680608 -0.039893642 -0.039988693 -0.040112581 -0.041446757 -0.043602776 -0.046295911 -0.048565306 -0.050086729][-0.034849986 -0.035346039 -0.035846606 -0.036094382 -0.036262043 -0.036779147 -0.037655242 -0.038591377 -0.039487042 -0.0407343 -0.04284212 -0.045392957 -0.047872949 -0.049240533 -0.049698744][-0.030814324 -0.031436987 -0.031966615 -0.032144509 -0.032303378 -0.032921456 -0.034087978 -0.035679996 -0.037395537 -0.039525636 -0.041831926 -0.044003557 -0.045715123 -0.046523083 -0.046557419][-0.027172007 -0.027850244 -0.028500594 -0.028709598 -0.028643817 -0.0289075 -0.0296227 -0.031107686 -0.03346885 -0.036267214 -0.038779259 -0.040676415 -0.041814022 -0.042129032 -0.041995492][-0.025417771 -0.026101716 -0.026580609 -0.02639677 -0.025578093 -0.025016561 -0.024847146 -0.025913753 -0.028471597 -0.03205882 -0.03513813 -0.037271589 -0.038211476 -0.03833494 -0.038539641][-0.026210953 -0.026841968 -0.027093049 -0.026389938 -0.024670597 -0.022569876 -0.020298839 -0.019381046 -0.022354323 -0.027847558 -0.032070238 -0.034132645 -0.034835823 -0.035034619 -0.035927072][-0.028188959 -0.028408743 -0.028326366 -0.027315985 -0.025483988 -0.022698298 -0.018954806 -0.016516518 -0.01984987 -0.026350882 -0.030759852 -0.0321782 -0.03230894 -0.032266531 -0.033660334][-0.026979119 -0.026329186 -0.026044689 -0.025386516 -0.024231341 -0.022612236 -0.020775724 -0.020360406 -0.02269192 -0.026671275 -0.029766953 -0.030651782 -0.030679617 -0.030715004 -0.03217122][-0.02196341 -0.02120221 -0.021009304 -0.02067899 -0.019699037 -0.018761523 -0.018594071 -0.019365273 -0.020921815 -0.0233825 -0.026049074 -0.02803592 -0.029867906 -0.030940218 -0.033001803][-0.016822476 -0.016976763 -0.017230883 -0.017138481 -0.016379561 -0.015631873 -0.015651122 -0.016501572 -0.017640732 -0.019097313 -0.021485727 -0.024832118 -0.0283911 -0.031345159 -0.035066854][-0.017982479 -0.019383293 -0.02041015 -0.020998124 -0.020254344 -0.019058228 -0.01814229 -0.018254597 -0.019056667 -0.019422907 -0.020706169 -0.023101602 -0.025428951 -0.028284509 -0.033218782][-0.026366565 -0.028228365 -0.028949391 -0.028840359 -0.027928684 -0.026957851 -0.025384195 -0.024026033 -0.02293307 -0.022387017 -0.022760019 -0.023087773 -0.022925492 -0.023348849 -0.026585128][-0.038062789 -0.040482953 -0.041117296 -0.041042 -0.039999828 -0.037969507 -0.035333373 -0.032716174 -0.030413616 -0.029070694 -0.027535409 -0.024901118 -0.022634912 -0.020544291 -0.020386968][-0.0516487 -0.054417375 -0.055399928 -0.055529475 -0.054054104 -0.051235355 -0.048088554 -0.044978596 -0.042063143 -0.038425989 -0.034287442 -0.029843569 -0.026012748 -0.022163697 -0.020099595]]...]
INFO - root - 2017-12-06 04:40:20.758336: step 3010, loss = 0.90, batch loss = 0.69 (33.1 examples/sec; 0.242 sec/batch; 22h:09m:08s remains)
INFO - root - 2017-12-06 04:40:23.080871: step 3020, loss = 0.90, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:52s remains)
INFO - root - 2017-12-06 04:40:25.404852: step 3030, loss = 0.87, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:10s remains)
INFO - root - 2017-12-06 04:40:27.700599: step 3040, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 21h:28m:23s remains)
INFO - root - 2017-12-06 04:40:29.990526: step 3050, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:11s remains)
INFO - root - 2017-12-06 04:40:32.267779: step 3060, loss = 0.87, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 21h:07m:10s remains)
INFO - root - 2017-12-06 04:40:34.590018: step 3070, loss = 0.83, batch loss = 0.62 (35.0 examples/sec; 0.228 sec/batch; 20h:53m:15s remains)
INFO - root - 2017-12-06 04:40:36.851913: step 3080, loss = 0.85, batch loss = 0.64 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:46s remains)
INFO - root - 2017-12-06 04:40:39.176260: step 3090, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:09s remains)
INFO - root - 2017-12-06 04:40:41.512518: step 3100, loss = 0.83, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 20h:31m:32s remains)
2017-12-06 04:40:41.861302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0979611 -0.097878247 -0.097885251 -0.099446818 -0.10005307 -0.098549306 -0.10131751 -0.10644631 -0.11276945 -0.1192949 -0.1192296 -0.11048295 -0.1016032 -0.095487654 -0.098620117][-0.070379235 -0.064620085 -0.060889564 -0.06312117 -0.067407548 -0.06851624 -0.069443069 -0.067969948 -0.066808544 -0.068205088 -0.065256938 -0.0536409 -0.045972254 -0.043852769 -0.050875906][-0.030323319 -0.028451439 -0.024574749 -0.021103423 -0.021911006 -0.026300773 -0.034397744 -0.039842382 -0.041832976 -0.039922521 -0.032634012 -0.019691207 -0.006656345 0.0037650838 0.0033783466][0.0060464665 -1.8347055e-05 -0.0024510361 -0.0018634982 -0.0025121048 -0.0063336566 -0.015025873 -0.027987387 -0.038483169 -0.040218834 -0.031486049 -0.016237173 -0.0007398352 0.018744029 0.034423582][0.021808036 0.0091411471 0.0010064766 -0.0037831292 -0.0074648708 -0.012953352 -0.023818135 -0.040464431 -0.056870326 -0.063877217 -0.057152461 -0.03949292 -0.017962351 0.0055566132 0.035618819][0.0041363016 -0.011061192 -0.02009096 -0.022453871 -0.020833425 -0.019553252 -0.024035402 -0.038052075 -0.057921164 -0.074675411 -0.079951555 -0.072019942 -0.053423796 -0.027932148 0.016725764][-0.018718459 -0.027915828 -0.032391064 -0.030317875 -0.020891294 -0.005766537 0.0047205687 -0.00046402961 -0.020447403 -0.046553563 -0.0668287 -0.072570331 -0.064966738 -0.048101686 -0.014604785][-0.032264851 -0.033044323 -0.032695234 -0.027966205 -0.014146186 0.009903796 0.033327334 0.040290967 0.024428047 -0.006602522 -0.035287157 -0.047749795 -0.046831686 -0.042085625 -0.031401549][-0.024288766 -0.022592507 -0.022543434 -0.02079083 -0.011459898 0.010518201 0.035437576 0.045459881 0.030942403 0.0014905557 -0.024992865 -0.036111958 -0.035978891 -0.033073038 -0.030708237][-0.021972768 -0.019266929 -0.019051373 -0.019949201 -0.017885905 -0.00887296 0.0042608157 0.010290377 0.0025485903 -0.011974629 -0.024411619 -0.029702984 -0.029401604 -0.027987327 -0.027278416][-0.022282977 -0.020118456 -0.019903537 -0.020330697 -0.021706365 -0.021516629 -0.018507272 -0.016610853 -0.018220503 -0.022196371 -0.025977433 -0.027975146 -0.027641125 -0.026393458 -0.025520317][-0.022602681 -0.020822417 -0.020568021 -0.020323049 -0.021139383 -0.021757822 -0.022173192 -0.021937117 -0.0216938 -0.02239285 -0.023406196 -0.02468786 -0.025137331 -0.025241539 -0.025479324][-0.022549435 -0.021139525 -0.021118157 -0.021060027 -0.021175511 -0.020893384 -0.021010306 -0.020836767 -0.021242149 -0.02182411 -0.022323865 -0.023312528 -0.024015974 -0.024756636 -0.025463834][-0.023144554 -0.022016533 -0.022193357 -0.022351652 -0.022542857 -0.022377089 -0.022044439 -0.021692842 -0.021846823 -0.02205997 -0.022332799 -0.022994503 -0.023610663 -0.024409164 -0.025535069][-0.025258046 -0.024344001 -0.024722662 -0.025266159 -0.025779825 -0.026046574 -0.026199464 -0.026181497 -0.025980603 -0.025933027 -0.025815316 -0.025932007 -0.026113875 -0.026511457 -0.02733136]]...]
INFO - root - 2017-12-06 04:40:44.168130: step 3110, loss = 0.87, batch loss = 0.66 (34.4 examples/sec; 0.233 sec/batch; 21h:17m:52s remains)
INFO - root - 2017-12-06 04:40:46.447619: step 3120, loss = 0.86, batch loss = 0.65 (35.0 examples/sec; 0.229 sec/batch; 20h:54m:28s remains)
INFO - root - 2017-12-06 04:40:48.752128: step 3130, loss = 0.91, batch loss = 0.70 (34.3 examples/sec; 0.233 sec/batch; 21h:20m:41s remains)
INFO - root - 2017-12-06 04:40:51.069512: step 3140, loss = 0.84, batch loss = 0.63 (34.4 examples/sec; 0.232 sec/batch; 21h:15m:55s remains)
INFO - root - 2017-12-06 04:40:53.348823: step 3150, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:30s remains)
INFO - root - 2017-12-06 04:40:55.605434: step 3160, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:01s remains)
INFO - root - 2017-12-06 04:40:57.861224: step 3170, loss = 0.86, batch loss = 0.65 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:10s remains)
INFO - root - 2017-12-06 04:41:00.130617: step 3180, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:04s remains)
INFO - root - 2017-12-06 04:41:02.393240: step 3190, loss = 0.91, batch loss = 0.70 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:15s remains)
INFO - root - 2017-12-06 04:41:04.693235: step 3200, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:41m:04s remains)
2017-12-06 04:41:04.980715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15833369 -0.1759403 -0.18844634 -0.210776 -0.24067473 -0.26665124 -0.27155945 -0.24625704 -0.1892302 -0.15221372 -0.14034151 -0.13340753 -0.12794524 -0.1219831 -0.11798847][0.077806763 0.067261837 0.053994313 -0.0016956367 -0.11559954 -0.23513094 -0.32069814 -0.34330127 -0.3014206 -0.25884119 -0.22870359 -0.19834375 -0.18188226 -0.16124827 -0.13268387][0.41382048 0.39225575 0.34891018 0.22951773 -0.0055124238 -0.2695224 -0.48627409 -0.61287713 -0.62709892 -0.60616857 -0.56444985 -0.497036 -0.43266377 -0.35768995 -0.27147222][0.67836362 0.63847888 0.561851 0.38923559 0.07273563 -0.28045538 -0.60155135 -0.81763989 -0.88965064 -0.90941679 -0.89397734 -0.81865287 -0.72996175 -0.623362 -0.48610049][0.56890208 0.51745427 0.46186659 0.346123 0.10810911 -0.14873759 -0.38755223 -0.55323988 -0.60670769 -0.64662606 -0.67104363 -0.67738909 -0.68536663 -0.6427598 -0.561539][0.14218006 0.08967758 0.093405731 0.11959451 0.13332182 0.17622164 0.22243232 0.27159804 0.34923697 0.34805179 0.29761758 0.18004206 -0.022542197 -0.18637936 -0.30982187][-0.27426848 -0.29058155 -0.15979332 0.081479885 0.40874439 0.80401778 1.1631151 1.4558158 1.659903 1.7252909 1.6532102 1.4426308 1.0422192 0.61535376 0.21100929][-0.47267365 -0.43451712 -0.19562814 0.2092196 0.75838953 1.4004079 1.979205 2.4237275 2.6825581 2.7424343 2.6370089 2.3479407 1.8309559 1.2484044 0.67993563][-0.42770463 -0.38741067 -0.14129902 0.26103747 0.80473608 1.4138714 1.9616789 2.343544 2.5309429 2.560143 2.4656911 2.2425661 1.7936497 1.255832 0.73429674][-0.3297174 -0.33776975 -0.18581893 0.08694052 0.45367119 0.84975404 1.1917869 1.3986877 1.4381517 1.4108572 1.3472079 1.2429684 0.98953336 0.64348155 0.35463721][-0.26202634 -0.3312507 -0.31030065 -0.22154832 -0.090774722 0.056974836 0.20203522 0.25789303 0.24140283 0.20885938 0.17667598 0.12690675 -0.0058678128 -0.1427822 -0.20048678][-0.21194106 -0.29836437 -0.35685286 -0.39574084 -0.44222111 -0.489214 -0.50602931 -0.53621149 -0.56002021 -0.55710763 -0.541955 -0.53327775 -0.55007803 -0.53256321 -0.48081973][-0.16511849 -0.2512573 -0.3517063 -0.45657644 -0.60271913 -0.74706978 -0.84676188 -0.90079886 -0.92254126 -0.90077519 -0.83363068 -0.77535689 -0.70003772 -0.58841288 -0.47772005][-0.13364318 -0.19739011 -0.28428537 -0.38508993 -0.535479 -0.69459212 -0.82213277 -0.89489853 -0.91368514 -0.90330148 -0.84767634 -0.75891703 -0.64177424 -0.50999594 -0.3759259][-0.10187441 -0.13205299 -0.18390185 -0.24424145 -0.34455231 -0.45748764 -0.56786638 -0.65045846 -0.68386042 -0.6808216 -0.62848943 -0.54101104 -0.43790182 -0.30917636 -0.21842015]]...]
INFO - root - 2017-12-06 04:41:07.280608: step 3210, loss = 0.90, batch loss = 0.69 (35.6 examples/sec; 0.224 sec/batch; 20h:32m:05s remains)
INFO - root - 2017-12-06 04:41:09.583448: step 3220, loss = 0.82, batch loss = 0.61 (33.3 examples/sec; 0.240 sec/batch; 21h:58m:56s remains)
INFO - root - 2017-12-06 04:41:11.892234: step 3230, loss = 0.87, batch loss = 0.66 (32.7 examples/sec; 0.245 sec/batch; 22h:22m:13s remains)
INFO - root - 2017-12-06 04:41:14.176188: step 3240, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:41s remains)
INFO - root - 2017-12-06 04:41:16.448958: step 3250, loss = 0.83, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 21h:22m:43s remains)
INFO - root - 2017-12-06 04:41:18.767362: step 3260, loss = 1.00, batch loss = 0.79 (33.8 examples/sec; 0.236 sec/batch; 21h:37m:35s remains)
INFO - root - 2017-12-06 04:41:21.056398: step 3270, loss = 0.83, batch loss = 0.62 (35.2 examples/sec; 0.227 sec/batch; 20h:47m:27s remains)
INFO - root - 2017-12-06 04:41:23.408480: step 3280, loss = 0.77, batch loss = 0.56 (33.7 examples/sec; 0.237 sec/batch; 21h:42m:17s remains)
INFO - root - 2017-12-06 04:41:25.702651: step 3290, loss = 0.84, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 20h:57m:38s remains)
INFO - root - 2017-12-06 04:41:27.995264: step 3300, loss = 0.84, batch loss = 0.63 (34.4 examples/sec; 0.233 sec/batch; 21h:16m:33s remains)
2017-12-06 04:41:28.295992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091812827 -0.11571059 -0.11100522 -0.098352417 -0.084238067 -0.073677644 -0.065972954 -0.060708676 -0.05357904 -0.054893732 -0.064028382 -0.082737722 -0.11202317 -0.14924024 -0.19250903][-0.098755017 -0.15074751 -0.17290759 -0.18973148 -0.20281705 -0.2134499 -0.22233975 -0.22995678 -0.23538917 -0.24152276 -0.24711755 -0.25161451 -0.25283596 -0.25475204 -0.25990188][-0.066077188 -0.14216828 -0.18625551 -0.22703189 -0.26606587 -0.30376664 -0.33475059 -0.35611469 -0.36871678 -0.37717593 -0.37667829 -0.36011472 -0.32921907 -0.29136807 -0.2536439][-0.062059313 -0.13625705 -0.17453949 -0.21083385 -0.2501688 -0.29350704 -0.33350277 -0.36125541 -0.37647641 -0.38214207 -0.37711588 -0.35176837 -0.30682021 -0.25392839 -0.20342892][-0.096875116 -0.15384889 -0.1643533 -0.17296587 -0.19013029 -0.21912286 -0.25137177 -0.27354574 -0.28634733 -0.29058662 -0.28681281 -0.26904321 -0.23604518 -0.18883869 -0.14622389][-0.16450766 -0.20612192 -0.1854746 -0.15778492 -0.14037322 -0.1411404 -0.15039122 -0.15412632 -0.15363768 -0.15243389 -0.15074855 -0.14135289 -0.12056486 -0.09487 -0.07400164][-0.21375802 -0.24766806 -0.20414165 -0.14492214 -0.094080105 -0.067295946 -0.051950168 -0.036148038 -0.022082306 -0.01914493 -0.024734393 -0.028999537 -0.022735879 -0.011111196 -0.012618337][-0.19538176 -0.23971254 -0.18866438 -0.1132868 -0.042864762 0.0046508983 0.039898738 0.068127744 0.087475307 0.0877822 0.073565908 0.058964774 0.05427368 0.048543856 0.026579261][-0.13354872 -0.19070378 -0.15689206 -0.096478231 -0.029692598 0.018812582 0.058518447 0.091086365 0.1092113 0.10384452 0.079929374 0.047700226 0.02570454 0.0095275193 -0.014912803][-0.085653484 -0.1686458 -0.16149294 -0.13242245 -0.093072847 -0.064387448 -0.038712271 -0.020786397 -0.011850797 -0.026047125 -0.053012196 -0.085630529 -0.10406008 -0.11903654 -0.14204386][-0.11825823 -0.20815137 -0.2166284 -0.21751529 -0.21315208 -0.21557075 -0.2162886 -0.21485779 -0.21268594 -0.22637495 -0.24892363 -0.27262574 -0.28411365 -0.29029953 -0.29479849][-0.18069282 -0.26576447 -0.27311844 -0.28759998 -0.305893 -0.32776883 -0.34373546 -0.35182023 -0.35594547 -0.36679721 -0.38279629 -0.39581805 -0.39831558 -0.39690587 -0.39677414][-0.25146508 -0.32405391 -0.32009083 -0.32437822 -0.33234572 -0.3493062 -0.35854563 -0.36238447 -0.36278823 -0.36566183 -0.37517086 -0.38665262 -0.39512026 -0.4026382 -0.40253282][-0.30426368 -0.36155647 -0.34313083 -0.33318725 -0.324754 -0.32274035 -0.31744817 -0.30771738 -0.3003535 -0.29646859 -0.30876854 -0.33297977 -0.35709512 -0.37997824 -0.38323963][-0.31067 -0.36398888 -0.34586906 -0.32954016 -0.31486645 -0.303808 -0.29195845 -0.28769839 -0.27976587 -0.28185838 -0.28785339 -0.30873388 -0.33398643 -0.36760926 -0.38064462]]...]
INFO - root - 2017-12-06 04:41:30.575388: step 3310, loss = 0.83, batch loss = 0.62 (35.9 examples/sec; 0.223 sec/batch; 20h:23m:49s remains)
INFO - root - 2017-12-06 04:41:32.865362: step 3320, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:51m:43s remains)
INFO - root - 2017-12-06 04:41:35.189118: step 3330, loss = 0.82, batch loss = 0.61 (33.5 examples/sec; 0.239 sec/batch; 21h:50m:32s remains)
INFO - root - 2017-12-06 04:41:37.457101: step 3340, loss = 0.86, batch loss = 0.65 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:44s remains)
INFO - root - 2017-12-06 04:41:39.723191: step 3350, loss = 0.95, batch loss = 0.74 (34.5 examples/sec; 0.232 sec/batch; 21h:10m:17s remains)
INFO - root - 2017-12-06 04:41:42.022768: step 3360, loss = 0.90, batch loss = 0.69 (34.3 examples/sec; 0.233 sec/batch; 21h:18m:54s remains)
INFO - root - 2017-12-06 04:41:44.302547: step 3370, loss = 0.89, batch loss = 0.68 (35.0 examples/sec; 0.228 sec/batch; 20h:52m:40s remains)
INFO - root - 2017-12-06 04:41:46.564855: step 3380, loss = 0.98, batch loss = 0.77 (32.8 examples/sec; 0.244 sec/batch; 22h:19m:25s remains)
INFO - root - 2017-12-06 04:41:48.853705: step 3390, loss = 0.89, batch loss = 0.68 (35.8 examples/sec; 0.224 sec/batch; 20h:25m:58s remains)
INFO - root - 2017-12-06 04:41:51.160637: step 3400, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:30s remains)
2017-12-06 04:41:51.436926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0368072 -1.1603817 -1.362188 -1.4885825 -1.6265991 -1.813238 -1.903684 -1.958523 -1.8816364 -1.6863791 -1.5079409 -1.4122448 -1.3915255 -1.4325571 -1.3962119][-1.2981287 -1.546772 -1.7384113 -1.8052942 -1.8496455 -1.9738553 -1.9561579 -1.8077102 -1.5567651 -1.2554413 -0.99477535 -0.86998469 -0.81628484 -0.81515491 -0.8202275][-1.6502116 -1.8009895 -1.9495649 -2.0236356 -2.0321574 -2.0181415 -1.8808964 -1.5678862 -1.1789733 -0.79146457 -0.49923861 -0.3156825 -0.20394728 -0.13845465 -0.24569985][-1.5160524 -1.6744999 -1.8027531 -1.8609396 -1.8110995 -1.6682929 -1.4270219 -1.0374516 -0.56819832 -0.11689663 0.18077052 0.3670679 0.37735239 0.38098112 0.21184498][-1.0092037 -1.1034061 -1.2506154 -1.3513916 -1.3273593 -1.1797208 -0.87933421 -0.47064036 -0.053521071 0.35091859 0.6240921 0.77394938 0.71236974 0.6994918 0.53550625][-0.22350827 -0.26320049 -0.52048594 -0.69692707 -0.67941684 -0.53819042 -0.26744235 0.10656817 0.50645405 0.82262135 0.99191278 1.049996 1.0097251 0.86501557 0.67082036][0.649487 0.52330893 0.27595845 0.11460236 0.1208406 0.21721581 0.43994316 0.73804164 1.0210838 1.1996062 1.2637416 1.1819345 1.0181 0.74720097 0.483206][1.1336118 1.0537275 0.79959667 0.75545424 0.81990844 0.96889824 1.1589632 1.3483911 1.4911047 1.5343679 1.442289 1.2050035 0.91290563 0.50063258 0.14845064][1.2796123 1.192572 1.0511461 1.0727617 1.189001 1.3687866 1.5674036 1.704123 1.7268244 1.6284364 1.4193323 1.0816042 0.74180508 0.26573762 -0.063466504][1.2330647 1.0796033 1.0588657 1.0370395 1.1012429 1.3157927 1.5544765 1.652276 1.6292479 1.4641209 1.1354876 0.83023131 0.52730393 0.12673032 -0.17170137][0.40955004 0.35764304 0.32895228 0.28932974 0.4852902 0.66940612 0.82584131 0.93199354 0.95127994 0.79319865 0.53637123 0.19288969 -0.18107 -0.60197294 -0.9815256][-0.51275516 -0.5984239 -0.65292227 -0.58434445 -0.35297269 -0.22142422 -0.030808914 0.01134783 -0.094485551 -0.28571856 -0.49955955 -0.78168041 -1.0559407 -1.4979256 -1.8284385][-1.4819533 -1.5036852 -1.4416881 -1.4164362 -1.2496176 -1.0246364 -0.914091 -0.86549014 -0.89320147 -1.0555148 -1.2516586 -1.6178374 -1.8640859 -2.2540703 -2.4933743][-2.1254356 -2.1268222 -2.080245 -2.106369 -1.9365422 -1.8489928 -1.7696487 -1.6995246 -1.7109441 -1.8152853 -1.9699409 -2.2690129 -2.4557273 -2.7085228 -2.790123][-2.4724944 -2.6530645 -2.6387377 -2.6722009 -2.5965433 -2.5623069 -2.5150809 -2.5211933 -2.577517 -2.6454394 -2.7583203 -2.8004563 -2.845058 -2.7943125 -2.715107]]...]
INFO - root - 2017-12-06 04:41:53.709958: step 3410, loss = 0.96, batch loss = 0.75 (34.3 examples/sec; 0.233 sec/batch; 21h:20m:25s remains)
INFO - root - 2017-12-06 04:41:55.997214: step 3420, loss = 0.84, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 20h:35m:56s remains)
INFO - root - 2017-12-06 04:41:58.304199: step 3430, loss = 0.83, batch loss = 0.62 (36.1 examples/sec; 0.221 sec/batch; 20h:14m:06s remains)
INFO - root - 2017-12-06 04:42:00.625196: step 3440, loss = 0.93, batch loss = 0.72 (33.9 examples/sec; 0.236 sec/batch; 21h:33m:33s remains)
INFO - root - 2017-12-06 04:42:02.940469: step 3450, loss = 0.94, batch loss = 0.73 (33.8 examples/sec; 0.237 sec/batch; 21h:39m:47s remains)
INFO - root - 2017-12-06 04:42:05.202798: step 3460, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:36s remains)
INFO - root - 2017-12-06 04:42:07.546092: step 3470, loss = 0.85, batch loss = 0.64 (35.6 examples/sec; 0.225 sec/batch; 20h:33m:44s remains)
INFO - root - 2017-12-06 04:42:09.850948: step 3480, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.230 sec/batch; 20h:58m:31s remains)
INFO - root - 2017-12-06 04:42:12.128788: step 3490, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:35s remains)
INFO - root - 2017-12-06 04:42:14.432633: step 3500, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 21h:08m:21s remains)
2017-12-06 04:42:14.720547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051634859 -0.051891357 -0.051928319 -0.05179045 -0.051725194 -0.051538777 -0.051058669 -0.050403424 -0.050351135 -0.050858006 -0.051435746 -0.052065987 -0.052882165 -0.053443309 -0.053510204][-0.051573485 -0.052048765 -0.052045058 -0.051739939 -0.0513831 -0.050889127 -0.050176822 -0.049576361 -0.049916305 -0.050748527 -0.05161719 -0.052479982 -0.053445093 -0.054134931 -0.054081634][-0.050766911 -0.051534582 -0.051465113 -0.051034618 -0.050505564 -0.049714733 -0.0488922 -0.04844315 -0.04903486 -0.050061252 -0.051230621 -0.052424356 -0.053434856 -0.054064307 -0.053966492][-0.047788911 -0.04893871 -0.049112968 -0.048785202 -0.048152588 -0.047358777 -0.046764232 -0.046647348 -0.047439359 -0.048530176 -0.049936574 -0.051381364 -0.052482951 -0.053199477 -0.053349584][-0.040686738 -0.042392544 -0.04320043 -0.043333981 -0.04310045 -0.042778507 -0.042735592 -0.043011982 -0.043919653 -0.045024313 -0.046388626 -0.047946204 -0.049262285 -0.050204303 -0.050861172][-0.030753478 -0.032427903 -0.033915065 -0.0346903 -0.035075903 -0.035603352 -0.036378041 -0.03717795 -0.03801167 -0.038958889 -0.04007379 -0.041474849 -0.042984396 -0.044444866 -0.046078775][-0.01911661 -0.020588305 -0.022818234 -0.024462894 -0.025653318 -0.026965901 -0.028408565 -0.029614024 -0.030241068 -0.0305572 -0.03090062 -0.031619664 -0.033257149 -0.035659969 -0.038700063][-0.010024145 -0.010764521 -0.012954235 -0.015135974 -0.017128069 -0.019248091 -0.021140251 -0.022504628 -0.022597414 -0.021843828 -0.020980436 -0.020853303 -0.022331204 -0.025436599 -0.029947575][-0.0035057627 -0.0035030842 -0.0055023544 -0.0078733228 -0.010344092 -0.013250135 -0.015818227 -0.017266013 -0.017112996 -0.015756648 -0.013921209 -0.012860712 -0.013556007 -0.016420979 -0.021424022][0.0022603869 0.0028933585 0.0012413859 -0.0012541488 -0.0042491257 -0.0076091029 -0.010694504 -0.012546383 -0.012407701 -0.010933261 -0.0088245645 -0.0073952377 -0.0074610114 -0.0097907223 -0.014617924][0.0069594756 0.0079882517 0.0066669956 0.0041902363 0.0008925125 -0.0027646646 -0.0061283149 -0.0081126727 -0.0080941767 -0.00660149 -0.0046816133 -0.0033919923 -0.0033507347 -0.0053883456 -0.0097697191][0.0097292513 0.010514274 0.0090105832 0.0064934865 0.0031620786 -0.00039274246 -0.0035309829 -0.0051885284 -0.0049909912 -0.0035456419 -0.0019984916 -0.0010661185 -0.0012727007 -0.0032940879 -0.0075013824][0.010345906 0.01056207 0.0085245892 0.0057106242 0.002268903 -0.0012107268 -0.0039375424 -0.0051408708 -0.004577592 -0.0030471906 -0.0018198639 -0.0013450235 -0.0017370433 -0.0038225949 -0.0080311783][0.0084834173 0.0081310794 0.0056228936 0.00256031 -0.00088723749 -0.0041421168 -0.0065592006 -0.0074405372 -0.0067567155 -0.00509746 -0.003820356 -0.003409531 -0.0039277039 -0.0060384125 -0.01019175][0.0032762066 0.0027832389 0.00023171306 -0.0026338622 -0.0057327226 -0.00856477 -0.010684613 -0.011598766 -0.01105243 -0.0095490664 -0.0082640164 -0.0076873414 -0.0081905834 -0.010107167 -0.013761494]]...]
INFO - root - 2017-12-06 04:42:16.992521: step 3510, loss = 0.90, batch loss = 0.69 (32.9 examples/sec; 0.243 sec/batch; 22h:14m:19s remains)
INFO - root - 2017-12-06 04:42:19.257260: step 3520, loss = 0.92, batch loss = 0.71 (35.6 examples/sec; 0.225 sec/batch; 20h:32m:32s remains)
INFO - root - 2017-12-06 04:42:21.571619: step 3530, loss = 0.90, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 20h:59m:03s remains)
INFO - root - 2017-12-06 04:42:23.872902: step 3540, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:04s remains)
INFO - root - 2017-12-06 04:42:26.196975: step 3550, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:39m:16s remains)
INFO - root - 2017-12-06 04:42:28.471232: step 3560, loss = 0.93, batch loss = 0.72 (35.3 examples/sec; 0.226 sec/batch; 20h:41m:31s remains)
INFO - root - 2017-12-06 04:42:30.743595: step 3570, loss = 0.85, batch loss = 0.64 (32.6 examples/sec; 0.245 sec/batch; 22h:24m:29s remains)
INFO - root - 2017-12-06 04:42:33.027585: step 3580, loss = 0.84, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:14s remains)
INFO - root - 2017-12-06 04:42:35.310510: step 3590, loss = 0.90, batch loss = 0.69 (34.4 examples/sec; 0.233 sec/batch; 21h:14m:42s remains)
INFO - root - 2017-12-06 04:42:37.622567: step 3600, loss = 0.92, batch loss = 0.71 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:47s remains)
2017-12-06 04:42:37.921779: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25779083 0.14871663 0.024276555 -0.080080844 -0.20571536 -0.32366627 -0.43006936 -0.4795225 -0.39973262 -0.1791776 0.13648024 0.46265614 0.78001207 1.012465 1.1612445][0.4258979 0.24528673 0.10440994 -0.032269366 -0.21078381 -0.37722969 -0.48422575 -0.50647092 -0.38676876 -0.120886 0.23641893 0.57645339 0.85954446 1.0381259 1.1216283][0.70740086 0.43733323 0.2530148 0.083361305 -0.11951925 -0.31146556 -0.39673105 -0.36896729 -0.19163778 0.070401944 0.38676307 0.69192314 0.89232969 0.98503065 0.95530367][0.89365935 0.5662508 0.38175946 0.17952761 -0.019048128 -0.17389305 -0.23273292 -0.14387053 0.078175984 0.3581996 0.60788321 0.80393451 0.89498544 0.88607842 0.75479895][0.94292343 0.57744849 0.41650379 0.25429243 0.10884418 0.023756504 0.04800804 0.1964488 0.43041337 0.69976604 0.83890384 0.87108731 0.79734129 0.63287836 0.40297937][0.85428494 0.50542432 0.40208802 0.28072172 0.19893721 0.18156078 0.25835717 0.4188239 0.6436516 0.81554371 0.89873058 0.84600013 0.64670449 0.34888253 0.021213666][0.57035983 0.2438522 0.18143818 0.10831337 0.099004917 0.16658929 0.30997434 0.5180034 0.7179628 0.86672807 0.9164089 0.81459796 0.55616528 0.18132907 -0.25730175][0.15937817 -0.13375686 -0.16007981 -0.14489758 -0.057807315 0.12511367 0.38327026 0.63646781 0.84661233 0.98587465 0.9649992 0.78815 0.42717898 -0.044530079 -0.61576468][-0.26139724 -0.50787383 -0.48086855 -0.38393167 -0.21779194 0.019313388 0.31138361 0.60349476 0.83899879 0.9692167 0.97577095 0.77687347 0.36962214 -0.16709268 -0.7568332][-0.56311643 -0.756398 -0.70835137 -0.56265163 -0.3618795 -0.13114303 0.14540529 0.41614303 0.62647009 0.75179434 0.76289946 0.59495693 0.23686382 -0.24084008 -0.74569315][-0.74351883 -0.86712486 -0.80084151 -0.64830321 -0.48113391 -0.29454347 -0.069785759 0.15273491 0.32359907 0.43084121 0.42143196 0.2655772 -0.055261202 -0.44669071 -0.83305836][-0.98195404 -1.0756972 -1.0161247 -0.884439 -0.761091 -0.65668708 -0.51626945 -0.36640543 -0.25813594 -0.19574744 -0.21140394 -0.32987362 -0.56631333 -0.83645242 -1.0873415][-1.2640959 -1.369399 -1.3645971 -1.2856209 -1.2096742 -1.1453041 -1.0577236 -0.97353268 -0.92682987 -0.91791081 -0.953018 -1.0430585 -1.1913559 -1.327974 -1.4209206][-1.4123112 -1.5447488 -1.5856739 -1.5664904 -1.5385319 -1.5035722 -1.4598501 -1.4263269 -1.4300228 -1.4445279 -1.4790798 -1.5566961 -1.6484052 -1.6885393 -1.6809145][-1.2635206 -1.4220339 -1.5251846 -1.561089 -1.5718466 -1.5665348 -1.5489509 -1.5583835 -1.600642 -1.6436926 -1.6860799 -1.7337556 -1.7714856 -1.7499968 -1.6464336]]...]
INFO - root - 2017-12-06 04:42:40.255733: step 3610, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:21s remains)
INFO - root - 2017-12-06 04:42:42.535101: step 3620, loss = 0.83, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:04s remains)
INFO - root - 2017-12-06 04:42:44.864233: step 3630, loss = 0.84, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:33s remains)
INFO - root - 2017-12-06 04:42:47.170611: step 3640, loss = 0.97, batch loss = 0.76 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:51s remains)
INFO - root - 2017-12-06 04:42:49.464973: step 3650, loss = 0.99, batch loss = 0.78 (34.4 examples/sec; 0.233 sec/batch; 21h:14m:34s remains)
INFO - root - 2017-12-06 04:42:51.782242: step 3660, loss = 0.87, batch loss = 0.66 (33.6 examples/sec; 0.238 sec/batch; 21h:46m:42s remains)
INFO - root - 2017-12-06 04:42:54.128329: step 3670, loss = 0.90, batch loss = 0.69 (34.3 examples/sec; 0.233 sec/batch; 21h:19m:26s remains)
INFO - root - 2017-12-06 04:42:56.376455: step 3680, loss = 0.87, batch loss = 0.65 (34.3 examples/sec; 0.233 sec/batch; 21h:16m:24s remains)
INFO - root - 2017-12-06 04:42:58.669499: step 3690, loss = 0.89, batch loss = 0.68 (34.7 examples/sec; 0.230 sec/batch; 21h:02m:56s remains)
INFO - root - 2017-12-06 04:43:00.938085: step 3700, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.229 sec/batch; 20h:53m:04s remains)
2017-12-06 04:43:01.274208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082778484 -0.087774254 -0.089670248 -0.08792232 -0.085380539 -0.082981333 -0.079571836 -0.077095561 -0.07414142 -0.069187611 -0.063835695 -0.052593835 -0.041270338 -0.02850071 -0.0229768][-0.048612244 -0.058958761 -0.067738354 -0.073847011 -0.077352755 -0.077425644 -0.073900081 -0.071951874 -0.067608 -0.060602784 -0.051415764 -0.037770797 -0.024953175 -0.0122707 -0.0070991702][0.074604705 0.064050704 0.051018044 0.037583537 0.026558056 0.018245079 0.016478829 0.018606253 0.024284206 0.030832887 0.040283836 0.052860163 0.055553913 0.056642495 0.048153482][0.3029983 0.29762381 0.28466511 0.26671934 0.245272 0.22419722 0.20946391 0.20140125 0.19788124 0.19996054 0.20731647 0.21436982 0.21103393 0.19371332 0.1684355][0.55693966 0.56609881 0.56418365 0.54675913 0.51968771 0.491956 0.4691633 0.4520331 0.44086337 0.43574065 0.43509245 0.43557811 0.42823583 0.38686967 0.34575337][0.75951773 0.77762669 0.78296 0.77279514 0.75988317 0.73517442 0.71036351 0.6881417 0.6710633 0.65970409 0.65195119 0.64590025 0.61912853 0.56207383 0.50691319][0.84379506 0.85637861 0.85550135 0.85990983 0.86250877 0.85450673 0.84382844 0.82623434 0.80812085 0.78786933 0.76800388 0.75061566 0.69210142 0.6193983 0.55917597][0.83629572 0.84271115 0.84589738 0.87127709 0.89607304 0.90295333 0.90573287 0.89808089 0.88462752 0.86199278 0.83450997 0.80516803 0.711252 0.61860752 0.55819762][0.7473684 0.76345879 0.76495355 0.78573668 0.80789322 0.82257026 0.83099991 0.83206892 0.82290864 0.80320007 0.7762413 0.72020054 0.60817784 0.5141151 0.45534742][0.59019578 0.60908204 0.61775315 0.63102055 0.64219332 0.65156883 0.6593647 0.66111177 0.65158272 0.63797152 0.60770321 0.54786068 0.45179296 0.375467 0.33396065][0.40342712 0.42260569 0.45018184 0.4716695 0.48153079 0.48462176 0.48742867 0.48791146 0.48273581 0.47531682 0.4442808 0.3913874 0.33129573 0.26964712 0.24354877][0.16160581 0.17198451 0.2046219 0.21858902 0.22526784 0.22624458 0.22996224 0.22416811 0.2098936 0.2081136 0.18241735 0.15600607 0.13245888 0.10312922 0.087378979][-0.09314052 -0.10589035 -0.089485861 -0.0756664 -0.061198194 -0.059928738 -0.0535477 -0.04118894 -0.033628017 -0.030889481 -0.065716363 -0.063691318 -0.054940142 -0.047182143 -0.043575779][-0.42128396 -0.44040644 -0.43404168 -0.41559374 -0.40262276 -0.3921057 -0.37775981 -0.37192553 -0.35917157 -0.34221095 -0.35467583 -0.33843845 -0.31940055 -0.27208588 -0.24610214][-0.78347278 -0.82366014 -0.82368517 -0.81187534 -0.80054855 -0.77980953 -0.75835925 -0.74749863 -0.73080295 -0.71146458 -0.71018422 -0.66692817 -0.62004477 -0.5582093 -0.51296395]]...]
INFO - root - 2017-12-06 04:43:03.524951: step 3710, loss = 0.89, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:28s remains)
INFO - root - 2017-12-06 04:43:05.814815: step 3720, loss = 0.91, batch loss = 0.70 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:18s remains)
INFO - root - 2017-12-06 04:43:08.099674: step 3730, loss = 0.91, batch loss = 0.70 (32.2 examples/sec; 0.248 sec/batch; 22h:40m:49s remains)
INFO - root - 2017-12-06 04:43:10.397007: step 3740, loss = 0.87, batch loss = 0.66 (36.3 examples/sec; 0.221 sec/batch; 20h:08m:53s remains)
INFO - root - 2017-12-06 04:43:12.671948: step 3750, loss = 0.87, batch loss = 0.66 (34.4 examples/sec; 0.232 sec/batch; 21h:13m:42s remains)
INFO - root - 2017-12-06 04:43:15.015735: step 3760, loss = 0.82, batch loss = 0.61 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:23s remains)
INFO - root - 2017-12-06 04:43:17.332122: step 3770, loss = 0.95, batch loss = 0.74 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:37s remains)
INFO - root - 2017-12-06 04:43:19.683117: step 3780, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:43s remains)
INFO - root - 2017-12-06 04:43:21.991530: step 3790, loss = 0.97, batch loss = 0.75 (34.2 examples/sec; 0.234 sec/batch; 21h:22m:45s remains)
INFO - root - 2017-12-06 04:43:24.280904: step 3800, loss = 0.93, batch loss = 0.72 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:26s remains)
2017-12-06 04:43:24.596402: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.078745477 -0.081028193 -0.083863318 -0.086132981 -0.087182574 -0.086879395 -0.085440651 -0.083269045 -0.08100003 -0.079082265 -0.077845767 -0.077057458 -0.0765943 -0.076277524 -0.076355822][-0.078124516 -0.080510654 -0.084272742 -0.08873491 -0.092744261 -0.095244534 -0.0954817 -0.093484692 -0.089991249 -0.086058684 -0.082782559 -0.080214038 -0.078491934 -0.0775568 -0.077347338][-0.070785679 -0.070653886 -0.073147766 -0.0784114 -0.085636124 -0.093147784 -0.098965004 -0.10182303 -0.10126135 -0.097785421 -0.093019247 -0.087744154 -0.083381452 -0.080543615 -0.079038709][-0.059282366 -0.053738471 -0.052256189 -0.056048449 -0.06469848 -0.076274067 -0.088262506 -0.098566592 -0.10523804 -0.10725269 -0.10515877 -0.099528372 -0.0926715 -0.086871646 -0.082807213][-0.049674049 -0.038717523 -0.03189854 -0.031749472 -0.03885996 -0.050974209 -0.065273143 -0.079734921 -0.09299881 -0.10314825 -0.1084404 -0.1075307 -0.10179025 -0.094559826 -0.0881879][-0.048593763 -0.034608766 -0.021980703 -0.014794644 -0.016080283 -0.024702873 -0.037200987 -0.051356174 -0.0663428 -0.081190109 -0.094045833 -0.10166493 -0.10238712 -0.097940952 -0.091821179][-0.05551672 -0.041067645 -0.02375922 -0.0075873844 0.0018487945 0.0019108579 -0.0049599074 -0.015896481 -0.030294459 -0.047330774 -0.065855905 -0.081683069 -0.091146581 -0.093208283 -0.0900944][-0.061780337 -0.049130663 -0.032029927 -0.012836449 0.0055863485 0.019389465 0.023902558 0.018431216 0.0054533556 -0.013296917 -0.03578344 -0.05759722 -0.073743939 -0.082037494 -0.083178326][-0.063945346 -0.053724438 -0.039591923 -0.022572361 -0.0033877976 0.016273633 0.031146638 0.034653336 0.027372502 0.010552354 -0.013490487 -0.038864292 -0.0588672 -0.071257249 -0.076010019][-0.065148652 -0.058107592 -0.048413362 -0.035811245 -0.020311207 -0.003788162 0.0095730722 0.016455747 0.016283527 0.0087610483 -0.0073779859 -0.029029377 -0.049004488 -0.063059293 -0.070551842][-0.068812877 -0.064607024 -0.059000302 -0.051057324 -0.040135771 -0.02800874 -0.017970156 -0.011605449 -0.0092905276 -0.011323422 -0.018844396 -0.031638127 -0.045851864 -0.058272954 -0.066791028][-0.073035896 -0.0709873 -0.068470985 -0.064537056 -0.058653638 -0.051819332 -0.045094073 -0.040136393 -0.037116632 -0.036090858 -0.038423598 -0.043562353 -0.050551273 -0.058453463 -0.065120175][-0.0762544 -0.075299181 -0.074546471 -0.073077604 -0.071071342 -0.068803512 -0.065980926 -0.063859172 -0.061735183 -0.059920579 -0.058820039 -0.058811054 -0.06038332 -0.063446365 -0.066650048][-0.077199541 -0.076982655 -0.077278748 -0.077711284 -0.078250967 -0.078661293 -0.078402892 -0.0780843 -0.0765412 -0.074287862 -0.071236938 -0.068893291 -0.0681593 -0.069049709 -0.069991991][-0.077604339 -0.077411711 -0.078257129 -0.079422079 -0.080493048 -0.08171016 -0.082406566 -0.082262173 -0.0810086 -0.079092361 -0.07628876 -0.074109383 -0.072642393 -0.072257623 -0.072676919]]...]
INFO - root - 2017-12-06 04:43:26.943704: step 3810, loss = 0.82, batch loss = 0.61 (34.7 examples/sec; 0.230 sec/batch; 21h:02m:06s remains)
INFO - root - 2017-12-06 04:43:29.292671: step 3820, loss = 0.79, batch loss = 0.58 (34.4 examples/sec; 0.232 sec/batch; 21h:12m:16s remains)
INFO - root - 2017-12-06 04:43:31.602170: step 3830, loss = 0.88, batch loss = 0.67 (34.4 examples/sec; 0.232 sec/batch; 21h:13m:12s remains)
INFO - root - 2017-12-06 04:43:33.947841: step 3840, loss = 0.92, batch loss = 0.71 (35.2 examples/sec; 0.227 sec/batch; 20h:45m:57s remains)
INFO - root - 2017-12-06 04:43:36.265499: step 3850, loss = 0.92, batch loss = 0.70 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:27s remains)
INFO - root - 2017-12-06 04:43:38.602526: step 3860, loss = 0.90, batch loss = 0.69 (34.2 examples/sec; 0.234 sec/batch; 21h:21m:50s remains)
INFO - root - 2017-12-06 04:43:40.934298: step 3870, loss = 0.81, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 20h:21m:06s remains)
INFO - root - 2017-12-06 04:43:43.224712: step 3880, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:54m:35s remains)
INFO - root - 2017-12-06 04:43:45.557208: step 3890, loss = 0.91, batch loss = 0.70 (34.5 examples/sec; 0.232 sec/batch; 21h:10m:38s remains)
INFO - root - 2017-12-06 04:43:47.887528: step 3900, loss = 0.84, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 20h:41m:24s remains)
2017-12-06 04:43:48.264124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.63496554 -0.65426064 -0.54970759 -0.37771574 -0.18555377 -0.027412228 0.067194566 -0.062107932 -0.17744997 -0.40140969 -0.53073049 -0.57607496 -0.55239463 -0.37739283 -0.2506853][-0.89682174 -0.879127 -0.71864915 -0.46915555 -0.20411576 -0.043951739 -0.009096764 -0.13898316 -0.2525875 -0.44414911 -0.59911513 -0.64656544 -0.572567 -0.36046821 -0.19201265][-1.0283035 -0.99130285 -0.77262044 -0.46415314 -0.18505222 -0.054537024 -0.037170757 -0.14979178 -0.25818259 -0.40102819 -0.48991624 -0.48613384 -0.39987689 -0.23508894 -0.07909251][-1.0337703 -0.97080696 -0.72198713 -0.36249095 -0.01132118 0.18719488 0.19472581 0.079128847 -0.10472429 -0.27062243 -0.36919242 -0.35036322 -0.27819973 -0.12236374 0.030743346][-1.0211053 -0.97849917 -0.76190341 -0.425751 -0.042582888 0.20712855 0.26047465 0.1658188 -0.0096299648 -0.1732372 -0.25899208 -0.23361899 -0.13931426 -0.030450284 0.085140631][-0.89803088 -0.85687506 -0.69187784 -0.39744627 -0.035495728 0.25555286 0.38027424 0.29543027 0.12772255 -0.021009512 -0.11476877 -0.11309367 -0.02082805 0.086567834 0.15593088][-0.69476008 -0.66215396 -0.54874706 -0.30431989 0.036069982 0.31158775 0.46963879 0.43157169 0.24375021 0.052183039 -0.075648576 -0.069570184 -0.0029495806 0.0647054 0.096899524][-0.35225987 -0.31888372 -0.23612019 -0.078695171 0.18286715 0.40259451 0.53275979 0.51158476 0.32658482 0.14627124 0.010605559 0.010796048 0.056691691 0.092239574 0.091370359][-0.021157768 -0.03639105 -0.0014243573 0.08982107 0.27709097 0.40551066 0.50400138 0.4916546 0.36514425 0.1776302 0.03268858 0.022700347 0.052450784 0.07968998 0.066977665][0.2955001 0.25457346 0.25856626 0.30426666 0.38408744 0.42254907 0.45345607 0.42453843 0.33923167 0.20040885 0.088255852 0.07603462 0.097118512 0.11145441 0.088168547][0.33987212 0.31171072 0.30507892 0.31566489 0.34053075 0.31521648 0.28969669 0.28061935 0.23148996 0.13315357 0.053105511 0.031546354 0.051323436 0.049659938 0.026458375][0.28100309 0.30999711 0.33479556 0.34387004 0.33724138 0.25721726 0.20215118 0.16057108 0.1114253 0.064860955 0.0042119026 -0.0033252835 0.003495276 0.010437593 0.0041205138][0.12638709 0.17093132 0.19830069 0.21742398 0.20597759 0.14482383 0.098661736 0.056393124 0.019066446 -0.0076356456 -0.018440578 -0.023216169 -0.020485461 -0.0090247728 -0.014773376][-0.031644654 0.016926371 0.06184876 0.10231426 0.11318195 0.067952976 0.052850008 0.021233089 -0.018224705 -0.035266936 -0.022109032 -0.02655264 -0.041259967 -0.035598665 -0.034567229][-0.088950545 -0.069873773 -0.034161545 0.0070562735 0.018529288 0.0093301684 -0.0092231967 0.0012255535 -0.024457391 -0.027840577 -0.026499227 -0.040225141 -0.036002185 -0.041177258 -0.044305798]]...]
INFO - root - 2017-12-06 04:43:50.569162: step 3910, loss = 0.96, batch loss = 0.75 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:29s remains)
INFO - root - 2017-12-06 04:43:52.917436: step 3920, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.234 sec/batch; 21h:23m:15s remains)
INFO - root - 2017-12-06 04:43:55.231111: step 3930, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.228 sec/batch; 20h:51m:00s remains)
INFO - root - 2017-12-06 04:43:57.545113: step 3940, loss = 0.82, batch loss = 0.60 (36.0 examples/sec; 0.222 sec/batch; 20h:17m:49s remains)
INFO - root - 2017-12-06 04:43:59.911985: step 3950, loss = 0.91, batch loss = 0.70 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:25s remains)
INFO - root - 2017-12-06 04:44:02.227918: step 3960, loss = 0.82, batch loss = 0.61 (32.9 examples/sec; 0.243 sec/batch; 22h:11m:51s remains)
INFO - root - 2017-12-06 04:44:04.594405: step 3970, loss = 0.89, batch loss = 0.68 (32.1 examples/sec; 0.249 sec/batch; 22h:45m:46s remains)
INFO - root - 2017-12-06 04:44:06.926484: step 3980, loss = 0.85, batch loss = 0.64 (33.8 examples/sec; 0.237 sec/batch; 21h:36m:03s remains)
INFO - root - 2017-12-06 04:44:09.288967: step 3990, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 21h:01m:46s remains)
INFO - root - 2017-12-06 04:44:11.635199: step 4000, loss = 0.84, batch loss = 0.63 (32.9 examples/sec; 0.243 sec/batch; 22h:09m:30s remains)
2017-12-06 04:44:12.055315: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0304583 0.695375 0.20110679 -0.30460417 -0.78966039 -1.158919 -1.3520863 -1.3603649 -1.2946066 -1.1897511 -1.0966957 -0.9711256 -0.82751948 -0.68146288 -0.57244045][1.4499977 1.1765933 0.74725085 0.26275948 -0.29766372 -0.78354943 -1.0980086 -1.2385007 -1.304283 -1.3014224 -1.2332453 -1.0743645 -0.88391876 -0.67798591 -0.52733016][1.4605292 1.3070354 1.0498978 0.72582173 0.28245336 -0.17391652 -0.55398935 -0.79205632 -0.92563671 -0.99867845 -0.99190527 -0.88621354 -0.7156198 -0.5208841 -0.36292791][1.174106 1.1451763 1.0726575 0.92889732 0.64563215 0.32014102 -0.013956141 -0.240756 -0.3492268 -0.39741948 -0.41709742 -0.38929772 -0.31034243 -0.20480821 -0.078159295][0.87212837 0.9666872 1.0847921 1.1135257 0.98518753 0.75836146 0.52013695 0.32731786 0.2201522 0.17507574 0.1714702 0.1705181 0.20184311 0.22762439 0.25722551][0.68687385 0.90886241 1.1680701 1.3224443 1.3248657 1.1927334 0.9958266 0.82187986 0.751076 0.734543 0.7229408 0.72978967 0.74017256 0.71739608 0.68344885][0.567536 0.89198095 1.2363726 1.4769746 1.5666331 1.5063756 1.3327227 1.1589105 1.0777991 1.0512092 1.0798689 1.0867229 1.1123269 1.1002342 1.089046][0.2897459 0.67312628 1.0984174 1.4171538 1.5941426 1.603941 1.481127 1.2990972 1.1520623 1.0717955 1.0941375 1.1408426 1.2364587 1.2577019 1.2378876][-0.28465536 0.081695773 0.54130119 0.93555439 1.20966 1.3131082 1.2874088 1.1537486 1.0220859 0.94232619 0.94740987 1.0011392 1.1131887 1.2031548 1.2324859][-0.95847034 -0.73468882 -0.3508279 0.05857718 0.40763947 0.61841094 0.69848841 0.66796422 0.60588765 0.55429387 0.56462568 0.64251071 0.76990426 0.90144837 1.0012852][-1.3772584 -1.3538225 -1.1522321 -0.83192009 -0.49384964 -0.21814734 -0.032264158 0.029244415 0.022108749 -0.01378246 -0.020110615 0.062094741 0.19834703 0.38440782 0.54390419][-1.4373896 -1.61421 -1.5842904 -1.3845195 -1.0840498 -0.78102976 -0.53560513 -0.40462059 -0.38872203 -0.44734418 -0.50745785 -0.48742023 -0.37736961 -0.1626925 0.047505327][-1.2067591 -1.5023496 -1.618853 -1.5207868 -1.2460704 -0.91967916 -0.660583 -0.50645339 -0.48896196 -0.58684009 -0.7316407 -0.79978245 -0.74990058 -0.55345744 -0.31407925][-0.92553991 -1.1507148 -1.2841758 -1.2499388 -0.99001217 -0.64599431 -0.3839379 -0.26035759 -0.32686156 -0.49904525 -0.71747285 -0.89055896 -0.93884289 -0.80308479 -0.57344383][-0.50962412 -0.64585912 -0.68775 -0.64066112 -0.4126063 -0.12927023 0.11759997 0.18478534 0.010953628 -0.274466 -0.61453378 -0.89920783 -1.0162072 -0.93699533 -0.73217118]]...]
INFO - root - 2017-12-06 04:44:14.363483: step 4010, loss = 0.88, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:22s remains)
INFO - root - 2017-12-06 04:44:16.684454: step 4020, loss = 0.81, batch loss = 0.60 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:10s remains)
INFO - root - 2017-12-06 04:44:19.024373: step 4030, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:30s remains)
INFO - root - 2017-12-06 04:44:21.325219: step 4040, loss = 0.87, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 21h:03m:29s remains)
INFO - root - 2017-12-06 04:44:23.649096: step 4050, loss = 0.83, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 21h:10m:18s remains)
INFO - root - 2017-12-06 04:44:25.927676: step 4060, loss = 0.90, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-06 04:44:28.212055: step 4070, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:46m:44s remains)
INFO - root - 2017-12-06 04:44:30.560547: step 4080, loss = 0.93, batch loss = 0.71 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:40s remains)
INFO - root - 2017-12-06 04:44:32.899910: step 4090, loss = 1.12, batch loss = 0.91 (33.8 examples/sec; 0.237 sec/batch; 21h:36m:19s remains)
INFO - root - 2017-12-06 04:44:35.227011: step 4100, loss = 0.87, batch loss = 0.66 (33.7 examples/sec; 0.237 sec/batch; 21h:37m:55s remains)
2017-12-06 04:44:40.044607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.008197207 -0.0078746527 -0.0078119263 -0.0077639669 -0.0077564865 -0.0079152361 -0.0080712773 -0.0082161538 -0.0083494559 -0.0085241906 -0.0086615123 -0.0087483414 -0.0089885592 -0.010084927 -0.013404887][-0.0085090213 -0.0082395189 -0.00837357 -0.0083004273 -0.008344084 -0.0084466 -0.0085778572 -0.0086862817 -0.0085939132 -0.0087443776 -0.0088047162 -0.0090627708 -0.009806402 -0.011977468 -0.016458586][-0.0090534948 -0.0086558275 -0.0090061054 -0.0089766346 -0.0093698874 -0.009405531 -0.009614341 -0.0096989051 -0.0094192177 -0.0095830709 -0.0093360916 -0.01002498 -0.011719801 -0.015762441 -0.022330806][-0.0094254538 -0.009007778 -0.009452302 -0.0094422139 -0.010125156 -0.010252148 -0.010927569 -0.010946531 -0.010757916 -0.01097583 -0.01070812 -0.011929892 -0.014683798 -0.021640997 -0.031387109][-0.0095300525 -0.009142112 -0.009629719 -0.0096568353 -0.010390677 -0.010561712 -0.011612467 -0.011810973 -0.012178905 -0.012579393 -0.013000399 -0.015460804 -0.020240813 -0.031002376 -0.044825297][-0.009394303 -0.0090777613 -0.0095837 -0.0096410178 -0.010535762 -0.01061295 -0.011857051 -0.012379795 -0.01337624 -0.014404278 -0.015924755 -0.020279288 -0.028077725 -0.043390349 -0.062369332][-0.0092686005 -0.008977592 -0.0095829666 -0.0097406432 -0.010814965 -0.010876782 -0.012030434 -0.012887221 -0.014346194 -0.016154535 -0.018865813 -0.025599103 -0.037261121 -0.057737414 -0.082320437][-0.0092694685 -0.00894215 -0.009504728 -0.0098080225 -0.011150461 -0.011499319 -0.012796123 -0.013784308 -0.015833065 -0.018338043 -0.02251792 -0.031827968 -0.047504552 -0.073127724 -0.10276498][-0.0095824637 -0.00921201 -0.0095826648 -0.0099402629 -0.011152472 -0.011980809 -0.013692174 -0.014986496 -0.017548539 -0.020940438 -0.027082279 -0.038871244 -0.057969157 -0.087445229 -0.12057614][-0.010465473 -0.0098729134 -0.010070652 -0.010340855 -0.011192508 -0.012194086 -0.013981063 -0.015734602 -0.018955547 -0.023683611 -0.03214284 -0.04649172 -0.0682793 -0.099177137 -0.13299426][-0.011897232 -0.01112524 -0.011109564 -0.011181951 -0.011854321 -0.012772452 -0.014362939 -0.016625188 -0.020416636 -0.026543539 -0.036870189 -0.053297106 -0.076415904 -0.10616479 -0.13762045][-0.013700634 -0.01286551 -0.012721807 -0.012652509 -0.013139673 -0.01394473 -0.015482888 -0.018042587 -0.022536725 -0.029835232 -0.041293107 -0.058651507 -0.08097671 -0.10743011 -0.13355757][-0.01551621 -0.014701568 -0.014500584 -0.014394879 -0.014635723 -0.015400227 -0.016793955 -0.01979823 -0.025285617 -0.033816133 -0.046352312 -0.062944554 -0.082918651 -0.10379897 -0.12266683][-0.017331526 -0.016496077 -0.0162572 -0.016147293 -0.016281903 -0.0169258 -0.01845248 -0.021944821 -0.028382126 -0.038162544 -0.051042911 -0.066208445 -0.082017258 -0.096471913 -0.10767131][-0.019646551 -0.018699169 -0.018428806 -0.018261515 -0.018298764 -0.018815335 -0.020398863 -0.024420653 -0.031791952 -0.042282097 -0.054951359 -0.068055414 -0.079661056 -0.08774475 -0.091712259]]...]
INFO - root - 2017-12-06 04:44:42.441987: step 4110, loss = 0.84, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 21h:06m:37s remains)
INFO - root - 2017-12-06 04:44:44.770693: step 4120, loss = 0.88, batch loss = 0.67 (35.3 examples/sec; 0.227 sec/batch; 20h:40m:58s remains)
INFO - root - 2017-12-06 04:44:47.044148: step 4130, loss = 0.75, batch loss = 0.54 (36.6 examples/sec; 0.219 sec/batch; 19h:56m:05s remains)
INFO - root - 2017-12-06 04:44:49.344712: step 4140, loss = 0.95, batch loss = 0.74 (35.7 examples/sec; 0.224 sec/batch; 20h:25m:52s remains)
INFO - root - 2017-12-06 04:44:51.686304: step 4150, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:05s remains)
INFO - root - 2017-12-06 04:44:54.005935: step 4160, loss = 0.82, batch loss = 0.61 (35.0 examples/sec; 0.228 sec/batch; 20h:50m:20s remains)
INFO - root - 2017-12-06 04:44:56.343068: step 4170, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:29s remains)
INFO - root - 2017-12-06 04:44:58.666902: step 4180, loss = 0.86, batch loss = 0.65 (36.5 examples/sec; 0.219 sec/batch; 19h:59m:48s remains)
INFO - root - 2017-12-06 04:45:00.969661: step 4190, loss = 0.88, batch loss = 0.67 (34.3 examples/sec; 0.233 sec/batch; 21h:17m:26s remains)
INFO - root - 2017-12-06 04:45:03.335816: step 4200, loss = 0.82, batch loss = 0.60 (35.3 examples/sec; 0.227 sec/batch; 20h:40m:04s remains)
2017-12-06 04:45:03.753405: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54522043 -0.301289 -0.69622582 -0.81521839 -0.74728614 -0.50188857 -0.11033015 0.31839913 0.55921727 0.53994745 0.44729143 0.33131406 0.33869356 0.054525316 -0.29125962][0.56605721 -0.1764155 -0.54409027 -0.65679878 -0.66194713 -0.51947087 -0.22358608 0.11817891 0.40681195 0.58788681 0.62447822 0.60483062 0.63056517 0.60011351 0.45108813][1.1972181 0.48199719 0.12412751 -0.048147127 -0.090087622 -0.046870913 0.13311201 0.39265648 0.6214993 0.78345966 0.90265369 1.0469457 1.2283595 1.2582774 1.016608][1.6500986 1.1272048 0.81725031 0.5730384 0.45835626 0.4251987 0.51201069 0.67923397 0.87749863 1.0598155 1.2151715 1.3499818 1.5075023 1.7145345 1.6478688][1.7317764 1.291099 1.0435714 0.93714297 0.79185504 0.64944565 0.66071081 0.81993335 0.96632719 1.1068698 1.2853363 1.5166017 1.7400763 1.8693092 1.8991039][1.5698912 1.2057909 0.93870509 0.75628757 0.62789237 0.55631149 0.55281746 0.59289366 0.68858433 0.82341975 1.006287 1.2450939 1.5206242 1.7714134 1.8944913][1.2049729 0.96175611 0.77043515 0.5859611 0.41244686 0.2930229 0.26836944 0.26639605 0.3102296 0.37656188 0.50718427 0.6934979 0.93045497 1.1887882 1.4125687][0.61201715 0.46226788 0.3411206 0.19956532 0.085294656 -0.0137445 -0.084539652 -0.077177994 -0.026671946 -0.024050977 0.024136566 0.087645732 0.22194549 0.38772041 0.65980905][0.0065236241 -0.047889359 -0.11948323 -0.20384756 -0.28442425 -0.32951733 -0.3252852 -0.32761985 -0.30586421 -0.30191654 -0.29635417 -0.31115991 -0.27402073 -0.20326129 -0.049619816][-0.33149794 -0.38996416 -0.39450002 -0.42007026 -0.45219103 -0.47881359 -0.48921642 -0.47663397 -0.43956542 -0.45443344 -0.4806821 -0.52050596 -0.54450405 -0.55416131 -0.483395][-0.51818365 -0.55517858 -0.55365247 -0.54559243 -0.51822519 -0.48433319 -0.45849633 -0.43456462 -0.40809813 -0.39996159 -0.39578563 -0.43672806 -0.48263112 -0.52186161 -0.51619637][-0.35417297 -0.384644 -0.3972736 -0.40232548 -0.38927957 -0.3707175 -0.33918461 -0.31473055 -0.29731452 -0.2946578 -0.3039946 -0.33054426 -0.35160443 -0.39238834 -0.4048005][-0.24784806 -0.26177865 -0.26962078 -0.27515283 -0.27050024 -0.26225722 -0.24935445 -0.23555857 -0.22344321 -0.21362334 -0.21086177 -0.21878108 -0.23095313 -0.24683177 -0.25167143][-0.17222846 -0.17653748 -0.17732553 -0.17721397 -0.17527571 -0.17353354 -0.17066893 -0.16949898 -0.17056334 -0.1700469 -0.17266214 -0.17363198 -0.17496388 -0.17912251 -0.1744469][-0.16246942 -0.1641545 -0.16337872 -0.16221794 -0.15986755 -0.15783821 -0.15480258 -0.15319796 -0.15243965 -0.15258941 -0.1546053 -0.15560506 -0.15641157 -0.15647055 -0.15476598]]...]
INFO - root - 2017-12-06 04:45:06.051247: step 4210, loss = 0.84, batch loss = 0.63 (33.5 examples/sec; 0.239 sec/batch; 21h:46m:14s remains)
INFO - root - 2017-12-06 04:45:08.393661: step 4220, loss = 0.80, batch loss = 0.59 (33.2 examples/sec; 0.241 sec/batch; 21h:57m:21s remains)
INFO - root - 2017-12-06 04:45:10.686699: step 4230, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.235 sec/batch; 21h:23m:59s remains)
INFO - root - 2017-12-06 04:45:12.967954: step 4240, loss = 0.93, batch loss = 0.72 (32.6 examples/sec; 0.245 sec/batch; 22h:21m:33s remains)
INFO - root - 2017-12-06 04:45:15.219520: step 4250, loss = 0.88, batch loss = 0.67 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:41s remains)
INFO - root - 2017-12-06 04:45:17.551441: step 4260, loss = 0.81, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 21h:16m:47s remains)
INFO - root - 2017-12-06 04:45:19.876245: step 4270, loss = 0.81, batch loss = 0.60 (33.9 examples/sec; 0.236 sec/batch; 21h:31m:35s remains)
INFO - root - 2017-12-06 04:45:22.209884: step 4280, loss = 0.88, batch loss = 0.67 (34.7 examples/sec; 0.231 sec/batch; 21h:01m:32s remains)
INFO - root - 2017-12-06 04:45:24.550738: step 4290, loss = 0.89, batch loss = 0.68 (33.6 examples/sec; 0.238 sec/batch; 21h:40m:32s remains)
INFO - root - 2017-12-06 04:45:26.857157: step 4300, loss = 0.90, batch loss = 0.69 (34.7 examples/sec; 0.230 sec/batch; 20h:59m:29s remains)
2017-12-06 04:45:27.365754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029411275 -0.028243978 -0.027980205 -0.028128613 -0.028550982 -0.02905057 -0.029436987 -0.02954651 -0.029150642 -0.028301246 -0.027248029 -0.026406392 -0.025943488 -0.025988497 -0.026916351][-0.028683692 -0.027592175 -0.027651615 -0.028363973 -0.029613964 -0.031055652 -0.032211665 -0.032576479 -0.031773545 -0.030008409 -0.027920008 -0.026272479 -0.025377337 -0.025220424 -0.026129216][-0.028032761 -0.027125478 -0.027723394 -0.029258098 -0.031822994 -0.035028111 -0.037997853 -0.039323334 -0.038101137 -0.034793124 -0.030648813 -0.027258079 -0.025314979 -0.024725731 -0.025468841][-0.027287599 -0.026804082 -0.028042387 -0.030445807 -0.034486059 -0.040080413 -0.045904458 -0.049064994 -0.047692969 -0.042386975 -0.035438318 -0.029424526 -0.025701053 -0.024263281 -0.02476066][-0.026615616 -0.026642241 -0.028520327 -0.031971134 -0.037877947 -0.046040326 -0.054750867 -0.059950471 -0.058651108 -0.051272042 -0.041210685 -0.032214779 -0.026403889 -0.024005976 -0.024172567][-0.026258841 -0.026807655 -0.029347885 -0.034149207 -0.042198356 -0.052810483 -0.063754112 -0.070418768 -0.069220729 -0.060091823 -0.047179282 -0.035258468 -0.027347606 -0.024000607 -0.023895703][-0.026015203 -0.027253695 -0.030825607 -0.03701935 -0.046815868 -0.059090313 -0.071287908 -0.078628607 -0.077487223 -0.067314036 -0.052455116 -0.038349491 -0.02858432 -0.02434263 -0.02396727][-0.025857553 -0.027752332 -0.032398269 -0.039778113 -0.050229423 -0.0627524 -0.074771874 -0.081813827 -0.08060167 -0.0703576 -0.055131447 -0.040194236 -0.029525246 -0.024666663 -0.024255484][-0.025751613 -0.027825955 -0.032791629 -0.04044304 -0.050274249 -0.061484035 -0.071964979 -0.077902377 -0.076663069 -0.067470692 -0.053703103 -0.039760668 -0.029535204 -0.024852984 -0.024642687][-0.025785252 -0.02743154 -0.031812482 -0.038667828 -0.047084138 -0.05623433 -0.064480543 -0.068950437 -0.067766406 -0.060383916 -0.049257845 -0.037689283 -0.028922718 -0.024960224 -0.024959583][-0.025692899 -0.026636098 -0.029860429 -0.035029296 -0.041444503 -0.048331525 -0.054299977 -0.057337645 -0.056486502 -0.05127044 -0.043270875 -0.034690812 -0.027996089 -0.024974354 -0.025235035][-0.025698856 -0.02591547 -0.027871735 -0.031158969 -0.035350658 -0.039945394 -0.043949939 -0.045884766 -0.045375239 -0.042179629 -0.037162565 -0.031630103 -0.027195353 -0.025251918 -0.025612835][-0.025932372 -0.025592305 -0.026446197 -0.028089326 -0.030349813 -0.033022135 -0.035404902 -0.036603305 -0.036395963 -0.034692682 -0.031996228 -0.028948389 -0.026506793 -0.025534272 -0.026072077][-0.026390549 -0.025749333 -0.02580015 -0.026261888 -0.027102418 -0.028254636 -0.029409345 -0.03011458 -0.030217089 -0.02953792 -0.028383277 -0.027063951 -0.026049353 -0.025802128 -0.02656316][-0.027816255 -0.027269013 -0.027154826 -0.027108684 -0.027152222 -0.027274732 -0.02754489 -0.027855176 -0.028023049 -0.027750511 -0.027361698 -0.026863564 -0.026494265 -0.02642712 -0.027197275]]...]
INFO - root - 2017-12-06 04:45:29.697964: step 4310, loss = 0.85, batch loss = 0.63 (35.4 examples/sec; 0.226 sec/batch; 20h:35m:13s remains)
INFO - root - 2017-12-06 04:45:31.989343: step 4320, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:48s remains)
INFO - root - 2017-12-06 04:45:34.307619: step 4330, loss = 0.81, batch loss = 0.59 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:13s remains)
INFO - root - 2017-12-06 04:45:36.623093: step 4340, loss = 0.78, batch loss = 0.57 (36.4 examples/sec; 0.220 sec/batch; 20h:02m:14s remains)
INFO - root - 2017-12-06 04:45:38.954340: step 4350, loss = 0.85, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:22s remains)
INFO - root - 2017-12-06 04:45:41.307090: step 4360, loss = 0.81, batch loss = 0.60 (33.0 examples/sec; 0.243 sec/batch; 22h:07m:16s remains)
INFO - root - 2017-12-06 04:45:43.652495: step 4370, loss = 0.74, batch loss = 0.53 (33.3 examples/sec; 0.240 sec/batch; 21h:53m:21s remains)
INFO - root - 2017-12-06 04:45:45.974353: step 4380, loss = 0.83, batch loss = 0.62 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:55s remains)
INFO - root - 2017-12-06 04:45:48.303893: step 4390, loss = 0.84, batch loss = 0.63 (34.2 examples/sec; 0.234 sec/batch; 21h:19m:54s remains)
INFO - root - 2017-12-06 04:45:50.581884: step 4400, loss = 0.82, batch loss = 0.61 (35.3 examples/sec; 0.227 sec/batch; 20h:40m:43s remains)
2017-12-06 04:45:56.847769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0079753771 0.0088556409 0.00695353 0.0047265217 0.0034620091 0.002747044 0.0022331625 0.00210426 0.0027123615 0.0049885362 0.0096480623 0.016392857 0.024346404 0.031251259 0.034876972][-0.0062958 -0.0056211054 -0.0069593154 -0.0077481493 -0.0067426711 -0.0051862895 -0.0044536442 -0.004631944 -0.0047782958 -0.0035599247 0.0004273802 0.0076995865 0.017329171 0.026279971 0.031544112][-0.021885309 -0.022527982 -0.023405015 -0.021842174 -0.017492276 -0.012204234 -0.0090955161 -0.00892831 -0.010150295 -0.010569498 -0.0078824349 -0.00069793314 0.010230914 0.021283023 0.028760992][-0.038276013 -0.041399874 -0.04197016 -0.037242297 -0.02848579 -0.018614102 -0.012501542 -0.011684053 -0.014353901 -0.016872641 -0.015838016 -0.0088175684 0.0033024624 0.016268589 0.02598574][-0.052396864 -0.059486322 -0.060408879 -0.052681737 -0.039971255 -0.025857434 -0.016737197 -0.014903363 -0.019062776 -0.023953915 -0.024788212 -0.018017255 -0.0047065094 0.010427602 0.022546425][-0.060539275 -0.073621713 -0.076374523 -0.066925414 -0.051569451 -0.034384344 -0.023118678 -0.020439073 -0.025523998 -0.032724775 -0.035086192 -0.028423734 -0.013920944 0.0035086572 0.018475197][-0.060343239 -0.081629321 -0.088699073 -0.079715349 -0.063459627 -0.044639148 -0.031549595 -0.028437093 -0.034079235 -0.042989731 -0.046482045 -0.039533675 -0.023592465 -0.0036870539 0.014051981][-0.049667284 -0.078975908 -0.091963604 -0.086510129 -0.072379589 -0.054183237 -0.040375128 -0.037455667 -0.043540813 -0.053595319 -0.057716951 -0.049537733 -0.031860605 -0.0095527209 0.01068484][-0.029405534 -0.062703796 -0.081248462 -0.080995753 -0.072372809 -0.057392731 -0.044346251 -0.042596936 -0.049379788 -0.061038531 -0.065860681 -0.056163177 -0.0370553 -0.01308351 0.0086609945][-0.0073049106 -0.038403209 -0.05851547 -0.062951416 -0.061079569 -0.050341211 -0.039654691 -0.039976835 -0.047418386 -0.060938604 -0.067248188 -0.056559373 -0.037301607 -0.013037592 0.0085566118][0.010689057 -0.015908282 -0.034333907 -0.041649614 -0.043999605 -0.036057048 -0.027942263 -0.029636089 -0.036109906 -0.051147245 -0.059294496 -0.049001105 -0.031489868 -0.0091651641 0.010683253][0.022059411 0.0023789108 -0.01310103 -0.022253893 -0.025207058 -0.019200288 -0.013342876 -0.015055444 -0.019739181 -0.034666382 -0.043457638 -0.035162162 -0.020548876 -0.0013771057 0.015402392][0.029863685 0.017330587 0.0059879497 -0.0024372712 -0.0053162053 -0.0022734106 0.001617454 0.00016245991 -0.003422603 -0.015816867 -0.023454245 -0.017831475 -0.00686777 0.0084391832 0.021510385][0.036251113 0.030028895 0.022340991 0.015608095 0.012996167 0.013091497 0.015026368 0.013661601 0.010391697 0.0017065629 -0.0039086118 -0.00056679547 0.0071025789 0.018122591 0.027161457][0.035494842 0.033996962 0.029296137 0.024841301 0.022217691 0.021620438 0.022719622 0.021776482 0.019269474 0.013565607 0.0097163171 0.010896325 0.015614584 0.022560135 0.028281972]]...]
INFO - root - 2017-12-06 04:45:59.231195: step 4410, loss = 0.83, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:13s remains)
INFO - root - 2017-12-06 04:46:01.527956: step 4420, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 20h:21m:15s remains)
INFO - root - 2017-12-06 04:46:03.810922: step 4430, loss = 0.83, batch loss = 0.61 (34.3 examples/sec; 0.233 sec/batch; 21h:15m:00s remains)
INFO - root - 2017-12-06 04:46:06.112161: step 4440, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.226 sec/batch; 20h:33m:35s remains)
INFO - root - 2017-12-06 04:46:08.372004: step 4450, loss = 0.88, batch loss = 0.67 (34.0 examples/sec; 0.235 sec/batch; 21h:26m:30s remains)
INFO - root - 2017-12-06 04:46:10.658674: step 4460, loss = 0.86, batch loss = 0.65 (36.3 examples/sec; 0.220 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-06 04:46:12.943446: step 4470, loss = 0.89, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:25s remains)
INFO - root - 2017-12-06 04:46:15.257267: step 4480, loss = 0.90, batch loss = 0.68 (34.5 examples/sec; 0.232 sec/batch; 21h:07m:48s remains)
INFO - root - 2017-12-06 04:46:17.569102: step 4490, loss = 0.81, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:32s remains)
INFO - root - 2017-12-06 04:46:19.829719: step 4500, loss = 0.82, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:39s remains)
2017-12-06 04:46:23.727087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4829412 -2.1373572 -2.6417503 -2.6375177 -2.3429863 -2.0584309 -1.8384408 -1.6006552 -1.5180275 -1.6010696 -2.1783574 -2.7794578 -3.2367313 -3.4203954 -3.3566263][-0.1940248 -0.89877594 -1.4446663 -1.6245927 -1.6161498 -1.2785684 -1.0843221 -0.97510105 -0.91636103 -1.2107444 -1.7821784 -2.3398435 -2.777194 -2.929323 -2.9825213][0.20326479 -0.27644154 -0.67238152 -0.91770494 -0.77515936 -0.46666646 -0.23804122 -0.19321516 -0.34712273 -0.82717818 -1.4716055 -2.2181742 -2.6501155 -2.7605472 -2.7757545][0.69590116 0.1218241 -0.255216 -0.52733368 -0.30862585 0.11574219 0.55884767 0.75399756 0.48968416 -0.15325542 -0.94459748 -1.8902706 -2.4309146 -2.7195385 -2.7012825][1.2242707 0.67533803 0.37712282 0.17057844 0.1783679 0.47870493 0.92237806 1.1993822 1.0180361 0.41746885 -0.63615704 -1.6494005 -2.4649215 -2.8460155 -3.0358627][1.5291922 1.0651029 0.77805454 0.65518832 0.70652497 0.73103952 1.0154872 1.3971332 1.3464383 0.77406704 -0.16483365 -1.4114954 -2.3696446 -2.8525496 -3.021687][1.8572994 1.5603983 1.3117516 1.2206105 1.2411788 1.1862961 1.2830369 1.5370462 1.6178633 1.1008139 0.3253597 -0.762732 -1.7152088 -2.2727559 -2.5370147][1.732163 1.4592694 1.2915803 1.2606031 1.1775157 1.0598558 1.064872 1.196906 1.2423768 0.97989422 0.4826948 -0.31769472 -0.80718488 -1.2177764 -1.487638][1.3810099 1.0691601 0.91284114 0.94832569 0.91644996 0.81318408 0.74237233 0.78534204 0.744407 0.58381832 0.19272666 -0.32006961 -0.59014648 -0.85815656 -0.90062577][0.95157355 0.79345495 0.6426484 0.634876 0.45659757 0.36472517 0.34213269 0.34566921 0.22698425 0.25889152 -0.053218074 -0.3339721 -0.39558446 -0.50918007 -0.57650888][0.334248 0.30970746 0.29925805 0.28031403 0.16521417 0.1885428 0.18306048 0.27533603 0.15181968 0.17727356 0.061256886 -0.24931106 -0.30808008 -0.23138534 -0.52588367][0.040905021 0.073494196 0.019758724 0.076223984 0.086710542 0.15573882 0.20379497 0.31351954 0.37135786 0.28029811 0.11246289 -0.041734956 -0.29940933 -0.20484859 -0.28529105][0.03641317 -0.083333813 -0.21836962 -0.19725995 -0.11139528 -0.082994528 -0.077764191 0.030382276 0.12331508 0.16954067 0.19793151 0.011372462 -0.18412793 0.016266502 0.14767855][-0.10784425 -0.20450279 -0.32483321 -0.34870404 -0.32414031 -0.2463257 -0.18966824 -0.10932907 -0.0036032796 0.23833515 0.16387039 0.1970738 0.0031110495 0.094085068 0.19901441][-0.058715634 -0.094264247 -0.23538406 -0.20785397 -0.20092462 -0.14845671 -0.090407424 0.069308847 0.16944239 0.38186592 0.48220295 0.58731985 0.62385505 0.56658781 0.49868679]]...]
INFO - root - 2017-12-06 04:46:25.973837: step 4510, loss = 0.88, batch loss = 0.66 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:23s remains)
INFO - root - 2017-12-06 04:46:28.272498: step 4520, loss = 0.88, batch loss = 0.67 (35.8 examples/sec; 0.223 sec/batch; 20h:19m:57s remains)
INFO - root - 2017-12-06 04:46:30.591444: step 4530, loss = 0.82, batch loss = 0.61 (34.7 examples/sec; 0.230 sec/batch; 20h:59m:27s remains)
INFO - root - 2017-12-06 04:46:32.873085: step 4540, loss = 0.88, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:18m:10s remains)
INFO - root - 2017-12-06 04:46:35.115658: step 4550, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:40m:12s remains)
INFO - root - 2017-12-06 04:46:37.417814: step 4560, loss = 0.86, batch loss = 0.65 (33.3 examples/sec; 0.241 sec/batch; 21h:54m:47s remains)
INFO - root - 2017-12-06 04:46:39.723798: step 4570, loss = 0.82, batch loss = 0.61 (34.7 examples/sec; 0.231 sec/batch; 20h:59m:58s remains)
INFO - root - 2017-12-06 04:46:42.005299: step 4580, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:09s remains)
INFO - root - 2017-12-06 04:46:44.288574: step 4590, loss = 0.91, batch loss = 0.70 (33.8 examples/sec; 0.237 sec/batch; 21h:33m:13s remains)
INFO - root - 2017-12-06 04:46:46.561518: step 4600, loss = 0.90, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:12m:05s remains)
2017-12-06 04:46:49.905689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.079571635 -0.011092335 0.057484016 -0.022765867 -0.0083065219 -0.028935019 -0.046793319 -0.084954724 -0.18225975 -0.25268489 -0.28806037 -0.3269574 -0.40478262 -0.43445256 -0.45539498][-0.027691808 0.020994335 0.11002432 0.03631001 0.074768275 0.081017211 0.039890096 0.018644601 -0.066056445 -0.16083884 -0.21998452 -0.28292298 -0.39558497 -0.40840214 -0.44997963][0.13004908 0.10327005 0.13183166 0.089353666 0.13355677 0.14043079 0.12539349 0.073247716 -0.0015071258 -0.062050186 -0.12901656 -0.21584478 -0.32733488 -0.40952885 -0.46127644][0.12187949 0.074267447 0.050427914 0.018629879 0.072096378 0.083529994 0.081035987 0.050177611 -0.010565083 -0.10717471 -0.18182571 -0.20844334 -0.3053301 -0.40379313 -0.47229037][-0.0063422918 -0.049099036 -0.10255733 -0.11813092 -0.059637181 -0.001988858 0.038500763 0.031394422 -0.017603118 -0.082682557 -0.14641616 -0.2494079 -0.42327982 -0.46507102 -0.522211][-0.14182073 -0.20560271 -0.22131824 -0.24383192 -0.17421189 -0.095373519 -0.043213781 -0.022151921 -0.033662796 -0.071845412 -0.13495384 -0.21932493 -0.38326591 -0.44212404 -0.50515604][-0.12391502 -0.31130278 -0.32472986 -0.34375772 -0.28833592 -0.20284954 -0.13782078 -0.09355659 -0.082012109 -0.11593261 -0.14577526 -0.20898364 -0.3577185 -0.40396869 -0.44612882][-0.21722992 -0.45123166 -0.4603501 -0.45812127 -0.40667623 -0.33000648 -0.27917588 -0.23176233 -0.19248596 -0.18730839 -0.20065318 -0.25346142 -0.35243654 -0.39705598 -0.45638573][-0.33965883 -0.46309242 -0.52854657 -0.54713792 -0.48852193 -0.42080811 -0.37867385 -0.33624324 -0.29578787 -0.26091191 -0.25283712 -0.24117056 -0.30027452 -0.36624917 -0.40946853][-0.37081358 -0.43908024 -0.52742624 -0.5752188 -0.52461743 -0.44892055 -0.39653358 -0.3581557 -0.3266086 -0.2928538 -0.25739044 -0.19982326 -0.21872239 -0.23115158 -0.26006404][-0.32367253 -0.33091834 -0.41509548 -0.50881505 -0.49354196 -0.44303793 -0.42709219 -0.40394834 -0.36406717 -0.32595539 -0.29154542 -0.23694506 -0.22988831 -0.24090447 -0.26119846][-0.1396428 -0.12784949 -0.28059596 -0.38754636 -0.38860217 -0.36987269 -0.38654885 -0.38023907 -0.3514677 -0.33574778 -0.32154521 -0.27867252 -0.28427523 -0.28726968 -0.26343727][-0.079656154 -0.0040675253 -0.10931263 -0.29297557 -0.33048031 -0.32641777 -0.33263791 -0.32718837 -0.32086077 -0.31458414 -0.3272447 -0.29453412 -0.2977356 -0.29771227 -0.28204465][0.00010270625 0.026685543 -0.041156486 -0.18223418 -0.21572575 -0.24450441 -0.27427852 -0.2999846 -0.30926889 -0.31474876 -0.3511523 -0.34458712 -0.34897968 -0.34081864 -0.32638836][0.059430107 0.029776514 -0.091116846 -0.20666456 -0.18546933 -0.16832422 -0.1723475 -0.21249646 -0.21814913 -0.23872048 -0.27802455 -0.25499272 -0.28214073 -0.28857672 -0.30974251]]...]
INFO - root - 2017-12-06 04:46:52.271962: step 4610, loss = 0.83, batch loss = 0.61 (36.3 examples/sec; 0.220 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-06 04:46:54.634662: step 4620, loss = 0.83, batch loss = 0.62 (34.3 examples/sec; 0.234 sec/batch; 21h:16m:15s remains)
INFO - root - 2017-12-06 04:46:56.930518: step 4630, loss = 0.77, batch loss = 0.56 (33.4 examples/sec; 0.240 sec/batch; 21h:48m:58s remains)
INFO - root - 2017-12-06 04:46:59.233234: step 4640, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.231 sec/batch; 21h:01m:00s remains)
INFO - root - 2017-12-06 04:47:01.489783: step 4650, loss = 0.79, batch loss = 0.58 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:19s remains)
INFO - root - 2017-12-06 04:47:03.794866: step 4660, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:50m:04s remains)
INFO - root - 2017-12-06 04:47:06.060374: step 4670, loss = 0.83, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 21h:19m:30s remains)
INFO - root - 2017-12-06 04:47:08.329266: step 4680, loss = 0.83, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 21h:19m:00s remains)
INFO - root - 2017-12-06 04:47:10.615432: step 4690, loss = 0.79, batch loss = 0.58 (34.1 examples/sec; 0.235 sec/batch; 21h:21m:23s remains)
INFO - root - 2017-12-06 04:47:12.893844: step 4700, loss = 0.88, batch loss = 0.67 (32.1 examples/sec; 0.249 sec/batch; 22h:39m:43s remains)
2017-12-06 04:47:13.340516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019848727 -0.019268952 -0.020288285 -0.02154148 -0.022924118 -0.024342917 -0.025445256 -0.026049767 -0.026113473 -0.025904939 -0.025562398 -0.025231209 -0.025084641 -0.025534987 -0.026676994][-0.01839678 -0.0178294 -0.019104801 -0.020672541 -0.022490989 -0.024498407 -0.026312232 -0.027501952 -0.027805272 -0.027657386 -0.027087707 -0.026309684 -0.025821913 -0.02628221 -0.02794715][-0.017633598 -0.017177664 -0.018697418 -0.020696186 -0.023167506 -0.026026431 -0.028862525 -0.031062339 -0.032175682 -0.0323394 -0.031476565 -0.030128829 -0.029227447 -0.029671665 -0.0317486][-0.017595991 -0.017192822 -0.018956434 -0.021396674 -0.024576351 -0.028474305 -0.03253733 -0.036105011 -0.038372446 -0.039226189 -0.038255669 -0.036286391 -0.03490724 -0.035308041 -0.037958764][-0.017900497 -0.017593026 -0.019616906 -0.022341251 -0.025942203 -0.030815002 -0.036258973 -0.04140177 -0.045191754 -0.046981514 -0.046411887 -0.044043355 -0.04206362 -0.04218141 -0.044972684][-0.018163793 -0.017931998 -0.019922849 -0.02263673 -0.026357412 -0.031759474 -0.038330127 -0.044994496 -0.050320223 -0.053499937 -0.053805634 -0.051674068 -0.049376152 -0.049173392 -0.051639006][-0.019042999 -0.01856542 -0.02004132 -0.022203438 -0.025563795 -0.030976459 -0.038275581 -0.04610474 -0.052727807 -0.057254728 -0.058634538 -0.057215191 -0.055171549 -0.055012722 -0.057319347][-0.020664733 -0.019779671 -0.02057993 -0.021888137 -0.024293281 -0.029102486 -0.036483888 -0.044864677 -0.051991403 -0.057251513 -0.059718005 -0.059450541 -0.058191214 -0.058421023 -0.061173018][-0.022495441 -0.021623123 -0.022050176 -0.022760741 -0.024363231 -0.028015953 -0.034186058 -0.041681252 -0.048278973 -0.053592078 -0.056955125 -0.058010954 -0.057965726 -0.058949493 -0.062243503][-0.023583449 -0.023013152 -0.023377594 -0.023995824 -0.025385205 -0.028228659 -0.032862388 -0.038181193 -0.043065332 -0.047505982 -0.05106698 -0.053071432 -0.054126069 -0.056131762 -0.060032785][-0.023799114 -0.023489069 -0.023935091 -0.024558466 -0.02568917 -0.02768847 -0.030863833 -0.034445751 -0.03779744 -0.041282959 -0.044564087 -0.047207844 -0.049213316 -0.051838383 -0.05588435][-0.023103226 -0.022808738 -0.02339948 -0.0241891 -0.025323011 -0.026851751 -0.028914928 -0.031233132 -0.033642456 -0.036326844 -0.0391747 -0.041974813 -0.044393241 -0.047088433 -0.050947987][-0.021354634 -0.021364074 -0.022286281 -0.023340642 -0.024537545 -0.025869902 -0.027368728 -0.028859019 -0.030788284 -0.0329222 -0.035247818 -0.037634384 -0.039902061 -0.042471357 -0.046194464][-0.019625809 -0.019885574 -0.021260761 -0.022660658 -0.024029776 -0.025318563 -0.026638519 -0.027931184 -0.029575706 -0.031327303 -0.033176824 -0.035106774 -0.037116542 -0.0396791 -0.043311566][-0.019776918 -0.019884001 -0.021225747 -0.022614542 -0.024006981 -0.025248405 -0.026500452 -0.027821135 -0.029314384 -0.030749947 -0.032182224 -0.033713203 -0.035650358 -0.038171794 -0.041686412]]...]
INFO - root - 2017-12-06 04:47:15.627833: step 4710, loss = 0.89, batch loss = 0.68 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:43s remains)
INFO - root - 2017-12-06 04:47:17.959691: step 4720, loss = 0.83, batch loss = 0.62 (33.1 examples/sec; 0.242 sec/batch; 21h:59m:22s remains)
INFO - root - 2017-12-06 04:47:20.265016: step 4730, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 20h:57m:05s remains)
INFO - root - 2017-12-06 04:47:22.582775: step 4740, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:12s remains)
INFO - root - 2017-12-06 04:47:24.904448: step 4750, loss = 0.87, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:37s remains)
INFO - root - 2017-12-06 04:47:27.199040: step 4760, loss = 0.94, batch loss = 0.72 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-06 04:47:29.529251: step 4770, loss = 0.97, batch loss = 0.76 (32.8 examples/sec; 0.244 sec/batch; 22h:14m:09s remains)
INFO - root - 2017-12-06 04:47:31.774605: step 4780, loss = 0.92, batch loss = 0.71 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:55s remains)
INFO - root - 2017-12-06 04:47:34.072853: step 4790, loss = 0.84, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:05s remains)
INFO - root - 2017-12-06 04:47:36.354460: step 4800, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:04m:55s remains)
2017-12-06 04:47:36.720204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.30458117 -0.1766668 -0.15375304 -0.27849498 -0.34356567 -0.30995068 -0.27138904 -0.17373866 -0.014445916 0.091404893 0.17276818 0.20647156 0.19902587 0.2224974 0.24950302][-0.46272394 -0.32025498 -0.21702787 -0.28340414 -0.33603296 -0.34151092 -0.3123917 -0.24096426 -0.14035997 -0.020962447 0.12549397 0.21408692 0.22880983 0.24466535 0.2592918][-0.59296316 -0.530542 -0.40267652 -0.41976479 -0.46757662 -0.42893207 -0.36033809 -0.29747412 -0.22450325 -0.14672482 -0.043158844 0.073759221 0.16575772 0.23260161 0.27193138][-0.5418179 -0.48976442 -0.38276672 -0.37992427 -0.37078494 -0.35184324 -0.34607813 -0.32528839 -0.3174454 -0.2874341 -0.24905545 -0.18255699 -0.073455371 0.047001377 0.14476696][-0.47098273 -0.44913703 -0.36641252 -0.31498984 -0.2414864 -0.19094124 -0.17391771 -0.17743835 -0.20038792 -0.20479375 -0.21117276 -0.1692344 -0.045924451 0.093882613 0.21689779][-0.39732045 -0.49840248 -0.3985534 -0.26140377 -0.14602554 -0.061706245 -0.0089801587 0.0030925125 -0.037396967 -0.090492412 -0.13467869 -0.1353801 -0.07128945 0.035758093 0.17060259][-0.34957686 -0.46367887 -0.39294884 -0.23756713 -0.10585168 -0.00033400208 0.070746474 0.089145504 0.044497535 -0.018869676 -0.069197305 -0.089087613 -0.070965506 -0.022483505 0.081018053][-0.39546236 -0.49268246 -0.37938166 -0.22043306 -0.049136721 0.058360673 0.10395809 0.094257288 0.028826304 -0.049376104 -0.11017732 -0.1216462 -0.10872488 -0.059148163 0.032870926][-0.48830873 -0.50000435 -0.37690881 -0.21648976 -0.014319316 0.077678137 0.09374968 0.050501175 -0.047896102 -0.14172465 -0.18959455 -0.17755392 -0.14044848 -0.073518723 0.0315125][-0.37675515 -0.40968731 -0.32674274 -0.17415488 0.025125884 0.12512329 0.11997711 0.038149171 -0.083174787 -0.21662921 -0.2992999 -0.27149639 -0.20301756 -0.11390078 0.020783342][-0.47036323 -0.34581187 -0.27422643 -0.16326992 0.034387328 0.14969215 0.1400665 0.06466838 -0.051310882 -0.19549453 -0.28314573 -0.29110923 -0.22837788 -0.1164839 0.037864126][-0.56332552 -0.3599582 -0.28336245 -0.21856177 -0.03712282 0.12292346 0.16419241 0.11480593 0.012890093 -0.12191273 -0.21320894 -0.2406013 -0.18135551 -0.10359636 0.041380912][-0.52591681 -0.33127761 -0.25645444 -0.24355927 -0.12258175 0.03741277 0.10912538 0.13992822 0.11041252 -0.0075438544 -0.11873704 -0.16055098 -0.1396493 -0.075436756 0.044143997][-0.29627764 -0.18748471 -0.11263359 -0.14527261 -0.088424578 0.0071875453 0.062950976 0.10504424 0.092596896 0.025266089 -0.034711603 -0.083961047 -0.11328684 -0.094289444 0.022613727][-0.17895004 0.020368919 0.066109084 0.011851527 0.030397289 0.07401634 0.14428046 0.13478616 0.095964707 0.033908792 -0.031901244 -0.08690992 -0.11434447 -0.089447163 -0.0048190728]]...]
INFO - root - 2017-12-06 04:47:39.019815: step 4810, loss = 0.90, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 21h:13m:20s remains)
INFO - root - 2017-12-06 04:47:41.319322: step 4820, loss = 0.92, batch loss = 0.71 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:53s remains)
INFO - root - 2017-12-06 04:47:43.637670: step 4830, loss = 0.75, batch loss = 0.54 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:05s remains)
INFO - root - 2017-12-06 04:47:45.943301: step 4840, loss = 0.87, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 21h:07m:30s remains)
INFO - root - 2017-12-06 04:47:48.242371: step 4850, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:48s remains)
INFO - root - 2017-12-06 04:47:50.501746: step 4860, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 20h:52m:24s remains)
INFO - root - 2017-12-06 04:47:52.809960: step 4870, loss = 0.81, batch loss = 0.60 (36.0 examples/sec; 0.222 sec/batch; 20h:12m:03s remains)
INFO - root - 2017-12-06 04:47:55.108602: step 4880, loss = 0.76, batch loss = 0.55 (34.0 examples/sec; 0.235 sec/batch; 21h:25m:21s remains)
INFO - root - 2017-12-06 04:47:57.421561: step 4890, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.232 sec/batch; 21h:09m:17s remains)
INFO - root - 2017-12-06 04:47:59.729322: step 4900, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:45s remains)
2017-12-06 04:48:00.151337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033061668 -0.047806159 -0.053900987 -0.045856178 -0.033572741 -0.024871845 -0.019337267 -0.019829921 -0.032980755 -0.059040681 -0.084069334 -0.0956106 -0.090923749 -0.073341906 -0.05267033][-0.015730128 -0.036306847 -0.05028053 -0.052884817 -0.051268227 -0.050567746 -0.04819094 -0.043133084 -0.047045846 -0.065033332 -0.0856835 -0.096469671 -0.093542829 -0.077210754 -0.058378443][-0.014516681 -0.033793297 -0.046909716 -0.053672634 -0.060425051 -0.067174926 -0.068516649 -0.062884681 -0.063088328 -0.076040581 -0.092580214 -0.10341808 -0.10189487 -0.086467952 -0.065166511][-0.017637402 -0.0312353 -0.038740572 -0.044524215 -0.056892339 -0.069878519 -0.070862778 -0.058864027 -0.052864052 -0.066105038 -0.089774847 -0.10921343 -0.11474031 -0.1051531 -0.086162016][-0.01681032 -0.028325886 -0.032242939 -0.0369399 -0.052933365 -0.07116501 -0.073113725 -0.056868635 -0.044246607 -0.051768392 -0.07429783 -0.096149907 -0.10794203 -0.10813046 -0.096274532][-0.011727922 -0.020249251 -0.023084246 -0.029878017 -0.049314857 -0.069913596 -0.073209316 -0.055807471 -0.040975787 -0.044960268 -0.0659182 -0.088705122 -0.10242587 -0.1054179 -0.092874914][-0.021907542 -0.023500968 -0.024021514 -0.032257311 -0.053210109 -0.073985815 -0.075206682 -0.055395912 -0.036332432 -0.038086917 -0.058191214 -0.082230493 -0.097040713 -0.099191137 -0.083147071][-0.043488532 -0.038857888 -0.031085681 -0.030913025 -0.044905595 -0.063698657 -0.066201292 -0.048914596 -0.029745482 -0.026790343 -0.042231828 -0.064424165 -0.083585739 -0.090558723 -0.0761799][-0.052801847 -0.047779437 -0.044064656 -0.045824111 -0.056459658 -0.067320585 -0.06488505 -0.049109206 -0.033167377 -0.030625042 -0.041962564 -0.057623658 -0.072840005 -0.08007843 -0.068760552][-0.061361209 -0.051572621 -0.050254591 -0.058746427 -0.075506888 -0.08690574 -0.081831679 -0.064197943 -0.049857885 -0.050060574 -0.057515208 -0.06518741 -0.072573289 -0.077934 -0.0701236][-0.082732737 -0.066907756 -0.060137808 -0.066198647 -0.081329629 -0.0910015 -0.087007262 -0.07391604 -0.065628737 -0.066720933 -0.070954 -0.074638352 -0.076839663 -0.079753205 -0.075273775][-0.10410757 -0.091385305 -0.08321923 -0.083471358 -0.088783421 -0.091111064 -0.087444738 -0.080479376 -0.080379315 -0.084427513 -0.088000052 -0.086949304 -0.083882682 -0.08421205 -0.083472833][-0.10905871 -0.10244216 -0.098450825 -0.0985697 -0.10006869 -0.098015547 -0.092250139 -0.087766431 -0.089981705 -0.095410988 -0.098891065 -0.096586585 -0.0919655 -0.090449795 -0.090306848][-0.10198916 -0.097702056 -0.094037235 -0.093094632 -0.092144594 -0.088918209 -0.085977912 -0.085408613 -0.088906169 -0.093432464 -0.096630752 -0.095607914 -0.091839127 -0.089820489 -0.090032071][-0.094935238 -0.0917361 -0.08668752 -0.081995927 -0.077899434 -0.075160682 -0.072880656 -0.07242728 -0.076525271 -0.0825324 -0.087533772 -0.089281976 -0.087120607 -0.085678935 -0.086943418]]...]
INFO - root - 2017-12-06 04:48:02.406558: step 4910, loss = 0.89, batch loss = 0.68 (36.4 examples/sec; 0.219 sec/batch; 19h:58m:23s remains)
INFO - root - 2017-12-06 04:48:04.715621: step 4920, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:43m:30s remains)
INFO - root - 2017-12-06 04:48:07.035627: step 4930, loss = 0.87, batch loss = 0.65 (34.2 examples/sec; 0.234 sec/batch; 21h:16m:48s remains)
INFO - root - 2017-12-06 04:48:09.363351: step 4940, loss = 0.86, batch loss = 0.65 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:11s remains)
INFO - root - 2017-12-06 04:48:11.678416: step 4950, loss = 0.88, batch loss = 0.67 (34.1 examples/sec; 0.235 sec/batch; 21h:20m:40s remains)
INFO - root - 2017-12-06 04:48:13.977051: step 4960, loss = 0.87, batch loss = 0.66 (34.2 examples/sec; 0.234 sec/batch; 21h:17m:43s remains)
INFO - root - 2017-12-06 04:48:16.279307: step 4970, loss = 0.84, batch loss = 0.63 (33.6 examples/sec; 0.238 sec/batch; 21h:40m:02s remains)
INFO - root - 2017-12-06 04:48:18.643584: step 4980, loss = 0.94, batch loss = 0.72 (33.4 examples/sec; 0.240 sec/batch; 21h:48m:53s remains)
INFO - root - 2017-12-06 04:48:20.955534: step 4990, loss = 0.81, batch loss = 0.59 (35.8 examples/sec; 0.223 sec/batch; 20h:18m:18s remains)
INFO - root - 2017-12-06 04:48:23.242455: step 5000, loss = 0.82, batch loss = 0.61 (35.0 examples/sec; 0.228 sec/batch; 20h:46m:56s remains)
2017-12-06 04:48:23.638623: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062947847 0.063663654 0.061297737 0.056878574 0.05069299 0.042799279 0.03435488 0.026667893 0.021912411 0.02001752 0.018999949 0.016924068 0.012518823 0.0051198229 -0.0071341246][0.057767592 0.059017293 0.058034874 0.055099614 0.049953222 0.042499714 0.033782527 0.024910428 0.018263072 0.014793724 0.013360202 0.011475213 0.0079632849 0.0016676933 -0.0097063035][0.049692094 0.051842116 0.052569382 0.051741458 0.048294507 0.041892879 0.033318281 0.023630932 0.015474677 0.010669217 0.008541882 0.0068324432 0.0040875822 -0.0013114586 -0.011621058][0.041097626 0.044116162 0.046634898 0.047986157 0.046625935 0.041810207 0.0339647 0.024186783 0.015108444 0.0091157779 0.0060735121 0.0043595657 0.0021113753 -0.0024498105 -0.011830747][0.033073239 0.037010767 0.04111883 0.044480406 0.0451423 0.041888826 0.035210572 0.025849149 0.016164258 0.009040907 0.0051706359 0.0034414008 0.0016036779 -0.0022508428 -0.010745198][0.027747236 0.032154188 0.037266642 0.042003155 0.044050023 0.041996278 0.03644044 0.027856626 0.017968386 0.010267295 0.0059302822 0.0043572485 0.002831459 -0.00068405271 -0.0087997876][0.023986854 0.028458871 0.033982739 0.03956233 0.042472713 0.041757315 0.03738679 0.029369034 0.019408084 0.011788189 0.0077144653 0.006428197 0.0048901513 0.0010633245 -0.0074057654][0.020988256 0.025575921 0.03116034 0.03681276 0.039979741 0.04006806 0.036821745 0.029387832 0.019748434 0.01281628 0.0095115453 0.0084273 0.0063998774 0.0017395541 -0.00767114][0.01944311 0.024087943 0.029427059 0.034538925 0.037329875 0.0371197 0.03369499 0.026674852 0.018457375 0.013072811 0.010847554 0.010024123 0.0075327903 0.0020382628 -0.0083405823][0.018564284 0.022721857 0.027102187 0.031111494 0.033320978 0.032720298 0.029159069 0.02278854 0.016042776 0.012353785 0.011046112 0.01028017 0.0076522306 0.0013781488 -0.009757176][0.016513489 0.02029559 0.023631863 0.026395582 0.027694441 0.026813522 0.023617245 0.018366449 0.013175532 0.010734573 0.010070883 0.0093625113 0.0066845194 3.6507845e-07 -0.011203859][0.013676889 0.017000593 0.019491762 0.021304168 0.021929242 0.020912439 0.01814732 0.013921045 0.00982555 0.0079770833 0.0075176507 0.0066236779 0.0039110184 -0.0023934469 -0.012678977][0.010059148 0.012827486 0.014618464 0.01598949 0.016434081 0.015684076 0.013679244 0.010597914 0.0076050088 0.0060608163 0.0052991584 0.0038433522 0.0011167377 -0.0044858307 -0.013262335][0.0065911487 0.0087132379 0.0098088458 0.010664046 0.011036605 0.01076182 0.0096717477 0.0077342317 0.0059536695 0.0049817786 0.0041768476 0.0027976483 0.0006230697 -0.0038978606 -0.010874394][0.0017418936 0.0032209456 0.0036434382 0.0040162653 0.0042617545 0.004375197 0.0040021837 0.003024213 0.0021843985 0.0015274584 0.0007186085 -0.00038214773 -0.0019394755 -0.00507921 -0.01019216]]...]
INFO - root - 2017-12-06 04:48:25.995886: step 5010, loss = 0.88, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 20h:24m:12s remains)
INFO - root - 2017-12-06 04:48:28.291314: step 5020, loss = 0.80, batch loss = 0.59 (33.8 examples/sec; 0.237 sec/batch; 21h:32m:12s remains)
INFO - root - 2017-12-06 04:48:30.602595: step 5030, loss = 0.87, batch loss = 0.66 (33.9 examples/sec; 0.236 sec/batch; 21h:26m:43s remains)
INFO - root - 2017-12-06 04:48:32.913869: step 5040, loss = 0.87, batch loss = 0.66 (35.8 examples/sec; 0.224 sec/batch; 20h:20m:39s remains)
INFO - root - 2017-12-06 04:48:35.217203: step 5050, loss = 0.81, batch loss = 0.60 (33.5 examples/sec; 0.239 sec/batch; 21h:44m:38s remains)
INFO - root - 2017-12-06 04:48:37.513300: step 5060, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.227 sec/batch; 20h:41m:22s remains)
INFO - root - 2017-12-06 04:48:39.900643: step 5070, loss = 0.85, batch loss = 0.64 (32.1 examples/sec; 0.249 sec/batch; 22h:40m:36s remains)
INFO - root - 2017-12-06 04:48:42.169786: step 5080, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 21h:08m:36s remains)
INFO - root - 2017-12-06 04:48:44.443264: step 5090, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 20h:52m:08s remains)
INFO - root - 2017-12-06 04:48:46.705433: step 5100, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 21h:02m:59s remains)
2017-12-06 04:48:47.047146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02082932 -0.015048567 -0.0090077519 -0.0026726052 0.0040808618 0.010162137 0.014387995 0.015871055 0.014969535 0.012022316 0.0078152493 0.0029896647 -0.0015289411 -0.0053388849 -0.0083586574][-0.032239728 -0.025773067 -0.01843128 -0.010547161 -0.0021112636 0.0051594004 0.0097392574 0.010917544 0.0093533993 0.005581215 0.00042143464 -0.0054563209 -0.010478679 -0.014088884 -0.01632829][-0.040791504 -0.034180749 -0.026255179 -0.017215118 -0.0073906332 0.00079304725 0.0055456758 0.0062689707 0.0038351044 -0.00087148696 -0.006896697 -0.013490915 -0.018800754 -0.022040062 -0.023431186][-0.045477938 -0.039016079 -0.030890882 -0.021301992 -0.010812581 -0.0020905584 0.002656348 0.0030620694 4.8622489e-05 -0.0051998869 -0.011727933 -0.018597979 -0.023896944 -0.026775181 -0.027621452][-0.045130055 -0.039328188 -0.031815153 -0.02226644 -0.011597302 -0.0027181506 0.0018045977 0.0021718517 -0.00095564127 -0.0064795986 -0.013159856 -0.019920964 -0.02511698 -0.027661838 -0.028311811][-0.040618546 -0.0359113 -0.029372618 -0.020536032 -0.010284085 -0.0017590597 0.0024082959 0.0028126016 -8.3968043e-05 -0.0054131746 -0.011775676 -0.017829098 -0.022474281 -0.024897024 -0.025664192][-0.03406848 -0.030434262 -0.024779506 -0.016808484 -0.0074257776 0.00033305585 0.0042613745 0.0048951283 0.0026214644 -0.0022234172 -0.0079791918 -0.013332661 -0.017350588 -0.019616876 -0.020564456][-0.027994659 -0.025035646 -0.0201861 -0.013104703 -0.0047572628 0.002098605 0.0058988482 0.00703191 0.0055395439 0.0016741678 -0.0031348914 -0.0074841529 -0.010641947 -0.012753993 -0.014042035][-0.022945818 -0.020645037 -0.01654885 -0.010516424 -0.0036137551 0.0023412183 0.0059944168 0.007693544 0.007024467 0.0043224171 0.00097536296 -0.0019895732 -0.0041747466 -0.0059918016 -0.0073056296][-0.018856745 -0.016809307 -0.01334865 -0.0083529055 -0.0029923171 0.0016122311 0.004543595 0.0062028244 0.0062443912 0.0048494935 0.0030782968 0.0015087947 0.00041606277 -0.00091779977 -0.0021824241][-0.014961038 -0.012802869 -0.0099640675 -0.00612808 -0.0023874193 0.00067844242 0.0024998412 0.0036297441 0.0039775 0.0036924854 0.0032308176 0.0027079955 0.002361618 0.0014125556 0.000145711][-0.010955039 -0.0082774013 -0.0058280751 -0.0031098351 -0.00096619129 0.00048828125 0.0011624992 0.0015997514 0.0018495396 0.0019032508 0.0019799992 0.0018981248 0.0018143728 0.001211755 -6.0223043e-05][-0.0067885518 -0.0033171177 -0.001281254 0.00033701211 0.0008501932 0.00070884079 0.00022833794 1.5646219e-07 2.0585954e-05 0.00010703504 0.00037830323 0.00059713423 0.00077086687 0.0005100891 -0.00074052066][-0.003664583 0.00045856088 0.0020792261 0.0027893558 0.0021242052 0.00082877278 -0.00048234314 -0.0011396408 -0.0011678711 -0.0010333657 -0.00066138804 -0.00029668957 4.016608e-05 9.2163682e-05 -0.001117155][-0.0028284118 0.0014966056 0.0024243966 0.0021566823 0.00069761276 -0.0014991388 -0.0033644587 -0.0042639226 -0.0041435882 -0.0039107949 -0.0036009178 -0.0033111498 -0.0029966161 -0.0028964356 -0.0040000752]]...]
INFO - root - 2017-12-06 04:48:49.379356: step 5110, loss = 0.91, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:35s remains)
INFO - root - 2017-12-06 04:48:51.677080: step 5120, loss = 0.84, batch loss = 0.63 (34.3 examples/sec; 0.234 sec/batch; 21h:14m:10s remains)
INFO - root - 2017-12-06 04:48:54.016856: step 5130, loss = 0.80, batch loss = 0.59 (32.8 examples/sec; 0.244 sec/batch; 22h:09m:05s remains)
INFO - root - 2017-12-06 04:48:56.354203: step 5140, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 20h:25m:30s remains)
INFO - root - 2017-12-06 04:48:58.682134: step 5150, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 21h:09m:36s remains)
INFO - root - 2017-12-06 04:49:00.939770: step 5160, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:19s remains)
INFO - root - 2017-12-06 04:49:03.268073: step 5170, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:37s remains)
INFO - root - 2017-12-06 04:49:05.658358: step 5180, loss = 0.92, batch loss = 0.71 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:18s remains)
INFO - root - 2017-12-06 04:49:07.938834: step 5190, loss = 0.89, batch loss = 0.67 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:01s remains)
INFO - root - 2017-12-06 04:49:10.245116: step 5200, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:44s remains)
2017-12-06 04:49:10.655882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040382888 -0.03893261 -0.038654462 -0.039856121 -0.044221908 -0.052470617 -0.063396364 -0.073407792 -0.079258911 -0.079102971 -0.072393082 -0.058930144 -0.042183883 -0.027381539 -0.019599762][-0.040535077 -0.039480709 -0.0395714 -0.040985897 -0.045248345 -0.05317463 -0.063351721 -0.072615944 -0.077594027 -0.077568747 -0.072326675 -0.060610678 -0.045476921 -0.03236657 -0.025718909][-0.04041145 -0.03913743 -0.038955826 -0.040198576 -0.043756209 -0.050525084 -0.059637044 -0.068008013 -0.072516404 -0.072906211 -0.068946131 -0.05959186 -0.047859438 -0.038427714 -0.033928782][-0.039894998 -0.037843704 -0.037050407 -0.037797891 -0.040277369 -0.045603238 -0.053414594 -0.060952216 -0.065344095 -0.066156067 -0.063076891 -0.056239784 -0.04905846 -0.044033103 -0.042015672][-0.039158974 -0.036282349 -0.0346863 -0.034168564 -0.034934364 -0.038414836 -0.044405721 -0.051103659 -0.05594879 -0.057409797 -0.055456914 -0.051514678 -0.048252933 -0.047122136 -0.047116481][-0.038472421 -0.034933697 -0.032399662 -0.030383766 -0.029292963 -0.030838426 -0.035085741 -0.041272312 -0.046908569 -0.048864946 -0.04814259 -0.046537522 -0.04634694 -0.047446955 -0.048775047][-0.038132347 -0.034460586 -0.031438485 -0.02820887 -0.025664862 -0.025334604 -0.027762439 -0.033572715 -0.039478119 -0.042240065 -0.042888459 -0.042863317 -0.04418505 -0.046338849 -0.048221812][-0.038360041 -0.034904942 -0.032006189 -0.028682068 -0.025892407 -0.024833333 -0.026300155 -0.031210657 -0.035868037 -0.038080238 -0.039147586 -0.040048733 -0.041966084 -0.044542279 -0.046626378][-0.039088931 -0.036128134 -0.033533793 -0.030504946 -0.02780275 -0.026394375 -0.027391382 -0.030922897 -0.034184441 -0.036351115 -0.037718181 -0.038967054 -0.040821433 -0.043142 -0.045053542][-0.040151931 -0.037767619 -0.035741102 -0.033263005 -0.030871414 -0.029474031 -0.030333824 -0.032625053 -0.0348134 -0.036843661 -0.038165543 -0.039237157 -0.040651616 -0.042154744 -0.043487847][-0.041213658 -0.03933239 -0.037883893 -0.036017615 -0.034195051 -0.033306748 -0.034276467 -0.035820797 -0.037273932 -0.038947269 -0.039902832 -0.040516652 -0.041250639 -0.04197596 -0.042788595][-0.041911587 -0.040354267 -0.039378338 -0.038092807 -0.036908157 -0.036827356 -0.038034823 -0.039375328 -0.040652003 -0.041914869 -0.042530756 -0.0426666 -0.042375494 -0.042103261 -0.042373151][-0.042334974 -0.04096292 -0.040275846 -0.03941948 -0.038719803 -0.039135106 -0.040478408 -0.042169355 -0.043683738 -0.044704221 -0.045039169 -0.044886798 -0.043972515 -0.043145314 -0.042941727][-0.042557959 -0.041429885 -0.04094287 -0.040432855 -0.040066831 -0.040547796 -0.041940618 -0.043924596 -0.045761786 -0.04686524 -0.047353961 -0.047182575 -0.046195872 -0.045035817 -0.044341713][-0.0428216 -0.041861951 -0.041447349 -0.041160427 -0.0411248 -0.041538231 -0.042964317 -0.045120016 -0.047046907 -0.048362661 -0.0491749 -0.049354814 -0.048708342 -0.047762819 -0.047011361]]...]
INFO - root - 2017-12-06 04:49:12.916842: step 5210, loss = 0.84, batch loss = 0.63 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:15s remains)
INFO - root - 2017-12-06 04:49:15.249133: step 5220, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 21h:00m:55s remains)
INFO - root - 2017-12-06 04:49:17.588580: step 5230, loss = 0.81, batch loss = 0.60 (35.6 examples/sec; 0.225 sec/batch; 20h:24m:48s remains)
INFO - root - 2017-12-06 04:49:19.926843: step 5240, loss = 0.82, batch loss = 0.61 (32.7 examples/sec; 0.245 sec/batch; 22h:15m:59s remains)
INFO - root - 2017-12-06 04:49:22.221147: step 5250, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.231 sec/batch; 20h:57m:49s remains)
INFO - root - 2017-12-06 04:49:24.504557: step 5260, loss = 0.77, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:14s remains)
INFO - root - 2017-12-06 04:49:26.795553: step 5270, loss = 0.79, batch loss = 0.58 (34.3 examples/sec; 0.233 sec/batch; 21h:10m:47s remains)
INFO - root - 2017-12-06 04:49:29.079524: step 5280, loss = 0.83, batch loss = 0.62 (36.3 examples/sec; 0.220 sec/batch; 20h:02m:03s remains)
INFO - root - 2017-12-06 04:49:31.369111: step 5290, loss = 0.81, batch loss = 0.60 (33.7 examples/sec; 0.237 sec/batch; 21h:35m:10s remains)
INFO - root - 2017-12-06 04:49:33.630472: step 5300, loss = 0.89, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:35s remains)
2017-12-06 04:49:34.109225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.34587798 -0.34008336 -0.56237692 -0.79394495 -0.65081739 -0.68069351 -0.681455 -0.52937174 -0.29772991 -0.088871092 0.016944893 -0.085260652 -0.14911205 -0.42324689 -0.7347765][0.76266754 0.6383996 0.31769273 -0.042542383 0.045847483 0.00024887174 -0.073760949 -0.020270899 0.2454668 0.42636627 0.46958992 0.42056668 0.34606317 -0.10323066 -0.45049524][1.3635472 1.3044852 0.92860305 0.50423694 0.53096473 0.42912641 0.35563532 0.33800912 0.44426003 0.51592767 0.51994729 0.48035756 0.32939458 -0.021186553 -0.27170873][1.5990514 1.5966462 1.1349944 0.60322785 0.67032659 0.634063 0.57954 0.55742645 0.64584649 0.69014359 0.72075927 0.64913833 0.65655518 0.31477317 -0.043796547][1.4736047 1.4394565 1.0448755 0.44260809 0.480051 0.48481342 0.46424821 0.38813421 0.37906921 0.36223108 0.38945106 0.44362637 0.50485456 0.13354811 -0.1601727][1.4654161 1.2192063 0.843562 0.11470278 0.15838307 0.20063701 0.22583136 0.23038018 0.29655588 0.28318992 0.20532548 0.22878948 0.27544686 -0.11782521 -0.41265777][1.4202853 0.87805665 0.57101071 -0.16428855 -0.26046652 -0.21905872 -0.20711476 -0.11894301 -0.037052102 0.022404864 0.11644387 0.037562624 0.012148179 -0.20672645 -0.47233054][1.3247633 0.43920276 0.20224267 -0.26748472 -0.45380881 -0.48693025 -0.50326532 -0.44281855 -0.32843423 -0.203263 0.023177728 0.037795603 0.057370305 -0.1411 -0.54099751][1.3636205 0.42842272 0.10489342 -0.18908465 -0.20261793 -0.27228552 -0.35640359 -0.33997744 -0.34360573 -0.19547927 -0.05321075 0.042415753 0.043600254 0.013810739 -0.17557716][1.2906487 0.34642175 -0.016035452 -0.12087538 -0.20444056 -0.11146893 -0.22111404 -0.28774828 -0.3094939 -0.21702363 -0.067162268 0.11034499 0.24270132 0.4676995 0.47265735][1.1814203 0.20210794 -0.22652929 -0.20557432 -0.11107725 0.13959022 0.19754088 0.11039507 0.031553216 0.15657626 0.30643573 0.49605259 0.68028879 0.93901956 0.99670315][0.98555374 0.08575438 -0.48527947 -0.46313378 -0.11427125 0.13423115 0.093894631 0.095229 0.17302921 0.33848765 0.58520818 0.80235636 1.1579845 1.3553931 1.4336311][1.1315693 0.29161984 -0.39199865 -0.41224256 -0.0919015 0.024578072 0.0057310238 0.001573205 0.15300837 0.29498407 0.4883692 0.75796163 1.1926836 1.3955094 1.5041441][1.0380129 0.30673516 -0.069257379 -0.078472041 0.30397174 0.51113153 0.63252139 0.5704962 0.54903853 0.61211085 0.71420324 0.86139143 1.1913331 1.2986418 1.3722686][1.4679211 0.94069922 0.71083093 0.56176734 0.73623276 1.0255325 1.2718797 1.3230866 1.1730238 1.2183928 1.2787539 1.330076 1.4612023 1.4016504 1.2592273]]...]
INFO - root - 2017-12-06 04:49:36.451597: step 5310, loss = 0.94, batch loss = 0.73 (35.7 examples/sec; 0.224 sec/batch; 20h:22m:24s remains)
INFO - root - 2017-12-06 04:49:38.783408: step 5320, loss = 0.82, batch loss = 0.61 (32.1 examples/sec; 0.249 sec/batch; 22h:38m:13s remains)
INFO - root - 2017-12-06 04:49:41.111615: step 5330, loss = 0.88, batch loss = 0.67 (35.5 examples/sec; 0.225 sec/batch; 20h:28m:42s remains)
INFO - root - 2017-12-06 04:49:43.479352: step 5340, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:49s remains)
INFO - root - 2017-12-06 04:49:45.773431: step 5350, loss = 0.86, batch loss = 0.65 (36.0 examples/sec; 0.222 sec/batch; 20h:12m:57s remains)
INFO - root - 2017-12-06 04:49:48.088630: step 5360, loss = 0.86, batch loss = 0.65 (32.7 examples/sec; 0.244 sec/batch; 22h:12m:28s remains)
INFO - root - 2017-12-06 04:49:50.373785: step 5370, loss = 0.82, batch loss = 0.61 (33.3 examples/sec; 0.240 sec/batch; 21h:49m:22s remains)
INFO - root - 2017-12-06 04:49:52.709837: step 5380, loss = 0.76, batch loss = 0.55 (35.8 examples/sec; 0.223 sec/batch; 20h:18m:27s remains)
INFO - root - 2017-12-06 04:49:55.040565: step 5390, loss = 0.94, batch loss = 0.73 (35.9 examples/sec; 0.223 sec/batch; 20h:14m:11s remains)
INFO - root - 2017-12-06 04:49:57.355210: step 5400, loss = 0.83, batch loss = 0.62 (33.6 examples/sec; 0.238 sec/batch; 21h:37m:22s remains)
2017-12-06 04:49:57.756118: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0717553 0.060557 0.018781804 -0.029056214 -0.07231389 -0.097848311 -0.11769293 -0.11427089 -0.096498579 -0.079256028 -0.07368169 -0.07415127 -0.072333708 -0.088931829 -0.11229187][-0.052014321 -0.095256492 -0.15751167 -0.21670428 -0.24756798 -0.26047754 -0.24926251 -0.2219055 -0.1996282 -0.18834209 -0.1886375 -0.18820664 -0.19198383 -0.20027459 -0.208962][-0.13560674 -0.17829327 -0.2419157 -0.29798174 -0.32741097 -0.33274874 -0.30040216 -0.25787032 -0.2301808 -0.21007442 -0.18513072 -0.18445477 -0.20879215 -0.23231629 -0.23536178][-0.1849142 -0.21891722 -0.25891995 -0.29750118 -0.30618361 -0.29544294 -0.27499151 -0.26249734 -0.27888826 -0.29495275 -0.27781561 -0.27724597 -0.27708241 -0.26844874 -0.24246842][-0.20275852 -0.23139423 -0.24920797 -0.25587526 -0.23950729 -0.21331066 -0.18823755 -0.19052376 -0.21790281 -0.24448848 -0.26304448 -0.28157958 -0.28499597 -0.271532 -0.23938808][-0.162355 -0.17637694 -0.17390466 -0.15439123 -0.10813802 -0.05850089 -0.029764246 -0.028545476 -0.064763315 -0.1095763 -0.16198254 -0.20885289 -0.22889659 -0.2272765 -0.21160573][-0.056155235 -0.047846824 -0.02476586 0.014508232 0.072881304 0.12728885 0.15702215 0.15038818 0.10244549 0.037012696 -0.048774436 -0.11466882 -0.15413791 -0.16773096 -0.17039952][0.0095951483 0.026425287 0.053400241 0.095206581 0.15285936 0.19741145 0.21088329 0.19487703 0.12867412 0.05262956 -0.036787488 -0.10411504 -0.13850114 -0.15292361 -0.13487661][-0.026130363 -0.023325071 -0.0026179031 0.032012589 0.07237377 0.097974353 0.092430852 0.0633071 -0.00088085234 -0.085287891 -0.16093044 -0.21352652 -0.23096433 -0.21926677 -0.17695776][-0.064960256 -0.070769958 -0.064957023 -0.046043426 -0.026806466 -0.01050359 -0.0094264522 -0.03029288 -0.083975129 -0.15697312 -0.2200129 -0.25233915 -0.2570163 -0.24276695 -0.18479589][-0.068453908 -0.074275874 -0.072325937 -0.07177937 -0.070540041 -0.060011297 -0.061290935 -0.080773234 -0.12961979 -0.19348457 -0.24108857 -0.26054761 -0.254322 -0.24117553 -0.19638067][-0.066210173 -0.079903886 -0.091601133 -0.10191301 -0.11516958 -0.126575 -0.13865787 -0.16265583 -0.20819554 -0.26488611 -0.30340138 -0.31134254 -0.2901355 -0.26149663 -0.21342811][-0.076239437 -0.099588692 -0.13338697 -0.16242434 -0.18909466 -0.2135635 -0.2329655 -0.25696728 -0.28574711 -0.32241467 -0.340232 -0.32707345 -0.28783622 -0.24496877 -0.18217906][-0.11380997 -0.14245087 -0.17955992 -0.21386349 -0.23692697 -0.25590321 -0.28175178 -0.30680898 -0.32203394 -0.33687046 -0.3256709 -0.29543731 -0.24878362 -0.20173711 -0.13360175][-0.11194147 -0.137369 -0.1679832 -0.19071604 -0.19106866 -0.19314538 -0.20194784 -0.21874774 -0.23035437 -0.23754847 -0.22821364 -0.20572254 -0.1761224 -0.14046358 -0.094724715]]...]
INFO - root - 2017-12-06 04:50:00.078779: step 5410, loss = 0.92, batch loss = 0.71 (35.3 examples/sec; 0.227 sec/batch; 20h:36m:16s remains)
INFO - root - 2017-12-06 04:50:02.365658: step 5420, loss = 0.90, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 20h:56m:46s remains)
INFO - root - 2017-12-06 04:50:04.671530: step 5430, loss = 1.00, batch loss = 0.79 (34.9 examples/sec; 0.229 sec/batch; 20h:48m:38s remains)
INFO - root - 2017-12-06 04:50:06.967273: step 5440, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 21h:00m:08s remains)
INFO - root - 2017-12-06 04:50:09.274961: step 5450, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.229 sec/batch; 20h:46m:22s remains)
INFO - root - 2017-12-06 04:50:11.561406: step 5460, loss = 0.94, batch loss = 0.73 (35.1 examples/sec; 0.228 sec/batch; 20h:43m:48s remains)
INFO - root - 2017-12-06 04:50:13.861180: step 5470, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.233 sec/batch; 21h:08m:31s remains)
INFO - root - 2017-12-06 04:50:16.157368: step 5480, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.224 sec/batch; 20h:18m:20s remains)
INFO - root - 2017-12-06 04:50:18.464848: step 5490, loss = 0.78, batch loss = 0.57 (34.0 examples/sec; 0.236 sec/batch; 21h:23m:33s remains)
INFO - root - 2017-12-06 04:50:20.799080: step 5500, loss = 0.77, batch loss = 0.55 (33.7 examples/sec; 0.238 sec/batch; 21h:35m:14s remains)
2017-12-06 04:50:21.250563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22896418 0.23027235 0.1205898 0.11685871 0.026259758 -0.016899809 0.027969994 0.058178134 -0.022604592 -0.10182555 -0.17104459 -0.15296201 -0.12428313 -0.059215575 -0.09495724][-0.0004081279 -0.1207456 -0.27214494 -0.27338916 -0.3189083 -0.31087485 -0.23979333 -0.15737449 -0.13428992 -0.07311894 -0.056722932 -0.052280724 -0.0599034 -0.12129764 -0.17208338][-0.22735152 -0.46118462 -0.64469332 -0.62756586 -0.55194533 -0.42031485 -0.26268467 -0.1186227 -0.1160816 -0.18748926 -0.30526188 -0.3396599 -0.34206614 -0.37442487 -0.36946306][-0.44148609 -0.73894083 -0.90461391 -0.92750657 -0.84698582 -0.662062 -0.42293122 -0.25272927 -0.19395296 -0.19503322 -0.26644158 -0.37256765 -0.40862614 -0.43709618 -0.40937328][-0.63706654 -0.91419554 -1.0497628 -1.1045456 -1.0292059 -0.81782883 -0.60053891 -0.45863736 -0.42324015 -0.43921259 -0.49089617 -0.54732388 -0.52145112 -0.51507026 -0.50550109][-0.76696175 -0.97308308 -1.0630152 -1.0910091 -0.98024929 -0.79418164 -0.59075242 -0.4394207 -0.39898115 -0.39398539 -0.37299392 -0.327142 -0.2497206 -0.1420584 -0.065743394][-0.87974036 -1.0155556 -1.0026126 -0.96711677 -0.83352631 -0.65116864 -0.49326414 -0.4029932 -0.37656054 -0.3447828 -0.25013122 -0.11343088 0.066623755 0.22559568 0.32632715][-0.96490669 -0.9630686 -0.89728171 -0.77748287 -0.610739 -0.50413257 -0.41811898 -0.39950007 -0.40713853 -0.42423961 -0.35962039 -0.1942656 0.067741074 0.32164714 0.49292222][-0.95999837 -0.92267793 -0.80256951 -0.60882318 -0.43208554 -0.34067893 -0.29163554 -0.33009878 -0.40800104 -0.46168974 -0.39331719 -0.26144961 -0.060601108 0.15827146 0.31723985][-0.82004464 -0.77993774 -0.63922071 -0.4787541 -0.30358574 -0.24344012 -0.20832327 -0.25538659 -0.34585288 -0.43945211 -0.44993222 -0.36143053 -0.17472307 -0.006466046 0.140466][-0.5688321 -0.57816184 -0.48172227 -0.34259173 -0.20251036 -0.12086944 -0.068946593 -0.10007336 -0.19547716 -0.30441117 -0.38737056 -0.40230697 -0.34739366 -0.22955376 -0.067117237][-0.17881539 -0.21848777 -0.19185677 -0.1124354 -0.049610913 0.0018253848 0.021184675 -0.0132607 -0.11166912 -0.20760691 -0.30147681 -0.34485081 -0.33794737 -0.28271458 -0.17562576][0.20997581 0.12857392 0.05236797 0.076162644 0.10692392 0.15018696 0.1632379 0.13914341 0.059024282 -0.013785731 -0.11437631 -0.1951538 -0.26719731 -0.24336714 -0.15801281][0.55158311 0.4304373 0.33227772 0.33368322 0.36655608 0.41229263 0.39208448 0.36403814 0.26741108 0.1954326 0.095222495 0.0036540329 -0.1106961 -0.14196515 -0.11999613][0.53595823 0.38720891 0.28774688 0.33005986 0.39918327 0.45648554 0.42817047 0.41400832 0.3539463 0.29358041 0.19615141 0.12886691 0.043033913 0.010585472 0.011762537]]...]
INFO - root - 2017-12-06 04:50:23.586623: step 5510, loss = 0.89, batch loss = 0.68 (33.5 examples/sec; 0.239 sec/batch; 21h:39m:53s remains)
INFO - root - 2017-12-06 04:50:25.926115: step 5520, loss = 0.85, batch loss = 0.64 (34.6 examples/sec; 0.231 sec/batch; 21h:01m:32s remains)
INFO - root - 2017-12-06 04:50:28.253212: step 5530, loss = 0.73, batch loss = 0.52 (32.7 examples/sec; 0.244 sec/batch; 22h:11m:23s remains)
INFO - root - 2017-12-06 04:50:30.563720: step 5540, loss = 0.82, batch loss = 0.61 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:55s remains)
INFO - root - 2017-12-06 04:50:32.892613: step 5550, loss = 0.77, batch loss = 0.56 (33.0 examples/sec; 0.243 sec/batch; 22h:01m:51s remains)
INFO - root - 2017-12-06 04:50:35.181668: step 5560, loss = 0.86, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:29m:57s remains)
INFO - root - 2017-12-06 04:50:37.517174: step 5570, loss = 0.83, batch loss = 0.62 (32.5 examples/sec; 0.246 sec/batch; 22h:23m:07s remains)
INFO - root - 2017-12-06 04:50:39.849607: step 5580, loss = 0.84, batch loss = 0.63 (35.7 examples/sec; 0.224 sec/batch; 20h:19m:47s remains)
INFO - root - 2017-12-06 04:50:42.139466: step 5590, loss = 0.81, batch loss = 0.60 (35.5 examples/sec; 0.226 sec/batch; 20h:29m:31s remains)
INFO - root - 2017-12-06 04:50:44.453988: step 5600, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:19m:22s remains)
2017-12-06 04:50:47.490409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.680586 -0.79505825 -1.0317622 -1.2299421 -1.4119884 -1.662423 -1.8790816 -2.00616 -2.005301 -1.922098 -1.9166805 -1.9607511 -1.9603989 -1.8785911 -1.853525][-0.42704093 -0.55378336 -0.78519851 -0.99923652 -1.1563095 -1.2822893 -1.3671874 -1.4359573 -1.4148331 -1.3228567 -1.2567719 -1.41297 -1.5876666 -1.6928157 -1.8434228][0.072158791 -0.12894541 -0.44244963 -0.71299195 -0.86305505 -0.99668986 -1.0968928 -1.1799151 -1.2422291 -1.2349623 -1.2229409 -1.3202726 -1.5003755 -1.7187676 -1.9155457][0.059778906 -0.14361301 -0.43589798 -0.64854103 -0.73193014 -0.787141 -0.82430589 -0.85839933 -0.89103144 -0.87704384 -0.8722204 -0.95024294 -1.1627638 -1.5437199 -1.8799902][0.15114513 -0.02852096 -0.26978815 -0.49647862 -0.60830933 -0.67707974 -0.72571719 -0.71324849 -0.71674126 -0.76465017 -0.81147158 -0.90862685 -1.2124317 -1.661741 -2.0430264][0.1765233 -0.0096689686 -0.19068035 -0.31455842 -0.36381683 -0.41456914 -0.45859969 -0.39741859 -0.3806009 -0.45916039 -0.56075996 -0.796097 -1.0764182 -1.4905388 -1.8423594][0.33502805 0.074791916 -0.13973641 -0.25534073 -0.32068342 -0.34319016 -0.33978498 -0.2603406 -0.19483814 -0.19904435 -0.31907919 -0.62023431 -1.0141971 -1.4127686 -1.8220574][0.28110576 0.020197548 -0.15381399 -0.21389848 -0.24392965 -0.25399306 -0.24561635 -0.18210718 -0.09228158 -0.096353769 -0.22543466 -0.57582092 -0.94268382 -1.3555908 -1.6916229][0.25200048 0.008648172 -0.10488974 -0.15495938 -0.15315926 -0.15641864 -0.14503039 -0.15747169 -0.14235923 -0.14321853 -0.22208843 -0.54561651 -0.95074725 -1.3412504 -1.6980529][0.18782982 -0.067662992 -0.14341745 -0.18721718 -0.16193795 -0.15208183 -0.13996331 -0.19074494 -0.19074441 -0.1999315 -0.25688702 -0.49282366 -0.8300302 -1.1933105 -1.5563904][0.12718615 -0.067323633 -0.14220239 -0.20221734 -0.21493846 -0.19831061 -0.18143784 -0.21169224 -0.22849885 -0.24291402 -0.30007488 -0.46041885 -0.74720269 -1.060007 -1.3403845][0.10748567 -0.022108637 -0.14910361 -0.21357349 -0.22150746 -0.21953979 -0.2264944 -0.23913059 -0.29508463 -0.3116318 -0.33448777 -0.38002032 -0.57307678 -0.76400542 -0.929263][0.10428966 0.024216533 -0.071107522 -0.13910033 -0.14662814 -0.14880854 -0.16102621 -0.17481855 -0.24879998 -0.27360561 -0.29824561 -0.28316846 -0.33779156 -0.35428825 -0.55923843][0.29853112 0.23212865 0.10838904 0.07858064 0.060943954 0.06708575 0.052750535 0.04113353 -0.041189052 -0.056642491 -0.035514686 -0.0714978 -0.17890917 -0.2721521 -0.39556444][0.54216939 0.426232 0.31930828 0.28613725 0.28491664 0.29608291 0.27120516 0.19824418 0.13108501 0.061185069 0.043711372 0.033061355 -0.061903428 -0.16108075 -0.22407207]]...]
INFO - root - 2017-12-06 04:50:49.826788: step 5610, loss = 0.78, batch loss = 0.57 (36.0 examples/sec; 0.222 sec/batch; 20h:10m:05s remains)
INFO - root - 2017-12-06 04:50:52.149344: step 5620, loss = 0.90, batch loss = 0.69 (35.0 examples/sec; 0.229 sec/batch; 20h:46m:07s remains)
INFO - root - 2017-12-06 04:50:54.442398: step 5630, loss = 0.81, batch loss = 0.60 (35.3 examples/sec; 0.227 sec/batch; 20h:34m:43s remains)
INFO - root - 2017-12-06 04:50:56.754800: step 5640, loss = 0.92, batch loss = 0.71 (34.3 examples/sec; 0.233 sec/batch; 21h:10m:18s remains)
INFO - root - 2017-12-06 04:50:59.049908: step 5650, loss = 0.75, batch loss = 0.54 (33.4 examples/sec; 0.240 sec/batch; 21h:45m:03s remains)
INFO - root - 2017-12-06 04:51:01.351291: step 5660, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 20h:43m:18s remains)
INFO - root - 2017-12-06 04:51:03.732829: step 5670, loss = 0.92, batch loss = 0.71 (32.0 examples/sec; 0.250 sec/batch; 22h:43m:28s remains)
INFO - root - 2017-12-06 04:51:06.060501: step 5680, loss = 0.78, batch loss = 0.57 (34.2 examples/sec; 0.234 sec/batch; 21h:13m:05s remains)
INFO - root - 2017-12-06 04:51:08.359615: step 5690, loss = 0.83, batch loss = 0.62 (34.1 examples/sec; 0.234 sec/batch; 21h:16m:03s remains)
INFO - root - 2017-12-06 04:51:10.673674: step 5700, loss = 0.84, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 20h:58m:37s remains)
2017-12-06 04:51:11.016379: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013735823 0.011968382 0.00843025 0.0041098967 -0.0011441857 -0.0071824864 -0.013985556 -0.021059845 -0.027491357 -0.03316734 -0.038123194 -0.041562468 -0.043120135 -0.043096565 -0.042418331][0.012851775 0.011160471 0.0070580244 0.0018526688 -0.0050520077 -0.013189621 -0.022421103 -0.032564197 -0.041860305 -0.050046902 -0.056432113 -0.060301915 -0.06165646 -0.060900725 -0.059650086][0.011073008 0.00924325 0.0044059232 -0.001993455 -0.010706581 -0.02128762 -0.033688691 -0.047477003 -0.060034454 -0.070743889 -0.078407854 -0.082640767 -0.083285026 -0.08151684 -0.079374127][0.00908567 0.0068047792 0.0010045692 -0.0068986043 -0.017806891 -0.031541962 -0.047646843 -0.065459445 -0.081572458 -0.094847284 -0.1035636 -0.10752814 -0.10699129 -0.10358248 -0.10011564][0.0072941631 0.0043802485 -0.0024155974 -0.012091041 -0.026018228 -0.043812919 -0.064398222 -0.0865784 -0.1061703 -0.12167814 -0.13050643 -0.13296486 -0.13014483 -0.12446797 -0.11936079][0.0057475269 0.0023218691 -0.0056263432 -0.017239574 -0.034403097 -0.056884773 -0.082338244 -0.10873736 -0.13138703 -0.14827728 -0.15668643 -0.15663174 -0.15094218 -0.14275356 -0.13584363][0.0043452382 0.00047944486 -0.0085581839 -0.021870408 -0.04192907 -0.068587936 -0.098388195 -0.12836695 -0.15336065 -0.17104813 -0.17805484 -0.17540818 -0.16687527 -0.15629427 -0.14783789][0.0039303675 -0.00024373829 -0.0099234357 -0.02488156 -0.047731087 -0.07772471 -0.11107469 -0.14365095 -0.17003156 -0.18738428 -0.19256774 -0.18705763 -0.17590159 -0.16358536 -0.15393163][0.0047334358 0.00042875856 -0.0095776245 -0.025693387 -0.050404385 -0.082831241 -0.11831615 -0.15226603 -0.17925689 -0.19588125 -0.19932863 -0.19176558 -0.1790944 -0.16633183 -0.15682681][0.0060807839 0.0019983053 -0.0082647949 -0.02488054 -0.05042664 -0.083708525 -0.11975422 -0.15356302 -0.17997237 -0.1954798 -0.19777727 -0.18944022 -0.17674811 -0.16551945 -0.15730789][0.0068289712 0.003144525 -0.0066622868 -0.022896599 -0.047806345 -0.080085337 -0.11514267 -0.14772353 -0.17299956 -0.18752649 -0.18978286 -0.18238679 -0.17135227 -0.16279224 -0.15688254][0.00702706 0.004163608 -0.0043426752 -0.019429047 -0.042431526 -0.072347313 -0.10518605 -0.13586113 -0.15993088 -0.17416127 -0.17760101 -0.17307533 -0.1653751 -0.16060296 -0.15839645][0.00673189 0.0047119483 -0.0022878647 -0.015323389 -0.035104141 -0.062001791 -0.0917482 -0.12012997 -0.14313474 -0.15788186 -0.16394947 -0.16337219 -0.16041961 -0.15998107 -0.16128287][0.0053270087 0.0042890906 -0.0014170483 -0.012390066 -0.028929345 -0.051735669 -0.077890128 -0.10379156 -0.12557921 -0.14097053 -0.15004793 -0.15383568 -0.15566054 -0.1592986 -0.16419441][0.0019734129 0.0016836971 -0.002817817 -0.011441022 -0.024736769 -0.043818384 -0.066061318 -0.089205541 -0.10952851 -0.12541004 -0.13634574 -0.14339864 -0.1490359 -0.15619656 -0.16409007]]...]
INFO - root - 2017-12-06 04:51:13.325280: step 5710, loss = 0.92, batch loss = 0.70 (34.9 examples/sec; 0.230 sec/batch; 20h:50m:12s remains)
INFO - root - 2017-12-06 04:51:15.608407: step 5720, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 20h:23m:36s remains)
INFO - root - 2017-12-06 04:51:17.895447: step 5730, loss = 0.89, batch loss = 0.68 (34.8 examples/sec; 0.230 sec/batch; 20h:52m:06s remains)
INFO - root - 2017-12-06 04:51:20.200948: step 5740, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 21h:03m:21s remains)
INFO - root - 2017-12-06 04:51:22.511201: step 5750, loss = 0.80, batch loss = 0.59 (33.7 examples/sec; 0.237 sec/batch; 21h:32m:25s remains)
INFO - root - 2017-12-06 04:51:24.827028: step 5760, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-06 04:51:27.140425: step 5770, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.233 sec/batch; 21h:07m:50s remains)
INFO - root - 2017-12-06 04:51:29.466633: step 5780, loss = 0.78, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 21h:01m:17s remains)
INFO - root - 2017-12-06 04:51:31.779320: step 5790, loss = 0.77, batch loss = 0.55 (34.7 examples/sec; 0.231 sec/batch; 20h:55m:35s remains)
INFO - root - 2017-12-06 04:51:34.082976: step 5800, loss = 0.77, batch loss = 0.55 (32.4 examples/sec; 0.247 sec/batch; 22h:22m:24s remains)
2017-12-06 04:51:34.489047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.342206 -2.5642185 -2.6795969 -2.6913459 -2.6452491 -2.6114476 -2.5983968 -2.6962082 -2.7535615 -2.8424597 -2.9429491 -2.77146 -2.4165969 -2.3156357 -2.1148415][-1.3408177 -1.6992631 -1.8286802 -1.8399439 -1.8713584 -1.9255527 -2.0179877 -2.0240428 -2.0881553 -2.0969114 -2.0105479 -1.6309721 -1.0007117 -0.64130735 -0.43665612][-0.37625265 -0.83941245 -1.0459374 -1.1180508 -1.1621199 -1.1973988 -1.2489518 -1.1959876 -1.1100087 -1.0039674 -1.0020552 -0.63806891 -0.11505652 0.48155421 0.77378005][0.50595653 0.068136752 -0.093628779 -0.14291769 -0.24047789 -0.39878142 -0.54826611 -0.55287969 -0.46479362 -0.25827175 -0.088475294 0.23775159 0.5825606 1.1866106 1.5096669][1.5074706 1.2203153 0.99546444 0.92545891 0.91241509 0.79695153 0.65778047 0.45898604 0.31507587 0.4339118 0.54066932 0.85478771 1.284214 1.6660751 1.9032761][2.4606671 2.3329329 2.1953471 2.1029658 2.0517609 1.9546345 1.8169711 1.6451255 1.4715134 1.4331486 1.4566625 1.7283179 2.1069248 2.4326344 2.4332297][3.1474085 3.0844009 3.0976837 3.0376592 3.1030059 3.0047133 2.86057 2.7766418 2.6292653 2.4151368 2.241756 2.2946653 2.4939897 2.5619206 2.4249921][3.5651906 3.3827751 3.3682714 3.48412 3.6288826 3.7116275 3.627569 3.4631929 3.260026 3.0959396 2.9400187 2.7393939 2.5974526 2.5235925 2.2324443][3.2213397 3.2129822 3.3612156 3.4949636 3.6285236 3.677273 3.5593765 3.3905871 3.1145205 2.8635967 2.6426022 2.5821221 2.4329505 2.1703062 1.7991478][2.3770885 2.3971212 2.5673609 2.7957094 2.973475 2.9691236 2.81912 2.6015327 2.3548048 2.1927867 2.0103858 1.8490613 1.7430004 1.6801168 1.3965449][1.1426388 1.156829 1.2785939 1.4241877 1.508225 1.4423902 1.2740412 1.1797076 1.0732526 0.97162521 0.87820232 0.81683618 0.73600352 0.669103 0.6365304][-0.19477546 -0.1960225 -0.11326244 0.11734144 0.27653962 0.29258877 0.21510597 0.070065767 -0.029743616 -0.11712734 -0.1670302 -0.198325 -0.19494954 -0.12482417 -0.036221948][-0.80544931 -0.82199436 -0.83081567 -0.69345719 -0.609293 -0.61561704 -0.69609082 -0.80660003 -0.92980993 -1.0332853 -1.0717959 -1.0301476 -0.91554928 -0.76174188 -0.59713525][-0.862161 -0.93990338 -1.0413527 -1.0377667 -1.0990237 -1.1922647 -1.3030932 -1.391957 -1.4638393 -1.5079685 -1.509105 -1.4571902 -1.3552181 -1.1948991 -1.0022645][-0.72991145 -0.80300421 -0.89985269 -0.9486261 -1.0388026 -1.1452616 -1.25095 -1.3588152 -1.4321619 -1.4732524 -1.4561468 -1.3899498 -1.2966564 -1.1713682 -0.98185968]]...]
INFO - root - 2017-12-06 04:51:36.790090: step 5810, loss = 0.85, batch loss = 0.64 (35.9 examples/sec; 0.223 sec/batch; 20h:14m:27s remains)
INFO - root - 2017-12-06 04:51:39.113883: step 5820, loss = 0.85, batch loss = 0.64 (34.0 examples/sec; 0.235 sec/batch; 21h:21m:01s remains)
INFO - root - 2017-12-06 04:51:41.409067: step 5830, loss = 0.87, batch loss = 0.66 (35.8 examples/sec; 0.224 sec/batch; 20h:16m:53s remains)
INFO - root - 2017-12-06 04:51:43.687550: step 5840, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:30s remains)
INFO - root - 2017-12-06 04:51:45.983957: step 5850, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.229 sec/batch; 20h:44m:01s remains)
INFO - root - 2017-12-06 04:51:48.311349: step 5860, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 20h:58m:11s remains)
INFO - root - 2017-12-06 04:51:50.654819: step 5870, loss = 0.88, batch loss = 0.67 (34.4 examples/sec; 0.233 sec/batch; 21h:07m:26s remains)
INFO - root - 2017-12-06 04:51:52.960596: step 5880, loss = 0.79, batch loss = 0.57 (33.5 examples/sec; 0.239 sec/batch; 21h:40m:41s remains)
INFO - root - 2017-12-06 04:51:55.241164: step 5890, loss = 0.84, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 20h:33m:39s remains)
INFO - root - 2017-12-06 04:51:57.543542: step 5900, loss = 0.91, batch loss = 0.70 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:29s remains)
2017-12-06 04:51:57.886510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.110025 -0.13829431 -0.16544276 -0.18873093 -0.20560476 -0.21595195 -0.21958166 -0.21729484 -0.21069542 -0.20239913 -0.19552957 -0.19232753 -0.1923359 -0.19487911 -0.19551498][-0.11594576 -0.15082243 -0.18628292 -0.21864733 -0.2437259 -0.26002848 -0.26620492 -0.26273939 -0.25186259 -0.23754531 -0.22489145 -0.21663973 -0.21334168 -0.2143631 -0.216111][-0.1123209 -0.15130253 -0.19429284 -0.23492202 -0.26800129 -0.29016343 -0.29931638 -0.29500079 -0.27916005 -0.25821626 -0.23830312 -0.22357962 -0.21562347 -0.21427816 -0.21620259][-0.097855546 -0.13915667 -0.1878359 -0.2348921 -0.27442977 -0.30146992 -0.31284192 -0.3069123 -0.28568098 -0.25797305 -0.23082185 -0.20977327 -0.19664133 -0.19207114 -0.19398166][-0.076858677 -0.11824973 -0.16960853 -0.22134367 -0.26578662 -0.29644182 -0.30936363 -0.3015323 -0.2754671 -0.24096385 -0.20740059 -0.18072698 -0.16317561 -0.15579805 -0.15733245][-0.053699538 -0.0941493 -0.14674433 -0.20093331 -0.24809712 -0.28119975 -0.2951012 -0.28574026 -0.25577894 -0.21589926 -0.1767624 -0.14512421 -0.12412954 -0.11461313 -0.11552668][-0.03201028 -0.068716317 -0.1192331 -0.1735924 -0.22203287 -0.25678807 -0.27225158 -0.26263654 -0.23055384 -0.18706286 -0.14387766 -0.10851771 -0.084906869 -0.074008144 -0.07467033][-0.01587902 -0.047825895 -0.093955643 -0.14554605 -0.19224221 -0.22691882 -0.24272251 -0.2335397 -0.20165914 -0.15773621 -0.11382963 -0.077484593 -0.053029723 -0.041778993 -0.042047657][-0.0040260553 -0.030756447 -0.071007982 -0.11725786 -0.16032714 -0.19273773 -0.20777145 -0.19993693 -0.17080134 -0.12972471 -0.088491455 -0.054411933 -0.031286784 -0.020386878 -0.020485118][0.0020141006 -0.018102497 -0.050378978 -0.088885874 -0.12540299 -0.15325242 -0.16665646 -0.16099961 -0.13677575 -0.10157087 -0.066362396 -0.037246253 -0.017755743 -0.00868167 -0.0092934445][0.0041880906 -0.009849526 -0.033874303 -0.063342378 -0.09183374 -0.11424834 -0.12573029 -0.12199213 -0.10309446 -0.075065047 -0.047004737 -0.023803078 -0.0084300563 -0.001831539 -0.0030928478][0.0047957003 -0.0044133216 -0.021167628 -0.042179681 -0.062862664 -0.079482332 -0.088081785 -0.085555732 -0.071893521 -0.051168729 -0.030037545 -0.012598563 -0.0011691973 0.00276798 0.00073449314][0.0047216788 -0.00075664371 -0.011574619 -0.02504985 -0.038482063 -0.049308859 -0.054863717 -0.053231824 -0.044094384 -0.029938985 -0.015488319 -0.0032653362 0.0046279505 0.0066514835 0.0041051656][0.0053132847 0.0022799373 -0.0042305961 -0.011986509 -0.019736409 -0.02597959 -0.029338904 -0.028353591 -0.022923157 -0.014337067 -0.0054017305 0.0026191697 0.0075363964 0.0082920715 0.0057688951][0.0041429177 0.0026819929 -0.0012570471 -0.0056664124 -0.0098921582 -0.012946259 -0.014452368 -0.013570119 -0.01059296 -0.0058055 -0.00083681941 0.0032939091 0.0057771504 0.0057755485 0.0034273416]]...]
INFO - root - 2017-12-06 04:52:00.175855: step 5910, loss = 0.75, batch loss = 0.53 (35.5 examples/sec; 0.226 sec/batch; 20h:28m:12s remains)
INFO - root - 2017-12-06 04:52:02.484163: step 5920, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:59s remains)
INFO - root - 2017-12-06 04:52:04.806300: step 5930, loss = 0.80, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:02s remains)
INFO - root - 2017-12-06 04:52:07.103714: step 5940, loss = 0.78, batch loss = 0.57 (35.4 examples/sec; 0.226 sec/batch; 20h:29m:25s remains)
INFO - root - 2017-12-06 04:52:09.416777: step 5950, loss = 0.73, batch loss = 0.52 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:02s remains)
INFO - root - 2017-12-06 04:52:11.717684: step 5960, loss = 0.82, batch loss = 0.61 (33.6 examples/sec; 0.238 sec/batch; 21h:37m:38s remains)
INFO - root - 2017-12-06 04:52:13.998907: step 5970, loss = 0.81, batch loss = 0.60 (32.7 examples/sec; 0.244 sec/batch; 22h:09m:46s remains)
INFO - root - 2017-12-06 04:52:16.281057: step 5980, loss = 0.77, batch loss = 0.55 (34.0 examples/sec; 0.235 sec/batch; 21h:20m:42s remains)
INFO - root - 2017-12-06 04:52:18.558533: step 5990, loss = 0.93, batch loss = 0.71 (35.8 examples/sec; 0.223 sec/batch; 20h:15m:40s remains)
INFO - root - 2017-12-06 04:52:20.872803: step 6000, loss = 0.78, batch loss = 0.57 (35.7 examples/sec; 0.224 sec/batch; 20h:19m:19s remains)
2017-12-06 04:52:21.275081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076851778 -0.07687147 -0.077541769 -0.077985115 -0.078612171 -0.078925557 -0.078687504 -0.077867962 -0.077249549 -0.077072538 -0.077093966 -0.076999977 -0.07653825 -0.0760733 -0.075909138][-0.074666694 -0.074569933 -0.07541687 -0.076094 -0.076735191 -0.077328168 -0.077566661 -0.077285953 -0.076952055 -0.0768522 -0.076849565 -0.076475956 -0.075817011 -0.075155176 -0.075234346][-0.073249124 -0.072962955 -0.073868476 -0.074962787 -0.075910196 -0.076677158 -0.077181138 -0.077393539 -0.077484667 -0.077518143 -0.077407382 -0.07696981 -0.076375432 -0.075877845 -0.076139413][-0.07242468 -0.071996175 -0.072801396 -0.0740904 -0.075414747 -0.076258823 -0.076830581 -0.077246413 -0.077669017 -0.077935383 -0.077896 -0.077472068 -0.077017628 -0.076646909 -0.077064022][-0.072578393 -0.071936049 -0.072532713 -0.073628858 -0.074829042 -0.075425453 -0.075851105 -0.076332539 -0.07698787 -0.077554867 -0.077606864 -0.0774101 -0.077185333 -0.077040128 -0.077350505][-0.073978268 -0.073104948 -0.073388383 -0.073754877 -0.074075684 -0.074297383 -0.074357115 -0.074657373 -0.075306416 -0.076026663 -0.07648319 -0.076600686 -0.076700181 -0.076692559 -0.076991968][-0.076155365 -0.074939996 -0.074685439 -0.074133642 -0.0733912 -0.072777405 -0.072458372 -0.072583772 -0.07314992 -0.074153841 -0.0752212 -0.0760418 -0.076490246 -0.076431811 -0.076489121][-0.078466095 -0.077119388 -0.076290764 -0.0750927 -0.073472418 -0.071789719 -0.0706709 -0.070518591 -0.07103765 -0.0723123 -0.073864609 -0.075351007 -0.076316185 -0.076433472 -0.076510109][-0.079676844 -0.078428388 -0.077528216 -0.075958274 -0.073746093 -0.071202211 -0.069237247 -0.0684525 -0.068449765 -0.069465853 -0.071310505 -0.0735263 -0.075323313 -0.076185569 -0.076452717][-0.079034656 -0.0783575 -0.077796482 -0.076163054 -0.073732518 -0.070860155 -0.068473876 -0.0668733 -0.06574817 -0.065840416 -0.067635849 -0.070696495 -0.073615581 -0.075368829 -0.076077543][-0.076466881 -0.076620556 -0.076582909 -0.075392514 -0.073269337 -0.070648007 -0.06795501 -0.065346159 -0.063047424 -0.062379718 -0.064269483 -0.06833633 -0.072295152 -0.074712187 -0.075682491][-0.072784528 -0.07395374 -0.0746777 -0.074004725 -0.072252557 -0.069991611 -0.06718237 -0.064054921 -0.061416302 -0.060937755 -0.06356547 -0.068299465 -0.072548896 -0.074890494 -0.075768538][-0.068102568 -0.070410982 -0.071767576 -0.0717866 -0.070598193 -0.06881842 -0.06646461 -0.064010546 -0.062360525 -0.063027367 -0.066438645 -0.070904158 -0.074069858 -0.0755979 -0.07592421][-0.064173356 -0.067453168 -0.068793878 -0.069010057 -0.0684037 -0.067396767 -0.066004604 -0.064855188 -0.06463249 -0.066451736 -0.0699693 -0.073429868 -0.0751586 -0.075919017 -0.075985141][-0.062757075 -0.065666668 -0.066110469 -0.066196188 -0.066117637 -0.066194221 -0.066348 -0.066748239 -0.067691475 -0.070071988 -0.073087186 -0.075226188 -0.07598076 -0.076248586 -0.076294921]]...]
INFO - root - 2017-12-06 04:52:23.612426: step 6010, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:26s remains)
INFO - root - 2017-12-06 04:52:25.935771: step 6020, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:30m:15s remains)
INFO - root - 2017-12-06 04:52:28.229825: step 6030, loss = 0.91, batch loss = 0.70 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:09s remains)
INFO - root - 2017-12-06 04:52:30.539887: step 6040, loss = 0.88, batch loss = 0.67 (34.0 examples/sec; 0.235 sec/batch; 21h:18m:27s remains)
INFO - root - 2017-12-06 04:52:32.830449: step 6050, loss = 0.87, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:31s remains)
INFO - root - 2017-12-06 04:52:35.110070: step 6060, loss = 0.84, batch loss = 0.62 (34.3 examples/sec; 0.234 sec/batch; 21h:10m:28s remains)
INFO - root - 2017-12-06 04:52:37.398631: step 6070, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:05s remains)
INFO - root - 2017-12-06 04:52:39.691750: step 6080, loss = 0.88, batch loss = 0.67 (35.8 examples/sec; 0.223 sec/batch; 20h:15m:14s remains)
INFO - root - 2017-12-06 04:52:42.002734: step 6090, loss = 0.83, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 20h:08m:20s remains)
INFO - root - 2017-12-06 04:52:44.296293: step 6100, loss = 0.90, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:09m:23s remains)
2017-12-06 04:52:46.049869: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.75844526 0.51107872 0.37492442 0.20355624 0.1460644 0.25745693 0.38216963 0.38763839 0.26425669 0.15643238 -0.094372 -0.28577757 -0.4084335 -0.55331284 -0.54374862][0.32732168 0.20274019 0.039518259 -0.10094266 -0.2295856 -0.1507012 -0.039492641 -0.030607965 -0.11243832 -0.21808802 -0.55848074 -0.868734 -1.0975193 -1.1766086 -1.1456273][0.085367694 -0.0768977 -0.25596017 -0.46382537 -0.67469382 -0.7225554 -0.67315948 -0.71077204 -0.71929944 -0.84121346 -1.1573659 -1.4210892 -1.5955348 -1.5808408 -1.5593612][-0.21611518 -0.48947287 -0.76771021 -1.0100908 -1.2625891 -1.3703676 -1.3162661 -1.3357894 -1.3677716 -1.4439716 -1.7302425 -1.9754217 -2.1372421 -2.1103427 -2.0947647][-0.30855817 -0.76506805 -1.1756165 -1.5174019 -1.8899015 -2.0846119 -2.0041034 -1.9295372 -1.9022574 -1.8883485 -1.9804393 -2.0961688 -2.149668 -2.106426 -2.1964395][-0.16407397 -0.83033216 -1.3327332 -1.7639478 -2.1696749 -2.3866842 -2.278131 -2.1303818 -1.9911661 -1.8859012 -1.791273 -1.7608248 -1.7198511 -1.853417 -1.9841859][0.25174564 -0.43732277 -0.90605986 -1.342708 -1.7325318 -1.9665235 -1.8667697 -1.6299407 -1.3961226 -1.1507425 -1.0198683 -0.97599578 -0.97704864 -1.3477496 -1.5715154][0.89431572 0.29632112 -0.070764735 -0.4403303 -0.87745988 -1.0968217 -1.0174576 -0.89004505 -0.70929849 -0.41377893 -0.16388661 -0.021991767 -0.19007832 -0.74736083 -1.0391568][1.3982506 0.79728746 0.48004362 0.31299117 0.097222224 -0.19751397 -0.29689208 -0.20787707 0.02928976 0.28140485 0.51243496 0.7113229 0.78602767 0.30856529 -0.039424006][1.5515119 1.0875854 0.91909957 0.77719319 0.54899395 0.32277566 0.23496321 0.22119221 0.345035 0.6035794 0.83633244 1.0246564 1.1445155 0.82794988 0.62815595][1.5656754 1.0277371 0.85504222 0.76126885 0.57604933 0.38228318 0.27710605 0.17128319 0.16820785 0.29062894 0.45275041 0.56211746 0.8109163 0.690945 0.54424286][1.1934031 0.57484496 0.38955623 0.27748549 0.085053727 -0.049906563 -0.17863804 -0.32022348 -0.27926993 -0.25153467 -0.22124059 0.0026836619 0.27720279 0.49999586 0.43156663][0.4874036 -0.097167388 -0.24202207 -0.32228279 -0.38927817 -0.47626892 -0.6190598 -0.68397892 -0.68310595 -0.72438025 -0.74846411 -0.61148596 -0.50475621 -0.36900297 -0.30887842][-0.012839288 -0.59283459 -0.66808832 -0.74614525 -0.78717971 -0.88309824 -0.96364284 -1.0063372 -1.0560358 -1.1983398 -1.2416226 -1.1114848 -0.95545912 -1.0634744 -1.3033372][-0.3073501 -1.0353832 -1.2033297 -1.2706629 -1.279412 -1.391712 -1.5292901 -1.5916619 -1.5972458 -1.7127748 -1.7620223 -1.5669191 -1.4799712 -1.7380408 -2.167855]]...]
INFO - root - 2017-12-06 04:52:48.322568: step 6110, loss = 0.90, batch loss = 0.69 (35.0 examples/sec; 0.228 sec/batch; 20h:41m:41s remains)
INFO - root - 2017-12-06 04:52:50.630902: step 6120, loss = 0.92, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 20h:45m:08s remains)
INFO - root - 2017-12-06 04:52:52.962064: step 6130, loss = 0.93, batch loss = 0.71 (31.6 examples/sec; 0.253 sec/batch; 22h:55m:15s remains)
INFO - root - 2017-12-06 04:52:55.277911: step 6140, loss = 0.87, batch loss = 0.66 (35.8 examples/sec; 0.223 sec/batch; 20h:14m:20s remains)
INFO - root - 2017-12-06 04:52:57.598003: step 6150, loss = 0.88, batch loss = 0.67 (34.2 examples/sec; 0.234 sec/batch; 21h:13m:45s remains)
INFO - root - 2017-12-06 04:52:59.921759: step 6160, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:45m:53s remains)
INFO - root - 2017-12-06 04:53:02.197619: step 6170, loss = 0.89, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:12m:25s remains)
INFO - root - 2017-12-06 04:53:04.519644: step 6180, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 21h:00m:54s remains)
INFO - root - 2017-12-06 04:53:06.798826: step 6190, loss = 0.84, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 20h:56m:52s remains)
INFO - root - 2017-12-06 04:53:09.087489: step 6200, loss = 0.81, batch loss = 0.60 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:32s remains)
2017-12-06 04:53:09.453284: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04979717 0.052083753 0.05307474 0.053938888 0.05460342 0.054346524 0.053136535 0.051441081 0.049212791 0.04621958 0.042442247 0.038190298 0.033739738 0.02848614 0.022520624][0.055260472 0.058083437 0.059526362 0.060761921 0.06142243 0.060735784 0.058869235 0.05646614 0.053556554 0.049851246 0.045344226 0.040492691 0.035531089 0.030020274 0.023552746][0.059341319 0.062669657 0.064555518 0.065996818 0.066437446 0.065103211 0.062414952 0.059272341 0.055697761 0.051452808 0.046523161 0.041423529 0.03627795 0.030609071 0.023898907][0.06246265 0.066338278 0.068394922 0.069775067 0.069677435 0.067573421 0.06408105 0.060240246 0.056265838 0.051770024 0.046739206 0.041674472 0.036487646 0.030788988 0.023934774][0.064667024 0.068644516 0.070348062 0.071183287 0.070348822 0.067552231 0.06348183 0.059162088 0.05516129 0.050859325 0.046133436 0.0413443 0.036206163 0.030754492 0.023921274][0.066479571 0.069970675 0.070763193 0.070505284 0.068710543 0.065333985 0.060923658 0.0565171 0.052778281 0.048967883 0.044786364 0.040442869 0.035397671 0.030071504 0.02338057][0.066517524 0.069296636 0.069157831 0.067583255 0.064837284 0.061019935 0.056601815 0.052361645 0.049163289 0.046087176 0.042705357 0.03903196 0.03433919 0.02918072 0.022655115][0.062901549 0.065175988 0.064236961 0.061753221 0.058376126 0.054522134 0.050677143 0.047059864 0.044555627 0.042436898 0.040049486 0.037233874 0.033106774 0.028113194 0.021745697][0.055898659 0.057672955 0.05635535 0.053785928 0.050413154 0.046997502 0.04391405 0.041172676 0.039433032 0.038183264 0.036741219 0.034854971 0.031343907 0.026913151 0.021011651][0.046892554 0.048553854 0.047306567 0.045156211 0.042341374 0.039645262 0.03740865 0.035571069 0.03460604 0.034159765 0.033500567 0.032342575 0.029514745 0.025783747 0.020413905][0.037517697 0.039229535 0.038340889 0.036888085 0.035148978 0.033480883 0.032125704 0.031129748 0.030745916 0.030778833 0.030596323 0.029818013 0.02750466 0.024359532 0.019619919][0.029282868 0.030994639 0.030679502 0.030175373 0.029466324 0.028774269 0.028221272 0.027881458 0.027841955 0.027989075 0.027985431 0.027603008 0.025774874 0.023154117 0.019057319][0.023679614 0.025285274 0.025415458 0.02561707 0.025592364 0.025531255 0.025599115 0.025702141 0.025746666 0.025720447 0.025627658 0.025258765 0.023804702 0.021584779 0.017824009][0.019606784 0.02100154 0.021097779 0.021564364 0.021919861 0.022264086 0.022650696 0.0231843 0.023522221 0.023454472 0.023357257 0.023023322 0.022071324 0.019887082 0.016121805][0.014419071 0.01564797 0.015533879 0.015795931 0.016045645 0.016311057 0.016614191 0.017193757 0.017586611 0.017494544 0.017408088 0.017285727 0.016847417 0.015074208 0.011739768]]...]
INFO - root - 2017-12-06 04:53:11.768418: step 6210, loss = 0.88, batch loss = 0.67 (35.1 examples/sec; 0.228 sec/batch; 20h:38m:02s remains)
INFO - root - 2017-12-06 04:53:14.034868: step 6220, loss = 0.79, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:28s remains)
INFO - root - 2017-12-06 04:53:16.329270: step 6230, loss = 0.92, batch loss = 0.71 (34.4 examples/sec; 0.233 sec/batch; 21h:04m:31s remains)
INFO - root - 2017-12-06 04:53:18.632926: step 6240, loss = 0.89, batch loss = 0.68 (35.4 examples/sec; 0.226 sec/batch; 20h:29m:55s remains)
INFO - root - 2017-12-06 04:53:20.919088: step 6250, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 21h:01m:32s remains)
INFO - root - 2017-12-06 04:53:23.223247: step 6260, loss = 0.89, batch loss = 0.68 (34.5 examples/sec; 0.232 sec/batch; 21h:00m:26s remains)
INFO - root - 2017-12-06 04:53:25.602118: step 6270, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.232 sec/batch; 21h:03m:16s remains)
INFO - root - 2017-12-06 04:53:27.901251: step 6280, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 20h:47m:18s remains)
INFO - root - 2017-12-06 04:53:30.154966: step 6290, loss = 0.80, batch loss = 0.59 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:06s remains)
INFO - root - 2017-12-06 04:53:32.488689: step 6300, loss = 0.92, batch loss = 0.71 (32.7 examples/sec; 0.245 sec/batch; 22h:10m:37s remains)
2017-12-06 04:53:32.870688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4813471 -1.4549528 -1.3621835 -1.3250424 -1.3325113 -1.3036722 -1.2447276 -1.1987599 -1.1940891 -1.2234861 -1.3410867 -1.5130584 -1.7423258 -1.8418666 -1.7725265][-1.0227035 -0.99828875 -0.85482997 -0.78584194 -0.78862625 -0.77581972 -0.75621706 -0.71550626 -0.673823 -0.71115905 -0.80352956 -0.97373784 -1.2130162 -1.3504006 -1.2710197][-0.66723806 -0.60810387 -0.44043005 -0.32583076 -0.26016206 -0.19315898 -0.17021742 -0.11988461 -0.0995864 -0.12005998 -0.16770743 -0.33792031 -0.52771765 -0.58338207 -0.48958224][-0.36451405 -0.26141596 -0.081336163 0.010285594 0.074004292 0.16661578 0.19802378 0.22675721 0.21035825 0.1635873 0.069216 -0.07064601 -0.22569725 -0.222135 -0.068824045][-0.16693088 0.041366905 0.21043913 0.29903138 0.34706748 0.40372497 0.3932516 0.37876022 0.31337488 0.21424185 0.092873424 -0.010101296 -0.13811678 -0.11208788 0.031310998][-0.054771878 0.22090842 0.39102536 0.50111461 0.49840212 0.48946929 0.45374346 0.41465473 0.31803447 0.22999595 0.12713031 0.030268237 -0.077293545 -0.10447597 -0.067974269][-0.0325917 0.31024945 0.508647 0.63260287 0.6280371 0.63485503 0.53353858 0.42400163 0.2834844 0.18352677 0.075863257 -0.061177671 -0.16520105 -0.21693774 -0.33870947][0.23362635 0.593183 0.77696735 0.89940709 0.87479734 0.76227504 0.64385647 0.53512663 0.3635866 0.24170016 0.09513019 -0.044252116 -0.25332144 -0.51318449 -0.63984978][0.43281937 0.72043294 0.86561757 0.94934219 0.88360381 0.76710683 0.64505649 0.48716766 0.41226161 0.38743865 0.29238653 0.078151718 -0.20566185 -0.53032809 -0.82134634][0.6022923 0.82961184 0.95509511 0.9721716 0.85755312 0.68719423 0.48189241 0.31689632 0.2393588 0.18922831 0.16327193 0.018412516 -0.24183701 -0.71260816 -1.0057837][0.39737171 0.57569718 0.65477341 0.62561172 0.52316558 0.34188664 0.17668669 0.068229705 0.033669807 0.036436334 0.072444007 -0.051905714 -0.30973923 -0.74695325 -1.1221755][0.13296326 0.25579435 0.2781648 0.23361228 0.1539814 0.050796717 -0.040472522 -0.079840131 -0.035466257 0.019332528 0.0949478 0.0655119 -0.1159322 -0.532583 -0.97343779][-0.011119179 0.054447502 0.098441124 0.0736554 0.019698597 -0.021281932 -0.069652252 -0.072881207 -0.0033260882 0.052908212 0.12927261 0.056785747 -0.15700451 -0.52509147 -1.0072502][-0.07121031 -0.029492166 0.0056050047 -0.038247507 -0.089399278 -0.11214694 -0.14190257 -0.14708534 -0.1000416 0.0077517182 0.10027878 0.09894146 -0.078551613 -0.37917691 -0.74547225][-0.09555836 -0.077413708 -0.070720017 -0.084229648 -0.089233249 -0.10552488 -0.10860074 -0.10630449 -0.087803692 -0.010423951 0.077709556 0.10219893 0.010520332 -0.23664829 -0.60050392]]...]
INFO - root - 2017-12-06 04:53:35.197132: step 6310, loss = 0.83, batch loss = 0.62 (34.3 examples/sec; 0.233 sec/batch; 21h:06m:36s remains)
INFO - root - 2017-12-06 04:53:37.495740: step 6320, loss = 0.95, batch loss = 0.74 (34.9 examples/sec; 0.229 sec/batch; 20h:46m:35s remains)
INFO - root - 2017-12-06 04:53:39.828680: step 6330, loss = 0.92, batch loss = 0.71 (33.6 examples/sec; 0.238 sec/batch; 21h:34m:00s remains)
INFO - root - 2017-12-06 04:53:42.120206: step 6340, loss = 0.95, batch loss = 0.74 (35.4 examples/sec; 0.226 sec/batch; 20h:27m:33s remains)
INFO - root - 2017-12-06 04:53:44.434142: step 6350, loss = 0.79, batch loss = 0.58 (35.0 examples/sec; 0.229 sec/batch; 20h:43m:24s remains)
INFO - root - 2017-12-06 04:53:46.708654: step 6360, loss = 0.97, batch loss = 0.76 (35.5 examples/sec; 0.226 sec/batch; 20h:26m:29s remains)
INFO - root - 2017-12-06 04:53:48.977167: step 6370, loss = 0.91, batch loss = 0.70 (34.7 examples/sec; 0.230 sec/batch; 20h:52m:21s remains)
INFO - root - 2017-12-06 04:53:51.339575: step 6380, loss = 0.93, batch loss = 0.72 (33.6 examples/sec; 0.238 sec/batch; 21h:32m:29s remains)
INFO - root - 2017-12-06 04:53:53.663599: step 6390, loss = 0.86, batch loss = 0.64 (34.5 examples/sec; 0.232 sec/batch; 20h:59m:30s remains)
INFO - root - 2017-12-06 04:53:55.951765: step 6400, loss = 0.90, batch loss = 0.69 (34.1 examples/sec; 0.235 sec/batch; 21h:16m:14s remains)
2017-12-06 04:53:57.694544: I tensorflow/core/kernels/logging_ops.cc:79] [[[2.0933752 1.6910368 1.3915707 1.0713573 0.6171186 0.14104983 -0.27077267 -0.79158229 -1.2018661 -1.6037558 -1.8634607 -2.0577555 -2.1863704 -2.2011068 -2.2000711][1.1586595 0.93592405 0.78561556 0.52615851 0.25000706 -0.096560113 -0.55554259 -1.0413448 -1.5544728 -1.9316274 -2.1570661 -2.2790754 -2.3742547 -2.3804438 -2.3920143][0.13719869 0.028696604 0.067448281 0.11701616 0.053526692 -0.15400311 -0.51275051 -0.82164782 -1.1521062 -1.4423249 -1.6283714 -1.7300476 -1.8867725 -2.0009694 -2.1554008][0.015363783 -0.057360344 -0.0090929791 0.1476703 0.2779313 0.33992004 0.20060545 -0.064555168 -0.43091044 -0.7944206 -1.0086375 -1.1771228 -1.372288 -1.6220024 -1.8832073][-0.44445217 -0.47878096 -0.36197564 -0.14674097 0.079864435 0.26191315 0.26814547 0.13679779 -0.13800515 -0.44611928 -0.67834651 -0.87068623 -1.0694183 -1.2238839 -1.4234554][-0.57128966 -0.50672334 -0.31652451 -0.037723348 0.28360477 0.56029111 0.67064542 0.63884431 0.407553 0.11852165 -0.12626384 -0.28828692 -0.40323403 -0.5316624 -0.72915035][-0.54910189 -0.49116918 -0.33543831 -0.029782034 0.3604416 0.78070259 1.0398808 1.1792231 1.1371795 0.98689115 0.78859019 0.63360178 0.56064123 0.48886967 0.31259629][-0.43264952 -0.40991881 -0.28596112 -0.046510719 0.26659873 0.61850631 0.92049611 1.2185309 1.3038449 1.2159945 1.1061271 0.97971475 0.95800722 0.96244931 0.82903957][-0.33374909 -0.31172287 -0.2335135 -0.050620291 0.20225045 0.46916348 0.69482809 0.89802611 1.0166183 1.0073888 0.95583677 0.90919369 0.89426464 0.8769719 0.726935][-0.246465 -0.252309 -0.23582163 -0.1507906 -0.0092694312 0.17671853 0.34134772 0.54357928 0.63264322 0.5490405 0.40633383 0.26680744 0.26459274 0.37113613 0.36346254][-0.19165203 -0.21924606 -0.24221557 -0.23398101 -0.2038731 -0.1667181 -0.13971338 -0.14529848 -0.20992425 -0.33812869 -0.47556576 -0.59888941 -0.64832175 -0.58132297 -0.47121298][-0.14523479 -0.18101379 -0.23500636 -0.27712673 -0.30106527 -0.30673453 -0.31577346 -0.32727221 -0.36338949 -0.49339631 -0.66140318 -0.79582369 -0.87337852 -0.85398227 -0.7723726][-0.12752868 -0.14389822 -0.18241987 -0.22829828 -0.27382192 -0.31295022 -0.35125643 -0.37719193 -0.43919855 -0.5311895 -0.62680268 -0.7211175 -0.78098214 -0.76678646 -0.70636493][-0.12522471 -0.13506252 -0.16050014 -0.18473482 -0.21406916 -0.24021098 -0.26042092 -0.26570973 -0.2767103 -0.29648644 -0.34054095 -0.3855049 -0.42632204 -0.43390766 -0.42182952][-0.11819379 -0.13066351 -0.15313399 -0.16245359 -0.17615175 -0.18876255 -0.19745716 -0.19422069 -0.19483462 -0.19850966 -0.21954235 -0.23998091 -0.26429278 -0.27145013 -0.26728684]]...]
INFO - root - 2017-12-06 04:53:59.960660: step 6410, loss = 0.79, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 20h:15m:26s remains)
INFO - root - 2017-12-06 04:54:02.270568: step 6420, loss = 0.93, batch loss = 0.72 (35.4 examples/sec; 0.226 sec/batch; 20h:27m:40s remains)
INFO - root - 2017-12-06 04:54:04.602286: step 6430, loss = 0.81, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:57m:15s remains)
INFO - root - 2017-12-06 04:54:06.913885: step 6440, loss = 0.94, batch loss = 0.73 (34.9 examples/sec; 0.230 sec/batch; 20h:47m:14s remains)
INFO - root - 2017-12-06 04:54:09.190816: step 6450, loss = 0.90, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:22m:54s remains)
INFO - root - 2017-12-06 04:54:11.465815: step 6460, loss = 0.88, batch loss = 0.67 (34.2 examples/sec; 0.234 sec/batch; 21h:10m:46s remains)
INFO - root - 2017-12-06 04:54:13.761529: step 6470, loss = 0.82, batch loss = 0.61 (33.9 examples/sec; 0.236 sec/batch; 21h:20m:49s remains)
INFO - root - 2017-12-06 04:54:16.051941: step 6480, loss = 0.89, batch loss = 0.68 (34.7 examples/sec; 0.230 sec/batch; 20h:52m:13s remains)
INFO - root - 2017-12-06 04:54:18.367825: step 6490, loss = 0.94, batch loss = 0.72 (34.7 examples/sec; 0.231 sec/batch; 20h:52m:48s remains)
INFO - root - 2017-12-06 04:54:20.657633: step 6500, loss = 0.85, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 20h:24m:12s remains)
2017-12-06 04:54:20.989612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091059782 -0.096301451 -0.12766352 -0.19420132 -0.28537288 -0.36796674 -0.42375892 -0.450792 -0.46191549 -0.46216547 -0.46647733 -0.4529759 -0.45388144 -0.46727908 -0.44799191][-0.095639847 -0.11024519 -0.16013399 -0.23915303 -0.32162935 -0.37663925 -0.4068622 -0.42465162 -0.46220443 -0.51683152 -0.57831407 -0.58898038 -0.60079277 -0.60178947 -0.55891168][-0.10399172 -0.13514896 -0.20916623 -0.30517676 -0.38453895 -0.4085848 -0.399074 -0.37856984 -0.39580867 -0.46097165 -0.56305778 -0.6075443 -0.66820544 -0.68511003 -0.63628936][-0.11285079 -0.16373852 -0.26773983 -0.3802073 -0.4534348 -0.43062612 -0.34046468 -0.22748911 -0.17040682 -0.20080025 -0.31537932 -0.41961578 -0.59974074 -0.6970318 -0.68387318][-0.12287751 -0.19384593 -0.31762147 -0.42819214 -0.47868142 -0.38834077 -0.21349105 -0.0054901093 0.10237642 0.067633204 -0.073374227 -0.24002811 -0.49137843 -0.63170636 -0.65215093][-0.13728973 -0.21984079 -0.35318848 -0.4465256 -0.45096517 -0.2879079 -0.070512161 0.16829345 0.28357145 0.26577121 0.14712667 -0.041294184 -0.35146821 -0.55604488 -0.63053614][-0.13607992 -0.21803135 -0.3464258 -0.43308339 -0.41184995 -0.19731463 0.069523431 0.36027989 0.48213673 0.46989161 0.36462507 0.16652054 -0.15009025 -0.39259475 -0.5334897][-0.12887749 -0.19448236 -0.29939488 -0.37262189 -0.33700848 -0.11610676 0.14952108 0.4409408 0.54926527 0.563986 0.50708294 0.31469008 0.026191689 -0.22161254 -0.38839319][-0.11875692 -0.16181001 -0.2220341 -0.24460545 -0.16000482 0.073552452 0.36633962 0.64455527 0.67020148 0.62021106 0.51893449 0.36540157 0.13707197 -0.10011038 -0.27172545][-0.11132711 -0.1442847 -0.18848097 -0.18166691 -0.062563434 0.18859681 0.51392728 0.79748774 0.84903365 0.76414651 0.5763371 0.36939687 0.15655395 -0.016165625 -0.16946858][-0.10576265 -0.13921839 -0.19774157 -0.21919873 -0.13378802 0.096925907 0.40843716 0.67964453 0.76499134 0.6854558 0.510138 0.29925808 0.098224945 -0.035711691 -0.14626241][-0.10949971 -0.14013098 -0.19153602 -0.23492011 -0.20083253 -0.039754286 0.18594617 0.4012402 0.44135517 0.36501497 0.22884795 0.089741074 -0.026953589 -0.090867117 -0.13562955][-0.10936084 -0.13158228 -0.16525742 -0.19962925 -0.18765187 -0.099462919 0.021635704 0.1447525 0.14338344 0.042030126 -0.0870992 -0.16361611 -0.18092927 -0.16493109 -0.15305808][-0.10259452 -0.12401709 -0.16135865 -0.20208031 -0.22027156 -0.18986885 -0.13098636 -0.068353981 -0.063007489 -0.12049936 -0.19430524 -0.2264773 -0.23094058 -0.19528356 -0.16002369][-0.0955006 -0.1063781 -0.13386899 -0.1663525 -0.19067043 -0.20427132 -0.1998813 -0.18807504 -0.18575726 -0.21204737 -0.24401614 -0.25383678 -0.23343658 -0.1850217 -0.14862803]]...]
INFO - root - 2017-12-06 04:54:23.270921: step 6510, loss = 0.82, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 21h:06m:40s remains)
INFO - root - 2017-12-06 04:54:25.575626: step 6520, loss = 0.96, batch loss = 0.75 (33.7 examples/sec; 0.237 sec/batch; 21h:30m:16s remains)
INFO - root - 2017-12-06 04:54:27.866019: step 6530, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.233 sec/batch; 21h:04m:31s remains)
INFO - root - 2017-12-06 04:54:30.158891: step 6540, loss = 0.78, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 20h:34m:35s remains)
INFO - root - 2017-12-06 04:54:32.454928: step 6550, loss = 0.98, batch loss = 0.76 (36.4 examples/sec; 0.220 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-06 04:54:34.767883: step 6560, loss = 0.95, batch loss = 0.73 (35.2 examples/sec; 0.227 sec/batch; 20h:33m:00s remains)
INFO - root - 2017-12-06 04:54:37.078215: step 6570, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 20h:58m:35s remains)
INFO - root - 2017-12-06 04:54:39.366874: step 6580, loss = 0.85, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:19m:33s remains)
INFO - root - 2017-12-06 04:54:41.617639: step 6590, loss = 0.88, batch loss = 0.66 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:45s remains)
INFO - root - 2017-12-06 04:54:43.949039: step 6600, loss = 0.86, batch loss = 0.64 (34.6 examples/sec; 0.231 sec/batch; 20h:57m:17s remains)
2017-12-06 04:54:44.322739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8019531 -2.1624961 -2.3992884 -2.594106 -2.8097088 -2.9051988 -2.8871288 -2.8373392 -2.6497037 -2.3002954 -1.9780747 -1.6869816 -1.2461947 -1.0588574 -0.84114522][-1.5147687 -1.7907302 -1.9920465 -2.2232053 -2.3669887 -2.5408738 -2.5671146 -2.4818182 -2.2850268 -2.0455332 -1.8600373 -1.4930314 -1.0457541 -0.82651204 -0.67599005][-1.1538641 -1.3661915 -1.6064619 -1.77386 -2.0427961 -2.10581 -2.1495345 -2.1781278 -2.1881356 -2.0869777 -1.8016013 -1.4689755 -1.0037842 -0.83670563 -0.69851422][-0.94876409 -1.1505278 -1.2306781 -1.4082148 -1.7051944 -1.6761535 -1.7007079 -1.7897788 -1.8946161 -1.953351 -1.7974895 -1.6417663 -1.3438127 -1.171196 -1.0414044][-0.85584885 -1.0649586 -1.3542303 -1.4480566 -1.5165793 -1.3863659 -1.2893397 -1.3631996 -1.6730908 -1.7857653 -1.8569989 -1.7194493 -1.464612 -1.3949594 -1.427158][-0.83328068 -1.1615484 -1.5381063 -1.6793829 -1.4251726 -1.1922272 -1.0670553 -1.0239842 -1.0854417 -1.135831 -1.4319992 -1.3675574 -1.3690637 -1.1463119 -1.3733648][-0.57824 -1.0634294 -1.3993115 -1.4828941 -1.2405568 -0.85311121 -0.81869453 -0.95588446 -1.1012619 -1.0895709 -1.2248051 -1.1815261 -1.3348563 -1.2371335 -1.4647744][-0.31784993 -0.77459604 -1.1951088 -1.1692375 -0.987213 -0.71100831 -0.81030482 -0.97780776 -1.1769398 -1.3312793 -1.4619588 -1.4306406 -1.4527857 -1.4764962 -1.7440748][-0.73732716 -0.88678527 -1.1526635 -1.3069713 -1.190104 -0.98877913 -1.0955659 -1.1940621 -1.38759 -1.4455605 -1.6376908 -1.7487392 -1.5589461 -1.5296 -1.6917346][-1.0740223 -1.4758756 -1.6825368 -1.832534 -1.8049054 -1.5884552 -1.574558 -1.5562155 -1.4985348 -1.4063735 -1.4433483 -1.5044562 -1.4467515 -1.4139777 -1.5946829][-1.4096537 -1.9916641 -2.4202628 -2.4238713 -2.2338572 -2.0435128 -1.9824585 -1.8763214 -1.7647392 -1.5505118 -1.3399494 -1.0837944 -0.76433837 -0.77094734 -1.1020917][-1.3697511 -2.0751266 -2.7865505 -2.8219452 -2.7740338 -2.50167 -2.3515041 -2.1961224 -2.0668609 -1.8345429 -1.6060913 -1.2525138 -0.79077387 -0.46422648 -0.4639416][-0.81218481 -1.6033529 -2.3517869 -2.6815331 -2.783298 -2.4214737 -2.3276238 -2.2423499 -2.1711535 -1.9913195 -1.7808249 -1.3729677 -0.96576607 -0.81480128 -0.76045161][-0.37487945 -1.021975 -1.6710799 -2.1529906 -2.3017659 -2.152719 -2.1955514 -2.0984628 -1.9538705 -1.8431062 -1.6742328 -1.4253722 -1.2613431 -1.1084076 -1.1984414][0.35375121 -0.46741441 -1.2591412 -1.4482782 -1.3975879 -1.4360406 -1.6430331 -1.5680363 -1.4333127 -1.3440249 -1.2193197 -1.0663168 -1.0976596 -1.3355322 -1.6830685]]...]
INFO - root - 2017-12-06 04:54:46.568645: step 6610, loss = 0.96, batch loss = 0.75 (35.2 examples/sec; 0.227 sec/batch; 20h:32m:43s remains)
INFO - root - 2017-12-06 04:54:48.875647: step 6620, loss = 0.93, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 20h:44m:56s remains)
INFO - root - 2017-12-06 04:54:51.169639: step 6630, loss = 0.90, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:23s remains)
INFO - root - 2017-12-06 04:54:53.464040: step 6640, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.231 sec/batch; 20h:51m:51s remains)
INFO - root - 2017-12-06 04:54:55.786842: step 6650, loss = 0.83, batch loss = 0.62 (33.4 examples/sec; 0.240 sec/batch; 21h:42m:16s remains)
INFO - root - 2017-12-06 04:54:58.093388: step 6660, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:46m:53s remains)
INFO - root - 2017-12-06 04:55:00.394184: step 6670, loss = 0.90, batch loss = 0.69 (33.7 examples/sec; 0.237 sec/batch; 21h:28m:25s remains)
INFO - root - 2017-12-06 04:55:02.685869: step 6680, loss = 0.87, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:46s remains)
INFO - root - 2017-12-06 04:55:04.980968: step 6690, loss = 0.85, batch loss = 0.64 (33.4 examples/sec; 0.239 sec/batch; 21h:39m:38s remains)
INFO - root - 2017-12-06 04:55:07.294204: step 6700, loss = 0.89, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:30m:40s remains)
2017-12-06 04:55:08.836709: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.83975321 0.73152936 0.53306127 0.32993796 0.33652174 0.53770822 0.74713 1.1531562 1.0569315 1.4723941 2.1263969 2.3514843 2.1601048 1.8132558 1.5433878][1.4728106 1.4808971 1.2680045 0.8123529 0.46044624 0.32768512 0.38312131 0.44730002 0.60279536 0.83685082 1.0062366 0.84067988 0.61391544 0.6671145 0.57743859][1.2868314 1.321345 1.3833929 1.1018971 0.6555174 0.21847755 -0.033889227 -0.10473446 0.070014842 0.32504851 0.40858105 0.56601161 0.57275754 0.17112151 -0.42964578][1.3918818 1.1550295 0.92084318 0.91297847 1.0072685 0.72445017 0.42910457 0.19349837 0.068490319 0.27360022 0.39455467 0.41201526 0.27397144 0.075153239 -0.41021925][1.8618058 1.815168 1.5826368 1.161244 0.91882318 0.89522934 0.69968539 0.70606506 1.0154139 1.1210012 1.3185477 1.2787995 1.2485839 0.53799331 -0.44443378][1.8209321 1.8380412 1.7351693 1.5371122 1.1989424 0.96888715 0.87693685 0.64193285 0.714388 1.074546 1.5396686 1.9175719 1.8733577 1.6965172 0.89837617][1.9789478 1.8495408 1.9262136 2.0588531 1.9254025 1.7228298 1.5735048 1.4437996 1.1638694 1.2137045 1.5102932 1.2967665 1.6226524 1.8105137 0.77684385][2.0729516 2.0726953 1.9647192 2.2117963 2.6080289 2.8482833 2.9508131 2.8456841 2.5359545 2.1542928 2.1079247 2.2925093 2.3689418 2.0380189 1.2263896][1.8626323 1.960528 1.8430791 1.6024584 1.6627192 2.0970552 2.5344357 3.173367 3.7090557 3.8596661 3.6362355 3.2786546 2.8894928 2.5686977 2.4414589][1.6562036 1.8023614 1.7943856 1.6473316 1.2821124 1.3945758 1.6649185 1.8424971 2.1927929 2.7955382 3.4489512 3.6985004 3.3966546 2.6012769 1.8567667][1.1722388 1.1598762 1.1817076 1.170836 1.0724378 0.85273689 0.6522699 1.2565846 1.7057467 1.6827149 1.7655587 1.9437102 1.9707967 1.794551 1.5467134][1.2996161 1.3536553 1.2368218 1.0802654 1.1574583 1.077226 0.91583741 0.98037654 1.3101213 1.5561295 1.434158 1.3097125 0.75227892 -0.20307073 -0.63851649][1.79571 1.5291158 1.4951102 1.4903806 1.3720813 1.0456332 0.70853615 0.77525008 0.62188482 1.1614788 2.140671 1.8275335 0.91905123 -0.563664 -1.8945707][1.405697 1.4821044 1.2445911 1.1625288 1.0953193 0.99475688 1.1188444 0.66846049 0.50474447 0.30883753 -0.017847493 0.44686079 0.9241665 0.295914 -0.99975371][0.792971 0.9249478 1.1310117 1.1478525 1.2078105 1.0254732 0.74527293 0.40722474 0.28471971 0.18504137 -0.051090419 -0.328857 -0.38579026 -0.34699908 -0.17736399]]...]
INFO - root - 2017-12-06 04:55:11.108282: step 6710, loss = 0.96, batch loss = 0.74 (32.9 examples/sec; 0.243 sec/batch; 22h:01m:23s remains)
INFO - root - 2017-12-06 04:55:13.407065: step 6720, loss = 0.95, batch loss = 0.74 (35.6 examples/sec; 0.225 sec/batch; 20h:20m:21s remains)
INFO - root - 2017-12-06 04:55:15.662685: step 6730, loss = 0.94, batch loss = 0.73 (34.9 examples/sec; 0.229 sec/batch; 20h:44m:08s remains)
INFO - root - 2017-12-06 04:55:17.974737: step 6740, loss = 0.91, batch loss = 0.70 (32.4 examples/sec; 0.247 sec/batch; 22h:19m:09s remains)
INFO - root - 2017-12-06 04:55:20.245791: step 6750, loss = 0.86, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 20h:16m:45s remains)
INFO - root - 2017-12-06 04:55:22.569642: step 6760, loss = 0.88, batch loss = 0.67 (31.1 examples/sec; 0.258 sec/batch; 23h:18m:23s remains)
INFO - root - 2017-12-06 04:55:24.921796: step 6770, loss = 0.84, batch loss = 0.62 (36.1 examples/sec; 0.222 sec/batch; 20h:03m:36s remains)
INFO - root - 2017-12-06 04:55:27.224348: step 6780, loss = 0.81, batch loss = 0.60 (35.4 examples/sec; 0.226 sec/batch; 20h:28m:31s remains)
INFO - root - 2017-12-06 04:55:29.523643: step 6790, loss = 0.86, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:42m:12s remains)
INFO - root - 2017-12-06 04:55:31.828140: step 6800, loss = 0.80, batch loss = 0.59 (33.8 examples/sec; 0.236 sec/batch; 21h:23m:05s remains)
2017-12-06 04:55:34.156299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0010724366 -0.0081925988 -0.020137951 -0.031650081 -0.041673303 -0.048607398 -0.052125175 -0.05341161 -0.053437293 -0.052668452 -0.051444627 -0.05023938 -0.049383327 -0.049297657 -0.050211258][0.000547871 -0.0089140758 -0.02110514 -0.032764051 -0.042617053 -0.049176767 -0.052556086 -0.053973012 -0.054572217 -0.054611191 -0.054233536 -0.053612031 -0.053158291 -0.053203128 -0.053892978][0.0017263815 -0.00674095 -0.018192731 -0.029233854 -0.038373906 -0.044358321 -0.047664493 -0.04940971 -0.050671209 -0.051561959 -0.051937163 -0.051859356 -0.051689915 -0.051815823 -0.052437477][0.0044803694 -0.0023927987 -0.01227551 -0.021921311 -0.029863223 -0.0351744 -0.038492352 -0.040771376 -0.042930115 -0.044660367 -0.045743249 -0.046003029 -0.045708474 -0.045580097 -0.045947585][0.0092020631 0.0044127405 -0.0030694827 -0.010770097 -0.01745753 -0.02226216 -0.025859471 -0.028903242 -0.032076139 -0.034723479 -0.036309794 -0.036545042 -0.035592686 -0.03486279 -0.034729876][0.014493555 0.011921182 0.0069863647 0.0015194267 -0.0038635507 -0.0082789212 -0.012332499 -0.016360935 -0.020610411 -0.023856495 -0.025423054 -0.024940096 -0.023123346 -0.021578312 -0.02102473][0.019547567 0.018904597 0.015874639 0.012056336 0.007480368 0.0032649264 -0.00095689297 -0.0054480061 -0.010336332 -0.013783813 -0.014970485 -0.013623491 -0.010659084 -0.0081216693 -0.0068011731][0.022634245 0.023420863 0.021331072 0.018117726 0.013675757 0.0091258958 0.0046217293 0.0003458932 -0.0043436289 -0.0074222088 -0.0077587664 -0.0053818524 -0.0010980889 0.0023790747 0.0043571144][0.023509756 0.024478324 0.022337064 0.018934317 0.014266223 0.0092938319 0.0046388283 0.00045654178 -0.0029769838 -0.0045903474 -0.0035429 -0.00017771125 0.004616186 0.0085544065 0.010661006][0.022549704 0.023259051 0.020594485 0.016799979 0.011994988 0.0070890784 0.0029124469 -0.00041854382 -0.0023013502 -0.002056703 0.00027220696 0.004200682 0.0086374953 0.01203423 0.013167515][0.021325037 0.021492541 0.018290117 0.014183268 0.0094165877 0.0050204843 0.0018382519 1.6592443e-05 -0.00027804077 0.0016212612 0.0050779432 0.0094023123 0.01312986 0.014763236 0.013784595][0.020219661 0.019776218 0.015971929 0.011322379 0.0064648613 0.0025030375 0.00042442977 0.00017005205 0.0019778311 0.0058767423 0.010622382 0.015102237 0.01739417 0.016353227 0.012673154][0.019064337 0.018055737 0.013389975 0.0080106482 0.002779074 -0.00097545981 -0.0022025108 -0.00080180168 0.0035201386 0.0096386448 0.015548758 0.019899085 0.02034647 0.016438819 0.0099597722][0.017116137 0.015215315 0.0095818341 0.0030838698 -0.0024103075 -0.0057471395 -0.0059514195 -0.0029544458 0.0033459514 0.011316158 0.01810573 0.021809742 0.020492852 0.014427982 0.0058251545][0.0120297 0.00959748 0.0033951625 -0.0037392378 -0.009784393 -0.012999266 -0.012536511 -0.0082255825 -0.00050254166 0.0086086765 0.016042471 0.019080289 0.016712725 0.0096426457 0.00053821504]]...]
INFO - root - 2017-12-06 04:55:36.457686: step 6810, loss = 0.83, batch loss = 0.61 (35.7 examples/sec; 0.224 sec/batch; 20h:17m:44s remains)
INFO - root - 2017-12-06 04:55:38.806101: step 6820, loss = 0.94, batch loss = 0.72 (33.7 examples/sec; 0.237 sec/batch; 21h:28m:59s remains)
INFO - root - 2017-12-06 04:55:41.141270: step 6830, loss = 0.89, batch loss = 0.68 (34.4 examples/sec; 0.233 sec/batch; 21h:03m:20s remains)
INFO - root - 2017-12-06 04:55:43.413486: step 6840, loss = 0.92, batch loss = 0.71 (34.8 examples/sec; 0.230 sec/batch; 20h:49m:03s remains)
INFO - root - 2017-12-06 04:55:45.734655: step 6850, loss = 0.90, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:34m:04s remains)
INFO - root - 2017-12-06 04:55:48.034525: step 6860, loss = 0.88, batch loss = 0.67 (33.6 examples/sec; 0.238 sec/batch; 21h:31m:28s remains)
INFO - root - 2017-12-06 04:55:50.325534: step 6870, loss = 0.86, batch loss = 0.65 (33.8 examples/sec; 0.236 sec/batch; 21h:23m:03s remains)
INFO - root - 2017-12-06 04:55:52.650381: step 6880, loss = 0.88, batch loss = 0.67 (35.5 examples/sec; 0.226 sec/batch; 20h:24m:24s remains)
INFO - root - 2017-12-06 04:55:54.981832: step 6890, loss = 0.87, batch loss = 0.66 (35.3 examples/sec; 0.227 sec/batch; 20h:31m:16s remains)
INFO - root - 2017-12-06 04:55:57.296500: step 6900, loss = 0.90, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:17m:41s remains)
2017-12-06 04:55:57.769524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.095355146 -0.077505514 -0.059179507 -0.051092349 -0.028822184 -0.030593574 -0.032353394 -0.021139558 -0.016115684 -0.017186224 -0.02056611 -0.02693389 -0.019783121 -0.0098140016 -0.0084010735][-0.11367418 -0.093223743 -0.074877195 -0.054222915 -0.023556337 7.4140728e-05 0.025506034 0.027160548 0.016356513 0.0043336824 -0.012928188 -0.028946687 -0.045902744 -0.05757051 -0.0550657][-0.13591501 -0.11487369 -0.091787726 -0.053413853 -0.0057431683 0.00070420653 0.014820188 0.027402431 0.031433731 0.018724144 -0.00048094988 -0.024957322 -0.04379769 -0.049911134 -0.055606931][-0.15507481 -0.10803661 -0.072627835 -0.038553692 -0.01334925 0.0030154213 0.020591736 0.017477304 0.014238991 -0.0021235049 -0.019902367 -0.038559947 -0.046723317 -0.053751789 -0.054801237][-0.17089978 -0.12039778 -0.10505976 -0.071210191 -0.040970776 -0.026941665 -0.022444248 -0.01867694 -0.01976214 -0.036368996 -0.046986111 -0.05707797 -0.062373787 -0.062893853 -0.0589502][-0.17576706 -0.11260797 -0.0808465 -0.064505756 -0.061137564 -0.042130951 -0.031599488 -0.033680722 -0.045074552 -0.054565709 -0.061881222 -0.064370856 -0.071641237 -0.06921646 -0.0665034][-0.12319449 -0.060508408 -0.020386644 -0.0034763739 -0.00612887 -0.0156475 -0.030921876 -0.025661521 -0.032131925 -0.045720339 -0.054100156 -0.057575889 -0.068425864 -0.067117371 -0.068094455][-0.099957816 -0.04132599 -0.0037832707 0.017612129 0.023745641 0.01621554 -0.0064180642 -0.025079109 -0.04437333 -0.049148992 -0.055352211 -0.055969097 -0.060973652 -0.061704859 -0.067085579][-0.093787804 -0.041424394 -0.016973019 -0.00412146 0.0080516562 0.0090795979 -0.0086797923 -0.019790392 -0.020588644 -0.041331217 -0.055003494 -0.0583532 -0.057644922 -0.06726364 -0.070569389][-0.12677136 -0.067675248 -0.040944282 -0.015235133 0.00955566 0.022069357 -0.001697138 -0.022907209 -0.021057457 -0.02770019 -0.052692968 -0.052464694 -0.0535503 -0.067572527 -0.0697857][-0.12268684 -0.067137711 -0.04497534 -0.029902764 -0.015749801 -0.010781318 -0.028410912 -0.034556866 -0.036708523 -0.040428549 -0.063237607 -0.0645926 -0.066324845 -0.066282883 -0.074522093][-0.12415749 -0.074422583 -0.040044703 -0.026314206 -0.013429418 -0.017858371 -0.01559636 -0.019398004 -0.04823371 -0.052478194 -0.0435215 -0.042514522 -0.045708034 -0.051655844 -0.069126278][-0.14854452 -0.087729238 -0.03890584 -0.01601477 -0.0079491809 -0.016876847 -0.019052736 -0.000804089 -0.00059952587 -0.0072152317 -0.023115836 -0.028860047 -0.043277167 -0.055959906 -0.07027176][-0.15763275 -0.087490626 -0.03244308 -0.017401509 -0.0084885359 -0.0085475668 -0.024779018 -0.020831611 -0.013639398 -0.00901787 -0.02815846 -0.041701179 -0.067761481 -0.0684489 -0.062573552][-0.12840293 -0.068292461 -0.023674704 -0.0050868168 -0.0053600296 -0.0096811429 -0.025300387 -0.040224884 -0.018452764 -0.015265912 -0.010577895 -0.027540386 -0.025630958 -0.033708438 -0.045802582]]...]
INFO - root - 2017-12-06 04:56:00.049434: step 6910, loss = 0.88, batch loss = 0.66 (35.4 examples/sec; 0.226 sec/batch; 20h:26m:17s remains)
INFO - root - 2017-12-06 04:56:02.374286: step 6920, loss = 0.81, batch loss = 0.59 (35.2 examples/sec; 0.227 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-06 04:56:04.694289: step 6930, loss = 0.86, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:27s remains)
INFO - root - 2017-12-06 04:56:07.008421: step 6940, loss = 0.95, batch loss = 0.74 (35.6 examples/sec; 0.224 sec/batch; 20h:18m:04s remains)
INFO - root - 2017-12-06 04:56:09.299387: step 6950, loss = 0.87, batch loss = 0.66 (33.1 examples/sec; 0.242 sec/batch; 21h:52m:32s remains)
INFO - root - 2017-12-06 04:56:11.640440: step 6960, loss = 0.88, batch loss = 0.67 (35.0 examples/sec; 0.229 sec/batch; 20h:40m:35s remains)
INFO - root - 2017-12-06 04:56:13.946405: step 6970, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 20h:20m:55s remains)
INFO - root - 2017-12-06 04:56:16.257624: step 6980, loss = 0.91, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 20h:51m:56s remains)
INFO - root - 2017-12-06 04:56:18.597561: step 6990, loss = 0.76, batch loss = 0.55 (34.1 examples/sec; 0.235 sec/batch; 21h:12m:45s remains)
INFO - root - 2017-12-06 04:56:20.919936: step 7000, loss = 0.95, batch loss = 0.74 (34.2 examples/sec; 0.234 sec/batch; 21h:09m:53s remains)
2017-12-06 04:56:21.299633: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03463544 0.035611995 0.035403714 0.034436487 0.032822452 0.030503325 0.028683007 0.028416716 0.029796489 0.031960197 0.033590965 0.034510769 0.034480222 0.034147866 0.032803312][0.041709162 0.042444594 0.042056516 0.040285103 0.036955528 0.032691695 0.029383145 0.027933039 0.028432228 0.030418634 0.032538302 0.035154745 0.037006147 0.038021319 0.036939435][0.045837447 0.04721763 0.04657121 0.044267215 0.040126063 0.0348775 0.030075282 0.027032845 0.025875971 0.026711896 0.029199779 0.032888882 0.035771683 0.037706211 0.036891147][0.048092641 0.050383948 0.050224937 0.04797662 0.04411836 0.038621493 0.032584526 0.027877592 0.024924263 0.024328254 0.025983699 0.028953351 0.031440273 0.033523977 0.03311985][0.048227869 0.051732682 0.052841611 0.051257543 0.04801213 0.042738244 0.0361609 0.029962398 0.024911799 0.022488371 0.022158973 0.023392282 0.02479142 0.026673883 0.026938722][0.046997987 0.052148007 0.054045491 0.053177364 0.050827853 0.046397172 0.0402418 0.03339123 0.026727624 0.021843925 0.018783569 0.017641872 0.017436959 0.01879812 0.019583337][0.046172649 0.052335449 0.054547913 0.054255642 0.052802809 0.049684502 0.044997424 0.038806491 0.031003967 0.023550712 0.017931007 0.014350429 0.012299754 0.012211435 0.012023322][0.046025619 0.052923 0.055429272 0.055655815 0.055031367 0.05336792 0.050032832 0.0441413 0.0352235 0.026254237 0.019052379 0.01340156 0.009109877 0.0067430213 0.0050460771][0.046922542 0.054020144 0.057131372 0.057942741 0.05791264 0.057015352 0.053874724 0.047387056 0.037900664 0.029162072 0.021898888 0.015214115 0.0093271062 0.0049789175 0.0011253729][0.047181241 0.054951213 0.058954664 0.060557775 0.06085559 0.059522532 0.055478178 0.048454978 0.039931029 0.032270022 0.025342062 0.018025987 0.010726757 0.0046020597 -0.0006590113][0.046491392 0.055082493 0.059568964 0.061753981 0.062214591 0.060403474 0.055638798 0.048750453 0.041812144 0.035234548 0.028197445 0.020299375 0.012252785 0.0053671449 -0.00036703795][0.045419335 0.054068483 0.059169181 0.06218072 0.062787823 0.060685091 0.055540703 0.048859514 0.042719126 0.036637343 0.029598489 0.021797203 0.013948709 0.0070287138 0.0011805221][0.044447429 0.052817769 0.058542915 0.062285729 0.062755875 0.060088761 0.05479721 0.048592038 0.042842321 0.036905847 0.030197427 0.022892728 0.015453108 0.0085128248 0.0028121769][0.041334614 0.049235396 0.055009641 0.058808573 0.059103109 0.056304567 0.051240735 0.04578907 0.040257089 0.034585275 0.028748147 0.022747912 0.016705818 0.010775074 0.0060468391][0.033602506 0.040403463 0.044927672 0.048165731 0.048646502 0.046559624 0.042529367 0.038445704 0.033795826 0.028727934 0.023714304 0.018978313 0.014262043 0.010078751 0.0069542527]]...]
INFO - root - 2017-12-06 04:56:23.613389: step 7010, loss = 0.87, batch loss = 0.66 (36.4 examples/sec; 0.220 sec/batch; 19h:51m:30s remains)
INFO - root - 2017-12-06 04:56:25.925394: step 7020, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 20h:39m:27s remains)
INFO - root - 2017-12-06 04:56:28.223154: step 7030, loss = 0.89, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 20h:38m:04s remains)
INFO - root - 2017-12-06 04:56:30.541347: step 7040, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 20h:42m:53s remains)
INFO - root - 2017-12-06 04:56:32.886428: step 7050, loss = 0.86, batch loss = 0.65 (34.1 examples/sec; 0.234 sec/batch; 21h:11m:38s remains)
INFO - root - 2017-12-06 04:56:35.268383: step 7060, loss = 0.97, batch loss = 0.75 (32.4 examples/sec; 0.247 sec/batch; 22h:19m:21s remains)
INFO - root - 2017-12-06 04:56:37.563951: step 7070, loss = 0.92, batch loss = 0.71 (35.8 examples/sec; 0.223 sec/batch; 20h:10m:26s remains)
INFO - root - 2017-12-06 04:56:39.910926: step 7080, loss = 0.96, batch loss = 0.75 (34.8 examples/sec; 0.230 sec/batch; 20h:48m:13s remains)
INFO - root - 2017-12-06 04:56:42.225115: step 7090, loss = 0.89, batch loss = 0.67 (32.7 examples/sec; 0.245 sec/batch; 22h:07m:41s remains)
INFO - root - 2017-12-06 04:56:44.569657: step 7100, loss = 0.83, batch loss = 0.62 (33.3 examples/sec; 0.240 sec/batch; 21h:42m:24s remains)
2017-12-06 04:56:45.016119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.73803461 -0.81031758 -0.6647858 -0.72901505 -0.66393274 -0.826487 -0.96652937 -1.1400493 -1.4521375 -1.4669852 -1.4479891 -1.5018119 -1.5646663 -1.49092 -1.2575299][-0.64471453 -0.78212637 -0.96253121 -0.91263396 -0.71908343 -0.68910873 -0.67986906 -0.8592146 -1.1402527 -1.1799074 -1.3306307 -1.363296 -1.3593611 -1.3758581 -1.347656][-0.970291 -0.90761834 -0.82940245 -0.780299 -0.64038521 -0.61618316 -0.62558234 -0.76285869 -1.0854164 -1.1796732 -1.4678992 -1.4991554 -1.5572227 -1.5830625 -1.3712456][-0.66257691 -0.66241616 -0.59441137 -0.57908589 -0.30259573 -0.49700421 -0.68657714 -0.835332 -0.999659 -1.0502651 -1.3190924 -1.4665389 -1.5531672 -1.7336161 -1.6926798][-0.43493348 -0.59500921 -0.6051383 -0.72121572 -0.5583272 -0.64686567 -0.7256192 -0.84810454 -0.94175977 -1.0077441 -1.2865855 -1.6974112 -1.8663429 -2.1280577 -1.8865767][-0.42784524 -0.62984884 -0.60845244 -0.58855414 -0.51122957 -0.55603373 -0.80010039 -1.1511344 -1.2683775 -1.3107896 -1.4991416 -1.6563983 -1.8940153 -1.9609793 -1.9279819][-0.33056706 -0.53137583 -0.57343012 -0.64840895 -0.60540897 -0.53818041 -0.78384119 -1.1449842 -1.3269962 -1.5072817 -1.7477083 -2.020432 -2.2774549 -2.1360345 -1.9895188][-0.19133535 -0.54368627 -0.72496551 -0.56261706 -0.43725032 -0.36364102 -0.51789486 -0.929726 -1.3040962 -1.6241754 -1.8518292 -2.0455363 -2.2629027 -2.1312449 -1.9666053][0.11552987 -0.12045541 -0.23344997 -0.26809412 -0.27289337 -0.28281704 -0.38898718 -0.62803704 -0.84987628 -1.2764472 -1.499234 -1.5098622 -1.4932617 -1.3460031 -1.4501103][0.022272989 -0.1416817 -0.15365918 -0.16206092 0.044734262 0.013500035 -0.11475049 -0.31121373 -0.57957363 -0.79031789 -0.87579381 -1.006722 -1.1650283 -0.95771378 -0.977881][-0.69419676 -0.96627581 -1.0469722 -1.0908719 -0.92412138 -0.67702 -0.59538543 -0.60859925 -0.63306987 -0.39149338 -0.25634342 -0.14987275 -0.20133948 -0.21953814 -0.39074332][-0.44175333 -0.82741308 -1.0876014 -0.8640402 -0.71338886 -0.69669843 -0.71942228 -0.65417451 -0.47526151 -0.29054192 -0.066273518 0.28571302 0.37065119 0.57643551 0.31339717][-0.189945 -0.42433035 -0.62449 -0.69002628 -0.79261506 -0.49013412 -0.38953882 -0.37142032 -0.23713119 0.0099631771 0.23260085 0.42470276 0.44548744 0.73948079 0.73477906][0.19046794 -0.17273389 -0.4227252 -0.27240145 -0.34654224 -0.47472376 -0.32617927 -0.0561117 0.10804069 0.47632861 0.64378214 0.78559965 0.80029249 0.64702892 0.39640719][1.1492295 0.8541491 0.43636245 0.31861752 0.33681947 0.23415734 0.10763671 0.079921544 0.20039366 0.21557702 0.408598 0.56218141 0.5813185 0.55636185 0.3734175]]...]
INFO - root - 2017-12-06 04:56:47.321297: step 7110, loss = 0.86, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 21h:07m:33s remains)
INFO - root - 2017-12-06 04:56:49.641600: step 7120, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 21h:01m:40s remains)
INFO - root - 2017-12-06 04:56:51.987748: step 7130, loss = 0.88, batch loss = 0.66 (32.9 examples/sec; 0.243 sec/batch; 21h:58m:38s remains)
INFO - root - 2017-12-06 04:56:54.310915: step 7140, loss = 0.89, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:12s remains)
INFO - root - 2017-12-06 04:56:56.639305: step 7150, loss = 0.93, batch loss = 0.71 (35.8 examples/sec; 0.223 sec/batch; 20h:11m:11s remains)
INFO - root - 2017-12-06 04:56:58.929172: step 7160, loss = 0.84, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 20h:26m:20s remains)
INFO - root - 2017-12-06 04:57:01.272719: step 7170, loss = 0.85, batch loss = 0.63 (35.7 examples/sec; 0.224 sec/batch; 20h:14m:04s remains)
INFO - root - 2017-12-06 04:57:03.562484: step 7180, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:36m:12s remains)
INFO - root - 2017-12-06 04:57:05.882046: step 7190, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 20h:57m:30s remains)
INFO - root - 2017-12-06 04:57:08.183559: step 7200, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.226 sec/batch; 20h:26m:59s remains)
2017-12-06 04:57:08.556176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.43047479 -0.584906 -0.55688375 -0.46273595 -0.44667673 -0.49870211 -0.50246584 -0.40643284 -0.31108895 -0.086932115 0.13599132 0.34580478 0.36704373 0.41801167 0.38619092][-0.40441468 -0.6022743 -0.63993454 -0.56887007 -0.49165291 -0.48133907 -0.42706093 -0.38649219 -0.36120898 -0.16196671 0.025890552 0.16954343 0.27117842 0.34310737 0.34562704][0.011136688 -0.14747533 -0.3078559 -0.3281762 -0.33377352 -0.28660396 -0.23947924 -0.2275202 -0.11463923 -0.086016305 0.073623896 0.19486499 0.25476131 0.30292794 0.26966006][0.45905468 0.37376961 0.20835578 0.14747134 0.10007034 0.078639254 0.063840047 0.060344532 0.13529418 0.17597237 0.27957541 0.27856541 0.31444016 0.28962117 0.25315306][0.53012013 0.50616479 0.53068542 0.40449834 0.3259185 0.32550728 0.30298713 0.32161218 0.41561937 0.47838554 0.45330206 0.39081186 0.23090643 0.065748036 -0.017278973][0.45043263 0.47860953 0.51844728 0.50699449 0.4243758 0.46371868 0.49715981 0.5099777 0.53138423 0.59767842 0.52247095 0.43350753 0.21168435 0.030823015 -0.080598041][0.28512564 0.45026067 0.546994 0.51216114 0.54289448 0.51190996 0.54500508 0.6184479 0.63415122 0.61850107 0.56188071 0.50628674 0.24103874 0.069728449 -0.066565625][0.15975827 0.39389843 0.45793989 0.56020486 0.621896 0.59270918 0.58023381 0.63363171 0.64372313 0.59727621 0.56536353 0.42647067 0.27604002 0.068292692 -0.10185948][-0.26514944 0.11064501 0.19382218 0.27237898 0.40789375 0.48146549 0.58213151 0.62901473 0.60541093 0.5143913 0.52272379 0.33310792 0.19739676 -0.066367641 -0.15938056][-0.64538217 -0.31666616 -0.082204893 0.047093436 0.099718496 0.16013853 0.33450791 0.45202157 0.44318858 0.36390007 0.28090653 0.10535702 -0.13675545 -0.2600075 -0.39327437][-0.59832871 -0.32136029 -0.14685702 -0.092144445 -0.052349053 -0.027092308 0.022049963 0.095971167 0.12932342 0.036524206 -0.066826805 -0.19083108 -0.51559174 -0.622293 -0.67514348][-0.65704632 -0.43214944 -0.21887322 -0.11188811 -0.12037577 -0.17661536 -0.19974396 -0.19163035 -0.28851658 -0.42338416 -0.54121506 -0.66474164 -0.85529268 -0.92242467 -0.858865][-0.89047277 -0.76890647 -0.487978 -0.47092146 -0.54635322 -0.51212567 -0.49549383 -0.53870368 -0.60127032 -0.74549115 -0.92406559 -1.0641944 -1.1397772 -1.200838 -1.1088955][-1.150226 -1.0686918 -0.92866111 -0.95219219 -1.0075077 -0.97475266 -1.0040375 -1.0228453 -1.0486038 -1.1343799 -1.1836741 -1.2423059 -1.302766 -1.3484049 -1.2665696][-1.5115345 -1.347162 -1.1895963 -1.2001461 -1.2365813 -1.2702494 -1.1802993 -1.2616643 -1.3358978 -1.4470208 -1.4811732 -1.4126077 -1.3738077 -1.2907113 -1.2119622]]...]
INFO - root - 2017-12-06 04:57:10.865274: step 7210, loss = 0.89, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:13m:47s remains)
INFO - root - 2017-12-06 04:57:13.132721: step 7220, loss = 0.88, batch loss = 0.66 (36.6 examples/sec; 0.219 sec/batch; 19h:45m:32s remains)
INFO - root - 2017-12-06 04:57:15.538524: step 7230, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 20h:54m:20s remains)
INFO - root - 2017-12-06 04:57:17.814217: step 7240, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:34m:29s remains)
INFO - root - 2017-12-06 04:57:20.181960: step 7250, loss = 0.89, batch loss = 0.68 (33.2 examples/sec; 0.241 sec/batch; 21h:45m:04s remains)
INFO - root - 2017-12-06 04:57:22.517062: step 7260, loss = 0.86, batch loss = 0.65 (32.2 examples/sec; 0.249 sec/batch; 22h:28m:38s remains)
INFO - root - 2017-12-06 04:57:24.849389: step 7270, loss = 0.89, batch loss = 0.68 (33.1 examples/sec; 0.242 sec/batch; 21h:51m:24s remains)
INFO - root - 2017-12-06 04:57:27.166168: step 7280, loss = 0.93, batch loss = 0.72 (33.8 examples/sec; 0.237 sec/batch; 21h:24m:38s remains)
INFO - root - 2017-12-06 04:57:29.531812: step 7290, loss = 0.81, batch loss = 0.60 (33.9 examples/sec; 0.236 sec/batch; 21h:17m:21s remains)
INFO - root - 2017-12-06 04:57:31.853177: step 7300, loss = 0.90, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 20h:45m:02s remains)
2017-12-06 04:57:33.169874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15467556 -0.1735242 -0.19322985 -0.20603305 -0.24971309 -0.3113932 -0.43710035 -0.583937 -0.72273827 -0.85122079 -0.93577164 -0.98264444 -1.0084291 -0.98696041 -0.92590022][-0.20410705 -0.26071998 -0.31447262 -0.33382004 -0.36358154 -0.44012356 -0.56138808 -0.72232932 -0.90826553 -1.0409389 -1.1489518 -1.2558011 -1.341379 -1.3768713 -1.3572905][-0.22740865 -0.31949645 -0.40553853 -0.44149941 -0.45443666 -0.452166 -0.49490523 -0.62537819 -0.79327208 -0.94406581 -1.0856223 -1.1863747 -1.3028691 -1.4419121 -1.5287883][-0.20470196 -0.29603595 -0.38128322 -0.39718169 -0.36131075 -0.28757897 -0.24796963 -0.27301437 -0.39335993 -0.52170819 -0.66578221 -0.798756 -0.95407844 -1.1173302 -1.2895392][-0.11152196 -0.14439079 -0.15697375 -0.1137657 -0.0060681924 0.17499995 0.30791238 0.36372304 0.30348328 0.13465837 -0.088749491 -0.28563398 -0.46435133 -0.65447193 -0.98274362][-0.039631579 0.0058565959 0.093458973 0.27666855 0.50786167 0.76734865 0.93512297 1.0607991 1.0390122 0.88446647 0.6625526 0.39704904 0.14419404 -0.11532413 -0.56201237][0.0061521456 0.1197199 0.3058905 0.59854096 0.94731748 1.3018692 1.4912986 1.6068283 1.5605981 1.4655533 1.2916931 1.0813483 0.88188636 0.64490581 0.31094807][0.008505784 0.13993669 0.3623662 0.708311 1.1227621 1.5607649 1.8493443 1.9778001 1.9110682 1.8004361 1.6447892 1.5616672 1.4391726 1.323313 1.1045644][-0.024353195 0.086576708 0.27451444 0.56907248 0.93332481 1.324366 1.6092156 1.8121569 1.8268037 1.7569089 1.6256071 1.5685595 1.5662556 1.5531325 1.5784285][-0.068987295 -0.014781483 0.085866548 0.26620767 0.48997882 0.76661021 0.99103963 1.1337928 1.1775308 1.219079 1.1482965 1.1856709 1.2091622 1.3262805 1.424718][-0.08735311 -0.074062452 -0.042977419 0.0067956075 0.074080236 0.18292239 0.2931188 0.40931305 0.49128798 0.58841145 0.68116796 0.77400935 0.89087528 1.0304481 1.0323503][-0.0990147 -0.10091333 -0.10629889 -0.11965367 -0.13016644 -0.14371336 -0.14213659 -0.094120547 -0.012335785 0.10618953 0.25183132 0.38519835 0.49272588 0.62777305 0.61503714][-0.10269067 -0.10853732 -0.12440196 -0.15587711 -0.1948906 -0.23157847 -0.24862221 -0.26471713 -0.25346449 -0.18374404 -0.080343425 -0.00047359616 0.057193927 0.17779201 0.23717132][-0.10143612 -0.10035534 -0.10386667 -0.11816931 -0.14294589 -0.18555441 -0.22444108 -0.25689557 -0.26899502 -0.26823258 -0.23087189 -0.17647666 -0.15170836 -0.080917418 -0.048568048][-0.10211246 -0.099427164 -0.10004242 -0.10743818 -0.12250105 -0.14625537 -0.17633407 -0.20749071 -0.2350741 -0.25191313 -0.24030125 -0.24136427 -0.22663006 -0.21385217 -0.18522644]]...]
INFO - root - 2017-12-06 04:57:35.513554: step 7310, loss = 0.88, batch loss = 0.67 (33.9 examples/sec; 0.236 sec/batch; 21h:19m:23s remains)
INFO - root - 2017-12-06 04:57:37.797250: step 7320, loss = 0.91, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-06 04:57:40.134973: step 7330, loss = 0.89, batch loss = 0.68 (35.4 examples/sec; 0.226 sec/batch; 20h:25m:35s remains)
INFO - root - 2017-12-06 04:57:42.434560: step 7340, loss = 0.83, batch loss = 0.62 (34.6 examples/sec; 0.231 sec/batch; 20h:53m:14s remains)
INFO - root - 2017-12-06 04:57:44.787070: step 7350, loss = 0.90, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:17m:04s remains)
INFO - root - 2017-12-06 04:57:47.077727: step 7360, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:24m:49s remains)
INFO - root - 2017-12-06 04:57:49.399310: step 7370, loss = 0.90, batch loss = 0.68 (34.9 examples/sec; 0.229 sec/batch; 20h:41m:32s remains)
INFO - root - 2017-12-06 04:57:51.751220: step 7380, loss = 0.83, batch loss = 0.62 (32.5 examples/sec; 0.246 sec/batch; 22h:15m:23s remains)
INFO - root - 2017-12-06 04:57:54.070209: step 7390, loss = 0.84, batch loss = 0.63 (34.7 examples/sec; 0.230 sec/batch; 20h:48m:45s remains)
INFO - root - 2017-12-06 04:57:56.379166: step 7400, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 20h:37m:49s remains)
2017-12-06 04:57:57.909145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21298477 -0.2722629 -0.34913969 -0.44952905 -0.56870234 -0.66124272 -0.75486493 -0.76406842 -0.77357197 -0.81216109 -0.92290354 -1.0009459 -1.0045233 -0.90066195 -0.69558322][-0.28942308 -0.4005205 -0.54452252 -0.696007 -0.84606725 -0.94792348 -0.98265815 -1.0135794 -1.1133063 -1.1955847 -1.3748392 -1.5153922 -1.5632298 -1.4640187 -1.2714901][-0.35142344 -0.52412373 -0.7220161 -0.92692584 -1.1227133 -1.2441143 -1.3306171 -1.3217355 -1.3553337 -1.5202047 -1.7729572 -1.9784296 -2.1073062 -2.0415802 -1.897431][-0.31541747 -0.51204264 -0.75102341 -1.0093889 -1.2284766 -1.3920728 -1.4668441 -1.4596603 -1.5631547 -1.6511456 -1.8088884 -2.0459847 -2.1891696 -2.2533548 -2.3104951][-0.24679218 -0.40049738 -0.58950263 -0.82939613 -1.0465596 -1.2118337 -1.3063774 -1.3224518 -1.3246566 -1.4034808 -1.6038451 -1.6562557 -1.7444994 -1.7876523 -1.8777125][-0.2150742 -0.30781269 -0.40058124 -0.50515288 -0.571961 -0.63753134 -0.65500677 -0.62867779 -0.63262093 -0.67080349 -0.73169404 -0.76095396 -0.7816844 -0.60149407 -0.4417783][-0.20621759 -0.26076609 -0.26030755 -0.19818819 -0.084915414 0.095017791 0.3201251 0.44918084 0.56622821 0.61285216 0.53705019 0.5638653 0.78258651 0.82549661 0.97525561][-0.24271856 -0.260567 -0.155831 0.037429824 0.29544979 0.60183132 0.93919122 1.3565406 1.7602437 1.847888 1.8399801 1.7386096 1.779063 1.9168763 2.013665][-0.30895704 -0.33464026 -0.20740643 0.12780452 0.56043357 0.94115388 1.1784968 1.3641961 1.5638782 1.9121517 2.1053791 2.071135 2.0886843 2.0977561 2.1444719][-0.39384317 -0.45841873 -0.37928939 -0.164453 0.20040448 0.70128906 1.0943321 1.2727884 1.2545387 1.170467 1.1849505 1.513447 1.8567539 2.0157225 2.0532269][-0.39386177 -0.52089989 -0.54177552 -0.47267491 -0.29905114 -0.083454326 0.07725206 0.32590866 0.38686359 0.35348445 0.28990632 0.28511131 0.45456934 0.82155043 1.0595274][-0.3083511 -0.44848341 -0.60001594 -0.65833884 -0.62701339 -0.58429885 -0.53271717 -0.54409969 -0.65477937 -0.63792086 -0.68810409 -0.72873253 -0.82038826 -0.74163884 -0.68800867][-0.23594791 -0.33945066 -0.47432232 -0.57003987 -0.68594611 -0.75054646 -0.78169048 -0.85449994 -0.9927125 -1.1814861 -1.3464084 -1.3483846 -1.3979952 -1.2575501 -1.3115733][-0.18401212 -0.24007669 -0.30848649 -0.40059733 -0.53846806 -0.65032178 -0.75650507 -0.81058347 -0.94215077 -1.1025875 -1.2104676 -1.3627172 -1.4201965 -1.236161 -1.2922577][-0.18104309 -0.21057086 -0.23738325 -0.29089272 -0.35609031 -0.47773248 -0.64982492 -0.75472605 -0.84540981 -0.88836843 -0.92909688 -1.0368114 -1.1190848 -1.1713746 -1.1368226]]...]
INFO - root - 2017-12-06 04:58:00.183778: step 7410, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 20h:13m:19s remains)
INFO - root - 2017-12-06 04:58:02.480005: step 7420, loss = 0.87, batch loss = 0.65 (33.4 examples/sec; 0.240 sec/batch; 21h:39m:11s remains)
INFO - root - 2017-12-06 04:58:04.787509: step 7430, loss = 0.90, batch loss = 0.69 (33.7 examples/sec; 0.237 sec/batch; 21h:26m:13s remains)
INFO - root - 2017-12-06 04:58:07.084366: step 7440, loss = 0.85, batch loss = 0.64 (34.5 examples/sec; 0.232 sec/batch; 20h:56m:17s remains)
INFO - root - 2017-12-06 04:58:09.390787: step 7450, loss = 0.81, batch loss = 0.60 (36.7 examples/sec; 0.218 sec/batch; 19h:42m:09s remains)
INFO - root - 2017-12-06 04:58:11.688635: step 7460, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.228 sec/batch; 20h:37m:35s remains)
INFO - root - 2017-12-06 04:58:13.975057: step 7470, loss = 0.83, batch loss = 0.62 (32.1 examples/sec; 0.249 sec/batch; 22h:29m:01s remains)
INFO - root - 2017-12-06 04:58:16.276599: step 7480, loss = 0.89, batch loss = 0.68 (35.6 examples/sec; 0.225 sec/batch; 20h:18m:34s remains)
INFO - root - 2017-12-06 04:58:18.592573: step 7490, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 20h:25m:06s remains)
INFO - root - 2017-12-06 04:58:20.861172: step 7500, loss = 0.89, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:28m:49s remains)
2017-12-06 04:58:21.256095: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.059404753 -0.23063147 -0.14553663 -0.38556322 -0.37123528 -0.5585351 -0.65679669 -0.18052891 -0.078982674 -0.55855173 -0.82230991 -0.7765125 -1.0074608 -0.50152147 -0.51183015][0.25756422 -0.16618438 -0.41642585 -0.30472794 -0.45670757 -0.55095083 -0.31354043 0.10662506 0.068394594 -0.12340486 -0.492734 -0.62124646 -0.55612195 -0.51996464 -0.48302695][0.13460621 0.074081771 -0.0053721666 -0.088252172 -0.21575886 -0.34754696 -0.18108836 -0.0045327246 -0.027892761 -0.20153773 -0.58737236 -0.74579626 -0.61089534 -0.58448571 -0.49119288][0.24183732 -0.01533924 -0.013387658 0.26476696 0.33844304 0.079175256 0.066183113 0.1630474 0.098185338 -0.053544175 -0.12161177 -0.4954024 -0.6124863 -0.65391171 -0.58728391][0.23119348 -0.070049509 0.16875607 0.22437906 0.35060045 0.54164755 0.70424575 0.76232493 0.54506868 0.42993259 0.32132429 -0.034990523 -0.21307337 -0.33466393 -0.30202422][0.84175986 0.32067776 0.18602204 0.20916548 0.39457655 0.53780645 0.60589194 0.82899094 0.94608283 0.81550092 0.40142658 0.090618424 0.0069618374 -0.11124264 -0.22587705][1.2130473 1.1587775 0.85143149 0.51500058 0.60792547 0.56335968 0.66408408 0.94243789 0.99116218 0.85800534 0.7921024 0.26992449 0.036225013 -0.01622697 -0.28398114][1.5976827 1.3569795 1.1286613 0.98066175 0.77226937 0.70778424 0.77865481 0.73017794 0.571123 0.54689026 0.37557697 0.12992239 0.062477045 0.071708389 -0.014421888][1.8495041 1.6058891 1.5947772 1.2620218 1.1730103 1.0002806 1.141605 0.92909884 0.76283747 0.65028989 0.38900214 0.48283148 0.48022753 0.27551728 0.03909307][1.4101683 1.3898673 1.4303607 1.2089522 0.80788153 0.68491572 0.5680446 0.4209097 0.2709575 0.42483622 0.65263331 0.34220165 0.29780737 0.47925019 0.35746515][0.82279366 0.69182587 0.61025971 0.64643389 0.55916548 0.35302618 0.31342864 0.22134039 0.16804349 0.25305021 0.15967828 0.32816944 0.65381968 0.5381645 0.47693413][0.82221371 0.65206271 0.65275133 0.68302405 0.60895735 0.66970009 0.74140853 0.57639182 0.42040718 0.26136383 0.17361894 0.22682363 0.49056631 0.94285333 1.3332329][0.40210375 0.5149104 0.56032145 0.34461415 0.13318539 0.26666591 0.40412697 0.35115111 0.2988745 0.11665807 0.072706707 0.19155872 0.34851488 0.77987766 1.2764848][0.035048574 0.0879583 0.029272206 0.10731051 0.268718 0.20441478 0.19603994 0.24197876 0.30479175 0.21731755 0.071491607 0.21130607 0.59251434 0.8766197 1.1920854][0.34074822 -0.012888089 -0.24730399 -0.1750356 -0.051191643 -0.088562265 -0.21165511 -0.25631705 0.013567351 0.14241195 0.27433506 0.070228495 0.023614243 0.51926982 0.87489873]]...]
INFO - root - 2017-12-06 04:58:23.541586: step 7510, loss = 0.90, batch loss = 0.68 (34.7 examples/sec; 0.231 sec/batch; 20h:49m:48s remains)
INFO - root - 2017-12-06 04:58:25.807561: step 7520, loss = 0.89, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:08m:21s remains)
INFO - root - 2017-12-06 04:58:28.127999: step 7530, loss = 0.88, batch loss = 0.66 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:50s remains)
INFO - root - 2017-12-06 04:58:30.415670: step 7540, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:51s remains)
INFO - root - 2017-12-06 04:58:32.724128: step 7550, loss = 0.87, batch loss = 0.66 (33.9 examples/sec; 0.236 sec/batch; 21h:19m:00s remains)
INFO - root - 2017-12-06 04:58:35.035805: step 7560, loss = 0.83, batch loss = 0.61 (35.8 examples/sec; 0.224 sec/batch; 20h:11m:34s remains)
INFO - root - 2017-12-06 04:58:37.341806: step 7570, loss = 0.87, batch loss = 0.65 (35.2 examples/sec; 0.227 sec/batch; 20h:31m:38s remains)
INFO - root - 2017-12-06 04:58:39.653452: step 7580, loss = 0.86, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 20h:43m:37s remains)
INFO - root - 2017-12-06 04:58:41.927704: step 7590, loss = 0.78, batch loss = 0.57 (36.2 examples/sec; 0.221 sec/batch; 19h:57m:37s remains)
INFO - root - 2017-12-06 04:58:44.246295: step 7600, loss = 0.88, batch loss = 0.67 (31.8 examples/sec; 0.251 sec/batch; 22h:40m:55s remains)
2017-12-06 04:58:44.689868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0035447627 9.9763274e-05 0.0027343258 0.0047933981 0.0057096258 0.0056072026 0.0046186671 0.0030917302 0.0022316054 0.0022870898 0.002769053 0.0035205334 0.0048986822 0.0057344213 0.0037819818][-0.0013892204 0.0029996186 0.0064418688 0.009323746 0.010806687 0.011097044 0.010256954 0.00835792 0.0065803081 0.0057179704 0.0057288483 0.006104432 0.0074705333 0.0088132024 0.0073665529][-8.1270933e-05 0.0050967857 0.0089660436 0.012281016 0.014322639 0.015127622 0.01468011 0.01282192 0.010499604 0.0084650218 0.0072526932 0.0066185892 0.0075404644 0.0090193748 0.0082051605][0.00080617517 0.0065719634 0.010757506 0.0142527 0.016762227 0.018244214 0.018219575 0.016620927 0.014092728 0.011135601 0.0083146393 0.00603915 0.0061607435 0.0076484755 0.00779105][0.0013022423 0.0070246235 0.011030786 0.014356442 0.016960107 0.018670678 0.018987671 0.018060774 0.016362123 0.013423122 0.0093189031 0.0057042465 0.004433848 0.0049144104 0.005041793][0.00058758259 0.0059701577 0.0094274133 0.012212656 0.014609769 0.016585618 0.017581202 0.017574504 0.017146744 0.015018769 0.010729156 0.0063843951 0.0035037473 0.0022356659 0.0011916459][-0.0010659844 0.0035918728 0.0062849894 0.0082061812 0.010119036 0.012369581 0.014137357 0.014775962 0.014862917 0.013558351 0.010494076 0.0072789118 0.0038797408 0.0010913089 -0.0015068576][-0.003555052 -8.3379447e-05 0.0011707023 0.0016032234 0.0025236309 0.0044884384 0.0069761723 0.0080894828 0.0088303164 0.0088965893 0.0083245039 0.0069717094 0.0042620748 0.0011463091 -0.0019267574][-0.0064286292 -0.0046884641 -0.0057402477 -0.0073671266 -0.0082267746 -0.0076577216 -0.0059559569 -0.003308259 -0.0007448867 0.0015213117 0.0034278631 0.0042282492 0.0027977079 0.0002123192 -0.0024830624][-0.0093746856 -0.0094993636 -0.012819685 -0.016489848 -0.019419003 -0.020973012 -0.02087016 -0.017960228 -0.01337032 -0.0088339895 -0.004702203 -0.0017940104 -0.0013907775 -0.0023708791 -0.0042600706][-0.012181535 -0.014126495 -0.019280948 -0.024940211 -0.030150782 -0.03423534 -0.036035452 -0.033686992 -0.028317064 -0.021993399 -0.016040891 -0.011405796 -0.0090730414 -0.0084858984 -0.0098066255][-0.013968393 -0.017221898 -0.023999374 -0.031708028 -0.039138526 -0.045086347 -0.048202049 -0.047040038 -0.042327721 -0.035620779 -0.029177815 -0.023716502 -0.019895092 -0.01809385 -0.018828981][-0.014535964 -0.01869195 -0.026807074 -0.036318246 -0.045353472 -0.052658938 -0.056802392 -0.056621112 -0.052798696 -0.047131322 -0.04118105 -0.035580549 -0.031329498 -0.028811187 -0.028806772][-0.014705852 -0.018843051 -0.027240898 -0.03722224 -0.046898734 -0.055184878 -0.060386665 -0.061359026 -0.058760215 -0.054290198 -0.049060032 -0.044020504 -0.040291872 -0.037885398 -0.037564661][-0.015886068 -0.019477095 -0.027680509 -0.037395764 -0.046743646 -0.055251207 -0.060884431 -0.062581085 -0.061133698 -0.057847813 -0.053895034 -0.050109323 -0.047447897 -0.045836132 -0.045753982]]...]
INFO - root - 2017-12-06 04:58:47.009592: step 7610, loss = 0.85, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:30m:27s remains)
INFO - root - 2017-12-06 04:58:49.319671: step 7620, loss = 0.84, batch loss = 0.62 (35.2 examples/sec; 0.227 sec/batch; 20h:30m:29s remains)
INFO - root - 2017-12-06 04:58:51.606456: step 7630, loss = 0.82, batch loss = 0.61 (35.0 examples/sec; 0.229 sec/batch; 20h:38m:36s remains)
INFO - root - 2017-12-06 04:58:53.975192: step 7640, loss = 0.79, batch loss = 0.57 (31.2 examples/sec; 0.256 sec/batch; 23h:08m:42s remains)
INFO - root - 2017-12-06 04:58:56.308967: step 7650, loss = 0.85, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 21h:07m:51s remains)
INFO - root - 2017-12-06 04:58:58.631306: step 7660, loss = 0.82, batch loss = 0.61 (35.6 examples/sec; 0.225 sec/batch; 20h:17m:49s remains)
INFO - root - 2017-12-06 04:59:00.938242: step 7670, loss = 0.86, batch loss = 0.65 (32.9 examples/sec; 0.244 sec/batch; 21h:58m:21s remains)
INFO - root - 2017-12-06 04:59:03.192495: step 7680, loss = 0.85, batch loss = 0.64 (36.1 examples/sec; 0.221 sec/batch; 19h:58m:12s remains)
INFO - root - 2017-12-06 04:59:05.546581: step 7690, loss = 0.90, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 20h:49m:34s remains)
INFO - root - 2017-12-06 04:59:07.840012: step 7700, loss = 0.92, batch loss = 0.70 (33.6 examples/sec; 0.238 sec/batch; 21h:28m:30s remains)
2017-12-06 04:59:08.174474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071602508 -0.071301527 -0.07117594 -0.071235687 -0.071513422 -0.071880661 -0.072256878 -0.072682425 -0.073019452 -0.073081464 -0.072946779 -0.072711982 -0.0725067 -0.072402939 -0.072342746][-0.070694745 -0.070409276 -0.070359878 -0.070539065 -0.071080968 -0.071666241 -0.0722035 -0.07291209 -0.073410809 -0.073437609 -0.07320369 -0.072921656 -0.072707757 -0.072570518 -0.072476707][-0.069841132 -0.069376349 -0.069375046 -0.069711186 -0.070465721 -0.071263984 -0.072013073 -0.07292714 -0.073491171 -0.07355357 -0.073402561 -0.073221788 -0.073049322 -0.072896928 -0.072776772][-0.069220223 -0.068475664 -0.06845244 -0.068850219 -0.069633268 -0.0705865 -0.071607366 -0.072607577 -0.073213249 -0.073402941 -0.073376067 -0.073329851 -0.073278949 -0.0731467 -0.072998792][-0.068668865 -0.067733072 -0.067699023 -0.0681058 -0.068988055 -0.070184387 -0.071417756 -0.072321393 -0.072891429 -0.07302697 -0.073059693 -0.073133208 -0.073184542 -0.0731635 -0.073122337][-0.068074584 -0.067145914 -0.067123964 -0.067694172 -0.0688281 -0.070348471 -0.071557105 -0.072333671 -0.072651267 -0.072560273 -0.072523706 -0.072593644 -0.072765239 -0.073007718 -0.073184][-0.067377724 -0.066533089 -0.06674923 -0.067466572 -0.068761773 -0.070346616 -0.071485668 -0.072193339 -0.072341278 -0.072100773 -0.071987823 -0.072040074 -0.072265387 -0.072752148 -0.073185667][-0.066972189 -0.066225454 -0.066668689 -0.067468368 -0.0686962 -0.070139609 -0.071172677 -0.072041824 -0.072370529 -0.072140269 -0.071818016 -0.071738772 -0.072049171 -0.072723284 -0.073209211][-0.066619292 -0.06610994 -0.066694662 -0.06757912 -0.068804212 -0.070152044 -0.071196787 -0.072251588 -0.072766364 -0.072630607 -0.072367996 -0.072169028 -0.072389163 -0.072903708 -0.073212355][-0.066228494 -0.066020168 -0.066944569 -0.067995429 -0.069347329 -0.070809044 -0.07189209 -0.072805837 -0.073341705 -0.073333465 -0.073137283 -0.072801091 -0.072883643 -0.073142439 -0.073206834][-0.066110559 -0.066290148 -0.067607053 -0.0690139 -0.070456959 -0.0718562 -0.07279148 -0.073378742 -0.073694915 -0.073612534 -0.073487177 -0.073302 -0.07332629 -0.073405765 -0.073369011][-0.066448078 -0.066944286 -0.068672843 -0.070311144 -0.071742915 -0.072850756 -0.073421814 -0.073706716 -0.073758967 -0.073747844 -0.073796906 -0.07372009 -0.073676191 -0.073612384 -0.073485374][-0.067038 -0.067666143 -0.069546625 -0.071315266 -0.072648562 -0.073422819 -0.073685564 -0.073754616 -0.073705785 -0.073707812 -0.073782437 -0.073696136 -0.0736133 -0.073523358 -0.073469117][-0.067618646 -0.068293422 -0.07019493 -0.071889624 -0.0730345 -0.073606513 -0.073716104 -0.073696375 -0.073606253 -0.073585376 -0.073606677 -0.073508017 -0.07343214 -0.073374294 -0.073371612][-0.068508618 -0.069048017 -0.070743144 -0.072200336 -0.07311061 -0.073524207 -0.07357087 -0.073515005 -0.07343208 -0.073373362 -0.073334962 -0.073257 -0.073231876 -0.073220119 -0.073274478]]...]
INFO - root - 2017-12-06 04:59:10.476959: step 7710, loss = 0.87, batch loss = 0.66 (34.3 examples/sec; 0.233 sec/batch; 21h:03m:25s remains)
INFO - root - 2017-12-06 04:59:12.795230: step 7720, loss = 0.90, batch loss = 0.68 (35.0 examples/sec; 0.229 sec/batch; 20h:38m:31s remains)
INFO - root - 2017-12-06 04:59:15.085408: step 7730, loss = 0.85, batch loss = 0.64 (36.1 examples/sec; 0.222 sec/batch; 20h:00m:56s remains)
INFO - root - 2017-12-06 04:59:17.360701: step 7740, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:21s remains)
INFO - root - 2017-12-06 04:59:19.602687: step 7750, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:22m:23s remains)
INFO - root - 2017-12-06 04:59:21.945618: step 7760, loss = 0.86, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:29m:02s remains)
INFO - root - 2017-12-06 04:59:24.247304: step 7770, loss = 0.84, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 20h:52m:18s remains)
INFO - root - 2017-12-06 04:59:26.550126: step 7780, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.235 sec/batch; 21h:10m:38s remains)
INFO - root - 2017-12-06 04:59:28.863082: step 7790, loss = 0.88, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 21h:32m:23s remains)
INFO - root - 2017-12-06 04:59:31.124746: step 7800, loss = 0.82, batch loss = 0.61 (33.9 examples/sec; 0.236 sec/batch; 21h:18m:28s remains)
2017-12-06 04:59:31.520410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076823443 -0.077393472 -0.078357428 -0.079136193 -0.07921569 -0.079190932 -0.079401366 -0.079974242 -0.081205167 -0.082645491 -0.084248312 -0.0842544 -0.08280398 -0.081136182 -0.079359286][-0.076441422 -0.076124586 -0.074972585 -0.072249278 -0.067441761 -0.061537888 -0.055696972 -0.051782422 -0.052705117 -0.058599062 -0.066979788 -0.073384158 -0.076344416 -0.076978154 -0.076197252][-0.07478562 -0.072584532 -0.0681371 -0.06039831 -0.049158715 -0.035947308 -0.023094814 -0.013739139 -0.014939852 -0.027718015 -0.046411395 -0.063241728 -0.073581204 -0.07788498 -0.07763923][-0.071738996 -0.069909789 -0.067681827 -0.066251934 -0.066484995 -0.067797378 -0.067840062 -0.066451274 -0.068172827 -0.074336387 -0.081956513 -0.087282114 -0.087828107 -0.085411109 -0.081165947][-0.068545811 -0.066964 -0.066127852 -0.068426535 -0.074725538 -0.082548469 -0.088832617 -0.092644691 -0.095416673 -0.0966636 -0.095121875 -0.091002092 -0.086294942 -0.082332887 -0.07820607][-0.067629293 -0.066371806 -0.066090211 -0.067066543 -0.0697385 -0.070891917 -0.069756374 -0.069947854 -0.073209569 -0.077433407 -0.078225635 -0.075113855 -0.072117455 -0.070389941 -0.068856567][-0.0708307 -0.069203541 -0.066178367 -0.058530547 -0.043441597 -0.019026276 0.0085558817 0.027357303 0.031045623 0.024120003 0.01363162 0.0033622459 -0.0092813745 -0.024170339 -0.036215041][-0.074062556 -0.073973246 -0.073474191 -0.0697304 -0.059644286 -0.041359544 -0.017486282 0.0016764477 0.010375582 0.0097562522 0.0041347146 -0.0040332675 -0.015600257 -0.029358149 -0.038703009][-0.072685748 -0.072971657 -0.073186234 -0.073075473 -0.069031864 -0.058875173 -0.042215921 -0.02539045 -0.013567962 -0.011022352 -0.015358642 -0.023919895 -0.034597468 -0.045609813 -0.051395312][-0.068032011 -0.068029307 -0.070099667 -0.075493358 -0.0819667 -0.086558513 -0.085427083 -0.078784287 -0.069619812 -0.0632142 -0.058111656 -0.056585766 -0.057529792 -0.061369374 -0.063780293][-0.064810008 -0.06396845 -0.065730788 -0.070683137 -0.078097954 -0.084275246 -0.086316533 -0.0845433 -0.07892663 -0.0719209 -0.063946486 -0.058520447 -0.0568531 -0.059581805 -0.063318588][-0.065063767 -0.063392006 -0.063513167 -0.065296084 -0.06937325 -0.073647007 -0.077462412 -0.079139695 -0.079367071 -0.077316359 -0.072212875 -0.067012578 -0.063996442 -0.063698351 -0.064553857][-0.066582039 -0.065908648 -0.066134006 -0.066749819 -0.067750193 -0.068472967 -0.069905981 -0.070490688 -0.071193516 -0.071441039 -0.0702359 -0.067176566 -0.065618925 -0.066963449 -0.068556249][-0.067369394 -0.067822434 -0.068943977 -0.070135638 -0.070547543 -0.070225708 -0.0701339 -0.070073172 -0.069100417 -0.069332 -0.0707746 -0.068752512 -0.067894086 -0.069491945 -0.072187759][-0.067480721 -0.0680555 -0.069209561 -0.070722416 -0.071910322 -0.07243602 -0.072269812 -0.071755655 -0.070357949 -0.070862494 -0.072776787 -0.071064487 -0.069711283 -0.0700767 -0.071491547]]...]
INFO - root - 2017-12-06 04:59:33.821460: step 7810, loss = 0.87, batch loss = 0.66 (34.1 examples/sec; 0.234 sec/batch; 21h:08m:20s remains)
INFO - root - 2017-12-06 04:59:36.130340: step 7820, loss = 0.87, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 20h:42m:24s remains)
INFO - root - 2017-12-06 04:59:38.427659: step 7830, loss = 0.95, batch loss = 0.74 (33.8 examples/sec; 0.237 sec/batch; 21h:20m:39s remains)
INFO - root - 2017-12-06 04:59:40.811243: step 7840, loss = 0.94, batch loss = 0.72 (35.1 examples/sec; 0.228 sec/batch; 20h:34m:24s remains)
INFO - root - 2017-12-06 04:59:43.116528: step 7850, loss = 0.89, batch loss = 0.68 (34.9 examples/sec; 0.229 sec/batch; 20h:41m:11s remains)
INFO - root - 2017-12-06 04:59:45.421344: step 7860, loss = 0.83, batch loss = 0.61 (34.7 examples/sec; 0.230 sec/batch; 20h:46m:15s remains)
INFO - root - 2017-12-06 04:59:47.702522: step 7870, loss = 1.05, batch loss = 0.84 (35.8 examples/sec; 0.224 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-06 04:59:50.000792: step 7880, loss = 1.09, batch loss = 0.87 (35.4 examples/sec; 0.226 sec/batch; 20h:21m:37s remains)
INFO - root - 2017-12-06 04:59:52.282939: step 7890, loss = 0.98, batch loss = 0.77 (35.4 examples/sec; 0.226 sec/batch; 20h:22m:03s remains)
INFO - root - 2017-12-06 04:59:54.573640: step 7900, loss = 1.00, batch loss = 0.79 (35.5 examples/sec; 0.226 sec/batch; 20h:20m:19s remains)
2017-12-06 04:59:56.162805: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018721223 0.024230652 0.031037569 0.03615436 0.035669021 0.028381735 0.016714096 0.0014912412 -0.013849691 -0.025331993 -0.029649727 -0.028471328 -0.025284905 -0.021466963 -0.019095849][0.0075166523 0.01314266 0.021037705 0.027313247 0.027469374 0.019130953 0.0045721009 -0.014992796 -0.035179835 -0.050547462 -0.056819964 -0.054602586 -0.048496813 -0.041428063 -0.036383282][0.002569437 0.00890854 0.017669365 0.024676956 0.024945915 0.01531034 -0.0022771582 -0.026937865 -0.053061485 -0.073254049 -0.081913114 -0.079545408 -0.071211793 -0.060507484 -0.05124113][0.0077442527 0.013487883 0.021707967 0.028815985 0.029308677 0.018608332 -0.0015385449 -0.029693253 -0.06064783 -0.085961856 -0.098064147 -0.096945889 -0.087249525 -0.073227227 -0.059441932][0.024955586 0.029203676 0.035586715 0.040913947 0.040174194 0.02752579 0.0049989 -0.025152743 -0.059340015 -0.088882387 -0.10479979 -0.10614745 -0.096645616 -0.080869839 -0.062531859][0.044571176 0.0479151 0.0520885 0.055709347 0.053483456 0.039720662 0.015505172 -0.016477667 -0.052598856 -0.085605718 -0.10636717 -0.11257377 -0.1060165 -0.091065884 -0.069256559][0.063193426 0.064294383 0.065346569 0.065458372 0.061178416 0.047657683 0.024119571 -0.0070600435 -0.043603219 -0.079389319 -0.10567435 -0.11810805 -0.1170912 -0.1059006 -0.0833859][0.077147886 0.075379342 0.072163492 0.067819789 0.059898108 0.045773491 0.023916274 -0.0046065077 -0.038843397 -0.075696439 -0.10637444 -0.12539473 -0.13119602 -0.12450445 -0.10369799][0.081177309 0.0758636 0.06833075 0.060472965 0.050350428 0.035942815 0.016088665 -0.0083732307 -0.039018109 -0.075404368 -0.10931832 -0.13381337 -0.14566019 -0.14490235 -0.12751998][0.0790773 0.070755288 0.059008941 0.046776772 0.033899248 0.018795885 0.0016604438 -0.018242829 -0.04345144 -0.076221853 -0.11171579 -0.14023049 -0.15738425 -0.16188031 -0.14856879][0.072389141 0.061711758 0.045997709 0.029930562 0.013970383 -0.0020395666 -0.01788386 -0.034623422 -0.054146569 -0.081464916 -0.1133292 -0.14114287 -0.15978366 -0.16819373 -0.15907469][0.065601528 0.052757755 0.034974158 0.016886614 -0.0015503988 -0.019990608 -0.036295306 -0.050713759 -0.06633462 -0.08721377 -0.11251615 -0.13520625 -0.15181032 -0.16052555 -0.15452364][0.059168249 0.046188712 0.028147884 0.0093307272 -0.0095151588 -0.028672878 -0.045827683 -0.060070787 -0.072919816 -0.088376522 -0.10683765 -0.12346555 -0.1353755 -0.14175086 -0.13715422][0.053283662 0.041884609 0.025353722 0.0075774118 -0.010940351 -0.029521592 -0.046072848 -0.058991864 -0.069772348 -0.0799778 -0.0911334 -0.10129767 -0.10965596 -0.11376379 -0.10975768][0.045960471 0.0365887 0.022261538 0.0063226372 -0.010438278 -0.026580192 -0.040966142 -0.052480064 -0.061299466 -0.067957021 -0.073886454 -0.07898929 -0.083384417 -0.084815428 -0.080861866]]...]
INFO - root - 2017-12-06 04:59:58.426262: step 7910, loss = 0.99, batch loss = 0.78 (35.4 examples/sec; 0.226 sec/batch; 20h:23m:16s remains)
INFO - root - 2017-12-06 05:00:00.750748: step 7920, loss = 0.95, batch loss = 0.73 (33.3 examples/sec; 0.240 sec/batch; 21h:37m:40s remains)
INFO - root - 2017-12-06 05:00:03.038945: step 7930, loss = 1.01, batch loss = 0.80 (35.8 examples/sec; 0.223 sec/batch; 20h:07m:59s remains)
INFO - root - 2017-12-06 05:00:05.320373: step 7940, loss = 1.00, batch loss = 0.79 (34.2 examples/sec; 0.234 sec/batch; 21h:06m:50s remains)
INFO - root - 2017-12-06 05:00:07.623793: step 7950, loss = 0.94, batch loss = 0.73 (35.2 examples/sec; 0.227 sec/batch; 20h:29m:49s remains)
INFO - root - 2017-12-06 05:00:09.924250: step 7960, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:41m:17s remains)
INFO - root - 2017-12-06 05:00:12.212549: step 7970, loss = 0.90, batch loss = 0.68 (35.9 examples/sec; 0.223 sec/batch; 20h:03m:44s remains)
INFO - root - 2017-12-06 05:00:14.495733: step 7980, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 20h:51m:20s remains)
INFO - root - 2017-12-06 05:00:16.794275: step 7990, loss = 1.03, batch loss = 0.82 (32.6 examples/sec; 0.245 sec/batch; 22h:07m:21s remains)
INFO - root - 2017-12-06 05:00:19.077345: step 8000, loss = 0.97, batch loss = 0.75 (36.0 examples/sec; 0.222 sec/batch; 20h:03m:01s remains)
2017-12-06 05:00:19.440673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039305232 -0.038757592 -0.03917386 -0.03866151 -0.038308918 -0.037516303 -0.035993688 -0.033414636 -0.030166812 -0.027865604 -0.025799736 -0.02320521 -0.018291965 -0.011599898 -0.0050724596][-0.043030851 -0.042586017 -0.043195441 -0.042514779 -0.041710675 -0.040711075 -0.039340314 -0.036465552 -0.033398155 -0.031865843 -0.030661773 -0.028697513 -0.024018146 -0.016876027 -0.0095141008][-0.043555781 -0.042732853 -0.042869929 -0.041750848 -0.040244035 -0.03849113 -0.035822511 -0.032857709 -0.030485857 -0.030022249 -0.030225087 -0.029535886 -0.026119448 -0.019809157 -0.011920229][-0.043299969 -0.042714793 -0.042882737 -0.041161187 -0.038514711 -0.035502952 -0.031846445 -0.027779747 -0.024746705 -0.024777655 -0.025671687 -0.025707714 -0.023331009 -0.018472444 -0.01129254][-0.043950353 -0.04304409 -0.043161374 -0.041381 -0.037996627 -0.033888258 -0.02905279 -0.023850106 -0.020211779 -0.01996839 -0.020977076 -0.021585621 -0.020533655 -0.018370569 -0.015272476][-0.04405712 -0.043154925 -0.043725114 -0.042294916 -0.038606983 -0.033656411 -0.028178606 -0.022486996 -0.018637728 -0.018341891 -0.019958697 -0.021203455 -0.020794522 -0.019285426 -0.017572127][-0.044434257 -0.04363206 -0.044358049 -0.043149471 -0.039791338 -0.034910627 -0.029310983 -0.023510922 -0.01940266 -0.019594036 -0.022149339 -0.024398826 -0.024465166 -0.022563532 -0.019957226][-0.046974327 -0.045882221 -0.046245482 -0.045415454 -0.042823374 -0.039078247 -0.034633264 -0.02962444 -0.026104093 -0.025520366 -0.027644496 -0.030150212 -0.030551545 -0.027945712 -0.023689341][-0.049656868 -0.048198007 -0.047909189 -0.046939418 -0.045146219 -0.043000042 -0.040321931 -0.036917347 -0.034617346 -0.033864297 -0.034859721 -0.03606049 -0.035458323 -0.032533109 -0.028096482][-0.051343128 -0.04957496 -0.049305618 -0.048506241 -0.047729198 -0.047105197 -0.0458965 -0.0439857 -0.042159487 -0.041379061 -0.041168965 -0.041144062 -0.040379681 -0.038036212 -0.034182642][-0.053010639 -0.05161982 -0.051112235 -0.050857022 -0.050659411 -0.050377302 -0.049726717 -0.048527714 -0.047709115 -0.046709988 -0.045674156 -0.044706486 -0.044006556 -0.043100957 -0.041185193][-0.05374331 -0.052903313 -0.052837327 -0.052735083 -0.052401993 -0.051986068 -0.051526792 -0.05074466 -0.05005195 -0.048931189 -0.047462102 -0.046114419 -0.045243405 -0.044720322 -0.044069327][-0.052733682 -0.052211106 -0.052555781 -0.052976605 -0.053126872 -0.053224623 -0.053365171 -0.053455167 -0.053212926 -0.05234281 -0.051079221 -0.049674474 -0.048648812 -0.047965225 -0.046991453][-0.051115125 -0.05045002 -0.05056899 -0.05065456 -0.050624035 -0.050774809 -0.050927147 -0.051108658 -0.051265158 -0.051247302 -0.050718106 -0.049887612 -0.049389113 -0.049035355 -0.048423313][-0.049140044 -0.04850265 -0.048593655 -0.048370462 -0.048226506 -0.048201274 -0.048253324 -0.048277918 -0.048646092 -0.04896374 -0.049082488 -0.048941806 -0.049002942 -0.048770662 -0.048360363]]...]
INFO - root - 2017-12-06 05:00:21.778736: step 8010, loss = 0.86, batch loss = 0.65 (34.2 examples/sec; 0.234 sec/batch; 21h:05m:20s remains)
INFO - root - 2017-12-06 05:00:24.104161: step 8020, loss = 0.94, batch loss = 0.73 (34.8 examples/sec; 0.230 sec/batch; 20h:43m:24s remains)
INFO - root - 2017-12-06 05:00:26.374843: step 8030, loss = 0.91, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:25m:10s remains)
INFO - root - 2017-12-06 05:00:28.642207: step 8040, loss = 0.91, batch loss = 0.70 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:11s remains)
INFO - root - 2017-12-06 05:00:30.950652: step 8050, loss = 0.93, batch loss = 0.71 (35.9 examples/sec; 0.223 sec/batch; 20h:04m:49s remains)
INFO - root - 2017-12-06 05:00:33.225209: step 8060, loss = 0.87, batch loss = 0.65 (35.5 examples/sec; 0.226 sec/batch; 20h:19m:28s remains)
INFO - root - 2017-12-06 05:00:35.554244: step 8070, loss = 0.90, batch loss = 0.69 (34.3 examples/sec; 0.234 sec/batch; 21h:02m:47s remains)
INFO - root - 2017-12-06 05:00:37.885792: step 8080, loss = 0.90, batch loss = 0.68 (31.7 examples/sec; 0.252 sec/batch; 22h:42m:45s remains)
INFO - root - 2017-12-06 05:00:40.184960: step 8090, loss = 0.90, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:20m:48s remains)
INFO - root - 2017-12-06 05:00:42.475018: step 8100, loss = 0.96, batch loss = 0.75 (35.0 examples/sec; 0.228 sec/batch; 20h:35m:03s remains)
2017-12-06 05:00:44.040901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25407451 -0.29128209 -0.39931878 -0.58933413 -0.79088986 -1.0493835 -1.3463732 -1.6069304 -1.7002583 -1.5459303 -1.2489415 -0.95047814 -0.68905795 -0.43107516 -0.33635679][-0.26602361 -0.2260541 -0.25292751 -0.44662061 -0.81929046 -1.2705317 -1.7719152 -2.300498 -2.5922132 -2.4688497 -2.0092473 -1.5654545 -1.1611303 -0.74145246 -0.38941559][-0.087696545 0.022110134 0.20474887 0.31155536 0.14309892 -0.39054605 -1.0973347 -1.9609665 -2.8231978 -2.9907227 -2.7286003 -2.1650543 -1.6420999 -1.1024313 -0.65153211][0.3196173 0.62155038 1.045404 1.407145 1.5190898 1.2337139 0.48573956 -0.60200191 -1.7739172 -2.6280606 -2.7743046 -2.311816 -1.7119476 -1.1393547 -0.64751339][0.89400446 1.4066495 1.99131 2.5939989 3.0417962 3.0540919 2.3807251 1.1692729 -0.11670394 -1.2060145 -1.6377921 -1.5885251 -1.4495962 -1.10341 -0.61724037][1.3444308 2.1046515 3.4198759 4.647151 5.296742 5.5015917 4.8832226 3.65624 2.0379891 0.53139895 -0.47895974 -0.74692172 -0.74291986 -0.5236544 -0.13662229][1.4895073 2.3707678 3.73757 5.3329477 7.1902127 7.8362193 7.4116039 6.1885362 4.5848007 2.9977951 1.8102548 0.93279982 0.26050273 0.013853297 0.0733306][1.5279361 2.2426062 3.2810702 4.6433058 6.2700658 7.6825457 8.491004 7.4777441 5.7387567 4.0582738 2.9187815 1.9687544 1.2990237 0.861904 0.64774191][1.3053977 2.026459 2.8488131 3.7397172 4.9407315 6.2118554 7.145463 7.1229105 6.4122205 4.6115475 3.3852556 2.2054245 1.445743 0.91633785 0.77925068][0.6191057 1.1039864 1.7432332 2.3236027 3.1297615 4.2241559 5.0775137 4.8977642 4.5679483 3.7347965 2.8666019 1.6052203 1.1092503 0.61901605 0.61313933][-0.1271461 0.12205445 0.36073568 0.77037424 1.1135834 1.7930866 2.4454682 2.7102687 2.625514 2.054199 1.5523055 0.8189463 0.35303465 0.21048844 0.1745638][-0.41891259 -0.48316979 -0.51666731 -0.4636746 -0.35825759 -0.013782054 0.25102469 0.3958182 0.41568792 0.34180102 0.086920626 -0.061457496 -0.25093186 -0.418458 -0.42079413][-0.57696718 -0.63048184 -0.68220806 -0.85233313 -0.86736012 -0.93729848 -1.0598685 -0.88883317 -0.85852307 -0.76834244 -0.64348233 -0.42712671 -0.50276124 -0.4065907 -0.40664595][-0.39393413 -0.55438972 -0.72673547 -0.9451195 -1.0779949 -1.2819269 -1.264123 -1.32081 -1.4053215 -1.174739 -0.89736068 -0.38126141 -0.32602727 -0.18823406 -0.24870935][-0.25566724 -0.39492244 -0.57851642 -0.77866322 -0.95492971 -1.2218348 -1.1827958 -1.1948987 -1.1122191 -1.0514817 -0.90396345 -0.54879475 -0.3786256 -0.18649635 -0.13028088]]...]
INFO - root - 2017-12-06 05:00:46.334534: step 8110, loss = 0.90, batch loss = 0.69 (34.2 examples/sec; 0.234 sec/batch; 21h:04m:01s remains)
INFO - root - 2017-12-06 05:00:48.663368: step 8120, loss = 0.89, batch loss = 0.68 (32.6 examples/sec; 0.246 sec/batch; 22h:08m:22s remains)
INFO - root - 2017-12-06 05:00:50.943835: step 8130, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 20h:58m:16s remains)
INFO - root - 2017-12-06 05:00:53.238502: step 8140, loss = 0.88, batch loss = 0.66 (35.8 examples/sec; 0.223 sec/batch; 20h:06m:37s remains)
INFO - root - 2017-12-06 05:00:55.497294: step 8150, loss = 0.87, batch loss = 0.66 (33.6 examples/sec; 0.238 sec/batch; 21h:28m:36s remains)
INFO - root - 2017-12-06 05:00:57.814262: step 8160, loss = 0.88, batch loss = 0.67 (35.4 examples/sec; 0.226 sec/batch; 20h:21m:09s remains)
INFO - root - 2017-12-06 05:01:00.076065: step 8170, loss = 0.87, batch loss = 0.66 (35.5 examples/sec; 0.226 sec/batch; 20h:19m:40s remains)
INFO - root - 2017-12-06 05:01:02.380352: step 8180, loss = 0.88, batch loss = 0.67 (34.2 examples/sec; 0.234 sec/batch; 21h:03m:55s remains)
INFO - root - 2017-12-06 05:01:04.714060: step 8190, loss = 0.92, batch loss = 0.71 (33.3 examples/sec; 0.241 sec/batch; 21h:40m:21s remains)
INFO - root - 2017-12-06 05:01:07.019069: step 8200, loss = 0.92, batch loss = 0.71 (35.0 examples/sec; 0.228 sec/batch; 20h:34m:58s remains)
2017-12-06 05:01:07.403098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2321078 -1.2138919 -1.263625 -1.3545312 -1.4937973 -1.6029526 -1.6617637 -1.5494193 -1.3700042 -1.2163562 -1.0874705 -0.97983104 -0.91513252 -0.92296088 -0.95853251][-0.78342217 -1.0191159 -1.2362812 -1.3727111 -1.4234995 -1.5099345 -1.7534701 -1.8862091 -1.9556729 -1.8700395 -1.712719 -1.5231264 -1.3207134 -1.1640041 -1.057184][-0.19331971 -0.28016704 -0.4522894 -0.79376191 -1.1244242 -1.2313796 -1.3022727 -1.4943429 -1.6337179 -1.6829445 -1.7460694 -1.6340939 -1.4536124 -1.2580054 -1.10834][0.1525985 0.34543273 0.36595306 0.15441567 -0.23914039 -0.68067229 -1.0735548 -1.3328519 -1.4799942 -1.5626452 -1.5876675 -1.558679 -1.5071164 -1.3703132 -1.2089242][0.54695219 0.85053551 1.0919312 1.290611 1.1696749 0.862846 0.43764523 -0.17106898 -0.70571047 -0.95792514 -1.050127 -1.0661273 -1.0178906 -0.98224342 -0.96522045][0.71998686 1.0794669 1.5096767 2.0220037 2.3342423 2.4670234 2.3788271 2.0794928 1.5725768 0.84505528 0.23469305 -0.0932102 -0.33264676 -0.46294346 -0.43281811][0.71238142 0.99394673 1.3202212 1.9117732 2.495661 2.9982097 3.1811337 3.1121609 2.8610482 2.4091966 1.8425083 1.1106757 0.42988637 0.087299608 0.009621568][0.37556615 0.81320548 1.3124316 1.9116904 2.4051452 2.9064736 3.2621481 3.4672029 3.3677809 3.085393 2.666688 2.0238552 1.2999067 0.56718791 0.16476926][-0.48389566 -0.060992453 0.48510447 1.1973041 1.8670419 2.4886613 2.9704549 3.2734637 3.3165252 3.1079717 2.7387941 2.3747051 1.9387379 1.3847288 0.875957][-1.3166635 -1.1397862 -0.82553142 -0.34629959 0.36155903 1.0263979 1.7332585 2.2919924 2.6776731 2.576158 2.2576022 1.8044362 1.4508828 1.1526618 1.0167402][-1.7876966 -1.809958 -1.759672 -1.6474971 -1.3524361 -0.98149067 -0.66615427 -0.0609217 0.71896547 1.262136 1.594698 1.3510412 0.99091 0.61077005 0.54847509][-1.5832523 -1.8001715 -2.0402982 -2.2233634 -2.2462974 -2.2008362 -2.1315341 -1.8721948 -1.4419211 -0.87795466 -0.3924349 0.0926292 0.44396922 0.38136521 0.33747247][-1.5531474 -1.5970833 -1.8079071 -2.1836307 -2.4081478 -2.7145655 -2.9457569 -2.8045771 -2.644937 -2.2000268 -1.845777 -1.3980994 -0.74895424 -0.48003373 -0.12374236][-1.0096369 -1.1830109 -1.3979832 -1.7724739 -2.1212637 -2.4103887 -2.6697681 -2.7957714 -2.8919663 -2.8354979 -2.6995933 -2.1678233 -1.6036626 -1.1440674 -0.69524378][-0.2222923 -0.42436129 -0.79971516 -1.2193333 -1.5130388 -1.7593396 -1.9888866 -2.3176255 -2.4844646 -2.3465433 -2.1645284 -1.8027651 -1.5200338 -1.3279921 -1.0617712]]...]
INFO - root - 2017-12-06 05:01:09.754192: step 8210, loss = 0.89, batch loss = 0.68 (34.4 examples/sec; 0.232 sec/batch; 20h:55m:42s remains)
INFO - root - 2017-12-06 05:01:12.030554: step 8220, loss = 0.87, batch loss = 0.66 (35.5 examples/sec; 0.225 sec/batch; 20h:17m:35s remains)
INFO - root - 2017-12-06 05:01:14.309809: step 8230, loss = 0.88, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 20h:40m:59s remains)
INFO - root - 2017-12-06 05:01:16.617086: step 8240, loss = 0.86, batch loss = 0.65 (35.4 examples/sec; 0.226 sec/batch; 20h:19m:36s remains)
INFO - root - 2017-12-06 05:01:18.943221: step 8250, loss = 0.90, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 21h:13m:49s remains)
INFO - root - 2017-12-06 05:01:21.293169: step 8260, loss = 0.82, batch loss = 0.61 (34.2 examples/sec; 0.234 sec/batch; 21h:04m:45s remains)
INFO - root - 2017-12-06 05:01:23.596948: step 8270, loss = 0.84, batch loss = 0.62 (33.7 examples/sec; 0.237 sec/batch; 21h:21m:55s remains)
INFO - root - 2017-12-06 05:01:25.946651: step 8280, loss = 0.91, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 20h:38m:54s remains)
INFO - root - 2017-12-06 05:01:28.246401: step 8290, loss = 0.88, batch loss = 0.67 (34.2 examples/sec; 0.234 sec/batch; 21h:04m:32s remains)
INFO - root - 2017-12-06 05:01:30.574177: step 8300, loss = 0.89, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:11m:44s remains)
2017-12-06 05:01:30.989982: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20282608 0.18539932 0.19918814 0.22906891 0.25822571 0.27525717 0.28531927 0.30426618 0.30873957 0.28991538 0.25765422 0.2286711 0.21227235 0.17164236 0.13035157][0.22324795 0.1895608 0.18406871 0.20048228 0.22447753 0.25678197 0.29357952 0.32544145 0.35078949 0.35857061 0.34105068 0.30470571 0.27086785 0.22678751 0.15200683][0.15291882 0.12201334 0.09564928 0.078682579 0.089453228 0.12875372 0.1829538 0.24230003 0.27723375 0.28660688 0.27409595 0.25072622 0.22541961 0.18476754 0.14021897][0.10397416 0.077932037 0.0675543 0.045962863 0.045283355 0.078125261 0.14869881 0.22625643 0.27958483 0.29504204 0.29112965 0.26136008 0.23218155 0.20774493 0.14942408][0.082582735 0.051101588 0.023133941 -0.0019845888 -0.010994464 0.024537742 0.10987536 0.20707095 0.27727661 0.31231824 0.31725132 0.28736651 0.23889375 0.17938012 0.10444745][0.091583155 0.022416838 -0.01829692 -0.049239412 -0.054600142 -0.0051655695 0.085647605 0.20035705 0.29925779 0.35452527 0.36700255 0.34958959 0.31808919 0.23412564 0.15549955][0.090432338 0.00097680092 -0.024610948 -0.04505372 -0.022548508 0.040394418 0.14392132 0.25683671 0.34943935 0.40591019 0.42609876 0.38899651 0.33807927 0.24892333 0.11737225][0.12851921 0.041138627 -0.00492198 -0.0013482049 0.046276949 0.13161635 0.23356977 0.32442307 0.39357623 0.43151897 0.43833733 0.39912659 0.34362191 0.21424004 0.10168969][0.16137043 0.07800018 0.02919469 0.022847056 0.054004483 0.13752174 0.224354 0.29877797 0.35294592 0.37488586 0.37419164 0.34361881 0.3365863 0.25489575 0.14036527][0.20988902 0.13269997 0.095534883 0.077097811 0.10128268 0.15143302 0.21681589 0.27540183 0.29300034 0.28865695 0.26437318 0.23092443 0.22568089 0.19926864 0.099071585][0.2055372 0.16206443 0.1506896 0.14326343 0.16578332 0.20519871 0.25403404 0.27835375 0.26401433 0.21342203 0.15432927 0.10820403 0.084435977 0.088728048 0.065346219][0.16449019 0.15429997 0.15621886 0.16619158 0.18794551 0.2209444 0.24614537 0.24568719 0.21632382 0.1532594 0.082769133 0.032672726 0.011649661 0.025159799 -0.0022937655][0.1573298 0.15976679 0.17544106 0.18896744 0.1974088 0.20084003 0.19109291 0.17651653 0.15137982 0.10400867 0.040288232 0.0022238418 -0.035092402 -0.0061462373 -0.0026004538][0.14321211 0.1582073 0.17757699 0.19468686 0.21538973 0.20501518 0.18351969 0.15059945 0.10025459 0.0479395 0.0066755638 -0.0047687069 -0.0052266121 0.020427942 0.063200988][0.18035406 0.190718 0.21589211 0.22482586 0.21041337 0.20061949 0.18135914 0.15500244 0.12609568 0.11693641 0.1044383 0.080986015 0.084655605 0.12823749 0.15550953]]...]
INFO - root - 2017-12-06 05:01:33.269337: step 8310, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 20h:47m:40s remains)
INFO - root - 2017-12-06 05:01:35.546423: step 8320, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:49s remains)
INFO - root - 2017-12-06 05:01:37.853946: step 8330, loss = 0.86, batch loss = 0.65 (34.2 examples/sec; 0.234 sec/batch; 21h:04m:01s remains)
INFO - root - 2017-12-06 05:01:40.172729: step 8340, loss = 0.88, batch loss = 0.67 (35.3 examples/sec; 0.227 sec/batch; 20h:25m:14s remains)
INFO - root - 2017-12-06 05:01:42.449626: step 8350, loss = 0.85, batch loss = 0.64 (35.5 examples/sec; 0.225 sec/batch; 20h:18m:14s remains)
INFO - root - 2017-12-06 05:01:44.758877: step 8360, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.233 sec/batch; 20h:57m:57s remains)
INFO - root - 2017-12-06 05:01:47.053865: step 8370, loss = 0.86, batch loss = 0.65 (35.0 examples/sec; 0.229 sec/batch; 20h:36m:13s remains)
INFO - root - 2017-12-06 05:01:49.331464: step 8380, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 20h:41m:21s remains)
INFO - root - 2017-12-06 05:01:51.628911: step 8390, loss = 0.89, batch loss = 0.67 (35.9 examples/sec; 0.223 sec/batch; 20h:04m:31s remains)
INFO - root - 2017-12-06 05:01:53.945386: step 8400, loss = 0.89, batch loss = 0.67 (33.1 examples/sec; 0.242 sec/batch; 21h:47m:02s remains)
2017-12-06 05:01:54.290299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.377949 0.37925225 0.38339919 0.48678529 0.45956641 0.33918178 0.276923 0.1559777 0.070068881 -0.11569028 -0.32072273 -0.34267253 -0.42515653 -0.46102566 -0.42747903][0.17968391 0.14735962 0.21619703 0.19780539 0.13996966 0.076008081 0.0043933094 -0.074389763 -0.19253492 -0.35193884 -0.52288526 -0.74865425 -0.89860624 -0.96836787 -1.0360628][-0.011407569 -0.020513035 -0.0015071779 -0.079012975 -0.17852591 -0.23392303 -0.33439273 -0.41046005 -0.48104668 -0.61074615 -0.75002652 -0.93718159 -1.1192552 -1.1890402 -1.2321186][0.09905161 0.010987945 -0.053147208 -0.15523803 -0.26268882 -0.34755421 -0.3912071 -0.40689307 -0.39486563 -0.42716664 -0.52820444 -0.68018979 -0.92048943 -1.0080274 -1.0699351][0.13254097 0.063382164 -0.005537279 -0.14091215 -0.31967151 -0.38156879 -0.39941967 -0.35916156 -0.26622254 -0.21036094 -0.186815 -0.3149268 -0.48773253 -0.69476587 -0.80647838][-0.073021784 -0.0729343 -0.075419582 -0.10415943 -0.11848553 -0.12360919 -0.14900976 -0.10827917 -0.064111151 -0.0066475868 0.038052313 0.02222275 -0.10162059 -0.23948169 -0.396394][-0.072018087 -0.068805277 -0.072852314 -0.087375723 -0.11755961 -0.17551908 -0.16118327 -0.08227504 0.059806347 0.24544726 0.41706973 0.46533436 0.42599291 0.27468461 0.11267981][-0.11890425 -0.13066767 -0.14301504 -0.16692132 -0.19170919 -0.19749084 -0.21401431 -0.21000643 -0.1068026 0.0051218346 0.17807411 0.36218637 0.49582213 0.52691859 0.44982988][-0.059519462 -0.045143791 -0.05407159 -0.085339986 -0.10640852 -0.1363042 -0.16425684 -0.1653755 -0.11351541 0.01618015 0.1598991 0.37048745 0.54701209 0.65792263 0.64229572][-0.046283055 -0.031551991 -0.028261185 -0.061560303 -0.11802499 -0.21772638 -0.29229817 -0.32163182 -0.22870244 -0.10391697 0.081970155 0.30343848 0.4579407 0.55514264 0.6315856][-0.16675773 -0.16444755 -0.15939 -0.14830455 -0.1534352 -0.13373607 -0.12325358 -0.10873704 -0.087815188 -0.0083460659 0.096864104 0.1901006 0.28099573 0.35910821 0.38910246][-0.10214129 -0.10141116 -0.097035296 -0.10355647 -0.11222553 -0.11129111 -0.09913262 -0.111038 -0.089262381 -0.011210881 0.055485934 0.17350547 0.25902236 0.27123082 0.23632671][-0.10630952 -0.12615623 -0.12723167 -0.1197776 -0.10352087 -0.070959814 -0.028954722 0.0019212738 0.062624574 0.082945958 0.11966327 0.16700737 0.18625762 0.19407289 0.184117][-0.10545114 -0.13207488 -0.13757312 -0.086989351 -0.029434651 0.030778341 0.094404936 0.14685138 0.16721742 0.23167647 0.25407839 0.21125181 0.14829825 0.089298069 0.019514784][0.12698025 0.11906242 0.1517746 0.20419185 0.27710271 0.35484117 0.43404394 0.46322107 0.46902907 0.42492616 0.33769614 0.22306265 0.080370605 -0.026839726 -0.12105742]]...]
INFO - root - 2017-12-06 05:01:56.607706: step 8410, loss = 0.83, batch loss = 0.61 (36.5 examples/sec; 0.219 sec/batch; 19h:43m:05s remains)
INFO - root - 2017-12-06 05:01:58.911117: step 8420, loss = 0.89, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 21h:28m:41s remains)
INFO - root - 2017-12-06 05:02:01.233520: step 8430, loss = 0.87, batch loss = 0.66 (33.7 examples/sec; 0.237 sec/batch; 21h:21m:51s remains)
INFO - root - 2017-12-06 05:02:03.527816: step 8440, loss = 0.91, batch loss = 0.70 (35.0 examples/sec; 0.228 sec/batch; 20h:33m:48s remains)
INFO - root - 2017-12-06 05:02:05.847392: step 8450, loss = 0.86, batch loss = 0.64 (35.5 examples/sec; 0.226 sec/batch; 20h:18m:17s remains)
INFO - root - 2017-12-06 05:02:08.149973: step 8460, loss = 0.86, batch loss = 0.64 (34.1 examples/sec; 0.234 sec/batch; 21h:06m:23s remains)
INFO - root - 2017-12-06 05:02:10.457184: step 8470, loss = 0.83, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 20h:36m:35s remains)
INFO - root - 2017-12-06 05:02:12.746263: step 8480, loss = 0.87, batch loss = 0.66 (34.3 examples/sec; 0.233 sec/batch; 20h:59m:16s remains)
INFO - root - 2017-12-06 05:02:15.040345: step 8490, loss = 0.88, batch loss = 0.66 (35.6 examples/sec; 0.225 sec/batch; 20h:14m:28s remains)
INFO - root - 2017-12-06 05:02:17.370728: step 8500, loss = 0.77, batch loss = 0.56 (33.1 examples/sec; 0.242 sec/batch; 21h:44m:16s remains)
2017-12-06 05:02:18.007250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10267002 -0.017880179 0.12610352 0.24645737 0.32648081 0.38569668 0.44143346 0.41763183 0.35248289 0.19990277 0.045598738 -0.10840796 -0.26579261 -0.39004362 -0.41535535][-0.18812457 -0.056248356 0.15754265 0.32966918 0.42386082 0.42831811 0.43301865 0.43166283 0.33918726 0.14496046 -0.023862489 -0.18711853 -0.33448762 -0.4356752 -0.53401512][-0.12540132 -0.065378867 0.15805906 0.35777786 0.51498818 0.4823505 0.416739 0.33957967 0.26411274 0.13165677 0.024032533 -0.064304337 -0.19731377 -0.33457991 -0.42390272][0.13479653 0.1350393 0.24865225 0.38074008 0.50714308 0.57248253 0.54829466 0.48207095 0.38250217 0.25973579 0.1134652 -0.028919138 -0.1977717 -0.37620434 -0.50207865][0.24425197 0.19711757 0.19690329 0.26586214 0.35415658 0.45128611 0.52227783 0.54565781 0.51060694 0.40574136 0.27818447 0.14999342 -0.038925394 -0.22926167 -0.4018352][0.23533425 0.1658411 0.15814045 0.18402314 0.25024796 0.333446 0.380192 0.43582335 0.48832676 0.49009785 0.44129387 0.34131986 0.23870441 -0.00017959625 -0.16498445][0.32108694 0.24049458 0.2010206 0.19608164 0.2254633 0.2750386 0.33898908 0.42445359 0.49836704 0.53690356 0.54882371 0.47983566 0.40580672 0.23578912 0.036363848][0.32482272 0.21028849 0.15178892 0.14672989 0.18339229 0.26667014 0.37230593 0.44308832 0.5058431 0.54331988 0.55501211 0.51267886 0.47092214 0.37855667 0.23501807][0.25350955 0.10824447 0.034753524 0.027322769 0.058639757 0.1181098 0.19000864 0.26526046 0.34067249 0.39628813 0.44226143 0.45419952 0.42321149 0.40598771 0.30273318][0.30481634 0.1592575 0.057544492 0.022491954 0.010895133 0.015854537 0.047080941 0.10518809 0.1658608 0.21608481 0.26465517 0.27130723 0.26655385 0.24263936 0.1701391][0.1521464 0.079535566 0.0007558763 -0.015146822 -0.0018855184 0.016221039 0.042261355 0.070734911 0.092442937 0.12087639 0.13851157 0.13619262 0.15034598 0.12132332 0.0579933][-0.014553346 -0.052545544 -0.077572025 -0.070531905 -0.045058377 -0.012066931 0.027752079 0.057804964 0.074744351 0.088357352 0.095068388 0.10058395 0.0971143 0.096452363 0.049483843][-0.017583162 -0.043121479 -0.063212909 -0.056995656 -0.031615343 0.001806058 0.040286861 0.071376123 0.088978864 0.094463639 0.090257592 0.079906218 0.086702637 0.10363144 0.097294666][-0.017839387 -0.040284559 -0.053164124 -0.049600895 -0.032212295 -0.0059013739 0.021761492 0.047342397 0.066244088 0.077192552 0.078761391 0.067892559 0.050239272 0.046094321 0.027033038][-0.016340762 -0.026972435 -0.031731389 -0.02915813 -0.016956724 -0.00031387061 0.019366309 0.034524791 0.047244452 0.056168325 0.059375457 0.05675108 0.04847277 0.033145875 0.01515685]]...]
INFO - root - 2017-12-06 05:02:20.318518: step 8510, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.232 sec/batch; 20h:54m:01s remains)
INFO - root - 2017-12-06 05:02:22.625809: step 8520, loss = 0.89, batch loss = 0.68 (35.5 examples/sec; 0.226 sec/batch; 20h:18m:21s remains)
INFO - root - 2017-12-06 05:02:24.939376: step 8530, loss = 0.89, batch loss = 0.67 (34.3 examples/sec; 0.234 sec/batch; 21h:00m:54s remains)
INFO - root - 2017-12-06 05:02:27.245338: step 8540, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 20h:47m:23s remains)
INFO - root - 2017-12-06 05:02:29.534291: step 8550, loss = 0.85, batch loss = 0.63 (35.2 examples/sec; 0.228 sec/batch; 20h:28m:46s remains)
INFO - root - 2017-12-06 05:02:31.835910: step 8560, loss = 0.84, batch loss = 0.63 (35.2 examples/sec; 0.228 sec/batch; 20h:28m:21s remains)
INFO - root - 2017-12-06 05:02:34.160620: step 8570, loss = 0.85, batch loss = 0.63 (34.5 examples/sec; 0.232 sec/batch; 20h:50m:38s remains)
INFO - root - 2017-12-06 05:02:36.470321: step 8580, loss = 0.83, batch loss = 0.62 (33.4 examples/sec; 0.239 sec/batch; 21h:32m:40s remains)
INFO - root - 2017-12-06 05:02:38.770228: step 8590, loss = 0.83, batch loss = 0.62 (33.0 examples/sec; 0.242 sec/batch; 21h:48m:28s remains)
INFO - root - 2017-12-06 05:02:41.132444: step 8600, loss = 0.85, batch loss = 0.63 (33.5 examples/sec; 0.239 sec/batch; 21h:30m:19s remains)
2017-12-06 05:02:41.485003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4906592 -2.4013309 -2.3163185 -2.1699429 -1.9308083 -1.782932 -1.6593978 -1.5215523 -1.3643805 -1.251848 -1.159696 -1.0597079 -1.0702943 -1.0811788 -1.0181187][-2.0969574 -2.1527367 -2.1576962 -2.161485 -2.0880258 -2.0015466 -1.8565656 -1.6675292 -1.5750074 -1.5116527 -1.433242 -1.3812455 -1.3440683 -1.3574837 -1.3268166][-1.0407866 -1.0611277 -1.1115409 -1.1520313 -1.2651254 -1.3611859 -1.4173542 -1.4768151 -1.4400952 -1.3743399 -1.395291 -1.3751187 -1.4083722 -1.4223942 -1.3566111][0.34647071 0.46215281 0.44323316 0.34424433 0.28387237 0.20855626 0.067054026 -0.03501258 -0.20185281 -0.43442249 -0.61911017 -0.87619776 -1.0987204 -1.2281859 -1.3406639][1.5630996 1.6077955 1.6423219 1.5090504 1.3739095 1.276634 1.1759602 1.0760298 0.88744247 0.604079 0.25398931 -0.15578148 -0.45004 -0.664507 -0.89305311][1.8149295 2.1286917 2.1259608 1.8766358 1.6134299 1.3429599 1.0633882 0.89881343 0.81690985 0.7338714 0.64522427 0.53387856 0.37817627 0.082642131 -0.19451158][1.8331112 2.1991122 2.3622191 2.4838305 2.2600131 1.8810263 1.4473157 1.0070971 0.70055205 0.42330411 0.2997047 0.20297912 0.14769441 0.2024377 0.19598672][2.5750964 2.5694582 2.5456092 2.4617069 2.3405833 2.3580596 2.2489483 1.9491704 1.4761108 1.0123775 0.57705104 0.3442032 0.24344307 0.29556903 0.35871473][3.7811217 3.5844157 3.4820361 3.2099171 2.8216248 2.4626555 2.2100451 2.0165305 1.6961659 1.2559241 0.89722705 0.72081363 0.56629592 0.51947647 0.52564979][3.810416 3.7221136 3.6079898 3.2626822 2.9232934 2.513432 2.1778021 1.9330084 1.7300094 1.7881904 1.7581205 1.4638692 1.1085486 0.89823949 0.76510137][3.6269705 3.245234 2.8386402 2.6501367 2.429882 2.0305655 1.8017193 1.5598032 1.3802742 1.2611121 1.3598013 1.6051693 1.8136475 1.6951995 1.4291245][3.8018131 3.362144 2.7775354 2.1953855 1.5794297 1.0412732 0.60297036 0.15649444 -0.15394026 -0.36124203 -0.42655739 -0.18637258 0.075630374 0.49678519 0.81589329][3.4820297 3.255127 2.9206755 2.4377978 2.0349207 1.1924475 0.46864149 -0.10705812 -0.5074237 -0.73787242 -0.97618 -1.133098 -1.1196522 -0.87851483 -0.62054479][2.3374739 2.4520693 2.5154181 1.9960129 1.4302788 0.68501055 -0.3304556 -0.91440123 -0.87490004 -0.82561028 -0.93869025 -1.2442049 -1.3427975 -1.2590749 -1.2402816][-0.23981339 0.1247251 0.31220534 0.082547642 0.11361187 -0.2320143 -0.65664053 -0.91509438 -1.1975359 -1.0968988 -1.1246526 -1.3059509 -1.2713494 -1.3696271 -1.4372389]]...]
INFO - root - 2017-12-06 05:02:43.762632: step 8610, loss = 0.84, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 20h:29m:54s remains)
INFO - root - 2017-12-06 05:02:46.088370: step 8620, loss = 0.86, batch loss = 0.64 (34.7 examples/sec; 0.231 sec/batch; 20h:44m:26s remains)
INFO - root - 2017-12-06 05:02:48.377391: step 8630, loss = 0.82, batch loss = 0.61 (35.9 examples/sec; 0.223 sec/batch; 20h:04m:18s remains)
INFO - root - 2017-12-06 05:02:50.672431: step 8640, loss = 0.79, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:41m:17s remains)
INFO - root - 2017-12-06 05:02:53.013602: step 8650, loss = 0.82, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 20h:47m:31s remains)
INFO - root - 2017-12-06 05:02:55.321787: step 8660, loss = 0.87, batch loss = 0.65 (34.1 examples/sec; 0.235 sec/batch; 21h:06m:25s remains)
INFO - root - 2017-12-06 05:02:57.640742: step 8670, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.232 sec/batch; 20h:53m:44s remains)
INFO - root - 2017-12-06 05:02:59.951744: step 8680, loss = 0.87, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 20h:29m:37s remains)
INFO - root - 2017-12-06 05:03:02.268742: step 8690, loss = 0.89, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 20h:31m:00s remains)
INFO - root - 2017-12-06 05:03:04.586499: step 8700, loss = 0.79, batch loss = 0.57 (34.2 examples/sec; 0.234 sec/batch; 21h:03m:51s remains)
2017-12-06 05:03:05.856732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14313439 -0.15809472 -0.17000315 -0.17793241 -0.18034555 -0.18156433 -0.14960364 -0.12399061 -0.099920191 -0.082800068 -0.065545991 -0.049060922 -0.064022742 -0.089944482 -0.11669681][-0.082574062 -0.083430916 -0.10669756 -0.14661816 -0.16500828 -0.1619575 -0.14470747 -0.11836539 -0.095620684 -0.080542833 -0.052145902 -0.027691215 -0.025029551 -0.042649183 -0.0749297][0.037312992 0.021625578 -0.046259757 -0.12190057 -0.15654078 -0.14154187 -0.12064892 -0.087662913 -0.05477912 -0.04623035 -0.040525228 -0.0051792637 0.0062168017 -0.015013665 -0.079545423][0.32462955 0.20467918 0.021357834 -0.13813555 -0.20708193 -0.19388996 -0.15967995 -0.088241056 -0.030982081 0.01831273 0.017814271 0.047435969 0.057047844 0.0089046881 -0.098697335][0.63209397 0.49168289 0.20847021 -0.047405049 -0.21422346 -0.23474015 -0.18448257 -0.096885182 -0.017355941 0.062180355 0.10278986 0.12321909 0.10015996 0.038058624 -0.083181217][0.77986574 0.59923577 0.33163434 0.084335119 -0.0835923 -0.13739176 -0.12888081 -0.085374393 -0.012172163 0.032257549 0.086825073 0.1396151 0.15566474 0.091423184 0.012687042][0.78805858 0.66182578 0.48677832 0.35500568 0.27800065 0.25942981 0.26977575 0.29290342 0.29663152 0.27893078 0.26930225 0.25199479 0.26626968 0.21347426 0.1535493][0.59132367 0.50696743 0.45744056 0.44961619 0.49823475 0.59176236 0.70378017 0.79800844 0.83602041 0.7709285 0.6776529 0.55529654 0.4682675 0.35176569 0.2601878][0.37348539 0.28556043 0.28307962 0.3370716 0.43401766 0.56285143 0.72055376 0.88119382 0.99662071 1.0028499 0.93554312 0.81991071 0.69703531 0.50684452 0.33558983][0.20546772 0.17388444 0.15900441 0.1851116 0.27609754 0.36656958 0.42696732 0.532683 0.64806753 0.70332485 0.68967044 0.64278567 0.59350085 0.47355312 0.34753084][0.054019079 0.048390836 0.03097266 0.062260404 0.11276251 0.14481014 0.1152073 0.10473883 0.081996709 0.053826958 0.050512642 0.037522011 0.05519639 0.10231672 0.11442277][-0.13193238 -0.10680597 -0.11425177 -0.099448994 -0.030130796 -0.021867197 -0.059149392 -0.11326586 -0.19571814 -0.31737512 -0.39989674 -0.44266117 -0.40214503 -0.27917457 -0.18124124][-0.36372942 -0.29959539 -0.2534188 -0.21164607 -0.129958 -0.1195238 -0.10613497 -0.15482271 -0.24498123 -0.38129318 -0.477198 -0.5730617 -0.54711169 -0.41196644 -0.2797758][-0.54773223 -0.49774981 -0.42164063 -0.356382 -0.28655234 -0.21115324 -0.13788128 -0.17044017 -0.24693432 -0.3284421 -0.38985163 -0.46086097 -0.42441833 -0.31804842 -0.20951822][-0.5511232 -0.54443371 -0.51639056 -0.47475272 -0.3662129 -0.28360495 -0.17775656 -0.17205244 -0.18205506 -0.20089847 -0.23188044 -0.23566659 -0.19806764 -0.13091041 -0.073378675]]...]
INFO - root - 2017-12-06 05:03:08.142590: step 8710, loss = 0.79, batch loss = 0.58 (34.4 examples/sec; 0.232 sec/batch; 20h:53m:22s remains)
INFO - root - 2017-12-06 05:03:10.464785: step 8720, loss = 0.88, batch loss = 0.67 (34.4 examples/sec; 0.233 sec/batch; 20h:55m:26s remains)
INFO - root - 2017-12-06 05:03:12.789920: step 8730, loss = 0.86, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 20h:24m:36s remains)
INFO - root - 2017-12-06 05:03:15.095561: step 8740, loss = 0.85, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:23m:07s remains)
INFO - root - 2017-12-06 05:03:17.417225: step 8750, loss = 0.92, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 20h:36m:56s remains)
INFO - root - 2017-12-06 05:03:19.721360: step 8760, loss = 0.81, batch loss = 0.60 (35.1 examples/sec; 0.228 sec/batch; 20h:30m:09s remains)
INFO - root - 2017-12-06 05:03:22.005868: step 8770, loss = 0.82, batch loss = 0.60 (35.3 examples/sec; 0.227 sec/batch; 20h:22m:59s remains)
INFO - root - 2017-12-06 05:03:24.353645: step 8780, loss = 0.83, batch loss = 0.62 (35.0 examples/sec; 0.228 sec/batch; 20h:32m:07s remains)
INFO - root - 2017-12-06 05:03:26.659515: step 8790, loss = 0.82, batch loss = 0.61 (34.3 examples/sec; 0.234 sec/batch; 21h:00m:01s remains)
INFO - root - 2017-12-06 05:03:28.930865: step 8800, loss = 0.84, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 20h:18m:04s remains)
2017-12-06 05:03:29.978099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060389124 -0.0552078 -0.041077711 -0.033063389 -0.035200987 -0.040530577 -0.04839455 -0.069609061 -0.080075428 -0.094894648 -0.11230869 -0.10880812 -0.1176312 -0.135181 -0.14549103][-0.061530918 -0.052772168 -0.033116747 -0.016730472 -0.021203607 -0.035890691 -0.047270607 -0.06984853 -0.070732668 -0.083805509 -0.096215785 -0.11053813 -0.12270486 -0.14298195 -0.15807317][-0.05946786 -0.047666453 -0.024889689 -0.007666558 -0.012087665 -0.029273707 -0.047196988 -0.06598679 -0.059436686 -0.061410371 -0.063991748 -0.086419947 -0.097589046 -0.12172063 -0.15466863][-0.059618384 -0.046278428 -0.022717923 -0.011917867 -0.01293125 -0.025735527 -0.042466067 -0.057033282 -0.043495931 -0.034681048 -0.028688308 -0.041601833 -0.052956108 -0.078354746 -0.099731751][-0.059196681 -0.048409894 -0.034315776 -0.030017536 -0.025963146 -0.032565791 -0.035221305 -0.038250171 -0.017752655 0.0065142363 0.013857707 0.012944788 0.0061664134 -0.013768911 -0.025403947][-0.057642929 -0.05070081 -0.043948922 -0.04414228 -0.044116557 -0.039133489 -0.020547636 -0.00094044954 0.025727466 0.047638595 0.042761952 0.034923159 0.027414031 0.0065115616 -0.013917178][-0.057598893 -0.052334163 -0.048509907 -0.05030451 -0.050043151 -0.035310853 -0.0068921521 0.034436524 0.0705304 0.085683376 0.073162854 0.030537702 0.0013058484 -0.010079354 -0.031642679][-0.0691077 -0.061160773 -0.059742786 -0.061580107 -0.062847249 -0.029960826 0.021513857 0.067085937 0.089465618 0.087103516 0.072598264 0.039801106 -0.01973407 -0.028311249 -0.058034681][-0.082280226 -0.075314917 -0.076769851 -0.073601432 -0.070725568 -0.02840301 0.025030889 0.087082252 0.11668023 0.0970995 0.057090312 0.020414315 -0.046657316 -0.065685786 -0.12299152][-0.074150547 -0.078789085 -0.082607232 -0.074466586 -0.0666702 -0.02362749 0.025579534 0.08525978 0.096590549 0.077987671 0.056467503 0.000932917 -0.077950656 -0.1556952 -0.24813804][-0.040397361 -0.052711379 -0.054240778 -0.05668629 -0.047155552 -0.0079769269 0.033008985 0.079503924 0.0914685 0.054069757 0.009007141 -0.053994387 -0.12071995 -0.20668939 -0.34300977][-0.019080281 -0.016473636 -0.0071325377 -0.017362162 -0.0091895759 0.014797002 0.037091523 0.059572637 0.058817014 0.030453198 0.0063657984 -0.089722827 -0.19217962 -0.25989598 -0.36936933][0.0015763417 0.0085506067 0.02064877 0.016373247 0.021781735 0.026882097 0.028427757 0.028073147 0.0052180588 -0.011671238 -0.043588839 -0.12218555 -0.19847901 -0.28068823 -0.38285565][0.01023785 0.023501292 0.028940387 0.022239022 0.016947649 0.0065814629 -0.015831791 -0.045970775 -0.088245168 -0.098989673 -0.13414735 -0.18689179 -0.24845678 -0.30008358 -0.34949166][0.011033706 0.026952058 0.033419691 0.032596469 0.021881238 -0.0027067289 -0.049075216 -0.096585587 -0.14550139 -0.1492649 -0.17186332 -0.21763176 -0.26863027 -0.31735229 -0.36817545]]...]
INFO - root - 2017-12-06 05:03:32.287399: step 8810, loss = 0.83, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:23s remains)
INFO - root - 2017-12-06 05:03:34.590020: step 8820, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:28m:30s remains)
INFO - root - 2017-12-06 05:03:36.877775: step 8830, loss = 0.87, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:38s remains)
INFO - root - 2017-12-06 05:03:39.163315: step 8840, loss = 0.85, batch loss = 0.64 (36.9 examples/sec; 0.217 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-06 05:03:41.461118: step 8850, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:37m:25s remains)
INFO - root - 2017-12-06 05:03:43.751838: step 8860, loss = 0.90, batch loss = 0.68 (35.6 examples/sec; 0.225 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-06 05:03:46.060432: step 8870, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:49s remains)
INFO - root - 2017-12-06 05:03:48.385823: step 8880, loss = 0.84, batch loss = 0.63 (34.7 examples/sec; 0.230 sec/batch; 20h:42m:04s remains)
INFO - root - 2017-12-06 05:03:50.711657: step 8890, loss = 0.83, batch loss = 0.62 (33.9 examples/sec; 0.236 sec/batch; 21h:14m:05s remains)
INFO - root - 2017-12-06 05:03:53.382847: step 8900, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.231 sec/batch; 20h:44m:11s remains)
2017-12-06 05:03:56.837444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8251176 -2.9731164 -3.0402761 -3.1652279 -3.1523521 -3.180836 -2.941329 -2.6144536 -2.3835018 -2.0400317 -1.9467577 -2.1400487 -2.5150697 -3.0130382 -3.0176377][-2.1308811 -2.2118354 -2.3112504 -2.1551785 -2.193898 -2.4984155 -2.4589782 -2.1332803 -1.9691095 -1.7291963 -1.8085247 -1.9921246 -2.1835434 -2.3178864 -2.1115246][-2.2916629 -2.2080662 -2.0995445 -1.9804665 -2.0093904 -2.081918 -2.3236279 -2.3371921 -2.4180284 -2.1311026 -2.3669629 -2.4338248 -2.3432088 -2.2924912 -2.0551331][-2.7993524 -2.7837529 -2.3932347 -2.225616 -1.9997416 -2.2666581 -2.402045 -2.3283606 -2.4762666 -2.3848495 -2.6226876 -2.5455756 -2.3207808 -2.0086467 -1.6434221][-2.4245028 -2.5021379 -2.2529252 -1.9694157 -1.8091697 -1.7798872 -1.5692474 -1.7913724 -1.9176103 -1.8120902 -2.0649776 -2.0907345 -1.9974023 -1.8638881 -1.5821877][-1.6624273 -1.958131 -2.2990589 -2.3456514 -2.2177978 -1.8144178 -1.2355955 -0.81863648 -0.56134015 -0.64927024 -0.93252593 -1.1207792 -1.3795104 -1.3710333 -1.5332609][-1.1264615 -1.3739188 -1.6841718 -2.1147418 -2.2047651 -1.7724683 -1.2870328 -0.69737816 -0.41470918 -0.3168087 -0.32781398 -0.28266326 -0.41648188 -0.55390185 -0.74302465][-1.1285281 -1.0006354 -1.1988382 -1.6468114 -1.7544998 -1.7387074 -1.4691952 -1.095408 -0.83775538 -0.76807249 -0.88445538 -0.75463206 -0.57265347 -0.49662554 -0.47797519][-1.87278 -1.4631448 -1.5863682 -1.7775018 -1.5118197 -1.4581974 -1.0957736 -0.73758888 -0.57357341 -0.29780269 -0.20813257 -0.32935607 -0.36338177 -0.43443245 -0.34920576][-2.2258413 -2.0113165 -1.8551412 -1.7621039 -1.6286346 -1.4482609 -1.2804815 -1.0298123 -1.0071523 -0.65182817 -0.53530711 -0.63404381 -0.4961713 -0.38539639 -0.21748635][-2.4296463 -2.3987615 -1.964664 -1.7229792 -1.361037 -1.2269191 -1.4107733 -1.5000526 -1.4197999 -1.4044245 -1.4318991 -1.4078386 -1.1845678 -0.95181853 -0.59806025][-2.509603 -2.599231 -2.2983506 -1.8618414 -1.4597759 -1.1455741 -1.3444097 -1.1208073 -1.134517 -1.1031049 -1.2105467 -1.3219516 -1.3245865 -1.1076318 -0.56963009][-1.1608429 -1.6244562 -1.876249 -1.8742003 -1.6405958 -1.0624243 -0.957812 -0.56743175 -0.58706594 -0.75208408 -0.77888936 -0.99314886 -1.2828051 -1.1058964 -0.87634146][-0.87139595 -1.1665262 -1.035363 -0.83477718 -0.62115353 -0.31828365 -0.47851449 -0.32353261 -0.52814245 -0.91876465 -1.2325445 -1.483294 -1.3566147 -1.2053566 -1.017969][-1.1641744 -1.3981862 -1.4463537 -1.1137447 -0.56390458 -0.016341507 0.20505732 0.3248325 0.082678653 -0.22115642 -0.72813618 -0.96958214 -1.0324762 -1.0977129 -0.89436859]]...]
INFO - root - 2017-12-06 05:03:59.177958: step 8910, loss = 0.81, batch loss = 0.60 (34.8 examples/sec; 0.230 sec/batch; 20h:40m:54s remains)
INFO - root - 2017-12-06 05:04:01.435691: step 8920, loss = 0.80, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:38s remains)
INFO - root - 2017-12-06 05:04:03.758425: step 8930, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 20h:50m:36s remains)
INFO - root - 2017-12-06 05:04:06.041698: step 8940, loss = 0.86, batch loss = 0.64 (33.3 examples/sec; 0.240 sec/batch; 21h:36m:46s remains)
INFO - root - 2017-12-06 05:04:08.343344: step 8950, loss = 0.85, batch loss = 0.64 (33.8 examples/sec; 0.237 sec/batch; 21h:16m:05s remains)
INFO - root - 2017-12-06 05:04:10.649155: step 8960, loss = 0.87, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 20h:49m:08s remains)
INFO - root - 2017-12-06 05:04:12.914343: step 8970, loss = 0.85, batch loss = 0.64 (34.0 examples/sec; 0.236 sec/batch; 21h:09m:55s remains)
INFO - root - 2017-12-06 05:04:15.221642: step 8980, loss = 0.89, batch loss = 0.68 (35.5 examples/sec; 0.225 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-06 05:04:17.506273: step 8990, loss = 0.87, batch loss = 0.66 (35.1 examples/sec; 0.228 sec/batch; 20h:29m:49s remains)
INFO - root - 2017-12-06 05:04:19.847715: step 9000, loss = 0.87, batch loss = 0.66 (34.1 examples/sec; 0.235 sec/batch; 21h:06m:06s remains)
2017-12-06 05:04:20.208014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066744119 -0.066773146 -0.06540864 -0.062867738 -0.059713829 -0.056700274 -0.054496713 -0.053061653 -0.051826529 -0.049661219 -0.045503423 -0.03884558 -0.029429428 -0.018212304 -0.0080686212][-0.066086777 -0.065173738 -0.062924311 -0.059635468 -0.055964321 -0.052673914 -0.05055606 -0.04977446 -0.049801935 -0.049374439 -0.047092494 -0.04197485 -0.033503681 -0.022465233 -0.012015358][-0.063589312 -0.061523005 -0.058222197 -0.0538435 -0.049148049 -0.045036715 -0.042618424 -0.042451937 -0.044034798 -0.045897134 -0.046202544 -0.043483682 -0.036909431 -0.026990239 -0.016609952][-0.060134269 -0.0564202 -0.051701248 -0.045972213 -0.0400202 -0.034951553 -0.03212725 -0.032378655 -0.035534598 -0.039954789 -0.043349475 -0.043752156 -0.040062491 -0.032458983 -0.023397248][-0.054454897 -0.049337551 -0.04335 -0.03642831 -0.029498816 -0.023734666 -0.020589523 -0.021232866 -0.02592146 -0.032998659 -0.039773617 -0.043890785 -0.043877758 -0.039447203 -0.0326982][-0.0462036 -0.040013306 -0.03318296 -0.025584005 -0.018205993 -0.01227513 -0.0093369111 -0.010561697 -0.016468957 -0.025763482 -0.035717819 -0.043594886 -0.047599837 -0.047176879 -0.0437256][-0.034911405 -0.027931113 -0.020635098 -0.013075486 -0.0062731728 -0.0012028962 0.0007917434 -0.0012971833 -0.0081074908 -0.018858522 -0.031320076 -0.042367421 -0.050028507 -0.053424265 -0.053405866][-0.020360678 -0.013536371 -0.0067782179 -0.00020895153 0.0051458403 0.0086798593 0.00937894 0.0064887777 -0.00062531978 -0.011728346 -0.025321595 -0.038267083 -0.048415381 -0.054665111 -0.057755932][-0.0041669831 0.0018878356 0.0074207485 0.01227802 0.015683919 0.017642371 0.017357782 0.014188692 0.0076757222 -0.0024458617 -0.015472353 -0.028984047 -0.040608946 -0.048817337 -0.054610059][0.010165714 0.015481919 0.019724853 0.023131706 0.025110692 0.025853917 0.024819143 0.021657377 0.016127937 0.0077097565 -0.0034006909 -0.015617281 -0.026799791 -0.035547365 -0.043104209][0.021455862 0.025913335 0.028795108 0.031221792 0.03254389 0.032877944 0.031831086 0.029149607 0.02479735 0.018278584 0.009602055 -0.00023462623 -0.009686999 -0.017705314 -0.025586251][0.027698018 0.031269915 0.032943428 0.034278631 0.034833863 0.035051696 0.034506522 0.032892749 0.030150473 0.025916003 0.020106234 0.013069347 0.0059308335 -0.00059851259 -0.0076433122][0.028010458 0.031731725 0.033371262 0.034548126 0.035260916 0.035831772 0.035907045 0.03542687 0.034410171 0.032656685 0.029489473 0.025021166 0.020166107 0.015117206 0.0091387182][0.01848717 0.022778325 0.024952061 0.026733495 0.028021336 0.029187031 0.030061096 0.030474447 0.030301988 0.030373186 0.029714279 0.028250352 0.02621787 0.023693673 0.02018299][-0.000532873 0.0048324019 0.0083400458 0.01176668 0.014717817 0.017483369 0.019996397 0.022219814 0.023944169 0.025087662 0.025886163 0.026053704 0.025495425 0.024554327 0.022692464]]...]
INFO - root - 2017-12-06 05:04:22.523539: step 9010, loss = 0.86, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 20h:45m:28s remains)
INFO - root - 2017-12-06 05:04:24.863009: step 9020, loss = 0.86, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 21h:08m:04s remains)
INFO - root - 2017-12-06 05:04:27.160506: step 9030, loss = 0.89, batch loss = 0.68 (36.2 examples/sec; 0.221 sec/batch; 19h:51m:30s remains)
INFO - root - 2017-12-06 05:04:29.463828: step 9040, loss = 0.83, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 20h:37m:45s remains)
INFO - root - 2017-12-06 05:04:31.791623: step 9050, loss = 0.90, batch loss = 0.68 (33.8 examples/sec; 0.237 sec/batch; 21h:17m:36s remains)
INFO - root - 2017-12-06 05:04:34.078172: step 9060, loss = 0.90, batch loss = 0.69 (33.8 examples/sec; 0.236 sec/batch; 21h:14m:28s remains)
INFO - root - 2017-12-06 05:04:36.378187: step 9070, loss = 0.84, batch loss = 0.62 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:25s remains)
INFO - root - 2017-12-06 05:04:38.709749: step 9080, loss = 0.88, batch loss = 0.67 (32.1 examples/sec; 0.250 sec/batch; 22h:25m:07s remains)
INFO - root - 2017-12-06 05:04:41.017668: step 9090, loss = 0.86, batch loss = 0.64 (32.8 examples/sec; 0.244 sec/batch; 21h:54m:35s remains)
INFO - root - 2017-12-06 05:04:43.359242: step 9100, loss = 0.88, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:34m:26s remains)
2017-12-06 05:04:43.754452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1034893 -1.7322053 -1.6635448 -1.9006329 -2.0839162 -2.3919897 -2.7981791 -2.7734127 -2.6928208 -2.5498085 -2.3287828 -1.5288086 -0.9131279 -0.60761654 -0.56241804][-1.1778492 -1.3737596 -1.7007741 -1.5134923 -1.4504758 -1.4621563 -1.7541372 -2.1602232 -2.0787938 -2.0392425 -2.4193945 -2.1859963 -1.7428012 -1.310038 -0.4240087][-0.83027536 -0.961943 -1.1318798 -1.1335862 -1.2104514 -1.0856898 -0.77102548 -0.64948082 -0.75764281 -1.1174206 -1.4585246 -1.9233071 -1.9673327 -1.4296701 -1.0419208][-0.73871535 -1.0117054 -1.0247829 -0.75199091 -0.60716438 -0.70339668 -0.20875835 0.30942678 0.23619759 0.026127346 -0.51655865 -0.99811059 -0.85308772 -1.4497691 -1.1708903][0.007344462 -0.096904017 0.020858474 0.24159679 0.6404314 1.1467763 1.286767 1.3916309 1.2013828 0.68362761 0.17743531 -0.25142366 0.074186735 -0.41155317 -0.75133085][0.3054837 0.6696164 0.80710763 1.0599831 1.0525721 1.4495317 1.8482177 2.0101874 1.6258191 1.3042461 1.0308125 0.85996985 0.25932094 0.016946509 -0.3956129][0.25557804 0.46856424 0.43848065 0.78237665 1.0271586 1.8191085 2.2204173 2.183069 2.1163402 1.5766153 1.1942871 1.0460588 0.67140895 0.55369478 -0.2407541][0.37775007 0.38387278 0.37152883 0.68741524 1.0773252 1.470938 1.5891696 1.8898855 2.0474994 1.5875206 1.4428071 0.48019502 0.44059744 0.38589531 0.17336729][1.2388692 1.3067789 1.1112714 1.2550652 1.2767141 1.5697182 1.9546596 2.1445379 2.3669226 1.91025 1.4484321 0.795698 0.3540948 0.47599396 0.95246094][0.45564851 0.58650744 0.8001169 0.727766 0.59557307 0.8427397 1.0947924 1.3911046 1.731735 1.2442173 0.872027 0.50765252 0.34926149 0.34811586 0.40548053][0.26823118 0.79903573 0.92659754 0.68932211 0.73762447 0.68543041 0.75321579 1.0066099 1.4464749 1.0295327 0.0990048 -0.1662031 -0.83585185 -0.39392745 -0.092706755][1.056825 1.0225831 1.0948858 1.3343832 1.1655555 1.1019549 1.0912186 0.97140032 1.0490359 1.03377 0.38043469 -0.20437114 -0.40090373 -0.47926033 -0.18262511][1.0960852 1.0652587 0.87227052 1.1170336 1.2899413 1.0080193 0.53864682 0.53961265 0.64390403 0.77812988 0.78340524 0.3945353 -0.42923707 -0.71918035 -0.42168882][2.2947521 2.0111644 1.8702292 1.8349994 1.7236689 1.7031325 1.4381093 1.01886 0.97268003 1.1627076 0.49153689 0.40525237 0.27258384 -0.694091 -0.88013381][2.8050647 2.9575062 3.1683052 2.6494257 2.3870516 2.4864089 2.2270427 1.8539431 1.5435265 1.40635 1.2659494 0.74367064 0.42167786 -0.36249852 -0.7091977]]...]
INFO - root - 2017-12-06 05:04:46.059278: step 9110, loss = 0.85, batch loss = 0.63 (34.4 examples/sec; 0.233 sec/batch; 20h:54m:35s remains)
INFO - root - 2017-12-06 05:04:48.358558: step 9120, loss = 0.83, batch loss = 0.61 (36.2 examples/sec; 0.221 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-06 05:04:50.693742: step 9130, loss = 0.88, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 20h:44m:38s remains)
INFO - root - 2017-12-06 05:04:53.016842: step 9140, loss = 0.82, batch loss = 0.60 (35.4 examples/sec; 0.226 sec/batch; 20h:16m:46s remains)
INFO - root - 2017-12-06 05:04:55.318153: step 9150, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:29m:30s remains)
INFO - root - 2017-12-06 05:04:57.611725: step 9160, loss = 0.79, batch loss = 0.58 (33.5 examples/sec; 0.239 sec/batch; 21h:28m:00s remains)
INFO - root - 2017-12-06 05:04:59.871761: step 9170, loss = 0.85, batch loss = 0.64 (34.7 examples/sec; 0.231 sec/batch; 20h:42m:09s remains)
INFO - root - 2017-12-06 05:05:02.175317: step 9180, loss = 0.84, batch loss = 0.63 (34.7 examples/sec; 0.231 sec/batch; 20h:42m:48s remains)
INFO - root - 2017-12-06 05:05:04.536797: step 9190, loss = 0.86, batch loss = 0.65 (32.6 examples/sec; 0.245 sec/batch; 22h:02m:20s remains)
INFO - root - 2017-12-06 05:05:06.845596: step 9200, loss = 0.84, batch loss = 0.63 (34.2 examples/sec; 0.234 sec/batch; 21h:00m:52s remains)
2017-12-06 05:05:07.246509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.085847266 -0.086317562 -0.086428359 -0.086094722 -0.085458316 -0.085186124 -0.085465819 -0.086133249 -0.0870723 -0.087740846 -0.088333786 -0.089003734 -0.089488074 -0.089690506 -0.089618489][-0.085812576 -0.08644864 -0.086688265 -0.0861377 -0.0850001 -0.084333152 -0.084594212 -0.085630894 -0.087119438 -0.088430181 -0.089631058 -0.090952672 -0.091859579 -0.092284665 -0.092063531][-0.085605912 -0.086558886 -0.087030187 -0.086151108 -0.084019795 -0.082126714 -0.0812632 -0.081342347 -0.082583405 -0.084538944 -0.087664932 -0.091391444 -0.0933194 -0.093668848 -0.093486138][-0.085611194 -0.087780558 -0.089214467 -0.087770894 -0.082317784 -0.07551688 -0.071469977 -0.0695857 -0.0709174 -0.074540749 -0.080765046 -0.088223554 -0.092621475 -0.093008786 -0.0931724][-0.086845376 -0.089483343 -0.086546794 -0.076039687 -0.061610676 -0.048592314 -0.041470062 -0.038482517 -0.039204903 -0.046033371 -0.059114493 -0.07368283 -0.0828251 -0.084934331 -0.086623952][-0.082974009 -0.073985025 -0.052552916 -0.02176176 0.0060709268 0.018495686 0.02145002 0.018967092 0.013390079 -0.0017029494 -0.022466227 -0.046513472 -0.064823329 -0.071079269 -0.075563654][-0.068462878 -0.042009648 -0.0019060597 0.04430683 0.076910242 0.0844603 0.081678748 0.072460547 0.061833322 0.038939975 0.012692869 -0.016241893 -0.041624479 -0.053236794 -0.061511114][-0.051757995 -0.01476185 0.035056904 0.088767394 0.12424855 0.13808191 0.15502958 0.16349198 0.15419693 0.11542469 0.068967357 0.022967964 -0.016109891 -0.03556139 -0.046714466][-0.042050939 -0.00072367489 0.058856547 0.12489842 0.172125 0.18901382 0.20360847 0.20635234 0.18995248 0.142778 0.087536663 0.037597097 -0.0021886528 -0.022741593 -0.034665231][-0.063572958 -0.033464298 0.01593522 0.074114382 0.11511821 0.126441 0.13305521 0.13349493 0.12191248 0.085144162 0.042095393 0.0067111254 -0.018685058 -0.031389274 -0.039515816][-0.10782044 -0.09736333 -0.072260909 -0.038530756 -0.0065430105 0.012331501 0.03577777 0.058522657 0.069671616 0.054592937 0.022162974 -0.0096996725 -0.035852507 -0.052364241 -0.061097976][-0.12425375 -0.12414557 -0.10975824 -0.085042678 -0.05282199 -0.022290438 0.0050800964 0.029467583 0.04131867 0.033590361 0.0089052171 -0.019572094 -0.046106193 -0.066309318 -0.076525569][-0.1196723 -0.11917931 -0.10773857 -0.089024223 -0.063457429 -0.039244782 -0.019635439 -0.0085595772 -0.0099189878 -0.02081877 -0.037386417 -0.053561337 -0.068371184 -0.079397239 -0.085366368][-0.11831626 -0.12101404 -0.1168216 -0.10795873 -0.097348392 -0.088759564 -0.08316116 -0.083767019 -0.08518023 -0.089677751 -0.094440028 -0.092891328 -0.091159828 -0.09234795 -0.093052305][-0.11878368 -0.13132507 -0.13894734 -0.14427981 -0.14474581 -0.14423749 -0.14267558 -0.14142761 -0.1386708 -0.1305839 -0.12003179 -0.10830411 -0.099147163 -0.096362747 -0.097352639]]...]
INFO - root - 2017-12-06 05:05:09.580007: step 9210, loss = 0.84, batch loss = 0.63 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:37s remains)
INFO - root - 2017-12-06 05:05:11.915263: step 9220, loss = 0.87, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 20h:51m:53s remains)
INFO - root - 2017-12-06 05:05:14.209019: step 9230, loss = 0.85, batch loss = 0.63 (35.4 examples/sec; 0.226 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-06 05:05:16.513977: step 9240, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 20h:30m:57s remains)
INFO - root - 2017-12-06 05:05:18.791053: step 9250, loss = 0.84, batch loss = 0.63 (34.7 examples/sec; 0.230 sec/batch; 20h:40m:26s remains)
INFO - root - 2017-12-06 05:05:21.096132: step 9260, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.226 sec/batch; 20h:19m:12s remains)
INFO - root - 2017-12-06 05:05:23.410876: step 9270, loss = 0.89, batch loss = 0.67 (36.1 examples/sec; 0.222 sec/batch; 19h:54m:55s remains)
INFO - root - 2017-12-06 05:05:25.763093: step 9280, loss = 0.81, batch loss = 0.60 (33.2 examples/sec; 0.241 sec/batch; 21h:38m:57s remains)
INFO - root - 2017-12-06 05:05:28.073814: step 9290, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:56s remains)
INFO - root - 2017-12-06 05:05:30.378533: step 9300, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:50s remains)
2017-12-06 05:05:30.727267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068625964 0.067975022 0.067229308 0.0671475 0.06793458 0.069239937 0.070337363 0.071101524 0.07127177 0.070361622 0.068033464 0.063776352 0.057961874 0.049678482 0.03816165][0.069741286 0.069332741 0.0683947 0.068044581 0.068609916 0.069596447 0.070454128 0.0710508 0.071071245 0.070083506 0.067725055 0.063773386 0.058320694 0.050306343 0.038689397][0.068909444 0.069287933 0.068570592 0.06815923 0.068418629 0.068980284 0.069486834 0.069869615 0.069732495 0.068602435 0.066192813 0.062447406 0.0573015 0.049564309 0.03790237][0.068967111 0.070404477 0.070091 0.069698848 0.06962768 0.0697571 0.069920488 0.070033513 0.069645517 0.068350174 0.06581258 0.062042989 0.056963198 0.049467437 0.037860222][0.069446914 0.072093152 0.072307475 0.072064318 0.071838669 0.07166823 0.071458288 0.071171708 0.0704832 0.06897556 0.06630709 0.062153332 0.056847237 0.049472921 0.037826516][0.07009346 0.073658146 0.074289136 0.074320875 0.0741686 0.073897012 0.073565178 0.073107772 0.07224346 0.070582353 0.067762069 0.063054331 0.057160489 0.049343951 0.037743449][0.0708633 0.075083546 0.075946249 0.0761484 0.076043285 0.07575535 0.075315319 0.074734934 0.073832057 0.072171025 0.069325276 0.064272113 0.05789464 0.049457498 0.038004667][0.071480818 0.07560546 0.076381393 0.076570161 0.076477446 0.076203384 0.075764053 0.075237416 0.074495412 0.0730444 0.070377268 0.065166332 0.058426805 0.049569957 0.038266361][0.071333461 0.074937858 0.075337909 0.075395994 0.075277187 0.075036265 0.074659862 0.074297957 0.07381963 0.072677486 0.070367657 0.065380774 0.058686458 0.049804635 0.03894455][0.0701681 0.073297314 0.07339216 0.073376052 0.073301069 0.073210336 0.073050551 0.072988711 0.072812267 0.071964972 0.069987409 0.0655885 0.058870278 0.050217621 0.03976988][0.068649031 0.071356185 0.07111039 0.070933186 0.070873387 0.070979662 0.0711338 0.071413405 0.071577676 0.071079396 0.069420807 0.065592833 0.05903367 0.050453283 0.040428348][0.066830672 0.069332726 0.069052778 0.068911083 0.06903813 0.0694182 0.06971956 0.0699619 0.070114605 0.069818653 0.068384342 0.064919464 0.058767281 0.050610505 0.041038938][0.064239763 0.066679724 0.066369511 0.066217639 0.066346206 0.066964768 0.067591466 0.068098195 0.068530671 0.068341814 0.067208059 0.064094745 0.058223344 0.050347634 0.040708296][0.059076719 0.061528616 0.061179914 0.060988195 0.061009355 0.061433084 0.061866261 0.062516831 0.063119493 0.063353635 0.062629558 0.060171209 0.054799803 0.047199689 0.037628353][0.047841661 0.05030293 0.049933188 0.049838521 0.050081395 0.050755523 0.051415868 0.052137859 0.052633204 0.052788369 0.051829092 0.049458794 0.044798873 0.038132913 0.029605649]]...]
INFO - root - 2017-12-06 05:05:32.996270: step 9310, loss = 0.87, batch loss = 0.66 (35.2 examples/sec; 0.227 sec/batch; 20h:22m:52s remains)
INFO - root - 2017-12-06 05:05:35.322591: step 9320, loss = 0.83, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-06 05:05:37.588737: step 9330, loss = 0.81, batch loss = 0.60 (35.5 examples/sec; 0.225 sec/batch; 20h:13m:31s remains)
INFO - root - 2017-12-06 05:05:39.868078: step 9340, loss = 0.93, batch loss = 0.71 (35.3 examples/sec; 0.227 sec/batch; 20h:21m:06s remains)
INFO - root - 2017-12-06 05:05:42.157930: step 9350, loss = 0.85, batch loss = 0.63 (35.0 examples/sec; 0.228 sec/batch; 20h:29m:47s remains)
INFO - root - 2017-12-06 05:05:44.442037: step 9360, loss = 0.89, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:41s remains)
INFO - root - 2017-12-06 05:05:46.706475: step 9370, loss = 0.88, batch loss = 0.66 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-06 05:05:48.999203: step 9380, loss = 0.88, batch loss = 0.67 (35.6 examples/sec; 0.224 sec/batch; 20h:08m:37s remains)
INFO - root - 2017-12-06 05:05:51.278249: step 9390, loss = 0.88, batch loss = 0.67 (33.7 examples/sec; 0.237 sec/batch; 21h:17m:31s remains)
INFO - root - 2017-12-06 05:05:53.514277: step 9400, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 20h:48m:44s remains)
2017-12-06 05:05:53.963446: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.048060194 -0.046997536 -0.047386754 -0.048418127 -0.049784623 -0.051084 -0.051944841 -0.052581109 -0.052823983 -0.052727275 -0.052662354 -0.053227685 -0.054911263 -0.0581637 -0.06194796][-0.044845048 -0.043719597 -0.043829091 -0.044779837 -0.045959022 -0.047104955 -0.048024558 -0.048568405 -0.04874808 -0.048734125 -0.048564065 -0.048973687 -0.05073427 -0.054543104 -0.059214137][-0.042949077 -0.041682623 -0.041418105 -0.04169561 -0.042289674 -0.043411009 -0.044874374 -0.045962427 -0.04637767 -0.046159029 -0.0456585 -0.045907829 -0.047951031 -0.052239679 -0.057646][-0.040266454 -0.039751548 -0.039480783 -0.039349686 -0.039911874 -0.041087747 -0.042571578 -0.043678056 -0.044235397 -0.044236723 -0.043947242 -0.044347335 -0.04658322 -0.051208079 -0.057193622][-0.04070523 -0.039613854 -0.038760535 -0.038576905 -0.039846778 -0.042057972 -0.044371482 -0.046107598 -0.046891097 -0.04750523 -0.04797544 -0.048011009 -0.048895281 -0.052225374 -0.057504833][-0.04428371 -0.041140266 -0.038237654 -0.036201853 -0.036637306 -0.039541446 -0.043996483 -0.048987322 -0.052908331 -0.055615496 -0.05649738 -0.055176385 -0.053901475 -0.055230647 -0.059338719][-0.047727168 -0.045265283 -0.042892996 -0.0403258 -0.037096959 -0.031405516 -0.022735961 -0.015691027 -0.015331671 -0.024509374 -0.037899528 -0.047879618 -0.053293861 -0.05706802 -0.061252519][-0.05339659 -0.051807228 -0.051015932 -0.050678033 -0.0506521 -0.04944092 -0.0457063 -0.040170006 -0.037087604 -0.0404637 -0.047979735 -0.054724429 -0.058480587 -0.061081834 -0.063898474][-0.057657771 -0.056104958 -0.055120215 -0.055314153 -0.058219396 -0.0638957 -0.070080176 -0.074084707 -0.074686542 -0.072994068 -0.070901491 -0.068914115 -0.067522824 -0.067549117 -0.068073086][-0.060015433 -0.058963012 -0.05803987 -0.057346947 -0.058092028 -0.060071811 -0.062100049 -0.063347511 -0.064441852 -0.065194793 -0.066186644 -0.066735029 -0.066625193 -0.0683058 -0.070466019][-0.062802926 -0.062156975 -0.061325744 -0.060658757 -0.061287418 -0.063159056 -0.065322094 -0.067829564 -0.069779359 -0.070947453 -0.070969075 -0.070119835 -0.06965217 -0.0705286 -0.071499556][-0.065738276 -0.065853834 -0.065774821 -0.065456636 -0.065675452 -0.066720806 -0.0680883 -0.069886513 -0.071775064 -0.073345609 -0.074067935 -0.0730644 -0.072008759 -0.07197459 -0.07223253][-0.068602175 -0.069705844 -0.07046809 -0.070733458 -0.070964843 -0.071598813 -0.072413683 -0.073232584 -0.073833048 -0.074254006 -0.07415577 -0.073182583 -0.071956232 -0.071339816 -0.071728021][-0.071547136 -0.072201759 -0.072877906 -0.073121272 -0.073332667 -0.073909871 -0.074616067 -0.075338826 -0.075905204 -0.075993985 -0.075915366 -0.075395852 -0.074797854 -0.074436925 -0.0743661][-0.070249327 -0.070288971 -0.070586331 -0.070769176 -0.071054526 -0.071470469 -0.071953923 -0.072514743 -0.07298369 -0.073554359 -0.07430128 -0.07485006 -0.075281337 -0.07577347 -0.075935818]]...]
INFO - root - 2017-12-06 05:05:56.306617: step 9410, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 20h:49m:47s remains)
INFO - root - 2017-12-06 05:05:58.621082: step 9420, loss = 0.90, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 19h:56m:14s remains)
INFO - root - 2017-12-06 05:06:00.897134: step 9430, loss = 0.92, batch loss = 0.70 (36.6 examples/sec; 0.219 sec/batch; 19h:37m:58s remains)
INFO - root - 2017-12-06 05:06:03.179662: step 9440, loss = 0.92, batch loss = 0.71 (35.5 examples/sec; 0.225 sec/batch; 20h:13m:51s remains)
INFO - root - 2017-12-06 05:06:05.428363: step 9450, loss = 0.95, batch loss = 0.73 (35.1 examples/sec; 0.228 sec/batch; 20h:28m:07s remains)
INFO - root - 2017-12-06 05:06:07.708162: step 9460, loss = 0.96, batch loss = 0.74 (35.4 examples/sec; 0.226 sec/batch; 20h:16m:32s remains)
INFO - root - 2017-12-06 05:06:10.000042: step 9470, loss = 0.84, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 19h:55m:54s remains)
INFO - root - 2017-12-06 05:06:12.279501: step 9480, loss = 0.89, batch loss = 0.67 (34.4 examples/sec; 0.232 sec/batch; 20h:50m:22s remains)
INFO - root - 2017-12-06 05:06:14.556178: step 9490, loss = 0.91, batch loss = 0.70 (35.8 examples/sec; 0.223 sec/batch; 20h:03m:09s remains)
INFO - root - 2017-12-06 05:06:16.821534: step 9500, loss = 0.95, batch loss = 0.74 (35.5 examples/sec; 0.225 sec/batch; 20h:11m:33s remains)
2017-12-06 05:06:17.289384: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.214347 0.17080656 0.12051321 0.10916298 0.11581764 0.14915618 0.21807003 0.34486708 0.4547953 0.53704661 0.60507518 0.68614447 0.71906054 0.72570384 0.66755873][0.43381244 0.37745431 0.25108913 0.14144674 0.091833256 0.097057424 0.1330246 0.23995748 0.34726778 0.41990697 0.49445629 0.570977 0.56891233 0.55978709 0.49638826][0.77626163 0.7359786 0.5797013 0.39478844 0.27091816 0.23932025 0.26244318 0.33039364 0.36905909 0.37101725 0.34135 0.33081558 0.28315833 0.27002499 0.22535971][1.045155 1.0273303 0.85829329 0.59453952 0.34367642 0.17409015 0.093565054 0.091085277 0.075115956 0.02457463 -0.013403125 -0.011395678 -0.030830853 -0.029121097 -0.055997029][0.90240091 0.92971861 0.82973665 0.6237489 0.35347548 0.12243145 -0.047365475 -0.14113304 -0.19379497 -0.23984429 -0.26702732 -0.25396255 -0.24501047 -0.23616588 -0.22712561][0.44951856 0.53178704 0.57739586 0.53300428 0.40905273 0.252525 0.079937331 -0.055997036 -0.16726384 -0.25321025 -0.30799502 -0.32792908 -0.31600007 -0.28411704 -0.24625582][0.26452786 0.33505493 0.43977541 0.50292742 0.52115923 0.43876302 0.31128302 0.16787592 0.025208764 -0.099993549 -0.20310551 -0.24845853 -0.23504972 -0.19018152 -0.14879894][0.046296827 0.15393931 0.29482496 0.38501513 0.47639328 0.52916861 0.50734788 0.40684444 0.29795918 0.20222741 0.071671806 -0.016015559 -0.047920097 -0.040955618 -0.028880589][-0.082433864 -0.02800604 0.081394516 0.1933611 0.3346816 0.46120006 0.54091388 0.513812 0.43966621 0.37976643 0.24786013 0.14319554 0.079866849 0.056931131 0.03868705][-0.12471083 -0.066792436 0.012709685 0.10832518 0.23379982 0.35640165 0.41187355 0.42013091 0.38680258 0.36143 0.27866012 0.18496135 0.11010205 0.064903878 0.0275741][-0.14321035 -0.11011488 -0.054850988 0.012941897 0.08753667 0.16780078 0.18897963 0.16138256 0.14436692 0.12969664 0.090618767 0.044950329 0.018922791 0.004273355 -0.016654782][-0.1049656 -0.10268674 -0.078030787 -0.040910598 -0.0083527863 0.016998768 0.036495283 0.032889113 0.019247428 0.024507508 0.010558777 -0.01094728 -0.023177709 -0.039211575 -0.05749695][-0.10266785 -0.11506286 -0.11747323 -0.1045965 -0.08551579 -0.077750325 -0.063565612 -0.050839029 -0.047047719 -0.053835671 -0.056668334 -0.061652705 -0.056740548 -0.059843931 -0.078788422][-0.096589796 -0.11046475 -0.12314473 -0.13311058 -0.1323435 -0.12559277 -0.11216112 -0.10772113 -0.093741171 -0.078443058 -0.072409235 -0.076535664 -0.073743917 -0.067503631 -0.091339][-0.091903746 -0.098131523 -0.10720754 -0.11783801 -0.12766171 -0.13482276 -0.1313301 -0.12804428 -0.12226221 -0.08800067 -0.0675594 -0.056347344 -0.049018256 -0.040097341 -0.06298276]]...]
INFO - root - 2017-12-06 05:06:19.537161: step 9510, loss = 1.02, batch loss = 0.81 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:25s remains)
INFO - root - 2017-12-06 05:06:21.784092: step 9520, loss = 0.97, batch loss = 0.76 (36.1 examples/sec; 0.221 sec/batch; 19h:52m:04s remains)
INFO - root - 2017-12-06 05:06:24.046734: step 9530, loss = 0.90, batch loss = 0.68 (33.7 examples/sec; 0.237 sec/batch; 21h:16m:33s remains)
INFO - root - 2017-12-06 05:06:26.363074: step 9540, loss = 0.88, batch loss = 0.67 (33.9 examples/sec; 0.236 sec/batch; 21h:08m:24s remains)
INFO - root - 2017-12-06 05:06:28.677779: step 9550, loss = 0.89, batch loss = 0.68 (31.5 examples/sec; 0.254 sec/batch; 22h:48m:11s remains)
INFO - root - 2017-12-06 05:06:30.971674: step 9560, loss = 0.88, batch loss = 0.67 (34.5 examples/sec; 0.232 sec/batch; 20h:46m:51s remains)
INFO - root - 2017-12-06 05:06:33.243064: step 9570, loss = 0.86, batch loss = 0.65 (34.5 examples/sec; 0.232 sec/batch; 20h:47m:28s remains)
INFO - root - 2017-12-06 05:06:35.611079: step 9580, loss = 0.85, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 24h:48m:07s remains)
INFO - root - 2017-12-06 05:06:37.913461: step 9590, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 20h:21m:12s remains)
INFO - root - 2017-12-06 05:06:40.230178: step 9600, loss = 0.86, batch loss = 0.65 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:51s remains)
2017-12-06 05:06:40.643840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82497615 -0.17067216 0.31286365 0.68171877 0.80428469 0.89152896 0.87700331 0.5527575 0.54690504 0.44915456 0.63880932 1.0836933 1.4329252 1.6644043 1.8791873][-1.5121137 -1.5533806 -1.8433814 -1.6864024 -1.5420402 -1.4958382 -1.3042444 -1.1546456 -0.91437179 -0.97599679 -0.87573373 -0.22527941 0.32803947 0.774975 1.1889242][-0.48184174 -0.8157813 -1.3441254 -1.9793296 -2.5287828 -2.5903664 -2.4235635 -2.2128105 -1.7674245 -1.3346819 -1.0129799 -0.80995083 -0.66435593 -0.36423022 0.042926908][0.880373 0.44701242 -0.34527427 -1.0947578 -1.8321885 -2.1959732 -2.0558755 -2.1110685 -2.1070483 -1.9324094 -1.5840968 -1.4029788 -1.0436057 -0.84919208 -0.65718412][1.5556102 1.8978739 1.7627338 1.0589542 -0.0055634081 -0.88663787 -1.2149241 -1.0866802 -0.729632 -0.68064493 -0.60389924 -0.55141962 -0.51242346 -0.61274004 -0.76138461][3.0026059 2.9788024 3.1052208 3.3165803 3.3295403 2.5974989 1.7287157 1.3214791 1.1609761 1.0173917 0.69524246 0.310094 -0.03843772 -0.12405308 -0.10799302][4.9994426 5.4232073 5.5272775 5.528965 5.6300616 5.8409104 5.6135736 4.7102385 3.740263 3.0142233 2.1348517 1.4526532 0.6196875 -0.0553292 -0.38662946][5.1913176 5.9022131 6.2735772 6.731976 7.0750551 7.1445413 6.8890686 6.180563 5.0570722 3.9688034 2.7573786 1.6954926 0.67609936 0.1159476 -0.046870276][4.0804496 4.65373 5.268023 5.9234347 6.3068953 6.5172839 6.0842681 5.2449083 4.2344275 2.9928732 2.1014476 1.3530233 0.79646486 0.16463022 -0.28994903][2.0981328 2.8719571 3.5635264 3.9852152 4.1282606 4.24669 3.9748111 3.1207166 2.1352062 1.6242428 1.0850878 0.65949827 0.38846749 0.091676116 -0.066072546][0.63704944 1.1219678 1.6814613 2.1242366 2.1963429 1.9539661 1.4616824 0.79271656 0.06090045 -0.28878266 -0.21521349 -0.14311954 0.020999536 0.14256997 0.13634323][0.041484475 0.12062047 0.040644988 0.086025208 0.066179663 -0.055697657 -0.2648955 -0.50099427 -0.87571359 -1.1554528 -0.93298405 -0.55113566 -0.1828272 0.19016366 0.35370308][0.38921589 0.38451904 0.35902339 0.28535777 0.2473626 0.075903714 -0.0026897267 -0.40106541 -0.61784911 -0.79327559 -0.98144162 -0.79890227 -0.50877881 -0.14884305 0.092126459][0.37371665 0.53729445 0.97682947 1.0235282 1.0579078 0.96933311 0.73871297 0.4805283 0.027397119 -0.1170672 -0.25729874 -0.43385482 -0.61691123 -0.41153508 -0.15157109][0.36702752 0.62080896 1.0431905 1.3077725 1.5956854 1.5517981 1.4582011 1.0725995 0.67273438 0.34104395 0.05174312 -0.082936317 -0.19923574 -0.42987007 -0.33102104]]...]
INFO - root - 2017-12-06 05:06:42.956832: step 9610, loss = 0.84, batch loss = 0.63 (35.9 examples/sec; 0.223 sec/batch; 19h:58m:41s remains)
INFO - root - 2017-12-06 05:06:45.248217: step 9620, loss = 0.81, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:08s remains)
INFO - root - 2017-12-06 05:06:47.509845: step 9630, loss = 0.91, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 20h:32m:53s remains)
INFO - root - 2017-12-06 05:06:49.815890: step 9640, loss = 0.92, batch loss = 0.70 (33.4 examples/sec; 0.239 sec/batch; 21h:28m:44s remains)
INFO - root - 2017-12-06 05:06:52.088064: step 9650, loss = 0.88, batch loss = 0.66 (35.8 examples/sec; 0.224 sec/batch; 20h:03m:05s remains)
INFO - root - 2017-12-06 05:06:54.362037: step 9660, loss = 0.85, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:30m:36s remains)
INFO - root - 2017-12-06 05:06:56.654092: step 9670, loss = 0.80, batch loss = 0.59 (34.6 examples/sec; 0.231 sec/batch; 20h:45m:27s remains)
INFO - root - 2017-12-06 05:06:58.972411: step 9680, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 21h:02m:19s remains)
INFO - root - 2017-12-06 05:07:01.249482: step 9690, loss = 0.84, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 20h:33m:36s remains)
INFO - root - 2017-12-06 05:07:03.577993: step 9700, loss = 0.86, batch loss = 0.65 (33.0 examples/sec; 0.243 sec/batch; 21h:45m:34s remains)
2017-12-06 05:07:03.959785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069912389 -0.074885428 -0.069534019 -0.065306529 -0.06342265 -0.062959053 -0.061818957 -0.061570618 -0.06299895 -0.06413082 -0.064118966 -0.063286237 -0.063945681 -0.064785659 -0.064813986][-0.097699068 -0.080960542 -0.069118261 -0.063962668 -0.062345095 -0.062351763 -0.0610973 -0.060516454 -0.062992044 -0.065861255 -0.067354031 -0.066036738 -0.065366037 -0.065400675 -0.065276995][-0.10748652 -0.0810276 -0.0650847 -0.059208564 -0.057408188 -0.0583582 -0.058827605 -0.059026726 -0.06121365 -0.06499368 -0.06723135 -0.067677647 -0.067695439 -0.067343429 -0.066920757][-0.10696425 -0.0797978 -0.064758629 -0.061813179 -0.061759237 -0.063987844 -0.065872639 -0.067837834 -0.069646306 -0.071889415 -0.073005825 -0.072421163 -0.070421167 -0.06824778 -0.067460515][-0.099368788 -0.072696455 -0.055575021 -0.056580387 -0.062954478 -0.0737007 -0.089325666 -0.10890565 -0.12567583 -0.13176893 -0.12394118 -0.10591651 -0.087456495 -0.076243408 -0.071503669][-0.078969322 -0.051982343 -0.030410409 -0.030412953 -0.032724783 -0.032711588 -0.0380369 -0.066499889 -0.11368813 -0.1532239 -0.16588178 -0.15035775 -0.12236169 -0.09914884 -0.085042119][-0.070249356 -0.049247358 -0.026374731 -0.0048000515 0.039325587 0.11055314 0.17271861 0.17430231 0.09749455 -0.0087112114 -0.089130089 -0.12188104 -0.11821847 -0.099577583 -0.08206743][-0.069248676 -0.053193573 -0.034941871 -0.0045707151 0.066729404 0.17789274 0.28911942 0.33190462 0.2674751 0.14267746 0.025429353 -0.048068855 -0.076355323 -0.075532705 -0.067441][-0.061633527 -0.056677323 -0.062490351 -0.066031605 -0.04248045 0.012792617 0.073582225 0.10466231 0.078739576 0.021515228 -0.036616705 -0.073420778 -0.088447541 -0.08539582 -0.07305444][-0.055948429 -0.061218634 -0.084071107 -0.10982274 -0.11492848 -0.087467104 -0.040162407 -0.0045362711 -0.0033078343 -0.025944594 -0.057968557 -0.081407487 -0.09237466 -0.087211832 -0.071424007][-0.05099012 -0.060855173 -0.087697506 -0.11414665 -0.11033934 -0.065478526 0.0017988607 0.050108902 0.05636128 0.025996991 -0.023149893 -0.066854276 -0.088327691 -0.085055545 -0.071660765][-0.043695338 -0.05375357 -0.082324162 -0.12020985 -0.13994505 -0.11786587 -0.066309161 -0.023235165 -0.014638096 -0.035509288 -0.069097847 -0.092287496 -0.097265668 -0.086086459 -0.072221614][-0.036062345 -0.043031741 -0.0667367 -0.10480782 -0.14395517 -0.16527823 -0.16118476 -0.14427269 -0.13030887 -0.12384519 -0.1172256 -0.10574805 -0.0915864 -0.079444781 -0.070875235][-0.029135138 -0.032565642 -0.044539448 -0.069134288 -0.1037966 -0.13419162 -0.14960855 -0.15080309 -0.14377907 -0.12588407 -0.10287271 -0.083771341 -0.072955564 -0.068581514 -0.0671404][-0.028977264 -0.027530339 -0.029854931 -0.03909542 -0.054609466 -0.071729869 -0.087097406 -0.096606225 -0.094168209 -0.084680095 -0.0746132 -0.068614274 -0.067025185 -0.06629207 -0.065961264]]...]
INFO - root - 2017-12-06 05:07:06.269501: step 9710, loss = 0.78, batch loss = 0.57 (35.0 examples/sec; 0.228 sec/batch; 20h:29m:07s remains)
INFO - root - 2017-12-06 05:07:08.591233: step 9720, loss = 0.80, batch loss = 0.58 (34.4 examples/sec; 0.232 sec/batch; 20h:49m:57s remains)
INFO - root - 2017-12-06 05:07:10.891141: step 9730, loss = 0.88, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 20h:41m:11s remains)
INFO - root - 2017-12-06 05:07:13.232869: step 9740, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 20h:31m:52s remains)
INFO - root - 2017-12-06 05:07:15.518575: step 9750, loss = 0.88, batch loss = 0.67 (33.2 examples/sec; 0.241 sec/batch; 21h:37m:07s remains)
INFO - root - 2017-12-06 05:07:17.880639: step 9760, loss = 0.86, batch loss = 0.64 (34.7 examples/sec; 0.231 sec/batch; 20h:40m:34s remains)
INFO - root - 2017-12-06 05:07:20.179119: step 9770, loss = 0.84, batch loss = 0.63 (36.8 examples/sec; 0.217 sec/batch; 19h:29m:33s remains)
INFO - root - 2017-12-06 05:07:22.518134: step 9780, loss = 0.81, batch loss = 0.60 (32.5 examples/sec; 0.246 sec/batch; 22h:02m:18s remains)
INFO - root - 2017-12-06 05:07:24.831942: step 9790, loss = 0.82, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 20h:32m:42s remains)
INFO - root - 2017-12-06 05:07:27.095059: step 9800, loss = 0.92, batch loss = 0.71 (33.7 examples/sec; 0.238 sec/batch; 21h:18m:22s remains)
2017-12-06 05:07:27.454983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.075223662 -0.076263271 -0.076894678 -0.077552356 -0.078087047 -0.078409985 -0.078379542 -0.077740408 -0.076542124 -0.075105056 -0.07375408 -0.072374433 -0.0712352 -0.0704738 -0.070340462][-0.077341042 -0.07786382 -0.078386307 -0.079186887 -0.080110058 -0.080857374 -0.081270181 -0.080759563 -0.079373375 -0.077621326 -0.075749867 -0.073931009 -0.072492473 -0.071104459 -0.070066229][-0.072627522 -0.072671376 -0.07349053 -0.074549139 -0.075488165 -0.076066174 -0.076376706 -0.076502778 -0.07607542 -0.074631862 -0.072568223 -0.070616268 -0.069231547 -0.068034247 -0.066964835][-0.069343932 -0.0692295 -0.069891252 -0.070992395 -0.072128467 -0.072883256 -0.073268555 -0.07357426 -0.073573314 -0.07270401 -0.070815265 -0.068617463 -0.067002527 -0.065785982 -0.064852111][-0.064703964 -0.064456046 -0.0650582 -0.066024818 -0.067058004 -0.067801148 -0.068184644 -0.06851396 -0.068855107 -0.068649 -0.067770749 -0.066436619 -0.065105028 -0.063739158 -0.062676623][-0.061288826 -0.060711391 -0.061154619 -0.061865859 -0.06272427 -0.063519366 -0.06409274 -0.06451983 -0.064969093 -0.065169036 -0.064892754 -0.064256564 -0.063430905 -0.062400326 -0.061541237][-0.058783658 -0.057975795 -0.058409363 -0.059075788 -0.059693694 -0.060211271 -0.060698889 -0.061278306 -0.06190772 -0.062321644 -0.062527731 -0.062562034 -0.062361717 -0.06179551 -0.061471656][-0.057371084 -0.056428313 -0.056650925 -0.057006672 -0.057500497 -0.058144286 -0.058799773 -0.059261154 -0.059756353 -0.060149707 -0.060446784 -0.060796984 -0.06110213 -0.061098188 -0.061396495][-0.056537621 -0.055482894 -0.055460379 -0.055528745 -0.05573713 -0.056376621 -0.05702693 -0.057522893 -0.058036208 -0.058444314 -0.058838077 -0.059324827 -0.059752472 -0.060080014 -0.060954958][-0.055850521 -0.054739885 -0.054647122 -0.054679178 -0.054731872 -0.054969314 -0.055521309 -0.056080185 -0.056567833 -0.056990143 -0.057415273 -0.057868574 -0.058339436 -0.058841813 -0.060043715][-0.055022821 -0.053901888 -0.053849619 -0.053953841 -0.053955864 -0.054042418 -0.054370224 -0.054789312 -0.05525228 -0.055790439 -0.056195475 -0.056623939 -0.057090875 -0.057636753 -0.05892032][-0.054332141 -0.053215113 -0.053154398 -0.053205848 -0.053307563 -0.053456377 -0.053784259 -0.054186035 -0.054548163 -0.054939695 -0.05541667 -0.055973195 -0.056333657 -0.056802109 -0.058059923][-0.053911362 -0.052667391 -0.052602366 -0.052674212 -0.052814074 -0.052972093 -0.0532993 -0.05369604 -0.054023735 -0.054336078 -0.054757945 -0.055272534 -0.05558924 -0.056002252 -0.057194598][-0.054217476 -0.052933604 -0.052856028 -0.052983545 -0.0532133 -0.053475503 -0.053878214 -0.054381974 -0.054838985 -0.055167176 -0.055612415 -0.056146879 -0.056466218 -0.056777854 -0.057818677][-0.057400279 -0.056398362 -0.056398831 -0.056418743 -0.056550782 -0.056789957 -0.05720076 -0.057639614 -0.058187559 -0.05891908 -0.05950794 -0.060096882 -0.060334027 -0.060353089 -0.061042674]]...]
INFO - root - 2017-12-06 05:07:29.757950: step 9810, loss = 0.85, batch loss = 0.64 (36.9 examples/sec; 0.217 sec/batch; 19h:27m:34s remains)
INFO - root - 2017-12-06 05:07:32.061068: step 9820, loss = 0.88, batch loss = 0.67 (33.4 examples/sec; 0.240 sec/batch; 21h:29m:02s remains)
INFO - root - 2017-12-06 05:07:34.328712: step 9830, loss = 0.80, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:36m:21s remains)
INFO - root - 2017-12-06 05:07:36.636810: step 9840, loss = 0.81, batch loss = 0.60 (34.1 examples/sec; 0.235 sec/batch; 21h:03m:19s remains)
INFO - root - 2017-12-06 05:07:38.927865: step 9850, loss = 0.78, batch loss = 0.56 (33.8 examples/sec; 0.237 sec/batch; 21h:12m:22s remains)
INFO - root - 2017-12-06 05:07:41.184577: step 9860, loss = 0.84, batch loss = 0.62 (35.5 examples/sec; 0.225 sec/batch; 20h:10m:48s remains)
INFO - root - 2017-12-06 05:07:43.487941: step 9870, loss = 0.84, batch loss = 0.62 (36.5 examples/sec; 0.219 sec/batch; 19h:39m:01s remains)
INFO - root - 2017-12-06 05:07:45.782884: step 9880, loss = 0.89, batch loss = 0.68 (35.3 examples/sec; 0.227 sec/batch; 20h:17m:57s remains)
INFO - root - 2017-12-06 05:07:48.047031: step 9890, loss = 0.84, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 20h:59m:16s remains)
INFO - root - 2017-12-06 05:07:50.334465: step 9900, loss = 0.79, batch loss = 0.57 (35.6 examples/sec; 0.225 sec/batch; 20h:09m:10s remains)
2017-12-06 05:07:50.711934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18573163 -0.24391091 -0.32612938 -0.42447251 -0.54019523 -0.66592765 -0.79077679 -0.85772532 -0.85170633 -0.78469348 -0.68228769 -0.5574457 -0.4430612 -0.34773478 -0.27085975][-0.30259383 -0.35842043 -0.39622465 -0.42121068 -0.45676175 -0.53169954 -0.64140236 -0.71397913 -0.75018215 -0.74399155 -0.69065487 -0.59385914 -0.48784551 -0.38965672 -0.30995283][-0.46917295 -0.48009464 -0.38813773 -0.24398991 -0.14935441 -0.12489311 -0.17693771 -0.24544263 -0.33130491 -0.39338452 -0.42477241 -0.41334444 -0.35299695 -0.2671403 -0.23823747][-0.61347544 -0.53931671 -0.24603519 0.18235242 0.521255 0.69390666 0.6918987 0.60023117 0.37104806 0.13233927 -0.024996053 -0.1352745 -0.13688403 -0.10384814 -0.11844556][-0.63832271 -0.46624359 0.025857195 0.72432071 1.4250222 1.8983537 1.9282348 1.6991719 1.2812562 0.83475214 0.47465622 0.20029742 0.057146303 -0.0023924112 -0.023136765][-0.52963179 -0.26876098 0.38549662 1.282356 2.1753953 2.8694921 3.1011522 2.8636758 2.3108048 1.6089462 0.98938745 0.50526142 0.21014765 0.057440422 0.0055519268][-0.42390409 -0.10630868 0.60403109 1.6107706 2.6741877 3.429879 3.6918495 3.4955933 2.9047766 2.0719168 1.2989686 0.72256422 0.31156918 0.060018487 -0.016775221][-0.42149529 -0.22135139 0.35317785 1.2669218 2.3835132 3.2901776 3.651819 3.4099531 2.8585441 2.0087106 1.1990643 0.64309323 0.24693406 0.03958825 -0.011747576][-0.54392689 -0.50658995 -0.22709358 0.33024639 1.1801696 2.0034785 2.4738624 2.367043 1.9645631 1.300097 0.70317513 0.33587366 0.072550051 0.0042136684 0.0037223697][-0.713473 -0.86842114 -0.87453991 -0.66065085 -0.1972262 0.356795 0.79825497 0.87177533 0.65029317 0.26841459 -0.078681111 -0.1753722 -0.22462326 -0.15492746 -0.13067201][-0.748017 -1.0465595 -1.2743232 -1.3550452 -1.2271169 -0.8717863 -0.46692663 -0.24877328 -0.24017096 -0.39435393 -0.51730973 -0.46245641 -0.33865827 -0.17231521 -0.12554324][-0.58841038 -0.88233924 -1.1743892 -1.4342049 -1.5445685 -1.4452175 -1.1949788 -0.87761223 -0.62330568 -0.54239482 -0.52454036 -0.40932506 -0.27380773 -0.15604116 -0.12735663][-0.38552096 -0.57935339 -0.77483845 -1.0118234 -1.160262 -1.1688013 -1.0693808 -0.90303361 -0.66784692 -0.51525289 -0.38869017 -0.27850175 -0.18449233 -0.12290455 -0.1131895][-0.22886401 -0.32535291 -0.36991853 -0.48381624 -0.57325667 -0.61075425 -0.59443742 -0.49697167 -0.3715843 -0.31420013 -0.21140343 -0.19861183 -0.12880763 -0.13780358 -0.11442105][-0.14814748 -0.18970191 -0.18438777 -0.20693156 -0.15997821 -0.14717652 -0.14342594 -0.11014781 -0.10699406 -0.12656826 -0.097345836 -0.18537545 -0.17287412 -0.17282197 -0.13593467]]...]
INFO - root - 2017-12-06 05:07:53.023026: step 9910, loss = 0.88, batch loss = 0.66 (36.5 examples/sec; 0.219 sec/batch; 19h:36m:49s remains)
INFO - root - 2017-12-06 05:07:55.333940: step 9920, loss = 0.89, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 20h:27m:02s remains)
INFO - root - 2017-12-06 05:07:57.654321: step 9930, loss = 0.76, batch loss = 0.55 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:54s remains)
INFO - root - 2017-12-06 05:07:59.934935: step 9940, loss = 0.80, batch loss = 0.59 (35.9 examples/sec; 0.223 sec/batch; 19h:57m:53s remains)
INFO - root - 2017-12-06 05:08:02.203238: step 9950, loss = 0.80, batch loss = 0.58 (34.9 examples/sec; 0.230 sec/batch; 20h:34m:00s remains)
INFO - root - 2017-12-06 05:08:04.502669: step 9960, loss = 0.85, batch loss = 0.64 (33.5 examples/sec; 0.239 sec/batch; 21h:22m:38s remains)
INFO - root - 2017-12-06 05:08:06.822972: step 9970, loss = 0.82, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 20h:42m:33s remains)
INFO - root - 2017-12-06 05:08:09.126386: step 9980, loss = 0.85, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:22s remains)
INFO - root - 2017-12-06 05:08:11.426025: step 9990, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.224 sec/batch; 20h:06m:32s remains)
INFO - root - 2017-12-06 05:08:13.700917: step 10000, loss = 0.82, batch loss = 0.60 (34.9 examples/sec; 0.229 sec/batch; 20h:31m:32s remains)
2017-12-06 05:08:14.083372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.35605475 -0.47827816 -0.6357283 -0.82348752 -1.0228918 -1.17321 -1.2038358 -1.1181806 -0.97021288 -0.744033 -0.51431113 -0.337576 -0.25501743 -0.22317964 -0.20205149][-0.43305993 -0.63505673 -0.89360183 -1.184917 -1.5102626 -1.8464394 -2.0937593 -2.1749084 -2.0849805 -1.7825117 -1.39502 -1.0354269 -0.77713543 -0.58620745 -0.44381607][-0.4654648 -0.67353714 -0.96035832 -1.3681294 -1.8251216 -2.2698073 -2.7044241 -3.0725977 -3.2381425 -3.0809529 -2.5946357 -2.0183172 -1.4801685 -1.0209938 -0.698911][-0.56354111 -0.7222833 -0.90574956 -1.2034967 -1.6872092 -2.304498 -2.9115427 -3.4749064 -3.8898225 -3.9356267 -3.5702815 -2.9482565 -2.2978709 -1.6201577 -1.0572337][-0.5547809 -0.67987657 -0.79373246 -0.96350157 -1.0942498 -1.4329075 -2.0645959 -2.8866916 -3.5853283 -3.9314127 -3.8143764 -3.2897482 -2.5620806 -1.7783098 -1.1272842][-0.40010747 -0.43866065 -0.43821609 -0.41290849 -0.33595705 -0.42287862 -0.63937908 -1.2575434 -2.0610039 -2.8164544 -2.9319258 -2.7463136 -2.4071238 -1.7758856 -1.1715678][-0.19387361 -0.2360746 -0.17512819 -0.058436863 0.24737364 0.41558295 0.51566994 0.36368507 -0.10819644 -0.73373061 -1.2983838 -1.5791082 -1.5215069 -1.2326655 -0.87314087][0.10351429 0.13346365 0.1974538 0.42659408 0.88603765 1.1958922 1.3975341 1.2657706 0.91036445 0.63648516 0.20577213 -0.21045129 -0.45980769 -0.46199915 -0.45104274][0.22480074 0.38382497 0.46481991 0.70920795 1.1949544 1.7107028 2.0298252 2.038141 1.8560605 1.6365291 1.2004801 0.79526836 0.541418 0.35763496 0.21686977][0.008101739 0.22392285 0.47042763 0.76079112 1.2014784 1.6748408 2.0217066 2.0109153 1.911947 1.675227 1.3751874 0.95634 0.66765785 0.32157153 0.22666723][-0.20357649 -0.19337586 -0.045747627 0.20132738 0.63067669 1.0971167 1.4703588 1.6979387 1.6771691 1.2846541 0.8758626 0.50911224 0.2652258 0.11813676 0.016525269][-0.19909069 -0.29316604 -0.372178 -0.36037803 -0.14594664 0.19551414 0.56180507 0.81842548 0.80661952 0.67236853 0.26686054 -0.041701913 -0.38908258 -0.51033419 -0.48306635][-0.14548835 -0.22090513 -0.31397083 -0.43702269 -0.518008 -0.52217364 -0.42464212 -0.21178144 0.0039405152 -0.072504483 -0.306176 -0.45254353 -0.35569367 -0.35524502 -0.47131032][-0.091063216 -0.11346143 -0.1762076 -0.25329328 -0.33683878 -0.36825863 -0.4354313 -0.5015462 -0.55407417 -0.45487702 -0.412433 -0.46703714 -0.44208789 -0.37023306 -0.31879774][-0.092766732 -0.10625373 -0.13473114 -0.17946054 -0.22553313 -0.20666115 -0.30587989 -0.28708488 -0.29362121 -0.31634924 -0.44268826 -0.44450098 -0.33168688 -0.2918154 -0.28617096]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:08:18.022807: step 10010, loss = 0.77, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:45m:09s remains)
INFO - root - 2017-12-06 05:08:20.293476: step 10020, loss = 0.87, batch loss = 0.66 (35.8 examples/sec; 0.223 sec/batch; 20h:01m:06s remains)
INFO - root - 2017-12-06 05:08:22.625031: step 10030, loss = 0.78, batch loss = 0.57 (35.1 examples/sec; 0.228 sec/batch; 20h:26m:41s remains)
INFO - root - 2017-12-06 05:08:24.962273: step 10040, loss = 0.86, batch loss = 0.64 (32.4 examples/sec; 0.247 sec/batch; 22h:06m:11s remains)
INFO - root - 2017-12-06 05:08:27.256526: step 10050, loss = 0.81, batch loss = 0.59 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-06 05:08:29.582924: step 10060, loss = 0.89, batch loss = 0.67 (37.0 examples/sec; 0.216 sec/batch; 19h:22m:15s remains)
INFO - root - 2017-12-06 05:08:31.843655: step 10070, loss = 0.75, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:55s remains)
INFO - root - 2017-12-06 05:08:34.127811: step 10080, loss = 0.79, batch loss = 0.57 (34.4 examples/sec; 0.232 sec/batch; 20h:48m:28s remains)
INFO - root - 2017-12-06 05:08:36.495873: step 10090, loss = 0.85, batch loss = 0.64 (34.5 examples/sec; 0.232 sec/batch; 20h:46m:37s remains)
INFO - root - 2017-12-06 05:08:38.840584: step 10100, loss = 0.76, batch loss = 0.54 (34.3 examples/sec; 0.233 sec/batch; 20h:52m:00s remains)
2017-12-06 05:08:39.209887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0358795 -1.3175677 -1.5368009 -1.6790607 -1.7428702 -1.8751918 -2.0113237 -2.1617088 -2.0336401 -1.7009392 -1.2744595 -1.115768 -1.0811814 -1.0890678 -1.0905761][-0.55615735 -0.87747282 -1.3747329 -1.5410254 -1.6913204 -2.0159247 -2.3609383 -2.6842289 -2.6524849 -2.5050254 -2.0370688 -1.6239202 -1.3816668 -1.2186408 -1.1067933][-0.51770657 -0.57619989 -0.7298913 -1.2093822 -1.4852411 -1.8653115 -2.2495487 -2.5808368 -2.8129203 -2.8454428 -2.6905096 -2.3397357 -2.0346949 -1.7350228 -1.508091][-0.36309195 -0.549176 -0.70771194 -0.97847039 -1.2140671 -1.4133673 -1.6679301 -1.8915154 -1.9818271 -2.1287169 -2.3290563 -2.3018079 -2.230413 -2.052918 -1.8569019][-0.3682816 -0.49779797 -0.80483985 -1.0093665 -1.2594804 -1.5465329 -1.6319991 -1.6944808 -1.79051 -1.7300503 -1.6935037 -1.7528989 -1.8418399 -1.7916906 -1.7614893][0.46250445 -0.13082759 -0.64729488 -0.89219838 -0.93907839 -0.83887047 -0.71883911 -0.72870481 -0.78469312 -1.0459597 -1.2033341 -1.1244608 -1.0058894 -1.0396618 -1.0439115][1.0647513 0.59893316 0.28282607 -0.027489327 -0.19088092 0.10378297 0.27209884 0.34376264 0.48284811 0.38420033 0.27128017 0.064222693 -0.13389012 -0.22239339 -0.25119478][0.8338154 0.78704953 0.78359652 0.92256361 1.0416392 0.98392504 0.86328757 0.97371405 1.1103357 1.0999347 1.1808223 1.0196717 0.92192119 0.7241599 0.47287995][0.60989529 0.70065373 0.89136088 1.1520141 1.3714429 1.4121237 1.5622712 1.5677112 1.5618172 1.818396 1.9422138 1.8456112 1.7116253 1.4655212 1.327258][0.24183287 0.59455061 0.99273735 1.2260569 1.2830533 1.3124218 1.2093891 1.2290149 1.3323226 1.3297578 1.3936914 1.6046783 1.6235523 1.3929017 1.1770298][0.24066119 0.074610054 0.31258923 0.43320465 0.53877103 0.42798126 0.43957388 0.27014947 0.09362711 0.15343595 0.3026647 0.43143165 0.57834494 0.87351316 0.92243761][0.73313725 0.08566457 -0.2614679 -0.30597788 -0.16220501 -0.25064963 -0.35493416 -0.47235709 -0.581251 -0.60266119 -0.47650188 -0.23622374 0.00034905225 0.077768341 0.22108309][2.015873 1.2594695 0.37282419 -0.48026955 -0.88443488 -0.98368704 -0.97464228 -1.1067253 -1.1950605 -1.2031971 -1.0921746 -0.87463552 -0.69050759 -0.50460619 -0.45052248][1.7001179 1.1875678 0.59110951 -0.18390407 -0.97488147 -1.678128 -1.9982469 -1.9488019 -1.8281496 -1.726424 -1.5188103 -1.248456 -1.1017882 -0.95805621 -0.94457376][1.7397677 1.1028656 0.22263847 -0.45563811 -1.1309844 -1.7794894 -2.0811486 -2.3844998 -2.4065058 -2.2241778 -2.0405505 -1.8408729 -1.7075257 -1.5686301 -1.5098523]]...]
INFO - root - 2017-12-06 05:08:41.509211: step 10110, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 20h:38m:47s remains)
INFO - root - 2017-12-06 05:08:43.806461: step 10120, loss = 0.73, batch loss = 0.52 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:51s remains)
INFO - root - 2017-12-06 05:08:46.071617: step 10130, loss = 0.79, batch loss = 0.57 (33.0 examples/sec; 0.243 sec/batch; 21h:44m:11s remains)
INFO - root - 2017-12-06 05:08:48.435347: step 10140, loss = 0.90, batch loss = 0.69 (33.4 examples/sec; 0.239 sec/batch; 21h:26m:08s remains)
INFO - root - 2017-12-06 05:08:50.722080: step 10150, loss = 0.76, batch loss = 0.55 (34.3 examples/sec; 0.233 sec/batch; 20h:53m:29s remains)
INFO - root - 2017-12-06 05:08:52.980060: step 10160, loss = 0.83, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 20h:31m:00s remains)
INFO - root - 2017-12-06 05:08:55.282026: step 10170, loss = 0.87, batch loss = 0.66 (34.1 examples/sec; 0.234 sec/batch; 20h:58m:37s remains)
INFO - root - 2017-12-06 05:08:57.556772: step 10180, loss = 0.79, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:42m:16s remains)
INFO - root - 2017-12-06 05:08:59.842031: step 10190, loss = 0.88, batch loss = 0.67 (34.6 examples/sec; 0.231 sec/batch; 20h:42m:30s remains)
INFO - root - 2017-12-06 05:09:02.140710: step 10200, loss = 0.83, batch loss = 0.61 (34.7 examples/sec; 0.231 sec/batch; 20h:40m:01s remains)
2017-12-06 05:09:02.587534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.085316978 -0.16265923 -0.2220341 -0.1836406 -0.1632027 -0.15111658 -0.19238725 -0.18467617 -0.22625667 -0.22734666 -0.20841338 -0.2062797 -0.19553557 -0.15553573 -0.1540451][-0.63629961 -0.59950322 -0.5164879 -0.39067128 -0.28503761 -0.2372337 -0.22798508 -0.20081678 -0.22814459 -0.23584592 -0.21160871 -0.18753165 -0.16178584 -0.18160589 -0.21325523][-0.73817337 -0.70199651 -0.59353828 -0.48533913 -0.36788079 -0.25975129 -0.21346417 -0.22098455 -0.22222117 -0.24463674 -0.24221829 -0.22973937 -0.23219657 -0.260902 -0.28155139][-0.59851027 -0.64938658 -0.63059664 -0.57007331 -0.51006413 -0.49090594 -0.46889615 -0.46556696 -0.4615382 -0.44787818 -0.40106493 -0.37008819 -0.32666162 -0.33213788 -0.31251085][-0.5182932 -0.56559396 -0.56084859 -0.54435694 -0.49456394 -0.51657468 -0.53590453 -0.6167286 -0.66935819 -0.67013109 -0.64049804 -0.570527 -0.49607038 -0.48691371 -0.47468153][-0.59093165 -0.56871825 -0.52675408 -0.37908685 -0.22570166 -0.12183905 -0.037419915 -0.098143779 -0.19809815 -0.35077313 -0.43814507 -0.42687848 -0.37840691 -0.3026756 -0.2906051][-0.47088784 -0.33731243 -0.19764921 0.038237818 0.27943233 0.554399 0.78619373 0.84225458 0.70823938 0.46882525 0.25849238 0.13554746 0.11243767 0.13230005 0.090880714][-0.25886512 -0.12270007 0.11109952 0.50686765 0.88024211 1.1998849 1.3786972 1.3856229 1.2732608 1.0025014 0.72557932 0.53069168 0.46876094 0.47310606 0.36744052][-0.16450441 -0.11931292 0.11500793 0.59079915 1.0120524 1.4213192 1.6189659 1.5692912 1.3854969 1.076625 0.81155443 0.61641085 0.52367014 0.51862091 0.42504159][-0.34898064 -0.43182355 -0.24711403 0.13706353 0.46691319 0.79512048 1.0108589 1.0196451 0.8612265 0.63818443 0.46870896 0.34501824 0.31280839 0.34137338 0.42208043][-0.5137682 -0.70097584 -0.61299825 -0.40003681 -0.25654161 -0.14867334 -0.11645667 -0.15902911 -0.26925528 -0.37919375 -0.424604 -0.36312976 -0.25493425 -0.132286 0.062731586][-0.78106713 -0.98098934 -0.90358108 -0.83443528 -0.79633284 -0.88370413 -0.98498154 -1.0810492 -1.1777945 -1.2161763 -1.1681546 -0.98497349 -0.75574487 -0.47965863 -0.16512337][-0.72622436 -0.91589582 -0.938292 -1.0073586 -1.1598783 -1.3676057 -1.5367117 -1.5913645 -1.6264341 -1.5826155 -1.4659585 -1.2491913 -1.0432353 -0.80539441 -0.45669615][-0.57382876 -0.65485561 -0.72431129 -0.84549814 -0.96466553 -1.0996177 -1.2655537 -1.3470521 -1.377023 -1.3395942 -1.2338293 -1.055717 -0.8966642 -0.76508421 -0.58819884][-0.48808008 -0.43794352 -0.42950737 -0.43293682 -0.45055476 -0.44064978 -0.42233744 -0.36584511 -0.32712829 -0.3775298 -0.50227588 -0.612477 -0.68485987 -0.71156871 -0.64963633]]...]
INFO - root - 2017-12-06 05:09:04.896306: step 10210, loss = 0.80, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 20h:12m:20s remains)
INFO - root - 2017-12-06 05:09:07.188710: step 10220, loss = 0.81, batch loss = 0.59 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:41s remains)
INFO - root - 2017-12-06 05:09:09.491211: step 10230, loss = 0.78, batch loss = 0.56 (35.5 examples/sec; 0.226 sec/batch; 20h:11m:26s remains)
INFO - root - 2017-12-06 05:09:11.744511: step 10240, loss = 0.83, batch loss = 0.61 (35.6 examples/sec; 0.224 sec/batch; 20h:05m:18s remains)
INFO - root - 2017-12-06 05:09:14.048831: step 10250, loss = 0.83, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:47m:04s remains)
INFO - root - 2017-12-06 05:09:16.394953: step 10260, loss = 0.88, batch loss = 0.67 (36.1 examples/sec; 0.222 sec/batch; 19h:49m:47s remains)
INFO - root - 2017-12-06 05:09:18.707745: step 10270, loss = 0.80, batch loss = 0.59 (33.9 examples/sec; 0.236 sec/batch; 21h:06m:41s remains)
INFO - root - 2017-12-06 05:09:20.981028: step 10280, loss = 0.87, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 20h:26m:18s remains)
INFO - root - 2017-12-06 05:09:23.252334: step 10290, loss = 0.77, batch loss = 0.56 (36.7 examples/sec; 0.218 sec/batch; 19h:29m:10s remains)
INFO - root - 2017-12-06 05:09:25.558519: step 10300, loss = 0.73, batch loss = 0.51 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:46s remains)
2017-12-06 05:09:25.937957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044597335 -0.043771863 -0.043204539 -0.042614739 -0.04210183 -0.04174557 -0.041687224 -0.042175636 -0.04323991 -0.044725418 -0.046556413 -0.048624665 -0.050748874 -0.052687105 -0.05474776][-0.041179329 -0.040053107 -0.039297771 -0.038596816 -0.038014825 -0.037603691 -0.037519716 -0.038075034 -0.039337676 -0.041201618 -0.043526582 -0.046162467 -0.048857812 -0.051292881 -0.053707898][-0.038804367 -0.037284035 -0.036334116 -0.035533678 -0.034818903 -0.034241371 -0.033955865 -0.034470234 -0.035942871 -0.038189746 -0.040937718 -0.044051949 -0.047266908 -0.050193068 -0.052930042][-0.036979116 -0.035122756 -0.033979248 -0.032940079 -0.031990465 -0.031173836 -0.030743729 -0.031250473 -0.032883383 -0.0354893 -0.038670279 -0.042261887 -0.046022881 -0.049469929 -0.052542437][-0.035737347 -0.033571463 -0.032168783 -0.03081841 -0.029519234 -0.02844445 -0.027897887 -0.028587878 -0.030509133 -0.03344585 -0.037033685 -0.041084312 -0.045337681 -0.049201764 -0.05255124][-0.035769258 -0.033522882 -0.031957176 -0.030159026 -0.028278038 -0.026679616 -0.025894042 -0.026771903 -0.02904151 -0.032308735 -0.036232509 -0.040649787 -0.045313384 -0.04948429 -0.053054146][-0.037337862 -0.035422258 -0.033991378 -0.031961992 -0.029656138 -0.027512722 -0.026210535 -0.026805405 -0.029023245 -0.032316469 -0.036355447 -0.040969126 -0.045812212 -0.05013426 -0.053788256][-0.040436532 -0.03905971 -0.037967194 -0.036084924 -0.03387114 -0.031865124 -0.030481111 -0.030556366 -0.032061309 -0.03466 -0.03814175 -0.042406008 -0.047005486 -0.0510549 -0.054593645][-0.044520456 -0.043379892 -0.04238157 -0.040572185 -0.038510054 -0.0369134 -0.03582396 -0.035621889 -0.036407828 -0.03816903 -0.040861748 -0.044406064 -0.048401695 -0.051947996 -0.055172708][-0.048974678 -0.047886215 -0.046824709 -0.044732705 -0.042440064 -0.040743977 -0.039769441 -0.039436892 -0.039750494 -0.040902589 -0.043000251 -0.045894597 -0.049235329 -0.052331571 -0.055326931][-0.05294143 -0.052292839 -0.051322579 -0.048964188 -0.046113517 -0.043945868 -0.042659752 -0.041972231 -0.041855138 -0.042490792 -0.044028711 -0.046340331 -0.049196139 -0.052094866 -0.055053703][-0.05618269 -0.055966578 -0.055061195 -0.052416302 -0.049146723 -0.046629149 -0.045045756 -0.044028535 -0.043587524 -0.043762561 -0.044678673 -0.046384912 -0.048748881 -0.051464617 -0.054446619][-0.058594961 -0.058636416 -0.057644211 -0.054559678 -0.050905321 -0.048018944 -0.046165738 -0.044952057 -0.044297822 -0.044228215 -0.044791557 -0.046098731 -0.048183642 -0.050794709 -0.053808533][-0.058989994 -0.05941622 -0.058612112 -0.055371772 -0.051253021 -0.047860857 -0.045548782 -0.044057209 -0.043250512 -0.043074537 -0.043592095 -0.044950433 -0.047181867 -0.049955804 -0.053145219][-0.058121819 -0.058713965 -0.058139332 -0.054957468 -0.050611854 -0.046870723 -0.04416379 -0.042396124 -0.041403215 -0.041108452 -0.041653816 -0.043223053 -0.045758508 -0.048893061 -0.0524009]]...]
INFO - root - 2017-12-06 05:09:28.300615: step 10310, loss = 0.81, batch loss = 0.59 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:25s remains)
INFO - root - 2017-12-06 05:09:30.584854: step 10320, loss = 0.84, batch loss = 0.62 (32.9 examples/sec; 0.243 sec/batch; 21h:46m:59s remains)
INFO - root - 2017-12-06 05:09:32.846917: step 10330, loss = 0.81, batch loss = 0.60 (35.5 examples/sec; 0.225 sec/batch; 20h:10m:07s remains)
INFO - root - 2017-12-06 05:09:35.144107: step 10340, loss = 0.75, batch loss = 0.53 (35.5 examples/sec; 0.225 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-06 05:09:37.518026: step 10350, loss = 0.82, batch loss = 0.60 (35.6 examples/sec; 0.225 sec/batch; 20h:06m:11s remains)
INFO - root - 2017-12-06 05:09:39.810981: step 10360, loss = 0.83, batch loss = 0.62 (35.5 examples/sec; 0.225 sec/batch; 20h:08m:58s remains)
INFO - root - 2017-12-06 05:09:42.125239: step 10370, loss = 0.80, batch loss = 0.58 (33.3 examples/sec; 0.240 sec/batch; 21h:29m:47s remains)
INFO - root - 2017-12-06 05:09:44.433710: step 10380, loss = 0.79, batch loss = 0.58 (34.3 examples/sec; 0.233 sec/batch; 20h:52m:54s remains)
INFO - root - 2017-12-06 05:09:46.716642: step 10390, loss = 0.84, batch loss = 0.62 (33.6 examples/sec; 0.238 sec/batch; 21h:18m:33s remains)
INFO - root - 2017-12-06 05:09:49.046346: step 10400, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 20h:50m:36s remains)
2017-12-06 05:09:49.428986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11351424 -0.13781473 -0.16555907 -0.1887334 -0.20053348 -0.19799207 -0.18156901 -0.15585193 -0.12890166 -0.10610326 -0.090936355 -0.0834645 -0.081781313 -0.085195422 -0.089948907][-0.15677728 -0.2047382 -0.26694435 -0.338522 -0.4019576 -0.43810225 -0.43041694 -0.38018274 -0.30663371 -0.22760345 -0.16164014 -0.11646742 -0.092859842 -0.087465696 -0.091236576][-0.19170651 -0.2368536 -0.29547483 -0.3893795 -0.512429 -0.63662159 -0.70970106 -0.70211446 -0.61724532 -0.4803023 -0.33502206 -0.21296842 -0.13525471 -0.10018797 -0.0913015][-0.18729444 -0.18176369 -0.17742042 -0.2244225 -0.34198388 -0.53535658 -0.72532237 -0.83513951 -0.83479786 -0.72265506 -0.54816812 -0.36477754 -0.22290561 -0.14024033 -0.10353926][-0.080139421 0.021513544 0.15862665 0.22815725 0.1748428 -0.035899837 -0.31721985 -0.57497323 -0.73303735 -0.7467854 -0.64423764 -0.46471179 -0.29467672 -0.17618459 -0.11364013][0.091812894 0.299036 0.57114685 0.78632617 0.86401248 0.70500362 0.39340726 0.028724 -0.29448393 -0.48424706 -0.52457964 -0.43413451 -0.30568039 -0.19583172 -0.12713699][0.22391412 0.49613318 0.84985125 1.1575055 1.3393693 1.3005893 1.043651 0.6487422 0.22413626 -0.095278293 -0.26560676 -0.27449405 -0.22067407 -0.15945086 -0.11468557][0.26018783 0.52873027 0.88074875 1.2164403 1.4593067 1.5037001 1.3139266 0.95317364 0.496933 0.13168976 -0.086628631 -0.13537547 -0.12647115 -0.095758744 -0.07460136][0.14674656 0.35833976 0.63265979 0.91649497 1.163149 1.2831624 1.1906271 0.89033711 0.49514577 0.13616493 -0.072267152 -0.10990932 -0.093207456 -0.0649153 -0.046691481][-0.056880467 0.060470641 0.22317958 0.41337427 0.5925293 0.72189522 0.72123384 0.55993938 0.28815696 0.014093705 -0.14936543 -0.17502779 -0.13194723 -0.0890704 -0.057127334][-0.22996539 -0.22290538 -0.17247745 -0.087432057 0.0071297958 0.11481674 0.17855135 0.14719212 0.022934355 -0.13293904 -0.21418527 -0.208384 -0.15249455 -0.10320772 -0.07560309][-0.30204594 -0.37711555 -0.421326 -0.44388288 -0.43172708 -0.34738407 -0.2521165 -0.18270338 -0.17282528 -0.21342073 -0.23725468 -0.2112833 -0.15460548 -0.11171053 -0.086448558][-0.26532078 -0.36350182 -0.46252644 -0.54159892 -0.576259 -0.54358995 -0.45274591 -0.33342826 -0.24072753 -0.2037984 -0.19165885 -0.16356245 -0.13133723 -0.10172842 -0.086349621][-0.18004635 -0.25189716 -0.32607079 -0.39527848 -0.45561138 -0.46047208 -0.41338226 -0.31867045 -0.22584984 -0.16626233 -0.13541402 -0.12184606 -0.10815834 -0.089452066 -0.080568507][-0.1116051 -0.14383203 -0.17727464 -0.20115142 -0.22706866 -0.23276006 -0.22593711 -0.1834819 -0.13421744 -0.108982 -0.091362312 -0.0910386 -0.091055915 -0.087345906 -0.080747031]]...]
INFO - root - 2017-12-06 05:09:51.771796: step 10410, loss = 0.78, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:35m:48s remains)
INFO - root - 2017-12-06 05:09:54.082646: step 10420, loss = 0.79, batch loss = 0.58 (36.5 examples/sec; 0.219 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-06 05:09:56.372069: step 10430, loss = 0.84, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 19h:59m:28s remains)
INFO - root - 2017-12-06 05:09:58.693973: step 10440, loss = 0.79, batch loss = 0.58 (33.2 examples/sec; 0.241 sec/batch; 21h:34m:48s remains)
INFO - root - 2017-12-06 05:10:00.970611: step 10450, loss = 0.82, batch loss = 0.60 (35.8 examples/sec; 0.223 sec/batch; 19h:58m:58s remains)
INFO - root - 2017-12-06 05:10:03.233556: step 10460, loss = 0.86, batch loss = 0.64 (34.6 examples/sec; 0.232 sec/batch; 20h:42m:37s remains)
INFO - root - 2017-12-06 05:10:05.503117: step 10470, loss = 0.82, batch loss = 0.61 (36.0 examples/sec; 0.222 sec/batch; 19h:53m:11s remains)
INFO - root - 2017-12-06 05:10:07.840419: step 10480, loss = 0.75, batch loss = 0.54 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:39s remains)
INFO - root - 2017-12-06 05:10:10.156537: step 10490, loss = 0.87, batch loss = 0.66 (34.0 examples/sec; 0.236 sec/batch; 21h:04m:05s remains)
INFO - root - 2017-12-06 05:10:12.445805: step 10500, loss = 0.84, batch loss = 0.63 (33.9 examples/sec; 0.236 sec/batch; 21h:07m:39s remains)
2017-12-06 05:10:13.058407: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.32406825 -0.42645082 -0.71588695 -0.86558008 -1.0984292 -1.3308791 -1.4876626 -1.3924326 -1.4636246 -1.4103125 -1.5449542 -2.1623008 -2.709569 -3.065238 -3.0829611][-0.03695007 -0.0074785128 0.16200271 0.11776521 0.01699958 -0.24014771 -0.70509112 -1.0262741 -1.0691272 -1.2391613 -1.3364856 -1.4337547 -1.4854308 -2.5718324 -3.0730493][0.41503006 0.64275211 0.80047816 0.97629905 1.0325859 0.80594003 0.56718767 0.11253083 -0.18729556 -0.42859653 -0.62165785 -0.62628359 -0.47826064 -0.9444375 -2.0303676][-0.13554919 0.50605011 1.0295846 1.1557355 1.3602792 1.437644 1.5090365 1.7187861 1.5376911 0.85261786 0.55432904 0.66768593 0.659121 0.36746976 -0.59033436][-0.55129421 -0.29542762 0.13172865 0.5497418 0.87766683 1.0741733 1.9770314 2.4600565 2.612371 2.7004244 2.2531359 1.760699 1.1777488 0.83241236 0.11147449][-0.98358417 -1.4861082 -1.6107649 -1.001117 -0.38185009 0.28425461 1.2559084 2.271462 3.142905 2.8696003 2.9212916 2.8228762 2.321486 1.5408332 0.89283019][-0.89767861 -1.1754127 -1.1746095 -0.86997449 -0.58575356 0.12207987 0.71968687 1.5038245 1.964491 2.6065371 2.6857178 2.5790186 2.0875149 1.7773433 0.89719981][-0.68758416 -0.93288767 -1.0533662 -0.56172466 0.012118109 0.18811184 0.506252 0.80863816 1.1826733 1.4132253 1.2547085 1.6266351 1.856374 1.96076 1.4178969][-1.0558029 -1.2697567 -0.86017227 -0.4105657 -0.0296481 0.35857254 0.6564123 0.52257007 0.21292785 0.82228535 1.1270095 0.95322454 0.8883732 1.3381439 1.4682878][-0.696719 -0.84974736 -0.80460328 -0.58403116 -0.511962 -0.4961969 -0.70975757 -0.53123271 -0.58214682 -0.66733217 -0.49335986 0.068158694 0.45726722 0.71848929 0.94415915][-1.1174252 -0.94096267 -0.95663345 -0.96717668 -0.83863455 -1.0964307 -1.3216813 -1.4666355 -1.4142994 -1.1843108 -1.4091879 -1.2336676 -0.91276067 -0.32009196 -0.1672738][-1.4611703 -1.6807114 -2.1057594 -2.0387452 -2.0056291 -2.1287947 -2.1243234 -2.2091558 -2.204711 -2.1094167 -1.9570038 -1.8906226 -1.8726394 -1.7176288 -1.8739163][-1.9306079 -2.0175939 -2.197051 -2.564101 -2.7387419 -2.7520034 -2.7814445 -2.8831739 -2.768214 -2.8840106 -2.7894542 -2.7662868 -2.9983623 -3.3645689 -3.6330268][-1.843697 -2.0014415 -2.6627059 -3.050112 -3.2948327 -3.5558991 -3.6306694 -3.6389303 -3.629416 -3.7890623 -3.6543107 -3.7959058 -4.0428119 -4.203424 -4.1384277][-2.3196819 -2.4548602 -2.848223 -3.2444801 -3.6780767 -3.9412985 -4.2150207 -4.3999124 -4.3946085 -4.4414592 -4.5579433 -4.4833565 -4.3830886 -4.5448394 -4.4906816]]...]
INFO - root - 2017-12-06 05:10:15.344806: step 10510, loss = 0.82, batch loss = 0.60 (35.8 examples/sec; 0.223 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-06 05:10:17.629255: step 10520, loss = 0.71, batch loss = 0.50 (35.4 examples/sec; 0.226 sec/batch; 20h:13m:17s remains)
INFO - root - 2017-12-06 05:10:19.939415: step 10530, loss = 0.79, batch loss = 0.58 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:46s remains)
INFO - root - 2017-12-06 05:10:22.226712: step 10540, loss = 0.78, batch loss = 0.57 (35.3 examples/sec; 0.227 sec/batch; 20h:15m:52s remains)
INFO - root - 2017-12-06 05:10:24.545526: step 10550, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 20h:47m:18s remains)
INFO - root - 2017-12-06 05:10:26.900305: step 10560, loss = 0.80, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:40m:50s remains)
INFO - root - 2017-12-06 05:10:29.200901: step 10570, loss = 0.79, batch loss = 0.58 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:32s remains)
INFO - root - 2017-12-06 05:10:31.520571: step 10580, loss = 0.78, batch loss = 0.57 (33.8 examples/sec; 0.237 sec/batch; 21h:10m:29s remains)
INFO - root - 2017-12-06 05:10:33.857292: step 10590, loss = 0.77, batch loss = 0.55 (34.2 examples/sec; 0.234 sec/batch; 20h:53m:29s remains)
INFO - root - 2017-12-06 05:10:36.168280: step 10600, loss = 0.86, batch loss = 0.65 (33.8 examples/sec; 0.237 sec/batch; 21h:10m:09s remains)
2017-12-06 05:10:36.534462: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12189674 0.24317086 0.34509617 0.39081758 0.36497027 0.25839207 0.1074521 -0.012663722 -0.12135561 -0.20189585 -0.29128897 -0.35463622 -0.36137423 -0.28611732 -0.19329074][0.012818605 0.14961252 0.28990704 0.36361051 0.32470214 0.21399954 0.058184378 -0.11101802 -0.27514434 -0.42471915 -0.57264632 -0.61505121 -0.606083 -0.49239689 -0.35948372][-0.18068883 -0.12600574 -0.057874717 0.016496152 0.041211404 0.00951308 -0.089550734 -0.2414878 -0.41853213 -0.57285273 -0.7050404 -0.71825844 -0.69531178 -0.57748079 -0.44757038][-0.27878168 -0.26681748 -0.23465681 -0.18529837 -0.17078891 -0.11823729 -0.12636371 -0.2037093 -0.32924071 -0.4728885 -0.61082017 -0.61864823 -0.61294848 -0.52139366 -0.42942][-0.36017987 -0.41067576 -0.42652938 -0.38889661 -0.34066528 -0.23527527 -0.16411912 -0.14260241 -0.19493298 -0.28004953 -0.38453072 -0.38545668 -0.39191741 -0.33543974 -0.27841192][-0.32621691 -0.40012744 -0.44113153 -0.43295386 -0.37399781 -0.20557451 -0.056716003 0.056884639 0.062683143 0.03997732 -0.067634523 -0.092615 -0.15312293 -0.19185352 -0.23741087][-0.24247637 -0.3055512 -0.34394109 -0.33380792 -0.27383524 -0.10265146 0.091427453 0.2789619 0.35651115 0.36019522 0.27890226 0.2245979 0.13670635 0.041110031 -0.0777816][-0.159122 -0.20260768 -0.23615095 -0.23680893 -0.15159166 0.038387142 0.27492726 0.52825183 0.69175375 0.77575552 0.72506934 0.6375863 0.50062174 0.34662691 0.11616362][-0.11374755 -0.13540193 -0.15087792 -0.14387427 -0.058475439 0.10600988 0.34116665 0.59473372 0.74034315 0.81285787 0.78113276 0.71633422 0.59396833 0.47162244 0.26857406][-0.092082955 -0.11277615 -0.1278851 -0.11819834 -0.045985982 0.093943752 0.30081898 0.5052532 0.65240455 0.7367267 0.70965427 0.64856148 0.51880181 0.41501924 0.28472763][-0.066566005 -0.097574674 -0.13112621 -0.14404015 -0.096096687 -0.0075744018 0.13257095 0.27131414 0.36194083 0.39562228 0.363061 0.31912604 0.24229291 0.18038699 0.12192961][-0.077520564 -0.09523765 -0.11612076 -0.14715192 -0.14266846 -0.1167929 -0.053995322 0.04541003 0.11332083 0.15522712 0.1213822 0.0684664 0.016547784 0.0091812536 -0.025097184][-0.089116409 -0.11040065 -0.14822954 -0.19355166 -0.2390621 -0.26251322 -0.24813932 -0.19708085 -0.13941517 -0.090003185 -0.072697438 -0.062729567 -0.085832976 -0.097521432 -0.10696545][-0.092232525 -0.10321489 -0.12603578 -0.15650004 -0.18792611 -0.21095085 -0.21832547 -0.21292177 -0.19300163 -0.16612241 -0.13738112 -0.11558634 -0.10809503 -0.10658702 -0.0949928][-0.08685226 -0.09098994 -0.10033547 -0.11106801 -0.12604737 -0.13479783 -0.13185149 -0.1220701 -0.10741957 -0.087967068 -0.078561485 -0.081940189 -0.097058177 -0.11962568 -0.1311537]]...]
INFO - root - 2017-12-06 05:10:38.810368: step 10610, loss = 0.71, batch loss = 0.50 (34.4 examples/sec; 0.233 sec/batch; 20h:48m:42s remains)
INFO - root - 2017-12-06 05:10:41.140144: step 10620, loss = 0.85, batch loss = 0.64 (33.2 examples/sec; 0.241 sec/batch; 21h:33m:02s remains)
INFO - root - 2017-12-06 05:10:43.458109: step 10630, loss = 0.78, batch loss = 0.57 (34.1 examples/sec; 0.234 sec/batch; 20h:56m:56s remains)
INFO - root - 2017-12-06 05:10:45.732978: step 10640, loss = 0.82, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:16s remains)
INFO - root - 2017-12-06 05:10:48.036394: step 10650, loss = 0.86, batch loss = 0.64 (33.8 examples/sec; 0.236 sec/batch; 21h:08m:32s remains)
INFO - root - 2017-12-06 05:10:50.394748: step 10660, loss = 0.81, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 20h:13m:11s remains)
INFO - root - 2017-12-06 05:10:52.678829: step 10670, loss = 0.82, batch loss = 0.61 (32.9 examples/sec; 0.243 sec/batch; 21h:44m:51s remains)
INFO - root - 2017-12-06 05:10:55.031709: step 10680, loss = 0.84, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 20h:00m:38s remains)
INFO - root - 2017-12-06 05:10:57.321693: step 10690, loss = 0.79, batch loss = 0.57 (33.6 examples/sec; 0.238 sec/batch; 21h:16m:52s remains)
INFO - root - 2017-12-06 05:10:59.688541: step 10700, loss = 0.79, batch loss = 0.58 (35.3 examples/sec; 0.226 sec/batch; 20h:13m:47s remains)
2017-12-06 05:11:00.052276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12683304 -0.15706076 -0.19995067 -0.25566012 -0.32688886 -0.39449862 -0.43585563 -0.44036713 -0.40112972 -0.32843229 -0.25656906 -0.19490111 -0.15429175 -0.14322737 -0.14485893][-0.16830467 -0.24068137 -0.31323624 -0.36675853 -0.4455063 -0.54181224 -0.63965583 -0.72863305 -0.76241994 -0.692834 -0.56970978 -0.414381 -0.2760554 -0.20461524 -0.18633825][-0.23701113 -0.34877473 -0.44800344 -0.50868046 -0.54866552 -0.60437369 -0.70512629 -0.86199284 -1.0070281 -1.0462174 -0.98221219 -0.77711618 -0.52196 -0.34959587 -0.25247809][-0.31853297 -0.45745209 -0.52739477 -0.53486192 -0.51392823 -0.46454567 -0.51115853 -0.68590558 -0.88766456 -1.0449605 -1.1270672 -1.0116403 -0.72904527 -0.4989582 -0.33756378][-0.36840576 -0.54239094 -0.5858807 -0.43804803 -0.18202689 0.097034678 0.21707833 0.093112782 -0.2516377 -0.60277891 -0.85390031 -0.89208519 -0.69552195 -0.51913959 -0.3544693][-0.38294929 -0.53863263 -0.57613242 -0.37191963 0.12776032 0.7403605 1.1147425 1.1614068 0.84248555 0.32660037 -0.13586843 -0.39241394 -0.41242772 -0.3645362 -0.27477974][-0.4095259 -0.54186773 -0.500339 -0.2260327 0.36032298 1.2369688 1.9731807 2.2136803 1.9415818 1.3417604 0.75500941 0.32609946 0.1207066 0.0012152195 -0.080415159][-0.37661561 -0.5090251 -0.43898031 -0.10439569 0.57543385 1.4906772 2.3973165 2.8716209 2.7430217 2.0733752 1.3888572 0.81280792 0.49952921 0.26194796 0.10083237][-0.31018633 -0.42829758 -0.39635432 -0.15173267 0.42521504 1.2321347 2.1058791 2.6513362 2.6590552 2.0901062 1.4019948 0.74584019 0.43074426 0.25238606 0.094025865][-0.29685342 -0.40563762 -0.38848653 -0.25389981 0.15861318 0.74354935 1.3914429 1.8927388 2.0100772 1.6538161 1.0948569 0.55404055 0.20206764 0.039392665 -0.027443688][-0.26676929 -0.38206705 -0.42704964 -0.38560197 -0.163403 0.20060921 0.64673448 0.97522974 1.1044533 0.96927881 0.521829 0.11368181 -0.10551739 -0.17386362 -0.15031366][-0.23022877 -0.33407974 -0.42912093 -0.48976564 -0.44897193 -0.27413166 -0.000771597 0.248155 0.41036433 0.35074118 0.047615185 -0.1632394 -0.27131039 -0.30594882 -0.21445593][-0.18490973 -0.27521271 -0.36899164 -0.45988366 -0.54825747 -0.53210938 -0.42220235 -0.26931712 -0.097098656 -0.060129896 -0.11425176 -0.15597543 -0.21136424 -0.26072109 -0.15048678][-0.13500419 -0.19077307 -0.25591868 -0.31108305 -0.3870286 -0.39768323 -0.37201709 -0.36189812 -0.33323723 -0.36711806 -0.36378816 -0.32742879 -0.29157442 -0.24195246 -0.16346496][-0.11124448 -0.12494561 -0.14488268 -0.18202572 -0.2281197 -0.21699579 -0.16232868 -0.17281896 -0.20647827 -0.26024812 -0.30206984 -0.32526082 -0.26315892 -0.22970687 -0.1859238]]...]
INFO - root - 2017-12-06 05:11:02.320685: step 10710, loss = 0.88, batch loss = 0.67 (36.3 examples/sec; 0.221 sec/batch; 19h:42m:56s remains)
INFO - root - 2017-12-06 05:11:04.600782: step 10720, loss = 0.82, batch loss = 0.61 (35.4 examples/sec; 0.226 sec/batch; 20h:13m:17s remains)
INFO - root - 2017-12-06 05:11:06.888357: step 10730, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 20h:35m:21s remains)
INFO - root - 2017-12-06 05:11:09.233160: step 10740, loss = 0.84, batch loss = 0.62 (33.7 examples/sec; 0.237 sec/batch; 21h:12m:11s remains)
INFO - root - 2017-12-06 05:11:11.491459: step 10750, loss = 0.84, batch loss = 0.62 (33.1 examples/sec; 0.242 sec/batch; 21h:37m:08s remains)
INFO - root - 2017-12-06 05:11:13.834520: step 10760, loss = 0.80, batch loss = 0.59 (35.7 examples/sec; 0.224 sec/batch; 20h:02m:49s remains)
INFO - root - 2017-12-06 05:11:16.145411: step 10770, loss = 0.83, batch loss = 0.61 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:48s remains)
INFO - root - 2017-12-06 05:11:18.444753: step 10780, loss = 0.81, batch loss = 0.60 (35.6 examples/sec; 0.225 sec/batch; 20h:04m:07s remains)
INFO - root - 2017-12-06 05:11:20.773875: step 10790, loss = 0.75, batch loss = 0.54 (33.6 examples/sec; 0.238 sec/batch; 21h:14m:54s remains)
INFO - root - 2017-12-06 05:11:23.086835: step 10800, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.230 sec/batch; 20h:34m:44s remains)
2017-12-06 05:11:23.564470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070876658 -0.076714456 -0.085339069 -0.096944548 -0.14442633 -0.21467845 -0.3199752 -0.41853303 -0.53365064 -0.59644103 -0.6492058 -0.65528959 -0.64852738 -0.55646586 -0.45292085][-0.091155738 -0.096140884 -0.10753065 -0.12922107 -0.19313562 -0.30530953 -0.45262665 -0.57713944 -0.69090813 -0.73406565 -0.74796689 -0.68759805 -0.66280907 -0.55669349 -0.45465636][-0.097364485 -0.10357555 -0.12783754 -0.16537699 -0.23605992 -0.36333656 -0.52191764 -0.64734209 -0.71757549 -0.70553082 -0.64292365 -0.5694536 -0.49348342 -0.37077212 -0.37123024][-0.12455126 -0.12662338 -0.1453214 -0.17495838 -0.22261487 -0.30357033 -0.416372 -0.50937033 -0.56038284 -0.5312224 -0.46923846 -0.3612482 -0.27592209 -0.24513465 -0.22359294][-0.14760271 -0.16143528 -0.18576211 -0.1891126 -0.16540608 -0.16622874 -0.1936079 -0.21591681 -0.21261333 -0.16360769 -0.13257022 0.024951436 0.15400806 0.23471047 0.27294546][-0.048656777 -0.061028957 -0.072809242 -0.0443789 0.017326549 0.069802031 0.080666408 0.09105362 0.13038509 0.22491427 0.25697154 0.4206292 0.57330889 0.68581945 0.70798063][-0.076728523 -0.07965938 -0.071034007 -0.019536428 0.072095618 0.15403162 0.21199124 0.29032296 0.36950153 0.47611529 0.53180653 0.67251629 0.73951876 0.77692169 0.74300265][0.049368143 0.066143319 0.053827837 0.078356519 0.13395207 0.20655312 0.25545126 0.34319025 0.41565734 0.48523545 0.50334448 0.60213858 0.693325 0.66207266 0.43630737][0.23790087 0.26844466 0.28485215 0.30899274 0.32387698 0.33559287 0.32302403 0.32357836 0.31554872 0.3186304 0.25013638 0.36459476 0.43095964 0.38747776 0.13734038][0.49992347 0.52869785 0.53573757 0.51277345 0.46037894 0.40171546 0.32924986 0.26414078 0.19650759 0.12711497 0.042312235 0.096798167 0.15293369 0.024044916 -0.2022306][0.69300652 0.71900755 0.68621123 0.60073149 0.48051387 0.3708272 0.26795089 0.17497377 0.0761587 0.0097505376 -0.0552716 -0.021891348 0.059717804 -0.099233195 -0.39185798][0.81701046 0.80728626 0.74377954 0.59783506 0.4702211 0.37640744 0.28476828 0.19801618 0.10041597 0.00047825277 -0.077080794 -0.038362443 -0.057550132 -0.22058071 -0.36869115][0.82197106 0.74224895 0.62097234 0.49862683 0.36813152 0.24011345 0.16301645 0.098461151 0.011051148 -0.092888683 -0.16687195 -0.16520944 -0.23596591 -0.38431728 -0.51660073][0.65105432 0.47101516 0.27349305 0.10399958 -0.069949672 -0.1696265 -0.25190777 -0.30176973 -0.31446791 -0.35412216 -0.40057415 -0.37094158 -0.44765043 -0.54020238 -0.56383741][0.30893207 0.11821027 -0.1169142 -0.3919338 -0.6451596 -0.75923842 -0.81660938 -0.75861984 -0.69667548 -0.6587835 -0.61219573 -0.57906955 -0.59902835 -0.744476 -0.76423347]]...]
INFO - root - 2017-12-06 05:11:25.867028: step 10810, loss = 0.82, batch loss = 0.61 (34.4 examples/sec; 0.232 sec/batch; 20h:45m:45s remains)
INFO - root - 2017-12-06 05:11:28.153092: step 10820, loss = 0.80, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 20h:07m:30s remains)
INFO - root - 2017-12-06 05:11:30.420122: step 10830, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:21m:48s remains)
INFO - root - 2017-12-06 05:11:32.740660: step 10840, loss = 0.88, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 21h:21m:52s remains)
INFO - root - 2017-12-06 05:11:35.025076: step 10850, loss = 0.88, batch loss = 0.67 (35.2 examples/sec; 0.228 sec/batch; 20h:19m:42s remains)
INFO - root - 2017-12-06 05:11:37.301547: step 10860, loss = 0.78, batch loss = 0.56 (35.3 examples/sec; 0.226 sec/batch; 20h:14m:10s remains)
INFO - root - 2017-12-06 05:11:39.605736: step 10870, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 20h:05m:46s remains)
INFO - root - 2017-12-06 05:11:41.944908: step 10880, loss = 0.82, batch loss = 0.61 (34.0 examples/sec; 0.235 sec/batch; 21h:01m:18s remains)
INFO - root - 2017-12-06 05:11:44.282678: step 10890, loss = 0.83, batch loss = 0.61 (34.4 examples/sec; 0.232 sec/batch; 20h:45m:11s remains)
INFO - root - 2017-12-06 05:11:46.594490: step 10900, loss = 0.83, batch loss = 0.62 (32.6 examples/sec; 0.246 sec/batch; 21h:55m:53s remains)
2017-12-06 05:11:46.985058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.50521052 -0.91028297 -1.1666267 -1.4176607 -1.5568447 -1.7741225 -1.6930947 -1.61742 -1.3579633 -1.0728803 -0.94915509 -0.92480248 -1.0502747 -1.3122501 -1.476433][-0.75363821 -1.3166246 -1.7857561 -2.0398483 -2.3079224 -2.4892311 -2.5527344 -2.2675695 -1.9427552 -1.8274839 -1.4656436 -1.2814565 -1.1534624 -0.84767413 -0.63191825][-2.0924125 -1.8406217 -2.0808344 -2.8000143 -3.1664298 -3.184634 -3.377233 -3.3958759 -3.167598 -2.5972941 -2.1084146 -1.873826 -1.5126143 -1.2694976 -0.82293725][-3.3950388 -3.4015875 -3.0848048 -2.7577567 -2.9359386 -3.5721807 -3.9767148 -3.7190881 -3.4097302 -3.2403662 -2.914273 -2.1962798 -1.6042962 -1.2173495 -0.91142672][-2.7910235 -2.9937129 -2.9157522 -2.707896 -2.5459416 -2.4769144 -2.7583537 -3.0307541 -3.1057184 -2.6690767 -2.3280025 -2.1431298 -1.7745693 -1.0323108 -0.40775895][-1.3635111 -1.5431185 -1.627147 -1.4882612 -1.2077236 -0.90687007 -0.8212117 -0.98423493 -1.1956053 -1.440011 -1.6462469 -1.2372971 -0.90738916 -0.7725783 -0.5835464][-0.21742965 -0.36856234 0.051459312 0.35960141 0.59381896 0.90739518 1.0997436 1.1946676 0.90586454 0.64890194 0.34529543 -0.11998069 -0.72904718 -0.67024809 -0.6017853][2.27685 1.3013954 0.62090111 0.861784 1.2377515 1.5758171 2.0090787 2.3526175 2.3347516 2.0406475 1.6696923 1.1430888 0.51268697 -0.33262229 -0.948184][2.2309277 1.8924837 1.522881 0.82271671 0.60959005 0.80618364 0.86481577 1.5172732 1.9846551 2.1271257 2.0279136 1.4960203 0.85227382 0.29520354 -0.44771883][0.73350495 0.19343579 0.0049352944 -0.21841978 -0.36737081 -0.33706355 -0.14160544 -0.2037589 -0.0447198 0.55481189 0.82580578 0.8089394 0.47542876 -0.16176228 -0.77317959][-0.99351257 -1.6062913 -1.6613214 -2.0320013 -2.0077875 -1.8340211 -1.7841239 -1.4333174 -1.2994738 -1.2579193 -0.852748 -0.32980275 0.0081278011 0.022451624 -0.35368252][-1.8769355 -2.4087222 -2.988251 -2.9657655 -2.5286748 -2.7025959 -3.0043948 -3.1471603 -2.7771437 -2.4707739 -2.0062222 -1.3319907 -0.62973994 -0.17273024 -0.013103463][-2.1730542 -2.4062531 -2.2975817 -2.3683579 -2.1815021 -1.7517886 -1.5705919 -2.230859 -2.5753653 -2.538249 -2.2548685 -1.9496615 -1.1564469 -0.092194349 0.61469293][-1.6927028 -1.9447279 -1.6680994 -1.5043728 -1.4578567 -1.318619 -0.74970144 -0.59158593 -0.72822231 -1.2744682 -1.5384219 -1.4255455 -1.0247265 -0.7998789 -0.1549836][-0.57931292 -0.89217919 -1.1538775 -1.1053879 -0.81952453 -0.7929523 -0.62360203 -0.57959038 -0.59758294 -0.75425935 -0.85375261 -0.93585014 -0.6961754 -0.49258152 -0.49839482]]...]
INFO - root - 2017-12-06 05:11:49.326037: step 10910, loss = 0.79, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:01s remains)
INFO - root - 2017-12-06 05:11:51.647761: step 10920, loss = 0.84, batch loss = 0.63 (32.8 examples/sec; 0.244 sec/batch; 21h:47m:19s remains)
INFO - root - 2017-12-06 05:11:53.995344: step 10930, loss = 0.76, batch loss = 0.55 (32.7 examples/sec; 0.245 sec/batch; 21h:52m:37s remains)
INFO - root - 2017-12-06 05:11:56.343412: step 10940, loss = 0.76, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:17s remains)
INFO - root - 2017-12-06 05:11:58.615014: step 10950, loss = 0.78, batch loss = 0.56 (35.6 examples/sec; 0.225 sec/batch; 20h:04m:01s remains)
INFO - root - 2017-12-06 05:12:00.925126: step 10960, loss = 0.84, batch loss = 0.62 (34.4 examples/sec; 0.233 sec/batch; 20h:46m:43s remains)
INFO - root - 2017-12-06 05:12:03.229064: step 10970, loss = 0.78, batch loss = 0.57 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:27s remains)
INFO - root - 2017-12-06 05:12:05.539515: step 10980, loss = 0.83, batch loss = 0.62 (32.6 examples/sec; 0.245 sec/batch; 21h:55m:00s remains)
INFO - root - 2017-12-06 05:12:07.881525: step 10990, loss = 0.82, batch loss = 0.61 (35.0 examples/sec; 0.229 sec/batch; 20h:25m:59s remains)
INFO - root - 2017-12-06 05:12:10.194425: step 11000, loss = 0.76, batch loss = 0.55 (34.3 examples/sec; 0.233 sec/batch; 20h:48m:58s remains)
2017-12-06 05:12:11.717155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14622584 -0.1790584 -0.21318859 -0.23908594 -0.25250578 -0.25279596 -0.23765141 -0.20696658 -0.17218241 -0.14226672 -0.12251891 -0.10964957 -0.10190271 -0.097644724 -0.095407575][-0.19700807 -0.24114001 -0.28134978 -0.31447938 -0.34615162 -0.37381023 -0.38431481 -0.36759025 -0.32374695 -0.26529205 -0.218176 -0.17907095 -0.14607459 -0.12669404 -0.11200962][-0.24042538 -0.27013019 -0.28705484 -0.31021404 -0.36372513 -0.43102756 -0.48789841 -0.52884763 -0.51875186 -0.45540813 -0.37853593 -0.30044815 -0.22342187 -0.18004091 -0.15113428][-0.22284415 -0.1909422 -0.13853012 -0.11964868 -0.18867928 -0.28692439 -0.38066283 -0.48045304 -0.54076517 -0.52360266 -0.46267873 -0.38394621 -0.28122863 -0.22088408 -0.18658549][-0.15286273 -0.031147379 0.13433698 0.24047369 0.21344575 0.12594602 0.026412159 -0.12405023 -0.2693626 -0.36396667 -0.38504365 -0.36807808 -0.2832863 -0.2216455 -0.18599153][-0.076814711 0.11419424 0.3661997 0.57204843 0.65773678 0.66556036 0.62377894 0.47980165 0.2974 0.091382273 -0.05602099 -0.15877762 -0.16628563 -0.15806907 -0.15927771][-0.027038857 0.19097424 0.48091328 0.75552511 0.9597466 1.0932367 1.1612802 1.0940785 0.91855085 0.65250862 0.40257969 0.17487946 0.052979209 -0.013408713 -0.068536863][-0.050599236 0.13958588 0.40806904 0.68454635 0.92764533 1.1246951 1.268845 1.2904782 1.1797874 0.95859694 0.68957973 0.42386812 0.23115033 0.1161826 0.025046639][-0.13521782 -0.023061588 0.17246398 0.39086989 0.60013926 0.78175527 0.91535187 0.98414087 0.95286894 0.85994679 0.70048386 0.49876571 0.31797412 0.1931372 0.083576642][-0.224668 -0.16866121 -0.047313254 0.10042054 0.25741395 0.399773 0.52206451 0.63619322 0.65354049 0.64055336 0.5591386 0.43151754 0.28156975 0.16937542 0.067300834][-0.29100066 -0.2962569 -0.24083456 -0.15041211 -0.050241392 0.05542811 0.15919232 0.26719946 0.31600788 0.34436744 0.30479091 0.22485352 0.11460259 0.03708192 -0.02037406][-0.32199076 -0.35553628 -0.37351963 -0.35342118 -0.30116391 -0.23690698 -0.1484372 -0.04555906 0.02348955 0.066028096 0.060837172 0.03336253 -0.028029073 -0.085275844 -0.11125021][-0.30643198 -0.37526676 -0.42543855 -0.43738461 -0.44546676 -0.42052737 -0.36649239 -0.29550865 -0.21918279 -0.15539286 -0.11263698 -0.10380085 -0.10152987 -0.11818194 -0.12453386][-0.21751648 -0.28144652 -0.36148354 -0.4145706 -0.43530515 -0.41867748 -0.39101714 -0.3571099 -0.31783766 -0.26952371 -0.20607471 -0.17046206 -0.13477427 -0.125501 -0.1167777][-0.12044868 -0.15136229 -0.19722837 -0.23954225 -0.28247833 -0.28798988 -0.27472287 -0.23521084 -0.20227101 -0.18399459 -0.13449763 -0.1249138 -0.099594064 -0.08835613 -0.089668758]]...]
INFO - root - 2017-12-06 05:12:13.993269: step 11010, loss = 0.82, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:38m:54s remains)
INFO - root - 2017-12-06 05:12:16.337756: step 11020, loss = 0.82, batch loss = 0.61 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:18s remains)
INFO - root - 2017-12-06 05:12:18.625286: step 11030, loss = 0.79, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:39m:24s remains)
INFO - root - 2017-12-06 05:12:20.897056: step 11040, loss = 0.85, batch loss = 0.63 (37.4 examples/sec; 0.214 sec/batch; 19h:04m:30s remains)
INFO - root - 2017-12-06 05:12:23.176355: step 11050, loss = 0.77, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 20h:15m:13s remains)
INFO - root - 2017-12-06 05:12:25.475344: step 11060, loss = 0.81, batch loss = 0.59 (34.0 examples/sec; 0.236 sec/batch; 21h:01m:58s remains)
INFO - root - 2017-12-06 05:12:27.798357: step 11070, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:22s remains)
INFO - root - 2017-12-06 05:12:30.097008: step 11080, loss = 0.82, batch loss = 0.61 (37.2 examples/sec; 0.215 sec/batch; 19h:11m:25s remains)
INFO - root - 2017-12-06 05:12:32.382227: step 11090, loss = 0.82, batch loss = 0.60 (33.3 examples/sec; 0.241 sec/batch; 21h:28m:46s remains)
INFO - root - 2017-12-06 05:12:34.658271: step 11100, loss = 0.87, batch loss = 0.65 (35.8 examples/sec; 0.223 sec/batch; 19h:55m:48s remains)
2017-12-06 05:12:35.209251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9829805 -3.9456716 -3.81445 -3.7447984 -3.6212373 -3.1092436 -2.3925953 -1.8966804 -1.5403159 -1.4657892 -1.4386095 -1.3773627 -1.2615741 -1.1518832 -1.0147748][-3.8694332 -4.4397464 -4.8537931 -4.9663706 -4.7315507 -4.5323877 -4.3040357 -3.7050612 -3.0140429 -2.544754 -2.0531645 -1.7363589 -1.4631974 -1.3202028 -1.0878075][-2.9344859 -3.3868361 -3.9057095 -4.6083975 -5.0430503 -5.1901088 -4.9024453 -4.7308936 -4.4800014 -3.836128 -2.9125264 -2.3169215 -1.6238933 -1.2884538 -1.0583299][-1.5602047 -1.950493 -2.216537 -2.746428 -3.3127544 -3.9930582 -4.4786682 -4.8474197 -4.4878325 -4.0482492 -3.3709309 -2.7158082 -1.770651 -1.3993108 -1.0602287][-0.43706247 -0.30970088 -0.50151139 -0.74083936 -0.97057271 -1.5731044 -2.1002178 -2.8564482 -3.4696324 -3.50011 -2.7766621 -2.3400502 -1.8399249 -1.3718752 -0.89569569][-0.37993935 0.16940811 0.70566297 0.80185318 0.74078536 0.79063308 0.93482369 0.41282919 -0.2550056 -0.99539572 -1.3300941 -1.2179651 -0.84435791 -0.81260991 -0.61258769][-0.25559783 0.08566799 0.77436268 1.583935 2.4192855 2.8892336 3.0543635 2.9260204 2.8109717 2.0057991 0.82981151 0.26336077 0.30228364 0.31199807 0.44381467][-0.10318036 0.010871589 0.45903406 1.4141135 2.7093146 3.7921395 4.6884651 4.7660546 4.2746453 3.7230437 2.9262068 1.9548037 1.1754487 0.83304524 0.74435729][0.003753975 -0.2041502 0.12811831 0.86368412 1.9643278 3.2863789 4.512711 5.0836143 5.0680985 4.393116 3.4238818 2.6209621 1.7637539 1.2310268 1.10578][0.32419503 -0.028895929 -0.0062933266 0.28364968 0.93484885 1.82424 2.8798263 3.6704173 3.970962 3.616745 2.8388555 2.0778215 1.4167434 1.0422238 0.91020012][-0.16135359 -0.019380167 -0.050825052 -0.17151947 -0.0087398887 0.23855448 0.66549486 0.97985893 1.2607021 1.3883809 1.2037995 1.0285544 0.89916605 0.71292359 0.44924566][-0.557884 -0.843192 -1.0523844 -1.2097324 -1.2511193 -1.4696784 -1.3843765 -1.0817302 -0.75401837 -0.43773225 -0.19314286 -0.00739599 0.062134512 0.19173893 0.10128816][-1.0912718 -1.1061239 -1.2769022 -1.5621504 -2.0510874 -2.3756506 -2.5681107 -2.6317296 -2.5013969 -1.951375 -1.339121 -0.91219842 -0.61001188 -0.37723082 -0.17002793][-2.1361094 -1.9981507 -1.7742409 -1.6983361 -1.773892 -1.8829396 -2.1558752 -2.3862286 -2.3701093 -2.1954625 -1.9668328 -1.6043358 -1.1215563 -0.71236622 -0.43976322][-1.7845893 -2.2101703 -2.3416822 -2.3685009 -2.1954126 -2.1358449 -1.8079628 -1.6070774 -1.43541 -1.3535875 -1.2836307 -1.125514 -1.024339 -0.83443195 -0.49427214]]...]
INFO - root - 2017-12-06 05:12:37.494644: step 11110, loss = 0.87, batch loss = 0.66 (31.9 examples/sec; 0.251 sec/batch; 22h:22m:30s remains)
INFO - root - 2017-12-06 05:12:39.800096: step 11120, loss = 0.78, batch loss = 0.57 (34.9 examples/sec; 0.229 sec/batch; 20h:27m:01s remains)
INFO - root - 2017-12-06 05:12:42.082672: step 11130, loss = 0.78, batch loss = 0.57 (36.7 examples/sec; 0.218 sec/batch; 19h:27m:37s remains)
INFO - root - 2017-12-06 05:12:44.441936: step 11140, loss = 0.81, batch loss = 0.60 (34.8 examples/sec; 0.230 sec/batch; 20h:31m:49s remains)
INFO - root - 2017-12-06 05:12:46.746520: step 11150, loss = 0.76, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:52m:52s remains)
INFO - root - 2017-12-06 05:12:49.063084: step 11160, loss = 0.82, batch loss = 0.61 (35.7 examples/sec; 0.224 sec/batch; 19h:59m:02s remains)
INFO - root - 2017-12-06 05:12:51.383125: step 11170, loss = 0.81, batch loss = 0.60 (35.2 examples/sec; 0.227 sec/batch; 20h:16m:33s remains)
INFO - root - 2017-12-06 05:12:53.673868: step 11180, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:19s remains)
INFO - root - 2017-12-06 05:12:56.038800: step 11190, loss = 0.75, batch loss = 0.53 (32.3 examples/sec; 0.248 sec/batch; 22h:07m:09s remains)
INFO - root - 2017-12-06 05:12:58.313984: step 11200, loss = 0.80, batch loss = 0.59 (36.5 examples/sec; 0.219 sec/batch; 19h:33m:27s remains)
2017-12-06 05:12:59.212088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10581726 -0.10716773 -0.10705298 -0.10507344 -0.10185407 -0.0980767 -0.093630292 -0.088516526 -0.083039939 -0.07734897 -0.071749628 -0.066741325 -0.062575586 -0.059896141 -0.05871639][-0.087869287 -0.089060105 -0.088800654 -0.086785786 -0.08369229 -0.080415316 -0.076839261 -0.073219247 -0.069770783 -0.066536121 -0.06350214 -0.06090726 -0.058853582 -0.057587821 -0.057291545][-0.07223127 -0.073078632 -0.072642058 -0.07045421 -0.067533694 -0.064691283 -0.061899103 -0.059592426 -0.057904013 -0.056889217 -0.0561744 -0.055781145 -0.05565444 -0.055727255 -0.056225762][-0.05906716 -0.059480898 -0.059273519 -0.057601988 -0.055402525 -0.053401709 -0.051533006 -0.050242558 -0.049748816 -0.050147012 -0.050980784 -0.052032977 -0.05319396 -0.054319058 -0.055503175][-0.050152715 -0.050229713 -0.050138187 -0.0491385 -0.04788303 -0.046781115 -0.045675863 -0.045125231 -0.045422956 -0.046646092 -0.048181474 -0.049883951 -0.051735591 -0.053464811 -0.055069618][-0.046015531 -0.045759168 -0.045639202 -0.044947449 -0.044210657 -0.04358929 -0.042894259 -0.042720698 -0.043393932 -0.044968441 -0.046778623 -0.048762284 -0.050907347 -0.052875508 -0.054655958][-0.044562832 -0.044138588 -0.044095822 -0.043728217 -0.043472681 -0.043143615 -0.042567309 -0.042333476 -0.042999893 -0.044467289 -0.046235278 -0.048233807 -0.050491881 -0.052583322 -0.05446722][-0.044822618 -0.044327434 -0.044415686 -0.044186823 -0.044167195 -0.044012 -0.043560337 -0.043325081 -0.04386387 -0.045099195 -0.046615042 -0.048429996 -0.050609592 -0.052625183 -0.054481793][-0.045380183 -0.044734366 -0.0447246 -0.044629894 -0.044704251 -0.0444594 -0.04403555 -0.043911524 -0.044551242 -0.04573724 -0.047167622 -0.048936877 -0.050992634 -0.052896485 -0.054614026][-0.045900393 -0.045117069 -0.045047291 -0.044997931 -0.045070231 -0.044727668 -0.044271797 -0.044179942 -0.044864129 -0.04609422 -0.047675475 -0.049553983 -0.051607724 -0.05341937 -0.055051893][-0.046123121 -0.045267608 -0.045322131 -0.045383178 -0.045466382 -0.045111217 -0.044615515 -0.044568177 -0.04523541 -0.046424806 -0.047947414 -0.049902234 -0.052052993 -0.053918257 -0.055521116][-0.045870986 -0.045136277 -0.045265369 -0.045452975 -0.045602351 -0.045344137 -0.04499469 -0.044965688 -0.045614906 -0.046793234 -0.048387762 -0.050384022 -0.052528981 -0.054393183 -0.056029562][-0.044580679 -0.043871496 -0.044092018 -0.044447713 -0.044762142 -0.0447303 -0.044529941 -0.044791095 -0.045561962 -0.046972491 -0.048798069 -0.050918363 -0.053222191 -0.055145234 -0.056889489][-0.043051396 -0.042246129 -0.042471115 -0.043039393 -0.043488987 -0.043662939 -0.043729976 -0.044179913 -0.04509585 -0.046671476 -0.048766837 -0.051079318 -0.05361085 -0.055770949 -0.057662033][-0.041825157 -0.040758945 -0.040794272 -0.041248761 -0.041699842 -0.042094808 -0.042424113 -0.043102115 -0.044243708 -0.046096042 -0.048440494 -0.051008787 -0.053804222 -0.056142766 -0.058260307]]...]
INFO - root - 2017-12-06 05:13:01.546054: step 11210, loss = 0.70, batch loss = 0.49 (31.7 examples/sec; 0.252 sec/batch; 22h:31m:56s remains)
INFO - root - 2017-12-06 05:13:03.842824: step 11220, loss = 0.80, batch loss = 0.58 (35.3 examples/sec; 0.227 sec/batch; 20h:13m:39s remains)
INFO - root - 2017-12-06 05:13:06.129852: step 11230, loss = 0.83, batch loss = 0.62 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-06 05:13:08.436683: step 11240, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 20h:35m:20s remains)
INFO - root - 2017-12-06 05:13:10.735799: step 11250, loss = 0.78, batch loss = 0.57 (32.1 examples/sec; 0.249 sec/batch; 22h:15m:16s remains)
INFO - root - 2017-12-06 05:13:13.036402: step 11260, loss = 0.80, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 20h:34m:30s remains)
INFO - root - 2017-12-06 05:13:15.330427: step 11270, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 20h:33m:48s remains)
INFO - root - 2017-12-06 05:13:17.701819: step 11280, loss = 0.80, batch loss = 0.58 (34.4 examples/sec; 0.233 sec/batch; 20h:46m:40s remains)
INFO - root - 2017-12-06 05:13:19.976644: step 11290, loss = 0.77, batch loss = 0.56 (35.9 examples/sec; 0.223 sec/batch; 19h:54m:33s remains)
INFO - root - 2017-12-06 05:13:22.246433: step 11300, loss = 0.76, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:42m:41s remains)
2017-12-06 05:13:22.613901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4504374 -1.8608423 -2.2293534 -2.4517982 -2.457727 -2.160265 -1.7494737 -1.3662617 -1.1319045 -1.2481099 -1.2628953 -1.2498797 -1.171646 -1.2161843 -1.1456225][-1.0871347 -1.5227926 -2.0482941 -2.3234129 -2.7116668 -3.0302093 -2.960309 -2.5491917 -1.9005978 -1.437909 -1.2680621 -1.281749 -1.1755642 -1.301981 -1.5552862][-1.4109542 -1.5852367 -1.9780979 -2.5800908 -3.0209329 -3.4174969 -3.7061996 -3.6722665 -3.6770544 -3.3098261 -2.7933822 -2.445142 -2.2945158 -1.9763433 -1.4320205][-0.89034456 -1.3797888 -1.9881698 -2.2431562 -2.776726 -3.3738163 -3.6222928 -3.5510023 -3.3972321 -3.1593704 -3.0862722 -2.9650061 -2.6755447 -2.5167882 -2.4808347][-1.132261 -0.92220777 -1.0743557 -1.5851159 -1.9041312 -2.3760545 -2.9534965 -2.9874029 -2.8224671 -2.6367772 -2.2158849 -1.698525 -1.7495594 -1.7733375 -1.6987766][-1.1784127 -0.82100308 -0.4008418 -0.075792588 0.038144447 0.078223296 -0.029516052 -0.32322463 -0.45785064 -0.58836895 -0.65549135 -0.50972074 -0.17593606 0.063552372 -0.073515505][-0.025306366 -0.21594352 0.25248268 0.76773864 1.2784617 1.5612205 1.9400556 1.9158676 1.8077317 1.5757145 1.4119656 1.1751508 0.99422073 0.55111879 0.33067143][1.4560354 1.5199381 1.6286993 1.6616292 2.1732523 2.5697851 2.7558241 2.7152653 2.7012854 2.4202285 2.16218 1.8759396 1.5454491 1.335606 1.1784728][1.4392877 1.7137101 1.7768016 1.7645345 1.7866552 1.7845829 1.80473 1.7745765 1.7002355 1.4173546 1.1050147 0.71291715 0.58035821 0.52316874 0.38244829][1.3030393 1.8478284 2.2220991 2.1086473 1.8259808 1.432605 1.3469276 1.1856288 0.95545387 0.60975808 0.15203732 -0.47355059 -1.0730138 -1.3962405 -1.2925025][1.2456965 1.7429686 1.8379087 1.8762192 1.5622237 1.3764354 0.95587027 0.74600583 0.54424894 0.17460451 -0.11500013 -0.35992387 -0.7349906 -1.4843999 -1.9108893][0.6780417 0.92280591 1.1500354 1.3329889 1.3363243 0.85186005 0.5566321 0.19924122 -0.20649326 -0.52614814 -0.89494544 -1.2045944 -1.0410684 -0.98828131 -1.2014219][0.87170053 0.42512903 0.17970836 0.50257212 0.69022149 1.3656285 1.5077311 1.1425898 0.69524527 0.4064678 0.066038914 -0.20061749 -0.35328704 -0.55587614 -0.55575305][1.2811985 0.9874022 0.66540271 0.16342613 -0.30867538 0.19133443 0.97532284 1.3261474 1.3670474 1.3859855 1.2506256 0.98588288 0.83513677 0.47444794 0.27824774][-0.13044885 -0.0066993386 0.19038841 0.012100466 0.017371461 -0.19176286 -0.13085747 0.22148147 0.62396586 1.0656635 1.3630793 1.3900169 1.4122237 1.3075213 0.81772524]]...]
INFO - root - 2017-12-06 05:13:24.942268: step 11310, loss = 0.78, batch loss = 0.57 (35.6 examples/sec; 0.224 sec/batch; 20h:01m:24s remains)
INFO - root - 2017-12-06 05:13:27.226270: step 11320, loss = 0.79, batch loss = 0.57 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:22s remains)
INFO - root - 2017-12-06 05:13:29.590127: step 11330, loss = 0.74, batch loss = 0.53 (33.5 examples/sec; 0.239 sec/batch; 21h:19m:00s remains)
INFO - root - 2017-12-06 05:13:31.953961: step 11340, loss = 0.89, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 20h:25m:39s remains)
INFO - root - 2017-12-06 05:13:34.251188: step 11350, loss = 0.80, batch loss = 0.58 (35.5 examples/sec; 0.226 sec/batch; 20h:07m:40s remains)
INFO - root - 2017-12-06 05:13:36.556930: step 11360, loss = 0.84, batch loss = 0.63 (34.4 examples/sec; 0.232 sec/batch; 20h:43m:24s remains)
INFO - root - 2017-12-06 05:13:38.850434: step 11370, loss = 0.77, batch loss = 0.56 (36.1 examples/sec; 0.221 sec/batch; 19h:44m:29s remains)
INFO - root - 2017-12-06 05:13:41.141599: step 11380, loss = 0.80, batch loss = 0.58 (33.9 examples/sec; 0.236 sec/batch; 21h:04m:02s remains)
INFO - root - 2017-12-06 05:13:43.448620: step 11390, loss = 0.93, batch loss = 0.72 (35.7 examples/sec; 0.224 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-06 05:13:45.752820: step 11400, loss = 0.76, batch loss = 0.55 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:34s remains)
2017-12-06 05:13:46.146729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.09355849 -0.093097 -0.0930384 -0.09335579 -0.09466242 -0.097715542 -0.10213516 -0.1062194 -0.10964184 -0.11145775 -0.11075234 -0.10685879 -0.099882148 -0.093954675 -0.092184775][-0.095030822 -0.094392009 -0.094335593 -0.0984944 -0.10896327 -0.12131551 -0.13227884 -0.14033458 -0.15662828 -0.1759163 -0.1866013 -0.1781292 -0.14875418 -0.11331093 -0.0960661][-0.098330349 -0.099668756 -0.10614537 -0.12423944 -0.15316984 -0.17082343 -0.17061897 -0.15916252 -0.18069929 -0.23918891 -0.29492378 -0.30554172 -0.25700793 -0.17488524 -0.11845409][-0.10332616 -0.12332308 -0.16081521 -0.20344347 -0.24550872 -0.22998212 -0.16018403 -0.0845402 -0.098113582 -0.22310923 -0.389486 -0.48203242 -0.44674915 -0.31132412 -0.18252039][-0.11226833 -0.14734983 -0.18602622 -0.18861605 -0.14008752 0.023807697 0.26321685 0.48627609 0.53864223 0.32516634 -0.061726496 -0.38675237 -0.48474497 -0.38854641 -0.23253462][-0.11731015 -0.15061866 -0.1348497 -0.0084040165 0.21383788 0.57664353 1.0177882 1.4089119 1.5432864 1.284842 0.72233313 0.12997425 -0.22016723 -0.29644102 -0.20216945][-0.12125885 -0.15619093 -0.10284644 0.1084307 0.43978083 0.927833 1.4948159 1.9810674 2.1744757 1.915616 1.2797599 0.55285549 0.050443381 -0.15015489 -0.13346717][-0.12690936 -0.17970997 -0.14649905 0.041196167 0.36156666 0.85217696 1.4628403 1.9933941 2.2273831 1.9896865 1.3517543 0.62451422 0.13337721 -0.080881484 -0.085526034][-0.1422364 -0.23496395 -0.28918171 -0.24286357 -0.062905408 0.31078279 0.85138494 1.3577539 1.6331719 1.489356 0.99761778 0.38878024 0.0018229559 -0.13209532 -0.0856829][-0.15270984 -0.2706542 -0.40418696 -0.49762064 -0.50420481 -0.3475644 0.049901336 0.51426107 0.82589018 0.803003 0.50936407 0.092563614 -0.15615126 -0.22404444 -0.14244615][-0.14490955 -0.2522946 -0.397403 -0.55447245 -0.6775316 -0.682695 -0.48369741 -0.18351862 0.10070969 0.20025481 0.10301337 -0.11040012 -0.23685758 -0.25365126 -0.16884625][-0.12604094 -0.20027494 -0.31699264 -0.47278041 -0.65111053 -0.78824031 -0.78017277 -0.64898157 -0.45744121 -0.30622584 -0.23356275 -0.25235581 -0.25999179 -0.23174383 -0.16832715][-0.1089108 -0.14957333 -0.22147968 -0.34312183 -0.51139241 -0.68244457 -0.7769388 -0.77814567 -0.6761651 -0.53442121 -0.3960827 -0.30985469 -0.23010048 -0.17380092 -0.13887914][-0.096796453 -0.11255399 -0.14675705 -0.2027587 -0.29879022 -0.4060843 -0.49243003 -0.5337261 -0.50844693 -0.43365628 -0.34307915 -0.26500705 -0.18573238 -0.13703223 -0.10601793][-0.092641361 -0.095602438 -0.10386469 -0.12164719 -0.15959629 -0.18145052 -0.22016406 -0.2339603 -0.20851885 -0.18984997 -0.16327477 -0.1318374 -0.11812344 -0.11398274 -0.090030134]]...]
INFO - root - 2017-12-06 05:13:48.417773: step 11410, loss = 0.84, batch loss = 0.63 (34.5 examples/sec; 0.232 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-06 05:13:50.728731: step 11420, loss = 0.86, batch loss = 0.65 (33.8 examples/sec; 0.237 sec/batch; 21h:05m:35s remains)
INFO - root - 2017-12-06 05:13:53.076147: step 11430, loss = 0.83, batch loss = 0.61 (33.8 examples/sec; 0.236 sec/batch; 21h:04m:46s remains)
INFO - root - 2017-12-06 05:13:55.418289: step 11440, loss = 0.81, batch loss = 0.59 (34.2 examples/sec; 0.234 sec/batch; 20h:50m:39s remains)
INFO - root - 2017-12-06 05:13:57.713501: step 11450, loss = 0.84, batch loss = 0.62 (34.7 examples/sec; 0.230 sec/batch; 20h:33m:22s remains)
INFO - root - 2017-12-06 05:14:00.058739: step 11460, loss = 0.84, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 20h:51m:57s remains)
INFO - root - 2017-12-06 05:14:02.338937: step 11470, loss = 0.81, batch loss = 0.60 (35.3 examples/sec; 0.226 sec/batch; 20h:10m:56s remains)
INFO - root - 2017-12-06 05:14:04.668560: step 11480, loss = 0.85, batch loss = 0.64 (33.9 examples/sec; 0.236 sec/batch; 21h:02m:35s remains)
INFO - root - 2017-12-06 05:14:06.964486: step 11490, loss = 0.80, batch loss = 0.59 (36.2 examples/sec; 0.221 sec/batch; 19h:42m:17s remains)
INFO - root - 2017-12-06 05:14:09.257317: step 11500, loss = 0.80, batch loss = 0.59 (33.0 examples/sec; 0.242 sec/batch; 21h:37m:11s remains)
2017-12-06 05:14:10.224057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082653642 -0.082664058 -0.083286934 -0.08469025 -0.086465165 -0.088267639 -0.089741737 -0.0898329 -0.0892145 -0.088674262 -0.087458089 -0.084779873 -0.082020648 -0.0802242 -0.079593882][-0.081932358 -0.082329437 -0.084515281 -0.088015363 -0.091890238 -0.097172588 -0.10307271 -0.10483968 -0.10261134 -0.10045774 -0.098577976 -0.093829021 -0.086998433 -0.082036957 -0.080104515][-0.081535496 -0.083462372 -0.088986665 -0.095132463 -0.10137544 -0.11194053 -0.126242 -0.13484548 -0.1322723 -0.12550038 -0.11889847 -0.11021611 -0.09723223 -0.085934617 -0.080951467][-0.081937768 -0.087797046 -0.0986372 -0.10863668 -0.11649739 -0.127992 -0.14598903 -0.16094784 -0.16411157 -0.15555857 -0.14408988 -0.13241655 -0.11455265 -0.0947111 -0.083652459][-0.083071239 -0.091933072 -0.10286495 -0.10754656 -0.10638905 -0.11118617 -0.13061668 -0.16155231 -0.18881577 -0.19301942 -0.18074402 -0.1661367 -0.14278039 -0.1115486 -0.089582667][-0.083040774 -0.086575024 -0.084083788 -0.064298496 -0.035417955 -0.018192552 -0.03276471 -0.088596232 -0.15368053 -0.18540896 -0.18721154 -0.18118769 -0.16236189 -0.12783629 -0.09677235][-0.081065916 -0.079047106 -0.063319027 -0.023282573 0.041363865 0.10851772 0.13873127 0.10286662 0.029671319 -0.032207016 -0.077246971 -0.11447079 -0.13240995 -0.12177274 -0.095189966][-0.080032125 -0.078742944 -0.064842671 -0.026342165 0.047681749 0.14056876 0.2033931 0.19462249 0.13495582 0.064783335 0.0038181767 -0.052411806 -0.090897053 -0.10138206 -0.085834749][-0.081361249 -0.083496012 -0.080753818 -0.058111437 -0.0055519417 0.065903574 0.11533375 0.10569444 0.052843541 -0.005058676 -0.043512531 -0.07090082 -0.089949228 -0.096149012 -0.0850438][-0.081989966 -0.086665861 -0.090156741 -0.082778089 -0.050648812 -0.00096187741 0.041663155 0.04359962 0.0087619275 -0.035642803 -0.066805661 -0.0822741 -0.09057492 -0.090029031 -0.080909155][-0.080194339 -0.083785132 -0.08980608 -0.091067113 -0.076122224 -0.042649463 -0.0038520768 0.010585427 -0.0053922236 -0.038079843 -0.0665499 -0.079617135 -0.085003145 -0.083800212 -0.079590879][-0.079395093 -0.082059771 -0.088254884 -0.094920993 -0.095263839 -0.082578838 -0.063958213 -0.051819958 -0.053619072 -0.066428773 -0.080338806 -0.082356073 -0.080932632 -0.077913158 -0.076744564][-0.07893382 -0.079875804 -0.086128511 -0.093795665 -0.10094374 -0.10362851 -0.10103089 -0.093295254 -0.088811062 -0.088464573 -0.0899235 -0.084506 -0.079759553 -0.0770329 -0.076428317][-0.078150988 -0.078244008 -0.081434622 -0.085575476 -0.095142968 -0.10125975 -0.10162033 -0.096066147 -0.089474551 -0.084073618 -0.081242315 -0.078351788 -0.077495866 -0.076606445 -0.076652966][-0.07792493 -0.077624626 -0.078219011 -0.080303833 -0.083969541 -0.085856125 -0.088292256 -0.0868756 -0.08473625 -0.081706218 -0.0793775 -0.07712765 -0.076761715 -0.076675527 -0.076837152]]...]
INFO - root - 2017-12-06 05:14:12.526367: step 11510, loss = 0.82, batch loss = 0.60 (35.0 examples/sec; 0.228 sec/batch; 20h:21m:35s remains)
INFO - root - 2017-12-06 05:14:14.810676: step 11520, loss = 0.77, batch loss = 0.56 (34.2 examples/sec; 0.234 sec/batch; 20h:51m:52s remains)
INFO - root - 2017-12-06 05:14:17.129140: step 11530, loss = 0.80, batch loss = 0.59 (32.7 examples/sec; 0.245 sec/batch; 21h:48m:08s remains)
INFO - root - 2017-12-06 05:14:19.437293: step 11540, loss = 0.83, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 20h:37m:33s remains)
INFO - root - 2017-12-06 05:14:21.730028: step 11550, loss = 0.77, batch loss = 0.55 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:08s remains)
INFO - root - 2017-12-06 05:14:24.010019: step 11560, loss = 0.74, batch loss = 0.52 (36.5 examples/sec; 0.219 sec/batch; 19h:33m:26s remains)
INFO - root - 2017-12-06 05:14:26.325910: step 11570, loss = 0.77, batch loss = 0.56 (34.0 examples/sec; 0.235 sec/batch; 20h:59m:22s remains)
INFO - root - 2017-12-06 05:14:28.662580: step 11580, loss = 0.94, batch loss = 0.73 (34.3 examples/sec; 0.233 sec/batch; 20h:46m:01s remains)
INFO - root - 2017-12-06 05:14:30.971904: step 11590, loss = 0.87, batch loss = 0.66 (32.6 examples/sec; 0.245 sec/batch; 21h:53m:00s remains)
INFO - root - 2017-12-06 05:14:33.247569: step 11600, loss = 0.81, batch loss = 0.59 (36.1 examples/sec; 0.222 sec/batch; 19h:45m:28s remains)
2017-12-06 05:14:33.560402: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30349624 0.17508765 -0.023979135 -0.26512283 -0.55090141 -0.85628396 -1.1868557 -1.4367354 -1.5957408 -1.6346666 -1.6577902 -1.6741436 -1.684005 -1.564644 -1.3658979][0.27787048 0.3160727 0.2692101 0.10580337 -0.14999816 -0.49083018 -0.90677416 -1.2326692 -1.5532461 -1.7464367 -1.8467603 -1.7903876 -1.7387124 -1.6525815 -1.5657703][0.19627787 0.33530295 0.41940361 0.51761162 0.51423752 0.34025002 0.0017617717 -0.41831446 -0.85288823 -1.1631242 -1.4115791 -1.594311 -1.7364646 -1.7125722 -1.6776494][0.097920567 0.23855321 0.38700342 0.57188165 0.69702452 0.79076159 0.72967255 0.48804355 0.12976232 -0.30224198 -0.78437245 -1.1549456 -1.5232928 -1.8261842 -2.0260913][0.03782998 0.14299093 0.26060987 0.3911044 0.50818658 0.63776314 0.69729084 0.71907133 0.60373861 0.41706562 0.086834893 -0.33075377 -0.76932997 -1.1947775 -1.5745248][0.11046933 0.22000162 0.34935081 0.48671764 0.6042 0.71397924 0.78070915 0.84534824 0.84191054 0.86681455 0.78386253 0.59289122 0.21712784 -0.27742112 -0.68324876][0.14829823 0.2573722 0.39503866 0.57481343 0.76095676 0.94608551 1.0555913 1.143682 1.1784086 1.2235045 1.1775646 1.1073949 0.97904509 0.652889 0.29078025][0.042277098 0.11932001 0.22184612 0.37626588 0.57913119 0.81396425 1.0102582 1.214339 1.329808 1.4237376 1.4233481 1.4080588 1.3488386 1.2084721 1.0834345][-0.061001077 -0.055547889 -0.021736182 0.056151703 0.19900902 0.40494919 0.6194157 0.85513908 1.043254 1.2492199 1.3532119 1.4240476 1.4605511 1.4897954 1.4736754][-0.11025883 -0.12496284 -0.1531245 -0.15431163 -0.13715827 -0.066292852 0.0426632 0.21382914 0.3927213 0.58024162 0.72952324 0.97801048 1.2073036 1.3610795 1.4579347][-0.10882752 -0.12412556 -0.14949825 -0.17471725 -0.19272287 -0.21419941 -0.21006906 -0.18989602 -0.1449088 -0.033273887 0.055415854 0.21228965 0.3750217 0.656241 0.90103221][-0.10399144 -0.10595665 -0.10908339 -0.127906 -0.14714095 -0.16796193 -0.19297083 -0.22259288 -0.23606646 -0.22560754 -0.21962227 -0.14558235 -0.051511955 0.065416858 0.21105443][-0.11936852 -0.12904428 -0.13489197 -0.1344389 -0.14761685 -0.16034141 -0.17691022 -0.18867761 -0.20555067 -0.20668682 -0.22289141 -0.24976388 -0.29825634 -0.28532186 -0.2534996][-0.11037132 -0.12083149 -0.13329701 -0.13465716 -0.15247148 -0.17371994 -0.20094723 -0.21419013 -0.22147048 -0.20688456 -0.19923471 -0.17013288 -0.16099878 -0.15434138 -0.17805502][-0.098409794 -0.10320524 -0.11453689 -0.11901031 -0.13398 -0.14431566 -0.16850281 -0.18017654 -0.21523702 -0.22635798 -0.2380324 -0.2280042 -0.22121346 -0.16906008 -0.1380856]]...]
INFO - root - 2017-12-06 05:14:35.848414: step 11610, loss = 0.72, batch loss = 0.51 (36.0 examples/sec; 0.222 sec/batch; 19h:49m:02s remains)
INFO - root - 2017-12-06 05:14:38.184545: step 11620, loss = 0.96, batch loss = 0.74 (33.2 examples/sec; 0.241 sec/batch; 21h:28m:09s remains)
INFO - root - 2017-12-06 05:14:40.479302: step 11630, loss = 0.74, batch loss = 0.52 (35.8 examples/sec; 0.224 sec/batch; 19h:55m:33s remains)
INFO - root - 2017-12-06 05:14:42.758840: step 11640, loss = 0.84, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 20h:20m:24s remains)
INFO - root - 2017-12-06 05:14:45.073621: step 11650, loss = 0.87, batch loss = 0.66 (34.4 examples/sec; 0.233 sec/batch; 20h:44m:34s remains)
INFO - root - 2017-12-06 05:14:47.392747: step 11660, loss = 0.79, batch loss = 0.58 (34.9 examples/sec; 0.229 sec/batch; 20h:25m:17s remains)
INFO - root - 2017-12-06 05:14:49.681595: step 11670, loss = 0.79, batch loss = 0.57 (35.5 examples/sec; 0.225 sec/batch; 20h:04m:40s remains)
INFO - root - 2017-12-06 05:14:51.999487: step 11680, loss = 0.81, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-06 05:14:54.283718: step 11690, loss = 0.84, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 20h:26m:52s remains)
INFO - root - 2017-12-06 05:14:56.545976: step 11700, loss = 0.81, batch loss = 0.60 (35.0 examples/sec; 0.229 sec/batch; 20h:23m:06s remains)
2017-12-06 05:14:56.985696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.9720704 -1.3133593 -1.8412373 -2.3209727 -2.5629251 -2.7003431 -2.4971297 -2.1208742 -1.9401226 -1.9282675 -1.6312361 -1.5105274 -1.2241759 -1.3447764 -1.5581999][0.34309196 -0.44518173 -0.93005353 -1.3033829 -1.7223313 -2.0232399 -2.3507695 -2.3213217 -1.8358045 -1.6211081 -1.2621315 -1.0193214 -0.57977474 -0.72924054 -1.0470796][0.06416747 -0.39595082 -0.76197034 -0.81446934 -1.1838157 -1.4150441 -1.3904161 -1.5980613 -1.546499 -1.2014902 -1.1529133 -1.050804 -1.1075833 -1.0986185 -1.1813545][-0.81490749 -0.87060279 -1.0854932 -1.2102849 -1.1639042 -1.3459055 -1.1190035 -1.1446855 -1.3152235 -1.1842313 -1.1921818 -0.9793781 -0.76522613 -0.87322408 -1.0305233][-0.825467 -1.0220838 -1.1346149 -0.78352404 -0.65746737 -0.58326662 -0.40296781 -0.20311579 -0.35960284 -0.48629805 -0.57262373 -0.57717621 -0.2174172 -0.29860902 -0.2200307][-0.77783293 -0.50437248 -0.6117205 -0.39465177 -0.081707776 0.21021569 0.29066828 0.42277151 0.5733676 0.39188391 0.31814829 0.1045263 0.054857731 0.26530644 0.55575776][0.90089607 1.0636368 0.83240068 0.85243481 1.0089741 1.3682892 1.813066 1.9557312 1.9809024 1.6195552 1.1771767 0.72578967 0.73892182 0.82996082 0.914558][1.4153285 1.5942426 1.3936405 1.1344852 1.2039394 1.4130065 1.3673999 1.7746563 2.218869 2.4524667 2.4226804 1.9762521 1.7033331 1.4368026 0.74817461][0.88835132 0.98082572 1.1609404 1.3071098 1.5667629 1.3641529 1.1436243 1.4102454 1.5848932 1.8544023 1.9192624 1.6648042 1.292814 0.99858111 0.61444288][-0.10029146 0.025587544 0.22930792 0.054784685 0.12162353 0.6057781 1.0861032 1.2620845 1.649761 1.8669388 1.7029719 1.4683971 1.4744394 1.044739 0.66131085][-1.5680039 -1.4116514 -1.4589112 -1.318531 -1.155355 -1.1484566 -1.1300223 -0.42415953 -0.021026015 -0.17722869 -0.013320297 0.24959025 0.25874189 0.50391519 0.48720437][-2.0581946 -2.3913858 -2.291692 -2.3719504 -2.3753183 -2.1074736 -1.7772942 -1.7640185 -1.6178741 -1.3895986 -1.1925383 -1.4234877 -1.3042684 -0.8584218 -0.34767357][-2.3892796 -2.3777564 -2.5409377 -2.8315656 -2.9497666 -2.9005566 -2.6248941 -2.4989157 -2.3829162 -2.5118618 -2.2442946 -1.914166 -1.9536116 -1.8458593 -1.3734639][-2.1044941 -2.5465803 -2.8134589 -2.89331 -2.7389774 -2.7600157 -2.7811673 -2.5180304 -2.354022 -2.5099142 -2.4529741 -2.66484 -2.4027896 -1.7430828 -1.2576351][-1.6980915 -1.7199926 -1.8699446 -2.1846886 -2.3884547 -2.2906871 -2.1048865 -2.0551078 -1.9075494 -1.9410427 -1.9249234 -1.891959 -1.7227149 -1.8976564 -1.372401]]...]
INFO - root - 2017-12-06 05:14:59.291968: step 11710, loss = 0.78, batch loss = 0.56 (34.2 examples/sec; 0.234 sec/batch; 20h:52m:23s remains)
INFO - root - 2017-12-06 05:15:01.597386: step 11720, loss = 0.78, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:30m:08s remains)
INFO - root - 2017-12-06 05:15:03.854706: step 11730, loss = 0.94, batch loss = 0.72 (36.4 examples/sec; 0.220 sec/batch; 19h:36m:35s remains)
INFO - root - 2017-12-06 05:15:06.140166: step 11740, loss = 0.71, batch loss = 0.50 (34.7 examples/sec; 0.230 sec/batch; 20h:31m:23s remains)
INFO - root - 2017-12-06 05:15:08.456510: step 11750, loss = 0.73, batch loss = 0.51 (32.7 examples/sec; 0.245 sec/batch; 21h:49m:27s remains)
INFO - root - 2017-12-06 05:15:10.792412: step 11760, loss = 0.78, batch loss = 0.56 (33.0 examples/sec; 0.243 sec/batch; 21h:36m:19s remains)
INFO - root - 2017-12-06 05:15:13.063834: step 11770, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:50s remains)
INFO - root - 2017-12-06 05:15:15.374442: step 11780, loss = 0.81, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-06 05:15:17.689967: step 11790, loss = 0.83, batch loss = 0.61 (36.3 examples/sec; 0.221 sec/batch; 19h:39m:00s remains)
INFO - root - 2017-12-06 05:15:19.997090: step 11800, loss = 0.82, batch loss = 0.61 (35.8 examples/sec; 0.224 sec/batch; 19h:55m:57s remains)
2017-12-06 05:15:20.470683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.95095038 -1.0385946 -1.3438284 -1.7363029 -2.2815073 -1.931959 -1.6530756 -1.2141552 -0.59948838 -0.49995124 -0.43291527 -0.25308514 -0.23907612 -0.49782312 -0.69157124][1.0284021 -0.027416382 -0.4818266 -0.847057 -1.2204287 -1.0543692 -1.0659599 -0.84910226 -0.86134708 -0.67465866 -0.53784871 -0.57823741 -0.71412361 -0.86593723 -0.42348143][0.57221973 0.30576444 0.013938718 -0.91567147 -1.4107472 -1.3142152 -1.1532459 -1.1149895 -1.4045702 -0.93507588 -0.36024603 -0.28719556 -0.48785269 -0.74928021 -0.64577341][1.4522202 0.44081995 -0.21664803 -0.69142056 -0.79459059 -1.0488063 -1.624748 -1.8196056 -1.9932948 -1.957706 -1.7467045 -0.96176744 -0.47051597 -0.55381596 -0.6678201][1.6018956 1.0202512 0.64957356 -0.30238265 -0.6864326 -0.80704188 -0.96792543 -1.5934124 -2.2538261 -2.0961995 -1.9703249 -1.8080851 -1.1717416 -0.93287754 -1.0241143][1.4982029 0.864578 0.51597822 0.4258413 0.60661817 0.0075944066 -0.032166477 -0.37792143 -0.56276107 -1.2434123 -1.6182318 -1.1262536 -1.0361242 -0.87518072 -1.1595422][1.5155187 1.3792717 0.84079075 0.346186 0.39319408 0.39312088 0.38194 -0.084275417 0.12206501 -0.0099466816 -0.355667 -0.62446 -1.1330659 -1.187618 -1.3510389][2.8569748 2.4904125 2.15786 1.6440837 1.0278659 0.7221837 0.50206363 0.61606371 0.6521455 0.55512655 0.65213084 0.38276663 0.22745809 -0.65099025 -1.2734339][1.5422161 1.8368825 1.8698516 1.5405693 1.1041133 0.7810092 0.2688815 0.11331813 0.43563381 1.0266185 1.51828 1.3075322 1.5870719 1.0684381 0.11627039][0.55826151 0.24904135 -0.0022722185 0.23081779 0.28832054 -0.34287113 -0.61986089 -0.50546151 -0.15061611 0.06325537 0.37300473 0.76329339 1.2474349 1.1011614 0.34879544][-0.40771827 -0.54444325 -0.8591491 -1.5036983 -1.6046207 -1.4935546 -1.5552512 -1.6544069 -1.4601686 -0.93991292 -0.6738404 -0.53257996 -0.24845015 0.536937 0.67924666][-0.75285709 -1.8091308 -2.2803466 -2.2255793 -1.570578 -1.9477845 -1.8946245 -1.5636512 -1.773972 -1.4922522 -0.97493517 -0.8892045 -0.98176587 -0.36570084 0.50829673][-0.96042979 -1.2124574 -1.5932332 -2.0305691 -2.3576946 -2.0178392 -1.9632658 -2.0892711 -2.2047083 -2.2141089 -2.1836295 -1.4325808 -0.24350773 0.071103364 -0.15182424][-0.073220506 -0.28949094 -0.63986015 -0.41984713 -0.72152889 -1.0229784 -1.1818211 -1.2489293 -1.0663905 -1.227057 -1.1758478 -1.303607 -1.4681852 -0.70936811 -0.26748264][0.94338882 1.053309 0.88208556 0.26693192 0.22400787 -0.31624925 -0.39508376 -0.54808724 -0.53538287 -0.77083683 -1.255182 -1.1177825 -1.0141312 -0.85420656 -0.902804]]...]
INFO - root - 2017-12-06 05:15:22.827260: step 11810, loss = 0.78, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:39m:49s remains)
INFO - root - 2017-12-06 05:15:25.169625: step 11820, loss = 0.76, batch loss = 0.54 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-06 05:15:27.509862: step 11830, loss = 0.85, batch loss = 0.64 (33.6 examples/sec; 0.238 sec/batch; 21h:12m:25s remains)
INFO - root - 2017-12-06 05:15:29.782578: step 11840, loss = 0.79, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 20h:02m:54s remains)
INFO - root - 2017-12-06 05:15:32.046305: step 11850, loss = 0.80, batch loss = 0.59 (34.6 examples/sec; 0.231 sec/batch; 20h:36m:13s remains)
INFO - root - 2017-12-06 05:15:34.345897: step 11860, loss = 0.78, batch loss = 0.56 (35.0 examples/sec; 0.229 sec/batch; 20h:22m:17s remains)
INFO - root - 2017-12-06 05:15:36.643787: step 11870, loss = 0.81, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:29m:30s remains)
INFO - root - 2017-12-06 05:15:38.903789: step 11880, loss = 0.79, batch loss = 0.58 (33.1 examples/sec; 0.242 sec/batch; 21h:32m:12s remains)
INFO - root - 2017-12-06 05:15:41.272577: step 11890, loss = 0.83, batch loss = 0.61 (33.5 examples/sec; 0.239 sec/batch; 21h:14m:45s remains)
INFO - root - 2017-12-06 05:15:43.562832: step 11900, loss = 0.77, batch loss = 0.56 (33.5 examples/sec; 0.239 sec/batch; 21h:14m:32s remains)
2017-12-06 05:15:43.995810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0763578 -0.073617116 -0.087384827 -0.09648595 -0.10375147 -0.12444307 -0.16106386 -0.20890786 -0.26380908 -0.30762905 -0.29978389 -0.25488091 -0.20374194 -0.15264151 -0.11428377][-0.12264476 -0.13750586 -0.14735201 -0.13283932 -0.10408647 -0.096004955 -0.13192567 -0.21346118 -0.319264 -0.39867142 -0.40348053 -0.34444031 -0.26286247 -0.18632548 -0.12831987][-0.13592845 -0.16541463 -0.17198533 -0.1248782 -0.037274238 0.02829697 0.011649802 -0.10115317 -0.28053486 -0.43316993 -0.4954972 -0.44271821 -0.33523369 -0.23300873 -0.15749972][-0.1520921 -0.19220071 -0.19569343 -0.12021481 0.044649556 0.21152443 0.26111749 0.15900722 -0.081013821 -0.332775 -0.49478334 -0.49907479 -0.40310279 -0.28543198 -0.19570035][-0.16767968 -0.21374792 -0.21895504 -0.13559745 0.080303937 0.34874335 0.49173844 0.45237029 0.20309761 -0.12114228 -0.38359302 -0.47498235 -0.41660938 -0.31261489 -0.22135946][-0.16161737 -0.19056422 -0.18736653 -0.10557784 0.13210963 0.45230091 0.65968078 0.70807105 0.51223707 0.181941 -0.15114775 -0.33031452 -0.34975892 -0.29339492 -0.22714974][-0.14830764 -0.15001896 -0.12588674 -0.027437683 0.23450437 0.59001595 0.84410548 0.94499588 0.7803039 0.46408308 0.10909297 -0.1312685 -0.21864998 -0.21167949 -0.18289819][-0.13053709 -0.10749981 -0.069504052 0.034850776 0.28988436 0.65111566 0.93279994 1.0861112 0.96189713 0.66208869 0.3120957 0.03481593 -0.098544389 -0.13018323 -0.11464455][-0.1331339 -0.10084367 -0.059389673 0.045464918 0.26300034 0.56116831 0.80603135 0.97621429 0.90559357 0.66411704 0.37259567 0.11658944 -0.032805219 -0.086725131 -0.089353181][-0.15708187 -0.14406843 -0.12214482 -0.045022346 0.11937752 0.33816594 0.54303294 0.69480747 0.65139526 0.48665017 0.28303865 0.077106252 -0.04811598 -0.088956073 -0.090301327][-0.16568449 -0.18974726 -0.21034047 -0.18877217 -0.094557837 0.03297966 0.17319581 0.30249009 0.2900674 0.18797928 0.069169775 -0.05711104 -0.12501058 -0.13278933 -0.10636277][-0.15496309 -0.20182037 -0.26431483 -0.3079915 -0.29152915 -0.26144698 -0.16953593 -0.057057589 -0.021803454 -0.052804079 -0.10238223 -0.16306496 -0.18780749 -0.17636195 -0.13684992][-0.13766533 -0.18317445 -0.25246939 -0.32592332 -0.36168295 -0.39262965 -0.36422071 -0.29042444 -0.22577867 -0.17905924 -0.15674466 -0.1628354 -0.16035905 -0.14231239 -0.11779198][-0.11620355 -0.14564699 -0.19276664 -0.24646538 -0.27828693 -0.31567836 -0.31450716 -0.27953631 -0.23417263 -0.20257324 -0.16661474 -0.14660829 -0.13012365 -0.11730677 -0.10649729][-0.094152264 -0.10763447 -0.13210982 -0.15794006 -0.1670294 -0.1897397 -0.19162123 -0.17213956 -0.15588439 -0.13784443 -0.11563393 -0.11518485 -0.10180505 -0.094983004 -0.096098527]]...]
INFO - root - 2017-12-06 05:15:46.320353: step 11910, loss = 0.79, batch loss = 0.57 (33.4 examples/sec; 0.239 sec/batch; 21h:17m:59s remains)
INFO - root - 2017-12-06 05:15:48.621150: step 11920, loss = 0.78, batch loss = 0.57 (34.2 examples/sec; 0.234 sec/batch; 20h:50m:30s remains)
INFO - root - 2017-12-06 05:15:50.900095: step 11930, loss = 0.89, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:07s remains)
INFO - root - 2017-12-06 05:15:53.160880: step 11940, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 20h:29m:15s remains)
INFO - root - 2017-12-06 05:15:55.485427: step 11950, loss = 0.79, batch loss = 0.58 (32.6 examples/sec; 0.246 sec/batch; 21h:51m:53s remains)
INFO - root - 2017-12-06 05:15:57.781103: step 11960, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 20h:52m:57s remains)
INFO - root - 2017-12-06 05:16:00.094091: step 11970, loss = 0.77, batch loss = 0.56 (34.1 examples/sec; 0.235 sec/batch; 20h:54m:16s remains)
INFO - root - 2017-12-06 05:16:02.416620: step 11980, loss = 0.84, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:39m:02s remains)
INFO - root - 2017-12-06 05:16:04.718133: step 11990, loss = 0.79, batch loss = 0.57 (33.2 examples/sec; 0.241 sec/batch; 21h:25m:39s remains)
INFO - root - 2017-12-06 05:16:06.984851: step 12000, loss = 0.81, batch loss = 0.60 (35.3 examples/sec; 0.227 sec/batch; 20h:10m:08s remains)
2017-12-06 05:16:07.387042: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017320573 0.023187235 0.02852267 0.033107951 0.037058875 0.04063718 0.043731317 0.04636009 0.047808424 0.04874447 0.049513847 0.04955098 0.047773629 0.044436604 0.036392294][0.029889859 0.037495077 0.04435502 0.050233781 0.055235937 0.059677511 0.063410684 0.066401407 0.068353578 0.070052162 0.071607322 0.072694942 0.071900159 0.068848431 0.059739128][0.040344641 0.049550623 0.057916597 0.0650778 0.070896327 0.075707912 0.079431549 0.082283482 0.0841333 0.086142331 0.088502958 0.090985611 0.091744781 0.0900792 0.081345528][0.048468903 0.059328586 0.06916748 0.077470407 0.083875775 0.08880879 0.092178226 0.094256535 0.095518336 0.097184449 0.099985376 0.10376847 0.10631351 0.10654193 0.0987186][0.054544076 0.066224694 0.076747358 0.08558251 0.091939181 0.096589684 0.099260375 0.10040201 0.10073687 0.10191628 0.10513732 0.11011878 0.11477929 0.11758161 0.11078492][0.058917135 0.071259946 0.081974328 0.090632305 0.09638159 0.1000466 0.10144049 0.10066165 0.099467769 0.099987164 0.10347973 0.1097644 0.11689332 0.12183148 0.11632012][0.062614754 0.075448573 0.085978061 0.0942442 0.099163145 0.10141495 0.10069835 0.097702727 0.094670877 0.093886375 0.097220615 0.10429333 0.1131209 0.1188768 0.11538988][0.067463621 0.080951363 0.091577843 0.099548459 0.10395221 0.1047902 0.10211651 0.096921772 0.091854885 0.089166567 0.09126687 0.097911805 0.10628082 0.11283807 0.1110516][0.073102742 0.088036463 0.099230111 0.1068759 0.11061676 0.11064182 0.10670286 0.099942535 0.093106434 0.088585079 0.088964105 0.09411101 0.1008676 0.10702229 0.10664333][0.077954188 0.094454482 0.10641718 0.11368553 0.11677994 0.11619815 0.11167759 0.10408716 0.096014977 0.090366676 0.089329556 0.092658058 0.097408727 0.10278027 0.10249832][0.080067858 0.097858474 0.11040299 0.1175953 0.12038413 0.11961254 0.11482467 0.10701197 0.098579094 0.0922741 0.090121448 0.091807529 0.094388768 0.097676665 0.096464753][0.078358278 0.096432507 0.10891868 0.11573657 0.11827201 0.11724131 0.11249185 0.10525167 0.097477928 0.091483638 0.088779196 0.08912766 0.088930741 0.0894551 0.087160826][0.072441876 0.089466363 0.10056159 0.10627967 0.10800631 0.10678798 0.10249484 0.09650746 0.090547517 0.085975677 0.083482072 0.083087608 0.080978692 0.079303741 0.075677231][0.061290652 0.075960025 0.084557563 0.088404566 0.088916689 0.087789 0.084261507 0.079945862 0.076165676 0.07325305 0.071468338 0.070572257 0.067839548 0.065194264 0.060574219][0.043662861 0.055497155 0.061588064 0.064111248 0.064327627 0.063016996 0.060321048 0.057566851 0.055532157 0.053939983 0.052655518 0.051428989 0.04882361 0.045892417 0.04130064]]...]
INFO - root - 2017-12-06 05:16:09.694076: step 12010, loss = 0.84, batch loss = 0.63 (34.0 examples/sec; 0.235 sec/batch; 20h:55m:27s remains)
INFO - root - 2017-12-06 05:16:12.010158: step 12020, loss = 0.73, batch loss = 0.51 (35.5 examples/sec; 0.225 sec/batch; 20h:04m:08s remains)
INFO - root - 2017-12-06 05:16:14.321845: step 12030, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.224 sec/batch; 19h:54m:19s remains)
INFO - root - 2017-12-06 05:16:16.615242: step 12040, loss = 0.75, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:48m:48s remains)
INFO - root - 2017-12-06 05:16:18.936607: step 12050, loss = 0.88, batch loss = 0.67 (33.3 examples/sec; 0.241 sec/batch; 21h:24m:32s remains)
INFO - root - 2017-12-06 05:16:21.267856: step 12060, loss = 0.90, batch loss = 0.68 (34.5 examples/sec; 0.232 sec/batch; 20h:39m:22s remains)
INFO - root - 2017-12-06 05:16:23.584689: step 12070, loss = 0.81, batch loss = 0.59 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:18s remains)
INFO - root - 2017-12-06 05:16:25.904431: step 12080, loss = 0.82, batch loss = 0.61 (32.9 examples/sec; 0.244 sec/batch; 21h:40m:30s remains)
INFO - root - 2017-12-06 05:16:28.220147: step 12090, loss = 0.80, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 20h:31m:16s remains)
INFO - root - 2017-12-06 05:16:30.538958: step 12100, loss = 0.81, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:17s remains)
2017-12-06 05:16:30.990582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066452339 -0.073791757 -0.078996494 -0.080895737 -0.079489127 -0.075672932 -0.06859269 -0.061849676 -0.056661762 -0.05615126 -0.056289203 -0.055689815 -0.055393077 -0.055024419 -0.055877566][-0.068270162 -0.07767871 -0.083607748 -0.083170764 -0.07488177 -0.064979196 -0.057580069 -0.055252034 -0.053798541 -0.05617889 -0.057904124 -0.056548957 -0.054141015 -0.054210167 -0.055889729][-0.068619534 -0.0794407 -0.088180833 -0.092669241 -0.087070405 -0.0769368 -0.067360304 -0.062609233 -0.059862278 -0.061776113 -0.063476339 -0.059961811 -0.055349935 -0.054397732 -0.05565282][-0.067093432 -0.077291511 -0.085618116 -0.092593335 -0.091333076 -0.085397094 -0.074044004 -0.063744195 -0.056739133 -0.057065316 -0.061292008 -0.060282327 -0.056311857 -0.054785609 -0.055359859][-0.06488134 -0.073928177 -0.079954214 -0.081747375 -0.07876531 -0.074241623 -0.063795149 -0.053191185 -0.044690304 -0.04432546 -0.051579677 -0.05564652 -0.054708522 -0.054098658 -0.054400165][-0.062038135 -0.069934987 -0.074753284 -0.072525673 -0.066578612 -0.062300295 -0.056339167 -0.05109049 -0.046411678 -0.046302754 -0.05223057 -0.055240609 -0.054477323 -0.053933207 -0.054355681][-0.059375845 -0.065729335 -0.070872545 -0.068941936 -0.0629489 -0.056856018 -0.049791254 -0.045963623 -0.043734483 -0.044564318 -0.051108591 -0.054637264 -0.053858038 -0.053463195 -0.054198969][-0.057457894 -0.061498132 -0.065641761 -0.0642861 -0.060479525 -0.056131084 -0.050392617 -0.046693515 -0.044171 -0.04291717 -0.048580624 -0.053024687 -0.053042717 -0.053010143 -0.054310102][-0.056489687 -0.058492467 -0.061238967 -0.061169565 -0.060030434 -0.057042252 -0.053556602 -0.052185718 -0.051519893 -0.049017582 -0.051306933 -0.053682838 -0.0538221 -0.053631876 -0.054734878][-0.056105051 -0.056487877 -0.057706632 -0.058322743 -0.058295794 -0.0568306 -0.053564634 -0.052712653 -0.052872214 -0.051575605 -0.053170163 -0.054883715 -0.055058032 -0.055440992 -0.056257747][-0.056107447 -0.055399705 -0.055370811 -0.055432063 -0.054930661 -0.055500545 -0.054696422 -0.05431829 -0.053986989 -0.05381522 -0.055066846 -0.056263089 -0.056182008 -0.056385379 -0.057140429][-0.056341689 -0.055438459 -0.054935869 -0.053800844 -0.05228645 -0.053988881 -0.053733435 -0.054638021 -0.055063374 -0.05489967 -0.055645656 -0.056194957 -0.056631278 -0.057123944 -0.057946391][-0.056585655 -0.055802919 -0.055819467 -0.054847423 -0.053514987 -0.052986536 -0.052802507 -0.0545974 -0.055169892 -0.055494271 -0.055707198 -0.056061726 -0.056484189 -0.056975406 -0.058033191][-0.056707166 -0.056005221 -0.055869985 -0.055632811 -0.055064842 -0.053659476 -0.052692737 -0.052494612 -0.054726787 -0.055173963 -0.055207625 -0.055479623 -0.05591495 -0.056557678 -0.057712026][-0.057249986 -0.056477446 -0.056096733 -0.056359667 -0.056463882 -0.05571647 -0.054438446 -0.053382598 -0.054225441 -0.054201502 -0.05456027 -0.054754715 -0.05517837 -0.05589474 -0.057369962]]...]
INFO - root - 2017-12-06 05:16:33.318349: step 12110, loss = 0.92, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 20h:24m:57s remains)
INFO - root - 2017-12-06 05:16:35.658082: step 12120, loss = 0.75, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:48s remains)
INFO - root - 2017-12-06 05:16:37.930844: step 12130, loss = 0.84, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 19h:47m:46s remains)
INFO - root - 2017-12-06 05:16:40.275479: step 12140, loss = 0.77, batch loss = 0.56 (32.9 examples/sec; 0.243 sec/batch; 21h:39m:51s remains)
INFO - root - 2017-12-06 05:16:42.590568: step 12150, loss = 0.86, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 20h:28m:33s remains)
INFO - root - 2017-12-06 05:16:44.885577: step 12160, loss = 0.82, batch loss = 0.61 (36.0 examples/sec; 0.222 sec/batch; 19h:47m:30s remains)
INFO - root - 2017-12-06 05:16:47.187093: step 12170, loss = 0.71, batch loss = 0.50 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:31s remains)
INFO - root - 2017-12-06 05:16:49.497495: step 12180, loss = 0.79, batch loss = 0.58 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:04s remains)
INFO - root - 2017-12-06 05:16:51.793544: step 12190, loss = 0.73, batch loss = 0.52 (36.2 examples/sec; 0.221 sec/batch; 19h:40m:09s remains)
INFO - root - 2017-12-06 05:16:54.112690: step 12200, loss = 0.86, batch loss = 0.65 (34.3 examples/sec; 0.233 sec/batch; 20h:45m:05s remains)
2017-12-06 05:16:54.481350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.27840728 -0.52483582 -0.92974716 -1.13531 -1.2989907 -1.5697962 -1.9333237 -1.9569185 -2.0912118 -2.057369 -1.4227591 -1.2479622 -1.1397392 -1.3562536 -1.3866104][0.49416891 -0.23549831 -0.60934222 -0.99875075 -1.3088837 -1.6257676 -1.6620709 -1.996179 -2.4865916 -2.6803594 -2.706033 -2.0186548 -1.5221776 -1.4155618 -1.1803522][0.38148037 0.447248 0.234498 -0.61528826 -1.2458181 -1.3779156 -1.513159 -2.0496004 -2.2955058 -2.3190455 -2.2886269 -1.8698772 -1.4924766 -1.3198131 -1.6071719][1.301029 0.9185372 0.48552713 0.12115031 -0.22876546 -0.88417077 -1.3765692 -1.7482857 -2.3051958 -2.6974075 -2.6374185 -2.216269 -1.3328757 -0.99941146 -1.1675138][1.6522288 1.6832516 1.6664561 1.159042 0.54896134 0.27457011 0.090216018 -0.93744391 -1.8494558 -2.3016388 -2.4334288 -2.2377236 -2.054708 -1.5686431 -1.12745][2.0327466 2.03912 1.6797377 1.698025 1.5897617 1.1154205 1.0466692 0.5943256 0.10793587 -1.1869284 -1.992753 -1.8174471 -1.6494731 -1.662899 -2.0231335][2.5754 2.1431618 1.7536949 1.6457928 1.3587298 1.3742461 1.2026304 0.797045 0.71799463 0.39491332 0.15547729 -0.18738592 -0.49101743 -0.69326448 -0.7735092][2.7672551 2.2452013 1.7975434 1.2530617 0.78351772 0.65459877 0.35668832 0.47244856 0.89620692 0.70206571 0.56911922 0.78733653 0.92044646 0.7099061 -0.023813017][1.2263472 1.3567013 1.0180805 0.77379876 0.45182076 0.037646316 -0.38546962 -0.27402186 -0.058786556 0.11314223 0.3299776 0.58801538 0.90184593 1.0830921 0.80846012][0.35287124 0.1812408 0.073680647 -0.30037269 -0.69799548 -0.84944707 -0.95126629 -1.0775099 -1.4503218 -1.3799509 -1.0800002 -0.5769546 0.51568186 0.79065627 0.42660508][-0.17076203 -0.42915073 -0.79158771 -1.3988005 -1.5447358 -1.4746934 -1.5222706 -1.6511366 -1.7326363 -1.8082428 -1.8921746 -1.7286011 -1.2189753 -0.30675489 0.16041112][0.10263721 -1.0701861 -2.3270183 -2.3474829 -2.2296631 -2.1689057 -1.9247298 -2.1353705 -2.1778548 -2.2453661 -2.1428146 -2.0514085 -1.9521188 -1.5104268 -1.1532971][-1.3022431 -1.424198 -1.3354399 -1.9813662 -2.4791074 -2.155642 -1.8493111 -1.9065098 -2.3021636 -2.6091034 -2.6758814 -2.1122673 -1.3866379 -1.1835352 -1.1125731][-1.448226 -1.9443035 -2.0478063 -1.7703642 -1.4581497 -1.8727989 -2.1905568 -1.7824392 -1.7878959 -1.6028577 -1.4772518 -1.5713062 -1.5590782 -1.4900138 -1.1992176][-0.66684395 -0.91012949 -1.2620769 -1.4295753 -1.4206958 -1.1608188 -1.352088 -1.5946993 -1.5551615 -1.5254449 -1.5608821 -1.2289221 -0.861968 -1.0855322 -1.0244912]]...]
INFO - root - 2017-12-06 05:16:56.788434: step 12210, loss = 0.74, batch loss = 0.53 (36.3 examples/sec; 0.220 sec/batch; 19h:35m:46s remains)
INFO - root - 2017-12-06 05:16:59.097234: step 12220, loss = 0.88, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:24m:54s remains)
INFO - root - 2017-12-06 05:17:01.389764: step 12230, loss = 0.77, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 20h:09m:17s remains)
INFO - root - 2017-12-06 05:17:03.736542: step 12240, loss = 0.77, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-06 05:17:06.069183: step 12250, loss = 0.83, batch loss = 0.61 (34.7 examples/sec; 0.230 sec/batch; 20h:29m:14s remains)
INFO - root - 2017-12-06 05:17:08.367834: step 12260, loss = 0.82, batch loss = 0.60 (33.9 examples/sec; 0.236 sec/batch; 21h:00m:58s remains)
INFO - root - 2017-12-06 05:17:10.665876: step 12270, loss = 0.75, batch loss = 0.54 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:39s remains)
INFO - root - 2017-12-06 05:17:12.955347: step 12280, loss = 0.86, batch loss = 0.64 (34.7 examples/sec; 0.230 sec/batch; 20h:29m:08s remains)
INFO - root - 2017-12-06 05:17:15.277137: step 12290, loss = 0.85, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 20h:11m:08s remains)
INFO - root - 2017-12-06 05:17:17.597330: step 12300, loss = 0.75, batch loss = 0.54 (33.5 examples/sec; 0.239 sec/batch; 21h:15m:13s remains)
2017-12-06 05:17:18.189851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.24509941 -0.35522848 -0.55282897 -0.80502534 -1.0044222 -1.1345385 -1.2088015 -1.1813076 -0.99535054 -0.7208342 -0.44459218 -0.24745077 -0.18305549 -0.15579422 -0.15144412][-0.11250538 -0.15775123 -0.35929239 -0.682673 -0.98254305 -1.2098576 -1.4322224 -1.6353462 -1.5698135 -1.1877621 -0.71684515 -0.3556872 -0.21463905 -0.18383363 -0.1823155][-0.014864631 -0.018730231 -0.16064876 -0.42203552 -0.64998978 -0.8624379 -1.200314 -1.6628077 -1.8952 -1.662328 -1.1049408 -0.53122741 -0.22345066 -0.1685383 -0.15907398][-0.0908482 -0.056239024 -0.063687906 -0.097696 -0.10202505 -0.14811188 -0.46741515 -1.0619878 -1.5793582 -1.7006091 -1.3582867 -0.75436461 -0.3260105 -0.16776168 -0.11466942][-0.093243636 -0.058033764 0.043405667 0.20236997 0.43641907 0.63943976 0.54163229 -0.05593428 -0.81434131 -1.30178 -1.2958034 -0.85207689 -0.41997182 -0.24610311 -0.16975772][-0.12439391 -0.033004373 0.16679232 0.49698925 0.952399 1.2743405 1.2577833 0.747442 0.025798678 -0.64829034 -0.965292 -0.77334833 -0.42467844 -0.21131292 -0.12486308][-0.17605233 -0.026714861 0.36553931 0.78571057 1.2494203 1.6993275 1.76677 1.3285501 0.66536003 -0.077050686 -0.6201793 -0.62413728 -0.37425452 -0.17543939 -0.1305988][-0.1342569 -0.00022833794 0.44171947 0.9902674 1.4137989 1.7575727 1.8130708 1.4134469 0.85510135 0.32122731 -0.2337756 -0.37888926 -0.24762508 -0.13148622 -0.10358378][-0.080281384 0.064612791 0.41597033 0.9315424 1.2838181 1.5383711 1.4654932 1.0840646 0.7249856 0.32116532 -0.13700417 -0.33643031 -0.25786588 -0.14724052 -0.076694228][-0.15122527 -0.01903668 0.22674917 0.59737694 0.94829631 1.2126747 1.121286 0.72849268 0.37884349 0.016769029 -0.21277982 -0.29514721 -0.20974487 -0.1612317 -0.11124687][-0.24854685 -0.23316692 -0.022095241 0.23582552 0.36255676 0.56194323 0.65516865 0.42119318 0.13867898 -0.12778692 -0.27301008 -0.34289044 -0.27812353 -0.14627236 -0.08288452][-0.22756851 -0.32378465 -0.35114759 -0.21249996 -0.084042318 0.039838 0.028308183 -0.024808101 -0.063647985 -0.13173127 -0.19147094 -0.25428778 -0.24520516 -0.17244485 -0.12191281][-0.15851685 -0.22731739 -0.24702702 -0.29492885 -0.27990744 -0.18203598 -0.078714356 -0.1153644 -0.11057068 -0.17213242 -0.18775295 -0.17423967 -0.11528125 -0.11084099 -0.11315861][-0.1100949 -0.12066106 -0.12385865 -0.19298726 -0.26401937 -0.28983977 -0.31447667 -0.31255245 -0.25652584 -0.22847682 -0.135196 -0.12338806 -0.13165575 -0.12137881 -0.11833787][-0.12006915 -0.1290924 -0.084793255 -0.077225916 -0.065949604 -0.11830813 -0.14966345 -0.19818592 -0.25158483 -0.24485031 -0.18093111 -0.16178939 -0.13904732 -0.11964199 -0.11917044]]...]
INFO - root - 2017-12-06 05:17:20.513422: step 12310, loss = 0.88, batch loss = 0.66 (34.5 examples/sec; 0.232 sec/batch; 20h:35m:42s remains)
INFO - root - 2017-12-06 05:17:22.800208: step 12320, loss = 0.80, batch loss = 0.59 (35.8 examples/sec; 0.224 sec/batch; 19h:53m:34s remains)
INFO - root - 2017-12-06 05:17:25.052000: step 12330, loss = 0.73, batch loss = 0.52 (36.8 examples/sec; 0.218 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-06 05:17:27.392943: step 12340, loss = 0.80, batch loss = 0.58 (35.7 examples/sec; 0.224 sec/batch; 19h:54m:52s remains)
INFO - root - 2017-12-06 05:17:29.771745: step 12350, loss = 0.86, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:23m:03s remains)
INFO - root - 2017-12-06 05:17:32.066905: step 12360, loss = 0.84, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 20h:11m:49s remains)
INFO - root - 2017-12-06 05:17:34.380416: step 12370, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.233 sec/batch; 20h:41m:10s remains)
INFO - root - 2017-12-06 05:17:36.655127: step 12380, loss = 0.77, batch loss = 0.56 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:27s remains)
INFO - root - 2017-12-06 05:17:38.962864: step 12390, loss = 0.75, batch loss = 0.54 (33.7 examples/sec; 0.237 sec/batch; 21h:05m:36s remains)
INFO - root - 2017-12-06 05:17:41.256722: step 12400, loss = 0.87, batch loss = 0.65 (34.7 examples/sec; 0.231 sec/batch; 20h:30m:16s remains)
2017-12-06 05:17:41.626721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16268662 0.26775759 0.17641646 -0.034059163 -0.16738316 -0.17852545 -0.26085314 -0.33946231 -0.4333885 -0.20758946 0.088582888 0.26974922 0.38940403 0.69466227 0.85473371][0.05028677 0.084715664 0.11262743 -0.056434672 -0.30263707 -0.44868195 -0.47745797 -0.51021552 -0.52819926 -0.52175885 -0.46284068 -0.3176721 -0.20448683 -0.12208694 0.027720608][-0.090044752 -0.14474404 -0.24674848 -0.3816177 -0.53518546 -0.67254364 -0.74900573 -0.70247072 -0.61731261 -0.59106463 -0.60399896 -0.48679528 -0.52146327 -0.49627212 -0.50656128][-0.030916147 -0.094270639 -0.042889748 -0.072443396 -0.14163435 -0.29792291 -0.44745338 -0.57298988 -0.62312073 -0.7263366 -0.83888108 -0.92990673 -0.9734807 -1.0193585 -0.84430367][0.35871431 0.34763989 0.45205718 0.53462034 0.62953746 0.54625982 0.35448229 -0.036713541 -0.32685709 -0.64461583 -0.77808458 -0.97467846 -1.1590279 -1.262435 -1.1427989][0.56934768 0.69418746 0.86770213 1.0914328 1.266098 1.2074226 0.84875834 0.33877674 -0.089144431 -0.49800757 -0.8609429 -1.1167682 -1.2278681 -1.3466784 -1.3156748][0.70873749 0.88859951 1.1983597 1.4073286 1.4696331 1.3739107 1.1169375 0.6026693 0.069636628 -0.44836387 -0.81685644 -1.1529329 -1.2935013 -1.2807075 -1.2007501][0.86151797 1.0836473 1.3700085 1.6651665 1.7144854 1.619817 1.3078853 0.76695347 0.24766558 -0.22809112 -0.61736292 -0.97055197 -1.0962718 -1.2246192 -1.2373828][0.46464968 0.72693032 1.0929946 1.3771807 1.5059282 1.5176473 1.2141111 0.76301682 0.23988795 -0.20781979 -0.51032639 -0.62753636 -0.72897094 -0.8208583 -0.79744971][0.26456445 0.43016779 0.58399075 0.8567844 1.0512803 1.1159958 0.94624889 0.66936207 0.18800238 -0.23617314 -0.44901252 -0.49405465 -0.4297097 -0.47168788 -0.4446494][-0.077014841 -0.033261511 0.027469113 0.051264465 0.086482644 0.22944459 0.13004491 0.010667995 -0.27986196 -0.56887937 -0.70834064 -0.50319809 -0.30753446 -0.18740404 -0.11532261][-0.42794746 -0.3578068 -0.48238268 -0.64371151 -0.71439809 -0.66496021 -0.58789432 -0.54778647 -0.63607287 -0.70355105 -0.74091566 -0.49762258 -0.2006176 0.094531953 0.22822523][-0.46268415 -0.53625548 -0.619997 -0.781921 -1.0066599 -1.0282916 -1.0051041 -0.84893787 -0.8033523 -0.75966686 -0.7906881 -0.66238683 -0.40841031 -0.049118645 0.19737843][-0.19980931 -0.30589837 -0.44803092 -0.57957244 -0.64244485 -0.73286253 -0.73867184 -0.64733088 -0.49122173 -0.38475662 -0.30824918 -0.33741885 -0.35627547 -0.24734499 -0.13062602][0.077066973 -0.010952905 -0.11471786 -0.21886009 -0.26012522 -0.32355988 -0.35671976 -0.36145437 -0.36312655 -0.25641379 -0.17176032 -0.21778964 -0.22510703 -0.26652613 -0.31775314]]...]
INFO - root - 2017-12-06 05:17:43.944988: step 12410, loss = 0.81, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 19h:55m:27s remains)
INFO - root - 2017-12-06 05:17:46.271182: step 12420, loss = 0.81, batch loss = 0.59 (33.1 examples/sec; 0.242 sec/batch; 21h:30m:39s remains)
INFO - root - 2017-12-06 05:17:48.569995: step 12430, loss = 0.83, batch loss = 0.61 (34.4 examples/sec; 0.232 sec/batch; 20h:39m:26s remains)
INFO - root - 2017-12-06 05:17:50.889738: step 12440, loss = 0.77, batch loss = 0.56 (35.6 examples/sec; 0.224 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-06 05:17:53.237218: step 12450, loss = 0.80, batch loss = 0.58 (31.2 examples/sec; 0.256 sec/batch; 22h:47m:57s remains)
INFO - root - 2017-12-06 05:17:55.548160: step 12460, loss = 0.83, batch loss = 0.61 (35.0 examples/sec; 0.228 sec/batch; 20h:18m:37s remains)
INFO - root - 2017-12-06 05:17:57.846213: step 12470, loss = 0.80, batch loss = 0.58 (35.2 examples/sec; 0.227 sec/batch; 20h:12m:01s remains)
INFO - root - 2017-12-06 05:18:00.130466: step 12480, loss = 0.82, batch loss = 0.60 (36.0 examples/sec; 0.222 sec/batch; 19h:45m:57s remains)
INFO - root - 2017-12-06 05:18:02.406049: step 12490, loss = 0.82, batch loss = 0.60 (33.8 examples/sec; 0.236 sec/batch; 21h:00m:53s remains)
INFO - root - 2017-12-06 05:18:04.692351: step 12500, loss = 0.84, batch loss = 0.63 (36.3 examples/sec; 0.220 sec/batch; 19h:34m:52s remains)
2017-12-06 05:18:05.070469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1861721 -0.12630436 -0.093865864 -0.068596408 -0.072279334 -0.065499522 -0.042161103 -0.027288925 -0.042188972 -0.084040336 -0.110698 -0.14241208 -0.10129082 -0.083015986 -0.059300557][-0.19219387 -0.13591942 -0.095568605 -0.083856516 -0.091416121 -0.074923217 -0.040986575 -0.014376022 -0.031082053 -0.0651499 -0.11023034 -0.12082288 -0.11978678 -0.070308521 -0.014506981][-0.16000994 -0.1616035 -0.13092843 -0.11864076 -0.1211444 -0.082504943 -0.057992812 -0.021018617 0.0071454123 -0.027811129 -0.05981759 -0.080451295 -0.097272694 -0.0658505 -0.026513834][-0.13148864 -0.21116653 -0.18876792 -0.15504323 -0.1384234 -0.11223288 -0.061437346 -0.019126929 0.0093978718 -0.0076143146 -0.020368338 -0.025384188 -0.043478612 -0.041226741 0.0058151409][-0.077785358 -0.17011344 -0.17478985 -0.12506956 -0.0999109 -0.06487827 -0.015685819 -0.015978657 0.0026072115 -0.020639814 -0.049945038 -0.052977167 -0.071066506 -0.055415414 -0.037667237][-0.057936855 -0.10226087 -0.14121543 -0.064615875 -0.016320318 0.023687199 0.039109372 0.030591317 0.015188679 -0.051496644 -0.12268473 -0.1523342 -0.20802855 -0.18708217 -0.12926282][-0.045865692 -0.072627582 -0.053514209 -0.021991387 0.016341709 0.064718671 0.10676769 0.0956739 0.012845874 -0.098215409 -0.21567234 -0.26943472 -0.34638822 -0.2978853 -0.24203742][-0.056949537 -0.021215074 -0.005690001 0.01756575 0.053474344 0.097782634 0.13100284 0.086084954 -0.0076365471 -0.14656575 -0.2855556 -0.37507111 -0.44223675 -0.37710381 -0.278183][-0.095702894 -0.075119227 -0.044398829 0.0074011832 0.047621913 0.11276587 0.14827368 0.1097024 -0.019691929 -0.18696117 -0.33666909 -0.4288981 -0.48854518 -0.41295639 -0.28232181][-0.1985005 -0.11573096 -0.10018566 -0.036996368 0.024594747 0.10251952 0.1361388 0.099151142 -0.041379694 -0.22631332 -0.37587994 -0.45311886 -0.49631792 -0.42557845 -0.29817909][-0.31451058 -0.21031144 -0.14881706 -0.10512251 -0.049857683 0.040242203 0.094514363 0.075927652 -0.053908203 -0.22429308 -0.37393403 -0.44110036 -0.44923615 -0.4071981 -0.32687065][-0.43184048 -0.34059024 -0.24691471 -0.24865404 -0.21738914 -0.15049726 -0.067762695 -0.022461012 -0.081616729 -0.22238097 -0.35772154 -0.41193128 -0.39753127 -0.3418996 -0.28488085][-0.60385233 -0.47974855 -0.34971818 -0.26442993 -0.24124706 -0.21134108 -0.14122535 -0.086418226 -0.13546982 -0.25620362 -0.37486371 -0.42790657 -0.3948234 -0.30920371 -0.22830087][-0.71642739 -0.58451134 -0.43267623 -0.31944767 -0.23966333 -0.18780503 -0.15132692 -0.13116908 -0.16656794 -0.27738282 -0.38726819 -0.46415782 -0.4450841 -0.34057251 -0.22247347][-0.6842984 -0.590279 -0.44917157 -0.33747035 -0.21779007 -0.15896577 -0.11319084 -0.094616063 -0.14562932 -0.2535201 -0.36757809 -0.4551343 -0.45953897 -0.36531329 -0.23664469]]...]
INFO - root - 2017-12-06 05:18:07.401205: step 12510, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 20h:06m:18s remains)
INFO - root - 2017-12-06 05:18:09.705203: step 12520, loss = 0.85, batch loss = 0.64 (36.2 examples/sec; 0.221 sec/batch; 19h:37m:35s remains)
INFO - root - 2017-12-06 05:18:12.015765: step 12530, loss = 0.75, batch loss = 0.54 (36.3 examples/sec; 0.220 sec/batch; 19h:34m:20s remains)
INFO - root - 2017-12-06 05:18:14.330816: step 12540, loss = 0.68, batch loss = 0.47 (35.1 examples/sec; 0.228 sec/batch; 20h:15m:49s remains)
INFO - root - 2017-12-06 05:18:16.628784: step 12550, loss = 0.77, batch loss = 0.55 (34.7 examples/sec; 0.231 sec/batch; 20h:29m:55s remains)
INFO - root - 2017-12-06 05:18:18.932432: step 12560, loss = 0.76, batch loss = 0.55 (36.6 examples/sec; 0.218 sec/batch; 19h:24m:16s remains)
INFO - root - 2017-12-06 05:18:21.246538: step 12570, loss = 0.78, batch loss = 0.56 (31.9 examples/sec; 0.251 sec/batch; 22h:18m:46s remains)
INFO - root - 2017-12-06 05:18:23.523572: step 12580, loss = 0.80, batch loss = 0.59 (33.0 examples/sec; 0.243 sec/batch; 21h:34m:07s remains)
INFO - root - 2017-12-06 05:18:25.854351: step 12590, loss = 0.72, batch loss = 0.51 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:21s remains)
INFO - root - 2017-12-06 05:18:28.155369: step 12600, loss = 0.71, batch loss = 0.50 (33.4 examples/sec; 0.240 sec/batch; 21h:17m:59s remains)
2017-12-06 05:18:28.544840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7281964 -2.671237 -2.7340515 -2.9479675 -2.7093611 -2.5103648 -2.5727849 -2.1720572 -1.7939534 -1.7321856 -1.2843353 -0.98780793 -0.64973193 -0.81353778 -1.0192144][-1.7137711 -1.9408127 -2.2406645 -2.4771976 -2.4486332 -2.4187145 -2.4276114 -2.4068239 -2.3854625 -2.0566671 -2.1057277 -1.5756907 -1.4114386 -1.0677904 -0.82072586][-1.2038436 -0.92509514 -0.84667629 -1.5994831 -2.0159526 -1.5005952 -1.1441722 -1.8830121 -2.4169314 -2.3375306 -1.9959879 -2.4859986 -2.1445363 -1.64534 -1.2474798][-0.50182879 -0.59230441 -0.4639003 -0.26592568 -0.55581003 -0.87945676 -1.2244595 -1.4097513 -2.07431 -2.5467284 -2.8592179 -2.6921082 -2.1238184 -1.8531735 -1.7374623][0.18956158 -0.002252318 -0.29968974 -0.033338688 0.25216565 0.21932185 0.0571621 -0.82829612 -1.8767569 -2.2131994 -2.2568181 -2.9422109 -2.57128 -1.9944091 -1.3000152][0.0861525 0.084107094 0.47889835 0.9206171 1.1582673 1.2581221 1.3373131 1.1806399 0.66024262 -0.35206097 -1.6300279 -2.0322869 -2.0442688 -2.1508586 -2.0205936][0.92877138 0.8940711 1.2726946 1.6272268 1.8188553 1.8222469 1.8609426 1.7180907 1.9095165 1.1950179 0.67763281 -0.34528023 -1.4098229 -1.5437051 -1.4451717][1.5282588 1.1248116 0.78061759 1.2565657 1.9915059 2.248225 1.9998088 2.1356056 2.2130034 1.6580411 1.6355137 1.1973443 0.67273879 -0.16199189 -1.1817526][0.62077606 0.8555705 1.5101335 1.4204642 1.0616087 1.6645944 1.9745615 2.100127 2.1635959 1.9515305 1.6885324 1.3339421 1.1875843 0.59067017 0.26213765][0.86262071 0.51588523 0.55094945 0.914075 1.3764241 1.1161767 0.62454659 0.73316896 0.650544 0.89889479 0.53673977 0.273703 0.2681753 -0.10565716 -0.39470547][-0.08425153 -0.33130312 -0.14066759 -0.50990021 -0.24101672 0.090522759 0.10484967 -0.30268356 -0.58492112 -0.89030766 -1.2802496 -1.1593057 -0.98824239 -0.885959 -1.0869743][-0.84003359 -1.0161531 -1.3613377 -0.98054218 -0.64298093 -0.90692192 -1.1877555 -1.2437005 -1.3855569 -1.5388755 -1.9703113 -2.3858879 -2.3237643 -1.6409841 -1.0766698][-0.86560816 -1.0298808 -0.86268717 -0.84993732 -1.3403467 -1.1780263 -1.2062176 -1.5835009 -1.7120744 -1.8805467 -2.1791232 -2.0965717 -1.5907967 -1.3559707 -1.4423736][-1.0433658 -1.0481385 -0.92223126 -0.5087077 -0.45114154 -0.95332634 -1.1292707 -1.1378641 -1.2828414 -1.3318709 -1.2761841 -1.59117 -1.3600789 -0.74095821 -0.11285958][0.2094759 0.34388453 0.53527862 0.31611302 -0.018545553 -0.10020763 -0.43120858 -0.69416362 -0.68078339 -1.0443228 -1.1260806 -0.94282633 -0.78545266 -0.71060944 -0.63296407]]...]
INFO - root - 2017-12-06 05:18:30.850639: step 12610, loss = 0.76, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 20h:25m:47s remains)
INFO - root - 2017-12-06 05:18:33.149532: step 12620, loss = 0.78, batch loss = 0.57 (35.7 examples/sec; 0.224 sec/batch; 19h:55m:11s remains)
INFO - root - 2017-12-06 05:18:35.446383: step 12630, loss = 0.79, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:48s remains)
INFO - root - 2017-12-06 05:18:37.776624: step 12640, loss = 0.74, batch loss = 0.53 (32.5 examples/sec; 0.246 sec/batch; 21h:53m:13s remains)
INFO - root - 2017-12-06 05:18:40.104466: step 12650, loss = 0.81, batch loss = 0.60 (35.2 examples/sec; 0.227 sec/batch; 20h:11m:24s remains)
INFO - root - 2017-12-06 05:18:42.436464: step 12660, loss = 0.74, batch loss = 0.53 (35.8 examples/sec; 0.223 sec/batch; 19h:49m:40s remains)
INFO - root - 2017-12-06 05:18:44.736158: step 12670, loss = 0.79, batch loss = 0.58 (32.9 examples/sec; 0.244 sec/batch; 21h:38m:00s remains)
INFO - root - 2017-12-06 05:18:47.035801: step 12680, loss = 0.80, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:24m:44s remains)
INFO - root - 2017-12-06 05:18:49.356275: step 12690, loss = 0.87, batch loss = 0.66 (33.1 examples/sec; 0.242 sec/batch; 21h:28m:53s remains)
INFO - root - 2017-12-06 05:18:51.626409: step 12700, loss = 0.89, batch loss = 0.68 (35.5 examples/sec; 0.226 sec/batch; 20h:02m:06s remains)
2017-12-06 05:18:51.979670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2707805 -0.24966356 -0.20544273 -0.19441071 -0.21485788 -0.28385061 -0.37700385 -0.44603062 -0.47900569 -0.40160823 -0.31223062 -0.21480942 -0.14002603 -0.13796964 -0.15658773][-0.27915871 -0.27276084 -0.29551739 -0.34082603 -0.394768 -0.48050871 -0.52185726 -0.48974767 -0.47942203 -0.46072564 -0.45613641 -0.40971079 -0.34590369 -0.29849258 -0.25231794][-0.21699089 -0.22867888 -0.33851594 -0.45482048 -0.54938537 -0.5934329 -0.58553392 -0.59904963 -0.61389631 -0.59519786 -0.55014622 -0.4388499 -0.33407643 -0.2692636 -0.22365642][-0.25086308 -0.28407848 -0.41903013 -0.57533371 -0.68401432 -0.772565 -0.8025943 -0.79714262 -0.7697897 -0.72898203 -0.65595281 -0.57071447 -0.51078451 -0.45898032 -0.37448448][-0.47127995 -0.60722357 -0.7504704 -0.76770586 -0.73831952 -0.70663273 -0.646097 -0.5565576 -0.506057 -0.49287522 -0.5015468 -0.52351445 -0.524487 -0.520805 -0.46458709][-0.57107234 -0.57749957 -0.63088983 -0.57354397 -0.49659377 -0.43380135 -0.32410419 -0.27241892 -0.23162374 -0.28793192 -0.39314774 -0.54548657 -0.64734846 -0.67949092 -0.62687123][-0.44956133 -0.45648289 -0.39709258 -0.25342569 -0.07994055 0.12843993 0.20577928 0.20549715 0.15109351 -0.038987372 -0.24962524 -0.49923143 -0.69303471 -0.808654 -0.76117545][-0.36611471 -0.30689824 -0.24917126 -0.11868475 0.10768635 0.2974351 0.4452979 0.4744302 0.30628631 -0.010196835 -0.34370849 -0.64219773 -0.7987386 -0.90866733 -0.86354321][-0.27745646 -0.2192727 -0.1236852 -0.10880579 -0.0585592 0.12349575 0.24212065 0.26330438 0.14866769 -0.10509764 -0.41552213 -0.67103606 -0.84546143 -0.86989695 -0.8355155][-0.41853118 -0.47532323 -0.48339823 -0.4869616 -0.30231109 -0.063510843 0.097944357 0.18028811 0.075139828 -0.19874862 -0.45935321 -0.59649116 -0.620622 -0.67801768 -0.67557359][-0.23724127 -0.37897885 -0.47424087 -0.5562135 -0.60843658 -0.63712823 -0.58601886 -0.56152266 -0.58860153 -0.668893 -0.76008672 -0.7694732 -0.65355778 -0.52254379 -0.44510412][0.075342678 -0.21494466 -0.476761 -0.69497603 -0.79844421 -0.74068159 -0.64341414 -0.68144518 -0.79230553 -0.9055258 -0.92274141 -0.84795916 -0.74341106 -0.6501168 -0.63270324][0.25563166 -0.017029241 -0.16009253 -0.3284179 -0.46048748 -0.44450152 -0.31191275 -0.18633765 -0.10618357 -0.11997187 -0.20877987 -0.34874424 -0.35596007 -0.42815506 -0.48613757][0.49132091 0.18489271 -0.017609321 -0.0915727 -0.11769871 -0.090408117 0.056183375 0.2960158 0.44201261 0.5089187 0.47213733 0.34518215 0.11931143 -0.080231115 -0.17434105][0.40454671 0.21321741 0.0981256 -0.045380063 -0.15126751 -0.078056812 0.081309594 0.22531125 0.29405227 0.38296792 0.42109394 0.40338585 0.28863537 0.077070184 -0.061216496]]...]
INFO - root - 2017-12-06 05:18:54.299500: step 12710, loss = 0.83, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 20h:31m:05s remains)
INFO - root - 2017-12-06 05:18:56.624167: step 12720, loss = 0.85, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 20h:50m:06s remains)
INFO - root - 2017-12-06 05:18:58.952230: step 12730, loss = 0.79, batch loss = 0.57 (35.5 examples/sec; 0.226 sec/batch; 20h:01m:52s remains)
INFO - root - 2017-12-06 05:19:01.300574: step 12740, loss = 0.74, batch loss = 0.53 (33.0 examples/sec; 0.242 sec/batch; 21h:31m:35s remains)
INFO - root - 2017-12-06 05:19:03.543194: step 12750, loss = 0.82, batch loss = 0.60 (36.8 examples/sec; 0.218 sec/batch; 19h:19m:41s remains)
INFO - root - 2017-12-06 05:19:05.848080: step 12760, loss = 0.81, batch loss = 0.60 (33.3 examples/sec; 0.240 sec/batch; 21h:21m:33s remains)
INFO - root - 2017-12-06 05:19:08.198008: step 12770, loss = 0.79, batch loss = 0.58 (33.1 examples/sec; 0.242 sec/batch; 21h:27m:07s remains)
INFO - root - 2017-12-06 05:19:10.461853: step 12780, loss = 0.77, batch loss = 0.56 (36.2 examples/sec; 0.221 sec/batch; 19h:37m:35s remains)
INFO - root - 2017-12-06 05:19:12.735653: step 12790, loss = 0.74, batch loss = 0.53 (34.0 examples/sec; 0.235 sec/batch; 20h:53m:10s remains)
INFO - root - 2017-12-06 05:19:15.054918: step 12800, loss = 0.80, batch loss = 0.58 (34.1 examples/sec; 0.235 sec/batch; 20h:50m:26s remains)
2017-12-06 05:19:15.459935: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35896978 0.17822051 -0.04604559 -0.31364927 -0.64981687 -0.83741963 -0.95022464 -0.99338937 -0.91723979 -0.78497732 -0.59938681 -0.49143183 -0.41362417 -0.21453068 0.0568991][0.76662433 0.53331792 0.16990694 -0.17436212 -0.52524626 -0.83717978 -1.030092 -1.1103963 -1.0524642 -0.91767371 -0.69903874 -0.47304645 -0.285469 -0.20468989 -0.041657437][0.51037717 0.2616272 0.034579992 -0.26803914 -0.61281908 -0.88417315 -1.05938 -1.2175275 -1.2528256 -1.1487148 -0.92551064 -0.7227968 -0.51805735 -0.25347561 0.01387389][0.12412491 -0.20344567 -0.39348692 -0.48136041 -0.64448416 -0.84855878 -1.0066857 -1.0540801 -1.0528795 -1.1006758 -1.0920802 -0.993209 -0.88993585 -0.79972148 -0.64921451][-0.27591145 -0.47838703 -0.57298791 -0.61269164 -0.57704371 -0.57832634 -0.67168689 -0.80929756 -0.95607281 -1.0618529 -1.1594535 -1.2602851 -1.3061688 -1.3042347 -1.1581805][-0.69248378 -0.73578322 -0.6321069 -0.5173437 -0.25764889 -0.14212067 -0.1674051 -0.38288161 -0.69230211 -0.96841967 -1.2033124 -1.3754259 -1.5246327 -1.5938917 -1.5234162][-0.83855069 -0.78640389 -0.67335284 -0.38607484 -0.10483374 0.13778508 0.25549155 0.18338692 -0.098154664 -0.54312277 -0.884339 -1.1227255 -1.3372306 -1.3851949 -1.3418282][-0.53400856 -0.62532175 -0.61107635 -0.324762 0.070507482 0.32272997 0.39210227 0.38161874 0.18710855 -0.085428871 -0.37398171 -0.68679309 -0.80335641 -0.92995954 -0.95283544][-0.36336115 -0.3136386 -0.19938228 -0.18984787 -0.0075785816 0.28859615 0.45325842 0.4032369 0.25478968 0.093965709 -0.11028851 -0.23930743 -0.21220401 -0.23996128 -0.2238733][-0.25173926 -0.2713466 -0.21032077 -0.13358164 -0.057424225 0.0056335703 0.1187744 0.2323918 0.23277456 0.11825202 -0.0003041327 -0.027380086 0.07579352 0.28781182 0.47944644][-0.033937771 -0.13503268 -0.14974894 -0.15320921 -0.24655674 -0.24250303 -0.26882398 -0.24629508 -0.14580074 -0.0843467 -0.036650945 0.060388267 0.30338225 0.54929125 0.81757557][0.054276928 0.038747743 0.036521964 -0.099552222 -0.29989314 -0.46805915 -0.54484141 -0.59478736 -0.6183064 -0.46717566 -0.26246125 -0.11480337 0.075279906 0.40430784 0.82071328][0.14209588 0.04552716 0.045832872 -0.010804512 -0.20590574 -0.39618054 -0.44563067 -0.47822452 -0.55179018 -0.60824192 -0.57526815 -0.4118644 -0.250969 -0.072813943 0.25684866][0.13225435 0.15232033 -0.030426726 -0.26961482 -0.36477461 -0.50944364 -0.630674 -0.62303495 -0.63393652 -0.74813831 -0.74530792 -0.74440145 -0.69450545 -0.56012237 -0.35262954][0.35708457 0.26682597 -0.020715289 -0.25407207 -0.35183603 -0.4992125 -0.66567945 -0.72672796 -0.78511965 -0.81898117 -0.7687099 -0.84482956 -0.81389177 -0.73287153 -0.55656636]]...]
INFO - root - 2017-12-06 05:19:17.805327: step 12810, loss = 0.75, batch loss = 0.54 (33.4 examples/sec; 0.240 sec/batch; 21h:16m:19s remains)
INFO - root - 2017-12-06 05:19:20.120236: step 12820, loss = 0.80, batch loss = 0.58 (34.8 examples/sec; 0.230 sec/batch; 20h:25m:31s remains)
INFO - root - 2017-12-06 05:19:22.451733: step 12830, loss = 0.78, batch loss = 0.56 (34.1 examples/sec; 0.235 sec/batch; 20h:50m:49s remains)
INFO - root - 2017-12-06 05:19:24.736829: step 12840, loss = 0.74, batch loss = 0.52 (35.6 examples/sec; 0.225 sec/batch; 19h:56m:27s remains)
INFO - root - 2017-12-06 05:19:27.069261: step 12850, loss = 0.84, batch loss = 0.63 (34.4 examples/sec; 0.233 sec/batch; 20h:39m:35s remains)
INFO - root - 2017-12-06 05:19:29.341460: step 12860, loss = 0.70, batch loss = 0.49 (35.9 examples/sec; 0.223 sec/batch; 19h:47m:51s remains)
INFO - root - 2017-12-06 05:19:31.624065: step 12870, loss = 0.85, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 19h:59m:15s remains)
INFO - root - 2017-12-06 05:19:33.976041: step 12880, loss = 0.81, batch loss = 0.60 (34.9 examples/sec; 0.229 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-06 05:19:36.291188: step 12890, loss = 0.80, batch loss = 0.59 (34.1 examples/sec; 0.235 sec/batch; 20h:50m:50s remains)
INFO - root - 2017-12-06 05:19:38.646270: step 12900, loss = 0.85, batch loss = 0.63 (32.8 examples/sec; 0.244 sec/batch; 21h:40m:50s remains)
2017-12-06 05:19:39.112032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14415063 -0.21857421 -0.30899426 -0.29603565 -0.22570878 -0.20641263 -0.18964088 -0.17133233 -0.13336284 -0.062940195 -0.017403655 -0.045176644 -0.069405466 -0.072818249 -0.075045533][-0.18928003 -0.35105416 -0.5584079 -0.63441914 -0.61917061 -0.49920574 -0.38599783 -0.26738954 -0.21426068 -0.1234755 -0.099199608 -0.10396293 -0.089741878 -0.075718418 -0.072725773][0.0014202818 -0.2094582 -0.4693397 -0.59792477 -0.62612009 -0.62423712 -0.60290736 -0.45958868 -0.35194284 -0.23737666 -0.18779534 -0.10450929 -0.079459496 -0.090419911 -0.091373689][0.35384735 0.13155232 -0.073200539 -0.28017926 -0.4325324 -0.47709924 -0.54115045 -0.50315517 -0.39926136 -0.29139423 -0.21631438 -0.14290105 -0.12959629 -0.10182306 -0.07557255][0.61243647 0.61315209 0.34854361 0.11190644 -0.035143148 -0.11895975 -0.22594258 -0.22256969 -0.31716263 -0.30207834 -0.31536484 -0.23168014 -0.097070538 -0.10747214 -0.11082419][0.59192586 0.67773247 0.63688928 0.47250694 0.22089076 0.06627579 -0.01301232 -0.052477431 -0.1419092 -0.19020942 -0.23590775 -0.20192671 -0.11054502 -0.11083527 -0.09385813][0.48075145 0.44185597 0.443968 0.34098381 0.20450193 0.10348371 0.062319502 0.043362603 -0.056738697 -0.099565059 -0.082188912 -0.048540439 -0.023841754 0.01810205 -0.021931753][0.35804513 0.29370809 0.35329345 0.25942498 0.25329664 0.13355523 0.047603056 -0.014191836 -0.10361733 -0.12845182 -0.084156983 -0.0021372512 0.0827329 0.067790553 0.025039442][0.24767295 0.23167336 0.14210089 0.039991796 0.039943904 0.12801079 0.087631285 0.01459828 -0.065449312 -0.19324777 -0.18579042 -0.11544278 -0.041313376 0.0084968507 0.029477738][0.28063938 0.39692956 0.29033616 0.18483958 0.015440799 0.19623157 0.098662168 0.13145366 0.07911247 -0.0073126331 -0.150312 -0.19430368 -0.14110091 -0.07604558 -0.070698932][0.21765572 0.42748135 0.54603845 0.39255852 0.21963298 0.31255263 0.175937 0.164617 0.26443064 0.24330029 0.065168485 -0.078033626 -0.13347447 -0.13673848 -0.091892973][0.40210509 0.4741978 0.48429567 0.34340018 0.28582293 0.42892379 0.40897408 0.42037231 0.49631834 0.50219488 0.3013618 0.0816955 -0.086535849 -0.19047678 -0.20491192][0.770225 0.92218089 0.92578316 0.69667834 0.48657298 0.39110839 0.39020756 0.533806 0.71598595 0.75093508 0.6265133 0.33410555 0.077917844 -0.18482305 -0.28464451][0.87100697 1.11565 1.0599586 0.845771 0.752178 0.61571908 0.51962906 0.60281748 0.77166504 0.90460092 0.76969957 0.589911 0.32803503 0.0084750205 -0.18874839][0.93392897 1.0952513 1.0503532 0.96364343 0.84840918 0.64621603 0.59696758 0.79936177 0.79689497 0.81672853 0.8397035 0.58881491 0.35351714 0.081714019 -0.13245147]]...]
INFO - root - 2017-12-06 05:19:41.400145: step 12910, loss = 0.77, batch loss = 0.55 (35.1 examples/sec; 0.228 sec/batch; 20h:12m:41s remains)
INFO - root - 2017-12-06 05:19:43.686622: step 12920, loss = 0.78, batch loss = 0.57 (35.0 examples/sec; 0.228 sec/batch; 20h:16m:19s remains)
INFO - root - 2017-12-06 05:19:45.986935: step 12930, loss = 0.79, batch loss = 0.58 (35.4 examples/sec; 0.226 sec/batch; 20h:02m:58s remains)
INFO - root - 2017-12-06 05:19:48.352260: step 12940, loss = 0.69, batch loss = 0.48 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:24s remains)
INFO - root - 2017-12-06 05:19:50.676337: step 12950, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 20h:42m:25s remains)
INFO - root - 2017-12-06 05:19:53.002528: step 12960, loss = 0.80, batch loss = 0.58 (32.5 examples/sec; 0.246 sec/batch; 21h:50m:17s remains)
INFO - root - 2017-12-06 05:19:55.355421: step 12970, loss = 0.81, batch loss = 0.59 (34.2 examples/sec; 0.234 sec/batch; 20h:44m:52s remains)
INFO - root - 2017-12-06 05:19:57.663535: step 12980, loss = 0.80, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:31m:44s remains)
INFO - root - 2017-12-06 05:20:00.032323: step 12990, loss = 0.81, batch loss = 0.60 (33.7 examples/sec; 0.237 sec/batch; 21h:03m:28s remains)
INFO - root - 2017-12-06 05:20:02.295846: step 13000, loss = 0.83, batch loss = 0.62 (35.6 examples/sec; 0.225 sec/batch; 19h:57m:02s remains)
2017-12-06 05:20:02.638323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053786352 -0.053510293 -0.053453349 -0.05340945 -0.05339044 -0.053357977 -0.053345412 -0.053340208 -0.053347357 -0.053361386 -0.053362515 -0.05339491 -0.053431455 -0.053473841 -0.053657725][-0.053788196 -0.053454924 -0.053352214 -0.053244788 -0.0531626 -0.053080983 -0.053023126 -0.052988186 -0.052990511 -0.053067777 -0.053108424 -0.053189036 -0.053289481 -0.053394042 -0.053513382][-0.053838797 -0.053264048 -0.053092808 -0.052899189 -0.052720591 -0.052558828 -0.052424755 -0.052330762 -0.05230958 -0.052424952 -0.052499142 -0.052687492 -0.052935489 -0.053190406 -0.053346727][-0.053814065 -0.052994832 -0.052731015 -0.052425578 -0.052118789 -0.051843055 -0.051606849 -0.051461671 -0.051413644 -0.05156114 -0.051715612 -0.052034277 -0.0524013 -0.052816797 -0.053185694][-0.053441379 -0.05254497 -0.052205462 -0.051812198 -0.05140733 -0.05102735 -0.050696302 -0.050498065 -0.050427165 -0.050601181 -0.050849803 -0.05131181 -0.051824886 -0.052413471 -0.053037539][-0.05426858 -0.052870795 -0.052083019 -0.051387005 -0.050824579 -0.050372191 -0.049992394 -0.049772091 -0.049703665 -0.049913041 -0.050247386 -0.050791793 -0.051402718 -0.052092411 -0.052891143][-0.058062926 -0.055365611 -0.05349071 -0.051996969 -0.050873891 -0.050165165 -0.049703531 -0.0494726 -0.049410611 -0.049693737 -0.050110921 -0.050768405 -0.051485106 -0.052263286 -0.0532514][-0.064758494 -0.060433909 -0.056930441 -0.053937174 -0.05177876 -0.05053686 -0.049857348 -0.049585361 -0.049519897 -0.049769796 -0.050177749 -0.050885487 -0.051648155 -0.052500434 -0.053647585][-0.074003443 -0.067745261 -0.062159009 -0.057395533 -0.053801 -0.051609248 -0.050438479 -0.050018862 -0.049959261 -0.050201841 -0.050616041 -0.051284473 -0.051971983 -0.052788708 -0.053987734][-0.084039435 -0.075884506 -0.068125084 -0.061482105 -0.056286633 -0.05305174 -0.051341098 -0.050674442 -0.050575275 -0.050767779 -0.051107034 -0.051709324 -0.052325632 -0.053052735 -0.054146186][-0.092347495 -0.082826249 -0.073392682 -0.065152019 -0.058585241 -0.054504558 -0.052309718 -0.051338382 -0.051177189 -0.051332682 -0.051590998 -0.052055802 -0.052517083 -0.0531749 -0.054155294][-0.096933544 -0.086624362 -0.076279327 -0.067264825 -0.060006727 -0.0554478 -0.052876543 -0.051713657 -0.051522806 -0.051577557 -0.051714864 -0.052056823 -0.052360248 -0.052853968 -0.053663827][-0.095922329 -0.086063668 -0.076055437 -0.067393437 -0.060351305 -0.055738423 -0.053072788 -0.051818375 -0.05157873 -0.051572293 -0.05164095 -0.051825091 -0.052004088 -0.05247093 -0.053181462][-0.090266392 -0.081727922 -0.073130004 -0.065675825 -0.059573688 -0.055507127 -0.053073749 -0.051802434 -0.051471785 -0.051415443 -0.051410548 -0.051524647 -0.051655434 -0.052027613 -0.052588418][-0.080868095 -0.074290395 -0.067941427 -0.06246157 -0.058034915 -0.055040691 -0.053172015 -0.052092321 -0.051686063 -0.051505502 -0.051346004 -0.051338144 -0.051400494 -0.051747572 -0.052275836]]...]
INFO - root - 2017-12-06 05:20:05.025386: step 13010, loss = 0.76, batch loss = 0.55 (33.7 examples/sec; 0.238 sec/batch; 21h:05m:33s remains)
INFO - root - 2017-12-06 05:20:07.311372: step 13020, loss = 0.82, batch loss = 0.61 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:50s remains)
INFO - root - 2017-12-06 05:20:09.655007: step 13030, loss = 0.77, batch loss = 0.55 (33.5 examples/sec; 0.239 sec/batch; 21h:10m:55s remains)
INFO - root - 2017-12-06 05:20:11.970884: step 13040, loss = 0.73, batch loss = 0.51 (33.9 examples/sec; 0.236 sec/batch; 20h:56m:21s remains)
INFO - root - 2017-12-06 05:20:14.252770: step 13050, loss = 0.81, batch loss = 0.59 (35.8 examples/sec; 0.224 sec/batch; 19h:50m:51s remains)
INFO - root - 2017-12-06 05:20:16.613525: step 13060, loss = 0.72, batch loss = 0.50 (32.6 examples/sec; 0.246 sec/batch; 21h:48m:02s remains)
INFO - root - 2017-12-06 05:20:18.882922: step 13070, loss = 0.77, batch loss = 0.55 (33.2 examples/sec; 0.241 sec/batch; 21h:22m:33s remains)
INFO - root - 2017-12-06 05:20:21.238524: step 13080, loss = 0.78, batch loss = 0.56 (36.2 examples/sec; 0.221 sec/batch; 19h:36m:10s remains)
INFO - root - 2017-12-06 05:20:23.530857: step 13090, loss = 0.70, batch loss = 0.49 (35.6 examples/sec; 0.224 sec/batch; 19h:54m:58s remains)
INFO - root - 2017-12-06 05:20:25.826619: step 13100, loss = 0.85, batch loss = 0.64 (33.3 examples/sec; 0.240 sec/batch; 21h:17m:55s remains)
2017-12-06 05:20:26.410131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2759051 -0.21120855 -0.18029842 -0.22032587 -0.29690552 -0.44621313 -0.59297794 -0.69733351 -0.74851477 -0.710026 -0.64290625 -0.55858427 -0.4702816 -0.368011 -0.33326298][-0.57849169 -0.52344614 -0.46767235 -0.43103337 -0.43645513 -0.51204884 -0.64100671 -0.80024439 -0.90959048 -0.98882425 -1.0419888 -1.0153744 -0.9322089 -0.83687872 -0.79280382][-0.56705415 -0.57119268 -0.51236922 -0.38181859 -0.31742442 -0.32320821 -0.40642112 -0.56362766 -0.73255503 -0.96829367 -1.1827352 -1.3278384 -1.3562731 -1.2865936 -1.2359167][-0.24779296 -0.23304988 -0.14402714 0.038570732 0.17146634 0.21952148 0.19237076 0.013859913 -0.27304721 -0.63883889 -1.0765624 -1.4298563 -1.653829 -1.7211571 -1.714944][0.30447918 0.4350931 0.55395138 0.6696524 0.73987138 0.78708404 0.7992487 0.64224118 0.30674231 -0.21363574 -0.81826782 -1.3606739 -1.79148 -2.0171103 -2.0271714][0.80212754 1.00801 1.1372921 1.19702 1.1551572 1.0968691 1.0421648 0.94117552 0.67566454 0.2618587 -0.22163785 -0.79960668 -1.3199464 -1.7006502 -1.8368962][1.0809575 1.356364 1.515981 1.5192151 1.4325745 1.3297198 1.1645254 0.99264389 0.78084052 0.54837286 0.2815541 -0.15389025 -0.58813131 -0.97068506 -1.185868][1.0585769 1.3587786 1.5177301 1.4756216 1.3461107 1.1818911 1.0259523 0.88688082 0.72985947 0.60076851 0.43024862 0.11556305 -0.17969866 -0.47595656 -0.61945713][0.80475509 1.0681678 1.2306082 1.2398975 1.0838059 0.83372128 0.64824939 0.49268204 0.34634167 0.2949028 0.21286659 0.079565749 -0.03792372 -0.16098368 -0.22875953][0.47999245 0.69042492 0.81734014 0.83432668 0.69902092 0.55230367 0.34622753 0.1425779 -0.052894577 -0.049554382 -0.069481961 -0.080918759 -0.13711023 -0.19974115 -0.25195587][0.1850846 0.31195867 0.39165288 0.38160938 0.28315151 0.17321314 0.0013730228 -0.22640957 -0.44468391 -0.52379811 -0.62820321 -0.61717272 -0.63087064 -0.56135732 -0.440431][-0.086331375 -0.062336426 -0.064086348 -0.10181427 -0.16232261 -0.27470338 -0.43870711 -0.67914903 -0.87257183 -1.0385725 -1.1717198 -1.1565268 -1.1109053 -0.99294186 -0.84900868][-0.18049955 -0.249467 -0.32117778 -0.4012506 -0.47385663 -0.60187572 -0.75024229 -0.957963 -1.1496965 -1.3337998 -1.3962649 -1.3829361 -1.314949 -1.1866281 -1.0298488][-0.16712491 -0.25034463 -0.36004233 -0.49052477 -0.61137182 -0.75395423 -0.87353468 -1.0190974 -1.1842403 -1.3111395 -1.3393487 -1.3894191 -1.3467655 -1.2298416 -1.0178379][-0.14876753 -0.20053858 -0.28429109 -0.40989149 -0.5611797 -0.73039955 -0.87823385 -1.0296679 -1.178787 -1.3047708 -1.3941321 -1.4053245 -1.2992468 -1.1401832 -0.88007814]]...]
INFO - root - 2017-12-06 05:20:28.752066: step 13110, loss = 0.81, batch loss = 0.60 (32.9 examples/sec; 0.244 sec/batch; 21h:36m:13s remains)
INFO - root - 2017-12-06 05:20:31.099193: step 13120, loss = 0.82, batch loss = 0.60 (33.6 examples/sec; 0.238 sec/batch; 21h:08m:53s remains)
INFO - root - 2017-12-06 05:20:33.442725: step 13130, loss = 0.78, batch loss = 0.57 (32.3 examples/sec; 0.248 sec/batch; 21h:57m:32s remains)
INFO - root - 2017-12-06 05:20:35.747411: step 13140, loss = 0.80, batch loss = 0.58 (35.4 examples/sec; 0.226 sec/batch; 20h:03m:33s remains)
INFO - root - 2017-12-06 05:20:38.102533: step 13150, loss = 0.76, batch loss = 0.54 (34.3 examples/sec; 0.234 sec/batch; 20h:42m:54s remains)
INFO - root - 2017-12-06 05:20:40.433071: step 13160, loss = 0.85, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 20h:31m:29s remains)
INFO - root - 2017-12-06 05:20:42.789834: step 13170, loss = 0.80, batch loss = 0.59 (32.8 examples/sec; 0.244 sec/batch; 21h:36m:32s remains)
INFO - root - 2017-12-06 05:20:45.113553: step 13180, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:21m:20s remains)
INFO - root - 2017-12-06 05:20:47.434750: step 13190, loss = 0.74, batch loss = 0.53 (34.2 examples/sec; 0.234 sec/batch; 20h:45m:32s remains)
INFO - root - 2017-12-06 05:20:49.783745: step 13200, loss = 0.83, batch loss = 0.62 (32.2 examples/sec; 0.248 sec/batch; 22h:00m:20s remains)
2017-12-06 05:20:50.178062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6290438 -3.1044683 -3.5077429 -3.75413 -3.8998582 -4.1753812 -4.6550364 -5.0103278 -4.9329395 -4.3564029 -3.4740989 -2.7879014 -2.4562881 -2.116235 -1.8826122][-2.6747689 -3.3276284 -3.8758318 -4.33963 -4.8701224 -5.2890239 -5.7560315 -6.2361069 -6.7572942 -6.9039578 -6.2441716 -4.7360959 -3.3465195 -2.3764143 -1.6348844][-3.4548521 -3.71425 -4.07158 -4.3204007 -4.4832168 -4.8224673 -5.7220163 -6.9034271 -7.9632964 -8.5970316 -8.321537 -7.2181087 -5.5088458 -3.5769594 -2.1212819][-3.0718052 -3.5465689 -3.7910533 -3.8372676 -3.8953426 -3.8406365 -4.0143967 -4.8267241 -6.5187826 -8.4064636 -9.0589571 -8.2760935 -6.6351762 -4.4520807 -2.5298991][-2.2541258 -2.5520351 -2.5697191 -2.4766834 -2.0890827 -1.8074523 -2.1046293 -2.9820015 -4.1400228 -5.8679309 -6.9538 -6.9102345 -5.991858 -4.3488197 -2.5209126][-1.8436246 -2.0524776 -1.8237821 -1.1989236 -0.56385213 -0.1358725 0.03988605 -0.35738018 -1.4654945 -3.1999371 -4.605926 -5.2781577 -4.9078207 -3.502578 -1.7659129][-1.1371222 -1.6130769 -1.5837182 -0.95584917 0.33492979 1.689629 2.5273445 2.3931413 2.0057976 0.64176559 -0.953986 -2.2568014 -2.6403639 -1.9996506 -1.2239697][-0.24794564 -0.65306979 -0.96687192 -0.79715729 -0.016956545 1.1835258 2.7906692 3.7695854 4.1603155 3.6262362 2.5400989 0.8950721 -0.51843125 -0.62436312 -0.16730326][-0.081359863 -0.37084463 -0.47854325 -0.15240149 0.42575657 1.3314584 2.5952747 3.9396665 4.54739 4.387795 3.8572948 2.480633 1.1175168 -0.0016886294 -0.20638248][-0.1505207 -0.4151592 -0.74043179 -0.57915318 -0.0028948933 0.98484588 2.3080759 3.5475175 4.4833694 4.5554261 3.804852 2.8551273 1.268561 0.18321547 -0.16442662][-0.44513965 -0.52943218 -0.63284963 -0.81088686 -0.69789952 -0.214131 0.70748192 1.8964176 2.9015987 3.2934172 3.0591357 2.5867143 1.9032844 0.9225781 0.3181293][-0.296296 -0.58016986 -0.81974947 -0.73139995 -0.64121896 -0.63764155 -0.43393832 -0.16743714 0.2234157 0.87868744 1.4505672 1.7953057 1.6050709 1.1389567 0.49863178][-0.31181377 -0.58104277 -1.0452862 -1.2624878 -1.2159028 -0.99799407 -0.908668 -0.90506756 -0.88204503 -0.934702 -1.011987 -0.723959 -0.45926487 -0.3010557 -0.3844119][-0.23557413 -0.36757845 -0.72498596 -1.2275823 -1.6280059 -1.7050937 -1.5662464 -1.4290209 -1.3868134 -1.392019 -1.3396907 -1.1950022 -1.1559427 -0.75162143 -0.41397706][-0.27131915 -0.35159481 -0.44573593 -0.58950877 -1.0345973 -1.4343895 -1.6454172 -1.8793494 -1.7296766 -1.3659599 -1.1887988 -1.0040158 -1.0654906 -1.0303301 -0.8864187]]...]
INFO - root - 2017-12-06 05:20:52.468995: step 13210, loss = 0.83, batch loss = 0.62 (34.8 examples/sec; 0.230 sec/batch; 20h:23m:29s remains)
INFO - root - 2017-12-06 05:20:54.794911: step 13220, loss = 0.79, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:22m:43s remains)
INFO - root - 2017-12-06 05:20:57.112158: step 13230, loss = 0.77, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:35m:19s remains)
INFO - root - 2017-12-06 05:20:59.413634: step 13240, loss = 0.71, batch loss = 0.50 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:46s remains)
INFO - root - 2017-12-06 05:21:01.740852: step 13250, loss = 0.81, batch loss = 0.59 (34.0 examples/sec; 0.235 sec/batch; 20h:52m:43s remains)
INFO - root - 2017-12-06 05:21:04.075688: step 13260, loss = 0.90, batch loss = 0.68 (35.0 examples/sec; 0.228 sec/batch; 20h:15m:00s remains)
INFO - root - 2017-12-06 05:21:06.403210: step 13270, loss = 0.89, batch loss = 0.67 (33.6 examples/sec; 0.238 sec/batch; 21h:07m:29s remains)
INFO - root - 2017-12-06 05:21:08.705127: step 13280, loss = 0.77, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:33m:52s remains)
INFO - root - 2017-12-06 05:21:11.056045: step 13290, loss = 0.79, batch loss = 0.58 (34.5 examples/sec; 0.232 sec/batch; 20h:33m:33s remains)
INFO - root - 2017-12-06 05:21:13.355501: step 13300, loss = 0.84, batch loss = 0.63 (33.4 examples/sec; 0.240 sec/batch; 21h:14m:17s remains)
2017-12-06 05:21:14.066767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1550734 -1.1805328 -1.1655971 -1.7010862 -2.474555 -3.2362385 -3.8361449 -3.1490829 -2.6140664 -2.9788251 -2.9036992 -2.4602859 -2.6525517 -3.0691469 -3.1764491][-0.372786 -1.1153383 -1.416972 -1.7619668 -2.2786896 -2.7647631 -2.8423455 -3.4119835 -3.2196751 -1.9669228 -1.7566015 -2.0145404 -2.0393679 -1.6101624 -2.0232894][0.033573195 -0.61923438 -1.8847287 -2.4371316 -2.6161189 -2.6240335 -2.6052117 -2.5751312 -1.9907228 -1.5875416 -1.4322478 -1.4658066 -1.6789677 -0.85957092 -0.89775139][-0.669365 -1.3154783 -2.2172518 -3.0288548 -3.8227382 -3.8507557 -3.08057 -2.2206776 -1.9214789 -1.789525 -1.3294835 -1.0653085 -0.83305562 -0.80280352 -0.99852383][-1.0245731 -2.195997 -3.0743849 -2.9751253 -3.2079511 -3.1569617 -2.9655688 -3.2767408 -2.5096924 -1.4894385 -0.77227712 -1.2230551 -0.94102579 -0.22527026 -0.39094377][-1.4945976 -2.0008132 -2.6558671 -3.5664325 -3.8188183 -3.2990942 -2.6572933 -1.9453768 -1.5355023 -1.7235607 -1.7896676 -0.6244151 -0.21374041 -0.18142413 -1.163306][-0.49906915 -1.3062848 -2.168643 -2.4266009 -2.1372752 -2.3874183 -2.2360322 -1.5625632 -0.36195481 0.9325546 1.4614832 1.4127514 0.83774424 1.3956554 1.2380317][-0.9434576 -1.1062016 -1.2529708 -0.798255 -0.54949474 -0.52387547 -0.45192122 -0.36839986 0.28352642 0.8100006 1.1023598 2.485553 3.3776121 2.9390154 2.3585322][-0.46518296 0.42691153 0.21599375 0.035473406 0.37663877 0.74810153 1.2691733 1.1627021 1.330519 1.8396745 2.9265671 3.0628505 2.4151161 3.3279612 3.3538456][0.34645891 0.32007563 0.25493586 0.3831529 1.1652805 0.74497575 0.34580541 1.1339272 1.612191 1.4644765 1.9432043 3.133816 3.8607841 3.3128467 2.9938226][0.018054157 0.43595439 0.63631427 0.23865397 0.007873185 0.77576888 1.7183903 1.5680759 0.86459231 1.4729568 2.2682812 2.2500546 3.0263538 4.1983991 4.4125643][1.126603 0.25290865 0.16221695 0.90492088 0.60355622 0.43538946 0.848929 2.2813697 2.5745301 1.4994113 0.53459358 1.0660068 1.9548353 2.334728 2.3568954][0.29353374 0.91558051 0.85789561 0.53338474 0.776449 1.5449238 2.5209184 2.1324306 1.2486489 1.0873225 0.9570893 0.91781521 0.54268956 0.11557436 0.12023854][1.1881686 0.52624249 -0.28774571 0.88452333 1.6065544 1.2140461 1.5722072 1.8600388 1.3734297 0.69254023 0.18745814 -0.22424711 -0.3444519 -0.36131144 -1.2226727][1.6521254 0.82978976 0.30736077 -0.82419527 -0.45935768 0.9261086 1.3970481 -0.024037249 -0.8064844 -0.40136409 -0.41477907 -1.1741303 -1.8268925 -2.1368608 -2.5151136]]...]
INFO - root - 2017-12-06 05:21:16.381994: step 13310, loss = 0.88, batch loss = 0.67 (35.2 examples/sec; 0.227 sec/batch; 20h:09m:12s remains)
INFO - root - 2017-12-06 05:21:18.672440: step 13320, loss = 0.87, batch loss = 0.66 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-06 05:21:20.968945: step 13330, loss = 0.82, batch loss = 0.61 (35.8 examples/sec; 0.223 sec/batch; 19h:48m:41s remains)
INFO - root - 2017-12-06 05:21:23.326513: step 13340, loss = 0.78, batch loss = 0.56 (34.8 examples/sec; 0.230 sec/batch; 20h:23m:00s remains)
INFO - root - 2017-12-06 05:21:25.626995: step 13350, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.234 sec/batch; 20h:47m:10s remains)
INFO - root - 2017-12-06 05:21:27.917226: step 13360, loss = 0.85, batch loss = 0.64 (34.6 examples/sec; 0.231 sec/batch; 20h:28m:29s remains)
INFO - root - 2017-12-06 05:21:30.222494: step 13370, loss = 0.87, batch loss = 0.65 (35.8 examples/sec; 0.223 sec/batch; 19h:48m:37s remains)
INFO - root - 2017-12-06 05:21:32.511558: step 13380, loss = 0.86, batch loss = 0.64 (35.0 examples/sec; 0.229 sec/batch; 20h:15m:47s remains)
INFO - root - 2017-12-06 05:21:34.824083: step 13390, loss = 0.82, batch loss = 0.60 (34.0 examples/sec; 0.236 sec/batch; 20h:52m:56s remains)
INFO - root - 2017-12-06 05:21:37.086746: step 13400, loss = 0.79, batch loss = 0.58 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:33s remains)
2017-12-06 05:21:37.442012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072396137 -0.0729039 -0.074171253 -0.076412871 -0.079452582 -0.083331943 -0.088008218 -0.092969812 -0.093588069 -0.087873578 -0.08295083 -0.085790344 -0.08605396 -0.084896736 -0.079805583][-0.072600774 -0.073270448 -0.074819818 -0.077294804 -0.079972975 -0.083873875 -0.0878919 -0.092210636 -0.091674514 -0.085013285 -0.081994914 -0.084397696 -0.083858661 -0.083700866 -0.0794925][-0.072723806 -0.073157236 -0.074430637 -0.076581009 -0.078273833 -0.0804825 -0.082923226 -0.088823222 -0.093493611 -0.0932875 -0.093877614 -0.092738591 -0.088957526 -0.086571284 -0.079864077][-0.072765082 -0.072669312 -0.073481396 -0.075013749 -0.075644381 -0.074864149 -0.075455613 -0.084444165 -0.10034035 -0.11036909 -0.11395673 -0.10846 -0.10450697 -0.09691152 -0.084974036][-0.072782069 -0.072695345 -0.0740105 -0.075939476 -0.076381385 -0.071399428 -0.066437215 -0.073311463 -0.097518519 -0.11327837 -0.11517388 -0.10753717 -0.10996726 -0.10490091 -0.089878038][-0.0728471 -0.072918959 -0.075407423 -0.078052953 -0.077639349 -0.066297606 -0.05386864 -0.055875368 -0.080051258 -0.09312325 -0.09055084 -0.083908513 -0.097598918 -0.10486224 -0.09234529][-0.072900593 -0.072640143 -0.075015508 -0.0769317 -0.073405281 -0.055678427 -0.038386185 -0.040791851 -0.0640856 -0.072247177 -0.064948052 -0.057867076 -0.0799142 -0.098770909 -0.090313904][-0.072910994 -0.072124861 -0.073779896 -0.074869193 -0.068896905 -0.045813203 -0.028118331 -0.039886739 -0.0634121 -0.067150235 -0.055650767 -0.049447477 -0.072183445 -0.094253182 -0.086894125][-0.072978847 -0.071445607 -0.072504133 -0.073896065 -0.065805338 -0.038336888 -0.022427283 -0.045780409 -0.065796353 -0.065143853 -0.050662212 -0.046875369 -0.066751577 -0.086909458 -0.082354009][-0.073133945 -0.07153213 -0.072741352 -0.074692406 -0.066309109 -0.036274802 -0.021674961 -0.052524447 -0.068021588 -0.064659275 -0.051506381 -0.049362663 -0.063869059 -0.080884427 -0.079096437][-0.073566683 -0.0727999 -0.074424915 -0.077829592 -0.072888143 -0.048835274 -0.038916893 -0.067347191 -0.076726705 -0.075110957 -0.067921177 -0.065613896 -0.069254965 -0.076147392 -0.077036634][-0.0740691 -0.074306712 -0.075496383 -0.078492537 -0.07639946 -0.061276007 -0.054777108 -0.071315385 -0.074997634 -0.07402356 -0.072821781 -0.073374584 -0.072810464 -0.074337386 -0.075198248][-0.074297868 -0.074461333 -0.075426504 -0.077331394 -0.077175722 -0.072363257 -0.068325035 -0.072794 -0.073212311 -0.071083583 -0.072114222 -0.073825419 -0.073906913 -0.073756687 -0.073597319][-0.074368447 -0.074205659 -0.074540034 -0.07523118 -0.075561978 -0.074809656 -0.073909447 -0.074773252 -0.072886862 -0.070762582 -0.069855437 -0.0713019 -0.073109522 -0.074381284 -0.074046694][-0.074500963 -0.074047461 -0.074098714 -0.074347921 -0.074250296 -0.074506931 -0.074759305 -0.075406983 -0.074997112 -0.072159082 -0.070249192 -0.069564678 -0.0716395 -0.074175164 -0.074641183]]...]
INFO - root - 2017-12-06 05:21:39.775072: step 13410, loss = 0.83, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 20h:43m:13s remains)
INFO - root - 2017-12-06 05:21:42.033014: step 13420, loss = 0.84, batch loss = 0.62 (36.1 examples/sec; 0.222 sec/batch; 19h:39m:33s remains)
INFO - root - 2017-12-06 05:21:44.294120: step 13430, loss = 0.80, batch loss = 0.59 (36.4 examples/sec; 0.220 sec/batch; 19h:28m:05s remains)
INFO - root - 2017-12-06 05:21:46.599284: step 13440, loss = 0.79, batch loss = 0.58 (35.6 examples/sec; 0.225 sec/batch; 19h:56m:00s remains)
INFO - root - 2017-12-06 05:21:48.882728: step 13450, loss = 0.80, batch loss = 0.59 (32.9 examples/sec; 0.244 sec/batch; 21h:34m:58s remains)
INFO - root - 2017-12-06 05:21:51.191207: step 13460, loss = 0.80, batch loss = 0.59 (34.3 examples/sec; 0.233 sec/batch; 20h:39m:21s remains)
INFO - root - 2017-12-06 05:21:53.477384: step 13470, loss = 0.86, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 20h:49m:39s remains)
INFO - root - 2017-12-06 05:21:55.825640: step 13480, loss = 0.75, batch loss = 0.53 (35.6 examples/sec; 0.225 sec/batch; 19h:55m:42s remains)
INFO - root - 2017-12-06 05:21:58.133730: step 13490, loss = 0.72, batch loss = 0.51 (35.4 examples/sec; 0.226 sec/batch; 20h:01m:08s remains)
INFO - root - 2017-12-06 05:22:00.443102: step 13500, loss = 0.75, batch loss = 0.54 (34.7 examples/sec; 0.230 sec/batch; 20h:24m:23s remains)
2017-12-06 05:22:02.110137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.94307959 -1.1064221 -1.4208373 -1.7273511 -1.8475232 -1.8464067 -1.7514646 -1.5469356 -1.2163321 -0.91746414 -0.719501 -0.65805972 -0.73303127 -0.98223513 -1.4823072][-1.3825332 -1.4349984 -1.7184819 -1.908829 -2.016505 -1.9885138 -1.858834 -1.6231363 -1.2850806 -0.89493483 -0.68167788 -0.55312371 -0.59561372 -0.87366533 -1.3407837][-1.5601728 -1.7438431 -1.8476422 -1.9443467 -1.928651 -1.7848697 -1.5627005 -1.2203681 -0.86032248 -0.626016 -0.5006066 -0.36621487 -0.45810142 -0.66053188 -0.98526967][-1.4601169 -1.7031965 -1.8530096 -1.8866698 -1.7203331 -1.322144 -0.80993807 -0.31724614 0.045300312 0.25133464 0.26148254 0.16994879 -0.00070382655 -0.20764928 -0.38690111][-1.0885414 -1.3632641 -1.401085 -1.4051965 -0.94870085 -0.37386584 0.15870124 0.57559842 0.89179343 1.0186856 1.0007477 0.93088818 0.71618825 0.55649638 0.39884356][-0.43083543 -0.61532384 -0.63956225 -0.43412232 0.097224288 0.72218049 1.2499483 1.6221991 1.8936319 1.9321579 1.731674 1.5305482 1.2369045 0.98150456 0.53220159][0.42863894 0.30217469 0.3185603 0.62112677 1.1022325 1.5641268 1.978423 2.2400866 2.3554525 2.1879818 1.9539539 1.7031426 1.1935711 0.84972906 0.42888629][0.88764226 0.66906083 0.68749189 1.0276536 1.4295197 1.8207996 2.1402514 2.22581 2.1016865 1.7939049 1.4663693 1.0973973 0.77211016 0.40603411 2.5846064e-05][1.0000714 0.81548309 0.63049769 0.80118221 1.1345242 1.4833761 1.6954195 1.6694859 1.4851271 1.2023499 0.70709491 0.29573607 -0.077020667 -0.44351584 -0.69584435][0.37735868 0.33722752 0.36600521 0.3958613 0.47839284 0.588914 0.69733089 0.72575349 0.54869157 0.22038156 -0.15160209 -0.45898187 -0.6045028 -0.91129959 -1.1269106][-0.15442258 -0.3204574 -0.50412327 -0.438475 -0.31789529 -0.18312371 -0.18769693 -0.28829083 -0.38586468 -0.522784 -0.85695571 -1.1130253 -1.1260694 -1.2650472 -1.3542103][-0.62745184 -0.88914138 -1.1549022 -1.329304 -1.3734016 -1.2310877 -1.0885668 -0.96307462 -1.0395164 -1.1073405 -1.2478255 -1.5323803 -1.5158941 -1.5826174 -1.5328914][-1.2242042 -1.4502486 -1.6438848 -1.7964736 -1.8700876 -1.8115875 -1.7505498 -1.6175085 -1.5939617 -1.6264671 -1.7019471 -1.7865444 -1.916564 -1.9711868 -1.9086336][-1.7480372 -2.0071807 -2.1602995 -2.0674636 -1.9905517 -1.8819612 -1.7329888 -1.8072126 -1.9232317 -1.9896027 -2.0551796 -2.06363 -2.0160089 -1.867661 -1.8189925][-1.9829682 -2.3244145 -2.4683938 -2.2050331 -2.0262971 -1.9272584 -1.691813 -1.6929464 -1.8711817 -2.0259249 -2.1931188 -2.2666731 -2.2482326 -1.8991221 -1.6435598]]...]
INFO - root - 2017-12-06 05:22:04.443145: step 13510, loss = 0.71, batch loss = 0.49 (35.4 examples/sec; 0.226 sec/batch; 20h:00m:56s remains)
INFO - root - 2017-12-06 05:22:06.751630: step 13520, loss = 0.71, batch loss = 0.49 (34.8 examples/sec; 0.230 sec/batch; 20h:21m:46s remains)
INFO - root - 2017-12-06 05:22:09.104621: step 13530, loss = 0.85, batch loss = 0.64 (33.7 examples/sec; 0.238 sec/batch; 21h:03m:27s remains)
INFO - root - 2017-12-06 05:22:11.406442: step 13540, loss = 0.75, batch loss = 0.53 (35.9 examples/sec; 0.223 sec/batch; 19h:44m:21s remains)
INFO - root - 2017-12-06 05:22:13.679602: step 13550, loss = 0.82, batch loss = 0.60 (35.6 examples/sec; 0.225 sec/batch; 19h:54m:20s remains)
INFO - root - 2017-12-06 05:22:16.039336: step 13560, loss = 0.78, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 20h:19m:38s remains)
INFO - root - 2017-12-06 05:22:18.326092: step 13570, loss = 0.82, batch loss = 0.60 (35.0 examples/sec; 0.229 sec/batch; 20h:15m:42s remains)
INFO - root - 2017-12-06 05:22:20.596583: step 13580, loss = 0.74, batch loss = 0.53 (35.2 examples/sec; 0.227 sec/batch; 20h:06m:26s remains)
INFO - root - 2017-12-06 05:22:22.913839: step 13590, loss = 0.81, batch loss = 0.60 (33.0 examples/sec; 0.242 sec/batch; 21h:28m:36s remains)
INFO - root - 2017-12-06 05:22:25.232793: step 13600, loss = 0.72, batch loss = 0.50 (34.9 examples/sec; 0.229 sec/batch; 20h:18m:41s remains)
2017-12-06 05:22:25.568801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37103251 -0.43653896 -0.55888116 -0.81127685 -1.2146332 -1.3161576 -1.3024395 -1.2656842 -1.2997185 -1.2432151 -1.1719745 -1.0935292 -0.98942018 -0.78285974 -0.49844539][-1.1649696 -1.3254917 -1.4512113 -1.5656604 -1.8326548 -2.0727634 -2.1927273 -2.1052635 -2.0812337 -2.0396256 -2.0544806 -1.9286829 -1.621453 -1.2885936 -1.0478507][-1.582031 -1.8811116 -2.001508 -1.9435778 -1.9290255 -2.0997972 -2.3561192 -2.4330423 -2.4890029 -2.520762 -2.6001253 -2.5911996 -2.4606886 -2.1639979 -1.8334326][-1.5756147 -1.8729081 -1.9470503 -1.699391 -1.4378203 -1.2924309 -1.3260113 -1.4856079 -1.7173028 -1.8789265 -1.9563351 -2.2020447 -2.285768 -2.0927103 -1.9553858][-1.2707801 -1.2254748 -1.2207705 -0.91618347 -0.29708707 0.1736103 0.30629662 0.21709082 -0.01264213 -0.37931782 -0.65355349 -0.87927955 -1.0400798 -1.2751042 -1.4570305][-0.90940577 -0.61854124 -0.20605679 0.39727855 1.1131649 1.6822653 1.9351236 1.7925104 1.4556364 1.040586 0.69971758 0.3613857 0.090862088 -0.25955448 -0.69421363][-0.42883146 -0.020464584 0.60066092 1.3556333 2.007374 2.6646283 3.0883365 2.9699953 2.5135777 1.9883059 1.5964228 1.1371429 0.80158359 0.40437427 0.052289851][0.05268658 0.33455175 0.84978557 1.5149152 2.0953236 2.4520628 2.6956518 2.7639446 2.4856715 1.8334188 1.3049427 0.85675377 0.45387766 0.12816942 -0.0048949122][0.327952 0.62449867 1.0782437 1.4098223 1.6093755 1.5803022 1.4693027 1.43313 1.1416934 0.63117725 0.063801013 -0.30799416 -0.34593889 -0.38476077 -0.32354197][0.29938933 0.37955689 0.49824992 0.73618925 0.98773426 0.76770574 0.24815333 -0.16490205 -0.60498363 -0.99061656 -1.2062888 -1.5803635 -1.7245826 -1.0287508 -0.29020235][0.057788931 0.41919467 0.51334548 0.24019933 0.18001857 0.10797345 -0.29651412 -1.0012264 -1.7045261 -2.0377908 -2.2836263 -2.2294269 -1.6234239 -0.82022792 -0.096334994][-0.60833186 -0.32497081 -0.04551689 0.031417072 -0.20951974 -0.48091561 -0.78884315 -0.99276429 -1.586112 -1.8665429 -2.0572071 -2.1141157 -1.8829948 -0.93399394 0.011053115][-0.65831721 -0.41524386 -0.33138165 -0.38062483 -0.35762313 -0.26718339 -0.40279415 -0.40382615 -0.6925897 -0.82006556 -1.0753938 -1.1832539 -1.2583905 -0.86458975 -0.11970925][-0.20319554 -0.13944177 -0.03299756 -0.06332276 -0.11778195 0.0076552704 0.018469937 0.1141221 -0.13479103 -0.30793872 -0.6799618 -0.83569872 -0.78961951 -0.45030025 -0.027961303][0.73033673 0.61928129 0.47182027 0.1719833 -0.053823594 0.061887048 0.084834449 0.12007385 -0.18790464 -0.45838162 -0.81725919 -0.92863846 -1.0383588 -0.74490917 -0.29697964]]...]
INFO - root - 2017-12-06 05:22:27.859829: step 13610, loss = 0.82, batch loss = 0.61 (34.6 examples/sec; 0.231 sec/batch; 20h:27m:55s remains)
INFO - root - 2017-12-06 05:22:30.179052: step 13620, loss = 0.78, batch loss = 0.57 (35.8 examples/sec; 0.224 sec/batch; 19h:48m:39s remains)
INFO - root - 2017-12-06 05:22:32.479242: step 13630, loss = 0.76, batch loss = 0.55 (35.8 examples/sec; 0.224 sec/batch; 19h:47m:50s remains)
INFO - root - 2017-12-06 05:22:34.741258: step 13640, loss = 0.86, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 20h:04m:00s remains)
INFO - root - 2017-12-06 05:22:37.013189: step 13650, loss = 0.74, batch loss = 0.52 (36.2 examples/sec; 0.221 sec/batch; 19h:33m:02s remains)
INFO - root - 2017-12-06 05:22:39.297341: step 13660, loss = 0.89, batch loss = 0.67 (34.8 examples/sec; 0.230 sec/batch; 20h:21m:52s remains)
INFO - root - 2017-12-06 05:22:41.587154: step 13670, loss = 0.77, batch loss = 0.55 (35.7 examples/sec; 0.224 sec/batch; 19h:52m:13s remains)
INFO - root - 2017-12-06 05:22:43.867654: step 13680, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.226 sec/batch; 19h:58m:52s remains)
INFO - root - 2017-12-06 05:22:46.171295: step 13690, loss = 0.80, batch loss = 0.59 (35.8 examples/sec; 0.223 sec/batch; 19h:46m:53s remains)
INFO - root - 2017-12-06 05:22:48.463226: step 13700, loss = 0.80, batch loss = 0.58 (35.5 examples/sec; 0.226 sec/batch; 19h:58m:42s remains)
2017-12-06 05:22:48.858213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.078447305 -0.22383179 -0.27777833 -0.27180842 -0.28914171 -0.22356832 -0.18867016 -0.19992246 -0.27629942 -0.34354192 -0.4012568 -0.48113585 -0.5043183 -0.35861015 -0.22685924][0.055654779 0.025227763 0.095406771 0.12082511 0.033849873 0.0903624 0.056811437 0.039174542 -0.028272748 -0.15231848 -0.24958269 -0.29274091 -0.20419914 -0.0872371 0.0361121][0.077746078 -0.00131277 -0.050604977 0.033957437 0.2104183 0.29227942 0.39064926 0.42896116 0.32571542 0.18917646 -0.012765527 -0.17102724 -0.31508744 -0.28663278 -0.14488164][-0.0041070729 -0.01877778 -0.12521201 -0.16507378 -0.13381839 0.0028418526 0.20372318 0.32827497 0.43787187 0.40674442 0.21503381 0.028937235 -0.136368 -0.18857987 -0.17736737][0.034133449 0.016100794 -0.052754931 -0.14511135 -0.19182345 -0.1425615 -0.09198565 -0.0016809627 0.060387671 0.097651809 0.07682234 -0.028636772 -0.20347878 -0.24727021 -0.20261995][-0.1331438 -0.12990713 -0.1325458 -0.065628529 -0.058878582 -0.0761585 -0.11257634 -0.16036393 -0.22926554 -0.18814093 -0.12184355 -0.1304459 -0.12064207 -0.17009884 -0.1609244][-0.041385327 0.036550641 0.17387311 0.27991164 0.2716825 0.25238335 0.13655713 0.060263023 0.017901853 -0.011280835 -0.025671624 0.036835596 0.11367342 0.17666139 0.18386914][0.057170242 0.12856774 0.25859433 0.40550703 0.49031734 0.46072549 0.33456826 0.24471422 0.0950492 0.00042887777 0.001974225 0.049801767 0.088047624 0.11647347 0.15350576][0.026528753 0.012243889 -0.011227332 0.046484157 0.14114177 0.23696528 0.24611743 0.13843161 -0.038412627 -0.14158712 -0.2235202 -0.23318566 -0.19271624 -0.1324738 -0.11636424][-0.029478464 -0.039238494 -0.087816708 -0.09577705 -0.0842127 -0.067758411 -0.12426498 -0.18941155 -0.25214183 -0.26512098 -0.25033331 -0.19785726 -0.13480642 -0.074752845 -0.031861831][-0.10945965 -0.12306808 -0.17203942 -0.22211123 -0.24767615 -0.25183609 -0.24032356 -0.25666124 -0.27317187 -0.31560975 -0.31254864 -0.23591547 -0.17079884 -0.14719284 -0.15755659][-0.1631327 -0.20422035 -0.24674064 -0.30751914 -0.36817396 -0.39652568 -0.40969098 -0.45750225 -0.47536039 -0.45723087 -0.41067559 -0.37398285 -0.32104403 -0.22930805 -0.1628262][-0.19241616 -0.20427221 -0.23121966 -0.28027344 -0.34565789 -0.43078929 -0.5096522 -0.57404971 -0.60538965 -0.5836975 -0.49712783 -0.37869543 -0.262187 -0.20908871 -0.19142875][-0.11736755 -0.13453795 -0.17084354 -0.20921654 -0.26663661 -0.36709625 -0.45964855 -0.49261898 -0.50818086 -0.535906 -0.52101648 -0.44170231 -0.41231877 -0.42902488 -0.45676458][-0.10603736 -0.16477467 -0.24859323 -0.33389619 -0.42242992 -0.56334251 -0.70817012 -0.82089776 -0.91306847 -0.90160537 -0.86908215 -0.80207461 -0.70586634 -0.5302552 -0.44625258]]...]
INFO - root - 2017-12-06 05:22:51.142924: step 13710, loss = 0.77, batch loss = 0.56 (35.2 examples/sec; 0.228 sec/batch; 20h:08m:52s remains)
INFO - root - 2017-12-06 05:22:53.465116: step 13720, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:57m:26s remains)
INFO - root - 2017-12-06 05:22:55.771444: step 13730, loss = 0.86, batch loss = 0.65 (33.1 examples/sec; 0.242 sec/batch; 21h:23m:41s remains)
INFO - root - 2017-12-06 05:22:58.076136: step 13740, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.229 sec/batch; 20h:14m:36s remains)
INFO - root - 2017-12-06 05:23:00.383219: step 13750, loss = 0.74, batch loss = 0.52 (34.4 examples/sec; 0.233 sec/batch; 20h:37m:05s remains)
INFO - root - 2017-12-06 05:23:02.669919: step 13760, loss = 0.76, batch loss = 0.55 (35.8 examples/sec; 0.224 sec/batch; 19h:47m:18s remains)
INFO - root - 2017-12-06 05:23:05.011951: step 13770, loss = 0.79, batch loss = 0.58 (36.2 examples/sec; 0.221 sec/batch; 19h:35m:05s remains)
INFO - root - 2017-12-06 05:23:07.303804: step 13780, loss = 0.73, batch loss = 0.51 (34.2 examples/sec; 0.234 sec/batch; 20h:40m:55s remains)
INFO - root - 2017-12-06 05:23:09.655079: step 13790, loss = 0.77, batch loss = 0.56 (33.7 examples/sec; 0.237 sec/batch; 21h:00m:57s remains)
INFO - root - 2017-12-06 05:23:11.976387: step 13800, loss = 0.77, batch loss = 0.55 (34.7 examples/sec; 0.230 sec/batch; 20h:22m:56s remains)
2017-12-06 05:23:12.821117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034550555 -0.51179379 0.0015103295 0.5508579 0.99618965 1.6131201 1.6737483 1.7256116 1.3715056 1.0274481 0.72888213 0.42572269 0.38790035 0.26456848 0.12438061][-0.036089495 -0.72647637 -0.68877149 -0.15127595 0.48870853 1.0766438 1.2535197 1.2515358 1.2122304 0.91399163 0.69406509 0.55297744 0.4682456 0.19314703 0.017770432][-0.44286907 -1.0365623 -1.0744681 -0.74600166 -0.39335272 0.0908976 0.5725801 0.7693432 0.598584 0.40442926 0.22258994 0.1196404 0.061981685 0.034672841 0.093893938][-0.13968806 -0.53460431 -1.0720835 -0.8485744 -0.54958034 -0.2261689 0.39508489 0.48446372 0.49491838 0.14189494 -0.29046482 -0.35208371 -0.33746526 -0.31069031 -0.21642736][0.47507426 0.157702 -0.43347341 -0.351091 -0.31189105 -0.37136376 -0.09226495 0.033167891 0.32756418 0.12294238 0.047300182 -0.25450796 -0.5205369 -0.50610578 -0.39988431][0.850347 0.75371307 0.410107 0.46404263 0.188236 0.18935069 0.41021088 0.027567238 0.05953712 -0.21902376 -0.11990812 -0.260824 -0.4213233 -0.41531059 -0.43011284][1.2405505 1.7859331 1.450124 1.3040184 1.0727232 0.91951555 0.81379664 0.50435936 0.53839833 -0.0021587983 -0.11145708 -0.32809988 -0.39223534 -0.42493358 -0.4767749][1.6629864 2.1642509 2.2030969 2.1517379 1.8257451 1.7573743 1.5677147 0.81365985 0.7153706 0.18719402 0.055119582 -0.19037828 -0.29655823 -0.30316502 -0.33800846][1.3493438 1.8305781 1.9866385 1.8651192 1.6331074 1.8809265 1.863691 1.19986 0.98431319 0.36131743 0.15426272 -0.044715196 -0.12899026 -0.23902547 -0.29160476][0.82413805 1.3062478 1.5984262 1.4536245 1.3401175 1.4246101 1.189868 1.1023878 1.0251313 0.54439676 0.36607423 0.12137976 0.00045843422 -0.062894091 -0.22111619][0.46208134 0.86534953 0.82550365 0.8068794 0.68475217 0.78301418 0.81250811 0.72725058 0.55675757 0.36066365 0.39670148 0.23338035 0.042265855 -0.027831011 -0.090062246][0.15352044 0.48665735 0.48640153 0.23150072 0.1758306 0.28690958 0.40485546 0.51440454 0.49965617 0.23358169 0.34194735 0.21685502 0.11616626 0.0088658631 -0.062469527][-0.043778609 0.16261768 0.11347049 -0.021662772 0.01657474 0.021180503 0.010030337 0.12747261 0.19224757 0.20697689 0.34473056 0.15051678 0.042970546 0.062805407 0.014841162][0.085207231 0.064455427 -0.11831196 -0.15912803 -0.052465569 -0.05211696 -0.05157274 -0.022168227 0.03327617 0.15139404 0.07128007 0.015577316 -0.082357608 -0.047135971 -0.050973244][0.073553763 0.038080595 -0.035060652 -0.14803618 -0.12350925 0.0094358921 0.0035154521 0.15956411 0.016011886 0.083644859 -0.019374721 -0.04164505 -0.019697569 -0.058599152 -0.052303597]]...]
INFO - root - 2017-12-06 05:23:15.144095: step 13810, loss = 0.81, batch loss = 0.60 (34.4 examples/sec; 0.233 sec/batch; 20h:36m:41s remains)
INFO - root - 2017-12-06 05:23:17.452350: step 13820, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.226 sec/batch; 19h:58m:04s remains)
INFO - root - 2017-12-06 05:23:19.819124: step 13830, loss = 0.77, batch loss = 0.55 (34.1 examples/sec; 0.234 sec/batch; 20h:44m:58s remains)
INFO - root - 2017-12-06 05:23:22.179806: step 13840, loss = 0.84, batch loss = 0.62 (33.3 examples/sec; 0.240 sec/batch; 21h:15m:02s remains)
INFO - root - 2017-12-06 05:23:24.513394: step 13850, loss = 0.77, batch loss = 0.56 (33.9 examples/sec; 0.236 sec/batch; 20h:53m:57s remains)
INFO - root - 2017-12-06 05:23:26.810533: step 13860, loss = 0.83, batch loss = 0.62 (33.8 examples/sec; 0.237 sec/batch; 20h:57m:50s remains)
INFO - root - 2017-12-06 05:23:29.117173: step 13870, loss = 0.79, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:13s remains)
INFO - root - 2017-12-06 05:23:31.377211: step 13880, loss = 0.88, batch loss = 0.67 (36.1 examples/sec; 0.221 sec/batch; 19h:35m:28s remains)
INFO - root - 2017-12-06 05:23:33.667288: step 13890, loss = 0.87, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 20h:27m:52s remains)
INFO - root - 2017-12-06 05:23:36.034127: step 13900, loss = 0.75, batch loss = 0.53 (31.3 examples/sec; 0.255 sec/batch; 22h:35m:28s remains)
2017-12-06 05:23:36.359788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0048948303 -0.017957926 -0.037245031 -0.061366897 -0.085309923 -0.10674521 -0.1218946 -0.13029373 -0.13143012 -0.12863569 -0.12303396 -0.11690971 -0.1108847 -0.098349333 -0.091064751][0.0068081915 0.0013924092 -0.0083047077 -0.02117686 -0.035441466 -0.048644714 -0.0602176 -0.069507763 -0.076353326 -0.080712773 -0.083300591 -0.085793689 -0.086107507 -0.0831166 -0.081744991][0.03057649 0.042534143 0.053445131 0.062135369 0.069994837 0.07341449 0.072409272 0.06563665 0.053812951 0.035528667 0.014117762 -0.0068667457 -0.023173653 -0.03567978 -0.047553994][0.039537415 0.043865502 0.046775177 0.063585386 0.082848743 0.10418631 0.1218778 0.13251935 0.13855194 0.14517236 0.14604113 0.14303015 0.13990641 0.12866455 0.097403184][0.13256033 0.14493412 0.15519573 0.16819274 0.18215922 0.19537774 0.20595241 0.2178053 0.22302127 0.22253057 0.21670061 0.21477765 0.21086609 0.21487367 0.21233347][0.19263473 0.23820186 0.27675977 0.32200021 0.36210677 0.3797195 0.39266667 0.390763 0.38754982 0.3861495 0.3803497 0.378287 0.37076986 0.36219516 0.34850881][0.14138773 0.16145372 0.16464058 0.17134589 0.17685363 0.18841645 0.2002345 0.21741754 0.24003372 0.25668678 0.26702121 0.28181142 0.28889412 0.29858565 0.2738426][-0.0043246076 -0.00530231 -0.0085442737 -0.033796359 -0.063148215 -0.079833508 -0.091539837 -0.0829411 -0.062805966 -0.024342611 0.012457453 0.04057841 0.066162571 0.0838754 0.076983124][-0.085802875 -0.090543061 -0.093382 -0.080504753 -0.070253372 -0.077237517 -0.094422922 -0.11244719 -0.12600473 -0.14405045 -0.15439278 -0.14866534 -0.13390586 -0.15240335 -0.15290651][-0.14445293 -0.16346867 -0.18307568 -0.20031989 -0.21908741 -0.23093024 -0.24233265 -0.25701272 -0.2660642 -0.27214241 -0.27524239 -0.27393395 -0.25968263 -0.24871428 -0.24619403][-0.17237437 -0.21067527 -0.24742857 -0.2748152 -0.30300122 -0.31915313 -0.32991192 -0.33008116 -0.33311823 -0.3471249 -0.36171785 -0.37120944 -0.37748268 -0.38746712 -0.38555041][-0.28382087 -0.31049705 -0.32888007 -0.35788608 -0.39131808 -0.4244996 -0.4546847 -0.45997459 -0.45611954 -0.43994746 -0.41666019 -0.39406753 -0.37896547 -0.38400966 -0.36276218][-0.32210976 -0.35688218 -0.37477121 -0.38192305 -0.38672218 -0.4142231 -0.44386628 -0.46329683 -0.48392266 -0.49446023 -0.49599081 -0.4892005 -0.48321226 -0.4635784 -0.44806832][-0.34145755 -0.35637477 -0.35766554 -0.34701011 -0.33398071 -0.31322211 -0.2960369 -0.30434388 -0.317993 -0.33138782 -0.34970278 -0.36699194 -0.37615314 -0.3825959 -0.38959938][-0.29539773 -0.33292222 -0.36234766 -0.36881143 -0.36399731 -0.34521151 -0.32889897 -0.3196044 -0.31217369 -0.31797335 -0.32508 -0.33295435 -0.33721912 -0.34607482 -0.34775725]]...]
INFO - root - 2017-12-06 05:23:38.665221: step 13910, loss = 0.83, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-06 05:23:41.030043: step 13920, loss = 0.85, batch loss = 0.64 (33.8 examples/sec; 0.237 sec/batch; 20h:56m:42s remains)
INFO - root - 2017-12-06 05:23:43.331362: step 13930, loss = 0.76, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:30m:22s remains)
INFO - root - 2017-12-06 05:23:45.633967: step 13940, loss = 0.84, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:52s remains)
INFO - root - 2017-12-06 05:23:47.906959: step 13950, loss = 0.80, batch loss = 0.58 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:38s remains)
INFO - root - 2017-12-06 05:23:50.201117: step 13960, loss = 0.88, batch loss = 0.66 (34.0 examples/sec; 0.235 sec/batch; 20h:48m:57s remains)
INFO - root - 2017-12-06 05:23:52.500934: step 13970, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 20h:17m:09s remains)
INFO - root - 2017-12-06 05:23:54.812000: step 13980, loss = 0.79, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:38s remains)
INFO - root - 2017-12-06 05:23:57.117095: step 13990, loss = 0.73, batch loss = 0.52 (33.3 examples/sec; 0.240 sec/batch; 21h:16m:00s remains)
INFO - root - 2017-12-06 05:23:59.422824: step 14000, loss = 0.72, batch loss = 0.50 (34.9 examples/sec; 0.230 sec/batch; 20h:18m:18s remains)
2017-12-06 05:24:03.656345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018078737 -0.009659946 -0.0010789335 0.0066337213 0.010457978 0.0071022734 -0.0049668625 -0.022663593 -0.047148686 -0.069584422 -0.088460624 -0.095846273 -0.091583155 -0.0774205 -0.061535895][-0.0108243 0.003760539 0.019512005 0.030320115 0.033894576 0.026383005 0.005464986 -0.022295184 -0.065877728 -0.11886746 -0.16499037 -0.19208157 -0.19670588 -0.17134967 -0.13293268][-0.015786186 -0.00044969469 0.016912535 0.031687148 0.040434279 0.04494869 0.049162917 0.045095973 0.028732389 -0.0018948391 -0.065355554 -0.1267228 -0.17067185 -0.18027589 -0.1604982][-0.032436322 -0.022003457 -0.0053293034 0.023739405 0.055275656 0.086895846 0.14551473 0.23626861 0.3049739 0.31958178 0.24532706 0.11819378 -0.015749529 -0.094733328 -0.12173679][-0.1450883 -0.12724327 -0.090331092 -0.034633588 0.043623634 0.14557958 0.29367507 0.4838478 0.6349777 0.69716507 0.610444 0.40515211 0.17993698 0.03280364 -0.058523975][-0.21264978 -0.18038279 -0.11761212 -0.022304714 0.10768721 0.25806686 0.44955486 0.66345286 0.83132362 0.88537371 0.78822768 0.51710123 0.25538307 0.073671363 -0.026444823][0.049476258 0.059990205 0.097515784 0.20493472 0.37261382 0.54229397 0.72321159 0.87935758 0.9567526 0.88091815 0.66179436 0.36153734 0.07938046 -0.075470805 -0.10738033][0.1470809 0.14708352 0.17900142 0.28554732 0.447528 0.61270159 0.78082806 0.87720287 0.88292474 0.7534861 0.49774194 0.18319288 -0.12643327 -0.28631851 -0.29501319][0.043439098 0.032106452 0.052478068 0.13869429 0.27740633 0.42336243 0.55180532 0.61329311 0.57498127 0.40460694 0.19265065 -0.093049459 -0.32759598 -0.44626546 -0.42147347][-0.02522254 -0.050862815 -0.048156265 0.00441394 0.10826255 0.23089254 0.3306061 0.36858353 0.28169554 0.075962089 -0.18586206 -0.41007087 -0.5669831 -0.62973756 -0.55027848][-0.13112442 -0.15652013 -0.16048436 -0.1331515 -0.075548194 0.024817802 0.12265211 0.16025645 0.067244865 -0.11844218 -0.34374946 -0.55340612 -0.6741305 -0.67229319 -0.58015972][-0.21676806 -0.22395763 -0.22353417 -0.19217379 -0.14846425 -0.076864824 -0.019189008 -0.011717655 -0.11724647 -0.2922006 -0.46801937 -0.62718379 -0.6928857 -0.65356827 -0.54815185][-0.32621613 -0.30515242 -0.28360146 -0.22944361 -0.16899158 -0.11401624 -0.10574257 -0.14088652 -0.25172487 -0.38824344 -0.51104116 -0.62211305 -0.65534341 -0.59843814 -0.48275557][-0.33540663 -0.30308959 -0.25642228 -0.20358245 -0.14820065 -0.11693764 -0.13246587 -0.18603438 -0.29597819 -0.40694088 -0.48529035 -0.5397051 -0.50660479 -0.40386739 -0.30952933][-0.23307827 -0.19792317 -0.15754241 -0.12077908 -0.09227255 -0.08308024 -0.10608628 -0.16049083 -0.24940255 -0.31983039 -0.3569662 -0.37364 -0.33412555 -0.23311988 -0.16363639]]...]
INFO - root - 2017-12-06 05:24:06.065592: step 14010, loss = 0.77, batch loss = 0.56 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:42s remains)
INFO - root - 2017-12-06 05:24:08.344777: step 14020, loss = 0.91, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 20h:31m:47s remains)
INFO - root - 2017-12-06 05:24:10.645993: step 14030, loss = 0.94, batch loss = 0.73 (33.9 examples/sec; 0.236 sec/batch; 20h:52m:05s remains)
INFO - root - 2017-12-06 05:24:12.952230: step 14040, loss = 0.76, batch loss = 0.55 (33.8 examples/sec; 0.237 sec/batch; 20h:56m:45s remains)
INFO - root - 2017-12-06 05:24:15.232895: step 14050, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-06 05:24:17.497993: step 14060, loss = 0.82, batch loss = 0.61 (34.2 examples/sec; 0.234 sec/batch; 20h:42m:52s remains)
INFO - root - 2017-12-06 05:24:19.832525: step 14070, loss = 0.76, batch loss = 0.55 (34.3 examples/sec; 0.233 sec/batch; 20h:38m:34s remains)
INFO - root - 2017-12-06 05:24:22.175229: step 14080, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 20h:38m:08s remains)
INFO - root - 2017-12-06 05:24:24.504036: step 14090, loss = 0.79, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:25m:51s remains)
INFO - root - 2017-12-06 05:24:26.781915: step 14100, loss = 0.75, batch loss = 0.54 (36.1 examples/sec; 0.221 sec/batch; 19h:34m:28s remains)
2017-12-06 05:24:27.172887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.69787073 -0.76225686 -0.71935922 -0.65773129 -0.60527623 -0.66518247 -0.68435162 -0.72101307 -0.76907384 -0.86712337 -0.89488685 -0.87959105 -0.85617733 -0.79370189 -0.68701541][-0.96408409 -1.0123444 -0.91730022 -0.89414012 -0.85762095 -0.84904015 -0.84375268 -0.84777158 -0.82583612 -0.77497846 -0.69520533 -0.64402473 -0.59115642 -0.53769249 -0.461526][-1.0890113 -1.1175981 -0.95630789 -0.80380887 -0.66505176 -0.64230162 -0.59634155 -0.55016255 -0.44160786 -0.27890903 -0.10509466 0.027412489 0.069631614 0.0721324 0.056407444][-0.8057934 -0.808189 -0.61481762 -0.37778634 -0.16554821 0.00067891181 0.10127034 0.15789393 0.24566761 0.32018733 0.37370792 0.36216703 0.30883425 0.32157964 0.2853204][-0.19854274 -0.21817315 -0.18541709 -0.12866277 -0.037303358 0.049615018 0.08685609 0.14118502 0.20044875 0.24975556 0.26354727 0.27191049 0.23215413 0.25798136 0.25562873][0.49672276 0.39938009 0.32013014 0.21096048 0.13352433 0.088849224 0.064017363 0.068362646 0.11396616 0.17446104 0.17232791 0.22024935 0.21468368 0.1657047 0.14086822][0.69714558 0.62245613 0.56952196 0.45022726 0.33443204 0.26817542 0.22967464 0.21732983 0.20675 0.17614046 0.14861977 0.16067755 0.15107915 0.12185148 0.069046907][0.59285754 0.5565151 0.53969151 0.39876884 0.27562886 0.12822351 0.052085377 -0.01619795 -0.059846111 -0.049266145 -0.03742 -0.063157625 -0.078530811 -0.0828674 -0.051668078][0.33164278 0.25832871 0.18489113 0.089124762 0.020362653 -0.0456552 -0.085140988 -0.13171317 -0.14263125 -0.16153426 -0.18612659 -0.15898554 -0.13679327 -0.11452037 -0.10292432][0.034662418 -0.072200112 -0.17297429 -0.15578151 -0.15797824 -0.15474316 -0.12653854 -0.11156286 -0.12007483 -0.1183856 -0.10499415 -0.10166139 -0.096055895 -0.083653219 -0.078580186][-0.1265066 -0.12387972 -0.11205401 -0.12913136 -0.12535636 -0.077852756 -0.057064127 -0.067224316 -0.085893266 -0.096017145 -0.10574096 -0.097793259 -0.086211413 -0.06880416 -0.055526126][-0.1610254 -0.10867983 -0.081045128 -0.061221905 -0.043893754 -0.052392542 -0.072275616 -0.0812326 -0.084917009 -0.086384967 -0.088163264 -0.092032261 -0.094738647 -0.08791659 -0.078502432][-0.10815427 -0.08332438 -0.076912284 -0.074394256 -0.080100976 -0.080698006 -0.083734356 -0.085817009 -0.087450184 -0.088662468 -0.089369915 -0.088453762 -0.086839214 -0.093149446 -0.099520877][-0.088953964 -0.079507761 -0.084436625 -0.090926126 -0.088228583 -0.084274538 -0.08506801 -0.0847428 -0.09102352 -0.090153821 -0.089111514 -0.08923018 -0.089134276 -0.086752951 -0.084242247][-0.091829769 -0.084414974 -0.083281487 -0.085969083 -0.084262744 -0.083488196 -0.08439482 -0.089940608 -0.08982607 -0.089310221 -0.090509012 -0.089908943 -0.090356447 -0.090099283 -0.089614131]]...]
INFO - root - 2017-12-06 05:24:29.434592: step 14110, loss = 0.76, batch loss = 0.54 (36.4 examples/sec; 0.220 sec/batch; 19h:26m:16s remains)
INFO - root - 2017-12-06 05:24:31.718634: step 14120, loss = 0.96, batch loss = 0.75 (35.1 examples/sec; 0.228 sec/batch; 20h:08m:17s remains)
INFO - root - 2017-12-06 05:24:34.033620: step 14130, loss = 0.86, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 20h:41m:30s remains)
INFO - root - 2017-12-06 05:24:36.314869: step 14140, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 20h:11m:15s remains)
INFO - root - 2017-12-06 05:24:38.565718: step 14150, loss = 0.90, batch loss = 0.69 (34.3 examples/sec; 0.233 sec/batch; 20h:37m:29s remains)
INFO - root - 2017-12-06 05:24:40.845704: step 14160, loss = 0.74, batch loss = 0.53 (34.7 examples/sec; 0.231 sec/batch; 20h:23m:39s remains)
INFO - root - 2017-12-06 05:24:43.128844: step 14170, loss = 0.78, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:29m:54s remains)
INFO - root - 2017-12-06 05:24:45.410113: step 14180, loss = 0.79, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 20h:23m:54s remains)
INFO - root - 2017-12-06 05:24:47.680788: step 14190, loss = 0.86, batch loss = 0.65 (37.1 examples/sec; 0.215 sec/batch; 19h:02m:50s remains)
INFO - root - 2017-12-06 05:24:50.018339: step 14200, loss = 0.82, batch loss = 0.61 (34.3 examples/sec; 0.233 sec/batch; 20h:36m:32s remains)
2017-12-06 05:24:56.204398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07920628 -0.079595424 -0.080557995 -0.081722423 -0.082572758 -0.082616575 -0.082153715 -0.081548519 -0.080849335 -0.080018528 -0.079106681 -0.078622356 -0.078453153 -0.078484446 -0.078651361][-0.079372816 -0.080036141 -0.081494287 -0.083722986 -0.086105682 -0.087814674 -0.088428773 -0.087983586 -0.086942695 -0.085378341 -0.08316838 -0.0809212 -0.079490505 -0.078915648 -0.0789157][-0.080133438 -0.081121445 -0.082468852 -0.085681766 -0.090155505 -0.095525719 -0.099569872 -0.10025513 -0.098940767 -0.096420757 -0.092604 -0.0872207 -0.082583718 -0.0802007 -0.079442538][-0.080927879 -0.082306877 -0.082093373 -0.08439628 -0.08779715 -0.094599269 -0.10204564 -0.10531819 -0.10547694 -0.10516044 -0.10379874 -0.0972593 -0.0884358 -0.082672812 -0.08002416][-0.081016749 -0.083163552 -0.079581529 -0.076877549 -0.071533859 -0.072129905 -0.078328863 -0.08458712 -0.088928156 -0.097234659 -0.10750577 -0.10726406 -0.096571609 -0.086901769 -0.081644811][-0.081465423 -0.086032152 -0.081491672 -0.0738838 -0.058044523 -0.048805621 -0.048888668 -0.053890396 -0.057420976 -0.07117784 -0.095072038 -0.10907872 -0.10185039 -0.090527676 -0.084545657][-0.082069755 -0.0893273 -0.088032745 -0.081548892 -0.066167377 -0.056526836 -0.050920069 -0.045891818 -0.037105411 -0.040123053 -0.066197515 -0.094816476 -0.10028657 -0.091748774 -0.086722463][-0.081886552 -0.089304551 -0.089606568 -0.084040053 -0.07600452 -0.076230533 -0.072465487 -0.060606737 -0.042659152 -0.031024683 -0.040679093 -0.068321586 -0.088825688 -0.088619679 -0.085573494][-0.081442378 -0.087410271 -0.086397432 -0.077258095 -0.070054218 -0.076407388 -0.07897985 -0.072402589 -0.063537054 -0.054764293 -0.049635902 -0.059568696 -0.079665609 -0.083985806 -0.08279521][-0.080673479 -0.086094595 -0.086868063 -0.077532217 -0.067101508 -0.070232496 -0.074356288 -0.074297592 -0.075491205 -0.076253563 -0.07170973 -0.068466246 -0.078424528 -0.080048673 -0.079259783][-0.079733327 -0.084568523 -0.088748515 -0.084137514 -0.073053658 -0.068882436 -0.068223983 -0.06919983 -0.0720588 -0.074926183 -0.074289411 -0.071099684 -0.078819558 -0.080919221 -0.080336384][-0.07878723 -0.081999578 -0.087342724 -0.088756613 -0.083515331 -0.078158848 -0.072750986 -0.070006 -0.0701015 -0.068747528 -0.067427032 -0.066246726 -0.073554859 -0.07628274 -0.078825161][-0.077813536 -0.07902205 -0.082463361 -0.085726529 -0.08569251 -0.082301758 -0.07681109 -0.073513329 -0.072667792 -0.072127581 -0.069673978 -0.067128904 -0.0695701 -0.07232669 -0.076996692][-0.077524059 -0.0776729 -0.07894586 -0.081311651 -0.083648704 -0.0827401 -0.079645872 -0.078316309 -0.079073213 -0.08042749 -0.077820726 -0.073541626 -0.071156815 -0.070987366 -0.07303679][-0.07785093 -0.077669278 -0.077655181 -0.0783229 -0.080469117 -0.081433617 -0.080884047 -0.08070121 -0.08145678 -0.082885861 -0.083149761 -0.080611713 -0.077782638 -0.075745836 -0.074008584]]...]
INFO - root - 2017-12-06 05:24:58.543132: step 14210, loss = 0.87, batch loss = 0.66 (37.4 examples/sec; 0.214 sec/batch; 18h:55m:11s remains)
INFO - root - 2017-12-06 05:25:00.846701: step 14220, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 19h:57m:49s remains)
INFO - root - 2017-12-06 05:25:03.117259: step 14230, loss = 0.89, batch loss = 0.68 (35.4 examples/sec; 0.226 sec/batch; 19h:57m:36s remains)
INFO - root - 2017-12-06 05:25:05.433016: step 14240, loss = 0.83, batch loss = 0.62 (35.2 examples/sec; 0.228 sec/batch; 20h:06m:52s remains)
INFO - root - 2017-12-06 05:25:07.671606: step 14250, loss = 0.77, batch loss = 0.55 (37.1 examples/sec; 0.216 sec/batch; 19h:04m:26s remains)
INFO - root - 2017-12-06 05:25:10.021808: step 14260, loss = 0.82, batch loss = 0.61 (34.4 examples/sec; 0.233 sec/batch; 20h:33m:40s remains)
INFO - root - 2017-12-06 05:25:12.337787: step 14270, loss = 0.80, batch loss = 0.58 (34.1 examples/sec; 0.234 sec/batch; 20h:43m:10s remains)
INFO - root - 2017-12-06 05:25:14.652358: step 14280, loss = 0.78, batch loss = 0.57 (36.7 examples/sec; 0.218 sec/batch; 19h:15m:10s remains)
INFO - root - 2017-12-06 05:25:16.954506: step 14290, loss = 0.74, batch loss = 0.52 (34.7 examples/sec; 0.231 sec/batch; 20h:22m:46s remains)
INFO - root - 2017-12-06 05:25:19.215650: step 14300, loss = 0.77, batch loss = 0.55 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:40s remains)
2017-12-06 05:25:19.632467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066856727 -0.066748239 -0.066580638 -0.065977208 -0.065091521 -0.064107478 -0.063251391 -0.062533706 -0.062117778 -0.061923165 -0.0619966 -0.062182322 -0.062486887 -0.0629151 -0.063436858][-0.065166652 -0.0649272 -0.064758621 -0.064332679 -0.063751467 -0.06312643 -0.062612407 -0.062205702 -0.062001649 -0.061944194 -0.062059291 -0.062225785 -0.062434979 -0.062761538 -0.0632153][-0.064239964 -0.063837163 -0.063577764 -0.063154608 -0.06270647 -0.0622757 -0.061987996 -0.061814927 -0.061842766 -0.061970472 -0.062181983 -0.062406272 -0.062659666 -0.063094065 -0.063754968][-0.063690007 -0.063154265 -0.062810086 -0.062364593 -0.061961539 -0.061628845 -0.06147325 -0.061467566 -0.061643243 -0.06190145 -0.06223077 -0.062566847 -0.062988296 -0.063730389 -0.065017715][-0.063428968 -0.062720224 -0.062287651 -0.061826274 -0.061450969 -0.061171897 -0.061083205 -0.0611677 -0.061410561 -0.061702885 -0.062117666 -0.0626031 -0.063320845 -0.064613357 -0.067004368][-0.063503593 -0.062632695 -0.062137313 -0.061654784 -0.061290003 -0.061018884 -0.060903005 -0.060984798 -0.06122563 -0.061477609 -0.061847597 -0.062473074 -0.063471153 -0.065482982 -0.069374047][-0.063698359 -0.06278611 -0.06234321 -0.061943129 -0.061650954 -0.061425377 -0.061280832 -0.061286703 -0.0614379 -0.061575346 -0.061847739 -0.062443949 -0.063669242 -0.066427782 -0.071759194][-0.063909337 -0.063028 -0.062728062 -0.062499307 -0.062336244 -0.062202372 -0.062086776 -0.062031843 -0.062022083 -0.061978366 -0.062147636 -0.062605545 -0.063924015 -0.067206427 -0.073584951][-0.064026266 -0.06326367 -0.063132361 -0.063080795 -0.063056871 -0.063009769 -0.062919512 -0.062819272 -0.0626778 -0.062452946 -0.062474757 -0.062793814 -0.064148046 -0.067586593 -0.074514762][-0.064019531 -0.063388549 -0.063400432 -0.063483387 -0.063565373 -0.063597 -0.063530385 -0.063405544 -0.06320639 -0.062895454 -0.062810332 -0.062961288 -0.064220026 -0.0676171 -0.074389987][-0.063979954 -0.063412964 -0.063514829 -0.063677169 -0.063835979 -0.063943341 -0.063942693 -0.063836075 -0.06362354 -0.063312925 -0.063185588 -0.0631734 -0.064254664 -0.0674652 -0.073652364][-0.064013258 -0.063435562 -0.063545093 -0.063725673 -0.063912123 -0.064073272 -0.064149871 -0.064115822 -0.063961364 -0.063711695 -0.0637386 -0.063818164 -0.065073207 -0.068224937 -0.074050836][-0.064085215 -0.063434936 -0.063451745 -0.063551918 -0.063682973 -0.06383279 -0.063950986 -0.0640087 -0.063987382 -0.063947722 -0.064165995 -0.064726293 -0.066595554 -0.070192531 -0.075822338][-0.064217143 -0.063446 -0.063301161 -0.063217148 -0.063170567 -0.06318447 -0.063241735 -0.063353665 -0.063486971 -0.0638049 -0.064514607 -0.065827668 -0.068790026 -0.073438987 -0.079645723][-0.064585857 -0.063660815 -0.063295208 -0.062908724 -0.06253168 -0.06228447 -0.062138192 -0.062187746 -0.062419564 -0.063130774 -0.064454988 -0.066819362 -0.070992023 -0.076945916 -0.084172621]]...]
INFO - root - 2017-12-06 05:25:21.932433: step 14310, loss = 0.79, batch loss = 0.58 (36.3 examples/sec; 0.220 sec/batch; 19h:28m:50s remains)
INFO - root - 2017-12-06 05:25:24.199383: step 14320, loss = 0.74, batch loss = 0.52 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:31s remains)
INFO - root - 2017-12-06 05:25:26.477891: step 14330, loss = 0.74, batch loss = 0.52 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:58s remains)
INFO - root - 2017-12-06 05:25:28.775603: step 14340, loss = 0.79, batch loss = 0.58 (33.4 examples/sec; 0.240 sec/batch; 21h:10m:45s remains)
INFO - root - 2017-12-06 05:25:31.094806: step 14350, loss = 0.79, batch loss = 0.57 (36.3 examples/sec; 0.221 sec/batch; 19h:30m:04s remains)
INFO - root - 2017-12-06 05:25:33.388892: step 14360, loss = 0.80, batch loss = 0.59 (35.6 examples/sec; 0.224 sec/batch; 19h:50m:12s remains)
INFO - root - 2017-12-06 05:25:35.711935: step 14370, loss = 0.71, batch loss = 0.50 (35.2 examples/sec; 0.227 sec/batch; 20h:05m:42s remains)
INFO - root - 2017-12-06 05:25:37.982840: step 14380, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.224 sec/batch; 19h:49m:47s remains)
INFO - root - 2017-12-06 05:25:40.287257: step 14390, loss = 0.77, batch loss = 0.56 (33.6 examples/sec; 0.238 sec/batch; 21h:00m:33s remains)
INFO - root - 2017-12-06 05:25:42.539936: step 14400, loss = 0.80, batch loss = 0.58 (35.0 examples/sec; 0.228 sec/batch; 20h:10m:41s remains)
2017-12-06 05:25:45.341833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19453496 -0.23403835 -0.29060963 -0.36341241 -0.45489529 -0.55243057 -0.6269297 -0.66210806 -0.649843 -0.52440721 -0.40407267 -0.27125689 -0.18032251 -0.1614276 -0.1421611][-0.31730521 -0.39060566 -0.42842862 -0.44685742 -0.52046961 -0.66002208 -0.81289548 -0.91916573 -0.945002 -0.78044772 -0.58975774 -0.36025739 -0.20851253 -0.14470199 -0.10948056][-0.43707567 -0.50486511 -0.48928189 -0.39750662 -0.3480089 -0.42243221 -0.63891709 -0.91814142 -1.0730227 -0.97161 -0.79632807 -0.53486156 -0.30668229 -0.15419719 -0.095377177][-0.59432322 -0.64756972 -0.49198055 -0.12824234 0.13552961 0.15415558 -0.075204737 -0.50320655 -0.90060651 -1.0430368 -0.97747195 -0.73800904 -0.45083329 -0.21328832 -0.10992409][-0.70936829 -0.76732057 -0.51417005 0.12025062 0.77808446 1.1869709 1.082477 0.44107398 -0.26065627 -0.67143029 -0.83199972 -0.77385503 -0.53499466 -0.32624593 -0.17855197][-0.63769668 -0.64420325 -0.22883219 0.72688329 1.7506416 2.4028435 2.5826941 2.1163776 1.1965805 0.35465345 -0.19031037 -0.45500788 -0.4479917 -0.33278367 -0.18848363][-0.60604036 -0.47287881 0.08843217 1.174853 2.5077403 3.5785015 3.997683 3.5604255 2.5856662 1.5679948 0.752594 0.32810849 0.078328647 -0.11006802 -0.10537424][-0.58014089 -0.50042796 0.022534914 1.0688943 2.4086034 3.6583483 4.3773603 4.2569275 3.3971608 2.2895217 1.3950084 0.762578 0.44430009 0.21121216 0.093037389][-0.65258127 -0.63001716 -0.23204049 0.62599134 1.8716773 3.1738746 4.0753641 4.2314334 3.5790756 2.495374 1.4781181 0.80271447 0.44776526 0.21504956 0.15702891][-0.86162549 -1.0223402 -0.95494932 -0.42748895 0.51640344 1.5999961 2.5079434 2.8997843 2.7142766 1.9811813 1.1704403 0.55603957 0.27060351 0.1029538 0.08766263][-0.95798326 -1.3326139 -1.5492531 -1.4483459 -0.90719175 -0.047793932 0.8197667 1.2570505 1.2511431 0.91753286 0.43382409 0.11340963 0.022147924 0.0073928759 -0.029891234][-0.80904073 -1.1767304 -1.5250685 -1.6703329 -1.5138233 -1.0807616 -0.45039192 0.005738996 0.22846439 0.18632105 -0.057761431 -0.18518904 -0.21223408 -0.14224592 -0.045296546][-0.58537549 -0.86757272 -1.1723046 -1.4234291 -1.576718 -1.4612129 -1.138657 -0.83550078 -0.45380321 -0.2644766 -0.27043891 -0.26241627 -0.23848385 -0.16197535 -0.10239231][-0.39478362 -0.55497319 -0.74533504 -0.94494164 -1.1649767 -1.316714 -1.2392236 -1.136534 -0.80446804 -0.56182343 -0.37440646 -0.19986238 -0.15238437 -0.13429227 -0.11513346][-0.21143629 -0.3034116 -0.39836356 -0.49854338 -0.69582075 -0.79516476 -0.84383637 -0.83687264 -0.57537603 -0.43180653 -0.28655821 -0.13199978 -0.10090131 -0.092992216 -0.098912187]]...]
INFO - root - 2017-12-06 05:25:47.609522: step 14410, loss = 0.80, batch loss = 0.59 (34.1 examples/sec; 0.235 sec/batch; 20h:45m:01s remains)
INFO - root - 2017-12-06 05:25:49.895891: step 14420, loss = 0.82, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 19h:49m:20s remains)
INFO - root - 2017-12-06 05:25:52.226055: step 14430, loss = 0.75, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:13s remains)
INFO - root - 2017-12-06 05:25:54.507849: step 14440, loss = 0.93, batch loss = 0.72 (35.8 examples/sec; 0.223 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-06 05:25:56.766700: step 14450, loss = 0.72, batch loss = 0.51 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:35s remains)
INFO - root - 2017-12-06 05:25:59.095851: step 14460, loss = 0.74, batch loss = 0.53 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:28s remains)
INFO - root - 2017-12-06 05:26:01.422045: step 14470, loss = 0.87, batch loss = 0.65 (35.3 examples/sec; 0.226 sec/batch; 20h:00m:06s remains)
INFO - root - 2017-12-06 05:26:03.719447: step 14480, loss = 0.78, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:19s remains)
INFO - root - 2017-12-06 05:26:06.006644: step 14490, loss = 0.86, batch loss = 0.65 (33.6 examples/sec; 0.238 sec/batch; 21h:00m:29s remains)
INFO - root - 2017-12-06 05:26:08.281717: step 14500, loss = 0.79, batch loss = 0.58 (36.3 examples/sec; 0.220 sec/batch; 19h:27m:46s remains)
2017-12-06 05:26:13.559368: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.5670952 1.4946654 1.4954942 1.4966483 1.5366726 1.5080161 1.2484239 0.52608758 -0.0015830398 -0.82686883 -1.3638107 -1.8530618 -1.8246979 -1.5153491 -1.2041432][1.0766492 1.0753834 1.1636556 1.2322026 1.2099713 1.3022994 1.2324787 0.72742862 0.026502974 -0.81783938 -1.755142 -2.6357818 -3.2368672 -3.0012815 -2.7051876][0.42309731 0.27998286 0.35051253 0.68251258 0.73157358 0.88626426 0.88490522 0.58468896 0.10947184 -0.53829074 -1.4760159 -2.6335993 -3.1751833 -3.5539792 -3.4941173][-0.22664046 -0.47292489 -0.53675836 -0.33975694 -0.11372925 0.27597451 0.57966703 0.60559958 0.43821317 0.11986352 -0.44602016 -1.501021 -2.4683785 -2.7249575 -2.745296][-0.33775374 -0.466447 -0.51896942 -0.35115585 -0.17802525 0.1310021 0.65555286 1.0334173 1.2326459 1.1265064 0.8037867 0.17246595 -0.56991088 -1.3395679 -1.3106046][-0.21217552 -0.23837721 -0.13030103 0.12312091 0.3372103 0.577702 1.010188 1.3322872 1.6709185 1.9946331 1.7994956 1.3860348 0.84053594 0.37730238 0.10767626][0.053285457 -0.034751561 0.053604119 0.30322969 0.59089625 1.0531723 1.5216093 1.8204296 2.2267501 2.488971 2.642508 2.6199558 1.8513502 1.4176062 1.1376585][-0.065574653 -0.021621801 0.13857654 0.15282357 0.3291246 0.68358779 1.1369002 1.5341278 1.9635056 2.2797885 2.4169781 2.4568443 1.9765886 1.794276 1.3571784][-0.42280096 -0.32547984 -0.28996947 -0.1258263 0.040662237 0.22599623 0.67216247 1.0533462 1.1493211 1.5219871 1.6675795 1.619848 1.085287 0.79983854 0.39773208][-0.60294545 -0.54877651 -0.55144691 -0.29444265 -0.17317969 -0.13882907 0.23422939 0.53548461 0.67577934 0.78528911 0.67002577 0.78363013 0.60467839 0.34570864 -0.15846537][-0.350501 -0.47498176 -0.6275304 -0.578638 -0.51343989 -0.43707108 -0.37829646 -0.23018256 -0.09280166 0.26015112 0.38501576 0.3331 -0.11191133 -0.39273927 -0.59563917][-0.36475062 -0.30866319 -0.41807023 -0.51003587 -0.43844607 -0.42508712 -0.41664779 -0.37689334 -0.31285003 -0.2288627 -0.033001333 0.24336085 -0.034225944 -0.57307869 -0.84983039][-0.21973023 -0.1614953 -0.19345723 -0.33792582 -0.42077163 -0.5847308 -0.63397431 -0.46187112 -0.26814055 -0.13302039 -0.053242095 0.24826127 0.16695863 -0.079661787 -0.49408737][-0.16437125 -0.16535258 -0.20307726 -0.23534057 -0.31083542 -0.42214236 -0.60800683 -0.63830429 -0.44758785 -0.21116418 -0.013644628 0.30724758 0.34765145 0.046783231 -0.25456762][-0.16939279 -0.15734075 -0.21162271 -0.31977493 -0.27583224 -0.32294047 -0.41955087 -0.50758129 -0.42756119 -0.29317945 -0.052035674 0.23448557 0.26900995 0.20030263 -0.012394004]]...]
INFO - root - 2017-12-06 05:26:15.943305: step 14510, loss = 0.77, batch loss = 0.55 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:47s remains)
INFO - root - 2017-12-06 05:26:18.200931: step 14520, loss = 0.76, batch loss = 0.55 (36.3 examples/sec; 0.221 sec/batch; 19h:28m:38s remains)
INFO - root - 2017-12-06 05:26:20.487642: step 14530, loss = 0.80, batch loss = 0.59 (32.1 examples/sec; 0.249 sec/batch; 21h:59m:26s remains)
INFO - root - 2017-12-06 05:26:22.811966: step 14540, loss = 0.85, batch loss = 0.64 (36.5 examples/sec; 0.219 sec/batch; 19h:20m:50s remains)
INFO - root - 2017-12-06 05:26:25.097452: step 14550, loss = 0.80, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 19h:58m:20s remains)
INFO - root - 2017-12-06 05:26:27.373761: step 14560, loss = 0.85, batch loss = 0.64 (36.3 examples/sec; 0.221 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-06 05:26:29.635051: step 14570, loss = 0.73, batch loss = 0.52 (33.1 examples/sec; 0.242 sec/batch; 21h:21m:44s remains)
INFO - root - 2017-12-06 05:26:31.935284: step 14580, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:54m:31s remains)
INFO - root - 2017-12-06 05:26:34.233533: step 14590, loss = 0.86, batch loss = 0.64 (36.4 examples/sec; 0.220 sec/batch; 19h:24m:34s remains)
INFO - root - 2017-12-06 05:26:36.525276: step 14600, loss = 0.90, batch loss = 0.68 (34.3 examples/sec; 0.233 sec/batch; 20h:36m:45s remains)
2017-12-06 05:26:36.937873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08501339 -0.088740073 -0.09589646 -0.10606128 -0.11910199 -0.12977211 -0.13509893 -0.13422582 -0.1263012 -0.11222177 -0.097795054 -0.088404648 -0.0845906 -0.083804794 -0.08406762][-0.091586687 -0.10506273 -0.12288034 -0.14267959 -0.16639765 -0.19292104 -0.21787289 -0.2317577 -0.22538924 -0.19505757 -0.15118103 -0.11477415 -0.0943288 -0.086573124 -0.084910125][-0.095302716 -0.10499334 -0.1100637 -0.11637729 -0.12900269 -0.16607158 -0.23448846 -0.30417702 -0.34157553 -0.32478487 -0.26029021 -0.18837112 -0.13508669 -0.1021704 -0.089439467][-0.091010079 -0.085958645 -0.056474186 -0.017208926 0.021396942 0.012861907 -0.071544833 -0.21152011 -0.34497049 -0.40447393 -0.371105 -0.2880317 -0.20131305 -0.13440122 -0.10256045][-0.0770228 -0.05496959 0.0074383169 0.10062272 0.21306539 0.27144605 0.21167874 0.033460163 -0.18918397 -0.34183112 -0.37987483 -0.33885852 -0.25507593 -0.17069107 -0.12070088][-0.056841131 -0.018457599 0.0780799 0.23138896 0.4166874 0.55164635 0.56309152 0.39429539 0.11036708 -0.13865352 -0.27194911 -0.302831 -0.25507906 -0.17640701 -0.11921166][-0.050200932 -0.0090290606 0.099096783 0.28469288 0.51670992 0.71007395 0.78096789 0.65590221 0.37988257 0.098639809 -0.096474759 -0.20368217 -0.21241698 -0.16332576 -0.11489525][-0.057062026 -0.02761279 0.057603709 0.21906522 0.45601544 0.68418878 0.79603589 0.72364831 0.48863694 0.22579768 0.0099171177 -0.13872792 -0.17908081 -0.14953654 -0.10543312][-0.074887767 -0.061841264 -0.010273583 0.10434397 0.28579429 0.49478182 0.61898118 0.59107852 0.43125764 0.23483562 0.048178591 -0.1067743 -0.16666675 -0.14815623 -0.10580058][-0.10027489 -0.11038633 -0.09709236 -0.041554905 0.065642469 0.22413975 0.31968236 0.3200545 0.25320292 0.14372763 0.018837839 -0.097498029 -0.15020497 -0.14047143 -0.10163289][-0.11544373 -0.15182284 -0.17964488 -0.17247033 -0.1248914 -0.020634063 0.047288425 0.059024237 0.044413723 0.0028776973 -0.058712564 -0.12844323 -0.16061582 -0.14100303 -0.1009556][-0.11278678 -0.15874566 -0.21683118 -0.2628434 -0.26650447 -0.20761511 -0.15617529 -0.13705757 -0.11281221 -0.11017256 -0.12436435 -0.15500823 -0.15679291 -0.13071546 -0.098549508][-0.10062763 -0.1372384 -0.19949356 -0.26400432 -0.30519474 -0.30102506 -0.28305304 -0.25922206 -0.21870086 -0.19429603 -0.16527188 -0.15654868 -0.13554756 -0.11276955 -0.094235331][-0.090029478 -0.10990137 -0.15675665 -0.212084 -0.26062 -0.27744025 -0.28675443 -0.28416592 -0.24841243 -0.21489835 -0.17288598 -0.1493724 -0.12402889 -0.10351151 -0.090702347][-0.085216917 -0.09232457 -0.11641631 -0.15187658 -0.1944688 -0.21393615 -0.22411194 -0.22557831 -0.20794378 -0.18517265 -0.14493939 -0.11805254 -0.10204849 -0.094950907 -0.088703632]]...]
INFO - root - 2017-12-06 05:26:39.251666: step 14610, loss = 0.77, batch loss = 0.55 (33.5 examples/sec; 0.239 sec/batch; 21h:03m:51s remains)
INFO - root - 2017-12-06 05:26:41.513279: step 14620, loss = 0.77, batch loss = 0.56 (35.9 examples/sec; 0.223 sec/batch; 19h:41m:45s remains)
INFO - root - 2017-12-06 05:26:43.834381: step 14630, loss = 0.91, batch loss = 0.69 (33.9 examples/sec; 0.236 sec/batch; 20h:51m:29s remains)
INFO - root - 2017-12-06 05:26:46.152728: step 14640, loss = 0.80, batch loss = 0.59 (34.5 examples/sec; 0.232 sec/batch; 20h:28m:11s remains)
INFO - root - 2017-12-06 05:26:48.424573: step 14650, loss = 0.85, batch loss = 0.64 (33.7 examples/sec; 0.238 sec/batch; 20h:58m:11s remains)
INFO - root - 2017-12-06 05:26:50.728444: step 14660, loss = 0.92, batch loss = 0.71 (34.5 examples/sec; 0.232 sec/batch; 20h:29m:33s remains)
INFO - root - 2017-12-06 05:26:53.055927: step 14670, loss = 0.78, batch loss = 0.56 (35.6 examples/sec; 0.225 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-06 05:26:55.368497: step 14680, loss = 0.81, batch loss = 0.60 (35.0 examples/sec; 0.229 sec/batch; 20h:11m:58s remains)
INFO - root - 2017-12-06 05:26:57.679991: step 14690, loss = 0.79, batch loss = 0.58 (33.2 examples/sec; 0.241 sec/batch; 21h:17m:03s remains)
INFO - root - 2017-12-06 05:26:59.983337: step 14700, loss = 0.83, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 20h:39m:14s remains)
2017-12-06 05:27:02.069915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.44037524 -0.4775711 -0.64075685 -0.93891913 -1.301833 -1.641616 -1.9951693 -2.2191529 -2.2699051 -2.2039697 -2.0176578 -1.8047816 -1.5584743 -1.3353983 -1.2192916][-1.1073442 -1.2483929 -1.5260096 -1.9211367 -2.4100816 -2.8323331 -3.1959746 -3.3841822 -3.5165989 -3.4944205 -3.3513644 -3.1070588 -2.7716658 -2.4047582 -2.0732994][-1.8525176 -2.0588255 -2.2553663 -2.4924102 -2.9038327 -3.3264017 -3.762794 -4.0658937 -4.3211117 -4.3670144 -4.3365135 -4.0715609 -3.6287031 -3.1163929 -2.6863856][-1.9251434 -2.187185 -2.3061984 -2.3212683 -2.3965349 -2.59442 -2.9147193 -3.2726154 -3.6540279 -3.997854 -4.3851595 -4.3854995 -4.0783725 -3.4785714 -2.8421671][-1.4885932 -1.52285 -1.3577924 -1.0638907 -0.7344318 -0.5306828 -0.58606714 -0.9190203 -1.3962922 -2.0640774 -2.7001607 -3.2315767 -3.6277587 -3.3505976 -2.8322461][-1.143284 -1.0017756 -0.53448075 0.25885749 1.1135989 1.7607282 2.170969 2.0536683 1.6380383 0.80879855 -0.16455403 -1.0356821 -1.4623052 -1.8146816 -2.0946872][-0.92233139 -0.71836364 -0.16028982 0.75969273 1.9318993 2.9974391 3.7559 3.9507258 3.7593236 3.0454803 2.2110443 1.1804522 0.36731118 -0.075060688 -0.32077059][-0.72764671 -0.56629032 -0.073472552 0.63225341 1.5942339 2.6069133 3.4502318 3.8435745 3.8880868 3.4514587 2.6996183 1.8742025 1.3072026 0.577114 0.13877648][-0.82253528 -0.79933953 -0.46065843 0.044218488 0.75748366 1.5059893 2.2126172 2.7167456 2.7230246 2.3572812 1.8365861 1.2125025 0.73931587 0.40298498 0.16400611][-0.9440757 -1.0849372 -0.95399648 -0.81941175 -0.56908447 -0.17286679 0.26254407 0.76054353 1.0127776 0.883595 0.40160596 -0.15301016 -0.39370438 -0.54759353 -0.5144664][-0.55830288 -1.0135492 -1.1824019 -1.3254503 -1.3774465 -1.3794988 -1.190269 -0.94327682 -0.69966114 -0.4669565 -0.5740937 -0.82027066 -1.2098478 -1.126623 -1.073298][0.021836713 -0.49374402 -0.92011726 -1.3319756 -1.6269948 -1.8023822 -1.7775559 -1.6325388 -1.5786529 -1.4386951 -1.3534409 -1.2673436 -1.2631018 -1.1433185 -1.2057589][0.53325665 -0.0052677765 -0.49954608 -1.010614 -1.5251555 -1.9867091 -2.2494204 -2.0878024 -1.8239342 -1.6364847 -1.6530643 -1.4521734 -1.4090525 -1.1212595 -1.0752356][0.72593713 0.21690735 -0.34246978 -0.79526359 -1.2821306 -1.7547969 -2.1948225 -2.4065869 -2.4802637 -2.2712994 -1.8373579 -1.7776452 -1.7152861 -1.3529587 -1.1430525][0.31928435 -0.026771106 -0.49872527 -0.90148354 -1.3307672 -1.756845 -2.1918147 -2.3927104 -2.3348572 -2.3749366 -2.2486448 -1.9795667 -1.5997598 -1.2189716 -1.1003463]]...]
INFO - root - 2017-12-06 05:27:04.375959: step 14710, loss = 0.91, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 20h:29m:53s remains)
INFO - root - 2017-12-06 05:27:06.645086: step 14720, loss = 0.70, batch loss = 0.48 (35.6 examples/sec; 0.225 sec/batch; 19h:49m:38s remains)
INFO - root - 2017-12-06 05:27:09.005184: step 14730, loss = 0.83, batch loss = 0.62 (33.6 examples/sec; 0.238 sec/batch; 21h:01m:05s remains)
INFO - root - 2017-12-06 05:27:11.282051: step 14740, loss = 0.75, batch loss = 0.54 (35.5 examples/sec; 0.226 sec/batch; 19h:54m:15s remains)
INFO - root - 2017-12-06 05:27:13.552495: step 14750, loss = 0.75, batch loss = 0.53 (34.6 examples/sec; 0.231 sec/batch; 20h:23m:49s remains)
INFO - root - 2017-12-06 05:27:15.904201: step 14760, loss = 0.75, batch loss = 0.53 (34.1 examples/sec; 0.235 sec/batch; 20h:42m:47s remains)
INFO - root - 2017-12-06 05:27:18.227968: step 14770, loss = 0.75, batch loss = 0.53 (36.1 examples/sec; 0.222 sec/batch; 19h:34m:23s remains)
INFO - root - 2017-12-06 05:27:20.553108: step 14780, loss = 0.81, batch loss = 0.60 (32.1 examples/sec; 0.249 sec/batch; 21h:59m:14s remains)
INFO - root - 2017-12-06 05:27:22.845635: step 14790, loss = 0.81, batch loss = 0.59 (36.4 examples/sec; 0.220 sec/batch; 19h:25m:08s remains)
INFO - root - 2017-12-06 05:27:25.159683: step 14800, loss = 0.83, batch loss = 0.62 (33.3 examples/sec; 0.240 sec/batch; 21h:10m:26s remains)
2017-12-06 05:27:25.555326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.084453076 -0.081482716 -0.076861374 -0.072760835 -0.069366165 -0.067605317 -0.066621661 -0.066208176 -0.066074818 -0.066284068 -0.066490516 -0.066446856 -0.066476889 -0.0666579 -0.067346454][-0.083917983 -0.081726357 -0.0775286 -0.07343699 -0.069564179 -0.067912444 -0.066942826 -0.066448577 -0.066268019 -0.066538274 -0.067258909 -0.0677433 -0.06784533 -0.067764163 -0.06768924][-0.085660882 -0.083618596 -0.079416029 -0.074443288 -0.070196711 -0.068192132 -0.067924067 -0.0680863 -0.068231776 -0.068952352 -0.068942323 -0.068596929 -0.068290785 -0.068139419 -0.068737835][-0.086578839 -0.083963558 -0.080724411 -0.076577544 -0.072906069 -0.0714525 -0.071003683 -0.071086578 -0.071482755 -0.071507268 -0.07067886 -0.069491372 -0.068312235 -0.068080485 -0.0685144][-0.08000081 -0.081017517 -0.081263073 -0.079665959 -0.078944787 -0.080187358 -0.081639595 -0.082406759 -0.082088523 -0.080732144 -0.078025214 -0.075074434 -0.07207723 -0.06932725 -0.06703154][-0.074796557 -0.079093114 -0.082837634 -0.083285868 -0.084654421 -0.088578485 -0.09374591 -0.096779309 -0.097073704 -0.095321715 -0.091084369 -0.08487469 -0.078516334 -0.073509313 -0.070875049][-0.07458178 -0.078248106 -0.079406053 -0.074778393 -0.070682585 -0.071350142 -0.076553322 -0.082960136 -0.087212943 -0.088743724 -0.087189555 -0.082925841 -0.077444665 -0.072849214 -0.070612788][-0.070730537 -0.071822986 -0.068485983 -0.059004486 -0.050770581 -0.050188649 -0.055805724 -0.062520556 -0.06708429 -0.069627441 -0.070791274 -0.071453959 -0.071012162 -0.069899462 -0.068824485][-0.0634745 -0.065839238 -0.064605668 -0.058591217 -0.053067647 -0.053336263 -0.057592638 -0.064564861 -0.07325995 -0.080013178 -0.081653476 -0.079297505 -0.074707508 -0.069882534 -0.067343563][-0.06912896 -0.070387639 -0.068858311 -0.0645096 -0.061777607 -0.062472157 -0.064857647 -0.070829406 -0.078520715 -0.084297568 -0.084821723 -0.08164984 -0.07763885 -0.072547227 -0.06846948][-0.076765373 -0.074426159 -0.07028427 -0.065764628 -0.063934058 -0.063864008 -0.063958414 -0.064988472 -0.068981044 -0.0736082 -0.075792164 -0.07635165 -0.07423152 -0.070167914 -0.066200256][-0.072335854 -0.071628079 -0.067845345 -0.063121207 -0.060593277 -0.060395651 -0.061141636 -0.062749475 -0.067089461 -0.07041534 -0.072460368 -0.073196247 -0.072862625 -0.070832342 -0.067562565][-0.071695 -0.070445411 -0.067499563 -0.064169809 -0.061722998 -0.060660832 -0.061252594 -0.063095994 -0.067831889 -0.071618661 -0.073735259 -0.073176041 -0.071944438 -0.070064455 -0.067496434][-0.071730524 -0.069984473 -0.069435552 -0.069464728 -0.069734238 -0.070547149 -0.071087942 -0.070817351 -0.070827112 -0.070529133 -0.070400745 -0.07003963 -0.069936849 -0.068609163 -0.06706772][-0.070725165 -0.069381215 -0.069538809 -0.070674792 -0.072582856 -0.074464507 -0.074294686 -0.07164079 -0.068193741 -0.06560272 -0.06492237 -0.06576556 -0.06696108 -0.06724406 -0.066985615]]...]
INFO - root - 2017-12-06 05:27:27.840112: step 14810, loss = 0.74, batch loss = 0.52 (34.3 examples/sec; 0.234 sec/batch; 20h:36m:40s remains)
INFO - root - 2017-12-06 05:27:30.132821: step 14820, loss = 0.75, batch loss = 0.54 (35.5 examples/sec; 0.225 sec/batch; 19h:52m:27s remains)
INFO - root - 2017-12-06 05:27:32.424612: step 14830, loss = 0.77, batch loss = 0.55 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:43s remains)
INFO - root - 2017-12-06 05:27:34.734874: step 14840, loss = 0.76, batch loss = 0.55 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:44s remains)
INFO - root - 2017-12-06 05:27:37.028138: step 14850, loss = 0.77, batch loss = 0.55 (34.1 examples/sec; 0.235 sec/batch; 20h:42m:28s remains)
INFO - root - 2017-12-06 05:27:39.359310: step 14860, loss = 0.82, batch loss = 0.61 (32.9 examples/sec; 0.243 sec/batch; 21h:27m:01s remains)
INFO - root - 2017-12-06 05:27:41.622313: step 14870, loss = 0.84, batch loss = 0.63 (34.6 examples/sec; 0.231 sec/batch; 20h:23m:43s remains)
INFO - root - 2017-12-06 05:27:43.962364: step 14880, loss = 0.78, batch loss = 0.57 (32.5 examples/sec; 0.246 sec/batch; 21h:42m:19s remains)
INFO - root - 2017-12-06 05:27:46.277427: step 14890, loss = 0.76, batch loss = 0.55 (35.2 examples/sec; 0.228 sec/batch; 20h:04m:45s remains)
INFO - root - 2017-12-06 05:27:48.553130: step 14900, loss = 0.73, batch loss = 0.52 (34.0 examples/sec; 0.236 sec/batch; 20h:47m:01s remains)
2017-12-06 05:27:48.906978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070264243 -0.070291534 -0.070651695 -0.071127206 -0.071674287 -0.072185725 -0.072591141 -0.072760671 -0.072675236 -0.072318144 -0.071775436 -0.07121671 -0.070683904 -0.070351452 -0.070166066][-0.072943732 -0.073376536 -0.073972858 -0.074486844 -0.074856035 -0.075162306 -0.075325929 -0.0753522 -0.075161457 -0.074765734 -0.074184634 -0.073509336 -0.072783805 -0.072092257 -0.071487509][-0.077213526 -0.078570679 -0.079841152 -0.080675334 -0.081051126 -0.081109121 -0.0809312 -0.080622993 -0.080162764 -0.079526126 -0.078606442 -0.077459775 -0.076146439 -0.074733607 -0.073353723][-0.082347848 -0.084962346 -0.087321922 -0.088824347 -0.089523755 -0.0895403 -0.089029521 -0.088280611 -0.08729811 -0.0861476 -0.084635571 -0.08285398 -0.080852941 -0.078583486 -0.076343261][-0.087669909 -0.091565758 -0.095095769 -0.0974625 -0.098672211 -0.099004641 -0.098587565 -0.09780255 -0.096463248 -0.094684832 -0.09245383 -0.089833975 -0.0868419 -0.0833011 -0.079885878][-0.092686623 -0.097628534 -0.10221606 -0.10544817 -0.10720077 -0.10789683 -0.10765746 -0.10687118 -0.10531235 -0.10309916 -0.10024705 -0.096812613 -0.092730291 -0.087971456 -0.083343305][-0.095454507 -0.10131209 -0.10670638 -0.11072697 -0.11297897 -0.11382423 -0.113539 -0.11267844 -0.11098149 -0.10839116 -0.10506698 -0.10109322 -0.096335381 -0.090765126 -0.085340448][-0.093856685 -0.099803582 -0.1054757 -0.10994042 -0.11275192 -0.11396753 -0.11393054 -0.11322481 -0.1114914 -0.10878711 -0.10531996 -0.10119241 -0.096214049 -0.090590715 -0.0852682][-0.08937 -0.0945193 -0.099485129 -0.10359813 -0.10634901 -0.10764791 -0.10785809 -0.10755074 -0.10625018 -0.10386804 -0.10080788 -0.0971601 -0.092743643 -0.087772965 -0.083145522][-0.0836045 -0.087595209 -0.091433659 -0.094711863 -0.096882783 -0.097802788 -0.097883247 -0.097689815 -0.096755646 -0.094913162 -0.092542842 -0.089726478 -0.086390458 -0.082676113 -0.079239383][-0.076399483 -0.079037182 -0.081752889 -0.084141344 -0.085622013 -0.085968375 -0.085757762 -0.085285828 -0.084435619 -0.082999907 -0.08145377 -0.079819828 -0.077932708 -0.075873256 -0.073949218][-0.069029018 -0.070493221 -0.072269537 -0.07385543 -0.074926391 -0.075035535 -0.074650288 -0.07405062 -0.0732732 -0.072285742 -0.071437351 -0.070668481 -0.069977649 -0.06932734 -0.068888977][-0.063617982 -0.064152837 -0.065180391 -0.066215843 -0.066950366 -0.066964217 -0.066556752 -0.065945536 -0.065261044 -0.064548373 -0.064120151 -0.063931286 -0.064021982 -0.064347543 -0.064917445][-0.060502313 -0.060670808 -0.061365712 -0.06203942 -0.062536024 -0.062570728 -0.062307343 -0.0617301 -0.061043277 -0.060453605 -0.060190819 -0.060213737 -0.060624972 -0.06133277 -0.062317766][-0.058849789 -0.058861956 -0.059544668 -0.060233425 -0.060848519 -0.061017796 -0.060864121 -0.060341492 -0.059698019 -0.059121713 -0.0588263 -0.058813181 -0.059172831 -0.059892628 -0.06093052]]...]
INFO - root - 2017-12-06 05:27:51.192980: step 14910, loss = 0.87, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 19h:46m:03s remains)
INFO - root - 2017-12-06 05:27:53.479346: step 14920, loss = 0.75, batch loss = 0.54 (34.3 examples/sec; 0.234 sec/batch; 20h:36m:14s remains)
INFO - root - 2017-12-06 05:27:55.855518: step 14930, loss = 0.71, batch loss = 0.50 (31.7 examples/sec; 0.252 sec/batch; 22h:14m:01s remains)
INFO - root - 2017-12-06 05:27:58.147048: step 14940, loss = 0.76, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 20h:18m:02s remains)
INFO - root - 2017-12-06 05:28:00.458024: step 14950, loss = 0.84, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:25m:36s remains)
INFO - root - 2017-12-06 05:28:02.726640: step 14960, loss = 0.79, batch loss = 0.58 (35.4 examples/sec; 0.226 sec/batch; 19h:56m:03s remains)
INFO - root - 2017-12-06 05:28:05.028637: step 14970, loss = 0.93, batch loss = 0.72 (34.2 examples/sec; 0.234 sec/batch; 20h:38m:04s remains)
INFO - root - 2017-12-06 05:28:07.351405: step 14980, loss = 0.78, batch loss = 0.56 (34.6 examples/sec; 0.231 sec/batch; 20h:23m:36s remains)
INFO - root - 2017-12-06 05:28:09.679287: step 14990, loss = 0.77, batch loss = 0.56 (34.3 examples/sec; 0.233 sec/batch; 20h:33m:11s remains)
INFO - root - 2017-12-06 05:28:11.979175: step 15000, loss = 0.72, batch loss = 0.50 (35.1 examples/sec; 0.228 sec/batch; 20h:07m:43s remains)
2017-12-06 05:28:12.442043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059269525 -0.05894937 -0.058773547 -0.058513813 -0.058164518 -0.05785618 -0.057783067 -0.057830833 -0.057948824 -0.058079354 -0.058245834 -0.058420688 -0.05855123 -0.058668628 -0.059004523][-0.059122898 -0.058601812 -0.058198921 -0.057595879 -0.056745 -0.05592978 -0.055641558 -0.05582536 -0.0562791 -0.056793496 -0.057376746 -0.057822939 -0.05809024 -0.058331795 -0.058667637][-0.0591792 -0.058434583 -0.057709608 -0.056584392 -0.055020954 -0.053467643 -0.052697912 -0.052890908 -0.05378731 -0.055003062 -0.05628635 -0.057198931 -0.057752911 -0.058208097 -0.058600031][-0.059446603 -0.058626175 -0.057636254 -0.055958595 -0.053563174 -0.051126134 -0.049754497 -0.04996293 -0.0514043 -0.053343095 -0.055260882 -0.056617204 -0.057437979 -0.058059894 -0.058597185][-0.060015943 -0.05924391 -0.058053508 -0.055962466 -0.053024288 -0.049980443 -0.0480616 -0.048222404 -0.049945641 -0.052316934 -0.054646578 -0.056353025 -0.05736994 -0.058056936 -0.058714122][-0.060978267 -0.060376689 -0.059196066 -0.05689631 -0.053747721 -0.050520387 -0.048163217 -0.047994867 -0.04963119 -0.0520302 -0.0545067 -0.056352772 -0.057541478 -0.058261618 -0.058978826][-0.062230334 -0.062011249 -0.061136555 -0.059051972 -0.056079004 -0.052925333 -0.050316308 -0.049477555 -0.050565913 -0.052550796 -0.054773033 -0.056599885 -0.057883505 -0.058616545 -0.059325203][-0.063516073 -0.063847125 -0.063794956 -0.062437326 -0.059991464 -0.057226676 -0.054519493 -0.053006776 -0.052867372 -0.05390485 -0.055555534 -0.056968663 -0.058235779 -0.058934838 -0.059640832][-0.065010339 -0.066276424 -0.067488104 -0.06738238 -0.065843746 -0.063514031 -0.060600884 -0.058224551 -0.056770075 -0.056422994 -0.057064243 -0.058054063 -0.059183974 -0.059633419 -0.060081754][-0.066720016 -0.069069669 -0.071396969 -0.072273746 -0.071479112 -0.069344364 -0.066223785 -0.063259616 -0.060911149 -0.05964987 -0.059501916 -0.060013663 -0.060547873 -0.060449064 -0.060616814][-0.067885473 -0.07055568 -0.073215134 -0.074395552 -0.073863178 -0.0717548 -0.068820789 -0.0659318 -0.063515857 -0.06204237 -0.06150347 -0.061548036 -0.061514743 -0.061001725 -0.060949974][-0.0675035 -0.069628961 -0.071678467 -0.07251934 -0.0720382 -0.070298038 -0.0679861 -0.065705344 -0.063812539 -0.062709667 -0.062138841 -0.06191241 -0.061562844 -0.060943048 -0.060933545][-0.06576018 -0.066900469 -0.068007641 -0.0683008 -0.067822486 -0.066671081 -0.065192491 -0.063790351 -0.062683806 -0.062090211 -0.061679877 -0.061345413 -0.061008852 -0.060611188 -0.060783371][-0.063694715 -0.064058721 -0.064576805 -0.064643383 -0.064312585 -0.063681 -0.062950224 -0.062231112 -0.061651036 -0.061302297 -0.061019938 -0.060743041 -0.060500253 -0.060272619 -0.060527306][-0.062167898 -0.062042885 -0.062243164 -0.062252615 -0.062076781 -0.061774116 -0.06145465 -0.061156683 -0.060892135 -0.060669914 -0.060482271 -0.060333814 -0.060205124 -0.060107484 -0.060419254]]...]
INFO - root - 2017-12-06 05:28:14.743662: step 15010, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.235 sec/batch; 20h:41m:16s remains)
INFO - root - 2017-12-06 05:28:17.074368: step 15020, loss = 0.81, batch loss = 0.60 (34.1 examples/sec; 0.234 sec/batch; 20h:40m:21s remains)
INFO - root - 2017-12-06 05:28:19.329785: step 15030, loss = 0.81, batch loss = 0.59 (36.2 examples/sec; 0.221 sec/batch; 19h:28m:10s remains)
INFO - root - 2017-12-06 05:28:21.591611: step 15040, loss = 0.76, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:38m:03s remains)
INFO - root - 2017-12-06 05:28:23.876390: step 15050, loss = 0.78, batch loss = 0.56 (35.8 examples/sec; 0.223 sec/batch; 19h:40m:55s remains)
INFO - root - 2017-12-06 05:28:26.179686: step 15060, loss = 0.80, batch loss = 0.59 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:51s remains)
INFO - root - 2017-12-06 05:28:28.480970: step 15070, loss = 0.78, batch loss = 0.57 (35.3 examples/sec; 0.226 sec/batch; 19h:57m:59s remains)
INFO - root - 2017-12-06 05:28:30.749995: step 15080, loss = 0.82, batch loss = 0.60 (34.7 examples/sec; 0.231 sec/batch; 20h:20m:17s remains)
INFO - root - 2017-12-06 05:28:33.026484: step 15090, loss = 0.88, batch loss = 0.67 (35.6 examples/sec; 0.225 sec/batch; 19h:47m:52s remains)
INFO - root - 2017-12-06 05:28:35.385296: step 15100, loss = 0.88, batch loss = 0.66 (31.8 examples/sec; 0.251 sec/batch; 22h:09m:31s remains)
2017-12-06 05:28:36.863128: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0583535 0.66313106 0.1286929 -0.48178461 -1.3224915 -1.5874403 -1.362138 -1.0480276 -0.5967164 -0.22606313 0.14257967 0.3148278 0.059153102 -0.148367 -0.43915132][0.75252521 0.47083464 -0.03052666 -0.68854237 -1.3100923 -1.5987152 -1.6427088 -1.1817918 -0.62159526 -0.31270915 -0.053035818 0.12673831 -0.098115653 -0.25789469 -0.66846609][0.505753 -0.12090832 -0.77973026 -1.2962738 -1.7085047 -1.9200485 -1.6543586 -1.3701751 -1.0357263 -0.60520917 -0.1551632 -0.14990741 -0.034375735 0.086040817 -0.36180443][0.035235882 -0.70383203 -1.3479241 -1.8948082 -2.079555 -2.1614194 -1.9089134 -1.3209093 -0.85594779 -0.69646418 -0.54769439 -0.58257914 -0.31928587 -0.38324213 -0.48496604][0.057029195 -0.54568911 -1.143342 -1.7383873 -1.8434261 -1.6770245 -1.208829 -0.83117682 -0.55190974 -0.26721469 -0.1327403 -0.22548413 -0.097704455 -0.1389603 -0.44610637][0.54324305 -0.29703295 -0.93838477 -1.1598492 -1.149377 -1.1243631 -0.91807204 -0.37136328 -0.00087002665 -0.067238346 -0.0070403591 0.083530866 0.0066995248 -0.067840762 -0.63911963][0.53845626 -0.0086168721 -0.42897251 -0.78873146 -0.75016445 -0.30756885 -0.012147643 0.18909025 0.47848549 0.62599134 0.50750744 0.26481515 0.19065872 -0.13712472 -0.23163962][0.57238191 0.062473096 -0.25078136 -0.35146302 -0.23166746 -0.035781223 0.45847854 0.65864038 0.90952665 0.97464824 0.7288056 0.58401763 0.14072883 -0.12676489 -0.44436023][0.2335366 -0.022445634 -0.14892352 -0.28430742 -0.011025235 0.18044767 0.56490505 0.72818595 0.96635151 0.80010808 0.66170967 0.64583778 0.33254081 -0.033039708 -0.24245051][0.043847524 -0.074017629 -0.16612023 -0.25240576 0.15161008 0.20468944 0.38592094 0.43833038 0.39279258 0.294348 0.29077211 0.057661422 -0.028487477 -0.25096592 -0.30891165][0.028767839 -0.3498145 -0.42774338 -0.32219881 -0.14618763 0.0079143792 0.12410364 0.061152481 -0.030428227 -0.28196272 -0.621375 -0.75284392 -0.78565049 -0.95711005 -0.77656996][0.19759202 -0.10202398 -0.35767815 -0.37951922 -0.38612252 -0.53417712 -0.59645283 -0.36441737 -0.23771691 -0.43596491 -0.73383778 -0.97720867 -1.2107615 -1.0572137 -0.88334024][-0.35449016 -0.58039826 -0.83114773 -0.74254054 -0.69447148 -0.56397057 -0.64408571 -0.69394231 -0.68720621 -0.81123823 -0.74134219 -0.935095 -0.80367476 -0.66846812 -0.5060395][-0.42342731 -0.51577032 -0.60070795 -0.37842548 -0.47993961 -0.53099042 -0.34501129 -0.28974414 -0.15911445 -0.24602297 -0.47342512 -0.69024861 -0.59160733 -0.77885866 -0.64956439][-0.28117812 -0.48065314 -0.6548363 -0.54668373 -0.5988189 -0.60246438 -0.71880829 -0.85436237 -0.86105496 -0.98539126 -0.95097148 -1.1870286 -1.200593 -0.97804034 -0.84822357]]...]
INFO - root - 2017-12-06 05:28:39.168912: step 15110, loss = 0.74, batch loss = 0.52 (31.3 examples/sec; 0.256 sec/batch; 22h:33m:21s remains)
INFO - root - 2017-12-06 05:28:41.475109: step 15120, loss = 0.81, batch loss = 0.60 (32.1 examples/sec; 0.249 sec/batch; 21h:58m:29s remains)
INFO - root - 2017-12-06 05:28:43.759184: step 15130, loss = 0.82, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:27s remains)
INFO - root - 2017-12-06 05:28:46.059949: step 15140, loss = 0.79, batch loss = 0.58 (35.8 examples/sec; 0.223 sec/batch; 19h:42m:07s remains)
INFO - root - 2017-12-06 05:28:48.371691: step 15150, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:06m:06s remains)
INFO - root - 2017-12-06 05:28:50.697431: step 15160, loss = 0.72, batch loss = 0.51 (35.4 examples/sec; 0.226 sec/batch; 19h:56m:46s remains)
INFO - root - 2017-12-06 05:28:52.963655: step 15170, loss = 0.85, batch loss = 0.63 (35.4 examples/sec; 0.226 sec/batch; 19h:54m:04s remains)
INFO - root - 2017-12-06 05:28:55.296808: step 15180, loss = 0.78, batch loss = 0.56 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:14s remains)
INFO - root - 2017-12-06 05:28:57.619735: step 15190, loss = 0.77, batch loss = 0.55 (34.4 examples/sec; 0.232 sec/batch; 20h:28m:11s remains)
INFO - root - 2017-12-06 05:28:59.945340: step 15200, loss = 0.79, batch loss = 0.58 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:43s remains)
2017-12-06 05:29:00.349041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10209752 -0.10065599 -0.098709449 -0.10569622 -0.10365063 -0.093897313 -0.084548518 -0.078516647 -0.086146176 -0.10082355 -0.10952172 -0.10627092 -0.1002138 -0.0945913 -0.089733332][-0.1254255 -0.11506586 -0.11125462 -0.12064382 -0.13180189 -0.13653237 -0.13701394 -0.13182521 -0.12563032 -0.11990219 -0.11432902 -0.10526689 -0.098697253 -0.093218915 -0.089624785][-0.12837459 -0.10576719 -0.098459028 -0.10378112 -0.11397029 -0.12067215 -0.12215579 -0.12070861 -0.11563201 -0.10786848 -0.10179622 -0.095363759 -0.091727391 -0.088568516 -0.087061934][-0.11986692 -0.09519925 -0.086149164 -0.086918831 -0.093343288 -0.098031349 -0.098864391 -0.10238611 -0.10753256 -0.10945184 -0.1088558 -0.10331942 -0.09435574 -0.088983908 -0.08591558][-0.10841516 -0.090584926 -0.085998684 -0.088491589 -0.094054744 -0.096317552 -0.095351472 -0.10333145 -0.11668141 -0.12967871 -0.13685197 -0.12970755 -0.11061984 -0.09638077 -0.086367369][-0.10099415 -0.090029813 -0.09032511 -0.094494864 -0.095838495 -0.088578 -0.083131723 -0.086529568 -0.10004879 -0.12269054 -0.14577052 -0.15032275 -0.13487087 -0.11382334 -0.09454862][-0.097140275 -0.088760495 -0.092577256 -0.096470423 -0.092003495 -0.077593088 -0.061538205 -0.041929916 -0.035430606 -0.058875177 -0.10149978 -0.12876427 -0.13400139 -0.11637937 -0.095672749][-0.09494143 -0.089018293 -0.095241144 -0.09790612 -0.08857552 -0.058992803 -0.008178018 0.060292423 0.095354274 0.061120197 -0.011424065 -0.071032822 -0.1078874 -0.1044364 -0.0865158][-0.092375033 -0.092426509 -0.10197704 -0.10521061 -0.088627249 -0.030859582 0.070699722 0.18489613 0.22871871 0.17423634 0.079362661 -0.0024217963 -0.06738662 -0.083944581 -0.077618711][-0.091328979 -0.095741153 -0.10929275 -0.11437956 -0.09318658 -0.01852271 0.10455006 0.21934991 0.24256976 0.18162714 0.098145291 0.023996219 -0.043666787 -0.068558849 -0.073543154][-0.091861792 -0.0978837 -0.11505098 -0.12615445 -0.11391885 -0.057055641 0.034216084 0.10598013 0.11142205 0.07454583 0.023765191 -0.02468437 -0.073415071 -0.084252939 -0.083652474][-0.090478934 -0.09804377 -0.11757879 -0.13826218 -0.14692998 -0.12590806 -0.081709437 -0.041782793 -0.026256755 -0.030425437 -0.05281743 -0.079542823 -0.10732201 -0.10326463 -0.094780505][-0.087378889 -0.096206263 -0.1151707 -0.14211364 -0.16837445 -0.17771834 -0.16475734 -0.13775903 -0.11071187 -0.0968525 -0.10170502 -0.10825828 -0.1135871 -0.10076638 -0.09374205][-0.086446427 -0.09321589 -0.10655396 -0.12986805 -0.15927985 -0.18353316 -0.19384672 -0.18851614 -0.17256974 -0.15620326 -0.144113 -0.12818609 -0.11304021 -0.096297823 -0.092328563][-0.086812235 -0.089542739 -0.095799483 -0.10867956 -0.12809813 -0.14968178 -0.16808453 -0.17875591 -0.17909405 -0.1678025 -0.14751136 -0.12478961 -0.10362473 -0.093882918 -0.092212453]]...]
INFO - root - 2017-12-06 05:29:02.669608: step 15210, loss = 0.71, batch loss = 0.49 (34.4 examples/sec; 0.233 sec/batch; 20h:29m:56s remains)
INFO - root - 2017-12-06 05:29:05.033997: step 15220, loss = 0.77, batch loss = 0.55 (32.3 examples/sec; 0.248 sec/batch; 21h:51m:22s remains)
INFO - root - 2017-12-06 05:29:07.377892: step 15230, loss = 0.71, batch loss = 0.50 (35.3 examples/sec; 0.226 sec/batch; 19h:57m:00s remains)
INFO - root - 2017-12-06 05:29:09.723398: step 15240, loss = 0.76, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:25m:51s remains)
INFO - root - 2017-12-06 05:29:11.997442: step 15250, loss = 0.80, batch loss = 0.58 (34.7 examples/sec; 0.230 sec/batch; 20h:18m:19s remains)
INFO - root - 2017-12-06 05:29:14.336462: step 15260, loss = 0.81, batch loss = 0.59 (33.8 examples/sec; 0.237 sec/batch; 20h:50m:35s remains)
INFO - root - 2017-12-06 05:29:16.659477: step 15270, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.234 sec/batch; 20h:38m:43s remains)
INFO - root - 2017-12-06 05:29:18.964020: step 15280, loss = 0.87, batch loss = 0.65 (34.6 examples/sec; 0.231 sec/batch; 20h:22m:21s remains)
INFO - root - 2017-12-06 05:29:21.265315: step 15290, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-06 05:29:23.574892: step 15300, loss = 0.76, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:35m:52s remains)
2017-12-06 05:29:23.986693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0672704 -0.067047775 -0.067105487 -0.067201704 -0.067414634 -0.067836069 -0.0684917 -0.069362767 -0.07030946 -0.071162328 -0.071816631 -0.072223373 -0.0724114 -0.072473332 -0.072538987][-0.067207627 -0.066953167 -0.066972136 -0.0670283 -0.067179762 -0.0675207 -0.06809403 -0.068896964 -0.06981118 -0.070652753 -0.071302243 -0.071718842 -0.071933113 -0.07201118 -0.072042115][-0.066896632 -0.066527329 -0.066543013 -0.066615954 -0.06677147 -0.067088723 -0.06762363 -0.068366148 -0.069232978 -0.0700423 -0.070681311 -0.071101367 -0.071350425 -0.071482562 -0.071510047][-0.066294186 -0.065858781 -0.0659045 -0.066022292 -0.066210523 -0.066533662 -0.067035422 -0.067701086 -0.068477 -0.06922622 -0.069868281 -0.070325494 -0.070638843 -0.070845246 -0.070905507][-0.065523982 -0.065089054 -0.065202579 -0.065406069 -0.065666087 -0.066009238 -0.066461377 -0.06702809 -0.067676857 -0.068325669 -0.068950325 -0.069474012 -0.069863118 -0.070122071 -0.070226319][-0.0648669 -0.064439833 -0.064663276 -0.06499999 -0.06536413 -0.065748073 -0.0661432 -0.06656336 -0.067040689 -0.067554988 -0.068117484 -0.068653323 -0.069088422 -0.069397368 -0.069545329][-0.064358152 -0.063915431 -0.064272881 -0.064766504 -0.065251671 -0.065686643 -0.066044986 -0.066349335 -0.066659592 -0.0670188 -0.06746392 -0.067941435 -0.068366013 -0.068700969 -0.06888561][-0.063949145 -0.063451692 -0.063899837 -0.064515509 -0.065116942 -0.065598123 -0.065953 -0.066215068 -0.066432767 -0.066659681 -0.066978768 -0.067366704 -0.067740582 -0.068040632 -0.068216577][-0.063668743 -0.063121222 -0.063559607 -0.064183518 -0.064842835 -0.065386206 -0.065777577 -0.066062883 -0.06625592 -0.0664149 -0.066641241 -0.066931322 -0.0672126 -0.06742809 -0.06759315][-0.0637641 -0.063146435 -0.063489541 -0.064038746 -0.064665519 -0.065218069 -0.065647036 -0.065960966 -0.066152871 -0.066287078 -0.066437744 -0.066623375 -0.0667965 -0.066932283 -0.067108572][-0.064293072 -0.063595131 -0.0638064 -0.064222425 -0.064749882 -0.065230407 -0.065641604 -0.065957919 -0.066155106 -0.066281974 -0.066378243 -0.06647969 -0.066557869 -0.066622749 -0.066798337][-0.065030612 -0.064234972 -0.064282924 -0.064516857 -0.064891964 -0.065263882 -0.065611586 -0.065920517 -0.066142969 -0.066283979 -0.06636259 -0.066421635 -0.066442974 -0.066449247 -0.066613756][-0.065785766 -0.064911932 -0.0648077 -0.064859867 -0.065061 -0.065311655 -0.065578654 -0.065852985 -0.066089354 -0.066242956 -0.066321246 -0.066362821 -0.066357352 -0.066338971 -0.066475496][-0.066456623 -0.065570049 -0.065390319 -0.065320253 -0.065382451 -0.065517269 -0.065704167 -0.065911159 -0.066100709 -0.066237725 -0.066304773 -0.066332839 -0.066319488 -0.066303559 -0.066409074][-0.067021713 -0.066143773 -0.065948114 -0.065832958 -0.06580957 -0.065839961 -0.065947138 -0.066078268 -0.066200681 -0.066296458 -0.066341445 -0.066354766 -0.066341765 -0.066353537 -0.066476166]]...]
INFO - root - 2017-12-06 05:29:26.308032: step 15310, loss = 0.88, batch loss = 0.66 (34.4 examples/sec; 0.233 sec/batch; 20h:29m:14s remains)
INFO - root - 2017-12-06 05:29:28.648766: step 15320, loss = 0.84, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 20h:05m:57s remains)
INFO - root - 2017-12-06 05:29:30.934706: step 15330, loss = 0.83, batch loss = 0.62 (35.5 examples/sec; 0.226 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-06 05:29:33.273374: step 15340, loss = 0.80, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:22m:13s remains)
INFO - root - 2017-12-06 05:29:35.601157: step 15350, loss = 0.85, batch loss = 0.64 (34.5 examples/sec; 0.232 sec/batch; 20h:27m:21s remains)
INFO - root - 2017-12-06 05:29:37.907896: step 15360, loss = 0.82, batch loss = 0.61 (35.6 examples/sec; 0.224 sec/batch; 19h:46m:36s remains)
INFO - root - 2017-12-06 05:29:40.226709: step 15370, loss = 0.72, batch loss = 0.50 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:13s remains)
INFO - root - 2017-12-06 05:29:42.541741: step 15380, loss = 0.82, batch loss = 0.61 (34.1 examples/sec; 0.234 sec/batch; 20h:39m:06s remains)
INFO - root - 2017-12-06 05:29:44.889757: step 15390, loss = 0.83, batch loss = 0.62 (33.3 examples/sec; 0.240 sec/batch; 21h:10m:10s remains)
INFO - root - 2017-12-06 05:29:47.191763: step 15400, loss = 0.80, batch loss = 0.59 (34.1 examples/sec; 0.234 sec/batch; 20h:38m:14s remains)
2017-12-06 05:29:50.422793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13897581 -0.13791756 -0.13345584 -0.11913643 -0.12562257 -0.15133643 -0.18687513 -0.24833715 -0.307355 -0.27395555 -0.24989277 -0.26754597 -0.31166834 -0.34795278 -0.31636915][-0.19453716 -0.21499014 -0.22643188 -0.20502183 -0.19799826 -0.22603399 -0.2883639 -0.36610147 -0.4509705 -0.55314845 -0.6902073 -0.74542785 -0.77319455 -0.718341 -0.47798282][-0.24985585 -0.3287771 -0.37879032 -0.38278586 -0.38168776 -0.33664191 -0.38972619 -0.55350721 -0.65701091 -0.72795564 -0.87191093 -1.023077 -1.0750948 -0.91210163 -0.54953355][-0.31383243 -0.48358744 -0.64765018 -0.72868985 -0.75204509 -0.66060758 -0.6339612 -0.65364552 -0.69136882 -0.78742075 -0.87823087 -0.9697026 -1.0295358 -0.89014703 -0.4927977][-0.36644578 -0.56769228 -0.7528044 -0.82390326 -0.75066251 -0.47720668 -0.30917054 -0.15515564 -0.074772865 -0.13209195 -0.3315233 -0.60504663 -0.73082668 -0.72729838 -0.46475643][-0.344119 -0.56766456 -0.7285496 -0.72638643 -0.51040339 0.026740901 0.50837553 0.86862552 0.99554491 0.9566251 0.71103817 0.31826678 0.060675286 -0.14247002 -0.039426308][-0.28645712 -0.47178632 -0.5814423 -0.53373235 -0.25988591 0.308162 0.97670531 1.6127884 1.8336958 1.7485592 1.4228656 0.81897622 0.27378121 -0.063312545 -0.013876274][-0.25306824 -0.34817895 -0.34566081 -0.22663292 0.044455431 0.51367676 1.0153009 1.5151685 1.7004967 1.5962888 1.10262 0.42894107 -0.17982739 -0.74055851 -0.937316][-0.28310555 -0.38902536 -0.38477597 -0.30199477 -0.049355209 0.3205561 0.70980996 1.0276493 1.0887233 0.92794561 0.50639522 -0.0743856 -0.62188858 -1.1111163 -1.3223052][-0.38882375 -0.54941773 -0.66638249 -0.69943273 -0.6143102 -0.54691291 -0.38939089 -0.23732388 -0.20711362 -0.25816944 -0.46747303 -0.79230887 -1.0602102 -1.1799858 -1.170069][-0.48806036 -0.6942873 -0.873643 -0.96553183 -0.99147308 -0.9944095 -0.99678576 -1.0422941 -1.0717365 -1.1107823 -1.2239814 -1.2788135 -1.2575514 -1.1197946 -0.93047047][-0.62807888 -0.92120856 -1.14597 -1.2377546 -1.2120004 -1.1944629 -1.2064519 -1.2057346 -1.1728892 -1.1018152 -1.0818198 -1.0058086 -0.76912129 -0.31777719 0.09804032][-0.82945383 -1.1302379 -1.2922757 -1.3113141 -1.1417419 -0.96320069 -0.853581 -0.82119209 -0.76120675 -0.5939945 -0.48567551 -0.35858113 -0.10046488 0.36703309 0.87625664][-0.79234111 -0.99669075 -1.1161675 -1.0795739 -0.78698969 -0.37865213 -0.087328576 0.048735283 0.039584644 0.024126306 0.041895844 0.069166221 0.21727082 0.52695256 0.8685773][-0.52573478 -0.64404386 -0.62382221 -0.52752483 -0.22754973 0.24185127 0.67436671 1.004199 1.0604938 0.85435772 0.53862852 0.35441592 0.36782941 0.58505869 0.805787]]...]
INFO - root - 2017-12-06 05:29:52.836481: step 15410, loss = 0.73, batch loss = 0.52 (32.9 examples/sec; 0.243 sec/batch; 21h:24m:04s remains)
INFO - root - 2017-12-06 05:29:55.211675: step 15420, loss = 0.78, batch loss = 0.56 (32.6 examples/sec; 0.245 sec/batch; 21h:36m:21s remains)
INFO - root - 2017-12-06 05:29:57.491524: step 15430, loss = 0.70, batch loss = 0.49 (34.0 examples/sec; 0.235 sec/batch; 20h:41m:43s remains)
INFO - root - 2017-12-06 05:29:59.798802: step 15440, loss = 0.79, batch loss = 0.58 (35.5 examples/sec; 0.226 sec/batch; 19h:52m:12s remains)
INFO - root - 2017-12-06 05:30:02.087496: step 15450, loss = 0.82, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-06 05:30:04.408433: step 15460, loss = 0.80, batch loss = 0.58 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:13s remains)
INFO - root - 2017-12-06 05:30:06.724367: step 15470, loss = 0.93, batch loss = 0.72 (34.4 examples/sec; 0.233 sec/batch; 20h:28m:47s remains)
INFO - root - 2017-12-06 05:30:09.049122: step 15480, loss = 0.78, batch loss = 0.56 (35.0 examples/sec; 0.229 sec/batch; 20h:08m:05s remains)
INFO - root - 2017-12-06 05:30:11.352006: step 15490, loss = 0.88, batch loss = 0.66 (35.8 examples/sec; 0.223 sec/batch; 19h:40m:27s remains)
INFO - root - 2017-12-06 05:30:13.636105: step 15500, loss = 0.80, batch loss = 0.58 (32.4 examples/sec; 0.247 sec/batch; 21h:44m:58s remains)
2017-12-06 05:30:14.664032: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.67464852 -0.14373451 -0.49082273 -0.66159391 -1.1293712 -1.5834218 -1.8784575 -2.1969941 -2.4056118 -2.3403034 -2.288976 -1.6394684 -1.3129379 -0.7572242 0.048084497][1.4618783 1.3021451 1.3975149 0.93195212 0.31147209 -0.18017203 -0.75902081 -1.3057358 -1.7177418 -2.03515 -2.1911705 -2.1862442 -1.9931662 -1.5039937 -0.90780163][0.97732353 0.45884767 0.523751 0.85375321 0.91970038 0.3669619 -0.25670502 -0.9422909 -1.7749017 -2.1619802 -2.5827315 -2.9020848 -3.0007763 -2.9477892 -2.5195971][-0.018494606 -0.16289291 -0.68179464 -0.86988878 -0.48093557 -0.34100226 -0.41620231 -0.90455711 -1.4987878 -1.5244187 -1.7085713 -2.0638862 -2.4144561 -3.0716865 -3.2877076][0.13206621 -0.20549408 -0.28373307 -0.22537871 -0.58879262 -0.65630877 -0.41111383 -0.45457342 -0.65984011 -0.77308321 -0.81580341 -0.9530443 -1.3570304 -1.9322999 -2.0627451][0.062354252 -0.12544572 -0.22871712 -0.29259276 -0.22727694 -0.083118893 -0.0883528 -0.22376348 -0.045778755 -0.019741848 -0.12777895 -0.32163316 -0.72826576 -1.207504 -1.4720321][-0.43973088 -0.33598268 -0.21453452 0.10403289 0.28038919 0.59405756 0.85432243 0.92883027 1.1093832 1.0394465 1.068007 0.80261612 0.30686527 0.0058092847 -0.15938139][-1.125526 -1.2852237 -1.3457373 -1.0898006 -0.77946317 -0.27884465 0.24326852 0.96952856 1.4465314 1.6906921 1.908646 1.7064109 1.5609951 1.2661788 1.1252761][-1.4054395 -1.6721162 -1.8022382 -1.7407707 -1.660517 -1.4355687 -1.0223564 -0.45040309 0.068281278 0.68798554 1.3075349 1.8350583 2.100337 1.9315739 2.0011158][-1.5680047 -1.6796997 -1.8044409 -2.0004537 -2.236429 -2.1671054 -1.8968747 -1.4307733 -0.925383 -0.559724 -0.23053017 0.41282675 0.69714844 1.1839648 1.6706419][-1.6715727 -1.9840471 -2.4111483 -2.5719409 -2.6257432 -2.7858019 -2.9054203 -2.6639621 -2.3836498 -2.1714144 -2.0491357 -1.7858468 -1.4515424 -0.9973594 -0.77315414][-1.6579698 -1.8443681 -2.0118649 -2.259084 -2.455394 -2.6732774 -2.859848 -3.1839805 -3.2917018 -3.0093217 -2.6986117 -2.4246166 -2.4262481 -2.1947234 -1.8230978][-1.4920124 -1.6588194 -1.7766117 -1.9776449 -2.1677756 -2.4178848 -2.6164203 -2.8247612 -3.0217607 -3.1365407 -3.1425889 -2.931792 -2.6901762 -2.2031288 -1.7844323][-1.1302894 -1.2564845 -1.3162258 -1.4979538 -1.6337337 -1.7381021 -1.9088026 -2.09629 -2.2828355 -2.5022371 -2.6861339 -2.8449352 -2.7681439 -2.4426932 -2.1629086][-0.86374366 -0.9838208 -1.0118119 -1.1653432 -1.2869776 -1.4270864 -1.5598407 -1.6654228 -1.8070362 -1.9689099 -2.1419981 -2.263684 -2.2350721 -2.2750807 -2.1160989]]...]
INFO - root - 2017-12-06 05:30:16.963719: step 15510, loss = 0.70, batch loss = 0.49 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:30s remains)
INFO - root - 2017-12-06 05:30:19.242304: step 15520, loss = 0.82, batch loss = 0.60 (35.1 examples/sec; 0.228 sec/batch; 20h:05m:25s remains)
INFO - root - 2017-12-06 05:30:21.575885: step 15530, loss = 0.76, batch loss = 0.55 (32.0 examples/sec; 0.250 sec/batch; 21h:58m:48s remains)
INFO - root - 2017-12-06 05:30:23.844029: step 15540, loss = 0.82, batch loss = 0.60 (35.0 examples/sec; 0.228 sec/batch; 20h:06m:53s remains)
INFO - root - 2017-12-06 05:30:26.118601: step 15550, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.225 sec/batch; 19h:50m:41s remains)
INFO - root - 2017-12-06 05:30:28.411941: step 15560, loss = 0.82, batch loss = 0.60 (34.9 examples/sec; 0.229 sec/batch; 20h:10m:56s remains)
INFO - root - 2017-12-06 05:30:30.707120: step 15570, loss = 0.95, batch loss = 0.74 (34.7 examples/sec; 0.230 sec/batch; 20h:16m:36s remains)
INFO - root - 2017-12-06 05:30:33.031071: step 15580, loss = 0.76, batch loss = 0.55 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:30s remains)
INFO - root - 2017-12-06 05:30:35.331908: step 15590, loss = 0.89, batch loss = 0.67 (35.0 examples/sec; 0.228 sec/batch; 20h:05m:45s remains)
INFO - root - 2017-12-06 05:30:37.636291: step 15600, loss = 0.81, batch loss = 0.59 (33.4 examples/sec; 0.239 sec/batch; 21h:03m:44s remains)
2017-12-06 05:30:41.411121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.14085716 -0.14374162 -0.13802442 -0.12794994 -0.11911748 -0.11420516 -0.10886908 -0.10468959 -0.10337055 -0.10008458 -0.098282829 -0.09528406 -0.091189481 -0.087834507 -0.087630168][-0.094503924 -0.10261217 -0.10401006 -0.098698042 -0.091758527 -0.088319376 -0.088413231 -0.093446046 -0.092327721 -0.094402172 -0.096092589 -0.096278921 -0.094454855 -0.090734765 -0.0856422][-0.076148696 -0.07940957 -0.078845084 -0.081063859 -0.0792349 -0.078151919 -0.081541784 -0.085239351 -0.089573964 -0.092966631 -0.095141262 -0.096558049 -0.096021689 -0.093941189 -0.089111514][-0.060048804 -0.06137915 -0.063672349 -0.064329162 -0.063005134 -0.064939737 -0.068716638 -0.07495299 -0.081367 -0.086858675 -0.090912469 -0.09168601 -0.089615077 -0.0874637 -0.084162742][-0.053624477 -0.054627888 -0.053037021 -0.054425281 -0.056221288 -0.058395728 -0.060500044 -0.065228641 -0.071200937 -0.077057928 -0.081941776 -0.08481928 -0.08529751 -0.084251218 -0.081706457][-0.0426568 -0.041550752 -0.043682195 -0.045242783 -0.046746638 -0.049901064 -0.053543191 -0.057643924 -0.062925018 -0.068356924 -0.072770447 -0.075283349 -0.076153256 -0.076598436 -0.076603994][-0.041835368 -0.040282138 -0.040069155 -0.042165954 -0.04713494 -0.050869223 -0.05447704 -0.059923083 -0.066212386 -0.066177443 -0.065155 -0.066882744 -0.06819427 -0.070095435 -0.072112858][-0.04496415 -0.043249615 -0.043251984 -0.045435552 -0.048962176 -0.051980939 -0.054645259 -0.056131918 -0.057863634 -0.060124278 -0.063442089 -0.06539423 -0.066199675 -0.067336395 -0.069317922][-0.045028307 -0.044748414 -0.045999512 -0.046970788 -0.047921196 -0.050229914 -0.05229475 -0.05307984 -0.0541365 -0.055350386 -0.056709997 -0.058177073 -0.060130417 -0.062540442 -0.065508023][-0.0459539 -0.045578543 -0.046545465 -0.048395135 -0.0486579 -0.049357712 -0.049530078 -0.050024435 -0.050414212 -0.051294796 -0.052473363 -0.053729072 -0.055282257 -0.057207979 -0.059722729][-0.047025487 -0.04622677 -0.046565134 -0.047052789 -0.047733497 -0.048663728 -0.048339397 -0.048294179 -0.048185084 -0.04848785 -0.049114268 -0.050071079 -0.051447079 -0.052906215 -0.05456014][-0.047686283 -0.046704777 -0.046768498 -0.046845682 -0.046916239 -0.047078252 -0.046752553 -0.04668723 -0.046679616 -0.046992879 -0.047423933 -0.048052732 -0.048645321 -0.049269967 -0.05069901][-0.047850575 -0.047793254 -0.046924792 -0.046505924 -0.046460859 -0.046448171 -0.046111021 -0.045523882 -0.046434823 -0.046711192 -0.046996396 -0.047419973 -0.047703385 -0.047905054 -0.048665766][-0.046916097 -0.046827666 -0.046356875 -0.048526019 -0.046909764 -0.047285695 -0.04553033 -0.043733962 -0.046121236 -0.046901021 -0.047237154 -0.047471125 -0.047497172 -0.047448676 -0.0478386][-0.048481129 -0.049984325 -0.045958709 -0.046021361 -0.046004575 -0.046532 -0.047473777 -0.045217514 -0.046975426 -0.047316015 -0.047605656 -0.047763094 -0.047711466 -0.047454961 -0.047514956]]...]
INFO - root - 2017-12-06 05:30:43.789849: step 15610, loss = 0.73, batch loss = 0.52 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:48s remains)
INFO - root - 2017-12-06 05:30:46.110448: step 15620, loss = 0.69, batch loss = 0.48 (33.1 examples/sec; 0.242 sec/batch; 21h:15m:40s remains)
INFO - root - 2017-12-06 05:30:48.398959: step 15630, loss = 0.84, batch loss = 0.63 (34.5 examples/sec; 0.232 sec/batch; 20h:24m:05s remains)
INFO - root - 2017-12-06 05:30:50.711543: step 15640, loss = 0.79, batch loss = 0.58 (34.2 examples/sec; 0.234 sec/batch; 20h:35m:51s remains)
INFO - root - 2017-12-06 05:30:53.032068: step 15650, loss = 1.02, batch loss = 0.80 (34.5 examples/sec; 0.232 sec/batch; 20h:24m:50s remains)
INFO - root - 2017-12-06 05:30:55.403083: step 15660, loss = 0.82, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 19h:44m:31s remains)
INFO - root - 2017-12-06 05:30:57.660239: step 15670, loss = 0.81, batch loss = 0.59 (34.3 examples/sec; 0.233 sec/batch; 20h:30m:11s remains)
INFO - root - 2017-12-06 05:30:59.965612: step 15680, loss = 0.90, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:56s remains)
INFO - root - 2017-12-06 05:31:02.258642: step 15690, loss = 0.83, batch loss = 0.62 (34.8 examples/sec; 0.230 sec/batch; 20h:14m:10s remains)
INFO - root - 2017-12-06 05:31:04.554815: step 15700, loss = 0.87, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:21s remains)
2017-12-06 05:31:04.912897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1011728 -0.11277644 -0.14629024 -0.21703702 -0.32663673 -0.46079808 -0.59856576 -0.70782274 -0.75359184 -0.71054775 -0.58962572 -0.43791753 -0.30147398 -0.20033608 -0.14001076][-0.10983118 -0.13529758 -0.19076407 -0.28504702 -0.42973387 -0.61638081 -0.8540383 -1.102036 -1.2719711 -1.2867029 -1.1412323 -0.89229518 -0.62459219 -0.39495891 -0.22963509][-0.11926195 -0.15101269 -0.2165942 -0.30292612 -0.410999 -0.55257744 -0.80780262 -1.1664861 -1.5064451 -1.6851597 -1.6441871 -1.4039803 -1.0480766 -0.68435997 -0.3895728][-0.1221931 -0.1435948 -0.17258498 -0.17273453 -0.13317287 -0.10149515 -0.24201745 -0.61092675 -1.0795087 -1.4822085 -1.6893244 -1.6406513 -1.3576336 -0.94245523 -0.54619133][-0.12638593 -0.12947896 -0.097807914 0.050764486 0.333686 0.6464417 0.73334342 0.51473445 0.033276737 -0.57776612 -1.0533713 -1.3128147 -1.3011307 -1.0302105 -0.6467309][-0.13013265 -0.12548132 -0.053785149 0.20331161 0.6762383 1.2702613 1.6704147 1.7211382 1.3654422 0.70622373 0.025056764 -0.56118733 -0.86215949 -0.83952904 -0.59860057][-0.13552752 -0.13076037 -0.03586125 0.27940571 0.84716094 1.6139935 2.2673926 2.5516577 2.34309 1.7337406 1.0038692 0.2488168 -0.30190116 -0.52004743 -0.47599179][-0.13761419 -0.15717381 -0.12340602 0.12192407 0.65223038 1.3860644 2.0864065 2.5041575 2.443723 1.9623423 1.3171004 0.59540707 -0.0076187477 -0.292489 -0.34143728][-0.14331786 -0.17924434 -0.20334344 -0.094496511 0.26831257 0.87108004 1.5076883 1.9039415 1.9468927 1.6153145 1.0828267 0.46652788 -0.037076533 -0.26718134 -0.30490845][-0.13322386 -0.17272305 -0.23087183 -0.24050996 -0.069725841 0.27046126 0.6987195 1.0094974 1.0780101 0.88082671 0.53651035 0.11462925 -0.20619282 -0.32692438 -0.3115496][-0.12390545 -0.15905572 -0.24194063 -0.33209825 -0.36270225 -0.28962192 -0.089082666 0.10129713 0.22457705 0.17526995 0.014059059 -0.16743976 -0.28137052 -0.31156957 -0.26737043][-0.11868356 -0.13728248 -0.2009754 -0.29679015 -0.40709138 -0.48229575 -0.4873113 -0.43550646 -0.35518795 -0.30610517 -0.28405395 -0.31354302 -0.31754887 -0.26582229 -0.20601782][-0.10981856 -0.11431153 -0.13474905 -0.17934096 -0.27970546 -0.37501669 -0.44688034 -0.47404248 -0.4561919 -0.40534472 -0.37678736 -0.32182595 -0.27038476 -0.21693614 -0.16385499][-0.10482375 -0.10539508 -0.10618854 -0.11194545 -0.14304934 -0.17114148 -0.22857435 -0.25213957 -0.25630069 -0.2128877 -0.21080941 -0.20466001 -0.19672576 -0.1718573 -0.13853541][-0.10175118 -0.10259378 -0.10315225 -0.10159667 -0.10381325 -0.095800661 -0.1093699 -0.13378713 -0.18140803 -0.17597117 -0.19590847 -0.16819164 -0.14896786 -0.1385469 -0.12381516]]...]
INFO - root - 2017-12-06 05:31:07.218508: step 15710, loss = 0.76, batch loss = 0.55 (34.2 examples/sec; 0.234 sec/batch; 20h:36m:35s remains)
INFO - root - 2017-12-06 05:31:09.517915: step 15720, loss = 0.83, batch loss = 0.62 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:59s remains)
INFO - root - 2017-12-06 05:31:11.834487: step 15730, loss = 0.82, batch loss = 0.60 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:48s remains)
INFO - root - 2017-12-06 05:31:14.169667: step 15740, loss = 0.86, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 20h:35m:56s remains)
INFO - root - 2017-12-06 05:31:16.505130: step 15750, loss = 0.90, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:54s remains)
INFO - root - 2017-12-06 05:31:18.802263: step 15760, loss = 0.77, batch loss = 0.56 (35.6 examples/sec; 0.225 sec/batch; 19h:45m:47s remains)
INFO - root - 2017-12-06 05:31:21.100109: step 15770, loss = 0.75, batch loss = 0.54 (34.7 examples/sec; 0.231 sec/batch; 20h:17m:54s remains)
INFO - root - 2017-12-06 05:31:23.380163: step 15780, loss = 0.82, batch loss = 0.61 (35.8 examples/sec; 0.224 sec/batch; 19h:40m:09s remains)
INFO - root - 2017-12-06 05:31:25.698556: step 15790, loss = 0.84, batch loss = 0.63 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:31s remains)
INFO - root - 2017-12-06 05:31:28.036337: step 15800, loss = 0.81, batch loss = 0.59 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:41s remains)
2017-12-06 05:31:30.178353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.60456139 -0.76652378 -0.93278307 -1.0811207 -1.2064617 -1.2417263 -1.2113652 -1.141207 -1.0927916 -0.94252294 -0.85942894 -0.8123529 -0.85896796 -0.99641126 -0.98830289][-0.82649642 -1.0307535 -1.1710522 -1.2408119 -1.2512923 -1.2334983 -1.1671069 -1.0524411 -0.94557554 -0.92505234 -1.0902171 -1.0272052 -1.0329309 -1.022923 -1.0978515][-0.69006842 -0.82718474 -0.88380855 -0.867066 -0.76865667 -0.72277606 -0.67066377 -0.63602537 -0.63434523 -0.68449545 -0.85445106 -1.0864816 -1.3641348 -1.3652512 -1.3753814][-0.40336344 -0.45189753 -0.39946955 -0.28040549 -0.1328101 0.047553115 0.20174661 0.11669832 -0.047292639 -0.28085589 -0.43406823 -0.70202816 -1.0385747 -1.263374 -1.5240772][-0.067200936 0.0080757588 0.18465325 0.42714646 0.66339034 0.80617321 0.92164081 0.9921959 0.94814128 0.57726389 0.42281881 0.087490983 -0.054493766 -0.42422494 -0.78509068][0.3254216 0.47068575 0.69937384 1.080817 1.466334 1.7324029 1.8091352 1.7512583 1.6484447 1.4841671 1.5894715 1.3619262 1.2265388 0.87329966 0.47244039][0.56950057 0.834586 1.1247964 1.4379706 1.7329543 2.0355582 2.1801546 2.1453185 1.9804275 1.81679 1.8855914 2.0481606 2.3848751 2.2782154 2.0809379][0.58162558 0.93587464 1.278309 1.4908698 1.5617434 1.5838784 1.6148568 1.7052776 1.6994174 1.647168 1.5686949 1.7024632 2.020304 2.1956916 2.4063175][0.29617003 0.70563817 1.0338176 1.1916556 1.1875 0.96386582 0.73093671 0.68287086 0.81188172 1.0262046 0.97406727 1.0347415 1.1247321 1.5174172 1.685663][-0.10386322 0.18509224 0.37666011 0.41508058 0.28330269 0.022376917 -0.19498311 -0.39450872 -0.38250121 -0.11863717 0.14216343 0.17454505 0.27736717 0.54691887 0.79392374][-0.20463629 -0.1050158 -0.14462769 -0.27304921 -0.56712997 -0.9316622 -1.1852062 -1.3080333 -1.303465 -1.2585471 -1.1218162 -1.0299275 -1.0663427 -1.0007198 -0.81553745][-0.18468183 -0.23935688 -0.34301141 -0.58470255 -0.95775223 -1.3901829 -1.7733715 -1.9717708 -1.9601063 -1.9832385 -1.9965742 -2.0250015 -2.2777441 -2.3945146 -2.5781314][0.0086359382 -0.15804139 -0.38176623 -0.61735678 -0.89447743 -1.2976952 -1.7286367 -2.0848539 -2.3047132 -2.4748647 -2.5974898 -2.7197866 -2.9911981 -3.2287691 -3.5011377][0.22596872 0.012692757 -0.22785431 -0.54287952 -0.89381045 -1.204482 -1.4481704 -1.7805536 -2.1080751 -2.3976784 -2.6312578 -2.8853824 -3.1936982 -3.5709808 -3.7976336][0.10831038 -0.14585295 -0.36568338 -0.56869709 -0.77772856 -1.0732564 -1.332083 -1.5601232 -1.8221829 -2.1382904 -2.3602884 -2.5700517 -2.9115057 -3.1931531 -3.4008737]]...]
INFO - root - 2017-12-06 05:31:32.463677: step 15810, loss = 0.86, batch loss = 0.65 (36.2 examples/sec; 0.221 sec/batch; 19h:25m:20s remains)
INFO - root - 2017-12-06 05:31:34.819236: step 15820, loss = 0.88, batch loss = 0.67 (34.0 examples/sec; 0.235 sec/batch; 20h:41m:26s remains)
INFO - root - 2017-12-06 05:31:37.095369: step 15830, loss = 0.72, batch loss = 0.51 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:19s remains)
INFO - root - 2017-12-06 05:31:39.452022: step 15840, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.231 sec/batch; 20h:17m:07s remains)
INFO - root - 2017-12-06 05:31:41.770322: step 15850, loss = 0.80, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 20h:16m:46s remains)
INFO - root - 2017-12-06 05:31:44.090190: step 15860, loss = 0.80, batch loss = 0.59 (33.1 examples/sec; 0.242 sec/batch; 21h:16m:48s remains)
INFO - root - 2017-12-06 05:31:46.386486: step 15870, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:59s remains)
INFO - root - 2017-12-06 05:31:48.684195: step 15880, loss = 0.77, batch loss = 0.55 (35.7 examples/sec; 0.224 sec/batch; 19h:42m:15s remains)
INFO - root - 2017-12-06 05:31:51.000524: step 15890, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.232 sec/batch; 20h:26m:18s remains)
INFO - root - 2017-12-06 05:31:53.305700: step 15900, loss = 0.80, batch loss = 0.59 (35.7 examples/sec; 0.224 sec/batch; 19h:43m:27s remains)
2017-12-06 05:31:53.679502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058707226 -0.058537364 -0.058692738 -0.058710571 -0.058736071 -0.058787029 -0.058807015 -0.058855109 -0.058933213 -0.058962442 -0.058918174 -0.058892384 -0.058954291 -0.059035428 -0.059402175][-0.057431083 -0.057102419 -0.057251435 -0.057210077 -0.057220932 -0.05731659 -0.057395041 -0.057564024 -0.057776075 -0.057875764 -0.057872545 -0.057923082 -0.058079291 -0.058253057 -0.058598328][-0.055646952 -0.055010088 -0.054981977 -0.054771692 -0.054719105 -0.054953903 -0.055330727 -0.05582381 -0.056349531 -0.056676988 -0.056809317 -0.056948856 -0.057164289 -0.057409886 -0.057805613][-0.053360198 -0.052199315 -0.051700793 -0.05106822 -0.050865892 -0.051283211 -0.052158706 -0.053273935 -0.054349393 -0.055099517 -0.055460278 -0.055723961 -0.055994872 -0.05631794 -0.056913488][-0.0506521 -0.049190063 -0.048013918 -0.046703864 -0.046079535 -0.046416767 -0.04773359 -0.049657568 -0.051494852 -0.052745555 -0.053487305 -0.054001659 -0.05445664 -0.054931749 -0.055803426][-0.048574556 -0.046551339 -0.044729412 -0.042635351 -0.041242417 -0.0409114 -0.042013332 -0.044435464 -0.047114503 -0.049115706 -0.050403453 -0.051387332 -0.052235231 -0.053064734 -0.0544257][-0.047343332 -0.045010872 -0.042747714 -0.040033247 -0.037818223 -0.036509752 -0.036747046 -0.038900517 -0.042073257 -0.044854593 -0.046619888 -0.047957372 -0.049235757 -0.0505901 -0.052679073][-0.047124572 -0.044920638 -0.042793676 -0.040009513 -0.037413392 -0.035335485 -0.034375694 -0.035360169 -0.037992056 -0.04095494 -0.042968262 -0.044396468 -0.045768447 -0.047551394 -0.050468575][-0.047657561 -0.0461312 -0.044676986 -0.04244718 -0.040176421 -0.037968848 -0.0364002 -0.036168996 -0.037172236 -0.039095011 -0.040656578 -0.041810185 -0.042847428 -0.044570196 -0.047983117][-0.048585325 -0.047924515 -0.047395911 -0.046017602 -0.044495426 -0.04282856 -0.041381147 -0.040627006 -0.040341128 -0.040503703 -0.040740557 -0.040992539 -0.041417755 -0.042679012 -0.046066716][-0.05010229 -0.049861122 -0.050093643 -0.049526885 -0.048792537 -0.047786307 -0.046662387 -0.045675963 -0.04462285 -0.043633997 -0.04266905 -0.041999962 -0.041871328 -0.0427302 -0.045715995][-0.051817141 -0.051737871 -0.052442588 -0.052521896 -0.052321274 -0.0518139 -0.051006645 -0.049908303 -0.04851808 -0.047127802 -0.0457257 -0.044663202 -0.044235229 -0.044812988 -0.047250304][-0.0539548 -0.053689271 -0.054455835 -0.054794513 -0.054864183 -0.054671112 -0.054167967 -0.053238206 -0.051970359 -0.05073021 -0.0495595 -0.048755296 -0.048344567 -0.048635993 -0.050401192][-0.0561623 -0.055591136 -0.056136671 -0.056491178 -0.056638457 -0.05655339 -0.056298036 -0.05570472 -0.054860521 -0.054071251 -0.053323679 -0.052803006 -0.052438539 -0.052598093 -0.053783044][-0.058104981 -0.057371378 -0.057693467 -0.057902627 -0.0579861 -0.058007982 -0.057937313 -0.057614475 -0.057152674 -0.056767493 -0.056368552 -0.056050062 -0.055776704 -0.055836059 -0.056746263]]...]
INFO - root - 2017-12-06 05:31:56.032729: step 15910, loss = 0.84, batch loss = 0.63 (33.5 examples/sec; 0.239 sec/batch; 21h:01m:53s remains)
INFO - root - 2017-12-06 05:31:58.345394: step 15920, loss = 0.77, batch loss = 0.55 (34.6 examples/sec; 0.231 sec/batch; 20h:20m:15s remains)
INFO - root - 2017-12-06 05:32:00.616090: step 15930, loss = 0.77, batch loss = 0.56 (34.7 examples/sec; 0.230 sec/batch; 20h:14m:49s remains)
INFO - root - 2017-12-06 05:32:02.903618: step 15940, loss = 0.74, batch loss = 0.52 (34.8 examples/sec; 0.230 sec/batch; 20h:12m:01s remains)
INFO - root - 2017-12-06 05:32:05.245852: step 15950, loss = 0.76, batch loss = 0.54 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:33s remains)
INFO - root - 2017-12-06 05:32:07.529105: step 15960, loss = 0.73, batch loss = 0.51 (36.4 examples/sec; 0.220 sec/batch; 19h:18m:12s remains)
INFO - root - 2017-12-06 05:32:09.845895: step 15970, loss = 0.83, batch loss = 0.61 (33.8 examples/sec; 0.237 sec/batch; 20h:48m:48s remains)
INFO - root - 2017-12-06 05:32:12.179347: step 15980, loss = 0.78, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 19h:57m:04s remains)
INFO - root - 2017-12-06 05:32:14.509148: step 15990, loss = 0.89, batch loss = 0.68 (32.5 examples/sec; 0.246 sec/batch; 21h:39m:14s remains)
INFO - root - 2017-12-06 05:32:16.832609: step 16000, loss = 0.84, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 19h:55m:42s remains)
2017-12-06 05:32:17.306245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12480935 -0.11610311 -0.11553815 -0.11207984 -0.10927779 -0.1087959 -0.11060975 -0.11363424 -0.11693683 -0.12266514 -0.12540819 -0.12524511 -0.1236309 -0.11435911 -0.10326991][-0.10920755 -0.10533755 -0.10130427 -0.099798933 -0.096543 -0.0935823 -0.092879109 -0.095640719 -0.099492513 -0.10330342 -0.10726088 -0.11043438 -0.11105069 -0.10655615 -0.10011335][-0.096208781 -0.092039876 -0.086105764 -0.080062181 -0.074917629 -0.072781742 -0.070760883 -0.071710356 -0.07410977 -0.078143507 -0.083224826 -0.089887582 -0.09402407 -0.09470702 -0.092400312][-0.081457995 -0.079551786 -0.07688614 -0.071834847 -0.066334032 -0.061530508 -0.060369119 -0.059782546 -0.061601594 -0.06443692 -0.068745814 -0.0753443 -0.080311626 -0.083325274 -0.084251791][-0.074029267 -0.069651544 -0.065400787 -0.063517384 -0.0608526 -0.05662271 -0.054207664 -0.052441273 -0.054419104 -0.056919724 -0.06103538 -0.065554626 -0.069953866 -0.073394574 -0.075294256][-0.07151258 -0.067329846 -0.063349411 -0.059055112 -0.054926366 -0.053146273 -0.052862756 -0.05469751 -0.056855939 -0.05867213 -0.06140846 -0.06440904 -0.06666182 -0.069201283 -0.070297942][-0.071226083 -0.067920178 -0.064969309 -0.06328392 -0.061127417 -0.056555975 -0.053383011 -0.055371244 -0.056856114 -0.056752492 -0.057094503 -0.058860727 -0.061878666 -0.064997658 -0.067144722][-0.068753347 -0.066442914 -0.0647033 -0.06280838 -0.060999602 -0.061058268 -0.060371995 -0.059990816 -0.0599508 -0.059926286 -0.060382683 -0.061125737 -0.062355932 -0.063699245 -0.06593284][-0.069628648 -0.067081116 -0.066015579 -0.065692089 -0.06508328 -0.064342469 -0.063624278 -0.06380178 -0.063212082 -0.063280337 -0.063715987 -0.06444525 -0.065251254 -0.065769948 -0.066243276][-0.074333906 -0.071781129 -0.071065858 -0.071075991 -0.071212046 -0.070731521 -0.070332766 -0.069887072 -0.069163173 -0.06905 -0.069088042 -0.0695327 -0.069571488 -0.069505595 -0.069755606][-0.078281656 -0.076992288 -0.077259459 -0.079177514 -0.080356941 -0.08052773 -0.080016576 -0.078970566 -0.077841096 -0.076969743 -0.076991826 -0.077504836 -0.077305391 -0.076740324 -0.0760816][-0.083986312 -0.083624594 -0.084650181 -0.086396739 -0.087495811 -0.088346362 -0.088287458 -0.087787211 -0.087001167 -0.0861783 -0.086060837 -0.086473368 -0.086568557 -0.086345591 -0.085693114][-0.090119615 -0.091276504 -0.093666352 -0.096073255 -0.098052137 -0.098179415 -0.096962668 -0.095874116 -0.094460368 -0.093721882 -0.093893245 -0.09449178 -0.094771758 -0.095109165 -0.095052965][-0.094680607 -0.098247707 -0.10253286 -0.10589924 -0.10750599 -0.10710119 -0.10524781 -0.10280927 -0.10004737 -0.097177133 -0.0961393 -0.096751057 -0.097587988 -0.098584861 -0.099347666][-0.095825583 -0.10108778 -0.10639762 -0.11038467 -0.1127085 -0.11320809 -0.11139607 -0.10815037 -0.1047057 -0.10231671 -0.10115521 -0.099130929 -0.0986699 -0.1005716 -0.10622961]]...]
INFO - root - 2017-12-06 05:32:19.584234: step 16010, loss = 0.86, batch loss = 0.64 (33.7 examples/sec; 0.237 sec/batch; 20h:52m:41s remains)
INFO - root - 2017-12-06 05:32:21.880814: step 16020, loss = 0.84, batch loss = 0.62 (34.0 examples/sec; 0.235 sec/batch; 20h:41m:16s remains)
INFO - root - 2017-12-06 05:32:24.197162: step 16030, loss = 0.79, batch loss = 0.57 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:22s remains)
INFO - root - 2017-12-06 05:32:26.546979: step 16040, loss = 0.80, batch loss = 0.58 (32.6 examples/sec; 0.245 sec/batch; 21h:33m:13s remains)
INFO - root - 2017-12-06 05:32:28.840334: step 16050, loss = 0.79, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 20h:16m:03s remains)
INFO - root - 2017-12-06 05:32:31.145451: step 16060, loss = 0.80, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:57s remains)
INFO - root - 2017-12-06 05:32:33.438056: step 16070, loss = 0.72, batch loss = 0.50 (34.6 examples/sec; 0.231 sec/batch; 20h:18m:20s remains)
INFO - root - 2017-12-06 05:32:35.774780: step 16080, loss = 0.85, batch loss = 0.64 (32.6 examples/sec; 0.246 sec/batch; 21h:35m:35s remains)
INFO - root - 2017-12-06 05:32:38.072239: step 16090, loss = 0.76, batch loss = 0.54 (34.4 examples/sec; 0.233 sec/batch; 20h:27m:54s remains)
INFO - root - 2017-12-06 05:32:40.366396: step 16100, loss = 0.85, batch loss = 0.63 (34.7 examples/sec; 0.230 sec/batch; 20h:14m:15s remains)
2017-12-06 05:32:40.742944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066512421 -0.0664192 -0.06643495 -0.066429928 -0.066420652 -0.06640856 -0.066391587 -0.06637805 -0.066376157 -0.066375867 -0.066369154 -0.066343248 -0.066300936 -0.066257253 -0.06629321][-0.066573106 -0.066474974 -0.066515207 -0.066501871 -0.066442817 -0.066353783 -0.066254854 -0.066169076 -0.066123106 -0.066119291 -0.066145584 -0.066167466 -0.066160485 -0.066152558 -0.06617482][-0.066719383 -0.06660825 -0.066699073 -0.066682547 -0.066551834 -0.066331923 -0.066074207 -0.065834127 -0.0656947 -0.065645657 -0.065683335 -0.065775931 -0.065841839 -0.0659147 -0.065970317][-0.066868648 -0.066794634 -0.06693013 -0.06691815 -0.066708744 -0.066312864 -0.065833837 -0.065388292 -0.065104537 -0.0649723 -0.064985447 -0.065130852 -0.065303743 -0.065495148 -0.065655492][-0.066998385 -0.067121871 -0.067298569 -0.067252696 -0.066903107 -0.066268869 -0.065521732 -0.064817593 -0.064354658 -0.064131081 -0.0640993 -0.064255781 -0.064554468 -0.064911045 -0.065236419][-0.067455977 -0.067723431 -0.067950115 -0.067855462 -0.067332029 -0.066366464 -0.06524089 -0.064264372 -0.063645825 -0.063348837 -0.063358352 -0.063599318 -0.063985832 -0.064473569 -0.064942382][-0.068154812 -0.068559378 -0.068893924 -0.068776123 -0.068131693 -0.066893876 -0.065363094 -0.064077176 -0.063311219 -0.06291528 -0.0629269 -0.06327109 -0.063695133 -0.064202622 -0.064727277][-0.068593055 -0.069097623 -0.069606006 -0.069593653 -0.068962738 -0.067681223 -0.065972991 -0.064370275 -0.063401252 -0.0629083 -0.062849253 -0.063145988 -0.063569918 -0.064048558 -0.064569607][-0.068655029 -0.069197029 -0.069824547 -0.069969617 -0.069503807 -0.068417646 -0.066894144 -0.065317065 -0.06416662 -0.063525841 -0.063312307 -0.063429311 -0.063788369 -0.064232387 -0.064695][-0.068427339 -0.068894185 -0.069518991 -0.0697562 -0.0694619 -0.068595365 -0.067370586 -0.066067263 -0.065029 -0.064369068 -0.064086668 -0.064085908 -0.06432455 -0.06468109 -0.065030023][-0.067864895 -0.068095818 -0.06861642 -0.068872184 -0.068754509 -0.068188 -0.06729833 -0.066333376 -0.065547839 -0.064984143 -0.064746611 -0.064756617 -0.064904444 -0.065151721 -0.065401591][-0.067089036 -0.067060068 -0.067422219 -0.067632742 -0.067653805 -0.067429595 -0.066941358 -0.066324815 -0.06580095 -0.065406688 -0.06519375 -0.065163881 -0.065261438 -0.065436907 -0.065625481][-0.066506304 -0.066276453 -0.0664643 -0.0665987 -0.06665156 -0.066621512 -0.06646198 -0.066185139 -0.065897822 -0.0656705 -0.065529265 -0.065475121 -0.06551224 -0.06562303 -0.065770611][-0.066328138 -0.065972239 -0.066018954 -0.06607563 -0.066122547 -0.066145778 -0.06612812 -0.0660602 -0.065968335 -0.065879181 -0.065819889 -0.0657839 -0.065796025 -0.065848276 -0.065925285][-0.066402555 -0.066033393 -0.066031061 -0.066038512 -0.066052191 -0.066065058 -0.066078641 -0.06605944 -0.066049561 -0.066032931 -0.06602332 -0.066013888 -0.06601762 -0.06603156 -0.066055089]]...]
INFO - root - 2017-12-06 05:32:43.087411: step 16110, loss = 0.82, batch loss = 0.60 (34.4 examples/sec; 0.233 sec/batch; 20h:27m:05s remains)
INFO - root - 2017-12-06 05:32:45.408043: step 16120, loss = 0.80, batch loss = 0.58 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:54s remains)
INFO - root - 2017-12-06 05:32:47.728906: step 16130, loss = 0.84, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 20h:08m:13s remains)
INFO - root - 2017-12-06 05:32:50.025203: step 16140, loss = 0.87, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 20h:11m:24s remains)
INFO - root - 2017-12-06 05:32:52.349892: step 16150, loss = 0.88, batch loss = 0.66 (34.7 examples/sec; 0.231 sec/batch; 20h:17m:09s remains)
INFO - root - 2017-12-06 05:32:54.694906: step 16160, loss = 0.75, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:31m:57s remains)
INFO - root - 2017-12-06 05:32:57.000062: step 16170, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:27s remains)
INFO - root - 2017-12-06 05:32:59.299689: step 16180, loss = 0.79, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:43s remains)
INFO - root - 2017-12-06 05:33:01.589514: step 16190, loss = 0.78, batch loss = 0.56 (35.5 examples/sec; 0.225 sec/batch; 19h:48m:08s remains)
INFO - root - 2017-12-06 05:33:03.890588: step 16200, loss = 0.80, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:11m:00s remains)
2017-12-06 05:33:05.841218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.5859459 -0.63931113 -0.66969258 -0.72400516 -0.79985052 -0.87273854 -0.94572967 -0.99843192 -1.0161818 -1.0076823 -0.99517047 -0.95248848 -0.88356531 -0.8291989 -0.76451921][-1.1135548 -1.2329452 -1.2741103 -1.2624912 -1.217742 -1.1581633 -1.0979669 -1.0494362 -1.0593776 -1.0671639 -1.0828511 -1.1111314 -1.110828 -1.0513308 -0.93277484][-1.3396842 -1.5124837 -1.5328928 -1.4154468 -1.2071832 -0.95174766 -0.71735 -0.55874109 -0.51499796 -0.61794883 -0.81903923 -0.94651067 -1.0361184 -1.107224 -1.0915339][-1.1400404 -1.2984493 -1.2716765 -1.0303228 -0.62504226 -0.14633976 0.28384197 0.56787688 0.64987338 0.49729985 0.16217788 -0.21001205 -0.54024065 -0.72276336 -0.88802034][-0.65123832 -0.6751436 -0.53191322 -0.21071455 0.308555 0.972488 1.607911 2.0124874 2.1052666 1.9065517 1.4971894 0.9677766 0.44685256 0.060774639 -0.24470161][0.054329693 0.12745269 0.36904961 0.75780571 1.2906847 1.9339492 2.5652244 3.013756 3.1965764 3.0319026 2.6107843 2.1261549 1.6512729 1.1171962 0.62802935][0.44783372 0.64038128 0.912206 1.2890275 1.7631493 2.2764528 2.7723434 3.1373081 3.2677662 3.192991 2.8845539 2.4418325 2.0651321 1.683285 1.4104508][0.34401971 0.53157121 0.734481 0.97430617 1.27416 1.6088455 1.9251773 2.1458614 2.2153897 2.113801 1.9229248 1.7412579 1.5425493 1.2727711 1.228017][0.0079345852 0.056457773 0.081290573 0.1303474 0.21496694 0.31565875 0.42800969 0.5583849 0.63575649 0.60120749 0.50268888 0.47072965 0.51299924 0.54456389 0.61013973][-0.46093488 -0.52288157 -0.62517828 -0.76797086 -0.94829547 -1.0849018 -1.1080508 -0.98213482 -0.98037678 -1.0252043 -0.95743549 -0.84807473 -0.63910574 -0.38169277 -0.17393443][-0.72503239 -0.96426105 -1.1899463 -1.4483472 -1.6264583 -1.8105551 -1.9359273 -1.9562995 -1.9281318 -1.8533018 -1.7467397 -1.7309707 -1.5920285 -1.2735938 -0.92676663][-0.62900257 -0.95631415 -1.3075801 -1.663146 -1.9199744 -2.1695807 -2.2506251 -2.0994077 -2.02358 -2.0498266 -1.9583989 -1.9201505 -1.854455 -1.5623916 -1.2493352][-0.44797021 -0.76683217 -1.0893664 -1.4320982 -1.7166649 -2.0381389 -2.2232296 -2.1633146 -2.0545158 -1.9730818 -1.774421 -1.7154919 -1.7208056 -1.490882 -1.1394675][-0.26223958 -0.50007564 -0.76386404 -1.0305012 -1.2481058 -1.5092279 -1.7484735 -1.7644671 -1.7231193 -1.7186998 -1.6330311 -1.4519813 -1.2702868 -1.0063922 -0.76177204][-0.20501386 -0.34620255 -0.53065634 -0.69959962 -0.85871696 -1.0413212 -1.2128855 -1.2275678 -1.2524738 -1.3067058 -1.2829094 -1.1259885 -0.86593735 -0.64939052 -0.42843568]]...]
INFO - root - 2017-12-06 05:33:08.153388: step 16210, loss = 0.83, batch loss = 0.62 (36.6 examples/sec; 0.218 sec/batch; 19h:11m:40s remains)
INFO - root - 2017-12-06 05:33:10.494748: step 16220, loss = 0.79, batch loss = 0.58 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:11s remains)
INFO - root - 2017-12-06 05:33:12.778544: step 16230, loss = 0.76, batch loss = 0.54 (33.5 examples/sec; 0.239 sec/batch; 20h:59m:03s remains)
INFO - root - 2017-12-06 05:33:15.069690: step 16240, loss = 0.88, batch loss = 0.67 (34.7 examples/sec; 0.231 sec/batch; 20h:15m:06s remains)
INFO - root - 2017-12-06 05:33:17.370084: step 16250, loss = 0.81, batch loss = 0.59 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-06 05:33:19.697192: step 16260, loss = 0.77, batch loss = 0.56 (34.0 examples/sec; 0.235 sec/batch; 20h:40m:00s remains)
INFO - root - 2017-12-06 05:33:22.006385: step 16270, loss = 0.82, batch loss = 0.60 (35.3 examples/sec; 0.226 sec/batch; 19h:53m:20s remains)
INFO - root - 2017-12-06 05:33:24.348267: step 16280, loss = 0.80, batch loss = 0.58 (35.9 examples/sec; 0.223 sec/batch; 19h:33m:12s remains)
INFO - root - 2017-12-06 05:33:26.654483: step 16290, loss = 0.91, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:33s remains)
INFO - root - 2017-12-06 05:33:28.928943: step 16300, loss = 0.77, batch loss = 0.55 (36.5 examples/sec; 0.219 sec/batch; 19h:16m:21s remains)
2017-12-06 05:33:29.296542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.075978905 -0.076338351 -0.076910853 -0.077494249 -0.078204222 -0.079187833 -0.08045131 -0.081775062 -0.082665682 -0.082615472 -0.081280492 -0.078671828 -0.075237826 -0.071519643 -0.068304859][-0.075806089 -0.076042026 -0.076412573 -0.076706424 -0.077070482 -0.077710971 -0.078649685 -0.079700232 -0.080447286 -0.080443323 -0.0793006 -0.07700149 -0.073943123 -0.070588484 -0.067666136][-0.075521767 -0.075432524 -0.075395368 -0.075133607 -0.074811362 -0.074729726 -0.074987017 -0.075460315 -0.075852767 -0.075848848 -0.075063989 -0.073426761 -0.071149759 -0.0685902 -0.066329271][-0.0754637 -0.074884951 -0.074179634 -0.073064357 -0.071741067 -0.070602246 -0.0698202 -0.0694028 -0.0692695 -0.069222853 -0.068950653 -0.068260714 -0.06715028 -0.065779105 -0.064590022][-0.075838111 -0.074759685 -0.073304206 -0.071230456 -0.068766691 -0.066426463 -0.064491034 -0.063023396 -0.062199507 -0.062042363 -0.062262934 -0.06251879 -0.062667124 -0.062664218 -0.062683165][-0.07670968 -0.075215884 -0.073136449 -0.070211679 -0.066739351 -0.063295633 -0.060282469 -0.0578658 -0.056395996 -0.056082658 -0.056647703 -0.057682108 -0.05881644 -0.059903733 -0.060955871][-0.078219205 -0.076524377 -0.074142881 -0.070773154 -0.066731937 -0.062614933 -0.05890242 -0.055908985 -0.053967763 -0.053468026 -0.054101735 -0.055390723 -0.056867465 -0.058386225 -0.059923381][-0.080271177 -0.078708038 -0.076472692 -0.073215216 -0.069255412 -0.065148667 -0.061410822 -0.058257136 -0.056053732 -0.05521892 -0.055481359 -0.056342054 -0.057397284 -0.058620885 -0.059963468][-0.0823004 -0.08116585 -0.079437472 -0.076816671 -0.073575951 -0.070188135 -0.06705261 -0.0642073 -0.062012538 -0.060777795 -0.06024266 -0.060086392 -0.060108729 -0.0604161 -0.061001502][-0.083665423 -0.083053194 -0.082056008 -0.08039853 -0.078334749 -0.076206952 -0.07419318 -0.072134681 -0.070250131 -0.068716131 -0.067251213 -0.065818124 -0.064399414 -0.0632994 -0.062695496][-0.084116705 -0.083990194 -0.083735108 -0.083115 -0.082294263 -0.081543878 -0.080822974 -0.079883836 -0.078615874 -0.077008992 -0.07480973 -0.072192483 -0.069339834 -0.066737168 -0.064782284][-0.083797649 -0.08406917 -0.084466241 -0.084750578 -0.085063756 -0.08555229 -0.085982025 -0.086042859 -0.085448407 -0.083969638 -0.081321336 -0.077808544 -0.073822677 -0.069964424 -0.066807359][-0.083500624 -0.084011987 -0.084891796 -0.085871965 -0.087030046 -0.088407554 -0.089695096 -0.090502046 -0.090411551 -0.08904168 -0.0861077 -0.081986748 -0.077198483 -0.072439514 -0.068423256][-0.083896041 -0.08466132 -0.086023092 -0.08759249 -0.089379594 -0.091272376 -0.0929654 -0.093980342 -0.093913227 -0.092429377 -0.089228056 -0.08472389 -0.079520166 -0.0742763 -0.069775492][-0.085649356 -0.0867472 -0.08856988 -0.090661496 -0.092828162 -0.094875917 -0.096567482 -0.097393386 -0.096889257 -0.094873533 -0.091233023 -0.086368077 -0.0808445 -0.075328164 -0.070608057]]...]
INFO - root - 2017-12-06 05:33:31.622340: step 16310, loss = 0.82, batch loss = 0.61 (36.1 examples/sec; 0.222 sec/batch; 19h:28m:27s remains)
INFO - root - 2017-12-06 05:33:33.890559: step 16320, loss = 0.79, batch loss = 0.57 (33.7 examples/sec; 0.237 sec/batch; 20h:50m:22s remains)
INFO - root - 2017-12-06 05:33:36.165070: step 16330, loss = 0.75, batch loss = 0.54 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:06s remains)
INFO - root - 2017-12-06 05:33:38.442999: step 16340, loss = 0.80, batch loss = 0.59 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:10s remains)
INFO - root - 2017-12-06 05:33:40.805827: step 16350, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 20h:16m:27s remains)
INFO - root - 2017-12-06 05:33:43.075543: step 16360, loss = 0.80, batch loss = 0.58 (35.7 examples/sec; 0.224 sec/batch; 19h:39m:28s remains)
INFO - root - 2017-12-06 05:33:45.397732: step 16370, loss = 0.83, batch loss = 0.62 (34.1 examples/sec; 0.235 sec/batch; 20h:36m:52s remains)
INFO - root - 2017-12-06 05:33:47.705734: step 16380, loss = 0.78, batch loss = 0.56 (34.2 examples/sec; 0.234 sec/batch; 20h:32m:59s remains)
INFO - root - 2017-12-06 05:33:49.972420: step 16390, loss = 0.81, batch loss = 0.60 (35.1 examples/sec; 0.228 sec/batch; 20h:01m:17s remains)
INFO - root - 2017-12-06 05:33:52.359742: step 16400, loss = 0.84, batch loss = 0.63 (35.0 examples/sec; 0.229 sec/batch; 20h:05m:18s remains)
2017-12-06 05:33:52.730941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049447078 -0.0500493 -0.050781518 -0.051215149 -0.051076088 -0.050299697 -0.049611259 -0.049754791 -0.05100669 -0.05329119 -0.056376081 -0.059690118 -0.062537618 -0.064665824 -0.066351689][-0.050256107 -0.049991418 -0.049893625 -0.049665332 -0.049109276 -0.047986083 -0.047058091 -0.047097869 -0.048426017 -0.050993163 -0.054477528 -0.058235958 -0.061534878 -0.064077646 -0.066077173][-0.052071773 -0.050704535 -0.049649879 -0.048767798 -0.047847465 -0.046529412 -0.045409787 -0.045400888 -0.046843067 -0.049522657 -0.053115528 -0.057116218 -0.060720738 -0.063594393 -0.06583719][-0.054101251 -0.051643327 -0.049669985 -0.048241988 -0.046993468 -0.045529313 -0.044224311 -0.044242024 -0.04592083 -0.048729524 -0.052386634 -0.056480598 -0.060275104 -0.063331619 -0.065705076][-0.05607418 -0.052665509 -0.049870446 -0.047939897 -0.046371233 -0.044797305 -0.043470621 -0.043593232 -0.045515563 -0.048533306 -0.052258447 -0.056312807 -0.06016412 -0.063313782 -0.065745309][-0.058092665 -0.053826213 -0.050324962 -0.047911461 -0.0460489 -0.044299081 -0.043056212 -0.043468207 -0.045682963 -0.048818611 -0.052555054 -0.056509234 -0.060271204 -0.063440338 -0.065849662][-0.059868723 -0.054898135 -0.050665863 -0.047743622 -0.045680568 -0.04392492 -0.042905115 -0.043689195 -0.046226088 -0.0494386 -0.053132392 -0.056970414 -0.060586493 -0.063663125 -0.065952681][-0.060980223 -0.055527378 -0.050833557 -0.047488622 -0.045282397 -0.043693449 -0.043054465 -0.044159565 -0.046839897 -0.050117429 -0.053798545 -0.057491612 -0.060962304 -0.063938767 -0.066134423][-0.061263233 -0.055677857 -0.050827712 -0.047314815 -0.045105267 -0.043765295 -0.043593455 -0.044940446 -0.047580674 -0.050835565 -0.054448526 -0.057995915 -0.061337486 -0.064211726 -0.066304579][-0.060878456 -0.055643905 -0.05112309 -0.047760323 -0.045630064 -0.044445407 -0.044556744 -0.046028696 -0.048474923 -0.051532861 -0.054956954 -0.058372043 -0.061604306 -0.064377189 -0.066393174][-0.059931561 -0.055341352 -0.051447768 -0.048513267 -0.046619225 -0.045523185 -0.04574817 -0.047267471 -0.049544092 -0.052362755 -0.055558983 -0.05883316 -0.061898842 -0.064515308 -0.066416][-0.058438767 -0.054690395 -0.051670704 -0.049368232 -0.047869477 -0.047042254 -0.047320124 -0.048809852 -0.050909162 -0.053478636 -0.056443971 -0.059500314 -0.062345624 -0.064749077 -0.066518351][-0.057077866 -0.054232974 -0.052076411 -0.050427146 -0.04932718 -0.048778709 -0.049093377 -0.050451998 -0.052367732 -0.054727286 -0.057450738 -0.06022872 -0.062808722 -0.064950235 -0.066551425][-0.056309875 -0.054315683 -0.052880827 -0.051798057 -0.051026825 -0.050665535 -0.051029138 -0.052230034 -0.053901941 -0.056019921 -0.058477376 -0.06096242 -0.063246511 -0.065136135 -0.066544428][-0.05639714 -0.055054631 -0.05423801 -0.053614154 -0.053144448 -0.052936096 -0.053271241 -0.05427409 -0.055653375 -0.057477724 -0.059602872 -0.06179902 -0.063806906 -0.065437004 -0.066655934]]...]
INFO - root - 2017-12-06 05:33:55.027322: step 16410, loss = 0.75, batch loss = 0.53 (35.9 examples/sec; 0.223 sec/batch; 19h:33m:00s remains)
INFO - root - 2017-12-06 05:33:57.309807: step 16420, loss = 0.79, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 19h:59m:25s remains)
INFO - root - 2017-12-06 05:33:59.638961: step 16430, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:10s remains)
INFO - root - 2017-12-06 05:34:01.974262: step 16440, loss = 0.73, batch loss = 0.52 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-06 05:34:04.256352: step 16450, loss = 0.83, batch loss = 0.61 (32.9 examples/sec; 0.243 sec/batch; 21h:20m:15s remains)
INFO - root - 2017-12-06 05:34:06.603237: step 16460, loss = 0.84, batch loss = 0.63 (31.8 examples/sec; 0.251 sec/batch; 22h:04m:33s remains)
INFO - root - 2017-12-06 05:34:08.948495: step 16470, loss = 0.82, batch loss = 0.60 (32.6 examples/sec; 0.246 sec/batch; 21h:34m:26s remains)
INFO - root - 2017-12-06 05:34:11.301104: step 16480, loss = 0.79, batch loss = 0.57 (33.3 examples/sec; 0.240 sec/batch; 21h:06m:06s remains)
INFO - root - 2017-12-06 05:34:13.632144: step 16490, loss = 0.79, batch loss = 0.57 (34.0 examples/sec; 0.235 sec/batch; 20h:38m:44s remains)
INFO - root - 2017-12-06 05:34:15.925492: step 16500, loss = 0.80, batch loss = 0.58 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:27s remains)
2017-12-06 05:34:16.274373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18801326 -0.34840068 -0.35947731 -0.8078993 -1.4385748 -1.9880899 -2.4380002 -2.4716785 -2.7194629 -2.7612159 -2.3679807 -1.2366263 -0.341373 0.042265661 0.3918055][0.34602231 0.061565585 -0.084889837 -0.402518 -0.59768575 -1.3722475 -2.0896654 -2.2128534 -2.1250837 -1.946074 -2.0175524 -1.5623403 -0.45467874 0.179477 0.53679788][0.12694171 0.23747379 0.3523373 -0.010811321 -0.24070957 -0.70312268 -1.1044775 -1.491302 -2.0686285 -2.2090797 -1.8241765 -1.279739 -0.58610225 0.0067876726 0.13833311][0.42701229 0.20992392 0.23742583 0.26704365 0.23764217 -0.15842646 -0.59529376 -1.0625002 -1.1902324 -1.3234053 -1.6844691 -1.7807654 -1.0771317 -0.50145787 -0.735432][0.214151 0.51919109 0.72872615 0.38778821 0.1864599 0.032068014 -0.15828401 -0.65682125 -1.0360458 -1.3201617 -1.3838621 -1.4925417 -1.227035 -1.0669407 -1.0045646][1.0946141 1.0300245 0.88249606 1.0423619 1.0022917 0.71279 0.7438696 0.54587924 0.18800381 -0.014202148 -0.48927054 -0.67875934 -0.8430835 -1.0057541 -1.5290942][1.7581283 1.7372975 1.4525349 1.1059012 0.91655624 1.1589115 1.4859676 1.5412464 1.3872985 1.4480177 1.6419131 1.357486 0.47672823 0.62146676 0.21968159][2.3203914 1.8977042 1.5792865 1.4782078 1.3685482 1.2134589 1.160182 1.5326576 1.6452531 1.7697424 2.4964671 2.8627381 2.8950388 2.4267454 1.461074][1.2423539 1.2608263 1.3776962 1.3196825 1.2864994 1.413938 1.2618073 1.090513 1.2064579 1.5191185 2.2063191 2.918066 3.0798163 3.0058935 2.3712368][-0.0070039034 -0.42857155 -0.56551957 -0.23166856 -0.011434413 -0.073202483 -0.15731955 -0.096400671 -0.068470247 0.15116441 0.95278549 1.8196377 2.5023451 2.3991797 1.9745145][-1.4174212 -1.6271918 -1.6972481 -1.8748572 -2.0186114 -2.1219521 -2.1560326 -2.0193756 -1.8947413 -2.0263584 -1.406113 -0.47874752 0.35165185 0.93417943 0.92294872][-1.1930345 -1.8403895 -2.1609957 -2.161849 -2.1643982 -2.2857766 -2.3935809 -2.5723565 -2.3775878 -2.3622911 -2.0214212 -1.5009226 -1.1466658 -0.92140692 -0.66669309][-0.99876332 -1.1610507 -1.2044451 -1.6078196 -2.4309285 -2.3657804 -2.3288381 -2.4936838 -2.3976338 -2.1286561 -1.6628876 -1.2903639 -0.87020165 -0.73680693 -0.749032][-0.023948848 -0.29194814 -0.59214091 -0.6102801 -1.0522522 -1.7261325 -2.2582345 -2.2127411 -1.8178073 -1.593493 -1.2369639 -1.2995243 -1.4188626 -1.2894925 -1.3537343][1.0541824 0.88625908 0.97688806 0.64725006 -0.3527675 -0.5559839 -0.65190476 -1.1946387 -1.4078972 -1.0837505 -0.955548 -1.0870036 -1.3879496 -1.7201059 -1.8223616]]...]
INFO - root - 2017-12-06 05:34:18.571867: step 16510, loss = 0.77, batch loss = 0.56 (35.0 examples/sec; 0.228 sec/batch; 20h:02m:09s remains)
INFO - root - 2017-12-06 05:34:20.868078: step 16520, loss = 0.84, batch loss = 0.62 (34.4 examples/sec; 0.232 sec/batch; 20h:23m:03s remains)
INFO - root - 2017-12-06 05:34:23.183352: step 16530, loss = 0.81, batch loss = 0.60 (32.2 examples/sec; 0.249 sec/batch; 21h:49m:37s remains)
INFO - root - 2017-12-06 05:34:25.538350: step 16540, loss = 0.71, batch loss = 0.50 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:52s remains)
INFO - root - 2017-12-06 05:34:27.829416: step 16550, loss = 0.86, batch loss = 0.64 (35.0 examples/sec; 0.228 sec/batch; 20h:03m:07s remains)
INFO - root - 2017-12-06 05:34:30.150141: step 16560, loss = 0.79, batch loss = 0.57 (33.9 examples/sec; 0.236 sec/batch; 20h:41m:25s remains)
INFO - root - 2017-12-06 05:34:32.437691: step 16570, loss = 0.86, batch loss = 0.64 (36.2 examples/sec; 0.221 sec/batch; 19h:24m:50s remains)
INFO - root - 2017-12-06 05:34:34.726080: step 16580, loss = 0.82, batch loss = 0.60 (35.8 examples/sec; 0.224 sec/batch; 19h:37m:02s remains)
INFO - root - 2017-12-06 05:34:36.996133: step 16590, loss = 0.77, batch loss = 0.56 (36.4 examples/sec; 0.220 sec/batch; 19h:18m:19s remains)
INFO - root - 2017-12-06 05:34:39.347550: step 16600, loss = 0.75, batch loss = 0.53 (34.5 examples/sec; 0.232 sec/batch; 20h:20m:57s remains)
2017-12-06 05:34:39.771118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053553451 -0.052689631 -0.052031461 -0.051282756 -0.050398998 -0.049119692 -0.047689267 -0.046393123 -0.045742478 -0.045912463 -0.046818562 -0.048468266 -0.050576229 -0.05263168 -0.05450825][-0.051585935 -0.050729 -0.050181225 -0.049731381 -0.049349912 -0.048570782 -0.04747887 -0.046320017 -0.045452129 -0.045358248 -0.045761608 -0.047027111 -0.0491394 -0.051405322 -0.053691253][-0.0490003 -0.048252873 -0.04823067 -0.048622061 -0.049143322 -0.049674775 -0.049715333 -0.049499281 -0.048969958 -0.048485111 -0.047966152 -0.048061091 -0.049491275 -0.051277548 -0.05326378][-0.045817986 -0.04543718 -0.0462567 -0.047496296 -0.049012508 -0.051020756 -0.05254541 -0.05359982 -0.053825002 -0.053433415 -0.052518517 -0.051682673 -0.052008018 -0.052711118 -0.053779479][-0.041770965 -0.042041682 -0.043569263 -0.045606021 -0.048110291 -0.05166978 -0.055169851 -0.057975087 -0.059055924 -0.058873314 -0.057912339 -0.056665506 -0.055980541 -0.055121887 -0.054959629][-0.038273647 -0.038640682 -0.040472806 -0.042855781 -0.045979045 -0.051081426 -0.056703977 -0.061215311 -0.062911481 -0.062898174 -0.061964821 -0.06071496 -0.059654728 -0.057836294 -0.056599464][-0.036407039 -0.036604606 -0.038353357 -0.040552523 -0.043971807 -0.050337918 -0.057621453 -0.063095935 -0.06512142 -0.0650152 -0.064110458 -0.063043669 -0.062312126 -0.059953671 -0.057940628][-0.037044674 -0.036778465 -0.038157526 -0.039862197 -0.043043483 -0.049755678 -0.057849534 -0.0636308 -0.065539353 -0.065127373 -0.064311378 -0.063748144 -0.063531771 -0.061339747 -0.059125639][-0.039393008 -0.039014421 -0.040230282 -0.041584149 -0.044540256 -0.050814517 -0.058428288 -0.063438751 -0.064811647 -0.064192392 -0.06343928 -0.063294306 -0.06316556 -0.061280653 -0.059513472][-0.04293523 -0.042556372 -0.04362604 -0.044992518 -0.047911473 -0.053295396 -0.059296723 -0.063019805 -0.063785456 -0.062933356 -0.062311038 -0.062440012 -0.0621664 -0.060431659 -0.059186149][-0.047079172 -0.046471182 -0.047309749 -0.048539545 -0.050714023 -0.054533061 -0.05849009 -0.061027743 -0.061554208 -0.060983464 -0.060768656 -0.060880251 -0.060691118 -0.059524234 -0.05869931][-0.051000621 -0.050291419 -0.050803624 -0.05163968 -0.052945815 -0.055002846 -0.057255737 -0.058627822 -0.05898571 -0.0588378 -0.058801007 -0.058881976 -0.058841363 -0.058471378 -0.057972074][-0.054096855 -0.053315595 -0.053587224 -0.054023292 -0.054636784 -0.055457119 -0.05650273 -0.057313513 -0.057621356 -0.057641212 -0.05759424 -0.057577137 -0.057409164 -0.057439741 -0.057378605][-0.056217283 -0.055366252 -0.055416111 -0.055539228 -0.055699937 -0.055903271 -0.056213308 -0.056506474 -0.056648407 -0.056681212 -0.056637004 -0.056626659 -0.056553129 -0.056870066 -0.057091769][-0.057504904 -0.056725163 -0.056746334 -0.056744017 -0.056799229 -0.056802783 -0.056846373 -0.056821264 -0.056852598 -0.056785833 -0.056646 -0.056558795 -0.056453303 -0.056781247 -0.05714396]]...]
INFO - root - 2017-12-06 05:34:42.104967: step 16610, loss = 0.77, batch loss = 0.56 (34.7 examples/sec; 0.231 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-06 05:34:44.390443: step 16620, loss = 0.78, batch loss = 0.57 (35.0 examples/sec; 0.228 sec/batch; 20h:02m:16s remains)
INFO - root - 2017-12-06 05:34:46.706282: step 16630, loss = 0.79, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:44s remains)
INFO - root - 2017-12-06 05:34:49.031759: step 16640, loss = 0.85, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:02s remains)
INFO - root - 2017-12-06 05:34:51.337325: step 16650, loss = 0.87, batch loss = 0.66 (36.4 examples/sec; 0.220 sec/batch; 19h:17m:42s remains)
INFO - root - 2017-12-06 05:34:53.641624: step 16660, loss = 0.87, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:09s remains)
INFO - root - 2017-12-06 05:34:55.918596: step 16670, loss = 0.78, batch loss = 0.57 (36.9 examples/sec; 0.217 sec/batch; 19h:02m:39s remains)
INFO - root - 2017-12-06 05:34:58.211922: step 16680, loss = 0.77, batch loss = 0.56 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:07s remains)
INFO - root - 2017-12-06 05:35:00.500634: step 16690, loss = 0.88, batch loss = 0.66 (35.3 examples/sec; 0.226 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-06 05:35:02.801240: step 16700, loss = 0.80, batch loss = 0.58 (33.0 examples/sec; 0.243 sec/batch; 21h:16m:31s remains)
2017-12-06 05:35:03.171349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12621243 -0.14110829 -0.15635258 -0.17244864 -0.19892043 -0.25446734 -0.33508581 -0.38153046 -0.36305511 -0.29702273 -0.20821121 -0.13347659 -0.09478201 -0.087776683 -0.0926566][-0.14257339 -0.17532659 -0.21380296 -0.26283503 -0.33012977 -0.39743036 -0.46673197 -0.54456919 -0.59307235 -0.56203747 -0.45342505 -0.32069272 -0.20428824 -0.13760091 -0.11040831][-0.18599188 -0.26306826 -0.33538228 -0.40150267 -0.47006094 -0.53931314 -0.61658514 -0.69093347 -0.74993312 -0.77250928 -0.71671051 -0.58370119 -0.42591268 -0.30496776 -0.22632225][-0.23793095 -0.36394346 -0.47373378 -0.54895186 -0.59108418 -0.62435621 -0.67207634 -0.738433 -0.80547708 -0.81276041 -0.77772182 -0.70812565 -0.6158511 -0.52476782 -0.43691415][-0.19457063 -0.27016544 -0.29857275 -0.2398074 -0.1259291 -0.027476676 0.0049647391 -0.053493425 -0.19313803 -0.32719493 -0.4333877 -0.51479018 -0.5738923 -0.64570361 -0.707075][-0.090105884 -0.051108092 0.08283405 0.32958418 0.65741718 0.9792974 1.1854476 1.2048171 1.0230154 0.77337736 0.48073149 0.25051945 0.04971467 -0.19383505 -0.43163389][0.00010682642 0.13105719 0.35891694 0.71231228 1.1722177 1.6598992 2.0577495 2.25536 2.23137 2.1264029 1.9204246 1.7167969 1.4615135 1.181283 0.766104][0.070266709 0.21395956 0.43383479 0.75406015 1.1622297 1.5921651 2.0439215 2.4278469 2.6760967 2.7985189 2.8511147 2.852145 2.7448368 2.4563198 2.0051713][0.033506 0.16363697 0.33302522 0.5422281 0.81188416 1.1311694 1.4950961 1.8326062 2.1668668 2.4963813 2.8294661 3.1321788 3.2630255 3.0605371 2.6684947][-0.17896727 -0.13072982 -0.060285993 0.031948388 0.15212326 0.29700303 0.51382959 0.74857944 1.0472457 1.4040725 1.8796461 2.3476763 2.711271 2.7417934 2.4842725][-0.40654421 -0.4387306 -0.4303959 -0.40914953 -0.38527435 -0.37966579 -0.32721776 -0.24745886 -0.046787538 0.19071485 0.58252788 1.0121406 1.4044302 1.5415456 1.4169112][-0.57540947 -0.68840438 -0.74194723 -0.75105655 -0.73523206 -0.76233232 -0.81539649 -0.878706 -0.82791638 -0.74480695 -0.54710925 -0.29008043 -0.016033977 0.12149796 0.070082352][-0.64336008 -0.82892662 -0.97125334 -1.0311927 -1.0246896 -1.0398636 -1.0582399 -1.1282252 -1.1809363 -1.2247642 -1.1937723 -1.0875691 -0.98181337 -0.82640314 -0.78064907][-0.55201554 -0.75593656 -0.96218151 -1.1294223 -1.2410059 -1.3217289 -1.3322803 -1.3149186 -1.32403 -1.3961006 -1.452755 -1.4560347 -1.4120057 -1.235543 -1.0973547][-0.39162266 -0.549708 -0.72007132 -0.89267117 -1.0446782 -1.1794713 -1.2767776 -1.332325 -1.3757147 -1.4062883 -1.4355475 -1.370434 -1.2357261 -1.055421 -0.90630633]]...]
INFO - root - 2017-12-06 05:35:05.438379: step 16710, loss = 0.80, batch loss = 0.59 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-06 05:35:07.751210: step 16720, loss = 0.78, batch loss = 0.57 (33.2 examples/sec; 0.241 sec/batch; 21h:06m:17s remains)
INFO - root - 2017-12-06 05:35:10.113194: step 16730, loss = 0.82, batch loss = 0.60 (30.8 examples/sec; 0.260 sec/batch; 22h:47m:56s remains)
INFO - root - 2017-12-06 05:35:12.406961: step 16740, loss = 0.79, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 20h:14m:32s remains)
INFO - root - 2017-12-06 05:35:14.675851: step 16750, loss = 0.81, batch loss = 0.60 (32.6 examples/sec; 0.245 sec/batch; 21h:30m:43s remains)
INFO - root - 2017-12-06 05:35:16.941282: step 16760, loss = 0.75, batch loss = 0.53 (34.7 examples/sec; 0.230 sec/batch; 20h:12m:34s remains)
INFO - root - 2017-12-06 05:35:19.225982: step 16770, loss = 0.83, batch loss = 0.62 (34.0 examples/sec; 0.235 sec/batch; 20h:37m:51s remains)
INFO - root - 2017-12-06 05:35:21.542548: step 16780, loss = 0.87, batch loss = 0.65 (33.7 examples/sec; 0.237 sec/batch; 20h:49m:42s remains)
INFO - root - 2017-12-06 05:35:23.797424: step 16790, loss = 0.79, batch loss = 0.57 (35.5 examples/sec; 0.225 sec/batch; 19h:46m:10s remains)
INFO - root - 2017-12-06 05:35:26.093759: step 16800, loss = 0.73, batch loss = 0.51 (33.3 examples/sec; 0.241 sec/batch; 21h:05m:46s remains)
2017-12-06 05:35:26.523570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1369867 -0.61665362 -0.88292044 -0.70256513 -0.46163052 -0.33088934 -0.13944784 0.020447016 0.36014012 0.36757541 0.29931706 0.47497532 0.52919608 0.39454153 0.3972595][-0.74543244 -0.555211 -0.43598214 -0.61957139 -0.62772745 -0.44748312 -0.55155462 -0.55721974 -0.3774696 -0.20547923 0.10652279 0.11924186 0.0765871 0.13666281 0.21538761][-1.0489454 -0.78683507 -0.32518014 -0.34663591 -0.55245709 -0.69000095 -0.62544131 -0.71419787 -0.80587494 -0.894156 -0.88736618 -0.85560191 -0.63067627 -0.52650976 -0.37623388][-0.051290035 -0.51639587 -0.36525694 -0.17582852 0.26227468 0.45242497 0.22603583 0.043308966 -0.23815018 -0.54307461 -0.75907117 -1.0268615 -1.2036241 -1.2509601 -1.0704989][1.3460816 0.80950534 0.10381014 -0.17202179 -0.11674603 0.047729082 0.29417706 0.58855706 0.2892957 -0.021980017 -0.38644806 -0.82169074 -1.0755467 -1.1294142 -1.1673361][1.3365918 1.2650419 1.2146335 0.94285017 0.20325705 0.17022753 0.061563857 0.0071294978 -0.05653305 -0.039181653 -0.518833 -0.87989354 -1.0968612 -1.2038026 -1.1110357][1.2725731 1.5125412 1.642266 1.6732564 1.4596821 0.7139014 0.079411976 -0.0338141 -0.30771288 -0.42585638 -0.567424 -0.83815283 -1.3356047 -1.4520346 -1.4425622][0.5821591 1.0542206 1.4820845 1.5167065 1.3831164 0.90767038 0.32974175 -0.25682062 -0.45259976 -0.620182 -0.88915503 -1.0719314 -1.1943226 -1.3794049 -1.4889716][-0.96669137 -0.08535324 0.58979595 0.83927685 0.68892032 0.40012574 -0.032395352 -0.53774005 -0.70493376 -0.81833982 -0.926969 -1.0275077 -1.1379765 -1.1469315 -1.0331479][-1.3814111 -0.733675 -0.11068762 0.39621311 0.67409563 0.2722404 -0.22063008 -0.67291665 -1.047594 -1.2083981 -1.1575572 -1.1143078 -1.0620333 -1.0424409 -0.90009731][-1.79691 -1.0270953 -0.56676394 -0.17407082 0.031111933 0.15634221 0.055100076 -0.36686829 -0.9034093 -1.2191164 -1.4120072 -1.3125371 -1.0371604 -0.81717664 -0.77133018][-2.9011569 -1.9099002 -1.1244318 -0.76391858 -0.42806992 -0.059773646 -0.070196286 -0.25044927 -0.39898619 -0.68520832 -1.0342451 -1.2290057 -1.1383241 -0.97931993 -0.628227][-1.7482579 -1.0994415 -0.83332735 -0.43782106 -0.053113241 0.024451725 -0.061919127 -0.063356593 -0.14853577 -0.26406914 -0.37527287 -0.54785544 -0.7694453 -0.80089158 -0.66232055][-1.012257 -0.31106576 -0.066388667 0.31995854 0.63633543 0.84494466 0.62759763 0.25641111 -0.05984094 -0.13455968 -0.11843312 -0.133371 -0.25654379 -0.53299904 -0.67569488][-2.3518183 -1.7197839 -0.9048242 -0.37727979 0.50982404 0.97718161 0.938392 0.74965781 0.39386529 0.015661508 -0.21833265 -0.29192588 -0.2308096 -0.27453488 -0.34891874]]...]
INFO - root - 2017-12-06 05:35:28.818514: step 16810, loss = 0.77, batch loss = 0.55 (36.1 examples/sec; 0.221 sec/batch; 19h:25m:08s remains)
INFO - root - 2017-12-06 05:35:31.081812: step 16820, loss = 0.80, batch loss = 0.59 (34.7 examples/sec; 0.230 sec/batch; 20h:12m:14s remains)
INFO - root - 2017-12-06 05:35:33.399215: step 16830, loss = 0.83, batch loss = 0.62 (31.8 examples/sec; 0.252 sec/batch; 22h:05m:11s remains)
INFO - root - 2017-12-06 05:35:35.664909: step 16840, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-06 05:35:37.930889: step 16850, loss = 0.80, batch loss = 0.58 (35.9 examples/sec; 0.223 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-06 05:35:40.211622: step 16860, loss = 0.78, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:07m:39s remains)
INFO - root - 2017-12-06 05:35:42.513582: step 16870, loss = 0.79, batch loss = 0.57 (34.0 examples/sec; 0.236 sec/batch; 20h:39m:13s remains)
INFO - root - 2017-12-06 05:35:44.771080: step 16880, loss = 0.84, batch loss = 0.62 (34.6 examples/sec; 0.231 sec/batch; 20h:15m:41s remains)
INFO - root - 2017-12-06 05:35:47.057797: step 16890, loss = 0.75, batch loss = 0.54 (36.5 examples/sec; 0.219 sec/batch; 19h:13m:13s remains)
INFO - root - 2017-12-06 05:35:49.324970: step 16900, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 20h:09m:54s remains)
2017-12-06 05:35:51.613543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070881948 -0.073995158 -0.077637568 -0.080994956 -0.083681516 -0.08531303 -0.086438179 -0.08740443 -0.089205734 -0.092984669 -0.099266477 -0.10785624 -0.11685768 -0.12492229 -0.13157682][-0.069782689 -0.07411845 -0.07885024 -0.083048351 -0.086763009 -0.089174248 -0.090004809 -0.089797951 -0.09075588 -0.093697652 -0.099087313 -0.10694409 -0.11573143 -0.12364744 -0.13033147][-0.067767933 -0.07284306 -0.078162618 -0.082690038 -0.086834349 -0.089256532 -0.08966016 -0.088792436 -0.088814884 -0.090253256 -0.09401875 -0.10049989 -0.10840154 -0.11608549 -0.12241254][-0.06480512 -0.0691154 -0.073515758 -0.077604443 -0.081362747 -0.083637387 -0.084192015 -0.083988354 -0.0841885 -0.0847821 -0.087204851 -0.091740757 -0.0979666 -0.10405751 -0.10948089][-0.061147168 -0.063404545 -0.065531515 -0.067970634 -0.070431769 -0.07230451 -0.073453225 -0.074527696 -0.075614519 -0.076359339 -0.078298882 -0.081385233 -0.085755482 -0.089985676 -0.094461933][-0.056509778 -0.056260243 -0.056174461 -0.056500718 -0.057157069 -0.0583244 -0.059782892 -0.061925106 -0.06409876 -0.065973029 -0.068054058 -0.070466205 -0.073522538 -0.076338395 -0.079684779][-0.052315872 -0.049714323 -0.047376733 -0.045786932 -0.045214176 -0.045566905 -0.046761509 -0.0495073 -0.052970726 -0.056296919 -0.059117895 -0.061582312 -0.06380637 -0.065727711 -0.068083785][-0.049960639 -0.04588224 -0.042227104 -0.039349414 -0.037758458 -0.037278108 -0.038234007 -0.04111404 -0.04485023 -0.048892476 -0.052557826 -0.055479057 -0.057499707 -0.059202336 -0.0611308][-0.049737383 -0.045155272 -0.041078933 -0.037667781 -0.035474744 -0.034503989 -0.035091415 -0.037608106 -0.041070256 -0.045152053 -0.048867345 -0.051939994 -0.054080419 -0.055773716 -0.057420772][-0.050914466 -0.046462428 -0.042567816 -0.03931288 -0.03693917 -0.035597485 -0.035781786 -0.037682284 -0.040445223 -0.044031635 -0.047421817 -0.050292142 -0.052456234 -0.054192569 -0.055717487][-0.052619327 -0.0490725 -0.045972735 -0.043181617 -0.041096963 -0.039772928 -0.039664652 -0.040906232 -0.04291467 -0.04552792 -0.047972895 -0.050214175 -0.052208707 -0.053965788 -0.055446967][-0.054485336 -0.052087512 -0.050160538 -0.048339132 -0.046860144 -0.045799483 -0.045604732 -0.046168871 -0.047266293 -0.048769932 -0.050263494 -0.051735859 -0.05312847 -0.054516137 -0.055890583][-0.056773823 -0.055241313 -0.054166086 -0.053110059 -0.052116532 -0.051259764 -0.051052127 -0.051286 -0.051778995 -0.052525543 -0.053310644 -0.054093946 -0.0548646 -0.055735677 -0.056758922][-0.058704056 -0.05783556 -0.057437666 -0.056890395 -0.056311574 -0.055731915 -0.055536479 -0.055539835 -0.055630021 -0.055875577 -0.056129664 -0.056449436 -0.056767184 -0.057148375 -0.057836216][-0.06000673 -0.059596077 -0.059657164 -0.059548594 -0.059353679 -0.05904907 -0.058919497 -0.058778387 -0.058613647 -0.058536123 -0.058485385 -0.058446631 -0.058397066 -0.058503482 -0.058988191]]...]
INFO - root - 2017-12-06 05:35:53.893198: step 16910, loss = 0.71, batch loss = 0.50 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:13s remains)
INFO - root - 2017-12-06 05:35:56.203449: step 16920, loss = 0.86, batch loss = 0.65 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:45s remains)
INFO - root - 2017-12-06 05:35:58.467354: step 16930, loss = 0.79, batch loss = 0.57 (35.3 examples/sec; 0.227 sec/batch; 19h:52m:19s remains)
INFO - root - 2017-12-06 05:36:00.770258: step 16940, loss = 0.82, batch loss = 0.61 (34.7 examples/sec; 0.230 sec/batch; 20h:10m:50s remains)
INFO - root - 2017-12-06 05:36:03.067861: step 16950, loss = 0.81, batch loss = 0.59 (34.2 examples/sec; 0.234 sec/batch; 20h:28m:32s remains)
INFO - root - 2017-12-06 05:36:05.337931: step 16960, loss = 0.75, batch loss = 0.54 (34.9 examples/sec; 0.229 sec/batch; 20h:06m:03s remains)
INFO - root - 2017-12-06 05:36:07.653075: step 16970, loss = 0.81, batch loss = 0.59 (36.1 examples/sec; 0.222 sec/batch; 19h:24m:50s remains)
INFO - root - 2017-12-06 05:36:09.976956: step 16980, loss = 0.76, batch loss = 0.54 (32.5 examples/sec; 0.246 sec/batch; 21h:33m:46s remains)
INFO - root - 2017-12-06 05:36:12.268274: step 16990, loss = 0.84, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:43s remains)
INFO - root - 2017-12-06 05:36:14.575800: step 17000, loss = 0.77, batch loss = 0.56 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:54s remains)
2017-12-06 05:36:16.329534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.131082 -0.16173257 -0.21101721 -0.31142053 -0.47326195 -0.70218766 -0.97224635 -1.2464206 -1.4602354 -1.606033 -1.6150709 -1.4900447 -1.2510327 -0.95194584 -0.67080557][-0.18682437 -0.25328651 -0.30275583 -0.38463548 -0.48759595 -0.665069 -0.93931377 -1.2926955 -1.6433301 -1.95427 -2.0957212 -2.0503368 -1.7970867 -1.3837698 -0.9800787][-0.24529797 -0.35305426 -0.40130055 -0.45219225 -0.43418819 -0.4902277 -0.625909 -0.90790921 -1.3461481 -1.8754412 -2.2830572 -2.4175825 -2.2290342 -1.7446687 -1.2186898][-0.25905672 -0.36561495 -0.42253256 -0.42633641 -0.262721 -0.16270506 -0.083513841 -0.21954957 -0.61520487 -1.2868942 -1.9411311 -2.3343797 -2.3219619 -1.8940227 -1.3540142][-0.21106414 -0.2771847 -0.26305839 -0.1834181 0.093640424 0.42851782 0.80153018 0.89444911 0.56693935 -0.10824621 -0.94154024 -1.5601814 -1.7678462 -1.5711392 -1.1668543][-0.14739312 -0.13615018 0.026668563 0.2610099 0.650705 1.1172738 1.7272261 2.1094689 1.9539759 1.3840222 0.49582356 -0.24352425 -0.719778 -0.85685921 -0.69666624][-0.11116247 -0.055111986 0.16502544 0.52767664 1.086062 1.6421342 2.3745124 2.9161742 2.988214 2.5641012 1.7310331 0.98213845 0.33500722 -0.053191792 -0.14320242][-0.1054041 -0.049252979 0.16983113 0.57858706 1.1954515 1.911622 2.811511 3.4165132 3.5599079 3.2280245 2.3873093 1.6247154 0.93710214 0.42511076 0.20023239][-0.17786205 -0.16036065 -0.03778087 0.2664175 0.82011086 1.5082921 2.335844 3.009016 3.2585485 2.9142807 2.0997722 1.3328109 0.68449062 0.26284549 0.10235333][-0.22620052 -0.29952446 -0.344001 -0.26573569 0.046289258 0.53646874 1.1571944 1.6983616 2.0583618 1.9409399 1.2632726 0.61684608 0.086857729 -0.21502474 -0.27250314][-0.19777732 -0.31260589 -0.48931721 -0.66242796 -0.68232942 -0.53860396 -0.11780234 0.31187788 0.60365397 0.73204875 0.43933392 -0.024576329 -0.42172536 -0.59857482 -0.56217271][-0.16276935 -0.24360484 -0.408949 -0.65904003 -0.89324063 -1.0449052 -0.95789659 -0.74291658 -0.48998317 -0.28538564 -0.31267667 -0.41544795 -0.580892 -0.65616596 -0.58505827][-0.1295374 -0.17291924 -0.27428055 -0.46503454 -0.71659356 -0.99939835 -1.2021003 -1.2531662 -1.1137699 -0.96421158 -0.79561156 -0.67245227 -0.61619985 -0.49858868 -0.39712366][-0.10901404 -0.11954813 -0.15786821 -0.25848308 -0.40596223 -0.6356951 -0.877624 -1.0257339 -1.0921843 -1.0071611 -0.92216295 -0.77904582 -0.63240492 -0.40404421 -0.26926064][-0.10755093 -0.10618804 -0.1099813 -0.15395202 -0.19904307 -0.32100528 -0.49717391 -0.58274871 -0.67099482 -0.63839114 -0.55503213 -0.50268829 -0.42107514 -0.27259016 -0.18487824]]...]
INFO - root - 2017-12-06 05:36:18.637616: step 17010, loss = 0.81, batch loss = 0.59 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:42s remains)
INFO - root - 2017-12-06 05:36:20.912523: step 17020, loss = 0.76, batch loss = 0.54 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:36s remains)
INFO - root - 2017-12-06 05:36:23.210857: step 17030, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 20h:10m:46s remains)
INFO - root - 2017-12-06 05:36:25.558861: step 17040, loss = 0.73, batch loss = 0.51 (35.2 examples/sec; 0.228 sec/batch; 19h:56m:19s remains)
INFO - root - 2017-12-06 05:36:27.857923: step 17050, loss = 0.90, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 19h:30m:01s remains)
INFO - root - 2017-12-06 05:36:30.180333: step 17060, loss = 0.78, batch loss = 0.57 (35.6 examples/sec; 0.225 sec/batch; 19h:40m:24s remains)
INFO - root - 2017-12-06 05:36:32.492422: step 17070, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.234 sec/batch; 20h:32m:39s remains)
INFO - root - 2017-12-06 05:36:34.795468: step 17080, loss = 0.78, batch loss = 0.57 (34.6 examples/sec; 0.232 sec/batch; 20h:17m:04s remains)
INFO - root - 2017-12-06 05:36:37.096589: step 17090, loss = 0.76, batch loss = 0.55 (36.1 examples/sec; 0.222 sec/batch; 19h:26m:09s remains)
INFO - root - 2017-12-06 05:36:39.328851: step 17100, loss = 0.82, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:02s remains)
2017-12-06 05:36:39.705465: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.099587806 0.16920316 0.15760949 0.081825919 -0.010895602 -0.093131356 -0.12328641 -0.091454841 -0.040035322 0.029617831 0.10061669 0.15008986 0.17073363 0.15251541 0.07671345][-0.25242317 -0.18361542 -0.12439662 -0.10209445 -0.15203089 -0.24520966 -0.34984332 -0.40540722 -0.41635793 -0.35353544 -0.25227758 -0.14646873 -0.069117323 -0.049611636 -0.058374427][-0.356823 -0.31556877 -0.26022646 -0.21522567 -0.21463361 -0.26171169 -0.34915853 -0.43640524 -0.50658441 -0.50028473 -0.41663289 -0.29294264 -0.18934374 -0.16071033 -0.17236303][-0.41100389 -0.392187 -0.31728941 -0.24900055 -0.21808824 -0.20901039 -0.26188079 -0.34809136 -0.47913039 -0.55852836 -0.54120034 -0.45312241 -0.35485098 -0.30304989 -0.29340076][-0.4189311 -0.43207934 -0.33892405 -0.19901785 -0.089437842 -0.022400551 -0.018159725 -0.083439305 -0.25875723 -0.45709142 -0.55678266 -0.52149493 -0.42138916 -0.33627445 -0.28375548][-0.33247322 -0.36107329 -0.26288912 -0.10576347 0.045527719 0.17925566 0.26864779 0.30498484 0.19346598 -0.037565876 -0.2455835 -0.32643303 -0.28766862 -0.19875568 -0.14286414][-0.24331656 -0.25792578 -0.13376987 0.05540318 0.21482795 0.34092644 0.48331237 0.64001381 0.6464588 0.46270144 0.21499333 0.054153614 0.012707233 0.071557857 0.12271913][-0.17143999 -0.17794549 -0.041396558 0.19837615 0.41883802 0.55974072 0.66348338 0.79194349 0.8635487 0.791896 0.6096251 0.42212629 0.30344018 0.28367966 0.29375285][-0.094470829 -0.1063268 -0.0011343211 0.20205954 0.4163301 0.55194819 0.62326968 0.71074867 0.76765537 0.73273969 0.62339318 0.47231519 0.35333911 0.28905618 0.27535295][-0.14242975 -0.17753017 -0.11866728 0.025057942 0.19112489 0.31403273 0.37787706 0.41842967 0.41047543 0.34132403 0.24967185 0.17482573 0.12114585 0.11108664 0.13479856][-0.19159861 -0.2613861 -0.2833721 -0.24534163 -0.1630518 -0.080380335 -0.02333802 0.0056176484 -0.009400934 -0.07477586 -0.12802263 -0.13024005 -0.096124984 -0.031116206 0.02553428][-0.16786942 -0.25275248 -0.31746623 -0.34915578 -0.32872465 -0.27513567 -0.20948431 -0.15099952 -0.12508017 -0.14193186 -0.16398817 -0.17513841 -0.14198472 -0.099683285 -0.067567423][-0.12393261 -0.178947 -0.24385509 -0.29632649 -0.31141269 -0.28902236 -0.24612013 -0.20925906 -0.18811744 -0.19016358 -0.19780537 -0.19124916 -0.16501848 -0.15577559 -0.14508146][-0.09468694 -0.11143619 -0.14426309 -0.18632935 -0.21618727 -0.22608194 -0.21128467 -0.18868405 -0.16893713 -0.16620278 -0.17635828 -0.16893774 -0.14778747 -0.13628896 -0.12443238][-0.090792365 -0.093453854 -0.096992694 -0.1087696 -0.12435713 -0.1315423 -0.1314601 -0.12670361 -0.10534318 -0.10550201 -0.1251018 -0.13751376 -0.14241309 -0.14476463 -0.13094604]]...]
INFO - root - 2017-12-06 05:36:42.003034: step 17110, loss = 0.76, batch loss = 0.55 (35.0 examples/sec; 0.228 sec/batch; 19h:59m:48s remains)
INFO - root - 2017-12-06 05:36:44.296256: step 17120, loss = 0.85, batch loss = 0.63 (35.0 examples/sec; 0.229 sec/batch; 20h:02m:49s remains)
INFO - root - 2017-12-06 05:36:46.633341: step 17130, loss = 0.87, batch loss = 0.65 (36.3 examples/sec; 0.220 sec/batch; 19h:17m:25s remains)
INFO - root - 2017-12-06 05:36:48.909627: step 17140, loss = 0.89, batch loss = 0.68 (33.6 examples/sec; 0.238 sec/batch; 20h:52m:37s remains)
INFO - root - 2017-12-06 05:36:51.229588: step 17150, loss = 0.72, batch loss = 0.51 (32.6 examples/sec; 0.246 sec/batch; 21h:30m:18s remains)
INFO - root - 2017-12-06 05:36:53.533890: step 17160, loss = 0.77, batch loss = 0.56 (35.0 examples/sec; 0.229 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-06 05:36:55.799470: step 17170, loss = 0.79, batch loss = 0.58 (36.1 examples/sec; 0.221 sec/batch; 19h:23m:21s remains)
INFO - root - 2017-12-06 05:36:58.111539: step 17180, loss = 0.78, batch loss = 0.56 (35.8 examples/sec; 0.224 sec/batch; 19h:34m:53s remains)
INFO - root - 2017-12-06 05:37:00.386848: step 17190, loss = 0.76, batch loss = 0.55 (34.4 examples/sec; 0.232 sec/batch; 20h:21m:38s remains)
INFO - root - 2017-12-06 05:37:02.695457: step 17200, loss = 0.79, batch loss = 0.58 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:07s remains)
2017-12-06 05:37:03.057805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.23469909 -0.2629354 -0.29944849 -0.34627518 -0.32498336 -0.27870834 -0.24618657 -0.19717705 -0.17272413 -0.17903289 -0.23493968 -0.30186445 -0.34004748 -0.32444736 -0.24376337][-0.57858104 -0.67066956 -0.73081541 -0.80907154 -0.91924673 -0.89748865 -0.79892027 -0.64016855 -0.47320381 -0.3855201 -0.35387188 -0.40242451 -0.45829409 -0.44378573 -0.32954508][-0.8500405 -0.96378922 -0.96137893 -1.0065732 -1.105612 -1.2445033 -1.3000596 -1.1822667 -0.97446525 -0.75746667 -0.57193947 -0.52120471 -0.5120548 -0.54543471 -0.48627248][-1.0638566 -1.2180612 -1.2377799 -1.1658187 -1.2068529 -1.294029 -1.3555779 -1.3913732 -1.2680645 -1.1364965 -0.93980938 -0.748912 -0.64474386 -0.597053 -0.509403][-1.1594689 -1.3186707 -1.2868714 -1.0951161 -0.93492723 -0.90913522 -0.97552162 -1.0933237 -0.98926026 -0.90750182 -0.85339153 -0.84048682 -0.78679836 -0.730731 -0.64776969][-1.0358706 -1.0368105 -0.9113574 -0.66249847 -0.36205134 -0.096219912 -0.014337912 -0.095495366 -0.1897441 -0.3012864 -0.37098882 -0.4783 -0.61135781 -0.69316304 -0.64978391][-0.77482092 -0.67804366 -0.40578982 -0.0093277916 0.37369239 0.65106344 0.84781885 0.76222688 0.57118016 0.38995171 0.18319398 -0.10934348 -0.4028571 -0.58514464 -0.60674322][-0.58703494 -0.43549538 0.047645077 0.49267453 0.91785115 1.248847 1.4902539 1.3814752 1.0647578 0.69166183 0.41815686 0.11768958 -0.17163289 -0.36042923 -0.47716051][-0.38497895 -0.40001923 -0.18462054 0.25611734 0.72976631 1.1157272 1.4929972 1.5398285 1.4039423 0.94947606 0.49142122 0.10052738 -0.093454763 -0.21812376 -0.28900576][-0.48869613 -0.53852034 -0.29604074 0.034268722 0.33465478 0.63245219 0.9205268 1.1580629 1.2321606 0.99793345 0.72930938 0.31180614 -0.046979003 -0.1838959 -0.22998704][-0.49515504 -0.70747089 -0.57505804 -0.36579898 -0.0962499 0.1965639 0.47569275 0.66924149 0.8097142 0.84729904 0.79714143 0.58875751 0.36745954 0.14696305 -0.046858374][-0.37253645 -0.59859413 -0.6136983 -0.49906391 -0.34520343 -0.082799032 0.23647371 0.52612329 0.75022846 0.75585222 0.70315564 0.70552123 0.66030771 0.55263984 0.39399281][0.038895711 -0.25526482 -0.35737067 -0.4001407 -0.33952993 -0.22894226 0.0095060095 0.29035884 0.54879975 0.70614046 0.84524119 0.86492968 0.81922996 0.71711934 0.59181219][0.72717053 0.33909917 0.13230486 -0.1121906 -0.18951982 -0.23149925 -0.12712578 0.041709632 0.30078492 0.49840355 0.58584434 0.67167568 0.63780206 0.52628678 0.3403011][0.93483967 0.59035873 0.32380298 0.086764321 -0.067665622 -0.16507441 -0.20526554 -0.069051042 0.071499288 0.21473357 0.33037135 0.36207089 0.36064744 0.27565229 0.1668202]]...]
INFO - root - 2017-12-06 05:37:05.371604: step 17210, loss = 0.76, batch loss = 0.55 (35.0 examples/sec; 0.229 sec/batch; 20h:01m:21s remains)
INFO - root - 2017-12-06 05:37:07.696135: step 17220, loss = 0.73, batch loss = 0.52 (32.3 examples/sec; 0.247 sec/batch; 21h:40m:26s remains)
INFO - root - 2017-12-06 05:37:09.988989: step 17230, loss = 0.76, batch loss = 0.55 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:28s remains)
INFO - root - 2017-12-06 05:37:12.357041: step 17240, loss = 0.76, batch loss = 0.55 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:16s remains)
INFO - root - 2017-12-06 05:37:14.718707: step 17250, loss = 0.92, batch loss = 0.71 (34.9 examples/sec; 0.229 sec/batch; 20h:02m:40s remains)
INFO - root - 2017-12-06 05:37:16.986000: step 17260, loss = 0.78, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 19h:50m:10s remains)
INFO - root - 2017-12-06 05:37:19.294200: step 17270, loss = 0.75, batch loss = 0.53 (35.3 examples/sec; 0.226 sec/batch; 19h:49m:24s remains)
INFO - root - 2017-12-06 05:37:21.644903: step 17280, loss = 0.93, batch loss = 0.72 (34.5 examples/sec; 0.232 sec/batch; 20h:19m:34s remains)
INFO - root - 2017-12-06 05:37:23.970170: step 17290, loss = 0.74, batch loss = 0.53 (34.5 examples/sec; 0.232 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-06 05:37:26.245402: step 17300, loss = 0.81, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:14m:37s remains)
2017-12-06 05:37:28.520023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.60495621 0.5609498 0.48450655 0.31993946 0.10931644 -0.026734151 -0.21602269 -0.39808103 -0.51748812 -0.51296175 -0.44934458 -0.2941514 -0.12148227 -0.0070454329 0.039003044][0.41547394 0.35119683 0.29941091 0.18107176 -0.0014261603 -0.18839273 -0.45173946 -0.67750877 -0.8483026 -0.89138454 -0.85460252 -0.7647562 -0.59548992 -0.42856297 -0.31739956][-0.0741559 -0.10977989 -0.10120723 -0.11655956 -0.1721971 -0.32318121 -0.52156466 -0.68620914 -0.8266266 -0.94023865 -0.97585922 -0.89313555 -0.82508647 -0.78759784 -0.72921753][-0.50710809 -0.49095148 -0.39758709 -0.30570358 -0.2087841 -0.18517622 -0.23017494 -0.36472631 -0.50172025 -0.59578741 -0.67074341 -0.69754994 -0.731217 -0.78224796 -0.78776735][-0.41573259 -0.36682349 -0.22901063 -0.047362678 0.14398257 0.28585318 0.36642647 0.30750653 0.17070141 0.012508497 -0.1530759 -0.31273633 -0.4510963 -0.5977034 -0.68535745][0.041797698 0.12904003 0.2995775 0.54862279 0.75889939 0.90603364 0.97321159 0.93044537 0.79057479 0.57941389 0.34333172 0.10567906 -0.12585203 -0.3401404 -0.53703284][0.49101853 0.62052447 0.74922651 0.93374354 1.1077065 1.2419872 1.2957056 1.2461283 1.1274004 0.95077008 0.7365908 0.49822861 0.24689379 0.0060798228 -0.24212343][0.66931438 0.80119574 0.92695361 1.0478926 1.1256719 1.1934142 1.1972709 1.1450174 1.0347073 0.88690919 0.7297911 0.56996554 0.38933495 0.21452501 0.008901827][0.54658359 0.68877667 0.83485442 0.92647344 0.96993679 0.96944743 0.90673006 0.81768966 0.67720115 0.56008649 0.46957767 0.3960157 0.30001739 0.24387309 0.20949364][0.15073574 0.21888769 0.27347785 0.29076159 0.29164043 0.26811305 0.2129004 0.15410781 0.076064661 0.015993826 -0.011951946 0.012195103 0.028619163 0.075815305 0.15763392][-0.064808868 -0.10524826 -0.15498991 -0.22763708 -0.28983933 -0.3602711 -0.426969 -0.47590044 -0.51614559 -0.49231264 -0.43400261 -0.35002047 -0.28784445 -0.18974879 -0.07183487][-0.064246558 -0.15632357 -0.26905125 -0.43574932 -0.57773781 -0.664695 -0.71644908 -0.75822961 -0.78653663 -0.7550118 -0.67712158 -0.57922316 -0.50038385 -0.38220266 -0.26434731][-0.062026344 -0.17446363 -0.31139451 -0.45248052 -0.56696326 -0.64736158 -0.67599589 -0.670295 -0.6597634 -0.6475057 -0.60700232 -0.54027462 -0.47146708 -0.39112729 -0.37975296][0.09596242 -0.063133277 -0.2273761 -0.38254309 -0.491288 -0.53356391 -0.5298872 -0.50190628 -0.47800991 -0.45704091 -0.42536873 -0.36894667 -0.29185426 -0.24794243 -0.29736263][0.19810176 0.095790893 -0.012467586 -0.16316745 -0.25259069 -0.29949409 -0.29114366 -0.24680527 -0.26186007 -0.25748527 -0.2279221 -0.19388789 -0.092356823 -0.0360026 -0.045496427]]...]
INFO - root - 2017-12-06 05:37:30.794558: step 17310, loss = 0.88, batch loss = 0.66 (35.7 examples/sec; 0.224 sec/batch; 19h:36m:15s remains)
INFO - root - 2017-12-06 05:37:33.072706: step 17320, loss = 0.76, batch loss = 0.55 (35.3 examples/sec; 0.227 sec/batch; 19h:51m:07s remains)
INFO - root - 2017-12-06 05:37:35.381019: step 17330, loss = 0.73, batch loss = 0.52 (34.9 examples/sec; 0.229 sec/batch; 20h:05m:05s remains)
INFO - root - 2017-12-06 05:37:37.672943: step 17340, loss = 0.75, batch loss = 0.54 (36.9 examples/sec; 0.217 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-06 05:37:39.975536: step 17350, loss = 0.80, batch loss = 0.58 (35.0 examples/sec; 0.228 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-06 05:37:42.268214: step 17360, loss = 0.80, batch loss = 0.58 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:17s remains)
INFO - root - 2017-12-06 05:37:44.605693: step 17370, loss = 0.80, batch loss = 0.59 (35.3 examples/sec; 0.227 sec/batch; 19h:50m:06s remains)
INFO - root - 2017-12-06 05:37:46.918929: step 17380, loss = 0.83, batch loss = 0.62 (35.5 examples/sec; 0.225 sec/batch; 19h:43m:02s remains)
INFO - root - 2017-12-06 05:37:49.243163: step 17390, loss = 0.80, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 19h:35m:08s remains)
INFO - root - 2017-12-06 05:37:51.575363: step 17400, loss = 0.85, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 20h:25m:34s remains)
2017-12-06 05:37:55.656051: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.82215 3.1971939 2.9457438 2.8157206 2.3932028 2.068665 2.1043887 1.9882519 1.911273 1.3208753 0.896709 0.76717472 1.123873 1.6022139 1.8863062][3.0109246 2.4167776 1.6622028 1.0237011 0.40295807 0.0084345415 -0.30247813 -0.54155052 -0.8120544 -1.1833702 -1.3419815 -1.4517225 -1.4589471 -1.1018018 -0.54505581][-0.48688257 -1.0027188 -1.4857256 -1.950803 -2.2254813 -2.4383771 -2.5806236 -2.3124282 -2.1915784 -2.3519318 -2.5547962 -2.8271425 -2.8133342 -2.4475818 -2.2892737][-2.8633826 -3.1840637 -3.2520866 -3.4064281 -3.3663354 -3.2384076 -3.2289073 -3.1341157 -3.3079431 -3.2785184 -3.1506321 -3.3105214 -3.227365 -3.1063192 -2.8268085][-3.0709741 -3.4054098 -3.5194452 -3.2588592 -2.913456 -2.5820262 -2.4042029 -2.7636831 -2.8895652 -2.8213148 -2.9280622 -2.8636785 -3.0587051 -3.2609491 -3.4245129][-2.1434844 -2.542429 -2.6345184 -1.8215634 -1.1225022 -0.50366086 -0.10037286 0.010314092 0.017918475 -0.37377715 -0.67634088 -0.73396522 -1.0678695 -1.4862856 -1.7827408][-1.2399566 -1.0294592 -0.9215 -0.74238628 -0.26383668 0.92008144 1.8179851 2.4827898 2.7067747 2.6308663 2.6689816 2.3035352 1.930759 1.5426981 0.98820084][-0.44467211 -0.095615938 0.48316073 0.79839873 0.91186231 0.97480136 1.2080997 1.9755106 2.6133378 2.8278725 3.1441939 3.3549321 3.5414231 3.2674546 2.866895][-0.055470675 0.06477683 0.38774985 0.97500807 1.6847426 1.9383888 1.8815309 1.6880033 1.3645183 1.5763651 1.7980678 2.0172541 2.5332963 2.6821034 2.5452807][-0.20848814 -0.23100573 -0.14115454 0.013933115 0.39102551 0.65635186 0.85987461 0.79172367 0.70892531 0.6370607 0.38802847 0.38653696 0.73793352 1.1633027 1.3403068][-0.41415781 -0.42770734 -0.55280709 -0.83140087 -1.0989217 -1.2259312 -1.3022234 -1.3006963 -1.3664783 -1.2813926 -0.97870755 -0.72863072 -0.43435547 -0.30867824 -0.078808963][-0.66449857 -0.99442983 -1.2365667 -1.4298965 -1.654956 -1.9881496 -2.2466559 -2.2957473 -2.270067 -2.1990566 -2.2216091 -1.9413117 -1.6603631 -1.219457 -0.85892957][-0.52632844 -0.75929666 -1.0501797 -1.5137112 -1.817274 -2.1294408 -2.3522694 -2.3661 -2.440201 -2.5386159 -2.4185407 -2.1554227 -1.9442645 -1.7960244 -1.5477837][-0.081334934 -0.35232294 -0.57862532 -0.92920017 -1.3595842 -1.6838191 -1.9247653 -2.1807532 -2.2196534 -2.1564958 -2.2187204 -2.3204169 -2.1755383 -2.0485194 -2.0199413][-0.13611045 -0.070306554 -0.070005067 -0.25713778 -0.5303905 -0.91782564 -1.274033 -1.514883 -1.736043 -1.9284031 -2.0100195 -1.9421325 -1.8903691 -1.7547783 -1.6804262]]...]
INFO - root - 2017-12-06 05:37:57.999533: step 17410, loss = 0.79, batch loss = 0.58 (32.8 examples/sec; 0.244 sec/batch; 21h:21m:21s remains)
INFO - root - 2017-12-06 05:38:00.285636: step 17420, loss = 0.81, batch loss = 0.60 (35.2 examples/sec; 0.227 sec/batch; 19h:52m:11s remains)
INFO - root - 2017-12-06 05:38:02.566915: step 17430, loss = 0.75, batch loss = 0.54 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:23s remains)
INFO - root - 2017-12-06 05:38:04.862141: step 17440, loss = 0.75, batch loss = 0.53 (34.9 examples/sec; 0.229 sec/batch; 20h:05m:04s remains)
INFO - root - 2017-12-06 05:38:07.194810: step 17450, loss = 0.75, batch loss = 0.54 (33.5 examples/sec; 0.239 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-06 05:38:09.514496: step 17460, loss = 0.87, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 20h:35m:21s remains)
INFO - root - 2017-12-06 05:38:11.838404: step 17470, loss = 0.78, batch loss = 0.56 (34.6 examples/sec; 0.232 sec/batch; 20h:15m:32s remains)
INFO - root - 2017-12-06 05:38:14.152655: step 17480, loss = 0.77, batch loss = 0.56 (34.3 examples/sec; 0.234 sec/batch; 20h:26m:19s remains)
INFO - root - 2017-12-06 05:38:16.462698: step 17490, loss = 0.75, batch loss = 0.53 (33.0 examples/sec; 0.243 sec/batch; 21h:14m:38s remains)
INFO - root - 2017-12-06 05:38:18.814957: step 17500, loss = 0.78, batch loss = 0.56 (32.9 examples/sec; 0.243 sec/batch; 21h:17m:27s remains)
2017-12-06 05:38:20.564690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6813765 -2.3778155 -2.0479672 -1.5966319 -1.0712409 -0.91157532 -0.58823717 -0.52633488 -0.36429989 -0.37679315 -0.49268752 -0.57950693 -0.70065457 -0.45189786 -0.30082968][-3.2322874 -3.4654169 -3.1764181 -2.6273348 -2.0918343 -1.759702 -1.2719153 -0.98033959 -0.67445487 -0.55029368 -0.377688 -0.40553772 -0.60549009 -0.73314828 -0.71968621][-3.4501958 -3.3291183 -2.9351113 -2.8453197 -2.4054074 -1.9351263 -1.8350447 -1.7346462 -1.2787654 -0.83513391 -0.45942008 -0.26244348 -0.18804967 -0.31226534 -0.42896342][-2.4137444 -2.4479558 -2.3291063 -2.07099 -1.3884727 -1.2514019 -1.1950291 -1.1752298 -1.4275113 -1.359579 -0.81616151 -0.53748333 -0.30973876 -0.29082417 -0.3109535][0.11761242 0.0054191723 0.17394988 0.069467917 0.077889308 0.50466746 0.61687547 0.0738727 -0.28237337 -0.544351 -0.9352181 -0.87239 -0.43397921 -0.48875612 -0.43481189][2.4676814 2.8846855 2.9175467 2.8018625 2.5035305 2.0416899 1.3950018 1.246722 0.81228143 0.46039271 -0.0032808706 -0.54657656 -0.63934505 -0.40497959 -0.21253642][3.537394 4.4908385 4.953301 4.9847484 4.2734041 3.5824616 2.7943544 1.9685738 1.4335879 1.2181473 0.9444167 0.51835775 -0.091027334 -0.4621498 -0.39921159][2.4957645 3.8975077 5.1050425 5.452476 5.1826539 4.5594263 3.7094223 2.9264081 2.4072728 1.9881594 1.4561478 1.1282514 0.80678576 0.11973189 -0.052951526][1.3848999 2.0553937 2.9779701 3.7544272 4.0629859 3.7420254 3.2274501 2.6747513 2.2343392 1.9359381 1.7026548 1.3843687 1.1034111 0.5510025 0.343233][-0.57067579 -0.11740802 1.0688157 1.9063621 2.3965678 2.4580317 2.0508256 1.634011 1.396577 1.1850884 1.2259371 1.2175126 1.1098545 0.599081 0.23156594][-1.8062613 -1.6101551 -1.0634756 -0.3012065 0.35677147 0.70480084 0.64584273 0.47249103 0.48902071 0.3710677 0.52127481 0.42057252 0.70426321 0.46790147 0.17512308][-2.4860785 -2.4511113 -2.0528934 -1.4146506 -1.1150721 -0.83629692 -0.60286677 -0.57846594 -0.51363957 -0.40558034 -0.096728593 0.0014562681 -0.040102661 -0.11923943 -0.056931384][-1.9338002 -1.9522582 -2.073159 -1.8434398 -1.5849227 -1.1297082 -1.1279942 -1.0339552 -0.92287922 -0.77002573 -0.54609823 -0.41045105 -0.25253087 -0.21566653 -0.20712039][-1.4888848 -1.3822478 -1.0129369 -1.0033262 -1.2958946 -1.0844907 -1.0075834 -0.9359706 -0.8780055 -0.76705432 -0.69318956 -0.62592411 -0.48636252 -0.32618111 -0.20338181][-1.1388284 -1.00521 -0.59156138 -0.17427148 0.066836715 -0.064252213 -0.35456342 -0.34055018 -0.22364807 -0.33869186 -0.49761331 -0.46866435 -0.35648608 -0.24981284 -0.18555412]]...]
INFO - root - 2017-12-06 05:38:22.874715: step 17510, loss = 0.84, batch loss = 0.63 (33.3 examples/sec; 0.240 sec/batch; 21h:01m:21s remains)
INFO - root - 2017-12-06 05:38:25.261201: step 17520, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.232 sec/batch; 20h:19m:55s remains)
INFO - root - 2017-12-06 05:38:27.621310: step 17530, loss = 0.87, batch loss = 0.66 (34.0 examples/sec; 0.235 sec/batch; 20h:34m:52s remains)
INFO - root - 2017-12-06 05:38:29.898078: step 17540, loss = 0.81, batch loss = 0.60 (34.9 examples/sec; 0.229 sec/batch; 20h:04m:15s remains)
INFO - root - 2017-12-06 05:38:32.181683: step 17550, loss = 0.79, batch loss = 0.57 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:19s remains)
INFO - root - 2017-12-06 05:38:34.527575: step 17560, loss = 0.84, batch loss = 0.63 (34.0 examples/sec; 0.235 sec/batch; 20h:35m:03s remains)
INFO - root - 2017-12-06 05:38:36.840958: step 17570, loss = 0.83, batch loss = 0.62 (34.4 examples/sec; 0.232 sec/batch; 20h:19m:42s remains)
INFO - root - 2017-12-06 05:38:39.115126: step 17580, loss = 1.00, batch loss = 0.79 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:32s remains)
INFO - root - 2017-12-06 05:38:41.424927: step 17590, loss = 0.86, batch loss = 0.64 (32.2 examples/sec; 0.248 sec/batch; 21h:42m:23s remains)
INFO - root - 2017-12-06 05:38:43.744745: step 17600, loss = 0.84, batch loss = 0.62 (34.2 examples/sec; 0.234 sec/batch; 20h:27m:03s remains)
2017-12-06 05:38:44.126598: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.88144815 0.76632506 0.45956847 0.090432756 -0.29055902 -0.59551567 -0.84821093 -0.97419232 -0.88771689 -0.6695087 -0.47332013 -0.36135131 -0.35719451 -0.35339797 -0.34944707][0.93083525 0.81882536 0.49649885 0.067232169 -0.42579132 -1.0213935 -1.4800501 -1.8190684 -1.9507453 -1.9009117 -1.7310034 -1.4261581 -1.1431197 -0.91904855 -0.74990654][0.19932532 0.13369122 -0.030160144 -0.3054404 -0.7100755 -1.2197005 -1.6629864 -2.0103741 -2.2704637 -2.3766606 -2.3273931 -2.1407061 -1.8824357 -1.5973018 -1.2722336][-1.0102514 -1.0038009 -1.0243618 -1.0883918 -1.1483983 -1.3329662 -1.5278541 -1.6626623 -1.7716621 -1.8214206 -1.8445473 -1.8106186 -1.682459 -1.4968191 -1.267357][-1.3802494 -1.5122724 -1.4729439 -1.3723029 -1.2548497 -1.1078341 -0.95360518 -0.86890888 -0.8706522 -0.93393993 -0.9959628 -1.0478705 -1.0876797 -1.079875 -0.98092949][-0.83726311 -0.95476019 -0.87201017 -0.66726118 -0.38220918 -0.090892896 0.18255651 0.28108281 0.34612495 0.23598379 0.068851523 -0.13798438 -0.33466923 -0.52713323 -0.60497606][-0.010412313 -0.14064544 -0.078262091 0.26030812 0.68632281 1.0370094 1.2657897 1.337082 1.282142 1.0068127 0.69835806 0.37952837 0.10960156 -0.16274548 -0.36178467][0.22632766 0.44226339 0.725709 0.99389946 1.3347732 1.7442582 2.0180354 2.0086017 1.7802823 1.4161801 0.945089 0.31399554 -0.19112338 -0.49986163 -0.74795204][0.23048615 0.50575823 0.85195524 1.1433685 1.4736929 1.6750443 1.7486602 1.7093866 1.4536508 0.94704783 0.34879348 -0.26869604 -0.74685681 -1.1127585 -1.3517901][-0.60723257 -0.27861479 0.07180319 0.33355218 0.52170545 0.59635776 0.6008876 0.49546888 0.21036312 -0.21178201 -0.67714846 -1.1788213 -1.6366895 -1.8937801 -1.9641683][-1.2051349 -1.1236998 -1.0170836 -1.0156485 -1.0605109 -1.1188225 -1.1959454 -1.4041305 -1.6283375 -1.8491457 -2.1144748 -2.3573177 -2.5948031 -2.7605133 -2.8073878][-1.8775711 -1.9470103 -2.0160711 -2.0066931 -1.9472837 -2.067106 -2.2052333 -2.3573818 -2.48509 -2.71956 -2.8744531 -2.8126314 -2.7071817 -2.6687796 -2.5350771][-1.4916155 -1.6373104 -1.8993112 -2.0070882 -1.9744337 -1.8259381 -1.56244 -1.4574437 -1.3661919 -1.3678818 -1.4061201 -1.4588592 -1.3568814 -1.2014861 -0.96723521][-0.83305341 -0.79024696 -0.83383709 -0.97964388 -1.0637025 -0.90903813 -0.59256148 -0.18681535 0.062976517 0.10690165 0.1761995 0.23733565 0.28968129 0.43079796 0.52086949][-0.48593968 -0.2029002 -0.12583251 -0.090446249 0.053239278 0.2024219 0.4480789 0.72802848 1.022216 1.1436503 1.1384695 1.1510925 1.1777322 1.4624947 1.5908344]]...]
INFO - root - 2017-12-06 05:38:46.428365: step 17610, loss = 0.76, batch loss = 0.55 (35.2 examples/sec; 0.228 sec/batch; 19h:54m:23s remains)
INFO - root - 2017-12-06 05:38:48.719596: step 17620, loss = 0.82, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:38s remains)
INFO - root - 2017-12-06 05:38:51.052309: step 17630, loss = 0.89, batch loss = 0.68 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:49s remains)
INFO - root - 2017-12-06 05:38:53.362611: step 17640, loss = 0.84, batch loss = 0.62 (34.8 examples/sec; 0.230 sec/batch; 20h:05m:23s remains)
INFO - root - 2017-12-06 05:38:55.716146: step 17650, loss = 0.83, batch loss = 0.62 (34.0 examples/sec; 0.236 sec/batch; 20h:36m:14s remains)
INFO - root - 2017-12-06 05:38:58.030585: step 17660, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 20h:16m:49s remains)
INFO - root - 2017-12-06 05:39:00.320975: step 17670, loss = 0.81, batch loss = 0.59 (34.8 examples/sec; 0.230 sec/batch; 20h:05m:49s remains)
INFO - root - 2017-12-06 05:39:02.624389: step 17680, loss = 0.80, batch loss = 0.58 (33.7 examples/sec; 0.237 sec/batch; 20h:43m:54s remains)
INFO - root - 2017-12-06 05:39:04.901643: step 17690, loss = 0.75, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 20h:12m:27s remains)
INFO - root - 2017-12-06 05:39:07.194881: step 17700, loss = 0.82, batch loss = 0.61 (34.4 examples/sec; 0.232 sec/batch; 20h:18m:51s remains)
2017-12-06 05:39:07.577188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.4950066 -2.2765174 -2.58501 -2.7117002 -2.6784649 -2.5911744 -2.5929832 -3.0167766 -3.3736615 -3.1850548 -2.5755842 -2.0415232 -1.5012667 -1.5269736 -2.00562][-1.4648463 -2.113054 -2.7271159 -3.1825783 -3.3499625 -3.3072181 -3.0225472 -3.0329747 -3.2168362 -3.057513 -2.8544226 -2.4665265 -1.9384581 -1.9755026 -2.3918877][-1.8946357 -2.0821712 -2.36858 -2.5899265 -2.9903116 -3.3148992 -3.4341822 -3.1890607 -2.7688742 -2.4419918 -2.338274 -2.0664351 -1.9584128 -1.864482 -2.0262828][-2.30375 -2.1980534 -2.2420726 -2.4804084 -2.6590052 -2.9510267 -3.1181638 -2.9209979 -2.742198 -2.2418332 -1.4524496 -0.866182 -0.73047107 -0.81841636 -1.0471267][-1.8936411 -1.8715857 -1.7945151 -1.7665303 -2.137049 -2.1828134 -2.2580173 -2.0371385 -1.4215447 -0.83945537 -0.48442551 -0.015682317 0.4203274 0.8088364 0.82387847][-1.497349 -1.3197676 -1.0383744 -1.0277835 -1.2174289 -1.3388507 -1.4415832 -1.0505447 -0.61826819 -0.052481785 0.56452042 1.2306751 1.6077158 1.6898152 1.6802537][-0.80843085 -1.1017768 -0.565894 -0.30831417 -0.37273461 -0.31645584 -0.13855527 -0.14883989 -0.063499689 0.17723137 0.49070731 1.2111584 1.7672396 1.9287548 2.0832241][0.55704445 -0.18339179 -0.23305094 0.080152743 0.78262627 1.0581245 0.79640412 0.25841379 0.11443003 0.26258984 0.41741171 0.84483767 1.353791 1.6780472 1.7854609][1.2696356 0.93550724 0.33292705 0.033143021 0.21447325 0.47100833 0.92353886 0.49997965 0.051828809 -0.34184736 -0.37645376 -0.062046684 0.16789213 0.48322526 0.94834322][1.4431466 1.3600359 1.0447284 0.71860075 0.022712708 -0.7022953 -1.0906888 -0.90918851 -0.57941258 -0.98685819 -1.162591 -1.2180862 -1.2828983 -0.89492619 -0.25678772][1.9392524 1.869812 1.2082365 1.0722662 0.53163123 -0.40843683 -1.2511454 -1.642872 -1.7854273 -1.3964132 -1.4300238 -1.4937881 -1.5307839 -1.8138864 -1.6939032][0.54118055 1.169209 1.266217 0.90068054 0.32252285 -0.09549354 -0.79353923 -1.0615594 -1.2619498 -1.2311677 -1.5853468 -1.7419978 -1.8668302 -2.0368667 -1.8270118][-0.81506324 -0.29688057 -0.13602698 0.05966071 0.028422542 -0.17110249 -0.41984606 -0.3323788 -0.688065 -0.8278116 -1.0134344 -1.4175223 -1.6190121 -1.8503727 -1.7828553][-2.4715636 -1.867959 -1.3864747 -1.377592 -1.4211483 -1.0277423 -0.54607481 -0.19563073 -0.27138668 -0.65878469 -1.0577439 -1.4483031 -1.4779208 -1.639401 -1.81136][-3.2586613 -2.7410643 -2.3214185 -2.2171061 -1.8869808 -1.4995102 -1.4558351 -1.2737787 -0.93738043 -0.72311264 -0.74319512 -1.1964024 -1.3508966 -1.3452833 -1.1928308]]...]
INFO - root - 2017-12-06 05:39:09.878061: step 17710, loss = 0.76, batch loss = 0.54 (35.3 examples/sec; 0.227 sec/batch; 19h:49m:05s remains)
INFO - root - 2017-12-06 05:39:12.182893: step 17720, loss = 0.73, batch loss = 0.52 (34.6 examples/sec; 0.231 sec/batch; 20h:12m:08s remains)
INFO - root - 2017-12-06 05:39:14.506889: step 17730, loss = 0.84, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 19h:56m:08s remains)
INFO - root - 2017-12-06 05:39:16.809340: step 17740, loss = 0.72, batch loss = 0.50 (33.1 examples/sec; 0.242 sec/batch; 21h:08m:49s remains)
INFO - root - 2017-12-06 05:39:19.112369: step 17750, loss = 0.78, batch loss = 0.57 (35.1 examples/sec; 0.228 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-06 05:39:21.392129: step 17760, loss = 0.73, batch loss = 0.51 (35.2 examples/sec; 0.227 sec/batch; 19h:50m:50s remains)
INFO - root - 2017-12-06 05:39:23.729684: step 17770, loss = 0.75, batch loss = 0.54 (34.9 examples/sec; 0.229 sec/batch; 20h:01m:23s remains)
INFO - root - 2017-12-06 05:39:26.047660: step 17780, loss = 0.88, batch loss = 0.66 (35.4 examples/sec; 0.226 sec/batch; 19h:45m:01s remains)
INFO - root - 2017-12-06 05:39:28.320155: step 17790, loss = 0.87, batch loss = 0.66 (35.4 examples/sec; 0.226 sec/batch; 19h:45m:04s remains)
INFO - root - 2017-12-06 05:39:30.587214: step 17800, loss = 0.81, batch loss = 0.60 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:18s remains)
2017-12-06 05:39:30.930616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.052470297 -0.052101042 -0.052113537 -0.052108351 -0.052153144 -0.052288983 -0.052892573 -0.053811044 -0.055122811 -0.056878444 -0.059652112 -0.063779883 -0.069138557 -0.075790279 -0.082817204][-0.053419143 -0.052551582 -0.052237354 -0.052238353 -0.052774675 -0.053624317 -0.054940533 -0.056384671 -0.057900611 -0.059562035 -0.06204924 -0.065721437 -0.070659578 -0.07681562 -0.083352357][-0.056755215 -0.055211086 -0.05439803 -0.054167606 -0.054947279 -0.056439385 -0.058463883 -0.060322165 -0.061776683 -0.06313207 -0.065070264 -0.067947693 -0.071791239 -0.0766394 -0.0819219][-0.061233174 -0.059274249 -0.057639163 -0.056756534 -0.057515923 -0.059419196 -0.061839491 -0.064006291 -0.0654798 -0.066415623 -0.067605227 -0.069413885 -0.072044119 -0.075322576 -0.07887321][-0.065520808 -0.063515693 -0.061333694 -0.059790663 -0.060121305 -0.061807841 -0.064327985 -0.0666151 -0.0681485 -0.068702921 -0.069108337 -0.069900312 -0.071109004 -0.072790906 -0.0748655][-0.068432927 -0.066579767 -0.064222634 -0.06221009 -0.061884053 -0.063014828 -0.065398365 -0.067728244 -0.069279768 -0.069657564 -0.069564544 -0.069459692 -0.069272861 -0.069540694 -0.070312619][-0.069633357 -0.068035029 -0.065670878 -0.063467547 -0.062614925 -0.063169375 -0.064965777 -0.066892684 -0.068501785 -0.068818748 -0.068414696 -0.067685381 -0.066690952 -0.066212907 -0.066349439][-0.068838775 -0.067564815 -0.065402 -0.063087583 -0.06181198 -0.061890423 -0.062937818 -0.06423834 -0.065488778 -0.065601848 -0.065168738 -0.064455643 -0.063400224 -0.062876329 -0.062983632][-0.066087618 -0.064874858 -0.063287452 -0.06124882 -0.059799097 -0.059405074 -0.060085788 -0.060947053 -0.061405037 -0.06095887 -0.060523652 -0.06024351 -0.059685998 -0.059608057 -0.059919573][-0.062614873 -0.061263841 -0.060071543 -0.058582585 -0.057359546 -0.056688596 -0.056949545 -0.057247143 -0.057152249 -0.056489911 -0.056103908 -0.056050815 -0.055995442 -0.056443065 -0.05713544][-0.05916027 -0.057783816 -0.056744818 -0.055604778 -0.054662753 -0.053881619 -0.053658307 -0.053564064 -0.053317007 -0.052606672 -0.052269548 -0.05237044 -0.052741576 -0.053579085 -0.05459841][-0.056387443 -0.054812107 -0.053854365 -0.052982442 -0.05221248 -0.051504117 -0.050980955 -0.050519567 -0.050095133 -0.049520649 -0.049286265 -0.049523465 -0.050193109 -0.051415604 -0.0527784][-0.054314096 -0.052524485 -0.05175937 -0.051002804 -0.050466888 -0.049845431 -0.049185351 -0.048602786 -0.048134323 -0.047748372 -0.047577549 -0.04793549 -0.048752185 -0.050189804 -0.051809635][-0.053456411 -0.051666889 -0.051027615 -0.0504721 -0.050034914 -0.049466245 -0.048808973 -0.048250835 -0.047785826 -0.047495846 -0.04741212 -0.047836475 -0.048737206 -0.050130356 -0.051753864][-0.053655874 -0.052119981 -0.05163243 -0.051343117 -0.051078916 -0.0506708 -0.050114803 -0.049582165 -0.049152311 -0.048847128 -0.04879244 -0.049179863 -0.049971607 -0.051115453 -0.052453857]]...]
INFO - root - 2017-12-06 05:39:33.224571: step 17810, loss = 0.78, batch loss = 0.57 (34.6 examples/sec; 0.231 sec/batch; 20h:11m:54s remains)
INFO - root - 2017-12-06 05:39:35.520421: step 17820, loss = 0.79, batch loss = 0.58 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:30s remains)
INFO - root - 2017-12-06 05:39:37.838286: step 17830, loss = 0.83, batch loss = 0.61 (32.9 examples/sec; 0.243 sec/batch; 21h:14m:55s remains)
INFO - root - 2017-12-06 05:39:40.179551: step 17840, loss = 0.73, batch loss = 0.52 (33.5 examples/sec; 0.239 sec/batch; 20h:53m:55s remains)
INFO - root - 2017-12-06 05:39:42.482507: step 17850, loss = 0.72, batch loss = 0.50 (35.0 examples/sec; 0.228 sec/batch; 19h:57m:00s remains)
INFO - root - 2017-12-06 05:39:44.783939: step 17860, loss = 0.74, batch loss = 0.53 (33.8 examples/sec; 0.237 sec/batch; 20h:42m:20s remains)
INFO - root - 2017-12-06 05:39:47.114169: step 17870, loss = 0.73, batch loss = 0.51 (34.1 examples/sec; 0.235 sec/batch; 20h:29m:41s remains)
INFO - root - 2017-12-06 05:39:49.448217: step 17880, loss = 0.80, batch loss = 0.59 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:44s remains)
INFO - root - 2017-12-06 05:39:51.762899: step 17890, loss = 0.80, batch loss = 0.58 (36.0 examples/sec; 0.222 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-06 05:39:54.058761: step 17900, loss = 0.81, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 20h:22m:40s remains)
2017-12-06 05:39:55.496050: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22266492 0.20854577 0.19671002 0.1500992 0.085354254 0.036790818 0.00096704066 0.017678328 0.016574062 0.05914025 0.091646478 0.093340755 0.081886113 0.058256567 0.017428443][-0.061698817 -0.047341682 0.0013558492 -0.020378441 -0.09687303 -0.16127494 -0.19662589 -0.17447025 -0.14976025 -0.069897309 -0.011655591 -0.016912043 -0.039443381 -0.06396234 -0.067782819][-0.16221511 -0.14438631 -0.12435788 -0.14628839 -0.20899108 -0.25400656 -0.25621778 -0.22047979 -0.1761332 -0.14913914 -0.13617612 -0.11910674 -0.11157358 -0.12726715 -0.12976751][-0.30241552 -0.2549288 -0.22814187 -0.23282598 -0.25336596 -0.25464156 -0.22741246 -0.1743926 -0.14100033 -0.13741527 -0.14062923 -0.14399903 -0.157639 -0.15749133 -0.14843258][-0.40192965 -0.32731178 -0.26244813 -0.21433839 -0.18542987 -0.16245291 -0.13529985 -0.10825498 -0.090888686 -0.10444488 -0.12913236 -0.13286728 -0.12280208 -0.11176671 -0.098672867][-0.32896715 -0.29347771 -0.24259304 -0.17248711 -0.13247091 -0.10693879 -0.087846592 -0.06873399 -0.061994363 -0.067573823 -0.080937475 -0.095726542 -0.10467023 -0.11181257 -0.1164515][-0.23977444 -0.20581993 -0.1667604 -0.13499378 -0.12598264 -0.11004281 -0.11117665 -0.10683651 -0.1115473 -0.09717641 -0.094539374 -0.083595335 -0.067565091 -0.068504542 -0.071107641][-0.25497007 -0.22073179 -0.16504334 -0.13542253 -0.11189163 -0.10082255 -0.089074813 -0.062629849 -0.047015034 -0.034593355 -0.038083896 -0.034142498 -0.0516396 -0.077495307 -0.096465774][-0.12675706 -0.13240063 -0.096286684 -0.064673208 -0.024175994 -0.018470235 -0.0026535615 0.034106128 0.069856763 0.11318296 0.1521399 0.15726449 0.12361239 0.08198455 0.032353416][0.36355066 0.32222456 0.33952922 0.35227138 0.36427313 0.37485176 0.38130397 0.37567627 0.40589806 0.45511419 0.48182344 0.47899938 0.43780714 0.35882217 0.257642][0.94530821 0.87259638 0.82995319 0.79616541 0.762364 0.71331125 0.67198461 0.69488043 0.76472497 0.8339588 0.9001326 0.87823826 0.76793736 0.60746551 0.42862374][1.1109717 1.005661 0.92577267 0.84823287 0.78691 0.73216939 0.66132742 0.64118868 0.67183644 0.75703084 0.82779425 0.86545926 0.81381333 0.69838619 0.52371222][0.69602203 0.56460118 0.48472041 0.41153008 0.3308872 0.33107972 0.30973142 0.30866387 0.33370703 0.42592162 0.536738 0.59218216 0.55139965 0.47378814 0.35528862][0.084418014 -0.0544935 -0.069875389 -0.076081924 -0.054998063 -0.013752699 -0.018250883 -0.063658215 -0.11388806 -0.094676428 -0.032363195 0.038534209 0.075953513 0.062655553 0.027161002][-0.33309877 -0.38485819 -0.34863621 -0.27333266 -0.18830204 -0.1450737 -0.13521779 -0.19091937 -0.28574237 -0.33315662 -0.32676274 -0.24117728 -0.19607522 -0.18476181 -0.17812906]]...]
INFO - root - 2017-12-06 05:39:57.819578: step 17910, loss = 0.83, batch loss = 0.62 (31.7 examples/sec; 0.252 sec/batch; 22h:01m:39s remains)
INFO - root - 2017-12-06 05:40:00.136658: step 17920, loss = 0.81, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 19h:53m:26s remains)
INFO - root - 2017-12-06 05:40:02.472405: step 17930, loss = 0.78, batch loss = 0.56 (33.9 examples/sec; 0.236 sec/batch; 20h:38m:38s remains)
INFO - root - 2017-12-06 05:40:04.790894: step 17940, loss = 0.80, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:56s remains)
INFO - root - 2017-12-06 05:40:07.123881: step 17950, loss = 0.79, batch loss = 0.57 (34.3 examples/sec; 0.233 sec/batch; 20h:23m:30s remains)
INFO - root - 2017-12-06 05:40:09.384776: step 17960, loss = 0.73, batch loss = 0.52 (35.7 examples/sec; 0.224 sec/batch; 19h:35m:46s remains)
INFO - root - 2017-12-06 05:40:11.664912: step 17970, loss = 0.70, batch loss = 0.48 (34.8 examples/sec; 0.230 sec/batch; 20h:05m:48s remains)
INFO - root - 2017-12-06 05:40:13.987661: step 17980, loss = 0.77, batch loss = 0.55 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:11s remains)
INFO - root - 2017-12-06 05:40:16.245323: step 17990, loss = 0.69, batch loss = 0.47 (34.2 examples/sec; 0.234 sec/batch; 20h:26m:18s remains)
INFO - root - 2017-12-06 05:40:18.503924: step 18000, loss = 0.81, batch loss = 0.60 (36.0 examples/sec; 0.222 sec/batch; 19h:25m:21s remains)
2017-12-06 05:40:19.679651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.095378466 -0.0966102 -0.098756447 -0.10368647 -0.11064687 -0.11862513 -0.12437491 -0.12479676 -0.12044957 -0.11325309 -0.10721308 -0.10247554 -0.099688277 -0.10078236 -0.10178866][-0.09682335 -0.10244316 -0.11462377 -0.13823915 -0.17464247 -0.21839945 -0.25768119 -0.28170562 -0.27321565 -0.24100833 -0.19717602 -0.15850247 -0.12694404 -0.10723795 -0.10199388][-0.10328609 -0.121758 -0.15654287 -0.21155566 -0.28295356 -0.36219615 -0.43056732 -0.47660702 -0.49482244 -0.47685957 -0.41267967 -0.32926577 -0.24600434 -0.16188669 -0.12437651][-0.11724204 -0.15573053 -0.22272047 -0.30703515 -0.38949305 -0.45429963 -0.48996454 -0.5116868 -0.54296774 -0.58345789 -0.5764094 -0.52586848 -0.4307825 -0.27609897 -0.18589047][-0.13699216 -0.19937669 -0.29293147 -0.39218557 -0.44896382 -0.44819713 -0.40216553 -0.34163612 -0.34841472 -0.42743808 -0.50692582 -0.55307072 -0.53225923 -0.37301743 -0.24818058][-0.15891077 -0.23751695 -0.34076333 -0.4400807 -0.45532578 -0.40040141 -0.25777945 -0.08816833 -0.030130874 -0.10446396 -0.24302474 -0.40418178 -0.48334628 -0.39148241 -0.27317256][-0.18193296 -0.265508 -0.35932726 -0.43015772 -0.40900838 -0.32101664 -0.11896747 0.14550816 0.35672367 0.3587237 0.18922906 -0.088863738 -0.31501 -0.35641861 -0.28379971][-0.19848225 -0.29624844 -0.37835449 -0.41403741 -0.33423245 -0.18845931 0.014469169 0.3044942 0.57053524 0.68144274 0.62221396 0.31709647 -0.01407665 -0.21490629 -0.26100859][-0.21382082 -0.31471866 -0.41100407 -0.44519758 -0.32949674 -0.13154687 0.0992586 0.43184686 0.694515 0.84263456 0.81103188 0.56553435 0.20891272 -0.089651644 -0.24254203][-0.2293855 -0.33354348 -0.42809689 -0.42373538 -0.31441814 -0.087917522 0.15717435 0.50042814 0.69511294 0.83500469 0.75371873 0.522595 0.23087062 -0.025772557 -0.19325466][-0.23513357 -0.34080565 -0.43440294 -0.43414229 -0.29477435 -0.041912492 0.17117576 0.48886043 0.64302 0.77798891 0.676147 0.48763168 0.19082634 -0.067073494 -0.18596879][-0.23239386 -0.3298305 -0.44324702 -0.46905476 -0.34945315 -0.14308262 0.11637011 0.39748442 0.49325049 0.56560785 0.44372874 0.30660725 0.1128228 -0.085792787 -0.20583612][-0.20778954 -0.30470395 -0.43311161 -0.49311537 -0.50429714 -0.34228849 -0.093458749 0.15218049 0.3749544 0.41860288 0.298055 0.16468637 -0.031347461 -0.13539632 -0.1886493][-0.17823575 -0.26801109 -0.38882118 -0.47377414 -0.51616353 -0.41854692 -0.31863484 -0.10987586 0.11416113 0.16920723 0.16467787 0.085683376 -0.021238215 -0.11122631 -0.16961923][-0.14580426 -0.20854798 -0.30319867 -0.4122774 -0.48101544 -0.43822998 -0.34189641 -0.18399198 -0.052647717 0.00073311478 -0.0017953366 -0.015176319 -0.040185839 -0.11248878 -0.15495551]]...]
INFO - root - 2017-12-06 05:40:22.009767: step 18010, loss = 0.78, batch loss = 0.57 (34.4 examples/sec; 0.233 sec/batch; 20h:20m:40s remains)
INFO - root - 2017-12-06 05:40:24.326344: step 18020, loss = 0.74, batch loss = 0.53 (33.5 examples/sec; 0.239 sec/batch; 20h:53m:19s remains)
INFO - root - 2017-12-06 05:40:26.621779: step 18030, loss = 0.80, batch loss = 0.58 (34.3 examples/sec; 0.234 sec/batch; 20h:23m:51s remains)
INFO - root - 2017-12-06 05:40:28.904654: step 18040, loss = 0.99, batch loss = 0.78 (34.9 examples/sec; 0.229 sec/batch; 20h:00m:09s remains)
INFO - root - 2017-12-06 05:40:31.200925: step 18050, loss = 0.83, batch loss = 0.61 (37.8 examples/sec; 0.212 sec/batch; 18h:29m:45s remains)
INFO - root - 2017-12-06 05:40:33.537370: step 18060, loss = 0.76, batch loss = 0.54 (32.6 examples/sec; 0.246 sec/batch; 21h:27m:36s remains)
INFO - root - 2017-12-06 05:40:35.815098: step 18070, loss = 0.79, batch loss = 0.58 (33.9 examples/sec; 0.236 sec/batch; 20h:35m:43s remains)
INFO - root - 2017-12-06 05:40:38.113780: step 18080, loss = 0.84, batch loss = 0.63 (35.0 examples/sec; 0.229 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-06 05:40:40.457878: step 18090, loss = 0.80, batch loss = 0.58 (34.3 examples/sec; 0.234 sec/batch; 20h:23m:49s remains)
INFO - root - 2017-12-06 05:40:42.795995: step 18100, loss = 0.82, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:12m:08s remains)
2017-12-06 05:40:43.177802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.42736486 -0.45919389 -0.44016129 -0.37999386 -0.42230523 -0.49229345 -0.59958404 -0.6369949 -0.6562928 -0.72691935 -0.81336313 -0.840848 -0.80267566 -0.7160657 -0.54052037][-0.4704437 -0.64182872 -0.78023958 -0.84381461 -0.87828338 -1.0282142 -1.2337511 -1.331604 -1.3279378 -1.3234192 -1.2792839 -1.2262843 -1.1718352 -1.1265911 -0.8642388][-0.73376763 -1.0572251 -1.3559405 -1.52352 -1.5187169 -1.4275002 -1.5239602 -1.6526711 -1.6861819 -1.7541937 -1.7636894 -1.8158997 -1.7749207 -1.652377 -1.3909024][-1.1492757 -1.600091 -2.0259926 -2.4800487 -2.8205462 -2.9297481 -2.8729968 -2.83551 -2.7542713 -2.700762 -2.5827136 -2.4531989 -2.1736672 -2.0359628 -1.8482473][-1.3707918 -1.7335362 -2.2220263 -2.6695828 -3.0044754 -3.1525056 -3.1404002 -2.9709065 -2.7846506 -2.6988161 -2.6197853 -2.6420367 -2.6110604 -2.4703965 -2.2890949][-1.5720541 -1.7794303 -2.0289447 -2.1753757 -2.2839177 -2.218272 -1.8722609 -1.4930406 -1.1276399 -0.86960858 -0.78493655 -0.98332924 -1.2959944 -1.556703 -1.75024][-1.2695109 -1.4012954 -1.4368826 -1.3611012 -1.1923735 -0.89699972 -0.49481532 0.013886951 0.57243145 1.1023066 1.3039151 1.2414554 0.80378389 0.24061054 -0.33292544][-0.94877332 -0.84117639 -0.69606078 -0.48393682 -0.18989578 0.1517083 0.55915624 1.0187898 1.4512303 1.9374033 2.2258294 2.3508642 2.1278915 1.6411622 1.0539844][-0.96797 -0.77273464 -0.29080886 0.055043004 0.39498723 0.67309344 0.971063 1.206279 1.5717263 1.866275 2.0855644 2.1466265 2.0948031 1.8425831 1.4600942][-0.87151366 -0.71913791 -0.45180672 -0.12325501 0.24531183 0.48229793 0.71093267 0.91674149 0.99534678 1.1632082 1.3526165 1.4230691 1.4465967 1.4638095 1.3286859][-0.69278133 -0.71431649 -0.71316421 -0.60224295 -0.42774579 -0.24780694 -0.23062429 -0.09189941 0.03714665 0.14698321 0.11880281 0.1780647 0.1504274 0.36519209 0.57855433][-0.60836649 -0.71419835 -0.79775125 -0.90977997 -0.93600267 -1.0498486 -1.1355371 -0.99125683 -0.98022497 -0.84339577 -0.75070447 -0.76234645 -0.7555058 -0.56753063 -0.400567][-0.13017771 -0.22149613 -0.34734458 -0.43631247 -0.82341921 -1.0707433 -1.3999119 -1.6983446 -1.8642642 -1.8684348 -1.8318518 -1.6072737 -1.304142 -1.2229805 -1.1411988][0.17563853 0.45836517 0.46738961 0.16818807 -0.17508924 -0.39743465 -0.89704871 -1.4532071 -1.9283587 -2.2116072 -2.2613025 -2.1459205 -1.9488662 -1.8374376 -1.8014684][0.62790889 0.68919444 0.76537067 0.65408188 0.41752395 0.05148121 -0.350307 -0.69884872 -1.1897418 -1.2800794 -1.284889 -1.2710056 -1.3572447 -1.4677941 -1.693368]]...]
INFO - root - 2017-12-06 05:40:45.481007: step 18110, loss = 0.84, batch loss = 0.62 (35.6 examples/sec; 0.225 sec/batch; 19h:38m:08s remains)
INFO - root - 2017-12-06 05:40:47.797529: step 18120, loss = 0.77, batch loss = 0.55 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:04s remains)
INFO - root - 2017-12-06 05:40:50.140846: step 18130, loss = 0.81, batch loss = 0.59 (35.0 examples/sec; 0.229 sec/batch; 19h:57m:30s remains)
INFO - root - 2017-12-06 05:40:52.449536: step 18140, loss = 0.76, batch loss = 0.55 (34.3 examples/sec; 0.233 sec/batch; 20h:22m:20s remains)
INFO - root - 2017-12-06 05:40:54.729877: step 18150, loss = 0.77, batch loss = 0.56 (34.1 examples/sec; 0.235 sec/batch; 20h:29m:42s remains)
INFO - root - 2017-12-06 05:40:57.024089: step 18160, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:29s remains)
INFO - root - 2017-12-06 05:40:59.312999: step 18170, loss = 0.80, batch loss = 0.59 (35.3 examples/sec; 0.227 sec/batch; 19h:46m:59s remains)
INFO - root - 2017-12-06 05:41:01.653218: step 18180, loss = 0.79, batch loss = 0.57 (35.3 examples/sec; 0.227 sec/batch; 19h:46m:59s remains)
INFO - root - 2017-12-06 05:41:03.953629: step 18190, loss = 0.81, batch loss = 0.60 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-06 05:41:06.276129: step 18200, loss = 0.88, batch loss = 0.66 (33.3 examples/sec; 0.240 sec/batch; 20h:58m:12s remains)
2017-12-06 05:41:06.672153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.78987819 -1.0695634 -1.4441745 -1.7026526 -1.851568 -1.8866808 -1.8996862 -1.8667835 -1.7303232 -1.5050256 -1.2765176 -1.1444848 -1.1504298 -1.221895 -1.2863243][-2.1415734 -2.4182906 -2.8405039 -3.2866988 -3.5099447 -3.4896095 -3.4435928 -3.3481669 -3.2677419 -2.9965298 -2.5069606 -1.9725177 -1.5710901 -1.3810344 -1.3743573][-3.2637408 -3.4403131 -3.7117774 -4.1077285 -4.3601952 -4.4839454 -4.4559774 -4.3280087 -4.1581621 -3.89632 -3.4474688 -2.7277865 -2.0104091 -1.5210245 -1.2285047][-3.5253751 -3.4824269 -3.465445 -3.526346 -3.5675466 -3.6198308 -3.6524413 -3.6654029 -3.6260254 -3.5494444 -3.2907808 -2.7300658 -2.0424469 -1.3977236 -0.95901829][-2.6651092 -2.3474977 -1.9159724 -1.5007052 -1.1059775 -1.0060345 -1.2013236 -1.5302744 -1.7633488 -2.161124 -2.4033308 -2.2330422 -1.7497023 -1.2164512 -0.86550796][-1.6286172 -1.1545049 -0.27050921 0.76878422 1.6782563 2.2176447 2.0732071 1.3302394 0.45729598 -0.43649936 -1.0911162 -1.443203 -1.3925456 -1.0659657 -0.73800093][-0.84371656 -0.39402631 0.55478841 1.7932407 3.1049943 4.1100249 4.4397435 3.9281728 2.7665641 1.3133168 0.16621307 -0.55588388 -0.77659971 -0.67463994 -0.45664036][0.009067364 0.37036592 1.1610591 2.2190778 3.4004226 4.4571548 4.9606042 4.699264 3.7923405 2.4292414 1.0390283 0.11602824 -0.19033615 -0.20464945 -0.072760522][0.16393262 0.31131262 0.84877867 1.5951816 2.5264957 3.4807634 4.0505738 4.1447983 3.6099498 2.6711049 1.6476922 0.7807548 0.36261305 0.22357306 0.247576][-0.15977511 -0.37937096 -0.39306307 -0.1231754 0.46311405 1.2781394 1.9228541 2.3205459 2.245512 1.7972636 1.2075033 0.71677434 0.42727807 0.2011795 0.15482664][-0.22669336 -0.6285885 -1.1091995 -1.3914077 -1.4203542 -1.1318034 -0.73486018 -0.25812945 0.061841018 0.18032783 0.1153998 0.081385054 0.10521092 0.098167248 0.10289287][-0.48168248 -0.75875509 -1.227326 -1.7078458 -1.986528 -2.1135554 -2.1744285 -2.0410423 -1.7818851 -1.4964709 -1.1468346 -0.76943719 -0.38567528 -0.13735989 0.12879169][-0.61002505 -0.764147 -1.0863177 -1.4861225 -1.9656951 -2.2603409 -2.3590958 -2.4259522 -2.4700518 -2.310235 -2.0322239 -1.6474853 -1.0839486 -0.72034913 -0.25899959][-0.26565138 -0.36825696 -0.69280112 -1.062214 -1.3912011 -1.725265 -2.0284646 -2.1472225 -2.1247032 -2.1416671 -2.1327538 -1.856624 -1.5041485 -1.1601198 -0.72396034][0.21123984 -0.0027142689 -0.35996744 -0.81621093 -1.1845773 -1.3791701 -1.5114336 -1.6168908 -1.7874029 -1.6704476 -1.4616379 -1.4318929 -1.4358084 -1.1952382 -0.94408959]]...]
INFO - root - 2017-12-06 05:41:08.987880: step 18210, loss = 0.82, batch loss = 0.61 (36.1 examples/sec; 0.222 sec/batch; 19h:20m:19s remains)
INFO - root - 2017-12-06 05:41:11.283579: step 18220, loss = 0.81, batch loss = 0.60 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:25s remains)
INFO - root - 2017-12-06 05:41:13.623526: step 18230, loss = 0.77, batch loss = 0.56 (34.4 examples/sec; 0.232 sec/batch; 20h:17m:32s remains)
INFO - root - 2017-12-06 05:41:15.911581: step 18240, loss = 0.79, batch loss = 0.57 (36.3 examples/sec; 0.221 sec/batch; 19h:15m:40s remains)
INFO - root - 2017-12-06 05:41:18.207982: step 18250, loss = 0.77, batch loss = 0.56 (33.8 examples/sec; 0.237 sec/batch; 20h:39m:17s remains)
INFO - root - 2017-12-06 05:41:20.502081: step 18260, loss = 0.79, batch loss = 0.57 (34.9 examples/sec; 0.229 sec/batch; 20h:01m:55s remains)
INFO - root - 2017-12-06 05:41:22.833349: step 18270, loss = 0.76, batch loss = 0.54 (33.8 examples/sec; 0.237 sec/batch; 20h:40m:42s remains)
INFO - root - 2017-12-06 05:41:25.148601: step 18280, loss = 0.72, batch loss = 0.51 (34.5 examples/sec; 0.232 sec/batch; 20h:15m:36s remains)
INFO - root - 2017-12-06 05:41:27.432405: step 18290, loss = 0.97, batch loss = 0.76 (32.7 examples/sec; 0.244 sec/batch; 21h:20m:12s remains)
INFO - root - 2017-12-06 05:41:29.749350: step 18300, loss = 0.79, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 19h:40m:48s remains)
2017-12-06 05:41:30.154504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063818581 -0.063237585 -0.06307812 -0.063055083 -0.063340552 -0.063827828 -0.064306274 -0.064547017 -0.064601421 -0.064868033 -0.06503547 -0.065147743 -0.06529668 -0.065352052 -0.065213382][-0.064107426 -0.063138828 -0.062686123 -0.062661082 -0.062895656 -0.063380584 -0.063786194 -0.06406711 -0.064196363 -0.064399414 -0.064595982 -0.064775467 -0.065006912 -0.065107927 -0.065045685][-0.05883785 -0.056292221 -0.0553638 -0.0556672 -0.056968924 -0.059371091 -0.061477996 -0.0629723 -0.063720584 -0.064104207 -0.064330414 -0.064491048 -0.064746939 -0.064896636 -0.064852566][-0.041035779 -0.032947987 -0.028642993 -0.028910283 -0.033857916 -0.041761789 -0.048904296 -0.055273589 -0.060033139 -0.062802918 -0.063847527 -0.064196959 -0.064541645 -0.0647853 -0.064783454][-0.082925461 -0.063378409 -0.044039667 -0.025033146 -0.011730671 -0.0060582608 -0.0078074709 -0.016012967 -0.025199264 -0.035332091 -0.045099061 -0.0529249 -0.058767851 -0.062327769 -0.063988067][-0.028971396 -0.0046103 0.020154335 0.034546115 0.0402947 0.043424942 0.046702735 0.045641102 0.039381735 0.030445561 0.019808024 0.0013422444 -0.018429279 -0.036271311 -0.04917806][0.26287121 0.33187214 0.38830385 0.43319526 0.4573305 0.45785162 0.4431009 0.4097175 0.35994172 0.29288277 0.23320374 0.16388527 0.098405577 0.043114923 0.0020345151][0.40855795 0.5264799 0.62309068 0.70270962 0.7519635 0.76890743 0.76448119 0.74725914 0.71663469 0.64967 0.56502807 0.49255976 0.38984984 0.26959595 0.15966097][0.15562254 0.17975277 0.18846703 0.18644181 0.17803887 0.16769767 0.15150964 0.16708097 0.17607802 0.18223459 0.17750207 0.16514713 0.16593891 0.13140559 0.082689963][-0.059129465 -0.094585776 -0.13584664 -0.17393577 -0.20938629 -0.23287165 -0.25581896 -0.26550889 -0.2867403 -0.29168779 -0.26680332 -0.21082549 -0.15518482 -0.10852993 -0.0681248][-0.044140693 -0.025013164 -0.00757432 -0.01481124 -0.033537436 -0.059422061 -0.082851261 -0.11833744 -0.14602108 -0.16787875 -0.170062 -0.16182631 -0.1495724 -0.12411693 -0.095313363][-0.24204326 -0.24845991 -0.24031407 -0.2423701 -0.24791682 -0.25943288 -0.25169891 -0.25085858 -0.24599534 -0.23233438 -0.20593321 -0.17017642 -0.14209718 -0.13072135 -0.13267627][-0.46488884 -0.50819194 -0.52961892 -0.52803439 -0.50733894 -0.50197864 -0.48385945 -0.47172251 -0.45166007 -0.422275 -0.376429 -0.31550539 -0.24525747 -0.19613966 -0.15555191][-0.45511103 -0.4802407 -0.4839296 -0.46918979 -0.44536737 -0.41623917 -0.38723704 -0.36782005 -0.33638513 -0.29285422 -0.24085751 -0.18816045 -0.11937025 -0.080605492 -0.058474571][-0.22685331 -0.22264093 -0.20933574 -0.19291449 -0.17674527 -0.17249954 -0.17228067 -0.17286529 -0.16126803 -0.13831471 -0.1090613 -0.0735746 -0.036137916 -0.01115223 -0.0025254264]]...]
INFO - root - 2017-12-06 05:41:32.461258: step 18310, loss = 0.83, batch loss = 0.62 (36.1 examples/sec; 0.221 sec/batch; 19h:19m:38s remains)
INFO - root - 2017-12-06 05:41:34.794489: step 18320, loss = 0.72, batch loss = 0.50 (34.4 examples/sec; 0.233 sec/batch; 20h:17m:52s remains)
INFO - root - 2017-12-06 05:41:37.091419: step 18330, loss = 0.80, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 19h:39m:59s remains)
INFO - root - 2017-12-06 05:41:39.425414: step 18340, loss = 0.78, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:15m:08s remains)
INFO - root - 2017-12-06 05:41:41.738190: step 18350, loss = 0.80, batch loss = 0.58 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-06 05:41:44.026086: step 18360, loss = 0.76, batch loss = 0.55 (33.8 examples/sec; 0.236 sec/batch; 20h:37m:31s remains)
INFO - root - 2017-12-06 05:41:46.330204: step 18370, loss = 0.80, batch loss = 0.59 (33.0 examples/sec; 0.242 sec/batch; 21h:08m:58s remains)
INFO - root - 2017-12-06 05:41:48.636310: step 18380, loss = 0.80, batch loss = 0.59 (37.6 examples/sec; 0.213 sec/batch; 18h:33m:43s remains)
INFO - root - 2017-12-06 05:41:50.955916: step 18390, loss = 0.78, batch loss = 0.57 (33.8 examples/sec; 0.236 sec/batch; 20h:37m:28s remains)
INFO - root - 2017-12-06 05:41:53.205275: step 18400, loss = 0.88, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:18s remains)
2017-12-06 05:41:54.266341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.073090725 -0.33961311 -0.6801666 -0.57858759 -0.49986127 -0.62766129 -0.69142449 -0.646887 -0.60558087 -0.6810897 -0.72721881 -0.66433817 -0.53405708 -0.397255 -0.39207828][-1.0357459 -1.2724773 -1.5040617 -1.65383 -1.520519 -1.6803701 -1.6432998 -1.4547883 -1.2202302 -1.2049675 -1.130209 -0.77697122 -0.56883049 -0.58215404 -0.49210724][-1.4242172 -1.8443398 -2.1750612 -2.3901355 -2.4273353 -2.752281 -2.9737582 -3.1622288 -2.8086028 -2.297739 -1.8490714 -1.4749742 -1.1073283 -0.69777691 -0.4304451][-2.0781291 -2.24991 -2.8576748 -3.4377604 -3.9177275 -4.402678 -4.7710409 -4.8271427 -4.3270326 -3.946105 -3.3627605 -2.496659 -1.8377498 -1.4093852 -1.0451883][-1.5021315 -2.2274821 -3.233614 -4.0442638 -4.6794062 -5.1228542 -5.4631343 -5.7215085 -5.4013672 -4.6159568 -3.9013455 -3.3798432 -2.6574914 -1.9628897 -1.5244977][-1.7210754 -2.3561912 -2.7195756 -3.121346 -3.5641992 -4.1223264 -4.5891562 -4.5588307 -4.2108541 -3.9025726 -3.5189707 -2.9206512 -2.4480944 -2.2606184 -2.1466019][-0.80888575 -1.318864 -1.7445585 -1.8730348 -2.2232902 -2.2144141 -2.0798128 -1.9236994 -1.6492715 -1.2280802 -1.0202988 -1.1739306 -1.3799534 -1.3713677 -1.4123763][0.0065513104 -0.31389457 -0.263978 0.084311716 0.16667455 0.32793429 0.52046144 0.80706275 0.97453952 0.91245389 0.84445757 0.68729687 0.27860188 -0.35565659 -0.75849652][0.77528238 0.68348867 1.0439309 1.3824302 1.7789174 2.1548584 2.5262046 2.8253739 2.9750674 2.7215405 2.1794584 1.6708633 1.079246 0.56148571 0.05093687][0.098760419 0.16801059 0.76308441 1.5063473 2.2041435 2.3847716 2.6757169 3.0944867 3.3105955 3.0018876 2.5692465 1.9328133 0.95581293 0.35486263 0.19458038][-0.27181807 -0.24215657 0.043566696 0.68833 1.3398955 1.5955954 1.8619789 1.9649712 1.9515561 1.8668474 1.5596416 1.0943124 0.55635291 0.26761565 -0.041240133][-0.98635519 -1.0692972 -0.90089893 -0.36016393 0.061695553 0.10876537 0.1859473 0.29886076 0.40865776 0.44806275 0.38160631 0.32323825 0.26478434 0.29666337 0.34364218][-1.4384623 -1.1975187 -1.4268985 -1.4583348 -1.5601006 -1.5678445 -1.5449364 -1.6164252 -1.2690967 -0.93192738 -0.59365112 -0.18911502 0.24102482 0.33227709 0.4146103][-1.802884 -1.5326667 -1.6922574 -1.9148009 -2.2339051 -2.4315927 -2.758383 -2.6781626 -2.3636923 -1.9548756 -1.3251233 -0.95197535 -0.55476522 -0.1880234 -0.083572321][-0.8765626 -1.3291091 -1.6081164 -2.2850595 -3.0035536 -3.1840808 -3.7413926 -3.6469536 -3.2916667 -2.5280395 -1.6487622 -1.135736 -0.71137691 -0.60845762 -0.49821556]]...]
INFO - root - 2017-12-06 05:41:56.594815: step 18410, loss = 0.75, batch loss = 0.53 (34.3 examples/sec; 0.233 sec/batch; 20h:19m:36s remains)
INFO - root - 2017-12-06 05:41:58.912450: step 18420, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 20h:11m:14s remains)
INFO - root - 2017-12-06 05:42:01.226500: step 18430, loss = 0.78, batch loss = 0.56 (34.7 examples/sec; 0.231 sec/batch; 20h:07m:40s remains)
INFO - root - 2017-12-06 05:42:03.528715: step 18440, loss = 0.81, batch loss = 0.60 (33.5 examples/sec; 0.239 sec/batch; 20h:50m:50s remains)
INFO - root - 2017-12-06 05:42:05.847238: step 18450, loss = 0.77, batch loss = 0.55 (33.5 examples/sec; 0.239 sec/batch; 20h:49m:58s remains)
INFO - root - 2017-12-06 05:42:08.170230: step 18460, loss = 0.77, batch loss = 0.55 (32.7 examples/sec; 0.244 sec/batch; 21h:19m:22s remains)
INFO - root - 2017-12-06 05:42:10.515047: step 18470, loss = 0.83, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:15m:13s remains)
INFO - root - 2017-12-06 05:42:12.875017: step 18480, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 20h:15m:06s remains)
INFO - root - 2017-12-06 05:42:15.158186: step 18490, loss = 0.83, batch loss = 0.62 (35.5 examples/sec; 0.226 sec/batch; 19h:40m:58s remains)
INFO - root - 2017-12-06 05:42:17.473342: step 18500, loss = 0.78, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:57s remains)
2017-12-06 05:42:18.301087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12610137 -0.34756884 -0.45595279 -0.8392458 -1.3893057 -1.7126467 -1.7451087 -1.6641586 -1.7114739 -1.8376566 -1.7379357 -1.1834593 -0.44430682 -0.23605227 0.033423565][0.64186466 0.34895363 0.12813674 -0.35235754 -0.84407377 -1.3424449 -1.6938713 -1.5742446 -1.5060822 -1.4741334 -1.533034 -1.0837746 -0.29742149 -0.065904215 0.096116453][0.52543175 0.20456442 0.17640239 -0.36456591 -0.83011818 -0.96365857 -1.0090569 -1.0756787 -1.3001329 -1.3135396 -1.1850604 -0.96453226 -0.64767003 -0.36178091 0.021743849][0.45227 0.057727948 -0.13603839 -0.36641729 -0.61920869 -0.81007981 -0.83002329 -0.78632319 -0.80897987 -0.82224286 -0.80119765 -0.54045767 -0.18148389 0.020940974 0.21616566][0.42051193 0.54421818 0.51192689 -0.0090956986 -0.32202315 -0.39474773 -0.41533098 -0.54825187 -0.74286044 -0.8648771 -0.86204314 -0.57353437 -0.090946667 0.45159355 0.60596871][0.809129 0.71572351 0.59747803 0.60705352 0.41725174 0.20947993 0.057138845 -0.19248335 -0.45069286 -0.76409972 -0.862929 -0.69344568 -0.33697647 -0.067245156 -0.29534107][1.1013073 1.0092639 0.95704448 0.81018424 0.49403551 0.48856398 0.24670991 0.026442155 -0.12274563 -0.64396858 -0.79108369 -1.0573941 -1.0909476 -0.68934166 -0.76003933][1.1296223 1.0156904 0.83546078 0.56990623 0.39664078 0.26440534 -0.016593628 -0.23302582 -0.45815966 -0.85137355 -1.0161939 -1.3155178 -1.3242574 -1.2072867 -1.3724064][0.79993582 0.872985 0.55428004 0.12955403 -0.21971446 -0.45342484 -0.60078526 -0.77389657 -1.0701352 -1.2800267 -1.3933774 -1.3191973 -1.0243859 -0.93888962 -1.0404476][-0.01359053 -0.20366207 -0.42307505 -0.89225447 -1.1628401 -1.314378 -1.2895595 -1.3943161 -1.6097437 -1.514919 -1.3561254 -0.95081151 -0.7556597 -0.5202927 -0.39913276][-0.48620552 -0.52168834 -0.7475673 -1.4318907 -1.6746634 -1.8737546 -1.9288576 -2.0977106 -2.1831312 -2.1247463 -1.8263392 -1.2412276 -0.780892 -0.621263 -0.65723407][-0.85787642 -1.2412943 -1.379946 -1.7618474 -1.9702152 -2.1275918 -2.2360947 -2.4452384 -2.3470418 -1.8987951 -1.6059424 -1.3914051 -1.1924233 -0.70205271 -0.3186312][0.07259725 -0.28386936 -0.53721464 -0.60943449 -0.5927304 -0.78131211 -1.0140618 -1.1332767 -1.1234246 -0.84663355 -0.28652358 0.17030841 0.28269649 0.50674236 0.94487059][0.3940255 0.505684 0.76865256 0.79244363 0.65389788 0.52918315 0.33106107 -0.017957143 -0.065307274 0.044795275 0.23970377 0.67749119 1.190241 1.4625589 1.6627996][0.96430242 0.97302651 0.89719677 1.1214116 1.2946939 1.3162082 1.0714847 0.584635 0.42812845 0.4508172 0.56858635 0.77881038 1.1572568 1.5454751 1.9700618]]...]
INFO - root - 2017-12-06 05:42:20.635101: step 18510, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 19h:37m:28s remains)
INFO - root - 2017-12-06 05:42:22.961302: step 18520, loss = 0.79, batch loss = 0.57 (34.1 examples/sec; 0.235 sec/batch; 20h:27m:51s remains)
INFO - root - 2017-12-06 05:42:25.252040: step 18530, loss = 0.75, batch loss = 0.54 (33.5 examples/sec; 0.238 sec/batch; 20h:47m:59s remains)
INFO - root - 2017-12-06 05:42:27.547537: step 18540, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 20h:18m:34s remains)
INFO - root - 2017-12-06 05:42:29.897371: step 18550, loss = 0.75, batch loss = 0.53 (34.6 examples/sec; 0.232 sec/batch; 20h:11m:27s remains)
INFO - root - 2017-12-06 05:42:32.173990: step 18560, loss = 0.84, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 19h:21m:22s remains)
INFO - root - 2017-12-06 05:42:34.437997: step 18570, loss = 0.82, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 20h:00m:42s remains)
INFO - root - 2017-12-06 05:42:36.756687: step 18580, loss = 0.76, batch loss = 0.54 (34.8 examples/sec; 0.230 sec/batch; 20h:04m:25s remains)
INFO - root - 2017-12-06 05:42:39.071013: step 18590, loss = 0.74, batch loss = 0.52 (35.7 examples/sec; 0.224 sec/batch; 19h:31m:32s remains)
INFO - root - 2017-12-06 05:42:41.344426: step 18600, loss = 0.82, batch loss = 0.61 (33.5 examples/sec; 0.238 sec/batch; 20h:47m:41s remains)
2017-12-06 05:42:43.244685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6999196 -2.0124071 -2.2725141 -2.4911711 -2.7087822 -2.7102919 -2.6086152 -2.5053191 -2.229156 -1.8949164 -1.6145298 -1.4626443 -1.2747554 -1.2398301 -1.3084942][-1.2004684 -1.6927366 -2.2371833 -2.6131952 -2.8429339 -2.8577909 -2.7513778 -2.5756178 -2.2732608 -1.986485 -1.6070929 -1.3228449 -1.0267991 -0.84465337 -0.63636416][-0.55905294 -1.2238871 -1.7260389 -2.1778026 -2.5532162 -2.6112244 -2.5768437 -2.5004594 -2.4376986 -2.2469833 -1.9009944 -1.5552081 -1.2315536 -0.94427383 -0.60641968][-0.20081753 -0.73686868 -0.98327726 -1.2993236 -1.4982071 -1.5339128 -1.5663546 -1.6864486 -1.7829254 -1.897498 -1.9040627 -1.654848 -1.42845 -1.0643183 -0.6471324][0.65678966 0.3406238 0.11992014 -0.19799666 -0.35035676 -0.46108946 -0.58789694 -0.7467742 -0.82004207 -0.98643637 -1.1008906 -1.2479154 -1.3601041 -1.1516385 -0.79142964][0.92223752 0.77012336 0.7609899 0.64294922 0.58484775 0.502301 0.45443052 0.247729 0.1920521 0.077927805 0.07160195 -0.039451305 -0.18972731 -0.36529258 -0.42096281][0.3147347 0.14705482 0.14070871 0.34251294 0.66779566 1.0654674 1.5130737 1.6145413 1.7201077 1.6658851 1.6404828 1.3086402 0.93346441 0.77635723 0.66484463][-0.3513428 -0.40046915 -0.26341355 0.075601868 0.61853904 1.2212368 1.8213848 2.3857446 2.8536282 3.0435333 3.2497141 2.8187654 2.1797936 1.5733718 1.1775098][-0.62614506 -0.41868418 -0.074469864 0.30309832 0.75569421 1.2057631 1.6871965 2.2015159 2.6996088 3.0247123 3.209898 3.1165311 2.7899613 2.1239283 1.5516609][-0.58825237 -0.40143272 -0.12614739 0.15260291 0.44581652 0.68625855 0.872473 1.2164117 1.5304505 1.806129 2.0166507 1.8795398 1.6619253 1.3340777 0.91342342][-0.68524629 -0.6137442 -0.50419319 -0.54880822 -0.55088705 -0.55211359 -0.56804031 -0.45890763 -0.40318665 -0.21696931 -0.019877739 0.096794508 0.10790125 -0.12048231 -0.31706583][-1.1083674 -1.365635 -1.5680307 -1.6292647 -1.7633953 -1.9446762 -2.0481772 -2.1045437 -2.2286024 -2.2419393 -2.1643362 -2.0361338 -1.8614168 -1.6689124 -1.5243558][-1.921286 -2.270189 -2.6795881 -2.8571262 -3.0744519 -3.254102 -3.3519363 -3.3721545 -3.3824911 -3.3485889 -3.3289654 -3.1450548 -2.8944118 -2.6258278 -2.3814139][-2.3545747 -2.6499994 -3.0364358 -3.2535052 -3.5298066 -3.7397945 -3.8420062 -3.9271226 -3.9496577 -3.8175414 -3.6336277 -3.3813851 -3.1672177 -2.8641834 -2.4706898][-1.8333994 -2.0944479 -2.3730452 -2.5196004 -2.8139362 -3.0465984 -3.2051303 -3.3843114 -3.4429502 -3.3105659 -3.0975962 -2.7815306 -2.4935861 -2.1907587 -1.8705735]]...]
INFO - root - 2017-12-06 05:42:45.533918: step 18610, loss = 0.82, batch loss = 0.61 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:30s remains)
INFO - root - 2017-12-06 05:42:47.850155: step 18620, loss = 0.83, batch loss = 0.61 (31.6 examples/sec; 0.253 sec/batch; 22h:02m:55s remains)
INFO - root - 2017-12-06 05:42:50.154454: step 18630, loss = 0.84, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 19h:47m:36s remains)
INFO - root - 2017-12-06 05:42:52.438890: step 18640, loss = 0.72, batch loss = 0.51 (34.7 examples/sec; 0.231 sec/batch; 20h:07m:30s remains)
INFO - root - 2017-12-06 05:42:54.758318: step 18650, loss = 0.77, batch loss = 0.56 (35.2 examples/sec; 0.227 sec/batch; 19h:49m:58s remains)
INFO - root - 2017-12-06 05:42:57.022646: step 18660, loss = 0.82, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 20h:09m:13s remains)
INFO - root - 2017-12-06 05:42:59.364885: step 18670, loss = 0.76, batch loss = 0.55 (33.8 examples/sec; 0.237 sec/batch; 20h:37m:21s remains)
INFO - root - 2017-12-06 05:43:01.655196: step 18680, loss = 0.78, batch loss = 0.56 (35.0 examples/sec; 0.229 sec/batch; 19h:56m:05s remains)
INFO - root - 2017-12-06 05:43:03.963839: step 18690, loss = 0.78, batch loss = 0.57 (35.1 examples/sec; 0.228 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-06 05:43:06.224689: step 18700, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.226 sec/batch; 19h:39m:25s remains)
2017-12-06 05:43:07.607251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40380302 -0.6466409 -0.77014804 -0.833576 -0.733928 -0.54658395 -0.41173416 -0.36187255 -0.338957 -0.25748619 -0.21364635 -0.178889 -0.16421205 -0.14017549 -0.10325928][0.15182713 -0.030555651 -0.29840508 -0.62610912 -0.70206344 -0.61314279 -0.48553553 -0.43736297 -0.4055047 -0.27760738 -0.21435314 -0.14548805 -0.10287952 -0.081962325 -0.073237285][0.66173047 0.75474524 0.66200131 0.26341581 -0.16260782 -0.27570939 -0.24738747 -0.31030068 -0.36094248 -0.24273852 -0.19025138 -0.14337192 -0.11437422 -0.063246258 -0.047054477][1.0141051 1.4034929 1.5305376 1.2723838 0.60946715 0.16233513 -0.04912534 -0.1149338 -0.22092515 -0.10708413 -0.059262045 -0.028240468 -0.030554235 -0.026768923 -0.053982817][1.3674967 1.8982109 2.1962016 2.0894222 1.3000231 0.70687383 0.23401102 -0.031205736 -0.24142346 -0.13548142 -0.11877789 -0.098852284 -0.11939318 -0.077685066 -0.0929503][1.3312819 2.2152338 2.7304282 2.7659526 2.0464752 1.3182255 0.57676172 0.19322562 -0.040069517 -0.019924395 0.00025755167 0.011790723 -0.055482198 -0.059970837 -0.10107293][0.8759231 1.7275468 2.3693857 2.6633775 2.1718 1.494736 0.76970279 0.33207896 0.037419386 -0.016788468 0.0070500597 0.013293743 -0.010389484 -0.034503818 -0.072356895][0.45039743 1.1164954 1.6462953 1.8630587 1.5611768 1.1890801 0.63812834 0.28589091 0.055419914 0.012984663 0.0037400648 -0.007261306 -0.021510184 -0.032910679 -0.055372838][0.17257044 0.57424545 0.9834488 1.1939335 0.9672572 0.7589286 0.42288941 0.25114852 0.093016036 0.036277719 -0.00218343 -0.036666095 -0.063554525 -0.07378611 -0.074902184][-0.27195221 0.11594995 0.54558837 0.60979682 0.52183497 0.41201836 0.15544376 0.09703695 0.0323572 0.0082834437 -0.0026334226 -0.02737914 -0.056396335 -0.067545623 -0.077558652][-0.46019673 -0.39336556 -0.11146519 0.07516817 0.1079232 0.092547856 0.0068279281 -0.057343967 -0.11493452 -0.1171207 -0.1000622 -0.077808313 -0.07135506 -0.070957161 -0.068129666][-0.40196621 -0.37302476 -0.27962634 -0.18777858 -0.10867637 0.0099099427 -0.039271142 -0.064506263 -0.071346261 -0.071977064 -0.070387304 -0.071475223 -0.074286461 -0.07673344 -0.078646183][-0.31196293 -0.38020387 -0.33464605 -0.2059132 -0.21860543 -0.12464288 -0.12092112 -0.09461166 -0.08302477 -0.08159072 -0.07916715 -0.078257173 -0.077367067 -0.079360537 -0.082298659][-0.099683061 -0.13957535 -0.14973462 -0.12676725 -0.17936704 -0.11611761 -0.15871704 -0.12188599 -0.089787543 -0.081241913 -0.079304345 -0.079669155 -0.0801016 -0.080926292 -0.082032137][-0.075039089 -0.12364518 -0.145502 -0.1641257 -0.17796509 -0.13229774 -0.14543581 -0.10983899 -0.084312029 -0.080710873 -0.079265639 -0.080178089 -0.080597945 -0.081090972 -0.081908688]]...]
INFO - root - 2017-12-06 05:43:09.894213: step 18710, loss = 0.85, batch loss = 0.64 (33.9 examples/sec; 0.236 sec/batch; 20h:32m:36s remains)
INFO - root - 2017-12-06 05:43:12.145626: step 18720, loss = 0.79, batch loss = 0.58 (35.7 examples/sec; 0.224 sec/batch; 19h:33m:11s remains)
INFO - root - 2017-12-06 05:43:14.470527: step 18730, loss = 0.87, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 20h:28m:55s remains)
INFO - root - 2017-12-06 05:43:16.749144: step 18740, loss = 0.73, batch loss = 0.52 (35.6 examples/sec; 0.225 sec/batch; 19h:35m:15s remains)
INFO - root - 2017-12-06 05:43:19.087924: step 18750, loss = 0.75, batch loss = 0.54 (34.7 examples/sec; 0.231 sec/batch; 20h:05m:20s remains)
INFO - root - 2017-12-06 05:43:21.441806: step 18760, loss = 0.75, batch loss = 0.54 (34.5 examples/sec; 0.232 sec/batch; 20h:11m:22s remains)
INFO - root - 2017-12-06 05:43:23.768646: step 18770, loss = 0.79, batch loss = 0.57 (36.0 examples/sec; 0.222 sec/batch; 19h:23m:23s remains)
INFO - root - 2017-12-06 05:43:26.110949: step 18780, loss = 0.84, batch loss = 0.63 (35.5 examples/sec; 0.226 sec/batch; 19h:39m:45s remains)
INFO - root - 2017-12-06 05:43:28.399863: step 18790, loss = 0.94, batch loss = 0.73 (34.8 examples/sec; 0.230 sec/batch; 20h:01m:37s remains)
INFO - root - 2017-12-06 05:43:30.701113: step 18800, loss = 0.74, batch loss = 0.52 (35.0 examples/sec; 0.228 sec/batch; 19h:54m:13s remains)
2017-12-06 05:43:31.461735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5988067 -1.9580945 -2.0097547 -2.1054265 -2.0101237 -1.6237459 -0.93846071 -0.24749228 0.52884227 0.97966588 1.1637138 1.382457 1.4260485 1.5030982 1.7007321][-1.5398909 -1.8601947 -2.0667217 -2.0328345 -2.0475082 -1.6743418 -1.0524442 -0.55693722 0.22952247 0.86214936 1.3541187 1.7053937 1.9345683 1.9516653 2.0805936][-1.6556824 -1.7238221 -1.6632029 -1.7890717 -1.805858 -1.7523034 -1.600917 -1.176833 -0.57152224 -0.095958576 0.43105382 0.67340827 1.0371952 1.1283704 1.2181593][-1.1233279 -1.2530237 -1.1960735 -1.2091264 -1.3466023 -1.5554602 -1.4420604 -1.4525143 -1.3118299 -0.88085306 -0.51123881 -0.28999081 -0.16818196 -0.083074391 0.10955475][-0.67796636 -0.5560146 -0.40674588 -0.49641016 -0.60509932 -0.95978642 -1.3893765 -1.464074 -1.2589158 -1.0616881 -0.85467321 -0.72473019 -0.70811576 -0.80352485 -0.72577387][0.14042181 0.015619114 0.093842708 0.11372059 0.15647554 -0.081632018 -0.21863565 -0.58565503 -0.71045256 -0.72491175 -0.65240043 -0.7582441 -0.81900597 -0.97129828 -1.0729922][0.89545256 1.1156737 1.0816689 1.0026439 1.0576376 0.86079949 0.76605564 0.52822018 0.15963709 -0.18442141 -0.29806265 -0.44239455 -0.69941944 -0.94966489 -1.1355487][1.056939 1.0979153 1.1943153 1.3441043 1.2859449 1.2070756 1.0272406 0.86726445 0.690923 0.15711746 -0.13889214 -0.41458079 -0.544749 -0.80573618 -0.98025745][0.755195 0.6975562 0.7355817 0.79129869 0.85208619 0.94269633 0.96485126 0.78610438 0.71359861 0.45906395 0.31501931 0.10342034 -0.27774638 -0.667603 -0.8626368][0.046187527 0.12930948 0.22006786 0.27026421 0.28641072 0.37265387 0.51385075 0.54011375 0.49618334 0.31822094 0.32733473 0.19162124 0.099444471 -0.13454658 -0.44099471][-0.66244543 -0.55152762 -0.45438966 -0.42499825 -0.35921341 -0.32817695 -0.42630646 -0.27823618 -0.0531222 -0.075935572 0.020699419 -0.04348 0.12351134 0.0703704 -0.042294141][-0.95974392 -1.0174747 -1.0053834 -1.0363476 -1.063172 -0.92950207 -0.94401479 -0.91621989 -0.82869077 -0.76908958 -0.53128594 -0.24990711 -0.044244293 0.057721563 0.089446105][-1.0124182 -1.1203408 -1.1579161 -1.2273004 -1.2886455 -1.2329721 -1.1687359 -1.0014638 -0.87788308 -0.86565459 -0.60733104 -0.34037861 -0.20655861 -0.0076118559 0.11655008][-0.65090191 -0.95002514 -0.99753892 -1.0786556 -1.0780731 -1.0659382 -0.997057 -0.86476475 -0.69929647 -0.63514048 -0.55188137 -0.53101361 -0.40295261 -0.21022458 -0.0090807453][-0.50697988 -0.70073766 -0.85224611 -0.887635 -0.91396308 -0.7792955 -0.70817614 -0.59705311 -0.49072343 -0.55305022 -0.42827684 -0.4091793 -0.30255449 -0.22245479 -0.13159977]]...]
INFO - root - 2017-12-06 05:43:33.708928: step 18810, loss = 0.71, batch loss = 0.49 (35.9 examples/sec; 0.223 sec/batch; 19h:25m:12s remains)
INFO - root - 2017-12-06 05:43:35.977929: step 18820, loss = 0.77, batch loss = 0.56 (30.8 examples/sec; 0.259 sec/batch; 22h:35m:48s remains)
INFO - root - 2017-12-06 05:43:38.284840: step 18830, loss = 0.78, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 20h:13m:55s remains)
INFO - root - 2017-12-06 05:43:40.618108: step 18840, loss = 0.83, batch loss = 0.62 (34.0 examples/sec; 0.235 sec/batch; 20h:30m:30s remains)
INFO - root - 2017-12-06 05:43:42.891190: step 18850, loss = 0.76, batch loss = 0.54 (34.3 examples/sec; 0.233 sec/batch; 20h:17m:31s remains)
INFO - root - 2017-12-06 05:43:45.177835: step 18860, loss = 0.75, batch loss = 0.54 (34.9 examples/sec; 0.229 sec/batch; 19h:57m:58s remains)
INFO - root - 2017-12-06 05:43:47.532638: step 18870, loss = 0.78, batch loss = 0.57 (35.8 examples/sec; 0.223 sec/batch; 19h:26m:47s remains)
INFO - root - 2017-12-06 05:43:49.810864: step 18880, loss = 0.83, batch loss = 0.62 (34.0 examples/sec; 0.235 sec/batch; 20h:29m:24s remains)
INFO - root - 2017-12-06 05:43:52.080391: step 18890, loss = 0.82, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 20h:17m:24s remains)
INFO - root - 2017-12-06 05:43:54.374843: step 18900, loss = 0.80, batch loss = 0.59 (37.4 examples/sec; 0.214 sec/batch; 18h:38m:28s remains)
2017-12-06 05:43:54.754440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0109026 -1.9070896 -1.895164 -2.1466217 -2.3701527 -2.4378836 -2.447787 -2.268595 -2.2844775 -2.0643265 -1.5794564 -1.3339489 -1.0109663 -0.37586761 0.36670023][-2.7550333 -2.9374869 -3.1299417 -3.1838424 -3.2357416 -3.2710309 -3.2552688 -3.0655653 -2.9262273 -2.8416116 -2.9542508 -2.7787058 -2.2260885 -1.6705559 -0.854581][-3.0074668 -3.1547685 -3.2063367 -3.246176 -3.2464359 -3.2009578 -3.0112627 -3.0753715 -3.1968305 -3.1225893 -3.3086448 -3.3420541 -3.3728461 -2.8624184 -2.2415555][-2.198293 -2.7088935 -2.982146 -2.5153637 -2.0441437 -1.9513862 -1.8560994 -2.2053058 -2.4241726 -2.8388293 -3.2078948 -3.1664627 -3.2635472 -3.1399295 -3.0315957][-1.6379989 -1.9175751 -1.9774191 -1.7008017 -1.1783941 -0.36878973 0.0430101 -0.38331753 -0.81056565 -1.4813999 -2.0103211 -2.2659266 -2.4083691 -2.2595766 -2.1313369][-0.11465632 -0.33255309 -0.37321389 -0.03676226 0.47594047 1.2357645 1.715737 1.8862119 1.7822191 0.88029027 0.14823954 -0.33256447 -0.87791824 -1.250405 -1.1448894][0.8718949 1.100723 1.5755603 2.0386143 2.4641347 3.1471467 3.4943721 3.5328121 3.3533998 2.5616527 1.8725563 1.0157081 0.77712929 0.56665546 0.14458442][2.0438724 2.0186 2.0744555 2.6890485 3.4350703 4.0543852 4.2435489 4.5845518 4.1415362 3.4464242 2.7859197 1.5639956 0.67393774 0.38878214 0.5197565][1.5051513 1.9433802 2.4124434 2.7204061 2.8916006 3.478848 3.8269653 3.729389 3.3474824 2.6299746 1.4827702 0.75630158 0.44953537 -0.19670522 -0.27577248][0.67117906 0.85853308 0.91162455 1.1991526 1.5526974 1.4817131 1.3487214 1.2667584 0.70276785 0.19361578 -0.53948528 -1.2313662 -1.48955 -1.3461 -0.663897][-0.42953169 -0.43358815 -0.47662354 -0.62394106 -0.69830006 -0.82502466 -0.89395374 -1.3814064 -2.0111947 -2.3855278 -2.6109095 -2.3525476 -1.8382771 -1.7853738 -1.2515633][-1.2404881 -1.4722053 -1.779037 -1.9646245 -2.135829 -2.4615014 -2.7197587 -3.0105851 -3.2026572 -3.2349663 -3.2799144 -2.9070179 -2.2441537 -1.5703478 -0.81745613][-1.2685822 -1.770033 -2.3515823 -2.7613895 -3.1348231 -3.2791011 -3.2447302 -3.3480771 -3.4284651 -3.379353 -3.2835011 -2.7645185 -2.3761067 -1.9797441 -1.0173948][-1.1388754 -1.5473468 -2.1208143 -2.6608875 -3.1776876 -3.3826766 -3.4566097 -3.3766704 -3.0807896 -2.7386322 -2.5685244 -2.1733212 -1.8482857 -1.4232106 -0.76346219][-1.0339803 -1.3036022 -1.713529 -2.0843778 -2.5251074 -2.5210557 -2.4971743 -2.4992454 -2.4487417 -2.3570502 -1.8988428 -1.5032676 -1.4612591 -1.0556159 -0.39376104]]...]
INFO - root - 2017-12-06 05:43:57.067971: step 18910, loss = 0.77, batch loss = 0.56 (35.0 examples/sec; 0.228 sec/batch; 19h:52m:56s remains)
INFO - root - 2017-12-06 05:43:59.414244: step 18920, loss = 0.78, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:11s remains)
INFO - root - 2017-12-06 05:44:01.709283: step 18930, loss = 0.92, batch loss = 0.70 (34.0 examples/sec; 0.236 sec/batch; 20h:31m:04s remains)
INFO - root - 2017-12-06 05:44:03.989884: step 18940, loss = 0.83, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:36m:02s remains)
INFO - root - 2017-12-06 05:44:06.321245: step 18950, loss = 0.78, batch loss = 0.57 (33.3 examples/sec; 0.241 sec/batch; 20h:57m:15s remains)
INFO - root - 2017-12-06 05:44:08.672993: step 18960, loss = 0.86, batch loss = 0.64 (34.3 examples/sec; 0.233 sec/batch; 20h:19m:49s remains)
INFO - root - 2017-12-06 05:44:11.055056: step 18970, loss = 0.83, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 19h:32m:20s remains)
INFO - root - 2017-12-06 05:44:13.350988: step 18980, loss = 0.77, batch loss = 0.56 (34.3 examples/sec; 0.234 sec/batch; 20h:20m:08s remains)
INFO - root - 2017-12-06 05:44:15.643347: step 18990, loss = 0.79, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 20h:04m:41s remains)
INFO - root - 2017-12-06 05:44:17.929813: step 19000, loss = 0.79, batch loss = 0.57 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:20s remains)
2017-12-06 05:44:18.524189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080814883 0.10066008 -0.24851644 -0.88270313 -1.6880895 -2.2199774 -2.8790622 -3.1376357 -2.9456394 -2.8029735 -2.3672888 -1.2462531 -0.58444285 -0.17008328 -0.44692603][1.3083309 0.79084277 0.31276321 0.05142688 -0.83010304 -1.9045526 -2.6638718 -2.8430367 -2.5623775 -2.3292449 -1.8662688 -1.4637619 -0.89350325 -0.40307873 -0.054815184][1.4143831 1.231534 1.1259552 0.44132316 -0.35059032 -0.91990107 -1.3947676 -1.9745187 -1.9176371 -1.5011146 -1.3146611 -1.2754141 -0.94568861 -0.44306937 -0.39986706][0.68468189 0.54730058 0.41343886 0.33921015 0.20125592 -0.84470248 -1.5773339 -1.6945069 -1.3784635 -1.3409615 -1.2418787 -0.97363263 -0.66782945 -0.55415267 -0.77160978][-0.31429854 0.06564603 0.39326286 0.26886052 0.12711111 0.019934669 -0.0813817 -0.81061095 -1.3131856 -1.2758856 -1.2122858 -1.348699 -1.1427876 -0.70791221 -0.54104406][0.056227975 0.45210862 0.70028162 1.0682997 1.1365038 0.762473 0.62504834 0.36203292 0.32299608 -0.30568835 -0.78330547 -1.0475136 -0.94404829 -1.0874531 -1.5367718][0.41181457 0.81729972 1.0797745 1.4831178 1.7300723 1.5827138 1.3716486 0.99491286 0.8877517 0.54722488 0.50917906 -0.12113045 -0.52551508 -0.27711043 -0.55424184][1.7797191 1.6680659 1.7225612 1.9710735 1.9926661 1.8913482 1.5534908 1.1666608 1.1392684 0.93706846 1.0673692 0.81184047 0.72975051 0.34639823 -0.0057584494][1.756231 2.2412412 2.0240712 1.7533014 1.5791099 1.6285301 1.7181979 1.4438618 1.1487613 0.91170669 1.23922 1.382032 1.6150252 1.360658 1.3305279][0.53036737 0.51905197 0.47689867 0.59182495 0.52018744 -0.035350736 -0.43461686 -0.287711 -0.22536057 -0.13692133 0.26685607 0.65120667 1.4045762 1.5136615 1.694279][-0.828774 -0.92520028 -1.1072829 -1.4988335 -1.920045 -1.8768088 -1.9033144 -2.0751307 -2.3224602 -1.8786834 -1.5793104 -0.90275335 -0.079181246 0.27335927 0.96825218][-1.5265497 -2.1321528 -2.8932297 -2.8578713 -2.9891486 -3.4480243 -3.7714767 -3.3607571 -2.7922187 -2.3366189 -2.2213383 -1.6568145 -0.76299214 -0.082353331 0.25882906][-1.6935956 -2.2050524 -2.4527414 -2.7514427 -3.2038612 -3.2261293 -3.4234793 -3.5807083 -3.4811282 -3.1497126 -2.6121018 -1.5165989 -0.44777206 0.014226139 -0.16707227][-1.0363266 -1.0094769 -1.4275042 -1.4755306 -1.7404205 -2.333046 -3.0987194 -3.1955829 -2.9089215 -2.2287641 -1.4284339 -1.1111883 -0.60751963 0.21140602 0.21895447][-0.029832762 -0.23716995 -0.39070421 -0.52454251 -0.96420759 -1.328733 -1.3964927 -1.583263 -1.6660936 -1.5780658 -1.2455716 -0.89069414 -0.69442594 -0.49643624 -0.32677156]]...]
INFO - root - 2017-12-06 05:44:20.793635: step 19010, loss = 0.77, batch loss = 0.55 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:54s remains)
INFO - root - 2017-12-06 05:44:23.090837: step 19020, loss = 0.85, batch loss = 0.63 (35.0 examples/sec; 0.229 sec/batch; 19h:53m:52s remains)
INFO - root - 2017-12-06 05:44:25.439067: step 19030, loss = 0.89, batch loss = 0.68 (33.9 examples/sec; 0.236 sec/batch; 20h:31m:32s remains)
INFO - root - 2017-12-06 05:44:27.779189: step 19040, loss = 0.77, batch loss = 0.55 (35.4 examples/sec; 0.226 sec/batch; 19h:39m:22s remains)
INFO - root - 2017-12-06 05:44:30.111856: step 19050, loss = 0.88, batch loss = 0.67 (33.7 examples/sec; 0.238 sec/batch; 20h:41m:21s remains)
INFO - root - 2017-12-06 05:44:32.456956: step 19060, loss = 0.78, batch loss = 0.57 (34.6 examples/sec; 0.231 sec/batch; 20h:08m:56s remains)
INFO - root - 2017-12-06 05:44:34.734232: step 19070, loss = 0.83, batch loss = 0.61 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:46s remains)
INFO - root - 2017-12-06 05:44:36.987512: step 19080, loss = 0.76, batch loss = 0.55 (36.2 examples/sec; 0.221 sec/batch; 19h:13m:18s remains)
INFO - root - 2017-12-06 05:44:39.267984: step 19090, loss = 0.76, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 20h:07m:03s remains)
INFO - root - 2017-12-06 05:44:41.551335: step 19100, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.225 sec/batch; 19h:37m:28s remains)
2017-12-06 05:44:45.035390: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.664126 -1.6910948 -1.7058939 -1.7331803 -1.8701106 -2.1785026 -2.4850802 -2.6404245 -2.7736423 -2.8292663 -2.6825688 -2.5547745 -2.0857625 -1.3689687 -0.64107227][-1.5588615 -1.4883115 -1.2765578 -1.2812827 -1.5104505 -1.8252652 -2.0171378 -2.286449 -2.6244771 -2.9139571 -3.067153 -3.1170275 -2.8212283 -2.4502573 -2.0046692][-1.3546406 -1.3183862 -1.2188286 -1.0685419 -0.93859208 -1.1064063 -1.2167863 -1.4572707 -1.789153 -2.2383742 -2.5091832 -2.7710559 -2.816292 -2.5342352 -2.1080914][-0.99548531 -0.96388179 -0.85737669 -0.71583241 -0.55737883 -0.41998768 -0.21242628 -0.27595386 -0.48785609 -0.84854871 -1.0465987 -1.4434845 -1.6668605 -1.8762839 -2.0131178][-0.31936294 -0.35961747 -0.3263526 -0.23658311 -0.06794142 0.17564137 0.46645606 0.57698256 0.41765654 0.083663091 -0.28307456 -0.56735551 -0.66134292 -0.87451035 -1.0083569][0.35820687 0.39978409 0.42654884 0.56478959 0.77445173 0.991944 1.3277843 1.5219127 1.5673213 1.3650146 0.94262034 0.65376341 0.37846482 0.045317054 -0.28197533][0.54860944 0.632627 0.71135664 0.74085844 0.80963695 1.1326864 1.5154784 1.7919025 1.9437732 1.8470356 1.7173656 1.4326185 1.0206186 0.62261564 0.27217466][0.20451014 0.21608658 0.25626165 0.35651475 0.46721941 0.56556332 0.73444164 1.0566908 1.3241998 1.4155331 1.5663903 1.5486202 1.4040805 1.013333 0.52800226][-0.47708756 -0.53320259 -0.53030473 -0.63588744 -0.68519396 -0.5652498 -0.53181446 -0.38511819 -0.11715086 0.30966192 0.67853594 0.71460092 0.649211 0.45199424 0.15049545][-0.88763124 -1.1741914 -1.4072429 -1.5421267 -1.7090698 -1.7852792 -1.664288 -1.5768536 -1.4198918 -1.046086 -0.59316844 -0.36774093 -0.291678 -0.32467607 -0.46907061][-0.80682629 -1.0934236 -1.4439512 -1.7751818 -1.9879868 -2.0649352 -2.1556921 -2.1140242 -2.0267327 -1.9148357 -1.729867 -1.4942287 -1.2485615 -1.1059783 -1.0830877][-0.6881156 -1.0013429 -1.3234758 -1.6404619 -1.9538921 -2.1589136 -2.267864 -2.3004103 -2.214952 -2.1520076 -2.1158483 -2.0277147 -1.8960911 -1.5836571 -1.2754606][-0.42492443 -0.65420961 -0.94727683 -1.2502893 -1.5070879 -1.7280796 -1.8951987 -2.0048068 -2.0251427 -1.9883308 -1.8901951 -1.7445852 -1.5532537 -1.387894 -1.1692435][-0.33253151 -0.45075029 -0.6085934 -0.8147105 -1.0302896 -1.2086575 -1.3513874 -1.3722206 -1.3741127 -1.337667 -1.3324387 -1.2677242 -1.1230817 -0.92871559 -0.74374312][-0.28039223 -0.37758982 -0.46012312 -0.54538846 -0.64268863 -0.74766773 -0.84252125 -0.81432861 -0.85074288 -0.8420105 -0.78687257 -0.683576 -0.680492 -0.60376894 -0.43752581]]...]
INFO - root - 2017-12-06 05:44:47.464023: step 19110, loss = 0.77, batch loss = 0.56 (33.0 examples/sec; 0.242 sec/batch; 21h:05m:13s remains)
INFO - root - 2017-12-06 05:44:49.784183: step 19120, loss = 0.83, batch loss = 0.61 (33.2 examples/sec; 0.241 sec/batch; 21h:00m:07s remains)
INFO - root - 2017-12-06 05:44:52.084315: step 19130, loss = 0.81, batch loss = 0.59 (35.3 examples/sec; 0.227 sec/batch; 19h:45m:03s remains)
INFO - root - 2017-12-06 05:44:54.429558: step 19140, loss = 0.82, batch loss = 0.60 (35.5 examples/sec; 0.226 sec/batch; 19h:38m:00s remains)
INFO - root - 2017-12-06 05:44:56.734977: step 19150, loss = 0.76, batch loss = 0.54 (33.7 examples/sec; 0.237 sec/batch; 20h:38m:08s remains)
INFO - root - 2017-12-06 05:44:59.058612: step 19160, loss = 0.79, batch loss = 0.58 (34.9 examples/sec; 0.229 sec/batch; 19h:57m:33s remains)
INFO - root - 2017-12-06 05:45:01.315667: step 19170, loss = 0.81, batch loss = 0.59 (34.7 examples/sec; 0.230 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-06 05:45:03.583265: step 19180, loss = 0.79, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:38s remains)
INFO - root - 2017-12-06 05:45:05.878114: step 19190, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 19h:51m:23s remains)
INFO - root - 2017-12-06 05:45:08.191302: step 19200, loss = 0.81, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:22m:36s remains)
2017-12-06 05:45:08.643982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.048653319 -0.049298823 -0.05100831 -0.054243241 -0.059718255 -0.066707738 -0.072857693 -0.077232584 -0.078002006 -0.075158149 -0.069991186 -0.06337063 -0.057652269 -0.053245157 -0.050308987][-0.04722346 -0.047285743 -0.048112649 -0.050766077 -0.056605585 -0.064772807 -0.0728012 -0.079036415 -0.081008032 -0.0790787 -0.074246824 -0.067723386 -0.061459288 -0.056128893 -0.05220174][-0.044442192 -0.043250527 -0.042875826 -0.045117702 -0.051437937 -0.059876673 -0.068603821 -0.076378293 -0.079575472 -0.07939373 -0.0764229 -0.071422666 -0.06576217 -0.059860438 -0.055195119][-0.039983887 -0.038127258 -0.037141796 -0.039442651 -0.045570571 -0.052961048 -0.06116467 -0.069031596 -0.073649973 -0.075736046 -0.075283639 -0.073030114 -0.069181859 -0.0636998 -0.058756076][-0.036297843 -0.034495849 -0.033540323 -0.035476282 -0.040606208 -0.046576377 -0.053918317 -0.061337672 -0.066955566 -0.071031839 -0.073200971 -0.073569924 -0.071445376 -0.067001492 -0.062121179][-0.035034515 -0.033209555 -0.032051712 -0.033093575 -0.0361921 -0.04052411 -0.046711549 -0.053784225 -0.060257144 -0.066233754 -0.07117001 -0.07396242 -0.073328525 -0.069977552 -0.065139115][-0.036560815 -0.033971213 -0.032152444 -0.031358376 -0.032140285 -0.0347575 -0.039455529 -0.046796419 -0.054538079 -0.062385835 -0.069653906 -0.074383676 -0.074755974 -0.071769334 -0.066866338][-0.038874175 -0.035633162 -0.033080433 -0.030965127 -0.029996559 -0.030713294 -0.03435665 -0.0421075 -0.051087782 -0.060007062 -0.067764387 -0.073022269 -0.073770583 -0.0707878 -0.066086411][-0.041350648 -0.038390148 -0.035905071 -0.03385064 -0.032752 -0.032882843 -0.035817072 -0.04260334 -0.050553575 -0.058455516 -0.065217085 -0.069436185 -0.069620505 -0.066834226 -0.062792227][-0.04392704 -0.041577324 -0.039714478 -0.038063232 -0.037274212 -0.0374243 -0.039363161 -0.044880357 -0.050759483 -0.057143264 -0.062260345 -0.064920358 -0.064923137 -0.062471639 -0.059132282][-0.046241973 -0.0444412 -0.043160461 -0.041857559 -0.041076355 -0.041026786 -0.042523615 -0.046166886 -0.050316293 -0.055020388 -0.058818012 -0.06048841 -0.060259502 -0.058116935 -0.055377394][-0.047813721 -0.046272453 -0.045226827 -0.044169344 -0.043880794 -0.043768045 -0.044854358 -0.047203 -0.050241575 -0.05339637 -0.055744283 -0.05655133 -0.056105636 -0.054299157 -0.05197075][-0.048702434 -0.047413342 -0.046737362 -0.046136606 -0.04611763 -0.046089642 -0.046741426 -0.048195291 -0.050093375 -0.051786408 -0.052902773 -0.052836191 -0.052507598 -0.051000368 -0.0495157][-0.049406286 -0.048370637 -0.048057802 -0.047770105 -0.047717959 -0.04774164 -0.048055876 -0.048815239 -0.049618989 -0.05015371 -0.050363474 -0.050081894 -0.049762003 -0.048833922 -0.048115332][-0.049874656 -0.049109865 -0.04901506 -0.0489301 -0.0489069 -0.048988335 -0.049222827 -0.049474794 -0.049574982 -0.049470436 -0.049250539 -0.049059365 -0.048741348 -0.048188612 -0.047919944]]...]
INFO - root - 2017-12-06 05:45:10.901918: step 19210, loss = 0.81, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:24m:53s remains)
INFO - root - 2017-12-06 05:45:13.212362: step 19220, loss = 0.75, batch loss = 0.54 (34.4 examples/sec; 0.232 sec/batch; 20h:13m:28s remains)
INFO - root - 2017-12-06 05:45:15.570071: step 19230, loss = 0.75, batch loss = 0.54 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:15s remains)
INFO - root - 2017-12-06 05:45:17.889609: step 19240, loss = 0.83, batch loss = 0.61 (32.0 examples/sec; 0.250 sec/batch; 21h:44m:33s remains)
INFO - root - 2017-12-06 05:45:20.145628: step 19250, loss = 0.77, batch loss = 0.56 (35.2 examples/sec; 0.228 sec/batch; 19h:47m:47s remains)
INFO - root - 2017-12-06 05:45:22.447246: step 19260, loss = 0.78, batch loss = 0.57 (36.4 examples/sec; 0.220 sec/batch; 19h:06m:42s remains)
INFO - root - 2017-12-06 05:45:24.787978: step 19270, loss = 0.86, batch loss = 0.65 (34.3 examples/sec; 0.233 sec/batch; 20h:16m:32s remains)
INFO - root - 2017-12-06 05:45:27.140153: step 19280, loss = 0.92, batch loss = 0.70 (35.0 examples/sec; 0.229 sec/batch; 19h:52m:56s remains)
INFO - root - 2017-12-06 05:45:29.469036: step 19290, loss = 0.86, batch loss = 0.65 (37.2 examples/sec; 0.215 sec/batch; 18h:41m:36s remains)
INFO - root - 2017-12-06 05:45:31.771168: step 19300, loss = 0.80, batch loss = 0.59 (35.5 examples/sec; 0.225 sec/batch; 19h:36m:38s remains)
2017-12-06 05:45:32.153244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85663915 -0.96332729 -1.0644784 -1.1695633 -1.3306503 -1.4473646 -1.3893168 -1.3137958 -1.1339273 -0.9652617 -0.9271751 -0.8719607 -0.72717565 -0.53711808 -0.25027388][-1.1689053 -1.1366472 -1.0454363 -1.0516199 -1.1857855 -1.5208414 -1.8487985 -1.9737611 -1.8815913 -1.6144185 -1.3650587 -1.1889458 -0.97256488 -0.6782347 -0.30742931][-1.3354475 -1.1127155 -0.86260742 -0.7032147 -0.69636059 -1.075052 -1.6436191 -2.1436744 -2.4469919 -2.2413626 -1.7713542 -1.3424726 -0.984915 -0.62370539 -0.29423052][-1.36409 -0.950006 -0.47089666 -0.11784099 0.024056554 -0.1208622 -0.61196822 -1.239717 -1.8192399 -2.0012128 -1.7627296 -1.3615994 -0.97006363 -0.56004608 -0.23072328][-1.1338062 -0.71413052 -0.20333657 0.46010309 1.1268055 1.3078747 1.0743036 0.554691 -0.33201641 -0.90801513 -1.1021957 -1.03931 -0.83066559 -0.56573272 -0.37672785][-0.87180239 -0.53849232 -0.069334492 0.75503492 1.7081704 2.5448475 2.9294436 2.6023686 1.7437475 0.80259949 0.12332925 -0.13125235 -0.28551245 -0.29047012 -0.21009031][-0.67432553 -0.38793191 -0.029935487 0.63950658 1.5552812 2.6730306 3.6352859 3.8877034 3.4559906 2.7241669 1.8136146 1.0061789 0.42988044 0.17698511 0.044457585][-0.49211496 -0.19883016 0.058906868 0.64897871 1.4963427 2.5165954 3.5197797 4.0160103 3.778244 3.1395512 2.2674117 1.5849059 0.99645919 0.60046476 0.35942286][-0.52452862 -0.3042295 -0.021331176 0.63469535 1.3581259 2.2092586 2.9822288 3.372236 3.346209 2.7675779 2.0537891 1.3595545 0.83213329 0.42456746 0.3180052][-0.7144044 -0.5681147 -0.38105819 0.046086982 0.67765671 1.4658043 1.949415 2.20095 2.0655384 1.7678976 1.3969324 0.90364438 0.43875605 0.19225514 0.17059746][-0.97287375 -0.98321712 -0.97021872 -0.71115655 -0.29460436 0.36202791 0.85393006 0.95645076 0.79827487 0.51260042 0.22888917 0.0010711551 -0.081557333 -0.15439321 -0.062150758][-1.2016184 -1.4453809 -1.5834951 -1.5742056 -1.3911889 -0.93008721 -0.34365377 -0.088311143 -0.092981875 -0.20688388 -0.34624502 -0.50153279 -0.50641018 -0.40051928 -0.1921699][-1.0985837 -1.462703 -1.8733807 -2.098628 -2.1232493 -1.9164906 -1.5042326 -1.1867814 -0.77706325 -0.53066063 -0.54086542 -0.53454924 -0.44810873 -0.35895273 -0.28572714][-0.84623814 -1.1108179 -1.4285605 -1.7204502 -1.9268773 -1.9696507 -1.8413358 -1.676568 -1.2853134 -0.84011388 -0.50301886 -0.33138987 -0.22970714 -0.21980716 -0.18504068][-0.52226067 -0.7093153 -0.8457697 -1.0412859 -1.1825218 -1.2854888 -1.4476602 -1.4831018 -1.294718 -0.89302158 -0.45194679 -0.15765321 -0.027277619 0.008580327 -0.036443803]]...]
INFO - root - 2017-12-06 05:45:34.422315: step 19310, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 19h:26m:34s remains)
INFO - root - 2017-12-06 05:45:36.709032: step 19320, loss = 0.73, batch loss = 0.51 (35.1 examples/sec; 0.228 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-06 05:45:39.039543: step 19330, loss = 0.78, batch loss = 0.57 (33.6 examples/sec; 0.238 sec/batch; 20h:42m:04s remains)
INFO - root - 2017-12-06 05:45:41.343073: step 19340, loss = 0.91, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 20h:04m:50s remains)
INFO - root - 2017-12-06 05:45:43.617772: step 19350, loss = 0.88, batch loss = 0.67 (34.9 examples/sec; 0.229 sec/batch; 19h:55m:13s remains)
INFO - root - 2017-12-06 05:45:45.907675: step 19360, loss = 0.79, batch loss = 0.58 (35.3 examples/sec; 0.226 sec/batch; 19h:41m:27s remains)
INFO - root - 2017-12-06 05:45:48.256634: step 19370, loss = 0.78, batch loss = 0.57 (34.4 examples/sec; 0.233 sec/batch; 20h:14m:39s remains)
INFO - root - 2017-12-06 05:45:50.546684: step 19380, loss = 0.76, batch loss = 0.55 (36.1 examples/sec; 0.222 sec/batch; 19h:17m:10s remains)
INFO - root - 2017-12-06 05:45:52.873934: step 19390, loss = 0.75, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 20h:07m:33s remains)
INFO - root - 2017-12-06 05:45:55.180308: step 19400, loss = 0.75, batch loss = 0.53 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:41s remains)
2017-12-06 05:45:55.588085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061823241 -0.06154792 -0.061545011 -0.061553717 -0.061584733 -0.061629541 -0.061673909 -0.061711732 -0.061732292 -0.061736032 -0.061720066 -0.061699931 -0.061672837 -0.061652191 -0.0617629][-0.061838407 -0.061568446 -0.0615623 -0.061555021 -0.061565556 -0.061568223 -0.061574943 -0.061594784 -0.061609387 -0.061612554 -0.061592713 -0.06155647 -0.061500017 -0.061448827 -0.061498828][-0.061890826 -0.06156785 -0.061559573 -0.061542861 -0.06156569 -0.061585486 -0.061601251 -0.061607782 -0.061591454 -0.061572663 -0.061518773 -0.061431549 -0.061321691 -0.061225191 -0.061187573][-0.06184648 -0.061459795 -0.061424851 -0.061408527 -0.061486594 -0.06161657 -0.061738562 -0.06179342 -0.061755624 -0.061668746 -0.061547808 -0.061364911 -0.061157148 -0.060994774 -0.060960658][-0.061695382 -0.061256468 -0.061135195 -0.061087191 -0.06124644 -0.061561819 -0.061864447 -0.06200958 -0.061994448 -0.06184281 -0.061639011 -0.061331086 -0.061007991 -0.06080614 -0.060850754][-0.061658874 -0.061060015 -0.0607977 -0.060620666 -0.060766615 -0.061123908 -0.0615182 -0.061725851 -0.061757322 -0.061686732 -0.061518755 -0.061208397 -0.060848344 -0.060665973 -0.060868859][-0.061694011 -0.060900003 -0.060433567 -0.060015328 -0.05995933 -0.060126286 -0.060401097 -0.060548946 -0.060708493 -0.060951035 -0.061005048 -0.060824975 -0.060580634 -0.060538083 -0.060929704][-0.0617515 -0.060809255 -0.060220994 -0.059629533 -0.059373058 -0.0592663 -0.059320748 -0.059407339 -0.059694923 -0.060205173 -0.060457785 -0.060444117 -0.060374364 -0.060487643 -0.061072282][-0.061735045 -0.060795829 -0.060242649 -0.059662268 -0.05936674 -0.059131429 -0.058987442 -0.058954503 -0.059253253 -0.059776641 -0.060056485 -0.060162909 -0.060267009 -0.060572654 -0.06127207][-0.061719775 -0.060886823 -0.060420632 -0.059920512 -0.059632774 -0.059339378 -0.059048116 -0.058865815 -0.059039947 -0.059446372 -0.059698306 -0.059926394 -0.060241967 -0.060697734 -0.061456837][-0.061834104 -0.061073586 -0.06069988 -0.06029775 -0.06001886 -0.059668384 -0.059280872 -0.059023406 -0.059078 -0.059364617 -0.059590794 -0.059898816 -0.060385637 -0.060888007 -0.061620094][-0.061936356 -0.061305031 -0.061076917 -0.06081301 -0.06060623 -0.0602852 -0.05990959 -0.05966606 -0.059664026 -0.059848607 -0.0600217 -0.060319066 -0.060742997 -0.061135933 -0.061750207][-0.06200733 -0.061499193 -0.061435193 -0.061349094 -0.061269887 -0.061081544 -0.060808651 -0.060608059 -0.060566746 -0.06063246 -0.060727522 -0.060904972 -0.061171621 -0.061398979 -0.061810784][-0.062052242 -0.061597705 -0.061608657 -0.061621167 -0.061634004 -0.061582446 -0.061458394 -0.061342105 -0.061294124 -0.061304115 -0.061328582 -0.061401095 -0.061524697 -0.061609495 -0.061883662][-0.062194161 -0.061727278 -0.061746564 -0.06178499 -0.061829433 -0.061857358 -0.061847061 -0.061819978 -0.061799318 -0.061773352 -0.06174317 -0.061725207 -0.061740853 -0.06176671 -0.062021814]]...]
INFO - root - 2017-12-06 05:45:57.900605: step 19410, loss = 0.78, batch loss = 0.56 (33.8 examples/sec; 0.236 sec/batch; 20h:33m:15s remains)
INFO - root - 2017-12-06 05:46:00.177520: step 19420, loss = 0.76, batch loss = 0.55 (34.7 examples/sec; 0.231 sec/batch; 20h:03m:59s remains)
INFO - root - 2017-12-06 05:46:02.499896: step 19430, loss = 0.80, batch loss = 0.58 (33.3 examples/sec; 0.241 sec/batch; 20h:54m:59s remains)
INFO - root - 2017-12-06 05:46:04.780558: step 19440, loss = 0.77, batch loss = 0.55 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:50s remains)
INFO - root - 2017-12-06 05:46:07.106996: step 19450, loss = 0.89, batch loss = 0.68 (32.7 examples/sec; 0.245 sec/batch; 21h:17m:05s remains)
INFO - root - 2017-12-06 05:46:09.409221: step 19460, loss = 0.77, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 19h:55m:22s remains)
INFO - root - 2017-12-06 05:46:11.720806: step 19470, loss = 0.85, batch loss = 0.63 (34.3 examples/sec; 0.233 sec/batch; 20h:18m:02s remains)
INFO - root - 2017-12-06 05:46:14.034145: step 19480, loss = 0.79, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 19h:26m:19s remains)
INFO - root - 2017-12-06 05:46:16.338954: step 19490, loss = 0.75, batch loss = 0.53 (35.6 examples/sec; 0.225 sec/batch; 19h:32m:20s remains)
INFO - root - 2017-12-06 05:46:18.641487: step 19500, loss = 0.77, batch loss = 0.56 (33.1 examples/sec; 0.242 sec/batch; 21h:01m:30s remains)
2017-12-06 05:46:18.980981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12549874 -0.12393655 -0.12426999 -0.12666471 -0.12862834 -0.13051978 -0.13030978 -0.12591302 -0.11689681 -0.10541929 -0.095123649 -0.093381725 -0.094240561 -0.09725602 -0.09842965][-0.1790514 -0.20092934 -0.22499363 -0.24706237 -0.26237917 -0.27061331 -0.27317587 -0.2621665 -0.23543906 -0.20011422 -0.15959881 -0.12890452 -0.10870354 -0.10497432 -0.10396548][-0.31549925 -0.36142564 -0.40730387 -0.4519906 -0.49199933 -0.5193848 -0.52902 -0.51476896 -0.46808496 -0.4033933 -0.31713212 -0.23994377 -0.17508191 -0.14036359 -0.12148342][-0.50827038 -0.57079017 -0.62093288 -0.65875149 -0.702043 -0.74466205 -0.7793014 -0.78795642 -0.73720938 -0.65407252 -0.52684736 -0.40004033 -0.26912603 -0.18893296 -0.14004815][-0.56792116 -0.6300503 -0.66534591 -0.6959393 -0.741056 -0.78977919 -0.84721339 -0.88514382 -0.86516273 -0.80052823 -0.6801151 -0.52678764 -0.35199738 -0.23544599 -0.15868488][-0.45600113 -0.47975653 -0.44234124 -0.40800625 -0.41267589 -0.46040961 -0.530266 -0.61130953 -0.680789 -0.69555789 -0.646879 -0.5370084 -0.37624177 -0.24332574 -0.14801952][-0.28716585 -0.22671248 -0.061052077 0.1174874 0.22683129 0.26432446 0.21272156 0.077900738 -0.10597216 -0.25398272 -0.3673667 -0.37846243 -0.28655452 -0.18755715 -0.11098154][0.091011852 0.17002076 0.40762442 0.67428094 0.89175445 1.0426459 1.0485702 0.90767407 0.60958064 0.32401359 0.045798749 -0.094408482 -0.12802762 -0.10875776 -0.081019506][0.40698436 0.49092048 0.72392583 0.97961432 1.2354755 1.4527564 1.5462055 1.473357 1.1798902 0.81094831 0.40304777 0.13835815 0.0027538761 -0.02463498 -0.028427463][0.37059593 0.40002856 0.58653635 0.83527893 1.0695677 1.2947657 1.4400041 1.4237146 1.2187073 0.88779819 0.48625433 0.18428567 0.034598343 -0.0095397085 -0.015594214][0.26024789 0.20900595 0.28025007 0.39923072 0.55754596 0.75359273 0.88619614 0.94120556 0.86917967 0.62890238 0.31719175 0.088736147 -0.015573494 -0.039359286 -0.023480684][0.083041742 0.010557316 0.029305562 0.037271976 0.060423732 0.1257415 0.23037782 0.34244773 0.36030167 0.27038211 0.10197414 -0.049397912 -0.11876401 -0.12915242 -0.10532011][-0.17241311 -0.24053165 -0.27564016 -0.31348431 -0.32520014 -0.30454081 -0.24265294 -0.15524361 -0.059008773 -0.0045784637 -0.049767535 -0.094935343 -0.14516678 -0.14704172 -0.11319645][-0.33515924 -0.39846942 -0.4378947 -0.47715724 -0.5054251 -0.50515127 -0.47837648 -0.39869136 -0.27257371 -0.18800679 -0.15107307 -0.12805097 -0.13892147 -0.12510559 -0.10752136][-0.31856513 -0.35474756 -0.39700782 -0.43203583 -0.43367273 -0.42749408 -0.41650033 -0.38005334 -0.28735757 -0.20118077 -0.15627678 -0.12041979 -0.11720447 -0.11802415 -0.10977496]]...]
INFO - root - 2017-12-06 05:46:21.314575: step 19510, loss = 0.82, batch loss = 0.61 (34.2 examples/sec; 0.234 sec/batch; 20h:20m:12s remains)
INFO - root - 2017-12-06 05:46:23.627431: step 19520, loss = 0.78, batch loss = 0.56 (35.8 examples/sec; 0.223 sec/batch; 19h:24m:32s remains)
INFO - root - 2017-12-06 05:46:25.913950: step 19530, loss = 0.81, batch loss = 0.60 (35.5 examples/sec; 0.225 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-06 05:46:28.242896: step 19540, loss = 0.79, batch loss = 0.58 (33.2 examples/sec; 0.241 sec/batch; 20h:57m:44s remains)
INFO - root - 2017-12-06 05:46:30.550010: step 19550, loss = 0.87, batch loss = 0.65 (34.0 examples/sec; 0.235 sec/batch; 20h:27m:54s remains)
INFO - root - 2017-12-06 05:46:32.843018: step 19560, loss = 0.73, batch loss = 0.51 (36.4 examples/sec; 0.220 sec/batch; 19h:05m:36s remains)
INFO - root - 2017-12-06 05:46:35.143326: step 19570, loss = 0.88, batch loss = 0.66 (35.0 examples/sec; 0.228 sec/batch; 19h:50m:30s remains)
INFO - root - 2017-12-06 05:46:37.453771: step 19580, loss = 0.83, batch loss = 0.62 (35.0 examples/sec; 0.228 sec/batch; 19h:51m:07s remains)
INFO - root - 2017-12-06 05:46:39.785419: step 19590, loss = 0.68, batch loss = 0.47 (35.4 examples/sec; 0.226 sec/batch; 19h:39m:54s remains)
INFO - root - 2017-12-06 05:46:42.076917: step 19600, loss = 0.83, batch loss = 0.61 (34.3 examples/sec; 0.233 sec/batch; 20h:15m:57s remains)
2017-12-06 05:46:42.433394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059300423 -0.059576087 -0.060126826 -0.060327508 -0.06240198 -0.0650548 -0.06770847 -0.069596834 -0.07420966 -0.075587519 -0.076333776 -0.078215443 -0.0763385 -0.07558462 -0.075013004][-0.079043329 -0.079642043 -0.080075927 -0.080900885 -0.082884409 -0.084610946 -0.086091608 -0.087871991 -0.089635819 -0.0898369 -0.088800631 -0.086913608 -0.082929209 -0.07947807 -0.07872878][-0.079899229 -0.079978369 -0.080645241 -0.081896141 -0.083900958 -0.086345583 -0.0889245 -0.093086392 -0.097108886 -0.097861774 -0.09317489 -0.087596349 -0.082491264 -0.078378968 -0.077001959][-0.076919712 -0.076904245 -0.077730015 -0.07942839 -0.082412884 -0.086489223 -0.089016467 -0.090142936 -0.098141633 -0.10409833 -0.099719644 -0.092470728 -0.084637493 -0.079214945 -0.076697677][-0.075578243 -0.075991154 -0.077119365 -0.079106204 -0.083004571 -0.088311173 -0.088594452 -0.082849763 -0.096814238 -0.10929802 -0.10671075 -0.099001631 -0.089101605 -0.081437834 -0.076853417][-0.075326204 -0.075349286 -0.076769739 -0.078765109 -0.082301751 -0.08358831 -0.073087916 -0.062367953 -0.085734159 -0.10596118 -0.10432728 -0.094606631 -0.089828812 -0.084042117 -0.078039855][-0.075076349 -0.075545564 -0.076679774 -0.078907922 -0.082009077 -0.078747161 -0.057869192 -0.039774191 -0.061568238 -0.086457409 -0.089709088 -0.081661858 -0.082932293 -0.082358137 -0.077835895][-0.075424664 -0.0758227 -0.077061087 -0.079630814 -0.082034811 -0.0776054 -0.057510696 -0.04034207 -0.056327567 -0.07803642 -0.084114239 -0.076717116 -0.0771113 -0.078310229 -0.075968623][-0.076046079 -0.075911969 -0.077354416 -0.080994718 -0.082912818 -0.079527318 -0.062217586 -0.047069166 -0.054885309 -0.069299638 -0.077467293 -0.075485006 -0.076231912 -0.077058725 -0.073449641][-0.075535 -0.074176386 -0.075911887 -0.07947351 -0.0827422 -0.083170287 -0.072144389 -0.063023262 -0.066692062 -0.073138334 -0.077367045 -0.075899169 -0.076333627 -0.077777475 -0.075492427][-0.076098442 -0.07554692 -0.075741038 -0.076631114 -0.078449458 -0.079144172 -0.073461249 -0.069781147 -0.072642557 -0.076397136 -0.079004817 -0.078305267 -0.077951312 -0.07763508 -0.076370776][-0.0761051 -0.075639054 -0.075749949 -0.076300606 -0.0780464 -0.078333244 -0.075917237 -0.074617811 -0.07510981 -0.076560006 -0.0770307 -0.076210409 -0.076035559 -0.076000705 -0.0759506][-0.076148346 -0.075749785 -0.075829744 -0.075982235 -0.077125251 -0.076928392 -0.075615771 -0.074495256 -0.073573217 -0.075166151 -0.075983353 -0.075914674 -0.07581383 -0.075715408 -0.075735062][-0.076231815 -0.075947963 -0.076044135 -0.076122746 -0.076612912 -0.076198295 -0.076164924 -0.074857585 -0.0734096 -0.074003771 -0.075345293 -0.075802527 -0.075811327 -0.07579837 -0.075899363][-0.076530546 -0.0763607 -0.076579317 -0.07660722 -0.077862315 -0.077443488 -0.076685257 -0.077131838 -0.074278377 -0.073566779 -0.074688926 -0.076006636 -0.076159105 -0.076181963 -0.076328546]]...]
INFO - root - 2017-12-06 05:46:44.728202: step 19610, loss = 0.86, batch loss = 0.64 (36.2 examples/sec; 0.221 sec/batch; 19h:13m:04s remains)
INFO - root - 2017-12-06 05:46:47.036057: step 19620, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:23s remains)
INFO - root - 2017-12-06 05:46:49.362575: step 19630, loss = 0.80, batch loss = 0.59 (34.5 examples/sec; 0.232 sec/batch; 20h:09m:16s remains)
INFO - root - 2017-12-06 05:46:51.655593: step 19640, loss = 0.82, batch loss = 0.60 (33.6 examples/sec; 0.238 sec/batch; 20h:40m:50s remains)
INFO - root - 2017-12-06 05:46:53.957832: step 19650, loss = 0.76, batch loss = 0.54 (35.1 examples/sec; 0.228 sec/batch; 19h:48m:31s remains)
INFO - root - 2017-12-06 05:46:56.290874: step 19660, loss = 0.79, batch loss = 0.58 (33.9 examples/sec; 0.236 sec/batch; 20h:30m:23s remains)
INFO - root - 2017-12-06 05:46:58.600649: step 19670, loss = 0.89, batch loss = 0.67 (32.7 examples/sec; 0.244 sec/batch; 21h:14m:35s remains)
INFO - root - 2017-12-06 05:47:00.922426: step 19680, loss = 0.84, batch loss = 0.62 (33.7 examples/sec; 0.237 sec/batch; 20h:36m:03s remains)
INFO - root - 2017-12-06 05:47:03.225326: step 19690, loss = 0.83, batch loss = 0.62 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:35s remains)
INFO - root - 2017-12-06 05:47:05.478308: step 19700, loss = 0.83, batch loss = 0.61 (35.8 examples/sec; 0.224 sec/batch; 19h:25m:51s remains)
2017-12-06 05:47:05.955688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.081916094 -0.078172773 -0.077106588 -0.077764705 -0.079138689 -0.08042831 -0.081641793 -0.082574487 -0.082896821 -0.082763582 -0.082261652 -0.081524044 -0.080612391 -0.079544038 -0.078525424][-0.08532241 -0.08067596 -0.078771137 -0.079037905 -0.080518566 -0.0820296 -0.083426684 -0.084100984 -0.08384382 -0.082924336 -0.081661113 -0.080258623 -0.07890334 -0.077684566 -0.076652624][-0.089353666 -0.084151112 -0.081713259 -0.081656985 -0.083204165 -0.085023895 -0.086451009 -0.086739093 -0.085689433 -0.083731823 -0.081489705 -0.079428531 -0.077615224 -0.07619781 -0.075145818][-0.093738452 -0.088634215 -0.086193293 -0.085824147 -0.08706709 -0.088666491 -0.089648016 -0.089421242 -0.087589331 -0.08482492 -0.081780925 -0.079021238 -0.076772593 -0.075187176 -0.07410457][-0.097224317 -0.093009964 -0.09111245 -0.090694152 -0.0915611 -0.092712976 -0.09307383 -0.092051052 -0.089418672 -0.085930616 -0.08207912 -0.078718416 -0.076156616 -0.074498937 -0.073423587][-0.099519052 -0.096673049 -0.095508151 -0.095080152 -0.095684245 -0.096180946 -0.095585823 -0.093505464 -0.090065449 -0.086017579 -0.081742384 -0.078080736 -0.075474769 -0.073953241 -0.073036924][-0.099228323 -0.098030321 -0.097770154 -0.097706914 -0.097953625 -0.097588181 -0.096072227 -0.093069568 -0.088935018 -0.0847225 -0.080619186 -0.07708741 -0.074623153 -0.073372625 -0.07273829][-0.096603587 -0.096838742 -0.097579 -0.097774692 -0.097778931 -0.09677583 -0.094448365 -0.09080445 -0.086457804 -0.082466342 -0.078778841 -0.075778656 -0.0737904 -0.072849162 -0.072431341][-0.091958039 -0.093172945 -0.094551921 -0.095171735 -0.095165126 -0.093868107 -0.091271058 -0.087513193 -0.083329953 -0.079673618 -0.076585464 -0.074313819 -0.07291276 -0.072327361 -0.072179988][-0.08681839 -0.087912761 -0.089418866 -0.090283461 -0.09030994 -0.089167349 -0.086951695 -0.083723217 -0.080027923 -0.076912336 -0.074565627 -0.072989747 -0.072166219 -0.071892269 -0.071996711][-0.082087129 -0.082859814 -0.083972223 -0.084643118 -0.08459156 -0.083727486 -0.082007691 -0.079705991 -0.077062942 -0.074822783 -0.073103711 -0.07204511 -0.071625479 -0.071623981 -0.071836576][-0.078091435 -0.078395806 -0.079039052 -0.0794145 -0.079358205 -0.078775711 -0.0776595 -0.076314762 -0.074700773 -0.073295981 -0.072162852 -0.071584389 -0.071375541 -0.071411185 -0.07163173][-0.075225711 -0.075208396 -0.075539835 -0.075734518 -0.07562232 -0.075280607 -0.074631363 -0.07385508 -0.072904482 -0.072110593 -0.071438484 -0.071209259 -0.0711454 -0.0712484 -0.071474291][-0.0735348 -0.073359206 -0.073538817 -0.07360135 -0.073504493 -0.073310629 -0.072931513 -0.072441541 -0.071834572 -0.071341246 -0.070903942 -0.070827678 -0.070800006 -0.070978791 -0.071206331][-0.072697073 -0.072411209 -0.072507776 -0.072567925 -0.072511218 -0.072385341 -0.072082408 -0.07177081 -0.071324609 -0.070979692 -0.070595443 -0.070488833 -0.070404939 -0.070573166 -0.070830412]]...]
INFO - root - 2017-12-06 05:47:08.247148: step 19710, loss = 0.72, batch loss = 0.51 (33.8 examples/sec; 0.237 sec/batch; 20h:35m:19s remains)
INFO - root - 2017-12-06 05:47:10.549880: step 19720, loss = 0.84, batch loss = 0.63 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:53s remains)
INFO - root - 2017-12-06 05:47:12.850804: step 19730, loss = 0.94, batch loss = 0.73 (32.6 examples/sec; 0.246 sec/batch; 21h:20m:24s remains)
INFO - root - 2017-12-06 05:47:15.252348: step 19740, loss = 0.77, batch loss = 0.56 (33.7 examples/sec; 0.237 sec/batch; 20h:36m:23s remains)
INFO - root - 2017-12-06 05:47:17.555069: step 19750, loss = 0.84, batch loss = 0.62 (34.0 examples/sec; 0.235 sec/batch; 20h:26m:25s remains)
INFO - root - 2017-12-06 05:47:19.843162: step 19760, loss = 0.79, batch loss = 0.57 (36.0 examples/sec; 0.222 sec/batch; 19h:18m:20s remains)
INFO - root - 2017-12-06 05:47:22.187781: step 19770, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 19h:27m:46s remains)
INFO - root - 2017-12-06 05:47:24.463324: step 19780, loss = 0.88, batch loss = 0.67 (35.5 examples/sec; 0.225 sec/batch; 19h:33m:43s remains)
INFO - root - 2017-12-06 05:47:26.777988: step 19790, loss = 0.84, batch loss = 0.63 (36.2 examples/sec; 0.221 sec/batch; 19h:10m:26s remains)
INFO - root - 2017-12-06 05:47:29.051822: step 19800, loss = 0.93, batch loss = 0.72 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:18s remains)
2017-12-06 05:47:29.433554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076091073 -0.074911386 -0.073475763 -0.071839936 -0.070166454 -0.068820558 -0.067881338 -0.067526191 -0.067934059 -0.069090933 -0.071289487 -0.074789286 -0.080781952 -0.0895564 -0.10047877][-0.075618461 -0.074016467 -0.072121076 -0.0700244 -0.067940451 -0.0662292 -0.065081634 -0.0645711 -0.064784907 -0.065678917 -0.0679236 -0.071949035 -0.078941859 -0.089336313 -0.10151178][-0.075352572 -0.073368385 -0.071026571 -0.068485931 -0.066001736 -0.063962474 -0.062652737 -0.0621185 -0.062255867 -0.0630023 -0.065127507 -0.069051959 -0.0767823 -0.08817184 -0.10118935][-0.075061478 -0.072727673 -0.070265636 -0.067224845 -0.064281181 -0.061984316 -0.060560964 -0.060054775 -0.06011913 -0.060929395 -0.062936857 -0.066624016 -0.0740098 -0.085713774 -0.099175431][-0.074927278 -0.072418138 -0.069833376 -0.066467062 -0.063331991 -0.060882404 -0.059440725 -0.059029393 -0.059092015 -0.0598818 -0.061602473 -0.064978585 -0.0719224 -0.083141096 -0.096994][-0.075110048 -0.072631508 -0.070078336 -0.0667492 -0.063774005 -0.061205521 -0.059758052 -0.059412066 -0.059343033 -0.059925497 -0.061060883 -0.063876286 -0.070105128 -0.081022725 -0.09504243][-0.075538166 -0.073231988 -0.07088463 -0.067698047 -0.064776093 -0.06200584 -0.060427003 -0.059920631 -0.059927113 -0.060227223 -0.060779549 -0.06291154 -0.068127058 -0.078306153 -0.092009418][-0.075966507 -0.073883519 -0.071738817 -0.068681911 -0.065824047 -0.062865958 -0.061086677 -0.060214341 -0.060030814 -0.060175918 -0.060675435 -0.062285464 -0.066711709 -0.075506635 -0.088082328][-0.076430351 -0.074674271 -0.072741769 -0.069945149 -0.067250729 -0.064349495 -0.062450491 -0.06108601 -0.060650092 -0.060567856 -0.060911775 -0.062391326 -0.065871671 -0.073024772 -0.0838481][-0.0770941 -0.075715549 -0.073957413 -0.07154347 -0.069120117 -0.066345446 -0.064389519 -0.062822409 -0.062214043 -0.061777946 -0.061970204 -0.063024335 -0.065820113 -0.0712281 -0.079869479][-0.077800639 -0.076741815 -0.0753105 -0.07329607 -0.0710936 -0.068510368 -0.066585161 -0.064960554 -0.064277306 -0.063680992 -0.063744783 -0.064416647 -0.066344455 -0.070124879 -0.076149389][-0.078345031 -0.07757166 -0.076503083 -0.074891239 -0.072988294 -0.070797257 -0.069025576 -0.067496277 -0.066774771 -0.066248223 -0.066307686 -0.066578157 -0.0678436 -0.070111945 -0.073999584][-0.078551315 -0.078074731 -0.077416033 -0.076255776 -0.074803166 -0.07313782 -0.071677789 -0.070420109 -0.069697447 -0.069226146 -0.069219977 -0.0693066 -0.069999613 -0.071218558 -0.073363535][-0.078501046 -0.078249551 -0.077899076 -0.077215247 -0.076262057 -0.075116955 -0.074084535 -0.073176295 -0.072649062 -0.072264642 -0.072193138 -0.072131507 -0.072374061 -0.072922111 -0.073875695][-0.078350939 -0.078225575 -0.078119494 -0.0778102 -0.077289626 -0.076625444 -0.076053038 -0.075493574 -0.075138487 -0.074846245 -0.074786365 -0.074638546 -0.0746371 -0.074773967 -0.07511571]]...]
INFO - root - 2017-12-06 05:47:31.735199: step 19810, loss = 0.80, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:48s remains)
INFO - root - 2017-12-06 05:47:34.052065: step 19820, loss = 0.90, batch loss = 0.69 (33.0 examples/sec; 0.243 sec/batch; 21h:04m:18s remains)
INFO - root - 2017-12-06 05:47:36.383823: step 19830, loss = 0.82, batch loss = 0.60 (33.6 examples/sec; 0.238 sec/batch; 20h:40m:48s remains)
INFO - root - 2017-12-06 05:47:38.694939: step 19840, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 19h:50m:38s remains)
INFO - root - 2017-12-06 05:47:40.983697: step 19850, loss = 0.77, batch loss = 0.56 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:39s remains)
INFO - root - 2017-12-06 05:47:43.292700: step 19860, loss = 0.75, batch loss = 0.53 (34.1 examples/sec; 0.234 sec/batch; 20h:21m:09s remains)
INFO - root - 2017-12-06 05:47:45.586287: step 19870, loss = 0.84, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 20h:07m:32s remains)
INFO - root - 2017-12-06 05:47:47.864249: step 19880, loss = 0.80, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 19h:25m:24s remains)
INFO - root - 2017-12-06 05:47:50.152922: step 19890, loss = 0.82, batch loss = 0.60 (36.4 examples/sec; 0.220 sec/batch; 19h:06m:01s remains)
INFO - root - 2017-12-06 05:47:52.474450: step 19900, loss = 0.81, batch loss = 0.60 (34.0 examples/sec; 0.236 sec/batch; 20h:27m:23s remains)
2017-12-06 05:47:52.962715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.80772352 -0.64667064 -1.011961 -1.0958302 -1.0366818 -1.2902166 -1.7900273 -1.9559038 -1.7898625 -1.8640686 -1.4323303 -0.491112 0.36746934 0.10625527 0.26203659][0.56351936 0.33458817 0.14124379 0.10834872 -0.20299345 -0.48217759 -0.83377022 -1.1893743 -1.4589238 -1.8559997 -1.5938019 -1.1700863 -1.0398263 -0.39344388 0.32844684][0.65993232 0.83025682 0.84567404 0.46404788 0.0032212362 -0.30358714 -0.6395337 -1.3608187 -1.6390977 -1.6508687 -1.7566335 -1.1496577 -0.77268374 -0.79655594 -1.4405084][0.77845347 0.40865853 0.4329789 0.87414253 0.73592943 0.032771327 -0.58648437 -1.0563169 -1.3422089 -1.3847892 -1.4502624 -1.0936468 -1.0818121 -1.0569723 -1.3497679][0.23795834 0.4603273 1.2110385 1.0470197 0.79578072 0.53837448 0.1442486 -0.47208351 -0.74356335 -0.99658656 -0.98693895 -1.1142753 -1.1115936 -1.1141981 -1.3037149][1.0549546 1.0323869 1.2279366 1.5621474 1.6114378 1.0179747 0.62463194 0.44001481 0.42106977 -0.10117275 -0.60320729 -0.54427481 -0.3375873 -0.4963991 -0.67393142][1.4900821 1.561729 1.4204311 1.001734 0.90144503 1.0074364 1.3141562 1.1952184 1.2076894 1.2130002 1.4782655 1.277858 1.0749189 1.5345907 1.3596071][1.6842505 1.1898594 1.0440034 0.75463378 0.47358444 0.42079648 0.56695575 1.3020477 1.9244748 1.6767721 1.7432461 2.3953724 3.0807285 2.5986116 1.732615][0.89513344 0.80470484 0.40357563 -0.073359869 -0.38047904 -0.10970173 0.1980381 0.36182669 0.49481425 0.98889714 1.7766522 1.9168006 2.2645423 2.4253607 2.4192827][-0.68345642 -0.76523644 -1.1891584 -1.0181826 -1.1670065 -1.1970781 -1.2933164 -1.0343581 -1.0867604 -1.2155523 -1.0530338 -0.17850752 0.82750338 0.85783124 0.78726506][-1.4640983 -1.665987 -1.9564925 -2.0269258 -2.1318085 -2.4952981 -2.7972596 -2.998431 -3.2113168 -3.1817431 -2.9956107 -2.4207907 -1.9396479 -1.0852493 -0.41643569][-1.2345076 -1.9281077 -2.0333529 -2.3125322 -2.4128249 -2.9888203 -3.5017583 -4.02748 -3.9371319 -3.8374307 -4.0222712 -3.6687791 -2.9998357 -2.5096719 -1.5884633][-0.71383542 -0.95333916 -0.907069 -1.2123401 -2.1635809 -2.5915058 -2.8477607 -3.413028 -3.4897695 -3.1111567 -2.9523346 -2.6213281 -2.4403472 -2.3303752 -2.054729][-0.33832395 -0.48871166 -0.79554367 -0.59266585 -1.3092988 -1.9994472 -2.4886391 -2.528986 -2.4761381 -2.0870228 -1.7050066 -1.7729236 -2.139559 -2.0128584 -2.2143667][0.035660334 0.088146366 0.018667206 -0.15477863 -1.0784681 -1.313305 -1.7742617 -1.699506 -1.5822856 -1.5524381 -1.4383457 -1.1034923 -0.9886027 -1.3225681 -1.6141702]]...]
INFO - root - 2017-12-06 05:47:55.272316: step 19910, loss = 0.75, batch loss = 0.54 (36.2 examples/sec; 0.221 sec/batch; 19h:09m:59s remains)
INFO - root - 2017-12-06 05:47:57.579866: step 19920, loss = 0.75, batch loss = 0.54 (34.7 examples/sec; 0.230 sec/batch; 20h:00m:43s remains)
INFO - root - 2017-12-06 05:47:59.861755: step 19930, loss = 0.83, batch loss = 0.61 (35.7 examples/sec; 0.224 sec/batch; 19h:28m:12s remains)
INFO - root - 2017-12-06 05:48:02.153678: step 19940, loss = 0.84, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:28s remains)
INFO - root - 2017-12-06 05:48:04.461106: step 19950, loss = 0.88, batch loss = 0.67 (34.7 examples/sec; 0.230 sec/batch; 19h:59m:37s remains)
INFO - root - 2017-12-06 05:48:06.764910: step 19960, loss = 0.84, batch loss = 0.62 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:15s remains)
INFO - root - 2017-12-06 05:48:09.051454: step 19970, loss = 0.81, batch loss = 0.60 (36.9 examples/sec; 0.217 sec/batch; 18h:50m:21s remains)
INFO - root - 2017-12-06 05:48:11.374072: step 19980, loss = 0.76, batch loss = 0.55 (33.3 examples/sec; 0.240 sec/batch; 20h:50m:06s remains)
INFO - root - 2017-12-06 05:48:13.693247: step 19990, loss = 0.70, batch loss = 0.48 (35.9 examples/sec; 0.223 sec/batch; 19h:20m:46s remains)
INFO - root - 2017-12-06 05:48:16.017302: step 20000, loss = 0.82, batch loss = 0.61 (34.0 examples/sec; 0.235 sec/batch; 20h:24m:44s remains)
2017-12-06 05:48:16.414576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07084737 -0.070115507 -0.06908378 -0.067752808 -0.066485524 -0.065587215 -0.065087609 -0.064914979 -0.0651079 -0.065657109 -0.066437751 -0.067204617 -0.067830794 -0.06820859 -0.068170443][-0.0735761 -0.072498456 -0.070725255 -0.068429828 -0.066376731 -0.064878717 -0.0639847 -0.063683115 -0.06402611 -0.064898275 -0.06609606 -0.067247517 -0.06826397 -0.068966076 -0.069013819][-0.0773312 -0.075775668 -0.07300923 -0.069470525 -0.066327319 -0.0640225 -0.062709548 -0.062330097 -0.062891781 -0.064175591 -0.065808542 -0.067379653 -0.068877324 -0.069915146 -0.070024684][-0.081720546 -0.079524435 -0.075789608 -0.070842758 -0.066067621 -0.0627392 -0.061152797 -0.060744852 -0.061726231 -0.0636051 -0.065826774 -0.067848369 -0.069767065 -0.071035832 -0.071152277][-0.082025379 -0.079303592 -0.075764865 -0.07025291 -0.064611465 -0.060818847 -0.05928342 -0.059404723 -0.061124429 -0.063995495 -0.06699656 -0.069276623 -0.071412705 -0.072585359 -0.072536483][-0.079342708 -0.078268617 -0.076115184 -0.07017085 -0.064270839 -0.061101556 -0.06009898 -0.060405791 -0.062225759 -0.067141287 -0.071439773 -0.073838763 -0.074978381 -0.075335741 -0.07451421][-0.079351671 -0.078916006 -0.075886525 -0.067850783 -0.060903974 -0.057372611 -0.054966964 -0.052683875 -0.054905143 -0.06218233 -0.069817573 -0.075829752 -0.078294151 -0.078013651 -0.07601618][-0.07944712 -0.077472836 -0.071970649 -0.06104064 -0.053281296 -0.048492592 -0.043291308 -0.039654244 -0.044117987 -0.05314995 -0.062455665 -0.07333415 -0.079621784 -0.0799361 -0.077293351][-0.0832376 -0.080809169 -0.076305106 -0.068418115 -0.063231982 -0.059781253 -0.056189958 -0.054192495 -0.058852561 -0.064824715 -0.068920463 -0.076408364 -0.081316896 -0.08084169 -0.077786505][-0.081880145 -0.080063909 -0.077392712 -0.072738633 -0.069977656 -0.06820482 -0.06666223 -0.067314185 -0.0711997 -0.074709423 -0.076252833 -0.080213554 -0.082805887 -0.081547216 -0.078101739][-0.080214933 -0.07838989 -0.07574264 -0.072329283 -0.070440963 -0.070410557 -0.07181561 -0.074138872 -0.07710129 -0.080162756 -0.081494704 -0.083023816 -0.083002821 -0.080677591 -0.077067144][-0.077557504 -0.076218612 -0.074329957 -0.072218992 -0.071105033 -0.071673341 -0.0732958 -0.07521484 -0.077177 -0.079251736 -0.080895752 -0.081601687 -0.08082214 -0.078397661 -0.075247318][-0.073950209 -0.073099755 -0.072010368 -0.070858367 -0.07038755 -0.070873536 -0.072082289 -0.073590681 -0.075412616 -0.076723993 -0.077401213 -0.077622615 -0.0770061 -0.075315237 -0.073129445][-0.071287185 -0.070735827 -0.070212185 -0.069730431 -0.06956733 -0.069845378 -0.070637867 -0.071791351 -0.072973624 -0.07381615 -0.074056 -0.073953941 -0.073589973 -0.072255448 -0.070685834][-0.069230482 -0.068859018 -0.068662979 -0.068521112 -0.068500593 -0.068644956 -0.069031224 -0.069488317 -0.07014063 -0.07063137 -0.070808455 -0.070610084 -0.070372388 -0.069679067 -0.068835855]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 05:48:20.676458: step 20010, loss = 0.79, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 20h:00m:33s remains)
INFO - root - 2017-12-06 05:48:23.031202: step 20020, loss = 0.85, batch loss = 0.64 (34.1 examples/sec; 0.235 sec/batch; 20h:22m:21s remains)
INFO - root - 2017-12-06 05:48:25.348315: step 20030, loss = 0.79, batch loss = 0.57 (35.7 examples/sec; 0.224 sec/batch; 19h:28m:00s remains)
INFO - root - 2017-12-06 05:48:27.644206: step 20040, loss = 0.78, batch loss = 0.56 (33.8 examples/sec; 0.237 sec/batch; 20h:31m:42s remains)
INFO - root - 2017-12-06 05:48:29.948703: step 20050, loss = 0.81, batch loss = 0.59 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-06 05:48:32.234076: step 20060, loss = 0.77, batch loss = 0.55 (35.1 examples/sec; 0.228 sec/batch; 19h:47m:38s remains)
INFO - root - 2017-12-06 05:48:34.562429: step 20070, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.230 sec/batch; 20h:00m:03s remains)
INFO - root - 2017-12-06 05:48:36.873849: step 20080, loss = 0.85, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 19h:30m:50s remains)
INFO - root - 2017-12-06 05:48:39.186675: step 20090, loss = 0.74, batch loss = 0.53 (34.9 examples/sec; 0.229 sec/batch; 19h:52m:18s remains)
INFO - root - 2017-12-06 05:48:41.502769: step 20100, loss = 0.78, batch loss = 0.57 (35.9 examples/sec; 0.223 sec/batch; 19h:21m:44s remains)
2017-12-06 05:48:41.910842: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.52419913 -0.84017158 -1.1147382 -1.5290821 -1.8214823 -1.7126453 -1.8882567 -2.1776671 -2.2614155 -2.2238462 -2.0831172 -1.9291726 -1.5975187 -1.5687847 -1.3535631][-0.35380504 -0.762539 -1.1947844 -1.5551893 -1.866084 -2.1465137 -2.3903599 -2.3180509 -2.2118633 -2.1754692 -2.104789 -2.0345492 -1.7765296 -1.6750447 -1.278033][-0.29531163 -0.63395584 -0.96858215 -1.4873533 -1.953023 -2.2384863 -2.4467783 -2.4508684 -2.4940724 -2.2347362 -1.9871068 -1.8367414 -1.7053621 -1.5696937 -1.4613128][-0.47617087 -0.68253517 -1.1376274 -1.4244636 -1.7537972 -2.1037428 -2.4852476 -2.6060059 -2.5983613 -2.4543996 -2.3424952 -1.9738984 -1.7976472 -1.6152905 -1.4353435][-0.42065054 -0.49645358 -0.76170409 -0.95906675 -1.2317526 -1.4143012 -1.6646556 -2.0595245 -2.3528991 -2.3305454 -2.1875889 -2.0967743 -2.0470827 -2.0012457 -1.8651263][-0.21104027 0.15408218 0.40474853 0.32710788 0.3640365 0.12279487 -0.26165748 -0.50709665 -0.80556965 -1.1542524 -1.512762 -1.5855514 -1.7203604 -1.8149929 -1.8756797][1.0652555 1.3940018 1.7053394 1.7320125 1.9496652 2.098927 2.0651031 1.7976463 1.3906153 0.97746289 0.5394367 0.031133018 -0.47283313 -0.78284228 -0.82258403][2.2466462 2.3812306 2.5901306 2.9624622 3.4621572 3.7260184 3.7517319 3.6065462 3.266746 2.6959472 2.2455688 1.8496584 1.4095069 0.87552392 0.36784935][1.8932985 2.0601654 2.1858187 2.7132909 3.1706536 3.5951824 3.7830162 3.8049583 3.5444894 3.1792679 2.7501464 2.4981003 2.2765224 2.0374739 1.7233266][0.22209081 0.44540635 0.79061973 1.0655843 1.4022284 1.8650351 2.0704141 2.0524061 2.0655448 1.997295 1.8696281 1.7345412 1.8291417 1.7850308 1.9209672][-0.82272744 -0.87183607 -0.82769227 -0.72812879 -0.53260422 -0.49453494 -0.3036027 -0.12406111 0.02645234 -0.0075037926 0.015005492 0.28915927 0.62055111 0.99684691 1.1467289][-1.5693452 -1.7981321 -1.9784334 -2.049984 -1.9805452 -2.074549 -2.0494642 -2.0937257 -2.047425 -1.8038001 -1.5323025 -1.0722387 -0.68153942 -0.18068784 0.32919446][-1.8683255 -2.1660635 -2.5086484 -2.7735527 -2.9413295 -3.0541239 -3.1698403 -3.0601287 -2.8654792 -2.5093138 -2.0512464 -1.4286884 -0.81853855 -0.52686244 -0.29021379][-1.3564297 -1.4142635 -1.4828266 -1.6793187 -2.0144634 -2.1500602 -2.2506905 -2.0986798 -1.941707 -1.5728638 -1.1857368 -0.997957 -0.85621631 -0.81298876 -0.66951013][-0.13448146 -0.59464705 -0.91248703 -1.1474085 -1.5298592 -1.637761 -1.9087911 -1.7962953 -1.7689954 -1.4615283 -1.2931833 -1.1915818 -1.1381687 -1.2592232 -1.1328509]]...]
INFO - root - 2017-12-06 05:48:44.192165: step 20110, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 19h:48m:04s remains)
INFO - root - 2017-12-06 05:48:46.493055: step 20120, loss = 0.89, batch loss = 0.68 (36.0 examples/sec; 0.222 sec/batch; 19h:17m:05s remains)
INFO - root - 2017-12-06 05:48:49.039973: step 20130, loss = 0.79, batch loss = 0.57 (18.7 examples/sec; 0.427 sec/batch; 37h:01m:51s remains)
INFO - root - 2017-12-06 05:48:51.291076: step 20140, loss = 0.80, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 19h:46m:10s remains)
INFO - root - 2017-12-06 05:48:53.599311: step 20150, loss = 0.76, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 20h:04m:50s remains)
INFO - root - 2017-12-06 05:48:55.873706: step 20160, loss = 0.72, batch loss = 0.51 (34.9 examples/sec; 0.229 sec/batch; 19h:53m:20s remains)
INFO - root - 2017-12-06 05:48:58.231327: step 20170, loss = 0.80, batch loss = 0.59 (35.7 examples/sec; 0.224 sec/batch; 19h:26m:19s remains)
INFO - root - 2017-12-06 05:49:00.518091: step 20180, loss = 0.75, batch loss = 0.53 (35.6 examples/sec; 0.225 sec/batch; 19h:29m:33s remains)
INFO - root - 2017-12-06 05:49:02.902844: step 20190, loss = 0.69, batch loss = 0.48 (35.6 examples/sec; 0.225 sec/batch; 19h:31m:02s remains)
INFO - root - 2017-12-06 05:49:05.182273: step 20200, loss = 0.73, batch loss = 0.52 (37.0 examples/sec; 0.216 sec/batch; 18h:46m:41s remains)
2017-12-06 05:49:05.610896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.19547129 -0.33569589 -0.021270052 0.096094228 0.031917907 0.017502367 -0.064856581 0.015584059 0.083646961 0.071622305 0.025241427 -0.055505734 -0.12771253 -0.076635428 -0.07972908][0.058112539 -0.1088897 0.01017642 0.14083043 0.19840899 0.33021984 0.38639003 0.50552922 0.43652827 0.47781134 0.28140733 0.035301082 -0.33455068 -0.50539309 -0.57655537][-0.30587566 -0.36469072 -0.25556773 -0.26457879 -0.12249597 0.063347884 0.12588716 0.372781 0.51191616 0.54991972 0.50261563 0.22540009 -0.18173786 -0.59222651 -0.98609352][0.27215698 -0.071750142 -0.12783456 -0.22532719 -0.33172607 -0.34077045 -0.15647914 0.016399369 0.23664683 0.39079127 0.34550771 0.082725175 -0.36656484 -0.80001485 -1.1467355][0.2221657 0.0020711496 0.10522484 -0.094710514 -0.18342918 -0.37395927 -0.48701298 -0.57282281 -0.42976192 -0.41575474 -0.3599554 -0.27411741 -0.33349454 -0.79965746 -1.1173518][0.57838506 0.13352633 0.14144176 0.13640583 0.10260106 -0.15327747 -0.31163734 -0.5217489 -0.54507613 -0.692608 -0.64161974 -0.59955204 -0.49004152 -0.70264256 -0.72195768][0.70333558 0.30703998 0.36028409 0.23781103 0.22489324 0.23070201 0.11279909 -0.092362218 -0.089660645 -0.26312393 -0.42233762 -0.30247897 0.18003222 0.41256917 0.59158438][0.18898273 -0.11460027 -0.23099434 -0.2108779 -0.036961917 0.13380042 0.30064005 0.44792581 0.56730878 0.6041013 0.58274251 0.43396151 0.70746624 1.343905 1.6047516][-0.725169 -0.91924864 -0.96931732 -0.85034418 -0.61014932 -0.26083833 0.14595398 0.47003311 0.92248881 1.166176 1.4009354 1.38268 1.4644032 1.8947388 2.3231258][-1.3804387 -1.4005609 -1.4278136 -1.3369328 -1.1997813 -0.86332762 -0.45620602 0.047598012 0.57961881 1.1469513 1.710422 2.04116 2.0799696 2.2593913 2.2194979][-1.8276863 -1.8319321 -1.7530322 -1.6564087 -1.5108337 -1.2857653 -0.96270514 -0.4824495 -0.031099275 0.55364251 1.1148871 1.6981081 1.8629181 1.9928631 2.0169034][-1.7582444 -1.8030399 -1.8019416 -1.7468897 -1.6789656 -1.4897964 -1.2416136 -0.9067502 -0.61550927 -0.19313926 0.26529652 0.58540475 0.72437119 0.9407711 0.93795836][-1.0984203 -1.2777891 -1.3539329 -1.3465769 -1.3747659 -1.3805808 -1.384322 -1.3057252 -1.1949915 -1.0507004 -0.82420725 -0.46005279 -0.29880762 -0.19776395 -0.085970476][-0.59934276 -0.69328839 -0.74750686 -0.91801149 -1.0092939 -1.0962442 -1.2600778 -1.3878701 -1.4830556 -1.4489081 -1.3429307 -1.2054588 -1.0339257 -0.9964776 -0.957655][-0.15006796 -0.23899302 -0.32605287 -0.39337113 -0.52754474 -0.73056668 -0.88554722 -1.0513053 -1.2458978 -1.4379493 -1.5497946 -1.5768772 -1.4973902 -1.3848479 -1.2754927]]...]
INFO - root - 2017-12-06 05:49:07.950275: step 20210, loss = 0.88, batch loss = 0.67 (35.8 examples/sec; 0.224 sec/batch; 19h:24m:33s remains)
INFO - root - 2017-12-06 05:49:10.235388: step 20220, loss = 0.77, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-06 05:49:12.572223: step 20230, loss = 0.70, batch loss = 0.49 (33.2 examples/sec; 0.241 sec/batch; 20h:53m:02s remains)
INFO - root - 2017-12-06 05:49:14.882342: step 20240, loss = 0.77, batch loss = 0.55 (34.3 examples/sec; 0.233 sec/batch; 20h:15m:06s remains)
INFO - root - 2017-12-06 05:49:17.214701: step 20250, loss = 0.85, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 19h:39m:44s remains)
INFO - root - 2017-12-06 05:49:19.537207: step 20260, loss = 0.86, batch loss = 0.65 (36.1 examples/sec; 0.222 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-06 05:49:21.844230: step 20270, loss = 0.91, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 20h:01m:27s remains)
INFO - root - 2017-12-06 05:49:24.151111: step 20280, loss = 0.91, batch loss = 0.70 (31.2 examples/sec; 0.256 sec/batch; 22h:14m:25s remains)
INFO - root - 2017-12-06 05:49:26.422010: step 20290, loss = 0.75, batch loss = 0.54 (37.0 examples/sec; 0.216 sec/batch; 18h:46m:15s remains)
INFO - root - 2017-12-06 05:49:28.789340: step 20300, loss = 0.84, batch loss = 0.63 (33.7 examples/sec; 0.237 sec/batch; 20h:34m:52s remains)
2017-12-06 05:49:29.280346: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6325305 1.4077524 1.6806147 1.4391664 1.080807 0.31471974 -0.18783632 -0.35979891 -0.73671687 -0.88351643 -0.70900041 -0.52056444 -0.16107902 -0.18472755 -0.95372295][2.4487772 2.3690352 2.0809696 1.8509539 1.3414595 0.6687693 -0.16042708 -0.31563467 -0.40086472 -0.67478257 -0.38352555 0.10306263 0.373756 0.261001 -0.23299539][1.6881397 1.4954374 1.7983963 1.2721401 0.35893422 0.10947734 -0.10639748 -0.49179959 -1.1160138 -0.60911989 0.28498006 0.16455118 -0.24549265 0.071035668 -0.25097692][1.3822364 0.72456914 0.56049746 0.41642869 0.22578613 -0.45491636 -1.1154976 -1.156406 -0.80165726 -0.62671715 -0.56487179 0.2845937 0.82718694 1.0322509 0.024995275][0.54032576 0.18546458 -0.11179444 -0.21064129 -0.59539312 -0.98229307 -1.0277759 -1.6912694 -1.9784799 -1.3377635 -0.69982868 -0.62813467 -0.069570005 0.21231191 0.310346][0.93687093 0.33390659 -0.13457917 -0.27248394 -0.37869322 -0.7227245 -1.0823511 -0.83960712 -0.5190416 -0.60302871 -0.83775312 -0.5706265 -0.37746179 0.39832002 -0.097289912][1.5597433 1.1210506 0.35120821 -0.43302071 -0.59974289 -0.16870427 0.31952643 0.42491871 0.28985262 0.659473 1.0189497 0.99783993 0.48241085 0.8833406 1.3807893][2.3307154 1.1971805 0.15264121 -0.19422042 -0.32617873 -0.37912649 -0.37251687 0.21044599 1.13897 1.0816905 1.3871125 1.9473305 2.3644757 1.7973499 1.4502757][1.3933713 0.86656916 0.14130148 -0.49124515 -0.48034102 -0.20922847 -0.30297661 -0.29081017 -0.18489075 0.54178715 1.217286 1.3650812 1.8223839 1.9418495 2.1132247][-0.16029009 -0.84637129 -1.2370647 -1.1957512 -1.2757154 -1.6749585 -1.7557364 -1.4662617 -1.2712965 -1.0111703 -0.34308553 0.61634827 1.4121002 1.2824413 1.0243495][-1.6617053 -1.833141 -1.9712403 -2.4755104 -2.6439507 -2.4791214 -2.6796017 -2.7500639 -2.5366969 -1.9460218 -1.4272745 -0.90430832 -0.48340583 0.28809738 1.0349737][-1.8801895 -2.5834084 -2.4679167 -2.2887483 -2.3637428 -2.6854837 -2.7406862 -2.798018 -2.9160085 -2.4499552 -2.1522574 -1.7774937 -1.8488638 -1.4355049 -1.06522][-1.750211 -1.4589641 -1.3996556 -1.3754435 -1.5057249 -1.8746319 -2.1158116 -2.0945902 -1.7907157 -1.9010289 -2.2326071 -1.7561 -1.2328614 -1.3126714 -1.7149845][-0.52939939 -0.73843229 -1.0687037 -0.74540609 -0.86926794 -1.1619401 -1.045113 -1.2858822 -1.2022527 -1.159219 -1.1007547 -1.3876433 -1.5405889 -1.155767 -1.4509811][0.080719978 -0.050225645 -0.19988778 -0.495718 -0.96461958 -1.0343499 -1.24593 -1.171864 -1.2385145 -1.4466468 -1.5337216 -1.5977267 -1.5899262 -1.7004882 -2.103734]]...]
INFO - root - 2017-12-06 05:49:31.579039: step 20310, loss = 0.78, batch loss = 0.57 (35.6 examples/sec; 0.225 sec/batch; 19h:30m:18s remains)
INFO - root - 2017-12-06 05:49:33.878751: step 20320, loss = 0.79, batch loss = 0.58 (35.5 examples/sec; 0.225 sec/batch; 19h:32m:56s remains)
INFO - root - 2017-12-06 05:49:36.212475: step 20330, loss = 0.78, batch loss = 0.56 (34.1 examples/sec; 0.235 sec/batch; 20h:21m:03s remains)
INFO - root - 2017-12-06 05:49:38.519687: step 20340, loss = 0.75, batch loss = 0.53 (34.2 examples/sec; 0.234 sec/batch; 20h:16m:32s remains)
INFO - root - 2017-12-06 05:49:40.871242: step 20350, loss = 0.82, batch loss = 0.61 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:55s remains)
INFO - root - 2017-12-06 05:49:43.194404: step 20360, loss = 0.83, batch loss = 0.61 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:09s remains)
INFO - root - 2017-12-06 05:49:45.465441: step 20370, loss = 0.77, batch loss = 0.56 (32.2 examples/sec; 0.248 sec/batch; 21h:32m:18s remains)
INFO - root - 2017-12-06 05:49:47.741172: step 20380, loss = 0.86, batch loss = 0.64 (36.2 examples/sec; 0.221 sec/batch; 19h:08m:10s remains)
INFO - root - 2017-12-06 05:49:50.070238: step 20390, loss = 0.83, batch loss = 0.62 (34.9 examples/sec; 0.229 sec/batch; 19h:52m:20s remains)
INFO - root - 2017-12-06 05:49:52.369494: step 20400, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.231 sec/batch; 20h:00m:36s remains)
2017-12-06 05:49:52.769135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7299047 -1.8386754 -2.0068 -1.9961933 -2.0389867 -2.0130563 -1.8492448 -1.698845 -1.6509211 -1.6068662 -1.5789306 -1.5495503 -1.6321924 -1.6125315 -1.6587236][-2.0053353 -2.1057491 -2.2880077 -2.6233726 -2.8025875 -2.7003541 -2.656769 -2.6206207 -2.4037964 -2.1356471 -1.986316 -1.8470201 -1.4334645 -1.1977183 -1.3462384][-1.8977345 -2.0263805 -2.2261469 -2.4369068 -2.6123643 -2.8004332 -2.9457262 -2.8091502 -2.6904657 -2.6190329 -2.37839 -2.2533896 -1.9089115 -1.2552652 -0.983652][-1.4393603 -1.5811248 -1.6794921 -1.7793977 -1.9302498 -2.0282125 -2.0799472 -2.2253528 -2.3071089 -2.154623 -2.1737103 -2.2686367 -2.1117828 -1.7256511 -1.6859093][-0.80106342 -0.84692442 -0.85643113 -0.86285686 -0.80638444 -0.79184747 -0.89338064 -0.94330037 -1.0338218 -1.1408285 -1.2116938 -1.2438872 -1.1952602 -1.4891222 -1.9964951][0.011537693 -0.18591468 -0.27572218 -0.279459 -0.25025648 -0.12354743 -0.0080530047 -0.022684306 -0.16257417 -0.1392184 -0.15188442 -0.28428429 -0.21116608 -0.098007828 -0.57755291][0.67839706 0.48788491 0.39526367 0.33975086 0.32152206 0.30894908 0.27406198 0.32286489 0.28832909 0.31429318 0.36361769 0.39706019 0.64841425 0.74981129 0.68462169][1.1784066 0.79281712 0.67535663 0.65532267 0.62589562 0.5566324 0.41997877 0.22711864 0.13477778 0.29540706 0.53115582 0.90920711 1.342056 1.5978825 1.8556242][0.98701954 0.86548674 0.85739255 0.65287876 0.6045121 0.5951395 0.34478956 0.17826924 0.070300788 0.29705489 0.68945527 1.0830973 1.7123884 2.2410247 2.710007][0.61787057 0.46631297 0.50207734 0.6065805 0.60238004 0.26106927 -0.013414301 -0.15817082 -0.41998726 -0.060415328 0.45492694 0.87347019 1.4124249 2.1288357 2.5996549][0.24795419 0.30405623 0.36134842 0.29199329 0.2649152 0.28046829 0.06113188 -0.42264706 -0.69842982 -0.46787104 -0.088736571 0.55576563 1.0080246 1.3560715 1.7698215][-0.06167569 -0.26925808 -0.47154722 -0.58699036 -0.67701125 -0.71705568 -0.90125966 -1.1214943 -1.271831 -1.3630344 -1.2818958 -1.0715684 -0.83475018 -0.35297096 0.12471336][-0.57250643 -0.75231957 -1.1649714 -1.5097549 -1.7152747 -1.8362205 -1.9689777 -2.1455758 -2.2589877 -2.1893957 -2.2005923 -2.3078356 -2.5597138 -2.5115843 -2.1668408][-1.096674 -1.4844763 -1.7435392 -2.00348 -2.3558671 -2.5370152 -2.6133273 -2.8058498 -2.8856187 -3.0130446 -3.1460488 -3.2352509 -3.4651253 -3.62694 -3.6365154][-1.6007105 -1.8172518 -2.0870762 -2.3524446 -2.5422223 -2.514039 -2.6230578 -2.7599294 -2.8718083 -3.1412718 -3.2868738 -3.4536281 -3.5399227 -3.5581963 -3.5974331]]...]
INFO - root - 2017-12-06 05:49:55.132169: step 20410, loss = 0.77, batch loss = 0.56 (35.2 examples/sec; 0.227 sec/batch; 19h:41m:39s remains)
INFO - root - 2017-12-06 05:49:57.376322: step 20420, loss = 0.81, batch loss = 0.59 (35.5 examples/sec; 0.225 sec/batch; 19h:30m:53s remains)
INFO - root - 2017-12-06 05:49:59.710567: step 20430, loss = 0.83, batch loss = 0.61 (34.3 examples/sec; 0.233 sec/batch; 20h:11m:35s remains)
INFO - root - 2017-12-06 05:50:01.987869: step 20440, loss = 0.82, batch loss = 0.60 (36.0 examples/sec; 0.222 sec/batch; 19h:15m:10s remains)
INFO - root - 2017-12-06 05:50:04.291473: step 20450, loss = 0.80, batch loss = 0.58 (35.4 examples/sec; 0.226 sec/batch; 19h:35m:42s remains)
INFO - root - 2017-12-06 05:50:06.598486: step 20460, loss = 0.81, batch loss = 0.60 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:45s remains)
INFO - root - 2017-12-06 05:50:08.930546: step 20470, loss = 0.83, batch loss = 0.61 (33.6 examples/sec; 0.238 sec/batch; 20h:37m:34s remains)
INFO - root - 2017-12-06 05:50:11.250299: step 20480, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:13s remains)
INFO - root - 2017-12-06 05:50:13.587998: step 20490, loss = 0.74, batch loss = 0.52 (34.5 examples/sec; 0.232 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-06 05:50:15.878977: step 20500, loss = 0.76, batch loss = 0.55 (33.6 examples/sec; 0.238 sec/batch; 20h:36m:51s remains)
2017-12-06 05:50:16.223060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062992483 -0.062940389 -0.06295073 -0.062992238 -0.063023329 -0.063001931 -0.0629652 -0.062931016 -0.062901974 -0.062903188 -0.062943026 -0.062999 -0.063026041 -0.06302163 -0.063226238][-0.062994614 -0.062828027 -0.062801905 -0.062863022 -0.062973693 -0.063048132 -0.063036576 -0.062955707 -0.062873267 -0.062840164 -0.062900111 -0.063009992 -0.06309741 -0.063123345 -0.063244522][-0.062767394 -0.062629014 -0.06256707 -0.062644005 -0.06281697 -0.0629521 -0.062943563 -0.062843956 -0.062752694 -0.062705278 -0.062775373 -0.062929109 -0.063075908 -0.063137613 -0.0632004][-0.062286064 -0.062226467 -0.062074997 -0.062102325 -0.062259778 -0.062416364 -0.062478453 -0.062459275 -0.06240169 -0.06234258 -0.062399082 -0.062566876 -0.062750056 -0.062867716 -0.063002259][-0.061678767 -0.061626777 -0.061335303 -0.061259449 -0.061327618 -0.061454967 -0.061607629 -0.061709668 -0.061713673 -0.061661437 -0.061688676 -0.061800405 -0.06198892 -0.062195621 -0.062554292][-0.061174922 -0.060925428 -0.060474142 -0.060237922 -0.060162164 -0.060193736 -0.060415048 -0.060635347 -0.060731925 -0.06069921 -0.060680497 -0.060744744 -0.060945954 -0.061242327 -0.061897803][-0.060710169 -0.060180336 -0.059566408 -0.059140716 -0.058911946 -0.05886957 -0.059116919 -0.059375092 -0.059577234 -0.059663907 -0.059678771 -0.059737943 -0.059923962 -0.06026227 -0.061166946][-0.060352623 -0.05953823 -0.058767676 -0.058187652 -0.057824373 -0.057685636 -0.057869781 -0.058113683 -0.058365989 -0.058554634 -0.058675967 -0.058793064 -0.058960978 -0.059271969 -0.060330786][-0.060056452 -0.059041634 -0.058160584 -0.057473313 -0.056995902 -0.056764841 -0.056841351 -0.056993295 -0.057210092 -0.057411931 -0.057564974 -0.057689246 -0.057808883 -0.058015153 -0.059068561][-0.05984953 -0.058724117 -0.057730343 -0.056956522 -0.056411095 -0.056124453 -0.056107134 -0.056144644 -0.056246452 -0.056366339 -0.056458704 -0.056506492 -0.056526534 -0.056605536 -0.057571296][-0.059823021 -0.058640927 -0.057559296 -0.05667761 -0.056046203 -0.055709556 -0.055598546 -0.0555843 -0.055626482 -0.055653151 -0.055638533 -0.055577002 -0.055496849 -0.055470649 -0.056336492][-0.059990413 -0.058910932 -0.057837334 -0.056890603 -0.056185409 -0.055816609 -0.055685479 -0.055649459 -0.055655219 -0.055640962 -0.055589639 -0.055509653 -0.055428527 -0.055380408 -0.056178547][-0.06050792 -0.059704706 -0.058807041 -0.058007319 -0.05742456 -0.057132568 -0.05701464 -0.056934908 -0.056913164 -0.056898408 -0.056880925 -0.056858186 -0.056838632 -0.056840342 -0.057544444][-0.061290957 -0.060879909 -0.060323052 -0.059827067 -0.0594563 -0.059256531 -0.059153352 -0.059056621 -0.05900972 -0.059003107 -0.05901837 -0.059057526 -0.059105664 -0.059162721 -0.059751257][-0.06198395 -0.061932072 -0.061707526 -0.061493207 -0.061331466 -0.061239474 -0.061172 -0.061093662 -0.06105765 -0.061061684 -0.061096393 -0.061156057 -0.06124156 -0.061334826 -0.061866321]]...]
INFO - root - 2017-12-06 05:50:18.483512: step 20510, loss = 0.83, batch loss = 0.62 (36.1 examples/sec; 0.221 sec/batch; 19h:11m:20s remains)
INFO - root - 2017-12-06 05:50:20.773700: step 20520, loss = 0.79, batch loss = 0.57 (36.6 examples/sec; 0.219 sec/batch; 18h:57m:33s remains)
INFO - root - 2017-12-06 05:50:23.066959: step 20530, loss = 0.77, batch loss = 0.56 (35.3 examples/sec; 0.226 sec/batch; 19h:36m:43s remains)
INFO - root - 2017-12-06 05:50:25.394057: step 20540, loss = 0.78, batch loss = 0.56 (36.5 examples/sec; 0.219 sec/batch; 18h:58m:13s remains)
INFO - root - 2017-12-06 05:50:27.716350: step 20550, loss = 0.72, batch loss = 0.50 (35.2 examples/sec; 0.227 sec/batch; 19h:40m:18s remains)
INFO - root - 2017-12-06 05:50:30.019817: step 20560, loss = 0.74, batch loss = 0.53 (33.1 examples/sec; 0.242 sec/batch; 20h:57m:53s remains)
INFO - root - 2017-12-06 05:50:32.312252: step 20570, loss = 0.76, batch loss = 0.55 (35.8 examples/sec; 0.224 sec/batch; 19h:22m:41s remains)
INFO - root - 2017-12-06 05:50:34.617117: step 20580, loss = 0.77, batch loss = 0.55 (35.1 examples/sec; 0.228 sec/batch; 19h:43m:14s remains)
INFO - root - 2017-12-06 05:50:36.909899: step 20590, loss = 0.88, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:15s remains)
INFO - root - 2017-12-06 05:50:39.210655: step 20600, loss = 0.79, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 20h:02m:02s remains)
2017-12-06 05:50:39.630565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13473217 -0.13788667 -0.13901423 -0.13705939 -0.1312114 -0.12189858 -0.11088982 -0.10007811 -0.09190432 -0.088554941 -0.089711912 -0.094116516 -0.098644212 -0.10119841 -0.10088067][-0.12249513 -0.12520602 -0.12627703 -0.1249385 -0.12023348 -0.11242442 -0.10362951 -0.095641635 -0.0904411 -0.089953676 -0.093948677 -0.10102629 -0.10811719 -0.11270422 -0.11353032][-0.11060417 -0.11288956 -0.1134207 -0.11177657 -0.1071983 -0.10059396 -0.093804173 -0.088537306 -0.086483143 -0.089016572 -0.095860049 -0.10567226 -0.11551703 -0.12253115 -0.1249692][-0.099955566 -0.101919 -0.10211211 -0.10004948 -0.095473751 -0.089706413 -0.084401123 -0.0811467 -0.0814052 -0.086060286 -0.094666667 -0.10628743 -0.11823872 -0.12758292 -0.13196841][-0.089828938 -0.091519609 -0.091572389 -0.089412354 -0.08545348 -0.08081869 -0.076921858 -0.075072475 -0.0765266 -0.081857011 -0.09086819 -0.10295418 -0.11597113 -0.12696415 -0.13321643][-0.080077514 -0.081298709 -0.081405327 -0.079755574 -0.07679332 -0.07355658 -0.070887759 -0.070092835 -0.071921527 -0.076901034 -0.084993161 -0.09600094 -0.10851149 -0.11988367 -0.12728745][-0.071302108 -0.072211854 -0.07256446 -0.0716243 -0.069909453 -0.06817916 -0.066715039 -0.066397235 -0.0678993 -0.07158789 -0.077730305 -0.08641243 -0.097018987 -0.10740622 -0.11494327][-0.064039305 -0.064547092 -0.065063909 -0.06486626 -0.064340882 -0.063829482 -0.063502587 -0.063663721 -0.064498544 -0.066524789 -0.070245989 -0.076056987 -0.083847538 -0.09206862 -0.098676823][-0.058388218 -0.058351733 -0.058787733 -0.058915526 -0.059134725 -0.059419908 -0.059905827 -0.060407393 -0.060878139 -0.061739035 -0.063562006 -0.066918179 -0.071927011 -0.077618852 -0.082543835][-0.053916994 -0.053536452 -0.053829279 -0.054094367 -0.054507677 -0.055132803 -0.055898067 -0.056497224 -0.056820579 -0.057186905 -0.058129404 -0.05997441 -0.062687814 -0.066015348 -0.069017731][-0.050683562 -0.05019217 -0.050439354 -0.050801147 -0.051238284 -0.051968239 -0.052749962 -0.053315684 -0.053633463 -0.053816911 -0.054217611 -0.05517254 -0.056514353 -0.058190897 -0.059805959][-0.048989031 -0.048382852 -0.04863048 -0.049066797 -0.0494914 -0.050088365 -0.05075654 -0.051358543 -0.051686972 -0.051869638 -0.052124634 -0.052667879 -0.053315744 -0.053985953 -0.054757327][-0.048108958 -0.047274172 -0.047401115 -0.047830064 -0.048298631 -0.048980013 -0.049736537 -0.050548006 -0.051122621 -0.051600128 -0.051960349 -0.052263267 -0.052449685 -0.052414365 -0.052542679][-0.048029717 -0.047057364 -0.047085077 -0.047400348 -0.047821917 -0.048549417 -0.049414802 -0.050417315 -0.051355608 -0.052171875 -0.052775569 -0.052950989 -0.052981023 -0.052720837 -0.052557688][-0.048568446 -0.047408283 -0.047291651 -0.047550179 -0.047888476 -0.0487256 -0.049660679 -0.05076845 -0.051794939 -0.052774988 -0.053594455 -0.053958248 -0.054215077 -0.053996533 -0.053920642]]...]
INFO - root - 2017-12-06 05:50:41.941770: step 20610, loss = 0.78, batch loss = 0.56 (36.1 examples/sec; 0.222 sec/batch; 19h:11m:53s remains)
INFO - root - 2017-12-06 05:50:44.200738: step 20620, loss = 0.72, batch loss = 0.50 (34.6 examples/sec; 0.231 sec/batch; 20h:03m:16s remains)
INFO - root - 2017-12-06 05:50:46.527282: step 20630, loss = 0.87, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 19h:55m:05s remains)
INFO - root - 2017-12-06 05:50:48.821201: step 20640, loss = 0.74, batch loss = 0.53 (34.9 examples/sec; 0.229 sec/batch; 19h:49m:53s remains)
INFO - root - 2017-12-06 05:50:51.131240: step 20650, loss = 0.85, batch loss = 0.63 (34.0 examples/sec; 0.235 sec/batch; 20h:22m:57s remains)
INFO - root - 2017-12-06 05:50:53.471418: step 20660, loss = 0.78, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:09s remains)
INFO - root - 2017-12-06 05:50:55.745093: step 20670, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:17s remains)
INFO - root - 2017-12-06 05:50:58.036752: step 20680, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.233 sec/batch; 20h:10m:13s remains)
INFO - root - 2017-12-06 05:51:00.324202: step 20690, loss = 0.83, batch loss = 0.61 (35.6 examples/sec; 0.225 sec/batch; 19h:28m:15s remains)
INFO - root - 2017-12-06 05:51:02.623814: step 20700, loss = 0.85, batch loss = 0.64 (34.7 examples/sec; 0.230 sec/batch; 19h:56m:37s remains)
2017-12-06 05:51:02.957610: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0072389171 -0.057454824 -0.023449905 -0.0085440129 0.03655649 0.099785231 0.13470772 0.11244325 0.047604494 -0.04610968 -0.12335221 -0.1607703 -0.14574043 -0.12476796 -0.12025449][0.049459718 -0.089133747 -0.092374511 -0.099089295 -0.079136625 -0.034516085 0.00478632 0.0031414106 -0.02101741 -0.041415438 -0.069027625 -0.086128369 -0.073225342 -0.05453191 -0.061854396][0.3043763 0.12104505 0.079278432 0.033136576 -0.011123553 -0.04405668 -0.083994925 -0.14838412 -0.19938782 -0.20940483 -0.18313685 -0.1540705 -0.10680541 -0.058838826 -0.041111916][0.57405961 0.399038 0.32073075 0.23652875 0.15346467 0.058385216 -0.04313736 -0.17215855 -0.28346866 -0.33163273 -0.3031255 -0.24072823 -0.161073 -0.080244146 -0.037881907][0.4385893 0.30733874 0.23080543 0.14853698 0.071822591 -0.019817851 -0.11903973 -0.2376532 -0.33618736 -0.38162002 -0.35123649 -0.2734296 -0.18634199 -0.10258628 -0.058049664][0.17469594 0.088796906 0.019194052 -0.043925151 -0.08791744 -0.11747049 -0.13143753 -0.16242473 -0.21855801 -0.264943 -0.2667833 -0.22545806 -0.16573158 -0.10889103 -0.080328718][-0.10325459 -0.1155885 -0.10014657 -0.06563691 0.019190162 0.13258964 0.25126207 0.28555432 0.18971485 0.046794094 -0.063001357 -0.11886411 -0.12748948 -0.10597998 -0.088553689][-0.20392957 -0.17691357 -0.10044292 0.014810689 0.17586583 0.33550745 0.48529097 0.52254522 0.37792811 0.17136413 0.015336007 -0.076470986 -0.1081991 -0.10075913 -0.084064983][-0.17235085 -0.15476 -0.097807862 -0.027632169 0.064365454 0.15070078 0.24538118 0.28732306 0.21511796 0.098731555 0.013806403 -0.042788964 -0.076613195 -0.08250422 -0.076779038][-0.097921319 -0.11476058 -0.12347721 -0.13919914 -0.14707683 -0.1256281 -0.059466973 -0.003030628 -0.0063147023 -0.042608622 -0.065958522 -0.071792156 -0.080346607 -0.085008718 -0.080485761][-0.11608437 -0.11666922 -0.12358748 -0.15071377 -0.1832608 -0.19732513 -0.17740564 -0.1433055 -0.12502795 -0.12165377 -0.11295725 -0.09565042 -0.088965118 -0.090004563 -0.089490525][-0.14740717 -0.15857816 -0.15008382 -0.14593816 -0.13854811 -0.14005688 -0.12767972 -0.10760105 -0.1028413 -0.11039308 -0.11159504 -0.1018354 -0.094267152 -0.088344865 -0.085438989][-0.12567757 -0.1385639 -0.13770321 -0.13563208 -0.11738645 -0.10336792 -0.093513906 -0.080317095 -0.076139785 -0.087779455 -0.0968166 -0.095642723 -0.092458114 -0.088664234 -0.086296491][-0.085570663 -0.1028535 -0.11107617 -0.12746654 -0.13014449 -0.12430784 -0.11117481 -0.088803351 -0.073319443 -0.073222615 -0.080528051 -0.0836839 -0.085391253 -0.085420661 -0.085538581][-0.068854772 -0.068437725 -0.0706361 -0.089468047 -0.10700832 -0.12131682 -0.12483464 -0.11297652 -0.09545844 -0.08020141 -0.078836076 -0.081050158 -0.084030978 -0.084261648 -0.084768504]]...]
INFO - root - 2017-12-06 05:51:05.275684: step 20710, loss = 0.78, batch loss = 0.57 (33.0 examples/sec; 0.242 sec/batch; 20h:59m:17s remains)
INFO - root - 2017-12-06 05:51:07.602399: step 20720, loss = 0.76, batch loss = 0.55 (33.4 examples/sec; 0.239 sec/batch; 20h:43m:55s remains)
INFO - root - 2017-12-06 05:51:09.902495: step 20730, loss = 0.76, batch loss = 0.55 (34.1 examples/sec; 0.235 sec/batch; 20h:19m:20s remains)
INFO - root - 2017-12-06 05:51:12.178859: step 20740, loss = 0.79, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 20h:04m:15s remains)
INFO - root - 2017-12-06 05:51:14.479593: step 20750, loss = 0.77, batch loss = 0.55 (34.4 examples/sec; 0.233 sec/batch; 20h:08m:39s remains)
INFO - root - 2017-12-06 05:51:16.791807: step 20760, loss = 0.75, batch loss = 0.53 (35.2 examples/sec; 0.228 sec/batch; 19h:42m:22s remains)
INFO - root - 2017-12-06 05:51:19.094292: step 20770, loss = 0.72, batch loss = 0.50 (35.0 examples/sec; 0.229 sec/batch; 19h:47m:23s remains)
INFO - root - 2017-12-06 05:51:21.440087: step 20780, loss = 0.87, batch loss = 0.65 (35.3 examples/sec; 0.227 sec/batch; 19h:37m:21s remains)
INFO - root - 2017-12-06 05:51:23.786633: step 20790, loss = 0.81, batch loss = 0.60 (36.1 examples/sec; 0.222 sec/batch; 19h:11m:51s remains)
INFO - root - 2017-12-06 05:51:26.087586: step 20800, loss = 0.81, batch loss = 0.59 (36.4 examples/sec; 0.220 sec/batch; 19h:01m:55s remains)
2017-12-06 05:51:26.441893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062730774 -0.062510528 -0.062484767 -0.062455177 -0.062423266 -0.062401652 -0.062405203 -0.062443353 -0.062490791 -0.062535465 -0.062575668 -0.062601693 -0.062615812 -0.062622406 -0.062731326][-0.062480226 -0.062154725 -0.062104188 -0.062044784 -0.061987307 -0.061948888 -0.061960403 -0.062036615 -0.062147066 -0.062256739 -0.062352866 -0.062425628 -0.062478058 -0.062509239 -0.06257809][-0.062130813 -0.061664429 -0.061552681 -0.061434858 -0.061329786 -0.061256789 -0.061271772 -0.061396144 -0.061597288 -0.061798446 -0.061972614 -0.062112197 -0.062222056 -0.062289461 -0.062319804][-0.061698824 -0.061132722 -0.060939442 -0.060734659 -0.060545597 -0.060405709 -0.060408622 -0.060580406 -0.060892403 -0.061215751 -0.061492443 -0.061713032 -0.0618884 -0.061994661 -0.062067688][-0.061316594 -0.060737297 -0.0604555 -0.060151245 -0.059865769 -0.059638869 -0.05960279 -0.059806533 -0.060225677 -0.060662813 -0.061030816 -0.061334692 -0.06157162 -0.061717533 -0.061850842][-0.061273813 -0.060603827 -0.060260057 -0.059873633 -0.059502028 -0.059196267 -0.059107672 -0.059311345 -0.059792053 -0.060301527 -0.060730048 -0.061085835 -0.06135942 -0.061535887 -0.061734293][-0.061545469 -0.060786523 -0.060423169 -0.060005769 -0.059586577 -0.059215844 -0.059056398 -0.059203904 -0.059668325 -0.060180619 -0.060623981 -0.060990714 -0.061274834 -0.061455831 -0.06171437][-0.062036317 -0.061223153 -0.060899381 -0.060523923 -0.0601291 -0.059753068 -0.059540749 -0.059596419 -0.059968818 -0.060417309 -0.060805954 -0.061127596 -0.061372392 -0.061525993 -0.061827622][-0.062475059 -0.061705895 -0.061479434 -0.061211243 -0.060915694 -0.060614716 -0.060406312 -0.060390823 -0.060637295 -0.060961228 -0.061243206 -0.061475173 -0.061648071 -0.061751969 -0.062052991][-0.062658042 -0.062003121 -0.061895527 -0.061754271 -0.061590679 -0.061413407 -0.061267555 -0.061228827 -0.061365783 -0.061561506 -0.06173154 -0.061869293 -0.061969966 -0.062027119 -0.062320862][-0.062677234 -0.062052429 -0.062036559 -0.06199906 -0.061942644 -0.061875336 -0.061810814 -0.061792888 -0.0618684 -0.06196855 -0.062060624 -0.062126465 -0.062175579 -0.062193844 -0.06249848][-0.062547229 -0.061953716 -0.061980069 -0.061997619 -0.062007885 -0.062011465 -0.062011044 -0.062024206 -0.06208143 -0.062145226 -0.062196512 -0.062225748 -0.062245417 -0.062245112 -0.062547937][-0.062442295 -0.061863467 -0.061908111 -0.061954636 -0.061997056 -0.062035918 -0.062073641 -0.062114362 -0.06216561 -0.06221687 -0.062253848 -0.062274911 -0.062285744 -0.062285826 -0.06255056][-0.062440425 -0.061860532 -0.061910845 -0.061971389 -0.062027827 -0.062078398 -0.062127464 -0.062177856 -0.06222631 -0.062269036 -0.062303111 -0.062325634 -0.062340032 -0.062345184 -0.0625667][-0.062595665 -0.06200628 -0.062051788 -0.062111579 -0.062167928 -0.062219441 -0.06227047 -0.062320091 -0.062365316 -0.062385619 -0.062401988 -0.062408719 -0.062409069 -0.062419247 -0.062624909]]...]
INFO - root - 2017-12-06 05:51:28.727766: step 20810, loss = 0.76, batch loss = 0.55 (35.5 examples/sec; 0.225 sec/batch; 19h:29m:15s remains)
INFO - root - 2017-12-06 05:51:31.048441: step 20820, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.224 sec/batch; 19h:21m:27s remains)
INFO - root - 2017-12-06 05:51:33.347767: step 20830, loss = 0.86, batch loss = 0.65 (33.0 examples/sec; 0.243 sec/batch; 21h:00m:03s remains)
INFO - root - 2017-12-06 05:51:35.627651: step 20840, loss = 0.79, batch loss = 0.57 (35.8 examples/sec; 0.224 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-06 05:51:37.899646: step 20850, loss = 0.85, batch loss = 0.64 (34.4 examples/sec; 0.233 sec/batch; 20h:08m:44s remains)
INFO - root - 2017-12-06 05:51:40.188387: step 20860, loss = 0.80, batch loss = 0.59 (35.8 examples/sec; 0.223 sec/batch; 19h:19m:36s remains)
INFO - root - 2017-12-06 05:51:42.474786: step 20870, loss = 0.76, batch loss = 0.54 (34.5 examples/sec; 0.232 sec/batch; 20h:05m:04s remains)
INFO - root - 2017-12-06 05:51:44.808280: step 20880, loss = 0.84, batch loss = 0.62 (34.1 examples/sec; 0.235 sec/batch; 20h:19m:39s remains)
INFO - root - 2017-12-06 05:51:47.073728: step 20890, loss = 0.74, batch loss = 0.52 (34.3 examples/sec; 0.233 sec/batch; 20h:11m:33s remains)
INFO - root - 2017-12-06 05:51:49.381058: step 20900, loss = 0.82, batch loss = 0.61 (35.6 examples/sec; 0.225 sec/batch; 19h:26m:11s remains)
2017-12-06 05:51:49.762432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069172889 -0.068996988 -0.068984181 -0.068974286 -0.068980008 -0.0690023 -0.069024757 -0.0690451 -0.069072843 -0.0691058 -0.069149271 -0.069196939 -0.069255687 -0.069336072 -0.069530375][-0.069223925 -0.068995826 -0.068989366 -0.068996035 -0.069033384 -0.069098622 -0.069162875 -0.069210254 -0.069243677 -0.069264486 -0.069281027 -0.069294035 -0.069318593 -0.069376186 -0.069523312][-0.069309659 -0.068982109 -0.068988279 -0.069026172 -0.069115639 -0.069244586 -0.069365844 -0.069451377 -0.069487832 -0.069479421 -0.06944707 -0.069405556 -0.069383763 -0.069411986 -0.069515094][-0.069337942 -0.068975747 -0.069010012 -0.06910222 -0.06926807 -0.069482051 -0.069672987 -0.069797263 -0.06982509 -0.069767565 -0.069664136 -0.069550872 -0.069473431 -0.069474548 -0.069602147][-0.069294356 -0.068980254 -0.069057778 -0.06922394 -0.069478258 -0.069787562 -0.070056617 -0.070207268 -0.070215292 -0.070092812 -0.069909617 -0.0697188 -0.069590606 -0.069581985 -0.069770381][-0.069303051 -0.068990938 -0.06912072 -0.069363162 -0.069710009 -0.07010933 -0.070441611 -0.07061141 -0.070581257 -0.070394441 -0.070134908 -0.069888651 -0.069735333 -0.0697453 -0.070022427][-0.069369823 -0.068996884 -0.06916979 -0.069475383 -0.069896892 -0.070357934 -0.070729077 -0.07090874 -0.070846863 -0.0706145 -0.070312366 -0.070055604 -0.0699196 -0.069981612 -0.070373848][-0.069423787 -0.068991005 -0.069184393 -0.069526143 -0.069986887 -0.07047271 -0.070863821 -0.071038991 -0.070962176 -0.0707136 -0.070420548 -0.070204832 -0.070128947 -0.070285678 -0.070820875][-0.069433145 -0.0689793 -0.069176525 -0.069518477 -0.069964483 -0.07043314 -0.070812143 -0.07097353 -0.070898131 -0.070679471 -0.070459753 -0.0703308 -0.070360705 -0.07063999 -0.0713059][-0.069408692 -0.068969533 -0.069145553 -0.069445625 -0.069833316 -0.070241615 -0.070566863 -0.0707121 -0.070674792 -0.07053785 -0.070423588 -0.070418149 -0.070589006 -0.071006596 -0.071783945][-0.0694247 -0.068962194 -0.069097735 -0.069327861 -0.069618434 -0.06992887 -0.070180707 -0.07031884 -0.070342 -0.070320383 -0.070339829 -0.070479155 -0.070798531 -0.071347 -0.072204508][-0.069431 -0.068956651 -0.0690442 -0.069196261 -0.069383912 -0.069595143 -0.06978631 -0.069930561 -0.070027918 -0.070113607 -0.070265956 -0.070541516 -0.070982583 -0.071607932 -0.072492406][-0.069419496 -0.068966009 -0.069015548 -0.069095612 -0.069195263 -0.069331169 -0.069482 -0.069635332 -0.069793805 -0.069977924 -0.070243873 -0.070608921 -0.0711059 -0.071731173 -0.072555885][-0.069429323 -0.068978727 -0.069002323 -0.069034114 -0.06907323 -0.069158256 -0.069285378 -0.069457605 -0.069669247 -0.06992878 -0.070266135 -0.070664 -0.071143627 -0.071697913 -0.072390057][-0.069506548 -0.069059074 -0.069065161 -0.069064781 -0.069070756 -0.069131732 -0.0692624 -0.069468081 -0.069733351 -0.0700386 -0.070387587 -0.070750251 -0.071135551 -0.071553357 -0.07205078]]...]
INFO - root - 2017-12-06 05:51:52.147170: step 20910, loss = 0.81, batch loss = 0.59 (32.7 examples/sec; 0.244 sec/batch; 21h:08m:45s remains)
INFO - root - 2017-12-06 05:51:54.440269: step 20920, loss = 0.80, batch loss = 0.58 (36.4 examples/sec; 0.220 sec/batch; 19h:00m:46s remains)
INFO - root - 2017-12-06 05:51:56.778129: step 20930, loss = 0.86, batch loss = 0.64 (33.2 examples/sec; 0.241 sec/batch; 20h:51m:25s remains)
INFO - root - 2017-12-06 05:51:59.035304: step 20940, loss = 0.85, batch loss = 0.64 (37.2 examples/sec; 0.215 sec/batch; 18h:37m:07s remains)
INFO - root - 2017-12-06 05:52:01.309439: step 20950, loss = 0.82, batch loss = 0.60 (36.1 examples/sec; 0.222 sec/batch; 19h:10m:25s remains)
INFO - root - 2017-12-06 05:52:03.586098: step 20960, loss = 0.78, batch loss = 0.56 (35.4 examples/sec; 0.226 sec/batch; 19h:34m:07s remains)
INFO - root - 2017-12-06 05:52:05.917718: step 20970, loss = 0.74, batch loss = 0.53 (34.8 examples/sec; 0.230 sec/batch; 19h:52m:25s remains)
INFO - root - 2017-12-06 05:52:08.257637: step 20980, loss = 0.88, batch loss = 0.66 (33.4 examples/sec; 0.240 sec/batch; 20h:45m:06s remains)
INFO - root - 2017-12-06 05:52:10.576205: step 20990, loss = 0.73, batch loss = 0.52 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-06 05:52:12.906939: step 21000, loss = 0.80, batch loss = 0.58 (35.8 examples/sec; 0.224 sec/batch; 19h:21m:37s remains)
2017-12-06 05:52:15.353521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10968931 -0.11440209 -0.10748036 -0.11112435 -0.10675743 -0.09949889 -0.10014211 -0.10227678 -0.11032975 -0.13245857 -0.14980285 -0.16993687 -0.18776515 -0.21702071 -0.24570121][-0.18050517 -0.17684966 -0.16040279 -0.15963879 -0.15352508 -0.14028071 -0.13430725 -0.12622462 -0.12434684 -0.13474154 -0.15487057 -0.18128762 -0.20815393 -0.23607841 -0.26266548][-0.27858078 -0.26202148 -0.24019635 -0.2212563 -0.19585803 -0.17600873 -0.16317633 -0.15529895 -0.15451241 -0.15926838 -0.17323416 -0.19334924 -0.21512875 -0.22331549 -0.2352441][-0.37235847 -0.31816465 -0.29064104 -0.25396121 -0.21383655 -0.19265653 -0.17793077 -0.17046744 -0.16765979 -0.17320654 -0.18972063 -0.20038508 -0.22002167 -0.22846009 -0.23311186][-0.30447894 -0.2750538 -0.24448597 -0.19218454 -0.14908062 -0.12751515 -0.13277031 -0.14042588 -0.14517716 -0.14156605 -0.14057355 -0.15452012 -0.17518085 -0.18910091 -0.19920489][-0.10250849 -0.09208066 -0.079338923 -0.044583362 0.00073985755 0.018434122 -0.0021701008 -0.029204167 -0.057731956 -0.061131664 -0.064327471 -0.075504668 -0.094901994 -0.12330233 -0.14399329][0.068728641 0.083737046 0.078143716 0.07235612 0.10109404 0.11875476 0.10528913 0.07217814 0.032910556 0.010995246 -0.0042131916 -0.0185804 -0.054191913 -0.091205887 -0.12490615][0.053647682 0.06812945 0.054767534 0.099963278 0.12412879 0.14167415 0.12998329 0.082905963 0.027007356 -0.013562769 -0.03802095 -0.0504429 -0.093940862 -0.12212396 -0.14995568][-0.092773162 -0.079476289 -0.088696212 -0.01959952 0.062201485 0.1325237 0.11632925 0.054820865 -0.024864562 -0.1003352 -0.15175007 -0.16836517 -0.18043092 -0.19774473 -0.1875324][-0.2143527 -0.21270494 -0.17285275 -0.0643106 0.010686092 0.072145939 0.080197662 0.039666817 -0.057605837 -0.15234081 -0.22427613 -0.25398445 -0.25771058 -0.27062035 -0.25475544][-0.26761344 -0.20397875 -0.13112274 0.017872684 0.11275996 0.12549222 0.090257272 0.033210546 -0.063213632 -0.16151732 -0.23905259 -0.26485068 -0.26697513 -0.2885536 -0.28369814][-0.24674925 -0.081737079 0.051749855 0.21592924 0.2917192 0.29292539 0.23253554 0.13063475 0.018935628 -0.091613054 -0.15201271 -0.19132024 -0.20257953 -0.20856407 -0.21070716][-0.18482885 -0.050504 0.055883348 0.23113427 0.33882022 0.35474482 0.31814021 0.23774412 0.11473536 -0.010832116 -0.0913468 -0.14961737 -0.17537445 -0.15723237 -0.14976633][-0.17930922 -0.10357693 -0.034332532 0.099385843 0.16092837 0.17045826 0.15967679 0.13098851 0.089613974 0.0069180354 -0.082109354 -0.15356606 -0.18844187 -0.18205965 -0.16117674][-0.21410838 -0.17070892 -0.16431803 -0.127227 -0.052670516 -0.025399715 -0.0043558478 0.017268777 0.015683107 -0.022226624 -0.068968967 -0.12664723 -0.17511714 -0.1913572 -0.18588713]]...]
INFO - root - 2017-12-06 05:52:17.633131: step 21010, loss = 0.75, batch loss = 0.54 (34.5 examples/sec; 0.232 sec/batch; 20h:02m:30s remains)
INFO - root - 2017-12-06 05:52:19.891406: step 21020, loss = 0.86, batch loss = 0.65 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:28s remains)
INFO - root - 2017-12-06 05:52:22.247157: step 21030, loss = 0.81, batch loss = 0.60 (34.7 examples/sec; 0.231 sec/batch; 19h:57m:34s remains)
INFO - root - 2017-12-06 05:52:24.520237: step 21040, loss = 0.82, batch loss = 0.60 (34.8 examples/sec; 0.230 sec/batch; 19h:51m:45s remains)
INFO - root - 2017-12-06 05:52:26.842334: step 21050, loss = 0.85, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 19h:39m:21s remains)
INFO - root - 2017-12-06 05:52:29.121435: step 21060, loss = 0.88, batch loss = 0.66 (34.3 examples/sec; 0.233 sec/batch; 20h:10m:55s remains)
INFO - root - 2017-12-06 05:52:31.463235: step 21070, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 20h:05m:29s remains)
INFO - root - 2017-12-06 05:52:33.726617: step 21080, loss = 0.85, batch loss = 0.64 (34.9 examples/sec; 0.229 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-06 05:52:36.102125: step 21090, loss = 0.81, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-06 05:52:38.429637: step 21100, loss = 0.86, batch loss = 0.64 (33.5 examples/sec; 0.239 sec/batch; 20h:40m:05s remains)
2017-12-06 05:52:38.836767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.30866817 -0.46030158 -0.58703244 -0.90613467 -1.2483532 -1.473245 -1.6393641 -1.6946161 -1.5430893 -1.3615497 -1.0915018 -0.86838233 -0.80590796 -0.74266785 -0.73806894][-0.72449541 -0.8137247 -1.039003 -1.2950617 -1.6252133 -1.9531659 -2.2251878 -2.3083856 -2.2878742 -2.1857753 -1.8800564 -1.5117387 -1.1956046 -1.0341765 -1.012279][-1.1966021 -1.1772457 -1.2462918 -1.3005751 -1.5374988 -1.7435384 -2.0448906 -2.3584862 -2.5719192 -2.5374978 -2.2947865 -2.043786 -1.7483308 -1.4707612 -1.2361467][-1.1780981 -1.2655331 -1.2851613 -1.0337526 -1.0047054 -1.0662529 -1.3250309 -1.6321893 -1.9301877 -2.1853347 -2.2420037 -2.1317413 -1.884038 -1.6279491 -1.3741717][-0.820079 -0.87618846 -0.82184285 -0.52626437 -0.20233634 0.069409423 0.025147103 -0.29514331 -0.754077 -1.1549087 -1.4012867 -1.5705343 -1.5760458 -1.4531673 -1.2565151][-0.32837105 -0.29481319 -0.07643684 0.23306689 0.71719366 1.1899675 1.5678879 1.4413252 0.95208395 0.36914578 -0.183058 -0.53903455 -0.72827697 -0.82532734 -0.85494816][0.19637796 0.21305877 0.43367538 0.80344087 1.4307271 1.9789973 2.391083 2.5131376 2.3308773 1.7071114 1.0407941 0.49239257 0.12758273 -0.020047382 -0.15299138][0.5240193 0.45219943 0.57576448 0.86089432 1.3434451 1.912825 2.44823 2.6966236 2.5317745 2.0584481 1.604839 1.0833024 0.6610266 0.41580614 0.22402212][0.37319562 0.30512828 0.40687546 0.47811678 0.66169679 1.0546486 1.4649673 1.7914379 1.8553877 1.6083584 1.2626872 0.96009541 0.75589997 0.52728462 0.33532926][-0.16657874 -0.25169066 -0.35838366 -0.50849468 -0.44838867 -0.36677137 -0.19281213 0.097412772 0.27977574 0.31967214 0.29161322 0.28126192 0.20532688 0.15686813 0.13802311][-0.69604808 -0.84684205 -0.97768956 -1.2212242 -1.3637631 -1.4276447 -1.3816031 -1.2329925 -1.1276498 -0.85174596 -0.63351607 -0.38181221 -0.2403962 -0.24395058 -0.23201618][-0.92571336 -1.1499054 -1.4538308 -1.7943566 -2.0212924 -2.1751199 -2.2627909 -2.2049057 -2.0124013 -1.7145615 -1.3860577 -0.95492822 -0.68352038 -0.56082469 -0.41994652][-0.95572585 -1.1552763 -1.4114183 -1.7511684 -2.050828 -2.1839085 -2.2068689 -2.1816614 -2.1157246 -1.9403583 -1.6523486 -1.2754329 -0.99974644 -0.86771816 -0.68546563][-0.8715896 -1.1381168 -1.374089 -1.5830991 -1.7998463 -1.9013515 -1.9856699 -2.0156047 -1.9357569 -1.8189927 -1.6548458 -1.4946327 -1.2569169 -1.0457671 -0.75013024][-0.57277328 -0.83489794 -1.1288624 -1.2182364 -1.3770803 -1.5498257 -1.5736916 -1.641139 -1.6882679 -1.72676 -1.7512636 -1.6261727 -1.3193754 -1.1078169 -0.79774493]]...]
INFO - root - 2017-12-06 05:52:41.177724: step 21110, loss = 0.82, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 19h:49m:58s remains)
INFO - root - 2017-12-06 05:52:43.493841: step 21120, loss = 0.90, batch loss = 0.69 (32.0 examples/sec; 0.250 sec/batch; 21h:36m:39s remains)
INFO - root - 2017-12-06 05:52:45.799696: step 21130, loss = 0.84, batch loss = 0.62 (33.3 examples/sec; 0.240 sec/batch; 20h:45m:18s remains)
INFO - root - 2017-12-06 05:52:48.091774: step 21140, loss = 0.78, batch loss = 0.57 (34.9 examples/sec; 0.230 sec/batch; 19h:51m:10s remains)
INFO - root - 2017-12-06 05:52:50.391688: step 21150, loss = 0.88, batch loss = 0.66 (34.4 examples/sec; 0.232 sec/batch; 20h:05m:17s remains)
INFO - root - 2017-12-06 05:52:52.671584: step 21160, loss = 0.79, batch loss = 0.58 (35.3 examples/sec; 0.226 sec/batch; 19h:34m:32s remains)
INFO - root - 2017-12-06 05:52:55.011344: step 21170, loss = 0.76, batch loss = 0.54 (35.1 examples/sec; 0.228 sec/batch; 19h:41m:32s remains)
INFO - root - 2017-12-06 05:52:57.298289: step 21180, loss = 0.88, batch loss = 0.67 (34.4 examples/sec; 0.232 sec/batch; 20h:05m:30s remains)
INFO - root - 2017-12-06 05:52:59.596234: step 21190, loss = 0.84, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 19h:43m:31s remains)
INFO - root - 2017-12-06 05:53:01.897307: step 21200, loss = 0.78, batch loss = 0.57 (34.7 examples/sec; 0.231 sec/batch; 19h:56m:05s remains)
2017-12-06 05:53:02.293725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066220395 -0.066689022 -0.066313908 -0.065043725 -0.063205563 -0.061477732 -0.060213219 -0.059518233 -0.0594019 -0.059672758 -0.060136363 -0.060602382 -0.061118968 -0.061812367 -0.062774755][-0.066481166 -0.066526145 -0.065837353 -0.06429676 -0.062261291 -0.060454816 -0.059200983 -0.058496196 -0.058361184 -0.05868151 -0.059234567 -0.059756935 -0.060366593 -0.061223172 -0.062308155][-0.065989889 -0.0655829 -0.064662978 -0.063027024 -0.061022859 -0.059309568 -0.058137022 -0.057463668 -0.057230063 -0.057436425 -0.057936784 -0.0583631 -0.058894839 -0.059748538 -0.060875867][-0.064590052 -0.063822165 -0.062818848 -0.061274614 -0.059492931 -0.058010317 -0.056982938 -0.056315131 -0.055912331 -0.055899225 -0.056147598 -0.056321058 -0.056668442 -0.057355706 -0.058425069][-0.062697634 -0.06178844 -0.060852453 -0.059511825 -0.058053225 -0.056869492 -0.056081004 -0.055430122 -0.054845762 -0.054584522 -0.05450232 -0.054350972 -0.054406751 -0.054853473 -0.055745076][-0.061243407 -0.060315281 -0.059553552 -0.058488149 -0.057361826 -0.056437541 -0.055811483 -0.055184476 -0.054494172 -0.053993639 -0.053532511 -0.052989811 -0.052749097 -0.052947313 -0.053620238][-0.060263209 -0.059363246 -0.058817286 -0.058031593 -0.057205375 -0.056453966 -0.055841237 -0.055233955 -0.054505084 -0.053848777 -0.0531169 -0.052254744 -0.051804285 -0.051755279 -0.052217323][-0.05943739 -0.058604389 -0.058247745 -0.057703614 -0.057109162 -0.056448881 -0.055800751 -0.055177085 -0.054467361 -0.053790957 -0.052916382 -0.051914971 -0.051382568 -0.051131912 -0.051486343][-0.058781326 -0.058039125 -0.057821542 -0.05745817 -0.057021361 -0.056418739 -0.055800159 -0.055224854 -0.054602947 -0.053965975 -0.053101745 -0.052148148 -0.051593762 -0.051192813 -0.05142872][-0.05825752 -0.057671625 -0.057588015 -0.057386689 -0.057072505 -0.056555796 -0.056026544 -0.055562489 -0.055103354 -0.054576423 -0.053863283 -0.053051218 -0.052545015 -0.052081287 -0.05219784][-0.05791188 -0.057379402 -0.057381477 -0.057289209 -0.057055444 -0.056622837 -0.056131076 -0.055758096 -0.055469945 -0.055069465 -0.05453812 -0.053933173 -0.053548072 -0.053200342 -0.053279161][-0.057626542 -0.057102542 -0.057133362 -0.0570743 -0.056869462 -0.056492135 -0.056026604 -0.055672668 -0.05550687 -0.055254731 -0.054936424 -0.054591853 -0.054396402 -0.054247078 -0.054408211][-0.057435364 -0.056907717 -0.056938939 -0.056873582 -0.056696817 -0.056393854 -0.055988673 -0.05563372 -0.055464961 -0.055280514 -0.055120375 -0.054980021 -0.054913338 -0.054985985 -0.055282637][-0.057499614 -0.056925211 -0.056929711 -0.056864057 -0.056713242 -0.05648125 -0.05613656 -0.055766124 -0.055547588 -0.055376422 -0.055342339 -0.055316258 -0.055333845 -0.055554185 -0.0560228][-0.057822697 -0.057193644 -0.057162669 -0.057083067 -0.056945711 -0.056736279 -0.056417428 -0.056044202 -0.055800065 -0.055624753 -0.055595469 -0.055607017 -0.055606823 -0.055861436 -0.0563587]]...]
INFO - root - 2017-12-06 05:53:04.583344: step 21210, loss = 0.77, batch loss = 0.55 (34.4 examples/sec; 0.233 sec/batch; 20h:06m:36s remains)
INFO - root - 2017-12-06 05:53:06.897227: step 21220, loss = 0.76, batch loss = 0.54 (36.1 examples/sec; 0.221 sec/batch; 19h:08m:12s remains)
INFO - root - 2017-12-06 05:53:09.235635: step 21230, loss = 0.86, batch loss = 0.64 (34.4 examples/sec; 0.232 sec/batch; 20h:04m:48s remains)
INFO - root - 2017-12-06 05:53:11.551028: step 21240, loss = 0.85, batch loss = 0.64 (36.1 examples/sec; 0.222 sec/batch; 19h:10m:05s remains)
INFO - root - 2017-12-06 05:53:13.846953: step 21250, loss = 0.78, batch loss = 0.56 (34.7 examples/sec; 0.231 sec/batch; 19h:56m:09s remains)
INFO - root - 2017-12-06 05:53:16.146444: step 21260, loss = 0.78, batch loss = 0.56 (33.8 examples/sec; 0.237 sec/batch; 20h:26m:56s remains)
INFO - root - 2017-12-06 05:53:18.445599: step 21270, loss = 0.82, batch loss = 0.61 (34.7 examples/sec; 0.231 sec/batch; 19h:55m:45s remains)
INFO - root - 2017-12-06 05:53:20.773598: step 21280, loss = 0.78, batch loss = 0.56 (31.5 examples/sec; 0.254 sec/batch; 21h:58m:36s remains)
INFO - root - 2017-12-06 05:53:23.126004: step 21290, loss = 0.83, batch loss = 0.61 (33.9 examples/sec; 0.236 sec/batch; 20h:23m:17s remains)
INFO - root - 2017-12-06 05:53:25.400413: step 21300, loss = 0.87, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:13s remains)
2017-12-06 05:53:28.991013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060235649 -0.040190764 -0.012424275 0.04352586 0.083568126 0.0711188 0.092744082 0.11644287 0.14430234 0.12563092 0.057074353 -0.047580186 -0.15482193 -0.20434058 -0.16738701][-0.11120313 -0.078272328 -0.039614975 0.017315201 0.0637676 0.087046936 0.1108108 0.1328471 0.17085926 0.14715207 0.087772891 -0.025685102 -0.15180942 -0.23263933 -0.23326069][-0.080089211 -0.0103276 0.0576649 0.12152573 0.1464047 0.16157551 0.17374609 0.19187586 0.21016149 0.18583681 0.12364686 0.026876502 -0.089741871 -0.17187956 -0.20021892][-0.034055844 0.03282664 0.11315688 0.19638164 0.20490097 0.1848238 0.16749786 0.16783075 0.17938931 0.18115138 0.1462034 0.060890868 -0.048047952 -0.12249789 -0.1706481][-0.0068513677 0.06715104 0.15869108 0.23960815 0.25867087 0.24707375 0.2328891 0.22427087 0.21443708 0.20458929 0.16299577 0.086357996 -0.022856049 -0.10580184 -0.17483336][-0.0081978962 0.05729416 0.16941027 0.2513743 0.29380435 0.30904526 0.29022741 0.26052219 0.23766963 0.21218373 0.1553822 0.074319944 -0.02535186 -0.11192048 -0.17479828][-0.071927249 -0.023409709 0.099767029 0.2222663 0.31051666 0.30906963 0.25170469 0.19229333 0.15512079 0.15048343 0.12405467 0.069294736 -0.015973017 -0.12654461 -0.20628989][-0.072800763 -0.035833009 0.040548533 0.10083066 0.16530548 0.1759107 0.1372347 0.078560144 0.053075314 0.052374661 0.059892848 0.03433574 -0.052450955 -0.14086196 -0.20477661][0.12524924 0.0964493 0.12989064 0.16771181 0.22189672 0.2510637 0.21205632 0.11896795 0.079994991 0.0632637 0.04908216 -0.0026337802 -0.10111541 -0.19982031 -0.22452949][0.42005509 0.37425721 0.36343741 0.39525712 0.41455239 0.36382717 0.27709442 0.18768536 0.15755206 0.13560694 0.1084761 0.036304116 -0.1020007 -0.20729122 -0.20422029][0.57377857 0.4969123 0.43512344 0.45159853 0.46592629 0.42482793 0.32045364 0.21207796 0.16875382 0.14665179 0.13939521 0.065190941 -0.06487909 -0.16661015 -0.14302558][0.55297488 0.49406463 0.41866702 0.38005739 0.33875185 0.27466691 0.19718166 0.13089517 0.12047924 0.14089175 0.1461495 0.073423922 -0.044106096 -0.1209266 -0.10112199][0.46423197 0.43046057 0.38345116 0.39403594 0.38177711 0.30511892 0.22405817 0.15506665 0.12949876 0.13462162 0.11794205 0.01966694 -0.11186533 -0.16140369 -0.11836091][0.37855166 0.31233048 0.26665944 0.22440954 0.19719176 0.16872828 0.13534707 0.09615618 0.0641022 0.057228297 0.028841518 -0.05936572 -0.1495823 -0.1636786 -0.12548245][0.025483474 0.05902952 0.099969208 0.12756948 0.11090097 0.05684875 -0.0053516477 -0.057395555 -0.073923916 -0.026357479 -0.04221116 -0.10826225 -0.16815822 -0.17194152 -0.13094825]]...]
INFO - root - 2017-12-06 05:53:31.335986: step 21310, loss = 0.78, batch loss = 0.56 (35.7 examples/sec; 0.224 sec/batch; 19h:23m:02s remains)
INFO - root - 2017-12-06 05:53:33.612576: step 21320, loss = 0.81, batch loss = 0.59 (34.2 examples/sec; 0.234 sec/batch; 20h:11m:59s remains)
INFO - root - 2017-12-06 05:53:35.875478: step 21330, loss = 0.80, batch loss = 0.58 (35.2 examples/sec; 0.227 sec/batch; 19h:39m:27s remains)
INFO - root - 2017-12-06 05:53:38.203464: step 21340, loss = 0.83, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 19h:23m:43s remains)
INFO - root - 2017-12-06 05:53:40.548346: step 21350, loss = 0.81, batch loss = 0.59 (36.3 examples/sec; 0.220 sec/batch; 19h:02m:51s remains)
INFO - root - 2017-12-06 05:53:42.856517: step 21360, loss = 0.82, batch loss = 0.61 (35.5 examples/sec; 0.225 sec/batch; 19h:27m:29s remains)
INFO - root - 2017-12-06 05:53:45.164318: step 21370, loss = 0.80, batch loss = 0.59 (35.9 examples/sec; 0.223 sec/batch; 19h:17m:04s remains)
INFO - root - 2017-12-06 05:53:47.468451: step 21380, loss = 0.77, batch loss = 0.56 (33.3 examples/sec; 0.241 sec/batch; 20h:47m:12s remains)
INFO - root - 2017-12-06 05:53:49.765220: step 21390, loss = 0.77, batch loss = 0.56 (34.5 examples/sec; 0.232 sec/batch; 20h:03m:50s remains)
INFO - root - 2017-12-06 05:53:52.058088: step 21400, loss = 0.79, batch loss = 0.57 (34.9 examples/sec; 0.230 sec/batch; 19h:50m:12s remains)
2017-12-06 05:53:52.434683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25580347 -0.31422806 -0.3405908 -0.35207012 -0.36672583 -0.36804217 -0.38764191 -0.41306266 -0.41969734 -0.4146235 -0.38703889 -0.33018541 -0.2615971 -0.2283776 -0.17174152][-0.20335037 -0.29315662 -0.34714457 -0.40861166 -0.43756133 -0.45245528 -0.47201231 -0.50827277 -0.5358541 -0.55728 -0.54516023 -0.46535152 -0.35236466 -0.29750645 -0.23545197][-0.08843796 -0.19438347 -0.27065009 -0.31534892 -0.33185872 -0.33255309 -0.33203912 -0.36574805 -0.42372984 -0.51200163 -0.56092745 -0.53817123 -0.46525526 -0.42625386 -0.4052256][-0.0012090355 -0.10046404 -0.12175411 -0.14497715 -0.10587008 -0.023931272 0.051848397 0.0598702 -0.045790877 -0.2083547 -0.35540071 -0.43918341 -0.46570814 -0.50658584 -0.5285933][0.0087226629 -0.0543326 0.022063561 0.079857513 0.20014524 0.35165289 0.5084486 0.58571351 0.50317824 0.31088072 0.084516361 -0.10419597 -0.25668353 -0.4354983 -0.55328214][-0.00092925131 0.00240504 0.16211525 0.31171584 0.46661124 0.66433239 0.88405097 1.0328251 0.98712015 0.85651541 0.63488531 0.41459242 0.19511709 -0.091835916 -0.31429908][0.0026921779 0.07566458 0.31828395 0.521844 0.66917443 0.88149941 1.1178672 1.2927924 1.2885712 1.2022902 1.054258 0.86938775 0.67619479 0.35377383 0.0780064][0.019780286 0.13351509 0.38811657 0.58502996 0.72097909 0.90829945 1.1320897 1.2913717 1.3244742 1.2573482 1.168082 1.0258417 0.84211922 0.55601847 0.2863647][0.028018489 0.1162506 0.31446427 0.46505079 0.55686653 0.69815254 0.88952434 1.0497512 1.0933203 1.084411 1.0831518 1.0092487 0.869573 0.61781049 0.34155166][-0.035688896 -0.030624032 0.091526628 0.13442864 0.15583837 0.24167311 0.41349909 0.57745695 0.63841987 0.68274879 0.70656478 0.6864506 0.60019517 0.44687977 0.28522319][-0.07548102 -0.15420312 -0.15432487 -0.18692882 -0.22965814 -0.19595572 -0.10978469 -0.017217234 0.026263528 0.08145225 0.15099211 0.1974839 0.19335836 0.12968807 0.06825313][-0.16038722 -0.27213258 -0.337425 -0.4431985 -0.55386519 -0.61163068 -0.60562551 -0.56648511 -0.55399084 -0.50858319 -0.4170641 -0.30962625 -0.22947948 -0.20071739 -0.17062406][-0.22245027 -0.34166947 -0.45019841 -0.56846529 -0.70430744 -0.8317883 -0.92228508 -0.966038 -1.0304048 -1.0208023 -0.92921054 -0.7687881 -0.60153496 -0.48132297 -0.35111609][-0.25975728 -0.33743054 -0.44040072 -0.55677032 -0.68490958 -0.8097235 -0.91366065 -0.99378228 -1.0892845 -1.1365998 -1.1031426 -0.99279082 -0.834087 -0.65768492 -0.51167512][-0.325139 -0.36902815 -0.41899377 -0.46914718 -0.54067194 -0.61237 -0.69221556 -0.79076576 -0.9026922 -1.0024962 -1.0284349 -0.98253751 -0.85857522 -0.71169615 -0.57792759]]...]
INFO - root - 2017-12-06 05:53:54.760367: step 21410, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 19h:32m:11s remains)
INFO - root - 2017-12-06 05:53:57.056064: step 21420, loss = 0.81, batch loss = 0.59 (36.2 examples/sec; 0.221 sec/batch; 19h:04m:57s remains)
INFO - root - 2017-12-06 05:53:59.335249: step 21430, loss = 0.80, batch loss = 0.59 (36.1 examples/sec; 0.221 sec/batch; 19h:08m:17s remains)
INFO - root - 2017-12-06 05:54:01.638241: step 21440, loss = 0.76, batch loss = 0.55 (34.9 examples/sec; 0.229 sec/batch; 19h:49m:36s remains)
INFO - root - 2017-12-06 05:54:03.942827: step 21450, loss = 0.77, batch loss = 0.56 (35.1 examples/sec; 0.228 sec/batch; 19h:42m:13s remains)
INFO - root - 2017-12-06 05:54:06.242425: step 21460, loss = 0.80, batch loss = 0.59 (34.6 examples/sec; 0.231 sec/batch; 19h:57m:28s remains)
INFO - root - 2017-12-06 05:54:08.618151: step 21470, loss = 0.85, batch loss = 0.64 (32.5 examples/sec; 0.246 sec/batch; 21h:16m:58s remains)
INFO - root - 2017-12-06 05:54:10.941664: step 21480, loss = 0.87, batch loss = 0.66 (36.5 examples/sec; 0.219 sec/batch; 18h:57m:37s remains)
INFO - root - 2017-12-06 05:54:13.249503: step 21490, loss = 0.79, batch loss = 0.57 (33.2 examples/sec; 0.241 sec/batch; 20h:49m:49s remains)
INFO - root - 2017-12-06 05:54:15.545409: step 21500, loss = 0.82, batch loss = 0.60 (35.8 examples/sec; 0.224 sec/batch; 19h:19m:05s remains)
2017-12-06 05:54:15.948422: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17939574 -0.027479529 -0.19161913 -0.34443834 -0.384986 -0.35848346 -0.25694776 -0.17126158 -0.067761332 0.064620219 0.079753168 0.022127375 -0.083619289 -0.20645526 -0.33772177][-0.39500332 -0.64390016 -0.845221 -0.98007196 -1.0902574 -1.1184909 -1.0705099 -0.99219149 -0.906212 -0.82000858 -0.73128343 -0.68080497 -0.74700987 -0.99823481 -1.2149329][-1.0521448 -1.1419408 -1.2683597 -1.446038 -1.6066861 -1.7061744 -1.7846999 -1.8201888 -1.7784019 -1.7200962 -1.6304649 -1.5914725 -1.5771598 -1.6571771 -1.7013838][-1.4217424 -1.5505075 -1.6896894 -1.833825 -1.9801421 -2.1327608 -2.2515502 -2.3361697 -2.3498962 -2.3051331 -2.1973231 -2.1157687 -2.0162973 -1.9943577 -2.008446][-1.3514807 -1.4569862 -1.5506206 -1.7470752 -1.9195728 -2.076031 -2.1764369 -2.2634921 -2.2972863 -2.3022592 -2.23612 -2.1636755 -2.0915613 -2.0268171 -1.9067254][-0.73446918 -0.85547131 -0.96027857 -1.0690978 -1.158993 -1.2671508 -1.3220129 -1.3852706 -1.4054213 -1.4772993 -1.5292927 -1.5695423 -1.5816454 -1.5740217 -1.53497][-0.093664177 -0.12665053 -0.13405877 -0.1572053 -0.17458403 -0.11383152 -0.051444102 -0.10936132 -0.17603487 -0.26803166 -0.3498235 -0.46936893 -0.58506095 -0.71339941 -0.81781292][0.48878345 0.5168848 0.541756 0.63879114 0.78807968 0.94595939 1.1011937 1.2143116 1.2599329 1.1985711 1.0777998 0.88892043 0.68648255 0.41839877 0.13118932][0.95873743 1.007372 1.0387605 1.1656803 1.3499275 1.6145614 1.8810138 2.0641055 2.1802123 2.1974783 2.1157744 1.9226873 1.6863419 1.3883184 1.0352957][1.0115223 1.1625252 1.2813452 1.4189032 1.5892655 1.8133798 1.9885004 2.1227343 2.2281179 2.2787402 2.260891 2.1827669 2.0255179 1.8309526 1.4777511][0.79073274 0.889143 1.0042093 1.1553292 1.2993194 1.4049672 1.4998578 1.6244149 1.698928 1.7338209 1.7121 1.6307509 1.5077468 1.4161662 1.2340536][0.57193196 0.56798989 0.57336915 0.62702811 0.71659952 0.8181445 0.86544085 0.90127909 0.91449767 0.96163768 0.96393818 0.882374 0.74135804 0.68218458 0.62139225][0.15985298 0.090241142 0.014742866 -0.00850071 0.040735133 0.085973881 0.14945653 0.18190691 0.20012501 0.2194351 0.20541483 0.15137091 0.10172125 0.066662811 -0.038809389][-0.27699786 -0.45950127 -0.61590761 -0.76523387 -0.75517017 -0.7005446 -0.65025192 -0.57963073 -0.49319726 -0.44104508 -0.45483938 -0.49714971 -0.51915741 -0.5038718 -0.56584418][-0.81890005 -1.0542241 -1.2613016 -1.3982418 -1.4876148 -1.4748244 -1.4536653 -1.4109877 -1.3245862 -1.1813915 -1.0981342 -1.0454357 -0.92504829 -0.84038264 -0.79669505]]...]
INFO - root - 2017-12-06 05:54:18.268221: step 21510, loss = 0.82, batch loss = 0.60 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:14s remains)
INFO - root - 2017-12-06 05:54:20.546104: step 21520, loss = 0.85, batch loss = 0.63 (35.2 examples/sec; 0.227 sec/batch; 19h:38m:52s remains)
INFO - root - 2017-12-06 05:54:22.842613: step 21530, loss = 0.76, batch loss = 0.54 (35.5 examples/sec; 0.225 sec/batch; 19h:27m:05s remains)
INFO - root - 2017-12-06 05:54:25.137233: step 21540, loss = 0.82, batch loss = 0.60 (35.5 examples/sec; 0.225 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-06 05:54:27.401351: step 21550, loss = 0.81, batch loss = 0.59 (35.3 examples/sec; 0.226 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-06 05:54:29.744362: step 21560, loss = 0.78, batch loss = 0.56 (33.0 examples/sec; 0.242 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-06 05:54:32.021441: step 21570, loss = 0.78, batch loss = 0.57 (33.3 examples/sec; 0.241 sec/batch; 20h:46m:38s remains)
INFO - root - 2017-12-06 05:54:34.306337: step 21580, loss = 0.75, batch loss = 0.53 (34.7 examples/sec; 0.230 sec/batch; 19h:53m:19s remains)
INFO - root - 2017-12-06 05:54:36.605974: step 21590, loss = 0.78, batch loss = 0.57 (35.0 examples/sec; 0.229 sec/batch; 19h:44m:25s remains)
INFO - root - 2017-12-06 05:54:38.973890: step 21600, loss = 0.84, batch loss = 0.63 (34.0 examples/sec; 0.235 sec/batch; 20h:19m:30s remains)
2017-12-06 05:54:39.377632: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.57913589 0.25105551 -0.14651726 -0.71077615 -1.0987794 -1.3259138 -1.4128712 -1.165652 -0.87492651 -0.913534 -0.86561233 -0.63613272 -0.46037522 -0.31611863 -0.44238281][1.5848883 0.9396081 0.26760349 -0.30098927 -0.79877317 -1.0637304 -1.054969 -0.71052313 -0.38754854 -0.32093507 -0.34431565 -0.40207088 -0.12323879 0.19248825 0.27262524][1.2618372 0.65054464 0.38025892 -0.24490449 -0.83731472 -1.0325253 -0.79828495 -0.63817751 -0.52327549 -0.44422629 -0.3141259 -0.30366606 -0.1460973 0.316673 0.32424048][1.0654086 0.26505831 0.057723038 -0.17883161 -0.54159671 -0.950681 -1.0277849 -0.72293919 -0.41609004 -0.38850579 -0.33667609 -0.069786608 0.38763359 0.66357446 0.59142184][0.92703259 0.28730705 0.11577413 -0.011180408 -0.32498276 -0.65686339 -0.49882063 -0.68574727 -0.82408172 -0.40282324 0.058577053 0.24898097 0.24410942 0.52384895 0.9270972][1.3118362 0.68780857 0.44683862 0.36433777 0.24003229 -0.12962529 -0.40469655 -0.55977768 -0.4286221 -0.57563049 -0.67855 -0.15793252 0.13538113 0.081961147 -0.11802571][1.2407031 1.0488445 0.84289289 0.38336408 -0.0260633 -0.19008295 -0.1135799 -0.20251296 0.010039173 -0.27011877 -0.33209327 -0.42718878 -0.58123976 -0.36415511 -0.28708345][1.256036 0.70297933 0.065102868 -0.23572558 -0.64511049 -0.65451419 -0.42383397 -0.50576162 -0.48535389 -0.43324015 -0.3970286 -0.42708975 -0.19463268 -0.44526809 -0.70774442][1.197365 0.62833083 0.24975294 -0.542614 -1.2113144 -0.99250036 -0.68398434 -0.85849476 -0.67733365 -0.630372 -0.72161871 -0.53781986 -0.44304749 -0.41856784 -0.43105647][0.19727716 0.13383803 -0.028496832 -0.6912204 -1.0562323 -1.0506431 -1.0891922 -1.0514171 -1.0390526 -0.93174338 -0.63872123 -0.3768436 -0.15198167 -0.27062544 -0.33183891][-0.00022175163 -0.1093469 -0.15188533 -0.8736248 -1.450651 -1.3565681 -1.2894979 -1.4484543 -1.4691504 -1.3820624 -1.4332451 -1.320145 -0.79938972 -0.29711369 -0.29566088][-0.16913241 -0.48201203 -0.93884695 -1.2025102 -1.3451794 -1.4883939 -1.7433516 -1.7046311 -1.8901159 -2.090596 -2.0894451 -1.9318631 -1.5830083 -1.3424312 -1.2610724][-0.11758986 0.053993337 -0.36178252 -0.9208433 -1.1295751 -0.81551528 -0.74046052 -0.99641591 -1.2210538 -1.1767913 -1.180488 -1.0695995 -0.89916217 -0.74971586 -0.58268076][-0.3964631 0.043025993 0.26556659 0.20074487 0.016579658 -0.10367744 -0.33000347 -0.25203902 -0.14478905 -0.18874985 -0.1467146 -0.20816593 -0.56148297 -0.44898289 -0.24471512][0.14905348 0.36207134 0.29153162 0.13231209 -0.11757008 -0.13661432 -0.31363744 -0.31001028 -0.41544053 -0.22297177 0.093008943 0.18351582 0.1869128 0.10397805 -0.002687037]]...]
INFO - root - 2017-12-06 05:54:41.703227: step 21610, loss = 0.80, batch loss = 0.58 (33.7 examples/sec; 0.237 sec/batch; 20h:29m:18s remains)
INFO - root - 2017-12-06 05:54:44.008648: step 21620, loss = 0.84, batch loss = 0.63 (33.7 examples/sec; 0.237 sec/batch; 20h:29m:31s remains)
INFO - root - 2017-12-06 05:54:46.338314: step 21630, loss = 0.83, batch loss = 0.61 (33.3 examples/sec; 0.240 sec/batch; 20h:44m:43s remains)
INFO - root - 2017-12-06 05:54:48.641529: step 21640, loss = 0.77, batch loss = 0.55 (35.3 examples/sec; 0.226 sec/batch; 19h:32m:39s remains)
INFO - root - 2017-12-06 05:54:50.961331: step 21650, loss = 0.74, batch loss = 0.53 (34.3 examples/sec; 0.233 sec/batch; 20h:06m:50s remains)
INFO - root - 2017-12-06 05:54:53.243443: step 21660, loss = 0.85, batch loss = 0.64 (35.6 examples/sec; 0.225 sec/batch; 19h:23m:07s remains)
INFO - root - 2017-12-06 05:54:55.538942: step 21670, loss = 0.78, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 19h:36m:11s remains)
INFO - root - 2017-12-06 05:54:57.875977: step 21680, loss = 0.83, batch loss = 0.62 (34.3 examples/sec; 0.233 sec/batch; 20h:08m:03s remains)
INFO - root - 2017-12-06 05:55:00.146860: step 21690, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 20h:02m:42s remains)
INFO - root - 2017-12-06 05:55:02.421639: step 21700, loss = 0.82, batch loss = 0.61 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:54s remains)
2017-12-06 05:55:04.467038: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10291753 0.021192603 -0.032939814 -0.015493751 -0.0085745528 -0.022488892 -0.031778406 -0.043033421 -0.055544209 -0.063178092 -0.065601647 -0.067163929 -0.069551989 -0.073641151 -0.076033644][0.10925627 0.0093175918 -0.06532602 -0.055811733 -0.017646275 0.00021009147 -0.015542962 -0.017279975 -0.039601665 -0.054540891 -0.069224909 -0.073876433 -0.076211087 -0.074587353 -0.074046917][0.28528616 0.16100207 0.078271709 0.040174671 0.035219468 0.032085553 0.01410871 0.0089069083 -0.02216237 -0.037973743 -0.060109284 -0.0704329 -0.077020355 -0.078262381 -0.078276739][0.45205551 0.37590688 0.33087623 0.2649405 0.18969589 0.12024174 0.072583757 0.054837786 0.016632356 -0.0064773038 -0.0450639 -0.063992716 -0.073686868 -0.077099942 -0.0781917][0.4652949 0.47767037 0.46260911 0.41313154 0.34820631 0.25246078 0.17423055 0.13456172 0.10393921 0.078447096 0.0034842193 -0.047165968 -0.068808414 -0.075787187 -0.077108711][0.25133955 0.34209478 0.39043185 0.35468617 0.34444785 0.30099881 0.24360409 0.20278749 0.17545304 0.14807814 0.069771655 -0.015444688 -0.054699361 -0.072130896 -0.076988988][-0.082317159 -0.0077775195 0.081046663 0.10890254 0.15709198 0.19766083 0.2155467 0.22679189 0.23200539 0.20855796 0.15553051 0.055294372 -0.032144241 -0.060672536 -0.071126744][-0.17705983 -0.14620967 -0.16479075 -0.12532763 -0.073563471 -0.015502825 0.045764141 0.11047942 0.17692068 0.19343007 0.18375897 0.12523976 0.023416489 -0.033500038 -0.058984943][-0.24917117 -0.18319651 -0.20547226 -0.19518375 -0.17592236 -0.14223813 -0.11060323 -0.04626048 0.033598371 0.082407929 0.10342691 0.096970506 0.027340636 -0.021284051 -0.047194667][-0.388297 -0.27181128 -0.28019938 -0.26998338 -0.24523333 -0.21037692 -0.20314026 -0.16733629 -0.12433902 -0.079629965 -0.036384147 -0.0267089 -0.0355661 -0.062683158 -0.058894537][-0.47016883 -0.34884688 -0.32795277 -0.28112957 -0.28149545 -0.27519107 -0.28126192 -0.25631344 -0.24578688 -0.21740758 -0.16815871 -0.12818936 -0.11625562 -0.098565951 -0.067220032][-0.38170472 -0.28960434 -0.31638759 -0.30211726 -0.28068218 -0.27198008 -0.29840675 -0.2812815 -0.26452073 -0.23917747 -0.19595528 -0.15176255 -0.1280944 -0.107845 -0.077934287][-0.29310632 -0.22457981 -0.20006272 -0.20921524 -0.2164084 -0.2273998 -0.2509883 -0.24278313 -0.231186 -0.21482891 -0.1698494 -0.12860233 -0.10934367 -0.0930008 -0.072964177][-0.22852188 -0.19836067 -0.16895419 -0.14885265 -0.13653904 -0.15203375 -0.17236996 -0.16926152 -0.168636 -0.14504664 -0.1242706 -0.10025542 -0.086441018 -0.079259194 -0.069506623][-0.1424067 -0.13013622 -0.1475774 -0.15134978 -0.14068647 -0.12737712 -0.12214582 -0.12442641 -0.12755109 -0.10046568 -0.074550673 -0.059804037 -0.059716791 -0.061658122 -0.066403374]]...]
INFO - root - 2017-12-06 05:55:06.795973: step 21710, loss = 0.86, batch loss = 0.65 (35.0 examples/sec; 0.228 sec/batch; 19h:43m:00s remains)
INFO - root - 2017-12-06 05:55:09.094362: step 21720, loss = 0.81, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 20h:02m:30s remains)
INFO - root - 2017-12-06 05:55:11.432794: step 21730, loss = 0.80, batch loss = 0.59 (34.2 examples/sec; 0.234 sec/batch; 20h:12m:30s remains)
INFO - root - 2017-12-06 05:55:13.729939: step 21740, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:09s remains)
INFO - root - 2017-12-06 05:55:16.035064: step 21750, loss = 0.84, batch loss = 0.63 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:52s remains)
INFO - root - 2017-12-06 05:55:18.325110: step 21760, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 19h:51m:39s remains)
INFO - root - 2017-12-06 05:55:20.637918: step 21770, loss = 0.81, batch loss = 0.59 (34.0 examples/sec; 0.235 sec/batch; 20h:16m:46s remains)
INFO - root - 2017-12-06 05:55:22.924887: step 21780, loss = 0.77, batch loss = 0.55 (35.7 examples/sec; 0.224 sec/batch; 19h:21m:32s remains)
INFO - root - 2017-12-06 05:55:25.253580: step 21790, loss = 0.84, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 23h:08m:24s remains)
INFO - root - 2017-12-06 05:55:27.585561: step 21800, loss = 0.80, batch loss = 0.59 (32.7 examples/sec; 0.244 sec/batch; 21h:05m:25s remains)
2017-12-06 05:55:28.663078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01696825 -0.016695634 -0.016963117 -0.018673584 -0.020350583 -0.021757185 -0.023688413 -0.026072785 -0.028951306 -0.032657448 -0.03621538 -0.038772836 -0.041222166 -0.04376876 -0.04653826][-0.0193979 -0.018919371 -0.018233269 -0.0194109 -0.02084367 -0.022630386 -0.025198743 -0.028319571 -0.032007322 -0.03631888 -0.040074334 -0.042664833 -0.044839837 -0.046948027 -0.0491352][-0.023355082 -0.022615477 -0.021515429 -0.021653898 -0.022395141 -0.023978256 -0.02658686 -0.029994622 -0.034129988 -0.038714569 -0.042454865 -0.045059357 -0.047088828 -0.048826922 -0.050574005][-0.027157925 -0.026232466 -0.025110126 -0.024049714 -0.023505919 -0.024250597 -0.026418366 -0.029676508 -0.03390792 -0.038595483 -0.042419706 -0.045104209 -0.047238056 -0.048802625 -0.050314307][-0.030675795 -0.029705275 -0.028279379 -0.026288815 -0.024691254 -0.024424434 -0.025809921 -0.028679006 -0.032734495 -0.037330758 -0.041205477 -0.044019692 -0.046157837 -0.047530007 -0.04884351][-0.033490866 -0.03247213 -0.030904241 -0.028455406 -0.026101366 -0.024842203 -0.025404178 -0.027734399 -0.031433843 -0.0357598 -0.03963279 -0.042505957 -0.044539168 -0.045698915 -0.04699482][-0.034775525 -0.033777934 -0.032309003 -0.029949185 -0.027394325 -0.025537431 -0.02535706 -0.02705957 -0.03035387 -0.034337405 -0.038020536 -0.040856469 -0.042774174 -0.043799777 -0.045154739][-0.033645302 -0.032773923 -0.031749468 -0.02993786 -0.027817383 -0.026124746 -0.025696367 -0.026967071 -0.029856205 -0.033438057 -0.036808729 -0.039502274 -0.041293621 -0.042326797 -0.043728728][-0.030442137 -0.029648084 -0.029168915 -0.028381333 -0.027393274 -0.026678808 -0.026719108 -0.027965628 -0.030502439 -0.033614434 -0.036540955 -0.038947284 -0.040594772 -0.041696519 -0.04313295][-0.026014782 -0.02530659 -0.025575995 -0.026030071 -0.02659823 -0.027385592 -0.028518047 -0.030237041 -0.032660674 -0.035334706 -0.037794124 -0.039793946 -0.041195825 -0.042255286 -0.043581635][-0.02154921 -0.020913407 -0.022004187 -0.023851305 -0.026143409 -0.028656997 -0.031153359 -0.03365235 -0.036170363 -0.038521033 -0.040516164 -0.042078342 -0.043173455 -0.044083055 -0.045241714][-0.017800309 -0.0174051 -0.019301742 -0.022444867 -0.026251182 -0.030345116 -0.034159131 -0.037496477 -0.040313765 -0.04256855 -0.04424927 -0.045471318 -0.04626314 -0.046966165 -0.047791228][-0.015427165 -0.01527296 -0.01772397 -0.021742143 -0.026688039 -0.031986311 -0.036846556 -0.040930532 -0.044129025 -0.046487562 -0.048085529 -0.049131334 -0.049697179 -0.050185371 -0.0506156][-0.014461473 -0.014530301 -0.0172484 -0.021590576 -0.027046055 -0.032947388 -0.038438439 -0.043022424 -0.046601072 -0.049203463 -0.0509621 -0.052119356 -0.052718218 -0.053048249 -0.053340167][-0.015112668 -0.015510321 -0.018420987 -0.022723585 -0.028055042 -0.033825692 -0.039268829 -0.043873031 -0.047581259 -0.050329603 -0.052342314 -0.053805195 -0.0546883 -0.055129629 -0.05560571]]...]
INFO - root - 2017-12-06 05:55:30.966135: step 21810, loss = 0.76, batch loss = 0.55 (36.5 examples/sec; 0.219 sec/batch; 18h:55m:58s remains)
INFO - root - 2017-12-06 05:55:33.263731: step 21820, loss = 0.72, batch loss = 0.51 (32.7 examples/sec; 0.245 sec/batch; 21h:06m:50s remains)
INFO - root - 2017-12-06 05:55:35.594314: step 21830, loss = 0.79, batch loss = 0.58 (31.8 examples/sec; 0.252 sec/batch; 21h:43m:10s remains)
INFO - root - 2017-12-06 05:55:37.911807: step 21840, loss = 0.82, batch loss = 0.60 (33.9 examples/sec; 0.236 sec/batch; 20h:21m:15s remains)
INFO - root - 2017-12-06 05:55:40.220247: step 21850, loss = 0.76, batch loss = 0.55 (34.6 examples/sec; 0.231 sec/batch; 19h:58m:06s remains)
INFO - root - 2017-12-06 05:55:42.527294: step 21860, loss = 0.83, batch loss = 0.62 (33.2 examples/sec; 0.241 sec/batch; 20h:49m:05s remains)
INFO - root - 2017-12-06 05:55:44.859281: step 21870, loss = 0.83, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:01s remains)
INFO - root - 2017-12-06 05:55:47.193092: step 21880, loss = 0.83, batch loss = 0.62 (34.8 examples/sec; 0.230 sec/batch; 19h:49m:32s remains)
INFO - root - 2017-12-06 05:55:49.481496: step 21890, loss = 0.84, batch loss = 0.62 (33.5 examples/sec; 0.239 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-06 05:55:51.795838: step 21900, loss = 0.76, batch loss = 0.55 (35.0 examples/sec; 0.229 sec/batch; 19h:43m:31s remains)
2017-12-06 05:55:53.106391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.17520753 -0.21216932 -0.25769079 -0.30637196 -0.34670377 -0.35552585 -0.33647504 -0.30558261 -0.27417067 -0.24176551 -0.2050146 -0.16885766 -0.14119433 -0.12588644 -0.12426925][-0.23120071 -0.28513914 -0.35439003 -0.44183195 -0.56384921 -0.67011166 -0.71826226 -0.69337147 -0.6204344 -0.51519436 -0.413976 -0.30613303 -0.21525037 -0.15570858 -0.12979111][-0.25221217 -0.27940956 -0.30243552 -0.34326732 -0.44937366 -0.59632987 -0.72922683 -0.77858114 -0.75074303 -0.65955466 -0.55623847 -0.41453928 -0.27748805 -0.18793836 -0.14259288][-0.27453774 -0.23982467 -0.16605109 -0.074486695 -0.042917248 -0.0917676 -0.2191809 -0.34427017 -0.4478569 -0.47483289 -0.44535911 -0.35303289 -0.26702702 -0.19339217 -0.1464715][-0.2513904 -0.13546427 0.062315434 0.29551977 0.50794429 0.6038987 0.54638523 0.38736206 0.17020734 -0.020928815 -0.15759632 -0.19458666 -0.18402667 -0.13696681 -0.077468306][-0.22550906 -0.033861145 0.31156009 0.74941194 1.165689 1.4567043 1.53006 1.4125257 1.1211338 0.72849917 0.36826116 0.12273075 -0.019438349 -0.067919 -0.069092862][-0.25107569 -0.032863826 0.36485392 0.89858347 1.4920204 1.9966054 2.1904972 2.1412239 1.840843 1.3733753 0.88372862 0.4819504 0.19871347 0.067276984 0.035800755][-0.35300374 -0.20528305 0.11867909 0.58938253 1.1434801 1.7190913 2.0528653 2.1443281 1.9330473 1.4963596 1.0076001 0.57091975 0.26670384 0.10687834 0.042114273][-0.37481511 -0.29477966 -0.081192471 0.28208578 0.75874317 1.2434249 1.5649267 1.7046927 1.55462 1.2393173 0.84024471 0.44655573 0.21501096 0.091075689 0.036634579][-0.4205963 -0.40400445 -0.27903765 -0.026539266 0.33643138 0.76477957 1.1105194 1.2799511 1.2382574 0.942103 0.5799939 0.27199727 0.095947221 0.002459541 -0.020021893][-0.50309181 -0.56484056 -0.53968573 -0.4165979 -0.20236009 0.0573155 0.33528447 0.53031987 0.57547849 0.44420332 0.1733904 0.022036903 -0.047997706 -0.086822525 -0.073685005][-0.4866693 -0.60416836 -0.68950659 -0.67928576 -0.61753857 -0.46710771 -0.2506099 -0.051851247 0.017934196 0.021974511 -0.11909689 -0.20204504 -0.18101834 -0.15418889 -0.11158069][-0.33828422 -0.44988632 -0.55979252 -0.63879794 -0.690551 -0.6587823 -0.49370527 -0.32845125 -0.21182793 -0.089541987 -0.10395937 -0.14125609 -0.1360781 -0.11047311 -0.10177221][-0.23941419 -0.31067592 -0.39167565 -0.43231744 -0.48698664 -0.49740696 -0.419995 -0.30351287 -0.21409068 -0.10316345 -0.099510491 -0.1005616 -0.094972521 -0.097724214 -0.090586253][-0.13821027 -0.16922441 -0.21243723 -0.22892189 -0.23392285 -0.25143605 -0.20402518 -0.14865011 -0.10537995 -0.057726543 -0.044449426 -0.0641787 -0.09269385 -0.096403323 -0.086638324]]...]
INFO - root - 2017-12-06 05:55:55.376256: step 21910, loss = 0.78, batch loss = 0.57 (36.4 examples/sec; 0.220 sec/batch; 18h:57m:21s remains)
INFO - root - 2017-12-06 05:55:57.618538: step 21920, loss = 0.82, batch loss = 0.61 (35.8 examples/sec; 0.223 sec/batch; 19h:16m:36s remains)
INFO - root - 2017-12-06 05:55:59.918870: step 21930, loss = 0.76, batch loss = 0.54 (34.0 examples/sec; 0.235 sec/batch; 20h:18m:15s remains)
INFO - root - 2017-12-06 05:56:02.213213: step 21940, loss = 0.73, batch loss = 0.52 (32.9 examples/sec; 0.243 sec/batch; 20h:59m:46s remains)
INFO - root - 2017-12-06 05:56:04.509773: step 21950, loss = 0.82, batch loss = 0.60 (34.7 examples/sec; 0.230 sec/batch; 19h:52m:22s remains)
INFO - root - 2017-12-06 05:56:06.863092: step 21960, loss = 0.76, batch loss = 0.55 (33.9 examples/sec; 0.236 sec/batch; 20h:20m:47s remains)
INFO - root - 2017-12-06 05:56:09.200417: step 21970, loss = 0.80, batch loss = 0.59 (35.1 examples/sec; 0.228 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-06 05:56:11.529719: step 21980, loss = 0.87, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 19h:39m:22s remains)
INFO - root - 2017-12-06 05:56:13.824036: step 21990, loss = 0.74, batch loss = 0.53 (35.2 examples/sec; 0.227 sec/batch; 19h:36m:38s remains)
INFO - root - 2017-12-06 05:56:16.138643: step 22000, loss = 0.76, batch loss = 0.55 (34.2 examples/sec; 0.234 sec/batch; 20h:10m:07s remains)
2017-12-06 05:56:16.514936: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50614095 0.40785861 0.41670996 0.44212085 0.31271428 0.054692224 -0.29869246 -0.74488878 -1.0990117 -1.3051611 -1.4601147 -1.5520732 -1.4877874 -1.3313684 -1.1321368][-0.056747776 -0.10564393 -0.075661734 0.020225003 0.016210064 -0.078754164 -0.26249593 -0.55344319 -0.85175556 -1.0333132 -1.1144696 -1.1583977 -1.0579286 -0.87964821 -0.78292429][-0.92551208 -0.73923826 -0.37260526 -0.071339473 0.10102408 0.021847554 -0.267106 -0.52119547 -0.73176438 -0.90194881 -0.8949604 -0.73727447 -0.49046516 -0.23509282 0.033997603][-1.159755 -1.0340027 -0.68713766 -0.31638739 -0.0071663111 0.10308543 -0.038071632 -0.28205928 -0.54692847 -0.76912838 -0.74675274 -0.45161414 -0.038758989 0.34982097 0.60345566][-0.92964774 -0.8267467 -0.45130181 -0.12089075 0.10205506 0.17717929 0.070863381 -0.078331053 -0.26522407 -0.57519722 -0.66219687 -0.4673776 -0.05227311 0.41980171 0.77908212][-0.62553668 -0.58288229 -0.32583994 0.052830443 0.35672009 0.41464072 0.28573 0.16793887 0.086333096 -0.10063738 -0.16723153 -0.1647149 0.09363246 0.512774 0.9360714][-0.48058087 -0.5518015 -0.35940295 -0.034247272 0.30152953 0.54472727 0.53159344 0.39702255 0.3699165 0.37667465 0.46440452 0.41192263 0.38907838 0.52882469 0.86039335][-0.047299717 -0.28328994 -0.24820553 -0.0032451004 0.37785137 0.63277841 0.72577471 0.73065454 0.71479613 0.61989009 0.58905929 0.59487444 0.67792904 0.82966256 1.0006074][0.22555183 0.0051824972 0.025103286 0.12310837 0.41945374 0.68043357 0.75672174 0.67517239 0.61265683 0.51969773 0.37942207 0.16741307 0.13123968 0.39160919 0.6063363][0.28241175 -0.063084751 -0.074676394 0.053210363 0.24676572 0.47970551 0.58429873 0.47735852 0.27159441 0.13060743 -0.011436708 -0.21596092 -0.45695776 -0.47809815 -0.4103899][0.43064082 0.1072996 -0.049519781 -0.033891696 -0.04526839 -0.02222348 -0.063923955 -0.20581399 -0.43410134 -0.64919734 -0.8788954 -1.0981328 -1.3387258 -1.4188432 -1.5418411][0.31502402 0.093728453 -0.098241031 -0.18460456 -0.34361732 -0.47834492 -0.68081194 -0.87348312 -1.0889072 -1.3145996 -1.5472898 -1.7472881 -1.9537244 -2.0589137 -2.0784028][-0.072089821 -0.17053661 -0.29308024 -0.40710485 -0.66746432 -0.89360493 -1.1385555 -1.3071874 -1.4552966 -1.5981959 -1.7656636 -1.983148 -2.1811709 -2.2744577 -2.3029656][-0.56446648 -0.61417645 -0.70677412 -0.83107889 -0.96919882 -1.1584575 -1.4107754 -1.5473928 -1.5746785 -1.5691036 -1.5540079 -1.5642676 -1.6532209 -1.8167999 -1.9164805][-0.86808425 -0.92928147 -0.99141932 -1.066276 -1.0673099 -1.097194 -1.1437509 -1.2165281 -1.2163328 -1.1804657 -1.1294441 -1.0595398 -1.0726068 -1.1112177 -1.184103]]...]
INFO - root - 2017-12-06 05:56:18.812217: step 22010, loss = 0.90, batch loss = 0.69 (34.0 examples/sec; 0.235 sec/batch; 20h:18m:37s remains)
INFO - root - 2017-12-06 05:56:21.142130: step 22020, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 19h:33m:00s remains)
INFO - root - 2017-12-06 05:56:23.490366: step 22030, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 19h:50m:43s remains)
INFO - root - 2017-12-06 05:56:25.812609: step 22040, loss = 0.79, batch loss = 0.58 (31.3 examples/sec; 0.255 sec/batch; 22h:01m:06s remains)
INFO - root - 2017-12-06 05:56:28.126584: step 22050, loss = 0.70, batch loss = 0.49 (35.4 examples/sec; 0.226 sec/batch; 19h:30m:48s remains)
INFO - root - 2017-12-06 05:56:30.458022: step 22060, loss = 0.73, batch loss = 0.51 (34.4 examples/sec; 0.232 sec/batch; 20h:01m:36s remains)
INFO - root - 2017-12-06 05:56:32.766267: step 22070, loss = 0.81, batch loss = 0.59 (34.3 examples/sec; 0.233 sec/batch; 20h:07m:57s remains)
INFO - root - 2017-12-06 05:56:35.064705: step 22080, loss = 0.85, batch loss = 0.63 (34.7 examples/sec; 0.231 sec/batch; 19h:53m:54s remains)
INFO - root - 2017-12-06 05:56:37.348829: step 22090, loss = 0.80, batch loss = 0.59 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:56s remains)
INFO - root - 2017-12-06 05:56:39.635635: step 22100, loss = 0.84, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 19h:46m:05s remains)
2017-12-06 05:56:41.867770: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.7317428 0.48653612 0.35493442 0.31202409 0.31244478 0.25415632 0.16717294 0.11244348 0.081068315 0.055110775 0.031944811 0.02068726 0.041154973 0.046646155 0.0059999749][0.99958712 0.67924172 0.53023046 0.44581237 0.41258273 0.35624582 0.30710396 0.26527661 0.20502648 0.15902895 0.12564859 0.091736518 0.10034391 0.10508848 0.054096349][0.79499054 0.58294308 0.43630525 0.28071514 0.21852049 0.20093098 0.19701567 0.17920169 0.14830559 0.10215523 0.0458693 0.00098545849 -0.019414149 -0.024372526 -0.011235803][0.48324588 0.29589754 0.1434615 0.0020915344 -0.055147212 -0.066759966 -0.056399643 -0.034148224 -0.028728362 -0.031771071 -0.0268199 -0.024527632 -0.048086014 -0.074799165 -0.063424289][0.15429589 0.044892453 -0.073433094 -0.1717305 -0.22624731 -0.2005491 -0.17721575 -0.1702629 -0.15785277 -0.13039795 -0.0929999 -0.082767762 -0.059611857 -0.05997261 -0.0723065][-0.10526372 -0.1341666 -0.17690361 -0.23542061 -0.25121203 -0.18561655 -0.14542443 -0.12384944 -0.11720734 -0.11450735 -0.083407015 -0.080170691 -0.048733216 -0.052037127 -0.060234118][-0.40618351 -0.35480779 -0.3408024 -0.33224386 -0.31912765 -0.26305836 -0.20330119 -0.14323586 -0.10117698 -0.084342211 -0.079368763 -0.081300721 -0.069046557 -0.073184893 -0.071915835][-0.48088622 -0.39254355 -0.33347106 -0.31111261 -0.29379424 -0.2591095 -0.2222091 -0.17051482 -0.13074879 -0.10303735 -0.093367212 -0.088112749 -0.081467636 -0.08168292 -0.081166461][-0.51508623 -0.40262535 -0.3300606 -0.2707524 -0.22989786 -0.19108132 -0.15904498 -0.1320896 -0.10717637 -0.094547562 -0.087527357 -0.085341156 -0.081331879 -0.081326328 -0.081416748][-0.47634068 -0.36748123 -0.25730938 -0.1807559 -0.14106308 -0.13006137 -0.11801027 -0.10108098 -0.087567337 -0.082903974 -0.08263661 -0.082074769 -0.081639424 -0.08220847 -0.082222506][-0.33279371 -0.25040531 -0.17564943 -0.11805839 -0.10245465 -0.092291564 -0.085205525 -0.080014445 -0.081405282 -0.082714863 -0.081965648 -0.082260936 -0.0824373 -0.082309082 -0.0822425][-0.18555488 -0.11800683 -0.0764548 -0.058428861 -0.072078049 -0.0803355 -0.089515589 -0.09292198 -0.087003253 -0.0829126 -0.08229664 -0.081986345 -0.082242407 -0.08226385 -0.082280554][-0.1703317 -0.14042926 -0.11625427 -0.071085453 -0.062228341 -0.076321639 -0.12240633 -0.1253601 -0.10755523 -0.09194386 -0.084839314 -0.084016919 -0.082728744 -0.082369454 -0.082402319][-0.13634981 -0.13313298 -0.11942762 -0.074231863 -0.055386845 -0.060887732 -0.062036749 -0.070684008 -0.0774902 -0.0836297 -0.0849457 -0.0840131 -0.082633272 -0.082554623 -0.0826176][-0.11980137 -0.10630381 -0.097437881 -0.08455079 -0.07828778 -0.074062593 -0.062881581 -0.0557848 -0.08057671 -0.086298138 -0.082537323 -0.085521534 -0.081693962 -0.082198441 -0.0825838]]...]
INFO - root - 2017-12-06 05:56:44.176704: step 22110, loss = 0.83, batch loss = 0.62 (34.6 examples/sec; 0.232 sec/batch; 19h:57m:35s remains)
INFO - root - 2017-12-06 05:56:46.507883: step 22120, loss = 0.82, batch loss = 0.60 (34.6 examples/sec; 0.231 sec/batch; 19h:57m:20s remains)
INFO - root - 2017-12-06 05:56:48.803710: step 22130, loss = 0.78, batch loss = 0.57 (34.8 examples/sec; 0.230 sec/batch; 19h:50m:26s remains)
INFO - root - 2017-12-06 05:56:51.109863: step 22140, loss = 0.77, batch loss = 0.56 (36.4 examples/sec; 0.220 sec/batch; 18h:57m:47s remains)
INFO - root - 2017-12-06 05:56:53.397718: step 22150, loss = 0.87, batch loss = 0.65 (36.9 examples/sec; 0.217 sec/batch; 18h:40m:14s remains)
INFO - root - 2017-12-06 05:56:55.740686: step 22160, loss = 0.78, batch loss = 0.57 (32.2 examples/sec; 0.249 sec/batch; 21h:25m:46s remains)
INFO - root - 2017-12-06 05:56:58.011690: step 22170, loss = 0.86, batch loss = 0.65 (36.3 examples/sec; 0.220 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-06 05:57:00.312211: step 22180, loss = 0.76, batch loss = 0.55 (33.8 examples/sec; 0.237 sec/batch; 20h:23m:31s remains)
INFO - root - 2017-12-06 05:57:02.621532: step 22190, loss = 0.76, batch loss = 0.54 (33.2 examples/sec; 0.241 sec/batch; 20h:44m:36s remains)
INFO - root - 2017-12-06 05:57:04.943438: step 22200, loss = 0.79, batch loss = 0.58 (33.7 examples/sec; 0.237 sec/batch; 20h:27m:55s remains)
2017-12-06 05:57:05.335086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40159875 -0.38166165 -0.33415249 -0.27962506 -0.22333518 -0.17317848 -0.12459946 -0.10986383 -0.12182903 -0.12933648 -0.11031574 -0.09738262 -0.096112356 -0.10194981 -0.1232347][-0.40627068 -0.44584721 -0.43921304 -0.39780718 -0.34257615 -0.26036096 -0.17700151 -0.13730331 -0.14169508 -0.14988858 -0.12369951 -0.089728892 -0.069489911 -0.063135944 -0.075815246][-0.5666641 -0.70028049 -0.76266086 -0.77154762 -0.7499696 -0.65898579 -0.53645819 -0.44095463 -0.38312346 -0.32892531 -0.26738611 -0.18116173 -0.13063711 -0.094485566 -0.079886988][-0.70326793 -0.94845027 -1.1088357 -1.2005025 -1.2469444 -1.2014867 -1.0820395 -0.93583095 -0.78146285 -0.59057176 -0.42602277 -0.30729812 -0.23772211 -0.19434857 -0.18190089][-0.4208746 -0.7357114 -0.95210481 -1.0490379 -1.1138376 -1.132733 -1.0662165 -0.92528635 -0.73210537 -0.49876285 -0.32407588 -0.22215024 -0.18505453 -0.1686516 -0.18130121][0.12742208 -0.11085773 -0.26622391 -0.33492848 -0.40767127 -0.50804222 -0.55712616 -0.52272612 -0.39744645 -0.21795976 -0.085864246 -0.010050319 -0.010454081 -0.029620066 -0.067049921][1.0938616 1.0147722 0.97426128 0.91264057 0.80564904 0.611144 0.43733329 0.35083884 0.32389551 0.32768691 0.3388887 0.34119278 0.29230309 0.22156553 0.14341792][1.7073855 1.6825776 1.6684424 1.6008645 1.482433 1.2647753 1.035167 0.82660896 0.641242 0.45686728 0.32136875 0.22369553 0.14655519 0.088976845 0.0686616][0.98457742 0.95945263 0.96275485 0.92535329 0.87765574 0.7732563 0.62002826 0.45335764 0.29702985 0.10913749 -0.016578346 -0.10684133 -0.16165438 -0.20204519 -0.16675219][-0.0945225 -0.1475722 -0.15288007 -0.12691043 -0.074598484 -0.048625942 -0.053894449 -0.10570727 -0.14946719 -0.22285385 -0.29532623 -0.34749591 -0.35445648 -0.34233803 -0.30528587][-0.73998618 -0.79782254 -0.79338807 -0.70801842 -0.55383968 -0.41965616 -0.32094485 -0.26944268 -0.22097938 -0.22241911 -0.26617873 -0.26797083 -0.24026249 -0.2241402 -0.20326516][-0.94058114 -0.99427038 -0.97824311 -0.90402418 -0.76758969 -0.598821 -0.47844106 -0.37419677 -0.27917448 -0.23261136 -0.21612062 -0.19631845 -0.16009286 -0.13504016 -0.12471844][-0.6422351 -0.63368809 -0.59860295 -0.57741237 -0.54934961 -0.47519511 -0.40698177 -0.34118217 -0.27192956 -0.21625689 -0.18779872 -0.12163253 -0.0754337 -0.052216917 -0.052361935][-0.22251962 -0.16616833 -0.11817907 -0.10468674 -0.10041289 -0.13148679 -0.15843102 -0.1624459 -0.14486074 -0.12122255 -0.12033693 -0.090975076 -0.044448223 -0.041245013 -0.038087003][-0.072247736 -0.025633894 0.020311743 0.034358494 0.02365429 -0.0081934109 -0.033139024 -0.063906737 -0.0875876 -0.083023988 -0.056909986 -0.052707624 -0.041192409 -0.042527113 -0.043351728]]...]
INFO - root - 2017-12-06 05:57:07.647866: step 22210, loss = 0.77, batch loss = 0.56 (34.8 examples/sec; 0.230 sec/batch; 19h:49m:12s remains)
INFO - root - 2017-12-06 05:57:10.009737: step 22220, loss = 0.79, batch loss = 0.57 (33.8 examples/sec; 0.237 sec/batch; 20h:25m:28s remains)
INFO - root - 2017-12-06 05:57:12.302736: step 22230, loss = 0.85, batch loss = 0.64 (35.1 examples/sec; 0.228 sec/batch; 19h:39m:01s remains)
INFO - root - 2017-12-06 05:57:14.630682: step 22240, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 19h:58m:54s remains)
INFO - root - 2017-12-06 05:57:16.912113: step 22250, loss = 0.84, batch loss = 0.63 (35.0 examples/sec; 0.228 sec/batch; 19h:41m:09s remains)
INFO - root - 2017-12-06 05:57:19.279163: step 22260, loss = 0.71, batch loss = 0.50 (34.1 examples/sec; 0.234 sec/batch; 20h:12m:07s remains)
INFO - root - 2017-12-06 05:57:21.575767: step 22270, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-06 05:57:23.887870: step 22280, loss = 0.85, batch loss = 0.63 (35.8 examples/sec; 0.223 sec/batch; 19h:14m:39s remains)
INFO - root - 2017-12-06 05:57:26.251406: step 22290, loss = 0.80, batch loss = 0.59 (33.9 examples/sec; 0.236 sec/batch; 20h:21m:08s remains)
INFO - root - 2017-12-06 05:57:28.578776: step 22300, loss = 0.79, batch loss = 0.58 (35.6 examples/sec; 0.225 sec/batch; 19h:22m:13s remains)
2017-12-06 05:57:28.978181: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.81451845 0.45919815 0.31421304 0.39568269 0.44567892 0.17524803 0.13564947 0.10692247 0.16326776 0.10794193 -0.24126351 -0.64760351 -1.0100851 -1.1674041 -1.329834][0.25478238 0.13180774 -0.12158658 -0.2804665 -0.23611999 0.10909816 0.20582771 0.17093852 -0.010426126 -0.12879179 -0.32128632 -0.60425466 -0.97245 -1.2575269 -1.4346581][0.0038477257 0.030907474 0.019673564 -0.051059928 -0.14526059 -0.2512126 -0.16646886 0.15161031 0.19049788 0.24818763 0.032112442 -0.20234826 -0.71600449 -1.1862037 -1.4311357][-0.12970538 -0.15867285 -0.073053479 -0.026871957 0.13466248 0.17098206 0.29157066 0.50532365 0.65452319 0.6767298 0.428539 0.1830492 -0.41902685 -0.94428855 -1.4789307][-0.24858016 -0.50801736 -0.56872475 -0.38643098 -0.019168831 0.37098041 0.61024082 1.1129891 1.4138585 1.6079171 1.4938284 0.9571023 0.18390256 -0.41305816 -1.0562816][-0.22779158 -0.52683938 -0.63949525 -0.58714139 -0.25752792 0.44091865 1.170595 1.6972879 2.205863 2.3286684 2.255033 1.765339 0.89538658 -0.080937579 -0.88896352][-0.17281294 -0.31961012 -0.47897378 -0.42262974 -0.097461671 0.5319072 1.3975612 2.2916713 2.8591375 2.9173963 2.6272593 2.0563903 1.0871738 0.23164451 -0.40794033][-0.47690779 -0.47690767 -0.53995454 -0.4399699 0.040781222 0.70068073 1.7209681 2.4580653 3.0921519 3.0292454 2.6335919 1.8418791 0.89651895 -0.018212341 -0.51143748][-0.75748223 -0.75439328 -0.49402589 -0.38752732 -0.27224851 0.3471278 1.3247137 2.401907 3.0219338 3.1476607 2.7255137 1.889429 0.90072912 -0.041748151 -0.5588088][-0.69465452 -0.87776417 -0.68022466 -0.265761 0.15077677 0.31158188 0.867318 1.5568438 2.1302948 2.3788013 2.071085 1.4183663 0.7126624 -0.081497185 -0.86997157][-0.50684232 -0.53633195 -0.5600509 -0.46684319 0.05470673 0.53464168 0.85129195 1.0862586 1.456194 1.4629188 1.0961277 0.6207276 -0.016471632 -0.46843714 -0.72735447][-0.58764845 -0.44659677 -0.34830195 -0.18731353 0.036882438 0.39064327 0.74265683 0.80210441 0.76775289 0.768715 0.549596 -0.04647309 -0.65986109 -0.96518141 -1.1374614][-0.25853926 -0.24572968 0.0039735138 0.095826425 0.12951016 0.23227748 0.4249371 0.61525893 0.5756858 0.27956954 -0.27301046 -0.53566855 -0.83419311 -0.943094 -1.0373816][0.36634353 0.15706801 0.11522817 0.29856104 0.57210541 0.63350606 0.52049494 0.47154704 0.36189154 0.21131057 -0.20033109 -0.76008189 -1.1234993 -1.2428381 -1.0972116][0.51025015 0.46782413 0.46309575 0.38448569 0.35743943 0.43143442 0.44297066 0.33927158 0.13219118 -0.13860779 -0.41772413 -0.73851234 -0.96811366 -1.0930037 -0.97058386]]...]
INFO - root - 2017-12-06 05:57:31.298694: step 22310, loss = 0.81, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 19h:40m:17s remains)
INFO - root - 2017-12-06 05:57:33.620612: step 22320, loss = 0.79, batch loss = 0.58 (34.2 examples/sec; 0.234 sec/batch; 20h:08m:06s remains)
INFO - root - 2017-12-06 05:57:35.945041: step 22330, loss = 0.77, batch loss = 0.56 (32.4 examples/sec; 0.247 sec/batch; 21h:15m:13s remains)
INFO - root - 2017-12-06 05:57:38.254745: step 22340, loss = 0.82, batch loss = 0.61 (32.4 examples/sec; 0.247 sec/batch; 21h:16m:55s remains)
INFO - root - 2017-12-06 05:57:40.571360: step 22350, loss = 0.77, batch loss = 0.56 (35.1 examples/sec; 0.228 sec/batch; 19h:38m:23s remains)
INFO - root - 2017-12-06 05:57:42.862703: step 22360, loss = 0.85, batch loss = 0.63 (35.6 examples/sec; 0.224 sec/batch; 19h:20m:03s remains)
INFO - root - 2017-12-06 05:57:45.130249: step 22370, loss = 0.89, batch loss = 0.68 (35.2 examples/sec; 0.228 sec/batch; 19h:36m:18s remains)
INFO - root - 2017-12-06 05:57:47.418800: step 22380, loss = 0.76, batch loss = 0.54 (35.3 examples/sec; 0.227 sec/batch; 19h:30m:49s remains)
INFO - root - 2017-12-06 05:57:49.733596: step 22390, loss = 0.78, batch loss = 0.56 (35.3 examples/sec; 0.227 sec/batch; 19h:32m:48s remains)
INFO - root - 2017-12-06 05:57:52.025346: step 22400, loss = 0.77, batch loss = 0.55 (33.5 examples/sec; 0.239 sec/batch; 20h:34m:46s remains)
2017-12-06 05:57:52.476267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22055529 -0.27750048 -0.32815415 -0.36132848 -0.37563342 -0.37560526 -0.38838282 -0.39296022 -0.40145838 -0.40037841 -0.38444415 -0.33735096 -0.27145395 -0.20268202 -0.15478921][-0.24959829 -0.29369253 -0.31426862 -0.32848623 -0.36490113 -0.41949356 -0.49954215 -0.53689539 -0.60646892 -0.61977506 -0.6049428 -0.50228 -0.37823743 -0.26615268 -0.19320397][-0.21456003 -0.20196465 -0.14773169 -0.085960381 -0.053895772 -0.089116953 -0.19368082 -0.27422911 -0.37093177 -0.40240875 -0.41504088 -0.32187867 -0.21901225 -0.11779507 -0.096917592][-0.078996122 0.03273309 0.19287893 0.33816797 0.43415132 0.41478774 0.32763717 0.18961239 0.042373911 -0.0055339783 -0.028838284 0.053650454 0.10152683 0.12282006 0.026951872][-0.0093744621 0.19656208 0.49790213 0.70489442 0.853716 0.86303127 0.83277881 0.76155376 0.71944559 0.6858604 0.74852943 0.8942982 0.94477797 0.85055947 0.60422885][0.088589191 0.29614243 0.62642527 0.98211884 1.2663784 1.3465418 1.3579998 1.2609499 1.2085593 1.2316118 1.4311714 1.6348662 1.7943152 1.7396387 1.4428955][0.19839939 0.41997448 0.72933245 1.0349783 1.3302704 1.538347 1.6479504 1.6656245 1.7057556 1.744288 1.9233112 2.1190667 2.2940238 2.2049839 1.991864][0.12443002 0.35916093 0.69868982 0.95735693 1.1487521 1.2594869 1.3751012 1.394986 1.4462812 1.5547193 1.8018674 1.9888699 2.1589913 2.1613262 1.95909][-0.071306914 0.073584676 0.3151179 0.54392695 0.73999321 0.80087137 0.81025946 0.77162755 0.78287435 0.91049206 1.1823627 1.422552 1.5513245 1.5079905 1.2957238][-0.31323624 -0.31139594 -0.22094436 -0.13166605 -0.037184771 0.014074787 0.014558651 -0.042345624 -0.062571466 -0.037690867 0.13118081 0.35257089 0.50113058 0.445572 0.28393278][-0.53185928 -0.64805734 -0.70686579 -0.73833537 -0.74562454 -0.77004492 -0.76067472 -0.78927624 -0.80383217 -0.80245125 -0.77199817 -0.75241303 -0.78329384 -0.87443709 -0.92578638][-0.595667 -0.79653609 -0.9576087 -1.1200635 -1.2748625 -1.3283043 -1.4036291 -1.4605082 -1.4978068 -1.5737486 -1.6380492 -1.7504811 -1.8640072 -1.8653458 -1.7625275][-0.45967549 -0.67816591 -0.9211148 -1.1463864 -1.3239545 -1.4567494 -1.5733722 -1.628268 -1.6746228 -1.7319319 -1.8153331 -1.9741967 -2.0103807 -1.9915248 -1.8294233][-0.32099444 -0.48332036 -0.66371143 -0.879688 -1.083535 -1.2460084 -1.3521934 -1.4366856 -1.4974256 -1.5439408 -1.5788552 -1.6199731 -1.6220326 -1.5001096 -1.2997675][-0.21346512 -0.30671647 -0.40800184 -0.5616647 -0.72821784 -0.851714 -0.94237328 -1.0031993 -1.0445552 -1.0557575 -1.070291 -1.0807939 -1.0516436 -0.91295838 -0.80254591]]...]
INFO - root - 2017-12-06 05:57:54.833824: step 22410, loss = 0.75, batch loss = 0.54 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:47s remains)
INFO - root - 2017-12-06 05:57:57.129299: step 22420, loss = 0.78, batch loss = 0.56 (34.0 examples/sec; 0.235 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-06 05:57:59.454872: step 22430, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 20h:00m:32s remains)
INFO - root - 2017-12-06 05:58:01.794700: step 22440, loss = 0.79, batch loss = 0.58 (34.0 examples/sec; 0.235 sec/batch; 20h:15m:10s remains)
INFO - root - 2017-12-06 05:58:04.079465: step 22450, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:32s remains)
INFO - root - 2017-12-06 05:58:06.392644: step 22460, loss = 0.84, batch loss = 0.63 (34.3 examples/sec; 0.233 sec/batch; 20h:04m:48s remains)
INFO - root - 2017-12-06 05:58:08.698422: step 22470, loss = 0.80, batch loss = 0.58 (34.6 examples/sec; 0.231 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-06 05:58:10.985582: step 22480, loss = 0.80, batch loss = 0.59 (36.0 examples/sec; 0.223 sec/batch; 19h:09m:46s remains)
INFO - root - 2017-12-06 05:58:13.278633: step 22490, loss = 0.82, batch loss = 0.61 (35.3 examples/sec; 0.226 sec/batch; 19h:29m:36s remains)
INFO - root - 2017-12-06 05:58:15.564405: step 22500, loss = 0.83, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 19h:14m:23s remains)
2017-12-06 05:58:15.954412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069301456 -0.068419158 -0.067633227 -0.067974418 -0.069696784 -0.07062497 -0.071510181 -0.073115408 -0.085634574 -0.12396243 -0.18533441 -0.23874603 -0.26970297 -0.29056472 -0.30607247][-0.069567859 -0.068477161 -0.067312837 -0.066699058 -0.067952149 -0.069889873 -0.071213245 -0.073340535 -0.080633275 -0.11167811 -0.18033566 -0.26810539 -0.33718938 -0.37203634 -0.38991749][-0.069956243 -0.069212787 -0.067975134 -0.066682346 -0.066853583 -0.0681162 -0.07003393 -0.0715876 -0.077597179 -0.099114187 -0.15428844 -0.24000658 -0.314913 -0.3579886 -0.35480165][-0.070193857 -0.070201233 -0.069676116 -0.068404287 -0.067504436 -0.067850709 -0.069227822 -0.071718007 -0.076078176 -0.09157075 -0.13126849 -0.19906248 -0.26651478 -0.28331077 -0.24396656][-0.070174575 -0.070456073 -0.07078521 -0.070504561 -0.071000338 -0.072328925 -0.074161112 -0.07368958 -0.074685827 -0.083699621 -0.099603944 -0.11368141 -0.11805733 -0.10086798 -0.07520476][-0.070368029 -0.070788041 -0.071584731 -0.07206957 -0.072730504 -0.073466673 -0.074313112 -0.073264852 -0.072143823 -0.077002823 -0.079517625 -0.068474762 -0.03447184 0.011339366 0.013398781][-0.072346926 -0.075847864 -0.079066724 -0.077207744 -0.073219739 -0.073533736 -0.074185073 -0.074786417 -0.077338 -0.079029113 -0.060097486 -0.017414063 0.033659555 0.055121407 0.044294089][-0.071369193 -0.073644385 -0.078249142 -0.081596658 -0.082516834 -0.080023222 -0.075050771 -0.08142259 -0.10578348 -0.14570102 -0.1575681 -0.11213998 -0.023383521 0.028539307 0.02190613][-0.070988372 -0.072299145 -0.08011502 -0.10055651 -0.13490826 -0.15781078 -0.14428796 -0.1188079 -0.13647008 -0.20888373 -0.25435635 -0.22312473 -0.13579687 -0.075498469 -0.077766553][-0.069150686 -0.070889413 -0.077060066 -0.090371832 -0.11554234 -0.14573546 -0.16858248 -0.17747942 -0.18153614 -0.21079111 -0.24584553 -0.20184708 -0.091499485 -0.031822536 -0.044748779][-0.068317935 -0.068764091 -0.070627779 -0.074451968 -0.090544119 -0.13168801 -0.19608532 -0.24482881 -0.29499608 -0.33179468 -0.31494346 -0.20180026 -0.031909037 0.097630337 0.18638551][-0.072412156 -0.059402987 -0.029957902 0.011386842 0.04602474 0.053427324 0.020690799 -0.015136771 -0.062365212 -0.10253453 -0.13242072 -0.055210683 0.065830752 0.17614982 0.28446159][-0.08003258 -0.070159256 -0.028296083 0.046232834 0.12383869 0.17506298 0.18884438 0.18336397 0.15795219 0.12845284 0.0809533 0.10507329 0.14564991 0.2007153 0.23627883][-0.08526621 -0.0817968 -0.05119339 0.011219531 0.079971984 0.131263 0.16549051 0.1833145 0.17306972 0.13827157 0.093001127 0.10028827 0.14401872 0.22549716 0.26618209][-0.095493607 -0.097898245 -0.074034318 -0.024125397 0.041882545 0.099745288 0.13378341 0.1242585 0.12496212 0.13067037 0.13160171 0.10912979 0.086046115 0.13530172 0.23037717]]...]
INFO - root - 2017-12-06 05:58:18.283258: step 22510, loss = 0.74, batch loss = 0.53 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:48s remains)
INFO - root - 2017-12-06 05:58:20.567188: step 22520, loss = 0.84, batch loss = 0.62 (35.8 examples/sec; 0.223 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-06 05:58:22.898587: step 22530, loss = 0.78, batch loss = 0.57 (34.3 examples/sec; 0.233 sec/batch; 20h:05m:43s remains)
INFO - root - 2017-12-06 05:58:25.217915: step 22540, loss = 0.93, batch loss = 0.71 (35.0 examples/sec; 0.229 sec/batch; 19h:42m:05s remains)
INFO - root - 2017-12-06 05:58:27.503123: step 22550, loss = 0.80, batch loss = 0.58 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:21s remains)
INFO - root - 2017-12-06 05:58:29.793619: step 22560, loss = 0.80, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:22s remains)
INFO - root - 2017-12-06 05:58:32.071369: step 22570, loss = 0.85, batch loss = 0.64 (35.4 examples/sec; 0.226 sec/batch; 19h:28m:00s remains)
INFO - root - 2017-12-06 05:58:34.328614: step 22580, loss = 0.81, batch loss = 0.60 (33.7 examples/sec; 0.238 sec/batch; 20h:26m:56s remains)
INFO - root - 2017-12-06 05:58:36.638457: step 22590, loss = 0.90, batch loss = 0.68 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:48s remains)
INFO - root - 2017-12-06 05:58:38.923231: step 22600, loss = 0.74, batch loss = 0.53 (34.6 examples/sec; 0.231 sec/batch; 19h:52m:31s remains)
2017-12-06 05:58:39.377081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13659242 -0.12242175 -0.13140094 -0.15907739 -0.20325679 -0.27345163 -0.34526646 -0.3795377 -0.40466657 -0.39984253 -0.3768084 -0.33267543 -0.29170561 -0.25082123 -0.2188202][-0.15466118 -0.15004592 -0.15858102 -0.2010588 -0.26426715 -0.36197382 -0.458374 -0.49963337 -0.52706945 -0.54587871 -0.552924 -0.4951795 -0.40908161 -0.31794772 -0.24353552][-0.13209419 -0.12173356 -0.12080588 -0.13429461 -0.20882332 -0.3507745 -0.47584945 -0.56490386 -0.60316539 -0.66053963 -0.72233295 -0.66414392 -0.542384 -0.40912968 -0.29239237][-0.10869506 -0.13639101 -0.12408753 -0.087367132 -0.10131052 -0.18494624 -0.31230122 -0.43958604 -0.50640804 -0.60426509 -0.73556769 -0.790969 -0.71733797 -0.58462447 -0.42378807][0.0073635057 -0.014062144 -0.0023501292 0.072411686 0.14856218 0.17915022 0.10072173 -0.12277171 -0.27087551 -0.39577171 -0.5587548 -0.75352359 -0.82851791 -0.76565945 -0.5954802][0.041547269 -0.0070866793 -0.019747213 0.061179936 0.20812836 0.38402525 0.44303051 0.24618575 -0.0050991997 -0.19559988 -0.38099545 -0.604326 -0.76035345 -0.82800269 -0.7198137][0.04586634 -0.01060652 -0.078375712 -0.050995525 0.1191255 0.37095448 0.52559423 0.395043 0.075857028 -0.17605975 -0.37730587 -0.56676751 -0.70632827 -0.8241334 -0.78721035][0.060903206 0.0099089965 -0.078669295 -0.12457477 -0.035072558 0.24436021 0.49943718 0.48714581 0.21689859 -0.081956029 -0.32209119 -0.48682734 -0.597455 -0.68766332 -0.69266319][-0.016830318 -0.076424792 -0.17106687 -0.20256919 -0.13030204 0.10838416 0.43935803 0.52539575 0.32047236 0.011190213 -0.24690156 -0.41650337 -0.50185597 -0.56824052 -0.5782032][0.030650817 -0.021611236 -0.076693013 -0.066691369 -0.011714883 0.13436519 0.43319735 0.59127855 0.48542908 0.19914794 -0.07385467 -0.28670451 -0.38832667 -0.41807997 -0.42348263][0.041471571 0.0054032505 -0.051928807 -0.041912477 -0.020910583 0.068088412 0.25023907 0.41391024 0.41870257 0.2586996 0.042070627 -0.13311747 -0.24574378 -0.27519056 -0.2789458][0.062178373 0.026859313 -0.011159204 -0.015007339 0.0093488917 0.0629556 0.1881395 0.33418867 0.39677328 0.32652912 0.16684228 0.010007858 -0.11463125 -0.17789409 -0.2058516][0.065269724 0.054588884 0.028115258 0.033735022 0.041568816 0.056137428 0.11511101 0.20595777 0.26391035 0.238193 0.16668251 0.079470456 -0.0105092 -0.079556912 -0.12306365][0.11192316 0.12617245 0.12952462 0.13413201 0.15047123 0.17370945 0.21562701 0.28261188 0.34876454 0.33742678 0.28371078 0.20281005 0.1087898 0.017247193 -0.049133342][0.078186378 0.11869276 0.12999921 0.14642377 0.16131493 0.17735747 0.19014972 0.2094073 0.24907318 0.25254709 0.21280295 0.14195149 0.058081925 -0.020303153 -0.075099096]]...]
INFO - root - 2017-12-06 05:58:41.696421: step 22610, loss = 0.73, batch loss = 0.52 (31.6 examples/sec; 0.253 sec/batch; 21h:48m:03s remains)
INFO - root - 2017-12-06 05:58:43.987795: step 22620, loss = 0.85, batch loss = 0.64 (35.8 examples/sec; 0.224 sec/batch; 19h:15m:01s remains)
INFO - root - 2017-12-06 05:58:46.261577: step 22630, loss = 0.83, batch loss = 0.62 (36.1 examples/sec; 0.221 sec/batch; 19h:03m:02s remains)
INFO - root - 2017-12-06 05:58:48.540714: step 22640, loss = 0.83, batch loss = 0.61 (36.7 examples/sec; 0.218 sec/batch; 18h:44m:14s remains)
INFO - root - 2017-12-06 05:58:50.874326: step 22650, loss = 0.80, batch loss = 0.59 (31.7 examples/sec; 0.252 sec/batch; 21h:42m:08s remains)
INFO - root - 2017-12-06 05:58:53.178198: step 22660, loss = 0.72, batch loss = 0.51 (35.9 examples/sec; 0.223 sec/batch; 19h:10m:23s remains)
INFO - root - 2017-12-06 05:58:55.528989: step 22670, loss = 0.75, batch loss = 0.53 (34.6 examples/sec; 0.231 sec/batch; 19h:54m:46s remains)
INFO - root - 2017-12-06 05:58:57.823656: step 22680, loss = 0.89, batch loss = 0.67 (34.7 examples/sec; 0.231 sec/batch; 19h:51m:53s remains)
INFO - root - 2017-12-06 05:59:00.107020: step 22690, loss = 0.91, batch loss = 0.70 (35.2 examples/sec; 0.227 sec/batch; 19h:33m:45s remains)
INFO - root - 2017-12-06 05:59:02.421912: step 22700, loss = 0.77, batch loss = 0.56 (35.5 examples/sec; 0.225 sec/batch; 19h:23m:13s remains)
2017-12-06 05:59:02.818191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082079031 -0.082216434 -0.082137942 -0.081937723 -0.081648432 -0.081255727 -0.080758922 -0.080276586 -0.079928674 -0.079753183 -0.079664394 -0.079599969 -0.079563506 -0.079548053 -0.079680905][-0.082971834 -0.083014935 -0.082792543 -0.082431182 -0.081990071 -0.081492081 -0.080924273 -0.080347687 -0.079891413 -0.079639249 -0.079532459 -0.079480991 -0.07945855 -0.07946258 -0.079578966][-0.083558962 -0.083352372 -0.082895912 -0.082362525 -0.081865929 -0.081430383 -0.080944374 -0.080385193 -0.079901509 -0.079611093 -0.07952933 -0.079509176 -0.079509668 -0.079516008 -0.07958509][-0.083740562 -0.083225057 -0.082516506 -0.081883721 -0.081444532 -0.081196174 -0.080869645 -0.080346905 -0.079870127 -0.079604462 -0.079605378 -0.079674251 -0.079687811 -0.079639651 -0.079654418][-0.083507121 -0.082758315 -0.081961267 -0.081405215 -0.081126735 -0.081051946 -0.080829576 -0.0803217 -0.079860136 -0.079625204 -0.079656459 -0.079779722 -0.079805307 -0.079717 -0.079703689][-0.0829013 -0.082030945 -0.08132001 -0.081037924 -0.080957614 -0.080897562 -0.080666445 -0.08022023 -0.079764992 -0.079549491 -0.07959231 -0.079728425 -0.0797726 -0.079700142 -0.079727516][-0.082060605 -0.08124882 -0.080797881 -0.080624729 -0.080555886 -0.0804593 -0.080296077 -0.07998 -0.079613239 -0.0794382 -0.079520941 -0.079676963 -0.079707757 -0.07965368 -0.079767354][-0.081256948 -0.080592923 -0.080259837 -0.080086552 -0.080010176 -0.079970442 -0.079881221 -0.07970129 -0.079483524 -0.079417482 -0.07952942 -0.079667807 -0.079699554 -0.079660669 -0.07981386][-0.080630615 -0.08001072 -0.079772726 -0.079613537 -0.079534873 -0.079518653 -0.079522848 -0.079501092 -0.07945323 -0.079497129 -0.079600096 -0.079699069 -0.079707749 -0.079692587 -0.079866111][-0.080176562 -0.079658709 -0.079500034 -0.079399362 -0.079331227 -0.079328842 -0.079391919 -0.079455152 -0.079518721 -0.079623558 -0.07969553 -0.07974878 -0.07972794 -0.079703853 -0.07986594][-0.07992176 -0.079418138 -0.079292536 -0.079204604 -0.079147011 -0.079143986 -0.0792336 -0.0793653 -0.079483226 -0.079617269 -0.079706073 -0.079730183 -0.079707824 -0.079696566 -0.079838336][-0.079753689 -0.079229206 -0.079071224 -0.078965381 -0.0788966 -0.078921758 -0.079032637 -0.079192236 -0.079337522 -0.079508394 -0.07962662 -0.07967636 -0.079670124 -0.07967063 -0.079755373][-0.079789549 -0.079319373 -0.079188906 -0.079084694 -0.079009056 -0.079029977 -0.079092994 -0.079182953 -0.079299554 -0.079441778 -0.079559445 -0.079612665 -0.0796259 -0.079636753 -0.079692893][-0.079929672 -0.079514809 -0.079426922 -0.079345666 -0.079283156 -0.079265743 -0.079282261 -0.07933151 -0.0794044 -0.079493061 -0.079570763 -0.079618208 -0.079628184 -0.079631358 -0.079674378][-0.079987951 -0.079639904 -0.079616137 -0.079599321 -0.079586655 -0.079593159 -0.0796236 -0.079646483 -0.079680353 -0.079703242 -0.0797132 -0.079704694 -0.07968156 -0.079655938 -0.079690158]]...]
INFO - root - 2017-12-06 05:59:05.120914: step 22710, loss = 0.81, batch loss = 0.60 (35.3 examples/sec; 0.226 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-06 05:59:07.367710: step 22720, loss = 0.69, batch loss = 0.48 (36.2 examples/sec; 0.221 sec/batch; 19h:02m:16s remains)
INFO - root - 2017-12-06 05:59:09.690302: step 22730, loss = 0.78, batch loss = 0.57 (34.5 examples/sec; 0.232 sec/batch; 19h:58m:37s remains)
INFO - root - 2017-12-06 05:59:11.978855: step 22740, loss = 0.85, batch loss = 0.63 (34.1 examples/sec; 0.234 sec/batch; 20h:09m:57s remains)
INFO - root - 2017-12-06 05:59:14.303816: step 22750, loss = 0.86, batch loss = 0.65 (34.4 examples/sec; 0.232 sec/batch; 19h:59m:50s remains)
INFO - root - 2017-12-06 05:59:16.607459: step 22760, loss = 0.86, batch loss = 0.65 (35.6 examples/sec; 0.225 sec/batch; 19h:21m:02s remains)
INFO - root - 2017-12-06 05:59:18.866128: step 22770, loss = 0.83, batch loss = 0.61 (35.2 examples/sec; 0.227 sec/batch; 19h:33m:53s remains)
INFO - root - 2017-12-06 05:59:21.140985: step 22780, loss = 0.83, batch loss = 0.62 (35.3 examples/sec; 0.227 sec/batch; 19h:31m:29s remains)
INFO - root - 2017-12-06 05:59:23.441337: step 22790, loss = 0.78, batch loss = 0.56 (35.6 examples/sec; 0.225 sec/batch; 19h:19m:01s remains)
INFO - root - 2017-12-06 05:59:25.716556: step 22800, loss = 0.79, batch loss = 0.58 (34.8 examples/sec; 0.230 sec/batch; 19h:46m:10s remains)
2017-12-06 05:59:26.136873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016169399 -0.018196717 -0.019872241 -0.020585589 -0.020913385 -0.021940425 -0.023897693 -0.026556447 -0.029210266 -0.031548906 -0.033466622 -0.034869112 -0.035753287 -0.036368743 -0.037127][-0.00040163845 -0.0027583316 -0.0046496093 -0.0051266775 -0.0052127242 -0.0069156811 -0.010611832 -0.0155719 -0.02077996 -0.025498502 -0.029419266 -0.0323508 -0.034211043 -0.035421982 -0.036437906][0.018026941 0.015945263 0.014508538 0.014629133 0.014801808 0.012722343 0.0076133385 0.00044891238 -0.0079044476 -0.016200513 -0.02337613 -0.028790779 -0.032281242 -0.0345008 -0.035989303][0.036276169 0.03498549 0.034321003 0.035306729 0.036359154 0.0348927 0.029536851 0.020541415 0.0086278245 -0.00425186 -0.01570335 -0.024326392 -0.029964607 -0.033414956 -0.035562176][0.051407926 0.050910331 0.051157303 0.053239875 0.056163035 0.056726895 0.052287795 0.04208722 0.026489362 0.0088225082 -0.0071885958 -0.019392364 -0.027385153 -0.032189339 -0.035134446][0.059994645 0.060807593 0.062095709 0.065577917 0.071051352 0.074602567 0.072322242 0.062294222 0.044161476 0.022353835 0.001867421 -0.014221042 -0.024829738 -0.031031322 -0.034852087][0.060528822 0.062794648 0.065045469 0.069938831 0.077610634 0.08437907 0.085337959 0.077074774 0.058528356 0.034363605 0.010189183 -0.0095097721 -0.022753231 -0.030258015 -0.034760367][0.053650759 0.056720488 0.059364133 0.064647071 0.07321351 0.082187407 0.086237632 0.081084289 0.064665221 0.040793248 0.015355319 -0.006238237 -0.021226212 -0.02965581 -0.034634192][0.041330703 0.044221036 0.046309151 0.050965093 0.059257515 0.068721093 0.074330516 0.071882017 0.058930211 0.037933923 0.014322907 -0.0063700229 -0.021034077 -0.029516682 -0.034732446][0.026107334 0.028480612 0.029416919 0.03242787 0.039193086 0.047343038 0.052771248 0.052380763 0.04359635 0.027278155 0.0080400407 -0.0094101131 -0.022208549 -0.030028507 -0.035202663][0.0097779557 0.011956178 0.012297228 0.013835713 0.018270321 0.024057657 0.028385319 0.028712086 0.023093686 0.012123093 -0.0015632287 -0.014527418 -0.024519242 -0.031189058 -0.036023904][-0.0056548491 -0.0038938522 -0.0038894936 -0.0033878386 -0.0011542067 0.0021110997 0.004878372 0.0053040534 0.0021141618 -0.004327856 -0.012634568 -0.020822592 -0.027756616 -0.032836225 -0.036854707][-0.01845517 -0.017209038 -0.017434217 -0.017698191 -0.017221391 -0.016208559 -0.015090935 -0.01477018 -0.016105473 -0.01945056 -0.023597345 -0.027536586 -0.031280503 -0.034566123 -0.037573677][-0.027532242 -0.026532628 -0.026769929 -0.027205549 -0.027628779 -0.027956769 -0.028106082 -0.02825968 -0.028644715 -0.02993615 -0.031433631 -0.032669291 -0.034211714 -0.036043335 -0.038229972][-0.034004893 -0.033236656 -0.033431005 -0.033732083 -0.034112439 -0.034591019 -0.035067122 -0.035463482 -0.035703328 -0.035972591 -0.036113936 -0.036123496 -0.036503032 -0.037322182 -0.038693994]]...]
INFO - root - 2017-12-06 05:59:28.442477: step 22810, loss = 0.77, batch loss = 0.56 (33.5 examples/sec; 0.239 sec/batch; 20h:32m:29s remains)
INFO - root - 2017-12-06 05:59:30.714974: step 22820, loss = 0.86, batch loss = 0.65 (35.2 examples/sec; 0.228 sec/batch; 19h:34m:31s remains)
INFO - root - 2017-12-06 05:59:33.023647: step 22830, loss = 0.79, batch loss = 0.58 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:21s remains)
INFO - root - 2017-12-06 05:59:35.355394: step 22840, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 19h:35m:59s remains)
INFO - root - 2017-12-06 05:59:37.679677: step 22850, loss = 0.91, batch loss = 0.70 (33.4 examples/sec; 0.239 sec/batch; 20h:35m:16s remains)
INFO - root - 2017-12-06 05:59:39.998872: step 22860, loss = 0.82, batch loss = 0.61 (34.4 examples/sec; 0.233 sec/batch; 20h:01m:18s remains)
INFO - root - 2017-12-06 05:59:42.260930: step 22870, loss = 0.81, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-06 05:59:44.575949: step 22880, loss = 0.79, batch loss = 0.58 (33.6 examples/sec; 0.238 sec/batch; 20h:27m:54s remains)
INFO - root - 2017-12-06 05:59:46.928579: step 22890, loss = 0.81, batch loss = 0.60 (33.3 examples/sec; 0.240 sec/batch; 20h:39m:55s remains)
INFO - root - 2017-12-06 05:59:49.216389: step 22900, loss = 0.85, batch loss = 0.63 (35.3 examples/sec; 0.227 sec/batch; 19h:29m:37s remains)
2017-12-06 05:59:49.656426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22829768 -0.27194235 -0.32646033 -0.3542648 -0.34713337 -0.30746502 -0.24111715 -0.16989152 -0.099615827 -0.047794521 -0.015311703 -0.029640492 -0.065055259 -0.10672797 -0.14270826][-0.68049955 -0.74829686 -0.8228662 -0.87075484 -0.87655807 -0.8486135 -0.78412938 -0.67794549 -0.54042959 -0.40753862 -0.28742108 -0.21296522 -0.15742096 -0.14985459 -0.1555161][-1.1088748 -1.180956 -1.2326891 -1.2771754 -1.2981015 -1.2962146 -1.2525488 -1.1637458 -1.0318516 -0.88303828 -0.71738458 -0.56765723 -0.42058128 -0.31079435 -0.22203803][-1.1930317 -1.2421918 -1.2804806 -1.320471 -1.3713095 -1.4164163 -1.4182754 -1.3753799 -1.2890085 -1.1851698 -1.0584332 -0.90782416 -0.72978079 -0.52817291 -0.35097879][-0.70712757 -0.61401653 -0.53156823 -0.53525257 -0.627326 -0.69362128 -0.76076269 -0.80667484 -0.84616911 -0.88456917 -0.91083384 -0.88116992 -0.77732706 -0.59833634 -0.41174838][-0.087818921 0.15522321 0.46314904 0.66258204 0.70709968 0.67467678 0.5335716 0.40390763 0.21725684 0.011147976 -0.2266731 -0.4191314 -0.49904963 -0.43897641 -0.33004138][0.11444414 0.42195442 0.87217128 1.2962002 1.6033982 1.7678756 1.7440399 1.6564775 1.3989291 1.0487355 0.6050272 0.19719896 -0.10367327 -0.24652988 -0.29096082][-0.095819689 0.091257691 0.40797389 0.85187614 1.2893517 1.6257392 1.8189868 1.8591206 1.6711484 1.3475322 0.87507093 0.3993859 0.010792434 -0.22111206 -0.33031183][-0.4606235 -0.37770692 -0.21606708 0.050488353 0.35852668 0.71964729 1.0932024 1.3199476 1.347006 1.1380222 0.75655222 0.35077277 -0.01882384 -0.24515031 -0.34005767][-0.7055409 -0.77475083 -0.81755471 -0.72288167 -0.57302463 -0.30710584 0.061069265 0.39099512 0.61146152 0.56051075 0.37772396 0.15853065 -0.05250841 -0.16702041 -0.17015821][-0.54653573 -0.72636843 -0.87015581 -0.99845517 -1.0279796 -0.855953 -0.54210436 -0.19363418 0.10481147 0.18688837 0.17525598 0.098821968 0.0287299 0.018518373 0.09630172][-0.23834418 -0.32659221 -0.38181338 -0.51827836 -0.61298811 -0.63272452 -0.51384693 -0.23971841 0.036160856 0.15391095 0.23540333 0.23301965 0.22397998 0.28446344 0.4048115][-0.22287486 -0.30802184 -0.30394411 -0.28178096 -0.28794271 -0.32437283 -0.26119828 -0.12315606 0.015931688 0.10563919 0.18702543 0.19485152 0.19239068 0.30129364 0.43512383][-0.20083976 -0.33685076 -0.42480257 -0.440663 -0.448956 -0.45965275 -0.39073634 -0.29588473 -0.2040614 -0.18366721 -0.070772536 -0.036123604 -0.0059005544 0.10244374 0.21497008][-0.23993921 -0.31684345 -0.39621902 -0.45981678 -0.48785895 -0.5529449 -0.55130553 -0.47541931 -0.41619536 -0.33772638 -0.2555159 -0.22774497 -0.17982632 -0.10102769 -0.049994465]]...]
INFO - root - 2017-12-06 05:59:51.956210: step 22910, loss = 0.76, batch loss = 0.55 (32.7 examples/sec; 0.245 sec/batch; 21h:03m:38s remains)
INFO - root - 2017-12-06 05:59:54.250273: step 22920, loss = 0.68, batch loss = 0.46 (33.3 examples/sec; 0.240 sec/batch; 20h:38m:14s remains)
INFO - root - 2017-12-06 05:59:56.535555: step 22930, loss = 0.73, batch loss = 0.52 (34.2 examples/sec; 0.234 sec/batch; 20h:06m:26s remains)
INFO - root - 2017-12-06 05:59:58.818202: step 22940, loss = 0.79, batch loss = 0.58 (36.3 examples/sec; 0.220 sec/batch; 18h:56m:10s remains)
INFO - root - 2017-12-06 06:00:01.131951: step 22950, loss = 0.82, batch loss = 0.61 (33.6 examples/sec; 0.238 sec/batch; 20h:27m:47s remains)
INFO - root - 2017-12-06 06:00:03.449393: step 22960, loss = 0.82, batch loss = 0.60 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-06 06:00:05.730369: step 22970, loss = 0.80, batch loss = 0.58 (34.7 examples/sec; 0.231 sec/batch; 19h:50m:42s remains)
INFO - root - 2017-12-06 06:00:08.048917: step 22980, loss = 0.75, batch loss = 0.53 (34.0 examples/sec; 0.235 sec/batch; 20h:13m:00s remains)
INFO - root - 2017-12-06 06:00:10.367691: step 22990, loss = 0.80, batch loss = 0.58 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:46s remains)
INFO - root - 2017-12-06 06:00:12.694932: step 23000, loss = 0.83, batch loss = 0.62 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:55s remains)
2017-12-06 06:00:13.127555: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.4783196 1.5643396 1.3274051 1.034632 0.89620215 0.62876159 0.25826329 -0.086240426 -0.10721649 0.15831198 0.45356977 0.64679337 0.67773873 0.55419272 0.35532379][1.8349752 2.0028861 1.9680586 1.5762012 1.0773551 0.44862145 0.018969804 -0.11902577 -0.21829461 -0.18713762 0.084933981 0.39814043 0.54860282 0.51063639 0.24180816][1.3693428 1.2205445 1.0555234 0.81420094 0.60974044 0.20713146 -0.18534392 -0.47152019 -0.46635556 -0.362755 -0.18188059 -0.13124442 0.018521398 0.10402818 0.081654817][0.592005 0.57246876 0.49367559 0.21583672 -0.17111276 -0.55425572 -0.84491247 -0.98821944 -1.1183436 -1.0585351 -0.88116491 -0.69590497 -0.56084937 -0.5419023 -0.61377019][-0.19009691 -0.39493805 -0.58249849 -0.71238887 -0.84130061 -1.0444202 -1.1201087 -1.0090555 -0.97639489 -0.95630842 -0.8767879 -0.88562608 -0.90507162 -0.95672196 -0.995339][-0.79572338 -0.756712 -0.68621022 -0.73837185 -0.73492748 -0.68224591 -0.77821052 -0.76279175 -0.560207 -0.31400004 -0.19469385 -0.31437379 -0.53435725 -0.7427265 -0.95543706][-1.1953963 -1.0755998 -0.46723074 -0.015825033 0.26266402 0.51442003 0.61645943 0.46761996 0.29622662 0.19450714 0.11712877 0.0482679 -0.12540856 -0.37480539 -0.63638991][-0.74598676 -0.4808476 0.063718915 0.74669117 1.0978477 1.2098967 1.2267972 1.0521489 0.84862375 0.55047911 0.28958464 0.003817983 -0.33578533 -0.68047595 -0.86923951][-0.61050975 -0.37890637 0.19604032 1.0863206 1.7969829 1.9517448 1.6360226 1.0887823 0.49874133 -0.0056685135 -0.22565635 -0.31128079 -0.43842059 -0.60348576 -0.70526022][-0.43992954 -0.19208732 0.23081182 0.97059828 1.5450827 1.7377745 1.6264708 1.112344 0.57141906 0.01495745 -0.37583506 -0.552918 -0.50496972 -0.41504955 -0.3958236][-0.63388026 -0.32147253 0.091142684 0.48636603 0.86865389 0.99264389 0.72456455 0.25563622 -0.17678294 -0.39450437 -0.53581214 -0.56128913 -0.4973526 -0.39039654 -0.32463294][-0.82469779 -0.7778762 -0.57349408 -0.46216977 -0.3592428 -0.25431988 -0.21466561 -0.28626814 -0.53162408 -0.72887743 -0.78547066 -0.68199027 -0.54820931 -0.39807916 -0.36144102][-0.39915746 -0.5581094 -0.659508 -0.72070664 -0.78117776 -0.87824857 -0.92412454 -0.82738471 -0.72058237 -0.59962261 -0.47493309 -0.42785215 -0.409707 -0.29079285 -0.2306623][0.24574105 0.16725655 -0.022759095 -0.35900319 -0.590417 -0.62550211 -0.66704196 -0.66851622 -0.65291417 -0.57814723 -0.45739239 -0.3446064 -0.25003427 -0.24571185 -0.25484642][0.35761386 0.32063341 0.29683048 0.14356977 -0.12678048 -0.32537442 -0.35045487 -0.28768721 -0.26004714 -0.28922993 -0.26497179 -0.14216694 -0.01912944 0.055663168 0.12031005]]...]
INFO - root - 2017-12-06 06:00:15.440139: step 23010, loss = 0.82, batch loss = 0.61 (36.2 examples/sec; 0.221 sec/batch; 19h:01m:10s remains)
INFO - root - 2017-12-06 06:00:17.730894: step 23020, loss = 0.81, batch loss = 0.60 (34.1 examples/sec; 0.235 sec/batch; 20h:09m:42s remains)
INFO - root - 2017-12-06 06:00:20.006902: step 23030, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-06 06:00:22.344419: step 23040, loss = 0.85, batch loss = 0.64 (34.5 examples/sec; 0.232 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-06 06:00:24.605969: step 23050, loss = 0.76, batch loss = 0.55 (34.0 examples/sec; 0.236 sec/batch; 20h:14m:58s remains)
INFO - root - 2017-12-06 06:00:26.893713: step 23060, loss = 0.87, batch loss = 0.66 (35.3 examples/sec; 0.227 sec/batch; 19h:30m:15s remains)
INFO - root - 2017-12-06 06:00:29.284075: step 23070, loss = 0.89, batch loss = 0.67 (33.6 examples/sec; 0.238 sec/batch; 20h:27m:21s remains)
INFO - root - 2017-12-06 06:00:31.577792: step 23080, loss = 0.77, batch loss = 0.56 (35.9 examples/sec; 0.223 sec/batch; 19h:09m:34s remains)
INFO - root - 2017-12-06 06:00:33.885619: step 23090, loss = 0.75, batch loss = 0.53 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:57s remains)
INFO - root - 2017-12-06 06:00:36.216077: step 23100, loss = 0.80, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 19h:49m:41s remains)
2017-12-06 06:00:36.591958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5318015 -1.482206 -1.2759042 -1.1078525 -1.0544617 -1.2930422 -1.4720502 -1.5665085 -1.5234224 -1.4219544 -1.230163 -0.89848536 -0.49385917 -0.2159584 -0.18736576][-1.8528086 -1.958523 -1.8918521 -1.7636639 -1.6409924 -1.4163717 -1.3056586 -1.3311388 -1.3243974 -1.1808046 -0.90324551 -0.53688782 -0.12098074 0.30477184 0.49708617][-1.5040816 -1.5756196 -1.5648918 -1.6136161 -1.7337167 -1.7193102 -1.6508771 -1.4562643 -1.1441395 -1.0157965 -0.82008171 -0.49619275 -0.12710999 0.19179831 0.4337073][-0.33006266 -0.83066893 -1.0214405 -1.0334475 -1.1617143 -1.3231651 -1.2729106 -1.168649 -0.98192328 -0.89807135 -0.82853585 -0.68781203 -0.40004998 -0.011763483 0.22839378][-0.090804577 -0.32157838 -0.270271 -0.42956293 -0.63368565 -0.88134229 -1.0257435 -0.86011428 -0.71215695 -0.63823 -0.46379751 -0.39989305 -0.45403045 -0.2480337 0.0126708][0.75314063 0.45048028 0.33402765 0.31276858 0.26829427 0.27454954 0.28642249 0.24092741 0.1068169 -0.0058024079 -0.094159909 -0.25657362 -0.26938757 -0.16188648 -0.097621992][0.96672052 0.81676877 0.8101216 0.85494721 0.96806747 1.0746596 1.155636 0.97410756 0.94003993 0.58507705 0.30947012 0.07631968 -0.044589017 -0.077542551 0.01839646][0.88510841 0.59526026 0.59421444 0.81753033 1.1245934 1.3279464 1.3635005 1.2818933 1.1687727 0.77684861 0.4588322 0.046960488 -0.23893371 -0.43146276 -0.4237023][0.53698689 0.18063973 0.11724736 0.28351146 0.43751037 0.6997776 0.85422605 0.67237109 0.56881297 0.26147652 -0.0425773 -0.37227082 -0.57613826 -0.79975533 -0.8555212][-0.33261049 -0.5492515 -0.51164162 -0.35837233 -0.19792059 -0.094726942 0.045871824 0.061349124 0.020940237 -0.096526407 -0.3124564 -0.61189938 -0.85179639 -1.0148158 -1.0313648][-1.1651151 -1.2704864 -1.1227008 -0.94597393 -0.78791851 -0.71925807 -0.68522894 -0.67100447 -0.70867568 -0.74072862 -0.77207714 -0.79594296 -0.81677794 -0.95839059 -0.98655277][-1.3388556 -1.4988385 -1.6310048 -1.652231 -1.5796242 -1.5727338 -1.4666923 -1.352355 -1.2062787 -1.1233271 -1.1048211 -1.0359864 -0.98210239 -0.92202705 -0.87490833][-1.0247521 -1.2540224 -1.3916783 -1.6104362 -1.724533 -1.7691053 -1.7355543 -1.606267 -1.4572455 -1.3465421 -1.180667 -1.0563754 -0.96134883 -0.8558532 -0.77936542][-0.34540457 -0.72402024 -1.0313754 -1.3012207 -1.4275647 -1.4973651 -1.5172387 -1.4784695 -1.3210791 -1.1411617 -0.97844726 -0.81925118 -0.68834275 -0.56030422 -0.54764825][-0.017371207 -0.32034141 -0.64295685 -0.97529536 -1.1026683 -1.1392887 -1.1356146 -1.1079006 -1.025665 -0.9100163 -0.76883686 -0.62631691 -0.50386029 -0.42837048 -0.37531614]]...]
INFO - root - 2017-12-06 06:00:38.900183: step 23110, loss = 0.84, batch loss = 0.63 (35.6 examples/sec; 0.225 sec/batch; 19h:19m:49s remains)
INFO - root - 2017-12-06 06:00:41.166403: step 23120, loss = 0.87, batch loss = 0.66 (34.6 examples/sec; 0.231 sec/batch; 19h:52m:35s remains)
INFO - root - 2017-12-06 06:00:43.494570: step 23130, loss = 0.88, batch loss = 0.67 (34.8 examples/sec; 0.230 sec/batch; 19h:44m:46s remains)
INFO - root - 2017-12-06 06:00:45.787517: step 23140, loss = 0.82, batch loss = 0.60 (35.0 examples/sec; 0.229 sec/batch; 19h:38m:27s remains)
INFO - root - 2017-12-06 06:00:48.100133: step 23150, loss = 0.80, batch loss = 0.59 (35.0 examples/sec; 0.228 sec/batch; 19h:37m:11s remains)
INFO - root - 2017-12-06 06:00:50.374178: step 23160, loss = 0.80, batch loss = 0.59 (34.7 examples/sec; 0.230 sec/batch; 19h:48m:03s remains)
INFO - root - 2017-12-06 06:00:52.669405: step 23170, loss = 0.82, batch loss = 0.60 (35.4 examples/sec; 0.226 sec/batch; 19h:26m:37s remains)
INFO - root - 2017-12-06 06:00:54.960821: step 23180, loss = 0.94, batch loss = 0.72 (36.4 examples/sec; 0.220 sec/batch; 18h:52m:54s remains)
INFO - root - 2017-12-06 06:00:57.221228: step 23190, loss = 0.77, batch loss = 0.55 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:19s remains)
INFO - root - 2017-12-06 06:00:59.497701: step 23200, loss = 0.75, batch loss = 0.54 (34.2 examples/sec; 0.234 sec/batch; 20h:06m:26s remains)
2017-12-06 06:00:59.871875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11241375 -0.11111699 -0.10994095 -0.10924754 -0.10934921 -0.10761444 -0.10567217 -0.10701911 -0.11557844 -0.12395161 -0.12602627 -0.12507406 -0.12098815 -0.11590842 -0.11307895][-0.1130522 -0.11369494 -0.1126496 -0.10941833 -0.1066204 -0.10846064 -0.11806676 -0.13254352 -0.14646277 -0.15146548 -0.14786716 -0.13846317 -0.13059703 -0.12182842 -0.11489965][-0.11227383 -0.12215948 -0.13702545 -0.16193247 -0.19168633 -0.23313466 -0.27798328 -0.32281381 -0.35830107 -0.36606368 -0.33719984 -0.28869608 -0.23569757 -0.18143412 -0.13569982][-0.11589758 -0.17516923 -0.2519311 -0.3377063 -0.41907614 -0.47983226 -0.47520459 -0.46487725 -0.47615379 -0.48192748 -0.46288797 -0.41075072 -0.344265 -0.26666167 -0.16836827][-0.14012924 -0.25093332 -0.3849901 -0.51079649 -0.56520641 -0.55367488 -0.41642186 -0.26920506 -0.15911183 -0.16179687 -0.20275438 -0.26100931 -0.30641505 -0.30753314 -0.24671832][-0.18471529 -0.3206369 -0.46952239 -0.57673347 -0.54040468 -0.36369315 -0.066618815 0.17514244 0.34040543 0.38097981 0.29945141 0.084761031 -0.11660424 -0.24020648 -0.25101706][-0.16183877 -0.30974853 -0.44039237 -0.47579315 -0.32809857 -0.014418177 0.42301983 0.75710958 0.95267582 0.9315052 0.75268674 0.43935406 0.1280179 -0.12994495 -0.2140432][-0.17458752 -0.28120485 -0.36383316 -0.33597451 -0.12060295 0.23114994 0.6619215 1.0091666 1.2058219 1.1591185 0.89716363 0.48679864 0.13501075 -0.1105624 -0.18911004][-0.20464188 -0.31420943 -0.38624704 -0.34699538 -0.16686352 0.14061698 0.516102 0.86290753 1.0486088 0.98884213 0.763305 0.38183776 0.068536185 -0.095787972 -0.12410184][-0.23096555 -0.33843011 -0.39882451 -0.41763365 -0.31420219 -0.088054918 0.19268659 0.4545424 0.61785638 0.59091693 0.429152 0.13954648 -0.085164048 -0.18425435 -0.18061161][-0.13277555 -0.28703105 -0.3458454 -0.36906576 -0.29777908 -0.15645783 0.028724916 0.26386175 0.40542465 0.38191372 0.22699168 0.02122321 -0.098785393 -0.17624173 -0.17846879][0.32390961 0.10261991 -0.0008271113 -0.074708611 -0.061087027 0.026556648 0.15421376 0.30359259 0.38279143 0.34257594 0.19807625 0.021707863 -0.0856481 -0.10909294 -0.10485002][0.986578 0.69756418 0.53001982 0.41330302 0.34468517 0.30307037 0.31966397 0.38767925 0.39649749 0.32034153 0.173074 0.00375887 -0.083077684 -0.095148318 -0.099365555][1.5981935 1.3246632 1.076453 0.85648209 0.728639 0.641593 0.56178617 0.54075432 0.49777985 0.37278876 0.16895267 0.016230561 -0.1051664 -0.1324276 -0.10461123][1.9416894 1.6683278 1.4565673 1.1346514 0.86470884 0.71922457 0.61391485 0.568393 0.5029068 0.37091595 0.20783302 0.068957664 -0.08036676 -0.11411754 -0.09678676]]...]
INFO - root - 2017-12-06 06:01:02.148762: step 23210, loss = 0.84, batch loss = 0.63 (34.7 examples/sec; 0.230 sec/batch; 19h:47m:42s remains)
INFO - root - 2017-12-06 06:01:04.447367: step 23220, loss = 0.78, batch loss = 0.57 (34.0 examples/sec; 0.235 sec/batch; 20h:12m:16s remains)
INFO - root - 2017-12-06 06:01:06.761100: step 23230, loss = 0.80, batch loss = 0.58 (35.3 examples/sec; 0.227 sec/batch; 19h:27m:59s remains)
INFO - root - 2017-12-06 06:01:09.063577: step 23240, loss = 0.79, batch loss = 0.57 (34.3 examples/sec; 0.233 sec/batch; 20h:02m:37s remains)
INFO - root - 2017-12-06 06:01:11.379081: step 23250, loss = 0.95, batch loss = 0.74 (33.5 examples/sec; 0.238 sec/batch; 20h:29m:03s remains)
INFO - root - 2017-12-06 06:01:13.685234: step 23260, loss = 0.80, batch loss = 0.59 (36.2 examples/sec; 0.221 sec/batch; 18h:59m:51s remains)
INFO - root - 2017-12-06 06:01:15.949804: step 23270, loss = 0.74, batch loss = 0.53 (34.0 examples/sec; 0.235 sec/batch; 20h:11m:52s remains)
INFO - root - 2017-12-06 06:01:18.323001: step 23280, loss = 0.83, batch loss = 0.61 (33.4 examples/sec; 0.240 sec/batch; 20h:34m:41s remains)
INFO - root - 2017-12-06 06:01:20.623558: step 23290, loss = 0.83, batch loss = 0.61 (35.3 examples/sec; 0.227 sec/batch; 19h:29m:16s remains)
INFO - root - 2017-12-06 06:01:22.912800: step 23300, loss = 0.79, batch loss = 0.58 (34.5 examples/sec; 0.232 sec/batch; 19h:54m:05s remains)
2017-12-06 06:01:23.430875: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.99443656 0.55328804 0.12725739 -0.18675783 -0.31922829 -0.36029959 -0.460671 -0.44604743 -0.36860716 -0.23464836 -0.052014731 0.13833246 0.21209131 0.063546523 -0.16993669][0.73817271 0.44449103 -0.044151675 -0.41730767 -0.6325621 -0.78429246 -0.96621585 -0.991337 -0.95409709 -0.81417024 -0.64366621 -0.45888531 -0.42537463 -0.5048328 -0.6249404][0.25124449 0.020697892 -0.35975468 -0.75094754 -1.0804924 -1.2590997 -1.4395181 -1.5380287 -1.4857235 -1.3699641 -1.2288693 -1.0047597 -0.90252256 -0.8335281 -0.70752496][-0.061553024 -0.093423739 -0.27084905 -0.53213751 -0.827387 -1.052338 -1.2759676 -1.4412762 -1.3804051 -1.2517177 -1.1336951 -1.0089836 -0.909194 -0.73752868 -0.49284953][-0.0026340708 0.040642664 -0.0079576373 -0.052727219 -0.15902239 -0.31237945 -0.45742017 -0.58019352 -0.54492807 -0.4590748 -0.37594491 -0.29180461 -0.25815541 -0.17969272 0.0012307465][0.093852893 0.13610767 0.35931295 0.57006353 0.74183089 0.83206439 0.81450844 0.75989854 0.831131 0.80882239 0.85596824 0.8899619 0.84865147 0.82724494 0.85306764][0.1151773 0.20287473 0.56413954 0.90480524 1.1934296 1.4398309 1.5825989 1.6459205 1.6653819 1.5817039 1.5179291 1.4026996 1.2754692 1.1767044 1.0895299][-0.092417136 -0.015110552 0.33385611 0.68835473 0.9644559 1.1705159 1.2605392 1.314545 1.3732617 1.3115858 1.2605059 1.1343414 0.98623663 0.82444292 0.68048179][-0.70547909 -0.639258 -0.36890316 -0.088567466 0.097547531 0.11942635 0.053590745 0.055300727 0.044006884 0.022377148 0.066940054 0.089966983 0.089816049 0.052484393 -0.014708847][-1.4744794 -1.4406512 -1.2516915 -1.0976952 -1.0289381 -1.0630597 -1.156325 -1.3279861 -1.4868878 -1.5169811 -1.4465125 -1.2697418 -1.0619317 -0.86294174 -0.71276563][-1.8981171 -1.9046458 -1.8317318 -1.7716522 -1.7624644 -1.89145 -2.0911238 -2.3036625 -2.4549186 -2.5153842 -2.4635727 -2.2361386 -1.9230099 -1.5627832 -1.2485667][-1.8742471 -1.9300762 -1.9148923 -1.9227667 -1.961365 -2.0646715 -2.2085013 -2.3943353 -2.5863247 -2.7100906 -2.6928384 -2.4824929 -2.157315 -1.7675344 -1.386135][-1.4476255 -1.5322404 -1.5964969 -1.6411805 -1.7015504 -1.7606053 -1.842244 -1.9351151 -2.0381384 -2.1419966 -2.170542 -2.0783868 -1.8358428 -1.470879 -1.0800802][-0.94448996 -1.0306172 -1.1145041 -1.1797925 -1.2403604 -1.27724 -1.312097 -1.3179954 -1.3465536 -1.3765944 -1.3678439 -1.312133 -1.1906863 -1.0101162 -0.75344348][-0.56032467 -0.60958833 -0.6594277 -0.68269747 -0.71234745 -0.73040348 -0.75956488 -0.77069432 -0.78792834 -0.80449724 -0.79281032 -0.71812874 -0.61948639 -0.53292978 -0.42494]]...]
INFO - root - 2017-12-06 06:01:25.711348: step 23310, loss = 0.70, batch loss = 0.49 (35.6 examples/sec; 0.225 sec/batch; 19h:18m:49s remains)
INFO - root - 2017-12-06 06:01:28.004616: step 23320, loss = 0.82, batch loss = 0.60 (35.9 examples/sec; 0.223 sec/batch; 19h:07m:17s remains)
INFO - root - 2017-12-06 06:01:30.278949: step 23330, loss = 0.78, batch loss = 0.57 (35.2 examples/sec; 0.227 sec/batch; 19h:30m:25s remains)
INFO - root - 2017-12-06 06:01:32.564710: step 23340, loss = 0.76, batch loss = 0.54 (35.3 examples/sec; 0.227 sec/batch; 19h:28m:08s remains)
INFO - root - 2017-12-06 06:01:34.866201: step 23350, loss = 0.73, batch loss = 0.51 (34.8 examples/sec; 0.230 sec/batch; 19h:44m:45s remains)
INFO - root - 2017-12-06 06:01:37.165433: step 23360, loss = 0.84, batch loss = 0.63 (34.1 examples/sec; 0.235 sec/batch; 20h:08m:31s remains)
INFO - root - 2017-12-06 06:01:39.455350: step 23370, loss = 0.79, batch loss = 0.58 (35.6 examples/sec; 0.225 sec/batch; 19h:18m:45s remains)
INFO - root - 2017-12-06 06:01:41.751080: step 23380, loss = 0.86, batch loss = 0.64 (34.2 examples/sec; 0.234 sec/batch; 20h:06m:12s remains)
INFO - root - 2017-12-06 06:01:43.994655: step 23390, loss = 0.87, batch loss = 0.65 (36.0 examples/sec; 0.222 sec/batch; 19h:03m:34s remains)
INFO - root - 2017-12-06 06:01:46.264428: step 23400, loss = 0.78, batch loss = 0.57 (34.3 examples/sec; 0.233 sec/batch; 20h:01m:43s remains)
2017-12-06 06:01:46.616919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063513644 -0.067327388 -0.0601409 -0.037283424 -0.012325808 0.007583566 0.011027046 0.0085696056 0.0038134605 -0.0069080666 -0.023937017 -0.042070307 -0.057974461 -0.0671659 -0.069378048][-0.061694041 -0.066128761 -0.059429333 -0.033759255 0.00957796 0.059144154 0.08845897 0.10183388 0.097700357 0.073526904 0.03753604 0.0052177981 -0.021015152 -0.036341891 -0.040948939][-0.06208241 -0.070770107 -0.074953437 -0.066858739 -0.03781363 0.0093674585 0.048436955 0.074344352 0.079719469 0.05909209 0.021443039 -0.01535023 -0.044223148 -0.060661986 -0.0648551][-0.063478127 -0.076352477 -0.094847046 -0.12122709 -0.14412025 -0.14892849 -0.14620571 -0.13157374 -0.10601478 -0.086469389 -0.073887214 -0.065408722 -0.064050421 -0.066163957 -0.066925995][-0.065815791 -0.080800571 -0.098844863 -0.12780432 -0.16270095 -0.19131914 -0.21257596 -0.21283761 -0.18722869 -0.15167373 -0.11775452 -0.089823842 -0.075353384 -0.070817217 -0.069292083][-0.065834984 -0.075680904 -0.08719378 -0.10238237 -0.11490554 -0.12030759 -0.1259104 -0.12773368 -0.11906663 -0.10647306 -0.094128773 -0.080602288 -0.073687546 -0.071944304 -0.071585216][-0.066605635 -0.074127324 -0.085728876 -0.10278767 -0.11917543 -0.13083035 -0.13876531 -0.14338279 -0.1363809 -0.12266877 -0.10627849 -0.088259146 -0.076405942 -0.071103178 -0.069993921][-0.064752124 -0.067112014 -0.0720741 -0.078899428 -0.085870408 -0.089659967 -0.092570186 -0.094764113 -0.094858535 -0.092653126 -0.086101584 -0.076881923 -0.070475191 -0.0687892 -0.06979461][-0.065901175 -0.066300556 -0.068260141 -0.071901739 -0.076009765 -0.070598826 -0.065007 -0.064335383 -0.065633722 -0.0684985 -0.072375327 -0.073025778 -0.070420578 -0.069349781 -0.070207313][-0.067180574 -0.067022428 -0.067760319 -0.069029719 -0.068967037 -0.066888824 -0.062265456 -0.060888946 -0.061725974 -0.065214291 -0.071014225 -0.071624205 -0.070474908 -0.069993272 -0.0702211][-0.067986041 -0.067936264 -0.0680942 -0.06660343 -0.065689847 -0.064781561 -0.063747585 -0.065188669 -0.065089636 -0.067464024 -0.068109289 -0.068380043 -0.06855838 -0.069033094 -0.0702295][-0.068514243 -0.068424657 -0.068836629 -0.0691119 -0.066435695 -0.065143123 -0.065930486 -0.066846639 -0.067779109 -0.068987235 -0.068527229 -0.067973979 -0.067519568 -0.068080314 -0.069049075][-0.06897793 -0.068721011 -0.068976164 -0.069165863 -0.065632947 -0.0600091 -0.057082865 -0.057953343 -0.060930002 -0.065250188 -0.069122612 -0.069656156 -0.06894017 -0.068901911 -0.069517404][-0.069364488 -0.069033325 -0.069078259 -0.069071047 -0.06889832 -0.068481073 -0.067425258 -0.0633795 -0.061664991 -0.062258527 -0.064505681 -0.066489466 -0.06821347 -0.068932064 -0.069337085][-0.069780327 -0.069393277 -0.069407046 -0.069381744 -0.069359533 -0.069256142 -0.068994127 -0.068301007 -0.063948572 -0.061292388 -0.061444115 -0.063080594 -0.065305009 -0.067588553 -0.069116324]]...]
INFO - root - 2017-12-06 06:01:48.870554: step 23410, loss = 0.78, batch loss = 0.56 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:08s remains)
INFO - root - 2017-12-06 06:01:51.146887: step 23420, loss = 0.86, batch loss = 0.65 (35.0 examples/sec; 0.228 sec/batch; 19h:36m:29s remains)
INFO - root - 2017-12-06 06:01:53.455723: step 23430, loss = 0.76, batch loss = 0.54 (35.0 examples/sec; 0.228 sec/batch; 19h:36m:28s remains)
INFO - root - 2017-12-06 06:01:55.749223: step 23440, loss = 0.81, batch loss = 0.60 (35.8 examples/sec; 0.223 sec/batch; 19h:09m:32s remains)
INFO - root - 2017-12-06 06:01:58.056736: step 23450, loss = 0.82, batch loss = 0.60 (34.4 examples/sec; 0.232 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-06 06:02:00.362389: step 23460, loss = 0.91, batch loss = 0.70 (34.4 examples/sec; 0.232 sec/batch; 19h:57m:12s remains)
INFO - root - 2017-12-06 06:02:02.658749: step 23470, loss = 0.82, batch loss = 0.60 (33.5 examples/sec; 0.239 sec/batch; 20h:29m:53s remains)
INFO - root - 2017-12-06 06:02:05.037900: step 23480, loss = 0.83, batch loss = 0.62 (33.8 examples/sec; 0.236 sec/batch; 20h:17m:41s remains)
INFO - root - 2017-12-06 06:02:07.371896: step 23490, loss = 0.78, batch loss = 0.57 (33.2 examples/sec; 0.241 sec/batch; 20h:41m:28s remains)
INFO - root - 2017-12-06 06:02:09.680744: step 23500, loss = 0.84, batch loss = 0.63 (33.9 examples/sec; 0.236 sec/batch; 20h:16m:48s remains)
2017-12-06 06:02:10.008801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18067981 -0.22786763 -0.24936861 -0.228259 -0.19365311 -0.15843552 -0.14945449 -0.17616639 -0.19267409 -0.19185326 -0.17817223 -0.17744666 -0.2003628 -0.23380616 -0.23468447][-0.14330125 -0.19258717 -0.2082794 -0.19461161 -0.1607518 -0.13091934 -0.11890102 -0.15860143 -0.20975547 -0.24486706 -0.23453188 -0.2197327 -0.22478911 -0.25832754 -0.26459438][-0.14764383 -0.18766496 -0.18404008 -0.1591433 -0.10706417 -0.0631017 -0.052217264 -0.10736084 -0.20656823 -0.28656763 -0.30333832 -0.27862316 -0.25963038 -0.28002596 -0.28126988][-0.17434195 -0.18641973 -0.17744473 -0.13902211 -0.068670861 0.0044830218 0.036144264 -0.019259043 -0.14673707 -0.27059507 -0.34299219 -0.35025483 -0.3300848 -0.31420776 -0.29473013][-0.2122063 -0.22613811 -0.1958082 -0.13058615 -0.030924335 0.070278414 0.1252082 0.078808807 -0.053978723 -0.21853754 -0.3467235 -0.40068108 -0.395419 -0.3661513 -0.31559959][-0.20306364 -0.20732084 -0.16622332 -0.10840968 -0.0054385811 0.10630051 0.1814836 0.15485713 0.01679609 -0.14474718 -0.29643187 -0.40101489 -0.42956311 -0.41032991 -0.34587824][-0.17218949 -0.16571216 -0.14154887 -0.10364698 -0.033291109 0.070504509 0.16071957 0.1663644 0.066281013 -0.079050377 -0.23896739 -0.36016431 -0.4233152 -0.4150196 -0.35122961][-0.1556886 -0.14438032 -0.12971227 -0.10647781 -0.05806087 0.033711508 0.12344217 0.15976539 0.084745355 -0.016578421 -0.15272658 -0.2861605 -0.38903737 -0.40508339 -0.35323054][-0.13929594 -0.12199821 -0.10607191 -0.080051728 -0.037588242 0.044146486 0.1197302 0.15011755 0.10457901 0.036832936 -0.057510715 -0.19041225 -0.31904474 -0.37136811 -0.343431][-0.12103385 -0.10250142 -0.081555367 -0.055478852 -0.013773568 0.048301108 0.11653983 0.15314788 0.11045312 0.076554991 -0.0034649596 -0.11658531 -0.23517054 -0.30421731 -0.309401][-0.10793119 -0.093803465 -0.068392605 -0.038763598 -0.0077392831 0.049764805 0.11566595 0.16254053 0.14294699 0.10629191 0.022032753 -0.075920954 -0.18015316 -0.23865587 -0.244232][-0.093305849 -0.090886235 -0.068228453 -0.040089767 -0.014795341 0.03699147 0.10518832 0.16077194 0.15604448 0.11446869 0.025140472 -0.07874994 -0.17693277 -0.21376152 -0.2115339][-0.075284421 -0.083802462 -0.072794929 -0.059694339 -0.042900585 -0.0059821084 0.055476494 0.11755066 0.12660143 0.078083344 -0.016308866 -0.10294843 -0.18219021 -0.208357 -0.18150762][-0.060262248 -0.078993291 -0.089540526 -0.0948855 -0.08805915 -0.063773617 -0.012531765 0.047865443 0.068155371 0.025409542 -0.051047213 -0.12444182 -0.17250098 -0.17296118 -0.14567676][-0.064821564 -0.081755392 -0.10165417 -0.11065107 -0.11045384 -0.098827153 -0.064745784 -0.01863189 0.0020700544 -0.022427775 -0.073808067 -0.12384302 -0.15613295 -0.15025948 -0.12213959]]...]
INFO - root - 2017-12-06 06:02:12.287067: step 23510, loss = 0.80, batch loss = 0.58 (35.0 examples/sec; 0.229 sec/batch; 19h:37m:18s remains)
INFO - root - 2017-12-06 06:02:14.563367: step 23520, loss = 0.80, batch loss = 0.59 (34.4 examples/sec; 0.233 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-06 06:02:16.858242: step 23530, loss = 0.77, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:46s remains)
INFO - root - 2017-12-06 06:02:19.140434: step 23540, loss = 0.80, batch loss = 0.59 (34.9 examples/sec; 0.229 sec/batch; 19h:41m:02s remains)
INFO - root - 2017-12-06 06:02:21.424154: step 23550, loss = 0.77, batch loss = 0.56 (35.5 examples/sec; 0.225 sec/batch; 19h:20m:04s remains)
INFO - root - 2017-12-06 06:02:23.742303: step 23560, loss = 0.89, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 20h:30m:35s remains)
INFO - root - 2017-12-06 06:02:26.031621: step 23570, loss = 0.79, batch loss = 0.57 (35.6 examples/sec; 0.225 sec/batch; 19h:18m:01s remains)
INFO - root - 2017-12-06 06:02:28.326535: step 23580, loss = 0.84, batch loss = 0.63 (34.2 examples/sec; 0.234 sec/batch; 20h:05m:46s remains)
INFO - root - 2017-12-06 06:02:30.608850: step 23590, loss = 0.88, batch loss = 0.67 (33.5 examples/sec; 0.239 sec/batch; 20h:30m:44s remains)
INFO - root - 2017-12-06 06:02:32.921358: step 23600, loss = 0.85, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 19h:43m:36s remains)
2017-12-06 06:02:33.274613: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3263011 0.5142197 -0.33941144 -0.78260869 -1.0649947 -1.4002113 -1.4470687 -1.4071414 -1.2766373 -1.2489433 -1.0272996 -0.80433542 -0.93024158 -1.3298361 -1.2855883][1.8655057 1.3920851 0.67484671 -0.41576278 -0.96855259 -1.2393243 -1.3620806 -1.0516756 -0.58282578 -0.60454422 -0.76280224 -0.66060787 -0.67988873 -1.4069989 -1.7995722][1.4028413 1.0237174 0.26183337 -0.50266325 -0.83724451 -0.97171611 -0.98426908 -0.94988042 -0.56016165 -0.56562769 -0.83894813 -0.86895037 -0.66950554 -0.8897416 -1.7991562][0.54218328 0.1056429 -0.480491 -0.97566104 -1.1890829 -1.2405822 -1.0805266 -0.66945696 -0.23048101 -0.20509157 -0.32952923 -0.57367474 -0.81386548 -0.88707244 -1.0514863][0.37696812 -0.20130426 -0.80604243 -1.1954207 -1.3923614 -1.2305636 -0.94438845 -0.66670531 -0.46196517 -0.25707236 -0.17652136 -0.16231453 -0.27363735 -0.74323 -0.72179174][1.1234078 0.42932713 -0.065330587 -0.1007627 -0.13882257 -0.33382621 -0.28163004 -0.077251457 -0.019578338 -0.091587424 -0.2449048 -0.49817374 -0.44071186 -0.79655963 -1.1384118][1.4398918 1.1675494 0.729131 0.58086491 0.58719182 0.7832703 0.9560737 1.0661077 1.2353795 0.89070976 0.1992313 -0.26486567 -0.50447559 -0.77543616 -1.4640617][1.6116004 1.0989027 0.83090079 0.93660718 1.2241709 1.3255923 1.5640211 1.7660072 1.5261967 1.1465411 0.88676667 0.22929206 -0.18581788 -0.65503132 -1.2452195][2.0696316 1.6051869 1.2460494 1.1666992 0.97103304 1.1001263 1.3663898 1.2355311 0.95076782 0.78333831 0.51142347 0.065769672 -0.15489328 -0.40027586 -0.82512259][1.0069363 0.54403472 0.38505244 0.30703971 0.45289296 0.47703058 0.49441993 0.48146558 0.31462839 -0.13238908 -0.23338467 -0.38382068 -0.46822923 -0.59925818 -0.74052423][-0.037751883 -0.22910917 -0.44787404 -0.33274812 -0.30496755 -0.194427 -0.35169324 -0.72848576 -1.0714436 -1.4177032 -1.4536295 -1.1478012 -1.1203802 -1.0567176 -1.0403296][-0.56005394 -1.1107652 -1.0176244 -0.92063266 -0.94654983 -1.1026964 -1.2723002 -1.4071543 -1.534456 -1.7382817 -1.9411986 -1.5390627 -1.5159721 -1.3059714 -1.0134969][-1.1099131 -1.4256895 -1.5607953 -1.3919625 -1.265605 -1.3348467 -1.3589473 -1.7878017 -2.290451 -1.8774304 -1.6360319 -1.3156505 -0.84878248 -0.71406448 -0.34835809][-0.91392535 -1.0670121 -1.1715641 -1.3328111 -1.3123269 -1.5286822 -1.7492394 -1.7959192 -1.7987094 -1.5641525 -1.0747972 -0.84601873 -0.77416694 -0.6167239 -0.55281454][-0.36258885 -0.70079064 -1.0537319 -1.533237 -1.6704268 -1.678983 -1.9474723 -1.9889998 -2.033869 -2.0285044 -1.6319008 -1.3162441 -1.1624355 -0.88105154 -0.46766675]]...]
INFO - root - 2017-12-06 06:02:35.545820: step 23610, loss = 0.80, batch loss = 0.58 (36.2 examples/sec; 0.221 sec/batch; 18h:59m:09s remains)
INFO - root - 2017-12-06 06:02:37.859306: step 23620, loss = 0.72, batch loss = 0.51 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:09s remains)
INFO - root - 2017-12-06 06:02:40.123215: step 23630, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 19h:12m:37s remains)
INFO - root - 2017-12-06 06:02:42.420069: step 23640, loss = 0.82, batch loss = 0.61 (34.1 examples/sec; 0.235 sec/batch; 20h:08m:20s remains)
INFO - root - 2017-12-06 06:02:44.733888: step 23650, loss = 0.76, batch loss = 0.55 (34.7 examples/sec; 0.230 sec/batch; 19h:45m:27s remains)
INFO - root - 2017-12-06 06:02:47.018379: step 23660, loss = 0.85, batch loss = 0.63 (35.5 examples/sec; 0.225 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-06 06:02:49.314889: step 23670, loss = 0.86, batch loss = 0.65 (36.1 examples/sec; 0.222 sec/batch; 19h:00m:49s remains)
INFO - root - 2017-12-06 06:02:51.605881: step 23680, loss = 0.86, batch loss = 0.64 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:51s remains)
INFO - root - 2017-12-06 06:02:53.951415: step 23690, loss = 0.80, batch loss = 0.59 (31.6 examples/sec; 0.253 sec/batch; 21h:44m:13s remains)
INFO - root - 2017-12-06 06:02:56.266370: step 23700, loss = 0.79, batch loss = 0.58 (34.3 examples/sec; 0.234 sec/batch; 20h:01m:50s remains)
2017-12-06 06:02:56.602728: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.92966521 0.97299004 0.77744126 0.50893492 0.28264 0.182051 -0.010374539 -0.15919769 -0.26284274 -0.22608313 -0.29219851 -0.77135772 -1.1591694 -1.4156491 -1.5559552][0.64222574 0.44029891 0.19040367 -0.015723877 -0.18567529 -0.30148071 -0.3610889 -0.43442383 -0.70458293 -1.1492169 -1.4182978 -1.634017 -1.817946 -1.8688381 -1.9803418][0.34154961 0.17524627 -0.0083842278 -0.28496212 -0.59448355 -0.68821985 -0.682948 -0.76497155 -0.82725871 -1.0359287 -1.2107762 -1.6387231 -2.1173608 -2.2948844 -2.3468347][0.049812414 0.015418582 -0.13553667 -0.44270536 -0.72130269 -0.737477 -0.72333133 -0.76451659 -0.67249393 -0.544728 -0.60045367 -0.99660665 -1.4742684 -1.9169737 -2.1312125][-0.16157785 -0.14316146 -0.19435093 -0.40115708 -0.53770971 -0.52789772 -0.41440246 -0.24860153 -0.06571985 0.21499407 0.43246484 0.22253835 -0.2181156 -0.82148015 -1.2368022][-0.48060352 -0.41182357 -0.30773449 -0.14366269 -0.051957335 0.11760249 0.28095055 0.36091027 0.7112053 1.2048568 1.5411714 1.5868914 1.453681 0.8949616 0.29965544][-0.71390933 -0.68292153 -0.57312596 -0.46028158 -0.10878739 0.2791709 0.68053639 0.98971343 1.302582 1.7037017 2.0599608 2.1705818 2.0912197 1.9535335 1.4018323][-0.61217248 -0.73623836 -0.76167679 -0.77469212 -0.38666633 0.18002316 0.72341263 1.0474373 1.2091966 1.3641664 1.5841403 1.7488581 1.6903328 1.6013075 1.410497][-0.422235 -0.52067786 -0.57111865 -0.52951711 -0.31833142 -0.0025104657 0.47312307 1.0032527 1.27748 1.2741216 1.2540085 1.1214216 1.0374236 0.92871284 0.74259347][-0.25206119 -0.32062972 -0.38040388 -0.40997663 -0.22705022 0.1073726 0.46966958 0.7744574 0.90573466 0.99388158 0.75995988 0.35815141 -0.019435212 -0.036193483 0.024246328][-0.22222397 -0.26772782 -0.30679941 -0.38470894 -0.29601595 -0.088306747 0.21385926 0.47956985 0.57002711 0.466713 0.12821808 -0.23202369 -0.80516124 -0.85543734 -0.6260758][-0.21627134 -0.29204631 -0.34626952 -0.3856757 -0.44122386 -0.42792746 -0.30038828 0.019659288 0.20828497 0.18677044 -0.061846178 -0.43637571 -0.76236433 -0.86290717 -0.90499228][-0.064081684 -0.14641534 -0.220588 -0.13954705 -0.25894177 -0.34848768 -0.44095525 -0.37128362 -0.29110792 -0.23685512 -0.31286216 -0.45297685 -0.65705657 -0.8154788 -0.88499421][-0.073278874 -0.11009638 -0.14298218 -0.12252279 -0.20855351 -0.30869034 -0.39058587 -0.46139836 -0.5277518 -0.47541174 -0.44639555 -0.55445665 -0.75146949 -0.77888262 -0.81127995][-0.10250127 -0.10346358 -0.036080692 -0.024529219 -0.12204821 -0.15404627 -0.19676208 -0.18009472 -0.24883896 -0.29449216 -0.33467823 -0.29593202 -0.24249998 -0.44563428 -0.6075362]]...]
INFO - root - 2017-12-06 06:02:58.937347: step 23710, loss = 0.77, batch loss = 0.56 (34.9 examples/sec; 0.229 sec/batch; 19h:38m:04s remains)
INFO - root - 2017-12-06 06:03:01.231435: step 23720, loss = 0.83, batch loss = 0.62 (35.4 examples/sec; 0.226 sec/batch; 19h:22m:48s remains)
INFO - root - 2017-12-06 06:03:03.539696: step 23730, loss = 0.80, batch loss = 0.59 (30.2 examples/sec; 0.265 sec/batch; 22h:43m:19s remains)
INFO - root - 2017-12-06 06:03:05.891837: step 23740, loss = 0.81, batch loss = 0.60 (33.4 examples/sec; 0.239 sec/batch; 20h:32m:24s remains)
INFO - root - 2017-12-06 06:03:08.232281: step 23750, loss = 0.82, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-06 06:03:10.531799: step 23760, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.231 sec/batch; 19h:47m:26s remains)
INFO - root - 2017-12-06 06:03:12.831820: step 23770, loss = 0.81, batch loss = 0.59 (36.1 examples/sec; 0.222 sec/batch; 19h:00m:46s remains)
INFO - root - 2017-12-06 06:03:15.139184: step 23780, loss = 0.90, batch loss = 0.68 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:43s remains)
INFO - root - 2017-12-06 06:03:17.412491: step 23790, loss = 0.78, batch loss = 0.57 (35.1 examples/sec; 0.228 sec/batch; 19h:31m:37s remains)
INFO - root - 2017-12-06 06:03:19.718147: step 23800, loss = 0.86, batch loss = 0.64 (33.9 examples/sec; 0.236 sec/batch; 20h:13m:52s remains)
2017-12-06 06:03:20.064213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0836563 -2.158581 -2.1322393 -1.9571991 -1.7407837 -1.4757472 -1.209533 -0.99300069 -0.82652533 -0.73552781 -0.65852875 -0.67648196 -0.54556757 -0.36496934 -0.26755217][-2.5693061 -2.7020364 -2.7406511 -2.5004046 -2.2730103 -2.0838726 -1.9476268 -1.6652665 -1.360186 -1.1761454 -1.0259076 -1.0182931 -0.79188573 -0.61148441 -0.53140223][-1.8732958 -2.0547745 -2.0680184 -2.0341113 -2.0600419 -1.9618702 -1.9024128 -1.976806 -1.9138762 -1.6744909 -1.4673649 -1.4072216 -1.2220514 -0.95746857 -0.7514326][-0.29020613 -0.67677569 -0.91496164 -0.9641633 -1.0840548 -1.2596986 -1.4908828 -1.6447694 -1.6856042 -1.704072 -1.6831497 -1.5678099 -1.3768306 -1.1073567 -0.85937363][1.2052909 0.74245346 0.46621543 0.18924931 -0.10680243 -0.26608342 -0.4715721 -0.65593815 -1.0279467 -1.1909252 -1.1689652 -1.1956978 -1.0866004 -0.98135543 -0.75229865][1.3520478 1.1206576 0.98343629 1.0580072 0.90746742 0.9556635 0.79284549 0.62250072 0.21803209 -0.12323366 -0.40673974 -0.43118587 -0.40045321 -0.3336561 -0.23678395][1.390633 1.2566794 1.2397624 1.5483902 1.7288574 1.9698459 1.9220239 1.8724754 1.5338614 1.0140045 0.5586518 0.29676977 0.11814708 0.16650203 0.20055029][1.3967501 1.2275724 1.3495884 1.6135308 2.1174383 2.5471234 2.7650125 2.7664535 2.3770952 1.9284247 1.5131955 0.97901875 0.48799914 0.32348076 0.23549077][0.68972015 0.55429685 0.90890008 1.2399232 1.8005867 2.2738113 2.7511747 2.7767844 2.5995374 2.095083 1.5445901 1.0491766 0.69444346 0.38704818 0.22743148][0.12736893 -0.18990913 -0.076304644 -0.028702777 0.46737736 0.79664636 1.1949992 1.3935783 1.4231894 1.2504247 1.0386938 0.68910706 0.35196966 0.12489196 0.090218581][-0.555823 -0.76291859 -0.78685284 -0.82532096 -0.68806767 -0.66156942 -0.45465237 -0.24141753 -0.046056554 0.061623536 0.082994945 0.018338457 -0.09010537 -0.17395669 -0.16381526][-1.4687861 -1.5157043 -1.7568232 -1.8440366 -1.7749821 -1.7904691 -1.6666549 -1.5122195 -1.3125968 -1.0687519 -0.7643007 -0.65982693 -0.49788091 -0.43331847 -0.27216825][-1.8748959 -1.8125627 -1.8681366 -1.8458923 -1.898145 -1.9007813 -1.8352666 -1.8331409 -1.6591743 -1.4257165 -1.1547865 -0.89858645 -0.65926105 -0.43627647 -0.29716477][-2.1440058 -2.0409269 -2.0053055 -1.8827087 -1.7530837 -1.6758358 -1.5639156 -1.450067 -1.2784979 -1.0263931 -0.87572277 -0.68964207 -0.43560031 -0.30080104 -0.24997044][-1.6702802 -1.7252301 -1.8362859 -1.5798697 -1.4909645 -1.3686959 -1.1934454 -1.0599072 -0.89740986 -0.63736624 -0.53237462 -0.40902078 -0.29273403 -0.21877849 -0.19920203]]...]
INFO - root - 2017-12-06 06:03:22.349926: step 23810, loss = 0.82, batch loss = 0.60 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:26s remains)
INFO - root - 2017-12-06 06:03:24.636128: step 23820, loss = 0.79, batch loss = 0.58 (36.4 examples/sec; 0.220 sec/batch; 18h:51m:47s remains)
INFO - root - 2017-12-06 06:03:26.963962: step 23830, loss = 0.90, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 19h:54m:14s remains)
INFO - root - 2017-12-06 06:03:29.273230: step 23840, loss = 0.83, batch loss = 0.61 (33.7 examples/sec; 0.237 sec/batch; 20h:21m:41s remains)
INFO - root - 2017-12-06 06:03:31.581774: step 23850, loss = 0.84, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 19h:01m:46s remains)
INFO - root - 2017-12-06 06:03:33.882564: step 23860, loss = 0.91, batch loss = 0.69 (34.3 examples/sec; 0.233 sec/batch; 20h:00m:35s remains)
INFO - root - 2017-12-06 06:03:36.158350: step 23870, loss = 0.87, batch loss = 0.65 (35.7 examples/sec; 0.224 sec/batch; 19h:13m:45s remains)
INFO - root - 2017-12-06 06:03:38.438900: step 23880, loss = 0.75, batch loss = 0.54 (35.3 examples/sec; 0.227 sec/batch; 19h:26m:01s remains)
INFO - root - 2017-12-06 06:03:40.733930: step 23890, loss = 0.85, batch loss = 0.63 (34.6 examples/sec; 0.232 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-06 06:03:43.044633: step 23900, loss = 0.90, batch loss = 0.68 (35.5 examples/sec; 0.225 sec/batch; 19h:18m:44s remains)
2017-12-06 06:03:43.433277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0672929 -0.069012791 -0.070590764 -0.071447618 -0.07092765 -0.068948753 -0.0664706 -0.064808071 -0.06372948 -0.063747421 -0.064237222 -0.06481041 -0.064735919 -0.064084567 -0.063840911][-0.071851611 -0.074764043 -0.07665921 -0.077171877 -0.077296659 -0.078237526 -0.07923571 -0.080132566 -0.079265915 -0.077998936 -0.075281516 -0.071267448 -0.068155482 -0.066656515 -0.065923616][-0.083195053 -0.089374125 -0.091304563 -0.08734896 -0.08095137 -0.076837033 -0.075511567 -0.074102417 -0.070170544 -0.067732818 -0.067204237 -0.067224771 -0.067519292 -0.066593036 -0.064317077][-0.10786724 -0.12547651 -0.13869897 -0.14468226 -0.1397119 -0.11858021 -0.084411666 -0.051524594 -0.025676303 -0.013905294 -0.016529351 -0.029053058 -0.044234447 -0.055685353 -0.060991812][-0.12098511 -0.13134192 -0.14011271 -0.15290248 -0.17169109 -0.1789538 -0.17011125 -0.14716589 -0.11567601 -0.091843359 -0.075805955 -0.066543788 -0.062268268 -0.063562982 -0.064861454][-0.10156826 -0.081787869 -0.053368084 -0.030722521 -0.04274983 -0.092379034 -0.14688081 -0.17931901 -0.18119648 -0.16047248 -0.13326198 -0.1075905 -0.088054456 -0.078202978 -0.072217196][-0.074177742 -0.038170107 0.008436799 0.063765593 0.10697158 0.11328561 0.068662815 -0.010479629 -0.082340732 -0.10961357 -0.11314903 -0.09793964 -0.082178153 -0.074601196 -0.07195434][-0.093701757 -0.079320148 -0.060066022 -0.01681374 0.043025889 0.11999399 0.17447877 0.20622775 0.18695301 0.14218906 0.075813629 0.028025694 -0.009806484 -0.032248441 -0.046048123][-0.10336863 -0.097472511 -0.087113917 -0.05072524 0.0022325292 0.051356576 0.075257368 0.10374846 0.091825373 0.052633397 -0.0080032349 -0.031703427 -0.042684544 -0.050234776 -0.057762846][-0.092351012 -0.07960669 -0.057552557 -0.023679912 -0.00072505325 0.003945142 -0.013260953 -0.010011725 -0.013797492 -0.027333021 -0.057488848 -0.0703477 -0.071153924 -0.06727422 -0.057982497][-0.094626807 -0.091281943 -0.082673907 -0.0630575 -0.039844424 -0.011673972 -0.01003591 -0.013047673 -0.019225895 -0.026621938 -0.051500037 -0.062277596 -0.064792506 -0.061337352 -0.057653468][-0.10682638 -0.12007672 -0.12986106 -0.12772167 -0.11348657 -0.086739138 -0.066055693 -0.062103018 -0.074289851 -0.069901593 -0.0698259 -0.064904287 -0.065477788 -0.06508778 -0.06693022][-0.10160939 -0.11280075 -0.11382812 -0.10836452 -0.10117353 -0.091294333 -0.081370384 -0.069212906 -0.068819411 -0.062870204 -0.061554596 -0.056478266 -0.061252754 -0.065184943 -0.067357853][-0.080112077 -0.084146589 -0.07900092 -0.066981107 -0.056547042 -0.05604488 -0.062852047 -0.05659426 -0.051047053 -0.051760182 -0.061416559 -0.061832562 -0.06136898 -0.062650017 -0.063634925][-0.0698942 -0.077315994 -0.080592044 -0.076702237 -0.068164289 -0.060171075 -0.048198871 -0.041857805 -0.028555434 -0.025253378 -0.039994661 -0.047759376 -0.052735124 -0.059108078 -0.06351462]]...]
INFO - root - 2017-12-06 06:03:45.725926: step 23910, loss = 0.73, batch loss = 0.52 (35.0 examples/sec; 0.228 sec/batch; 19h:34m:35s remains)
INFO - root - 2017-12-06 06:03:48.042633: step 23920, loss = 0.89, batch loss = 0.67 (35.7 examples/sec; 0.224 sec/batch; 19h:11m:10s remains)
INFO - root - 2017-12-06 06:03:50.347082: step 23930, loss = 0.83, batch loss = 0.61 (34.7 examples/sec; 0.231 sec/batch; 19h:45m:36s remains)
INFO - root - 2017-12-06 06:03:52.652179: step 23940, loss = 0.93, batch loss = 0.71 (34.2 examples/sec; 0.234 sec/batch; 20h:01m:13s remains)
INFO - root - 2017-12-06 06:03:54.990948: step 23950, loss = 0.81, batch loss = 0.59 (34.7 examples/sec; 0.231 sec/batch; 19h:46m:56s remains)
INFO - root - 2017-12-06 06:03:57.302957: step 23960, loss = 0.78, batch loss = 0.57 (33.3 examples/sec; 0.240 sec/batch; 20h:33m:38s remains)
INFO - root - 2017-12-06 06:03:59.616728: step 23970, loss = 0.79, batch loss = 0.57 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:49s remains)
INFO - root - 2017-12-06 06:04:01.915177: step 23980, loss = 0.71, batch loss = 0.50 (35.8 examples/sec; 0.224 sec/batch; 19h:09m:19s remains)
INFO - root - 2017-12-06 06:04:04.173227: step 23990, loss = 0.85, batch loss = 0.63 (35.1 examples/sec; 0.228 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-06 06:04:06.447916: step 24000, loss = 0.84, batch loss = 0.63 (33.7 examples/sec; 0.237 sec/batch; 20h:20m:12s remains)
2017-12-06 06:04:06.792317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2793677 -2.341543 -2.498019 -2.5747306 -2.3710771 -2.120945 -2.025785 -1.6348796 -1.3104067 -1.1159685 -0.89549303 -0.79126149 -0.76427907 -0.74269515 -0.67881519][-1.1600668 -2.2881107 -2.9907422 -2.9115274 -2.8084211 -2.5312989 -2.4688363 -2.3157396 -2.0078869 -1.6335762 -1.2604015 -0.82954454 -0.46648738 -0.24711286 -0.12752773][-1.2516022 -1.5506904 -1.9700272 -2.5546298 -2.7817903 -2.5884678 -2.1976173 -1.994596 -2.13663 -2.0354669 -1.6728635 -1.4145539 -1.2382581 -0.85285622 -0.4184874][-0.58907706 -1.2527635 -1.7395151 -1.7993891 -1.7628226 -1.8595624 -1.989928 -1.871882 -1.7065098 -1.6713803 -1.7039454 -1.5426812 -1.3064795 -1.2378969 -1.1339011][0.30652958 0.21130073 -0.17156658 -0.3092007 -0.31130216 -0.35367531 -0.50954795 -0.85443413 -1.3102734 -1.289551 -1.3055534 -1.5016339 -1.498595 -1.4004071 -1.3140874][1.4379444 1.39099 1.3088717 1.211648 1.3281729 1.1510274 0.99590796 0.68904489 0.3735958 -0.22546369 -0.91263258 -1.0942798 -1.1775491 -1.389164 -1.5753202][2.1569574 1.942374 2.0402439 2.2631395 2.4064107 2.1237977 1.9431014 1.6490211 1.4161422 0.90232307 0.22155124 -0.44933468 -1.0981424 -1.3635685 -1.3584697][2.0516579 1.9313023 1.8506687 1.6119852 1.5467784 1.6264427 1.7396626 1.7523026 1.8301539 1.5709655 1.2633128 0.56323874 -0.061469316 -0.56071913 -1.0425075][0.59435767 0.50163394 0.52956742 0.28718254 0.2851108 0.59039015 0.66557628 0.79668063 0.95035857 1.0765951 1.136405 0.72840345 0.5631988 0.11292019 -0.36332166][-2.0160475 -1.8864558 -1.7116358 -1.5156984 -1.3120513 -1.2669311 -1.0671566 -0.74034303 -0.49998677 -0.15721343 0.12505156 0.19152892 0.29593205 0.30643377 0.29342061][-3.2793672 -3.5271659 -3.3860953 -3.2259712 -2.7914808 -2.3427792 -2.0572822 -1.8163021 -1.4943645 -1.0590816 -0.75666964 -0.39935291 -0.027824387 0.12988885 0.20465469][-3.404752 -3.7375944 -3.4622052 -3.2751596 -3.1044316 -3.0970812 -2.952239 -2.8026249 -2.6639209 -2.1767387 -1.6927071 -1.2809188 -1.0090499 -0.5245012 -0.15882586][-2.8497694 -3.1411371 -3.0262513 -2.8116791 -2.6252408 -2.6070426 -2.5777874 -2.4803066 -2.4218693 -2.2769473 -2.0175567 -1.5416238 -1.220855 -0.98605317 -0.8292585][-2.1169026 -2.1137073 -2.1783302 -2.2603695 -2.2818489 -2.1738989 -2.0264986 -2.033062 -1.8504126 -1.764564 -1.5536714 -1.4501841 -1.4136083 -1.2196219 -1.0648804][-0.69777304 -0.91983807 -1.0117013 -1.2267301 -1.5389402 -1.6738136 -1.6070459 -1.4014707 -1.309145 -1.2166193 -0.96271908 -0.84599382 -1.0055382 -1.1882463 -1.3015714]]...]
INFO - root - 2017-12-06 06:04:09.091206: step 24010, loss = 0.88, batch loss = 0.67 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:01s remains)
INFO - root - 2017-12-06 06:04:11.382191: step 24020, loss = 0.77, batch loss = 0.56 (33.7 examples/sec; 0.238 sec/batch; 20h:21m:45s remains)
INFO - root - 2017-12-06 06:04:13.698144: step 24030, loss = 0.83, batch loss = 0.62 (35.7 examples/sec; 0.224 sec/batch; 19h:13m:12s remains)
INFO - root - 2017-12-06 06:04:16.005904: step 24040, loss = 0.79, batch loss = 0.57 (34.2 examples/sec; 0.234 sec/batch; 20h:01m:42s remains)
INFO - root - 2017-12-06 06:04:18.317023: step 24050, loss = 0.83, batch loss = 0.61 (35.4 examples/sec; 0.226 sec/batch; 19h:22m:36s remains)
INFO - root - 2017-12-06 06:04:20.607563: step 24060, loss = 0.82, batch loss = 0.61 (34.1 examples/sec; 0.235 sec/batch; 20h:07m:27s remains)
INFO - root - 2017-12-06 06:04:22.906523: step 24070, loss = 0.85, batch loss = 0.64 (35.7 examples/sec; 0.224 sec/batch; 19h:11m:41s remains)
INFO - root - 2017-12-06 06:04:25.228341: step 24080, loss = 0.83, batch loss = 0.61 (33.2 examples/sec; 0.241 sec/batch; 20h:40m:14s remains)
INFO - root - 2017-12-06 06:04:27.498457: step 24090, loss = 0.77, batch loss = 0.56 (34.2 examples/sec; 0.234 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-06 06:04:29.825438: step 24100, loss = 0.83, batch loss = 0.61 (34.3 examples/sec; 0.233 sec/batch; 19h:57m:24s remains)
2017-12-06 06:04:30.216968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21986362 -0.34009793 -0.3592332 -0.53318995 -0.6218313 -0.61771 -0.64840329 -0.58000314 -0.57298529 -0.66254056 -0.68669796 -0.71471566 -0.67260641 -0.44913769 -0.1015881][0.18272045 0.011188038 -0.20267335 -0.42009342 -0.5835318 -0.61733305 -0.64847308 -0.69831532 -0.7546621 -0.81712842 -0.83445352 -0.83162743 -0.75360525 -0.59568566 -0.28144753][-0.18060052 -0.2710965 -0.32512391 -0.52078867 -0.69578773 -0.73434734 -0.78848881 -0.85088927 -0.86958742 -0.901765 -0.91289932 -0.89081091 -0.84629953 -0.61093187 -0.331525][-0.18701023 -0.44860342 -0.62132943 -0.76294237 -0.83119905 -0.86377662 -0.91527551 -0.96188992 -0.98548508 -1.0797708 -1.1105748 -1.0145267 -0.8955062 -0.68988687 -0.394845][-0.33386776 -0.54847068 -0.74482709 -0.853746 -0.89535546 -0.89122415 -0.9119814 -0.94506311 -0.97325796 -1.0395993 -1.0965959 -1.1049392 -1.0385923 -0.79157823 -0.54278487][-0.342353 -0.53918189 -0.63267946 -0.71343976 -0.67324841 -0.68669873 -0.745161 -0.78170741 -0.81115037 -0.86696947 -0.98149586 -1.0421287 -0.99924743 -0.82329124 -0.62813967][-0.066811129 -0.2479735 -0.26014557 -0.37706748 -0.34388277 -0.33246866 -0.37638137 -0.43165952 -0.4957841 -0.61117542 -0.73193723 -0.82230109 -0.88110775 -0.7802124 -0.63000965][0.22611406 0.026644491 0.019512713 -0.0066304877 0.0018399581 0.0032636598 -0.052852772 -0.1228179 -0.20336926 -0.27925128 -0.37576285 -0.51565444 -0.57822567 -0.48471078 -0.4656218][0.25734979 0.12673238 0.13831294 0.099317573 0.07610216 0.084651656 0.053647436 -0.027417205 -0.10940096 -0.1893342 -0.2693184 -0.30654722 -0.31602654 -0.21486567 -0.11365629][0.14023322 -0.062405188 -0.073717467 -0.065012053 -0.10544088 -0.10662477 -0.13326605 -0.22266281 -0.3038314 -0.34809306 -0.34791526 -0.37490597 -0.40238598 -0.17933148 0.069679938][-0.22261003 -0.37367553 -0.41953993 -0.42953047 -0.45534876 -0.499927 -0.53701663 -0.57765758 -0.61551231 -0.63959378 -0.60991639 -0.60461956 -0.49782658 -0.3139832 -0.20268375][-0.40425509 -0.5110954 -0.6713655 -0.789154 -0.81300664 -0.79451811 -0.78711581 -0.81169814 -0.83460873 -0.86501086 -0.83331859 -0.76529574 -0.7246449 -0.56734562 -0.40907821][-0.410089 -0.58053768 -0.80477768 -0.93909824 -0.96192044 -1.0047455 -1.0010262 -0.976585 -0.93768632 -0.9539538 -0.93331558 -0.89205736 -0.8176949 -0.6647141 -0.55194932][-0.32288927 -0.40464756 -0.50476336 -0.62101245 -0.64922506 -0.7177639 -0.7492379 -0.73900014 -0.69530559 -0.73403782 -0.75617611 -0.72799605 -0.68608618 -0.55909556 -0.51042479][-0.076755993 -0.18473458 -0.26483014 -0.31668741 -0.30402586 -0.31308064 -0.28773049 -0.30359298 -0.32877722 -0.44074041 -0.51113826 -0.42198882 -0.37646273 -0.35439602 -0.21991432]]...]
INFO - root - 2017-12-06 06:04:32.501997: step 24110, loss = 0.79, batch loss = 0.58 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:15s remains)
INFO - root - 2017-12-06 06:04:34.798215: step 24120, loss = 0.70, batch loss = 0.49 (37.0 examples/sec; 0.216 sec/batch; 18h:31m:12s remains)
INFO - root - 2017-12-06 06:04:37.127344: step 24130, loss = 0.77, batch loss = 0.56 (34.0 examples/sec; 0.235 sec/batch; 20h:08m:01s remains)
INFO - root - 2017-12-06 06:04:39.416899: step 24140, loss = 0.82, batch loss = 0.61 (33.7 examples/sec; 0.237 sec/batch; 20h:18m:23s remains)
INFO - root - 2017-12-06 06:04:41.752301: step 24150, loss = 0.82, batch loss = 0.60 (34.3 examples/sec; 0.233 sec/batch; 19h:57m:01s remains)
INFO - root - 2017-12-06 06:04:44.050434: step 24160, loss = 0.81, batch loss = 0.59 (34.6 examples/sec; 0.231 sec/batch; 19h:48m:34s remains)
INFO - root - 2017-12-06 06:04:46.377925: step 24170, loss = 0.88, batch loss = 0.67 (35.6 examples/sec; 0.225 sec/batch; 19h:16m:19s remains)
INFO - root - 2017-12-06 06:04:48.716175: step 24180, loss = 0.75, batch loss = 0.53 (35.2 examples/sec; 0.227 sec/batch; 19h:26m:18s remains)
INFO - root - 2017-12-06 06:04:50.987233: step 24190, loss = 0.84, batch loss = 0.63 (35.8 examples/sec; 0.223 sec/batch; 19h:07m:17s remains)
INFO - root - 2017-12-06 06:04:53.253126: step 24200, loss = 0.76, batch loss = 0.55 (32.5 examples/sec; 0.246 sec/batch; 21h:03m:55s remains)
2017-12-06 06:04:53.762767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0816609 -0.083388977 -0.090852268 -0.11675391 -0.15394479 -0.18711433 -0.20640495 -0.20578164 -0.19085445 -0.17736709 -0.1631 -0.13432087 -0.08911863 -0.054827906 -0.046876747][-0.084067747 -0.084778383 -0.088559642 -0.10774896 -0.13657543 -0.15965252 -0.17052081 -0.16952968 -0.16700399 -0.17769004 -0.17654917 -0.13836439 -0.065791681 -0.0065603852 0.011244707][-0.10192912 -0.10756129 -0.10273121 -0.0942643 -0.089998677 -0.082109295 -0.066037595 -0.053034913 -0.06796705 -0.12577935 -0.1717144 -0.15968785 -0.09724918 -0.039830208 -0.014925666][-0.12690039 -0.13816883 -0.12525064 -0.092664033 -0.0634971 -0.0305079 0.026365645 0.081249267 0.077584341 -0.0065305308 -0.09994515 -0.13323846 -0.11040922 -0.088467509 -0.079974726][-0.13693541 -0.13655058 -0.10055568 -0.048505966 -0.020040855 0.0026237816 0.081144184 0.188263 0.21790233 0.12854941 0.00089102983 -0.0710889 -0.084445313 -0.090114541 -0.10151417][-0.11181144 -0.070118211 0.017767429 0.10276321 0.13129386 0.12835027 0.18198577 0.28722322 0.29606447 0.17539483 0.023935556 -0.054163918 -0.06264203 -0.058656223 -0.071905352][-0.097235292 -0.030971404 0.098239794 0.22039953 0.28193468 0.30375433 0.36169532 0.44630682 0.39253145 0.20808366 0.0165594 -0.071588665 -0.071198478 -0.047723014 -0.053986922][-0.10667689 -0.062390961 0.044574156 0.15702449 0.22962585 0.29033083 0.38833368 0.48563296 0.42616534 0.23820457 0.046868041 -0.047302198 -0.055571984 -0.037780508 -0.044927366][-0.1367906 -0.12631926 -0.068338536 0.00011982024 0.050035894 0.1127459 0.21712029 0.31232604 0.2763207 0.14816807 0.014205471 -0.0571255 -0.073224753 -0.06447757 -0.067959487][-0.14715753 -0.15787446 -0.1307984 -0.092874572 -0.066537872 -0.027106352 0.040773302 0.085173175 0.051786631 -0.010332607 -0.065128773 -0.085936047 -0.08318305 -0.075444125 -0.077692509][-0.13924879 -0.16409558 -0.16930762 -0.16510148 -0.14965336 -0.11330541 -0.070259705 -0.048945267 -0.066870406 -0.088577084 -0.10470533 -0.10073772 -0.09239541 -0.085648529 -0.088583574][-0.11453584 -0.13662373 -0.15007272 -0.16355622 -0.17223763 -0.15118122 -0.12104724 -0.0988457 -0.093059629 -0.084844023 -0.081445366 -0.072576672 -0.076071613 -0.077842966 -0.084855564][-0.089434378 -0.096831933 -0.10278277 -0.11575672 -0.12625217 -0.11824245 -0.10869826 -0.1065913 -0.10132551 -0.088453971 -0.076482825 -0.064118013 -0.067929335 -0.06561996 -0.071871214][-0.078156441 -0.0763243 -0.07516776 -0.0783151 -0.074753344 -0.064052984 -0.061753586 -0.076435149 -0.083420068 -0.072771542 -0.074043624 -0.072166465 -0.070739828 -0.0631749 -0.068633571][-0.076421045 -0.073352009 -0.070952982 -0.0673276 -0.057228226 -0.048889954 -0.046229541 -0.054000795 -0.055964004 -0.05021305 -0.064141542 -0.0698244 -0.072779857 -0.066828161 -0.075145014]]...]
INFO - root - 2017-12-06 06:04:56.066225: step 24210, loss = 0.66, batch loss = 0.45 (34.7 examples/sec; 0.231 sec/batch; 19h:46m:16s remains)
INFO - root - 2017-12-06 06:04:58.388003: step 24220, loss = 0.82, batch loss = 0.61 (34.5 examples/sec; 0.232 sec/batch; 19h:50m:11s remains)
INFO - root - 2017-12-06 06:05:00.681141: step 24230, loss = 0.83, batch loss = 0.62 (35.5 examples/sec; 0.225 sec/batch; 19h:16m:19s remains)
INFO - root - 2017-12-06 06:05:03.016368: step 24240, loss = 0.78, batch loss = 0.56 (33.3 examples/sec; 0.240 sec/batch; 20h:33m:04s remains)
INFO - root - 2017-12-06 06:05:05.349670: step 24250, loss = 0.72, batch loss = 0.50 (34.0 examples/sec; 0.236 sec/batch; 20h:10m:30s remains)
INFO - root - 2017-12-06 06:05:07.661457: step 24260, loss = 0.78, batch loss = 0.57 (35.4 examples/sec; 0.226 sec/batch; 19h:22m:01s remains)
INFO - root - 2017-12-06 06:05:09.970713: step 24270, loss = 0.83, batch loss = 0.62 (33.8 examples/sec; 0.236 sec/batch; 20h:14m:54s remains)
INFO - root - 2017-12-06 06:05:12.283132: step 24280, loss = 0.83, batch loss = 0.61 (35.1 examples/sec; 0.228 sec/batch; 19h:29m:28s remains)
INFO - root - 2017-12-06 06:05:14.562393: step 24290, loss = 0.85, batch loss = 0.64 (35.3 examples/sec; 0.226 sec/batch; 19h:23m:12s remains)
INFO - root - 2017-12-06 06:05:16.868747: step 24300, loss = 0.86, batch loss = 0.64 (34.8 examples/sec; 0.230 sec/batch; 19h:42m:31s remains)
2017-12-06 06:05:17.211005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3209964 -1.2862238 -1.1472192 -1.0692545 -1.0056343 -1.1063514 -0.98013288 -0.77926105 -0.52010471 -0.37303394 -0.21756801 -0.054393277 0.40851793 1.2337655 1.4816561][-1.5795869 -1.6464434 -1.5895356 -1.5588933 -1.5909601 -1.703204 -1.731374 -1.7320548 -1.6177946 -1.4921879 -1.3279642 -1.0844029 -0.84133595 -0.44053081 -0.79332393][-1.8903607 -2.2302423 -2.4368286 -2.4118042 -2.3561513 -2.3492062 -2.3589537 -2.1637018 -2.03193 -2.0950446 -2.2573159 -2.5941491 -2.8750124 -2.6359155 -2.534903][-1.9400162 -2.3426614 -2.6825774 -2.8620398 -2.9779785 -2.7964365 -2.5936568 -2.4655752 -2.2317054 -2.1012158 -2.1370819 -2.7973619 -3.6373463 -4.1768203 -4.465066][-1.7251608 -2.1362276 -2.3792686 -2.5695844 -2.7066665 -2.7512102 -2.7240276 -2.4274204 -2.1853042 -2.1789398 -2.0675108 -2.2204347 -2.6158297 -3.2920771 -3.7724228][-1.0643904 -1.4529399 -1.6046708 -1.8014684 -1.77554 -1.6193974 -1.4312189 -1.191933 -0.95534235 -0.70181179 -0.52482814 -0.4337858 -0.55573714 -0.77493024 -1.3952968][-0.049002744 -0.14442001 -0.22076559 -0.29766682 -0.27837327 -0.24978793 -0.13481617 0.19266096 0.50400609 0.829204 1.1746923 1.583249 1.6816139 1.4423627 1.2016097][0.6521399 0.75184137 0.90829897 1.1402646 1.3462799 1.4545386 1.5110964 1.694568 1.9046861 2.1601832 2.5127268 3.0934494 3.4555426 3.5131247 3.0731218][0.89176053 1.2029203 1.5257449 1.7272329 2.0639775 2.2571597 2.4751697 2.5834973 2.5964198 2.6928039 2.9013436 3.0572546 3.2804241 3.3497181 3.3666482][0.569914 0.79268056 1.0044218 1.2887346 1.6243894 1.8522774 2.1172214 2.2605455 2.3367362 2.4300284 2.4389293 2.586302 2.8303208 2.9085929 2.8185353][0.077209853 0.21112821 0.35589543 0.49633363 0.68323743 0.79064226 0.81982583 0.828303 0.81928396 0.84984404 0.82774252 0.83437228 0.78911126 1.189491 1.5933881][-0.34674409 -0.54618323 -0.64362049 -0.67054009 -0.76578993 -0.63936192 -0.53882414 -0.56385994 -0.69101334 -0.80183136 -0.803087 -0.7424134 -0.81756884 -0.57897812 -0.16548215][-0.40573284 -0.61943835 -0.91542625 -1.1370784 -1.354787 -1.5323578 -1.7196846 -1.8390951 -2.0085368 -2.1233754 -2.3595469 -2.4218955 -2.3699684 -2.2166612 -2.0436313][-0.38725042 -0.5798251 -0.86618608 -1.2037146 -1.4869142 -1.7194889 -1.8712198 -2.0954871 -2.3594694 -2.6248364 -2.795887 -2.9544365 -3.1587052 -3.0595078 -2.8202434][-0.31247023 -0.47845644 -0.69043159 -0.92043239 -1.0942216 -1.3663216 -1.6177064 -1.88309 -2.0993493 -2.4073374 -2.6626494 -2.7292061 -2.6773782 -2.6266787 -2.5590913]]...]
INFO - root - 2017-12-06 06:05:19.537214: step 24310, loss = 0.84, batch loss = 0.63 (35.9 examples/sec; 0.223 sec/batch; 19h:06m:07s remains)
INFO - root - 2017-12-06 06:05:21.848709: step 24320, loss = 0.82, batch loss = 0.60 (34.8 examples/sec; 0.230 sec/batch; 19h:42m:09s remains)
INFO - root - 2017-12-06 06:05:24.207452: step 24330, loss = 0.75, batch loss = 0.54 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:15s remains)
INFO - root - 2017-12-06 06:05:26.532385: step 24340, loss = 0.79, batch loss = 0.58 (33.9 examples/sec; 0.236 sec/batch; 20h:12m:50s remains)
INFO - root - 2017-12-06 06:05:28.800302: step 24350, loss = 0.84, batch loss = 0.63 (34.3 examples/sec; 0.233 sec/batch; 19h:56m:44s remains)
INFO - root - 2017-12-06 06:05:31.118418: step 24360, loss = 0.80, batch loss = 0.59 (36.0 examples/sec; 0.222 sec/batch; 19h:01m:04s remains)
INFO - root - 2017-12-06 06:05:33.429063: step 24370, loss = 0.86, batch loss = 0.65 (35.1 examples/sec; 0.228 sec/batch; 19h:30m:11s remains)
INFO - root - 2017-12-06 06:05:35.725831: step 24380, loss = 0.82, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:24s remains)
INFO - root - 2017-12-06 06:05:38.026447: step 24390, loss = 0.88, batch loss = 0.67 (33.7 examples/sec; 0.237 sec/batch; 20h:18m:47s remains)
INFO - root - 2017-12-06 06:05:40.333478: step 24400, loss = 0.74, batch loss = 0.52 (34.5 examples/sec; 0.232 sec/batch; 19h:52m:17s remains)
2017-12-06 06:05:40.693164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060301777 -0.084219843 -0.10029737 -0.13384977 -0.1501465 -0.1542044 -0.14092536 -0.12082708 -0.1220675 -0.1211081 -0.12202807 -0.13138981 -0.14128174 -0.14749512 -0.13779509][-0.10448088 -0.14186995 -0.16422795 -0.18925519 -0.20844394 -0.23072414 -0.23475024 -0.19267583 -0.16481026 -0.15700844 -0.1465079 -0.1354554 -0.13994169 -0.13582242 -0.13674584][-0.15560886 -0.2076637 -0.20627123 -0.18778032 -0.19889861 -0.23693132 -0.27842844 -0.23374563 -0.17910416 -0.1581668 -0.13834018 -0.12199804 -0.1130711 -0.11464615 -0.1554656][-0.19064993 -0.26303297 -0.22055827 -0.16078085 -0.15010133 -0.19195965 -0.2692973 -0.25571144 -0.1992029 -0.19027 -0.18027803 -0.15229416 -0.12637118 -0.12593584 -0.16763034][-0.18186274 -0.23496136 -0.12561841 0.017910972 0.078230038 0.0235629 -0.099990681 -0.14236689 -0.13774151 -0.1606323 -0.13895538 -0.1329346 -0.1112073 -0.12849486 -0.16657612][-0.097160265 -0.081219658 0.10686906 0.35430604 0.53063363 0.53084594 0.36123782 0.20432003 0.082025155 -0.017237775 -0.035637222 -0.079883017 -0.086288556 -0.10000354 -0.12696835][-0.011385716 0.10092114 0.38180113 0.70733577 0.96113241 0.98377252 0.76313591 0.50775415 0.29418164 0.12348598 0.057195917 -0.021522306 -0.068993047 -0.083786651 -0.095185488][0.098069668 0.24197586 0.51613426 0.88699013 1.1868153 1.2312921 1.0182834 0.73324466 0.50733751 0.29504603 0.15371443 0.053744137 -0.03723529 -0.055001646 -0.069550276][0.13008073 0.30822253 0.59204352 0.92563426 1.2070364 1.2931167 1.1591487 0.9030996 0.63256627 0.39539754 0.19729571 0.10175875 0.05650945 0.020137131 -0.036389876][-0.017100185 0.13845745 0.36674023 0.66324991 0.94019473 1.0430809 0.9701761 0.76466447 0.53753251 0.33114856 0.14008416 0.063730374 0.052905574 0.046935037 -0.00082618743][-0.24535467 -0.18079519 -0.0044144839 0.22259723 0.43786085 0.57478982 0.58139014 0.45260459 0.30224961 0.17490025 0.072436184 0.0489721 0.091231436 0.11219801 0.065836996][-0.37037992 -0.34669894 -0.24121188 -0.090378836 0.10036391 0.21708171 0.25981647 0.18428771 0.1129944 0.038056865 -0.0083754212 0.0061601773 0.061615229 0.10956271 0.099010453][-0.53006226 -0.53368181 -0.4840039 -0.41222245 -0.34219611 -0.22953247 -0.12216581 -0.083432876 -0.042228058 -0.055642251 -0.061214432 -0.040586736 0.011183172 0.044322088 0.058889613][-0.6254549 -0.70146734 -0.75826812 -0.77204478 -0.72736323 -0.59429717 -0.43913364 -0.30312395 -0.20105232 -0.15952632 -0.12485997 -0.11621364 -0.085940018 -0.056651626 -0.03406572][-0.50788409 -0.58650559 -0.67500943 -0.74287641 -0.71011084 -0.5883401 -0.41704673 -0.26116365 -0.17945537 -0.12688263 -0.097195208 -0.093808167 -0.10791007 -0.12381762 -0.10764018]]...]
INFO - root - 2017-12-06 06:05:43.005278: step 24410, loss = 0.82, batch loss = 0.61 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:49s remains)
INFO - root - 2017-12-06 06:05:45.373693: step 24420, loss = 0.83, batch loss = 0.62 (34.7 examples/sec; 0.230 sec/batch; 19h:43m:16s remains)
INFO - root - 2017-12-06 06:05:47.674475: step 24430, loss = 0.84, batch loss = 0.63 (36.2 examples/sec; 0.221 sec/batch; 18h:55m:08s remains)
INFO - root - 2017-12-06 06:05:50.012534: step 24440, loss = 0.90, batch loss = 0.69 (35.3 examples/sec; 0.227 sec/batch; 19h:24m:40s remains)
INFO - root - 2017-12-06 06:05:52.306833: step 24450, loss = 0.77, batch loss = 0.56 (31.9 examples/sec; 0.251 sec/batch; 21h:27m:11s remains)
INFO - root - 2017-12-06 06:05:54.631525: step 24460, loss = 0.78, batch loss = 0.56 (35.7 examples/sec; 0.224 sec/batch; 19h:09m:31s remains)
INFO - root - 2017-12-06 06:05:56.927717: step 24470, loss = 0.74, batch loss = 0.53 (36.3 examples/sec; 0.221 sec/batch; 18h:52m:36s remains)
INFO - root - 2017-12-06 06:05:59.246036: step 24480, loss = 0.83, batch loss = 0.61 (34.9 examples/sec; 0.229 sec/batch; 19h:36m:49s remains)
INFO - root - 2017-12-06 06:06:01.560189: step 24490, loss = 0.78, batch loss = 0.56 (35.0 examples/sec; 0.229 sec/batch; 19h:34m:48s remains)
INFO - root - 2017-12-06 06:06:03.878352: step 24500, loss = 0.86, batch loss = 0.65 (34.8 examples/sec; 0.230 sec/batch; 19h:39m:06s remains)
2017-12-06 06:06:04.265441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1184447 -1.1102844 -1.1010267 -1.0484309 -0.93240112 -0.87003475 -0.82233685 -0.8285085 -0.81926423 -0.82486814 -0.83787018 -0.80215788 -0.63402867 -0.34487343 0.018123709][-0.69345087 -0.76976478 -0.82157654 -0.9198221 -0.92863452 -0.94665086 -0.96132457 -0.99213505 -1.011958 -1.0722525 -1.1003655 -1.0951596 -1.0080086 -0.86229658 -0.60300386][0.25158155 0.072683975 -0.085563853 -0.19039729 -0.25514764 -0.41307384 -0.56795788 -0.70115542 -0.7733978 -0.8651787 -0.96104878 -1.0201942 -0.99183041 -0.88991469 -0.69096565][0.81280023 0.45655185 0.23197313 0.14589734 0.12458655 0.13244411 0.04486385 -0.16580929 -0.34959316 -0.54692578 -0.689707 -0.80729038 -0.8503381 -0.82069671 -0.70347106][0.8472389 0.50885981 0.27000034 0.15107922 0.24922465 0.4200325 0.49973214 0.39214283 0.10821752 -0.1579769 -0.36728466 -0.48718578 -0.5261429 -0.52428 -0.52288675][0.69716805 0.40820265 0.31092143 0.23696847 0.37030113 0.62633687 0.83103693 0.87351012 0.71409929 0.4758153 0.17929567 -0.0056394637 -0.10595255 -0.1713686 -0.2799316][0.48853856 0.26950157 0.35942984 0.49328625 0.6868757 0.96531016 1.161727 1.1671206 1.0565696 0.912661 0.7236495 0.56163192 0.39407027 0.27201802 0.10613777][0.11848302 -0.06253773 0.12563546 0.44518679 0.74165255 0.93937415 1.1003441 1.1868612 1.1041775 1.0189217 0.923765 0.8769787 0.76157433 0.53123349 0.26754081][-0.3620078 -0.4257217 -0.29107279 -0.022261441 0.32623893 0.49975121 0.59962487 0.73886049 0.85290772 0.9522149 0.93267447 0.91344661 0.84869051 0.73468417 0.58131474][-0.80082345 -0.72764236 -0.64170736 -0.52556086 -0.33290857 -0.17896646 -0.038022116 0.11149557 0.29273158 0.46958 0.57416731 0.693905 0.73118395 0.70071948 0.64912081][-1.1028278 -1.0625352 -0.9888401 -0.93651193 -0.82703328 -0.75053293 -0.65063959 -0.47812575 -0.25607976 -0.086922079 0.02469372 0.15413189 0.27025408 0.35454232 0.42536628][-1.1691555 -1.1894078 -1.2196759 -1.2725302 -1.2617292 -1.206643 -1.0826024 -0.90537304 -0.73307687 -0.60385114 -0.51167661 -0.4459672 -0.40579063 -0.37859619 -0.26776803][-1.0475715 -1.1157048 -1.196399 -1.3037164 -1.4100093 -1.4030371 -1.3216733 -1.1735125 -1.0172557 -0.920722 -0.8734805 -0.84151208 -0.82972491 -0.85864842 -0.83497876][-0.95151889 -0.99672705 -1.0191923 -1.1320165 -1.2660209 -1.3519062 -1.3662287 -1.318967 -1.2337016 -1.1369431 -1.0523587 -1.0225389 -1.047411 -1.0867565 -1.0995992][-0.77151096 -0.84143025 -0.91392738 -0.99214977 -1.0650924 -1.1149573 -1.1238849 -1.122779 -1.1061789 -1.051075 -0.98218113 -0.93596834 -0.90688562 -0.92852145 -0.97736835]]...]
INFO - root - 2017-12-06 06:06:06.577933: step 24510, loss = 0.76, batch loss = 0.55 (33.8 examples/sec; 0.237 sec/batch; 20h:15m:03s remains)
INFO - root - 2017-12-06 06:06:08.850265: step 24520, loss = 0.84, batch loss = 0.63 (34.9 examples/sec; 0.229 sec/batch; 19h:36m:40s remains)
INFO - root - 2017-12-06 06:06:11.185038: step 24530, loss = 0.88, batch loss = 0.67 (36.1 examples/sec; 0.222 sec/batch; 18h:58m:25s remains)
INFO - root - 2017-12-06 06:06:13.462835: step 24540, loss = 0.67, batch loss = 0.46 (35.5 examples/sec; 0.225 sec/batch; 19h:15m:27s remains)
INFO - root - 2017-12-06 06:06:15.752107: step 24550, loss = 0.86, batch loss = 0.64 (34.0 examples/sec; 0.235 sec/batch; 20h:06m:07s remains)
INFO - root - 2017-12-06 06:06:18.015325: step 24560, loss = 0.88, batch loss = 0.66 (34.9 examples/sec; 0.229 sec/batch; 19h:35m:04s remains)
INFO - root - 2017-12-06 06:06:20.313731: step 24570, loss = 0.79, batch loss = 0.58 (32.5 examples/sec; 0.246 sec/batch; 21h:01m:55s remains)
INFO - root - 2017-12-06 06:06:22.566064: step 24580, loss = 0.72, batch loss = 0.51 (34.1 examples/sec; 0.235 sec/batch; 20h:03m:51s remains)
INFO - root - 2017-12-06 06:06:24.965519: step 24590, loss = 0.79, batch loss = 0.57 (34.4 examples/sec; 0.232 sec/batch; 19h:52m:40s remains)
INFO - root - 2017-12-06 06:06:27.304139: step 24600, loss = 0.73, batch loss = 0.52 (36.3 examples/sec; 0.220 sec/batch; 18h:50m:33s remains)
2017-12-06 06:06:27.672283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057136077 -0.057294831 -0.057627939 -0.05779973 -0.057744436 -0.057529103 -0.057214186 -0.056859411 -0.056523792 -0.056315079 -0.056275543 -0.056324359 -0.056397114 -0.056477714 -0.056811005][-0.057457786 -0.057641119 -0.058047734 -0.058227092 -0.058091357 -0.05768564 -0.057133917 -0.056577139 -0.05606921 -0.055773683 -0.055743296 -0.055866025 -0.05603198 -0.056213215 -0.056614634][-0.057789404 -0.057925634 -0.058392413 -0.058536846 -0.058266815 -0.057601772 -0.056724161 -0.055843446 -0.055118524 -0.05471674 -0.054712877 -0.054962508 -0.055319522 -0.05571717 -0.0562732][-0.057958256 -0.058090072 -0.058625817 -0.058721222 -0.058274195 -0.057281855 -0.055997446 -0.054727878 -0.053680178 -0.053032838 -0.052989274 -0.05338528 -0.054033808 -0.054725163 -0.055583406][-0.057944026 -0.058092784 -0.058578942 -0.058535382 -0.057821069 -0.05645984 -0.054730728 -0.053016037 -0.051506005 -0.050549645 -0.050353035 -0.050841931 -0.0517798 -0.052867267 -0.05419803][-0.057910975 -0.057930749 -0.0582399 -0.057939589 -0.056878809 -0.055130951 -0.052934505 -0.050758388 -0.048851475 -0.047608316 -0.047241472 -0.047773156 -0.048957136 -0.050437458 -0.052369617][-0.057839371 -0.057632778 -0.057797607 -0.057281259 -0.05594372 -0.053848285 -0.051222522 -0.04867946 -0.046509515 -0.044990133 -0.044450022 -0.044999316 -0.046496209 -0.048385032 -0.050801247][-0.057687271 -0.057310682 -0.057418849 -0.056865953 -0.055472933 -0.053244174 -0.050435625 -0.047704272 -0.04534341 -0.043609213 -0.042863846 -0.043342873 -0.044919129 -0.047062684 -0.049823947][-0.057481311 -0.057001557 -0.057076938 -0.056569673 -0.055287059 -0.053226594 -0.05061942 -0.048008639 -0.04560275 -0.043809976 -0.042889547 -0.04304947 -0.044331223 -0.046413597 -0.049230758][-0.057180196 -0.056661133 -0.056680858 -0.056278888 -0.055268969 -0.053590328 -0.051395547 -0.0491502 -0.047068972 -0.0454481 -0.044447016 -0.044327632 -0.045217514 -0.046941437 -0.049534217][-0.05706425 -0.056443438 -0.05642622 -0.056157824 -0.055460714 -0.054237414 -0.052618135 -0.050926514 -0.0494457 -0.048262674 -0.047462627 -0.047233094 -0.047804158 -0.049076311 -0.051143169][-0.057037339 -0.056401603 -0.056372292 -0.056208149 -0.055767126 -0.054983851 -0.053941522 -0.052842729 -0.051902018 -0.051165275 -0.050618175 -0.050393891 -0.050712496 -0.051520523 -0.0530351][-0.0570402 -0.056442089 -0.056430612 -0.056359384 -0.056137361 -0.055734958 -0.055181317 -0.054581892 -0.054065067 -0.053669143 -0.053353686 -0.053170171 -0.053263471 -0.053653263 -0.054600723][-0.057084437 -0.056501649 -0.056509405 -0.056492563 -0.056407847 -0.056254119 -0.056026753 -0.055755556 -0.055496275 -0.055278856 -0.05509926 -0.054999568 -0.05501562 -0.055129237 -0.055685174][-0.057248149 -0.05666231 -0.056676693 -0.056682639 -0.056661706 -0.056623496 -0.056568768 -0.056489725 -0.056422465 -0.056326393 -0.056240205 -0.056171261 -0.056123454 -0.056139648 -0.056492247]]...]
INFO - root - 2017-12-06 06:06:30.797700: step 24610, loss = 0.84, batch loss = 0.62 (24.5 examples/sec; 0.326 sec/batch; 27h:52m:56s remains)
INFO - root - 2017-12-06 06:06:33.872890: step 24620, loss = 0.74, batch loss = 0.52 (21.3 examples/sec; 0.376 sec/batch; 32h:09m:25s remains)
INFO - root - 2017-12-06 06:06:37.218199: step 24630, loss = 0.82, batch loss = 0.61 (22.9 examples/sec; 0.350 sec/batch; 29h:54m:32s remains)
INFO - root - 2017-12-06 06:06:40.429223: step 24640, loss = 0.89, batch loss = 0.68 (26.1 examples/sec; 0.307 sec/batch; 26h:12m:52s remains)
INFO - root - 2017-12-06 06:06:43.854770: step 24650, loss = 0.85, batch loss = 0.64 (26.5 examples/sec; 0.302 sec/batch; 25h:48m:08s remains)
INFO - root - 2017-12-06 06:06:47.225327: step 24660, loss = 0.87, batch loss = 0.66 (21.9 examples/sec; 0.365 sec/batch; 31h:10m:44s remains)
INFO - root - 2017-12-06 06:06:50.576362: step 24670, loss = 0.78, batch loss = 0.57 (26.0 examples/sec; 0.307 sec/batch; 26h:17m:36s remains)
INFO - root - 2017-12-06 06:06:53.985386: step 24680, loss = 0.80, batch loss = 0.59 (26.0 examples/sec; 0.308 sec/batch; 26h:19m:53s remains)
INFO - root - 2017-12-06 06:06:57.475159: step 24690, loss = 0.80, batch loss = 0.59 (23.6 examples/sec; 0.339 sec/batch; 28h:58m:17s remains)
INFO - root - 2017-12-06 06:07:01.119608: step 24700, loss = 0.83, batch loss = 0.62 (22.7 examples/sec; 0.353 sec/batch; 30h:08m:21s remains)
2017-12-06 06:07:01.740434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.45335764 -0.46336532 -0.43770456 -0.41516173 -0.40727174 -0.38696563 -0.37437803 -0.37486702 -0.37420326 -0.36821538 -0.36316895 -0.37752885 -0.39333534 -0.41724885 -0.42641544][-0.48724926 -0.49717033 -0.4943046 -0.49109125 -0.48531491 -0.47080749 -0.45239937 -0.4320665 -0.40154815 -0.37664163 -0.34758711 -0.33481887 -0.32121685 -0.32537556 -0.32760081][-0.1595111 -0.1594201 -0.15967152 -0.16092271 -0.16508308 -0.16765302 -0.16685018 -0.16458985 -0.15713221 -0.1510421 -0.14782356 -0.14065015 -0.12622094 -0.11848086 -0.11019053][-0.099233851 -0.099362873 -0.099707156 -0.10038106 -0.10609538 -0.10210317 -0.09564811 -0.093411013 -0.095679104 -0.10223629 -0.12096256 -0.12617415 -0.12169056 -0.1105029 -0.099083319][-0.10253734 -0.10998902 -0.12241282 -0.13516077 -0.15395221 -0.15184787 -0.13340111 -0.10206487 -0.079220906 -0.072480559 -0.085158944 -0.099399216 -0.10345007 -0.10335699 -0.098529883][-0.10420702 -0.11416929 -0.12700872 -0.14056854 -0.15732226 -0.14796261 -0.11726521 -0.07009311 -0.032384537 -0.025826424 -0.052932605 -0.082463846 -0.092486329 -0.092296809 -0.089156091][-0.12014119 -0.12928429 -0.13563506 -0.14734431 -0.1642895 -0.14915641 -0.11643918 -0.074243076 -0.04706154 -0.036071457 -0.0568082 -0.082629934 -0.092492372 -0.094341882 -0.090515241][-0.11543763 -0.12159657 -0.12757194 -0.1316871 -0.13929467 -0.12800124 -0.11250126 -0.098505236 -0.090496443 -0.087328225 -0.10181759 -0.10945312 -0.10514797 -0.10087451 -0.094327822][-0.10208737 -0.099419519 -0.098466434 -0.095786817 -0.097496405 -0.090078 -0.091690168 -0.10628043 -0.11766149 -0.12171883 -0.1293088 -0.1265154 -0.11522456 -0.10563573 -0.10406447][-0.089928806 -0.082599461 -0.07335376 -0.065427206 -0.071053825 -0.065533608 -0.06370984 -0.079689406 -0.09546382 -0.10648397 -0.11311416 -0.110944 -0.11449827 -0.11294045 -0.10839913][-0.080229737 -0.076884881 -0.079121284 -0.0721054 -0.0774994 -0.086778566 -0.095420413 -0.094360605 -0.095231429 -0.099839047 -0.10128997 -0.097978048 -0.1043324 -0.10968042 -0.111799][-0.089010768 -0.077711716 -0.075164914 -0.071341679 -0.070017934 -0.072324365 -0.077155456 -0.074267775 -0.072306886 -0.079771809 -0.085414439 -0.088744581 -0.092446811 -0.09551128 -0.098556191][-0.10749799 -0.10527235 -0.100283 -0.09877345 -0.10063893 -0.10028712 -0.0996341 -0.099423267 -0.099324852 -0.099746577 -0.10049781 -0.09996862 -0.098432735 -0.098434791 -0.098640516][-0.10917775 -0.10853549 -0.10757683 -0.10164878 -0.10020889 -0.10001166 -0.098820664 -0.098995931 -0.098807819 -0.097804859 -0.096646219 -0.097022444 -0.095835157 -0.096481085 -0.09730006][-0.10653806 -0.1078351 -0.10784543 -0.10290389 -0.097085826 -0.096961692 -0.0946277 -0.094054617 -0.094946526 -0.096611217 -0.096293271 -0.097852848 -0.099303149 -0.098922379 -0.0992718]]...]
INFO - root - 2017-12-06 06:07:05.405181: step 24710, loss = 0.92, batch loss = 0.71 (21.8 examples/sec; 0.367 sec/batch; 31h:24m:28s remains)
INFO - root - 2017-12-06 06:07:09.254810: step 24720, loss = 0.76, batch loss = 0.55 (19.2 examples/sec; 0.417 sec/batch; 35h:36m:57s remains)
INFO - root - 2017-12-06 06:07:13.028827: step 24730, loss = 0.74, batch loss = 0.52 (21.2 examples/sec; 0.377 sec/batch; 32h:13m:07s remains)
INFO - root - 2017-12-06 06:07:15.860766: step 24740, loss = 0.93, batch loss = 0.72 (26.5 examples/sec; 0.302 sec/batch; 25h:48m:14s remains)
INFO - root - 2017-12-06 06:07:18.683429: step 24750, loss = 0.79, batch loss = 0.58 (25.7 examples/sec; 0.312 sec/batch; 26h:39m:03s remains)
INFO - root - 2017-12-06 06:07:21.511547: step 24760, loss = 0.82, batch loss = 0.61 (30.7 examples/sec; 0.261 sec/batch; 22h:16m:49s remains)
INFO - root - 2017-12-06 06:07:24.488267: step 24770, loss = 0.79, batch loss = 0.57 (25.9 examples/sec; 0.308 sec/batch; 26h:21m:15s remains)
INFO - root - 2017-12-06 06:07:27.658022: step 24780, loss = 0.83, batch loss = 0.61 (23.9 examples/sec; 0.335 sec/batch; 28h:37m:00s remains)
INFO - root - 2017-12-06 06:07:30.983805: step 24790, loss = 0.80, batch loss = 0.58 (23.9 examples/sec; 0.335 sec/batch; 28h:37m:38s remains)
INFO - root - 2017-12-06 06:07:34.358166: step 24800, loss = 0.80, batch loss = 0.58 (23.5 examples/sec; 0.341 sec/batch; 29h:09m:24s remains)
2017-12-06 06:07:34.761976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045382462 -0.046221342 -0.046456017 -0.05699037 -0.071909063 -0.0909273 -0.10847338 -0.11714231 -0.12292333 -0.12772189 -0.12813734 -0.11566538 -0.10820322 -0.11175601 -0.11697397][-0.043081544 -0.04155387 -0.050397884 -0.063678317 -0.09938132 -0.14286257 -0.19506425 -0.2526255 -0.3038376 -0.33964318 -0.36420164 -0.38768914 -0.41159078 -0.41346985 -0.41099179][-0.16570346 -0.19524932 -0.22359632 -0.24402764 -0.27498913 -0.3109982 -0.3564879 -0.38637277 -0.42252845 -0.45958579 -0.47581166 -0.48622474 -0.48716316 -0.47964537 -0.45843744][-0.31285006 -0.30907005 -0.33045268 -0.37780383 -0.42015752 -0.37417698 -0.31900248 -0.25575376 -0.17370999 -0.09230347 -0.032241534 -0.036998868 -0.048189558 -0.0587087 -0.068386406][-0.25621915 -0.26513645 -0.25096256 -0.18508159 -0.14724518 -0.1439404 -0.13300788 -0.12221587 -0.11138906 -0.10070285 -0.081498548 -0.0726536 -0.084014609 -0.10080113 -0.10772771][-0.18299741 -0.16009754 -0.12465692 -0.10734064 -0.10249923 -0.11677278 -0.1417681 -0.17335528 -0.20154157 -0.20189527 -0.20352522 -0.21316451 -0.19986604 -0.18430844 -0.16290419][-0.29883033 -0.29393795 -0.28134245 -0.23947896 -0.19385457 -0.14209898 -0.088283345 -0.044958077 -0.01416529 -0.0058831722 0.0034750029 0.011675738 0.026873112 0.043074563 0.056399137][0.060334042 0.049889803 0.038470283 0.029450372 0.008303389 -0.00056453049 -0.00530643 0.011413395 0.020432845 0.020514995 0.029375188 0.03896445 0.047247052 0.054663196 0.071262553][0.11151011 0.12870236 0.14108832 0.11268675 0.074199721 0.021157749 -0.032605309 -0.068839625 -0.10397451 -0.10054223 -0.089379542 -0.078989334 -0.089146324 -0.095033795 -0.099839874][0.062510446 0.044053733 0.0089031085 -0.022286475 -0.040375549 -0.038679115 -0.034070265 -0.024936505 -0.0192739 0.003588222 0.034275874 0.092241272 0.12532195 0.14164263 0.15369444][0.22658068 0.24726823 0.25605839 0.2506707 0.24524266 0.22939262 0.2175554 0.210814 0.20752153 0.19594768 0.16826269 0.14387381 0.10332668 0.060371578 0.012416437][0.36828178 0.41269469 0.42007655 0.41989511 0.4095155 0.35798225 0.32315266 0.30071107 0.28406468 0.2446574 0.21528864 0.1806505 0.14152803 0.0756765 0.0031613335][0.29095972 0.31774232 0.31830859 0.28882092 0.26526481 0.22796422 0.18388882 0.13782769 0.1103123 0.054760396 0.0092227161 -0.032838132 -0.084829852 -0.16791016 -0.23903224][0.04091011 0.017871119 0.02040419 0.043136254 0.057165906 0.040068254 0.031073302 0.014357999 -0.018135495 -0.040054165 -0.047662441 -0.086132646 -0.08491043 -0.052038006 -0.017426975][-0.078027561 -0.11674743 -0.17599738 -0.22145435 -0.22023931 -0.14641848 -0.070983447 -0.049689993 -0.030440632 -0.051520571 -0.072735727 -0.066148177 -0.047608152 -0.075847268 -0.069226965]]...]
INFO - root - 2017-12-06 06:07:38.137174: step 24810, loss = 0.78, batch loss = 0.56 (24.0 examples/sec; 0.334 sec/batch; 28h:30m:59s remains)
INFO - root - 2017-12-06 06:07:41.364594: step 24820, loss = 0.81, batch loss = 0.60 (29.3 examples/sec; 0.273 sec/batch; 23h:18m:12s remains)
INFO - root - 2017-12-06 06:07:44.399752: step 24830, loss = 0.80, batch loss = 0.59 (23.8 examples/sec; 0.336 sec/batch; 28h:43m:53s remains)
INFO - root - 2017-12-06 06:07:47.360412: step 24840, loss = 0.85, batch loss = 0.63 (32.1 examples/sec; 0.249 sec/batch; 21h:16m:51s remains)
INFO - root - 2017-12-06 06:07:50.319622: step 24850, loss = 0.75, batch loss = 0.53 (27.3 examples/sec; 0.293 sec/batch; 25h:03m:46s remains)
INFO - root - 2017-12-06 06:07:53.560287: step 24860, loss = 0.82, batch loss = 0.60 (24.0 examples/sec; 0.334 sec/batch; 28h:32m:12s remains)
INFO - root - 2017-12-06 06:07:56.952749: step 24870, loss = 0.83, batch loss = 0.62 (22.2 examples/sec; 0.361 sec/batch; 30h:51m:13s remains)
INFO - root - 2017-12-06 06:08:00.346646: step 24880, loss = 0.72, batch loss = 0.51 (23.9 examples/sec; 0.335 sec/batch; 28h:39m:21s remains)
INFO - root - 2017-12-06 06:08:03.551130: step 24890, loss = 0.78, batch loss = 0.57 (27.0 examples/sec; 0.296 sec/batch; 25h:16m:49s remains)
INFO - root - 2017-12-06 06:08:07.164937: step 24900, loss = 0.82, batch loss = 0.61 (23.2 examples/sec; 0.344 sec/batch; 29h:24m:36s remains)
2017-12-06 06:08:07.575193: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07804288 -0.086246446 -0.096767023 -0.10824364 -0.11912864 -0.127752 -0.13359225 -0.13568552 -0.13453251 -0.13195188 -0.12853329 -0.12579928 -0.1237461 -0.12286438 -0.12394617][-0.074738875 -0.080917083 -0.089365415 -0.098804861 -0.10814148 -0.11592291 -0.12135597 -0.12338263 -0.12225564 -0.11928143 -0.11551087 -0.11220925 -0.10956885 -0.10821646 -0.10847937][-0.072411239 -0.077264458 -0.084168181 -0.0918094 -0.099290386 -0.10561685 -0.11004546 -0.11158729 -0.11046752 -0.10757523 -0.10379617 -0.099922195 -0.09659224 -0.094157457 -0.093206622][-0.069534481 -0.073022448 -0.078412227 -0.084386632 -0.0901868 -0.095034577 -0.098399356 -0.099469811 -0.09839841 -0.095985442 -0.092774853 -0.089312658 -0.086224921 -0.08364781 -0.082256816][-0.067679375 -0.0700829 -0.074159928 -0.078795 -0.083289221 -0.086661547 -0.088744022 -0.089112736 -0.0878711 -0.085648447 -0.082795732 -0.079996318 -0.077592604 -0.075554624 -0.074396752][-0.066942632 -0.068671942 -0.071874678 -0.075537294 -0.07893417 -0.081082292 -0.082017906 -0.08155112 -0.080044337 -0.077862404 -0.075363606 -0.073178023 -0.071502268 -0.070262045 -0.069596626][-0.066576764 -0.068009265 -0.070839822 -0.073961556 -0.076600291 -0.077852271 -0.077856667 -0.07668075 -0.07488852 -0.072867714 -0.070823945 -0.069267169 -0.06831564 -0.067803055 -0.067618616][-0.066640235 -0.0677469 -0.070209995 -0.072965652 -0.075156115 -0.07603614 -0.075588249 -0.074044712 -0.072090015 -0.070150867 -0.0684421 -0.06742841 -0.067024529 -0.0669761 -0.067165121][-0.067084804 -0.067845762 -0.069819406 -0.072120331 -0.073928118 -0.07466317 -0.07414829 -0.072631471 -0.070691079 -0.068785727 -0.067315266 -0.066577509 -0.066504739 -0.066807956 -0.067267552][-0.06743136 -0.067945153 -0.069464862 -0.07126417 -0.072615311 -0.073010951 -0.072459191 -0.071137846 -0.069525845 -0.067917258 -0.066816583 -0.066394947 -0.066527136 -0.06698066 -0.067627966][-0.067740992 -0.067840651 -0.0688696 -0.070119366 -0.070969135 -0.071066283 -0.070518263 -0.069474332 -0.068300761 -0.06721741 -0.066517532 -0.066335149 -0.06661468 -0.067166358 -0.067918822][-0.068084858 -0.067882881 -0.068356156 -0.068974562 -0.069409415 -0.069335014 -0.068873227 -0.0680967 -0.067312196 -0.066658929 -0.066358969 -0.066430546 -0.066825747 -0.067387961 -0.068189092][-0.068292379 -0.067866094 -0.068039693 -0.0681944 -0.068316773 -0.068130106 -0.067780614 -0.067217581 -0.066749007 -0.066431269 -0.066367209 -0.066574529 -0.067035407 -0.067577414 -0.068322323][-0.068367608 -0.06788896 -0.0679756 -0.067905977 -0.067867234 -0.067483567 -0.06711112 -0.066660076 -0.066361062 -0.066240288 -0.066311985 -0.0665738 -0.06703186 -0.067570075 -0.068241917][-0.068241023 -0.067770317 -0.067803927 -0.067688406 -0.067656055 -0.067268908 -0.066983759 -0.0665931 -0.066343248 -0.066306531 -0.066372618 -0.066674165 -0.067057669 -0.0675134 -0.0681363]]...]
INFO - root - 2017-12-06 06:08:11.102653: step 24910, loss = 0.74, batch loss = 0.53 (21.1 examples/sec; 0.380 sec/batch; 32h:27m:14s remains)
INFO - root - 2017-12-06 06:08:14.658895: step 24920, loss = 0.82, batch loss = 0.60 (21.3 examples/sec; 0.376 sec/batch; 32h:08m:46s remains)
INFO - root - 2017-12-06 06:08:18.316072: step 24930, loss = 0.88, batch loss = 0.66 (22.0 examples/sec; 0.364 sec/batch; 31h:07m:12s remains)
INFO - root - 2017-12-06 06:08:21.762283: step 24940, loss = 0.85, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 23h:34m:42s remains)
INFO - root - 2017-12-06 06:08:25.119109: step 24950, loss = 0.79, batch loss = 0.58 (22.9 examples/sec; 0.349 sec/batch; 29h:47m:54s remains)
INFO - root - 2017-12-06 06:08:28.671217: step 24960, loss = 0.83, batch loss = 0.62 (21.6 examples/sec; 0.370 sec/batch; 31h:37m:22s remains)
INFO - root - 2017-12-06 06:08:32.227346: step 24970, loss = 0.77, batch loss = 0.56 (21.0 examples/sec; 0.381 sec/batch; 32h:33m:53s remains)
INFO - root - 2017-12-06 06:08:36.081986: step 24980, loss = 0.77, batch loss = 0.55 (21.4 examples/sec; 0.373 sec/batch; 31h:54m:01s remains)
INFO - root - 2017-12-06 06:08:39.879611: step 24990, loss = 0.95, batch loss = 0.74 (22.1 examples/sec; 0.362 sec/batch; 30h:53m:06s remains)
INFO - root - 2017-12-06 06:08:43.758366: step 25000, loss = 0.90, batch loss = 0.69 (19.2 examples/sec; 0.416 sec/batch; 35h:30m:29s remains)
2017-12-06 06:08:44.285853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088003024 -0.080826052 -0.0742613 -0.069121115 -0.065930076 -0.064718112 -0.064815335 -0.065699011 -0.067203112 -0.068751231 -0.069843605 -0.070334807 -0.070362195 -0.069967471 -0.069243811][-0.083504558 -0.076306678 -0.070374846 -0.066068932 -0.063528657 -0.0626574 -0.062844008 -0.0636867 -0.065090157 -0.066615567 -0.067885689 -0.068778545 -0.069290325 -0.0693681 -0.06890922][-0.077044584 -0.070657425 -0.065884851 -0.062722571 -0.060896315 -0.060259078 -0.060414474 -0.061102845 -0.06237166 -0.0638801 -0.065393247 -0.06675142 -0.067872539 -0.068608843 -0.068538547][-0.070203707 -0.0648275 -0.061408944 -0.059500411 -0.058353338 -0.058010332 -0.058266353 -0.058966484 -0.060034033 -0.061506718 -0.063324332 -0.065165266 -0.066709913 -0.067773968 -0.068005741][-0.064645022 -0.059962276 -0.057627894 -0.056445416 -0.055757739 -0.055645954 -0.055967469 -0.056756131 -0.057599306 -0.059090532 -0.061185651 -0.063515835 -0.065437771 -0.066761822 -0.067309141][-0.060204256 -0.056385856 -0.054756761 -0.053856127 -0.053163305 -0.05287556 -0.05297368 -0.053414635 -0.054182976 -0.055842791 -0.058509044 -0.061388254 -0.063721694 -0.065441944 -0.066436477][-0.057028528 -0.053838778 -0.0526063 -0.051800057 -0.050885573 -0.050164469 -0.049952805 -0.049954157 -0.050723583 -0.052878689 -0.05610279 -0.059380516 -0.062136307 -0.064299211 -0.065660678][-0.056111995 -0.053142816 -0.051775347 -0.050594322 -0.04938617 -0.048475679 -0.048203349 -0.048195615 -0.0488573 -0.051315486 -0.054723389 -0.058025803 -0.061009616 -0.063423641 -0.065100677][-0.056599811 -0.053698149 -0.052026588 -0.050522864 -0.049072959 -0.048255954 -0.048096932 -0.048354257 -0.049344555 -0.051543802 -0.054515406 -0.057518337 -0.060462218 -0.062884226 -0.064724922][-0.057444856 -0.0545836 -0.052703429 -0.051118311 -0.049695488 -0.048903491 -0.04878575 -0.049266312 -0.050586764 -0.052496795 -0.055064861 -0.0577186 -0.060466979 -0.062770166 -0.064629421][-0.058374919 -0.055734631 -0.05377885 -0.052158114 -0.050795626 -0.050037898 -0.050014779 -0.050758895 -0.052144371 -0.053891938 -0.056099936 -0.058423851 -0.060879905 -0.062979653 -0.064748362][-0.059423909 -0.057196412 -0.055508532 -0.05402077 -0.052690454 -0.051986933 -0.052036155 -0.05280574 -0.054022845 -0.055533931 -0.057424244 -0.059434935 -0.061585337 -0.06339585 -0.065049455][-0.060441487 -0.058678746 -0.057417583 -0.056344371 -0.055220891 -0.0547087 -0.054836191 -0.055486079 -0.056489874 -0.057741977 -0.059269283 -0.060882688 -0.062588148 -0.063990839 -0.065339342][-0.061590005 -0.060305983 -0.0594773 -0.058814518 -0.05809968 -0.057866491 -0.058016151 -0.058626242 -0.059367377 -0.060277097 -0.061381705 -0.062502868 -0.06368953 -0.064636931 -0.065614842][-0.062727965 -0.061799958 -0.061322197 -0.060944416 -0.060578346 -0.060576905 -0.06077411 -0.061359949 -0.06194073 -0.062596567 -0.06334126 -0.064051583 -0.064813405 -0.065340295 -0.06596861]]...]
INFO - root - 2017-12-06 06:08:48.248277: step 25010, loss = 0.86, batch loss = 0.65 (19.9 examples/sec; 0.402 sec/batch; 34h:21m:09s remains)
INFO - root - 2017-12-06 06:08:52.094990: step 25020, loss = 0.87, batch loss = 0.65 (20.4 examples/sec; 0.392 sec/batch; 33h:28m:33s remains)
INFO - root - 2017-12-06 06:08:55.954028: step 25030, loss = 0.86, batch loss = 0.65 (19.7 examples/sec; 0.407 sec/batch; 34h:43m:29s remains)
INFO - root - 2017-12-06 06:08:59.113822: step 25040, loss = 0.80, batch loss = 0.58 (25.5 examples/sec; 0.313 sec/batch; 26h:45m:19s remains)
INFO - root - 2017-12-06 06:09:02.644582: step 25050, loss = 0.76, batch loss = 0.55 (22.2 examples/sec; 0.361 sec/batch; 30h:47m:38s remains)
INFO - root - 2017-12-06 06:09:05.784477: step 25060, loss = 0.76, batch loss = 0.54 (34.6 examples/sec; 0.231 sec/batch; 19h:44m:00s remains)
INFO - root - 2017-12-06 06:09:09.197209: step 25070, loss = 0.71, batch loss = 0.50 (22.4 examples/sec; 0.358 sec/batch; 30h:32m:16s remains)
INFO - root - 2017-12-06 06:09:13.011658: step 25080, loss = 0.79, batch loss = 0.58 (20.9 examples/sec; 0.383 sec/batch; 32h:40m:28s remains)
INFO - root - 2017-12-06 06:09:16.699228: step 25090, loss = 0.79, batch loss = 0.58 (22.0 examples/sec; 0.364 sec/batch; 31h:06m:41s remains)
INFO - root - 2017-12-06 06:09:20.398827: step 25100, loss = 0.78, batch loss = 0.57 (23.3 examples/sec; 0.343 sec/batch; 29h:17m:31s remains)
2017-12-06 06:09:20.806449: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19006056 0.12599996 0.18587518 0.30429909 0.41288334 0.450247 0.20375934 0.01327607 -0.18159649 -0.18314593 -0.085046716 0.0041492283 0.030862764 -0.031102598 -0.27397475][0.37696704 0.1400342 -0.06586381 -0.16285665 -0.64419591 -0.7308802 -0.70026845 -0.92018133 -1.0883814 -1.2943565 -1.2087696 -0.96829784 -0.73732787 -0.73170865 -0.82835537][-0.44231516 -0.75866288 -0.89046007 -0.9360289 -1.2177564 -1.4134358 -1.7390302 -1.8224372 -1.8843415 -1.9259245 -1.9299757 -1.7464767 -1.3129598 -1.139743 -1.0047388][-1.5338655 -1.9378586 -2.1026402 -1.8844323 -1.7583528 -1.7613465 -1.8629192 -2.1502397 -2.2969265 -2.2129714 -2.2762249 -2.1492147 -1.7937961 -1.570909 -1.2548771][-2.2359543 -2.6197255 -2.7306938 -2.7393191 -2.6068327 -2.2480354 -2.017282 -2.1459134 -2.2882175 -2.32802 -2.1875212 -2.0400467 -1.8129286 -1.6988688 -1.3759677][-2.126267 -2.449532 -2.5016365 -2.4622729 -2.1394944 -1.7460171 -1.3951902 -1.0388583 -0.8804394 -1.0297273 -1.1829834 -1.2489861 -1.0240556 -1.0146632 -0.79184407][-1.0738736 -1.4028198 -1.4959637 -1.4887463 -1.2138863 -0.78382349 -0.034880575 0.65445268 1.1210713 1.1656207 0.9200747 0.46636575 0.14181909 -0.076371543 0.024673469][0.31859821 -0.040852562 -0.27649856 -0.3026588 -0.026373863 0.55140328 1.0813192 1.8612609 2.5883021 2.9850688 3.0722847 2.5364981 1.9103842 1.3225819 1.0259318][0.86170983 0.62186491 0.40228704 0.22418144 0.42383683 1.0273825 1.9083661 2.7942913 3.3265095 3.7656748 3.7646611 3.3824244 2.8322554 2.1112015 1.5021275][1.0575126 0.79890478 0.50500137 0.47908133 0.46576303 0.72465521 1.3072006 2.086941 2.779686 3.3859789 3.3806853 3.0906413 2.589143 2.0631962 1.3883647][0.71748459 0.50907028 0.27129504 0.079082631 -0.13126758 -0.15059346 -0.12384271 0.18229589 0.62223256 1.1455705 1.4863757 1.7382313 1.5413471 1.2154231 0.77177221][0.11589313 -0.13003691 -0.33692375 -0.40715858 -0.67505896 -0.93101931 -1.1148076 -1.1182966 -1.0957866 -0.86641783 -0.54722792 -0.20708442 -0.036973234 0.092916228 -0.061330117][-0.27285883 -0.49499339 -0.73751563 -1.0747888 -1.341054 -1.5833204 -1.8495039 -1.9627796 -1.8945948 -1.7080567 -1.4942112 -1.1036813 -0.88311827 -0.71310967 -0.56236917][-0.48018846 -0.65046018 -0.8628372 -1.1605024 -1.5779846 -2.0466321 -2.3047652 -2.4025826 -2.3350446 -2.0979517 -1.8140657 -1.4281458 -1.1270785 -0.86400622 -0.70698863][-0.56752944 -0.64347887 -0.79248744 -0.93889272 -1.0932571 -1.3277385 -1.6440527 -1.9006829 -1.9603394 -1.7968327 -1.5852984 -1.3570571 -1.0768473 -0.83059084 -0.65849388]]...]
INFO - root - 2017-12-06 06:09:24.301153: step 25110, loss = 0.81, batch loss = 0.60 (21.3 examples/sec; 0.375 sec/batch; 32h:03m:34s remains)
INFO - root - 2017-12-06 06:09:28.066764: step 25120, loss = 0.82, batch loss = 0.60 (20.6 examples/sec; 0.387 sec/batch; 33h:04m:54s remains)
INFO - root - 2017-12-06 06:09:31.958364: step 25130, loss = 0.86, batch loss = 0.65 (19.9 examples/sec; 0.401 sec/batch; 34h:14m:53s remains)
INFO - root - 2017-12-06 06:09:35.470233: step 25140, loss = 0.81, batch loss = 0.59 (23.9 examples/sec; 0.335 sec/batch; 28h:33m:36s remains)
INFO - root - 2017-12-06 06:09:38.780966: step 25150, loss = 0.81, batch loss = 0.59 (25.4 examples/sec; 0.315 sec/batch; 26h:52m:49s remains)
INFO - root - 2017-12-06 06:09:42.178496: step 25160, loss = 0.83, batch loss = 0.61 (23.9 examples/sec; 0.334 sec/batch; 28h:32m:58s remains)
INFO - root - 2017-12-06 06:09:45.561205: step 25170, loss = 0.93, batch loss = 0.72 (25.7 examples/sec; 0.311 sec/batch; 26h:33m:49s remains)
INFO - root - 2017-12-06 06:09:48.966572: step 25180, loss = 0.73, batch loss = 0.51 (24.1 examples/sec; 0.332 sec/batch; 28h:20m:41s remains)
INFO - root - 2017-12-06 06:09:52.302542: step 25190, loss = 0.72, batch loss = 0.50 (23.1 examples/sec; 0.347 sec/batch; 29h:36m:08s remains)
INFO - root - 2017-12-06 06:09:55.596934: step 25200, loss = 0.81, batch loss = 0.59 (22.6 examples/sec; 0.354 sec/batch; 30h:14m:42s remains)
2017-12-06 06:09:56.022288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057293247 -0.055093292 -0.052494854 -0.050310437 -0.048771769 -0.048226092 -0.048996121 -0.051203717 -0.054237135 -0.055482745 -0.054443888 -0.051758595 -0.047890231 -0.043604892 -0.040411353][-0.057685904 -0.055619519 -0.0533153 -0.0513752 -0.050435849 -0.050839879 -0.052890163 -0.056251995 -0.060106508 -0.06248001 -0.06217929 -0.059275623 -0.054198626 -0.048194423 -0.042980473][-0.058109276 -0.056206014 -0.054335751 -0.052718375 -0.051981959 -0.052877232 -0.056192584 -0.061376154 -0.066428885 -0.07029368 -0.071497716 -0.068965912 -0.06335678 -0.055452991 -0.04806802][-0.058376133 -0.056713954 -0.055089012 -0.053592648 -0.052942134 -0.054026827 -0.057841908 -0.06453561 -0.071694106 -0.077853248 -0.0807144 -0.079413459 -0.073742643 -0.064509287 -0.055157572][-0.058422621 -0.056848552 -0.055279832 -0.053778935 -0.053286463 -0.054446857 -0.058449429 -0.065877855 -0.07479962 -0.083083138 -0.087969162 -0.088355571 -0.083120078 -0.073474631 -0.062794194][-0.058117863 -0.056441359 -0.054859426 -0.053520903 -0.052995667 -0.053994317 -0.0575725 -0.064531825 -0.0735966 -0.083080955 -0.08963047 -0.091436438 -0.087602161 -0.078775033 -0.068129525][-0.057855319 -0.055809584 -0.053897403 -0.052266251 -0.051447079 -0.0516961 -0.054018836 -0.059497125 -0.067749843 -0.07710202 -0.084414363 -0.087519333 -0.085716031 -0.078847826 -0.069430076][-0.057810351 -0.055479959 -0.053271547 -0.051171411 -0.049468685 -0.048372243 -0.048933636 -0.052605491 -0.059383564 -0.067968547 -0.075014919 -0.078680068 -0.078453034 -0.0736399 -0.066004522][-0.057984941 -0.055716503 -0.05335683 -0.050930392 -0.048545398 -0.0464682 -0.046048518 -0.048106227 -0.052963573 -0.059396874 -0.064651459 -0.067511804 -0.067802139 -0.065082595 -0.059261713][-0.058275465 -0.056417555 -0.054290827 -0.051843196 -0.049318571 -0.047045466 -0.045966331 -0.0466939 -0.049458288 -0.053037144 -0.055876613 -0.057169285 -0.057418197 -0.055813838 -0.05175671][-0.058593541 -0.057083543 -0.055364795 -0.053161431 -0.0508046 -0.048614759 -0.047258127 -0.047023725 -0.0479812 -0.049187839 -0.049995936 -0.049976017 -0.049557552 -0.048397511 -0.046007607][-0.058757719 -0.057580777 -0.05629554 -0.054501019 -0.052492034 -0.050517384 -0.049014356 -0.048209127 -0.048150249 -0.04820171 -0.047704831 -0.046745237 -0.045784488 -0.044804722 -0.043435488][-0.059087854 -0.058177005 -0.057335522 -0.056051239 -0.054391541 -0.052578215 -0.051049907 -0.049991835 -0.049396634 -0.048935559 -0.048051104 -0.046747729 -0.045339309 -0.044084802 -0.042700913][-0.059334584 -0.058613203 -0.058140669 -0.057359457 -0.05622286 -0.054900795 -0.053636476 -0.05255881 -0.051854197 -0.051152 -0.050069548 -0.0486088 -0.046810504 -0.045037836 -0.04331113][-0.059593596 -0.058952928 -0.058625989 -0.058102887 -0.057311811 -0.056482166 -0.055599172 -0.054768946 -0.054234929 -0.053661756 -0.052829541 -0.051427931 -0.049692463 -0.047806103 -0.045730721]]...]
INFO - root - 2017-12-06 06:09:59.444097: step 25210, loss = 0.84, batch loss = 0.62 (21.2 examples/sec; 0.378 sec/batch; 32h:16m:34s remains)
INFO - root - 2017-12-06 06:10:03.104384: step 25220, loss = 0.86, batch loss = 0.64 (27.6 examples/sec; 0.290 sec/batch; 24h:46m:32s remains)
INFO - root - 2017-12-06 06:10:06.586371: step 25230, loss = 0.73, batch loss = 0.51 (20.0 examples/sec; 0.400 sec/batch; 34h:07m:13s remains)
INFO - root - 2017-12-06 06:10:10.327633: step 25240, loss = 0.89, batch loss = 0.68 (23.9 examples/sec; 0.335 sec/batch; 28h:34m:59s remains)
INFO - root - 2017-12-06 06:10:13.465474: step 25250, loss = 0.85, batch loss = 0.63 (25.0 examples/sec; 0.321 sec/batch; 27h:21m:20s remains)
INFO - root - 2017-12-06 06:10:16.752741: step 25260, loss = 0.74, batch loss = 0.52 (23.8 examples/sec; 0.336 sec/batch; 28h:38m:36s remains)
INFO - root - 2017-12-06 06:10:20.048187: step 25270, loss = 0.77, batch loss = 0.56 (23.6 examples/sec; 0.339 sec/batch; 28h:56m:54s remains)
INFO - root - 2017-12-06 06:10:23.613642: step 25280, loss = 0.80, batch loss = 0.59 (22.0 examples/sec; 0.364 sec/batch; 31h:02m:57s remains)
INFO - root - 2017-12-06 06:10:27.461883: step 25290, loss = 0.74, batch loss = 0.52 (20.1 examples/sec; 0.398 sec/batch; 33h:56m:49s remains)
INFO - root - 2017-12-06 06:10:31.367405: step 25300, loss = 0.79, batch loss = 0.57 (21.0 examples/sec; 0.380 sec/batch; 32h:25m:53s remains)
2017-12-06 06:10:31.809666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.085277274 -0.086493805 -0.089179724 -0.093563348 -0.099774256 -0.10635612 -0.11089453 -0.11340031 -0.11205176 -0.10428284 -0.096496783 -0.090495847 -0.087658674 -0.086318776 -0.085455827][-0.085920207 -0.08880803 -0.094420627 -0.10260851 -0.11106057 -0.11740135 -0.12222327 -0.12628168 -0.12609182 -0.11560901 -0.10423933 -0.09365689 -0.089473523 -0.087693386 -0.086473532][-0.085763834 -0.090263277 -0.099405035 -0.11012092 -0.1142011 -0.11171079 -0.11112549 -0.11564478 -0.12066784 -0.11703305 -0.11033376 -0.096744172 -0.091130883 -0.088551529 -0.086267188][-0.085451305 -0.090765782 -0.099364772 -0.10464026 -0.098147966 -0.081290752 -0.071716987 -0.0743961 -0.08754871 -0.10401542 -0.11344473 -0.10279898 -0.095826156 -0.091094993 -0.086729422][-0.086368926 -0.093477428 -0.10230953 -0.1060917 -0.096788421 -0.076476991 -0.056867808 -0.042317282 -0.047234967 -0.077287726 -0.10817418 -0.10605702 -0.097225 -0.09457352 -0.090168253][-0.08786387 -0.095552333 -0.10487226 -0.11505111 -0.11919925 -0.10576797 -0.079736896 -0.043958146 -0.025616236 -0.049177919 -0.090578534 -0.09903314 -0.095097691 -0.098590739 -0.10153291][-0.086802617 -0.087218 -0.086582884 -0.096816123 -0.10922368 -0.097437322 -0.061185218 -0.013238482 0.011844732 -0.012245074 -0.062706426 -0.079507552 -0.08312735 -0.098480411 -0.10930556][-0.083437949 -0.0774458 -0.073273778 -0.090964347 -0.11249177 -0.10190521 -0.062958464 -0.027064748 -0.018590741 -0.043839339 -0.080589212 -0.0815371 -0.0856024 -0.09968508 -0.10820192][-0.082491 -0.075143211 -0.072748713 -0.090869531 -0.10359861 -0.0704666 -0.011107363 0.010120459 -0.019026972 -0.06664338 -0.1050712 -0.098554127 -0.097672664 -0.0998622 -0.095345348][-0.086197987 -0.083358936 -0.084212869 -0.093970515 -0.084923066 -0.028302144 0.042779982 0.050147682 0.0016298294 -0.056235455 -0.10085696 -0.10477048 -0.10801713 -0.10131467 -0.08308623][-0.087666959 -0.0876048 -0.0914459 -0.099725761 -0.093280792 -0.05409212 -0.0038216338 0.0065185055 -0.025042899 -0.066321358 -0.10436609 -0.1206992 -0.12462868 -0.11009559 -0.087797321][-0.08591076 -0.086944908 -0.093396269 -0.10532256 -0.11327846 -0.10759836 -0.089734741 -0.082340941 -0.083858162 -0.090957537 -0.1053584 -0.11372651 -0.10350114 -0.08025489 -0.064926624][-0.085409388 -0.085716367 -0.08774019 -0.095246106 -0.1024803 -0.10412438 -0.0988739 -0.092285819 -0.090672739 -0.097055033 -0.10894427 -0.10799359 -0.078054085 -0.042455386 -0.027218297][-0.084438846 -0.083632104 -0.083094046 -0.083707079 -0.084323831 -0.084888123 -0.085435204 -0.087194309 -0.090049185 -0.0949542 -0.10435805 -0.10063061 -0.069566295 -0.033709176 -0.016784869][-0.084225133 -0.0838932 -0.08312995 -0.082611367 -0.082539111 -0.082891695 -0.083601795 -0.086473756 -0.091723472 -0.096910238 -0.10433406 -0.10119904 -0.077493608 -0.041725703 -0.01446452]]...]
INFO - root - 2017-12-06 06:10:35.674438: step 25310, loss = 0.71, batch loss = 0.49 (19.8 examples/sec; 0.403 sec/batch; 34h:25m:28s remains)
INFO - root - 2017-12-06 06:10:38.982408: step 25320, loss = 0.95, batch loss = 0.74 (23.3 examples/sec; 0.343 sec/batch; 29h:16m:51s remains)
INFO - root - 2017-12-06 06:10:42.664551: step 25330, loss = 0.75, batch loss = 0.53 (20.9 examples/sec; 0.382 sec/batch; 32h:36m:17s remains)
INFO - root - 2017-12-06 06:10:46.301886: step 25340, loss = 0.81, batch loss = 0.59 (21.0 examples/sec; 0.381 sec/batch; 32h:28m:28s remains)
INFO - root - 2017-12-06 06:10:50.082520: step 25350, loss = 0.80, batch loss = 0.59 (20.1 examples/sec; 0.398 sec/batch; 33h:55m:04s remains)
INFO - root - 2017-12-06 06:10:54.057597: step 25360, loss = 0.83, batch loss = 0.62 (21.6 examples/sec; 0.371 sec/batch; 31h:39m:25s remains)
INFO - root - 2017-12-06 06:10:57.795545: step 25370, loss = 0.79, batch loss = 0.57 (20.2 examples/sec; 0.396 sec/batch; 33h:48m:34s remains)
INFO - root - 2017-12-06 06:11:01.657144: step 25380, loss = 0.71, batch loss = 0.50 (21.6 examples/sec; 0.370 sec/batch; 31h:33m:43s remains)
INFO - root - 2017-12-06 06:11:05.590115: step 25390, loss = 0.92, batch loss = 0.71 (20.5 examples/sec; 0.390 sec/batch; 33h:14m:25s remains)
INFO - root - 2017-12-06 06:11:08.901876: step 25400, loss = 0.85, batch loss = 0.64 (22.6 examples/sec; 0.355 sec/batch; 30h:15m:41s remains)
2017-12-06 06:11:09.358854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1929722 -1.2753099 -1.2743741 -1.4613048 -1.8381503 -2.1026235 -2.2181439 -2.1842623 -2.000041 -1.7807428 -1.5883713 -1.3953656 -1.0878398 -0.793916 -0.61935568][-1.7868779 -2.1563253 -2.4801474 -2.7471814 -2.9272962 -3.0955029 -3.2094312 -3.2680507 -3.1235294 -2.9098253 -2.6535783 -2.2478952 -1.6970344 -1.1954142 -0.90702313][-2.0418828 -2.2684102 -2.5108318 -2.8526759 -3.1908822 -3.2399535 -3.2595606 -3.3806677 -3.4455357 -3.3624754 -3.1523623 -2.7943697 -2.1346092 -1.5276366 -1.1777898][-1.7048796 -1.7907073 -1.7561482 -1.8327153 -2.1827154 -2.3133507 -2.3620749 -2.4440684 -2.6173482 -2.9789333 -3.1011147 -2.8963141 -2.3185077 -1.6404207 -1.0907522][-0.70264292 -0.73749226 -0.62648475 -0.43500096 -0.43044734 -0.28195527 -0.3423053 -0.560902 -1.0474374 -1.6820786 -2.22082 -2.5287266 -2.1515713 -1.4634217 -0.89786965][-0.25481817 -0.0056621134 0.46171629 0.79829085 1.1224275 1.5312271 1.6844934 1.6174986 1.0406363 0.22368577 -0.51776797 -1.1134539 -1.3633217 -1.1740842 -0.671609][-0.37258834 0.088824287 0.80698675 1.4511395 2.0698318 2.5969949 2.9913497 3.5086899 3.4740915 2.5617123 1.41379 0.58886546 -0.21592146 -0.51984847 -0.20661291][-0.38347262 -0.071882509 0.49557531 1.1668615 1.9647781 2.6037416 2.9281774 3.3053994 3.699842 3.5140247 2.8051195 1.8679271 1.0037302 0.4178856 0.31897327][-0.45863119 -0.23207761 0.13609901 0.61043823 1.2559004 1.9153446 2.4554968 2.7021089 2.9161644 2.8862123 2.3333049 1.7387918 1.1154697 0.62956774 0.54143894][-0.34836251 -0.36055374 -0.31134629 -0.053260379 0.35250849 0.86747253 1.5274464 1.8230386 1.9991065 1.9164981 1.5519273 1.2391765 0.43007749 0.035031304 0.059553117][-0.38683936 -0.46869138 -0.56603765 -0.68063557 -0.54484254 -0.24748166 0.046929777 0.43071669 0.41304249 0.28754407 0.19811237 0.039107978 -0.30105144 -0.42641035 -0.42246714][-0.73092979 -0.73521423 -0.82683814 -1.0083203 -1.1818815 -1.3397164 -1.3893566 -1.3914433 -1.5463594 -1.5070348 -1.5111942 -1.1958016 -0.957349 -0.88301516 -0.75056446][-0.88357276 -1.0579022 -1.2249948 -1.3621967 -1.4339082 -1.4920793 -1.6155953 -1.9968989 -2.278091 -2.2315874 -2.1896324 -1.9368454 -1.4998312 -1.1179112 -0.737437][-0.66229874 -0.86669528 -1.1488342 -1.4552367 -1.7361064 -1.9450082 -2.0752611 -1.9892015 -1.9704484 -1.973554 -1.6945058 -1.3020109 -1.1109011 -0.9212386 -0.55708265][-0.55922115 -0.72535616 -0.95501888 -1.1656092 -1.3794994 -1.5748502 -1.7429376 -1.8127275 -1.8958058 -1.5747393 -1.2645448 -1.0225692 -0.73444408 -0.47543976 -0.2919212]]...]
INFO - root - 2017-12-06 06:11:12.671128: step 25410, loss = 0.82, batch loss = 0.60 (23.8 examples/sec; 0.336 sec/batch; 28h:38m:58s remains)
INFO - root - 2017-12-06 06:11:16.070827: step 25420, loss = 0.79, batch loss = 0.58 (21.3 examples/sec; 0.376 sec/batch; 32h:02m:49s remains)
INFO - root - 2017-12-06 06:11:19.490328: step 25430, loss = 0.79, batch loss = 0.57 (22.0 examples/sec; 0.363 sec/batch; 30h:57m:16s remains)
INFO - root - 2017-12-06 06:11:22.960141: step 25440, loss = 0.83, batch loss = 0.61 (34.7 examples/sec; 0.231 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-06 06:11:26.249278: step 25450, loss = 0.77, batch loss = 0.56 (19.5 examples/sec; 0.409 sec/batch; 34h:55m:10s remains)
INFO - root - 2017-12-06 06:11:29.352332: step 25460, loss = 0.86, batch loss = 0.65 (23.6 examples/sec; 0.339 sec/batch; 28h:55m:18s remains)
INFO - root - 2017-12-06 06:11:32.602651: step 25470, loss = 0.73, batch loss = 0.52 (26.4 examples/sec; 0.303 sec/batch; 25h:48m:46s remains)
INFO - root - 2017-12-06 06:11:36.172924: step 25480, loss = 0.82, batch loss = 0.61 (23.0 examples/sec; 0.348 sec/batch; 29h:41m:17s remains)
INFO - root - 2017-12-06 06:11:39.753310: step 25490, loss = 0.80, batch loss = 0.59 (21.3 examples/sec; 0.375 sec/batch; 31h:58m:16s remains)
INFO - root - 2017-12-06 06:11:43.202888: step 25500, loss = 0.77, batch loss = 0.55 (21.1 examples/sec; 0.379 sec/batch; 32h:17m:30s remains)
2017-12-06 06:11:43.721951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05037215 -0.04852004 -0.04802518 -0.052697469 -0.085564613 -0.15341622 -0.23595515 -0.3377561 -0.43775493 -0.48692307 -0.53248894 -0.63313258 -0.73463595 -0.96677196 -1.2202809][-0.050321706 -0.052173648 -0.056997068 -0.06392055 -0.083473042 -0.126071 -0.20198607 -0.32797223 -0.46447837 -0.569509 -0.65663695 -0.70734686 -0.72714531 -0.81488478 -0.89150757][-0.053366322 -0.060351841 -0.069956332 -0.077444643 -0.078642376 -0.089406908 -0.12806408 -0.20063347 -0.29207495 -0.39108598 -0.45164531 -0.48042729 -0.51105297 -0.53877258 -0.58803231][-0.060865633 -0.0659887 -0.071277626 -0.073482066 -0.06330888 -0.06007421 -0.055813253 -0.060001493 -0.0660752 -0.075191937 -0.083943419 -0.12004831 -0.18513623 -0.25015706 -0.30699092][-0.0626635 -0.063166164 -0.0639483 -0.072394878 -0.08140678 -0.090260632 -0.080100641 -0.060565133 -0.017459787 0.031083621 0.080005996 0.11300416 0.1264956 0.12293854 0.075763144][-0.061327852 -0.058835797 -0.056804493 -0.06232471 -0.074701332 -0.088146374 -0.088038176 -0.070822313 -0.029469285 0.02864328 0.10541049 0.15851924 0.2018224 0.26120529 0.29608488][-0.059116796 -0.037011035 -0.0049164444 0.029356748 0.061852582 0.097661458 0.12885717 0.16495249 0.22368684 0.30588388 0.39832008 0.45609996 0.50126 0.51323408 0.50515491][-0.07539171 -0.048543155 0.0019914806 0.085731812 0.18944663 0.31994012 0.4269869 0.5201931 0.58938855 0.64589196 0.67720443 0.7068727 0.73720312 0.77000058 0.77724266][-0.089136191 -0.085468367 -0.057805218 0.019380771 0.1351029 0.30214605 0.42239794 0.52842385 0.58205283 0.5750609 0.5232963 0.47489545 0.43526325 0.42507496 0.41254994][-0.094421245 -0.12565848 -0.13690743 -0.10101625 -0.018269256 0.12899736 0.2626 0.4354333 0.55026889 0.5803116 0.49738243 0.30470386 0.056942947 -0.15624124 -0.28398594][-0.081930183 -0.13730147 -0.18978512 -0.22529599 -0.22403201 -0.13623706 -0.012652241 0.164666 0.30977917 0.42781886 0.4637225 0.35175407 0.14661485 -0.13717249 -0.36886427][-0.19903834 -0.30083507 -0.3790738 -0.4317269 -0.45100957 -0.41295323 -0.33864796 -0.19806322 -0.062025577 0.098729007 0.23558018 0.29138184 0.28809687 0.19088024 0.019855514][-0.19017427 -0.30139634 -0.39530212 -0.46718672 -0.5009461 -0.48648578 -0.44622356 -0.39607447 -0.35708755 -0.27094918 -0.18381195 -0.035276309 0.078660153 0.12709776 0.12097051][-0.1373 -0.2236751 -0.32263675 -0.43221021 -0.52193034 -0.60593283 -0.65934831 -0.70350468 -0.74502218 -0.75930387 -0.74359035 -0.6210078 -0.47649983 -0.3032704 -0.14923915][-0.13857418 -0.19500113 -0.28649113 -0.40075028 -0.51303983 -0.62185419 -0.70875084 -0.8110885 -0.88746393 -0.93113768 -0.9495945 -0.90011072 -0.79726028 -0.62107968 -0.43372211]]...]
INFO - root - 2017-12-06 06:11:47.567440: step 25510, loss = 0.78, batch loss = 0.57 (20.2 examples/sec; 0.396 sec/batch; 33h:45m:00s remains)
INFO - root - 2017-12-06 06:11:51.246992: step 25520, loss = 0.80, batch loss = 0.59 (20.7 examples/sec; 0.387 sec/batch; 33h:01m:35s remains)
INFO - root - 2017-12-06 06:11:54.846841: step 25530, loss = 0.91, batch loss = 0.70 (20.6 examples/sec; 0.388 sec/batch; 33h:06m:31s remains)
INFO - root - 2017-12-06 06:11:58.652792: step 25540, loss = 0.86, batch loss = 0.65 (20.0 examples/sec; 0.400 sec/batch; 34h:08m:38s remains)
INFO - root - 2017-12-06 06:12:02.439652: step 25550, loss = 0.83, batch loss = 0.62 (20.8 examples/sec; 0.384 sec/batch; 32h:44m:00s remains)
INFO - root - 2017-12-06 06:12:06.277753: step 25560, loss = 0.80, batch loss = 0.59 (20.7 examples/sec; 0.386 sec/batch; 32h:56m:59s remains)
INFO - root - 2017-12-06 06:12:09.967790: step 25570, loss = 0.79, batch loss = 0.58 (24.2 examples/sec; 0.330 sec/batch; 28h:07m:51s remains)
INFO - root - 2017-12-06 06:12:13.775351: step 25580, loss = 0.73, batch loss = 0.52 (21.6 examples/sec; 0.371 sec/batch; 31h:36m:37s remains)
INFO - root - 2017-12-06 06:12:17.493993: step 25590, loss = 0.84, batch loss = 0.62 (20.0 examples/sec; 0.401 sec/batch; 34h:10m:24s remains)
INFO - root - 2017-12-06 06:12:21.109497: step 25600, loss = 0.70, batch loss = 0.48 (20.9 examples/sec; 0.382 sec/batch; 32h:34m:30s remains)
2017-12-06 06:12:21.545184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.073696934 -0.078256518 -0.084570453 -0.091631763 -0.09834253 -0.1035789 -0.10615333 -0.10600095 -0.10370459 -0.098579086 -0.092029631 -0.085083343 -0.079135425 -0.074897289 -0.07273832][-0.072670296 -0.076485835 -0.081126407 -0.085419074 -0.089111425 -0.09251564 -0.094821371 -0.096304759 -0.096487716 -0.095049039 -0.090964995 -0.085422881 -0.080549322 -0.075693794 -0.073072873][-0.07151565 -0.073974982 -0.076818131 -0.079186305 -0.0815098 -0.084088087 -0.086100437 -0.087660469 -0.088701427 -0.088723324 -0.086827308 -0.083552755 -0.079940625 -0.075930968 -0.072718531][-0.070441142 -0.071691848 -0.073386759 -0.074638762 -0.07527867 -0.075693063 -0.076597117 -0.078756787 -0.081382476 -0.08248695 -0.081686467 -0.080106087 -0.077404708 -0.074460804 -0.072681949][-0.069634415 -0.070364051 -0.071742237 -0.0723651 -0.071326092 -0.068928562 -0.068294816 -0.070837036 -0.07472419 -0.077541322 -0.078017391 -0.077402383 -0.075699121 -0.07359942 -0.072334014][-0.0689717 -0.069170408 -0.070002139 -0.070267558 -0.068618886 -0.065338239 -0.063522093 -0.065542594 -0.070060968 -0.07359162 -0.074788526 -0.075050212 -0.074303806 -0.07305748 -0.072280914][-0.0681826 -0.067970961 -0.068338022 -0.068300515 -0.066955946 -0.06385792 -0.061191127 -0.061998375 -0.06579335 -0.069700032 -0.071805105 -0.07293281 -0.073191829 -0.072805457 -0.072473451][-0.067228056 -0.066767 -0.066903293 -0.06699916 -0.066705234 -0.065341145 -0.06319797 -0.062171333 -0.063753553 -0.06683971 -0.069231071 -0.070960052 -0.071881533 -0.072101742 -0.072156683][-0.066204041 -0.06562335 -0.065607838 -0.065584071 -0.06547682 -0.065022185 -0.064121731 -0.063370839 -0.064037263 -0.066264234 -0.068457782 -0.070173964 -0.071315378 -0.071895123 -0.072205849][-0.065201506 -0.064563751 -0.064444356 -0.06430617 -0.064116552 -0.0639607 -0.063910685 -0.06412147 -0.064951621 -0.066364832 -0.067785569 -0.06926544 -0.070624888 -0.071571305 -0.072134405][-0.064448252 -0.063760512 -0.063601762 -0.0634173 -0.063124016 -0.062842876 -0.0627227 -0.063205376 -0.064559385 -0.066087633 -0.06746459 -0.0690369 -0.0703767 -0.071450964 -0.072114952][-0.064064167 -0.06335932 -0.063193455 -0.063015237 -0.062774643 -0.062617615 -0.062549859 -0.062927827 -0.063912667 -0.065590665 -0.067521237 -0.069078073 -0.070261359 -0.071367837 -0.072084315][-0.064118735 -0.063384928 -0.063201725 -0.062996559 -0.062778816 -0.062686995 -0.062803388 -0.063436918 -0.06450697 -0.065859027 -0.067181394 -0.068800092 -0.070245266 -0.071297415 -0.072028629][-0.064461023 -0.063750178 -0.063602343 -0.063396841 -0.063198954 -0.063082993 -0.063111506 -0.06357713 -0.06457331 -0.065968074 -0.067423135 -0.068936169 -0.070285007 -0.071270838 -0.071989767][-0.065132633 -0.0644376 -0.064187586 -0.063954026 -0.063746884 -0.063712828 -0.063896105 -0.064380005 -0.065272585 -0.066468135 -0.067831367 -0.069242783 -0.070484623 -0.071435846 -0.072171763]]...]
INFO - root - 2017-12-06 06:12:23.856459: step 25610, loss = 0.80, batch loss = 0.59 (33.5 examples/sec; 0.239 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-06 06:12:27.692547: step 25620, loss = 0.84, batch loss = 0.62 (20.1 examples/sec; 0.398 sec/batch; 33h:57m:37s remains)
INFO - root - 2017-12-06 06:12:31.350150: step 25630, loss = 0.86, batch loss = 0.64 (21.3 examples/sec; 0.376 sec/batch; 32h:04m:55s remains)
INFO - root - 2017-12-06 06:12:35.070261: step 25640, loss = 0.76, batch loss = 0.55 (20.6 examples/sec; 0.389 sec/batch; 33h:09m:18s remains)
INFO - root - 2017-12-06 06:12:38.915021: step 25650, loss = 0.75, batch loss = 0.53 (21.1 examples/sec; 0.379 sec/batch; 32h:16m:19s remains)
INFO - root - 2017-12-06 06:12:42.785713: step 25660, loss = 0.79, batch loss = 0.57 (20.5 examples/sec; 0.390 sec/batch; 33h:14m:28s remains)
INFO - root - 2017-12-06 06:12:45.691259: step 25670, loss = 0.92, batch loss = 0.71 (30.1 examples/sec; 0.266 sec/batch; 22h:38m:08s remains)
INFO - root - 2017-12-06 06:12:49.213188: step 25680, loss = 0.78, batch loss = 0.56 (23.1 examples/sec; 0.347 sec/batch; 29h:33m:28s remains)
INFO - root - 2017-12-06 06:12:52.787350: step 25690, loss = 0.74, batch loss = 0.52 (23.2 examples/sec; 0.345 sec/batch; 29h:24m:32s remains)
INFO - root - 2017-12-06 06:12:56.308577: step 25700, loss = 0.80, batch loss = 0.58 (22.1 examples/sec; 0.362 sec/batch; 30h:49m:11s remains)
2017-12-06 06:12:56.785142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.055641562 -0.0556156 -0.055794176 -0.055996705 -0.056230769 -0.056493517 -0.057128008 -0.058212377 -0.060076345 -0.063065715 -0.067695335 -0.0742178 -0.081942923 -0.09059342 -0.099663749][-0.055812046 -0.055787355 -0.056214739 -0.056726 -0.057221398 -0.0577127 -0.058438502 -0.059484396 -0.061216097 -0.063895062 -0.068245769 -0.074458249 -0.082024992 -0.0906992 -0.099600293][-0.055961162 -0.056004304 -0.056678344 -0.057496041 -0.058346208 -0.059192918 -0.060078979 -0.061147612 -0.0628365 -0.065246314 -0.069088116 -0.07426016 -0.080700025 -0.088426724 -0.0964323][-0.055977173 -0.056028206 -0.056955628 -0.058155064 -0.059380189 -0.060582817 -0.061638623 -0.062749952 -0.064299792 -0.066332996 -0.069316268 -0.073176049 -0.078105047 -0.084302746 -0.091009982][-0.056082346 -0.056239847 -0.05727632 -0.058692191 -0.060238522 -0.061642095 -0.062789194 -0.063842468 -0.06519945 -0.066783562 -0.068870395 -0.071532995 -0.07495933 -0.079477958 -0.084756039][-0.056557864 -0.056688558 -0.057656862 -0.05906697 -0.060666755 -0.062100302 -0.063247241 -0.064247131 -0.065474346 -0.066662692 -0.06804499 -0.069560006 -0.0716542 -0.074728206 -0.078667462][-0.057452396 -0.057379462 -0.058227357 -0.059470672 -0.060904048 -0.06221205 -0.063251622 -0.064110175 -0.065112166 -0.066009328 -0.066805094 -0.067502789 -0.068569869 -0.070504867 -0.07324788][-0.058488965 -0.05818522 -0.058809627 -0.059804469 -0.060931891 -0.061992742 -0.0628136 -0.063523941 -0.064321347 -0.064893976 -0.065216973 -0.065520331 -0.066078015 -0.067227 -0.069019072][-0.059545428 -0.05906117 -0.059428293 -0.060039677 -0.060858194 -0.061563648 -0.06206499 -0.062547751 -0.063113376 -0.063426726 -0.063541368 -0.063772619 -0.06423004 -0.065050967 -0.066307373][-0.060334876 -0.059850477 -0.060048025 -0.0604062 -0.060906772 -0.061264664 -0.061526392 -0.061747424 -0.061957866 -0.062045597 -0.062116254 -0.062423594 -0.06291829 -0.063680433 -0.064697675][-0.060875237 -0.06038069 -0.060643744 -0.060909465 -0.061203495 -0.06132118 -0.061382018 -0.061393529 -0.06129586 -0.061228454 -0.061212771 -0.061586596 -0.062218845 -0.06300357 -0.063931987][-0.061109006 -0.060758315 -0.061109863 -0.061451256 -0.06171643 -0.06170696 -0.061590519 -0.061438859 -0.061200805 -0.060971722 -0.060838703 -0.061200045 -0.06185947 -0.062660292 -0.063573122][-0.061161511 -0.060897909 -0.061354447 -0.061740629 -0.062002353 -0.061968319 -0.061854817 -0.061651174 -0.061387643 -0.061121956 -0.060973078 -0.06125243 -0.061814442 -0.062489748 -0.0633457][-0.061215129 -0.060974136 -0.061512515 -0.061987981 -0.062308788 -0.062303074 -0.062213186 -0.062018938 -0.06179069 -0.061524846 -0.061410978 -0.061597098 -0.061979122 -0.062470324 -0.06318143][-0.061729435 -0.06146187 -0.062015727 -0.062575676 -0.063013025 -0.063134991 -0.063122563 -0.062898844 -0.062609836 -0.06231764 -0.062144257 -0.062217407 -0.062436275 -0.062786594 -0.0633958]]...]
INFO - root - 2017-12-06 06:13:00.412157: step 25710, loss = 0.76, batch loss = 0.55 (21.2 examples/sec; 0.378 sec/batch; 32h:10m:37s remains)
INFO - root - 2017-12-06 06:13:04.300055: step 25720, loss = 0.83, batch loss = 0.62 (21.0 examples/sec; 0.381 sec/batch; 32h:28m:31s remains)
INFO - root - 2017-12-06 06:13:08.200821: step 25730, loss = 0.86, batch loss = 0.65 (19.5 examples/sec; 0.411 sec/batch; 34h:59m:41s remains)
INFO - root - 2017-12-06 06:13:12.035732: step 25740, loss = 0.85, batch loss = 0.64 (19.7 examples/sec; 0.406 sec/batch; 34h:34m:31s remains)
INFO - root - 2017-12-06 06:13:15.879989: step 25750, loss = 0.77, batch loss = 0.56 (21.8 examples/sec; 0.368 sec/batch; 31h:19m:24s remains)
INFO - root - 2017-12-06 06:13:19.783345: step 25760, loss = 0.93, batch loss = 0.72 (20.4 examples/sec; 0.393 sec/batch; 33h:29m:05s remains)
INFO - root - 2017-12-06 06:13:23.648598: step 25770, loss = 0.77, batch loss = 0.56 (19.4 examples/sec; 0.413 sec/batch; 35h:12m:26s remains)
INFO - root - 2017-12-06 06:13:27.506360: step 25780, loss = 0.89, batch loss = 0.67 (18.7 examples/sec; 0.427 sec/batch; 36h:25m:01s remains)
INFO - root - 2017-12-06 06:13:31.480532: step 25790, loss = 0.87, batch loss = 0.66 (21.1 examples/sec; 0.380 sec/batch; 32h:22m:01s remains)
INFO - root - 2017-12-06 06:13:35.390710: step 25800, loss = 0.81, batch loss = 0.60 (20.0 examples/sec; 0.399 sec/batch; 34h:01m:33s remains)
2017-12-06 06:13:35.958321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024106063 -0.024060622 -0.023369521 -0.021965526 -0.019874781 -0.017840251 -0.016689189 -0.015717968 -0.014324278 -0.011881799 -0.0092915446 -0.008177422 -0.0091278329 -0.012655109 -0.019552685][-0.017724074 -0.017769568 -0.017066777 -0.015526809 -0.0135061 -0.011540003 -0.010305464 -0.00912676 -0.0076521635 -0.0051165596 -0.0021073669 -0.00037253648 -0.0012169629 -0.0048606619 -0.012493022][-0.013109148 -0.01291509 -0.012052663 -0.010453291 -0.0087399408 -0.0068641454 -0.0054246634 -0.0039444715 -0.0025849342 -1.7099082e-05 0.0032544881 0.0054274127 0.0048533231 0.000747174 -0.007115595][-0.010213092 -0.0094641373 -0.0086130947 -0.0073782876 -0.0058752894 -0.003922388 -0.0021675974 -0.00023144484 0.0012457371 0.003834419 0.0076849908 0.010522835 0.010125697 0.0052004158 -0.0029530302][-0.0077521652 -0.0073831454 -0.0074515715 -0.0071620867 -0.0060675442 -0.0037798062 -0.001571618 0.00076841563 0.002700448 0.0058843568 0.010588229 0.014279127 0.014225788 0.0088792518 0.00038740039][-0.0055620372 -0.0053851977 -0.0067729577 -0.008040078 -0.007972151 -0.0055161119 -0.002304472 0.00076063722 0.0033789277 0.0072887838 0.012927555 0.017447025 0.017792508 0.012154423 0.0026064068][-0.0031729639 -0.0029718801 -0.0056472272 -0.0086398944 -0.0096860453 -0.007429719 -0.0033370331 0.00054278225 0.0036125779 0.0084270462 0.015025638 0.019985214 0.020565689 0.014699675 0.004051432][-0.00051002204 3.2626092e-05 -0.0034748763 -0.00756938 -0.0096050426 -0.0083253533 -0.005257532 -0.0017483607 0.0023479834 0.00846386 0.016160667 0.021921307 0.023011632 0.017470196 0.0061577111][0.0020451099 0.00270278 -0.0014264509 -0.0060841367 -0.0088047683 -0.0090043917 -0.0082421675 -0.0062811822 -0.0014795661 0.0057696626 0.014455527 0.021460548 0.022998214 0.017848924 0.0065871179][0.0013598204 0.0023176149 -0.0011076853 -0.005082652 -0.0077703148 -0.0092047155 -0.010432817 -0.010461859 -0.0067412108 0.00013391674 0.0087963194 0.016039789 0.018050201 0.013764806 0.0033915937][-0.0035105646 -0.0018531159 -0.0033417344 -0.0052829981 -0.0072264075 -0.0097347051 -0.012427844 -0.01366777 -0.011301704 -0.0061243474 0.00093287975 0.0068996027 0.0084051713 0.0049053729 -0.0041563511][-0.010323785 -0.00836809 -0.0084416792 -0.0087876394 -0.0099939033 -0.012798652 -0.01605919 -0.017887942 -0.016625389 -0.013451532 -0.0086682662 -0.004497312 -0.0036512688 -0.0065363124 -0.013770141][-0.018704057 -0.017045781 -0.016740575 -0.016595565 -0.017487928 -0.019692957 -0.022446208 -0.024023771 -0.023429453 -0.02165845 -0.018834539 -0.016395397 -0.016408645 -0.018749364 -0.024115376][-0.027860157 -0.026868835 -0.02691932 -0.026835069 -0.027658097 -0.029313039 -0.031531185 -0.032854572 -0.032707881 -0.031916898 -0.030173764 -0.028619628 -0.028608005 -0.030328367 -0.034113418][-0.037656911 -0.03709263 -0.037311595 -0.036983833 -0.03736382 -0.038701572 -0.040476803 -0.041546054 -0.041752491 -0.041645717 -0.040862057 -0.040088352 -0.040008388 -0.040954936 -0.04323101]]...]
INFO - root - 2017-12-06 06:13:39.671287: step 25810, loss = 0.93, batch loss = 0.71 (20.9 examples/sec; 0.383 sec/batch; 32h:35m:35s remains)
INFO - root - 2017-12-06 06:13:43.504265: step 25820, loss = 0.78, batch loss = 0.57 (20.8 examples/sec; 0.384 sec/batch; 32h:43m:44s remains)
INFO - root - 2017-12-06 06:13:47.429534: step 25830, loss = 0.81, batch loss = 0.60 (20.7 examples/sec; 0.387 sec/batch; 32h:59m:44s remains)
INFO - root - 2017-12-06 06:13:51.324766: step 25840, loss = 0.79, batch loss = 0.57 (21.3 examples/sec; 0.375 sec/batch; 31h:57m:25s remains)
INFO - root - 2017-12-06 06:13:55.191152: step 25850, loss = 0.79, batch loss = 0.58 (20.3 examples/sec; 0.394 sec/batch; 33h:33m:31s remains)
INFO - root - 2017-12-06 06:13:59.082851: step 25860, loss = 0.75, batch loss = 0.53 (20.9 examples/sec; 0.383 sec/batch; 32h:37m:05s remains)
INFO - root - 2017-12-06 06:14:02.960727: step 25870, loss = 0.84, batch loss = 0.62 (20.6 examples/sec; 0.389 sec/batch; 33h:08m:11s remains)
INFO - root - 2017-12-06 06:14:06.873330: step 25880, loss = 0.89, batch loss = 0.68 (22.4 examples/sec; 0.357 sec/batch; 30h:22m:43s remains)
INFO - root - 2017-12-06 06:14:10.731909: step 25890, loss = 0.79, batch loss = 0.57 (20.3 examples/sec; 0.395 sec/batch; 33h:36m:39s remains)
INFO - root - 2017-12-06 06:14:14.635582: step 25900, loss = 0.80, batch loss = 0.58 (20.4 examples/sec; 0.392 sec/batch; 33h:21m:32s remains)
2017-12-06 06:14:15.130787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070986256 -0.070663348 -0.070533141 -0.070412219 -0.070368841 -0.070396155 -0.0705061 -0.07063587 -0.070746884 -0.070861243 -0.070950143 -0.071016744 -0.071048923 -0.071065441 -0.071089484][-0.070958257 -0.070447013 -0.070174173 -0.069943555 -0.069869243 -0.0699128 -0.07004974 -0.070251435 -0.070450477 -0.070671134 -0.070854038 -0.071002513 -0.071107164 -0.071167812 -0.071187064][-0.070995152 -0.070344955 -0.069988705 -0.06964542 -0.069485486 -0.069495946 -0.069649071 -0.069871664 -0.070131294 -0.070399247 -0.070619568 -0.070815854 -0.070978977 -0.071082406 -0.071117416][-0.070959568 -0.070268989 -0.069900647 -0.069510937 -0.069269851 -0.0691866 -0.06928917 -0.069501474 -0.069776922 -0.070021458 -0.070179455 -0.070352979 -0.070539176 -0.07070291 -0.070805207][-0.070746861 -0.070073575 -0.069649346 -0.069219992 -0.068962112 -0.068845525 -0.068904713 -0.069069646 -0.069287091 -0.069457307 -0.069542535 -0.069639891 -0.069825023 -0.07011 -0.070365295][-0.070512205 -0.069771588 -0.069308043 -0.068843551 -0.068578459 -0.068474911 -0.06855955 -0.068742305 -0.068901293 -0.069041178 -0.06907478 -0.069112122 -0.069284648 -0.069627 -0.070029736][-0.070529506 -0.0697005 -0.069263764 -0.068826325 -0.068576813 -0.068500109 -0.068638571 -0.068849362 -0.068943948 -0.068985671 -0.0689062 -0.068831638 -0.068929635 -0.069257662 -0.06974107][-0.070774592 -0.069973126 -0.06966915 -0.069398165 -0.06922847 -0.069167107 -0.069288746 -0.06940452 -0.06937632 -0.069234572 -0.069019966 -0.068835631 -0.068862267 -0.069163337 -0.069671459][-0.071085006 -0.070335336 -0.070136361 -0.070019811 -0.069929034 -0.069878533 -0.069957837 -0.070021346 -0.069906592 -0.069676369 -0.0693831 -0.069228843 -0.069219716 -0.0694814 -0.069952756][-0.071339525 -0.070702367 -0.070608571 -0.070584483 -0.07054501 -0.070514522 -0.070581131 -0.070676059 -0.070594855 -0.070373386 -0.07008981 -0.069919191 -0.069844633 -0.070010692 -0.070396729][-0.071622536 -0.0710766 -0.071090437 -0.071160637 -0.07120268 -0.071194492 -0.071206354 -0.071263552 -0.071147412 -0.070947379 -0.070696875 -0.07052929 -0.0704442 -0.070476241 -0.070739478][-0.071820766 -0.071374804 -0.071435094 -0.071520716 -0.071584105 -0.071577772 -0.0715582 -0.071543321 -0.07142435 -0.0712431 -0.071035236 -0.070864789 -0.070777513 -0.070757158 -0.070952475][-0.0717853 -0.071453661 -0.071535893 -0.071613811 -0.071664318 -0.071636394 -0.071573585 -0.071492374 -0.071375251 -0.071237728 -0.071079582 -0.07097277 -0.070911966 -0.070896342 -0.071043953][-0.071612753 -0.071256407 -0.071303613 -0.071342453 -0.071375117 -0.071354888 -0.071318179 -0.071260035 -0.071192734 -0.071124524 -0.071029045 -0.070973508 -0.070925139 -0.070910081 -0.071023181][-0.071555369 -0.071148112 -0.071170405 -0.071185321 -0.071199276 -0.07119745 -0.071178637 -0.0711496 -0.071101815 -0.071053125 -0.071000539 -0.070952624 -0.070916191 -0.070922852 -0.071049616]]...]
INFO - root - 2017-12-06 06:14:19.031489: step 25910, loss = 0.78, batch loss = 0.56 (21.3 examples/sec; 0.375 sec/batch; 31h:55m:44s remains)
INFO - root - 2017-12-06 06:14:22.873452: step 25920, loss = 0.86, batch loss = 0.64 (20.6 examples/sec; 0.388 sec/batch; 33h:05m:02s remains)
INFO - root - 2017-12-06 06:14:26.762081: step 25930, loss = 0.78, batch loss = 0.56 (22.2 examples/sec; 0.360 sec/batch; 30h:40m:41s remains)
INFO - root - 2017-12-06 06:14:30.667056: step 25940, loss = 0.77, batch loss = 0.55 (20.8 examples/sec; 0.384 sec/batch; 32h:41m:29s remains)
INFO - root - 2017-12-06 06:14:34.620060: step 25950, loss = 0.78, batch loss = 0.57 (19.5 examples/sec; 0.409 sec/batch; 34h:51m:21s remains)
INFO - root - 2017-12-06 06:14:38.516154: step 25960, loss = 0.73, batch loss = 0.51 (21.1 examples/sec; 0.379 sec/batch; 32h:15m:08s remains)
INFO - root - 2017-12-06 06:14:42.391010: step 25970, loss = 0.78, batch loss = 0.57 (19.3 examples/sec; 0.415 sec/batch; 35h:21m:42s remains)
INFO - root - 2017-12-06 06:14:46.226722: step 25980, loss = 0.77, batch loss = 0.55 (20.0 examples/sec; 0.401 sec/batch; 34h:08m:20s remains)
INFO - root - 2017-12-06 06:14:50.043692: step 25990, loss = 0.72, batch loss = 0.51 (22.9 examples/sec; 0.349 sec/batch; 29h:43m:14s remains)
INFO - root - 2017-12-06 06:14:53.870532: step 26000, loss = 0.83, batch loss = 0.61 (21.3 examples/sec; 0.375 sec/batch; 31h:57m:03s remains)
2017-12-06 06:14:54.398510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.084797412 -0.086121812 -0.093673326 -0.11436594 -0.1515169 -0.20799418 -0.26860976 -0.31840867 -0.33605331 -0.31419823 -0.26336208 -0.20152955 -0.14918636 -0.11889399 -0.10437503][-0.09070465 -0.094249479 -0.11014925 -0.14563912 -0.20064616 -0.28625125 -0.38512313 -0.48472205 -0.55258995 -0.56126344 -0.50552386 -0.39830512 -0.28196317 -0.19134498 -0.13530628][-0.11393937 -0.13446914 -0.17479374 -0.23146707 -0.30314317 -0.391906 -0.50393367 -0.63778412 -0.76546091 -0.83796138 -0.81880653 -0.70395184 -0.53295833 -0.35920715 -0.22632751][-0.1431292 -0.18436325 -0.24378309 -0.30458084 -0.36420023 -0.42713445 -0.52527118 -0.652378 -0.81414449 -0.96567035 -1.0400907 -0.98860019 -0.82132721 -0.59460241 -0.38649622][-0.14656986 -0.18856925 -0.22947061 -0.23700887 -0.20733196 -0.15259604 -0.17462526 -0.26245347 -0.45883828 -0.70615739 -0.90572447 -0.98539859 -0.926445 -0.7468763 -0.53416556][-0.12703618 -0.15142626 -0.14483084 -0.07456506 0.060475208 0.24129426 0.31759897 0.29023483 0.08854384 -0.20832446 -0.50226426 -0.69867951 -0.75020665 -0.69924152 -0.57358545][-0.07703577 -0.061867982 0.014096677 0.17922539 0.42747593 0.72690243 0.86856914 0.87832308 0.68700647 0.37493593 0.038165234 -0.24027842 -0.40034959 -0.46475115 -0.44059712][-0.014670298 0.062217124 0.22022653 0.47706091 0.82038367 1.1886621 1.381506 1.4095155 1.2257189 0.91722941 0.54364187 0.21262315 -0.010026358 -0.16184975 -0.22819614][-0.0056007206 0.07084801 0.20015916 0.41431767 0.70624536 1.0415483 1.2228005 1.2375065 1.0954592 0.90281188 0.63381141 0.34586498 0.13405761 -0.020048626 -0.10522765][-0.050672393 -0.0082373321 0.075235493 0.22040293 0.39992252 0.63308382 0.79421914 0.85583895 0.79474294 0.652338 0.45737708 0.27525809 0.13606188 0.026772454 -0.03998759][-0.071024261 -0.054542772 -0.045560546 -0.0065553859 0.073790826 0.18014476 0.26741153 0.34462714 0.34677461 0.28168923 0.17914295 0.065953694 -0.017943047 -0.058433965 -0.075282775][-0.10946722 -0.13058069 -0.17428057 -0.22864512 -0.27131218 -0.28055328 -0.23776031 -0.19212312 -0.13991575 -0.10458079 -0.09312322 -0.10292275 -0.12343751 -0.13427983 -0.13749427][-0.11479135 -0.13871357 -0.19116776 -0.26606852 -0.35640731 -0.41411847 -0.45621455 -0.46063447 -0.40133941 -0.325675 -0.24692583 -0.18754336 -0.14924483 -0.14237782 -0.14487788][-0.098104626 -0.10464691 -0.12767991 -0.16898519 -0.22389796 -0.28875351 -0.35644177 -0.39273271 -0.39007121 -0.36882713 -0.31885031 -0.26268727 -0.21409404 -0.18240669 -0.16224453][-0.094383582 -0.094309695 -0.096188769 -0.1044635 -0.12449451 -0.15699545 -0.18323636 -0.21245369 -0.22373733 -0.22444171 -0.1981857 -0.1774402 -0.16635247 -0.14922488 -0.13194145]]...]
INFO - root - 2017-12-06 06:14:58.255070: step 26010, loss = 0.79, batch loss = 0.58 (20.5 examples/sec; 0.391 sec/batch; 33h:14m:48s remains)
INFO - root - 2017-12-06 06:15:02.136790: step 26020, loss = 0.83, batch loss = 0.62 (19.9 examples/sec; 0.403 sec/batch; 34h:16m:41s remains)
INFO - root - 2017-12-06 06:15:05.976269: step 26030, loss = 0.79, batch loss = 0.58 (23.1 examples/sec; 0.347 sec/batch; 29h:30m:30s remains)
INFO - root - 2017-12-06 06:15:09.820267: step 26040, loss = 0.82, batch loss = 0.60 (21.2 examples/sec; 0.377 sec/batch; 32h:05m:21s remains)
INFO - root - 2017-12-06 06:15:13.718235: step 26050, loss = 0.78, batch loss = 0.57 (20.6 examples/sec; 0.388 sec/batch; 33h:02m:29s remains)
INFO - root - 2017-12-06 06:15:17.652659: step 26060, loss = 0.77, batch loss = 0.55 (20.7 examples/sec; 0.387 sec/batch; 32h:57m:59s remains)
INFO - root - 2017-12-06 06:15:21.468339: step 26070, loss = 0.77, batch loss = 0.56 (20.5 examples/sec; 0.390 sec/batch; 33h:14m:17s remains)
INFO - root - 2017-12-06 06:15:25.298649: step 26080, loss = 0.74, batch loss = 0.52 (20.6 examples/sec; 0.387 sec/batch; 32h:58m:48s remains)
INFO - root - 2017-12-06 06:15:29.195279: step 26090, loss = 0.83, batch loss = 0.62 (20.0 examples/sec; 0.400 sec/batch; 34h:01m:42s remains)
INFO - root - 2017-12-06 06:15:32.953179: step 26100, loss = 0.86, batch loss = 0.65 (21.4 examples/sec; 0.374 sec/batch; 31h:50m:06s remains)
2017-12-06 06:15:33.481973: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.80227256 0.63616943 0.46639061 0.31878769 0.079155743 -0.17528518 -0.36163431 -0.42851561 -0.50283724 -0.56433129 -0.64875376 -0.63714343 -0.55715293 -0.40580887 -0.26158357][-0.049185831 -0.062684774 -0.046511244 -0.0038609877 0.061323315 0.20297734 0.21209393 0.18003394 0.085334331 -0.12367149 -0.324226 -0.44200248 -0.46746248 -0.3858574 -0.33106109][-0.68043417 -0.5204612 -0.31815797 -0.15390795 0.028937943 0.28843188 0.451092 0.5456503 0.55855858 0.4120633 0.1883574 -0.10074878 -0.29928204 -0.27613097 -0.28904581][-0.85129166 -0.82407683 -0.64195234 -0.37744248 -0.07594309 0.24205889 0.45067835 0.66711229 0.71218443 0.6545912 0.46471882 0.1481339 -0.18716708 -0.31235805 -0.33740777][-0.727288 -0.77829558 -0.76575363 -0.64160091 -0.3948828 -0.045738149 0.21003132 0.44160593 0.55575538 0.545858 0.34178126 0.035042226 -0.254338 -0.46924114 -0.56817043][-0.42856848 -0.5507651 -0.61813593 -0.56830621 -0.44677526 -0.17851236 0.077110291 0.20466153 0.18187346 0.218709 0.18306895 -0.072761759 -0.38363868 -0.50915134 -0.52675611][-0.19259471 -0.25121903 -0.29813081 -0.22531894 -0.14167431 -0.030488603 0.0697563 0.18554147 0.10937388 0.053021654 0.0065766126 -0.12504721 -0.31689531 -0.43410504 -0.40929043][-0.16101354 -0.21434353 -0.19711256 -0.084615193 0.050722376 0.17282645 0.20427997 0.17900591 0.11939296 0.1377517 0.11167888 -0.0011542365 -0.095538266 -0.10772734 -0.1510442][-0.20177051 -0.28795415 -0.24041688 -0.11600445 -0.040390644 0.069646567 0.064922377 -0.035122186 -0.12850608 -0.018722095 0.043077946 -0.012874715 -0.089576334 -0.016179778 0.020313524][-0.19164559 -0.25856602 -0.25813872 -0.21082717 -0.21379665 -0.23153673 -0.26374489 -0.3054297 -0.35411811 -0.31898877 -0.24038044 -0.19717817 -0.1761632 -0.023984976 0.10659544][-0.13695502 -0.21520767 -0.24226493 -0.23647009 -0.27552181 -0.34287989 -0.47446525 -0.56800056 -0.552398 -0.45931321 -0.38675421 -0.31092423 -0.19324842 -0.0049609095 0.12423503][-0.070760965 -0.17138186 -0.26802605 -0.30573872 -0.35416639 -0.39738196 -0.45736098 -0.46837527 -0.40460616 -0.3577553 -0.31595847 -0.2473623 -0.18589267 -0.10358819 -0.076079123][-0.077838078 -0.12604246 -0.22547115 -0.27400741 -0.26893604 -0.30001193 -0.32938677 -0.30834082 -0.26251394 -0.23987542 -0.22659723 -0.17913377 -0.15316063 -0.12148187 -0.16952164][-0.017530188 -0.046548024 -0.10697135 -0.14941041 -0.17451525 -0.17351854 -0.16939829 -0.14975628 -0.14835456 -0.15820196 -0.17311296 -0.18875858 -0.21134396 -0.22141671 -0.25683039][-0.094945915 -0.10691883 -0.13098872 -0.11555546 -0.080261156 -0.065333322 -0.0745777 -0.092036083 -0.10942918 -0.11903328 -0.15865985 -0.19853708 -0.23182389 -0.24389945 -0.3137795]]...]
INFO - root - 2017-12-06 06:15:37.260223: step 26110, loss = 0.88, batch loss = 0.66 (21.1 examples/sec; 0.379 sec/batch; 32h:17m:34s remains)
INFO - root - 2017-12-06 06:15:41.046297: step 26120, loss = 0.84, batch loss = 0.63 (21.3 examples/sec; 0.375 sec/batch; 31h:54m:32s remains)
INFO - root - 2017-12-06 06:15:44.900880: step 26130, loss = 0.75, batch loss = 0.54 (19.4 examples/sec; 0.413 sec/batch; 35h:08m:52s remains)
INFO - root - 2017-12-06 06:15:48.718753: step 26140, loss = 0.80, batch loss = 0.58 (20.4 examples/sec; 0.391 sec/batch; 33h:18m:59s remains)
INFO - root - 2017-12-06 06:15:52.599212: step 26150, loss = 0.82, batch loss = 0.61 (21.0 examples/sec; 0.381 sec/batch; 32h:27m:13s remains)
INFO - root - 2017-12-06 06:15:56.420942: step 26160, loss = 0.81, batch loss = 0.59 (20.6 examples/sec; 0.388 sec/batch; 33h:01m:36s remains)
INFO - root - 2017-12-06 06:16:00.195981: step 26170, loss = 0.75, batch loss = 0.54 (21.9 examples/sec; 0.365 sec/batch; 31h:05m:55s remains)
INFO - root - 2017-12-06 06:16:04.043019: step 26180, loss = 0.81, batch loss = 0.60 (20.5 examples/sec; 0.390 sec/batch; 33h:09m:26s remains)
INFO - root - 2017-12-06 06:16:07.213614: step 26190, loss = 0.79, batch loss = 0.57 (24.1 examples/sec; 0.333 sec/batch; 28h:17m:58s remains)
INFO - root - 2017-12-06 06:16:10.357789: step 26200, loss = 0.88, batch loss = 0.67 (24.5 examples/sec; 0.327 sec/batch; 27h:49m:59s remains)
2017-12-06 06:16:10.809599: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0045651346 -0.064263649 -0.21476407 -0.41231054 -0.60844439 -0.75869709 -0.81303537 -0.81694967 -0.79621756 -0.77965069 -0.78917235 -0.7570231 -0.68029869 -0.55835855 -0.45296109][0.034679383 -0.052980993 -0.19867116 -0.44677943 -0.68342406 -0.81940758 -0.880244 -0.94700688 -0.99503636 -1.0500498 -1.1150163 -1.2172663 -1.2687739 -1.1683097 -0.91207492][-0.050040137 -0.18544531 -0.32289672 -0.45184267 -0.55249619 -0.65735257 -0.69592053 -0.69670564 -0.73655134 -0.87338388 -1.0639846 -1.2311949 -1.3389312 -1.3526012 -1.2051244][-0.35870671 -0.4522391 -0.5209263 -0.56413275 -0.53063929 -0.39690673 -0.2623378 -0.25465751 -0.3524099 -0.50495458 -0.7126264 -0.88911176 -1.0641067 -1.133229 -1.0718237][-0.717633 -0.80546248 -0.75771016 -0.60650378 -0.41328567 -0.15554559 0.10108209 0.24780677 0.17100139 -0.024494126 -0.21555512 -0.36326611 -0.50450182 -0.60650164 -0.62816757][-0.859307 -0.88673854 -0.75225312 -0.45233524 -0.051697019 0.3520298 0.59259945 0.68414694 0.63618284 0.49225968 0.30272913 0.16802834 0.066844165 -0.040617608 -0.15197247][-0.7281695 -0.70024353 -0.55155337 -0.26866728 0.11619847 0.56267542 0.87116426 0.930269 0.87527353 0.77644527 0.66390526 0.54318094 0.53199637 0.49481219 0.43916249][-0.5227133 -0.4398554 -0.34014988 -0.1501227 0.12567583 0.50128049 0.80524641 0.91351449 0.931021 0.95200622 0.9067145 0.80512708 0.84356296 0.86121267 0.89706856][-0.35732931 -0.30451268 -0.23860052 -0.13531815 0.037275359 0.27418411 0.4861722 0.65819615 0.79203892 0.85968226 0.94276989 0.97609591 1.0300888 1.0774131 1.1344409][-0.17102486 -0.18117426 -0.16933556 -0.24027301 -0.30131304 -0.26867041 -0.11213504 -0.041046835 0.097622588 0.26178324 0.4454667 0.62538952 0.73887777 0.82448965 0.86706036][0.045322955 0.022625625 -0.015859976 -0.20194638 -0.42559403 -0.63091171 -0.70152664 -0.74293077 -0.69497836 -0.59359384 -0.38443452 -0.18869549 -0.080992728 -0.023361199 0.065421566][0.12903619 0.14183955 0.10971804 -0.1305304 -0.46259421 -0.75378996 -0.92164165 -1.0735227 -1.1846647 -1.2064543 -1.125034 -1.0070018 -0.94073671 -0.84187996 -0.74657029][0.2349038 0.21813272 0.11281101 -0.12987675 -0.44740742 -0.69507968 -0.84078354 -0.90943515 -1.0143611 -1.1219856 -1.2268274 -1.2749361 -1.3501643 -1.2656137 -1.1546103][0.4364568 0.34820193 0.14937824 -0.14111008 -0.44002151 -0.64641249 -0.77679074 -0.71426433 -0.69729149 -0.69625032 -0.78128827 -0.8903833 -1.0246729 -1.0544337 -1.0441332][0.86618513 0.84066916 0.58888733 0.16550015 -0.25388211 -0.49505854 -0.57757914 -0.50398093 -0.44176316 -0.34694386 -0.32748193 -0.37244064 -0.49968708 -0.5424996 -0.58947664]]...]
INFO - root - 2017-12-06 06:16:13.972043: step 26210, loss = 0.81, batch loss = 0.59 (25.0 examples/sec; 0.320 sec/batch; 27h:15m:58s remains)
INFO - root - 2017-12-06 06:16:17.622679: step 26220, loss = 0.82, batch loss = 0.61 (20.4 examples/sec; 0.392 sec/batch; 33h:22m:14s remains)
INFO - root - 2017-12-06 06:16:21.270712: step 26230, loss = 0.73, batch loss = 0.52 (22.4 examples/sec; 0.358 sec/batch; 30h:25m:48s remains)
INFO - root - 2017-12-06 06:16:24.825829: step 26240, loss = 0.80, batch loss = 0.58 (20.5 examples/sec; 0.391 sec/batch; 33h:15m:39s remains)
INFO - root - 2017-12-06 06:16:28.591267: step 26250, loss = 0.83, batch loss = 0.62 (23.2 examples/sec; 0.344 sec/batch; 29h:16m:59s remains)
INFO - root - 2017-12-06 06:16:32.378429: step 26260, loss = 0.79, batch loss = 0.58 (21.6 examples/sec; 0.371 sec/batch; 31h:31m:27s remains)
INFO - root - 2017-12-06 06:16:36.158661: step 26270, loss = 0.82, batch loss = 0.60 (21.8 examples/sec; 0.367 sec/batch; 31h:15m:33s remains)
INFO - root - 2017-12-06 06:16:40.031919: step 26280, loss = 0.73, batch loss = 0.52 (19.9 examples/sec; 0.402 sec/batch; 34h:12m:45s remains)
INFO - root - 2017-12-06 06:16:43.868767: step 26290, loss = 0.83, batch loss = 0.61 (22.2 examples/sec; 0.361 sec/batch; 30h:43m:03s remains)
INFO - root - 2017-12-06 06:16:47.618830: step 26300, loss = 0.91, batch loss = 0.69 (21.5 examples/sec; 0.372 sec/batch; 31h:36m:43s remains)
2017-12-06 06:16:48.104455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0984018 -0.092152305 -0.084119022 -0.085376948 -0.091037817 -0.11384189 -0.14110944 -0.13750544 -0.14319403 -0.15870683 -0.19487873 -0.23928471 -0.29386169 -0.32170188 -0.31289566][-0.097265847 -0.093464807 -0.08839149 -0.11216349 -0.14280061 -0.20010769 -0.31367555 -0.44733232 -0.60545397 -0.69889319 -0.72619575 -0.70510966 -0.70715022 -0.70654792 -0.67381281][-0.11008394 -0.1335384 -0.16663103 -0.22250472 -0.29312867 -0.39203918 -0.57909924 -0.84943032 -1.2020546 -1.4293535 -1.4664334 -1.4304365 -1.4136888 -1.4003797 -1.3270208][-0.11047068 -0.14308211 -0.19607204 -0.28206921 -0.36362141 -0.42300171 -0.55707294 -0.84592515 -1.2753171 -1.6647391 -1.8512571 -1.916544 -1.9213709 -1.9102836 -1.8087369][-0.086126745 -0.081976421 -0.093607582 -0.1307407 -0.13512877 -0.10248515 -0.089120522 -0.21826126 -0.59056014 -1.136008 -1.5164273 -1.7237263 -1.7723188 -1.8085011 -1.8190941][-0.062152565 -0.02333007 0.026882045 0.10884079 0.26897019 0.5199132 0.76408935 0.92189395 0.78278732 0.28998876 -0.25528139 -0.64498866 -0.79314983 -0.88711125 -1.0333815][-0.042530615 0.051865086 0.19758637 0.43723482 0.79133272 1.3497432 1.9638728 2.4450531 2.5293632 2.191824 1.6572492 1.1992517 0.94212461 0.62717366 0.21082933][-0.030330237 0.088053495 0.30032611 0.64208853 1.1023421 1.7287223 2.3931439 2.969156 3.1274531 2.8524489 2.3925223 2.1458209 1.9670213 1.5834577 1.0191554][-0.068423137 -0.0093210489 0.13599224 0.397725 0.74342841 1.2624298 1.8180239 2.2839577 2.4261975 2.2927732 2.0235393 1.8728774 1.7535658 1.4562764 0.95026231][-0.10155696 -0.10232362 -0.06257163 0.027150601 0.15034658 0.44902021 0.842132 1.2196929 1.4405912 1.4456359 1.3142272 1.1895598 1.0427701 0.77890223 0.44042635][-0.1069215 -0.13059542 -0.16851176 -0.20623887 -0.2262997 -0.16456878 -0.052460212 0.15033679 0.36188519 0.54189509 0.62822044 0.63599408 0.53215539 0.2613138 -0.084088989][-0.10668559 -0.13146922 -0.19889149 -0.30487594 -0.41677177 -0.53806317 -0.64756721 -0.72903717 -0.76543713 -0.61050826 -0.49748796 -0.41569883 -0.3347235 -0.34845364 -0.44151109][-0.0969268 -0.10529386 -0.13251393 -0.19707544 -0.31906253 -0.47630149 -0.66248184 -0.84524071 -1.0239342 -1.1783228 -1.2859256 -1.1900988 -1.0012894 -0.86713231 -0.76187605][-0.096111037 -0.096687831 -0.10252065 -0.12455984 -0.18749216 -0.29877332 -0.45901918 -0.60330284 -0.79084313 -0.95690489 -1.1038496 -1.1330266 -0.98887014 -0.79792184 -0.61915177][-0.096289091 -0.09670639 -0.097243138 -0.093796924 -0.10535318 -0.13199832 -0.18163532 -0.23437405 -0.31184095 -0.389073 -0.44755805 -0.46313083 -0.410841 -0.31795481 -0.21159965]]...]
INFO - root - 2017-12-06 06:16:51.959682: step 26310, loss = 0.90, batch loss = 0.69 (20.4 examples/sec; 0.393 sec/batch; 33h:24m:29s remains)
INFO - root - 2017-12-06 06:16:55.762877: step 26320, loss = 0.86, batch loss = 0.64 (21.4 examples/sec; 0.373 sec/batch; 31h:45m:15s remains)
INFO - root - 2017-12-06 06:16:59.453345: step 26330, loss = 0.86, batch loss = 0.64 (19.7 examples/sec; 0.407 sec/batch; 34h:37m:16s remains)
INFO - root - 2017-12-06 06:17:03.204405: step 26340, loss = 0.82, batch loss = 0.61 (21.2 examples/sec; 0.378 sec/batch; 32h:09m:53s remains)
INFO - root - 2017-12-06 06:17:07.030603: step 26350, loss = 0.78, batch loss = 0.57 (21.3 examples/sec; 0.375 sec/batch; 31h:55m:49s remains)
INFO - root - 2017-12-06 06:17:10.283002: step 26360, loss = 0.78, batch loss = 0.57 (22.1 examples/sec; 0.361 sec/batch; 30h:43m:35s remains)
INFO - root - 2017-12-06 06:17:13.819893: step 26370, loss = 0.78, batch loss = 0.56 (21.0 examples/sec; 0.382 sec/batch; 32h:27m:03s remains)
INFO - root - 2017-12-06 06:17:17.434553: step 26380, loss = 0.78, batch loss = 0.56 (21.1 examples/sec; 0.379 sec/batch; 32h:13m:49s remains)
INFO - root - 2017-12-06 06:17:21.221745: step 26390, loss = 0.81, batch loss = 0.60 (25.3 examples/sec; 0.317 sec/batch; 26h:55m:22s remains)
INFO - root - 2017-12-06 06:17:25.158794: step 26400, loss = 0.76, batch loss = 0.54 (20.2 examples/sec; 0.395 sec/batch; 33h:36m:00s remains)
2017-12-06 06:17:25.590074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28884852 -0.26479548 -0.25915581 -0.52285892 -0.8142947 -1.2377764 -1.7554238 -1.5148386 -1.4538188 -1.1520034 -0.81294537 -0.0740864 0.54089457 0.4983204 0.17814787][1.1827897 0.80706614 0.59372187 0.67571676 0.60871166 0.047362119 -0.53295553 -0.96459448 -1.5893499 -1.2713637 -0.69041735 -0.43492985 -0.18622142 0.16893317 0.80171359][1.0017929 1.0296644 1.1549134 0.8835904 0.60786206 0.1392947 -0.67924154 -1.2281592 -1.7817825 -1.5430852 -0.99170572 -0.79693121 -0.8307606 -0.51297045 -0.72900862][0.85098648 0.86756068 0.96688813 0.86954534 0.83210176 0.17043827 -1.0309297 -1.3634909 -1.8789421 -1.8492965 -1.4801069 -0.96098131 -0.93737268 -0.941247 -1.5378203][0.51964724 0.84358394 1.3081009 1.2382426 0.81646687 0.49599993 0.15566564 -0.29580596 -1.0564543 -1.0451238 -1.1020212 -1.0663425 -1.2584134 -1.2026662 -1.6205664][0.98809868 0.92249471 0.96919984 1.3043395 1.3564924 0.89142549 0.61574727 0.38311285 0.35289836 -0.06207069 -0.7226426 -0.46630687 -0.51595408 -0.33501297 -1.066937][1.4980904 1.5251378 1.6819142 1.3323715 0.81200492 0.58725148 0.44998002 0.26363027 0.33484942 0.52609366 0.83226645 0.74247408 0.097982377 0.54890889 0.25890779][2.0126905 1.9093024 1.740834 1.3868217 1.0094042 0.51698858 0.2506327 0.056932822 0.14747129 0.55814421 1.2035511 1.5484382 1.7356801 1.4099438 0.86696297][0.74398994 0.86964536 0.77915049 0.22647692 -0.049150009 0.03703779 0.079707235 0.023754232 -0.19314688 0.024173938 0.55246013 1.0540023 0.9493162 1.273646 1.5960848][-0.49761653 -0.64518869 -0.72403479 -0.96245956 -0.82917625 -0.97721744 -1.2411009 -0.789913 -0.90378118 -0.82690686 -0.53995281 0.489852 1.1807469 0.89384139 1.1346629][-1.8408886 -2.3264649 -2.1011131 -2.5023956 -2.7200246 -2.7805812 -2.7181613 -2.6223838 -2.7794418 -2.6127286 -1.9964188 -1.3228804 -0.782404 0.05119586 0.78619832][-1.8409538 -2.6344166 -2.7547452 -2.6682482 -2.4397714 -3.03641 -3.335572 -3.2572932 -3.0408745 -2.9038298 -2.4767458 -1.8460591 -1.4504038 -0.89506942 -0.51664793][-1.8076448 -1.8815298 -2.0007372 -2.1720154 -2.504719 -2.6890564 -2.7565939 -2.6255748 -2.3933775 -2.3181808 -2.0809314 -1.5846181 -1.3219118 -1.1164043 -0.94499725][-0.81026685 -0.63518822 -0.83030784 -0.57909274 -0.86295116 -1.1679516 -1.4044924 -1.3923908 -1.4042594 -1.0846314 -0.48757261 -0.68429059 -0.68346667 -0.73896503 -1.1167547][0.851174 0.65181059 0.59379315 0.51491225 0.31733382 0.10505766 -0.24122417 -0.40718013 -0.48678643 -0.56637859 -0.37954485 -0.10449187 -0.0081090778 -0.30700243 -0.33204323]]...]
INFO - root - 2017-12-06 06:17:29.195295: step 26410, loss = 0.72, batch loss = 0.51 (25.2 examples/sec; 0.318 sec/batch; 27h:00m:37s remains)
INFO - root - 2017-12-06 06:17:32.955280: step 26420, loss = 0.84, batch loss = 0.63 (23.4 examples/sec; 0.342 sec/batch; 29h:06m:50s remains)
INFO - root - 2017-12-06 06:17:36.777645: step 26430, loss = 0.78, batch loss = 0.56 (23.6 examples/sec; 0.339 sec/batch; 28h:50m:11s remains)
INFO - root - 2017-12-06 06:17:40.515734: step 26440, loss = 0.88, batch loss = 0.66 (22.1 examples/sec; 0.363 sec/batch; 30h:49m:52s remains)
INFO - root - 2017-12-06 06:17:44.367953: step 26450, loss = 0.83, batch loss = 0.61 (22.1 examples/sec; 0.363 sec/batch; 30h:49m:12s remains)
INFO - root - 2017-12-06 06:17:48.127273: step 26460, loss = 0.81, batch loss = 0.60 (21.0 examples/sec; 0.381 sec/batch; 32h:22m:58s remains)
INFO - root - 2017-12-06 06:17:51.834897: step 26470, loss = 0.80, batch loss = 0.58 (22.5 examples/sec; 0.356 sec/batch; 30h:15m:31s remains)
INFO - root - 2017-12-06 06:17:55.649343: step 26480, loss = 0.81, batch loss = 0.60 (20.9 examples/sec; 0.382 sec/batch; 32h:29m:32s remains)
INFO - root - 2017-12-06 06:17:59.336955: step 26490, loss = 0.84, batch loss = 0.63 (26.4 examples/sec; 0.303 sec/batch; 25h:43m:04s remains)
INFO - root - 2017-12-06 06:18:02.543260: step 26500, loss = 0.90, batch loss = 0.68 (21.3 examples/sec; 0.375 sec/batch; 31h:52m:45s remains)
2017-12-06 06:18:03.020453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0710507 -0.070684008 -0.070712715 -0.070712417 -0.070697829 -0.070653319 -0.070484228 -0.070466116 -0.070399322 -0.070498563 -0.070653893 -0.070807531 -0.071049161 -0.071254753 -0.071440727][-0.071049035 -0.07071618 -0.070775315 -0.070799395 -0.070817351 -0.070833869 -0.070796005 -0.070841283 -0.070788532 -0.07082624 -0.070900857 -0.070966013 -0.071004905 -0.071089141 -0.07117][-0.070821866 -0.070526332 -0.070564315 -0.070483565 -0.069891326 -0.068744585 -0.0681173 -0.068165235 -0.068587951 -0.069214113 -0.069936156 -0.070594192 -0.070780136 -0.070775762 -0.070688441][-0.070606604 -0.070266068 -0.070262544 -0.070205152 -0.069902413 -0.068854988 -0.067673877 -0.067230687 -0.067977861 -0.068886012 -0.069633745 -0.070308566 -0.07054393 -0.07046479 -0.0703464][-0.070418164 -0.070167832 -0.070284963 -0.070682369 -0.071944162 -0.073029652 -0.072924271 -0.073151425 -0.073533013 -0.073223487 -0.072344 -0.071544975 -0.070890382 -0.070386544 -0.070208549][-0.070056781 -0.069669873 -0.06956879 -0.069781348 -0.070946932 -0.072242081 -0.072488382 -0.072585531 -0.0726045 -0.0720651 -0.0711562 -0.070330456 -0.069746874 -0.069585 -0.069799773][-0.070502 -0.070249088 -0.07046783 -0.070947021 -0.071936056 -0.073040232 -0.073574275 -0.073834486 -0.073948607 -0.073474109 -0.072515823 -0.071517847 -0.070682943 -0.070275143 -0.070421129][-0.07099659 -0.070768334 -0.071133144 -0.071655869 -0.072423309 -0.073198974 -0.073827848 -0.074134715 -0.074262127 -0.073914982 -0.072958112 -0.07190793 -0.071005732 -0.070514008 -0.070777][-0.070929818 -0.0707159 -0.070989564 -0.071612492 -0.072433963 -0.073151886 -0.073622584 -0.073796779 -0.073683769 -0.073096484 -0.072117113 -0.07101246 -0.070078269 -0.069605887 -0.07021822][-0.069726914 -0.067565359 -0.064565651 -0.061184943 -0.0570777 -0.053934913 -0.050634611 -0.047741868 -0.045258109 -0.045640487 -0.049518291 -0.054639947 -0.059824202 -0.065073982 -0.0697602][-0.027886987 -0.035719912 -0.039166838 -0.035829794 -0.02516032 -0.013544433 0.0019138753 0.014048249 0.024335265 0.026986331 0.019477218 0.0019407794 -0.019955769 -0.041503936 -0.060582675][0.076138422 0.070500374 0.068810791 0.078642666 0.099843904 0.11290616 0.13119109 0.13838322 0.1390249 0.13452056 0.12465857 0.10679035 0.084381551 0.060803562 0.0336167][0.31072307 0.30725121 0.31133181 0.3320086 0.35970294 0.37973732 0.40104473 0.40909952 0.40203559 0.39681298 0.39174354 0.37806529 0.35943991 0.33741021 0.312315][0.680275 0.65137726 0.62308985 0.61668074 0.62193114 0.62648064 0.64051813 0.64779037 0.64837056 0.63721478 0.63198882 0.62374943 0.60892642 0.58659136 0.56247985][0.78141332 0.75704861 0.71627837 0.6918602 0.68167341 0.676868 0.687448 0.70266515 0.72295541 0.736817 0.75376165 0.77418679 0.79396921 0.80113053 0.80386078]]...]
INFO - root - 2017-12-06 06:18:06.530844: step 26510, loss = 0.90, batch loss = 0.69 (19.5 examples/sec; 0.410 sec/batch; 34h:53m:09s remains)
INFO - root - 2017-12-06 06:18:10.260231: step 26520, loss = 0.76, batch loss = 0.55 (20.6 examples/sec; 0.389 sec/batch; 33h:02m:51s remains)
INFO - root - 2017-12-06 06:18:14.174927: step 26530, loss = 0.83, batch loss = 0.62 (19.5 examples/sec; 0.411 sec/batch; 34h:53m:27s remains)
INFO - root - 2017-12-06 06:18:17.981656: step 26540, loss = 0.71, batch loss = 0.49 (19.2 examples/sec; 0.417 sec/batch; 35h:26m:05s remains)
INFO - root - 2017-12-06 06:18:21.865391: step 26550, loss = 0.82, batch loss = 0.60 (20.2 examples/sec; 0.396 sec/batch; 33h:40m:06s remains)
INFO - root - 2017-12-06 06:18:25.666751: step 26560, loss = 0.91, batch loss = 0.69 (21.5 examples/sec; 0.373 sec/batch; 31h:40m:54s remains)
INFO - root - 2017-12-06 06:18:29.509957: step 26570, loss = 0.87, batch loss = 0.65 (21.3 examples/sec; 0.375 sec/batch; 31h:50m:51s remains)
INFO - root - 2017-12-06 06:18:33.334374: step 26580, loss = 0.82, batch loss = 0.61 (20.7 examples/sec; 0.386 sec/batch; 32h:47m:40s remains)
INFO - root - 2017-12-06 06:18:37.125993: step 26590, loss = 0.83, batch loss = 0.61 (22.2 examples/sec; 0.361 sec/batch; 30h:40m:24s remains)
INFO - root - 2017-12-06 06:18:40.689150: step 26600, loss = 0.83, batch loss = 0.61 (29.2 examples/sec; 0.274 sec/batch; 23h:14m:37s remains)
2017-12-06 06:18:41.100750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.98264676 -1.4126787 -2.0339704 -2.5342903 -3.0734951 -3.2328374 -3.3609285 -3.4655013 -3.4880252 -3.2631309 -2.8237264 -2.8864412 -2.9378486 -3.194674 -3.3109992][-0.304225 -0.68124235 -1.2107832 -1.9641449 -2.8822975 -3.4280105 -3.8287048 -3.8963723 -3.7779481 -3.5639403 -3.3030837 -2.853806 -2.346417 -2.3113761 -2.2718515][0.13961372 0.062481746 -0.46534473 -1.5093409 -2.4178245 -3.140959 -3.7891464 -3.9939756 -4.0352917 -3.7694097 -3.3870244 -2.8988974 -2.4643841 -1.7796435 -1.3255353][-0.24688502 -0.28024837 -0.60319555 -1.1216303 -1.8519344 -2.7127855 -3.4359419 -3.7586987 -3.8984334 -3.5324996 -3.0680232 -2.5393739 -2.1569986 -1.6027486 -1.3693676][-1.397191 -1.4635619 -1.4109813 -1.2809844 -1.1899832 -1.4771827 -2.0946529 -2.5551786 -2.7996147 -2.6966839 -2.6291764 -2.2969589 -1.9824308 -1.615273 -1.6329708][-1.4882578 -1.2068021 -0.99753064 -0.98313713 -0.80023158 -0.60978395 -0.77359778 -1.1016767 -1.4355284 -1.7741104 -2.0774443 -1.8892626 -1.6892809 -1.4863495 -1.5796024][-0.94162446 -0.90061778 -0.35146034 -0.059786931 0.16850333 0.23927252 0.074525177 -0.28684786 -0.6052478 -0.94904763 -1.1883627 -1.5872409 -1.8881031 -1.5552459 -1.6672647][0.71721566 0.33036214 0.2952497 0.567429 1.0656533 1.3037595 1.3570873 1.0935438 0.65832639 0.11465026 -0.24269035 -0.55459529 -0.76649964 -0.87871134 -1.2733562][0.9391703 0.949292 1.1480619 1.2626531 1.1341065 1.1962336 1.230311 1.132942 1.108807 0.7092222 0.43530959 0.23560096 0.35037857 -1.616776e-06 -0.37365216][0.8224715 0.26651245 0.37603581 0.40173924 0.20287149 0.12897845 0.10657078 -0.012052953 -0.31893259 -0.38360816 -0.1471999 -0.19523865 -0.12323505 -0.14324817 -0.29002237][-0.5289048 -0.91667622 -1.2072095 -1.3074759 -1.5426134 -1.3933574 -1.2416297 -1.2385707 -1.3270427 -1.3223925 -1.119391 -0.50361836 0.1531902 0.3972438 0.43539929][-1.2030412 -1.6891453 -1.8223851 -2.0522897 -2.2399027 -2.3185673 -2.5624418 -2.6792064 -2.4972968 -2.004467 -1.3674293 -0.74551421 -0.28814322 0.29541588 0.57426453][-0.6603387 -1.2181145 -1.4971247 -1.5283824 -2.0625372 -2.5758896 -2.8385687 -2.8858767 -2.5000145 -2.2319262 -1.7269032 -0.90112042 -0.24844536 0.35522825 0.60869908][-0.39894336 -0.50238132 -0.022442266 -0.15055522 -0.94057 -1.3920492 -1.752336 -1.8223443 -1.4033817 -1.0400231 -0.68434215 -0.44471627 -0.29236543 0.14465733 0.38355923][-0.18727577 -0.40043265 -0.29949626 -0.373464 -0.99697781 -1.562899 -2.3230698 -2.1606588 -1.8906243 -1.8339491 -1.3859508 -1.070766 -0.6336838 -0.23616213 0.14800113]]...]
INFO - root - 2017-12-06 06:18:44.577644: step 26610, loss = 0.76, batch loss = 0.55 (20.3 examples/sec; 0.394 sec/batch; 33h:30m:21s remains)
INFO - root - 2017-12-06 06:18:48.389487: step 26620, loss = 0.83, batch loss = 0.62 (21.1 examples/sec; 0.379 sec/batch; 32h:12m:11s remains)
INFO - root - 2017-12-06 06:18:52.307593: step 26630, loss = 0.72, batch loss = 0.51 (21.4 examples/sec; 0.373 sec/batch; 31h:43m:15s remains)
INFO - root - 2017-12-06 06:18:56.083011: step 26640, loss = 0.86, batch loss = 0.65 (22.1 examples/sec; 0.362 sec/batch; 30h:47m:49s remains)
INFO - root - 2017-12-06 06:18:59.966786: step 26650, loss = 0.80, batch loss = 0.58 (20.0 examples/sec; 0.400 sec/batch; 33h:58m:31s remains)
INFO - root - 2017-12-06 06:19:03.863142: step 26660, loss = 0.90, batch loss = 0.68 (20.4 examples/sec; 0.391 sec/batch; 33h:15m:25s remains)
INFO - root - 2017-12-06 06:19:07.275409: step 26670, loss = 0.85, batch loss = 0.64 (22.6 examples/sec; 0.354 sec/batch; 30h:01m:51s remains)
INFO - root - 2017-12-06 06:19:10.671086: step 26680, loss = 0.83, batch loss = 0.62 (26.2 examples/sec; 0.305 sec/batch; 25h:56m:45s remains)
INFO - root - 2017-12-06 06:19:14.270404: step 26690, loss = 0.81, batch loss = 0.59 (20.0 examples/sec; 0.400 sec/batch; 33h:59m:48s remains)
INFO - root - 2017-12-06 06:19:18.023041: step 26700, loss = 0.76, batch loss = 0.54 (21.1 examples/sec; 0.380 sec/batch; 32h:15m:26s remains)
2017-12-06 06:19:18.487980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070684165 -0.070398368 -0.070257753 -0.070180073 -0.070557751 -0.071299292 -0.071860321 -0.071867362 -0.071693674 -0.07160008 -0.071773767 -0.07214164 -0.072531879 -0.07287804 -0.073559836][-0.06908273 -0.068925224 -0.068983272 -0.06953299 -0.071321405 -0.0742504 -0.077048756 -0.077797994 -0.076552056 -0.074614048 -0.07310497 -0.072382741 -0.07229951 -0.072567329 -0.073227867][-0.067360565 -0.067345276 -0.068365581 -0.070710145 -0.074814111 -0.081704728 -0.089347593 -0.092998415 -0.089878626 -0.083924919 -0.078883141 -0.075153761 -0.072964609 -0.072664328 -0.073116228][-0.065857351 -0.066569149 -0.069072656 -0.072384566 -0.076923966 -0.088320307 -0.10341368 -0.11633412 -0.11392244 -0.10170235 -0.09087 -0.082324408 -0.075596668 -0.073878236 -0.073724225][-0.063789017 -0.065244339 -0.067977652 -0.069495574 -0.06975542 -0.0783302 -0.0985053 -0.12858656 -0.13742134 -0.1239674 -0.10776714 -0.092688762 -0.080169164 -0.075769596 -0.0747439][-0.061617166 -0.061750736 -0.059542757 -0.052735008 -0.038174506 -0.030062765 -0.039356295 -0.084985428 -0.12156844 -0.12355687 -0.1122292 -0.098184437 -0.084125832 -0.076327063 -0.07524009][-0.059302393 -0.055803522 -0.045271266 -0.027396947 0.0037021413 0.029754877 0.035069898 -0.019187428 -0.089171082 -0.11944737 -0.11728443 -0.1029488 -0.086585864 -0.076028928 -0.075559855][-0.057287738 -0.049673241 -0.0302861 -0.0046090707 0.032789469 0.066209242 0.086993992 0.044914588 -0.037828568 -0.096326366 -0.11387078 -0.10760518 -0.088503152 -0.073460624 -0.072793745][-0.058294486 -0.049925495 -0.026215419 0.0020251274 0.039704546 0.071385175 0.0918953 0.059987277 -0.014347315 -0.0791787 -0.1052364 -0.10554382 -0.087866604 -0.073317334 -0.073668048][-0.060207136 -0.057442598 -0.039975271 -0.018771216 0.003899619 0.017503627 0.024810508 0.0018058121 -0.0520194 -0.096108355 -0.11077878 -0.10696942 -0.088226758 -0.072225638 -0.071780041][-0.061299935 -0.06268318 -0.052320443 -0.037070449 -0.018685512 -0.0064340755 0.0026830509 -0.010876507 -0.055350777 -0.094158955 -0.10892037 -0.10578727 -0.091821551 -0.074682452 -0.070621707][-0.062116541 -0.0662246 -0.065306664 -0.060711674 -0.056817811 -0.055856697 -0.046920437 -0.045668703 -0.068152189 -0.090909548 -0.10159468 -0.098655552 -0.09145008 -0.078707322 -0.071296863][-0.062744431 -0.068122432 -0.072826035 -0.076055631 -0.081322238 -0.090617739 -0.089118034 -0.078323483 -0.079504974 -0.08700183 -0.092274256 -0.089493334 -0.085295983 -0.080715552 -0.07497783][-0.063369833 -0.068254046 -0.076735333 -0.086316831 -0.098612361 -0.11365249 -0.12089931 -0.1150237 -0.10590798 -0.096802436 -0.092223771 -0.085771918 -0.079741754 -0.077770449 -0.075869896][-0.064972058 -0.067961529 -0.075063184 -0.0832178 -0.090447687 -0.097954273 -0.10399169 -0.10580965 -0.098182067 -0.08853554 -0.082629651 -0.079418048 -0.076338544 -0.075186461 -0.074540116]]...]
INFO - root - 2017-12-06 06:19:22.303120: step 26710, loss = 0.80, batch loss = 0.58 (21.1 examples/sec; 0.379 sec/batch; 32h:09m:03s remains)
INFO - root - 2017-12-06 06:19:26.218915: step 26720, loss = 0.85, batch loss = 0.64 (19.2 examples/sec; 0.416 sec/batch; 35h:22m:13s remains)
INFO - root - 2017-12-06 06:19:30.180892: step 26730, loss = 0.82, batch loss = 0.60 (20.0 examples/sec; 0.400 sec/batch; 33h:58m:11s remains)
INFO - root - 2017-12-06 06:19:33.985506: step 26740, loss = 0.96, batch loss = 0.75 (20.6 examples/sec; 0.388 sec/batch; 32h:54m:56s remains)
INFO - root - 2017-12-06 06:19:37.840084: step 26750, loss = 0.89, batch loss = 0.67 (20.7 examples/sec; 0.387 sec/batch; 32h:53m:08s remains)
INFO - root - 2017-12-06 06:19:41.676497: step 26760, loss = 0.76, batch loss = 0.55 (20.7 examples/sec; 0.387 sec/batch; 32h:54m:01s remains)
INFO - root - 2017-12-06 06:19:45.065892: step 26770, loss = 0.78, batch loss = 0.56 (29.4 examples/sec; 0.272 sec/batch; 23h:07m:59s remains)
INFO - root - 2017-12-06 06:19:48.447925: step 26780, loss = 0.78, batch loss = 0.56 (23.0 examples/sec; 0.348 sec/batch; 29h:35m:15s remains)
INFO - root - 2017-12-06 06:19:52.327197: step 26790, loss = 0.77, batch loss = 0.56 (19.3 examples/sec; 0.415 sec/batch; 35h:12m:08s remains)
INFO - root - 2017-12-06 06:19:55.951926: step 26800, loss = 0.90, batch loss = 0.68 (22.2 examples/sec; 0.360 sec/batch; 30h:32m:00s remains)
2017-12-06 06:19:56.461303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080463141 -0.082196914 -0.0854074 -0.08684244 -0.086421661 -0.089341789 -0.10126991 -0.11080796 -0.10758331 -0.10033502 -0.094565824 -0.090118267 -0.087402456 -0.084791206 -0.084658876][-0.082578808 -0.089080408 -0.099961042 -0.10592116 -0.10286391 -0.10263862 -0.12369339 -0.14725083 -0.14532757 -0.12936664 -0.11753121 -0.10707925 -0.099454924 -0.086498193 -0.083236866][-0.087454312 -0.104182 -0.1293292 -0.13192154 -0.10139946 -0.075164907 -0.090878591 -0.12321662 -0.133768 -0.13316467 -0.13217482 -0.12520224 -0.11785021 -0.092603385 -0.082011908][-0.094323695 -0.12435029 -0.15847765 -0.13842988 -0.046435438 0.036416657 0.039415739 -0.005237855 -0.035041571 -0.056843851 -0.094655424 -0.12474863 -0.12642834 -0.097454317 -0.084246583][-0.10159446 -0.14218722 -0.17345661 -0.11281118 0.057367988 0.20752826 0.24252284 0.20949608 0.17346162 0.12576625 0.032227479 -0.064869165 -0.11537991 -0.1038672 -0.07996738][-0.10946767 -0.15835856 -0.18719408 -0.099244 0.11534075 0.30848819 0.3820529 0.39611286 0.39305109 0.35419464 0.21958557 0.049323492 -0.07558386 -0.099143848 -0.081213944][-0.11338453 -0.1716958 -0.21031891 -0.12743378 0.085448794 0.27818927 0.37494725 0.43779889 0.48888 0.47330549 0.32739076 0.1326772 -0.024007671 -0.082797393 -0.078560114][-0.11717704 -0.18625787 -0.25176162 -0.21506371 -0.05125485 0.11251657 0.22603482 0.309342 0.37628052 0.37935385 0.27761963 0.12245864 -0.014616728 -0.069388859 -0.072360344][-0.11449342 -0.18945318 -0.28462684 -0.3073971 -0.22445223 -0.10737695 -0.0085407719 0.0529486 0.10190041 0.11008076 0.083288424 0.023764707 -0.03847583 -0.070278428 -0.073399305][-0.10896976 -0.17723028 -0.2832047 -0.35737082 -0.34999251 -0.29248351 -0.23024425 -0.18352675 -0.1608966 -0.1513032 -0.12373442 -0.096459948 -0.083367959 -0.081381559 -0.078350618][-0.098901041 -0.14896373 -0.24035466 -0.33850461 -0.39042467 -0.3965421 -0.37856039 -0.35949272 -0.34744558 -0.32025591 -0.25785765 -0.18045211 -0.12261316 -0.091265731 -0.080690607][-0.0932721 -0.12678623 -0.19655386 -0.29213664 -0.37841424 -0.42651546 -0.43230373 -0.41233072 -0.38470596 -0.35214964 -0.28798211 -0.20860693 -0.1435093 -0.09884014 -0.083907664][-0.0922429 -0.11489694 -0.16282186 -0.22939917 -0.28415224 -0.29823145 -0.26032117 -0.20530111 -0.18822944 -0.20155996 -0.20571053 -0.18633729 -0.14604181 -0.10850194 -0.089210406][-0.093708746 -0.11151448 -0.13378538 -0.13567555 -0.084035918 0.026005968 0.15983692 0.23805818 0.19981787 0.076907776 -0.053244695 -0.12468837 -0.1292306 -0.10814692 -0.091740675][-0.095473826 -0.10904329 -0.11113016 -0.063921183 0.071664982 0.29091188 0.52739477 0.65288329 0.59639204 0.39708787 0.16286445 -0.0024282485 -0.081351995 -0.097450458 -0.093228824]]...]
INFO - root - 2017-12-06 06:20:00.375331: step 26810, loss = 0.78, batch loss = 0.57 (21.6 examples/sec; 0.371 sec/batch; 31h:30m:48s remains)
INFO - root - 2017-12-06 06:20:04.118575: step 26820, loss = 0.87, batch loss = 0.66 (30.3 examples/sec; 0.264 sec/batch; 22h:24m:51s remains)
INFO - root - 2017-12-06 06:20:07.713043: step 26830, loss = 0.86, batch loss = 0.64 (21.6 examples/sec; 0.370 sec/batch; 31h:26m:35s remains)
INFO - root - 2017-12-06 06:20:11.283383: step 26840, loss = 0.81, batch loss = 0.60 (21.6 examples/sec; 0.370 sec/batch; 31h:26m:19s remains)
INFO - root - 2017-12-06 06:20:15.057917: step 26850, loss = 0.87, batch loss = 0.66 (21.9 examples/sec; 0.365 sec/batch; 30h:58m:44s remains)
INFO - root - 2017-12-06 06:20:18.845131: step 26860, loss = 0.82, batch loss = 0.60 (20.7 examples/sec; 0.386 sec/batch; 32h:48m:35s remains)
INFO - root - 2017-12-06 06:20:22.776466: step 26870, loss = 0.76, batch loss = 0.54 (18.7 examples/sec; 0.428 sec/batch; 36h:18m:53s remains)
INFO - root - 2017-12-06 06:20:26.237869: step 26880, loss = 0.73, batch loss = 0.51 (26.3 examples/sec; 0.304 sec/batch; 25h:50m:37s remains)
INFO - root - 2017-12-06 06:20:29.725008: step 26890, loss = 0.75, batch loss = 0.54 (22.0 examples/sec; 0.364 sec/batch; 30h:51m:33s remains)
INFO - root - 2017-12-06 06:20:33.722654: step 26900, loss = 0.77, batch loss = 0.55 (19.2 examples/sec; 0.417 sec/batch; 35h:26m:07s remains)
2017-12-06 06:20:34.273166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.18725111 -0.23547319 -0.29103553 -0.34981805 -0.40411019 -0.44334534 -0.45550433 -0.44012117 -0.3980343 -0.34029993 -0.277851 -0.22601438 -0.18713647 -0.16448981 -0.1542753][-0.26352513 -0.34479421 -0.44944617 -0.57137376 -0.6881845 -0.78981441 -0.8620519 -0.89041764 -0.85658324 -0.76919436 -0.6530087 -0.51181161 -0.38318554 -0.282388 -0.22732523][-0.50552154 -0.6472224 -0.79077238 -0.93417126 -1.0820869 -1.2330371 -1.3860567 -1.5038505 -1.5482025 -1.4983212 -1.35604 -1.1043644 -0.87761182 -0.68873215 -0.56132281][-0.65149796 -0.82291156 -0.98326695 -1.1306367 -1.2594846 -1.365649 -1.5303658 -1.6990759 -1.8171493 -1.8669195 -1.8523332 -1.7103264 -1.5327283 -1.299753 -1.0827415][-0.41187426 -0.5293448 -0.60365361 -0.65525264 -0.67701542 -0.70063347 -0.82835656 -0.95463669 -1.1232524 -1.3055073 -1.4460533 -1.5046245 -1.5117019 -1.3800557 -1.1973102][-0.10845321 -0.099780835 -0.020336099 0.11094972 0.2479603 0.366997 0.330827 0.30091658 0.1590873 -0.067492858 -0.30693951 -0.55526805 -0.72465748 -0.76700652 -0.72439522][0.22112104 0.38020402 0.6121155 0.89799404 1.234331 1.5385532 1.6301361 1.6703075 1.5751985 1.3517201 1.0521705 0.70715064 0.40200129 0.18690994 0.06392131][0.39870048 0.63779712 0.98945028 1.3956295 1.8349452 2.235975 2.4466388 2.5481095 2.4512761 2.2432408 1.9447944 1.6074134 1.3265709 1.057632 0.80593455][0.12281383 0.3690263 0.75996178 1.2243065 1.6851183 2.0740285 2.2821636 2.3350835 2.225718 2.079565 1.8682858 1.6435939 1.4454191 1.1899911 0.9507851][-0.27332136 -0.10943576 0.20121163 0.56481689 0.93125206 1.2206671 1.3772163 1.3924598 1.3015378 1.1972438 1.0263038 0.87122726 0.73973507 0.56671304 0.41980168][-0.541803 -0.53052795 -0.41973183 -0.22846395 -0.018725246 0.15461239 0.29114464 0.34749532 0.30446652 0.27165651 0.18694416 0.12371372 0.071506746 -0.012686655 -0.069282934][-0.6748445 -0.81010175 -0.91212606 -0.96137089 -0.96227103 -0.94982988 -0.89516181 -0.81363106 -0.77366817 -0.73481756 -0.70599031 -0.61669755 -0.5296976 -0.47380459 -0.42806998][-0.60098785 -0.80870426 -1.0229552 -1.203521 -1.326946 -1.3910619 -1.3668203 -1.316659 -1.2689776 -1.1431949 -1.0325871 -0.89798546 -0.77326995 -0.67954534 -0.60946208][-0.36200967 -0.52738321 -0.71739948 -0.8794713 -1.0450914 -1.1683441 -1.2154771 -1.2304671 -1.206001 -1.0380058 -0.91574818 -0.76876956 -0.62783337 -0.53430212 -0.47478163][-0.16377729 -0.22038072 -0.30960405 -0.36879906 -0.5054372 -0.60543817 -0.64351445 -0.657416 -0.62661576 -0.48963204 -0.39937037 -0.33026436 -0.29381263 -0.25832832 -0.2541886]]...]
INFO - root - 2017-12-06 06:20:38.159823: step 26910, loss = 0.76, batch loss = 0.55 (20.6 examples/sec; 0.389 sec/batch; 33h:00m:13s remains)
INFO - root - 2017-12-06 06:20:42.049507: step 26920, loss = 0.80, batch loss = 0.59 (20.1 examples/sec; 0.398 sec/batch; 33h:44m:31s remains)
INFO - root - 2017-12-06 06:20:45.949175: step 26930, loss = 0.90, batch loss = 0.68 (20.5 examples/sec; 0.391 sec/batch; 33h:11m:37s remains)
INFO - root - 2017-12-06 06:20:49.661162: step 26940, loss = 0.77, batch loss = 0.56 (26.6 examples/sec; 0.301 sec/batch; 25h:33m:28s remains)
INFO - root - 2017-12-06 06:20:53.452283: step 26950, loss = 0.85, batch loss = 0.63 (20.7 examples/sec; 0.386 sec/batch; 32h:46m:56s remains)
INFO - root - 2017-12-06 06:20:57.357024: step 26960, loss = 0.76, batch loss = 0.55 (20.3 examples/sec; 0.394 sec/batch; 33h:26m:52s remains)
INFO - root - 2017-12-06 06:21:01.153407: step 26970, loss = 0.72, batch loss = 0.51 (21.3 examples/sec; 0.375 sec/batch; 31h:48m:11s remains)
INFO - root - 2017-12-06 06:21:04.835436: step 26980, loss = 0.83, batch loss = 0.61 (25.5 examples/sec; 0.313 sec/batch; 26h:35m:26s remains)
INFO - root - 2017-12-06 06:21:08.425879: step 26990, loss = 0.84, batch loss = 0.63 (22.0 examples/sec; 0.364 sec/batch; 30h:52m:57s remains)
INFO - root - 2017-12-06 06:21:12.187930: step 27000, loss = 0.83, batch loss = 0.62 (20.8 examples/sec; 0.385 sec/batch; 32h:39m:31s remains)
2017-12-06 06:21:12.669146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069484472 -0.068972707 -0.068752877 -0.068839423 -0.068753809 -0.068525121 -0.068329379 -0.068127371 -0.068081811 -0.068163939 -0.068283677 -0.068453714 -0.068795621 -0.069171451 -0.069732539][-0.069191813 -0.068115443 -0.0673722 -0.067173228 -0.066920966 -0.066424817 -0.066051811 -0.065869205 -0.066018328 -0.066378005 -0.06684687 -0.067353413 -0.067922443 -0.068523586 -0.069227174][-0.068663336 -0.066941313 -0.065609641 -0.064829588 -0.064247645 -0.063648015 -0.06302844 -0.062996 -0.063419804 -0.064123206 -0.065032557 -0.066075578 -0.067026258 -0.067900471 -0.068736888][-0.067804046 -0.065485291 -0.063548028 -0.062116943 -0.061345723 -0.06052082 -0.059861682 -0.06015899 -0.061114766 -0.062289618 -0.063552789 -0.064962052 -0.066201642 -0.067208871 -0.068122089][-0.067106567 -0.064465806 -0.061895028 -0.059831515 -0.058170449 -0.056955714 -0.056388162 -0.0571119 -0.058827139 -0.060638592 -0.062254556 -0.063918419 -0.065405086 -0.0665786 -0.067618757][-0.066937745 -0.064099245 -0.061111994 -0.0582113 -0.055339396 -0.053098083 -0.052555356 -0.054126188 -0.056761291 -0.059217982 -0.0613401 -0.063384518 -0.0650682 -0.066206425 -0.06736739][-0.067281678 -0.064322233 -0.0611294 -0.058020044 -0.05440066 -0.050979044 -0.050075654 -0.052872702 -0.057132211 -0.060284331 -0.062166423 -0.063801974 -0.065165043 -0.066121936 -0.067381918][-0.068036683 -0.065436654 -0.062564068 -0.059928648 -0.05709694 -0.054206673 -0.05300628 -0.05662699 -0.061432447 -0.064765334 -0.06622266 -0.066503905 -0.06634979 -0.066518448 -0.0675384][-0.069151334 -0.06726107 -0.064974554 -0.063217014 -0.062065452 -0.06166121 -0.062556215 -0.065618336 -0.068900213 -0.070918962 -0.071503632 -0.070420638 -0.068531618 -0.067470223 -0.067998312][-0.070190623 -0.06883467 -0.067295179 -0.066037506 -0.065585 -0.066647038 -0.069127992 -0.072338849 -0.07460326 -0.075590611 -0.07538864 -0.073443808 -0.070671424 -0.06866537 -0.068632849][-0.070678249 -0.06978897 -0.068941034 -0.068012305 -0.06757462 -0.068574108 -0.07142368 -0.074656814 -0.07654231 -0.077135965 -0.076852649 -0.074816108 -0.0719423 -0.069799826 -0.069362372][-0.0706381 -0.069923647 -0.069378719 -0.0687844 -0.068587348 -0.069492213 -0.071713492 -0.074299052 -0.075974077 -0.076550715 -0.076055832 -0.0742888 -0.072012916 -0.070344113 -0.069792241][-0.070377313 -0.069685191 -0.068997912 -0.068463311 -0.068382382 -0.069045529 -0.070664644 -0.072534226 -0.073908888 -0.074404716 -0.073955864 -0.072611146 -0.071062669 -0.070064172 -0.069829151][-0.070255935 -0.069591857 -0.068812892 -0.068098024 -0.067772873 -0.06807676 -0.069133043 -0.070465133 -0.071601182 -0.072057888 -0.071817 -0.071016289 -0.070148587 -0.069775075 -0.069911674][-0.07007663 -0.069574594 -0.068993285 -0.068418659 -0.068012506 -0.068084896 -0.068576276 -0.069406331 -0.07023596 -0.070673011 -0.07063929 -0.070294432 -0.069937147 -0.069899075 -0.070123561]]...]
INFO - root - 2017-12-06 06:21:16.484218: step 27010, loss = 0.74, batch loss = 0.53 (20.3 examples/sec; 0.394 sec/batch; 33h:27m:52s remains)
INFO - root - 2017-12-06 06:21:20.389997: step 27020, loss = 0.82, batch loss = 0.60 (19.6 examples/sec; 0.408 sec/batch; 34h:35m:35s remains)
INFO - root - 2017-12-06 06:21:24.250802: step 27030, loss = 0.78, batch loss = 0.57 (19.8 examples/sec; 0.404 sec/batch; 34h:17m:58s remains)
INFO - root - 2017-12-06 06:21:28.187651: step 27040, loss = 0.79, batch loss = 0.58 (20.4 examples/sec; 0.391 sec/batch; 33h:12m:43s remains)
INFO - root - 2017-12-06 06:21:31.557781: step 27050, loss = 0.82, batch loss = 0.60 (25.3 examples/sec; 0.316 sec/batch; 26h:48m:44s remains)
INFO - root - 2017-12-06 06:21:35.181396: step 27060, loss = 0.79, batch loss = 0.58 (21.3 examples/sec; 0.376 sec/batch; 31h:52m:47s remains)
INFO - root - 2017-12-06 06:21:38.960626: step 27070, loss = 0.83, batch loss = 0.62 (21.4 examples/sec; 0.374 sec/batch; 31h:43m:20s remains)
INFO - root - 2017-12-06 06:21:42.530847: step 27080, loss = 0.76, batch loss = 0.54 (20.8 examples/sec; 0.385 sec/batch; 32h:37m:41s remains)
INFO - root - 2017-12-06 06:21:46.333311: step 27090, loss = 0.82, batch loss = 0.61 (21.7 examples/sec; 0.369 sec/batch; 31h:16m:19s remains)
INFO - root - 2017-12-06 06:21:50.116288: step 27100, loss = 0.73, batch loss = 0.52 (20.9 examples/sec; 0.384 sec/batch; 32h:32m:46s remains)
2017-12-06 06:21:50.677291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12693815 -0.13221811 -0.14112696 -0.15848203 -0.19285226 -0.23851046 -0.28317061 -0.31697372 -0.31715775 -0.30493975 -0.26295325 -0.21218166 -0.16618997 -0.14362374 -0.13591634][-0.13070652 -0.13837963 -0.14381377 -0.13925354 -0.13143423 -0.12121531 -0.14227858 -0.19508411 -0.25145063 -0.27963477 -0.28248051 -0.24445489 -0.19144481 -0.15046406 -0.13716172][-0.13012989 -0.13844526 -0.14642431 -0.15018848 -0.14764203 -0.11137801 -0.086186513 -0.10050301 -0.15960634 -0.21107239 -0.23782143 -0.21177557 -0.18370597 -0.14131288 -0.1204378][-0.12367941 -0.11859551 -0.11641789 -0.11024874 -0.10566057 -0.056631457 -0.018310897 0.038637675 0.026211806 -0.024168611 -0.092226095 -0.13441047 -0.14400584 -0.11954235 -0.11540937][-0.11891976 -0.11206558 -0.10929386 -0.08603622 -0.073033109 -0.013883531 0.042434387 0.082594432 0.10562836 0.066721223 -0.0066067427 -0.070844159 -0.10857839 -0.10757644 -0.10533771][-0.11872147 -0.11689668 -0.10596332 -0.09547016 -0.086138748 -0.051220186 -0.020944707 0.0063443184 0.025707051 -0.008239381 -0.036494385 -0.083429195 -0.10833144 -0.10561374 -0.10140391][-0.11963029 -0.11692882 -0.10527865 -0.090328492 -0.087703705 -0.0643408 -0.051865332 -0.069868594 -0.060874961 -0.05797378 -0.057855267 -0.080726594 -0.101468 -0.107913 -0.10970402][-0.12144029 -0.11444151 -0.11889201 -0.10435992 -0.11904985 -0.11680501 -0.093962178 -0.083975226 -0.097647727 -0.1185786 -0.10894302 -0.11208671 -0.12172439 -0.12071816 -0.12421058][-0.12372664 -0.12084951 -0.12713933 -0.12229358 -0.13429357 -0.14623515 -0.15376887 -0.15210912 -0.17770511 -0.21826398 -0.21644312 -0.20971771 -0.18014416 -0.15156044 -0.13709635][-0.12780347 -0.12707432 -0.12679461 -0.13276671 -0.13551919 -0.13903528 -0.14921482 -0.16911025 -0.201822 -0.2547881 -0.30840534 -0.32268474 -0.29347879 -0.22595015 -0.17519835][-0.12890151 -0.12705868 -0.12673706 -0.12653765 -0.14272848 -0.1586304 -0.15589048 -0.15333053 -0.13587579 -0.14886497 -0.19473889 -0.27835 -0.32287067 -0.28688556 -0.24059209][-0.13469641 -0.13295148 -0.13037604 -0.13016632 -0.14086848 -0.16670088 -0.17202659 -0.17121452 -0.18644279 -0.20154148 -0.20464665 -0.24237078 -0.2728422 -0.27789927 -0.27809575][-0.15862431 -0.16110429 -0.15444577 -0.15098634 -0.15724581 -0.19362503 -0.22230634 -0.19934137 -0.17432347 -0.19982922 -0.28172055 -0.38049307 -0.49233243 -0.59025049 -0.66110045][-0.16674414 -0.18465868 -0.18513754 -0.18607789 -0.18699946 -0.20274737 -0.24115238 -0.22733143 -0.21594658 -0.19775805 -0.23500359 -0.40850559 -0.74089503 -1.0496947 -1.2501738][-0.14815369 -0.16812477 -0.18634295 -0.20071374 -0.21296968 -0.22117281 -0.24356443 -0.23459503 -0.27096429 -0.30778617 -0.37542224 -0.53995162 -0.85501367 -1.1773969 -1.4929104]]...]
INFO - root - 2017-12-06 06:21:54.372157: step 27110, loss = 0.84, batch loss = 0.63 (19.5 examples/sec; 0.411 sec/batch; 34h:49m:43s remains)
INFO - root - 2017-12-06 06:21:57.804411: step 27120, loss = 0.75, batch loss = 0.54 (28.1 examples/sec; 0.285 sec/batch; 24h:09m:41s remains)
INFO - root - 2017-12-06 06:22:01.554131: step 27130, loss = 0.83, batch loss = 0.61 (21.3 examples/sec; 0.376 sec/batch; 31h:51m:46s remains)
INFO - root - 2017-12-06 06:22:05.449120: step 27140, loss = 0.86, batch loss = 0.65 (20.7 examples/sec; 0.386 sec/batch; 32h:42m:48s remains)
INFO - root - 2017-12-06 06:22:09.329829: step 27150, loss = 0.85, batch loss = 0.63 (20.6 examples/sec; 0.389 sec/batch; 32h:58m:49s remains)
INFO - root - 2017-12-06 06:22:12.796612: step 27160, loss = 0.87, batch loss = 0.66 (24.4 examples/sec; 0.328 sec/batch; 27h:47m:55s remains)
INFO - root - 2017-12-06 06:22:16.275912: step 27170, loss = 0.78, batch loss = 0.57 (20.7 examples/sec; 0.386 sec/batch; 32h:42m:18s remains)
INFO - root - 2017-12-06 06:22:20.221798: step 27180, loss = 0.76, batch loss = 0.55 (21.4 examples/sec; 0.374 sec/batch; 31h:41m:44s remains)
INFO - root - 2017-12-06 06:22:23.995220: step 27190, loss = 0.81, batch loss = 0.60 (21.4 examples/sec; 0.375 sec/batch; 31h:46m:12s remains)
INFO - root - 2017-12-06 06:22:27.973709: step 27200, loss = 0.87, batch loss = 0.65 (20.6 examples/sec; 0.389 sec/batch; 32h:58m:20s remains)
2017-12-06 06:22:28.463774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072918333 -0.072460622 -0.072548166 -0.072709724 -0.073410831 -0.073819309 -0.073707566 -0.073241845 -0.072512157 -0.071817771 -0.071148 -0.070673704 -0.070417687 -0.070398048 -0.07075835][-0.072598867 -0.072478592 -0.072779715 -0.0727067 -0.073265657 -0.073774941 -0.07363949 -0.072829455 -0.071799539 -0.070949525 -0.070050746 -0.069350265 -0.068988994 -0.068976544 -0.06936495][-0.074105561 -0.074631706 -0.074866772 -0.074325681 -0.074697942 -0.074349374 -0.07359013 -0.072789311 -0.071520962 -0.070232041 -0.069008715 -0.068165988 -0.067703955 -0.067655645 -0.068153046][-0.075854816 -0.076613665 -0.076528326 -0.075787336 -0.07562381 -0.074679069 -0.073364362 -0.07226786 -0.070962071 -0.069488317 -0.06807857 -0.067040533 -0.066507578 -0.0664787 -0.066968173][-0.077119514 -0.077742651 -0.077443652 -0.076532185 -0.075683877 -0.074064605 -0.071876906 -0.070729643 -0.069558024 -0.068186738 -0.066830739 -0.06575767 -0.065304272 -0.065449633 -0.066263139][-0.0779453 -0.078626096 -0.078509822 -0.077376872 -0.075969458 -0.072772481 -0.068753511 -0.067188248 -0.066721514 -0.066138893 -0.065625079 -0.064560443 -0.064357668 -0.064679414 -0.0657852][-0.077997357 -0.078802593 -0.079183772 -0.0790973 -0.07776735 -0.073595509 -0.068541139 -0.067181237 -0.066731393 -0.06594412 -0.065396965 -0.064465083 -0.064017236 -0.064631552 -0.065860443][-0.077620447 -0.078358367 -0.079410955 -0.080034748 -0.079852283 -0.076417312 -0.072562993 -0.071218625 -0.069979981 -0.06824588 -0.066558257 -0.065505758 -0.065010481 -0.065132946 -0.066020444][-0.076754592 -0.077572666 -0.079352453 -0.080802321 -0.081012353 -0.079404987 -0.077003181 -0.074925192 -0.072665609 -0.07006669 -0.0677859 -0.066332221 -0.065234751 -0.065425925 -0.066487767][-0.075140327 -0.076207817 -0.078439571 -0.080398172 -0.080985032 -0.080511153 -0.079555944 -0.07784564 -0.075210318 -0.071830608 -0.06930913 -0.066788793 -0.065060608 -0.064776212 -0.066020422][-0.073838979 -0.074888654 -0.077442221 -0.079359591 -0.080603912 -0.08075501 -0.080421604 -0.079326808 -0.077427119 -0.074596077 -0.071353287 -0.068060338 -0.065407515 -0.064555511 -0.065614209][-0.07331422 -0.074334607 -0.076796584 -0.078910358 -0.080410846 -0.081205726 -0.080988467 -0.080846079 -0.079789132 -0.0779641 -0.075233117 -0.071612507 -0.06781783 -0.065703474 -0.066307239][-0.073451892 -0.074139193 -0.075950675 -0.077802673 -0.079647116 -0.081147105 -0.081438549 -0.0817162 -0.081548236 -0.080995865 -0.079398856 -0.075975314 -0.072283618 -0.069291741 -0.068548717][-0.074081816 -0.074444868 -0.075418584 -0.0769006 -0.078717 -0.08039254 -0.081236526 -0.08186724 -0.082038745 -0.081994705 -0.080886245 -0.078279637 -0.0748339 -0.071873173 -0.070537113][-0.074952863 -0.075035729 -0.075636961 -0.076743878 -0.078284673 -0.079584569 -0.080592811 -0.081606172 -0.0820318 -0.082018428 -0.081020415 -0.079131365 -0.0763705 -0.073691562 -0.071966678]]...]
INFO - root - 2017-12-06 06:22:31.395268: step 27210, loss = 0.76, batch loss = 0.54 (29.4 examples/sec; 0.272 sec/batch; 23h:06m:11s remains)
INFO - root - 2017-12-06 06:22:34.344949: step 27220, loss = 0.80, batch loss = 0.59 (28.2 examples/sec; 0.283 sec/batch; 24h:01m:39s remains)
INFO - root - 2017-12-06 06:22:37.464282: step 27230, loss = 0.84, batch loss = 0.62 (25.1 examples/sec; 0.318 sec/batch; 26h:58m:39s remains)
INFO - root - 2017-12-06 06:22:40.878076: step 27240, loss = 0.88, batch loss = 0.67 (23.7 examples/sec; 0.337 sec/batch; 28h:36m:49s remains)
INFO - root - 2017-12-06 06:22:44.444762: step 27250, loss = 0.80, batch loss = 0.59 (25.4 examples/sec; 0.315 sec/batch; 26h:41m:56s remains)
INFO - root - 2017-12-06 06:22:48.158781: step 27260, loss = 0.75, batch loss = 0.54 (25.2 examples/sec; 0.317 sec/batch; 26h:54m:03s remains)
INFO - root - 2017-12-06 06:22:51.781402: step 27270, loss = 0.76, batch loss = 0.55 (17.8 examples/sec; 0.448 sec/batch; 38h:00m:22s remains)
INFO - root - 2017-12-06 06:22:57.267947: step 27280, loss = 0.84, batch loss = 0.62 (13.7 examples/sec; 0.583 sec/batch; 49h:28m:09s remains)
INFO - root - 2017-12-06 06:23:03.422696: step 27290, loss = 0.77, batch loss = 0.55 (13.0 examples/sec; 0.614 sec/batch; 52h:04m:41s remains)
INFO - root - 2017-12-06 06:23:09.411671: step 27300, loss = 0.73, batch loss = 0.52 (12.6 examples/sec; 0.635 sec/batch; 53h:50m:06s remains)
2017-12-06 06:23:10.166887: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30368069 0.2425831 0.033500642 -0.076803915 -0.1896226 -0.25406444 -0.31067529 -0.28852248 -0.23696807 -0.16080056 -0.086420089 0.029510513 0.10491286 0.20114672 0.28251079][0.10130282 0.05840224 -0.10626111 -0.20726115 -0.27806243 -0.27509481 -0.28170818 -0.240965 -0.19945717 -0.1598098 -0.085126862 -0.028540649 0.033570558 0.13845348 0.23335174][-0.047031537 -0.16688313 -0.30626911 -0.38111532 -0.43907887 -0.4395788 -0.47017655 -0.46535522 -0.46944126 -0.41958666 -0.33437794 -0.25381869 -0.138191 -0.03743159 0.11448824][-0.25363302 -0.37343797 -0.428137 -0.47669044 -0.535267 -0.53409266 -0.55546725 -0.6145041 -0.66096425 -0.648944 -0.5872246 -0.50205344 -0.39465615 -0.2634531 -0.10413433][-0.46961281 -0.52647316 -0.54689085 -0.55856353 -0.56240356 -0.554958 -0.56981492 -0.59977615 -0.63585734 -0.638522 -0.59964347 -0.54049903 -0.46708694 -0.41234666 -0.344044][-0.26672649 -0.24636562 -0.20607376 -0.17648852 -0.1087855 -0.08251366 -0.014568023 -0.0065795332 -0.01939971 -0.088211678 -0.14637721 -0.19828093 -0.24591796 -0.2753163 -0.29356492][0.25550282 0.31613654 0.36471155 0.4697648 0.61408937 0.69136333 0.773347 0.80386579 0.79009438 0.66213417 0.48331568 0.27653179 0.070708111 -0.051645022 -0.11795256][0.4537259 0.51373982 0.63308871 0.84762096 1.0692462 1.1970348 1.2968117 1.3203175 1.2652283 1.0721438 0.81809008 0.54242742 0.27126783 0.0594694 -0.057983752][0.23111486 0.36425632 0.49898145 0.67333317 0.81341791 0.91813695 0.94485855 0.9378227 0.850489 0.69236314 0.51676738 0.32336655 0.14223415 0.028148226 -0.030394517][0.064450309 0.1441455 0.23685953 0.35478491 0.40822911 0.39470044 0.31515321 0.26951274 0.20672652 0.15991476 0.13676813 0.090661079 0.029375218 -0.054006815 -0.086509138][-0.095006444 -0.1349967 -0.15951905 -0.18528567 -0.23839013 -0.31755713 -0.41680783 -0.42134002 -0.39386892 -0.31456494 -0.21339548 -0.1458652 -0.10583551 -0.13926293 -0.14120854][-0.14349671 -0.35688573 -0.48692146 -0.58697331 -0.68954718 -0.81710434 -0.868539 -0.79924119 -0.69840729 -0.54705441 -0.39392343 -0.29956341 -0.26102728 -0.23539932 -0.22490716][0.11020052 -0.22898579 -0.46392056 -0.59896731 -0.71641779 -0.81327236 -0.83471394 -0.77866077 -0.66386724 -0.48926696 -0.38399345 -0.29623485 -0.2655195 -0.23803985 -0.25188756][0.31411818 -0.077294879 -0.35786092 -0.50670218 -0.65053618 -0.65370643 -0.59433341 -0.49471354 -0.40020335 -0.28007495 -0.21989855 -0.18321893 -0.17457625 -0.16675273 -0.18206212][0.3378253 0.078288317 -0.10546111 -0.18009302 -0.23409922 -0.20650555 -0.12485089 -0.060640477 -0.032655545 -0.030296762 -0.052700508 -0.096066713 -0.11648642 -0.10960817 -0.12847929]]...]
INFO - root - 2017-12-06 06:23:16.227975: step 27310, loss = 0.79, batch loss = 0.58 (13.4 examples/sec; 0.599 sec/batch; 50h:46m:24s remains)
INFO - root - 2017-12-06 06:23:22.301571: step 27320, loss = 0.81, batch loss = 0.59 (12.8 examples/sec; 0.625 sec/batch; 52h:56m:51s remains)
INFO - root - 2017-12-06 06:23:28.157292: step 27330, loss = 0.81, batch loss = 0.59 (13.4 examples/sec; 0.599 sec/batch; 50h:44m:07s remains)
INFO - root - 2017-12-06 06:23:34.340906: step 27340, loss = 0.76, batch loss = 0.55 (13.3 examples/sec; 0.603 sec/batch; 51h:05m:42s remains)
INFO - root - 2017-12-06 06:23:40.508944: step 27350, loss = 0.89, batch loss = 0.67 (12.6 examples/sec; 0.633 sec/batch; 53h:41m:15s remains)
INFO - root - 2017-12-06 06:23:46.602809: step 27360, loss = 0.72, batch loss = 0.50 (13.3 examples/sec; 0.602 sec/batch; 51h:02m:08s remains)
INFO - root - 2017-12-06 06:23:52.777500: step 27370, loss = 0.93, batch loss = 0.72 (12.9 examples/sec; 0.620 sec/batch; 52h:30m:38s remains)
INFO - root - 2017-12-06 06:23:58.697167: step 27380, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 55h:20m:11s remains)
INFO - root - 2017-12-06 06:24:04.895779: step 27390, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.638 sec/batch; 54h:06m:50s remains)
INFO - root - 2017-12-06 06:24:11.094266: step 27400, loss = 0.87, batch loss = 0.65 (12.5 examples/sec; 0.639 sec/batch; 54h:07m:15s remains)
2017-12-06 06:24:11.668498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0833815 -0.067276172 -0.049104974 -0.036092233 -0.028715137 -0.03138544 -0.039943594 -0.047875326 -0.052879289 -0.051050205 -0.046897281 -0.04586814 -0.045346703 -0.048003316 -0.054851308][-0.083378419 -0.070136309 -0.049412817 -0.033932194 -0.025006115 -0.019345179 -0.020825095 -0.024739847 -0.030374847 -0.033161253 -0.035617962 -0.035852674 -0.036729343 -0.03892877 -0.045995891][-0.081937671 -0.068506107 -0.053448688 -0.040228315 -0.02726607 -0.019448489 -0.017294884 -0.020592324 -0.026892506 -0.034396909 -0.040024374 -0.042131916 -0.0437646 -0.047167219 -0.052425042][-0.081020772 -0.068805747 -0.055743311 -0.044615325 -0.037572172 -0.032007825 -0.028156057 -0.028978005 -0.032851979 -0.039356183 -0.042524397 -0.046620823 -0.049971502 -0.053057671 -0.055981271][-0.082444161 -0.071998008 -0.060155559 -0.051513847 -0.045557674 -0.040001225 -0.038673252 -0.039213978 -0.04171655 -0.045370776 -0.049467817 -0.054178648 -0.05685132 -0.058197748 -0.059211258][-0.080168054 -0.072099417 -0.062048689 -0.054017026 -0.048065837 -0.043408148 -0.042319521 -0.043903787 -0.046933651 -0.049908023 -0.05240576 -0.054877013 -0.055321671 -0.056108657 -0.05697839][-0.075082988 -0.067152023 -0.057734746 -0.051006567 -0.047113646 -0.044232473 -0.043923818 -0.046178579 -0.04997085 -0.052752469 -0.055230144 -0.053503659 -0.056006167 -0.053950943 -0.056174032][-0.072110407 -0.066129223 -0.058006849 -0.050517865 -0.0469568 -0.044127818 -0.044103306 -0.046932209 -0.050141409 -0.052810743 -0.054549918 -0.056129713 -0.057563629 -0.05944033 -0.060959067][-0.066424347 -0.063523278 -0.058433052 -0.050052721 -0.048529904 -0.0437719 -0.044777472 -0.04801267 -0.050737355 -0.052678168 -0.053769425 -0.055029914 -0.05699848 -0.059602991 -0.061448194][-0.065154746 -0.061732434 -0.061818015 -0.054400954 -0.054073654 -0.04924123 -0.047995087 -0.049108412 -0.050578039 -0.05155525 -0.05158399 -0.051980566 -0.0536324 -0.057671715 -0.06276647][-0.068007857 -0.066624478 -0.064633653 -0.061451636 -0.058844559 -0.055905014 -0.052986629 -0.052460004 -0.052803379 -0.053892557 -0.054244962 -0.055060185 -0.056800727 -0.060818903 -0.067411117][-0.072379783 -0.070722423 -0.068455942 -0.06582281 -0.063484922 -0.060309026 -0.057570379 -0.056205224 -0.055767402 -0.057267431 -0.058805045 -0.06033035 -0.062591054 -0.06729506 -0.0754499][-0.078582369 -0.077894866 -0.076434515 -0.074039727 -0.0718565 -0.067804053 -0.065556675 -0.063882545 -0.063163862 -0.0653756 -0.066971347 -0.06947954 -0.073150508 -0.078466818 -0.08747898][-0.090826474 -0.089037389 -0.08722914 -0.085787758 -0.084744342 -0.08148317 -0.078462772 -0.075525858 -0.0768348 -0.077764943 -0.079137921 -0.082476959 -0.087059841 -0.093151785 -0.10106099][-0.10865849 -0.10490952 -0.10212588 -0.10032717 -0.10016594 -0.099202067 -0.097920172 -0.095671363 -0.094798841 -0.094889551 -0.096052438 -0.099157095 -0.10372017 -0.10922859 -0.11482148]]...]
INFO - root - 2017-12-06 06:24:17.098271: step 27410, loss = 0.75, batch loss = 0.53 (13.8 examples/sec; 0.579 sec/batch; 49h:05m:32s remains)
INFO - root - 2017-12-06 06:24:22.493043: step 27420, loss = 0.81, batch loss = 0.60 (14.7 examples/sec; 0.545 sec/batch; 46h:10m:47s remains)
INFO - root - 2017-12-06 06:24:28.258893: step 27430, loss = 0.80, batch loss = 0.59 (13.4 examples/sec; 0.597 sec/batch; 50h:32m:56s remains)
INFO - root - 2017-12-06 06:24:34.116589: step 27440, loss = 0.81, batch loss = 0.59 (13.7 examples/sec; 0.584 sec/batch; 49h:29m:00s remains)
INFO - root - 2017-12-06 06:24:40.176555: step 27450, loss = 0.93, batch loss = 0.72 (12.9 examples/sec; 0.620 sec/batch; 52h:30m:11s remains)
INFO - root - 2017-12-06 06:24:46.171219: step 27460, loss = 0.80, batch loss = 0.58 (13.1 examples/sec; 0.610 sec/batch; 51h:43m:26s remains)
INFO - root - 2017-12-06 06:24:51.976583: step 27470, loss = 0.86, batch loss = 0.64 (13.2 examples/sec; 0.605 sec/batch; 51h:13m:17s remains)
INFO - root - 2017-12-06 06:24:58.032491: step 27480, loss = 0.84, batch loss = 0.63 (13.0 examples/sec; 0.616 sec/batch; 52h:11m:09s remains)
INFO - root - 2017-12-06 06:25:04.160434: step 27490, loss = 0.93, batch loss = 0.72 (13.1 examples/sec; 0.613 sec/batch; 51h:54m:22s remains)
INFO - root - 2017-12-06 06:25:10.262816: step 27500, loss = 0.87, batch loss = 0.65 (13.3 examples/sec; 0.602 sec/batch; 50h:57m:50s remains)
2017-12-06 06:25:10.938874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050560832 -0.050165292 -0.050953738 -0.052310642 -0.053781059 -0.05495831 -0.055496365 -0.055465821 -0.055244174 -0.055491015 -0.0566217 -0.058758158 -0.061550565 -0.064397618 -0.067232139][-0.046407159 -0.045738854 -0.046863861 -0.049046695 -0.051530857 -0.053856872 -0.055444967 -0.056109484 -0.056228604 -0.056482006 -0.0573792 -0.059047781 -0.06126143 -0.063443691 -0.065676272][-0.042250782 -0.041369747 -0.04299299 -0.046103071 -0.049669564 -0.05305016 -0.055648841 -0.057188634 -0.057792902 -0.058118276 -0.058723986 -0.059813313 -0.061285924 -0.062764928 -0.064330891][-0.038416211 -0.0375373 -0.039663464 -0.043440774 -0.047884181 -0.052052055 -0.055612117 -0.058163028 -0.059474111 -0.060053304 -0.060442269 -0.060912605 -0.06165259 -0.06241028 -0.063299924][-0.035655703 -0.035008542 -0.037383884 -0.041539997 -0.046409596 -0.051091716 -0.055288583 -0.05867222 -0.060720775 -0.061597049 -0.061897568 -0.06198585 -0.062124766 -0.062186137 -0.062398948][-0.034834392 -0.034313265 -0.036692049 -0.040831722 -0.045773841 -0.050702084 -0.055212002 -0.059005138 -0.061663117 -0.062880307 -0.063197941 -0.063074954 -0.062765479 -0.062259234 -0.061964691][-0.036277175 -0.035617378 -0.037826754 -0.041592225 -0.046265908 -0.051056635 -0.055546664 -0.059439011 -0.062499426 -0.0641642 -0.064702109 -0.064564042 -0.063989572 -0.063109249 -0.06248527][-0.039647646 -0.038884513 -0.040734317 -0.043933503 -0.0479943 -0.05230087 -0.056471344 -0.060271055 -0.063389726 -0.065345019 -0.066112995 -0.066067174 -0.065464608 -0.064468674 -0.06375967][-0.04446613 -0.043741528 -0.04516596 -0.047610108 -0.050810825 -0.054345295 -0.057921018 -0.061320394 -0.064155154 -0.066099674 -0.067025051 -0.067123443 -0.066669628 -0.065795541 -0.06514845][-0.049879424 -0.049271189 -0.05017693 -0.051854786 -0.054149777 -0.056746557 -0.05953373 -0.062272444 -0.064644456 -0.066312239 -0.067213953 -0.0674151 -0.067109719 -0.066406943 -0.065892234][-0.055138554 -0.054479942 -0.054992341 -0.055977408 -0.057426907 -0.059136249 -0.061065037 -0.06302169 -0.064808637 -0.0660951 -0.066820435 -0.067034975 -0.0668776 -0.0664088 -0.066096231][-0.059421428 -0.058816925 -0.059086513 -0.059563883 -0.060322311 -0.061292 -0.062406838 -0.06365861 -0.064816244 -0.06569472 -0.066222847 -0.066447459 -0.066449359 -0.066263482 -0.066211253][-0.062577948 -0.06201309 -0.06215252 -0.062335748 -0.062622957 -0.063015327 -0.063524485 -0.064179748 -0.064800538 -0.065291755 -0.065636069 -0.065858245 -0.066001028 -0.066024087 -0.066159524][-0.064450264 -0.0639082 -0.063947856 -0.063961037 -0.063990146 -0.064043723 -0.064180553 -0.064414844 -0.064665809 -0.064890012 -0.0650708 -0.065240875 -0.065446079 -0.065597966 -0.065842152][-0.065453261 -0.064834237 -0.064772978 -0.064718649 -0.064658232 -0.064571686 -0.064521462 -0.064560235 -0.06462355 -0.064665183 -0.064706445 -0.064785071 -0.064937919 -0.065097205 -0.065394074]]...]
INFO - root - 2017-12-06 06:25:17.068443: step 27510, loss = 0.80, batch loss = 0.58 (13.7 examples/sec; 0.583 sec/batch; 49h:23m:42s remains)
INFO - root - 2017-12-06 06:25:23.201030: step 27520, loss = 0.85, batch loss = 0.64 (13.0 examples/sec; 0.617 sec/batch; 52h:17m:13s remains)
INFO - root - 2017-12-06 06:25:29.301681: step 27530, loss = 0.90, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 53h:11m:59s remains)
INFO - root - 2017-12-06 06:25:35.525653: step 27540, loss = 0.87, batch loss = 0.65 (13.1 examples/sec; 0.612 sec/batch; 51h:51m:43s remains)
INFO - root - 2017-12-06 06:25:41.735135: step 27550, loss = 0.84, batch loss = 0.63 (12.9 examples/sec; 0.619 sec/batch; 52h:28m:35s remains)
INFO - root - 2017-12-06 06:25:47.810286: step 27560, loss = 0.74, batch loss = 0.53 (12.6 examples/sec; 0.636 sec/batch; 53h:52m:00s remains)
INFO - root - 2017-12-06 06:25:53.755888: step 27570, loss = 0.85, batch loss = 0.64 (12.8 examples/sec; 0.623 sec/batch; 52h:46m:57s remains)
INFO - root - 2017-12-06 06:25:59.956429: step 27580, loss = 0.85, batch loss = 0.64 (12.9 examples/sec; 0.620 sec/batch; 52h:30m:37s remains)
INFO - root - 2017-12-06 06:26:06.062284: step 27590, loss = 0.87, batch loss = 0.65 (13.1 examples/sec; 0.612 sec/batch; 51h:48m:51s remains)
INFO - root - 2017-12-06 06:26:12.272415: step 27600, loss = 0.70, batch loss = 0.49 (13.3 examples/sec; 0.603 sec/batch; 51h:03m:14s remains)
2017-12-06 06:26:12.850817: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2791703 0.56390864 0.037697531 -0.16735005 -0.28922936 -0.029382646 0.45180395 0.51069582 0.50413662 0.31609887 0.40688574 0.16878015 -0.26400971 -0.95657068 -1.278035][1.6111994 1.2636896 0.96465611 0.51692528 0.022086881 0.22187591 0.47122535 0.6941365 0.75195062 0.58991653 0.08924035 -0.0903415 -0.40936118 -1.2802471 -2.0086982][0.45649889 0.66216654 0.62734914 0.43299767 0.24955598 0.30425382 0.6978088 0.87916255 0.68748087 0.43514636 0.053255297 -0.5645541 -1.2854971 -1.5650485 -2.0625291][-0.24018326 -0.085448138 -0.20880526 -0.16949591 0.030926608 -0.084036827 0.1697444 0.20685837 0.26763648 -0.16092595 -0.0694641 -0.54780793 -1.0468963 -1.6335895 -1.8289602][-0.40925279 -0.11663528 0.22631687 0.53678715 0.56040019 0.50464016 0.19992924 0.10006779 0.19989166 0.20233387 0.069098555 0.027576625 -0.38623253 -0.85878897 -1.1796292][0.55694085 0.70828217 0.68537343 0.93747056 1.4278184 1.3958536 1.1175611 0.69723541 0.86908406 0.98087168 0.75201315 0.5776428 0.5232113 0.18643701 -0.34162715][1.0494643 1.0940812 1.2097762 1.2256413 1.2765709 1.3565828 1.5188024 1.5489368 1.6700066 1.381144 1.5364269 1.475574 1.1036495 0.967852 0.51922578][1.0087098 0.84668541 0.53464431 0.78906244 0.78639209 0.99298882 1.1370393 1.3825352 1.4929914 1.3368367 1.3717196 1.2736382 1.3396305 0.80576587 0.40201557][0.9896735 0.23701316 0.055420123 0.056516103 -0.15858352 0.098093204 0.36028272 0.31674767 0.095140032 0.10527238 -0.12922941 0.13393831 0.24344292 -0.00075123459 -0.018983051][0.44480583 -0.37103385 -0.78451091 -0.80095983 -0.85027 -0.99377412 -1.1533 -1.1064123 -1.0985305 -1.30061 -1.3501633 -0.9945128 -0.78041714 -0.73198855 -0.46576056][-0.060613971 -0.46401951 -0.60127544 -0.85526431 -0.95140171 -0.95397943 -1.2002181 -1.6491604 -1.7566527 -1.8846068 -2.017832 -1.9084361 -1.5711004 -1.3905472 -1.4009149][0.11382506 0.16814885 -0.013536148 -0.12225963 -0.41267055 -0.89231193 -1.2317246 -1.4969954 -1.6268679 -2.1098533 -2.1954117 -2.0983148 -1.8721128 -1.7833389 -1.5389807][-0.26904967 -0.2388415 -0.33330616 -0.28608114 -0.4168607 -0.78749806 -1.2309024 -1.464296 -1.5655658 -1.355248 -1.5987836 -1.5554569 -1.3462994 -1.4131761 -1.2576092][-0.22370932 -0.24770817 -0.514415 -0.50701481 -0.69306046 -0.846904 -1.1177503 -1.0407749 -1.1281184 -0.99829757 -1.0422176 -1.1326269 -1.1626582 -1.4279745 -1.9519345][0.02529601 -0.18146542 -0.30169913 -0.76484245 -1.0030944 -1.1260443 -1.6131707 -1.9744468 -2.1236818 -2.0259845 -1.6174941 -1.6746145 -1.8325247 -1.5390085 -1.712844]]...]
INFO - root - 2017-12-06 06:26:19.010659: step 27610, loss = 0.87, batch loss = 0.65 (13.0 examples/sec; 0.613 sec/batch; 51h:55m:30s remains)
INFO - root - 2017-12-06 06:26:25.057314: step 27620, loss = 0.84, batch loss = 0.63 (13.5 examples/sec; 0.591 sec/batch; 50h:02m:27s remains)
INFO - root - 2017-12-06 06:26:31.251897: step 27630, loss = 0.93, batch loss = 0.72 (13.2 examples/sec; 0.604 sec/batch; 51h:09m:39s remains)
INFO - root - 2017-12-06 06:26:37.016656: step 27640, loss = 0.77, batch loss = 0.55 (12.8 examples/sec; 0.627 sec/batch; 53h:05m:24s remains)
INFO - root - 2017-12-06 06:26:42.867752: step 27650, loss = 0.77, batch loss = 0.56 (13.1 examples/sec; 0.612 sec/batch; 51h:49m:45s remains)
INFO - root - 2017-12-06 06:26:48.426871: step 27660, loss = 0.80, batch loss = 0.59 (13.6 examples/sec; 0.587 sec/batch; 49h:40m:19s remains)
INFO - root - 2017-12-06 06:26:53.950068: step 27670, loss = 0.79, batch loss = 0.57 (13.6 examples/sec; 0.587 sec/batch; 49h:43m:29s remains)
INFO - root - 2017-12-06 06:27:00.076297: step 27680, loss = 0.81, batch loss = 0.60 (13.8 examples/sec; 0.578 sec/batch; 48h:58m:26s remains)
INFO - root - 2017-12-06 06:27:06.167298: step 27690, loss = 0.80, batch loss = 0.59 (13.7 examples/sec; 0.586 sec/batch; 49h:36m:18s remains)
INFO - root - 2017-12-06 06:27:12.357801: step 27700, loss = 0.91, batch loss = 0.69 (13.2 examples/sec; 0.607 sec/batch; 51h:22m:59s remains)
2017-12-06 06:27:13.097185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3447008 -1.5429555 -1.627053 -1.5695785 -1.4697473 -2.0444469 -2.4073884 -2.7478278 -3.0336821 -2.8096235 -2.1825895 -1.42497 -0.79833579 -0.38466021 0.032560304][-1.6971223 -1.6717541 -1.6641387 -1.9028305 -2.4161043 -3.0294945 -3.5013766 -4.4492908 -5.19498 -5.1800547 -4.7653627 -3.8502386 -3.2249486 -2.1500096 -1.3074667][-2.6508982 -2.6112561 -2.621542 -2.2747397 -2.0575793 -2.4767368 -3.229918 -4.2236738 -4.913363 -5.4391141 -5.3013992 -5.0543828 -4.481554 -2.9913983 -2.5435419][-2.851491 -2.8638694 -2.5016096 -2.3846946 -2.2735233 -1.611864 -1.4567168 -2.1894705 -3.289731 -4.3761244 -4.5993195 -4.7983737 -4.684998 -3.6993291 -3.3032241][-1.961645 -2.0454006 -1.9355673 -1.6522305 -1.2772412 -1.3471044 -1.6424621 -1.6820468 -2.0333695 -2.4322958 -2.4968112 -3.1857493 -3.8980095 -3.3309145 -3.2052486][-0.70905554 -0.60628104 -0.50842577 -0.36486739 -0.36995217 -0.12133025 -0.0036680475 0.056765504 -0.03824814 -0.20495087 -0.067874305 -0.14563321 -0.0046307519 -0.31808612 -1.1073418][-0.11291902 0.16276589 0.32656878 0.61071628 1.104236 1.4757621 1.4391167 1.7483407 2.0537996 2.5275276 2.7048681 2.5186474 2.19658 2.3015556 1.9405566][0.4949415 0.82505882 1.1181146 1.6061926 2.0215201 2.4338734 2.8887153 3.1300666 3.1702833 3.4563158 3.5650947 3.9231782 3.7612641 3.3882625 2.8911171][0.47130814 0.76682007 1.0772995 1.6146652 2.1157794 2.4774818 2.7895229 2.8976977 3.093133 2.8795512 2.5321941 2.4502397 2.5072041 2.4049852 1.6642948][0.14067489 0.22243553 0.29984194 0.58240592 1.0755866 1.3673191 1.544337 1.5265541 1.5522885 1.4145588 1.2013348 1.0280788 1.0728414 1.0789677 0.90281212][-0.33496815 -0.23641118 -0.35843337 -0.37994167 -0.37293726 -0.28279832 -0.084895283 -0.204905 -0.44328377 -0.52306628 -0.59939152 -0.49273911 -0.49480104 -0.41906422 -0.56474894][-1.2622963 -1.521457 -1.7122152 -2.1171603 -2.4591727 -2.3638675 -2.2997026 -2.1485415 -2.2257519 -2.3904853 -2.3061163 -2.1981664 -2.2140195 -2.1823163 -2.1779292][-1.0616117 -1.6739912 -2.1045146 -2.3521736 -2.5114892 -2.9262137 -3.3054478 -3.3217885 -3.3372147 -3.4326499 -3.487699 -3.4018705 -3.2712054 -3.2891483 -3.262428][-0.62948579 -1.0527836 -1.657405 -2.3483734 -2.7205966 -3.0722938 -3.2167692 -3.56463 -3.8981435 -3.8732631 -3.72246 -3.6820371 -3.6038945 -3.4007995 -3.0149744][-0.79331654 -1.0294274 -1.3757927 -1.8753865 -2.4649203 -2.9204841 -3.0120394 -3.1477563 -3.0937843 -3.31107 -3.4234509 -3.2037578 -2.9102888 -2.7086263 -2.4355664]]...]
INFO - root - 2017-12-06 06:27:19.156689: step 27710, loss = 0.83, batch loss = 0.61 (13.4 examples/sec; 0.595 sec/batch; 50h:23m:49s remains)
INFO - root - 2017-12-06 06:27:25.440376: step 27720, loss = 0.83, batch loss = 0.62 (13.5 examples/sec; 0.594 sec/batch; 50h:18m:43s remains)
INFO - root - 2017-12-06 06:27:31.633024: step 27730, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.648 sec/batch; 54h:51m:13s remains)
INFO - root - 2017-12-06 06:27:37.767350: step 27740, loss = 0.80, batch loss = 0.58 (13.0 examples/sec; 0.616 sec/batch; 52h:07m:05s remains)
INFO - root - 2017-12-06 06:27:43.927242: step 27750, loss = 0.78, batch loss = 0.57 (13.8 examples/sec; 0.578 sec/batch; 48h:54m:42s remains)
INFO - root - 2017-12-06 06:27:49.961536: step 27760, loss = 0.79, batch loss = 0.58 (13.1 examples/sec; 0.611 sec/batch; 51h:40m:47s remains)
INFO - root - 2017-12-06 06:27:56.199920: step 27770, loss = 0.76, batch loss = 0.55 (12.8 examples/sec; 0.624 sec/batch; 52h:47m:20s remains)
INFO - root - 2017-12-06 06:28:02.353907: step 27780, loss = 0.85, batch loss = 0.64 (13.0 examples/sec; 0.616 sec/batch; 52h:10m:35s remains)
INFO - root - 2017-12-06 06:28:08.607010: step 27790, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.645 sec/batch; 54h:36m:35s remains)
INFO - root - 2017-12-06 06:28:14.868604: step 27800, loss = 0.88, batch loss = 0.66 (12.9 examples/sec; 0.622 sec/batch; 52h:40m:56s remains)
2017-12-06 06:28:15.524921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1926627 -1.2343549 -1.1291927 -0.95779359 -0.77067089 -0.6297406 -0.514184 -0.48561209 -0.51797789 -0.57798147 -0.59851837 -0.58883917 -0.43722311 -0.18491521 0.085888982][-1.5742173 -1.7086957 -1.7792625 -1.6960043 -1.5029563 -1.2886478 -1.1499408 -1.1443268 -1.1693951 -1.2557023 -1.3129857 -1.2509909 -0.99300444 -0.66748786 -0.24782835][-1.2779776 -1.4520102 -1.6487569 -1.7438204 -1.6804725 -1.6305697 -1.6431856 -1.5946405 -1.603525 -1.6521418 -1.6927049 -1.727868 -1.5787722 -1.1988261 -0.64712727][-0.73240638 -0.77336907 -0.98331928 -1.0569121 -1.0887457 -1.2215006 -1.3348038 -1.490537 -1.5637996 -1.5258697 -1.467307 -1.4476515 -1.3483182 -1.2547485 -0.93855989][-0.2081798 -0.12869705 -0.27453572 -0.38643724 -0.45925742 -0.50046706 -0.61361957 -0.8406142 -0.96596396 -1.0898334 -1.0747888 -0.97727334 -0.91225946 -0.8672235 -0.65973771][0.3043631 0.28122896 0.25960481 0.22196397 0.2666797 0.31356433 0.20623475 0.10234971 -0.068249173 -0.20239499 -0.20861533 -0.28090358 -0.37033144 -0.33400267 -0.19937688][0.77307963 0.70815849 0.82577467 0.97885394 1.115573 1.22377 1.2086416 1.0709044 0.80849946 0.65830863 0.60804 0.39164478 0.1972037 0.05593808 -0.064535335][0.82277989 0.95765412 1.1699868 1.4364204 1.7680562 2.0044386 2.0314286 1.9107383 1.5669751 1.1978277 0.938164 0.72159648 0.50288296 0.26001203 0.051463842][0.52122784 0.71191013 0.9508847 1.3596485 1.7689321 2.1332915 2.3353076 2.2569847 1.9635466 1.530751 1.0487707 0.61912119 0.27763769 0.14383315 0.014101692][-0.081188962 0.041092142 0.33776075 0.68862963 1.0491366 1.4541482 1.7663165 1.769042 1.5962033 1.2340994 0.81736338 0.42980114 0.10542688 -0.19207093 -0.30329025][-0.80905211 -0.68062115 -0.4471772 -0.15396708 0.17328537 0.45434353 0.70236671 0.757033 0.68817437 0.4778339 0.23710084 0.01272089 -0.11920288 -0.2033938 -0.20894334][-1.5270587 -1.4730624 -1.3439696 -1.1471872 -0.8152796 -0.57988524 -0.35718521 -0.24916041 -0.20741484 -0.20802072 -0.25719339 -0.26553318 -0.26724923 -0.18041196 -0.1374072][-1.7742345 -1.6768392 -1.5938072 -1.5488122 -1.3924347 -1.2271119 -1.01443 -0.74544704 -0.51458913 -0.40819314 -0.34866649 -0.32641888 -0.32242656 -0.25005198 -0.18522343][-1.6633227 -1.7303041 -1.6563331 -1.5230665 -1.4354081 -1.2961017 -1.1368846 -0.89710009 -0.64701879 -0.4006401 -0.25092989 -0.21511814 -0.19336137 -0.19869487 -0.18220566][-1.3836763 -1.4769036 -1.5142421 -1.4292341 -1.2760164 -1.0503736 -0.86887622 -0.630528 -0.42837486 -0.27102202 -0.2000058 -0.14513943 -0.13116089 -0.15374729 -0.14794613]]...]
INFO - root - 2017-12-06 06:28:21.645017: step 27810, loss = 0.76, batch loss = 0.55 (13.0 examples/sec; 0.614 sec/batch; 51h:59m:34s remains)
INFO - root - 2017-12-06 06:28:27.829079: step 27820, loss = 0.78, batch loss = 0.57 (13.2 examples/sec; 0.606 sec/batch; 51h:16m:17s remains)
INFO - root - 2017-12-06 06:28:33.857232: step 27830, loss = 0.84, batch loss = 0.62 (13.3 examples/sec; 0.600 sec/batch; 50h:44m:41s remains)
INFO - root - 2017-12-06 06:28:39.835110: step 27840, loss = 0.81, batch loss = 0.60 (12.7 examples/sec; 0.630 sec/batch; 53h:18m:59s remains)
INFO - root - 2017-12-06 06:28:45.719636: step 27850, loss = 0.82, batch loss = 0.61 (13.2 examples/sec; 0.608 sec/batch; 51h:28m:56s remains)
INFO - root - 2017-12-06 06:28:51.807460: step 27860, loss = 0.77, batch loss = 0.55 (12.9 examples/sec; 0.622 sec/batch; 52h:36m:06s remains)
INFO - root - 2017-12-06 06:28:57.938909: step 27870, loss = 0.86, batch loss = 0.64 (13.3 examples/sec; 0.604 sec/batch; 51h:05m:17s remains)
INFO - root - 2017-12-06 06:29:04.128192: step 27880, loss = 0.78, batch loss = 0.56 (12.1 examples/sec; 0.661 sec/batch; 55h:56m:18s remains)
INFO - root - 2017-12-06 06:29:09.884400: step 27890, loss = 0.77, batch loss = 0.56 (15.2 examples/sec; 0.525 sec/batch; 44h:26m:43s remains)
INFO - root - 2017-12-06 06:29:15.491652: step 27900, loss = 0.76, batch loss = 0.54 (12.8 examples/sec; 0.626 sec/batch; 53h:00m:21s remains)
2017-12-06 06:29:16.106134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062739424 -0.062393218 -0.062445946 -0.062798627 -0.063346788 -0.064002573 -0.064692505 -0.065333769 -0.065783188 -0.065983377 -0.065862991 -0.06545651 -0.064843386 -0.064257756 -0.063972555][-0.063511387 -0.063207619 -0.063349292 -0.063770339 -0.064354166 -0.065025568 -0.065708712 -0.066321678 -0.066731311 -0.066907585 -0.066746511 -0.066284835 -0.065597989 -0.06490238 -0.064498149][-0.064429149 -0.064056866 -0.064249381 -0.0647171 -0.065344989 -0.066009462 -0.066648945 -0.067206383 -0.067552522 -0.06767977 -0.0674805 -0.066993892 -0.066305041 -0.06556353 -0.065078244][-0.065027304 -0.064594 -0.064800717 -0.065338917 -0.066034511 -0.066711381 -0.067309789 -0.067811437 -0.068115525 -0.068188429 -0.067947172 -0.067472443 -0.066840045 -0.066143252 -0.065692335][-0.065294877 -0.064976133 -0.065234065 -0.065811694 -0.066510946 -0.067136049 -0.067649409 -0.068043225 -0.068271816 -0.068262734 -0.067983627 -0.067531154 -0.066966891 -0.066355884 -0.066029727][-0.065476149 -0.065274209 -0.0656074 -0.066191852 -0.0667977 -0.067245036 -0.067542374 -0.067737974 -0.067864791 -0.067809343 -0.067523621 -0.067119054 -0.066668347 -0.066172183 -0.065968089][-0.065445572 -0.065283209 -0.065633252 -0.066165671 -0.066632792 -0.066882536 -0.066935241 -0.066955239 -0.067055933 -0.067053385 -0.0668303 -0.066507876 -0.066161744 -0.065799549 -0.065744154][-0.06509877 -0.064907126 -0.065221176 -0.065686323 -0.066085905 -0.066229329 -0.066179104 -0.066135995 -0.066257939 -0.066314973 -0.066191308 -0.065931663 -0.065586708 -0.065276489 -0.065340072][-0.064566277 -0.064327605 -0.06462118 -0.0650643 -0.065462992 -0.065616757 -0.065613657 -0.065602913 -0.065721318 -0.065787248 -0.065707475 -0.065428846 -0.064993039 -0.064591274 -0.064627744][-0.0641251 -0.063858971 -0.0641394 -0.064556211 -0.06494607 -0.06515491 -0.065283582 -0.065380581 -0.065518223 -0.065572619 -0.065497935 -0.065190241 -0.064681761 -0.064179413 -0.06411849][-0.0639188 -0.063538939 -0.063786089 -0.064184248 -0.064608283 -0.064931296 -0.065235652 -0.065523192 -0.065767393 -0.065887421 -0.06585218 -0.06554433 -0.064989552 -0.064368047 -0.064105][-0.063966461 -0.063525237 -0.063736275 -0.064133257 -0.064677365 -0.065219 -0.065743625 -0.066283479 -0.066750683 -0.066985957 -0.06699127 -0.066671975 -0.066011652 -0.065167792 -0.064552307][-0.064437851 -0.063979715 -0.064175107 -0.064583763 -0.06524086 -0.066032432 -0.066848584 -0.067722939 -0.068474948 -0.068868086 -0.068901017 -0.068512142 -0.067719944 -0.066562988 -0.065564573][-0.065229714 -0.064738058 -0.064870179 -0.065285236 -0.066035733 -0.067062713 -0.068245336 -0.069538146 -0.070669323 -0.071308732 -0.071400404 -0.070899844 -0.069873281 -0.0683799 -0.0669711][-0.066125177 -0.065568067 -0.065629557 -0.066040635 -0.066910416 -0.068189539 -0.069755793 -0.071525216 -0.073039956 -0.0739907 -0.074215718 -0.073597014 -0.072270565 -0.070406415 -0.068608478]]...]
INFO - root - 2017-12-06 06:29:22.120602: step 27910, loss = 0.77, batch loss = 0.56 (14.1 examples/sec; 0.567 sec/batch; 47h:58m:07s remains)
INFO - root - 2017-12-06 06:29:28.215354: step 27920, loss = 0.79, batch loss = 0.57 (13.2 examples/sec; 0.606 sec/batch; 51h:16m:41s remains)
INFO - root - 2017-12-06 06:29:34.259129: step 27930, loss = 0.85, batch loss = 0.64 (13.1 examples/sec; 0.608 sec/batch; 51h:28m:23s remains)
INFO - root - 2017-12-06 06:29:39.933874: step 27940, loss = 0.83, batch loss = 0.61 (14.4 examples/sec; 0.555 sec/batch; 46h:59m:31s remains)
INFO - root - 2017-12-06 06:29:45.818139: step 27950, loss = 0.75, batch loss = 0.53 (14.6 examples/sec; 0.549 sec/batch; 46h:25m:54s remains)
INFO - root - 2017-12-06 06:29:51.738574: step 27960, loss = 0.81, batch loss = 0.59 (13.5 examples/sec; 0.593 sec/batch; 50h:08m:21s remains)
INFO - root - 2017-12-06 06:29:57.907351: step 27970, loss = 0.81, batch loss = 0.60 (13.0 examples/sec; 0.614 sec/batch; 51h:58m:15s remains)
INFO - root - 2017-12-06 06:30:04.010545: step 27980, loss = 0.88, batch loss = 0.66 (13.2 examples/sec; 0.607 sec/batch; 51h:21m:29s remains)
INFO - root - 2017-12-06 06:30:09.795832: step 27990, loss = 0.80, batch loss = 0.59 (14.7 examples/sec; 0.545 sec/batch; 46h:06m:54s remains)
INFO - root - 2017-12-06 06:30:15.751810: step 28000, loss = 0.73, batch loss = 0.52 (13.1 examples/sec; 0.610 sec/batch; 51h:36m:15s remains)
2017-12-06 06:30:16.437127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13254608 -0.592368 -1.0243005 -1.6743054 -2.1207652 -1.381771 -0.4661378 0.19305718 0.92685175 1.4328207 2.0710461 1.9750913 1.7632885 1.7184789 1.6954914][0.78518456 0.26568618 -0.80361444 -1.3317297 -1.6207812 -1.551607 -1.2882383 -0.85618979 -0.56563103 -0.39475 -0.31389749 0.23801354 0.37334657 -0.096669137 0.0684712][-0.56392819 -1.0548739 -1.5549808 -2.2248106 -3.0043881 -2.9160042 -2.6437986 -2.2649498 -2.152812 -2.053283 -1.9587612 -2.3016403 -2.4530237 -1.9198251 -1.2717545][0.37001836 -0.46489343 -1.4011737 -1.983578 -2.3322306 -2.8027585 -3.2739797 -3.2904894 -3.0417821 -3.3500321 -3.5445251 -3.2699332 -2.9397082 -3.0177631 -2.8098791][0.60862994 0.21874315 -0.099985436 -0.92833233 -1.2178518 -1.5144868 -1.8407581 -2.4416497 -3.1175363 -3.1267939 -3.228817 -3.6949246 -3.7820694 -3.5313108 -3.0685661][0.90184367 0.71759665 0.42703527 0.32246634 0.34160286 -0.14850858 -0.39386868 -0.51923656 -1.1060351 -1.7196583 -2.2884762 -2.5087762 -2.4559021 -2.5020163 -2.3747611][0.98108459 1.1554258 0.97395229 0.8443504 1.0978005 1.523177 2.0119758 1.7040011 1.6108119 0.93372762 0.055098914 -0.3807089 -0.84048152 -1.0669056 -1.1945871][1.4138415 1.0773306 1.1380363 1.4034948 1.1602616 1.4043087 1.6418656 1.9538234 2.1093688 1.9543387 2.0614953 1.2601132 0.67378819 0.93883705 0.51080251][0.74554604 1.1207364 1.1404787 1.0083873 1.1918517 1.360978 1.1801078 1.3232481 1.3952447 1.1288083 1.0963888 1.401574 1.5711156 1.1322809 0.70185286][-0.61046845 -0.41047981 -0.2164872 -0.25298715 -0.3340317 -0.24842137 -0.13074799 -0.32565868 -0.40301195 0.20258883 0.565588 0.39842764 0.54689133 1.0659828 1.3229344][-1.5843487 -1.5515543 -1.6982679 -2.0188365 -2.3160982 -2.5065694 -2.5254905 -2.4467835 -2.3066642 -2.066649 -1.6637764 -0.97320503 -0.66397434 -0.34779996 0.45168698][-2.1157889 -2.51727 -2.8335578 -3.239989 -3.5628521 -3.5637493 -3.4757714 -3.2766197 -2.9166739 -2.6331708 -2.3607898 -2.0859263 -1.8174583 -1.1244357 -0.51968175][-2.1238225 -2.2134805 -2.4104667 -2.8669786 -3.3388915 -3.4270034 -3.2676256 -2.8971565 -2.651823 -2.3590004 -2.0658722 -1.7665534 -1.4074005 -1.0231088 -0.75218123][-1.1620722 -1.8402727 -2.5299566 -2.7056687 -2.6962464 -2.9539373 -3.1235812 -2.7186553 -2.3453777 -1.9902209 -1.6666018 -1.7057229 -1.6947082 -1.2203431 -0.82232344][-1.0212313 -1.4746355 -1.8155544 -2.4126012 -2.9192874 -2.8067446 -2.5190227 -2.5104363 -2.3055475 -1.9441667 -1.7567465 -1.5231169 -1.3971908 -1.2467713 -1.2772056]]...]
INFO - root - 2017-12-06 06:30:22.487288: step 28010, loss = 0.86, batch loss = 0.64 (13.4 examples/sec; 0.595 sec/batch; 50h:18m:31s remains)
INFO - root - 2017-12-06 06:30:28.601853: step 28020, loss = 0.85, batch loss = 0.64 (13.1 examples/sec; 0.609 sec/batch; 51h:28m:50s remains)
INFO - root - 2017-12-06 06:30:34.793395: step 28030, loss = 0.79, batch loss = 0.57 (13.0 examples/sec; 0.614 sec/batch; 51h:54m:55s remains)
INFO - root - 2017-12-06 06:30:40.551403: step 28040, loss = 0.81, batch loss = 0.59 (13.0 examples/sec; 0.615 sec/batch; 52h:02m:02s remains)
INFO - root - 2017-12-06 06:30:46.173649: step 28050, loss = 0.84, batch loss = 0.62 (13.5 examples/sec; 0.592 sec/batch; 50h:03m:23s remains)
INFO - root - 2017-12-06 06:30:52.230714: step 28060, loss = 0.68, batch loss = 0.47 (12.3 examples/sec; 0.649 sec/batch; 54h:51m:34s remains)
INFO - root - 2017-12-06 06:30:58.186793: step 28070, loss = 0.77, batch loss = 0.55 (15.5 examples/sec; 0.516 sec/batch; 43h:39m:29s remains)
INFO - root - 2017-12-06 06:31:04.230977: step 28080, loss = 0.82, batch loss = 0.61 (12.6 examples/sec; 0.634 sec/batch; 53h:36m:20s remains)
INFO - root - 2017-12-06 06:31:10.164008: step 28090, loss = 0.78, batch loss = 0.57 (13.4 examples/sec; 0.599 sec/batch; 50h:38m:19s remains)
INFO - root - 2017-12-06 06:31:16.136426: step 28100, loss = 0.88, batch loss = 0.67 (15.8 examples/sec; 0.506 sec/batch; 42h:46m:45s remains)
2017-12-06 06:31:16.700168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37688464 -1.1765125 -1.7651906 -2.3743088 -2.8144071 -3.0447869 -2.9238 -2.4194374 -1.8990278 -1.4344587 -0.95986938 -0.51250243 -0.24691299 -0.19236708 -0.136206][-0.33471397 -1.1700543 -1.9286609 -2.4852316 -2.7770209 -2.8832519 -2.7936792 -2.513459 -2.070159 -1.5768011 -1.2082151 -0.95867479 -0.67313725 -0.57853323 -0.6045391][-0.75657624 -1.2518727 -1.8119936 -2.1060348 -2.1424296 -2.0002196 -1.7269387 -1.713864 -1.5333432 -1.4369519 -1.4036903 -1.2891239 -1.2018313 -1.1628262 -1.0342202][-0.94193286 -1.2286015 -1.3047916 -1.2967421 -1.3553759 -1.3473061 -1.3226769 -1.1442522 -0.87382162 -0.73476315 -0.68877268 -0.76685888 -0.75792104 -0.69312304 -0.80742991][-0.68312442 -0.72845614 -0.39999938 -0.26823398 -0.2316651 -0.18246718 -0.1173403 -0.27592757 -0.3906157 -0.27143034 -0.16816838 -0.15609542 -0.11720295 -0.06393034 -0.08305531][0.10067996 -0.071275629 0.15996453 0.70654923 0.87541419 0.94938022 1.0786086 1.0728854 1.0269765 0.86374617 0.74844283 0.75702679 0.73799777 0.78106463 0.8104068][1.0687164 0.77981234 0.87663788 1.14251 1.2036604 1.5262148 1.7967253 2.00741 2.1936185 2.0245917 1.7830334 1.5301589 1.3324044 1.3111067 1.4079316][0.899164 0.76307625 0.84307808 0.9832415 1.1128098 1.47082 1.8050779 2.1353507 2.396874 2.3665988 2.1605766 1.8655432 1.7740539 1.359466 0.97263449][0.16469991 0.16924286 0.39445218 0.44689339 0.64094234 0.87678432 1.0943664 1.3105563 1.3350737 1.3845605 1.3936414 1.2443048 0.95438212 0.69014013 0.52395523][-1.1619688 -1.0623335 -0.86065805 -0.72406507 -0.53262562 -0.37110353 -0.26493537 -0.16384193 -0.062325083 0.13313061 0.17791763 0.18520454 0.17632025 -0.006170176 -0.22877717][-2.5318968 -2.3076196 -2.0816765 -1.998989 -1.980466 -2.139158 -2.2551045 -2.3195848 -2.3732839 -2.1379046 -1.9867922 -1.6968466 -1.3869857 -1.150212 -0.91931665][-3.1097379 -3.2054117 -3.3027031 -3.1925769 -3.2122183 -3.2518368 -3.3530366 -3.5094979 -3.6528976 -3.6616998 -3.5992174 -3.2567317 -2.823945 -2.4645936 -1.9998657][-3.0125673 -3.1935565 -3.3374872 -3.4216006 -3.5509095 -3.6538582 -3.7754045 -3.8727865 -3.9777989 -4.0650826 -4.0631914 -3.8903081 -3.5876527 -3.2189486 -2.9110649][-2.5986595 -2.8084462 -3.00937 -3.0421841 -3.1251407 -3.244978 -3.3787563 -3.5362947 -3.6414683 -3.6416459 -3.6369267 -3.6693227 -3.5799918 -3.4460108 -3.1785836][-1.6194232 -1.7455212 -1.8788911 -1.9865085 -2.1946766 -2.3350441 -2.4952497 -2.6599336 -2.7756429 -2.864078 -2.8827059 -2.8586712 -2.8882875 -2.9681966 -2.8193023]]...]
INFO - root - 2017-12-06 06:31:22.420873: step 28110, loss = 0.81, batch loss = 0.60 (13.9 examples/sec; 0.576 sec/batch; 48h:40m:31s remains)
INFO - root - 2017-12-06 06:31:28.515490: step 28120, loss = 0.85, batch loss = 0.63 (13.4 examples/sec; 0.595 sec/batch; 50h:20m:52s remains)
INFO - root - 2017-12-06 06:31:34.378487: step 28130, loss = 0.76, batch loss = 0.55 (13.8 examples/sec; 0.581 sec/batch; 49h:09m:26s remains)
INFO - root - 2017-12-06 06:31:40.444701: step 28140, loss = 0.86, batch loss = 0.65 (12.5 examples/sec; 0.638 sec/batch; 53h:56m:29s remains)
INFO - root - 2017-12-06 06:31:46.558108: step 28150, loss = 0.84, batch loss = 0.63 (13.7 examples/sec; 0.584 sec/batch; 49h:22m:29s remains)
INFO - root - 2017-12-06 06:31:52.447034: step 28160, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.658 sec/batch; 55h:37m:46s remains)
INFO - root - 2017-12-06 06:31:58.525075: step 28170, loss = 0.64, batch loss = 0.43 (13.4 examples/sec; 0.598 sec/batch; 50h:33m:27s remains)
INFO - root - 2017-12-06 06:32:04.573812: step 28180, loss = 0.71, batch loss = 0.49 (13.6 examples/sec; 0.590 sec/batch; 49h:53m:39s remains)
INFO - root - 2017-12-06 06:32:10.767285: step 28190, loss = 0.81, batch loss = 0.60 (13.1 examples/sec; 0.612 sec/batch; 51h:42m:49s remains)
INFO - root - 2017-12-06 06:32:16.930454: step 28200, loss = 0.91, batch loss = 0.69 (13.1 examples/sec; 0.610 sec/batch; 51h:32m:41s remains)
2017-12-06 06:32:17.546934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.35479352 -0.37022841 -0.39323524 -0.38298821 -0.37719882 -0.36316937 -0.36368042 -0.37096718 -0.37452543 -0.37257245 -0.34891012 -0.3068864 -0.25473782 -0.21042538 -0.1736629][-0.23552978 -0.25443283 -0.29576948 -0.32649529 -0.36443582 -0.36550137 -0.38979584 -0.39295036 -0.39723459 -0.37413391 -0.33997205 -0.27090129 -0.20159307 -0.14860116 -0.11043403][-0.12756953 -0.14417841 -0.17512909 -0.20260203 -0.23256505 -0.27134168 -0.31858072 -0.30889693 -0.29963785 -0.27260926 -0.2435469 -0.18858171 -0.16648063 -0.12686774 -0.10513913][-0.068272531 -0.10305128 -0.12976429 -0.14431569 -0.14540747 -0.15677115 -0.17854121 -0.1742284 -0.16991939 -0.13369064 -0.11087098 -0.073342182 -0.080830209 -0.081265941 -0.081210263][-0.058696359 -0.072195813 -0.088311017 -0.063959137 -0.00014320761 0.058537744 0.067156412 0.05493883 0.0098345429 -0.013984002 -0.016568504 -0.027550802 -0.063823842 -0.088522486 -0.11149685][-0.027148284 -0.030885421 -0.0308159 0.016721182 0.12075091 0.221998 0.27628586 0.26860893 0.1820018 0.120456 0.076800771 0.014465354 -0.035157982 -0.10241619 -0.12802894][0.016856499 0.044799723 0.079315878 0.10120314 0.17149475 0.27840653 0.37300682 0.39611903 0.33229536 0.22354516 0.15291125 0.07559327 0.024774827 -0.04553372 -0.097604312][0.059663214 0.076522224 0.1072093 0.13442037 0.18338698 0.25079891 0.33426496 0.3588787 0.35229257 0.27668908 0.20776477 0.12238256 0.059914522 -0.016165785 -0.070561387][0.085478969 0.0663194 0.06877061 0.082881875 0.10965837 0.14123577 0.2027024 0.21847725 0.22411835 0.18310401 0.14282992 0.0838987 0.022440754 -0.023954563 -0.072189555][-0.062599249 -0.10368091 -0.12910084 -0.11175714 -0.057274107 -0.013399959 0.038937204 0.048853837 0.052886419 0.028606586 0.0012324303 -0.026943192 -0.051103197 -0.057093978 -0.070140384][-0.25547987 -0.29679143 -0.31809694 -0.29342058 -0.24228755 -0.20043691 -0.16907606 -0.16810662 -0.16328903 -0.17510316 -0.17112231 -0.1686087 -0.14492089 -0.11207858 -0.10092325][-0.37770736 -0.40970564 -0.41693476 -0.38753524 -0.35331434 -0.31287259 -0.3056843 -0.30257469 -0.31324881 -0.30330667 -0.28038275 -0.25151104 -0.19896318 -0.15102151 -0.11725231][-0.43023354 -0.44131497 -0.43397677 -0.38978365 -0.35584188 -0.31183583 -0.29357481 -0.29052663 -0.28256407 -0.30313027 -0.2902669 -0.26312342 -0.22285727 -0.1904631 -0.15444022][-0.36742225 -0.38921794 -0.38682717 -0.34144491 -0.29756683 -0.25106406 -0.22321293 -0.19812554 -0.18003708 -0.19144288 -0.18777527 -0.16888329 -0.14537339 -0.13064842 -0.11610544][-0.27691397 -0.30512416 -0.31372637 -0.27819449 -0.24984133 -0.21624398 -0.17701983 -0.13847509 -0.10205469 -0.097120911 -0.0927954 -0.076710612 -0.068646811 -0.070364922 -0.082914017]]...]
INFO - root - 2017-12-06 06:32:23.536990: step 28210, loss = 0.81, batch loss = 0.59 (13.5 examples/sec; 0.592 sec/batch; 50h:02m:40s remains)
INFO - root - 2017-12-06 06:32:29.722047: step 28220, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.629 sec/batch; 53h:08m:04s remains)
INFO - root - 2017-12-06 06:32:35.428061: step 28230, loss = 0.90, batch loss = 0.68 (13.2 examples/sec; 0.605 sec/batch; 51h:08m:15s remains)
INFO - root - 2017-12-06 06:32:41.559604: step 28240, loss = 0.79, batch loss = 0.57 (13.3 examples/sec; 0.602 sec/batch; 50h:52m:51s remains)
INFO - root - 2017-12-06 06:32:47.645899: step 28250, loss = 0.83, batch loss = 0.62 (13.5 examples/sec; 0.593 sec/batch; 50h:05m:23s remains)
INFO - root - 2017-12-06 06:32:53.678628: step 28260, loss = 0.95, batch loss = 0.73 (12.9 examples/sec; 0.621 sec/batch; 52h:29m:24s remains)
INFO - root - 2017-12-06 06:32:59.931250: step 28270, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.633 sec/batch; 53h:30m:26s remains)
INFO - root - 2017-12-06 06:33:06.198210: step 28280, loss = 0.84, batch loss = 0.62 (11.7 examples/sec; 0.684 sec/batch; 57h:50m:23s remains)
INFO - root - 2017-12-06 06:33:12.384278: step 28290, loss = 0.73, batch loss = 0.51 (13.6 examples/sec; 0.589 sec/batch; 49h:48m:21s remains)
INFO - root - 2017-12-06 06:33:18.023093: step 28300, loss = 0.83, batch loss = 0.61 (13.7 examples/sec; 0.582 sec/batch; 49h:10m:53s remains)
2017-12-06 06:33:18.699734: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.055028781 -0.064349078 -0.22701563 -0.34204298 -0.42696214 -0.47829229 -0.52113485 -0.52217352 -0.45049369 -0.42053449 -0.41331202 -0.33422154 -0.15514292 -0.08807043 -0.16227765][-0.33925819 -0.46403414 -0.65497977 -0.85378766 -0.96735042 -0.99642074 -0.96556944 -0.89366734 -0.7917071 -0.649991 -0.5110414 -0.48548806 -0.4185248 -0.35246211 -0.36924958][-0.76166165 -0.87858868 -1.0114073 -1.1563598 -1.2725555 -1.3370134 -1.3428487 -1.237645 -1.04458 -0.89258808 -0.80958754 -0.69547737 -0.5836907 -0.56698853 -0.62985712][-1.1066393 -1.1228883 -1.1888011 -1.2667208 -1.3284366 -1.3428016 -1.3428193 -1.3319489 -1.2681521 -1.0905389 -0.85008705 -0.70449835 -0.7187593 -0.77194268 -0.9069525][-1.2821084 -1.1891938 -1.1462566 -1.1637342 -1.1738421 -1.1100749 -1.039909 -0.98729491 -0.9726122 -0.982446 -0.93527448 -0.83113468 -0.83624828 -0.95458663 -1.134909][-1.0091952 -0.93105227 -0.84982812 -0.82211936 -0.7903226 -0.70767289 -0.60844797 -0.55947161 -0.55586016 -0.5846259 -0.58454144 -0.640732 -0.81969631 -1.0856187 -1.3326489][-0.49575311 -0.49705154 -0.4716574 -0.46848309 -0.51364195 -0.46343976 -0.35869664 -0.30008188 -0.29680476 -0.33583719 -0.36567438 -0.4164952 -0.64386153 -0.95617074 -1.1742349][-0.0086480975 -0.09429983 -0.1675016 -0.23181745 -0.32979357 -0.36990333 -0.32435474 -0.28973681 -0.25133544 -0.25928235 -0.18591496 -0.057950441 -0.069189124 -0.33247703 -0.77431595][0.40058768 0.18693449 0.030860789 -0.1242052 -0.32278916 -0.46785814 -0.49648112 -0.51866263 -0.473018 -0.45849919 -0.34051508 -0.14642173 -0.0048273057 0.0045590848 -0.17846721][0.4406274 0.25855726 0.11929306 -0.028501887 -0.24831589 -0.4529863 -0.55240285 -0.61148262 -0.62259167 -0.5904277 -0.49640495 -0.28498691 0.0631347 0.4006294 0.43707955][0.11060926 -0.065861046 -0.05766489 -0.025201544 -0.10097641 -0.24882524 -0.37765503 -0.46112406 -0.59934235 -0.684607 -0.58836 -0.3845197 0.0294822 0.59873194 1.0166446][-0.074057281 -0.14046639 -0.045464486 0.04941301 0.054420575 0.075966567 0.055715889 -0.036300372 -0.26762015 -0.46889079 -0.48226005 -0.43166459 -0.084913768 0.45980418 0.94364816][0.012695409 0.11385301 0.42946118 0.62639743 0.63748777 0.65019125 0.69422227 0.75326645 0.53239828 0.24942367 0.058255687 -0.085544445 -0.0018001795 0.36961597 0.81760126][0.213951 0.35256708 0.60718638 0.86180776 1.0108582 1.107067 1.1254874 1.1584574 0.8880353 0.61718279 0.51261646 0.43039441 0.42080504 0.557448 0.72727895][0.62306029 0.66194594 0.826105 0.94894797 1.0119544 1.1466202 1.3083836 1.3309823 1.0790591 0.73711061 0.53220958 0.50996059 0.539374 0.56112462 0.5244568]]...]
INFO - root - 2017-12-06 06:33:24.762320: step 28310, loss = 0.83, batch loss = 0.62 (13.3 examples/sec; 0.602 sec/batch; 50h:49m:52s remains)
INFO - root - 2017-12-06 06:33:30.768884: step 28320, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.625 sec/batch; 52h:50m:14s remains)
INFO - root - 2017-12-06 06:33:36.970969: step 28330, loss = 0.81, batch loss = 0.59 (12.8 examples/sec; 0.625 sec/batch; 52h:49m:50s remains)
INFO - root - 2017-12-06 06:33:43.266350: step 28340, loss = 0.76, batch loss = 0.54 (13.0 examples/sec; 0.614 sec/batch; 51h:52m:21s remains)
INFO - root - 2017-12-06 06:33:49.511427: step 28350, loss = 0.71, batch loss = 0.50 (13.0 examples/sec; 0.617 sec/batch; 52h:06m:58s remains)
INFO - root - 2017-12-06 06:33:55.546579: step 28360, loss = 0.74, batch loss = 0.52 (13.3 examples/sec; 0.603 sec/batch; 50h:57m:14s remains)
INFO - root - 2017-12-06 06:34:01.695291: step 28370, loss = 0.75, batch loss = 0.54 (13.6 examples/sec; 0.590 sec/batch; 49h:49m:10s remains)
INFO - root - 2017-12-06 06:34:07.858895: step 28380, loss = 0.84, batch loss = 0.62 (13.1 examples/sec; 0.611 sec/batch; 51h:37m:04s remains)
INFO - root - 2017-12-06 06:34:14.023529: step 28390, loss = 0.76, batch loss = 0.54 (12.6 examples/sec; 0.637 sec/batch; 53h:46m:25s remains)
INFO - root - 2017-12-06 06:34:20.009272: step 28400, loss = 0.85, batch loss = 0.64 (13.4 examples/sec; 0.598 sec/batch; 50h:29m:52s remains)
2017-12-06 06:34:20.655363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40076756 -0.58309078 -0.57589453 -0.90711766 -1.2844884 -1.5799435 -1.6530566 -1.6257793 -1.4053907 -1.14424 -1.0044521 -0.83132833 -0.52999127 -0.31673014 -0.12625638][-0.20521101 -0.62862217 -0.90181243 -1.1772954 -1.4580772 -1.7829015 -1.9058273 -1.8946291 -1.8267206 -1.6803408 -1.4798859 -1.2021854 -0.8536815 -0.56221986 -0.26825064][-0.60885108 -0.7669518 -0.916066 -1.3734845 -1.7940086 -1.9717461 -1.9857963 -1.942564 -1.860352 -1.7697093 -1.5924966 -1.300823 -0.93370008 -0.42905819 -0.12047464][-0.57682961 -0.73300225 -0.87647367 -1.042785 -1.3526723 -1.5462712 -1.6366693 -1.6487185 -1.6522633 -1.5738981 -1.4071454 -1.2429532 -0.95249742 -0.5150367 -0.20347849][-0.17043677 -0.29019272 -0.44659463 -0.54699886 -0.6011228 -0.665059 -0.73964328 -0.80733192 -0.92238611 -1.0115485 -1.1172607 -1.1011918 -0.95825624 -0.7108022 -0.37824804][0.27619919 0.20273814 0.21365216 0.11448683 0.11987421 0.17687979 0.20303264 0.053998858 -0.14607494 -0.36285827 -0.62482119 -0.8465994 -0.96740234 -0.9948594 -0.82248849][0.60543388 0.55944085 0.60602713 0.73392129 0.84953731 0.900206 0.85175669 0.77155173 0.62177986 0.32002911 -0.13122118 -0.53853214 -0.77687669 -0.86391163 -0.96620512][1.0238322 1.0474081 0.94099426 0.975039 1.0061264 1.0417695 1.0114436 0.8926999 0.69646257 0.5318476 0.2780683 -0.20197102 -0.581471 -0.76422495 -0.94192553][0.5724228 0.49056387 0.51746976 0.51225406 0.48196489 0.36621568 0.35450238 0.30814895 0.1343741 0.029379711 -0.19477728 -0.41699392 -0.5133698 -0.57386053 -0.66589677][-0.16729568 -0.22716627 -0.28814954 -0.49140248 -0.71340322 -0.77164137 -0.733355 -0.66546327 -0.65994203 -0.69910133 -0.83811861 -0.89508522 -0.81724721 -0.59876227 -0.35614049][-0.60952449 -0.80773675 -0.97399181 -1.2282516 -1.4548243 -1.5252974 -1.5072922 -1.446149 -1.3760675 -1.290134 -1.2139103 -1.0601542 -0.89414614 -0.60739785 -0.28377509][-0.92857707 -1.1884531 -1.3689922 -1.5749089 -1.7731084 -1.8707314 -1.8450518 -1.7641059 -1.6063261 -1.4529035 -1.3156728 -1.0469928 -0.90737194 -0.67623329 -0.3553884][-0.76246595 -0.96634191 -1.2033117 -1.4396608 -1.4930378 -1.4183336 -1.3654959 -1.3480827 -1.2877157 -1.2330185 -1.1212751 -0.95470005 -0.86042011 -0.70669359 -0.57808614][-0.35931396 -0.34525815 -0.37498119 -0.51604009 -0.52522415 -0.59830993 -0.66767019 -0.74187672 -0.73611242 -0.64460254 -0.572639 -0.54309297 -0.492566 -0.3543919 -0.34333956][0.30487213 0.20067236 -0.006096594 -0.14942846 -0.23619077 -0.24127097 -0.25521317 -0.33992743 -0.4289251 -0.52098894 -0.54244846 -0.44001 -0.25274211 -0.19518435 -0.18791437]]...]
INFO - root - 2017-12-06 06:34:26.560160: step 28410, loss = 0.81, batch loss = 0.60 (15.8 examples/sec; 0.506 sec/batch; 42h:44m:59s remains)
INFO - root - 2017-12-06 06:34:32.606787: step 28420, loss = 0.83, batch loss = 0.61 (13.5 examples/sec; 0.592 sec/batch; 50h:00m:49s remains)
INFO - root - 2017-12-06 06:34:38.696747: step 28430, loss = 0.82, batch loss = 0.61 (13.4 examples/sec; 0.596 sec/batch; 50h:19m:41s remains)
INFO - root - 2017-12-06 06:34:44.855651: step 28440, loss = 0.74, batch loss = 0.53 (13.2 examples/sec; 0.608 sec/batch; 51h:19m:19s remains)
INFO - root - 2017-12-06 06:34:51.033864: step 28450, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.634 sec/batch; 53h:34m:27s remains)
INFO - root - 2017-12-06 06:34:56.930598: step 28460, loss = 0.80, batch loss = 0.59 (18.1 examples/sec; 0.442 sec/batch; 37h:18m:01s remains)
INFO - root - 2017-12-06 06:35:02.515659: step 28470, loss = 0.84, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:38m:17s remains)
INFO - root - 2017-12-06 06:35:08.456185: step 28480, loss = 0.94, batch loss = 0.73 (12.8 examples/sec; 0.627 sec/batch; 52h:56m:42s remains)
INFO - root - 2017-12-06 06:35:14.528723: step 28490, loss = 0.81, batch loss = 0.60 (13.3 examples/sec; 0.603 sec/batch; 50h:54m:20s remains)
INFO - root - 2017-12-06 06:35:20.690896: step 28500, loss = 0.82, batch loss = 0.61 (13.5 examples/sec; 0.592 sec/batch; 50h:00m:27s remains)
2017-12-06 06:35:21.390234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.084897704 -0.085200146 -0.086968809 -0.092789546 -0.10414252 -0.11497401 -0.11303701 -0.10328494 -0.10317747 -0.11221683 -0.11793931 -0.1207613 -0.1200507 -0.10873915 -0.09864147][-0.077818662 -0.077293694 -0.084991314 -0.10488229 -0.13124818 -0.14219216 -0.11860654 -0.074889466 -0.047935259 -0.054577831 -0.084680505 -0.13116553 -0.17311692 -0.17670983 -0.15464136][-0.095331609 -0.10480871 -0.12790224 -0.16598016 -0.20600659 -0.22869459 -0.2265577 -0.21771431 -0.22213125 -0.23854798 -0.25892931 -0.2916449 -0.32059765 -0.3084603 -0.26267916][-0.14130089 -0.17422938 -0.20891497 -0.2388863 -0.25003383 -0.24558225 -0.24846925 -0.27541274 -0.31864759 -0.34987676 -0.35306764 -0.35132074 -0.35168195 -0.3278231 -0.28815687][-0.14121261 -0.18389016 -0.21368182 -0.20465875 -0.14085326 -0.040969096 0.03944093 0.068633676 0.069849133 0.042731762 -0.00012560934 -0.047682285 -0.10655907 -0.15212345 -0.18320842][-0.081187911 -0.097102627 -0.098856166 -0.053831775 0.053384438 0.20231457 0.35007995 0.46861649 0.5610404 0.57610476 0.50065339 0.3618499 0.20401375 0.057056949 -0.062222585][-0.037523128 -0.018630169 0.0076068193 0.057597041 0.15281148 0.29113442 0.4517014 0.6049723 0.73370224 0.78854054 0.71815294 0.55309337 0.35823351 0.16614039 0.020248398][-0.056826353 -0.029277246 0.013036996 0.071233362 0.15517128 0.2732321 0.42386 0.57312614 0.68485564 0.73235977 0.67315972 0.533366 0.38498473 0.22198175 0.084586561][-0.085448712 -0.052171186 0.0018494651 0.066866741 0.13903166 0.21967645 0.31699312 0.42092562 0.51793152 0.5766381 0.55278832 0.44374204 0.3127116 0.16628666 0.057621554][-0.13075875 -0.12206074 -0.084959224 -0.030919403 0.018429607 0.052292734 0.088868767 0.14569639 0.23829557 0.32177305 0.34180743 0.25903732 0.12474267 -0.019551665 -0.088518925][-0.16808867 -0.20048587 -0.21085428 -0.19947818 -0.17731622 -0.17168096 -0.17957383 -0.16928652 -0.1151047 -0.046485271 -0.001700677 -0.015827298 -0.056145083 -0.13406222 -0.16468163][-0.16392872 -0.21798357 -0.26908666 -0.30395445 -0.32256746 -0.34515393 -0.37849015 -0.40868503 -0.399342 -0.35680485 -0.29381374 -0.23591718 -0.17911282 -0.15017703 -0.14246969][-0.13119818 -0.17873988 -0.23730128 -0.29513979 -0.34515423 -0.39101303 -0.42927355 -0.45916855 -0.45975035 -0.42983174 -0.37632346 -0.3068386 -0.2346454 -0.15357615 -0.12245858][-0.10198191 -0.12871312 -0.16666827 -0.20777243 -0.2464103 -0.27911457 -0.30359185 -0.31979105 -0.31876582 -0.30920815 -0.28162071 -0.25317472 -0.19961381 -0.12241286 -0.089923784][-0.087193958 -0.0961578 -0.11140767 -0.13086835 -0.15199561 -0.17343324 -0.19204226 -0.20024729 -0.19033465 -0.17039043 -0.1309223 -0.10063641 -0.064758837 -0.031177942 -0.031093989]]...]
INFO - root - 2017-12-06 06:35:27.244704: step 28510, loss = 0.81, batch loss = 0.59 (15.7 examples/sec; 0.510 sec/batch; 43h:02m:16s remains)
INFO - root - 2017-12-06 06:35:32.772572: step 28520, loss = 0.81, batch loss = 0.60 (14.2 examples/sec; 0.562 sec/batch; 47h:29m:04s remains)
INFO - root - 2017-12-06 06:35:38.632325: step 28530, loss = 0.80, batch loss = 0.59 (13.1 examples/sec; 0.610 sec/batch; 51h:28m:44s remains)
INFO - root - 2017-12-06 06:35:44.684105: step 28540, loss = 0.79, batch loss = 0.58 (13.4 examples/sec; 0.596 sec/batch; 50h:19m:50s remains)
INFO - root - 2017-12-06 06:35:50.911485: step 28550, loss = 0.78, batch loss = 0.57 (13.2 examples/sec; 0.605 sec/batch; 51h:04m:37s remains)
INFO - root - 2017-12-06 06:35:56.967246: step 28560, loss = 0.88, batch loss = 0.66 (12.6 examples/sec; 0.634 sec/batch; 53h:33m:54s remains)
INFO - root - 2017-12-06 06:36:02.469343: step 28570, loss = 0.71, batch loss = 0.50 (14.5 examples/sec; 0.553 sec/batch; 46h:43m:28s remains)
INFO - root - 2017-12-06 06:36:08.373639: step 28580, loss = 0.82, batch loss = 0.61 (12.9 examples/sec; 0.618 sec/batch; 52h:10m:20s remains)
INFO - root - 2017-12-06 06:36:14.386135: step 28590, loss = 0.80, batch loss = 0.58 (13.6 examples/sec; 0.588 sec/batch; 49h:37m:38s remains)
INFO - root - 2017-12-06 06:36:20.110876: step 28600, loss = 0.86, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 44h:18m:00s remains)
2017-12-06 06:36:20.842502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.078391068 -0.089776747 -0.097343892 -0.10440028 -0.10618342 -0.10381366 -0.097504385 -0.089599945 -0.084172547 -0.081977159 -0.07989125 -0.074515 -0.071455874 -0.075702555 -0.087024391][-0.040556844 -0.053309776 -0.066432908 -0.071535662 -0.077281147 -0.08113391 -0.08289405 -0.077074148 -0.075127929 -0.073631689 -0.070440248 -0.068029456 -0.062367588 -0.053145066 -0.046991918][-0.05634696 -0.064228207 -0.068946257 -0.071016677 -0.070808113 -0.0678597 -0.067191519 -0.07045652 -0.070953071 -0.070950672 -0.067486525 -0.067695223 -0.064332932 -0.057240188 -0.029024944][-0.020885587 -0.062482193 -0.073321708 -0.071446009 -0.068294644 -0.069413088 -0.067762829 -0.059866071 -0.058693595 -0.063259907 -0.066712186 -0.070664853 -0.069180921 -0.05396286 -0.029854141][0.2262367 0.10689264 -0.0024511889 -0.041049 -0.050219264 -0.047614142 -0.046336707 -0.042439524 -0.047446977 -0.038917419 -0.04227538 -0.079148374 -0.075362056 -0.076287493 -0.045320943][0.46527863 0.27929109 0.0975352 -0.010921277 -0.033907987 -0.027520895 -0.013866991 -0.017501332 -0.035202 -0.018905297 -0.030165233 -0.068137817 -0.098193429 -0.10395106 -0.12970769][0.61359721 0.37147167 0.15530264 0.017563045 -0.011336565 -0.010004863 0.0022551119 0.0030262098 -0.010506079 0.003588438 -0.0032548755 -0.0633668 -0.10400631 -0.11017097 -0.15899426][0.70836633 0.44016397 0.19304922 0.034605943 -0.012039177 -0.0073354095 0.0051430538 -0.013602033 -0.020625964 -0.011398584 -0.016777486 -0.071890935 -0.11315819 -0.1323289 -0.19146663][0.54640967 0.45315886 0.24029905 0.056397103 -0.005280979 0.0011099577 -0.01382529 -0.012896091 -0.017554522 -0.030161973 -0.040379934 -0.07732325 -0.10464422 -0.12548546 -0.18981488][0.32188356 0.32692796 0.17393559 0.0588947 -4.837662e-05 -0.0041197538 -0.010454796 -0.010715619 -0.023911618 -0.028032251 -0.038252149 -0.070451595 -0.074738696 -0.082461558 -0.11426514][0.12546891 0.17379361 0.097980551 0.03834451 -0.0030594468 -0.012180552 -0.021415181 -0.026667081 -0.021857247 -0.020263523 -0.015202962 -0.020399012 -0.029451504 -0.05024457 -0.068744518][-0.052786231 0.044018827 0.039980225 0.014868446 -0.0069483221 -0.017078839 -0.029539846 -0.020882465 -0.014541313 0.0051132441 0.023785688 0.040658392 0.023469716 0.0031881481 -0.026519485][-0.13787624 -0.055432569 -0.028757293 -0.012182564 -0.025730275 -0.013440356 -0.027870134 -0.041134253 -0.04224648 -0.024728261 -0.0093270168 -0.000857234 0.0015162304 -0.025664419 -0.044497941][-0.14100429 -0.099247627 -0.054645263 -0.038313545 -0.028081641 -0.021562748 -0.039372388 -0.048475966 -0.055231504 -0.056010213 -0.050632048 -0.056662481 -0.05199714 -0.067773558 -0.095852554][-0.079009131 -0.068090141 -0.048472211 -0.041668322 -0.038883809 -0.037944458 -0.047450881 -0.060191795 -0.078058913 -0.0959716 -0.10154621 -0.09601064 -0.097758852 -0.10340001 -0.12029105]]...]
INFO - root - 2017-12-06 06:36:26.948328: step 28610, loss = 0.88, batch loss = 0.67 (13.1 examples/sec; 0.612 sec/batch; 51h:39m:49s remains)
INFO - root - 2017-12-06 06:36:33.093957: step 28620, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 53h:05m:03s remains)
INFO - root - 2017-12-06 06:36:39.152185: step 28630, loss = 0.87, batch loss = 0.66 (12.9 examples/sec; 0.618 sec/batch; 52h:12m:14s remains)
INFO - root - 2017-12-06 06:36:45.023183: step 28640, loss = 0.82, batch loss = 0.61 (16.4 examples/sec; 0.486 sec/batch; 41h:03m:30s remains)
INFO - root - 2017-12-06 06:36:50.679580: step 28650, loss = 0.79, batch loss = 0.57 (13.9 examples/sec; 0.574 sec/batch; 48h:25m:00s remains)
INFO - root - 2017-12-06 06:36:56.671769: step 28660, loss = 0.83, batch loss = 0.61 (13.1 examples/sec; 0.612 sec/batch; 51h:40m:13s remains)
INFO - root - 2017-12-06 06:37:02.829382: step 28670, loss = 0.78, batch loss = 0.56 (13.7 examples/sec; 0.583 sec/batch; 49h:14m:33s remains)
INFO - root - 2017-12-06 06:37:09.017756: step 28680, loss = 0.76, batch loss = 0.55 (13.3 examples/sec; 0.603 sec/batch; 50h:54m:03s remains)
INFO - root - 2017-12-06 06:37:15.163575: step 28690, loss = 0.84, batch loss = 0.63 (13.3 examples/sec; 0.602 sec/batch; 50h:49m:42s remains)
INFO - root - 2017-12-06 06:37:20.974405: step 28700, loss = 0.82, batch loss = 0.60 (13.9 examples/sec; 0.577 sec/batch; 48h:39m:37s remains)
2017-12-06 06:37:21.618839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1101445 -0.21298808 -0.39857426 -0.66127539 -0.90811872 -1.0192938 -1.1393013 -1.2514184 -1.3501482 -1.3693838 -1.2425702 -1.0841053 -0.93312794 -0.63664496 -0.21419249][-0.19287816 -0.47167358 -0.6935333 -0.89596254 -1.1455543 -1.3151402 -1.5013142 -1.6582196 -1.8148806 -1.8783286 -1.77792 -1.5319159 -1.2613273 -0.95277721 -0.51919973][-0.53620911 -0.78133684 -0.79778582 -0.94583 -1.1598458 -1.3352494 -1.5272064 -1.7484674 -1.8746581 -1.9015112 -1.7946754 -1.5408099 -1.2622371 -0.98056555 -0.63327682][-0.63384962 -0.90614456 -1.07499 -1.034425 -1.0758117 -1.1457486 -1.228075 -1.3254189 -1.4070022 -1.4574032 -1.4134996 -1.306633 -1.1405292 -0.876422 -0.59063995][-0.57753193 -0.79532772 -0.93200231 -0.81162721 -0.67536229 -0.59158617 -0.59246582 -0.6103617 -0.653092 -0.7089147 -0.73245436 -0.72338504 -0.77260768 -0.72854769 -0.5958547][-0.34364358 -0.43541294 -0.37723652 -0.18588826 0.084494695 0.32359004 0.46222246 0.4388957 0.32376951 0.15392387 -0.052567665 -0.20172158 -0.35984153 -0.39704958 -0.46478939][-0.072272584 -0.031815376 0.080267489 0.34047332 0.66846812 1.0053201 1.1946228 1.2677491 1.1942937 0.87108922 0.51169735 0.14441633 -0.19307478 -0.33561903 -0.39006963][0.20390546 0.24457535 0.35027492 0.67229289 0.94398528 1.264739 1.4145985 1.4426646 1.2650061 0.96768016 0.62120736 0.15341565 -0.22669509 -0.47695738 -0.60695195][0.24075064 0.22266972 0.33877805 0.41341412 0.58010072 0.80546725 0.92796582 0.89976645 0.67884958 0.38109097 0.0094963834 -0.34015119 -0.5707792 -0.71616322 -0.75963056][0.00063162297 -0.11818523 -0.24047561 -0.33142245 -0.27194047 -0.18842204 -0.11811604 -0.075780243 -0.12964508 -0.29738683 -0.52757967 -0.73231733 -0.85409242 -0.78530979 -0.64758289][-0.35344735 -0.51074737 -0.77910304 -0.87395424 -0.92599756 -0.97050905 -0.947346 -0.95457053 -0.92982483 -0.90430993 -0.84884554 -0.82487172 -0.78785568 -0.706353 -0.54120326][-0.48030588 -0.77954197 -1.0400743 -1.1976473 -1.2990091 -1.3042777 -1.2901704 -1.3080621 -1.2197156 -1.0816925 -0.93023926 -0.77836329 -0.65279311 -0.47601762 -0.33200362][-0.47226468 -0.65677649 -0.95948255 -1.10112 -1.1511137 -1.1690185 -1.1099806 -1.1209373 -1.0919511 -0.92482007 -0.71955788 -0.59307027 -0.48576024 -0.32585615 -0.26827651][-0.24411993 -0.2681815 -0.46707231 -0.63736451 -0.77351993 -0.82669616 -0.77045816 -0.78294533 -0.77224582 -0.73409349 -0.58457518 -0.42375249 -0.3300561 -0.20268665 -0.13554046][0.15466976 0.099694684 -0.15069148 -0.21213421 -0.25571641 -0.26239163 -0.22761931 -0.35114709 -0.44906482 -0.52575403 -0.47219273 -0.39854205 -0.2292518 -0.07372269 0.028220847]]...]
INFO - root - 2017-12-06 06:37:27.735570: step 28710, loss = 0.75, batch loss = 0.53 (12.4 examples/sec; 0.643 sec/batch; 54h:15m:05s remains)
INFO - root - 2017-12-06 06:37:34.050593: step 28720, loss = 0.80, batch loss = 0.59 (12.8 examples/sec; 0.624 sec/batch; 52h:40m:26s remains)
INFO - root - 2017-12-06 06:37:40.239574: step 28730, loss = 0.78, batch loss = 0.56 (13.2 examples/sec; 0.608 sec/batch; 51h:18m:46s remains)
INFO - root - 2017-12-06 06:37:46.503234: step 28740, loss = 0.82, batch loss = 0.61 (12.6 examples/sec; 0.637 sec/batch; 53h:43m:00s remains)
INFO - root - 2017-12-06 06:37:52.716323: step 28750, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.656 sec/batch; 55h:23m:03s remains)
INFO - root - 2017-12-06 06:37:58.864648: step 28760, loss = 0.78, batch loss = 0.56 (12.7 examples/sec; 0.628 sec/batch; 52h:59m:28s remains)
INFO - root - 2017-12-06 06:38:04.666942: step 28770, loss = 0.95, batch loss = 0.74 (14.0 examples/sec; 0.573 sec/batch; 48h:19m:04s remains)
INFO - root - 2017-12-06 06:38:10.720074: step 28780, loss = 0.72, batch loss = 0.50 (13.2 examples/sec; 0.604 sec/batch; 50h:59m:16s remains)
INFO - root - 2017-12-06 06:38:16.646286: step 28790, loss = 0.80, batch loss = 0.59 (13.2 examples/sec; 0.605 sec/batch; 51h:04m:14s remains)
INFO - root - 2017-12-06 06:38:22.880518: step 28800, loss = 0.84, batch loss = 0.62 (13.6 examples/sec; 0.586 sec/batch; 49h:27m:26s remains)
2017-12-06 06:38:23.512048: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19249034 0.29522952 0.028532274 -0.31251463 -0.87067151 -1.1184076 -1.4557332 -1.6147238 -1.6906078 -1.4179966 -1.0201315 -0.80277908 -0.86966187 -1.39085 -1.7159599][0.4162133 0.3220264 -0.14827675 -0.35325336 -0.74954879 -1.4624192 -2.1330187 -2.1805038 -2.266695 -2.3771524 -2.5889072 -2.0608089 -1.6440244 -1.7387645 -1.7801867][0.34276375 0.28552151 0.14628041 -0.32428688 -0.833674 -0.95782536 -1.2009912 -1.7027155 -2.2814054 -2.6068382 -2.9314115 -3.0457015 -3.2803991 -2.944993 -2.3719389][0.2412805 -0.13679311 0.16158289 0.16996211 -0.027950011 -0.72285503 -1.5085372 -1.4868808 -1.7716091 -2.4861643 -3.2002165 -3.516031 -3.5779004 -3.6382873 -3.6350842][0.4562448 0.11789375 0.10254627 0.10007361 0.2591753 0.0083709732 -0.37492964 -1.0646856 -1.8530527 -2.0873189 -2.4686978 -3.0087302 -3.590205 -3.6796753 -3.5406935][0.93850571 0.5092932 0.40629715 0.5030005 0.49253532 0.33051091 0.21484932 0.13644642 -0.21953025 -0.9508726 -1.7028488 -2.0530255 -2.3235562 -2.8705597 -3.1914299][1.6776057 0.99898916 0.39867648 0.41569939 0.43574658 0.49686185 0.66001868 0.6335271 0.79477954 0.83768314 0.52743596 -0.01345893 -0.5285095 -0.82484931 -0.88103986][1.3240337 0.67636466 0.068345986 -0.090934515 -0.1644752 -0.04410018 0.245861 0.70476538 1.2474666 1.5129676 1.9872782 1.9326098 1.536785 1.1787866 0.69719505][-0.090528078 -0.138189 -0.19972371 -0.51324677 -0.713402 -0.56872892 -0.33673018 -0.056707431 0.40926498 1.4217224 2.3161347 2.7885146 3.2434647 2.9578447 2.3049283][-1.0322887 -1.1019241 -1.2230536 -1.2534896 -1.3469689 -1.3642076 -1.2569669 -0.74494809 -0.10664814 0.52017438 1.2986764 2.5790665 3.7233994 3.9404366 3.9990966][-2.2738214 -2.0274289 -1.9474295 -2.2152278 -2.244333 -1.7210449 -1.3957458 -1.322991 -1.165693 -0.63496357 0.23266283 1.0916167 1.869903 2.9859331 3.4969747][-2.240005 -2.5590677 -2.3730857 -2.0336032 -1.9464418 -2.043191 -2.0677691 -1.7904096 -1.5916417 -1.1698964 -0.67737812 -0.42859527 0.14744669 0.78154892 1.0244261][-2.33472 -2.226155 -1.9972987 -2.1034462 -2.1223571 -1.8291335 -1.7944666 -1.8349415 -2.0380077 -1.6911938 -1.3242819 -1.0193261 -0.70861429 -0.68252975 -0.4252601][-1.2184438 -1.2418938 -1.3398383 -0.99767888 -1.114216 -1.4134921 -1.477035 -1.3218828 -1.5389714 -1.4261723 -1.3822262 -1.4595116 -1.4018837 -1.4093592 -1.4869984][0.45354703 0.5639711 0.18745023 0.058835022 -0.21457008 -0.656663 -1.0309118 -1.2641712 -1.4973378 -1.6182654 -1.5544684 -1.5250552 -1.7311385 -1.8660567 -1.9845864]]...]
INFO - root - 2017-12-06 06:38:29.689542: step 28810, loss = 0.80, batch loss = 0.58 (13.2 examples/sec; 0.608 sec/batch; 51h:14m:53s remains)
INFO - root - 2017-12-06 06:38:35.983669: step 28820, loss = 0.74, batch loss = 0.52 (12.6 examples/sec; 0.634 sec/batch; 53h:29m:10s remains)
INFO - root - 2017-12-06 06:38:42.091408: step 28830, loss = 0.81, batch loss = 0.59 (12.7 examples/sec; 0.630 sec/batch; 53h:06m:30s remains)
INFO - root - 2017-12-06 06:38:47.130036: step 28840, loss = 0.86, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 38h:16m:53s remains)
INFO - root - 2017-12-06 06:38:51.559353: step 28850, loss = 0.87, batch loss = 0.66 (18.7 examples/sec; 0.428 sec/batch; 36h:07m:28s remains)
INFO - root - 2017-12-06 06:38:56.068565: step 28860, loss = 0.83, batch loss = 0.61 (18.3 examples/sec; 0.438 sec/batch; 36h:54m:09s remains)
INFO - root - 2017-12-06 06:39:00.452376: step 28870, loss = 0.78, batch loss = 0.57 (17.6 examples/sec; 0.455 sec/batch; 38h:24m:28s remains)
INFO - root - 2017-12-06 06:39:04.896513: step 28880, loss = 0.89, batch loss = 0.67 (19.8 examples/sec; 0.403 sec/batch; 33h:59m:52s remains)
INFO - root - 2017-12-06 06:39:09.138965: step 28890, loss = 0.80, batch loss = 0.58 (17.1 examples/sec; 0.468 sec/batch; 39h:28m:24s remains)
INFO - root - 2017-12-06 06:39:13.629476: step 28900, loss = 0.87, batch loss = 0.65 (18.0 examples/sec; 0.444 sec/batch; 37h:24m:47s remains)
2017-12-06 06:39:14.191644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086235337 -0.086147286 -0.08626914 -0.086633243 -0.0875622 -0.08925949 -0.091558859 -0.093575008 -0.093998283 -0.092518508 -0.090144478 -0.088117175 -0.086906672 -0.086325675 -0.086488195][-0.086209171 -0.08620438 -0.08663132 -0.087859124 -0.090669446 -0.095207155 -0.10128695 -0.10772416 -0.11131473 -0.10907751 -0.10189249 -0.094056889 -0.089008234 -0.086796358 -0.086696312][-0.0863752 -0.086502358 -0.087608062 -0.09063103 -0.095661022 -0.098595254 -0.099415071 -0.10411084 -0.11299314 -0.11815497 -0.11260551 -0.10085276 -0.091550954 -0.087057225 -0.086669475][-0.086421348 -0.086725488 -0.088995554 -0.095202416 -0.10278429 -0.10169077 -0.0917565 -0.088536829 -0.099780783 -0.11598823 -0.11811192 -0.10621081 -0.093612075 -0.08693336 -0.086243644][-0.086448736 -0.087063596 -0.090801783 -0.098676771 -0.1020372 -0.090038523 -0.071195439 -0.070284046 -0.091129996 -0.1178729 -0.12518591 -0.1124754 -0.095940918 -0.086140484 -0.085213408][-0.086507156 -0.087630525 -0.09261097 -0.099342473 -0.088083372 -0.046549223 0.00022821128 0.0072503611 -0.029835522 -0.083875813 -0.11229061 -0.10940818 -0.094605 -0.084152244 -0.08296115][-0.086697787 -0.088262133 -0.094819061 -0.10238773 -0.08609473 -0.031680282 0.032692119 0.055297084 0.019937478 -0.047183357 -0.091767505 -0.10150709 -0.092397317 -0.083417714 -0.082278684][-0.086908571 -0.0887731 -0.095939085 -0.10193524 -0.088062562 -0.046564505 0.004976362 0.025942929 -0.0010360256 -0.057901349 -0.094914943 -0.10171268 -0.092586331 -0.084602281 -0.083840989][-0.0870008 -0.089492448 -0.0972764 -0.10183743 -0.089886472 -0.057148788 -0.018060461 -0.00066038966 -0.017605118 -0.06122452 -0.091673329 -0.098461352 -0.092151389 -0.085866123 -0.085084021][-0.086867981 -0.089708589 -0.098518223 -0.10871166 -0.10846689 -0.087307587 -0.053766131 -0.030254591 -0.032541029 -0.059591085 -0.083046414 -0.091528893 -0.089404218 -0.085881472 -0.085902892][-0.086543985 -0.088318571 -0.094475381 -0.10510705 -0.11211655 -0.10313308 -0.079198271 -0.0574137 -0.0534387 -0.068571135 -0.0831479 -0.088706374 -0.086890854 -0.084090769 -0.083896033][-0.086269028 -0.086862206 -0.0894805 -0.095549591 -0.10305914 -0.10385583 -0.094570182 -0.083078958 -0.078326918 -0.08285597 -0.08764 -0.089148484 -0.087730825 -0.0862298 -0.086068667][-0.086210683 -0.0863686 -0.08741378 -0.090453386 -0.095383018 -0.099197417 -0.098289333 -0.095068157 -0.092026606 -0.091361649 -0.09002167 -0.08833617 -0.087208807 -0.0866505 -0.086689927][-0.086329073 -0.086186633 -0.086566061 -0.087707922 -0.089025743 -0.088132232 -0.085437208 -0.083141029 -0.082901783 -0.084377833 -0.085535109 -0.085954443 -0.086107925 -0.086041242 -0.086227767][-0.086608209 -0.086275086 -0.086357608 -0.088051721 -0.089208484 -0.088404424 -0.085878618 -0.083347149 -0.082772687 -0.083512016 -0.085287228 -0.085761614 -0.086000457 -0.085954912 -0.086192638]]...]
INFO - root - 2017-12-06 06:39:18.594216: step 28910, loss = 0.83, batch loss = 0.62 (18.3 examples/sec; 0.437 sec/batch; 36h:53m:17s remains)
INFO - root - 2017-12-06 06:39:23.072303: step 28920, loss = 0.81, batch loss = 0.60 (18.4 examples/sec; 0.434 sec/batch; 36h:35m:40s remains)
INFO - root - 2017-12-06 06:39:27.568325: step 28930, loss = 0.87, batch loss = 0.65 (17.5 examples/sec; 0.458 sec/batch; 38h:38m:33s remains)
INFO - root - 2017-12-06 06:39:31.963726: step 28940, loss = 0.72, batch loss = 0.50 (18.2 examples/sec; 0.440 sec/batch; 37h:07m:10s remains)
INFO - root - 2017-12-06 06:39:36.513178: step 28950, loss = 0.84, batch loss = 0.62 (18.2 examples/sec; 0.439 sec/batch; 36h:59m:19s remains)
INFO - root - 2017-12-06 06:39:41.040516: step 28960, loss = 0.82, batch loss = 0.61 (17.7 examples/sec; 0.453 sec/batch; 38h:11m:00s remains)
INFO - root - 2017-12-06 06:39:45.552675: step 28970, loss = 0.74, batch loss = 0.53 (17.9 examples/sec; 0.447 sec/batch; 37h:40m:32s remains)
INFO - root - 2017-12-06 06:39:49.699983: step 28980, loss = 0.80, batch loss = 0.59 (17.8 examples/sec; 0.450 sec/batch; 37h:55m:08s remains)
INFO - root - 2017-12-06 06:39:54.301426: step 28990, loss = 0.81, batch loss = 0.60 (17.4 examples/sec; 0.461 sec/batch; 38h:51m:14s remains)
INFO - root - 2017-12-06 06:39:58.715099: step 29000, loss = 0.75, batch loss = 0.54 (17.8 examples/sec; 0.450 sec/batch; 37h:57m:18s remains)
2017-12-06 06:39:59.182328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.090406835 -0.090669118 -0.090801433 -0.090923123 -0.090997763 -0.091011614 -0.090985849 -0.090951584 -0.090947688 -0.0910465 -0.091337621 -0.091744453 -0.092019118 -0.091953389 -0.091365747][-0.090274327 -0.090543896 -0.090674035 -0.0908052 -0.090885855 -0.090891011 -0.090827458 -0.0907421 -0.090701982 -0.090809382 -0.091215946 -0.091873042 -0.092315942 -0.092182979 -0.091068804][-0.090432152 -0.090644188 -0.090799548 -0.090981111 -0.091109268 -0.091120817 -0.091020674 -0.090857983 -0.090770952 -0.090914614 -0.091462471 -0.092382215 -0.093024038 -0.092923783 -0.091482744][-0.090685464 -0.090929694 -0.091142 -0.091380395 -0.09154062 -0.091509119 -0.091262095 -0.090977423 -0.090791456 -0.090922415 -0.091525793 -0.092540383 -0.093316741 -0.093305595 -0.092121892][-0.090929009 -0.091148406 -0.091354817 -0.091558158 -0.091629095 -0.091438375 -0.091003358 -0.090520158 -0.0901622 -0.09020558 -0.0907813 -0.091698095 -0.092446916 -0.09253934 -0.091973543][-0.09102425 -0.091205828 -0.091360033 -0.091479406 -0.091375388 -0.090953983 -0.090272866 -0.089496464 -0.088916957 -0.088954717 -0.08957281 -0.090490922 -0.091295488 -0.091605283 -0.091423236][-0.0909378 -0.091238037 -0.091462769 -0.091613755 -0.091382936 -0.090598591 -0.089518286 -0.088416785 -0.087752469 -0.088003643 -0.088879809 -0.089904554 -0.0907626 -0.0912594 -0.0913084][-0.090886496 -0.09132535 -0.091727287 -0.092042893 -0.091839731 -0.090847023 -0.089259386 -0.087731391 -0.087156475 -0.08769612 -0.088793628 -0.089848675 -0.090711012 -0.091185823 -0.091274761][-0.090810195 -0.09139885 -0.0920018 -0.092486739 -0.092384584 -0.091334939 -0.089534052 -0.087885372 -0.087518021 -0.08829429 -0.089454785 -0.090380557 -0.0909787 -0.091224536 -0.091273651][-0.090719476 -0.0913424 -0.091939308 -0.092414953 -0.092341416 -0.091492645 -0.090115726 -0.08893542 -0.088696353 -0.089376658 -0.090282783 -0.090924911 -0.09124998 -0.09135028 -0.091676563][-0.090494744 -0.091036715 -0.091433667 -0.091714211 -0.091568716 -0.091025382 -0.090230145 -0.08950039 -0.089453861 -0.089949094 -0.09050186 -0.090947896 -0.091267355 -0.091534235 -0.092197374][-0.0904384 -0.090832904 -0.091054171 -0.091141418 -0.091009028 -0.090697154 -0.090222441 -0.089760982 -0.089736409 -0.090008281 -0.0903646 -0.090656817 -0.090946004 -0.091321126 -0.092123918][-0.090468235 -0.090842977 -0.090984769 -0.090994768 -0.090958118 -0.090909317 -0.090704456 -0.09044794 -0.09039849 -0.090460129 -0.090517096 -0.090438157 -0.090419404 -0.090730324 -0.091376022][-0.090482913 -0.090841219 -0.090955608 -0.090958491 -0.090973057 -0.0910271 -0.091039 -0.0910741 -0.091207765 -0.091340341 -0.091085427 -0.090399593 -0.089848362 -0.0898962 -0.090404093][-0.090448417 -0.090757 -0.090837032 -0.090846777 -0.090891644 -0.091057852 -0.091439642 -0.092006557 -0.092590973 -0.092735708 -0.091881461 -0.090130448 -0.088836059 -0.088764809 -0.089628175]]...]
INFO - root - 2017-12-06 06:40:03.674368: step 29010, loss = 0.84, batch loss = 0.62 (17.5 examples/sec; 0.458 sec/batch; 38h:35m:14s remains)
INFO - root - 2017-12-06 06:40:08.183723: step 29020, loss = 0.83, batch loss = 0.61 (18.0 examples/sec; 0.444 sec/batch; 37h:23m:51s remains)
INFO - root - 2017-12-06 06:40:12.642204: step 29030, loss = 0.78, batch loss = 0.57 (18.1 examples/sec; 0.442 sec/batch; 37h:16m:36s remains)
INFO - root - 2017-12-06 06:40:17.150525: step 29040, loss = 0.81, batch loss = 0.60 (18.5 examples/sec; 0.433 sec/batch; 36h:29m:05s remains)
INFO - root - 2017-12-06 06:40:21.700846: step 29050, loss = 0.77, batch loss = 0.55 (17.2 examples/sec; 0.464 sec/batch; 39h:05m:59s remains)
INFO - root - 2017-12-06 06:40:26.201590: step 29060, loss = 0.82, batch loss = 0.61 (17.4 examples/sec; 0.459 sec/batch; 38h:42m:52s remains)
INFO - root - 2017-12-06 06:40:30.415067: step 29070, loss = 0.75, batch loss = 0.53 (18.0 examples/sec; 0.445 sec/batch; 37h:28m:34s remains)
INFO - root - 2017-12-06 06:40:34.867258: step 29080, loss = 0.74, batch loss = 0.53 (16.9 examples/sec; 0.473 sec/batch; 39h:52m:43s remains)
INFO - root - 2017-12-06 06:40:39.360010: step 29090, loss = 0.78, batch loss = 0.57 (17.8 examples/sec; 0.450 sec/batch; 37h:55m:41s remains)
INFO - root - 2017-12-06 06:40:43.751277: step 29100, loss = 0.71, batch loss = 0.50 (18.4 examples/sec; 0.435 sec/batch; 36h:38m:47s remains)
2017-12-06 06:40:44.302944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.87819785 -0.88432837 -0.81157553 -0.80934751 -0.79064244 -0.85651129 -1.0321685 -1.1742487 -1.3726803 -1.4505024 -1.3834226 -1.3279121 -1.3104167 -1.278963 -1.121848][-0.38874918 -0.22333875 -0.1584051 -0.25048864 -0.39308006 -0.582112 -0.67655224 -0.90613514 -1.1479952 -1.1085825 -1.0533344 -0.84548092 -0.67294079 -0.34030473 -0.10559453][-0.54592365 -0.44195855 -0.36331731 -0.27731866 -0.22989564 -0.38075048 -0.59559643 -0.76201624 -0.81618667 -0.82847983 -0.83635962 -0.57505429 -0.3158263 0.055380017 0.36340964][-0.53681809 -0.52112859 -0.56522471 -0.6329295 -0.63290077 -0.57218581 -0.50681978 -0.47942609 -0.47457182 -0.5221464 -0.56168735 -0.5037359 -0.35380828 -0.016842417 0.12886265][-0.62506282 -0.52743047 -0.61036462 -0.67303693 -0.79494977 -0.8652032 -0.81025606 -0.52287453 -0.28371766 -0.21188273 -0.20493713 -0.27484488 -0.26451045 -0.24355023 -0.20121861][-0.5749644 -0.55461663 -0.59104228 -0.62838286 -0.68361622 -0.61267561 -0.62148386 -0.64544785 -0.49323493 -0.27104455 -0.17094901 -0.12643869 -0.25524205 -0.39456165 -0.32757604][-0.408508 -0.30641341 -0.24816096 -0.13914418 -0.020734437 0.16354768 0.3015995 0.42078131 0.31879461 0.1208439 0.058265358 0.01521112 -0.11834082 -0.22699745 -0.34061274][-0.096647367 -0.12640633 -0.071475834 0.22224419 0.54809922 0.93428421 1.2017384 1.4492829 1.4504755 1.1968079 0.80096734 0.35285437 0.01724001 -0.16366509 -0.23845708][0.22572489 0.15347774 0.23328738 0.44346094 0.70824653 1.098791 1.3917409 1.5517982 1.5305337 1.4546094 1.2123976 0.75780982 0.22661535 -0.20247409 -0.46220988][0.18406211 0.22541071 0.30746716 0.44600636 0.72676659 0.98724341 1.1702031 1.1877253 0.98216832 0.78674507 0.62373638 0.47494221 0.1470172 -0.26522946 -0.64694118][-0.23452604 -0.22758268 -0.16335532 -0.037005793 0.098987341 0.24131353 0.40670675 0.43445045 0.303558 0.11853263 -0.095625348 -0.17410523 -0.24539283 -0.33126482 -0.50254351][-0.50723147 -0.49478555 -0.48160136 -0.39495707 -0.29104695 -0.22057085 -0.141847 -0.12896544 -0.17311348 -0.27650273 -0.43168896 -0.48668724 -0.54752892 -0.49068522 -0.49333221][-0.6307115 -0.69129241 -0.72432083 -0.73819649 -0.62633836 -0.48678929 -0.38943714 -0.33175671 -0.25910169 -0.34359145 -0.44571573 -0.50194937 -0.50681406 -0.48173583 -0.52063572][-0.41195083 -0.5437935 -0.63924032 -0.65811712 -0.622821 -0.59955668 -0.54897416 -0.45783383 -0.38117039 -0.37307489 -0.3604942 -0.3834343 -0.37941295 -0.35407597 -0.37217069][-0.27360982 -0.34651285 -0.4702704 -0.53036869 -0.46639162 -0.37388086 -0.35950398 -0.33523253 -0.29365903 -0.30931264 -0.30597374 -0.2850329 -0.28338808 -0.31893796 -0.31690517]]...]
INFO - root - 2017-12-06 06:40:48.753791: step 29110, loss = 0.86, batch loss = 0.65 (17.1 examples/sec; 0.468 sec/batch; 39h:26m:47s remains)
INFO - root - 2017-12-06 06:40:53.223177: step 29120, loss = 0.84, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 37h:15m:17s remains)
INFO - root - 2017-12-06 06:40:57.726487: step 29130, loss = 0.85, batch loss = 0.64 (18.8 examples/sec; 0.425 sec/batch; 35h:50m:05s remains)
INFO - root - 2017-12-06 06:41:02.234212: step 29140, loss = 0.84, batch loss = 0.62 (18.2 examples/sec; 0.440 sec/batch; 37h:05m:19s remains)
INFO - root - 2017-12-06 06:41:06.642779: step 29150, loss = 0.85, batch loss = 0.63 (18.2 examples/sec; 0.440 sec/batch; 37h:03m:46s remains)
INFO - root - 2017-12-06 06:41:11.198103: step 29160, loss = 0.80, batch loss = 0.58 (18.7 examples/sec; 0.429 sec/batch; 36h:08m:04s remains)
INFO - root - 2017-12-06 06:41:15.423170: step 29170, loss = 0.85, batch loss = 0.64 (18.1 examples/sec; 0.443 sec/batch; 37h:19m:14s remains)
INFO - root - 2017-12-06 06:41:19.926929: step 29180, loss = 0.78, batch loss = 0.56 (18.0 examples/sec; 0.445 sec/batch; 37h:30m:35s remains)
INFO - root - 2017-12-06 06:41:24.457758: step 29190, loss = 0.77, batch loss = 0.55 (17.0 examples/sec; 0.471 sec/batch; 39h:39m:10s remains)
INFO - root - 2017-12-06 06:41:29.029002: step 29200, loss = 0.89, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 37h:30m:41s remains)
2017-12-06 06:41:29.526282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10758337 -0.13864657 -0.19047011 -0.26719916 -0.36860916 -0.47689065 -0.56217712 -0.6259824 -0.64795637 -0.61788416 -0.51695275 -0.34210008 -0.15362054 -0.012248106 0.11145173][-0.1282824 -0.17346518 -0.23560864 -0.29840487 -0.3737914 -0.46944478 -0.56392121 -0.66821027 -0.7418009 -0.77786744 -0.73894584 -0.61283374 -0.44852722 -0.24721842 0.0050030053][-0.14479116 -0.19342959 -0.22918193 -0.2520088 -0.28428492 -0.34135419 -0.4260349 -0.57336724 -0.71894121 -0.83092332 -0.85736489 -0.77472997 -0.61990285 -0.40159449 -0.065756373][-0.13519998 -0.17618972 -0.17107771 -0.13747576 -0.10923398 -0.10329998 -0.15110907 -0.33060029 -0.56509036 -0.76357114 -0.83243692 -0.73357868 -0.53984106 -0.31588259 0.011037588][-0.14035177 -0.17028306 -0.13555071 -0.029750381 0.11747536 0.23436457 0.24664369 0.043960854 -0.26814389 -0.58519912 -0.77920878 -0.75452936 -0.55930185 -0.3580201 -0.13128093][-0.13790336 -0.15683514 -0.10804773 0.038194641 0.24241495 0.4956449 0.65064692 0.56203854 0.26693442 -0.11104875 -0.44693777 -0.60148633 -0.523221 -0.34449092 -0.075637735][-0.10455739 -0.1140549 -0.05631328 0.10034309 0.31486148 0.60772908 0.84578443 0.87947071 0.6743288 0.31509966 -0.043841865 -0.315853 -0.41728202 -0.35194612 -0.16239715][-0.051815163 -0.034408879 0.013371743 0.1229846 0.31025797 0.60464811 0.86117172 0.99664354 0.89575589 0.58792055 0.2303797 -0.088351667 -0.28000647 -0.3522802 -0.30145448][-0.043992821 0.0032588542 0.068940982 0.16361004 0.30620205 0.55471468 0.76482737 0.88461018 0.81309378 0.56551683 0.28955898 0.0417742 -0.10010477 -0.18818361 -0.23794363][-0.065555491 -0.037009194 0.0046109557 0.051872849 0.14461216 0.319803 0.47524473 0.59849191 0.57491779 0.40107435 0.17667454 -0.0062263533 -0.081866533 -0.1014933 -0.13673022][-0.082555175 -0.088041782 -0.090479657 -0.076253794 -0.033323705 0.068202868 0.19091105 0.26751241 0.23390725 0.13991408 0.025190085 -0.083483957 -0.11320811 -0.11544624 -0.10941834][-0.0891048 -0.10507848 -0.13470864 -0.15779781 -0.15684649 -0.12702173 -0.0654054 0.0076853856 0.00966645 -0.01329679 -0.052839763 -0.092767149 -0.095416017 -0.091287464 -0.086217165][-0.085762948 -0.093426459 -0.11758198 -0.15350206 -0.16271004 -0.17673951 -0.16480458 -0.14837781 -0.1499716 -0.15756509 -0.13819978 -0.11902791 -0.086429514 -0.080095813 -0.078487173][-0.08550252 -0.087281242 -0.093772478 -0.10787821 -0.10812528 -0.11114726 -0.10443907 -0.10314412 -0.11203449 -0.1178825 -0.11632445 -0.10111799 -0.074223645 -0.064584583 -0.064854629][-0.085290194 -0.0858395 -0.089926444 -0.096254639 -0.10174143 -0.10143121 -0.096044824 -0.096102282 -0.0954707 -0.095398195 -0.09333308 -0.084723011 -0.070091382 -0.062411852 -0.051156081]]...]
INFO - root - 2017-12-06 06:41:33.924156: step 29210, loss = 0.88, batch loss = 0.66 (18.1 examples/sec; 0.443 sec/batch; 37h:17m:03s remains)
INFO - root - 2017-12-06 06:41:38.407569: step 29220, loss = 0.80, batch loss = 0.59 (17.1 examples/sec; 0.467 sec/batch; 39h:21m:30s remains)
INFO - root - 2017-12-06 06:41:42.761201: step 29230, loss = 0.76, batch loss = 0.55 (18.2 examples/sec; 0.441 sec/batch; 37h:07m:47s remains)
INFO - root - 2017-12-06 06:41:47.254376: step 29240, loss = 0.75, batch loss = 0.54 (18.8 examples/sec; 0.426 sec/batch; 35h:54m:33s remains)
INFO - root - 2017-12-06 06:41:51.701381: step 29250, loss = 0.76, batch loss = 0.54 (17.2 examples/sec; 0.466 sec/batch; 39h:12m:53s remains)
INFO - root - 2017-12-06 06:41:55.922994: step 29260, loss = 0.71, batch loss = 0.49 (18.3 examples/sec; 0.436 sec/batch; 36h:43m:26s remains)
INFO - root - 2017-12-06 06:42:00.364301: step 29270, loss = 0.82, batch loss = 0.61 (18.7 examples/sec; 0.427 sec/batch; 36h:00m:27s remains)
INFO - root - 2017-12-06 06:42:04.856058: step 29280, loss = 0.73, batch loss = 0.52 (17.5 examples/sec; 0.458 sec/batch; 38h:34m:46s remains)
INFO - root - 2017-12-06 06:42:09.406618: step 29290, loss = 0.85, batch loss = 0.63 (16.0 examples/sec; 0.499 sec/batch; 42h:02m:00s remains)
INFO - root - 2017-12-06 06:42:13.842673: step 29300, loss = 0.88, batch loss = 0.66 (18.3 examples/sec; 0.437 sec/batch; 36h:48m:52s remains)
2017-12-06 06:42:14.305954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071451291 -0.071488485 -0.071615115 -0.071722142 -0.071825787 -0.071866252 -0.071857348 -0.071806155 -0.071759358 -0.071782075 -0.071854137 -0.071899459 -0.071929604 -0.071946114 -0.072034284][-0.071961008 -0.07191819 -0.071981087 -0.072080255 -0.072127484 -0.072134092 -0.072071344 -0.071973294 -0.071912795 -0.071971983 -0.072125889 -0.072303765 -0.072505921 -0.072686285 -0.072866261][-0.071670361 -0.071401566 -0.071398 -0.07146582 -0.071511835 -0.071467645 -0.071330361 -0.071133837 -0.071021236 -0.071122818 -0.071337126 -0.071608059 -0.071941674 -0.072294779 -0.072598949][-0.071538717 -0.071194634 -0.071174175 -0.071227223 -0.071290828 -0.07116507 -0.070891559 -0.070556596 -0.070357919 -0.070434421 -0.070668526 -0.071004719 -0.07141684 -0.071873024 -0.072321773][-0.071590394 -0.0713032 -0.071297 -0.071345009 -0.071321376 -0.07101576 -0.070436373 -0.069777019 -0.069412172 -0.069523841 -0.069880031 -0.07038182 -0.070928589 -0.07148134 -0.072023615][-0.071250707 -0.070855722 -0.070720591 -0.07060799 -0.07039158 -0.069809057 -0.068916261 -0.067998812 -0.067672916 -0.0680206 -0.068661675 -0.069458291 -0.0702275 -0.071001932 -0.071750894][-0.070757404 -0.070220135 -0.069954216 -0.069667369 -0.069239132 -0.068533815 -0.067567691 -0.066595338 -0.066391215 -0.066933721 -0.067759864 -0.068733692 -0.069658741 -0.070610479 -0.071526408][-0.070611671 -0.070033759 -0.0698085 -0.069647759 -0.06934163 -0.068940796 -0.068261385 -0.067470863 -0.067152694 -0.067347184 -0.068002343 -0.068833657 -0.069619067 -0.070526965 -0.07143946][-0.070522033 -0.070046887 -0.070037618 -0.070130385 -0.070151195 -0.070146576 -0.069863528 -0.069327436 -0.068866283 -0.06872955 -0.0690752 -0.06964457 -0.070213646 -0.070909619 -0.071645513][-0.070306629 -0.069907434 -0.07002721 -0.070322819 -0.070722327 -0.071175709 -0.071355581 -0.071285315 -0.070943058 -0.070603766 -0.070541948 -0.070622608 -0.070777453 -0.071166605 -0.071697786][-0.0702358 -0.069948778 -0.07022918 -0.070816562 -0.07157138 -0.072325975 -0.072777838 -0.072802469 -0.0724341 -0.071898118 -0.071491465 -0.071225323 -0.071188629 -0.0713971 -0.071784616][-0.070229113 -0.0699656 -0.070246637 -0.070787176 -0.07156156 -0.0723608 -0.072842732 -0.072899185 -0.072577693 -0.072094813 -0.0717296 -0.071496129 -0.071493328 -0.071686305 -0.072020218][-0.070495009 -0.070183083 -0.070242912 -0.07035505 -0.07064943 -0.07104867 -0.071347505 -0.071477666 -0.071420752 -0.071343578 -0.071383581 -0.071433522 -0.071549818 -0.071795076 -0.072137535][-0.071115442 -0.070783846 -0.070688657 -0.070648953 -0.070688829 -0.070783831 -0.07090354 -0.0709861 -0.071032532 -0.071140364 -0.071343288 -0.071520306 -0.071739368 -0.071978472 -0.072228596][-0.071831912 -0.071572758 -0.071506068 -0.071414866 -0.071312577 -0.0712412 -0.071265176 -0.071348414 -0.071446031 -0.071573339 -0.071780413 -0.0719011 -0.072041772 -0.072215065 -0.072376572]]...]
INFO - root - 2017-12-06 06:42:18.777661: step 29310, loss = 0.71, batch loss = 0.50 (16.3 examples/sec; 0.490 sec/batch; 41h:14m:45s remains)
INFO - root - 2017-12-06 06:42:23.317227: step 29320, loss = 0.82, batch loss = 0.61 (18.7 examples/sec; 0.427 sec/batch; 35h:59m:06s remains)
INFO - root - 2017-12-06 06:42:27.798135: step 29330, loss = 0.87, batch loss = 0.65 (18.7 examples/sec; 0.428 sec/batch; 36h:05m:07s remains)
INFO - root - 2017-12-06 06:42:32.242203: step 29340, loss = 0.82, batch loss = 0.60 (18.6 examples/sec; 0.431 sec/batch; 36h:18m:15s remains)
INFO - root - 2017-12-06 06:42:36.430643: step 29350, loss = 0.80, batch loss = 0.59 (17.6 examples/sec; 0.455 sec/batch; 38h:16m:48s remains)
INFO - root - 2017-12-06 06:42:40.918277: step 29360, loss = 0.81, batch loss = 0.60 (15.8 examples/sec; 0.505 sec/batch; 42h:33m:23s remains)
INFO - root - 2017-12-06 06:42:45.395277: step 29370, loss = 0.84, batch loss = 0.63 (17.3 examples/sec; 0.463 sec/batch; 38h:58m:19s remains)
INFO - root - 2017-12-06 06:42:49.766276: step 29380, loss = 0.81, batch loss = 0.60 (18.4 examples/sec; 0.435 sec/batch; 36h:38m:02s remains)
INFO - root - 2017-12-06 06:42:54.213077: step 29390, loss = 0.78, batch loss = 0.57 (16.0 examples/sec; 0.500 sec/batch; 42h:07m:01s remains)
INFO - root - 2017-12-06 06:42:58.630662: step 29400, loss = 0.71, batch loss = 0.50 (18.4 examples/sec; 0.435 sec/batch; 36h:39m:36s remains)
2017-12-06 06:42:59.124710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.21400347 -0.12804301 0.0033593625 -0.086313367 -0.35076633 -0.42966813 -0.34244785 -0.31067967 -0.2486909 -0.22171548 -0.20618734 -0.13089737 -0.22024235 -0.15891284 0.10675905][0.044613235 0.019841939 0.14174491 0.10827696 -0.0654708 -0.22990561 -0.31606609 -0.20412058 -0.084852427 -0.023097187 -0.017500281 -0.026279509 -0.053437904 -0.03221627 0.1257982][-0.071441226 -0.039354559 0.045176126 0.028431647 -0.12209668 -0.22997755 -0.29189011 -0.34447068 -0.33724871 -0.3192848 -0.32586813 -0.31070527 -0.20619498 -0.099698812 -0.014687389][0.20856005 0.078863047 0.048478119 -0.018625982 -0.13686751 -0.28436315 -0.40862525 -0.40556571 -0.36822829 -0.35177052 -0.43298429 -0.45451325 -0.45500779 -0.39258909 -0.19988051][0.3810378 0.40301144 0.350256 0.17397127 0.012797609 -0.11533026 -0.25147459 -0.25278172 -0.20161374 -0.19717187 -0.25233093 -0.38861391 -0.37870383 -0.24196818 -0.026968643][0.37697092 0.44742253 0.42066059 0.308807 0.1233632 -0.088274069 -0.15597451 -0.15867871 -0.22773317 -0.17682053 -0.17052853 -0.19302249 -0.25055018 -0.3577323 -0.23459452][0.5395965 0.66790473 0.65860343 0.49535355 0.30392906 0.14452714 0.033728458 0.020292833 -0.052771956 -0.079708651 -0.045073852 -0.10025333 -0.19308814 -0.22782812 -0.25721309][0.50978154 0.53843963 0.51853347 0.47632936 0.3246474 0.1667271 0.088222988 0.011420265 -0.10265715 -0.1334424 -0.18360519 -0.20528425 -0.21398219 -0.28663245 -0.2439754][0.38567021 0.40357777 0.34414318 0.20988572 0.081136368 0.015431911 -0.04073419 -0.0541331 -0.11803859 -0.21605641 -0.21779937 -0.22212166 -0.26546124 -0.38313359 -0.39889461][0.21610022 0.15260315 0.054517455 -0.0074256584 -0.099168032 -0.15480348 -0.17283142 -0.14050741 -0.099276468 -0.098814927 -0.13252313 -0.13077131 -0.22780931 -0.38578039 -0.45870528][0.20965138 0.11982154 0.021855257 -0.058421012 -0.14756116 -0.21861333 -0.24572518 -0.25022534 -0.16126043 -0.0854186 -0.042190213 -0.064842165 -0.10014917 -0.18271607 -0.27587643][0.21945715 0.1471633 0.059851982 0.0020187795 -0.052191697 -0.083489344 -0.10685727 -0.071109891 -0.018594585 0.060468845 0.021081209 -0.012876436 0.068018042 0.021770708 -0.039519895][0.42366764 0.28104395 0.16545954 0.084411912 0.05768574 0.080017425 0.12964097 0.096871965 0.087394573 0.088394873 0.056961991 0.019235291 0.028666429 0.078076385 0.1309866][0.46444711 0.48393312 0.47468629 0.35711512 0.2976585 0.32360396 0.30700606 0.31484976 0.25882298 0.3112109 0.2632536 0.10741638 0.025356553 0.051799394 0.097778566][0.80983108 0.79759145 0.77570879 0.72418207 0.69529533 0.66887319 0.65856951 0.62738234 0.638871 0.55456632 0.38566008 0.36396247 0.2218653 0.15111369 0.26405221]]...]
INFO - root - 2017-12-06 06:43:03.614578: step 29410, loss = 0.87, batch loss = 0.65 (18.2 examples/sec; 0.440 sec/batch; 37h:00m:48s remains)
INFO - root - 2017-12-06 06:43:08.178738: step 29420, loss = 0.80, batch loss = 0.58 (17.4 examples/sec; 0.461 sec/batch; 38h:47m:50s remains)
INFO - root - 2017-12-06 06:43:12.563584: step 29430, loss = 0.75, batch loss = 0.54 (18.4 examples/sec; 0.434 sec/batch; 36h:33m:07s remains)
INFO - root - 2017-12-06 06:43:17.012454: step 29440, loss = 0.90, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 37h:37m:13s remains)
INFO - root - 2017-12-06 06:43:21.244124: step 29450, loss = 0.75, batch loss = 0.53 (18.3 examples/sec; 0.436 sec/batch; 36h:44m:17s remains)
INFO - root - 2017-12-06 06:43:25.668688: step 29460, loss = 0.81, batch loss = 0.60 (17.9 examples/sec; 0.448 sec/batch; 37h:41m:28s remains)
INFO - root - 2017-12-06 06:43:30.030480: step 29470, loss = 0.80, batch loss = 0.59 (18.1 examples/sec; 0.442 sec/batch; 37h:10m:19s remains)
INFO - root - 2017-12-06 06:43:34.467631: step 29480, loss = 0.76, batch loss = 0.55 (18.5 examples/sec; 0.431 sec/batch; 36h:18m:47s remains)
INFO - root - 2017-12-06 06:43:38.926141: step 29490, loss = 0.84, batch loss = 0.62 (18.6 examples/sec; 0.430 sec/batch; 36h:09m:53s remains)
INFO - root - 2017-12-06 06:43:43.329562: step 29500, loss = 0.78, batch loss = 0.57 (17.7 examples/sec; 0.453 sec/batch; 38h:07m:48s remains)
2017-12-06 06:43:43.853969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3921263 -2.5982857 -2.7262387 -2.9285934 -3.0217857 -2.8817494 -2.6384706 -2.3764791 -2.0585127 -1.6338263 -1.0644063 -0.3881337 0.26056328 0.69089437 0.79807532][-1.6267017 -1.8540611 -2.1836243 -2.5329149 -2.8324697 -3.0713642 -3.0596848 -2.8250384 -2.5362806 -2.1899598 -1.6015936 -0.87782168 -0.10508049 0.49018636 0.8362242][-0.53601748 -0.69202864 -1.0752622 -1.5503329 -1.984789 -2.3308964 -2.5512128 -2.6605656 -2.6726451 -2.5305424 -2.2218392 -1.7128286 -1.1212307 -0.57055694 -0.0810099][0.53080189 0.55109954 0.27668151 -0.043296494 -0.42635822 -0.91827154 -1.4529655 -1.7772927 -1.9235519 -2.1009309 -2.2129803 -2.0934079 -1.8464241 -1.4795262 -1.0533437][1.2073476 1.3963794 1.2873876 1.3117903 1.1832337 0.78491092 0.12249929 -0.60196579 -1.1751326 -1.474522 -1.5792005 -1.6700373 -1.7665707 -1.7350599 -1.5300502][1.5386916 1.7439878 1.9023417 2.055846 2.0820906 2.0533845 1.70173 1.1744649 0.56034696 -0.034716934 -0.33282316 -0.49343657 -0.7013396 -1.0022333 -1.1371661][1.8288851 2.005929 2.171145 2.3741167 2.5513399 2.62053 2.4507923 2.2407167 1.8837522 1.6585383 1.5944489 1.4306885 0.99559236 0.51197445 0.19285974][1.5823293 1.8648109 2.1713598 2.3254027 2.4362063 2.5455282 2.5167978 2.4306259 2.2808161 2.3607674 2.6043246 2.7763464 2.5124714 1.9270005 1.33547][0.54542255 0.86334753 1.2672689 1.4977312 1.6882142 1.7849883 1.8640141 1.9590325 2.0855355 2.3144643 2.6417654 2.9651775 3.0768309 2.8495224 2.2932236][-0.94644141 -0.62253451 -0.22847885 0.04482986 0.28667891 0.42593518 0.62863445 0.8397783 1.1148635 1.4266338 1.8117167 2.1684549 2.3696959 2.40434 2.1641624][-2.6337218 -2.4143198 -2.0459957 -1.689368 -1.3342254 -1.0606394 -0.74850869 -0.54772341 -0.29174221 -0.090160295 0.11712633 0.44494072 0.75421381 0.98256111 1.0252993][-3.3484333 -3.6465333 -3.7402949 -3.5251331 -3.1601455 -2.6879482 -2.2086658 -1.8758812 -1.6433582 -1.5682547 -1.468661 -1.3211663 -1.1791825 -0.96349263 -0.76126587][-3.3507783 -3.8223186 -4.1705484 -4.3155613 -4.2077656 -3.8901212 -3.526576 -3.1452744 -2.8639481 -2.7090623 -2.6057656 -2.5794125 -2.4663661 -2.2357788 -1.912621][-2.8759677 -3.3918371 -3.7501543 -3.9544959 -3.9859121 -3.8877923 -3.7214403 -3.5489683 -3.4163687 -3.2232842 -3.0355015 -2.9260302 -2.7556283 -2.5627356 -2.1997297][-2.060523 -2.5769038 -3.0107627 -3.2421565 -3.2960982 -3.1982112 -3.1260507 -3.0395579 -3.0320218 -2.9511688 -2.809454 -2.6376073 -2.3348193 -2.0025866 -1.5870414]]...]
INFO - root - 2017-12-06 06:43:48.273563: step 29510, loss = 0.79, batch loss = 0.58 (18.0 examples/sec; 0.444 sec/batch; 37h:21m:02s remains)
INFO - root - 2017-12-06 06:43:52.655662: step 29520, loss = 0.83, batch loss = 0.61 (17.9 examples/sec; 0.447 sec/batch; 37h:35m:41s remains)
INFO - root - 2017-12-06 06:43:57.122926: step 29530, loss = 0.87, batch loss = 0.66 (18.9 examples/sec; 0.422 sec/batch; 35h:31m:50s remains)
INFO - root - 2017-12-06 06:44:01.336972: step 29540, loss = 0.80, batch loss = 0.58 (18.6 examples/sec; 0.430 sec/batch; 36h:11m:46s remains)
INFO - root - 2017-12-06 06:44:05.706461: step 29550, loss = 0.78, batch loss = 0.57 (18.5 examples/sec; 0.432 sec/batch; 36h:23m:33s remains)
INFO - root - 2017-12-06 06:44:10.282477: step 29560, loss = 0.77, batch loss = 0.56 (17.4 examples/sec; 0.460 sec/batch; 38h:42m:24s remains)
INFO - root - 2017-12-06 06:44:14.787255: step 29570, loss = 0.79, batch loss = 0.58 (17.6 examples/sec; 0.455 sec/batch; 38h:16m:13s remains)
INFO - root - 2017-12-06 06:44:19.226564: step 29580, loss = 0.78, batch loss = 0.57 (17.0 examples/sec; 0.469 sec/batch; 39h:29m:09s remains)
INFO - root - 2017-12-06 06:44:23.746212: step 29590, loss = 0.76, batch loss = 0.55 (17.2 examples/sec; 0.464 sec/batch; 39h:04m:19s remains)
INFO - root - 2017-12-06 06:44:28.152127: step 29600, loss = 0.78, batch loss = 0.56 (18.2 examples/sec; 0.440 sec/batch; 37h:02m:12s remains)
2017-12-06 06:44:28.697691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.099528067 -0.12117597 -0.14965507 -0.19781965 -0.26639205 -0.34244063 -0.41901311 -0.47555086 -0.49847111 -0.48839182 -0.45310166 -0.40383685 -0.34273881 -0.29739469 -0.26320127][-0.098706417 -0.11352696 -0.12997869 -0.16611446 -0.23655105 -0.32267976 -0.43303877 -0.54157537 -0.62037611 -0.67593127 -0.69701284 -0.69219142 -0.65242416 -0.59326273 -0.51254106][-0.11960177 -0.13830331 -0.14341554 -0.15843441 -0.21805605 -0.30800882 -0.45248073 -0.62514746 -0.78162044 -0.92089957 -1.0355018 -1.117889 -1.1224755 -1.0446144 -0.89486706][-0.13442691 -0.16171077 -0.1627543 -0.14806917 -0.16085461 -0.19349378 -0.32577232 -0.54003847 -0.77537042 -0.99576372 -1.1674497 -1.2844898 -1.3312266 -1.2758864 -1.1088041][-0.1342292 -0.16392806 -0.15202782 -0.12101817 -0.09163978 -0.045598552 -0.0870807 -0.21026105 -0.4211638 -0.64901596 -0.868965 -1.0247468 -1.1157058 -1.1121823 -1.0068521][-0.10440074 -0.11207289 -0.07896319 -0.0093135312 0.063045196 0.16440007 0.19968617 0.18192977 0.044494979 -0.14914574 -0.37858737 -0.565474 -0.70295519 -0.77289146 -0.76588553][-0.055275995 -0.021236017 0.06985455 0.2075792 0.35459748 0.54200935 0.6849407 0.78146511 0.77475214 0.658995 0.41578427 0.17351654 -0.067513317 -0.26453295 -0.38481042][-0.03869839 0.026068471 0.15683794 0.34563088 0.55752659 0.80371273 1.0043179 1.1805042 1.250926 1.1368124 0.9012019 0.620278 0.31242061 0.033485971 -0.178877][-0.066532589 -0.020628624 0.076250248 0.23605222 0.45553359 0.70937496 0.935473 1.1092795 1.219698 1.160897 0.97090727 0.72235227 0.4693701 0.20301363 -0.031464167][-0.10106552 -0.10470747 -0.076681226 -0.0063126013 0.13435939 0.34172946 0.55937594 0.761634 0.93951625 0.95856327 0.86928356 0.72948289 0.56647515 0.33788612 0.093331955][-0.11337271 -0.14203519 -0.17278329 -0.21143785 -0.19708297 -0.10056386 0.018431358 0.158128 0.3113907 0.41574946 0.45105794 0.43061587 0.3543773 0.1801227 -0.0064074323][-0.10740673 -0.13080901 -0.17507894 -0.23518324 -0.29184157 -0.31208396 -0.31064004 -0.26204649 -0.15109795 -0.069383308 -0.0019622073 0.041264303 0.028116249 -0.0652671 -0.18955164][-0.097504884 -0.1072971 -0.12874496 -0.17343813 -0.23658317 -0.28656471 -0.34991914 -0.42470655 -0.43374988 -0.44116956 -0.40351185 -0.3548637 -0.31432784 -0.26848939 -0.25377718][-0.094494075 -0.096889384 -0.10465018 -0.13038418 -0.17968106 -0.22856149 -0.27706295 -0.32883415 -0.33986241 -0.39982131 -0.3957431 -0.36317742 -0.33179092 -0.27200016 -0.2271544][-0.091447711 -0.092188984 -0.090898491 -0.096948221 -0.11093844 -0.13757974 -0.15525076 -0.19265258 -0.19654351 -0.22674763 -0.2444503 -0.24877796 -0.21450767 -0.17948619 -0.13906854]]...]
INFO - root - 2017-12-06 06:44:33.123920: step 29610, loss = 0.75, batch loss = 0.53 (17.8 examples/sec; 0.450 sec/batch; 37h:50m:23s remains)
INFO - root - 2017-12-06 06:44:37.609526: step 29620, loss = 0.79, batch loss = 0.58 (16.8 examples/sec; 0.477 sec/batch; 40h:06m:40s remains)
INFO - root - 2017-12-06 06:44:41.838683: step 29630, loss = 0.75, batch loss = 0.54 (16.6 examples/sec; 0.482 sec/batch; 40h:31m:00s remains)
INFO - root - 2017-12-06 06:44:46.289472: step 29640, loss = 0.75, batch loss = 0.53 (18.5 examples/sec; 0.433 sec/batch; 36h:25m:59s remains)
INFO - root - 2017-12-06 06:44:50.798872: step 29650, loss = 0.80, batch loss = 0.59 (17.9 examples/sec; 0.446 sec/batch; 37h:30m:32s remains)
INFO - root - 2017-12-06 06:44:55.297915: step 29660, loss = 0.89, batch loss = 0.68 (18.4 examples/sec; 0.435 sec/batch; 36h:36m:31s remains)
INFO - root - 2017-12-06 06:44:59.707596: step 29670, loss = 0.81, batch loss = 0.60 (17.9 examples/sec; 0.447 sec/batch; 37h:36m:30s remains)
INFO - root - 2017-12-06 06:45:04.150938: step 29680, loss = 0.76, batch loss = 0.55 (18.6 examples/sec; 0.431 sec/batch; 36h:14m:43s remains)
INFO - root - 2017-12-06 06:45:08.597851: step 29690, loss = 0.83, batch loss = 0.62 (17.9 examples/sec; 0.448 sec/batch; 37h:39m:36s remains)
INFO - root - 2017-12-06 06:45:13.084745: step 29700, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.436 sec/batch; 36h:40m:20s remains)
2017-12-06 06:45:13.576587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043287456 -0.042976089 -0.042557418 -0.041711703 -0.040142063 -0.037772942 -0.034962591 -0.033083543 -0.03343688 -0.036305871 -0.040534288 -0.045028608 -0.048184365 -0.049028635 -0.048054531][-0.043431021 -0.042931944 -0.042506024 -0.041616675 -0.040007964 -0.037491277 -0.034680478 -0.033152647 -0.0346221 -0.039122991 -0.045002554 -0.051006347 -0.055128284 -0.056011993 -0.053875394][-0.04353036 -0.042893842 -0.042436842 -0.041508939 -0.039880507 -0.037407756 -0.0347659 -0.033650313 -0.035946216 -0.041463777 -0.048503883 -0.055532292 -0.060336374 -0.061180051 -0.057977967][-0.043499913 -0.042816881 -0.04229147 -0.041305184 -0.039698269 -0.037380472 -0.034976065 -0.034144245 -0.036860332 -0.042647805 -0.049854212 -0.057014577 -0.061952755 -0.062657617 -0.05886076][-0.043412246 -0.042710982 -0.04208174 -0.040988378 -0.039407644 -0.037282195 -0.035134885 -0.03438184 -0.037032872 -0.0424278 -0.048937961 -0.05521138 -0.059480041 -0.059982032 -0.05616124][-0.043394633 -0.042608332 -0.04184255 -0.0405882 -0.03896774 -0.036927193 -0.034919187 -0.034126662 -0.036223225 -0.040647995 -0.045838688 -0.050685335 -0.05382771 -0.053938959 -0.050830502][-0.043490261 -0.0424877 -0.04153436 -0.040014114 -0.038202375 -0.036115784 -0.034113828 -0.033125684 -0.034476709 -0.037706591 -0.041430138 -0.044669013 -0.046646018 -0.046598569 -0.044549648][-0.043570943 -0.0423224 -0.041123487 -0.039220769 -0.0370183 -0.034724645 -0.032557294 -0.031217456 -0.031823635 -0.034074742 -0.036724284 -0.038848225 -0.04007813 -0.0401824 -0.03955999][-0.04359863 -0.042191617 -0.040765058 -0.038517378 -0.035895165 -0.033220213 -0.03078844 -0.029116336 -0.029159352 -0.030758977 -0.032866545 -0.034680203 -0.035988767 -0.036784429 -0.037718803][-0.043576241 -0.042171404 -0.040641055 -0.038179379 -0.035212602 -0.032128815 -0.029374413 -0.027380541 -0.027008735 -0.028275449 -0.030382954 -0.03242873 -0.034369476 -0.0362367 -0.038768195][-0.043680768 -0.042253196 -0.040732887 -0.03820968 -0.03501026 -0.031579997 -0.028452192 -0.026193142 -0.025499858 -0.026625603 -0.02904363 -0.031920973 -0.03506716 -0.038275916 -0.042204618][-0.043683279 -0.042375673 -0.040955938 -0.038551331 -0.03540061 -0.031913374 -0.028663151 -0.026258424 -0.025449537 -0.026559263 -0.029306099 -0.032995015 -0.03726225 -0.041611049 -0.046400204][-0.043705188 -0.042541541 -0.041306045 -0.039185051 -0.036294386 -0.033058833 -0.029980958 -0.027622744 -0.026765652 -0.027826436 -0.030697599 -0.034789458 -0.039552525 -0.044219084 -0.049162008][-0.043737903 -0.042748027 -0.041783471 -0.04007094 -0.037622426 -0.034800071 -0.032056447 -0.029912665 -0.02902915 -0.029853076 -0.032487784 -0.036425062 -0.040869396 -0.044923306 -0.049196698][-0.044058464 -0.043185804 -0.042528648 -0.041279819 -0.039470311 -0.037263844 -0.034941 -0.033023622 -0.032122385 -0.032611944 -0.03467394 -0.037907857 -0.041364592 -0.044450734 -0.047538709]]...]
INFO - root - 2017-12-06 06:45:18.021390: step 29710, loss = 0.85, batch loss = 0.64 (18.0 examples/sec; 0.443 sec/batch; 37h:16m:52s remains)
INFO - root - 2017-12-06 06:45:22.476896: step 29720, loss = 0.88, batch loss = 0.67 (20.2 examples/sec; 0.396 sec/batch; 33h:17m:18s remains)
INFO - root - 2017-12-06 06:45:26.776438: step 29730, loss = 0.84, batch loss = 0.62 (18.1 examples/sec; 0.442 sec/batch; 37h:08m:10s remains)
INFO - root - 2017-12-06 06:45:31.227623: step 29740, loss = 0.76, batch loss = 0.54 (17.5 examples/sec; 0.458 sec/batch; 38h:29m:47s remains)
INFO - root - 2017-12-06 06:45:35.629159: step 29750, loss = 0.84, batch loss = 0.63 (18.4 examples/sec; 0.435 sec/batch; 36h:33m:20s remains)
INFO - root - 2017-12-06 06:45:40.136876: step 29760, loss = 0.83, batch loss = 0.62 (16.4 examples/sec; 0.488 sec/batch; 41h:01m:08s remains)
INFO - root - 2017-12-06 06:45:44.534260: step 29770, loss = 0.83, batch loss = 0.61 (17.8 examples/sec; 0.451 sec/batch; 37h:53m:55s remains)
INFO - root - 2017-12-06 06:45:48.994285: step 29780, loss = 0.81, batch loss = 0.60 (18.4 examples/sec; 0.434 sec/batch; 36h:30m:10s remains)
INFO - root - 2017-12-06 06:45:53.448494: step 29790, loss = 0.83, batch loss = 0.62 (18.3 examples/sec; 0.438 sec/batch; 36h:48m:50s remains)
INFO - root - 2017-12-06 06:45:57.886198: step 29800, loss = 0.70, batch loss = 0.48 (18.1 examples/sec; 0.442 sec/batch; 37h:07m:32s remains)
2017-12-06 06:45:58.452219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.85050941 -1.2553711 -1.4161955 -1.5335958 -1.7596759 -1.8267267 -1.8929901 -1.8077351 -1.6235987 -1.4033355 -1.0968366 -0.89144218 -0.50065714 -0.13987391 0.30816385][-0.28024989 -0.79393959 -1.2122706 -1.6506443 -2.182585 -2.5967031 -2.8241017 -2.9472396 -2.8527853 -2.5334511 -1.8835934 -1.3204069 -0.8680141 -0.65480876 -0.58087027][0.21549252 -0.057752747 -0.316392 -0.80263066 -1.4889064 -2.2028077 -2.8202558 -3.208946 -3.3180757 -3.023901 -2.4169755 -1.8421079 -1.3125651 -1.1478393 -1.103613][0.25576434 0.19739413 0.21726587 0.092863373 -0.090461917 -0.57115787 -1.119581 -1.6731476 -1.9824613 -2.0107834 -1.7412657 -1.3997308 -1.1052053 -0.93506753 -0.91671026][-0.084257461 0.070591561 0.52569741 0.8593384 1.0945848 0.88554233 0.67825478 0.23772365 -0.26463124 -0.63484377 -0.75022888 -0.66422039 -0.57471651 -0.53504729 -0.41350809][-0.12604272 0.11729024 0.66636443 1.1768101 1.9985379 2.4538827 2.6856771 2.2556837 1.7095426 1.1260875 0.51042861 0.13537544 0.0084841922 0.040434204 0.11825559][-0.37330446 0.16243443 0.87765664 1.3236696 1.8528558 2.42887 2.9781511 2.9887292 2.4916136 1.6675973 0.81457162 0.3713083 0.075160675 -0.00017419457 0.12434291][-0.53184712 -0.45687246 0.043656074 0.71054554 1.4336294 1.8592728 2.156059 2.3148837 2.0226626 1.4134684 0.69253874 0.16658559 -0.12054958 -0.1222876 -0.030374601][-0.365439 -0.624899 -0.63592273 -0.46430272 -0.0088735968 0.6208322 0.98883706 1.0280064 0.80819684 0.46934223 0.055274539 -0.20114818 -0.26359877 -0.17379533 -0.097081363][-0.585176 -0.98653448 -1.1650145 -1.2073507 -1.0439925 -0.81772822 -0.64231545 -0.46353742 -0.4345428 -0.47731808 -0.44467476 -0.40760094 -0.25691739 -0.13950828 -0.10823476][-0.56941307 -1.0963296 -1.4186165 -1.5763136 -1.5655354 -1.5488982 -1.4979455 -1.4685711 -1.403854 -1.2099878 -0.904257 -0.57952058 -0.26242217 -0.13951951 -0.087798737][-0.53862447 -0.80210686 -1.0204024 -1.3053429 -1.5144267 -1.639034 -1.6863848 -1.5641336 -1.3463198 -1.1137134 -0.82121772 -0.50712585 -0.28552175 -0.18128017 -0.12097794][-0.43119437 -0.45484841 -0.57913607 -0.81899345 -1.0302533 -1.2016169 -1.2857481 -1.1917131 -1.0368979 -0.7679159 -0.47294515 -0.27656192 -0.192366 -0.18627053 -0.1453066][-0.24266276 -0.21098465 -0.2226992 -0.39509308 -0.61944169 -0.74709755 -0.80820262 -0.76069361 -0.61250669 -0.44491589 -0.26672015 -0.16167755 -0.11964091 -0.14514241 -0.13446751][-0.308642 -0.28583831 -0.30532289 -0.32721853 -0.40285042 -0.46743 -0.48947328 -0.43139356 -0.350676 -0.26174724 -0.1881187 -0.12974906 -0.077728339 -0.083135761 -0.097139478]]...]
INFO - root - 2017-12-06 06:46:02.928146: step 29810, loss = 0.84, batch loss = 0.63 (18.3 examples/sec; 0.437 sec/batch; 36h:46m:40s remains)
INFO - root - 2017-12-06 06:46:07.058612: step 29820, loss = 0.75, batch loss = 0.53 (18.4 examples/sec; 0.436 sec/batch; 36h:37m:32s remains)
INFO - root - 2017-12-06 06:46:11.601599: step 29830, loss = 0.86, batch loss = 0.64 (17.5 examples/sec; 0.456 sec/batch; 38h:22m:12s remains)
INFO - root - 2017-12-06 06:46:16.123012: step 29840, loss = 0.83, batch loss = 0.61 (17.6 examples/sec; 0.456 sec/batch; 38h:17m:57s remains)
INFO - root - 2017-12-06 06:46:20.587383: step 29850, loss = 0.82, batch loss = 0.61 (17.4 examples/sec; 0.459 sec/batch; 38h:34m:01s remains)
INFO - root - 2017-12-06 06:46:25.089186: step 29860, loss = 0.83, batch loss = 0.62 (17.9 examples/sec; 0.446 sec/batch; 37h:31m:03s remains)
INFO - root - 2017-12-06 06:46:29.659038: step 29870, loss = 0.84, batch loss = 0.63 (17.5 examples/sec; 0.456 sec/batch; 38h:19m:42s remains)
INFO - root - 2017-12-06 06:46:34.297982: step 29880, loss = 0.84, batch loss = 0.62 (18.0 examples/sec; 0.444 sec/batch; 37h:18m:17s remains)
INFO - root - 2017-12-06 06:46:40.214827: step 29890, loss = 0.85, batch loss = 0.63 (5.3 examples/sec; 1.521 sec/batch; 127h:51m:59s remains)
INFO - root - 2017-12-06 06:46:46.348272: step 29900, loss = 0.74, batch loss = 0.53 (11.4 examples/sec; 0.700 sec/batch; 58h:48m:21s remains)
2017-12-06 06:46:47.005231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040765911 -0.0422219 -0.043041032 -0.0429536 -0.041894749 -0.038962677 -0.034480315 -0.030175719 -0.026685081 -0.024038523 -0.02257061 -0.022136025 -0.021501936 -0.019716986 -0.01800663][-0.039002836 -0.041241102 -0.04250481 -0.042478424 -0.04113172 -0.037734177 -0.032706838 -0.02728229 -0.022171304 -0.017615966 -0.014431924 -0.012810424 -0.011360854 -0.00951948 -0.008119233][-0.035130505 -0.037727091 -0.039409216 -0.039677974 -0.038378872 -0.035006206 -0.029918801 -0.023878977 -0.017630212 -0.011635982 -0.0073559135 -0.0049717352 -0.0026643202 -0.00081693381 -2.4870038e-05][-0.028866176 -0.031033833 -0.032187566 -0.032306097 -0.031130504 -0.02815076 -0.023374327 -0.017484993 -0.011171669 -0.0050627366 -0.0007501021 0.0017753243 0.0043094978 0.0057318062 0.0054752454][-0.022873998 -0.02301918 -0.022105038 -0.020584524 -0.018912151 -0.016279787 -0.012096331 -0.0072300062 -0.0021832213 0.0023787767 0.0053022727 0.0068315342 0.0083052441 0.0085944533 0.0069757923][-0.01940237 -0.016445652 -0.012465179 -0.0082273558 -0.0048528686 -0.0017505512 0.0022146776 0.0057756752 0.0083517879 0.0099064186 0.01014778 0.00942643 0.0084451512 0.0066347271 0.0032861903][-0.01909519 -0.01314079 -0.0064644665 0.00021666288 0.005710803 0.010134891 0.014427848 0.017133065 0.016819619 0.014825888 0.012035772 0.0083630085 0.0041569918 -0.0001001507 -0.0051955283][-0.022308938 -0.014350064 -0.0058841258 0.0021113977 0.0087645128 0.014070608 0.018546514 0.020817421 0.018543795 0.014266483 0.0093120337 0.0029610023 -0.0042212456 -0.010550581 -0.01677075][-0.028415218 -0.019960448 -0.011124916 -0.0031081215 0.0036250055 0.0090711638 0.012793444 0.013823427 0.010936901 0.0062729716 0.00055222213 -0.0070064515 -0.01546853 -0.022440627 -0.028765749][-0.0356573 -0.028241463 -0.020579606 -0.01364091 -0.0075741485 -0.0027522668 -8.3550811e-05 -0.00019362569 -0.0028544292 -0.0069866106 -0.012532763 -0.019710921 -0.027311467 -0.033329077 -0.038898356][-0.042381544 -0.036778297 -0.031258933 -0.026174903 -0.021515712 -0.017826125 -0.016051002 -0.016445152 -0.018404387 -0.021450453 -0.025946468 -0.031688988 -0.037296362 -0.041818835 -0.046203908][-0.047496043 -0.044020649 -0.040868055 -0.037773773 -0.034836583 -0.032430682 -0.031286258 -0.031510156 -0.032617409 -0.034492426 -0.037415739 -0.041136246 -0.044643652 -0.04758342 -0.050674818][-0.051304087 -0.049483806 -0.048087113 -0.046654996 -0.04516663 -0.043837614 -0.043172963 -0.043186255 -0.04354969 -0.044514645 -0.046073414 -0.048044819 -0.049910605 -0.0515426 -0.053520229][-0.054324973 -0.053640027 -0.053283021 -0.05282405 -0.052205343 -0.051621847 -0.051337514 -0.051300563 -0.05128197 -0.05165131 -0.05227178 -0.05304382 -0.053766184 -0.054478254 -0.055636868][-0.05694079 -0.056742262 -0.056841616 -0.056814432 -0.056684796 -0.056575917 -0.056500655 -0.056461353 -0.056363035 -0.056430519 -0.056600239 -0.056744203 -0.056823172 -0.056969505 -0.057626579]]...]
INFO - root - 2017-12-06 06:46:53.358330: step 29910, loss = 0.83, batch loss = 0.62 (12.6 examples/sec; 0.636 sec/batch; 53h:25m:03s remains)
INFO - root - 2017-12-06 06:46:59.839241: step 29920, loss = 0.80, batch loss = 0.59 (11.7 examples/sec; 0.682 sec/batch; 57h:20m:07s remains)
INFO - root - 2017-12-06 06:47:06.480014: step 29930, loss = 0.91, batch loss = 0.70 (12.0 examples/sec; 0.666 sec/batch; 55h:57m:18s remains)
INFO - root - 2017-12-06 06:47:13.147190: step 29940, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.650 sec/batch; 54h:39m:51s remains)
INFO - root - 2017-12-06 06:47:19.840080: step 29950, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.673 sec/batch; 56h:32m:47s remains)
INFO - root - 2017-12-06 06:47:26.434335: step 29960, loss = 0.76, batch loss = 0.54 (12.3 examples/sec; 0.650 sec/batch; 54h:35m:43s remains)
INFO - root - 2017-12-06 06:47:33.140387: step 29970, loss = 0.78, batch loss = 0.57 (11.8 examples/sec; 0.680 sec/batch; 57h:10m:36s remains)
INFO - root - 2017-12-06 06:47:39.692485: step 29980, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.648 sec/batch; 54h:28m:53s remains)
INFO - root - 2017-12-06 06:47:46.425332: step 29990, loss = 0.81, batch loss = 0.59 (12.0 examples/sec; 0.668 sec/batch; 56h:08m:30s remains)
INFO - root - 2017-12-06 06:47:53.013944: step 30000, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 54h:19m:55s remains)
2017-12-06 06:47:53.673086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12205303 -0.1180068 -0.11021516 -0.099881947 -0.089054108 -0.079342686 -0.071996465 -0.067596905 -0.0656877 -0.065201633 -0.06576249 -0.06659779 -0.067071989 -0.0668491 -0.066327423][-0.12400682 -0.11990941 -0.11198334 -0.10171977 -0.090673305 -0.080127567 -0.071114995 -0.064741343 -0.060832672 -0.058827523 -0.058134176 -0.058324862 -0.058814608 -0.059408791 -0.060002655][-0.12016889 -0.11662146 -0.10974205 -0.10065375 -0.090510994 -0.08000993 -0.070473045 -0.063146591 -0.058081564 -0.05498825 -0.053523656 -0.053312469 -0.053723697 -0.05449827 -0.055394132][-0.11245303 -0.10885052 -0.10302467 -0.095272 -0.086221524 -0.076587051 -0.067384556 -0.060058437 -0.05490547 -0.051734861 -0.050155006 -0.049854942 -0.050353304 -0.051136535 -0.052043602][-0.10151092 -0.097545519 -0.0922826 -0.085722849 -0.077861607 -0.069339983 -0.060976643 -0.054282244 -0.049692363 -0.047101509 -0.046045307 -0.046267763 -0.047161214 -0.048354086 -0.049444921][-0.08838217 -0.084551565 -0.079776146 -0.074057907 -0.067218207 -0.059719738 -0.052561436 -0.04697014 -0.043387037 -0.041803353 -0.041703649 -0.042659696 -0.044014875 -0.045431484 -0.046759274][-0.075405791 -0.071870975 -0.067484245 -0.062134832 -0.055906821 -0.049383078 -0.043606091 -0.039360348 -0.037033703 -0.036502007 -0.037270952 -0.038939081 -0.040851332 -0.042409573 -0.043828506][-0.063977629 -0.060568113 -0.056527525 -0.05138984 -0.045721166 -0.040199261 -0.035594869 -0.032742862 -0.031683326 -0.03229497 -0.033682719 -0.035672545 -0.037784617 -0.039248303 -0.040702365][-0.056090303 -0.052948304 -0.049115874 -0.044266712 -0.039138403 -0.034500632 -0.030940596 -0.028999958 -0.028643493 -0.029750563 -0.031381443 -0.033248127 -0.035128463 -0.036643956 -0.038083371][-0.05133 -0.048467346 -0.044878956 -0.04029585 -0.035576019 -0.031723924 -0.029188525 -0.028181631 -0.028402936 -0.029466152 -0.030878361 -0.032421645 -0.033858541 -0.034808613 -0.036095496][-0.048094157 -0.045576729 -0.042466305 -0.038494725 -0.034459304 -0.031269372 -0.029444698 -0.029161617 -0.029796261 -0.030842289 -0.032048374 -0.033285663 -0.034435697 -0.0351682 -0.036585096][-0.046527188 -0.044624221 -0.042159293 -0.03913011 -0.036162142 -0.033819579 -0.032404091 -0.031848233 -0.032133237 -0.033029791 -0.03406899 -0.035141766 -0.036303435 -0.037918527 -0.040051557][-0.047784116 -0.046567831 -0.045012474 -0.042729795 -0.0404109 -0.038345929 -0.037019759 -0.035960142 -0.035345964 -0.035655409 -0.036286864 -0.037482664 -0.039232809 -0.041766189 -0.044775777][-0.052807733 -0.052130643 -0.05095255 -0.049321227 -0.0475819 -0.045424484 -0.043616444 -0.041715689 -0.040256139 -0.039558638 -0.039395161 -0.040539678 -0.042886972 -0.046434432 -0.050692137][-0.062070116 -0.061714619 -0.060870428 -0.059482217 -0.057644363 -0.054874364 -0.051949814 -0.048543472 -0.045594569 -0.043837819 -0.042560246 -0.043261975 -0.045855511 -0.050779071 -0.05710813]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 06:48:00.943979: step 30010, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 54h:51m:52s remains)
INFO - root - 2017-12-06 06:48:07.486499: step 30020, loss = 0.78, batch loss = 0.57 (12.0 examples/sec; 0.667 sec/batch; 56h:04m:22s remains)
INFO - root - 2017-12-06 06:48:14.131173: step 30030, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 54h:28m:18s remains)
INFO - root - 2017-12-06 06:48:20.741533: step 30040, loss = 0.81, batch loss = 0.59 (11.7 examples/sec; 0.685 sec/batch; 57h:32m:50s remains)
INFO - root - 2017-12-06 06:48:27.364608: step 30050, loss = 0.76, batch loss = 0.55 (11.9 examples/sec; 0.674 sec/batch; 56h:38m:42s remains)
INFO - root - 2017-12-06 06:48:34.060462: step 30060, loss = 0.92, batch loss = 0.71 (11.9 examples/sec; 0.674 sec/batch; 56h:35m:19s remains)
INFO - root - 2017-12-06 06:48:40.811866: step 30070, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.658 sec/batch; 55h:16m:21s remains)
INFO - root - 2017-12-06 06:48:47.321536: step 30080, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.642 sec/batch; 53h:57m:57s remains)
INFO - root - 2017-12-06 06:48:53.999593: step 30090, loss = 0.82, batch loss = 0.61 (11.5 examples/sec; 0.695 sec/batch; 58h:23m:27s remains)
INFO - root - 2017-12-06 06:49:00.514988: step 30100, loss = 0.76, batch loss = 0.55 (12.3 examples/sec; 0.653 sec/batch; 54h:50m:37s remains)
2017-12-06 06:49:01.211206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28091583 -0.64455265 -0.92561364 -1.3484421 -1.7522708 -1.8051001 -1.6246185 -1.2495376 -0.71446615 -0.488556 -0.27169514 -0.13312189 0.20966068 0.65966827 0.7203452][-0.19929737 -0.6704973 -1.0709054 -1.1201099 -1.3144414 -1.5152643 -1.637135 -1.409551 -1.0925704 -0.76292622 -0.38346177 -0.054432012 0.40544143 0.8801716 1.0966823][-0.19842076 -0.59133178 -0.64077049 -0.94727379 -1.3381028 -1.4716791 -1.5153122 -1.4632723 -1.260139 -1.0287848 -0.72550672 -0.3489241 -0.072648823 0.33594576 0.65175009][-0.19937529 -0.33067867 -0.35870007 -0.41572738 -0.69500619 -1.2929965 -1.6640075 -1.6491089 -1.5484961 -1.4730026 -1.2636807 -0.99646962 -0.71903807 -0.49191147 -0.58586854][0.30440706 0.1766443 0.012073912 -0.01406353 -0.21803647 -0.61514932 -0.87669909 -1.1452723 -1.243381 -1.1768229 -1.237833 -1.3703854 -1.3353122 -1.3431627 -1.347317][1.4305283 1.0288717 0.68490225 0.54441607 0.24115863 -0.0086569265 -0.21043831 -0.37611032 -0.41951934 -0.55089152 -0.83962405 -0.95699376 -1.166973 -1.5124824 -1.7435744][2.0303402 1.7430573 1.2977508 0.79945517 0.66028732 0.52026695 0.24557081 0.23626283 0.18114498 0.067062028 -0.12283731 -0.33306274 -0.7610178 -0.98690945 -1.1865983][1.9783487 1.492763 1.24342 1.0559731 0.88224679 0.85169774 0.79016072 0.75403345 0.64833254 0.72498107 0.73723543 0.53065807 0.29725996 0.02225522 -0.44858781][1.2680856 0.97845614 0.87084329 0.6889292 0.57151133 0.58662194 0.62744457 0.72586083 0.70483166 0.83013487 0.86498922 1.0180572 1.0403985 0.91505647 0.78344685][-0.0083693564 -0.1520763 -0.14208846 0.063869588 0.052656196 0.031747967 0.06943547 0.076156043 0.092837833 0.17592049 0.27597177 0.58295262 0.80105376 0.91966844 1.1600609][-1.0853418 -1.0216273 -0.98593044 -0.92199111 -0.806966 -0.76410753 -0.69118184 -0.50997162 -0.36253372 -0.30805874 -0.39441574 -0.1601842 0.31596833 0.62564689 0.74558175][-1.2780489 -1.5385499 -1.6057771 -1.4663427 -1.3717647 -1.331797 -1.1058354 -0.90421277 -0.72744 -0.48424143 -0.30913928 -0.22521016 -0.20330843 0.094664179 0.33967713][-1.1215347 -1.3563945 -1.4264473 -1.4910374 -1.5467488 -1.466386 -1.3196741 -1.0614449 -0.83968842 -0.54383177 -0.37107748 -0.30465469 -0.3117829 -0.27389359 -0.12322089][-0.69940305 -0.85508484 -0.83450282 -0.79497606 -0.61029911 -0.64614516 -0.57010788 -0.37418395 -0.11208232 0.014711559 0.068323873 -0.071092293 -0.20753336 -0.24687019 -0.19994769][-0.42510518 -0.6971333 -0.83328187 -0.78145617 -0.80478358 -0.8057245 -0.72997105 -0.62323654 -0.55745631 -0.47416312 -0.27313572 -0.18964359 -0.1058964 -0.039835423 -0.21077061]]...]
INFO - root - 2017-12-06 06:49:07.830699: step 30110, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.667 sec/batch; 56h:01m:18s remains)
INFO - root - 2017-12-06 06:49:14.380375: step 30120, loss = 0.86, batch loss = 0.65 (12.0 examples/sec; 0.669 sec/batch; 56h:09m:08s remains)
INFO - root - 2017-12-06 06:49:20.957816: step 30130, loss = 0.85, batch loss = 0.64 (12.0 examples/sec; 0.665 sec/batch; 55h:53m:40s remains)
INFO - root - 2017-12-06 06:49:27.543001: step 30140, loss = 0.85, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:34m:44s remains)
INFO - root - 2017-12-06 06:49:34.156100: step 30150, loss = 0.82, batch loss = 0.60 (11.9 examples/sec; 0.672 sec/batch; 56h:25m:22s remains)
INFO - root - 2017-12-06 06:49:40.856049: step 30160, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.656 sec/batch; 55h:07m:11s remains)
INFO - root - 2017-12-06 06:49:47.424731: step 30170, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 55h:09m:45s remains)
INFO - root - 2017-12-06 06:49:54.061329: step 30180, loss = 0.77, batch loss = 0.55 (11.7 examples/sec; 0.686 sec/batch; 57h:35m:01s remains)
INFO - root - 2017-12-06 06:50:00.543644: step 30190, loss = 0.72, batch loss = 0.51 (12.5 examples/sec; 0.639 sec/batch; 53h:41m:08s remains)
INFO - root - 2017-12-06 06:50:07.162051: step 30200, loss = 0.79, batch loss = 0.57 (12.0 examples/sec; 0.665 sec/batch; 55h:51m:56s remains)
2017-12-06 06:50:07.843425: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034811072 -0.62542492 -1.0227896 -1.3259065 -1.7618122 -1.8243591 -1.8122256 -1.3581271 -0.65325862 -0.46100995 -0.16935745 0.18309781 0.37391943 0.49798989 0.44598985][0.70426285 0.55117625 0.16618016 -0.20322344 -0.6794613 -1.0175952 -1.3194445 -1.0950123 -0.64672816 -0.28951246 0.076805227 0.46810871 0.90971494 1.1307768 1.1569694][1.3758754 1.0734549 0.79828036 0.43315643 0.00010223687 -0.35146335 -0.62873113 -0.69014353 -0.59032744 -0.1733561 0.30186468 0.44182575 0.6593923 0.95057571 1.0087572][1.8691254 1.5146807 1.1113863 0.82950878 0.55704051 0.16442439 -0.24854878 -0.35364223 -0.29246911 -0.28031325 -0.21768647 -0.025800519 0.11196844 -0.023219436 -0.36632594][1.2835456 1.0235909 0.73738676 0.39358491 0.2475071 -0.067167714 -0.38511941 -0.576891 -0.54098845 -0.34987175 -0.39926246 -0.55422133 -0.64389265 -0.67230928 -0.66213059][1.7598279 1.3169446 0.79161817 0.16816336 -0.37409261 -0.72778052 -0.97381127 -1.1238103 -1.0528923 -1.0751747 -1.1869717 -1.2933588 -1.1934776 -1.2616502 -1.4102278][1.9924986 1.435051 0.75287616 0.057634316 -0.51607317 -0.97747654 -1.3426509 -1.3542825 -1.4565591 -1.611313 -1.6202307 -1.4677598 -1.3561677 -1.2677495 -1.1746305][1.6523182 1.4481144 0.77127939 -0.059440844 -0.58427823 -1.1942868 -1.7254829 -1.9209936 -1.8732356 -1.7875683 -1.765341 -1.7038532 -1.6718953 -1.4848747 -1.1931387][1.6568334 1.2300469 0.5098747 -0.27059734 -0.61996514 -1.0134417 -1.4362571 -1.7329185 -1.8061057 -1.8386518 -1.7326921 -1.3809811 -1.2335694 -1.2858694 -1.0902708][0.75394005 0.53800833 0.11686748 -0.26806447 -0.778661 -1.2846599 -1.6226763 -1.7399684 -1.583832 -1.1690927 -0.90602624 -0.75449878 -0.57761025 -0.52518034 -0.41629347][-0.29318717 -0.61948562 -0.91640478 -1.2340512 -1.6044357 -1.8252248 -2.03138 -2.3031826 -2.2293277 -1.8368213 -1.3709038 -0.89735311 -0.35773712 0.0068935081 -0.093608111][-1.1349549 -1.4333116 -1.557106 -1.4859189 -1.5446947 -1.7079695 -1.8843683 -1.9123009 -1.9666922 -2.1442721 -1.9465194 -1.5027524 -1.0957088 -0.61245626 -0.19568481][-1.3138471 -1.2992997 -0.88898432 -0.99274391 -1.0547065 -0.77204704 -0.80478841 -1.1030308 -1.162302 -0.90226263 -0.89078838 -0.78096193 -0.43966362 -0.16948345 0.12958598][-0.9788956 -0.68446773 -0.38466635 -0.12419379 0.031070821 0.048141576 0.095399283 0.24868116 0.14389539 0.058825873 0.0653026 0.22609603 0.1898866 0.26685536 0.47110283][-0.24786189 -0.23944545 0.073852174 0.3774974 0.48904854 0.58593863 0.50799859 0.41005653 0.38616765 0.38908359 0.095303304 0.026183657 0.046061508 -0.10716426 -0.29366335]]...]
INFO - root - 2017-12-06 06:50:14.494354: step 30210, loss = 0.87, batch loss = 0.66 (12.0 examples/sec; 0.664 sec/batch; 55h:45m:18s remains)
INFO - root - 2017-12-06 06:50:21.125314: step 30220, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.666 sec/batch; 55h:53m:06s remains)
INFO - root - 2017-12-06 06:50:27.199676: step 30230, loss = 0.82, batch loss = 0.61 (18.7 examples/sec; 0.428 sec/batch; 35h:55m:12s remains)
INFO - root - 2017-12-06 06:50:31.731358: step 30240, loss = 0.88, batch loss = 0.66 (17.9 examples/sec; 0.447 sec/batch; 37h:29m:42s remains)
INFO - root - 2017-12-06 06:50:36.262804: step 30250, loss = 0.76, batch loss = 0.54 (17.7 examples/sec; 0.452 sec/batch; 37h:58m:52s remains)
INFO - root - 2017-12-06 06:50:40.614434: step 30260, loss = 0.85, batch loss = 0.64 (19.3 examples/sec; 0.414 sec/batch; 34h:45m:27s remains)
INFO - root - 2017-12-06 06:50:45.123003: step 30270, loss = 0.80, batch loss = 0.58 (18.3 examples/sec; 0.437 sec/batch; 36h:41m:01s remains)
INFO - root - 2017-12-06 06:50:49.519442: step 30280, loss = 0.78, batch loss = 0.57 (17.4 examples/sec; 0.460 sec/batch; 38h:36m:33s remains)
INFO - root - 2017-12-06 06:50:53.737795: step 30290, loss = 0.83, batch loss = 0.62 (16.3 examples/sec; 0.492 sec/batch; 41h:16m:32s remains)
INFO - root - 2017-12-06 06:50:58.260827: step 30300, loss = 0.87, batch loss = 0.65 (17.6 examples/sec; 0.454 sec/batch; 38h:04m:41s remains)
2017-12-06 06:50:58.754712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.37857062 -0.36944014 -0.39921385 -0.39824224 -0.41565973 -0.49644768 -0.48510015 -0.37925059 -0.22092207 -0.043965183 0.19164847 0.24908428 0.17559706 0.022044212 -0.16014411][-0.89300388 -0.90915728 -0.98624951 -1.1823223 -1.3315282 -1.3842781 -1.3760417 -1.3700624 -1.2322396 -0.99456149 -0.6363613 -0.37062454 -0.14863671 -0.045183863 -0.019914955][-1.248225 -1.3707213 -1.5293868 -1.6797327 -1.8201039 -2.1081095 -2.3102353 -2.3153281 -2.1272593 -1.9022928 -1.6050472 -1.2513498 -0.84437984 -0.55340356 -0.31631356][-1.2153736 -1.535338 -1.7277579 -1.8978071 -2.0522237 -2.2432353 -2.4983501 -2.6925895 -2.660712 -2.4803369 -2.2311544 -1.9191793 -1.491731 -1.1047094 -0.7541315][-1.1361377 -1.2202415 -1.2084335 -1.2982016 -1.2826908 -1.3542663 -1.4044789 -1.6120077 -1.7805083 -1.9142572 -1.9980607 -1.9494535 -1.7054162 -1.4550641 -1.2625706][-0.45929974 -0.56268042 -0.37471175 -0.13067821 0.12872559 0.29571831 0.40751636 0.22816144 -0.11098619 -0.49451941 -0.8532322 -1.1755235 -1.351601 -1.363829 -1.2793471][0.021907926 -0.014749795 0.24060701 0.61470377 1.0612544 1.4898615 1.8623978 1.9390556 1.6828145 1.1245284 0.45846725 -0.19944437 -0.71638781 -1.0338799 -1.1899085][0.544272 0.66734046 0.7867617 1.0705627 1.4113369 1.71851 2.1290109 2.4146266 2.3207467 1.8467293 1.2134572 0.29184276 -0.42432028 -0.82624596 -0.95982152][0.15376416 0.3766163 0.53563493 0.69481075 0.89067137 1.1377219 1.5066943 1.8898017 2.1432877 2.0555232 1.6348217 0.9000054 0.25944251 -0.28722659 -0.59656781][-0.57393837 -0.66443253 -0.58030063 -0.40769827 -0.24747095 -0.055387143 0.1743155 0.46615565 0.79813355 1.003482 1.0932673 0.79708606 0.46676522 0.14554508 -0.15985835][-1.0157703 -1.1778717 -1.2264529 -1.2355714 -1.1231015 -1.0371351 -0.84307587 -0.6384663 -0.33657461 0.014776655 0.33736515 0.41757971 0.43534404 0.28430241 0.182786][-1.5835084 -1.8106276 -1.7632246 -1.7977935 -1.6850562 -1.6455797 -1.4211969 -1.2343616 -0.865144 -0.49689996 -0.14977531 0.16563828 0.47769934 0.56198424 0.61506903][-1.5048573 -1.727453 -1.7519215 -1.9201757 -1.9442226 -2.02165 -1.8550261 -1.614043 -1.2610178 -0.97864962 -0.61956269 -0.23800606 0.08895807 0.24918495 0.29936111][-1.0345588 -1.300321 -1.3836915 -1.4564174 -1.4550009 -1.5451992 -1.5052822 -1.3922486 -1.0729605 -0.75982213 -0.41813827 -0.08119151 0.24803226 0.55115122 0.61336482][-0.23421025 -0.4994604 -0.72079843 -0.9134897 -1.0584378 -1.099859 -1.0306991 -1.126338 -1.1549877 -0.92714512 -0.55103958 -0.21332073 0.099516779 0.39713866 0.57266134]]...]
INFO - root - 2017-12-06 06:51:03.281674: step 30310, loss = 0.77, batch loss = 0.55 (17.4 examples/sec; 0.459 sec/batch; 38h:32m:34s remains)
INFO - root - 2017-12-06 06:51:07.710994: step 30320, loss = 0.75, batch loss = 0.53 (16.5 examples/sec; 0.484 sec/batch; 40h:37m:11s remains)
INFO - root - 2017-12-06 06:51:12.198022: step 30330, loss = 0.86, batch loss = 0.65 (18.1 examples/sec; 0.441 sec/batch; 37h:00m:01s remains)
INFO - root - 2017-12-06 06:51:16.654691: step 30340, loss = 0.90, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 36h:44m:47s remains)
INFO - root - 2017-12-06 06:51:21.117530: step 30350, loss = 0.90, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 36h:32m:43s remains)
INFO - root - 2017-12-06 06:51:25.614719: step 30360, loss = 0.76, batch loss = 0.55 (18.2 examples/sec; 0.440 sec/batch; 36h:57m:49s remains)
INFO - root - 2017-12-06 06:51:30.104009: step 30370, loss = 0.78, batch loss = 0.57 (17.7 examples/sec; 0.452 sec/batch; 37h:56m:17s remains)
INFO - root - 2017-12-06 06:51:34.289893: step 30380, loss = 0.83, batch loss = 0.61 (16.0 examples/sec; 0.500 sec/batch; 41h:57m:52s remains)
INFO - root - 2017-12-06 06:51:38.768424: step 30390, loss = 0.77, batch loss = 0.56 (18.3 examples/sec; 0.437 sec/batch; 36h:38m:20s remains)
INFO - root - 2017-12-06 06:51:43.244228: step 30400, loss = 0.81, batch loss = 0.60 (17.5 examples/sec; 0.457 sec/batch; 38h:19m:06s remains)
2017-12-06 06:51:43.850568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062898785 -0.062749766 -0.063075013 -0.062934481 -0.063017815 -0.062933475 -0.062796444 -0.062810674 -0.062771469 -0.062720135 -0.062796369 -0.063104689 -0.063595176 -0.064241931 -0.065094054][-0.062401984 -0.062013227 -0.062111929 -0.061890274 -0.061735626 -0.061756156 -0.061779886 -0.061886892 -0.061989497 -0.062196054 -0.062579967 -0.062727146 -0.062965706 -0.063452527 -0.064031959][-0.06183083 -0.060868256 -0.060751297 -0.060376741 -0.060298756 -0.060295977 -0.060343929 -0.060963109 -0.061645098 -0.062533423 -0.063740261 -0.064608037 -0.065383255 -0.066077791 -0.066699624][-0.061204996 -0.060093813 -0.059708059 -0.058861133 -0.058507949 -0.058749136 -0.0594498 -0.060566336 -0.061937895 -0.063956738 -0.066082574 -0.068086237 -0.069779627 -0.071448326 -0.072604969][-0.061368637 -0.059979662 -0.059180193 -0.057774473 -0.056910109 -0.057091787 -0.058190949 -0.060131058 -0.062574908 -0.065971836 -0.0696834 -0.073161379 -0.076320246 -0.079503864 -0.081360213][-0.061385736 -0.060331449 -0.05970896 -0.057779014 -0.056071192 -0.0557859 -0.056865394 -0.059378 -0.062866174 -0.067866735 -0.073557124 -0.079628378 -0.084977463 -0.089780778 -0.093377464][-0.060487617 -0.059710521 -0.059438273 -0.058033012 -0.056071412 -0.055117898 -0.055948041 -0.05877284 -0.063205749 -0.069907226 -0.077580445 -0.0861639 -0.094383195 -0.10166071 -0.10718922][-0.060146961 -0.059345994 -0.059213202 -0.058212385 -0.056982413 -0.05576374 -0.055640038 -0.058288895 -0.063327655 -0.071013488 -0.080583781 -0.091422722 -0.10203516 -0.11152698 -0.11957791][-0.059880905 -0.058809292 -0.058373053 -0.057592224 -0.056969266 -0.05647992 -0.056800239 -0.058773253 -0.063861541 -0.072086141 -0.0826151 -0.095008254 -0.10732913 -0.11859383 -0.12883545][-0.059768379 -0.05839844 -0.057720467 -0.056836076 -0.056376319 -0.056481969 -0.057677213 -0.060293827 -0.06491825 -0.073026612 -0.084001124 -0.097106315 -0.11035636 -0.12256536 -0.13473921][-0.060583547 -0.058943357 -0.057819266 -0.056831487 -0.056057595 -0.056090117 -0.057755064 -0.061149318 -0.067036226 -0.075273857 -0.086279988 -0.099373259 -0.1128828 -0.12605672 -0.13867715][-0.061182626 -0.059552703 -0.058376741 -0.057566345 -0.056597751 -0.056160022 -0.057871193 -0.062016234 -0.0686304 -0.078212231 -0.090290092 -0.10365354 -0.11696208 -0.12991053 -0.14234269][-0.061881568 -0.060478047 -0.059433416 -0.058635507 -0.057750251 -0.056833968 -0.05793871 -0.062391549 -0.07030496 -0.081408158 -0.095408231 -0.11005621 -0.12356693 -0.13568126 -0.14649397][-0.062583767 -0.061367683 -0.060517892 -0.059818245 -0.059075043 -0.058247961 -0.059068404 -0.063546181 -0.072420634 -0.085469477 -0.1014652 -0.11769767 -0.13215962 -0.14384195 -0.1519208][-0.063990049 -0.062708408 -0.061984703 -0.061295372 -0.060395375 -0.059608392 -0.060838386 -0.065775409 -0.075149126 -0.089277372 -0.10701223 -0.12461902 -0.13914117 -0.15000701 -0.15606296]]...]
INFO - root - 2017-12-06 06:51:48.392903: step 30410, loss = 0.81, batch loss = 0.59 (17.6 examples/sec; 0.454 sec/batch; 38h:05m:43s remains)
INFO - root - 2017-12-06 06:51:53.123559: step 30420, loss = 0.79, batch loss = 0.58 (18.2 examples/sec; 0.438 sec/batch; 36h:47m:27s remains)
INFO - root - 2017-12-06 06:51:57.949812: step 30430, loss = 0.91, batch loss = 0.70 (12.9 examples/sec; 0.619 sec/batch; 51h:58m:33s remains)
INFO - root - 2017-12-06 06:52:04.456611: step 30440, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.658 sec/batch; 55h:10m:54s remains)
INFO - root - 2017-12-06 06:52:10.837588: step 30450, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.649 sec/batch; 54h:27m:22s remains)
INFO - root - 2017-12-06 06:52:17.246916: step 30460, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 54h:10m:10s remains)
INFO - root - 2017-12-06 06:52:23.635182: step 30470, loss = 0.83, batch loss = 0.62 (11.6 examples/sec; 0.689 sec/batch; 57h:48m:29s remains)
INFO - root - 2017-12-06 06:52:30.083714: step 30480, loss = 0.67, batch loss = 0.45 (12.2 examples/sec; 0.654 sec/batch; 54h:50m:42s remains)
INFO - root - 2017-12-06 06:52:36.450876: step 30490, loss = 0.80, batch loss = 0.59 (13.0 examples/sec; 0.614 sec/batch; 51h:30m:44s remains)
INFO - root - 2017-12-06 06:52:42.949782: step 30500, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.670 sec/batch; 56h:11m:10s remains)
2017-12-06 06:52:43.626321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10811726 -0.11683902 -0.13524631 -0.16664322 -0.20658869 -0.24687511 -0.27630481 -0.29155222 -0.28936183 -0.2645759 -0.22067215 -0.17598328 -0.13726653 -0.11450079 -0.10821711][-0.12331068 -0.1501087 -0.19580182 -0.26002604 -0.34495175 -0.45400292 -0.56541908 -0.65874839 -0.71195179 -0.68533593 -0.57607466 -0.43222135 -0.27867657 -0.16412525 -0.12321626][-0.1357805 -0.17451036 -0.22984073 -0.29346317 -0.35091484 -0.43471658 -0.54372168 -0.69635314 -0.86426181 -0.96026671 -0.91609925 -0.75437146 -0.51416332 -0.30329746 -0.18837088][-0.15354028 -0.19825509 -0.24011439 -0.23682088 -0.1648061 -0.092077278 -0.044835389 -0.13877037 -0.37484819 -0.60251522 -0.74108708 -0.75704551 -0.6223489 -0.40161949 -0.2398265][-0.16774 -0.2018526 -0.19197366 -0.073747411 0.20134746 0.50249988 0.766092 0.82217753 0.62856543 0.3089152 -0.067387857 -0.32361895 -0.39252168 -0.3238607 -0.20986208][-0.16906089 -0.18828718 -0.12356189 0.11576654 0.57283837 1.1226698 1.6299906 1.8341244 1.6451131 1.2433431 0.71739876 0.24163242 -0.050567586 -0.14484525 -0.12545304][-0.17564091 -0.19096525 -0.11250111 0.18132026 0.72652948 1.3894198 2.0395765 2.3734982 2.2596114 1.8220177 1.2257286 0.66115355 0.25173938 0.057794377 -0.010823704][-0.19218743 -0.23239781 -0.18252632 0.071206957 0.55701828 1.2285637 1.8671561 2.2173026 2.1867831 1.785027 1.2334806 0.71063179 0.3152805 0.10177688 0.015123866][-0.21327293 -0.29231817 -0.31097862 -0.16053173 0.20212199 0.75646168 1.3220515 1.643114 1.6020843 1.2881895 0.84398574 0.4162057 0.12219673 -0.006726265 -0.024197914][-0.22109316 -0.33325177 -0.43095809 -0.40365672 -0.2055642 0.16276242 0.59264648 0.87112057 0.87262315 0.65237808 0.32174534 0.063816652 -0.069708019 -0.10080785 -0.089999624][-0.20651555 -0.33025885 -0.48198414 -0.57851684 -0.55375695 -0.37928927 -0.11898737 0.093540087 0.14316577 0.065603584 -0.11086457 -0.21480426 -0.20947424 -0.16177332 -0.1204757][-0.1801545 -0.28227547 -0.43623739 -0.60863984 -0.72548813 -0.72578263 -0.62735873 -0.52043116 -0.45936948 -0.39849842 -0.39031112 -0.36596739 -0.27911782 -0.18080729 -0.1335884][-0.14372012 -0.20886067 -0.31946909 -0.46133095 -0.61217231 -0.71307 -0.72983009 -0.69248021 -0.65569717 -0.56064677 -0.45137137 -0.34571207 -0.23188567 -0.16479218 -0.13421449][-0.12297137 -0.14684832 -0.18886027 -0.26335812 -0.36536795 -0.44292206 -0.48199463 -0.48168039 -0.47748411 -0.41326338 -0.33171245 -0.24837114 -0.1777005 -0.1525059 -0.1296709][-0.11551677 -0.11921725 -0.13444984 -0.14565124 -0.17065275 -0.20170683 -0.22775954 -0.23604567 -0.24652293 -0.23005226 -0.21533622 -0.16796269 -0.13177721 -0.12867728 -0.11756496]]...]
INFO - root - 2017-12-06 06:52:50.169295: step 30510, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.657 sec/batch; 55h:07m:36s remains)
INFO - root - 2017-12-06 06:52:56.701322: step 30520, loss = 0.86, batch loss = 0.65 (12.1 examples/sec; 0.659 sec/batch; 55h:17m:59s remains)
INFO - root - 2017-12-06 06:53:03.137598: step 30530, loss = 0.85, batch loss = 0.64 (11.9 examples/sec; 0.674 sec/batch; 56h:34m:20s remains)
INFO - root - 2017-12-06 06:53:09.649253: step 30540, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.648 sec/batch; 54h:23m:15s remains)
INFO - root - 2017-12-06 06:53:16.132724: step 30550, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.640 sec/batch; 53h:42m:29s remains)
INFO - root - 2017-12-06 06:53:22.453688: step 30560, loss = 0.84, batch loss = 0.63 (12.8 examples/sec; 0.623 sec/batch; 52h:14m:58s remains)
INFO - root - 2017-12-06 06:53:28.811521: step 30570, loss = 0.70, batch loss = 0.49 (12.5 examples/sec; 0.638 sec/batch; 53h:29m:56s remains)
INFO - root - 2017-12-06 06:53:35.154708: step 30580, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.637 sec/batch; 53h:26m:13s remains)
INFO - root - 2017-12-06 06:53:41.638986: step 30590, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.655 sec/batch; 54h:54m:36s remains)
INFO - root - 2017-12-06 06:53:48.120669: step 30600, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.649 sec/batch; 54h:25m:25s remains)
2017-12-06 06:53:48.867140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10239802 -0.1068622 -0.11963631 -0.14428946 -0.18258193 -0.23123103 -0.28143185 -0.32275718 -0.3442862 -0.33291411 -0.2868326 -0.22548723 -0.16573599 -0.12457977 -0.10932277][-0.10352805 -0.11214186 -0.13488169 -0.17046726 -0.21235307 -0.25833377 -0.31269655 -0.381642 -0.45082194 -0.48130959 -0.44170222 -0.35147667 -0.24392053 -0.16484691 -0.12540255][-0.10571305 -0.11822995 -0.14162715 -0.1616829 -0.16396037 -0.14452425 -0.13500226 -0.19495064 -0.31671345 -0.43711793 -0.48076332 -0.4206441 -0.30516243 -0.2030406 -0.13862005][-0.1079944 -0.12375069 -0.14536299 -0.13633744 -0.066576406 0.0664439 0.1899305 0.18281436 0.027743071 -0.20740053 -0.38842389 -0.41287395 -0.3291693 -0.22488159 -0.14642304][-0.10864213 -0.12241419 -0.13875191 -0.10021865 0.05727049 0.35740405 0.6660794 0.775418 0.62187648 0.27183798 -0.088704415 -0.25602138 -0.26989648 -0.21085012 -0.13608949][-0.10817699 -0.12555724 -0.14711612 -0.09874893 0.10842907 0.52830654 0.9910599 1.2325803 1.1359729 0.71945834 0.2306307 -0.062972084 -0.18501551 -0.18011147 -0.1175794][-0.10679067 -0.1302142 -0.17411515 -0.17643553 -0.019999854 0.37933505 0.8476336 1.1422381 1.1206614 0.75470978 0.28929433 0.0014967173 -0.14436984 -0.15855953 -0.10603578][-0.10650043 -0.13150136 -0.18972994 -0.24327049 -0.16365479 0.15098611 0.55252653 0.84777987 0.88857752 0.61632663 0.24740481 0.015729442 -0.12122371 -0.14425443 -0.10618063][-0.10629722 -0.12700541 -0.18518698 -0.26586989 -0.26907176 -0.087807171 0.17287964 0.38963154 0.46021569 0.3131046 0.10587186 -0.02343782 -0.11113781 -0.12998816 -0.10441677][-0.10481262 -0.11890657 -0.16683389 -0.25814074 -0.31135261 -0.24940366 -0.14131929 -0.039343484 0.013532221 -0.035775524 -0.090480596 -0.10809987 -0.12590797 -0.12529528 -0.10703421][-0.10340261 -0.10930719 -0.13873722 -0.21092576 -0.2767351 -0.27525088 -0.23500234 -0.1931465 -0.15686972 -0.15166743 -0.13749763 -0.11987902 -0.11466485 -0.11182019 -0.10143147][-0.10290933 -0.10516708 -0.12128083 -0.17015353 -0.22360295 -0.2527537 -0.25808772 -0.24858087 -0.22508958 -0.20108551 -0.16637316 -0.14114645 -0.12405974 -0.11243099 -0.10069595][-0.10271629 -0.10297071 -0.10940164 -0.13223094 -0.16226748 -0.18576506 -0.19952354 -0.19955787 -0.18674274 -0.17002837 -0.14866319 -0.13244912 -0.11875039 -0.10774739 -0.098847196][-0.10282636 -0.10134211 -0.10279828 -0.11142818 -0.11734719 -0.12098079 -0.12627831 -0.1320681 -0.12886919 -0.11862288 -0.11339661 -0.1053186 -0.10620619 -0.10139546 -0.097217888][-0.10290118 -0.10277714 -0.10161147 -0.10320119 -0.10792878 -0.10852836 -0.11252787 -0.11764451 -0.12144574 -0.12118438 -0.11835478 -0.10456763 -0.10820384 -0.10294405 -0.099376544]]...]
INFO - root - 2017-12-06 06:53:55.239246: step 30610, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.640 sec/batch; 53h:41m:04s remains)
INFO - root - 2017-12-06 06:54:01.842968: step 30620, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.645 sec/batch; 54h:05m:51s remains)
INFO - root - 2017-12-06 06:54:08.325199: step 30630, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.638 sec/batch; 53h:31m:25s remains)
INFO - root - 2017-12-06 06:54:14.828680: step 30640, loss = 0.87, batch loss = 0.66 (11.9 examples/sec; 0.673 sec/batch; 56h:27m:00s remains)
INFO - root - 2017-12-06 06:54:21.261012: step 30650, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.647 sec/batch; 54h:13m:33s remains)
INFO - root - 2017-12-06 06:54:27.686175: step 30660, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.666 sec/batch; 55h:48m:24s remains)
INFO - root - 2017-12-06 06:54:34.165618: step 30670, loss = 0.83, batch loss = 0.61 (11.9 examples/sec; 0.674 sec/batch; 56h:31m:41s remains)
INFO - root - 2017-12-06 06:54:40.617005: step 30680, loss = 0.81, batch loss = 0.59 (12.1 examples/sec; 0.664 sec/batch; 55h:37m:52s remains)
INFO - root - 2017-12-06 06:54:47.135750: step 30690, loss = 0.75, batch loss = 0.53 (12.3 examples/sec; 0.649 sec/batch; 54h:26m:13s remains)
INFO - root - 2017-12-06 06:54:53.656911: step 30700, loss = 0.89, batch loss = 0.67 (12.3 examples/sec; 0.653 sec/batch; 54h:43m:39s remains)
2017-12-06 06:54:54.462571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0698682 -0.069500007 -0.0693058 -0.069105357 -0.068921715 -0.068737477 -0.068518795 -0.068220727 -0.067893982 -0.067588404 -0.067362383 -0.067286335 -0.067379445 -0.067635238 -0.068023704][-0.069550544 -0.068956517 -0.068630986 -0.068336606 -0.068141028 -0.068024017 -0.067868143 -0.067577519 -0.067186095 -0.066752508 -0.066352561 -0.066106439 -0.066095546 -0.066357434 -0.066849694][-0.06905821 -0.068179429 -0.067707196 -0.067353427 -0.067211553 -0.067249186 -0.067283988 -0.067151681 -0.066799715 -0.066275418 -0.065651804 -0.065150112 -0.064937159 -0.065096669 -0.065614045][-0.068590418 -0.067510881 -0.0669313 -0.066550985 -0.066503972 -0.066736 -0.067024864 -0.067099676 -0.066845536 -0.066278681 -0.065491289 -0.064765766 -0.064336434 -0.064348832 -0.064795159][-0.06816531 -0.067093767 -0.066493437 -0.066145174 -0.066210918 -0.0666285 -0.067069292 -0.067253955 -0.067045584 -0.066462331 -0.065618679 -0.064777367 -0.064171553 -0.064024225 -0.064371213][-0.067894809 -0.066825226 -0.0662588 -0.065955728 -0.066083476 -0.066570818 -0.066942729 -0.067075178 -0.066928059 -0.06644088 -0.065680504 -0.064882755 -0.064242966 -0.063988268 -0.064244971][-0.067663468 -0.066555135 -0.065994889 -0.065721065 -0.0658376 -0.066204809 -0.066396579 -0.066414677 -0.066354781 -0.066106275 -0.065588631 -0.064975888 -0.064441733 -0.064176366 -0.064383507][-0.067514084 -0.066337764 -0.065728188 -0.065432 -0.065473028 -0.065708987 -0.0658388 -0.065831088 -0.065903567 -0.065914772 -0.065711707 -0.065336019 -0.064961694 -0.064795911 -0.065001041][-0.067632966 -0.066461109 -0.065851711 -0.065511391 -0.065559529 -0.065798722 -0.06593401 -0.065949023 -0.065988585 -0.066016361 -0.065943845 -0.0657412 -0.065567508 -0.065556258 -0.065786891][-0.0680147 -0.066927716 -0.066367373 -0.066062972 -0.0661376 -0.066395178 -0.066464804 -0.066458195 -0.066464052 -0.066435374 -0.066411778 -0.0663392 -0.06626045 -0.066282667 -0.066536359][-0.068429843 -0.067442484 -0.066974558 -0.066735029 -0.066795245 -0.067027345 -0.067156956 -0.067171231 -0.0671873 -0.067213736 -0.067209132 -0.067149207 -0.067065544 -0.0670516 -0.067294925][-0.068802267 -0.067901492 -0.06757535 -0.067424864 -0.06747257 -0.067721359 -0.067949854 -0.068076327 -0.068117946 -0.068119004 -0.068103552 -0.068004727 -0.06788867 -0.067857563 -0.068085521][-0.069308318 -0.068566442 -0.068374343 -0.068298832 -0.068362758 -0.068616137 -0.068864323 -0.069035649 -0.069108054 -0.069100067 -0.069042265 -0.068869255 -0.068688639 -0.068639025 -0.068798348][-0.070015036 -0.069443129 -0.069369435 -0.069371752 -0.069476172 -0.069683805 -0.069884047 -0.070040077 -0.0701288 -0.070125014 -0.070012063 -0.069774061 -0.069511451 -0.06935671 -0.069423437][-0.070545726 -0.070128649 -0.070136316 -0.070200831 -0.070317149 -0.070479438 -0.0706308 -0.070745185 -0.070804894 -0.07076674 -0.070611626 -0.070353612 -0.070082694 -0.069865227 -0.069822639]]...]
INFO - root - 2017-12-06 06:55:00.940740: step 30710, loss = 0.85, batch loss = 0.64 (12.6 examples/sec; 0.634 sec/batch; 53h:09m:29s remains)
INFO - root - 2017-12-06 06:55:07.289654: step 30720, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.643 sec/batch; 53h:54m:29s remains)
INFO - root - 2017-12-06 06:55:13.756218: step 30730, loss = 0.77, batch loss = 0.56 (12.6 examples/sec; 0.637 sec/batch; 53h:25m:58s remains)
INFO - root - 2017-12-06 06:55:20.235359: step 30740, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.632 sec/batch; 52h:59m:09s remains)
INFO - root - 2017-12-06 06:55:26.613730: step 30750, loss = 0.82, batch loss = 0.61 (13.1 examples/sec; 0.612 sec/batch; 51h:18m:35s remains)
INFO - root - 2017-12-06 06:55:33.075314: step 30760, loss = 0.86, batch loss = 0.64 (11.7 examples/sec; 0.682 sec/batch; 57h:07m:34s remains)
INFO - root - 2017-12-06 06:55:39.605936: step 30770, loss = 0.88, batch loss = 0.66 (12.2 examples/sec; 0.654 sec/batch; 54h:48m:56s remains)
INFO - root - 2017-12-06 06:55:46.155925: step 30780, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.651 sec/batch; 54h:35m:19s remains)
INFO - root - 2017-12-06 06:55:52.630657: step 30790, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.674 sec/batch; 56h:31m:17s remains)
INFO - root - 2017-12-06 06:55:59.066805: step 30800, loss = 0.84, batch loss = 0.63 (12.8 examples/sec; 0.626 sec/batch; 52h:28m:15s remains)
2017-12-06 06:55:59.680674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11399051 -0.0885096 -0.056084227 -0.035158925 -0.064391837 -0.15413854 -0.3101781 -0.48185998 -0.62971139 -0.700392 -0.71968389 -0.64484143 -0.54070425 -0.41981044 -0.32135186][-0.15687706 -0.11443223 -0.054656047 0.016745344 0.054352894 0.037024766 -0.1021236 -0.31682965 -0.55746067 -0.71838224 -0.81672192 -0.78273618 -0.68216074 -0.53048539 -0.38276389][-0.18769149 -0.11908387 -0.022585116 0.087773427 0.17513528 0.233401 0.17915541 0.0069129393 -0.2632252 -0.48596913 -0.68986785 -0.74702036 -0.71961415 -0.58884782 -0.4294301][-0.20577246 -0.13249144 -0.030725531 0.060525596 0.15050466 0.26516658 0.32628095 0.26343089 0.063625678 -0.16823861 -0.47011745 -0.62709129 -0.68854153 -0.60885608 -0.47379702][-0.18520609 -0.11367856 -0.0095462352 0.066493407 0.12977856 0.23907611 0.35520136 0.41953751 0.40030733 0.26819652 -0.039716065 -0.282559 -0.44396693 -0.47245738 -0.38802165][-0.14173731 -0.066827506 0.055896968 0.13554275 0.1852904 0.26624328 0.37245354 0.4899815 0.59612393 0.56129169 0.2887165 0.0066523924 -0.2601521 -0.40970653 -0.40105924][-0.12688577 -0.059409384 0.074631587 0.15969464 0.20994782 0.26799792 0.33320564 0.41755816 0.53431642 0.53465843 0.29592597 0.035128355 -0.25286531 -0.43105906 -0.46093509][-0.13122109 -0.1060876 -0.011118911 0.057189062 0.11061278 0.1727905 0.23525921 0.33392885 0.47901538 0.52837861 0.36712793 0.17536718 -0.05861856 -0.23510981 -0.30778486][-0.14695878 -0.16518492 -0.12507525 -0.091257937 -0.043057632 0.01848577 0.10380878 0.24232993 0.44662437 0.52206194 0.44280258 0.31959951 0.14102511 -0.0322166 -0.13137245][-0.1358963 -0.17429197 -0.17858653 -0.18568817 -0.15242103 -0.094134547 -0.0078947693 0.13086353 0.34512097 0.42380312 0.3974438 0.30116659 0.19444293 0.072397232 -0.011989579][-0.090529859 -0.11023638 -0.12941757 -0.15916273 -0.14823109 -0.11847621 -0.07402724 0.032425441 0.21032453 0.31030104 0.33463839 0.29033411 0.2269879 0.15311193 0.072695985][-0.0749561 -0.083556287 -0.10798653 -0.14328369 -0.15499431 -0.16655289 -0.18332095 -0.13157369 0.00090444833 0.082738951 0.13199639 0.14273706 0.13550043 0.095643625 0.060124844][-0.092961363 -0.1067567 -0.12568948 -0.15593556 -0.18142699 -0.22651091 -0.29853827 -0.33049196 -0.28287715 -0.25744367 -0.21742639 -0.18132074 -0.156958 -0.11752605 -0.084831052][-0.10472066 -0.10844298 -0.11004718 -0.12875949 -0.1620366 -0.21503246 -0.30352372 -0.38337156 -0.40381461 -0.41205174 -0.38205644 -0.35264826 -0.32481152 -0.25644755 -0.18864468][-0.10966568 -0.11030722 -0.0999316 -0.112045 -0.13200228 -0.15546164 -0.21233514 -0.28268686 -0.31306085 -0.34230703 -0.33229 -0.31713045 -0.28805006 -0.24921392 -0.20936881]]...]
INFO - root - 2017-12-06 06:56:06.171770: step 30810, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.638 sec/batch; 53h:28m:39s remains)
INFO - root - 2017-12-06 06:56:12.611719: step 30820, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.652 sec/batch; 54h:38m:53s remains)
INFO - root - 2017-12-06 06:56:19.210781: step 30830, loss = 0.85, batch loss = 0.63 (12.5 examples/sec; 0.641 sec/batch; 53h:44m:04s remains)
INFO - root - 2017-12-06 06:56:25.656482: step 30840, loss = 0.86, batch loss = 0.64 (12.4 examples/sec; 0.645 sec/batch; 54h:01m:37s remains)
INFO - root - 2017-12-06 06:56:31.888134: step 30850, loss = 0.87, batch loss = 0.65 (12.5 examples/sec; 0.638 sec/batch; 53h:26m:17s remains)
INFO - root - 2017-12-06 06:56:38.318537: step 30860, loss = 0.83, batch loss = 0.62 (13.1 examples/sec; 0.612 sec/batch; 51h:18m:25s remains)
INFO - root - 2017-12-06 06:56:44.608560: step 30870, loss = 0.71, batch loss = 0.50 (12.0 examples/sec; 0.669 sec/batch; 56h:03m:35s remains)
INFO - root - 2017-12-06 06:56:51.058337: step 30880, loss = 0.73, batch loss = 0.51 (12.1 examples/sec; 0.663 sec/batch; 55h:34m:26s remains)
INFO - root - 2017-12-06 06:56:57.636078: step 30890, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.661 sec/batch; 55h:20m:47s remains)
INFO - root - 2017-12-06 06:57:04.088931: step 30900, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.650 sec/batch; 54h:28m:42s remains)
2017-12-06 06:57:04.772480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.092411049 -0.088209711 -0.083235569 -0.0812196 -0.081032455 -0.083434559 -0.086886272 -0.091321267 -0.097350992 -0.10126685 -0.10295329 -0.10494411 -0.10839091 -0.10897301 -0.10842952][-0.096543677 -0.09188734 -0.08577349 -0.082568772 -0.08117459 -0.082002513 -0.0833262 -0.084113806 -0.082627922 -0.079400137 -0.07600712 -0.0727237 -0.072288208 -0.0735331 -0.073910527][-0.092207931 -0.08842437 -0.084610961 -0.082769506 -0.081813313 -0.082284883 -0.082355484 -0.080771409 -0.076440811 -0.071316756 -0.066280007 -0.061214603 -0.058507398 -0.057110775 -0.056297593][-0.078574069 -0.075210087 -0.072199851 -0.070458725 -0.069672257 -0.069218032 -0.068657175 -0.067666255 -0.065754116 -0.062718168 -0.059309691 -0.055810392 -0.053327717 -0.050815888 -0.050102528][-0.06957972 -0.067729384 -0.06526579 -0.063433938 -0.061620787 -0.060745232 -0.06036821 -0.059183009 -0.057386752 -0.054766327 -0.052388746 -0.050264224 -0.0484218 -0.048510227 -0.050922018][-0.060475022 -0.058358114 -0.057703603 -0.056979269 -0.056444157 -0.056498442 -0.056335066 -0.054981336 -0.052531764 -0.049435351 -0.04689315 -0.0450875 -0.045354526 -0.047228687 -0.051599156][-0.053666014 -0.052438464 -0.051856741 -0.051899336 -0.052279424 -0.052258931 -0.051823 -0.050852519 -0.049586102 -0.047749277 -0.045887366 -0.044907283 -0.045118596 -0.047330577 -0.051777124][-0.054144118 -0.053752571 -0.054115906 -0.0542659 -0.053155314 -0.051428746 -0.049868815 -0.048766814 -0.048291776 -0.047283322 -0.046597671 -0.045983568 -0.046179485 -0.048080612 -0.051427726][-0.0565053 -0.055940878 -0.056332409 -0.055639982 -0.053648043 -0.051417571 -0.050068025 -0.049198575 -0.049213108 -0.048958141 -0.04844781 -0.048144083 -0.0485043 -0.050393783 -0.053164687][-0.057955511 -0.056127332 -0.05559396 -0.054856237 -0.052916341 -0.051097464 -0.050319567 -0.050003577 -0.050601114 -0.051051419 -0.051090967 -0.051047456 -0.050777268 -0.052408669 -0.054891445][-0.058428869 -0.056986243 -0.056517802 -0.0556936 -0.053992227 -0.052576922 -0.052169107 -0.052229527 -0.052817006 -0.053098522 -0.0532253 -0.053307317 -0.053290822 -0.05480767 -0.056823242][-0.060561061 -0.059500903 -0.058755621 -0.057649221 -0.056209736 -0.055102758 -0.054718785 -0.054937284 -0.055490091 -0.055741332 -0.055511892 -0.055387255 -0.055621158 -0.056895185 -0.058641218][-0.062358655 -0.061749112 -0.061196961 -0.0602376 -0.058966711 -0.057869449 -0.057444029 -0.057663191 -0.057895925 -0.057974089 -0.057705473 -0.057492677 -0.05767088 -0.058586385 -0.060058929][-0.062780827 -0.062212989 -0.061893225 -0.061274856 -0.06043794 -0.059685588 -0.059418771 -0.059392635 -0.059358038 -0.05937393 -0.059156179 -0.059097487 -0.059250176 -0.059900686 -0.061074771][-0.063119516 -0.062570006 -0.062378243 -0.062038533 -0.061591215 -0.061140947 -0.060922578 -0.060795076 -0.060604893 -0.060557418 -0.060307823 -0.060367864 -0.060628638 -0.061195455 -0.062210694]]...]
INFO - root - 2017-12-06 06:57:11.180469: step 30910, loss = 0.86, batch loss = 0.64 (12.6 examples/sec; 0.634 sec/batch; 53h:07m:26s remains)
INFO - root - 2017-12-06 06:57:17.738957: step 30920, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.645 sec/batch; 53h:59m:56s remains)
INFO - root - 2017-12-06 06:57:24.263322: step 30930, loss = 0.85, batch loss = 0.63 (12.5 examples/sec; 0.641 sec/batch; 53h:39m:33s remains)
INFO - root - 2017-12-06 06:57:30.654496: step 30940, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.657 sec/batch; 54h:59m:44s remains)
INFO - root - 2017-12-06 06:57:37.164108: step 30950, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 53h:47m:16s remains)
INFO - root - 2017-12-06 06:57:43.582088: step 30960, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.640 sec/batch; 53h:34m:43s remains)
INFO - root - 2017-12-06 06:57:50.099974: step 30970, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.668 sec/batch; 55h:56m:35s remains)
INFO - root - 2017-12-06 06:57:56.583937: step 30980, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.642 sec/batch; 53h:47m:25s remains)
INFO - root - 2017-12-06 06:58:03.095300: step 30990, loss = 0.92, batch loss = 0.71 (11.9 examples/sec; 0.674 sec/batch; 56h:27m:02s remains)
INFO - root - 2017-12-06 06:58:09.681854: step 31000, loss = 0.89, batch loss = 0.68 (12.2 examples/sec; 0.658 sec/batch; 55h:05m:04s remains)
2017-12-06 06:58:10.317905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.052906688 -0.052764133 -0.052778386 -0.052815206 -0.052862324 -0.05289185 -0.052907053 -0.052892629 -0.05284863 -0.052784 -0.05272257 -0.052660689 -0.052602358 -0.052560605 -0.05284968][-0.0524192 -0.052128032 -0.052136444 -0.052189693 -0.052287012 -0.052360278 -0.052419141 -0.052419528 -0.05237215 -0.052284352 -0.052190442 -0.052104037 -0.052027959 -0.051978678 -0.052192852][-0.051758152 -0.051238403 -0.051243123 -0.051315989 -0.051462933 -0.051574741 -0.051686458 -0.051720373 -0.051686306 -0.051591229 -0.051484358 -0.051412046 -0.0513677 -0.051362503 -0.05154423][-0.050627042 -0.050032273 -0.050043218 -0.050126173 -0.05029539 -0.050416712 -0.050544225 -0.050600987 -0.050609831 -0.050560381 -0.050507594 -0.050521992 -0.05059192 -0.050698731 -0.050948475][-0.049336802 -0.048756413 -0.048750613 -0.048840832 -0.049009435 -0.04909366 -0.049177639 -0.049266282 -0.049364667 -0.049415018 -0.049489778 -0.049652576 -0.049884979 -0.050115138 -0.050485402][-0.04829371 -0.047731727 -0.047741383 -0.047820523 -0.047938533 -0.047967877 -0.047997121 -0.048124544 -0.048330851 -0.048530344 -0.048767064 -0.049080126 -0.04943819 -0.049753215 -0.05029732][-0.047739357 -0.047124356 -0.047188044 -0.047267124 -0.047343113 -0.047333397 -0.047299616 -0.047417719 -0.047686961 -0.048015531 -0.048393603 -0.04883286 -0.0492864 -0.049672231 -0.05037133][-0.047762655 -0.047062 -0.047176227 -0.047272883 -0.047333669 -0.047304232 -0.047239758 -0.047339711 -0.047632493 -0.048014347 -0.048472673 -0.048976842 -0.049480025 -0.049900852 -0.050698765][-0.048301551 -0.0476527 -0.047774591 -0.047897503 -0.047956921 -0.047927633 -0.047860965 -0.047963183 -0.048246074 -0.048635509 -0.049081448 -0.049557652 -0.050015669 -0.050374456 -0.051165059][-0.049254511 -0.048705943 -0.04883581 -0.048971511 -0.049035542 -0.049055923 -0.049025897 -0.049122 -0.049354877 -0.049674239 -0.050034892 -0.050398663 -0.050720442 -0.050954916 -0.051671389][-0.050367642 -0.049782235 -0.049890179 -0.050005384 -0.050072618 -0.050132778 -0.050170727 -0.05027872 -0.05046133 -0.050701723 -0.0509537 -0.051189464 -0.051382177 -0.051506396 -0.052154187][-0.051214252 -0.050614119 -0.050676551 -0.05075939 -0.050830994 -0.05091856 -0.05100581 -0.051123552 -0.051271334 -0.051439341 -0.05160046 -0.051738851 -0.051843237 -0.051903132 -0.052493233][-0.051779665 -0.051182691 -0.051199343 -0.051240843 -0.0512984 -0.051382795 -0.051482074 -0.051597927 -0.051722027 -0.051838595 -0.051933352 -0.05201266 -0.052061528 -0.0520835 -0.052641958][-0.052152175 -0.051518969 -0.051492751 -0.051495798 -0.051520213 -0.05157556 -0.051663432 -0.051755212 -0.051844165 -0.051916257 -0.051960863 -0.052000988 -0.052018873 -0.052019138 -0.05256423][-0.052468713 -0.051779948 -0.051721241 -0.051692896 -0.051687673 -0.051716875 -0.051784847 -0.051857237 -0.051915135 -0.051922545 -0.051902272 -0.051876791 -0.051836982 -0.051824704 -0.052372549]]...]
INFO - root - 2017-12-06 06:58:16.730865: step 31010, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 53h:46m:42s remains)
INFO - root - 2017-12-06 06:58:23.225446: step 31020, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 52h:47m:55s remains)
INFO - root - 2017-12-06 06:58:29.475567: step 31030, loss = 0.79, batch loss = 0.57 (13.2 examples/sec; 0.604 sec/batch; 50h:33m:44s remains)
INFO - root - 2017-12-06 06:58:35.994407: step 31040, loss = 0.83, batch loss = 0.61 (12.6 examples/sec; 0.636 sec/batch; 53h:15m:55s remains)
INFO - root - 2017-12-06 06:58:42.337569: step 31050, loss = 0.75, batch loss = 0.54 (12.4 examples/sec; 0.645 sec/batch; 54h:02m:05s remains)
INFO - root - 2017-12-06 06:58:48.850070: step 31060, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.656 sec/batch; 54h:56m:49s remains)
INFO - root - 2017-12-06 06:58:55.351378: step 31070, loss = 0.87, batch loss = 0.65 (12.7 examples/sec; 0.632 sec/batch; 52h:54m:06s remains)
INFO - root - 2017-12-06 06:59:01.765854: step 31080, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.642 sec/batch; 53h:44m:01s remains)
INFO - root - 2017-12-06 06:59:08.287907: step 31090, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.649 sec/batch; 54h:18m:45s remains)
INFO - root - 2017-12-06 06:59:14.719605: step 31100, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.650 sec/batch; 54h:27m:16s remains)
2017-12-06 06:59:15.311044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080288686 -0.079745546 -0.079583265 -0.079999462 -0.080406822 -0.081187122 -0.084258489 -0.087346017 -0.091226086 -0.094950609 -0.10206735 -0.10960845 -0.11651417 -0.11712007 -0.1170268][-0.078472942 -0.078014538 -0.077958427 -0.078289747 -0.078679159 -0.080603369 -0.085497051 -0.091370828 -0.099281169 -0.10739999 -0.11988547 -0.13480669 -0.14652835 -0.15249613 -0.15355077][-0.077132806 -0.076984122 -0.077039026 -0.077228226 -0.077568457 -0.08078739 -0.086562887 -0.093174629 -0.10408858 -0.11667486 -0.13224688 -0.14966542 -0.15971188 -0.1691511 -0.16693719][-0.083551779 -0.081526406 -0.078687221 -0.081995316 -0.0862403 -0.092274494 -0.099583909 -0.10548478 -0.11446121 -0.12515762 -0.13518339 -0.14265747 -0.14292526 -0.14439178 -0.1302004][-0.10067244 -0.096695736 -0.093153223 -0.090940312 -0.089727409 -0.087779224 -0.086496748 -0.082803942 -0.0820785 -0.083893724 -0.084535941 -0.080767475 -0.074529171 -0.062869526 -0.047700807][-0.092846476 -0.089961007 -0.088342205 -0.085309453 -0.083348013 -0.0793397 -0.075870275 -0.068716332 -0.060124494 -0.053420085 -0.047836978 -0.035508458 -0.026959762 -0.00356327 0.0012754798][-0.091192611 -0.087807015 -0.084920913 -0.077158771 -0.070002258 -0.06449008 -0.061622329 -0.051492259 -0.039200798 -0.032843329 -0.031183898 -0.024154946 -0.022785857 0.0026851818 -0.010763168][-0.068302721 -0.070532814 -0.071804836 -0.057644214 -0.041334558 -0.026613832 -0.014511153 -0.00079786032 0.011976108 0.025656976 0.030446745 0.035688683 0.038278461 0.066711172 0.038231775][0.0056835189 -0.0036533475 -0.0055324882 0.0032632798 0.019024752 0.039120793 0.057130918 0.073632419 0.083586782 0.085805625 0.081090689 0.093010664 0.10249703 0.1091834 0.058844402][0.056183174 0.048161983 0.050064519 0.05752039 0.068920553 0.080568746 0.088371485 0.092931718 0.095781744 0.088975251 0.080808491 0.088015333 0.095940858 0.0629165 0.013564363][0.07703121 0.077367023 0.083088666 0.088585824 0.096931234 0.10145673 0.10140479 0.10465603 0.10317616 0.10441774 0.10649547 0.11345476 0.11876063 0.062712058 0.0086392686][0.10085182 0.11792433 0.13590752 0.14059675 0.14125785 0.13975875 0.13645339 0.13855295 0.13136715 0.13324 0.12631573 0.1298562 0.13564928 0.087296486 0.021824203][0.10931772 0.13159129 0.15454596 0.16425212 0.16817941 0.16125585 0.14773571 0.14510714 0.13368037 0.12648797 0.10950279 0.10876849 0.10851379 0.081676751 0.028745241][0.056347951 0.076053783 0.09464623 0.10981208 0.11662675 0.10831347 0.089624569 0.084962592 0.064447075 0.067578539 0.059222668 0.068762556 0.072411418 0.068406239 0.042400762][0.013482623 0.032071367 0.040565386 0.048127592 0.04837434 0.042475939 0.022347249 0.014440767 -0.0079683512 -0.0069006011 -0.017281555 -0.0041817725 -0.010099053 0.0081745535 -0.0045795441]]...]
INFO - root - 2017-12-06 06:59:21.785037: step 31110, loss = 0.71, batch loss = 0.49 (12.4 examples/sec; 0.645 sec/batch; 53h:59m:08s remains)
INFO - root - 2017-12-06 06:59:28.241785: step 31120, loss = 0.92, batch loss = 0.71 (12.7 examples/sec; 0.631 sec/batch; 52h:51m:37s remains)
INFO - root - 2017-12-06 06:59:34.646880: step 31130, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.637 sec/batch; 53h:21m:57s remains)
INFO - root - 2017-12-06 06:59:41.097187: step 31140, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.674 sec/batch; 56h:26m:14s remains)
INFO - root - 2017-12-06 06:59:47.506954: step 31150, loss = 0.80, batch loss = 0.58 (12.7 examples/sec; 0.631 sec/batch; 52h:50m:25s remains)
INFO - root - 2017-12-06 06:59:53.997385: step 31160, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.648 sec/batch; 54h:16m:26s remains)
INFO - root - 2017-12-06 07:00:00.589267: step 31170, loss = 0.86, batch loss = 0.65 (11.8 examples/sec; 0.678 sec/batch; 56h:43m:04s remains)
INFO - root - 2017-12-06 07:00:07.076039: step 31180, loss = 0.79, batch loss = 0.57 (12.4 examples/sec; 0.645 sec/batch; 54h:01m:11s remains)
INFO - root - 2017-12-06 07:00:13.561887: step 31190, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.638 sec/batch; 53h:21m:59s remains)
INFO - root - 2017-12-06 07:00:20.037996: step 31200, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.651 sec/batch; 54h:30m:20s remains)
2017-12-06 07:00:20.709389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.078974679 -0.078877196 -0.078870982 -0.07890176 -0.078952476 -0.078971781 -0.079069808 -0.079436332 -0.080286413 -0.082354791 -0.085976481 -0.090495721 -0.095165096 -0.099594638 -0.10253581][-0.078641757 -0.078541063 -0.0784947 -0.078492746 -0.0785103 -0.0785318 -0.078673273 -0.079076104 -0.080221854 -0.083277404 -0.088685311 -0.094895154 -0.099767178 -0.10289556 -0.10558762][-0.078462481 -0.078352123 -0.078321919 -0.078277841 -0.078264736 -0.078339867 -0.078645289 -0.07912866 -0.080534808 -0.08459267 -0.0911674 -0.097286314 -0.098369464 -0.095439255 -0.094195746][-0.078420117 -0.078366935 -0.078370735 -0.078291975 -0.078241721 -0.0783289 -0.078728817 -0.079457253 -0.081061855 -0.085555337 -0.093149744 -0.099618137 -0.098377831 -0.091852471 -0.086774305][-0.078258917 -0.078379907 -0.078464292 -0.078366295 -0.07820195 -0.078087434 -0.078499153 -0.07971786 -0.082216 -0.08736416 -0.094369352 -0.0991291 -0.09603066 -0.090440765 -0.089705653][-0.078117587 -0.078294933 -0.078479141 -0.078338131 -0.077869833 -0.0775335 -0.077843778 -0.07929676 -0.082423136 -0.088268876 -0.094563022 -0.094469815 -0.083429612 -0.071143061 -0.070326537][-0.077948019 -0.07825125 -0.078658983 -0.078436926 -0.077520095 -0.076518416 -0.076640844 -0.07822784 -0.081393667 -0.087093085 -0.093497805 -0.092503242 -0.0781705 -0.058637805 -0.048762217][-0.077885836 -0.078465462 -0.079088762 -0.07900992 -0.077819653 -0.076139808 -0.075681679 -0.077608585 -0.081339918 -0.086647265 -0.091346882 -0.0885807 -0.073523186 -0.053796954 -0.041456167][-0.077882342 -0.07853584 -0.079348162 -0.079530209 -0.078614183 -0.076916873 -0.076072179 -0.077303678 -0.080020666 -0.084443443 -0.087863892 -0.0828545 -0.065026283 -0.043322727 -0.028428759][-0.077706419 -0.078370742 -0.079114549 -0.079360127 -0.078819193 -0.077727452 -0.0770005 -0.077284545 -0.078915454 -0.082946308 -0.087078013 -0.085104391 -0.072627604 -0.054312706 -0.041105479][-0.077430233 -0.077928841 -0.07853467 -0.078735568 -0.0783884 -0.077743106 -0.077284776 -0.077441737 -0.078542441 -0.081822276 -0.08721745 -0.089422561 -0.08378347 -0.072843589 -0.066977382][-0.077371411 -0.07758119 -0.0779328 -0.07817962 -0.078097411 -0.07778044 -0.077450171 -0.077376515 -0.078092925 -0.080582745 -0.085912511 -0.090958692 -0.089479573 -0.080562741 -0.074357212][-0.07773599 -0.077651024 -0.077780306 -0.077928752 -0.078074835 -0.078070819 -0.07794106 -0.077764831 -0.077822447 -0.078983784 -0.082514472 -0.087200247 -0.088145062 -0.082182951 -0.074917689][-0.0782309 -0.078009427 -0.077985115 -0.078004159 -0.078181185 -0.0783003 -0.078407057 -0.078387104 -0.078399681 -0.078516215 -0.079417996 -0.081728488 -0.083913334 -0.083837107 -0.081193209][-0.078633882 -0.078472428 -0.078459129 -0.078475788 -0.078589313 -0.0786351 -0.078758769 -0.078810945 -0.078957558 -0.079079449 -0.079375908 -0.080114454 -0.0809446 -0.0812919 -0.081159733]]...]
INFO - root - 2017-12-06 07:00:27.227159: step 31210, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.655 sec/batch; 54h:49m:38s remains)
INFO - root - 2017-12-06 07:00:33.518020: step 31220, loss = 0.79, batch loss = 0.57 (12.3 examples/sec; 0.648 sec/batch; 54h:13m:09s remains)
INFO - root - 2017-12-06 07:00:40.016923: step 31230, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.638 sec/batch; 53h:25m:02s remains)
INFO - root - 2017-12-06 07:00:46.530814: step 31240, loss = 0.73, batch loss = 0.52 (12.6 examples/sec; 0.636 sec/batch; 53h:15m:37s remains)
INFO - root - 2017-12-06 07:00:53.055538: step 31250, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.643 sec/batch; 53h:50m:13s remains)
INFO - root - 2017-12-06 07:00:59.483492: step 31260, loss = 0.86, batch loss = 0.65 (12.1 examples/sec; 0.660 sec/batch; 55h:13m:29s remains)
INFO - root - 2017-12-06 07:01:05.969000: step 31270, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.656 sec/batch; 54h:52m:21s remains)
INFO - root - 2017-12-06 07:01:12.321804: step 31280, loss = 0.87, batch loss = 0.66 (12.7 examples/sec; 0.632 sec/batch; 52h:54m:20s remains)
INFO - root - 2017-12-06 07:01:18.782896: step 31290, loss = 0.83, batch loss = 0.62 (12.3 examples/sec; 0.652 sec/batch; 54h:32m:02s remains)
INFO - root - 2017-12-06 07:01:25.401052: step 31300, loss = 0.77, batch loss = 0.55 (12.2 examples/sec; 0.654 sec/batch; 54h:42m:06s remains)
2017-12-06 07:01:26.071990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.097128823 -0.099856317 -0.099161774 -0.0971334 -0.091615662 -0.08405792 -0.064836234 -0.027608976 -0.036406912 -0.055602755 -0.071693949 -0.10038573 -0.13490577 -0.072074406 -0.0073141605][-0.097271144 -0.10411933 -0.11540233 -0.12937386 -0.1424737 -0.15988313 -0.16447414 -0.13360932 -0.12976427 -0.16285962 -0.22023988 -0.22255066 -0.20033176 -0.14529131 -0.0905186][-0.0990262 -0.10649247 -0.12175506 -0.14494425 -0.16832444 -0.20344327 -0.23953913 -0.24810621 -0.29086736 -0.33840433 -0.37273818 -0.40299606 -0.41976893 -0.38168383 -0.33153683][-0.098014534 -0.1066052 -0.12760134 -0.15662825 -0.16436292 -0.1304208 -0.056375567 0.048679262 0.061848611 -0.018919662 -0.12284744 -0.23511629 -0.38383949 -0.49328727 -0.51845509][-0.097532384 -0.11323914 -0.14103296 -0.18442625 -0.20136741 -0.15355602 -0.032840844 0.18316866 0.33297938 0.36909622 0.29270309 0.15134533 -0.137996 -0.43805158 -0.65614331][-0.097185828 -0.11077326 -0.14028729 -0.21243225 -0.28168046 -0.29386634 -0.2196504 -0.029533431 0.11812203 0.17132159 0.092622787 -0.0022735074 -0.25111568 -0.58139378 -0.85972303][-0.12462068 -0.15368858 -0.20145331 -0.29235452 -0.36307234 -0.37906313 -0.2787199 -0.1030203 0.03816238 0.089981079 0.014305733 -0.15299681 -0.4198218 -0.77025139 -1.0681819][-0.12616745 -0.14289162 -0.19656101 -0.29564297 -0.37547255 -0.42765576 -0.33147338 -0.12611365 0.030373298 0.076313555 0.03406401 -0.12823939 -0.36512524 -0.68505186 -1.0072075][-0.10045248 -0.088753678 -0.12224794 -0.2103285 -0.28558594 -0.31856054 -0.24556468 -0.082760558 0.041276529 0.10669312 0.10436307 -0.033374961 -0.21142131 -0.46544814 -0.66526353][-0.10191263 -0.090831116 -0.10372715 -0.16168997 -0.21219119 -0.18744898 -0.050686866 0.13136058 0.25185275 0.2929129 0.268853 0.23397411 0.14062937 -0.0030917227 -0.13816556][-0.11016244 -0.12761591 -0.16354451 -0.22895509 -0.30569232 -0.29175425 -0.15806673 0.012544744 0.12305652 0.18551438 0.20759292 0.24695291 0.14170966 0.087275118 0.017565824][-0.1115534 -0.16344553 -0.25527459 -0.39625168 -0.55435872 -0.64509755 -0.64742023 -0.57477564 -0.52231765 -0.53119904 -0.47904754 -0.34061471 -0.33770162 -0.37275803 -0.49447936][-0.11409149 -0.15645173 -0.26184338 -0.42124075 -0.5694961 -0.68259031 -0.74602854 -0.77203244 -0.80174863 -0.88709593 -0.87343413 -0.69825917 -0.55571663 -0.58569276 -0.77495968][-0.07256189 -0.09121564 -0.14262433 -0.21422878 -0.256355 -0.24809034 -0.16320391 -0.050750833 -0.0049026608 0.026973635 0.062863231 0.1567784 0.14499016 -0.034746185 -0.20734181][-0.023954287 -0.0038774684 -0.012591153 -0.0065507218 0.041257963 0.15423207 0.35935193 0.65931422 0.89414769 1.0222775 1.0612389 1.2918509 1.4434346 1.3524655 1.2018776]]...]
INFO - root - 2017-12-06 07:01:32.465309: step 31310, loss = 0.77, batch loss = 0.55 (15.6 examples/sec; 0.512 sec/batch; 42h:52m:28s remains)
INFO - root - 2017-12-06 07:01:38.935328: step 31320, loss = 0.86, batch loss = 0.65 (12.1 examples/sec; 0.659 sec/batch; 55h:08m:29s remains)
INFO - root - 2017-12-06 07:01:45.463523: step 31330, loss = 0.84, batch loss = 0.63 (13.0 examples/sec; 0.618 sec/batch; 51h:40m:17s remains)
INFO - root - 2017-12-06 07:01:51.893529: step 31340, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.645 sec/batch; 53h:55m:19s remains)
INFO - root - 2017-12-06 07:01:58.451340: step 31350, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 53h:29m:17s remains)
INFO - root - 2017-12-06 07:02:04.971518: step 31360, loss = 0.84, batch loss = 0.63 (13.0 examples/sec; 0.614 sec/batch; 51h:22m:57s remains)
INFO - root - 2017-12-06 07:02:11.473621: step 31370, loss = 0.83, batch loss = 0.62 (12.6 examples/sec; 0.636 sec/batch; 53h:14m:24s remains)
INFO - root - 2017-12-06 07:02:17.982225: step 31380, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.654 sec/batch; 54h:43m:22s remains)
INFO - root - 2017-12-06 07:02:24.448060: step 31390, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.640 sec/batch; 53h:33m:32s remains)
INFO - root - 2017-12-06 07:02:30.909703: step 31400, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.644 sec/batch; 53h:49m:55s remains)
2017-12-06 07:02:31.509258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066637032 -0.066743463 -0.067127772 -0.0683227 -0.070374474 -0.072871223 -0.075716436 -0.078678928 -0.0812916 -0.0829419 -0.083986826 -0.084491149 -0.0845824 -0.084016517 -0.083166875][-0.066667348 -0.066720828 -0.066970617 -0.067736551 -0.06908416 -0.0707688 -0.072826169 -0.075040527 -0.077147342 -0.078549623 -0.079580739 -0.080203421 -0.08038523 -0.079886161 -0.079025231][-0.066717975 -0.066586435 -0.06663508 -0.066914372 -0.067533188 -0.068396837 -0.069580644 -0.070956782 -0.072419241 -0.073583394 -0.0745514 -0.0752508 -0.075519666 -0.0752236 -0.07456252][-0.066707976 -0.066476315 -0.066381641 -0.066260174 -0.066231519 -0.066333368 -0.066672616 -0.067231253 -0.067993671 -0.068794437 -0.06959153 -0.070248738 -0.070551872 -0.070461422 -0.070034295][-0.066626087 -0.066380873 -0.066219516 -0.065848947 -0.0653602 -0.064890064 -0.06454958 -0.06442 -0.064583577 -0.064998664 -0.065562688 -0.066089444 -0.066421531 -0.066470332 -0.066290863][-0.066585153 -0.066290751 -0.066090807 -0.065608196 -0.06486965 -0.064004496 -0.063155577 -0.062466752 -0.062122375 -0.062106367 -0.062411018 -0.062843055 -0.063225217 -0.063455164 -0.063633218][-0.066614941 -0.0662269 -0.066014349 -0.065485328 -0.064616308 -0.063469134 -0.062231839 -0.061062545 -0.060279466 -0.05992217 -0.060017593 -0.060407467 -0.060867082 -0.061267212 -0.061679274][-0.066668712 -0.066187382 -0.065979645 -0.065442555 -0.064500466 -0.063177913 -0.061662905 -0.060122877 -0.058990721 -0.05833824 -0.058250964 -0.058568493 -0.059084605 -0.059624106 -0.060200736][-0.0666596 -0.066135056 -0.065924153 -0.065379366 -0.064416744 -0.063024551 -0.061366007 -0.059620805 -0.058259979 -0.057404261 -0.057176623 -0.05746277 -0.058010634 -0.058648523 -0.059316777][-0.066635154 -0.066103816 -0.065880746 -0.065333724 -0.064396627 -0.063040927 -0.061390594 -0.059627138 -0.058203626 -0.057290055 -0.056979571 -0.057232317 -0.057777509 -0.058471236 -0.059131104][-0.066656977 -0.066097371 -0.065881893 -0.065363742 -0.064509638 -0.063287564 -0.061769836 -0.0601547 -0.058848936 -0.058015838 -0.057723351 -0.057959314 -0.058499955 -0.059170436 -0.05974324][-0.066661134 -0.066109434 -0.0659105 -0.065447927 -0.064712204 -0.063706204 -0.062450267 -0.061105296 -0.060032636 -0.059384957 -0.059182569 -0.059382338 -0.05985922 -0.060438044 -0.060904428][-0.066654652 -0.066120908 -0.065935254 -0.065536 -0.064924337 -0.064150557 -0.063205034 -0.062213778 -0.061432496 -0.06097756 -0.060871042 -0.061039723 -0.061398126 -0.061816059 -0.062153518][-0.066643849 -0.0661416 -0.0659654 -0.065617353 -0.0651147 -0.064528272 -0.063834816 -0.063139319 -0.062607221 -0.062346395 -0.06234327 -0.062501594 -0.062746711 -0.063017048 -0.0632532][-0.066641495 -0.066149831 -0.065984279 -0.065670282 -0.065226853 -0.06474036 -0.064210057 -0.063703157 -0.063338444 -0.063195363 -0.063252047 -0.063411981 -0.06359078 -0.063761935 -0.063975833]]...]
INFO - root - 2017-12-06 07:02:37.844847: step 31410, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.647 sec/batch; 54h:08m:16s remains)
INFO - root - 2017-12-06 07:02:44.411663: step 31420, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 53h:26m:21s remains)
INFO - root - 2017-12-06 07:02:50.757038: step 31430, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.641 sec/batch; 53h:34m:50s remains)
INFO - root - 2017-12-06 07:02:57.156561: step 31440, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 53h:21m:45s remains)
INFO - root - 2017-12-06 07:03:03.760700: step 31450, loss = 0.77, batch loss = 0.55 (11.9 examples/sec; 0.672 sec/batch; 56h:12m:58s remains)
INFO - root - 2017-12-06 07:03:10.297378: step 31460, loss = 0.95, batch loss = 0.74 (12.0 examples/sec; 0.664 sec/batch; 55h:32m:44s remains)
INFO - root - 2017-12-06 07:03:16.813111: step 31470, loss = 0.80, batch loss = 0.59 (11.8 examples/sec; 0.676 sec/batch; 56h:29m:41s remains)
INFO - root - 2017-12-06 07:03:23.300595: step 31480, loss = 0.81, batch loss = 0.59 (12.1 examples/sec; 0.660 sec/batch; 55h:10m:05s remains)
INFO - root - 2017-12-06 07:03:29.875247: step 31490, loss = 0.85, batch loss = 0.64 (11.8 examples/sec; 0.679 sec/batch; 56h:46m:48s remains)
INFO - root - 2017-12-06 07:03:36.312357: step 31500, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.645 sec/batch; 53h:53m:24s remains)
2017-12-06 07:03:36.914122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063901246 -0.058900222 -0.057425037 -0.059604041 -0.062386684 -0.065755561 -0.068499334 -0.070055842 -0.071869008 -0.074634835 -0.076841727 -0.0773022 -0.078237794 -0.080057018 -0.082381561][-0.062688708 -0.062892243 -0.064445972 -0.0658481 -0.066515371 -0.068061575 -0.071071349 -0.074123859 -0.076360092 -0.078360319 -0.080080561 -0.08055906 -0.080349922 -0.08074493 -0.080981947][-0.072168194 -0.072541595 -0.0738853 -0.076158173 -0.078254908 -0.079447575 -0.08054477 -0.081930093 -0.08331728 -0.084532514 -0.084814429 -0.084375858 -0.083509937 -0.082294568 -0.081174247][-0.081770293 -0.082094423 -0.0829877 -0.084743015 -0.087796018 -0.091165245 -0.094422914 -0.096710853 -0.097171731 -0.094804645 -0.090830386 -0.087148957 -0.084413894 -0.082432866 -0.081174321][-0.080713145 -0.080866218 -0.081632145 -0.084507093 -0.089415729 -0.095203981 -0.10125168 -0.10533307 -0.10610449 -0.10343156 -0.097573273 -0.091202304 -0.086223543 -0.082888134 -0.08122541][-0.0804246 -0.080923617 -0.084005684 -0.086791426 -0.089537062 -0.093915835 -0.10140467 -0.10881066 -0.11287002 -0.112348 -0.10683869 -0.098212637 -0.090179391 -0.084656246 -0.08185596][-0.080630466 -0.080284491 -0.082492277 -0.085497245 -0.086494356 -0.088290684 -0.093984589 -0.10271961 -0.11094722 -0.11507276 -0.11276677 -0.10447797 -0.094575047 -0.086710095 -0.0825431][-0.08148627 -0.083116174 -0.08623483 -0.089051545 -0.089632817 -0.090688772 -0.095233805 -0.10128685 -0.10610344 -0.108051 -0.10767898 -0.10312984 -0.095649078 -0.087583937 -0.082715943][-0.08184436 -0.083562061 -0.086838461 -0.08935415 -0.08861082 -0.085753322 -0.085774831 -0.090161264 -0.096346825 -0.10009953 -0.10029246 -0.0967903 -0.091772348 -0.08634951 -0.083653085][-0.079917952 -0.081626244 -0.085009575 -0.086272821 -0.082799524 -0.0771115 -0.072927274 -0.073478438 -0.077484481 -0.081759259 -0.086549886 -0.089719608 -0.088780187 -0.084120564 -0.081687525][-0.0815934 -0.082028627 -0.083014086 -0.081568666 -0.077114664 -0.073270366 -0.072255351 -0.073811129 -0.075515375 -0.077390455 -0.080799147 -0.083574906 -0.08450181 -0.083293796 -0.082225405][-0.081691034 -0.082111843 -0.0830347 -0.080950506 -0.075598441 -0.070863232 -0.070198409 -0.072275311 -0.074689828 -0.076724082 -0.079939477 -0.083360724 -0.0850587 -0.083731115 -0.0820645][-0.081403032 -0.081901334 -0.083175972 -0.082372814 -0.078096151 -0.072352655 -0.068600193 -0.067765914 -0.068719856 -0.071173809 -0.076314054 -0.081673265 -0.08441212 -0.083743587 -0.082552083][-0.081215493 -0.081641249 -0.082986735 -0.083606794 -0.081935935 -0.078522496 -0.075464137 -0.073686048 -0.073543131 -0.07542444 -0.079405949 -0.082230531 -0.08313033 -0.0825835 -0.08236403][-0.081377864 -0.081383593 -0.082195014 -0.083244711 -0.083439335 -0.08237116 -0.080932848 -0.0798482 -0.079487234 -0.080576032 -0.083138652 -0.084775984 -0.084588572 -0.083457425 -0.082739107]]...]
INFO - root - 2017-12-06 07:03:43.464433: step 31510, loss = 0.75, batch loss = 0.53 (11.9 examples/sec; 0.673 sec/batch; 56h:15m:35s remains)
INFO - root - 2017-12-06 07:03:49.921787: step 31520, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.658 sec/batch; 55h:02m:38s remains)
INFO - root - 2017-12-06 07:03:56.404871: step 31530, loss = 0.85, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 53h:41m:03s remains)
INFO - root - 2017-12-06 07:04:02.947828: step 31540, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.660 sec/batch; 55h:09m:23s remains)
INFO - root - 2017-12-06 07:04:09.421248: step 31550, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.660 sec/batch; 55h:11m:07s remains)
INFO - root - 2017-12-06 07:04:15.907580: step 31560, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.642 sec/batch; 53h:42m:29s remains)
INFO - root - 2017-12-06 07:04:22.354308: step 31570, loss = 0.85, batch loss = 0.63 (12.2 examples/sec; 0.657 sec/batch; 54h:55m:27s remains)
INFO - root - 2017-12-06 07:04:28.852099: step 31580, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 54h:07m:00s remains)
INFO - root - 2017-12-06 07:04:35.413201: step 31590, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.653 sec/batch; 54h:35m:09s remains)
INFO - root - 2017-12-06 07:04:41.731487: step 31600, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.632 sec/batch; 52h:51m:44s remains)
2017-12-06 07:04:42.380317: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27910802 0.34783185 0.29914117 0.19169146 -0.051346857 -0.25998789 -0.52333617 -0.83385479 -1.0753347 -1.2353963 -1.2100074 -0.94225764 -0.62751365 -0.38010961 -0.22663625][0.293122 0.45151153 0.4857612 0.40816876 0.097894296 -0.29709607 -0.80519319 -1.2956533 -1.5956272 -1.6722879 -1.5399008 -1.1565167 -0.7532295 -0.46241197 -0.28758347][0.21297368 0.52985525 0.67534125 0.646032 0.32139093 -0.2105605 -0.90132654 -1.6196856 -2.1116049 -2.1700966 -1.9243233 -1.4361794 -0.92372596 -0.50882244 -0.26404855][0.11059232 0.48010233 0.71696293 0.76553369 0.53500354 0.020343497 -0.64964187 -1.3839234 -2.0015104 -2.1738193 -1.9746861 -1.5097257 -0.99333084 -0.5429337 -0.25452781][0.02038835 0.31424928 0.46215782 0.45066473 0.2778824 -0.0831341 -0.48589474 -0.93565774 -1.3679117 -1.6204824 -1.6447816 -1.4021424 -1.0245997 -0.60553741 -0.29765087][0.15183583 0.40009829 0.56750894 0.55314636 0.39286387 0.12373349 -0.17104079 -0.44345903 -0.66963005 -0.91217411 -1.0399655 -0.957881 -0.74518323 -0.49285686 -0.25539562][0.50039124 0.74706686 0.918856 0.92952073 0.79227245 0.52862847 0.18498734 -0.11349455 -0.29967475 -0.48704246 -0.61541617 -0.61989331 -0.52269214 -0.35488212 -0.17510131][0.54183149 0.82267332 1.0539501 1.1201106 1.0438302 0.83577442 0.45337722 0.15357816 -0.022645026 -0.17838506 -0.28686726 -0.32660663 -0.3006897 -0.22449994 -0.14260766][0.29875556 0.56884384 0.82664704 0.96375644 0.93295538 0.74545252 0.38761997 0.10820691 -0.088903062 -0.22057888 -0.28545803 -0.29544741 -0.23450252 -0.13850832 -0.079095781][0.079234213 0.19655082 0.32246196 0.40249175 0.42514804 0.29078466 0.032272451 -0.17654516 -0.29463309 -0.41254973 -0.43484232 -0.37978503 -0.26832914 -0.13827834 -0.055046316][-0.13705762 -0.10630756 -0.1013099 -0.084826082 -0.0964267 -0.14912556 -0.28077793 -0.36222556 -0.41557163 -0.49413818 -0.49043652 -0.40155548 -0.264027 -0.16248241 -0.084185][-0.21950904 -0.25545073 -0.30704898 -0.38999498 -0.516304 -0.54091942 -0.63484442 -0.59675062 -0.58865643 -0.56030154 -0.52012628 -0.41729063 -0.27367204 -0.17527628 -0.10983825][-0.14214467 -0.18274486 -0.27080023 -0.41749114 -0.59890664 -0.69242728 -0.78251529 -0.68343556 -0.5717839 -0.44921386 -0.39690664 -0.28557765 -0.20156533 -0.13123423 -0.0960631][-0.10510559 -0.11658297 -0.17626509 -0.31006932 -0.4702915 -0.5074296 -0.51556933 -0.43304983 -0.3154844 -0.25238371 -0.2700088 -0.20160058 -0.1417775 -0.10355031 -0.08078111][-0.094134994 -0.11156429 -0.14989841 -0.19554313 -0.28703457 -0.33847097 -0.32683927 -0.29713255 -0.19057536 -0.14391069 -0.17932779 -0.19436024 -0.11409576 -0.082158916 -0.065335855]]...]
INFO - root - 2017-12-06 07:04:48.962637: step 31610, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.643 sec/batch; 53h:42m:23s remains)
INFO - root - 2017-12-06 07:04:55.455552: step 31620, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.642 sec/batch; 53h:37m:57s remains)
INFO - root - 2017-12-06 07:05:01.974682: step 31630, loss = 0.80, batch loss = 0.59 (12.1 examples/sec; 0.663 sec/batch; 55h:27m:00s remains)
INFO - root - 2017-12-06 07:05:08.411107: step 31640, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.635 sec/batch; 53h:02m:48s remains)
INFO - root - 2017-12-06 07:05:14.891600: step 31650, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.633 sec/batch; 52h:54m:14s remains)
INFO - root - 2017-12-06 07:05:21.476076: step 31660, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 54h:23m:16s remains)
INFO - root - 2017-12-06 07:05:28.025611: step 31670, loss = 0.72, batch loss = 0.51 (12.3 examples/sec; 0.650 sec/batch; 54h:17m:38s remains)
INFO - root - 2017-12-06 07:05:34.492744: step 31680, loss = 0.74, batch loss = 0.53 (12.0 examples/sec; 0.668 sec/batch; 55h:51m:18s remains)
INFO - root - 2017-12-06 07:05:40.785544: step 31690, loss = 0.85, batch loss = 0.63 (12.3 examples/sec; 0.649 sec/batch; 54h:15m:02s remains)
INFO - root - 2017-12-06 07:05:47.320425: step 31700, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.661 sec/batch; 55h:14m:35s remains)
2017-12-06 07:05:47.976724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.095762148 -0.089185171 -0.0799774 -0.071853206 -0.068171091 -0.070411757 -0.078659125 -0.090761393 -0.10386072 -0.11472993 -0.12131874 -0.12213805 -0.11709979 -0.10746019 -0.095808819][-0.091911 -0.084833972 -0.073931485 -0.063101634 -0.056520645 -0.056491073 -0.063357413 -0.075226426 -0.089188382 -0.10232365 -0.11230887 -0.11700287 -0.11531609 -0.10810515 -0.097548656][-0.088308543 -0.080977023 -0.069765456 -0.05774834 -0.048909325 -0.046329249 -0.050754882 -0.060962468 -0.074267343 -0.088251673 -0.10054316 -0.10850123 -0.11043543 -0.10648794 -0.097978][-0.086007573 -0.078973904 -0.068289667 -0.055523582 -0.044712942 -0.039609008 -0.041015271 -0.048763685 -0.06102208 -0.075188488 -0.088462904 -0.098451927 -0.10342889 -0.10267756 -0.096700765][-0.087833516 -0.081255853 -0.07037247 -0.056597844 -0.043755118 -0.0359277 -0.034427971 -0.039309118 -0.049622778 -0.063523576 -0.077625319 -0.088953383 -0.096155472 -0.098003022 -0.094406933][-0.091026738 -0.084625922 -0.074051626 -0.059652515 -0.045502175 -0.03531396 -0.030890096 -0.032816358 -0.040806644 -0.053703055 -0.067891322 -0.080027469 -0.088855185 -0.092654735 -0.091040805][-0.092645608 -0.08636266 -0.076406941 -0.062450387 -0.048083611 -0.036610737 -0.030132253 -0.02984776 -0.035825226 -0.047019944 -0.060198087 -0.072206043 -0.081692144 -0.086755067 -0.086820766][-0.093099423 -0.087267652 -0.07776995 -0.064994007 -0.051070012 -0.039178304 -0.031709529 -0.030252788 -0.034896046 -0.044364929 -0.055862661 -0.06678959 -0.075951785 -0.0814978 -0.082657449][-0.094779342 -0.089128554 -0.079900466 -0.067907065 -0.054731291 -0.043339133 -0.035671715 -0.033490088 -0.037088148 -0.044792257 -0.054714859 -0.064419188 -0.072837725 -0.078093506 -0.079580694][-0.09365198 -0.088209964 -0.080053471 -0.069688551 -0.058195028 -0.048206892 -0.041460883 -0.039256796 -0.041651558 -0.047548462 -0.055522773 -0.063713089 -0.070854262 -0.075412728 -0.076785356][-0.08877866 -0.084124885 -0.0776059 -0.06954366 -0.06049639 -0.052610435 -0.047344673 -0.045478269 -0.04699342 -0.051174957 -0.057219051 -0.063515216 -0.068955384 -0.072562762 -0.073711067][-0.081554174 -0.078264505 -0.073668413 -0.068062223 -0.061617546 -0.055910885 -0.05188784 -0.050274458 -0.051243026 -0.054061841 -0.058315255 -0.062768504 -0.066711068 -0.069433421 -0.070356339][-0.074388459 -0.072211273 -0.069281831 -0.065542743 -0.061330751 -0.05764313 -0.054850258 -0.053607985 -0.054199148 -0.056129973 -0.059007507 -0.06199586 -0.064544842 -0.066543728 -0.067508049][-0.068047121 -0.066485077 -0.064766668 -0.06254448 -0.060157597 -0.058111552 -0.056589685 -0.055936698 -0.056346953 -0.057541985 -0.059256554 -0.06109434 -0.06265372 -0.0639785 -0.064883269][-0.064053819 -0.062914267 -0.061990034 -0.060849041 -0.059562057 -0.0585343 -0.057834029 -0.057666827 -0.057920169 -0.058457632 -0.059266005 -0.060219157 -0.061000824 -0.061902218 -0.062737472]]...]
INFO - root - 2017-12-06 07:05:54.418198: step 31710, loss = 0.76, batch loss = 0.55 (11.8 examples/sec; 0.679 sec/batch; 56h:42m:13s remains)
INFO - root - 2017-12-06 07:06:00.981325: step 31720, loss = 0.80, batch loss = 0.59 (11.6 examples/sec; 0.692 sec/batch; 57h:49m:22s remains)
INFO - root - 2017-12-06 07:06:07.502975: step 31730, loss = 0.86, batch loss = 0.65 (12.3 examples/sec; 0.650 sec/batch; 54h:18m:06s remains)
INFO - root - 2017-12-06 07:06:14.038302: step 31740, loss = 0.85, batch loss = 0.63 (11.9 examples/sec; 0.674 sec/batch; 56h:20m:31s remains)
INFO - root - 2017-12-06 07:06:20.526409: step 31750, loss = 0.79, batch loss = 0.57 (12.3 examples/sec; 0.652 sec/batch; 54h:29m:30s remains)
INFO - root - 2017-12-06 07:06:26.900565: step 31760, loss = 0.77, batch loss = 0.55 (12.7 examples/sec; 0.630 sec/batch; 52h:37m:08s remains)
INFO - root - 2017-12-06 07:06:33.419566: step 31770, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.642 sec/batch; 53h:38m:49s remains)
INFO - root - 2017-12-06 07:06:39.637059: step 31780, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.639 sec/batch; 53h:23m:24s remains)
INFO - root - 2017-12-06 07:06:46.149377: step 31790, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.650 sec/batch; 54h:20m:08s remains)
INFO - root - 2017-12-06 07:06:52.714836: step 31800, loss = 0.84, batch loss = 0.62 (11.7 examples/sec; 0.682 sec/batch; 56h:59m:20s remains)
2017-12-06 07:06:53.516529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.73288316 -0.63369519 -0.46264234 -0.21908909 -0.067729384 -0.27405268 -0.46759269 -0.40004221 -0.33907947 -0.53777373 -0.80895329 -0.76384795 -0.67807204 -0.74618518 -0.916785][-0.99109238 -0.78819597 -0.60650378 -0.49191985 -0.45810381 -0.3286764 -0.10432818 -0.31952044 -0.65227121 -0.88076115 -1.1942786 -1.2051332 -1.3170472 -1.6547786 -2.0372198][-1.0688162 -1.031302 -1.0021455 -0.84690309 -0.75839335 -0.83734083 -1.0164459 -0.96566808 -0.67450476 -0.82037568 -1.2354715 -1.6714193 -2.0823834 -2.2000041 -2.586556][-0.72265804 -0.76461375 -0.88822263 -1.0635115 -1.2216471 -1.251385 -1.2527984 -1.3039938 -1.4630773 -1.3030185 -1.1112347 -1.4612488 -2.0572674 -2.4186292 -2.9015071][0.018972151 -0.33603007 -0.574284 -0.75203073 -1.000277 -1.2653949 -1.434729 -1.5382777 -1.5788873 -1.3920264 -1.1664233 -1.0379715 -0.97036672 -1.283667 -1.7285157][0.74591666 0.40692991 0.11690273 -0.16896307 -0.33143398 -0.43617725 -0.54314905 -0.70160061 -0.64604646 -0.5324111 -0.17872071 0.052252717 0.28781193 0.28947103 0.12358669][0.62963116 0.2359879 0.17539868 0.010739058 -0.083332717 -0.06289354 -0.057157028 -0.032505721 -0.014968306 0.288393 0.84185159 1.3282988 1.8136567 1.7364734 1.9869194][0.6335845 0.252 -0.16009225 -0.31532884 -0.28729278 -0.18345492 -0.10101241 0.14334148 0.45230666 0.73814386 0.936414 1.5422878 2.1157691 2.3459332 2.44121][-0.15772372 -0.14384016 -0.18248391 -0.33329955 -0.39570847 -0.37845162 -0.20810953 0.1494683 0.3740046 0.65536338 1.0098052 1.2756082 1.4809417 2.1111929 2.396575][-0.53015029 -0.46985212 -0.501161 -0.38180524 -0.32087454 -0.40454167 -0.40784726 -0.25671634 -0.12352945 -0.0017909408 0.03582982 0.3321695 0.5962953 0.88303274 1.1847187][-0.93199623 -1.0369786 -1.1343818 -1.1559361 -1.1479442 -0.96912789 -0.9024294 -0.8854751 -0.91225404 -1.0365208 -1.1952709 -1.2829489 -1.2361293 -0.61786103 -0.1155479][-0.70415288 -0.82862079 -1.1367038 -1.2833737 -1.4380903 -1.5127597 -1.5327579 -1.291353 -1.1884538 -1.3839611 -1.7324815 -2.0761886 -2.4311397 -2.5205383 -2.1091306][-0.93864864 -1.0034013 -1.0690532 -1.2197033 -1.4310271 -1.3909086 -1.5133051 -1.6175787 -1.7495928 -1.8868793 -2.0079434 -2.3869033 -2.8104963 -3.0556424 -3.5091946][-0.82210064 -0.88005453 -0.99978459 -1.0684754 -1.0738239 -1.1567061 -1.3403717 -1.4374937 -1.6887174 -2.0058436 -2.3129451 -2.6767445 -2.985127 -3.3304408 -3.4894366][-0.88287783 -0.87394309 -0.90243995 -0.87277567 -0.89096487 -0.95674688 -1.0342484 -1.2344016 -1.5298427 -1.8062917 -2.0556753 -2.4872794 -2.8462381 -3.1174049 -3.1979291]]...]
INFO - root - 2017-12-06 07:07:00.002045: step 31810, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 54h:23m:43s remains)
INFO - root - 2017-12-06 07:07:06.443051: step 31820, loss = 0.93, batch loss = 0.71 (12.4 examples/sec; 0.647 sec/batch; 54h:02m:13s remains)
INFO - root - 2017-12-06 07:07:12.929310: step 31830, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 53h:19m:01s remains)
INFO - root - 2017-12-06 07:07:19.561031: step 31840, loss = 0.72, batch loss = 0.51 (12.1 examples/sec; 0.661 sec/batch; 55h:12m:43s remains)
INFO - root - 2017-12-06 07:07:25.970005: step 31850, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.657 sec/batch; 54h:53m:20s remains)
INFO - root - 2017-12-06 07:07:32.404305: step 31860, loss = 0.84, batch loss = 0.62 (12.4 examples/sec; 0.648 sec/batch; 54h:05m:23s remains)
INFO - root - 2017-12-06 07:07:38.859457: step 31870, loss = 0.80, batch loss = 0.58 (12.6 examples/sec; 0.635 sec/batch; 53h:02m:58s remains)
INFO - root - 2017-12-06 07:07:45.196679: step 31880, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 52h:33m:05s remains)
INFO - root - 2017-12-06 07:07:51.648153: step 31890, loss = 0.75, batch loss = 0.53 (12.6 examples/sec; 0.632 sec/batch; 52h:48m:36s remains)
INFO - root - 2017-12-06 07:07:58.250507: step 31900, loss = 0.88, batch loss = 0.66 (12.5 examples/sec; 0.642 sec/batch; 53h:35m:47s remains)
2017-12-06 07:07:58.938189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12155325 -0.11565393 -0.052038983 -0.062732458 -0.052198268 -0.019398145 0.04759714 0.097877637 0.1392792 0.13871591 0.14264658 0.12333255 0.068799287 -0.01635243 -0.098151579][-0.17827037 -0.1474092 -0.052861903 0.0076590478 0.05800052 0.066401988 0.094057783 0.14187989 0.19708197 0.20677577 0.19897561 0.16529991 0.12388942 0.076356679 0.018720806][-0.15717942 -0.17023781 -0.14129102 -0.15453935 -0.15635413 -0.13344109 -0.091398455 -0.023522772 0.066811919 0.13400073 0.19329266 0.19426249 0.155096 0.10811244 0.062799871][-0.16333482 -0.21810359 -0.21509299 -0.21394345 -0.24714002 -0.2659815 -0.26187861 -0.20103186 -0.11022803 -0.015450485 0.071126714 0.1142763 0.12152247 0.089854375 0.048646212][-0.2307055 -0.27293372 -0.26701987 -0.23344496 -0.26618522 -0.29210389 -0.30887821 -0.293912 -0.24143104 -0.16391176 -0.095987417 -0.056044351 -0.021561421 -0.018892348 -0.021728195][-0.084249325 -0.13628814 -0.18648954 -0.21938232 -0.28184685 -0.30497503 -0.310754 -0.32155842 -0.31499293 -0.28930062 -0.25352085 -0.22444779 -0.18337619 -0.154104 -0.11564119][0.19504784 0.22787781 0.1807311 0.10792238 0.028426275 -0.059956633 -0.1151944 -0.1276781 -0.12555405 -0.13357583 -0.16226541 -0.20486245 -0.20908317 -0.19625002 -0.15842038][0.33186716 0.36570585 0.34649855 0.3168053 0.28129971 0.22856735 0.1900536 0.15352505 0.10741132 0.057333857 -0.0030854046 -0.084039859 -0.14645994 -0.18532816 -0.17596865][0.26542729 0.275329 0.23204277 0.22869222 0.22470205 0.20079304 0.17467488 0.13231714 0.081793889 0.020867951 -0.045201264 -0.11431376 -0.17158309 -0.20166335 -0.1929121][0.12807383 0.090748429 0.043429554 0.035829917 0.046373069 0.062617525 0.0680767 0.042589292 -0.015842974 -0.078203812 -0.13014585 -0.16225514 -0.19033536 -0.2009356 -0.19768184][-0.0066944137 -0.075544521 -0.12623224 -0.1438823 -0.1213378 -0.10285195 -0.091924965 -0.088006511 -0.11156016 -0.1487104 -0.19262788 -0.21272084 -0.22533849 -0.21445364 -0.19478706][-0.072815448 -0.13686222 -0.19804996 -0.24137074 -0.24276058 -0.22919489 -0.20551758 -0.19194457 -0.19260003 -0.1849505 -0.2077104 -0.21866247 -0.22675139 -0.21503821 -0.19504797][-0.10159439 -0.12976322 -0.15077771 -0.18014771 -0.18311034 -0.18400146 -0.17834315 -0.17503703 -0.17596674 -0.16431287 -0.17288825 -0.18757595 -0.19443133 -0.1877826 -0.17659475][-0.075195588 -0.11660139 -0.15707107 -0.18860875 -0.2049568 -0.19148836 -0.17808865 -0.17817292 -0.14711067 -0.12818985 -0.12301946 -0.14296602 -0.14631069 -0.14776066 -0.14147179][-0.0454319 -0.094535336 -0.11024643 -0.14572933 -0.18743712 -0.23341715 -0.23801674 -0.23455183 -0.20078397 -0.15987001 -0.11519432 -0.11150766 -0.094823785 -0.088019125 -0.084105834]]...]
INFO - root - 2017-12-06 07:08:05.569838: step 31910, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.633 sec/batch; 52h:53m:05s remains)
INFO - root - 2017-12-06 07:08:12.073570: step 31920, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.652 sec/batch; 54h:26m:02s remains)
INFO - root - 2017-12-06 07:08:18.583424: step 31930, loss = 0.79, batch loss = 0.57 (11.9 examples/sec; 0.670 sec/batch; 55h:58m:28s remains)
INFO - root - 2017-12-06 07:08:25.057910: step 31940, loss = 0.85, batch loss = 0.63 (12.7 examples/sec; 0.631 sec/batch; 52h:39m:38s remains)
INFO - root - 2017-12-06 07:08:31.508369: step 31950, loss = 0.74, batch loss = 0.52 (12.5 examples/sec; 0.638 sec/batch; 53h:17m:54s remains)
INFO - root - 2017-12-06 07:08:38.014189: step 31960, loss = 0.71, batch loss = 0.49 (12.3 examples/sec; 0.653 sec/batch; 54h:28m:45s remains)
INFO - root - 2017-12-06 07:08:44.346292: step 31970, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.655 sec/batch; 54h:38m:55s remains)
INFO - root - 2017-12-06 07:08:50.912257: step 31980, loss = 0.86, batch loss = 0.65 (12.2 examples/sec; 0.653 sec/batch; 54h:31m:10s remains)
INFO - root - 2017-12-06 07:08:57.252556: step 31990, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 54h:06m:27s remains)
INFO - root - 2017-12-06 07:09:03.802648: step 32000, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.670 sec/batch; 55h:56m:24s remains)
2017-12-06 07:09:04.452091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088517241 -0.084708989 -0.083779752 -0.080665872 -0.10277072 -0.14180112 -0.1713149 -0.20430782 -0.2463613 -0.28423548 -0.29930121 -0.27919057 -0.2362479 -0.18566789 -0.14292561][-0.094165176 -0.088449724 -0.084488869 -0.077235535 -0.10599414 -0.15553531 -0.21741366 -0.29748288 -0.40018159 -0.487825 -0.53417271 -0.49109623 -0.40281358 -0.29852533 -0.2073265][-0.10127819 -0.093457863 -0.098972805 -0.090201959 -0.11923713 -0.17059988 -0.25500947 -0.40042078 -0.58169389 -0.7400822 -0.82180047 -0.76514655 -0.62759131 -0.46453765 -0.31416756][-0.10255367 -0.090983182 -0.081917867 -0.049996987 -0.039650824 -0.049460266 -0.13337459 -0.37569314 -0.67447305 -0.92130303 -1.072968 -1.0403376 -0.87424 -0.65732539 -0.43652859][-0.095510788 -0.082112864 -0.030672722 0.044720784 0.11547144 0.19503319 0.08540982 -0.23381494 -0.62287915 -0.95103067 -1.1914177 -1.2110176 -1.0510597 -0.80531561 -0.52876371][-0.0703831 -0.039757315 0.039647639 0.16530713 0.24344584 0.3664912 0.36887029 0.07998696 -0.4051204 -0.8367812 -1.1751218 -1.2806323 -1.1498926 -0.88423526 -0.58237159][-0.061635025 -0.017706007 0.072015181 0.21795583 0.36124685 0.561478 0.64278185 0.47222382 0.04482384 -0.44532263 -0.9311136 -1.1727452 -1.124759 -0.87635857 -0.5729894][-0.066547036 -0.015819065 0.096700922 0.25316045 0.48232102 0.75737458 0.88802016 0.791439 0.4457761 0.0013515279 -0.53390741 -0.83952326 -0.88726497 -0.74952853 -0.5117439][-0.056872506 0.026319593 0.19394988 0.3974058 0.57189262 0.79105264 0.92994958 0.84763724 0.58182979 0.19379312 -0.25922066 -0.52266669 -0.58010674 -0.48952135 -0.35156333][-0.054876257 0.047283098 0.23626092 0.43535942 0.61556143 0.72677535 0.70139241 0.63799685 0.44825548 0.12394315 -0.16896085 -0.38131884 -0.42724442 -0.36178982 -0.26430893][-0.062907614 0.01876507 0.19645208 0.37574372 0.54305422 0.58937436 0.45580405 0.28005144 0.10440925 -0.090675883 -0.33472431 -0.47044948 -0.4206526 -0.34501573 -0.24941367][-0.064788744 0.015327558 0.18377215 0.35392135 0.46054286 0.441108 0.25847784 0.0079449415 -0.18490967 -0.32094735 -0.43626341 -0.49544936 -0.45295712 -0.35630381 -0.2319549][-0.082799338 -0.0094795525 0.14248809 0.28131008 0.37251422 0.33191457 0.12877879 -0.13311493 -0.27682507 -0.36868304 -0.44033635 -0.42952651 -0.36256218 -0.27151227 -0.18817633][-0.10730826 -0.079767928 0.02203349 0.19079125 0.32179034 0.31944749 0.16489223 -0.052335735 -0.26701498 -0.38633043 -0.46750537 -0.40879986 -0.323111 -0.21535215 -0.14884724][-0.10349417 -0.090391152 -0.0261283 0.12550536 0.26155594 0.29593322 0.22291827 -0.0021617338 -0.22751257 -0.34602758 -0.43732065 -0.38329631 -0.2702775 -0.19424272 -0.10494734]]...]
INFO - root - 2017-12-06 07:09:10.929974: step 32010, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.638 sec/batch; 53h:16m:44s remains)
INFO - root - 2017-12-06 07:09:17.486111: step 32020, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.654 sec/batch; 54h:32m:47s remains)
INFO - root - 2017-12-06 07:09:23.946813: step 32030, loss = 0.73, batch loss = 0.51 (12.2 examples/sec; 0.656 sec/batch; 54h:45m:46s remains)
INFO - root - 2017-12-06 07:09:30.409730: step 32040, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.640 sec/batch; 53h:25m:36s remains)
INFO - root - 2017-12-06 07:09:36.905913: step 32050, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.624 sec/batch; 52h:02m:24s remains)
INFO - root - 2017-12-06 07:09:43.260219: step 32060, loss = 0.76, batch loss = 0.55 (12.7 examples/sec; 0.630 sec/batch; 52h:32m:25s remains)
INFO - root - 2017-12-06 07:09:49.795411: step 32070, loss = 0.88, batch loss = 0.66 (12.2 examples/sec; 0.658 sec/batch; 54h:54m:30s remains)
INFO - root - 2017-12-06 07:09:56.337775: step 32080, loss = 0.73, batch loss = 0.52 (12.4 examples/sec; 0.647 sec/batch; 54h:01m:13s remains)
INFO - root - 2017-12-06 07:10:02.813317: step 32090, loss = 0.85, batch loss = 0.64 (12.1 examples/sec; 0.659 sec/batch; 54h:58m:43s remains)
INFO - root - 2017-12-06 07:10:09.405205: step 32100, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.644 sec/batch; 53h:42m:05s remains)
2017-12-06 07:10:10.039439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.35264644 -0.46129167 -0.5794996 -0.67758524 -0.73132193 -0.7389034 -0.75728106 -0.78716362 -0.83324349 -0.85930979 -0.81901979 -0.740571 -0.62426913 -0.47663105 -0.31609851][-0.289793 -0.31521252 -0.37954444 -0.45565709 -0.5397743 -0.59233141 -0.61841631 -0.65809417 -0.73457372 -0.82409084 -0.85202539 -0.82013547 -0.73061264 -0.6132561 -0.50162047][-0.3223947 -0.29258108 -0.27925566 -0.33145383 -0.43046492 -0.52795064 -0.61286509 -0.67321146 -0.75220728 -0.83829713 -0.8860116 -0.86652029 -0.77647674 -0.67543137 -0.6347506][-0.26270527 -0.24155279 -0.21340711 -0.20247966 -0.22860864 -0.33727723 -0.46125209 -0.60470784 -0.74841583 -0.82848835 -0.91387522 -0.95290589 -0.90137196 -0.7971046 -0.73901439][-0.17679888 -0.19653396 -0.18760338 -0.10821243 -0.037861411 -0.039307378 -0.10976253 -0.29348648 -0.53591549 -0.82242954 -1.0959332 -1.2344841 -1.2913465 -1.2369254 -1.1433417][0.010328472 -0.03436951 -0.053619083 -0.076419607 -0.067474388 0.019268751 0.068249196 0.029281341 -0.12761721 -0.42426553 -0.81392395 -1.191933 -1.4897276 -1.6135309 -1.6332991][0.37726209 0.336031 0.28360954 0.19765389 0.11735319 0.04134053 0.014733836 0.065513477 0.065005511 -0.030914892 -0.25550494 -0.68852866 -1.1597897 -1.5442377 -1.8516532][0.65887117 0.68841803 0.68882406 0.63057184 0.56092775 0.43349519 0.31078494 0.18811 0.10296507 0.087545067 0.004750751 -0.24681501 -0.54703963 -0.95359409 -1.3708094][0.73904133 0.80956209 0.86652803 0.9094007 0.91984141 0.85801673 0.77889669 0.63534081 0.48210725 0.35006246 0.23413667 0.13835254 -0.0076963976 -0.21152075 -0.42975062][0.53831255 0.64269578 0.730674 0.81567943 0.88437808 0.93216312 0.95826364 0.93129671 0.86615181 0.75375831 0.63865221 0.50184751 0.48649314 0.46663103 0.41769341][0.217639 0.28555992 0.36601871 0.47316065 0.5825659 0.66911864 0.76280093 0.84975469 0.9108423 0.94211888 0.94629443 0.93377757 0.8470999 0.85804212 0.92089438][-0.010714829 0.0095703527 0.03719838 0.12763542 0.22483858 0.32673323 0.43131062 0.53446949 0.64616048 0.76432121 0.88598657 1.0097141 1.1026288 1.1637727 1.2148836][-0.099391505 -0.11216898 -0.12633267 -0.087525561 -0.034027103 0.024318218 0.09369126 0.16984168 0.24292177 0.3455312 0.49401084 0.681239 0.86193073 1.0393994 1.1338347][-0.11895437 -0.120332 -0.12837663 -0.13136695 -0.13178241 -0.10892387 -0.079899848 -0.033987835 0.010921605 0.081326559 0.17335042 0.30046418 0.43619671 0.58175051 0.73330057][-0.14171782 -0.13332899 -0.13140643 -0.12669806 -0.1338145 -0.13160929 -0.12728231 -0.11601301 -0.10013813 -0.065802507 -0.014837407 0.072050586 0.16128781 0.24943751 0.31474274]]...]
INFO - root - 2017-12-06 07:10:16.541711: step 32110, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.653 sec/batch; 54h:29m:53s remains)
INFO - root - 2017-12-06 07:10:23.165128: step 32120, loss = 0.89, batch loss = 0.67 (12.2 examples/sec; 0.656 sec/batch; 54h:43m:31s remains)
INFO - root - 2017-12-06 07:10:29.519741: step 32130, loss = 0.83, batch loss = 0.62 (12.1 examples/sec; 0.660 sec/batch; 55h:01m:35s remains)
INFO - root - 2017-12-06 07:10:35.998189: step 32140, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.647 sec/batch; 53h:58m:00s remains)
INFO - root - 2017-12-06 07:10:42.540614: step 32150, loss = 0.83, batch loss = 0.61 (12.8 examples/sec; 0.624 sec/batch; 52h:05m:06s remains)
INFO - root - 2017-12-06 07:10:48.905190: step 32160, loss = 0.86, batch loss = 0.64 (13.3 examples/sec; 0.600 sec/batch; 50h:03m:58s remains)
INFO - root - 2017-12-06 07:10:55.406627: step 32170, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.638 sec/batch; 53h:13m:54s remains)
INFO - root - 2017-12-06 07:11:01.844169: step 32180, loss = 0.79, batch loss = 0.58 (12.9 examples/sec; 0.620 sec/batch; 51h:42m:03s remains)
INFO - root - 2017-12-06 07:11:08.341731: step 32190, loss = 0.75, batch loss = 0.53 (12.7 examples/sec; 0.631 sec/batch; 52h:39m:14s remains)
INFO - root - 2017-12-06 07:11:14.836848: step 32200, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.661 sec/batch; 55h:10m:16s remains)
2017-12-06 07:11:15.427189: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34926462 0.25969353 -0.05555534 -0.43600813 -0.83558381 -1.083124 -1.2046642 -1.2261293 -1.2358854 -1.1455986 -1.0064464 -0.84789348 -0.73121285 -0.63114721 -0.517598][0.35123485 0.42646697 0.25912136 -0.043942943 -0.46937221 -0.82249689 -1.1286093 -1.387095 -1.5463703 -1.5485529 -1.3751204 -1.1204464 -0.87166244 -0.67565477 -0.54273158][0.38069028 0.67060375 0.69015771 0.53570682 0.25440371 -0.098068267 -0.41681784 -0.74742991 -1.032919 -1.1738247 -1.1216949 -0.97326076 -0.78592533 -0.61176568 -0.48827484][0.2630938 0.66924423 0.87127382 0.918057 0.8599059 0.70231611 0.51155835 0.22559369 -0.1118238 -0.39643085 -0.538117 -0.52380359 -0.43415982 -0.304932 -0.18780108][-0.086780973 0.16161352 0.39755014 0.59031755 0.75026029 1.0026184 1.1384559 1.0974243 0.85100067 0.56311756 0.28101131 0.058175035 -0.075195067 -0.13726746 -0.10696419][-0.094900392 -0.025384806 0.092530556 0.29328829 0.63325846 1.1215544 1.5444671 1.7694324 1.6681405 1.346832 1.0043699 0.63936627 0.34794065 0.12700561 0.00635387][-0.052060485 -0.03171603 0.015191197 0.21080568 0.55424416 1.0744092 1.5898682 1.9741046 2.1039088 1.9415538 1.592718 1.1495353 0.71920145 0.35810715 0.11560678][-0.20341529 -0.1778129 -0.10861136 0.087234341 0.40098348 0.86020082 1.3078781 1.6791365 1.9030256 1.8919739 1.722024 1.401446 1.0043666 0.58862221 0.25667977][-0.31226805 -0.31692725 -0.28443244 -0.15495449 0.079450019 0.43371364 0.78666586 1.1022534 1.3409615 1.4376323 1.3644326 1.1795988 0.88838041 0.53149438 0.22053087][-0.38005286 -0.43684223 -0.47423285 -0.46449119 -0.3850815 -0.21177539 -0.011610426 0.24053857 0.49092677 0.67981416 0.72329909 0.63361347 0.507078 0.31441215 0.11725139][-0.30318809 -0.39989984 -0.51843864 -0.62889946 -0.70094013 -0.70565313 -0.64853644 -0.52236181 -0.35389769 -0.15691997 -0.056572612 0.00076965243 0.0078335106 -0.0072825775 -0.025578141][-0.187543 -0.2789354 -0.4144544 -0.5821175 -0.76432204 -0.90305692 -0.95806921 -0.92110783 -0.81294549 -0.66177237 -0.51690495 -0.38549086 -0.29592732 -0.21099991 -0.1317656][-0.1643227 -0.23246911 -0.33944681 -0.48642859 -0.66184884 -0.81930643 -0.91277933 -0.92197955 -0.84086126 -0.71223843 -0.56248319 -0.43793654 -0.34878302 -0.25507861 -0.14994836][-0.15037787 -0.18435074 -0.24044803 -0.31577781 -0.4168146 -0.52244014 -0.59894824 -0.62185711 -0.5817436 -0.49111024 -0.38437811 -0.30334187 -0.23443237 -0.19690892 -0.1440206][-0.13113634 -0.1431911 -0.161146 -0.18505433 -0.21884957 -0.25672439 -0.28894487 -0.30178696 -0.28737184 -0.25248498 -0.19730763 -0.16231182 -0.12567972 -0.10478535 -0.089476474]]...]
INFO - root - 2017-12-06 07:11:21.941316: step 32210, loss = 0.85, batch loss = 0.64 (12.5 examples/sec; 0.641 sec/batch; 53h:29m:52s remains)
INFO - root - 2017-12-06 07:11:28.514635: step 32220, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.632 sec/batch; 52h:45m:21s remains)
INFO - root - 2017-12-06 07:11:35.088498: step 32230, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.654 sec/batch; 54h:32m:29s remains)
INFO - root - 2017-12-06 07:11:41.610489: step 32240, loss = 0.88, batch loss = 0.66 (12.1 examples/sec; 0.663 sec/batch; 55h:17m:35s remains)
INFO - root - 2017-12-06 07:11:47.895252: step 32250, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.634 sec/batch; 52h:54m:36s remains)
INFO - root - 2017-12-06 07:11:54.357735: step 32260, loss = 0.84, batch loss = 0.62 (12.4 examples/sec; 0.644 sec/batch; 53h:44m:42s remains)
INFO - root - 2017-12-06 07:12:00.718421: step 32270, loss = 0.85, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 53h:34m:31s remains)
INFO - root - 2017-12-06 07:12:07.191101: step 32280, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.650 sec/batch; 54h:11m:56s remains)
INFO - root - 2017-12-06 07:12:13.775417: step 32290, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.642 sec/batch; 53h:30m:59s remains)
INFO - root - 2017-12-06 07:12:20.262646: step 32300, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.657 sec/batch; 54h:49m:00s remains)
2017-12-06 07:12:20.966017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.8249312 -0.76480615 -0.64401191 -0.53090847 -0.43314829 -0.37601584 -0.32187831 -0.12036861 0.064624988 0.28242743 0.41969955 0.49827468 0.447492 0.38999033 0.35558248][-0.84502769 -0.69289446 -0.60282028 -0.54575235 -0.50369519 -0.38861871 -0.33183745 -0.33316082 -0.26686963 -0.06439203 0.088905312 0.18138266 0.17963567 0.17901465 0.3414633][-0.93498164 -0.63735944 -0.48526913 -0.57678992 -0.6819768 -0.8447631 -0.92416811 -0.902702 -0.80832958 -0.64900458 -0.519845 -0.46868992 -0.4848046 -0.47157079 -0.30943263][-1.0429338 -0.85051554 -0.61403739 -0.44023478 -0.50370854 -0.60109067 -0.81192583 -0.91686761 -0.9411341 -0.82596463 -0.719024 -0.59545189 -0.61055213 -0.68182206 -0.66926157][-0.87329131 -0.61700493 -0.36862132 -0.16297941 -0.11929445 -0.1694254 -0.41477686 -0.59017056 -0.73065078 -0.71041495 -0.692893 -0.69821781 -0.75716043 -0.74179047 -0.763158][-0.57812625 -0.45436081 -0.30047122 0.0080262572 0.20747751 0.26982793 0.16781041 -0.017810397 -0.20793486 -0.30494764 -0.41986215 -0.43658856 -0.56922454 -0.81731218 -0.87387222][-0.43561736 -0.24128252 0.035521977 0.2930747 0.61357564 0.78822941 0.798198 0.65521389 0.4697507 0.29543728 0.06591206 -0.19455278 -0.30271149 -0.42483088 -0.5202623][-0.48807302 -0.4287158 -0.11064272 0.24609071 0.67568588 1.0159545 1.1679661 1.1096351 0.94839257 0.71649611 0.40810657 0.06517259 -0.17992543 -0.40739775 -0.41013348][-0.68496162 -0.53414237 -0.33733994 0.03328979 0.43699384 0.72824109 0.88530934 0.89377928 0.77190554 0.58776146 0.42413825 0.12111302 -0.12624836 -0.2724801 -0.3563019][-0.58662874 -0.59201461 -0.45918906 -0.06378597 0.32675233 0.56423748 0.68864447 0.60487819 0.40706941 0.1912066 0.029819779 -0.068923436 -0.083784096 -0.23630586 -0.24351537][-0.4737266 -0.49902287 -0.4378573 -0.20362836 0.18439129 0.49874258 0.58788961 0.51205325 0.30786017 0.034788929 -0.12229755 -0.27313083 -0.35931802 -0.33160731 -0.30256009][-0.48963079 -0.61539435 -0.63027763 -0.37913615 -0.04805604 0.27067077 0.4412722 0.3646341 0.17764172 -0.056557305 -0.27717987 -0.37945467 -0.4735823 -0.494236 -0.44428191][-0.66539079 -0.66620183 -0.58392072 -0.46209514 -0.28710586 0.029741712 0.21876398 0.29938096 0.22343916 -0.049311236 -0.26699969 -0.43971905 -0.56747228 -0.56630576 -0.52161872][-0.52190107 -0.77153826 -0.73079008 -0.53575081 -0.17094316 0.0049108118 0.090559132 0.11606056 0.03507065 0.014478207 -0.14450994 -0.32472888 -0.38317698 -0.52482283 -0.53454685][-0.42674005 -0.52802479 -0.5811134 -0.51723534 -0.26346955 0.026225172 0.17008156 0.14743859 0.080480181 -0.069234453 -0.18088664 -0.34459314 -0.47813585 -0.46369642 -0.46785739]]...]
INFO - root - 2017-12-06 07:12:27.448891: step 32310, loss = 0.80, batch loss = 0.59 (11.7 examples/sec; 0.686 sec/batch; 57h:12m:17s remains)
INFO - root - 2017-12-06 07:12:33.895855: step 32320, loss = 0.94, batch loss = 0.73 (12.3 examples/sec; 0.649 sec/batch; 54h:07m:06s remains)
INFO - root - 2017-12-06 07:12:40.398083: step 32330, loss = 0.87, batch loss = 0.66 (11.4 examples/sec; 0.701 sec/batch; 58h:26m:57s remains)
INFO - root - 2017-12-06 07:12:46.807848: step 32340, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.639 sec/batch; 53h:18m:13s remains)
INFO - root - 2017-12-06 07:12:53.274488: step 32350, loss = 0.73, batch loss = 0.52 (12.4 examples/sec; 0.646 sec/batch; 53h:52m:04s remains)
INFO - root - 2017-12-06 07:12:59.831412: step 32360, loss = 0.76, batch loss = 0.55 (12.1 examples/sec; 0.662 sec/batch; 55h:12m:39s remains)
INFO - root - 2017-12-06 07:13:06.398004: step 32370, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 53h:57m:36s remains)
INFO - root - 2017-12-06 07:13:12.861656: step 32380, loss = 0.74, batch loss = 0.53 (12.5 examples/sec; 0.641 sec/batch; 53h:24m:53s remains)
INFO - root - 2017-12-06 07:13:19.343741: step 32390, loss = 0.82, batch loss = 0.61 (12.7 examples/sec; 0.629 sec/batch; 52h:26m:41s remains)
INFO - root - 2017-12-06 07:13:25.803636: step 32400, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.643 sec/batch; 53h:35m:16s remains)
2017-12-06 07:13:26.562647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.2854236 -0.89496785 -0.47365546 -0.76681364 -0.82905978 -0.65155715 -0.89783829 -1.229177 -1.4268593 -1.4310807 -1.2651587 -1.0800486 -0.59079319 0.10000483 0.68957633][-0.38239038 -0.2150735 -0.18830056 0.036256708 0.27508914 -0.030898027 -0.47071475 -0.56650621 -0.7759276 -0.88508904 -1.098946 -0.7942856 -0.56964004 -0.20155215 0.03442084][-0.5959354 -0.34952706 -0.077114888 -0.1078422 -0.007356219 0.012845717 -0.18386316 -0.49817327 -0.90545845 -1.2504748 -1.3477383 -0.83938527 -0.410355 0.53988129 0.79902762][-0.21835452 -0.065519646 -0.097458966 -0.0086505264 0.12165058 0.013659544 -0.11626444 -0.35960287 -0.30252054 -0.47313857 -0.54293448 -0.60907453 0.0089100748 0.75155884 0.88254452][0.17274943 0.48119277 0.93688256 0.96208233 0.624811 0.38931668 0.4752875 0.2951991 0.23306027 0.042900346 -0.068262659 -0.13333952 0.6441853 1.1963853 1.5408478][0.52241147 1.2097993 1.5167103 1.8473055 1.9120766 1.5718334 1.0621603 0.87687945 1.0323248 1.005482 0.95472378 0.58438468 0.98829752 1.4249159 1.6838273][1.0768114 1.6282252 1.9723433 2.3634794 2.4614341 2.6116819 2.5202808 2.0423391 1.7397935 1.6480693 1.8035126 1.7878231 1.8720124 2.0385954 2.2870338][1.807879 2.190021 2.2838204 2.7587738 2.8682003 2.6461637 2.440979 2.2834048 2.045501 2.0589859 1.9206997 1.9415988 1.7863634 1.7885853 1.7808659][1.9699515 1.9224597 1.6014317 1.5557296 1.4460615 1.5388955 1.3328739 1.0205014 0.75837374 0.74761224 0.73663616 0.80119586 0.620594 0.52641916 0.42411536][1.5375314 1.3525229 0.85102022 0.4557237 0.10206246 -0.16331705 -0.2478435 -0.035919588 0.13171452 -0.098457649 -0.11597707 0.017927617 -0.30000091 -0.31492245 -0.50754184][0.93652517 0.4704591 -0.022264741 -0.66239017 -1.1022714 -1.1905648 -1.2113762 -1.2450215 -1.1503789 -0.92163724 -0.78022659 -0.82731241 -1.113584 -1.2782989 -1.1767825][0.22585565 -0.0014926791 -0.50267708 -0.9109934 -1.3771999 -1.7922833 -1.9792823 -1.8925084 -1.8109523 -1.7395937 -1.5888762 -1.3434098 -1.1287998 -1.441708 -1.5888656][0.4743911 0.1677511 -0.35535458 -0.75699246 -1.1538001 -1.3823142 -1.4071376 -1.4895937 -1.4491863 -1.336153 -1.1506476 -0.90020651 -0.73972684 -0.562761 -0.37750265][0.96186036 0.879303 0.65677446 0.097687863 -0.17377105 -0.37363797 -0.35038406 -0.18895328 -0.15684862 -0.14465757 -0.028487049 0.22476962 0.6517207 0.65563333 0.92136115][1.5010124 1.5435882 1.4395027 1.3078178 1.2890117 1.1258805 1.1859777 1.2199529 1.2512928 1.2580161 1.2802601 1.418278 1.688836 1.8286127 1.7885548]]...]
INFO - root - 2017-12-06 07:13:32.934716: step 32410, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 53h:58m:18s remains)
INFO - root - 2017-12-06 07:13:39.490215: step 32420, loss = 0.86, batch loss = 0.64 (11.8 examples/sec; 0.676 sec/batch; 56h:22m:02s remains)
INFO - root - 2017-12-06 07:13:45.786425: step 32430, loss = 0.80, batch loss = 0.59 (13.4 examples/sec; 0.599 sec/batch; 49h:54m:24s remains)
INFO - root - 2017-12-06 07:13:52.321775: step 32440, loss = 0.70, batch loss = 0.48 (12.2 examples/sec; 0.655 sec/batch; 54h:36m:17s remains)
INFO - root - 2017-12-06 07:13:58.776834: step 32450, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.654 sec/batch; 54h:32m:41s remains)
INFO - root - 2017-12-06 07:14:05.294317: step 32460, loss = 0.84, batch loss = 0.63 (12.0 examples/sec; 0.668 sec/batch; 55h:41m:59s remains)
INFO - root - 2017-12-06 07:14:11.808781: step 32470, loss = 0.87, batch loss = 0.66 (11.8 examples/sec; 0.678 sec/batch; 56h:29m:52s remains)
INFO - root - 2017-12-06 07:14:18.282108: step 32480, loss = 0.75, batch loss = 0.53 (12.3 examples/sec; 0.652 sec/batch; 54h:22m:07s remains)
INFO - root - 2017-12-06 07:14:24.760555: step 32490, loss = 0.83, batch loss = 0.61 (12.6 examples/sec; 0.637 sec/batch; 53h:06m:24s remains)
INFO - root - 2017-12-06 07:14:31.257244: step 32500, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 53h:30m:28s remains)
2017-12-06 07:14:31.913573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058905583 -0.12376598 0.17538628 0.20696369 0.15402734 0.22432631 0.18315002 -0.032366171 -0.10538115 -0.13934447 -0.34758493 -0.38845381 -0.33230233 -0.26051557 -0.29143271][0.30998963 0.60655904 0.75806147 0.94820881 1.054358 1.0121881 0.95234013 0.91729 0.74170971 0.66639119 0.58628416 0.48990232 0.41904557 0.37839529 0.082723424][-0.29769588 0.019157149 0.32614711 0.48526412 0.54527342 0.63831574 0.53128004 0.55954134 0.54550105 0.64509147 0.67103744 0.59416419 0.58057195 0.58362228 0.45506805][-0.055082932 -0.08552292 -0.099412158 0.1237634 0.086293668 0.038133174 0.05521971 0.051481515 -0.039910618 0.0023260638 0.085769504 0.32590106 0.4529925 0.44978607 0.4597314][-0.11913328 -0.050215773 -0.13434181 -0.23240666 -0.23011139 -0.30556571 -0.1791794 -0.061366335 -0.0037553906 0.024513781 -0.042922989 0.017405771 0.16964585 0.34222209 0.31877676][0.43586379 0.3503294 0.11513387 0.055087253 -0.0050025284 -0.090853155 -0.06197311 -0.013518021 0.11966002 0.080146179 0.040141374 -0.14002308 -0.15931663 -0.090109065 -0.12360962][0.7686134 0.78279388 0.71960419 0.62001765 0.72573912 0.80523521 0.83529687 0.81340027 0.83731478 0.81920487 0.66260421 0.510294 0.33298722 0.20651251 0.075295374][1.2114336 1.2912056 1.2166011 1.3045561 1.386026 1.4964796 1.7042141 1.6109108 1.4084625 1.2831395 1.0450774 0.84605765 0.56129962 0.49034047 0.4396717][1.3717543 1.4659147 1.3160977 1.3870617 1.5514987 1.600826 1.5966252 1.4533669 1.2452047 0.96147203 0.615193 0.32682595 0.04696326 -0.10415998 -0.17254549][1.1731248 1.2502098 1.1796714 1.179211 1.1857269 1.2411202 1.1769806 1.1082586 0.97222233 0.71886533 0.34917769 0.033396877 -0.29884267 -0.39076087 -0.45239037][0.74839979 0.67384958 0.62496269 0.59079742 0.56293476 0.59920883 0.60186374 0.58089989 0.39975426 0.23478222 0.015045822 -0.10617097 -0.29432684 -0.32662833 -0.31957912][0.32610103 0.19486389 -0.0040354952 -0.14957377 -0.2738148 -0.31150624 -0.32861996 -0.34758583 -0.33658597 -0.27439943 -0.39659 -0.31438667 -0.31602997 -0.23325782 -0.15186709][-0.40729451 -0.48779348 -0.4795056 -0.6297003 -0.677807 -0.76160717 -0.72671241 -0.67038292 -0.6861071 -0.60011995 -0.4964112 -0.32107759 -0.289734 -0.042107489 0.032959104][-0.55688381 -0.8139962 -0.81423485 -0.85513866 -0.86522561 -0.89526021 -0.88516283 -0.77226108 -0.65493065 -0.44395906 -0.39278388 -0.17691362 -0.13440633 0.016305357 0.11615905][-0.41976914 -0.71195823 -0.90809995 -0.95902658 -0.88529533 -0.8035593 -0.70576453 -0.55333889 -0.387507 -0.2056019 -0.1223453 -0.015065618 -0.0065150633 0.10378644 0.048423931]]...]
INFO - root - 2017-12-06 07:14:38.488669: step 32510, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.661 sec/batch; 55h:06m:48s remains)
INFO - root - 2017-12-06 07:14:44.934355: step 32520, loss = 0.82, batch loss = 0.60 (12.6 examples/sec; 0.636 sec/batch; 52h:59m:58s remains)
INFO - root - 2017-12-06 07:14:51.316802: step 32530, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.645 sec/batch; 53h:47m:00s remains)
INFO - root - 2017-12-06 07:14:57.866953: step 32540, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.642 sec/batch; 53h:31m:30s remains)
INFO - root - 2017-12-06 07:15:04.155632: step 32550, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.643 sec/batch; 53h:34m:19s remains)
INFO - root - 2017-12-06 07:15:10.709315: step 32560, loss = 0.73, batch loss = 0.52 (12.9 examples/sec; 0.621 sec/batch; 51h:45m:54s remains)
INFO - root - 2017-12-06 07:15:17.244516: step 32570, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.650 sec/batch; 54h:10m:18s remains)
INFO - root - 2017-12-06 07:15:23.698517: step 32580, loss = 1.03, batch loss = 0.81 (12.2 examples/sec; 0.658 sec/batch; 54h:47m:05s remains)
INFO - root - 2017-12-06 07:15:30.237091: step 32590, loss = 0.79, batch loss = 0.57 (12.7 examples/sec; 0.632 sec/batch; 52h:36m:47s remains)
INFO - root - 2017-12-06 07:15:36.743015: step 32600, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.654 sec/batch; 54h:31m:22s remains)
2017-12-06 07:15:37.336824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066567339 -0.066378839 -0.066377133 -0.066374496 -0.066377223 -0.066382527 -0.066393241 -0.0664283 -0.066516124 -0.06663093 -0.066729173 -0.066774428 -0.066786833 -0.066770069 -0.06680537][-0.066473894 -0.066227041 -0.066234812 -0.066241562 -0.066246711 -0.066254787 -0.066277176 -0.066355519 -0.066536248 -0.066770025 -0.066989809 -0.0670972 -0.0671111 -0.067047514 -0.066967517][-0.066348858 -0.066020451 -0.066059366 -0.066090494 -0.066099882 -0.066098765 -0.066133581 -0.066282764 -0.066589676 -0.066971436 -0.067340687 -0.067553774 -0.067576088 -0.06743332 -0.06718263][-0.066150531 -0.065858781 -0.065985277 -0.066056296 -0.066075906 -0.066060081 -0.066103593 -0.066328578 -0.066773728 -0.067351744 -0.067900918 -0.068241037 -0.068265438 -0.068025224 -0.067665584][-0.065846734 -0.065707833 -0.065982975 -0.06615968 -0.066221058 -0.066178828 -0.066207193 -0.066477045 -0.067067973 -0.067911722 -0.068720594 -0.069231525 -0.069281809 -0.06890218 -0.06836281][-0.065594032 -0.065544054 -0.065967135 -0.066282973 -0.066448554 -0.066431575 -0.066452608 -0.066678964 -0.067302667 -0.068352655 -0.06943278 -0.070133895 -0.070212394 -0.069709435 -0.069003329][-0.065570042 -0.065527432 -0.066070169 -0.066514254 -0.066818058 -0.066902541 -0.066934004 -0.067085549 -0.067593485 -0.068554908 -0.069667242 -0.070430815 -0.07058356 -0.070119686 -0.06942679][-0.065755934 -0.065712236 -0.06635569 -0.066887885 -0.067266539 -0.067415386 -0.067462265 -0.06756036 -0.067952827 -0.068715088 -0.069653876 -0.070358083 -0.070587747 -0.070229672 -0.06966912][-0.065879732 -0.06583754 -0.066519946 -0.06708651 -0.067473926 -0.0676394 -0.067679375 -0.067706004 -0.067941979 -0.0684951 -0.069222569 -0.069822051 -0.070082396 -0.069837086 -0.069441758][-0.065823346 -0.065719374 -0.066301 -0.066790417 -0.067134008 -0.067303307 -0.067337081 -0.067273676 -0.067337282 -0.067678183 -0.068219319 -0.06867668 -0.068934947 -0.068869881 -0.068731889][-0.065765813 -0.065542333 -0.065987825 -0.066366307 -0.066649944 -0.066807643 -0.066814363 -0.066704482 -0.066651791 -0.066747688 -0.067015812 -0.067329608 -0.06757319 -0.067680515 -0.06782][-0.065677941 -0.065377615 -0.065680228 -0.06592349 -0.066095695 -0.06620574 -0.066211626 -0.066111 -0.066013455 -0.065977141 -0.066084512 -0.066309094 -0.066544026 -0.066722393 -0.067065381][-0.065495782 -0.065132841 -0.06532225 -0.065465033 -0.065550916 -0.065607317 -0.06562908 -0.065571964 -0.065493427 -0.065453455 -0.065508127 -0.065688752 -0.065921739 -0.066130191 -0.066532783][-0.065341577 -0.0648907 -0.06498152 -0.065052018 -0.065094188 -0.06512028 -0.065132007 -0.0651056 -0.065076366 -0.065071627 -0.065128759 -0.065298788 -0.065551937 -0.065815672 -0.066232279][-0.065361291 -0.064833112 -0.064855956 -0.064871043 -0.064879954 -0.064883277 -0.064901687 -0.064899154 -0.0648998 -0.064918779 -0.064998813 -0.065179951 -0.065444671 -0.065743327 -0.066133395]]...]
INFO - root - 2017-12-06 07:15:43.794590: step 32610, loss = 0.83, batch loss = 0.62 (12.1 examples/sec; 0.662 sec/batch; 55h:08m:29s remains)
INFO - root - 2017-12-06 07:15:50.162617: step 32620, loss = 0.79, batch loss = 0.57 (12.4 examples/sec; 0.644 sec/batch; 53h:39m:13s remains)
INFO - root - 2017-12-06 07:15:56.659084: step 32630, loss = 0.91, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 53h:25m:00s remains)
INFO - root - 2017-12-06 07:16:03.206697: step 32640, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.647 sec/batch; 53h:54m:28s remains)
INFO - root - 2017-12-06 07:16:09.709006: step 32650, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 53h:30m:24s remains)
INFO - root - 2017-12-06 07:16:16.224542: step 32660, loss = 0.74, batch loss = 0.53 (12.6 examples/sec; 0.635 sec/batch; 52h:53m:26s remains)
INFO - root - 2017-12-06 07:16:22.737643: step 32670, loss = 0.84, batch loss = 0.62 (12.1 examples/sec; 0.661 sec/batch; 55h:04m:33s remains)
INFO - root - 2017-12-06 07:16:29.232869: step 32680, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.655 sec/batch; 54h:33m:51s remains)
INFO - root - 2017-12-06 07:16:35.665943: step 32690, loss = 0.88, batch loss = 0.67 (15.5 examples/sec; 0.516 sec/batch; 42h:56m:30s remains)
INFO - root - 2017-12-06 07:16:42.141785: step 32700, loss = 0.78, batch loss = 0.56 (12.7 examples/sec; 0.632 sec/batch; 52h:35m:43s remains)
2017-12-06 07:16:42.753319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0086458474 0.020872265 -0.03818344 -0.10238741 -0.19404963 -0.30528069 -0.41668689 -0.32508212 -0.35824972 -0.4310638 -0.41518062 -0.34421396 -0.12296177 0.11573479 0.27064291][0.14365229 0.19305983 0.096815363 -0.1811485 -0.37555796 -0.33501369 -0.31787765 -0.38155755 -0.54992843 -0.53850687 -0.49670035 -0.27863055 -0.0074755922 0.24554971 0.44455069][-0.6057694 -0.52122092 -0.61032212 -0.7165857 -0.81980652 -0.95550728 -0.94832838 -0.96165991 -0.97329354 -1.051976 -1.0766656 -0.89278841 -0.5988012 -0.1941101 0.12115203][-0.50443363 -0.58493835 -0.62877917 -0.74031007 -0.86883348 -0.86756051 -0.81121421 -0.804324 -0.74711347 -0.84106612 -0.87459964 -0.84995544 -0.71257293 -0.45993149 -0.17240384][-0.51568913 -0.54059887 -0.40585431 -0.48537514 -0.56421131 -0.58999372 -0.51296985 -0.47069541 -0.45250013 -0.596075 -0.73335373 -0.84332615 -0.78807 -0.5989486 -0.32123691][-0.50038964 -0.47728017 -0.34369016 -0.26732755 -0.24147759 -0.15962192 -0.025410071 -0.06054708 -0.096365005 -0.27333069 -0.39370996 -0.5530718 -0.5313611 -0.40682575 -0.19954795][-0.23351437 -0.19956426 -0.066256464 -0.00753475 0.049013063 0.18043646 0.28844392 0.30624527 0.265792 0.049445257 -0.095466934 -0.31072927 -0.35576943 -0.32848346 -0.20430148][0.067798093 0.058589876 0.12014641 0.28677925 0.411238 0.50429696 0.59174484 0.62400484 0.45155632 0.17216924 -0.02564431 -0.27306661 -0.32090655 -0.31923795 -0.25349903][0.12447333 0.1327178 0.15993658 0.19949126 0.3460632 0.56300217 0.62660372 0.60430038 0.52491963 0.35128346 0.08218497 -0.14946494 -0.28980023 -0.36143306 -0.33103013][0.034890234 0.072528467 0.11508018 0.14691886 0.31805751 0.47512686 0.58996755 0.5995869 0.49871576 0.3761186 0.21763331 -0.042634543 -0.26116329 -0.39937052 -0.43730563][-0.20898926 -0.128498 -0.074742988 -0.08902365 0.052834556 0.17007396 0.29946589 0.33618659 0.2420918 0.17997125 0.047423095 -0.033445042 -0.18400043 -0.28942165 -0.39553505][-0.38605344 -0.25176853 -0.20158933 -0.21707727 -0.1355467 -0.098103687 0.021014594 0.077948943 0.09316507 0.10992002 0.10202019 0.019578241 -0.083540551 -0.14701667 -0.20912972][-0.3777734 -0.3046096 -0.25372529 -0.20778504 -0.18960804 -0.17144015 -0.10140786 -0.038725436 -0.020184517 -0.019052997 0.034500286 0.070032641 0.013045236 -0.0853932 -0.14319065][-0.25809377 -0.14914298 -0.18042175 -0.18143009 -0.21162656 -0.18650773 -0.15546407 -0.16161102 -0.13902739 -0.088794909 -0.029207271 -0.044438954 -0.010455579 -0.026745446 -0.09145014][-0.092257045 0.010204852 -0.0024524257 -0.0316856 -0.065748468 -0.087518409 -0.071493693 -0.065776169 -0.078547113 -0.096909672 -0.13845626 -0.071456984 -0.069762535 -0.12447365 -0.11605152]]...]
INFO - root - 2017-12-06 07:16:49.139705: step 32710, loss = 0.83, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 44h:31m:34s remains)
INFO - root - 2017-12-06 07:16:55.617880: step 32720, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 54h:16m:47s remains)
INFO - root - 2017-12-06 07:17:02.086072: step 32730, loss = 0.80, batch loss = 0.58 (12.8 examples/sec; 0.623 sec/batch; 51h:54m:32s remains)
INFO - root - 2017-12-06 07:17:08.509670: step 32740, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.658 sec/batch; 54h:48m:29s remains)
INFO - root - 2017-12-06 07:17:14.996877: step 32750, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.631 sec/batch; 52h:34m:33s remains)
INFO - root - 2017-12-06 07:17:21.470890: step 32760, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.630 sec/batch; 52h:28m:11s remains)
INFO - root - 2017-12-06 07:17:27.983231: step 32770, loss = 0.73, batch loss = 0.52 (12.0 examples/sec; 0.669 sec/batch; 55h:40m:40s remains)
INFO - root - 2017-12-06 07:17:34.467408: step 32780, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.630 sec/batch; 52h:26m:44s remains)
INFO - root - 2017-12-06 07:17:41.020923: step 32790, loss = 0.76, batch loss = 0.55 (12.1 examples/sec; 0.662 sec/batch; 55h:04m:30s remains)
INFO - root - 2017-12-06 07:17:47.545267: step 32800, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.632 sec/batch; 52h:35m:02s remains)
2017-12-06 07:17:48.195027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.39055979 -0.48252377 -0.54349524 -0.56231922 -0.5676378 -0.56310916 -0.55163807 -0.56434357 -0.59399217 -0.65127164 -0.68830454 -0.74202651 -0.76527721 -0.72194868 -0.64917904][-0.5430516 -0.63652062 -0.74351943 -0.84116405 -0.8945958 -0.86148971 -0.81294042 -0.77059323 -0.75242484 -0.80251658 -0.85436559 -0.88477975 -0.87805736 -0.84711337 -0.771703][-0.56301683 -0.63434589 -0.74731481 -0.8899737 -0.9695791 -0.97867477 -0.98105973 -0.93662804 -0.88495129 -0.86978489 -0.88383907 -0.95310831 -1.0294119 -1.0165948 -0.96281797][-0.36578268 -0.43428463 -0.54028469 -0.68471116 -0.77657521 -0.7947889 -0.80017579 -0.80535108 -0.832026 -0.83956504 -0.84942758 -0.89779365 -0.98092979 -0.99316233 -1.0352347][-0.14997765 -0.19763272 -0.2981931 -0.40883338 -0.48385417 -0.50559914 -0.50897962 -0.5160979 -0.54113257 -0.59192771 -0.67286372 -0.761615 -0.907438 -0.95075583 -1.0526209][0.013838366 -0.017796889 -0.035699431 -0.075663716 -0.13312341 -0.15823361 -0.16721155 -0.17141826 -0.17034137 -0.22530833 -0.31802604 -0.43163157 -0.62841535 -0.75963336 -0.98043931][0.11928593 0.10095736 0.082468383 0.085452817 0.083987512 0.0652178 0.074387275 0.0850743 0.11110757 0.10107709 0.07683111 0.044909336 -0.041270722 -0.20002159 -0.49417537][-0.025896996 -0.060048692 -0.089640409 -0.10481002 -0.091238379 -0.094461955 -0.038847331 0.0044115484 0.083375759 0.13096926 0.18104887 0.24014831 0.36004251 0.31895816 0.163259][-0.12381591 -0.19301905 -0.25656131 -0.24296305 -0.2402873 -0.20932955 -0.13851854 -0.058300193 0.079189382 0.19509566 0.32591793 0.38508314 0.57255685 0.62409157 0.563863][-0.10569896 -0.19811247 -0.23579353 -0.21534953 -0.22924474 -0.20031694 -0.16041684 -0.091335319 0.03892266 0.16753942 0.31348759 0.42264146 0.6596157 0.74113959 0.75444758][-0.10556139 -0.19982913 -0.18647221 -0.128047 -0.10554277 -0.079852052 -0.040297132 -0.03524404 0.036385216 0.098999135 0.18722278 0.28252074 0.42704183 0.61445338 0.716149][-0.087525941 -0.13485569 -0.096619077 -0.071326092 -0.046501257 -0.009259522 0.015062459 0.0034767166 0.021502413 -0.008274287 -0.032411527 -0.025659695 0.0066163763 0.13848457 0.23381007][0.0094449893 -0.056657966 -0.022714891 0.0045513138 0.058011629 0.068048112 0.0718453 0.013369694 -0.034266576 -0.13737759 -0.270811 -0.35876051 -0.45313358 -0.44766706 -0.38873714][0.071363859 0.024513654 0.03395734 0.010797277 0.036305942 0.019245625 0.013230458 -0.09674444 -0.2366311 -0.3828522 -0.54507095 -0.67061669 -0.79251432 -0.82099777 -0.7818889][0.16906422 0.046446435 -0.0078423992 -0.063745424 -0.094532028 -0.13607043 -0.18411633 -0.27029988 -0.37501457 -0.47933677 -0.61088353 -0.71170789 -0.80892712 -0.85904723 -0.85456663]]...]
INFO - root - 2017-12-06 07:17:54.526063: step 32810, loss = 0.79, batch loss = 0.58 (12.8 examples/sec; 0.625 sec/batch; 52h:01m:10s remains)
INFO - root - 2017-12-06 07:18:01.106117: step 32820, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.652 sec/batch; 54h:15m:50s remains)
INFO - root - 2017-12-06 07:18:07.491975: step 32830, loss = 0.90, batch loss = 0.69 (13.5 examples/sec; 0.594 sec/batch; 49h:25m:04s remains)
INFO - root - 2017-12-06 07:18:13.898774: step 32840, loss = 0.85, batch loss = 0.63 (12.1 examples/sec; 0.658 sec/batch; 54h:48m:46s remains)
INFO - root - 2017-12-06 07:18:20.327156: step 32850, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.639 sec/batch; 53h:12m:20s remains)
INFO - root - 2017-12-06 07:18:26.846617: step 32860, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.655 sec/batch; 54h:31m:45s remains)
INFO - root - 2017-12-06 07:18:33.369053: step 32870, loss = 0.83, batch loss = 0.62 (11.9 examples/sec; 0.671 sec/batch; 55h:50m:55s remains)
INFO - root - 2017-12-06 07:18:39.816584: step 32880, loss = 0.76, batch loss = 0.55 (12.3 examples/sec; 0.648 sec/batch; 53h:55m:20s remains)
INFO - root - 2017-12-06 07:18:46.321912: step 32890, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.644 sec/batch; 53h:35m:56s remains)
INFO - root - 2017-12-06 07:18:52.714840: step 32900, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.640 sec/batch; 53h:17m:21s remains)
2017-12-06 07:18:53.403876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8396963 -1.9722723 -1.9036285 -1.9511513 -1.8890811 -1.7958248 -1.6095281 -1.5373073 -1.3978673 -1.2947814 -1.2139865 -1.1872171 -1.1817805 -1.1857822 -1.2270945][-2.2020566 -2.4802935 -2.7389092 -2.7235215 -2.7485142 -2.7943566 -2.6361341 -2.40277 -2.0719404 -1.9589552 -1.7502152 -1.5607083 -1.4154577 -1.241272 -1.0907621][-2.1790552 -2.5276656 -2.8835962 -3.2970138 -3.6003339 -3.5359476 -3.4773321 -3.53397 -3.3079028 -2.9837587 -2.6869252 -2.4726026 -2.1373997 -1.7994688 -1.5174646][-1.2765992 -2.0104771 -2.6077445 -3.0658584 -3.509208 -3.908251 -4.1760082 -4.235085 -4.1219239 -4.0179 -3.828434 -3.490196 -3.12787 -2.7381952 -2.195435][0.10554076 -0.45352876 -1.3045866 -2.0938845 -2.6580865 -2.989907 -3.3043265 -3.6412034 -3.8537695 -3.7942398 -3.7137792 -3.6648717 -3.4584954 -3.1008847 -2.8089871][0.67282712 0.23306319 -0.15906319 -0.46747535 -0.8843202 -1.3027375 -1.6118263 -1.7229753 -1.8934801 -1.9956735 -2.0891607 -2.0837712 -2.1092265 -2.2057986 -2.1214185][0.78149229 0.82865047 0.85971522 0.78976166 0.85207784 0.66425043 0.38307276 0.26625013 0.25766426 0.3520987 0.2804361 0.085304014 -0.1827566 -0.34615421 -0.48100585][0.38695279 0.45556426 0.61581504 0.8198238 1.0375812 1.2243583 1.4813182 1.7099228 1.8839746 1.9029403 1.921347 1.8264853 1.4980263 1.0030048 0.51839024][-0.20138833 -0.1661053 0.14378166 0.31608552 0.46335882 0.81766248 1.3221322 1.7923644 2.2964063 2.7090263 2.9872441 2.9217322 2.8191526 2.4782166 1.9928774][-1.0608013 -0.95627892 -0.49582121 -0.083667621 0.20140466 0.37756458 0.62181258 1.0066912 1.5271837 1.9791535 2.4178352 2.7628286 3.0117338 2.9979312 2.900141][-1.5048815 -1.7058793 -1.6529315 -1.4120728 -0.91033924 -0.570375 -0.25106746 -0.045877088 0.19253367 0.59869152 1.1921264 1.6150194 1.9826421 2.1306129 2.214009][-1.3518698 -1.5686322 -1.5658108 -1.333438 -1.1373472 -1.2188401 -1.159057 -1.0674295 -0.87956268 -0.72856188 -0.35314652 0.12352533 0.80663627 1.132942 1.2924864][-1.4137527 -1.6541636 -1.7272676 -1.9606975 -2.07654 -1.930022 -1.6989282 -1.7955763 -1.7278181 -1.7301531 -1.4879632 -1.155947 -0.72511762 -0.40023172 -0.13821626][-1.0077296 -1.5760581 -1.8269881 -2.2534096 -2.5994482 -2.7945294 -2.7514355 -2.6174898 -2.3580625 -2.2695622 -2.1529331 -2.1230793 -1.9450577 -1.650885 -1.3744895][-1.191413 -1.5645695 -1.9323148 -2.2855902 -2.4817691 -2.879746 -3.223999 -3.4915669 -3.5647914 -3.429858 -3.1790962 -2.9719141 -2.7187052 -2.4079068 -2.1389587]]...]
INFO - root - 2017-12-06 07:18:59.900502: step 32910, loss = 0.74, batch loss = 0.52 (12.2 examples/sec; 0.655 sec/batch; 54h:31m:46s remains)
INFO - root - 2017-12-06 07:19:06.406260: step 32920, loss = 0.83, batch loss = 0.61 (12.8 examples/sec; 0.626 sec/batch; 52h:04m:19s remains)
INFO - root - 2017-12-06 07:19:12.934588: step 32930, loss = 0.69, batch loss = 0.48 (12.4 examples/sec; 0.645 sec/batch; 53h:38m:31s remains)
INFO - root - 2017-12-06 07:19:19.457777: step 32940, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.646 sec/batch; 53h:46m:50s remains)
INFO - root - 2017-12-06 07:19:25.956031: step 32950, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 54h:26m:44s remains)
INFO - root - 2017-12-06 07:19:32.396927: step 32960, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.639 sec/batch; 53h:11m:07s remains)
INFO - root - 2017-12-06 07:19:38.842461: step 32970, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.659 sec/batch; 54h:49m:33s remains)
INFO - root - 2017-12-06 07:19:45.229411: step 32980, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 54h:28m:29s remains)
INFO - root - 2017-12-06 07:19:51.763852: step 32990, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.646 sec/batch; 53h:44m:40s remains)
INFO - root - 2017-12-06 07:19:58.167997: step 33000, loss = 0.87, batch loss = 0.65 (12.1 examples/sec; 0.660 sec/batch; 54h:52m:33s remains)
2017-12-06 07:19:58.797560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069895081 -0.064089894 -0.058690723 -0.053992711 -0.050455529 -0.048384055 -0.04786247 -0.048644647 -0.050133049 -0.051856 -0.053544171 -0.054762863 -0.055437688 -0.055750459 -0.0564113][-0.076329611 -0.069308192 -0.062293649 -0.055942334 -0.050923176 -0.047552489 -0.046139143 -0.04652147 -0.048134748 -0.050226912 -0.052410398 -0.05407967 -0.054853763 -0.055055056 -0.055419497][-0.081183746 -0.073082976 -0.064660236 -0.056944922 -0.050505191 -0.046108961 -0.044005606 -0.044111282 -0.045847319 -0.048388351 -0.051095076 -0.053132236 -0.054058034 -0.05421672 -0.054337714][-0.085461535 -0.0765954 -0.066669881 -0.057285063 -0.049313605 -0.043528836 -0.040296432 -0.040294651 -0.042319272 -0.045245986 -0.048418555 -0.050765287 -0.051840268 -0.052018426 -0.052170776][-0.086144954 -0.076730825 -0.065968879 -0.055758316 -0.04697711 -0.040047161 -0.036067113 -0.035922721 -0.038204543 -0.04141821 -0.044970796 -0.04744206 -0.04851051 -0.0486162 -0.048892807][-0.083658107 -0.073494084 -0.062296331 -0.052200429 -0.043623798 -0.036586881 -0.032536305 -0.032612763 -0.035186071 -0.038603511 -0.042261481 -0.044712879 -0.045498285 -0.045394223 -0.045837812][-0.077554673 -0.066874295 -0.055681486 -0.046104748 -0.038315617 -0.032378606 -0.029572714 -0.030649275 -0.033673804 -0.037107993 -0.04033849 -0.042402368 -0.042910255 -0.042726178 -0.04332345][-0.070105828 -0.059376962 -0.048969883 -0.040191732 -0.033542261 -0.028952654 -0.027494639 -0.029295094 -0.032639522 -0.0359613 -0.038632687 -0.040055655 -0.040154204 -0.03972524 -0.040495936][-0.061920486 -0.052650366 -0.043639697 -0.035807345 -0.030176263 -0.027006991 -0.026752219 -0.029027037 -0.032331 -0.035496473 -0.03772366 -0.038455471 -0.037953135 -0.037325621 -0.038317233][-0.054429282 -0.047098737 -0.040150825 -0.034254622 -0.030160923 -0.02793745 -0.028411213 -0.030624952 -0.033763498 -0.036486197 -0.038197618 -0.03827465 -0.037220746 -0.036282469 -0.036937252][-0.04873772 -0.043783005 -0.038746245 -0.034514677 -0.0319767 -0.031048208 -0.032053374 -0.034095656 -0.036339305 -0.038316455 -0.039523702 -0.038971532 -0.037596785 -0.0363678 -0.036757387][-0.045745846 -0.042640533 -0.039521109 -0.036896735 -0.035127584 -0.035099007 -0.036050882 -0.037739348 -0.039593812 -0.040901847 -0.04123088 -0.040018652 -0.03823211 -0.03694246 -0.037133735][-0.044876084 -0.043410365 -0.041805793 -0.040391691 -0.039493963 -0.039594587 -0.040257737 -0.041344561 -0.04222209 -0.042980343 -0.042913169 -0.041664626 -0.039791103 -0.038474031 -0.038205534][-0.044361081 -0.044182308 -0.043794926 -0.043460388 -0.043226384 -0.043479711 -0.044061892 -0.044639722 -0.044892967 -0.045266077 -0.044756398 -0.043280493 -0.041811764 -0.040959042 -0.041006662][-0.044196345 -0.044686981 -0.044993 -0.045183346 -0.045306295 -0.045568127 -0.046219397 -0.046899758 -0.046960086 -0.046932928 -0.046378888 -0.045118954 -0.04397193 -0.043504506 -0.043736942]]...]
INFO - root - 2017-12-06 07:20:05.273978: step 33010, loss = 0.81, batch loss = 0.59 (11.8 examples/sec; 0.676 sec/batch; 56h:13m:02s remains)
INFO - root - 2017-12-06 07:20:11.778116: step 33020, loss = 0.94, batch loss = 0.73 (12.0 examples/sec; 0.665 sec/batch; 55h:18m:48s remains)
INFO - root - 2017-12-06 07:20:18.246998: step 33030, loss = 0.74, batch loss = 0.53 (12.5 examples/sec; 0.642 sec/batch; 53h:23m:11s remains)
INFO - root - 2017-12-06 07:20:24.797623: step 33040, loss = 0.79, batch loss = 0.57 (11.5 examples/sec; 0.698 sec/batch; 58h:03m:50s remains)
INFO - root - 2017-12-06 07:20:31.242254: step 33050, loss = 0.77, batch loss = 0.55 (12.2 examples/sec; 0.655 sec/batch; 54h:26m:50s remains)
INFO - root - 2017-12-06 07:20:37.812350: step 33060, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 54h:17m:11s remains)
INFO - root - 2017-12-06 07:20:44.222214: step 33070, loss = 0.79, batch loss = 0.57 (12.4 examples/sec; 0.645 sec/batch; 53h:39m:09s remains)
INFO - root - 2017-12-06 07:20:50.707295: step 33080, loss = 0.77, batch loss = 0.55 (12.2 examples/sec; 0.656 sec/batch; 54h:33m:46s remains)
INFO - root - 2017-12-06 07:20:57.022506: step 33090, loss = 0.86, batch loss = 0.64 (12.9 examples/sec; 0.619 sec/batch; 51h:29m:24s remains)
INFO - root - 2017-12-06 07:21:03.543302: step 33100, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.654 sec/batch; 54h:24m:49s remains)
2017-12-06 07:21:04.171542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.047988661 -0.04618492 -0.28336906 -0.656304 -1.0762224 -1.4165215 -1.5458028 -1.5024604 -1.4278551 -1.2538884 -0.98078948 -0.91383791 -0.928986 -0.90348381 -0.92790741][-0.87138128 -0.79383838 -0.79549795 -0.89115739 -1.0990409 -1.372048 -1.6648705 -1.8708165 -1.924288 -1.9411834 -1.8536959 -1.5715536 -1.3040415 -1.2355704 -1.0689516][-1.6323119 -1.6671118 -1.6264325 -1.5324552 -1.5122514 -1.6022875 -1.7550204 -1.9173995 -2.0694118 -2.2536154 -2.4721897 -2.6970706 -2.6058815 -2.2227771 -1.8556066][-1.7731999 -1.8239866 -1.8898507 -1.7997535 -1.6148102 -1.4778618 -1.3947968 -1.5189511 -1.8330548 -2.1584597 -2.4310246 -2.7337894 -3.0408664 -3.2005427 -3.1358416][-1.6751046 -1.7122515 -1.4875861 -1.2656112 -1.0152019 -0.62516409 -0.43923908 -0.468401 -0.67145854 -1.1928136 -1.6919103 -2.1606662 -2.7054436 -3.0852697 -3.4478271][-1.0313609 -1.4180334 -1.5559006 -1.2553102 -0.68177807 0.040243432 0.83604515 1.2074494 1.183611 0.86364764 0.31035089 -0.40869218 -1.1967748 -2.1500862 -2.8175657][0.27119988 -0.17092809 -0.52479964 -0.57868135 -0.39846712 0.13152371 0.97783214 1.845124 2.4486632 2.6122472 2.4980278 1.9745169 1.3855325 0.5642994 -0.33739728][0.8673107 0.74401522 0.48500156 0.040156156 -0.20392919 -0.14315853 0.27363509 1.1028262 1.9759092 2.3847039 2.72952 2.9795926 2.962008 2.5658195 2.0058699][0.56219929 0.72400188 0.65358073 0.43649578 0.11355355 -0.14875591 -0.097346477 0.30050147 0.89591485 1.5148143 2.0856471 2.5392022 3.1649771 3.3829973 3.3029077][-0.23110497 0.22633095 0.45116943 0.49128616 0.52000403 0.36952269 0.19478811 0.28950036 0.43992037 0.5130235 0.97912365 1.4735709 2.1809673 2.8277986 3.3797545][-1.3769914 -0.58765751 0.02013088 0.21282871 0.20553641 0.16084312 0.1993304 0.044368684 -0.33907628 -0.639461 -0.53486395 -0.46379131 0.23641486 1.1575316 2.3171477][-2.2558081 -1.6070873 -1.04946 -0.52118695 -0.19370595 0.00645867 0.1484216 0.19538604 0.12770654 -0.43295211 -1.0212902 -1.3130534 -1.2508373 -0.92764461 -0.27998805][-2.0396369 -1.7379836 -1.4367442 -1.075284 -0.94693184 -0.73487651 -0.63681537 -0.41975904 -0.34629053 -0.58974528 -0.95510685 -1.2472185 -1.552873 -2.0553734 -2.1830595][-0.85943425 -0.68408912 -0.64264071 -0.46266747 -0.507634 -0.69297159 -0.87657511 -0.9419207 -1.1880546 -1.2749549 -1.5021654 -1.7685769 -2.1427937 -2.5262229 -2.7180793][0.1329966 0.16011252 0.18929286 0.22616561 0.065401822 -0.03796275 -0.26093864 -0.53517711 -0.97661942 -1.5078863 -2.169214 -2.2305734 -2.6105397 -2.8078132 -3.0595767]]...]
INFO - root - 2017-12-06 07:21:10.637915: step 33110, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.656 sec/batch; 54h:35m:36s remains)
INFO - root - 2017-12-06 07:21:17.044633: step 33120, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.646 sec/batch; 53h:44m:22s remains)
INFO - root - 2017-12-06 07:21:23.638248: step 33130, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 54h:07m:58s remains)
INFO - root - 2017-12-06 07:21:30.123806: step 33140, loss = 0.84, batch loss = 0.62 (11.8 examples/sec; 0.680 sec/batch; 56h:33m:03s remains)
INFO - root - 2017-12-06 07:21:36.663544: step 33150, loss = 0.76, batch loss = 0.55 (12.2 examples/sec; 0.656 sec/batch; 54h:34m:50s remains)
INFO - root - 2017-12-06 07:21:42.902280: step 33160, loss = 0.89, batch loss = 0.67 (12.8 examples/sec; 0.627 sec/batch; 52h:09m:31s remains)
INFO - root - 2017-12-06 07:21:49.358964: step 33170, loss = 0.72, batch loss = 0.50 (12.8 examples/sec; 0.625 sec/batch; 51h:56m:24s remains)
INFO - root - 2017-12-06 07:21:55.753680: step 33180, loss = 0.86, batch loss = 0.65 (11.8 examples/sec; 0.678 sec/batch; 56h:21m:57s remains)
INFO - root - 2017-12-06 07:22:02.184381: step 33190, loss = 0.89, batch loss = 0.67 (12.7 examples/sec; 0.628 sec/batch; 52h:10m:43s remains)
INFO - root - 2017-12-06 07:22:08.653711: step 33200, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.641 sec/batch; 53h:15m:22s remains)
2017-12-06 07:22:09.371479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.16932784 -0.18592826 -0.16088514 -0.14823981 -0.16114077 -0.21960463 -0.28519011 -0.3346619 -0.34986782 -0.32872438 -0.29580772 -0.2651611 -0.24571718 -0.20828646 -0.17070854][-0.24435188 -0.28824151 -0.28066516 -0.24509396 -0.2026443 -0.22193956 -0.2770009 -0.36395133 -0.43868375 -0.45948738 -0.45067197 -0.3957085 -0.34566534 -0.29576886 -0.23469178][-0.23702514 -0.28743774 -0.29152405 -0.25452277 -0.17403415 -0.12023968 -0.12157106 -0.21538773 -0.358701 -0.45311904 -0.51009041 -0.48094273 -0.42005074 -0.34956187 -0.25237185][-0.21910444 -0.25773725 -0.25111812 -0.20388556 -0.10554152 0.023604289 0.12644494 0.10708009 -0.059977006 -0.25676543 -0.4162454 -0.4454838 -0.41654271 -0.34280032 -0.21827389][-0.16835634 -0.16983156 -0.13214622 -0.0644943 0.068508357 0.25318044 0.43218774 0.52760476 0.43230605 0.21135117 -0.039311491 -0.15109533 -0.16955236 -0.08648891 0.026984937][-0.10169475 -0.055632032 0.048414826 0.1671886 0.31782347 0.50313628 0.70360917 0.86086577 0.860379 0.69190872 0.42226493 0.25324291 0.18293057 0.24092795 0.34285867][-0.06620799 0.0045430213 0.13149792 0.28932667 0.46273577 0.61392349 0.76179338 0.91568333 0.98030049 0.89317548 0.68551409 0.48778719 0.34399205 0.36075872 0.45563358][-0.11896974 -0.065710977 0.06051679 0.21293043 0.35367817 0.44142848 0.49890059 0.56558567 0.62412775 0.64226466 0.547136 0.4163785 0.30632097 0.27897936 0.34046328][-0.20653015 -0.20137458 -0.14666747 -0.052550793 0.058693528 0.12972592 0.15092468 0.15001678 0.13972946 0.12827557 0.092829749 0.049144283 -0.025040857 -0.059156682 -0.0018115714][-0.29444766 -0.35925472 -0.38521463 -0.3612625 -0.293803 -0.21717356 -0.167577 -0.16918547 -0.19421639 -0.22813837 -0.2739926 -0.30480558 -0.33280236 -0.34870636 -0.33069909][-0.26593283 -0.37306219 -0.46799451 -0.51891083 -0.51272792 -0.46710002 -0.41877759 -0.39996392 -0.39879733 -0.40010333 -0.40860647 -0.42295629 -0.44841444 -0.45648527 -0.43990791][-0.19773662 -0.29158676 -0.40615356 -0.50250953 -0.54242504 -0.51831776 -0.44732493 -0.37935209 -0.32613406 -0.29731619 -0.30093938 -0.33590406 -0.38580626 -0.42502046 -0.43463784][-0.12422001 -0.17260233 -0.24102031 -0.31461516 -0.36112845 -0.355869 -0.29450119 -0.20909357 -0.12719665 -0.073999837 -0.0632963 -0.087167546 -0.14534432 -0.20314229 -0.25169486][-0.093946643 -0.10625823 -0.1269099 -0.15367523 -0.17681381 -0.183207 -0.15665638 -0.10784933 -0.0634771 -0.038600132 -0.040482882 -0.055772383 -0.081639633 -0.10482766 -0.12682185][-0.0894145 -0.091784567 -0.094779789 -0.099568091 -0.10107157 -0.095795721 -0.08379893 -0.06710396 -0.051976208 -0.047180232 -0.059102379 -0.074428394 -0.092278473 -0.10673915 -0.1195118]]...]
INFO - root - 2017-12-06 07:22:15.912936: step 33210, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.641 sec/batch; 53h:19m:43s remains)
INFO - root - 2017-12-06 07:22:22.413492: step 33220, loss = 0.85, batch loss = 0.63 (12.3 examples/sec; 0.653 sec/batch; 54h:17m:14s remains)
INFO - root - 2017-12-06 07:22:28.837820: step 33230, loss = 0.87, batch loss = 0.66 (12.4 examples/sec; 0.646 sec/batch; 53h:40m:14s remains)
INFO - root - 2017-12-06 07:22:35.347037: step 33240, loss = 0.92, batch loss = 0.70 (12.1 examples/sec; 0.661 sec/batch; 54h:58m:25s remains)
INFO - root - 2017-12-06 07:22:41.897569: step 33250, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.654 sec/batch; 54h:20m:07s remains)
INFO - root - 2017-12-06 07:22:48.336269: step 33260, loss = 0.76, batch loss = 0.54 (11.9 examples/sec; 0.675 sec/batch; 56h:04m:43s remains)
INFO - root - 2017-12-06 07:22:54.789127: step 33270, loss = 0.96, batch loss = 0.74 (12.5 examples/sec; 0.640 sec/batch; 53h:09m:25s remains)
INFO - root - 2017-12-06 07:23:01.187596: step 33280, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.645 sec/batch; 53h:37m:22s remains)
INFO - root - 2017-12-06 07:23:07.781602: step 33290, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.662 sec/batch; 55h:00m:23s remains)
INFO - root - 2017-12-06 07:23:14.265245: step 33300, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 54h:36m:06s remains)
2017-12-06 07:23:14.916090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28985846 -0.2552996 -0.1396794 0.010026358 0.12211478 0.12293594 0.0035708323 -0.16809618 -0.29725915 -0.36967775 -0.3862749 -0.35472071 -0.24949279 -0.28103086 -0.37599188][-0.38160136 -0.41751477 -0.37143832 -0.26862547 -0.13791132 -0.069842249 -0.092922047 -0.18340459 -0.26080847 -0.28661963 -0.27946827 -0.26995897 -0.25588277 -0.34286571 -0.48627222][-0.38390166 -0.45707208 -0.45752057 -0.39710936 -0.27856255 -0.1727106 -0.11807982 -0.14747909 -0.18957263 -0.21267927 -0.22603342 -0.22072321 -0.21258706 -0.2856591 -0.40527681][-0.33517849 -0.37032408 -0.35513762 -0.28834018 -0.18152304 -0.077058189 -0.03484799 -0.063927621 -0.096549392 -0.10574453 -0.089460991 -0.073530927 -0.07360626 -0.12487808 -0.26231271][-0.20659915 -0.19300994 -0.16450366 -0.099880189 -0.0013484731 0.079045385 0.097425327 0.057942197 -0.0064126775 -0.040777292 -0.019215666 0.0010102093 0.014951289 4.7832727e-06 -0.10542428][-0.11616191 -0.0732463 -0.012094609 0.065245092 0.15676954 0.221472 0.23998049 0.19767544 0.11489457 0.037142903 0.0012725294 -0.01887168 0.0061587021 0.016144104 -0.063748933][0.040441826 0.030464336 0.041754127 0.11495681 0.21501064 0.29268783 0.31106725 0.26975113 0.16953984 0.072132751 -0.0074800104 -0.069197319 -0.058306031 -0.023901507 -0.050731279][0.074201256 0.020609371 0.023458898 0.11402024 0.2064814 0.28900653 0.33217061 0.31446645 0.20781863 0.10022584 0.016045064 -0.071346067 -0.10056129 -0.096759155 -0.071700886][0.14192469 0.04376857 0.011889793 0.066279516 0.13454981 0.22450817 0.25360695 0.23186371 0.12330197 0.029483788 -0.039928004 -0.068852782 -0.091207117 -0.079547919 -0.059731416][0.088921636 -0.036558691 -0.081397645 -0.034029745 0.029154561 0.065441117 0.039200634 0.0034212172 -0.069615684 -0.12751052 -0.11709014 -0.084053263 -0.085938767 -0.025784478 0.025340356][-0.024526909 -0.21043104 -0.30255181 -0.26748648 -0.1898751 -0.12174529 -0.11926892 -0.16574368 -0.21597242 -0.26387873 -0.21096665 -0.10829706 -0.056208681 -0.00030358136 0.019754939][-0.309767 -0.4534336 -0.52315688 -0.49141085 -0.3953371 -0.31128865 -0.28782514 -0.30302715 -0.29525989 -0.32012802 -0.24630196 -0.10092713 0.0067771375 0.069037393 0.048013017][-0.47597221 -0.61848426 -0.68016905 -0.655952 -0.57480192 -0.4800882 -0.40033138 -0.36235937 -0.33360469 -0.34652597 -0.26182538 -0.12733859 0.011447683 0.12876962 0.17083743][-0.55509579 -0.666807 -0.70678371 -0.6899488 -0.61872333 -0.52858317 -0.418146 -0.35327387 -0.32710147 -0.32574761 -0.24256012 -0.1003599 0.066353112 0.17829666 0.22981453][-0.536093 -0.63567024 -0.66188467 -0.63540572 -0.56215829 -0.48055989 -0.37180427 -0.29433754 -0.27364272 -0.286067 -0.23950554 -0.14267534 0.0090445206 0.14681886 0.25055936]]...]
INFO - root - 2017-12-06 07:23:21.410604: step 33310, loss = 0.83, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 54h:26m:28s remains)
INFO - root - 2017-12-06 07:23:27.983260: step 33320, loss = 0.85, batch loss = 0.64 (12.6 examples/sec; 0.635 sec/batch; 52h:48m:03s remains)
INFO - root - 2017-12-06 07:23:34.468415: step 33330, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.642 sec/batch; 53h:20m:59s remains)
INFO - root - 2017-12-06 07:23:40.931117: step 33340, loss = 0.82, batch loss = 0.60 (13.1 examples/sec; 0.610 sec/batch; 50h:40m:36s remains)
INFO - root - 2017-12-06 07:23:47.541078: step 33350, loss = 0.75, batch loss = 0.54 (11.6 examples/sec; 0.689 sec/batch; 57h:13m:22s remains)
INFO - root - 2017-12-06 07:23:54.006244: step 33360, loss = 0.89, batch loss = 0.68 (12.5 examples/sec; 0.640 sec/batch; 53h:08m:57s remains)
INFO - root - 2017-12-06 07:24:00.362743: step 33370, loss = 0.89, batch loss = 0.68 (12.7 examples/sec; 0.632 sec/batch; 52h:29m:23s remains)
INFO - root - 2017-12-06 07:24:06.863570: step 33380, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.650 sec/batch; 54h:01m:22s remains)
INFO - root - 2017-12-06 07:24:13.365976: step 33390, loss = 0.80, batch loss = 0.58 (12.6 examples/sec; 0.634 sec/batch; 52h:40m:11s remains)
INFO - root - 2017-12-06 07:24:19.780316: step 33400, loss = 0.79, batch loss = 0.57 (12.8 examples/sec; 0.623 sec/batch; 51h:45m:03s remains)
2017-12-06 07:24:20.451486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.01336766 -0.012061037 -0.010174774 -0.0077184439 -0.0048820823 -0.0027553886 -0.0020125136 -0.0025333613 -0.0039394721 -0.006428048 -0.010703467 -0.017375231 -0.02649039 -0.037429269 -0.049303491][-0.0095678642 -0.0084359646 -0.0068338066 -0.0046105087 -0.0018838868 0.00040715933 0.0012760162 0.0010935888 4.3287873e-06 -0.0021666363 -0.0059960335 -0.012262471 -0.021128573 -0.0320835 -0.044243291][-0.008222647 -0.0074269921 -0.0064287782 -0.0048178434 -0.0024892762 -0.00037550926 0.00040518492 0.00046853721 -0.00013294816 -0.0015572757 -0.004341267 -0.0094475448 -0.017103024 -0.02700261 -0.038528379][-0.0089207813 -0.0083157271 -0.00788901 -0.006933935 -0.005306229 -0.0035384148 -0.0027371123 -0.0024203509 -0.0024760142 -0.0029709488 -0.0045551434 -0.008300744 -0.014513291 -0.022841565 -0.032923955][-0.011048399 -0.010197937 -0.0098303035 -0.0092380121 -0.0081594586 -0.006766744 -0.0059933364 -0.0054894909 -0.0053443313 -0.005420886 -0.0061495453 -0.0087369233 -0.013445206 -0.020219028 -0.028663382][-0.014629379 -0.012749925 -0.011630528 -0.010719799 -0.0096909553 -0.0084292069 -0.0076541007 -0.0072338805 -0.0072586536 -0.007570222 -0.00834395 -0.010472864 -0.014276966 -0.019902229 -0.02705428][-0.019241929 -0.015777305 -0.013241202 -0.011373252 -0.0097614452 -0.008325994 -0.0075992793 -0.0074019507 -0.0078603923 -0.00887993 -0.010316104 -0.01268068 -0.016386896 -0.021536112 -0.027960882][-0.023614809 -0.018950656 -0.015051439 -0.011792786 -0.0091219023 -0.0072396845 -0.0063322112 -0.006088838 -0.0069488958 -0.0089173317 -0.011383429 -0.01471898 -0.019004412 -0.024049856 -0.029938329][-0.026468933 -0.021204785 -0.016438372 -0.012219347 -0.0086689219 -0.006223999 -0.0049476922 -0.004732132 -0.0062033758 -0.0092095211 -0.012640141 -0.016440384 -0.021079049 -0.02622205 -0.031708859][-0.027410567 -0.021972261 -0.016820341 -0.012268379 -0.0085732639 -0.0061389059 -0.0052883998 -0.0058322772 -0.0079188347 -0.010913163 -0.014343657 -0.01801008 -0.022490375 -0.027336687 -0.032401][-0.027190715 -0.021876045 -0.016993396 -0.012744121 -0.0095226616 -0.0075095743 -0.0067443773 -0.0075040609 -0.0095843375 -0.012253456 -0.015147284 -0.018265232 -0.022321083 -0.027017549 -0.031926107][-0.026448376 -0.021618143 -0.017348714 -0.013704911 -0.011107378 -0.0096559152 -0.0090697184 -0.0096347481 -0.011235975 -0.013254352 -0.015455827 -0.017929398 -0.021435365 -0.025889665 -0.030737605][-0.026544407 -0.022337183 -0.018624514 -0.015526161 -0.013641283 -0.012915298 -0.012751266 -0.013203993 -0.014244556 -0.015484422 -0.016788073 -0.01836431 -0.021122761 -0.025278248 -0.030163761][-0.028134897 -0.024606116 -0.021652207 -0.019303367 -0.018119141 -0.01807376 -0.018289082 -0.01868581 -0.019063987 -0.019545168 -0.020120941 -0.020934954 -0.022922352 -0.02648171 -0.031214185][-0.032273769 -0.029688582 -0.027796283 -0.026327439 -0.025687858 -0.025799006 -0.026033878 -0.02623339 -0.02613683 -0.026051305 -0.026104279 -0.026480488 -0.027734131 -0.030449003 -0.034453306]]...]
INFO - root - 2017-12-06 07:24:26.931332: step 33410, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.655 sec/batch; 54h:23m:04s remains)
INFO - root - 2017-12-06 07:24:33.513381: step 33420, loss = 0.88, batch loss = 0.67 (12.2 examples/sec; 0.654 sec/batch; 54h:18m:56s remains)
INFO - root - 2017-12-06 07:24:40.016466: step 33430, loss = 0.88, batch loss = 0.66 (12.0 examples/sec; 0.665 sec/batch; 55h:15m:32s remains)
INFO - root - 2017-12-06 07:24:46.489996: step 33440, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 54h:04m:35s remains)
INFO - root - 2017-12-06 07:24:52.996429: step 33450, loss = 0.89, batch loss = 0.68 (12.5 examples/sec; 0.639 sec/batch; 53h:03m:21s remains)
INFO - root - 2017-12-06 07:24:59.445198: step 33460, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.655 sec/batch; 54h:23m:39s remains)
INFO - root - 2017-12-06 07:25:05.911266: step 33470, loss = 0.74, batch loss = 0.53 (12.0 examples/sec; 0.664 sec/batch; 55h:09m:28s remains)
INFO - root - 2017-12-06 07:25:12.548269: step 33480, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.642 sec/batch; 53h:19m:33s remains)
INFO - root - 2017-12-06 07:25:19.068695: step 33490, loss = 0.88, batch loss = 0.66 (11.8 examples/sec; 0.680 sec/batch; 56h:31m:00s remains)
INFO - root - 2017-12-06 07:25:25.544915: step 33500, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.659 sec/batch; 54h:41m:47s remains)
2017-12-06 07:25:26.232527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.15829131 -0.12385628 -0.10400195 -0.11237545 -0.12684718 -0.14530003 -0.16640055 -0.18473977 -0.18952844 -0.18370703 -0.18332545 -0.17522669 -0.1755096 -0.19007739 -0.19348398][-0.14638287 -0.13028094 -0.113998 -0.087890357 -0.068865523 -0.0845348 -0.099526457 -0.12456657 -0.15412308 -0.18681943 -0.19223611 -0.17387775 -0.14297591 -0.1380406 -0.14159623][-0.131422 -0.11346389 -0.10568316 -0.085676581 -0.078177989 -0.080761358 -0.0774417 -0.086096339 -0.10144757 -0.12613401 -0.14420411 -0.1549482 -0.14220855 -0.11970527 -0.10239329][-0.10198202 -0.086596444 -0.078772485 -0.064570017 -0.056373693 -0.048321124 -0.043636955 -0.050480027 -0.05843249 -0.071805716 -0.07651481 -0.085167207 -0.092976823 -0.08848092 -0.084999248][-0.10457195 -0.075087324 -0.041839622 -0.037098803 -0.036525767 -0.039354563 -0.043875463 -0.044850431 -0.049995534 -0.058313109 -0.065818474 -0.066933043 -0.077137582 -0.073415361 -0.072471477][-0.074537367 -0.083283447 -0.074788451 -0.053182617 -0.035858326 -0.039893854 -0.047990862 -0.053311624 -0.057243519 -0.054551937 -0.052109662 -0.049706407 -0.055780984 -0.057236921 -0.056492094][-0.081273973 -0.081066035 -0.078520194 -0.082289211 -0.070379473 -0.052445881 -0.046020355 -0.053089745 -0.065615013 -0.071464963 -0.075915255 -0.069456786 -0.065262556 -0.064436905 -0.067690238][-0.086973861 -0.085993692 -0.08727058 -0.087683588 -0.086538456 -0.064364776 -0.050087504 -0.04560595 -0.053613164 -0.067567632 -0.081336282 -0.089584425 -0.092196062 -0.09338852 -0.093326837][-0.096464537 -0.090627432 -0.082161337 -0.073971048 -0.068360172 -0.062998414 -0.058987487 -0.05141728 -0.048710451 -0.053724684 -0.060417011 -0.072139747 -0.083856478 -0.090258636 -0.095923059][-0.10787938 -0.10088051 -0.088840075 -0.075214915 -0.063939631 -0.0603133 -0.065406255 -0.067919105 -0.065355 -0.063506484 -0.0739591 -0.070791028 -0.07317441 -0.080100991 -0.083344556][-0.12178752 -0.11699355 -0.10474344 -0.089286543 -0.0741525 -0.065028071 -0.062027879 -0.06571608 -0.076964587 -0.070886411 -0.069773555 -0.073045783 -0.075079419 -0.081293076 -0.085047364][-0.10122445 -0.098099105 -0.089272261 -0.079317257 -0.0701065 -0.062531084 -0.057469394 -0.0593158 -0.065704659 -0.069413975 -0.072329849 -0.071233705 -0.0738886 -0.0814466 -0.090391114][-0.09520065 -0.097390644 -0.089711308 -0.0781215 -0.066332951 -0.058098949 -0.053152435 -0.0583 -0.0604582 -0.06832689 -0.071540169 -0.072662354 -0.076429851 -0.0772327 -0.080458745][-0.0911468 -0.092413135 -0.08951465 -0.08435408 -0.077456191 -0.07088621 -0.065039858 -0.064434931 -0.060345702 -0.065376468 -0.069640562 -0.073795684 -0.07522437 -0.078463495 -0.080817044][-0.089832857 -0.087411039 -0.08478304 -0.081906863 -0.07728938 -0.074504696 -0.071177393 -0.0673895 -0.062939 -0.064509384 -0.065336809 -0.071001261 -0.078076474 -0.077984475 -0.077960059]]...]
INFO - root - 2017-12-06 07:25:32.761230: step 33510, loss = 0.75, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 54h:43m:42s remains)
INFO - root - 2017-12-06 07:25:39.226959: step 33520, loss = 0.73, batch loss = 0.51 (12.1 examples/sec; 0.660 sec/batch; 54h:47m:45s remains)
INFO - root - 2017-12-06 07:25:45.696039: step 33530, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.672 sec/batch; 55h:48m:17s remains)
INFO - root - 2017-12-06 07:25:52.023996: step 33540, loss = 0.74, batch loss = 0.52 (12.1 examples/sec; 0.663 sec/batch; 55h:03m:26s remains)
INFO - root - 2017-12-06 07:25:58.531163: step 33550, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.641 sec/batch; 53h:15m:10s remains)
INFO - root - 2017-12-06 07:26:04.817382: step 33560, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.657 sec/batch; 54h:33m:16s remains)
INFO - root - 2017-12-06 07:26:11.312000: step 33570, loss = 0.87, batch loss = 0.65 (12.6 examples/sec; 0.636 sec/batch; 52h:50m:38s remains)
INFO - root - 2017-12-06 07:26:17.814483: step 33580, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.650 sec/batch; 53h:57m:46s remains)
INFO - root - 2017-12-06 07:26:24.234938: step 33590, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.642 sec/batch; 53h:20m:42s remains)
INFO - root - 2017-12-06 07:26:30.850643: step 33600, loss = 0.69, batch loss = 0.48 (12.6 examples/sec; 0.637 sec/batch; 52h:54m:01s remains)
2017-12-06 07:26:31.482130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.96131212 -0.830233 -0.77260923 -0.76876229 -0.88865829 -1.0404485 -1.1087993 -1.0623543 -1.009455 -0.97364247 -0.82968962 -0.54218441 -0.060407963 0.3083173 0.50932056][-1.4700656 -1.3196303 -1.1822907 -1.092165 -1.0940536 -1.2130282 -1.4570963 -1.6379308 -1.6155336 -1.4082282 -1.1626551 -0.89732295 -0.54542369 -0.1735819 0.320061][-1.8009344 -1.687822 -1.550838 -1.4015867 -1.3297292 -1.3465534 -1.3858933 -1.5413595 -1.6603118 -1.6234002 -1.5181552 -1.3205428 -1.0292025 -0.73714989 -0.34561619][-1.9180177 -1.7926511 -1.6296433 -1.4722279 -1.2929434 -1.192378 -1.1723104 -1.2077848 -1.3228062 -1.374117 -1.3371222 -1.1931019 -1.0143893 -0.84711152 -0.58415478][-1.6130601 -1.4287421 -1.1712778 -0.92796707 -0.71144658 -0.54900873 -0.48763561 -0.57135022 -0.68542904 -0.85677439 -1.0012172 -0.94495952 -0.94612789 -0.85875332 -0.69474584][-0.896174 -0.65364146 -0.34155929 -0.033675328 0.18459147 0.33570176 0.43041575 0.34419388 0.2236149 0.010331035 -0.1740475 -0.41518089 -0.517466 -0.53234237 -0.59377587][-0.32533014 -0.029894717 0.3748574 0.65887851 0.909832 1.1024719 1.2498014 1.2539567 1.2192601 1.0556605 0.863546 0.51593029 0.20947734 0.036807589 -0.10359499][0.070607267 0.35467786 0.73900515 1.0956084 1.402106 1.5667136 1.6851761 1.7897254 1.8204074 1.7383211 1.592981 1.2256919 0.76978171 0.38313672 0.1542967][-0.015560858 0.15822095 0.49413365 0.91072893 1.2955074 1.6099302 1.7789229 1.8499926 1.8145125 1.8091964 1.674351 1.2118992 0.8269825 0.40994281 0.001007475][-0.321258 -0.3132371 -0.15296161 0.20395482 0.65688306 1.0701107 1.3746839 1.5188274 1.4812969 1.3614508 1.1576189 0.85384458 0.44006664 -0.031421941 -0.29742381][-0.66846639 -0.76886016 -0.70506722 -0.49174199 -0.18281704 0.20141679 0.53256267 0.69082117 0.70184046 0.6923303 0.49991602 0.23005301 -0.039566465 -0.35382143 -0.51876503][-0.84081012 -1.0076348 -1.1693137 -1.1096556 -0.91554546 -0.62426275 -0.35062939 -0.18227202 -0.11335312 -0.17615256 -0.27112731 -0.39454967 -0.6369499 -0.83321679 -0.74738753][-0.92859775 -1.1072816 -1.255109 -1.3325162 -1.3766569 -1.2240263 -1.0077869 -0.80380279 -0.69465744 -0.68293905 -0.72904229 -0.81620759 -0.905076 -0.99259192 -0.93505704][-0.53133523 -0.82063848 -1.1296757 -1.2919325 -1.3852748 -1.3249047 -1.2177143 -1.1061951 -0.99799746 -0.91224009 -0.84792012 -0.84887135 -0.86191779 -0.94157344 -0.89097714][-0.30319044 -0.569875 -0.86938524 -1.137615 -1.2692778 -1.3159329 -1.2608688 -1.177669 -1.0662112 -0.980863 -0.88850605 -0.81129563 -0.71720904 -0.63098466 -0.6284222]]...]
INFO - root - 2017-12-06 07:26:37.962273: step 33610, loss = 0.83, batch loss = 0.62 (12.3 examples/sec; 0.653 sec/batch; 54h:12m:26s remains)
INFO - root - 2017-12-06 07:26:44.574314: step 33620, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.646 sec/batch; 53h:39m:35s remains)
INFO - root - 2017-12-06 07:26:51.010010: step 33630, loss = 0.78, batch loss = 0.56 (12.7 examples/sec; 0.629 sec/batch; 52h:11m:09s remains)
INFO - root - 2017-12-06 07:26:57.559327: step 33640, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.653 sec/batch; 54h:14m:57s remains)
INFO - root - 2017-12-06 07:27:03.914017: step 33650, loss = 0.88, batch loss = 0.66 (12.2 examples/sec; 0.656 sec/batch; 54h:26m:45s remains)
INFO - root - 2017-12-06 07:27:10.480925: step 33660, loss = 0.96, batch loss = 0.75 (12.5 examples/sec; 0.639 sec/batch; 53h:01m:37s remains)
INFO - root - 2017-12-06 07:27:17.044058: step 33670, loss = 0.80, batch loss = 0.58 (11.7 examples/sec; 0.684 sec/batch; 56h:45m:37s remains)
INFO - root - 2017-12-06 07:27:23.544607: step 33680, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.662 sec/batch; 54h:57m:55s remains)
INFO - root - 2017-12-06 07:27:29.921789: step 33690, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.652 sec/batch; 54h:05m:41s remains)
INFO - root - 2017-12-06 07:27:36.402021: step 33700, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 52h:59m:53s remains)
2017-12-06 07:27:37.088035: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20708187 -0.21549481 -0.48819095 -0.91602349 -1.3538842 -1.604369 -1.6755836 -1.664948 -1.5403129 -1.2845556 -0.9814555 -0.95538712 -0.6992389 -0.34776682 0.12689723][-0.38884842 -1.0844356 -1.418713 -1.636054 -1.7144455 -1.9776422 -2.1845829 -2.3135817 -2.2210934 -1.9825639 -1.6727837 -1.2001981 -0.73550874 -0.46469027 -0.18389514][-1.6341361 -1.9430822 -2.0235481 -2.1271904 -2.1301994 -2.1212673 -2.1382244 -2.2663903 -2.3642819 -2.374593 -2.3218756 -2.0905752 -1.668785 -1.0986353 -0.39629859][-2.3749168 -2.5391183 -2.4172938 -2.2286594 -2.0521553 -1.7427013 -1.6861908 -1.8496468 -2.192435 -2.499929 -2.7058175 -2.5603528 -2.2892625 -1.9581432 -1.3855875][-2.0996985 -1.9428918 -1.781191 -1.515017 -1.4162292 -1.3093503 -1.2976334 -1.4471619 -1.7974445 -2.3407114 -2.8621175 -3.0664246 -2.8961256 -2.6421609 -2.2367485][-1.7568787 -1.1838448 -0.62633038 -0.15110707 -0.03639257 -0.18360721 -0.47978067 -0.85324019 -1.1240714 -1.4825236 -1.8144169 -2.1319137 -2.3934708 -2.4969249 -2.4300089][-0.81083745 -0.29658282 0.29701209 0.93409276 1.3430636 1.2657703 0.86239773 0.41800153 0.070452288 -0.19780251 -0.31520653 -0.50480342 -0.71196127 -1.0847732 -1.5015081][0.47256076 0.72610158 1.2375423 1.8120027 2.183737 2.0816314 1.7202688 1.4997654 1.2299335 0.98032916 0.9094255 0.9993335 1.1154479 0.83910781 0.25953162][0.48889756 0.70283467 1.1524001 1.5280923 1.9938462 2.1291065 2.0052824 1.687551 1.3495935 1.1580782 0.9547832 1.2530704 1.6897933 1.8363363 1.5849166][0.23662244 0.056263894 0.056383684 0.24889202 0.36201787 0.56931311 0.73023331 0.7220211 0.68335032 0.4855839 0.40076578 0.66046697 0.938177 1.4534546 1.7435195][-0.31749004 -0.55056894 -0.60913044 -0.80822664 -1.1210394 -1.1310869 -1.0734329 -0.93093008 -0.73072958 -0.5895164 -0.47976875 -0.35442215 0.025258698 0.48451954 0.78249353][-1.0268273 -1.2794983 -1.3153003 -1.459026 -1.7426959 -1.9653602 -2.0493226 -1.8525683 -1.7175388 -1.419578 -1.1638004 -1.0588673 -0.74575508 -0.39531481 0.20582311][-1.0147933 -1.2819625 -1.3594416 -1.5687716 -1.8871198 -2.037858 -1.9851743 -1.9153866 -1.7459003 -1.4781584 -1.3172212 -1.2565342 -0.99847376 -0.8616218 -0.60826021][-0.71922231 -0.69876772 -0.67732865 -0.90625256 -1.0348464 -1.2275108 -1.2662182 -1.1204371 -1.0387162 -0.99428505 -0.89228028 -0.88547051 -0.82422394 -0.680033 -0.56474763][0.21353601 0.12932369 0.041956276 -0.29669622 -0.43240756 -0.53569514 -0.47562659 -0.43712121 -0.551151 -0.69314033 -0.71579 -0.63938808 -0.68987072 -0.64464 -0.40645093]]...]
INFO - root - 2017-12-06 07:27:43.555457: step 33710, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.652 sec/batch; 54h:09m:19s remains)
INFO - root - 2017-12-06 07:27:50.052283: step 33720, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.635 sec/batch; 52h:41m:23s remains)
INFO - root - 2017-12-06 07:27:56.454165: step 33730, loss = 0.86, batch loss = 0.64 (12.3 examples/sec; 0.649 sec/batch; 53h:53m:28s remains)
INFO - root - 2017-12-06 07:28:02.827651: step 33740, loss = 0.82, batch loss = 0.61 (15.7 examples/sec; 0.510 sec/batch; 42h:19m:55s remains)
INFO - root - 2017-12-06 07:28:09.445444: step 33750, loss = 0.80, batch loss = 0.59 (12.6 examples/sec; 0.637 sec/batch; 52h:52m:22s remains)
INFO - root - 2017-12-06 07:28:16.055188: step 33760, loss = 0.77, batch loss = 0.56 (11.8 examples/sec; 0.677 sec/batch; 56h:08m:24s remains)
INFO - root - 2017-12-06 07:28:22.553376: step 33770, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.668 sec/batch; 55h:27m:36s remains)
INFO - root - 2017-12-06 07:28:28.965612: step 33780, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.643 sec/batch; 53h:19m:07s remains)
INFO - root - 2017-12-06 07:28:35.434004: step 33790, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 52h:55m:44s remains)
INFO - root - 2017-12-06 07:28:41.976975: step 33800, loss = 0.74, batch loss = 0.52 (12.6 examples/sec; 0.637 sec/batch; 52h:49m:49s remains)
2017-12-06 07:28:42.607828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0604519 -1.116709 -0.97713 -0.869791 -0.67193544 -0.45107403 -0.26399145 -0.26875353 -0.17139164 -0.032023005 -0.00817249 -0.11739305 -0.26538125 -0.33152673 -0.29866782][-1.1426769 -1.2801607 -1.1153359 -0.90700567 -0.76359046 -0.61055541 -0.32291272 -0.16655339 -0.013768613 -0.054603416 -0.26694188 -0.34918553 -0.43724209 -0.58356929 -0.564794][-1.0572866 -1.2225908 -1.2380793 -1.0426636 -0.7143839 -0.535115 -0.35500225 -0.20039991 -0.11744288 -0.13696726 -0.20670485 -0.52834451 -0.80451089 -0.82703382 -1.0083719][-1.0031903 -1.103451 -0.98662 -0.87154835 -0.5403071 -0.31636408 -0.21033248 -0.22948304 -0.31251824 -0.21216097 -0.21459688 -0.40156537 -0.80070692 -1.1184131 -1.2263464][-0.786517 -0.85171604 -0.6879741 -0.37011588 -0.049514122 0.094339006 0.27852181 0.34127131 0.33653972 0.062088452 -0.11562885 -0.0259488 0.034626372 -0.4940576 -0.96451664][-0.3989912 -0.41234958 -0.22296625 0.064179592 0.326694 0.75569952 1.046149 1.1700631 1.2635368 1.1240472 0.96119231 0.73111516 0.67258155 0.85193306 0.57556939][-0.46231863 -0.30351284 0.010506324 0.41456419 0.77753025 1.0634068 1.4234949 1.732603 1.8486679 1.8503118 1.9263431 1.6304384 1.4848424 1.4738795 1.6513923][-0.41552395 -0.17009333 0.17624024 0.58517545 1.0158175 1.4591825 1.8356208 2.1426029 2.3710377 2.3472672 2.2680507 2.0781181 2.0352528 1.6819133 1.8235551][-0.43016949 -0.14820477 0.22224137 0.5568009 0.92281681 1.3403999 1.7039164 2.0626128 2.3019459 2.3682992 2.3122215 2.0729823 2.1123114 1.6900618 1.6071627][-0.595942 -0.39663681 -0.048381679 0.30922157 0.66027033 0.97722524 1.269258 1.4818287 1.5561354 1.5405344 1.3496467 1.3523986 1.4166565 1.1659393 1.0424576][-0.82831007 -0.59252173 -0.34031922 -0.019612886 0.28128868 0.51313621 0.62167072 0.68719673 0.71869564 0.65242 0.42076772 0.1597414 -0.022477448 -8.7730587e-05 0.034251139][-0.73728895 -0.5709942 -0.42164633 -0.28504226 -0.14532679 -0.015392996 0.049985297 0.063263543 0.0099285394 -0.091825031 -0.17545685 -0.3702445 -0.6179902 -0.88955593 -0.95401257][-0.61655688 -0.54719412 -0.4997175 -0.42552692 -0.39408648 -0.29657218 -0.24213138 -0.14071502 -0.16535839 -0.19786571 -0.23726755 -0.20223489 -0.516223 -0.83287632 -1.0765262][-0.34954858 -0.265417 -0.16716534 -0.097598575 -0.086445026 -0.14067101 -0.19075552 -0.23282078 -0.31588078 -0.33099976 -0.39823681 -0.4627077 -0.58827174 -0.76549411 -0.956754][-0.042810112 -0.021474876 0.018484384 0.042841204 0.10906575 0.069690146 -0.0066403374 -0.076151915 -0.12151057 -0.11804361 -0.1552449 -0.26132938 -0.37439397 -0.55602062 -1.0477825]]...]
INFO - root - 2017-12-06 07:28:49.130222: step 33810, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.653 sec/batch; 54h:12m:32s remains)
INFO - root - 2017-12-06 07:28:55.517786: step 33820, loss = 0.85, batch loss = 0.64 (12.1 examples/sec; 0.660 sec/batch; 54h:44m:23s remains)
INFO - root - 2017-12-06 07:29:01.921874: step 33830, loss = 0.78, batch loss = 0.56 (12.9 examples/sec; 0.622 sec/batch; 51h:37m:26s remains)
INFO - root - 2017-12-06 07:29:08.167541: step 33840, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.658 sec/batch; 54h:34m:56s remains)
INFO - root - 2017-12-06 07:29:14.718724: step 33850, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.653 sec/batch; 54h:08m:55s remains)
INFO - root - 2017-12-06 07:29:21.150210: step 33860, loss = 0.78, batch loss = 0.57 (12.7 examples/sec; 0.632 sec/batch; 52h:26m:28s remains)
INFO - root - 2017-12-06 07:29:27.651918: step 33870, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.667 sec/batch; 55h:17m:24s remains)
INFO - root - 2017-12-06 07:29:34.140511: step 33880, loss = 0.84, batch loss = 0.63 (11.9 examples/sec; 0.672 sec/batch; 55h:43m:39s remains)
INFO - root - 2017-12-06 07:29:40.559245: step 33890, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.646 sec/batch; 53h:35m:36s remains)
INFO - root - 2017-12-06 07:29:47.043906: step 33900, loss = 0.84, batch loss = 0.63 (12.1 examples/sec; 0.663 sec/batch; 54h:59m:16s remains)
2017-12-06 07:29:47.701891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2062187 -2.3231776 -2.4880772 -2.7010489 -2.9887998 -3.3283496 -3.556226 -3.6182837 -3.4945 -3.1426585 -2.8013387 -2.5667777 -2.395705 -2.0910733 -1.747089][-3.1851165 -3.0942502 -3.0425005 -3.364418 -3.8176596 -4.1678157 -4.5042415 -4.9182267 -5.1341944 -5.051434 -4.684298 -4.0422587 -3.5786376 -3.1498728 -2.5845454][-3.040906 -2.9947865 -2.8954463 -2.803942 -2.8607178 -3.3786011 -3.9150844 -4.3982425 -4.82018 -5.1842527 -5.3941417 -5.3478427 -5.069973 -4.4332108 -3.9085038][-2.0609486 -2.1492281 -2.1280074 -2.1039875 -2.0286851 -1.8790189 -1.8855145 -2.4066637 -2.9769707 -3.5873349 -4.0877566 -4.8176622 -5.2718792 -5.4440217 -5.4287992][-0.84024775 -0.89697641 -0.95619076 -0.90119416 -0.68572372 -0.53934813 -0.34184316 -0.18727569 -0.27478662 -0.9948408 -1.8645003 -2.5615404 -3.2704682 -4.1947784 -5.0499239][0.10364229 -0.054557793 -0.22639167 -0.092381649 0.13729143 0.37025443 0.73007542 1.0337436 1.2115768 1.323887 1.1119766 0.44737998 -0.57309657 -1.7506193 -2.8443897][0.76935059 0.60285664 0.36640143 0.22374579 0.27812806 0.65318227 1.0857469 1.3991946 1.7263496 2.1785672 2.50064 2.756952 2.4158857 1.5778831 0.56542361][0.95395625 0.77691567 0.47724715 0.18435964 0.064082287 0.10329878 0.29783022 0.70157212 1.1942786 1.7435415 2.3149951 3.2894533 3.9905746 3.7565958 3.01393][0.298097 0.40386018 0.29558876 0.038743757 -0.15512767 -0.28723598 -0.40345711 -0.47178006 -0.13342218 0.64697349 1.4591444 2.6144068 3.8537416 4.5097113 4.5589261][-0.315227 -0.35958809 -0.40139052 -0.41763169 -0.56819546 -0.74722135 -0.98321807 -1.2026316 -1.2619355 -0.82473296 -0.23640227 0.761158 1.9758883 2.9951439 3.5786982][-1.0234669 -1.0056115 -1.1574695 -1.1896324 -1.3671683 -1.5310717 -1.5897914 -1.9137697 -2.234041 -2.3159652 -2.0067277 -1.3198497 -0.40965244 0.65599829 1.6640972][-1.3358761 -1.5128111 -1.7241369 -1.8657502 -2.0660014 -2.1995025 -2.4602871 -2.5220623 -2.9287822 -3.3040133 -3.5553124 -3.6641426 -3.5989454 -2.8652124 -1.8778607][-1.3508412 -1.6986344 -2.0340412 -2.3589222 -2.7084863 -2.7937167 -3.0560093 -3.1856079 -3.4860435 -3.7463002 -4.0007658 -4.2839146 -4.6622238 -4.6245279 -4.3246427][-1.0073922 -1.407411 -1.9000553 -2.29663 -2.5832987 -2.924176 -3.3251581 -3.3362758 -3.4623852 -3.7292113 -4.0332465 -4.3360372 -4.6976562 -4.802474 -4.9869952][-0.77700579 -0.98363304 -1.2152588 -1.680396 -2.1536303 -2.4207451 -2.7336609 -3.3592391 -3.7479162 -3.9275074 -3.6647613 -3.7594166 -4.1605706 -4.0817189 -4.0142045]]...]
INFO - root - 2017-12-06 07:29:54.285851: step 33910, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.652 sec/batch; 54h:03m:43s remains)
INFO - root - 2017-12-06 07:30:00.784268: step 33920, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.650 sec/batch; 53h:55m:29s remains)
INFO - root - 2017-12-06 07:30:07.214920: step 33930, loss = 0.95, batch loss = 0.73 (12.6 examples/sec; 0.636 sec/batch; 52h:44m:53s remains)
INFO - root - 2017-12-06 07:30:13.681011: step 33940, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.653 sec/batch; 54h:11m:11s remains)
INFO - root - 2017-12-06 07:30:20.126893: step 33950, loss = 0.87, batch loss = 0.65 (12.6 examples/sec; 0.634 sec/batch; 52h:35m:10s remains)
INFO - root - 2017-12-06 07:30:26.644792: step 33960, loss = 0.95, batch loss = 0.73 (11.5 examples/sec; 0.696 sec/batch; 57h:42m:48s remains)
INFO - root - 2017-12-06 07:30:33.212798: step 33970, loss = 0.75, batch loss = 0.54 (12.6 examples/sec; 0.633 sec/batch; 52h:27m:36s remains)
INFO - root - 2017-12-06 07:30:39.656369: step 33980, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.639 sec/batch; 52h:58m:46s remains)
INFO - root - 2017-12-06 07:30:46.189603: step 33990, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.643 sec/batch; 53h:21m:26s remains)
INFO - root - 2017-12-06 07:30:52.683026: step 34000, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.654 sec/batch; 54h:13m:25s remains)
2017-12-06 07:30:53.337754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.78934 -0.70899284 -0.63726228 -0.59478927 -0.40208858 -0.4270153 -0.2272929 -0.11615418 0.026906148 0.037118405 0.05176276 0.15533765 0.22089526 0.19741857 0.13637693][-0.92668092 -0.84466952 -0.73121387 -0.63288856 -0.44722149 -0.29937649 -0.058663726 0.027969956 0.1256483 0.16427392 0.31542724 0.29110649 0.25324023 0.19983163 0.094301641][-0.61771661 -0.50131214 -0.51038927 -0.52476346 -0.46821332 -0.41267028 -0.22051069 -0.057348296 0.087906688 0.15736368 0.21713588 0.12957655 0.11102244 0.12893894 0.14501043][-0.44734386 -0.36479095 -0.45366466 -0.42972618 -0.32313061 -0.30381411 -0.17286822 -0.10378085 -0.055676606 -0.10531652 -0.12152846 -0.19844514 -0.23740056 -0.191481 -0.14520647][-0.34483838 -0.16420189 -0.1943998 -0.24779743 -0.28370234 -0.2709578 -0.14307347 -0.11009713 0.04327251 0.068989411 0.025658369 -0.081535786 -0.12356664 -0.215236 -0.19660392][-0.30795032 -0.15528893 -0.082479022 -0.070545681 -0.059032556 -0.082247689 -0.14341187 -0.0921226 0.059575081 0.053134426 0.044620737 -0.023868024 -0.13178167 -0.22185335 -0.2426738][-0.20449248 -0.1472512 -0.067343272 -0.096623808 -0.050530475 0.022433765 0.062501967 0.061560154 0.11459996 0.12792534 0.12772758 0.11640269 0.080201671 -0.028410401 -0.1777423][-0.12426265 -0.07396961 -0.061250154 -0.0071795583 0.14221446 0.15179247 0.20212409 0.22404575 0.21223655 0.199424 0.17242113 0.2140018 0.28264463 0.18261796 0.066924348][-0.093156286 -0.039235074 0.054574549 0.19958371 0.35325605 0.44532079 0.54740667 0.51515359 0.49748826 0.39415914 0.24691483 0.19630027 0.22422886 0.15069291 0.074348345][-0.29090369 -0.19800496 -0.0596357 0.1832425 0.47887582 0.70903134 0.866682 0.92828155 0.95210326 0.813236 0.64414829 0.4503811 0.33072582 0.20912957 0.13775855][-0.1009798 -0.0067767128 0.10390639 0.23643875 0.39812815 0.63709974 0.82179713 0.89928591 0.93311155 0.79893309 0.709902 0.57125366 0.45398045 0.2341269 0.10957542][0.26437005 0.31784776 0.34178814 0.42551756 0.56664944 0.63958806 0.64373279 0.64901781 0.61158282 0.56058276 0.46771872 0.38966432 0.31060278 0.18750554 0.14122605][0.30315819 0.28762087 0.30464888 0.2998597 0.30238509 0.3179954 0.32218271 0.34393859 0.32724911 0.28690252 0.25044596 0.2485553 0.18637425 0.13238437 0.12798803][-0.046597764 -0.013184309 -0.010816336 -0.010680147 -0.03960349 0.0059851855 0.069048688 0.064797252 0.058961704 0.036897883 0.033433884 0.057151049 0.065618679 0.077590287 0.047433212][-0.07531409 -0.0091842413 0.036175475 0.022214897 0.0033008307 -0.0035777539 -0.032578658 -0.012354277 0.016007937 0.011168309 0.014326304 0.017843582 0.05885531 0.070133239 0.067043573]]...]
INFO - root - 2017-12-06 07:30:59.785423: step 34010, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.651 sec/batch; 53h:58m:05s remains)
INFO - root - 2017-12-06 07:31:06.282942: step 34020, loss = 0.78, batch loss = 0.57 (12.8 examples/sec; 0.627 sec/batch; 51h:59m:53s remains)
INFO - root - 2017-12-06 07:31:12.604536: step 34030, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 52h:54m:38s remains)
INFO - root - 2017-12-06 07:31:19.117848: step 34040, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.649 sec/batch; 53h:50m:21s remains)
INFO - root - 2017-12-06 07:31:25.653860: step 34050, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.644 sec/batch; 53h:24m:59s remains)
INFO - root - 2017-12-06 07:31:32.127242: step 34060, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.657 sec/batch; 54h:28m:42s remains)
INFO - root - 2017-12-06 07:31:38.626691: step 34070, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.647 sec/batch; 53h:36m:36s remains)
INFO - root - 2017-12-06 07:31:45.100826: step 34080, loss = 0.84, batch loss = 0.63 (12.9 examples/sec; 0.620 sec/batch; 51h:24m:46s remains)
INFO - root - 2017-12-06 07:31:51.564215: step 34090, loss = 0.75, batch loss = 0.53 (12.8 examples/sec; 0.625 sec/batch; 51h:48m:11s remains)
INFO - root - 2017-12-06 07:31:57.904978: step 34100, loss = 0.81, batch loss = 0.60 (13.2 examples/sec; 0.605 sec/batch; 50h:07m:27s remains)
2017-12-06 07:31:58.645122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.087511286 -0.088495046 -0.091893122 -0.097696796 -0.10540643 -0.11328444 -0.1183922 -0.11891276 -0.11514033 -0.10875203 -0.1014798 -0.09519913 -0.091762222 -0.092254527 -0.10113983][-0.08816471 -0.093306907 -0.10829505 -0.13514817 -0.17376843 -0.21819139 -0.25355303 -0.26894167 -0.25900298 -0.22827798 -0.18728706 -0.14896554 -0.12145227 -0.10904137 -0.1156085][-0.089664519 -0.10116157 -0.12926781 -0.18042752 -0.25599381 -0.34695992 -0.43313149 -0.48814493 -0.4922041 -0.44262043 -0.35758379 -0.26603052 -0.19062041 -0.14472555 -0.13222894][-0.091364413 -0.10690162 -0.13734296 -0.18444873 -0.25716528 -0.34633669 -0.44866744 -0.53943169 -0.590325 -0.57417983 -0.50068426 -0.39167061 -0.27909133 -0.19514006 -0.15036914][-0.091890261 -0.10570533 -0.12657917 -0.15346459 -0.19924366 -0.25239542 -0.34843031 -0.4544968 -0.55372822 -0.59667474 -0.57030821 -0.48116773 -0.35747117 -0.24460337 -0.158748][-0.091919206 -0.098167323 -0.093156256 -0.054788429 -0.017206013 0.016249776 -0.03077171 -0.12978797 -0.28985694 -0.42180058 -0.48654932 -0.47311845 -0.38612565 -0.27442509 -0.1653423][-0.0898647 -0.084338322 -0.052249741 0.032935448 0.14171767 0.28543046 0.32745078 0.27434981 0.10081749 -0.096733928 -0.25679943 -0.34001309 -0.3235116 -0.25015691 -0.15440667][-0.08579234 -0.074259095 -0.037876472 0.064881511 0.18067351 0.31064922 0.35202488 0.33589903 0.20934191 0.033440836 -0.13136794 -0.23955548 -0.25105256 -0.20742264 -0.13531315][-0.084924459 -0.070260033 -0.031212237 0.059393235 0.1802392 0.32119983 0.38120115 0.36899549 0.26177806 0.0994293 -0.051543504 -0.16439924 -0.19347137 -0.16962893 -0.11923625][-0.083684534 -0.072060019 -0.040002372 0.045776851 0.14587975 0.24837732 0.30062535 0.31221214 0.241166 0.1023948 -0.031142622 -0.12728715 -0.15018168 -0.13068452 -0.10219573][-0.083897375 -0.075246453 -0.049949989 0.012215622 0.093922548 0.17702043 0.20909202 0.21270278 0.15553522 0.050763331 -0.042694148 -0.11336111 -0.12710413 -0.11911147 -0.10968506][-0.086514644 -0.085104443 -0.0757652 -0.04294239 0.009941496 0.053271167 0.076323889 0.086636029 0.055468239 -0.015828758 -0.068067685 -0.10562211 -0.10643844 -0.099592626 -0.093985036][-0.088969395 -0.093587786 -0.098294929 -0.092686661 -0.070948318 -0.04572615 -0.029383231 -0.031322569 -0.059818506 -0.093609057 -0.1061711 -0.11148766 -0.1002106 -0.0927717 -0.08601933][-0.0886851 -0.092564367 -0.10079913 -0.10752766 -0.10923 -0.11114793 -0.10087328 -0.095336631 -0.10269652 -0.11155466 -0.12164387 -0.11938635 -0.10278028 -0.092842259 -0.083716273][-0.088697143 -0.089061409 -0.091660857 -0.097043142 -0.10446771 -0.12080331 -0.1215997 -0.12924944 -0.13362017 -0.1275343 -0.12888977 -0.12045787 -0.10832767 -0.096434623 -0.083378717]]...]
INFO - root - 2017-12-06 07:32:05.147429: step 34110, loss = 0.74, batch loss = 0.53 (12.0 examples/sec; 0.667 sec/batch; 55h:19m:01s remains)
INFO - root - 2017-12-06 07:32:11.553999: step 34120, loss = 0.78, batch loss = 0.57 (11.5 examples/sec; 0.697 sec/batch; 57h:44m:55s remains)
INFO - root - 2017-12-06 07:32:18.122668: step 34130, loss = 0.93, batch loss = 0.72 (11.7 examples/sec; 0.682 sec/batch; 56h:30m:22s remains)
INFO - root - 2017-12-06 07:32:24.592599: step 34140, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.639 sec/batch; 52h:59m:53s remains)
INFO - root - 2017-12-06 07:32:31.082610: step 34150, loss = 0.95, batch loss = 0.73 (12.5 examples/sec; 0.639 sec/batch; 52h:59m:42s remains)
INFO - root - 2017-12-06 07:32:37.601643: step 34160, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.646 sec/batch; 53h:33m:11s remains)
INFO - root - 2017-12-06 07:32:44.053332: step 34170, loss = 0.79, batch loss = 0.57 (12.7 examples/sec; 0.629 sec/batch; 52h:09m:00s remains)
INFO - root - 2017-12-06 07:32:50.585505: step 34180, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.658 sec/batch; 54h:29m:07s remains)
INFO - root - 2017-12-06 07:32:57.025711: step 34190, loss = 0.94, batch loss = 0.73 (12.6 examples/sec; 0.634 sec/batch; 52h:31m:37s remains)
INFO - root - 2017-12-06 07:33:03.597285: step 34200, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 54h:49m:29s remains)
2017-12-06 07:33:04.277607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.7646476 -1.8031876 -1.7134905 -1.542194 -1.2164624 -1.0655918 -0.66939652 -0.21287018 -0.063359767 0.1916804 0.22823107 0.30522421 0.039856032 -0.33252406 -0.59090781][-1.7155466 -1.9442028 -1.9868526 -2.004564 -1.5738682 -0.85623932 -0.41833889 -0.0085234344 -0.037199214 -0.19285715 -0.49526522 -0.52187181 -0.66769505 -0.40043366 -0.39151868][-1.2655506 -1.5124857 -1.614063 -1.530215 -1.5559529 -1.4109472 -1.2920271 -1.2768159 -1.4568186 -1.7037922 -1.6398987 -1.7219795 -1.7779555 -1.446799 -1.3102436][0.052162051 -0.42518225 -0.80480289 -0.95922756 -1.1417817 -1.0916017 -1.0626045 -1.1264535 -1.4256488 -1.9830737 -2.4019485 -2.4912024 -2.3727274 -2.1395223 -2.1546991][1.2196636 0.75150573 -0.05430644 -0.66512108 -0.98392427 -1.1840895 -1.4517813 -1.3114904 -1.262966 -1.3727541 -1.5437243 -1.8925012 -2.2340157 -2.1980338 -1.6532437][1.7550342 1.8768237 1.3290075 0.56487525 -0.19236802 -0.47265798 -0.52619684 -0.7076323 -0.53574288 -0.54105711 -0.59092903 -0.9416039 -1.2183536 -1.9267746 -2.0841813][1.9367007 2.2692587 2.0799243 1.7396953 1.0732795 0.52099311 0.32152322 0.66536665 1.2546024 1.3744595 1.4789826 0.96642518 0.43824133 0.030029774 -0.40717518][1.7172741 2.0130408 1.7202135 1.7615701 1.3746996 1.166177 1.2952738 1.6352422 2.3663352 2.9253719 3.256094 2.9039259 2.690026 1.9469758 1.3186481][0.90335953 1.3064276 1.5410802 1.3896518 1.2804191 1.1438156 0.90551329 1.3356352 2.0079141 2.7639434 3.362633 3.6771369 3.6620746 3.6426022 3.3716731][-2.0364127 -1.0054588 -0.081457883 0.36628875 0.30190316 0.25752503 0.28911233 0.45360091 0.80058587 1.558601 2.2132885 2.6949527 3.1493764 3.4040554 3.6195483][-3.3135757 -3.0821033 -2.362026 -1.1930443 -0.34504119 -0.25065312 -0.325172 -0.29218102 -0.066425107 0.13220508 0.3986403 0.6070267 1.1487089 1.5621133 1.9276282][-2.4775846 -2.9655271 -2.9763205 -2.8365154 -2.1925373 -1.6165909 -0.944247 -0.72250283 -0.59132493 -0.39443141 -0.23748657 -0.21566895 -0.36177239 -0.22111689 0.019376166][-2.0477924 -2.3029015 -2.1526327 -2.4038825 -2.6060929 -2.4631872 -1.95134 -1.6295959 -1.2703379 -1.1176884 -1.0865084 -0.92250788 -0.93564403 -1.0706749 -0.81506264][-1.1738979 -1.6594603 -2.0938289 -2.007302 -2.0436997 -2.4843938 -2.413908 -2.1349 -1.7231175 -1.4909593 -1.2651151 -1.232749 -1.354321 -1.3911881 -1.2671636][-0.86678636 -1.0782564 -1.4597197 -2.0154576 -2.3275349 -2.3370886 -2.5169425 -2.431627 -2.3955948 -2.0782373 -1.7456087 -1.465278 -1.3120782 -1.3405147 -1.2304407]]...]
INFO - root - 2017-12-06 07:33:10.696149: step 34210, loss = 0.84, batch loss = 0.62 (12.0 examples/sec; 0.669 sec/batch; 55h:24m:44s remains)
INFO - root - 2017-12-06 07:33:17.239706: step 34220, loss = 0.82, batch loss = 0.60 (11.8 examples/sec; 0.676 sec/batch; 56h:00m:10s remains)
INFO - root - 2017-12-06 07:33:23.768023: step 34230, loss = 0.86, batch loss = 0.65 (11.9 examples/sec; 0.671 sec/batch; 55h:36m:54s remains)
INFO - root - 2017-12-06 07:33:30.339075: step 34240, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.662 sec/batch; 54h:51m:07s remains)
INFO - root - 2017-12-06 07:33:36.920305: step 34250, loss = 0.90, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 52h:17m:54s remains)
INFO - root - 2017-12-06 07:33:43.492139: step 34260, loss = 0.81, batch loss = 0.59 (12.1 examples/sec; 0.659 sec/batch; 54h:33m:31s remains)
INFO - root - 2017-12-06 07:33:49.925272: step 34270, loss = 0.87, batch loss = 0.65 (12.6 examples/sec; 0.636 sec/batch; 52h:42m:16s remains)
INFO - root - 2017-12-06 07:33:56.468969: step 34280, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.660 sec/batch; 54h:40m:49s remains)
INFO - root - 2017-12-06 07:34:03.061864: step 34290, loss = 0.86, batch loss = 0.65 (11.7 examples/sec; 0.683 sec/batch; 56h:35m:51s remains)
INFO - root - 2017-12-06 07:34:09.488865: step 34300, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.660 sec/batch; 54h:39m:26s remains)
2017-12-06 07:34:10.174432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064342991 -0.064105526 -0.064139679 -0.064169146 -0.0641983 -0.064223751 -0.064233691 -0.064203627 -0.064128444 -0.064022712 -0.063930571 -0.063879684 -0.063848019 -0.063810892 -0.063854679][-0.064381473 -0.064107388 -0.0641544 -0.064199731 -0.064225309 -0.064261444 -0.064310469 -0.064297855 -0.064231381 -0.064122453 -0.063995063 -0.063916974 -0.063885517 -0.063867338 -0.063893072][-0.064448878 -0.064062446 -0.0641614 -0.0642225 -0.064223155 -0.06422466 -0.064279042 -0.06428238 -0.064241484 -0.064187244 -0.064057574 -0.06394738 -0.063899085 -0.063892543 -0.063889191][-0.06443505 -0.063998774 -0.064186424 -0.064328082 -0.064352885 -0.064325795 -0.064282291 -0.064226225 -0.064198852 -0.064211272 -0.064151742 -0.064050488 -0.063991182 -0.063983545 -0.064001739][-0.064410947 -0.063964956 -0.06419453 -0.064433977 -0.064520769 -0.064486548 -0.064334743 -0.064178482 -0.06413424 -0.064186461 -0.064227611 -0.064147711 -0.06408634 -0.064078562 -0.064137645][-0.064472429 -0.063978046 -0.064163253 -0.064456448 -0.06463825 -0.064680442 -0.064507961 -0.064316861 -0.064204246 -0.064214446 -0.06424921 -0.064156525 -0.064092591 -0.064098187 -0.064234063][-0.064681619 -0.064077392 -0.06419716 -0.064512216 -0.064796939 -0.065024056 -0.06489367 -0.064670451 -0.064479671 -0.064440578 -0.064355537 -0.064140156 -0.064029217 -0.064075768 -0.06434644][-0.0649886 -0.064352527 -0.064457037 -0.064803489 -0.065212481 -0.065642416 -0.065687329 -0.065559596 -0.065375552 -0.065185294 -0.06493178 -0.064462885 -0.064164013 -0.064099334 -0.06439168][-0.065312281 -0.064776637 -0.064951785 -0.065341845 -0.065923519 -0.066558376 -0.066871375 -0.066831246 -0.066631004 -0.066222556 -0.065729171 -0.065030932 -0.06446524 -0.06419979 -0.064380117][-0.065563262 -0.065188162 -0.065513894 -0.066031009 -0.066805162 -0.067632444 -0.068160713 -0.068272687 -0.068056419 -0.0675666 -0.066819325 -0.065822355 -0.064916685 -0.064379632 -0.064455137][-0.065690756 -0.065405108 -0.065894395 -0.06657882 -0.067532815 -0.068566561 -0.069338858 -0.069702171 -0.06961254 -0.069114029 -0.0681021 -0.066802561 -0.065562494 -0.064719543 -0.064652286][-0.0657416 -0.065501407 -0.066087537 -0.06687659 -0.067963183 -0.0691422 -0.070139475 -0.070797116 -0.07091628 -0.070510685 -0.069448538 -0.06803143 -0.066608906 -0.065536484 -0.065179341][-0.0657921 -0.065575771 -0.066155709 -0.066928454 -0.067998938 -0.069177881 -0.070284128 -0.071168669 -0.071569674 -0.071373083 -0.070465967 -0.069124505 -0.067686416 -0.066441558 -0.065777391][-0.065882258 -0.0656243 -0.066139862 -0.066799641 -0.067728907 -0.068776309 -0.069816254 -0.070695296 -0.071183547 -0.071171351 -0.070495814 -0.069359176 -0.068074077 -0.066913769 -0.066242129][-0.066056795 -0.065734074 -0.0661194 -0.06660255 -0.06732069 -0.068164825 -0.069032982 -0.069798969 -0.070225269 -0.070283681 -0.069846995 -0.069044426 -0.06806311 -0.067126185 -0.066588014]]...]
INFO - root - 2017-12-06 07:34:16.487671: step 34310, loss = 0.69, batch loss = 0.48 (12.2 examples/sec; 0.656 sec/batch; 54h:22m:28s remains)
INFO - root - 2017-12-06 07:34:23.024775: step 34320, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 52h:57m:13s remains)
INFO - root - 2017-12-06 07:34:29.511873: step 34330, loss = 0.78, batch loss = 0.56 (11.4 examples/sec; 0.703 sec/batch; 58h:14m:28s remains)
INFO - root - 2017-12-06 07:34:35.988957: step 34340, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.641 sec/batch; 53h:06m:38s remains)
INFO - root - 2017-12-06 07:34:42.465380: step 34350, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 54h:34m:53s remains)
INFO - root - 2017-12-06 07:34:48.984094: step 34360, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.671 sec/batch; 55h:34m:19s remains)
INFO - root - 2017-12-06 07:34:55.444218: step 34370, loss = 0.95, batch loss = 0.74 (12.1 examples/sec; 0.661 sec/batch; 54h:45m:37s remains)
INFO - root - 2017-12-06 07:35:01.892031: step 34380, loss = 0.78, batch loss = 0.56 (15.1 examples/sec; 0.528 sec/batch; 43h:43m:52s remains)
INFO - root - 2017-12-06 07:35:08.308371: step 34390, loss = 0.77, batch loss = 0.55 (11.8 examples/sec; 0.676 sec/batch; 55h:58m:15s remains)
INFO - root - 2017-12-06 07:35:14.777124: step 34400, loss = 0.76, batch loss = 0.55 (11.9 examples/sec; 0.671 sec/batch; 55h:31m:50s remains)
2017-12-06 07:35:15.472158: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0017437637 -0.067795932 -0.087628677 -0.088524207 -0.082247272 -0.093776412 -0.088960573 -0.093497463 -0.096243732 -0.098954678 -0.10343648 -0.098988421 -0.10323531 -0.10175233 -0.095680878][0.024047889 -0.046522789 -0.0843336 -0.09001565 -0.0820763 -0.093584575 -0.085911505 -0.0938635 -0.095611863 -0.096740641 -0.10308488 -0.099618033 -0.10518025 -0.10503957 -0.098121643][0.038815841 -0.034530055 -0.081449673 -0.090411529 -0.083427019 -0.091089919 -0.085792057 -0.091516078 -0.0949879 -0.095067404 -0.10168175 -0.098058172 -0.10442533 -0.11125743 -0.11001527][0.045158565 -0.033512451 -0.081436232 -0.083299324 -0.083353892 -0.08422301 -0.086206824 -0.090490483 -0.095678858 -0.088206343 -0.099687427 -0.096340105 -0.1011526 -0.11516623 -0.12408784][0.062772706 -0.020081475 -0.08405517 -0.085048027 -0.085432976 -0.085339926 -0.084928647 -0.092449807 -0.094899945 -0.093452349 -0.096572757 -0.094443537 -0.096855223 -0.11779358 -0.13503803][0.057277009 -0.020966925 -0.071695752 -0.093372732 -0.0878039 -0.090951264 -0.09175095 -0.097055763 -0.097816437 -0.10060427 -0.10078361 -0.099804178 -0.10141608 -0.11695071 -0.14279011][0.034138471 -0.026043676 -0.068830632 -0.088570371 -0.09245842 -0.090146132 -0.093398236 -0.099171326 -0.096169293 -0.098963924 -0.096082054 -0.097601354 -0.097450256 -0.11783042 -0.14824465][0.031768017 -0.026163071 -0.072364286 -0.086915955 -0.083226375 -0.0855072 -0.085712954 -0.094441436 -0.088096954 -0.093145348 -0.088983193 -0.090205848 -0.088690542 -0.1052823 -0.15184774][0.01352983 -0.03428749 -0.05923932 -0.071131632 -0.07684508 -0.083251722 -0.080278724 -0.090937756 -0.08077465 -0.091156565 -0.086209148 -0.08136823 -0.080903508 -0.10803212 -0.15356296][0.0018398613 -0.037716392 -0.057477679 -0.064454556 -0.071450874 -0.07950744 -0.082905784 -0.086370774 -0.082094327 -0.095889159 -0.084951349 -0.078838021 -0.078526273 -0.1165083 -0.15416574][0.0020056292 -0.043448433 -0.069158874 -0.064212881 -0.078284942 -0.080168135 -0.080237813 -0.084501751 -0.0797233 -0.085972264 -0.083219379 -0.078546107 -0.081391223 -0.11992857 -0.14568475][0.0018696561 -0.035327654 -0.067660674 -0.070249386 -0.080036119 -0.0752811 -0.073408552 -0.085402191 -0.080699421 -0.081563413 -0.084421009 -0.081103012 -0.081692308 -0.11069009 -0.14658624][-0.0083933771 -0.032191891 -0.071038231 -0.082315788 -0.079255417 -0.078910328 -0.081663989 -0.0874301 -0.084106162 -0.084661677 -0.0840028 -0.083310224 -0.083806619 -0.11157064 -0.15373209][-0.016714528 -0.038329225 -0.078217775 -0.08530727 -0.080484331 -0.074281313 -0.086588293 -0.090109654 -0.086603768 -0.083306774 -0.086207077 -0.090726249 -0.089966811 -0.11881044 -0.15865904][-0.04739558 -0.052619874 -0.079450056 -0.084071293 -0.080829054 -0.082316972 -0.080681264 -0.090184987 -0.086890236 -0.086483374 -0.088201329 -0.093687914 -0.09561222 -0.12670794 -0.18396504]]...]
INFO - root - 2017-12-06 07:35:21.956794: step 34410, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 53h:56m:42s remains)
INFO - root - 2017-12-06 07:35:28.355205: step 34420, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.625 sec/batch; 51h:46m:39s remains)
INFO - root - 2017-12-06 07:35:34.852470: step 34430, loss = 0.80, batch loss = 0.59 (11.8 examples/sec; 0.679 sec/batch; 56h:10m:41s remains)
INFO - root - 2017-12-06 07:35:41.240354: step 34440, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 53h:08m:53s remains)
INFO - root - 2017-12-06 07:35:47.849994: step 34450, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.656 sec/batch; 54h:18m:24s remains)
INFO - root - 2017-12-06 07:35:54.431920: step 34460, loss = 0.95, batch loss = 0.74 (12.3 examples/sec; 0.648 sec/batch; 53h:37m:53s remains)
INFO - root - 2017-12-06 07:36:00.851844: step 34470, loss = 0.87, batch loss = 0.65 (12.3 examples/sec; 0.651 sec/batch; 53h:53m:34s remains)
INFO - root - 2017-12-06 07:36:07.378956: step 34480, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 53h:28m:08s remains)
INFO - root - 2017-12-06 07:36:13.771945: step 34490, loss = 0.79, batch loss = 0.57 (12.1 examples/sec; 0.664 sec/batch; 54h:55m:48s remains)
INFO - root - 2017-12-06 07:36:20.275846: step 34500, loss = 0.81, batch loss = 0.59 (11.5 examples/sec; 0.697 sec/batch; 57h:39m:35s remains)
2017-12-06 07:36:20.899587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040234752 -0.039881084 -0.039765816 -0.039658617 -0.0395664 -0.039499406 -0.039469816 -0.039487988 -0.039533608 -0.039607469 -0.039725035 -0.03984962 -0.039967265 -0.040089216 -0.040611353][-0.039021637 -0.038426723 -0.03811644 -0.037821446 -0.037562829 -0.037361667 -0.037246171 -0.037252508 -0.037374519 -0.03761458 -0.037973031 -0.038384717 -0.038779575 -0.039135959 -0.039812338][-0.037732523 -0.036683563 -0.036113009 -0.035567436 -0.03507518 -0.034673858 -0.034408778 -0.034344103 -0.034531839 -0.034966148 -0.035640322 -0.03646902 -0.037299745 -0.038049333 -0.039008334][-0.036079217 -0.034638431 -0.033780579 -0.032962549 -0.032200359 -0.031547349 -0.031076305 -0.030889582 -0.031083565 -0.031692948 -0.032709483 -0.034014497 -0.035377327 -0.036642 -0.038093437][-0.034162723 -0.032524519 -0.03142181 -0.030399542 -0.029433236 -0.028584674 -0.027917862 -0.027585447 -0.02772028 -0.028419413 -0.029719926 -0.031446643 -0.033325538 -0.035133034 -0.037180483][-0.032656737 -0.030785762 -0.029572751 -0.028486092 -0.027458467 -0.026538581 -0.025752947 -0.02528052 -0.02530925 -0.025981143 -0.0273915 -0.029347856 -0.031563431 -0.033776481 -0.036408979][-0.031906992 -0.029902313 -0.028761275 -0.027791016 -0.026890904 -0.026051618 -0.025255613 -0.024698079 -0.024603948 -0.02513773 -0.026421696 -0.028348476 -0.030642975 -0.032990955 -0.035860017][-0.032008614 -0.02996926 -0.029018007 -0.028285239 -0.027652673 -0.027043693 -0.026411556 -0.025904432 -0.025734477 -0.026077531 -0.027091287 -0.028740685 -0.030792359 -0.032952677 -0.03580096][-0.032678977 -0.030713692 -0.029970873 -0.029475451 -0.029142108 -0.028862055 -0.028533537 -0.028240208 -0.028122023 -0.028337516 -0.029043771 -0.030226935 -0.031780444 -0.033511274 -0.035998669][-0.033568602 -0.031894132 -0.031342633 -0.031046938 -0.03097244 -0.03099373 -0.030992378 -0.030991171 -0.031055234 -0.031261619 -0.031717703 -0.032439545 -0.033434957 -0.034599908 -0.036521647][-0.034740448 -0.033250451 -0.032839954 -0.032645844 -0.032689042 -0.032881096 -0.03311158 -0.033353053 -0.033643875 -0.033948466 -0.034296878 -0.034725681 -0.035279468 -0.035920557 -0.037286755][-0.035703219 -0.034368973 -0.034034882 -0.033823039 -0.033813596 -0.033990875 -0.034295302 -0.034688234 -0.035164364 -0.035618577 -0.036002491 -0.036346871 -0.03669668 -0.037022378 -0.038039681][-0.03643674 -0.035239868 -0.034925908 -0.034656923 -0.034516551 -0.034545105 -0.034768097 -0.035147563 -0.035664596 -0.036187578 -0.036636151 -0.037003994 -0.037306171 -0.037535507 -0.038406894][-0.037266754 -0.03617496 -0.035881836 -0.035576772 -0.035337862 -0.035222385 -0.035289329 -0.035539124 -0.035949845 -0.036406565 -0.036832009 -0.037190843 -0.037483435 -0.037691336 -0.038436554][-0.038316119 -0.037295602 -0.037089948 -0.036825471 -0.036565889 -0.036380455 -0.036329295 -0.03643731 -0.036700021 -0.037037048 -0.037341692 -0.0376481 -0.0379116 -0.038097531 -0.038781006]]...]
INFO - root - 2017-12-06 07:36:27.373759: step 34510, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.648 sec/batch; 53h:39m:36s remains)
INFO - root - 2017-12-06 07:36:33.759479: step 34520, loss = 0.82, batch loss = 0.60 (15.2 examples/sec; 0.525 sec/batch; 43h:28m:53s remains)
INFO - root - 2017-12-06 07:36:40.271855: step 34530, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.661 sec/batch; 54h:41m:13s remains)
INFO - root - 2017-12-06 07:36:46.808099: step 34540, loss = 0.83, batch loss = 0.61 (11.9 examples/sec; 0.672 sec/batch; 55h:38m:02s remains)
INFO - root - 2017-12-06 07:36:53.301069: step 34550, loss = 0.83, batch loss = 0.61 (12.8 examples/sec; 0.627 sec/batch; 51h:52m:13s remains)
INFO - root - 2017-12-06 07:36:59.829604: step 34560, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.619 sec/batch; 51h:14m:22s remains)
INFO - root - 2017-12-06 07:37:06.426083: step 34570, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.659 sec/batch; 54h:30m:06s remains)
INFO - root - 2017-12-06 07:37:12.881023: step 34580, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.668 sec/batch; 55h:18m:41s remains)
INFO - root - 2017-12-06 07:37:19.310211: step 34590, loss = 0.83, batch loss = 0.61 (11.5 examples/sec; 0.695 sec/batch; 57h:28m:25s remains)
INFO - root - 2017-12-06 07:37:25.762472: step 34600, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.639 sec/batch; 52h:53m:55s remains)
2017-12-06 07:37:26.387528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07757511 -0.076592088 -0.075215623 -0.073514521 -0.071685672 -0.069695972 -0.067886025 -0.066476151 -0.065429144 -0.064788423 -0.064340033 -0.064197376 -0.064209118 -0.064194486 -0.064366505][-0.08268863 -0.08133734 -0.079391658 -0.076875612 -0.074088514 -0.071066923 -0.068350166 -0.066217557 -0.06471397 -0.063879229 -0.063439161 -0.063401967 -0.063591279 -0.063660651 -0.063642755][-0.089189731 -0.087607168 -0.084669426 -0.081122309 -0.076896347 -0.07222297 -0.068031833 -0.065010831 -0.063148506 -0.062247816 -0.061879851 -0.062033854 -0.062397458 -0.062539443 -0.062384382][-0.094446152 -0.092430137 -0.0892389 -0.084531575 -0.078476295 -0.072277725 -0.06681858 -0.06262742 -0.060437698 -0.059551828 -0.0592024 -0.059144873 -0.059994914 -0.060379669 -0.060280316][-0.094727606 -0.091370821 -0.088543557 -0.083562456 -0.076862223 -0.069697894 -0.063092411 -0.058582518 -0.056086618 -0.055211384 -0.0549476 -0.055355273 -0.056515913 -0.056912627 -0.057088289][-0.0926986 -0.087727644 -0.084176131 -0.079141617 -0.071902715 -0.064145744 -0.057605829 -0.053212613 -0.050823711 -0.0500647 -0.050162081 -0.05063498 -0.051876284 -0.052499179 -0.053020984][-0.086672254 -0.081092745 -0.077925533 -0.073080458 -0.065700062 -0.058647692 -0.052665956 -0.04800117 -0.045848042 -0.045514803 -0.04573141 -0.046365656 -0.047279358 -0.047875736 -0.048794467][-0.076993488 -0.07150723 -0.068675444 -0.064554535 -0.05881422 -0.052857962 -0.047852993 -0.044334054 -0.042318217 -0.042080615 -0.042399853 -0.042898729 -0.043672625 -0.044095248 -0.044972382][-0.066473678 -0.063334264 -0.061569076 -0.058176767 -0.053460017 -0.049115755 -0.045466349 -0.043110095 -0.041375011 -0.040688135 -0.040799886 -0.041166969 -0.041797787 -0.042081874 -0.0432316][-0.059207376 -0.05705582 -0.056154124 -0.053919621 -0.050723385 -0.04800209 -0.045640964 -0.043861862 -0.042147093 -0.041366275 -0.041255098 -0.041572016 -0.042121697 -0.0425153 -0.043756716][-0.054383479 -0.053731129 -0.053283788 -0.051943712 -0.049924042 -0.048413448 -0.04706496 -0.045726109 -0.044589683 -0.043939263 -0.043762255 -0.044059139 -0.044509605 -0.044915881 -0.0461696][-0.052442271 -0.052846517 -0.052722547 -0.052215967 -0.051301204 -0.050364755 -0.049497835 -0.048821334 -0.048305161 -0.047907311 -0.047783464 -0.047980998 -0.048374444 -0.048879 -0.050122473][-0.053946413 -0.054039732 -0.054052107 -0.054112535 -0.053854637 -0.053571649 -0.053295236 -0.053153105 -0.052971773 -0.052840158 -0.052799143 -0.05299193 -0.053349648 -0.05396273 -0.055069767][-0.056683362 -0.057211917 -0.057434268 -0.057097245 -0.057351433 -0.057277516 -0.057262041 -0.057318516 -0.0572725 -0.057227343 -0.057198916 -0.057455491 -0.057900164 -0.05855187 -0.05957536][-0.060520831 -0.06068429 -0.060970671 -0.060720354 -0.060538173 -0.060734179 -0.060777221 -0.060866617 -0.060875259 -0.060859449 -0.060872454 -0.061089728 -0.061418474 -0.061981834 -0.062938131]]...]
INFO - root - 2017-12-06 07:37:32.835826: step 34610, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.661 sec/batch; 54h:42m:24s remains)
INFO - root - 2017-12-06 07:37:39.300143: step 34620, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.650 sec/batch; 53h:45m:11s remains)
INFO - root - 2017-12-06 07:37:45.825471: step 34630, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.656 sec/batch; 54h:16m:49s remains)
INFO - root - 2017-12-06 07:37:52.286174: step 34640, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 53h:49m:36s remains)
INFO - root - 2017-12-06 07:37:58.673353: step 34650, loss = 0.80, batch loss = 0.58 (12.7 examples/sec; 0.628 sec/batch; 51h:56m:39s remains)
INFO - root - 2017-12-06 07:38:05.168080: step 34660, loss = 0.75, batch loss = 0.54 (12.8 examples/sec; 0.625 sec/batch; 51h:43m:08s remains)
INFO - root - 2017-12-06 07:38:11.527770: step 34670, loss = 0.88, batch loss = 0.67 (13.1 examples/sec; 0.611 sec/batch; 50h:32m:24s remains)
INFO - root - 2017-12-06 07:38:17.830851: step 34680, loss = 0.85, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 52h:02m:21s remains)
INFO - root - 2017-12-06 07:38:24.392861: step 34690, loss = 0.85, batch loss = 0.63 (11.9 examples/sec; 0.674 sec/batch; 55h:46m:39s remains)
INFO - root - 2017-12-06 07:38:30.846906: step 34700, loss = 0.73, batch loss = 0.52 (12.5 examples/sec; 0.642 sec/batch; 53h:08m:11s remains)
2017-12-06 07:38:31.476454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.82976878 -0.87975675 -0.95465112 -1.0445989 -1.1379479 -1.2149558 -1.3416233 -1.4561113 -1.4710099 -1.4535162 -1.4366727 -1.3762314 -1.3075289 -1.2247467 -1.1584381][-1.4033201 -1.4192359 -1.480976 -1.5811727 -1.6941341 -1.8147652 -1.909766 -2.0556047 -2.1398902 -2.1219044 -2.1198134 -2.0782661 -2.026443 -1.9127946 -1.7854334][-1.5077044 -1.6037679 -1.6865503 -1.7374595 -1.7970644 -1.8575015 -1.9227166 -1.9824175 -2.0242724 -2.0312927 -2.0715179 -2.1154323 -2.2336261 -2.2938552 -2.2341731][-1.5218503 -1.5692056 -1.5856586 -1.6005546 -1.58744 -1.5105677 -1.4351575 -1.4193772 -1.4554986 -1.4890707 -1.5521418 -1.6341221 -1.7103453 -1.7367318 -1.7026941][-1.1698812 -1.1065241 -1.0902125 -1.0086355 -0.91815358 -0.91230142 -0.88220114 -0.81271213 -0.75528044 -0.7571916 -0.77651191 -0.78638732 -0.7446413 -0.70158249 -0.62311548][-0.64966071 -0.53681684 -0.46762875 -0.38846281 -0.35897911 -0.45095402 -0.52291071 -0.61700422 -0.69745719 -0.67485905 -0.65245795 -0.54958719 -0.37916103 -0.21416575 -0.078861766][-0.41068202 -0.32590336 -0.22681364 -0.075494729 0.018861763 0.076904543 0.11919776 0.18273076 0.24836767 0.28017741 0.31387562 0.42362106 0.55621082 0.62867445 0.68690664][0.6363951 0.68393624 0.68673927 0.78644973 0.91600907 1.0434968 1.1414617 1.2228938 1.3058709 1.3982652 1.4963688 1.573698 1.6382914 1.5964292 1.5353202][1.7806504 1.8154508 1.730876 1.7225252 1.6924659 1.7489266 1.8157066 1.8515509 1.9055392 1.9568303 2.0392957 2.199914 2.3323667 2.3100059 2.2249122][2.1668267 2.3534992 2.4273362 2.3557897 2.270339 2.3813298 2.3796587 2.319293 2.2790985 2.2590983 2.263381 2.3035753 2.335917 2.3480754 2.3330128][2.1649537 2.3146911 2.3632262 2.4362419 2.5150466 2.5393777 2.5298474 2.600774 2.5725064 2.4507113 2.3112092 2.2027605 2.1226158 1.9302294 1.7441924][2.2278221 2.3979616 2.4448161 2.4786055 2.4724884 2.4695871 2.5014787 2.4592514 2.3647017 2.2539451 2.0093663 1.685488 1.3989383 1.0883791 0.81294936][2.1349707 2.0962765 1.9780149 1.8436432 1.8011212 1.7509952 1.730406 1.6021737 1.4451836 1.2655685 1.0304378 0.75135344 0.47301114 0.22716683 0.03776855][2.0535808 1.7942317 1.5512277 1.3060521 1.166267 1.1217583 1.0273726 0.832485 0.639514 0.44209719 0.27370554 0.18260962 0.093398638 0.12123636 0.21687597][1.689423 1.4525446 1.1028692 0.80224353 0.59272295 0.64779663 0.77702272 0.77385473 0.72438139 0.50044936 0.2567749 0.12791082 0.10389683 0.20935133 0.40357664]]...]
INFO - root - 2017-12-06 07:38:38.020388: step 34710, loss = 0.71, batch loss = 0.50 (12.5 examples/sec; 0.640 sec/batch; 52h:58m:39s remains)
INFO - root - 2017-12-06 07:38:44.526969: step 34720, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 54h:08m:53s remains)
INFO - root - 2017-12-06 07:38:50.940247: step 34730, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.628 sec/batch; 51h:54m:52s remains)
INFO - root - 2017-12-06 07:38:57.400440: step 34740, loss = 0.79, batch loss = 0.58 (13.0 examples/sec; 0.615 sec/batch; 50h:50m:07s remains)
INFO - root - 2017-12-06 07:39:03.863455: step 34750, loss = 0.75, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 54h:32m:11s remains)
INFO - root - 2017-12-06 07:39:10.410672: step 34760, loss = 0.86, batch loss = 0.64 (12.7 examples/sec; 0.631 sec/batch; 52h:11m:12s remains)
INFO - root - 2017-12-06 07:39:16.799594: step 34770, loss = 0.86, batch loss = 0.64 (12.9 examples/sec; 0.618 sec/batch; 51h:07m:14s remains)
INFO - root - 2017-12-06 07:39:23.214171: step 34780, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.648 sec/batch; 53h:37m:00s remains)
INFO - root - 2017-12-06 07:39:29.692663: step 34790, loss = 0.83, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 54h:47m:45s remains)
INFO - root - 2017-12-06 07:39:36.193022: step 34800, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.657 sec/batch; 54h:17m:29s remains)
2017-12-06 07:39:36.845095: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31064329 0.58336192 0.86884892 1.029242 1.0070448 0.903364 0.80987096 0.68313324 0.42115733 0.14005658 -0.0090042278 0.10991286 0.36572543 0.55985653 0.5114032][0.39439696 0.70752639 1.0109295 1.1109706 0.97454661 0.76679659 0.50524229 0.29584482 0.061194547 -0.13839516 -0.20160913 -0.18826592 -0.092064217 0.077641316 0.090903886][0.29004997 0.47982517 0.68493539 0.84080917 0.8100571 0.51201552 0.16395399 -0.082126 -0.19623584 -0.26275432 -0.26374698 -0.21188356 -0.10316729 -0.081706993 -0.13754478][-0.05238428 0.066179074 0.2718651 0.39115348 0.40844029 0.2415283 -0.017594635 -0.35745722 -0.63375831 -0.69386011 -0.58210617 -0.50563711 -0.45995528 -0.4145835 -0.35651755][-0.059319988 -0.098532505 -0.093122929 0.0076102912 0.19209385 0.27248052 0.17730874 -0.1197976 -0.44428873 -0.66952014 -0.70927674 -0.62489736 -0.53165269 -0.47418773 -0.41185206][-0.030675434 -0.028979685 -0.0044276491 0.10608868 0.37688786 0.73675317 0.88282478 0.73428321 0.41938254 0.018467098 -0.28340229 -0.37203395 -0.37700561 -0.35709068 -0.35797247][0.0057165548 0.011408307 0.057956062 0.25505969 0.60606796 1.1303506 1.468733 1.505875 1.2782238 0.89902222 0.54206789 0.2627424 0.074146919 -0.047029361 -0.21211503][-0.051507786 -0.026357226 0.064187013 0.31848127 0.758937 1.3363156 1.6499977 1.7371479 1.6149116 1.3254497 0.96746486 0.5512619 0.22981805 0.036426179 -0.11476835][-0.099991366 -0.026144929 0.065984823 0.26752397 0.58841461 0.994458 1.3102341 1.4015683 1.289463 1.0467961 0.72250623 0.32902128 0.0089727342 -0.20300275 -0.3212142][-0.18809101 -0.12539929 -0.03238098 0.053033344 0.18971267 0.41773215 0.62023616 0.71312505 0.66493642 0.46340027 0.17176682 -0.14616601 -0.3941882 -0.49047849 -0.48301774][-0.18930954 -0.20138803 -0.18787852 -0.13771898 -0.099167131 -0.020972565 0.078941591 0.1462386 0.095388822 -0.067938723 -0.24589205 -0.41306219 -0.47121164 -0.47106421 -0.42235962][-0.15529352 -0.18704301 -0.2101008 -0.22059149 -0.22346461 -0.20205429 -0.19665465 -0.22047934 -0.25409186 -0.3365891 -0.37374502 -0.37281618 -0.36724162 -0.31084633 -0.2559171][-0.10669366 -0.11557311 -0.15766676 -0.21663436 -0.2770007 -0.31761891 -0.34023109 -0.35426331 -0.37467209 -0.39648733 -0.3042253 -0.21480632 -0.1697353 -0.10750993 -0.095850229][-0.093986921 -0.094280265 -0.095954522 -0.12756501 -0.18219039 -0.25077683 -0.31813306 -0.35931334 -0.3794567 -0.3775849 -0.31151554 -0.13384688 -0.055778876 -0.020843521 -0.0052889287][-0.090154529 -0.09934745 -0.12002399 -0.14255993 -0.17179078 -0.19962269 -0.21907318 -0.25530058 -0.28214455 -0.28602037 -0.23978415 -0.10899936 -0.042090021 -0.011335127 0.0029252172]]...]
INFO - root - 2017-12-06 07:39:43.206442: step 34810, loss = 0.89, batch loss = 0.67 (12.3 examples/sec; 0.651 sec/batch; 53h:51m:32s remains)
INFO - root - 2017-12-06 07:39:49.647607: step 34820, loss = 0.77, batch loss = 0.55 (13.0 examples/sec; 0.614 sec/batch; 50h:44m:34s remains)
INFO - root - 2017-12-06 07:39:56.165931: step 34830, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 54h:10m:04s remains)
INFO - root - 2017-12-06 07:40:02.671017: step 34840, loss = 0.85, batch loss = 0.63 (12.4 examples/sec; 0.645 sec/batch; 53h:18m:16s remains)
INFO - root - 2017-12-06 07:40:09.214304: step 34850, loss = 0.79, batch loss = 0.58 (11.9 examples/sec; 0.670 sec/batch; 55h:22m:46s remains)
INFO - root - 2017-12-06 07:40:15.738567: step 34860, loss = 0.78, batch loss = 0.56 (11.9 examples/sec; 0.675 sec/batch; 55h:48m:40s remains)
INFO - root - 2017-12-06 07:40:22.152101: step 34870, loss = 0.82, batch loss = 0.60 (12.7 examples/sec; 0.628 sec/batch; 51h:56m:47s remains)
INFO - root - 2017-12-06 07:40:28.595462: step 34880, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.647 sec/batch; 53h:28m:34s remains)
INFO - root - 2017-12-06 07:40:35.099456: step 34890, loss = 0.84, batch loss = 0.62 (12.6 examples/sec; 0.635 sec/batch; 52h:31m:27s remains)
INFO - root - 2017-12-06 07:40:41.522833: step 34900, loss = 0.75, batch loss = 0.53 (12.6 examples/sec; 0.635 sec/batch; 52h:28m:40s remains)
2017-12-06 07:40:42.151608: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.09059041 -0.091267735 -0.090636753 -0.089049436 -0.086608529 -0.082907028 -0.078391992 -0.075375378 -0.074212022 -0.073538043 -0.074395478 -0.07744953 -0.080848664 -0.084423706 -0.087837622][-0.090235531 -0.090836614 -0.090240315 -0.088786893 -0.0871966 -0.084084705 -0.079591058 -0.076046742 -0.074194387 -0.072670266 -0.072633013 -0.075769596 -0.079225175 -0.082975835 -0.086560823][-0.08953362 -0.089336917 -0.088126943 -0.086469516 -0.085385323 -0.083266251 -0.079870209 -0.076975904 -0.075229689 -0.073453188 -0.072917327 -0.075481638 -0.078638233 -0.08231923 -0.085828505][-0.088511676 -0.086915582 -0.08450453 -0.0823275 -0.08230187 -0.082025543 -0.080916658 -0.079934038 -0.078730553 -0.077262245 -0.076483242 -0.077545293 -0.07962767 -0.082774773 -0.085796326][-0.086993665 -0.083882205 -0.079908177 -0.0770449 -0.07764826 -0.079194777 -0.080165379 -0.081282571 -0.081688367 -0.081190296 -0.080804236 -0.0814198 -0.082347006 -0.08444228 -0.08657819][-0.085382506 -0.081305586 -0.07603053 -0.071982242 -0.071699679 -0.07388974 -0.076000549 -0.078492247 -0.080302306 -0.081134439 -0.082226895 -0.083894283 -0.0850378 -0.086471654 -0.087656163][-0.084607475 -0.080353327 -0.074618459 -0.06944374 -0.067226008 -0.068692744 -0.07126008 -0.074405834 -0.075845137 -0.076151639 -0.0794317 -0.084468193 -0.087517522 -0.088714868 -0.089025371][-0.085184529 -0.081120193 -0.075890526 -0.070645392 -0.0669664 -0.066295758 -0.06841521 -0.0712812 -0.070925027 -0.067564085 -0.070570126 -0.0795567 -0.086742848 -0.089332409 -0.08938773][-0.086474985 -0.082957163 -0.078723319 -0.074527994 -0.0709905 -0.068853587 -0.069187388 -0.070121229 -0.065988176 -0.056566395 -0.057621382 -0.069416448 -0.08157213 -0.087778345 -0.088704951][-0.088385671 -0.085729644 -0.082642071 -0.079676084 -0.077133782 -0.075076558 -0.073998034 -0.073398046 -0.066540286 -0.05111032 -0.048136644 -0.061679773 -0.078451365 -0.087484494 -0.088332415][-0.089772619 -0.087860286 -0.085978188 -0.084215939 -0.082945168 -0.081875958 -0.080949649 -0.081079684 -0.075638331 -0.059881642 -0.052789018 -0.060310323 -0.074891135 -0.08597973 -0.087889247][-0.090057112 -0.088531747 -0.087374106 -0.086622871 -0.086628422 -0.086963885 -0.087069511 -0.088348791 -0.084596969 -0.071016639 -0.061972495 -0.064024068 -0.07341551 -0.083923779 -0.087680385][-0.089450136 -0.0880865 -0.087284356 -0.087089472 -0.087503307 -0.088544108 -0.089543156 -0.092563614 -0.094130844 -0.089446992 -0.083916388 -0.081527837 -0.083035439 -0.087149523 -0.088813022][-0.089098915 -0.087718293 -0.08710482 -0.087164946 -0.0877484 -0.088799328 -0.090096369 -0.093651891 -0.09763182 -0.098687127 -0.098133154 -0.096020959 -0.093199395 -0.0914312 -0.090237506][-0.089469455 -0.087833777 -0.0872299 -0.087196358 -0.087836422 -0.088816 -0.089559041 -0.090192974 -0.090686619 -0.090600058 -0.090671688 -0.09085951 -0.090724722 -0.090193674 -0.089675359]]...]
INFO - root - 2017-12-06 07:40:48.494392: step 34910, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.660 sec/batch; 54h:35m:55s remains)
INFO - root - 2017-12-06 07:40:55.001789: step 34920, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.638 sec/batch; 52h:43m:37s remains)
INFO - root - 2017-12-06 07:41:01.491061: step 34930, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.639 sec/batch; 52h:48m:27s remains)
INFO - root - 2017-12-06 07:41:07.993119: step 34940, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.673 sec/batch; 55h:36m:23s remains)
INFO - root - 2017-12-06 07:41:14.280914: step 34950, loss = 0.92, batch loss = 0.70 (12.2 examples/sec; 0.655 sec/batch; 54h:09m:05s remains)
INFO - root - 2017-12-06 07:41:20.619653: step 34960, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.644 sec/batch; 53h:13m:37s remains)
INFO - root - 2017-12-06 07:41:27.142781: step 34970, loss = 0.86, batch loss = 0.64 (12.2 examples/sec; 0.654 sec/batch; 54h:05m:27s remains)
INFO - root - 2017-12-06 07:41:33.615167: step 34980, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.655 sec/batch; 54h:06m:09s remains)
INFO - root - 2017-12-06 07:41:40.125375: step 34990, loss = 0.75, batch loss = 0.54 (12.3 examples/sec; 0.649 sec/batch; 53h:39m:43s remains)
INFO - root - 2017-12-06 07:41:46.656343: step 35000, loss = 0.84, batch loss = 0.62 (11.7 examples/sec; 0.685 sec/batch; 56h:38m:40s remains)
2017-12-06 07:41:47.258688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08679495 -0.087471731 -0.090758666 -0.097762384 -0.10844177 -0.12059681 -0.13118845 -0.13545486 -0.13067465 -0.11892106 -0.10540126 -0.10232083 -0.12023404 -0.15582077 -0.16234973][-0.087536849 -0.091302171 -0.10299298 -0.12462834 -0.15191638 -0.18127865 -0.20876119 -0.2243721 -0.21858224 -0.19026425 -0.15195677 -0.12900007 -0.13559458 -0.1656926 -0.16751459][-0.09031567 -0.10177731 -0.1284651 -0.16614053 -0.19945315 -0.23064239 -0.26941535 -0.30860698 -0.32346541 -0.29345182 -0.22883892 -0.17518683 -0.1613088 -0.1847581 -0.1955426][-0.096490242 -0.12004492 -0.16195488 -0.198511 -0.20475274 -0.20679875 -0.24123971 -0.31198102 -0.3770721 -0.37923038 -0.31006461 -0.23184848 -0.18982232 -0.19905221 -0.22060938][-0.10536159 -0.13869561 -0.1760605 -0.1756469 -0.11663757 -0.067122161 -0.092105336 -0.20966332 -0.3504312 -0.41345972 -0.37132603 -0.28507173 -0.211256 -0.1933367 -0.20647979][-0.11477385 -0.148096 -0.16486365 -0.103292 0.037689418 0.15509002 0.17022951 0.033726 -0.18430141 -0.34038803 -0.36860925 -0.31085974 -0.2230524 -0.18056652 -0.1770785][-0.12714288 -0.14880963 -0.13839254 -0.030889023 0.16920798 0.34738588 0.43018085 0.32426029 0.081184492 -0.15056355 -0.27282137 -0.28053913 -0.20509449 -0.15608528 -0.14640366][-0.14615136 -0.1544542 -0.13131244 -0.016334437 0.1937554 0.41084355 0.56238055 0.51544785 0.28809875 0.021495648 -0.17306268 -0.23953903 -0.1907458 -0.12822446 -0.11387111][-0.17090045 -0.16965327 -0.15070611 -0.052864596 0.12885821 0.34318411 0.51927191 0.53143466 0.36439115 0.12410265 -0.07746537 -0.18477224 -0.16876572 -0.1111981 -0.096123427][-0.19369555 -0.19718526 -0.19442356 -0.1440748 -0.011045367 0.17991866 0.3670969 0.44015545 0.34665966 0.17051561 -0.0040784031 -0.12018611 -0.1241058 -0.092491686 -0.088501088][-0.19969866 -0.202205 -0.19382054 -0.16851774 -0.096411072 0.018883005 0.15296587 0.22748072 0.18880574 0.094638929 -0.016150124 -0.099390738 -0.10247036 -0.0790197 -0.0822827][-0.18076724 -0.184484 -0.17891368 -0.17314453 -0.13796766 -0.0811228 -0.0052490756 0.043564722 0.033524483 -0.0090993345 -0.058417413 -0.09792798 -0.087362811 -0.072640307 -0.079015538][-0.15103444 -0.15300861 -0.14756691 -0.15288565 -0.14509776 -0.12795338 -0.08696913 -0.057531334 -0.061483949 -0.082336918 -0.0991326 -0.099214196 -0.083986849 -0.0729067 -0.074742451][-0.13409987 -0.12654105 -0.11438254 -0.12491107 -0.13484237 -0.14593011 -0.13119225 -0.11639245 -0.10665527 -0.11128972 -0.10948728 -0.099878959 -0.087576054 -0.077475034 -0.07546556][-0.120029 -0.11143529 -0.091059968 -0.083562762 -0.085703552 -0.087056562 -0.088200316 -0.0878707 -0.088793747 -0.092880495 -0.097021051 -0.094261386 -0.090357661 -0.0811237 -0.074474782]]...]
INFO - root - 2017-12-06 07:41:53.723075: step 35010, loss = 0.82, batch loss = 0.61 (12.6 examples/sec; 0.634 sec/batch; 52h:21m:07s remains)
INFO - root - 2017-12-06 07:42:00.311603: step 35020, loss = 0.88, batch loss = 0.66 (12.2 examples/sec; 0.658 sec/batch; 54h:20m:56s remains)
INFO - root - 2017-12-06 07:42:06.915263: step 35030, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.652 sec/batch; 53h:50m:07s remains)
INFO - root - 2017-12-06 07:42:13.475707: step 35040, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.647 sec/batch; 53h:29m:44s remains)
INFO - root - 2017-12-06 07:42:19.796668: step 35050, loss = 0.80, batch loss = 0.59 (14.5 examples/sec; 0.552 sec/batch; 45h:37m:02s remains)
INFO - root - 2017-12-06 07:42:26.300260: step 35060, loss = 0.79, batch loss = 0.57 (12.0 examples/sec; 0.668 sec/batch; 55h:12m:06s remains)
INFO - root - 2017-12-06 07:42:32.727706: step 35070, loss = 0.93, batch loss = 0.72 (12.7 examples/sec; 0.632 sec/batch; 52h:10m:38s remains)
INFO - root - 2017-12-06 07:42:39.275379: step 35080, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.645 sec/batch; 53h:19m:05s remains)
INFO - root - 2017-12-06 07:42:45.566077: step 35090, loss = 0.91, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 52h:51m:46s remains)
INFO - root - 2017-12-06 07:42:52.005581: step 35100, loss = 0.77, batch loss = 0.55 (12.7 examples/sec; 0.629 sec/batch; 51h:57m:58s remains)
2017-12-06 07:42:52.657576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.4790861 -0.47842383 -0.41917476 -0.34751973 -0.22424126 -0.14668787 -0.16733289 -0.23232079 -0.29620579 -0.34148088 -0.33931234 -0.32195196 -0.29117337 -0.22450003 -0.12422965][-0.51412296 -0.44748235 -0.34232861 -0.23391145 -0.10957287 -0.057710566 -0.056387026 -0.10766187 -0.14905542 -0.12938443 -0.038882151 0.062501915 0.15960187 0.21134001 0.16939428][-0.44739312 -0.37572506 -0.25247088 -0.18440312 -0.093320265 0.022679366 0.12305052 0.16923863 0.16200909 0.091073506 0.04943762 0.074082993 0.13833106 0.17730364 0.17191517][-0.17509295 -0.11048884 0.0084725767 0.072267957 0.10062543 0.11086792 0.11452129 0.16624585 0.22776696 0.26623642 0.2860882 0.26803496 0.25122517 0.29123515 0.31443986][-0.0077967644 0.064937793 0.11404034 0.13258183 0.10105809 0.016263641 -0.097492874 -0.10347308 -0.054807223 0.026719727 0.15323064 0.26466107 0.30681297 0.28299531 0.21259308][0.087616049 0.1289252 0.16310582 0.19657853 0.14737925 0.0693733 -0.077887319 -0.17695233 -0.18703139 -0.12833072 -0.042629004 0.02439934 0.094809107 0.087201916 0.061685078][0.085686617 0.16497043 0.22976714 0.26671731 0.28719041 0.23985362 0.15325451 0.022273481 -0.09579622 -0.16543886 -0.18201765 -0.16085944 -0.11807559 -0.120024 -0.10001435][-0.04493152 0.0087270364 0.082829528 0.14970711 0.17069921 0.14569753 0.13185075 0.05545304 0.044223271 0.0050277561 -0.068181835 -0.12076344 -0.15941429 -0.18433172 -0.17264727][-0.15836614 -0.12260395 -0.075422309 -0.033385631 -0.0089911371 -0.0016311482 0.023012124 0.0091216192 0.03829854 0.018286742 -0.0065159127 -0.031420022 -0.0812028 -0.073833615 -0.038429156][-0.2039564 -0.17500025 -0.1365606 -0.11506733 -0.10108767 -0.1017736 -0.10312777 -0.10436441 -0.059009578 -0.027374759 -0.0091981068 -0.0034932122 0.0055710375 0.039807416 0.089331277][-0.19828553 -0.14039674 -0.0905927 -0.055697348 -0.036106769 -0.017929249 -0.022587694 -0.046101917 -0.062039144 -0.063561983 -0.042938411 -0.035639234 -0.0057469234 0.033435531 0.0650722][-0.081199549 -0.023169056 0.021439523 0.035329707 0.022983126 0.0033575147 -0.018469281 -0.012837477 -0.012317672 -0.027601883 -0.027441598 -0.0036696196 0.02458588 0.054909132 0.082724549][-0.10295641 -0.053519655 -0.0073480159 0.0095806941 0.0049897283 -0.014283188 -0.030359697 -0.038926553 -0.024846494 -0.025473297 -0.030680589 -0.049690105 -0.02546566 0.0073265284 0.037275307][-0.076615147 -0.078173436 -0.080350786 -0.079075329 -0.077934347 -0.082404509 -0.079912148 -0.08032281 -0.0775315 -0.058861461 -0.036694147 -0.027126104 -0.0094906911 0.0085485205 0.00093535334][0.0056848377 -0.020333536 -0.054456677 -0.067563884 -0.081896082 -0.097980075 -0.10432374 -0.10150286 -0.089526482 -0.070843905 -0.036911156 -0.034633707 -0.019783773 0.042488538 0.028922483]]...]
INFO - root - 2017-12-06 07:42:59.139793: step 35110, loss = 0.81, batch loss = 0.59 (12.7 examples/sec; 0.631 sec/batch; 52h:09m:45s remains)
INFO - root - 2017-12-06 07:43:05.644285: step 35120, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 52h:44m:02s remains)
INFO - root - 2017-12-06 07:43:12.255401: step 35130, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.655 sec/batch; 54h:07m:01s remains)
INFO - root - 2017-12-06 07:43:18.866361: step 35140, loss = 0.78, batch loss = 0.56 (12.0 examples/sec; 0.665 sec/batch; 54h:57m:55s remains)
INFO - root - 2017-12-06 07:43:25.192211: step 35150, loss = 0.74, batch loss = 0.53 (12.3 examples/sec; 0.650 sec/batch; 53h:40m:47s remains)
INFO - root - 2017-12-06 07:43:31.798568: step 35160, loss = 0.76, batch loss = 0.55 (12.2 examples/sec; 0.654 sec/batch; 54h:01m:47s remains)
INFO - root - 2017-12-06 07:43:38.370350: step 35170, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 54h:04m:29s remains)
INFO - root - 2017-12-06 07:43:44.866956: step 35180, loss = 0.77, batch loss = 0.55 (12.4 examples/sec; 0.647 sec/batch; 53h:25m:31s remains)
INFO - root - 2017-12-06 07:43:51.411380: step 35190, loss = 0.75, batch loss = 0.54 (12.7 examples/sec; 0.629 sec/batch; 51h:54m:33s remains)
INFO - root - 2017-12-06 07:43:57.978500: step 35200, loss = 0.89, batch loss = 0.67 (11.8 examples/sec; 0.680 sec/batch; 56h:09m:33s remains)
2017-12-06 07:43:58.617967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071239822 -0.0661504 -0.060999222 -0.04594221 -0.063819155 -0.12718409 -0.21976715 -0.29424036 -0.35465637 -0.37792498 -0.3767848 -0.3545233 -0.31016237 -0.26578292 -0.19434804][-0.11526214 -0.13965406 -0.14037898 -0.11242226 -0.12125739 -0.19336152 -0.28665933 -0.3856889 -0.48329139 -0.53777117 -0.57470685 -0.58434188 -0.55250412 -0.48597428 -0.40785652][-0.16496781 -0.20525081 -0.19982302 -0.15538491 -0.14118181 -0.18290655 -0.21284509 -0.29174265 -0.41980082 -0.50219476 -0.57732821 -0.62472057 -0.62676674 -0.58492637 -0.53212583][-0.18007305 -0.20204379 -0.15179078 -0.073340021 -0.0093953237 -0.0040097907 0.0020852983 -0.025078863 -0.14327003 -0.25567964 -0.37154457 -0.46237022 -0.48036495 -0.45314324 -0.4439733][-0.14052123 -0.10065863 0.030250013 0.1416485 0.26372585 0.36266676 0.43881059 0.40689167 0.30273536 0.19540128 0.06605307 -0.078235738 -0.13120075 -0.16448325 -0.20558128][-0.053594571 0.053251378 0.23309043 0.44764227 0.64091724 0.76674652 0.82979876 0.82767922 0.70769233 0.53986681 0.38081381 0.17294523 0.045909218 -0.036547478 -0.091730468][0.041275151 0.22518286 0.44098437 0.70709169 0.97828579 1.18327 1.2242502 1.1504012 0.95387983 0.77298975 0.59294826 0.34448743 0.17783827 0.0532823 -0.021877497][0.089717261 0.30461511 0.55179542 0.82939136 1.0859433 1.2791795 1.3217062 1.2239159 0.9880805 0.77209991 0.58159548 0.36423868 0.13838172 -0.020842373 -0.13420019][0.084022827 0.29262882 0.55108023 0.824265 1.0178156 1.1152922 1.077046 0.96507549 0.76142472 0.62834376 0.49877864 0.30090341 0.078619443 -0.0985296 -0.24091932][0.054959454 0.22902471 0.44331861 0.65538985 0.79861087 0.76279312 0.65289944 0.48805988 0.25978225 0.15250957 0.12309962 0.042012297 -0.1001559 -0.22581771 -0.34739596][-0.018625803 0.11857707 0.30976602 0.47268325 0.55081344 0.43234777 0.306941 0.089255236 -0.14979133 -0.21630502 -0.21860227 -0.1935226 -0.24510312 -0.25850713 -0.25363982][-0.13202217 -0.038979262 0.11776037 0.27699202 0.32271603 0.200326 0.024767816 -0.20256951 -0.41516966 -0.45152798 -0.43439776 -0.36897796 -0.34733048 -0.26966411 -0.18788087][-0.23335296 -0.17035829 -0.061029781 0.1023891 0.1802159 0.091568254 -0.081169173 -0.32779968 -0.52261722 -0.57362872 -0.51155084 -0.41306654 -0.34267884 -0.19846985 -0.092813775][-0.14519297 -0.1258661 -0.048354205 0.08676485 0.12886143 0.074121363 -0.096125774 -0.31995204 -0.50020003 -0.57443577 -0.54206342 -0.44198549 -0.35048249 -0.18103783 -0.038396567][-0.084569052 -0.093310982 -0.011954658 0.11142278 0.15639102 0.086441346 -0.060169347 -0.23359188 -0.39424935 -0.48320255 -0.4695324 -0.39962658 -0.32763883 -0.15690187 -0.0052459314]]...]
INFO - root - 2017-12-06 07:44:05.171327: step 35210, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.657 sec/batch; 54h:13m:02s remains)
INFO - root - 2017-12-06 07:44:11.745954: step 35220, loss = 0.83, batch loss = 0.62 (12.1 examples/sec; 0.660 sec/batch; 54h:28m:12s remains)
INFO - root - 2017-12-06 07:44:18.152189: step 35230, loss = 0.73, batch loss = 0.52 (12.5 examples/sec; 0.642 sec/batch; 52h:58m:50s remains)
INFO - root - 2017-12-06 07:44:24.550572: step 35240, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.659 sec/batch; 54h:22m:47s remains)
INFO - root - 2017-12-06 07:44:31.189762: step 35250, loss = 0.82, batch loss = 0.61 (12.4 examples/sec; 0.647 sec/batch; 53h:26m:00s remains)
INFO - root - 2017-12-06 07:44:37.656625: step 35260, loss = 0.84, batch loss = 0.62 (12.9 examples/sec; 0.619 sec/batch; 51h:04m:32s remains)
INFO - root - 2017-12-06 07:44:44.307926: step 35270, loss = 0.80, batch loss = 0.59 (11.6 examples/sec; 0.691 sec/batch; 57h:05m:18s remains)
INFO - root - 2017-12-06 07:44:52.646068: step 35280, loss = 0.74, batch loss = 0.52 (9.6 examples/sec; 0.831 sec/batch; 68h:36m:27s remains)
INFO - root - 2017-12-06 07:45:00.983520: step 35290, loss = 0.81, batch loss = 0.59 (9.4 examples/sec; 0.853 sec/batch; 70h:23m:32s remains)
INFO - root - 2017-12-06 07:45:09.367715: step 35300, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.827 sec/batch; 68h:15m:29s remains)
2017-12-06 07:45:10.108105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.097479291 -0.098115593 -0.095941171 -0.092813581 -0.081828557 -0.069326624 -0.056867022 -0.049062677 -0.05282443 -0.065160133 -0.078776404 -0.089732438 -0.099504195 -0.10460584 -0.10933101][-0.096659385 -0.097666144 -0.098006092 -0.10216384 -0.098042473 -0.087743647 -0.073836796 -0.05304702 -0.046026893 -0.060238369 -0.0836529 -0.10508391 -0.12124451 -0.12730427 -0.12629545][-0.095593117 -0.0977763 -0.10442204 -0.12885854 -0.14959916 -0.15930822 -0.16782382 -0.157431 -0.1563575 -0.18027291 -0.22519785 -0.27666831 -0.31258959 -0.29925728 -0.26540816][-0.095329516 -0.10607545 -0.13893409 -0.20284513 -0.258888 -0.28567681 -0.29962933 -0.2831175 -0.2739746 -0.28237838 -0.31416202 -0.36614275 -0.43108481 -0.44278389 -0.40876412][-0.095572576 -0.11532706 -0.17014509 -0.25413066 -0.31932968 -0.32050121 -0.29034308 -0.21266222 -0.15110976 -0.11427292 -0.12073362 -0.1656244 -0.26371455 -0.34666032 -0.39707422][-0.094675735 -0.11289866 -0.16240577 -0.22765219 -0.25248235 -0.19101134 -0.087268479 0.051266119 0.15949775 0.24921252 0.25545532 0.19112308 0.024232589 -0.14096743 -0.27594393][-0.092589505 -0.10538101 -0.14196105 -0.17496717 -0.13695617 0.016165532 0.21009286 0.43104798 0.60554838 0.73868805 0.75308615 0.63386428 0.36706603 0.086950421 -0.15841922][-0.088005118 -0.0898904 -0.10215986 -0.085451141 0.018366821 0.23199512 0.48661649 0.76330793 0.98670512 1.1399279 1.1240343 0.93304461 0.56196004 0.16934679 -0.16483584][-0.086490147 -0.076600417 -0.056856845 -0.0051100254 0.13369171 0.37162817 0.6144622 0.82960624 0.99093932 1.1151896 1.0714602 0.85426486 0.47858286 0.0970083 -0.22462317][-0.09630724 -0.1011084 -0.11147739 -0.097507462 -0.0020532981 0.17673628 0.358249 0.47978354 0.55297226 0.54899174 0.42367381 0.22878312 -0.065955296 -0.33532259 -0.54609376][-0.11109792 -0.13706429 -0.17702237 -0.21379766 -0.19889608 -0.11829872 -0.038061861 -0.0040847436 -0.02250883 -0.07614892 -0.19117233 -0.34936821 -0.55635405 -0.71110934 -0.78422916][-0.14024031 -0.17544986 -0.21226373 -0.24503596 -0.26387954 -0.21786194 -0.18371232 -0.20301935 -0.27361059 -0.33201489 -0.42200345 -0.49879503 -0.58104789 -0.6373021 -0.64695019][-0.16179629 -0.20570788 -0.24844171 -0.27161014 -0.27875704 -0.228351 -0.18459207 -0.20711404 -0.25615636 -0.28697658 -0.32075191 -0.33030856 -0.36595464 -0.40019596 -0.42149562][-0.14947188 -0.17212586 -0.18876418 -0.17046607 -0.13293496 -0.05853644 -0.0045755729 0.017094709 -0.029900588 -0.0536321 -0.048564132 -0.036152411 -0.051545922 -0.10285682 -0.16645269][-0.11026847 -0.083245263 -0.033219717 0.058127224 0.14574961 0.23724549 0.29780465 0.32870311 0.30379677 0.30984098 0.33947486 0.37467736 0.41258609 0.3696354 0.29118752]]...]
INFO - root - 2017-12-06 07:45:18.416534: step 35310, loss = 0.90, batch loss = 0.68 (10.1 examples/sec; 0.795 sec/batch; 65h:39m:18s remains)
INFO - root - 2017-12-06 07:45:26.656847: step 35320, loss = 0.82, batch loss = 0.61 (9.5 examples/sec; 0.842 sec/batch; 69h:28m:41s remains)
INFO - root - 2017-12-06 07:45:34.959721: step 35330, loss = 0.82, batch loss = 0.60 (10.3 examples/sec; 0.773 sec/batch; 63h:48m:48s remains)
INFO - root - 2017-12-06 07:45:43.283983: step 35340, loss = 0.78, batch loss = 0.57 (9.4 examples/sec; 0.851 sec/batch; 70h:14m:22s remains)
INFO - root - 2017-12-06 07:45:51.548844: step 35350, loss = 0.85, batch loss = 0.64 (9.9 examples/sec; 0.807 sec/batch; 66h:35m:19s remains)
INFO - root - 2017-12-06 07:45:59.831088: step 35360, loss = 0.81, batch loss = 0.60 (9.4 examples/sec; 0.847 sec/batch; 69h:54m:30s remains)
INFO - root - 2017-12-06 07:46:07.977991: step 35370, loss = 0.80, batch loss = 0.59 (10.0 examples/sec; 0.802 sec/batch; 66h:11m:38s remains)
INFO - root - 2017-12-06 07:46:16.342102: step 35380, loss = 0.75, batch loss = 0.54 (9.4 examples/sec; 0.851 sec/batch; 70h:15m:51s remains)
INFO - root - 2017-12-06 07:46:24.499867: step 35390, loss = 0.81, batch loss = 0.60 (9.7 examples/sec; 0.827 sec/batch; 68h:15m:36s remains)
INFO - root - 2017-12-06 07:46:32.847813: step 35400, loss = 0.84, batch loss = 0.63 (9.9 examples/sec; 0.808 sec/batch; 66h:40m:43s remains)
2017-12-06 07:46:33.613047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.44459182 -0.62729239 -0.91087359 -1.2906804 -1.6733671 -2.0307643 -2.1961594 -2.3225796 -2.3893559 -2.296633 -2.0771706 -1.9110061 -1.7782786 -1.5798099 -1.4383917][-0.68715376 -0.85671574 -1.051775 -1.3976607 -1.7516772 -2.0966549 -2.3474517 -2.6598873 -2.8791821 -2.9352584 -2.8314857 -2.5782702 -2.434885 -2.309922 -2.2527847][-0.99628669 -1.1829346 -1.261398 -1.2616955 -1.2493262 -1.3558913 -1.5103946 -1.7841617 -2.1550193 -2.4037261 -2.4811618 -2.4368932 -2.5198591 -2.4392316 -2.3648703][-1.1257051 -1.4631878 -1.6330922 -1.5867169 -1.3245665 -0.97380883 -0.76640159 -0.75079536 -0.90597326 -1.1809784 -1.2891107 -1.2958263 -1.5216304 -1.8028474 -2.0897474][-0.97276759 -1.2135211 -1.3247803 -1.2934747 -1.0483606 -0.76014715 -0.42717808 -0.16655061 -0.027133241 -0.03324477 -0.004248932 -0.080961704 -0.33725798 -0.78817946 -1.276031][-0.45708996 -0.59424496 -0.59736741 -0.180395 0.40321136 0.71068537 0.79715568 0.67514485 0.66868895 0.43347532 0.3818844 0.41713929 0.38029563 0.11817902 -0.33007848][0.17770156 0.28190389 0.42141098 0.77975374 1.3896933 1.9652004 2.1280103 1.7305244 1.3356481 0.74699342 0.507765 0.37370911 0.26691282 0.27645439 0.26338479][0.608384 0.92942017 1.2592852 1.6932241 2.1164732 2.42669 2.398468 2.0582321 1.622744 0.9372887 0.55694681 0.21529612 0.05438859 0.30529544 0.75936353][0.32922128 0.64983481 1.1300119 1.5937437 1.9071858 2.0440886 1.8517991 1.4472469 0.97346836 0.33931085 -0.0629865 -0.2833018 -0.21325576 0.18838257 0.783137][-0.23356536 -0.16954577 -0.056245044 0.10494552 0.42637634 0.64069337 0.566864 0.26541874 -0.23266003 -0.61567575 -0.77020431 -0.77580631 -0.74974471 -0.13128455 0.81990242][-0.76776886 -0.89044106 -0.962456 -1.0545862 -1.093228 -0.98258114 -0.87984544 -0.89232635 -1.0372736 -1.2191563 -1.3097156 -1.2434585 -1.0876617 -0.72416788 0.092099033][-0.86184466 -1.1396983 -1.4092325 -1.5656182 -1.6922474 -1.6905771 -1.6347022 -1.5993332 -1.5552363 -1.5833492 -1.5296504 -1.3857867 -1.3413254 -1.1403375 -0.76303953][-0.56643182 -0.84386742 -1.1613387 -1.5261227 -1.7908103 -1.8802338 -1.8420768 -1.6868482 -1.5008863 -1.2967553 -1.2527496 -1.1873856 -1.2503114 -1.326995 -1.3916057][-0.24880034 -0.34317634 -0.5182972 -0.87262678 -1.3121008 -1.5386919 -1.617344 -1.5773939 -1.4257344 -1.2632003 -1.1560298 -1.1766497 -1.2863187 -1.5177336 -1.721418][-0.11943413 -0.11969207 -0.12979814 -0.39991933 -0.69160175 -0.93889856 -1.1343437 -1.1956422 -1.0959111 -1.0537789 -1.159337 -1.2296227 -1.4406787 -1.5120147 -1.6854532]]...]
INFO - root - 2017-12-06 07:46:41.901480: step 35410, loss = 0.86, batch loss = 0.64 (9.7 examples/sec; 0.821 sec/batch; 67h:47m:37s remains)
INFO - root - 2017-12-06 07:46:50.182509: step 35420, loss = 0.79, batch loss = 0.57 (9.7 examples/sec; 0.821 sec/batch; 67h:47m:19s remains)
INFO - root - 2017-12-06 07:46:58.332978: step 35430, loss = 0.87, batch loss = 0.65 (9.9 examples/sec; 0.804 sec/batch; 66h:23m:08s remains)
INFO - root - 2017-12-06 07:47:06.532970: step 35440, loss = 0.71, batch loss = 0.49 (9.9 examples/sec; 0.808 sec/batch; 66h:39m:12s remains)
INFO - root - 2017-12-06 07:47:14.823723: step 35450, loss = 0.72, batch loss = 0.51 (9.8 examples/sec; 0.816 sec/batch; 67h:21m:06s remains)
INFO - root - 2017-12-06 07:47:23.064381: step 35460, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.825 sec/batch; 68h:04m:27s remains)
INFO - root - 2017-12-06 07:47:31.359883: step 35470, loss = 0.91, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:17m:56s remains)
INFO - root - 2017-12-06 07:47:39.651672: step 35480, loss = 0.87, batch loss = 0.65 (9.7 examples/sec; 0.827 sec/batch; 68h:14m:05s remains)
INFO - root - 2017-12-06 07:47:47.931657: step 35490, loss = 0.88, batch loss = 0.67 (9.5 examples/sec; 0.845 sec/batch; 69h:44m:43s remains)
INFO - root - 2017-12-06 07:47:56.347705: step 35500, loss = 0.81, batch loss = 0.59 (9.8 examples/sec; 0.813 sec/batch; 67h:02m:22s remains)
2017-12-06 07:47:57.081711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082481287 -0.082589127 -0.082578816 -0.082584493 -0.083305918 -0.087465838 -0.09553238 -0.11063671 -0.12433569 -0.12764713 -0.11724142 -0.10225473 -0.090177625 -0.084113374 -0.082022525][-0.082259439 -0.081992865 -0.081569284 -0.081464335 -0.081800885 -0.083527818 -0.086575121 -0.10884923 -0.14060166 -0.16319247 -0.16171461 -0.14465961 -0.12180582 -0.1031165 -0.091016427][-0.082165852 -0.081509709 -0.080435559 -0.079892509 -0.0800228 -0.076025613 -0.071733378 -0.099995211 -0.15585668 -0.21272537 -0.24139217 -0.23800787 -0.20546177 -0.16241319 -0.12378621][-0.081996359 -0.081168227 -0.079492941 -0.078307383 -0.078573093 -0.06674882 -0.043670498 -0.053863466 -0.11483125 -0.19816944 -0.26547611 -0.30062804 -0.28436804 -0.23322935 -0.17207575][-0.081616692 -0.080661118 -0.078836016 -0.077104 -0.074612327 -0.048020434 0.0015790388 0.027545132 -0.0097374469 -0.0856323 -0.18375653 -0.27052897 -0.30116498 -0.27208745 -0.20900792][-0.081311263 -0.080506347 -0.079615124 -0.07865312 -0.07054548 -0.032087669 0.036396503 0.093294054 0.091498375 0.0518689 -0.029646441 -0.13684604 -0.21603264 -0.23476622 -0.20164216][-0.081361584 -0.08101137 -0.08117865 -0.080811143 -0.066154957 -0.021548472 0.061091378 0.14520153 0.18146503 0.19166365 0.15101387 0.050131693 -0.059154693 -0.1271752 -0.1428667][-0.081544787 -0.0813614 -0.081604078 -0.081570782 -0.064036265 -0.015587665 0.083087623 0.18516454 0.23847744 0.26457646 0.25205138 0.16660777 0.052647859 -0.040045779 -0.088849977][-0.08157254 -0.081301175 -0.081328355 -0.08130011 -0.067423925 -0.027368002 0.058942512 0.13674797 0.16128668 0.1689631 0.17141116 0.11927646 0.035836831 -0.038421448 -0.0757165][-0.0816663 -0.08150658 -0.081699356 -0.081725352 -0.076649159 -0.057455264 -0.009028323 0.038966641 0.053666651 0.05349043 0.059406653 0.02659262 -0.033566721 -0.080759428 -0.0933378][-0.081874207 -0.08183004 -0.08204744 -0.081968635 -0.084062614 -0.085596323 -0.075649552 -0.043082278 -0.028481375 -0.030046321 -0.032935917 -0.06870839 -0.11413757 -0.12845498 -0.11138681][-0.082011171 -0.081999749 -0.082207151 -0.0826946 -0.088265888 -0.096796282 -0.10890795 -0.10220278 -0.10670936 -0.11492615 -0.12738916 -0.14807537 -0.16239876 -0.14807329 -0.11863875][-0.082033157 -0.082062125 -0.082216404 -0.082446449 -0.084058866 -0.088105641 -0.10133382 -0.10920011 -0.12303138 -0.13443206 -0.14701101 -0.14789435 -0.13955331 -0.12056465 -0.10333237][-0.082052492 -0.082038954 -0.082116373 -0.082192667 -0.082488954 -0.084620416 -0.091081053 -0.10267506 -0.11301388 -0.12355227 -0.1290691 -0.12207182 -0.1084317 -0.095367141 -0.088508159][-0.082082562 -0.0820226 -0.082059711 -0.08207126 -0.081867807 -0.081567883 -0.083054356 -0.090821512 -0.096705116 -0.10726511 -0.11134974 -0.10972551 -0.098964117 -0.0872526 -0.080464572]]...]
INFO - root - 2017-12-06 07:48:05.246827: step 35510, loss = 0.78, batch loss = 0.56 (9.2 examples/sec; 0.871 sec/batch; 71h:48m:58s remains)
INFO - root - 2017-12-06 07:48:13.477271: step 35520, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 68h:47m:08s remains)
INFO - root - 2017-12-06 07:48:21.724753: step 35530, loss = 0.78, batch loss = 0.56 (9.8 examples/sec; 0.818 sec/batch; 67h:27m:18s remains)
INFO - root - 2017-12-06 07:48:29.973493: step 35540, loss = 0.91, batch loss = 0.69 (9.9 examples/sec; 0.805 sec/batch; 66h:23m:43s remains)
INFO - root - 2017-12-06 07:48:38.324344: step 35550, loss = 0.80, batch loss = 0.59 (9.5 examples/sec; 0.842 sec/batch; 69h:27m:31s remains)
INFO - root - 2017-12-06 07:48:46.656999: step 35560, loss = 0.84, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 65h:45m:23s remains)
INFO - root - 2017-12-06 07:48:55.058301: step 35570, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.844 sec/batch; 69h:34m:32s remains)
INFO - root - 2017-12-06 07:49:03.316830: step 35580, loss = 0.80, batch loss = 0.58 (10.2 examples/sec; 0.781 sec/batch; 64h:22m:37s remains)
INFO - root - 2017-12-06 07:49:11.642747: step 35590, loss = 0.83, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 69h:01m:54s remains)
INFO - root - 2017-12-06 07:49:19.982069: step 35600, loss = 0.90, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 68h:44m:28s remains)
2017-12-06 07:49:20.729292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064066835 -0.064885266 -0.066701792 -0.0703489 -0.075610958 -0.080835119 -0.083621673 -0.08390525 -0.07932236 -0.07191442 -0.067200415 -0.064883634 -0.062929004 -0.061894454 -0.062988155][-0.064573526 -0.066209137 -0.070038192 -0.075834654 -0.082997352 -0.0833933 -0.083783939 -0.090279751 -0.087853596 -0.079354741 -0.073529378 -0.069460422 -0.065680705 -0.062003348 -0.062536515][-0.064982235 -0.066438735 -0.070719339 -0.077098653 -0.082971752 -0.064617246 -0.052983843 -0.0781055 -0.088731118 -0.084167928 -0.078411736 -0.076682396 -0.071524173 -0.061888181 -0.061151005][-0.065386578 -0.0669577 -0.069362678 -0.078178369 -0.082992889 -0.038335774 -0.017762132 -0.075590074 -0.093595006 -0.081603229 -0.0681236 -0.073207647 -0.076694354 -0.06171754 -0.059183192][-0.066460676 -0.068309233 -0.07059525 -0.087136738 -0.0972204 -0.034719389 -0.0010565966 -0.082727343 -0.10431652 -0.072234549 -0.03722233 -0.049877614 -0.07617604 -0.062169943 -0.059979752][-0.066772655 -0.068843737 -0.068419427 -0.08798863 -0.098290175 -0.012045294 0.050716944 -0.055504736 -0.11431985 -0.086141683 -0.03802079 -0.040719356 -0.074694164 -0.062257297 -0.061462592][-0.067535929 -0.069354162 -0.066917121 -0.086734675 -0.1044421 -0.0082691759 0.10099774 0.0013888031 -0.097953834 -0.091207244 -0.051988166 -0.04340725 -0.07338243 -0.062649637 -0.063099429][-0.067790873 -0.06998612 -0.065181896 -0.080287531 -0.10405298 -0.01291763 0.13088694 0.062435843 -0.058044486 -0.066841982 -0.041518249 -0.035727516 -0.067731634 -0.062549219 -0.062995777][-0.067515314 -0.068771243 -0.063077122 -0.076148808 -0.1050433 -0.038023852 0.094051756 0.056738682 -0.053313576 -0.071462817 -0.061748259 -0.0498561 -0.065326855 -0.062449284 -0.062941641][-0.067686267 -0.069535449 -0.067506194 -0.081434242 -0.11898369 -0.093984544 -0.0069056675 -0.0081879944 -0.06963028 -0.078721404 -0.076396152 -0.065998085 -0.066160142 -0.063124143 -0.063998155][-0.067421079 -0.069303937 -0.069265194 -0.079050295 -0.11036237 -0.11527906 -0.073905095 -0.053089354 -0.06868156 -0.068084478 -0.066623271 -0.064351171 -0.065413773 -0.063641906 -0.06451679][-0.066175625 -0.066832334 -0.067378856 -0.072293252 -0.086058944 -0.091837235 -0.0802334 -0.066124849 -0.064837173 -0.062322892 -0.061818257 -0.061713342 -0.0656594 -0.064297087 -0.065260649][-0.065599747 -0.064947784 -0.065018944 -0.066967376 -0.071313575 -0.071798511 -0.066573583 -0.062693618 -0.062827222 -0.061924938 -0.061178993 -0.06087929 -0.065691121 -0.064531647 -0.065592661][-0.065767765 -0.064627111 -0.064438276 -0.064601041 -0.066165082 -0.066613659 -0.063084759 -0.060326315 -0.062347546 -0.062133349 -0.060840633 -0.060720984 -0.064109012 -0.064520873 -0.065598279][-0.0662574 -0.065215319 -0.065436877 -0.065544486 -0.065694571 -0.068814874 -0.06995099 -0.064135104 -0.064029112 -0.063834891 -0.062973678 -0.062407158 -0.063674077 -0.064820945 -0.065771677]]...]
INFO - root - 2017-12-06 07:49:29.022031: step 35610, loss = 0.75, batch loss = 0.54 (9.7 examples/sec; 0.824 sec/batch; 67h:55m:46s remains)
INFO - root - 2017-12-06 07:49:37.251356: step 35620, loss = 0.84, batch loss = 0.62 (10.1 examples/sec; 0.793 sec/batch; 65h:23m:30s remains)
INFO - root - 2017-12-06 07:49:45.599593: step 35630, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.823 sec/batch; 67h:53m:55s remains)
INFO - root - 2017-12-06 07:49:53.986362: step 35640, loss = 0.86, batch loss = 0.64 (9.5 examples/sec; 0.839 sec/batch; 69h:09m:35s remains)
INFO - root - 2017-12-06 07:50:02.075128: step 35650, loss = 0.85, batch loss = 0.64 (11.0 examples/sec; 0.726 sec/batch; 59h:52m:48s remains)
INFO - root - 2017-12-06 07:50:10.381345: step 35660, loss = 0.86, batch loss = 0.65 (9.9 examples/sec; 0.810 sec/batch; 66h:49m:26s remains)
INFO - root - 2017-12-06 07:50:18.700240: step 35670, loss = 0.78, batch loss = 0.57 (9.6 examples/sec; 0.837 sec/batch; 68h:58m:31s remains)
INFO - root - 2017-12-06 07:50:26.944106: step 35680, loss = 0.76, batch loss = 0.54 (9.6 examples/sec; 0.834 sec/batch; 68h:47m:00s remains)
INFO - root - 2017-12-06 07:50:35.275041: step 35690, loss = 0.79, batch loss = 0.57 (9.5 examples/sec; 0.841 sec/batch; 69h:18m:32s remains)
INFO - root - 2017-12-06 07:50:43.457700: step 35700, loss = 0.84, batch loss = 0.62 (9.7 examples/sec; 0.827 sec/batch; 68h:11m:04s remains)
2017-12-06 07:50:44.262315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.62920475 -0.2931743 0.088036761 0.065307543 -0.044462167 -0.15115912 -0.32083502 -0.35879964 -0.43733233 -0.42565137 -0.38621843 -0.37831664 -0.27060717 -0.20236002 -0.13659976][0.17107587 0.33175844 0.56371945 0.52492106 0.49937922 0.4697448 0.42924005 0.42224097 0.3295489 0.32489526 0.32906222 0.244116 0.21764703 0.14656907 0.0582792][0.11872566 0.17527024 0.23228456 0.27092081 0.30438143 0.24241282 0.13155486 0.12494247 0.13357466 0.1366186 0.056317538 -0.011306889 0.071667 0.086552635 0.13744454][0.35283488 0.21227185 0.13124424 -0.0090822056 -0.0629233 -0.0094724521 0.021464653 -0.084727816 -0.10839711 -0.11342254 -0.1419628 -0.12122295 -0.07412573 -0.0048048645 0.024745859][0.58937794 0.570278 0.48677254 0.11352648 -0.078552216 -0.20871961 -0.25554585 -0.24164148 -0.15239765 -0.14798 -0.15963182 -0.15807988 -0.20049211 -0.10280676 0.024883814][0.67859894 0.81553012 0.904481 0.86256152 0.73068953 0.47382545 0.20470549 -0.011853963 -0.12135815 -0.0924965 -0.0059401318 0.011120811 -0.0082782656 -0.065819666 0.14121592][0.65282935 0.63397408 0.73474866 0.76933962 0.80587149 0.66787463 0.36777818 0.0081390366 -0.18474695 -0.24706659 -0.16520479 -0.096784189 -0.0475122 -0.079297364 0.039576247][0.48655909 0.33490825 0.17917426 0.27550602 0.18747099 0.032074414 -0.15254033 -0.29957509 -0.54332823 -0.64865714 -0.65392113 -0.53973132 -0.42875898 -0.40240693 -0.14983179][0.25919962 -0.26437634 -0.75918949 -1.1101745 -1.3340675 -1.3397825 -1.4104284 -1.5292494 -1.4935162 -1.3887134 -1.4007192 -1.2806019 -1.0826218 -0.8620314 -0.5536831][-0.066663019 -0.52269417 -1.0263358 -1.4809672 -1.6688182 -1.7562475 -1.7353793 -1.6442813 -1.6098732 -1.5622762 -1.3539971 -1.213147 -1.0658237 -0.74236965 -0.47174382][-0.34063333 -0.87775254 -1.3402803 -1.6527306 -1.732268 -1.8838078 -1.9956874 -2.0443611 -2.0139713 -1.8374789 -1.6981889 -1.5082462 -1.2718729 -1.0136713 -0.64694458][-0.31536219 -0.7786352 -1.1227577 -1.434656 -1.528621 -1.4418839 -1.3929211 -1.4825805 -1.5574988 -1.6412224 -1.6966447 -1.5342257 -1.2693632 -1.0080667 -0.79683185][-0.18105078 -0.6473543 -1.0914049 -1.3282473 -1.3132116 -1.2937435 -1.252964 -1.2562721 -1.1978352 -1.3201612 -1.4027848 -1.5677004 -1.6112217 -1.2542752 -0.81776863][0.47613287 -0.13678208 -0.58810657 -1.0270914 -1.2043886 -1.2583088 -1.1133509 -1.0718582 -1.1075431 -1.1987846 -1.3368255 -1.6294361 -1.708779 -1.6093702 -1.1831948][0.64848089 0.24024816 -0.23163004 -0.5991044 -0.85040426 -1.1096536 -1.2790771 -1.3274351 -1.4443359 -1.4882438 -1.6124344 -1.7007117 -1.896945 -1.7914315 -1.3613406]]...]
INFO - root - 2017-12-06 07:50:52.448433: step 35710, loss = 0.74, batch loss = 0.53 (9.5 examples/sec; 0.842 sec/batch; 69h:24m:02s remains)
INFO - root - 2017-12-06 07:51:00.743715: step 35720, loss = 0.78, batch loss = 0.56 (9.5 examples/sec; 0.846 sec/batch; 69h:46m:35s remains)
INFO - root - 2017-12-06 07:51:09.014778: step 35730, loss = 0.79, batch loss = 0.58 (9.6 examples/sec; 0.837 sec/batch; 68h:59m:51s remains)
INFO - root - 2017-12-06 07:51:17.309148: step 35740, loss = 0.85, batch loss = 0.64 (9.7 examples/sec; 0.823 sec/batch; 67h:49m:43s remains)
INFO - root - 2017-12-06 07:51:25.731149: step 35750, loss = 0.84, batch loss = 0.63 (9.2 examples/sec; 0.867 sec/batch; 71h:26m:01s remains)
INFO - root - 2017-12-06 07:51:33.944040: step 35760, loss = 0.77, batch loss = 0.55 (9.3 examples/sec; 0.860 sec/batch; 70h:54m:42s remains)
INFO - root - 2017-12-06 07:51:42.170139: step 35770, loss = 0.75, batch loss = 0.53 (9.7 examples/sec; 0.822 sec/batch; 67h:44m:36s remains)
INFO - root - 2017-12-06 07:51:50.470920: step 35780, loss = 0.80, batch loss = 0.59 (9.3 examples/sec; 0.864 sec/batch; 71h:10m:20s remains)
INFO - root - 2017-12-06 07:51:58.720453: step 35790, loss = 0.77, batch loss = 0.56 (9.4 examples/sec; 0.854 sec/batch; 70h:22m:16s remains)
INFO - root - 2017-12-06 07:52:06.805556: step 35800, loss = 0.78, batch loss = 0.57 (10.9 examples/sec; 0.735 sec/batch; 60h:33m:31s remains)
2017-12-06 07:52:07.532802: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16319558 0.2299822 0.27835986 0.33241421 0.38497445 0.39816138 0.44424069 0.39368063 0.28543147 0.16318458 0.058404051 -0.0071155205 -0.076325044 -0.1226998 -0.15607478][0.068552308 0.13097206 0.17065737 0.21408832 0.26704371 0.31511033 0.31326196 0.24375764 0.16352698 0.030230545 -0.061333276 -0.13149525 -0.18085155 -0.20795572 -0.2453174][-0.010490552 0.033275016 0.039349698 0.047702275 0.083343454 0.15133318 0.18165511 0.14527652 0.08617001 -0.012268126 -0.086366057 -0.14631055 -0.18725912 -0.21428758 -0.24191213][-0.050306194 -0.00423871 0.0092235282 -0.0097607374 -0.0038610175 0.061801471 0.095416956 0.06258478 0.0024214312 -0.068131685 -0.12180814 -0.15690529 -0.18738088 -0.20438454 -0.19951043][-0.054261547 -0.003084518 0.025063977 0.0039874613 -0.019501887 0.0079522878 0.040821187 0.028683528 -0.039176986 -0.093431689 -0.12759012 -0.14887339 -0.18092726 -0.18401727 -0.1835545][-0.046351194 0.0019957125 0.043256246 0.039365955 0.0049703121 0.0041245148 0.055166535 0.077157624 0.020835944 -0.02670607 -0.067204818 -0.087086141 -0.13656309 -0.1504828 -0.14373097][-0.045274734 -0.0037061274 0.062318124 0.10377984 0.071370415 0.063705526 0.11490572 0.17385566 0.13609037 0.087893717 0.042795546 0.01436913 -0.042165846 -0.05598991 -0.061626174][-0.056474715 -0.031897135 0.02837716 0.075790741 0.060172446 0.060765378 0.11410626 0.20942432 0.19788319 0.16124368 0.10648663 0.086795487 0.021171972 0.0013538003 0.00017538667][-0.073135413 -0.052044261 -0.0070222467 0.034498654 0.017107442 0.026610926 0.066806786 0.16673535 0.15510261 0.11588507 0.042493157 0.014244273 -0.03563254 -0.05859651 -0.05871756][-0.074527018 -0.064758413 -0.029966395 -0.0081054643 -0.018767 -0.013981774 0.00919164 0.090217315 0.06678126 0.028261952 -0.035191383 -0.054116182 -0.09140417 -0.11138999 -0.11819686][-0.080838241 -0.070049144 -0.06232968 -0.067623317 -0.069642648 -0.061466813 -0.046148967 0.0041882545 -0.01243259 -0.033789977 -0.076126836 -0.085707083 -0.10330006 -0.1110642 -0.11432119][-0.083060756 -0.07784915 -0.075509354 -0.081611231 -0.088647231 -0.089172326 -0.077496484 -0.050539423 -0.058316723 -0.063104972 -0.0786397 -0.076249659 -0.084365636 -0.0848583 -0.090755053][-0.083914675 -0.081796125 -0.084948272 -0.088940553 -0.095905162 -0.095571585 -0.092999808 -0.082517549 -0.084867306 -0.086096287 -0.08923354 -0.079276085 -0.082717285 -0.079816304 -0.083814859][-0.0846 -0.08317057 -0.084848873 -0.0888834 -0.095415667 -0.094497964 -0.092342913 -0.089585021 -0.093591169 -0.097825438 -0.088076256 -0.0812088 -0.084493279 -0.087485492 -0.086320542][-0.088511489 -0.088600136 -0.089443259 -0.092352204 -0.097907849 -0.098582111 -0.09918768 -0.09708719 -0.11101464 -0.12321003 -0.10881148 -0.095670305 -0.09910205 -0.10750723 -0.10103545]]...]
INFO - root - 2017-12-06 07:52:15.805992: step 35810, loss = 0.79, batch loss = 0.57 (9.6 examples/sec; 0.834 sec/batch; 68h:46m:13s remains)
INFO - root - 2017-12-06 07:52:23.956492: step 35820, loss = 0.75, batch loss = 0.53 (9.9 examples/sec; 0.805 sec/batch; 66h:20m:24s remains)
INFO - root - 2017-12-06 07:52:32.314640: step 35830, loss = 0.75, batch loss = 0.53 (9.5 examples/sec; 0.839 sec/batch; 69h:08m:07s remains)
INFO - root - 2017-12-06 07:52:40.564286: step 35840, loss = 0.77, batch loss = 0.56 (9.2 examples/sec; 0.873 sec/batch; 71h:55m:08s remains)
INFO - root - 2017-12-06 07:52:48.759187: step 35850, loss = 0.81, batch loss = 0.59 (9.8 examples/sec; 0.818 sec/batch; 67h:26m:36s remains)
INFO - root - 2017-12-06 07:52:57.128307: step 35860, loss = 0.78, batch loss = 0.57 (9.6 examples/sec; 0.834 sec/batch; 68h:41m:37s remains)
INFO - root - 2017-12-06 07:53:05.424349: step 35870, loss = 0.79, batch loss = 0.57 (9.4 examples/sec; 0.854 sec/batch; 70h:22m:18s remains)
INFO - root - 2017-12-06 07:53:13.755238: step 35880, loss = 0.74, batch loss = 0.52 (9.9 examples/sec; 0.810 sec/batch; 66h:43m:51s remains)
INFO - root - 2017-12-06 07:53:22.147262: step 35890, loss = 0.73, batch loss = 0.52 (9.3 examples/sec; 0.858 sec/batch; 70h:41m:57s remains)
INFO - root - 2017-12-06 07:53:30.433275: step 35900, loss = 0.91, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:19m:58s remains)
2017-12-06 07:53:31.205124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.3605864 -0.56808811 -0.51248723 -0.44377029 -0.42804679 -0.48867252 -0.47124702 -0.35189798 -0.31276497 -0.31798393 -0.358809 -0.26435876 -0.080933228 0.1663267 0.30834198][-0.59834665 -0.8803972 -0.772624 -0.62787068 -0.60994864 -0.63140404 -0.63474047 -0.605912 -0.49514917 -0.44291064 -0.53986132 -0.50208366 -0.35448492 -0.077583164 0.084506728][-0.66183662 -0.830843 -0.79342073 -0.75091332 -0.64960241 -0.7323038 -0.78831941 -0.71663648 -0.67328525 -0.64089477 -0.70231664 -0.68539077 -0.59101003 -0.35722306 -0.17971992][-0.89869922 -1.0437392 -1.0289596 -0.99000525 -0.83900768 -0.711967 -0.68966186 -0.74309117 -0.69696581 -0.60650069 -0.55395484 -0.56057549 -0.53817457 -0.38811809 -0.1549128][-0.79323846 -1.0554435 -1.1486455 -1.0963219 -0.89456868 -0.59784478 -0.30102688 -0.19617251 -0.18280062 -0.33115289 -0.33060262 -0.33786088 -0.34557104 -0.37574315 -0.29366332][-0.54885954 -0.800066 -0.87871522 -0.95300186 -0.77847326 -0.46334416 -0.092189282 0.14939857 0.22293773 0.13585076 -0.019641161 -0.16153258 -0.15041934 -0.20386565 -0.26699987][-0.37080878 -0.45319271 -0.532573 -0.61714256 -0.54690152 -0.21039233 0.19452617 0.39788002 0.43456879 0.27025139 0.12823191 -0.086071067 -0.16272858 -0.22802678 -0.21273577][-0.40426269 -0.37802276 -0.31100553 -0.1617783 -0.091193974 -0.024571091 0.23727185 0.5647319 0.6855067 0.49668941 0.22546038 -0.049953397 -0.24073735 -0.34568262 -0.22966465][-0.40842149 -0.33548504 -0.050587516 0.17865011 0.27119195 0.18938476 0.25770128 0.50026649 0.64985341 0.55505347 0.31659862 -0.020813316 -0.23536286 -0.34481928 -0.28145742][-0.26297268 -0.24523157 0.04361137 0.38136965 0.44482329 0.31488442 0.12686917 0.095882468 0.17858091 0.2446737 0.07880459 -0.11830948 -0.33275375 -0.32307741 -0.28141785][-0.19897622 -0.10540174 0.13076234 0.43320718 0.53046745 0.25727937 -0.1657787 -0.32242566 -0.3040114 -0.24158663 -0.19375786 -0.21004811 -0.3796517 -0.38674763 -0.31444478][-0.32892147 -0.13622853 0.04793904 0.2982243 0.38779658 0.20479473 -0.20247455 -0.51215041 -0.59504133 -0.5357042 -0.47510839 -0.39604536 -0.43846452 -0.44340482 -0.37518477][-0.30856359 -0.12250196 0.079201274 0.23874271 0.282622 0.19458574 -0.14111345 -0.50101185 -0.72193271 -0.71023542 -0.62589109 -0.57044208 -0.54215777 -0.4415814 -0.28865221][-0.33211643 -0.099783361 0.12368653 0.36356598 0.51000929 0.33496693 -0.023228154 -0.39232165 -0.66857588 -0.76795906 -0.7131573 -0.4794392 -0.33245635 -0.30690178 -0.20116711][-0.24657708 -0.050168306 0.14542818 0.30765492 0.40914878 0.38463151 0.18704721 -0.13160613 -0.42093775 -0.5492847 -0.44773978 -0.31991547 -0.11182144 -0.016287036 0.082944058]]...]
INFO - root - 2017-12-06 07:53:39.539212: step 35910, loss = 0.74, batch loss = 0.52 (9.3 examples/sec; 0.863 sec/batch; 71h:07m:16s remains)
INFO - root - 2017-12-06 07:53:47.802568: step 35920, loss = 0.80, batch loss = 0.58 (9.9 examples/sec; 0.809 sec/batch; 66h:36m:28s remains)
INFO - root - 2017-12-06 07:53:56.105808: step 35930, loss = 0.81, batch loss = 0.59 (9.7 examples/sec; 0.828 sec/batch; 68h:14m:49s remains)
INFO - root - 2017-12-06 07:54:04.263039: step 35940, loss = 0.85, batch loss = 0.63 (9.6 examples/sec; 0.833 sec/batch; 68h:37m:48s remains)
INFO - root - 2017-12-06 07:54:12.577027: step 35950, loss = 0.74, batch loss = 0.53 (11.0 examples/sec; 0.726 sec/batch; 59h:49m:20s remains)
INFO - root - 2017-12-06 07:54:20.736517: step 35960, loss = 0.86, batch loss = 0.65 (9.5 examples/sec; 0.845 sec/batch; 69h:36m:35s remains)
INFO - root - 2017-12-06 07:54:29.052668: step 35970, loss = 0.80, batch loss = 0.59 (9.7 examples/sec; 0.824 sec/batch; 67h:53m:48s remains)
INFO - root - 2017-12-06 07:54:37.438253: step 35980, loss = 0.80, batch loss = 0.59 (9.6 examples/sec; 0.834 sec/batch; 68h:40m:48s remains)
INFO - root - 2017-12-06 07:54:45.521288: step 35990, loss = 0.80, batch loss = 0.59 (9.7 examples/sec; 0.824 sec/batch; 67h:53m:24s remains)
INFO - root - 2017-12-06 07:54:53.868334: step 36000, loss = 0.83, batch loss = 0.62 (9.8 examples/sec; 0.820 sec/batch; 67h:34m:27s remains)
2017-12-06 07:54:54.654989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10546098 -0.11940694 -0.14290142 -0.16091406 -0.17589083 -0.16825156 -0.12713906 -0.054579318 0.0036702678 -0.036373194 -0.14361715 -0.26563895 -0.2904633 -0.26610535 -0.16920617][-0.13187414 -0.15296857 -0.20082399 -0.23985533 -0.27043247 -0.27591351 -0.27379858 -0.24297743 -0.17850068 -0.15497136 -0.20563799 -0.28587431 -0.30796516 -0.22193085 -0.06363672][-0.16653588 -0.16037792 -0.19021329 -0.22756113 -0.24803218 -0.24138811 -0.22661079 -0.19079828 -0.15251023 -0.13644657 -0.18635124 -0.27760023 -0.29877836 -0.19913071 -0.070742458][-0.16466078 -0.15983918 -0.16530044 -0.17215911 -0.16333458 -0.14336374 -0.10555045 -0.057996288 -0.040759753 -0.03315381 -0.064621933 -0.15125953 -0.15544619 -0.12861393 -0.038645647][-0.17607361 -0.16825199 -0.16287899 -0.17049721 -0.16769695 -0.16192964 -0.13352156 -0.10322005 -0.082542047 -0.027554594 -0.011894062 -0.0090717375 -0.0099625215 -0.022889562 0.057322532][-0.17518115 -0.15193832 -0.12578355 -0.12161769 -0.13620213 -0.12744814 -0.080346979 -0.031200949 0.0013141483 0.019884638 -0.007422097 0.012007706 0.0074847639 -0.024044305 0.015020169][-0.18518522 -0.17978409 -0.12820572 -0.071799129 -0.057696246 -0.019079655 0.079388395 0.19846733 0.3147819 0.32973337 0.26147312 0.18477918 0.085169181 -0.046241473 -0.098143846][-0.14682554 -0.20537114 -0.16594113 -0.10120734 -0.044900496 0.0065923259 0.094547153 0.19920878 0.327816 0.3695848 0.35746002 0.26837409 0.13713074 -0.016477525 -0.09665671][-0.11805435 -0.16863027 -0.14401449 -0.089480691 -0.015394986 0.074574128 0.14853069 0.20981143 0.26426584 0.2529121 0.17110159 0.020767264 -0.096996263 -0.20478758 -0.19906327][-0.094641194 -0.13265255 -0.089580953 -0.05989138 -0.049116019 0.024824992 0.13498306 0.27354527 0.36367786 0.37482232 0.22126327 -0.039072506 -0.28012168 -0.40547734 -0.36430877][-0.091541074 -0.10308383 -0.10737516 -0.081053048 -0.046925552 -0.026207633 0.056242764 0.232684 0.40276986 0.47265971 0.34866643 0.078674033 -0.25321519 -0.44744289 -0.41503847][-0.21977875 -0.2185799 -0.17574984 -0.10786942 -0.099791452 -0.07055223 0.0096071661 0.14151642 0.27203894 0.33122784 0.22507162 0.012227409 -0.22522047 -0.39897346 -0.34694427][-0.31573841 -0.33953053 -0.25980091 -0.1892322 -0.16197516 -0.1156239 -0.0736562 -0.010231279 0.076198518 0.12934658 0.014184102 -0.20765063 -0.424617 -0.52655661 -0.46615183][-0.27698335 -0.31225139 -0.29629993 -0.26232523 -0.27357662 -0.27765071 -0.26258796 -0.19835162 -0.096349671 -0.013315544 -0.035134815 -0.16507827 -0.3435176 -0.50533992 -0.48323137][-0.26889879 -0.24404335 -0.24506374 -0.27823746 -0.36836958 -0.48796231 -0.5333963 -0.51293623 -0.41038018 -0.28256077 -0.19228823 -0.18777445 -0.26673669 -0.37836498 -0.36068511]]...]
INFO - root - 2017-12-06 07:55:02.933710: step 36010, loss = 0.88, batch loss = 0.66 (9.5 examples/sec; 0.842 sec/batch; 69h:20m:54s remains)
INFO - root - 2017-12-06 07:55:11.283831: step 36020, loss = 0.85, batch loss = 0.64 (9.1 examples/sec; 0.881 sec/batch; 72h:31m:54s remains)
INFO - root - 2017-12-06 07:55:19.634404: step 36030, loss = 0.77, batch loss = 0.56 (9.9 examples/sec; 0.805 sec/batch; 66h:19m:03s remains)
INFO - root - 2017-12-06 07:55:27.902684: step 36040, loss = 0.81, batch loss = 0.59 (9.5 examples/sec; 0.841 sec/batch; 69h:15m:21s remains)
INFO - root - 2017-12-06 07:55:36.286144: step 36050, loss = 0.84, batch loss = 0.63 (9.8 examples/sec; 0.815 sec/batch; 67h:08m:18s remains)
INFO - root - 2017-12-06 07:55:44.500402: step 36060, loss = 0.82, batch loss = 0.61 (9.9 examples/sec; 0.806 sec/batch; 66h:24m:01s remains)
INFO - root - 2017-12-06 07:55:52.814439: step 36070, loss = 0.80, batch loss = 0.59 (9.7 examples/sec; 0.821 sec/batch; 67h:36m:05s remains)
INFO - root - 2017-12-06 07:56:01.175720: step 36080, loss = 0.72, batch loss = 0.50 (9.7 examples/sec; 0.828 sec/batch; 68h:10m:49s remains)
INFO - root - 2017-12-06 07:56:09.381414: step 36090, loss = 0.88, batch loss = 0.67 (9.8 examples/sec; 0.814 sec/batch; 67h:01m:56s remains)
INFO - root - 2017-12-06 07:56:17.703970: step 36100, loss = 0.83, batch loss = 0.61 (9.8 examples/sec; 0.816 sec/batch; 67h:11m:39s remains)
2017-12-06 07:56:18.408610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.75033361 -0.73990232 -0.61785507 -0.881478 -0.73535818 -1.2612094 -1.9911004 -2.604845 -3.0547616 -2.9659696 -3.1278791 -2.9556155 -2.8582451 -2.6442375 -2.1096127][-1.2483687 -1.1921102 -1.1544884 -1.3281225 -1.5962871 -2.333035 -2.968431 -3.1001701 -3.4530141 -3.7236202 -3.8109355 -3.4496288 -3.3170969 -3.0396438 -2.4320405][-2.9640429 -2.6410286 -2.1651356 -1.8233639 -1.6941839 -2.0358217 -2.5169485 -2.7012854 -2.9135852 -3.140887 -3.6186962 -3.4985595 -3.280617 -2.5318522 -2.0884986][-3.3626151 -3.3276303 -3.1480179 -2.8923705 -2.8514514 -2.601449 -2.754247 -3.0647039 -2.8848441 -2.860754 -2.9259884 -2.7064257 -2.7895291 -2.4539557 -2.5788581][-3.0257084 -2.9964681 -2.7619927 -2.5452249 -2.2912929 -2.0734019 -2.0867274 -1.9888878 -1.9005742 -1.9693992 -2.0367627 -1.916095 -1.8291515 -1.7776641 -2.0683444][-1.672471 -1.5185809 -1.3318996 -1.0393171 -0.9140541 -0.5008629 -0.20096193 0.094234355 0.38299745 0.55371487 0.66490245 0.82342803 0.39424416 0.016172856 -0.48623469][-0.034815762 0.068011396 0.20601761 0.61223716 0.94776589 1.3215327 1.4066645 1.7618362 1.983142 2.1753979 2.0112512 1.9215623 1.8209999 1.8454314 1.4797771][0.98806447 1.089722 1.1629541 1.3550571 1.6341292 2.06083 2.3009472 2.6779253 2.8033612 2.6620655 2.5962398 2.5913165 2.6605508 2.692292 2.590441][0.87213576 0.9761557 1.0676968 1.1126848 1.0939889 1.1036261 1.118992 1.338378 1.6174546 1.7044984 1.6708721 1.6822102 1.9709746 2.0778596 2.2853537][0.17040667 -0.044695709 -0.13341507 -0.091711804 -0.022596404 -0.13938165 -0.29305157 -0.27127916 -0.16855484 -0.033692822 0.0662104 0.47886243 0.96252781 1.3926746 1.8151432][-0.60662907 -0.65881008 -1.0068055 -1.2754365 -1.410619 -1.4586095 -1.3694504 -1.417703 -1.5437047 -1.6372402 -1.7830434 -1.6140809 -1.3580725 -0.89271128 -0.23406431][-1.3518924 -1.492301 -1.6789535 -1.880029 -2.1252258 -2.291909 -2.4531658 -2.4197743 -2.506681 -2.536602 -2.5771263 -2.5677843 -2.5802147 -2.2611353 -1.8878922][-1.1901367 -1.632566 -2.0152092 -2.3427792 -2.6632454 -2.8968391 -2.9235852 -2.8334007 -2.964529 -2.8632767 -2.9283783 -3.0159585 -2.7610416 -2.7997229 -2.7265077][-0.82982945 -1.2586673 -1.7572888 -2.0489733 -2.3299654 -2.4125595 -2.5313094 -2.7200594 -2.7440183 -2.5180373 -2.46247 -2.5939684 -2.6174204 -2.5620754 -2.4063187][-0.93937969 -1.1393281 -1.4905193 -1.7315073 -1.994581 -1.9032872 -1.8235062 -1.6616542 -1.6187986 -1.4931488 -1.5980805 -1.6058675 -1.6370882 -1.7064413 -1.8532487]]...]
INFO - root - 2017-12-06 07:56:26.904996: step 36110, loss = 0.86, batch loss = 0.64 (9.4 examples/sec; 0.855 sec/batch; 70h:24m:02s remains)
INFO - root - 2017-12-06 07:56:35.154170: step 36120, loss = 0.78, batch loss = 0.57 (9.5 examples/sec; 0.843 sec/batch; 69h:24m:39s remains)
INFO - root - 2017-12-06 07:56:43.588571: step 36130, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.844 sec/batch; 69h:29m:34s remains)
INFO - root - 2017-12-06 07:56:51.918102: step 36140, loss = 0.83, batch loss = 0.62 (9.3 examples/sec; 0.856 sec/batch; 70h:29m:03s remains)
INFO - root - 2017-12-06 07:57:00.222644: step 36150, loss = 0.81, batch loss = 0.59 (9.5 examples/sec; 0.843 sec/batch; 69h:24m:54s remains)
INFO - root - 2017-12-06 07:57:08.513176: step 36160, loss = 0.73, batch loss = 0.52 (9.9 examples/sec; 0.808 sec/batch; 66h:28m:29s remains)
INFO - root - 2017-12-06 07:57:16.838925: step 36170, loss = 0.76, batch loss = 0.54 (9.3 examples/sec; 0.863 sec/batch; 71h:03m:21s remains)
INFO - root - 2017-12-06 07:57:25.115698: step 36180, loss = 0.85, batch loss = 0.64 (9.9 examples/sec; 0.812 sec/batch; 66h:48m:19s remains)
INFO - root - 2017-12-06 07:57:33.459983: step 36190, loss = 0.79, batch loss = 0.57 (9.9 examples/sec; 0.808 sec/batch; 66h:32m:44s remains)
INFO - root - 2017-12-06 07:57:41.736250: step 36200, loss = 0.78, batch loss = 0.57 (9.4 examples/sec; 0.848 sec/batch; 69h:50m:05s remains)
2017-12-06 07:57:42.521423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22852507 -0.19823936 -0.25162575 -0.26349223 -0.26609796 -0.23713608 -0.10303269 -0.04868491 0.14395273 0.15382013 0.11545213 -0.047479145 -0.16820163 -0.20871584 -0.18457897][-0.018047288 -0.21826577 -0.26216549 -0.33174816 -0.441464 -0.51953286 -0.41278803 -0.46632826 -0.34674758 -0.32486814 -0.2062714 -0.26736224 -0.29382217 -0.32936618 -0.28220421][0.093269542 -0.10890873 -0.12541367 -0.15383944 -0.22662401 -0.43847352 -0.52244788 -0.66835338 -0.61752832 -0.66170728 -0.59405249 -0.55905 -0.40644211 -0.37124133 -0.30418909][-0.040978245 -0.2309417 -0.17981404 -0.069413267 1.1831522e-05 -0.1369639 -0.20227829 -0.40380073 -0.48775911 -0.58094096 -0.529846 -0.54422665 -0.42541206 -0.35393202 -0.27087146][-0.288056 -0.43900728 -0.40818465 -0.28717351 -0.042162962 0.070233032 0.14647284 0.040760875 -0.12521558 -0.26011217 -0.28369412 -0.3538776 -0.30592966 -0.26880962 -0.22378445][-0.19115946 -0.32340074 -0.3649624 -0.30871034 0.00082698464 0.25433993 0.51092094 0.60055447 0.44138092 0.23072867 0.074303627 0.0034458116 -0.06327419 -0.094851017 -0.094566725][-0.22484212 -0.23440352 -0.15035942 -0.069133908 0.23735185 0.55531681 0.85093224 1.0065213 1.0095898 0.82015425 0.54390484 0.32569432 0.14627631 0.074230358 0.014641084][-0.25677091 -0.30632657 -0.18442324 -0.0041683167 0.40899098 0.84750193 1.1676223 1.247919 1.2202355 1.0913538 0.84393722 0.57508534 0.34023261 0.18188052 0.08309117][-0.14934841 -0.28255761 -0.23341414 -0.11570525 0.22986241 0.71638906 1.1556406 1.3703241 1.3133775 1.1069763 0.82836694 0.59397048 0.37605721 0.22980754 0.15562829][-0.2321568 -0.33902544 -0.3964808 -0.40078634 -0.15403843 0.22649436 0.69222051 1.0434203 1.1371201 1.0004084 0.68126005 0.39076167 0.16978319 0.066141278 0.038682461][-0.45267147 -0.62787259 -0.67299306 -0.68508291 -0.5534237 -0.32393521 0.053425997 0.33224481 0.563376 0.57535976 0.42732209 0.18471463 -0.042780448 -0.10971424 -0.0991746][-0.34401757 -0.70200163 -0.95261014 -1.0187117 -0.89423579 -0.7518965 -0.46597195 -0.22125959 -0.0075487718 0.030367844 0.041688576 -0.068131827 -0.15626821 -0.20787609 -0.19207627][-0.209887 -0.50003606 -0.71382165 -0.96827328 -1.0724778 -0.99116969 -0.71050733 -0.51864141 -0.33116829 -0.28515792 -0.23603228 -0.29772413 -0.30240947 -0.30965874 -0.27015075][-0.0051456615 -0.24242008 -0.45613122 -0.63017654 -0.64596069 -0.72365654 -0.6655696 -0.49831444 -0.29765663 -0.25915462 -0.21514964 -0.305938 -0.31318083 -0.32919389 -0.29853803][0.075454324 -0.061876327 -0.30498019 -0.46113509 -0.38354015 -0.31191897 -0.16386764 -0.033312108 -0.0048630238 0.038518861 0.082459986 -0.020878129 -0.076685645 -0.16463511 -0.18469402]]...]
INFO - root - 2017-12-06 07:57:50.870858: step 36210, loss = 0.82, batch loss = 0.60 (9.3 examples/sec; 0.861 sec/batch; 70h:50m:29s remains)
INFO - root - 2017-12-06 07:57:59.217010: step 36220, loss = 0.81, batch loss = 0.59 (10.0 examples/sec; 0.801 sec/batch; 65h:55m:30s remains)
INFO - root - 2017-12-06 07:58:07.532134: step 36230, loss = 0.78, batch loss = 0.56 (9.7 examples/sec; 0.822 sec/batch; 67h:41m:16s remains)
INFO - root - 2017-12-06 07:58:15.922762: step 36240, loss = 0.93, batch loss = 0.72 (9.8 examples/sec; 0.814 sec/batch; 66h:59m:43s remains)
INFO - root - 2017-12-06 07:58:24.270533: step 36250, loss = 0.80, batch loss = 0.58 (9.6 examples/sec; 0.831 sec/batch; 68h:23m:12s remains)
INFO - root - 2017-12-06 07:58:32.646157: step 36260, loss = 0.78, batch loss = 0.56 (9.5 examples/sec; 0.842 sec/batch; 69h:19m:39s remains)
INFO - root - 2017-12-06 07:58:40.870278: step 36270, loss = 0.83, batch loss = 0.62 (9.7 examples/sec; 0.828 sec/batch; 68h:06m:41s remains)
INFO - root - 2017-12-06 07:58:49.184412: step 36280, loss = 0.95, batch loss = 0.74 (9.4 examples/sec; 0.851 sec/batch; 70h:01m:52s remains)
INFO - root - 2017-12-06 07:58:57.624769: step 36290, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.852 sec/batch; 70h:06m:36s remains)
INFO - root - 2017-12-06 07:59:06.108719: step 36300, loss = 0.83, batch loss = 0.61 (9.5 examples/sec; 0.839 sec/batch; 69h:02m:47s remains)
2017-12-06 07:59:06.887204: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3089945 1.0143616 0.66933525 0.39008975 0.238306 0.48976129 0.81690538 0.92444062 0.79661423 0.64058685 0.44608235 0.14699188 0.02914267 -0.18782857 -0.47043628][1.597504 1.1914402 0.86409181 0.43693829 0.0831293 0.18443789 0.18459843 0.30725753 0.33875334 0.34477443 0.240683 0.083770424 -0.31364262 -0.69772518 -0.92692482][1.1627823 0.75697935 0.35250157 0.08628872 -0.1357424 -0.27061442 -0.34323549 -0.27706671 -0.29016125 -0.32100356 -0.29056025 -0.33027238 -0.45014453 -0.63241476 -0.82943374][0.37900168 0.15022637 -0.38951319 -0.53799826 -0.564855 -0.68622434 -0.77687544 -0.73186415 -0.64206636 -0.57941371 -0.67894948 -0.71900648 -0.85717535 -1.0285562 -1.1171714][-0.23690207 -0.4305622 -0.7335642 -0.91115159 -1.0352358 -0.95478278 -0.78549814 -0.87949181 -0.89027005 -0.85774904 -0.85238826 -0.91866344 -1.031652 -1.1916598 -1.3085079][-0.22174107 -0.5045526 -0.82371837 -1.0272355 -1.0658095 -0.82518131 -0.63206184 -0.6411258 -0.66075331 -0.72747654 -0.82612437 -0.93226904 -0.97453374 -1.1881151 -1.2350374][-0.51168793 -0.61996162 -0.7132414 -0.76944453 -0.871369 -0.59058642 -0.35009915 -0.23539925 -0.15782943 -0.16533574 -0.12838103 -0.24937263 -0.44246024 -0.80283058 -1.0553273][-0.617924 -0.68830866 -0.7202096 -0.55685359 -0.33520788 -0.16787061 0.0049703494 0.31587565 0.46104109 0.40094507 0.37551981 0.16928647 0.04256852 -0.21704952 -0.5722186][-0.13233306 -0.2175952 -0.16807812 -0.10871402 0.052298412 0.38438958 0.54966724 0.534105 0.59357935 0.72194344 0.76476794 0.68785721 0.577973 0.36214721 0.10085216][0.30376512 0.19394441 0.22214793 0.2484381 0.29477537 0.35292447 0.43515509 0.67075253 0.7594493 0.67591894 0.75370115 0.7680372 0.887716 0.78588206 0.72451133][0.53527576 0.29507059 0.26922578 0.43026268 0.46758991 0.45754516 0.40274382 0.311208 0.16701327 0.1948234 0.27176952 0.36119711 0.38691157 0.3341099 0.32619262][0.7509163 0.63368189 0.67077148 0.6431638 0.6148755 0.62953293 0.49337649 0.36930174 0.13928385 0.046875685 -0.066384949 -0.17803079 -0.16271594 0.17925651 0.23656096][0.70710778 0.61721432 0.48737645 0.48987633 0.64700484 0.57438368 0.48086953 0.34806466 0.33147281 0.21607326 0.067621812 -0.017960116 0.026224792 0.28610426 0.41305006][0.3405937 0.42873657 0.46868843 0.42964327 0.37668389 0.51400745 0.60238212 0.63170248 0.51811635 0.38747454 0.35030264 0.33701015 0.31774646 0.38277668 0.13683131][0.25220108 0.17694311 0.20973568 0.17460908 0.44759053 0.21157236 0.12978278 0.060938194 0.14923823 0.16994981 0.051661387 -0.16799361 -0.094319284 -0.22970919 -0.25278184]]...]
INFO - root - 2017-12-06 07:59:15.197933: step 36310, loss = 0.72, batch loss = 0.51 (9.7 examples/sec; 0.822 sec/batch; 67h:37m:39s remains)
INFO - root - 2017-12-06 07:59:23.628052: step 36320, loss = 0.81, batch loss = 0.60 (9.5 examples/sec; 0.843 sec/batch; 69h:21m:45s remains)
INFO - root - 2017-12-06 07:59:32.146075: step 36330, loss = 0.82, batch loss = 0.60 (9.5 examples/sec; 0.846 sec/batch; 69h:33m:46s remains)
INFO - root - 2017-12-06 07:59:40.531405: step 36340, loss = 0.79, batch loss = 0.58 (9.8 examples/sec; 0.816 sec/batch; 67h:08m:11s remains)
INFO - root - 2017-12-06 07:59:48.934827: step 36350, loss = 0.75, batch loss = 0.53 (9.7 examples/sec; 0.826 sec/batch; 67h:57m:28s remains)
INFO - root - 2017-12-06 07:59:57.241857: step 36360, loss = 0.83, batch loss = 0.61 (10.0 examples/sec; 0.796 sec/batch; 65h:28m:54s remains)
INFO - root - 2017-12-06 08:00:05.457067: step 36370, loss = 0.77, batch loss = 0.56 (9.6 examples/sec; 0.834 sec/batch; 68h:37m:34s remains)
INFO - root - 2017-12-06 08:00:13.835489: step 36380, loss = 0.77, batch loss = 0.56 (9.4 examples/sec; 0.852 sec/batch; 70h:03m:55s remains)
INFO - root - 2017-12-06 08:00:22.217400: step 36390, loss = 0.84, batch loss = 0.63 (9.5 examples/sec; 0.842 sec/batch; 69h:16m:28s remains)
INFO - root - 2017-12-06 08:00:30.566320: step 36400, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.821 sec/batch; 67h:29m:44s remains)
2017-12-06 08:00:31.256374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.6525768 -0.81929886 -0.78248084 -0.74142087 -0.72408569 -0.716455 -0.62305629 -0.55918908 -0.47924024 -0.38739881 -0.23679891 -0.13428459 -0.11075919 -0.10917007 -0.12180632][-0.57722366 -0.93841136 -1.105507 -1.1738352 -1.1890446 -1.1540854 -1.0812511 -0.96892619 -0.75883317 -0.55585229 -0.35324848 -0.20893908 -0.13118839 -0.10171793 -0.11356879][-0.23127189 -0.58577746 -0.90583134 -1.1007084 -1.2748244 -1.34632 -1.3192629 -1.2809976 -1.1606255 -0.97061849 -0.67152786 -0.41603079 -0.23123832 -0.16362375 -0.12019544][0.0657887 -0.15579087 -0.42702141 -0.60632253 -0.79299605 -0.9515661 -1.0714809 -1.1369907 -1.085104 -0.97867656 -0.83697653 -0.6611762 -0.45432556 -0.30655777 -0.25245047][0.46141496 0.54328918 0.59480178 0.57976949 0.43944374 0.24541733 0.014951818 -0.17850882 -0.35040975 -0.44414157 -0.52276218 -0.46415836 -0.39356935 -0.29887995 -0.24086955][0.4743329 0.7294023 1.0413263 1.3770372 1.5832306 1.5657414 1.4478381 1.2789414 1.0578507 0.81525362 0.55363238 0.30463853 0.089194521 -0.014216341 -0.0381254][0.32820496 0.71240461 1.1616678 1.5662023 1.9363256 2.1564837 2.248553 2.1220055 1.9924235 1.7437353 1.474172 1.2043555 0.88507044 0.60373795 0.425361][0.10450043 0.38011062 0.76812518 1.2067635 1.7557744 2.0976 2.3481152 2.337033 2.3207159 2.1413023 1.9043174 1.7316855 1.4871074 1.2083837 0.97321832][-0.43919542 -0.27818626 2.7559698e-05 0.33498675 0.75765395 1.1290953 1.5391299 1.7629884 1.9063507 1.8468866 1.7916335 1.6427201 1.4773643 1.3270307 1.2162287][-0.84488428 -0.90363348 -0.870319 -0.743621 -0.45953155 -0.14098173 0.25184548 0.536157 0.91427445 1.0945119 1.2620988 1.2810097 1.375581 1.3367501 1.1458664][-1.0207803 -1.1922116 -1.2688096 -1.4191464 -1.3948954 -1.3280309 -1.0482026 -0.73960793 -0.40309519 -0.1012237 0.26009542 0.5335207 0.71990097 0.82586312 0.7758714][-1.2621735 -1.507218 -1.6436071 -1.8157225 -1.7931966 -1.8371465 -1.7666568 -1.5936273 -1.3605305 -1.1229237 -0.77873123 -0.47831121 -0.12086283 0.1356912 0.19761372][-1.1665297 -1.4364257 -1.5919269 -1.8487204 -1.9824433 -2.0964479 -1.9389536 -1.8888009 -1.8043486 -1.6169568 -1.3682553 -1.1105313 -0.84986079 -0.57961756 -0.37153214][-0.95419621 -1.235229 -1.4905475 -1.7294258 -1.9106988 -2.1206777 -2.1253693 -2.0456152 -1.8221233 -1.6515139 -1.4255905 -1.2036159 -1.0380543 -0.79328895 -0.57542][-0.72539723 -0.98267579 -1.1557393 -1.3992866 -1.5846404 -1.7914026 -1.9124131 -1.932593 -1.8755689 -1.7148032 -1.4206258 -1.1251329 -0.83637953 -0.5934304 -0.43426931]]...]
INFO - root - 2017-12-06 08:00:39.632662: step 36410, loss = 0.84, batch loss = 0.63 (9.4 examples/sec; 0.852 sec/batch; 70h:02m:55s remains)
INFO - root - 2017-12-06 08:00:48.054032: step 36420, loss = 0.79, batch loss = 0.57 (9.5 examples/sec; 0.838 sec/batch; 68h:55m:57s remains)
INFO - root - 2017-12-06 08:00:56.170524: step 36430, loss = 0.83, batch loss = 0.62 (9.7 examples/sec; 0.828 sec/batch; 68h:04m:23s remains)
INFO - root - 2017-12-06 08:01:04.455549: step 36440, loss = 0.82, batch loss = 0.60 (9.9 examples/sec; 0.810 sec/batch; 66h:35m:29s remains)
INFO - root - 2017-12-06 08:01:12.760998: step 36450, loss = 0.86, batch loss = 0.64 (9.2 examples/sec; 0.868 sec/batch; 71h:22m:48s remains)
INFO - root - 2017-12-06 08:01:21.064276: step 36460, loss = 0.90, batch loss = 0.68 (9.8 examples/sec; 0.818 sec/batch; 67h:14m:01s remains)
INFO - root - 2017-12-06 08:01:29.299838: step 36470, loss = 0.81, batch loss = 0.60 (9.5 examples/sec; 0.839 sec/batch; 68h:57m:49s remains)
INFO - root - 2017-12-06 08:01:37.585257: step 36480, loss = 0.78, batch loss = 0.56 (9.5 examples/sec; 0.840 sec/batch; 69h:05m:42s remains)
INFO - root - 2017-12-06 08:01:46.001399: step 36490, loss = 0.71, batch loss = 0.49 (9.6 examples/sec; 0.833 sec/batch; 68h:27m:51s remains)
INFO - root - 2017-12-06 08:01:54.362613: step 36500, loss = 0.85, batch loss = 0.63 (9.6 examples/sec; 0.830 sec/batch; 68h:12m:23s remains)
2017-12-06 08:01:55.077796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08789818 -0.0889693 -0.089765221 -0.090232722 -0.090571165 -0.090789974 -0.09080153 -0.090555795 -0.090242386 -0.090015657 -0.090114579 -0.090394527 -0.090962484 -0.091490321 -0.0913823][-0.081288174 -0.082114466 -0.082850531 -0.083299235 -0.083777949 -0.084098481 -0.08408948 -0.083892152 -0.083580792 -0.083313167 -0.083353415 -0.083661571 -0.084337637 -0.084999062 -0.085229576][-0.071107492 -0.07153853 -0.072347656 -0.073093757 -0.073939458 -0.074504681 -0.074622184 -0.0745811 -0.074345075 -0.074004084 -0.073827721 -0.074156843 -0.074792385 -0.075552858 -0.076171413][-0.060817428 -0.06076308 -0.061475493 -0.062396545 -0.063452721 -0.064232461 -0.064458862 -0.064514875 -0.064390168 -0.063969374 -0.06362319 -0.063747361 -0.064224645 -0.064909481 -0.065764412][-0.051669616 -0.051353633 -0.051919464 -0.052703798 -0.0537424 -0.054571189 -0.0549004 -0.055070084 -0.055035684 -0.054574005 -0.054097351 -0.053998873 -0.054345451 -0.054943591 -0.056090705][-0.04454365 -0.043765076 -0.044017769 -0.044574257 -0.045457259 -0.04617539 -0.046478633 -0.046766076 -0.046838023 -0.046401579 -0.045840185 -0.04558339 -0.045856651 -0.046331838 -0.047530502][-0.040224869 -0.039030042 -0.039127737 -0.039526407 -0.040132187 -0.040516123 -0.040552378 -0.040684462 -0.040691357 -0.040302679 -0.039828908 -0.039627492 -0.039853953 -0.040306456 -0.041589316][-0.039046012 -0.037564848 -0.037539817 -0.03784607 -0.038228743 -0.038328376 -0.038156785 -0.038074777 -0.037973337 -0.037675481 -0.037413802 -0.037246976 -0.037310727 -0.037719306 -0.039010316][-0.041614175 -0.040101394 -0.040033534 -0.040225126 -0.040433243 -0.040449008 -0.04025751 -0.040069357 -0.039975014 -0.039904356 -0.03985266 -0.039840452 -0.039887641 -0.040168442 -0.041081604][-0.047182504 -0.046059143 -0.04607445 -0.046312712 -0.046500381 -0.046335511 -0.045904983 -0.04548648 -0.045298327 -0.045268018 -0.045355983 -0.045500971 -0.045579892 -0.045731146 -0.046335161][-0.052998014 -0.052295081 -0.052581154 -0.053007483 -0.053362694 -0.0532949 -0.052824691 -0.05224102 -0.051911265 -0.051882446 -0.052060179 -0.052338663 -0.052454025 -0.05250198 -0.052800674][-0.059811153 -0.059479214 -0.059997879 -0.060504772 -0.060974579 -0.060995214 -0.060499016 -0.059747372 -0.059234153 -0.059172958 -0.0593133 -0.05944787 -0.059513196 -0.059422538 -0.059436306][-0.068062268 -0.068111733 -0.068866968 -0.069348916 -0.069818065 -0.069906592 -0.069424368 -0.0685967 -0.067964762 -0.067743793 -0.067652866 -0.067600839 -0.06747105 -0.067044571 -0.066586621][-0.076954938 -0.0772761 -0.078154936 -0.078719795 -0.07915476 -0.079278223 -0.078887366 -0.0781584 -0.077565 -0.077211864 -0.076935269 -0.07666418 -0.076310977 -0.075617574 -0.074785694][-0.083481111 -0.083813056 -0.084635943 -0.085109 -0.085490435 -0.085660852 -0.0854763 -0.084988914 -0.084486611 -0.084141292 -0.083844483 -0.083519749 -0.08310283 -0.082339354 -0.081369191]]...]
INFO - root - 2017-12-06 08:02:03.341559: step 36510, loss = 0.72, batch loss = 0.51 (9.6 examples/sec; 0.835 sec/batch; 68h:37m:18s remains)
INFO - root - 2017-12-06 08:02:11.671601: step 36520, loss = 0.81, batch loss = 0.60 (9.4 examples/sec; 0.847 sec/batch; 69h:37m:53s remains)
INFO - root - 2017-12-06 08:02:19.937703: step 36530, loss = 0.81, batch loss = 0.59 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:34s remains)
INFO - root - 2017-12-06 08:02:28.342985: step 36540, loss = 0.82, batch loss = 0.60 (9.7 examples/sec; 0.826 sec/batch; 67h:54m:04s remains)
INFO - root - 2017-12-06 08:02:36.549676: step 36550, loss = 0.83, batch loss = 0.62 (9.6 examples/sec; 0.832 sec/batch; 68h:25m:18s remains)
INFO - root - 2017-12-06 08:02:44.835221: step 36560, loss = 0.78, batch loss = 0.57 (9.4 examples/sec; 0.854 sec/batch; 70h:13m:57s remains)
INFO - root - 2017-12-06 08:02:53.192666: step 36570, loss = 0.81, batch loss = 0.60 (9.5 examples/sec; 0.839 sec/batch; 68h:58m:30s remains)
INFO - root - 2017-12-06 08:03:01.525836: step 36580, loss = 0.87, batch loss = 0.66 (9.7 examples/sec; 0.822 sec/batch; 67h:34m:22s remains)
INFO - root - 2017-12-06 08:03:09.930551: step 36590, loss = 0.87, batch loss = 0.65 (9.6 examples/sec; 0.834 sec/batch; 68h:32m:17s remains)
INFO - root - 2017-12-06 08:03:18.367844: step 36600, loss = 0.81, batch loss = 0.59 (9.2 examples/sec; 0.867 sec/batch; 71h:16m:24s remains)
2017-12-06 08:03:19.115196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.087970182 -0.083066233 -0.079115085 -0.076062806 -0.0740239 -0.073194467 -0.073436663 -0.074511528 -0.075949952 -0.077505507 -0.078373045 -0.078323431 -0.077215731 -0.07528428 -0.072989032][-0.096110031 -0.090425953 -0.085437149 -0.081523493 -0.078758806 -0.077384256 -0.077336155 -0.078459859 -0.080276892 -0.082372792 -0.083675258 -0.083782494 -0.082367353 -0.0797985 -0.076576173][-0.10525485 -0.099287219 -0.093666106 -0.088826716 -0.085072771 -0.082579978 -0.081536584 -0.082184516 -0.083987661 -0.086415857 -0.088136077 -0.088536769 -0.087126806 -0.084149808 -0.08023718][-0.11464352 -0.10869172 -0.10288874 -0.097205333 -0.0921755 -0.088248141 -0.085721657 -0.085138574 -0.086208932 -0.088644907 -0.090756476 -0.091641918 -0.090670608 -0.08793363 -0.083888136][-0.12311309 -0.11800584 -0.11227749 -0.1057604 -0.099343628 -0.093712009 -0.089334689 -0.087211818 -0.087206125 -0.089232624 -0.09147834 -0.092881784 -0.092598386 -0.090418078 -0.0866712][-0.12928106 -0.1252692 -0.11979602 -0.11277495 -0.10518454 -0.097973727 -0.092068508 -0.088594794 -0.087412052 -0.088676646 -0.090672448 -0.092287391 -0.092537947 -0.091085441 -0.088024847][-0.13159642 -0.12948681 -0.12499487 -0.1178948 -0.1095871 -0.10119643 -0.093972906 -0.089237064 -0.086995363 -0.087502979 -0.089117415 -0.090763427 -0.091394968 -0.090535492 -0.088149384][-0.12974977 -0.12915394 -0.12578894 -0.11921844 -0.11079364 -0.10196387 -0.094110161 -0.088656947 -0.085653186 -0.085375339 -0.086522974 -0.08802592 -0.088819243 -0.088408157 -0.086691581][-0.12349202 -0.12397103 -0.12157609 -0.11591671 -0.10811795 -0.099809907 -0.092298143 -0.086907923 -0.083767869 -0.083028726 -0.083660826 -0.084848017 -0.085596889 -0.085368954 -0.084079884][-0.11450209 -0.11533219 -0.11356983 -0.10887989 -0.10241519 -0.095419019 -0.089058593 -0.084367469 -0.081475407 -0.080465071 -0.080582775 -0.081278205 -0.081753716 -0.081580207 -0.0806445][-0.10428616 -0.10536515 -0.10417146 -0.10052852 -0.09547928 -0.089874916 -0.084776774 -0.08096166 -0.078553535 -0.077506334 -0.077340625 -0.07764367 -0.077874251 -0.077753782 -0.077205047][-0.0942212 -0.0952704 -0.094599739 -0.091934063 -0.0881508 -0.083979689 -0.08028163 -0.07739608 -0.075569883 -0.074758962 -0.074538372 -0.074671432 -0.07482183 -0.074765235 -0.074439794][-0.086915374 -0.087564029 -0.08709076 -0.085094064 -0.082245015 -0.079192743 -0.076482385 -0.074319705 -0.072948687 -0.072346345 -0.072187304 -0.072308861 -0.07246042 -0.072524369 -0.072465442][-0.083986066 -0.084302142 -0.083553277 -0.081620649 -0.079245076 -0.076776825 -0.074434742 -0.072555065 -0.071366645 -0.070816696 -0.070645086 -0.070709005 -0.070811667 -0.07096155 -0.071098879][-0.086438932 -0.086738385 -0.085607335 -0.083088391 -0.079996735 -0.076935723 -0.07424368 -0.072118878 -0.070794947 -0.070111819 -0.06979461 -0.069802746 -0.069891348 -0.07007584 -0.070283368]]...]
INFO - root - 2017-12-06 08:03:27.449560: step 36610, loss = 0.82, batch loss = 0.61 (9.7 examples/sec; 0.821 sec/batch; 67h:29m:48s remains)
INFO - root - 2017-12-06 08:03:35.908237: step 36620, loss = 0.86, batch loss = 0.64 (9.8 examples/sec; 0.815 sec/batch; 66h:59m:17s remains)
INFO - root - 2017-12-06 08:03:44.285035: step 36630, loss = 0.91, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 66h:15m:27s remains)
INFO - root - 2017-12-06 08:03:52.614369: step 36640, loss = 0.77, batch loss = 0.55 (9.4 examples/sec; 0.855 sec/batch; 70h:16m:50s remains)
INFO - root - 2017-12-06 08:04:00.575803: step 36650, loss = 0.79, batch loss = 0.57 (9.6 examples/sec; 0.838 sec/batch; 68h:50m:16s remains)
INFO - root - 2017-12-06 08:04:09.030617: step 36660, loss = 0.82, batch loss = 0.60 (9.6 examples/sec; 0.832 sec/batch; 68h:24m:42s remains)
INFO - root - 2017-12-06 08:04:17.283821: step 36670, loss = 0.76, batch loss = 0.55 (9.4 examples/sec; 0.851 sec/batch; 69h:55m:49s remains)
INFO - root - 2017-12-06 08:04:25.683823: step 36680, loss = 0.73, batch loss = 0.52 (9.9 examples/sec; 0.810 sec/batch; 66h:32m:58s remains)
INFO - root - 2017-12-06 08:04:34.029316: step 36690, loss = 0.83, batch loss = 0.62 (9.0 examples/sec; 0.886 sec/batch; 72h:47m:43s remains)
INFO - root - 2017-12-06 08:04:42.379034: step 36700, loss = 0.91, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 68h:34m:31s remains)
2017-12-06 08:04:43.128734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044967983 -0.044304777 -0.043514661 -0.04267348 -0.041821092 -0.040822662 -0.039855223 -0.039335381 -0.040173821 -0.043031946 -0.048354771 -0.056227736 -0.065692924 -0.0747944 -0.08169923][-0.04062942 -0.039881751 -0.03915387 -0.03845707 -0.037698083 -0.036701053 -0.035700951 -0.035167318 -0.036155716 -0.039570827 -0.046078097 -0.055364132 -0.066431746 -0.077163056 -0.085441463][-0.036062032 -0.035154115 -0.034660403 -0.03424463 -0.033649631 -0.032740634 -0.031745471 -0.031161327 -0.032259714 -0.036144916 -0.04353885 -0.053920664 -0.066071227 -0.078049883 -0.087563545][-0.031534258 -0.030311488 -0.029880732 -0.029720228 -0.029308625 -0.028518341 -0.027589411 -0.02709198 -0.028418597 -0.032816082 -0.040987834 -0.052192062 -0.065230563 -0.078189254 -0.088592045][-0.026971459 -0.025414646 -0.024796925 -0.024572663 -0.024095185 -0.023314491 -0.022504456 -0.022310771 -0.024140671 -0.029332552 -0.038551945 -0.050621334 -0.064538524 -0.078203753 -0.08925879][-0.023450464 -0.021280766 -0.020341605 -0.019884832 -0.019191772 -0.018346377 -0.017651625 -0.017935611 -0.020454213 -0.026571706 -0.03687286 -0.049870204 -0.064656533 -0.078966595 -0.090457022][-0.021474779 -0.018703826 -0.017348953 -0.016431749 -0.015359409 -0.01435297 -0.013810568 -0.0146145 -0.01787027 -0.025071628 -0.036419336 -0.050381616 -0.065803453 -0.080419131 -0.092123285][-0.020967089 -0.017768845 -0.015939973 -0.014408924 -0.012815401 -0.011549957 -0.010989681 -0.012116507 -0.016064964 -0.0244191 -0.036875192 -0.05185784 -0.067858137 -0.08247865 -0.094158575][-0.021800041 -0.018296644 -0.01598601 -0.013974756 -0.01211115 -0.01061482 -0.010033652 -0.011354819 -0.015792452 -0.024790041 -0.037901048 -0.053450413 -0.069667161 -0.084222108 -0.095663466][-0.024097517 -0.020643756 -0.018107407 -0.015837416 -0.01388479 -0.012305908 -0.011654817 -0.012865163 -0.017190188 -0.026010334 -0.038920756 -0.054201052 -0.07003051 -0.084144838 -0.094990321][-0.027582392 -0.024449795 -0.02211903 -0.020019248 -0.018237382 -0.016735382 -0.016060606 -0.01701387 -0.020732075 -0.028577529 -0.040287942 -0.054339778 -0.068930306 -0.08184877 -0.091835938][-0.032238889 -0.029613174 -0.027701229 -0.025916792 -0.024378009 -0.023032472 -0.022275299 -0.022797257 -0.025676273 -0.032113459 -0.041945796 -0.05392044 -0.066663243 -0.077952117 -0.086826086][-0.037485048 -0.035429418 -0.034020662 -0.032652568 -0.031450819 -0.030311234 -0.029533427 -0.029655863 -0.031620931 -0.036341265 -0.043895341 -0.05349179 -0.063907295 -0.073257178 -0.08077424][-0.042428605 -0.040916543 -0.040009279 -0.039120272 -0.038307972 -0.037513368 -0.036912885 -0.03685398 -0.038009357 -0.041000195 -0.046227627 -0.053076405 -0.060901497 -0.068026066 -0.073949888][-0.046953935 -0.045877934 -0.045444988 -0.044969965 -0.044484232 -0.043981191 -0.043534476 -0.043397404 -0.043974787 -0.045610711 -0.048800256 -0.053246569 -0.058598261 -0.063474685 -0.067651466]]...]
INFO - root - 2017-12-06 08:04:51.560461: step 36710, loss = 0.82, batch loss = 0.61 (9.8 examples/sec; 0.817 sec/batch; 67h:10m:01s remains)
INFO - root - 2017-12-06 08:04:59.811250: step 36720, loss = 0.82, batch loss = 0.61 (9.7 examples/sec; 0.824 sec/batch; 67h:40m:52s remains)
INFO - root - 2017-12-06 08:05:08.217545: step 36730, loss = 0.71, batch loss = 0.49 (9.6 examples/sec; 0.830 sec/batch; 68h:11m:06s remains)
INFO - root - 2017-12-06 08:05:16.555422: step 36740, loss = 0.78, batch loss = 0.57 (9.5 examples/sec; 0.840 sec/batch; 69h:00m:17s remains)
INFO - root - 2017-12-06 08:05:24.628883: step 36750, loss = 0.92, batch loss = 0.71 (9.7 examples/sec; 0.823 sec/batch; 67h:36m:00s remains)
INFO - root - 2017-12-06 08:05:32.999074: step 36760, loss = 0.78, batch loss = 0.56 (9.7 examples/sec; 0.826 sec/batch; 67h:49m:48s remains)
INFO - root - 2017-12-06 08:05:41.358753: step 36770, loss = 0.83, batch loss = 0.62 (9.7 examples/sec; 0.827 sec/batch; 67h:55m:07s remains)
INFO - root - 2017-12-06 08:05:49.618661: step 36780, loss = 0.88, batch loss = 0.67 (9.4 examples/sec; 0.853 sec/batch; 70h:03m:08s remains)
INFO - root - 2017-12-06 08:05:58.007559: step 36790, loss = 0.76, batch loss = 0.55 (10.2 examples/sec; 0.788 sec/batch; 64h:44m:10s remains)
INFO - root - 2017-12-06 08:06:06.237151: step 36800, loss = 0.79, batch loss = 0.58 (9.4 examples/sec; 0.848 sec/batch; 69h:40m:57s remains)
2017-12-06 08:06:06.977952: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.3709481 1.3937209 1.0514768 0.6099602 0.25836703 0.063410208 -0.035698321 -0.064990193 -0.069028996 -0.089670032 -0.085643545 -0.062624648 -0.05631512 -0.057239886 -0.05905167][1.07451 1.3994879 1.4215081 1.2445356 0.86532414 0.54336965 0.30482385 0.098057389 -0.020845406 -0.048444781 -0.047169786 -0.051726978 -0.057857182 -0.056388643 -0.057790257][0.58238465 0.95071709 1.2318248 1.4197972 1.3891616 1.1963387 0.95024359 0.59684765 0.29192755 0.06395638 -0.033849642 -0.034858324 -0.023892775 -0.032129228 -0.050983615][0.48382407 0.65245032 0.88966691 1.2452456 1.4999831 1.613346 1.5773823 1.3175423 0.90365684 0.44749153 0.13941579 -0.0083570108 -0.033175983 -0.019412614 -0.016654223][0.361479 0.49339396 0.69011271 0.93710089 1.2526847 1.5280578 1.659427 1.5701256 1.2492965 0.75665981 0.30630475 0.053807527 -0.03271161 -0.041602161 -0.036780506][-0.057740681 0.10821196 0.27551678 0.5277195 0.84008133 1.0638411 1.2774739 1.2639024 1.049397 0.68422496 0.33958316 0.1162852 -0.011300862 -0.045144975 -0.054366015][-0.27437782 -0.38731685 -0.30231455 -0.054535028 0.23421746 0.51005715 0.7530058 0.81347853 0.64327222 0.40452817 0.1639688 0.042179495 -0.023108356 -0.0462137 -0.062908545][-0.2640698 -0.44095129 -0.64599997 -0.63079333 -0.4205009 -0.14177582 0.11634764 0.24433991 0.20798808 0.050785765 -0.0667935 -0.095253557 -0.07798937 -0.06168887 -0.064333215][-0.20983833 -0.38182256 -0.63388312 -0.77755332 -0.79632682 -0.62768155 -0.40366608 -0.23792583 -0.16268462 -0.16178739 -0.17442282 -0.18993086 -0.1422213 -0.08731205 -0.067590192][-0.10456099 -0.26240247 -0.50653374 -0.69334137 -0.84375763 -0.79012913 -0.62716639 -0.46964365 -0.31777808 -0.2571367 -0.21764828 -0.17697215 -0.13582788 -0.096041843 -0.072895542][0.16527936 -0.043428771 -0.27223194 -0.379824 -0.51699704 -0.51642245 -0.467814 -0.37658226 -0.24165106 -0.15384579 -0.1163253 -0.11732691 -0.10787234 -0.086114496 -0.07503292][0.419963 0.2915774 0.0073562637 -0.14595941 -0.22954598 -0.23509383 -0.18724526 -0.1260711 -0.053262651 -0.011879787 -0.01226753 -0.026953928 -0.017656222 -0.045592442 -0.07491459][0.48660862 0.31899047 0.0715552 -0.052426629 -0.069131024 -0.025926687 0.054692 0.0839203 0.11117424 0.10332161 0.085513115 0.034968749 -0.017321296 -0.044119839 -0.07369265][0.49554974 0.35850611 0.24844348 0.14626028 0.063822895 0.038095996 0.054632962 0.11188595 0.14568667 0.1584911 0.11801383 0.060244218 -0.0084075034 -0.045345958 -0.096507013][0.4699291 0.38549033 0.22703105 0.14615928 0.12716888 0.083419874 0.041860223 0.039238781 0.049018845 0.072981611 0.060746998 0.02497533 -0.022891924 -0.053232644 -0.054712489]]...]
INFO - root - 2017-12-06 08:06:15.264987: step 36810, loss = 0.85, batch loss = 0.64 (10.0 examples/sec; 0.803 sec/batch; 65h:58m:42s remains)
INFO - root - 2017-12-06 08:06:23.619093: step 36820, loss = 0.83, batch loss = 0.62 (9.5 examples/sec; 0.844 sec/batch; 69h:18m:49s remains)
INFO - root - 2017-12-06 08:06:31.976517: step 36830, loss = 0.83, batch loss = 0.62 (9.6 examples/sec; 0.837 sec/batch; 68h:46m:49s remains)
INFO - root - 2017-12-06 08:06:40.378799: step 36840, loss = 0.81, batch loss = 0.59 (9.1 examples/sec; 0.884 sec/batch; 72h:35m:23s remains)
INFO - root - 2017-12-06 08:06:48.846488: step 36850, loss = 0.83, batch loss = 0.61 (9.6 examples/sec; 0.835 sec/batch; 68h:33m:49s remains)
INFO - root - 2017-12-06 08:06:57.194040: step 36860, loss = 0.74, batch loss = 0.53 (9.4 examples/sec; 0.853 sec/batch; 70h:05m:25s remains)
INFO - root - 2017-12-06 08:07:05.551268: step 36870, loss = 0.85, batch loss = 0.63 (9.7 examples/sec; 0.827 sec/batch; 67h:56m:32s remains)
INFO - root - 2017-12-06 08:07:13.929684: step 36880, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.846 sec/batch; 69h:26m:43s remains)
INFO - root - 2017-12-06 08:07:22.235245: step 36890, loss = 0.77, batch loss = 0.55 (9.8 examples/sec; 0.812 sec/batch; 66h:42m:02s remains)
INFO - root - 2017-12-06 08:07:30.567775: step 36900, loss = 0.86, batch loss = 0.64 (10.0 examples/sec; 0.796 sec/batch; 65h:22m:46s remains)
2017-12-06 08:07:31.292525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063947707 -0.081429437 -0.090130724 -0.085638888 -0.0879221 -0.093250282 -0.084397607 -0.041199956 0.0012646466 0.0033767074 -0.011092536 -0.011750147 -0.019440785 -0.058093533 -0.10481479][-0.067255713 -0.085101619 -0.091992758 -0.092017248 -0.099538036 -0.10790037 -0.1015588 -0.069567792 -0.048721761 -0.059349786 -0.066018067 -0.057606749 -0.059550196 -0.088557 -0.12067711][-0.093487337 -0.10213394 -0.10284094 -0.10011137 -0.10560329 -0.11845426 -0.11371909 -0.085566461 -0.066920392 -0.073895134 -0.074880853 -0.058738954 -0.056252971 -0.079124928 -0.10455581][-0.12475941 -0.1189244 -0.11153351 -0.11045541 -0.11885142 -0.13113046 -0.12258683 -0.090814017 -0.07190647 -0.075191192 -0.064555191 -0.038892772 -0.030848447 -0.046042319 -0.066154912][-0.12646621 -0.12031914 -0.11257409 -0.10972356 -0.11268555 -0.12056258 -0.11115501 -0.073750123 -0.0515976 -0.049435444 -0.035475355 -0.013805561 -0.013857916 -0.028619323 -0.048901059][-0.13115464 -0.1157703 -0.10300861 -0.09281414 -0.08765249 -0.087655969 -0.074588127 -0.043009717 -0.035311613 -0.044208575 -0.041315891 -0.034688085 -0.038934283 -0.046030346 -0.053277142][-0.10429977 -0.091439836 -0.072295621 -0.0504853 -0.033068158 -0.026333816 -0.017308384 0.00037608296 -0.015890613 -0.048688535 -0.060321055 -0.066755466 -0.076311029 -0.079461031 -0.077791251][-0.07318411 -0.062385973 -0.040904481 -0.022196583 -0.0059614107 -0.0038792416 -0.0065669268 -0.0097917542 -0.042388752 -0.082674019 -0.10323717 -0.1116345 -0.11395848 -0.10991746 -0.099523388][-0.05491583 -0.0517282 -0.046782978 -0.045128819 -0.04303433 -0.046079006 -0.047941543 -0.049127508 -0.07048364 -0.097200684 -0.11230928 -0.11544497 -0.11165535 -0.10267061 -0.091316231][-0.090973653 -0.092184193 -0.094364449 -0.097603105 -0.098810732 -0.096682407 -0.088829167 -0.088452861 -0.098349333 -0.1044233 -0.10691574 -0.10272053 -0.094381951 -0.088199414 -0.082390554][-0.10630335 -0.1053938 -0.10681886 -0.11132304 -0.11836813 -0.12459368 -0.12269795 -0.11802152 -0.10627081 -0.095678635 -0.091230437 -0.089138523 -0.089536853 -0.085199192 -0.082430467][-0.10004658 -0.094995089 -0.090928212 -0.092324793 -0.096795008 -0.099956743 -0.097621426 -0.094483949 -0.090549894 -0.083675779 -0.07984262 -0.079759642 -0.081658944 -0.081469588 -0.08448644][-0.092438459 -0.088606082 -0.086042911 -0.085422173 -0.0855725 -0.085774757 -0.083653733 -0.08145459 -0.080707058 -0.078735895 -0.078971013 -0.084599383 -0.085553341 -0.08759924 -0.093634963][-0.08956781 -0.086067088 -0.084765583 -0.084419757 -0.08443664 -0.0844918 -0.084047377 -0.083711132 -0.081720866 -0.078689866 -0.084078223 -0.090964921 -0.087285854 -0.088939443 -0.099881314][-0.086581118 -0.085460871 -0.085111849 -0.084907077 -0.0847539 -0.084650405 -0.084849186 -0.085338533 -0.0839117 -0.080196425 -0.079320937 -0.081342109 -0.085345685 -0.087561794 -0.094445817]]...]
INFO - root - 2017-12-06 08:07:39.741152: step 36910, loss = 0.71, batch loss = 0.50 (9.4 examples/sec; 0.851 sec/batch; 69h:52m:07s remains)
INFO - root - 2017-12-06 08:07:47.948844: step 36920, loss = 0.89, batch loss = 0.68 (9.9 examples/sec; 0.809 sec/batch; 66h:27m:42s remains)
INFO - root - 2017-12-06 08:07:56.160621: step 36930, loss = 0.72, batch loss = 0.51 (9.0 examples/sec; 0.884 sec/batch; 72h:35m:14s remains)
INFO - root - 2017-12-06 08:08:04.454844: step 36940, loss = 0.76, batch loss = 0.55 (10.1 examples/sec; 0.793 sec/batch; 65h:08m:25s remains)
INFO - root - 2017-12-06 08:08:12.931664: step 36950, loss = 0.78, batch loss = 0.57 (9.4 examples/sec; 0.853 sec/batch; 70h:02m:02s remains)
INFO - root - 2017-12-06 08:08:21.347964: step 36960, loss = 0.83, batch loss = 0.61 (9.6 examples/sec; 0.834 sec/batch; 68h:30m:07s remains)
INFO - root - 2017-12-06 08:08:29.647168: step 36970, loss = 0.85, batch loss = 0.64 (9.7 examples/sec; 0.827 sec/batch; 67h:51m:23s remains)
INFO - root - 2017-12-06 08:08:38.060182: step 36980, loss = 0.76, batch loss = 0.55 (9.1 examples/sec; 0.877 sec/batch; 71h:57m:58s remains)
INFO - root - 2017-12-06 08:08:46.363476: step 36990, loss = 0.78, batch loss = 0.56 (9.4 examples/sec; 0.849 sec/batch; 69h:42m:16s remains)
INFO - root - 2017-12-06 08:08:54.711509: step 37000, loss = 0.79, batch loss = 0.58 (9.2 examples/sec; 0.870 sec/batch; 71h:25m:59s remains)
2017-12-06 08:08:55.448161: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17271936 -0.055313881 -0.090101115 -0.011529103 0.11163098 0.24619752 0.3035993 0.44813222 0.54092592 0.53588235 0.5321691 0.42754078 0.44774759 0.6679011 0.98602533][-0.056594003 -0.15066814 -0.51431489 -0.79916352 -0.985387 -0.6760757 -0.31779844 -0.16729805 -0.11710106 0.025024407 0.045533314 0.25587451 0.44996095 0.67837018 1.0116106][-0.30538231 -0.3232854 -0.61230749 -0.89760673 -1.0323025 -1.0276574 -0.78060246 -0.56781572 -0.43084949 -0.42878324 -0.38520822 -0.25284928 -0.078649193 0.10289125 0.21851751][-0.31785867 -0.0876297 -0.32973909 -0.64683193 -0.81137568 -0.82595658 -0.56664431 -0.53150272 -0.33621979 -0.36896172 -0.60196686 -0.69004047 -0.87386441 -0.75676078 -0.54550612][-0.66113538 -0.2006893 -0.11261341 -0.24625434 -0.36016127 -0.27552444 -0.21623419 -0.0794937 -0.062783368 -0.10203753 -0.14764255 -0.40290678 -0.61306024 -0.80374134 -0.95472][-0.36717123 0.40843177 0.98618937 1.0349412 0.83151674 0.67160279 0.41939974 0.38800102 0.21434501 -0.0688737 -0.22543651 -0.35291272 -0.50602937 -0.53235734 -0.45691305][-0.21268412 0.86484265 1.7518351 2.1895685 1.9968147 1.9711986 1.5341313 1.1556649 0.84171295 0.26290146 -0.076163128 -0.39911273 -0.51411641 -0.49100411 -0.46362188][0.15907007 0.99540567 1.8980898 2.507762 2.2123921 2.1721833 1.8629006 1.6051506 1.2680957 0.6410166 0.24666941 0.012656547 -0.22235711 -0.28941259 -0.36982676][0.17974824 0.83129084 1.4397995 2.1739998 2.3056281 2.3180583 2.1171906 1.8365753 1.4165441 0.93114614 0.54410708 0.30027777 -0.022275828 -0.19463542 -0.29280818][0.078078389 0.57478738 0.96389687 1.3430566 1.6799669 1.6996204 1.5538794 1.3165339 1.1546625 0.79084843 0.6120103 0.42804831 0.25442085 0.10135135 -0.10538218][-0.45365664 -0.06201411 0.37620759 0.82144308 1.3074987 1.2830678 1.0917959 0.86626005 0.57883263 0.38358968 0.30562621 0.26009065 0.17268035 0.15594171 0.091831967][-1.0808561 -0.89788163 -0.55001754 -0.24326165 0.10323827 0.37856194 0.46411389 0.35710651 0.2023415 0.22414276 0.14459082 0.08449778 0.063045442 0.030963406 0.018960014][-1.0869985 -0.96728778 -0.70521891 -0.39360136 -0.014768571 0.17503875 0.23422351 0.22161785 0.13853627 -0.0083568841 -0.0098825172 0.031674787 -0.037990473 -0.041549098 -0.042124916][-0.7648325 -0.72652727 -0.60836273 -0.43420491 -0.19706431 -0.00829909 0.027458183 -0.022990271 -0.14101517 -0.14201529 -0.078098044 0.022358447 -0.10921803 -0.071924396 -0.0634826][-0.35801694 -0.35954136 -0.24507789 -0.16134748 -0.010892786 0.082103968 0.048279226 0.025024451 -0.1052635 -0.16929956 -0.17135334 -0.0027414784 -0.094524935 -0.070912771 -0.050687447]]...]
INFO - root - 2017-12-06 08:09:03.889504: step 37010, loss = 0.82, batch loss = 0.61 (9.8 examples/sec; 0.815 sec/batch; 66h:55m:38s remains)
INFO - root - 2017-12-06 08:09:12.213360: step 37020, loss = 0.85, batch loss = 0.64 (9.5 examples/sec; 0.842 sec/batch; 69h:08m:29s remains)
INFO - root - 2017-12-06 08:09:20.372576: step 37030, loss = 0.80, batch loss = 0.58 (9.6 examples/sec; 0.831 sec/batch; 68h:13m:51s remains)
INFO - root - 2017-12-06 08:09:28.852299: step 37040, loss = 0.80, batch loss = 0.59 (9.3 examples/sec; 0.860 sec/batch; 70h:34m:38s remains)
INFO - root - 2017-12-06 08:09:37.348647: step 37050, loss = 0.76, batch loss = 0.54 (9.6 examples/sec; 0.835 sec/batch; 68h:30m:38s remains)
INFO - root - 2017-12-06 08:09:45.754464: step 37060, loss = 0.80, batch loss = 0.58 (9.8 examples/sec; 0.815 sec/batch; 66h:51m:28s remains)
INFO - root - 2017-12-06 08:09:54.066523: step 37070, loss = 0.80, batch loss = 0.59 (9.5 examples/sec; 0.842 sec/batch; 69h:06m:52s remains)
INFO - root - 2017-12-06 08:10:02.313640: step 37080, loss = 1.00, batch loss = 0.79 (9.2 examples/sec; 0.870 sec/batch; 71h:22m:35s remains)
INFO - root - 2017-12-06 08:10:10.644463: step 37090, loss = 0.81, batch loss = 0.60 (9.5 examples/sec; 0.841 sec/batch; 68h:59m:08s remains)
INFO - root - 2017-12-06 08:10:19.030057: step 37100, loss = 0.78, batch loss = 0.57 (9.1 examples/sec; 0.876 sec/batch; 71h:50m:23s remains)
2017-12-06 08:10:19.831853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.28591987 -0.43178403 -0.57717311 -0.68017471 -0.71792018 -0.63162524 -0.38559586 -0.23952386 -0.28690082 -0.47979274 -0.70146525 -0.96988249 -0.97518939 -0.87703073 -0.74564862][-0.37066665 -0.58397251 -0.81359029 -1.0370332 -1.2007059 -1.2331425 -1.1038088 -1.0459988 -1.0391445 -1.1991242 -1.4204566 -1.596054 -1.5187821 -1.2317336 -0.95489407][-0.48393983 -0.7505489 -1.0560189 -1.32749 -1.4819925 -1.3603317 -0.99501336 -0.72184396 -0.64137369 -0.76809061 -0.9794637 -1.1093755 -1.0839301 -0.94548088 -0.65698606][-0.57870805 -0.841628 -1.1931319 -1.5182585 -1.730605 -1.6822397 -1.2777066 -0.75936472 -0.342657 -0.25793687 -0.38008097 -0.55772758 -0.68134916 -0.71284378 -0.66243553][-0.35414642 -0.46258691 -0.69186419 -0.97001106 -1.311164 -1.4822705 -1.4157084 -1.1645937 -0.73198444 -0.37588149 -0.19487242 -0.14932877 -0.22706136 -0.18734032 -0.090151846][-0.047090158 0.18163714 0.3334569 0.25425646 -0.076285765 -0.45440695 -0.69163889 -0.74066508 -0.655313 -0.559145 -0.45818642 -0.32085839 -0.17565876 -0.083850756 -0.055086635][0.15102974 0.54780382 1.0231175 1.2201008 1.0631003 0.6479829 0.053390719 -0.3614606 -0.58110207 -0.60449588 -0.55057526 -0.44407943 -0.21987575 -0.1240919 -0.055241022][0.32736892 0.67884326 1.1415024 1.4463844 1.3963155 1.0921906 0.60007924 0.018540494 -0.45047855 -0.65326262 -0.63173562 -0.51132452 -0.25845191 -0.13196129 -0.020490736][0.2208223 0.50706983 0.8793925 1.1311272 1.1479584 1.0069292 0.65954387 0.33285591 -0.047041103 -0.41960812 -0.5118826 -0.49772123 -0.35310081 -0.17052187 -0.018279053][-0.11349818 0.086582355 0.32168105 0.50596029 0.58563197 0.55612195 0.41188276 0.24728534 0.12898651 -0.017404176 -0.10040546 -0.19546022 -0.27350727 -0.22904006 -0.144723][-0.35642347 -0.30658731 -0.21563253 -0.080006711 -0.029523551 -0.069948182 -0.023778781 -0.028690234 -0.059776865 -0.068252608 -0.054849137 -0.0379281 -0.09796603 -0.12189443 -0.14454994][-0.47933352 -0.51321572 -0.49830845 -0.44553947 -0.38711452 -0.3676371 -0.50924009 -0.48849571 -0.35637856 -0.27434346 -0.23851085 -0.19302773 -0.11579934 -0.11046502 -0.11003072][-0.37965938 -0.46335921 -0.50685966 -0.51103204 -0.48333636 -0.51347023 -0.44415078 -0.4966473 -0.62974358 -0.59433734 -0.43642396 -0.36169082 -0.15911095 -0.14052977 -0.11912043][-0.32868153 -0.37903693 -0.42888632 -0.38182759 -0.31267449 -0.39696354 -0.45281777 -0.42380628 -0.41147244 -0.38572952 -0.44179389 -0.40208218 -0.15722096 -0.15091661 -0.10844126][-0.23309612 -0.27588746 -0.33398151 -0.35595894 -0.25474423 -0.2384415 -0.13212372 -0.10255225 -0.23256144 -0.20507663 -0.15191789 -0.16041622 -0.10604854 -0.10651947 -0.10302004]]...]
INFO - root - 2017-12-06 08:10:28.216340: step 37110, loss = 0.76, batch loss = 0.54 (9.4 examples/sec; 0.850 sec/batch; 69h:42m:25s remains)
INFO - root - 2017-12-06 08:10:36.586822: step 37120, loss = 0.73, batch loss = 0.52 (10.5 examples/sec; 0.758 sec/batch; 62h:13m:29s remains)
INFO - root - 2017-12-06 08:10:44.897786: step 37130, loss = 0.81, batch loss = 0.60 (9.8 examples/sec; 0.817 sec/batch; 67h:01m:09s remains)
INFO - root - 2017-12-06 08:10:53.161496: step 37140, loss = 0.84, batch loss = 0.63 (9.6 examples/sec; 0.837 sec/batch; 68h:42m:03s remains)
INFO - root - 2017-12-06 08:11:01.486106: step 37150, loss = 0.89, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 68h:21m:37s remains)
INFO - root - 2017-12-06 08:11:09.845650: step 37160, loss = 0.77, batch loss = 0.56 (9.6 examples/sec; 0.836 sec/batch; 68h:32m:55s remains)
INFO - root - 2017-12-06 08:11:18.238056: step 37170, loss = 0.86, batch loss = 0.65 (9.4 examples/sec; 0.848 sec/batch; 69h:35m:01s remains)
INFO - root - 2017-12-06 08:11:26.604561: step 37180, loss = 0.91, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 68h:28m:06s remains)
INFO - root - 2017-12-06 08:11:34.937684: step 37190, loss = 0.82, batch loss = 0.60 (9.5 examples/sec; 0.844 sec/batch; 69h:14m:33s remains)
INFO - root - 2017-12-06 08:11:43.227409: step 37200, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.828 sec/batch; 67h:56m:30s remains)
2017-12-06 08:11:43.934585: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.73940581 0.49718821 0.29428679 0.16043045 -0.020627581 -0.33242834 -0.34326565 -0.2648069 -0.36469775 0.033706479 0.71259326 1.5477091 2.0611653 1.6427237 1.0806241][1.203065 1.2886374 1.3540944 0.93976754 0.55688632 0.37270987 0.0067149997 -0.031493757 0.30364937 0.5345642 0.62100828 0.82398534 1.2208618 0.98852247 0.22894545][1.718696 1.6574664 1.7572901 1.543223 0.86432022 0.60508293 0.17678221 -0.0067533553 -0.0762902 0.29442185 0.61176133 0.65834033 0.48211789 -0.01315219 -0.9343729][1.453698 1.5021693 1.3434293 1.1382271 1.0078108 0.90459591 0.34016007 -0.023052171 -0.037759334 -0.23064749 -0.36238503 -0.3457135 -0.71262938 -1.047834 -1.2978379][0.8622241 1.0602449 1.3455321 1.0830541 1.0659676 1.1808208 1.1357374 0.51477748 0.160071 -0.22405955 -0.78692055 -1.014046 -1.4433619 -1.4843698 -1.4145348][1.8591439 1.7251283 1.6423057 1.2697108 1.3105382 1.5198393 1.5333322 1.3896145 1.1113628 0.38541996 -0.46351963 -0.56626076 -0.81319034 -0.65249753 -0.6909399][2.1799538 2.3390925 2.4678915 2.1707809 1.467543 1.1051184 1.0106976 1.0153103 0.83527243 0.63990051 0.64222252 0.67914379 0.38303888 0.65858036 1.0840666][2.7793403 2.5986488 2.1561511 1.7657342 1.42994 0.92025059 0.55486143 0.36939168 0.078027859 0.0060027614 0.25330037 0.57684141 1.0770911 0.88190693 0.80569053][1.1085579 1.0050093 0.72557873 0.2001041 -0.23885025 -0.35577875 -0.18742025 -0.38494104 -0.79042685 -0.29619956 0.32570815 0.42642689 0.70961529 0.90792537 1.1124605][-0.578555 -0.617179 -0.91130948 -1.0906335 -1.0629963 -1.4300336 -1.5448793 -1.3076812 -1.2538412 -1.2017715 -0.90083539 0.38011873 1.0626372 0.86512434 0.93732232][-2.4074962 -2.7572567 -2.9339721 -3.0749683 -3.1158957 -2.9286938 -2.78116 -2.795491 -2.8388183 -2.3052497 -2.0472474 -1.636048 -0.934702 -0.23664416 0.092104271][-1.8723992 -2.540499 -2.7732189 -2.5566919 -2.661556 -2.5331879 -2.3332334 -2.0066893 -1.8789167 -1.8058952 -1.6417176 -1.2357384 -1.1563331 -1.0605788 -0.72642928][-2.5385048 -2.6740079 -3.1370122 -3.4047687 -3.102582 -2.9620874 -2.8369262 -2.4787934 -2.1626875 -2.1396818 -2.1144328 -1.7374877 -1.0632895 -0.87913913 -1.2220531][-2.2175057 -2.2585247 -2.6669683 -3.0662239 -3.11689 -2.9646342 -2.5876732 -2.2116785 -2.0616367 -2.0903382 -1.8451688 -1.7731878 -1.6776471 -1.5548067 -1.432699][-0.220546 -0.54545724 -0.99341136 -1.2830803 -1.4813023 -1.8200281 -1.9901265 -1.9852192 -2.0559304 -1.9470894 -1.5919427 -1.5831983 -1.4687958 -1.7512186 -1.9301393]]...]
INFO - root - 2017-12-06 08:11:52.202087: step 37210, loss = 0.77, batch loss = 0.55 (10.5 examples/sec; 0.763 sec/batch; 62h:36m:18s remains)
INFO - root - 2017-12-06 08:12:00.443566: step 37220, loss = 0.77, batch loss = 0.55 (9.6 examples/sec; 0.834 sec/batch; 68h:26m:23s remains)
INFO - root - 2017-12-06 08:12:08.807974: step 37230, loss = 0.89, batch loss = 0.68 (9.7 examples/sec; 0.823 sec/batch; 67h:30m:36s remains)
INFO - root - 2017-12-06 08:12:17.286004: step 37240, loss = 0.98, batch loss = 0.77 (9.6 examples/sec; 0.832 sec/batch; 68h:13m:14s remains)
INFO - root - 2017-12-06 08:12:25.625380: step 37250, loss = 0.89, batch loss = 0.67 (9.5 examples/sec; 0.840 sec/batch; 68h:55m:29s remains)
INFO - root - 2017-12-06 08:12:34.105280: step 37260, loss = 0.88, batch loss = 0.66 (9.2 examples/sec; 0.871 sec/batch; 71h:28m:16s remains)
INFO - root - 2017-12-06 08:12:42.424132: step 37270, loss = 0.77, batch loss = 0.56 (9.7 examples/sec; 0.829 sec/batch; 67h:57m:02s remains)
INFO - root - 2017-12-06 08:12:50.861741: step 37280, loss = 0.71, batch loss = 0.50 (9.4 examples/sec; 0.847 sec/batch; 69h:25m:28s remains)
INFO - root - 2017-12-06 08:12:59.094375: step 37290, loss = 0.76, batch loss = 0.55 (9.6 examples/sec; 0.836 sec/batch; 68h:33m:23s remains)
INFO - root - 2017-12-06 08:13:07.508377: step 37300, loss = 0.77, batch loss = 0.55 (9.2 examples/sec; 0.868 sec/batch; 71h:10m:28s remains)
2017-12-06 08:13:08.230134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044605047 -0.043371852 -0.042285535 -0.041531023 -0.041213829 -0.0411737 -0.041521061 -0.042052548 -0.042640392 -0.043099582 -0.043578692 -0.044203486 -0.044930689 -0.045890506 -0.047180146][-0.040289644 -0.038096476 -0.036139484 -0.034636386 -0.033988997 -0.033927493 -0.034461118 -0.035234515 -0.036111917 -0.036828671 -0.037644617 -0.038759135 -0.040083118 -0.041518737 -0.043293882][-0.034895759 -0.031266488 -0.027935915 -0.025289476 -0.023987763 -0.023657702 -0.02458778 -0.025762483 -0.026984751 -0.027890474 -0.029056337 -0.030793618 -0.032758251 -0.034793839 -0.037330423][-0.028853767 -0.023653075 -0.018770427 -0.014750488 -0.012489624 -0.011624694 -0.012444168 -0.01359944 -0.015049003 -0.016313404 -0.018045567 -0.020394698 -0.023109056 -0.026065521 -0.029511169][-0.023353972 -0.016937584 -0.010586895 -0.005125761 -0.0018006861 -0.00023486465 -0.00051531941 -0.0010746345 -0.0019635856 -0.0032836199 -0.0055498183 -0.00877922 -0.012390092 -0.016242765 -0.020761706][-0.019267894 -0.012051731 -0.0045898184 0.001901418 0.0060365275 0.0082041472 0.0085008219 0.0085869581 0.00846307 0.0073487684 0.0049456805 0.0014278218 -0.0024428144 -0.0065372661 -0.012332402][-0.016839214 -0.0093945339 -0.0015280768 0.0054597929 0.0099948272 0.012602478 0.013503924 0.014193222 0.0145391 0.013675638 0.011487015 0.0083427355 0.0048220381 0.0010449216 -0.0054078549][-0.015281245 -0.0084918961 -0.0013898611 0.0051097795 0.0096054226 0.012437068 0.013891108 0.014960311 0.015449829 0.014900744 0.013348527 0.011071019 0.0083519667 0.0052659214 -0.0010307804][-0.014283434 -0.008799471 -0.0031527728 0.0021318346 0.0060834214 0.00873068 0.010511212 0.011700973 0.012301467 0.012170434 0.0113381 0.010021254 0.0082763582 0.0059734583 0.00046290457][-0.013732046 -0.0094414726 -0.0053032637 -0.00134781 0.0017158464 0.0040095523 0.0056714714 0.0067447871 0.0072493181 0.0071948394 0.0067825764 0.0060527027 0.0049943849 0.0034783483 -0.0010371432][-0.013061881 -0.009756811 -0.0069243014 -0.0042621344 -0.0020974055 -0.00047276914 0.00069693476 0.0015459582 0.00195764 0.0017993897 0.0013583228 0.0008444488 0.00035093725 -0.0006210804 -0.0043177828][-0.012043208 -0.0095208883 -0.0075997189 -0.00591442 -0.0045483261 -0.0035914779 -0.0029127821 -0.0023703352 -0.0022782311 -0.0027347729 -0.0034827664 -0.004026942 -0.0043402761 -0.0050110742 -0.0079162791][-0.0107366 -0.0087771863 -0.0074084997 -0.0062125549 -0.0052586347 -0.0045804456 -0.0041330159 -0.0039229989 -0.0042569116 -0.0051551387 -0.0063005537 -0.007255137 -0.0078070685 -0.0083532035 -0.010893755][-0.00935363 -0.007629171 -0.0063457191 -0.0050919503 -0.0039587244 -0.0030217618 -0.0024460182 -0.0023899227 -0.0031039938 -0.0044843033 -0.0062488392 -0.0079716966 -0.0091378316 -0.010151692 -0.012826174][-0.010020204 -0.0086204559 -0.00767041 -0.006246157 -0.0047231391 -0.0031978637 -0.0020494685 -0.0018251687 -0.0026263893 -0.0042440072 -0.0064498633 -0.0087798163 -0.010615662 -0.012248568 -0.014974497]]...]
INFO - root - 2017-12-06 08:13:16.442465: step 37310, loss = 0.89, batch loss = 0.68 (10.1 examples/sec; 0.794 sec/batch; 65h:04m:53s remains)
INFO - root - 2017-12-06 08:13:24.732444: step 37320, loss = 0.90, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:49m:01s remains)
INFO - root - 2017-12-06 08:13:33.189104: step 37330, loss = 0.83, batch loss = 0.62 (9.8 examples/sec; 0.818 sec/batch; 67h:03m:19s remains)
INFO - root - 2017-12-06 08:13:41.522493: step 37340, loss = 0.75, batch loss = 0.54 (9.7 examples/sec; 0.825 sec/batch; 67h:36m:08s remains)
INFO - root - 2017-12-06 08:13:49.840312: step 37350, loss = 0.83, batch loss = 0.62 (10.0 examples/sec; 0.796 sec/batch; 65h:15m:57s remains)
INFO - root - 2017-12-06 08:13:58.206881: step 37360, loss = 0.73, batch loss = 0.52 (9.4 examples/sec; 0.851 sec/batch; 69h:45m:00s remains)
INFO - root - 2017-12-06 08:14:06.502295: step 37370, loss = 0.82, batch loss = 0.61 (9.3 examples/sec; 0.864 sec/batch; 70h:50m:44s remains)
INFO - root - 2017-12-06 08:14:14.864207: step 37380, loss = 0.72, batch loss = 0.51 (9.6 examples/sec; 0.834 sec/batch; 68h:22m:00s remains)
INFO - root - 2017-12-06 08:14:23.201106: step 37390, loss = 0.81, batch loss = 0.60 (10.0 examples/sec; 0.798 sec/batch; 65h:27m:19s remains)
INFO - root - 2017-12-06 08:14:31.468046: step 37400, loss = 0.78, batch loss = 0.57 (10.4 examples/sec; 0.773 sec/batch; 63h:20m:10s remains)
2017-12-06 08:14:32.194980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041332986 -0.040049225 -0.040631451 -0.043130372 -0.047504596 -0.053400636 -0.060045127 -0.066629805 -0.071948014 -0.075660884 -0.077754013 -0.078770742 -0.078825392 -0.077116922 -0.073339678][-0.036292642 -0.034637567 -0.035044238 -0.03792515 -0.043134186 -0.050099272 -0.058230247 -0.066500217 -0.073391944 -0.0786165 -0.082058564 -0.084174551 -0.084777094 -0.083461493 -0.079461642][-0.030332416 -0.028240781 -0.028148897 -0.031128913 -0.036912307 -0.044732369 -0.053649403 -0.062582068 -0.07048589 -0.077266634 -0.082178414 -0.085442089 -0.08698988 -0.086557209 -0.082910642][-0.023504876 -0.02095858 -0.020473674 -0.02343227 -0.029519085 -0.037477776 -0.046273448 -0.055241678 -0.063745849 -0.07164067 -0.078102618 -0.08271125 -0.0854246 -0.085915819 -0.08307492][-0.016319498 -0.013988137 -0.013455711 -0.016352534 -0.022412315 -0.030128464 -0.038380027 -0.046754889 -0.054732416 -0.06283813 -0.070111938 -0.075856715 -0.079608157 -0.081123039 -0.079924077][-0.011349253 -0.0093520135 -0.0088740885 -0.01135017 -0.017013103 -0.023799919 -0.030446969 -0.037474684 -0.044658709 -0.052392151 -0.059932824 -0.066663533 -0.071386993 -0.074103318 -0.074635178][-0.0098183677 -0.008002758 -0.0075521395 -0.009273611 -0.013865471 -0.018771194 -0.023040555 -0.02775041 -0.033932131 -0.04161863 -0.0491776 -0.056090392 -0.061257735 -0.0650045 -0.067195885][-0.011287346 -0.0092606843 -0.0090258121 -0.010280877 -0.013198018 -0.016256541 -0.018354319 -0.020519935 -0.02517999 -0.032020796 -0.039137643 -0.045580309 -0.051117308 -0.055679496 -0.059007563][-0.015453391 -0.013665877 -0.0136507 -0.014701158 -0.016428888 -0.018206492 -0.019055761 -0.019565679 -0.021968462 -0.02661173 -0.031967849 -0.037761524 -0.042949412 -0.047684733 -0.051771961][-0.020720981 -0.019036263 -0.019079998 -0.019976556 -0.021297112 -0.022410177 -0.022833318 -0.022891246 -0.023822546 -0.026184715 -0.029657111 -0.033908807 -0.0380914 -0.042525232 -0.046584085][-0.026734546 -0.025005326 -0.025097497 -0.025824532 -0.026733316 -0.027430266 -0.027669236 -0.027634643 -0.027917042 -0.028962016 -0.030988321 -0.033669 -0.036719222 -0.04046363 -0.044125885][-0.032434028 -0.030794542 -0.030901469 -0.0314501 -0.032091387 -0.032543115 -0.032705117 -0.03273806 -0.032865383 -0.033292688 -0.034245383 -0.035850037 -0.037908636 -0.040638354 -0.04365623][-0.038108949 -0.036668494 -0.036581162 -0.036783013 -0.037058152 -0.037232697 -0.037232175 -0.037214331 -0.037308283 -0.03751355 -0.03797188 -0.038820479 -0.040096834 -0.042047784 -0.044416711][-0.042923778 -0.041641377 -0.041446168 -0.041408539 -0.041451558 -0.041382745 -0.041269775 -0.041233156 -0.041305352 -0.041419722 -0.041646615 -0.042157792 -0.042960998 -0.044315036 -0.046055365][-0.046365604 -0.045317143 -0.045224175 -0.04522799 -0.045291871 -0.045271672 -0.045240786 -0.045281183 -0.045333251 -0.045358323 -0.045406383 -0.045596018 -0.045979187 -0.04674 -0.047887441]]...]
INFO - root - 2017-12-06 08:14:40.569592: step 37410, loss = 0.79, batch loss = 0.58 (9.4 examples/sec; 0.847 sec/batch; 69h:27m:37s remains)
INFO - root - 2017-12-06 08:14:49.083079: step 37420, loss = 0.98, batch loss = 0.76 (9.1 examples/sec; 0.884 sec/batch; 72h:25m:35s remains)
INFO - root - 2017-12-06 08:14:57.499369: step 37430, loss = 0.93, batch loss = 0.71 (10.0 examples/sec; 0.801 sec/batch; 65h:39m:59s remains)
INFO - root - 2017-12-06 08:15:05.953960: step 37440, loss = 0.74, batch loss = 0.53 (9.4 examples/sec; 0.851 sec/batch; 69h:42m:51s remains)
INFO - root - 2017-12-06 08:15:14.230465: step 37450, loss = 0.74, batch loss = 0.52 (10.0 examples/sec; 0.799 sec/batch; 65h:28m:28s remains)
INFO - root - 2017-12-06 08:15:22.564186: step 37460, loss = 0.87, batch loss = 0.65 (9.9 examples/sec; 0.806 sec/batch; 66h:02m:53s remains)
INFO - root - 2017-12-06 08:15:30.819372: step 37470, loss = 0.85, batch loss = 0.63 (9.4 examples/sec; 0.847 sec/batch; 69h:26m:44s remains)
INFO - root - 2017-12-06 08:15:39.250489: step 37480, loss = 0.76, batch loss = 0.55 (9.9 examples/sec; 0.810 sec/batch; 66h:22m:54s remains)
INFO - root - 2017-12-06 08:15:47.757889: step 37490, loss = 0.88, batch loss = 0.67 (9.8 examples/sec; 0.816 sec/batch; 66h:50m:03s remains)
INFO - root - 2017-12-06 08:15:56.009581: step 37500, loss = 0.77, batch loss = 0.56 (9.3 examples/sec; 0.856 sec/batch; 70h:10m:23s remains)
2017-12-06 08:15:56.756307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0571732 -0.061474413 -0.066072658 -0.070601247 -0.075039446 -0.079201087 -0.082762927 -0.084484905 -0.083817385 -0.080362737 -0.074743927 -0.068149582 -0.0626381 -0.060578451 -0.0631932][-0.054787375 -0.059672222 -0.065640427 -0.07226067 -0.079001151 -0.08542905 -0.091096357 -0.094683014 -0.09503033 -0.091890417 -0.085865878 -0.078313291 -0.071210869 -0.067034923 -0.067383446][-0.051595222 -0.056330435 -0.063040853 -0.071159355 -0.079752736 -0.087803848 -0.094621904 -0.098901525 -0.099994406 -0.097414695 -0.091630213 -0.083919853 -0.076058693 -0.070508555 -0.069025636][-0.04872328 -0.052834738 -0.059408862 -0.067859024 -0.076725148 -0.085061558 -0.092063092 -0.096394278 -0.09737283 -0.094965085 -0.089688584 -0.08263731 -0.075229496 -0.069422849 -0.066748008][-0.04681832 -0.049887814 -0.0553519 -0.062779889 -0.070629418 -0.077911139 -0.084097661 -0.087732352 -0.088289596 -0.085798971 -0.081121713 -0.075218037 -0.069071144 -0.063891262 -0.060999397][-0.045701981 -0.047553804 -0.051504944 -0.057148069 -0.06312006 -0.068571515 -0.073069304 -0.075571753 -0.075670213 -0.07342869 -0.069718458 -0.065226413 -0.060591608 -0.056794342 -0.054443285][-0.045400266 -0.04617558 -0.048737824 -0.052345961 -0.056083176 -0.059510756 -0.062235564 -0.06352143 -0.06325651 -0.061513059 -0.059035037 -0.056022417 -0.052935608 -0.050476156 -0.048944224][-0.045713574 -0.045722853 -0.047171615 -0.049080122 -0.050888076 -0.052466538 -0.053582367 -0.053796995 -0.05320356 -0.051875632 -0.050426193 -0.048622839 -0.046931442 -0.045615867 -0.044863094][-0.045963019 -0.045532003 -0.046223618 -0.047029693 -0.047644172 -0.047942266 -0.04790302 -0.047382377 -0.046558056 -0.045606628 -0.044867735 -0.043983046 -0.043252029 -0.042697743 -0.0424591][-0.04562683 -0.044967595 -0.045252647 -0.045469493 -0.045583189 -0.045449425 -0.04501893 -0.044347778 -0.043647874 -0.042979524 -0.042604957 -0.042199239 -0.041977018 -0.041835431 -0.041830495][-0.044963375 -0.044107314 -0.044186562 -0.044257727 -0.044347983 -0.044321887 -0.0440257 -0.04354877 -0.043008003 -0.042479202 -0.042213243 -0.041969489 -0.041935332 -0.042062674 -0.042261217][-0.04468796 -0.043648627 -0.043601237 -0.043639213 -0.043843038 -0.044194438 -0.04428957 -0.044119518 -0.043750204 -0.043238439 -0.042824574 -0.042514 -0.042465515 -0.042633772 -0.043072067][-0.044415534 -0.043361761 -0.043293256 -0.043504778 -0.043990768 -0.04462361 -0.045059584 -0.045130245 -0.044839978 -0.04429289 -0.043721136 -0.043282565 -0.043139569 -0.043219954 -0.043736853][-0.044075772 -0.043081716 -0.043241572 -0.043786198 -0.044518128 -0.045423515 -0.046155509 -0.046331421 -0.045978054 -0.045339637 -0.04472322 -0.044226225 -0.043997277 -0.043965194 -0.044539131][-0.043960683 -0.043267649 -0.04364749 -0.044438042 -0.045360092 -0.046445444 -0.047311381 -0.047549926 -0.047241509 -0.0465921 -0.045950528 -0.045396943 -0.045122132 -0.045041032 -0.045637522]]...]
INFO - root - 2017-12-06 08:16:04.943107: step 37510, loss = 0.77, batch loss = 0.56 (9.3 examples/sec; 0.864 sec/batch; 70h:45m:55s remains)
INFO - root - 2017-12-06 08:16:13.409604: step 37520, loss = 0.80, batch loss = 0.58 (9.7 examples/sec; 0.822 sec/batch; 67h:23m:38s remains)
INFO - root - 2017-12-06 08:16:21.764542: step 37530, loss = 0.84, batch loss = 0.63 (9.5 examples/sec; 0.842 sec/batch; 69h:00m:46s remains)
INFO - root - 2017-12-06 08:16:30.199359: step 37540, loss = 0.85, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 68h:36m:40s remains)
INFO - root - 2017-12-06 08:16:38.491938: step 37550, loss = 0.80, batch loss = 0.59 (9.5 examples/sec; 0.846 sec/batch; 69h:19m:04s remains)
INFO - root - 2017-12-06 08:16:46.886883: step 37560, loss = 0.82, batch loss = 0.60 (9.3 examples/sec; 0.857 sec/batch; 70h:12m:06s remains)
INFO - root - 2017-12-06 08:16:55.243695: step 37570, loss = 0.79, batch loss = 0.57 (9.9 examples/sec; 0.812 sec/batch; 66h:32m:10s remains)
INFO - root - 2017-12-06 08:17:03.569182: step 37580, loss = 0.76, batch loss = 0.54 (10.2 examples/sec; 0.785 sec/batch; 64h:18m:26s remains)
INFO - root - 2017-12-06 08:17:12.042597: step 37590, loss = 0.75, batch loss = 0.53 (9.4 examples/sec; 0.848 sec/batch; 69h:28m:50s remains)
INFO - root - 2017-12-06 08:17:20.418161: step 37600, loss = 0.95, batch loss = 0.73 (9.3 examples/sec; 0.861 sec/batch; 70h:29m:43s remains)
2017-12-06 08:17:21.174692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.77391779 -0.8632611 -1.0689261 -1.4926524 -1.9734476 -2.4482965 -2.7279735 -3.1690257 -3.4320481 -3.2960241 -3.1738706 -2.9419978 -2.8238008 -2.8294315 -2.7444041][-1.2110554 -1.4586278 -1.770121 -2.1488383 -2.408566 -2.6766419 -2.875546 -3.0406883 -3.1466787 -3.5503294 -3.9268403 -3.9888661 -3.9690683 -3.6671195 -3.236542][-1.0546013 -0.93094707 -0.76449084 -0.88735116 -1.3262603 -1.7834314 -2.0798829 -2.5386 -3.0365531 -3.3980091 -3.589113 -3.94209 -4.3619924 -4.5145764 -4.4486051][-0.86981 -0.32142112 0.17871794 0.80578804 1.1652035 1.2242951 0.71155858 0.0625294 -0.5568772 -1.5006597 -2.3213897 -2.5229328 -2.6130993 -3.0494361 -3.3564959][-1.3601663 -0.78239167 0.32523713 1.3609121 2.357559 3.3286862 3.7744098 3.8135395 3.0633368 2.1232665 1.2126019 0.34125167 -0.45295331 -0.8010844 -1.2242029][-1.5510933 -1.2496641 -0.28915328 0.757074 1.9823579 3.506362 4.6760807 5.5217428 5.7821283 5.5771995 5.0847573 4.3396616 3.3715932 2.3639305 1.2269448][-1.0707949 -0.82289743 -0.35421774 0.39692181 1.4727653 2.5385606 3.5533185 4.62714 5.3585396 5.9836235 6.3730364 6.3965483 6.0035691 5.1042786 4.1352615][-0.55034471 -0.64351881 -0.58539671 -0.19875106 0.25223333 1.1061853 2.0988467 3.1556146 4.0565677 4.9356155 5.6968665 6.2289686 6.5821366 6.410058 6.0220728][-0.84942257 -1.0673727 -1.3920532 -1.7545103 -1.8666091 -1.3737696 -0.61840129 0.41148219 1.3701502 2.3306382 3.341392 4.2149377 5.0152016 5.5146546 5.9676728][0.421338 0.1468278 -0.85310018 -1.5998138 -2.0853477 -2.3329759 -2.6044915 -2.7290106 -2.5956805 -2.0766761 -1.1494253 0.19823918 1.5757136 2.5296006 3.1528668][-0.1185541 -0.39006126 -0.46019152 -0.88266432 -1.6000249 -1.9463091 -2.3514016 -3.0904994 -4.0269027 -4.3763466 -4.0570011 -3.4392388 -2.3458865 -1.1296338 -0.24248356][-0.16404171 -0.90892804 -1.5584289 -1.9340868 -1.9111991 -2.4171131 -3.2144587 -3.8766539 -4.1247678 -4.5946922 -4.8927131 -4.9772863 -4.6016631 -3.8893909 -3.0652082][-0.35564083 -1.2942028 -1.8816706 -2.4172082 -3.1200461 -3.4366503 -3.8206682 -4.7305961 -5.6293163 -6.4782081 -6.7512875 -6.4730997 -5.9171367 -5.3129144 -4.2889853][-0.73606455 -1.6348659 -2.3148069 -2.8838718 -3.2928689 -3.7720892 -3.9657516 -4.255197 -4.6888747 -5.6420527 -6.3984456 -6.9127522 -6.884 -6.3129344 -5.4696021][-1.9979978 -2.2711701 -2.5079648 -2.6059113 -3.0093465 -3.5238016 -4.07329 -4.186861 -4.1852994 -4.4575305 -4.6498957 -4.7564139 -4.7706409 -4.3841872 -4.2284074]]...]
INFO - root - 2017-12-06 08:17:29.545302: step 37610, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 70h:58m:25s remains)
INFO - root - 2017-12-06 08:17:37.894384: step 37620, loss = 0.85, batch loss = 0.63 (9.8 examples/sec; 0.813 sec/batch; 66h:34m:34s remains)
INFO - root - 2017-12-06 08:17:46.299451: step 37630, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:14m:46s remains)
INFO - root - 2017-12-06 08:17:54.544483: step 37640, loss = 0.82, batch loss = 0.60 (10.1 examples/sec; 0.794 sec/batch; 65h:03m:59s remains)
INFO - root - 2017-12-06 08:18:02.851943: step 37650, loss = 0.94, batch loss = 0.73 (9.6 examples/sec; 0.835 sec/batch; 68h:24m:52s remains)
INFO - root - 2017-12-06 08:18:11.126675: step 37660, loss = 0.80, batch loss = 0.59 (9.6 examples/sec; 0.834 sec/batch; 68h:18m:45s remains)
INFO - root - 2017-12-06 08:18:19.490960: step 37670, loss = 0.79, batch loss = 0.58 (9.4 examples/sec; 0.848 sec/batch; 69h:26m:40s remains)
INFO - root - 2017-12-06 08:18:27.890893: step 37680, loss = 0.82, batch loss = 0.61 (9.3 examples/sec; 0.863 sec/batch; 70h:40m:28s remains)
INFO - root - 2017-12-06 08:18:36.071703: step 37690, loss = 0.81, batch loss = 0.60 (9.8 examples/sec; 0.812 sec/batch; 66h:31m:24s remains)
INFO - root - 2017-12-06 08:18:44.476072: step 37700, loss = 0.77, batch loss = 0.56 (9.4 examples/sec; 0.853 sec/batch; 69h:49m:23s remains)
2017-12-06 08:18:45.212533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.073668979 -0.11014864 -0.13189088 -0.11877345 -0.10847085 -0.10245781 -0.088467106 -0.06423153 -0.058222014 -0.069214262 -0.092937261 -0.0995566 -0.079550192 -0.043221474 -0.00010950118][-0.0023794696 -0.01801534 -0.013389833 0.020628668 0.048472017 0.049670115 0.038292661 0.02605845 0.013117611 -0.0031247139 -0.020220071 -0.036226183 -0.049225636 -0.050319023 -0.027987286][-0.07690201 -0.087023392 -0.061645925 -0.016832657 0.0059861168 -0.0029168203 -0.034981407 -0.064518884 -0.08872471 -0.10777751 -0.11994041 -0.11640682 -0.10875023 -0.10693893 -0.093690716][-0.10275138 -0.1265458 -0.11720391 -0.090648018 -0.069403782 -0.069868036 -0.10123728 -0.14781103 -0.18977614 -0.21460804 -0.23040111 -0.23226118 -0.2213601 -0.21248075 -0.20573267][-0.17734924 -0.22145012 -0.25306678 -0.26686555 -0.26225695 -0.23396981 -0.22014114 -0.22943456 -0.24009687 -0.24084541 -0.24584728 -0.2491885 -0.24433486 -0.23735744 -0.23527105][-0.11697786 -0.16863185 -0.21083568 -0.22334069 -0.21651369 -0.209097 -0.20649755 -0.19862482 -0.19362794 -0.1799317 -0.16352949 -0.14757006 -0.13160686 -0.12221463 -0.11297983][-0.038713858 -0.077872626 -0.11591037 -0.14600667 -0.14979781 -0.13466598 -0.10872365 -0.08098641 -0.062594764 -0.058108643 -0.061939776 -0.062116362 -0.057702556 -0.046930373 -0.032274708][0.077544913 0.033315212 -0.016276516 -0.05937979 -0.060739934 -0.015534379 0.029161371 0.047032729 0.055502728 0.062408373 0.058871 0.031492278 0.0029980168 -0.016280837 -0.0239003][0.045316771 -0.0048589706 -0.06193411 -0.10000475 -0.096882619 -0.052020617 0.0011242479 0.040944025 0.057886764 0.042723775 0.022081569 -0.0066210628 -0.042960506 -0.081460543 -0.11807573][-0.071324907 -0.12669551 -0.19399725 -0.25003791 -0.26605541 -0.23089297 -0.16790362 -0.10087433 -0.06864243 -0.074823648 -0.083573163 -0.089029461 -0.09647318 -0.12935698 -0.17298859][-0.034653697 -0.058624715 -0.10671732 -0.16914591 -0.20441204 -0.19098803 -0.15097398 -0.10544379 -0.083232157 -0.084006 -0.092220336 -0.095675692 -0.10238791 -0.10912273 -0.11918134][0.024792768 0.019020259 -0.020237185 -0.070903786 -0.10564752 -0.1026931 -0.067217156 -0.034875475 -0.030780725 -0.053221967 -0.08373484 -0.10825849 -0.1110546 -0.091248088 -0.074152678][-0.043529011 -0.044970959 -0.057532083 -0.082351163 -0.099203318 -0.10213403 -0.086677074 -0.060604624 -0.034149792 -0.02448006 -0.036110569 -0.063495293 -0.085282162 -0.085421808 -0.072481677][-0.13644873 -0.14918303 -0.15657422 -0.1667726 -0.18277115 -0.19320926 -0.20993334 -0.21032438 -0.20563212 -0.20004386 -0.1954847 -0.18504162 -0.180033 -0.19194631 -0.17998207][-0.16368945 -0.17720139 -0.17723846 -0.18561721 -0.20034237 -0.21538043 -0.24671696 -0.26875669 -0.27161974 -0.25003237 -0.21949871 -0.19666144 -0.18625057 -0.19717637 -0.194639]]...]
INFO - root - 2017-12-06 08:18:53.662171: step 37710, loss = 0.86, batch loss = 0.65 (9.3 examples/sec; 0.859 sec/batch; 70h:18m:38s remains)
INFO - root - 2017-12-06 08:19:02.033490: step 37720, loss = 0.81, batch loss = 0.59 (9.5 examples/sec; 0.842 sec/batch; 68h:57m:35s remains)
INFO - root - 2017-12-06 08:19:10.398114: step 37730, loss = 0.83, batch loss = 0.62 (9.4 examples/sec; 0.849 sec/batch; 69h:28m:57s remains)
INFO - root - 2017-12-06 08:19:18.817180: step 37740, loss = 0.81, batch loss = 0.60 (9.6 examples/sec; 0.837 sec/batch; 68h:34m:01s remains)
INFO - root - 2017-12-06 08:19:27.141406: step 37750, loss = 0.75, batch loss = 0.53 (9.2 examples/sec; 0.868 sec/batch; 71h:06m:19s remains)
INFO - root - 2017-12-06 08:19:35.528770: step 37760, loss = 0.77, batch loss = 0.56 (9.6 examples/sec; 0.830 sec/batch; 67h:56m:26s remains)
INFO - root - 2017-12-06 08:19:43.912173: step 37770, loss = 0.74, batch loss = 0.53 (9.2 examples/sec; 0.872 sec/batch; 71h:21m:14s remains)
INFO - root - 2017-12-06 08:19:52.107750: step 37780, loss = 0.83, batch loss = 0.62 (9.5 examples/sec; 0.846 sec/batch; 69h:15m:38s remains)
INFO - root - 2017-12-06 08:20:00.516260: step 37790, loss = 0.80, batch loss = 0.58 (10.0 examples/sec; 0.803 sec/batch; 65h:46m:10s remains)
INFO - root - 2017-12-06 08:20:08.794149: step 37800, loss = 0.78, batch loss = 0.57 (9.7 examples/sec; 0.826 sec/batch; 67h:37m:25s remains)
2017-12-06 08:20:09.547524: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.080129974 -0.0028015524 -0.12648159 -0.20852897 -0.16995859 -0.19531135 -0.27094209 -0.30344993 -0.30922872 -0.2614373 -0.19991361 -0.13259713 -0.10405681 -0.10554358 -0.10861314][0.018345125 -0.26526207 -0.51770175 -0.55854821 -0.53752053 -0.51639509 -0.53121519 -0.5362094 -0.54789144 -0.49546173 -0.47370687 -0.40286455 -0.32851177 -0.30630428 -0.31086579][-0.07173837 -0.39764655 -0.59208965 -0.75280893 -0.80972606 -0.75368989 -0.73371363 -0.72759187 -0.75588739 -0.74368745 -0.71252877 -0.68462592 -0.6440832 -0.56264812 -0.49477464][-0.42554078 -0.71135944 -0.84526217 -0.96689308 -1.0274088 -0.96811116 -0.9352181 -0.9080162 -0.95325 -0.95732427 -0.92287713 -0.88082892 -0.81809515 -0.73125976 -0.65690738][-0.3044914 -0.58886075 -0.81474388 -0.85027683 -0.8947773 -0.88554895 -0.96247494 -1.0925795 -1.2393682 -1.281666 -1.220351 -1.1032535 -1.0185589 -0.92084503 -0.79483461][-0.25059208 -0.39789033 -0.56297636 -0.55113971 -0.40920046 -0.26287466 -0.29611358 -0.50498897 -0.7726633 -0.891235 -0.94584221 -0.95547688 -0.90830851 -0.79619062 -0.69153696][0.043082707 -0.19085833 -0.22950557 -0.011664309 0.27048305 0.44859651 0.41055694 0.23524639 0.036492534 -0.21938601 -0.424361 -0.51194626 -0.48292422 -0.4290902 -0.38271302][0.32012191 0.090210147 0.087764077 0.32519886 0.62047184 0.87631762 0.95634907 0.83001256 0.58216071 0.3519364 0.19629866 0.13058865 0.14027116 0.10143077 0.046748914][0.20732182 0.10776978 0.15234482 0.40962151 0.76190341 1.0803449 1.1579136 1.0660498 0.87096816 0.71555847 0.57908458 0.5328002 0.53611404 0.49853131 0.427016][-0.03051943 -0.11427294 -0.087468907 0.11986219 0.41503158 0.71652 0.90971333 0.9444235 0.845604 0.78629631 0.715786 0.71032935 0.75904787 0.77718127 0.77647161][-0.29372206 -0.2499429 -0.32280058 -0.26804787 -0.087438136 0.12967324 0.32530147 0.40143523 0.47335956 0.51132786 0.515099 0.58108395 0.65712178 0.71487343 0.76874095][-0.62340868 -0.49817047 -0.43656301 -0.37894726 -0.2358996 -0.099655859 -0.003468737 0.0679596 0.13571686 0.23687616 0.35689649 0.44149485 0.49884573 0.59208894 0.6835503][-0.69980234 -0.6097635 -0.52508396 -0.44158605 -0.36472538 -0.30992207 -0.17595217 -0.040195785 0.032325372 0.10561188 0.19671777 0.25984991 0.39025575 0.51378131 0.56386691][-0.53279877 -0.55609554 -0.56246394 -0.58780479 -0.56333876 -0.52741069 -0.48235652 -0.39520755 -0.30360198 -0.25108376 -0.19202483 -0.0763084 0.031161711 0.17636707 0.33558646][-0.46520823 -0.535663 -0.60395753 -0.63859457 -0.67628741 -0.66986334 -0.62666231 -0.56906843 -0.5618757 -0.58433628 -0.57105041 -0.51476407 -0.44683573 -0.31019774 -0.1222347]]...]
INFO - root - 2017-12-06 08:20:17.931457: step 37810, loss = 0.86, batch loss = 0.65 (9.4 examples/sec; 0.853 sec/batch; 69h:48m:24s remains)
INFO - root - 2017-12-06 08:20:26.279674: step 37820, loss = 0.78, batch loss = 0.57 (9.5 examples/sec; 0.845 sec/batch; 69h:09m:00s remains)
INFO - root - 2017-12-06 08:20:34.720690: step 37830, loss = 0.80, batch loss = 0.59 (9.2 examples/sec; 0.869 sec/batch; 71h:07m:53s remains)
INFO - root - 2017-12-06 08:20:42.998098: step 37840, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.840 sec/batch; 68h:46m:49s remains)
INFO - root - 2017-12-06 08:20:51.374981: step 37850, loss = 0.76, batch loss = 0.54 (9.2 examples/sec; 0.865 sec/batch; 70h:49m:38s remains)
INFO - root - 2017-12-06 08:20:59.729701: step 37860, loss = 0.85, batch loss = 0.63 (9.1 examples/sec; 0.878 sec/batch; 71h:49m:27s remains)
INFO - root - 2017-12-06 08:21:08.002777: step 37870, loss = 0.83, batch loss = 0.61 (9.9 examples/sec; 0.805 sec/batch; 65h:53m:26s remains)
INFO - root - 2017-12-06 08:21:16.436706: step 37880, loss = 0.77, batch loss = 0.55 (9.7 examples/sec; 0.824 sec/batch; 67h:28m:24s remains)
INFO - root - 2017-12-06 08:21:24.894601: step 37890, loss = 0.81, batch loss = 0.60 (9.3 examples/sec; 0.862 sec/batch; 70h:31m:21s remains)
INFO - root - 2017-12-06 08:21:33.283464: step 37900, loss = 0.78, batch loss = 0.56 (9.2 examples/sec; 0.867 sec/batch; 70h:55m:13s remains)
2017-12-06 08:21:33.976887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0056018 -1.0603584 -1.0091392 -0.8284499 -0.54167604 -0.26545882 -0.073296383 -0.032704137 -0.0672475 -0.096107319 -0.10497557 -0.11539903 -0.068404868 -0.01844722 0.058757871][-0.80155277 -1.0176225 -0.96836537 -0.80875516 -0.47987577 -0.22411473 -0.081234634 -0.0075468794 -0.036818914 -0.11253369 -0.15626323 -0.1967718 -0.18023378 -0.17918016 -0.13004939][-0.37960479 -0.71560472 -0.7719487 -0.61675107 -0.43530831 -0.17046332 -0.021610923 0.054255113 -0.011406124 -0.14343378 -0.23195137 -0.23247829 -0.21750191 -0.21233389 -0.18379925][-0.03598956 -0.14251594 -0.30693519 -0.23449068 -0.071659319 0.1090347 0.24187151 0.31698045 0.20324135 0.1011478 -0.017170198 -0.036759768 -0.049298003 -0.061312124 -0.093045853][0.35074282 0.33011788 0.20326236 0.20765457 0.22329971 0.27634746 0.30244145 0.42500639 0.39557934 0.29089886 0.16753709 0.11786664 0.075702146 0.056775361 0.03994][0.60008836 0.55561113 0.49607193 0.38519213 0.29805094 0.29818538 0.31517762 0.3572188 0.36566693 0.35582346 0.26599166 0.18687296 0.046388388 0.036470696 0.065535784][0.48206621 0.50209856 0.31784165 0.27494285 0.26923582 0.36411622 0.37729472 0.42066681 0.40025407 0.39547953 0.32114884 0.26487967 0.14010602 0.0675489 0.042951927][0.11678031 0.21531942 -0.0027191043 0.0054496378 0.044002518 0.14359206 0.27563179 0.37462005 0.2943233 0.241258 0.20511314 0.17820871 0.12081257 0.040508538 0.010245405][-0.38531756 -0.24669534 -0.34437749 -0.41930589 -0.33881724 -0.1575748 -0.040841892 0.060954466 0.13749057 0.12898639 0.063141689 0.039293826 0.069257483 -0.0055528656 -0.081530116][-0.92335111 -0.83884686 -0.8243838 -0.88284487 -0.74938929 -0.56700861 -0.32977617 -0.18297806 -0.13663945 -0.1120655 -0.11630155 -0.094272606 -0.062657416 -0.091326959 -0.14402768][-1.3532128 -1.3705834 -1.2791129 -1.1794993 -1.0529513 -0.81296837 -0.59263861 -0.41386586 -0.32816434 -0.27437961 -0.24156849 -0.22122622 -0.19146605 -0.21206224 -0.24986146][-1.3764193 -1.4375373 -1.3235164 -1.1803051 -0.97007644 -0.76364088 -0.52926141 -0.36701325 -0.331123 -0.30818325 -0.31598431 -0.30977261 -0.28906041 -0.26873487 -0.22989446][-1.0893406 -1.154201 -1.0992434 -0.82181072 -0.67654061 -0.5113067 -0.2705124 -0.16820073 -0.11710313 -0.083683528 -0.16160187 -0.17362899 -0.16559854 -0.11981706 -0.0276796][-0.57540238 -0.65532249 -0.63965571 -0.52072251 -0.24773833 -0.087979637 0.023710154 0.1121297 0.12563598 0.22390148 0.16527689 0.11493269 0.1975379 0.19981176 0.24456766][-0.10162897 -0.13068862 -0.19549078 -0.0039767176 0.12685589 0.27971524 0.33321726 0.44611961 0.47712505 0.52703851 0.42851144 0.32939962 0.43741018 0.4218874 0.46092689]]...]
INFO - root - 2017-12-06 08:21:42.365068: step 37910, loss = 0.84, batch loss = 0.62 (10.1 examples/sec; 0.795 sec/batch; 65h:02m:26s remains)
INFO - root - 2017-12-06 08:21:50.849899: step 37920, loss = 0.85, batch loss = 0.64 (9.4 examples/sec; 0.855 sec/batch; 69h:58m:23s remains)
INFO - root - 2017-12-06 08:21:59.162092: step 37930, loss = 0.84, batch loss = 0.63 (9.7 examples/sec; 0.825 sec/batch; 67h:28m:32s remains)
INFO - root - 2017-12-06 08:22:07.485586: step 37940, loss = 0.79, batch loss = 0.58 (9.7 examples/sec; 0.827 sec/batch; 67h:38m:08s remains)
INFO - root - 2017-12-06 08:22:15.684403: step 37950, loss = 0.84, batch loss = 0.62 (9.9 examples/sec; 0.810 sec/batch; 66h:17m:18s remains)
INFO - root - 2017-12-06 08:22:24.035961: step 37960, loss = 0.80, batch loss = 0.59 (9.8 examples/sec; 0.820 sec/batch; 67h:04m:41s remains)
INFO - root - 2017-12-06 08:22:32.438090: step 37970, loss = 0.77, batch loss = 0.56 (9.7 examples/sec; 0.822 sec/batch; 67h:13m:47s remains)
INFO - root - 2017-12-06 08:22:40.577799: step 37980, loss = 0.77, batch loss = 0.56 (10.0 examples/sec; 0.796 sec/batch; 65h:08m:36s remains)
INFO - root - 2017-12-06 08:22:48.867135: step 37990, loss = 0.77, batch loss = 0.56 (9.6 examples/sec; 0.833 sec/batch; 68h:09m:13s remains)
INFO - root - 2017-12-06 08:22:57.091812: step 38000, loss = 0.81, batch loss = 0.59 (9.8 examples/sec; 0.816 sec/batch; 66h:45m:08s remains)
2017-12-06 08:22:57.827412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036360469 -0.037308756 -0.039416675 -0.042777751 -0.047111027 -0.051913984 -0.056926891 -0.061679807 -0.065567262 -0.06822025 -0.069656089 -0.070147641 -0.069570728 -0.067694731 -0.065130547][-0.035279974 -0.036664903 -0.03955188 -0.043752495 -0.048854675 -0.054267876 -0.059499875 -0.063983239 -0.06709642 -0.068694413 -0.068999559 -0.068530194 -0.067296959 -0.06512557 -0.062455796][-0.035191078 -0.036718104 -0.039951961 -0.04447091 -0.049842227 -0.055202372 -0.060071405 -0.063917935 -0.066083834 -0.066517904 -0.065801844 -0.064583637 -0.062924393 -0.060722254 -0.058426555][-0.035829965 -0.037095573 -0.040178668 -0.04438388 -0.049320344 -0.05403265 -0.058071133 -0.06100601 -0.062255602 -0.061784394 -0.060239445 -0.058396183 -0.056464054 -0.054456964 -0.052818283][-0.036472052 -0.037421905 -0.040011834 -0.043501407 -0.047570806 -0.05136364 -0.054485559 -0.056418139 -0.056784883 -0.055676393 -0.053675119 -0.051526282 -0.049589526 -0.048005134 -0.047176704][-0.037601113 -0.038068037 -0.040032044 -0.042660963 -0.045599435 -0.04815115 -0.05010708 -0.051067933 -0.050852906 -0.04955814 -0.047720544 -0.045936268 -0.044451885 -0.04343947 -0.043344364][-0.039146684 -0.039119754 -0.040452279 -0.04221718 -0.044032313 -0.045373429 -0.046285357 -0.0465247 -0.045975566 -0.04479761 -0.043379016 -0.042168885 -0.041286793 -0.040843494 -0.041418284][-0.040940605 -0.040433992 -0.041162577 -0.04205852 -0.042841692 -0.043200266 -0.043308854 -0.043077789 -0.042433795 -0.041452777 -0.040387806 -0.039577592 -0.039138846 -0.039185509 -0.040223654][-0.042253125 -0.041497804 -0.041687913 -0.041803971 -0.0417119 -0.041345078 -0.04088163 -0.040243827 -0.039483048 -0.038695503 -0.037966777 -0.037466425 -0.037390947 -0.037865572 -0.039269935][-0.042886324 -0.041935764 -0.04171817 -0.04134351 -0.040757965 -0.039975807 -0.039165732 -0.03826296 -0.037416972 -0.036736295 -0.036188461 -0.035883773 -0.035995919 -0.036706753 -0.038330309][-0.04312126 -0.042090356 -0.041651513 -0.041067291 -0.0402587 -0.039215367 -0.038142961 -0.037103165 -0.036219437 -0.035584 -0.035151768 -0.034972351 -0.035143167 -0.035871424 -0.037629161][-0.04333533 -0.042326409 -0.041833472 -0.041168861 -0.040331382 -0.039297789 -0.03829354 -0.037300035 -0.036431588 -0.035793375 -0.035438247 -0.035325803 -0.035507657 -0.0361618 -0.037820164][-0.043831158 -0.042864293 -0.042444047 -0.041858472 -0.041151233 -0.040340394 -0.039512947 -0.038630035 -0.037868354 -0.037326775 -0.0370497 -0.036974542 -0.037181396 -0.037795756 -0.039297312][-0.044807054 -0.043861046 -0.043580223 -0.043187067 -0.042730268 -0.04225282 -0.04168161 -0.041039392 -0.040525943 -0.040200088 -0.040071268 -0.040131487 -0.040437 -0.040996343 -0.042293884][-0.046504106 -0.045573842 -0.045371082 -0.045186915 -0.044999387 -0.044810098 -0.044545077 -0.044253998 -0.044043913 -0.043914929 -0.04392571 -0.044107351 -0.044453535 -0.044942372 -0.046090849]]...]
INFO - root - 2017-12-06 08:23:06.252750: step 38010, loss = 0.80, batch loss = 0.59 (9.9 examples/sec; 0.811 sec/batch; 66h:18m:23s remains)
INFO - root - 2017-12-06 08:23:14.437499: step 38020, loss = 0.81, batch loss = 0.60 (9.7 examples/sec; 0.825 sec/batch; 67h:29m:47s remains)
INFO - root - 2017-12-06 08:23:22.856113: step 38030, loss = 0.80, batch loss = 0.59 (9.3 examples/sec; 0.861 sec/batch; 70h:26m:15s remains)
INFO - root - 2017-12-06 08:23:31.154422: step 38040, loss = 0.80, batch loss = 0.59 (9.4 examples/sec; 0.854 sec/batch; 69h:51m:07s remains)
INFO - root - 2017-12-06 08:23:39.440450: step 38050, loss = 0.85, batch loss = 0.63 (10.0 examples/sec; 0.800 sec/batch; 65h:28m:22s remains)
INFO - root - 2017-12-06 08:23:47.779862: step 38060, loss = 0.79, batch loss = 0.58 (9.7 examples/sec; 0.825 sec/batch; 67h:26m:28s remains)
INFO - root - 2017-12-06 08:23:56.080710: step 38070, loss = 0.84, batch loss = 0.62 (10.2 examples/sec; 0.783 sec/batch; 64h:02m:17s remains)
INFO - root - 2017-12-06 08:24:04.352303: step 38080, loss = 0.85, batch loss = 0.63 (9.6 examples/sec; 0.834 sec/batch; 68h:11m:44s remains)
INFO - root - 2017-12-06 08:24:12.539921: step 38090, loss = 0.83, batch loss = 0.62 (10.0 examples/sec; 0.797 sec/batch; 65h:09m:52s remains)
INFO - root - 2017-12-06 08:24:20.912468: step 38100, loss = 0.85, batch loss = 0.64 (9.7 examples/sec; 0.821 sec/batch; 67h:09m:00s remains)
2017-12-06 08:24:21.627720: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.66795325 -1.039027 -1.3419571 -1.6011801 -1.7425398 -1.780589 -1.7160773 -1.5502809 -1.3278449 -1.1244454 -0.90604061 -0.7069335 -0.54338282 -0.29395163 -0.031879667][-0.93806994 -1.3901433 -1.716987 -2.0032096 -2.1793165 -2.32947 -2.334995 -2.217597 -2.0522971 -1.7737441 -1.3681583 -1.0737717 -0.72977626 -0.46732515 -0.20511556][-1.3532699 -1.5459719 -1.7472812 -1.996576 -2.1693096 -2.2997923 -2.4510558 -2.4296529 -2.1918206 -1.9851222 -1.73559 -1.3488547 -0.91718626 -0.57535589 -0.26311415][-1.4450705 -1.4127411 -1.3859555 -1.3634665 -1.4243095 -1.5081567 -1.6462585 -1.70789 -1.7389358 -1.5938941 -1.4232733 -1.2040795 -0.92832416 -0.62437904 -0.32275859][-0.91507262 -0.7415747 -0.59170157 -0.35421535 -0.12916401 -0.17647037 -0.3513476 -0.55181491 -0.71639943 -0.7700569 -0.82073343 -0.75220984 -0.64780128 -0.52456141 -0.36051071][-0.26670974 0.054729432 0.4309485 0.73522866 0.94114518 0.94032395 0.84413469 0.56591487 0.28412533 0.0051453188 -0.21676256 -0.36689615 -0.40848997 -0.45649981 -0.43739086][0.49451941 0.80504388 1.1946912 1.5291588 1.5776416 1.473327 1.2473505 0.98648608 0.66241056 0.23370159 -0.08265277 -0.26325926 -0.40176392 -0.43885392 -0.44537082][0.64915663 0.921703 1.2858243 1.5480652 1.5941501 1.3481189 0.93893123 0.58318681 0.12813087 -0.20683196 -0.43266857 -0.59679484 -0.612022 -0.57637286 -0.53369641][0.20396087 0.3733817 0.68237436 0.81995761 0.81098688 0.70187336 0.36304528 -0.13256474 -0.48512647 -0.70040309 -0.81901509 -0.82714623 -0.79737473 -0.71515971 -0.58339775][-0.43056205 -0.41971472 -0.31598991 -0.21828105 -0.16639262 -0.20836779 -0.41635665 -0.66772056 -0.8780092 -0.97203535 -0.92746943 -0.85547423 -0.75496823 -0.66637826 -0.54392076][-0.691163 -0.87153286 -0.98111671 -0.95419967 -0.93596715 -0.93564504 -0.94700676 -0.94913262 -0.94355029 -0.91779727 -0.81834221 -0.69503605 -0.56976032 -0.521524 -0.39959595][-0.78652382 -0.98470879 -1.0862184 -1.1508981 -1.1018568 -0.97976381 -0.92823535 -0.84672141 -0.74935061 -0.63994175 -0.51138788 -0.40468609 -0.35023245 -0.33176029 -0.28385466][-0.56987369 -0.8448875 -0.99613094 -1.0364851 -0.99272656 -0.89637882 -0.71183282 -0.59471285 -0.53592151 -0.42404705 -0.33829275 -0.23256196 -0.15953745 -0.10196634 -0.047365647][-0.31756026 -0.54375434 -0.691888 -0.74108779 -0.68325084 -0.62506956 -0.54083675 -0.41743097 -0.3369354 -0.32201529 -0.27680767 -0.21090066 -0.15145199 -0.11323963 -0.020718269][-0.13978668 -0.26374722 -0.38238531 -0.39797977 -0.34644374 -0.32874304 -0.2703222 -0.2491699 -0.22529431 -0.23604734 -0.25095522 -0.25570166 -0.17550245 -0.16188785 -0.056875385]]...]
INFO - root - 2017-12-06 08:24:30.138098: step 38110, loss = 0.79, batch loss = 0.57 (8.9 examples/sec; 0.894 sec/batch; 73h:08m:09s remains)
INFO - root - 2017-12-06 08:24:38.556034: step 38120, loss = 0.78, batch loss = 0.56 (9.8 examples/sec; 0.818 sec/batch; 66h:51m:02s remains)
INFO - root - 2017-12-06 08:24:46.901963: step 38130, loss = 0.80, batch loss = 0.59 (9.5 examples/sec; 0.842 sec/batch; 68h:49m:37s remains)
INFO - root - 2017-12-06 08:24:55.199651: step 38140, loss = 0.84, batch loss = 0.63 (9.5 examples/sec; 0.843 sec/batch; 68h:56m:36s remains)
INFO - root - 2017-12-06 08:25:03.553873: step 38150, loss = 0.79, batch loss = 0.58 (9.0 examples/sec; 0.886 sec/batch; 72h:24m:20s remains)
INFO - root - 2017-12-06 08:25:11.694678: step 38160, loss = 0.87, batch loss = 0.65 (9.8 examples/sec; 0.814 sec/batch; 66h:33m:10s remains)
INFO - root - 2017-12-06 08:25:20.037346: step 38170, loss = 0.86, batch loss = 0.64 (9.6 examples/sec; 0.837 sec/batch; 68h:26m:33s remains)
INFO - root - 2017-12-06 08:25:28.778897: step 38180, loss = 0.79, batch loss = 0.58 (9.5 examples/sec; 0.843 sec/batch; 68h:57m:04s remains)
INFO - root - 2017-12-06 08:25:37.072391: step 38190, loss = 0.95, batch loss = 0.74 (9.9 examples/sec; 0.807 sec/batch; 65h:57m:47s remains)
INFO - root - 2017-12-06 08:25:45.438785: step 38200, loss = 0.87, batch loss = 0.66 (9.9 examples/sec; 0.809 sec/batch; 66h:06m:13s remains)
2017-12-06 08:25:46.123477: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5152131 -1.6272738 -1.3646995 -1.2430151 -1.6015575 -1.7592797 -1.8859171 -1.6413507 -1.277264 -1.238484 -1.0424126 -0.74931955 0.096149638 0.27548975 0.21263675][-0.79893148 -1.1226794 -1.2359271 -1.2277584 -1.3679868 -1.6114233 -1.8323514 -1.6644996 -1.3710035 -0.90058607 -0.5454042 -0.4328233 0.12591547 0.52992904 0.85302979][-0.58387959 -0.51317126 -0.20276913 -0.49819052 -0.719984 -0.80377823 -1.3086481 -1.4181247 -1.5141995 -1.0843177 -0.64519411 -0.40503049 0.037472576 0.57095039 0.82073396][0.40663451 0.127592 0.16561328 0.012894943 0.14536604 -0.38582212 -1.1391349 -1.3700378 -1.7024947 -1.6756755 -1.4777688 -0.74570167 -0.24262775 0.18798916 0.20520587][0.18147646 0.34674 0.84052575 0.639671 0.521913 0.16591103 -0.25694442 -1.2869302 -2.0780888 -1.9539169 -1.8592089 -1.3863965 -1.1638051 -0.52236336 -0.27359939][1.1395844 1.2405764 1.4476389 1.5794402 1.4582654 0.77582043 0.10324641 -0.49406153 -0.84659022 -1.2854061 -1.6644613 -1.5063668 -1.4547412 -1.272954 -1.1833937][1.7818948 1.9770045 2.0381165 1.7468753 1.4114997 1.0223434 0.43016315 -0.1978223 -0.4509232 -0.560546 -0.52233505 -1.076039 -1.4722627 -1.2175107 -1.0197562][2.4213068 2.2829909 1.8906423 1.6717167 1.2314074 0.66515106 0.037684277 -0.17355347 -0.17221278 -0.32634634 -0.15360068 -0.41241437 -0.44348538 -0.72652161 -1.1676247][1.8652359 1.7730528 1.5744437 1.3268164 0.74236107 0.65532857 0.44480449 0.14203893 0.087826058 0.16090505 0.1657678 0.078339368 0.26271105 0.22000863 0.21075191][0.045555249 0.21860383 0.15956382 0.0936652 0.13331893 -0.13547356 -0.2217641 -0.2192772 -0.32791463 -0.071597129 0.028695762 0.4029355 1.0541219 1.1693674 0.9557687][-0.7667352 -1.0497442 -1.018792 -0.81976378 -0.80981922 -0.69529504 -0.53997612 -0.46944147 -0.74864209 -0.52600467 -0.28526872 -0.012553617 0.47896004 0.75999993 0.902038][-1.309799 -2.0232477 -2.3172672 -2.2475221 -1.7548302 -1.7410328 -1.7165163 -1.4379692 -1.5223842 -1.4526879 -1.260216 -0.95712012 -0.54418194 -0.17023529 0.09468244][-1.9771264 -2.1780858 -2.0581176 -2.0644212 -1.99947 -1.838055 -1.6591506 -1.7420512 -1.7797475 -2.0029154 -1.7015036 -1.0807836 -0.37075758 0.070680827 0.40307921][-1.5452684 -1.3376023 -1.0854559 -0.89594597 -0.993292 -0.98255908 -1.0408779 -1.2519586 -1.0508494 -0.862878 -0.30659172 -0.16678965 0.01059293 0.28296816 0.49018025][-0.42671871 -0.46409577 -0.51903081 -0.203465 -0.53197366 -0.737903 -0.69816417 -0.8767134 -0.68834293 -0.32990882 0.24610506 0.61949921 0.73847723 0.53887391 0.58406734]]...]
INFO - root - 2017-12-06 08:25:54.476840: step 38210, loss = 0.89, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 70h:54m:36s remains)
INFO - root - 2017-12-06 08:26:02.943657: step 38220, loss = 0.82, batch loss = 0.61 (9.6 examples/sec; 0.829 sec/batch; 67h:46m:17s remains)
INFO - root - 2017-12-06 08:26:11.328754: step 38230, loss = 0.82, batch loss = 0.61 (9.7 examples/sec; 0.828 sec/batch; 67h:41m:19s remains)
INFO - root - 2017-12-06 08:26:19.627573: step 38240, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.843 sec/batch; 68h:54m:48s remains)
INFO - root - 2017-12-06 08:26:27.851000: step 38250, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.838 sec/batch; 68h:29m:51s remains)
INFO - root - 2017-12-06 08:26:36.148316: step 38260, loss = 0.71, batch loss = 0.50 (9.2 examples/sec; 0.866 sec/batch; 70h:45m:25s remains)
INFO - root - 2017-12-06 08:26:44.459283: step 38270, loss = 0.81, batch loss = 0.60 (9.8 examples/sec; 0.816 sec/batch; 66h:41m:44s remains)
INFO - root - 2017-12-06 08:26:52.843983: step 38280, loss = 0.77, batch loss = 0.56 (9.6 examples/sec; 0.832 sec/batch; 67h:59m:17s remains)
INFO - root - 2017-12-06 08:27:01.169482: step 38290, loss = 0.77, batch loss = 0.56 (9.5 examples/sec; 0.843 sec/batch; 68h:55m:17s remains)
INFO - root - 2017-12-06 08:27:09.601425: step 38300, loss = 0.84, batch loss = 0.63 (9.2 examples/sec; 0.873 sec/batch; 71h:18m:18s remains)
2017-12-06 08:27:10.311671: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29067174 0.69317615 1.1103572 1.1278212 0.89862168 0.44012055 0.044424608 -0.14559379 -0.083227314 0.0405948 0.0045707896 0.0035876483 -0.091199346 -0.084490173 -0.07053145][1.3927619 1.5591818 1.7511 2.1474152 1.9685056 1.1171423 0.53078592 0.051966175 -0.12946135 -0.1068657 -0.043200959 0.054857671 0.087454811 0.070967 0.011302374][2.0784121 2.3360095 2.7950761 3.3351507 3.2780809 2.7128859 1.4540867 0.56294286 0.10901614 -0.092203707 -0.1226088 -0.13189584 -0.066946991 0.069561705 0.12682523][1.763271 2.3562362 3.1316464 3.797368 4.0980949 3.8418279 2.7257669 1.6064204 0.79169977 0.277916 -0.051116608 -0.18820463 -0.14179948 -0.10548392 -0.017743804][1.4764988 1.9588029 2.6707306 3.5032821 3.9435058 3.6702724 2.9048817 2.1771419 1.4606693 0.86539018 0.3882632 0.052234963 -0.10136192 -0.14807291 -0.13573886][0.41638258 1.0846763 1.9307203 2.9653685 3.5731468 3.3195863 2.6706562 2.0829349 1.6404892 1.236109 0.73149574 0.35948968 0.11564688 -0.010441311 -0.097497351][-1.1772597 -0.51412559 0.71980679 1.9995413 2.7394781 3.0335574 2.6645505 2.0016034 1.3931053 0.91450131 0.7245599 0.47062561 0.21761507 0.10502298 0.0025431812][-1.7023898 -1.3483417 -1.1430259 0.1446481 1.6786087 2.4221478 2.1815174 1.9100456 1.2605361 0.65539289 0.27567223 0.12529948 0.18630216 0.1096258 0.024312839][-1.2791572 -0.94453442 -1.4737196 -0.75179577 0.1335531 1.0355269 1.4248048 1.4491491 1.071403 0.62461507 0.091959491 -0.19140741 -0.30282074 -0.0936005 -0.0024189726][-1.3888386 -0.60601425 -0.501413 -0.45083019 -0.43749604 0.075599104 0.507751 0.73152125 0.63773715 0.26845402 -0.21769615 -0.41653723 -0.71086168 -0.60295522 -0.3230598][-1.478915 -0.99255633 -0.39952216 0.31212619 0.14912817 0.015927039 -0.13170837 0.0097937062 0.060014293 -0.16909906 -0.6374805 -1.0877458 -1.1033199 -1.1719159 -0.90352631][-1.1311713 -0.48413029 -0.44672143 0.11075053 0.69295967 0.57521057 -0.025088593 -0.34100622 -0.42684117 -0.42634743 -0.69214237 -1.2738956 -1.4636062 -1.4720638 -1.1952223][-0.14496469 0.11246723 0.084954873 0.29711249 0.56054437 0.86623347 0.68432438 0.032625668 -0.47649184 -0.56547916 -0.57072926 -0.71640837 -0.99454367 -0.98658013 -1.0279502][-0.35864124 -0.36834258 0.21476614 0.43051973 0.50679159 0.8312583 0.82152593 0.48143485 -0.035189729 -0.417968 -0.43691853 -0.42918903 -0.36630309 -0.39740452 -0.57333869][-0.21108609 -0.10528317 -0.49892992 -0.28133565 0.382827 0.44907269 0.558007 0.53577232 0.55680645 0.22113225 -0.1785524 -0.35360995 -0.24654777 -0.19962949 -0.207699]]...]
INFO - root - 2017-12-06 08:27:18.762446: step 38310, loss = 0.83, batch loss = 0.62 (9.8 examples/sec; 0.817 sec/batch; 66h:46m:28s remains)
INFO - root - 2017-12-06 08:27:26.888376: step 38320, loss = 0.80, batch loss = 0.59 (9.8 examples/sec; 0.813 sec/batch; 66h:28m:03s remains)
INFO - root - 2017-12-06 08:27:35.262457: step 38330, loss = 0.78, batch loss = 0.57 (9.6 examples/sec; 0.835 sec/batch; 68h:12m:18s remains)
INFO - root - 2017-12-06 08:27:43.596050: step 38340, loss = 0.90, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:16m:25s remains)
INFO - root - 2017-12-06 08:27:51.885880: step 38350, loss = 0.77, batch loss = 0.56 (9.7 examples/sec; 0.822 sec/batch; 67h:08m:35s remains)
INFO - root - 2017-12-06 08:28:00.339724: step 38360, loss = 0.82, batch loss = 0.61 (9.3 examples/sec; 0.856 sec/batch; 69h:54m:32s remains)
INFO - root - 2017-12-06 08:28:08.565692: step 38370, loss = 0.88, batch loss = 0.67 (9.6 examples/sec; 0.834 sec/batch; 68h:07m:40s remains)
INFO - root - 2017-12-06 08:28:16.959227: step 38380, loss = 0.85, batch loss = 0.63 (9.5 examples/sec; 0.840 sec/batch; 68h:39m:59s remains)
INFO - root - 2017-12-06 08:28:25.372186: step 38390, loss = 0.75, batch loss = 0.54 (9.4 examples/sec; 0.854 sec/batch; 69h:47m:24s remains)
INFO - root - 2017-12-06 08:28:33.679979: step 38400, loss = 0.77, batch loss = 0.56 (9.3 examples/sec; 0.858 sec/batch; 70h:05m:19s remains)
2017-12-06 08:28:34.375989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.077417277 -0.070985332 -0.063741989 -0.051788665 -0.044053461 -0.058573343 -0.11482211 -0.18242651 -0.20252295 -0.11406302 0.096897826 0.36785847 0.65569943 0.86184961 0.96031892][-0.070194557 -0.0640214 -0.047739509 -0.0082083642 0.035316944 0.06244576 0.026296243 -0.07238026 -0.19772306 -0.27478918 -0.22664256 -0.049776938 0.26071072 0.59818685 0.90906239][-0.057199929 -0.043095466 -0.020534158 0.01823733 0.059157535 0.10969113 0.10493396 0.021171458 -0.12611903 -0.27166867 -0.37909824 -0.42515343 -0.28858781 0.025178902 0.47671878][-0.055412058 -0.045424722 -0.03464878 -0.017539904 -0.01727239 -0.017006025 -0.046713509 -0.12646005 -0.24541725 -0.35165405 -0.40296632 -0.432289 -0.39716345 -0.27436391 0.017597131][-0.082422584 -0.091451734 -0.11098386 -0.13493717 -0.16612238 -0.20800537 -0.28810608 -0.4000079 -0.53330892 -0.59859824 -0.58944106 -0.51899976 -0.39765072 -0.27983069 -0.14662881][-0.092721738 -0.11639363 -0.13936491 -0.15744229 -0.17030853 -0.17732459 -0.23573034 -0.38347208 -0.58604074 -0.7173965 -0.72746092 -0.62915683 -0.4686088 -0.31138623 -0.13866505][-0.096461 -0.11896013 -0.12125857 -0.0800851 -0.0090517849 0.1166501 0.19841234 0.12822154 -0.09526173 -0.35605049 -0.51576459 -0.53000826 -0.452667 -0.33686632 -0.17700058][-0.11700012 -0.13128434 -0.095752761 -0.00029353797 0.12316778 0.26776475 0.35631651 0.36717421 0.24888568 0.016880028 -0.19342804 -0.29302436 -0.30315924 -0.29538769 -0.27358693][-0.10852173 -0.14983663 -0.17877755 -0.13997538 -0.031594388 0.11628456 0.21702896 0.25990427 0.18170024 0.055725396 -0.071782336 -0.17774531 -0.19964114 -0.20345111 -0.24440353][-0.11612205 -0.11878004 -0.1053463 -0.080278538 -0.030277938 0.064636827 0.14781979 0.19483651 0.13090332 0.017585739 -0.0741314 -0.14401019 -0.16483003 -0.15506604 -0.1984565][-0.11094332 -0.10529388 -0.063475855 0.007261619 0.083236322 0.15056013 0.18682732 0.20049386 0.16132475 0.050531521 -0.04596946 -0.097117841 -0.11554635 -0.09846396 -0.14255086][-0.11414151 -0.12563418 -0.10526101 -0.036464207 0.077516049 0.1988536 0.27296281 0.27141231 0.19559707 0.092405573 0.0013925731 -0.073688015 -0.11436613 -0.085460491 -0.082971759][-0.099536493 -0.11646684 -0.11997028 -0.086067349 -0.0030144751 0.094444245 0.15564652 0.14811011 0.073124737 -0.0016595051 -0.042033039 -0.058407426 -0.082876079 -0.092900947 -0.0861955][-0.086363971 -0.10095282 -0.12106085 -0.13149938 -0.10993384 -0.045964394 0.018838763 0.03860946 0.010752849 -0.050330088 -0.088697277 -0.078200608 -0.067359895 -0.069631256 -0.066408046][-0.080328472 -0.082178578 -0.089109324 -0.10030636 -0.10521016 -0.092514224 -0.068150073 -0.052060448 -0.055440553 -0.072453186 -0.0834843 -0.080957882 -0.075419471 -0.05678172 -0.023933485]]...]
INFO - root - 2017-12-06 08:28:42.706110: step 38410, loss = 0.81, batch loss = 0.59 (9.9 examples/sec; 0.805 sec/batch; 65h:43m:38s remains)
INFO - root - 2017-12-06 08:28:50.975130: step 38420, loss = 0.84, batch loss = 0.62 (9.6 examples/sec; 0.836 sec/batch; 68h:16m:56s remains)
INFO - root - 2017-12-06 08:28:59.281519: step 38430, loss = 0.79, batch loss = 0.57 (9.7 examples/sec; 0.822 sec/batch; 67h:07m:57s remains)
INFO - root - 2017-12-06 08:29:05.939105: step 38440, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.643 sec/batch; 52h:31m:27s remains)
INFO - root - 2017-12-06 08:29:12.432652: step 38450, loss = 0.95, batch loss = 0.73 (12.3 examples/sec; 0.652 sec/batch; 53h:16m:24s remains)
INFO - root - 2017-12-06 08:29:19.112259: step 38460, loss = 0.76, batch loss = 0.55 (11.9 examples/sec; 0.671 sec/batch; 54h:49m:27s remains)
INFO - root - 2017-12-06 08:29:25.689062: step 38470, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.653 sec/batch; 53h:18m:22s remains)
INFO - root - 2017-12-06 08:29:32.124772: step 38480, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 52h:49m:11s remains)
INFO - root - 2017-12-06 08:29:38.665356: step 38490, loss = 0.80, batch loss = 0.58 (12.0 examples/sec; 0.668 sec/batch; 54h:34m:44s remains)
INFO - root - 2017-12-06 08:29:45.171615: step 38500, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.647 sec/batch; 52h:48m:16s remains)
2017-12-06 08:29:45.796095: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037511963 -0.035110325 -0.033618324 -0.033398766 -0.034570023 -0.036417719 -0.038152929 -0.039214052 -0.039941039 -0.040591933 -0.041282717 -0.042047244 -0.042889312 -0.043682314 -0.044495359][-0.031204805 -0.026203059 -0.022826061 -0.022050776 -0.024079941 -0.027726591 -0.031228531 -0.033440951 -0.034887243 -0.036168996 -0.037547491 -0.038945403 -0.040389434 -0.041816585 -0.043208417][-0.023086332 -0.014025167 -0.0073468834 -0.0050629526 -0.0077191964 -0.013408214 -0.019259818 -0.023374535 -0.026139714 -0.028594542 -0.03132515 -0.034062538 -0.036723886 -0.03923719 -0.041537765][-0.013917036 4.8927963e-05 0.011407577 0.016622536 0.014542669 0.0074562803 -0.00077550113 -0.0074116811 -0.012284905 -0.016836546 -0.021903768 -0.026981257 -0.031630486 -0.035743702 -0.039524864][-0.0056916103 0.013087817 0.029586345 0.038857773 0.038779408 0.031528533 0.021680281 0.012893371 0.0057451278 -0.0016651228 -0.010054938 -0.018322714 -0.025722362 -0.031938113 -0.037456479][-0.00022534281 0.022045098 0.042575642 0.055777386 0.058703497 0.052937791 0.043023214 0.033002816 0.023536205 0.012993336 0.0011006445 -0.010389902 -0.020455204 -0.028846059 -0.036066312][0.00091318041 0.024516828 0.046991989 0.062589571 0.067859486 0.063617349 0.053911 0.042861477 0.031410672 0.018781692 0.0050761327 -0.0079966784 -0.019238561 -0.028520271 -0.036331888][-0.0029311627 0.019359596 0.040974006 0.056708261 0.062630713 0.059018403 0.049146637 0.037029222 0.024354212 0.011375092 -0.0016348436 -0.013536945 -0.023640856 -0.03185669 -0.038698059][-0.010738365 0.0080121458 0.026289366 0.039662674 0.04428266 0.040165633 0.030359685 0.018699378 0.0071995556 -0.0035503283 -0.013626792 -0.022548191 -0.030128852 -0.036326356 -0.041518252][-0.020421483 -0.006396234 0.0069612414 0.016318291 0.018547647 0.013800099 0.0051279664 -0.0040356219 -0.012134194 -0.01897236 -0.025196567 -0.030721173 -0.035680123 -0.040000457 -0.043678209][-0.029479098 -0.020232797 -0.011912748 -0.0066263974 -0.0064789429 -0.010833576 -0.017122157 -0.022714742 -0.026853479 -0.029985901 -0.032988384 -0.036014177 -0.039100874 -0.042087842 -0.044733804][-0.03604329 -0.030500613 -0.026053123 -0.023633063 -0.024382569 -0.027538054 -0.03121949 -0.033745691 -0.035100397 -0.035981476 -0.037201874 -0.03881469 -0.0409096 -0.043040659 -0.045063332][-0.039995331 -0.036825057 -0.034701742 -0.033745989 -0.034480743 -0.036353379 -0.038188752 -0.038921 -0.038916755 -0.038848162 -0.039355107 -0.040385172 -0.041973319 -0.043546814 -0.045156296][-0.042353559 -0.04038237 -0.0393026 -0.038751848 -0.039105471 -0.039972406 -0.040709298 -0.040822219 -0.040567026 -0.040454011 -0.040820394 -0.041604158 -0.04280293 -0.044028476 -0.045212585][-0.044252649 -0.042896982 -0.04237337 -0.042034015 -0.042144112 -0.042390767 -0.042526893 -0.0424363 -0.042234328 -0.042107973 -0.042355996 -0.042884856 -0.043658227 -0.044504791 -0.045411982]]...]
INFO - root - 2017-12-06 08:29:52.147016: step 38510, loss = 0.79, batch loss = 0.58 (12.6 examples/sec; 0.637 sec/batch; 52h:02m:12s remains)
INFO - root - 2017-12-06 08:29:58.573659: step 38520, loss = 0.76, batch loss = 0.55 (12.1 examples/sec; 0.662 sec/batch; 54h:01m:13s remains)
INFO - root - 2017-12-06 08:30:05.066467: step 38530, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.648 sec/batch; 52h:56m:01s remains)
INFO - root - 2017-12-06 08:30:11.630802: step 38540, loss = 0.80, batch loss = 0.59 (11.8 examples/sec; 0.676 sec/batch; 55h:13m:01s remains)
INFO - root - 2017-12-06 08:30:18.166217: step 38550, loss = 0.82, batch loss = 0.60 (11.7 examples/sec; 0.684 sec/batch; 55h:51m:35s remains)
INFO - root - 2017-12-06 08:30:24.757829: step 38560, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 52h:48m:57s remains)
INFO - root - 2017-12-06 08:30:31.359858: step 38570, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 51h:19m:50s remains)
INFO - root - 2017-12-06 08:30:37.848989: step 38580, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.637 sec/batch; 52h:02m:51s remains)
INFO - root - 2017-12-06 08:30:44.284847: step 38590, loss = 0.73, batch loss = 0.52 (12.6 examples/sec; 0.637 sec/batch; 52h:01m:51s remains)
INFO - root - 2017-12-06 08:30:50.898681: step 38600, loss = 0.74, batch loss = 0.53 (12.1 examples/sec; 0.659 sec/batch; 53h:47m:03s remains)
2017-12-06 08:30:51.536015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11053086 -0.10279539 -0.10935986 -0.12235413 -0.10872255 -0.10990135 -0.11852528 -0.14084503 -0.16078658 -0.20181358 -0.30162492 -0.40575945 -0.47249159 -0.47815537 -0.48176026][-0.3039422 -0.32469696 -0.32391131 -0.30146512 -0.27259374 -0.28215182 -0.31586626 -0.42075825 -0.52825743 -0.6449362 -0.79270142 -0.90976632 -0.98398936 -0.95829922 -0.91015345][-0.43380308 -0.50631636 -0.53093773 -0.48194885 -0.41010132 -0.34230176 -0.3492904 -0.44418585 -0.572404 -0.75912488 -1.0022317 -1.2322478 -1.4019519 -1.3992691 -1.3487986][-0.39566746 -0.50340557 -0.57428443 -0.5542928 -0.49981779 -0.38348523 -0.32015997 -0.32505637 -0.38395038 -0.49998814 -0.73332703 -1.0262791 -1.274935 -1.3661089 -1.3787668][-0.27588776 -0.36285117 -0.42404947 -0.42894721 -0.37952447 -0.2460959 -0.1724 -0.091989785 -0.058736898 -0.056140188 -0.22240403 -0.46428961 -0.74195766 -0.91175848 -1.0498871][-0.014744677 -0.0507517 -0.092393719 -0.078746714 -0.032937177 0.080128722 0.18609035 0.33545017 0.436278 0.54491204 0.43445134 0.23228157 -0.0639295 -0.26956943 -0.45162767][0.19814977 0.25263748 0.29221979 0.34854174 0.40814981 0.57500654 0.71142131 0.88691264 1.031743 1.1878916 1.1106552 0.96865624 0.70111579 0.4430424 0.1551953][0.281585 0.42169952 0.55527115 0.6917327 0.81865072 1.0147933 1.1899662 1.3947541 1.5254891 1.6711669 1.6573085 1.6000136 1.3612138 1.0884044 0.78576422][0.20578805 0.37059039 0.54014826 0.72616911 0.88655269 1.0820646 1.2147586 1.3339291 1.4062822 1.5169991 1.4951016 1.4651219 1.3539507 1.174682 0.931047][0.073051937 0.16749868 0.26977906 0.40592343 0.5408842 0.68507248 0.77085161 0.87599653 0.9271968 0.97969228 0.99609047 1.0222449 0.92835373 0.78195053 0.63108611][-0.072909832 -0.052984852 -0.032960888 -0.023588039 0.026253536 0.086597733 0.11475389 0.145816 0.169709 0.26632184 0.29129946 0.30814153 0.32176548 0.28081924 0.20410568][-0.14949645 -0.18253779 -0.23726127 -0.316166 -0.3829613 -0.49935174 -0.60219473 -0.65197206 -0.62063056 -0.56637985 -0.49744216 -0.3563365 -0.25092307 -0.22862193 -0.22151148][-0.15522549 -0.2134085 -0.29635525 -0.39799079 -0.51923835 -0.66500634 -0.81563854 -0.96651518 -1.0334822 -1.015267 -0.90974927 -0.78010535 -0.63172704 -0.48845723 -0.40349475][-0.1259079 -0.16707289 -0.22864914 -0.318579 -0.42766187 -0.56600153 -0.72471452 -0.87593937 -0.964705 -1.0027834 -0.95106804 -0.83147746 -0.65395665 -0.52070606 -0.41362652][-0.10717548 -0.13281855 -0.17056265 -0.22441185 -0.27225947 -0.36875781 -0.48285902 -0.60478473 -0.69669062 -0.729804 -0.69486028 -0.59148973 -0.4539434 -0.31983575 -0.22950917]]...]
INFO - root - 2017-12-06 08:30:58.173469: step 38610, loss = 0.71, batch loss = 0.50 (12.4 examples/sec; 0.647 sec/batch; 52h:48m:35s remains)
INFO - root - 2017-12-06 08:31:04.746661: step 38620, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 53h:30m:08s remains)
INFO - root - 2017-12-06 08:31:11.160298: step 38630, loss = 0.84, batch loss = 0.62 (12.2 examples/sec; 0.658 sec/batch; 53h:44m:40s remains)
INFO - root - 2017-12-06 08:31:17.648648: step 38640, loss = 0.87, batch loss = 0.66 (12.8 examples/sec; 0.625 sec/batch; 51h:00m:24s remains)
INFO - root - 2017-12-06 08:31:24.063840: step 38650, loss = 0.80, batch loss = 0.58 (11.8 examples/sec; 0.679 sec/batch; 55h:27m:00s remains)
INFO - root - 2017-12-06 08:31:30.673444: step 38660, loss = 0.77, batch loss = 0.56 (11.5 examples/sec; 0.698 sec/batch; 56h:57m:50s remains)
INFO - root - 2017-12-06 08:31:37.202031: step 38670, loss = 0.81, batch loss = 0.60 (11.8 examples/sec; 0.678 sec/batch; 55h:22m:14s remains)
INFO - root - 2017-12-06 08:31:43.774445: step 38680, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 53h:11m:06s remains)
INFO - root - 2017-12-06 08:31:50.344032: step 38690, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.654 sec/batch; 53h:23m:25s remains)
INFO - root - 2017-12-06 08:31:56.894333: step 38700, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.657 sec/batch; 53h:38m:41s remains)
2017-12-06 08:31:57.538753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12488116 -0.12391746 -0.12155969 -0.11904874 -0.1167273 -0.11544812 -0.11564024 -0.11701356 -0.11901459 -0.12130219 -0.12280872 -0.12280938 -0.12123773 -0.1177051 -0.11317903][-0.1224352 -0.12073258 -0.11770806 -0.11463149 -0.11221588 -0.11068164 -0.11083308 -0.11245 -0.11492883 -0.11759286 -0.11982952 -0.12062287 -0.11959829 -0.11642399 -0.11243652][-0.11791156 -0.11583179 -0.11238635 -0.10894974 -0.1060847 -0.10434946 -0.10430671 -0.10591459 -0.10829841 -0.11130963 -0.11409524 -0.1159044 -0.11611618 -0.11413586 -0.11111651][-0.11155098 -0.1092952 -0.10516671 -0.10142438 -0.09853556 -0.096610337 -0.096479833 -0.09809038 -0.10069656 -0.10411334 -0.10753623 -0.11043297 -0.11150568 -0.11042801 -0.10836764][-0.10465202 -0.10196542 -0.09789113 -0.094058178 -0.0908541 -0.089019306 -0.089163579 -0.09106718 -0.093894966 -0.09760534 -0.10141829 -0.10467275 -0.10652667 -0.10641666 -0.10533395][-0.098654844 -0.0957241 -0.091824241 -0.087976769 -0.084895469 -0.083193421 -0.0832288 -0.085066758 -0.087823756 -0.091433652 -0.0954621 -0.098740458 -0.10081123 -0.10154834 -0.10166175][-0.093590982 -0.090808108 -0.087234125 -0.083867639 -0.081081465 -0.079312049 -0.079307213 -0.080896378 -0.083211765 -0.086442947 -0.090224676 -0.093281992 -0.095470466 -0.097032517 -0.098017633][-0.089919478 -0.087309286 -0.084027857 -0.080991745 -0.078653067 -0.077082649 -0.076762505 -0.077918 -0.079915434 -0.082563408 -0.085769847 -0.088738851 -0.0908791 -0.092645265 -0.094274953][-0.087141261 -0.084695406 -0.081766859 -0.079135545 -0.077296086 -0.076070651 -0.075964659 -0.076998584 -0.078615032 -0.080759719 -0.083478644 -0.085933268 -0.08778242 -0.089363836 -0.0909109][-0.085070305 -0.082976706 -0.080715664 -0.078719959 -0.077375971 -0.076667011 -0.076950304 -0.078009471 -0.079316132 -0.080896109 -0.0827985 -0.084420346 -0.085789651 -0.086953245 -0.087795533][-0.084088609 -0.082432836 -0.08079154 -0.079453766 -0.0786925 -0.07839372 -0.0787599 -0.079537377 -0.080480471 -0.081619665 -0.08308094 -0.084321387 -0.085279472 -0.085940205 -0.086277455][-0.083855852 -0.082681671 -0.081684552 -0.081005938 -0.080732614 -0.080694258 -0.081059165 -0.081654087 -0.0824227 -0.0833238 -0.084440917 -0.085403077 -0.085911095 -0.086009063 -0.085786007][-0.08448758 -0.083748311 -0.083221458 -0.082960933 -0.083008029 -0.083181486 -0.083684862 -0.084349185 -0.085108869 -0.08589568 -0.086520158 -0.0868629 -0.086847216 -0.086631559 -0.086422935][-0.08573316 -0.085330054 -0.085088566 -0.085032195 -0.085193492 -0.085510604 -0.086032316 -0.08661183 -0.087157354 -0.087616041 -0.087818407 -0.087746561 -0.087504491 -0.087253883 -0.087042406][-0.086812034 -0.086563438 -0.0864624 -0.086489916 -0.086659327 -0.086940587 -0.087283067 -0.087614238 -0.0878528 -0.087976649 -0.08796563 -0.087849863 -0.087680228 -0.087536849 -0.087442189]]...]
INFO - root - 2017-12-06 08:32:04.044423: step 38710, loss = 0.76, batch loss = 0.55 (11.8 examples/sec; 0.679 sec/batch; 55h:26m:26s remains)
INFO - root - 2017-12-06 08:32:10.504966: step 38720, loss = 0.89, batch loss = 0.67 (12.4 examples/sec; 0.647 sec/batch; 52h:46m:04s remains)
INFO - root - 2017-12-06 08:32:17.127016: step 38730, loss = 0.85, batch loss = 0.64 (12.8 examples/sec; 0.626 sec/batch; 51h:05m:15s remains)
INFO - root - 2017-12-06 08:32:23.684399: step 38740, loss = 0.94, batch loss = 0.72 (12.0 examples/sec; 0.669 sec/batch; 54h:33m:36s remains)
INFO - root - 2017-12-06 08:32:30.142009: step 38750, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.652 sec/batch; 53h:14m:27s remains)
INFO - root - 2017-12-06 08:32:36.678165: step 38760, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.650 sec/batch; 53h:00m:59s remains)
INFO - root - 2017-12-06 08:32:43.183196: step 38770, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.639 sec/batch; 52h:09m:33s remains)
INFO - root - 2017-12-06 08:32:49.599497: step 38780, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.649 sec/batch; 52h:55m:57s remains)
INFO - root - 2017-12-06 08:32:56.030142: step 38790, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.635 sec/batch; 51h:47m:17s remains)
INFO - root - 2017-12-06 08:33:02.548904: step 38800, loss = 0.89, batch loss = 0.67 (11.8 examples/sec; 0.677 sec/batch; 55h:15m:30s remains)
2017-12-06 08:33:03.144001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054369859 -0.36403939 -0.57820487 -0.86860007 -1.0769166 -1.2738789 -1.3412644 -1.3877313 -1.3839718 -1.4537731 -1.5515867 -1.2920604 -1.061781 -1.0737253 -0.9336828][0.10141177 -0.28049853 -0.55439717 -0.85679257 -1.0103503 -1.2366542 -1.4091749 -1.4817154 -1.5263016 -1.614931 -1.6439142 -1.4082375 -1.1241698 -0.81819671 -0.62251282][-0.0015963018 -0.2774986 -0.47780862 -0.71349007 -0.9385156 -1.1853325 -1.3419836 -1.4068782 -1.4611073 -1.5468575 -1.5247464 -1.3125157 -1.1989149 -0.82543325 -0.5962798][0.10700678 -0.039392885 -0.19258156 -0.21299668 -0.30979323 -0.51056367 -0.73822784 -0.8504073 -0.90994895 -0.87587291 -0.80050373 -0.56425726 -0.38986179 -0.18007064 -0.14229885][0.37051728 0.28889209 0.13102403 0.12814626 0.077564009 -0.046355419 -0.15804325 -0.30108011 -0.39170828 -0.37172818 -0.31966427 -0.13585925 -0.07984706 0.12087647 0.044643916][0.74934936 0.6484105 0.525004 0.56992477 0.56442344 0.51477182 0.46048114 0.41125062 0.35363391 0.23229718 0.16987255 0.20122167 0.13454074 0.2007542 -0.17066331][0.870331 0.874691 0.74815285 0.61931652 0.5730505 0.579244 0.56367636 0.60226226 0.72412658 0.67130053 0.53463596 0.40143719 0.3256568 0.24634993 -0.19055392][0.91905415 0.90523994 0.65468979 0.48239729 0.43113056 0.37261578 0.33552629 0.32682818 0.38709164 0.45799318 0.41307476 0.28605622 0.30168244 0.12633088 -0.10873424][0.49963757 0.43549654 0.19428599 -0.10338382 -0.29444939 -0.32989919 -0.357266 -0.36343881 -0.30814189 -0.28646758 -0.24883366 -0.20476958 0.00875859 -0.04484006 -0.25293529][-0.00089383125 -0.0805752 -0.29038045 -0.55326986 -0.82590514 -0.94081771 -0.99893481 -0.9667297 -0.87310076 -0.70386314 -0.641906 -0.63531548 -0.49837619 -0.28758812 -0.3067207][-0.19263296 -0.30859259 -0.50735533 -0.8201372 -1.1104398 -1.2573484 -1.2841793 -1.2825856 -1.2151632 -1.0009871 -0.85937566 -0.78489888 -0.71946663 -0.5459516 -0.4923757][-0.34152383 -0.60227191 -0.78988957 -0.99452096 -1.1883327 -1.302711 -1.3362861 -1.3381876 -1.3715782 -1.3309633 -1.2539318 -1.0600866 -1.1036468 -0.98236048 -0.91726917][-0.27398935 -0.5560497 -0.79437596 -0.94581908 -1.0210794 -1.0423185 -1.0820743 -1.115683 -1.0949647 -1.1341885 -1.0678302 -0.95863312 -0.9881286 -0.882419 -0.69405085][-0.13596627 -0.17876007 -0.33135214 -0.45442244 -0.58128089 -0.60659474 -0.68549705 -0.75774533 -0.78296578 -0.78734505 -0.79531747 -0.747893 -0.652369 -0.57980907 -0.58634174][0.23115185 0.032314949 -0.15896833 -0.17516994 -0.25332174 -0.24547526 -0.34021488 -0.42013642 -0.492282 -0.51322305 -0.54856777 -0.57420522 -0.48009667 -0.41512325 -0.25634107]]...]
INFO - root - 2017-12-06 08:33:09.523106: step 38810, loss = 0.90, batch loss = 0.68 (12.3 examples/sec; 0.653 sec/batch; 53h:16m:23s remains)
INFO - root - 2017-12-06 08:33:16.116478: step 38820, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 53h:19m:28s remains)
INFO - root - 2017-12-06 08:33:22.550830: step 38830, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 51h:18m:54s remains)
INFO - root - 2017-12-06 08:33:29.095378: step 38840, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.653 sec/batch; 53h:17m:01s remains)
INFO - root - 2017-12-06 08:33:35.647930: step 38850, loss = 0.82, batch loss = 0.61 (11.8 examples/sec; 0.680 sec/batch; 55h:28m:55s remains)
INFO - root - 2017-12-06 08:33:42.263808: step 38860, loss = 0.72, batch loss = 0.51 (12.2 examples/sec; 0.654 sec/batch; 53h:22m:48s remains)
INFO - root - 2017-12-06 08:33:48.692570: step 38870, loss = 0.80, batch loss = 0.59 (11.7 examples/sec; 0.684 sec/batch; 55h:49m:32s remains)
INFO - root - 2017-12-06 08:33:55.264913: step 38880, loss = 0.78, batch loss = 0.56 (12.5 examples/sec; 0.641 sec/batch; 52h:18m:48s remains)
INFO - root - 2017-12-06 08:34:01.803732: step 38890, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 52h:23m:57s remains)
INFO - root - 2017-12-06 08:34:08.280196: step 38900, loss = 0.79, batch loss = 0.57 (12.1 examples/sec; 0.662 sec/batch; 54h:00m:59s remains)
2017-12-06 08:34:08.877422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027733296 -0.077656187 -0.059104953 0.012023315 0.11919943 0.15081143 0.14994872 0.15221786 0.15157846 0.11384954 0.072796069 0.042768009 0.023617648 0.026939727 0.042320274][0.073461048 0.023580857 -0.004576616 -0.0076312721 0.052655689 0.1037072 0.1379813 0.1384514 0.1371372 0.1056707 0.06844037 0.023843348 -0.0034314245 0.00041047484 0.029421717][0.18053886 0.15153569 0.10939809 0.11148497 0.14047831 0.11848073 0.10145668 0.064127319 0.069259055 0.047944941 0.0043394342 -0.046869647 -0.081346184 -0.065261245 -0.019825406][0.16190121 0.18429145 0.18887529 0.20076489 0.19874498 0.16810685 0.10227776 0.0035636127 -0.010197736 -0.0048316643 -0.013917699 -0.047224626 -0.082581475 -0.069800921 -0.015658848][-0.03082709 0.0061592534 0.0900087 0.1835013 0.21015948 0.14765817 0.038886659 -0.040639721 -0.07164827 -0.0689948 -0.045512863 -0.050804302 -0.064354166 -0.0479656 -0.029264733][-0.21961796 -0.16983953 -0.011518165 0.14328948 0.23638356 0.24519092 0.15687543 0.030738145 -0.0657831 -0.09596926 -0.096121334 -0.10659254 -0.10859056 -0.099109277 -0.076030165][-0.25940228 -0.2192232 -0.046963528 0.16545022 0.31008282 0.35008609 0.28386968 0.17263675 0.027780794 -0.068825275 -0.10574608 -0.10966836 -0.10833094 -0.104204 -0.090045251][-0.23967829 -0.1789054 -0.022630736 0.18620369 0.37144423 0.45170423 0.42246982 0.29270872 0.13402054 0.043155052 -0.0094740465 -0.031261623 -0.052119449 -0.065905154 -0.068225838][-0.22200304 -0.16727281 -0.00562305 0.19089845 0.36866766 0.47402868 0.47647253 0.34430921 0.1767841 0.060896 0.00063268095 -0.016280778 -0.027577139 -0.034422204 -0.042327944][-0.12502426 -0.10244543 -0.012923948 0.12549561 0.26738408 0.32698661 0.31931612 0.22027338 0.096751146 0.0212304 -0.031262964 -0.057507094 -0.071845338 -0.07522893 -0.073484808][-0.077410571 -0.069688842 -0.060709644 -0.024776854 0.076153941 0.12086367 0.10473683 0.036423452 -0.030846477 -0.052621845 -0.06771782 -0.075680248 -0.081393972 -0.084905744 -0.086470023][-0.062153209 -0.082020357 -0.11852103 -0.10594678 -0.045144852 0.007683292 0.028979689 -0.019116335 -0.064558208 -0.082526937 -0.084401891 -0.085981831 -0.086069077 -0.08661896 -0.087494381][-0.027586184 -0.06672591 -0.092871286 -0.10655673 -0.0870965 -0.06759014 -0.04199405 -0.052951816 -0.0658365 -0.089264661 -0.0836084 -0.087213606 -0.085356034 -0.085743658 -0.085943617][0.025715321 -0.030157596 -0.078281671 -0.088321552 -0.091503561 -0.087931864 -0.08216162 -0.075206652 -0.0717471 -0.0988505 -0.08758612 -0.089898907 -0.085369237 -0.086525358 -0.086345226][0.029108658 -0.029393546 -0.0802504 -0.092951842 -0.10221791 -0.092163488 -0.082445346 -0.088611938 -0.077205665 -0.096977279 -0.093553931 -0.089609213 -0.082119353 -0.087534964 -0.0866991]]...]
INFO - root - 2017-12-06 08:34:15.229727: step 38910, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.646 sec/batch; 52h:39m:18s remains)
INFO - root - 2017-12-06 08:34:21.743042: step 38920, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.658 sec/batch; 53h:38m:17s remains)
INFO - root - 2017-12-06 08:34:28.088558: step 38930, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.656 sec/batch; 53h:30m:46s remains)
INFO - root - 2017-12-06 08:34:34.639466: step 38940, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 53h:32m:29s remains)
INFO - root - 2017-12-06 08:34:41.157923: step 38950, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.650 sec/batch; 53h:02m:12s remains)
INFO - root - 2017-12-06 08:34:47.772126: step 38960, loss = 0.81, batch loss = 0.59 (12.0 examples/sec; 0.669 sec/batch; 54h:32m:22s remains)
INFO - root - 2017-12-06 08:34:54.280734: step 38970, loss = 0.83, batch loss = 0.62 (11.6 examples/sec; 0.689 sec/batch; 56h:11m:22s remains)
INFO - root - 2017-12-06 08:35:00.836099: step 38980, loss = 0.96, batch loss = 0.75 (12.2 examples/sec; 0.655 sec/batch; 53h:24m:02s remains)
INFO - root - 2017-12-06 08:35:07.411683: step 38990, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.657 sec/batch; 53h:34m:09s remains)
INFO - root - 2017-12-06 08:35:13.820047: step 39000, loss = 0.78, batch loss = 0.57 (11.6 examples/sec; 0.688 sec/batch; 56h:03m:34s remains)
2017-12-06 08:35:14.481419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05623275 -0.055753928 -0.055801734 -0.055848088 -0.055920511 -0.055964988 -0.055969436 -0.055924539 -0.055830166 -0.055677321 -0.05550741 -0.055437259 -0.055409562 -0.0554138 -0.05569192][-0.056433354 -0.055952638 -0.05594258 -0.055970829 -0.056151882 -0.05639 -0.056568697 -0.056591354 -0.056474019 -0.056289896 -0.056037158 -0.055827953 -0.055676479 -0.055637646 -0.055877071][-0.057160079 -0.056448817 -0.056322109 -0.056264076 -0.056466617 -0.056908824 -0.057324387 -0.0575172 -0.057482645 -0.05722551 -0.056826465 -0.056422397 -0.05607621 -0.055870578 -0.05593273][-0.058485024 -0.057628252 -0.057449516 -0.057165246 -0.057100236 -0.057514742 -0.058208555 -0.058672346 -0.058773424 -0.058490764 -0.057935175 -0.057311397 -0.056783505 -0.056393143 -0.056287181][-0.060400579 -0.059868596 -0.059909221 -0.059519265 -0.059111275 -0.059088223 -0.059523597 -0.060042076 -0.060418434 -0.060153432 -0.059365612 -0.058390573 -0.057586271 -0.05704252 -0.056915626][-0.062957838 -0.06291312 -0.063372537 -0.063404858 -0.062995151 -0.062565118 -0.06252268 -0.062440578 -0.062216643 -0.061852969 -0.061092239 -0.05973728 -0.058461022 -0.057602171 -0.057465907][-0.0658591 -0.066557042 -0.06758792 -0.068051964 -0.068133488 -0.067893289 -0.067547448 -0.066858187 -0.0656834 -0.064256452 -0.062716834 -0.060834147 -0.059071034 -0.057904806 -0.057834741][-0.07107693 -0.071919814 -0.073062785 -0.073776737 -0.074079305 -0.073794656 -0.0731966 -0.072086528 -0.070328265 -0.067975253 -0.0651247 -0.062060911 -0.059543349 -0.058096595 -0.057997856][-0.076646909 -0.078308865 -0.0800874 -0.081127122 -0.081313752 -0.080160126 -0.078759991 -0.076973289 -0.07444413 -0.070929065 -0.066849612 -0.06290178 -0.059715897 -0.057991162 -0.057832275][-0.078922331 -0.081727453 -0.084653549 -0.086589649 -0.087239377 -0.085912071 -0.083713718 -0.0807396 -0.07695695 -0.072321855 -0.067386374 -0.062921181 -0.059461381 -0.057599343 -0.057444047][-0.079266995 -0.082144134 -0.085108414 -0.087603644 -0.088700213 -0.087566443 -0.085417144 -0.082031004 -0.07769452 -0.072426125 -0.067209058 -0.062591925 -0.058920104 -0.05696981 -0.056999646][-0.079610035 -0.082337238 -0.084565766 -0.086120538 -0.08671733 -0.085441142 -0.082782254 -0.079136513 -0.074951433 -0.070285566 -0.065722682 -0.061669804 -0.058484778 -0.056719661 -0.0567148][-0.077530682 -0.080128253 -0.08215633 -0.083393976 -0.0835453 -0.082048774 -0.078966253 -0.074992716 -0.071078226 -0.067130454 -0.063430794 -0.060276411 -0.057974476 -0.056797307 -0.056732][-0.074402452 -0.076512806 -0.078216195 -0.079310849 -0.079388104 -0.077852853 -0.075149126 -0.071581885 -0.068185 -0.064768896 -0.061722994 -0.05937757 -0.057716094 -0.056803063 -0.056693438][-0.071493551 -0.073249616 -0.074783079 -0.075478 -0.075119928 -0.073629193 -0.0713421 -0.068692878 -0.066355623 -0.063710436 -0.061236054 -0.05922633 -0.057834473 -0.056844521 -0.056570038]]...]
INFO - root - 2017-12-06 08:35:21.004943: step 39010, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.665 sec/batch; 54h:10m:33s remains)
INFO - root - 2017-12-06 08:35:27.533943: step 39020, loss = 0.86, batch loss = 0.65 (12.3 examples/sec; 0.650 sec/batch; 52h:58m:06s remains)
INFO - root - 2017-12-06 08:35:34.033167: step 39030, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 52h:45m:04s remains)
INFO - root - 2017-12-06 08:35:40.418669: step 39040, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 53h:23m:43s remains)
INFO - root - 2017-12-06 08:35:46.979846: step 39050, loss = 0.77, batch loss = 0.56 (11.9 examples/sec; 0.670 sec/batch; 54h:36m:45s remains)
INFO - root - 2017-12-06 08:35:53.497749: step 39060, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.643 sec/batch; 52h:24m:54s remains)
INFO - root - 2017-12-06 08:35:59.880433: step 39070, loss = 0.75, batch loss = 0.54 (13.4 examples/sec; 0.597 sec/batch; 48h:41m:25s remains)
INFO - root - 2017-12-06 08:36:06.327993: step 39080, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.641 sec/batch; 52h:14m:13s remains)
INFO - root - 2017-12-06 08:36:12.799731: step 39090, loss = 0.73, batch loss = 0.52 (13.7 examples/sec; 0.584 sec/batch; 47h:34m:14s remains)
INFO - root - 2017-12-06 08:36:19.318749: step 39100, loss = 0.88, batch loss = 0.66 (11.9 examples/sec; 0.671 sec/batch; 54h:40m:08s remains)
2017-12-06 08:36:19.997274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6186215 -1.9062932 -2.1355066 -2.3476686 -2.3426702 -2.3074307 -2.2712536 -1.9263327 -1.4494187 -1.1723967 -0.71506846 -0.34817696 0.047165632 0.254444 0.55780482][-1.5056581 -1.6823338 -1.7676423 -2.0072422 -2.1067531 -2.3526971 -2.6541696 -2.726095 -2.6013117 -2.3716235 -1.9413488 -1.3861834 -0.48311108 0.096295521 0.20205547][-1.5857838 -1.6562651 -1.7033637 -1.9216102 -1.9649996 -2.1522431 -2.5472484 -2.9490483 -3.0499728 -2.960228 -2.7369478 -2.3926387 -1.689378 -1.0204858 -0.40571129][-1.8862205 -1.9129951 -1.8427877 -1.7302914 -1.5751889 -1.8227994 -2.0919883 -2.4738429 -2.7486634 -2.8518639 -2.8971806 -2.8868213 -2.7165005 -2.2121058 -1.6446736][-1.549997 -1.4549167 -1.2877296 -1.0893145 -1.0508991 -0.93654817 -0.83520931 -1.3263124 -1.7703314 -1.988888 -2.154285 -2.3118627 -2.4443064 -2.1434989 -1.7206388][-0.48754615 -0.3763085 -0.20283592 0.36802906 0.749842 0.96545851 1.0998194 1.0457708 1.0488572 0.65956616 0.24499057 0.0069472864 -0.20834574 -0.42931479 -0.38401151][0.67565924 1.0666401 1.3537738 1.7495674 2.0632615 2.596945 2.9551337 3.1623242 3.1872311 3.0131576 2.8310945 2.3377452 1.8134459 1.289569 0.72103024][1.4062308 1.7546289 2.07812 2.5459316 2.8707967 3.2187202 3.3943408 3.4953659 3.5490863 3.7087846 3.7065632 3.4459589 3.1485567 2.4954119 1.6471876][0.60616159 1.0312362 1.4422324 1.5960591 1.7597891 2.05324 2.2441931 2.5919373 2.7706826 2.8966303 2.7065916 2.5536883 2.2668445 1.6436756 1.2107141][-0.8563689 -0.848566 -0.85749215 -0.86583906 -0.83918869 -0.76907623 -0.65828425 -0.43028682 -0.23215282 0.24170659 0.55706203 0.72454172 0.6420579 0.25483859 -0.1777333][-2.1614988 -2.4478412 -2.664155 -2.9321871 -3.0733535 -3.1875558 -3.2526917 -3.1639559 -3.0583534 -2.7474842 -2.4576375 -1.9199084 -1.6619753 -1.6528203 -1.683184][-2.6951997 -3.4575841 -4.0686259 -4.4968204 -4.67646 -4.8781629 -4.9881639 -5.0027676 -4.8836379 -4.6201982 -4.3196807 -3.8715143 -3.5275958 -2.9934349 -2.5746272][-2.7211742 -3.4158983 -3.9371982 -4.4667592 -4.8694072 -5.2100759 -5.4583468 -5.639225 -5.5815449 -5.3858547 -5.1380825 -4.7174182 -4.3950419 -4.0042343 -3.440872][-2.0536451 -2.7713125 -3.4281201 -3.809144 -4.0039344 -4.1833334 -4.4170527 -4.6439118 -4.8004141 -4.7655549 -4.5430713 -4.2952785 -3.927098 -3.4254453 -2.9770048][-1.1881979 -1.7933351 -2.3119636 -2.6947777 -3.0151587 -3.1749914 -3.2752619 -3.2927895 -3.2824888 -3.1507964 -3.0998597 -3.0802896 -2.9818437 -2.7343163 -2.3682754]]...]
INFO - root - 2017-12-06 08:36:26.631722: step 39110, loss = 0.86, batch loss = 0.65 (11.8 examples/sec; 0.676 sec/batch; 55h:03m:27s remains)
INFO - root - 2017-12-06 08:36:33.195048: step 39120, loss = 0.76, batch loss = 0.55 (12.0 examples/sec; 0.664 sec/batch; 54h:08m:33s remains)
INFO - root - 2017-12-06 08:36:39.656578: step 39130, loss = 0.74, batch loss = 0.52 (12.8 examples/sec; 0.624 sec/batch; 50h:49m:18s remains)
INFO - root - 2017-12-06 08:36:46.155302: step 39140, loss = 0.73, batch loss = 0.51 (12.7 examples/sec; 0.629 sec/batch; 51h:17m:04s remains)
INFO - root - 2017-12-06 08:36:52.653092: step 39150, loss = 0.73, batch loss = 0.52 (12.5 examples/sec; 0.641 sec/batch; 52h:13m:10s remains)
INFO - root - 2017-12-06 08:36:59.104181: step 39160, loss = 0.79, batch loss = 0.58 (12.8 examples/sec; 0.625 sec/batch; 50h:56m:32s remains)
INFO - root - 2017-12-06 08:37:05.634595: step 39170, loss = 0.83, batch loss = 0.62 (12.1 examples/sec; 0.659 sec/batch; 53h:42m:53s remains)
INFO - root - 2017-12-06 08:37:12.017503: step 39180, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.642 sec/batch; 52h:19m:36s remains)
INFO - root - 2017-12-06 08:37:18.387706: step 39190, loss = 0.80, batch loss = 0.59 (12.6 examples/sec; 0.633 sec/batch; 51h:32m:34s remains)
INFO - root - 2017-12-06 08:37:24.837041: step 39200, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 53h:56m:28s remains)
2017-12-06 08:37:25.483499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.26773757 -0.26449174 -0.25557166 -0.26067343 -0.26357216 -0.26630226 -0.27979764 -0.3281633 -0.38717785 -0.42136097 -0.45452926 -0.47146004 -0.48801109 -0.48258588 -0.44754696][-0.2117714 -0.23294844 -0.2293039 -0.22353685 -0.22802953 -0.21044606 -0.21086434 -0.24707828 -0.30165094 -0.3431012 -0.40659824 -0.47851315 -0.54881346 -0.59013307 -0.54527569][-0.25504583 -0.31057218 -0.32635546 -0.29539734 -0.29447937 -0.2639662 -0.25726265 -0.24932785 -0.27497289 -0.31407234 -0.39980704 -0.52621448 -0.64080417 -0.73488033 -0.73106313][-0.25033221 -0.30390361 -0.3215515 -0.27949554 -0.26015788 -0.20910539 -0.15658812 -0.12152874 -0.13226321 -0.18281299 -0.27673429 -0.41513893 -0.56382024 -0.70291817 -0.77314258][-0.14582051 -0.16283545 -0.16080467 -0.13228439 -0.11023644 -0.040524449 0.03701888 0.10164066 0.10322885 0.054710031 -0.04865922 -0.18653926 -0.36647487 -0.5443055 -0.68045473][-0.04206853 -0.053116955 -0.050637607 -0.030614842 -0.003917858 0.049608648 0.10914508 0.15782717 0.16256896 0.14200222 0.044311598 -0.080411166 -0.2483495 -0.42714921 -0.56319261][0.0095221028 0.0049888492 0.025312155 0.062016219 0.1037382 0.12478605 0.16127783 0.19742665 0.21440133 0.19263422 0.11320609 0.043327615 -0.082108222 -0.24077253 -0.34272975][0.0043802187 0.0071375892 0.048660904 0.12899753 0.20921901 0.26197544 0.32345259 0.34073743 0.34770712 0.33680683 0.2936489 0.23529381 0.1571002 0.053725958 -0.016607739][-0.001515232 -0.0099509507 0.033220105 0.11550151 0.22232559 0.32467118 0.41692755 0.47140613 0.5113616 0.47932741 0.44761208 0.39227414 0.32024264 0.23881483 0.15512803][-0.027312458 -0.060885154 -0.051727735 0.0077968687 0.09143883 0.19563794 0.27759928 0.36027196 0.44508383 0.47125271 0.48103592 0.42014477 0.33284038 0.22843173 0.14335883][-0.10912563 -0.13424541 -0.13387018 -0.11208461 -0.063649207 -0.014113784 0.022039086 0.084566325 0.17105779 0.25992787 0.30665085 0.28760561 0.2238346 0.11319681 0.0086992159][-0.14871386 -0.12866759 -0.11660653 -0.10312548 -0.091714218 -0.093758479 -0.095707566 -0.079417534 -0.050638888 -0.001320608 0.030405171 0.033000909 -0.0056851879 -0.07307516 -0.11696335][-0.080530182 -0.055065718 -0.019203685 -0.017271034 -0.033857044 -0.058493886 -0.068295479 -0.083884761 -0.10256551 -0.10531093 -0.11031277 -0.11445142 -0.13783935 -0.16169962 -0.16532114][-0.0964589 -0.077473991 -0.044653878 -0.040419213 -0.042416569 -0.053429905 -0.055796124 -0.069689326 -0.0929 -0.11481921 -0.13326915 -0.13964765 -0.15252621 -0.15897848 -0.15673855][-0.0892025 -0.082218818 -0.065002605 -0.061281446 -0.063674524 -0.065783307 -0.066250473 -0.0783203 -0.099962495 -0.1174615 -0.12983847 -0.139178 -0.14799951 -0.1362453 -0.12470174]]...]
INFO - root - 2017-12-06 08:37:31.943068: step 39210, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.660 sec/batch; 53h:47m:49s remains)
INFO - root - 2017-12-06 08:37:38.387593: step 39220, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.641 sec/batch; 52h:14m:14s remains)
INFO - root - 2017-12-06 08:37:44.947156: step 39230, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.652 sec/batch; 53h:05m:51s remains)
INFO - root - 2017-12-06 08:37:51.331914: step 39240, loss = 0.86, batch loss = 0.65 (13.0 examples/sec; 0.616 sec/batch; 50h:09m:17s remains)
INFO - root - 2017-12-06 08:37:57.858393: step 39250, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.652 sec/batch; 53h:06m:29s remains)
INFO - root - 2017-12-06 08:38:04.461158: step 39260, loss = 0.78, batch loss = 0.56 (11.7 examples/sec; 0.685 sec/batch; 55h:46m:41s remains)
INFO - root - 2017-12-06 08:38:11.042694: step 39270, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.658 sec/batch; 53h:36m:02s remains)
INFO - root - 2017-12-06 08:38:17.419548: step 39280, loss = 0.83, batch loss = 0.61 (12.6 examples/sec; 0.637 sec/batch; 51h:54m:44s remains)
INFO - root - 2017-12-06 08:38:23.926695: step 39290, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 53h:22m:51s remains)
INFO - root - 2017-12-06 08:38:30.479595: step 39300, loss = 0.76, batch loss = 0.55 (12.3 examples/sec; 0.649 sec/batch; 52h:51m:30s remains)
2017-12-06 08:38:31.052195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.42782253 -0.51835006 -0.54767162 -0.40733221 -0.18781203 0.075274147 0.21733633 0.30916902 0.20063418 -0.021749899 -0.2004467 -0.33407846 -0.45335609 -0.44010448 -0.38973269][-0.78385836 -0.88736421 -0.98398077 -0.94839495 -0.749716 -0.42036989 -0.15488411 -0.063416027 -0.13551697 -0.19959125 -0.4718985 -0.74274921 -0.80297637 -0.81321341 -0.75920814][-1.2676799 -1.3625705 -1.4700221 -1.4859091 -1.4415933 -1.2651547 -1.0620339 -0.90245503 -0.85515934 -0.974437 -1.1567349 -1.2500696 -1.3426647 -1.2456585 -1.1147077][-1.5374235 -1.729969 -1.9536667 -2.1056843 -2.14746 -2.1215246 -2.0798457 -2.1032817 -2.116755 -2.1193352 -2.0797842 -2.0984416 -2.0794191 -1.8907074 -1.7055864][-1.2490941 -1.4762166 -1.6595247 -1.8684529 -2.0976472 -2.2417281 -2.3409162 -2.4757726 -2.632194 -2.7833755 -2.8765845 -2.741864 -2.6119673 -2.6281636 -2.626482][-0.71016741 -0.91149616 -1.1332681 -1.3831764 -1.6198015 -1.820052 -1.9494057 -2.07611 -2.1929643 -2.2452271 -2.5480003 -2.7923312 -2.8005197 -2.8961849 -3.1280637][0.015369095 -0.21456292 -0.24747175 -0.35434109 -0.53216803 -0.73827082 -0.92779255 -1.1069003 -1.3332508 -1.5893848 -1.9070847 -2.3664901 -2.6093833 -2.8974721 -3.1705139][0.1841678 0.044737138 0.10632671 0.16043481 0.15414023 0.0891295 -0.067082569 -0.24236882 -0.48900771 -0.80690348 -1.0351808 -1.2700027 -1.5401695 -1.7791506 -2.1376324][0.082689069 -0.14156805 -0.046939652 0.13214061 0.25462621 0.26038027 0.1880635 0.075608753 -0.06056454 -0.17081766 -0.25134 -0.29306665 -0.26381603 -0.34784597 -0.7766133][0.0752823 -0.02680739 0.11119463 0.18718961 0.29395345 0.34976614 0.3109397 0.21749562 0.13633758 0.10642175 0.24778822 0.55714881 0.84713894 1.0688028 0.97894228][0.4389337 0.19809288 0.28412595 0.53229272 0.75320745 0.77020973 0.59407973 0.33461928 0.16543275 0.23201466 0.43824574 0.89851326 1.4642684 1.8524529 1.9467157][0.72861809 0.85084885 1.0081862 1.0581424 1.1623869 1.2205284 1.0724013 0.8586908 0.56113565 0.48603991 0.68169385 1.1204177 1.6474127 1.9341077 1.9209327][1.1234373 1.0883466 1.0687964 1.0553836 1.0623314 0.9759376 0.83200014 0.66310537 0.65101832 0.67227209 0.7836439 0.95294452 1.20869 1.4486785 1.4139735][0.94468236 0.89274013 0.76619434 0.64369446 0.59001434 0.51978153 0.50505239 0.35804653 0.20625338 0.23330641 0.23141646 0.32072183 0.37716097 0.3959299 0.34190151][0.63136572 0.37392193 0.052483313 -0.15242475 -0.24988958 -0.38900459 -0.39436129 -0.370648 -0.48676929 -0.52800196 -0.59084511 -0.74494773 -0.82055235 -0.87158334 -0.94200617]]...]
INFO - root - 2017-12-06 08:38:37.610706: step 39310, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.653 sec/batch; 53h:12m:44s remains)
INFO - root - 2017-12-06 08:38:44.152409: step 39320, loss = 0.88, batch loss = 0.67 (12.4 examples/sec; 0.646 sec/batch; 52h:35m:16s remains)
INFO - root - 2017-12-06 08:38:50.632517: step 39330, loss = 0.86, batch loss = 0.64 (12.1 examples/sec; 0.663 sec/batch; 53h:57m:12s remains)
INFO - root - 2017-12-06 08:38:57.197588: step 39340, loss = 0.76, batch loss = 0.55 (11.5 examples/sec; 0.693 sec/batch; 56h:26m:15s remains)
INFO - root - 2017-12-06 08:39:03.747329: step 39350, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 52h:59m:29s remains)
INFO - root - 2017-12-06 08:39:10.042353: step 39360, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.651 sec/batch; 53h:00m:22s remains)
INFO - root - 2017-12-06 08:39:16.516434: step 39370, loss = 0.81, batch loss = 0.60 (13.5 examples/sec; 0.593 sec/batch; 48h:15m:48s remains)
INFO - root - 2017-12-06 08:39:23.007136: step 39380, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.645 sec/batch; 52h:32m:54s remains)
INFO - root - 2017-12-06 08:39:29.467394: step 39390, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.673 sec/batch; 54h:47m:43s remains)
INFO - root - 2017-12-06 08:39:35.924002: step 39400, loss = 0.79, batch loss = 0.58 (12.6 examples/sec; 0.634 sec/batch; 51h:36m:17s remains)
2017-12-06 08:39:36.519340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.53385794 -0.68309718 -0.85954219 -1.1866452 -1.5356698 -1.7792201 -1.9015833 -1.8041881 -1.6313167 -1.4942794 -1.3260434 -1.2100888 -1.0868753 -1.0460691 -0.86133987][-1.2753184 -1.3248004 -1.4097548 -1.5890597 -1.8350211 -2.0826857 -2.3251584 -2.4584489 -2.5061398 -2.4041545 -2.2172556 -1.9138572 -1.5569566 -1.3868724 -1.179028][-2.001034 -1.7702379 -1.6853421 -1.8571697 -2.073772 -2.3629811 -2.6944356 -2.9832728 -3.2205861 -3.2237058 -2.9750147 -2.4834049 -2.0252306 -1.6630229 -1.4098654][-2.2112384 -1.9633189 -1.6264578 -1.5261083 -1.5607784 -1.8107752 -2.1164954 -2.5406208 -3.0156226 -3.3607545 -3.3244457 -2.845372 -2.2815619 -1.7808951 -1.3844711][-1.7656144 -1.5861862 -1.3399353 -1.0808208 -0.70186579 -0.53054309 -0.63140947 -1.0539153 -1.5360516 -2.1129076 -2.5025616 -2.4892888 -2.0732408 -1.5609947 -1.1639776][-0.92749655 -0.68421787 -0.49893066 -0.33145508 0.15502414 0.75613958 1.2702074 1.1374317 0.52079296 -0.31235644 -0.77251977 -0.98835838 -1.0260683 -0.84414077 -0.57389528][-0.35619122 -0.02999042 0.33992314 0.73123568 1.2066001 1.7453604 2.5079527 2.8835633 2.7615008 1.8001043 0.82365078 0.35389414 0.18081832 0.21984607 0.28265592][0.066265292 0.26614341 0.57323551 1.123426 1.7340299 2.413 3.1086278 3.3045182 3.3420138 2.6921439 1.8990357 1.2582237 0.79806882 0.62853652 0.54728591][0.1032645 0.2672464 0.54044056 0.95202839 1.4818988 2.197983 2.8802738 3.1325431 2.9980974 2.2556808 1.5103599 0.98443985 0.60993224 0.41934183 0.42860022][-0.14852561 -0.077971257 0.037839063 0.25330862 0.62147671 1.2121731 1.7692562 2.0188229 1.9357145 1.4256878 0.72679806 0.17215386 -0.19077304 -0.24808449 -0.011922367][-0.44052103 -0.47377536 -0.53503585 -0.561833 -0.42528564 -0.19985662 0.14999416 0.4326168 0.48514196 0.17057326 -0.22746247 -0.52703893 -0.75247496 -0.69308817 -0.47856107][-0.78491163 -0.84316397 -0.99669206 -1.140098 -1.2322091 -1.2306006 -1.0740705 -0.99638104 -0.89297819 -0.9388234 -0.99364966 -1.078759 -1.0514348 -0.85537469 -0.58078259][-0.87461925 -1.0288666 -1.1873797 -1.3236561 -1.4357193 -1.5191954 -1.5799379 -1.5778563 -1.5252979 -1.4563465 -1.364764 -1.2454647 -1.0754665 -0.88851321 -0.55733627][-0.71937644 -0.92265838 -1.1498709 -1.3211439 -1.4378948 -1.4927138 -1.5300843 -1.4957552 -1.4499732 -1.3636634 -1.2401923 -1.0159718 -0.84696674 -0.70624405 -0.44436815][-0.595059 -0.75953287 -0.94609439 -1.1306905 -1.2743297 -1.3264891 -1.3004394 -1.2976232 -1.2320881 -1.0866737 -0.91133326 -0.75437725 -0.56261772 -0.40164807 -0.25125194]]...]
INFO - root - 2017-12-06 08:39:43.086280: step 39410, loss = 0.97, batch loss = 0.76 (12.1 examples/sec; 0.660 sec/batch; 53h:44m:26s remains)
INFO - root - 2017-12-06 08:39:49.601512: step 39420, loss = 0.78, batch loss = 0.57 (11.7 examples/sec; 0.685 sec/batch; 55h:45m:07s remains)
INFO - root - 2017-12-06 08:39:56.078070: step 39430, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.652 sec/batch; 53h:03m:05s remains)
INFO - root - 2017-12-06 08:40:02.605503: step 39440, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.653 sec/batch; 53h:07m:49s remains)
INFO - root - 2017-12-06 08:40:08.978997: step 39450, loss = 0.82, batch loss = 0.60 (12.6 examples/sec; 0.633 sec/batch; 51h:29m:41s remains)
INFO - root - 2017-12-06 08:40:15.431533: step 39460, loss = 0.86, batch loss = 0.64 (12.2 examples/sec; 0.654 sec/batch; 53h:16m:00s remains)
INFO - root - 2017-12-06 08:40:21.917892: step 39470, loss = 0.96, batch loss = 0.74 (12.0 examples/sec; 0.667 sec/batch; 54h:15m:23s remains)
INFO - root - 2017-12-06 08:40:28.438121: step 39480, loss = 0.97, batch loss = 0.76 (11.7 examples/sec; 0.682 sec/batch; 55h:29m:01s remains)
INFO - root - 2017-12-06 08:40:34.942184: step 39490, loss = 0.83, batch loss = 0.62 (12.6 examples/sec; 0.636 sec/batch; 51h:47m:00s remains)
INFO - root - 2017-12-06 08:40:41.264029: step 39500, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.640 sec/batch; 52h:03m:14s remains)
2017-12-06 08:40:41.929183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061752066 -0.063233256 -0.065940678 -0.069376782 -0.072629735 -0.074793212 -0.075375386 -0.074045055 -0.071378119 -0.068257011 -0.065208733 -0.061911628 -0.058679163 -0.056363955 -0.056059163][-0.062006921 -0.064990327 -0.069628954 -0.075370081 -0.08114662 -0.0853428 -0.08571215 -0.081582762 -0.074848175 -0.068537429 -0.064928941 -0.062603146 -0.060512282 -0.058176138 -0.057585742][-0.062416244 -0.0663428 -0.072115123 -0.078449667 -0.085393764 -0.09316919 -0.097503006 -0.093131952 -0.082606412 -0.071060739 -0.063086711 -0.060440179 -0.060596943 -0.060462847 -0.059493534][-0.063272379 -0.067332506 -0.072684795 -0.07725054 -0.081471637 -0.087120764 -0.095249504 -0.097306021 -0.092750482 -0.0840026 -0.074249506 -0.068904266 -0.065680735 -0.064218968 -0.062392093][-0.063769169 -0.067419738 -0.071597293 -0.074128352 -0.07544937 -0.077677496 -0.083438322 -0.086625934 -0.085956492 -0.084140472 -0.079106674 -0.075399451 -0.070592061 -0.067235351 -0.064532615][-0.062705107 -0.06575869 -0.067757711 -0.06696336 -0.063026607 -0.058604728 -0.060111269 -0.06302534 -0.06163675 -0.062875949 -0.061944611 -0.064376265 -0.06466049 -0.06564334 -0.064789444][-0.060126863 -0.062910065 -0.063651383 -0.0611316 -0.053626705 -0.045797821 -0.046384763 -0.053119816 -0.054318149 -0.057547491 -0.055959158 -0.057906203 -0.05973212 -0.063792579 -0.064634651][-0.060863525 -0.062443372 -0.06341996 -0.06175521 -0.054377034 -0.04747593 -0.04821194 -0.053529724 -0.053837609 -0.05921286 -0.059168939 -0.059838437 -0.060238536 -0.063072689 -0.064301379][-0.062895074 -0.0633603 -0.063756853 -0.062756114 -0.058969069 -0.053465385 -0.050466802 -0.054125004 -0.055511 -0.059652459 -0.061399773 -0.061680138 -0.0615707 -0.063495226 -0.064346984][-0.063886583 -0.06369178 -0.063924283 -0.063499734 -0.06243635 -0.059701063 -0.058090717 -0.059551023 -0.059924562 -0.061693452 -0.06275966 -0.063340813 -0.063499376 -0.064029738 -0.064515509][-0.064544991 -0.064017683 -0.064002536 -0.063498884 -0.062854648 -0.062676355 -0.062721983 -0.06342338 -0.063496396 -0.063799232 -0.064143464 -0.0643067 -0.064252384 -0.064625606 -0.064626656][-0.064498328 -0.063775577 -0.063801244 -0.063193165 -0.062037975 -0.062688544 -0.063941807 -0.064511076 -0.0648497 -0.06425339 -0.064146444 -0.064085938 -0.064458981 -0.0648879 -0.064807937][-0.0641461 -0.063521296 -0.063636884 -0.062696658 -0.06163156 -0.061224274 -0.063966751 -0.0643722 -0.065363728 -0.064703584 -0.064784974 -0.064459011 -0.064497605 -0.064776294 -0.065084279][-0.064006977 -0.06377469 -0.063862965 -0.06356667 -0.0630515 -0.062118262 -0.064245135 -0.064304374 -0.064714722 -0.064241417 -0.06471315 -0.06424281 -0.064091749 -0.06425532 -0.064758912][-0.063622177 -0.063178591 -0.063523382 -0.063754141 -0.06372305 -0.063839927 -0.064083368 -0.064044513 -0.0640428 -0.064010181 -0.064243689 -0.063959733 -0.06395451 -0.063859925 -0.064339124]]...]
INFO - root - 2017-12-06 08:40:48.496721: step 39510, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.656 sec/batch; 53h:24m:00s remains)
INFO - root - 2017-12-06 08:40:55.015177: step 39520, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.654 sec/batch; 53h:14m:46s remains)
INFO - root - 2017-12-06 08:41:01.495898: step 39530, loss = 0.85, batch loss = 0.63 (11.7 examples/sec; 0.681 sec/batch; 55h:25m:54s remains)
INFO - root - 2017-12-06 08:41:07.962022: step 39540, loss = 0.81, batch loss = 0.60 (11.8 examples/sec; 0.678 sec/batch; 55h:10m:16s remains)
INFO - root - 2017-12-06 08:41:14.555877: step 39550, loss = 0.80, batch loss = 0.58 (11.6 examples/sec; 0.687 sec/batch; 55h:54m:37s remains)
INFO - root - 2017-12-06 08:41:20.991587: step 39560, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 52h:14m:00s remains)
INFO - root - 2017-12-06 08:41:27.490797: step 39570, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.632 sec/batch; 51h:23m:45s remains)
INFO - root - 2017-12-06 08:41:33.976048: step 39580, loss = 0.85, batch loss = 0.64 (12.7 examples/sec; 0.632 sec/batch; 51h:26m:31s remains)
INFO - root - 2017-12-06 08:41:40.541802: step 39590, loss = 0.87, batch loss = 0.66 (12.6 examples/sec; 0.633 sec/batch; 51h:32m:04s remains)
INFO - root - 2017-12-06 08:41:46.990076: step 39600, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 52h:17m:40s remains)
2017-12-06 08:41:47.640656: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0051693097 0.079806708 0.48750103 0.95754504 0.81784052 -0.0030791387 -1.1389523 -2.1997418 -2.895807 -2.635251 -2.5745225 -2.0135744 -1.4707946 -1.7098392 -2.1470923][1.2874392 0.73281872 0.36088532 0.84118485 1.1553167 0.85848576 -0.18280455 -1.23363 -1.966984 -2.9381583 -3.6054475 -3.2138891 -3.0234489 -2.2459767 -1.4110264][1.9560292 1.833521 0.8751663 0.00059984624 -1.0386862 -0.47287741 -0.073478125 -1.0308166 -2.0226243 -1.996778 -1.9900006 -1.8852254 -1.2597301 -0.73680329 -0.64695352][1.5662699 0.22000185 -0.74068648 -1.2495457 -1.9406058 -2.9778478 -3.7824221 -3.5844338 -3.3175404 -3.434526 -3.647656 -2.7819397 -1.5608085 -0.71044356 -0.6363163][1.7024405 1.2565829 0.58406007 0.2891646 -0.043268051 -0.639253 -0.72510028 -0.79447144 -0.93653876 -0.54581565 0.066189475 0.16042921 0.03818237 0.050082259 -0.49836057][1.8992946 1.6900944 1.698889 1.0251244 0.53866011 0.31983054 0.15236652 0.72828746 1.4481922 2.1134267 1.7912208 2.5321133 3.0168171 2.184094 1.2941686][1.4886733 1.4153545 1.329435 1.3164258 1.0707198 0.51973748 1.0106398 1.5132923 1.6918912 2.5547745 3.9567773 4.291966 4.2610316 3.5929754 3.1183348][2.9683776 2.6792936 1.7407413 1.2805376 0.764746 0.9971801 1.7477808 2.464844 3.6991544 3.9967358 4.2923689 4.2790112 4.6391363 3.7132735 3.0343943][1.3571695 1.3253714 1.5009364 1.0094665 0.72834939 0.92156112 1.6752765 2.5743444 3.4323058 4.5376744 5.8627281 5.7241335 5.4117389 4.4998803 3.8918097][-0.46306497 -0.34608063 -0.64508975 -0.58177161 0.1233454 0.505816 0.56361777 0.845684 1.3088921 1.9110157 2.9055822 4.2023873 4.477694 3.7304542 3.3441019][-2.7414463 -2.4543879 -2.1322041 -1.5761137 -1.3546263 -0.70416492 0.33553511 0.5988791 0.53568763 0.41269749 0.26399493 0.053965427 0.40432277 0.85225409 0.86351085][-2.046468 -2.0991609 -2.3720448 -2.0465605 -0.97742778 -0.42741263 -0.14445123 0.56091642 0.81484354 0.22688043 -0.54583913 -1.4544376 -1.8044002 -1.6975173 -0.36353084][-0.88577676 -0.53491592 -0.66469389 -0.96282971 -1.191722 -1.2708899 -1.234179 -1.1863911 -1.5432651 -2.1382694 -2.8904116 -3.4805257 -3.8543837 -3.552434 -2.9505079][0.3196941 0.87012333 0.90781629 0.81287777 1.2447063 0.67040968 -0.93969119 -1.6491512 -2.1399214 -3.5662487 -4.6952968 -5.3046765 -5.8130059 -5.4795804 -4.5046554][1.4968666 1.6862091 1.8337038 1.2332624 0.75773627 0.48588967 -0.044210821 -1.0451764 -2.4670675 -3.8054807 -4.8489313 -5.33801 -5.4288459 -5.3001451 -5.3748183]]...]
INFO - root - 2017-12-06 08:41:54.141542: step 39610, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 53h:17m:50s remains)
INFO - root - 2017-12-06 08:42:00.687780: step 39620, loss = 0.79, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 52h:20m:17s remains)
INFO - root - 2017-12-06 08:42:07.203094: step 39630, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.655 sec/batch; 53h:16m:32s remains)
INFO - root - 2017-12-06 08:42:13.519844: step 39640, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.641 sec/batch; 52h:08m:26s remains)
INFO - root - 2017-12-06 08:42:20.033391: step 39650, loss = 0.86, batch loss = 0.65 (12.6 examples/sec; 0.634 sec/batch; 51h:34m:14s remains)
INFO - root - 2017-12-06 08:42:26.531626: step 39660, loss = 0.80, batch loss = 0.58 (11.6 examples/sec; 0.691 sec/batch; 56h:12m:56s remains)
INFO - root - 2017-12-06 08:42:33.050772: step 39670, loss = 0.81, batch loss = 0.60 (12.7 examples/sec; 0.628 sec/batch; 51h:05m:06s remains)
INFO - root - 2017-12-06 08:42:39.647495: step 39680, loss = 0.79, batch loss = 0.57 (12.0 examples/sec; 0.668 sec/batch; 54h:18m:33s remains)
INFO - root - 2017-12-06 08:42:46.163219: step 39690, loss = 0.76, batch loss = 0.55 (12.4 examples/sec; 0.644 sec/batch; 52h:23m:57s remains)
INFO - root - 2017-12-06 08:42:52.669924: step 39700, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.652 sec/batch; 53h:00m:39s remains)
2017-12-06 08:42:53.296837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059181906 -0.058173742 -0.057675473 -0.057335012 -0.057165861 -0.057118442 -0.057074584 -0.057072721 -0.056975164 -0.056829248 -0.056689639 -0.056679927 -0.056908503 -0.05731786 -0.057982396][-0.0585636 -0.057320781 -0.056639552 -0.05617027 -0.055903647 -0.055802859 -0.055742532 -0.055659924 -0.05551853 -0.055324491 -0.055173602 -0.055200171 -0.055533096 -0.056194749 -0.05709463][-0.057526 -0.05607431 -0.055240162 -0.054699596 -0.054404244 -0.05430704 -0.054232467 -0.054115478 -0.053958271 -0.053756762 -0.053588916 -0.053649884 -0.05413558 -0.0550605 -0.056198925][-0.056320511 -0.054676473 -0.053668872 -0.052985568 -0.052566655 -0.052359104 -0.052226003 -0.052078679 -0.051898412 -0.051740039 -0.051701643 -0.051983181 -0.052758709 -0.053984378 -0.055516187][-0.039543424 -0.033368114 -0.030490261 -0.028760083 -0.02651976 -0.023953699 -0.02227626 -0.021410652 -0.020431444 -0.019440167 -0.022533424 -0.030166619 -0.039082613 -0.046347462 -0.051528286][-0.039121754 -0.033060621 -0.031494822 -0.031705655 -0.030252289 -0.027286299 -0.024004653 -0.0203792 -0.015381061 -0.010358408 -0.011858322 -0.020672455 -0.032722805 -0.042785686 -0.049790349][-0.06038649 -0.056164414 -0.054802358 -0.055911463 -0.056054734 -0.054132786 -0.049663778 -0.04427097 -0.039468296 -0.034567662 -0.034049749 -0.038402155 -0.045761772 -0.05171803 -0.055640966][-0.072708048 -0.070266955 -0.0678714 -0.067373574 -0.068381749 -0.06887193 -0.067326121 -0.06555067 -0.062916636 -0.059075374 -0.057289671 -0.057392515 -0.059083134 -0.06033653 -0.061241914][-0.07367643 -0.073271096 -0.072179794 -0.072108947 -0.072993837 -0.074801438 -0.076897971 -0.07920216 -0.078997493 -0.075248986 -0.070877507 -0.067562766 -0.066007249 -0.064722821 -0.063702695][-0.0686758 -0.067064837 -0.065720633 -0.06475994 -0.064697556 -0.066112593 -0.067386545 -0.068605989 -0.068393528 -0.065994538 -0.064079672 -0.064656042 -0.066369355 -0.066564471 -0.065315187][-0.072321326 -0.071680687 -0.071291737 -0.0710639 -0.071323827 -0.072243869 -0.073739171 -0.075680755 -0.077514723 -0.078147411 -0.077799566 -0.077202596 -0.07614994 -0.073974788 -0.071574643][-0.074373432 -0.07410872 -0.073923245 -0.073949158 -0.074760854 -0.076753139 -0.079743952 -0.083230339 -0.086279824 -0.088108614 -0.087973751 -0.085745841 -0.082271218 -0.07815598 -0.074589968][-0.073525392 -0.073304258 -0.073064834 -0.072767712 -0.072938323 -0.073565878 -0.07478445 -0.076458462 -0.07781221 -0.078468472 -0.078274272 -0.077612832 -0.07624197 -0.073832355 -0.07108701][-0.070695937 -0.070392326 -0.070171289 -0.06983453 -0.069837347 -0.070161819 -0.07084208 -0.07189 -0.072965942 -0.073845826 -0.07418175 -0.073876634 -0.072523423 -0.070183173 -0.067542486][-0.067690395 -0.067347959 -0.067219816 -0.066946447 -0.066886269 -0.067064196 -0.067472726 -0.068077162 -0.068732791 -0.06934879 -0.069733694 -0.069682553 -0.069021538 -0.067794159 -0.066347942]]...]
INFO - root - 2017-12-06 08:42:59.745330: step 39710, loss = 0.71, batch loss = 0.50 (12.8 examples/sec; 0.627 sec/batch; 50h:57m:24s remains)
INFO - root - 2017-12-06 08:43:06.326627: step 39720, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.643 sec/batch; 52h:19m:41s remains)
INFO - root - 2017-12-06 08:43:12.916492: step 39730, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.629 sec/batch; 51h:08m:56s remains)
INFO - root - 2017-12-06 08:43:19.327610: step 39740, loss = 0.90, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 52h:13m:34s remains)
INFO - root - 2017-12-06 08:43:25.807709: step 39750, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.657 sec/batch; 53h:25m:22s remains)
INFO - root - 2017-12-06 08:43:32.289809: step 39760, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.655 sec/batch; 53h:16m:12s remains)
INFO - root - 2017-12-06 08:43:38.705317: step 39770, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.652 sec/batch; 53h:01m:01s remains)
INFO - root - 2017-12-06 08:43:45.180117: step 39780, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.647 sec/batch; 52h:34m:51s remains)
INFO - root - 2017-12-06 08:43:51.729228: step 39790, loss = 0.79, batch loss = 0.58 (12.6 examples/sec; 0.634 sec/batch; 51h:33m:43s remains)
INFO - root - 2017-12-06 08:43:58.305981: step 39800, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 52h:42m:32s remains)
2017-12-06 08:43:58.920471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.9932228 -2.0537479 -2.1347682 -2.1563909 -2.1490691 -2.3543124 -2.6000433 -2.5908358 -2.5223114 -2.1268063 -1.7779248 -1.0966208 -0.47719944 0.11703524 0.54033923][-2.6676579 -2.8735223 -2.8511372 -2.6875439 -2.4964242 -2.4726536 -2.5091033 -2.7146859 -3.0495906 -3.1383114 -3.2834845 -2.8199008 -2.3358989 -1.8422799 -1.6116892][-2.6920497 -3.0519567 -3.1956434 -3.0053611 -2.5323575 -2.1549616 -1.8728597 -2.0828738 -2.391696 -2.8987362 -3.369355 -3.6639619 -3.8010912 -3.3897612 -3.1009851][-2.0306211 -2.5117049 -2.6767187 -2.735579 -2.6122284 -2.1051953 -1.4521835 -1.2074071 -1.1775668 -1.6250856 -2.0445876 -2.5837095 -2.978512 -3.4976752 -3.8550682][-1.1364605 -1.506615 -1.6806602 -1.7746171 -1.7012342 -1.5505944 -1.2714051 -0.90093064 -0.59318674 -0.5914495 -0.51851225 -0.835258 -1.1248823 -1.5660591 -1.9532588][-0.20122942 -0.3608188 -0.56921583 -0.63284469 -0.5024212 -0.38352066 -0.12409511 0.12067878 0.27979839 0.41500044 0.45900118 0.51032907 0.77518958 0.41758859 0.098039225][0.52415538 0.53721076 0.59150493 0.52225226 0.42034197 0.4686076 0.57824534 0.68031627 0.7643044 1.0184827 1.3215253 1.6217946 1.8944838 1.8503412 2.0032055][0.87744343 1.0580338 1.1688675 1.1588333 1.1467975 0.972935 0.743283 0.74806666 0.782684 0.85188448 1.0804614 1.8627927 2.5882571 2.7161288 2.9115565][0.35771793 0.60828918 0.84583241 0.91744167 1.0094188 0.94536942 0.75827205 0.505469 0.20888238 0.30685365 0.5213716 1.3329076 1.8165085 2.2180936 2.8288655][-0.24227691 -0.11353047 0.0025084019 0.090470821 0.30514729 0.28959376 0.14144143 -0.27486262 -0.67227656 -1.0094736 -1.0733635 -0.3053748 0.25285989 0.85158592 1.2637987][-0.64672339 -0.75692695 -0.86019367 -0.8888756 -0.83850229 -0.74235237 -0.87657362 -1.5228118 -1.9635798 -2.3194022 -2.5158901 -2.1731672 -1.9001346 -1.3261566 -0.81251186][-0.65901512 -0.86920011 -1.2168107 -1.4976563 -1.7634743 -1.9062485 -2.2216377 -2.7035623 -2.8369148 -3.1333768 -3.2592578 -3.1752949 -3.1011214 -2.7938306 -2.6103148][-0.38589752 -0.6479634 -0.97877634 -1.3541543 -1.8996218 -2.4318202 -2.9928446 -3.2963805 -3.535399 -3.7904058 -3.71959 -3.7622173 -3.801378 -3.8145034 -3.7338266][-0.16438371 -0.38775837 -0.7086516 -1.0819283 -1.5839213 -2.160306 -2.9331462 -3.3851428 -3.77205 -3.8337948 -3.6710157 -3.6744819 -3.6240897 -3.7630599 -3.9905541][-0.13614056 -0.27928543 -0.44215703 -0.75800449 -1.2378803 -1.7223667 -2.2824624 -2.7782509 -3.3594418 -3.6640198 -3.6428378 -3.59609 -3.3773992 -3.3302944 -3.3738143]]...]
INFO - root - 2017-12-06 08:44:05.416497: step 39810, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.657 sec/batch; 53h:22m:52s remains)
INFO - root - 2017-12-06 08:44:11.949429: step 39820, loss = 0.75, batch loss = 0.54 (12.7 examples/sec; 0.629 sec/batch; 51h:08m:16s remains)
INFO - root - 2017-12-06 08:44:18.525776: step 39830, loss = 0.83, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 53h:52m:26s remains)
INFO - root - 2017-12-06 08:44:24.891909: step 39840, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 53h:11m:11s remains)
INFO - root - 2017-12-06 08:44:31.405790: step 39850, loss = 0.75, batch loss = 0.53 (12.3 examples/sec; 0.650 sec/batch; 52h:49m:50s remains)
INFO - root - 2017-12-06 08:44:37.906446: step 39860, loss = 0.85, batch loss = 0.63 (12.1 examples/sec; 0.663 sec/batch; 53h:54m:56s remains)
INFO - root - 2017-12-06 08:44:44.369095: step 39870, loss = 0.83, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 53h:13m:06s remains)
INFO - root - 2017-12-06 08:44:50.926441: step 39880, loss = 0.88, batch loss = 0.67 (12.4 examples/sec; 0.647 sec/batch; 52h:35m:53s remains)
INFO - root - 2017-12-06 08:44:57.331332: step 39890, loss = 0.84, batch loss = 0.62 (12.4 examples/sec; 0.643 sec/batch; 52h:17m:33s remains)
INFO - root - 2017-12-06 08:45:03.838533: step 39900, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.632 sec/batch; 51h:22m:09s remains)
2017-12-06 08:45:04.489227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11062209 -0.10170346 -0.10195344 -0.14313607 -0.18225795 -0.19925138 -0.22122642 -0.22930527 -0.23721313 -0.24367434 -0.25986153 -0.27673882 -0.30094594 -0.32295603 -0.33557063][-0.08039172 -0.040294506 -0.0091383234 -0.01441212 -0.027851909 -0.056770407 -0.085358992 -0.084535845 -0.062224176 -0.065344468 -0.078141421 -0.1077031 -0.13772322 -0.14640155 -0.15764609][-0.11347574 -0.077158205 -0.022848241 0.0008052662 0.0071967319 -0.005387269 -0.0072311386 -0.0099083558 0.03867048 0.064622089 0.072593823 0.048875943 0.015627064 -0.015379362 -0.038290516][-0.11982387 -0.12810625 -0.090534024 -0.04554997 -0.020401463 -0.029205363 -0.043182682 -0.043530364 0.010868125 0.062445849 0.10835357 0.11866616 0.11651132 0.094425336 0.075136572][-0.16221344 -0.16808392 -0.14559838 -0.094468541 -0.07763201 -0.053989235 -0.04120791 -0.064199552 -0.069049507 -0.032497153 0.012118064 0.046785906 0.080562145 0.083734185 0.085473433][-0.0915181 -0.10403555 -0.10946666 -0.0920704 -0.10550474 -0.10962985 -0.096062757 -0.06655702 -0.0523605 -0.050738636 -0.044269864 -0.025158569 -0.00043486059 0.015642367 0.027790107][0.15497588 0.14820433 0.093293995 0.052929059 0.019618802 0.0087150261 0.0081387013 0.023964211 0.0341051 0.041112274 0.033920072 0.0049132928 -0.028297603 -0.034991022 -0.037299879][0.27407417 0.26521197 0.21706599 0.16389418 0.12020876 0.090241566 0.067945629 0.069774523 0.069163561 0.052910388 0.0291178 -0.0010030046 -0.03327271 -0.059331544 -0.07650169][0.21015748 0.17882687 0.12510984 0.07885313 0.05032061 0.031363189 0.0045539066 -0.019626871 -0.042581134 -0.052421648 -0.06539122 -0.08181683 -0.10029124 -0.11705441 -0.12997919][0.04762648 0.018755823 -0.026269875 -0.067461096 -0.080232695 -0.074772991 -0.076903157 -0.081868052 -0.098026194 -0.11239695 -0.12717108 -0.12950073 -0.13077708 -0.13807911 -0.14490651][-0.067016132 -0.098241821 -0.13229465 -0.15406431 -0.14630735 -0.13454628 -0.13170801 -0.12689769 -0.13384867 -0.13692626 -0.14644718 -0.15065926 -0.15499514 -0.15267873 -0.14965513][-0.13565153 -0.15062341 -0.1545852 -0.15250979 -0.13434044 -0.11915211 -0.11165093 -0.10998336 -0.11712632 -0.12715608 -0.14414939 -0.15522479 -0.16446626 -0.16230123 -0.15457857][-0.17333238 -0.17674753 -0.17059574 -0.15818079 -0.14540663 -0.14093727 -0.14287835 -0.14488518 -0.14285953 -0.13902912 -0.1375825 -0.14007398 -0.14580408 -0.14540941 -0.14059561][-0.17291526 -0.17651084 -0.17321479 -0.16188014 -0.15312755 -0.14806405 -0.14688888 -0.14919709 -0.14966324 -0.15157029 -0.15048243 -0.14890464 -0.14796257 -0.14233856 -0.13174455][-0.14185148 -0.1501925 -0.15355878 -0.15105636 -0.14641666 -0.14106543 -0.13641354 -0.13559201 -0.13888592 -0.14845707 -0.15767096 -0.16471702 -0.16783887 -0.16032192 -0.14746317]]...]
INFO - root - 2017-12-06 08:45:11.009082: step 39910, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.673 sec/batch; 54h:39m:34s remains)
INFO - root - 2017-12-06 08:45:17.422938: step 39920, loss = 0.79, batch loss = 0.57 (12.3 examples/sec; 0.648 sec/batch; 52h:38m:53s remains)
INFO - root - 2017-12-06 08:45:23.938008: step 39930, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.645 sec/batch; 52h:26m:26s remains)
INFO - root - 2017-12-06 08:45:30.323179: step 39940, loss = 0.84, batch loss = 0.62 (12.1 examples/sec; 0.661 sec/batch; 53h:43m:58s remains)
INFO - root - 2017-12-06 08:45:36.831246: step 39950, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 51h:33m:04s remains)
INFO - root - 2017-12-06 08:45:43.301139: step 39960, loss = 0.77, batch loss = 0.55 (12.4 examples/sec; 0.646 sec/batch; 52h:27m:38s remains)
INFO - root - 2017-12-06 08:45:49.804494: step 39970, loss = 0.88, batch loss = 0.66 (12.7 examples/sec; 0.628 sec/batch; 50h:59m:38s remains)
INFO - root - 2017-12-06 08:45:56.398739: step 39980, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.645 sec/batch; 52h:22m:35s remains)
INFO - root - 2017-12-06 08:46:02.932713: step 39990, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.646 sec/batch; 52h:29m:16s remains)
INFO - root - 2017-12-06 08:46:09.374752: step 40000, loss = 0.84, batch loss = 0.62 (12.0 examples/sec; 0.667 sec/batch; 54h:09m:13s remains)
2017-12-06 08:46:09.965410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080714874 -0.084677 -0.092721038 -0.0967723 -0.12055006 -0.12559812 -0.14481872 -0.14585263 -0.16115919 -0.15914017 -0.17281449 -0.18544529 -0.18801391 -0.1160944 -0.011510015][-0.076819435 -0.084127426 -0.10100188 -0.11196738 -0.1344977 -0.13047895 -0.14577705 -0.12875611 -0.11656069 -0.054176714 -0.013247944 0.023904845 -0.00010313839 -0.0037420541 0.029113196][-0.06901262 -0.069635913 -0.073226929 -0.082969844 -0.092002288 -0.077634573 -0.076086923 -0.028608385 0.013265721 0.12535679 0.23331216 0.35205656 0.31159943 0.30794871 0.24016729][-0.066533916 -0.04498855 -0.018742196 0.0038230717 0.061387815 0.11891486 0.15697578 0.18380454 0.23388338 0.34882912 0.50878453 0.68088031 0.7485286 0.72824824 0.51382977][-0.064026713 -0.028814033 0.043004788 0.12390999 0.25798067 0.38679948 0.50056863 0.55788606 0.58024663 0.63634688 0.77338815 0.9761194 1.1837002 1.1643572 0.99625146][-0.059903879 -0.0027609169 0.11272249 0.2843183 0.49566275 0.70624685 0.90467435 1.0054736 1.0529305 1.0581738 1.0705864 1.2292529 1.5053087 1.4733274 1.456458][-0.064545736 0.0024096891 0.14311427 0.35804868 0.59800947 0.83988875 1.0666535 1.1990666 1.2893533 1.313357 1.2993133 1.2795727 1.397341 1.4454604 1.4324373][-0.072879896 -0.021328196 0.10872743 0.30306226 0.49249935 0.69484735 0.8597275 0.9531554 1.0142205 1.062199 1.0786042 1.0971047 0.94941425 0.93582869 0.86831623][-0.08628948 -0.062107719 0.022356167 0.16393262 0.30151352 0.3931483 0.429758 0.4556703 0.44255865 0.44476026 0.495355 0.5523594 0.41233349 0.34235653 0.30120519][-0.093484268 -0.096510686 -0.072950631 -0.024900138 0.033623092 0.084641345 0.068548165 0.00010802597 -0.078871518 -0.15316775 -0.15789293 -0.081642106 -0.072697729 -0.059679367 -0.26434857][-0.08895079 -0.10584959 -0.13319685 -0.16971858 -0.20713805 -0.22724 -0.27235487 -0.35401881 -0.48413038 -0.56598872 -0.62315273 -0.61631507 -0.65203124 -0.47814819 -0.51747662][-0.085054733 -0.096266523 -0.13692479 -0.21299574 -0.30219832 -0.38138264 -0.42755866 -0.48783508 -0.58323383 -0.71088761 -0.77955717 -0.83772647 -0.92111611 -0.82876807 -0.71419382][-0.083706744 -0.08722499 -0.11491981 -0.17681953 -0.26632613 -0.35531339 -0.4088473 -0.43840629 -0.48547944 -0.58781272 -0.71906251 -0.79587036 -0.93077183 -0.98995852 -1.0281951][-0.082852609 -0.083910473 -0.094448134 -0.12680005 -0.1861746 -0.25038487 -0.29367232 -0.31076205 -0.32548869 -0.38122371 -0.49246383 -0.62329441 -0.74224603 -0.90375859 -1.0562147][-0.082334436 -0.082236879 -0.084135287 -0.089281633 -0.10667707 -0.13000892 -0.15884954 -0.16938153 -0.17583621 -0.20630938 -0.26092148 -0.38252994 -0.50725353 -0.67212647 -0.86020863]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-12-6fromscratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 08:46:17.424086: step 40010, loss = 0.69, batch loss = 0.48 (11.9 examples/sec; 0.673 sec/batch; 54h:41m:36s remains)
INFO - root - 2017-12-06 08:46:23.953643: step 40020, loss = 0.77, batch loss = 0.55 (12.5 examples/sec; 0.641 sec/batch; 52h:05m:18s remains)
INFO - root - 2017-12-06 08:46:30.392124: step 40030, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.667 sec/batch; 54h:12m:50s remains)
INFO - root - 2017-12-06 08:46:36.833926: step 40040, loss = 0.75, batch loss = 0.53 (12.9 examples/sec; 0.619 sec/batch; 50h:14m:53s remains)
INFO - root - 2017-12-06 08:46:43.289076: step 40050, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 51h:50m:47s remains)
INFO - root - 2017-12-06 08:46:49.641786: step 40060, loss = 0.86, batch loss = 0.65 (11.3 examples/sec; 0.708 sec/batch; 57h:30m:06s remains)
INFO - root - 2017-12-06 08:46:56.143640: step 40070, loss = 0.78, batch loss = 0.57 (12.0 examples/sec; 0.665 sec/batch; 53h:59m:02s remains)
INFO - root - 2017-12-06 08:47:02.659514: step 40080, loss = 0.86, batch loss = 0.64 (11.8 examples/sec; 0.679 sec/batch; 55h:07m:30s remains)
INFO - root - 2017-12-06 08:47:09.162700: step 40090, loss = 0.92, batch loss = 0.71 (11.6 examples/sec; 0.692 sec/batch; 56h:14m:03s remains)
INFO - root - 2017-12-06 08:47:15.571545: step 40100, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.639 sec/batch; 51h:54m:46s remains)
2017-12-06 08:47:16.178219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.047720652 -0.038561516 -0.03926412 -0.042100944 -0.047950447 -0.051919349 -0.053123392 -0.052430026 -0.051471196 -0.053848032 -0.056889948 -0.057674143 -0.05705468 -0.054340936 -0.051524948][-0.058208443 -0.050144859 -0.0480618 -0.047440805 -0.049972482 -0.052732643 -0.053367477 -0.052152976 -0.049328871 -0.053531278 -0.060553811 -0.062479183 -0.061838429 -0.056933235 -0.051945649][-0.062178742 -0.05579754 -0.053311255 -0.052428577 -0.052890792 -0.053214721 -0.05235479 -0.0499162 -0.045139968 -0.050332785 -0.063426524 -0.066922128 -0.06742686 -0.0609824 -0.053701174][-0.073120154 -0.061207272 -0.055291969 -0.056308739 -0.055898592 -0.053188626 -0.050514206 -0.046043418 -0.037243482 -0.043651596 -0.059207805 -0.063766643 -0.071063392 -0.065320045 -0.057782311][-0.10754421 -0.085429817 -0.069798529 -0.061727028 -0.060369812 -0.053997878 -0.048016742 -0.042101786 -0.031350639 -0.038616188 -0.055741418 -0.062902294 -0.077851728 -0.071665227 -0.0625427][-0.14481726 -0.11514555 -0.095068023 -0.07298056 -0.067860425 -0.060386147 -0.0462882 -0.036387172 -0.029841162 -0.048160784 -0.069440089 -0.073476672 -0.0902612 -0.080362596 -0.070075177][-0.16566229 -0.14139172 -0.11710551 -0.085508607 -0.075303584 -0.0629066 -0.044387862 -0.029010076 -0.022027366 -0.050019633 -0.073972747 -0.080089308 -0.10103227 -0.085707724 -0.0746219][-0.17068824 -0.15409191 -0.13416635 -0.095804915 -0.079612978 -0.0663038 -0.039765228 -0.022403724 -0.018226355 -0.049231622 -0.07326965 -0.087557584 -0.11560606 -0.0920889 -0.0770062][-0.1641489 -0.15149719 -0.13910544 -0.10353488 -0.079013027 -0.063321844 -0.032264527 -0.011950828 -0.012099981 -0.039651796 -0.063073337 -0.082255207 -0.11298601 -0.089401752 -0.074483559][-0.1822457 -0.16900228 -0.1453861 -0.10974504 -0.072714396 -0.052283883 -0.030745901 -0.013772652 -0.02136004 -0.043197572 -0.057466622 -0.0735523 -0.10026725 -0.0799448 -0.068964213][-0.21085161 -0.19609651 -0.16254623 -0.12120947 -0.081657022 -0.049487177 -0.0355188 -0.026673682 -0.040316507 -0.057354212 -0.058384851 -0.068117231 -0.090986282 -0.077547692 -0.068169147][-0.23788899 -0.21265927 -0.19043624 -0.14492598 -0.10993315 -0.075906783 -0.057611175 -0.041541841 -0.050116561 -0.059970722 -0.060815565 -0.066051587 -0.076691337 -0.068108544 -0.062797844][-0.26825514 -0.22459009 -0.19175875 -0.14168128 -0.11248346 -0.078335166 -0.064962611 -0.045991611 -0.047078446 -0.05529996 -0.059967771 -0.069174439 -0.065667421 -0.060116578 -0.055030558][-0.22588652 -0.19246936 -0.1631031 -0.12231073 -0.10433465 -0.079569422 -0.062041819 -0.049517181 -0.049086411 -0.05471972 -0.057571534 -0.06490463 -0.071463548 -0.06419012 -0.058000926][-0.1675972 -0.15488392 -0.13971874 -0.11756547 -0.10594888 -0.082477257 -0.060679942 -0.058170535 -0.06660454 -0.07108742 -0.072605304 -0.072440036 -0.072179064 -0.064719088 -0.058497392]]...]
INFO - root - 2017-12-06 08:47:22.687110: step 40110, loss = 1.01, batch loss = 0.80 (11.9 examples/sec; 0.675 sec/batch; 54h:46m:59s remains)
INFO - root - 2017-12-06 08:47:29.116795: step 40120, loss = 0.86, batch loss = 0.64 (12.3 examples/sec; 0.652 sec/batch; 52h:57m:13s remains)
INFO - root - 2017-12-06 08:47:35.601127: step 40130, loss = 0.83, batch loss = 0.62 (12.0 examples/sec; 0.667 sec/batch; 54h:12m:05s remains)
INFO - root - 2017-12-06 08:47:42.086358: step 40140, loss = 0.85, batch loss = 0.64 (12.1 examples/sec; 0.662 sec/batch; 53h:47m:35s remains)
INFO - root - 2017-12-06 08:47:48.593251: step 40150, loss = 0.75, batch loss = 0.53 (12.5 examples/sec; 0.640 sec/batch; 51h:56m:38s remains)
INFO - root - 2017-12-06 08:47:55.102029: step 40160, loss = 0.75, batch loss = 0.54 (11.7 examples/sec; 0.682 sec/batch; 55h:21m:46s remains)
INFO - root - 2017-12-06 08:48:01.595814: step 40170, loss = 0.86, batch loss = 0.65 (12.1 examples/sec; 0.660 sec/batch; 53h:34m:32s remains)
INFO - root - 2017-12-06 08:48:08.177359: step 40180, loss = 0.75, batch loss = 0.54 (12.4 examples/sec; 0.644 sec/batch; 52h:15m:40s remains)
INFO - root - 2017-12-06 08:48:14.796709: step 40190, loss = 0.89, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 52h:31m:28s remains)
INFO - root - 2017-12-06 08:48:21.229310: step 40200, loss = 0.81, batch loss = 0.60 (13.0 examples/sec; 0.617 sec/batch; 50h:06m:22s remains)
2017-12-06 08:48:21.877274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.073668987 -0.073188156 -0.072830513 -0.072429314 -0.072053194 -0.071785316 -0.071599275 -0.071474656 -0.071401417 -0.071313523 -0.071229473 -0.071129322 -0.071031421 -0.070966884 -0.071244121][-0.073969163 -0.073609322 -0.073492095 -0.07331948 -0.073041439 -0.072684184 -0.072399691 -0.072212696 -0.0720489 -0.07179895 -0.071512222 -0.071250074 -0.071079634 -0.0709927 -0.0711067][-0.074192695 -0.074135467 -0.07440117 -0.074591056 -0.074497312 -0.073992312 -0.073405437 -0.072992928 -0.072730869 -0.0723833 -0.071859345 -0.071370423 -0.071084455 -0.070953295 -0.070965849][-0.074538276 -0.074864022 -0.07565216 -0.076400116 -0.076636113 -0.075748935 -0.074494846 -0.073679969 -0.073331952 -0.072981969 -0.072321996 -0.071623795 -0.071183547 -0.070936814 -0.0709065][-0.074919321 -0.075683683 -0.077166341 -0.078630634 -0.079222642 -0.077710278 -0.075379007 -0.073740914 -0.073368177 -0.073397338 -0.072833292 -0.071981221 -0.071321279 -0.070939712 -0.070940375][-0.075324267 -0.076495647 -0.078696311 -0.081010796 -0.082081139 -0.080002621 -0.07602331 -0.072599888 -0.071894579 -0.073092058 -0.0731662 -0.072238237 -0.071433529 -0.070966363 -0.071014315][-0.075582184 -0.077131063 -0.07995712 -0.083116971 -0.084957443 -0.082911909 -0.077292748 -0.071275756 -0.070134908 -0.072527774 -0.073285893 -0.072308578 -0.071467 -0.070989713 -0.071139783][-0.07577756 -0.077525288 -0.08074829 -0.0845473 -0.087179214 -0.085898444 -0.0797039 -0.072183877 -0.070295841 -0.072984509 -0.07386715 -0.072842255 -0.071853489 -0.071286023 -0.071526244][-0.075820118 -0.077685215 -0.081082135 -0.085128337 -0.0881995 -0.087990344 -0.08248473 -0.074760966 -0.072079971 -0.073966824 -0.074604593 -0.0737361 -0.072858527 -0.072183862 -0.072420284][-0.075728573 -0.077527881 -0.080915719 -0.084947221 -0.088007435 -0.0887025 -0.084698007 -0.077678956 -0.074351862 -0.0752804 -0.075558543 -0.074933365 -0.074428052 -0.073981568 -0.074144445][-0.075558007 -0.077111952 -0.080194771 -0.084015392 -0.086961024 -0.088165581 -0.085818842 -0.080667078 -0.077720322 -0.077810846 -0.077641219 -0.077021472 -0.076732546 -0.076566964 -0.07691545][-0.075314343 -0.076532543 -0.079232953 -0.08267162 -0.085367277 -0.086760469 -0.085705176 -0.082731411 -0.080893844 -0.0805877 -0.080421329 -0.079815783 -0.079459563 -0.079100944 -0.079187706][-0.075053811 -0.0759499 -0.07818947 -0.081102565 -0.083501875 -0.084943824 -0.08486592 -0.083839029 -0.083136916 -0.082980588 -0.082883053 -0.082032219 -0.080573462 -0.078890055 -0.077859692][-0.074917391 -0.0754677 -0.077236123 -0.079631157 -0.081816778 -0.083302751 -0.083899222 -0.084069878 -0.084365174 -0.084793866 -0.084818371 -0.083244868 -0.079984225 -0.076721936 -0.074303478][-0.074945427 -0.07518243 -0.076507948 -0.0784487 -0.080488458 -0.0821491 -0.083226077 -0.084190622 -0.085412949 -0.08660154 -0.087148808 -0.085747577 -0.082157135 -0.07818529 -0.074799053]]...]
INFO - root - 2017-12-06 08:48:28.222744: step 40210, loss = 0.75, batch loss = 0.54 (14.1 examples/sec; 0.568 sec/batch; 46h:05m:56s remains)
INFO - root - 2017-12-06 08:48:34.742909: step 40220, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 52h:07m:26s remains)
INFO - root - 2017-12-06 08:48:41.305810: step 40230, loss = 0.80, batch loss = 0.59 (12.1 examples/sec; 0.662 sec/batch; 53h:44m:27s remains)
INFO - root - 2017-12-06 08:48:47.787803: step 40240, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 53h:31m:07s remains)
INFO - root - 2017-12-06 08:48:54.276696: step 40250, loss = 0.79, batch loss = 0.57 (12.6 examples/sec; 0.634 sec/batch; 51h:27m:29s remains)
INFO - root - 2017-12-06 08:49:00.763035: step 40260, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 52h:08m:02s remains)
INFO - root - 2017-12-06 08:49:07.216392: step 40270, loss = 0.76, batch loss = 0.54 (11.8 examples/sec; 0.676 sec/batch; 54h:51m:56s remains)
INFO - root - 2017-12-06 08:49:13.701646: step 40280, loss = 0.84, batch loss = 0.63 (13.0 examples/sec; 0.613 sec/batch; 49h:46m:59s remains)
INFO - root - 2017-12-06 08:49:20.130828: step 40290, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.638 sec/batch; 51h:48m:40s remains)
INFO - root - 2017-12-06 08:49:26.552975: step 40300, loss = 0.95, batch loss = 0.74 (12.3 examples/sec; 0.648 sec/batch; 52h:37m:55s remains)
2017-12-06 08:49:27.156192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051402856 -0.050583251 -0.049980793 -0.0495392 -0.049438395 -0.049798984 -0.050614808 -0.051721778 -0.052873749 -0.053940065 -0.054864407 -0.055573624 -0.055999827 -0.0562114 -0.056553785][-0.052776609 -0.051779952 -0.051051836 -0.050401602 -0.050034504 -0.050137963 -0.05078673 -0.051842004 -0.052986577 -0.054071929 -0.055048514 -0.055806413 -0.056270331 -0.056522738 -0.056841966][-0.054486655 -0.053343348 -0.052521344 -0.051676396 -0.051003385 -0.050763946 -0.051148742 -0.052034639 -0.053070579 -0.054108735 -0.0551069 -0.055934 -0.056452584 -0.056775436 -0.057137825][-0.055973504 -0.054947969 -0.054117303 -0.053146079 -0.052248675 -0.051698014 -0.051757675 -0.052366853 -0.053199861 -0.054125275 -0.055096064 -0.055938315 -0.056509767 -0.056927681 -0.0573919][-0.05711529 -0.056308772 -0.055591073 -0.054617036 -0.053586077 -0.052805651 -0.052604981 -0.052923828 -0.053491842 -0.054217435 -0.055061802 -0.05585213 -0.056459077 -0.056976676 -0.057580791][-0.057948206 -0.057266291 -0.056707013 -0.055820681 -0.054746866 -0.0538192 -0.053423885 -0.053499661 -0.053833634 -0.054359742 -0.05507144 -0.055779908 -0.056376409 -0.056947138 -0.057659935][-0.058474142 -0.057835825 -0.057444312 -0.056707203 -0.055700462 -0.054687716 -0.054134369 -0.05403734 -0.054197636 -0.054583296 -0.055146493 -0.055769846 -0.0563191 -0.056875069 -0.057650276][-0.058746874 -0.058136925 -0.057936359 -0.057425067 -0.05657696 -0.055574488 -0.054890808 -0.054606937 -0.054577563 -0.054776445 -0.055196885 -0.055729873 -0.056231417 -0.056744535 -0.057584237][-0.058837853 -0.058282379 -0.058234613 -0.057941064 -0.057302758 -0.056400422 -0.055570614 -0.055032495 -0.054769196 -0.054769531 -0.055036791 -0.055473857 -0.055970829 -0.056475516 -0.057328377][-0.058781404 -0.058288079 -0.058323625 -0.058172148 -0.057712473 -0.056945138 -0.056042869 -0.055288628 -0.054778617 -0.054584321 -0.054697212 -0.05502706 -0.055513032 -0.05603895 -0.056927312][-0.058760297 -0.058227185 -0.058288325 -0.058224488 -0.057912681 -0.057292923 -0.056421615 -0.055543795 -0.054815527 -0.054427356 -0.054386947 -0.054594912 -0.055028606 -0.055563293 -0.056511454][-0.058700107 -0.058154803 -0.058207046 -0.05818937 -0.057983071 -0.057496678 -0.056704037 -0.055806689 -0.054947417 -0.05437338 -0.054168306 -0.054278988 -0.054658134 -0.055188593 -0.056168135][-0.058630288 -0.0581059 -0.058157422 -0.05814242 -0.057992931 -0.057616122 -0.056917127 -0.056048177 -0.055150513 -0.054490861 -0.054179054 -0.054214027 -0.054536212 -0.055008195 -0.055899758][-0.058588989 -0.058077876 -0.058136474 -0.058130439 -0.058013484 -0.057728991 -0.057144668 -0.056340415 -0.055454705 -0.054751113 -0.054373477 -0.054335229 -0.054588202 -0.054974645 -0.055705052][-0.058654658 -0.058141969 -0.058214478 -0.058234856 -0.058172941 -0.057996713 -0.05758182 -0.05694307 -0.056187086 -0.055500619 -0.0550918 -0.054967985 -0.055066232 -0.055321708 -0.055862129]]...]
INFO - root - 2017-12-06 08:49:33.589601: step 40310, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.644 sec/batch; 52h:16m:16s remains)
INFO - root - 2017-12-06 08:49:40.087742: step 40320, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.646 sec/batch; 52h:23m:53s remains)
INFO - root - 2017-12-06 08:49:46.506584: step 40330, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.637 sec/batch; 51h:42m:57s remains)
INFO - root - 2017-12-06 08:49:52.990732: step 40340, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.652 sec/batch; 52h:54m:50s remains)
INFO - root - 2017-12-06 08:49:59.445633: step 40350, loss = 0.80, batch loss = 0.58 (12.0 examples/sec; 0.668 sec/batch; 54h:14m:43s remains)
INFO - root - 2017-12-06 08:50:06.018361: step 40360, loss = 0.84, batch loss = 0.63 (11.8 examples/sec; 0.677 sec/batch; 54h:58m:17s remains)
INFO - root - 2017-12-06 08:50:12.563446: step 40370, loss = 0.86, batch loss = 0.65 (12.4 examples/sec; 0.644 sec/batch; 52h:15m:37s remains)
INFO - root - 2017-12-06 08:50:19.084024: step 40380, loss = 0.87, batch loss = 0.66 (12.7 examples/sec; 0.629 sec/batch; 51h:04m:14s remains)
INFO - root - 2017-12-06 08:50:25.534593: step 40390, loss = 0.77, batch loss = 0.55 (12.8 examples/sec; 0.625 sec/batch; 50h:44m:42s remains)
INFO - root - 2017-12-06 08:50:31.963436: step 40400, loss = 0.72, batch loss = 0.51 (12.6 examples/sec; 0.635 sec/batch; 51h:32m:44s remains)
2017-12-06 08:50:32.581858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12425245 -0.12677126 -0.12842913 -0.12917252 -0.13025931 -0.13032937 -0.13053294 -0.13208227 -0.13395198 -0.13554144 -0.13685833 -0.13787025 -0.13685811 -0.13127239 -0.12793818][-0.10539747 -0.10644369 -0.10755205 -0.10745691 -0.10809581 -0.10856226 -0.10997809 -0.11090063 -0.11115377 -0.11279988 -0.11350046 -0.11369792 -0.11441072 -0.11034022 -0.10629809][-0.082816832 -0.080854073 -0.078149579 -0.077581964 -0.079755619 -0.080076143 -0.07965932 -0.079514824 -0.079506516 -0.079768091 -0.080017142 -0.080415413 -0.080817759 -0.078802876 -0.075897016][-0.066875137 -0.059596412 -0.055739488 -0.054555271 -0.054613523 -0.055386782 -0.056548081 -0.056869738 -0.056595884 -0.058193583 -0.059282165 -0.061209157 -0.0638213 -0.066773593 -0.070861675][-0.074239567 -0.068935513 -0.06406825 -0.060632847 -0.061773855 -0.060889192 -0.058730368 -0.058436632 -0.058187533 -0.057962533 -0.056574538 -0.054900788 -0.054235753 -0.054237131 -0.057349805][-0.05880091 -0.057773896 -0.058864411 -0.060559772 -0.062776625 -0.062683754 -0.062096626 -0.059888192 -0.056937031 -0.056239106 -0.055371802 -0.05312486 -0.0527203 -0.053041775 -0.05404152][-0.047549989 -0.049462114 -0.056018114 -0.065606661 -0.073978163 -0.077572264 -0.076952346 -0.0717545 -0.067630135 -0.062542692 -0.056817964 -0.054465447 -0.055176575 -0.056342576 -0.0569923][-0.040914882 -0.043021783 -0.047360305 -0.055855457 -0.064881079 -0.072834477 -0.075525649 -0.073200531 -0.069371462 -0.063694991 -0.059689742 -0.055678908 -0.055085689 -0.057059057 -0.058801547][-0.043759838 -0.044997707 -0.048707191 -0.053447694 -0.058212765 -0.064131759 -0.066957183 -0.066112049 -0.063015431 -0.059888165 -0.05753639 -0.05482142 -0.054132059 -0.054278839 -0.055344705][-0.047360916 -0.047975343 -0.051130123 -0.055621389 -0.060059976 -0.064830244 -0.067846194 -0.067835875 -0.066294074 -0.063803151 -0.060934965 -0.05887159 -0.058121223 -0.058136452 -0.057635631][-0.058088265 -0.057558503 -0.057066631 -0.058815427 -0.06059587 -0.063584417 -0.06612853 -0.066837907 -0.068066888 -0.066881269 -0.064318731 -0.062105842 -0.061213695 -0.060816515 -0.059168756][-0.06537386 -0.064907551 -0.06477762 -0.065524265 -0.065792538 -0.068400294 -0.071210623 -0.074321195 -0.076901272 -0.077439934 -0.078317352 -0.077348664 -0.076141529 -0.075119659 -0.072248407][-0.082746588 -0.082284972 -0.081879042 -0.081227854 -0.080719218 -0.0802472 -0.0806349 -0.0827307 -0.085607871 -0.088769689 -0.090787366 -0.092013612 -0.093626589 -0.093264557 -0.091147088][-0.10114851 -0.10103832 -0.098908976 -0.096074484 -0.093200274 -0.09167733 -0.092296436 -0.091446124 -0.091901995 -0.095180452 -0.097135164 -0.09796533 -0.098395057 -0.097421139 -0.096814722][-0.095111065 -0.095606513 -0.093165092 -0.091684334 -0.091512322 -0.090223826 -0.090137124 -0.091582932 -0.094119519 -0.098161653 -0.10258617 -0.1043123 -0.10364714 -0.10241884 -0.10298125]]...]
INFO - root - 2017-12-06 08:50:39.051866: step 40410, loss = 0.76, batch loss = 0.54 (13.0 examples/sec; 0.617 sec/batch; 50h:03m:33s remains)
INFO - root - 2017-12-06 08:50:45.636659: step 40420, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.666 sec/batch; 54h:00m:07s remains)
INFO - root - 2017-12-06 08:50:52.105278: step 40430, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.629 sec/batch; 51h:02m:29s remains)
INFO - root - 2017-12-06 08:50:58.656720: step 40440, loss = 0.85, batch loss = 0.64 (12.1 examples/sec; 0.659 sec/batch; 53h:29m:16s remains)
INFO - root - 2017-12-06 08:51:05.075085: step 40450, loss = 0.80, batch loss = 0.59 (12.7 examples/sec; 0.630 sec/batch; 51h:08m:22s remains)
INFO - root - 2017-12-06 08:51:11.618289: step 40460, loss = 0.93, batch loss = 0.72 (12.4 examples/sec; 0.648 sec/batch; 52h:32m:34s remains)
INFO - root - 2017-12-06 08:51:18.248621: step 40470, loss = 0.78, batch loss = 0.56 (11.5 examples/sec; 0.696 sec/batch; 56h:28m:09s remains)
INFO - root - 2017-12-06 08:51:24.793235: step 40480, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.654 sec/batch; 53h:01m:56s remains)
INFO - root - 2017-12-06 08:51:31.225247: step 40490, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.642 sec/batch; 52h:03m:17s remains)
INFO - root - 2017-12-06 08:51:37.650205: step 40500, loss = 0.84, batch loss = 0.63 (12.1 examples/sec; 0.662 sec/batch; 53h:43m:46s remains)
2017-12-06 08:51:38.328277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0358863 -0.028040878 -0.026970461 -0.059590638 -0.065929383 -0.083789483 -0.05062991 -0.028515872 0.017478153 0.0411053 0.071878821 0.0965922 0.10681809 0.11991043 0.12286489][-0.090560779 -0.030280266 0.024924718 0.013690449 -0.0085374564 -0.0023365617 0.0078798905 0.011282049 0.020981811 0.025835469 0.0446824 0.055678308 0.081379279 0.097025707 0.11488672][-0.085390493 -0.017965563 0.044055268 0.061049193 0.01457385 -0.0059642196 -0.02909698 -0.053509422 -0.081310451 -0.075736612 -0.0601191 -0.045640673 -0.017665364 0.017525204 0.046878517][-0.090800874 -0.037826166 0.044840977 0.052825347 0.022559427 -0.005951412 -0.045100134 -0.08341594 -0.11400527 -0.099280566 -0.07562393 -0.047582928 -0.0068678036 0.024322495 0.046563953][-0.022214063 -0.022650421 0.0048628449 -0.015485339 -0.075489894 -0.11598875 -0.1388707 -0.15850717 -0.1689871 -0.16459614 -0.13765301 -0.10681448 -0.067881033 -0.038038235 -0.014985047][-0.046140127 -0.034203988 -0.015061907 -0.054945543 -0.12079877 -0.1843767 -0.22112736 -0.24452655 -0.25558931 -0.25133428 -0.23596449 -0.2293364 -0.20383571 -0.17517033 -0.14316162][-0.12215848 -0.11468848 -0.11657342 -0.13673715 -0.16900006 -0.19104859 -0.20165162 -0.20639065 -0.2363787 -0.24597903 -0.25550777 -0.267071 -0.26704454 -0.24926263 -0.2219184][-0.1987858 -0.2028695 -0.20451054 -0.18879461 -0.17938718 -0.15145147 -0.13926169 -0.12744598 -0.16004081 -0.18211472 -0.19848679 -0.20720616 -0.22692063 -0.20849769 -0.20105991][-0.1037574 -0.13107854 -0.12880714 -0.12456914 -0.093318261 -0.071937382 -0.049677629 -0.021374047 -0.046827782 -0.068112053 -0.090340637 -0.11262182 -0.14908031 -0.12104144 -0.10691248][-0.012944743 -0.027509809 -0.046399105 -0.053301506 -0.022563174 0.0011531115 0.033403553 0.080345184 0.12525193 0.12423985 0.097324178 0.068596929 0.0085057691 -0.0046332106 -0.0052941889][0.0899933 0.064908385 0.03164196 0.013331875 0.040315077 0.075321436 0.12764585 0.18934895 0.24001323 0.26736367 0.25223243 0.22170417 0.16963483 0.15475877 0.11811121][0.13497292 0.12767626 0.11658469 0.10555701 0.1124272 0.1533176 0.2039272 0.27147478 0.31958967 0.35354221 0.34013456 0.30805582 0.27044433 0.24756764 0.19949825][0.14900614 0.15832786 0.14891225 0.14824244 0.15533894 0.19243719 0.22462939 0.26843351 0.31343335 0.35429376 0.35897857 0.32854235 0.29243594 0.25774086 0.19317101][0.16948412 0.16604696 0.16247733 0.17430763 0.19234203 0.20262228 0.21179773 0.24212147 0.26819396 0.2814855 0.28350055 0.25652319 0.22670348 0.19649728 0.13818061][0.14421086 0.15620954 0.18410586 0.21626569 0.28390306 0.27534235 0.2755509 0.27667868 0.28494471 0.27552003 0.24308248 0.17576863 0.11064628 0.07339564 0.033485204]]...]
INFO - root - 2017-12-06 08:51:44.780927: step 40510, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.643 sec/batch; 52h:11m:22s remains)
INFO - root - 2017-12-06 08:51:51.263700: step 40520, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.657 sec/batch; 53h:15m:12s remains)
INFO - root - 2017-12-06 08:51:57.709486: step 40530, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.634 sec/batch; 51h:22m:44s remains)
INFO - root - 2017-12-06 08:52:04.285858: step 40540, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.665 sec/batch; 53h:56m:08s remains)
INFO - root - 2017-12-06 08:52:10.750853: step 40550, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.662 sec/batch; 53h:41m:49s remains)
INFO - root - 2017-12-06 08:52:17.264654: step 40560, loss = 0.81, batch loss = 0.59 (11.9 examples/sec; 0.670 sec/batch; 54h:21m:51s remains)
INFO - root - 2017-12-06 08:52:23.783129: step 40570, loss = 0.73, batch loss = 0.52 (12.5 examples/sec; 0.642 sec/batch; 52h:04m:11s remains)
INFO - root - 2017-12-06 08:52:30.306312: step 40580, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.643 sec/batch; 52h:07m:03s remains)
INFO - root - 2017-12-06 08:52:36.746194: step 40590, loss = 0.73, batch loss = 0.51 (12.2 examples/sec; 0.654 sec/batch; 53h:04m:09s remains)
INFO - root - 2017-12-06 08:52:43.281790: step 40600, loss = 0.86, batch loss = 0.64 (12.4 examples/sec; 0.646 sec/batch; 52h:22m:54s remains)
2017-12-06 08:52:43.926776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071251586 -0.071821943 -0.072762929 -0.073788494 -0.074738808 -0.075499758 -0.076052018 -0.076336242 -0.076323926 -0.075993977 -0.075493306 -0.074943073 -0.074465476 -0.074218884 -0.0742778][-0.0716676 -0.072298914 -0.073328115 -0.074501306 -0.0755839 -0.076447889 -0.077069782 -0.077414416 -0.077367432 -0.076853774 -0.076159224 -0.075481437 -0.074895106 -0.074495308 -0.074450791][-0.073001705 -0.073693052 -0.074870855 -0.076127134 -0.077317245 -0.078285553 -0.078938134 -0.079229467 -0.079009712 -0.078300096 -0.077400029 -0.076526217 -0.075815476 -0.075323105 -0.075215481][-0.074285507 -0.075102195 -0.0763896 -0.07775227 -0.079130478 -0.080203526 -0.080821648 -0.080984361 -0.080593586 -0.079680167 -0.078553505 -0.077531122 -0.076767847 -0.076293148 -0.076198027][-0.075267807 -0.076231807 -0.077677861 -0.07924439 -0.080791563 -0.081881084 -0.082376659 -0.08251173 -0.082102939 -0.081047259 -0.079752266 -0.078578204 -0.077726543 -0.077172361 -0.077069245][-0.076023519 -0.077064596 -0.078611687 -0.080283426 -0.0818357 -0.082725637 -0.082968563 -0.083065391 -0.0827934 -0.082035892 -0.080961645 -0.07979919 -0.078799725 -0.078046836 -0.077812538][-0.076372676 -0.07739006 -0.078993805 -0.080756567 -0.082305945 -0.083017223 -0.082788981 -0.082385279 -0.082290322 -0.082191981 -0.081671439 -0.080832206 -0.079908252 -0.078974172 -0.0785782][-0.076426469 -0.077233717 -0.078722641 -0.080443956 -0.082019642 -0.082759678 -0.08238142 -0.081960514 -0.082384907 -0.082671873 -0.082220338 -0.0814644 -0.080594793 -0.079680018 -0.079313844][-0.076532818 -0.077134266 -0.07846339 -0.080189906 -0.081750385 -0.082484193 -0.082287863 -0.082283437 -0.082843512 -0.082932979 -0.082297221 -0.08154159 -0.080723643 -0.079986729 -0.079754382][-0.076525763 -0.077090591 -0.078438617 -0.080288172 -0.081894383 -0.082807563 -0.083061546 -0.083251119 -0.083676428 -0.083517872 -0.082736388 -0.08184465 -0.081035644 -0.080272295 -0.080065474][-0.076631851 -0.077117823 -0.078267343 -0.079774648 -0.081251517 -0.082456984 -0.083078615 -0.083410069 -0.083558805 -0.083215527 -0.082432494 -0.0815413 -0.080792725 -0.080117509 -0.079973206][-0.0769819 -0.077271417 -0.077988975 -0.078965113 -0.080078945 -0.08104077 -0.081531718 -0.081762537 -0.08171659 -0.081449918 -0.081005841 -0.0803902 -0.079811066 -0.079304077 -0.079208344][-0.077670895 -0.077528358 -0.077755965 -0.078269422 -0.078977145 -0.079646796 -0.079933368 -0.079981685 -0.079869166 -0.079742327 -0.079465345 -0.079033948 -0.0785473 -0.078154236 -0.078195453][-0.07901182 -0.078363433 -0.078084782 -0.078137383 -0.078488536 -0.0789821 -0.079290219 -0.079447217 -0.079442039 -0.079278864 -0.078945406 -0.078526653 -0.078090273 -0.077703983 -0.077594213][-0.080463789 -0.079640046 -0.0790693 -0.078789942 -0.078820951 -0.079047859 -0.079293407 -0.079559848 -0.079690762 -0.079576857 -0.079197094 -0.078753166 -0.078245588 -0.077759847 -0.07751213]]...]
INFO - root - 2017-12-06 08:52:50.427854: step 40610, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.657 sec/batch; 53h:16m:23s remains)
INFO - root - 2017-12-06 08:52:56.913754: step 40620, loss = 0.75, batch loss = 0.54 (12.6 examples/sec; 0.634 sec/batch; 51h:21m:58s remains)
INFO - root - 2017-12-06 08:53:03.346665: step 40630, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.648 sec/batch; 52h:31m:33s remains)
INFO - root - 2017-12-06 08:53:09.917756: step 40640, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.659 sec/batch; 53h:23m:14s remains)
INFO - root - 2017-12-06 08:53:16.402776: step 40650, loss = 0.84, batch loss = 0.63 (12.3 examples/sec; 0.650 sec/batch; 52h:39m:38s remains)
INFO - root - 2017-12-06 08:53:22.911625: step 40660, loss = 0.85, batch loss = 0.63 (12.2 examples/sec; 0.654 sec/batch; 52h:58m:44s remains)
INFO - root - 2017-12-06 08:53:29.337828: step 40670, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.638 sec/batch; 51h:44m:41s remains)
INFO - root - 2017-12-06 08:53:35.735421: step 40680, loss = 0.87, batch loss = 0.66 (12.2 examples/sec; 0.658 sec/batch; 53h:19m:37s remains)
INFO - root - 2017-12-06 08:53:42.209080: step 40690, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.674 sec/batch; 54h:39m:55s remains)
INFO - root - 2017-12-06 08:53:48.730140: step 40700, loss = 0.89, batch loss = 0.67 (12.2 examples/sec; 0.654 sec/batch; 53h:01m:49s remains)
2017-12-06 08:53:49.342679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.58341563 -0.97104496 -1.569031 -2.2147181 -2.8662386 -3.2694013 -3.2971635 -2.9150932 -2.1803224 -1.5449511 -0.88930631 -0.58596295 -0.57521623 -0.85581762 -1.0606619][-1.3798246 -1.7557558 -2.4582732 -3.1786122 -3.6970351 -4.0600724 -4.3124604 -4.3130579 -3.8351572 -3.1241615 -2.1334641 -1.4464058 -1.1906874 -1.06344 -1.0661795][-1.706355 -1.7921367 -1.9049073 -2.6789868 -3.5036738 -4.1581135 -4.5002189 -4.7661285 -4.8523011 -4.4462047 -3.6177952 -2.8511126 -2.1334004 -1.6976047 -1.4779125][-2.06612 -1.970835 -1.88673 -1.8848487 -2.1485312 -2.8066664 -3.4232967 -3.9649837 -4.4861851 -4.6314692 -4.4443579 -3.7716551 -3.0478547 -2.7207575 -2.2655458][-1.9067761 -1.7552475 -1.5438491 -1.5551577 -1.6459824 -1.7209069 -1.9851542 -2.6588538 -3.1714182 -3.4069495 -3.378021 -3.1008415 -2.66868 -2.4441605 -2.2226727][-0.51820207 -0.213996 0.19769242 0.60033792 0.80457377 0.79285628 0.78897333 0.41143575 -0.17512986 -0.85037643 -1.1988406 -1.3408573 -1.1798322 -1.0498105 -0.9495939][0.64362633 0.96397972 1.3562304 1.7957585 2.2964342 2.8711283 3.2663646 3.3026483 3.0901911 2.325592 1.5391623 0.89374554 0.62969536 0.21093944 -0.079863168][0.82686812 1.1856073 1.6430269 2.1732049 2.7577288 3.3161144 3.7951505 4.17508 4.4255652 4.15188 3.3724687 2.4807727 1.9063588 1.2218409 0.72310036][0.35568774 0.45137528 0.7108925 1.0912434 1.559016 2.0689061 2.4845293 2.7795308 3.1204238 3.0520062 2.5824335 1.7963321 1.0980302 0.557973 0.29540423][-0.81483424 -0.89000344 -0.83857471 -0.74049014 -0.41769713 0.010323569 0.51098573 0.96818411 1.5414528 1.643072 1.3704802 1.0176394 0.45000097 0.0118903 -0.30785641][-1.5761323 -1.8626072 -1.9833817 -1.9510872 -1.8611037 -1.79267 -1.7174569 -1.6347924 -1.0981171 -0.74573332 -0.63088793 -0.5144859 -0.62961084 -0.7014358 -0.62174964][-1.4946275 -1.8368998 -2.1963003 -2.5000484 -2.6046574 -2.6280448 -2.6511817 -2.8758128 -3.0487983 -3.0443745 -2.6610677 -2.0131485 -1.4293848 -1.0643452 -0.89138442][-1.316998 -1.6992726 -2.0282197 -2.3600857 -2.6075203 -2.8664949 -3.0489261 -3.1831539 -3.0800238 -3.1473479 -3.0708179 -2.5565829 -2.100683 -1.5823327 -1.071851][-0.91845942 -1.3808184 -1.7775474 -2.1112847 -2.3807991 -2.6051536 -2.810981 -2.9829202 -2.9721916 -2.7845566 -2.5274656 -2.1909642 -1.8291942 -1.4444546 -0.93997097][-0.79656214 -1.13578 -1.4718549 -1.7963204 -2.0040002 -2.1487541 -2.2937951 -2.4017181 -2.4010105 -2.3046253 -2.0003028 -1.7328995 -1.4852368 -1.2597233 -0.94759709]]...]
INFO - root - 2017-12-06 08:53:55.896843: step 40710, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.673 sec/batch; 54h:33m:40s remains)
INFO - root - 2017-12-06 08:54:02.392762: step 40720, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.650 sec/batch; 52h:42m:42s remains)
INFO - root - 2017-12-06 08:54:08.908579: step 40730, loss = 0.82, batch loss = 0.60 (12.0 examples/sec; 0.669 sec/batch; 54h:12m:02s remains)
INFO - root - 2017-12-06 08:54:15.405864: step 40740, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.658 sec/batch; 53h:19m:24s remains)
INFO - root - 2017-12-06 08:54:21.929839: step 40750, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 51h:41m:27s remains)
INFO - root - 2017-12-06 08:54:28.498233: step 40760, loss = 0.74, batch loss = 0.52 (12.5 examples/sec; 0.640 sec/batch; 51h:53m:10s remains)
INFO - root - 2017-12-06 08:54:34.791575: step 40770, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.658 sec/batch; 53h:20m:55s remains)
INFO - root - 2017-12-06 08:54:41.295613: step 40780, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.658 sec/batch; 53h:19m:58s remains)
INFO - root - 2017-12-06 08:54:47.854445: step 40790, loss = 0.73, batch loss = 0.52 (12.3 examples/sec; 0.649 sec/batch; 52h:34m:19s remains)
INFO - root - 2017-12-06 08:54:54.399663: step 40800, loss = 0.82, batch loss = 0.60 (11.6 examples/sec; 0.688 sec/batch; 55h:46m:51s remains)
2017-12-06 08:54:54.976481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1051989 -1.2342161 -1.5630711 -1.7085599 -1.9825029 -2.3318172 -2.3875253 -2.3767786 -2.2360926 -1.909237 -1.6769993 -1.6877072 -1.5731062 -1.4391613 -1.4651972][-1.4282753 -1.4579904 -1.5723004 -1.5673759 -1.5906854 -1.6548837 -1.859923 -2.0193274 -2.1571515 -2.1754577 -2.0264778 -1.8294787 -1.4757413 -1.4670514 -1.536695][-1.4629738 -1.1015164 -1.1550773 -1.0665598 -1.0001724 -0.97623438 -1.1135674 -1.2861944 -1.1182429 -1.1233221 -1.527355 -1.837436 -1.7609032 -1.6066121 -1.3816473][-0.76693475 -0.831195 -0.8218621 -0.510544 0.024055265 0.31107849 0.37941915 0.35480216 0.13165179 -0.067212455 -0.38762373 -0.63601381 -0.90902919 -1.0077177 -1.234934][0.0825152 0.53438032 0.73927915 0.91906524 1.4806362 1.6167634 1.5927367 1.6793567 1.6746542 1.4620985 0.93953824 0.617725 0.10884992 -0.32343858 -0.750233][0.134464 0.49815318 1.2419559 1.5351378 2.0405824 2.1850388 2.1266606 2.1101792 2.0324426 1.8799812 1.58464 1.3487734 1.0310621 0.61144012 -0.0097193941][1.0009766 1.3954296 1.9316195 2.4852469 3.2327583 3.1357934 2.8394458 2.550298 2.2383735 1.7807219 1.2577965 0.93475652 0.57276261 0.24560389 -0.11410823][1.75881 2.17164 2.5290809 2.5411751 2.8624525 2.9052036 2.9731083 2.775121 2.3754816 1.896753 1.4237248 0.93567479 0.41140375 0.0742049 -0.26632112][1.8618079 1.737974 1.6001697 1.7073613 1.9438428 1.889343 1.8229688 1.797879 1.7994727 1.6099985 1.4116975 1.2028741 0.83479 0.26157248 -0.076654948][0.95344627 0.64696968 0.4394252 0.099356391 0.18937653 0.32087916 0.62333196 0.60111415 0.83995354 0.75058746 0.64773953 0.66232795 0.91757262 0.91361094 0.75527281][0.57444072 0.14123684 -0.52086669 -0.89552003 -0.93307889 -0.87158424 -0.49089193 -0.21988085 0.17752904 0.33655745 0.55009806 0.61478436 0.71341246 0.83605754 0.99851727][0.69827759 0.47072807 -0.032280106 -0.39938143 -0.85391134 -0.99963236 -1.2192347 -1.0974536 -0.77695215 -0.615314 -0.49642137 0.045610316 0.50061035 0.69939321 0.65817791][0.27918839 0.0877709 -0.25351426 -0.39868942 -1.1269205 -1.3262351 -1.8405635 -1.970155 -2.2034202 -2.0364127 -1.7469926 -1.2832313 -0.95941311 -0.64791715 -0.63106078][-0.10431794 -0.72885591 -0.9916743 -1.48006 -2.408165 -2.4857068 -2.7955785 -2.9148226 -2.9992886 -2.7691898 -2.4240363 -2.00662 -1.9480399 -1.6580319 -1.5836229][-0.24323055 -0.78035307 -1.2606894 -2.0551355 -2.9975843 -3.6694083 -3.8265743 -3.6481421 -3.4656675 -3.0654321 -2.6610732 -2.0905652 -1.762597 -1.3446643 -0.91140014]]...]
INFO - root - 2017-12-06 08:55:01.525013: step 40810, loss = 0.82, batch loss = 0.61 (11.8 examples/sec; 0.675 sec/batch; 54h:42m:06s remains)
INFO - root - 2017-12-06 08:55:08.045231: step 40820, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.652 sec/batch; 52h:50m:06s remains)
INFO - root - 2017-12-06 08:55:14.605412: step 40830, loss = 0.74, batch loss = 0.52 (11.9 examples/sec; 0.675 sec/batch; 54h:39m:40s remains)
INFO - root - 2017-12-06 08:55:21.111561: step 40840, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.644 sec/batch; 52h:11m:01s remains)
INFO - root - 2017-12-06 08:55:27.561878: step 40850, loss = 0.87, batch loss = 0.66 (13.3 examples/sec; 0.601 sec/batch; 48h:39m:27s remains)
INFO - root - 2017-12-06 08:55:34.055696: step 40860, loss = 0.86, batch loss = 0.65 (12.3 examples/sec; 0.650 sec/batch; 52h:37m:48s remains)
INFO - root - 2017-12-06 08:55:40.515849: step 40870, loss = 0.72, batch loss = 0.51 (12.0 examples/sec; 0.668 sec/batch; 54h:06m:30s remains)
INFO - root - 2017-12-06 08:55:47.035230: step 40880, loss = 0.85, batch loss = 0.63 (12.4 examples/sec; 0.647 sec/batch; 52h:22m:20s remains)
INFO - root - 2017-12-06 08:55:53.540974: step 40890, loss = 0.86, batch loss = 0.64 (12.1 examples/sec; 0.659 sec/batch; 53h:23m:55s remains)
INFO - root - 2017-12-06 08:56:00.053691: step 40900, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.656 sec/batch; 53h:08m:36s remains)
2017-12-06 08:56:00.642961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6495776 -2.0391936 -2.3495498 -2.8361025 -3.1524165 -3.0079644 -2.5517797 -2.0746641 -1.6497835 -1.5419127 -1.5657026 -1.5466405 -1.4805304 -1.3342966 -1.1087725][-2.4902864 -2.9805746 -3.4305038 -3.7999954 -4.1675096 -4.36912 -4.3292 -3.9512041 -3.3614366 -2.8910828 -2.4285574 -2.25882 -2.0801377 -1.7241805 -1.3360403][-2.7825818 -2.91295 -3.3375878 -3.9345002 -4.4899011 -4.8711624 -5.1280403 -5.0623131 -4.6927094 -4.1795282 -3.6206377 -3.1561942 -2.6006396 -2.1709909 -1.6738089][-2.9568892 -2.8821025 -2.8345823 -2.9275663 -3.2563114 -3.7864089 -4.4075794 -4.8010592 -4.9220657 -4.6496172 -4.0980678 -3.547015 -3.0233359 -2.4327309 -1.7300863][-2.5779796 -2.1403689 -1.89733 -1.5164732 -1.3339406 -1.5048642 -2.0228388 -2.7345591 -3.3457065 -3.5660641 -3.5202339 -3.1904857 -2.6827402 -2.202513 -1.7967298][-2.2989402 -1.4691447 -0.674696 -0.045922052 0.26151055 0.57780367 0.52304393 -0.063532844 -0.74013656 -1.2713597 -1.6393176 -1.7393651 -1.730476 -1.5405324 -1.2479025][-1.0429444 -0.1296531 0.98373723 2.0413587 2.66061 2.7157185 2.2623358 1.8205222 1.2722284 0.6330353 0.16936353 -0.18518656 -0.50902152 -0.55771345 -0.55625457][0.70279557 1.3216867 2.0783179 3.0085125 3.8314762 4.1306095 3.8612728 3.0720751 2.0479124 1.3621625 0.99311113 0.64158744 0.37329933 0.0927149 -0.15404218][1.4622365 1.7186265 2.1606972 2.6839833 2.9737084 3.2281823 3.2579038 2.9508529 2.4169064 1.6383537 0.95079267 0.6448192 0.55429715 0.33275273 0.15566638][0.4236103 0.52739739 0.95472431 1.270826 1.3232057 1.2559677 1.1258632 1.1063445 1.0759468 1.0603271 1.0231012 0.7603696 0.43971583 0.3427209 0.22425991][-1.0383915 -1.075664 -0.87825394 -0.76178235 -0.84539646 -0.85566932 -0.74903268 -0.55322093 -0.29248184 -0.019660056 0.28527603 0.51873219 0.60035557 0.43481776 0.14265308][-2.0079081 -2.1304986 -2.1510987 -2.1945822 -2.394912 -2.5352678 -2.4329588 -1.9847175 -1.4294707 -0.85227346 -0.35537639 -0.0032347739 0.21635669 0.37434128 0.43793145][-1.7899655 -1.9911247 -2.1582453 -2.5670097 -2.937752 -3.1069541 -3.0376191 -2.7285063 -2.19557 -1.5550461 -0.95333743 -0.48199663 -0.20537001 -0.037788764 0.08680322][-1.5747077 -1.8578833 -2.2104392 -2.590379 -2.7641642 -2.9563475 -3.042182 -2.7854989 -2.3836699 -1.9614944 -1.484303 -1.0468036 -0.73637289 -0.53959757 -0.39902255][-1.3148688 -1.7309294 -2.216387 -2.4785721 -2.5488846 -2.6112738 -2.54896 -2.3628359 -2.1939428 -2.0457149 -1.7679586 -1.4193171 -0.99636751 -0.66947609 -0.50880563]]...]
INFO - root - 2017-12-06 08:56:07.116938: step 40910, loss = 0.77, batch loss = 0.56 (11.9 examples/sec; 0.671 sec/batch; 54h:20m:13s remains)
INFO - root - 2017-12-06 08:56:13.718472: step 40920, loss = 0.84, batch loss = 0.63 (12.3 examples/sec; 0.648 sec/batch; 52h:27m:59s remains)
INFO - root - 2017-12-06 08:56:20.345866: step 40930, loss = 0.73, batch loss = 0.52 (12.0 examples/sec; 0.669 sec/batch; 54h:11m:47s remains)
INFO - root - 2017-12-06 08:56:27.000536: step 40940, loss = 0.77, batch loss = 0.55 (11.9 examples/sec; 0.671 sec/batch; 54h:19m:18s remains)
INFO - root - 2017-12-06 08:56:33.541763: step 40950, loss = 0.75, batch loss = 0.53 (12.2 examples/sec; 0.655 sec/batch; 53h:03m:33s remains)
INFO - root - 2017-12-06 08:56:39.955559: step 40960, loss = 0.88, batch loss = 0.66 (12.2 examples/sec; 0.653 sec/batch; 52h:53m:34s remains)
INFO - root - 2017-12-06 08:56:46.470958: step 40970, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.671 sec/batch; 54h:18m:31s remains)
INFO - root - 2017-12-06 08:56:52.979632: step 40980, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 53h:38m:19s remains)
INFO - root - 2017-12-06 08:56:59.503226: step 40990, loss = 0.75, batch loss = 0.54 (12.1 examples/sec; 0.663 sec/batch; 53h:40m:28s remains)
INFO - root - 2017-12-06 08:57:06.008261: step 41000, loss = 0.77, batch loss = 0.55 (11.7 examples/sec; 0.684 sec/batch; 55h:22m:52s remains)
2017-12-06 08:57:06.674597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04608703 -0.045602486 -0.045794308 -0.046304483 -0.046917662 -0.047243826 -0.047085941 -0.046521217 -0.046307713 -0.047283679 -0.049198456 -0.051463738 -0.053411569 -0.05496002 -0.056250196][-0.044492561 -0.044305343 -0.044738233 -0.045300011 -0.045838241 -0.046228044 -0.046429992 -0.046293255 -0.046376962 -0.04753273 -0.049537864 -0.05189497 -0.053742379 -0.055098612 -0.056197524][-0.044843182 -0.044716716 -0.045320358 -0.045788631 -0.045984097 -0.046193056 -0.046658583 -0.04699352 -0.047313731 -0.048408128 -0.050421648 -0.052757744 -0.05443744 -0.055501457 -0.056266829][-0.046157964 -0.045945641 -0.046507005 -0.046789605 -0.046868958 -0.047046654 -0.047849737 -0.048757445 -0.049343161 -0.050444193 -0.052100245 -0.054155175 -0.055488467 -0.056127992 -0.056476377][-0.047431991 -0.047219984 -0.047615144 -0.04784739 -0.048030723 -0.048479989 -0.049681798 -0.051138122 -0.052178659 -0.053327922 -0.054579325 -0.055954888 -0.056811891 -0.05689415 -0.056810163][-0.048517104 -0.048011471 -0.048184626 -0.048367403 -0.048807297 -0.049694803 -0.051328573 -0.053341478 -0.055025004 -0.056354988 -0.057133012 -0.057672828 -0.05790735 -0.057468295 -0.057182852][-0.049415402 -0.048496503 -0.048381604 -0.048456725 -0.049106058 -0.050392389 -0.0524869 -0.055029366 -0.057267651 -0.058655608 -0.05905097 -0.058935575 -0.058634926 -0.057965342 -0.057550326][-0.050198205 -0.048843324 -0.048400413 -0.048393503 -0.049146868 -0.050617196 -0.052848708 -0.0555171 -0.057818323 -0.05914674 -0.059476286 -0.059305854 -0.058890987 -0.058290672 -0.057833541][-0.050767906 -0.049233515 -0.048539 -0.048343495 -0.048903435 -0.050035767 -0.051905092 -0.054296769 -0.05650406 -0.057936907 -0.058391992 -0.058610793 -0.058510061 -0.058093224 -0.057800386][-0.051088024 -0.049709741 -0.049052984 -0.04875927 -0.048914384 -0.049405705 -0.050470803 -0.052161857 -0.054036468 -0.055484295 -0.056202926 -0.056891587 -0.057423182 -0.057467196 -0.057555925][-0.051272631 -0.050155364 -0.049730308 -0.049384732 -0.04912873 -0.048962668 -0.049213912 -0.05007286 -0.051336486 -0.052722529 -0.053863037 -0.055099625 -0.056223676 -0.056857657 -0.057356346][-0.051014025 -0.050232977 -0.050049875 -0.04970628 -0.049207702 -0.048634198 -0.048252486 -0.048359193 -0.049092934 -0.05042512 -0.051893394 -0.053568356 -0.055159431 -0.056299362 -0.057136662][-0.050440606 -0.04989019 -0.049846314 -0.049495358 -0.048882376 -0.048083726 -0.047352362 -0.04703946 -0.047463797 -0.048757616 -0.050493058 -0.052481271 -0.054354448 -0.055762805 -0.056797538][-0.049949683 -0.049498215 -0.049533524 -0.049216136 -0.048574768 -0.047734119 -0.0469276 -0.046427581 -0.046723671 -0.047988009 -0.04981412 -0.051911309 -0.053888012 -0.055409461 -0.056541033][-0.049935952 -0.04946648 -0.049537785 -0.049299028 -0.048747342 -0.048062362 -0.047437187 -0.047011878 -0.047255021 -0.04838391 -0.050105292 -0.052062012 -0.053896476 -0.055378579 -0.056550022]]...]
INFO - root - 2017-12-06 08:57:13.203999: step 41010, loss = 0.68, batch loss = 0.47 (12.0 examples/sec; 0.665 sec/batch; 53h:51m:28s remains)
INFO - root - 2017-12-06 08:57:19.779750: step 41020, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.640 sec/batch; 51h:47m:21s remains)
INFO - root - 2017-12-06 08:57:26.317349: step 41030, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 51h:20m:58s remains)
INFO - root - 2017-12-06 08:57:32.898344: step 41040, loss = 0.80, batch loss = 0.58 (11.8 examples/sec; 0.676 sec/batch; 54h:44m:15s remains)
INFO - root - 2017-12-06 08:57:39.439254: step 41050, loss = 0.87, batch loss = 0.66 (11.5 examples/sec; 0.696 sec/batch; 56h:21m:14s remains)
INFO - root - 2017-12-06 08:57:45.871950: step 41060, loss = 0.79, batch loss = 0.58 (11.8 examples/sec; 0.677 sec/batch; 54h:46m:18s remains)
INFO - root - 2017-12-06 08:57:52.393848: step 41070, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.641 sec/batch; 51h:53m:22s remains)
INFO - root - 2017-12-06 08:57:59.033956: step 41080, loss = 0.87, batch loss = 0.65 (12.3 examples/sec; 0.648 sec/batch; 52h:27m:41s remains)
INFO - root - 2017-12-06 08:58:05.648143: step 41090, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.642 sec/batch; 51h:58m:53s remains)
INFO - root - 2017-12-06 08:58:12.094956: step 41100, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.636 sec/batch; 51h:28m:10s remains)
2017-12-06 08:58:12.675506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.57916391 -0.65095556 -0.72726256 -0.82522112 -0.90979654 -1.0069964 -1.1007456 -1.1392559 -1.1208693 -1.0668137 -0.97601247 -0.88171077 -0.77022195 -0.62535816 -0.46463144][-0.9895308 -1.1065956 -1.1214658 -1.0787308 -0.99895906 -1.0003986 -1.1019012 -1.2004514 -1.244435 -1.2582848 -1.2212777 -1.1247021 -1.0244302 -0.90857756 -0.7635923][-1.2005259 -1.3487051 -1.3275543 -1.149088 -0.85004425 -0.59395605 -0.53716141 -0.63817906 -0.88661963 -1.1712279 -1.2951826 -1.2332921 -1.1445547 -0.96690577 -0.73378992][-1.034658 -1.1415539 -1.0513556 -0.73398083 -0.21708843 0.32140851 0.6699515 0.77383661 0.57695633 0.10626847 -0.366431 -0.682045 -0.81329036 -0.77701747 -0.62044591][-0.48805803 -0.53693688 -0.40163246 -0.0047405064 0.66460592 1.4406272 2.0917864 2.506268 2.5953548 2.2066648 1.5023762 0.83953977 0.40057042 0.11737021 -0.058041755][0.061520748 0.16666612 0.35666302 0.72688377 1.3726132 2.246001 3.19719 3.9582119 4.2422137 3.9829435 3.325099 2.5373034 1.8375686 1.2291657 0.73269916][0.2697238 0.47326413 0.76652628 1.1751604 1.7776669 2.5525594 3.4202964 4.1730962 4.5946379 4.5627756 4.0657945 3.3379109 2.5471468 1.7582905 1.1417383][0.10019464 0.28387469 0.50895858 0.89369428 1.4685566 2.1604142 2.9093025 3.550395 3.9515672 4.0525889 3.7437959 3.2035913 2.5261321 1.8084228 1.1837715][-0.44374141 -0.39275962 -0.27816 -0.10200309 0.1970188 0.69089657 1.2886978 1.8496296 2.286015 2.5797243 2.5352092 2.1660829 1.6544842 1.1641171 0.72621536][-1.0803455 -1.1766738 -1.2003173 -1.1625234 -1.1143775 -1.0070918 -0.75680929 -0.42159849 -0.030802283 0.41588774 0.62782073 0.74139935 0.56226921 0.29603273 0.086033873][-1.3804237 -1.7394339 -2.045311 -2.1948473 -2.2711761 -2.3408861 -2.3154163 -2.1429622 -1.8187312 -1.3070946 -0.88649565 -0.6393767 -0.57586157 -0.50861818 -0.46136785][-1.3091528 -1.7466105 -2.1556833 -2.557142 -2.918556 -3.1279204 -3.1220834 -2.9253526 -2.613759 -2.1982548 -1.748179 -1.3850603 -1.0894798 -0.866044 -0.64155555][-0.94843936 -1.3075153 -1.699632 -2.092356 -2.4280829 -2.6912138 -2.7303097 -2.6678076 -2.51651 -2.1380615 -1.6289061 -1.2745306 -0.92490286 -0.6662789 -0.56422943][-0.56791162 -0.820963 -1.1243651 -1.4061776 -1.6134518 -1.7855178 -1.7670559 -1.723137 -1.5884318 -1.4080828 -1.111155 -0.71859741 -0.43996727 -0.44447532 -0.31017998][-0.29955408 -0.42424294 -0.57337159 -0.73118287 -0.84306377 -0.947873 -0.94384575 -1.0091637 -0.8979246 -0.75480485 -0.54784626 -0.3570151 -0.1918551 -0.15896586 -0.069701016]]...]
INFO - root - 2017-12-06 08:58:19.232638: step 41110, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.659 sec/batch; 53h:21m:13s remains)
INFO - root - 2017-12-06 08:58:25.759999: step 41120, loss = 0.86, batch loss = 0.65 (12.5 examples/sec; 0.639 sec/batch; 51h:43m:12s remains)
INFO - root - 2017-12-06 08:58:32.305611: step 41130, loss = 0.92, batch loss = 0.71 (11.8 examples/sec; 0.677 sec/batch; 54h:47m:46s remains)
INFO - root - 2017-12-06 08:58:38.818903: step 41140, loss = 0.78, batch loss = 0.56 (12.7 examples/sec; 0.631 sec/batch; 51h:02m:10s remains)
INFO - root - 2017-12-06 08:58:45.271270: step 41150, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.656 sec/batch; 53h:06m:13s remains)
INFO - root - 2017-12-06 08:58:51.734757: step 41160, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.644 sec/batch; 52h:07m:15s remains)
INFO - root - 2017-12-06 08:58:58.320802: step 41170, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 51h:58m:51s remains)
INFO - root - 2017-12-06 08:59:04.853708: step 41180, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 51h:20m:03s remains)
INFO - root - 2017-12-06 08:59:11.221701: step 41190, loss = 0.82, batch loss = 0.60 (12.3 examples/sec; 0.650 sec/batch; 52h:35m:08s remains)
INFO - root - 2017-12-06 08:59:17.723672: step 41200, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 52h:01m:48s remains)
2017-12-06 08:59:18.336283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061794765 -0.061392039 -0.061237514 -0.061072558 -0.060906503 -0.060745612 -0.060592167 -0.060453255 -0.060349382 -0.060302153 -0.060298495 -0.060311761 -0.060331844 -0.060351938 -0.060494125][-0.061904117 -0.061457381 -0.061389513 -0.061312914 -0.061216556 -0.061086804 -0.060918577 -0.06072852 -0.06055348 -0.060436308 -0.060376875 -0.060357373 -0.060361862 -0.060376786 -0.060456209][-0.062113792 -0.061652206 -0.061756402 -0.061849937 -0.061886109 -0.061821688 -0.0616431 -0.061354533 -0.061027832 -0.060750991 -0.060562126 -0.060460955 -0.060412839 -0.060401544 -0.060411125][-0.062303185 -0.062000893 -0.062322848 -0.062643439 -0.062865511 -0.062888369 -0.062678471 -0.062235322 -0.061686091 -0.061165504 -0.060776003 -0.060548842 -0.060429595 -0.060390942 -0.060424574][-0.062506087 -0.062534571 -0.063145913 -0.063724786 -0.064143762 -0.064239934 -0.063970953 -0.063327089 -0.062490322 -0.061678685 -0.061053403 -0.060664259 -0.060457893 -0.060386304 -0.060468845][-0.06280411 -0.063075833 -0.0639821 -0.064822763 -0.065424792 -0.065605551 -0.065293945 -0.064485796 -0.063389309 -0.062283687 -0.06141711 -0.060879778 -0.060589172 -0.060476378 -0.060607202][-0.063144542 -0.0635367 -0.06466715 -0.065682486 -0.066383123 -0.066583082 -0.066232458 -0.065339454 -0.064140618 -0.062901169 -0.061866559 -0.06119813 -0.060819834 -0.060665563 -0.0608509][-0.063432992 -0.063890606 -0.06517823 -0.066303432 -0.067017242 -0.067134671 -0.066689424 -0.065736353 -0.064499795 -0.063240163 -0.062185504 -0.061491057 -0.061094023 -0.060938783 -0.061208133][-0.06339515 -0.063867643 -0.065228254 -0.06644661 -0.067235313 -0.067370437 -0.066866532 -0.065838993 -0.064575821 -0.063365854 -0.06241823 -0.061798289 -0.061459668 -0.061342143 -0.061654933][-0.063026071 -0.063389681 -0.06464725 -0.065844037 -0.066697143 -0.066955127 -0.066602513 -0.065743193 -0.064615533 -0.063528568 -0.062701672 -0.062187068 -0.061919678 -0.061838813 -0.062170796][-0.062858909 -0.062878288 -0.063836008 -0.0648063 -0.065575406 -0.065927133 -0.065787584 -0.065211341 -0.064391062 -0.063561313 -0.062898427 -0.062480394 -0.062252648 -0.062183551 -0.062538385][-0.062777832 -0.062534325 -0.063141942 -0.063808791 -0.064376846 -0.064683683 -0.0646835 -0.064378962 -0.063876711 -0.06333264 -0.062876 -0.0625554 -0.062359951 -0.062302768 -0.062671557][-0.06265489 -0.062194355 -0.062480226 -0.062848426 -0.063206173 -0.063435189 -0.063506693 -0.063410319 -0.063181721 -0.062897488 -0.062633209 -0.062421814 -0.062277935 -0.062239237 -0.06256552][-0.062567145 -0.061914206 -0.061951526 -0.062071703 -0.062234737 -0.062373418 -0.062467493 -0.062500954 -0.062452249 -0.062340282 -0.062210679 -0.062087931 -0.061992165 -0.061962865 -0.062220927][-0.062656149 -0.061915476 -0.061808512 -0.061755531 -0.061751954 -0.061779477 -0.061831936 -0.061889295 -0.06191048 -0.061865278 -0.061788164 -0.061697491 -0.06160564 -0.061567876 -0.061783068]]...]
INFO - root - 2017-12-06 08:59:24.848852: step 41210, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.642 sec/batch; 51h:58m:24s remains)
INFO - root - 2017-12-06 08:59:31.356306: step 41220, loss = 0.74, batch loss = 0.53 (12.0 examples/sec; 0.667 sec/batch; 53h:59m:06s remains)
INFO - root - 2017-12-06 08:59:37.889248: step 41230, loss = 0.76, batch loss = 0.55 (12.3 examples/sec; 0.650 sec/batch; 52h:37m:35s remains)
INFO - root - 2017-12-06 08:59:44.312478: step 41240, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.658 sec/batch; 53h:13m:10s remains)
INFO - root - 2017-12-06 08:59:50.820667: step 41250, loss = 0.80, batch loss = 0.58 (11.6 examples/sec; 0.689 sec/batch; 55h:46m:25s remains)
INFO - root - 2017-12-06 08:59:57.236103: step 41260, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.638 sec/batch; 51h:37m:01s remains)
INFO - root - 2017-12-06 09:00:03.677316: step 41270, loss = 0.70, batch loss = 0.49 (12.7 examples/sec; 0.632 sec/batch; 51h:06m:02s remains)
INFO - root - 2017-12-06 09:00:10.084670: step 41280, loss = 0.71, batch loss = 0.49 (12.8 examples/sec; 0.627 sec/batch; 50h:43m:57s remains)
INFO - root - 2017-12-06 09:00:16.622709: step 41290, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 51h:53m:31s remains)
INFO - root - 2017-12-06 09:00:23.109163: step 41300, loss = 0.94, batch loss = 0.73 (12.0 examples/sec; 0.666 sec/batch; 53h:51m:13s remains)
2017-12-06 09:00:23.691358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.3619988 -1.5441833 -1.6462758 -1.6726063 -1.6267451 -1.5439355 -1.4395292 -1.3153684 -1.1909448 -1.1118369 -0.99857646 -0.90853834 -0.82497466 -0.76795739 -0.76593274][-1.06986 -1.400968 -1.6842356 -1.8879775 -2.0281677 -2.1131263 -2.1164021 -2.0653732 -1.9455351 -1.7804357 -1.5528746 -1.3314096 -1.1359479 -1.0094969 -0.93938506][-0.29456288 -0.70554566 -1.1184363 -1.4897054 -1.8381606 -2.1385434 -2.4013388 -2.6000638 -2.5884693 -2.441844 -2.1565552 -1.7996683 -1.4433734 -1.1493144 -0.96706885][0.36778429 0.058318473 -0.27915755 -0.69888932 -1.1196783 -1.5802435 -2.0036182 -2.4185746 -2.6609833 -2.629916 -2.3861537 -2.0271943 -1.6821595 -1.3848634 -1.0894395][0.41906926 0.081624992 -0.11931749 -0.31649286 -0.48576179 -0.73330635 -1.062361 -1.4819248 -1.8545623 -2.068367 -2.0405469 -1.8750315 -1.6528673 -1.3990006 -1.1494911][0.15724817 -0.10411509 -0.1734519 -0.24488553 -0.24101648 -0.25490794 -0.33640409 -0.54808283 -0.85937881 -1.0793616 -1.1956191 -1.2298552 -1.1950575 -1.0887216 -0.97694683][0.25463939 0.064382739 0.018279888 0.067298092 0.21113688 0.41002831 0.606678 0.59061027 0.439225 0.28464037 0.060851268 -0.09691602 -0.24906987 -0.34337336 -0.46322006][0.60365415 0.62085932 0.67332578 0.78102189 0.99662566 1.2625781 1.5350202 1.7000134 1.6962514 1.5480239 1.2800722 1.0205353 0.69007528 0.38770768 0.086945824][0.54580206 0.73146355 0.979594 1.2383286 1.5753697 1.9529597 2.2901175 2.5128241 2.5158727 2.3026552 1.9407481 1.5714304 1.2446725 0.93043864 0.56614017][0.12080976 0.31081215 0.56585819 0.85866725 1.2146707 1.5788535 1.9468066 2.2114551 2.2377107 2.0093858 1.6386093 1.197453 0.83865649 0.61390853 0.40747496][-0.44049796 -0.4087784 -0.35168692 -0.26028985 -0.10738155 0.050200231 0.27927151 0.6108731 0.87281412 0.893049 0.75517374 0.52625138 0.37382135 0.29083 0.17589924][-0.49100706 -0.62491888 -0.74571079 -0.87583303 -0.94585288 -0.99069995 -0.98001939 -0.91081494 -0.75045395 -0.53367192 -0.40174049 -0.34735209 -0.25347459 -0.12817913 -0.020745248][-0.29455811 -0.46386322 -0.64355677 -0.86797661 -1.0714614 -1.2224573 -1.2935219 -1.291425 -1.1711966 -0.97082484 -0.75579607 -0.52500021 -0.33895031 -0.21703959 -0.079684794][-0.10809934 -0.22258613 -0.3889854 -0.55734313 -0.75996244 -0.94062662 -1.0610871 -1.0852473 -0.99715573 -0.83718061 -0.63528711 -0.44096935 -0.22386125 -0.15329349 -0.10351327][-0.095166028 -0.11970692 -0.17584246 -0.23846412 -0.33837247 -0.45304075 -0.55547494 -0.64986783 -0.67742485 -0.6019364 -0.45610642 -0.26226512 -0.11868075 -0.08044447 0.0083846524]]...]
INFO - root - 2017-12-06 09:00:30.124285: step 41310, loss = 0.83, batch loss = 0.61 (12.7 examples/sec; 0.632 sec/batch; 51h:06m:57s remains)
INFO - root - 2017-12-06 09:00:36.624384: step 41320, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.635 sec/batch; 51h:23m:44s remains)
INFO - root - 2017-12-06 09:00:43.098368: step 41330, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.633 sec/batch; 51h:11m:37s remains)
INFO - root - 2017-12-06 09:00:49.612582: step 41340, loss = 0.78, batch loss = 0.56 (11.8 examples/sec; 0.678 sec/batch; 54h:50m:37s remains)
INFO - root - 2017-12-06 09:00:56.075723: step 41350, loss = 0.78, batch loss = 0.57 (13.1 examples/sec; 0.610 sec/batch; 49h:18m:40s remains)
INFO - root - 2017-12-06 09:01:02.697082: step 41360, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.638 sec/batch; 51h:37m:14s remains)
INFO - root - 2017-12-06 09:01:09.198568: step 41370, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.656 sec/batch; 53h:04m:16s remains)
INFO - root - 2017-12-06 09:01:15.846274: step 41380, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.663 sec/batch; 53h:34m:31s remains)
INFO - root - 2017-12-06 09:01:22.351352: step 41390, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 51h:38m:31s remains)
INFO - root - 2017-12-06 09:01:28.890439: step 41400, loss = 0.84, batch loss = 0.62 (11.6 examples/sec; 0.688 sec/batch; 55h:39m:43s remains)
2017-12-06 09:01:29.507112: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.20006855 -0.24188393 -0.31241134 -0.36722916 -0.40946972 -0.38498342 -0.44228497 -0.49776608 -0.51225191 -0.52019352 -0.57042706 -0.5922007 -0.61385483 -0.49584654 -0.34641317][-0.29713967 -0.28888273 -0.33865887 -0.36291236 -0.38992211 -0.43573275 -0.46689638 -0.60667056 -0.78159583 -0.86675018 -0.93652219 -0.99306631 -1.0717092 -1.1582862 -1.2013634][-0.83641118 -0.82724178 -0.72655582 -0.63313049 -0.6083135 -0.63409686 -0.76760662 -0.92828578 -1.0948186 -1.346307 -1.6163911 -1.8200765 -1.996052 -2.0685618 -2.1655078][-1.3640326 -1.4847174 -1.3745378 -1.1936567 -1.1195047 -1.1148632 -1.2418524 -1.4919765 -1.7428253 -2.0064738 -2.3275447 -2.6271582 -2.8288817 -3.0593295 -3.3383157][-1.3216209 -1.575924 -1.5063198 -1.4256965 -1.3856405 -1.4572145 -1.6200593 -1.8154442 -2.0079074 -2.1771302 -2.41473 -2.6522954 -2.7913191 -3.0379314 -3.3924072][-0.71331275 -1.1496363 -1.0838029 -1.0246446 -0.95826381 -1.0388492 -1.1841899 -1.4187618 -1.5911912 -1.610976 -1.7002404 -1.9462672 -2.1522772 -2.2213728 -2.3730891][0.076162942 -0.37925375 -0.38363841 -0.3271535 -0.18781304 -0.23456833 -0.27022097 -0.29511061 -0.36957908 -0.42104897 -0.4692083 -0.48520365 -0.3163698 -0.31431863 -0.44149333][0.66504657 0.2585285 0.21987781 0.29625109 0.51414603 0.57988209 0.68400264 0.77492315 0.8603189 0.98728728 1.0258932 1.0208392 1.0869439 1.2167479 1.3041359][0.64164805 0.40028614 0.42435792 0.50400805 0.70872217 0.92012656 1.0780056 1.2899776 1.5001203 1.7454314 1.9520162 2.1288564 2.2287443 2.5051742 2.7423553][0.47156933 0.24613544 0.137903 0.16043884 0.24294892 0.4192259 0.55812907 0.69396609 0.83119363 1.1630644 1.4639159 1.7620344 2.0220456 2.2178979 2.3576672][0.066957094 -0.024458744 -0.22413149 -0.36170119 -0.38041618 -0.32495534 -0.23289192 -0.16695087 -0.17074335 -0.029073264 0.058958314 0.29287183 0.58777094 0.81826758 1.0333772][-0.28563032 -0.43430576 -0.61518824 -0.74016154 -0.81337929 -0.79486996 -0.77609575 -0.85357082 -1.0171318 -1.1212213 -1.2272673 -1.2223612 -1.1867832 -1.0163566 -0.76838064][-0.81345195 -0.83308762 -0.8817361 -0.94177169 -0.97426021 -0.91409796 -0.89149672 -1.0511409 -1.3401062 -1.6226289 -1.8245834 -1.8952285 -1.9713496 -1.8980709 -1.8725522][-0.74859315 -0.84633261 -0.83039415 -0.82088363 -0.74005556 -0.70875174 -0.74348 -0.9122107 -1.1139686 -1.410789 -1.6427817 -1.8743842 -2.088167 -2.1634007 -2.091861][-0.50785846 -0.53410906 -0.51513827 -0.49242324 -0.4584029 -0.39745936 -0.38291726 -0.50010121 -0.70879108 -0.9325797 -1.1407148 -1.3164015 -1.4932413 -1.7248144 -1.8626413]]...]
INFO - root - 2017-12-06 09:01:36.034264: step 41410, loss = 0.86, batch loss = 0.65 (12.5 examples/sec; 0.639 sec/batch; 51h:38m:49s remains)
INFO - root - 2017-12-06 09:01:42.594963: step 41420, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 53h:37m:22s remains)
INFO - root - 2017-12-06 09:01:49.043686: step 41430, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 52h:59m:50s remains)
INFO - root - 2017-12-06 09:01:55.537266: step 41440, loss = 0.72, batch loss = 0.51 (12.4 examples/sec; 0.645 sec/batch; 52h:09m:50s remains)
INFO - root - 2017-12-06 09:02:01.991347: step 41450, loss = 0.76, batch loss = 0.55 (12.1 examples/sec; 0.662 sec/batch; 53h:32m:17s remains)
INFO - root - 2017-12-06 09:02:08.381545: step 41460, loss = 0.88, batch loss = 0.66 (12.3 examples/sec; 0.649 sec/batch; 52h:29m:40s remains)
INFO - root - 2017-12-06 09:02:14.814068: step 41470, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.651 sec/batch; 52h:36m:39s remains)
INFO - root - 2017-12-06 09:02:21.368889: step 41480, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.654 sec/batch; 52h:49m:49s remains)
INFO - root - 2017-12-06 09:02:27.854100: step 41490, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 52h:08m:12s remains)
INFO - root - 2017-12-06 09:02:34.380955: step 41500, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 53h:33m:18s remains)
2017-12-06 09:02:35.006733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061895572 -0.061356887 -0.060838893 -0.060068287 -0.059339717 -0.058697406 -0.058241762 -0.058138859 -0.057863571 -0.057844177 -0.057715356 -0.057325579 -0.056494944 -0.055198357 -0.054528102][-0.061976172 -0.061413661 -0.060880225 -0.059943378 -0.058874514 -0.057662096 -0.056448463 -0.055881519 -0.05560419 -0.055908088 -0.056060608 -0.055698194 -0.055080656 -0.05407019 -0.05353472][-0.062209696 -0.061686367 -0.061224036 -0.060273588 -0.058973458 -0.057400148 -0.055720482 -0.054794725 -0.054740086 -0.055415902 -0.055923577 -0.055831123 -0.05540948 -0.054637507 -0.05411626][-0.062496215 -0.062030781 -0.06168022 -0.060785603 -0.059431233 -0.057567038 -0.055595435 -0.054590415 -0.054847356 -0.055961087 -0.056733046 -0.056806415 -0.056489561 -0.055982202 -0.055510886][-0.062666491 -0.062330522 -0.062057935 -0.061217312 -0.059780352 -0.057626791 -0.055360619 -0.054403465 -0.055338394 -0.056875 -0.057944283 -0.0581674 -0.058000553 -0.057624452 -0.0572509][-0.062868439 -0.062470675 -0.062211204 -0.061358616 -0.059833236 -0.057448138 -0.05506476 -0.054319113 -0.055918396 -0.057799272 -0.059159424 -0.059584983 -0.059678614 -0.05944287 -0.0592519][-0.06293308 -0.062417492 -0.06215113 -0.06133613 -0.059821807 -0.057482116 -0.0551957 -0.054845933 -0.056761216 -0.058743376 -0.060156483 -0.06070435 -0.060897868 -0.060825154 -0.060855955][-0.062921047 -0.062237054 -0.061912339 -0.061088033 -0.059592634 -0.057539795 -0.055672385 -0.055839725 -0.057479195 -0.059102371 -0.060281098 -0.060859405 -0.061105516 -0.061152406 -0.061405525][-0.062755637 -0.061888408 -0.061443441 -0.06050843 -0.059003834 -0.057187736 -0.055861261 -0.056105912 -0.057204735 -0.058567565 -0.059696965 -0.060363181 -0.060642838 -0.060686633 -0.061063439][-0.062461488 -0.061381944 -0.060710739 -0.059577651 -0.058007203 -0.056388892 -0.055318765 -0.055497255 -0.056427449 -0.057644866 -0.058803778 -0.059495054 -0.059856437 -0.059874766 -0.060332395][-0.062174786 -0.060854305 -0.059885718 -0.058461215 -0.056722324 -0.055122033 -0.054204945 -0.054394551 -0.055317208 -0.056526575 -0.0578243 -0.058623288 -0.059023548 -0.059063677 -0.05958255][-0.061909534 -0.060393896 -0.059128746 -0.057402909 -0.05545244 -0.053785246 -0.052899688 -0.053092226 -0.054012943 -0.055338632 -0.056739572 -0.057692572 -0.058240127 -0.058430094 -0.059037786][-0.061686922 -0.060035177 -0.058485549 -0.056483041 -0.054298315 -0.052417289 -0.051363453 -0.05139862 -0.052377291 -0.05390536 -0.055514585 -0.056724723 -0.057578493 -0.058017448 -0.058709554][-0.061612166 -0.059916496 -0.05825327 -0.056138743 -0.053841006 -0.0517458 -0.050357185 -0.050129104 -0.051045816 -0.052596554 -0.054306127 -0.055814687 -0.057018593 -0.057808947 -0.0586243][-0.061741531 -0.060147054 -0.058593839 -0.056599282 -0.054378051 -0.052209686 -0.050614059 -0.050148178 -0.050862722 -0.052187651 -0.053760651 -0.055416033 -0.056850657 -0.05790966 -0.058896612]]...]
INFO - root - 2017-12-06 09:02:41.533084: step 41510, loss = 0.80, batch loss = 0.59 (12.6 examples/sec; 0.636 sec/batch; 51h:23m:24s remains)
INFO - root - 2017-12-06 09:02:47.905679: step 41520, loss = 0.89, batch loss = 0.67 (12.7 examples/sec; 0.628 sec/batch; 50h:45m:31s remains)
INFO - root - 2017-12-06 09:02:54.356336: step 41530, loss = 0.88, batch loss = 0.66 (12.8 examples/sec; 0.627 sec/batch; 50h:38m:48s remains)
INFO - root - 2017-12-06 09:03:00.838958: step 41540, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.647 sec/batch; 52h:15m:44s remains)
INFO - root - 2017-12-06 09:03:07.189465: step 41550, loss = 0.75, batch loss = 0.54 (12.0 examples/sec; 0.667 sec/batch; 53h:54m:24s remains)
INFO - root - 2017-12-06 09:03:13.767298: step 41560, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.639 sec/batch; 51h:39m:32s remains)
INFO - root - 2017-12-06 09:03:20.280688: step 41570, loss = 0.88, batch loss = 0.67 (12.7 examples/sec; 0.631 sec/batch; 50h:59m:07s remains)
INFO - root - 2017-12-06 09:03:26.841652: step 41580, loss = 0.86, batch loss = 0.65 (12.1 examples/sec; 0.664 sec/batch; 53h:37m:57s remains)
INFO - root - 2017-12-06 09:03:33.510896: step 41590, loss = 0.85, batch loss = 0.64 (11.0 examples/sec; 0.724 sec/batch; 58h:31m:08s remains)
INFO - root - 2017-12-06 09:03:39.989468: step 41600, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.655 sec/batch; 52h:55m:47s remains)
2017-12-06 09:03:40.671889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05742548 -0.057365552 -0.057360619 -0.057190783 -0.05691753 -0.056676034 -0.056585472 -0.056678291 -0.056926195 -0.057289522 -0.057624597 -0.057882089 -0.058054164 -0.058186937 -0.058478985][-0.056424432 -0.056138251 -0.05586905 -0.05533728 -0.054704666 -0.05420037 -0.054039 -0.054294463 -0.054864168 -0.055621859 -0.056339655 -0.05693052 -0.057326987 -0.057597406 -0.057918914][-0.05563245 -0.054879427 -0.054007113 -0.052774679 -0.05147421 -0.050494257 -0.050194014 -0.050693117 -0.051809624 -0.053201098 -0.05453711 -0.055681355 -0.056480985 -0.05699341 -0.057446126][-0.0548862 -0.053533323 -0.051814225 -0.0495546 -0.047184639 -0.045420002 -0.044804294 -0.045596872 -0.047476105 -0.049804483 -0.052040365 -0.053950693 -0.055352356 -0.05631078 -0.057044175][-0.054064475 -0.052227803 -0.049642559 -0.046262857 -0.042597745 -0.039655156 -0.038346451 -0.039256085 -0.041977584 -0.045482878 -0.048900545 -0.051784821 -0.053963203 -0.055491727 -0.056708496][-0.053223815 -0.051050916 -0.047900382 -0.043548111 -0.038622718 -0.034304678 -0.031951629 -0.032554235 -0.035904005 -0.040603433 -0.045371871 -0.0494656 -0.052543323 -0.054708794 -0.05644815][-0.052252393 -0.050107535 -0.046839181 -0.042073332 -0.036424637 -0.031027876 -0.027465574 -0.027279384 -0.030667424 -0.036002472 -0.0417874 -0.047022477 -0.051126204 -0.0539515 -0.0561935][-0.050706159 -0.048935696 -0.046291806 -0.042062432 -0.0366093 -0.030944962 -0.026641093 -0.025358692 -0.0278216 -0.032810658 -0.038881157 -0.044836976 -0.049839471 -0.053354897 -0.056046631][-0.048142724 -0.047103141 -0.04565201 -0.04274385 -0.038506456 -0.0336569 -0.029488973 -0.027587213 -0.028738078 -0.032574221 -0.037913755 -0.043821137 -0.049196616 -0.053058404 -0.055992112][-0.045267072 -0.045075357 -0.04487351 -0.043504007 -0.040915772 -0.037591197 -0.034386765 -0.032523055 -0.032808658 -0.035218246 -0.039215285 -0.04421249 -0.049219169 -0.053097215 -0.056093488][-0.04272563 -0.043107606 -0.043903664 -0.043855943 -0.042871341 -0.041210465 -0.039287385 -0.037918318 -0.037860636 -0.039266273 -0.042068366 -0.045921154 -0.05003769 -0.05342767 -0.056202892][-0.04144913 -0.041859943 -0.0430171 -0.043691684 -0.04377244 -0.043435555 -0.042694658 -0.042122774 -0.042252153 -0.043235943 -0.045301009 -0.048136942 -0.051267285 -0.053916503 -0.05629807][-0.042308435 -0.042278524 -0.043116808 -0.043792091 -0.044235524 -0.044585489 -0.044740569 -0.044885185 -0.045367036 -0.046357188 -0.048068717 -0.050219279 -0.052497681 -0.054481059 -0.056363679][-0.045242846 -0.044589896 -0.044689257 -0.044833697 -0.045154084 -0.045738 -0.046351757 -0.046929769 -0.047652811 -0.048673678 -0.050104015 -0.051724736 -0.053372588 -0.054787148 -0.056255195][-0.049300656 -0.048080459 -0.047548819 -0.047246657 -0.047350664 -0.047880348 -0.048599742 -0.049306951 -0.050053842 -0.050827995 -0.051836569 -0.052898575 -0.05396843 -0.054915074 -0.05602695]]...]
INFO - root - 2017-12-06 09:03:47.038763: step 41610, loss = 0.82, batch loss = 0.60 (12.0 examples/sec; 0.669 sec/batch; 54h:01m:27s remains)
INFO - root - 2017-12-06 09:03:53.513277: step 41620, loss = 0.91, batch loss = 0.70 (11.7 examples/sec; 0.683 sec/batch; 55h:13m:00s remains)
INFO - root - 2017-12-06 09:04:00.045335: step 41630, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 53h:33m:15s remains)
INFO - root - 2017-12-06 09:04:06.587998: step 41640, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.639 sec/batch; 51h:36m:16s remains)
INFO - root - 2017-12-06 09:04:13.161641: step 41650, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.636 sec/batch; 51h:23m:42s remains)
INFO - root - 2017-12-06 09:04:19.682894: step 41660, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.650 sec/batch; 52h:30m:41s remains)
INFO - root - 2017-12-06 09:04:26.239653: step 41670, loss = 0.89, batch loss = 0.68 (12.7 examples/sec; 0.630 sec/batch; 50h:53m:39s remains)
INFO - root - 2017-12-06 09:04:32.779398: step 41680, loss = 0.79, batch loss = 0.58 (11.8 examples/sec; 0.681 sec/batch; 55h:00m:01s remains)
INFO - root - 2017-12-06 09:04:39.349797: step 41690, loss = 0.83, batch loss = 0.61 (12.0 examples/sec; 0.669 sec/batch; 54h:01m:27s remains)
INFO - root - 2017-12-06 09:04:45.893611: step 41700, loss = 0.86, batch loss = 0.64 (12.3 examples/sec; 0.650 sec/batch; 52h:30m:39s remains)
2017-12-06 09:04:46.531890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.068591617 -0.06845098 -0.068076268 -0.067447893 -0.067025438 -0.0669243 -0.066863127 -0.066637106 -0.06649515 -0.066410929 -0.066570975 -0.0665248 -0.066348761 -0.066137582 -0.066106737][-0.069506228 -0.069839023 -0.070201024 -0.0701251 -0.06961634 -0.068829626 -0.068151489 -0.067557551 -0.067349568 -0.067195356 -0.067237087 -0.067023888 -0.066765592 -0.066530108 -0.06632477][-0.070153281 -0.070762873 -0.071639426 -0.072279461 -0.072176121 -0.071052343 -0.069639251 -0.068794839 -0.068317816 -0.067914478 -0.067665271 -0.067307204 -0.067064606 -0.06677568 -0.066426232][-0.070201561 -0.070802428 -0.072356753 -0.074212052 -0.075268008 -0.0746187 -0.072528973 -0.071121037 -0.07059028 -0.070063874 -0.069514625 -0.068540178 -0.067671537 -0.066846386 -0.0662146][-0.0717334 -0.073207453 -0.075240627 -0.077721611 -0.079821154 -0.079862453 -0.077629581 -0.075587414 -0.0749976 -0.074140392 -0.072539546 -0.07052175 -0.06882605 -0.067275226 -0.066325605][-0.070718296 -0.071425386 -0.071787812 -0.073237687 -0.076636843 -0.0799683 -0.080876872 -0.080688976 -0.080875427 -0.080179624 -0.077666029 -0.0740659 -0.070738405 -0.068053722 -0.066565223][-0.067583814 -0.068107449 -0.068370625 -0.069733262 -0.074019358 -0.078511067 -0.0800891 -0.081163473 -0.083600916 -0.084882535 -0.082499906 -0.077707909 -0.0728141 -0.068889864 -0.066774152][-0.069959909 -0.071829461 -0.073271677 -0.074789807 -0.077056266 -0.077031247 -0.073606156 -0.072407804 -0.076190054 -0.080296382 -0.079986341 -0.076834463 -0.073431492 -0.070102811 -0.067550763][-0.06748233 -0.068760544 -0.069395527 -0.069685146 -0.071253717 -0.072645031 -0.072148129 -0.072820924 -0.077394 -0.082050666 -0.082320765 -0.078981571 -0.074870504 -0.070871741 -0.067984991][-0.06642919 -0.068195559 -0.06970039 -0.070531569 -0.071867041 -0.073482096 -0.07457561 -0.077268705 -0.082480133 -0.087906212 -0.0886753 -0.084757134 -0.078831241 -0.073175743 -0.06921754][-0.065264389 -0.066705815 -0.0683905 -0.069561422 -0.071216322 -0.072486937 -0.07322143 -0.074521907 -0.078158617 -0.083166115 -0.0850536 -0.082872249 -0.079154208 -0.075118035 -0.071363136][-0.065090969 -0.067023575 -0.070083536 -0.072616816 -0.073660433 -0.07287012 -0.071476 -0.071910053 -0.075067759 -0.078628317 -0.07979475 -0.078786694 -0.076945461 -0.074427113 -0.071131267][-0.065080911 -0.066691659 -0.069713622 -0.072826184 -0.074504465 -0.073877528 -0.071998119 -0.071176738 -0.07315582 -0.076132312 -0.077693857 -0.077526979 -0.07665804 -0.074761689 -0.071853392][-0.065677643 -0.067193992 -0.070150174 -0.073415264 -0.075900391 -0.076865226 -0.076791629 -0.0764507 -0.077022165 -0.078279428 -0.078255221 -0.076809585 -0.075274877 -0.073785052 -0.071652219][-0.066773444 -0.068261318 -0.070980504 -0.073789008 -0.075967968 -0.076931 -0.076445118 -0.074724734 -0.074234039 -0.075476356 -0.076987818 -0.077448912 -0.075661972 -0.072643615 -0.070597827]]...]
INFO - root - 2017-12-06 09:04:52.971295: step 41710, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.640 sec/batch; 51h:44m:03s remains)
INFO - root - 2017-12-06 09:04:59.408973: step 41720, loss = 0.71, batch loss = 0.50 (12.2 examples/sec; 0.654 sec/batch; 52h:48m:49s remains)
INFO - root - 2017-12-06 09:05:05.949188: step 41730, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.650 sec/batch; 52h:30m:29s remains)
INFO - root - 2017-12-06 09:05:12.549657: step 41740, loss = 0.89, batch loss = 0.67 (12.5 examples/sec; 0.642 sec/batch; 51h:49m:24s remains)
INFO - root - 2017-12-06 09:05:18.975885: step 41750, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.638 sec/batch; 51h:30m:08s remains)
INFO - root - 2017-12-06 09:05:25.453145: step 41760, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.639 sec/batch; 51h:34m:05s remains)
INFO - root - 2017-12-06 09:05:31.956424: step 41770, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.653 sec/batch; 52h:46m:31s remains)
INFO - root - 2017-12-06 09:05:38.533793: step 41780, loss = 0.89, batch loss = 0.67 (12.1 examples/sec; 0.662 sec/batch; 53h:27m:52s remains)
INFO - root - 2017-12-06 09:05:44.943568: step 41790, loss = 0.79, batch loss = 0.57 (12.6 examples/sec; 0.633 sec/batch; 51h:05m:36s remains)
INFO - root - 2017-12-06 09:05:51.374093: step 41800, loss = 0.91, batch loss = 0.69 (13.9 examples/sec; 0.574 sec/batch; 46h:22m:59s remains)
2017-12-06 09:05:51.956527: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.9312855 0.51072621 0.044855058 -0.476346 -1.2399364 -1.8163203 -2.214751 -2.2830644 -2.1185133 -1.8218955 -1.3487817 -0.54944026 -0.17414445 0.010314316 -0.10561336][1.2867409 0.83245671 0.21909666 -0.23311841 -1.0658476 -1.9595928 -2.0497689 -1.8623823 -1.7072726 -1.6333929 -1.153195 -0.25069854 0.5156064 0.87986839 1.2085414][1.2569758 0.72082126 0.1508632 -0.52531743 -1.6889833 -2.1778333 -2.1647866 -1.8663366 -1.3255539 -0.89990187 -0.32708982 0.062172011 0.547243 0.73467314 1.0334207][1.2055444 0.50281632 -0.075559787 -0.65109897 -1.0887007 -1.8915173 -2.5676513 -2.1841471 -1.8554523 -1.3177048 -0.77229953 -0.072278753 0.35900149 0.048965931 -0.20774409][1.2511128 0.97902143 0.53329515 -0.094940633 -0.54406285 -0.78308916 -1.0351551 -1.4917738 -1.8687395 -1.2982433 -0.77731013 -0.42981821 -0.0032716915 0.37584135 0.1972886][2.1889179 1.5416552 1.0640327 0.88116264 0.78018618 0.076819018 0.041024372 -0.036937147 -0.10437398 -0.19006783 -0.30074996 0.34700075 0.82483792 0.77331305 0.29961354][3.1222579 2.6782196 2.1874437 1.5145427 1.1343154 0.91151619 0.64698052 0.68280053 0.75503981 0.76515591 0.7530266 0.91372561 0.81861722 1.3143843 1.3767271][3.1335986 2.87673 1.9689249 1.1064085 0.57875407 0.40046281 0.5669167 0.48589173 0.56943786 0.91655135 1.1555486 0.895331 1.2216656 1.5832822 0.95990419][1.9510244 1.2772937 0.6389935 0.13339353 -0.54609096 -1.0003707 -0.99956596 -0.79937124 -0.79338491 -0.65064573 -0.37142733 -0.10385957 0.74426043 1.0397485 0.42328557][-0.38372907 -0.68874574 -1.1410003 -1.3774899 -1.5581663 -1.9291838 -2.2792585 -2.2352211 -2.0643713 -1.7062944 -1.1313019 -0.48208088 -0.23152779 -0.0023044273 0.40039507][-0.729007 -1.0454813 -1.3722366 -1.4315974 -1.5719087 -1.7762785 -1.949561 -1.757664 -1.8626662 -2.1441689 -1.9653631 -1.5402663 -1.4078717 -1.1478146 -0.627203][-0.97174561 -1.7289211 -2.0038128 -1.7458403 -1.8165619 -1.8222731 -1.7842387 -1.7064644 -1.8605193 -2.0161493 -2.4564991 -2.4100189 -2.3255668 -1.9564023 -1.766408][-1.4389857 -1.6678069 -1.8834628 -2.2836566 -2.1348658 -1.5659925 -1.5176078 -1.5700231 -1.2203187 -1.1286459 -1.4467814 -1.7237815 -1.7232007 -1.5933454 -1.5283291][-1.5427902 -1.4126644 -1.3355033 -1.5151445 -1.7643247 -1.7758956 -1.5758954 -1.4476813 -1.3944836 -1.2619269 -0.92154765 -1.066784 -1.2432 -1.1106825 -1.1443783][-1.4881155 -1.3860865 -1.0770808 -1.0227268 -1.4061601 -1.6100671 -1.6793945 -1.6805466 -1.682654 -1.4680706 -1.2683048 -1.0714923 -0.92032254 -0.9081645 -0.95636582]]...]
INFO - root - 2017-12-06 09:05:58.406187: step 41810, loss = 0.91, batch loss = 0.70 (12.8 examples/sec; 0.623 sec/batch; 50h:18m:19s remains)
INFO - root - 2017-12-06 09:06:04.973904: step 41820, loss = 0.76, batch loss = 0.54 (11.7 examples/sec; 0.685 sec/batch; 55h:16m:33s remains)
INFO - root - 2017-12-06 09:06:11.480279: step 41830, loss = 0.85, batch loss = 0.64 (12.0 examples/sec; 0.666 sec/batch; 53h:46m:46s remains)
INFO - root - 2017-12-06 09:06:17.956218: step 41840, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 52h:32m:03s remains)
INFO - root - 2017-12-06 09:06:24.508826: step 41850, loss = 0.88, batch loss = 0.67 (12.0 examples/sec; 0.667 sec/batch; 53h:49m:22s remains)
INFO - root - 2017-12-06 09:06:30.960233: step 41860, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.639 sec/batch; 51h:34m:52s remains)
INFO - root - 2017-12-06 09:06:37.577609: step 41870, loss = 0.90, batch loss = 0.68 (11.8 examples/sec; 0.679 sec/batch; 54h:51m:02s remains)
INFO - root - 2017-12-06 09:06:44.116000: step 41880, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.633 sec/batch; 51h:05m:15s remains)
INFO - root - 2017-12-06 09:06:50.527016: step 41890, loss = 0.82, batch loss = 0.61 (13.0 examples/sec; 0.614 sec/batch; 49h:33m:45s remains)
INFO - root - 2017-12-06 09:06:56.895535: step 41900, loss = 0.75, batch loss = 0.54 (11.9 examples/sec; 0.673 sec/batch; 54h:19m:35s remains)
2017-12-06 09:06:57.535944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.27951223 -0.2086941 -0.11130413 -0.058319382 -0.032232638 -0.006670393 0.00475996 0.00051288307 -0.018818833 -0.043980483 -0.055946015 -0.063645586 -0.072804749 -0.068588875 -0.072188616][-0.24621578 -0.18171823 -0.090358935 -0.011217348 0.063799813 0.12445757 0.13212992 0.12169129 0.078367844 0.028406709 -0.010788471 -0.034100596 -0.037419003 -0.038626809 -0.053408012][-0.21115361 -0.14550297 -0.061950326 0.016739763 0.097512648 0.18186206 0.20498261 0.1463328 0.058883697 0.0048521459 -0.028408337 -0.04336156 -0.04257717 -0.054614786 -0.075353764][-0.19779246 -0.11970884 -0.066785157 0.011092491 0.10455371 0.19618738 0.21435523 0.15116449 0.059435695 -0.0075215697 -0.05771412 -0.056169011 -0.049546164 -0.062481362 -0.055856492][-0.19368702 -0.12774269 -0.081918724 0.001255393 0.10130753 0.17916358 0.19323689 0.13323259 0.012385957 -0.054397464 -0.081135243 -0.052710474 -0.019455254 0.0047505349 0.065041408][-0.14474156 -0.11761205 -0.11745658 -0.039210524 0.056499511 0.086154863 0.075260758 0.00895749 -0.086109728 -0.12986952 -0.10456178 -0.049094606 0.0052164346 0.076189354 0.20193419][-0.12257459 -0.13039553 -0.14570346 -0.096292868 -0.037476294 -0.025223427 -0.063597187 -0.14872178 -0.2198088 -0.22404513 -0.18419173 -0.098076321 -0.016324811 0.10840712 0.29441461][-0.087529056 -0.1203683 -0.18524304 -0.19550338 -0.18803808 -0.20107406 -0.22602999 -0.28207088 -0.31861642 -0.28574502 -0.23784325 -0.15055872 -0.06369558 0.078592554 0.26073676][-0.0798402 -0.10730602 -0.16741215 -0.20650369 -0.21422264 -0.22534333 -0.23843423 -0.27550414 -0.29768682 -0.27125561 -0.23549001 -0.17664535 -0.1390824 -0.041046042 0.09923172][-0.084131457 -0.10373729 -0.15268826 -0.19426936 -0.20709985 -0.2100917 -0.20672905 -0.21792142 -0.22766471 -0.21474338 -0.20172668 -0.17202653 -0.1792348 -0.15073518 -0.0709464][-0.060477104 -0.083431721 -0.12295756 -0.15439206 -0.16028535 -0.15740323 -0.14839078 -0.16233337 -0.17872526 -0.17566051 -0.18404451 -0.19984005 -0.2588113 -0.2721175 -0.23263127][0.03153424 -0.021310352 -0.077060491 -0.12776975 -0.14477 -0.13800171 -0.12732819 -0.14083393 -0.13059598 -0.13391824 -0.1473451 -0.18044654 -0.25925851 -0.30930573 -0.29612964][0.17138454 0.10212862 0.020955309 -0.033791121 -0.061989918 -0.086391181 -0.093114235 -0.10936724 -0.11038711 -0.12040925 -0.12573597 -0.16768047 -0.24543262 -0.28189331 -0.25876564][0.263861 0.16521543 0.087344885 0.042372122 -0.0041387826 -0.04364939 -0.054312758 -0.07512664 -0.077352412 -0.08989273 -0.09852726 -0.13344029 -0.18804699 -0.22266909 -0.22018325][0.27713788 0.20686427 0.1849066 0.15441889 0.11722901 0.055317715 0.018708766 -0.0071063638 -0.045578077 -0.083280146 -0.087082572 -0.1059856 -0.11899866 -0.12826529 -0.11783037]]...]
INFO - root - 2017-12-06 09:07:03.989372: step 41910, loss = 0.87, batch loss = 0.66 (12.0 examples/sec; 0.668 sec/batch; 53h:53m:59s remains)
INFO - root - 2017-12-06 09:07:10.520760: step 41920, loss = 0.74, batch loss = 0.53 (11.9 examples/sec; 0.673 sec/batch; 54h:18m:34s remains)
INFO - root - 2017-12-06 09:07:17.070680: step 41930, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 52h:50m:47s remains)
INFO - root - 2017-12-06 09:07:23.583636: step 41940, loss = 0.78, batch loss = 0.57 (11.8 examples/sec; 0.680 sec/batch; 54h:55m:04s remains)
INFO - root - 2017-12-06 09:07:30.165360: step 41950, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.638 sec/batch; 51h:29m:46s remains)
INFO - root - 2017-12-06 09:07:36.660045: step 41960, loss = 0.80, batch loss = 0.58 (12.9 examples/sec; 0.621 sec/batch; 50h:05m:16s remains)
INFO - root - 2017-12-06 09:07:43.147258: step 41970, loss = 0.85, batch loss = 0.64 (12.9 examples/sec; 0.622 sec/batch; 50h:09m:49s remains)
INFO - root - 2017-12-06 09:07:49.610812: step 41980, loss = 0.84, batch loss = 0.62 (12.6 examples/sec; 0.634 sec/batch; 51h:11m:54s remains)
INFO - root - 2017-12-06 09:07:56.129540: step 41990, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.656 sec/batch; 52h:56m:20s remains)
INFO - root - 2017-12-06 09:08:02.696150: step 42000, loss = 0.86, batch loss = 0.65 (12.3 examples/sec; 0.652 sec/batch; 52h:35m:08s remains)
2017-12-06 09:08:03.339060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.13893105 -0.14604284 -0.14766544 -0.13574995 -0.12227567 -0.11336054 -0.10376326 -0.080606721 -0.062851518 -0.040695578 -0.037254713 -0.03411622 -0.039449118 -0.03947122 -0.043899357][-0.1410999 -0.14205088 -0.13154629 -0.11516884 -0.10933306 -0.09997838 -0.097549826 -0.076177776 -0.066836759 -0.053162523 -0.053471304 -0.050251335 -0.051504415 -0.049013812 -0.060548786][-0.13963254 -0.13980304 -0.13475814 -0.11406059 -0.09970101 -0.0753002 -0.074142247 -0.050167724 -0.052552048 -0.046062797 -0.0545392 -0.057740629 -0.065174833 -0.072128884 -0.084359787][-0.11215971 -0.097552925 -0.086728618 -0.068035826 -0.059873566 -0.031028382 -0.027943879 -0.0055966824 -0.020518534 -0.02038493 -0.030000109 -0.043574966 -0.050319932 -0.063650951 -0.078770444][-0.099015288 -0.078602687 -0.064404234 -0.048220985 -0.037394863 -0.020628169 -0.017696857 -0.00976371 -0.026893653 -0.033293273 -0.041681044 -0.056574158 -0.066510737 -0.074060455 -0.083669342][-0.09211684 -0.079915144 -0.075594872 -0.071608059 -0.061897751 -0.043286625 -0.028028496 -0.020272247 -0.0314853 -0.037570968 -0.048081364 -0.057971567 -0.064522654 -0.07086049 -0.077169187][-0.080621757 -0.075949863 -0.073372059 -0.067894064 -0.057901427 -0.039665923 -0.030189488 -0.028899211 -0.036356762 -0.033291407 -0.038938675 -0.047262143 -0.053591188 -0.059578851 -0.06110765][-0.076421186 -0.07645835 -0.072323523 -0.056542747 -0.032135498 -0.0045176893 0.0075517446 0.000375472 -0.015307412 -0.025161855 -0.028261796 -0.037275329 -0.044375151 -0.053061042 -0.057854515][-0.065375879 -0.065038413 -0.066722319 -0.061539531 -0.044114016 -0.0231793 -0.010019779 -0.015565239 -0.023727417 -0.032032155 -0.032273352 -0.044801269 -0.047630753 -0.056029152 -0.061893016][-0.064981461 -0.065019779 -0.069028907 -0.070733257 -0.063596219 -0.050473481 -0.0423044 -0.046268187 -0.04980965 -0.053713668 -0.052115921 -0.057436515 -0.057389274 -0.055930275 -0.059313051][-0.0632831 -0.061164692 -0.06165269 -0.05811166 -0.052645255 -0.045061622 -0.044234924 -0.047250323 -0.051612161 -0.057517596 -0.058579762 -0.0648396 -0.066736862 -0.0679892 -0.072497316][-0.068536952 -0.069563173 -0.07116019 -0.069749042 -0.064583212 -0.056913331 -0.057029907 -0.05970794 -0.06256941 -0.067823045 -0.067362405 -0.0700759 -0.07124377 -0.070677787 -0.07325384][-0.072371244 -0.072891816 -0.074898764 -0.076463632 -0.074603043 -0.070172288 -0.067496032 -0.067856878 -0.069229864 -0.071406454 -0.072159365 -0.072747819 -0.073624507 -0.074183628 -0.075295806][-0.0750802 -0.0745285 -0.074462734 -0.074474163 -0.074783094 -0.074514568 -0.073871642 -0.073388144 -0.073192753 -0.073561534 -0.074197441 -0.073613979 -0.075201824 -0.075483695 -0.076609157][-0.075911671 -0.075062722 -0.0746983 -0.074033819 -0.0737823 -0.0740447 -0.074657261 -0.075351045 -0.0753174 -0.075620517 -0.0756469 -0.075099848 -0.075807706 -0.07623671 -0.076643318]]...]
INFO - root - 2017-12-06 09:08:09.897901: step 42010, loss = 0.82, batch loss = 0.60 (12.3 examples/sec; 0.652 sec/batch; 52h:35m:17s remains)
INFO - root - 2017-12-06 09:08:16.371620: step 42020, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.646 sec/batch; 52h:06m:19s remains)
INFO - root - 2017-12-06 09:08:22.830102: step 42030, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.652 sec/batch; 52h:38m:38s remains)
INFO - root - 2017-12-06 09:08:29.338862: step 42040, loss = 0.89, batch loss = 0.67 (12.0 examples/sec; 0.668 sec/batch; 53h:55m:32s remains)
INFO - root - 2017-12-06 09:08:35.939743: step 42050, loss = 0.82, batch loss = 0.61 (12.4 examples/sec; 0.645 sec/batch; 52h:04m:00s remains)
INFO - root - 2017-12-06 09:08:42.492396: step 42060, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.652 sec/batch; 52h:37m:08s remains)
INFO - root - 2017-12-06 09:08:49.105321: step 42070, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 51h:10m:16s remains)
INFO - root - 2017-12-06 09:08:55.471539: step 42080, loss = 0.96, batch loss = 0.74 (12.6 examples/sec; 0.637 sec/batch; 51h:23m:41s remains)
INFO - root - 2017-12-06 09:09:01.973473: step 42090, loss = 0.80, batch loss = 0.58 (13.0 examples/sec; 0.613 sec/batch; 49h:28m:29s remains)
INFO - root - 2017-12-06 09:09:08.511443: step 42100, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.653 sec/batch; 52h:42m:40s remains)
2017-12-06 09:09:09.136992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1870258 -1.1961074 -1.2378725 -1.217243 -1.175909 -1.2251315 -1.2364411 -1.1233878 -1.008747 -0.97657812 -1.0659192 -1.0562551 -1.0781955 -1.2879387 -1.41241][-0.75352168 -0.78341138 -0.711056 -0.81120086 -0.93102312 -1.1535244 -1.3302234 -1.3895316 -1.3512169 -1.0528551 -0.90079618 -1.0142018 -1.0499349 -0.89475882 -0.87613177][0.4374195 0.75343692 0.96116507 0.72754931 0.37372565 -0.20504326 -0.72769487 -1.0685121 -1.1983187 -1.2925307 -1.3278568 -1.1137066 -0.8911562 -0.86093569 -0.84306967][1.9014685 2.1667891 2.3249438 2.2276323 1.5191059 0.66762769 0.038641751 -0.35457003 -0.47180697 -0.59927857 -0.73977947 -0.80170548 -0.81718326 -0.89959896 -0.94112074][4.2505274 4.1265116 3.5586913 2.7083917 1.8280857 0.96615219 0.33144897 0.03816399 -0.13191219 -0.24351974 -0.25241396 -0.28277427 -0.37845406 -0.55389893 -0.72524834][5.8406739 5.4505725 4.6998205 3.5706577 2.5195687 1.6694924 0.8888036 0.47293827 0.23262903 0.12528282 0.038403675 -0.014448196 0.045747429 0.0091595352 -0.16398379][5.146842 4.9380488 4.1295347 3.4405692 2.6645584 1.8706868 1.2909147 0.81317759 0.45497337 0.16526252 0.054509759 0.032927051 0.076841548 0.14112732 0.14939563][3.4374864 3.0741596 2.8912647 2.5799212 2.1016304 1.7104309 1.0837516 0.51828456 0.27575707 0.19532067 0.060588121 0.20259282 0.46252319 0.57910085 0.5858109][1.8495322 2.0242264 1.9498798 1.662799 1.6063899 1.5668236 1.0310414 0.62513757 0.39699769 0.19370964 0.12235349 0.21175024 0.62053025 1.0617892 1.2320138][-1.8528318 -0.64995718 0.044131458 0.69921517 0.98788571 0.61636472 0.22650886 -0.23933297 -0.68826652 -0.71014225 -0.44698882 -0.13822295 0.53422356 1.0954819 1.5125343][-3.1284428 -2.3243194 -1.7913457 -0.80594587 -0.094813406 -0.00986848 -0.23622051 -0.73615789 -1.1708107 -1.4572221 -1.5484827 -0.81925285 -0.064415425 0.724136 1.3657898][-4.4568229 -3.1912718 -2.4834969 -1.8312178 -1.1121311 -0.8320806 -0.79500103 -1.0293945 -1.0286546 -1.0637374 -1.2062328 -1.1198382 -0.32204342 0.36813417 0.81709003][-4.228579 -3.5958996 -2.9436989 -2.1640306 -1.6994694 -1.5084125 -1.2098372 -1.5467873 -1.5557702 -1.4845812 -0.97387528 -0.18706661 0.44797423 1.011091 1.3141801][-3.9923451 -3.7220478 -3.4188881 -2.6751301 -2.062953 -1.7510768 -1.6378437 -1.5668637 -1.394092 -1.248664 -0.80132747 -0.20672512 0.29373249 0.85637093 1.1489263][-3.8301988 -3.3491435 -3.1531258 -2.7143314 -1.9827729 -1.3899286 -1.1019977 -1.3817697 -1.3642849 -1.2859278 -1.0884858 -0.36076829 0.039688706 0.52112842 0.61847675]]...]
INFO - root - 2017-12-06 09:09:15.653388: step 42110, loss = 0.86, batch loss = 0.65 (12.9 examples/sec; 0.619 sec/batch; 49h:54m:51s remains)
INFO - root - 2017-12-06 09:09:22.188069: step 42120, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 53h:29m:28s remains)
INFO - root - 2017-12-06 09:09:28.690378: step 42130, loss = 0.99, batch loss = 0.77 (12.1 examples/sec; 0.664 sec/batch; 53h:31m:13s remains)
INFO - root - 2017-12-06 09:09:35.199285: step 42140, loss = 0.84, batch loss = 0.63 (12.2 examples/sec; 0.657 sec/batch; 52h:59m:11s remains)
INFO - root - 2017-12-06 09:09:41.714502: step 42150, loss = 0.73, batch loss = 0.51 (12.1 examples/sec; 0.662 sec/batch; 53h:21m:54s remains)
INFO - root - 2017-12-06 09:09:48.193806: step 42160, loss = 0.92, batch loss = 0.71 (12.1 examples/sec; 0.661 sec/batch; 53h:17m:28s remains)
INFO - root - 2017-12-06 09:09:54.654740: step 42170, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.629 sec/batch; 50h:41m:56s remains)
INFO - root - 2017-12-06 09:10:01.092431: step 42180, loss = 0.83, batch loss = 0.62 (12.1 examples/sec; 0.658 sec/batch; 53h:06m:08s remains)
INFO - root - 2017-12-06 09:10:07.544893: step 42190, loss = 0.79, batch loss = 0.57 (11.7 examples/sec; 0.687 sec/batch; 55h:22m:25s remains)
INFO - root - 2017-12-06 09:10:14.006915: step 42200, loss = 0.79, batch loss = 0.58 (12.6 examples/sec; 0.636 sec/batch; 51h:16m:49s remains)
2017-12-06 09:10:14.672110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.090176314 -0.092622414 -0.093743883 -0.09264832 -0.089121044 -0.083964415 -0.077954009 -0.071933709 -0.0673001 -0.0646936 -0.064091712 -0.06471622 -0.06583152 -0.067324772 -0.069348745][-0.076390021 -0.07844162 -0.079536185 -0.07878232 -0.075822391 -0.071360186 -0.066030145 -0.060675263 -0.056630891 -0.054674439 -0.054857813 -0.056324713 -0.058367692 -0.060574412 -0.062820189][-0.066295765 -0.067565553 -0.068357326 -0.067738108 -0.065336719 -0.061800376 -0.057539057 -0.053198546 -0.050002377 -0.048899557 -0.05005182 -0.0522418 -0.054918703 -0.057661816 -0.06031137][-0.059125997 -0.059494693 -0.059827454 -0.0593196 -0.057513744 -0.054934759 -0.051835693 -0.048774146 -0.046726059 -0.046660259 -0.048726272 -0.051786263 -0.055140097 -0.058449578 -0.0615257][-0.054278221 -0.054151703 -0.054258525 -0.053848222 -0.05259658 -0.050877169 -0.048995584 -0.047119707 -0.046224657 -0.047111046 -0.049737494 -0.05344208 -0.057279378 -0.061060622 -0.0645162][-0.051249851 -0.050712936 -0.050701417 -0.050400469 -0.049630418 -0.048621237 -0.047556069 -0.046548422 -0.046317611 -0.047732681 -0.050695345 -0.054868728 -0.059225582 -0.063375935 -0.067146637][-0.049739417 -0.048916169 -0.048830032 -0.048692029 -0.048269637 -0.047680371 -0.046966843 -0.046354931 -0.046351843 -0.047649268 -0.050425723 -0.054377366 -0.05879325 -0.063006923 -0.066944376][-0.049140744 -0.048205946 -0.048128631 -0.0480274 -0.047793753 -0.04743515 -0.046966903 -0.046608016 -0.046705827 -0.047689762 -0.049826343 -0.052978355 -0.056623142 -0.060276859 -0.063709587][-0.048813522 -0.047970306 -0.047999669 -0.047921877 -0.047733374 -0.04745204 -0.047098067 -0.046852104 -0.046919737 -0.047421068 -0.048669595 -0.050816752 -0.053403772 -0.056044608 -0.058714885][-0.048410356 -0.047731865 -0.047879215 -0.0478385 -0.047692873 -0.047430422 -0.047027152 -0.046669856 -0.046421 -0.046456583 -0.0468826 -0.047975045 -0.049535111 -0.051207375 -0.05321008][-0.04813 -0.0475052 -0.047691315 -0.047688965 -0.047585092 -0.047344998 -0.046922415 -0.046481583 -0.045973182 -0.045520224 -0.04530229 -0.045587804 -0.046336036 -0.04737433 -0.048950143][-0.04786374 -0.0472396 -0.047433585 -0.047436729 -0.047307685 -0.047029778 -0.046592966 -0.046117105 -0.045507796 -0.044798318 -0.044132926 -0.043788735 -0.043876264 -0.044499945 -0.045813184][-0.047607414 -0.046923921 -0.04706192 -0.047040794 -0.046853524 -0.04655534 -0.046188097 -0.04578343 -0.045252204 -0.044515349 -0.043744609 -0.04317601 -0.042942666 -0.043285422 -0.044419594][-0.047939032 -0.047201186 -0.047256231 -0.04720217 -0.04694536 -0.046524212 -0.046118546 -0.045819476 -0.0454308 -0.044818856 -0.04415679 -0.043619696 -0.043338794 -0.043572966 -0.044578932][-0.049043264 -0.048204422 -0.048151359 -0.047962479 -0.0476649 -0.047174145 -0.04681937 -0.046636924 -0.046426985 -0.046022344 -0.045588072 -0.045276977 -0.0451061 -0.045374498 -0.046188362]]...]
INFO - root - 2017-12-06 09:10:21.214586: step 42210, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.658 sec/batch; 53h:02m:05s remains)
INFO - root - 2017-12-06 09:10:27.750728: step 42220, loss = 0.73, batch loss = 0.52 (12.5 examples/sec; 0.638 sec/batch; 51h:28m:57s remains)
INFO - root - 2017-12-06 09:10:34.302511: step 42230, loss = 0.82, batch loss = 0.60 (12.0 examples/sec; 0.665 sec/batch; 53h:37m:22s remains)
INFO - root - 2017-12-06 09:10:40.782322: step 42240, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 52h:08m:53s remains)
INFO - root - 2017-12-06 09:10:47.363030: step 42250, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.649 sec/batch; 52h:17m:15s remains)
INFO - root - 2017-12-06 09:10:53.950207: step 42260, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.628 sec/batch; 50h:37m:28s remains)
INFO - root - 2017-12-06 09:11:00.301527: step 42270, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.648 sec/batch; 52h:14m:35s remains)
INFO - root - 2017-12-06 09:11:06.825050: step 42280, loss = 0.80, batch loss = 0.59 (12.7 examples/sec; 0.629 sec/batch; 50h:43m:19s remains)
INFO - root - 2017-12-06 09:11:13.414293: step 42290, loss = 0.82, batch loss = 0.60 (12.1 examples/sec; 0.661 sec/batch; 53h:18m:35s remains)
INFO - root - 2017-12-06 09:11:19.805532: step 42300, loss = 0.72, batch loss = 0.51 (12.9 examples/sec; 0.621 sec/batch; 50h:04m:02s remains)
2017-12-06 09:11:20.536866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1527199 -1.0712498 -0.9691267 -0.86010391 -0.7294662 -0.62314451 -0.526052 -0.45585513 -0.39319351 -0.37504905 -0.37352744 -0.46900669 -0.56179136 -0.59085262 -0.58495349][-1.8054315 -1.8187869 -1.8239961 -1.8156863 -1.7982492 -1.8009807 -1.7632962 -1.6312262 -1.3765019 -1.162936 -0.86259943 -0.71020991 -0.59750539 -0.56017935 -0.50457621][-2.0471821 -2.3679309 -2.6893606 -2.9807522 -3.1319866 -3.3021941 -3.4164042 -3.454561 -3.2103827 -2.8018024 -2.1361368 -1.6153066 -1.139842 -0.81234288 -0.55323964][-1.4946512 -2.0698166 -2.7109931 -3.1213088 -3.4229541 -3.8839092 -4.2328129 -4.3734331 -4.25467 -3.877578 -3.1535552 -2.4118545 -1.695008 -1.192517 -0.80771691][-0.44174936 -1.0120023 -1.6147445 -2.077409 -2.4396992 -2.825948 -3.2197952 -3.5437264 -3.7518265 -3.5291195 -3.0071359 -2.361227 -1.6845886 -1.2108146 -0.85389775][0.17945218 -0.083772518 -0.22982261 -0.33465803 -0.33585164 -0.4577561 -0.7397126 -1.0642048 -1.5330746 -1.7341139 -1.6931225 -1.3972502 -0.983221 -0.69412887 -0.50313526][0.054324351 0.14644241 0.61191219 1.1046991 1.6652708 1.9326257 1.9934076 1.9104904 1.4099904 0.85093689 0.36067089 0.1018968 0.0069105923 0.026512004 0.01475767][-0.15656137 -0.026242822 0.67407322 1.5666119 2.6163545 3.2483168 3.6497107 3.6334081 3.2465179 2.6558335 2.0209124 1.3566222 0.79191113 0.52453184 0.36631793][-0.35439485 -0.069277428 0.69896704 1.5560272 2.5918608 3.3592896 4.005713 4.1236191 3.8447549 3.1013162 2.3240817 1.5714116 0.96684724 0.51120442 0.27119154][-0.028895043 0.081918471 0.645031 1.3258306 2.0916963 2.6209507 3.111146 3.406549 3.3151941 2.7474604 2.0047989 1.2797658 0.65771538 0.21908978 0.057425849][0.50474119 0.34696096 0.49921468 0.685109 0.91540116 1.0783708 1.2981811 1.5398412 1.6572167 1.3631955 0.87885404 0.42756745 0.059823744 -0.16363718 -0.23388806][0.10383875 -0.15804949 -0.35499373 -0.48805535 -0.5063346 -0.61944026 -0.6385178 -0.47974771 -0.18551731 -0.24652934 -0.29574275 -0.36695358 -0.41034031 -0.41904354 -0.31244606][-0.61458379 -1.0584601 -1.4094582 -1.6340997 -1.7983536 -1.8798047 -1.8189224 -1.6911049 -1.3895838 -1.1795043 -0.88753349 -0.74402004 -0.62941277 -0.46351486 -0.33353114][-1.0827694 -1.4502355 -1.7802207 -1.9387954 -2.0029252 -2.1243703 -2.0840333 -1.8929009 -1.6257714 -1.3294798 -0.90942842 -0.68312782 -0.53788549 -0.31803009 -0.26819339][-1.2049575 -1.3618238 -1.5136726 -1.551042 -1.562935 -1.547869 -1.3463539 -1.2232238 -0.98021394 -0.80640984 -0.51839083 -0.365031 -0.30267572 -0.19776195 -0.16385517]]...]
INFO - root - 2017-12-06 09:11:27.025878: step 42310, loss = 0.93, batch loss = 0.71 (12.7 examples/sec; 0.632 sec/batch; 50h:54m:55s remains)
INFO - root - 2017-12-06 09:11:33.423046: step 42320, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.650 sec/batch; 52h:22m:48s remains)
INFO - root - 2017-12-06 09:11:39.927009: step 42330, loss = 0.86, batch loss = 0.65 (11.7 examples/sec; 0.686 sec/batch; 55h:17m:19s remains)
INFO - root - 2017-12-06 09:11:46.404326: step 42340, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.657 sec/batch; 52h:58m:01s remains)
INFO - root - 2017-12-06 09:11:52.903930: step 42350, loss = 0.88, batch loss = 0.67 (12.8 examples/sec; 0.623 sec/batch; 50h:11m:19s remains)
INFO - root - 2017-12-06 09:11:59.354592: step 42360, loss = 0.85, batch loss = 0.63 (12.2 examples/sec; 0.654 sec/batch; 52h:44m:48s remains)
INFO - root - 2017-12-06 09:12:05.837543: step 42370, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 52h:27m:34s remains)
INFO - root - 2017-12-06 09:12:12.370989: step 42380, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.657 sec/batch; 52h:55m:47s remains)
INFO - root - 2017-12-06 09:12:18.904646: step 42390, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.669 sec/batch; 53h:56m:15s remains)
INFO - root - 2017-12-06 09:12:25.287013: step 42400, loss = 0.87, batch loss = 0.65 (12.6 examples/sec; 0.636 sec/batch; 51h:15m:04s remains)
2017-12-06 09:12:25.930675: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.1824446 0.77142286 0.42409176 -0.33043027 -0.92991209 -1.2177072 -1.5410812 -1.7193427 -1.7558078 -1.5484511 -1.3002626 -1.5392011 -1.1232994 -1.1500624 -1.2951213][1.4940932 1.1784376 0.51487082 0.18971916 -0.33932763 -1.0514697 -1.3740795 -1.4927486 -1.347669 -1.5976205 -1.6714463 -1.5760742 -1.5874938 -1.3685242 -1.2249931][1.4762419 0.94526643 0.51264954 -0.32863933 -1.1715707 -1.2843343 -1.343874 -1.5459565 -1.5886668 -1.4886037 -1.5814313 -1.6688867 -1.5946665 -1.1785007 -1.1920092][0.40836328 0.028415047 -0.7155326 -0.69817495 -0.696557 -1.2194049 -1.6049451 -1.3997852 -1.3484498 -1.3473853 -1.3315595 -1.4377745 -1.3027331 -1.2245914 -1.1847725][-0.66172349 -0.94532323 -0.79206884 -0.98211783 -0.8677153 -0.52153647 -0.29970419 -0.53423959 -0.75759697 -0.70198262 -0.72580284 -0.76983619 -0.78932 -0.56020778 -0.59422714][-0.6022774 -0.65698451 -0.48991513 -0.33686244 0.027737804 0.18651451 0.26604682 0.48213232 0.65122277 0.25434154 -0.13226998 -0.093207784 -0.13270675 -0.4973411 -0.79993713][-0.4120695 -0.033984832 0.15347801 0.37800103 0.59796375 0.85755211 1.2150666 1.3711348 1.2881597 1.0648991 0.67320794 0.27381235 -0.15009958 -0.40699518 -0.53804839][0.66006327 0.38359123 0.357363 0.60080755 0.990035 1.376258 1.484888 1.7652947 1.8276865 1.3746953 0.97304422 0.87903231 0.87702465 0.4686141 -0.17624497][0.82013 0.92813617 0.8595615 0.73828095 0.52849948 0.70240372 1.0697131 1.150527 0.90834367 0.82423711 0.75903529 0.70901114 0.91738135 0.8449133 0.74470073][0.1282873 -0.028704274 -0.1446352 0.16406576 -0.10114653 -0.36681998 -0.62630367 -0.64353579 -0.42300045 -0.42887938 -0.25424892 0.089098886 0.46498477 0.4929508 0.47518224][-0.49314767 -0.95711583 -0.75139695 -0.80949253 -1.2587007 -1.3883668 -1.3875009 -1.4875941 -1.975728 -1.8917869 -1.6617458 -1.1616043 -0.51764351 -0.35885751 -0.22695436][-0.85656673 -1.4993302 -1.5149167 -1.3951122 -1.5833502 -1.9005351 -2.2734473 -2.544553 -2.705328 -2.7896144 -3.0168631 -2.5198119 -2.0290191 -1.5784695 -1.2600838][-0.71599871 -1.4448073 -1.3773038 -1.3514575 -1.4866127 -1.5126234 -1.6805437 -2.3551679 -2.6223958 -2.5790372 -2.5287225 -2.1747091 -1.7609749 -1.474059 -1.1925151][-1.2115517 -1.041629 -0.99723983 -1.2227206 -1.3950692 -1.4942722 -1.6175297 -1.5538979 -1.6185431 -1.7688333 -1.7186364 -1.7720783 -1.852591 -1.1418843 -1.3582015][0.092868656 -0.25374502 -0.67575616 -1.0600822 -1.333123 -1.1199441 -1.0432011 -1.5131894 -1.721393 -1.6993877 -1.6494455 -1.366994 -1.0626943 -0.73100775 -0.92818546]]...]
INFO - root - 2017-12-06 09:12:32.421509: step 42410, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.650 sec/batch; 52h:21m:11s remains)
INFO - root - 2017-12-06 09:12:38.924829: step 42420, loss = 0.80, batch loss = 0.58 (12.1 examples/sec; 0.662 sec/batch; 53h:21m:00s remains)
INFO - root - 2017-12-06 09:12:45.536069: step 42430, loss = 0.83, batch loss = 0.61 (12.0 examples/sec; 0.666 sec/batch; 53h:40m:24s remains)
INFO - root - 2017-12-06 09:12:52.053370: step 42440, loss = 0.95, batch loss = 0.74 (12.3 examples/sec; 0.649 sec/batch; 52h:16m:02s remains)
INFO - root - 2017-12-06 09:12:58.633068: step 42450, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.645 sec/batch; 51h:57m:04s remains)
INFO - root - 2017-12-06 09:13:05.005744: step 42460, loss = 0.87, batch loss = 0.66 (12.3 examples/sec; 0.649 sec/batch; 52h:16m:15s remains)
INFO - root - 2017-12-06 09:13:11.500028: step 42470, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 52h:34m:32s remains)
INFO - root - 2017-12-06 09:13:18.020178: step 42480, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 51h:22m:57s remains)
INFO - root - 2017-12-06 09:13:24.638549: step 42490, loss = 0.77, batch loss = 0.56 (11.9 examples/sec; 0.671 sec/batch; 54h:03m:45s remains)
INFO - root - 2017-12-06 09:13:31.194479: step 42500, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.663 sec/batch; 53h:22m:21s remains)
2017-12-06 09:13:31.818014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.22503123 -0.21163014 -0.25365612 -0.39394677 -0.63042629 -0.77111214 -0.924764 -1.1489834 -1.4655735 -1.5624502 -1.5025369 -1.2689135 -1.1499646 -1.0296782 -0.85714805][-0.59980172 -0.74842817 -0.87417245 -0.9764595 -1.1463934 -1.3516569 -1.5297838 -1.5895237 -1.6391076 -1.8567988 -2.0849955 -1.994795 -1.7385412 -1.4188124 -1.1983798][-1.0272669 -1.2844999 -1.505455 -1.598664 -1.5635052 -1.5375187 -1.5926245 -1.6442963 -1.6783329 -1.7543083 -1.8589541 -1.9654642 -1.857448 -1.4421514 -1.0398256][-1.4050387 -1.7640163 -1.9044425 -1.8056499 -1.5494498 -1.2910351 -1.1367577 -1.0200961 -1.026356 -1.1505797 -1.2744142 -1.352622 -1.3203766 -1.1512827 -0.82721126][-1.2483339 -1.4835835 -1.5260259 -1.3887321 -0.9662118 -0.63211471 -0.45757276 -0.19869502 0.027829371 0.030651361 -0.19667061 -0.47521171 -0.59951997 -0.64113396 -0.6666193][-0.74419308 -0.791483 -0.70819306 -0.5285188 -0.18519476 0.22453648 0.60177231 0.68334448 0.67408758 0.66090703 0.49529445 0.23986202 0.0013275892 -0.10124335 -0.15191343][-0.15284474 -0.14847222 -0.0034283921 0.25482678 0.50328046 0.79419863 1.1124071 1.2463593 1.3155363 1.1340332 0.74832886 0.36303702 -0.043741416 -0.17081243 -0.14872217][0.42455941 0.58660591 0.70232391 0.82760769 0.98327357 1.1931149 1.4065971 1.4427468 1.3841243 1.175725 0.73368031 0.19557002 -0.24875292 -0.530797 -0.5336374][0.30934542 0.650131 1.0150797 1.1651152 1.1719549 1.1826652 1.2257195 1.1882044 1.072992 0.71993107 0.18240771 -0.43789625 -0.85474324 -1.1119381 -1.0928677][-0.13279268 0.026808672 0.32126841 0.51232249 0.56073374 0.56297237 0.52409649 0.38442519 0.10284857 -0.32365751 -0.77968317 -1.349792 -1.7422706 -1.8627338 -1.7538478][-0.54073745 -0.54662424 -0.47786325 -0.47050482 -0.4860687 -0.49176893 -0.47656849 -0.48677152 -0.72683555 -1.1625111 -1.6930289 -2.1878877 -2.3139651 -2.1622055 -1.7337123][-0.81660187 -1.0430462 -1.2322707 -1.3199353 -1.3279059 -1.3104836 -1.2388505 -1.179872 -1.1462536 -1.3083631 -1.6512871 -1.8977172 -1.8540455 -1.5622317 -1.0948077][-0.71434772 -1.0299824 -1.3946663 -1.7418551 -1.9746593 -2.0038009 -1.7480711 -1.4082048 -1.1098773 -0.96966404 -0.88493335 -0.89248258 -0.82646006 -0.38566464 0.28219363][-0.52369338 -0.71886945 -0.92657316 -1.214031 -1.4760647 -1.6778661 -1.6314925 -1.2222306 -0.75170684 -0.38391811 -0.18106049 -0.019909464 0.32428685 0.68846828 1.1035969][-0.24505749 -0.38518476 -0.550157 -0.74942029 -0.906608 -1.0227156 -1.0019962 -0.75140476 -0.33614382 0.073377542 0.3405188 0.41097003 0.46019274 0.63653076 0.86949766]]...]
INFO - root - 2017-12-06 09:13:38.353828: step 42510, loss = 0.79, batch loss = 0.58 (11.8 examples/sec; 0.675 sec/batch; 54h:24m:31s remains)
INFO - root - 2017-12-06 09:13:44.751584: step 42520, loss = 0.75, batch loss = 0.54 (12.7 examples/sec; 0.629 sec/batch; 50h:41m:14s remains)
INFO - root - 2017-12-06 09:13:51.378490: step 42530, loss = 0.74, batch loss = 0.52 (12.1 examples/sec; 0.663 sec/batch; 53h:25m:03s remains)
INFO - root - 2017-12-06 09:13:57.858792: step 42540, loss = 0.85, batch loss = 0.63 (11.7 examples/sec; 0.685 sec/batch; 55h:08m:47s remains)
INFO - root - 2017-12-06 09:14:04.236616: step 42550, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 52h:34m:13s remains)
INFO - root - 2017-12-06 09:14:10.759835: step 42560, loss = 0.77, batch loss = 0.55 (12.0 examples/sec; 0.665 sec/batch; 53h:31m:22s remains)
INFO - root - 2017-12-06 09:14:17.300622: step 42570, loss = 0.81, batch loss = 0.60 (11.3 examples/sec; 0.710 sec/batch; 57h:10m:58s remains)
INFO - root - 2017-12-06 09:14:23.771215: step 42580, loss = 0.82, batch loss = 0.60 (12.0 examples/sec; 0.664 sec/batch; 53h:28m:07s remains)
INFO - root - 2017-12-06 09:14:30.349686: step 42590, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.640 sec/batch; 51h:30m:38s remains)
INFO - root - 2017-12-06 09:14:36.842466: step 42600, loss = 0.78, batch loss = 0.57 (11.9 examples/sec; 0.670 sec/batch; 53h:55m:53s remains)
2017-12-06 09:14:37.464514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.55368179 -0.74978608 -1.0293115 -1.1506938 -1.1154978 -1.0298343 -0.95406139 -0.75575042 -0.62470722 -0.55270469 -0.39582586 -0.32272223 -0.39092809 -0.24193788 0.020013563][-0.69505596 -0.78038287 -1.0515316 -1.3221145 -1.5160795 -1.6371784 -1.7941312 -1.7669189 -1.6303835 -1.3273344 -1.1060528 -0.9303959 -0.63666779 -0.434139 -0.24262804][-0.59091383 -0.49428576 -0.41319662 -0.80581939 -1.2760683 -1.572365 -1.8107809 -2.1427791 -2.2897427 -1.9970419 -1.6788398 -1.693809 -1.5156726 -1.0820371 -0.5914014][-0.72185296 -0.63153553 -0.46963784 -0.56144369 -0.90178865 -1.3130496 -1.8368894 -2.2378826 -2.4810534 -2.6165335 -2.4844437 -2.170078 -1.9678345 -1.8139386 -1.4319218][-1.0306976 -0.86700135 -0.63403642 -0.55125558 -0.518012 -0.72510791 -1.2370751 -2.0566344 -2.9438887 -3.3936126 -3.4617493 -3.2451749 -2.8203504 -2.39832 -2.0087531][-0.5855481 -0.68343365 -0.45127514 0.032205775 0.49050522 0.51222396 0.064857759 -0.70365191 -1.6609677 -2.7484951 -3.5266635 -3.4882076 -3.2127142 -2.8731611 -2.3945663][-0.24384376 -0.11947184 0.0936094 0.64193481 1.1769906 1.5568681 1.4565306 0.75935608 -0.31069359 -1.1989763 -1.7934197 -2.3073416 -2.5865507 -2.2783206 -1.8313906][0.75001413 0.72369027 0.95388764 1.5757868 2.1748197 2.4698167 2.4646506 2.0040276 1.2244538 0.46195215 -0.19019781 -0.6447193 -0.78131396 -0.95467377 -0.89468145][0.90256774 1.2222656 1.7039672 2.1750712 2.8019614 3.3537264 3.3573086 3.0095937 2.5127728 1.8483994 1.2659005 0.8930046 0.68169093 0.59892029 0.73043418][0.64949167 0.46791041 0.40902439 0.84675777 1.3679856 1.7086623 1.9579786 2.1812558 2.2382503 2.0900111 1.9181279 1.7272335 1.4928792 1.4517051 1.4182525][-0.54435331 -0.75849319 -0.89448154 -1.0964125 -1.1158485 -0.85907543 -0.542373 -0.22321746 0.23665971 0.80152208 1.2163854 1.4283797 1.4831818 1.4987524 1.4880024][-1.0887904 -1.4547026 -1.8870291 -2.1716115 -2.4402869 -2.7538133 -2.7538533 -2.6014042 -2.121495 -1.390866 -0.68345541 0.00732702 0.43563414 0.7150017 0.86475354][-1.4002992 -1.6190233 -1.9119265 -2.5410473 -3.1829638 -3.5075867 -3.6972988 -3.8249712 -3.5235379 -3.0117986 -2.2616224 -1.3971833 -0.73953152 -0.13435411 0.26042983][-0.79729331 -0.96662456 -1.4363031 -1.9510872 -2.3195302 -2.7998238 -3.3009198 -3.4843709 -3.6307702 -3.3702877 -2.9009223 -2.3013353 -1.8729181 -1.3946159 -0.8505519][0.19280881 -0.073171519 -0.49597898 -0.949029 -1.5032623 -1.9177375 -2.1838889 -2.4744046 -2.6914968 -2.6752851 -2.6552682 -2.5864534 -2.1902955 -1.8257334 -1.4171325]]...]
INFO - root - 2017-12-06 09:14:43.953471: step 42610, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.649 sec/batch; 52h:16m:45s remains)
INFO - root - 2017-12-06 09:14:50.468757: step 42620, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.656 sec/batch; 52h:49m:23s remains)
INFO - root - 2017-12-06 09:14:56.942627: step 42630, loss = 0.83, batch loss = 0.62 (12.8 examples/sec; 0.625 sec/batch; 50h:17m:47s remains)
INFO - root - 2017-12-06 09:15:03.314132: step 42640, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 52h:46m:39s remains)
INFO - root - 2017-12-06 09:15:09.854556: step 42650, loss = 0.84, batch loss = 0.63 (12.0 examples/sec; 0.669 sec/batch; 53h:51m:35s remains)
INFO - root - 2017-12-06 09:15:16.377078: step 42660, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.641 sec/batch; 51h:34m:53s remains)
INFO - root - 2017-12-06 09:15:22.937017: step 42670, loss = 0.80, batch loss = 0.59 (11.5 examples/sec; 0.698 sec/batch; 56h:09m:38s remains)
INFO - root - 2017-12-06 09:15:29.500804: step 42680, loss = 0.88, batch loss = 0.67 (12.3 examples/sec; 0.653 sec/batch; 52h:33m:47s remains)
INFO - root - 2017-12-06 09:15:36.024109: step 42690, loss = 0.81, batch loss = 0.59 (12.7 examples/sec; 0.630 sec/batch; 50h:42m:57s remains)
INFO - root - 2017-12-06 09:15:42.547449: step 42700, loss = 0.78, batch loss = 0.57 (12.3 examples/sec; 0.650 sec/batch; 52h:21m:21s remains)
2017-12-06 09:15:43.177764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027976982 -0.029656816 -0.035622843 -0.045324713 -0.057011619 -0.068220153 -0.076169908 -0.078169219 -0.074173018 -0.065377012 -0.053818252 -0.042835448 -0.035258491 -0.031555638 -0.03190548][-0.01726225 -0.018927827 -0.025846183 -0.037284866 -0.051326115 -0.065261617 -0.075441286 -0.078551322 -0.074352466 -0.064189881 -0.050660703 -0.037573278 -0.028592862 -0.024423435 -0.025404535][-0.0089192837 -0.010843217 -0.018426985 -0.030780163 -0.046153516 -0.061993711 -0.07418935 -0.078895733 -0.075410187 -0.065139756 -0.050928906 -0.036703072 -0.026582435 -0.022067994 -0.023612738][-0.0038552359 -0.0058280826 -0.013501458 -0.025774881 -0.041252233 -0.05775309 -0.07114803 -0.077493072 -0.075507119 -0.066082224 -0.052286211 -0.038048461 -0.027598374 -0.022897691 -0.024768643][-0.0026460513 -0.0043243393 -0.01108826 -0.02202677 -0.036445744 -0.052059647 -0.065346569 -0.072906569 -0.072870441 -0.065186262 -0.052907225 -0.039704178 -0.029677495 -0.025007829 -0.026781514][-0.0056920052 -0.0064435154 -0.011539236 -0.020239942 -0.032335814 -0.046208251 -0.058878276 -0.067275055 -0.068846807 -0.062955253 -0.0525285 -0.040571067 -0.031142373 -0.026796296 -0.028720189][-0.011904381 -0.011623427 -0.014642373 -0.020782456 -0.030026685 -0.041431349 -0.052861564 -0.061499014 -0.064492948 -0.060836293 -0.052392706 -0.042055838 -0.033423018 -0.029404644 -0.031247471][-0.020179272 -0.019120999 -0.020429552 -0.024254799 -0.03071842 -0.039619923 -0.049393594 -0.057469666 -0.061523177 -0.059947643 -0.053627305 -0.045194022 -0.037721649 -0.033982724 -0.035237][-0.029476758 -0.027971759 -0.028137013 -0.030353948 -0.034871046 -0.041642696 -0.049632624 -0.056782905 -0.0611581 -0.061137378 -0.056920551 -0.050458856 -0.044332337 -0.040845029 -0.041288674][-0.038874168 -0.037508298 -0.037019331 -0.038104612 -0.041186944 -0.046180457 -0.052356321 -0.058295637 -0.062482554 -0.063452974 -0.061043642 -0.05649922 -0.051722854 -0.048718356 -0.048484426][-0.047413539 -0.046338234 -0.045731314 -0.046080045 -0.047926858 -0.05131083 -0.055739745 -0.060181886 -0.063705169 -0.0651236 -0.064082041 -0.061277073 -0.058084618 -0.055781454 -0.055180598][-0.053594753 -0.052992065 -0.0526769 -0.052834343 -0.053843681 -0.055883218 -0.05869402 -0.06163967 -0.064107761 -0.065367326 -0.065222159 -0.063850611 -0.062042426 -0.060536787 -0.059927784][-0.057721209 -0.057374336 -0.0572586 -0.057378206 -0.057910606 -0.05893093 -0.060408123 -0.062047951 -0.063552126 -0.064511076 -0.064773217 -0.064319536 -0.063487366 -0.062706605 -0.062360264][-0.059920512 -0.059645295 -0.059584003 -0.059598058 -0.059799418 -0.060226098 -0.06086364 -0.0616167 -0.062354028 -0.062878475 -0.06311284 -0.063061289 -0.062794819 -0.06261304 -0.06261719][-0.060565837 -0.060292207 -0.060250379 -0.0602158 -0.0602495 -0.06035009 -0.060522724 -0.060750313 -0.060976766 -0.061142087 -0.06124372 -0.061299033 -0.0613131 -0.061434235 -0.061744064]]...]
INFO - root - 2017-12-06 09:15:49.559904: step 42710, loss = 0.87, batch loss = 0.65 (12.3 examples/sec; 0.651 sec/batch; 52h:26m:14s remains)
INFO - root - 2017-12-06 09:15:56.091688: step 42720, loss = 0.87, batch loss = 0.65 (12.8 examples/sec; 0.625 sec/batch; 50h:18m:36s remains)
INFO - root - 2017-12-06 09:16:02.580392: step 42730, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.661 sec/batch; 53h:13m:05s remains)
INFO - root - 2017-12-06 09:16:08.813657: step 42740, loss = 0.76, batch loss = 0.54 (12.3 examples/sec; 0.653 sec/batch; 52h:31m:15s remains)
INFO - root - 2017-12-06 09:16:15.305843: step 42750, loss = 0.79, batch loss = 0.57 (11.7 examples/sec; 0.682 sec/batch; 54h:52m:51s remains)
INFO - root - 2017-12-06 09:16:21.784704: step 42760, loss = 0.75, batch loss = 0.54 (12.4 examples/sec; 0.647 sec/batch; 52h:05m:48s remains)
INFO - root - 2017-12-06 09:16:28.291610: step 42770, loss = 0.92, batch loss = 0.71 (12.1 examples/sec; 0.662 sec/batch; 53h:14m:39s remains)
INFO - root - 2017-12-06 09:16:34.719165: step 42780, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.639 sec/batch; 51h:26m:42s remains)
INFO - root - 2017-12-06 09:16:41.216008: step 42790, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.630 sec/batch; 50h:40m:42s remains)
INFO - root - 2017-12-06 09:16:47.794968: step 42800, loss = 0.91, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 51h:17m:49s remains)
2017-12-06 09:16:48.406075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.098111913 -0.10972461 -0.13133016 -0.16522801 -0.20633322 -0.25503987 -0.29876047 -0.32881778 -0.33859956 -0.3318975 -0.31953913 -0.2986832 -0.26031989 -0.20340762 -0.14165156][-0.10074914 -0.11636086 -0.13924009 -0.16475686 -0.19626993 -0.23684698 -0.28248328 -0.32810539 -0.35956228 -0.37394333 -0.37519711 -0.34925073 -0.28804365 -0.19205864 -0.089134][-0.10453605 -0.12091784 -0.13751115 -0.13987991 -0.14554912 -0.15528855 -0.19565701 -0.25583988 -0.31461984 -0.35846645 -0.37874007 -0.34891444 -0.26742285 -0.13902527 -0.0013029277][-0.10730198 -0.12234825 -0.12933433 -0.10705739 -0.085221283 -0.055927321 -0.074455388 -0.11958708 -0.17895123 -0.22866181 -0.26776558 -0.26741344 -0.21679233 -0.10127877 0.029973343][-0.1075982 -0.11427437 -0.09777309 -0.03061365 0.051642478 0.15446399 0.18196484 0.15978774 0.091282248 0.0055752173 -0.08892522 -0.15250845 -0.16830361 -0.1293768 -0.062624671][-0.10387063 -0.10272013 -0.059303954 0.059398398 0.21051878 0.36018673 0.39988694 0.37521973 0.28512737 0.15576719 0.0086106881 -0.10617066 -0.16805735 -0.17083681 -0.14453295][-0.10096584 -0.10129392 -0.054651309 0.07068108 0.25826749 0.45697823 0.54133427 0.52623785 0.4116824 0.24677971 0.057003677 -0.10354061 -0.20418897 -0.23799953 -0.22944419][-0.099365339 -0.096148431 -0.042245708 0.085700095 0.27994812 0.48599419 0.60823762 0.62125015 0.50192308 0.28055927 0.031652182 -0.17097071 -0.28376514 -0.32698005 -0.32618141][-0.10400901 -0.11146819 -0.086574011 0.0019894168 0.17030561 0.37683621 0.51672685 0.53825653 0.44010213 0.23802307 0.0021666139 -0.19916354 -0.30661452 -0.35552192 -0.36158451][-0.10884469 -0.12937626 -0.14265221 -0.12427114 -0.037433792 0.11572962 0.26885584 0.33988151 0.30647132 0.18468463 0.011177406 -0.1430597 -0.22288235 -0.24444994 -0.23046657][-0.10868895 -0.13642916 -0.17554122 -0.21410333 -0.20784824 -0.13640486 -0.022428341 0.050711498 0.071800485 0.045759112 -0.030000355 -0.096568763 -0.11818399 -0.11985642 -0.099250346][-0.1049621 -0.12810519 -0.1712231 -0.22940576 -0.26600361 -0.25622308 -0.1927605 -0.13498066 -0.082969435 -0.049944546 -0.055866491 -0.058606416 -0.034728065 -0.014539629 -0.0047370717][-0.096256137 -0.10683873 -0.13014328 -0.17037764 -0.22090776 -0.25209981 -0.23039712 -0.20407018 -0.16162428 -0.10877416 -0.067500077 -0.013853125 0.049078733 0.10267669 0.13387555][-0.093535624 -0.092689715 -0.093386814 -0.111368 -0.15122655 -0.18865454 -0.20573223 -0.21109998 -0.19967917 -0.17544161 -0.13403444 -0.061989449 0.02969408 0.11733612 0.18141568][-0.092979908 -0.0915162 -0.0886526 -0.089111 -0.10066508 -0.12149195 -0.14118184 -0.16077571 -0.17172405 -0.17799498 -0.17284007 -0.13061406 -0.059961185 0.020953789 0.076235861]]...]
INFO - root - 2017-12-06 09:16:54.928863: step 42810, loss = 0.70, batch loss = 0.49 (12.4 examples/sec; 0.646 sec/batch; 51h:57m:29s remains)
INFO - root - 2017-12-06 09:17:01.416459: step 42820, loss = 0.76, batch loss = 0.54 (11.8 examples/sec; 0.675 sec/batch; 54h:20m:54s remains)
INFO - root - 2017-12-06 09:17:07.858103: step 42830, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.624 sec/batch; 50h:12m:11s remains)
INFO - root - 2017-12-06 09:17:14.313382: step 42840, loss = 0.86, batch loss = 0.65 (12.2 examples/sec; 0.656 sec/batch; 52h:49m:04s remains)
INFO - root - 2017-12-06 09:17:20.871045: step 42850, loss = 0.87, batch loss = 0.65 (12.0 examples/sec; 0.668 sec/batch; 53h:45m:06s remains)
INFO - root - 2017-12-06 09:17:27.374916: step 42860, loss = 0.75, batch loss = 0.53 (12.0 examples/sec; 0.667 sec/batch; 53h:42m:02s remains)
INFO - root - 2017-12-06 09:17:33.881454: step 42870, loss = 0.90, batch loss = 0.68 (12.0 examples/sec; 0.666 sec/batch; 53h:35m:32s remains)
INFO - root - 2017-12-06 09:17:40.257856: step 42880, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.642 sec/batch; 51h:40m:44s remains)
INFO - root - 2017-12-06 09:17:46.753671: step 42890, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.648 sec/batch; 52h:07m:27s remains)
INFO - root - 2017-12-06 09:17:53.270092: step 42900, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.647 sec/batch; 52h:00m:39s remains)
2017-12-06 09:17:53.898107: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.67318392 0.15934381 -0.49956021 -1.0927905 -1.6394565 -2.102911 -2.6122742 -2.9001331 -3.02778 -2.9459677 -2.7362924 -2.3809712 -2.2311833 -2.4711218 -2.5310352][1.2442455 0.47971341 -0.37044621 -1.1027563 -1.6861011 -2.442637 -2.8300326 -2.85637 -2.9493823 -3.0525653 -3.0073788 -2.883812 -2.5494311 -2.2224817 -2.2212646][0.80693054 0.29153749 -0.46365371 -1.4352913 -2.1486454 -2.5855536 -2.7823539 -2.9062948 -2.8715408 -2.9185951 -2.978739 -2.8500051 -2.5482748 -2.2546582 -2.1087582][0.45129219 -0.12324435 -0.59680665 -0.91466832 -1.4363906 -2.2590308 -2.721807 -2.5223186 -2.2754593 -2.3688307 -2.3957283 -2.3366854 -2.1544075 -1.8977315 -1.7668179][0.92259288 0.43732479 0.089100733 0.014863297 -0.51880121 -0.90108919 -1.277742 -1.5845113 -1.689792 -1.5188044 -1.4038447 -1.5165848 -1.3302237 -1.0343273 -1.1255609][1.2107236 0.95288825 0.86264575 0.54420984 0.13258807 -0.090817109 -0.22970493 0.0023041293 0.22363076 -0.076983504 -0.35682774 -0.19145033 0.03490223 -0.2861973 -0.56726885][1.613322 0.96939528 0.40953198 0.23976833 0.026591145 -0.2599408 -0.21924686 0.32624581 0.74807906 1.0219562 1.1233805 0.93975937 0.67534459 0.4429898 0.40871441][1.3604451 0.78584278 0.0742995 -0.65479445 -0.9367888 -0.76045597 -0.40503988 0.15189379 0.72206211 0.8550787 0.97626841 0.98459411 0.92403257 0.59500873 -0.035338636][0.3195554 -0.053610627 -0.54344952 -1.345064 -1.9460965 -1.766396 -1.1412598 -0.71815252 -0.39186266 -0.022402152 0.25133631 0.41279402 0.63456666 0.41681495 -0.14981814][-0.71722019 -1.0017818 -1.3743901 -1.8546424 -2.3112607 -2.313828 -2.1064034 -1.6193411 -1.2290974 -1.1028596 -0.976109 -0.53079116 -0.11128194 0.12158254 0.10146685][-1.0636544 -1.3587073 -1.5455368 -1.8787359 -2.1624784 -2.0744941 -1.9398605 -2.0799956 -2.17287 -1.9892305 -1.7959375 -1.6128738 -1.1230636 -0.41642317 -0.3089155][-1.1663452 -1.3725539 -1.3681108 -1.5747585 -1.6039219 -1.6842535 -1.6699791 -1.5792578 -1.7062362 -1.9714288 -2.0490279 -1.9462503 -1.2800837 -0.83640075 -0.60634243][-0.982041 -1.0527331 -0.92582941 -1.0443083 -1.0225576 -0.88944924 -0.79463851 -1.0291196 -1.1819032 -1.2068264 -1.1963103 -0.94538784 -0.72109795 -0.42101136 -0.19366282][-0.52330393 -0.52089119 -0.44008085 -0.35426468 -0.20489886 -0.28750759 -0.21719261 0.0827274 -0.082803771 -0.60924339 -0.52384943 0.0456385 0.019167483 0.25856224 0.10138461][0.76046216 0.52278459 0.088539511 0.081434205 0.03503117 0.081473127 0.13798819 -0.33154437 -0.8283031 -0.702863 -0.65016985 -0.63602686 -0.41426468 -0.31152749 -0.29832029]]...]
INFO - root - 2017-12-06 09:18:00.474251: step 42910, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.645 sec/batch; 51h:54m:33s remains)
INFO - root - 2017-12-06 09:18:06.875432: step 42920, loss = 0.76, batch loss = 0.55 (13.8 examples/sec; 0.579 sec/batch; 46h:33m:57s remains)
INFO - root - 2017-12-06 09:18:13.388775: step 42930, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.640 sec/batch; 51h:26m:53s remains)
INFO - root - 2017-12-06 09:18:19.958438: step 42940, loss = 0.87, batch loss = 0.66 (12.0 examples/sec; 0.665 sec/batch; 53h:30m:42s remains)
INFO - root - 2017-12-06 09:18:26.386727: step 42950, loss = 0.85, batch loss = 0.63 (12.2 examples/sec; 0.656 sec/batch; 52h:44m:55s remains)
INFO - root - 2017-12-06 09:18:32.947780: step 42960, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.639 sec/batch; 51h:21m:18s remains)
INFO - root - 2017-12-06 09:18:39.545137: step 42970, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.659 sec/batch; 52h:59m:08s remains)
INFO - root - 2017-12-06 09:18:46.005575: step 42980, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.643 sec/batch; 51h:41m:39s remains)
INFO - root - 2017-12-06 09:18:52.540822: step 42990, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.648 sec/batch; 52h:05m:07s remains)
INFO - root - 2017-12-06 09:18:59.041567: step 43000, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.643 sec/batch; 51h:43m:08s remains)
2017-12-06 09:18:59.680312: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058591589 -0.3340627 -0.59082019 -0.66437256 -0.90929186 -0.97974944 -1.1451658 -0.96472478 -0.73203278 -0.63377559 -0.33920398 -0.065627091 0.024211936 -0.0020153448 0.18304273][0.81900072 0.37244898 0.042694584 -0.24645051 -0.550358 -0.91218853 -1.2841623 -1.0747973 -0.8020556 -0.60852754 -0.27514231 0.1058068 0.31604126 0.36686009 0.38487223][1.0306412 0.83689153 0.49517378 0.14272077 -0.24184643 -0.57616353 -0.77570105 -0.86727035 -0.88785076 -0.65196848 -0.31874335 0.1058199 0.34246981 0.51847064 0.58935547][1.1187469 0.74824131 0.400883 0.012290537 -0.30376524 -0.62612617 -0.89329028 -0.919819 -0.81095684 -0.6276685 -0.37278867 0.10539272 0.357383 0.25609094 0.12122715][0.95093954 0.85971797 0.74922013 0.46734312 0.1628471 -0.20071274 -0.5101825 -0.79372692 -0.85157204 -0.6139617 -0.38113075 -0.16103208 -0.09155608 -0.18737364 -0.20656131][1.3834894 1.2684749 0.946548 0.63066125 0.40360749 0.052342758 -0.29324543 -0.616248 -0.65810966 -0.64795268 -0.68778777 -0.58887821 -0.49752584 -0.73452413 -1.0525346][1.3978031 1.3877888 1.078566 0.59919131 0.28945684 0.063214228 -0.052587263 -0.0969992 -0.20447615 -0.37865412 -0.50010759 -0.46879745 -0.57298076 -0.81621945 -0.96109021][1.4342535 1.1854998 0.74106932 0.4281396 0.2315625 0.015008993 -0.20919847 -0.25324333 -0.26188213 -0.25818706 -0.39496079 -0.38511562 -0.23290047 -0.47721767 -0.859622][1.0723228 0.93842077 0.6951189 0.22581476 -0.18544427 -0.23640154 -0.2666496 -0.30188859 -0.30037934 -0.29095587 -0.26135519 -0.0076552704 0.15039992 -0.14677875 -0.40699553][0.34008753 0.10059798 -0.096408188 -0.19767898 -0.39931631 -0.39465305 -0.47799888 -0.4585174 -0.34783313 -0.25118607 -0.20354255 0.21981335 0.42112771 0.31253308 0.20023611][-0.35174474 -0.59274387 -0.60955536 -0.60163355 -0.7462014 -0.64736116 -0.68881404 -0.66827595 -0.57004619 -0.47411916 -0.41507509 0.01709152 0.20779213 0.39129198 0.52114534][-0.60056043 -0.96535277 -0.97135139 -0.98696685 -1.0218316 -0.95372355 -1.0287328 -1.0274667 -0.89804471 -0.73567295 -0.6951052 -0.48333848 -0.41536885 -0.19803329 0.048429072][-0.64016426 -0.77283168 -0.80602872 -0.81425 -0.80908597 -0.76061916 -0.92182589 -0.93707776 -0.89000809 -0.7534802 -0.66890657 -0.63014424 -0.46326834 -0.1140862 0.23529249][-0.299984 -0.1626309 -0.15172544 -0.19275689 -0.22630963 -0.29265237 -0.38837168 -0.30955464 -0.24958356 -0.059717119 0.041731432 -0.073589176 -0.14163655 0.0527495 0.35706541][0.11202252 0.022641532 -0.060015127 -0.17927909 -0.31597143 -0.46436691 -0.51384652 -0.56859463 -0.54034913 -0.14417434 0.07920517 0.072325274 0.097747728 0.096540526 0.21187678]]...]
INFO - root - 2017-12-06 09:19:06.225461: step 43010, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.633 sec/batch; 50h:53m:20s remains)
INFO - root - 2017-12-06 09:19:12.636035: step 43020, loss = 0.80, batch loss = 0.59 (11.6 examples/sec; 0.692 sec/batch; 55h:37m:22s remains)
INFO - root - 2017-12-06 09:19:19.099628: step 43030, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.656 sec/batch; 52h:46m:18s remains)
INFO - root - 2017-12-06 09:19:25.689768: step 43040, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.643 sec/batch; 51h:41m:45s remains)
INFO - root - 2017-12-06 09:19:32.162426: step 43050, loss = 0.75, batch loss = 0.53 (11.9 examples/sec; 0.675 sec/batch; 54h:14m:13s remains)
INFO - root - 2017-12-06 09:19:38.635832: step 43060, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.644 sec/batch; 51h:48m:27s remains)
INFO - root - 2017-12-06 09:19:45.153806: step 43070, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.641 sec/batch; 51h:32m:54s remains)
INFO - root - 2017-12-06 09:19:51.697482: step 43080, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.651 sec/batch; 52h:22m:34s remains)
INFO - root - 2017-12-06 09:19:58.284644: step 43090, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.650 sec/batch; 52h:17m:16s remains)
INFO - root - 2017-12-06 09:20:04.889708: step 43100, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.667 sec/batch; 53h:36m:43s remains)
2017-12-06 09:20:05.470467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.083989054 -0.083865359 -0.083976433 -0.084126212 -0.084286176 -0.084388204 -0.08443296 -0.084411345 -0.084324807 -0.084207065 -0.084067747 -0.083939493 -0.083845235 -0.08377675 -0.084001794][-0.084083937 -0.084068857 -0.084453277 -0.085001759 -0.0856453 -0.086109638 -0.086393334 -0.08645723 -0.086176395 -0.085658692 -0.08495526 -0.084365644 -0.083986245 -0.0837675 -0.083888367][-0.084268175 -0.084451571 -0.0853101 -0.086563125 -0.08807002 -0.089467153 -0.090599984 -0.091210507 -0.0909689 -0.089839131 -0.088088572 -0.086313814 -0.084767938 -0.083997242 -0.083892137][-0.084338568 -0.084840395 -0.086195081 -0.087968983 -0.090204179 -0.0928085 -0.095540091 -0.096907988 -0.096255466 -0.094310537 -0.09160845 -0.089279748 -0.08648964 -0.0847178 -0.084121518][-0.084454417 -0.085204035 -0.087098695 -0.089708976 -0.094186276 -0.10109038 -0.10792895 -0.1099047 -0.10551828 -0.098328561 -0.0916873 -0.08926098 -0.087148264 -0.08519882 -0.08417289][-0.084750906 -0.085866928 -0.088407122 -0.091519415 -0.09614668 -0.10465914 -0.11544606 -0.12406942 -0.12436851 -0.11494094 -0.0999252 -0.091218255 -0.087175451 -0.085062087 -0.0839004][-0.084948726 -0.086183459 -0.087792352 -0.085010573 -0.073651619 -0.060310122 -0.05868911 -0.076301575 -0.10135305 -0.11499429 -0.10815336 -0.09629149 -0.088445954 -0.085052013 -0.084198795][-0.085290633 -0.0869686 -0.087718382 -0.07825233 -0.051998649 -0.020102754 -0.004125759 -0.01616919 -0.048218429 -0.081533857 -0.096096881 -0.09472122 -0.089079931 -0.085418753 -0.084418669][-0.08559566 -0.087914176 -0.090064161 -0.0847521 -0.067709073 -0.047470808 -0.0346147 -0.033861976 -0.049009085 -0.073381767 -0.088208184 -0.090029933 -0.086175516 -0.083609268 -0.084029809][-0.085491821 -0.08751189 -0.089595377 -0.087217405 -0.079204269 -0.071374379 -0.067742668 -0.0665776 -0.071803257 -0.083275273 -0.090073436 -0.090163961 -0.087043613 -0.084164195 -0.0837007][-0.085167937 -0.086467296 -0.088009268 -0.08588393 -0.078949243 -0.072029717 -0.07042136 -0.072299838 -0.076833844 -0.083770886 -0.08707352 -0.086720034 -0.084969968 -0.083960339 -0.084042028][-0.084933661 -0.085995123 -0.087464824 -0.085744254 -0.0790045 -0.071893454 -0.069558412 -0.072112538 -0.078658409 -0.085252106 -0.086471029 -0.085339516 -0.0843648 -0.084054232 -0.084078655][-0.084494978 -0.085154824 -0.086748183 -0.086726904 -0.083045855 -0.077850617 -0.074969471 -0.076890804 -0.081613764 -0.085145794 -0.0854681 -0.084428929 -0.084033921 -0.083804637 -0.083884664][-0.084186472 -0.08429087 -0.085203774 -0.086305328 -0.086297564 -0.084665231 -0.082317591 -0.082053632 -0.083326831 -0.084243119 -0.08432921 -0.0840057 -0.083799504 -0.083671115 -0.083823048][-0.084177993 -0.083893679 -0.084009819 -0.084439 -0.0851134 -0.0853799 -0.084193982 -0.083098449 -0.0837096 -0.084092319 -0.083729 -0.083807543 -0.083754063 -0.083697528 -0.083887324]]...]
INFO - root - 2017-12-06 09:20:11.924230: step 43110, loss = 0.79, batch loss = 0.58 (11.7 examples/sec; 0.685 sec/batch; 55h:02m:19s remains)
INFO - root - 2017-12-06 09:20:18.481625: step 43120, loss = 0.85, batch loss = 0.64 (12.6 examples/sec; 0.637 sec/batch; 51h:11m:54s remains)
INFO - root - 2017-12-06 09:20:24.998939: step 43130, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.672 sec/batch; 54h:02m:20s remains)
INFO - root - 2017-12-06 09:20:31.435651: step 43140, loss = 0.85, batch loss = 0.64 (12.5 examples/sec; 0.640 sec/batch; 51h:27m:26s remains)
INFO - root - 2017-12-06 09:20:38.010108: step 43150, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.669 sec/batch; 53h:45m:50s remains)
INFO - root - 2017-12-06 09:20:44.372474: step 43160, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.644 sec/batch; 51h:45m:10s remains)
INFO - root - 2017-12-06 09:20:50.803017: step 43170, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.636 sec/batch; 51h:09m:11s remains)
INFO - root - 2017-12-06 09:20:57.360273: step 43180, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.641 sec/batch; 51h:28m:55s remains)
INFO - root - 2017-12-06 09:21:03.843336: step 43190, loss = 0.79, batch loss = 0.58 (12.8 examples/sec; 0.627 sec/batch; 50h:24m:07s remains)
INFO - root - 2017-12-06 09:21:10.328274: step 43200, loss = 0.80, batch loss = 0.59 (12.1 examples/sec; 0.663 sec/batch; 53h:17m:32s remains)
2017-12-06 09:21:10.885406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0674482 -0.06601043 -0.065013058 -0.064135723 -0.063554749 -0.062638029 -0.062615655 -0.063243464 -0.063457139 -0.062941089 -0.062502146 -0.062093832 -0.062249765 -0.062834688 -0.063614525][-0.087851316 -0.090058722 -0.093332678 -0.093298875 -0.08743988 -0.07863272 -0.074693374 -0.073797636 -0.070116766 -0.0656099 -0.06346108 -0.063003786 -0.06336832 -0.063907258 -0.063600615][-0.10289351 -0.10831892 -0.11965232 -0.11794665 -0.0934614 -0.069018275 -0.068787746 -0.075846933 -0.07290563 -0.067128055 -0.064828984 -0.064288296 -0.063743532 -0.063564762 -0.064591266][-0.046237785 -0.055858485 -0.084194809 -0.092689618 -0.061110869 -0.036143377 -0.053226165 -0.073563173 -0.07472381 -0.06835369 -0.063599646 -0.06017828 -0.058626287 -0.059752449 -0.060977869][0.085708432 0.080333509 0.012879908 -0.044161569 -0.038033981 -0.02547814 -0.04885865 -0.069687314 -0.072656244 -0.069271632 -0.064927883 -0.060522176 -0.055377923 -0.053884465 -0.055071548][0.16954765 0.15856951 0.038951986 -0.084765367 -0.12440825 -0.11901862 -0.11085866 -0.096059494 -0.081579365 -0.073586471 -0.069171526 -0.0660183 -0.060290568 -0.053745344 -0.049267624][0.2680428 0.25288117 0.12069755 -0.01885622 -0.095355138 -0.12299015 -0.11902386 -0.098900959 -0.082729816 -0.075330332 -0.0710347 -0.068013616 -0.0649866 -0.057511359 -0.052958593][0.28350109 0.26204216 0.15675524 0.04304307 -0.040834516 -0.087587625 -0.095199533 -0.086177692 -0.078417286 -0.074059732 -0.071019329 -0.068364456 -0.065640859 -0.063544467 -0.062454462][0.11453969 0.10083935 0.047624193 -0.0206085 -0.078011826 -0.10484477 -0.097381249 -0.084564336 -0.077749781 -0.074030675 -0.070632204 -0.067798 -0.065089092 -0.0639908 -0.063189186][-0.083082043 -0.066313945 -0.061446242 -0.087536722 -0.11400198 -0.11561646 -0.097066574 -0.081854425 -0.074527964 -0.071390308 -0.06891185 -0.066111341 -0.063287728 -0.062034905 -0.06152124][-0.14576983 -0.12563983 -0.098499522 -0.10019766 -0.10425923 -0.096175022 -0.086383894 -0.079072848 -0.071790606 -0.068652764 -0.066288546 -0.0638496 -0.061257575 -0.060044743 -0.059570059][-0.15865037 -0.15518877 -0.1353384 -0.11611892 -0.091614686 -0.07535626 -0.067903876 -0.063796259 -0.061362945 -0.061776467 -0.062289067 -0.061072327 -0.059388865 -0.058781531 -0.058903363][-0.1727148 -0.17946933 -0.15866293 -0.12179963 -0.086908512 -0.068836823 -0.063486606 -0.057118423 -0.052721471 -0.055083685 -0.0577711 -0.057992533 -0.05758635 -0.057629827 -0.058440246][-0.15542382 -0.15325075 -0.12798089 -0.0992533 -0.07399819 -0.062773019 -0.063959219 -0.062464692 -0.057463109 -0.054834861 -0.055338938 -0.055794436 -0.05620439 -0.056857657 -0.058215987][-0.10358565 -0.096344776 -0.086264066 -0.070113763 -0.060440525 -0.053748649 -0.057154533 -0.060550332 -0.058830634 -0.055978019 -0.054478157 -0.055299457 -0.0562884 -0.057685956 -0.059497051]]...]
INFO - root - 2017-12-06 09:21:17.458535: step 43210, loss = 0.75, batch loss = 0.53 (11.8 examples/sec; 0.678 sec/batch; 54h:30m:10s remains)
INFO - root - 2017-12-06 09:21:23.955508: step 43220, loss = 0.86, batch loss = 0.64 (12.8 examples/sec; 0.626 sec/batch; 50h:20m:33s remains)
INFO - root - 2017-12-06 09:21:30.380699: step 43230, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.639 sec/batch; 51h:22m:50s remains)
INFO - root - 2017-12-06 09:21:36.920259: step 43240, loss = 0.71, batch loss = 0.50 (12.6 examples/sec; 0.635 sec/batch; 51h:01m:54s remains)
INFO - root - 2017-12-06 09:21:43.347890: step 43250, loss = 0.82, batch loss = 0.61 (12.7 examples/sec; 0.630 sec/batch; 50h:36m:24s remains)
INFO - root - 2017-12-06 09:21:49.941239: step 43260, loss = 0.73, batch loss = 0.52 (12.2 examples/sec; 0.656 sec/batch; 52h:41m:37s remains)
INFO - root - 2017-12-06 09:21:56.428886: step 43270, loss = 0.86, batch loss = 0.65 (12.4 examples/sec; 0.644 sec/batch; 51h:46m:05s remains)
INFO - root - 2017-12-06 09:22:02.986387: step 43280, loss = 0.72, batch loss = 0.51 (12.4 examples/sec; 0.647 sec/batch; 51h:59m:41s remains)
INFO - root - 2017-12-06 09:22:09.474082: step 43290, loss = 0.75, batch loss = 0.54 (12.6 examples/sec; 0.636 sec/batch; 51h:06m:28s remains)
INFO - root - 2017-12-06 09:22:15.829446: step 43300, loss = 0.76, batch loss = 0.55 (12.8 examples/sec; 0.625 sec/batch; 50h:13m:31s remains)
2017-12-06 09:22:16.446295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.074066356 -0.07366161 -0.073622368 -0.073600546 -0.07381209 -0.07417018 -0.07436242 -0.074207827 -0.074051365 -0.074149743 -0.074381113 -0.074520446 -0.074588105 -0.07466834 -0.074920461][-0.073198587 -0.072930291 -0.072722152 -0.071609773 -0.069105834 -0.065592334 -0.062673539 -0.061847605 -0.06462279 -0.069417529 -0.073332563 -0.075114176 -0.07514865 -0.074850455 -0.074842185][-0.073030166 -0.072263852 -0.070113316 -0.06259574 -0.046897925 -0.027711272 -0.011749558 -0.007862024 -0.019791648 -0.040534027 -0.061192 -0.072906978 -0.075439446 -0.07474862 -0.074973866][-0.073181979 -0.071453638 -0.066146225 -0.04952186 -0.01878918 0.016266316 0.043481827 0.049136475 0.031772912 0.00097631663 -0.036664646 -0.0638566 -0.0713572 -0.073283404 -0.075406417][-0.073299885 -0.073398814 -0.073091254 -0.063454717 -0.037981648 -0.0023873746 0.035824612 0.048695326 0.030062534 -0.00089898705 -0.036574047 -0.063910909 -0.07049603 -0.073232539 -0.075084232][-0.073994391 -0.076218978 -0.0833005 -0.087635808 -0.080795169 -0.058870606 -0.019640662 0.0042906553 -0.0076647922 -0.032778822 -0.056502603 -0.07593061 -0.078152418 -0.075816333 -0.075261876][-0.075004995 -0.077909358 -0.088452071 -0.10177419 -0.10962715 -0.10383082 -0.076238587 -0.056792263 -0.068984747 -0.090203561 -0.10402738 -0.10594465 -0.094830669 -0.082513325 -0.0759207][-0.076284386 -0.079012305 -0.088074081 -0.10385844 -0.11995683 -0.12747718 -0.11442924 -0.10389213 -0.11141618 -0.1239642 -0.13009655 -0.12361108 -0.10610435 -0.088118449 -0.078565896][-0.077550963 -0.079379216 -0.08342085 -0.095947757 -0.11266683 -0.13160816 -0.13222507 -0.12920374 -0.12897676 -0.1301131 -0.12783742 -0.11994801 -0.10630132 -0.091528028 -0.082579248][-0.081816763 -0.08118847 -0.081208572 -0.085855037 -0.093523107 -0.10926507 -0.11279245 -0.11512817 -0.11489784 -0.11248393 -0.10853289 -0.10363746 -0.096532322 -0.087538034 -0.081129581][-0.084649488 -0.082510471 -0.080712639 -0.079330787 -0.077695474 -0.078548506 -0.075514227 -0.078052342 -0.0797341 -0.082680665 -0.086219206 -0.088297434 -0.086809307 -0.0829378 -0.080322079][-0.086460143 -0.083863981 -0.081677839 -0.079727963 -0.075933218 -0.068889804 -0.059491388 -0.055965774 -0.059669219 -0.069039293 -0.077942811 -0.082632288 -0.082475223 -0.078653991 -0.076259278][-0.086763427 -0.084023975 -0.08157929 -0.080104284 -0.077130735 -0.0711554 -0.064387016 -0.060000256 -0.060638249 -0.066986173 -0.074355669 -0.079956546 -0.081315823 -0.07921955 -0.077170543][-0.084895276 -0.083197 -0.081099667 -0.080268845 -0.079292767 -0.078223288 -0.076102711 -0.07428287 -0.072459392 -0.074305058 -0.0775655 -0.079429656 -0.078950614 -0.0776214 -0.076645322][-0.085136078 -0.082895786 -0.080579713 -0.078754589 -0.078078419 -0.078805365 -0.079518825 -0.080137014 -0.075667977 -0.075736985 -0.077060804 -0.077948719 -0.07810989 -0.077385068 -0.076565467]]...]
INFO - root - 2017-12-06 09:22:23.008380: step 43310, loss = 0.87, batch loss = 0.66 (12.2 examples/sec; 0.655 sec/batch; 52h:36m:31s remains)
INFO - root - 2017-12-06 09:22:29.553790: step 43320, loss = 0.92, batch loss = 0.71 (12.4 examples/sec; 0.646 sec/batch; 51h:53m:55s remains)
INFO - root - 2017-12-06 09:22:36.050857: step 43330, loss = 0.85, batch loss = 0.63 (12.3 examples/sec; 0.651 sec/batch; 52h:18m:32s remains)
INFO - root - 2017-12-06 09:22:42.550174: step 43340, loss = 0.84, batch loss = 0.63 (12.2 examples/sec; 0.656 sec/batch; 52h:42m:16s remains)
INFO - root - 2017-12-06 09:22:49.015726: step 43350, loss = 0.70, batch loss = 0.49 (12.3 examples/sec; 0.650 sec/batch; 52h:14m:05s remains)
INFO - root - 2017-12-06 09:22:55.547265: step 43360, loss = 0.76, batch loss = 0.54 (12.3 examples/sec; 0.650 sec/batch; 52h:13m:05s remains)
INFO - root - 2017-12-06 09:23:02.065731: step 43370, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.642 sec/batch; 51h:31m:53s remains)
INFO - root - 2017-12-06 09:23:08.622530: step 43380, loss = 0.73, batch loss = 0.52 (12.1 examples/sec; 0.660 sec/batch; 53h:01m:13s remains)
INFO - root - 2017-12-06 09:23:14.990350: step 43390, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.649 sec/batch; 52h:09m:36s remains)
INFO - root - 2017-12-06 09:23:21.464752: step 43400, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.634 sec/batch; 50h:53m:47s remains)
2017-12-06 09:23:22.068841: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034075946 -0.029779822 -0.026414588 -0.024045669 -0.023103647 -0.023274638 -0.024663933 -0.026580349 -0.029050104 -0.031729206 -0.03413748 -0.035790365 -0.036575697 -0.036371507 -0.037087034][-0.032056615 -0.027322561 -0.023432001 -0.020348027 -0.018902801 -0.018813446 -0.019970417 -0.021884046 -0.024476916 -0.02745641 -0.030278534 -0.032407403 -0.033736072 -0.034217782 -0.034926541][-0.0314316 -0.026080102 -0.021554306 -0.017942242 -0.016092464 -0.0157789 -0.01679153 -0.018799379 -0.02150996 -0.024575256 -0.02763363 -0.030105602 -0.031985987 -0.033212624 -0.034403589][-0.031242192 -0.02536869 -0.020095699 -0.015849158 -0.013509065 -0.013128564 -0.014288813 -0.016510569 -0.019379511 -0.022510648 -0.025556386 -0.028126448 -0.030506447 -0.032365154 -0.034283884][-0.031933583 -0.025759287 -0.019836381 -0.014941141 -0.012146972 -0.011645161 -0.012983076 -0.015485331 -0.018495731 -0.021503776 -0.024191953 -0.026453108 -0.028811496 -0.030995127 -0.033672757][-0.033380132 -0.027015649 -0.020823359 -0.015654385 -0.01254116 -0.011899963 -0.013151646 -0.015669905 -0.018551707 -0.021103419 -0.023155972 -0.024770066 -0.026662707 -0.028864745 -0.032054819][-0.035425585 -0.029217735 -0.022983424 -0.017702773 -0.014347196 -0.013450608 -0.014442302 -0.016544409 -0.01896093 -0.020848081 -0.022195399 -0.023137696 -0.02449619 -0.026410416 -0.029911641][-0.038345646 -0.032580193 -0.026719593 -0.021510623 -0.017897733 -0.016546018 -0.016958587 -0.018355086 -0.019917831 -0.021008633 -0.021631926 -0.021894746 -0.022785299 -0.024243526 -0.027643181][-0.042139288 -0.037022188 -0.031767637 -0.026813827 -0.023086414 -0.021245889 -0.02096846 -0.021506205 -0.022144608 -0.022439331 -0.022362553 -0.022026598 -0.022424482 -0.023504153 -0.026488706][-0.046606626 -0.042332549 -0.037787177 -0.033329178 -0.029757682 -0.027674437 -0.026790522 -0.026585504 -0.026479609 -0.026026778 -0.025288925 -0.024297759 -0.024161495 -0.024805985 -0.027212322][-0.051628567 -0.0482354 -0.044604406 -0.040879037 -0.037755411 -0.0356641 -0.034421641 -0.033657718 -0.032994751 -0.032022487 -0.03074453 -0.029183373 -0.028483246 -0.028584603 -0.029981978][-0.056319907 -0.053934444 -0.051318776 -0.048539154 -0.04612634 -0.044329435 -0.043046966 -0.042030577 -0.041073628 -0.039821461 -0.038270876 -0.036382064 -0.035211653 -0.034680244 -0.034978956][-0.059841298 -0.058370173 -0.056783218 -0.055056 -0.053501025 -0.052217271 -0.051125076 -0.050132047 -0.049108382 -0.04782258 -0.046307966 -0.044474885 -0.043093719 -0.041939497 -0.041177642][-0.06183606 -0.060995847 -0.060209543 -0.059361804 -0.058593147 -0.057885844 -0.057175133 -0.056383498 -0.055521492 -0.054508209 -0.053329121 -0.051812962 -0.050435543 -0.048958652 -0.047609907][-0.06278833 -0.062279254 -0.06194362 -0.061621957 -0.061372422 -0.061111875 -0.060778931 -0.060331304 -0.059842307 -0.059271842 -0.058574479 -0.057503492 -0.056304011 -0.054808248 -0.053123746]]...]
INFO - root - 2017-12-06 09:23:28.622109: step 43410, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.665 sec/batch; 53h:26m:06s remains)
INFO - root - 2017-12-06 09:23:35.191215: step 43420, loss = 0.70, batch loss = 0.48 (11.4 examples/sec; 0.705 sec/batch; 56h:35m:32s remains)
INFO - root - 2017-12-06 09:23:41.710276: step 43430, loss = 0.89, batch loss = 0.68 (11.9 examples/sec; 0.673 sec/batch; 54h:00m:01s remains)
INFO - root - 2017-12-06 09:23:48.114132: step 43440, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.631 sec/batch; 50h:39m:51s remains)
INFO - root - 2017-12-06 09:23:54.654106: step 43450, loss = 0.90, batch loss = 0.68 (12.7 examples/sec; 0.629 sec/batch; 50h:31m:54s remains)
INFO - root - 2017-12-06 09:24:01.143263: step 43460, loss = 0.76, batch loss = 0.55 (12.4 examples/sec; 0.647 sec/batch; 51h:58m:51s remains)
INFO - root - 2017-12-06 09:24:07.666576: step 43470, loss = 0.83, batch loss = 0.62 (12.3 examples/sec; 0.652 sec/batch; 52h:20m:36s remains)
INFO - root - 2017-12-06 09:24:14.152344: step 43480, loss = 0.77, batch loss = 0.56 (12.7 examples/sec; 0.628 sec/batch; 50h:25m:23s remains)
INFO - root - 2017-12-06 09:24:20.501898: step 43490, loss = 0.93, batch loss = 0.71 (12.0 examples/sec; 0.667 sec/batch; 53h:30m:47s remains)
INFO - root - 2017-12-06 09:24:27.027700: step 43500, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.643 sec/batch; 51h:38m:11s remains)
2017-12-06 09:24:27.678773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10183243 -0.10145484 -0.10011138 -0.098223716 -0.095476046 -0.09266 -0.088998675 -0.085454307 -0.082844391 -0.081029914 -0.080579318 -0.0844157 -0.090954714 -0.10074742 -0.11119898][-0.10021789 -0.099050529 -0.097175442 -0.094166063 -0.09030284 -0.086250789 -0.081189841 -0.076780058 -0.073238045 -0.071808577 -0.07287892 -0.079214595 -0.090466104 -0.10578816 -0.12171505][-0.097824484 -0.096012734 -0.092817657 -0.088270076 -0.08319246 -0.077673018 -0.071477205 -0.06628187 -0.062096141 -0.061655968 -0.065406166 -0.07606405 -0.093038984 -0.11414602 -0.13444024][-0.095978923 -0.0932121 -0.08901526 -0.082928345 -0.075909555 -0.068904057 -0.062006257 -0.056038983 -0.051823951 -0.052831847 -0.059656925 -0.075370915 -0.097985208 -0.12476356 -0.14826569][-0.09523578 -0.091375276 -0.085905246 -0.07824415 -0.0695683 -0.061051704 -0.05274136 -0.046736065 -0.043357618 -0.04580586 -0.05553031 -0.074976027 -0.10177065 -0.13369474 -0.16099089][-0.093607835 -0.089033671 -0.083042607 -0.074075259 -0.064121574 -0.05453958 -0.045788389 -0.039570618 -0.036587104 -0.041075345 -0.053050779 -0.074640118 -0.10340548 -0.13751435 -0.16724187][-0.091744557 -0.08671017 -0.080235623 -0.070724107 -0.060888484 -0.051125806 -0.042398635 -0.036289725 -0.034273658 -0.040328734 -0.053625576 -0.075198576 -0.10325798 -0.13625532 -0.16539738][-0.092053548 -0.086527921 -0.0790737 -0.06933102 -0.059638672 -0.050253477 -0.042350203 -0.03671864 -0.03599754 -0.04251929 -0.05571546 -0.075620145 -0.1001448 -0.12930498 -0.15690516][-0.094165571 -0.087986305 -0.080008432 -0.070296064 -0.060647372 -0.051227171 -0.043923825 -0.038877688 -0.039147533 -0.045369267 -0.057029732 -0.073654927 -0.093585193 -0.11827855 -0.14438654][-0.096323952 -0.089978777 -0.081536859 -0.071617037 -0.061838645 -0.052811429 -0.046110921 -0.041584041 -0.042330466 -0.047587439 -0.057025548 -0.069882944 -0.084924527 -0.10417729 -0.12697336][-0.099504024 -0.093128532 -0.084299989 -0.074017629 -0.063942268 -0.054855034 -0.048668113 -0.045100689 -0.0455859 -0.049377922 -0.05603236 -0.06517984 -0.075950339 -0.090203166 -0.10853434][-0.10434805 -0.097741835 -0.088187747 -0.077335775 -0.0663525 -0.056683052 -0.050562251 -0.046952058 -0.046858281 -0.049639326 -0.054228712 -0.060377471 -0.067700163 -0.077999443 -0.091826513][-0.11065637 -0.10365358 -0.09312243 -0.081058525 -0.0687439 -0.058087934 -0.051479749 -0.048017841 -0.047585029 -0.04900169 -0.052373923 -0.056802072 -0.061721206 -0.069078341 -0.079293638][-0.11655628 -0.10899683 -0.09810853 -0.085127965 -0.071731888 -0.059936255 -0.05236695 -0.048420157 -0.047493033 -0.048784 -0.051445071 -0.054473035 -0.05799057 -0.063097425 -0.070180222][-0.12136892 -0.11372381 -0.10272224 -0.089401685 -0.075595409 -0.062950663 -0.054438397 -0.049921453 -0.048296873 -0.049215209 -0.051385406 -0.053800415 -0.056409154 -0.06006413 -0.065170839]]...]
INFO - root - 2017-12-06 09:24:34.244185: step 43510, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.642 sec/batch; 51h:32m:53s remains)
INFO - root - 2017-12-06 09:24:40.816229: step 43520, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.643 sec/batch; 51h:36m:47s remains)
INFO - root - 2017-12-06 09:24:47.363564: step 43530, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.645 sec/batch; 51h:45m:57s remains)
INFO - root - 2017-12-06 09:24:53.880450: step 43540, loss = 0.78, batch loss = 0.57 (11.8 examples/sec; 0.680 sec/batch; 54h:34m:55s remains)
INFO - root - 2017-12-06 09:25:00.444723: step 43550, loss = 0.86, batch loss = 0.65 (11.7 examples/sec; 0.681 sec/batch; 54h:41m:37s remains)
INFO - root - 2017-12-06 09:25:06.909470: step 43560, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 52h:32m:55s remains)
INFO - root - 2017-12-06 09:25:13.518231: step 43570, loss = 0.86, batch loss = 0.65 (11.9 examples/sec; 0.675 sec/batch; 54h:10m:01s remains)
INFO - root - 2017-12-06 09:25:19.759613: step 43580, loss = 0.87, batch loss = 0.66 (13.7 examples/sec; 0.583 sec/batch; 46h:48m:33s remains)
INFO - root - 2017-12-06 09:25:26.291694: step 43590, loss = 0.86, batch loss = 0.65 (12.7 examples/sec; 0.631 sec/batch; 50h:38m:35s remains)
INFO - root - 2017-12-06 09:25:32.862693: step 43600, loss = 0.80, batch loss = 0.58 (11.7 examples/sec; 0.684 sec/batch; 54h:55m:02s remains)
2017-12-06 09:25:33.425296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.60533065 -0.75130624 -0.851042 -0.97264248 -1.1181531 -1.3010162 -1.4760904 -1.4853419 -1.5367917 -1.5358436 -1.4514822 -1.18934 -0.85113394 -0.44367814 -0.1066009][-1.2832992 -1.374555 -1.4134407 -1.6067901 -1.780298 -1.9332836 -2.1159544 -2.242429 -2.3230515 -2.2876976 -2.1779776 -1.8639191 -1.5409354 -1.2415785 -0.91462529][-1.688189 -1.7274823 -1.6712023 -1.6567061 -1.7113591 -1.952974 -2.172792 -2.3062606 -2.4405282 -2.4717875 -2.4820487 -2.3059704 -2.0455492 -1.8464009 -1.4456158][-1.5444491 -1.5112042 -1.4343489 -1.4349486 -1.454742 -1.4458228 -1.5180924 -1.7261184 -1.8524561 -1.8904275 -1.927905 -1.9000158 -1.7628553 -1.6085652 -1.4225713][-1.1080115 -0.87837213 -0.68671548 -0.60523373 -0.62557948 -0.67689979 -0.69254971 -0.74656796 -0.81797636 -0.98208183 -1.0479454 -1.0780962 -1.1189201 -1.0844148 -1.0357015][-0.25918752 -0.038222715 0.15144514 0.41824585 0.55760407 0.51469439 0.39890927 0.31225938 0.27580053 0.16418622 0.092386827 -0.021167994 -0.036106832 -0.2234515 -0.32786015][0.5445264 0.81187338 1.1358507 1.3119061 1.3744206 1.4990509 1.5091484 1.4547592 1.3609468 1.2445575 1.2060213 1.0686898 0.98643684 0.87439084 0.69810367][0.83417314 1.0897042 1.4216734 1.7322946 2.0314159 2.110348 2.1443357 2.2292283 2.2270203 2.2457626 2.2284744 2.1116655 2.0726943 1.8920511 1.6884438][0.25850469 0.47111624 0.81643784 1.208132 1.5613731 1.7916571 1.9980569 2.0870955 2.1284492 2.2417166 2.2256668 2.25035 2.222579 2.1843317 2.1344969][-0.087019376 -0.20957962 -0.1513451 0.035004735 0.28052646 0.48064506 0.67252165 0.88761628 1.1254272 1.2688128 1.3751328 1.4218067 1.4596647 1.5324433 1.6441197][-0.19840784 -0.45592022 -0.6812073 -0.8879025 -0.93349248 -0.89139503 -0.80572653 -0.70137435 -0.62912917 -0.51149505 -0.41889256 -0.22523545 -0.0098810717 0.14476591 0.32100904][0.20735504 -0.30200782 -0.87953347 -1.3079141 -1.6814097 -1.8954856 -1.9802725 -1.9799191 -1.9400294 -1.9472433 -1.9867706 -1.8627871 -1.7089103 -1.4069073 -1.0518134][0.50374669 -0.079829924 -0.70151353 -1.3177537 -1.9069979 -2.2745912 -2.5555482 -2.6734259 -2.7564695 -2.8333938 -2.9010603 -2.865181 -2.8050938 -2.6603339 -2.4515159][0.38782388 -0.17637911 -0.74704891 -1.2684498 -1.8061062 -2.2749569 -2.6543927 -2.8666284 -3.02158 -3.1136537 -3.202122 -3.2517579 -3.2740729 -3.1908388 -3.0889404][-0.29855394 -0.74743444 -1.0989205 -1.4893999 -1.8861153 -2.2397044 -2.5210669 -2.7354269 -2.8678939 -2.9781704 -3.0529506 -3.062782 -3.076561 -3.0020764 -2.9199247]]...]
INFO - root - 2017-12-06 09:25:40.067074: step 43610, loss = 0.83, batch loss = 0.62 (11.3 examples/sec; 0.705 sec/batch; 56h:34m:07s remains)
INFO - root - 2017-12-06 09:25:46.634566: step 43620, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.654 sec/batch; 52h:27m:28s remains)
INFO - root - 2017-12-06 09:25:53.166214: step 43630, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.625 sec/batch; 50h:07m:37s remains)
INFO - root - 2017-12-06 09:25:59.716388: step 43640, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.643 sec/batch; 51h:34m:45s remains)
INFO - root - 2017-12-06 09:26:06.262786: step 43650, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.660 sec/batch; 52h:55m:50s remains)
INFO - root - 2017-12-06 09:26:12.749622: step 43660, loss = 0.89, batch loss = 0.68 (12.3 examples/sec; 0.648 sec/batch; 52h:01m:47s remains)
INFO - root - 2017-12-06 09:26:19.183518: step 43670, loss = 0.76, batch loss = 0.55 (12.4 examples/sec; 0.645 sec/batch; 51h:43m:18s remains)
INFO - root - 2017-12-06 09:26:25.773393: step 43680, loss = 0.76, batch loss = 0.54 (12.7 examples/sec; 0.627 sec/batch; 50h:20m:31s remains)
INFO - root - 2017-12-06 09:26:32.211741: step 43690, loss = 0.82, batch loss = 0.60 (12.3 examples/sec; 0.651 sec/batch; 52h:14m:15s remains)
INFO - root - 2017-12-06 09:26:38.705796: step 43700, loss = 0.75, batch loss = 0.54 (12.5 examples/sec; 0.639 sec/batch; 51h:16m:23s remains)
2017-12-06 09:26:39.320784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061977997 -0.0663506 -0.071472771 -0.076380663 -0.080199182 -0.081973836 -0.081342749 -0.078889094 -0.075449228 -0.07182321 -0.068596907 -0.066102043 -0.064277545 -0.063031331 -0.062809989][-0.058896784 -0.061502114 -0.064855054 -0.068188615 -0.070835784 -0.071988687 -0.071270958 -0.069043122 -0.065893218 -0.062480178 -0.059340626 -0.056740593 -0.0546483 -0.053352475 -0.053435154][-0.056203015 -0.057271291 -0.058998249 -0.060962491 -0.062555134 -0.063264444 -0.062740929 -0.06109482 -0.05857848 -0.055595886 -0.052661534 -0.0499823 -0.047762804 -0.046472915 -0.046666153][-0.054043036 -0.0537942 -0.054191221 -0.054945253 -0.05578519 -0.056260075 -0.056067228 -0.055051398 -0.053234786 -0.050812304 -0.048209697 -0.045625463 -0.043442708 -0.042258427 -0.042575445][-0.052957937 -0.051724073 -0.051003996 -0.050692759 -0.05070623 -0.050817363 -0.050758459 -0.050239384 -0.049127437 -0.047382146 -0.045302711 -0.043076057 -0.041256506 -0.040456023 -0.040941421][-0.052624069 -0.050679833 -0.049114775 -0.047941174 -0.04716111 -0.046791993 -0.046741772 -0.046708256 -0.046377257 -0.045475431 -0.044126127 -0.042393059 -0.040873345 -0.040316343 -0.040928196][-0.05280989 -0.050450411 -0.048331328 -0.046530157 -0.045313481 -0.04476859 -0.045059524 -0.045722615 -0.046437304 -0.04656167 -0.04586222 -0.044342611 -0.042732958 -0.042052053 -0.042630911][-0.053317238 -0.050918963 -0.048663389 -0.046567764 -0.045123816 -0.044624016 -0.04532804 -0.046868093 -0.04865209 -0.049790729 -0.049669202 -0.048301749 -0.046567112 -0.045573715 -0.045926224][-0.053997781 -0.051898774 -0.049928375 -0.048043281 -0.046715077 -0.04640248 -0.047604196 -0.049949966 -0.052629873 -0.054722298 -0.05520492 -0.053984091 -0.052053269 -0.050665807 -0.05062072][-0.054950103 -0.05347117 -0.052084204 -0.050776988 -0.05000389 -0.050344836 -0.052324723 -0.055631615 -0.059652291 -0.062763706 -0.063726559 -0.06214758 -0.059438918 -0.057286095 -0.056674715][-0.056186315 -0.0554783 -0.054917309 -0.054340284 -0.054424118 -0.055649117 -0.05847111 -0.062710777 -0.067559093 -0.07134226 -0.072237052 -0.070194013 -0.066482171 -0.063350432 -0.062055085][-0.057463285 -0.057510149 -0.057776757 -0.057930995 -0.058871072 -0.0606691 -0.063953407 -0.068291612 -0.073080011 -0.07677111 -0.07752385 -0.075212896 -0.070978954 -0.067284569 -0.065494105][-0.058215935 -0.058819413 -0.059665345 -0.060283318 -0.061469205 -0.06311328 -0.066068791 -0.070142269 -0.074597746 -0.078225687 -0.079070017 -0.076910585 -0.072521448 -0.068327472 -0.066305414][-0.058024939 -0.058642313 -0.059542045 -0.060111523 -0.061052121 -0.062189884 -0.064429916 -0.067783974 -0.07180123 -0.075331107 -0.076489389 -0.074973315 -0.070798352 -0.066531859 -0.064544782][-0.057458222 -0.057567783 -0.057915565 -0.057849724 -0.058203854 -0.05848828 -0.059823751 -0.062401257 -0.065786868 -0.069097921 -0.070556618 -0.069837645 -0.066308074 -0.062409408 -0.06068442]]...]
INFO - root - 2017-12-06 09:26:45.806337: step 43710, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.639 sec/batch; 51h:17m:10s remains)
INFO - root - 2017-12-06 09:26:52.286418: step 43720, loss = 0.93, batch loss = 0.72 (12.8 examples/sec; 0.627 sec/batch; 50h:19m:02s remains)
INFO - root - 2017-12-06 09:26:58.849625: step 43730, loss = 0.78, batch loss = 0.56 (12.0 examples/sec; 0.669 sec/batch; 53h:38m:12s remains)
INFO - root - 2017-12-06 09:27:05.407341: step 43740, loss = 0.78, batch loss = 0.57 (13.1 examples/sec; 0.612 sec/batch; 49h:05m:41s remains)
INFO - root - 2017-12-06 09:27:11.947390: step 43750, loss = 0.81, batch loss = 0.59 (12.0 examples/sec; 0.666 sec/batch; 53h:26m:04s remains)
INFO - root - 2017-12-06 09:27:18.412644: step 43760, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.638 sec/batch; 51h:09m:43s remains)
INFO - root - 2017-12-06 09:27:24.924548: step 43770, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.658 sec/batch; 52h:45m:26s remains)
INFO - root - 2017-12-06 09:27:31.407262: step 43780, loss = 0.92, batch loss = 0.71 (12.2 examples/sec; 0.658 sec/batch; 52h:45m:20s remains)
INFO - root - 2017-12-06 09:27:37.924001: step 43790, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.653 sec/batch; 52h:20m:06s remains)
INFO - root - 2017-12-06 09:27:44.500851: step 43800, loss = 0.86, batch loss = 0.64 (12.6 examples/sec; 0.634 sec/batch; 50h:49m:43s remains)
2017-12-06 09:27:45.092098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031344533 -0.0351779 -0.041434396 -0.050256722 -0.060831331 -0.070915453 -0.077885404 -0.0791998 -0.075217173 -0.068016417 -0.059700467 -0.052532468 -0.047593929 -0.045504522 -0.045442171][-0.027116843 -0.030000281 -0.035537284 -0.043638851 -0.054032527 -0.064387 -0.072582066 -0.075377949 -0.072583213 -0.066390976 -0.058468156 -0.051263753 -0.046233609 -0.043692678 -0.043108251][-0.023490675 -0.024821013 -0.028763816 -0.035189703 -0.044045538 -0.053578231 -0.061587013 -0.065370932 -0.064285748 -0.059924439 -0.053625472 -0.047455594 -0.042779308 -0.040226862 -0.039458822][-0.021138921 -0.020687312 -0.022413552 -0.026282974 -0.032598212 -0.040113807 -0.047008779 -0.050863881 -0.05128492 -0.049070526 -0.044961337 -0.040491156 -0.036901336 -0.035023537 -0.034418378][-0.019907095 -0.017991908 -0.017659061 -0.018807292 -0.021797515 -0.026492931 -0.031952161 -0.035748672 -0.03719233 -0.036661688 -0.03462461 -0.032172516 -0.029803593 -0.02870132 -0.028513204][-0.019116446 -0.016313456 -0.014471084 -0.013235427 -0.013239481 -0.015125632 -0.018872268 -0.02251745 -0.024933651 -0.026189327 -0.025975384 -0.024914533 -0.023517512 -0.022964679 -0.023369044][-0.017711066 -0.014966726 -0.012614295 -0.010170221 -0.0082280561 -0.0078647286 -0.0097763687 -0.012945995 -0.016146131 -0.018790498 -0.020327173 -0.020846464 -0.020659961 -0.020791233 -0.021671541][-0.0157931 -0.013481975 -0.011333019 -0.008598946 -0.0057379529 -0.0039922 -0.0043800473 -0.00706283 -0.011067025 -0.015241623 -0.018751197 -0.021173067 -0.022649638 -0.023666933 -0.02500701][-0.014388032 -0.012501635 -0.010770641 -0.0082271919 -0.0051934123 -0.0028066039 -0.0023243129 -0.0044555962 -0.0088968351 -0.014371015 -0.019827835 -0.024365097 -0.027767412 -0.030283246 -0.032587674][-0.014786921 -0.013123579 -0.011661589 -0.0092772618 -0.0062913224 -0.003589049 -0.0024044961 -0.0039211661 -0.0084742978 -0.015168175 -0.022586249 -0.029418413 -0.035215277 -0.039861996 -0.043814193][-0.017461069 -0.015876904 -0.014508203 -0.012129709 -0.0090302378 -0.0059682503 -0.0041494891 -0.0050491914 -0.0093605 -0.016656406 -0.025626704 -0.034973752 -0.043654785 -0.05095439 -0.056933615][-0.021996096 -0.020347834 -0.018956646 -0.016561642 -0.013304852 -0.0098496154 -0.0074464083 -0.0075732544 -0.01115986 -0.018346511 -0.028436366 -0.040039036 -0.051429421 -0.061408184 -0.069457352][-0.02804438 -0.026260152 -0.02466163 -0.022202477 -0.018900976 -0.015222982 -0.012338862 -0.011651933 -0.014256544 -0.020785317 -0.030932363 -0.043339536 -0.056219127 -0.068028376 -0.077633232][-0.034868278 -0.03305985 -0.031269155 -0.028659619 -0.025252506 -0.021446288 -0.018210821 -0.016770735 -0.018303961 -0.023698665 -0.032830186 -0.044491339 -0.0571213 -0.069381163 -0.079722293][-0.041457344 -0.039929219 -0.038270965 -0.035683271 -0.032419913 -0.028779674 -0.025427304 -0.023359984 -0.023728102 -0.02754727 -0.034854513 -0.044578739 -0.055634886 -0.066899687 -0.076794706]]...]
INFO - root - 2017-12-06 09:27:51.613389: step 43810, loss = 0.75, batch loss = 0.54 (12.4 examples/sec; 0.646 sec/batch; 51h:49m:31s remains)
INFO - root - 2017-12-06 09:27:58.076587: step 43820, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.666 sec/batch; 53h:22m:10s remains)
INFO - root - 2017-12-06 09:28:04.543511: step 43830, loss = 0.81, batch loss = 0.60 (12.7 examples/sec; 0.632 sec/batch; 50h:39m:32s remains)
INFO - root - 2017-12-06 09:28:11.065029: step 43840, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.663 sec/batch; 53h:11m:32s remains)
INFO - root - 2017-12-06 09:28:17.482098: step 43850, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.646 sec/batch; 51h:49m:22s remains)
INFO - root - 2017-12-06 09:28:23.903777: step 43860, loss = 0.82, batch loss = 0.60 (12.7 examples/sec; 0.631 sec/batch; 50h:33m:16s remains)
INFO - root - 2017-12-06 09:28:30.347818: step 43870, loss = 0.83, batch loss = 0.61 (11.4 examples/sec; 0.700 sec/batch; 56h:07m:03s remains)
INFO - root - 2017-12-06 09:28:36.938733: step 43880, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 52h:20m:55s remains)
INFO - root - 2017-12-06 09:28:43.497412: step 43890, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.670 sec/batch; 53h:44m:08s remains)
INFO - root - 2017-12-06 09:28:50.055369: step 43900, loss = 0.89, batch loss = 0.68 (12.7 examples/sec; 0.631 sec/batch; 50h:33m:18s remains)
2017-12-06 09:28:50.663080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053310774 -0.052154355 -0.051363464 -0.050906461 -0.050870854 -0.051129386 -0.051308706 -0.051529862 -0.051622856 -0.05152173 -0.051285185 -0.051249005 -0.05156485 -0.052146561 -0.053354613][-0.052238453 -0.050063375 -0.04848136 -0.047479879 -0.047258325 -0.047443822 -0.0475978 -0.047767118 -0.047696989 -0.047215 -0.046378292 -0.045758605 -0.045775972 -0.046490591 -0.048302565][-0.052115344 -0.048443686 -0.045448404 -0.043451291 -0.042842492 -0.043037586 -0.043439332 -0.04397621 -0.044184856 -0.043485411 -0.041879468 -0.040417243 -0.040007163 -0.040848326 -0.043309398][-0.053971037 -0.048027784 -0.042954374 -0.039531577 -0.038021121 -0.03790985 -0.038706351 -0.039973371 -0.040945377 -0.040507317 -0.038515553 -0.036521368 -0.035977997 -0.037117917 -0.04037074][-0.057955578 -0.049295012 -0.041923612 -0.036853239 -0.0341806 -0.03361398 -0.034846921 -0.037135225 -0.039152808 -0.039403878 -0.037587918 -0.035568204 -0.035048556 -0.036631782 -0.040655632][-0.062819466 -0.051657584 -0.042147603 -0.035687923 -0.032009266 -0.031084891 -0.032892969 -0.036362424 -0.039576791 -0.040788572 -0.039720144 -0.038117707 -0.03786229 -0.039854228 -0.044291832][-0.066694856 -0.053812772 -0.042572133 -0.034948774 -0.030450169 -0.029144794 -0.031399783 -0.036117326 -0.040804565 -0.043370362 -0.043716591 -0.043378338 -0.043876454 -0.046277784 -0.050863653][-0.068367265 -0.055055737 -0.043239228 -0.034845967 -0.029601805 -0.02780883 -0.030001067 -0.035335165 -0.041107625 -0.045266457 -0.047657911 -0.049298737 -0.051200002 -0.054138564 -0.058909088][-0.067786552 -0.055531051 -0.044554763 -0.036437865 -0.030994359 -0.028821569 -0.030373242 -0.035352539 -0.041366067 -0.046698079 -0.051052239 -0.054773733 -0.058289222 -0.062144358 -0.06714426][-0.06520611 -0.055082608 -0.045938931 -0.039006084 -0.034076046 -0.032011803 -0.033108216 -0.03710141 -0.042438194 -0.048151456 -0.053676788 -0.05904071 -0.063847423 -0.068655767 -0.074041851][-0.061570115 -0.053878482 -0.047004975 -0.0416984 -0.037787113 -0.03601383 -0.036702465 -0.039867058 -0.044291273 -0.04961298 -0.055393867 -0.061554089 -0.067031637 -0.072276354 -0.077996][-0.057412826 -0.052142035 -0.047646724 -0.044089697 -0.041439787 -0.040076163 -0.040400874 -0.042723812 -0.04624429 -0.05090145 -0.056270957 -0.062382296 -0.068047106 -0.073405892 -0.079076611][-0.0540409 -0.050780021 -0.048362318 -0.046438068 -0.044972967 -0.044155791 -0.044249151 -0.04558818 -0.0479517 -0.051757943 -0.056302547 -0.061694935 -0.067074396 -0.072209716 -0.077686094][-0.05191116 -0.050040618 -0.04902662 -0.0483257 -0.047805987 -0.047458861 -0.047619995 -0.048375819 -0.049707 -0.052296892 -0.055726737 -0.060222 -0.065034211 -0.069810182 -0.074647114][-0.050981559 -0.04986896 -0.049591344 -0.049495719 -0.049581341 -0.049711142 -0.050054211 -0.050642975 -0.051486716 -0.053005088 -0.055219382 -0.058631323 -0.062383339 -0.066353872 -0.07038369]]...]
INFO - root - 2017-12-06 09:28:57.101973: step 43910, loss = 0.81, batch loss = 0.59 (11.6 examples/sec; 0.692 sec/batch; 55h:27m:23s remains)
INFO - root - 2017-12-06 09:29:03.590975: step 43920, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.663 sec/batch; 53h:08m:51s remains)
INFO - root - 2017-12-06 09:29:10.089219: step 43930, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.652 sec/batch; 52h:16m:54s remains)
INFO - root - 2017-12-06 09:29:16.536326: step 43940, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.645 sec/batch; 51h:40m:02s remains)
INFO - root - 2017-12-06 09:29:23.018559: step 43950, loss = 0.87, batch loss = 0.66 (11.7 examples/sec; 0.682 sec/batch; 54h:39m:11s remains)
INFO - root - 2017-12-06 09:29:29.489913: step 43960, loss = 0.83, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 52h:28m:20s remains)
INFO - root - 2017-12-06 09:29:36.036078: step 43970, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 51h:46m:43s remains)
INFO - root - 2017-12-06 09:29:42.512070: step 43980, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 51h:47m:55s remains)
INFO - root - 2017-12-06 09:29:48.995460: step 43990, loss = 0.73, batch loss = 0.52 (12.7 examples/sec; 0.632 sec/batch; 50h:40m:29s remains)
INFO - root - 2017-12-06 09:29:55.508571: step 44000, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.645 sec/batch; 51h:41m:32s remains)
2017-12-06 09:29:56.070809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054027405 -0.05417408 -0.054855831 -0.055990841 -0.057368346 -0.058667131 -0.059657022 -0.060231321 -0.060144626 -0.059418183 -0.058330845 -0.057488218 -0.057090811 -0.056960195 -0.057311956][-0.054321714 -0.054520894 -0.055487875 -0.056992203 -0.058754638 -0.060388591 -0.061550975 -0.062107198 -0.061743814 -0.060671039 -0.059195437 -0.057944074 -0.057245284 -0.056917489 -0.05714472][-0.0548323 -0.055024929 -0.056185305 -0.057961702 -0.060009539 -0.061861508 -0.063098088 -0.063591234 -0.063092753 -0.061777562 -0.060046777 -0.058494974 -0.057533056 -0.0570132 -0.057070196][-0.055271033 -0.055501852 -0.056741081 -0.058513694 -0.060525976 -0.06233228 -0.063467979 -0.063878283 -0.063391507 -0.062136106 -0.060429834 -0.058845632 -0.057779137 -0.057146132 -0.057093803][-0.055528942 -0.055822071 -0.057013541 -0.0586187 -0.060397226 -0.061959073 -0.062930465 -0.063298658 -0.062917441 -0.061883353 -0.06040512 -0.058971044 -0.057934508 -0.057268638 -0.057147138][-0.055669513 -0.055907156 -0.056958158 -0.058304828 -0.05979827 -0.061131313 -0.061970018 -0.062294155 -0.062032014 -0.061254583 -0.060116932 -0.0589432 -0.058013368 -0.057349313 -0.057268463][-0.055726096 -0.05578018 -0.056647357 -0.057756331 -0.058983117 -0.060081419 -0.0607562 -0.061034173 -0.060935639 -0.060427606 -0.059610009 -0.058745109 -0.057960767 -0.057386093 -0.057364684][-0.055599336 -0.05547474 -0.056162268 -0.056999646 -0.057953287 -0.058794614 -0.059356626 -0.059639648 -0.059667334 -0.059408132 -0.058919121 -0.0583428 -0.057747208 -0.057309248 -0.057407111][-0.055357661 -0.055120435 -0.055679563 -0.056281228 -0.057034027 -0.057707869 -0.058239263 -0.058538463 -0.058624305 -0.058527507 -0.058207121 -0.05782618 -0.057405416 -0.057142034 -0.057334282][-0.055177882 -0.054934889 -0.055412982 -0.055914294 -0.056564238 -0.057146359 -0.057602841 -0.057866234 -0.05795791 -0.057950653 -0.0577733 -0.057475042 -0.057140511 -0.057008345 -0.057289138][-0.055222753 -0.054919135 -0.05534374 -0.055800047 -0.056366291 -0.056833521 -0.057185903 -0.057387579 -0.05749964 -0.057568382 -0.0574516 -0.05724135 -0.057023514 -0.05699968 -0.057354107][-0.055265881 -0.05496414 -0.055340879 -0.055742111 -0.056212842 -0.056602091 -0.056874268 -0.057020284 -0.057140455 -0.057237584 -0.057183295 -0.057062179 -0.056975227 -0.057029456 -0.057443626][-0.0552436 -0.054931663 -0.055232368 -0.055549376 -0.055935394 -0.056244325 -0.056455787 -0.056572095 -0.056704238 -0.05681961 -0.056793105 -0.056798957 -0.056855354 -0.056993909 -0.057439923][-0.055145003 -0.054774664 -0.055047903 -0.055312485 -0.055608176 -0.055823684 -0.055960234 -0.056078665 -0.056193985 -0.056316536 -0.056352574 -0.0564559 -0.056641661 -0.056872368 -0.057378251][-0.055081449 -0.05462689 -0.054841544 -0.055077795 -0.055343397 -0.055511773 -0.055608105 -0.055728193 -0.055866905 -0.055974472 -0.05603642 -0.056191877 -0.056451116 -0.056767631 -0.057337139]]...]
INFO - root - 2017-12-06 09:30:02.456378: step 44010, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 52h:22m:33s remains)
INFO - root - 2017-12-06 09:30:09.079329: step 44020, loss = 0.77, batch loss = 0.55 (12.6 examples/sec; 0.635 sec/batch; 50h:54m:50s remains)
INFO - root - 2017-12-06 09:30:15.650573: step 44030, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.641 sec/batch; 51h:23m:59s remains)
INFO - root - 2017-12-06 09:30:22.159777: step 44040, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.661 sec/batch; 52h:55m:33s remains)
INFO - root - 2017-12-06 09:30:28.497136: step 44050, loss = 0.82, batch loss = 0.61 (12.5 examples/sec; 0.639 sec/batch; 51h:12m:04s remains)
INFO - root - 2017-12-06 09:30:35.065363: step 44060, loss = 0.68, batch loss = 0.47 (12.3 examples/sec; 0.649 sec/batch; 51h:58m:45s remains)
INFO - root - 2017-12-06 09:30:41.643351: step 44070, loss = 0.79, batch loss = 0.58 (11.9 examples/sec; 0.674 sec/batch; 53h:57m:59s remains)
INFO - root - 2017-12-06 09:30:48.091382: step 44080, loss = 0.90, batch loss = 0.68 (12.6 examples/sec; 0.634 sec/batch; 50h:45m:20s remains)
INFO - root - 2017-12-06 09:30:54.561424: step 44090, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.668 sec/batch; 53h:30m:58s remains)
INFO - root - 2017-12-06 09:31:01.030766: step 44100, loss = 0.85, batch loss = 0.63 (12.1 examples/sec; 0.661 sec/batch; 52h:58m:00s remains)
2017-12-06 09:31:01.631571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10514742 -0.11928175 -0.13926986 -0.17255579 -0.20596108 -0.21787913 -0.24412203 -0.28246751 -0.30044034 -0.29823208 -0.2374748 -0.1193885 0.013019368 0.093319952 0.072987288][-0.15460606 -0.17979312 -0.21458471 -0.26094362 -0.30284828 -0.31729591 -0.36817497 -0.45308751 -0.54229367 -0.59137291 -0.54452646 -0.4308787 -0.2979809 -0.21821131 -0.22712499][-0.14624527 -0.17101614 -0.21329325 -0.267201 -0.30198044 -0.30213833 -0.36010545 -0.46174783 -0.5836215 -0.67290604 -0.65810645 -0.56355077 -0.42987126 -0.35060388 -0.35318524][-0.10207443 -0.10782917 -0.14552948 -0.20371246 -0.24199821 -0.23417826 -0.26606131 -0.33028817 -0.43813628 -0.52953815 -0.52396441 -0.43281275 -0.30228084 -0.23944813 -0.27048212][-0.045471486 -0.023924373 -0.056896474 -0.1135978 -0.15728229 -0.15416455 -0.15584435 -0.17079568 -0.25678033 -0.37191451 -0.42907983 -0.40057832 -0.30505982 -0.22802597 -0.23423815][0.030832 0.11093581 0.13533138 0.13233638 0.11839998 0.13174886 0.17167725 0.18608122 0.075317547 -0.10912823 -0.25632763 -0.32222915 -0.29953378 -0.23882541 -0.22375545][0.012648262 0.075572044 0.11277449 0.15562794 0.21478997 0.30497706 0.44161016 0.54721469 0.49620646 0.32096535 0.14877775 0.011017427 -0.063668296 -0.07998573 -0.11041524][-0.13246076 -0.10116448 -0.064641111 -0.0026707873 0.10562083 0.26946962 0.49944818 0.68903285 0.70410675 0.56297052 0.40272498 0.22700234 0.088790536 0.0066902265 -0.05970303][-0.083738223 -0.0538823 -0.0045891479 0.067772582 0.17146195 0.28363383 0.43772012 0.58503848 0.58779985 0.45249003 0.32078296 0.17241575 0.046430975 -0.039633621 -0.095441245][-0.093445361 -0.07943321 -0.048958585 0.011141501 0.11655763 0.23842694 0.36853248 0.48233843 0.49404436 0.39677364 0.30501258 0.2283098 0.15659389 0.096430674 0.028988883][-0.11722195 -0.13145004 -0.1354306 -0.12082272 -0.068538904 -0.011841007 0.060389385 0.13828743 0.17274238 0.12250489 0.062926963 0.021501333 -0.015989318 -0.0188016 -0.02111546][-0.10456182 -0.12098186 -0.14849764 -0.17240568 -0.17428485 -0.15992267 -0.12116068 -0.078250021 -0.048694037 -0.066741683 -0.0919892 -0.097843371 -0.11098029 -0.10109472 -0.080236249][-0.097700782 -0.11333533 -0.14556041 -0.17494558 -0.19150802 -0.19492298 -0.18651962 -0.187878 -0.19395578 -0.24652641 -0.28594965 -0.26521367 -0.22701065 -0.17035012 -0.11926146][-0.087179609 -0.091328636 -0.10596444 -0.12384555 -0.13712808 -0.13961512 -0.12962197 -0.12726031 -0.11854486 -0.14143281 -0.15822515 -0.1657808 -0.15042126 -0.1127637 -0.079902858][-0.087116443 -0.087531865 -0.092157528 -0.097589217 -0.10283363 -0.10251348 -0.084394604 -0.08369948 -0.081980109 -0.11218616 -0.12960987 -0.1483988 -0.13732651 -0.12090641 -0.087810561]]...]
INFO - root - 2017-12-06 09:31:08.072322: step 44110, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 52h:27m:16s remains)
INFO - root - 2017-12-06 09:31:14.560353: step 44120, loss = 0.82, batch loss = 0.60 (12.1 examples/sec; 0.662 sec/batch; 53h:02m:05s remains)
INFO - root - 2017-12-06 09:31:21.050460: step 44130, loss = 0.84, batch loss = 0.62 (12.5 examples/sec; 0.641 sec/batch; 51h:19m:14s remains)
INFO - root - 2017-12-06 09:31:27.492211: step 44140, loss = 0.84, batch loss = 0.63 (12.0 examples/sec; 0.668 sec/batch; 53h:31m:15s remains)
INFO - root - 2017-12-06 09:31:33.903093: step 44150, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.635 sec/batch; 50h:49m:56s remains)
INFO - root - 2017-12-06 09:31:40.639463: step 44160, loss = 0.80, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 52h:38m:12s remains)
INFO - root - 2017-12-06 09:31:47.148942: step 44170, loss = 0.80, batch loss = 0.59 (12.5 examples/sec; 0.640 sec/batch; 51h:15m:04s remains)
INFO - root - 2017-12-06 09:31:53.635783: step 44180, loss = 0.76, batch loss = 0.54 (12.7 examples/sec; 0.628 sec/batch; 50h:19m:15s remains)
INFO - root - 2017-12-06 09:32:00.152183: step 44190, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.636 sec/batch; 50h:56m:39s remains)
INFO - root - 2017-12-06 09:32:06.531569: step 44200, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.642 sec/batch; 51h:24m:46s remains)
2017-12-06 09:32:07.148997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.39558208 -0.64784294 -0.99625921 -1.3873091 -1.7719744 -1.9258076 -1.7342316 -1.5399896 -1.3844359 -1.2662219 -1.1399528 -0.99859446 -0.77318823 -0.51453745 -0.31651518][-0.32692552 -0.7968384 -1.1726598 -1.6587799 -2.0269325 -2.3144066 -2.2849793 -2.1356356 -1.9364495 -1.7216507 -1.5718687 -1.3443911 -0.95566958 -0.56919569 -0.38229847][-0.64710534 -0.88780814 -1.0532336 -1.6045089 -2.1029894 -2.3342981 -2.2816038 -2.2512596 -2.1617348 -2.0044353 -1.8729467 -1.6286294 -1.3223625 -0.89276111 -0.54707217][-0.54112417 -0.77295262 -0.89065111 -1.1532454 -1.5776123 -1.8974156 -2.0171306 -1.9021794 -1.7699845 -1.7111129 -1.6179793 -1.4590135 -1.2601379 -1.026551 -0.76731294][-0.57034326 -0.85993922 -0.79270852 -0.7773186 -0.9136613 -1.0376786 -1.1156367 -1.2267829 -1.3196207 -1.1903759 -1.1278948 -1.1117308 -1.1271337 -0.98166323 -0.84177303][-0.02866821 -0.20029123 -0.31601971 -0.31487072 -0.20215416 -0.18977949 -0.15390307 -0.20651484 -0.2790826 -0.39295334 -0.57749093 -0.62440288 -0.76825345 -0.78581476 -0.81649393][0.69936168 0.72301108 0.5839448 0.54198784 0.57937926 0.59171784 0.52446014 0.50617474 0.50372839 0.3433544 0.19552414 -0.19039109 -0.61444885 -0.62780321 -0.58377904][1.1367428 0.99756694 1.1067702 1.1285392 1.0106848 0.92656529 0.87057292 0.82633257 0.68486637 0.58593035 0.48043823 0.073927924 -0.25662565 -0.49674863 -0.6689595][1.144856 1.0094253 1.0733088 0.9404242 0.77936381 0.53122771 0.41704828 0.3684178 0.34279841 0.20860992 0.079182148 -0.089445911 -0.22619498 -0.31100988 -0.33722645][0.055860475 0.13622437 0.23727788 0.056076065 -0.25433213 -0.46384954 -0.5249272 -0.62154382 -0.60766959 -0.59532547 -0.53342086 -0.60393286 -0.6369586 -0.4262408 -0.19874486][-1.2858412 -1.2537236 -1.1805395 -1.1750064 -1.334348 -1.5414016 -1.5881122 -1.50723 -1.4758725 -1.4207083 -1.2520188 -1.0844311 -0.85774845 -0.50431782 -0.29025549][-1.9435987 -2.0478046 -2.0396574 -2.1545694 -2.2567065 -2.2619271 -2.1174634 -2.0368149 -1.9225491 -1.693306 -1.4930842 -1.286212 -1.1394749 -0.88515496 -0.62122136][-1.9808475 -2.1345937 -2.2616618 -2.5269313 -2.5582664 -2.4265637 -2.254698 -2.1193442 -1.8919444 -1.7292749 -1.61364 -1.426513 -1.3400449 -1.1214355 -0.90656728][-1.7368019 -1.6160487 -1.863807 -2.0012026 -1.8682456 -1.8863676 -1.8268623 -1.7084183 -1.602316 -1.5850751 -1.471949 -1.3828652 -1.2610798 -1.092503 -0.97433436][-0.99969244 -1.1702976 -1.4861966 -1.7436075 -1.7686718 -1.6429205 -1.5965126 -1.573579 -1.5166066 -1.3936846 -1.302433 -1.1695819 -0.92523205 -0.73461562 -0.63211244]]...]
INFO - root - 2017-12-06 09:32:13.737011: step 44210, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.675 sec/batch; 54h:01m:26s remains)
INFO - root - 2017-12-06 09:32:20.312595: step 44220, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.632 sec/batch; 50h:35m:38s remains)
INFO - root - 2017-12-06 09:32:26.710271: step 44230, loss = 0.75, batch loss = 0.54 (12.6 examples/sec; 0.635 sec/batch; 50h:51m:08s remains)
INFO - root - 2017-12-06 09:32:33.272649: step 44240, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.663 sec/batch; 53h:03m:25s remains)
INFO - root - 2017-12-06 09:32:39.792037: step 44250, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.657 sec/batch; 52h:35m:06s remains)
INFO - root - 2017-12-06 09:32:46.442783: step 44260, loss = 0.87, batch loss = 0.65 (12.7 examples/sec; 0.630 sec/batch; 50h:26m:53s remains)
INFO - root - 2017-12-06 09:32:53.006764: step 44270, loss = 0.78, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 51h:05m:36s remains)
INFO - root - 2017-12-06 09:32:59.605297: step 44280, loss = 0.76, batch loss = 0.55 (11.6 examples/sec; 0.688 sec/batch; 55h:06m:23s remains)
INFO - root - 2017-12-06 09:33:06.066898: step 44290, loss = 0.74, batch loss = 0.53 (12.1 examples/sec; 0.659 sec/batch; 52h:46m:52s remains)
INFO - root - 2017-12-06 09:33:12.602323: step 44300, loss = 0.74, batch loss = 0.53 (12.6 examples/sec; 0.637 sec/batch; 50h:57m:38s remains)
2017-12-06 09:33:13.255311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051659644 -0.054134261 -0.061364431 -0.06570489 -0.057371244 -0.041178212 -0.031485666 -0.027889818 -0.028313596 -0.030532632 -0.036740635 -0.041540679 -0.04572843 -0.047378566 -0.048729379][-0.057095896 -0.061778724 -0.072766736 -0.080415279 -0.068687946 -0.048963156 -0.034894742 -0.0301666 -0.028358683 -0.024184175 -0.028196789 -0.0352583 -0.042120378 -0.045505449 -0.04781663][-0.065894313 -0.062936962 -0.067413472 -0.073235288 -0.05557166 -0.034257039 -0.024548434 -0.018653989 -0.020550728 -0.019883357 -0.025099948 -0.031380787 -0.039379403 -0.042690452 -0.042990316][-0.070761248 -0.063161455 -0.065994963 -0.07242807 -0.052021697 -0.023962907 -0.010165945 -0.0016224682 -0.0022134036 -0.0064523071 -0.016882636 -0.028771229 -0.0376965 -0.040295113 -0.037128355][-0.086419478 -0.079169206 -0.073996745 -0.067379564 -0.041403983 -0.0053818226 0.020624369 0.021432862 0.013037264 0.012073606 -0.0045479462 -0.020111531 -0.0341598 -0.04147961 -0.043105841][-0.10478475 -0.095169283 -0.079489246 -0.064237453 -0.031561166 0.0092852637 0.046360083 0.058357812 0.039669193 0.024620555 0.0064354837 -0.010989346 -0.0292859 -0.039097875 -0.041552059][-0.13065776 -0.11689958 -0.084994867 -0.060785811 -0.025730819 0.018150032 0.061607726 0.078692533 0.067136444 0.040031098 0.013056912 -0.0053999647 -0.018671878 -0.0338466 -0.039527781][-0.16166931 -0.14595769 -0.10710739 -0.06947533 -0.024615586 0.021025509 0.072825007 0.097095229 0.081801735 0.047840454 0.013757899 -0.0045114309 -0.013445087 -0.025719881 -0.036830429][-0.16476053 -0.14952758 -0.12266497 -0.09028665 -0.041737203 0.0010715425 0.04553204 0.06033022 0.043868072 0.012355417 -0.0066082329 -0.0078377873 -0.009338066 -0.016731605 -0.031948026][-0.17301561 -0.15905622 -0.13012499 -0.099839747 -0.051310569 -0.0074194446 0.022209778 0.020732939 -0.0026895478 -0.025851034 -0.034582183 -0.030245323 -0.020337626 -0.024569005 -0.037344884][-0.17653346 -0.15431002 -0.12135787 -0.091694936 -0.04870525 -0.02523835 -0.011424363 -0.015444569 -0.03221444 -0.045576077 -0.053393491 -0.047318149 -0.041682061 -0.03577926 -0.043391805][-0.1690408 -0.15236801 -0.11826038 -0.091097876 -0.060007073 -0.046492469 -0.035619725 -0.03447447 -0.047365103 -0.065812334 -0.078641824 -0.066746183 -0.04779543 -0.040514212 -0.04202633][-0.16653314 -0.15614963 -0.12614651 -0.09175954 -0.0574726 -0.042588633 -0.042335231 -0.050307233 -0.063262984 -0.080138654 -0.090475112 -0.079260111 -0.068326324 -0.0544889 -0.053762704][-0.13865791 -0.13774922 -0.11619095 -0.09577138 -0.070782624 -0.051123958 -0.053111941 -0.06158983 -0.074217379 -0.084611706 -0.093410805 -0.087733425 -0.074175835 -0.064949363 -0.059327163][-0.10229683 -0.10801955 -0.098894194 -0.085321411 -0.076495171 -0.05720713 -0.053192962 -0.052510716 -0.068284974 -0.079178713 -0.089460842 -0.0839306 -0.074249364 -0.064264894 -0.057398636]]...]
INFO - root - 2017-12-06 09:33:19.801479: step 44310, loss = 0.90, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 51h:22m:04s remains)
INFO - root - 2017-12-06 09:33:26.380000: step 44320, loss = 0.87, batch loss = 0.66 (11.9 examples/sec; 0.674 sec/batch; 53h:55m:53s remains)
INFO - root - 2017-12-06 09:33:32.832747: step 44330, loss = 0.78, batch loss = 0.57 (11.8 examples/sec; 0.678 sec/batch; 54h:14m:23s remains)
INFO - root - 2017-12-06 09:33:39.347501: step 44340, loss = 0.78, batch loss = 0.56 (12.8 examples/sec; 0.625 sec/batch; 49h:59m:24s remains)
INFO - root - 2017-12-06 09:33:45.948901: step 44350, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.652 sec/batch; 52h:09m:54s remains)
INFO - root - 2017-12-06 09:33:52.504769: step 44360, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.655 sec/batch; 52h:23m:14s remains)
INFO - root - 2017-12-06 09:33:58.936368: step 44370, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.641 sec/batch; 51h:19m:00s remains)
INFO - root - 2017-12-06 09:34:05.479102: step 44380, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.642 sec/batch; 51h:21m:21s remains)
INFO - root - 2017-12-06 09:34:12.011297: step 44390, loss = 0.76, batch loss = 0.55 (12.0 examples/sec; 0.667 sec/batch; 53h:22m:24s remains)
INFO - root - 2017-12-06 09:34:18.503300: step 44400, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 52h:07m:54s remains)
2017-12-06 09:34:19.121670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063510314 -0.06140345 -0.060018677 -0.059313327 -0.059141275 -0.059043609 -0.058965843 -0.058907639 -0.058842797 -0.058774628 -0.058698229 -0.058610633 -0.058517758 -0.058444388 -0.058673106][-0.063291535 -0.061093658 -0.059676807 -0.058962364 -0.058785245 -0.058689013 -0.058629729 -0.058603931 -0.058583204 -0.058558408 -0.058530495 -0.05848502 -0.058415651 -0.0583571 -0.058530442][-0.062965162 -0.060648538 -0.059173483 -0.058423065 -0.058217134 -0.058124069 -0.058100734 -0.058134664 -0.058192238 -0.058256019 -0.05830748 -0.058313575 -0.058274176 -0.058238186 -0.058356728][-0.062648632 -0.060245015 -0.058706138 -0.057908546 -0.05766407 -0.057563607 -0.057579122 -0.057677794 -0.057822477 -0.057987377 -0.058122192 -0.058185451 -0.058183733 -0.058169421 -0.058293812][-0.062472291 -0.060034364 -0.058435764 -0.05758005 -0.057286076 -0.0571399 -0.057147905 -0.057273746 -0.057481039 -0.057714246 -0.057925858 -0.058068462 -0.058124159 -0.058150891 -0.05832383][-0.062489223 -0.060011756 -0.058373336 -0.057468414 -0.057100326 -0.0568932 -0.056856558 -0.056964248 -0.057179891 -0.057446685 -0.057711851 -0.057919189 -0.058043953 -0.0581211 -0.058409318][-0.062578417 -0.060044527 -0.058408048 -0.05747943 -0.057068914 -0.056818556 -0.056718268 -0.056771845 -0.056950614 -0.057201806 -0.057480317 -0.05772315 -0.057899505 -0.058023039 -0.058427721][-0.062515914 -0.059947364 -0.058311075 -0.057393182 -0.057000116 -0.056753132 -0.056628224 -0.056640554 -0.05677123 -0.056979209 -0.057233565 -0.057476562 -0.057678565 -0.057845842 -0.058381792][-0.062248275 -0.059637465 -0.057970475 -0.0570569 -0.056712959 -0.056532335 -0.056449145 -0.056477714 -0.056600437 -0.056767683 -0.056977153 -0.05719728 -0.057408262 -0.057626687 -0.058255702][-0.061818972 -0.059147876 -0.057431187 -0.056527358 -0.056265526 -0.056200761 -0.0562174 -0.05631952 -0.056469165 -0.056629132 -0.056791965 -0.056966949 -0.057159077 -0.057393078 -0.058082197][-0.061472438 -0.058708295 -0.05695574 -0.056076709 -0.055912115 -0.055971548 -0.056094259 -0.056270912 -0.056458462 -0.056608245 -0.056712937 -0.056807596 -0.056912571 -0.057076357 -0.057739988][-0.061227556 -0.058448058 -0.056680869 -0.055821549 -0.055716421 -0.055856492 -0.056051649 -0.056263216 -0.05645933 -0.056597538 -0.056644477 -0.056637112 -0.056563105 -0.056577828 -0.057145312][-0.061097279 -0.058333591 -0.056575406 -0.055748969 -0.055670317 -0.055847954 -0.05606354 -0.056256875 -0.056408111 -0.056500204 -0.056488872 -0.056371998 -0.056083422 -0.055938274 -0.056382854][-0.061176986 -0.058462191 -0.056778204 -0.056009822 -0.055966973 -0.056142408 -0.056323789 -0.0564467 -0.056498151 -0.056503233 -0.056406081 -0.056159921 -0.05566502 -0.055351738 -0.055630438][-0.061666507 -0.0590644 -0.057511549 -0.056841258 -0.056843154 -0.0569822 -0.057081055 -0.057088096 -0.057022948 -0.0568996 -0.056688998 -0.056301218 -0.055577196 -0.055011317 -0.055097606]]...]
INFO - root - 2017-12-06 09:34:25.602549: step 44410, loss = 0.83, batch loss = 0.61 (12.6 examples/sec; 0.633 sec/batch; 50h:39m:22s remains)
INFO - root - 2017-12-06 09:34:31.972238: step 44420, loss = 0.79, batch loss = 0.57 (11.8 examples/sec; 0.678 sec/batch; 54h:14m:10s remains)
INFO - root - 2017-12-06 09:34:38.301579: step 44430, loss = 0.74, batch loss = 0.53 (12.6 examples/sec; 0.637 sec/batch; 50h:58m:36s remains)
INFO - root - 2017-12-06 09:34:44.838516: step 44440, loss = 0.81, batch loss = 0.59 (12.0 examples/sec; 0.665 sec/batch; 53h:10m:23s remains)
INFO - root - 2017-12-06 09:34:51.400290: step 44450, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 51h:02m:55s remains)
INFO - root - 2017-12-06 09:34:57.850665: step 44460, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 52h:31m:56s remains)
INFO - root - 2017-12-06 09:35:04.376990: step 44470, loss = 0.84, batch loss = 0.62 (12.1 examples/sec; 0.661 sec/batch; 52h:54m:32s remains)
INFO - root - 2017-12-06 09:35:10.859614: step 44480, loss = 0.77, batch loss = 0.55 (12.3 examples/sec; 0.652 sec/batch; 52h:11m:00s remains)
INFO - root - 2017-12-06 09:35:17.302330: step 44490, loss = 0.78, batch loss = 0.56 (12.8 examples/sec; 0.627 sec/batch; 50h:10m:57s remains)
INFO - root - 2017-12-06 09:35:23.896067: step 44500, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 51h:51m:18s remains)
2017-12-06 09:35:24.558746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059978321 -0.059782203 -0.059789956 -0.0597914 -0.059781574 -0.05976126 -0.059736677 -0.059713006 -0.059680797 -0.059648786 -0.059621673 -0.059603021 -0.059599139 -0.059597328 -0.059755072][-0.060071014 -0.059792429 -0.059806116 -0.05980007 -0.059767433 -0.059702553 -0.059611369 -0.059511404 -0.059388418 -0.059274144 -0.059182648 -0.05913021 -0.059124958 -0.05915514 -0.059300572][-0.060162224 -0.059796825 -0.059796829 -0.059753828 -0.059668586 -0.059522338 -0.059314612 -0.059069037 -0.05877877 -0.058512039 -0.058297485 -0.058176816 -0.058180105 -0.058285784 -0.058495644][-0.060150381 -0.059719052 -0.059669048 -0.059541367 -0.059337944 -0.059040595 -0.058647942 -0.058166977 -0.057576519 -0.05704869 -0.056697432 -0.056518231 -0.056567144 -0.056861535 -0.05738521][-0.059960335 -0.05949875 -0.059346635 -0.059051212 -0.058609884 -0.058037076 -0.057310414 -0.056468748 -0.055526834 -0.054738879 -0.054317031 -0.054196265 -0.054421671 -0.055019289 -0.05603366][-0.0597582 -0.059111837 -0.058793619 -0.058265373 -0.057505537 -0.056543097 -0.055388261 -0.054107007 -0.05273525 -0.051731385 -0.051305562 -0.051364087 -0.051942911 -0.052955985 -0.05460823][-0.059509076 -0.058594417 -0.058067236 -0.057298459 -0.056246717 -0.054912992 -0.053349648 -0.051763434 -0.050149262 -0.049059834 -0.048712939 -0.049015585 -0.049942411 -0.051343709 -0.053514816][-0.059298258 -0.058111604 -0.057387229 -0.056427896 -0.055186987 -0.053619761 -0.051843245 -0.050192602 -0.048608366 -0.047585648 -0.047415387 -0.047995221 -0.049258098 -0.050934438 -0.05337346][-0.059134994 -0.057821892 -0.056991164 -0.055970579 -0.054709196 -0.053131331 -0.051343627 -0.049760848 -0.048400093 -0.047624312 -0.047663435 -0.048484366 -0.049952328 -0.051731404 -0.054147568][-0.059108023 -0.057792831 -0.05695764 -0.056016549 -0.054955266 -0.053640384 -0.052116424 -0.0508365 -0.049841031 -0.04941928 -0.04955728 -0.050335504 -0.051749159 -0.053329047 -0.055476587][-0.059297286 -0.058044296 -0.057327226 -0.05659394 -0.055864755 -0.054973751 -0.053951427 -0.053149197 -0.052572414 -0.052399825 -0.052563377 -0.053169232 -0.054227386 -0.05542903 -0.057098884][-0.059543297 -0.058496483 -0.057996612 -0.057531986 -0.057130456 -0.056667045 -0.056128841 -0.05575636 -0.055572227 -0.055578664 -0.055734381 -0.056146353 -0.056800682 -0.057513963 -0.058632523][-0.059856493 -0.059026331 -0.058739707 -0.058496758 -0.058337081 -0.058172543 -0.05798332 -0.057880286 -0.057896994 -0.057997238 -0.058115061 -0.058360253 -0.058669221 -0.058968287 -0.059633907][-0.06010209 -0.059441209 -0.059315439 -0.0592076 -0.059153203 -0.059109569 -0.059071612 -0.059087545 -0.059133511 -0.059230845 -0.059295941 -0.05940393 -0.059525922 -0.059603073 -0.06000194][-0.060329527 -0.059735917 -0.059694353 -0.05965326 -0.059637159 -0.059633069 -0.059633583 -0.059654772 -0.059687063 -0.059727725 -0.059742689 -0.059760105 -0.059779979 -0.059801679 -0.060096875]]...]
INFO - root - 2017-12-06 09:35:30.927163: step 44510, loss = 0.84, batch loss = 0.62 (12.2 examples/sec; 0.657 sec/batch; 52h:32m:39s remains)
INFO - root - 2017-12-06 09:35:37.456159: step 44520, loss = 0.79, batch loss = 0.58 (11.7 examples/sec; 0.683 sec/batch; 54h:36m:34s remains)
INFO - root - 2017-12-06 09:35:43.963097: step 44530, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 51h:25m:03s remains)
INFO - root - 2017-12-06 09:35:50.457822: step 44540, loss = 0.76, batch loss = 0.54 (12.6 examples/sec; 0.634 sec/batch; 50h:41m:54s remains)
INFO - root - 2017-12-06 09:35:56.994218: step 44550, loss = 0.78, batch loss = 0.57 (11.9 examples/sec; 0.674 sec/batch; 53h:55m:33s remains)
INFO - root - 2017-12-06 09:36:03.540307: step 44560, loss = 0.80, batch loss = 0.58 (12.0 examples/sec; 0.668 sec/batch; 53h:27m:55s remains)
INFO - root - 2017-12-06 09:36:09.639714: step 44570, loss = 0.80, batch loss = 0.58 (12.8 examples/sec; 0.623 sec/batch; 49h:49m:11s remains)
INFO - root - 2017-12-06 09:36:16.097780: step 44580, loss = 0.84, batch loss = 0.62 (12.4 examples/sec; 0.648 sec/batch; 51h:48m:06s remains)
INFO - root - 2017-12-06 09:36:22.742396: step 44590, loss = 0.89, batch loss = 0.67 (11.8 examples/sec; 0.679 sec/batch; 54h:16m:15s remains)
INFO - root - 2017-12-06 09:36:29.183200: step 44600, loss = 0.80, batch loss = 0.59 (12.9 examples/sec; 0.622 sec/batch; 49h:45m:54s remains)
2017-12-06 09:36:29.861509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072236188 -0.071929932 -0.071864545 -0.071801856 -0.071738109 -0.071689025 -0.071675293 -0.071690477 -0.071731173 -0.07179717 -0.071906254 -0.072047681 -0.072201841 -0.072350949 -0.072482273][-0.071993247 -0.0716675 -0.071614787 -0.071542934 -0.07143344 -0.0713224 -0.071229935 -0.071172155 -0.07116729 -0.071233511 -0.071372546 -0.0715775 -0.071813785 -0.072058827 -0.072281055][-0.071871065 -0.071577221 -0.071634606 -0.0716109 -0.0714704 -0.071275145 -0.071059681 -0.0708591 -0.070728652 -0.070738725 -0.07087481 -0.071110323 -0.0714222 -0.071758628 -0.072068468][-0.071864992 -0.071774513 -0.072060615 -0.072171107 -0.072033532 -0.071713179 -0.07130833 -0.0708919 -0.070570976 -0.070465714 -0.07055752 -0.070811018 -0.071159825 -0.071541741 -0.071901232][-0.072004013 -0.072235234 -0.072818942 -0.07313925 -0.073043957 -0.072611324 -0.072005361 -0.071342841 -0.070790641 -0.070496641 -0.0704969 -0.070723377 -0.071066678 -0.071461104 -0.071820475][-0.072285958 -0.072807059 -0.073695451 -0.074246511 -0.074221626 -0.073741712 -0.07299982 -0.072156288 -0.071364507 -0.070885882 -0.070780337 -0.070949845 -0.071244523 -0.071601942 -0.071936235][-0.072595678 -0.073307768 -0.07442335 -0.075158514 -0.075226113 -0.074775904 -0.074004069 -0.073082358 -0.072162844 -0.071576625 -0.071371146 -0.07146132 -0.071683086 -0.071961641 -0.072238654][-0.072823741 -0.073619843 -0.074863404 -0.075727455 -0.075902969 -0.075537682 -0.07485231 -0.073983572 -0.073085651 -0.072456114 -0.072170824 -0.07218273 -0.072324 -0.072501764 -0.072657228][-0.0728916 -0.073584162 -0.074766941 -0.075632334 -0.075908087 -0.075653613 -0.075157873 -0.074511826 -0.073768705 -0.073174961 -0.072897226 -0.072874151 -0.072946712 -0.073028579 -0.073053993][-0.072896115 -0.073365591 -0.074331082 -0.075093329 -0.075398028 -0.075302318 -0.074970715 -0.074554026 -0.074048027 -0.073636331 -0.0734487 -0.073436081 -0.073470645 -0.073456362 -0.07337901][-0.072892226 -0.073053651 -0.07372845 -0.074314773 -0.074586868 -0.074600451 -0.07444156 -0.074229084 -0.074002571 -0.073811881 -0.073752344 -0.073770106 -0.073790081 -0.0737202 -0.073582843][-0.072921962 -0.072838679 -0.073234461 -0.073574841 -0.073766328 -0.073850393 -0.073806629 -0.073762283 -0.073734447 -0.07370767 -0.073748574 -0.073803417 -0.073834442 -0.073762037 -0.073599294][-0.073069431 -0.072811231 -0.072990075 -0.073128983 -0.07321088 -0.0732767 -0.073278934 -0.073301941 -0.073379546 -0.073458947 -0.073544517 -0.073618509 -0.073665217 -0.0736 -0.073462628][-0.073309794 -0.072958149 -0.073028944 -0.073054761 -0.073040158 -0.0730146 -0.073013216 -0.073043019 -0.07312604 -0.073229127 -0.073321506 -0.0733947 -0.073436253 -0.073392145 -0.073282011][-0.073413014 -0.073036239 -0.073061921 -0.073050588 -0.073015496 -0.072969891 -0.072943315 -0.072948121 -0.072993785 -0.073061824 -0.073135912 -0.073194854 -0.073223248 -0.073192351 -0.07312192]]...]
INFO - root - 2017-12-06 09:36:36.244186: step 44610, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 50h:22m:39s remains)
INFO - root - 2017-12-06 09:36:42.762787: step 44620, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.663 sec/batch; 53h:02m:46s remains)
INFO - root - 2017-12-06 09:36:49.238156: step 44630, loss = 0.79, batch loss = 0.57 (12.9 examples/sec; 0.622 sec/batch; 49h:41m:57s remains)
INFO - root - 2017-12-06 09:36:55.774807: step 44640, loss = 0.74, batch loss = 0.53 (12.7 examples/sec; 0.632 sec/batch; 50h:33m:00s remains)
INFO - root - 2017-12-06 09:37:02.245753: step 44650, loss = 0.78, batch loss = 0.56 (12.6 examples/sec; 0.633 sec/batch; 50h:36m:06s remains)
INFO - root - 2017-12-06 09:37:08.715009: step 44660, loss = 0.77, batch loss = 0.55 (12.6 examples/sec; 0.637 sec/batch; 50h:56m:53s remains)
INFO - root - 2017-12-06 09:37:15.238720: step 44670, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.639 sec/batch; 51h:06m:27s remains)
INFO - root - 2017-12-06 09:37:21.737614: step 44680, loss = 0.83, batch loss = 0.62 (11.8 examples/sec; 0.676 sec/batch; 54h:02m:01s remains)
INFO - root - 2017-12-06 09:37:28.228993: step 44690, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.648 sec/batch; 51h:46m:13s remains)
INFO - root - 2017-12-06 09:37:34.660293: step 44700, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.628 sec/batch; 50h:11m:24s remains)
2017-12-06 09:37:35.296891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076582924 -0.076859072 -0.079646692 -0.085448645 -0.092695951 -0.099995255 -0.10742856 -0.11563279 -0.12144417 -0.12001287 -0.11029021 -0.097089589 -0.08720585 -0.0830727 -0.0825353][-0.078644194 -0.079205006 -0.083408177 -0.089629762 -0.0917348 -0.087978259 -0.0853183 -0.093403906 -0.11076821 -0.125377 -0.12544927 -0.11122722 -0.094891541 -0.085780032 -0.082993239][-0.08115913 -0.081796914 -0.0867286 -0.092266142 -0.0877133 -0.071310014 -0.055413213 -0.055823237 -0.076760896 -0.10642159 -0.12320879 -0.1179912 -0.10184298 -0.088595584 -0.081909709][-0.082838096 -0.083555982 -0.089307107 -0.094598763 -0.085698918 -0.059710748 -0.032265406 -0.020040162 -0.036071546 -0.073628753 -0.10528392 -0.11333799 -0.10370908 -0.089530334 -0.080000788][-0.082412086 -0.0832114 -0.090063907 -0.09670651 -0.087317295 -0.056902677 -0.019920915 0.0066422373 0.0009553507 -0.036790982 -0.077013895 -0.0984474 -0.10096326 -0.090318739 -0.0802667][-0.080460183 -0.081128985 -0.088270053 -0.097189426 -0.090929076 -0.063547358 -0.02588626 0.0094302371 0.015074611 -0.016108587 -0.05482737 -0.08193028 -0.093545243 -0.087354489 -0.078750789][-0.079181664 -0.079899594 -0.085848264 -0.094515406 -0.091608785 -0.074734963 -0.049216777 -0.020242728 -0.0099081025 -0.028912209 -0.054093156 -0.07538978 -0.08859057 -0.085990138 -0.080683231][-0.079781085 -0.080322251 -0.084427215 -0.090206891 -0.087697342 -0.074168541 -0.055130918 -0.035150595 -0.030005053 -0.045320913 -0.061798297 -0.076639324 -0.085742719 -0.083902173 -0.081418827][-0.081751652 -0.081900038 -0.085477412 -0.091229685 -0.091066606 -0.086707309 -0.077339962 -0.066313826 -0.061799254 -0.067296021 -0.073110767 -0.079970814 -0.083871432 -0.082914293 -0.082109071][-0.083810814 -0.0837397 -0.08591821 -0.090895891 -0.096129447 -0.10070802 -0.10261416 -0.09954492 -0.096726812 -0.092726074 -0.088475578 -0.085861072 -0.084271714 -0.083195649 -0.083269492][-0.085215636 -0.084877186 -0.084977217 -0.084653236 -0.083655633 -0.082277119 -0.081235118 -0.081062511 -0.081592456 -0.08264754 -0.08303839 -0.083489738 -0.083487317 -0.083926804 -0.084265023][-0.085423037 -0.084925428 -0.08499644 -0.085009083 -0.084071174 -0.082683258 -0.081637278 -0.081723996 -0.08299569 -0.084321909 -0.084576353 -0.08448261 -0.084426627 -0.084406644 -0.084595911][-0.084841512 -0.084320225 -0.084602848 -0.08481437 -0.084302247 -0.083290391 -0.082192056 -0.082173124 -0.083156236 -0.0845801 -0.084527679 -0.084197648 -0.084140882 -0.084180377 -0.084501646][-0.084260754 -0.083859146 -0.084255077 -0.085235104 -0.084837094 -0.083892263 -0.082287595 -0.08214961 -0.082801506 -0.083979905 -0.083852105 -0.083666213 -0.083494686 -0.083728358 -0.084087044][-0.084033974 -0.083699182 -0.083803564 -0.084628977 -0.08478909 -0.084276885 -0.083780959 -0.08267805 -0.082911327 -0.083646841 -0.083552919 -0.083375961 -0.083287753 -0.083474912 -0.083819039]]...]
INFO - root - 2017-12-06 09:37:41.710341: step 44710, loss = 0.72, batch loss = 0.51 (12.3 examples/sec; 0.650 sec/batch; 51h:59m:15s remains)
INFO - root - 2017-12-06 09:37:48.135125: step 44720, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.642 sec/batch; 51h:17m:30s remains)
INFO - root - 2017-12-06 09:37:54.670654: step 44730, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 51h:48m:59s remains)
INFO - root - 2017-12-06 09:38:01.224367: step 44740, loss = 0.69, batch loss = 0.48 (12.0 examples/sec; 0.664 sec/batch; 53h:05m:59s remains)
INFO - root - 2017-12-06 09:38:07.755542: step 44750, loss = 0.79, batch loss = 0.57 (12.8 examples/sec; 0.624 sec/batch; 49h:50m:37s remains)
INFO - root - 2017-12-06 09:38:14.235315: step 44760, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.655 sec/batch; 52h:20m:04s remains)
INFO - root - 2017-12-06 09:38:20.801946: step 44770, loss = 0.78, batch loss = 0.57 (12.0 examples/sec; 0.669 sec/batch; 53h:26m:02s remains)
INFO - root - 2017-12-06 09:38:27.322939: step 44780, loss = 0.78, batch loss = 0.57 (12.8 examples/sec; 0.625 sec/batch; 49h:57m:07s remains)
INFO - root - 2017-12-06 09:38:33.652728: step 44790, loss = 0.91, batch loss = 0.69 (15.8 examples/sec; 0.507 sec/batch; 40h:33m:09s remains)
INFO - root - 2017-12-06 09:38:40.151025: step 44800, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.648 sec/batch; 51h:48m:58s remains)
2017-12-06 09:38:40.818327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.56234527 -0.95119506 -1.1482639 -1.2273551 -1.1472569 -1.0181642 -0.85062438 -0.59486103 -0.24412462 0.067250378 0.23800009 0.2926397 0.20989427 0.085630335 -0.048549108][-0.53467858 -0.90316761 -1.2424606 -1.4797564 -1.5741712 -1.6587409 -1.6898438 -1.547405 -1.2521604 -0.92535108 -0.62644643 -0.41182703 -0.31600007 -0.34149018 -0.39242789][-0.5192858 -0.72629505 -0.97524089 -1.2911968 -1.5462334 -1.7216146 -1.8278238 -1.8344269 -1.7534409 -1.5846704 -1.4041239 -1.1905044 -0.959432 -0.76437283 -0.55874038][-0.25370988 -0.40361902 -0.48337489 -0.65699965 -0.8279115 -1.071792 -1.2658724 -1.3352225 -1.3101352 -1.2540442 -1.1639402 -1.0719528 -0.96168303 -0.78233194 -0.58955836][-0.047875259 0.029787317 0.15926856 0.0838659 -0.060635686 -0.24148113 -0.4574098 -0.65554368 -0.6926446 -0.66183734 -0.639041 -0.64042532 -0.61595678 -0.52719688 -0.41018927][0.23260897 0.3883864 0.63796735 0.78914607 0.79441357 0.61999381 0.40900633 0.22730312 0.11361129 0.041317038 -0.028699469 -0.10559193 -0.16838509 -0.19354048 -0.18918262][0.39979464 0.64666331 0.92382431 1.1097631 1.2171106 1.2160507 1.1239381 0.97041953 0.86816996 0.71684712 0.50060022 0.3158316 0.19112831 0.10675114 0.047909655][0.35543472 0.55711007 0.835663 1.0341908 1.1669781 1.1835005 1.180203 1.1576405 1.1251856 1.0481993 0.84998953 0.62873995 0.3926191 0.21476421 0.081979431][0.22018966 0.35622469 0.52780569 0.70203251 0.86486667 0.97659755 1.0840051 1.1146346 1.1032681 0.97778237 0.79756588 0.5841502 0.33563861 0.1586538 0.0402255][-0.016854569 0.075725518 0.20641959 0.30790958 0.4243207 0.562207 0.75747335 0.89533943 0.930529 0.8630684 0.72020096 0.52801955 0.31578463 0.1458137 0.022767678][-0.31687167 -0.31460035 -0.27258638 -0.18699278 -0.086401843 -0.017503902 0.075938381 0.21005642 0.304775 0.320507 0.27216092 0.17966601 0.0628457 -0.0020765588 -0.024974056][-0.39255872 -0.44571093 -0.47647041 -0.47358936 -0.46543849 -0.42422894 -0.39294052 -0.3622871 -0.30073082 -0.22304645 -0.19303544 -0.16127372 -0.1155948 -0.12525195 -0.11888417][-0.24359354 -0.29820603 -0.34780955 -0.37431374 -0.39048156 -0.40883678 -0.45366633 -0.4649446 -0.45779482 -0.38860893 -0.32187647 -0.27530253 -0.18361697 -0.13008136 -0.10010422][-0.1424102 -0.16164295 -0.17991859 -0.21391429 -0.25698191 -0.27392441 -0.29937845 -0.31081873 -0.32654461 -0.32596451 -0.33589992 -0.31259295 -0.22063005 -0.18932232 -0.14424601][-0.12480657 -0.13331953 -0.13326496 -0.11851047 -0.10559147 -0.1196104 -0.158734 -0.17757559 -0.18995509 -0.20015338 -0.21875471 -0.22644335 -0.20324717 -0.20673829 -0.15413055]]...]
INFO - root - 2017-12-06 09:38:47.283421: step 44810, loss = 0.71, batch loss = 0.50 (12.6 examples/sec; 0.635 sec/batch; 50h:44m:19s remains)
INFO - root - 2017-12-06 09:38:53.831773: step 44820, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.632 sec/batch; 50h:31m:27s remains)
INFO - root - 2017-12-06 09:39:00.369721: step 44830, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.649 sec/batch; 51h:49m:53s remains)
INFO - root - 2017-12-06 09:39:06.908268: step 44840, loss = 0.86, batch loss = 0.64 (12.7 examples/sec; 0.631 sec/batch; 50h:23m:18s remains)
INFO - root - 2017-12-06 09:39:13.414726: step 44850, loss = 0.77, batch loss = 0.55 (13.7 examples/sec; 0.583 sec/batch; 46h:33m:16s remains)
INFO - root - 2017-12-06 09:39:19.943760: step 44860, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.670 sec/batch; 53h:30m:22s remains)
INFO - root - 2017-12-06 09:39:26.461800: step 44870, loss = 0.74, batch loss = 0.53 (12.6 examples/sec; 0.636 sec/batch; 50h:50m:59s remains)
INFO - root - 2017-12-06 09:39:32.939502: step 44880, loss = 0.85, batch loss = 0.64 (12.9 examples/sec; 0.622 sec/batch; 49h:40m:53s remains)
INFO - root - 2017-12-06 09:39:39.414504: step 44890, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.657 sec/batch; 52h:30m:54s remains)
INFO - root - 2017-12-06 09:39:45.867607: step 44900, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.672 sec/batch; 53h:40m:44s remains)
2017-12-06 09:39:46.532539: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.2248525 -0.23643026 -0.25280493 -0.27336645 -0.27248842 -0.24266368 -0.22091451 -0.18845819 -0.17965904 -0.15359592 -0.14555167 -0.1566488 -0.15454559 -0.13499844 -0.1223212][-0.17448816 -0.17340666 -0.18653676 -0.22006229 -0.26709291 -0.2634739 -0.24878374 -0.23302615 -0.24429125 -0.22979748 -0.21893913 -0.20930478 -0.1851498 -0.13268165 -0.11749478][-0.1196461 -0.10843214 -0.11838949 -0.14532031 -0.2002344 -0.21584409 -0.21052986 -0.18583764 -0.22469285 -0.2191937 -0.2213597 -0.22957569 -0.18707412 -0.1368804 -0.12554848][-0.027640723 0.003970392 0.018430129 0.0105552 -0.055026311 -0.091560185 -0.11073749 -0.11729892 -0.18939611 -0.21011071 -0.23299044 -0.26857808 -0.2214458 -0.15105236 -0.1251227][-0.052970253 -0.0060336962 0.047451936 0.07756149 0.034832336 0.011342615 0.0059562325 0.012612805 -0.096454345 -0.15594092 -0.20384029 -0.27456152 -0.24956504 -0.17844445 -0.15117586][-0.038169026 -0.0042180195 0.065783627 0.089679606 0.054998003 0.058958553 0.081555493 0.1100003 0.027856849 -0.0448684 -0.098699413 -0.16450763 -0.17818519 -0.1512116 -0.15352981][-0.04092009 -0.024819829 0.028447092 0.048277281 0.019741654 0.044014864 0.096279271 0.15649006 0.096177004 0.030361146 -0.029570233 -0.079563521 -0.1005412 -0.10339331 -0.1120441][-0.041973095 -0.036218576 -0.0069786236 -0.0032111108 -0.034062613 -0.0037501082 0.050826721 0.098319717 0.06390252 0.040579833 0.014538221 -0.0030209795 -0.025773786 -0.0630506 -0.099344581][-0.0704215 -0.069266766 -0.051497076 -0.05693157 -0.068574868 -0.035229009 0.027512066 0.084289186 0.045871608 0.025393635 0.024194837 0.036204033 0.021859832 -0.022850685 -0.04949465][-0.09416417 -0.10057314 -0.088292234 -0.0866627 -0.098416127 -0.068926811 -0.0068355575 0.0725055 0.066501893 0.076493688 0.097974218 0.12074155 0.11059528 0.051871307 0.011197336][-0.092883922 -0.10136327 -0.096619375 -0.1068085 -0.11634776 -0.11094134 -0.0663103 0.00080938637 0.0074619874 0.030530497 0.069113038 0.091771223 0.08223965 0.046669953 0.040806286][-0.079323173 -0.093942441 -0.091324359 -0.10188221 -0.10989752 -0.11524629 -0.090607718 -0.045081835 -0.033384647 -0.0077172443 0.033976667 0.057359062 0.053845488 0.025006697 0.0068759918][-0.074878365 -0.081242509 -0.078011177 -0.092680015 -0.094639868 -0.10566399 -0.1071411 -0.08077164 -0.077407539 -0.063098378 -0.034244306 -0.010263234 -0.00826256 -0.020220421 -0.020793155][-0.073865026 -0.074958667 -0.075401649 -0.09516529 -0.091395587 -0.10381428 -0.11940442 -0.10963956 -0.11471556 -0.10262707 -0.083656818 -0.064751625 -0.054050844 -0.060511466 -0.05840328][-0.074608967 -0.073800325 -0.075129353 -0.078526393 -0.076977678 -0.088960223 -0.098031051 -0.10662905 -0.11823997 -0.11135279 -0.10559133 -0.095630877 -0.085375533 -0.085717186 -0.078944407]]...]
INFO - root - 2017-12-06 09:39:53.105752: step 44910, loss = 0.82, batch loss = 0.60 (12.1 examples/sec; 0.662 sec/batch; 52h:51m:59s remains)
INFO - root - 2017-12-06 09:39:59.644074: step 44920, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.648 sec/batch; 51h:44m:35s remains)
INFO - root - 2017-12-06 09:40:06.343238: step 44930, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.647 sec/batch; 51h:42m:48s remains)
INFO - root - 2017-12-06 09:40:12.890685: step 44940, loss = 0.83, batch loss = 0.62 (12.6 examples/sec; 0.637 sec/batch; 50h:51m:57s remains)
INFO - root - 2017-12-06 09:40:19.448760: step 44950, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.643 sec/batch; 51h:22m:33s remains)
INFO - root - 2017-12-06 09:40:26.014391: step 44960, loss = 0.81, batch loss = 0.60 (11.7 examples/sec; 0.683 sec/batch; 54h:31m:37s remains)
INFO - root - 2017-12-06 09:40:32.449086: step 44970, loss = 0.70, batch loss = 0.48 (12.3 examples/sec; 0.652 sec/batch; 52h:06m:33s remains)
INFO - root - 2017-12-06 09:40:38.983714: step 44980, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.669 sec/batch; 53h:23m:54s remains)
INFO - root - 2017-12-06 09:40:45.507387: step 44990, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.660 sec/batch; 52h:41m:56s remains)
INFO - root - 2017-12-06 09:40:51.939355: step 45000, loss = 0.78, batch loss = 0.57 (12.0 examples/sec; 0.664 sec/batch; 53h:02m:33s remains)
2017-12-06 09:40:52.604326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12073597 -0.1241719 -0.12809022 -0.13289176 -0.13516334 -0.13591073 -0.13553664 -0.13397339 -0.13357502 -0.13224298 -0.12681925 -0.11962754 -0.11688022 -0.11566284 -0.11394969][-0.18012679 -0.20180549 -0.22609815 -0.24649201 -0.24993707 -0.24227548 -0.23181179 -0.2202068 -0.20690024 -0.19095051 -0.17147955 -0.15093789 -0.13608791 -0.13204572 -0.13262011][-0.32648063 -0.38146034 -0.44529396 -0.49398351 -0.50932217 -0.506508 -0.48598447 -0.44862714 -0.40042061 -0.33810195 -0.27290583 -0.21799964 -0.16810417 -0.14063819 -0.13374083][-0.35786524 -0.39152834 -0.42408848 -0.46630615 -0.48455396 -0.47948515 -0.47893646 -0.46497658 -0.44372267 -0.38362342 -0.30727309 -0.24520238 -0.17550308 -0.12917724 -0.12952915][-0.10987145 -0.05797274 -0.036918543 -0.025820278 -0.017809279 -0.023200333 -0.041269444 -0.071871124 -0.12072776 -0.13883206 -0.15998593 -0.16858464 -0.14873683 -0.13172603 -0.13311279][0.340571 0.52294719 0.65076691 0.70477206 0.72685647 0.68442112 0.60258865 0.46692812 0.26722488 0.1383974 0.026960857 -0.064543225 -0.10700893 -0.1257219 -0.12612697][0.790772 1.0568849 1.2650191 1.3766096 1.3493267 1.2491009 1.092641 0.84114826 0.50865912 0.27142197 0.10499385 -0.01224827 -0.095023304 -0.13157396 -0.13770983][0.95481765 1.2132574 1.3962883 1.5009106 1.504001 1.3880999 1.1725433 0.88505709 0.53670019 0.28872123 0.13042194 0.019223467 -0.067331895 -0.1103261 -0.11916199][0.69468355 0.87604 1.0119842 1.0496221 1.0232059 0.95312166 0.79549307 0.57700419 0.32882249 0.15082888 0.051353619 0.010883152 -0.028750014 -0.070548326 -0.082903378][0.2093806 0.29291192 0.34637362 0.37049559 0.356609 0.29652643 0.21392712 0.10018924 0.00074991584 -0.04442472 -0.046676282 -0.027737968 -0.015721098 -0.025162645 -0.049822647][-0.28721264 -0.29258418 -0.32125092 -0.32870549 -0.29813105 -0.24966 -0.23862031 -0.21426845 -0.17056882 -0.12535803 -0.10657976 -0.084300473 -0.059096884 -0.062037505 -0.073218778][-0.59237581 -0.62847656 -0.68708056 -0.68731374 -0.62955868 -0.51902831 -0.39925632 -0.2910572 -0.19160627 -0.1446541 -0.11557651 -0.11130106 -0.10816857 -0.10859594 -0.10557152][-0.61292309 -0.64842081 -0.6694656 -0.66578817 -0.6159867 -0.53623313 -0.4372254 -0.30215353 -0.21945062 -0.17275736 -0.15510254 -0.1665961 -0.16490857 -0.15001324 -0.13322927][-0.50223947 -0.52649665 -0.53537858 -0.49736285 -0.43243346 -0.3578901 -0.31370258 -0.23584791 -0.17218773 -0.1509545 -0.14571612 -0.15319598 -0.16088156 -0.15718958 -0.1365882][-0.37945214 -0.37837738 -0.36233765 -0.30707034 -0.23927505 -0.16761878 -0.12544757 -0.10493091 -0.099244922 -0.10444914 -0.1074502 -0.1240942 -0.13287865 -0.13301851 -0.11919296]]...]
INFO - root - 2017-12-06 09:40:59.104914: step 45010, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 51h:57m:52s remains)
INFO - root - 2017-12-06 09:41:05.713014: step 45020, loss = 0.95, batch loss = 0.74 (12.2 examples/sec; 0.655 sec/batch; 52h:20m:27s remains)
INFO - root - 2017-12-06 09:41:12.171289: step 45030, loss = 0.86, batch loss = 0.65 (12.6 examples/sec; 0.633 sec/batch; 50h:34m:56s remains)
INFO - root - 2017-12-06 09:41:18.686615: step 45040, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 50h:55m:40s remains)
INFO - root - 2017-12-06 09:41:25.137739: step 45050, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.647 sec/batch; 51h:41m:18s remains)
INFO - root - 2017-12-06 09:41:31.694302: step 45060, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.662 sec/batch; 52h:50m:41s remains)
INFO - root - 2017-12-06 09:41:38.008080: step 45070, loss = 0.84, batch loss = 0.62 (12.9 examples/sec; 0.618 sec/batch; 49h:22m:30s remains)
INFO - root - 2017-12-06 09:41:44.492956: step 45080, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.660 sec/batch; 52h:42m:17s remains)
INFO - root - 2017-12-06 09:41:50.981849: step 45090, loss = 0.83, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 52h:15m:52s remains)
INFO - root - 2017-12-06 09:41:57.463999: step 45100, loss = 0.83, batch loss = 0.62 (12.6 examples/sec; 0.637 sec/batch; 50h:51m:09s remains)
2017-12-06 09:41:58.119506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.51617062 -1.0280423 -1.1087211 -0.93197483 -0.44759837 -0.55536336 -0.87436461 -0.051452011 0.89802742 1.33095 2.2364461 2.5732703 2.696492 2.5109897 2.8937497][0.40435767 -0.45783436 -1.1568508 -1.3146733 -1.6304681 -2.078043 -2.1074271 -2.6210251 -2.5210922 -1.3927833 -0.51368439 -0.054973707 0.70096332 1.2749679 1.0908834][-0.41725013 -0.63259596 -0.82818961 -1.4133375 -2.137773 -2.9206011 -3.5895915 -3.8887937 -3.874186 -3.6452138 -3.354408 -2.9731538 -2.2994673 -1.6064413 -1.2441236][1.7102115 0.83995169 0.31321287 -0.12341441 -0.582569 -1.2289079 -1.3711371 -2.0514591 -3.3662534 -3.7993844 -3.738816 -3.7203994 -3.74415 -3.1225884 -2.4263759][3.0052505 2.5415838 2.6714046 1.8967221 0.96672738 0.18339583 -0.34371245 -0.064630248 -0.67062306 -1.5342803 -2.4955804 -2.3321571 -2.5559869 -2.4380462 -1.9841427][3.9321146 3.4078846 3.184227 3.2568564 2.8338048 1.9505026 1.6150986 1.330109 0.63270128 0.12639266 -0.33571526 -1.1346874 -1.61375 -1.6873227 -1.4750803][2.4845908 2.70482 2.8225107 2.6316264 2.1266811 1.441244 1.3453459 1.5268251 1.3728255 0.45826864 0.0721304 -0.31243736 -0.86159086 -1.3401378 -1.8631318][0.1912148 0.18405464 0.034519762 -0.06269075 -0.36351451 -0.33888456 -0.61494315 -0.82522893 -1.0702209 -0.83930582 -0.80400604 -1.0295786 -1.2388629 -1.3346658 -1.1964844][-1.3686858 -1.7179589 -2.3317242 -2.8524063 -3.2644928 -3.2417643 -3.1274498 -2.7443588 -2.7919974 -2.610944 -2.1566536 -1.8109039 -1.6381509 -1.6602008 -1.4105691][-3.6666839 -4.0609093 -4.2020483 -4.3298583 -4.4865842 -3.994179 -3.652432 -3.4102471 -3.7433765 -3.6775465 -3.4502056 -3.0841043 -2.4885163 -2.2250323 -2.0406587][-3.804951 -4.7874684 -5.6520562 -5.8733745 -5.7702851 -5.6823044 -5.2571507 -4.9344349 -4.5208917 -4.2330794 -3.6708782 -2.8278844 -2.1126757 -1.7949888 -1.4076488][-1.6156027 -2.5727894 -3.6455195 -4.3378763 -4.5269566 -4.5151677 -4.5365171 -4.529583 -4.6765003 -4.5887356 -3.9576321 -3.3333797 -2.8516297 -2.0182705 -1.4353453][-1.4709859 -1.4939796 -1.9022651 -2.3934641 -2.5796545 -2.5244846 -2.8054755 -2.9187908 -2.8678446 -2.7793257 -2.8633392 -2.5342503 -2.2942882 -1.8222008 -1.4193918][-1.7204883 -1.734585 -1.799088 -1.9447635 -2.0542018 -2.0591545 -1.8745034 -1.8367443 -2.0194294 -1.809413 -1.2999078 -1.1051524 -1.1882231 -1.0805608 -0.81781864][-1.7475704 -1.4791601 -1.4081173 -1.7810912 -2.1482947 -2.0574737 -1.6982793 -1.5335968 -1.3819596 -1.4731807 -1.550678 -0.907157 -0.45810735 -0.42934197 -0.054320052]]...]
INFO - root - 2017-12-06 09:42:04.646912: step 45110, loss = 0.75, batch loss = 0.54 (12.1 examples/sec; 0.662 sec/batch; 52h:51m:13s remains)
INFO - root - 2017-12-06 09:42:11.286379: step 45120, loss = 0.78, batch loss = 0.56 (11.8 examples/sec; 0.681 sec/batch; 54h:19m:23s remains)
INFO - root - 2017-12-06 09:42:17.784778: step 45130, loss = 0.91, batch loss = 0.69 (12.0 examples/sec; 0.666 sec/batch; 53h:12m:01s remains)
INFO - root - 2017-12-06 09:42:24.113885: step 45140, loss = 0.80, batch loss = 0.58 (12.3 examples/sec; 0.648 sec/batch; 51h:45m:14s remains)
INFO - root - 2017-12-06 09:42:30.616675: step 45150, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.628 sec/batch; 50h:08m:33s remains)
INFO - root - 2017-12-06 09:42:37.121164: step 45160, loss = 0.81, batch loss = 0.59 (12.7 examples/sec; 0.628 sec/batch; 50h:08m:14s remains)
INFO - root - 2017-12-06 09:42:43.527985: step 45170, loss = 0.80, batch loss = 0.58 (12.6 examples/sec; 0.637 sec/batch; 50h:50m:36s remains)
INFO - root - 2017-12-06 09:42:50.058871: step 45180, loss = 0.76, batch loss = 0.55 (12.9 examples/sec; 0.619 sec/batch; 49h:24m:58s remains)
INFO - root - 2017-12-06 09:42:56.536573: step 45190, loss = 0.87, batch loss = 0.65 (11.9 examples/sec; 0.672 sec/batch; 53h:40m:05s remains)
INFO - root - 2017-12-06 09:43:02.976954: step 45200, loss = 0.77, batch loss = 0.56 (12.6 examples/sec; 0.636 sec/batch; 50h:44m:38s remains)
2017-12-06 09:43:03.655553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15260309 0.2104969 0.19416326 0.024582893 -0.59016007 -1.2127368 -1.624153 -1.8206375 -1.9507743 -1.8699594 -1.7945271 -1.5225066 -1.2779791 -1.0522488 -0.572866][0.32233793 0.39989069 0.3700206 0.050798923 -0.49700138 -1.0592527 -1.5145097 -1.7037084 -1.9044883 -2.2618582 -2.2820158 -1.9840679 -1.737828 -1.5620853 -1.2753507][0.12639563 0.17565241 -0.090626232 -0.24794208 -0.23563237 -0.76896459 -1.2992488 -1.6214066 -1.8554822 -2.0571437 -2.2246196 -2.3248527 -2.2770662 -1.7984698 -1.2350355][-0.27163216 -0.12186949 -0.13912857 -0.073512681 -0.052834548 -0.29131365 -0.49621347 -0.87309986 -1.2157851 -1.6124756 -1.9659591 -1.9647943 -2.0352783 -1.9465648 -1.6188838][0.10170187 0.054639697 0.012522392 0.38807866 0.76184887 0.80939752 0.47932649 0.021615684 -0.38393414 -0.80213928 -1.2445058 -1.7623794 -2.1811781 -1.9892625 -1.8249907][0.84753436 0.86625677 1.0271198 1.2155524 1.4907132 1.6235309 1.3719542 0.99380267 0.59401828 -0.035061896 -0.81268823 -1.3572564 -1.7449007 -2.0529079 -2.4134197][1.2052813 1.5541055 1.6878593 1.7892611 1.805421 1.9979508 2.0814037 1.8845479 1.4030266 0.75560665 0.13407661 -0.65092611 -1.2895647 -1.5328258 -1.7805252][1.1301382 1.1538373 1.3950866 1.6190039 1.7250601 1.8665267 1.8834093 1.7939005 1.5371273 1.0903834 0.38759109 -0.26480362 -0.70102447 -1.1050714 -1.5998089][0.79525924 0.85650724 0.85748285 0.83107442 1.0189071 0.94435918 0.92912161 1.0548766 1.0281621 0.77706081 0.35076085 -0.110502 -0.23797572 -0.55865872 -0.84511161][0.28364864 -0.11882409 -0.092719026 0.0052912459 -0.16199116 -0.228738 -0.27574873 -0.37518823 -0.26932514 -0.19878946 -0.36504322 -0.31583175 -0.083308563 -0.22595066 -0.383687][0.055387095 -0.17538482 -0.36908123 -0.81983191 -1.1714661 -1.3988568 -1.375626 -1.2454426 -1.110847 -1.0735128 -0.80852294 -0.68774194 -0.59328026 -0.42990559 -0.36001229][0.028318316 -0.43873712 -0.75541496 -0.9753533 -1.2161126 -1.5374395 -1.6733829 -1.681805 -1.6972119 -1.5136592 -1.3678768 -1.1222161 -0.81191784 -0.65564221 -0.3916893][0.62327534 0.39062282 -0.1763041 -0.46339789 -0.843685 -1.3405142 -1.6237217 -1.7202584 -1.6116025 -1.7637997 -1.8114594 -1.4683405 -1.0824399 -0.98670554 -0.87497473][1.082383 1.0250174 0.8401376 0.55482858 0.25117546 -0.27182412 -0.62367141 -1.0879891 -1.3137598 -1.3447576 -1.3682221 -1.4542325 -1.4673189 -1.3059007 -1.2250317][1.5824351 1.3014736 1.4566983 1.2855687 0.77733064 0.3061651 -0.17400807 -0.67528844 -0.98873305 -1.1444443 -1.0745606 -1.1121272 -1.3588238 -1.4170914 -1.196481]]...]
INFO - root - 2017-12-06 09:43:10.188272: step 45210, loss = 0.86, batch loss = 0.65 (12.3 examples/sec; 0.649 sec/batch; 51h:48m:12s remains)
INFO - root - 2017-12-06 09:43:16.722118: step 45220, loss = 0.78, batch loss = 0.56 (12.2 examples/sec; 0.654 sec/batch; 52h:13m:05s remains)
INFO - root - 2017-12-06 09:43:23.084566: step 45230, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.663 sec/batch; 52h:54m:53s remains)
INFO - root - 2017-12-06 09:43:29.642703: step 45240, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.664 sec/batch; 52h:58m:12s remains)
INFO - root - 2017-12-06 09:43:36.150929: step 45250, loss = 0.89, batch loss = 0.67 (12.2 examples/sec; 0.656 sec/batch; 52h:20m:10s remains)
INFO - root - 2017-12-06 09:43:42.572280: step 45260, loss = 0.85, batch loss = 0.63 (12.3 examples/sec; 0.650 sec/batch; 51h:52m:04s remains)
INFO - root - 2017-12-06 09:43:49.154946: step 45270, loss = 0.81, batch loss = 0.59 (12.2 examples/sec; 0.655 sec/batch; 52h:13m:33s remains)
INFO - root - 2017-12-06 09:43:55.533671: step 45280, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.652 sec/batch; 51h:59m:32s remains)
INFO - root - 2017-12-06 09:44:02.144337: step 45290, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.659 sec/batch; 52h:32m:31s remains)
INFO - root - 2017-12-06 09:44:08.719368: step 45300, loss = 0.79, batch loss = 0.58 (11.9 examples/sec; 0.675 sec/batch; 53h:49m:34s remains)
2017-12-06 09:44:09.371404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.78744477 -0.74884313 -0.62595814 -0.42394269 -0.19792214 -0.07853435 -0.0055839866 0.057528652 0.038848482 -0.0522574 -0.27462167 -0.41012481 -0.47724646 -0.57190913 -0.76579565][-0.83644581 -0.86715853 -0.7914148 -0.63882887 -0.3774474 -0.16621432 -0.0603398 -0.057898063 -0.11978798 -0.16896014 -0.18504775 -0.18616132 -0.27302229 -0.32947353 -0.25431508][-0.72672009 -0.87589335 -0.89283437 -0.76464987 -0.57546407 -0.38601413 -0.23908642 -0.13086136 -0.074496709 -0.12203264 -0.19362155 -0.21041702 -0.24916515 -0.26876467 -0.29534233][-0.32991937 -0.54497766 -0.67981929 -0.67080289 -0.60931468 -0.4698036 -0.33912873 -0.22837067 -0.11866449 -0.018627085 -0.045990787 -0.11795679 -0.22034395 -0.26740885 -0.37282345][0.0027542934 -0.12670149 -0.16103499 -0.25908765 -0.23895848 -0.18972534 -0.058977015 0.034984641 0.10437267 0.18649092 0.1332722 0.033329934 -0.032019872 -0.16424945 -0.28027278][0.3606348 0.21519637 0.10260016 0.097895063 0.17000508 0.22764176 0.29293346 0.32194614 0.37303787 0.40539894 0.4266988 0.30987534 0.13526791 -0.047679584 -0.08591906][0.25989273 0.10429346 0.10292345 0.17357656 0.28047764 0.37089336 0.44312739 0.48216838 0.49571311 0.43419272 0.43414813 0.39761215 0.28094485 0.11721461 -0.014495611][-0.34155488 -0.30574757 -0.32939366 -0.21348104 -0.032787316 0.065671436 0.18150792 0.22805503 0.22396335 0.2256124 0.18212581 0.14093557 0.079454966 -0.032689378 -0.086813867][-1.1406372 -1.0095022 -0.81673408 -0.62209779 -0.42078528 -0.24649593 -0.12524225 -0.11400538 -0.096909583 -0.15140377 -0.20553118 -0.25895587 -0.28788745 -0.30939853 -0.26149985][-1.7783732 -1.6024823 -1.2751926 -0.97695047 -0.75145942 -0.51045233 -0.36680424 -0.28143606 -0.27498174 -0.33961329 -0.36154798 -0.38948408 -0.43671137 -0.45872334 -0.42243344][-1.8799859 -1.7475542 -1.4666418 -1.1993406 -0.94033104 -0.70368731 -0.52692497 -0.44288659 -0.43284968 -0.40524408 -0.37280226 -0.35761136 -0.36684209 -0.4333438 -0.38842919][-1.5212146 -1.3349242 -1.1536208 -0.96402347 -0.81806278 -0.64786172 -0.50166649 -0.42062473 -0.38384223 -0.38098785 -0.34502721 -0.29174724 -0.25984517 -0.29498249 -0.33813816][-1.2692432 -1.0596917 -0.80111706 -0.64771312 -0.53364313 -0.41282994 -0.33691555 -0.27137911 -0.1922317 -0.17316644 -0.1255562 -0.097694948 -0.032135654 -0.084851094 -0.18171908][-0.84697586 -0.71699381 -0.581038 -0.47170368 -0.33675191 -0.21847376 -0.05480969 0.0089511722 0.062064372 0.070122235 0.16617751 0.28353131 0.34006253 0.29068497 0.16640994][-0.55252 -0.53328437 -0.42236972 -0.32730427 -0.18235579 -0.11806786 -0.071648665 -0.0028180107 0.057047792 0.060846947 0.14848304 0.29657003 0.41560841 0.43934268 0.3794758]]...]
INFO - root - 2017-12-06 09:44:15.920331: step 45310, loss = 0.80, batch loss = 0.58 (12.9 examples/sec; 0.621 sec/batch; 49h:30m:30s remains)
INFO - root - 2017-12-06 09:44:22.543384: step 45320, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.636 sec/batch; 50h:43m:28s remains)
INFO - root - 2017-12-06 09:44:29.107608: step 45330, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.639 sec/batch; 50h:58m:48s remains)
INFO - root - 2017-12-06 09:44:35.631725: step 45340, loss = 0.82, batch loss = 0.60 (11.8 examples/sec; 0.679 sec/batch; 54h:09m:59s remains)
INFO - root - 2017-12-06 09:44:42.070243: step 45350, loss = 0.77, batch loss = 0.56 (11.5 examples/sec; 0.694 sec/batch; 55h:21m:45s remains)
INFO - root - 2017-12-06 09:44:48.598250: step 45360, loss = 0.84, batch loss = 0.63 (12.1 examples/sec; 0.659 sec/batch; 52h:34m:40s remains)
INFO - root - 2017-12-06 09:44:55.133950: step 45370, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.645 sec/batch; 51h:25m:47s remains)
INFO - root - 2017-12-06 09:45:01.738378: step 45380, loss = 0.85, batch loss = 0.64 (12.5 examples/sec; 0.641 sec/batch; 51h:06m:43s remains)
INFO - root - 2017-12-06 09:45:08.284222: step 45390, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.660 sec/batch; 52h:38m:12s remains)
INFO - root - 2017-12-06 09:45:14.699011: step 45400, loss = 0.78, batch loss = 0.57 (12.8 examples/sec; 0.626 sec/batch; 49h:56m:01s remains)
2017-12-06 09:45:15.342486: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.060977377 0.083952047 0.071315773 0.081939422 -0.010223724 -0.13618645 -0.26225388 -0.30984253 -0.3748053 -0.3666302 -0.34303665 -0.29723382 -0.23987693 -0.19423561 -0.20173292][-0.10422973 -0.12988365 -0.13634086 -0.16603363 -0.31710002 -0.44282135 -0.55649143 -0.57766855 -0.598028 -0.6250425 -0.5968973 -0.53180778 -0.45520636 -0.41873163 -0.3644028][-0.21573135 -0.25256827 -0.28384551 -0.37125444 -0.47680542 -0.64268261 -0.73763919 -0.77966875 -0.81228471 -0.8448835 -0.80860168 -0.69293082 -0.59015149 -0.48092282 -0.37836054][-0.2452566 -0.31116843 -0.41171136 -0.52463365 -0.6170513 -0.67523789 -0.70277649 -0.78789663 -0.7865153 -0.81189185 -0.83915651 -0.80595243 -0.743458 -0.63046616 -0.48980904][-0.25871173 -0.43681479 -0.54863828 -0.58123958 -0.61629653 -0.70626438 -0.71001148 -0.71014422 -0.66520172 -0.69686 -0.72021252 -0.73120463 -0.72591865 -0.67794722 -0.61299562][-0.19949366 -0.30596003 -0.3973836 -0.34593934 -0.23720333 -0.20302357 -0.14691798 -0.11708185 -0.067422114 -0.07588575 -0.10719669 -0.16823739 -0.21415204 -0.25016978 -0.25453851][-0.35510936 -0.4434754 -0.45682582 -0.33317012 -0.12122617 0.082903393 0.22775534 0.28201506 0.30877879 0.31564012 0.29176307 0.26437023 0.24040422 0.17570555 0.071275346][-0.18746194 -0.21121123 -0.20406845 -0.06030529 0.18641722 0.39880741 0.51748991 0.56065452 0.58743709 0.62579465 0.613711 0.59545743 0.60916555 0.54705179 0.35180816][-0.30487385 -0.38407576 -0.38157046 -0.20261276 0.081467442 0.38736233 0.61301106 0.74402523 0.79063874 0.75377738 0.70810652 0.69679022 0.66081524 0.60245681 0.49070364][-0.18219723 -0.3437494 -0.42078066 -0.44001526 -0.33181232 -0.099246986 0.13240376 0.27803004 0.36363345 0.33590385 0.40775022 0.58860749 0.65480369 0.53035158 0.36983103][-0.23063642 -0.35806 -0.45510405 -0.44876951 -0.33419013 -0.1780656 0.073859267 0.39497805 0.67186028 0.76058954 0.84735781 1.015263 1.0350642 0.843546 0.59377694][-0.21517812 -0.35377404 -0.48972383 -0.5359084 -0.48450184 -0.35565785 -0.15443984 0.027670614 0.25304762 0.42211097 0.60632235 0.79219609 0.951825 0.88164604 0.7151311][-0.19669643 -0.34251952 -0.41489521 -0.48910713 -0.50083494 -0.41132715 -0.26359037 -0.080686629 0.053946756 0.15292987 0.36983359 0.53995055 0.645029 0.62602884 0.582848][0.1746434 0.065146841 0.061639957 0.047433592 0.029398143 -0.0032950491 -0.060330056 -0.021241106 0.029877745 0.15978232 0.37012792 0.57202435 0.77159357 0.75219935 0.56902027][0.27026027 0.032494716 -0.059035309 -0.072577044 -0.071414441 -0.092326656 -0.10952269 -0.10925233 -0.042850163 0.068874471 0.20531765 0.32871586 0.45478225 0.53758031 0.58248186]]...]
INFO - root - 2017-12-06 09:45:21.804354: step 45410, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 51h:18m:49s remains)
INFO - root - 2017-12-06 09:45:28.245628: step 45420, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.634 sec/batch; 50h:32m:55s remains)
INFO - root - 2017-12-06 09:45:34.655937: step 45430, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 51h:34m:44s remains)
INFO - root - 2017-12-06 09:45:41.166396: step 45440, loss = 0.71, batch loss = 0.50 (12.7 examples/sec; 0.629 sec/batch; 50h:07m:57s remains)
INFO - root - 2017-12-06 09:45:47.535633: step 45450, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.663 sec/batch; 52h:53m:55s remains)
INFO - root - 2017-12-06 09:45:54.113285: step 45460, loss = 0.85, batch loss = 0.63 (11.7 examples/sec; 0.681 sec/batch; 54h:19m:56s remains)
INFO - root - 2017-12-06 09:46:00.620525: step 45470, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.638 sec/batch; 50h:52m:46s remains)
INFO - root - 2017-12-06 09:46:07.150306: step 45480, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 51h:14m:43s remains)
INFO - root - 2017-12-06 09:46:13.690959: step 45490, loss = 0.80, batch loss = 0.58 (11.7 examples/sec; 0.684 sec/batch; 54h:34m:14s remains)
INFO - root - 2017-12-06 09:46:20.289617: step 45500, loss = 0.76, batch loss = 0.54 (12.0 examples/sec; 0.667 sec/batch; 53h:10m:24s remains)
2017-12-06 09:46:20.902735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053647004 -0.053368732 -0.053187761 -0.052811246 -0.0524568 -0.05247882 -0.052707419 -0.05291225 -0.052995566 -0.052976616 -0.052804891 -0.052441671 -0.052307639 -0.052806806 -0.0541249][-0.052412413 -0.052110169 -0.051952895 -0.05167957 -0.05141696 -0.051469404 -0.051746022 -0.051865868 -0.051631361 -0.051273879 -0.050847623 -0.050307795 -0.050099634 -0.05074311 -0.052509494][-0.051475592 -0.050879568 -0.05070062 -0.050499678 -0.050356124 -0.050522495 -0.050900877 -0.051007934 -0.050539196 -0.049898107 -0.04932816 -0.048688509 -0.048503119 -0.049295988 -0.051371224][-0.050746515 -0.049824238 -0.049504906 -0.049345654 -0.049308836 -0.049540631 -0.049963221 -0.0500527 -0.0495524 -0.048797626 -0.04820684 -0.047653716 -0.047592357 -0.048555922 -0.050829004][-0.050084744 -0.048994504 -0.04850569 -0.048323944 -0.048387587 -0.048735887 -0.049194567 -0.04931777 -0.048791185 -0.048053838 -0.047482274 -0.047133137 -0.047344368 -0.048482537 -0.050900795][-0.04952319 -0.048286218 -0.047693193 -0.047482509 -0.047600169 -0.048055913 -0.048582748 -0.048844114 -0.04809618 -0.046280615 -0.0452888 -0.046034932 -0.047334034 -0.049038131 -0.051507872][-0.048591603 -0.047084913 -0.046427786 -0.04633281 -0.046273597 -0.046040237 -0.046005689 -0.046023548 -0.041614037 -0.032842174 -0.027794972 -0.029258359 -0.037265409 -0.047890324 -0.054662764][-0.04263629 -0.0386015 -0.038804669 -0.041138794 -0.041422702 -0.037360433 -0.031406656 -0.023577519 -0.0091127977 0.0064603984 0.014389791 0.017926723 0.0081250593 -0.01414144 -0.037297226][-0.0351787 -0.024871558 -0.018882804 -0.019018129 -0.01934994 -0.015530132 -0.0030719936 0.01869224 0.039282493 0.04217004 0.028314278 0.015177235 0.0031417832 -0.012900099 -0.031200372][-0.050016098 -0.052866179 -0.053872537 -0.052806411 -0.053037565 -0.055641714 -0.056170668 -0.048115753 -0.039624117 -0.044513341 -0.061314143 -0.078364581 -0.080811024 -0.07475578 -0.070747413][-0.052668385 -0.057303343 -0.061020311 -0.06171868 -0.062772751 -0.066360511 -0.073110074 -0.0756701 -0.075395055 -0.075176038 -0.0788281 -0.086686261 -0.088692717 -0.085066728 -0.078189082][-0.049145889 -0.048652388 -0.050030228 -0.051400442 -0.053024966 -0.057633061 -0.06460949 -0.0725875 -0.078391358 -0.079518408 -0.079410851 -0.081672072 -0.090193532 -0.095044427 -0.090136558][-0.051420096 -0.051167119 -0.052624792 -0.053878926 -0.055726975 -0.05926713 -0.06539946 -0.076405667 -0.0872506 -0.091458626 -0.088475391 -0.08748731 -0.092418 -0.095677905 -0.093021974][-0.054447766 -0.054935236 -0.056032196 -0.057243444 -0.057889592 -0.059739329 -0.063364744 -0.072273612 -0.086638421 -0.10055527 -0.10748681 -0.10827938 -0.10446731 -0.097177371 -0.089344136][-0.056600232 -0.05666507 -0.057907771 -0.059046715 -0.059245806 -0.060582764 -0.062626414 -0.067637041 -0.077657931 -0.090726927 -0.10013095 -0.10198503 -0.098372154 -0.090761758 -0.083477728]]...]
INFO - root - 2017-12-06 09:46:27.371031: step 45510, loss = 0.79, batch loss = 0.57 (12.1 examples/sec; 0.660 sec/batch; 52h:38m:51s remains)
INFO - root - 2017-12-06 09:46:33.975208: step 45520, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.657 sec/batch; 52h:20m:41s remains)
INFO - root - 2017-12-06 09:46:40.510993: step 45530, loss = 0.86, batch loss = 0.65 (11.7 examples/sec; 0.684 sec/batch; 54h:29m:29s remains)
INFO - root - 2017-12-06 09:46:46.949717: step 45540, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.646 sec/batch; 51h:28m:32s remains)
INFO - root - 2017-12-06 09:46:53.548250: step 45550, loss = 0.85, batch loss = 0.63 (12.4 examples/sec; 0.644 sec/batch; 51h:19m:24s remains)
INFO - root - 2017-12-06 09:46:59.887570: step 45560, loss = 0.86, batch loss = 0.64 (11.9 examples/sec; 0.670 sec/batch; 53h:24m:33s remains)
INFO - root - 2017-12-06 09:47:06.507427: step 45570, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.660 sec/batch; 52h:34m:55s remains)
INFO - root - 2017-12-06 09:47:13.045998: step 45580, loss = 0.82, batch loss = 0.60 (12.3 examples/sec; 0.649 sec/batch; 51h:45m:44s remains)
INFO - root - 2017-12-06 09:47:19.538369: step 45590, loss = 0.75, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 52h:30m:38s remains)
INFO - root - 2017-12-06 09:47:26.136489: step 45600, loss = 0.75, batch loss = 0.53 (12.3 examples/sec; 0.648 sec/batch; 51h:40m:51s remains)
2017-12-06 09:47:26.746567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.098477677 -0.10019776 -0.10118485 -0.10161687 -0.10175443 -0.10172303 -0.10172315 -0.10176945 -0.10174768 -0.10162838 -0.10166302 -0.10176609 -0.10192174 -0.10228534 -0.10217909][-0.098667026 -0.10040118 -0.10128197 -0.1016169 -0.10179665 -0.10182102 -0.10181634 -0.10191054 -0.1019957 -0.10204804 -0.1020857 -0.10227989 -0.10251383 -0.1027487 -0.10259622][-0.093285233 -0.094269745 -0.09469305 -0.094923012 -0.095161624 -0.095311038 -0.095484674 -0.095739163 -0.095982388 -0.096232787 -0.096322656 -0.096258119 -0.096209973 -0.096237041 -0.096159413][-0.083360627 -0.083356507 -0.08334849 -0.083454445 -0.083701484 -0.083982058 -0.084462941 -0.085078366 -0.085610487 -0.085847043 -0.085816473 -0.085506633 -0.08522673 -0.085065849 -0.085311539][-0.073163085 -0.0724061 -0.072165839 -0.072371125 -0.072794825 -0.073292911 -0.074023873 -0.074829623 -0.075365752 -0.075582288 -0.075537831 -0.075215138 -0.074928366 -0.074619919 -0.0750033][-0.063391246 -0.062223859 -0.061866548 -0.06200029 -0.062574148 -0.063409172 -0.064423874 -0.065346137 -0.065851942 -0.065964721 -0.065786019 -0.065498367 -0.065245442 -0.065006204 -0.065632649][-0.055920392 -0.054200049 -0.053696193 -0.053853448 -0.05478321 -0.05584307 -0.057022583 -0.058023531 -0.058600418 -0.058758896 -0.058573309 -0.058352496 -0.058201905 -0.058196116 -0.059094086][-0.050796878 -0.04864708 -0.048148673 -0.04859075 -0.049674168 -0.050810352 -0.051754061 -0.052512936 -0.05329765 -0.053854115 -0.053901531 -0.053895477 -0.053798079 -0.053898435 -0.054942396][-0.049243003 -0.047105815 -0.046882458 -0.047416884 -0.048317153 -0.049175184 -0.049732283 -0.0502206 -0.0512183 -0.052149441 -0.052367505 -0.052504744 -0.052630544 -0.052952655 -0.05381275][-0.051494706 -0.049875077 -0.049954291 -0.050351825 -0.050915331 -0.05156384 -0.052082442 -0.052748192 -0.053601783 -0.054121111 -0.054070249 -0.054137181 -0.054488119 -0.055028073 -0.05579574][-0.0559549 -0.054799467 -0.055069789 -0.055241253 -0.055593938 -0.056082983 -0.056510434 -0.057004597 -0.057650831 -0.057758603 -0.057291143 -0.057166882 -0.057782017 -0.058601763 -0.059243694][-0.061868027 -0.061031133 -0.061131544 -0.061096467 -0.061193995 -0.061247662 -0.061519131 -0.061912835 -0.062486436 -0.062262066 -0.061617278 -0.061461233 -0.062117897 -0.06303782 -0.063512683][-0.069914982 -0.069816761 -0.070240989 -0.07011582 -0.069668934 -0.069018975 -0.068987928 -0.069082811 -0.06918253 -0.06871932 -0.068052255 -0.068045363 -0.068700328 -0.069482721 -0.069928974][-0.078911886 -0.079588838 -0.0802351 -0.079891875 -0.079366952 -0.078777343 -0.078635648 -0.078327075 -0.077847578 -0.077204704 -0.07662718 -0.076781422 -0.077277884 -0.0778795 -0.07833796][-0.086897649 -0.087993987 -0.088830411 -0.0888639 -0.088883542 -0.088768318 -0.088328041 -0.087692991 -0.086729206 -0.085779838 -0.085370213 -0.085721821 -0.086011976 -0.086235382 -0.086513467]]...]
INFO - root - 2017-12-06 09:47:33.362238: step 45610, loss = 0.90, batch loss = 0.68 (11.9 examples/sec; 0.674 sec/batch; 53h:41m:32s remains)
INFO - root - 2017-12-06 09:47:39.854403: step 45620, loss = 0.80, batch loss = 0.59 (12.2 examples/sec; 0.656 sec/batch; 52h:14m:44s remains)
INFO - root - 2017-12-06 09:47:46.213131: step 45630, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.653 sec/batch; 52h:03m:05s remains)
INFO - root - 2017-12-06 09:47:52.675561: step 45640, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.671 sec/batch; 53h:29m:50s remains)
INFO - root - 2017-12-06 09:47:59.163083: step 45650, loss = 0.85, batch loss = 0.63 (12.4 examples/sec; 0.646 sec/batch; 51h:29m:21s remains)
INFO - root - 2017-12-06 09:48:05.653298: step 45660, loss = 0.79, batch loss = 0.57 (12.0 examples/sec; 0.669 sec/batch; 53h:17m:09s remains)
INFO - root - 2017-12-06 09:48:12.230547: step 45670, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.657 sec/batch; 52h:18m:25s remains)
INFO - root - 2017-12-06 09:48:18.801984: step 45680, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.667 sec/batch; 53h:06m:52s remains)
INFO - root - 2017-12-06 09:48:25.351666: step 45690, loss = 0.74, batch loss = 0.52 (12.6 examples/sec; 0.637 sec/batch; 50h:44m:31s remains)
INFO - root - 2017-12-06 09:48:31.732366: step 45700, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.639 sec/batch; 50h:56m:38s remains)
2017-12-06 09:48:32.405603: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.089039765 -0.44759887 -0.36673498 -0.34187472 -1.0985088 -1.3048967 -1.2554612 -1.0507345 -0.99430114 -1.5010378 -1.5993267 -0.99383289 -0.64214355 -0.48173437 -0.56666011][0.74621868 0.083216541 -0.18599983 -0.21386904 -0.457264 -0.77159536 -1.2043349 -1.3824198 -1.1706138 -0.92085409 -0.71057123 -0.80848968 -0.47996342 -0.054768194 0.18943939][0.4545944 0.2668317 0.045800306 0.10729549 -0.28773084 -0.72232741 -0.71376038 -0.63263726 -0.80468988 -0.89639342 -0.53868204 -0.43125778 -0.64217794 -0.33133838 0.12986717][1.1486058 0.68969733 0.13746974 0.38651395 0.821375 0.4217793 -0.052288685 -0.059706748 0.13456196 0.18687713 0.25922883 0.47293767 0.5849278 0.42034081 0.3673915][0.8867141 0.91165417 0.90990514 1.0432897 1.3841614 1.2222403 0.92960888 0.69509876 0.55537927 0.56903219 0.77077711 0.77144438 0.44616666 0.44936183 0.92422718][1.6821393 1.2783587 1.0255193 1.2268114 1.4774144 1.2961982 0.97176808 0.5993911 0.30783352 0.09416268 0.11300161 0.3966921 0.520591 0.28068864 0.4782736][2.248373 1.936774 1.5656205 1.1408997 0.74618208 0.56417954 0.62125355 0.51546311 0.35437098 0.20761019 0.16738147 0.090731777 0.039649777 0.43119618 1.1764206][1.6638551 1.4971385 1.1593174 0.83061463 0.15895829 0.079573445 -0.21081051 -0.36686629 -0.191715 -0.18297303 -0.053872794 0.28939581 0.40290657 -0.034887068 0.26153797][0.88949561 0.92877084 0.62129992 0.25802261 -0.1669552 -0.6500169 -0.85083508 -0.71685404 -0.57990038 -0.27137995 0.069996737 0.16856122 0.099328808 -0.083350837 0.19251052][0.81110162 0.44045684 0.36232629 0.1556797 -0.40704778 -0.952208 -1.0843182 -1.1953127 -1.0251884 -0.7534287 -0.61314827 0.023736969 0.33792591 -0.21179613 -0.0045587048][0.46194437 0.65641367 0.50581348 0.049155079 -0.39968416 -1.0618101 -1.5304832 -1.4299605 -1.4061961 -1.7453172 -1.4822083 -0.84730637 -0.53162718 -0.49710861 -0.40650216][-0.26664695 -0.56773549 -0.79331809 -0.75786519 -0.86821532 -1.134346 -1.4967278 -1.7206334 -1.5693847 -1.6696775 -1.8339806 -1.5429815 -1.3458433 -1.5668845 -0.95331365][-0.32955357 -0.53057969 -0.47362813 -0.88550627 -1.0572289 -1.085535 -1.1208196 -1.2214671 -1.0570637 -1.1711842 -1.0898649 -0.80363184 -0.73448658 -0.60039556 -0.15404049][-0.14310841 -0.1929341 0.18537262 0.10454588 -0.29790452 -0.43917087 -0.44031656 -0.38538027 -0.16727519 0.16537163 0.20404273 0.147244 0.27930784 0.20034027 0.42037758][0.65404147 0.26244169 0.35634309 0.40708473 0.090118714 -0.0075544342 -0.17673835 -0.075476818 0.16597331 0.37313616 0.48479179 0.58901548 0.73163748 0.49362996 0.38989869]]...]
INFO - root - 2017-12-06 09:48:38.844006: step 45710, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.637 sec/batch; 50h:45m:33s remains)
INFO - root - 2017-12-06 09:48:45.390155: step 45720, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.632 sec/batch; 50h:23m:03s remains)
INFO - root - 2017-12-06 09:48:51.682944: step 45730, loss = 0.83, batch loss = 0.61 (12.5 examples/sec; 0.639 sec/batch; 50h:54m:39s remains)
INFO - root - 2017-12-06 09:48:58.203664: step 45740, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 51h:33m:32s remains)
INFO - root - 2017-12-06 09:49:04.705245: step 45750, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.640 sec/batch; 51h:00m:08s remains)
INFO - root - 2017-12-06 09:49:11.237851: step 45760, loss = 0.79, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 52h:04m:55s remains)
INFO - root - 2017-12-06 09:49:17.722710: step 45770, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.647 sec/batch; 51h:33m:33s remains)
INFO - root - 2017-12-06 09:49:24.259173: step 45780, loss = 0.80, batch loss = 0.59 (12.1 examples/sec; 0.659 sec/batch; 52h:27m:28s remains)
INFO - root - 2017-12-06 09:49:30.784894: step 45790, loss = 0.76, batch loss = 0.55 (12.4 examples/sec; 0.646 sec/batch; 51h:27m:41s remains)
INFO - root - 2017-12-06 09:49:37.320951: step 45800, loss = 0.76, batch loss = 0.54 (12.8 examples/sec; 0.626 sec/batch; 49h:52m:17s remains)
2017-12-06 09:49:37.925328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.39746022 -0.55038166 -0.68426621 -0.82107586 -0.9466235 -1.010452 -1.1265838 -1.1133888 -1.0360807 -0.98420441 -0.90261346 -0.83242893 -0.79849434 -0.91486788 -0.96211964][-0.97487128 -1.0597184 -1.2018852 -1.3561691 -1.4455748 -1.4793632 -1.509606 -1.4953846 -1.4959043 -1.2866173 -1.0121143 -0.87031251 -0.81462485 -0.86274815 -0.82767856][-1.3005767 -1.3517634 -1.3669561 -1.4225973 -1.538453 -1.6635424 -1.70969 -1.6065402 -1.5016277 -1.337256 -1.1484424 -0.952233 -0.850054 -0.88362223 -0.8877933][-1.1009535 -1.0618392 -1.0034046 -1.0291101 -1.0919335 -1.1894 -1.3674289 -1.5657437 -1.5384949 -1.3318384 -1.2095525 -1.129022 -1.1012487 -1.1256003 -1.0938144][-0.55195487 -0.42314649 -0.23437251 -0.091418058 -0.072871506 -0.18979776 -0.35360515 -0.62444085 -0.87518418 -0.95870394 -0.92759174 -0.97174424 -1.1229037 -1.2398138 -1.2245587][0.11867473 0.27042854 0.51168048 0.68578941 0.800288 0.78931296 0.53949058 0.27220184 0.19072174 0.085622206 -0.11447256 -0.3267498 -0.66141045 -0.956558 -1.0453668][0.406505 0.45504773 0.6283077 0.80916786 0.99412495 1.0947053 1.0614389 0.99101657 0.878944 0.76285708 0.65086508 0.4531343 0.18166013 -0.11522391 -0.31087035][0.2184722 0.15474162 0.11521451 0.13559449 0.23884691 0.3398993 0.3590703 0.44047469 0.48394966 0.55564392 0.60480523 0.52435082 0.38239104 0.25344044 0.22769476][0.023778923 -0.089929275 -0.19870442 -0.3401126 -0.48355132 -0.62018955 -0.69101948 -0.52855289 -0.28469798 -0.078969836 0.080825821 0.17442133 0.17817135 0.17106213 0.33275479][-0.27367744 -0.34288198 -0.3390823 -0.33635443 -0.51213425 -0.73421 -0.96368378 -1.0542594 -0.99715924 -0.81581503 -0.54106474 -0.28108579 -0.18906564 -0.15737347 0.04848057][-0.60854411 -0.6165207 -0.60356671 -0.50804669 -0.51046407 -0.66847116 -0.85349411 -0.90329623 -0.857394 -0.79952168 -0.601129 -0.40481412 -0.31312647 -0.27345961 -0.14468387][-0.83383942 -0.75576961 -0.5543859 -0.32894719 -0.21269962 -0.17935684 -0.28225434 -0.32753575 -0.37493998 -0.47799647 -0.49225038 -0.41921479 -0.44576639 -0.46366191 -0.38357359][-0.9565562 -0.82636595 -0.58333439 -0.25693512 0.033406578 0.095493242 0.011512093 -0.0851 -0.24551335 -0.34019658 -0.48023754 -0.54009867 -0.546135 -0.53796172 -0.54026985][-0.86630672 -0.81687737 -0.64960235 -0.42377186 -0.14486422 -0.030938126 -0.18590015 -0.27877682 -0.43975943 -0.55815089 -0.55231184 -0.50104523 -0.53124589 -0.55092263 -0.53197771][-0.35710859 -0.35314274 -0.31982428 -0.32181478 -0.328171 -0.32702509 -0.38528836 -0.56538492 -0.72702152 -0.72288233 -0.7201336 -0.62972003 -0.49256176 -0.42383963 -0.38962704]]...]
INFO - root - 2017-12-06 09:49:44.447796: step 45810, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.644 sec/batch; 51h:16m:31s remains)
INFO - root - 2017-12-06 09:49:50.796427: step 45820, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.656 sec/batch; 52h:13m:00s remains)
INFO - root - 2017-12-06 09:49:57.298747: step 45830, loss = 0.76, batch loss = 0.54 (12.4 examples/sec; 0.646 sec/batch; 51h:24m:45s remains)
INFO - root - 2017-12-06 09:50:03.735732: step 45840, loss = 0.88, batch loss = 0.67 (12.6 examples/sec; 0.636 sec/batch; 50h:37m:59s remains)
INFO - root - 2017-12-06 09:50:10.192797: step 45850, loss = 0.79, batch loss = 0.57 (12.4 examples/sec; 0.647 sec/batch; 51h:30m:39s remains)
INFO - root - 2017-12-06 09:50:16.793844: step 45860, loss = 0.84, batch loss = 0.62 (12.6 examples/sec; 0.636 sec/batch; 50h:40m:01s remains)
INFO - root - 2017-12-06 09:50:23.279777: step 45870, loss = 0.83, batch loss = 0.62 (12.9 examples/sec; 0.621 sec/batch; 49h:27m:41s remains)
INFO - root - 2017-12-06 09:50:29.850175: step 45880, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.673 sec/batch; 53h:36m:34s remains)
INFO - root - 2017-12-06 09:50:36.445818: step 45890, loss = 0.78, batch loss = 0.56 (12.6 examples/sec; 0.633 sec/batch; 50h:25m:35s remains)
INFO - root - 2017-12-06 09:50:42.911016: step 45900, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.648 sec/batch; 51h:34m:25s remains)
2017-12-06 09:50:43.656905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.24518223 -0.26553312 -0.30235797 -0.36460781 -0.44949949 -0.57061279 -0.71962005 -0.87698907 -0.98403394 -1.039649 -1.0539807 -0.95517308 -0.74197978 -0.40387648 -0.035751324][-0.59692115 -0.64681512 -0.65595013 -0.62313044 -0.6158936 -0.66312337 -0.81470871 -1.032017 -1.199451 -1.3406829 -1.4290304 -1.3481777 -1.1258265 -0.79946941 -0.52440065][-0.93168783 -1.0541456 -1.0790778 -0.96398908 -0.796653 -0.61664134 -0.56570786 -0.6761573 -0.88442892 -1.1073431 -1.268591 -1.2525963 -1.1683332 -0.97118664 -0.7368961][-1.0153276 -1.1910998 -1.2442191 -1.117144 -0.86854243 -0.48657757 -0.21499838 -0.065067381 -0.092766434 -0.30688107 -0.59164387 -0.72939217 -0.839106 -0.82337177 -0.77886081][-0.77909482 -0.95223975 -1.0045984 -0.89833391 -0.6251176 -0.17060836 0.21575968 0.53717524 0.595895 0.46974647 0.20141001 0.024147779 -0.14207885 -0.26995009 -0.40843713][-0.33909273 -0.44558209 -0.45729905 -0.33172813 -0.034118436 0.43742394 0.89491367 1.3212974 1.4362851 1.3605103 1.0975469 0.76963747 0.37966108 0.21317993 0.095628217][0.08138825 0.049606234 0.051536024 0.14204133 0.38815939 0.82401437 1.2487309 1.66345 1.7463953 1.6536242 1.4088649 1.1089901 0.68544304 0.41135341 0.18659584][0.25556403 0.27332771 0.28717762 0.33619523 0.48749912 0.7792272 1.1053159 1.4510535 1.5145267 1.4539608 1.2869339 1.0340972 0.723968 0.45952135 0.21477838][0.13501275 0.14070845 0.11628178 0.11864088 0.1799048 0.34744972 0.54614657 0.78024679 0.78170317 0.6738649 0.50860322 0.39810097 0.31743532 0.19242258 0.10733753][-0.0784502 -0.14410692 -0.26154062 -0.39388418 -0.46007419 -0.41065639 -0.28366381 -0.070848577 0.03197062 0.060807317 -0.019621655 -0.092517912 -0.085535213 -0.11641011 -0.093344979][-0.24874152 -0.39275998 -0.59459162 -0.81404006 -0.99075466 -1.0200727 -0.91072589 -0.66078568 -0.47600579 -0.3729074 -0.38112402 -0.34968793 -0.24013031 -0.17515406 -0.08737994][-0.2632632 -0.42804396 -0.65991277 -0.90020847 -1.0967345 -1.184763 -1.1271846 -0.88807762 -0.60358745 -0.37765878 -0.35828555 -0.40128535 -0.40025347 -0.36640477 -0.26429659][-0.20394745 -0.29947391 -0.47054613 -0.6943298 -0.88774931 -0.96684092 -0.9296326 -0.76505482 -0.51942569 -0.30069536 -0.20732108 -0.20103626 -0.22000481 -0.25167972 -0.20689936][-0.18567306 -0.2314366 -0.28841138 -0.40038246 -0.54478675 -0.6291573 -0.603903 -0.48815942 -0.31075755 -0.1534633 -0.079519942 -0.087278351 -0.11024087 -0.13808668 -0.10751159][-0.17746837 -0.23497018 -0.27888024 -0.315789 -0.3707549 -0.41024578 -0.40816164 -0.32068419 -0.17422614 -0.045615695 0.016169056 0.014124013 -0.0027465448 -0.036132742 -0.032856632]]...]
INFO - root - 2017-12-06 09:50:50.020647: step 45910, loss = 0.85, batch loss = 0.63 (11.9 examples/sec; 0.670 sec/batch; 53h:18m:17s remains)
INFO - root - 2017-12-06 09:50:56.543189: step 45920, loss = 0.79, batch loss = 0.58 (12.8 examples/sec; 0.627 sec/batch; 49h:53m:09s remains)
INFO - root - 2017-12-06 09:51:03.103228: step 45930, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.638 sec/batch; 50h:48m:56s remains)
INFO - root - 2017-12-06 09:51:09.617683: step 45940, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.637 sec/batch; 50h:43m:22s remains)
INFO - root - 2017-12-06 09:51:16.176647: step 45950, loss = 0.75, batch loss = 0.53 (11.9 examples/sec; 0.671 sec/batch; 53h:25m:28s remains)
INFO - root - 2017-12-06 09:51:22.630856: step 45960, loss = 0.94, batch loss = 0.73 (13.0 examples/sec; 0.615 sec/batch; 48h:55m:14s remains)
INFO - root - 2017-12-06 09:51:29.262505: step 45970, loss = 0.84, batch loss = 0.63 (12.1 examples/sec; 0.663 sec/batch; 52h:46m:37s remains)
INFO - root - 2017-12-06 09:51:35.750024: step 45980, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.668 sec/batch; 53h:10m:52s remains)
INFO - root - 2017-12-06 09:51:42.175963: step 45990, loss = 0.83, batch loss = 0.61 (12.2 examples/sec; 0.654 sec/batch; 52h:04m:14s remains)
INFO - root - 2017-12-06 09:51:48.693191: step 46000, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.648 sec/batch; 51h:36m:05s remains)
2017-12-06 09:51:49.372371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0946891 -0.08998578 -0.22423835 -0.45890766 -0.73520964 -0.94431055 -1.053703 -1.0459247 -0.96615642 -0.81810069 -0.59911025 -0.36553878 -0.15948093 0.0049734861 -0.0040540248][0.050249085 0.096697748 -0.015477195 -0.2899763 -0.68295473 -1.0977237 -1.4054998 -1.5225233 -1.5064005 -1.3410174 -1.0764025 -0.7807678 -0.548037 -0.36164361 -0.28737241][0.15549293 0.25803149 0.23434441 -0.028013654 -0.46056867 -0.95907074 -1.3956237 -1.6684766 -1.7356946 -1.6106189 -1.31751 -0.96983945 -0.65057862 -0.43215317 -0.36080712][0.16904332 0.29101086 0.41248941 0.25233972 -0.096638441 -0.59477949 -1.1441247 -1.5618563 -1.7281013 -1.6501476 -1.3754896 -1.0277277 -0.71037006 -0.48188579 -0.38997012][0.09566243 0.17719854 0.37493861 0.35311347 0.15320243 -0.252064 -0.73839992 -1.1881346 -1.4473461 -1.5084145 -1.3410929 -1.0423657 -0.70974189 -0.43323255 -0.27556795][0.028652608 0.10514607 0.37018204 0.46002054 0.44613135 0.19679509 -0.20499837 -0.63543832 -0.95115632 -1.1167945 -1.0649152 -0.87603128 -0.62362927 -0.39781964 -0.26902211][0.038478434 0.098253518 0.3131448 0.49005991 0.61188036 0.49961263 0.24604906 -0.14021496 -0.48965979 -0.6826539 -0.74370265 -0.65672028 -0.51561338 -0.33410412 -0.18211088][0.061587259 0.12423696 0.30846953 0.53500158 0.74898142 0.77443653 0.669713 0.40976882 0.082877234 -0.16634682 -0.33925983 -0.35484272 -0.31614608 -0.24482477 -0.15122069][0.076129436 0.15678108 0.34011364 0.59379637 0.85428345 0.99504679 0.97223669 0.78989875 0.49171507 0.22108708 -0.022933282 -0.11918575 -0.15478267 -0.14122346 -0.098652512][0.03835614 0.11469947 0.26187986 0.48180717 0.69329405 0.8212986 0.82537574 0.69566697 0.49633557 0.28688002 0.090144232 -0.028126888 -0.079653531 -0.089915283 -0.0769834][-0.058338154 -0.029729862 0.053413853 0.16819172 0.31836319 0.44420105 0.46117282 0.36989415 0.22541444 0.088732645 -0.0096531659 -0.06978792 -0.096330792 -0.10604805 -0.08355432][-0.098825321 -0.102692 -0.070645079 -0.002919279 0.094228074 0.16669543 0.19670035 0.14774279 0.075741991 -0.013455316 -0.076667182 -0.099385016 -0.10064859 -0.10191643 -0.087516822][-0.10521434 -0.13959377 -0.15543526 -0.1348093 -0.055009488 0.0071938708 0.026677661 -0.010897554 -0.051153671 -0.0843016 -0.10867164 -0.10533399 -0.094794273 -0.082640179 -0.079737216][-0.091533557 -0.11000226 -0.13788168 -0.1405012 -0.1120954 -0.044346515 -0.044454087 -0.077017009 -0.10503196 -0.11831736 -0.12145044 -0.10754849 -0.089246139 -0.074444972 -0.072443455][-0.089186646 -0.10208704 -0.12676127 -0.1299905 -0.11435202 -0.046254061 -0.051696014 -0.086291455 -0.12222925 -0.14220434 -0.14206421 -0.12300476 -0.10209063 -0.084068887 -0.074007012]]...]
INFO - root - 2017-12-06 09:51:55.460829: step 46010, loss = 0.78, batch loss = 0.56 (12.6 examples/sec; 0.635 sec/batch; 50h:33m:10s remains)
INFO - root - 2017-12-06 09:52:02.016778: step 46020, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.660 sec/batch; 52h:33m:39s remains)
INFO - root - 2017-12-06 09:52:08.450570: step 46030, loss = 0.79, batch loss = 0.58 (12.4 examples/sec; 0.645 sec/batch; 51h:21m:40s remains)
INFO - root - 2017-12-06 09:52:15.030843: step 46040, loss = 0.76, batch loss = 0.55 (11.9 examples/sec; 0.674 sec/batch; 53h:40m:17s remains)
INFO - root - 2017-12-06 09:52:21.548466: step 46050, loss = 0.84, batch loss = 0.62 (12.7 examples/sec; 0.630 sec/batch; 50h:09m:02s remains)
INFO - root - 2017-12-06 09:52:27.990623: step 46060, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.646 sec/batch; 51h:24m:35s remains)
INFO - root - 2017-12-06 09:52:34.591110: step 46070, loss = 0.75, batch loss = 0.54 (12.2 examples/sec; 0.654 sec/batch; 52h:02m:07s remains)
INFO - root - 2017-12-06 09:52:41.093310: step 46080, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.644 sec/batch; 51h:12m:52s remains)
INFO - root - 2017-12-06 09:52:47.583266: step 46090, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.642 sec/batch; 51h:02m:35s remains)
INFO - root - 2017-12-06 09:52:54.039046: step 46100, loss = 0.88, batch loss = 0.67 (12.1 examples/sec; 0.661 sec/batch; 52h:34m:59s remains)
2017-12-06 09:52:54.691549: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053163975 -0.21816815 -0.54908979 -0.81315082 -1.3713055 -1.8067852 -2.2050266 -2.0989704 -2.2486546 -2.2477653 -1.6876643 -1.1058866 -0.2126503 -0.34749848 -0.45159864][0.60648763 0.17000474 -0.44124293 -0.4601208 -0.73335147 -1.4389508 -2.1542761 -2.2289877 -2.2381361 -2.3661058 -2.2126832 -2.0831428 -1.5349541 -0.81826311 -0.057790708][0.86333936 0.96427447 0.99946362 0.55098456 -0.24417798 -0.73984504 -1.2301638 -1.842526 -2.334408 -2.3747029 -2.4464304 -2.1610961 -1.7319734 -1.2575111 -1.3378426][0.79169649 0.8481943 0.82734209 0.99406475 0.92156142 -0.060673155 -1.0463637 -1.5635726 -1.750245 -2.0118585 -2.3346436 -2.0455408 -1.7694392 -1.0992314 -1.1242486][0.9379589 0.96036142 1.2436664 1.0072819 0.4750604 0.45026392 0.26763248 -0.38825011 -1.1324847 -1.2012151 -1.412276 -1.5491636 -1.3359469 -1.060455 -0.97071004][1.5649217 1.6880181 1.7860469 1.6942488 1.4012583 0.81310868 0.19411053 0.045118466 -0.0071608573 -0.48875767 -0.92977744 -0.51806462 -0.12709339 -0.28327107 -1.085271][1.8742741 1.7616711 1.4873153 1.2426639 0.82625747 0.72830284 0.54738671 0.065123692 -0.1470401 -0.35676348 -0.11806711 -0.061303496 0.061664969 0.73795444 0.44432962][1.9168301 1.5147048 1.1442103 0.64247191 0.40756762 0.095141128 -0.15304379 -0.12550884 0.1325698 0.14255503 0.33220094 0.84704578 1.9549816 1.8423214 0.98325247][0.79581809 0.65035105 0.27942789 -0.15839951 -0.36571056 -0.29657739 -0.59159678 -0.80985785 -0.9456507 -0.52928519 0.25519842 0.95074242 1.289638 2.0257378 2.1061807][-0.62080294 -0.89312023 -0.96986908 -1.0479574 -1.0297962 -1.3001238 -1.4960638 -1.5742396 -1.6422708 -1.2891827 -0.74845517 -0.05438235 0.58065283 0.8514117 1.3181114][-1.154441 -1.5626407 -1.87769 -2.1474614 -2.4573741 -2.392379 -2.2081451 -2.2778401 -2.3762293 -2.3917108 -2.3224661 -1.6140653 -1.0515471 -0.13179559 0.42346406][-1.1770275 -1.9706055 -2.3912952 -2.2356493 -2.562917 -2.8852825 -3.3220136 -3.3815157 -3.4172978 -3.2819161 -2.9305224 -2.6104312 -2.1788795 -2.0950637 -1.555002][-0.8553831 -1.2821863 -1.5947037 -1.9016463 -2.714776 -2.6310918 -2.7996857 -3.0666492 -3.2491484 -3.037329 -2.5369773 -2.0075622 -1.5549486 -1.5194658 -1.7193848][-1.2588999 -0.98738593 -0.858169 -0.68045789 -0.80128497 -1.6336111 -2.1915727 -2.1875877 -1.8946933 -1.7403198 -1.3490419 -1.1822931 -1.1492682 -1.2291799 -1.0839463][0.28794074 -0.006250456 -0.43278193 -0.54325694 -0.83614069 -0.84072864 -1.103076 -1.624856 -1.940313 -1.8532926 -1.6957486 -1.2467798 -1.2747036 -1.5176675 -1.6630572]]...]
INFO - root - 2017-12-06 09:53:01.208474: step 46110, loss = 0.83, batch loss = 0.61 (12.5 examples/sec; 0.642 sec/batch; 51h:04m:14s remains)
INFO - root - 2017-12-06 09:53:07.593846: step 46120, loss = 0.75, batch loss = 0.53 (12.2 examples/sec; 0.655 sec/batch; 52h:07m:42s remains)
INFO - root - 2017-12-06 09:53:14.042880: step 46130, loss = 0.82, batch loss = 0.61 (11.8 examples/sec; 0.679 sec/batch; 54h:02m:09s remains)
INFO - root - 2017-12-06 09:53:20.565972: step 46140, loss = 0.88, batch loss = 0.67 (12.1 examples/sec; 0.660 sec/batch; 52h:30m:37s remains)
INFO - root - 2017-12-06 09:53:27.070346: step 46150, loss = 0.76, batch loss = 0.54 (12.7 examples/sec; 0.628 sec/batch; 49h:56m:48s remains)
INFO - root - 2017-12-06 09:53:33.596878: step 46160, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.644 sec/batch; 51h:13m:48s remains)
INFO - root - 2017-12-06 09:53:40.110749: step 46170, loss = 0.78, batch loss = 0.57 (12.0 examples/sec; 0.665 sec/batch; 52h:54m:43s remains)
INFO - root - 2017-12-06 09:53:46.642303: step 46180, loss = 0.78, batch loss = 0.57 (11.9 examples/sec; 0.671 sec/batch; 53h:21m:24s remains)
INFO - root - 2017-12-06 09:53:53.156604: step 46190, loss = 0.81, batch loss = 0.59 (12.9 examples/sec; 0.621 sec/batch; 49h:25m:02s remains)
INFO - root - 2017-12-06 09:53:59.629240: step 46200, loss = 0.87, batch loss = 0.65 (11.5 examples/sec; 0.697 sec/batch; 55h:26m:03s remains)
2017-12-06 09:54:00.291177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013661981 -0.01484067 -0.015638515 -0.016538389 -0.017575912 -0.018760353 -0.019979611 -0.020922147 -0.021233454 -0.020694055 -0.019790031 -0.019364499 -0.020024516 -0.02198188 -0.02531977][-0.0049343333 -0.0064865053 -0.0076353848 -0.0087124482 -0.0099584162 -0.011202507 -0.012375303 -0.012922183 -0.012643367 -0.011575729 -0.0104471 -0.010289699 -0.011902228 -0.015339032 -0.02015987][0.0020399466 0.000404045 -0.00080388784 -0.0017069355 -0.0026301816 -0.0031828135 -0.0034748465 -0.0030469075 -0.0018838346 -0.000481382 0.00041603297 -0.00032326579 -0.003312923 -0.00843209 -0.014964171][0.0062692761 0.0047237277 0.003728345 0.0034290552 0.0036967397 0.004715614 0.0060034767 0.0078587011 0.0097691268 0.011069067 0.011220157 0.0090499148 0.0043436512 -0.0024133325 -0.010470934][0.00691317 0.0058323294 0.0057718903 0.0068778545 0.0088485628 0.011804141 0.014891125 0.018017709 0.020145513 0.020567358 0.019271046 0.015380792 0.0091528222 0.0012747571 -0.0076894909][0.0034435615 0.0034078136 0.0047369525 0.0080315918 0.012426399 0.017746851 0.022481181 0.026010454 0.027470186 0.026229374 0.022921182 0.017141834 0.009842366 0.0016505867 -0.0072324723][-0.0037219003 -0.0020979866 0.0011447519 0.0066546872 0.013492793 0.020745553 0.026640035 0.029803462 0.029953361 0.026795551 0.021278009 0.014083616 0.0063396692 -0.0012939125 -0.0093061849][-0.011976048 -0.0088640377 -0.0038188398 0.0033825263 0.011718176 0.0197903 0.025874928 0.028286345 0.026854768 0.02211123 0.015134208 0.0074265376 -9.8690391e-05 -0.0067751557 -0.013712764][-0.01786989 -0.013922371 -0.0080064833 -0.00015235692 0.0084457546 0.01631368 0.021563329 0.022518031 0.019482523 0.013567008 0.0062710345 -0.0010670573 -0.0077698231 -0.013452895 -0.019115061][-0.02060321 -0.016355574 -0.01048173 -0.0031248108 0.0046783835 0.011245385 0.014923632 0.014383979 0.010413043 0.0042434111 -0.0026663393 -0.0090591535 -0.014620572 -0.01929611 -0.023724489][-0.022280388 -0.018034905 -0.012843922 -0.0066372752 -0.00015413761 0.0050487518 0.007339932 0.0060372427 0.0020847693 -0.003402248 -0.0092791989 -0.01460252 -0.01907298 -0.022865735 -0.026465848][-0.023482077 -0.019921198 -0.015859574 -0.011010438 -0.0060315952 -0.0021335259 -0.00042892247 -0.0014967695 -0.0045398548 -0.00877969 -0.013342507 -0.017501295 -0.021024548 -0.024206124 -0.02741041][-0.024109609 -0.021632776 -0.019028775 -0.015760541 -0.012209184 -0.0091890395 -0.007619679 -0.0079628676 -0.0097113177 -0.01242274 -0.015578389 -0.01856561 -0.021191664 -0.02403637 -0.027190477][-0.024236307 -0.022791289 -0.021495678 -0.019673102 -0.017418295 -0.015102856 -0.013512321 -0.013046563 -0.013599955 -0.014887795 -0.016626656 -0.018603742 -0.020640239 -0.023284905 -0.026508138][-0.023784682 -0.023224726 -0.023052424 -0.022406794 -0.021270931 -0.019678295 -0.018211924 -0.017155319 -0.0167398 -0.016790204 -0.01737643 -0.018594779 -0.020157009 -0.022587992 -0.025767103]]...]
INFO - root - 2017-12-06 09:54:06.744367: step 46210, loss = 0.74, batch loss = 0.52 (12.2 examples/sec; 0.655 sec/batch; 52h:05m:42s remains)
INFO - root - 2017-12-06 09:54:13.358927: step 46220, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.656 sec/batch; 52h:09m:51s remains)
INFO - root - 2017-12-06 09:54:19.880581: step 46230, loss = 0.82, batch loss = 0.60 (11.7 examples/sec; 0.682 sec/batch; 54h:11m:53s remains)
INFO - root - 2017-12-06 09:54:26.310606: step 46240, loss = 0.86, batch loss = 0.65 (12.2 examples/sec; 0.656 sec/batch; 52h:09m:50s remains)
INFO - root - 2017-12-06 09:54:32.803821: step 46250, loss = 0.75, batch loss = 0.54 (12.7 examples/sec; 0.631 sec/batch; 50h:09m:09s remains)
INFO - root - 2017-12-06 09:54:39.251736: step 46260, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.666 sec/batch; 52h:55m:57s remains)
INFO - root - 2017-12-06 09:54:45.775233: step 46270, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.657 sec/batch; 52h:14m:36s remains)
INFO - root - 2017-12-06 09:54:52.265456: step 46280, loss = 0.87, batch loss = 0.66 (12.1 examples/sec; 0.662 sec/batch; 52h:37m:11s remains)
INFO - root - 2017-12-06 09:54:58.766042: step 46290, loss = 0.84, batch loss = 0.63 (11.7 examples/sec; 0.681 sec/batch; 54h:10m:38s remains)
INFO - root - 2017-12-06 09:55:05.183225: step 46300, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 50h:55m:20s remains)
2017-12-06 09:55:05.817749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.24832875 -0.25308788 -0.25891721 -0.26559526 -0.26725057 -0.26337057 -0.26788542 -0.24930784 -0.20785111 -0.19949892 -0.2052394 -0.20016392 -0.1875959 -0.15098903 -0.12413166][-0.28364444 -0.31177148 -0.32224146 -0.35472912 -0.39391771 -0.43622252 -0.48129418 -0.46529022 -0.38466606 -0.32606831 -0.30602351 -0.30680197 -0.31488734 -0.25662237 -0.18150252][-0.28539258 -0.32798603 -0.34527919 -0.40321526 -0.4994339 -0.61330044 -0.7476843 -0.78532487 -0.71142173 -0.61610371 -0.55438215 -0.53447211 -0.53840059 -0.45070344 -0.30314419][-0.239176 -0.30567688 -0.33475918 -0.39513239 -0.51137096 -0.65029132 -0.83216745 -0.95777488 -0.95402 -0.88202816 -0.7897079 -0.7306267 -0.72237444 -0.61724091 -0.43311456][-0.11723875 -0.20342487 -0.23239496 -0.25880861 -0.35076 -0.44692388 -0.60453403 -0.75644624 -0.79256219 -0.77174383 -0.68169034 -0.62149572 -0.61920589 -0.55038029 -0.42045468][-0.03764629 -0.066130914 -4.3720007e-05 0.070631318 0.082315005 0.05683773 -0.029939834 -0.13521162 -0.12811294 -0.13007011 -0.095072635 -0.10694435 -0.16814429 -0.22325575 -0.25613704][-0.078973122 -0.071472645 0.085980363 0.29681244 0.4888618 0.61314368 0.67300278 0.6775443 0.74772424 0.79403162 0.78591859 0.63521534 0.41954088 0.21659154 0.016134568][-0.10165205 -0.080322 0.073469542 0.27514434 0.505396 0.71012318 0.91565353 1.1057925 1.3177804 1.4096774 1.3591825 1.137549 0.80876619 0.49563086 0.23266518][-0.12099085 -0.090375766 0.034244478 0.1789352 0.35739911 0.52787656 0.75072032 0.97308713 1.2117232 1.3863765 1.394701 1.1887069 0.87483937 0.57765096 0.29679757][-0.15324365 -0.12936413 -0.035032474 0.10296082 0.27085939 0.43154627 0.61388052 0.75309616 0.88937622 1.0030506 0.98174435 0.834451 0.61483556 0.39790222 0.19912976][-0.19363871 -0.19369748 -0.13077652 -0.034321927 0.089500777 0.23479551 0.37508658 0.45769954 0.53822875 0.6095584 0.60656446 0.50961572 0.3511768 0.19753733 0.067673065][-0.21535939 -0.26169494 -0.27637812 -0.25703034 -0.22324836 -0.17929626 -0.10860336 -0.048970129 0.024413258 0.10795664 0.18349403 0.23430917 0.17263624 0.042183213 -0.05751463][-0.22038493 -0.29514816 -0.36110073 -0.43111926 -0.48149836 -0.51238257 -0.50452262 -0.47392949 -0.36961624 -0.28254521 -0.20162997 -0.056950275 -0.016954698 -0.035136059 -0.081130467][-0.17940092 -0.24469846 -0.32642847 -0.40338519 -0.44691363 -0.48172432 -0.48590639 -0.44757554 -0.37748918 -0.30020592 -0.24979296 -0.13806057 -0.11281675 -0.1126897 -0.11142348][-0.11372453 -0.13427347 -0.16415168 -0.19246386 -0.2174376 -0.24297988 -0.26259333 -0.23002198 -0.17702328 -0.12587526 -0.07108216 -0.05427257 -0.046501357 -0.060451575 -0.095597349]]...]
INFO - root - 2017-12-06 09:55:12.372365: step 46310, loss = 0.89, batch loss = 0.67 (12.5 examples/sec; 0.638 sec/batch; 50h:42m:45s remains)
INFO - root - 2017-12-06 09:55:18.915925: step 46320, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 51h:03m:06s remains)
INFO - root - 2017-12-06 09:55:25.454976: step 46330, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.658 sec/batch; 52h:19m:08s remains)
INFO - root - 2017-12-06 09:55:32.074955: step 46340, loss = 0.84, batch loss = 0.62 (11.7 examples/sec; 0.685 sec/batch; 54h:26m:02s remains)
INFO - root - 2017-12-06 09:55:38.658286: step 46350, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.659 sec/batch; 52h:21m:32s remains)
INFO - root - 2017-12-06 09:55:45.179596: step 46360, loss = 0.80, batch loss = 0.59 (12.9 examples/sec; 0.619 sec/batch; 49h:13m:14s remains)
INFO - root - 2017-12-06 09:55:51.738034: step 46370, loss = 0.85, batch loss = 0.64 (12.5 examples/sec; 0.641 sec/batch; 50h:59m:08s remains)
INFO - root - 2017-12-06 09:55:58.183708: step 46380, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.657 sec/batch; 52h:11m:35s remains)
INFO - root - 2017-12-06 09:56:04.610375: step 46390, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.641 sec/batch; 50h:57m:19s remains)
INFO - root - 2017-12-06 09:56:11.047615: step 46400, loss = 0.80, batch loss = 0.58 (14.8 examples/sec; 0.541 sec/batch; 43h:01m:58s remains)
2017-12-06 09:56:11.734690: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12378753 -0.15345331 -0.20511815 -0.28396994 -0.37965721 -0.47302419 -0.55723345 -0.63319057 -0.68833971 -0.6935035 -0.66538584 -0.60508668 -0.52719504 -0.47996825 -0.47039342][-0.14879721 -0.2067477 -0.28847921 -0.37678736 -0.47589052 -0.57415712 -0.66434836 -0.75705254 -0.82141 -0.79722309 -0.73158389 -0.61772752 -0.47637028 -0.363966 -0.30455908][-0.18925495 -0.28139165 -0.38854364 -0.4796125 -0.55533677 -0.61359304 -0.69389617 -0.79168582 -0.85523736 -0.78022587 -0.65924507 -0.46371332 -0.24847487 -0.094967827 -0.011452764][-0.24337938 -0.36790475 -0.47981909 -0.54428846 -0.5733946 -0.57510465 -0.62016678 -0.69826823 -0.75778419 -0.68250877 -0.55699861 -0.32841519 -0.06787321 0.11921623 0.20480135][-0.30311212 -0.43919525 -0.49877864 -0.41681024 -0.28310233 -0.16920587 -0.18499452 -0.31924856 -0.48033959 -0.4771629 -0.380604 -0.1701045 0.065567471 0.25606933 0.31584105][-0.35615781 -0.4801555 -0.43930086 -0.15885788 0.26031211 0.62832826 0.71836072 0.56611395 0.28439626 0.047317587 -0.021891423 0.022242278 0.12887231 0.24462149 0.30161738][-0.38086519 -0.4748742 -0.31662068 0.19583881 0.86818093 1.5566282 1.8976501 1.7504767 1.3163854 0.79772741 0.48062938 0.2783674 0.19892231 0.15180537 0.11933563][-0.34252319 -0.37313834 -0.11156297 0.54334974 1.3763976 2.1600981 2.5630777 2.4470909 1.979346 1.286925 0.77591527 0.44074631 0.25614849 0.086011387 0.0073702708][-0.29586414 -0.29568663 -0.048135549 0.52766836 1.2906796 1.9765427 2.2554407 2.0976987 1.6763923 1.0425594 0.53887862 0.19228879 -0.0069506988 -0.15775517 -0.23219797][-0.28591794 -0.34330079 -0.23292935 0.13554603 0.63078451 1.0964411 1.2644789 1.0818181 0.70895481 0.27638894 -0.068013534 -0.26273328 -0.35024902 -0.4319635 -0.45491219][-0.25998774 -0.36614916 -0.39507133 -0.28076032 -0.070304684 0.15228173 0.2131747 0.093056284 -0.13830298 -0.32877639 -0.45686969 -0.4402858 -0.39033556 -0.35197589 -0.30587006][-0.24319854 -0.38657352 -0.52169263 -0.5857237 -0.55742341 -0.45740461 -0.35372919 -0.31791383 -0.3783645 -0.4131301 -0.38371912 -0.29063609 -0.24323401 -0.18776494 -0.12776208][-0.20649207 -0.3366265 -0.49083218 -0.62278962 -0.67943329 -0.63591719 -0.51444554 -0.38263202 -0.2949264 -0.24147385 -0.19571811 -0.15082751 -0.14110109 -0.10728855 -0.06660296][-0.14067659 -0.20704983 -0.3066009 -0.41158348 -0.48842004 -0.51745027 -0.45772046 -0.35692802 -0.24705908 -0.16391769 -0.10533762 -0.072819561 -0.074789405 -0.075404182 -0.076364353][-0.10901318 -0.12197542 -0.14629726 -0.18180285 -0.21597189 -0.23315084 -0.23031867 -0.21280929 -0.17100012 -0.12859802 -0.07607086 -0.0605539 -0.082817525 -0.10565756 -0.11950251]]...]
INFO - root - 2017-12-06 09:56:18.243764: step 46410, loss = 0.72, batch loss = 0.50 (12.2 examples/sec; 0.655 sec/batch; 52h:03m:44s remains)
INFO - root - 2017-12-06 09:56:24.751621: step 46420, loss = 0.73, batch loss = 0.52 (12.1 examples/sec; 0.662 sec/batch; 52h:34m:23s remains)
INFO - root - 2017-12-06 09:56:31.345153: step 46430, loss = 0.87, batch loss = 0.65 (12.1 examples/sec; 0.661 sec/batch; 52h:32m:53s remains)
INFO - root - 2017-12-06 09:56:37.816849: step 46440, loss = 0.86, batch loss = 0.65 (12.6 examples/sec; 0.635 sec/batch; 50h:25m:13s remains)
INFO - root - 2017-12-06 09:56:44.333853: step 46450, loss = 0.78, batch loss = 0.56 (13.1 examples/sec; 0.609 sec/batch; 48h:24m:12s remains)
INFO - root - 2017-12-06 09:56:50.851583: step 46460, loss = 0.89, batch loss = 0.67 (11.9 examples/sec; 0.671 sec/batch; 53h:19m:26s remains)
INFO - root - 2017-12-06 09:56:57.292865: step 46470, loss = 0.80, batch loss = 0.59 (14.5 examples/sec; 0.551 sec/batch; 43h:45m:29s remains)
INFO - root - 2017-12-06 09:57:03.767712: step 46480, loss = 0.74, batch loss = 0.52 (12.0 examples/sec; 0.668 sec/batch; 53h:04m:29s remains)
INFO - root - 2017-12-06 09:57:10.303656: step 46490, loss = 0.81, batch loss = 0.60 (12.7 examples/sec; 0.631 sec/batch; 50h:09m:42s remains)
INFO - root - 2017-12-06 09:57:16.768425: step 46500, loss = 0.83, batch loss = 0.62 (12.3 examples/sec; 0.650 sec/batch; 51h:38m:28s remains)
2017-12-06 09:57:17.400518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.1479872 -0.1854651 -0.23623446 -0.27120379 -0.28297845 -0.24702115 -0.15959635 -0.0492206 0.0786162 0.18657053 0.2793096 0.31689435 0.3032001 0.27001736 0.20578152][-0.24813052 -0.34306896 -0.417951 -0.4407019 -0.41169956 -0.35618931 -0.25787312 -0.17125914 -0.04872473 0.086812556 0.19597507 0.25558749 0.26167649 0.25188512 0.21548623][-0.33644405 -0.47459063 -0.56950772 -0.542202 -0.41831329 -0.29162848 -0.21608754 -0.21762118 -0.17985272 -0.095925055 -0.010415718 0.0031521022 -0.0062733442 0.0095457509 0.057795554][-0.34866226 -0.44581091 -0.47789428 -0.36485535 -0.18142985 -0.0052093118 0.11511192 0.12144856 0.13210303 0.096909449 -0.019983701 -0.16783768 -0.30009633 -0.3058486 -0.20864302][-0.14375386 -0.092006408 0.064232022 0.33433744 0.594642 0.75821275 0.88137221 0.93231994 0.98685354 0.9595632 0.70128757 0.31974232 -0.11468397 -0.29187155 -0.29728895][0.054930016 0.21450868 0.47902465 0.84887552 1.1934206 1.395438 1.5848548 1.7188027 1.8437905 1.7892058 1.3977034 0.86695325 0.25669882 -0.078287795 -0.20671257][0.26526669 0.52642649 0.82721907 1.1812305 1.4832726 1.7201586 2.0563138 2.3572986 2.554285 2.4680061 2.0835555 1.4960692 0.76896131 0.25585723 -0.0078483671][0.28973535 0.55566382 0.83614194 1.0935464 1.2458606 1.365809 1.615979 1.9480741 2.2090213 2.2287247 1.9198892 1.3875077 0.78211725 0.31755394 0.069959983][0.085145131 0.23196083 0.44828039 0.63619703 0.733596 0.77234989 0.81260526 0.98768407 1.2088082 1.2476904 1.0287127 0.78920358 0.41948736 0.098377764 -0.047809124][-0.23533615 -0.23774686 -0.1919511 -0.14966662 -0.11989364 -0.094407916 -0.0783813 -0.055244967 -0.0028184727 0.0061923563 -0.032704227 -0.064627737 -0.21077482 -0.31546462 -0.31148076][-0.490485 -0.6202575 -0.71872675 -0.8058753 -0.89935422 -0.97278625 -0.96873939 -0.97573018 -0.945425 -0.90042442 -0.825497 -0.6299603 -0.5190196 -0.44436342 -0.3813208][-0.67762536 -0.90945077 -1.1410594 -1.3241 -1.480588 -1.6137159 -1.6901903 -1.6960621 -1.5280249 -1.3242631 -1.1147823 -0.83384812 -0.60043013 -0.40887719 -0.30055684][-0.663036 -0.9432072 -1.2436521 -1.469949 -1.6562829 -1.7906761 -1.8334091 -1.7937067 -1.5969229 -1.2897599 -0.97943449 -0.674369 -0.42228577 -0.24253212 -0.14632612][-0.53246391 -0.77880794 -1.0657854 -1.3471956 -1.5746064 -1.6582167 -1.6642904 -1.5886841 -1.3830457 -1.0716841 -0.70964044 -0.38394 -0.15121037 -0.038882639 -0.039162733][-0.38725704 -0.57276475 -0.79387116 -1.0175495 -1.1970906 -1.2808483 -1.2770717 -1.1652327 -0.96950066 -0.71061045 -0.44451702 -0.19918135 -0.055388179 0.01358556 0.021593511]]...]
INFO - root - 2017-12-06 09:57:23.959984: step 46510, loss = 0.82, batch loss = 0.61 (12.0 examples/sec; 0.666 sec/batch; 52h:56m:18s remains)
INFO - root - 2017-12-06 09:57:30.523942: step 46520, loss = 0.79, batch loss = 0.57 (12.7 examples/sec; 0.629 sec/batch; 49h:58m:14s remains)
INFO - root - 2017-12-06 09:57:37.028513: step 46530, loss = 0.77, batch loss = 0.56 (12.8 examples/sec; 0.626 sec/batch; 49h:42m:46s remains)
INFO - root - 2017-12-06 09:57:43.469557: step 46540, loss = 0.73, batch loss = 0.52 (15.1 examples/sec; 0.528 sec/batch; 41h:58m:02s remains)
INFO - root - 2017-12-06 09:57:50.036605: step 46550, loss = 0.87, batch loss = 0.65 (12.6 examples/sec; 0.636 sec/batch; 50h:30m:22s remains)
INFO - root - 2017-12-06 09:57:56.557907: step 46560, loss = 0.76, batch loss = 0.54 (12.3 examples/sec; 0.650 sec/batch; 51h:39m:50s remains)
INFO - root - 2017-12-06 09:58:02.960592: step 46570, loss = 0.81, batch loss = 0.60 (12.9 examples/sec; 0.620 sec/batch; 49h:13m:52s remains)
INFO - root - 2017-12-06 09:58:09.483594: step 46580, loss = 0.88, batch loss = 0.67 (12.3 examples/sec; 0.650 sec/batch; 51h:38m:23s remains)
INFO - root - 2017-12-06 09:58:15.990076: step 46590, loss = 0.79, batch loss = 0.57 (12.1 examples/sec; 0.661 sec/batch; 52h:29m:33s remains)
INFO - root - 2017-12-06 09:58:22.556464: step 46600, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.649 sec/batch; 51h:30m:46s remains)
2017-12-06 09:58:23.200743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05684587 -0.060497593 -0.065743946 -0.07199344 -0.07851135 -0.083685555 -0.086262576 -0.085683748 -0.083055981 -0.079273529 -0.075373359 -0.072138056 -0.070263863 -0.070028126 -0.071319848][-0.054484241 -0.056515623 -0.059946112 -0.0644767 -0.069554374 -0.073947974 -0.076347433 -0.076172188 -0.074276149 -0.07131958 -0.0682347 -0.065598167 -0.064044535 -0.06386517 -0.064914733][-0.053203288 -0.053751096 -0.055470731 -0.058210507 -0.061592713 -0.064798959 -0.066672757 -0.066660024 -0.065337658 -0.063165352 -0.0608762 -0.058921076 -0.057688963 -0.057431355 -0.058041047][-0.05213641 -0.051418517 -0.05167371 -0.052726857 -0.054483511 -0.056475975 -0.057875272 -0.058084406 -0.057323925 -0.055865262 -0.054261517 -0.052895922 -0.051931981 -0.051458143 -0.05166588][-0.051077478 -0.049341608 -0.048433814 -0.048014916 -0.048223846 -0.049028669 -0.050096828 -0.050845928 -0.050923571 -0.050275523 -0.049281336 -0.048352811 -0.047517397 -0.046826027 -0.046659075][-0.050107449 -0.047441959 -0.045656066 -0.044173319 -0.043095328 -0.042697046 -0.043386292 -0.044630155 -0.045750044 -0.046222124 -0.045973323 -0.045500726 -0.044763997 -0.043769024 -0.043164957][-0.049286447 -0.045932919 -0.043552268 -0.041472368 -0.03976205 -0.038622692 -0.038594563 -0.039697543 -0.04147229 -0.042966183 -0.043591246 -0.043638691 -0.042993449 -0.0418672 -0.041044552][-0.048972763 -0.045232885 -0.04246496 -0.040169962 -0.038423929 -0.037184369 -0.036609814 -0.0368701 -0.03833054 -0.040219661 -0.041504871 -0.042054791 -0.041847385 -0.04108078 -0.04043081][-0.049232613 -0.04552966 -0.042614926 -0.040289741 -0.038821597 -0.037769493 -0.037196066 -0.037113924 -0.037803169 -0.039201122 -0.040463455 -0.041310422 -0.041529525 -0.041223124 -0.040911779][-0.049932249 -0.046544347 -0.04379851 -0.041590221 -0.040208928 -0.039323322 -0.038899831 -0.038761239 -0.039105181 -0.039980657 -0.040998925 -0.041726116 -0.04210097 -0.042121302 -0.042185415][-0.051030882 -0.048148006 -0.045878015 -0.043905422 -0.042533487 -0.041650303 -0.041129582 -0.040835932 -0.040970378 -0.041650988 -0.042527057 -0.043156624 -0.043524913 -0.043693043 -0.044076372][-0.052132882 -0.049858011 -0.04817871 -0.046639971 -0.045525782 -0.044681195 -0.043954097 -0.043376833 -0.043232661 -0.043717854 -0.044407703 -0.044916905 -0.045312744 -0.045567997 -0.04613103][-0.053107295 -0.051293839 -0.050162345 -0.049124513 -0.048354417 -0.047686107 -0.046954852 -0.046349887 -0.046060674 -0.046219341 -0.0466112 -0.046987522 -0.047373462 -0.047693957 -0.048248686][-0.053862836 -0.052433223 -0.051695369 -0.051012952 -0.050488606 -0.049994539 -0.049438573 -0.048994739 -0.04879494 -0.048846371 -0.04902184 -0.049200933 -0.049460486 -0.049721103 -0.050222356][-0.054780059 -0.053555992 -0.053036526 -0.052558821 -0.052186858 -0.051862285 -0.05153773 -0.051264834 -0.0511629 -0.051208165 -0.051330246 -0.051412322 -0.051538281 -0.051693555 -0.052222792]]...]
INFO - root - 2017-12-06 09:58:29.776365: step 46610, loss = 0.85, batch loss = 0.63 (12.0 examples/sec; 0.666 sec/batch; 52h:51m:31s remains)
INFO - root - 2017-12-06 09:58:36.266401: step 46620, loss = 0.91, batch loss = 0.69 (12.7 examples/sec; 0.632 sec/batch; 50h:11m:23s remains)
INFO - root - 2017-12-06 09:58:42.699019: step 46630, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 51h:17m:05s remains)
INFO - root - 2017-12-06 09:58:49.250443: step 46640, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.657 sec/batch; 52h:11m:26s remains)
INFO - root - 2017-12-06 09:58:55.751967: step 46650, loss = 0.82, batch loss = 0.60 (11.7 examples/sec; 0.684 sec/batch; 54h:17m:27s remains)
INFO - root - 2017-12-06 09:59:02.111097: step 46660, loss = 0.76, batch loss = 0.54 (11.8 examples/sec; 0.676 sec/batch; 53h:39m:16s remains)
INFO - root - 2017-12-06 09:59:08.588124: step 46670, loss = 0.75, batch loss = 0.54 (12.3 examples/sec; 0.651 sec/batch; 51h:39m:21s remains)
INFO - root - 2017-12-06 09:59:15.022865: step 46680, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.631 sec/batch; 50h:03m:36s remains)
INFO - root - 2017-12-06 09:59:21.444014: step 46690, loss = 0.80, batch loss = 0.58 (12.1 examples/sec; 0.659 sec/batch; 52h:19m:53s remains)
INFO - root - 2017-12-06 09:59:27.904395: step 46700, loss = 0.87, batch loss = 0.66 (12.5 examples/sec; 0.643 sec/batch; 51h:00m:43s remains)
2017-12-06 09:59:28.539622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044092894 -0.0067611933 0.039724603 0.052561358 0.035730943 0.0040346161 -0.060437907 -0.12909426 -0.18728265 -0.26079029 -0.23951519 -0.20466214 -0.1816195 -0.16313404 -0.13166055][-0.047053583 -0.012093164 0.035834283 0.069929227 0.09117502 0.0910618 0.035407588 -0.040181514 -0.10752359 -0.17771566 -0.19705459 -0.20066455 -0.19327658 -0.16418868 -0.11189116][-0.050772585 -0.026991256 0.0043144897 0.032260664 0.069835991 0.0908148 0.047798127 -0.013859861 -0.1046327 -0.1753931 -0.2204475 -0.23246352 -0.22500072 -0.20255938 -0.15043457][-0.053728566 -0.042308223 -0.026531659 -0.018586971 0.013095684 0.046531186 0.026430912 -0.046027996 -0.12985897 -0.2011244 -0.22494848 -0.21570611 -0.19132891 -0.15367675 -0.1280944][-0.053776242 -0.0432574 -0.027992748 -0.015239723 -0.00023448467 0.015510768 -0.0029582903 -0.049453594 -0.11526743 -0.16305214 -0.18486738 -0.17384396 -0.14776967 -0.11341681 -0.10485028][-0.055793568 -0.053493645 -0.048366114 -0.030853711 -0.010569163 0.002516441 -0.011578768 -0.058667593 -0.12201618 -0.15845588 -0.17223868 -0.16111821 -0.1399422 -0.11496691 -0.1073043][-0.05375636 -0.052980144 -0.052175365 -0.051556148 -0.020960227 0.040751964 0.095258147 0.072707117 -0.025173642 -0.090792991 -0.12590738 -0.13647281 -0.12757105 -0.10854562 -0.085578524][-0.050137918 -0.047123816 -0.047691453 -0.05071497 -0.023104541 0.0077999085 0.057371065 0.068869442 -0.01280091 -0.075306334 -0.11695318 -0.12752289 -0.11783355 -0.10023655 -0.09477739][-0.052430809 -0.045806631 -0.04037166 -0.043840952 -0.029370885 -0.017762445 -0.010745078 -0.01238618 -0.072311714 -0.11471233 -0.13287646 -0.13348359 -0.12172285 -0.10008948 -0.090371855][-0.054273237 -0.054701019 -0.047327206 -0.047241528 -0.029634193 -0.0048793405 0.030885674 0.03996776 0.0066129565 -0.050764393 -0.08158996 -0.10067672 -0.10385089 -0.093274452 -0.077883914][-0.05489466 -0.053359985 -0.05910667 -0.057946082 -0.044196624 -0.029781811 -0.00030367821 0.014755078 -0.018061295 -0.053029761 -0.077151492 -0.09152481 -0.093239732 -0.08472807 -0.0726486][-0.057007365 -0.054625288 -0.052414548 -0.052165069 -0.044152025 -0.047840226 -0.046773396 -0.032735396 -0.038159784 -0.053565726 -0.078888863 -0.086093813 -0.084717125 -0.077768825 -0.07070557][-0.059488583 -0.05818858 -0.053597324 -0.052469663 -0.045021929 -0.052094951 -0.064329267 -0.053275876 -0.035357527 -0.022039711 -0.03433736 -0.051335115 -0.064253956 -0.066980623 -0.066671535][-0.063055277 -0.060597714 -0.060922269 -0.062560469 -0.055340786 -0.048889294 -0.06648843 -0.074127294 -0.043648183 -0.019213863 -0.032422725 -0.048179209 -0.05722744 -0.060572356 -0.061809033][-0.0675963 -0.063556321 -0.057277821 -0.05513376 -0.050236881 -0.061109368 -0.053440571 -0.059594207 -0.05816932 -0.05428094 -0.049149211 -0.050482236 -0.04709753 -0.055709708 -0.057927128]]...]
INFO - root - 2017-12-06 09:59:35.158254: step 46710, loss = 0.80, batch loss = 0.58 (12.1 examples/sec; 0.659 sec/batch; 52h:17m:46s remains)
INFO - root - 2017-12-06 09:59:41.590043: step 46720, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.659 sec/batch; 52h:17m:28s remains)
INFO - root - 2017-12-06 09:59:48.045223: step 46730, loss = 0.78, batch loss = 0.56 (12.5 examples/sec; 0.638 sec/batch; 50h:41m:01s remains)
INFO - root - 2017-12-06 09:59:54.627275: step 46740, loss = 0.78, batch loss = 0.57 (12.7 examples/sec; 0.628 sec/batch; 49h:53m:02s remains)
INFO - root - 2017-12-06 10:00:01.046697: step 46750, loss = 0.77, batch loss = 0.55 (13.4 examples/sec; 0.595 sec/batch; 47h:13m:46s remains)
INFO - root - 2017-12-06 10:00:07.634941: step 46760, loss = 0.78, batch loss = 0.56 (12.3 examples/sec; 0.652 sec/batch; 51h:46m:41s remains)
INFO - root - 2017-12-06 10:00:14.138594: step 46770, loss = 0.83, batch loss = 0.62 (12.4 examples/sec; 0.644 sec/batch; 51h:07m:00s remains)
INFO - root - 2017-12-06 10:00:20.564871: step 46780, loss = 0.76, batch loss = 0.54 (12.5 examples/sec; 0.639 sec/batch; 50h:40m:55s remains)
INFO - root - 2017-12-06 10:00:27.178444: step 46790, loss = 0.80, batch loss = 0.59 (12.0 examples/sec; 0.666 sec/batch; 52h:51m:32s remains)
INFO - root - 2017-12-06 10:00:33.681986: step 46800, loss = 0.82, batch loss = 0.61 (11.6 examples/sec; 0.688 sec/batch; 54h:35m:56s remains)
2017-12-06 10:00:34.408581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.0323783 -1.3503823 -1.6656933 -1.9652497 -2.1395786 -2.189177 -2.1947196 -2.112344 -1.8836288 -1.565046 -1.3285956 -1.122142 -0.91197956 -0.77055693 -0.65536046][-1.8100488 -2.1351302 -2.5134361 -2.8815715 -3.2299612 -3.3980436 -3.446703 -3.394345 -3.1597688 -2.8635352 -2.5902929 -2.3551586 -2.0678618 -1.7273221 -1.4638193][-2.1736085 -2.4822927 -2.7788792 -3.0665267 -3.4596126 -3.8075576 -4.0411448 -4.1438565 -4.0818725 -3.8432164 -3.5466681 -3.22283 -2.8960068 -2.5192909 -2.1727371][-1.7840183 -1.9333783 -2.0117428 -2.1667371 -2.4518089 -2.660753 -3.0184071 -3.5542674 -3.7720413 -3.6780331 -3.3385448 -3.1186833 -3.0068827 -2.745688 -2.4196224][-1.036787 -0.84955442 -0.74167871 -0.65078461 -0.63841903 -0.7338779 -1.1241037 -1.5675005 -1.9366769 -2.1902113 -2.1840951 -2.1506283 -2.2960558 -2.2799578 -2.0730546][-0.34670892 -0.065892562 0.24529591 0.71617293 1.0248349 1.1294991 0.84181309 0.37897921 0.11154096 -0.028799709 -0.17911533 -0.43653339 -0.80494058 -1.110635 -1.1299326][0.21275827 0.34607443 0.71824861 1.2793007 1.8701794 2.3080487 2.1022837 1.8994159 1.6855055 1.7058011 1.5669572 1.0641867 0.49173215 0.13829938 -0.030504588][0.49639061 0.72445762 1.0503372 1.5080383 2.119549 2.6179311 2.6120272 2.6614971 2.5285482 2.5387225 2.4495106 2.0796456 1.6340361 1.0820369 0.75957918][0.27606037 0.38363576 0.71900618 1.1393584 1.5066694 1.8330512 2.0177376 2.2441947 2.2814572 2.4650543 2.5216794 2.3120747 1.968294 1.4935688 1.1233256][-0.19725806 -0.13228078 0.066284716 0.2848528 0.64021993 0.98245895 1.1156232 1.1838086 1.2600906 1.3771011 1.5381838 1.6853713 1.609388 1.3159698 1.1158818][-0.80785215 -0.79946637 -0.75855565 -0.65897548 -0.45525682 -0.28717083 -0.21174644 -0.17101429 -0.14023812 0.014903583 0.23552972 0.53566372 0.63320911 0.61280906 0.48230645][-1.1027389 -1.3132484 -1.4241904 -1.4154552 -1.2943954 -1.1899917 -1.2119784 -1.3665206 -1.4427819 -1.3508952 -1.0558431 -0.84685576 -0.70178783 -0.61576092 -0.39906496][-0.95505083 -1.2748065 -1.6254508 -1.8543888 -1.9000955 -1.9264098 -1.7981863 -1.8101977 -1.8050561 -1.9050052 -1.9233968 -1.8083163 -1.5319233 -1.3163795 -1.0369351][-0.62525308 -0.91004 -1.3064718 -1.654716 -1.8612632 -1.9655991 -1.9426777 -1.8996866 -1.8097711 -1.7812186 -1.7131244 -1.5798999 -1.4084646 -1.2345192 -1.0235033][-0.43824092 -0.6221118 -0.93179262 -1.2838184 -1.5766495 -1.7005197 -1.6528542 -1.584278 -1.5046685 -1.4521282 -1.3405951 -1.135264 -0.86816967 -0.72132707 -0.58384228]]...]
INFO - root - 2017-12-06 10:00:41.008799: step 46810, loss = 0.74, batch loss = 0.53 (12.1 examples/sec; 0.663 sec/batch; 52h:36m:44s remains)
INFO - root - 2017-12-06 10:00:47.509807: step 46820, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.653 sec/batch; 51h:47m:28s remains)
INFO - root - 2017-12-06 10:00:53.870597: step 46830, loss = 0.79, batch loss = 0.58 (12.0 examples/sec; 0.665 sec/batch; 52h:47m:52s remains)
INFO - root - 2017-12-06 10:01:00.409971: step 46840, loss = 0.77, batch loss = 0.55 (12.3 examples/sec; 0.650 sec/batch; 51h:35m:19s remains)
INFO - root - 2017-12-06 10:01:06.879744: step 46850, loss = 0.73, batch loss = 0.52 (11.9 examples/sec; 0.674 sec/batch; 53h:26m:47s remains)
INFO - root - 2017-12-06 10:01:13.390688: step 46860, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.637 sec/batch; 50h:32m:33s remains)
INFO - root - 2017-12-06 10:01:19.902062: step 46870, loss = 0.84, batch loss = 0.63 (12.2 examples/sec; 0.657 sec/batch; 52h:07m:20s remains)
INFO - root - 2017-12-06 10:01:26.555372: step 46880, loss = 0.76, batch loss = 0.54 (12.2 examples/sec; 0.656 sec/batch; 52h:01m:50s remains)
INFO - root - 2017-12-06 10:01:33.016595: step 46890, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.655 sec/batch; 51h:56m:27s remains)
INFO - root - 2017-12-06 10:01:39.493971: step 46900, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.650 sec/batch; 51h:36m:08s remains)
2017-12-06 10:01:40.102211: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20033038 0.2342259 0.26719221 0.28328627 0.25892633 0.21274281 0.154173 0.047862232 -0.078259014 -0.22091821 -0.36766732 -0.4569006 -0.47695327 -0.42866111 -0.45857838][0.12717503 0.16563672 0.23525596 0.26616976 0.26444367 0.23839739 0.17057407 0.069238827 -0.059533052 -0.20911825 -0.35235891 -0.46237418 -0.51860321 -0.48616108 -0.53669816][-0.0086999908 0.024159148 0.077508107 0.12669201 0.1551279 0.11377227 0.045530707 -0.035050135 -0.14258561 -0.22158444 -0.34071168 -0.43358251 -0.48006395 -0.47598818 -0.5296762][-0.16324219 -0.11910686 -0.083343521 -0.037625913 -0.00027880818 0.0046058446 -0.0099540129 -0.056749281 -0.12652501 -0.15716648 -0.23252444 -0.29552811 -0.31773022 -0.35236368 -0.42621028][-0.1786797 -0.16924779 -0.17227146 -0.16034047 -0.14771385 -0.12568431 -0.098011538 -0.0819685 -0.068857051 -0.0680717 -0.088092521 -0.091539957 -0.083778322 -0.12397411 -0.16482216][-0.10370935 -0.11457618 -0.11295576 -0.12429227 -0.14298692 -0.13598403 -0.11669094 -0.075695515 -0.0020620525 0.039150938 0.10271154 0.16185388 0.18226993 0.16222912 0.13515122][0.01688125 0.024489194 0.0473195 0.053172052 0.057333857 0.059517384 0.0659083 0.090123832 0.1284703 0.17564684 0.23769727 0.30103794 0.33696821 0.33978066 0.30162269][0.042328909 0.094260588 0.15086642 0.19393805 0.23226196 0.26944789 0.27636433 0.28713745 0.30518892 0.31162441 0.32909298 0.37741932 0.39357772 0.39865625 0.3685655][0.047080413 0.090319023 0.1441271 0.18311879 0.22271258 0.25890338 0.26364452 0.29021761 0.31068316 0.30964449 0.3061856 0.30894777 0.31085327 0.32008204 0.30284998][-0.015678048 0.023267858 0.064765751 0.10573204 0.1353296 0.15807599 0.14823443 0.14440536 0.13711689 0.13269334 0.11597982 0.091445521 0.091350794 0.12017468 0.1199591][-0.16075139 -0.097582132 -0.054516155 -0.019417077 0.010760933 0.018701434 -0.0063285455 -0.039470572 -0.088904865 -0.13187873 -0.15745452 -0.17761621 -0.15732789 -0.12138374 -0.10421903][-0.36270168 -0.2822414 -0.23299208 -0.19300762 -0.16411062 -0.16720098 -0.19645862 -0.23644543 -0.29576844 -0.34406012 -0.37948895 -0.36950523 -0.31870139 -0.26771313 -0.25655943][-0.45102224 -0.386736 -0.31757772 -0.28621745 -0.27839851 -0.2969864 -0.33129102 -0.3711544 -0.41275382 -0.45117882 -0.48097879 -0.46748418 -0.42923146 -0.37838474 -0.35524455][-0.43774042 -0.39270273 -0.33799887 -0.30596638 -0.29577696 -0.31397432 -0.34637922 -0.38285026 -0.40841 -0.43276915 -0.44188821 -0.4306007 -0.41243631 -0.36890575 -0.35000876][-0.30034012 -0.30091572 -0.29085517 -0.2776241 -0.26605809 -0.27027678 -0.28512627 -0.29750335 -0.30494976 -0.31662261 -0.3091495 -0.31233037 -0.304856 -0.28717488 -0.27016479]]...]
INFO - root - 2017-12-06 10:01:46.648109: step 46910, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.661 sec/batch; 52h:27m:57s remains)
INFO - root - 2017-12-06 10:01:53.082578: step 46920, loss = 0.76, batch loss = 0.54 (12.5 examples/sec; 0.642 sec/batch; 50h:53m:22s remains)
INFO - root - 2017-12-06 10:01:59.588492: step 46930, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.664 sec/batch; 52h:39m:47s remains)
INFO - root - 2017-12-06 10:02:05.916761: step 46940, loss = 0.79, batch loss = 0.57 (11.9 examples/sec; 0.672 sec/batch; 53h:19m:09s remains)
INFO - root - 2017-12-06 10:02:12.416463: step 46950, loss = 0.83, batch loss = 0.61 (11.9 examples/sec; 0.675 sec/batch; 53h:31m:35s remains)
INFO - root - 2017-12-06 10:02:19.047492: step 46960, loss = 0.72, batch loss = 0.51 (12.6 examples/sec; 0.635 sec/batch; 50h:21m:45s remains)
INFO - root - 2017-12-06 10:02:25.470904: step 46970, loss = 0.87, batch loss = 0.66 (12.0 examples/sec; 0.665 sec/batch; 52h:46m:49s remains)
INFO - root - 2017-12-06 10:02:31.923517: step 46980, loss = 0.77, batch loss = 0.55 (12.3 examples/sec; 0.653 sec/batch; 51h:45m:56s remains)
INFO - root - 2017-12-06 10:02:38.437173: step 46990, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.657 sec/batch; 52h:04m:31s remains)
INFO - root - 2017-12-06 10:02:44.944290: step 47000, loss = 0.85, batch loss = 0.63 (12.6 examples/sec; 0.633 sec/batch; 50h:13m:38s remains)
2017-12-06 10:02:45.613023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01597327 0.059723251 0.077355675 -0.06563779 -0.30937412 -0.63067913 -0.85146 -0.90359074 -0.84460968 -0.738585 -0.58120018 -0.46253234 -0.377259 -0.31444314 -0.27187616][0.081938855 0.19577932 0.26708406 0.13179085 -0.15521166 -0.5253281 -0.87664253 -1.0800811 -1.1621186 -1.184015 -1.1463233 -1.0866619 -1.0158435 -0.91078013 -0.76289386][-0.021097854 0.1575841 0.27702361 0.19947901 -0.056616873 -0.48560226 -0.91519743 -1.1825588 -1.3539031 -1.4902551 -1.578006 -1.6511024 -1.6557771 -1.5418918 -1.390226][-0.16964816 0.0090200976 0.16656312 0.17524743 -0.015283406 -0.30426931 -0.62277859 -0.91679484 -1.122234 -1.2736297 -1.4396292 -1.5406327 -1.7278445 -1.7918949 -1.7086537][-0.013182148 0.15228316 0.26809964 0.30760548 0.18027645 -0.031234622 -0.27595246 -0.50129956 -0.63594395 -0.67885005 -0.72199816 -0.79339963 -0.93266964 -1.1131787 -1.2681483][0.39498761 0.54681265 0.64009368 0.68156123 0.5492093 0.40443292 0.30472025 0.17407295 0.14505804 0.27508941 0.32701552 0.35150483 0.2378307 0.010219157 -0.28702214][0.84932369 0.9965384 1.0656933 1.1001447 0.98796487 0.86033219 0.82764435 0.89247733 1.0231647 1.2484795 1.485166 1.4528701 1.3563874 1.0955544 0.71761185][1.0120728 1.1611689 1.2203476 1.2606051 1.2462494 1.137051 1.0824263 1.1414087 1.3527849 1.6645871 1.9956123 2.1376412 2.0577905 1.8492196 1.4543569][0.65251958 0.84177542 0.95944083 0.99950314 1.0720624 1.0520228 1.0390069 1.0612233 1.1568911 1.3357905 1.6177373 1.800228 1.7151294 1.5498381 1.2808219][0.020236894 0.14825335 0.37970534 0.53197622 0.6660077 0.68358821 0.67505473 0.68769771 0.67441523 0.68681914 0.76822788 0.84280032 0.84177303 0.75357604 0.56044292][-0.31241918 -0.22534603 -0.06310682 0.073707364 0.17182928 0.18236831 0.13946873 0.036731042 -0.012225442 -0.027492248 -0.050736379 0.016515628 0.070910729 0.0765431 0.0025854483][-0.33971879 -0.27622545 -0.24365622 -0.19623424 -0.23600948 -0.29998031 -0.35867417 -0.44876125 -0.55763465 -0.60852474 -0.66050893 -0.70096463 -0.74940532 -0.78707373 -0.88137853][-0.13248476 -0.089660913 -0.20070216 -0.37880713 -0.62580055 -0.82475895 -0.91100115 -0.93272418 -0.97112948 -1.0136136 -1.0488853 -1.0587294 -1.1142278 -1.2499361 -1.2665448][0.13130221 0.077421658 -0.13902155 -0.45509765 -0.79004276 -1.080287 -1.2669704 -1.2991008 -1.2844387 -1.2473012 -1.211529 -1.1769379 -1.1663872 -1.1739604 -1.1617481][0.16899359 0.037934013 -0.21875754 -0.55541879 -0.832798 -1.0319141 -1.1804773 -1.2364316 -1.2482438 -1.2532564 -1.2319578 -1.1896276 -1.1396204 -1.0893894 -0.994304]]...]
INFO - root - 2017-12-06 10:02:52.085007: step 47010, loss = 0.82, batch loss = 0.60 (11.9 examples/sec; 0.672 sec/batch; 53h:17m:33s remains)
INFO - root - 2017-12-06 10:02:58.570625: step 47020, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.657 sec/batch; 52h:06m:22s remains)
INFO - root - 2017-12-06 10:03:04.847619: step 47030, loss = 0.77, batch loss = 0.56 (14.2 examples/sec; 0.565 sec/batch; 44h:48m:58s remains)
INFO - root - 2017-12-06 10:03:11.295971: step 47040, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.640 sec/batch; 50h:44m:50s remains)
INFO - root - 2017-12-06 10:03:17.820521: step 47050, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.658 sec/batch; 52h:09m:14s remains)
INFO - root - 2017-12-06 10:03:24.304855: step 47060, loss = 0.90, batch loss = 0.68 (12.5 examples/sec; 0.640 sec/batch; 50h:45m:26s remains)
INFO - root - 2017-12-06 10:03:30.893716: step 47070, loss = 0.90, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 50h:48m:35s remains)
INFO - root - 2017-12-06 10:03:37.530480: step 47080, loss = 0.77, batch loss = 0.55 (12.3 examples/sec; 0.651 sec/batch; 51h:34m:55s remains)
INFO - root - 2017-12-06 10:03:44.020294: step 47090, loss = 0.81, batch loss = 0.60 (12.3 examples/sec; 0.649 sec/batch; 51h:26m:57s remains)
INFO - root - 2017-12-06 10:03:50.531609: step 47100, loss = 0.79, batch loss = 0.57 (12.3 examples/sec; 0.651 sec/batch; 51h:36m:35s remains)
2017-12-06 10:03:51.184010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.11249772 -0.12084971 -0.13631904 -0.1613234 -0.19700405 -0.23287964 -0.25781998 -0.26735684 -0.25970545 -0.23567352 -0.20434606 -0.16936269 -0.13943334 -0.11932743 -0.11419003][-0.14984865 -0.19917133 -0.2692596 -0.35820857 -0.48744473 -0.636457 -0.7649346 -0.83327115 -0.82332385 -0.73726171 -0.60041171 -0.43490681 -0.29327086 -0.19185218 -0.13905762][-0.23203915 -0.33060548 -0.46954268 -0.63552552 -0.86299378 -1.1266843 -1.4099461 -1.6429614 -1.7404972 -1.6676881 -1.4186164 -1.054911 -0.70159608 -0.42204094 -0.24580541][-0.32575446 -0.4758698 -0.64990813 -0.84218311 -1.0852629 -1.3598466 -1.7402858 -2.137131 -2.4595823 -2.5583262 -2.3581 -1.9026251 -1.3257078 -0.81985742 -0.46234238][-0.36276957 -0.51467234 -0.67017859 -0.79013318 -0.89111185 -1.0172697 -1.2866986 -1.7415993 -2.2653353 -2.6169932 -2.6749551 -2.3585122 -1.7805552 -1.1763451 -0.6967023][-0.3147054 -0.36871824 -0.37365624 -0.29228735 -0.11846963 0.10756373 0.12765914 -0.19578174 -0.80508971 -1.4408964 -1.856261 -1.9285033 -1.6443093 -1.1860182 -0.75432569][-0.21033353 -0.16208698 0.033019178 0.38507882 0.97019482 1.6768545 2.0786161 2.0362859 1.4423243 0.60129648 -0.1745702 -0.70720094 -0.82211596 -0.68920988 -0.50426984][-0.114048 0.0081108212 0.27949345 0.76030332 1.5665395 2.5635679 3.3567564 3.6425111 3.2649126 2.4042487 1.4439441 0.62897134 0.16686288 -0.043992206 -0.084444113][-0.14359578 -0.016656756 0.28379869 0.8262856 1.6711686 2.71851 3.6689076 4.1681356 3.9980288 3.2523959 2.2463975 1.2833197 0.67969215 0.30636975 0.16872454][-0.24562356 -0.17840835 0.059593253 0.50352889 1.2360584 2.1798306 3.0070345 3.4682729 3.3793442 2.7564301 1.903154 1.0988742 0.52055204 0.13120627 0.024632491][-0.30026388 -0.34800217 -0.29547346 -0.080743089 0.37593633 1.0206565 1.6696723 2.088726 2.1708124 1.7987056 1.1387414 0.49802563 0.070120566 -0.16199698 -0.20025629][-0.29972154 -0.42075387 -0.54782408 -0.5921399 -0.49923477 -0.23881909 0.18348736 0.54803306 0.71857673 0.58331746 0.25531366 -0.037741154 -0.28381398 -0.36182287 -0.3500635][-0.256833 -0.39263672 -0.58845824 -0.78894174 -0.9729054 -1.0766724 -1.0291988 -0.8205514 -0.5461033 -0.39236483 -0.36032417 -0.38505617 -0.44414362 -0.37706587 -0.31082776][-0.16772634 -0.26331314 -0.41624367 -0.59966242 -0.81024063 -1.0373311 -1.1964221 -1.2311085 -1.1397839 -0.95713049 -0.76017767 -0.58598757 -0.46988383 -0.329589 -0.23264116][-0.11641081 -0.13713774 -0.1885331 -0.27941552 -0.37581298 -0.54109931 -0.66403425 -0.72509217 -0.74352539 -0.68572122 -0.55554974 -0.41593727 -0.329584 -0.21976954 -0.17690162]]...]
INFO - root - 2017-12-06 10:03:57.544000: step 47110, loss = 0.78, batch loss = 0.57 (11.9 examples/sec; 0.673 sec/batch; 53h:19m:16s remains)
INFO - root - 2017-12-06 10:04:04.047258: step 47120, loss = 0.88, batch loss = 0.66 (12.5 examples/sec; 0.639 sec/batch; 50h:38m:36s remains)
INFO - root - 2017-12-06 10:04:10.474936: step 47130, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.660 sec/batch; 52h:18m:21s remains)
INFO - root - 2017-12-06 10:04:16.930104: step 47140, loss = 0.83, batch loss = 0.62 (12.2 examples/sec; 0.654 sec/batch; 51h:52m:03s remains)
INFO - root - 2017-12-06 10:04:23.340818: step 47150, loss = 0.80, batch loss = 0.58 (12.9 examples/sec; 0.622 sec/batch; 49h:18m:28s remains)
INFO - root - 2017-12-06 10:04:29.790581: step 47160, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.643 sec/batch; 50h:58m:38s remains)
INFO - root - 2017-12-06 10:04:36.363099: step 47170, loss = 0.77, batch loss = 0.55 (11.7 examples/sec; 0.682 sec/batch; 54h:05m:15s remains)
INFO - root - 2017-12-06 10:04:42.814151: step 47180, loss = 0.86, batch loss = 0.64 (12.6 examples/sec; 0.635 sec/batch; 50h:21m:06s remains)
INFO - root - 2017-12-06 10:04:49.299013: step 47190, loss = 0.84, batch loss = 0.63 (11.8 examples/sec; 0.677 sec/batch; 53h:39m:09s remains)
INFO - root - 2017-12-06 10:04:55.789318: step 47200, loss = 0.88, batch loss = 0.66 (12.1 examples/sec; 0.659 sec/batch; 52h:12m:38s remains)
2017-12-06 10:04:56.400012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.075040035 -0.074389607 -0.073206834 -0.071732074 -0.0701518 -0.06868811 -0.067754507 -0.06750498 -0.067747273 -0.068186119 -0.0686623 -0.069114693 -0.069497496 -0.069793291 -0.06993252][-0.077635556 -0.076711424 -0.074996792 -0.072710648 -0.070242062 -0.067970455 -0.06663987 -0.0662591 -0.066659063 -0.067273363 -0.067971215 -0.068701044 -0.069342226 -0.06982109 -0.070051037][-0.080943741 -0.079709381 -0.077416725 -0.074198082 -0.070714995 -0.067600369 -0.065893918 -0.06527786 -0.065637946 -0.066447958 -0.067470752 -0.068514906 -0.069379248 -0.069981314 -0.070274591][-0.084965087 -0.08371938 -0.080957793 -0.076809913 -0.072117485 -0.068125285 -0.065810278 -0.064889647 -0.065117031 -0.06613405 -0.067430407 -0.06869296 -0.069677368 -0.070291668 -0.070584044][-0.0882406 -0.087235421 -0.084409513 -0.079824828 -0.074366264 -0.06958124 -0.0668052 -0.065751575 -0.065850377 -0.066748247 -0.067880318 -0.069091022 -0.070096806 -0.070721634 -0.070883736][-0.09051241 -0.089766107 -0.087120958 -0.082631484 -0.077015169 -0.071681648 -0.0685007 -0.067242175 -0.067238934 -0.067851454 -0.068735391 -0.069809727 -0.0706536 -0.071063161 -0.071010813][-0.092286214 -0.091588184 -0.089114308 -0.084797285 -0.079280645 -0.073696777 -0.070242077 -0.069030158 -0.069166675 -0.069738276 -0.070367947 -0.070925549 -0.0713284 -0.071389705 -0.071149632][-0.092646211 -0.092179127 -0.089905456 -0.08612413 -0.081235789 -0.076219238 -0.072991565 -0.071851909 -0.0719606 -0.072244026 -0.072432429 -0.072370924 -0.072208054 -0.071811028 -0.071313307][-0.091268517 -0.090917319 -0.088952087 -0.08573211 -0.08158946 -0.077815846 -0.075611085 -0.074616648 -0.07438805 -0.07423348 -0.073978037 -0.073515333 -0.072827019 -0.071981147 -0.071261831][-0.087795883 -0.087503448 -0.085763328 -0.082802936 -0.079433441 -0.076786153 -0.0755444 -0.075021125 -0.075053066 -0.075061381 -0.074726067 -0.073938355 -0.0728641 -0.07180512 -0.071063161][-0.083704583 -0.083188958 -0.081492633 -0.078876875 -0.076307528 -0.074415781 -0.073838547 -0.073929295 -0.0744109 -0.074536018 -0.074108206 -0.073301494 -0.072310686 -0.071350381 -0.07076259][-0.079620428 -0.078667134 -0.077103093 -0.074956335 -0.07315173 -0.072101347 -0.072209261 -0.072665133 -0.073128834 -0.073239833 -0.072931185 -0.072274446 -0.07150241 -0.07083302 -0.070481747][-0.076078586 -0.075008675 -0.073699765 -0.072170928 -0.071023762 -0.070358858 -0.070677608 -0.071207523 -0.071737096 -0.071951225 -0.0718057 -0.071352623 -0.070822775 -0.070421048 -0.070252322][-0.073349476 -0.072332017 -0.071387716 -0.070395842 -0.0697178 -0.069408581 -0.069730587 -0.070274524 -0.0707722 -0.071008891 -0.070929877 -0.070655294 -0.070344642 -0.070133172 -0.070078693][-0.071678616 -0.07096611 -0.070423864 -0.069816962 -0.069368884 -0.069221765 -0.069436818 -0.069805138 -0.070119157 -0.0702645 -0.070253395 -0.070116095 -0.069957696 -0.069873564 -0.0699099]]...]
INFO - root - 2017-12-06 10:05:02.838454: step 47210, loss = 0.83, batch loss = 0.62 (11.9 examples/sec; 0.670 sec/batch; 53h:05m:43s remains)
INFO - root - 2017-12-06 10:05:09.260886: step 47220, loss = 0.80, batch loss = 0.58 (12.1 examples/sec; 0.661 sec/batch; 52h:25m:03s remains)
INFO - root - 2017-12-06 10:05:15.726605: step 47230, loss = 0.81, batch loss = 0.60 (11.8 examples/sec; 0.679 sec/batch; 53h:49m:35s remains)
INFO - root - 2017-12-06 10:05:22.095189: step 47240, loss = 0.79, batch loss = 0.58 (12.2 examples/sec; 0.654 sec/batch; 51h:50m:52s remains)
INFO - root - 2017-12-06 10:05:28.525314: step 47250, loss = 0.82, batch loss = 0.60 (11.5 examples/sec; 0.695 sec/batch; 55h:01m:53s remains)
INFO - root - 2017-12-06 10:05:34.959010: step 47260, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.629 sec/batch; 49h:49m:08s remains)
INFO - root - 2017-12-06 10:05:41.380429: step 47270, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.655 sec/batch; 51h:55m:58s remains)
INFO - root - 2017-12-06 10:05:47.920928: step 47280, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.655 sec/batch; 51h:55m:31s remains)
INFO - root - 2017-12-06 10:05:54.444517: step 47290, loss = 0.92, batch loss = 0.70 (12.0 examples/sec; 0.665 sec/batch; 52h:38m:54s remains)
INFO - root - 2017-12-06 10:06:00.958229: step 47300, loss = 0.94, batch loss = 0.73 (12.5 examples/sec; 0.638 sec/batch; 50h:33m:53s remains)
2017-12-06 10:06:01.586641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091337666 -0.0942912 -0.10142045 -0.11600251 -0.13923234 -0.17124748 -0.20943718 -0.24836236 -0.2771233 -0.28288871 -0.25801522 -0.21004373 -0.15662535 -0.12007805 -0.1011818][-0.093769945 -0.10342285 -0.12428056 -0.15890908 -0.20884861 -0.27355278 -0.3465547 -0.42344558 -0.48780051 -0.52377719 -0.50989449 -0.43439585 -0.31113708 -0.20130977 -0.13032177][-0.10098848 -0.12456216 -0.16556168 -0.22168472 -0.29348898 -0.37808266 -0.46190441 -0.54882103 -0.62562919 -0.68884438 -0.71839261 -0.66692448 -0.51313615 -0.32840815 -0.18211734][-0.11428024 -0.15448153 -0.2069549 -0.25660723 -0.30109057 -0.34848657 -0.39381313 -0.45503885 -0.51791328 -0.59152907 -0.67870075 -0.70877469 -0.61138326 -0.41786337 -0.23025838][-0.13172407 -0.18300357 -0.22539851 -0.22700417 -0.17591363 -0.0931474 -0.017462224 0.007156983 -0.028240252 -0.13750294 -0.32473356 -0.48940888 -0.53437 -0.4205097 -0.25487843][-0.14981389 -0.20435983 -0.21788132 -0.14864695 0.013374023 0.24532172 0.48558614 0.64866704 0.68386149 0.56716257 0.29878882 -0.0014186725 -0.23436826 -0.27602774 -0.21442986][-0.16046888 -0.2039938 -0.18169081 -0.047352504 0.21721879 0.55513507 0.88109326 1.1040105 1.157405 1.0027312 0.697477 0.34417918 0.035209753 -0.10565208 -0.14397144][-0.1554113 -0.19484767 -0.17773807 -0.051030755 0.21492389 0.57978535 0.95248532 1.2224385 1.2850049 1.1035181 0.74757946 0.37543142 0.085569657 -0.051782046 -0.092540763][-0.15416124 -0.19274065 -0.1974842 -0.11379358 0.10500533 0.45038107 0.84173751 1.1614448 1.2929746 1.1629304 0.80858594 0.41599068 0.12072166 -0.029818363 -0.079028383][-0.14712025 -0.18564564 -0.20362151 -0.16292849 -0.02341906 0.23058727 0.54813975 0.81980705 0.94299042 0.84155512 0.560042 0.24490049 0.030371673 -0.064143255 -0.068633281][-0.13236192 -0.17810468 -0.22812039 -0.25160402 -0.22065935 -0.10218319 0.08744093 0.278149 0.38551924 0.32872388 0.16678572 -0.017192677 -0.13139623 -0.15926388 -0.11753652][-0.11439302 -0.15300088 -0.21458277 -0.28372648 -0.34513772 -0.35742885 -0.30827904 -0.21530692 -0.11562392 -0.077770129 -0.10325363 -0.14006376 -0.16211058 -0.15125157 -0.11695749][-0.096538588 -0.11364204 -0.14564189 -0.1958504 -0.25811705 -0.307467 -0.32652056 -0.30849206 -0.25821748 -0.1993546 -0.16971727 -0.13827896 -0.13365692 -0.12527171 -0.10921205][-0.090630591 -0.091668449 -0.095673278 -0.10855097 -0.13133334 -0.16361758 -0.19037127 -0.1997903 -0.17635337 -0.14354543 -0.11311588 -0.089801274 -0.09776935 -0.10132932 -0.10547265][-0.090507224 -0.090387776 -0.087267376 -0.086011566 -0.086460918 -0.09346766 -0.10384566 -0.11257576 -0.10901421 -0.10619766 -0.095524818 -0.080631778 -0.083628364 -0.077290833 -0.090422437]]...]
INFO - root - 2017-12-06 10:06:07.885776: step 47310, loss = 0.77, batch loss = 0.55 (14.2 examples/sec; 0.563 sec/batch; 44h:34m:03s remains)
INFO - root - 2017-12-06 10:06:14.368713: step 47320, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.643 sec/batch; 50h:56m:53s remains)
INFO - root - 2017-12-06 10:06:20.858045: step 47330, loss = 0.91, batch loss = 0.70 (12.6 examples/sec; 0.635 sec/batch; 50h:18m:49s remains)
INFO - root - 2017-12-06 10:06:27.344193: step 47340, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.638 sec/batch; 50h:31m:57s remains)
INFO - root - 2017-12-06 10:06:33.929248: step 47350, loss = 0.83, batch loss = 0.61 (12.8 examples/sec; 0.623 sec/batch; 49h:19m:37s remains)
INFO - root - 2017-12-06 10:06:40.375707: step 47360, loss = 0.87, batch loss = 0.65 (12.3 examples/sec; 0.652 sec/batch; 51h:39m:21s remains)
INFO - root - 2017-12-06 10:06:46.827698: step 47370, loss = 0.85, batch loss = 0.64 (12.4 examples/sec; 0.646 sec/batch; 51h:09m:25s remains)
INFO - root - 2017-12-06 10:06:53.231776: step 47380, loss = 0.82, batch loss = 0.61 (12.7 examples/sec; 0.631 sec/batch; 49h:57m:55s remains)
INFO - root - 2017-12-06 10:06:59.597078: step 47390, loss = 0.76, batch loss = 0.54 (12.1 examples/sec; 0.663 sec/batch; 52h:29m:41s remains)
INFO - root - 2017-12-06 10:07:06.059380: step 47400, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.647 sec/batch; 51h:12m:38s remains)
2017-12-06 10:07:06.668816: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026890717 0.020879619 0.024769403 -0.0013022795 0.010124967 0.0044348389 -0.010579683 -0.034318887 -0.036552995 -0.023694389 -0.0030151308 0.01228822 0.021713562 0.027953736 0.038440011][-0.13110727 -0.12204863 -0.11978482 -0.12457687 -0.089881293 -0.10227394 -0.12727824 -0.18391955 -0.23142979 -0.23840055 -0.25363424 -0.26157168 -0.25353116 -0.21920151 -0.1740922][-0.017213598 -0.055186428 -0.099836655 -0.11733112 -0.1145172 -0.12656343 -0.11077972 -0.1272461 -0.15468548 -0.16840905 -0.20211357 -0.22086766 -0.216205 -0.17036685 -0.1389924][0.042522378 0.0040164515 0.002778925 -0.037821 -0.078642339 -0.080575861 -0.044497 -0.035089895 -0.021434493 -0.01174695 -0.026272126 -0.037086982 -0.053504758 -0.013798699 0.0033160374][-0.061238285 -0.09012004 -0.056138508 -0.028343387 -0.034308985 -0.030009061 -0.02530627 -0.020832367 -0.027159341 -0.012856297 -0.021214724 -0.0087685138 -0.021133438 0.0087033212 0.0070453063][-0.045786552 -0.046418533 0.016426325 0.052249096 0.071830444 0.092636012 0.11016595 0.11959974 0.1082503 0.0968669 0.076034315 0.067602642 0.044219531 0.050734989 0.020831719][0.034565277 0.020631962 0.061848588 0.09605398 0.11419452 0.12192815 0.119008 0.11464968 0.099170811 0.059580766 0.0057080239 -0.049722362 -0.089071237 -0.11139899 -0.14461617][-0.047940549 -0.059137303 -0.05241438 -0.032906666 -0.030623328 -0.04761124 -0.07039468 -0.0924562 -0.10916643 -0.1309371 -0.15016823 -0.18162127 -0.19612244 -0.2051473 -0.22997057][-0.16128936 -0.15634668 -0.15350612 -0.17388448 -0.20242357 -0.22355786 -0.22751623 -0.20610884 -0.17682044 -0.16612265 -0.16203529 -0.16166018 -0.16688421 -0.18172362 -0.20539656][-0.26680315 -0.26368681 -0.24666017 -0.2438516 -0.24268019 -0.23752713 -0.22300038 -0.19298752 -0.15047956 -0.10582845 -0.084888063 -0.095015734 -0.11025681 -0.11948407 -0.12166163][-0.27964395 -0.28830582 -0.27416313 -0.26845416 -0.26140425 -0.24570343 -0.21866658 -0.183434 -0.14668359 -0.10524414 -0.074849449 -0.064920232 -0.064669147 -0.071347684 -0.0769879][-0.16967478 -0.18414186 -0.18615893 -0.19398765 -0.19991407 -0.19851312 -0.19042867 -0.17720583 -0.16315839 -0.14473616 -0.12203054 -0.1010737 -0.088340424 -0.090608515 -0.098288439][-0.0981143 -0.10947919 -0.12435422 -0.13136439 -0.14029859 -0.14008762 -0.1389194 -0.13620107 -0.13213253 -0.12566179 -0.11376348 -0.10031733 -0.0924981 -0.094121277 -0.10070333][-0.08597184 -0.088326767 -0.098714046 -0.10407196 -0.11627738 -0.11232276 -0.10631809 -0.099326432 -0.091784216 -0.08589454 -0.083953425 -0.085082471 -0.085990667 -0.084250212 -0.084290221][-0.10337774 -0.10562397 -0.10512214 -0.10391366 -0.10305359 -0.10240287 -0.10024698 -0.095991157 -0.088994868 -0.081882082 -0.077880159 -0.077293664 -0.078175642 -0.079933047 -0.083867]]...]
INFO - root - 2017-12-06 10:07:13.158485: step 47410, loss = 0.76, batch loss = 0.55 (11.9 examples/sec; 0.670 sec/batch; 53h:01m:37s remains)
INFO - root - 2017-12-06 10:07:19.594676: step 47420, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.648 sec/batch; 51h:20m:00s remains)
INFO - root - 2017-12-06 10:07:26.094660: step 47430, loss = 0.82, batch loss = 0.61 (11.7 examples/sec; 0.686 sec/batch; 54h:21m:11s remains)
INFO - root - 2017-12-06 10:07:32.601650: step 47440, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.642 sec/batch; 50h:48m:04s remains)
INFO - root - 2017-12-06 10:07:39.073406: step 47450, loss = 0.76, batch loss = 0.55 (12.4 examples/sec; 0.644 sec/batch; 50h:57m:10s remains)
INFO - root - 2017-12-06 10:07:45.617753: step 47460, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.654 sec/batch; 51h:49m:09s remains)
INFO - root - 2017-12-06 10:07:52.065066: step 47470, loss = 0.76, batch loss = 0.55 (12.2 examples/sec; 0.657 sec/batch; 51h:59m:18s remains)
INFO - root - 2017-12-06 10:07:58.647568: step 47480, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 51h:13m:20s remains)
INFO - root - 2017-12-06 10:08:05.250383: step 47490, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.659 sec/batch; 52h:08m:31s remains)
INFO - root - 2017-12-06 10:08:11.630845: step 47500, loss = 0.79, batch loss = 0.57 (12.1 examples/sec; 0.663 sec/batch; 52h:29m:16s remains)
2017-12-06 10:08:12.314335: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.6035944 -2.377878 -2.8597543 -3.1040869 -3.0357382 -2.697119 -2.2386243 -1.9204828 -1.7339877 -1.5431808 -1.3576522 -1.0986619 -0.79685575 -0.58994275 -0.20444405][-2.7965221 -3.5653985 -4.1824207 -4.4179025 -4.4274836 -4.3030515 -3.8296258 -3.2936838 -2.721384 -2.2315845 -1.8235925 -1.4886105 -0.95676541 -0.60992956 -0.1986066][-2.7993124 -3.6180146 -4.4023876 -4.9038868 -5.1933823 -5.2642016 -4.8113952 -4.336165 -3.7476215 -3.0681014 -2.2590525 -1.731717 -1.111066 -0.68737656 -0.39532053][-1.8901038 -2.2999949 -2.9111543 -3.6404574 -4.2026467 -4.768219 -4.8177204 -4.472621 -3.9322157 -3.331475 -2.561507 -1.84073 -1.1365924 -0.64955294 -0.28503746][-1.0558333 -1.0414561 -1.1206632 -1.3554406 -1.6848428 -2.4590149 -3.0875585 -3.4643807 -3.2861338 -2.7568197 -2.1139767 -1.4857243 -0.96056348 -0.71034878 -0.52915436][-0.026300341 0.33675858 0.78000164 0.93921816 0.99905384 0.96621335 0.52258205 -0.51095849 -0.98905218 -1.3157191 -1.2287315 -0.74692571 -0.46407291 -0.30885854 -0.29139546][1.1309409 1.251729 2.1426239 2.8731353 3.3187375 3.8290312 3.5288117 2.6751416 1.7557392 0.70856953 0.12666157 0.0047703683 0.00068034232 0.055457748 -0.039492205][2.2457461 2.1714513 3.0332484 3.9663658 4.8390589 5.5386014 5.3757358 4.5833783 3.627923 2.5045741 1.5053357 0.69753689 0.33896419 0.083566822 0.064905681][2.5099614 2.467746 2.9708159 3.5962183 4.2421937 5.0499325 5.655303 5.21733 4.167222 2.97712 1.88977 0.9917959 0.36255366 -0.089623131 -0.17180672][2.1308167 1.8964934 2.1124377 2.381434 2.6504984 3.1511247 3.8083904 3.8125141 3.2483041 2.2600629 1.1915054 0.3897194 -0.072001547 -0.2875399 -0.18066519][0.74678707 0.7381894 0.82163858 0.96084297 0.83401716 0.87875932 1.1758837 1.3449985 1.0447068 0.594378 0.089093663 -0.33606359 -0.48465985 -0.53801781 -0.36382735][-1.0084952 -1.1208429 -1.1733353 -1.2979032 -1.2103631 -1.2530353 -0.91815662 -0.65249354 -0.47799909 -0.57370114 -0.66785413 -0.79806197 -0.65892357 -0.50997007 -0.36705497][-2.3358324 -2.440038 -2.572866 -2.6847973 -2.7003052 -2.6150284 -2.4103954 -2.0890403 -1.6830173 -1.1926643 -1.0146344 -0.97719222 -0.77572823 -0.54571778 -0.34429514][-2.6536884 -2.772115 -2.8757968 -2.9566648 -2.914351 -2.9620683 -2.7165096 -2.4110682 -1.9771599 -1.4487339 -1.0363597 -0.84117025 -0.59345162 -0.39241764 -0.28930342][-2.3995814 -2.5862312 -2.6465354 -2.6167552 -2.5954535 -2.4745877 -2.2566831 -1.9323103 -1.5480678 -1.1448152 -0.84977418 -0.6169951 -0.40247723 -0.21546486 -0.1450693]]...]
INFO - root - 2017-12-06 10:08:18.801040: step 47510, loss = 0.87, batch loss = 0.66 (12.4 examples/sec; 0.646 sec/batch; 51h:09m:19s remains)
INFO - root - 2017-12-06 10:08:25.372235: step 47520, loss = 0.85, batch loss = 0.63 (12.0 examples/sec; 0.668 sec/batch; 52h:53m:44s remains)
INFO - root - 2017-12-06 10:08:31.735185: step 47530, loss = 0.77, batch loss = 0.56 (12.0 examples/sec; 0.664 sec/batch; 52h:34m:24s remains)
INFO - root - 2017-12-06 10:08:38.237057: step 47540, loss = 0.81, batch loss = 0.59 (12.3 examples/sec; 0.652 sec/batch; 51h:34m:33s remains)
INFO - root - 2017-12-06 10:08:44.610740: step 47550, loss = 0.76, batch loss = 0.54 (12.6 examples/sec; 0.637 sec/batch; 50h:25m:23s remains)
INFO - root - 2017-12-06 10:08:51.124314: step 47560, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.644 sec/batch; 50h:56m:10s remains)
INFO - root - 2017-12-06 10:08:57.655539: step 47570, loss = 0.74, batch loss = 0.52 (11.8 examples/sec; 0.676 sec/batch; 53h:28m:51s remains)
INFO - root - 2017-12-06 10:09:04.061913: step 47580, loss = 0.98, batch loss = 0.76 (12.2 examples/sec; 0.655 sec/batch; 51h:50m:10s remains)
INFO - root - 2017-12-06 10:09:10.639211: step 47590, loss = 0.81, batch loss = 0.59 (11.8 examples/sec; 0.677 sec/batch; 53h:34m:53s remains)
INFO - root - 2017-12-06 10:09:16.932868: step 47600, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.638 sec/batch; 50h:30m:59s remains)
2017-12-06 10:09:17.599596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.25721738 -0.45110026 -0.61862177 -0.82405216 -0.98539925 -1.0850688 -1.1633617 -1.1575577 -1.1343955 -1.0366659 -0.917533 -0.81327212 -0.75412655 -0.74001527 -0.77831656][-0.33383942 -0.42288095 -0.51897848 -0.73237258 -0.99524182 -1.2536469 -1.4423339 -1.5361079 -1.5844678 -1.5531183 -1.4831083 -1.3522321 -1.2227074 -1.1163342 -1.0288194][-0.15980339 -0.19667113 -0.20820209 -0.32259959 -0.53207833 -0.876271 -1.2485112 -1.5289932 -1.6702225 -1.6656318 -1.6164787 -1.4797498 -1.3815025 -1.2546227 -1.1014681][0.11315007 0.13398278 0.17529696 0.071570717 -0.097783133 -0.30717033 -0.55995113 -0.86916226 -1.1875021 -1.3770345 -1.45095 -1.3175632 -1.1882907 -1.002897 -0.842009][0.0620263 0.16575885 0.32298541 0.3572087 0.2597422 0.096454732 -0.11225233 -0.31794247 -0.57760006 -0.77625483 -0.91257596 -0.86106366 -0.79011625 -0.62327915 -0.45060575][0.14611912 0.21566021 0.37398729 0.53300881 0.59469116 0.59253055 0.53479254 0.38417962 0.17325965 0.040700607 -0.050666653 0.00046657771 0.087712072 0.19033861 0.22488192][0.15830445 0.20817393 0.36163366 0.52290493 0.63551116 0.75875688 0.86301422 0.88386452 0.79841453 0.62906361 0.46630433 0.4186087 0.45118514 0.57745689 0.69681478][0.067991771 0.15996966 0.3396894 0.53864449 0.687949 0.81108665 0.90342551 1.0170822 1.0160877 0.8942458 0.6663239 0.43749616 0.29593846 0.34399703 0.4898769][-0.000603199 0.10677565 0.28764486 0.52216446 0.74128556 0.88649011 0.9694339 1.0158186 1.0367389 1.0837995 0.99416447 0.78747725 0.50171387 0.34137636 0.3415052][-0.080535546 -0.024857104 0.080051966 0.23037127 0.38241675 0.51655591 0.6072017 0.66692734 0.68062496 0.64636612 0.606348 0.65041351 0.61453891 0.51257831 0.36190116][-0.085294969 -0.10537656 -0.12455028 -0.10750857 -0.038535252 0.018748038 0.042844705 0.055359803 0.052541681 0.068721272 0.085165076 0.10003052 0.14490715 0.23075479 0.2666778][-0.12221917 -0.15584442 -0.19620806 -0.22260773 -0.23164016 -0.19991523 -0.14910761 -0.12434154 -0.13565266 -0.16184741 -0.20069131 -0.23203921 -0.21536094 -0.18149209 -0.095036849][-0.097381227 -0.11683934 -0.14789514 -0.18314771 -0.20391768 -0.2045331 -0.17997648 -0.14532137 -0.10934264 -0.095808253 -0.10567059 -0.13745873 -0.18416515 -0.22721097 -0.23359069][-0.085987225 -0.09354569 -0.10098133 -0.1028338 -0.09450449 -0.07918109 -0.063087612 -0.0596184 -0.066745169 -0.080894448 -0.093838118 -0.10904042 -0.12253267 -0.12328281 -0.12923086][-0.092511825 -0.10620839 -0.11720125 -0.11896998 -0.11329743 -0.096299112 -0.077630989 -0.064187855 -0.055568621 -0.0619405 -0.086670816 -0.11514343 -0.1282016 -0.12456548 -0.11543331]]...]
INFO - root - 2017-12-06 10:09:24.103408: step 47610, loss = 0.74, batch loss = 0.53 (12.2 examples/sec; 0.657 sec/batch; 51h:58m:02s remains)
INFO - root - 2017-12-06 10:09:30.570671: step 47620, loss = 0.81, batch loss = 0.60 (12.7 examples/sec; 0.631 sec/batch; 49h:54m:45s remains)
INFO - root - 2017-12-06 10:09:37.053937: step 47630, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.656 sec/batch; 51h:54m:54s remains)
INFO - root - 2017-12-06 10:09:43.583273: step 47640, loss = 0.80, batch loss = 0.59 (12.9 examples/sec; 0.621 sec/batch; 49h:06m:18s remains)
INFO - root - 2017-12-06 10:09:50.111982: step 47650, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.656 sec/batch; 51h:55m:55s remains)
INFO - root - 2017-12-06 10:09:56.765916: step 47660, loss = 0.88, batch loss = 0.66 (12.1 examples/sec; 0.659 sec/batch; 52h:09m:41s remains)
INFO - root - 2017-12-06 10:10:03.075008: step 47670, loss = 0.79, batch loss = 0.57 (12.3 examples/sec; 0.651 sec/batch; 51h:31m:03s remains)
INFO - root - 2017-12-06 10:10:09.610843: step 47680, loss = 0.82, batch loss = 0.61 (12.4 examples/sec; 0.643 sec/batch; 50h:50m:55s remains)
INFO - root - 2017-12-06 10:10:16.005776: step 47690, loss = 0.76, batch loss = 0.55 (12.7 examples/sec; 0.628 sec/batch; 49h:39m:01s remains)
INFO - root - 2017-12-06 10:10:22.534767: step 47700, loss = 0.84, batch loss = 0.63 (12.0 examples/sec; 0.668 sec/batch; 52h:52m:05s remains)
2017-12-06 10:10:23.323105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.020970762 -0.021733202 -0.023233876 -0.024844579 -0.026446104 -0.028054126 -0.029519692 -0.030617762 -0.031314325 -0.031755533 -0.032106157 -0.032723963 -0.034039225 -0.036015507 -0.03871103][-0.015547082 -0.016038783 -0.017620943 -0.019641519 -0.021756738 -0.023684122 -0.025414616 -0.026923075 -0.028211199 -0.029345065 -0.030532088 -0.031977978 -0.033849575 -0.036077842 -0.038663033][-0.011437237 -0.011173822 -0.012337729 -0.014221147 -0.016455606 -0.018746532 -0.020955086 -0.023062073 -0.025265314 -0.027451612 -0.029732075 -0.032113429 -0.0345695 -0.036964189 -0.0393677][-0.0088117793 -0.0077444091 -0.0080914721 -0.0093102381 -0.011277311 -0.013848729 -0.016712822 -0.019646615 -0.022850871 -0.026167646 -0.029594488 -0.032816563 -0.035726566 -0.038278066 -0.040572364][-0.0083451793 -0.0066442788 -0.0060967952 -0.0065129027 -0.0079641938 -0.010490172 -0.01394181 -0.017764486 -0.021890707 -0.026105173 -0.030401666 -0.034135021 -0.037190713 -0.039698564 -0.041879397][-0.010850668 -0.0086961165 -0.007421121 -0.0070994571 -0.0080957934 -0.010366343 -0.013867877 -0.018145353 -0.022853307 -0.027512833 -0.032025341 -0.035798848 -0.038658816 -0.040834177 -0.042831715][-0.016086906 -0.013687 -0.012057498 -0.011327662 -0.011895083 -0.013779029 -0.016890928 -0.020903513 -0.025528304 -0.0300387 -0.034208484 -0.037513543 -0.039895173 -0.041618 -0.043408427][-0.022986919 -0.020649716 -0.019088887 -0.0181387 -0.018277474 -0.019682504 -0.02219025 -0.02551499 -0.029376611 -0.033177618 -0.036578592 -0.039159354 -0.040934086 -0.042242628 -0.043778054][-0.030212719 -0.028231051 -0.026929833 -0.025948025 -0.025838435 -0.02670379 -0.028469142 -0.030938227 -0.033768032 -0.036511172 -0.038921777 -0.040691454 -0.041839182 -0.042754389 -0.043985605][-0.036562756 -0.035101321 -0.034099285 -0.033311535 -0.033075143 -0.0334833 -0.03452063 -0.036111049 -0.03794327 -0.039684825 -0.041127756 -0.042172946 -0.042860277 -0.043442234 -0.044448737][-0.041608807 -0.040541377 -0.039818063 -0.039134175 -0.038823966 -0.03892855 -0.03940101 -0.0402402 -0.041281383 -0.042274591 -0.043035947 -0.043535657 -0.043905724 -0.0443568 -0.045204885][-0.044870477 -0.044122804 -0.043666273 -0.043186869 -0.042868502 -0.042808067 -0.042957533 -0.043342944 -0.043838345 -0.044314038 -0.044629183 -0.044799305 -0.044970386 -0.045414485 -0.046164189][-0.046763834 -0.046210069 -0.045982856 -0.045725916 -0.045508053 -0.045420054 -0.045444354 -0.045580875 -0.045765363 -0.045941029 -0.046026554 -0.046016179 -0.046085048 -0.04646799 -0.0471739][-0.047945261 -0.047457431 -0.047341846 -0.047209576 -0.047081511 -0.047020886 -0.047003604 -0.047025569 -0.047077 -0.04714255 -0.047165651 -0.047125619 -0.047207035 -0.047581326 -0.048226018][-0.048912592 -0.04847176 -0.04844084 -0.048327725 -0.048202794 -0.048104063 -0.04804118 -0.047963168 -0.047900751 -0.047896404 -0.047881283 -0.047871303 -0.047935475 -0.048245065 -0.048862386]]...]
INFO - root - 2017-12-06 10:10:29.861950: step 47710, loss = 0.84, batch loss = 0.63 (12.7 examples/sec; 0.629 sec/batch; 49h:46m:46s remains)
INFO - root - 2017-12-06 10:10:36.281763: step 47720, loss = 0.86, batch loss = 0.65 (12.6 examples/sec; 0.637 sec/batch; 50h:21m:16s remains)
INFO - root - 2017-12-06 10:10:42.730647: step 47730, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.654 sec/batch; 51h:42m:40s remains)
INFO - root - 2017-12-06 10:10:49.166012: step 47740, loss = 0.80, batch loss = 0.59 (13.3 examples/sec; 0.600 sec/batch; 47h:27m:08s remains)
INFO - root - 2017-12-06 10:10:55.558995: step 47750, loss = 0.82, batch loss = 0.61 (12.6 examples/sec; 0.633 sec/batch; 50h:04m:32s remains)
INFO - root - 2017-12-06 10:11:02.023154: step 47760, loss = 0.76, batch loss = 0.55 (12.7 examples/sec; 0.630 sec/batch; 49h:51m:18s remains)
INFO - root - 2017-12-06 10:11:08.511294: step 47770, loss = 0.79, batch loss = 0.57 (12.6 examples/sec; 0.637 sec/batch; 50h:24m:23s remains)
INFO - root - 2017-12-06 10:11:14.821484: step 47780, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.628 sec/batch; 49h:39m:06s remains)
INFO - root - 2017-12-06 10:11:21.413519: step 47790, loss = 0.80, batch loss = 0.58 (11.9 examples/sec; 0.674 sec/batch; 53h:16m:15s remains)
INFO - root - 2017-12-06 10:11:27.853720: step 47800, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 51h:35m:24s remains)
2017-12-06 10:11:28.567617: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0093702525 0.087772392 0.15952432 0.2404004 0.3199636 0.29640692 0.16126651 -0.01956623 -0.12401046 -0.12569284 -0.068645656 -0.047541559 -0.081548221 -0.11516663 -0.086370096][0.029124342 0.14504248 0.26352206 0.31853658 0.37255916 0.37475443 0.26868656 0.070825778 -0.081008442 -0.10056625 -0.015545323 0.037537359 0.034321658 0.011605337 0.052720986][0.051627345 0.18217933 0.33638743 0.47437537 0.56093681 0.49634171 0.32903734 0.12336481 -0.023630835 -0.063916259 -0.029311236 0.045525633 0.073460884 0.053671487 0.076149695][0.066831507 0.18941033 0.32719368 0.46376419 0.56107777 0.5474655 0.39616472 0.16957468 -0.022518128 -0.085984014 -0.043332525 -0.06630186 -0.12607737 -0.17601696 -0.17993057][0.085872732 0.20833266 0.35127297 0.45814973 0.50991493 0.49108219 0.33017915 0.1197558 -0.03387548 -0.08263471 -0.038280725 -0.041272882 -0.10542198 -0.195499 -0.28317946][0.11885395 0.22750425 0.35412279 0.44248611 0.46585381 0.39668089 0.26405784 0.08781638 -0.021899104 -0.0076223761 0.032738879 0.0086181611 -0.099291414 -0.20377856 -0.36924493][0.18909839 0.28106508 0.3721095 0.42363471 0.4275263 0.34632781 0.22478032 0.086134307 0.0089043826 0.027556077 0.089849032 0.11851083 0.025755018 -0.12007639 -0.27298486][0.12973517 0.23400995 0.31887692 0.35675171 0.35502341 0.29564795 0.21147066 0.11512964 0.052676417 0.094342433 0.15881702 0.17330655 0.1243393 0.019649096 -0.092698492][-0.014503658 0.045700677 0.11986709 0.13891754 0.15612635 0.13731924 0.093078069 0.02262374 0.01500012 0.098270692 0.18571603 0.27211949 0.29393929 0.25430185 0.18301964][-0.19325328 -0.18496647 -0.1840128 -0.18779504 -0.18949452 -0.1641892 -0.12943351 -0.12041552 -0.088365979 0.039218448 0.20520312 0.330116 0.34778342 0.32095304 0.31145677][-0.29791152 -0.35220221 -0.397375 -0.42394122 -0.45175654 -0.42832276 -0.39977968 -0.36238894 -0.32092685 -0.22253069 -0.070240475 0.092700131 0.17413631 0.19840273 0.23209718][-0.32650843 -0.42169189 -0.49888766 -0.55709743 -0.58924228 -0.58714885 -0.55749476 -0.54013455 -0.54183149 -0.5257768 -0.46186626 -0.34280953 -0.26269072 -0.21974391 -0.15533322][-0.27789903 -0.38042879 -0.48656195 -0.57332754 -0.60680676 -0.62218815 -0.61381936 -0.58819324 -0.56509465 -0.56008589 -0.55256796 -0.52567643 -0.49280548 -0.4519555 -0.42048016][-0.18064569 -0.25003034 -0.3075912 -0.39986041 -0.45345131 -0.47696567 -0.47645408 -0.49484006 -0.55131972 -0.57430309 -0.57128614 -0.58313334 -0.59851819 -0.60057062 -0.5730257][-0.1344679 -0.16297176 -0.18553352 -0.24996895 -0.29923287 -0.33228913 -0.332011 -0.33354905 -0.42469952 -0.52242953 -0.5795204 -0.60752386 -0.64176381 -0.65645677 -0.64560026]]...]
INFO - root - 2017-12-06 10:11:34.951179: step 47810, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.646 sec/batch; 51h:07m:30s remains)
INFO - root - 2017-12-06 10:11:41.466106: step 47820, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.650 sec/batch; 51h:25m:23s remains)
INFO - root - 2017-12-06 10:11:47.944479: step 47830, loss = 0.76, batch loss = 0.54 (12.6 examples/sec; 0.635 sec/batch; 50h:14m:25s remains)
INFO - root - 2017-12-06 10:11:54.424127: step 47840, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.671 sec/batch; 53h:04m:07s remains)
INFO - root - 2017-12-06 10:12:00.886331: step 47850, loss = 0.81, batch loss = 0.59 (12.8 examples/sec; 0.626 sec/batch; 49h:28m:03s remains)
INFO - root - 2017-12-06 10:12:07.386277: step 47860, loss = 0.81, batch loss = 0.60 (12.5 examples/sec; 0.641 sec/batch; 50h:40m:21s remains)
INFO - root - 2017-12-06 10:12:13.915769: step 47870, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.641 sec/batch; 50h:40m:23s remains)
INFO - root - 2017-12-06 10:12:20.239194: step 47880, loss = 0.76, batch loss = 0.55 (12.6 examples/sec; 0.635 sec/batch; 50h:11m:17s remains)
INFO - root - 2017-12-06 10:12:26.712183: step 47890, loss = 0.79, batch loss = 0.58 (12.8 examples/sec; 0.627 sec/batch; 49h:32m:44s remains)
INFO - root - 2017-12-06 10:12:33.137214: step 47900, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.649 sec/batch; 51h:19m:16s remains)
2017-12-06 10:12:33.811916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03581835 -0.055537827 -0.093704447 -0.12392449 -0.14726344 -0.17214864 -0.18624271 -0.170845 -0.16971031 -0.21398836 -0.23692062 -0.18315327 -0.11187726 -0.081461057 -0.084734835][-0.34997874 -0.37671497 -0.40562713 -0.42304805 -0.44814587 -0.46612453 -0.45893034 -0.4090547 -0.34078977 -0.28616345 -0.23337531 -0.1724008 -0.059918728 0.0057762787 -0.00052627176][-0.76004064 -0.81283182 -0.83679026 -0.84502941 -0.88313 -0.87931061 -0.86286807 -0.79110396 -0.65069765 -0.47904855 -0.32331449 -0.20858096 -0.09442731 -0.015091017 0.010615952][-0.95490277 -1.0430999 -1.1152313 -1.1518979 -1.1722937 -1.1246308 -1.1302316 -1.1200649 -1.0302016 -0.84028977 -0.614652 -0.41059747 -0.24202895 -0.14707321 -0.11259528][-0.8321622 -0.8694573 -0.95637745 -1.0395629 -1.0773691 -1.0701793 -1.1278408 -1.1803403 -1.2467412 -1.1908185 -0.98679882 -0.70521468 -0.43421727 -0.27400675 -0.20150736][-0.6723215 -0.65361661 -0.71106976 -0.84821463 -0.98796636 -1.0490588 -1.0873219 -1.1841637 -1.3393304 -1.3870363 -1.2662776 -0.97299761 -0.63165969 -0.40281183 -0.27665967][-0.43769708 -0.41227716 -0.45716125 -0.58226132 -0.71257484 -0.83313966 -0.86958706 -0.88328642 -0.971816 -1.1262988 -1.1758002 -0.99767262 -0.69989967 -0.45842978 -0.29262689][-0.077861935 -0.10564025 -0.14628233 -0.21061489 -0.2861526 -0.32152039 -0.40930963 -0.40479779 -0.39135304 -0.50206041 -0.68234879 -0.73919994 -0.57897627 -0.39805168 -0.26170424][0.47382122 0.50048018 0.49832648 0.48001516 0.43922889 0.38599321 0.349641 0.29442868 0.27545872 0.21856466 0.049219422 -0.139079 -0.1863718 -0.15054038 -0.084126636][0.59629858 0.61507547 0.649059 0.71294761 0.76763827 0.81600547 0.81183195 0.78148258 0.74857336 0.70931977 0.58837575 0.40098837 0.25128874 0.17300522 0.10548595][0.4633854 0.45798552 0.5131039 0.62799579 0.77495807 0.89403892 0.91957176 0.88682294 0.8455193 0.78852141 0.69502193 0.59694695 0.47448188 0.37695464 0.27018949][0.25213474 0.24077272 0.25832182 0.3121013 0.43442667 0.52687132 0.51644474 0.44795221 0.4040049 0.38498789 0.33503014 0.31374681 0.30439603 0.28428426 0.216598][0.0844314 0.10608394 0.12897247 0.21483958 0.37144423 0.51025116 0.54095608 0.39450285 0.20360044 0.089338444 -0.01752945 -0.087097995 -0.076587863 -0.0027076602 0.026760243][0.005911909 0.015179656 0.031497225 0.066112377 0.17958707 0.35318926 0.38964519 0.27147222 0.074274786 -0.0656765 -0.18428788 -0.28278232 -0.28369632 -0.21688673 -0.14445667][-0.096541412 -0.068182819 -0.064645946 -0.035794772 0.044313811 0.13359123 0.1520845 0.075654946 -0.039781764 -0.14515433 -0.2112361 -0.26387629 -0.23401764 -0.15717062 -0.12305085]]...]
INFO - root - 2017-12-06 10:12:40.266637: step 47910, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 50h:17m:25s remains)
INFO - root - 2017-12-06 10:12:46.786903: step 47920, loss = 0.88, batch loss = 0.66 (12.3 examples/sec; 0.652 sec/batch; 51h:31m:20s remains)
INFO - root - 2017-12-06 10:12:53.335930: step 47930, loss = 0.78, batch loss = 0.56 (12.1 examples/sec; 0.662 sec/batch; 52h:17m:34s remains)
INFO - root - 2017-12-06 10:12:59.770275: step 47940, loss = 0.80, batch loss = 0.59 (12.7 examples/sec; 0.629 sec/batch; 49h:43m:32s remains)
INFO - root - 2017-12-06 10:13:06.154529: step 47950, loss = 0.75, batch loss = 0.54 (12.3 examples/sec; 0.650 sec/batch; 51h:22m:56s remains)
INFO - root - 2017-12-06 10:13:12.633854: step 47960, loss = 0.71, batch loss = 0.50 (12.7 examples/sec; 0.629 sec/batch; 49h:45m:16s remains)
INFO - root - 2017-12-06 10:13:19.021676: step 47970, loss = 0.89, batch loss = 0.68 (11.8 examples/sec; 0.676 sec/batch; 53h:25m:31s remains)
INFO - root - 2017-12-06 10:13:25.442564: step 47980, loss = 0.73, batch loss = 0.51 (12.5 examples/sec; 0.641 sec/batch; 50h:38m:02s remains)
INFO - root - 2017-12-06 10:13:31.941780: step 47990, loss = 0.82, batch loss = 0.61 (11.7 examples/sec; 0.682 sec/batch; 53h:54m:06s remains)
INFO - root - 2017-12-06 10:13:38.487850: step 48000, loss = 0.82, batch loss = 0.61 (11.8 examples/sec; 0.677 sec/batch; 53h:28m:01s remains)
2017-12-06 10:13:39.227199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.343767 -0.37698346 -0.42643696 -0.46030328 -0.47348675 -0.48806724 -0.49532488 -0.47127935 -0.43132895 -0.47474793 -0.53877771 -0.53375685 -0.53287333 -0.46871194 -0.41191483][-0.51806831 -0.43303597 -0.42493683 -0.44262564 -0.45421728 -0.4363665 -0.42856282 -0.40276995 -0.38076055 -0.3809073 -0.40296227 -0.44332984 -0.47735748 -0.52295 -0.51387745][-0.46894598 -0.40543842 -0.40267709 -0.39690283 -0.35136089 -0.33063656 -0.30006212 -0.24757573 -0.20031278 -0.18221828 -0.17006379 -0.21390194 -0.23584476 -0.2706601 -0.32943535][-0.22793454 -0.15416169 -0.14081234 -0.14866763 -0.10615531 -0.053058583 0.047663286 0.064075381 0.071314037 0.10722321 0.1157798 0.090835765 0.046807706 0.042300537 0.010954723][-0.16177224 -0.15189928 -0.14637513 -0.16823128 -0.11937106 -0.077153318 -0.01666835 0.05948171 0.16299459 0.21902397 0.24653509 0.29807949 0.27810663 0.28300637 0.26027378][-0.19585669 -0.26247853 -0.31986451 -0.30858177 -0.23700368 -0.14304452 -0.075657547 0.0053130537 0.075023621 0.13489629 0.20778549 0.24066156 0.22928551 0.20779213 0.2157023][-0.14766744 -0.17697334 -0.1102686 -0.10699582 -0.049396351 -0.0034207553 0.020055734 0.091045991 0.12837781 0.14842542 0.1796191 0.18325749 0.18440551 0.16810092 0.11820742][-0.031908188 0.014877848 0.11682379 0.17689934 0.22869357 0.19469774 0.20638576 0.19948485 0.17130333 0.13957809 0.12392178 0.10468508 0.091158748 0.048243791 0.019349746][0.11804007 0.066517234 0.046124712 0.05175446 0.052173585 0.017402172 0.013744883 0.019793071 0.033772856 0.0028524622 -0.018727578 -0.033657737 -0.035132103 -0.077522084 -0.10292734][0.087417662 0.0098591968 -0.00796406 -0.047883768 -0.069186911 -0.056826081 -0.045469392 -0.033875242 -0.014354959 -0.022505835 -0.02855457 -0.056104243 -0.092062958 -0.13496627 -0.1537506][-0.24676675 -0.25114164 -0.21765013 -0.20622106 -0.17849217 -0.15049267 -0.13259396 -0.068905182 -0.042820182 -0.037484851 -0.034861721 -0.0402129 -0.055291422 -0.10476736 -0.13247095][-0.45092747 -0.39772928 -0.32682651 -0.28923944 -0.26513422 -0.22941126 -0.16281316 -0.10510097 -0.07048291 -0.045171782 -0.032455072 -0.018640853 -0.04453123 -0.068717368 -0.08575476][-0.59995162 -0.50333691 -0.42346802 -0.35992271 -0.30536512 -0.26225364 -0.20941857 -0.15144552 -0.066566855 -0.035222895 -0.012602836 -0.00077275932 -0.016106971 -0.02878103 -0.063762784][-0.45836654 -0.3917129 -0.38201457 -0.33123159 -0.27812004 -0.24534491 -0.20381808 -0.14230616 -0.10107791 -0.071831144 -0.040570773 -0.020211481 -0.017347313 -0.0084148124 -0.0220742][-0.23582593 -0.24670278 -0.25668263 -0.26325393 -0.26014519 -0.2471265 -0.223671 -0.19245011 -0.15506493 -0.11807453 -0.097850323 -0.058303822 -0.047831092 -0.014553569 -0.023141563]]...]
INFO - root - 2017-12-06 10:13:45.797573: step 48010, loss = 0.79, batch loss = 0.58 (12.3 examples/sec; 0.650 sec/batch; 51h:23m:50s remains)
INFO - root - 2017-12-06 10:13:52.299245: step 48020, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 49h:46m:35s remains)
INFO - root - 2017-12-06 10:13:58.770302: step 48030, loss = 0.74, batch loss = 0.52 (12.2 examples/sec; 0.656 sec/batch; 51h:51m:17s remains)
INFO - root - 2017-12-06 10:14:05.260459: step 48040, loss = 0.88, batch loss = 0.67 (12.2 examples/sec; 0.654 sec/batch; 51h:38m:25s remains)
INFO - root - 2017-12-06 10:14:11.826635: step 48050, loss = 0.85, batch loss = 0.63 (12.2 examples/sec; 0.654 sec/batch; 51h:41m:26s remains)
INFO - root - 2017-12-06 10:14:18.243146: step 48060, loss = 0.79, batch loss = 0.57 (12.8 examples/sec; 0.626 sec/batch; 49h:27m:33s remains)
INFO - root - 2017-12-06 10:14:24.836169: step 48070, loss = 0.79, batch loss = 0.58 (11.9 examples/sec; 0.672 sec/batch; 53h:05m:47s remains)
INFO - root - 2017-12-06 10:14:31.336615: step 48080, loss = 0.81, batch loss = 0.60 (12.1 examples/sec; 0.661 sec/batch; 52h:13m:29s remains)
INFO - root - 2017-12-06 10:14:37.619333: step 48090, loss = 0.86, batch loss = 0.64 (12.6 examples/sec; 0.635 sec/batch; 50h:11m:29s remains)
INFO - root - 2017-12-06 10:14:44.120895: step 48100, loss = 0.82, batch loss = 0.60 (12.7 examples/sec; 0.630 sec/batch; 49h:45m:47s remains)
2017-12-06 10:14:44.803416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.29924697 -0.73467869 -1.0963467 -1.3292973 -1.4569421 -1.3710477 -1.2385937 -1.0265477 -0.90137905 -0.875282 -0.81906688 -0.72076392 -0.55832553 -0.43253917 -0.49419743][0.36737651 0.06508711 -0.49483362 -1.2093817 -1.7747964 -1.7951031 -1.6748886 -1.4050539 -1.157935 -0.9393537 -0.77448028 -0.65616345 -0.56675076 -0.43217283 -0.3229627][1.22222 0.91957188 0.464117 -0.21954571 -1.0127656 -1.4964396 -1.6772597 -1.5661336 -1.3674191 -1.1101041 -0.7858386 -0.50402564 -0.31797802 -0.2155858 -0.14119743][1.6387987 1.4325891 1.0563526 0.45357502 -0.23971152 -0.72938186 -1.0044839 -1.1742176 -1.2068306 -0.90330142 -0.65126574 -0.4385342 -0.27969962 -0.0981524 0.02922146][1.46034 1.4142052 1.3367542 1.003661 0.48670095 0.034286976 -0.27935517 -0.53132188 -0.74315983 -0.74591982 -0.71712434 -0.62249357 -0.52108061 -0.39448982 -0.27997142][1.3751916 1.3824728 1.4054589 1.3847241 1.1278117 0.83890265 0.61786479 0.39044482 0.060555711 -0.072132915 -0.19631679 -0.30235142 -0.33774829 -0.3481155 -0.28861347][1.2506526 1.204687 1.143041 1.0667263 0.78111249 0.58233166 0.67211235 0.54174149 0.28883594 0.18076879 0.057375222 -0.12657811 -0.19738233 -0.22834632 -0.18887931][0.52572703 0.51822644 0.42753977 0.42077291 0.271824 0.12133579 0.21777228 0.32741055 0.26042867 0.21690011 0.15928479 0.070616841 0.064927787 0.059261113 -0.0036207214][-0.11163604 -0.1688332 -0.34845251 -0.32183719 -0.32474318 -0.27991509 -0.0884891 0.085573211 0.15317628 0.1880303 0.15952539 0.077287495 0.011073746 0.024191968 0.036469102][-0.78259927 -0.70035326 -0.72466534 -0.75394094 -0.68616772 -0.41956338 -0.14953911 0.10287258 0.15047689 0.16529247 0.11231999 0.06806533 0.036557496 0.029938653 0.010532402][-1.1594003 -1.0876803 -0.96238405 -0.81918079 -0.6462186 -0.4504638 -0.1809998 0.087930009 0.17216444 0.10930936 -0.028884929 -0.11182714 -0.10144477 -0.076214276 -0.078739971][-0.84511405 -0.86250371 -0.8288874 -0.71257168 -0.5275234 -0.25705445 -0.057316937 0.066640958 0.080416486 0.02268751 -0.079996943 -0.13197803 -0.14936137 -0.14211546 -0.15202251][-0.755926 -0.74544984 -0.70242947 -0.67115408 -0.58689022 -0.41112372 -0.2590698 -0.11253101 -0.0812889 -0.11931781 -0.17983079 -0.176191 -0.1832328 -0.190281 -0.22651207][-0.57233745 -0.63231075 -0.59879297 -0.57836372 -0.54844457 -0.5641973 -0.56181306 -0.51049161 -0.40785396 -0.36335596 -0.31724706 -0.25937003 -0.22164461 -0.21745193 -0.26496175][-0.3362748 -0.36761287 -0.33266518 -0.42074683 -0.46456718 -0.52959448 -0.53483266 -0.56268048 -0.53034866 -0.46365198 -0.32500827 -0.26142728 -0.27889228 -0.25675225 -0.26573402]]...]
INFO - root - 2017-12-06 10:14:51.284255: step 48110, loss = 0.81, batch loss = 0.59 (12.5 examples/sec; 0.641 sec/batch; 50h:39m:26s remains)
INFO - root - 2017-12-06 10:14:57.814001: step 48120, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.640 sec/batch; 50h:35m:36s remains)
INFO - root - 2017-12-06 10:15:04.451023: step 48130, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.647 sec/batch; 51h:06m:39s remains)
INFO - root - 2017-12-06 10:15:10.921956: step 48140, loss = 0.82, batch loss = 0.61 (12.4 examples/sec; 0.644 sec/batch; 50h:49m:51s remains)
INFO - root - 2017-12-06 10:15:17.435586: step 48150, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.650 sec/batch; 51h:19m:08s remains)
INFO - root - 2017-12-06 10:15:23.793711: step 48160, loss = 0.92, batch loss = 0.71 (12.5 examples/sec; 0.639 sec/batch; 50h:30m:05s remains)
INFO - root - 2017-12-06 10:15:30.276514: step 48170, loss = 0.80, batch loss = 0.59 (12.8 examples/sec; 0.624 sec/batch; 49h:17m:33s remains)
INFO - root - 2017-12-06 10:15:36.791118: step 48180, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.646 sec/batch; 51h:02m:47s remains)
INFO - root - 2017-12-06 10:15:43.224600: step 48190, loss = 0.83, batch loss = 0.62 (12.5 examples/sec; 0.638 sec/batch; 50h:23m:00s remains)
INFO - root - 2017-12-06 10:15:49.769716: step 48200, loss = 0.84, batch loss = 0.62 (13.0 examples/sec; 0.617 sec/batch; 48h:43m:18s remains)
2017-12-06 10:15:50.454968: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.79207087 0.32044566 0.053643011 -0.19869797 -0.51327252 -0.66049486 -0.64487195 -0.68143594 -0.76912731 -0.7438516 -0.68194556 -0.76131421 -0.84920239 -0.829166 -0.76933545][-0.48973459 -1.0019816 -1.1993791 -1.4864333 -1.6320217 -1.7247391 -1.7175261 -1.5480602 -1.3156034 -1.1207484 -0.92837369 -0.83231962 -0.66855931 -0.59860283 -0.68607694][-1.5421871 -2.0665514 -2.4162436 -2.6091309 -2.6546254 -2.7158241 -2.6553679 -2.4906223 -2.2167692 -1.7862763 -1.3870342 -1.1989782 -0.96410865 -0.78168625 -0.64497358][-2.0435965 -2.6472442 -2.9625328 -3.0253038 -3.2960773 -3.442198 -3.348336 -2.9634538 -2.5893514 -2.2744794 -1.9841269 -1.6830777 -1.2925297 -1.0905552 -0.9112708][-1.6236436 -2.2858341 -2.7051065 -2.9571688 -3.2394786 -3.1481702 -2.8992758 -2.6286376 -2.2701623 -1.870827 -1.6979781 -1.5732148 -1.2924311 -1.1092768 -0.96215242][-1.042652 -1.693921 -2.0216494 -2.085151 -1.9680676 -1.7712673 -1.5624497 -1.1976227 -0.96095693 -0.79819906 -0.78705424 -0.73599833 -0.74555331 -0.75352317 -0.83074254][-0.3384071 -0.66627491 -0.77937704 -0.65414578 -0.27156651 0.22768617 0.57625896 0.80840111 0.94335449 0.94965374 0.72121984 0.43989873 0.1168908 -0.040053576 -0.14312164][0.19333136 0.091021039 0.29540011 0.77017277 1.2367771 1.6116008 1.9676927 2.2155237 2.2143395 2.0171304 1.7501514 1.3810793 0.847797 0.34380049 0.072120719][0.63125312 0.57279539 1.0425512 1.4441125 1.8751322 2.1918628 2.3983119 2.451463 2.3436451 2.1000607 1.7163749 1.418281 1.0117356 0.66484016 0.32838067][0.56666183 0.645894 0.90452236 1.2307709 1.5293459 1.6812216 1.804372 1.7759069 1.6050313 1.3062428 1.0065194 0.6634112 0.34450522 0.28183207 0.27362257][-0.0057632923 0.089085646 0.28965607 0.4295851 0.53504485 0.60373259 0.5690648 0.56736505 0.46913445 0.29750371 0.10021093 -0.082646862 -0.17134008 -0.14006995 -0.0085542127][-0.42702153 -0.56635535 -0.61013389 -0.536604 -0.41625276 -0.40694258 -0.40555742 -0.30380926 -0.2927216 -0.35132366 -0.40798774 -0.41887766 -0.38732892 -0.20921144 -0.018700026][-0.81858277 -0.89647305 -1.0565588 -1.2464223 -1.1913892 -0.93436491 -0.72245854 -0.61207926 -0.56196034 -0.590396 -0.5888074 -0.57528847 -0.51891977 -0.38944879 -0.26669967][-0.78372723 -0.94449025 -1.1632156 -1.1757809 -1.2144221 -1.1429712 -1.0967069 -0.93976492 -0.81609243 -0.76307571 -0.72799647 -0.80450994 -0.76630044 -0.66395503 -0.57617384][-0.29359183 -0.57189858 -0.94555974 -1.2007931 -1.2369829 -1.10185 -1.1416805 -1.1651006 -1.1933551 -1.1113232 -1.0119292 -0.90077841 -0.83580238 -0.89115757 -0.87740237]]...]
INFO - root - 2017-12-06 10:15:56.924653: step 48210, loss = 0.81, batch loss = 0.59 (12.6 examples/sec; 0.637 sec/batch; 50h:17m:25s remains)
INFO - root - 2017-12-06 10:16:03.410212: step 48220, loss = 0.90, batch loss = 0.68 (12.5 examples/sec; 0.642 sec/batch; 50h:40m:35s remains)
INFO - root - 2017-12-06 10:16:09.716787: step 48230, loss = 0.71, batch loss = 0.50 (12.3 examples/sec; 0.652 sec/batch; 51h:30m:21s remains)
INFO - root - 2017-12-06 10:16:16.266904: step 48240, loss = 0.72, batch loss = 0.51 (12.3 examples/sec; 0.652 sec/batch; 51h:29m:13s remains)
INFO - root - 2017-12-06 10:16:22.657928: step 48250, loss = 0.73, batch loss = 0.52 (12.2 examples/sec; 0.655 sec/batch; 51h:41m:10s remains)
INFO - root - 2017-12-06 10:16:29.123274: step 48260, loss = 0.81, batch loss = 0.60 (11.7 examples/sec; 0.686 sec/batch; 54h:11m:36s remains)
INFO - root - 2017-12-06 10:16:35.537919: step 48270, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.640 sec/batch; 50h:32m:24s remains)
INFO - root - 2017-12-06 10:16:42.001227: step 48280, loss = 0.75, batch loss = 0.54 (12.4 examples/sec; 0.647 sec/batch; 51h:06m:50s remains)
INFO - root - 2017-12-06 10:16:48.575972: step 48290, loss = 0.90, batch loss = 0.68 (12.3 examples/sec; 0.651 sec/batch; 51h:25m:17s remains)
INFO - root - 2017-12-06 10:16:55.080409: step 48300, loss = 0.94, batch loss = 0.72 (12.5 examples/sec; 0.639 sec/batch; 50h:27m:25s remains)
2017-12-06 10:16:55.763021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041990072 -0.041189745 -0.040874 -0.040592652 -0.040287487 -0.039955895 -0.039622426 -0.039350383 -0.039232682 -0.039314892 -0.039554521 -0.039913859 -0.040253974 -0.040546648 -0.041341063][-0.042327505 -0.040756974 -0.040116016 -0.039818376 -0.039649654 -0.039515469 -0.039202441 -0.038660266 -0.038202271 -0.037958361 -0.037941784 -0.03819216 -0.0385476 -0.039046906 -0.04011013][-0.042926461 -0.040587716 -0.039636854 -0.039391898 -0.03946754 -0.039551251 -0.039197251 -0.038301852 -0.037294485 -0.036474466 -0.036025833 -0.035965655 -0.036201358 -0.036753375 -0.038066372][-0.042988311 -0.0403375 -0.039117556 -0.038838323 -0.039043285 -0.039205935 -0.038697872 -0.03744318 -0.035987496 -0.0347418 -0.033918656 -0.033519417 -0.033478457 -0.033942956 -0.035492711][-0.04207179 -0.039401066 -0.038062785 -0.037691791 -0.037837423 -0.037946153 -0.037317529 -0.035797503 -0.034057338 -0.032677643 -0.031722672 -0.031122219 -0.030795723 -0.031102601 -0.032860782][-0.040475942 -0.037938874 -0.036689937 -0.036221236 -0.036196794 -0.03622146 -0.03549169 -0.033825126 -0.031851508 -0.030394293 -0.02953871 -0.02896107 -0.028544083 -0.028767854 -0.03072571][-0.038601357 -0.036202576 -0.035029322 -0.034428965 -0.034190502 -0.034174424 -0.033479173 -0.031897221 -0.029925875 -0.028567661 -0.027980879 -0.027588628 -0.02724988 -0.027484633 -0.029590517][-0.037375681 -0.035113849 -0.033967454 -0.033193044 -0.032725785 -0.032629956 -0.032144196 -0.030948196 -0.029368561 -0.028301071 -0.027884543 -0.027648285 -0.027470902 -0.027705193 -0.029746931][-0.037455224 -0.035239667 -0.0340831 -0.03314276 -0.032538421 -0.032306392 -0.031927776 -0.031148188 -0.030107956 -0.02937565 -0.029144723 -0.029192228 -0.029292498 -0.029672123 -0.031491496][-0.038493097 -0.036400814 -0.035301127 -0.034377918 -0.033784356 -0.033511881 -0.033198509 -0.032765336 -0.03217322 -0.03179032 -0.031823669 -0.032159403 -0.032523315 -0.03300998 -0.03453856][-0.040446956 -0.038446322 -0.03745557 -0.036642261 -0.036136989 -0.03585409 -0.035568118 -0.035291865 -0.035036266 -0.034955926 -0.03520339 -0.035717059 -0.036230426 -0.036757417 -0.038034059][-0.042706292 -0.040946834 -0.04008295 -0.039406195 -0.039037507 -0.038864344 -0.038672447 -0.038519733 -0.038515855 -0.038619187 -0.038907282 -0.039389908 -0.039918344 -0.040445268 -0.041450519][-0.044587757 -0.04317911 -0.042550951 -0.042088013 -0.041873533 -0.041803487 -0.041760493 -0.04174082 -0.041834187 -0.041982692 -0.042205162 -0.042556804 -0.042965733 -0.043444384 -0.04425684][-0.045420144 -0.044353995 -0.044014413 -0.043784954 -0.043723591 -0.043771368 -0.0438469 -0.04392837 -0.044093784 -0.044301447 -0.044505276 -0.044802666 -0.045137949 -0.045550376 -0.046307873][-0.045473605 -0.044679116 -0.044635754 -0.04464281 -0.044735532 -0.044920504 -0.045158554 -0.045364551 -0.045624252 -0.04590204 -0.046153888 -0.046435937 -0.046727624 -0.047049746 -0.047830176]]...]
INFO - root - 2017-12-06 10:17:02.301548: step 48310, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 52h:16m:37s remains)
INFO - root - 2017-12-06 10:17:08.798086: step 48320, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.653 sec/batch; 51h:31m:25s remains)
INFO - root - 2017-12-06 10:17:15.304903: step 48330, loss = 0.78, batch loss = 0.57 (12.5 examples/sec; 0.642 sec/batch; 50h:40m:36s remains)
INFO - root - 2017-12-06 10:17:21.717758: step 48340, loss = 0.80, batch loss = 0.58 (11.9 examples/sec; 0.672 sec/batch; 53h:01m:11s remains)
INFO - root - 2017-12-06 10:17:28.186890: step 48350, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.642 sec/batch; 50h:42m:24s remains)
INFO - root - 2017-12-06 10:17:34.655261: step 48360, loss = 0.81, batch loss = 0.60 (11.8 examples/sec; 0.679 sec/batch; 53h:33m:34s remains)
INFO - root - 2017-12-06 10:17:41.061770: step 48370, loss = 0.80, batch loss = 0.59 (13.8 examples/sec; 0.578 sec/batch; 45h:36m:48s remains)
INFO - root - 2017-12-06 10:17:47.553977: step 48380, loss = 0.79, batch loss = 0.57 (12.0 examples/sec; 0.665 sec/batch; 52h:29m:27s remains)
INFO - root - 2017-12-06 10:17:54.121635: step 48390, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.645 sec/batch; 50h:55m:06s remains)
INFO - root - 2017-12-06 10:18:00.659002: step 48400, loss = 0.75, batch loss = 0.53 (12.6 examples/sec; 0.634 sec/batch; 50h:00m:24s remains)
2017-12-06 10:18:01.321143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012768187 0.17943117 0.5482372 0.65547854 0.43908489 0.36246586 0.36137828 0.17138821 -0.2596224 -0.35310388 -0.36724651 -0.18861112 0.0021333769 0.37834054 0.9066186][-0.16498421 -0.17651062 0.054488517 0.34070912 0.59342533 0.58896005 0.25831118 0.058936395 0.17100269 -0.28698152 -0.64082009 -0.43693641 -0.294162 -0.29454708 -0.25028607][-0.10859531 0.021238908 -0.11032476 -0.095480151 0.074920528 0.23765171 0.19154656 0.24717104 0.19919822 -0.011249818 -0.077247411 -0.371919 -0.86560953 -0.68954557 -0.39952916][-0.25615093 -0.21442375 -0.12797572 -0.061587408 -0.32924592 -0.46075961 -0.37425867 -0.17492506 -0.32002318 -0.074027926 -0.061214067 -0.29307997 -0.2668308 -0.7345475 -1.3528827][-0.26756254 -0.34654236 -0.28562275 -0.19143073 -0.16211948 -0.3230052 -0.47865489 -0.32898042 -0.0098405406 0.17780247 -0.13441792 -0.20542553 -0.14668941 -0.11518724 -0.80438495][-0.069538362 -0.19366287 -0.36649162 -0.24604636 -0.058185518 -0.047911845 0.13980165 0.19825274 0.37482712 0.83309221 1.0190899 0.82475573 0.41425288 0.10454818 -0.17570332][0.36147404 0.17171812 0.056015156 -0.056224231 0.071744956 0.31186932 0.56243658 0.71594328 1.0010694 1.3492204 1.7535145 2.2646453 2.1052294 1.4493536 1.0987644][0.75560737 0.56066161 0.4672997 0.36926734 0.51095128 0.58969831 0.87326509 1.1340144 1.555559 1.9395374 2.2169273 2.6879373 3.2867696 3.1744921 2.6089215][0.4145931 0.66395974 0.823741 0.72945619 0.84008259 1.0525323 1.2605256 1.4670317 1.8432668 2.195467 2.5882506 2.9541631 3.2140229 3.3738062 3.3345652][-0.29638129 -0.045100633 0.25111258 0.42645389 0.44653535 0.59607196 0.84562874 1.1998523 1.423038 1.5888109 2.1529841 2.3796613 2.3795621 2.7651813 2.86528][-0.41938826 -0.084406577 -0.18345451 0.19021395 0.29045078 0.27611044 0.2602714 0.42983061 0.85712516 1.0916045 0.99300754 1.0361331 1.1653047 1.257144 1.108049][-0.0083643645 0.13981313 0.18460384 0.42689788 0.26784402 0.32648727 0.22032917 0.15597472 0.052450918 0.019634046 -0.029594958 -0.20368004 -0.52001458 -0.99707228 -0.82828569][-0.099929065 0.050609387 0.16002482 0.25375786 0.23216265 0.14497757 -0.060613696 -0.34652463 -0.78419214 -0.96098369 -1.3755651 -1.6940879 -1.9836613 -2.4645927 -2.8572178][-0.47788221 -0.29808354 -0.047408741 0.11445696 0.18541715 0.20648649 0.082515039 -0.21355011 -0.50219858 -1.2263049 -1.6665541 -1.9599905 -2.7337785 -3.0141926 -3.0180535][-0.073756091 -0.12019543 0.069342576 0.33325985 0.56977183 0.5841912 0.4927814 0.072480433 -0.2046507 -0.74005979 -1.3597795 -2.0517673 -2.6074414 -2.9298992 -2.6514943]]...]
INFO - root - 2017-12-06 10:18:07.777187: step 48410, loss = 0.96, batch loss = 0.75 (12.3 examples/sec; 0.648 sec/batch; 51h:08m:34s remains)
INFO - root - 2017-12-06 10:18:14.268938: step 48420, loss = 0.83, batch loss = 0.61 (12.4 examples/sec; 0.646 sec/batch; 51h:00m:22s remains)
INFO - root - 2017-12-06 10:18:20.903193: step 48430, loss = 0.84, batch loss = 0.63 (12.6 examples/sec; 0.636 sec/batch; 50h:12m:36s remains)
INFO - root - 2017-12-06 10:18:27.235528: step 48440, loss = 0.78, batch loss = 0.57 (12.6 examples/sec; 0.635 sec/batch; 50h:07m:06s remains)
INFO - root - 2017-12-06 10:18:33.752901: step 48450, loss = 0.84, batch loss = 0.62 (12.1 examples/sec; 0.660 sec/batch; 52h:02m:56s remains)
INFO - root - 2017-12-06 10:18:40.273610: step 48460, loss = 0.76, batch loss = 0.55 (12.5 examples/sec; 0.638 sec/batch; 50h:22m:10s remains)
INFO - root - 2017-12-06 10:18:46.795911: step 48470, loss = 0.72, batch loss = 0.50 (12.1 examples/sec; 0.660 sec/batch; 52h:03m:22s remains)
INFO - root - 2017-12-06 10:18:53.324654: step 48480, loss = 0.84, batch loss = 0.63 (11.4 examples/sec; 0.702 sec/batch; 55h:22m:08s remains)
INFO - root - 2017-12-06 10:18:59.830509: step 48490, loss = 0.75, batch loss = 0.53 (12.0 examples/sec; 0.669 sec/batch; 52h:46m:51s remains)
INFO - root - 2017-12-06 10:19:06.397507: step 48500, loss = 0.73, batch loss = 0.52 (12.7 examples/sec; 0.632 sec/batch; 49h:50m:16s remains)
2017-12-06 10:19:07.046683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.26803812 -0.68133748 -0.82002193 -0.77439541 -0.63056463 -0.52490717 -0.41055849 -0.43902919 -0.60768151 -0.88339823 -1.1636649 -1.4345526 -1.7008826 -1.917168 -1.9499421][-0.50438923 -0.98785925 -1.0930573 -0.99058306 -0.74809974 -0.40457514 -0.12218086 -0.0013233498 -0.089205526 -0.36450559 -0.74156618 -1.1761581 -1.6113044 -2.0077541 -2.270694][-1.1366725 -1.6341017 -1.728508 -1.4540464 -1.0130478 -0.58638209 -0.25517738 -0.012379572 0.030523136 -0.042058121 -0.32402259 -0.63369286 -1.052913 -1.4617344 -1.8652924][-1.8342582 -2.1221285 -2.0002775 -1.6162075 -1.0807519 -0.52047187 -0.15081373 0.059066094 0.050125904 -0.020532764 -0.2060954 -0.34707457 -0.56701475 -0.93112713 -1.338984][-2.0474825 -2.2410796 -1.857383 -1.1910453 -0.59047282 -0.04000603 0.30646983 0.44348186 0.41469753 0.29716679 0.13985965 -0.083752736 -0.23645279 -0.41075876 -0.82232136][-2.2375975 -2.1399128 -1.5921277 -0.86397159 -0.108413 0.51006389 0.84881222 0.94352388 0.90060782 0.77697843 0.63537049 0.47112048 0.39137912 0.16478425 0.029129714][-1.8820734 -1.7279526 -1.1378126 -0.36395082 0.32637557 0.95674562 1.3985288 1.5491081 1.4302502 1.2155741 1.0080385 0.75835329 0.6989184 0.67396581 0.5384838][-1.462592 -1.2704037 -0.72454506 -0.014728777 0.66820216 1.2235997 1.6904429 2.0249727 2.1448715 1.9982859 1.7155159 1.4668343 1.2882836 1.1679453 0.80946529][-0.57987243 -0.76576591 -0.52812862 -0.16983257 0.27910149 0.81792289 1.2899088 1.683708 1.927507 1.8830221 1.763082 1.3770772 1.0201495 0.8674565 0.62731808][0.010462798 -0.40718827 -0.50565892 -0.30166185 -0.0925349 0.18160844 0.55037272 0.98255229 1.2645552 1.3652561 1.2722319 1.1220844 0.75966209 0.38980889 0.10920878][0.22711638 -0.12678938 -0.38206932 -0.49450645 -0.39456287 -0.24234107 -0.047588106 0.1932784 0.40955031 0.47810054 0.40268174 0.24498025 0.069809355 -0.10373726 -0.28930849][-0.18087628 -0.44860551 -0.52859455 -0.59867966 -0.61566925 -0.60548365 -0.5447337 -0.54954708 -0.55632555 -0.45145062 -0.49295709 -0.51055086 -0.5821417 -0.64576918 -0.63357854][-0.61521071 -0.75447965 -0.69719815 -0.58395779 -0.49882087 -0.52013665 -0.57902724 -0.74336022 -0.94886643 -1.0711572 -1.1576415 -1.1552497 -1.1421914 -1.0248755 -0.85973257][-0.96150863 -0.96401775 -0.81245577 -0.57611191 -0.38693124 -0.30496925 -0.35395271 -0.51451021 -0.72165173 -0.95734048 -1.1440959 -1.2515897 -1.273829 -1.1911588 -1.105675][-0.98118263 -0.869152 -0.6905486 -0.49015656 -0.32104412 -0.25281873 -0.27524042 -0.36723804 -0.49512768 -0.69031364 -0.91208243 -1.0311812 -1.0810896 -1.0695785 -0.9950788]]...]
INFO - root - 2017-12-06 10:19:13.393804: step 48510, loss = 0.75, batch loss = 0.53 (14.4 examples/sec; 0.557 sec/batch; 43h:58m:33s remains)
INFO - root - 2017-12-06 10:19:19.906900: step 48520, loss = 0.94, batch loss = 0.72 (12.7 examples/sec; 0.628 sec/batch; 49h:31m:00s remains)
INFO - root - 2017-12-06 10:19:26.379094: step 48530, loss = 0.80, batch loss = 0.58 (12.4 examples/sec; 0.643 sec/batch; 50h:41m:32s remains)
INFO - root - 2017-12-06 10:19:32.796937: step 48540, loss = 0.78, batch loss = 0.56 (12.0 examples/sec; 0.668 sec/batch; 52h:41m:46s remains)
INFO - root - 2017-12-06 10:19:39.287024: step 48550, loss = 0.77, batch loss = 0.56 (12.4 examples/sec; 0.646 sec/batch; 50h:57m:30s remains)
INFO - root - 2017-12-06 10:19:45.721668: step 48560, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.663 sec/batch; 52h:16m:05s remains)
INFO - root - 2017-12-06 10:19:52.221810: step 48570, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.654 sec/batch; 51h:33m:22s remains)
INFO - root - 2017-12-06 10:19:58.691417: step 48580, loss = 0.97, batch loss = 0.75 (12.4 examples/sec; 0.644 sec/batch; 50h:49m:22s remains)
INFO - root - 2017-12-06 10:20:05.160973: step 48590, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.631 sec/batch; 49h:44m:03s remains)
INFO - root - 2017-12-06 10:20:11.688449: step 48600, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.643 sec/batch; 50h:42m:37s remains)
2017-12-06 10:20:12.354252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.074300013 -0.074103288 -0.07387 -0.073485762 -0.073036574 -0.072615184 -0.072240323 -0.072106473 -0.072151378 -0.072240219 -0.072357826 -0.072455436 -0.072551392 -0.072647266 -0.0729158][-0.075070605 -0.075025052 -0.074788868 -0.074194364 -0.073322684 -0.072465174 -0.071821667 -0.071616255 -0.071650773 -0.071819641 -0.072015606 -0.072151043 -0.072282806 -0.072420128 -0.072660044][-0.077331632 -0.077106893 -0.076425791 -0.075166248 -0.073712535 -0.072350025 -0.071247548 -0.070815556 -0.070820868 -0.071055055 -0.071325421 -0.07156159 -0.0717798 -0.07201764 -0.07230892][-0.080582917 -0.079744712 -0.0783258 -0.076384194 -0.074376665 -0.072538473 -0.070929632 -0.070120238 -0.069926016 -0.070082456 -0.0704018 -0.070722178 -0.071073875 -0.071446776 -0.071900666][-0.084568508 -0.082947157 -0.080616668 -0.077812448 -0.0752412 -0.072809912 -0.070662975 -0.069492891 -0.069085352 -0.069189981 -0.069572926 -0.069970787 -0.070437983 -0.070955977 -0.071591385][-0.089391969 -0.086955264 -0.083344325 -0.079477012 -0.075998843 -0.072786108 -0.069961078 -0.068453349 -0.068084106 -0.06831675 -0.068907954 -0.069491029 -0.07010071 -0.070688106 -0.071443006][-0.095275559 -0.091804326 -0.086772844 -0.081445605 -0.076406814 -0.072098784 -0.068603635 -0.066942625 -0.066839211 -0.067531511 -0.068488061 -0.069343612 -0.070075676 -0.070707545 -0.071493782][-0.10045326 -0.0961437 -0.089971915 -0.083357215 -0.076951683 -0.071689472 -0.0678146 -0.066007361 -0.066175215 -0.06719555 -0.068342067 -0.06937506 -0.070159405 -0.070821077 -0.071703628][-0.10389929 -0.098813757 -0.091825329 -0.084665485 -0.077863142 -0.072255462 -0.068257496 -0.066437341 -0.066655248 -0.067750111 -0.068768665 -0.069697469 -0.070455566 -0.071165852 -0.07209304][-0.1049583 -0.099143073 -0.091886528 -0.084749915 -0.077998586 -0.072463371 -0.068899348 -0.06743671 -0.067584559 -0.068412632 -0.069252908 -0.070066616 -0.070791245 -0.071464412 -0.0723482][-0.10380086 -0.097590283 -0.090341911 -0.083446272 -0.07697399 -0.071796522 -0.068871215 -0.067973129 -0.068232521 -0.069012165 -0.069748379 -0.070486084 -0.071055941 -0.071694709 -0.072493456][-0.10144484 -0.095326185 -0.088146269 -0.081424154 -0.075344339 -0.0706655 -0.068311907 -0.067838043 -0.068422586 -0.069233462 -0.069951855 -0.070702732 -0.071251884 -0.071816713 -0.07253173][-0.098748595 -0.093126737 -0.086211666 -0.079772167 -0.074035048 -0.069690809 -0.0676304 -0.06758967 -0.068517171 -0.069484606 -0.070209816 -0.070780933 -0.07117039 -0.071700737 -0.072431505][-0.096082017 -0.090764426 -0.084548488 -0.078560822 -0.0729605 -0.069052108 -0.067423806 -0.067589939 -0.068634942 -0.069639161 -0.070337459 -0.070791371 -0.0710938 -0.071510173 -0.072105326][-0.093422763 -0.088368133 -0.082704917 -0.077119678 -0.072145723 -0.069132045 -0.068127573 -0.068297908 -0.0691194 -0.069908239 -0.070427343 -0.0707441 -0.07101582 -0.071439132 -0.072022408]]...]
INFO - root - 2017-12-06 10:20:18.880114: step 48610, loss = 0.79, batch loss = 0.58 (12.5 examples/sec; 0.638 sec/batch; 50h:19m:06s remains)
INFO - root - 2017-12-06 10:20:25.244089: step 48620, loss = 0.83, batch loss = 0.61 (12.5 examples/sec; 0.640 sec/batch; 50h:28m:16s remains)
INFO - root - 2017-12-06 10:20:31.716939: step 48630, loss = 0.79, batch loss = 0.58 (12.6 examples/sec; 0.634 sec/batch; 49h:59m:35s remains)
INFO - root - 2017-12-06 10:20:38.191639: step 48640, loss = 0.77, batch loss = 0.55 (12.4 examples/sec; 0.644 sec/batch; 50h:46m:50s remains)
INFO - root - 2017-12-06 10:20:44.609004: step 48650, loss = 0.79, batch loss = 0.57 (14.2 examples/sec; 0.563 sec/batch; 44h:25m:01s remains)
INFO - root - 2017-12-06 10:20:51.044023: step 48660, loss = 0.82, batch loss = 0.60 (12.6 examples/sec; 0.633 sec/batch; 49h:54m:42s remains)
INFO - root - 2017-12-06 10:20:57.543599: step 48670, loss = 0.88, batch loss = 0.67 (12.4 examples/sec; 0.643 sec/batch; 50h:42m:46s remains)
INFO - root - 2017-12-06 10:21:04.026306: step 48680, loss = 0.91, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 49h:38m:37s remains)
INFO - root - 2017-12-06 10:21:10.617300: step 48690, loss = 0.77, batch loss = 0.56 (11.9 examples/sec; 0.672 sec/batch; 52h:56m:39s remains)
INFO - root - 2017-12-06 10:21:17.138452: step 48700, loss = 0.82, batch loss = 0.60 (12.2 examples/sec; 0.655 sec/batch; 51h:36m:30s remains)
2017-12-06 10:21:17.794790: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1646392 0.20582187 0.077030115 -0.16435134 -0.40346557 -0.49456468 -0.4730022 -0.5797714 -0.69602847 -0.70265979 -0.61573005 -0.55920738 -0.39436272 -0.25317863 -0.15137669][-0.073939152 -0.06781882 -0.040653169 -0.11095432 -0.38353193 -0.75064021 -0.95505351 -0.94167191 -0.9306879 -0.9736014 -0.94522148 -0.7756393 -0.50996184 -0.30298784 -0.1769976][-0.32476464 -0.19628945 -0.11255465 -0.058123462 -0.11066839 -0.44139916 -0.912584 -1.1766144 -1.1824023 -1.1736745 -1.1685044 -0.98357362 -0.55359763 -0.28673071 -0.20420532][-0.52226055 -0.44330928 -0.1312592 0.026360184 0.076508947 0.0040855557 -0.24843904 -0.71970272 -1.1536809 -1.3011656 -1.2045175 -1.0078856 -0.700105 -0.3341971 -0.097862616][-0.63697594 -0.58087683 -0.25958735 0.31426442 0.75480253 0.79452771 0.51321489 0.014458448 -0.64929849 -1.2110084 -1.312098 -1.0659263 -0.68282223 -0.39696777 -0.1798338][-0.44235027 -0.37523761 0.014587991 0.54957479 1.2564969 1.6714123 1.477578 0.85914588 0.13057888 -0.54098904 -1.0856525 -1.166896 -0.78059322 -0.41096535 -0.16391313][-0.25723255 -0.15772817 0.26470071 1.1187383 2.0343742 2.447237 2.4839718 1.8841622 0.92591739 -0.026639774 -0.58922756 -0.75697982 -0.73829383 -0.44867113 -0.13083574][-0.19900766 -0.18125311 0.17040327 0.79578459 1.6510612 2.5272951 2.7820969 2.2622409 1.4521585 0.4276613 -0.34882331 -0.69456571 -0.578945 -0.31985661 -0.079361558][-0.10259008 -0.021493137 0.16467249 0.53217965 1.148374 1.7501163 2.1710279 2.0804605 1.4764845 0.54173028 -0.2437287 -0.65809512 -0.58796394 -0.36909968 -0.10729744][-0.18159416 -0.14957157 0.023317456 0.29807428 0.67028266 1.026284 1.3393945 1.4010807 1.109643 0.29682857 -0.47260097 -0.75250673 -0.76307458 -0.50337428 -0.075218067][-0.18084973 -0.24160916 -0.17066097 -0.012983903 0.10913185 0.25127628 0.31732425 0.287872 0.1414732 -0.20157705 -0.67554921 -1.0151944 -1.0057653 -0.65341055 -0.27370596][-0.31466356 -0.41428024 -0.36267284 -0.26483411 -0.18234819 -0.084722988 -0.094658554 -0.32011524 -0.73427725 -1.1513696 -1.3840078 -1.3255194 -1.0707896 -0.79342103 -0.53961688][-0.34333917 -0.45539129 -0.4422904 -0.35261333 -0.13299882 0.04938934 0.022176757 -0.17830381 -0.65593773 -1.1764995 -1.5555148 -1.6388544 -1.3412516 -0.9081077 -0.57265538][-0.36620802 -0.40401307 -0.314316 -0.16975448 0.080019988 0.45039606 0.62307048 0.23939681 -0.33014807 -0.86815721 -1.4056149 -1.3828131 -1.1311108 -0.85620815 -0.60542053][-0.31687421 -0.27582484 -0.078759477 0.15913203 0.4957304 0.83444607 0.86262834 0.70191437 0.29362631 -0.40493521 -0.90468776 -1.0055537 -0.94884455 -0.698166 -0.44546962]]...]
INFO - root - 2017-12-06 10:21:24.274117: step 48710, loss = 0.88, batch loss = 0.66 (14.1 examples/sec; 0.568 sec/batch; 44h:44m:56s remains)
INFO - root - 2017-12-06 10:21:30.703404: step 48720, loss = 0.79, batch loss = 0.58 (12.1 examples/sec; 0.664 sec/batch; 52h:19m:28s remains)
INFO - root - 2017-12-06 10:21:37.172406: step 48730, loss = 0.90, batch loss = 0.68 (12.1 examples/sec; 0.660 sec/batch; 52h:00m:26s remains)
INFO - root - 2017-12-06 10:21:43.648284: step 48740, loss = 0.91, batch loss = 0.70 (12.1 examples/sec; 0.661 sec/batch; 52h:05m:43s remains)
INFO - root - 2017-12-06 10:21:50.131867: step 48750, loss = 0.77, batch loss = 0.56 (12.2 examples/sec; 0.654 sec/batch; 51h:34m:02s remains)
INFO - root - 2017-12-06 10:21:56.670310: step 48760, loss = 0.82, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 52h:09m:16s remains)
INFO - root - 2017-12-06 10:22:03.098701: step 48770, loss = 0.81, batch loss = 0.59 (12.1 examples/sec; 0.663 sec/batch; 52h:14m:01s remains)
INFO - root - 2017-12-06 10:22:09.566597: step 48780, loss = 0.75, batch loss = 0.53 (12.8 examples/sec; 0.625 sec/batch; 49h:17m:13s remains)
INFO - root - 2017-12-06 10:22:16.061677: step 48790, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 53h:18m:27s remains)
INFO - root - 2017-12-06 10:22:22.348548: step 48800, loss = 0.75, batch loss = 0.54 (12.9 examples/sec; 0.622 sec/batch; 49h:02m:07s remains)
2017-12-06 10:22:22.981753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.12646678 -0.18014047 -0.28312275 -0.41965663 -0.56463653 -0.56004643 -0.43162209 -0.23987938 0.019236252 0.27952796 0.39922702 0.33488011 0.16823794 -0.078511514 -0.27350116][-0.1553514 -0.22428617 -0.35911155 -0.56952327 -0.8313272 -0.98452306 -1.0367124 -1.0311989 -0.92705911 -0.72695035 -0.54887325 -0.40649825 -0.32241592 -0.33541024 -0.34909332][-0.20200679 -0.26567149 -0.37564206 -0.54259664 -0.77581018 -0.93521351 -1.0439538 -1.1706716 -1.2484933 -1.2052948 -1.0928277 -0.88154256 -0.63196218 -0.4547022 -0.3089323][-0.264192 -0.33261907 -0.413548 -0.456284 -0.52548885 -0.54842263 -0.63075382 -0.81924826 -1.0509015 -1.2311277 -1.3058711 -1.1652025 -0.86677051 -0.61504942 -0.39586782][-0.29295295 -0.33610478 -0.34463823 -0.24661854 -0.095409513 0.14574179 0.22907643 0.045937836 -0.34084839 -0.72670829 -1.0230566 -1.0571058 -0.85351831 -0.63408279 -0.41838843][-0.2230216 -0.21794528 -0.11932889 0.15260591 0.52340084 0.98254979 1.2526896 1.1846485 0.74883217 0.18332748 -0.34375447 -0.58861071 -0.55048913 -0.43743211 -0.3114655][-0.11550583 -0.054150462 0.12378396 0.51505816 1.0559006 1.672071 2.0540557 2.00505 1.5799947 0.98818207 0.36649036 -0.049436029 -0.16250987 -0.16614856 -0.17494863][-0.059239376 0.042417094 0.26000869 0.67489487 1.2561643 1.9757751 2.4998281 2.5564239 2.1823392 1.584612 0.94082737 0.42954522 0.16170014 0.021808602 -0.074924983][-0.081835993 -0.01558134 0.20055045 0.597138 1.0949535 1.7303728 2.2328608 2.3623834 2.0995073 1.5830925 0.98086238 0.47457182 0.21759982 0.0891169 0.00096247345][-0.16805451 -0.19654679 -0.11989655 0.12825561 0.49439406 0.96956372 1.3676819 1.5273622 1.367366 1.0565199 0.681593 0.26490283 0.046286047 0.001398094 -0.055028863][-0.2776745 -0.38476932 -0.431127 -0.38581669 -0.27370188 -0.033719666 0.25049514 0.46976978 0.45057988 0.30758709 0.13184178 -0.072672 -0.14862475 -0.13503274 -0.13026772][-0.29624337 -0.44703048 -0.58763415 -0.67531157 -0.71822917 -0.68849856 -0.59481859 -0.46584213 -0.40503222 -0.3290754 -0.32373154 -0.33643866 -0.29562938 -0.22997348 -0.17804977][-0.23821205 -0.36243188 -0.49832004 -0.62747335 -0.77663708 -0.87300807 -0.86155444 -0.77671945 -0.7480067 -0.64360839 -0.53261441 -0.40913659 -0.29693615 -0.20484661 -0.13875629][-0.1639163 -0.23868464 -0.32086986 -0.36557859 -0.47216618 -0.59386712 -0.66887903 -0.66059422 -0.64209145 -0.59865308 -0.50778657 -0.35820991 -0.22826231 -0.1661438 -0.12300837][-0.11764495 -0.14560854 -0.18220556 -0.18772151 -0.18679397 -0.18022671 -0.20019904 -0.2287254 -0.25109115 -0.22845669 -0.22783446 -0.21921447 -0.13691536 -0.11859663 -0.11226266]]...]
INFO - root - 2017-12-06 10:22:29.329232: step 48810, loss = 0.84, batch loss = 0.62 (11.9 examples/sec; 0.673 sec/batch; 53h:03m:54s remains)
INFO - root - 2017-12-06 10:22:35.739563: step 48820, loss = 0.90, batch loss = 0.68 (12.4 examples/sec; 0.647 sec/batch; 50h:59m:10s remains)
INFO - root - 2017-12-06 10:22:42.261380: step 48830, loss = 0.84, batch loss = 0.63 (12.3 examples/sec; 0.649 sec/batch; 51h:09m:57s remains)
INFO - root - 2017-12-06 10:22:48.738290: step 48840, loss = 0.77, batch loss = 0.56 (12.5 examples/sec; 0.641 sec/batch; 50h:30m:26s remains)
INFO - root - 2017-12-06 10:22:55.219178: step 48850, loss = 0.80, batch loss = 0.59 (11.9 examples/sec; 0.673 sec/batch; 53h:02m:09s remains)
INFO - root - 2017-12-06 10:23:01.633439: step 48860, loss = 0.79, batch loss = 0.58 (12.7 examples/sec; 0.630 sec/batch; 49h:40m:26s remains)
INFO - root - 2017-12-06 10:23:08.120301: step 48870, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.642 sec/batch; 50h:33m:54s remains)
INFO - root - 2017-12-06 10:23:14.617699: step 48880, loss = 0.83, batch loss = 0.62 (12.0 examples/sec; 0.669 sec/batch; 52h:43m:12s remains)
INFO - root - 2017-12-06 10:23:21.140640: step 48890, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.643 sec/batch; 50h:37m:41s remains)
INFO - root - 2017-12-06 10:23:27.496363: step 48900, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 50h:56m:04s remains)
2017-12-06 10:23:28.148854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086210847 -0.080738544 -0.06315662 -0.041200504 -0.028724685 -0.014574014 0.028759591 0.054133035 0.037848242 0.053105257 0.09356498 0.1964134 0.2639693 0.3218725 0.30084667][-0.085893348 -0.078632131 -0.063799769 -0.046713885 -0.027948655 0.019047774 0.077837072 0.053750686 -0.000758104 -0.051503845 -0.0810136 -0.047168124 0.020454437 0.03398107 -0.04463518][-0.078603983 -0.080598578 -0.083577022 -0.058519185 -0.036406856 0.021560654 0.099317037 0.10349729 0.045803972 -0.055827249 -0.12364309 -0.14501391 -0.14374049 -0.17581174 -0.25052893][-0.07666938 -0.080938339 -0.097442813 -0.10832139 -0.085758291 -0.0021782964 0.10674898 0.14183441 0.10837693 0.037350066 -0.04872705 -0.15696451 -0.24843267 -0.31615472 -0.47262409][-0.072845936 -0.066614531 -0.0856963 -0.089956254 -0.050974749 0.056315251 0.19645214 0.23825279 0.18424928 0.079295509 -0.0019848645 -0.10439134 -0.18532994 -0.27289784 -0.4113068][-0.068501189 -0.047390513 -0.038739741 -0.016882844 0.048119433 0.18094385 0.34200943 0.417928 0.36716962 0.24186102 0.10491846 -0.018175043 -0.099931523 -0.18332291 -0.29623994][-0.061231755 -0.029071871 0.018267728 0.077598207 0.17054102 0.3282997 0.47926846 0.58015925 0.54797977 0.43933544 0.2808255 0.1142971 -0.026417054 -0.12257649 -0.21526396][-0.057740923 -0.018226273 0.052134611 0.14095506 0.26761231 0.41702685 0.57162541 0.65033954 0.6068455 0.53548712 0.39676288 0.27052051 0.12420601 -0.0084244907 -0.12924701][-0.058363788 -0.012196049 0.0788236 0.18386865 0.32984707 0.4642925 0.59448552 0.62643337 0.59017438 0.52143472 0.39861879 0.33456275 0.21281859 0.1848098 0.16052487][-0.066823959 -0.03142653 0.037368141 0.13918632 0.27041718 0.40241611 0.53044623 0.56014729 0.49023947 0.40204528 0.31493321 0.25830746 0.18675685 0.19124562 0.23473272][-0.081141666 -0.062049523 -0.019901335 0.058379941 0.14400509 0.2652747 0.36896402 0.39029688 0.34669489 0.28278548 0.20208421 0.15485942 0.13719276 0.17003778 0.24736297][-0.086519539 -0.078822076 -0.055687863 -0.0067709535 0.07108169 0.16639757 0.23718908 0.23170546 0.16987351 0.11748693 0.076111086 0.074932314 0.058555476 0.10366108 0.13400191][-0.0980102 -0.091878206 -0.081739709 -0.058721442 -0.011452757 0.074427225 0.15160149 0.13717958 0.07208439 -0.0045876876 -0.0604884 -0.040653795 -0.040002376 0.0018603951 0.02914606][-0.09767659 -0.10566585 -0.12033778 -0.093590051 -0.061634347 0.032200716 0.13340446 0.15800571 0.0968013 0.0033242404 -0.05614455 -0.078204751 -0.11180644 -0.068454191 -0.045936756][-0.10620424 -0.11032163 -0.11445969 -0.11974171 -0.10605583 -0.018922515 0.09535078 0.15794623 0.146112 0.091551609 0.049363531 -0.0050434396 -0.054098554 -0.063891888 -0.15307529]]...]
INFO - root - 2017-12-06 10:23:34.558303: step 48910, loss = 0.85, batch loss = 0.63 (12.3 examples/sec; 0.651 sec/batch; 51h:15m:52s remains)
INFO - root - 2017-12-06 10:23:40.941007: step 48920, loss = 0.89, batch loss = 0.67 (12.5 examples/sec; 0.640 sec/batch; 50h:26m:46s remains)
INFO - root - 2017-12-06 10:23:47.434516: step 48930, loss = 0.86, batch loss = 0.65 (12.6 examples/sec; 0.637 sec/batch; 50h:11m:41s remains)
INFO - root - 2017-12-06 10:23:53.837948: step 48940, loss = 0.82, batch loss = 0.61 (11.9 examples/sec; 0.675 sec/batch; 53h:10m:17s remains)
INFO - root - 2017-12-06 10:24:00.341587: step 48950, loss = 0.76, batch loss = 0.54 (12.9 examples/sec; 0.621 sec/batch; 48h:54m:29s remains)
INFO - root - 2017-12-06 10:24:06.780576: step 48960, loss = 0.77, batch loss = 0.56 (12.3 examples/sec; 0.648 sec/batch; 51h:01m:37s remains)
INFO - root - 2017-12-06 10:24:13.284329: step 48970, loss = 0.78, batch loss = 0.56 (12.4 examples/sec; 0.644 sec/batch; 50h:43m:07s remains)
INFO - root - 2017-12-06 10:24:19.708068: step 48980, loss = 0.82, batch loss = 0.61 (12.7 examples/sec; 0.632 sec/batch; 49h:45m:11s remains)
INFO - root - 2017-12-06 10:24:26.220880: step 48990, loss = 0.78, batch loss = 0.57 (12.9 examples/sec; 0.619 sec/batch; 48h:42m:58s remains)
INFO - root - 2017-12-06 10:24:32.569571: step 49000, loss = 0.85, batch loss = 0.63 (12.1 examples/sec; 0.662 sec/batch; 52h:05m:40s remains)
2017-12-06 10:24:33.169951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062561549 -0.06985537 -0.077296227 -0.078513071 -0.08033549 -0.083780318 -0.090645947 -0.09240853 -0.097377963 -0.11309514 -0.12731639 -0.13615268 -0.1351905 -0.13580103 -0.13719875][-0.051000439 -0.051765453 -0.061912339 -0.070327446 -0.072410882 -0.081787437 -0.093603231 -0.10595866 -0.11699961 -0.13172105 -0.15278967 -0.17814854 -0.2020528 -0.21898055 -0.23614858][-0.036348436 -0.042524304 -0.056850314 -0.063096888 -0.06811659 -0.070366271 -0.076179385 -0.088417821 -0.099772133 -0.11136921 -0.12907505 -0.14901012 -0.17149423 -0.18470293 -0.19194242][-0.038926132 -0.048464574 -0.058831982 -0.058553964 -0.056509227 -0.055039626 -0.055177853 -0.060153242 -0.069823056 -0.0797842 -0.090534225 -0.099013187 -0.10814729 -0.11621565 -0.12103219][-0.05813428 -0.050786994 -0.048553124 -0.043022655 -0.039075121 -0.039962512 -0.046294421 -0.05595357 -0.065083027 -0.073415264 -0.081846215 -0.087931745 -0.09207432 -0.094295673 -0.093353227][-0.043082044 -0.037388291 -0.037859745 -0.0425796 -0.04881753 -0.055030756 -0.063056126 -0.072380096 -0.079811394 -0.082350649 -0.086606018 -0.082565993 -0.074707516 -0.070080221 -0.066261023][-0.033691477 -0.046146356 -0.067136571 -0.080846585 -0.084115304 -0.074531391 -0.057930551 -0.042334288 -0.039330006 -0.034937151 -0.033155058 -0.029746141 -0.031724654 -0.039873391 -0.044824176][-0.079339154 -0.081801049 -0.081257664 -0.073825113 -0.051980011 -0.022626974 0.0032973215 0.017968185 0.013533682 0.0027859211 -0.015206575 -0.032530691 -0.051170114 -0.070044726 -0.081719585][-0.095791943 -0.067163944 -0.0371927 -0.015576608 0.00047944486 0.00877817 -0.0022238418 -0.021087602 -0.046288192 -0.071510881 -0.08909221 -0.10188007 -0.10250999 -0.1039738 -0.10559035][-0.058721423 -0.030606415 -0.020134926 -0.02722124 -0.040754758 -0.051152088 -0.064677849 -0.072709709 -0.076877296 -0.081275724 -0.082379662 -0.075272463 -0.067648768 -0.066206783 -0.061290797][-0.049207713 -0.046896271 -0.055550065 -0.071650051 -0.080599941 -0.07656619 -0.066632614 -0.049929932 -0.035291612 -0.024815924 -0.02171281 -0.018564917 -0.021436393 -0.033743277 -0.043419361][-0.088744856 -0.090696178 -0.095478 -0.098337069 -0.0926976 -0.07715413 -0.063595518 -0.052787837 -0.047896069 -0.044060312 -0.047858309 -0.048212223 -0.050576881 -0.059259508 -0.069699883][-0.14092347 -0.13348603 -0.12540372 -0.11988981 -0.11317751 -0.10143664 -0.091396935 -0.085951313 -0.085061677 -0.083234042 -0.0795158 -0.07712952 -0.078095786 -0.084732458 -0.090141371][-0.14081664 -0.13510051 -0.13069031 -0.13036081 -0.13219056 -0.12659994 -0.11539472 -0.10159265 -0.091265537 -0.086155981 -0.082284257 -0.079693444 -0.080418028 -0.088550434 -0.094535805][-0.11970368 -0.12208106 -0.12553003 -0.127357 -0.12675625 -0.12070881 -0.11286896 -0.10248201 -0.096343577 -0.096449532 -0.096122973 -0.092207052 -0.084782556 -0.082678132 -0.082432717]]...]
INFO - root - 2017-12-06 10:24:39.611139: step 49010, loss = 0.83, batch loss = 0.61 (12.0 examples/sec; 0.664 sec/batch; 52h:18m:16s remains)
INFO - root - 2017-12-06 10:24:46.133513: step 49020, loss = 0.81, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 50h:56m:17s remains)
INFO - root - 2017-12-06 10:24:52.656397: step 49030, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.660 sec/batch; 51h:57m:03s remains)
INFO - root - 2017-12-06 10:24:59.152247: step 49040, loss = 0.82, batch loss = 0.60 (12.3 examples/sec; 0.649 sec/batch; 51h:04m:45s remains)
INFO - root - 2017-12-06 10:25:05.628726: step 49050, loss = 0.81, batch loss = 0.60 (12.0 examples/sec; 0.668 sec/batch; 52h:34m:59s remains)
INFO - root - 2017-12-06 10:25:12.079987: step 49060, loss = 0.77, batch loss = 0.56 (11.9 examples/sec; 0.670 sec/batch; 52h:44m:26s remains)
INFO - root - 2017-12-06 10:25:18.645578: step 49070, loss = 0.78, batch loss = 0.57 (11.6 examples/sec; 0.687 sec/batch; 54h:07m:10s remains)
INFO - root - 2017-12-06 10:25:25.054747: step 49080, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.626 sec/batch; 49h:17m:21s remains)
INFO - root - 2017-12-06 10:25:31.368933: step 49090, loss = 0.76, batch loss = 0.55 (12.7 examples/sec; 0.630 sec/batch; 49h:36m:50s remains)
INFO - root - 2017-12-06 10:25:37.834408: step 49100, loss = 0.78, batch loss = 0.57 (12.2 examples/sec; 0.655 sec/batch; 51h:35m:55s remains)
2017-12-06 10:25:38.507427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.053149804 -0.052221581 -0.051925037 -0.051999569 -0.052331876 -0.052676979 -0.052862778 -0.052846409 -0.052562244 -0.0521029 -0.051652685 -0.051441256 -0.051643334 -0.05219752 -0.053184122][-0.05172547 -0.050741 -0.05030955 -0.050201271 -0.050320841 -0.050551035 -0.05075857 -0.050928745 -0.05097476 -0.050872605 -0.050713159 -0.050689943 -0.050916426 -0.051385242 -0.052214418][-0.051575456 -0.050546836 -0.050156895 -0.050074689 -0.050256517 -0.050578468 -0.050867971 -0.051138353 -0.051288903 -0.051224992 -0.051031183 -0.05089058 -0.050923057 -0.051172327 -0.051757254][-0.051988669 -0.050875235 -0.050545115 -0.050512746 -0.050792903 -0.051286835 -0.051739626 -0.052118022 -0.052272461 -0.052157205 -0.0518372 -0.051527109 -0.051359348 -0.051404022 -0.051809531][-0.052747309 -0.0516256 -0.0512048 -0.051104665 -0.051404078 -0.051994309 -0.05254351 -0.052994907 -0.053183947 -0.053066395 -0.052724145 -0.05232852 -0.052059382 -0.051947 -0.052203003][-0.054439876 -0.052966677 -0.052219227 -0.051891834 -0.052072626 -0.05259122 -0.05311814 -0.053558737 -0.053758372 -0.053667516 -0.053353965 -0.052970532 -0.052697491 -0.052498847 -0.052719641][-0.057090864 -0.054847993 -0.05352949 -0.052779727 -0.052680004 -0.053003717 -0.053429477 -0.05380328 -0.053987481 -0.053907502 -0.053634446 -0.053299528 -0.05302931 -0.052809548 -0.053070158][-0.060172826 -0.056872644 -0.054729998 -0.0534315 -0.053020038 -0.053164911 -0.053494807 -0.053835757 -0.054015931 -0.053989656 -0.053791195 -0.053516023 -0.053266726 -0.053093143 -0.053457987][-0.063659079 -0.058977965 -0.05574182 -0.053842161 -0.05321832 -0.053277921 -0.053572617 -0.053912669 -0.05411233 -0.054142442 -0.0540182 -0.053789027 -0.053536858 -0.053367686 -0.05376346][-0.067157395 -0.060934484 -0.056594495 -0.054184753 -0.053393204 -0.053385198 -0.053643033 -0.0539476 -0.054135382 -0.054201823 -0.054131206 -0.053927455 -0.053651258 -0.053446557 -0.053784285][-0.070142984 -0.062716939 -0.057523329 -0.054690555 -0.053613078 -0.053461581 -0.053654153 -0.053870089 -0.054020051 -0.054095834 -0.054072153 -0.053911578 -0.053620249 -0.053395148 -0.053722519][-0.071801856 -0.063799836 -0.058059536 -0.054961268 -0.053722292 -0.053469934 -0.053598832 -0.05378693 -0.053921275 -0.054005045 -0.054032382 -0.0539252 -0.053652093 -0.053424843 -0.053698871][-0.071452081 -0.063641064 -0.057976414 -0.054891679 -0.053680271 -0.0534616 -0.053654719 -0.053888887 -0.054057177 -0.054164767 -0.054223027 -0.054149743 -0.053894375 -0.053616744 -0.053806666][-0.069686264 -0.062602349 -0.057557095 -0.054746158 -0.053643826 -0.053381089 -0.053668145 -0.053970657 -0.054192927 -0.0543428 -0.054433528 -0.054400411 -0.054207437 -0.053938724 -0.054069653][-0.067088418 -0.06135907 -0.057320897 -0.055030294 -0.05399489 -0.053608947 -0.053838219 -0.054132421 -0.054391604 -0.054526698 -0.054612305 -0.054598935 -0.054474697 -0.054332603 -0.054547206]]...]
INFO - root - 2017-12-06 10:25:44.983199: step 49110, loss = 0.80, batch loss = 0.59 (12.4 examples/sec; 0.647 sec/batch; 50h:56m:25s remains)
INFO - root - 2017-12-06 10:25:51.523009: step 49120, loss = 0.83, batch loss = 0.61 (12.5 examples/sec; 0.640 sec/batch; 50h:24m:53s remains)
INFO - root - 2017-12-06 10:25:58.002557: step 49130, loss = 0.83, batch loss = 0.62 (11.9 examples/sec; 0.672 sec/batch; 52h:55m:33s remains)
INFO - root - 2017-12-06 10:26:04.515929: step 49140, loss = 0.76, batch loss = 0.55 (12.1 examples/sec; 0.660 sec/batch; 51h:55m:22s remains)
INFO - root - 2017-12-06 10:26:10.956497: step 49150, loss = 0.82, batch loss = 0.60 (12.4 examples/sec; 0.645 sec/batch; 50h:44m:25s remains)
INFO - root - 2017-12-06 10:26:17.473737: step 49160, loss = 0.85, batch loss = 0.63 (11.8 examples/sec; 0.680 sec/batch; 53h:31m:35s remains)
INFO - root - 2017-12-06 10:26:23.910161: step 49170, loss = 0.76, batch loss = 0.55 (12.7 examples/sec; 0.632 sec/batch; 49h:43m:08s remains)
INFO - root - 2017-12-06 10:26:30.305533: step 49180, loss = 0.85, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:09m:31s remains)
INFO - root - 2017-12-06 10:26:36.828255: step 49190, loss = 0.83, batch loss = 0.61 (11.6 examples/sec; 0.688 sec/batch; 54h:09m:01s remains)
INFO - root - 2017-12-06 10:26:43.316243: step 49200, loss = 0.84, batch loss = 0.62 (12.3 examples/sec; 0.651 sec/batch; 51h:13m:58s remains)
2017-12-06 10:26:44.027076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.070790209 -0.070589647 -0.070717417 -0.070846424 -0.070926934 -0.0709248 -0.070880376 -0.070851758 -0.070765756 -0.070599534 -0.070449755 -0.070431769 -0.07049837 -0.070563406 -0.070730075][-0.067892246 -0.067688964 -0.067954 -0.068255126 -0.068458617 -0.068507932 -0.068489186 -0.068504132 -0.068429925 -0.0681691 -0.067864507 -0.06771417 -0.067697793 -0.067674495 -0.067722335][-0.066616654 -0.066481076 -0.067003153 -0.067650959 -0.068111986 -0.068266124 -0.068285614 -0.068320125 -0.068193235 -0.067686826 -0.067108937 -0.066721305 -0.066555269 -0.066393808 -0.066262349][-0.06615597 -0.066202119 -0.0670714 -0.068252027 -0.06918069 -0.06970033 -0.06990096 -0.069892012 -0.069557607 -0.0686287 -0.067571871 -0.066776834 -0.066372558 -0.066063337 -0.065781325][-0.066083625 -0.066359125 -0.067616522 -0.069348276 -0.070897818 -0.071991742 -0.07249476 -0.072437927 -0.071835816 -0.070397452 -0.0687394 -0.067269593 -0.066452459 -0.06602107 -0.065655671][-0.066152066 -0.066552214 -0.068095781 -0.070340395 -0.0724833 -0.07418377 -0.075055607 -0.075103395 -0.07440915 -0.072528049 -0.070241868 -0.068034992 -0.066686541 -0.066050626 -0.06561666][-0.066334821 -0.066802926 -0.06858179 -0.071211427 -0.073867038 -0.076144055 -0.077431932 -0.0776572 -0.076892518 -0.074688628 -0.071872756 -0.06898322 -0.067032456 -0.066055454 -0.065517321][-0.066412494 -0.066872865 -0.068787552 -0.071634509 -0.074650869 -0.077326074 -0.078947172 -0.07926286 -0.078458659 -0.076132432 -0.07301604 -0.069678485 -0.067194313 -0.065957218 -0.065322094][-0.066400915 -0.066775858 -0.068624772 -0.07142584 -0.074550495 -0.077331804 -0.079048723 -0.07937064 -0.078586295 -0.076380715 -0.073246688 -0.069814183 -0.067142993 -0.065779455 -0.06510523][-0.066169843 -0.066404246 -0.067985848 -0.07043869 -0.073320538 -0.075976215 -0.077536531 -0.07779181 -0.077035755 -0.075198494 -0.072461069 -0.06934537 -0.066891134 -0.065592952 -0.064981438][-0.066061825 -0.066133864 -0.067375191 -0.069303669 -0.071650237 -0.073843449 -0.075180121 -0.075319678 -0.074602075 -0.073134422 -0.071017168 -0.068565145 -0.06661018 -0.0654853 -0.064917959][-0.065847643 -0.06583491 -0.06685666 -0.068271562 -0.069999948 -0.071665749 -0.072786056 -0.072800167 -0.072011709 -0.070779145 -0.069253251 -0.067519315 -0.06606321 -0.065179162 -0.064792223][-0.065749839 -0.065655455 -0.066425711 -0.0673926 -0.068521269 -0.069678359 -0.07042595 -0.070299327 -0.069541924 -0.068522677 -0.067501649 -0.066374578 -0.065437213 -0.06487149 -0.064671338][-0.065870978 -0.065817691 -0.066454887 -0.067076609 -0.067715585 -0.068413839 -0.068831727 -0.068593211 -0.067856915 -0.06697657 -0.066231281 -0.065522268 -0.06495712 -0.06464576 -0.064595677][-0.066164717 -0.0663133 -0.066994078 -0.06748084 -0.067805514 -0.068145566 -0.0683213 -0.067950509 -0.067164443 -0.066347972 -0.065787286 -0.065316819 -0.064914346 -0.064702474 -0.064748757]]...]
INFO - root - 2017-12-06 10:26:50.556189: step 49210, loss = 0.80, batch loss = 0.58 (11.8 examples/sec; 0.679 sec/batch; 53h:23m:34s remains)
INFO - root - 2017-12-06 10:26:56.961737: step 49220, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.661 sec/batch; 52h:03m:04s remains)
INFO - root - 2017-12-06 10:27:03.519798: step 49230, loss = 0.83, batch loss = 0.62 (12.0 examples/sec; 0.666 sec/batch; 52h:23m:19s remains)
INFO - root - 2017-12-06 10:27:09.987830: step 49240, loss = 0.71, batch loss = 0.49 (12.5 examples/sec; 0.639 sec/batch; 50h:17m:03s remains)
INFO - root - 2017-12-06 10:27:16.482204: step 49250, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.646 sec/batch; 50h:47m:57s remains)
INFO - root - 2017-12-06 10:27:22.943565: step 49260, loss = 0.84, batch loss = 0.63 (12.4 examples/sec; 0.643 sec/batch; 50h:35m:59s remains)
INFO - root - 2017-12-06 10:27:29.392054: step 49270, loss = 0.87, batch loss = 0.66 (13.0 examples/sec; 0.615 sec/batch; 48h:23m:48s remains)
INFO - root - 2017-12-06 10:27:35.857981: step 49280, loss = 0.76, batch loss = 0.54 (12.3 examples/sec; 0.650 sec/batch; 51h:08m:36s remains)
INFO - root - 2017-12-06 10:27:42.379213: step 49290, loss = 0.78, batch loss = 0.57 (12.1 examples/sec; 0.660 sec/batch; 51h:52m:58s remains)
INFO - root - 2017-12-06 10:27:48.949308: step 49300, loss = 0.72, batch loss = 0.51 (12.6 examples/sec; 0.635 sec/batch; 49h:58m:32s remains)
2017-12-06 10:27:49.565780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.081034318 -0.081832536 -0.085056171 -0.0920432 -0.10104389 -0.10705464 -0.10986598 -0.11100579 -0.12327218 -0.18937552 -0.28808522 -0.30966786 -0.17755963 0.12888294 0.576326][-0.082044058 -0.083475821 -0.087639973 -0.094085895 -0.095550694 -0.086374827 -0.068749428 -0.049041033 -0.043466166 -0.10183395 -0.22984946 -0.33530533 -0.35287589 -0.20841524 0.10315406][-0.082917318 -0.084439121 -0.088355258 -0.093429431 -0.089331679 -0.071109757 -0.040554479 -0.0035663396 0.024785966 -0.0003991127 -0.10186575 -0.2372199 -0.33700082 -0.31850168 -0.12506893][-0.083634064 -0.08502762 -0.088208638 -0.088501155 -0.076307431 -0.050866175 -0.019854754 0.0082939491 0.020955421 0.0069395155 -0.053164013 -0.16266236 -0.2883698 -0.36852384 -0.3267675][-0.0837643 -0.084536448 -0.0872319 -0.087795675 -0.078427695 -0.062025324 -0.04353008 -0.033577818 -0.039343368 -0.054688968 -0.080723666 -0.14455689 -0.25272149 -0.36007693 -0.39243472][-0.083394662 -0.08385051 -0.087352008 -0.089834988 -0.086987033 -0.080972046 -0.07391113 -0.068048261 -0.07799913 -0.093263015 -0.10002804 -0.1258616 -0.18845484 -0.26791516 -0.31946445][-0.083233431 -0.082714729 -0.0850196 -0.087358043 -0.083581343 -0.072119281 -0.056912813 -0.046144623 -0.050993998 -0.067771517 -0.076331586 -0.086441614 -0.1113802 -0.14892556 -0.19092277][-0.083165139 -0.0829099 -0.084038779 -0.086598761 -0.087195888 -0.080934405 -0.067751594 -0.052514318 -0.047532924 -0.056215137 -0.065796316 -0.076116875 -0.0867 -0.094016537 -0.10603977][-0.083147787 -0.082932658 -0.083499335 -0.08474537 -0.085941695 -0.085603386 -0.080473654 -0.071422048 -0.065347113 -0.066506349 -0.070234552 -0.0727417 -0.077812277 -0.08410731 -0.089016251][-0.083397 -0.083110154 -0.08328516 -0.084173 -0.085379235 -0.086268835 -0.084865957 -0.079991214 -0.073692426 -0.07298933 -0.07445541 -0.073079921 -0.072182044 -0.069749832 -0.075844027][-0.083628118 -0.083460227 -0.083337843 -0.083284594 -0.08317861 -0.082516357 -0.080292113 -0.076628536 -0.072426014 -0.071925916 -0.074273139 -0.073280588 -0.074143574 -0.076192439 -0.0826837][-0.083747692 -0.083718613 -0.0837199 -0.084269322 -0.084932648 -0.084760129 -0.083083771 -0.080148332 -0.076207787 -0.075238936 -0.074119747 -0.070234515 -0.068812728 -0.07484708 -0.084289506][-0.083454713 -0.0833726 -0.08348161 -0.08419463 -0.085654549 -0.0883066 -0.090915821 -0.091726772 -0.089434341 -0.085781574 -0.080783986 -0.070672281 -0.06733612 -0.068753108 -0.075598136][-0.083328307 -0.083120607 -0.083077595 -0.083239883 -0.083472468 -0.084105037 -0.084400684 -0.084838189 -0.084980562 -0.08453282 -0.082600668 -0.077990592 -0.074412823 -0.071712255 -0.0759841][-0.083441526 -0.083279625 -0.083246879 -0.08317738 -0.083261743 -0.08331272 -0.083535232 -0.083720125 -0.084095277 -0.084247462 -0.083354689 -0.082222462 -0.080574848 -0.080781221 -0.082772017]]...]
INFO - root - 2017-12-06 10:27:56.056982: step 49310, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.656 sec/batch; 51h:34m:47s remains)
INFO - root - 2017-12-06 10:28:02.661572: step 49320, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.643 sec/batch; 50h:33m:34s remains)
INFO - root - 2017-12-06 10:28:09.249612: step 49330, loss = 0.78, batch loss = 0.56 (11.9 examples/sec; 0.671 sec/batch; 52h:49m:08s remains)
INFO - root - 2017-12-06 10:28:15.693850: step 49340, loss = 0.87, batch loss = 0.65 (12.4 examples/sec; 0.645 sec/batch; 50h:43m:38s remains)
INFO - root - 2017-12-06 10:28:22.221528: step 49350, loss = 0.80, batch loss = 0.59 (12.1 examples/sec; 0.660 sec/batch; 51h:54m:44s remains)
INFO - root - 2017-12-06 10:28:28.709573: step 49360, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.649 sec/batch; 51h:03m:28s remains)
INFO - root - 2017-12-06 10:28:35.262004: step 49370, loss = 0.82, batch loss = 0.61 (12.2 examples/sec; 0.658 sec/batch; 51h:43m:38s remains)
INFO - root - 2017-12-06 10:28:41.732194: step 49380, loss = 0.86, batch loss = 0.64 (12.5 examples/sec; 0.640 sec/batch; 50h:19m:21s remains)
INFO - root - 2017-12-06 10:28:48.240868: step 49390, loss = 0.83, batch loss = 0.62 (12.7 examples/sec; 0.629 sec/batch; 49h:26m:48s remains)
INFO - root - 2017-12-06 10:28:54.832584: step 49400, loss = 0.81, batch loss = 0.60 (12.6 examples/sec; 0.636 sec/batch; 50h:00m:25s remains)
2017-12-06 10:28:55.548583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038059372 -0.037729386 -0.03759544 -0.037413646 -0.037136555 -0.03689345 -0.036755882 -0.036862727 -0.037176955 -0.037730742 -0.038646113 -0.039873876 -0.041288886 -0.042705324 -0.044545561][-0.033502389 -0.0329924 -0.032760888 -0.032492608 -0.032113373 -0.031857971 -0.031789526 -0.032003872 -0.032434478 -0.033223346 -0.03447384 -0.036174238 -0.038189303 -0.040289849 -0.042808186][-0.030068927 -0.029308535 -0.028993491 -0.028591752 -0.028090276 -0.027718574 -0.027626462 -0.02784691 -0.028304093 -0.029206235 -0.030687202 -0.032813534 -0.035417251 -0.038218517 -0.041429374][-0.027648136 -0.026874438 -0.026501216 -0.025959037 -0.025285363 -0.024680533 -0.02433376 -0.024381161 -0.024777822 -0.025750078 -0.027426839 -0.029926978 -0.0331139 -0.036633555 -0.040532067][-0.026226014 -0.0256048 -0.025229037 -0.024551988 -0.023649633 -0.022682689 -0.021951981 -0.021731563 -0.021998979 -0.023016937 -0.024875246 -0.027784504 -0.031517237 -0.035643768 -0.040114656][-0.02629599 -0.025704734 -0.025394626 -0.024630934 -0.023578547 -0.022389628 -0.02137211 -0.020890027 -0.021025315 -0.022107765 -0.024154276 -0.027318701 -0.031294692 -0.035657857 -0.040560253][-0.027625158 -0.026973672 -0.026655167 -0.025838248 -0.024685375 -0.023328163 -0.022116326 -0.021325186 -0.021349147 -0.022443846 -0.024607152 -0.027857341 -0.031873133 -0.036258988 -0.041289322][-0.029480513 -0.028637748 -0.028206337 -0.027345873 -0.026232325 -0.024930917 -0.023696586 -0.022744507 -0.022729851 -0.023773104 -0.025897577 -0.029050753 -0.032953769 -0.037267976 -0.042124391][-0.031514794 -0.030481003 -0.030016199 -0.029200319 -0.028239809 -0.027168781 -0.026141904 -0.025290698 -0.025161527 -0.025953442 -0.027826294 -0.030735131 -0.034376398 -0.038413294 -0.042877238][-0.03363112 -0.032628898 -0.032172907 -0.03149673 -0.030726127 -0.030027229 -0.029369105 -0.028728381 -0.02853791 -0.029031944 -0.0305458 -0.033015251 -0.036140427 -0.039692894 -0.04362667][-0.03622333 -0.035071097 -0.03460373 -0.034039561 -0.033449639 -0.033017617 -0.032639049 -0.032153375 -0.031959131 -0.03224241 -0.033352692 -0.035313096 -0.037842523 -0.040851075 -0.044298392][-0.038781807 -0.037550766 -0.03705461 -0.036545515 -0.03605796 -0.035714045 -0.035418086 -0.034992415 -0.03474405 -0.034832619 -0.03560276 -0.037103187 -0.039148074 -0.041815028 -0.044882976][-0.041153729 -0.040008418 -0.039524987 -0.039070237 -0.038639955 -0.038260859 -0.037816986 -0.037252404 -0.036800209 -0.03669652 -0.037192952 -0.038340837 -0.040062547 -0.042444162 -0.045267057][-0.04331984 -0.04232512 -0.041901011 -0.041491114 -0.041085362 -0.040613949 -0.039984606 -0.039235856 -0.038575549 -0.038289391 -0.038547218 -0.039452013 -0.040914908 -0.043071683 -0.045583468][-0.045087922 -0.0442019 -0.043908607 -0.043609153 -0.043246005 -0.042737354 -0.042030655 -0.041183267 -0.0403914 -0.0399257 -0.04000324 -0.040736619 -0.041975293 -0.043845963 -0.046076022]]...]
INFO - root - 2017-12-06 10:29:02.074705: step 49410, loss = 0.80, batch loss = 0.59 (12.3 examples/sec; 0.651 sec/batch; 51h:09m:39s remains)
INFO - root - 2017-12-06 10:29:08.547276: step 49420, loss = 0.83, batch loss = 0.61 (12.3 examples/sec; 0.650 sec/batch; 51h:08m:43s remains)
INFO - root - 2017-12-06 10:29:15.003968: step 49430, loss = 0.80, batch loss = 0.58 (11.8 examples/sec; 0.677 sec/batch; 53h:14m:23s remains)
INFO - root - 2017-12-06 10:29:21.513294: step 49440, loss = 0.81, batch loss = 0.60 (12.2 examples/sec; 0.657 sec/batch; 51h:41m:31s remains)
INFO - root - 2017-12-06 10:29:28.073565: step 49450, loss = 0.83, batch loss = 0.61 (12.1 examples/sec; 0.662 sec/batch; 52h:01m:59s remains)
INFO - root - 2017-12-06 10:29:34.424174: step 49460, loss = 0.76, batch loss = 0.54 (12.6 examples/sec; 0.633 sec/batch; 49h:44m:46s remains)
INFO - root - 2017-12-06 10:29:40.977210: step 49470, loss = 0.74, batch loss = 0.53 (12.4 examples/sec; 0.644 sec/batch; 50h:36m:41s remains)
INFO - root - 2017-12-06 10:29:47.517686: step 49480, loss = 0.81, batch loss = 0.60 (11.9 examples/sec; 0.671 sec/batch; 52h:44m:42s remains)
INFO - root - 2017-12-06 10:29:53.978697: step 49490, loss = 0.85, batch loss = 0.63 (13.0 examples/sec; 0.616 sec/batch; 48h:27m:21s remains)
INFO - root - 2017-12-06 10:30:00.273054: step 49500, loss = 0.77, batch loss = 0.56 (12.1 examples/sec; 0.663 sec/batch; 52h:05m:13s remains)
2017-12-06 10:30:00.929670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6678016 -2.7916825 -2.8666465 -2.7915704 -2.5990317 -2.3835075 -2.3621764 -2.0434084 -1.5284721 -1.0407937 -0.8153019 -0.85092747 -0.93519491 -0.64723575 -0.42968559][-1.5041697 -2.2290115 -2.9004014 -2.8575974 -2.7927279 -2.466459 -2.3612061 -2.2863946 -2.199229 -1.8656381 -1.6119554 -1.108026 -0.73505032 -0.39118475 -0.05778154][-1.5329012 -1.6559551 -1.9679728 -2.4274724 -2.7944908 -2.26412 -1.8494388 -1.9083823 -2.0432348 -1.9915067 -1.9448386 -1.8036995 -1.603386 -1.1026317 -0.71454382][-1.0446324 -1.4223469 -1.8921056 -2.0353777 -2.0147116 -2.1003153 -2.1567385 -2.0974302 -2.0657096 -2.1041336 -2.1053543 -2.0253692 -1.7646328 -1.3764775 -1.1318253][-0.29821676 -0.51354438 -0.63475943 -0.8850885 -0.99725324 -0.8736037 -0.84196192 -1.2944394 -1.6010367 -1.6050051 -1.6771733 -1.7953689 -1.8505609 -1.792729 -1.5287899][0.695318 0.6667819 0.40758941 0.21038675 0.15515587 0.1296185 0.14934281 0.037913851 -0.15054509 -0.705434 -1.1654751 -1.1785045 -1.2616616 -1.3362824 -1.3644229][1.6228462 1.4576397 1.3014693 1.2678691 1.2194769 1.0409139 0.9008069 1.0106729 1.012032 0.63687193 0.30802867 -0.39327884 -1.0514628 -1.1877133 -1.185963][1.6634192 1.5836539 1.5148385 1.3255768 1.2564747 1.2342259 1.1570326 1.1770889 1.3531272 1.4111933 1.3333683 0.62017184 0.12491984 -0.41445196 -0.89655113][0.27614859 0.30283922 0.29836631 0.18755415 0.38629827 0.77142167 0.93643588 0.96887928 1.0439967 1.1722956 1.2754731 1.1700875 1.0633491 0.64211625 0.29304954][-2.2365139 -2.0514374 -1.8838097 -1.6307058 -1.3180995 -1.0037934 -0.64200616 -0.28732085 -0.1477939 0.11328121 0.49165934 0.8079828 0.86816067 0.87494349 0.82597882][-2.9398229 -3.2029953 -3.122925 -3.0487702 -2.7821364 -2.3670986 -1.8162571 -1.4538392 -1.0991762 -0.76442695 -0.55868149 -0.14853838 0.34737992 0.66510183 0.79832065][-2.6420889 -3.1603155 -3.3445821 -3.3031759 -3.1722462 -3.0965512 -2.9376683 -2.725384 -2.5437388 -1.9601924 -1.4980147 -1.0973383 -0.69192028 -0.10102117 0.35625029][-2.3078883 -2.3130493 -2.2803228 -2.460278 -2.4905176 -2.6509991 -2.6037295 -2.5636969 -2.4392557 -2.3027525 -2.1519134 -1.7699598 -1.2456423 -0.77355951 -0.51605952][-1.747116 -1.8571231 -1.922134 -2.0161803 -1.9397743 -1.958794 -2.0710716 -2.0815232 -1.9879912 -1.8469191 -1.7294418 -1.7011214 -1.5328523 -1.1425594 -0.97086686][-0.32583448 -0.61668038 -0.82424837 -0.97928429 -1.1602964 -1.2918289 -1.3629357 -1.3425134 -1.181221 -1.1523616 -1.1858622 -1.1178646 -1.1054732 -1.203758 -1.3140545]]...]
INFO - root - 2017-12-06 10:30:07.479182: step 49510, loss = 0.77, batch loss = 0.55 (12.2 examples/sec; 0.654 sec/batch; 51h:26m:00s remains)
INFO - root - 2017-12-06 10:30:13.993947: step 49520, loss = 0.82, batch loss = 0.61 (12.8 examples/sec; 0.627 sec/batch; 49h:18m:53s remains)
INFO - root - 2017-12-06 10:30:20.512307: step 49530, loss = 0.79, batch loss = 0.57 (12.5 examples/sec; 0.639 sec/batch; 50h:13m:17s remains)
INFO - root - 2017-12-06 10:30:27.092432: step 49540, loss = 0.84, batch loss = 0.62 (12.2 examples/sec; 0.655 sec/batch; 51h:31m:16s remains)
INFO - root - 2017-12-06 10:30:33.593276: step 49550, loss = 0.68, batch loss = 0.47 (12.0 examples/sec; 0.667 sec/batch; 52h:23m:58s remains)
INFO - root - 2017-12-06 10:30:39.994061: step 49560, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.651 sec/batch; 51h:08m:15s remains)
INFO - root - 2017-12-06 10:30:46.478847: step 49570, loss = 0.80, batch loss = 0.58 (12.5 examples/sec; 0.639 sec/batch; 50h:15m:33s remains)
INFO - root - 2017-12-06 10:30:52.895855: step 49580, loss = 0.78, batch loss = 0.57 (12.4 examples/sec; 0.647 sec/batch; 50h:50m:11s remains)
INFO - root - 2017-12-06 10:30:59.419825: step 49590, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.641 sec/batch; 50h:24m:13s remains)
INFO - root - 2017-12-06 10:31:05.941892: step 49600, loss = 0.88, batch loss = 0.66 (12.4 examples/sec; 0.643 sec/batch; 50h:32m:23s remains)
2017-12-06 10:31:06.567401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.09951558 -0.10498665 -0.10415103 -0.10361098 -0.10249801 -0.09722089 -0.094949909 -0.094221927 -0.093212329 -0.091437005 -0.085829258 -0.081764027 -0.077851914 -0.076124817 -0.075677335][-0.082902096 -0.075235605 -0.069765784 -0.06682647 -0.066555455 -0.069321 -0.077281162 -0.084554248 -0.088000745 -0.087652557 -0.085943915 -0.082987376 -0.080619335 -0.078597493 -0.077246085][-0.07314305 -0.072733432 -0.074768022 -0.073921248 -0.073240213 -0.075192459 -0.076790377 -0.082582437 -0.086502232 -0.089733258 -0.090246364 -0.0879333 -0.0847592 -0.08211384 -0.080201641][-0.06562613 -0.069894746 -0.072818816 -0.076477014 -0.078939423 -0.081109494 -0.0841604 -0.088245444 -0.091992952 -0.092966765 -0.092612408 -0.090285622 -0.087247014 -0.084154427 -0.082310185][-0.062001966 -0.066380367 -0.07207875 -0.081398144 -0.08769498 -0.093000464 -0.099047035 -0.10346028 -0.10653274 -0.10586727 -0.10277227 -0.097278476 -0.091884717 -0.087329917 -0.083828889][-0.068708152 -0.076945335 -0.084278211 -0.094765 -0.10146219 -0.10560184 -0.11122666 -0.11539893 -0.1157363 -0.11405355 -0.10804944 -0.1008217 -0.094815612 -0.089806236 -0.08684507][-0.074258462 -0.072277881 -0.077988893 -0.089799032 -0.10230715 -0.11563477 -0.12709077 -0.12830362 -0.12653616 -0.12314801 -0.11867964 -0.1086897 -0.10091195 -0.092154287 -0.08719442][-0.0848358 -0.080554537 -0.078455806 -0.08643204 -0.0987956 -0.11292015 -0.130328 -0.14138058 -0.14693969 -0.14506687 -0.13174672 -0.11847812 -0.10589934 -0.0961698 -0.089160554][-0.090975955 -0.090860739 -0.088564187 -0.085791148 -0.089893281 -0.10327622 -0.12268718 -0.13599271 -0.14173803 -0.14002226 -0.13054933 -0.11752144 -0.10788658 -0.096303932 -0.090782218][-0.094325893 -0.0938633 -0.091151722 -0.086504646 -0.082990237 -0.084016711 -0.097069278 -0.11084553 -0.12114085 -0.12197252 -0.1176696 -0.11014752 -0.099420667 -0.092131205 -0.086655773][-0.091790296 -0.0925881 -0.092260331 -0.087061226 -0.078228384 -0.07015264 -0.070098221 -0.081475362 -0.093771473 -0.099130221 -0.098514885 -0.093123704 -0.088639252 -0.085076235 -0.08419168][-0.083044991 -0.081302285 -0.077695645 -0.074344315 -0.068782471 -0.062083662 -0.059829384 -0.060997367 -0.066315278 -0.073387071 -0.078667939 -0.079794578 -0.080483571 -0.080161333 -0.079744138][-0.078335792 -0.074025549 -0.066280052 -0.059775271 -0.05313104 -0.047907189 -0.044667728 -0.046211883 -0.052931149 -0.063544437 -0.07215751 -0.0765604 -0.078386858 -0.0807782 -0.081526376][-0.0775467 -0.076427452 -0.072153404 -0.063692309 -0.053670112 -0.045629837 -0.041852586 -0.041143961 -0.043969464 -0.054792266 -0.066033259 -0.076521724 -0.079602048 -0.081382506 -0.081706926][-0.076947093 -0.075043306 -0.074080244 -0.07142204 -0.066120334 -0.057795268 -0.051015228 -0.050999887 -0.05563518 -0.062881663 -0.070602246 -0.076141581 -0.079805151 -0.080553271 -0.080762751]]...]
INFO - root - 2017-12-06 10:31:13.074272: step 49610, loss = 0.92, batch loss = 0.70 (12.0 examples/sec; 0.668 sec/batch; 52h:31m:36s remains)
INFO - root - 2017-12-06 10:31:19.583626: step 49620, loss = 0.82, batch loss = 0.61 (13.0 examples/sec; 0.614 sec/batch; 48h:14m:14s remains)
INFO - root - 2017-12-06 10:31:26.105559: step 49630, loss = 0.86, batch loss = 0.65 (12.5 examples/sec; 0.640 sec/batch; 50h:15m:14s remains)
INFO - root - 2017-12-06 10:31:32.410851: step 49640, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 51h:00m:59s remains)
INFO - root - 2017-12-06 10:31:38.836230: step 49650, loss = 0.84, batch loss = 0.63 (12.5 examples/sec; 0.639 sec/batch; 50h:11m:47s remains)
INFO - root - 2017-12-06 10:31:45.327159: step 49660, loss = 0.75, batch loss = 0.53 (12.7 examples/sec; 0.630 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-06 10:31:51.793937: step 49670, loss = 0.74, batch loss = 0.53 (12.0 examples/sec; 0.669 sec/batch; 52h:31m:35s remains)
INFO - root - 2017-12-06 10:31:58.273701: step 49680, loss = 0.87, batch loss = 0.65 (11.9 examples/sec; 0.674 sec/batch; 52h:58m:55s remains)
INFO - root - 2017-12-06 10:32:04.761838: step 49690, loss = 0.81, batch loss = 0.60 (12.4 examples/sec; 0.647 sec/batch; 50h:48m:23s remains)
INFO - root - 2017-12-06 10:32:11.427575: step 49700, loss = 0.74, batch loss = 0.53 (11.8 examples/sec; 0.678 sec/batch; 53h:14m:39s remains)
2017-12-06 10:32:12.109716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.093481146 -0.10194537 -0.12099998 -0.15803781 -0.20607629 -0.25603589 -0.29793674 -0.32243291 -0.32538506 -0.30175865 -0.25962785 -0.21967712 -0.20149797 -0.20706555 -0.22772047][-0.098674774 -0.11173106 -0.13940743 -0.18887514 -0.24633679 -0.30396795 -0.36169395 -0.41058949 -0.44724575 -0.45226568 -0.424046 -0.38466662 -0.35574377 -0.34716475 -0.35496408][-0.10124839 -0.11544132 -0.14522015 -0.19289377 -0.24244303 -0.29333872 -0.3606776 -0.44417164 -0.5452106 -0.61594516 -0.62816876 -0.59508407 -0.53295 -0.46568394 -0.41572309][-0.096194126 -0.0994149 -0.10886369 -0.12295483 -0.12368691 -0.12957057 -0.17608261 -0.26900071 -0.43622914 -0.60030669 -0.68434042 -0.67524523 -0.57602358 -0.43431357 -0.30804402][-0.082684577 -0.068708226 -0.044734132 -0.0048523843 0.0569584 0.11243678 0.099072017 0.011083148 -0.20040871 -0.42516446 -0.55685145 -0.56845438 -0.44320405 -0.24546352 -0.067957886][-0.077200748 -0.064092085 -0.03727138 0.019020461 0.11620209 0.23783159 0.30290049 0.31237489 0.15364248 -0.049498811 -0.18506736 -0.22482547 -0.11566054 0.071689717 0.23465672][-0.085365772 -0.076246753 -0.036964539 0.076768227 0.27491805 0.53233564 0.74541527 0.86738992 0.75176173 0.53204954 0.34462565 0.21416646 0.18909651 0.2373054 0.2761713][-0.097315967 -0.10312103 -0.075719506 0.044185527 0.26420945 0.5418883 0.7900489 0.95133346 0.85374635 0.63565403 0.41272438 0.20336446 0.067533322 0.029101394 0.016141325][-0.10059273 -0.12227388 -0.1438833 -0.096647233 0.046041645 0.25242534 0.43358374 0.53186071 0.44598424 0.280594 0.11246941 -0.0433949 -0.13295409 -0.13824241 -0.11040941][-0.0930152 -0.10615844 -0.13106914 -0.1336177 -0.057072878 0.058047764 0.14350781 0.18251744 0.12106439 0.020847596 -0.066909254 -0.15640749 -0.20241031 -0.21183944 -0.20508495][-0.091656633 -0.10426861 -0.13576494 -0.18432561 -0.19275859 -0.17518261 -0.15163769 -0.12106512 -0.12341996 -0.16253984 -0.19095698 -0.2287108 -0.2629976 -0.28545016 -0.29561245][-0.091609508 -0.10075361 -0.12633355 -0.17359875 -0.21754107 -0.25307667 -0.26763892 -0.25517121 -0.23126292 -0.22021547 -0.2007203 -0.18859838 -0.18322206 -0.18058062 -0.18620144][-0.091068417 -0.092894793 -0.10472049 -0.132882 -0.16567869 -0.19978592 -0.21158552 -0.2013239 -0.17195734 -0.15424791 -0.13483882 -0.11873036 -0.10843403 -0.097951725 -0.089954533][-0.091319308 -0.091149747 -0.090553045 -0.094358675 -0.097270817 -0.10252531 -0.10642424 -0.10675675 -0.087840527 -0.080619417 -0.081443109 -0.084295973 -0.08556661 -0.090382196 -0.0792355][-0.091608264 -0.091582723 -0.088953353 -0.08797881 -0.088447019 -0.080377907 -0.074114412 -0.077081904 -0.073476821 -0.073195018 -0.080904149 -0.087416783 -0.091968708 -0.096544646 -0.088638395]]...]
INFO - root - 2017-12-06 10:32:18.692792: step 49710, loss = 0.77, batch loss = 0.56 (11.7 examples/sec; 0.682 sec/batch; 53h:32m:54s remains)
INFO - root - 2017-12-06 10:32:25.359584: step 49720, loss = 0.88, batch loss = 0.66 (12.4 examples/sec; 0.644 sec/batch; 50h:34m:32s remains)
INFO - root - 2017-12-06 10:32:31.895683: step 49730, loss = 0.85, batch loss = 0.64 (12.2 examples/sec; 0.656 sec/batch; 51h:31m:17s remains)
INFO - root - 2017-12-06 10:32:38.191550: step 49740, loss = 0.80, batch loss = 0.58 (11.8 examples/sec; 0.676 sec/batch; 53h:04m:22s remains)
INFO - root - 2017-12-06 10:32:44.696074: step 49750, loss = 0.82, batch loss = 0.61 (12.3 examples/sec; 0.652 sec/batch; 51h:10m:13s remains)
INFO - root - 2017-12-06 10:32:51.171505: step 49760, loss = 0.88, batch loss = 0.67 (12.6 examples/sec; 0.635 sec/batch; 49h:53m:41s remains)
INFO - root - 2017-12-06 10:32:57.701442: step 49770, loss = 0.81, batch loss = 0.59 (11.7 examples/sec; 0.681 sec/batch; 53h:29m:51s remains)
INFO - root - 2017-12-06 10:33:04.231846: step 49780, loss = 0.88, batch loss = 0.66 (12.1 examples/sec; 0.659 sec/batch; 51h:43m:55s remains)
INFO - root - 2017-12-06 10:33:10.770017: step 49790, loss = 0.85, batch loss = 0.64 (12.3 examples/sec; 0.652 sec/batch; 51h:10m:01s remains)
INFO - root - 2017-12-06 10:33:17.277781: step 49800, loss = 0.82, batch loss = 0.60 (12.5 examples/sec; 0.639 sec/batch; 50h:11m:23s remains)
2017-12-06 10:33:17.987749: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10207362 0.08597783 -0.19729549 -1.0032185 -1.4441205 -1.2188946 -0.96212274 -0.96228015 -0.96287519 -0.74564338 -0.020356163 0.2538237 0.33147681 -0.44451374 -0.74970335][1.3586015 0.7237218 0.29910606 0.13628909 -0.26750445 -0.79289365 -1.0492686 -0.89097625 -0.63198811 -0.86912727 -0.9051339 -0.87240815 -0.66549253 -0.84009016 -0.90809071][1.3626816 0.93737167 0.78311342 0.34091315 -0.16365954 -0.28960502 -0.56621909 -0.77643514 -0.95285684 -1.4344895 -1.6449144 -1.8562857 -2.199791 -2.0435219 -1.4835179][0.25907969 0.25964031 0.23447517 0.049565636 0.018416762 -0.38161552 -0.99792451 -1.2340621 -1.55248 -1.7071224 -2.0806229 -2.5427082 -2.7500782 -2.8493552 -2.5914471][-0.92558914 -0.69190532 -0.13684505 0.086017929 0.0067087561 -0.41209412 -0.552154 -1.1091568 -1.8117286 -1.8352669 -2.376476 -2.6074314 -3.0413942 -3.2439697 -2.8877518][-0.93555588 -0.44016775 0.097375773 0.55571496 0.4330681 0.35073707 0.35597765 -0.036467332 -0.22335169 -0.54677397 -1.1192148 -1.1857491 -1.3749 -1.486604 -1.4534078][0.13123792 0.37366644 1.1123372 1.2456639 1.3669481 1.3845422 1.1302706 1.2368556 1.6041827 1.446861 1.1842719 0.866731 0.64915174 0.66195196 0.52969134][1.9554505 1.7761966 1.4725944 1.5358614 1.9660363 2.1180346 2.3926003 2.6606495 3.0798342 3.1708219 3.0496769 2.5716038 2.4743419 2.1791046 1.6084301][1.7033716 1.9658909 1.7995641 1.4835199 1.4283829 1.7863916 2.244149 2.4065726 2.9896603 3.71952 3.9590166 3.9076915 4.140533 3.6964571 2.8158648][0.36310282 0.34228927 0.24394637 0.085171767 -0.10442467 -0.17061803 -0.43336958 -0.026497558 0.88161707 1.5928041 2.1423995 2.6826079 3.049778 3.1844885 2.8777535][-1.0051897 -1.2826223 -1.2727216 -1.6393899 -2.1387732 -2.244709 -2.4812715 -2.3502789 -2.0595772 -1.4583668 -0.79301131 -0.24645019 0.37791052 1.1987677 1.5555928][-1.3487545 -1.8747717 -1.8439358 -1.717351 -2.0336804 -2.6297591 -3.0577462 -3.0328357 -3.0783575 -3.1256588 -2.7683296 -2.310843 -1.7814189 -0.78554046 0.44050682][-2.1317422 -2.2323813 -1.8606086 -1.4844269 -1.316449 -1.8415034 -2.2952874 -2.4201486 -2.5923381 -3.023551 -3.2611275 -2.7353024 -2.2344573 -1.9820578 -1.5626442][-2.4211178 -2.4062932 -2.0668104 -1.5093817 -1.3783096 -1.7848756 -1.7385776 -1.5750493 -1.4015247 -1.3474154 -1.3390552 -1.77398 -2.0025618 -1.9028392 -1.8709888][-0.47150519 -0.63368243 -0.55937135 -0.22682032 -0.09607473 -0.6907528 -1.146419 -1.3442031 -1.1802133 -0.66413081 -0.48991871 -0.28996456 -0.084329337 -0.66224355 -1.235718]]...]
