INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "172"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.005-clip-zeroinit-moving_average-from-scratch
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
2017-12-09 06:24:28.846688: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:24:28.846727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:24:28.846734: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:24:28.846738: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:24:28.846742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-09 06:24:34.483813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.38GiB
2017-12-09 06:24:34.483851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 06:24:34.483857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 06:24:34.483865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 06:25:02.086649: step 0, loss = 0.90, batch loss = 0.69 (0.4 examples/sec; 18.539 sec/batch; 1712h:18m:20s remains)
2017-12-09 06:25:03.122855: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00013520694 0.00013473391 0.00013546055 0.00013740451 0.00014642734 0.00015094894 0.00015228661 0.00015496722 0.00015744695 0.00015301039 0.00015207235 0.00015092989 0.00014579021 0.00014740448 0.00014740477][0.00014098966 0.00014008647 0.00014461098 0.00014946432 0.00015377105 0.00015605675 0.0001656301 0.00017633662 0.00018270813 0.00018035325 0.00017805448 0.00018145602 0.0001701107 0.00016148695 0.00015668181][0.00014386255 0.00014374845 0.00014715856 0.00015306482 0.00016074716 0.00016799489 0.00019207013 0.00022090458 0.00024568802 0.00025123535 0.00023763973 0.00022635717 0.00020331432 0.00018062338 0.00016793874][0.00014571749 0.00014525143 0.00014980271 0.00015917847 0.00017941519 0.00020856631 0.00025849813 0.00032573566 0.00036248352 0.00036009331 0.00034081691 0.00030603545 0.00025682696 0.00020945938 0.00018498654][0.00015089236 0.00015316196 0.0001564734 0.00017108316 0.00020735236 0.00026776356 0.00035065098 0.00044075309 0.000501891 0.00049875688 0.00045263316 0.00038706049 0.00031137394 0.00024198568 0.0001996904][0.00015781794 0.0001575677 0.00015673258 0.00018058003 0.00023992494 0.00033077161 0.00045493373 0.00060432521 0.00072928349 0.00069245987 0.0005794618 0.00045250435 0.00033957776 0.00025586848 0.00020281771][0.00016776763 0.00016603491 0.00016616439 0.00019464233 0.00027946907 0.0003916971 0.0005511234 0.00076836586 0.00096472882 0.00081155228 0.00061449589 0.00046635236 0.00033413648 0.00024693582 0.00019923213][0.00017556822 0.00017590677 0.00017886328 0.00020879951 0.0002733572 0.00037604253 0.00050577451 0.0006595694 0.00074929942 0.000620431 0.00048579916 0.00038750269 0.00028654965 0.00022056194 0.0001903067][0.00017712763 0.00018087044 0.00018694176 0.00021998578 0.00026986661 0.00033916687 0.00041476273 0.00046673423 0.00045725048 0.00039958156 0.00034415181 0.00029087663 0.00022785709 0.00019131562 0.00018015713][0.00017366286 0.00017698726 0.00018780809 0.00021959739 0.00026183974 0.00031574821 0.00034769674 0.00034019447 0.00031503217 0.00028450435 0.000254219 0.00022744045 0.00019730136 0.00017916158 0.00017774173][0.00016575836 0.00017206561 0.00018921759 0.00021771116 0.00024669338 0.000273452 0.00027278339 0.00025087746 0.00023838735 0.00022262744 0.00020395771 0.00019622433 0.00018467593 0.00017531781 0.00017410793][0.00016552515 0.00016983812 0.00018188941 0.00019912048 0.00021554559 0.00021947366 0.00021431726 0.00021206659 0.00020681413 0.00019502181 0.00017936125 0.00017548037 0.00017066073 0.000169339 0.00017091993][0.00016586533 0.00016817403 0.00017309964 0.00017653829 0.00018624672 0.00019211581 0.0001926727 0.0001886267 0.00019337195 0.00018686625 0.00017220675 0.00016726639 0.0001628649 0.00016206258 0.00016420342][0.00016974041 0.00016833605 0.00017017481 0.00017010143 0.00017659586 0.0001830893 0.00018712005 0.00018794485 0.0001930423 0.00018544197 0.00017247135 0.0001701521 0.00016497445 0.00016352751 0.0001607241][0.00017403571 0.00017536816 0.00017789296 0.00017839682 0.00017594898 0.00017806103 0.00018026799 0.00018927956 0.00019943586 0.0001897468 0.00017736822 0.00016985512 0.00016384458 0.0001636253 0.00015890114]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.005-clip-zeroinit-moving_average-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.005-clip-zeroinit-moving_average-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 06:25:10.492051: step 10, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:37m:03s remains)
INFO - root - 2017-12-09 06:25:16.953939: step 20, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 63h:58m:38s remains)
INFO - root - 2017-12-09 06:25:23.527873: step 30, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:44m:34s remains)
INFO - root - 2017-12-09 06:25:29.884550: step 40, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 59h:11m:17s remains)
INFO - root - 2017-12-09 06:25:36.399296: step 50, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 58h:07m:29s remains)
INFO - root - 2017-12-09 06:25:42.843665: step 60, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 61h:04m:17s remains)
INFO - root - 2017-12-09 06:25:49.319717: step 70, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:42m:04s remains)
INFO - root - 2017-12-09 06:25:55.816725: step 80, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:53m:21s remains)
INFO - root - 2017-12-09 06:26:02.393291: step 90, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 60h:16m:38s remains)
INFO - root - 2017-12-09 06:26:08.741837: step 100, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.608 sec/batch; 56h:06m:05s remains)
2017-12-09 06:26:09.493940: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00048702836 0.00050115742 0.00049976481 0.00050268276 0.00051260175 0.00053047767 0.00055328861 0.00057181646 0.00058296561 0.00058126583 0.00054903328 0.00047887242 0.00037827212 0.00027096434 0.00016647374][0.00052104559 0.00053743273 0.00053741538 0.00054275646 0.000556218 0.0005780604 0.00060632423 0.0006313948 0.0006496876 0.00065500371 0.00062818738 0.00055796385 0.00044592781 0.000323285 0.00020164336][0.00052516948 0.00054746185 0.00055090804 0.00055822672 0.00057277334 0.00059556635 0.000623915 0.00064992573 0.00067086669 0.00068091264 0.00066108909 0.00059721724 0.00048560745 0.00035897575 0.00022954325][0.00052554748 0.00055604783 0.00056382117 0.00057301106 0.00058755255 0.00060943188 0.00063546706 0.0006591985 0.00068010308 0.00069202052 0.00067727052 0.00061977108 0.00051197648 0.00038571903 0.00025313481][0.00053316454 0.00056624471 0.00057807873 0.00058909855 0.00060245232 0.00062063243 0.00064233813 0.00066177489 0.0006805475 0.0006937568 0.00068241515 0.00062953355 0.00052582007 0.00040243848 0.00026985837][0.0005488309 0.00058057648 0.00059403636 0.00060581125 0.00061643746 0.00062937691 0.00064439554 0.00065773097 0.000672554 0.00068441295 0.00067323993 0.00062312541 0.00052424765 0.00040504552 0.000278242][0.00056307408 0.00059313094 0.00060400856 0.00061405305 0.00062054215 0.000628326 0.00063875737 0.00064903055 0.00066002668 0.00066807651 0.00065604 0.00060796249 0.00051327952 0.00039848313 0.00028003819][0.0005609027 0.00058985286 0.00059717061 0.00060195924 0.00060296094 0.00060746173 0.000618385 0.00063097209 0.00064298679 0.00064897374 0.00063772115 0.00059480179 0.00050493155 0.00039397334 0.00028174231][0.000523331 0.0005535711 0.00055963447 0.00055956427 0.00055603508 0.00055834843 0.00057259813 0.00059093966 0.000610615 0.00062655832 0.000624854 0.00059250009 0.00050992705 0.00040278508 0.00029105766][0.00045393317 0.00048274887 0.00048807645 0.000487057 0.00048204188 0.00048347932 0.00049884594 0.00052324211 0.00055490533 0.00058730337 0.00060406089 0.000587016 0.00051881431 0.00041858357 0.00030566409][0.00039194623 0.00041599461 0.00041811547 0.00041417571 0.00040583502 0.00040412231 0.00041645564 0.00044342157 0.00048298817 0.00052576384 0.000559337 0.00055993831 0.0005154534 0.00043322507 0.00032632577][0.00034727444 0.00036381176 0.00035979619 0.00035045083 0.00033741316 0.00033014498 0.0003383171 0.00036535709 0.00040726617 0.00045565833 0.00049569784 0.00051077129 0.00048786376 0.000431958 0.00034322837][0.00030064036 0.00030866431 0.00029739813 0.0002821823 0.00026641408 0.00025928431 0.00027154153 0.0003047073 0.00035062258 0.00040269317 0.00044534376 0.00046865302 0.00045870707 0.00041843369 0.00034996626][0.0002304966 0.00023181486 0.00021829459 0.00020227846 0.00018891522 0.000187231 0.00020926233 0.00025407193 0.00031260052 0.00037279847 0.00042226491 0.00045050986 0.000446495 0.00041135977 0.00035213077][0.00014007135 0.00013766282 0.00012478678 0.0001116413 0.00010433867 0.00011179177 0.00014154357 0.00019559651 0.00026942897 0.00034407017 0.00040511761 0.00044255544 0.00044546594 0.0004113256 0.00035068323]]...]
INFO - root - 2017-12-09 06:26:16.013784: step 110, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:49m:46s remains)
INFO - root - 2017-12-09 06:26:22.623385: step 120, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 58h:03m:45s remains)
INFO - root - 2017-12-09 06:26:29.185062: step 130, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:45m:59s remains)
INFO - root - 2017-12-09 06:26:35.451151: step 140, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.678 sec/batch; 62h:33m:26s remains)
INFO - root - 2017-12-09 06:26:41.907891: step 150, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.647 sec/batch; 59h:43m:10s remains)
INFO - root - 2017-12-09 06:26:48.214511: step 160, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:28m:32s remains)
INFO - root - 2017-12-09 06:26:54.751405: step 170, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 60h:07m:39s remains)
INFO - root - 2017-12-09 06:27:01.223102: step 180, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.664 sec/batch; 61h:18m:48s remains)
INFO - root - 2017-12-09 06:27:07.806392: step 190, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:56m:45s remains)
INFO - root - 2017-12-09 06:27:14.295419: step 200, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.689 sec/batch; 63h:37m:42s remains)
2017-12-09 06:27:15.032256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00036429183 -0.00036461756 -0.00036273152 -0.00035998409 -0.00035836676 -0.00035798323 -0.00035907733 -0.00036089434 -0.00036281123 -0.00036450691 -0.0003653001 -0.00036526078 -0.00036605014 -0.00036663422 -0.00036602715][-0.00036473168 -0.00036480327 -0.00036296807 -0.00036062946 -0.00035946141 -0.00035954404 -0.00036076549 -0.00036239217 -0.00036404585 -0.00036529652 -0.00036569402 -0.0003655584 -0.0003660603 -0.00036655273 -0.00036609589][-0.00036530642 -0.00036547438 -0.00036403351 -0.00036236492 -0.00036152688 -0.0003619268 -0.00036296892 -0.00036444113 -0.00036550307 -0.00036570657 -0.00036544836 -0.00036516003 -0.000365815 -0.00036661467 -0.00036645061][-0.0003663805 -0.00036678551 -0.00036583544 -0.00036445344 -0.00036335771 -0.00036334858 -0.00036373487 -0.00036449486 -0.00036447219 -0.00036365114 -0.000363242 -0.00036360734 -0.00036519815 -0.00036656106 -0.00036676889][-0.000367632 -0.00036839637 -0.00036796054 -0.00036676804 -0.0003654146 -0.0003646283 -0.0003639521 -0.0003634163 -0.00036183459 -0.0003600941 -0.00035988571 -0.00036135735 -0.00036387803 -0.00036588928 -0.00036682287][-0.00036813677 -0.00036896358 -0.00036885682 -0.00036814937 -0.0003667619 -0.00036520505 -0.00036335894 -0.00036110071 -0.00035797633 -0.00035606156 -0.00035704253 -0.000359638 -0.00036264383 -0.00036533925 -0.0003666229][-0.00036862548 -0.00036933989 -0.0003691797 -0.0003684015 -0.0003665144 -0.00036398415 -0.00036049148 -0.00035608077 -0.00035252376 -0.00035221595 -0.00035464126 -0.00035825692 -0.00036201836 -0.00036498485 -0.00036628856][-0.00036880322 -0.00036949551 -0.00036912752 -0.00036773377 -0.00036484771 -0.00036066776 -0.00035513093 -0.0003488005 -0.00034679813 -0.0003489189 -0.000353035 -0.00035769824 -0.00036203407 -0.00036515878 -0.00036619609][-0.00036914967 -0.00036946932 -0.00036858435 -0.0003667392 -0.00036332043 -0.0003580928 -0.00035150128 -0.00034510263 -0.00034532882 -0.00034902355 -0.00035386439 -0.00035873905 -0.00036296854 -0.0003657451 -0.00036632016][-0.00036907831 -0.00036904408 -0.00036797859 -0.00036603981 -0.00036287843 -0.00035785686 -0.00035199305 -0.00034787139 -0.00034880306 -0.00035207707 -0.00035645405 -0.00036089341 -0.00036444893 -0.0003667273 -0.00036673542][-0.00036910019 -0.00036870115 -0.000367598 -0.00036553125 -0.00036290131 -0.00035879877 -0.00035457587 -0.00035256 -0.00035355487 -0.00035609456 -0.00035983749 -0.00036334689 -0.00036613233 -0.00036776197 -0.00036726982][-0.00036944746 -0.00036879693 -0.00036789721 -0.00036620061 -0.00036424256 -0.00036145322 -0.00035881854 -0.00035764521 -0.00035815171 -0.00035998339 -0.00036275823 -0.00036532554 -0.000367444 -0.00036855141 -0.00036778796][-0.00036969117 -0.00036922819 -0.00036847329 -0.00036726645 -0.00036600797 -0.00036429614 -0.000362894 -0.00036201385 -0.00036201192 -0.00036310675 -0.00036483124 -0.00036658821 -0.0003682707 -0.00036888864 -0.00036790047][-0.00036983821 -0.00036946981 -0.00036900881 -0.00036832661 -0.0003676062 -0.00036654665 -0.00036570604 -0.00036506954 -0.00036496855 -0.00036563684 -0.00036667835 -0.0003677606 -0.00036885747 -0.00036887461 -0.0003676503][-0.00036996344 -0.00036968844 -0.00036950194 -0.00036915884 -0.00036880962 -0.00036823249 -0.00036773851 -0.00036741112 -0.00036728129 -0.00036753991 -0.00036809908 -0.00036867379 -0.00036919914 -0.00036880886 -0.00036744648]]...]
INFO - root - 2017-12-09 06:27:21.397100: step 210, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.638 sec/batch; 58h:52m:35s remains)
INFO - root - 2017-12-09 06:27:27.773828: step 220, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 60h:02m:01s remains)
INFO - root - 2017-12-09 06:27:34.221438: step 230, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:20m:37s remains)
INFO - root - 2017-12-09 06:27:40.664367: step 240, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.644 sec/batch; 59h:28m:00s remains)
INFO - root - 2017-12-09 06:27:47.025635: step 250, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 59h:10m:29s remains)
INFO - root - 2017-12-09 06:27:53.125982: step 260, loss = 0.90, batch loss = 0.69 (12.9 examples/sec; 0.621 sec/batch; 57h:20m:13s remains)
INFO - root - 2017-12-09 06:27:59.601555: step 270, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.659 sec/batch; 60h:48m:42s remains)
INFO - root - 2017-12-09 06:28:06.127416: step 280, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.679 sec/batch; 62h:40m:24s remains)
INFO - root - 2017-12-09 06:28:12.674726: step 290, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.662 sec/batch; 61h:06m:06s remains)
INFO - root - 2017-12-09 06:28:19.312044: step 300, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 60h:06m:48s remains)
2017-12-09 06:28:20.017371: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0098925326 0.019212443 0.033250716 0.052477449 0.07638032 0.10374267 0.13236198 0.1582081 0.17708275 0.18549497 0.18225688 0.1692815 0.14920817 0.12449521 0.097811542][0.021745861 0.036618274 0.057282023 0.083386965 0.11342229 0.14568086 0.17769855 0.20544972 0.22532606 0.23402895 0.23053004 0.21664657 0.19472285 0.16663438 0.13460392][0.041140497 0.06241139 0.089917347 0.12242661 0.15745528 0.19289316 0.22628622 0.25429538 0.27409115 0.28250587 0.27869269 0.26412067 0.24079084 0.2099935 0.17354472][0.068407647 0.09588097 0.12920736 0.16667587 0.20515601 0.24228539 0.2753081 0.30225059 0.3210541 0.32897395 0.32487854 0.30983627 0.28554589 0.25268418 0.21287636][0.10125819 0.1342627 0.17221031 0.2129326 0.25282985 0.28926083 0.31977656 0.34363985 0.35958013 0.36593997 0.36141282 0.34639466 0.32193068 0.28837237 0.24728961][0.13395868 0.17128631 0.21248037 0.25533494 0.29566845 0.33043244 0.35743243 0.37705651 0.38917127 0.3931618 0.38784277 0.37301314 0.34907579 0.316029 0.27533805][0.15806974 0.1979847 0.24067064 0.28446645 0.32469338 0.35815918 0.38271075 0.39931336 0.408437 0.41004616 0.40339106 0.38822687 0.36480847 0.33295578 0.29410192][0.16393436 0.20375663 0.24538571 0.28812408 0.32709175 0.35954157 0.38340887 0.39957294 0.40783173 0.40808311 0.40010291 0.38412 0.36099419 0.33095655 0.29578674][0.14796834 0.18446727 0.22225453 0.261471 0.29757494 0.32871798 0.3527483 0.36987615 0.37879893 0.37891981 0.37035972 0.3538841 0.33147269 0.30414253 0.27418238][0.11586908 0.14641528 0.17820486 0.21176092 0.2431826 0.2711862 0.29410076 0.31164989 0.32153061 0.32245618 0.31459114 0.29912513 0.27884075 0.2554203 0.23146077][0.0796117 0.10287475 0.12734678 0.1534892 0.17839795 0.20125139 0.2209112 0.23687707 0.24679425 0.24894209 0.24326952 0.23056935 0.21396974 0.19539192 0.17744859][0.047724068 0.063988432 0.0813461 0.099549234 0.1169486 0.1330857 0.14726081 0.15920003 0.16715696 0.16993351 0.16710846 0.15881145 0.14785722 0.13543952 0.12391253][0.025027802 0.035180554 0.046208948 0.057870433 0.068922825 0.078422233 0.086616524 0.093549691 0.098132558 0.099989153 0.098939911 0.094826892 0.089426853 0.08318273 0.078062385][0.011233638 0.016960192 0.023344062 0.030168409 0.036757287 0.042137131 0.04640191 0.049255744 0.050672095 0.051156033 0.050582703 0.048674446 0.046702158 0.0444996 0.043456491][0.0042826775 0.0073362915 0.010855759 0.014776436 0.01888557 0.022290241 0.024867367 0.02634624 0.02670493 0.026275329 0.025490025 0.024602311 0.024033984 0.023015369 0.023017589]]...]
INFO - root - 2017-12-09 06:28:26.407206: step 310, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:22m:32s remains)
INFO - root - 2017-12-09 06:28:32.969796: step 320, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.626 sec/batch; 57h:45m:02s remains)
INFO - root - 2017-12-09 06:28:39.481395: step 330, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.672 sec/batch; 62h:01m:01s remains)
INFO - root - 2017-12-09 06:28:45.667540: step 340, loss = 0.89, batch loss = 0.68 (16.0 examples/sec; 0.499 sec/batch; 46h:05m:03s remains)
INFO - root - 2017-12-09 06:28:52.224160: step 350, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.642 sec/batch; 59h:13m:49s remains)
INFO - root - 2017-12-09 06:28:58.400128: step 360, loss = 0.89, batch loss = 0.68 (12.6 examples/sec; 0.637 sec/batch; 58h:47m:05s remains)
INFO - root - 2017-12-09 06:29:05.055086: step 370, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.638 sec/batch; 58h:53m:36s remains)
INFO - root - 2017-12-09 06:29:11.750958: step 380, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.659 sec/batch; 60h:45m:05s remains)
INFO - root - 2017-12-09 06:29:18.362768: step 390, loss = 0.91, batch loss = 0.70 (11.6 examples/sec; 0.691 sec/batch; 63h:42m:35s remains)
INFO - root - 2017-12-09 06:29:24.926123: step 400, loss = 0.91, batch loss = 0.70 (12.5 examples/sec; 0.642 sec/batch; 59h:14m:16s remains)
2017-12-09 06:29:25.623121: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.090484068 0.082071364 0.075669266 0.072536342 0.072574072 0.076337509 0.087109268 0.10544891 0.13222003 0.1666311 0.2068132 0.24713549 0.27755913 0.2918568 0.28738469][0.08544635 0.076303758 0.070727855 0.069742523 0.073394142 0.08233542 0.10000117 0.12568358 0.16014419 0.20258561 0.25129 0.29925808 0.33436674 0.3495332 0.34127542][0.0859135 0.078181386 0.074793987 0.07733243 0.086190864 0.10167342 0.12681577 0.16011938 0.20182455 0.25076854 0.30543908 0.3578268 0.39469528 0.40807384 0.39432457][0.097275652 0.093390062 0.0942862 0.10171633 0.1159341 0.13793969 0.17000949 0.20970665 0.25648904 0.30915397 0.36615863 0.41925493 0.45501289 0.4649381 0.44550973][0.12027635 0.12274326 0.12949139 0.14316259 0.16360192 0.19135262 0.22854431 0.27279741 0.32206404 0.37505704 0.43028647 0.48023534 0.51172334 0.51629251 0.49078381][0.15311758 0.16427664 0.17801154 0.19811511 0.22434764 0.25690037 0.297269 0.34308451 0.39187089 0.44233313 0.49274504 0.53675103 0.56199938 0.56023782 0.52877861][0.19202097 0.21258411 0.23345231 0.25926265 0.28964573 0.32482189 0.36586148 0.41135338 0.458266 0.50453013 0.5487951 0.58550209 0.60347819 0.59500468 0.5580551][0.23042038 0.26006156 0.28697172 0.3166979 0.34903544 0.3844873 0.42373985 0.46632278 0.50967342 0.55101347 0.58871955 0.61774439 0.62919754 0.6153394 0.57463282][0.26078004 0.29768845 0.32914686 0.36096397 0.39341024 0.42681608 0.46251372 0.49985814 0.53758967 0.5732 0.60452414 0.62689388 0.63303715 0.61645365 0.5755747][0.27742279 0.31897387 0.35300854 0.38525194 0.41666216 0.44732606 0.47819802 0.50935322 0.54053408 0.56977034 0.59517545 0.61235124 0.6151098 0.59796721 0.55976349][0.27937606 0.32249439 0.35691363 0.3881107 0.41703081 0.44404405 0.4698123 0.49480185 0.51972246 0.54305041 0.56341046 0.57645357 0.57767195 0.56234884 0.52939481][0.26903617 0.31102219 0.34417233 0.3730647 0.39811218 0.42033032 0.44090796 0.46044892 0.4796707 0.49774247 0.51379085 0.52395058 0.52452081 0.51204866 0.48558134][0.24977729 0.288507 0.3184306 0.34405524 0.36524439 0.38296416 0.39875522 0.41346967 0.42760345 0.44104517 0.45319495 0.46108267 0.46192217 0.45311651 0.43369523][0.22544336 0.25958702 0.285391 0.30729389 0.32499984 0.33946171 0.35143027 0.36219782 0.37222204 0.38178682 0.39089549 0.39730495 0.39879519 0.39375684 0.38114527][0.20230915 0.23182264 0.2532976 0.27102628 0.28504673 0.29651615 0.30572087 0.31394392 0.3215313 0.32844132 0.33536559 0.34081548 0.34314224 0.34088337 0.33344206]]...]
INFO - root - 2017-12-09 06:29:31.985071: step 410, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.630 sec/batch; 58h:08m:54s remains)
INFO - root - 2017-12-09 06:29:38.519113: step 420, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.664 sec/batch; 61h:15m:04s remains)
INFO - root - 2017-12-09 06:29:45.050455: step 430, loss = 0.89, batch loss = 0.68 (12.4 examples/sec; 0.643 sec/batch; 59h:20m:10s remains)
INFO - root - 2017-12-09 06:29:51.466514: step 440, loss = 0.90, batch loss = 0.69 (15.6 examples/sec; 0.514 sec/batch; 47h:27m:11s remains)
INFO - root - 2017-12-09 06:29:57.768420: step 450, loss = 0.89, batch loss = 0.68 (12.5 examples/sec; 0.641 sec/batch; 59h:09m:14s remains)
INFO - root - 2017-12-09 06:30:03.989301: step 460, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.663 sec/batch; 61h:06m:44s remains)
INFO - root - 2017-12-09 06:30:10.569236: step 470, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:07m:45s remains)
INFO - root - 2017-12-09 06:30:17.253983: step 480, loss = 0.90, batch loss = 0.69 (11.5 examples/sec; 0.696 sec/batch; 64h:12m:04s remains)
INFO - root - 2017-12-09 06:30:23.840078: step 490, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:52m:06s remains)
INFO - root - 2017-12-09 06:30:30.475395: step 500, loss = 0.90, batch loss = 0.69 (11.8 examples/sec; 0.676 sec/batch; 62h:19m:15s remains)
2017-12-09 06:30:31.166181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0045311609 -0.0045310231 -0.0045310133 -0.00453112 -0.0045313737 -0.004531709 -0.004532042 -0.0045323079 -0.0045323563 -0.004532055 -0.0045315768 -0.0045309593 -0.0045302617 -0.00452961 -0.0045289653][-0.0045306706 -0.0045303982 -0.004530272 -0.0045303032 -0.004530556 -0.0045309621 -0.0045313071 -0.0045315074 -0.0045314096 -0.0045309551 -0.0045303111 -0.0045296191 -0.0045288969 -0.0045281984 -0.004527566][-0.0045302776 -0.004529817 -0.0045295013 -0.0045294 -0.0045296415 -0.0045300587 -0.0045303572 -0.0045304522 -0.0045302031 -0.0045296587 -0.004529011 -0.0045283795 -0.0045277015 -0.0045270245 -0.0045265015][-0.0045297993 -0.0045290831 -0.0045285253 -0.0045282566 -0.0045283958 -0.0045287884 -0.0045291241 -0.0045292084 -0.0045289611 -0.004528407 -0.0045278231 -0.0045272661 -0.0045267767 -0.0045263991 -0.004526088][-0.004529099 -0.0045282394 -0.0045275129 -0.0045269839 -0.0045269248 -0.0045272494 -0.004527647 -0.0045278613 -0.0045277653 -0.0045273276 -0.004526793 -0.0045263348 -0.0045261052 -0.004526034 -0.0045259115][-0.0045280987 -0.0045272284 -0.0045264871 -0.0045258547 -0.0045256168 -0.00452575 -0.0045259981 -0.0045262687 -0.0045263995 -0.0045262221 -0.0045258603 -0.0045255255 -0.0045253835 -0.0045254333 -0.0045254948][-0.004526976 -0.0045262319 -0.0045255767 -0.0045249714 -0.0045246715 -0.0045246053 -0.0045246114 -0.004524786 -0.0045251027 -0.0045252275 -0.0045250808 -0.0045248335 -0.0045246826 -0.0045246989 -0.0045248461][-0.004525898 -0.0045253285 -0.0045247916 -0.004524291 -0.00452408 -0.0045240135 -0.0045239083 -0.0045239083 -0.0045242184 -0.0045244773 -0.0045245066 -0.0045244079 -0.0045242934 -0.0045242347 -0.0045243278][-0.0045249644 -0.0045244996 -0.0045241169 -0.0045237984 -0.0045237183 -0.0045237169 -0.0045236745 -0.0045236871 -0.0045239427 -0.0045241844 -0.004524265 -0.004524258 -0.0045242445 -0.0045241858 -0.0045241467][-0.0045242785 -0.004523905 -0.0045236871 -0.0045235464 -0.004523546 -0.0045236056 -0.0045236596 -0.0045237276 -0.0045239283 -0.0045241555 -0.0045242412 -0.0045242552 -0.0045242771 -0.0045242347 -0.0045241094][-0.0045239083 -0.0045236377 -0.004523539 -0.0045234947 -0.004523518 -0.0045235869 -0.004523675 -0.0045237583 -0.0045239092 -0.0045241136 -0.0045241849 -0.0045241565 -0.0045241569 -0.0045241355 -0.004524034][-0.0045237625 -0.0045235669 -0.0045235292 -0.0045235306 -0.0045235311 -0.0045235679 -0.0045236279 -0.0045236759 -0.0045237648 -0.0045239115 -0.0045239506 -0.0045239134 -0.0045239073 -0.0045239171 -0.0045238957][-0.00452366 -0.0045235059 -0.0045234985 -0.0045234892 -0.0045234766 -0.0045234882 -0.0045234826 -0.0045234878 -0.0045235292 -0.0045236098 -0.004523627 -0.0045235921 -0.0045235748 -0.0045235939 -0.0045236112][-0.0045234971 -0.0045233713 -0.0045233737 -0.0045233653 -0.004523349 -0.0045233294 -0.0045232913 -0.0045232675 -0.004523261 -0.0045232787 -0.0045232717 -0.0045232447 -0.0045232247 -0.0045232349 -0.0045232596][-0.004523397 -0.0045232838 -0.0045232913 -0.0045232857 -0.0045232666 -0.0045232312 -0.0045232074 -0.0045231613 -0.004523112 -0.0045230794 -0.0045230337 -0.004522996 -0.0045229695 -0.0045229681 -0.0045230035]]...]
INFO - root - 2017-12-09 06:30:37.659200: step 510, loss = 0.89, batch loss = 0.68 (12.0 examples/sec; 0.664 sec/batch; 61h:15m:07s remains)
INFO - root - 2017-12-09 06:30:44.128168: step 520, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:26m:31s remains)
INFO - root - 2017-12-09 06:30:50.706532: step 530, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 60h:02m:08s remains)
INFO - root - 2017-12-09 06:30:57.219179: step 540, loss = 0.90, batch loss = 0.69 (11.7 examples/sec; 0.684 sec/batch; 63h:04m:53s remains)
INFO - root - 2017-12-09 06:31:03.564290: step 550, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:53m:50s remains)
INFO - root - 2017-12-09 06:31:09.809791: step 560, loss = 0.90, batch loss = 0.69 (15.9 examples/sec; 0.503 sec/batch; 46h:21m:21s remains)
INFO - root - 2017-12-09 06:31:16.264987: step 570, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.665 sec/batch; 61h:17m:26s remains)
INFO - root - 2017-12-09 06:31:22.659631: step 580, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:27m:49s remains)
INFO - root - 2017-12-09 06:31:29.095978: step 590, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:59m:07s remains)
INFO - root - 2017-12-09 06:31:35.464142: step 600, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.659 sec/batch; 60h:44m:03s remains)
2017-12-09 06:31:36.139283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0048572947 -0.0034644695 -0.00073396927 0.0040588961 0.011455014 0.021410353 0.033347011 0.045464758 0.055613983 0.062501639 0.064821787 0.062244702 0.054790646 0.044813216 0.034357693][-0.0037040173 -0.00086545432 0.0040828376 0.011591751 0.021770924 0.033814557 0.046769131 0.058846366 0.06798625 0.073081434 0.0730252 0.067732394 0.057294633 0.044524509 0.031877536][-0.0015058257 0.0034911116 0.011388039 0.022154072 0.035368778 0.049704272 0.063354865 0.074637949 0.081884451 0.084768064 0.081976436 0.0739358 0.0611114 0.04626324 0.031998318][0.0025750236 0.010247909 0.021282241 0.035147663 0.050934162 0.066900484 0.080913931 0.091123044 0.096362211 0.096981473 0.091919184 0.081582382 0.066642068 0.050342161 0.034949169][0.0093825255 0.019848537 0.033499207 0.049313836 0.066170178 0.082182504 0.095352061 0.10414349 0.10773626 0.10634942 0.099475518 0.0878919 0.071769156 0.054343525 0.038121659][0.019629326 0.032717466 0.048370074 0.0648027 0.080968149 0.095287941 0.1062768 0.11291973 0.11441956 0.11110379 0.1029384 0.090621807 0.074210137 0.056665171 0.040300462][0.033312146 0.0487764 0.066009469 0.082566693 0.097476937 0.10940573 0.11753087 0.1212199 0.1196516 0.11356048 0.10337666 0.089728251 0.073170349 0.056224875 0.040847782][0.048176177 0.065105416 0.082886808 0.098761126 0.11224833 0.12209272 0.12786236 0.12883255 0.12447864 0.11606991 0.10413432 0.089601874 0.073097415 0.056784227 0.042563181][0.060409088 0.076920554 0.093480334 0.10802061 0.11996359 0.12842581 0.1330542 0.1325741 0.12718002 0.117962 0.10595705 0.091754183 0.076033659 0.060846698 0.04756213][0.068945743 0.08371111 0.097936518 0.11034649 0.12070116 0.12859552 0.13299005 0.1325748 0.12802704 0.12041847 0.11068084 0.098559678 0.085040733 0.0712836 0.058671135][0.0758615 0.089153424 0.10120802 0.1117541 0.12050141 0.12763397 0.13190329 0.13222492 0.12946227 0.12448436 0.11764693 0.10847057 0.097798347 0.085870951 0.073898233][0.083579049 0.096790686 0.10810385 0.11803757 0.12602727 0.13230163 0.13632461 0.137466 0.13621037 0.13291416 0.12780537 0.12058967 0.11182805 0.10178529 0.090455778][0.091773733 0.10670543 0.11888575 0.12960474 0.13841337 0.14505364 0.14956538 0.15195355 0.15212005 0.15004559 0.14560108 0.13882938 0.13036142 0.12052827 0.10900816][0.099015817 0.11614245 0.12983435 0.14201772 0.15220325 0.16031966 0.16656719 0.17067277 0.17251144 0.17211334 0.16888919 0.1627508 0.15445088 0.14441599 0.1328973][0.10538195 0.12414166 0.13964313 0.15360159 0.16569178 0.17539194 0.18329702 0.18954863 0.19356446 0.19521385 0.19382074 0.18955135 0.18241704 0.17261618 0.16065726]]...]
INFO - root - 2017-12-09 06:31:42.580587: step 610, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:10m:34s remains)
INFO - root - 2017-12-09 06:31:49.001276: step 620, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:57m:42s remains)
INFO - root - 2017-12-09 06:31:55.448950: step 630, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.637 sec/batch; 58h:44m:39s remains)
INFO - root - 2017-12-09 06:32:01.863771: step 640, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:52m:20s remains)
INFO - root - 2017-12-09 06:32:08.005957: step 650, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:31m:01s remains)
INFO - root - 2017-12-09 06:32:14.447105: step 660, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.648 sec/batch; 59h:43m:03s remains)
INFO - root - 2017-12-09 06:32:20.778184: step 670, loss = 0.89, batch loss = 0.68 (12.6 examples/sec; 0.637 sec/batch; 58h:41m:52s remains)
INFO - root - 2017-12-09 06:32:27.258707: step 680, loss = 0.91, batch loss = 0.70 (12.5 examples/sec; 0.639 sec/batch; 58h:51m:07s remains)
INFO - root - 2017-12-09 06:32:33.786727: step 690, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.669 sec/batch; 61h:40m:13s remains)
INFO - root - 2017-12-09 06:32:40.340244: step 700, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:29m:13s remains)
2017-12-09 06:32:41.066365: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0068365629 -0.00683693 -0.0068375147 -0.0068378723 -0.0068378514 -0.0068374015 -0.006836717 -0.0068359729 -0.00683539 -0.0068350211 -0.0068348809 -0.0068349415 -0.0068350728 -0.006835205 -0.0068353238][-0.0068368735 -0.006837531 -0.006838432 -0.006839016 -0.0068390719 -0.0068385205 -0.006837592 -0.0068365363 -0.0068356623 -0.0068350723 -0.0068348069 -0.0068347994 -0.0068349014 -0.0068350062 -0.0068351049][-0.0068375892 -0.0068386812 -0.0068400265 -0.0068409848 -0.0068412251 -0.0068405764 -0.0068393173 -0.0068378435 -0.0068365303 -0.0068355603 -0.0068350551 -0.0068349363 -0.0068349987 -0.0068350672 -0.0068351403][-0.006838291 -0.0068398956 -0.0068417392 -0.0068431394 -0.0068436055 -0.006842874 -0.0068412586 -0.0068393247 -0.0068375058 -0.006836107 -0.0068353349 -0.0068350811 -0.0068350905 -0.0068351254 -0.0068351729][-0.0068387673 -0.0068407459 -0.0068430207 -0.0068448596 -0.0068455832 -0.006844881 -0.0068430421 -0.0068407725 -0.0068385177 -0.0068367114 -0.0068356823 -0.0068352404 -0.0068351668 -0.0068351589 -0.0068351757][-0.0068388567 -0.0068409746 -0.00684345 -0.0068456074 -0.006846616 -0.0068460484 -0.0068441979 -0.0068418612 -0.0068394118 -0.0068373275 -0.0068360604 -0.0068354318 -0.0068352665 -0.0068352069 -0.00683518][-0.0068385093 -0.0068405536 -0.0068429722 -0.0068452121 -0.0068463776 -0.0068459888 -0.0068443213 -0.0068421736 -0.0068398211 -0.0068376497 -0.0068362746 -0.0068355715 -0.0068353433 -0.0068352497 -0.0068351813][-0.0068378816 -0.0068396404 -0.0068417834 -0.0068438253 -0.0068449485 -0.0068447511 -0.0068434319 -0.0068416907 -0.006839667 -0.0068376758 -0.0068363515 -0.0068356651 -0.0068354085 -0.0068352935 -0.0068351934][-0.0068371142 -0.0068384125 -0.0068400982 -0.0068417448 -0.0068427036 -0.006842711 -0.0068418332 -0.0068406034 -0.0068390733 -0.0068374248 -0.0068362639 -0.0068356479 -0.0068354104 -0.0068352893 -0.006835191][-0.0068363352 -0.0068371124 -0.0068382677 -0.0068394374 -0.0068401913 -0.0068403706 -0.0068399291 -0.0068392162 -0.0068382323 -0.0068370081 -0.0068360567 -0.0068355631 -0.0068353768 -0.0068352753 -0.0068351822][-0.0068357792 -0.0068360879 -0.00683675 -0.0068374607 -0.0068379757 -0.0068381866 -0.0068380386 -0.0068377336 -0.0068372237 -0.0068364637 -0.0068357945 -0.0068354667 -0.0068353657 -0.0068352837 -0.006835185][-0.0068354812 -0.0068354709 -0.0068357787 -0.0068361233 -0.006836412 -0.0068365508 -0.0068365461 -0.0068364884 -0.006836304 -0.0068359426 -0.0068355841 -0.0068354174 -0.0068353759 -0.0068352954 -0.0068352018][-0.00683535 -0.0068351626 -0.00683526 -0.0068353862 -0.0068355282 -0.0068356306 -0.0068357033 -0.0068357573 -0.0068357573 -0.0068356548 -0.0068355175 -0.00683544 -0.0068353987 -0.006835313 -0.0068352465][-0.0068352739 -0.0068350132 -0.0068350113 -0.0068350527 -0.006835151 -0.0068352758 -0.0068354104 -0.0068355259 -0.006835612 -0.0068356185 -0.0068355538 -0.0068354872 -0.00683542 -0.00683534 -0.0068352888][-0.0068352818 -0.006835009 -0.0068349885 -0.0068350211 -0.0068351356 -0.0068352926 -0.0068354639 -0.0068356195 -0.0068357429 -0.0068357787 -0.0068357168 -0.0068356204 -0.0068355282 -0.0068354341 -0.0068353615]]...]
INFO - root - 2017-12-09 06:32:47.440048: step 710, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:32m:26s remains)
INFO - root - 2017-12-09 06:32:53.798700: step 720, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.632 sec/batch; 58h:17m:17s remains)
INFO - root - 2017-12-09 06:33:00.310426: step 730, loss = 0.91, batch loss = 0.70 (12.4 examples/sec; 0.646 sec/batch; 59h:32m:59s remains)
INFO - root - 2017-12-09 06:33:06.807831: step 740, loss = 0.91, batch loss = 0.70 (11.8 examples/sec; 0.679 sec/batch; 62h:35m:04s remains)
INFO - root - 2017-12-09 06:33:13.159845: step 750, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.655 sec/batch; 60h:22m:07s remains)
INFO - root - 2017-12-09 06:33:19.593218: step 760, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:34m:33s remains)
INFO - root - 2017-12-09 06:33:25.797343: step 770, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.645 sec/batch; 59h:26m:42s remains)
INFO - root - 2017-12-09 06:33:32.292179: step 780, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.660 sec/batch; 60h:50m:38s remains)
INFO - root - 2017-12-09 06:33:38.874268: step 790, loss = 0.91, batch loss = 0.70 (12.0 examples/sec; 0.669 sec/batch; 61h:36m:29s remains)
INFO - root - 2017-12-09 06:33:45.359428: step 800, loss = 0.91, batch loss = 0.70 (11.9 examples/sec; 0.671 sec/batch; 61h:47m:24s remains)
2017-12-09 06:33:46.066961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0081446329 -0.0081444783 -0.0081444345 -0.0081442418 -0.00814382 -0.0081431149 -0.0081421621 -0.00814109 -0.0081401132 -0.0081393765 -0.0081389323 -0.0081387395 -0.008138827 -0.00813907 -0.0081393393][-0.0081445538 -0.0081444122 -0.0081444466 -0.008144387 -0.0081441505 -0.00814365 -0.0081429072 -0.0081420392 -0.0081412345 -0.0081406161 -0.0081402566 -0.0081401253 -0.0081402557 -0.0081405127 -0.0081407791][-0.0081446143 -0.0081444876 -0.008144578 -0.0081446292 -0.0081445659 -0.00814427 -0.0081437426 -0.00814308 -0.0081424508 -0.0081419479 -0.0081416471 -0.0081415419 -0.0081416676 -0.0081418995 -0.0081421165][-0.0081446832 -0.0081445817 -0.0081447037 -0.0081448238 -0.0081448779 -0.0081447288 -0.0081443712 -0.0081438785 -0.0081433989 -0.0081429929 -0.0081427386 -0.0081426529 -0.0081427461 -0.0081429118 -0.0081430431][-0.0081447661 -0.0081446785 -0.0081448145 -0.0081449719 -0.0081450874 -0.0081450315 -0.0081447875 -0.0081444075 -0.0081440164 -0.0081436587 -0.0081434147 -0.0081433114 -0.0081433309 -0.0081433905 -0.0081434147][-0.0081447847 -0.0081447111 -0.0081448313 -0.0081449775 -0.0081450911 -0.0081450595 -0.0081448676 -0.0081445593 -0.0081442269 -0.0081439056 -0.0081436662 -0.0081435358 -0.0081434809 -0.0081434334 -0.0081433412][-0.0081446674 -0.008144591 -0.0081446683 -0.00814477 -0.0081448406 -0.00814479 -0.0081446078 -0.008144334 -0.0081440378 -0.0081437519 -0.0081435191 -0.0081433607 -0.0081432508 -0.0081431447 -0.008142977][-0.0081443479 -0.0081442557 -0.008144279 -0.0081443144 -0.0081443256 -0.0081442427 -0.0081440611 -0.0081438208 -0.0081435619 -0.008143316 -0.0081431111 -0.0081429556 -0.0081428234 -0.0081426874 -0.0081424955][-0.0081438133 -0.0081437128 -0.00814371 -0.0081437044 -0.0081436653 -0.0081435544 -0.0081433682 -0.0081431512 -0.0081429193 -0.0081427181 -0.00814256 -0.0081424294 -0.0081422972 -0.0081421472 -0.0081419535][-0.0081430851 -0.008142977 -0.0081429621 -0.0081429435 -0.0081428932 -0.0081427917 -0.0081426287 -0.00814244 -0.0081422385 -0.0081420755 -0.0081419479 -0.0081418343 -0.0081417141 -0.00814157 -0.0081413845][-0.0081422441 -0.0081421286 -0.0081421062 -0.0081420811 -0.008142042 -0.0081419656 -0.0081418371 -0.0081416871 -0.0081415232 -0.00814139 -0.0081412839 -0.00814119 -0.0081410948 -0.0081409719 -0.0081408005][-0.0081413705 -0.0081412466 -0.0081412327 -0.0081412168 -0.0081411945 -0.0081411488 -0.0081410594 -0.0081409514 -0.0081408257 -0.0081407176 -0.0081406347 -0.0081405677 -0.0081404969 -0.0081403926 -0.0081402324][-0.0081406059 -0.0081404671 -0.0081404625 -0.0081404569 -0.00814045 -0.0081404289 -0.0081403758 -0.0081403088 -0.008140224 -0.0081401486 -0.0081400909 -0.0081400471 -0.0081399968 -0.0081399065 -0.0081397556][-0.0081400424 -0.0081399065 -0.0081399083 -0.0081399092 -0.0081399111 -0.0081399046 -0.0081398776 -0.0081398422 -0.0081397956 -0.0081397546 -0.008139723 -0.0081396988 -0.0081396606 -0.0081395805 -0.00813944][-0.0081397109 -0.008139574 -0.0081395777 -0.0081395833 -0.0081395926 -0.008139601 -0.008139601 -0.0081395945 -0.00813958 -0.0081395628 -0.0081395516 -0.0081395423 -0.0081395162 -0.0081394492 -0.008139329]]...]
INFO - root - 2017-12-09 06:33:52.530865: step 810, loss = 0.89, batch loss = 0.68 (12.3 examples/sec; 0.652 sec/batch; 60h:06m:26s remains)
INFO - root - 2017-12-09 06:33:59.125763: step 820, loss = 0.91, batch loss = 0.70 (12.0 examples/sec; 0.667 sec/batch; 61h:29m:42s remains)
INFO - root - 2017-12-09 06:34:05.725975: step 830, loss = 0.91, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:52m:33s remains)
INFO - root - 2017-12-09 06:34:12.202349: step 840, loss = 0.89, batch loss = 0.68 (12.7 examples/sec; 0.632 sec/batch; 58h:14m:08s remains)
INFO - root - 2017-12-09 06:34:18.462537: step 850, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 60h:02m:01s remains)
INFO - root - 2017-12-09 06:34:24.748677: step 860, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:06m:34s remains)
INFO - root - 2017-12-09 06:34:30.547628: step 870, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:58m:16s remains)
INFO - root - 2017-12-09 06:34:36.992034: step 880, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.636 sec/batch; 58h:32m:26s remains)
INFO - root - 2017-12-09 06:34:43.485262: step 890, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:33m:31s remains)
INFO - root - 2017-12-09 06:34:50.107654: step 900, loss = 0.90, batch loss = 0.69 (11.9 examples/sec; 0.674 sec/batch; 62h:03m:54s remains)
2017-12-09 06:34:50.815082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0088849813 -0.0088868281 -0.0088882456 -0.0088886637 -0.0088879848 -0.0088866353 -0.0088850083 -0.008883466 -0.008882395 -0.0088819787 -0.0088824453 -0.0088835238 -0.0088848956 -0.0088869268 -0.0088903559][-0.0088857086 -0.0088877464 -0.0088892737 -0.0088898037 -0.0088890642 -0.00888751 -0.0088856295 -0.0088838264 -0.008882707 -0.0088824816 -0.0088833049 -0.008884958 -0.008886815 -0.008889352 -0.0088930223][-0.00888641 -0.0088886218 -0.0088902274 -0.0088907741 -0.0088900393 -0.0088884356 -0.0088864379 -0.0088845752 -0.0088834446 -0.0088833915 -0.00888455 -0.0088867005 -0.0088891 -0.00889213 -0.00889594][-0.0088869762 -0.0088892477 -0.0088908281 -0.0088913329 -0.0088905767 -0.0088889953 -0.0088870255 -0.00888518 -0.00888418 -0.0088843442 -0.0088859433 -0.0088884821 -0.0088912006 -0.0088944482 -0.0088979863][-0.0088868467 -0.0088890065 -0.0088903774 -0.0088908933 -0.0088902656 -0.0088888267 -0.008887006 -0.0088853268 -0.0088846209 -0.0088850455 -0.0088868644 -0.0088895811 -0.0088923452 -0.0088953506 -0.0088983579][-0.0088855224 -0.0088874195 -0.0088886051 -0.0088891964 -0.00888888 -0.0088877743 -0.0088862376 -0.0088848975 -0.0088846264 -0.00888539 -0.0088870879 -0.0088895606 -0.0088920081 -0.0088945329 -0.0088969087][-0.0088833049 -0.0088849273 -0.0088859955 -0.0088865757 -0.0088865142 -0.008885962 -0.0088849366 -0.0088839931 -0.008884253 -0.00888524 -0.0088866493 -0.0088886451 -0.0088904733 -0.0088923723 -0.0088941][-0.0088813882 -0.0088825282 -0.0088833952 -0.0088838488 -0.008883955 -0.0088838777 -0.0088834567 -0.0088830218 -0.0088835508 -0.0088843685 -0.0088853911 -0.0088866483 -0.0088877752 -0.00888905 -0.0088901268][-0.0088806655 -0.00888119 -0.0088816592 -0.0088818809 -0.00888198 -0.0088820746 -0.0088820718 -0.0088819815 -0.0088823941 -0.00888291 -0.0088836094 -0.0088842865 -0.0088849133 -0.0088856025 -0.0088861445][-0.0088826995 -0.0088823522 -0.0088819666 -0.0088814478 -0.0088810222 -0.0088807447 -0.0088806115 -0.0088805733 -0.0088808686 -0.0088812578 -0.0088816835 -0.0088819256 -0.0088822134 -0.0088824844 -0.0088827489][-0.008888023 -0.0088871131 -0.0088858185 -0.0088842725 -0.0088828439 -0.0088815764 -0.0088805454 -0.008879846 -0.0088796951 -0.00887989 -0.0088800341 -0.0088800006 -0.0088800136 -0.008880022 -0.0088801766][-0.008894735 -0.0088941669 -0.008892756 -0.0088907313 -0.0088884 -0.0088860169 -0.0088838357 -0.00888199 -0.0088807642 -0.0088800974 -0.0088796886 -0.0088792918 -0.0088791121 -0.0088790441 -0.0088790981][-0.0089004822 -0.0089008845 -0.0089002643 -0.0088987295 -0.0088964114 -0.0088937161 -0.0088909371 -0.00888819 -0.0088858809 -0.00888411 -0.0088827284 -0.00888148 -0.0088806981 -0.0088801933 -0.0088797575][-0.0089045716 -0.0089059742 -0.0089063514 -0.0089057321 -0.0089041749 -0.0089021632 -0.0088998564 -0.0088972971 -0.0088948617 -0.0088927336 -0.0088905869 -0.00888823 -0.0088861594 -0.0088843647 -0.0088826967][-0.0089066783 -0.0089090383 -0.0089104045 -0.0089107491 -0.00891028 -0.0089093037 -0.0089078657 -0.008906085 -0.0089041851 -0.008902261 -0.00889987 -0.0088968016 -0.0088935606 -0.0088902647 -0.0088870479]]...]
INFO - root - 2017-12-09 06:34:57.077833: step 910, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:25m:11s remains)
INFO - root - 2017-12-09 06:35:03.556100: step 920, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:57m:23s remains)
INFO - root - 2017-12-09 06:35:10.156790: step 930, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.651 sec/batch; 59h:59m:36s remains)
INFO - root - 2017-12-09 06:35:16.606529: step 940, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:51m:47s remains)
INFO - root - 2017-12-09 06:35:22.829375: step 950, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.629 sec/batch; 57h:55m:21s remains)
INFO - root - 2017-12-09 06:35:29.213744: step 960, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.650 sec/batch; 59h:51m:25s remains)
INFO - root - 2017-12-09 06:35:35.618347: step 970, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.657 sec/batch; 60h:29m:53s remains)
INFO - root - 2017-12-09 06:35:42.148286: step 980, loss = 0.89, batch loss = 0.68 (12.4 examples/sec; 0.646 sec/batch; 59h:29m:42s remains)
INFO - root - 2017-12-09 06:35:48.646804: step 990, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.653 sec/batch; 60h:05m:17s remains)
INFO - root - 2017-12-09 06:35:55.134456: step 1000, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:21m:31s remains)
2017-12-09 06:35:55.888712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0095674526 -0.00956726 -0.0095672188 -0.0095671713 -0.0095671276 -0.0095670838 -0.0095670363 -0.0095670028 -0.0095670186 -0.0095670782 -0.0095671574 -0.0095672449 -0.009567339 -0.0095674265 -0.0095674926][-0.00956729 -0.0095670512 -0.0095670028 -0.00956695 -0.0095669022 -0.00956686 -0.0095668146 -0.0095667839 -0.0095668016 -0.0095668649 -0.009566946 -0.0095670279 -0.0095671052 -0.0095671713 -0.009567216][-0.0095672915 -0.0095670354 -0.0095669907 -0.0095669432 -0.0095669022 -0.0095668687 -0.0095668314 -0.0095668044 -0.009566823 -0.0095668826 -0.0095669553 -0.0095670251 -0.0095670847 -0.009567135 -0.0095671695][-0.0095673585 -0.0095671248 -0.0095670819 -0.009567041 -0.00956701 -0.0095669851 -0.0095669581 -0.00956694 -0.0095669627 -0.0095670167 -0.0095670782 -0.0095671359 -0.0095671816 -0.0095672188 -0.0095672468][-0.0095674852 -0.0095672859 -0.0095672477 -0.0095672123 -0.0095671918 -0.0095671806 -0.00956717 -0.0095671676 -0.0095671918 -0.0095672412 -0.0095672971 -0.0095673446 -0.0095673781 -0.00956741 -0.00956744][-0.0095676621 -0.0095674973 -0.0095674684 -0.0095674424 -0.0095674321 -0.0095674405 -0.0095674563 -0.0095674777 -0.0095675122 -0.0095675671 -0.009567624 -0.00956766 -0.0095676836 -0.0095677162 -0.0095677581][-0.00956779 -0.0095676817 -0.0095676687 -0.0095676621 -0.0095676724 -0.0095677022 -0.0095677534 -0.0095678139 -0.0095678745 -0.0095679378 -0.00956799 -0.00956801 -0.0095680226 -0.0095680552 -0.0095681][-0.0095678093 -0.0095677655 -0.0095677841 -0.0095678121 -0.0095678577 -0.00956792 -0.0095680095 -0.009568111 -0.009568193 -0.0095682647 -0.00956831 -0.0095683243 -0.0095683336 -0.0095683653 -0.0095684091][-0.0095677571 -0.0095677488 -0.0095678074 -0.0095678717 -0.0095679481 -0.0095680505 -0.0095681818 -0.00956832 -0.0095684286 -0.0095685143 -0.0095685693 -0.009568586 -0.0095685916 -0.00956862 -0.0095686605][-0.0095676947 -0.0095677 -0.0095677935 -0.0095678894 -0.0095679946 -0.0095681278 -0.0095682843 -0.0095684379 -0.0095685609 -0.0095686577 -0.00956872 -0.0095687434 -0.0095687546 -0.0095687862 -0.0095688244][-0.00956763 -0.0095676277 -0.0095677394 -0.0095678577 -0.0095679834 -0.0095681371 -0.0095683029 -0.0095684659 -0.0095686009 -0.00956871 -0.0095687788 -0.0095688095 -0.0095688365 -0.0095688691 -0.0095689][-0.00956756 -0.00956754 -0.009567664 -0.0095678056 -0.0095679564 -0.0095681222 -0.0095682936 -0.0095684528 -0.0095685869 -0.0095686959 -0.0095687686 -0.0095688161 -0.009568857 -0.0095688943 -0.0095689269][-0.009567555 -0.0095674992 -0.0095676146 -0.0095677627 -0.0095679322 -0.0095681194 -0.0095682954 -0.0095684472 -0.00956857 -0.0095686754 -0.0095687518 -0.0095688 -0.0095688459 -0.0095688924 -0.0095689306][-0.0095676249 -0.0095675495 -0.009567651 -0.0095677925 -0.0095679658 -0.00956816 -0.0095683439 -0.0095684994 -0.0095686167 -0.0095687136 -0.0095687862 -0.0095688235 -0.0095688561 -0.0095689 -0.0095689455][-0.00956775 -0.0095676547 -0.0095677357 -0.0095678726 -0.009568058 -0.0095682722 -0.009568478 -0.0095686512 -0.0095687723 -0.0095688561 -0.009568898 -0.0095688971 -0.0095689027 -0.0095689278 -0.0095689632]]...]
INFO - root - 2017-12-09 06:36:02.413061: step 1010, loss = 0.91, batch loss = 0.70 (12.3 examples/sec; 0.650 sec/batch; 59h:52m:27s remains)
INFO - root - 2017-12-09 06:36:08.985692: step 1020, loss = 0.89, batch loss = 0.68 (12.2 examples/sec; 0.655 sec/batch; 60h:19m:05s remains)
INFO - root - 2017-12-09 06:36:15.653530: step 1030, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.658 sec/batch; 60h:33m:43s remains)
INFO - root - 2017-12-09 06:36:22.272389: step 1040, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.653 sec/batch; 60h:04m:57s remains)
INFO - root - 2017-12-09 06:36:28.578237: step 1050, loss = 0.90, batch loss = 0.68 (12.7 examples/sec; 0.628 sec/batch; 57h:46m:43s remains)
INFO - root - 2017-12-09 06:36:34.978730: step 1060, loss = 0.90, batch loss = 0.69 (13.2 examples/sec; 0.606 sec/batch; 55h:49m:19s remains)
INFO - root - 2017-12-09 06:36:41.024485: step 1070, loss = 0.89, batch loss = 0.68 (13.6 examples/sec; 0.589 sec/batch; 54h:14m:01s remains)
INFO - root - 2017-12-09 06:36:47.362413: step 1080, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.624 sec/batch; 57h:27m:39s remains)
INFO - root - 2017-12-09 06:36:53.808065: step 1090, loss = 0.89, batch loss = 0.68 (12.4 examples/sec; 0.643 sec/batch; 59h:13m:18s remains)
INFO - root - 2017-12-09 06:37:00.342496: step 1100, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.638 sec/batch; 58h:41m:46s remains)
2017-12-09 06:37:01.070279: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094102107 0.10674321 0.11731063 0.12533472 0.13067234 0.13180351 0.12894493 0.12111324 0.10848709 0.091576651 0.074049249 0.055945329 0.040073849 0.028446058 0.021096667][0.11281799 0.12378232 0.13167265 0.13946764 0.14516191 0.14740752 0.14712185 0.14110082 0.12940109 0.11172056 0.091736041 0.070305094 0.050142154 0.035097137 0.024991727][0.13603994 0.14428411 0.14790642 0.15240891 0.15664996 0.15981385 0.16122545 0.15826291 0.14983937 0.13248688 0.11005979 0.085405461 0.061167344 0.041410588 0.027257936][0.16495404 0.17112054 0.17091683 0.1710238 0.17118703 0.17257942 0.17265393 0.1715512 0.16573435 0.15098229 0.12946132 0.1033703 0.076582111 0.052893676 0.034588687][0.18939628 0.19764166 0.19891277 0.19661808 0.19518308 0.1929826 0.18971014 0.18746623 0.1811889 0.16799127 0.14616264 0.12006615 0.09155304 0.064953372 0.043985248][0.20892341 0.21886462 0.22179654 0.2215897 0.22208242 0.21966672 0.21750307 0.21457288 0.20755303 0.19355944 0.16999683 0.14198631 0.10882507 0.078607231 0.053711116][0.22556491 0.23650044 0.240188 0.242107 0.24455842 0.24441084 0.24516061 0.24346352 0.23730396 0.22275063 0.19807513 0.16700539 0.12938982 0.093942694 0.063037507][0.24148181 0.25320432 0.25646561 0.2584739 0.26167408 0.26344791 0.26655802 0.26716396 0.26387271 0.2503334 0.22645956 0.19407417 0.15398532 0.11358665 0.0761691][0.26251242 0.27713948 0.27945381 0.27987272 0.28223419 0.28276688 0.28673446 0.28701535 0.2838175 0.27265403 0.25088155 0.2187195 0.17749374 0.13502541 0.094993748][0.2838572 0.3025474 0.30659366 0.30718124 0.30902353 0.307833 0.31013951 0.30721456 0.30156887 0.28816387 0.2645618 0.23312128 0.19277743 0.14969304 0.10866663][0.2994568 0.32067862 0.32612944 0.32928225 0.33261856 0.32995388 0.32962283 0.32307529 0.31301981 0.29525498 0.26837686 0.23565608 0.19545254 0.15554327 0.1180523][0.30695927 0.32969609 0.33726165 0.34219357 0.34671476 0.34636989 0.34657204 0.33712789 0.32342592 0.30095565 0.26953757 0.23331229 0.19199617 0.15335348 0.1178755][0.31132475 0.33335367 0.34168357 0.3483029 0.35291824 0.35374093 0.35427925 0.3475126 0.33523995 0.31063837 0.27684164 0.23825429 0.19441085 0.15326296 0.11736077][0.31180111 0.33239308 0.34151018 0.34891614 0.3527095 0.35399297 0.35384208 0.34781066 0.33704212 0.31493 0.2834371 0.24255681 0.19671905 0.15318201 0.11493027][0.30679551 0.3256298 0.33485037 0.34273368 0.34580529 0.34706071 0.3459985 0.34099057 0.33220476 0.31311831 0.28469864 0.2462759 0.20139429 0.15671106 0.11589162]]...]
INFO - root - 2017-12-09 06:37:07.318313: step 1110, loss = 0.91, batch loss = 0.70 (12.7 examples/sec; 0.631 sec/batch; 58h:07m:05s remains)
INFO - root - 2017-12-09 06:37:13.740545: step 1120, loss = 0.89, batch loss = 0.68 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:09s remains)
INFO - root - 2017-12-09 06:37:20.266268: step 1130, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.634 sec/batch; 58h:21m:03s remains)
INFO - root - 2017-12-09 06:37:26.982482: step 1140, loss = 0.89, batch loss = 0.68 (12.1 examples/sec; 0.662 sec/batch; 60h:56m:38s remains)
INFO - root - 2017-12-09 06:37:33.343199: step 1150, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:44m:02s remains)
INFO - root - 2017-12-09 06:37:39.709303: step 1160, loss = 0.90, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 56h:40m:09s remains)
INFO - root - 2017-12-09 06:37:45.986657: step 1170, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:24m:02s remains)
INFO - root - 2017-12-09 06:37:52.418319: step 1180, loss = 0.90, batch loss = 0.69 (11.6 examples/sec; 0.690 sec/batch; 63h:28m:15s remains)
INFO - root - 2017-12-09 06:37:58.909358: step 1190, loss = 0.91, batch loss = 0.70 (12.5 examples/sec; 0.642 sec/batch; 59h:07m:11s remains)
INFO - root - 2017-12-09 06:38:05.304397: step 1200, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.639 sec/batch; 58h:48m:30s remains)
2017-12-09 06:38:06.143233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.013571986 -0.013585995 -0.013457942 -0.013183146 -0.012787841 -0.012395483 -0.012308651 -0.012625877 -0.012966767 -0.013218926 -0.013081422 -0.012716913 -0.012556378 -0.012675467 -0.012850104][-0.012945538 -0.013198282 -0.012975133 -0.012051965 -0.01071594 -0.0096034044 -0.0095165856 -0.010342898 -0.011442545 -0.01209856 -0.012173487 -0.011838689 -0.011808455 -0.012105666 -0.01245312][-0.0093449075 -0.010116718 -0.010295411 -0.009001812 -0.0067282938 -0.0050272513 -0.0053261323 -0.0071369577 -0.0092979055 -0.010484396 -0.010964416 -0.010996989 -0.011121499 -0.01145453 -0.011978369][-0.0031007202 -0.0029120948 -0.0026936093 -0.0014248407 0.00089578051 0.0021269778 0.000595361 -0.0026413128 -0.006045619 -0.0079998355 -0.0091824559 -0.0095758662 -0.0097437277 -0.010264058 -0.011231988][0.0047150617 0.0064298408 0.0079091946 0.010446963 0.013297246 0.013172061 0.00951963 0.0043004667 -0.0006421525 -0.0039090011 -0.0064024823 -0.00777574 -0.0084195789 -0.00859535 -0.0090322085][0.014188376 0.017707679 0.020575259 0.024368744 0.028556358 0.029004425 0.025282662 0.018206891 0.011156692 0.0052675856 0.00052149128 -0.0026608855 -0.0045614839 -0.0049931649 -0.0057824552][0.02131683 0.027105711 0.032772616 0.039408494 0.045549974 0.04714036 0.044686791 0.036893554 0.028323267 0.019092154 0.010991209 0.0041891737 -0.00052629318 -0.0015155394 -0.001757808][0.026036758 0.032025788 0.03831945 0.046848726 0.056932576 0.06386359 0.066382065 0.060484223 0.052032486 0.03932441 0.026900984 0.015967775 0.0081888 0.0042002415 0.002419821][0.029616509 0.034183539 0.039250877 0.046903253 0.057642452 0.068614483 0.077387437 0.078348286 0.074351117 0.062150039 0.047549561 0.032171242 0.019916408 0.013476933 0.010719326][0.031005193 0.033518136 0.035998944 0.041009139 0.050372541 0.062601231 0.07481841 0.082784258 0.086230047 0.080094151 0.067760691 0.051030658 0.035453334 0.0248672 0.019145668][0.029367428 0.029921576 0.029913388 0.032137375 0.038759362 0.049742527 0.063161179 0.076858707 0.087249331 0.088822253 0.0819598 0.068116084 0.052644432 0.040121987 0.031734087][0.02701221 0.026838239 0.025324579 0.025522839 0.030097213 0.039422106 0.0518122 0.067304656 0.08156772 0.08944045 0.088917561 0.080428161 0.068437569 0.05724299 0.047941446][0.02127533 0.020929474 0.018513128 0.017479934 0.020652656 0.028616179 0.041005425 0.058034122 0.075530246 0.088157505 0.093044467 0.089875765 0.081887476 0.073377267 0.064717241][0.014903172 0.015322744 0.013439857 0.013572267 0.016153868 0.023240179 0.035174593 0.052265234 0.070912659 0.086608551 0.096647777 0.099448584 0.096010976 0.089140892 0.080729283][0.0094517833 0.010410675 0.012299568 0.014508841 0.019206643 0.028228782 0.042123422 0.060209304 0.079450831 0.096806996 0.10843066 0.11348338 0.11219849 0.10637165 0.097767182]]...]
INFO - root - 2017-12-09 06:38:12.536637: step 1210, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.640 sec/batch; 58h:55m:45s remains)
INFO - root - 2017-12-09 06:38:18.984764: step 1220, loss = 0.90, batch loss = 0.69 (12.6 examples/sec; 0.633 sec/batch; 58h:16m:52s remains)
INFO - root - 2017-12-09 06:38:25.487274: step 1230, loss = 0.90, batch loss = 0.69 (12.0 examples/sec; 0.667 sec/batch; 61h:20m:56s remains)
INFO - root - 2017-12-09 06:38:32.044206: step 1240, loss = 0.90, batch loss = 0.69 (12.4 examples/sec; 0.643 sec/batch; 59h:10m:55s remains)
INFO - root - 2017-12-09 06:38:38.233004: step 1250, loss = 0.90, batch loss = 0.69 (12.8 examples/sec; 0.625 sec/batch; 57h:33m:02s remains)
INFO - root - 2017-12-09 06:38:44.545444: step 1260, loss = 0.90, batch loss = 0.69 (12.7 examples/sec; 0.631 sec/batch; 58h:05m:19s remains)
INFO - root - 2017-12-09 06:38:50.885145: step 1270, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.664 sec/batch; 61h:04m:17s remains)
INFO - root - 2017-12-09 06:38:57.256509: step 1280, loss = 0.91, batch loss = 0.70 (12.4 examples/sec; 0.646 sec/batch; 59h:26m:15s remains)
INFO - root - 2017-12-09 06:39:03.666555: step 1290, loss = 0.91, batch loss = 0.70 (12.5 examples/sec; 0.640 sec/batch; 58h:54m:35s remains)
INFO - root - 2017-12-09 06:39:10.011371: step 1300, loss = 0.90, batch loss = 0.69 (12.5 examples/sec; 0.641 sec/batch; 58h:57m:01s remains)
2017-12-09 06:39:10.770953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014358488 -0.01435822 -0.014357996 -0.014357757 -0.014357541 -0.014357366 -0.014357256 -0.014357249 -0.014357368 -0.014357574 -0.014357847 -0.014358156 -0.014358438 -0.014358644 -0.014358774][-0.014358225 -0.014357916 -0.014357674 -0.014357436 -0.01435724 -0.014357102 -0.014357034 -0.014357063 -0.014357208 -0.014357436 -0.014357726 -0.014358037 -0.014358322 -0.014358534 -0.014358669][-0.014358069 -0.014357737 -0.01435749 -0.01435726 -0.014357096 -0.014357 -0.014356973 -0.014357027 -0.014357183 -0.014357415 -0.014357702 -0.014358013 -0.014358302 -0.014358521 -0.014358663][-0.014357917 -0.014357573 -0.014357327 -0.014357113 -0.014356978 -0.01435692 -0.014356923 -0.014356991 -0.014357151 -0.014357383 -0.01435767 -0.014357985 -0.014358281 -0.01435851 -0.014358656][-0.014357768 -0.014357432 -0.014357192 -0.014356994 -0.014356882 -0.014356851 -0.014356869 -0.014356937 -0.014357095 -0.014357327 -0.01435762 -0.014357944 -0.014358254 -0.014358495 -0.01435865][-0.014357647 -0.014357318 -0.014357086 -0.014356903 -0.014356805 -0.014356782 -0.014356796 -0.014356853 -0.014357006 -0.014357244 -0.014357549 -0.01435789 -0.014358221 -0.014358479 -0.014358643][-0.014357548 -0.014357241 -0.014357022 -0.014356848 -0.014356751 -0.014356717 -0.014356713 -0.014356758 -0.014356907 -0.014357151 -0.014357474 -0.014357837 -0.014358191 -0.014358466 -0.014358639][-0.014357496 -0.014357204 -0.014356994 -0.014356825 -0.014356717 -0.014356663 -0.014356634 -0.014356668 -0.014356818 -0.014357073 -0.014357413 -0.014357798 -0.014358172 -0.014358458 -0.014358636][-0.014357458 -0.014357191 -0.014356988 -0.014356819 -0.0143567 -0.01435662 -0.01435657 -0.014356598 -0.014356752 -0.014357019 -0.014357377 -0.014357779 -0.014358168 -0.014358458 -0.014358637][-0.014357451 -0.014357198 -0.014357001 -0.014356827 -0.014356701 -0.014356608 -0.014356547 -0.014356574 -0.014356739 -0.01435702 -0.014357388 -0.014357797 -0.014358185 -0.014358471 -0.014358644][-0.01435749 -0.014357244 -0.014357055 -0.014356884 -0.014356756 -0.014356658 -0.014356595 -0.01435663 -0.014356808 -0.014357097 -0.014357463 -0.01435786 -0.014358232 -0.0143585 -0.014358657][-0.014357589 -0.014357354 -0.014357182 -0.014357029 -0.014356914 -0.014356823 -0.014356769 -0.014356816 -0.014357001 -0.014357279 -0.014357616 -0.014357977 -0.01435831 -0.014358543 -0.014358677][-0.014357761 -0.014357542 -0.014357395 -0.014357263 -0.014357167 -0.01435709 -0.01435705 -0.014357109 -0.014357288 -0.01435754 -0.014357831 -0.014358136 -0.014358412 -0.014358598 -0.0143587][-0.014357965 -0.014357775 -0.014357657 -0.014357551 -0.014357475 -0.014357418 -0.014357393 -0.014357453 -0.014357615 -0.014357826 -0.014358061 -0.014358301 -0.014358511 -0.014358648 -0.014358722][-0.014358209 -0.014358044 -0.014357957 -0.014357878 -0.014357824 -0.014357788 -0.014357781 -0.01435784 -0.014357971 -0.014358132 -0.014358301 -0.014358467 -0.014358611 -0.014358697 -0.014358739]]...]
INFO - root - 2017-12-09 06:39:17.375430: step 1310, loss = 0.90, batch loss = 0.69 (12.3 examples/sec; 0.652 sec/batch; 59h:56m:43s remains)
INFO - root - 2017-12-09 06:39:23.895793: step 1320, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.661 sec/batch; 60h:51m:03s remains)
INFO - root - 2017-12-09 06:39:30.474434: step 1330, loss = 0.90, batch loss = 0.69 (12.1 examples/sec; 0.662 sec/batch; 60h:53m:59s remains)
INFO - root - 2017-12-09 06:39:36.940757: step 1340, loss = 0.90, batch loss = 0.69 (12.2 examples/sec; 0.656 sec/batch; 60h:21m:01s remains)
