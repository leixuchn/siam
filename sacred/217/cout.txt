INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "217"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 10:48:57.907847: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:48:57.907916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:48:57.907942: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:48:57.907963: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:48:57.907983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 10:48:58.590459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-11 10:48:58.590531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 10:48:58.590558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 10:48:58.590587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-11 10:49:03.635903: step 0, loss = 2.28, batch loss = 2.23 (2.2 examples/sec; 3.697 sec/batch; 341h:25m:00s remains)
2017-12-11 10:49:04.147614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3787293 -4.378623 -4.37855 -4.3784575 -4.3783517 -4.3782387 -4.3781261 -4.3780289 -4.3779726 -4.3779755 -4.3780427 -4.3781776 -4.3783588 -4.3785515 -4.37874][-4.37857 -4.3784308 -4.3783188 -4.3781786 -4.3780146 -4.3778343 -4.3776512 -4.3775 -4.3774247 -4.3774447 -4.3775611 -4.3777719 -4.3780375 -4.3783073 -4.3785663][-4.378448 -4.3782763 -4.3781219 -4.3779259 -4.3776913 -4.3774242 -4.3771505 -4.3769341 -4.3768377 -4.3768868 -4.3770733 -4.3773808 -4.3777475 -4.3781061 -4.3784418][-4.3783174 -4.3781176 -4.3779235 -4.377666 -4.3773527 -4.3769941 -4.376627 -4.3763514 -4.3762455 -4.3763385 -4.376605 -4.3770142 -4.3774829 -4.3779254 -4.3783259][-4.3781824 -4.3779597 -4.3777347 -4.3774209 -4.3770356 -4.3765965 -4.3761554 -4.3758359 -4.3757358 -4.3758879 -4.37624 -4.3767433 -4.3772955 -4.3777981 -4.3782415][-4.3780532 -4.3778081 -4.3775539 -4.3771949 -4.3767557 -4.3762674 -4.3757915 -4.3754592 -4.3753982 -4.375628 -4.3760567 -4.3766217 -4.3772135 -4.377739 -4.3781939][-4.3779349 -4.3776684 -4.37739 -4.3770032 -4.37654 -4.3760448 -4.3755832 -4.3752842 -4.3753042 -4.3756275 -4.376112 -4.3766875 -4.3772583 -4.3777609 -4.3781939][-4.3778353 -4.3775487 -4.3772526 -4.3768616 -4.3764167 -4.3759694 -4.3755813 -4.3753738 -4.375514 -4.3759108 -4.376399 -4.3769193 -4.3774133 -4.3778524 -4.3782349][-4.3777533 -4.3774595 -4.3771687 -4.3768096 -4.3764286 -4.3760881 -4.37584 -4.3757749 -4.3760037 -4.3763943 -4.3768163 -4.3772292 -4.377614 -4.3779731 -4.3782969][-4.3777008 -4.3774123 -4.3771505 -4.3768539 -4.3765717 -4.3763652 -4.3762708 -4.3763251 -4.3765712 -4.376894 -4.3772159 -4.3775105 -4.3777966 -4.3780866 -4.3783631][-4.3776946 -4.3774266 -4.377212 -4.3769927 -4.376811 -4.376718 -4.37673 -4.3768415 -4.3770576 -4.3772988 -4.3775225 -4.3777227 -4.3779407 -4.3781829 -4.3784227][-4.3777251 -4.377502 -4.3773522 -4.3772159 -4.3771219 -4.3771024 -4.377161 -4.3772726 -4.3774285 -4.3775854 -4.377728 -4.3778663 -4.3780432 -4.3782554 -4.3784719][-4.3777928 -4.3776336 -4.3775558 -4.3774948 -4.3774629 -4.3774748 -4.3775315 -4.3776007 -4.3776779 -4.3777523 -4.3778319 -4.3779345 -4.3780932 -4.3782926 -4.3784976][-4.3778696 -4.3777733 -4.3777595 -4.3777528 -4.3777533 -4.3777614 -4.3777819 -4.3777862 -4.3777838 -4.3777895 -4.3778267 -4.3779163 -4.37808 -4.3782868 -4.3784962][-4.377914 -4.3778586 -4.3778853 -4.3779073 -4.3779149 -4.3778987 -4.3778667 -4.3778071 -4.3777452 -4.377708 -4.3777318 -4.3778334 -4.3780208 -4.3782487 -4.3784728]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-nosplit-clip50
INFO - root - 2017-12-11 10:49:07.050583: step 10, loss = 2.27, batch loss = 2.17 (35.4 examples/sec; 0.226 sec/batch; 20h:53m:16s remains)
INFO - root - 2017-12-11 10:49:09.222427: step 20, loss = 2.18, batch loss = 2.06 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:49s remains)
INFO - root - 2017-12-11 10:49:11.345891: step 30, loss = 1.93, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:47s remains)
INFO - root - 2017-12-11 10:49:13.527690: step 40, loss = 2.19, batch loss = 2.02 (35.6 examples/sec; 0.224 sec/batch; 20h:43m:29s remains)
INFO - root - 2017-12-11 10:49:15.663662: step 50, loss = 2.14, batch loss = 1.95 (37.1 examples/sec; 0.216 sec/batch; 19h:54m:39s remains)
INFO - root - 2017-12-11 10:49:17.803659: step 60, loss = 1.95, batch loss = 1.73 (37.4 examples/sec; 0.214 sec/batch; 19h:46m:09s remains)
INFO - root - 2017-12-11 10:49:19.921583: step 70, loss = 1.96, batch loss = 1.72 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:47s remains)
INFO - root - 2017-12-11 10:49:22.055926: step 80, loss = 2.01, batch loss = 1.76 (38.0 examples/sec; 0.210 sec/batch; 19h:25m:37s remains)
INFO - root - 2017-12-11 10:49:24.166428: step 90, loss = 2.77, batch loss = 2.50 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:35s remains)
INFO - root - 2017-12-11 10:49:26.298360: step 100, loss = 4.82, batch loss = 4.52 (35.5 examples/sec; 0.225 sec/batch; 20h:47m:07s remains)
2017-12-11 10:49:26.689652: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.1630177 1.8014903 2.2316923 2.718194 3.6520457 4.4981437 5.0836806 5.3073897 5.5059505 4.8670831 4.4418607 4.2663274 3.9084835 3.5199013 3.2358751][1.6335244 2.0236883 2.7125206 3.4034414 4.2406459 5.1653152 5.7314515 6.02286 6.1302333 5.4217591 4.9203019 4.5852628 4.19379 3.9876914 3.5782528][2.2928119 2.7478395 3.1567554 3.7838573 4.7260032 5.5911517 6.1676335 6.5612254 6.6538539 6.2386785 5.578259 5.0986743 4.7723432 4.4242997 3.3168006][2.7275515 3.2705107 3.7166772 4.4028945 5.429235 6.118669 6.7290998 6.965241 7.18453 7.0619836 6.4788547 5.815331 5.3840041 4.5470738 3.9869542][2.68892 3.0161257 3.4430208 4.13516 4.9558196 5.6179519 6.32795 6.723103 7.1856046 7.4365306 7.1543727 6.8361621 5.9069667 5.0572762 4.3901887][2.0260205 2.4294586 2.8722682 3.1905079 3.8275433 4.8673635 5.5204539 6.1200442 6.6618943 6.9527745 6.5978541 6.2799754 5.6821837 4.6796679 3.6376958][1.7486267 2.0280347 2.5458007 3.0473232 3.5890851 4.2051625 4.9247184 5.4752021 6.0282235 6.5067124 6.5007873 6.3353572 5.7757916 4.3101134 3.0739326][1.9705358 2.4054675 2.5253482 2.773025 3.2850409 4.2984204 4.551 4.9848313 5.2624888 5.482739 5.0934596 4.7739835 4.1781631 3.1741247 1.5953817][3.2207232 3.2530398 3.0112853 2.9381309 3.2392011 3.8046317 4.4666 4.9125247 5.291347 5.359261 4.9305682 4.63257 3.7991953 2.56325 1.4829497][3.1389742 3.2235742 3.274673 3.2694139 3.0909071 3.670207 4.2287049 4.8426557 5.004087 4.8579993 4.8545995 4.8533216 3.4683194 2.1336069 1.0932722][3.0089636 3.1921883 3.4770398 3.3636403 3.2352138 3.3836536 3.2763963 3.4791894 3.7369914 3.7614841 4.12853 4.1979394 3.7532535 2.405498 1.0683131][3.5373678 3.2152281 3.0546846 2.9624395 3.1784244 3.115366 2.9307528 2.7396183 2.6692815 2.82401 3.4479284 4.0616956 3.5925946 2.5464497 1.4823542][4.3809752 3.8917017 3.3679943 3.0410547 2.6258268 2.6130691 2.4394813 2.1181774 2.0993562 2.4072256 2.9948516 3.4437451 3.1694608 2.6548047 1.579474][4.6480327 4.762445 4.0046158 3.3731518 3.1202059 2.9523478 2.1753411 1.9460464 1.8928447 2.3629947 2.9708757 3.5736504 3.3908591 2.8170691 2.051909][4.2196231 4.6002393 4.6871905 4.1205325 3.6484189 3.2503924 2.8461113 2.3485351 2.1842475 2.5794902 3.319243 3.6672058 3.3904719 3.2317414 2.2407527]]...]
INFO - root - 2017-12-11 10:49:28.844423: step 110, loss = 13.72, batch loss = 13.40 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:43s remains)
INFO - root - 2017-12-11 10:49:30.990820: step 120, loss = 1.58, batch loss = 1.23 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:18s remains)
INFO - root - 2017-12-11 10:49:33.130397: step 130, loss = 1.74, batch loss = 1.35 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:28s remains)
INFO - root - 2017-12-11 10:49:35.232389: step 140, loss = 2.35, batch loss = 1.95 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:14s remains)
INFO - root - 2017-12-11 10:49:37.335504: step 150, loss = 2.36, batch loss = 1.93 (38.7 examples/sec; 0.207 sec/batch; 19h:06m:28s remains)
INFO - root - 2017-12-11 10:49:39.474030: step 160, loss = 2.38, batch loss = 1.92 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:39s remains)
INFO - root - 2017-12-11 10:49:41.540842: step 170, loss = 2.29, batch loss = 1.81 (39.8 examples/sec; 0.201 sec/batch; 18h:33m:33s remains)
INFO - root - 2017-12-11 10:49:43.601747: step 180, loss = 2.23, batch loss = 1.72 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:59s remains)
INFO - root - 2017-12-11 10:49:45.708556: step 190, loss = 2.06, batch loss = 1.53 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:58s remains)
INFO - root - 2017-12-11 10:49:47.821844: step 200, loss = 2.29, batch loss = 1.72 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:55s remains)
2017-12-11 10:49:48.185038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2197876 -1.6513731 -1.8237299 -1.9702024 -1.6497189 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016][-1.7018011 -1.739719 -2.0889831 -1.7635077 -1.911317 -1.9929894 -1.7030487 -3.3525019 -3.3525019 -3.3525019 -3.3525016 -3.3525019 -3.3525019 -3.3525016 -3.3525014][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525021 -3.3525021 -3.3525021 -3.3525021 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525021 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525021 -3.3525021 -3.3525021 -3.3525021 -3.3525021 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525021 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525021 -3.3525019 -3.3525021 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016 -3.3525016][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016 -3.3525016 -3.3525016 -3.3525016][-3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525019 -3.3525016 -3.3525019 -3.3525019 -3.3525019 -3.3525019]]...]
INFO - root - 2017-12-11 10:49:50.244459: step 210, loss = 2.28, batch loss = 1.68 (39.7 examples/sec; 0.202 sec/batch; 18h:37m:07s remains)
INFO - root - 2017-12-11 10:49:52.327517: step 220, loss = 2.28, batch loss = 1.65 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:37s remains)
INFO - root - 2017-12-11 10:49:54.421900: step 230, loss = 2.25, batch loss = 1.59 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:15s remains)
INFO - root - 2017-12-11 10:49:56.511861: step 240, loss = 2.16, batch loss = 1.47 (38.0 examples/sec; 0.210 sec/batch; 19h:25m:16s remains)
INFO - root - 2017-12-11 10:49:58.622085: step 250, loss = 2.23, batch loss = 1.51 (38.4 examples/sec; 0.209 sec/batch; 19h:15m:00s remains)
INFO - root - 2017-12-11 10:50:00.725667: step 260, loss = 2.17, batch loss = 1.43 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-11 10:50:02.787580: step 270, loss = 2.17, batch loss = 1.40 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:30s remains)
INFO - root - 2017-12-11 10:50:04.869816: step 280, loss = 2.13, batch loss = 1.34 (38.9 examples/sec; 0.205 sec/batch; 18h:57m:46s remains)
INFO - root - 2017-12-11 10:50:06.933500: step 290, loss = 2.13, batch loss = 1.30 (39.7 examples/sec; 0.202 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-11 10:50:09.014505: step 300, loss = 2.10, batch loss = 1.25 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:18s remains)
2017-12-11 10:50:09.348274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782][-2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782 -2.3060782]]...]
INFO - root - 2017-12-11 10:50:11.417693: step 310, loss = 2.13, batch loss = 1.25 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:28s remains)
INFO - root - 2017-12-11 10:50:13.499791: step 320, loss = 2.06, batch loss = 1.17 (38.0 examples/sec; 0.210 sec/batch; 19h:24m:32s remains)
INFO - root - 2017-12-11 10:50:15.584058: step 330, loss = 2.02, batch loss = 1.11 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:08s remains)
INFO - root - 2017-12-11 10:50:17.644639: step 340, loss = 1.96, batch loss = 1.04 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:39s remains)
INFO - root - 2017-12-11 10:50:19.724532: step 350, loss = 1.97, batch loss = 1.03 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:59s remains)
INFO - root - 2017-12-11 10:50:21.776916: step 360, loss = 1.92, batch loss = 0.97 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:00s remains)
INFO - root - 2017-12-11 10:50:23.893190: step 370, loss = 1.93, batch loss = 0.94 (37.8 examples/sec; 0.211 sec/batch; 19h:30m:43s remains)
INFO - root - 2017-12-11 10:50:25.973736: step 380, loss = 1.90, batch loss = 0.90 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:44s remains)
INFO - root - 2017-12-11 10:50:28.049405: step 390, loss = 1.90, batch loss = 0.86 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:52s remains)
INFO - root - 2017-12-11 10:50:30.095003: step 400, loss = 1.90, batch loss = 0.84 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:38s remains)
2017-12-11 10:50:30.447066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534][-1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534 -1.1229534]]...]
INFO - root - 2017-12-11 10:50:32.468207: step 410, loss = 1.90, batch loss = 0.82 (38.7 examples/sec; 0.206 sec/batch; 19h:02m:53s remains)
INFO - root - 2017-12-11 10:50:34.499486: step 420, loss = 1.91, batch loss = 0.80 (40.0 examples/sec; 0.200 sec/batch; 18h:25m:51s remains)
INFO - root - 2017-12-11 10:50:36.596336: step 430, loss = 1.94, batch loss = 0.80 (38.4 examples/sec; 0.208 sec/batch; 19h:13m:03s remains)
INFO - root - 2017-12-11 10:50:38.719261: step 440, loss = 1.94, batch loss = 0.78 (38.2 examples/sec; 0.210 sec/batch; 19h:19m:59s remains)
INFO - root - 2017-12-11 10:50:40.775378: step 450, loss = 2.00, batch loss = 0.82 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:24s remains)
INFO - root - 2017-12-11 10:50:42.811787: step 460, loss = 2.00, batch loss = 0.79 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:22s remains)
INFO - root - 2017-12-11 10:50:44.838691: step 470, loss = 1.99, batch loss = 0.76 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:46s remains)
INFO - root - 2017-12-11 10:50:46.885358: step 480, loss = 2.03, batch loss = 0.75 (39.5 examples/sec; 0.202 sec/batch; 18h:39m:22s remains)
INFO - root - 2017-12-11 10:50:48.944187: step 490, loss = 2.04, batch loss = 0.74 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:48s remains)
INFO - root - 2017-12-11 10:50:51.009633: step 500, loss = 2.05, batch loss = 0.73 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:20s remains)
2017-12-11 10:50:51.387775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662][-0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662 -0.56171662]]...]
INFO - root - 2017-12-11 10:50:53.474305: step 510, loss = 2.10, batch loss = 0.74 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:21s remains)
INFO - root - 2017-12-11 10:50:55.526110: step 520, loss = 2.12, batch loss = 0.73 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:17s remains)
INFO - root - 2017-12-11 10:50:57.552621: step 530, loss = 2.16, batch loss = 0.73 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:55s remains)
INFO - root - 2017-12-11 10:50:59.623683: step 540, loss = 2.18, batch loss = 0.72 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:30s remains)
INFO - root - 2017-12-11 10:51:01.677427: step 550, loss = 2.20, batch loss = 0.72 (39.9 examples/sec; 0.201 sec/batch; 18h:30m:38s remains)
INFO - root - 2017-12-11 10:51:03.698294: step 560, loss = 2.22, batch loss = 0.71 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:35s remains)
INFO - root - 2017-12-11 10:51:05.775441: step 570, loss = 2.26, batch loss = 0.71 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:43s remains)
INFO - root - 2017-12-11 10:51:07.860519: step 580, loss = 2.28, batch loss = 0.71 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:49s remains)
INFO - root - 2017-12-11 10:51:09.923705: step 590, loss = 2.31, batch loss = 0.71 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:52s remains)
INFO - root - 2017-12-11 10:51:11.940098: step 600, loss = 2.32, batch loss = 0.70 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:02s remains)
2017-12-11 10:51:12.242143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538][-0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538 -0.27953538]]...]
INFO - root - 2017-12-11 10:51:14.247167: step 610, loss = 2.35, batch loss = 0.70 (39.9 examples/sec; 0.200 sec/batch; 18h:28m:01s remains)
INFO - root - 2017-12-11 10:51:16.257191: step 620, loss = 2.37, batch loss = 0.70 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:39s remains)
INFO - root - 2017-12-11 10:51:18.265143: step 630, loss = 2.38, batch loss = 0.70 (41.1 examples/sec; 0.195 sec/batch; 17h:57m:09s remains)
INFO - root - 2017-12-11 10:51:20.265418: step 640, loss = 2.39, batch loss = 0.70 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:21s remains)
INFO - root - 2017-12-11 10:51:22.257660: step 650, loss = 2.41, batch loss = 0.70 (40.2 examples/sec; 0.199 sec/batch; 18h:20m:18s remains)
INFO - root - 2017-12-11 10:51:24.261934: step 660, loss = 2.42, batch loss = 0.70 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:42s remains)
INFO - root - 2017-12-11 10:51:26.315232: step 670, loss = 2.43, batch loss = 0.70 (40.2 examples/sec; 0.199 sec/batch; 18h:19m:49s remains)
INFO - root - 2017-12-11 10:51:28.385089: step 680, loss = 2.45, batch loss = 0.70 (39.6 examples/sec; 0.202 sec/batch; 18h:36m:24s remains)
INFO - root - 2017-12-11 10:51:30.423497: step 690, loss = 2.48, batch loss = 0.70 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:11s remains)
INFO - root - 2017-12-11 10:51:32.472228: step 700, loss = 2.50, batch loss = 0.69 (39.7 examples/sec; 0.202 sec/batch; 18h:35m:27s remains)
2017-12-11 10:51:32.810105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834][-0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834 -0.10711834]]...]
INFO - root - 2017-12-11 10:51:34.819154: step 710, loss = 2.51, batch loss = 0.69 (39.7 examples/sec; 0.201 sec/batch; 18h:33m:26s remains)
INFO - root - 2017-12-11 10:51:36.818730: step 720, loss = 2.52, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:05s remains)
INFO - root - 2017-12-11 10:51:38.854528: step 730, loss = 2.53, batch loss = 0.69 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:13s remains)
INFO - root - 2017-12-11 10:51:40.914225: step 740, loss = 2.55, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:46s remains)
INFO - root - 2017-12-11 10:51:42.962922: step 750, loss = 2.57, batch loss = 0.69 (39.9 examples/sec; 0.201 sec/batch; 18h:29m:10s remains)
INFO - root - 2017-12-11 10:51:45.005935: step 760, loss = 2.58, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:06s remains)
