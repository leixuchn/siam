INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "124"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-batch-16
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(16, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(16, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(16, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(16, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/def1/transpose:0", shape=(16, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/def1/transpose_1:0", shape=(16, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(16, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(16, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(16, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(16, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/def1/transpose:0", shape=(16, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/def1/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/def1/transpose_1:0", shape=(16, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(16, 15, 15), dtype=float32)
2017-12-07 02:14:55.234459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:14:55.234498: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:14:55.234504: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:14:55.234508: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:14:55.234512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 02:14:55.589355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-07 02:14:55.589390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 02:14:55.589399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 02:14:55.589406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-1/model.ckpt-20000
INFO - root - Starting threads 0 ...

INFO - root - training for 166250 steps
INFO - root - 2017-12-07 02:14:59.696127: step 0, loss = 2.07, batch loss = 2.01 (4.7 examples/sec; 3.423 sec/batch; 158h:03m:14s remains)
2017-12-07 02:15:00.148396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438741 -4.2160621 -4.1992679 -4.2107592 -4.2220531 -4.2314682 -4.226079 -4.2058711 -4.1697593 -4.1491561 -4.1594491 -4.1896672 -4.2060022 -4.2175007 -4.2451224][-4.2303681 -4.1922832 -4.1718817 -4.1949372 -4.2157631 -4.2254319 -4.2166348 -4.1972361 -4.1668015 -4.1503949 -4.157423 -4.1892343 -4.2060156 -4.2169728 -4.2439752][-4.2077708 -4.1661477 -4.1398726 -4.1668134 -4.1991954 -4.2140989 -4.2061081 -4.1954236 -4.1800876 -4.1695375 -4.1724882 -4.1976991 -4.2119074 -4.220468 -4.2414193][-4.180923 -4.1443396 -4.1129608 -4.1316209 -4.1716657 -4.1927109 -4.19285 -4.1929345 -4.1865249 -4.1804156 -4.1856551 -4.2063503 -4.21664 -4.2217288 -4.236783][-4.1556535 -4.1292996 -4.0994644 -4.1053219 -4.1459332 -4.174643 -4.1776457 -4.1724143 -4.1605577 -4.1551237 -4.1685119 -4.1858087 -4.1976929 -4.2063417 -4.2206721][-4.1242051 -4.1163592 -4.1054168 -4.1124959 -4.1463518 -4.16958 -4.1603308 -4.1373191 -4.1096215 -4.100955 -4.1307907 -4.1576433 -4.17693 -4.1869917 -4.2036376][-4.1088653 -4.1201406 -4.1315446 -4.1437836 -4.163928 -4.1690621 -4.1388073 -4.0941319 -4.0489688 -4.0391154 -4.0900512 -4.139647 -4.1716576 -4.1811037 -4.1984258][-4.1306281 -4.1517873 -4.1728387 -4.1813498 -4.1889586 -4.1806684 -4.1392312 -4.0816145 -4.0216413 -4.0082564 -4.0682082 -4.1348228 -4.1748676 -4.1852527 -4.2026634][-4.1696086 -4.1885543 -4.2122726 -4.2181358 -4.2157879 -4.2016754 -4.164598 -4.1111374 -4.0458403 -4.0226541 -4.0733094 -4.1410847 -4.183073 -4.1959023 -4.2156224][-4.2100811 -4.2215381 -4.242404 -4.2491693 -4.2474322 -4.2351003 -4.2095265 -4.1667366 -4.1090178 -4.0808182 -4.112186 -4.1667714 -4.2004213 -4.211329 -4.2330737][-4.2418013 -4.2468162 -4.2631168 -4.2719431 -4.27755 -4.2734895 -4.2597585 -4.2280135 -4.1808991 -4.1498413 -4.16073 -4.1980395 -4.2204361 -4.2279816 -4.2490435][-4.2728834 -4.2741065 -4.2834382 -4.2908182 -4.2979765 -4.2992263 -4.2934241 -4.2714252 -4.2288094 -4.1940022 -4.1934795 -4.2233615 -4.2414327 -4.2448721 -4.2635818][-4.2951379 -4.2959819 -4.2998977 -4.3033557 -4.3084984 -4.3135824 -4.3108649 -4.2920122 -4.2467871 -4.2099547 -4.2079024 -4.2345996 -4.2542067 -4.2586746 -4.275424][-4.3048282 -4.30366 -4.30478 -4.3069959 -4.3107405 -4.3157115 -4.3141336 -4.3003211 -4.2564359 -4.2212672 -4.2209506 -4.24491 -4.2637753 -4.2699404 -4.2869534][-4.3096628 -4.3090329 -4.3083959 -4.3101869 -4.3143229 -4.3181458 -4.3157754 -4.3060713 -4.266952 -4.2351346 -4.23668 -4.2560792 -4.2690258 -4.2735424 -4.2890759]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-batch-16/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-from1-batch-16/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 02:15:04.329116: step 10, loss = 2.07, batch loss = 2.01 (43.3 examples/sec; 0.369 sec/batch; 17h:02m:45s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset1/biases:0' shape=(72,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 02:15:08.061808: step 20, loss = 2.09, batch loss = 2.03 (42.2 examples/sec; 0.379 sec/batch; 17h:30m:31s remains)
INFO - root - 2017-12-07 02:15:11.760381: step 30, loss = 2.07, batch loss = 2.01 (43.4 examples/sec; 0.369 sec/batch; 17h:01m:19s remains)
INFO - root - 2017-12-07 02:15:15.465656: step 40, loss = 2.08, batch loss = 2.02 (42.9 examples/sec; 0.373 sec/batch; 17h:13m:40s remains)
INFO - root - 2017-12-07 02:15:19.182255: step 50, loss = 2.08, batch loss = 2.02 (42.0 examples/sec; 0.381 sec/batch; 17h:35m:43s remains)
INFO - root - 2017-12-07 02:15:22.921596: step 60, loss = 2.07, batch loss = 2.02 (43.3 examples/sec; 0.369 sec/batch; 17h:02m:34s remains)
INFO - root - 2017-12-07 02:15:26.683012: step 70, loss = 2.07, batch loss = 2.01 (43.1 examples/sec; 0.371 sec/batch; 17h:07m:56s remains)
INFO - root - 2017-12-07 02:15:30.397567: step 80, loss = 2.08, batch loss = 2.02 (42.7 examples/sec; 0.374 sec/batch; 17h:16m:37s remains)
INFO - root - 2017-12-07 02:15:34.142069: step 90, loss = 2.07, batch loss = 2.02 (43.7 examples/sec; 0.366 sec/batch; 16h:53m:02s remains)
INFO - root - 2017-12-07 02:15:37.917679: step 100, loss = 2.07, batch loss = 2.01 (41.3 examples/sec; 0.387 sec/batch; 17h:52m:08s remains)
2017-12-07 02:15:38.319211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2127724 -4.2089591 -4.1946745 -4.1959605 -4.2052493 -4.2227387 -4.2344313 -4.2324858 -4.2237434 -4.2205954 -4.2224436 -4.22572 -4.2332606 -4.242188 -4.2428184][-4.1858869 -4.1878829 -4.1757336 -4.1768723 -4.1856966 -4.1961446 -4.2008796 -4.1991377 -4.1897707 -4.1853275 -4.1890531 -4.1968732 -4.2061324 -4.2167149 -4.220583][-4.1762381 -4.1809678 -4.176168 -4.1767879 -4.1801972 -4.1815619 -4.1804643 -4.18043 -4.1708961 -4.1624851 -4.1663237 -4.1772766 -4.1889944 -4.2028651 -4.2103162][-4.1818452 -4.187479 -4.19093 -4.1940541 -4.1887989 -4.180912 -4.1752162 -4.1751 -4.1630745 -4.150847 -4.1540065 -4.1678886 -4.18584 -4.2062359 -4.2179027][-4.1981707 -4.2039242 -4.2105961 -4.2139835 -4.2030926 -4.1873064 -4.1768765 -4.17493 -4.1610093 -4.1489267 -4.153636 -4.1693349 -4.1917138 -4.2138624 -4.2265477][-4.2128415 -4.2197347 -4.2279339 -4.2316046 -4.2154164 -4.1902766 -4.174716 -4.1714134 -4.164959 -4.16242 -4.17092 -4.1841445 -4.2011447 -4.2151141 -4.2263379][-4.22592 -4.2329021 -4.2390575 -4.23919 -4.2148933 -4.1785054 -4.1539078 -4.1489363 -4.1553464 -4.1679654 -4.1806164 -4.1882715 -4.1931314 -4.1934443 -4.2011805][-4.2424135 -4.2491484 -4.2538214 -4.2490034 -4.2141833 -4.1616449 -4.1167946 -4.1029892 -4.12146 -4.149559 -4.1707754 -4.17788 -4.171977 -4.1582074 -4.1662087][-4.2550287 -4.2650676 -4.2735925 -4.269002 -4.2293396 -4.1622543 -4.0954461 -4.0669169 -4.0896635 -4.1290507 -4.1554632 -4.161931 -4.1494036 -4.1312103 -4.1451459][-4.2536492 -4.267272 -4.2808018 -4.2818685 -4.2485051 -4.181735 -4.1036782 -4.0597663 -4.0737653 -4.1111207 -4.1357021 -4.1399326 -4.1300764 -4.1189184 -4.136641][-4.2311292 -4.2487068 -4.2668581 -4.276329 -4.2574635 -4.2055249 -4.1373062 -4.0873046 -4.0821204 -4.1025648 -4.1216903 -4.1261182 -4.1236563 -4.1197567 -4.1369596][-4.2060328 -4.2217088 -4.2395496 -4.2552552 -4.2523952 -4.2218695 -4.1751695 -4.1333432 -4.1184306 -4.1247768 -4.1380439 -4.1432147 -4.1423235 -4.1355362 -4.1445327][-4.1869936 -4.1981382 -4.2105889 -4.2273154 -4.2366352 -4.226562 -4.2034397 -4.1801333 -4.17435 -4.1823287 -4.1945305 -4.1993418 -4.1945505 -4.1806722 -4.1737318][-4.170238 -4.174202 -4.1789112 -4.1939898 -4.2114143 -4.2185779 -4.2171721 -4.2103128 -4.2156682 -4.2319884 -4.2487841 -4.2548218 -4.2490253 -4.2303405 -4.2123561][-4.165462 -4.1615124 -4.1587162 -4.1737676 -4.1950612 -4.2125 -4.2239008 -4.2255478 -4.235229 -4.2547121 -4.2755923 -4.2849035 -4.2805181 -4.2661486 -4.2491012]]...]
INFO - root - 2017-12-07 02:15:42.093127: step 110, loss = 2.09, batch loss = 2.03 (43.0 examples/sec; 0.372 sec/batch; 17h:09m:54s remains)
INFO - root - 2017-12-07 02:15:45.871436: step 120, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.383 sec/batch; 17h:41m:18s remains)
INFO - root - 2017-12-07 02:15:49.612320: step 130, loss = 2.08, batch loss = 2.02 (43.3 examples/sec; 0.369 sec/batch; 17h:02m:20s remains)
INFO - root - 2017-12-07 02:15:53.402021: step 140, loss = 2.07, batch loss = 2.01 (41.8 examples/sec; 0.382 sec/batch; 17h:38m:56s remains)
INFO - root - 2017-12-07 02:15:57.216608: step 150, loss = 2.09, batch loss = 2.03 (43.6 examples/sec; 0.367 sec/batch; 16h:56m:39s remains)
INFO - root - 2017-12-07 02:16:00.979718: step 160, loss = 2.07, batch loss = 2.02 (41.8 examples/sec; 0.383 sec/batch; 17h:39m:26s remains)
INFO - root - 2017-12-07 02:16:04.719255: step 170, loss = 2.06, batch loss = 2.00 (42.5 examples/sec; 0.376 sec/batch; 17h:21m:24s remains)
INFO - root - 2017-12-07 02:16:08.478468: step 180, loss = 2.08, batch loss = 2.03 (42.7 examples/sec; 0.375 sec/batch; 17h:16m:51s remains)
INFO - root - 2017-12-07 02:16:12.273907: step 190, loss = 2.08, batch loss = 2.02 (43.4 examples/sec; 0.369 sec/batch; 17h:00m:12s remains)
INFO - root - 2017-12-07 02:16:16.045217: step 200, loss = 2.07, batch loss = 2.02 (41.6 examples/sec; 0.385 sec/batch; 17h:44m:19s remains)
2017-12-07 02:16:16.443395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3596168 -4.3621426 -4.3617539 -4.3578858 -4.349988 -4.3411255 -4.3333325 -4.3286567 -4.329514 -4.340004 -4.3516307 -4.3603029 -4.3658123 -4.3653464 -4.3600144][-4.3442359 -4.3424983 -4.3402677 -4.3340964 -4.3221283 -4.3099194 -4.2974348 -4.2899246 -4.2935629 -4.3129396 -4.3329387 -4.3473086 -4.3590574 -4.3628893 -4.3585453][-4.3144388 -4.3052487 -4.2998695 -4.2951212 -4.2844777 -4.2732291 -4.2580447 -4.25261 -4.2634411 -4.2925763 -4.3194294 -4.3374333 -4.3531103 -4.3592534 -4.3560796][-4.2793388 -4.265873 -4.2582574 -4.2583094 -4.25639 -4.2522979 -4.2420726 -4.2396407 -4.2534604 -4.2878833 -4.3195505 -4.3385615 -4.354084 -4.3586659 -4.35431][-4.2436485 -4.2340317 -4.2273178 -4.2268653 -4.2282763 -4.2272353 -4.2179627 -4.2150135 -4.2282872 -4.2688017 -4.3093715 -4.3353119 -4.3540745 -4.3589082 -4.3527145][-4.2090955 -4.2004657 -4.19011 -4.1834016 -4.1794209 -4.1665421 -4.1477475 -4.1334929 -4.144732 -4.1963305 -4.2535233 -4.2960639 -4.3267927 -4.3428035 -4.3431873][-4.1828351 -4.1744432 -4.1577334 -4.1391649 -4.1185508 -4.0877757 -4.0543509 -4.0324779 -4.0454254 -4.1053772 -4.1747317 -4.2339191 -4.2768273 -4.3062773 -4.3188686][-4.162446 -4.1511621 -4.1328006 -4.1052523 -4.0637288 -4.007391 -3.9582582 -3.9371893 -3.9636221 -4.0343971 -4.1121478 -4.1838908 -4.2372122 -4.2751865 -4.2989631][-4.1596403 -4.1442394 -4.1256909 -4.0981016 -4.0477452 -3.9786165 -3.9236684 -3.9013624 -3.9336076 -4.00438 -4.0777698 -4.1523848 -4.2129583 -4.2540045 -4.2807374][-4.1799374 -4.162756 -4.145638 -4.1235585 -4.0784855 -4.0116906 -3.9618669 -3.945915 -3.9744956 -4.0319037 -4.0895829 -4.1532469 -4.208055 -4.2466774 -4.2724881][-4.2093978 -4.192596 -4.1811237 -4.1715631 -4.1416035 -4.0920057 -4.0571394 -4.04942 -4.0689716 -4.1051464 -4.1436524 -4.191143 -4.2309604 -4.2598581 -4.2814684][-4.2362509 -4.2229457 -4.2168961 -4.2203465 -4.2123122 -4.1900015 -4.172596 -4.1667175 -4.1733832 -4.1873064 -4.2078433 -4.2386532 -4.2628813 -4.2829647 -4.3012457][-4.2659397 -4.2582207 -4.2559929 -4.2634449 -4.2698393 -4.2687793 -4.2627563 -4.2572608 -4.257946 -4.263938 -4.2748384 -4.2903113 -4.3005619 -4.309824 -4.3186679][-4.2942472 -4.2913918 -4.293118 -4.3019705 -4.3135066 -4.32174 -4.3233542 -4.320106 -4.3191118 -4.3214793 -4.3251982 -4.3323612 -4.335412 -4.3351507 -4.334486][-4.3153706 -4.3164439 -4.3197145 -4.3261609 -4.3339486 -4.3405108 -4.3433566 -4.3422074 -4.3414426 -4.3422518 -4.3429613 -4.3463092 -4.3482261 -4.346601 -4.3438125]]...]
INFO - root - 2017-12-07 02:16:20.223830: step 210, loss = 2.07, batch loss = 2.02 (42.5 examples/sec; 0.376 sec/batch; 17h:20m:46s remains)
INFO - root - 2017-12-07 02:16:24.004534: step 220, loss = 2.10, batch loss = 2.04 (42.7 examples/sec; 0.375 sec/batch; 17h:17m:32s remains)
INFO - root - 2017-12-07 02:16:27.842511: step 230, loss = 2.08, batch loss = 2.02 (39.6 examples/sec; 0.404 sec/batch; 18h:36m:53s remains)
INFO - root - 2017-12-07 02:16:31.611169: step 240, loss = 2.10, batch loss = 2.05 (43.1 examples/sec; 0.372 sec/batch; 17h:07m:59s remains)
INFO - root - 2017-12-07 02:16:35.405931: step 250, loss = 2.09, batch loss = 2.03 (42.3 examples/sec; 0.378 sec/batch; 17h:26m:21s remains)
INFO - root - 2017-12-07 02:16:39.194655: step 260, loss = 2.08, batch loss = 2.02 (42.0 examples/sec; 0.381 sec/batch; 17h:34m:33s remains)
INFO - root - 2017-12-07 02:16:43.003338: step 270, loss = 2.06, batch loss = 2.00 (42.5 examples/sec; 0.377 sec/batch; 17h:22m:06s remains)
INFO - root - 2017-12-07 02:16:46.798680: step 280, loss = 2.07, batch loss = 2.01 (43.4 examples/sec; 0.369 sec/batch; 17h:00m:49s remains)
INFO - root - 2017-12-07 02:16:50.581835: step 290, loss = 2.09, batch loss = 2.03 (42.5 examples/sec; 0.376 sec/batch; 17h:21m:16s remains)
INFO - root - 2017-12-07 02:16:54.378175: step 300, loss = 2.07, batch loss = 2.01 (41.3 examples/sec; 0.387 sec/batch; 17h:51m:12s remains)
2017-12-07 02:16:54.754634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393484 -4.2335615 -4.2155075 -4.20111 -4.2094979 -4.2247548 -4.2293386 -4.2268438 -4.2338729 -4.2476897 -4.2640882 -4.2805948 -4.2812848 -4.2677026 -4.2412777][-4.2449536 -4.2374158 -4.2180238 -4.2024856 -4.2110109 -4.2264304 -4.2263441 -4.2197523 -4.222393 -4.2317982 -4.2483163 -4.2671947 -4.2716255 -4.2590632 -4.2317338][-4.242662 -4.2351642 -4.2168636 -4.2030225 -4.2065358 -4.2170019 -4.2128038 -4.2002029 -4.1946936 -4.1971316 -4.2157989 -4.2409935 -4.2559824 -4.2512441 -4.2287335][-4.2222838 -4.2217255 -4.2088361 -4.19805 -4.1910772 -4.1872277 -4.17557 -4.1554236 -4.141151 -4.1406784 -4.1653647 -4.2024174 -4.2335625 -4.2437139 -4.2311206][-4.1891131 -4.1965837 -4.1923394 -4.18446 -4.1626143 -4.1414146 -4.1203303 -4.0886865 -4.0622535 -4.0641217 -4.1017017 -4.153811 -4.2017708 -4.2255182 -4.2245173][-4.130199 -4.1426849 -4.1532025 -4.1571875 -4.133749 -4.1003447 -4.0630474 -4.0112224 -3.9697654 -3.9793978 -4.03962 -4.1103935 -4.1686716 -4.1965342 -4.1974349][-4.0738616 -4.0844026 -4.1068864 -4.124835 -4.107347 -4.0608416 -4.003572 -3.9320104 -3.8759484 -3.8957393 -3.979933 -4.0690212 -4.1353517 -4.1612778 -4.1562309][-4.0681248 -4.0702744 -4.0920753 -4.1150656 -4.102675 -4.0538087 -3.989923 -3.909173 -3.846096 -3.868377 -3.9539406 -4.0466957 -4.116991 -4.143291 -4.1367245][-4.0916052 -4.0902596 -4.107409 -4.1280193 -4.1216307 -4.0862703 -4.035409 -3.9668455 -3.9106984 -3.9159176 -3.9734392 -4.0486064 -4.1128497 -4.1396527 -4.1383495][-4.13276 -4.1316919 -4.1435466 -4.1569343 -4.1539264 -4.1340618 -4.101377 -4.0525994 -4.0037713 -3.986191 -4.0112991 -4.064085 -4.119513 -4.1489229 -4.1548452][-4.1862812 -4.1851912 -4.19104 -4.1973643 -4.1956315 -4.188858 -4.1725392 -4.1391506 -4.0956659 -4.0640268 -4.0647936 -4.0959578 -4.1395907 -4.1694889 -4.1785603][-4.2331214 -4.2327366 -4.2359467 -4.239 -4.2382736 -4.2372837 -4.2315655 -4.2125196 -4.1793718 -4.1460576 -4.1334372 -4.1445317 -4.1700354 -4.1914635 -4.19956][-4.2649374 -4.265357 -4.2678189 -4.2700748 -4.2707434 -4.2718019 -4.2706113 -4.2613893 -4.2411542 -4.2159743 -4.1962008 -4.1916294 -4.199501 -4.2098827 -4.2143936][-4.2791686 -4.2809076 -4.2835646 -4.2857141 -4.2868462 -4.2881551 -4.2877083 -4.2844086 -4.2744813 -4.2587624 -4.2394066 -4.2257133 -4.2202272 -4.2213793 -4.2241254][-4.2848091 -4.2865424 -4.2893467 -4.2913847 -4.2915797 -4.2914581 -4.2907476 -4.2898316 -4.2864842 -4.27803 -4.26284 -4.246295 -4.2332149 -4.2277927 -4.2294168]]...]
INFO - root - 2017-12-07 02:16:58.560616: step 310, loss = 2.07, batch loss = 2.01 (43.0 examples/sec; 0.372 sec/batch; 17h:10m:12s remains)
INFO - root - 2017-12-07 02:17:02.371309: step 320, loss = 2.09, batch loss = 2.03 (42.2 examples/sec; 0.379 sec/batch; 17h:27m:17s remains)
INFO - root - 2017-12-07 02:17:06.162808: step 330, loss = 2.09, batch loss = 2.03 (42.9 examples/sec; 0.373 sec/batch; 17h:12m:29s remains)
INFO - root - 2017-12-07 02:17:09.944879: step 340, loss = 2.08, batch loss = 2.02 (42.7 examples/sec; 0.375 sec/batch; 17h:16m:21s remains)
INFO - root - 2017-12-07 02:17:13.776329: step 350, loss = 2.07, batch loss = 2.01 (40.8 examples/sec; 0.392 sec/batch; 18h:04m:38s remains)
INFO - root - 2017-12-07 02:17:17.569822: step 360, loss = 2.07, batch loss = 2.01 (42.2 examples/sec; 0.379 sec/batch; 17h:27m:52s remains)
INFO - root - 2017-12-07 02:17:21.403225: step 370, loss = 2.09, batch loss = 2.03 (42.3 examples/sec; 0.378 sec/batch; 17h:25m:28s remains)
INFO - root - 2017-12-07 02:17:25.183100: step 380, loss = 2.09, batch loss = 2.03 (42.4 examples/sec; 0.378 sec/batch; 17h:23m:52s remains)
INFO - root - 2017-12-07 02:17:28.973219: step 390, loss = 2.09, batch loss = 2.03 (42.4 examples/sec; 0.378 sec/batch; 17h:24m:10s remains)
INFO - root - 2017-12-07 02:17:32.754302: step 400, loss = 2.08, batch loss = 2.03 (42.3 examples/sec; 0.378 sec/batch; 17h:25m:51s remains)
2017-12-07 02:17:33.143830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2252593 -4.2341557 -4.2310214 -4.2370481 -4.2523804 -4.2628541 -4.2687359 -4.2702055 -4.2669287 -4.2674375 -4.2717228 -4.2719431 -4.2681408 -4.2636132 -4.2586727][-4.2292056 -4.2382388 -4.2307606 -4.2345386 -4.2513909 -4.263042 -4.2710204 -4.2739134 -4.2677207 -4.2642717 -4.2684116 -4.2701716 -4.2670736 -4.2664595 -4.266016][-4.2305684 -4.2355833 -4.2272234 -4.2299261 -4.2444806 -4.2539916 -4.2654591 -4.2754865 -4.2774053 -4.279654 -4.2872219 -4.2922082 -4.2882271 -4.2914457 -4.2988081][-4.203124 -4.2061992 -4.2046018 -4.2088971 -4.2185383 -4.22006 -4.2299123 -4.2507696 -4.2735338 -4.2896605 -4.30308 -4.3110261 -4.3085294 -4.3153434 -4.3279471][-4.1480675 -4.16292 -4.1764436 -4.1809 -4.1827097 -4.1708012 -4.1716661 -4.202714 -4.2501292 -4.2834063 -4.302268 -4.3088622 -4.3036051 -4.30932 -4.3258739][-4.0637956 -4.1100526 -4.1433358 -4.1461177 -4.1364889 -4.1129222 -4.1048875 -4.1484423 -4.2166944 -4.2579775 -4.2763486 -4.2789454 -4.2694883 -4.2729807 -4.2931519][-3.956775 -4.0351505 -4.0930028 -4.0987577 -4.0756555 -4.039897 -4.0270562 -4.0829415 -4.1711078 -4.22034 -4.2321563 -4.2287822 -4.2162709 -4.2221866 -4.2477918][-3.9064381 -3.9927933 -4.064589 -4.0773416 -4.0531063 -4.0109992 -3.9812698 -4.0237446 -4.1199903 -4.1745238 -4.1813383 -4.1733813 -4.165113 -4.1788878 -4.2112665][-3.966228 -4.0310636 -4.0895057 -4.1053138 -4.0942597 -4.058835 -4.0205708 -4.0349956 -4.1032195 -4.1412435 -4.138185 -4.1298695 -4.1291127 -4.1480942 -4.1763988][-4.0703917 -4.108429 -4.1445484 -4.1580915 -4.1534357 -4.1237373 -4.0916395 -4.0924487 -4.1260056 -4.1427517 -4.133954 -4.1264939 -4.1271615 -4.1415935 -4.1569676][-4.1542888 -4.1735258 -4.1891184 -4.1964879 -4.1908851 -4.16268 -4.1344337 -4.1324563 -4.14528 -4.1505075 -4.1459255 -4.1467381 -4.1513839 -4.1613 -4.1683664][-4.2100391 -4.2156787 -4.2155414 -4.2127805 -4.2029452 -4.1759133 -4.1472063 -4.14285 -4.1498566 -4.1542869 -4.1541667 -4.1630259 -4.1723995 -4.1832576 -4.1943274][-4.2489281 -4.2441163 -4.2342181 -4.2215953 -4.2068725 -4.1798205 -4.1496973 -4.139986 -4.1473494 -4.1586175 -4.1614676 -4.1765604 -4.186584 -4.1955366 -4.215004][-4.2701173 -4.2612262 -4.2490454 -4.2303243 -4.2071705 -4.1750984 -4.1372375 -4.1179781 -4.1258645 -4.1498246 -4.1636577 -4.1869187 -4.1968465 -4.2076068 -4.2315154][-4.2889566 -4.2774343 -4.2660456 -4.2449994 -4.2152643 -4.1789942 -4.135932 -4.1055646 -4.1065097 -4.1323638 -4.1577377 -4.1917911 -4.2120962 -4.2283521 -4.250474]]...]
INFO - root - 2017-12-07 02:17:36.955427: step 410, loss = 2.08, batch loss = 2.02 (42.5 examples/sec; 0.377 sec/batch; 17h:20m:57s remains)
INFO - root - 2017-12-07 02:17:40.742680: step 420, loss = 2.09, batch loss = 2.03 (43.0 examples/sec; 0.372 sec/batch; 17h:08m:37s remains)
INFO - root - 2017-12-07 02:17:44.547825: step 430, loss = 2.06, batch loss = 2.01 (40.9 examples/sec; 0.392 sec/batch; 18h:02m:00s remains)
INFO - root - 2017-12-07 02:17:48.372789: step 440, loss = 2.08, batch loss = 2.02 (41.7 examples/sec; 0.383 sec/batch; 17h:39m:17s remains)
INFO - root - 2017-12-07 02:17:52.194800: step 450, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.387 sec/batch; 17h:48m:06s remains)
INFO - root - 2017-12-07 02:17:55.998649: step 460, loss = 2.09, batch loss = 2.03 (42.1 examples/sec; 0.380 sec/batch; 17h:30m:04s remains)
INFO - root - 2017-12-07 02:17:59.798697: step 470, loss = 2.06, batch loss = 2.00 (42.3 examples/sec; 0.379 sec/batch; 17h:25m:55s remains)
INFO - root - 2017-12-07 02:18:03.617545: step 480, loss = 2.08, batch loss = 2.02 (42.2 examples/sec; 0.379 sec/batch; 17h:26m:26s remains)
INFO - root - 2017-12-07 02:18:07.431836: step 490, loss = 2.08, batch loss = 2.02 (42.1 examples/sec; 0.380 sec/batch; 17h:29m:17s remains)
INFO - root - 2017-12-07 02:18:11.252585: step 500, loss = 2.05, batch loss = 2.00 (42.8 examples/sec; 0.374 sec/batch; 17h:13m:29s remains)
2017-12-07 02:18:11.637575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3112822 -4.3147016 -4.3194027 -4.324151 -4.3257647 -4.32426 -4.3211141 -4.3186154 -4.3194962 -4.3206563 -4.3183217 -4.3150125 -4.3105311 -4.3063164 -4.3052249][-4.30346 -4.3054328 -4.3063707 -4.3077717 -4.3089881 -4.3105984 -4.31178 -4.3122268 -4.316761 -4.3203721 -4.3189139 -4.3145552 -4.3079028 -4.3019695 -4.3019853][-4.291285 -4.2876787 -4.2801867 -4.275351 -4.2755771 -4.2803874 -4.2862225 -4.2912369 -4.3006792 -4.3075485 -4.3094268 -4.309031 -4.3057694 -4.301867 -4.3038483][-4.2742605 -4.2603292 -4.2424955 -4.2296581 -4.2256603 -4.2283311 -4.2333794 -4.2421265 -4.2594967 -4.2731652 -4.2824059 -4.2906432 -4.2972693 -4.3014956 -4.3084984][-4.26134 -4.2403603 -4.2143879 -4.1933527 -4.1798253 -4.1696978 -4.159771 -4.1633987 -4.1891923 -4.2123375 -4.2300172 -4.248549 -4.2689848 -4.2859869 -4.302556][-4.2589922 -4.2408729 -4.2145867 -4.1876163 -4.1590776 -4.1211295 -4.0778546 -4.0728722 -4.1135449 -4.1507263 -4.1760383 -4.2010369 -4.2318053 -4.2578521 -4.28371][-4.2650146 -4.2548079 -4.2326031 -4.2026463 -4.1570854 -4.0835404 -3.99864 -3.9903128 -4.0565162 -4.1104155 -4.1401906 -4.1642218 -4.1953592 -4.2226553 -4.253634][-4.2763457 -4.2752452 -4.2563539 -4.2260342 -4.1689987 -4.0718656 -3.9665952 -3.9672372 -4.0473151 -4.1050835 -4.1294165 -4.1432991 -4.1638412 -4.1832075 -4.210968][-4.287827 -4.29265 -4.2773805 -4.2506065 -4.1969838 -4.1079779 -4.0209384 -4.0309563 -4.0967045 -4.1421051 -4.1565661 -4.15284 -4.1519871 -4.1550617 -4.1691804][-4.2926183 -4.2997079 -4.2883358 -4.2682991 -4.2288852 -4.1688423 -4.1160078 -4.1275678 -4.1698127 -4.1982241 -4.2005467 -4.1821742 -4.1635218 -4.152247 -4.1492815][-4.288053 -4.2945628 -4.2872248 -4.2719674 -4.2492733 -4.2235527 -4.202457 -4.2130589 -4.2374978 -4.251852 -4.2444525 -4.2223563 -4.2001762 -4.1827574 -4.1674089][-4.2793226 -4.2819076 -4.2729955 -4.2584481 -4.2516675 -4.2555609 -4.2565012 -4.2693515 -4.2865882 -4.295126 -4.2863622 -4.2692165 -4.2525353 -4.2366805 -4.2177749][-4.2691622 -4.2618089 -4.2455754 -4.2294564 -4.2386255 -4.2632976 -4.2783561 -4.2961092 -4.3130426 -4.3190761 -4.3121266 -4.301321 -4.29298 -4.2828016 -4.2696705][-4.2628136 -4.245091 -4.2209268 -4.2066808 -4.2278352 -4.2629371 -4.2841749 -4.3063059 -4.3230648 -4.3271027 -4.3215342 -4.3164458 -4.3141909 -4.310864 -4.3058367][-4.2652712 -4.2429132 -4.2161312 -4.2025943 -4.2276778 -4.2646918 -4.2895145 -4.3137064 -4.3268108 -4.3273044 -4.3215919 -4.3186216 -4.3197737 -4.3217344 -4.3231368]]...]
INFO - root - 2017-12-07 02:18:15.436709: step 510, loss = 2.09, batch loss = 2.04 (42.0 examples/sec; 0.381 sec/batch; 17h:31m:17s remains)
INFO - root - 2017-12-07 02:18:19.266391: step 520, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.387 sec/batch; 17h:48m:38s remains)
INFO - root - 2017-12-07 02:18:23.106984: step 530, loss = 2.08, batch loss = 2.02 (40.7 examples/sec; 0.393 sec/batch; 18h:06m:06s remains)
INFO - root - 2017-12-07 02:18:26.886432: step 540, loss = 2.09, batch loss = 2.03 (42.5 examples/sec; 0.376 sec/batch; 17h:19m:29s remains)
INFO - root - 2017-12-07 02:18:30.701069: step 550, loss = 2.08, batch loss = 2.02 (42.5 examples/sec; 0.377 sec/batch; 17h:19m:52s remains)
INFO - root - 2017-12-07 02:18:34.497233: step 560, loss = 2.09, batch loss = 2.03 (42.0 examples/sec; 0.381 sec/batch; 17h:32m:37s remains)
INFO - root - 2017-12-07 02:18:38.308524: step 570, loss = 2.09, batch loss = 2.04 (42.5 examples/sec; 0.377 sec/batch; 17h:20m:05s remains)
INFO - root - 2017-12-07 02:18:42.149296: step 580, loss = 2.08, batch loss = 2.02 (43.1 examples/sec; 0.371 sec/batch; 17h:04m:09s remains)
INFO - root - 2017-12-07 02:18:45.969626: step 590, loss = 2.06, batch loss = 2.00 (41.7 examples/sec; 0.383 sec/batch; 17h:38m:08s remains)
INFO - root - 2017-12-07 02:18:49.800486: step 600, loss = 2.08, batch loss = 2.02 (42.4 examples/sec; 0.377 sec/batch; 17h:21m:25s remains)
2017-12-07 02:18:50.169653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3296866 -4.3175712 -4.2976737 -4.2671757 -4.2330737 -4.218998 -4.2384386 -4.2747765 -4.3045483 -4.3295979 -4.3445778 -4.3442221 -4.3364658 -4.3315167 -4.3292327][-4.3256621 -4.3053036 -4.2737288 -4.223896 -4.1694331 -4.1417747 -4.1640105 -4.2178388 -4.2631745 -4.3028893 -4.33267 -4.3425865 -4.3392086 -4.3338304 -4.3290272][-4.3246164 -4.3009868 -4.2611589 -4.1939287 -4.1156726 -4.0647764 -4.0789671 -4.1483154 -4.214282 -4.2693076 -4.3142023 -4.3380666 -4.3413138 -4.337739 -4.3316374][-4.3202972 -4.3021946 -4.2620544 -4.1872416 -4.088747 -4.0084496 -4.0003352 -4.0742736 -4.1629467 -4.234252 -4.2918019 -4.3302116 -4.3418941 -4.3395815 -4.3329158][-4.3158422 -4.3053613 -4.2661109 -4.188642 -4.0767021 -3.9655619 -3.9218884 -3.98858 -4.0997405 -4.1916432 -4.2622709 -4.3140516 -4.3367939 -4.3375654 -4.3314519][-4.3098583 -4.308363 -4.2734365 -4.1999822 -4.0831323 -3.947957 -3.8643818 -3.9084063 -4.0310717 -4.1439619 -4.229229 -4.2936573 -4.3254738 -4.3310804 -4.3272657][-4.2979379 -4.3104348 -4.2851791 -4.2230482 -4.1140738 -3.9771943 -3.8764019 -3.8979025 -4.0126467 -4.13099 -4.2234631 -4.2916236 -4.3231654 -4.3279691 -4.3237886][-4.2877345 -4.3109512 -4.2973008 -4.2498198 -4.1601882 -4.0450163 -3.9595811 -3.9751046 -4.0635166 -4.1621428 -4.2441273 -4.3040409 -4.3278151 -4.3269525 -4.3211055][-4.2984056 -4.3209391 -4.3122349 -4.2770524 -4.2112336 -4.12463 -4.0605059 -4.0707488 -4.13167 -4.2030191 -4.2679925 -4.3150368 -4.3317394 -4.327024 -4.320065][-4.318676 -4.3348479 -4.3248119 -4.29922 -4.2545805 -4.1942515 -4.1472945 -4.1522636 -4.1923389 -4.2443995 -4.2921834 -4.3262382 -4.3374872 -4.330544 -4.3228984][-4.3345284 -4.3458652 -4.3370728 -4.3193164 -4.2927132 -4.253787 -4.2215252 -4.2231917 -4.2493672 -4.287293 -4.3180685 -4.3382564 -4.3439436 -4.3353176 -4.3263063][-4.3506627 -4.3557739 -4.3472557 -4.3347392 -4.32195 -4.3016014 -4.2838902 -4.2848158 -4.3017249 -4.3263168 -4.3406897 -4.3467255 -4.3448176 -4.3356767 -4.3269019][-4.3684478 -4.3646441 -4.3516684 -4.3388653 -4.3327703 -4.3243589 -4.3171382 -4.3183441 -4.3297615 -4.3462472 -4.350596 -4.3469849 -4.3401489 -4.3309078 -4.3243794][-4.3771105 -4.3659964 -4.3505893 -4.3377819 -4.3354163 -4.3317766 -4.3293915 -4.3310843 -4.3386321 -4.3497519 -4.350883 -4.3450513 -4.336472 -4.3279977 -4.3235078][-4.3737092 -4.3618846 -4.3458295 -4.3339558 -4.3341036 -4.3330755 -4.3322086 -4.333076 -4.3367863 -4.342525 -4.3431773 -4.3390236 -4.332273 -4.3265257 -4.3240881]]...]
INFO - root - 2017-12-07 02:18:53.979260: step 610, loss = 2.08, batch loss = 2.02 (41.6 examples/sec; 0.384 sec/batch; 17h:40m:59s remains)
INFO - root - 2017-12-07 02:18:57.773812: step 620, loss = 2.07, batch loss = 2.01 (42.5 examples/sec; 0.376 sec/batch; 17h:19m:16s remains)
INFO - root - 2017-12-07 02:19:01.570858: step 630, loss = 2.09, batch loss = 2.03 (42.4 examples/sec; 0.378 sec/batch; 17h:22m:36s remains)
INFO - root - 2017-12-07 02:19:05.432027: step 640, loss = 2.10, batch loss = 2.04 (41.6 examples/sec; 0.385 sec/batch; 17h:42m:13s remains)
INFO - root - 2017-12-07 02:19:09.283156: step 650, loss = 2.07, batch loss = 2.01 (42.3 examples/sec; 0.378 sec/batch; 17h:22m:46s remains)
INFO - root - 2017-12-07 02:19:13.173439: step 660, loss = 2.08, batch loss = 2.02 (40.0 examples/sec; 0.400 sec/batch; 18h:24m:48s remains)
INFO - root - 2017-12-07 02:19:17.008155: step 670, loss = 2.08, batch loss = 2.02 (41.3 examples/sec; 0.387 sec/batch; 17h:48m:04s remains)
INFO - root - 2017-12-07 02:19:20.824652: step 680, loss = 2.08, batch loss = 2.02 (42.4 examples/sec; 0.378 sec/batch; 17h:22m:10s remains)
INFO - root - 2017-12-07 02:19:24.649331: step 690, loss = 2.06, batch loss = 2.00 (41.2 examples/sec; 0.388 sec/batch; 17h:51m:43s remains)
INFO - root - 2017-12-07 02:19:28.462489: step 700, loss = 2.07, batch loss = 2.02 (42.0 examples/sec; 0.381 sec/batch; 17h:30m:32s remains)
2017-12-07 02:19:28.846043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120818 -4.2918983 -4.2734861 -4.25875 -4.2564435 -4.258132 -4.2568455 -4.2603097 -4.2633624 -4.2647996 -4.2704883 -4.2828822 -4.282053 -4.2698126 -4.2580309][-4.3094788 -4.2866659 -4.2648163 -4.2473135 -4.2450809 -4.2444839 -4.237083 -4.2356811 -4.2374749 -4.2463222 -4.2622194 -4.2784719 -4.2775707 -4.2654767 -4.2545919][-4.3094463 -4.2878575 -4.266274 -4.2430859 -4.2358918 -4.232657 -4.21908 -4.2107949 -4.2146478 -4.2304621 -4.2500691 -4.2664104 -4.26463 -4.2575784 -4.254849][-4.3101025 -4.2936025 -4.2767119 -4.2488604 -4.2318773 -4.2210011 -4.1978607 -4.1780348 -4.1800346 -4.2011523 -4.2239227 -4.2406497 -4.2418585 -4.2445874 -4.2502766][-4.3004336 -4.2844892 -4.26678 -4.2338305 -4.2059293 -4.1827884 -4.1478162 -4.1171522 -4.11837 -4.1458888 -4.17284 -4.1924157 -4.2006264 -4.2151184 -4.2288761][-4.2761602 -4.2531552 -4.2272367 -4.1853766 -4.1459689 -4.1099772 -4.0659008 -4.0299096 -4.0378094 -4.073832 -4.1017976 -4.1256275 -4.1457748 -4.1701336 -4.1906443][-4.2488456 -4.2157307 -4.1770191 -4.12357 -4.0692258 -4.0181179 -3.9690413 -3.9382906 -3.9586155 -4.0031252 -4.0281439 -4.053637 -4.084558 -4.1148877 -4.1420741][-4.2364569 -4.200099 -4.1544266 -4.0927725 -4.0266681 -3.9644675 -3.9148979 -3.8960073 -3.932162 -3.9796894 -3.9978247 -4.01793 -4.0464678 -4.0737944 -4.1046867][-4.2536073 -4.2219787 -4.1784935 -4.1178465 -4.0506954 -3.9888444 -3.9507279 -3.9481473 -3.9884541 -4.0298047 -4.0422335 -4.0499287 -4.0643492 -4.0809951 -4.1086416][-4.2880564 -4.2673531 -4.2304955 -4.1782036 -4.1221638 -4.0727863 -4.0498042 -4.0559616 -4.0871348 -4.1160994 -4.1220074 -4.1242476 -4.130929 -4.1384292 -4.1562843][-4.3150353 -4.3070774 -4.2829757 -4.2441158 -4.2065582 -4.1750779 -4.1613545 -4.1696491 -4.1899476 -4.2072215 -4.2102747 -4.2091875 -4.2107735 -4.2124281 -4.2205691][-4.325736 -4.3265424 -4.315927 -4.29304 -4.2721968 -4.2546105 -4.2477984 -4.2554412 -4.2682381 -4.2791405 -4.2813258 -4.28013 -4.2786164 -4.2775183 -4.2801552][-4.3260765 -4.3292651 -4.3265276 -4.3167386 -4.3069649 -4.2978745 -4.2950411 -4.3007636 -4.3080587 -4.3150377 -4.3180923 -4.3170581 -4.3151784 -4.3145165 -4.316577][-4.3209438 -4.3214211 -4.321547 -4.3186345 -4.3151512 -4.3125725 -4.3132429 -4.31672 -4.3211737 -4.326097 -4.3291183 -4.3280106 -4.3267617 -4.3279881 -4.330689][-4.3211741 -4.3176451 -4.3167267 -4.315196 -4.3146133 -4.3152628 -4.3170209 -4.318759 -4.3209867 -4.3232422 -4.3250957 -4.325069 -4.324935 -4.3266869 -4.3293338]]...]
INFO - root - 2017-12-07 02:19:32.667070: step 710, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.385 sec/batch; 17h:42m:41s remains)
INFO - root - 2017-12-07 02:19:36.450752: step 720, loss = 2.08, batch loss = 2.02 (42.5 examples/sec; 0.377 sec/batch; 17h:19m:16s remains)
INFO - root - 2017-12-07 02:19:40.243318: step 730, loss = 2.06, batch loss = 2.01 (42.3 examples/sec; 0.378 sec/batch; 17h:22m:34s remains)
INFO - root - 2017-12-07 02:19:44.040168: step 740, loss = 2.08, batch loss = 2.02 (42.3 examples/sec; 0.378 sec/batch; 17h:23m:34s remains)
INFO - root - 2017-12-07 02:19:47.853729: step 750, loss = 2.10, batch loss = 2.04 (42.3 examples/sec; 0.378 sec/batch; 17h:23m:22s remains)
INFO - root - 2017-12-07 02:19:51.658739: step 760, loss = 2.07, batch loss = 2.01 (42.9 examples/sec; 0.373 sec/batch; 17h:09m:19s remains)
INFO - root - 2017-12-07 02:19:55.556068: step 770, loss = 2.09, batch loss = 2.03 (41.3 examples/sec; 0.388 sec/batch; 17h:48m:55s remains)
INFO - root - 2017-12-07 02:19:59.384839: step 780, loss = 2.08, batch loss = 2.02 (40.7 examples/sec; 0.393 sec/batch; 18h:04m:51s remains)
INFO - root - 2017-12-07 02:20:03.224073: step 790, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.386 sec/batch; 17h:43m:10s remains)
INFO - root - 2017-12-07 02:20:07.017394: step 800, loss = 2.06, batch loss = 2.00 (42.2 examples/sec; 0.379 sec/batch; 17h:25m:12s remains)
2017-12-07 02:20:07.417833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2665157 -4.2534795 -4.2503939 -4.2522235 -4.2413049 -4.202076 -4.1575408 -4.1259441 -4.1246076 -4.1598797 -4.2133837 -4.2653818 -4.2867012 -4.2801642 -4.250134][-4.2468138 -4.2316375 -4.2321076 -4.2427487 -4.2429314 -4.2114449 -4.1666079 -4.1404877 -4.141541 -4.1684833 -4.2164259 -4.255167 -4.273407 -4.267869 -4.240767][-4.2256021 -4.2110381 -4.2171354 -4.2308183 -4.2383943 -4.2168522 -4.172184 -4.1474748 -4.14635 -4.1609936 -4.2048326 -4.2382922 -4.2556214 -4.2498641 -4.2188854][-4.2070613 -4.1949263 -4.2095771 -4.2272534 -4.2366147 -4.2207246 -4.1781373 -4.1465578 -4.1383376 -4.147512 -4.1869674 -4.2177958 -4.23246 -4.2230444 -4.1878042][-4.1819267 -4.1755195 -4.2035589 -4.2215066 -4.2249079 -4.203795 -4.1604781 -4.1143346 -4.0993767 -4.1190968 -4.1591659 -4.1853757 -4.1913743 -4.1731496 -4.1349249][-4.1492543 -4.1461654 -4.1840334 -4.1993451 -4.1855011 -4.1506133 -4.0914 -4.01998 -3.9971385 -4.0500746 -4.1087427 -4.135107 -4.1320205 -4.1051397 -4.0729809][-4.1179256 -4.1145067 -4.150969 -4.1585503 -4.1203313 -4.049932 -3.9539969 -3.8388994 -3.81361 -3.9297411 -4.0225983 -4.0646791 -4.0694494 -4.0547276 -4.0345755][-4.1058683 -4.0968432 -4.1210394 -4.1141291 -4.05582 -3.9521911 -3.8134344 -3.6443803 -3.6236708 -3.8179867 -3.9507413 -4.016077 -4.0410361 -4.0421429 -4.0318351][-4.12578 -4.1110277 -4.1219778 -4.1054449 -4.043962 -3.9393167 -3.8009729 -3.6505718 -3.6607924 -3.8489008 -3.974859 -4.0441165 -4.0727324 -4.0739503 -4.0648537][-4.1570444 -4.143508 -4.1526256 -4.1403775 -4.0894337 -4.0063729 -3.9083259 -3.8304775 -3.8637278 -3.9829588 -4.0601025 -4.1080275 -4.1257439 -4.1223965 -4.1136956][-4.1734133 -4.1647725 -4.1812396 -4.1830392 -4.1444292 -4.0806775 -4.0199342 -3.9934464 -4.0308266 -4.0985236 -4.1435504 -4.16793 -4.1731129 -4.1679916 -4.1599197][-4.1833878 -4.1825843 -4.2070074 -4.2196903 -4.1936207 -4.1487756 -4.11577 -4.1162925 -4.1483793 -4.1869969 -4.215755 -4.2267747 -4.2247415 -4.2211418 -4.2170277][-4.20639 -4.2096224 -4.2340994 -4.2496076 -4.2344708 -4.2072043 -4.1951728 -4.2061434 -4.2289033 -4.2507086 -4.2682018 -4.274384 -4.273304 -4.2718053 -4.268106][-4.2380276 -4.2382793 -4.2569876 -4.2722225 -4.2653847 -4.25045 -4.2485251 -4.2601123 -4.275135 -4.2877307 -4.2980418 -4.3017707 -4.3024158 -4.3031588 -4.300087][-4.2690277 -4.2640505 -4.2757411 -4.2888055 -4.2896857 -4.282836 -4.2837281 -4.2915111 -4.3001752 -4.3072286 -4.3122454 -4.3134651 -4.313817 -4.3146706 -4.3135176]]...]
INFO - root - 2017-12-07 02:20:11.217706: step 810, loss = 2.09, batch loss = 2.03 (41.9 examples/sec; 0.382 sec/batch; 17h:33m:10s remains)
INFO - root - 2017-12-07 02:20:15.014305: step 820, loss = 2.09, batch loss = 2.03 (42.3 examples/sec; 0.378 sec/batch; 17h:23m:12s remains)
INFO - root - 2017-12-07 02:20:18.854292: step 830, loss = 2.08, batch loss = 2.02 (41.2 examples/sec; 0.389 sec/batch; 17h:51m:16s remains)
INFO - root - 2017-12-07 02:20:22.678686: step 840, loss = 2.08, batch loss = 2.02 (42.0 examples/sec; 0.381 sec/batch; 17h:30m:00s remains)
INFO - root - 2017-12-07 02:20:26.585240: step 850, loss = 2.09, batch loss = 2.03 (41.5 examples/sec; 0.385 sec/batch; 17h:41m:36s remains)
INFO - root - 2017-12-07 02:20:30.453616: step 860, loss = 2.08, batch loss = 2.02 (41.5 examples/sec; 0.385 sec/batch; 17h:42m:36s remains)
INFO - root - 2017-12-07 02:20:34.328483: step 870, loss = 2.09, batch loss = 2.03 (40.2 examples/sec; 0.398 sec/batch; 18h:16m:50s remains)
INFO - root - 2017-12-07 02:20:38.131809: step 880, loss = 2.08, batch loss = 2.02 (42.2 examples/sec; 0.379 sec/batch; 17h:24m:23s remains)
INFO - root - 2017-12-07 02:20:41.965699: step 890, loss = 2.09, batch loss = 2.03 (41.6 examples/sec; 0.385 sec/batch; 17h:39m:55s remains)
INFO - root - 2017-12-07 02:20:45.759553: step 900, loss = 2.07, batch loss = 2.02 (42.4 examples/sec; 0.378 sec/batch; 17h:21m:08s remains)
2017-12-07 02:20:46.163845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2714496 -4.2721071 -4.2681942 -4.2608247 -4.2485361 -4.2395568 -4.2455845 -4.2621193 -4.2759728 -4.2817879 -4.2826891 -4.2831054 -4.2849393 -4.2861257 -4.2823944][-4.2388659 -4.233861 -4.230092 -4.2241178 -4.2117391 -4.2001882 -4.2054935 -4.2252426 -4.2434568 -4.2541275 -4.2604322 -4.2638831 -4.2693048 -4.2755327 -4.2748][-4.2066593 -4.194078 -4.1901817 -4.1858654 -4.1733232 -4.1598406 -4.1604848 -4.1796265 -4.2040424 -4.2259154 -4.2415414 -4.249959 -4.2583075 -4.2691221 -4.2725053][-4.1887565 -4.1668925 -4.1568632 -4.1498365 -4.1337233 -4.1153688 -4.1061249 -4.1189346 -4.1521297 -4.1893291 -4.2181845 -4.235662 -4.2494879 -4.2645125 -4.271852][-4.1777735 -4.1482177 -4.1289282 -4.11498 -4.0945697 -4.0680423 -4.0425072 -4.044086 -4.0859628 -4.1419277 -4.185678 -4.2170959 -4.2405195 -4.2593155 -4.2697763][-4.1610146 -4.1294165 -4.1055794 -4.0774107 -4.0483303 -4.0106778 -3.9581804 -3.9416242 -3.9976816 -4.0780673 -4.1379404 -4.1824765 -4.2177877 -4.2402287 -4.2517428][-4.1478529 -4.1209383 -4.0978932 -4.0611215 -4.0237985 -3.9656196 -3.868988 -3.822078 -3.8995287 -4.0103126 -4.0866318 -4.1427865 -4.1846881 -4.20619 -4.2166386][-4.1631203 -4.1411967 -4.11974 -4.0854897 -4.0487471 -3.9813204 -3.867677 -3.8085635 -3.8867955 -3.9941795 -4.0682497 -4.1197634 -4.1579785 -4.1757188 -4.1833258][-4.2090626 -4.1911936 -4.1731706 -4.1468415 -4.1198149 -4.0642824 -3.984772 -3.9481034 -3.9903374 -4.0534778 -4.1019516 -4.1339455 -4.1571264 -4.1683459 -4.1700349][-4.259387 -4.2516866 -4.2402377 -4.2249951 -4.2084932 -4.1682329 -4.1180882 -4.0960188 -4.1083913 -4.1374259 -4.1627779 -4.1774116 -4.1886978 -4.1932616 -4.1850524][-4.2905493 -4.292398 -4.2920871 -4.2917585 -4.2845964 -4.2575483 -4.22338 -4.20355 -4.2003751 -4.2118192 -4.2229314 -4.2296381 -4.2374105 -4.24088 -4.2293468][-4.3041735 -4.3142838 -4.3253818 -4.3348432 -4.3336134 -4.3170786 -4.2939177 -4.2755151 -4.2666759 -4.2688713 -4.2704997 -4.2726712 -4.2798419 -4.2833495 -4.27552][-4.3080988 -4.3260975 -4.3443904 -4.3572803 -4.3597169 -4.3495603 -4.3353 -4.32096 -4.3121729 -4.3090386 -4.3039951 -4.3014617 -4.3076367 -4.3150363 -4.3164082][-4.3098621 -4.3290482 -4.3477578 -4.359695 -4.3638535 -4.3595796 -4.3521948 -4.3435588 -4.3385706 -4.3333392 -4.3252845 -4.3218865 -4.3277645 -4.3374391 -4.3456678][-4.3139343 -4.3259935 -4.3384557 -4.3465772 -4.3511033 -4.3510923 -4.3482761 -4.3441105 -4.3436408 -4.3393946 -4.3308306 -4.3263497 -4.3292851 -4.3377604 -4.34643]]...]
INFO - root - 2017-12-07 02:20:50.035387: step 910, loss = 2.08, batch loss = 2.02 (41.1 examples/sec; 0.390 sec/batch; 17h:53m:56s remains)
INFO - root - 2017-12-07 02:20:53.889661: step 920, loss = 2.09, batch loss = 2.03 (41.5 examples/sec; 0.386 sec/batch; 17h:42m:44s remains)
INFO - root - 2017-12-07 02:20:57.695877: step 930, loss = 2.08, batch loss = 2.02 (41.9 examples/sec; 0.382 sec/batch; 17h:32m:26s remains)
INFO - root - 2017-12-07 02:21:01.527349: step 940, loss = 2.06, batch loss = 2.00 (40.6 examples/sec; 0.394 sec/batch; 18h:05m:43s remains)
INFO - root - 2017-12-07 02:21:05.408213: step 950, loss = 2.08, batch loss = 2.02 (41.3 examples/sec; 0.388 sec/batch; 17h:47m:50s remains)
INFO - root - 2017-12-07 02:21:09.252068: step 960, loss = 2.08, batch loss = 2.02 (41.3 examples/sec; 0.388 sec/batch; 17h:48m:19s remains)
INFO - root - 2017-12-07 02:21:13.092125: step 970, loss = 2.06, batch loss = 2.00 (40.7 examples/sec; 0.393 sec/batch; 18h:01m:38s remains)
INFO - root - 2017-12-07 02:21:16.916761: step 980, loss = 2.08, batch loss = 2.02 (41.4 examples/sec; 0.387 sec/batch; 17h:45m:41s remains)
INFO - root - 2017-12-07 02:21:20.740426: step 990, loss = 2.07, batch loss = 2.01 (41.5 examples/sec; 0.386 sec/batch; 17h:42m:13s remains)
INFO - root - 2017-12-07 02:21:26.108234: step 1000, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 1.387 sec/batch; 63h:38m:54s remains)
2017-12-07 02:21:26.870674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2899542 -4.2837133 -4.27422 -4.2702584 -4.27297 -4.2762294 -4.2726445 -4.2608504 -4.2519917 -4.2528596 -4.2583146 -4.2509451 -4.2463236 -4.2405081 -4.2337723][-4.2746973 -4.2710886 -4.2618265 -4.2589874 -4.2669578 -4.27115 -4.268055 -4.2547526 -4.2468309 -4.2445078 -4.2453966 -4.2343035 -4.2226181 -4.2166085 -4.2156348][-4.2539053 -4.2497215 -4.2421117 -4.2449784 -4.2581234 -4.2622995 -4.2608924 -4.2526789 -4.2491765 -4.24284 -4.2365637 -4.2215762 -4.2083392 -4.2044873 -4.2133594][-4.2362876 -4.2288718 -4.224215 -4.2348137 -4.2483788 -4.25002 -4.2485824 -4.2440848 -4.2447238 -4.2419934 -4.2351842 -4.2195559 -4.2048655 -4.202322 -4.2169642][-4.2180209 -4.2095957 -4.2014241 -4.2037044 -4.207355 -4.2053375 -4.20124 -4.197835 -4.20123 -4.2106042 -4.215261 -4.2054682 -4.1904297 -4.1914897 -4.2140832][-4.1971054 -4.1808715 -4.1541967 -4.1392174 -4.1277823 -4.1179023 -4.1091285 -4.0970359 -4.0957046 -4.1262407 -4.1569777 -4.1670828 -4.1586566 -4.166256 -4.1972518][-4.1794705 -4.1496344 -4.1038704 -4.0647812 -4.0314074 -4.0162592 -4.0113792 -3.9960597 -3.9848194 -4.0361872 -4.0990682 -4.1312432 -4.1382742 -4.149991 -4.1829638][-4.1931405 -4.1667256 -4.1248507 -4.0818357 -4.0410519 -4.0289869 -4.0392265 -4.0410342 -4.0379238 -4.077806 -4.1302843 -4.1576881 -4.1601205 -4.1622105 -4.1880112][-4.2373986 -4.2250814 -4.2007251 -4.1676888 -4.133925 -4.1264939 -4.1403909 -4.1493163 -4.1523633 -4.1739058 -4.1966763 -4.2002115 -4.1779304 -4.1578274 -4.1715345][-4.2708931 -4.2699327 -4.2601109 -4.2432489 -4.2287 -4.227355 -4.2365613 -4.24551 -4.2524834 -4.2628851 -4.2659621 -4.2496667 -4.2085495 -4.1687088 -4.1656237][-4.2728686 -4.2768946 -4.2764115 -4.2779522 -4.2813482 -4.2865057 -4.2921677 -4.2970982 -4.3038764 -4.3102722 -4.310554 -4.288342 -4.2435627 -4.1974516 -4.1878][-4.2559409 -4.2577624 -4.262536 -4.27243 -4.2874784 -4.3017507 -4.3133278 -4.3183308 -4.3215995 -4.3259 -4.3245225 -4.301487 -4.2617612 -4.227345 -4.2196856][-4.2234683 -4.2258544 -4.2344842 -4.249105 -4.2660017 -4.2851348 -4.3045073 -4.3158679 -4.3223777 -4.3258724 -4.3200641 -4.2995987 -4.2729821 -4.2539272 -4.247663][-4.1861281 -4.1954589 -4.2125049 -4.2306848 -4.2451773 -4.264246 -4.28667 -4.2969656 -4.3025622 -4.3024035 -4.2923422 -4.2756243 -4.2635083 -4.2604761 -4.2579751][-4.1636353 -4.1845264 -4.2089386 -4.2318535 -4.2439704 -4.2539625 -4.2695265 -4.2725434 -4.2693634 -4.2607574 -4.24557 -4.2316265 -4.233007 -4.241621 -4.2463655]]...]
INFO - root - 2017-12-07 02:21:34.418136: step 1010, loss = 2.10, batch loss = 2.05 (19.9 examples/sec; 0.804 sec/batch; 36h:54m:31s remains)
INFO - root - 2017-12-07 02:21:42.326088: step 1020, loss = 2.09, batch loss = 2.03 (20.1 examples/sec; 0.797 sec/batch; 36h:35m:04s remains)
INFO - root - 2017-12-07 02:21:50.226509: step 1030, loss = 2.07, batch loss = 2.01 (19.7 examples/sec; 0.813 sec/batch; 37h:18m:33s remains)
INFO - root - 2017-12-07 02:21:58.144665: step 1040, loss = 2.08, batch loss = 2.02 (19.7 examples/sec; 0.811 sec/batch; 37h:12m:00s remains)
INFO - root - 2017-12-07 02:22:06.052538: step 1050, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.784 sec/batch; 35h:57m:20s remains)
INFO - root - 2017-12-07 02:22:14.096069: step 1060, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.785 sec/batch; 36h:02m:16s remains)
INFO - root - 2017-12-07 02:22:22.056574: step 1070, loss = 2.09, batch loss = 2.03 (20.2 examples/sec; 0.792 sec/batch; 36h:19m:38s remains)
INFO - root - 2017-12-07 02:22:29.987208: step 1080, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.782 sec/batch; 35h:54m:03s remains)
INFO - root - 2017-12-07 02:22:37.716337: step 1090, loss = 2.08, batch loss = 2.02 (20.5 examples/sec; 0.780 sec/batch; 35h:47m:26s remains)
INFO - root - 2017-12-07 02:22:45.690526: step 1100, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.786 sec/batch; 36h:02m:37s remains)
2017-12-07 02:22:46.412423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0933743 -4.0721602 -4.0319128 -4.0145264 -4.0231447 -4.0551844 -4.08338 -4.0831623 -4.0671496 -4.0624647 -4.072093 -4.0909662 -4.0981445 -4.0845079 -4.0732656][-4.0783267 -4.0683579 -4.045464 -4.0446806 -4.0653515 -4.1023359 -4.1257286 -4.1209803 -4.1027746 -4.097033 -4.105629 -4.1229773 -4.1291876 -4.1124268 -4.0976629][-4.0952511 -4.1002212 -4.0986557 -4.1114678 -4.13581 -4.1646771 -4.1750808 -4.1616917 -4.1407313 -4.1331949 -4.1398735 -4.1547251 -4.1610045 -4.1440344 -4.1260371][-4.1308808 -4.1469564 -4.1569004 -4.1721692 -4.1912608 -4.2050915 -4.1966758 -4.1695142 -4.14237 -4.1323314 -4.1392035 -4.1551986 -4.1654572 -4.1551743 -4.1383371][-4.1654739 -4.1848717 -4.1954556 -4.2021651 -4.2057104 -4.1991706 -4.1717515 -4.1302738 -4.0957217 -4.084712 -4.0999184 -4.1265335 -4.1456518 -4.1440511 -4.1269088][-4.1913857 -4.2074232 -4.2110114 -4.2024679 -4.1830845 -4.1535873 -4.1091681 -4.056107 -4.0155892 -4.0111685 -4.048357 -4.1003561 -4.1314435 -4.1293731 -4.1039019][-4.210042 -4.2173123 -4.2069712 -4.1778088 -4.1347227 -4.0852861 -4.0285983 -3.9669335 -3.921351 -3.9240804 -3.9867752 -4.0622783 -4.103487 -4.1008029 -4.0718083][-4.2205215 -4.2141657 -4.1864595 -4.1383972 -4.0786562 -4.0188732 -3.9610713 -3.9035597 -3.8614573 -3.8665676 -3.9341252 -4.0144854 -4.0633116 -4.068069 -4.0502906][-4.2193713 -4.2006125 -4.159997 -4.101738 -4.0369849 -3.9796841 -3.935766 -3.8986726 -3.8707952 -3.8749204 -3.9258113 -3.9943726 -4.043179 -4.0593047 -4.0581965][-4.2089167 -4.1848397 -4.1395807 -4.0819397 -4.0243821 -3.983021 -3.9604328 -3.945045 -3.9294295 -3.9309478 -3.962328 -4.0148735 -4.0578012 -4.0803838 -4.0879955][-4.19793 -4.1759491 -4.1367326 -4.0899234 -4.04949 -4.0287251 -4.0221424 -4.0178161 -4.0097418 -4.0133224 -4.0332966 -4.0681252 -4.098228 -4.1159844 -4.1220746][-4.1940622 -4.1804409 -4.1545434 -4.12333 -4.1005349 -4.0923104 -4.0927315 -4.0945654 -4.0944586 -4.1022792 -4.1161661 -4.1356187 -4.14869 -4.1544962 -4.1523714][-4.2007256 -4.1966248 -4.1826568 -4.163908 -4.1517353 -4.14954 -4.1538706 -4.1608248 -4.1665187 -4.1757016 -4.1857686 -4.1933508 -4.1923342 -4.1881943 -4.1819658][-4.2180843 -4.2194686 -4.2127504 -4.2010264 -4.1931691 -4.1921096 -4.1975279 -4.2062612 -4.2149434 -4.2241354 -4.2311025 -4.231564 -4.2246184 -4.2171845 -4.2107334][-4.2441573 -4.245295 -4.2408404 -4.2321119 -4.2254429 -4.2233119 -4.2268038 -4.2339759 -4.2429862 -4.2525296 -4.2589378 -4.2572908 -4.250175 -4.2430243 -4.2384458]]...]
INFO - root - 2017-12-07 02:22:54.308022: step 1110, loss = 2.10, batch loss = 2.04 (20.1 examples/sec; 0.794 sec/batch; 36h:25m:53s remains)
INFO - root - 2017-12-07 02:23:02.222196: step 1120, loss = 2.07, batch loss = 2.01 (19.7 examples/sec; 0.812 sec/batch; 37h:14m:51s remains)
INFO - root - 2017-12-07 02:23:10.114560: step 1130, loss = 2.08, batch loss = 2.03 (20.0 examples/sec; 0.799 sec/batch; 36h:38m:14s remains)
INFO - root - 2017-12-07 02:23:18.016046: step 1140, loss = 2.10, batch loss = 2.04 (20.8 examples/sec; 0.768 sec/batch; 35h:12m:40s remains)
INFO - root - 2017-12-07 02:23:25.889956: step 1150, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.790 sec/batch; 36h:14m:41s remains)
INFO - root - 2017-12-07 02:23:33.850391: step 1160, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.798 sec/batch; 36h:36m:25s remains)
INFO - root - 2017-12-07 02:23:41.764616: step 1170, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.802 sec/batch; 36h:45m:57s remains)
INFO - root - 2017-12-07 02:23:49.452717: step 1180, loss = 2.09, batch loss = 2.03 (20.5 examples/sec; 0.781 sec/batch; 35h:48m:06s remains)
INFO - root - 2017-12-07 02:23:57.377402: step 1190, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.798 sec/batch; 36h:34m:27s remains)
INFO - root - 2017-12-07 02:24:05.309582: step 1200, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.808 sec/batch; 37h:01m:26s remains)
2017-12-07 02:24:05.980852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120737 -4.31073 -4.3126111 -4.3146605 -4.3141 -4.314023 -4.3111272 -4.30549 -4.3046832 -4.3092537 -4.3155713 -4.3211374 -4.3254819 -4.3282466 -4.3334188][-4.3000436 -4.2942586 -4.2941446 -4.293375 -4.2915578 -4.2919307 -4.2866335 -4.276505 -4.2751822 -4.2820988 -4.2901821 -4.2983003 -4.3067083 -4.3138423 -4.3232484][-4.2913265 -4.2793155 -4.2760167 -4.2703552 -4.2637644 -4.2588272 -4.2452993 -4.2218 -4.2155576 -4.2242956 -4.2335997 -4.245924 -4.2639432 -4.2820296 -4.3006225][-4.2717834 -4.2498603 -4.2420883 -4.232049 -4.2196579 -4.2078876 -4.1869512 -4.1509619 -4.1392016 -4.1494422 -4.1600466 -4.176929 -4.2056665 -4.2377043 -4.2676072][-4.2397103 -4.2016296 -4.1808915 -4.1605239 -4.1416173 -4.1257148 -4.09971 -4.0571313 -4.0511608 -4.0733986 -4.0883889 -4.1116824 -4.1528659 -4.198061 -4.2350121][-4.2080564 -4.1507845 -4.1152878 -4.0827479 -4.0511775 -4.0311184 -3.998281 -3.9539773 -3.9660084 -4.0123196 -4.0434546 -4.0784769 -4.1270957 -4.175395 -4.2110972][-4.191565 -4.1221628 -4.0718622 -4.0265994 -3.9850295 -3.9616883 -3.9171319 -3.8731778 -3.9122143 -3.9855905 -4.0363326 -4.0850606 -4.13457 -4.1767035 -4.2016883][-4.1959658 -4.1256928 -4.0723505 -4.0242929 -3.9776356 -3.9519753 -3.900641 -3.8634553 -3.9210904 -4.0083909 -4.0721884 -4.1303329 -4.1758518 -4.2043257 -4.2138085][-4.2124429 -4.1542139 -4.1114974 -4.0779233 -4.0397825 -4.0165787 -3.9721093 -3.9482014 -4.000267 -4.0722647 -4.1301937 -4.1829457 -4.2178731 -4.2335835 -4.234952][-4.2353096 -4.1933155 -4.1650624 -4.1430693 -4.1165171 -4.1014128 -4.0754461 -4.066267 -4.1028576 -4.1511183 -4.18829 -4.2252045 -4.2507982 -4.2610974 -4.2628675][-4.2591319 -4.2356291 -4.218328 -4.2005124 -4.1797466 -4.1734505 -4.1652651 -4.1695924 -4.1934123 -4.22175 -4.240972 -4.2621236 -4.2818666 -4.2915082 -4.2969651][-4.2778482 -4.2650008 -4.2550092 -4.2423739 -4.2293153 -4.2303462 -4.2327371 -4.241271 -4.2572055 -4.2761474 -4.2837448 -4.2927842 -4.3068919 -4.3173046 -4.3250284][-4.293448 -4.2823486 -4.2744865 -4.267179 -4.260839 -4.2653666 -4.2719517 -4.2805591 -4.2891631 -4.3007464 -4.3044443 -4.3077173 -4.3166528 -4.32783 -4.3378534][-4.3054066 -4.2976665 -4.2923131 -4.28892 -4.2862272 -4.2904258 -4.2966328 -4.3025475 -4.3064866 -4.3107648 -4.3104682 -4.311326 -4.3176627 -4.3279982 -4.3373175][-4.3174353 -4.3130379 -4.3109026 -4.3098011 -4.3092084 -4.3116655 -4.3155546 -4.3195066 -4.3213196 -4.3209648 -4.3178983 -4.3174062 -4.3207393 -4.3267927 -4.3320351]]...]
INFO - root - 2017-12-07 02:24:13.869347: step 1210, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.784 sec/batch; 35h:55m:42s remains)
INFO - root - 2017-12-07 02:24:21.791461: step 1220, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.794 sec/batch; 36h:23m:04s remains)
INFO - root - 2017-12-07 02:24:29.664898: step 1230, loss = 2.08, batch loss = 2.02 (20.5 examples/sec; 0.782 sec/batch; 35h:49m:26s remains)
INFO - root - 2017-12-07 02:24:37.587476: step 1240, loss = 2.07, batch loss = 2.01 (20.6 examples/sec; 0.776 sec/batch; 35h:34m:22s remains)
INFO - root - 2017-12-07 02:24:45.476834: step 1250, loss = 2.06, batch loss = 2.01 (20.1 examples/sec; 0.797 sec/batch; 36h:31m:06s remains)
INFO - root - 2017-12-07 02:24:53.429537: step 1260, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.777 sec/batch; 35h:37m:01s remains)
INFO - root - 2017-12-07 02:25:01.170012: step 1270, loss = 2.06, batch loss = 2.01 (19.9 examples/sec; 0.806 sec/batch; 36h:55m:14s remains)
INFO - root - 2017-12-07 02:25:09.180978: step 1280, loss = 2.06, batch loss = 2.00 (19.9 examples/sec; 0.806 sec/batch; 36h:55m:22s remains)
INFO - root - 2017-12-07 02:25:17.094751: step 1290, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.788 sec/batch; 36h:07m:47s remains)
INFO - root - 2017-12-07 02:25:25.011454: step 1300, loss = 2.07, batch loss = 2.02 (20.3 examples/sec; 0.790 sec/batch; 36h:10m:41s remains)
2017-12-07 02:25:25.698179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0874496 -4.0859814 -4.1013207 -4.0984106 -4.0927663 -4.0890207 -4.0857177 -4.0818615 -4.10115 -4.1178284 -4.0911 -4.0890312 -4.1488991 -4.2059236 -4.2520108][-4.1139545 -4.1124544 -4.1187119 -4.1075068 -4.0982666 -4.0914116 -4.089776 -4.0934753 -4.1132121 -4.1236262 -4.0976 -4.1044297 -4.1654429 -4.2176538 -4.258975][-4.1434388 -4.13864 -4.129838 -4.1080065 -4.0999169 -4.0915804 -4.0876951 -4.0966854 -4.1201129 -4.1313624 -4.1106715 -4.1234345 -4.185389 -4.2317219 -4.2668481][-4.1734457 -4.1654348 -4.1481738 -4.1232028 -4.109766 -4.0891094 -4.0685577 -4.0690479 -4.0923781 -4.1114926 -4.10828 -4.1308541 -4.1946392 -4.2390056 -4.2736068][-4.1843915 -4.1771603 -4.1576328 -4.1265965 -4.1065359 -4.0736065 -4.0404606 -4.030777 -4.0546513 -4.0851412 -4.0945129 -4.121069 -4.18817 -4.2367315 -4.2752666][-4.15631 -4.1452041 -4.1189947 -4.0791941 -4.0474782 -4.0040865 -3.9657445 -3.9690261 -4.012991 -4.061975 -4.0810108 -4.1106687 -4.1826892 -4.237071 -4.2789645][-4.0956497 -4.0687866 -4.0298581 -3.9813998 -3.9332047 -3.8685722 -3.81338 -3.8447671 -3.929455 -4.0007577 -4.0295753 -4.0701332 -4.1590209 -4.2307425 -4.2797632][-4.0449553 -3.9988251 -3.9468017 -3.8954065 -3.8375878 -3.74507 -3.657567 -3.7153964 -3.8479259 -3.933744 -3.9654853 -4.0176225 -4.1212635 -4.210412 -4.2689414][-4.065052 -4.02192 -3.9761972 -3.933517 -3.8886716 -3.8124242 -3.7310679 -3.7744222 -3.890759 -3.9597542 -3.982883 -4.027267 -4.1191821 -4.2062836 -4.265883][-4.1058531 -4.067543 -4.028471 -3.9983473 -3.9729908 -3.9336355 -3.8823662 -3.9113638 -3.9902592 -4.0332742 -4.0486689 -4.0903983 -4.1657667 -4.2359872 -4.2851062][-4.1288881 -4.0955787 -4.0657573 -4.0541449 -4.0476055 -4.03299 -4.0117478 -4.0389929 -4.0874205 -4.1116509 -4.1232257 -4.1589179 -4.2180548 -4.2708578 -4.3069735][-4.1406207 -4.1110511 -4.0928783 -4.1030507 -4.1097136 -4.1015911 -4.0980287 -4.1284981 -4.1628947 -4.1766243 -4.1808572 -4.2053037 -4.2503943 -4.2879519 -4.3162451][-4.1488881 -4.12791 -4.1224771 -4.1446438 -4.1608634 -4.1563182 -4.1564384 -4.1795187 -4.2006478 -4.204421 -4.2040625 -4.2199931 -4.2541308 -4.2858357 -4.3124704][-4.1460538 -4.1318421 -4.1310668 -4.155489 -4.1771607 -4.1769104 -4.1735964 -4.183311 -4.1880064 -4.1867714 -4.1860132 -4.2014122 -4.2348909 -4.2684965 -4.2977028][-4.1571665 -4.1485395 -4.1484485 -4.16467 -4.18001 -4.1784177 -4.1689167 -4.1663489 -4.1622195 -4.1625643 -4.1669879 -4.185617 -4.2210541 -4.2564259 -4.2867455]]...]
INFO - root - 2017-12-07 02:25:33.558200: step 1310, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.808 sec/batch; 37h:01m:29s remains)
INFO - root - 2017-12-07 02:25:41.550753: step 1320, loss = 2.09, batch loss = 2.03 (20.1 examples/sec; 0.795 sec/batch; 36h:26m:12s remains)
INFO - root - 2017-12-07 02:25:49.453991: step 1330, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.802 sec/batch; 36h:44m:16s remains)
INFO - root - 2017-12-07 02:25:57.387959: step 1340, loss = 2.08, batch loss = 2.02 (20.6 examples/sec; 0.775 sec/batch; 35h:29m:45s remains)
INFO - root - 2017-12-07 02:26:05.398973: step 1350, loss = 2.07, batch loss = 2.01 (19.6 examples/sec; 0.818 sec/batch; 37h:27m:20s remains)
INFO - root - 2017-12-07 02:26:13.137675: step 1360, loss = 2.07, batch loss = 2.01 (20.8 examples/sec; 0.771 sec/batch; 35h:18m:33s remains)
INFO - root - 2017-12-07 02:26:21.049225: step 1370, loss = 2.10, batch loss = 2.04 (20.3 examples/sec; 0.788 sec/batch; 36h:05m:00s remains)
INFO - root - 2017-12-07 02:26:28.969245: step 1380, loss = 2.07, batch loss = 2.01 (20.5 examples/sec; 0.780 sec/batch; 35h:44m:33s remains)
INFO - root - 2017-12-07 02:26:36.884130: step 1390, loss = 2.06, batch loss = 2.00 (20.6 examples/sec; 0.776 sec/batch; 35h:31m:15s remains)
INFO - root - 2017-12-07 02:26:44.802519: step 1400, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.787 sec/batch; 36h:02m:26s remains)
2017-12-07 02:26:45.473371: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0294542 -4.0312428 -4.0954161 -4.183022 -4.2696075 -4.3310504 -4.3577204 -4.35845 -4.3476472 -4.3349972 -4.3305125 -4.32763 -4.3243675 -4.3208923 -4.3176551][-3.890974 -3.9132564 -4.0198164 -4.1472654 -4.2577996 -4.327796 -4.3555918 -4.3567381 -4.3479671 -4.3408175 -4.3407679 -4.3392015 -4.3353019 -4.3294296 -4.3223805][-3.8039484 -3.8618774 -4.0076318 -4.1561718 -4.2638144 -4.3186941 -4.3317924 -4.3247247 -4.3177 -4.3208623 -4.3320665 -4.3399782 -4.3423309 -4.3386459 -4.329391][-3.8706775 -3.9465253 -4.0810719 -4.2027011 -4.2752471 -4.2960181 -4.28216 -4.2633886 -4.2621474 -4.2814789 -4.311089 -4.334929 -4.3464985 -4.3459234 -4.3356848][-4.0186954 -4.0822592 -4.1730461 -4.2423973 -4.2638707 -4.2393112 -4.1957664 -4.1684895 -4.1817279 -4.2254367 -4.2789564 -4.3219337 -4.3446989 -4.347218 -4.3371654][-4.1520896 -4.1914616 -4.2333961 -4.2452426 -4.2099967 -4.1371417 -4.0611997 -4.0300198 -4.067646 -4.1434894 -4.2249317 -4.2916193 -4.3301716 -4.3390017 -4.3324871][-4.2410836 -4.2541552 -4.2493386 -4.2051253 -4.1169057 -4.0002608 -3.8977442 -3.8738527 -3.9487913 -4.0628505 -4.1744781 -4.2607346 -4.3085046 -4.3231893 -4.3217111][-4.2861218 -4.2753878 -4.2368283 -4.1525683 -4.0260224 -3.8831861 -3.7733395 -3.7710719 -3.8809261 -4.025157 -4.1529531 -4.2438664 -4.29282 -4.3096547 -4.31098][-4.2973118 -4.2724767 -4.2193546 -4.124918 -4.0006266 -3.8814082 -3.8099995 -3.834687 -3.9431832 -4.0751982 -4.1864009 -4.2590785 -4.2936993 -4.3052392 -4.3061109][-4.2980762 -4.2681651 -4.216753 -4.1384044 -4.0489616 -3.9792743 -3.9512038 -3.9852915 -4.0699148 -4.1677279 -4.2450361 -4.2872381 -4.301548 -4.3046103 -4.303185][-4.3014507 -4.27519 -4.2362757 -4.1838832 -4.132165 -4.1022472 -4.1011677 -4.1333694 -4.1904755 -4.2503257 -4.2904081 -4.305634 -4.3050508 -4.3025975 -4.3010063][-4.307498 -4.2911859 -4.2690587 -4.2395349 -4.2140036 -4.2061257 -4.214849 -4.2380047 -4.2684059 -4.2940931 -4.3069429 -4.3064871 -4.3004551 -4.2980952 -4.300118][-4.3134851 -4.3061428 -4.2972732 -4.2840137 -4.2733493 -4.2731032 -4.2814679 -4.293025 -4.3026128 -4.306262 -4.3046112 -4.2996907 -4.2976184 -4.2996511 -4.304059][-4.3179612 -4.3158736 -4.3147507 -4.3105783 -4.3066082 -4.3069677 -4.3107204 -4.3123856 -4.3108544 -4.3063989 -4.3019085 -4.3002763 -4.3023925 -4.3059816 -4.3090153][-4.3224745 -4.323339 -4.3259296 -4.3266859 -4.3264751 -4.3272228 -4.3275213 -4.3251257 -4.3203592 -4.3143277 -4.3102717 -4.3095841 -4.3106728 -4.3111734 -4.3104811]]...]
INFO - root - 2017-12-07 02:26:53.398004: step 1410, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.789 sec/batch; 36h:06m:52s remains)
INFO - root - 2017-12-07 02:27:01.277750: step 1420, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.793 sec/batch; 36h:19m:04s remains)
INFO - root - 2017-12-07 02:27:09.195540: step 1430, loss = 2.10, batch loss = 2.04 (21.0 examples/sec; 0.762 sec/batch; 34h:54m:01s remains)
INFO - root - 2017-12-07 02:27:17.034047: step 1440, loss = 2.08, batch loss = 2.02 (22.6 examples/sec; 0.709 sec/batch; 32h:27m:05s remains)
INFO - root - 2017-12-07 02:27:24.957112: step 1450, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.797 sec/batch; 36h:28m:20s remains)
INFO - root - 2017-12-07 02:27:32.924645: step 1460, loss = 2.08, batch loss = 2.02 (20.7 examples/sec; 0.774 sec/batch; 35h:24m:55s remains)
INFO - root - 2017-12-07 02:27:40.869706: step 1470, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.793 sec/batch; 36h:16m:46s remains)
INFO - root - 2017-12-07 02:27:48.885996: step 1480, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.795 sec/batch; 36h:24m:26s remains)
INFO - root - 2017-12-07 02:27:56.851218: step 1490, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.792 sec/batch; 36h:16m:02s remains)
INFO - root - 2017-12-07 02:28:04.782481: step 1500, loss = 2.09, batch loss = 2.03 (20.3 examples/sec; 0.790 sec/batch; 36h:08m:02s remains)
2017-12-07 02:28:05.553349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418208 -4.2546358 -4.2736497 -4.2928591 -4.3018217 -4.3048706 -4.2963529 -4.2842851 -4.2693195 -4.252377 -4.243897 -4.2498512 -4.2697086 -4.2958441 -4.3169637][-4.2157617 -4.2337546 -4.2567697 -4.2793245 -4.2875819 -4.2857866 -4.2723951 -4.2595024 -4.2458372 -4.2223177 -4.2076464 -4.2094131 -4.2264338 -4.2535429 -4.2780404][-4.1826448 -4.2056518 -4.2313251 -4.2527714 -4.2584348 -4.2514591 -4.2315173 -4.2180161 -4.2042561 -4.1758471 -4.1564164 -4.1578689 -4.1730704 -4.2038889 -4.2327528][-4.13931 -4.1618714 -4.1860247 -4.2026782 -4.2032185 -4.1938562 -4.1745939 -4.1640754 -4.1524663 -4.1226449 -4.1029377 -4.1096563 -4.1299686 -4.164145 -4.1998067][-4.0847836 -4.0986686 -4.1166077 -4.1264935 -4.1193929 -4.1059394 -4.0908637 -4.0907598 -4.0886641 -4.0619698 -4.0418963 -4.0575953 -4.0875955 -4.1246605 -4.1654835][-4.024519 -4.0228553 -4.0272121 -4.0240974 -4.0111203 -3.9954388 -3.9906874 -4.0111337 -4.0242624 -4.0037546 -3.9855545 -4.0041757 -4.0376749 -4.0759912 -4.1170983][-3.9710226 -3.9560189 -3.9477777 -3.9329665 -3.9134536 -3.8978584 -3.901484 -3.9356129 -3.9610622 -3.9547057 -3.9402013 -3.9513125 -3.9793231 -4.0149269 -4.0535836][-3.9250419 -3.903249 -3.8868976 -3.8660834 -3.8451488 -3.8344183 -3.8432264 -3.8826919 -3.917716 -3.9269536 -3.9116943 -3.9074297 -3.9229 -3.9519122 -3.9865639][-3.8986204 -3.8749313 -3.85956 -3.8410959 -3.8254285 -3.8221035 -3.8339946 -3.8675227 -3.8983073 -3.9116082 -3.8950756 -3.881886 -3.8901796 -3.9156327 -3.9483507][-3.919975 -3.8972466 -3.8852835 -3.8716362 -3.8625507 -3.8639209 -3.8717117 -3.8924985 -3.9083512 -3.9150734 -3.9030788 -3.8916011 -3.9010282 -3.9286656 -3.9665005][-3.9878819 -3.9691014 -3.9591503 -3.9484088 -3.9425564 -3.9427783 -3.946172 -3.9584661 -3.9654727 -3.96858 -3.963433 -3.9587934 -3.9693251 -3.9941795 -4.0285473][-4.0753784 -4.064198 -4.0599914 -4.053452 -4.0488029 -4.0472441 -4.0474238 -4.054554 -4.0601492 -4.0636187 -4.0620689 -4.0612278 -4.0706363 -4.0883875 -4.1127911][-4.1575942 -4.15268 -4.1529016 -4.1517138 -4.1506982 -4.1488209 -4.1479535 -4.151113 -4.1544313 -4.1553631 -4.1543303 -4.1538405 -4.1592884 -4.1711569 -4.188715][-4.2186632 -4.21631 -4.2183647 -4.2196 -4.220098 -4.219131 -4.2173657 -4.217063 -4.2181129 -4.2191968 -4.2194843 -4.2197857 -4.2233734 -4.2317057 -4.2436562][-4.25688 -4.2544723 -4.2558012 -4.2567587 -4.2575088 -4.2573109 -4.2559204 -4.2552195 -4.2557583 -4.2572808 -4.2591166 -4.2607026 -4.2635269 -4.267942 -4.273468]]...]
INFO - root - 2017-12-07 02:28:13.507457: step 1510, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.809 sec/batch; 37h:02m:22s remains)
INFO - root - 2017-12-07 02:28:21.373918: step 1520, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.790 sec/batch; 36h:09m:00s remains)
INFO - root - 2017-12-07 02:28:29.187802: step 1530, loss = 2.10, batch loss = 2.04 (20.7 examples/sec; 0.774 sec/batch; 35h:23m:31s remains)
INFO - root - 2017-12-07 02:28:37.107931: step 1540, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.793 sec/batch; 36h:17m:23s remains)
INFO - root - 2017-12-07 02:28:45.002159: step 1550, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.789 sec/batch; 36h:05m:14s remains)
INFO - root - 2017-12-07 02:28:52.899907: step 1560, loss = 2.07, batch loss = 2.01 (20.7 examples/sec; 0.773 sec/batch; 35h:22m:47s remains)
INFO - root - 2017-12-07 02:29:00.805514: step 1570, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.791 sec/batch; 36h:11m:24s remains)
INFO - root - 2017-12-07 02:29:08.723653: step 1580, loss = 2.09, batch loss = 2.03 (20.5 examples/sec; 0.781 sec/batch; 35h:43m:37s remains)
INFO - root - 2017-12-07 02:29:16.664848: step 1590, loss = 2.09, batch loss = 2.03 (20.3 examples/sec; 0.789 sec/batch; 36h:06m:19s remains)
INFO - root - 2017-12-07 02:29:24.588373: step 1600, loss = 2.06, batch loss = 2.00 (20.4 examples/sec; 0.784 sec/batch; 35h:50m:13s remains)
2017-12-07 02:29:25.245715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3534131 -4.3522124 -4.3508973 -4.3540668 -4.361567 -4.3654361 -4.3612065 -4.3570538 -4.3598094 -4.3646197 -4.36832 -4.3696632 -4.3655286 -4.3559413 -4.3449874][-4.3389993 -4.3361893 -4.3347416 -4.339745 -4.3515525 -4.3580089 -4.3525267 -4.3460345 -4.350246 -4.3568769 -4.3607965 -4.3633971 -4.3591151 -4.3488989 -4.3363876][-4.3247247 -4.3179364 -4.3138933 -4.3154025 -4.3264165 -4.3307991 -4.320241 -4.3107562 -4.3162246 -4.3242879 -4.3310609 -4.3378534 -4.3386383 -4.3326273 -4.3235035][-4.3107305 -4.298152 -4.2903986 -4.2862883 -4.2925315 -4.2893834 -4.2707124 -4.2557616 -4.2605104 -4.2705517 -4.282908 -4.295506 -4.3045254 -4.3076153 -4.3061662][-4.28483 -4.2615638 -4.2455454 -4.2376666 -4.2375908 -4.2278352 -4.2012939 -4.1829052 -4.18758 -4.2009192 -4.2190866 -4.2354593 -4.2552085 -4.2734022 -4.2834091][-4.2449293 -4.2080283 -4.1827273 -4.1692963 -4.15929 -4.1403246 -4.1104183 -4.097106 -4.1039834 -4.1181078 -4.139277 -4.1607237 -4.1970267 -4.2344036 -4.2583046][-4.1951742 -4.1448936 -4.1084819 -4.0840688 -4.0620918 -4.0357313 -4.0057721 -3.9929235 -3.9972141 -4.0096889 -4.0350113 -4.0696144 -4.1302133 -4.1925616 -4.2332611][-4.1469736 -4.085968 -4.0379686 -4.0048356 -3.9782164 -3.9483027 -3.9133759 -3.891679 -3.8890882 -3.8949211 -3.9187427 -3.9686072 -4.0572662 -4.1454673 -4.2048907][-4.1056805 -4.044508 -3.9923406 -3.9551136 -3.9267812 -3.8960109 -3.8521969 -3.8205125 -3.81878 -3.8284907 -3.8539805 -3.9141545 -4.018858 -4.1216083 -4.191505][-4.1026754 -4.0537391 -4.0107331 -3.9767365 -3.9490247 -3.9205825 -3.8775167 -3.8487077 -3.8562469 -3.8746049 -3.9042437 -3.962074 -4.0512519 -4.1410017 -4.2051864][-4.1433911 -4.1083488 -4.0804243 -4.0596104 -4.03948 -4.0218687 -3.9945731 -3.9799767 -3.9914775 -4.0088563 -4.0340972 -4.0762229 -4.1356344 -4.1981816 -4.2434063][-4.1952457 -4.1754279 -4.1645217 -4.1584878 -4.1500273 -4.1433792 -4.1336412 -4.1320605 -4.1420159 -4.1543779 -4.1723814 -4.1999717 -4.2333732 -4.2676606 -4.2911849][-4.2471251 -4.2376184 -4.2373276 -4.2394547 -4.2365985 -4.2348766 -4.2337832 -4.2377176 -4.2445478 -4.2513423 -4.2635889 -4.2827053 -4.3015571 -4.3176112 -4.3257275][-4.2820969 -4.2786756 -4.28193 -4.2852521 -4.285007 -4.2871389 -4.2887979 -4.292397 -4.2966108 -4.3003488 -4.3078566 -4.3209743 -4.3320422 -4.3388095 -4.3398833][-4.3077908 -4.3068709 -4.3099751 -4.3122416 -4.31214 -4.3134279 -4.3144336 -4.3159986 -4.3179989 -4.3203449 -4.3250513 -4.3333735 -4.3407955 -4.3445692 -4.3443518]]...]
INFO - root - 2017-12-07 02:29:33.149782: step 1610, loss = 2.07, batch loss = 2.01 (20.7 examples/sec; 0.774 sec/batch; 35h:23m:24s remains)
INFO - root - 2017-12-07 02:29:40.892405: step 1620, loss = 2.09, batch loss = 2.03 (20.5 examples/sec; 0.781 sec/batch; 35h:43m:01s remains)
INFO - root - 2017-12-07 02:29:48.828483: step 1630, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.784 sec/batch; 35h:49m:59s remains)
INFO - root - 2017-12-07 02:29:56.746414: step 1640, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.793 sec/batch; 36h:15m:20s remains)
INFO - root - 2017-12-07 02:30:04.670229: step 1650, loss = 2.08, batch loss = 2.02 (19.9 examples/sec; 0.802 sec/batch; 36h:40m:31s remains)
INFO - root - 2017-12-07 02:30:12.633941: step 1660, loss = 2.12, batch loss = 2.06 (19.8 examples/sec; 0.809 sec/batch; 36h:59m:33s remains)
INFO - root - 2017-12-07 02:30:20.502452: step 1670, loss = 2.09, batch loss = 2.03 (20.9 examples/sec; 0.766 sec/batch; 35h:01m:46s remains)
INFO - root - 2017-12-07 02:30:28.405557: step 1680, loss = 2.06, batch loss = 2.01 (21.2 examples/sec; 0.756 sec/batch; 34h:34m:05s remains)
INFO - root - 2017-12-07 02:30:36.340257: step 1690, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.800 sec/batch; 36h:34m:48s remains)
INFO - root - 2017-12-07 02:30:44.261987: step 1700, loss = 2.10, batch loss = 2.04 (19.7 examples/sec; 0.813 sec/batch; 37h:10m:44s remains)
2017-12-07 02:30:44.925733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3496747 -4.3508034 -4.3521886 -4.3527613 -4.3532205 -4.3525739 -4.3509874 -4.3479323 -4.3433743 -4.3390675 -4.3350086 -4.3325419 -4.3335304 -4.3361864 -4.3403316][-4.3487921 -4.3505268 -4.3517361 -4.3515744 -4.3516426 -4.3496146 -4.3449478 -4.3379421 -4.330904 -4.3261557 -4.3247061 -4.3258986 -4.3293176 -4.3332434 -4.3378897][-4.3511682 -4.3538671 -4.3545156 -4.3529739 -4.3509674 -4.3456678 -4.3353033 -4.3207111 -4.3074 -4.3042226 -4.3118038 -4.321578 -4.3303308 -4.33574 -4.3404937][-4.3524671 -4.355185 -4.3553605 -4.3513975 -4.3448596 -4.33313 -4.3118339 -4.2845178 -4.2634192 -4.2668266 -4.2892857 -4.3125024 -4.3295369 -4.3384571 -4.3430004][-4.3521056 -4.3538032 -4.3524408 -4.3443704 -4.3310204 -4.3070917 -4.2693195 -4.2265568 -4.1990914 -4.2119484 -4.2504258 -4.2890887 -4.3190875 -4.3380594 -4.3456173][-4.3503418 -4.3502707 -4.3445215 -4.3324394 -4.3120942 -4.2741613 -4.218431 -4.1608772 -4.1304383 -4.1559491 -4.2103844 -4.2620587 -4.3049188 -4.33359 -4.346375][-4.3462634 -4.3415236 -4.3285232 -4.3081303 -4.2768331 -4.2237468 -4.1490879 -4.0724821 -4.0362191 -4.0777917 -4.1546416 -4.2250047 -4.2827606 -4.3223677 -4.3417006][-4.3431234 -4.3348675 -4.3166795 -4.288229 -4.2487807 -4.1877046 -4.1006122 -4.0040846 -3.9667432 -4.0245686 -4.1189852 -4.2016416 -4.2688828 -4.3150973 -4.3374596][-4.3462443 -4.3370628 -4.3172488 -4.2866759 -4.2476354 -4.1935263 -4.1140661 -4.0258441 -3.9992046 -4.0548816 -4.1428528 -4.2194462 -4.28211 -4.3226743 -4.3399124][-4.3497577 -4.34134 -4.324306 -4.2995181 -4.2681046 -4.2276912 -4.168745 -4.1103573 -4.1039333 -4.1467743 -4.208478 -4.2662878 -4.313066 -4.3387632 -4.3464394][-4.3497176 -4.3429918 -4.3317881 -4.3153796 -4.2933726 -4.260108 -4.2156878 -4.1849685 -4.1970429 -4.231452 -4.2729764 -4.310575 -4.3387032 -4.3518758 -4.3525438][-4.3466458 -4.3415112 -4.3334589 -4.3189549 -4.297605 -4.2650824 -4.2324519 -4.2258039 -4.2513056 -4.2841558 -4.314806 -4.3380027 -4.3526607 -4.3582492 -4.3565469][-4.3430772 -4.3394413 -4.3305588 -4.3111639 -4.281002 -4.2464643 -4.2299471 -4.2477808 -4.2856708 -4.3183002 -4.3406782 -4.3532019 -4.358717 -4.3590474 -4.3559022][-4.3392892 -4.3344369 -4.3183441 -4.2880063 -4.2458363 -4.2115879 -4.2168679 -4.2579966 -4.3041039 -4.3368416 -4.353601 -4.357492 -4.3559775 -4.3526721 -4.3489308][-4.33416 -4.3234463 -4.2983789 -4.25763 -4.208724 -4.1830029 -4.2113137 -4.2701797 -4.3187757 -4.348217 -4.3598084 -4.3570614 -4.3497958 -4.34416 -4.3418117]]...]
INFO - root - 2017-12-07 02:30:52.609843: step 1710, loss = 2.07, batch loss = 2.02 (19.9 examples/sec; 0.806 sec/batch; 36h:49m:46s remains)
INFO - root - 2017-12-07 02:31:00.583406: step 1720, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.802 sec/batch; 36h:38m:17s remains)
INFO - root - 2017-12-07 02:31:08.499538: step 1730, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.800 sec/batch; 36h:32m:59s remains)
INFO - root - 2017-12-07 02:31:16.406155: step 1740, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.797 sec/batch; 36h:24m:08s remains)
INFO - root - 2017-12-07 02:31:24.306189: step 1750, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.782 sec/batch; 35h:43m:37s remains)
INFO - root - 2017-12-07 02:31:32.237265: step 1760, loss = 2.07, batch loss = 2.02 (20.4 examples/sec; 0.784 sec/batch; 35h:48m:01s remains)
INFO - root - 2017-12-07 02:31:40.168485: step 1770, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.783 sec/batch; 35h:45m:48s remains)
INFO - root - 2017-12-07 02:31:48.265079: step 1780, loss = 2.09, batch loss = 2.04 (19.8 examples/sec; 0.807 sec/batch; 36h:52m:06s remains)
INFO - root - 2017-12-07 02:31:56.014236: step 1790, loss = 2.08, batch loss = 2.02 (28.8 examples/sec; 0.556 sec/batch; 25h:22m:55s remains)
INFO - root - 2017-12-07 02:32:03.947231: step 1800, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.810 sec/batch; 37h:00m:05s remains)
2017-12-07 02:32:04.688228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2458749 -4.2865648 -4.308527 -4.3088737 -4.300447 -4.2977581 -4.2991862 -4.2968116 -4.2897553 -4.2803216 -4.2644496 -4.2362747 -4.2135558 -4.2084236 -4.2219515][-4.2103276 -4.2588124 -4.2917738 -4.29728 -4.2871041 -4.2801456 -4.2802682 -4.2827463 -4.2814865 -4.2767749 -4.2663083 -4.2505555 -4.2321692 -4.2180052 -4.2211714][-4.196548 -4.2412086 -4.276073 -4.2855773 -4.2746005 -4.2594991 -4.2573738 -4.2631097 -4.2684803 -4.2728758 -4.2698145 -4.2668366 -4.2563457 -4.2389636 -4.2350445][-4.2006817 -4.2345648 -4.2671671 -4.2795463 -4.2689881 -4.2455373 -4.2346025 -4.2384157 -4.2519712 -4.2674966 -4.2751613 -4.2864032 -4.2828436 -4.2667651 -4.2587156][-4.220305 -4.2450442 -4.2722678 -4.2825179 -4.2690916 -4.2366424 -4.2152181 -4.2137222 -4.2340035 -4.2631311 -4.2841167 -4.3032227 -4.3015671 -4.2851605 -4.273243][-4.2401524 -4.2574997 -4.27833 -4.2802086 -4.2555928 -4.2103171 -4.1711583 -4.1541228 -4.1848106 -4.2382703 -4.2755566 -4.29692 -4.2990756 -4.2840285 -4.2686257][-4.2487411 -4.26178 -4.2759881 -4.26927 -4.2340975 -4.1772332 -4.1147461 -4.0756812 -4.1154957 -4.1958284 -4.2462373 -4.2678642 -4.2753344 -4.265202 -4.2505379][-4.2557416 -4.2682343 -4.2774062 -4.2627783 -4.2211137 -4.1603727 -4.0890827 -4.0370359 -4.0703249 -4.1573024 -4.2128038 -4.2331386 -4.2378926 -4.2287741 -4.2189059][-4.2786088 -4.2873912 -4.2909636 -4.2725282 -4.2315254 -4.1810632 -4.1243825 -4.0792189 -4.0931644 -4.1551003 -4.1989288 -4.2133064 -4.2079077 -4.190979 -4.1796713][-4.2939115 -4.2957306 -4.29441 -4.2735319 -4.2379804 -4.2048564 -4.1728139 -4.1461668 -4.1495986 -4.1841941 -4.211484 -4.2209511 -4.2069435 -4.1772766 -4.1599216][-4.2922282 -4.2858272 -4.2825136 -4.2649546 -4.2375932 -4.2194948 -4.2082381 -4.2003508 -4.2002916 -4.215539 -4.2314539 -4.2371845 -4.2217188 -4.1893959 -4.1728516][-4.2770834 -4.26489 -4.2597985 -4.2505751 -4.2371073 -4.2336817 -4.238184 -4.2402916 -4.2361765 -4.2382903 -4.2441978 -4.2427945 -4.2260017 -4.1994781 -4.1865578][-4.2562938 -4.2381873 -4.2284865 -4.2253313 -4.2282128 -4.23933 -4.2527151 -4.2591543 -4.2535758 -4.2474184 -4.2425747 -4.2342844 -4.2190437 -4.2021813 -4.1923385][-4.2529111 -4.2299891 -4.2165847 -4.2145853 -4.22446 -4.2397428 -4.2530632 -4.2600231 -4.2555161 -4.2470074 -4.2352028 -4.2235894 -4.212739 -4.2072988 -4.2009211][-4.2705646 -4.2505455 -4.238421 -4.2356482 -4.2441468 -4.2548676 -4.2619696 -4.2643819 -4.2561555 -4.2419291 -4.2246065 -4.2096653 -4.2012153 -4.2067504 -4.207871]]...]
INFO - root - 2017-12-07 02:32:12.654447: step 1810, loss = 2.07, batch loss = 2.02 (20.0 examples/sec; 0.801 sec/batch; 36h:36m:00s remains)
INFO - root - 2017-12-07 02:32:20.593260: step 1820, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.785 sec/batch; 35h:50m:03s remains)
INFO - root - 2017-12-07 02:32:28.546495: step 1830, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.800 sec/batch; 36h:32m:43s remains)
INFO - root - 2017-12-07 02:32:36.477253: step 1840, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.801 sec/batch; 36h:35m:01s remains)
INFO - root - 2017-12-07 02:32:44.435457: step 1850, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.798 sec/batch; 36h:25m:58s remains)
INFO - root - 2017-12-07 02:32:52.356565: step 1860, loss = 2.10, batch loss = 2.04 (20.3 examples/sec; 0.788 sec/batch; 35h:58m:19s remains)
INFO - root - 2017-12-07 02:33:00.281717: step 1870, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.797 sec/batch; 36h:23m:21s remains)
INFO - root - 2017-12-07 02:33:08.104137: step 1880, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.800 sec/batch; 36h:30m:32s remains)
INFO - root - 2017-12-07 02:33:16.062240: step 1890, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.789 sec/batch; 36h:02m:16s remains)
INFO - root - 2017-12-07 02:33:23.992005: step 1900, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.786 sec/batch; 35h:53m:17s remains)
2017-12-07 02:33:24.620430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499733 -4.26022 -4.2740364 -4.2703228 -4.251246 -4.2236695 -4.2056689 -4.2102008 -4.2078185 -4.1899776 -4.1862464 -4.2070971 -4.2425208 -4.274137 -4.2981424][-4.1964931 -4.2174463 -4.244401 -4.2467432 -4.2314196 -4.2099524 -4.2006059 -4.2043042 -4.1974597 -4.1825781 -4.1808343 -4.2094283 -4.2542553 -4.2912083 -4.3151846][-4.1696997 -4.204494 -4.246305 -4.2583013 -4.2474842 -4.2275786 -4.2196608 -4.2204633 -4.2098317 -4.1966109 -4.1973619 -4.2248263 -4.2618523 -4.2966642 -4.322001][-4.2024827 -4.2377219 -4.2733603 -4.280405 -4.2664909 -4.2439127 -4.2304115 -4.2236223 -4.2160997 -4.2134113 -4.2190409 -4.2416711 -4.2679358 -4.298327 -4.321322][-4.2456737 -4.2683821 -4.2863908 -4.279706 -4.2544227 -4.2215252 -4.1941547 -4.1832924 -4.193728 -4.2106905 -4.216207 -4.2290635 -4.2497272 -4.2805748 -4.3066912][-4.2717533 -4.2744446 -4.2647657 -4.2363539 -4.1896915 -4.1344838 -4.0833879 -4.0614214 -4.0976424 -4.1478052 -4.1694713 -4.1847715 -4.2132106 -4.2498755 -4.283092][-4.268024 -4.249 -4.2131519 -4.1572471 -4.0847492 -3.9990468 -3.8981776 -3.8466492 -3.9260609 -4.0300632 -4.0862856 -4.1241307 -4.1699157 -4.2175713 -4.2564688][-4.2447214 -4.2106957 -4.1574521 -4.0847564 -3.9961364 -3.8904514 -3.7497211 -3.6737351 -3.8059034 -3.9643309 -4.0492806 -4.1033106 -4.1558237 -4.2049618 -4.2469916][-4.2299457 -4.1936479 -4.1424665 -4.0832119 -4.0204229 -3.9514508 -3.8560541 -3.8087411 -3.9081435 -4.0317988 -4.1037054 -4.1503043 -4.19123 -4.2298365 -4.267828][-4.2353868 -4.2067318 -4.1703897 -4.1376081 -4.1076651 -4.0762219 -4.0317039 -4.0120964 -4.0610018 -4.1277642 -4.1762848 -4.2102189 -4.2368903 -4.2615271 -4.29064][-4.2523427 -4.2296195 -4.2088275 -4.1897035 -4.17795 -4.1710491 -4.1553559 -4.1507287 -4.1710234 -4.1994352 -4.226769 -4.2510667 -4.2714825 -4.2894669 -4.3097253][-4.2711916 -4.2524581 -4.2388887 -4.2275982 -4.2242441 -4.2286248 -4.2284746 -4.2295365 -4.2334146 -4.2439613 -4.260211 -4.2749496 -4.2908249 -4.3065405 -4.3220315][-4.2875252 -4.2720432 -4.2589674 -4.2492676 -4.24645 -4.252388 -4.2581325 -4.2595038 -4.2554617 -4.2587481 -4.2706151 -4.2802181 -4.2927041 -4.30816 -4.3235974][-4.3065896 -4.2902026 -4.2734094 -4.2620058 -4.2598362 -4.2633309 -4.2693477 -4.2707806 -4.2633052 -4.2644997 -4.2750192 -4.2838321 -4.2934408 -4.3037176 -4.3158469][-4.3154716 -4.3027668 -4.2871389 -4.2744989 -4.2712932 -4.2724724 -4.2770023 -4.2799416 -4.2760773 -4.2771931 -4.2828474 -4.2863145 -4.291522 -4.2981558 -4.307322]]...]
INFO - root - 2017-12-07 02:33:32.567549: step 1910, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.788 sec/batch; 35h:58m:22s remains)
INFO - root - 2017-12-07 02:33:40.479190: step 1920, loss = 2.06, batch loss = 2.00 (20.4 examples/sec; 0.785 sec/batch; 35h:50m:07s remains)
INFO - root - 2017-12-07 02:33:48.406233: step 1930, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.785 sec/batch; 35h:48m:45s remains)
INFO - root - 2017-12-07 02:33:56.341918: step 1940, loss = 2.07, batch loss = 2.02 (19.9 examples/sec; 0.804 sec/batch; 36h:42m:48s remains)
INFO - root - 2017-12-07 02:34:04.214264: step 1950, loss = 2.06, batch loss = 2.01 (20.0 examples/sec; 0.801 sec/batch; 36h:34m:32s remains)
INFO - root - 2017-12-07 02:34:12.147873: step 1960, loss = 2.07, batch loss = 2.02 (20.0 examples/sec; 0.800 sec/batch; 36h:31m:42s remains)
INFO - root - 2017-12-07 02:34:19.882818: step 1970, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.789 sec/batch; 35h:59m:44s remains)
INFO - root - 2017-12-07 02:34:27.833855: step 1980, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.795 sec/batch; 36h:16m:13s remains)
INFO - root - 2017-12-07 02:34:35.738969: step 1990, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.794 sec/batch; 36h:13m:12s remains)
INFO - root - 2017-12-07 02:34:43.724317: step 2000, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.783 sec/batch; 35h:43m:51s remains)
2017-12-07 02:34:44.466291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734575 -4.3146806 -4.3309288 -4.3304415 -4.3132272 -4.303216 -4.3005104 -4.2893486 -4.2847772 -4.3008614 -4.3253322 -4.3498883 -4.3673124 -4.3743916 -4.3711305][-4.2853875 -4.336709 -4.3619242 -4.3690982 -4.3595943 -4.3509622 -4.3394785 -4.31557 -4.3000007 -4.3005962 -4.3094335 -4.3277497 -4.3470068 -4.3602743 -4.360816][-4.2993431 -4.3520436 -4.3800392 -4.3852158 -4.3735652 -4.3560071 -4.3300576 -4.2915659 -4.2612844 -4.2475467 -4.2494125 -4.2741632 -4.30708 -4.3349204 -4.3455915][-4.3202391 -4.3621545 -4.3825145 -4.3784924 -4.3598032 -4.3293548 -4.28218 -4.2214551 -4.1759133 -4.1602268 -4.1744056 -4.2196679 -4.27464 -4.317781 -4.3380575][-4.3405914 -4.36605 -4.3721514 -4.3554139 -4.3234015 -4.2708564 -4.1918597 -4.1031804 -4.0568814 -4.0648928 -4.1155806 -4.1940765 -4.2675266 -4.3170223 -4.3394389][-4.3504725 -4.3588791 -4.3458819 -4.3088365 -4.2541018 -4.1727476 -4.0595942 -3.9489174 -3.9249542 -3.9827671 -4.0844483 -4.1953659 -4.274374 -4.3214393 -4.34255][-4.3475933 -4.3410816 -4.3092828 -4.2478542 -4.1682892 -4.0626006 -3.9271162 -3.81754 -3.84204 -3.9510236 -4.0854535 -4.2069488 -4.2813621 -4.3251553 -4.3448586][-4.3339529 -4.3143277 -4.2670379 -4.1867886 -4.0951557 -3.986598 -3.8636866 -3.793139 -3.8616931 -3.9917455 -4.1271181 -4.2352829 -4.2968273 -4.3324251 -4.3475208][-4.3145676 -4.2855425 -4.2311163 -4.1479425 -4.0655775 -3.9854536 -3.9124956 -3.8959413 -3.9790697 -4.0962925 -4.2058921 -4.2852416 -4.3247018 -4.3441992 -4.349515][-4.3049879 -4.277091 -4.2312727 -4.1646218 -4.1078687 -4.0660243 -4.038404 -4.0515161 -4.124599 -4.2127972 -4.2878604 -4.3347187 -4.3525105 -4.3559356 -4.3511443][-4.3130679 -4.29366 -4.2638097 -4.2227645 -4.1967707 -4.1888452 -4.1845431 -4.2009616 -4.2493572 -4.3047276 -4.3468781 -4.3667383 -4.3683171 -4.3635077 -4.3539166][-4.3323126 -4.3225536 -4.3077693 -4.2891054 -4.2848706 -4.2942023 -4.2972875 -4.3044786 -4.3252096 -4.3515062 -4.367631 -4.37072 -4.367826 -4.3642812 -4.3565812][-4.35151 -4.3509207 -4.3488441 -4.3443789 -4.3470893 -4.3534055 -4.34838 -4.3404226 -4.3402982 -4.3493 -4.3518276 -4.3529696 -4.3555336 -4.3584681 -4.355269][-4.3636332 -4.368257 -4.3711133 -4.36976 -4.3706694 -4.3665509 -4.3488216 -4.3238463 -4.308578 -4.3046832 -4.3024764 -4.3128848 -4.3296471 -4.3447843 -4.3492084][-4.3691254 -4.3748937 -4.37759 -4.3742871 -4.3687735 -4.3488507 -4.3110781 -4.2636733 -4.2309728 -4.2205887 -4.2289577 -4.2617607 -4.3009067 -4.3320265 -4.344862]]...]
INFO - root - 2017-12-07 02:34:52.382179: step 2010, loss = 2.07, batch loss = 2.01 (20.6 examples/sec; 0.776 sec/batch; 35h:24m:32s remains)
INFO - root - 2017-12-07 02:35:00.244036: step 2020, loss = 2.06, batch loss = 2.00 (20.5 examples/sec; 0.782 sec/batch; 35h:41m:27s remains)
INFO - root - 2017-12-07 02:35:08.155008: step 2030, loss = 2.08, batch loss = 2.02 (19.6 examples/sec; 0.818 sec/batch; 37h:18m:21s remains)
INFO - root - 2017-12-07 02:35:16.121737: step 2040, loss = 2.08, batch loss = 2.03 (20.3 examples/sec; 0.787 sec/batch; 35h:52m:55s remains)
INFO - root - 2017-12-07 02:35:24.018585: step 2050, loss = 2.08, batch loss = 2.02 (19.9 examples/sec; 0.803 sec/batch; 36h:38m:34s remains)
INFO - root - 2017-12-07 02:35:31.756913: step 2060, loss = 2.08, batch loss = 2.02 (19.9 examples/sec; 0.803 sec/batch; 36h:37m:11s remains)
INFO - root - 2017-12-07 02:35:39.697847: step 2070, loss = 2.09, batch loss = 2.03 (19.8 examples/sec; 0.809 sec/batch; 36h:54m:55s remains)
INFO - root - 2017-12-07 02:35:47.578328: step 2080, loss = 2.11, batch loss = 2.05 (20.4 examples/sec; 0.784 sec/batch; 35h:44m:23s remains)
INFO - root - 2017-12-07 02:35:55.561052: step 2090, loss = 2.07, batch loss = 2.01 (20.3 examples/sec; 0.789 sec/batch; 35h:59m:39s remains)
INFO - root - 2017-12-07 02:36:03.539696: step 2100, loss = 2.08, batch loss = 2.03 (20.7 examples/sec; 0.772 sec/batch; 35h:12m:57s remains)
2017-12-07 02:36:04.281104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2345071 -4.2275939 -4.2144785 -4.1831479 -4.1483731 -4.1266861 -4.125144 -4.1580081 -4.2066369 -4.2269244 -4.2064834 -4.1876454 -4.1921849 -4.2208281 -4.2608957][-4.2479773 -4.2423229 -4.2322974 -4.2072005 -4.1742811 -4.1428456 -4.1307797 -4.1486354 -4.1853857 -4.2111311 -4.203258 -4.1949534 -4.2060008 -4.2304506 -4.2651978][-4.2488909 -4.2483606 -4.2399845 -4.2206254 -4.1914954 -4.1607246 -4.1449652 -4.15682 -4.184135 -4.2073846 -4.2027669 -4.1962581 -4.206594 -4.2317338 -4.2664723][-4.244206 -4.243825 -4.2360215 -4.2179346 -4.1912966 -4.1654334 -4.1522088 -4.1672683 -4.1921611 -4.2118068 -4.2075973 -4.197329 -4.2010012 -4.2254853 -4.2658772][-4.2450652 -4.2323842 -4.2121539 -4.1877875 -4.1562285 -4.1319356 -4.1301389 -4.1601667 -4.195303 -4.2186856 -4.2163749 -4.203455 -4.202322 -4.2284803 -4.2701683][-4.2560692 -4.2260423 -4.1817889 -4.1356111 -4.0819154 -4.0469985 -4.0585117 -4.1170421 -4.1766329 -4.2145391 -4.220901 -4.2125297 -4.2127295 -4.237884 -4.2767568][-4.2771492 -4.2339759 -4.1657395 -4.0829744 -3.9813988 -3.9041574 -3.9204352 -4.0127125 -4.1090417 -4.1772575 -4.2049813 -4.2117004 -4.2214961 -4.2488251 -4.2858558][-4.2900028 -4.248827 -4.1798153 -4.080121 -3.9417405 -3.8149838 -3.8116102 -3.919421 -4.0437326 -4.1401711 -4.1902618 -4.2110438 -4.2285066 -4.2574325 -4.2909513][-4.2983193 -4.2689753 -4.2185 -4.136796 -4.0189123 -3.9008803 -3.8644078 -3.9281271 -4.0342512 -4.13523 -4.1974616 -4.2220993 -4.23768 -4.2628164 -4.2920947][-4.2998896 -4.2814093 -4.2495041 -4.1979103 -4.1254735 -4.0466948 -3.9946411 -4.0031056 -4.0656004 -4.1488962 -4.2086611 -4.2324972 -4.2456474 -4.2671976 -4.2926664][-4.2976952 -4.2859063 -4.2679548 -4.2394519 -4.2025151 -4.1539483 -4.1018 -4.0844879 -4.1212816 -4.1842995 -4.2309618 -4.248343 -4.2554407 -4.27164 -4.2926989][-4.2977152 -4.2892103 -4.2776556 -4.2590427 -4.2386484 -4.2125449 -4.17669 -4.1597891 -4.1847363 -4.2285318 -4.2544641 -4.2591791 -4.2589135 -4.2709756 -4.2898088][-4.2986422 -4.2907367 -4.2819581 -4.2680473 -4.2495418 -4.2288976 -4.2048545 -4.1963153 -4.2196231 -4.2543812 -4.2645664 -4.2572312 -4.2513523 -4.2611933 -4.2812109][-4.3051043 -4.2939167 -4.2836237 -4.2684417 -4.2452164 -4.2195716 -4.1951289 -4.1922369 -4.2196031 -4.2512069 -4.2554178 -4.2434855 -4.2367911 -4.2478933 -4.2707443][-4.3110819 -4.2985525 -4.28583 -4.2689614 -4.2429242 -4.2124944 -4.188899 -4.1906905 -4.2157516 -4.2384648 -4.2384076 -4.2258091 -4.2208104 -4.2344127 -4.2622986]]...]
INFO - root - 2017-12-07 02:36:12.205364: step 2110, loss = 2.09, batch loss = 2.04 (19.5 examples/sec; 0.822 sec/batch; 37h:28m:34s remains)
INFO - root - 2017-12-07 02:36:20.143790: step 2120, loss = 2.08, batch loss = 2.03 (19.8 examples/sec; 0.810 sec/batch; 36h:55m:11s remains)
INFO - root - 2017-12-07 02:36:28.043422: step 2130, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.799 sec/batch; 36h:25m:35s remains)
INFO - root - 2017-12-07 02:36:35.776151: step 2140, loss = 2.08, batch loss = 2.02 (20.6 examples/sec; 0.776 sec/batch; 35h:21m:39s remains)
INFO - root - 2017-12-07 02:36:43.631543: step 2150, loss = 2.06, batch loss = 2.00 (20.7 examples/sec; 0.774 sec/batch; 35h:15m:32s remains)
INFO - root - 2017-12-07 02:36:51.536760: step 2160, loss = 2.08, batch loss = 2.02 (20.9 examples/sec; 0.765 sec/batch; 34h:51m:51s remains)
INFO - root - 2017-12-07 02:36:59.472070: step 2170, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.793 sec/batch; 36h:09m:08s remains)
INFO - root - 2017-12-07 02:37:07.415001: step 2180, loss = 2.09, batch loss = 2.03 (20.9 examples/sec; 0.767 sec/batch; 34h:57m:02s remains)
INFO - root - 2017-12-07 02:37:15.391005: step 2190, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.796 sec/batch; 36h:15m:52s remains)
INFO - root - 2017-12-07 02:37:23.344018: step 2200, loss = 2.08, batch loss = 2.02 (19.5 examples/sec; 0.820 sec/batch; 37h:20m:48s remains)
2017-12-07 02:37:24.075059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3228168 -4.3228946 -4.3230853 -4.3248043 -4.3269644 -4.328496 -4.3288231 -4.329143 -4.3294969 -4.3303533 -4.3315043 -4.3315859 -4.3303981 -4.3267112 -4.321312][-4.3080473 -4.30645 -4.30623 -4.3098545 -4.312706 -4.3135958 -4.3117595 -4.3102956 -4.3110738 -4.313498 -4.3165236 -4.3172512 -4.3136907 -4.3055763 -4.2957993][-4.2821646 -4.2792869 -4.2764292 -4.2779703 -4.280385 -4.2801476 -4.2766175 -4.273211 -4.2746334 -4.2793522 -4.2844567 -4.2851477 -4.2769952 -4.2650356 -4.2520466][-4.2355313 -4.2343807 -4.2305574 -4.2294321 -4.2301908 -4.2294106 -4.22513 -4.2170305 -4.2156639 -4.2204256 -4.2256131 -4.2236681 -4.2135525 -4.2052503 -4.1973143][-4.1793394 -4.1834726 -4.1794691 -4.1744504 -4.1703115 -4.1663613 -4.1558976 -4.13693 -4.130857 -4.1370859 -4.1454744 -4.1476679 -4.14516 -4.1501255 -4.1555228][-4.1218867 -4.1227207 -4.1142192 -4.1056623 -4.1003942 -4.0843048 -4.0493464 -4.0101056 -4.0067596 -4.02911 -4.05818 -4.0836625 -4.10128 -4.1255646 -4.1413832][-4.0734129 -4.0605626 -4.0421748 -4.0314555 -4.0231495 -3.9845521 -3.9087186 -3.8406343 -3.8584661 -3.9200306 -3.984092 -4.045239 -4.0933371 -4.1306858 -4.1464181][-4.0457172 -4.0230656 -3.9960117 -3.9763384 -3.9554586 -3.8913448 -3.7680228 -3.6596794 -3.7061868 -3.8295784 -3.9412487 -4.0366282 -4.1069236 -4.1445675 -4.1499352][-4.0341692 -4.0069165 -3.9759898 -3.9474413 -3.9109244 -3.8371387 -3.7013712 -3.5801272 -3.6502225 -3.82123 -3.9643977 -4.0729642 -4.1376891 -4.1585946 -4.150455][-4.0477791 -4.0253906 -4.0013542 -3.9762914 -3.9376407 -3.8816013 -3.7988076 -3.7333829 -3.7854366 -3.91741 -4.0405159 -4.1282887 -4.1641068 -4.1608396 -4.1417294][-4.0894766 -4.0775561 -4.068892 -4.0576158 -4.0276213 -3.9937537 -3.961653 -3.9433312 -3.9735155 -4.04527 -4.121943 -4.1705103 -4.1786785 -4.1558371 -4.1281486][-4.149127 -4.1446667 -4.1411977 -4.1393695 -4.1249828 -4.11224 -4.1105752 -4.1201773 -4.1377125 -4.1656661 -4.1980081 -4.2049522 -4.1847854 -4.1445632 -4.1075964][-4.2076883 -4.2080283 -4.2052941 -4.2087069 -4.2039604 -4.2026329 -4.2109809 -4.2269368 -4.2354159 -4.2382045 -4.2425871 -4.2213583 -4.1798534 -4.1261897 -4.0830235][-4.24841 -4.2561545 -4.2555423 -4.2592897 -4.2520552 -4.2515473 -4.2620821 -4.2739177 -4.2757611 -4.2719412 -4.2631431 -4.2237482 -4.164526 -4.1003523 -4.0580015][-4.2645631 -4.2772107 -4.2831955 -4.2877383 -4.2768874 -4.2721829 -4.2789736 -4.2888436 -4.2904553 -4.2858667 -4.2719769 -4.2297931 -4.1674385 -4.1053519 -4.0674744]]...]
INFO - root - 2017-12-07 02:37:32.002754: step 2210, loss = 2.08, batch loss = 2.02 (19.7 examples/sec; 0.810 sec/batch; 36h:55m:26s remains)
INFO - root - 2017-12-07 02:37:39.988644: step 2220, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.789 sec/batch; 35h:56m:40s remains)
INFO - root - 2017-12-07 02:37:47.733798: step 2230, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.801 sec/batch; 36h:30m:49s remains)
INFO - root - 2017-12-07 02:37:55.725217: step 2240, loss = 2.07, batch loss = 2.01 (19.9 examples/sec; 0.803 sec/batch; 36h:33m:41s remains)
INFO - root - 2017-12-07 02:38:03.616685: step 2250, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.783 sec/batch; 35h:40m:37s remains)
INFO - root - 2017-12-07 02:38:11.501713: step 2260, loss = 2.09, batch loss = 2.03 (20.2 examples/sec; 0.790 sec/batch; 35h:59m:55s remains)
INFO - root - 2017-12-07 02:38:19.416092: step 2270, loss = 2.06, batch loss = 2.00 (20.1 examples/sec; 0.794 sec/batch; 36h:10m:20s remains)
INFO - root - 2017-12-07 02:38:27.305340: step 2280, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.802 sec/batch; 36h:31m:20s remains)
INFO - root - 2017-12-07 02:38:35.175899: step 2290, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.799 sec/batch; 36h:23m:21s remains)
INFO - root - 2017-12-07 02:38:43.042215: step 2300, loss = 2.08, batch loss = 2.03 (20.5 examples/sec; 0.782 sec/batch; 35h:37m:44s remains)
2017-12-07 02:38:43.739653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2395782 -4.2369471 -4.2359867 -4.23632 -4.2415328 -4.2461123 -4.2391553 -4.2276053 -4.2096157 -4.1886778 -4.1681166 -4.1598129 -4.1663284 -4.1878586 -4.2210746][-4.2526183 -4.2499471 -4.2443347 -4.2409348 -4.2436028 -4.2443824 -4.2292223 -4.2026544 -4.1686611 -4.1434984 -4.1366868 -4.1496778 -4.1763663 -4.2056975 -4.2388353][-4.2503548 -4.2524614 -4.2491336 -4.24566 -4.244082 -4.2383347 -4.2137985 -4.1796331 -4.1438293 -4.1263008 -4.1355963 -4.160975 -4.1959233 -4.2262979 -4.2488251][-4.2249346 -4.2349949 -4.2406826 -4.2397766 -4.2348413 -4.2239232 -4.1948757 -4.1609826 -4.1361856 -4.1344738 -4.1546369 -4.1813483 -4.2122536 -4.230216 -4.2350883][-4.1836991 -4.1976976 -4.2138257 -4.2193451 -4.217042 -4.2050142 -4.1760764 -4.139339 -4.1187139 -4.1302714 -4.1570668 -4.1826887 -4.2067051 -4.2117138 -4.2037516][-4.1573911 -4.1615143 -4.1780586 -4.1882963 -4.18425 -4.16545 -4.133183 -4.0898976 -4.0689178 -4.0991778 -4.135448 -4.1622024 -4.1873465 -4.1884289 -4.1764326][-4.1583638 -4.1570344 -4.1721339 -4.18039 -4.1676164 -4.13155 -4.075407 -4.0002627 -3.9602032 -4.0200877 -4.089129 -4.1344495 -4.1718535 -4.1817279 -4.1755714][-4.1752181 -4.182188 -4.201086 -4.2069731 -4.1838875 -4.1334047 -4.0544472 -3.9454746 -3.8730273 -3.9484992 -4.0488334 -4.1150365 -4.1633635 -4.18533 -4.1931438][-4.1724362 -4.1891093 -4.212657 -4.2213955 -4.1981277 -4.1549525 -4.0956488 -4.0142679 -3.9592762 -3.9998186 -4.0711994 -4.1274428 -4.174221 -4.1989923 -4.2140718][-4.1406937 -4.1670122 -4.1992912 -4.2153687 -4.2034173 -4.1766386 -4.1433916 -4.0981 -4.0711112 -4.08499 -4.1167541 -4.153194 -4.188755 -4.2101674 -4.225173][-4.1067 -4.1394773 -4.1796937 -4.2036452 -4.2037649 -4.1899619 -4.1716261 -4.1446791 -4.1294141 -4.1315165 -4.1409359 -4.1625214 -4.1887121 -4.2083941 -4.21938][-4.1198044 -4.1409025 -4.1666379 -4.1852612 -4.1959524 -4.20199 -4.1985936 -4.1785145 -4.1609325 -4.1547718 -4.15551 -4.1661425 -4.1835084 -4.201664 -4.2151923][-4.1660924 -4.1705437 -4.1709957 -4.1703658 -4.1826243 -4.2079048 -4.2223973 -4.2074704 -4.1794734 -4.1631289 -4.1605554 -4.1666188 -4.1759043 -4.1935091 -4.2112007][-4.1991687 -4.1915541 -4.1812916 -4.1739721 -4.1875067 -4.2214165 -4.2474508 -4.2361236 -4.1988707 -4.1723132 -4.1651592 -4.164299 -4.1662884 -4.188591 -4.2130837][-4.2339334 -4.2220349 -4.21336 -4.2132387 -4.229032 -4.2619214 -4.2845187 -4.2677708 -4.2234526 -4.1864929 -4.1700678 -4.161787 -4.1664658 -4.1968069 -4.2250395]]...]
INFO - root - 2017-12-07 02:38:51.609036: step 2310, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.783 sec/batch; 35h:38m:21s remains)
INFO - root - 2017-12-07 02:38:59.401349: step 2320, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.809 sec/batch; 36h:49m:18s remains)
INFO - root - 2017-12-07 02:39:07.402975: step 2330, loss = 2.09, batch loss = 2.03 (19.5 examples/sec; 0.820 sec/batch; 37h:19m:54s remains)
INFO - root - 2017-12-07 02:39:15.380699: step 2340, loss = 2.07, batch loss = 2.02 (20.1 examples/sec; 0.794 sec/batch; 36h:09m:53s remains)
INFO - root - 2017-12-07 02:39:23.330596: step 2350, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.799 sec/batch; 36h:23m:40s remains)
INFO - root - 2017-12-07 02:39:31.254928: step 2360, loss = 2.08, batch loss = 2.02 (20.0 examples/sec; 0.801 sec/batch; 36h:27m:14s remains)
INFO - root - 2017-12-07 02:39:39.191229: step 2370, loss = 2.09, batch loss = 2.03 (19.9 examples/sec; 0.804 sec/batch; 36h:37m:15s remains)
INFO - root - 2017-12-07 02:39:47.071941: step 2380, loss = 2.09, batch loss = 2.03 (20.4 examples/sec; 0.785 sec/batch; 35h:45m:13s remains)
INFO - root - 2017-12-07 02:39:54.955345: step 2390, loss = 2.08, batch loss = 2.02 (20.6 examples/sec; 0.776 sec/batch; 35h:18m:36s remains)
INFO - root - 2017-12-07 02:40:02.849042: step 2400, loss = 2.07, batch loss = 2.02 (20.2 examples/sec; 0.793 sec/batch; 36h:04m:14s remains)
2017-12-07 02:40:03.555195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2302012 -4.2236991 -4.2214904 -4.2229829 -4.2288504 -4.2413478 -4.2605004 -4.2809258 -4.298264 -4.3132505 -4.3263707 -4.3355927 -4.3397293 -4.3400927 -4.3382916][-4.2395482 -4.2368374 -4.2369566 -4.2351871 -4.2351241 -4.2413378 -4.2563667 -4.2770414 -4.296905 -4.3131747 -4.3272376 -4.3368239 -4.3413968 -4.3420115 -4.3401747][-4.2424564 -4.2421632 -4.2435102 -4.2362971 -4.2276282 -4.2262783 -4.2377067 -4.26179 -4.2893591 -4.3103848 -4.3258095 -4.3356895 -4.3410177 -4.34212 -4.3405666][-4.23418 -4.234797 -4.2362509 -4.22354 -4.2058487 -4.196909 -4.2035112 -4.2314386 -4.2691298 -4.2991509 -4.318213 -4.3300934 -4.3376565 -4.3405333 -4.3400974][-4.2143326 -4.2141418 -4.2162614 -4.200367 -4.1740417 -4.1555719 -4.1542768 -4.1818643 -4.2288089 -4.2682962 -4.2942085 -4.3118982 -4.3263135 -4.334343 -4.3369489][-4.1802845 -4.1769142 -4.1789227 -4.1605415 -4.1275682 -4.1009502 -4.0931072 -4.1177754 -4.1690254 -4.2160988 -4.2523022 -4.2809315 -4.3062882 -4.322722 -4.3307896][-4.1274905 -4.12063 -4.12432 -4.1088495 -4.0762744 -4.0472178 -4.0352435 -4.0557694 -4.1077271 -4.1596031 -4.2070484 -4.2488723 -4.2852044 -4.3106322 -4.3241897][-4.092103 -4.0830531 -4.0872436 -4.0763335 -4.0490279 -4.018702 -3.9989254 -4.0129943 -4.0613561 -4.1143785 -4.1712837 -4.2260475 -4.2706504 -4.3017106 -4.3196077][-4.0907459 -4.0812879 -4.0825281 -4.0738726 -4.0536094 -4.02723 -4.0031004 -4.0109067 -4.0513029 -4.0988779 -4.1566319 -4.2157598 -4.2627296 -4.2951159 -4.3158016][-4.1056676 -4.0966115 -4.0965056 -4.0922894 -4.0832763 -4.0683761 -4.0480137 -4.0487852 -4.0761781 -4.1124163 -4.1615005 -4.2137156 -4.2562509 -4.2866297 -4.3079085][-4.1244431 -4.1165371 -4.1184626 -4.1215672 -4.1252213 -4.1250868 -4.1136923 -4.1083388 -4.1205058 -4.142312 -4.1760192 -4.21338 -4.2454376 -4.2713623 -4.2922182][-4.157043 -4.1507535 -4.15447 -4.1613026 -4.1717086 -4.1822076 -4.1817288 -4.1749206 -4.1727719 -4.1780109 -4.194037 -4.21296 -4.2309794 -4.2496696 -4.2690516][-4.2025418 -4.1987677 -4.2023444 -4.2081656 -4.2187858 -4.2321758 -4.2378283 -4.2308354 -4.2176938 -4.2078152 -4.2066927 -4.2096229 -4.2156315 -4.2258973 -4.24133][-4.2389646 -4.239099 -4.2438827 -4.24763 -4.2548857 -4.2657752 -4.2713556 -4.2645984 -4.2473521 -4.2272353 -4.2121139 -4.20304 -4.2010207 -4.2057571 -4.2163777][-4.2567172 -4.2606287 -4.2673078 -4.2696223 -4.273335 -4.2797713 -4.2832952 -4.2779388 -4.2620468 -4.2383528 -4.2148814 -4.1979256 -4.1896858 -4.1904449 -4.1981025]]...]
INFO - root - 2017-12-07 02:40:11.261683: step 2410, loss = 2.09, batch loss = 2.03 (20.0 examples/sec; 0.800 sec/batch; 36h:24m:30s remains)
INFO - root - 2017-12-07 02:40:19.207535: step 2420, loss = 2.09, batch loss = 2.03 (20.3 examples/sec; 0.787 sec/batch; 35h:49m:29s remains)
INFO - root - 2017-12-07 02:40:27.076339: step 2430, loss = 2.09, batch loss = 2.03 (19.9 examples/sec; 0.803 sec/batch; 36h:33m:48s remains)
INFO - root - 2017-12-07 02:40:35.006096: step 2440, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.809 sec/batch; 36h:49m:56s remains)
INFO - root - 2017-12-07 02:40:42.956683: step 2450, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.791 sec/batch; 36h:00m:36s remains)
INFO - root - 2017-12-07 02:40:50.856070: step 2460, loss = 2.08, batch loss = 2.02 (19.9 examples/sec; 0.803 sec/batch; 36h:30m:59s remains)
INFO - root - 2017-12-07 02:40:58.708949: step 2470, loss = 2.08, batch loss = 2.02 (20.9 examples/sec; 0.765 sec/batch; 34h:48m:31s remains)
INFO - root - 2017-12-07 02:41:06.582016: step 2480, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.787 sec/batch; 35h:47m:51s remains)
INFO - root - 2017-12-07 02:41:14.465584: step 2490, loss = 2.08, batch loss = 2.03 (20.0 examples/sec; 0.800 sec/batch; 36h:22m:11s remains)
INFO - root - 2017-12-07 02:41:22.083055: step 2500, loss = 2.06, batch loss = 2.00 (20.8 examples/sec; 0.771 sec/batch; 35h:03m:29s remains)
2017-12-07 02:41:22.807159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3278055 -4.310638 -4.294848 -4.2842412 -4.2783127 -4.2661476 -4.2366042 -4.2008157 -4.1927943 -4.2177849 -4.2409887 -4.2426915 -4.2426863 -4.2506766 -4.2513094][-4.3226976 -4.3005295 -4.2829018 -4.2730012 -4.2704616 -4.2645793 -4.2397528 -4.2099266 -4.2113886 -4.2362585 -4.2478824 -4.2372479 -4.227066 -4.2274971 -4.2230821][-4.3219905 -4.2989388 -4.2800465 -4.2665677 -4.2609448 -4.2580862 -4.23543 -4.2119818 -4.2200217 -4.24205 -4.2474442 -4.2345467 -4.2177982 -4.2039027 -4.1895123][-4.3256946 -4.3058376 -4.2873321 -4.2680283 -4.2553973 -4.2477212 -4.2228818 -4.2009754 -4.2083068 -4.225256 -4.2325 -4.2275205 -4.2132993 -4.1910377 -4.1678653][-4.3203 -4.3043265 -4.2873626 -4.2617445 -4.2361512 -4.2136712 -4.1787024 -4.1549854 -4.1600714 -4.1774244 -4.1937385 -4.203228 -4.2039518 -4.1892891 -4.1673141][-4.2946067 -4.2804027 -4.2625284 -4.230021 -4.1877856 -4.1400247 -4.0868373 -4.0622039 -4.074388 -4.1021204 -4.1347752 -4.1641479 -4.1870322 -4.1933889 -4.1849031][-4.2418404 -4.2288413 -4.2068844 -4.1648808 -4.1049504 -4.0319209 -3.9632087 -3.9449921 -3.97348 -4.017993 -4.0705891 -4.1216674 -4.1658659 -4.1938486 -4.2008858][-4.1757126 -4.1613388 -4.1326628 -4.0802069 -4.006351 -3.919292 -3.8530588 -3.8515599 -3.8957636 -3.9518578 -4.0156174 -4.0788436 -4.1343417 -4.1750007 -4.1938581][-4.12913 -4.1124229 -4.0803113 -4.0256281 -3.9503431 -3.8733666 -3.8285778 -3.8403673 -3.8842587 -3.9350238 -3.9931629 -4.0505881 -4.1016951 -4.1420345 -4.162859][-4.119348 -4.10476 -4.079422 -4.0390563 -3.9820375 -3.9278662 -3.9037247 -3.9116974 -3.9384027 -3.9756565 -4.0185928 -4.0567088 -4.0904212 -4.1180177 -4.1351047][-4.1360865 -4.1244841 -4.1108375 -4.0902185 -4.0566077 -4.0263395 -4.0151172 -4.0144057 -4.0276427 -4.0525517 -4.0778594 -4.0950274 -4.1072521 -4.1196613 -4.1323237][-4.1702638 -4.1630478 -4.1613703 -4.1565442 -4.1401958 -4.1244364 -4.1173992 -4.11137 -4.1174173 -4.1328611 -4.1443734 -4.1476645 -4.1453352 -4.1467838 -4.1566672][-4.220623 -4.2164288 -4.2200785 -4.2223935 -4.2152181 -4.2067771 -4.2007151 -4.1933165 -4.1957107 -4.20616 -4.2110233 -4.2082319 -4.2000465 -4.1966949 -4.2030559][-4.2776828 -4.2747321 -4.2782836 -4.2816019 -4.2784162 -4.2741938 -4.2709341 -4.2657056 -4.2680941 -4.2749543 -4.27681 -4.2717891 -4.2644143 -4.2600327 -4.2616525][-4.3258195 -4.3224535 -4.3225169 -4.3238425 -4.3236217 -4.3233147 -4.3226514 -4.3209763 -4.3242702 -4.3285336 -4.3280787 -4.3235364 -4.3186846 -4.3151441 -4.3136435]]...]
INFO - root - 2017-12-07 02:41:30.818340: step 2510, loss = 2.10, batch loss = 2.04 (19.5 examples/sec; 0.820 sec/batch; 37h:16m:58s remains)
INFO - root - 2017-12-07 02:41:38.707154: step 2520, loss = 2.10, batch loss = 2.04 (20.3 examples/sec; 0.790 sec/batch; 35h:55m:50s remains)
INFO - root - 2017-12-07 02:41:46.559162: step 2530, loss = 2.07, batch loss = 2.01 (20.6 examples/sec; 0.776 sec/batch; 35h:18m:47s remains)
INFO - root - 2017-12-07 02:41:54.479092: step 2540, loss = 2.08, batch loss = 2.02 (20.5 examples/sec; 0.780 sec/batch; 35h:27m:49s remains)
INFO - root - 2017-12-07 02:42:02.410736: step 2550, loss = 2.08, batch loss = 2.02 (20.1 examples/sec; 0.797 sec/batch; 36h:14m:57s remains)
INFO - root - 2017-12-07 02:42:10.297084: step 2560, loss = 2.08, batch loss = 2.02 (19.7 examples/sec; 0.813 sec/batch; 36h:58m:42s remains)
INFO - root - 2017-12-07 02:42:18.210153: step 2570, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.809 sec/batch; 36h:46m:12s remains)
INFO - root - 2017-12-07 02:42:25.970981: step 2580, loss = 2.07, batch loss = 2.02 (20.2 examples/sec; 0.791 sec/batch; 35h:56m:54s remains)
INFO - root - 2017-12-07 02:42:33.900701: step 2590, loss = 2.09, batch loss = 2.03 (19.8 examples/sec; 0.809 sec/batch; 36h:47m:04s remains)
INFO - root - 2017-12-07 02:42:41.768433: step 2600, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.791 sec/batch; 35h:56m:53s remains)
2017-12-07 02:42:42.462370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2965178 -4.298337 -4.2932024 -4.2906995 -4.2858763 -4.2810106 -4.2814746 -4.2888026 -4.2908821 -4.2865782 -4.2792506 -4.2688937 -4.2553496 -4.2458725 -4.2523661][-4.2747641 -4.2719412 -4.2615671 -4.258718 -4.2514935 -4.2437963 -4.2463613 -4.2549729 -4.256763 -4.25189 -4.2409029 -4.229629 -4.2180076 -4.2113 -4.2233458][-4.2452903 -4.2361107 -4.2255731 -4.2252512 -4.216794 -4.2096996 -4.2169032 -4.2281327 -4.2281771 -4.2209888 -4.207 -4.1951137 -4.1890445 -4.1900368 -4.2085571][-4.224277 -4.2149515 -4.2105961 -4.2139435 -4.2067952 -4.2003641 -4.2085838 -4.2167568 -4.2096577 -4.1970792 -4.1808209 -4.1699824 -4.1701379 -4.1773553 -4.1974678][-4.21506 -4.2081404 -4.2099161 -4.2119122 -4.2030892 -4.1980133 -4.2034812 -4.1996107 -4.1794972 -4.1577687 -4.1410131 -4.1389461 -4.1499133 -4.1613669 -4.1762052][-4.1958313 -4.18615 -4.190268 -4.1922703 -4.1839237 -4.1832328 -4.1795626 -4.15635 -4.1220155 -4.0942144 -4.0869412 -4.1054525 -4.1341829 -4.1510081 -4.1580858][-4.1817975 -4.1678848 -4.174356 -4.1778231 -4.1723137 -4.1703587 -4.1485014 -4.1011529 -4.0495677 -4.0147924 -4.0215726 -4.0650153 -4.1180315 -4.145555 -4.1487932][-4.1885328 -4.176631 -4.1838303 -4.1836367 -4.1708703 -4.15309 -4.1047282 -4.0300536 -3.9593782 -3.9242816 -3.9512811 -4.0221081 -4.0961628 -4.1363158 -4.1463022][-4.1986928 -4.1892633 -4.1936059 -4.1848345 -4.1610627 -4.1257482 -4.0586958 -3.9754972 -3.9131134 -3.8998148 -3.9439495 -4.0196605 -4.0947685 -4.135859 -4.1527758][-4.1879396 -4.1788945 -4.1803055 -4.1675034 -4.1473331 -4.1219354 -4.0718455 -4.0092187 -3.9765451 -3.9871433 -4.024044 -4.0767756 -4.1328621 -4.16788 -4.1884708][-4.1742129 -4.1667733 -4.1666861 -4.1552691 -4.1443934 -4.1380744 -4.109808 -4.069201 -4.0618157 -4.0797691 -4.1011457 -4.1333604 -4.1743274 -4.2059941 -4.2285309][-4.1886649 -4.1858282 -4.1843996 -4.1735773 -4.16634 -4.167243 -4.1504807 -4.1247039 -4.1254358 -4.1364827 -4.1488757 -4.1736856 -4.2054048 -4.2318311 -4.2522063][-4.2197313 -4.2191973 -4.2170048 -4.2045159 -4.1986361 -4.2055945 -4.2035136 -4.1916523 -4.1895471 -4.1928186 -4.2012691 -4.2194037 -4.2402468 -4.2572732 -4.2700844][-4.2563128 -4.2547116 -4.2523389 -4.2449446 -4.2458229 -4.2586913 -4.2668514 -4.2646556 -4.2608995 -4.2585726 -4.2621026 -4.2756224 -4.2887311 -4.2981205 -4.3044963][-4.2904477 -4.2890749 -4.2870383 -4.285933 -4.29228 -4.3051519 -4.3141103 -4.3146338 -4.31198 -4.3091545 -4.3112655 -4.3229504 -4.3318477 -4.3359447 -4.3373003]]...]
INFO - root - 2017-12-07 02:42:50.382111: step 2610, loss = 2.08, batch loss = 2.03 (20.8 examples/sec; 0.770 sec/batch; 35h:00m:55s remains)
INFO - root - 2017-12-07 02:42:58.293321: step 2620, loss = 2.08, batch loss = 2.03 (20.6 examples/sec; 0.776 sec/batch; 35h:16m:34s remains)
INFO - root - 2017-12-07 02:43:06.144887: step 2630, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.792 sec/batch; 36h:00m:45s remains)
INFO - root - 2017-12-07 02:43:14.056457: step 2640, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.792 sec/batch; 36h:00m:14s remains)
INFO - root - 2017-12-07 02:43:22.019773: step 2650, loss = 2.09, batch loss = 2.03 (20.5 examples/sec; 0.781 sec/batch; 35h:30m:40s remains)
INFO - root - 2017-12-07 02:43:29.922486: step 2660, loss = 2.09, batch loss = 2.03 (19.9 examples/sec; 0.802 sec/batch; 36h:26m:54s remains)
INFO - root - 2017-12-07 02:43:37.673807: step 2670, loss = 2.06, batch loss = 2.00 (20.0 examples/sec; 0.800 sec/batch; 36h:20m:46s remains)
INFO - root - 2017-12-07 02:43:45.603552: step 2680, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.797 sec/batch; 36h:11m:59s remains)
INFO - root - 2017-12-07 02:43:53.542611: step 2690, loss = 2.10, batch loss = 2.04 (20.0 examples/sec; 0.799 sec/batch; 36h:19m:07s remains)
INFO - root - 2017-12-07 02:44:01.482336: step 2700, loss = 2.09, batch loss = 2.03 (20.3 examples/sec; 0.788 sec/batch; 35h:47m:25s remains)
2017-12-07 02:44:02.164265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2815814 -4.2920804 -4.3007765 -4.3008051 -4.2856236 -4.2678328 -4.2568436 -4.2489724 -4.2497978 -4.2534957 -4.2558031 -4.2596068 -4.2613883 -4.2605495 -4.2615566][-4.282196 -4.2933326 -4.3034263 -4.3031263 -4.2848277 -4.2623763 -4.2449989 -4.23069 -4.2306347 -4.2377234 -4.2461395 -4.2580738 -4.2639971 -4.2607327 -4.2618217][-4.2746496 -4.2844143 -4.296576 -4.2989855 -4.2804356 -4.2536726 -4.2331462 -4.2133617 -4.2136669 -4.2232409 -4.2366934 -4.2569642 -4.2677507 -4.2649984 -4.2664404][-4.2576694 -4.2664 -4.2784381 -4.2793965 -4.2591925 -4.2314329 -4.2094288 -4.1891651 -4.1953459 -4.2107439 -4.2301636 -4.257092 -4.2718468 -4.2718205 -4.2732334][-4.2297897 -4.2349596 -4.2444439 -4.2456021 -4.2254577 -4.1962085 -4.1697612 -4.146224 -4.15727 -4.1840911 -4.2156386 -4.2494063 -4.2707372 -4.2757721 -4.2759023][-4.1912756 -4.1882796 -4.1953554 -4.1961627 -4.1773453 -4.14619 -4.1172915 -4.0945158 -4.1084328 -4.1467085 -4.1899805 -4.2298932 -4.2576351 -4.269073 -4.2722507][-4.1478014 -4.1369476 -4.1422839 -4.144413 -4.1287661 -4.0896716 -4.0471907 -4.0189128 -4.0445724 -4.1005087 -4.1572876 -4.2027 -4.2346644 -4.2527785 -4.2639589][-4.0700269 -4.0529332 -4.0593424 -4.0664229 -4.0570326 -4.0075474 -3.9317017 -3.8777087 -3.9243546 -4.0180268 -4.1014051 -4.1618843 -4.2004728 -4.2281189 -4.2483807][-3.9991343 -3.9694197 -3.9759028 -3.9935064 -3.9989653 -3.9526904 -3.8512313 -3.7601488 -3.8151228 -3.9430702 -4.0505857 -4.1253467 -4.1692572 -4.201941 -4.2273765][-4.035 -4.0050936 -4.0119615 -4.0366468 -4.0537138 -4.0276074 -3.9488854 -3.869482 -3.8930852 -3.9858112 -4.0713987 -4.1313415 -4.165627 -4.1924777 -4.2152352][-4.1109538 -4.0891924 -4.0957208 -4.1174483 -4.1347656 -4.1212234 -4.0752554 -4.0283852 -4.0333295 -4.0768814 -4.12711 -4.16054 -4.1818743 -4.1997719 -4.2159138][-4.1803513 -4.1658607 -4.16939 -4.1831717 -4.1969624 -4.1883793 -4.1598849 -4.1353054 -4.1341748 -4.15024 -4.1747785 -4.1907649 -4.2024207 -4.2136283 -4.223875][-4.2340269 -4.2250366 -4.2242723 -4.2290416 -4.2373028 -4.2298117 -4.2103763 -4.198585 -4.1973844 -4.2026238 -4.2137723 -4.2224932 -4.228837 -4.2366061 -4.2428942][-4.2726626 -4.2664981 -4.2663851 -4.2698822 -4.2746434 -4.2705979 -4.2579789 -4.2489681 -4.2439222 -4.2427311 -4.2466846 -4.2525883 -4.2558007 -4.2604141 -4.2644219][-4.301106 -4.2968388 -4.2978797 -4.3020644 -4.3066783 -4.3048992 -4.29828 -4.29104 -4.282773 -4.2767682 -4.276351 -4.2806597 -4.2837586 -4.287632 -4.2902431]]...]
INFO - root - 2017-12-07 02:44:10.166718: step 2710, loss = 2.07, batch loss = 2.01 (20.2 examples/sec; 0.793 sec/batch; 36h:00m:16s remains)
INFO - root - 2017-12-07 02:44:18.154816: step 2720, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.808 sec/batch; 36h:41m:25s remains)
INFO - root - 2017-12-07 02:44:26.085343: step 2730, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.810 sec/batch; 36h:47m:37s remains)
INFO - root - 2017-12-07 02:44:33.957425: step 2740, loss = 2.06, batch loss = 2.01 (20.2 examples/sec; 0.792 sec/batch; 35h:58m:04s remains)
INFO - root - 2017-12-07 02:44:41.922410: step 2750, loss = 2.08, batch loss = 2.02 (20.2 examples/sec; 0.791 sec/batch; 35h:56m:30s remains)
INFO - root - 2017-12-07 02:44:49.671311: step 2760, loss = 2.09, batch loss = 2.03 (20.5 examples/sec; 0.780 sec/batch; 35h:25m:34s remains)
INFO - root - 2017-12-07 02:44:57.618767: step 2770, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.783 sec/batch; 35h:34m:37s remains)
INFO - root - 2017-12-07 02:45:05.594095: step 2780, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.790 sec/batch; 35h:51m:30s remains)
INFO - root - 2017-12-07 02:45:13.557190: step 2790, loss = 2.08, batch loss = 2.02 (20.3 examples/sec; 0.789 sec/batch; 35h:50m:40s remains)
INFO - root - 2017-12-07 02:45:21.542196: step 2800, loss = 2.07, batch loss = 2.01 (20.5 examples/sec; 0.781 sec/batch; 35h:26m:34s remains)
2017-12-07 02:45:22.266199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2820797 -4.2939568 -4.2963881 -4.2958484 -4.2904668 -4.2824121 -4.2786012 -4.2843127 -4.2894664 -4.2909975 -4.2839165 -4.2597823 -4.223577 -4.2049818 -4.2043376][-4.2706428 -4.28094 -4.2823563 -4.2792234 -4.2715611 -4.2570944 -4.2524762 -4.2594161 -4.2675567 -4.2743788 -4.2753859 -4.2559729 -4.224525 -4.2137 -4.2200804][-4.2573576 -4.2713051 -4.2686691 -4.2608337 -4.2516732 -4.233758 -4.2252779 -4.22755 -4.2341604 -4.2416329 -4.2470779 -4.2331896 -4.2109976 -4.2097735 -4.2240467][-4.2465262 -4.2606649 -4.2561321 -4.2436318 -4.2322426 -4.2092772 -4.1912909 -4.18268 -4.1799245 -4.1892667 -4.1997848 -4.1931667 -4.1827703 -4.1906624 -4.2130756][-4.239677 -4.2539105 -4.2475023 -4.23122 -4.2145238 -4.1822333 -4.148375 -4.1238723 -4.1139989 -4.1285253 -4.148417 -4.1539497 -4.1598239 -4.1801295 -4.2081676][-4.2391009 -4.2522364 -4.2442346 -4.2231827 -4.1977 -4.1611028 -4.1156154 -4.075561 -4.0596418 -4.0854688 -4.1223626 -4.1435838 -4.1641259 -4.1907949 -4.2190137][-4.2445865 -4.2516432 -4.242949 -4.2243505 -4.19611 -4.1599436 -4.1122475 -4.0645247 -4.0459394 -4.0767441 -4.1218634 -4.1538849 -4.1770344 -4.2026081 -4.2269979][-4.2522368 -4.2493086 -4.2408295 -4.226624 -4.2026615 -4.1739459 -4.1359267 -4.0950117 -4.08084 -4.1051226 -4.1411934 -4.16833 -4.18657 -4.2075062 -4.223886][-4.2479048 -4.2397103 -4.2338443 -4.2284665 -4.2125659 -4.1936312 -4.1727576 -4.1495204 -4.1409054 -4.1541204 -4.1696873 -4.1819015 -4.1936765 -4.2041364 -4.2116375][-4.2314143 -4.2244406 -4.2255239 -4.2282305 -4.2222104 -4.2113938 -4.1998243 -4.187489 -4.1804781 -4.1853805 -4.1893644 -4.1927891 -4.1979036 -4.1993756 -4.2031512][-4.2218928 -4.2185564 -4.2232337 -4.2293725 -4.2285132 -4.2205696 -4.2094169 -4.200614 -4.1971588 -4.1997275 -4.2056308 -4.2085891 -4.2101183 -4.2069683 -4.2088027][-4.2226329 -4.2223563 -4.2250528 -4.2285671 -4.2277169 -4.2170362 -4.203557 -4.1994095 -4.2013383 -4.2062588 -4.2123222 -4.2157388 -4.2206917 -4.2196565 -4.2206588][-4.2303748 -4.23036 -4.22861 -4.2267184 -4.2225833 -4.2134361 -4.2051554 -4.204361 -4.2081823 -4.213254 -4.2162743 -4.2212009 -4.2296877 -4.2333751 -4.2344923][-4.2467775 -4.2429352 -4.2357249 -4.2272062 -4.2211113 -4.2182679 -4.2175307 -4.2169533 -4.220675 -4.22661 -4.2286658 -4.2352552 -4.2447705 -4.2482705 -4.2480969][-4.26609 -4.2610478 -4.2486296 -4.2359262 -4.22903 -4.2279124 -4.2302027 -4.2320871 -4.2350473 -4.2392488 -4.2392025 -4.24576 -4.2548285 -4.2587266 -4.2586274]]...]
INFO - root - 2017-12-07 02:45:30.195616: step 2810, loss = 2.06, batch loss = 2.00 (20.2 examples/sec; 0.793 sec/batch; 36h:00m:21s remains)
INFO - root - 2017-12-07 02:45:38.146482: step 2820, loss = 2.06, batch loss = 2.01 (20.1 examples/sec; 0.795 sec/batch; 36h:06m:30s remains)
INFO - root - 2017-12-07 02:45:46.028936: step 2830, loss = 2.07, batch loss = 2.01 (20.4 examples/sec; 0.784 sec/batch; 35h:35m:41s remains)
INFO - root - 2017-12-07 02:45:53.986238: step 2840, loss = 2.07, batch loss = 2.01 (20.0 examples/sec; 0.801 sec/batch; 36h:20m:55s remains)
INFO - root - 2017-12-07 02:46:01.653084: step 2850, loss = 2.07, batch loss = 2.01 (19.8 examples/sec; 0.807 sec/batch; 36h:38m:53s remains)
INFO - root - 2017-12-07 02:46:09.518985: step 2860, loss = 2.07, batch loss = 2.01 (20.1 examples/sec; 0.798 sec/batch; 36h:12m:15s remains)
INFO - root - 2017-12-07 02:46:17.439137: step 2870, loss = 2.08, batch loss = 2.02 (19.8 examples/sec; 0.807 sec/batch; 36h:36m:50s remains)
INFO - root - 2017-12-07 02:46:25.385054: step 2880, loss = 2.08, batch loss = 2.03 (20.2 examples/sec; 0.790 sec/batch; 35h:52m:01s remains)
INFO - root - 2017-12-07 02:46:33.282927: step 2890, loss = 2.05, batch loss = 1.99 (20.1 examples/sec; 0.797 sec/batch; 36h:11m:04s remains)
INFO - root - 2017-12-07 02:46:41.183116: step 2900, loss = 2.09, batch loss = 2.04 (20.3 examples/sec; 0.788 sec/batch; 35h:46m:39s remains)
2017-12-07 02:46:41.905502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3142595 -4.2990952 -4.2783122 -4.2673564 -4.2693367 -4.280489 -4.2949524 -4.2992868 -4.2776875 -4.2422953 -4.2102304 -4.1997252 -4.2204061 -4.2545848 -4.2857442][-4.2906256 -4.2741432 -4.2529306 -4.2404909 -4.2383051 -4.2458344 -4.2636366 -4.2786841 -4.2739105 -4.2525849 -4.2270675 -4.2135963 -4.2315211 -4.2615366 -4.2917132][-4.2696543 -4.2503114 -4.2297959 -4.2164235 -4.2082343 -4.2067623 -4.22151 -4.2429404 -4.2532787 -4.2467742 -4.2308779 -4.2208543 -4.2370205 -4.2648416 -4.293664][-4.2514277 -4.2240663 -4.2033534 -4.1896181 -4.1753736 -4.1617141 -4.1634607 -4.17848 -4.1947556 -4.2017322 -4.2027822 -4.2055454 -4.2272964 -4.2567797 -4.2877035][-4.23159 -4.19175 -4.1654725 -4.1504965 -4.1309695 -4.1025181 -4.0809717 -4.0775633 -4.0902686 -4.110467 -4.1356654 -4.1611848 -4.1967926 -4.2332683 -4.2737446][-4.2157168 -4.1568484 -4.1164966 -4.0949411 -4.067749 -4.0278 -3.9826484 -3.9537184 -3.9549522 -3.9871871 -4.0401478 -4.0931792 -4.1492476 -4.1981626 -4.2512631][-4.2062478 -4.1290913 -4.0711784 -4.0396471 -4.0117331 -3.9755704 -3.9355638 -3.9036674 -3.8930573 -3.9180913 -3.9792376 -4.045517 -4.1120725 -4.1699758 -4.2335734][-4.1990933 -4.1102057 -4.03472 -3.9895651 -3.9654882 -3.9526649 -3.9472952 -3.9432111 -3.9336183 -3.9404075 -3.986033 -4.0493069 -4.1149626 -4.1740351 -4.2407203][-4.1857285 -4.0965242 -4.009819 -3.9544148 -3.9346585 -3.9433839 -3.9677644 -3.98862 -3.9909778 -3.9961443 -4.0317464 -4.0893764 -4.1524158 -4.2112217 -4.2733512][-4.1620641 -4.0885572 -4.009872 -3.9518008 -3.9315884 -3.9504409 -3.9917998 -4.0304666 -4.0517464 -4.0680203 -4.1027756 -4.1548991 -4.2135282 -4.2661881 -4.3138194][-4.1443615 -4.0989242 -4.0439873 -3.9968379 -3.9789424 -4.0020356 -4.050571 -4.0970292 -4.1281476 -4.1533461 -4.189549 -4.2333941 -4.2778792 -4.3145442 -4.3425646][-4.1509228 -4.1360269 -4.1121249 -4.0882115 -4.0827093 -4.1068034 -4.1500621 -4.1920772 -4.2200227 -4.2427015 -4.2693295 -4.2971373 -4.3206282 -4.3390751 -4.3506308][-4.1761661 -4.1891708 -4.1929083 -4.1911216 -4.1954765 -4.2151165 -4.2437944 -4.2709351 -4.2890849 -4.3021512 -4.3157983 -4.3285689 -4.3369479 -4.3419147 -4.3419752][-4.2074728 -4.2366543 -4.2533636 -4.2612004 -4.2667208 -4.2770429 -4.2901406 -4.3034196 -4.3127656 -4.31777 -4.323132 -4.3290854 -4.3331537 -4.3348751 -4.3322415][-4.240315 -4.26881 -4.2832508 -4.2894678 -4.291183 -4.2906895 -4.2903118 -4.2939644 -4.2965374 -4.29876 -4.303278 -4.3110819 -4.3191767 -4.3246517 -4.3242421]]...]
INFO - root - 2017-12-07 02:46:49.843609: step 2910, loss = 2.08, batch loss = 2.02 (20.8 examples/sec; 0.769 sec/batch; 34h:52m:46s remains)
INFO - root - 2017-12-07 02:46:57.798956: step 2920, loss = 2.08, batch loss = 2.03 (19.8 examples/sec; 0.809 sec/batch; 36h:42m:18s remains)
INFO - root - 2017-12-07 02:47:05.594274: step 2930, loss = 2.09, batch loss = 2.03 (20.3 examples/sec; 0.789 sec/batch; 35h:47m:00s remains)
INFO - root - 2017-12-07 02:47:13.504635: step 2940, loss = 2.08, batch loss = 2.02 (20.4 examples/sec; 0.783 sec/batch; 35h:31m:37s remains)
INFO - root - 2017-12-07 02:47:21.436170: step 2950, loss = 2.09, batch loss = 2.03 (20.1 examples/sec; 0.796 sec/batch; 36h:07m:11s remains)
INFO - root - 2017-12-07 02:47:29.867627: step 2960, loss = 2.08, batch loss = 2.02 (13.5 examples/sec; 1.184 sec/batch; 53h:42m:12s remains)
INFO - root - 2017-12-07 02:47:44.539340: step 2970, loss = 2.08, batch loss = 2.02 (17.1 examples/sec; 0.935 sec/batch; 42h:23m:33s remains)
INFO - root - 2017-12-07 02:47:57.344399: step 2980, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 1.295 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-07 02:48:10.121663: step 2990, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 1.297 sec/batch; 58h:50m:09s remains)
INFO - root - 2017-12-07 02:48:22.842223: step 3000, loss = 2.09, batch loss = 2.04 (13.1 examples/sec; 1.223 sec/batch; 55h:28m:45s remains)
2017-12-07 02:48:23.903381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1605673 -4.155324 -4.1340857 -4.1169529 -4.1182532 -4.1307626 -4.141098 -4.1415391 -4.1338043 -4.129477 -4.1270542 -4.1281853 -4.1415558 -4.1580114 -4.1703472][-4.1488647 -4.1446834 -4.1345811 -4.1280489 -4.13106 -4.1387563 -4.1433592 -4.1383314 -4.1298923 -4.1352639 -4.1456594 -4.1559386 -4.1720037 -4.1827126 -4.18217][-4.1572437 -4.1494246 -4.1394939 -4.1344452 -4.1362829 -4.1385584 -4.1323161 -4.1147513 -4.1023951 -4.122745 -4.1559567 -4.1804919 -4.1999955 -4.2060623 -4.1947489][-4.1741371 -4.1628294 -4.1488247 -4.1411791 -4.1411109 -4.1371055 -4.11847 -4.081708 -4.0608196 -4.0985622 -4.1585 -4.1965208 -4.2155862 -4.2166095 -4.200983][-4.1908813 -4.1781168 -4.1623535 -4.151794 -4.1478405 -4.1385274 -4.108799 -4.0530887 -4.0185719 -4.0669694 -4.1474867 -4.1990275 -4.2191334 -4.2165852 -4.1988239][-4.1960158 -4.1835351 -4.1713734 -4.1644731 -4.1632538 -4.1528788 -4.1126413 -4.0419092 -3.9942195 -4.0395546 -4.1262922 -4.1884747 -4.2139707 -4.2106261 -4.1927905][-4.1878557 -4.1776204 -4.1743541 -4.1778121 -4.1853852 -4.1782804 -4.1354914 -4.05833 -3.9989257 -4.0242844 -4.1030073 -4.170651 -4.2032137 -4.2030721 -4.1873231][-4.1725206 -4.1644249 -4.1717877 -4.1884055 -4.2042913 -4.201406 -4.1619644 -4.0834541 -4.0099812 -4.0093408 -4.0724788 -4.14139 -4.1810932 -4.1874766 -4.1750693][-4.1618118 -4.1540389 -4.1675186 -4.1927595 -4.2147331 -4.2147427 -4.1806779 -4.105566 -4.02595 -4.0050588 -4.0477014 -4.1081119 -4.1505113 -4.1646419 -4.1587906][-4.1605268 -4.1517558 -4.1690788 -4.1990037 -4.2229767 -4.2233205 -4.1960659 -4.1351147 -4.0692911 -4.0455685 -4.0652571 -4.1017394 -4.1332269 -4.1481571 -4.1484489][-4.1757889 -4.1686697 -4.186101 -4.21261 -4.2302485 -4.2274384 -4.2083826 -4.1703081 -4.1311245 -4.1152234 -4.1197491 -4.1324663 -4.1448669 -4.15236 -4.1533895][-4.1942682 -4.1914988 -4.2074876 -4.228601 -4.2381988 -4.2316337 -4.2220974 -4.2063131 -4.1925225 -4.1873331 -4.1836309 -4.1780567 -4.1717949 -4.1704698 -4.16953][-4.2074394 -4.2073121 -4.222611 -4.2394195 -4.2467561 -4.2413816 -4.2383146 -4.2368135 -4.2366328 -4.2351723 -4.2242546 -4.2073689 -4.1908841 -4.1870918 -4.1900368][-4.2132969 -4.2165785 -4.2322578 -4.2480917 -4.2566342 -4.2546086 -4.2538223 -4.25807 -4.2641349 -4.2649179 -4.2522931 -4.2280602 -4.2064333 -4.2045174 -4.2151723][-4.2176352 -4.2240233 -4.2405338 -4.2549834 -4.2621489 -4.2621694 -4.2629476 -4.2698374 -4.2791839 -4.2827053 -4.2717862 -4.2463822 -4.2234674 -4.2244534 -4.2401519]]...]
INFO - root - 2017-12-07 02:48:36.638142: step 3010, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 1.257 sec/batch; 56h:59m:53s remains)
INFO - root - 2017-12-07 02:48:49.380981: step 3020, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 1.307 sec/batch; 59h:16m:01s remains)
INFO - root - 2017-12-07 02:49:02.056530: step 3030, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 1.238 sec/batch; 56h:08m:14s remains)
INFO - root - 2017-12-07 02:49:14.845081: step 3040, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 1.253 sec/batch; 56h:47m:06s remains)
INFO - root - 2017-12-07 02:49:30.401900: step 3050, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 1.287 sec/batch; 58h:21m:06s remains)
INFO - root - 2017-12-07 02:49:43.221018: step 3060, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 1.275 sec/batch; 57h:46m:41s remains)
INFO - root - 2017-12-07 02:49:55.973737: step 3070, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 1.309 sec/batch; 59h:18m:46s remains)
INFO - root - 2017-12-07 02:50:08.578188: step 3080, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 1.254 sec/batch; 56h:50m:16s remains)
INFO - root - 2017-12-07 02:50:21.248198: step 3090, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 1.294 sec/batch; 58h:38m:51s remains)
INFO - root - 2017-12-07 02:50:33.889606: step 3100, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 1.244 sec/batch; 56h:22m:44s remains)
2017-12-07 02:50:34.897150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1406059 -4.1189833 -4.1906338 -4.2583981 -4.2889466 -4.3008504 -4.2809796 -4.2334952 -4.1783853 -4.1274934 -4.095706 -4.0799308 -4.0957031 -4.1392083 -4.1852951][-4.1416321 -4.1049414 -4.1743646 -4.2530847 -4.2880554 -4.2993116 -4.271318 -4.2124677 -4.1527138 -4.102881 -4.0690947 -4.0527091 -4.0614824 -4.1028957 -4.1498733][-4.1390657 -4.0881763 -4.1490307 -4.2387352 -4.277843 -4.2835431 -4.2444582 -4.1797056 -4.1259995 -4.0874562 -4.0585465 -4.0458097 -4.047956 -4.0797825 -4.1230521][-4.1431055 -4.0841689 -4.1356955 -4.2278328 -4.2665257 -4.2597542 -4.2048306 -4.1332717 -4.0962548 -4.0859661 -4.0728397 -4.068944 -4.0703573 -4.0892239 -4.1233611][-4.14971 -4.0899482 -4.1329932 -4.2156692 -4.2499609 -4.2303705 -4.1536856 -4.0767207 -4.0716114 -4.1010051 -4.1038995 -4.1012878 -4.1014647 -4.1095791 -4.1309385][-4.1557631 -4.1037011 -4.138813 -4.2063518 -4.2313914 -4.1959124 -4.0905466 -4.0078931 -4.047307 -4.1166739 -4.1323714 -4.1234565 -4.1213741 -4.122992 -4.1332736][-4.1590586 -4.1160169 -4.1484065 -4.203424 -4.2157 -4.15507 -4.0130339 -3.9310105 -4.0206938 -4.1183972 -4.1412454 -4.1285405 -4.1170235 -4.113596 -4.1228924][-4.1643357 -4.1232505 -4.1497345 -4.190753 -4.185945 -4.09457 -3.9222329 -3.863503 -4.0075049 -4.1288118 -4.15672 -4.1391592 -4.1088223 -4.0869956 -4.0919266][-4.1627026 -4.1192393 -4.1408095 -4.1729269 -4.150732 -4.0360475 -3.8626044 -3.852622 -4.0377431 -4.1670337 -4.1916513 -4.163527 -4.1084552 -4.0597329 -4.0530725][-4.169311 -4.1240897 -4.1377559 -4.1652551 -4.1335049 -4.0175776 -3.8787146 -3.9140077 -4.1018229 -4.2190914 -4.2376237 -4.1958241 -4.1137595 -4.0398254 -4.0187759][-4.1830363 -4.1421723 -4.1503062 -4.1797104 -4.1510963 -4.0555625 -3.9655 -4.0177293 -4.1727362 -4.2719555 -4.2872958 -4.2367878 -4.1351566 -4.0389142 -4.0001974][-4.2049613 -4.1721606 -4.1782603 -4.2120562 -4.1984348 -4.1314178 -4.0784192 -4.117609 -4.2255249 -4.3097892 -4.3262091 -4.2736983 -4.1697707 -4.0607586 -3.9992816][-4.2336769 -4.2104907 -4.21583 -4.2477493 -4.246058 -4.2059994 -4.1717381 -4.1839094 -4.2448978 -4.3152924 -4.3418064 -4.2980623 -4.2084923 -4.0980988 -4.015842][-4.2506638 -4.2372322 -4.2442222 -4.2689233 -4.2682447 -4.2419629 -4.2153625 -4.2067628 -4.2345843 -4.2972574 -4.3366356 -4.3081789 -4.23647 -4.1305408 -4.0386634][-4.2592759 -4.2514377 -4.2531223 -4.2642708 -4.2594018 -4.2426267 -4.2236633 -4.2076664 -4.2177014 -4.27553 -4.3260393 -4.3165207 -4.2606239 -4.1631155 -4.0735908]]...]
INFO - root - 2017-12-07 02:50:47.586922: step 3110, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 1.266 sec/batch; 57h:21m:26s remains)
INFO - root - 2017-12-07 02:51:00.375470: step 3120, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 1.319 sec/batch; 59h:47m:25s remains)
INFO - root - 2017-12-07 02:51:13.162878: step 3130, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 1.218 sec/batch; 55h:11m:38s remains)
INFO - root - 2017-12-07 02:51:25.901691: step 3140, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 1.270 sec/batch; 57h:32m:10s remains)
INFO - root - 2017-12-07 02:51:38.605061: step 3150, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 1.277 sec/batch; 57h:52m:30s remains)
INFO - root - 2017-12-07 02:51:51.350927: step 3160, loss = 2.09, batch loss = 2.03 (13.3 examples/sec; 1.201 sec/batch; 54h:23m:50s remains)
INFO - root - 2017-12-07 02:52:04.115322: step 3170, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 1.299 sec/batch; 58h:50m:09s remains)
INFO - root - 2017-12-07 02:52:16.879578: step 3180, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 1.262 sec/batch; 57h:10m:38s remains)
INFO - root - 2017-12-07 02:52:29.635646: step 3190, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 1.254 sec/batch; 56h:47m:50s remains)
INFO - root - 2017-12-07 02:52:42.313750: step 3200, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 1.239 sec/batch; 56h:05m:42s remains)
2017-12-07 02:52:43.330176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2137408 -4.2210827 -4.2236638 -4.2233891 -4.2218833 -4.2254987 -4.2188549 -4.20734 -4.2050424 -4.214304 -4.2314963 -4.2407579 -4.2458477 -4.251452 -4.2564735][-4.21475 -4.2204785 -4.2298255 -4.2360392 -4.2397919 -4.2467232 -4.2375579 -4.21846 -4.2096596 -4.2170477 -4.230176 -4.2368526 -4.2383595 -4.2393184 -4.2395959][-4.2281628 -4.2360773 -4.2493563 -4.2581668 -4.2608123 -4.2613506 -4.2471757 -4.227396 -4.22201 -4.2334991 -4.2432122 -4.2433238 -4.2422352 -4.2406759 -4.2375779][-4.2447824 -4.254281 -4.2656808 -4.2691278 -4.2634125 -4.2502728 -4.2307367 -4.2156844 -4.2225542 -4.2450294 -4.2564764 -4.2514682 -4.2473793 -4.2451057 -4.238214][-4.2580175 -4.2704549 -4.2751584 -4.2663608 -4.247407 -4.2184129 -4.1909714 -4.1813693 -4.2055717 -4.2456169 -4.2655644 -4.259882 -4.2556944 -4.2520766 -4.2410793][-4.2701592 -4.2837672 -4.278748 -4.2517166 -4.2118306 -4.1585855 -4.1162891 -4.11278 -4.1555114 -4.2168 -4.2541494 -4.2623138 -4.2645922 -4.2636185 -4.2545185][-4.2714005 -4.2850561 -4.2726197 -4.2267237 -4.1589 -4.0775385 -4.0182567 -4.016974 -4.0756278 -4.1587105 -4.2195053 -4.2488561 -4.2616224 -4.2665677 -4.2645025][-4.2620254 -4.2717209 -4.2573385 -4.2007933 -4.1069384 -4.00132 -3.9317641 -3.9313738 -3.9979022 -4.0922117 -4.1714206 -4.2180061 -4.2444668 -4.2597947 -4.2643318][-4.2582917 -4.2625108 -4.2492118 -4.195816 -4.1035624 -4.0058103 -3.9426134 -3.9334738 -3.9816678 -4.0595775 -4.1358671 -4.1894636 -4.2279539 -4.2512183 -4.2596869][-4.2654643 -4.2656341 -4.2555132 -4.2181945 -4.156065 -4.0975676 -4.0569448 -4.0390506 -4.0535507 -4.0923653 -4.1417837 -4.1842113 -4.2208724 -4.2465744 -4.2547536][-4.2646356 -4.2610312 -4.2527337 -4.2315617 -4.2037516 -4.1806211 -4.1617403 -4.1459284 -4.1409044 -4.1503134 -4.1751 -4.1978593 -4.2207708 -4.2396326 -4.2449961][-4.2586722 -4.2504463 -4.2417903 -4.2337413 -4.2284288 -4.227314 -4.2246108 -4.21784 -4.2104568 -4.2048659 -4.20685 -4.211164 -4.2181454 -4.2249908 -4.2277546][-4.247581 -4.2396817 -4.2342596 -4.23469 -4.24052 -4.2514648 -4.2615 -4.2626119 -4.2554312 -4.2405624 -4.223949 -4.2153358 -4.2160215 -4.2161746 -4.2157955][-4.23835 -4.2397027 -4.2433615 -4.2527852 -4.2666388 -4.2810636 -4.2910528 -4.2899318 -4.2779632 -4.253489 -4.2279348 -4.2157469 -4.2184153 -4.2202024 -4.2168641][-4.2330003 -4.2421775 -4.2541938 -4.2730336 -4.2942371 -4.3096919 -4.3144746 -4.3054132 -4.2834988 -4.25306 -4.227428 -4.2184658 -4.22427 -4.2305818 -4.2294741]]...]
INFO - root - 2017-12-07 02:52:56.000806: step 3210, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 1.288 sec/batch; 58h:20m:10s remains)
INFO - root - 2017-12-07 02:53:08.847482: step 3220, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 1.338 sec/batch; 60h:35m:12s remains)
INFO - root - 2017-12-07 02:53:21.577000: step 3230, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 1.210 sec/batch; 54h:47m:05s remains)
INFO - root - 2017-12-07 02:53:34.378095: step 3240, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 1.286 sec/batch; 58h:14m:02s remains)
INFO - root - 2017-12-07 02:53:47.055441: step 3250, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 1.249 sec/batch; 56h:34m:14s remains)
INFO - root - 2017-12-07 02:53:59.714765: step 3260, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 1.231 sec/batch; 55h:43m:03s remains)
INFO - root - 2017-12-07 02:54:12.477830: step 3270, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 1.265 sec/batch; 57h:17m:05s remains)
INFO - root - 2017-12-07 02:54:25.388408: step 3280, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 1.314 sec/batch; 59h:29m:21s remains)
INFO - root - 2017-12-07 02:54:38.038502: step 3290, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 1.244 sec/batch; 56h:19m:22s remains)
INFO - root - 2017-12-07 02:54:50.721058: step 3300, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 1.360 sec/batch; 61h:33m:23s remains)
2017-12-07 02:54:51.793652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593889 -4.2434106 -4.2082572 -4.1680083 -4.1456289 -4.1378369 -4.1392808 -4.1402359 -4.1679354 -4.2272496 -4.276834 -4.2913103 -4.2765832 -4.240458 -4.1917062][-4.2562704 -4.2356286 -4.1995597 -4.1624055 -4.1418209 -4.132504 -4.1239996 -4.1160078 -4.1395311 -4.1989121 -4.2514124 -4.2711077 -4.2550883 -4.210937 -4.1499076][-4.2561288 -4.2345262 -4.2005081 -4.1673923 -4.1460648 -4.1308513 -4.1079211 -4.0894446 -4.1109662 -4.1696229 -4.2197976 -4.243299 -4.2306166 -4.183917 -4.1169519][-4.2531533 -4.2378826 -4.2089372 -4.1782131 -4.1522555 -4.12369 -4.0815334 -4.0485778 -4.0711861 -4.1341882 -4.1867533 -4.2189035 -4.2127843 -4.1741629 -4.1062613][-4.2456412 -4.239593 -4.2163596 -4.1843905 -4.1458712 -4.093287 -4.0253768 -3.9776635 -4.0153775 -4.098743 -4.1654363 -4.2089477 -4.2107286 -4.1825304 -4.1187291][-4.2307878 -4.2330894 -4.2160482 -4.1811414 -4.1259775 -4.0453544 -3.9546168 -3.9013577 -3.9679487 -4.0782719 -4.1623654 -4.2144308 -4.2229605 -4.2017341 -4.1447825][-4.2055669 -4.2148557 -4.2026467 -4.1649294 -4.1006761 -4.0080509 -3.9122832 -3.8615451 -3.9452953 -4.0681481 -4.1593008 -4.2179909 -4.2377977 -4.2260838 -4.1794448][-4.17706 -4.1880727 -4.178194 -4.1421018 -4.0802865 -3.9933228 -3.9128735 -3.8777368 -3.956831 -4.0673285 -4.1523376 -4.2120557 -4.2396564 -4.2376866 -4.2028561][-4.1645112 -4.1754794 -4.1689267 -4.1346545 -4.0797462 -4.0093322 -3.9536257 -3.9354641 -3.99622 -4.08225 -4.1506109 -4.2037029 -4.2327223 -4.2375736 -4.2147446][-4.1768494 -4.1864824 -4.1802926 -4.14767 -4.0978642 -4.0399871 -4.0035019 -3.9995532 -4.0474744 -4.1102223 -4.16331 -4.2091889 -4.2378774 -4.2476835 -4.2350073][-4.2008791 -4.2083859 -4.1999283 -4.1662841 -4.1181393 -4.0660672 -4.0404091 -4.0498648 -4.0917692 -4.1423111 -4.1872263 -4.2308741 -4.2604942 -4.2708492 -4.2645993][-4.2271795 -4.2339106 -4.22267 -4.1870146 -4.1395979 -4.0928769 -4.0710759 -4.0864658 -4.1263547 -4.1741362 -4.2171335 -4.2588115 -4.2845168 -4.2926211 -4.2907505][-4.2473583 -4.2518678 -4.2411819 -4.2104177 -4.1709356 -4.1294088 -4.1076813 -4.1234889 -4.1622357 -4.2075133 -4.248332 -4.2847166 -4.304337 -4.3093662 -4.3099537][-4.2572017 -4.2618241 -4.2548943 -4.2326427 -4.2029905 -4.1685314 -4.1481509 -4.1600366 -4.195518 -4.2365088 -4.273088 -4.302815 -4.3167562 -4.3195624 -4.3202238][-4.2662334 -4.27129 -4.2705321 -4.2592349 -4.2398911 -4.213696 -4.1952229 -4.2020445 -4.2285314 -4.2617459 -4.2915378 -4.313067 -4.3212409 -4.3222418 -4.3222694]]...]
INFO - root - 2017-12-07 02:55:04.529718: step 3310, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 1.278 sec/batch; 57h:51m:08s remains)
INFO - root - 2017-12-07 02:55:17.262371: step 3320, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 1.273 sec/batch; 57h:36m:03s remains)
INFO - root - 2017-12-07 02:55:30.039579: step 3330, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 1.305 sec/batch; 59h:03m:00s remains)
INFO - root - 2017-12-07 02:55:42.669571: step 3340, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 1.240 sec/batch; 56h:07m:59s remains)
INFO - root - 2017-12-07 02:55:55.347102: step 3350, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 1.268 sec/batch; 57h:22m:16s remains)
INFO - root - 2017-12-07 02:56:08.191756: step 3360, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 1.326 sec/batch; 59h:59m:24s remains)
INFO - root - 2017-12-07 02:56:20.931951: step 3370, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 1.286 sec/batch; 58h:11m:31s remains)
INFO - root - 2017-12-07 02:56:36.569843: step 3380, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 1.296 sec/batch; 58h:37m:03s remains)
INFO - root - 2017-12-07 02:56:49.277516: step 3390, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 1.256 sec/batch; 56h:50m:02s remains)
INFO - root - 2017-12-07 02:57:02.006087: step 3400, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 1.310 sec/batch; 59h:16m:04s remains)
2017-12-07 02:57:03.022731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1961975 -4.1953678 -4.1969132 -4.1981006 -4.1994772 -4.2009416 -4.2016892 -4.2011938 -4.1994958 -4.1969261 -4.194355 -4.1921258 -4.1901188 -4.1888871 -4.1887388][-4.1873326 -4.1876574 -4.1905942 -4.1929779 -4.1956596 -4.1985455 -4.2009025 -4.201901 -4.2007756 -4.1976967 -4.1937909 -4.1893821 -4.1845202 -4.1808133 -4.1793385][-4.175714 -4.1761341 -4.1793776 -4.1821175 -4.1855578 -4.1901565 -4.1955261 -4.2005262 -4.20349 -4.2034044 -4.2004085 -4.1946535 -4.1863484 -4.1785088 -4.17393][-4.1617064 -4.1591058 -4.1596112 -4.1605458 -4.1635146 -4.1693563 -4.1783614 -4.1892271 -4.1992459 -4.2061067 -4.2080112 -4.2042284 -4.1948519 -4.1838994 -4.1756053][-4.1483755 -4.139061 -4.1328783 -4.1282983 -4.128161 -4.1334715 -4.14512 -4.1620259 -4.18083 -4.1973243 -4.2071643 -4.2086005 -4.2017379 -4.1910925 -4.1812191][-4.1397309 -4.1212296 -4.1049232 -4.0909142 -4.0836267 -4.0842395 -4.0941572 -4.113028 -4.1385741 -4.1649561 -4.1852355 -4.1957951 -4.1961303 -4.1904893 -4.1831908][-4.1351795 -4.1072531 -4.0801225 -4.0554342 -4.0387506 -4.0311432 -4.0343161 -4.0495567 -4.0768557 -4.1100531 -4.1405187 -4.1628022 -4.1739807 -4.1768861 -4.1755781][-4.1356196 -4.1018038 -4.0682783 -4.0371413 -4.0141721 -3.9999998 -3.9953046 -4.0026278 -4.0247812 -4.0573111 -4.0917883 -4.1225004 -4.1441007 -4.156415 -4.1626749][-4.1427555 -4.1101327 -4.0782847 -4.0489364 -4.0270534 -4.0124145 -4.0037756 -4.0028014 -4.0142307 -4.0367217 -4.0650964 -4.0952454 -4.1212153 -4.1398234 -4.152246][-4.1563821 -4.1309376 -4.107276 -4.0859756 -4.0707288 -4.0608511 -4.0534143 -4.0480404 -4.0493507 -4.0589523 -4.0750494 -4.0961719 -4.1183152 -4.137042 -4.1511889][-4.1727548 -4.1566768 -4.1432385 -4.1313863 -4.1232667 -4.1183796 -4.113492 -4.1073418 -4.1034045 -4.1042414 -4.109911 -4.1203032 -4.1341453 -4.1477642 -4.1590552][-4.1848245 -4.1763163 -4.1712823 -4.1667714 -4.1636162 -4.1617 -4.1587811 -4.1539583 -4.149487 -4.1472692 -4.1478262 -4.151443 -4.15823 -4.165772 -4.1719155][-4.18948 -4.1848745 -4.1842246 -4.1834912 -4.1826539 -4.1819077 -4.1802654 -4.17741 -4.1747379 -4.1732159 -4.1729918 -4.1741924 -4.1772904 -4.1808696 -4.1829271][-4.1890588 -4.1857638 -4.1865287 -4.1871791 -4.1872439 -4.186861 -4.1857486 -4.1840267 -4.1826782 -4.1820488 -4.1819215 -4.1823521 -4.1836963 -4.1854525 -4.1861467][-4.1883111 -4.1848135 -4.1854477 -4.1861577 -4.1864634 -4.1864042 -4.1858177 -4.1849442 -4.184267 -4.1838584 -4.1835313 -4.1832819 -4.1834116 -4.1839495 -4.1842794]]...]
INFO - root - 2017-12-07 02:57:15.627037: step 3410, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 1.260 sec/batch; 57h:00m:34s remains)
INFO - root - 2017-12-07 02:57:28.394661: step 3420, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 1.227 sec/batch; 55h:30m:40s remains)
INFO - root - 2017-12-07 02:57:41.130777: step 3430, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 1.306 sec/batch; 59h:04m:32s remains)
INFO - root - 2017-12-07 02:57:53.758471: step 3440, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 1.294 sec/batch; 58h:31m:43s remains)
INFO - root - 2017-12-07 02:58:06.502261: step 3450, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 1.331 sec/batch; 60h:10m:27s remains)
INFO - root - 2017-12-07 02:58:19.262471: step 3460, loss = 2.08, batch loss = 2.02 (13.4 examples/sec; 1.197 sec/batch; 54h:08m:57s remains)
INFO - root - 2017-12-07 02:58:31.987183: step 3470, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 1.261 sec/batch; 57h:00m:25s remains)
INFO - root - 2017-12-07 02:58:44.824945: step 3480, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 1.301 sec/batch; 58h:49m:41s remains)
INFO - root - 2017-12-07 02:58:57.565025: step 3490, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 1.235 sec/batch; 55h:50m:43s remains)
INFO - root - 2017-12-07 02:59:10.339536: step 3500, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 1.307 sec/batch; 59h:05m:50s remains)
2017-12-07 02:59:11.369635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3370476 -4.3437767 -4.3534307 -4.3540564 -4.3521895 -4.3514585 -4.3549271 -4.3577447 -4.3647833 -4.3691344 -4.3618836 -4.3413911 -4.3092489 -4.2640729 -4.2173066][-4.3379779 -4.3493462 -4.3608637 -4.3579493 -4.3506994 -4.342823 -4.3391256 -4.3387237 -4.3478026 -4.3562756 -4.3527303 -4.3327789 -4.293035 -4.2382383 -4.1882129][-4.317853 -4.336226 -4.3489637 -4.3449826 -4.3334556 -4.3174644 -4.304503 -4.3057747 -4.324841 -4.3388596 -4.3385806 -4.3172035 -4.2686696 -4.2028713 -4.1530681][-4.2861919 -4.3118076 -4.3262453 -4.3227654 -4.3026595 -4.27293 -4.246325 -4.2536578 -4.2902455 -4.317142 -4.3228278 -4.3004756 -4.2445498 -4.17094 -4.1276278][-4.2615495 -4.2882109 -4.3007946 -4.2949309 -4.2658715 -4.2169909 -4.175477 -4.1868467 -4.2456594 -4.2934361 -4.3097878 -4.292583 -4.24015 -4.1751719 -4.1466231][-4.262651 -4.284318 -4.2888446 -4.2758346 -4.2355475 -4.164866 -4.0957327 -4.0985446 -4.181602 -4.2575831 -4.289659 -4.2893267 -4.2575579 -4.216125 -4.204524][-4.2902884 -4.2997413 -4.2878714 -4.261457 -4.2099738 -4.1168661 -4.0071254 -3.9836996 -4.0919657 -4.2059984 -4.2611203 -4.2847028 -4.2800035 -4.2602487 -4.2556839][-4.3190465 -4.3140669 -4.2862964 -4.2489271 -4.1897221 -4.0765834 -3.9256392 -3.86586 -3.9945114 -4.1479387 -4.2320676 -4.2754755 -4.2870283 -4.282218 -4.2798915][-4.3272862 -4.30952 -4.2733326 -4.2313676 -4.1717873 -4.0591493 -3.8949451 -3.8156693 -3.9492078 -4.1247849 -4.2280388 -4.2789354 -4.286427 -4.2787519 -4.2682862][-4.3088064 -4.2847886 -4.2501616 -4.2136693 -4.1654892 -4.0752521 -3.9430451 -3.8770916 -3.9877954 -4.1478095 -4.244915 -4.2864146 -4.2766089 -4.2501631 -4.2174969][-4.2745204 -4.2498264 -4.2224 -4.1957436 -4.162909 -4.1034718 -4.0217648 -3.9827611 -4.0609493 -4.1848912 -4.2628617 -4.2867455 -4.2607722 -4.2143955 -4.1580048][-4.2413154 -4.218483 -4.1980553 -4.1805053 -4.1637778 -4.1328206 -4.095089 -4.0809846 -4.1341448 -4.2209234 -4.2768073 -4.2835054 -4.2463617 -4.1865845 -4.1178188][-4.2087793 -4.1955738 -4.1869731 -4.17606 -4.1645603 -4.151206 -4.1416163 -4.1443219 -4.1824069 -4.2430148 -4.2824 -4.2824917 -4.2480769 -4.1895866 -4.1250873][-4.1860337 -4.1857529 -4.1893458 -4.1817651 -4.1707659 -4.1683226 -4.1748075 -4.1851187 -4.2139297 -4.2566819 -4.2859836 -4.2885027 -4.2695451 -4.2264323 -4.1775918][-4.1885719 -4.2025485 -4.2134118 -4.2044296 -4.1910238 -4.1924863 -4.2053471 -4.2198291 -4.2432251 -4.2715964 -4.291615 -4.2981749 -4.2960954 -4.2726345 -4.2388625]]...]
INFO - root - 2017-12-07 02:59:24.132793: step 3510, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 1.310 sec/batch; 59h:14m:07s remains)
INFO - root - 2017-12-07 02:59:36.875392: step 3520, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 1.253 sec/batch; 56h:37m:05s remains)
INFO - root - 2017-12-07 02:59:49.466371: step 3530, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 1.246 sec/batch; 56h:19m:43s remains)
