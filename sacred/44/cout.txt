INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "44"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-03 09:31:39.621993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:31:39.622032: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:31:39.622038: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:31:39.622042: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:31:39.622046: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-03 09:31:40.261823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-03 09:31:40.261863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-03 09:31:40.261870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-03 09:31:40.261878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-03 09:31:42.948530: step 0, loss = 1.29, batch loss = 1.04 (3.6 examples/sec; 2.220 sec/batch; 205h:03m:14s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-03 09:31:45.259671: step 10, loss = 1.21, batch loss = 0.96 (43.7 examples/sec; 0.183 sec/batch; 16h:55m:35s remains)
INFO - root - 2017-12-03 09:31:47.084972: step 20, loss = 1.25, batch loss = 1.01 (42.5 examples/sec; 0.188 sec/batch; 17h:24m:10s remains)
INFO - root - 2017-12-03 09:31:48.918977: step 30, loss = 1.22, batch loss = 0.97 (43.6 examples/sec; 0.184 sec/batch; 16h:57m:06s remains)
INFO - root - 2017-12-03 09:31:50.756201: step 40, loss = 1.29, batch loss = 1.04 (42.0 examples/sec; 0.191 sec/batch; 17h:35m:42s remains)
INFO - root - 2017-12-03 09:31:52.588082: step 50, loss = 1.37, batch loss = 1.13 (44.2 examples/sec; 0.181 sec/batch; 16h:42m:14s remains)
INFO - root - 2017-12-03 09:31:54.430464: step 60, loss = 1.21, batch loss = 0.96 (42.4 examples/sec; 0.189 sec/batch; 17h:24m:29s remains)
INFO - root - 2017-12-03 09:31:56.327032: step 70, loss = 1.32, batch loss = 1.07 (43.3 examples/sec; 0.185 sec/batch; 17h:02m:33s remains)
INFO - root - 2017-12-03 09:31:58.190625: step 80, loss = 1.21, batch loss = 0.97 (43.2 examples/sec; 0.185 sec/batch; 17h:05m:32s remains)
INFO - root - 2017-12-03 09:32:00.056382: step 90, loss = 1.26, batch loss = 1.01 (42.2 examples/sec; 0.190 sec/batch; 17h:29m:56s remains)
INFO - root - 2017-12-03 09:32:01.929004: step 100, loss = 1.30, batch loss = 1.05 (43.8 examples/sec; 0.183 sec/batch; 16h:51m:43s remains)
INFO - root - 2017-12-03 09:32:03.881146: step 110, loss = 1.30, batch loss = 1.05 (42.2 examples/sec; 0.190 sec/batch; 17h:31m:10s remains)
INFO - root - 2017-12-03 09:32:05.749693: step 120, loss = 1.33, batch loss = 1.08 (43.0 examples/sec; 0.186 sec/batch; 17h:09m:48s remains)
INFO - root - 2017-12-03 09:32:07.593711: step 130, loss = 1.25, batch loss = 1.00 (44.0 examples/sec; 0.182 sec/batch; 16h:48m:09s remains)
INFO - root - 2017-12-03 09:32:09.435655: step 140, loss = 1.26, batch loss = 1.01 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:26s remains)
INFO - root - 2017-12-03 09:32:11.294689: step 150, loss = 1.29, batch loss = 1.04 (43.3 examples/sec; 0.185 sec/batch; 17h:03m:51s remains)
INFO - root - 2017-12-03 09:32:13.161500: step 160, loss = 1.29, batch loss = 1.04 (43.4 examples/sec; 0.184 sec/batch; 17h:00m:07s remains)
INFO - root - 2017-12-03 09:32:15.028249: step 170, loss = 1.31, batch loss = 1.06 (43.3 examples/sec; 0.185 sec/batch; 17h:04m:01s remains)
INFO - root - 2017-12-03 09:32:16.912756: step 180, loss = 1.27, batch loss = 1.03 (42.4 examples/sec; 0.189 sec/batch; 17h:24m:41s remains)
INFO - root - 2017-12-03 09:32:18.815282: step 190, loss = 1.40, batch loss = 1.15 (42.1 examples/sec; 0.190 sec/batch; 17h:32m:27s remains)
INFO - root - 2017-12-03 09:32:20.715494: step 200, loss = 1.30, batch loss = 1.05 (42.0 examples/sec; 0.190 sec/batch; 17h:34m:26s remains)
INFO - root - 2017-12-03 09:32:22.656407: step 210, loss = 1.34, batch loss = 1.09 (44.6 examples/sec; 0.180 sec/batch; 16h:34m:30s remains)
INFO - root - 2017-12-03 09:32:24.512182: step 220, loss = 1.26, batch loss = 1.01 (41.6 examples/sec; 0.192 sec/batch; 17h:44m:41s remains)
INFO - root - 2017-12-03 09:32:26.409493: step 230, loss = 1.20, batch loss = 0.96 (40.1 examples/sec; 0.199 sec/batch; 18h:23m:53s remains)
INFO - root - 2017-12-03 09:32:28.290096: step 240, loss = 1.23, batch loss = 0.98 (43.5 examples/sec; 0.184 sec/batch; 16h:58m:46s remains)
INFO - root - 2017-12-03 09:32:30.168445: step 250, loss = 1.13, batch loss = 0.88 (41.2 examples/sec; 0.194 sec/batch; 17h:55m:39s remains)
INFO - root - 2017-12-03 09:32:32.063109: step 260, loss = 1.28, batch loss = 1.03 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:46s remains)
INFO - root - 2017-12-03 09:32:33.931256: step 270, loss = 1.29, batch loss = 1.04 (43.7 examples/sec; 0.183 sec/batch; 16h:52m:53s remains)
INFO - root - 2017-12-03 09:32:35.805021: step 280, loss = 1.30, batch loss = 1.05 (42.6 examples/sec; 0.188 sec/batch; 17h:18m:39s remains)
INFO - root - 2017-12-03 09:32:37.679157: step 290, loss = 1.30, batch loss = 1.06 (43.3 examples/sec; 0.185 sec/batch; 17h:02m:25s remains)
INFO - root - 2017-12-03 09:32:39.554951: step 300, loss = 1.38, batch loss = 1.13 (43.1 examples/sec; 0.186 sec/batch; 17h:07m:49s remains)
INFO - root - 2017-12-03 09:32:41.547873: step 310, loss = 1.30, batch loss = 1.05 (43.0 examples/sec; 0.186 sec/batch; 17h:11m:07s remains)
INFO - root - 2017-12-03 09:32:43.426495: step 320, loss = 1.27, batch loss = 1.02 (42.4 examples/sec; 0.189 sec/batch; 17h:24m:57s remains)
INFO - root - 2017-12-03 09:32:45.302788: step 330, loss = 1.32, batch loss = 1.07 (42.5 examples/sec; 0.188 sec/batch; 17h:21m:41s remains)
INFO - root - 2017-12-03 09:32:47.188624: step 340, loss = 1.22, batch loss = 0.97 (43.5 examples/sec; 0.184 sec/batch; 16h:58m:14s remains)
INFO - root - 2017-12-03 09:32:49.082395: step 350, loss = 1.25, batch loss = 1.00 (42.0 examples/sec; 0.190 sec/batch; 17h:33m:18s remains)
INFO - root - 2017-12-03 09:32:50.961096: step 360, loss = 1.24, batch loss = 0.99 (41.5 examples/sec; 0.193 sec/batch; 17h:46m:12s remains)
INFO - root - 2017-12-03 09:32:52.847689: step 370, loss = 1.20, batch loss = 0.95 (41.4 examples/sec; 0.193 sec/batch; 17h:48m:42s remains)
INFO - root - 2017-12-03 09:32:54.747617: step 380, loss = 1.25, batch loss = 1.00 (41.1 examples/sec; 0.195 sec/batch; 17h:57m:03s remains)
INFO - root - 2017-12-03 09:32:56.657881: step 390, loss = 1.28, batch loss = 1.03 (43.4 examples/sec; 0.184 sec/batch; 17h:01m:02s remains)
INFO - root - 2017-12-03 09:32:58.532175: step 400, loss = 1.27, batch loss = 1.02 (42.8 examples/sec; 0.187 sec/batch; 17h:15m:22s remains)
INFO - root - 2017-12-03 09:33:00.501804: step 410, loss = 1.22, batch loss = 0.97 (41.3 examples/sec; 0.194 sec/batch; 17h:51m:07s remains)
INFO - root - 2017-12-03 09:33:02.412327: step 420, loss = 1.25, batch loss = 1.00 (41.2 examples/sec; 0.194 sec/batch; 17h:53m:57s remains)
INFO - root - 2017-12-03 09:33:04.293428: step 430, loss = 1.25, batch loss = 1.01 (42.9 examples/sec; 0.186 sec/batch; 17h:11m:15s remains)
INFO - root - 2017-12-03 09:33:06.210481: step 440, loss = 1.34, batch loss = 1.09 (41.9 examples/sec; 0.191 sec/batch; 17h:36m:23s remains)
INFO - root - 2017-12-03 09:33:08.102941: step 450, loss = 1.24, batch loss = 0.99 (43.6 examples/sec; 0.183 sec/batch; 16h:55m:16s remains)
INFO - root - 2017-12-03 09:33:09.985185: step 460, loss = 1.18, batch loss = 0.93 (42.1 examples/sec; 0.190 sec/batch; 17h:31m:21s remains)
INFO - root - 2017-12-03 09:33:11.869880: step 470, loss = 1.25, batch loss = 1.00 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:59s remains)
INFO - root - 2017-12-03 09:33:13.772412: step 480, loss = 1.31, batch loss = 1.07 (41.1 examples/sec; 0.195 sec/batch; 17h:57m:08s remains)
INFO - root - 2017-12-03 09:33:15.653419: step 490, loss = 1.21, batch loss = 0.96 (44.1 examples/sec; 0.181 sec/batch; 16h:43m:20s remains)
INFO - root - 2017-12-03 09:33:17.526712: step 500, loss = 1.30, batch loss = 1.05 (42.1 examples/sec; 0.190 sec/batch; 17h:30m:43s remains)
INFO - root - 2017-12-03 09:33:19.434157: step 510, loss = 1.30, batch loss = 1.06 (42.3 examples/sec; 0.189 sec/batch; 17h:27m:35s remains)
INFO - root - 2017-12-03 09:33:21.351555: step 520, loss = 1.33, batch loss = 1.08 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:42s remains)
INFO - root - 2017-12-03 09:33:23.235934: step 530, loss = 1.27, batch loss = 1.02 (42.7 examples/sec; 0.187 sec/batch; 17h:17m:17s remains)
INFO - root - 2017-12-03 09:33:25.136236: step 540, loss = 1.33, batch loss = 1.09 (41.4 examples/sec; 0.193 sec/batch; 17h:49m:39s remains)
INFO - root - 2017-12-03 09:33:27.049474: step 550, loss = 1.37, batch loss = 1.12 (42.1 examples/sec; 0.190 sec/batch; 17h:32m:13s remains)
INFO - root - 2017-12-03 09:33:28.976878: step 560, loss = 1.26, batch loss = 1.02 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:26s remains)
INFO - root - 2017-12-03 09:33:30.887833: step 570, loss = 1.21, batch loss = 0.96 (41.5 examples/sec; 0.193 sec/batch; 17h:45m:52s remains)
INFO - root - 2017-12-03 09:33:32.778969: step 580, loss = 1.30, batch loss = 1.05 (42.1 examples/sec; 0.190 sec/batch; 17h:31m:49s remains)
INFO - root - 2017-12-03 09:33:34.676841: step 590, loss = 1.20, batch loss = 0.95 (40.4 examples/sec; 0.198 sec/batch; 18h:15m:32s remains)
INFO - root - 2017-12-03 09:33:36.559735: step 600, loss = 1.35, batch loss = 1.11 (42.9 examples/sec; 0.186 sec/batch; 17h:11m:03s remains)
INFO - root - 2017-12-03 09:33:38.540282: step 610, loss = 1.31, batch loss = 1.07 (41.2 examples/sec; 0.194 sec/batch; 17h:53m:51s remains)
INFO - root - 2017-12-03 09:33:40.473505: step 620, loss = 1.24, batch loss = 1.00 (37.5 examples/sec; 0.213 sec/batch; 19h:40m:44s remains)
INFO - root - 2017-12-03 09:33:42.371606: step 630, loss = 1.34, batch loss = 1.10 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:07s remains)
INFO - root - 2017-12-03 09:33:44.269141: step 640, loss = 1.29, batch loss = 1.04 (42.6 examples/sec; 0.188 sec/batch; 17h:17m:30s remains)
INFO - root - 2017-12-03 09:33:46.171472: step 650, loss = 1.24, batch loss = 0.99 (41.9 examples/sec; 0.191 sec/batch; 17h:36m:15s remains)
INFO - root - 2017-12-03 09:33:48.048733: step 660, loss = 1.26, batch loss = 1.01 (42.7 examples/sec; 0.187 sec/batch; 17h:15m:44s remains)
INFO - root - 2017-12-03 09:33:49.962461: step 670, loss = 1.25, batch loss = 1.00 (41.4 examples/sec; 0.193 sec/batch; 17h:48m:02s remains)
INFO - root - 2017-12-03 09:33:51.875071: step 680, loss = 1.23, batch loss = 0.98 (42.4 examples/sec; 0.189 sec/batch; 17h:22m:50s remains)
INFO - root - 2017-12-03 09:33:53.735864: step 690, loss = 1.25, batch loss = 1.00 (43.0 examples/sec; 0.186 sec/batch; 17h:07m:47s remains)
INFO - root - 2017-12-03 09:33:55.689262: step 700, loss = 1.26, batch loss = 1.01 (41.1 examples/sec; 0.194 sec/batch; 17h:55m:17s remains)
INFO - root - 2017-12-03 09:33:57.680895: step 710, loss = 1.37, batch loss = 1.12 (41.8 examples/sec; 0.191 sec/batch; 17h:38m:35s remains)
INFO - root - 2017-12-03 09:33:59.578686: step 720, loss = 1.21, batch loss = 0.97 (41.3 examples/sec; 0.194 sec/batch; 17h:52m:04s remains)
INFO - root - 2017-12-03 09:34:01.493326: step 730, loss = 1.19, batch loss = 0.94 (42.0 examples/sec; 0.191 sec/batch; 17h:34m:08s remains)
INFO - root - 2017-12-03 09:34:03.377970: step 740, loss = 1.23, batch loss = 0.99 (42.6 examples/sec; 0.188 sec/batch; 17h:17m:14s remains)
INFO - root - 2017-12-03 09:34:05.274689: step 750, loss = 1.43, batch loss = 1.19 (41.7 examples/sec; 0.192 sec/batch; 17h:41m:47s remains)
INFO - root - 2017-12-03 09:34:07.181173: step 760, loss = 1.32, batch loss = 1.08 (42.1 examples/sec; 0.190 sec/batch; 17h:31m:49s remains)
INFO - root - 2017-12-03 09:34:09.088162: step 770, loss = 1.27, batch loss = 1.02 (41.1 examples/sec; 0.194 sec/batch; 17h:55m:15s remains)
INFO - root - 2017-12-03 09:34:11.009733: step 780, loss = 1.23, batch loss = 0.99 (42.3 examples/sec; 0.189 sec/batch; 17h:26m:16s remains)
INFO - root - 2017-12-03 09:34:12.913788: step 790, loss = 1.21, batch loss = 0.97 (40.3 examples/sec; 0.198 sec/batch; 18h:16m:27s remains)
INFO - root - 2017-12-03 09:34:14.816189: step 800, loss = 1.27, batch loss = 1.03 (43.6 examples/sec; 0.184 sec/batch; 16h:55m:04s remains)
INFO - root - 2017-12-03 09:34:16.763441: step 810, loss = 1.24, batch loss = 0.99 (42.3 examples/sec; 0.189 sec/batch; 17h:25m:02s remains)
INFO - root - 2017-12-03 09:34:18.659892: step 820, loss = 1.27, batch loss = 1.03 (41.1 examples/sec; 0.195 sec/batch; 17h:56m:13s remains)
INFO - root - 2017-12-03 09:34:20.563664: step 830, loss = 1.26, batch loss = 1.02 (40.6 examples/sec; 0.197 sec/batch; 18h:10m:12s remains)
INFO - root - 2017-12-03 09:34:22.457894: step 840, loss = 1.27, batch loss = 1.02 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:44s remains)
INFO - root - 2017-12-03 09:34:24.380748: step 850, loss = 1.29, batch loss = 1.04 (40.1 examples/sec; 0.200 sec/batch; 18h:22m:58s remains)
INFO - root - 2017-12-03 09:34:26.268669: step 860, loss = 1.37, batch loss = 1.12 (42.6 examples/sec; 0.188 sec/batch; 17h:19m:06s remains)
INFO - root - 2017-12-03 09:34:28.185575: step 870, loss = 1.25, batch loss = 1.00 (41.5 examples/sec; 0.193 sec/batch; 17h:44m:12s remains)
INFO - root - 2017-12-03 09:34:30.090077: step 880, loss = 1.29, batch loss = 1.04 (41.0 examples/sec; 0.195 sec/batch; 17h:57m:11s remains)
INFO - root - 2017-12-03 09:34:31.992100: step 890, loss = 1.29, batch loss = 1.05 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:38s remains)
INFO - root - 2017-12-03 09:34:33.890288: step 900, loss = 1.22, batch loss = 0.97 (42.8 examples/sec; 0.187 sec/batch; 17h:11m:58s remains)
INFO - root - 2017-12-03 09:34:35.853629: step 910, loss = 1.36, batch loss = 1.11 (41.3 examples/sec; 0.194 sec/batch; 17h:50m:09s remains)
INFO - root - 2017-12-03 09:34:37.759492: step 920, loss = 1.32, batch loss = 1.07 (40.3 examples/sec; 0.198 sec/batch; 18h:16m:41s remains)
INFO - root - 2017-12-03 09:34:39.680680: step 930, loss = 1.29, batch loss = 1.04 (40.8 examples/sec; 0.196 sec/batch; 18h:04m:34s remains)
INFO - root - 2017-12-03 09:34:41.615949: step 940, loss = 1.25, batch loss = 1.00 (40.5 examples/sec; 0.197 sec/batch; 18h:10m:32s remains)
INFO - root - 2017-12-03 09:34:43.514249: step 950, loss = 1.27, batch loss = 1.02 (41.0 examples/sec; 0.195 sec/batch; 17h:57m:39s remains)
INFO - root - 2017-12-03 09:34:45.417890: step 960, loss = 1.32, batch loss = 1.07 (42.3 examples/sec; 0.189 sec/batch; 17h:24m:22s remains)
INFO - root - 2017-12-03 09:34:47.327113: step 970, loss = 1.29, batch loss = 1.04 (43.5 examples/sec; 0.184 sec/batch; 16h:55m:03s remains)
INFO - root - 2017-12-03 09:34:49.233828: step 980, loss = 1.37, batch loss = 1.12 (41.3 examples/sec; 0.194 sec/batch; 17h:50m:41s remains)
INFO - root - 2017-12-03 09:34:51.132182: step 990, loss = 1.21, batch loss = 0.96 (40.6 examples/sec; 0.197 sec/batch; 18h:08m:27s remains)
INFO - root - 2017-12-03 09:34:53.050097: step 1000, loss = 1.31, batch loss = 1.06 (42.4 examples/sec; 0.189 sec/batch; 17h:23m:25s remains)
INFO - root - 2017-12-03 09:34:55.042611: step 1010, loss = 1.30, batch loss = 1.06 (41.5 examples/sec; 0.193 sec/batch; 17h:45m:19s remains)
INFO - root - 2017-12-03 09:34:56.964019: step 1020, loss = 1.32, batch loss = 1.07 (42.4 examples/sec; 0.189 sec/batch; 17h:21m:30s remains)
INFO - root - 2017-12-03 09:34:58.858144: step 1030, loss = 1.23, batch loss = 0.98 (42.3 examples/sec; 0.189 sec/batch; 17h:25m:35s remains)
INFO - root - 2017-12-03 09:35:00.767167: step 1040, loss = 1.25, batch loss = 1.00 (43.0 examples/sec; 0.186 sec/batch; 17h:07m:54s remains)
INFO - root - 2017-12-03 09:35:02.742646: step 1050, loss = 1.31, batch loss = 1.07 (42.7 examples/sec; 0.187 sec/batch; 17h:14m:20s remains)
INFO - root - 2017-12-03 09:35:04.660301: step 1060, loss = 1.33, batch loss = 1.08 (40.5 examples/sec; 0.198 sec/batch; 18h:11m:22s remains)
INFO - root - 2017-12-03 09:35:06.567211: step 1070, loss = 1.31, batch loss = 1.06 (41.4 examples/sec; 0.193 sec/batch; 17h:48m:27s remains)
INFO - root - 2017-12-03 09:35:08.471632: step 1080, loss = 1.31, batch loss = 1.07 (41.8 examples/sec; 0.191 sec/batch; 17h:37m:19s remains)
INFO - root - 2017-12-03 09:35:10.401768: step 1090, loss = 1.25, batch loss = 1.00 (40.1 examples/sec; 0.199 sec/batch; 18h:21m:42s remains)
INFO - root - 2017-12-03 09:35:12.312611: step 1100, loss = 1.35, batch loss = 1.10 (41.2 examples/sec; 0.194 sec/batch; 17h:52m:26s remains)
INFO - root - 2017-12-03 09:35:14.311227: step 1110, loss = 1.25, batch loss = 1.00 (42.5 examples/sec; 0.188 sec/batch; 17h:20m:42s remains)
INFO - root - 2017-12-03 09:35:16.226377: step 1120, loss = 1.27, batch loss = 1.03 (41.3 examples/sec; 0.194 sec/batch; 17h:49m:13s remains)
INFO - root - 2017-12-03 09:35:18.133006: step 1130, loss = 1.22, batch loss = 0.97 (40.4 examples/sec; 0.198 sec/batch; 18h:14m:29s remains)
