INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "333"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
inputs Tensor("train/batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("train/siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("train/siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("train/siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("train/siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("train/batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("train/siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("train/siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("train/siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("train/siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
INFO - root - preproces -- None
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
2017-12-16 14:31:45.748447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 14:31:45.748518: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 14:31:45.748545: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 14:31:45.748567: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 14:31:45.748588: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-16 14:31:46.636560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-16 14:31:46.636628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-16 14:31:46.636655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-16 14:31:46.636681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO - root - Restore from last checkpoint: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-80000
inputs Tensor("val/batch:0", shape=(8, 127, 127, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("val/siamese_fc/pool1/MaxPool:0", shape=(8, 29, 29, 96), dtype=float32)
net Tensor("val/siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("val/siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("val/siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
inputs Tensor("val/batch:1", shape=(8, 239, 239, 3), dtype=float32, device=/device:CPU:0)
conv1 Tensor("val/siamese_fc_1/pool1/MaxPool:0", shape=(8, 57, 57, 96), dtype=float32)
net Tensor("val/siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("val/siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("val/siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-80000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-clip-4--4/model.ckpt-80000
INFO - root - Starting threads 0 ...

INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-16 14:31:52.345208: step 0, loss = 1.60, batch loss = 1.44 (2.1 examples/sec; 3.744 sec/batch; 345h:45m:59s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2--------/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2--------/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-16 14:31:56.454526: step 10, loss = 0.88, batch loss = 0.71 (32.0 examples/sec; 0.250 sec/batch; 23h:06m:44s remains)
INFO - root - 2017-12-16 14:31:58.853607: step 20, loss = 0.85, batch loss = 0.68 (31.1 examples/sec; 0.257 sec/batch; 23h:45m:54s remains)
INFO - root - 2017-12-16 14:32:01.218330: step 30, loss = 0.85, batch loss = 0.67 (31.5 examples/sec; 0.254 sec/batch; 23h:25m:58s remains)
INFO - root - 2017-12-16 14:32:03.432558: step 40, loss = 0.89, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:07s remains)
INFO - root - 2017-12-16 14:32:05.660794: step 50, loss = 0.89, batch loss = 0.68 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:43s remains)
INFO - root - 2017-12-16 14:32:07.912209: step 60, loss = 0.89, batch loss = 0.66 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:08s remains)
INFO - root - 2017-12-16 14:32:10.177848: step 70, loss = 0.90, batch loss = 0.65 (35.0 examples/sec; 0.228 sec/batch; 21h:04m:36s remains)
INFO - root - 2017-12-16 14:32:12.434645: step 80, loss = 0.92, batch loss = 0.65 (37.7 examples/sec; 0.212 sec/batch; 19h:36m:22s remains)
INFO - root - 2017-12-16 14:32:14.679472: step 90, loss = 0.90, batch loss = 0.61 (36.3 examples/sec; 0.220 sec/batch; 20h:21m:25s remains)
INFO - root - 2017-12-16 14:32:16.927482: step 100, loss = 0.89, batch loss = 0.59 (35.8 examples/sec; 0.224 sec/batch; 20h:38m:35s remains)
INFO - root - 2017-12-16 14:32:19.265768: step 110, loss = 0.80, batch loss = 0.50 (37.4 examples/sec; 0.214 sec/batch; 19h:46m:18s remains)
INFO - root - 2017-12-16 14:32:21.535047: step 120, loss = 0.83, batch loss = 0.52 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:46s remains)
INFO - root - 2017-12-16 14:32:23.759994: step 130, loss = 0.91, batch loss = 0.59 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:51s remains)
INFO - root - 2017-12-16 14:32:25.963470: step 140, loss = 0.86, batch loss = 0.53 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:16s remains)
INFO - root - 2017-12-16 14:32:28.223737: step 150, loss = 0.89, batch loss = 0.55 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:24s remains)
INFO - root - 2017-12-16 14:32:30.437468: step 160, loss = 0.87, batch loss = 0.53 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:01s remains)
INFO - root - 2017-12-16 14:32:32.668606: step 170, loss = 0.92, batch loss = 0.58 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:40s remains)
INFO - root - 2017-12-16 14:32:34.913077: step 180, loss = 1.00, batch loss = 0.66 (36.6 examples/sec; 0.218 sec/batch; 20h:10m:05s remains)
INFO - root - 2017-12-16 14:32:37.123778: step 190, loss = 0.88, batch loss = 0.54 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:26s remains)
INFO - root - 2017-12-16 14:32:39.350381: step 200, loss = 0.90, batch loss = 0.56 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:33s remains)
INFO - root - 2017-12-16 14:32:41.775589: step 210, loss = 0.92, batch loss = 0.58 (30.4 examples/sec; 0.263 sec/batch; 24h:16m:13s remains)
INFO - root - 2017-12-16 14:32:43.987643: step 220, loss = 0.92, batch loss = 0.58 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:25s remains)
INFO - root - 2017-12-16 14:32:46.258428: step 230, loss = 0.87, batch loss = 0.53 (34.9 examples/sec; 0.229 sec/batch; 21h:10m:40s remains)
INFO - root - 2017-12-16 14:32:48.475074: step 240, loss = 0.97, batch loss = 0.63 (35.5 examples/sec; 0.226 sec/batch; 20h:49m:06s remains)
INFO - root - 2017-12-16 14:32:50.724782: step 250, loss = 0.83, batch loss = 0.49 (34.5 examples/sec; 0.232 sec/batch; 21h:25m:03s remains)
INFO - root - 2017-12-16 14:32:52.933853: step 260, loss = 0.86, batch loss = 0.52 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:35s remains)
INFO - root - 2017-12-16 14:32:55.141902: step 270, loss = 0.85, batch loss = 0.51 (36.7 examples/sec; 0.218 sec/batch; 20h:05m:30s remains)
INFO - root - 2017-12-16 14:32:57.383728: step 280, loss = 0.83, batch loss = 0.49 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:58s remains)
INFO - root - 2017-12-16 14:32:59.612858: step 290, loss = 0.88, batch loss = 0.54 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:56s remains)
INFO - root - 2017-12-16 14:33:01.842391: step 300, loss = 1.12, batch loss = 0.78 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:46s remains)
INFO - root - 2017-12-16 14:33:04.185755: step 310, loss = 1.07, batch loss = 0.73 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:41s remains)
INFO - root - 2017-12-16 14:33:06.456276: step 320, loss = 0.96, batch loss = 0.62 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:08s remains)
